
----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/integrations.py----------------------------------------
A:transformers.integrations.logger->transformers.utils.logging.get_logger(__name__)
A:transformers.integrations.model_path->os.path.join(checkpoint_dir, subdir)
A:transformers.integrations.metrics->trainer.evaluate()
A:transformers.integrations.trainer.objective->trainer.compute_objective(metrics)
A:transformers.integrations.timeout->kwargs.pop('timeout', None)
A:transformers.integrations.n_jobs->int(kwargs.pop('n_jobs', 1))
A:transformers.integrations.study->optuna.create_study(direction=direction, **kwargs)
A:transformers.integrations.num_gpus_per_trial->int(np.ceil(num_gpus_per_trial / n_jobs))
A:transformers.integrations.kwargs['progress_reporter']->CLIReporter(metric_columns=['objective'])
A:transformers.integrations.analysis->ray.tune.run(_objective, config=trainer.hp_space(None), num_samples=n_trials, **kwargs)
A:transformers.integrations.best_trial->ray.tune.run(_objective, config=trainer.hp_space(None), num_samples=n_trials, **kwargs).get_best_trial(metric='objective', mode=direction[:3])
A:transformers.integrations.best_run->BestRun(best_trial.trial_id, best_trial.last_result['objective'], best_trial.config)
transformers.integrations.default_hp_search_backend()
transformers.integrations.is_comet_available()
transformers.integrations.is_optuna_available()
transformers.integrations.is_ray_available()
transformers.integrations.is_tensorboard_available()
transformers.integrations.is_wandb_available()
transformers.integrations.run_hp_search_optuna(trainer,n_trials:int,direction:str,**kwargs)->BestRun
transformers.integrations.run_hp_search_ray(trainer,n_trials:int,direction:str,**kwargs)->BestRun
transformers.is_comet_available()
transformers.is_optuna_available()
transformers.is_ray_available()
transformers.is_tensorboard_available()
transformers.is_wandb_available()


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/modeling_tf_xlnet.py----------------------------------------
A:transformers.modeling_tf_xlnet.logger->utils.logging.get_logger(__name__)
A:transformers.modeling_tf_xlnet.self.layer_norm->tensorflow.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name='layer_norm')
A:transformers.modeling_tf_xlnet.self.dropout->tensorflow.keras.layers.Dropout(config.dropout)
A:transformers.modeling_tf_xlnet.initializer->get_initializer(self.initializer_range)
A:transformers.modeling_tf_xlnet.self.q->self.add_weight(shape=(self.d_model, self.n_head, self.d_head), initializer=initializer, trainable=True, name='q')
A:transformers.modeling_tf_xlnet.self.k->self.add_weight(shape=(self.d_model, self.n_head, self.d_head), initializer=initializer, trainable=True, name='k')
A:transformers.modeling_tf_xlnet.self.v->self.add_weight(shape=(self.d_model, self.n_head, self.d_head), initializer=initializer, trainable=True, name='v')
A:transformers.modeling_tf_xlnet.self.o->self.add_weight(shape=(self.d_model, self.n_head, self.d_head), initializer=initializer, trainable=True, name='o')
A:transformers.modeling_tf_xlnet.self.r->self.add_weight(shape=(self.d_model, self.n_head, self.d_head), initializer=initializer, trainable=True, name='r')
A:transformers.modeling_tf_xlnet.self.r_r_bias->self.add_weight(shape=(self.n_head, self.d_head), initializer='zeros', trainable=True, name='r_r_bias')
A:transformers.modeling_tf_xlnet.self.r_s_bias->self.add_weight(shape=(self.n_head, self.d_head), initializer='zeros', trainable=True, name='r_s_bias')
A:transformers.modeling_tf_xlnet.self.r_w_bias->self.add_weight(shape=(self.n_head, self.d_head), initializer='zeros', trainable=True, name='r_w_bias')
A:transformers.modeling_tf_xlnet.self.seg_embed->self.add_weight(shape=(2, self.n_head, self.d_head), initializer=initializer, trainable=True, name='seg_embed')
A:transformers.modeling_tf_xlnet.x_size->shape_list(x)
A:transformers.modeling_tf_xlnet.x->tensorflow.reshape(x, (x_size[0], x_size[1] - 1, x_size[2], x_size[3]))
A:transformers.modeling_tf_xlnet.ac->tensorflow.einsum('ibnd,jbnd->ijbn', q_head + self.r_w_bias, k_head_h)
A:transformers.modeling_tf_xlnet.bd->self.rel_shift(bd, klen=shape_list(ac)[1])
A:transformers.modeling_tf_xlnet.ef->tensorflow.einsum('ijbs,ibns->ijbn', seg_mat, ef)
A:transformers.modeling_tf_xlnet.attn_prob->self.dropout(attn_prob, training=training)
A:transformers.modeling_tf_xlnet.attn_vec->self.rel_attn_core(q_head_h, k_head_h, v_head_h, k_head_r, seg_mat, attn_mask_h, head_mask, output_attentions, training=training)
A:transformers.modeling_tf_xlnet.attn_out->self.dropout(attn_out, training=training)
A:transformers.modeling_tf_xlnet.output->self.sequence_summary(output)
A:transformers.modeling_tf_xlnet.cat->tensorflow.concat([mems, h], axis=0)
A:transformers.modeling_tf_xlnet.k_head_h->tensorflow.einsum('ibh,hnd->ibnd', cat, self.k)
A:transformers.modeling_tf_xlnet.v_head_h->tensorflow.einsum('ibh,hnd->ibnd', cat, self.v)
A:transformers.modeling_tf_xlnet.k_head_r->tensorflow.einsum('ibh,hnd->ibnd', r, self.r)
A:transformers.modeling_tf_xlnet.q_head_h->tensorflow.einsum('ibh,hnd->ibnd', h, self.q)
A:transformers.modeling_tf_xlnet.attn_vec_h->self.rel_attn_core(q_head_h, k_head_h, v_head_h, k_head_r, seg_mat, attn_mask_h, head_mask, output_attentions, training=training)
A:transformers.modeling_tf_xlnet.output_h->self.dropout(word_emb_k, training=training)
A:transformers.modeling_tf_xlnet.q_head_g->tensorflow.einsum('mbnd,mlb->lbnd', q_head_g, target_mapping)
A:transformers.modeling_tf_xlnet.attn_vec_g->self.rel_attn_core(q_head_g, k_head_h, v_head_h, k_head_r, seg_mat, attn_mask_g, head_mask, output_attentions, training=training)
A:transformers.modeling_tf_xlnet.output_g->self.dropout(word_emb_q, training=training)
A:transformers.modeling_tf_xlnet.self.layer_1->tensorflow.keras.layers.Dense(config.d_inner, kernel_initializer=get_initializer(config.initializer_range), name='layer_1')
A:transformers.modeling_tf_xlnet.self.layer_2->tensorflow.keras.layers.Dense(config.d_model, kernel_initializer=get_initializer(config.initializer_range), name='layer_2')
A:transformers.modeling_tf_xlnet.self.rel_attn->TFXLNetRelativeAttention(config, name='rel_attn')
A:transformers.modeling_tf_xlnet.self.ff->TFXLNetFeedForward(config, name='ff')
A:transformers.modeling_tf_xlnet.outputs->self.transformer(inputs, **kwargs)
A:transformers.modeling_tf_xlnet.self.bias->self.add_weight(shape=(self.vocab_size,), initializer='zeros', trainable=True, name='bias')
A:transformers.modeling_tf_xlnet.hidden_states->tuple((tf.transpose(hs, perm=(1, 0, 2)) for hs in hidden_states))
A:transformers.modeling_tf_xlnet.self.word_embedding->TFSharedEmbeddings(config.vocab_size, config.d_model, initializer_range=config.initializer_range, name='word_embedding')
A:transformers.modeling_tf_xlnet.self.mask_emb->self.add_weight(shape=(1, 1, self.d_model), initializer=initializer, trainable=True, name='mask_emb')
A:transformers.modeling_tf_xlnet.attn_mask->tensorflow.cast(attn_mask > 0, dtype=dtype_float)
A:transformers.modeling_tf_xlnet.mask_u->tensorflow.matrix_band_part(attn_mask, 0, -1)
A:transformers.modeling_tf_xlnet.mask_dia->tensorflow.matrix_band_part(attn_mask, 0, 0)
A:transformers.modeling_tf_xlnet.attn_mask_pad->tensorflow.zeros([qlen, mlen], dtype=dtype)
A:transformers.modeling_tf_xlnet.ret->tensorflow.concat([ret[:, :qlen] + mask_l - mask_dia, ret[:, qlen:]], 1)
A:transformers.modeling_tf_xlnet.mask_l->tensorflow.matrix_band_part(attn_mask, -1, 0)
A:transformers.modeling_tf_xlnet.sinusoid_inp->tensorflow.einsum('i,d->id', pos_seq, inv_freq)
A:transformers.modeling_tf_xlnet.pos_emb->self.dropout(pos_emb, training=training)
A:transformers.modeling_tf_xlnet.freq_seq->tensorflow.cast(freq_seq, dtype=dtype)
A:transformers.modeling_tf_xlnet.fwd_pos_seq->tensorflow.clip_by_value(fwd_pos_seq, -self.clamp_len, self.clamp_len)
A:transformers.modeling_tf_xlnet.bwd_pos_seq->tensorflow.clip_by_value(bwd_pos_seq, -self.clamp_len, self.clamp_len)
A:transformers.modeling_tf_xlnet.fwd_pos_emb->self.positional_embedding(fwd_pos_seq, inv_freq)
A:transformers.modeling_tf_xlnet.bwd_pos_emb->self.positional_embedding(bwd_pos_seq, inv_freq)
A:transformers.modeling_tf_xlnet.input_ids->tensorflow.concat([inputs, dummy_token], axis=1).get('input_ids')
A:transformers.modeling_tf_xlnet.attention_mask->tensorflow.concat([inputs, dummy_token], axis=1).get('attention_mask', attention_mask)
A:transformers.modeling_tf_xlnet.mems->tensorflow.concat([inputs, dummy_token], axis=1).get('mems', mems)
A:transformers.modeling_tf_xlnet.perm_mask->tensorflow.concat([inputs, dummy_token], axis=1).get('perm_mask', perm_mask)
A:transformers.modeling_tf_xlnet.target_mapping->tensorflow.concat([inputs, dummy_token], axis=1).get('target_mapping', target_mapping)
A:transformers.modeling_tf_xlnet.token_type_ids->tensorflow.concat([inputs, dummy_token], axis=1).get('token_type_ids', token_type_ids)
A:transformers.modeling_tf_xlnet.input_mask->tensorflow.concat([inputs, dummy_token], axis=1).get('input_mask', input_mask)
A:transformers.modeling_tf_xlnet.head_mask->tensorflow.concat([inputs, dummy_token], axis=1).get('head_mask', head_mask)
A:transformers.modeling_tf_xlnet.inputs_embeds->tensorflow.concat([inputs, dummy_token], axis=1).get('inputs_embeds', inputs_embeds)
A:transformers.modeling_tf_xlnet.use_cache->tensorflow.concat([inputs, dummy_token], axis=1).get('use_cache', use_cache)
A:transformers.modeling_tf_xlnet.output_attentions->tensorflow.concat([inputs, dummy_token], axis=1).get('output_attentions', output_attentions)
A:transformers.modeling_tf_xlnet.output_hidden_states->tensorflow.concat([inputs, dummy_token], axis=1).get('output_hidden_states', output_hidden_states)
A:transformers.modeling_tf_xlnet.return_dict->tensorflow.concat([inputs, dummy_token], axis=1).get('return_dict', return_dict)
A:transformers.modeling_tf_xlnet.mems_mask->tensorflow.zeros([shape_list(data_mask)[0], mlen, bsz], dtype=dtype_float)
A:transformers.modeling_tf_xlnet.data_mask->tensorflow.concat([mems_mask, data_mask], axis=1)
A:transformers.modeling_tf_xlnet.non_tgt_mask->tensorflow.cast(attn_mask + non_tgt_mask[:, :, None, None] > 0, dtype=dtype_float)
A:transformers.modeling_tf_xlnet.word_emb_k->self.word_embedding(input_ids)
A:transformers.modeling_tf_xlnet.word_emb_q->tensorflow.tile(self.mask_emb, [shape_list(target_mapping)[0], bsz, 1])
A:transformers.modeling_tf_xlnet.mem_pad->tensorflow.zeros([mlen, bsz], dtype=tf.int32)
A:transformers.modeling_tf_xlnet.cat_ids->tensorflow.concat([mem_pad, token_type_ids], 0)
A:transformers.modeling_tf_xlnet.seg_mat->tensorflow.one_hot(seg_mat, 2, dtype=dtype_float)
A:transformers.modeling_tf_xlnet.attentions->tuple((tf.transpose(t, perm=(2, 3, 0, 1)) for t in attentions))
A:transformers.modeling_tf_xlnet.self.transformer->TFXLNetMainLayer(config, name='transformer')
A:transformers.modeling_tf_xlnet.self.lm_loss->TFXLNetLMHead(config, self.transformer.word_embedding, name='lm_loss')
A:transformers.modeling_tf_xlnet.dummy_token->tensorflow.zeros((effective_batch_size, 1), dtype=tf.int32)
A:transformers.modeling_tf_xlnet.inputs->tensorflow.concat([inputs, dummy_token], axis=1)
A:transformers.modeling_tf_xlnet.perm_mask_seq_end->tensorflow.ones((effective_batch_size, sequence_length, 1), dtype=tf.float32)
A:transformers.modeling_tf_xlnet.target_mapping_seq_end->tensorflow.ones((effective_batch_size, 1, 1), dtype=tf.float32)
A:transformers.modeling_tf_xlnet.inputs['mems']->tuple((layer_past[:-offset, :, :] for layer_past in past))
A:transformers.modeling_tf_xlnet.labels->tensorflow.concat([inputs, dummy_token], axis=1).pop('labels', labels)
A:transformers.modeling_tf_xlnet.transformer_outputs->self.transformer(inputs, attention_mask=attention_mask, mems=mems, perm_mask=perm_mask, target_mapping=target_mapping, token_type_ids=token_type_ids, input_mask=input_mask, head_mask=head_mask, inputs_embeds=inputs_embeds, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, training=training)
A:transformers.modeling_tf_xlnet.logits->self.qa_outputs(sequence_output)
A:transformers.modeling_tf_xlnet.loss->self.compute_loss(labels, (start_logits, end_logits))
A:transformers.modeling_tf_xlnet.self.sequence_summary->TFSequenceSummary(config, initializer_range=config.initializer_range, name='sequence_summary')
A:transformers.modeling_tf_xlnet.self.logits_proj->tensorflow.keras.layers.Dense(1, kernel_initializer=get_initializer(config.initializer_range), name='logits_proj')
A:transformers.modeling_tf_xlnet.reshaped_logits->tensorflow.reshape(logits, (-1, num_choices))
A:transformers.modeling_tf_xlnet.self.classifier->tensorflow.keras.layers.Dense(config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name='classifier')
A:transformers.modeling_tf_xlnet.self.qa_outputs->tensorflow.keras.layers.Dense(config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name='qa_outputs')
A:transformers.modeling_tf_xlnet.start_positions->tensorflow.concat([inputs, dummy_token], axis=1).pop('start_positions', start_positions)
A:transformers.modeling_tf_xlnet.end_positions->tensorflow.concat([inputs, dummy_token], axis=1).pop('end_positions', start_positions)
A:transformers.modeling_tf_xlnet.(start_logits, end_logits)->tensorflow.split(logits, 2, axis=-1)
A:transformers.modeling_tf_xlnet.start_logits->tensorflow.squeeze(start_logits, axis=-1)
A:transformers.modeling_tf_xlnet.end_logits->tensorflow.squeeze(end_logits, axis=-1)
transformers.TFXLNetForMultipleChoice(self,config,*inputs,**kwargs)
transformers.TFXLNetForMultipleChoice.call(self,inputs=None,token_type_ids=None,input_mask=None,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,head_mask=None,inputs_embeds=None,use_cache=True,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.TFXLNetForMultipleChoice.dummy_inputs(self)
transformers.TFXLNetForMultipleChoiceOutput(ModelOutput)
transformers.TFXLNetForQuestionAnsweringSimple(self,config,*inputs,**kwargs)
transformers.TFXLNetForQuestionAnsweringSimple.call(self,inputs=None,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,token_type_ids=None,input_mask=None,head_mask=None,inputs_embeds=None,use_cache=True,output_attentions=None,output_hidden_states=None,return_dict=None,start_positions=None,end_positions=None,training=False)
transformers.TFXLNetForQuestionAnsweringSimpleOutput(ModelOutput)
transformers.TFXLNetForSequenceClassification(self,config,*inputs,**kwargs)
transformers.TFXLNetForSequenceClassification.call(self,inputs=None,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,token_type_ids=None,input_mask=None,head_mask=None,inputs_embeds=None,use_cache=True,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.TFXLNetForSequenceClassificationOutput(ModelOutput)
transformers.TFXLNetForTokenClassification(self,config,*inputs,**kwargs)
transformers.TFXLNetForTokenClassification.call(self,inputs=None,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,token_type_ids=None,input_mask=None,head_mask=None,inputs_embeds=None,use_cache=True,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.TFXLNetForTokenClassificationOutput(ModelOutput)
transformers.TFXLNetLMHeadModel(self,config,*inputs,**kwargs)
transformers.TFXLNetLMHeadModel.call(self,inputs,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,token_type_ids=None,input_mask=None,head_mask=None,inputs_embeds=None,use_cache=True,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.TFXLNetLMHeadModel.get_output_embeddings(self)
transformers.TFXLNetLMHeadModel.prepare_inputs_for_generation(self,inputs,past,**kwargs)
transformers.TFXLNetLMHeadModelOutput(ModelOutput)
transformers.TFXLNetMainLayer(self,config,**kwargs)
transformers.TFXLNetMainLayer._prune_heads(self,heads_to_prune)
transformers.TFXLNetMainLayer._resize_token_embeddings(self,new_num_tokens)
transformers.TFXLNetMainLayer.build(self,input_shape)
transformers.TFXLNetMainLayer.cache_mem(self,curr_out,prev_mem)
transformers.TFXLNetMainLayer.call(self,inputs,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,token_type_ids=None,input_mask=None,head_mask=None,inputs_embeds=None,use_cache=True,output_attentions=None,output_hidden_states=None,return_dict=None,training=False)
transformers.TFXLNetMainLayer.create_mask(self,qlen,mlen,dtype=tf.float32)
transformers.TFXLNetMainLayer.get_input_embeddings(self)
transformers.TFXLNetMainLayer.positional_embedding(pos_seq,inv_freq,bsz=None)
transformers.TFXLNetMainLayer.relative_positional_encoding(self,qlen,klen,bsz=None,dtype=None)
transformers.TFXLNetMainLayer.set_input_embeddings(self,value)
transformers.TFXLNetModel(self,config,*inputs,**kwargs)
transformers.TFXLNetModel.call(self,inputs,**kwargs)
transformers.TFXLNetModelOutput(ModelOutput)
transformers.TFXLNetPreTrainedModel(TFPreTrainedModel)
transformers.modeling_tf_xlnet.TFXLNetFeedForward(self,config,**kwargs)
transformers.modeling_tf_xlnet.TFXLNetFeedForward.__init__(self,config,**kwargs)
transformers.modeling_tf_xlnet.TFXLNetFeedForward.call(self,inp,training=False)
transformers.modeling_tf_xlnet.TFXLNetForMultipleChoice(self,config,*inputs,**kwargs)
transformers.modeling_tf_xlnet.TFXLNetForMultipleChoice.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_xlnet.TFXLNetForMultipleChoice.call(self,inputs=None,token_type_ids=None,input_mask=None,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,head_mask=None,inputs_embeds=None,use_cache=True,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.modeling_tf_xlnet.TFXLNetForMultipleChoice.dummy_inputs(self)
transformers.modeling_tf_xlnet.TFXLNetForMultipleChoiceOutput(ModelOutput)
transformers.modeling_tf_xlnet.TFXLNetForQuestionAnsweringSimple(self,config,*inputs,**kwargs)
transformers.modeling_tf_xlnet.TFXLNetForQuestionAnsweringSimple.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_xlnet.TFXLNetForQuestionAnsweringSimple.call(self,inputs=None,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,token_type_ids=None,input_mask=None,head_mask=None,inputs_embeds=None,use_cache=True,output_attentions=None,output_hidden_states=None,return_dict=None,start_positions=None,end_positions=None,training=False)
transformers.modeling_tf_xlnet.TFXLNetForQuestionAnsweringSimpleOutput(ModelOutput)
transformers.modeling_tf_xlnet.TFXLNetForSequenceClassification(self,config,*inputs,**kwargs)
transformers.modeling_tf_xlnet.TFXLNetForSequenceClassification.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_xlnet.TFXLNetForSequenceClassification.call(self,inputs=None,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,token_type_ids=None,input_mask=None,head_mask=None,inputs_embeds=None,use_cache=True,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.modeling_tf_xlnet.TFXLNetForSequenceClassificationOutput(ModelOutput)
transformers.modeling_tf_xlnet.TFXLNetForTokenClassification(self,config,*inputs,**kwargs)
transformers.modeling_tf_xlnet.TFXLNetForTokenClassification.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_xlnet.TFXLNetForTokenClassification.call(self,inputs=None,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,token_type_ids=None,input_mask=None,head_mask=None,inputs_embeds=None,use_cache=True,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.modeling_tf_xlnet.TFXLNetForTokenClassificationOutput(ModelOutput)
transformers.modeling_tf_xlnet.TFXLNetLMHead(self,config,input_embeddings,**kwargs)
transformers.modeling_tf_xlnet.TFXLNetLMHead.__init__(self,config,input_embeddings,**kwargs)
transformers.modeling_tf_xlnet.TFXLNetLMHead.build(self,input_shape)
transformers.modeling_tf_xlnet.TFXLNetLMHead.call(self,hidden_states)
transformers.modeling_tf_xlnet.TFXLNetLMHeadModel(self,config,*inputs,**kwargs)
transformers.modeling_tf_xlnet.TFXLNetLMHeadModel.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_xlnet.TFXLNetLMHeadModel.call(self,inputs,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,token_type_ids=None,input_mask=None,head_mask=None,inputs_embeds=None,use_cache=True,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.modeling_tf_xlnet.TFXLNetLMHeadModel.get_output_embeddings(self)
transformers.modeling_tf_xlnet.TFXLNetLMHeadModel.prepare_inputs_for_generation(self,inputs,past,**kwargs)
transformers.modeling_tf_xlnet.TFXLNetLMHeadModelOutput(ModelOutput)
transformers.modeling_tf_xlnet.TFXLNetLayer(self,config,**kwargs)
transformers.modeling_tf_xlnet.TFXLNetLayer.__init__(self,config,**kwargs)
transformers.modeling_tf_xlnet.TFXLNetLayer.call(self,output_h,output_g,non_tgt_mask,attn_mask,pos_emb,seg_mat,mems,target_mapping,head_mask,output_attentions,training=False)
transformers.modeling_tf_xlnet.TFXLNetMainLayer(self,config,**kwargs)
transformers.modeling_tf_xlnet.TFXLNetMainLayer.__init__(self,config,**kwargs)
transformers.modeling_tf_xlnet.TFXLNetMainLayer._prune_heads(self,heads_to_prune)
transformers.modeling_tf_xlnet.TFXLNetMainLayer._resize_token_embeddings(self,new_num_tokens)
transformers.modeling_tf_xlnet.TFXLNetMainLayer.build(self,input_shape)
transformers.modeling_tf_xlnet.TFXLNetMainLayer.cache_mem(self,curr_out,prev_mem)
transformers.modeling_tf_xlnet.TFXLNetMainLayer.call(self,inputs,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,token_type_ids=None,input_mask=None,head_mask=None,inputs_embeds=None,use_cache=True,output_attentions=None,output_hidden_states=None,return_dict=None,training=False)
transformers.modeling_tf_xlnet.TFXLNetMainLayer.create_mask(self,qlen,mlen,dtype=tf.float32)
transformers.modeling_tf_xlnet.TFXLNetMainLayer.get_input_embeddings(self)
transformers.modeling_tf_xlnet.TFXLNetMainLayer.positional_embedding(pos_seq,inv_freq,bsz=None)
transformers.modeling_tf_xlnet.TFXLNetMainLayer.relative_positional_encoding(self,qlen,klen,bsz=None,dtype=None)
transformers.modeling_tf_xlnet.TFXLNetMainLayer.set_input_embeddings(self,value)
transformers.modeling_tf_xlnet.TFXLNetModel(self,config,*inputs,**kwargs)
transformers.modeling_tf_xlnet.TFXLNetModel.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_xlnet.TFXLNetModel.call(self,inputs,**kwargs)
transformers.modeling_tf_xlnet.TFXLNetModelOutput(ModelOutput)
transformers.modeling_tf_xlnet.TFXLNetPreTrainedModel(TFPreTrainedModel)
transformers.modeling_tf_xlnet.TFXLNetRelativeAttention(self,config,**kwargs)
transformers.modeling_tf_xlnet.TFXLNetRelativeAttention.__init__(self,config,**kwargs)
transformers.modeling_tf_xlnet.TFXLNetRelativeAttention.build(self,input_shape)
transformers.modeling_tf_xlnet.TFXLNetRelativeAttention.call(self,h,g,attn_mask_h,attn_mask_g,r,seg_mat,mems,target_mapping,head_mask,output_attentions,training=False)
transformers.modeling_tf_xlnet.TFXLNetRelativeAttention.post_attention(self,h,attn_vec,residual=True,training=False)
transformers.modeling_tf_xlnet.TFXLNetRelativeAttention.prune_heads(self,heads)
transformers.modeling_tf_xlnet.TFXLNetRelativeAttention.rel_attn_core(self,q_head,k_head_h,v_head_h,k_head_r,seg_mat,attn_mask,head_mask,output_attentions,training=False)
transformers.modeling_tf_xlnet.TFXLNetRelativeAttention.rel_shift(self,x,klen=-1)
transformers.modeling_tf_xlnet.gelu(x)
transformers.modeling_tf_xlnet.swish(x)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/convert_gpt2_original_tf_checkpoint_to_pytorch.py----------------------------------------
A:transformers.convert_gpt2_original_tf_checkpoint_to_pytorch.config->transformers.GPT2Config.from_json_file(gpt2_config_file)
A:transformers.convert_gpt2_original_tf_checkpoint_to_pytorch.model->GPT2Model(config)
A:transformers.convert_gpt2_original_tf_checkpoint_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.convert_gpt2_original_tf_checkpoint_to_pytorch.args->argparse.ArgumentParser().parse_args()
transformers.convert_gpt2_original_tf_checkpoint_to_pytorch.convert_gpt2_checkpoint_to_pytorch(gpt2_checkpoint_path,gpt2_config_file,pytorch_dump_folder_path)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/tokenization_utils_base.py----------------------------------------
A:transformers.tokenization_utils_base.logger->utils.logging.get_logger(__name__)
A:transformers.tokenization_utils_base.VERY_LARGE_INTEGER->int(1e+30)
A:transformers.tokenization_utils_base.LARGE_INTEGER->int(1e+20)
A:transformers.tokenization_utils_base.tensor_type->TensorType(tensor_type)
A:transformers.tokenization_utils_base.tensor->as_tensor(value)
A:transformers.tokenization_utils_base.attr_value->getattr(self, '_' + attr)
A:transformers.tokenization_utils_base.set_attr[attr]->str(attr_value)
A:transformers.tokenization_utils_base.all_toks->list(OrderedDict.fromkeys(all_toks))
A:transformers.tokenization_utils_base.all_ids->self.convert_tokens_to_ids(all_toks)
A:transformers.tokenization_utils_base.model_max_length->kwargs.pop('model_max_length', kwargs.pop('max_len', None))
A:transformers.tokenization_utils_base.self.padding_side->kwargs.pop('padding_side', self.padding_side)
A:transformers.tokenization_utils_base.self.model_input_names->kwargs.pop('model_input_names', self.model_input_names)
A:transformers.tokenization_utils_base.cache_dir->kwargs.pop('cache_dir', None)
A:transformers.tokenization_utils_base.force_download->kwargs.pop('force_download', False)
A:transformers.tokenization_utils_base.resume_download->kwargs.pop('resume_download', False)
A:transformers.tokenization_utils_base.proxies->kwargs.pop('proxies', None)
A:transformers.tokenization_utils_base.local_files_only->kwargs.pop('local_files_only', False)
A:transformers.tokenization_utils_base.s3_models->list(cls.max_model_input_sizes.keys())
A:transformers.tokenization_utils_base.init_configuration->cls.pretrained_init_configuration[pretrained_model_name_or_path].copy()
A:transformers.tokenization_utils_base.full_file_name->hf_bucket_url(pretrained_model_name_or_path, filename=file_name, use_cdn=False)
A:transformers.tokenization_utils_base.resolved_vocab_files[file_id]->cached_path(file_path, cache_dir=cache_dir, force_download=force_download, proxies=proxies, resume_download=resume_download, local_files_only=local_files_only)
A:transformers.tokenization_utils_base.msg->"Model name '{}' was not found in tokenizers model name list ({}). We assumed '{}' was a path or url to a directory containing vocabulary files named {}, but couldn't find such vocabulary files at this path or url.".format(pretrained_model_name_or_path, ', '.join(s3_models), pretrained_model_name_or_path, list(cls.vocab_files_names.values()))
A:transformers.tokenization_utils_base.tokenizer_config_file->os.path.join(save_directory, TOKENIZER_CONFIG_FILE)
A:transformers.tokenization_utils_base.init_kwargs->json.load(tokenizer_config_handle)
A:transformers.tokenization_utils_base.saved_init_inputs->json.load(tokenizer_config_handle).pop('init_inputs', ())
A:transformers.tokenization_utils_base.init_kwargs['model_max_length']->min(init_kwargs.get('model_max_length', int(1e+30)), model_max_length)
A:transformers.tokenization_utils_base.added_tokens_file->os.path.join(save_directory, ADDED_TOKENS_FILE)
A:transformers.tokenization_utils_base.tokenizer->cls(*init_inputs, **init_kwargs)
A:transformers.tokenization_utils_base.special_tokens_map_file->os.path.join(save_directory, SPECIAL_TOKENS_MAP_FILE)
A:transformers.tokenization_utils_base.special_tokens_map->json.load(special_tokens_map_handle)
A:transformers.tokenization_utils_base.value->AddedToken(**value)
A:transformers.tokenization_utils_base.added_tok_encoder->json.load(added_tokens_handle)
A:transformers.tokenization_utils_base.added_tok_encoder_sorted->list(sorted(added_tok_encoder.items(), key=lambda x: x[1]))
A:transformers.tokenization_utils_base.added_tokens->cls(*init_inputs, **init_kwargs).sanitize_special_tokens()
A:transformers.tokenization_utils_base.tokenizer_config->copy.deepcopy(self.init_kwargs)
A:transformers.tokenization_utils_base.tokenizer_config['init_inputs']->copy.deepcopy(self.init_inputs)
A:transformers.tokenization_utils_base.write_dict[key]->AddedToken(**value).__getstate__()
A:transformers.tokenization_utils_base.added_vocab->self.get_added_vocab()
A:transformers.tokenization_utils_base.out_str->json.dumps(added_vocab, ensure_ascii=False)
A:transformers.tokenization_utils_base.vocab_files->self.save_vocabulary(save_directory)
A:transformers.tokenization_utils_base.encoded_inputs->self.pad(encoded_inputs, max_length=max_length, padding=padding_strategy.value, pad_to_multiple_of=pad_to_multiple_of, return_attention_mask=return_attention_mask)
A:transformers.tokenization_utils_base.old_truncation_strategy->kwargs.pop('truncation_strategy', 'do_not_truncate')
A:transformers.tokenization_utils_base.old_pad_to_max_length->kwargs.pop('pad_to_max_length', False)
A:transformers.tokenization_utils_base.padding_strategy->PaddingStrategy(padding)
A:transformers.tokenization_utils_base.truncation_strategy->TruncationStrategy(truncation_strategy)
A:transformers.tokenization_utils_base.is_batched->bool(not is_pretokenized and isinstance(text, (list, tuple)) or (is_pretokenized and isinstance(text, (list, tuple)) and text and isinstance(text[0], (list, tuple))))
A:transformers.tokenization_utils_base.(padding_strategy, truncation_strategy, max_length, kwargs)->self._get_padding_truncation_strategies(padding=padding, truncation=truncation, max_length=max_length, pad_to_multiple_of=pad_to_multiple_of, verbose=verbose, **kwargs)
A:transformers.tokenization_utils_base.encoded_inputs[key]->to_py_obj(value)
A:transformers.tokenization_utils_base.(padding_strategy, _, max_length, _)->self._get_padding_truncation_strategies(padding=padding, max_length=max_length, verbose=verbose)
A:transformers.tokenization_utils_base.batch_size->len(encoded_inputs['input_ids'])
A:transformers.tokenization_utils_base.max_length->len(encoded_inputs['input_ids'])
A:transformers.tokenization_utils_base.inputs->dict(((k, v[i]) for (k, v) in encoded_inputs.items()))
A:transformers.tokenization_utils_base.outputs->self._pad(inputs, max_length=max_length, padding_strategy=padding_strategy, pad_to_multiple_of=pad_to_multiple_of, return_attention_mask=return_attention_mask)
A:transformers.tokenization_utils_base.pair->bool(pair_ids is not None)
A:transformers.tokenization_utils_base.len_ids->len(ids)
A:transformers.tokenization_utils_base.(ids, pair_ids, overflowing_tokens)->self.truncate_sequences(ids, pair_ids=pair_ids, num_tokens_to_remove=total_len - max_length, truncation_strategy=truncation_strategy, stride=stride)
A:transformers.tokenization_utils_base.sequence->self.build_inputs_with_special_tokens(ids, pair_ids)
A:transformers.tokenization_utils_base.token_type_ids->self.create_token_type_ids_from_sequences(ids, pair_ids)
A:transformers.tokenization_utils_base.encoded_inputs['special_tokens_mask']->self.get_special_tokens_mask(ids, pair_ids)
A:transformers.tokenization_utils_base.encoded_inputs['length']->len(encoded_inputs['input_ids'])
A:transformers.tokenization_utils_base.batch_outputs->BatchEncoding(encoded_inputs, tensor_type=return_tensors, prepend_batch_axis=prepend_batch_axis)
A:transformers.tokenization_utils_base.window_len->min(len(pair_ids), stride + num_tokens_to_remove)
A:transformers.tokenization_utils_base.out_string->out_string.replace(' .', '.').replace(' ?', '?').replace(' !', '!').replace(' ,', ',').replace(" ' ", "'").replace(" n't", "n't").replace(" 'm", "'m").replace(" 's", "'s").replace(" 've", "'ve").replace(" 're", "'re").replace(' .', '.').replace(' ?', '?').replace(' !', '!').replace(' ,', ',').replace(" ' ", "'").replace(" n't", "n't").replace(" 'm", "'m").replace(" 's", "'s").replace(" 've", "'ve").replace(" 're", "'re")
transformers.BatchEncoding(self,data:Optional[Dict[str,Any]]=None,encoding:Optional[Union[EncodingFast,Sequence[EncodingFast]]]=None,tensor_type:Union[None,str,TensorType]=None,prepend_batch_axis:bool=False)
transformers.BatchEncoding.__getattr__(self,item:str)
transformers.BatchEncoding.__getitem__(self,item:Union[int,str])->Union[Any, EncodingFast]
transformers.BatchEncoding.__getstate__(self)
transformers.BatchEncoding.__setstate__(self,state)
transformers.BatchEncoding.char_to_token(self,batch_or_char_index:int,char_index:Optional[int]=None)->int
transformers.BatchEncoding.char_to_word(self,batch_or_char_index:int,char_index:Optional[int]=None)->int
transformers.BatchEncoding.convert_to_tensors(self,tensor_type:Optional[Union[str,TensorType]]=None,prepend_batch_axis:bool=False)
transformers.BatchEncoding.encodings(self)->Optional[List[EncodingFast]]
transformers.BatchEncoding.is_fast(self)->bool
transformers.BatchEncoding.items(self)
transformers.BatchEncoding.keys(self)
transformers.BatchEncoding.to(self,device:str)->'BatchEncoding'
transformers.BatchEncoding.token_to_chars(self,batch_or_token_index:int,token_index:Optional[int]=None)->CharSpan
transformers.BatchEncoding.token_to_word(self,batch_or_token_index:int,token_index:Optional[int]=None)->int
transformers.BatchEncoding.tokens(self,batch_index:int=0)->List[str]
transformers.BatchEncoding.values(self)
transformers.BatchEncoding.word_to_chars(self,batch_or_word_index:int,word_index:Optional[int]=None)->CharSpan
transformers.BatchEncoding.word_to_tokens(self,batch_or_word_index:int,word_index:Optional[int]=None)->TokenSpan
transformers.BatchEncoding.words(self,batch_index:int=0)->List[Optional[int]]
transformers.CharSpan(NamedTuple)
transformers.PreTrainedTokenizerBase(self,**kwargs)
transformers.PreTrainedTokenizerBase._batch_encode_plus(self,batch_text_or_text_pairs:Union[List[TextInput],List[TextInputPair],List[PreTokenizedInput],List[PreTokenizedInputPair],List[EncodedInput],List[EncodedInputPair]],add_special_tokens:bool=True,padding_strategy:PaddingStrategy=PaddingStrategy.DO_NOT_PAD,truncation_strategy:TruncationStrategy=TruncationStrategy.DO_NOT_TRUNCATE,max_length:Optional[int]=None,stride:int=0,is_pretokenized:bool=False,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[Union[str,TensorType]]=None,return_token_type_ids:Optional[bool]=None,return_attention_mask:Optional[bool]=None,return_overflowing_tokens:bool=False,return_special_tokens_mask:bool=False,return_offsets_mapping:bool=False,return_length:bool=False,verbose:bool=True,**kwargs)->BatchEncoding
transformers.PreTrainedTokenizerBase._encode_plus(self,text:Union[TextInput,PreTokenizedInput,EncodedInput],text_pair:Optional[Union[TextInput,PreTokenizedInput,EncodedInput]]=None,add_special_tokens:bool=True,padding_strategy:PaddingStrategy=PaddingStrategy.DO_NOT_PAD,truncation_strategy:TruncationStrategy=TruncationStrategy.DO_NOT_TRUNCATE,max_length:Optional[int]=None,stride:int=0,is_pretokenized:bool=False,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[Union[str,TensorType]]=None,return_token_type_ids:Optional[bool]=None,return_attention_mask:Optional[bool]=None,return_overflowing_tokens:bool=False,return_special_tokens_mask:bool=False,return_offsets_mapping:bool=False,return_length:bool=False,verbose:bool=True,**kwargs)->BatchEncoding
transformers.PreTrainedTokenizerBase._from_pretrained(cls,pretrained_model_name_or_path,*init_inputs,**kwargs)
transformers.PreTrainedTokenizerBase._get_padding_truncation_strategies(self,padding=False,truncation=False,max_length=None,pad_to_multiple_of=None,verbose=True,**kwargs)
transformers.PreTrainedTokenizerBase._pad(self,encoded_inputs:Union[Dict[str,EncodedInput],BatchEncoding],max_length:Optional[int]=None,padding_strategy:PaddingStrategy=PaddingStrategy.DO_NOT_PAD,pad_to_multiple_of:Optional[int]=None,return_attention_mask:Optional[bool]=None)->dict
transformers.PreTrainedTokenizerBase.batch_decode(self,sequences:List[List[int]],skip_special_tokens:bool=False,clean_up_tokenization_spaces:bool=True)->List[str]
transformers.PreTrainedTokenizerBase.batch_encode_plus(self,batch_text_or_text_pairs:Union[List[TextInput],List[TextInputPair],List[PreTokenizedInput],List[PreTokenizedInputPair],List[EncodedInput],List[EncodedInputPair]],add_special_tokens:bool=True,padding:Union[bool,str,PaddingStrategy]=False,truncation:Union[bool,str,TruncationStrategy]=False,max_length:Optional[int]=None,stride:int=0,is_pretokenized:bool=False,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[Union[str,TensorType]]=None,return_token_type_ids:Optional[bool]=None,return_attention_mask:Optional[bool]=None,return_overflowing_tokens:bool=False,return_special_tokens_mask:bool=False,return_offsets_mapping:bool=False,return_length:bool=False,verbose:bool=True,**kwargs)->BatchEncoding
transformers.PreTrainedTokenizerBase.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.PreTrainedTokenizerBase.clean_up_tokenization(out_string:str)->str
transformers.PreTrainedTokenizerBase.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.PreTrainedTokenizerBase.decode(self,token_ids:List[int],skip_special_tokens:bool=False,clean_up_tokenization_spaces:bool=True)->str
transformers.PreTrainedTokenizerBase.encode(self,text:Union[TextInput,PreTokenizedInput,EncodedInput],text_pair:Optional[Union[TextInput,PreTokenizedInput,EncodedInput]]=None,add_special_tokens:bool=True,padding:Union[bool,str,PaddingStrategy]=False,truncation:Union[bool,str,TruncationStrategy]=False,max_length:Optional[int]=None,stride:int=0,return_tensors:Optional[Union[str,TensorType]]=None,**kwargs)->List[int]
transformers.PreTrainedTokenizerBase.encode_plus(self,text:Union[TextInput,PreTokenizedInput,EncodedInput],text_pair:Optional[Union[TextInput,PreTokenizedInput,EncodedInput]]=None,add_special_tokens:bool=True,padding:Union[bool,str,PaddingStrategy]=False,truncation:Union[bool,str,TruncationStrategy]=False,max_length:Optional[int]=None,stride:int=0,is_pretokenized:bool=False,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[Union[str,TensorType]]=None,return_token_type_ids:Optional[bool]=None,return_attention_mask:Optional[bool]=None,return_overflowing_tokens:bool=False,return_special_tokens_mask:bool=False,return_offsets_mapping:bool=False,return_length:bool=False,verbose:bool=True,**kwargs)->BatchEncoding
transformers.PreTrainedTokenizerBase.from_pretrained(cls,*inputs,**kwargs)
transformers.PreTrainedTokenizerBase.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.PreTrainedTokenizerBase.max_len(self)->int
transformers.PreTrainedTokenizerBase.max_len_sentences_pair(self)->int
transformers.PreTrainedTokenizerBase.max_len_sentences_pair(self,value)->int
transformers.PreTrainedTokenizerBase.max_len_single_sentence(self)->int
transformers.PreTrainedTokenizerBase.max_len_single_sentence(self,value)->int
transformers.PreTrainedTokenizerBase.num_special_tokens_to_add(self,pair:bool=False)->int
transformers.PreTrainedTokenizerBase.pad(self,encoded_inputs:Union[BatchEncoding,List[BatchEncoding],Dict[str,EncodedInput],Dict[str,List[EncodedInput]],List[Dict[str,EncodedInput]]],padding:Union[bool,str,PaddingStrategy]=True,max_length:Optional[int]=None,pad_to_multiple_of:Optional[int]=None,return_attention_mask:Optional[bool]=None,return_tensors:Optional[Union[str,TensorType]]=None,verbose:bool=True)->BatchEncoding
transformers.PreTrainedTokenizerBase.prepare_for_model(self,ids:List[int],pair_ids:Optional[List[int]]=None,add_special_tokens:bool=True,padding:Union[bool,str,PaddingStrategy]=False,truncation:Union[bool,str,TruncationStrategy]=False,max_length:Optional[int]=None,stride:int=0,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[Union[str,TensorType]]=None,return_token_type_ids:Optional[bool]=None,return_attention_mask:Optional[bool]=None,return_overflowing_tokens:bool=False,return_special_tokens_mask:bool=False,return_offsets_mapping:bool=False,return_length:bool=False,verbose:bool=True,prepend_batch_axis:bool=False,**kwargs)->BatchEncoding
transformers.PreTrainedTokenizerBase.save_pretrained(self,save_directory:str)->Tuple[str]
transformers.PreTrainedTokenizerBase.truncate_sequences(self,ids:List[int],pair_ids:Optional[List[int]]=None,num_tokens_to_remove:int=0,truncation_strategy:Union[str,TruncationStrategy]='longest_first',stride:int=0)->Tuple[List[int], List[int], List[int]]
transformers.SpecialTokensMixin(self,verbose=True,**kwargs)
transformers.SpecialTokensMixin.add_special_tokens(self,special_tokens_dict:Dict[str,Union[str,AddedToken]])->int
transformers.SpecialTokensMixin.add_tokens(self,new_tokens:Union[str,AddedToken,List[Union[str,AddedToken]]],special_tokens:bool=False)->int
transformers.SpecialTokensMixin.additional_special_tokens(self)->List[str]
transformers.SpecialTokensMixin.additional_special_tokens(self,value)
transformers.SpecialTokensMixin.additional_special_tokens_ids(self)->List[int]
transformers.SpecialTokensMixin.all_special_ids(self)->List[int]
transformers.SpecialTokensMixin.all_special_tokens(self)->List[str]
transformers.SpecialTokensMixin.all_special_tokens_extended(self)->List[Union[str, AddedToken]]
transformers.SpecialTokensMixin.bos_token(self)->str
transformers.SpecialTokensMixin.bos_token(self,value)
transformers.SpecialTokensMixin.bos_token_id(self)->Optional[int]
transformers.SpecialTokensMixin.cls_token(self)->str
transformers.SpecialTokensMixin.cls_token(self,value)
transformers.SpecialTokensMixin.cls_token_id(self)->Optional[int]
transformers.SpecialTokensMixin.eos_token(self)->str
transformers.SpecialTokensMixin.eos_token(self,value)
transformers.SpecialTokensMixin.eos_token_id(self)->Optional[int]
transformers.SpecialTokensMixin.mask_token(self)->str
transformers.SpecialTokensMixin.mask_token(self,value)
transformers.SpecialTokensMixin.mask_token_id(self)->Optional[int]
transformers.SpecialTokensMixin.pad_token(self)->str
transformers.SpecialTokensMixin.pad_token(self,value)
transformers.SpecialTokensMixin.pad_token_id(self)->Optional[int]
transformers.SpecialTokensMixin.pad_token_type_id(self)->int
transformers.SpecialTokensMixin.sanitize_special_tokens(self)->int
transformers.SpecialTokensMixin.sep_token(self)->str
transformers.SpecialTokensMixin.sep_token(self,value)
transformers.SpecialTokensMixin.sep_token_id(self)->Optional[int]
transformers.SpecialTokensMixin.special_tokens_map(self)->Dict[str, Union[str, List[str]]]
transformers.SpecialTokensMixin.special_tokens_map_extended(self)->Dict[str, Union[str, AddedToken, List[Union[str, AddedToken]]]]
transformers.SpecialTokensMixin.unk_token(self)->str
transformers.SpecialTokensMixin.unk_token(self,value)
transformers.SpecialTokensMixin.unk_token_id(self)->Optional[int]
transformers.TensorType(ExplicitEnum)
transformers.TokenSpan(NamedTuple)
transformers.tokenization_utils_base.BatchEncoding(self,data:Optional[Dict[str,Any]]=None,encoding:Optional[Union[EncodingFast,Sequence[EncodingFast]]]=None,tensor_type:Union[None,str,TensorType]=None,prepend_batch_axis:bool=False)
transformers.tokenization_utils_base.BatchEncoding.__getattr__(self,item:str)
transformers.tokenization_utils_base.BatchEncoding.__getitem__(self,item:Union[int,str])->Union[Any, EncodingFast]
transformers.tokenization_utils_base.BatchEncoding.__getstate__(self)
transformers.tokenization_utils_base.BatchEncoding.__init__(self,data:Optional[Dict[str,Any]]=None,encoding:Optional[Union[EncodingFast,Sequence[EncodingFast]]]=None,tensor_type:Union[None,str,TensorType]=None,prepend_batch_axis:bool=False)
transformers.tokenization_utils_base.BatchEncoding.__setstate__(self,state)
transformers.tokenization_utils_base.BatchEncoding.char_to_token(self,batch_or_char_index:int,char_index:Optional[int]=None)->int
transformers.tokenization_utils_base.BatchEncoding.char_to_word(self,batch_or_char_index:int,char_index:Optional[int]=None)->int
transformers.tokenization_utils_base.BatchEncoding.convert_to_tensors(self,tensor_type:Optional[Union[str,TensorType]]=None,prepend_batch_axis:bool=False)
transformers.tokenization_utils_base.BatchEncoding.encodings(self)->Optional[List[EncodingFast]]
transformers.tokenization_utils_base.BatchEncoding.is_fast(self)->bool
transformers.tokenization_utils_base.BatchEncoding.items(self)
transformers.tokenization_utils_base.BatchEncoding.keys(self)
transformers.tokenization_utils_base.BatchEncoding.to(self,device:str)->'BatchEncoding'
transformers.tokenization_utils_base.BatchEncoding.token_to_chars(self,batch_or_token_index:int,token_index:Optional[int]=None)->CharSpan
transformers.tokenization_utils_base.BatchEncoding.token_to_word(self,batch_or_token_index:int,token_index:Optional[int]=None)->int
transformers.tokenization_utils_base.BatchEncoding.tokens(self,batch_index:int=0)->List[str]
transformers.tokenization_utils_base.BatchEncoding.values(self)
transformers.tokenization_utils_base.BatchEncoding.word_to_chars(self,batch_or_word_index:int,word_index:Optional[int]=None)->CharSpan
transformers.tokenization_utils_base.BatchEncoding.word_to_tokens(self,batch_or_word_index:int,word_index:Optional[int]=None)->TokenSpan
transformers.tokenization_utils_base.BatchEncoding.words(self,batch_index:int=0)->List[Optional[int]]
transformers.tokenization_utils_base.CharSpan(NamedTuple)
transformers.tokenization_utils_base.ExplicitEnum(Enum)
transformers.tokenization_utils_base.ExplicitEnum._missing_(cls,value)
transformers.tokenization_utils_base.PaddingStrategy(ExplicitEnum)
transformers.tokenization_utils_base.PreTrainedTokenizerBase(self,**kwargs)
transformers.tokenization_utils_base.PreTrainedTokenizerBase.__init__(self,**kwargs)
transformers.tokenization_utils_base.PreTrainedTokenizerBase._batch_encode_plus(self,batch_text_or_text_pairs:Union[List[TextInput],List[TextInputPair],List[PreTokenizedInput],List[PreTokenizedInputPair],List[EncodedInput],List[EncodedInputPair]],add_special_tokens:bool=True,padding_strategy:PaddingStrategy=PaddingStrategy.DO_NOT_PAD,truncation_strategy:TruncationStrategy=TruncationStrategy.DO_NOT_TRUNCATE,max_length:Optional[int]=None,stride:int=0,is_pretokenized:bool=False,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[Union[str,TensorType]]=None,return_token_type_ids:Optional[bool]=None,return_attention_mask:Optional[bool]=None,return_overflowing_tokens:bool=False,return_special_tokens_mask:bool=False,return_offsets_mapping:bool=False,return_length:bool=False,verbose:bool=True,**kwargs)->BatchEncoding
transformers.tokenization_utils_base.PreTrainedTokenizerBase._encode_plus(self,text:Union[TextInput,PreTokenizedInput,EncodedInput],text_pair:Optional[Union[TextInput,PreTokenizedInput,EncodedInput]]=None,add_special_tokens:bool=True,padding_strategy:PaddingStrategy=PaddingStrategy.DO_NOT_PAD,truncation_strategy:TruncationStrategy=TruncationStrategy.DO_NOT_TRUNCATE,max_length:Optional[int]=None,stride:int=0,is_pretokenized:bool=False,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[Union[str,TensorType]]=None,return_token_type_ids:Optional[bool]=None,return_attention_mask:Optional[bool]=None,return_overflowing_tokens:bool=False,return_special_tokens_mask:bool=False,return_offsets_mapping:bool=False,return_length:bool=False,verbose:bool=True,**kwargs)->BatchEncoding
transformers.tokenization_utils_base.PreTrainedTokenizerBase._from_pretrained(cls,pretrained_model_name_or_path,*init_inputs,**kwargs)
transformers.tokenization_utils_base.PreTrainedTokenizerBase._get_padding_truncation_strategies(self,padding=False,truncation=False,max_length=None,pad_to_multiple_of=None,verbose=True,**kwargs)
transformers.tokenization_utils_base.PreTrainedTokenizerBase._pad(self,encoded_inputs:Union[Dict[str,EncodedInput],BatchEncoding],max_length:Optional[int]=None,padding_strategy:PaddingStrategy=PaddingStrategy.DO_NOT_PAD,pad_to_multiple_of:Optional[int]=None,return_attention_mask:Optional[bool]=None)->dict
transformers.tokenization_utils_base.PreTrainedTokenizerBase.batch_decode(self,sequences:List[List[int]],skip_special_tokens:bool=False,clean_up_tokenization_spaces:bool=True)->List[str]
transformers.tokenization_utils_base.PreTrainedTokenizerBase.batch_encode_plus(self,batch_text_or_text_pairs:Union[List[TextInput],List[TextInputPair],List[PreTokenizedInput],List[PreTokenizedInputPair],List[EncodedInput],List[EncodedInputPair]],add_special_tokens:bool=True,padding:Union[bool,str,PaddingStrategy]=False,truncation:Union[bool,str,TruncationStrategy]=False,max_length:Optional[int]=None,stride:int=0,is_pretokenized:bool=False,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[Union[str,TensorType]]=None,return_token_type_ids:Optional[bool]=None,return_attention_mask:Optional[bool]=None,return_overflowing_tokens:bool=False,return_special_tokens_mask:bool=False,return_offsets_mapping:bool=False,return_length:bool=False,verbose:bool=True,**kwargs)->BatchEncoding
transformers.tokenization_utils_base.PreTrainedTokenizerBase.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.tokenization_utils_base.PreTrainedTokenizerBase.clean_up_tokenization(out_string:str)->str
transformers.tokenization_utils_base.PreTrainedTokenizerBase.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.tokenization_utils_base.PreTrainedTokenizerBase.decode(self,token_ids:List[int],skip_special_tokens:bool=False,clean_up_tokenization_spaces:bool=True)->str
transformers.tokenization_utils_base.PreTrainedTokenizerBase.encode(self,text:Union[TextInput,PreTokenizedInput,EncodedInput],text_pair:Optional[Union[TextInput,PreTokenizedInput,EncodedInput]]=None,add_special_tokens:bool=True,padding:Union[bool,str,PaddingStrategy]=False,truncation:Union[bool,str,TruncationStrategy]=False,max_length:Optional[int]=None,stride:int=0,return_tensors:Optional[Union[str,TensorType]]=None,**kwargs)->List[int]
transformers.tokenization_utils_base.PreTrainedTokenizerBase.encode_plus(self,text:Union[TextInput,PreTokenizedInput,EncodedInput],text_pair:Optional[Union[TextInput,PreTokenizedInput,EncodedInput]]=None,add_special_tokens:bool=True,padding:Union[bool,str,PaddingStrategy]=False,truncation:Union[bool,str,TruncationStrategy]=False,max_length:Optional[int]=None,stride:int=0,is_pretokenized:bool=False,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[Union[str,TensorType]]=None,return_token_type_ids:Optional[bool]=None,return_attention_mask:Optional[bool]=None,return_overflowing_tokens:bool=False,return_special_tokens_mask:bool=False,return_offsets_mapping:bool=False,return_length:bool=False,verbose:bool=True,**kwargs)->BatchEncoding
transformers.tokenization_utils_base.PreTrainedTokenizerBase.from_pretrained(cls,*inputs,**kwargs)
transformers.tokenization_utils_base.PreTrainedTokenizerBase.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.tokenization_utils_base.PreTrainedTokenizerBase.max_len(self)->int
transformers.tokenization_utils_base.PreTrainedTokenizerBase.max_len_sentences_pair(self)->int
transformers.tokenization_utils_base.PreTrainedTokenizerBase.max_len_sentences_pair(self,value)->int
transformers.tokenization_utils_base.PreTrainedTokenizerBase.max_len_single_sentence(self)->int
transformers.tokenization_utils_base.PreTrainedTokenizerBase.max_len_single_sentence(self,value)->int
transformers.tokenization_utils_base.PreTrainedTokenizerBase.num_special_tokens_to_add(self,pair:bool=False)->int
transformers.tokenization_utils_base.PreTrainedTokenizerBase.pad(self,encoded_inputs:Union[BatchEncoding,List[BatchEncoding],Dict[str,EncodedInput],Dict[str,List[EncodedInput]],List[Dict[str,EncodedInput]]],padding:Union[bool,str,PaddingStrategy]=True,max_length:Optional[int]=None,pad_to_multiple_of:Optional[int]=None,return_attention_mask:Optional[bool]=None,return_tensors:Optional[Union[str,TensorType]]=None,verbose:bool=True)->BatchEncoding
transformers.tokenization_utils_base.PreTrainedTokenizerBase.prepare_for_model(self,ids:List[int],pair_ids:Optional[List[int]]=None,add_special_tokens:bool=True,padding:Union[bool,str,PaddingStrategy]=False,truncation:Union[bool,str,TruncationStrategy]=False,max_length:Optional[int]=None,stride:int=0,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[Union[str,TensorType]]=None,return_token_type_ids:Optional[bool]=None,return_attention_mask:Optional[bool]=None,return_overflowing_tokens:bool=False,return_special_tokens_mask:bool=False,return_offsets_mapping:bool=False,return_length:bool=False,verbose:bool=True,prepend_batch_axis:bool=False,**kwargs)->BatchEncoding
transformers.tokenization_utils_base.PreTrainedTokenizerBase.save_pretrained(self,save_directory:str)->Tuple[str]
transformers.tokenization_utils_base.PreTrainedTokenizerBase.truncate_sequences(self,ids:List[int],pair_ids:Optional[List[int]]=None,num_tokens_to_remove:int=0,truncation_strategy:Union[str,TruncationStrategy]='longest_first',stride:int=0)->Tuple[List[int], List[int], List[int]]
transformers.tokenization_utils_base.SpecialTokensMixin(self,verbose=True,**kwargs)
transformers.tokenization_utils_base.SpecialTokensMixin.__init__(self,verbose=True,**kwargs)
transformers.tokenization_utils_base.SpecialTokensMixin.add_special_tokens(self,special_tokens_dict:Dict[str,Union[str,AddedToken]])->int
transformers.tokenization_utils_base.SpecialTokensMixin.add_tokens(self,new_tokens:Union[str,AddedToken,List[Union[str,AddedToken]]],special_tokens:bool=False)->int
transformers.tokenization_utils_base.SpecialTokensMixin.additional_special_tokens(self)->List[str]
transformers.tokenization_utils_base.SpecialTokensMixin.additional_special_tokens(self,value)
transformers.tokenization_utils_base.SpecialTokensMixin.additional_special_tokens_ids(self)->List[int]
transformers.tokenization_utils_base.SpecialTokensMixin.all_special_ids(self)->List[int]
transformers.tokenization_utils_base.SpecialTokensMixin.all_special_tokens(self)->List[str]
transformers.tokenization_utils_base.SpecialTokensMixin.all_special_tokens_extended(self)->List[Union[str, AddedToken]]
transformers.tokenization_utils_base.SpecialTokensMixin.bos_token(self)->str
transformers.tokenization_utils_base.SpecialTokensMixin.bos_token(self,value)
transformers.tokenization_utils_base.SpecialTokensMixin.bos_token_id(self)->Optional[int]
transformers.tokenization_utils_base.SpecialTokensMixin.cls_token(self)->str
transformers.tokenization_utils_base.SpecialTokensMixin.cls_token(self,value)
transformers.tokenization_utils_base.SpecialTokensMixin.cls_token_id(self)->Optional[int]
transformers.tokenization_utils_base.SpecialTokensMixin.eos_token(self)->str
transformers.tokenization_utils_base.SpecialTokensMixin.eos_token(self,value)
transformers.tokenization_utils_base.SpecialTokensMixin.eos_token_id(self)->Optional[int]
transformers.tokenization_utils_base.SpecialTokensMixin.mask_token(self)->str
transformers.tokenization_utils_base.SpecialTokensMixin.mask_token(self,value)
transformers.tokenization_utils_base.SpecialTokensMixin.mask_token_id(self)->Optional[int]
transformers.tokenization_utils_base.SpecialTokensMixin.pad_token(self)->str
transformers.tokenization_utils_base.SpecialTokensMixin.pad_token(self,value)
transformers.tokenization_utils_base.SpecialTokensMixin.pad_token_id(self)->Optional[int]
transformers.tokenization_utils_base.SpecialTokensMixin.pad_token_type_id(self)->int
transformers.tokenization_utils_base.SpecialTokensMixin.sanitize_special_tokens(self)->int
transformers.tokenization_utils_base.SpecialTokensMixin.sep_token(self)->str
transformers.tokenization_utils_base.SpecialTokensMixin.sep_token(self,value)
transformers.tokenization_utils_base.SpecialTokensMixin.sep_token_id(self)->Optional[int]
transformers.tokenization_utils_base.SpecialTokensMixin.special_tokens_map(self)->Dict[str, Union[str, List[str]]]
transformers.tokenization_utils_base.SpecialTokensMixin.special_tokens_map_extended(self)->Dict[str, Union[str, AddedToken, List[Union[str, AddedToken]]]]
transformers.tokenization_utils_base.SpecialTokensMixin.unk_token(self)->str
transformers.tokenization_utils_base.SpecialTokensMixin.unk_token(self,value)
transformers.tokenization_utils_base.SpecialTokensMixin.unk_token_id(self)->Optional[int]
transformers.tokenization_utils_base.TensorType(ExplicitEnum)
transformers.tokenization_utils_base.TokenSpan(NamedTuple)
transformers.tokenization_utils_base.TruncationStrategy(ExplicitEnum)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/configuration_encoder_decoder.py----------------------------------------
A:transformers.configuration_encoder_decoder.logger->utils.logging.get_logger(__name__)
A:transformers.configuration_encoder_decoder.encoder_config->kwargs.pop('encoder')
A:transformers.configuration_encoder_decoder.encoder_model_type->kwargs.pop('encoder').pop('model_type')
A:transformers.configuration_encoder_decoder.decoder_config->kwargs.pop('decoder')
A:transformers.configuration_encoder_decoder.decoder_model_type->kwargs.pop('decoder').pop('model_type')
A:transformers.configuration_encoder_decoder.self.encoder->configuration_auto.AutoConfig.for_model(encoder_model_type, **encoder_config)
A:transformers.configuration_encoder_decoder.self.decoder->configuration_auto.AutoConfig.for_model(decoder_model_type, **decoder_config)
A:transformers.configuration_encoder_decoder.output->copy.deepcopy(self.__dict__)
A:transformers.configuration_encoder_decoder.output['encoder']->self.encoder.to_dict()
A:transformers.configuration_encoder_decoder.output['decoder']->self.decoder.to_dict()
transformers.EncoderDecoderConfig(self,**kwargs)
transformers.EncoderDecoderConfig.from_encoder_decoder_configs(cls,encoder_config:PretrainedConfig,decoder_config:PretrainedConfig,**kwargs)->PretrainedConfig
transformers.EncoderDecoderConfig.to_dict(self)
transformers.configuration_encoder_decoder.EncoderDecoderConfig(self,**kwargs)
transformers.configuration_encoder_decoder.EncoderDecoderConfig.__init__(self,**kwargs)
transformers.configuration_encoder_decoder.EncoderDecoderConfig.from_encoder_decoder_configs(cls,encoder_config:PretrainedConfig,decoder_config:PretrainedConfig,**kwargs)->PretrainedConfig
transformers.configuration_encoder_decoder.EncoderDecoderConfig.to_dict(self)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/optimization_tf.py----------------------------------------
A:transformers.optimization_tf.global_step_float->tensorflow.cast(step, tf.float32)
A:transformers.optimization_tf.warmup_steps_float->tensorflow.cast(self.warmup_steps, tf.float32)
A:transformers.optimization_tf.lr_schedule->WarmUp(initial_learning_rate=init_lr, decay_schedule_fn=lr_schedule, warmup_steps=num_warmup_steps)
A:transformers.optimization_tf.optimizer->tensorflow.keras.optimizers.Adam(learning_rate=lr_schedule, beta_1=adam_beta1, beta_2=adam_beta2, epsilon=adam_epsilon)
A:transformers.optimization_tf.apply_state[var_device, var_dtype]['weight_decay_rate']->tensorflow.constant(self.weight_decay_rate, name='adam_weight_decay_rate')
A:transformers.optimization_tf.do_decay->self._do_use_weight_decay(var.name)
A:transformers.optimization_tf.(grads, tvars)->list(zip(*grads_and_vars))
A:transformers.optimization_tf.coefficients->self._fallback_apply_state(var_device, var_dtype)
A:transformers.optimization_tf.(lr_t, kwargs)->self._get_lr(var.device, var.dtype.base_dtype, apply_state)
A:transformers.optimization_tf.decay->self._decay_weights_op(var, lr_t, apply_state)
A:transformers.optimization_tf.config->super().get_config()
A:transformers.optimization_tf.self._accum_steps->tensorflow.Variable(tf.constant(0, dtype=tf.int64), trainable=False, synchronization=tf.VariableSynchronization.ON_READ, aggregation=tf.VariableAggregation.ONLY_FIRST_REPLICA)
transformers.AdamWeightDecay(self,learning_rate:Union[float,tf.keras.optimizers.schedules.LearningRateSchedule]=0.001,beta_1:float=0.9,beta_2:float=0.999,epsilon:float=1e-07,amsgrad:bool=False,weight_decay_rate:float=0.0,include_in_weight_decay:Optional[List[str]]=None,exclude_from_weight_decay:Optional[List[str]]=None,name:str='AdamWeightDecay',**kwargs)
transformers.AdamWeightDecay._decay_weights_op(self,var,learning_rate,apply_state)
transformers.AdamWeightDecay._do_use_weight_decay(self,param_name)
transformers.AdamWeightDecay._get_lr(self,var_device,var_dtype,apply_state)
transformers.AdamWeightDecay._prepare_local(self,var_device,var_dtype,apply_state)
transformers.AdamWeightDecay._resource_apply_dense(self,grad,var,apply_state=None)
transformers.AdamWeightDecay._resource_apply_sparse(self,grad,var,indices,apply_state=None)
transformers.AdamWeightDecay.apply_gradients(self,grads_and_vars,name=None,**kwargs)
transformers.AdamWeightDecay.from_config(cls,config)
transformers.AdamWeightDecay.get_config(self)
transformers.GradientAccumulator(self)
transformers.GradientAccumulator.gradients(self)
transformers.GradientAccumulator.reset(self)
transformers.GradientAccumulator.step(self)
transformers.WarmUp(self,initial_learning_rate:float,decay_schedule_fn:Callable,warmup_steps:int,power:float=1.0,name:str=None)
transformers.WarmUp.get_config(self)
transformers.create_optimizer(init_lr:float,num_train_steps:int,num_warmup_steps:int,min_lr_ratio:float=0.0,adam_beta1:float=0.9,adam_beta2:float=0.999,adam_epsilon:float=1e-08,weight_decay_rate:float=0.0,include_in_weight_decay:Optional[List[str]]=None)
transformers.optimization_tf.AdamWeightDecay(self,learning_rate:Union[float,tf.keras.optimizers.schedules.LearningRateSchedule]=0.001,beta_1:float=0.9,beta_2:float=0.999,epsilon:float=1e-07,amsgrad:bool=False,weight_decay_rate:float=0.0,include_in_weight_decay:Optional[List[str]]=None,exclude_from_weight_decay:Optional[List[str]]=None,name:str='AdamWeightDecay',**kwargs)
transformers.optimization_tf.AdamWeightDecay.__init__(self,learning_rate:Union[float,tf.keras.optimizers.schedules.LearningRateSchedule]=0.001,beta_1:float=0.9,beta_2:float=0.999,epsilon:float=1e-07,amsgrad:bool=False,weight_decay_rate:float=0.0,include_in_weight_decay:Optional[List[str]]=None,exclude_from_weight_decay:Optional[List[str]]=None,name:str='AdamWeightDecay',**kwargs)
transformers.optimization_tf.AdamWeightDecay._decay_weights_op(self,var,learning_rate,apply_state)
transformers.optimization_tf.AdamWeightDecay._do_use_weight_decay(self,param_name)
transformers.optimization_tf.AdamWeightDecay._get_lr(self,var_device,var_dtype,apply_state)
transformers.optimization_tf.AdamWeightDecay._prepare_local(self,var_device,var_dtype,apply_state)
transformers.optimization_tf.AdamWeightDecay._resource_apply_dense(self,grad,var,apply_state=None)
transformers.optimization_tf.AdamWeightDecay._resource_apply_sparse(self,grad,var,indices,apply_state=None)
transformers.optimization_tf.AdamWeightDecay.apply_gradients(self,grads_and_vars,name=None,**kwargs)
transformers.optimization_tf.AdamWeightDecay.from_config(cls,config)
transformers.optimization_tf.AdamWeightDecay.get_config(self)
transformers.optimization_tf.GradientAccumulator(self)
transformers.optimization_tf.GradientAccumulator.__init__(self)
transformers.optimization_tf.GradientAccumulator.gradients(self)
transformers.optimization_tf.GradientAccumulator.reset(self)
transformers.optimization_tf.GradientAccumulator.step(self)
transformers.optimization_tf.WarmUp(self,initial_learning_rate:float,decay_schedule_fn:Callable,warmup_steps:int,power:float=1.0,name:str=None)
transformers.optimization_tf.WarmUp.__init__(self,initial_learning_rate:float,decay_schedule_fn:Callable,warmup_steps:int,power:float=1.0,name:str=None)
transformers.optimization_tf.WarmUp.get_config(self)
transformers.optimization_tf.create_optimizer(init_lr:float,num_train_steps:int,num_warmup_steps:int,min_lr_ratio:float=0.0,adam_beta1:float=0.9,adam_beta2:float=0.999,adam_epsilon:float=1e-08,weight_decay_rate:float=0.0,include_in_weight_decay:Optional[List[str]]=None)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/tokenization_utils.py----------------------------------------
A:transformers.tokenization_utils.logger->utils.logging.get_logger(__name__)
A:transformers.tokenization_utils.cat->unicodedata.category(char)
A:transformers.tokenization_utils.cp->ord(char)
A:transformers.tokenization_utils.token->token.lower().lower()
A:transformers.tokenization_utils.added_tok_encoder->dict(((tok, len(self) + i) for (i, tok) in enumerate(tokens_to_add)))
A:transformers.tokenization_utils.self.unique_no_split_tokens->sorted(set(self.unique_no_split_tokens).union(set(tokens_to_add)))
A:transformers.tokenization_utils.all_special_tokens_extended->dict(((str(t), t) for t in self.all_special_tokens_extended if isinstance(t, AddedToken)))
A:transformers.tokenization_utils.(text, kwargs)->self.prepare_for_tokenization(text, **kwargs)
A:transformers.tokenization_utils.text->' '.join(sub_texts)
A:transformers.tokenization_utils.tok_extended->dict(((str(t), t) for t in self.all_special_tokens_extended if isinstance(t, AddedToken))).get(tok, None)
A:transformers.tokenization_utils.split_text->' '.join(sub_texts).split(tok)
A:transformers.tokenization_utils.sub_text->sub_text.lstrip().lstrip()
A:transformers.tokenization_utils.tokenized_text->split_on_tokens(no_split_token, text)
A:transformers.tokenization_utils.tokens->list(itertools.chain(*(self.tokenize(t, is_pretokenized=True, **kwargs) for t in text)))
A:transformers.tokenization_utils.first_ids->get_input_ids(ids)
A:transformers.tokenization_utils.batch_outputs->BatchEncoding(batch_outputs, tensor_type=return_tensors)
A:transformers.tokenization_utils.outputs->self.prepare_for_model(first_ids, second_ids, add_special_tokens=add_special_tokens, padding=PaddingStrategy.DO_NOT_PAD.value, truncation=truncation_strategy.value, max_length=max_length, stride=stride, pad_to_multiple_of=None, return_attention_mask=False, return_token_type_ids=return_token_type_ids, return_overflowing_tokens=return_overflowing_tokens, return_special_tokens_mask=return_special_tokens_mask, return_length=return_length, return_tensors=None, prepend_batch_axis=False, verbose=verbose)
A:transformers.tokenization_utils.index->int(index)
A:transformers.tokenization_utils.filtered_tokens->self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)
A:transformers.tokenization_utils.clean_text->self.clean_up_tokenization(text)
transformers.PreTrainedTokenizer(self,**kwargs)
transformers.PreTrainedTokenizer.__len__(self)
transformers.PreTrainedTokenizer._add_tokens(self,new_tokens:Union[List[str],List[AddedToken]],special_tokens:bool=False)->int
transformers.PreTrainedTokenizer._batch_encode_plus(self,batch_text_or_text_pairs:Union[List[TextInput],List[TextInputPair],List[PreTokenizedInput],List[PreTokenizedInputPair],List[EncodedInput],List[EncodedInputPair]],add_special_tokens:bool=True,padding_strategy:PaddingStrategy=PaddingStrategy.DO_NOT_PAD,truncation_strategy:TruncationStrategy=TruncationStrategy.DO_NOT_TRUNCATE,max_length:Optional[int]=None,stride:int=0,is_pretokenized:bool=False,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[Union[str,TensorType]]=None,return_token_type_ids:Optional[bool]=None,return_attention_mask:Optional[bool]=None,return_overflowing_tokens:bool=False,return_special_tokens_mask:bool=False,return_offsets_mapping:bool=False,return_length:bool=False,verbose:bool=True,**kwargs)->BatchEncoding
transformers.PreTrainedTokenizer._batch_prepare_for_model(self,batch_ids_pairs:List[Union[PreTokenizedInputPair,Tuple[List[int],None]]],add_special_tokens:bool=True,padding_strategy:PaddingStrategy=PaddingStrategy.DO_NOT_PAD,truncation_strategy:TruncationStrategy=TruncationStrategy.DO_NOT_TRUNCATE,max_length:Optional[int]=None,stride:int=0,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[str]=None,return_token_type_ids:Optional[bool]=None,return_attention_mask:Optional[bool]=None,return_overflowing_tokens:bool=False,return_special_tokens_mask:bool=False,return_length:bool=False,verbose:bool=True)->BatchEncoding
transformers.PreTrainedTokenizer._convert_id_to_token(self,index:int)->str
transformers.PreTrainedTokenizer._convert_token_to_id(self,token)
transformers.PreTrainedTokenizer._convert_token_to_id_with_added_voc(self,token)
transformers.PreTrainedTokenizer._encode_plus(self,text:Union[TextInput,PreTokenizedInput,EncodedInput],text_pair:Optional[Union[TextInput,PreTokenizedInput,EncodedInput]]=None,add_special_tokens:bool=True,padding_strategy:PaddingStrategy=PaddingStrategy.DO_NOT_PAD,truncation_strategy:TruncationStrategy=TruncationStrategy.DO_NOT_TRUNCATE,max_length:Optional[int]=None,stride:int=0,is_pretokenized:bool=False,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[Union[str,TensorType]]=None,return_token_type_ids:Optional[bool]=None,return_attention_mask:Optional[bool]=None,return_overflowing_tokens:bool=False,return_special_tokens_mask:bool=False,return_offsets_mapping:bool=False,return_length:bool=False,verbose:bool=True,**kwargs)->BatchEncoding
transformers.PreTrainedTokenizer._tokenize(self,text,**kwargs)
transformers.PreTrainedTokenizer.convert_ids_to_tokens(self,ids:Union[int,List[int]],skip_special_tokens:bool=False)->Union[str, List[str]]
transformers.PreTrainedTokenizer.convert_tokens_to_ids(self,tokens:Union[str,List[str]])->Union[int, List[int]]
transformers.PreTrainedTokenizer.convert_tokens_to_string(self,tokens:List[str])->str
transformers.PreTrainedTokenizer.decode(self,token_ids:List[int],skip_special_tokens:bool=False,clean_up_tokenization_spaces:bool=True)->str
transformers.PreTrainedTokenizer.get_added_vocab(self)->Dict[str, int]
transformers.PreTrainedTokenizer.get_special_tokens_mask(self,token_ids_0:List,token_ids_1:Optional[List]=None,already_has_special_tokens:bool=False)->List[int]
transformers.PreTrainedTokenizer.get_vocab(self)->Dict[str, int]
transformers.PreTrainedTokenizer.is_fast(self)->bool
transformers.PreTrainedTokenizer.num_special_tokens_to_add(self,pair:bool=False)->int
transformers.PreTrainedTokenizer.prepare_for_tokenization(self,text:str,is_pretokenized:bool=False,**kwargs)->Tuple[str, Dict[str, Any]]
transformers.PreTrainedTokenizer.save_vocabulary(self,save_directory)->Tuple[str]
transformers.PreTrainedTokenizer.tokenize(self,text:TextInput,**kwargs)->List[str]
transformers.PreTrainedTokenizer.vocab_size(self)->int
transformers.tokenization_utils.PreTrainedTokenizer(self,**kwargs)
transformers.tokenization_utils.PreTrainedTokenizer.__init__(self,**kwargs)
transformers.tokenization_utils.PreTrainedTokenizer.__len__(self)
transformers.tokenization_utils.PreTrainedTokenizer._add_tokens(self,new_tokens:Union[List[str],List[AddedToken]],special_tokens:bool=False)->int
transformers.tokenization_utils.PreTrainedTokenizer._batch_encode_plus(self,batch_text_or_text_pairs:Union[List[TextInput],List[TextInputPair],List[PreTokenizedInput],List[PreTokenizedInputPair],List[EncodedInput],List[EncodedInputPair]],add_special_tokens:bool=True,padding_strategy:PaddingStrategy=PaddingStrategy.DO_NOT_PAD,truncation_strategy:TruncationStrategy=TruncationStrategy.DO_NOT_TRUNCATE,max_length:Optional[int]=None,stride:int=0,is_pretokenized:bool=False,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[Union[str,TensorType]]=None,return_token_type_ids:Optional[bool]=None,return_attention_mask:Optional[bool]=None,return_overflowing_tokens:bool=False,return_special_tokens_mask:bool=False,return_offsets_mapping:bool=False,return_length:bool=False,verbose:bool=True,**kwargs)->BatchEncoding
transformers.tokenization_utils.PreTrainedTokenizer._batch_prepare_for_model(self,batch_ids_pairs:List[Union[PreTokenizedInputPair,Tuple[List[int],None]]],add_special_tokens:bool=True,padding_strategy:PaddingStrategy=PaddingStrategy.DO_NOT_PAD,truncation_strategy:TruncationStrategy=TruncationStrategy.DO_NOT_TRUNCATE,max_length:Optional[int]=None,stride:int=0,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[str]=None,return_token_type_ids:Optional[bool]=None,return_attention_mask:Optional[bool]=None,return_overflowing_tokens:bool=False,return_special_tokens_mask:bool=False,return_length:bool=False,verbose:bool=True)->BatchEncoding
transformers.tokenization_utils.PreTrainedTokenizer._convert_id_to_token(self,index:int)->str
transformers.tokenization_utils.PreTrainedTokenizer._convert_token_to_id(self,token)
transformers.tokenization_utils.PreTrainedTokenizer._convert_token_to_id_with_added_voc(self,token)
transformers.tokenization_utils.PreTrainedTokenizer._encode_plus(self,text:Union[TextInput,PreTokenizedInput,EncodedInput],text_pair:Optional[Union[TextInput,PreTokenizedInput,EncodedInput]]=None,add_special_tokens:bool=True,padding_strategy:PaddingStrategy=PaddingStrategy.DO_NOT_PAD,truncation_strategy:TruncationStrategy=TruncationStrategy.DO_NOT_TRUNCATE,max_length:Optional[int]=None,stride:int=0,is_pretokenized:bool=False,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[Union[str,TensorType]]=None,return_token_type_ids:Optional[bool]=None,return_attention_mask:Optional[bool]=None,return_overflowing_tokens:bool=False,return_special_tokens_mask:bool=False,return_offsets_mapping:bool=False,return_length:bool=False,verbose:bool=True,**kwargs)->BatchEncoding
transformers.tokenization_utils.PreTrainedTokenizer._tokenize(self,text,**kwargs)
transformers.tokenization_utils.PreTrainedTokenizer.convert_ids_to_tokens(self,ids:Union[int,List[int]],skip_special_tokens:bool=False)->Union[str, List[str]]
transformers.tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids(self,tokens:Union[str,List[str]])->Union[int, List[int]]
transformers.tokenization_utils.PreTrainedTokenizer.convert_tokens_to_string(self,tokens:List[str])->str
transformers.tokenization_utils.PreTrainedTokenizer.decode(self,token_ids:List[int],skip_special_tokens:bool=False,clean_up_tokenization_spaces:bool=True)->str
transformers.tokenization_utils.PreTrainedTokenizer.get_added_vocab(self)->Dict[str, int]
transformers.tokenization_utils.PreTrainedTokenizer.get_special_tokens_mask(self,token_ids_0:List,token_ids_1:Optional[List]=None,already_has_special_tokens:bool=False)->List[int]
transformers.tokenization_utils.PreTrainedTokenizer.get_vocab(self)->Dict[str, int]
transformers.tokenization_utils.PreTrainedTokenizer.is_fast(self)->bool
transformers.tokenization_utils.PreTrainedTokenizer.num_special_tokens_to_add(self,pair:bool=False)->int
transformers.tokenization_utils.PreTrainedTokenizer.prepare_for_tokenization(self,text:str,is_pretokenized:bool=False,**kwargs)->Tuple[str, Dict[str, Any]]
transformers.tokenization_utils.PreTrainedTokenizer.save_vocabulary(self,save_directory)->Tuple[str]
transformers.tokenization_utils.PreTrainedTokenizer.tokenize(self,text:TextInput,**kwargs)->List[str]
transformers.tokenization_utils.PreTrainedTokenizer.vocab_size(self)->int
transformers.tokenization_utils._is_control(char)
transformers.tokenization_utils._is_end_of_word(text)
transformers.tokenization_utils._is_punctuation(char)
transformers.tokenization_utils._is_start_of_word(text)
transformers.tokenization_utils._is_whitespace(char)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/file_utils.py----------------------------------------
A:transformers.file_utils.logger->utils.logging.get_logger(__name__)
A:transformers.file_utils.USE_TF->os.environ.get('USE_TF', 'AUTO').upper()
A:transformers.file_utils.USE_TORCH->os.environ.get('USE_TORCH', 'AUTO').upper()
A:transformers.file_utils.torch_cache_home->os.path.expanduser(os.getenv('TORCH_HOME', os.path.join(os.getenv('XDG_CACHE_HOME', '~/.cache'), 'torch')))
A:transformers.file_utils.default_cache_path->os.path.join(torch_cache_home, 'transformers')
A:transformers.file_utils.PYTORCH_PRETRAINED_BERT_CACHE->os.getenv('PYTORCH_PRETRAINED_BERT_CACHE', default_cache_path)
A:transformers.file_utils.PYTORCH_TRANSFORMERS_CACHE->os.getenv('PYTORCH_TRANSFORMERS_CACHE', PYTORCH_PRETRAINED_BERT_CACHE)
A:transformers.file_utils.TRANSFORMERS_CACHE->os.getenv('TRANSFORMERS_CACHE', PYTORCH_TRANSFORMERS_CACHE)
A:transformers.file_utils.class_name->':class:`~transformers.{}`'.format(fn.__qualname__.split('.')[0])
A:transformers.file_utils.intro->intro.format(full_output_type=full_output_type, config_class=config_class).format(full_output_type=full_output_type, config_class=config_class)
A:transformers.file_utils.search->re.search('^(\\s*)\\S', t)
A:transformers.file_utils.indent->_get_indent(output_args_doc)
A:transformers.file_utils.blocks[i]->re.sub(':\\s*\\n\\s*(\\S)', ' -- \\1', blocks[i])
A:transformers.file_utils.lines->'\n'.join(lines).split('\n')
A:transformers.file_utils.docstrings->'\n'.join(lines)
A:transformers.file_utils.built_doc->code_sample.format(model_class=model_class, tokenizer_class=tokenizer_class, checkpoint=checkpoint)
A:transformers.file_utils.lines[i]->_prepare_output_docstrings(output_type, config_class)
A:transformers.file_utils.parsed->urlparse(url_or_filename)
A:transformers.file_utils.url_bytes->url.encode('utf-8')
A:transformers.file_utils.url_hash->sha256(url_bytes)
A:transformers.file_utils.filename->url_to_filename(url, etag)
A:transformers.file_utils.etag_bytes->response.headers.get('ETag').encode('utf-8')
A:transformers.file_utils.etag_hash->sha256(etag_bytes)
A:transformers.file_utils.cache_dir->str(cache_dir)
A:transformers.file_utils.cache_path->os.path.join(cache_dir, filename)
A:transformers.file_utils.metadata->json.load(meta_file)
A:transformers.file_utils.url_or_filename->str(url_or_filename)
A:transformers.file_utils.output_path->get_from_cache(url_or_filename, cache_dir=cache_dir, force_download=force_download, proxies=proxies, resume_download=resume_download, user_agent=user_agent, local_files_only=local_files_only)
A:transformers.file_utils.(output_dir, output_file)->os.path.split(output_path)
A:transformers.file_utils.output_path_extracted->os.path.join(output_dir, output_extract_dir_name)
A:transformers.file_utils.tar_file->tarfile.open(output_path)
A:transformers.file_utils.ua->'transformers/{}; python/{}'.format(__version__, sys.version.split()[0])
A:transformers.file_utils.response->requests.head(url, allow_redirects=True, proxies=proxies, timeout=etag_timeout)
A:transformers.file_utils.content_length->requests.head(url, allow_redirects=True, proxies=proxies, timeout=etag_timeout).headers.get('Content-Length')
A:transformers.file_utils.progress->tqdm(unit='B', unit_scale=True, total=total, initial=resume_size, desc='Downloading', disable=bool(logging.get_verbosity() == logging.NOTSET))
A:transformers.file_utils.etag->requests.head(url, allow_redirects=True, proxies=proxies, timeout=etag_timeout).headers.get('ETag')
A:transformers.file_utils.temp_file_manager->partial(tempfile.NamedTemporaryFile, dir=cache_dir, delete=False)
A:transformers.file_utils.cached->self.fget(obj)
A:transformers.file_utils.class_fields->fields(self)
A:transformers.file_utils.first_field->getattr(self, class_fields[0].name)
A:transformers.file_utils.other_fields_are_none->all((getattr(self, field.name) is None for field in class_fields[1:]))
A:transformers.file_utils.iterator->iter(first_field)
A:transformers.file_utils.v->getattr(self, field.name)
transformers.add_end_docstrings(*docstr)
transformers.add_start_docstrings(*docstr)
transformers.add_start_docstrings_to_callable(*docstr)
transformers.cached_path(url_or_filename,cache_dir=None,force_download=False,proxies=None,resume_download=False,user_agent:Union[Dict,str,None]=None,extract_compressed_file=False,force_extract=False,local_files_only=False)->Optional[str]
transformers.file_utils.ModelOutput(OrderedDict)
transformers.file_utils.ModelOutput.__delitem__(self,*args,**kwargs)
transformers.file_utils.ModelOutput.__getitem__(self,k)
transformers.file_utils.ModelOutput.__post_init__(self)
transformers.file_utils.ModelOutput.pop(self,*args,**kwargs)
transformers.file_utils.ModelOutput.setdefault(self,*args,**kwargs)
transformers.file_utils.ModelOutput.to_tuple(self)->Tuple[Any]
transformers.file_utils.ModelOutput.update(self,*args,**kwargs)
transformers.file_utils._convert_output_args_doc(output_args_doc)
transformers.file_utils._get_indent(t)
transformers.file_utils._prepare_output_docstrings(output_type,config_class)
transformers.file_utils.add_code_sample_docstrings(*docstr,tokenizer_class=None,checkpoint=None,output_type=None,config_class=None)
transformers.file_utils.add_end_docstrings(*docstr)
transformers.file_utils.add_start_docstrings(*docstr)
transformers.file_utils.add_start_docstrings_to_callable(*docstr)
transformers.file_utils.cached_path(url_or_filename,cache_dir=None,force_download=False,proxies=None,resume_download=False,user_agent:Union[Dict,str,None]=None,extract_compressed_file=False,force_extract=False,local_files_only=False)->Optional[str]
transformers.file_utils.cached_property(property)
transformers.file_utils.cached_property.__get__(self,obj,objtype=None)
transformers.file_utils.filename_to_url(filename,cache_dir=None)
transformers.file_utils.get_from_cache(url,cache_dir=None,force_download=False,proxies=None,etag_timeout=10,resume_download=False,user_agent:Union[Dict,str,None]=None,local_files_only=False)->Optional[str]
transformers.file_utils.hf_bucket_url(model_id:str,filename:str,use_cdn=True)->str
transformers.file_utils.http_get(url,temp_file,proxies=None,resume_size=0,user_agent:Union[Dict,str,None]=None)
transformers.file_utils.is_apex_available()
transformers.file_utils.is_nlp_available()
transformers.file_utils.is_psutil_available()
transformers.file_utils.is_py3nvml_available()
transformers.file_utils.is_remote_url(url_or_filename)
transformers.file_utils.is_tensor(x)
transformers.file_utils.is_tf_available()
transformers.file_utils.is_torch_available()
transformers.file_utils.is_torch_tpu_available()
transformers.file_utils.replace_return_docstrings(output_type=None,config_class=None)
transformers.file_utils.tf_required(func)
transformers.file_utils.torch_required(func)
transformers.file_utils.url_to_filename(url,etag=None)
transformers.is_apex_available()
transformers.is_nlp_available()
transformers.is_psutil_available()
transformers.is_py3nvml_available()
transformers.is_tf_available()
transformers.is_torch_available()
transformers.is_torch_tpu_available()


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/configuration_ctrl.py----------------------------------------
A:transformers.configuration_ctrl.logger->utils.logging.get_logger(__name__)
transformers.CTRLConfig(self,vocab_size=246534,n_positions=256,n_ctx=256,n_embd=1280,dff=8192,n_layer=48,n_head=16,resid_pdrop=0.1,embd_pdrop=0.1,attn_pdrop=0.1,layer_norm_epsilon=1e-06,initializer_range=0.02,summary_type='cls_index',summary_use_proj=True,summary_activation=None,summary_proj_to_labels=True,summary_first_dropout=0.1,**kwargs)
transformers.CTRLConfig.hidden_size(self)
transformers.CTRLConfig.max_position_embeddings(self)
transformers.CTRLConfig.num_attention_heads(self)
transformers.CTRLConfig.num_hidden_layers(self)
transformers.configuration_ctrl.CTRLConfig(self,vocab_size=246534,n_positions=256,n_ctx=256,n_embd=1280,dff=8192,n_layer=48,n_head=16,resid_pdrop=0.1,embd_pdrop=0.1,attn_pdrop=0.1,layer_norm_epsilon=1e-06,initializer_range=0.02,summary_type='cls_index',summary_use_proj=True,summary_activation=None,summary_proj_to_labels=True,summary_first_dropout=0.1,**kwargs)
transformers.configuration_ctrl.CTRLConfig.__init__(self,vocab_size=246534,n_positions=256,n_ctx=256,n_embd=1280,dff=8192,n_layer=48,n_head=16,resid_pdrop=0.1,embd_pdrop=0.1,attn_pdrop=0.1,layer_norm_epsilon=1e-06,initializer_range=0.02,summary_type='cls_index',summary_use_proj=True,summary_activation=None,summary_proj_to_labels=True,summary_first_dropout=0.1,**kwargs)
transformers.configuration_ctrl.CTRLConfig.hidden_size(self)
transformers.configuration_ctrl.CTRLConfig.max_position_embeddings(self)
transformers.configuration_ctrl.CTRLConfig.num_attention_heads(self)
transformers.configuration_ctrl.CTRLConfig.num_hidden_layers(self)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/configuration_gpt2.py----------------------------------------
A:transformers.configuration_gpt2.logger->utils.logging.get_logger(__name__)
transformers.GPT2Config(self,vocab_size=50257,n_positions=1024,n_ctx=1024,n_embd=768,n_layer=12,n_head=12,n_inner=None,activation_function='gelu_new',resid_pdrop=0.1,embd_pdrop=0.1,attn_pdrop=0.1,layer_norm_epsilon=1e-05,initializer_range=0.02,summary_type='cls_index',summary_use_proj=True,summary_activation=None,summary_proj_to_labels=True,summary_first_dropout=0.1,bos_token_id=50256,eos_token_id=50256,**kwargs)
transformers.GPT2Config.hidden_size(self)
transformers.GPT2Config.max_position_embeddings(self)
transformers.GPT2Config.num_attention_heads(self)
transformers.GPT2Config.num_hidden_layers(self)
transformers.configuration_gpt2.GPT2Config(self,vocab_size=50257,n_positions=1024,n_ctx=1024,n_embd=768,n_layer=12,n_head=12,n_inner=None,activation_function='gelu_new',resid_pdrop=0.1,embd_pdrop=0.1,attn_pdrop=0.1,layer_norm_epsilon=1e-05,initializer_range=0.02,summary_type='cls_index',summary_use_proj=True,summary_activation=None,summary_proj_to_labels=True,summary_first_dropout=0.1,bos_token_id=50256,eos_token_id=50256,**kwargs)
transformers.configuration_gpt2.GPT2Config.__init__(self,vocab_size=50257,n_positions=1024,n_ctx=1024,n_embd=768,n_layer=12,n_head=12,n_inner=None,activation_function='gelu_new',resid_pdrop=0.1,embd_pdrop=0.1,attn_pdrop=0.1,layer_norm_epsilon=1e-05,initializer_range=0.02,summary_type='cls_index',summary_use_proj=True,summary_activation=None,summary_proj_to_labels=True,summary_first_dropout=0.1,bos_token_id=50256,eos_token_id=50256,**kwargs)
transformers.configuration_gpt2.GPT2Config.hidden_size(self)
transformers.configuration_gpt2.GPT2Config.max_position_embeddings(self)
transformers.configuration_gpt2.GPT2Config.num_attention_heads(self)
transformers.configuration_gpt2.GPT2Config.num_hidden_layers(self)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/generation_tf_utils.py----------------------------------------
A:transformers.generation_tf_utils.logger->utils.logging.get_logger(__name__)
A:transformers.generation_tf_utils.input_ids->tensorflow.concat([input_ids, tf.expand_dims(beam_tokens, 1)], axis=-1)
A:transformers.generation_tf_utils.attention_mask->tensorflow.concat([attention_mask, tf.ones((shape_list(attention_mask)[0], 1), dtype=tf.int32)], axis=-1)
A:transformers.generation_tf_utils.encoder->self.get_encoder()
A:transformers.generation_tf_utils.encoder_outputs->encoder(input_ids, attention_mask=attention_mask)
A:transformers.generation_tf_utils.expanded_batch_idxs->tensorflow.reshape(tf.repeat(tf.expand_dims(tf.range(batch_size), -1), repeats=num_beams * effective_batch_mult, axis=1), shape=(-1,))
A:transformers.generation_tf_utils.output->self._generate_no_beam_search(input_ids, cur_len=cur_len, max_length=max_length, min_length=min_length, do_sample=do_sample, temperature=temperature, top_k=top_k, top_p=top_p, repetition_penalty=repetition_penalty, no_repeat_ngram_size=no_repeat_ngram_size, bad_words_ids=bad_words_ids, pad_token_id=pad_token_id, eos_token_id=eos_token_id, batch_size=effective_batch_size, vocab_size=vocab_size, encoder_outputs=encoder_outputs, attention_mask=attention_mask, use_cache=use_cache)
A:transformers.generation_tf_utils.unfinished_sents->tensorflow.ones_like(input_ids[:, 0])
A:transformers.generation_tf_utils.model_inputs->self.prepare_inputs_for_generation(input_ids, past=past, attention_mask=attention_mask, use_cache=use_cache)
A:transformers.generation_tf_utils.outputs->self(**model_inputs)
A:transformers.generation_tf_utils.next_token_logits_penalties->_create_next_token_logits_penalties(input_ids, next_token_logits, repetition_penalty)
A:transformers.generation_tf_utils.next_token_logits->tensorflow.math.multiply(next_token_logits, next_token_logits_penalties)
A:transformers.generation_tf_utils.banned_tokens->calc_banned_bad_words_ids(input_ids, bad_words_ids)
A:transformers.generation_tf_utils.is_token_logit_eos_token->tensorflow.convert_to_tensor([True if token is eos_token_id else False for token in range(vocab_size)], dtype=tf.bool)
A:transformers.generation_tf_utils.eos_token_indices_mask->tensorflow.broadcast_to(is_token_logit_eos_token, [num_batch_hypotheses, vocab_size])
A:transformers.generation_tf_utils.next_token->tensorflow.math.argmax(next_token_logits, axis=-1, output_type=tf.int32)
A:transformers.generation_tf_utils.is_sents_unfinished_and_token_to_add_is_eos->tensorflow.math.multiply(unfinished_sents, tf.cast(eos_in_sents, tf.int32))
A:transformers.generation_tf_utils.min_sent_length->tensorflow.math.reduce_min(sent_lengths)
A:transformers.generation_tf_utils.max_sent_length->tensorflow.math.reduce_max(sent_lengths)
A:transformers.generation_tf_utils.broad_casted_sent_lengths->tensorflow.broadcast_to(tf.expand_dims(sent_lengths, -1), [batch_size, max_sent_length])
A:transformers.generation_tf_utils.broad_casted_range->tensorflow.transpose(tf.broadcast_to(tf.expand_dims(tf.range(max_sent_length), -1), [max_sent_length, batch_size]))
A:transformers.generation_tf_utils.decoded->tensorflow.stack(best)
A:transformers.generation_tf_utils.beam_scores_begin->tensorflow.zeros((batch_size, 1), dtype=tf.float32)
A:transformers.generation_tf_utils.beam_scores->tensorflow.convert_to_tensor([x[0] for x in next_batch_beam], dtype=tf.float32)
A:transformers.generation_tf_utils.scores->set_tensor_by_indices_to_value(scores, tf.convert_to_tensor(banned_tokens_indices_mask, dtype=tf.bool), -float('inf'))
A:transformers.generation_tf_utils._scores->tensorflow.reshape(_scores, (batch_size, num_beams * vocab_size))
A:transformers.generation_tf_utils.next_tokens->tensorflow.gather(next_tokens, next_scores_indices, batch_dims=1)
A:transformers.generation_tf_utils.next_scores->tensorflow.reshape(next_scores, (batch_size, num_beams * vocab_size))
A:transformers.generation_tf_utils.next_scores_indices->tensorflow.argsort(next_scores, direction='DESCENDING', axis=1)
A:transformers.generation_tf_utils.(next_scores, next_tokens)->tensorflow.math.top_k(next_scores, k=2 * num_beams, sorted=True)
A:transformers.generation_tf_utils.beam_tokens->tensorflow.convert_to_tensor([x[1] for x in next_batch_beam], dtype=tf.int32)
A:transformers.generation_tf_utils.beam_idx->tensorflow.convert_to_tensor([x[2] for x in next_batch_beam], dtype=tf.int32)
A:transformers.generation_tf_utils.past->self._reorder_cache(past, beam_idx)
A:transformers.generation_tf_utils.final_score->beam_scores[effective_beam_id].numpy().item()
A:transformers.generation_tf_utils.sorted_hyps->sorted(hypotheses.beams, key=lambda x: x[0])
A:transformers.generation_tf_utils.sent_lengths->tensorflow.convert_to_tensor(sent_lengths_list, dtype=tf.int32)
A:transformers.generation_tf_utils.sent_max_len->min(tf.reduce_max(sent_lengths).numpy() + 1, max_length)
A:transformers.generation_tf_utils.decoded_slice->tensorflow.where(tf.range(sent_max_len, dtype=tf.int32) == sent_lengths[i], eos_token_id * tf.ones((sent_max_len,), dtype=tf.int32), decoded_slice)
A:transformers.generation_tf_utils.token_penalties->numpy.ones(shape_list(logits))
A:transformers.generation_tf_utils.logit_penalties->numpy.zeros(logit_penalized.shape)
A:transformers.generation_tf_utils.gen_tokens->prev_input_ids[idx].numpy().tolist()
A:transformers.generation_tf_utils.prev_ngram_tuple->tuple(ngram[:-1])
A:transformers.generation_tf_utils.ngram_idx->tuple(prev_input_ids[hypo_idx, start_idx:cur_len].numpy().tolist())
A:transformers.generation_tf_utils.logits_shape->shape_list(logits)
A:transformers.generation_tf_utils.top_k->min(max(top_k, min_tokens_to_keep), logits_shape[-1])
A:transformers.generation_tf_utils.logits->set_tensor_by_indices_to_value(logits, indices_to_remove, filter_value)
A:transformers.generation_tf_utils.sorted_indices->tensorflow.argsort(logits, direction='DESCENDING')
A:transformers.generation_tf_utils.sorted_logits->tensorflow.gather(logits, sorted_indices, axis=-1, batch_dims=1)
A:transformers.generation_tf_utils.cumulative_probs->tensorflow.math.cumsum(tf.nn.softmax(sorted_logits, axis=-1), axis=-1)
A:transformers.generation_tf_utils.sorted_indices_to_remove->tensorflow.concat([tf.zeros_like(sorted_indices_to_remove[:, :1]), sorted_indices_to_remove[:, 1:]], -1)
A:transformers.generation_tf_utils.indices_to_remove->scatter_values_on_batch_indices(sorted_indices_to_remove, sorted_indices)
A:transformers.generation_tf_utils.shape->shape_list(batch_indices)
A:transformers.generation_tf_utils.broad_casted_batch_dims->tensorflow.reshape(tf.broadcast_to(tf.expand_dims(tf.range(shape[0]), axis=-1), shape), [1, -1])
A:transformers.generation_tf_utils.pair_indices->tensorflow.transpose(tf.concat([broad_casted_batch_dims, tf.reshape(batch_indices, [1, -1])], 0))
A:transformers.generation_tf_utils.(_, indices)->tensorflow.nn.top_k(logits + z, num_samples)
A:transformers.generation_tf_utils.static->x.shape.as_list()
A:transformers.generation_tf_utils.dynamic->tensorflow.shape(x)
A:transformers.generation_tf_utils.sorted_scores->sorted([(s, idx) for (idx, (s, _)) in enumerate(self.beams)])
A:transformers.generation_tf_utils.self.worst_score->min(score, self.worst_score)
transformers.generation_tf_utils.BeamHypotheses(self,num_beams,max_length,length_penalty,early_stopping)
transformers.generation_tf_utils.BeamHypotheses.__init__(self,num_beams,max_length,length_penalty,early_stopping)
transformers.generation_tf_utils.BeamHypotheses.__len__(self)
transformers.generation_tf_utils.BeamHypotheses.add(self,hyp,sum_logprobs)
transformers.generation_tf_utils.BeamHypotheses.is_done(self,best_sum_logprobs,cur_len)
transformers.generation_tf_utils.TFGenerationMixin
transformers.generation_tf_utils.TFGenerationMixin._generate_beam_search(self,input_ids,cur_len,max_length,min_length,do_sample,early_stopping,temperature,top_k,top_p,repetition_penalty,no_repeat_ngram_size,bad_words_ids,pad_token_id,eos_token_id,batch_size,num_return_sequences,length_penalty,num_beams,vocab_size,encoder_outputs,attention_mask,use_cache)
transformers.generation_tf_utils.TFGenerationMixin._generate_no_beam_search(self,input_ids,cur_len,max_length,min_length,do_sample,temperature,top_k,top_p,repetition_penalty,no_repeat_ngram_size,bad_words_ids,pad_token_id,eos_token_id,batch_size,vocab_size,encoder_outputs,attention_mask,use_cache)
transformers.generation_tf_utils.TFGenerationMixin._reorder_cache(past,beam_idx)
transformers.generation_tf_utils.TFGenerationMixin._use_cache(self,outputs,use_cache)
transformers.generation_tf_utils.TFGenerationMixin.generate(self,input_ids=None,max_length=None,min_length=None,do_sample=None,early_stopping=None,num_beams=None,temperature=None,top_k=None,top_p=None,repetition_penalty=None,bad_words_ids=None,bos_token_id=None,pad_token_id=None,eos_token_id=None,length_penalty=None,no_repeat_ngram_size=None,num_return_sequences=None,attention_mask=None,decoder_start_token_id=None,use_cache=None)
transformers.generation_tf_utils.TFGenerationMixin.prepare_inputs_for_generation(self,inputs,**kwargs)
transformers.generation_tf_utils._create_next_token_logits_penalties(input_ids,logits,repetition_penalty)
transformers.generation_tf_utils.calc_banned_bad_words_ids(prev_input_ids,bad_words_ids)
transformers.generation_tf_utils.calc_banned_ngram_tokens(prev_input_ids,num_hypos,no_repeat_ngram_size,cur_len)
transformers.generation_tf_utils.sample_without_replacement(logits,num_samples)
transformers.generation_tf_utils.scatter_values_on_batch_indices(values,batch_indices)
transformers.generation_tf_utils.set_tensor_by_indices_to_value(tensor,indices,value)
transformers.generation_tf_utils.shape_list(x)
transformers.generation_tf_utils.tf_top_k_top_p_filtering(logits,top_k=0,top_p=1.0,filter_value=-float('Inf'),min_tokens_to_keep=1)
transformers.tf_top_k_top_p_filtering(logits,top_k=0,top_p=1.0,filter_value=-float('Inf'),min_tokens_to_keep=1)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/configuration_albert.py----------------------------------------
transformers.AlbertConfig(self,vocab_size=30000,embedding_size=128,hidden_size=4096,num_hidden_layers=12,num_hidden_groups=1,num_attention_heads=64,intermediate_size=16384,inner_group_num=1,hidden_act='gelu_new',hidden_dropout_prob=0,attention_probs_dropout_prob=0,max_position_embeddings=512,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,classifier_dropout_prob=0.1,pad_token_id=0,bos_token_id=2,eos_token_id=3,**kwargs)
transformers.configuration_albert.AlbertConfig(self,vocab_size=30000,embedding_size=128,hidden_size=4096,num_hidden_layers=12,num_hidden_groups=1,num_attention_heads=64,intermediate_size=16384,inner_group_num=1,hidden_act='gelu_new',hidden_dropout_prob=0,attention_probs_dropout_prob=0,max_position_embeddings=512,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,classifier_dropout_prob=0.1,pad_token_id=0,bos_token_id=2,eos_token_id=3,**kwargs)
transformers.configuration_albert.AlbertConfig.__init__(self,vocab_size=30000,embedding_size=128,hidden_size=4096,num_hidden_layers=12,num_hidden_groups=1,num_attention_heads=64,intermediate_size=16384,inner_group_num=1,hidden_act='gelu_new',hidden_dropout_prob=0,attention_probs_dropout_prob=0,max_position_embeddings=512,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,classifier_dropout_prob=0.1,pad_token_id=0,bos_token_id=2,eos_token_id=3,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/modeling_transfo_xl_utilities.py----------------------------------------
A:transformers.modeling_transfo_xl_utilities.self.cluster_weight->torch.nn.Parameter(torch.zeros(self.n_clusters, self.d_embed))
A:transformers.modeling_transfo_xl_utilities.self.cluster_bias->torch.nn.Parameter(torch.zeros(self.n_clusters))
A:transformers.modeling_transfo_xl_utilities.self.out_layers->torch.nn.ModuleList()
A:transformers.modeling_transfo_xl_utilities.self.out_projs->torch.nn.ParameterList()
A:transformers.modeling_transfo_xl_utilities.logit->self._compute_logit(hidden, self.out_layers[0].weight, self.out_layers[0].bias, self.out_projs[0])
A:transformers.modeling_transfo_xl_utilities.proj_hid->torch.nn.functional.linear(hidden, proj.t().contiguous())
A:transformers.modeling_transfo_xl_utilities.hidden->hidden.view(-1, hidden.size(-1)).view(-1, hidden.size(-1))
A:transformers.modeling_transfo_xl_utilities.labels->labels.view(-1).view(-1)
A:transformers.modeling_transfo_xl_utilities.out->hidden.view(-1, hidden.size(-1)).view(-1, hidden.size(-1)).new_empty((head_logit.size(0), self.n_token))
A:transformers.modeling_transfo_xl_utilities.weight_i->torch.cat([weight_i, self.cluster_weight], dim=0)
A:transformers.modeling_transfo_xl_utilities.bias_i->torch.cat([bias_i, self.cluster_bias], dim=0)
A:transformers.modeling_transfo_xl_utilities.head_logit->self._compute_logit(hidden, head_weight, head_bias, head_proj)
A:transformers.modeling_transfo_xl_utilities.head_logprob->torch.nn.functional.log_softmax(head_logit, dim=1)
A:transformers.modeling_transfo_xl_utilities.indices_i->mask_i.nonzero().squeeze()
A:transformers.modeling_transfo_xl_utilities.head_logprob_i->torch.nn.functional.log_softmax(head_logit, dim=1).index_select(0, indices_i)
A:transformers.modeling_transfo_xl_utilities.hidden_i->hidden.view(-1, hidden.size(-1)).view(-1, hidden.size(-1)).index_select(0, indices_i)
A:transformers.modeling_transfo_xl_utilities.logprob_i->torch.nn.functional.log_softmax(head_logit, dim=1).index_select(0, indices_i).gather(1, target_i[:, None]).squeeze(1)
A:transformers.modeling_transfo_xl_utilities.tail_logit_i->self._compute_logit(hidden, weight_i, bias_i, proj_i)
A:transformers.modeling_transfo_xl_utilities.tail_logprob_i->torch.nn.functional.log_softmax(tail_logit_i, dim=1)
transformers.modeling_transfo_xl_utilities.ProjectedAdaptiveLogSoftmax(self,n_token,d_embed,d_proj,cutoffs,div_val=1,keep_order=False)
transformers.modeling_transfo_xl_utilities.ProjectedAdaptiveLogSoftmax.__init__(self,n_token,d_embed,d_proj,cutoffs,div_val=1,keep_order=False)
transformers.modeling_transfo_xl_utilities.ProjectedAdaptiveLogSoftmax._compute_logit(self,hidden,weight,bias,proj)
transformers.modeling_transfo_xl_utilities.ProjectedAdaptiveLogSoftmax.forward(self,hidden,labels=None,keep_order=False)
transformers.modeling_transfo_xl_utilities.ProjectedAdaptiveLogSoftmax.log_prob(self,hidden)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/convert_marian_to_pytorch.py----------------------------------------
A:transformers.convert_marian_to_pytorch.fields->lmap(str.strip, x.replace('\t', '').split('|')[1:-1])
A:transformers.convert_marian_to_pytorch.md_content->Path(readme_path).open().read()
A:transformers.convert_marian_to_pytorch.data->lmap(_process_benchmark_table_row, entries)
A:transformers.convert_marian_to_pytorch.old_reg->old_reg.set_index('id').set_index('id')
A:transformers.convert_marian_to_pytorch.released['fname']->released['url_base'].apply(lambda x: remove_suffix(remove_prefix(x, 'https://object.pouta.csc.fi/Tatoeba-Challenge/opus'), '.zip'))
A:transformers.convert_marian_to_pytorch.released['2m']->released.fname.str.startswith('2m')
A:transformers.convert_marian_to_pytorch.released['date']->pandas.to_datetime(released['fname'].apply(lambda x: remove_prefix(remove_prefix(x, '2m-'), '-')))
A:transformers.convert_marian_to_pytorch.newest_released->released.dsort('date').drop_duplicates(['short_pair'], keep='first')
A:transformers.convert_marian_to_pytorch.short_to_long->released.groupby('short_pair').pair.first().to_dict()
A:transformers.convert_marian_to_pytorch.overlap_short->old_reg.set_index('id').set_index('id').index.intersection(released.short_pair.unique())
A:transformers.convert_marian_to_pytorch.bm_data->process_last_benchmark_table(pat.format(o))
A:transformers.convert_marian_to_pytorch.tab->pandas.DataFrame(bm_data, columns=['testset', 'bleu', 'chr-f'])
A:transformers.convert_marian_to_pytorch.cmp_df->pandas.DataFrame(dict(short=overlap_short, long=overlap_long, old_bleu=old_bleu, new_bleu=new_reported_bleu)).fillna(-1)
A:transformers.convert_marian_to_pytorch.blacklist->dominated.long.unique().tolist()
A:transformers.convert_marian_to_pytorch.stripped->remove_prefix(k, layer_prefix)
A:transformers.convert_marian_to_pytorch.sd[converter[stripped]]->torch.tensor(v).squeeze()
A:transformers.convert_marian_to_pytorch.sd->convert_encoder_layer(opus_state, layer_tag, converter)
A:transformers.convert_marian_to_pytorch.api->HfApi()
A:transformers.convert_marian_to_pytorch.model_list->HfApi().model_list()
A:transformers.convert_marian_to_pytorch.embs_to_add->numpy.zeros((n_special_tokens, d_model))
A:transformers.convert_marian_to_pytorch.new_embs->numpy.concatenate([wemb, embs_to_add])
A:transformers.convert_marian_to_pytorch.bias_to_add->numpy.zeros((n_special_tokens, 1))
A:transformers.convert_marian_to_pytorch.new_bias->numpy.concatenate((final_bias, bias_to_add), axis=1)
A:transformers.convert_marian_to_pytorch.cfg_str->''.join([chr(x) for x in opus_dict[CONFIG_KEY]])
A:transformers.convert_marian_to_pytorch.yaml_cfg->yaml.load(cfg_str[:-1], Loader=yaml.BaseLoader)
A:transformers.convert_marian_to_pytorch.model_files->list(Path(dest_dir).glob('*.npz'))
A:transformers.convert_marian_to_pytorch.x->x.replace(substr, grp_name).replace(substr, grp_name)
A:transformers.convert_marian_to_pytorch.hf_model_name->remove_prefix(hf_model_name, ORG_NAME)
A:transformers.convert_marian_to_pytorch.opus_w_prefix->remove_prefix(hf_model_name, ORG_NAME).replace('_', '+')
A:transformers.convert_marian_to_pytorch.opus_readme_path->Path(repo_root).joinpath('models', opus_name, 'README.md')
A:transformers.convert_marian_to_pytorch.content->'*'.join(splat)
A:transformers.convert_marian_to_pytorch.items->'\n\n'.join([f'- {k}: {v}' for (k, v) in metadata.items()])
A:transformers.convert_marian_to_pytorch.n_dash->p.name.count('-')
A:transformers.convert_marian_to_pytorch.lns->Path(fname).open().readlines()
A:transformers.convert_marian_to_pytorch.results[p.name]->_parse_readme(lns)
A:transformers.convert_marian_to_pytorch.save_dir->Path('marian_ckpt')
A:transformers.convert_marian_to_pytorch.dest_dir->Path(dest_dir)
A:transformers.convert_marian_to_pytorch.pair_name->convert_opus_name_to_hf_name(k)
A:transformers.convert_marian_to_pytorch.fname->wget.download(test_set_url, 'opus_test.txt')
A:transformers.convert_marian_to_pytorch.src->lmap(str.strip, lns[::4])
A:transformers.convert_marian_to_pytorch.gold->lmap(str.strip, lns[1::4])
A:transformers.convert_marian_to_pytorch.mar_model->lmap(str.strip, lns[2::4])
A:transformers.convert_marian_to_pytorch.ln->ln[1:].strip()
A:transformers.convert_marian_to_pytorch.splat->ln[1:].strip().split(':')
A:transformers.convert_marian_to_pytorch.dname->Path(dest_dir).name.split('-')
A:transformers.convert_marian_to_pytorch.dct->dict(target_lang=dname[-1], source_lang='-'.join(dname[:-1]))
A:transformers.convert_marian_to_pytorch.vocab->load_yaml(find_vocab_file(model_dir))
A:transformers.convert_marian_to_pytorch.num_added->add_to_vocab_(vocab, ['<pad>'])
A:transformers.convert_marian_to_pytorch.dest->Path(save_directory)
A:transformers.convert_marian_to_pytorch.src_path->Path(self.init_kwargs['source_spm'])
A:transformers.convert_marian_to_pytorch.npz_path->find_model_file(source_dir)
A:transformers.convert_marian_to_pytorch.self.state_dict->dict(self.state_dict)
A:transformers.convert_marian_to_pytorch.cfg->load_config_from_state_dict(self.state_dict)
A:transformers.convert_marian_to_pytorch.(self.wemb, self.final_bias)->add_emb_entries(self.state_dict['Wemb'], self.state_dict[BIAS_KEY], 1)
A:transformers.convert_marian_to_pytorch.self.state_keys->list(self.state_dict.keys())
A:transformers.convert_marian_to_pytorch.decoder_yml->cast_marian_config(load_yaml(source_dir / 'decoder.yml'))
A:transformers.convert_marian_to_pytorch.self.hf_config->MarianConfig(vocab_size=cfg['vocab_size'], decoder_layers=cfg['dec-depth'], encoder_layers=cfg['enc-depth'], decoder_attention_heads=cfg['transformer-heads'], encoder_attention_heads=cfg['transformer-heads'], decoder_ffn_dim=cfg['transformer-dim-ffn'], encoder_ffn_dim=cfg['transformer-dim-ffn'], d_model=cfg['dim-emb'], activation_function=cfg['transformer-aan-activation'], pad_token_id=self.pad_token_id, eos_token_id=0, bos_token_id=0, max_position_embeddings=cfg['dim-emb'], scale_embedding=True, normalize_embedding='n' in cfg['transformer-preprocess'], static_position_embeddings=not cfg['transformer-train-position-embeddings'], dropout=0.1, num_beams=decoder_yml['beam-size'], decoder_start_token_id=self.pad_token_id, bad_words_ids=[[self.pad_token_id]], max_length=512)
A:transformers.convert_marian_to_pytorch.self.encoder_l1->self.sub_keys('encoder_l1')
A:transformers.convert_marian_to_pytorch.self.decoder_l1->self.sub_keys('decoder_l1')
A:transformers.convert_marian_to_pytorch.self.decoder_l2->self.sub_keys('decoder_l2')
A:transformers.convert_marian_to_pytorch.model->OpusState(source_dir).load_marian_model()
A:transformers.convert_marian_to_pytorch.wemb_tensor->torch.nn.Parameter(torch.FloatTensor(self.wemb))
A:transformers.convert_marian_to_pytorch.bias_tensor->torch.nn.Parameter(torch.FloatTensor(self.final_bias))
A:transformers.convert_marian_to_pytorch.wpos_tensor->torch.tensor(state_dict['Wpos'])
A:transformers.convert_marian_to_pytorch.filename->wget.download(url)
A:transformers.convert_marian_to_pytorch.tokenizer->transformers.MarianTokenizer.from_pretrained(str(source_dir))
A:transformers.convert_marian_to_pytorch.opus_state->OpusState(source_dir)
A:transformers.convert_marian_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.convert_marian_to_pytorch.args->argparse.ArgumentParser().parse_args()
A:transformers.convert_marian_to_pytorch.source_dir->Path(args.src)
transformers.convert_marian_to_pytorch.OpusState(self,source_dir)
transformers.convert_marian_to_pytorch.OpusState.__init__(self,source_dir)
transformers.convert_marian_to_pytorch.OpusState._check_layer_entries(self)
transformers.convert_marian_to_pytorch.OpusState.extra_keys(self)
transformers.convert_marian_to_pytorch.OpusState.load_marian_model(self)->MarianMTModel
transformers.convert_marian_to_pytorch.OpusState.sub_keys(self,layer_prefix)
transformers.convert_marian_to_pytorch._cast_yaml_str(v)
transformers.convert_marian_to_pytorch._parse_readme(lns)
transformers.convert_marian_to_pytorch._process_benchmark_table_row(x)
transformers.convert_marian_to_pytorch.add_emb_entries(wemb,final_bias,n_special_tokens=1)
transformers.convert_marian_to_pytorch.add_special_tokens_to_vocab(model_dir:Path)->None
transformers.convert_marian_to_pytorch.add_to_vocab_(vocab:Dict[str,int],special_tokens:List[str])
transformers.convert_marian_to_pytorch.cast_marian_config(raw_cfg:Dict[str,str])->Dict
transformers.convert_marian_to_pytorch.check_equal(marian_cfg,k1,k2)
transformers.convert_marian_to_pytorch.check_if_models_are_dominated(old_repo_path='OPUS-MT-train/models',new_repo_path='Tatoeba-Challenge/models/')
transformers.convert_marian_to_pytorch.check_marian_cfg_assumptions(marian_cfg)
transformers.convert_marian_to_pytorch.convert(source_dir:Path,dest_dir)
transformers.convert_marian_to_pytorch.convert_all_sentencepiece_models(model_list=None,repo_path=None)
transformers.convert_marian_to_pytorch.convert_encoder_layer(opus_dict,layer_prefix:str,converter:dict)
transformers.convert_marian_to_pytorch.convert_hf_name_to_opus_name(hf_model_name)
transformers.convert_marian_to_pytorch.convert_opus_name_to_hf_name(x)
transformers.convert_marian_to_pytorch.convert_whole_dir(path=Path('marian_ckpt/'))
transformers.convert_marian_to_pytorch.download_and_unzip(url,dest_dir)
transformers.convert_marian_to_pytorch.fetch_test_set(test_set_url)
transformers.convert_marian_to_pytorch.find_model_file(dest_dir)
transformers.convert_marian_to_pytorch.find_pretrained_model(src_lang:str,tgt_lang:str)->List[str]
transformers.convert_marian_to_pytorch.find_vocab_file(model_dir)
transformers.convert_marian_to_pytorch.get_clean_model_id_mapping(multiling_model_ids)
transformers.convert_marian_to_pytorch.get_system_metadata(repo_root)
transformers.convert_marian_to_pytorch.lmap(f,x)->List
transformers.convert_marian_to_pytorch.load_config_from_state_dict(opus_dict)
transformers.convert_marian_to_pytorch.load_layers_(layer_lst:torch.nn.ModuleList,opus_state:dict,converter,is_decoder=False)
transformers.convert_marian_to_pytorch.load_yaml(path)
transformers.convert_marian_to_pytorch.make_registry(repo_path='Opus-MT-train/models')
transformers.convert_marian_to_pytorch.make_tatoeba_registry(repo_path='Tatoeba-Challenge/models')
transformers.convert_marian_to_pytorch.process_last_benchmark_table(readme_path)->List[Tuple[str, float, float]]
transformers.convert_marian_to_pytorch.remove_prefix(text:str,prefix:str)
transformers.convert_marian_to_pytorch.remove_suffix(text:str,suffix:str)
transformers.convert_marian_to_pytorch.save_json(content:Union[Dict,List],path:str)->None
transformers.convert_marian_to_pytorch.save_tokenizer(self,save_directory)
transformers.convert_marian_to_pytorch.save_tokenizer_config(dest_dir:Path)
transformers.convert_marian_to_pytorch.unzip(zip_path:str,dest_dir:str)->None
transformers.convert_marian_to_pytorch.write_model_card(hf_model_name:str,repo_root='OPUS-MT-train',save_dir=Path('marian_converted'),dry_run=False,extra_metadata={})->str


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/modeling_tf_camembert.py----------------------------------------
A:transformers.modeling_tf_camembert.logger->utils.logging.get_logger(__name__)
transformers.TFCamembertForMaskedLM(TFRobertaForMaskedLM)
transformers.TFCamembertForMultipleChoice(TFRobertaForMultipleChoice)
transformers.TFCamembertForQuestionAnswering(TFRobertaForQuestionAnswering)
transformers.TFCamembertForSequenceClassification(TFRobertaForSequenceClassification)
transformers.TFCamembertForTokenClassification(TFRobertaForTokenClassification)
transformers.TFCamembertModel(TFRobertaModel)
transformers.modeling_tf_camembert.TFCamembertForMaskedLM(TFRobertaForMaskedLM)
transformers.modeling_tf_camembert.TFCamembertForMultipleChoice(TFRobertaForMultipleChoice)
transformers.modeling_tf_camembert.TFCamembertForQuestionAnswering(TFRobertaForQuestionAnswering)
transformers.modeling_tf_camembert.TFCamembertForSequenceClassification(TFRobertaForSequenceClassification)
transformers.modeling_tf_camembert.TFCamembertForTokenClassification(TFRobertaForTokenClassification)
transformers.modeling_tf_camembert.TFCamembertModel(TFRobertaModel)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/tokenization_dpr.py----------------------------------------
A:transformers.tokenization_dpr.logger->utils.logging.get_logger(__name__)
A:transformers.tokenization_dpr.DPRSpanPrediction->collections.namedtuple('DPRSpanPrediction', ['span_score', 'relevance_score', 'doc_id', 'start_index', 'end_index', 'text'])
A:transformers.tokenization_dpr.DPRReaderOutput->collections.namedtuple('DPRReaderOutput', ['start_logits', 'end_logits', 'relevance_logits'])
A:transformers.tokenization_dpr.n_passages->len(relevance_logits)
A:transformers.tokenization_dpr.sorted_docs->sorted(range(n_passages), reverse=True, key=relevance_logits.__getitem__)
A:transformers.tokenization_dpr.sequence_ids->list(input_ids[doc_id])
A:transformers.tokenization_dpr.sequence_len->len(sequence_ids)
A:transformers.tokenization_dpr.best_spans->self._get_best_spans(start_logits=start_logits[doc_id][passage_offset:sequence_len], end_logits=end_logits[doc_id][passage_offset:sequence_len], max_answer_length=max_answer_length, top_spans=num_spans_per_passage)
A:transformers.tokenization_dpr.scores->sorted(scores, key=lambda x: x[1], reverse=True)
transformers.DPRContextEncoderTokenizer(BertTokenizer)
transformers.DPRContextEncoderTokenizerFast(BertTokenizerFast)
transformers.DPRQuestionEncoderTokenizer(BertTokenizer)
transformers.DPRQuestionEncoderTokenizerFast(BertTokenizerFast)
transformers.DPRReaderTokenizer(CustomDPRReaderTokenizerMixin,BertTokenizer)
transformers.DPRReaderTokenizerFast(CustomDPRReaderTokenizerMixin,BertTokenizerFast)
transformers.tokenization_dpr.CustomDPRReaderTokenizerMixin(self,questions,titles:Optional[str]=None,texts:Optional[str]=None,padding:Union[bool,str]=False,truncation:Union[bool,str]=False,max_length:Optional[int]=None,return_tensors:Optional[Union[str,TensorType]]=None,return_attention_mask:Optional[bool]=None,**kwargs)
transformers.tokenization_dpr.CustomDPRReaderTokenizerMixin.__call__(self,questions,titles:Optional[str]=None,texts:Optional[str]=None,padding:Union[bool,str]=False,truncation:Union[bool,str]=False,max_length:Optional[int]=None,return_tensors:Optional[Union[str,TensorType]]=None,return_attention_mask:Optional[bool]=None,**kwargs)
transformers.tokenization_dpr.CustomDPRReaderTokenizerMixin._get_best_spans(self,start_logits:List[int],end_logits:List[int],max_answer_length:int,top_spans:int)->List[DPRSpanPrediction]
transformers.tokenization_dpr.CustomDPRReaderTokenizerMixin.decode_best_spans(self,reader_input:BatchEncoding,reader_output:DPRReaderOutput,num_spans:int=16,max_answer_length:int=64,num_spans_per_passage:int=4)->List[DPRSpanPrediction]
transformers.tokenization_dpr.DPRContextEncoderTokenizer(BertTokenizer)
transformers.tokenization_dpr.DPRContextEncoderTokenizerFast(BertTokenizerFast)
transformers.tokenization_dpr.DPRQuestionEncoderTokenizer(BertTokenizer)
transformers.tokenization_dpr.DPRQuestionEncoderTokenizerFast(BertTokenizerFast)
transformers.tokenization_dpr.DPRReaderTokenizer(CustomDPRReaderTokenizerMixin,BertTokenizer)
transformers.tokenization_dpr.DPRReaderTokenizerFast(CustomDPRReaderTokenizerMixin,BertTokenizerFast)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/modelcard.py----------------------------------------
A:transformers.modelcard.logger->utils.logging.get_logger(__name__)
A:transformers.modelcard.self.model_details->kwargs.pop('model_details', {})
A:transformers.modelcard.self.intended_use->kwargs.pop('intended_use', {})
A:transformers.modelcard.self.factors->kwargs.pop('factors', {})
A:transformers.modelcard.self.metrics->kwargs.pop('metrics', {})
A:transformers.modelcard.self.evaluation_data->kwargs.pop('evaluation_data', {})
A:transformers.modelcard.self.training_data->kwargs.pop('training_data', {})
A:transformers.modelcard.self.quantitative_analyses->kwargs.pop('quantitative_analyses', {})
A:transformers.modelcard.self.ethical_considerations->kwargs.pop('ethical_considerations', {})
A:transformers.modelcard.self.caveats_and_recommendations->kwargs.pop('caveats_and_recommendations', {})
A:transformers.modelcard.output_model_card_file->os.path.join(save_directory_or_file, MODEL_CARD_NAME)
A:transformers.modelcard.cache_dir->kwargs.pop('cache_dir', None)
A:transformers.modelcard.proxies->kwargs.pop('proxies', None)
A:transformers.modelcard.find_from_standard_name->kwargs.pop('find_from_standard_name', True)
A:transformers.modelcard.return_unused_kwargs->kwargs.pop('return_unused_kwargs', False)
A:transformers.modelcard.model_card_file->model_card_file.replace(TF2_WEIGHTS_NAME, MODEL_CARD_NAME).replace(TF2_WEIGHTS_NAME, MODEL_CARD_NAME)
A:transformers.modelcard.resolved_model_card_file->cached_path(model_card_file, cache_dir=cache_dir, proxies=proxies)
A:transformers.modelcard.modelcard->cls()
A:transformers.modelcard.text->reader.read()
A:transformers.modelcard.dict_obj->json.loads(text)
A:transformers.modelcard.output->copy.deepcopy(self.__dict__)
transformers.ModelCard(self,**kwargs)
transformers.ModelCard.__eq__(self,other)
transformers.ModelCard.__repr__(self)
transformers.ModelCard.from_dict(cls,json_object)
transformers.ModelCard.from_json_file(cls,json_file)
transformers.ModelCard.from_pretrained(cls,pretrained_model_name_or_path,**kwargs)
transformers.ModelCard.save_pretrained(self,save_directory_or_file)
transformers.ModelCard.to_dict(self)
transformers.ModelCard.to_json_file(self,json_file_path)
transformers.ModelCard.to_json_string(self)
transformers.modelcard.ModelCard(self,**kwargs)
transformers.modelcard.ModelCard.__eq__(self,other)
transformers.modelcard.ModelCard.__init__(self,**kwargs)
transformers.modelcard.ModelCard.__repr__(self)
transformers.modelcard.ModelCard.from_dict(cls,json_object)
transformers.modelcard.ModelCard.from_json_file(cls,json_file)
transformers.modelcard.ModelCard.from_pretrained(cls,pretrained_model_name_or_path,**kwargs)
transformers.modelcard.ModelCard.save_pretrained(self,save_directory_or_file)
transformers.modelcard.ModelCard.to_dict(self)
transformers.modelcard.ModelCard.to_json_file(self,json_file_path)
transformers.modelcard.ModelCard.to_json_string(self)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/convert_mbart_original_checkpoint_to_pytorch.py----------------------------------------
A:transformers.convert_mbart_original_checkpoint_to_pytorch.mbart_config->transformers.MBartConfig.from_pretrained(hf_config_path, vocab_size=vocab_size)
A:transformers.convert_mbart_original_checkpoint_to_pytorch.model->convert_fairseq_mbart_checkpoint_from_disk(args.fairseq_path, hf_config_path=args.hf_config)
A:transformers.convert_mbart_original_checkpoint_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.convert_mbart_original_checkpoint_to_pytorch.args->argparse.ArgumentParser().parse_args()
transformers.convert_mbart_original_checkpoint_to_pytorch.convert_fairseq_mbart_checkpoint_from_disk(checkpoint_path,hf_config_path='facebook/mbart-large-en-ro')


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/convert_electra_original_tf_checkpoint_to_pytorch.py----------------------------------------
A:transformers.convert_electra_original_tf_checkpoint_to_pytorch.config->transformers.ElectraConfig.from_json_file(config_file)
A:transformers.convert_electra_original_tf_checkpoint_to_pytorch.model->ElectraForMaskedLM(config)
A:transformers.convert_electra_original_tf_checkpoint_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.convert_electra_original_tf_checkpoint_to_pytorch.args->argparse.ArgumentParser().parse_args()
transformers.convert_electra_original_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(tf_checkpoint_path,config_file,pytorch_dump_path,discriminator_or_generator)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/modeling_roberta.py----------------------------------------
A:transformers.modeling_roberta.logger->utils.logging.get_logger(__name__)
A:transformers.modeling_roberta.self.word_embeddings->torch.nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=self.padding_idx)
A:transformers.modeling_roberta.self.position_embeddings->torch.nn.Embedding(config.max_position_embeddings, config.hidden_size, padding_idx=self.padding_idx)
A:transformers.modeling_roberta.position_ids->torch.arange(self.padding_idx + 1, sequence_length + self.padding_idx + 1, dtype=torch.long, device=inputs_embeds.device)
A:transformers.modeling_roberta.self.embeddings->RobertaEmbeddings(config)
A:transformers.modeling_roberta.self.roberta->RobertaModel(config)
A:transformers.modeling_roberta.self.lm_head->RobertaLMHead(config)
A:transformers.modeling_roberta.outputs->self.roberta(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.modeling_roberta.prediction_scores->self.lm_head(sequence_output)
A:transformers.modeling_roberta.shifted_prediction_scores->prediction_scores[:, :-1, :].contiguous()
A:transformers.modeling_roberta.labels->kwargs.pop('masked_lm_labels')
A:transformers.modeling_roberta.loss_fct->CrossEntropyLoss(ignore_index=ignored_index)
A:transformers.modeling_roberta.lm_loss->loss_fct(shifted_prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))
A:transformers.modeling_roberta.attention_mask->input_ids.new_ones(input_shape)
A:transformers.modeling_roberta.masked_lm_loss->loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))
A:transformers.modeling_roberta.self.dense->torch.nn.Linear(config.hidden_size, config.hidden_size)
A:transformers.modeling_roberta.self.layer_norm->BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)
A:transformers.modeling_roberta.self.decoder->torch.nn.Linear(config.hidden_size, config.vocab_size, bias=False)
A:transformers.modeling_roberta.self.bias->torch.nn.Parameter(torch.zeros(config.vocab_size))
A:transformers.modeling_roberta.x->self.out_proj(x)
A:transformers.modeling_roberta.self.classifier->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.modeling_roberta.logits->self.qa_outputs(sequence_output)
A:transformers.modeling_roberta.loss->loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
A:transformers.modeling_roberta.self.dropout->torch.nn.Dropout(config.hidden_dropout_prob)
A:transformers.modeling_roberta.pooled_output->self.dropout(pooled_output)
A:transformers.modeling_roberta.reshaped_logits->self.qa_outputs(sequence_output).view(-1, num_choices)
A:transformers.modeling_roberta.sequence_output->self.dropout(sequence_output)
A:transformers.modeling_roberta.active_logits->self.qa_outputs(sequence_output).view(-1, self.num_labels)
A:transformers.modeling_roberta.active_labels->torch.where(active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels))
A:transformers.modeling_roberta.self.out_proj->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.modeling_roberta.self.qa_outputs->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.modeling_roberta.(start_logits, end_logits)->self.qa_outputs(sequence_output).split(1, dim=-1)
A:transformers.modeling_roberta.start_logits->start_logits.squeeze(-1).squeeze(-1)
A:transformers.modeling_roberta.end_logits->end_logits.squeeze(-1).squeeze(-1)
A:transformers.modeling_roberta.start_positions->start_positions.squeeze(-1).squeeze(-1)
A:transformers.modeling_roberta.end_positions->end_positions.squeeze(-1).squeeze(-1)
A:transformers.modeling_roberta.ignored_index->start_logits.squeeze(-1).squeeze(-1).size(1)
A:transformers.modeling_roberta.start_loss->loss_fct(start_logits, start_positions)
A:transformers.modeling_roberta.end_loss->loss_fct(end_logits, end_positions)
A:transformers.modeling_roberta.mask->input_ids.ne(padding_idx).int()
transformers.RobertaForCausalLM(self,config)
transformers.RobertaForCausalLM.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.RobertaForCausalLM.get_output_embeddings(self)
transformers.RobertaForCausalLM.prepare_inputs_for_generation(self,input_ids,attention_mask=None,**model_kwargs)
transformers.RobertaForMaskedLM(self,config)
transformers.RobertaForMaskedLM.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.RobertaForMaskedLM.get_output_embeddings(self)
transformers.RobertaForMultipleChoice(self,config)
transformers.RobertaForMultipleChoice.forward(self,input_ids=None,token_type_ids=None,attention_mask=None,labels=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.RobertaForQuestionAnswering(self,config)
transformers.RobertaForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,start_positions=None,end_positions=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.RobertaForSequenceClassification(self,config)
transformers.RobertaForSequenceClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.RobertaForTokenClassification(self,config)
transformers.RobertaForTokenClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.RobertaModel(self,config)
transformers.RobertaModel.get_input_embeddings(self)
transformers.RobertaModel.set_input_embeddings(self,value)
transformers.modeling_roberta.RobertaClassificationHead(self,config)
transformers.modeling_roberta.RobertaClassificationHead.__init__(self,config)
transformers.modeling_roberta.RobertaClassificationHead.forward(self,features,**kwargs)
transformers.modeling_roberta.RobertaEmbeddings(self,config)
transformers.modeling_roberta.RobertaEmbeddings.__init__(self,config)
transformers.modeling_roberta.RobertaEmbeddings.create_position_ids_from_inputs_embeds(self,inputs_embeds)
transformers.modeling_roberta.RobertaEmbeddings.forward(self,input_ids=None,token_type_ids=None,position_ids=None,inputs_embeds=None)
transformers.modeling_roberta.RobertaForCausalLM(self,config)
transformers.modeling_roberta.RobertaForCausalLM.__init__(self,config)
transformers.modeling_roberta.RobertaForCausalLM.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.modeling_roberta.RobertaForCausalLM.get_output_embeddings(self)
transformers.modeling_roberta.RobertaForCausalLM.prepare_inputs_for_generation(self,input_ids,attention_mask=None,**model_kwargs)
transformers.modeling_roberta.RobertaForMaskedLM(self,config)
transformers.modeling_roberta.RobertaForMaskedLM.__init__(self,config)
transformers.modeling_roberta.RobertaForMaskedLM.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.modeling_roberta.RobertaForMaskedLM.get_output_embeddings(self)
transformers.modeling_roberta.RobertaForMultipleChoice(self,config)
transformers.modeling_roberta.RobertaForMultipleChoice.__init__(self,config)
transformers.modeling_roberta.RobertaForMultipleChoice.forward(self,input_ids=None,token_type_ids=None,attention_mask=None,labels=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.modeling_roberta.RobertaForQuestionAnswering(self,config)
transformers.modeling_roberta.RobertaForQuestionAnswering.__init__(self,config)
transformers.modeling_roberta.RobertaForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,start_positions=None,end_positions=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.modeling_roberta.RobertaForSequenceClassification(self,config)
transformers.modeling_roberta.RobertaForSequenceClassification.__init__(self,config)
transformers.modeling_roberta.RobertaForSequenceClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.modeling_roberta.RobertaForTokenClassification(self,config)
transformers.modeling_roberta.RobertaForTokenClassification.__init__(self,config)
transformers.modeling_roberta.RobertaForTokenClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.modeling_roberta.RobertaLMHead(self,config)
transformers.modeling_roberta.RobertaLMHead.__init__(self,config)
transformers.modeling_roberta.RobertaLMHead.forward(self,features,**kwargs)
transformers.modeling_roberta.RobertaModel(self,config)
transformers.modeling_roberta.RobertaModel.__init__(self,config)
transformers.modeling_roberta.RobertaModel.get_input_embeddings(self)
transformers.modeling_roberta.RobertaModel.set_input_embeddings(self,value)
transformers.modeling_roberta.create_position_ids_from_input_ids(input_ids,padding_idx)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/modeling_tf_albert.py----------------------------------------
A:transformers.modeling_tf_albert.logger->utils.logging.get_logger(__name__)
A:transformers.modeling_tf_albert.self.position_embeddings->tensorflow.keras.layers.Embedding(config.max_position_embeddings, config.embedding_size, embeddings_initializer=get_initializer(self.config.initializer_range), name='position_embeddings')
A:transformers.modeling_tf_albert.self.token_type_embeddings->tensorflow.keras.layers.Embedding(config.type_vocab_size, config.embedding_size, embeddings_initializer=get_initializer(self.config.initializer_range), name='token_type_embeddings')
A:transformers.modeling_tf_albert.self.LayerNorm->tensorflow.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name='LayerNorm')
A:transformers.modeling_tf_albert.self.dropout->tensorflow.keras.layers.Dropout(config.hidden_dropout_prob)
A:transformers.modeling_tf_albert.self.word_embeddings->self.add_weight('weight', shape=[self.config.vocab_size, self.config.embedding_size], initializer=get_initializer(self.config.initializer_range))
A:transformers.modeling_tf_albert.input_shape->shape_list(input_ids)
A:transformers.modeling_tf_albert.token_type_ids->inputs.get('token_type_ids', token_type_ids)
A:transformers.modeling_tf_albert.inputs_embeds->inputs.get('inputs_embeds', inputs_embeds)
A:transformers.modeling_tf_albert.position_embeddings->self.position_embeddings(position_ids)
A:transformers.modeling_tf_albert.token_type_embeddings->self.token_type_embeddings(token_type_ids)
A:transformers.modeling_tf_albert.embeddings->self.dropout(embeddings, training=training)
A:transformers.modeling_tf_albert.x->tensorflow.reshape(x, (batch_size, -1, self.num_attention_heads, self.attention_head_size))
A:transformers.modeling_tf_albert.logits->self.classifier(pooled_output)
A:transformers.modeling_tf_albert.self.attention_head_size->int(config.hidden_size / config.num_attention_heads)
A:transformers.modeling_tf_albert.self.query->tensorflow.keras.layers.Dense(self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name='query')
A:transformers.modeling_tf_albert.self.key->tensorflow.keras.layers.Dense(self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name='key')
A:transformers.modeling_tf_albert.self.value->tensorflow.keras.layers.Dense(self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name='value')
A:transformers.modeling_tf_albert.mixed_query_layer->self.query(input_tensor)
A:transformers.modeling_tf_albert.mixed_key_layer->self.key(input_tensor)
A:transformers.modeling_tf_albert.mixed_value_layer->self.value(input_tensor)
A:transformers.modeling_tf_albert.query_layer->self.transpose_for_scores(mixed_query_layer, batch_size)
A:transformers.modeling_tf_albert.key_layer->self.transpose_for_scores(mixed_key_layer, batch_size)
A:transformers.modeling_tf_albert.value_layer->self.transpose_for_scores(mixed_value_layer, batch_size)
A:transformers.modeling_tf_albert.attention_scores->tensorflow.matmul(query_layer, key_layer, transpose_b=True)
A:transformers.modeling_tf_albert.dk->tensorflow.cast(shape_list(key_layer)[-1], tf.float32)
A:transformers.modeling_tf_albert.attention_probs->self.attention_dropout(attention_probs, training=training)
A:transformers.modeling_tf_albert.context_layer->tensorflow.reshape(context_layer, (batch_size, -1, self.all_head_size))
A:transformers.modeling_tf_albert.self.dense->tensorflow.keras.layers.Dense(config.embedding_size, kernel_initializer=get_initializer(config.initializer_range), name='dense')
A:transformers.modeling_tf_albert.hidden_states->self.LayerNorm(hidden_states)
A:transformers.modeling_tf_albert.self.pruned_heads->set()
A:transformers.modeling_tf_albert.self.attention_dropout->tensorflow.keras.layers.Dropout(config.attention_probs_dropout_prob)
A:transformers.modeling_tf_albert.self.output_dropout->tensorflow.keras.layers.Dropout(config.hidden_dropout_prob)
A:transformers.modeling_tf_albert.attention_output->self.LayerNorm(hidden_states + input_tensor)
A:transformers.modeling_tf_albert.self.attention->TFAlbertAttention(config, name='attention')
A:transformers.modeling_tf_albert.self.ffn->tensorflow.keras.layers.Dense(config.intermediate_size, kernel_initializer=get_initializer(config.initializer_range), name='ffn')
A:transformers.modeling_tf_albert.self.ffn_output->tensorflow.keras.layers.Dense(config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name='ffn_output')
A:transformers.modeling_tf_albert.self.full_layer_layer_norm->tensorflow.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name='full_layer_layer_norm')
A:transformers.modeling_tf_albert.attention_outputs->self.attention(hidden_states, attention_mask, head_mask, output_attentions, training=training)
A:transformers.modeling_tf_albert.ffn_output->self.dropout(ffn_output, training=training)
A:transformers.modeling_tf_albert.layer_output->albert_layer(hidden_states, attention_mask, head_mask[layer_index], output_attentions, training=training)
A:transformers.modeling_tf_albert.self.embedding_hidden_mapping_in->tensorflow.keras.layers.Dense(config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name='embedding_hidden_mapping_in')
A:transformers.modeling_tf_albert.layers_per_group->int(self.config.num_hidden_layers / self.config.num_hidden_groups)
A:transformers.modeling_tf_albert.group_idx->int(i / (self.config.num_hidden_layers / self.config.num_hidden_groups))
A:transformers.modeling_tf_albert.layer_group_output->self.albert_layer_groups[group_idx](hidden_states, attention_mask, head_mask[group_idx * layers_per_group:(group_idx + 1) * layers_per_group], output_attentions, output_hidden_states, training=training)
A:transformers.modeling_tf_albert.self.bias->self.add_weight(shape=(self.vocab_size,), initializer='zeros', trainable=True, name='bias')
A:transformers.modeling_tf_albert.self.decoder_bias->self.add_weight(shape=(self.vocab_size,), initializer='zeros', trainable=True, name='decoder/bias')
A:transformers.modeling_tf_albert.self.embeddings->TFAlbertEmbeddings(config, name='embeddings')
A:transformers.modeling_tf_albert.self.encoder->TFAlbertTransformer(config, name='encoder')
A:transformers.modeling_tf_albert.self.pooler->tensorflow.keras.layers.Dense(config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), activation='tanh', name='pooler')
A:transformers.modeling_tf_albert.input_ids->inputs.get('input_ids')
A:transformers.modeling_tf_albert.attention_mask->inputs.get('attention_mask', attention_mask)
A:transformers.modeling_tf_albert.position_ids->inputs.get('position_ids', position_ids)
A:transformers.modeling_tf_albert.head_mask->inputs.get('head_mask', head_mask)
A:transformers.modeling_tf_albert.output_attentions->inputs.get('output_attentions', output_attentions)
A:transformers.modeling_tf_albert.output_hidden_states->inputs.get('output_hidden_states', output_hidden_states)
A:transformers.modeling_tf_albert.return_dict->inputs.get('return_dict', return_dict)
A:transformers.modeling_tf_albert.extended_attention_mask->tensorflow.cast(extended_attention_mask, tf.float32)
A:transformers.modeling_tf_albert.embedding_output->self.embeddings(input_ids, position_ids, token_type_ids, inputs_embeds, training=training)
A:transformers.modeling_tf_albert.encoder_outputs->self.encoder(embedding_output, extended_attention_mask, head_mask, output_attentions, output_hidden_states, return_dict, training=training)
A:transformers.modeling_tf_albert.pooled_output->self.dropout(pooled_output, training=training)
A:transformers.modeling_tf_albert.self.albert->TFAlbertMainLayer(config, name='albert')
A:transformers.modeling_tf_albert.outputs->self.albert(flat_input_ids, flat_attention_mask, flat_token_type_ids, flat_position_ids, head_mask, flat_inputs_embeds, output_attentions, output_hidden_states, return_dict=return_dict, training=training)
A:transformers.modeling_tf_albert.self.predictions->TFAlbertMLMHead(config, self.albert.embeddings, name='predictions')
A:transformers.modeling_tf_albert.self.sop_classifier->TFAlbertSOPHead(config, name='sop_classifier')
A:transformers.modeling_tf_albert.prediction_scores->self.predictions(sequence_output, training=training)
A:transformers.modeling_tf_albert.sop_scores->self.sop_classifier(pooled_output, training=kwargs.get('training', False))
A:transformers.modeling_tf_albert.self.classifier->tensorflow.keras.layers.Dense(1, kernel_initializer=get_initializer(config.initializer_range), name='classifier')
A:transformers.modeling_tf_albert.dropout_pooled_output->self.dropout(pooled_output, training=training)
A:transformers.modeling_tf_albert.labels->inputs.get('labels', labels)
A:transformers.modeling_tf_albert.sequence_output->self.dropout(sequence_output, training=training)
A:transformers.modeling_tf_albert.self.qa_outputs->tensorflow.keras.layers.Dense(config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name='qa_outputs')
A:transformers.modeling_tf_albert.start_positions->inputs.pop('start_positions', start_positions)
A:transformers.modeling_tf_albert.end_positions->inputs.pop('end_positions', start_positions)
A:transformers.modeling_tf_albert.(start_logits, end_logits)->tensorflow.split(logits, 2, axis=-1)
A:transformers.modeling_tf_albert.start_logits->tensorflow.squeeze(start_logits, axis=-1)
A:transformers.modeling_tf_albert.end_logits->tensorflow.squeeze(end_logits, axis=-1)
A:transformers.modeling_tf_albert.loss->self.compute_loss(labels, (start_logits, end_logits))
A:transformers.modeling_tf_albert.reshaped_logits->tensorflow.reshape(logits, (-1, num_choices))
transformers.TFAlbertForMaskedLM(self,config,*inputs,**kwargs)
transformers.TFAlbertForMaskedLM.call(self,inputs=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.TFAlbertForMaskedLM.get_output_embeddings(self)
transformers.TFAlbertForMultipleChoice(self,config,*inputs,**kwargs)
transformers.TFAlbertForMultipleChoice.call(self,inputs,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.TFAlbertForMultipleChoice.dummy_inputs(self)
transformers.TFAlbertForPreTraining(self,config,*inputs,**kwargs)
transformers.TFAlbertForPreTraining.call(self,inputs,**kwargs)
transformers.TFAlbertForPreTraining.get_output_embeddings(self)
transformers.TFAlbertForPreTrainingOutput(ModelOutput)
transformers.TFAlbertForQuestionAnswering(self,config,*inputs,**kwargs)
transformers.TFAlbertForQuestionAnswering.call(self,inputs=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,start_positions=None,end_positions=None,training=False)
transformers.TFAlbertForSequenceClassification(self,config,*inputs,**kwargs)
transformers.TFAlbertForSequenceClassification.call(self,inputs=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.TFAlbertForTokenClassification(self,config,*inputs,**kwargs)
transformers.TFAlbertForTokenClassification.call(self,inputs=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.TFAlbertMainLayer(self,config,**kwargs)
transformers.TFAlbertMainLayer._prune_heads(self,heads_to_prune)
transformers.TFAlbertMainLayer._resize_token_embeddings(self,new_num_tokens)
transformers.TFAlbertMainLayer.call(self,inputs,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False)
transformers.TFAlbertMainLayer.get_input_embeddings(self)
transformers.TFAlbertMainLayer.set_input_embeddings(self,value)
transformers.TFAlbertModel(self,config,*inputs,**kwargs)
transformers.TFAlbertModel.call(self,inputs,**kwargs)
transformers.TFAlbertPreTrainedModel(TFPreTrainedModel)
transformers.modeling_tf_albert.TFAlbertAttention(self,config,**kwargs)
transformers.modeling_tf_albert.TFAlbertAttention.__init__(self,config,**kwargs)
transformers.modeling_tf_albert.TFAlbertAttention.call(self,input_tensor,attention_mask,head_mask,output_attentions,training=False)
transformers.modeling_tf_albert.TFAlbertAttention.prune_heads(self,heads)
transformers.modeling_tf_albert.TFAlbertEmbeddings(self,config,**kwargs)
transformers.modeling_tf_albert.TFAlbertEmbeddings.__init__(self,config,**kwargs)
transformers.modeling_tf_albert.TFAlbertEmbeddings._embedding(self,input_ids,position_ids,token_type_ids,inputs_embeds,training=False)
transformers.modeling_tf_albert.TFAlbertEmbeddings._linear(self,inputs)
transformers.modeling_tf_albert.TFAlbertEmbeddings.build(self,input_shape)
transformers.modeling_tf_albert.TFAlbertEmbeddings.call(self,input_ids=None,position_ids=None,token_type_ids=None,inputs_embeds=None,mode='embedding',training=False)
transformers.modeling_tf_albert.TFAlbertForMaskedLM(self,config,*inputs,**kwargs)
transformers.modeling_tf_albert.TFAlbertForMaskedLM.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_albert.TFAlbertForMaskedLM.call(self,inputs=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.modeling_tf_albert.TFAlbertForMaskedLM.get_output_embeddings(self)
transformers.modeling_tf_albert.TFAlbertForMultipleChoice(self,config,*inputs,**kwargs)
transformers.modeling_tf_albert.TFAlbertForMultipleChoice.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_albert.TFAlbertForMultipleChoice.call(self,inputs,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.modeling_tf_albert.TFAlbertForMultipleChoice.dummy_inputs(self)
transformers.modeling_tf_albert.TFAlbertForPreTraining(self,config,*inputs,**kwargs)
transformers.modeling_tf_albert.TFAlbertForPreTraining.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_albert.TFAlbertForPreTraining.call(self,inputs,**kwargs)
transformers.modeling_tf_albert.TFAlbertForPreTraining.get_output_embeddings(self)
transformers.modeling_tf_albert.TFAlbertForPreTrainingOutput(ModelOutput)
transformers.modeling_tf_albert.TFAlbertForQuestionAnswering(self,config,*inputs,**kwargs)
transformers.modeling_tf_albert.TFAlbertForQuestionAnswering.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_albert.TFAlbertForQuestionAnswering.call(self,inputs=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,start_positions=None,end_positions=None,training=False)
transformers.modeling_tf_albert.TFAlbertForSequenceClassification(self,config,*inputs,**kwargs)
transformers.modeling_tf_albert.TFAlbertForSequenceClassification.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_albert.TFAlbertForSequenceClassification.call(self,inputs=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.modeling_tf_albert.TFAlbertForTokenClassification(self,config,*inputs,**kwargs)
transformers.modeling_tf_albert.TFAlbertForTokenClassification.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_albert.TFAlbertForTokenClassification.call(self,inputs=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.modeling_tf_albert.TFAlbertLayer(self,config,**kwargs)
transformers.modeling_tf_albert.TFAlbertLayer.__init__(self,config,**kwargs)
transformers.modeling_tf_albert.TFAlbertLayer.call(self,hidden_states,attention_mask,head_mask,output_attentions,training=False)
transformers.modeling_tf_albert.TFAlbertLayerGroup(self,config,**kwargs)
transformers.modeling_tf_albert.TFAlbertLayerGroup.__init__(self,config,**kwargs)
transformers.modeling_tf_albert.TFAlbertLayerGroup.call(self,hidden_states,attention_mask,head_mask,output_attentions,output_hidden_states,training=False)
transformers.modeling_tf_albert.TFAlbertMLMHead(self,config,input_embeddings,**kwargs)
transformers.modeling_tf_albert.TFAlbertMLMHead.__init__(self,config,input_embeddings,**kwargs)
transformers.modeling_tf_albert.TFAlbertMLMHead.build(self,input_shape)
transformers.modeling_tf_albert.TFAlbertMLMHead.call(self,hidden_states)
transformers.modeling_tf_albert.TFAlbertMainLayer(self,config,**kwargs)
transformers.modeling_tf_albert.TFAlbertMainLayer.__init__(self,config,**kwargs)
transformers.modeling_tf_albert.TFAlbertMainLayer._prune_heads(self,heads_to_prune)
transformers.modeling_tf_albert.TFAlbertMainLayer._resize_token_embeddings(self,new_num_tokens)
transformers.modeling_tf_albert.TFAlbertMainLayer.call(self,inputs,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False)
transformers.modeling_tf_albert.TFAlbertMainLayer.get_input_embeddings(self)
transformers.modeling_tf_albert.TFAlbertMainLayer.set_input_embeddings(self,value)
transformers.modeling_tf_albert.TFAlbertModel(self,config,*inputs,**kwargs)
transformers.modeling_tf_albert.TFAlbertModel.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_albert.TFAlbertModel.call(self,inputs,**kwargs)
transformers.modeling_tf_albert.TFAlbertPreTrainedModel(TFPreTrainedModel)
transformers.modeling_tf_albert.TFAlbertSOPHead(self,config,**kwargs)
transformers.modeling_tf_albert.TFAlbertSOPHead.__init__(self,config,**kwargs)
transformers.modeling_tf_albert.TFAlbertSOPHead.call(self,pooled_output,training:bool)
transformers.modeling_tf_albert.TFAlbertSelfAttention(self,config,**kwargs)
transformers.modeling_tf_albert.TFAlbertSelfAttention.__init__(self,config,**kwargs)
transformers.modeling_tf_albert.TFAlbertSelfAttention.call(self,hidden_states,attention_mask,head_mask,output_attentions,training=False)
transformers.modeling_tf_albert.TFAlbertSelfAttention.transpose_for_scores(self,x,batch_size)
transformers.modeling_tf_albert.TFAlbertSelfOutput(self,config,**kwargs)
transformers.modeling_tf_albert.TFAlbertSelfOutput.__init__(self,config,**kwargs)
transformers.modeling_tf_albert.TFAlbertSelfOutput.call(self,hidden_states,input_tensor,training=False)
transformers.modeling_tf_albert.TFAlbertTransformer(self,config,**kwargs)
transformers.modeling_tf_albert.TFAlbertTransformer.__init__(self,config,**kwargs)
transformers.modeling_tf_albert.TFAlbertTransformer.call(self,hidden_states,attention_mask,head_mask,output_attentions,output_hidden_states,return_dict,training=False)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/tokenization_xlm_roberta.py----------------------------------------
A:transformers.tokenization_xlm_roberta.logger->utils.logging.get_logger(__name__)
A:transformers.tokenization_xlm_roberta.self.sp_model->sentencepiece.SentencePieceProcessor()
A:transformers.tokenization_xlm_roberta.state->self.__dict__.copy()
A:transformers.tokenization_xlm_roberta.spm_id->self.sp_model.PieceToId(token)
A:transformers.tokenization_xlm_roberta.out_string->''.join(tokens).replace(SPIECE_UNDERLINE, ' ').strip()
A:transformers.tokenization_xlm_roberta.out_vocab_file->os.path.join(save_directory, VOCAB_FILES_NAMES['vocab_file'])
transformers.XLMRobertaTokenizer(self,vocab_file,bos_token='<s>',eos_token='</s>',sep_token='</s>',cls_token='<s>',unk_token='<unk>',pad_token='<pad>',mask_token='<mask>',**kwargs)
transformers.XLMRobertaTokenizer.__getstate__(self)
transformers.XLMRobertaTokenizer.__setstate__(self,d)
transformers.XLMRobertaTokenizer._convert_id_to_token(self,index)
transformers.XLMRobertaTokenizer._convert_token_to_id(self,token)
transformers.XLMRobertaTokenizer._tokenize(self,text)
transformers.XLMRobertaTokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.XLMRobertaTokenizer.convert_tokens_to_string(self,tokens)
transformers.XLMRobertaTokenizer.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.XLMRobertaTokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.XLMRobertaTokenizer.get_vocab(self)
transformers.XLMRobertaTokenizer.save_vocabulary(self,save_directory)
transformers.XLMRobertaTokenizer.vocab_size(self)
transformers.tokenization_xlm_roberta.XLMRobertaTokenizer(self,vocab_file,bos_token='<s>',eos_token='</s>',sep_token='</s>',cls_token='<s>',unk_token='<unk>',pad_token='<pad>',mask_token='<mask>',**kwargs)
transformers.tokenization_xlm_roberta.XLMRobertaTokenizer.__getstate__(self)
transformers.tokenization_xlm_roberta.XLMRobertaTokenizer.__init__(self,vocab_file,bos_token='<s>',eos_token='</s>',sep_token='</s>',cls_token='<s>',unk_token='<unk>',pad_token='<pad>',mask_token='<mask>',**kwargs)
transformers.tokenization_xlm_roberta.XLMRobertaTokenizer.__setstate__(self,d)
transformers.tokenization_xlm_roberta.XLMRobertaTokenizer._convert_id_to_token(self,index)
transformers.tokenization_xlm_roberta.XLMRobertaTokenizer._convert_token_to_id(self,token)
transformers.tokenization_xlm_roberta.XLMRobertaTokenizer._tokenize(self,text)
transformers.tokenization_xlm_roberta.XLMRobertaTokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.tokenization_xlm_roberta.XLMRobertaTokenizer.convert_tokens_to_string(self,tokens)
transformers.tokenization_xlm_roberta.XLMRobertaTokenizer.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.tokenization_xlm_roberta.XLMRobertaTokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.tokenization_xlm_roberta.XLMRobertaTokenizer.get_vocab(self)
transformers.tokenization_xlm_roberta.XLMRobertaTokenizer.save_vocabulary(self,save_directory)
transformers.tokenization_xlm_roberta.XLMRobertaTokenizer.vocab_size(self)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/convert_pegasus_tf_to_pytorch.py----------------------------------------
A:transformers.convert_pegasus_tf_to_pytorch.k->k.replace(pegasus_name, bart_name).replace(pegasus_name, bart_name)
A:transformers.convert_pegasus_tf_to_pytorch.cfg_kwargs->transformers.configuration_pegasus.DEFAULTS.copy()
A:transformers.convert_pegasus_tf_to_pytorch.cfg->PegasusConfig(**cfg_updates)
A:transformers.convert_pegasus_tf_to_pytorch.bart->PegasusForConditionalGeneration(cfg)
A:transformers.convert_pegasus_tf_to_pytorch.sd->convert_pegasus_to_bart(tf_weights, cfg_updates).state_dict()
A:transformers.convert_pegasus_tf_to_pytorch.new_k->rename_state_dict_key(k)
A:transformers.convert_pegasus_tf_to_pytorch.mapping[new_k]->torch.tensor(v, dtype=sd[new_k].dtype)
A:transformers.convert_pegasus_tf_to_pytorch.mapping['shared.weight'][cfg.pad_token_id]->torch.zeros_like(mapping['shared.weight'][cfg.pad_token_id + 1])
A:transformers.convert_pegasus_tf_to_pytorch.(missing, extra)->PegasusForConditionalGeneration(cfg).model.load_state_dict(mapping, strict=False)
A:transformers.convert_pegasus_tf_to_pytorch.init_vars->tensorflow.train.list_variables(path)
A:transformers.convert_pegasus_tf_to_pytorch.skip_key->any([pat in name for pat in ignore_name])
A:transformers.convert_pegasus_tf_to_pytorch.array->tensorflow.train.load_variable(path, name)
A:transformers.convert_pegasus_tf_to_pytorch.tok->transformers.PegasusTokenizer.from_pretrained('sshleifer/pegasus', model_max_length=desired_max_model_length)
A:transformers.convert_pegasus_tf_to_pytorch.tf_weights->get_tf_weights_as_numpy(ckpt_path)
A:transformers.convert_pegasus_tf_to_pytorch.torch_model->convert_pegasus_to_bart(tf_weights, cfg_updates)
A:transformers.convert_pegasus_tf_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.convert_pegasus_tf_to_pytorch.args->argparse.ArgumentParser().parse_args()
A:transformers.convert_pegasus_tf_to_pytorch.args.save_dir->os.path.join('pegasus', dataset)
transformers.convert_pegasus_tf_to_pytorch.convert_pegasus_ckpt_to_pytorch(ckpt_path:str,save_dir:str)
transformers.convert_pegasus_tf_to_pytorch.convert_pegasus_to_bart(tf_weights:dict,cfg_updates:dict)->PegasusForConditionalGeneration
transformers.convert_pegasus_tf_to_pytorch.get_tf_weights_as_numpy(path='./ckpt/aeslc/model.ckpt-32000')->Dict
transformers.convert_pegasus_tf_to_pytorch.rename_state_dict_key(k)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/convert_longformer_original_pytorch_lightning_to_pytorch.py----------------------------------------
A:transformers.convert_longformer_original_pytorch_lightning_to_pytorch.self.qa_outputs->torch.nn.Linear(self.model.config.hidden_size, self.num_labels)
A:transformers.convert_longformer_original_pytorch_lightning_to_pytorch.longformer->transformers.modeling_longformer.LongformerModel.from_pretrained(longformer_model)
A:transformers.convert_longformer_original_pytorch_lightning_to_pytorch.lightning_model->LightningModel(longformer)
A:transformers.convert_longformer_original_pytorch_lightning_to_pytorch.ckpt->torch.load(longformer_question_answering_ckpt_path, map_location=torch.device('cpu'))
A:transformers.convert_longformer_original_pytorch_lightning_to_pytorch.longformer_for_qa->transformers.modeling_longformer.LongformerForQuestionAnswering.from_pretrained(longformer_model)
A:transformers.convert_longformer_original_pytorch_lightning_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.convert_longformer_original_pytorch_lightning_to_pytorch.args->argparse.ArgumentParser().parse_args()
transformers.convert_longformer_original_pytorch_lightning_to_pytorch.LightningModel(self,model)
transformers.convert_longformer_original_pytorch_lightning_to_pytorch.LightningModel.__init__(self,model)
transformers.convert_longformer_original_pytorch_lightning_to_pytorch.LightningModel.forward(self)
transformers.convert_longformer_original_pytorch_lightning_to_pytorch.convert_longformer_qa_checkpoint_to_pytorch(longformer_model:str,longformer_question_answering_ckpt_path:str,pytorch_dump_folder_path:str)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/modeling_tf_xlm_roberta.py----------------------------------------
A:transformers.modeling_tf_xlm_roberta.logger->utils.logging.get_logger(__name__)
transformers.TFXLMRobertaForMaskedLM(TFRobertaForMaskedLM)
transformers.TFXLMRobertaForMultipleChoice(TFRobertaForMultipleChoice)
transformers.TFXLMRobertaForQuestionAnswering(TFRobertaForQuestionAnswering)
transformers.TFXLMRobertaForSequenceClassification(TFRobertaForSequenceClassification)
transformers.TFXLMRobertaForTokenClassification(TFRobertaForTokenClassification)
transformers.TFXLMRobertaModel(TFRobertaModel)
transformers.modeling_tf_xlm_roberta.TFXLMRobertaForMaskedLM(TFRobertaForMaskedLM)
transformers.modeling_tf_xlm_roberta.TFXLMRobertaForMultipleChoice(TFRobertaForMultipleChoice)
transformers.modeling_tf_xlm_roberta.TFXLMRobertaForQuestionAnswering(TFRobertaForQuestionAnswering)
transformers.modeling_tf_xlm_roberta.TFXLMRobertaForSequenceClassification(TFRobertaForSequenceClassification)
transformers.modeling_tf_xlm_roberta.TFXLMRobertaForTokenClassification(TFRobertaForTokenClassification)
transformers.modeling_tf_xlm_roberta.TFXLMRobertaModel(TFRobertaModel)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/modeling_mbart.py----------------------------------------
transformers.MBartForConditionalGeneration(BartForConditionalGeneration)
transformers.modeling_mbart.MBartForConditionalGeneration(BartForConditionalGeneration)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/configuration_electra.py----------------------------------------
A:transformers.configuration_electra.logger->utils.logging.get_logger(__name__)
transformers.ElectraConfig(self,vocab_size=30522,embedding_size=128,hidden_size=256,num_hidden_layers=12,num_attention_heads=4,intermediate_size=1024,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=512,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,summary_type='first',summary_use_proj=True,summary_activation='gelu',summary_last_dropout=0.1,pad_token_id=0,**kwargs)
transformers.configuration_electra.ElectraConfig(self,vocab_size=30522,embedding_size=128,hidden_size=256,num_hidden_layers=12,num_attention_heads=4,intermediate_size=1024,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=512,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,summary_type='first',summary_use_proj=True,summary_activation='gelu',summary_last_dropout=0.1,pad_token_id=0,**kwargs)
transformers.configuration_electra.ElectraConfig.__init__(self,vocab_size=30522,embedding_size=128,hidden_size=256,num_hidden_layers=12,num_attention_heads=4,intermediate_size=1024,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=512,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,summary_type='first',summary_use_proj=True,summary_activation='gelu',summary_last_dropout=0.1,pad_token_id=0,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/tokenization_marian.py----------------------------------------
A:transformers.tokenization_marian.language_code_re->re.compile('>>.+<<')
A:transformers.tokenization_marian.self.encoder->load_json(vocab)
A:transformers.tokenization_marian.self.spm_source->load_spm(source_spm)
A:transformers.tokenization_marian.self.spm_target->load_spm(target_spm)
A:transformers.tokenization_marian.match->self.language_code_re.match(text)
A:transformers.tokenization_marian.(code, text)->self.remove_language_code(text)
A:transformers.tokenization_marian.pieces->self.current_spm.EncodeAsPieces(text)
A:transformers.tokenization_marian.tokenizer_kwargs->dict(add_special_tokens=True, return_tensors=return_tensors, max_length=max_length, truncation=truncation, padding=padding)
A:transformers.tokenization_marian.save_dir->Path(save_directory)
A:transformers.tokenization_marian.vocab->self.encoder.copy()
A:transformers.tokenization_marian.state->self.__dict__.copy()
A:transformers.tokenization_marian.all_special_ids->set(self.all_special_ids)
A:transformers.tokenization_marian.spm->sentencepiece.SentencePieceProcessor()
transformers.MarianTokenizer(self,vocab,source_spm,target_spm,source_lang=None,target_lang=None,unk_token='<unk>',eos_token='</s>',pad_token='<pad>',model_max_length=512,**kwargs)
transformers.MarianTokenizer.__getstate__(self)->Dict
transformers.MarianTokenizer.__setstate__(self,d:Dict)->None
transformers.MarianTokenizer._convert_id_to_token(self,index:int)->str
transformers.MarianTokenizer._convert_token_to_id(self,token)
transformers.MarianTokenizer._setup_normalizer(self)
transformers.MarianTokenizer._special_token_mask(self,seq)
transformers.MarianTokenizer._tokenize(self,text:str)->List[str]
transformers.MarianTokenizer.build_inputs_with_special_tokens(self,token_ids_0,token_ids_1=None)->List[int]
transformers.MarianTokenizer.convert_tokens_to_string(self,tokens:List[str])->str
transformers.MarianTokenizer.get_special_tokens_mask(self,token_ids_0:List,token_ids_1:Optional[List]=None,already_has_special_tokens:bool=False)->List[int]
transformers.MarianTokenizer.get_vocab(self)->Dict
transformers.MarianTokenizer.normalize(self,x:str)->str
transformers.MarianTokenizer.num_special_tokens_to_add(self,**unused)
transformers.MarianTokenizer.prepare_seq2seq_batch(self,src_texts:List[str],tgt_texts:Optional[List[str]]=None,max_length:Optional[int]=None,max_target_length:Optional[int]=None,return_tensors:str='pt',truncation=True,padding='longest',**unused)->BatchEncoding
transformers.MarianTokenizer.remove_language_code(self,text:str)
transformers.MarianTokenizer.save_vocabulary(self,save_directory:str)->Tuple[str]
transformers.MarianTokenizer.vocab_size(self)->int
transformers.tokenization_marian.MarianTokenizer(self,vocab,source_spm,target_spm,source_lang=None,target_lang=None,unk_token='<unk>',eos_token='</s>',pad_token='<pad>',model_max_length=512,**kwargs)
transformers.tokenization_marian.MarianTokenizer.__getstate__(self)->Dict
transformers.tokenization_marian.MarianTokenizer.__init__(self,vocab,source_spm,target_spm,source_lang=None,target_lang=None,unk_token='<unk>',eos_token='</s>',pad_token='<pad>',model_max_length=512,**kwargs)
transformers.tokenization_marian.MarianTokenizer.__setstate__(self,d:Dict)->None
transformers.tokenization_marian.MarianTokenizer._convert_id_to_token(self,index:int)->str
transformers.tokenization_marian.MarianTokenizer._convert_token_to_id(self,token)
transformers.tokenization_marian.MarianTokenizer._setup_normalizer(self)
transformers.tokenization_marian.MarianTokenizer._special_token_mask(self,seq)
transformers.tokenization_marian.MarianTokenizer._tokenize(self,text:str)->List[str]
transformers.tokenization_marian.MarianTokenizer.build_inputs_with_special_tokens(self,token_ids_0,token_ids_1=None)->List[int]
transformers.tokenization_marian.MarianTokenizer.convert_tokens_to_string(self,tokens:List[str])->str
transformers.tokenization_marian.MarianTokenizer.get_special_tokens_mask(self,token_ids_0:List,token_ids_1:Optional[List]=None,already_has_special_tokens:bool=False)->List[int]
transformers.tokenization_marian.MarianTokenizer.get_vocab(self)->Dict
transformers.tokenization_marian.MarianTokenizer.normalize(self,x:str)->str
transformers.tokenization_marian.MarianTokenizer.num_special_tokens_to_add(self,**unused)
transformers.tokenization_marian.MarianTokenizer.prepare_seq2seq_batch(self,src_texts:List[str],tgt_texts:Optional[List[str]]=None,max_length:Optional[int]=None,max_target_length:Optional[int]=None,return_tensors:str='pt',truncation=True,padding='longest',**unused)->BatchEncoding
transformers.tokenization_marian.MarianTokenizer.remove_language_code(self,text:str)
transformers.tokenization_marian.MarianTokenizer.save_vocabulary(self,save_directory:str)->Tuple[str]
transformers.tokenization_marian.MarianTokenizer.vocab_size(self)->int
transformers.tokenization_marian.load_json(path:str)->Union[Dict, List]
transformers.tokenization_marian.load_spm(path:str)->sentencepiece.SentencePieceProcessor
transformers.tokenization_marian.save_json(data,path:str)->None


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/modeling_tf_flaubert.py----------------------------------------
A:transformers.modeling_tf_flaubert.logger->utils.logging.get_logger(__name__)
A:transformers.modeling_tf_flaubert.self.transformer->TFFlaubertMainLayer(config, name='transformer')
A:transformers.modeling_tf_flaubert.self.layerdrop->getattr(config, 'layerdrop', 0.0)
A:transformers.modeling_tf_flaubert.self.pre_norm->getattr(config, 'pre_norm', False)
A:transformers.modeling_tf_flaubert.input_ids->inputs.get('input_ids')
A:transformers.modeling_tf_flaubert.attention_mask->inputs.get('attention_mask', attention_mask)
A:transformers.modeling_tf_flaubert.langs->inputs.get('langs', langs)
A:transformers.modeling_tf_flaubert.token_type_ids->inputs.get('token_type_ids', token_type_ids)
A:transformers.modeling_tf_flaubert.position_ids->tensorflow.expand_dims(tf.range(slen), axis=0)
A:transformers.modeling_tf_flaubert.lengths->tensorflow.convert_to_tensor([slen] * bs, tf.int32)
A:transformers.modeling_tf_flaubert.cache->inputs.get('cache', cache)
A:transformers.modeling_tf_flaubert.head_mask->inputs.get('head_mask', head_mask)
A:transformers.modeling_tf_flaubert.inputs_embeds->self.embeddings(input_ids)
A:transformers.modeling_tf_flaubert.output_attentions->inputs.get('output_attentions', output_attentions)
A:transformers.modeling_tf_flaubert.output_hidden_states->inputs.get('output_hidden_states', output_hidden_states)
A:transformers.modeling_tf_flaubert.return_dict->inputs.get('return_dict', return_dict)
A:transformers.modeling_tf_flaubert.(bs, slen)->shape_list(input_ids)
A:transformers.modeling_tf_flaubert.(mask, attn_mask)->get_masks(slen, lengths, self.causal, padding_mask=attention_mask)
A:transformers.modeling_tf_flaubert.tensor->self.layer_norm2[i](tensor)
A:transformers.modeling_tf_flaubert.dropout_probability->random.uniform(0, 1)
A:transformers.modeling_tf_flaubert.attn_outputs->self.attentions[i](tensor_normalized, attn_mask, None, cache, head_mask[i], output_attentions, training=training)
A:transformers.modeling_tf_flaubert.attn->self.dropout(attn, training=training)
A:transformers.modeling_tf_flaubert.tensor_normalized->self.layer_norm2[i](tensor)
A:transformers.modeling_tf_flaubert.self.pred_layer->TFXLMPredLayer(config, self.transformer.embeddings, name='pred_layer_._proj')
transformers.TFFlaubertForMultipleChoice(self,config,*inputs,**kwargs)
transformers.TFFlaubertForQuestionAnsweringSimple(self,config,*inputs,**kwargs)
transformers.TFFlaubertForSequenceClassification(self,config,*inputs,**kwargs)
transformers.TFFlaubertForTokenClassification(self,config,*inputs,**kwargs)
transformers.TFFlaubertModel(self,config,*inputs,**kwargs)
transformers.TFFlaubertWithLMHeadModel(self,config,*inputs,**kwargs)
transformers.modeling_tf_flaubert.TFFlaubertForMultipleChoice(self,config,*inputs,**kwargs)
transformers.modeling_tf_flaubert.TFFlaubertForMultipleChoice.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_flaubert.TFFlaubertForQuestionAnsweringSimple(self,config,*inputs,**kwargs)
transformers.modeling_tf_flaubert.TFFlaubertForQuestionAnsweringSimple.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_flaubert.TFFlaubertForSequenceClassification(self,config,*inputs,**kwargs)
transformers.modeling_tf_flaubert.TFFlaubertForSequenceClassification.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_flaubert.TFFlaubertForTokenClassification(self,config,*inputs,**kwargs)
transformers.modeling_tf_flaubert.TFFlaubertForTokenClassification.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_flaubert.TFFlaubertMainLayer(self,config,*inputs,**kwargs)
transformers.modeling_tf_flaubert.TFFlaubertMainLayer.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_flaubert.TFFlaubertMainLayer.call(self,inputs,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False)
transformers.modeling_tf_flaubert.TFFlaubertModel(self,config,*inputs,**kwargs)
transformers.modeling_tf_flaubert.TFFlaubertModel.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_flaubert.TFFlaubertWithLMHeadModel(self,config,*inputs,**kwargs)
transformers.modeling_tf_flaubert.TFFlaubertWithLMHeadModel.__init__(self,config,*inputs,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/modeling_ctrl.py----------------------------------------
A:transformers.modeling_ctrl.logger->utils.logging.get_logger(__name__)
A:transformers.modeling_ctrl.angle_rads->angle_defn(torch.arange(position, dtype=dtype).unsqueeze(1), torch.arange(d_model_size, dtype=dtype).unsqueeze(0), d_model_size)
A:transformers.modeling_ctrl.sines->torch.sin(angle_rads[:, 0::2])
A:transformers.modeling_ctrl.cosines->torch.cos(angle_rads[:, 1::2])
A:transformers.modeling_ctrl.pos_encoding->torch.cat([sines, cosines], dim=-1)
A:transformers.modeling_ctrl.matmul_qk->torch.matmul(q, k.permute(0, 1, 3, 2))
A:transformers.modeling_ctrl.attention_weights->torch.softmax(scaled_attention_logits, dim=-1)
A:transformers.modeling_ctrl.output->self.dense(original_size_attention)
A:transformers.modeling_ctrl.self.depth->int(d_model_size / self.num_heads)
A:transformers.modeling_ctrl.self.Wq->prune_linear_layer(self.Wq, index)
A:transformers.modeling_ctrl.self.Wk->prune_linear_layer(self.Wk, index)
A:transformers.modeling_ctrl.self.Wv->prune_linear_layer(self.Wv, index)
A:transformers.modeling_ctrl.self.dense->prune_linear_layer(self.dense, index, dim=1)
A:transformers.modeling_ctrl.self.pruned_heads->self.pruned_heads.union(heads)
A:transformers.modeling_ctrl.(heads, index)->find_pruneable_heads_and_indices(heads, self.num_heads, attention_head_size, self.pruned_heads)
A:transformers.modeling_ctrl.x->x.reshape(batch_size, -1, self.num_heads, self.depth).reshape(batch_size, -1, self.num_heads, self.depth)
A:transformers.modeling_ctrl.q->self.split_into_heads(q, batch_size)
A:transformers.modeling_ctrl.k->torch.cat((past_key, k), dim=-2)
A:transformers.modeling_ctrl.v->torch.cat((past_value, v), dim=-2)
A:transformers.modeling_ctrl.present->torch.stack((k, v))
A:transformers.modeling_ctrl.scaled_attention->output[0].permute([0, 2, 1, 3])
A:transformers.modeling_ctrl.original_size_attention->output[0].permute([0, 2, 1, 3]).reshape(batch_size, -1, self.d_model_size)
A:transformers.modeling_ctrl.self.multi_head_attention->MultiHeadAttention(d_model_size, num_heads)
A:transformers.modeling_ctrl.self.ffn->point_wise_feed_forward_network(d_model_size, dff)
A:transformers.modeling_ctrl.self.layernorm1->torch.nn.LayerNorm(d_model_size, eps=1e-06)
A:transformers.modeling_ctrl.self.layernorm2->torch.nn.LayerNorm(d_model_size, eps=1e-06)
A:transformers.modeling_ctrl.self.dropout1->torch.nn.Dropout(rate)
A:transformers.modeling_ctrl.self.dropout2->torch.nn.Dropout(rate)
A:transformers.modeling_ctrl.normed->self.layernorm1(x)
A:transformers.modeling_ctrl.attn_outputs->self.multi_head_attention(normed, normed, normed, mask, layer_past=layer_past, attention_mask=attention_mask, head_mask=head_mask, use_cache=use_cache, output_attentions=output_attentions)
A:transformers.modeling_ctrl.attn_output->self.dropout1(attn_output)
A:transformers.modeling_ctrl.out2->self.layernorm2(out1)
A:transformers.modeling_ctrl.ffn_output->self.dropout2(ffn_output)
A:transformers.modeling_ctrl.self.pos_encoding->positional_encoding(config.n_positions, self.d_model_size, torch.float)
A:transformers.modeling_ctrl.self.w->torch.nn.Embedding(config.vocab_size, config.n_embd)
A:transformers.modeling_ctrl.self.dropout->torch.nn.Dropout(config.embd_pdrop)
A:transformers.modeling_ctrl.self.h->torch.nn.ModuleList([EncoderLayer(config.n_embd, config.n_head, config.dff, config.resid_pdrop) for _ in range(config.n_layer)])
A:transformers.modeling_ctrl.self.layernorm->torch.nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)
A:transformers.modeling_ctrl.past_key_values->kwargs.pop('past')
A:transformers.modeling_ctrl.input_shape->input_ids[:, -1].unsqueeze(-1).size()
A:transformers.modeling_ctrl.input_ids->input_ids[:, -1].unsqueeze(-1)
A:transformers.modeling_ctrl.past_length->past_key_values[0][0].size(-2)
A:transformers.modeling_ctrl.position_ids->position_ids.view(-1, input_shape[-1]).view(-1, input_shape[-1])
A:transformers.modeling_ctrl.attention_mask->attention_mask.to(dtype=self.dtype).to(dtype=self.dtype)
A:transformers.modeling_ctrl.head_mask->self.get_head_mask(head_mask, self.config.n_layer)
A:transformers.modeling_ctrl.token_type_ids->token_type_ids.view(-1, input_shape[-1]).view(-1, input_shape[-1])
A:transformers.modeling_ctrl.token_type_embeds->self.w(token_type_ids)
A:transformers.modeling_ctrl.inputs_embeds->self.w(input_ids)
A:transformers.modeling_ctrl.mask->torch.triu(torch.ones(seq_len + past_length, seq_len + past_length), 1).to(inputs_embeds.device)
A:transformers.modeling_ctrl.pos_embeds->self.pos_encoding[position_ids, :].to(inputs_embeds.device)
A:transformers.modeling_ctrl.hidden_states->hidden_states.view(*output_shape).view(*output_shape)
A:transformers.modeling_ctrl.outputs->h(hidden_states, mask, layer_past=layer_past, attention_mask=attention_mask, head_mask=head_mask[i], use_cache=use_cache, output_attentions=output_attentions)
A:transformers.modeling_ctrl.all_attentions->tuple((t.view(*attention_output_shape) for t in all_attentions))
A:transformers.modeling_ctrl.self.transformer->CTRLModel(config)
A:transformers.modeling_ctrl.self.lm_head->torch.nn.Linear(config.n_embd, config.vocab_size, bias=True)
A:transformers.modeling_ctrl.transformer_outputs->self.transformer(input_ids, past_key_values=past_key_values, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.modeling_ctrl.lm_logits->self.lm_head(hidden_states)
A:transformers.modeling_ctrl.shift_logits->lm_logits[..., :-1, :].contiguous()
A:transformers.modeling_ctrl.shift_labels->labels[..., 1:].contiguous()
A:transformers.modeling_ctrl.loss_fct->CrossEntropyLoss()
A:transformers.modeling_ctrl.loss->loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
transformers.CTRLLMHeadModel(self,config)
transformers.CTRLLMHeadModel.forward(self,input_ids=None,past_key_values=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.CTRLLMHeadModel.get_output_embeddings(self)
transformers.CTRLLMHeadModel.prepare_inputs_for_generation(self,input_ids,past,**kwargs)
transformers.CTRLModel(self,config)
transformers.CTRLModel._prune_heads(self,heads_to_prune)
transformers.CTRLModel.forward(self,input_ids=None,past_key_values=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.CTRLModel.get_input_embeddings(self)
transformers.CTRLModel.set_input_embeddings(self,new_embeddings)
transformers.CTRLPreTrainedModel(PreTrainedModel)
transformers.CTRLPreTrainedModel._init_weights(self,module)
transformers.modeling_ctrl.CTRLLMHeadModel(self,config)
transformers.modeling_ctrl.CTRLLMHeadModel.__init__(self,config)
transformers.modeling_ctrl.CTRLLMHeadModel.forward(self,input_ids=None,past_key_values=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.modeling_ctrl.CTRLLMHeadModel.get_output_embeddings(self)
transformers.modeling_ctrl.CTRLLMHeadModel.prepare_inputs_for_generation(self,input_ids,past,**kwargs)
transformers.modeling_ctrl.CTRLModel(self,config)
transformers.modeling_ctrl.CTRLModel.__init__(self,config)
transformers.modeling_ctrl.CTRLModel._prune_heads(self,heads_to_prune)
transformers.modeling_ctrl.CTRLModel.forward(self,input_ids=None,past_key_values=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.modeling_ctrl.CTRLModel.get_input_embeddings(self)
transformers.modeling_ctrl.CTRLModel.set_input_embeddings(self,new_embeddings)
transformers.modeling_ctrl.CTRLPreTrainedModel(PreTrainedModel)
transformers.modeling_ctrl.CTRLPreTrainedModel._init_weights(self,module)
transformers.modeling_ctrl.EncoderLayer(self,d_model_size,num_heads,dff,rate=0.1)
transformers.modeling_ctrl.EncoderLayer.__init__(self,d_model_size,num_heads,dff,rate=0.1)
transformers.modeling_ctrl.EncoderLayer.forward(self,x,mask,layer_past=None,attention_mask=None,head_mask=None,use_cache=False,output_attentions=False)
transformers.modeling_ctrl.MultiHeadAttention(self,d_model_size,num_heads)
transformers.modeling_ctrl.MultiHeadAttention.__init__(self,d_model_size,num_heads)
transformers.modeling_ctrl.MultiHeadAttention.forward(self,v,k,q,mask,layer_past=None,attention_mask=None,head_mask=None,use_cache=False,output_attentions=False)
transformers.modeling_ctrl.MultiHeadAttention.prune_heads(self,heads)
transformers.modeling_ctrl.MultiHeadAttention.split_into_heads(self,x,batch_size)
transformers.modeling_ctrl.angle_defn(pos,i,d_model_size)
transformers.modeling_ctrl.point_wise_feed_forward_network(d_model_size,dff)
transformers.modeling_ctrl.positional_encoding(position,d_model_size,dtype)
transformers.modeling_ctrl.scaled_dot_product_attention(q,k,v,mask,attention_mask=None,head_mask=None)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/tokenization_bert.py----------------------------------------
A:transformers.tokenization_bert.logger->utils.logging.get_logger(__name__)
A:transformers.tokenization_bert.vocab->collections.OrderedDict()
A:transformers.tokenization_bert.tokens->unicodedata.normalize('NFD', text).split()
A:transformers.tokenization_bert.token->self._run_strip_accents(token)
A:transformers.tokenization_bert.text->unicodedata.normalize('NFD', text)
A:transformers.tokenization_bert.self.vocab->load_vocab(vocab_file)
A:transformers.tokenization_bert.self.ids_to_tokens->collections.OrderedDict([(ids, tok) for (tok, ids) in self.vocab.items()])
A:transformers.tokenization_bert.self.basic_tokenizer->BasicTokenizer(do_lower_case=do_lower_case, never_split=never_split, tokenize_chinese_chars=tokenize_chinese_chars, strip_accents=strip_accents)
A:transformers.tokenization_bert.self.wordpiece_tokenizer->WordpieceTokenizer(vocab=self.vocab, unk_token=self.unk_token)
A:transformers.tokenization_bert.split_tokens->self.wordpiece_tokenizer.tokenize(text)
A:transformers.tokenization_bert.out_string->' '.join(tokens).replace(' ##', '').strip()
A:transformers.tokenization_bert.vocab_file->os.path.join(vocab_path, VOCAB_FILES_NAMES['vocab_file'])
A:transformers.tokenization_bert.self.never_split->set(never_split)
A:transformers.tokenization_bert.orig_tokens->whitespace_tokenize(text)
A:transformers.tokenization_bert.output_tokens->whitespace_tokenize(' '.join(split_tokens))
A:transformers.tokenization_bert.cat->unicodedata.category(char)
A:transformers.tokenization_bert.chars->list(token)
A:transformers.tokenization_bert.cp->ord(char)
A:transformers.tokenization_bert.end->len(chars)
A:transformers.tokenization_bert.substr->''.join(chars[start:end])
transformers.BasicTokenizer(self,do_lower_case=True,never_split=None,tokenize_chinese_chars=True,strip_accents=None)
transformers.BasicTokenizer._clean_text(self,text)
transformers.BasicTokenizer._is_chinese_char(self,cp)
transformers.BasicTokenizer._run_split_on_punc(self,text,never_split=None)
transformers.BasicTokenizer._run_strip_accents(self,text)
transformers.BasicTokenizer._tokenize_chinese_chars(self,text)
transformers.BasicTokenizer.tokenize(self,text,never_split=None)
transformers.BertTokenizer(self,vocab_file,do_lower_case=True,do_basic_tokenize=True,never_split=None,unk_token='[UNK]',sep_token='[SEP]',pad_token='[PAD]',cls_token='[CLS]',mask_token='[MASK]',tokenize_chinese_chars=True,strip_accents=None,**kwargs)
transformers.BertTokenizer._convert_id_to_token(self,index)
transformers.BertTokenizer._convert_token_to_id(self,token)
transformers.BertTokenizer._tokenize(self,text)
transformers.BertTokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.BertTokenizer.convert_tokens_to_string(self,tokens)
transformers.BertTokenizer.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.BertTokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.BertTokenizer.get_vocab(self)
transformers.BertTokenizer.save_vocabulary(self,vocab_path)
transformers.BertTokenizer.vocab_size(self)
transformers.BertTokenizerFast(self,vocab_file,do_lower_case=True,unk_token='[UNK]',sep_token='[SEP]',pad_token='[PAD]',cls_token='[CLS]',mask_token='[MASK]',clean_text=True,tokenize_chinese_chars=True,strip_accents=None,wordpieces_prefix='##',**kwargs)
transformers.BertTokenizerFast.build_inputs_with_special_tokens(self,token_ids_0,token_ids_1=None)
transformers.BertTokenizerFast.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.WordpieceTokenizer(self,vocab,unk_token,max_input_chars_per_word=100)
transformers.WordpieceTokenizer.tokenize(self,text)
transformers.tokenization_bert.BasicTokenizer(self,do_lower_case=True,never_split=None,tokenize_chinese_chars=True,strip_accents=None)
transformers.tokenization_bert.BasicTokenizer.__init__(self,do_lower_case=True,never_split=None,tokenize_chinese_chars=True,strip_accents=None)
transformers.tokenization_bert.BasicTokenizer._clean_text(self,text)
transformers.tokenization_bert.BasicTokenizer._is_chinese_char(self,cp)
transformers.tokenization_bert.BasicTokenizer._run_split_on_punc(self,text,never_split=None)
transformers.tokenization_bert.BasicTokenizer._run_strip_accents(self,text)
transformers.tokenization_bert.BasicTokenizer._tokenize_chinese_chars(self,text)
transformers.tokenization_bert.BasicTokenizer.tokenize(self,text,never_split=None)
transformers.tokenization_bert.BertTokenizer(self,vocab_file,do_lower_case=True,do_basic_tokenize=True,never_split=None,unk_token='[UNK]',sep_token='[SEP]',pad_token='[PAD]',cls_token='[CLS]',mask_token='[MASK]',tokenize_chinese_chars=True,strip_accents=None,**kwargs)
transformers.tokenization_bert.BertTokenizer.__init__(self,vocab_file,do_lower_case=True,do_basic_tokenize=True,never_split=None,unk_token='[UNK]',sep_token='[SEP]',pad_token='[PAD]',cls_token='[CLS]',mask_token='[MASK]',tokenize_chinese_chars=True,strip_accents=None,**kwargs)
transformers.tokenization_bert.BertTokenizer._convert_id_to_token(self,index)
transformers.tokenization_bert.BertTokenizer._convert_token_to_id(self,token)
transformers.tokenization_bert.BertTokenizer._tokenize(self,text)
transformers.tokenization_bert.BertTokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.tokenization_bert.BertTokenizer.convert_tokens_to_string(self,tokens)
transformers.tokenization_bert.BertTokenizer.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.tokenization_bert.BertTokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.tokenization_bert.BertTokenizer.get_vocab(self)
transformers.tokenization_bert.BertTokenizer.save_vocabulary(self,vocab_path)
transformers.tokenization_bert.BertTokenizer.vocab_size(self)
transformers.tokenization_bert.BertTokenizerFast(self,vocab_file,do_lower_case=True,unk_token='[UNK]',sep_token='[SEP]',pad_token='[PAD]',cls_token='[CLS]',mask_token='[MASK]',clean_text=True,tokenize_chinese_chars=True,strip_accents=None,wordpieces_prefix='##',**kwargs)
transformers.tokenization_bert.BertTokenizerFast.__init__(self,vocab_file,do_lower_case=True,unk_token='[UNK]',sep_token='[SEP]',pad_token='[PAD]',cls_token='[CLS]',mask_token='[MASK]',clean_text=True,tokenize_chinese_chars=True,strip_accents=None,wordpieces_prefix='##',**kwargs)
transformers.tokenization_bert.BertTokenizerFast.build_inputs_with_special_tokens(self,token_ids_0,token_ids_1=None)
transformers.tokenization_bert.BertTokenizerFast.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.tokenization_bert.WordpieceTokenizer(self,vocab,unk_token,max_input_chars_per_word=100)
transformers.tokenization_bert.WordpieceTokenizer.__init__(self,vocab,unk_token,max_input_chars_per_word=100)
transformers.tokenization_bert.WordpieceTokenizer.tokenize(self,text)
transformers.tokenization_bert.load_vocab(vocab_file)
transformers.tokenization_bert.whitespace_tokenize(text)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/modeling_tf_longformer.py----------------------------------------
A:transformers.modeling_tf_longformer.logger->utils.logging.get_logger(__name__)
A:transformers.modeling_tf_longformer.question_end_index->tensorflow.cast(question_end_index[:, None], tf.dtypes.int32)
A:transformers.modeling_tf_longformer.attention_mask->tensorflow.constant([[1, 1, 0, 0, 1], [1, 1, 1, 0, 0], [1, 0, 0, 1, 1]])
A:transformers.modeling_tf_longformer.self.head_dim->int(config.hidden_size / config.num_attention_heads)
A:transformers.modeling_tf_longformer.self.query->tensorflow.keras.layers.Dense(self.embed_dim, kernel_initializer=get_initializer(config.initializer_range), name='query')
A:transformers.modeling_tf_longformer.self.key->tensorflow.keras.layers.Dense(self.embed_dim, kernel_initializer=get_initializer(config.initializer_range), name='key')
A:transformers.modeling_tf_longformer.self.value->tensorflow.keras.layers.Dense(self.embed_dim, kernel_initializer=get_initializer(config.initializer_range), name='value')
A:transformers.modeling_tf_longformer.self.query_global->tensorflow.keras.layers.Dense(self.embed_dim, kernel_initializer=get_initializer(config.initializer_range), name='query_global')
A:transformers.modeling_tf_longformer.self.key_global->tensorflow.keras.layers.Dense(self.embed_dim, kernel_initializer=get_initializer(config.initializer_range), name='key_global')
A:transformers.modeling_tf_longformer.self.value_global->tensorflow.keras.layers.Dense(self.embed_dim, kernel_initializer=get_initializer(config.initializer_range), name='value_global')
A:transformers.modeling_tf_longformer.self.dropout->tensorflow.keras.layers.Dropout(config.attention_probs_dropout_prob)
A:transformers.modeling_tf_longformer.self.global_dropout->tensorflow.keras.layers.Dropout(config.attention_probs_dropout_prob)
A:transformers.modeling_tf_longformer.query_vectors->tensorflow.reshape(query_vectors, (batch_size, seq_len, self.num_heads, self.head_dim))
A:transformers.modeling_tf_longformer.key_vectors->tensorflow.reshape(key_vectors, (batch_size, seq_len, self.num_heads, self.head_dim))
A:transformers.modeling_tf_longformer.value_vectors->tensorflow.reshape(value_vectors, (batch_size, seq_len, self.num_heads, self.head_dim))
A:transformers.modeling_tf_longformer.(batch_size, seq_len, embed_dim)->shape_list(hidden_states)
A:transformers.modeling_tf_longformer.attn_scores->tensorflow.concat((attn_probs_from_global_key, attn_scores), axis=-1)
A:transformers.modeling_tf_longformer.diagonal_mask->self._sliding_chunks_query_key_matmul(tf.ones(shape_list(attention_mask), dtype=tf.float32), attention_mask, self.one_sided_attn_window_size)
A:transformers.modeling_tf_longformer.(max_num_global_attn_indices, is_index_global_attn_nonzero, is_local_index_global_attn_nonzero, is_local_index_no_global_attn_nonzero)->self._get_global_attn_indices(is_index_global_attn)
A:transformers.modeling_tf_longformer.attn_probs->tensorflow.concat([attn_probs[:, :, :, :max_num_global_attn_indices], tf.zeros_like(attn_probs)[:, :, :, max_num_global_attn_indices:]], axis=-1)
A:transformers.modeling_tf_longformer.attn_output->tensorflow.tensor_scatter_nd_update(attn_output, is_index_global_attn_nonzero, nonzero_global_attn_output)
A:transformers.modeling_tf_longformer.(batch_size, seq_len, num_heads, head_dim)->shape_list(value)
A:transformers.modeling_tf_longformer.query->tensorflow.reshape(tf.transpose(query, (0, 2, 1, 3)), (batch_size * num_heads, seq_len, head_dim))
A:transformers.modeling_tf_longformer.key->tensorflow.reshape(tf.transpose(key, (0, 2, 1, 3)), (batch_size * num_heads, seq_len, head_dim))
A:transformers.modeling_tf_longformer.chunked_query->self._chunk(query, window_overlap)
A:transformers.modeling_tf_longformer.chunked_key->self._chunk(key, window_overlap)
A:transformers.modeling_tf_longformer.chunked_attention_scores->tensorflow.einsum('bcxd,bcyd->bcxy', chunked_query, chunked_key)
A:transformers.modeling_tf_longformer.paddings->tensorflow.constant([[0, 0], [0, padding_len]])
A:transformers.modeling_tf_longformer.diagonal_chunked_attention_scores->self._pad_and_transpose_last_two_dims(chunked_attention_scores, paddings)
A:transformers.modeling_tf_longformer.diagonal_attn_scores_up_triang->tensorflow.concat([diagonal_chunked_attention_scores[:, :, :window_overlap, :window_overlap + 1], diagonal_chunked_attention_scores[:, -1:, window_overlap:, :window_overlap + 1]], axis=1)
A:transformers.modeling_tf_longformer.diagonal_attn_scores_low_triang->tensorflow.where(first_chunk_mask, diagonal_attn_scores_first_chunk, diagonal_attn_scores_low_triang)
A:transformers.modeling_tf_longformer.diagonal_attn_scores_first_chunk->tensorflow.concat([tf.roll(diagonal_chunked_attention_scores, shift=[1, window_overlap], axis=[2, 3])[:, :, :window_overlap, :window_overlap], tf.zeros((batch_size * num_heads, 1, window_overlap, window_overlap))], axis=1)
A:transformers.modeling_tf_longformer.diagonal_attention_scores->self._mask_invalid_locations(diagonal_attention_scores, window_overlap)
A:transformers.modeling_tf_longformer.mask_2d_upper->tensorflow.reverse(tf.linalg.band_part(tf.ones(shape=(window_overlap, window_overlap + 1)), -1, 0), axis=[0])
A:transformers.modeling_tf_longformer.padding->tensorflow.constant([[0, shape_list(input_tensor)[1] - window_overlap], [0, shape_list(input_tensor)[3] - window_overlap - 1]])
A:transformers.modeling_tf_longformer.mask_2d->tensorflow.pad(mask_2d_upper, padding)
A:transformers.modeling_tf_longformer.mask_4d->tensorflow.broadcast_to(mask_2d[None, :, None, :], shape_list(input_tensor))
A:transformers.modeling_tf_longformer.input_tensor->tensorflow.where(tf.math.greater(mask_4d, 0), inf_tensor, input_tensor)
A:transformers.modeling_tf_longformer.chunked_attn_probs->self._pad_and_diagonalize(chunked_attn_probs)
A:transformers.modeling_tf_longformer.value->tensorflow.reshape(tf.transpose(value, (0, 2, 1, 3)), (batch_size * num_heads, seq_len, head_dim))
A:transformers.modeling_tf_longformer.padded_value->tensorflow.pad(value, paddings, constant_values=-1)
A:transformers.modeling_tf_longformer.chunked_value->tensorflow.reshape(chunked_value, (batch_size * num_heads, chunks_count + 1, 3 * window_overlap, head_dim))
A:transformers.modeling_tf_longformer.context->tensorflow.transpose(tf.reshape(context, (batch_size, num_heads, seq_len, head_dim)), (0, 2, 1, 3))
A:transformers.modeling_tf_longformer.hidden_states_padded->tensorflow.reshape(hidden_states_padded, (batch_size, chunk_size, hidden_dim, seq_length))
A:transformers.modeling_tf_longformer.(batch_size, chunk_size, seq_length, hidden_dim)->shape_list(hidden_states_padded)
A:transformers.modeling_tf_longformer.(total_num_heads, num_chunks, window_overlap, hidden_dim)->shape_list(chunked_hidden_states)
A:transformers.modeling_tf_longformer.chunked_hidden_states->tensorflow.reshape(chunked_hidden_states, (batch_size, num_output_chunks, 2 * window_overlap, hidden_dim))
A:transformers.modeling_tf_longformer.(batch_size, seq_length, hidden_dim)->shape_list(hidden_states)
A:transformers.modeling_tf_longformer.hidden_states->tensorflow.reshape(hidden_states, (batch_size, seq_length * hidden_dim))
A:transformers.modeling_tf_longformer.num_global_attn_indices->tensorflow.reduce_sum(tf.cast(is_index_global_attn, dtype=tf.dtypes.int32), axis=1)
A:transformers.modeling_tf_longformer.max_num_global_attn_indices->tensorflow.reduce_max(num_global_attn_indices)
A:transformers.modeling_tf_longformer.is_index_global_attn_nonzero->tensorflow.where(is_index_global_attn)
A:transformers.modeling_tf_longformer.is_local_index_global_attn_nonzero->tensorflow.where(is_local_index_global_attn)
A:transformers.modeling_tf_longformer.is_local_index_no_global_attn_nonzero->tensorflow.where(tf.math.logical_not(is_local_index_global_attn))
A:transformers.modeling_tf_longformer.global_key_vectors->self.reshape_and_transpose(global_key_vectors, batch_size)
A:transformers.modeling_tf_longformer.key_vectors_only_global->tensorflow.scatter_nd(is_local_index_global_attn_nonzero, global_key_vectors, shape=(batch_size, max_num_global_attn_indices, self.num_heads, self.head_dim))
A:transformers.modeling_tf_longformer.attn_probs_from_global_key->tensorflow.transpose(attn_probs_from_global_key_trans, (0, 2, 3, 1))
A:transformers.modeling_tf_longformer.attn_probs_from_global_key_trans->tensorflow.tensor_scatter_nd_update(attn_probs_from_global_key_trans, is_local_index_no_global_attn_nonzero, mask)
A:transformers.modeling_tf_longformer.global_value_vectors->self.reshape_and_transpose(global_value_vectors, batch_size)
A:transformers.modeling_tf_longformer.value_vectors_only_global->tensorflow.scatter_nd(is_local_index_global_attn_nonzero, global_value_vectors, shape=(batch_size, max_num_global_attn_indices, self.num_heads, self.head_dim))
A:transformers.modeling_tf_longformer.attn_output_only_global->tensorflow.einsum('blhs,bshd->blhd', attn_probs_only_global, value_vectors_only_global)
A:transformers.modeling_tf_longformer.attn_output_without_global->self._sliding_chunks_matmul_attn_probs_value(attn_probs_without_global, value_vectors, self.one_sided_attn_window_size)
A:transformers.modeling_tf_longformer.global_attn_hidden_states->tensorflow.scatter_nd(is_local_index_global_attn_nonzero, global_attn_hidden_states, shape=(batch_size, max_num_global_attn_indices, self.embed_dim))
A:transformers.modeling_tf_longformer.global_query_vectors_only_global->self.reshape_and_transpose(global_query_vectors_only_global, batch_size)
A:transformers.modeling_tf_longformer.global_attn_scores->tensorflow.reshape(global_attn_scores, (batch_size * self.num_heads, max_num_global_attn_indices, seq_len))
A:transformers.modeling_tf_longformer.global_attn_scores_trans->tensorflow.tensor_scatter_nd_update(global_attn_scores_trans, is_local_index_no_global_attn_nonzero, global_attn_mask)
A:transformers.modeling_tf_longformer.attn_mask->tensorflow.broadcast_to(is_index_masked[:, None, None, :], shape_list(global_attn_scores))
A:transformers.modeling_tf_longformer.global_attn_probs_float->tensorflow.nn.softmax(global_attn_scores, axis=-1)
A:transformers.modeling_tf_longformer.global_attn_probs->self.global_dropout(global_attn_probs_float, training=training)
A:transformers.modeling_tf_longformer.global_attn_output->tensorflow.reshape(global_attn_output, (batch_size, self.num_heads, max_num_global_attn_indices, self.head_dim))
A:transformers.modeling_tf_longformer.nonzero_global_attn_output->tensorflow.reshape(nonzero_global_attn_output, (shape_list(is_local_index_global_attn_nonzero)[0], -1))
A:transformers.modeling_tf_longformer.self.self_attention->TFLongformerSelfAttention(config, layer_id, name='self')
A:transformers.modeling_tf_longformer.self.dense_output->TFBertSelfOutput(config, name='output')
A:transformers.modeling_tf_longformer.self_outputs->self.self_attention([hidden_states, attention_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions], training=training)
A:transformers.modeling_tf_longformer.attention_output->self.dense_output(self_outputs[0], hidden_states, training=training)
A:transformers.modeling_tf_longformer.self.attention->TFLongformerAttention(config, layer_id, name='attention')
A:transformers.modeling_tf_longformer.self.intermediate->TFBertIntermediate(config, name='intermediate')
A:transformers.modeling_tf_longformer.self.longformer_output->TFBertOutput(config, name='output')
A:transformers.modeling_tf_longformer.attention_outputs->self.attention([hidden_states, attention_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions], training=training)
A:transformers.modeling_tf_longformer.intermediate_output->self.intermediate(attention_output)
A:transformers.modeling_tf_longformer.layer_output->self.longformer_output(intermediate_output, attention_output, training=training)
A:transformers.modeling_tf_longformer.layer_outputs->layer_module([hidden_states, attention_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions], training=training)
A:transformers.modeling_tf_longformer.self.embeddings->TFRobertaEmbeddings(config, name='embeddings')
A:transformers.modeling_tf_longformer.self.encoder->TFLongformerEncoder(config, name='encoder')
A:transformers.modeling_tf_longformer.self.pooler->TFBertPooler(config, name='pooler')
A:transformers.modeling_tf_longformer.input_ids->inputs.get('input_ids', inputs)
A:transformers.modeling_tf_longformer.global_attention_mask->_compute_global_attention_mask(shape_list(input_ids), sep_token_indices)
A:transformers.modeling_tf_longformer.token_type_ids->tensorflow.pad(token_type_ids, paddings, constant_values=0)
A:transformers.modeling_tf_longformer.position_ids->tensorflow.pad(position_ids, paddings, constant_values=pad_token_id)
A:transformers.modeling_tf_longformer.inputs_embeds->tensorflow.concat([inputs_embeds, inputs_embeds_padding], axis=-2)
A:transformers.modeling_tf_longformer.output_attentions->inputs.get('output_attentions', output_attentions)
A:transformers.modeling_tf_longformer.output_hidden_states->inputs.get('output_hidden_states', output_hidden_states)
A:transformers.modeling_tf_longformer.return_dict->inputs.get('return_dict', return_dict)
A:transformers.modeling_tf_longformer.input_shape->shape_list(input_ids)
A:transformers.modeling_tf_longformer.(padding_len, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds)->self._pad_to_window_size(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, inputs_embeds=inputs_embeds, pad_token_id=self.pad_token_id)
A:transformers.modeling_tf_longformer.is_index_masked->tensorflow.math.less(attention_mask, 1)
A:transformers.modeling_tf_longformer.is_index_global_attn->tensorflow.math.greater(attention_mask, 1)
A:transformers.modeling_tf_longformer.is_global_attn->tensorflow.math.reduce_any(is_index_global_attn)
A:transformers.modeling_tf_longformer.embedding_output->self.embeddings(input_ids, position_ids, token_type_ids, inputs_embeds, training=training)
A:transformers.modeling_tf_longformer.encoder_outputs->self.encoder(embedding_output, attention_mask=extended_attention_mask, padding_len=padding_len, is_index_masked=is_index_masked, is_index_global_attn=is_index_global_attn, is_global_attn=is_global_attn, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, training=training)
A:transformers.modeling_tf_longformer.pooled_output->self.pooler(sequence_output)
A:transformers.modeling_tf_longformer.input_ids_padding->tensorflow.fill((batch_size, padding_len), self.pad_token_id)
A:transformers.modeling_tf_longformer.inputs_embeds_padding->self.embeddings(input_ids_padding)
A:transformers.modeling_tf_longformer.self.longformer->TFLongformerMainLayer(config, name='longformer')
A:transformers.modeling_tf_longformer.outputs->self.longformer(inputs, attention_mask=attention_mask, global_attention_mask=global_attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, training=training)
A:transformers.modeling_tf_longformer.self.lm_head->TFRobertaLMHead(config, self.longformer.embeddings, name='lm_head')
A:transformers.modeling_tf_longformer.labels->inputs.pop('labels', labels)
A:transformers.modeling_tf_longformer.prediction_scores->self.lm_head(sequence_output, training=training)
A:transformers.modeling_tf_longformer.self.qa_outputs->tensorflow.keras.layers.Dense(config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name='qa_outputs')
A:transformers.modeling_tf_longformer.start_positions->inputs.pop('start_positions', start_positions)
A:transformers.modeling_tf_longformer.end_positions->inputs.pop('end_positions', start_positions)
A:transformers.modeling_tf_longformer.sep_token_indices->tensorflow.where(input_ids == self.config.sep_token_id)
A:transformers.modeling_tf_longformer.logits->self.qa_outputs(sequence_output)
A:transformers.modeling_tf_longformer.(start_logits, end_logits)->tensorflow.split(logits, 2, axis=-1)
A:transformers.modeling_tf_longformer.start_logits->tensorflow.squeeze(start_logits, axis=-1)
A:transformers.modeling_tf_longformer.end_logits->tensorflow.squeeze(end_logits, axis=-1)
A:transformers.modeling_tf_longformer.loss->self.compute_loss(labels, (start_logits, end_logits))
transformers.TFLongformerForMaskedLM(self,config,*inputs,**kwargs)
transformers.TFLongformerForMaskedLM.call(self,inputs=None,attention_mask=None,global_attention_mask=None,token_type_ids=None,position_ids=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.TFLongformerForMaskedLM.get_output_embeddings(self)
transformers.TFLongformerForQuestionAnswering(self,config,*inputs,**kwargs)
transformers.TFLongformerForQuestionAnswering.call(self,inputs=None,attention_mask=None,global_attention_mask=None,token_type_ids=None,position_ids=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,start_positions=None,end_positions=None,training=False)
transformers.TFLongformerModel(self,config,*inputs,**kwargs)
transformers.TFLongformerModel.call(self,inputs,**kwargs)
transformers.TFLongformerSelfAttention(self,config,layer_id,**kwargs)
transformers.TFLongformerSelfAttention._chunk(hidden_states,window_overlap)
transformers.TFLongformerSelfAttention._compute_attn_output_with_global_indices(self,value_vectors,attn_probs,max_num_global_attn_indices,is_index_global_attn_nonzero,is_local_index_global_attn_nonzero)
transformers.TFLongformerSelfAttention._compute_global_attn_output_from_hidden(self,attn_output,hidden_states,max_num_global_attn_indices,is_local_index_global_attn_nonzero,is_index_global_attn_nonzero,is_local_index_no_global_attn_nonzero,is_index_masked,training)
transformers.TFLongformerSelfAttention._concat_with_global_key_attn_probs(self,attn_scores,key_vectors,query_vectors,max_num_global_attn_indices,is_index_global_attn_nonzero,is_local_index_global_attn_nonzero,is_local_index_no_global_attn_nonzero)
transformers.TFLongformerSelfAttention._get_global_attn_indices(is_index_global_attn)
transformers.TFLongformerSelfAttention._get_global_attn_probs(attn_probs,max_num_global_attn_indices)
transformers.TFLongformerSelfAttention._mask_invalid_locations(input_tensor,window_overlap)
transformers.TFLongformerSelfAttention._pad_and_diagonalize(chunked_hidden_states)
transformers.TFLongformerSelfAttention._pad_and_transpose_last_two_dims(hidden_states_padded,paddings)
transformers.TFLongformerSelfAttention._sliding_chunks_matmul_attn_probs_value(self,attn_probs,value,window_overlap)
transformers.TFLongformerSelfAttention._sliding_chunks_query_key_matmul(self,query,key,window_overlap)
transformers.TFLongformerSelfAttention.call(self,inputs,training=False)
transformers.TFLongformerSelfAttention.reshape_and_transpose(self,vector,batch_size)
transformers.modeling_tf_longformer.TFLongformerAttention(self,config,layer_id=0,**kwargs)
transformers.modeling_tf_longformer.TFLongformerAttention.__init__(self,config,layer_id=0,**kwargs)
transformers.modeling_tf_longformer.TFLongformerAttention.call(self,inputs,training=False)
transformers.modeling_tf_longformer.TFLongformerAttention.prune_heads(self,heads)
transformers.modeling_tf_longformer.TFLongformerEncoder(self,config,**kwargs)
transformers.modeling_tf_longformer.TFLongformerEncoder.__init__(self,config,**kwargs)
transformers.modeling_tf_longformer.TFLongformerEncoder.call(self,hidden_states,attention_mask=None,head_mask=None,padding_len=0,is_index_masked=None,is_index_global_attn=None,is_global_attn=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False)
transformers.modeling_tf_longformer.TFLongformerForMaskedLM(self,config,*inputs,**kwargs)
transformers.modeling_tf_longformer.TFLongformerForMaskedLM.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_longformer.TFLongformerForMaskedLM.call(self,inputs=None,attention_mask=None,global_attention_mask=None,token_type_ids=None,position_ids=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.modeling_tf_longformer.TFLongformerForMaskedLM.get_output_embeddings(self)
transformers.modeling_tf_longformer.TFLongformerForQuestionAnswering(self,config,*inputs,**kwargs)
transformers.modeling_tf_longformer.TFLongformerForQuestionAnswering.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_longformer.TFLongformerForQuestionAnswering.call(self,inputs=None,attention_mask=None,global_attention_mask=None,token_type_ids=None,position_ids=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,start_positions=None,end_positions=None,training=False)
transformers.modeling_tf_longformer.TFLongformerLayer(self,config,layer_id=0,**kwargs)
transformers.modeling_tf_longformer.TFLongformerLayer.__init__(self,config,layer_id=0,**kwargs)
transformers.modeling_tf_longformer.TFLongformerLayer.call(self,inputs,training=False)
transformers.modeling_tf_longformer.TFLongformerMainLayer(self,config,**kwargs)
transformers.modeling_tf_longformer.TFLongformerMainLayer.__init__(self,config,**kwargs)
transformers.modeling_tf_longformer.TFLongformerMainLayer._merge_to_attention_mask(attention_mask:tf.Tensor,global_attention_mask:tf.Tensor)
transformers.modeling_tf_longformer.TFLongformerMainLayer._pad_to_window_size(self,input_ids,attention_mask,token_type_ids,position_ids,inputs_embeds,pad_token_id)
transformers.modeling_tf_longformer.TFLongformerMainLayer._prune_heads(self,heads_to_prune)
transformers.modeling_tf_longformer.TFLongformerMainLayer.call(self,inputs,attention_mask=None,global_attention_mask=None,token_type_ids=None,position_ids=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False)
transformers.modeling_tf_longformer.TFLongformerMainLayer.get_input_embeddings(self)
transformers.modeling_tf_longformer.TFLongformerMainLayer.set_input_embeddings(self,value)
transformers.modeling_tf_longformer.TFLongformerModel(self,config,*inputs,**kwargs)
transformers.modeling_tf_longformer.TFLongformerModel.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_longformer.TFLongformerModel.call(self,inputs,**kwargs)
transformers.modeling_tf_longformer.TFLongformerPreTrainedModel(TFPreTrainedModel)
transformers.modeling_tf_longformer.TFLongformerPreTrainedModel.dummy_inputs(self)
transformers.modeling_tf_longformer.TFLongformerSelfAttention(self,config,layer_id,**kwargs)
transformers.modeling_tf_longformer.TFLongformerSelfAttention.__init__(self,config,layer_id,**kwargs)
transformers.modeling_tf_longformer.TFLongformerSelfAttention._chunk(hidden_states,window_overlap)
transformers.modeling_tf_longformer.TFLongformerSelfAttention._compute_attn_output_with_global_indices(self,value_vectors,attn_probs,max_num_global_attn_indices,is_index_global_attn_nonzero,is_local_index_global_attn_nonzero)
transformers.modeling_tf_longformer.TFLongformerSelfAttention._compute_global_attn_output_from_hidden(self,attn_output,hidden_states,max_num_global_attn_indices,is_local_index_global_attn_nonzero,is_index_global_attn_nonzero,is_local_index_no_global_attn_nonzero,is_index_masked,training)
transformers.modeling_tf_longformer.TFLongformerSelfAttention._concat_with_global_key_attn_probs(self,attn_scores,key_vectors,query_vectors,max_num_global_attn_indices,is_index_global_attn_nonzero,is_local_index_global_attn_nonzero,is_local_index_no_global_attn_nonzero)
transformers.modeling_tf_longformer.TFLongformerSelfAttention._get_global_attn_indices(is_index_global_attn)
transformers.modeling_tf_longformer.TFLongformerSelfAttention._get_global_attn_probs(attn_probs,max_num_global_attn_indices)
transformers.modeling_tf_longformer.TFLongformerSelfAttention._mask_invalid_locations(input_tensor,window_overlap)
transformers.modeling_tf_longformer.TFLongformerSelfAttention._pad_and_diagonalize(chunked_hidden_states)
transformers.modeling_tf_longformer.TFLongformerSelfAttention._pad_and_transpose_last_two_dims(hidden_states_padded,paddings)
transformers.modeling_tf_longformer.TFLongformerSelfAttention._sliding_chunks_matmul_attn_probs_value(self,attn_probs,value,window_overlap)
transformers.modeling_tf_longformer.TFLongformerSelfAttention._sliding_chunks_query_key_matmul(self,query,key,window_overlap)
transformers.modeling_tf_longformer.TFLongformerSelfAttention.call(self,inputs,training=False)
transformers.modeling_tf_longformer.TFLongformerSelfAttention.reshape_and_transpose(self,vector,batch_size)
transformers.modeling_tf_longformer._compute_global_attention_mask(input_ids_shape,sep_token_indices,before_sep_token=True)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/modeling_retribert.py----------------------------------------
A:transformers.modeling_retribert.logger->utils.logging.get_logger(__name__)
A:transformers.modeling_retribert.self.bert_query->BertModel(config)
A:transformers.modeling_retribert.self.dropout->torch.nn.Dropout(config.hidden_dropout_prob)
A:transformers.modeling_retribert.self.project_query->torch.nn.Linear(config.hidden_size, config.projection_dim, bias=False)
A:transformers.modeling_retribert.self.project_doc->torch.nn.Linear(config.hidden_size, config.projection_dim, bias=False)
A:transformers.modeling_retribert.self.ce_loss->torch.nn.CrossEntropyLoss(reduction='mean')
A:transformers.modeling_retribert.input_shape->input_ids.size()
A:transformers.modeling_retribert.token_type_ids->torch.zeros(input_shape, dtype=torch.long, device=device)
A:transformers.modeling_retribert.encoder_outputs->sent_encoder.encoder(inputs[0], attention_mask=inputs[1], head_mask=head_mask)
A:transformers.modeling_retribert.pooled_output->torch.utils.checkpoint.checkpoint(partial_encode, b_embedding_output, b_attention_mask)
A:transformers.modeling_retribert.embedding_output->sent_encoder.embeddings(input_ids=input_ids, position_ids=None, token_type_ids=token_type_ids, inputs_embeds=None)
A:transformers.modeling_retribert.q_reps->self.embed_questions(input_ids_query, attention_mask_query, checkpoint_batch_size)
A:transformers.modeling_retribert.a_reps->self.embed_answers(input_ids_doc, attention_mask_doc, checkpoint_batch_size)
A:transformers.modeling_retribert.compare_scores->torch.mm(q_reps, a_reps.t())
A:transformers.modeling_retribert.loss_qa->self.ce_loss(compare_scores, torch.arange(compare_scores.shape[1]).to(device))
A:transformers.modeling_retribert.loss_aq->self.ce_loss(compare_scores.t(), torch.arange(compare_scores.shape[0]).to(device))
transformers.RetriBertModel(self,config)
transformers.RetriBertModel.embed_answers(self,input_ids,attention_mask=None,checkpoint_batch_size=-1)
transformers.RetriBertModel.embed_questions(self,input_ids,attention_mask=None,checkpoint_batch_size=-1)
transformers.RetriBertModel.embed_sentences_checkpointed(self,input_ids,attention_mask,sent_encoder,checkpoint_batch_size=-1)
transformers.RetriBertModel.forward(self,input_ids_query,attention_mask_query,input_ids_doc,attention_mask_doc,checkpoint_batch_size=-1)
transformers.RetriBertPreTrainedModel(PreTrainedModel)
transformers.RetriBertPreTrainedModel._init_weights(self,module)
transformers.modeling_retribert.RetriBertModel(self,config)
transformers.modeling_retribert.RetriBertModel.__init__(self,config)
transformers.modeling_retribert.RetriBertModel.embed_answers(self,input_ids,attention_mask=None,checkpoint_batch_size=-1)
transformers.modeling_retribert.RetriBertModel.embed_questions(self,input_ids,attention_mask=None,checkpoint_batch_size=-1)
transformers.modeling_retribert.RetriBertModel.embed_sentences_checkpointed(self,input_ids,attention_mask,sent_encoder,checkpoint_batch_size=-1)
transformers.modeling_retribert.RetriBertModel.forward(self,input_ids_query,attention_mask_query,input_ids_doc,attention_mask_doc,checkpoint_batch_size=-1)
transformers.modeling_retribert.RetriBertPreTrainedModel(PreTrainedModel)
transformers.modeling_retribert.RetriBertPreTrainedModel._init_weights(self,module)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/modeling_xlm_roberta.py----------------------------------------
A:transformers.modeling_xlm_roberta.logger->utils.logging.get_logger(__name__)
transformers.XLMRobertaForMaskedLM(RobertaForMaskedLM)
transformers.XLMRobertaForMultipleChoice(RobertaForMultipleChoice)
transformers.XLMRobertaForQuestionAnswering(RobertaForQuestionAnswering)
transformers.XLMRobertaForSequenceClassification(RobertaForSequenceClassification)
transformers.XLMRobertaForTokenClassification(RobertaForTokenClassification)
transformers.XLMRobertaModel(RobertaModel)
transformers.modeling_xlm_roberta.XLMRobertaForMaskedLM(RobertaForMaskedLM)
transformers.modeling_xlm_roberta.XLMRobertaForMultipleChoice(RobertaForMultipleChoice)
transformers.modeling_xlm_roberta.XLMRobertaForQuestionAnswering(RobertaForQuestionAnswering)
transformers.modeling_xlm_roberta.XLMRobertaForSequenceClassification(RobertaForSequenceClassification)
transformers.modeling_xlm_roberta.XLMRobertaForTokenClassification(RobertaForTokenClassification)
transformers.modeling_xlm_roberta.XLMRobertaModel(RobertaModel)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/modeling_t5.py----------------------------------------
A:transformers.modeling_t5.logger->utils.logging.get_logger(__name__)
A:transformers.modeling_t5.tf_path->os.path.abspath(tf_checkpoint_path)
A:transformers.modeling_t5.init_vars->tensorflow.train.list_variables(tf_path)
A:transformers.modeling_t5.array->numpy.transpose(array)
A:transformers.modeling_t5.name->txt_name.split('/')
A:transformers.modeling_t5.scope_names->re.split('_(\\d+)', m_name)
A:transformers.modeling_t5.pointer->getattr(pointer, 'weight')
A:transformers.modeling_t5.num->int(scope_names[1])
A:transformers.modeling_t5.pointer.data->torch.from_numpy(array.astype(np.float32))
A:transformers.modeling_t5.self.weight->torch.nn.Parameter(torch.ones(hidden_size))
A:transformers.modeling_t5.variance->x.to(torch.float16).to(torch.float32).pow(2).mean(-1, keepdim=True)
A:transformers.modeling_t5.x->x.to(torch.float16).to(torch.float16)
A:transformers.modeling_t5.self.wi->torch.nn.Linear(config.d_model, config.d_ff, bias=False)
A:transformers.modeling_t5.self.wo->torch.nn.Linear(config.d_ff, config.d_model, bias=False)
A:transformers.modeling_t5.self.dropout->torch.nn.Dropout(config.dropout_rate)
A:transformers.modeling_t5.h->self.wo(h)
A:transformers.modeling_t5.self.DenseReluDense->T5DenseReluDense(config)
A:transformers.modeling_t5.self.layer_norm->T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)
A:transformers.modeling_t5.norm_x->self.layer_norm(hidden_states)
A:transformers.modeling_t5.y->self.DenseReluDense(norm_x)
A:transformers.modeling_t5.self.q->prune_linear_layer(self.q, index)
A:transformers.modeling_t5.self.k->prune_linear_layer(self.k, index)
A:transformers.modeling_t5.self.v->prune_linear_layer(self.v, index)
A:transformers.modeling_t5.self.o->prune_linear_layer(self.o, index, dim=1)
A:transformers.modeling_t5.self.relative_attention_bias->torch.nn.Embedding(self.relative_attention_num_buckets, self.n_heads)
A:transformers.modeling_t5.self.pruned_heads->self.pruned_heads.union(heads)
A:transformers.modeling_t5.(heads, index)->find_pruneable_heads_and_indices(heads, self.n_heads, self.d_kv, self.pruned_heads)
A:transformers.modeling_t5.n->torch.max(n, torch.zeros_like(n))
A:transformers.modeling_t5.val_if_large->torch.min(val_if_large, torch.full_like(val_if_large, num_buckets - 1))
A:transformers.modeling_t5.rp_bucket->rp_bucket.to(self.relative_attention_bias.weight.device).to(self.relative_attention_bias.weight.device)
A:transformers.modeling_t5.values->values.permute([2, 0, 1]).unsqueeze(0).permute([2, 0, 1]).unsqueeze(0)
A:transformers.modeling_t5.(bs, qlen, dim)->input.size()
A:transformers.modeling_t5.klen->kv.size(1)
A:transformers.modeling_t5.q->shape(self.q(input))
A:transformers.modeling_t5.k->torch.cat([k_, k], dim=2)
A:transformers.modeling_t5.v->torch.cat([v_, v], dim=2)
A:transformers.modeling_t5.scores->torch.matmul(q, k.transpose(3, 2))
A:transformers.modeling_t5.position_bias->self.compute_bias(real_qlen, klen)
A:transformers.modeling_t5.weights->torch.nn.functional.dropout(weights, p=self.dropout, training=self.training)
A:transformers.modeling_t5.context->self.o(context)
A:transformers.modeling_t5.self.SelfAttention->T5Attention(config, has_relative_attention_bias=has_relative_attention_bias)
A:transformers.modeling_t5.attention_output->self.EncDecAttention(norm_x, mask=attention_mask, kv=kv, position_bias=position_bias, head_mask=head_mask, past_key_value_state=past_key_value_state, use_cache=use_cache, query_length=query_length, output_attentions=output_attentions)
A:transformers.modeling_t5.self.EncDecAttention->T5Attention(config, has_relative_attention_bias=has_relative_attention_bias)
A:transformers.modeling_t5.self.layer->torch.nn.ModuleList()
A:transformers.modeling_t5.error_message->'There should be {} past states. 2 (past / key) for self attention.{} Got {} past key / value states'.format(expected_num_past_key_value_states, '2 (past / key) for cross attention' if expected_num_past_key_value_states == 4 else '', len(past_key_value_state))
A:transformers.modeling_t5.self_attention_outputs->self.layer[0](hidden_states, attention_mask=attention_mask, position_bias=position_bias, head_mask=head_mask, past_key_value_state=self_attn_past_key_value_state, use_cache=use_cache, output_attentions=output_attentions)
A:transformers.modeling_t5.cross_attention_outputs->self.layer[1](hidden_states, kv=encoder_hidden_states, attention_mask=encoder_attention_mask, position_bias=encoder_decoder_position_bias, head_mask=head_mask, past_key_value_state=cross_attn_past_key_value_state, query_length=query_length, use_cache=use_cache, output_attentions=output_attentions)
A:transformers.modeling_t5.hidden_states->self.dropout(hidden_states)
A:transformers.modeling_t5.input_ids->input_ids.view(-1, input_shape[-1]).view(-1, input_shape[-1])
A:transformers.modeling_t5.input_mask->torch.tensor(DUMMY_MASK)
A:transformers.modeling_t5.shifted_input_ids->input_ids.view(-1, input_shape[-1]).view(-1, input_shape[-1]).new_zeros(input_ids.shape)
A:transformers.modeling_t5.shifted_input_ids[..., 1:]->input_ids[..., :-1].clone()
A:transformers.modeling_t5.self.block->torch.nn.ModuleList([T5Block(config, has_relative_attention_bias=bool(i == 0)) for i in range(config.num_layers)])
A:transformers.modeling_t5.self.final_layer_norm->T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)
A:transformers.modeling_t5.input_shape->input_ids.view(-1, input_shape[-1]).view(-1, input_shape[-1]).size()
A:transformers.modeling_t5.inputs_embeds->self.embed_tokens(input_ids)
A:transformers.modeling_t5.attention_mask->torch.ones(batch_size, mask_seq_length).to(inputs_embeds.device)
A:transformers.modeling_t5.encoder_attention_mask->torch.ones(batch_size, encoder_seq_length, device=inputs_embeds.device, dtype=torch.long)
A:transformers.modeling_t5.extended_attention_mask->self.get_extended_attention_mask(attention_mask, input_shape, inputs_embeds.device)
A:transformers.modeling_t5.encoder_extended_attention_mask->self.invert_attention_mask(encoder_attention_mask)
A:transformers.modeling_t5.head_mask->self.get_head_mask(head_mask, self.config.num_layers)
A:transformers.modeling_t5.layer_outputs->layer_module(hidden_states, attention_mask=extended_attention_mask, position_bias=position_bias, encoder_hidden_states=encoder_hidden_states, encoder_attention_mask=encoder_extended_attention_mask, encoder_decoder_position_bias=encoder_decoder_position_bias, head_mask=head_mask[i], past_key_value_state=past_key_value_state, use_cache=use_cache, output_attentions=output_attentions)
A:transformers.modeling_t5.self.shared->torch.nn.Embedding(config.vocab_size, config.d_model)
A:transformers.modeling_t5.encoder_config->copy.deepcopy(config)
A:transformers.modeling_t5.self.encoder->T5Stack(encoder_config, self.shared)
A:transformers.modeling_t5.decoder_config->copy.deepcopy(config)
A:transformers.modeling_t5.self.decoder->T5Stack(decoder_config, self.shared)
A:transformers.modeling_t5.past_key_values->kwargs.pop('decoder_past_key_values')
A:transformers.modeling_t5.encoder_outputs->BaseModelOutput(last_hidden_state=encoder_outputs[0], hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None, attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None)
A:transformers.modeling_t5.decoder_outputs->self.decoder(input_ids=decoder_input_ids, attention_mask=decoder_attention_mask, inputs_embeds=decoder_inputs_embeds, past_key_value_states=past_key_values, encoder_hidden_states=hidden_states, encoder_attention_mask=attention_mask, head_mask=head_mask, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.modeling_t5.self.lm_head->torch.nn.Linear(config.d_model, config.vocab_size, bias=False)
A:transformers.modeling_t5.labels->kwargs.pop('lm_labels')
A:transformers.modeling_t5.decoder_input_ids->self._shift_right(labels)
A:transformers.modeling_t5.lm_logits->self.lm_head(sequence_output)
A:transformers.modeling_t5.loss_fct->CrossEntropyLoss(ignore_index=-100)
A:transformers.modeling_t5.loss->loss_fct(lm_logits.view(-1, lm_logits.size(-1)), labels.view(-1))
transformers.T5ForConditionalGeneration(self,config)
transformers.T5ForConditionalGeneration._reorder_cache(self,past,beam_idx)
transformers.T5ForConditionalGeneration.forward(self,input_ids=None,attention_mask=None,encoder_outputs=None,decoder_input_ids=None,decoder_attention_mask=None,past_key_values=None,use_cache=None,labels=None,inputs_embeds=None,decoder_inputs_embeds=None,head_mask=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.T5ForConditionalGeneration.get_decoder(self)
transformers.T5ForConditionalGeneration.get_encoder(self)
transformers.T5ForConditionalGeneration.get_input_embeddings(self)
transformers.T5ForConditionalGeneration.get_output_embeddings(self)
transformers.T5ForConditionalGeneration.prepare_inputs_for_generation(self,input_ids,past,attention_mask,use_cache,encoder_outputs,**kwargs)
transformers.T5ForConditionalGeneration.set_input_embeddings(self,new_embeddings)
transformers.T5Model(self,config)
transformers.T5Model._prune_heads(self,heads_to_prune)
transformers.T5Model.forward(self,input_ids=None,attention_mask=None,encoder_outputs=None,decoder_input_ids=None,decoder_attention_mask=None,past_key_values=None,use_cache=None,inputs_embeds=None,decoder_inputs_embeds=None,head_mask=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.T5Model.get_decoder(self)
transformers.T5Model.get_encoder(self)
transformers.T5Model.get_input_embeddings(self)
transformers.T5Model.set_input_embeddings(self,new_embeddings)
transformers.T5PreTrainedModel(PreTrainedModel)
transformers.T5PreTrainedModel._init_weights(self,module)
transformers.T5PreTrainedModel._shift_right(self,input_ids)
transformers.T5PreTrainedModel.dummy_inputs(self)
transformers.load_tf_weights_in_t5(model,config,tf_checkpoint_path)
transformers.modeling_t5.T5Attention(self,config:T5Config,has_relative_attention_bias=False)
transformers.modeling_t5.T5Attention.__init__(self,config:T5Config,has_relative_attention_bias=False)
transformers.modeling_t5.T5Attention._relative_position_bucket(relative_position,bidirectional=True,num_buckets=32,max_distance=128)
transformers.modeling_t5.T5Attention.compute_bias(self,qlen,klen)
transformers.modeling_t5.T5Attention.forward(self,input,mask=None,kv=None,position_bias=None,past_key_value_state=None,head_mask=None,query_length=None,use_cache=False,output_attentions=False)
transformers.modeling_t5.T5Attention.prune_heads(self,heads)
transformers.modeling_t5.T5Block(self,config,has_relative_attention_bias=False)
transformers.modeling_t5.T5Block.__init__(self,config,has_relative_attention_bias=False)
transformers.modeling_t5.T5Block.forward(self,hidden_states,attention_mask=None,position_bias=None,encoder_hidden_states=None,encoder_attention_mask=None,encoder_decoder_position_bias=None,head_mask=None,past_key_value_state=None,use_cache=False,output_attentions=False)
transformers.modeling_t5.T5DenseReluDense(self,config)
transformers.modeling_t5.T5DenseReluDense.__init__(self,config)
transformers.modeling_t5.T5DenseReluDense.forward(self,hidden_states)
transformers.modeling_t5.T5ForConditionalGeneration(self,config)
transformers.modeling_t5.T5ForConditionalGeneration.__init__(self,config)
transformers.modeling_t5.T5ForConditionalGeneration._reorder_cache(self,past,beam_idx)
transformers.modeling_t5.T5ForConditionalGeneration.forward(self,input_ids=None,attention_mask=None,encoder_outputs=None,decoder_input_ids=None,decoder_attention_mask=None,past_key_values=None,use_cache=None,labels=None,inputs_embeds=None,decoder_inputs_embeds=None,head_mask=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.modeling_t5.T5ForConditionalGeneration.get_decoder(self)
transformers.modeling_t5.T5ForConditionalGeneration.get_encoder(self)
transformers.modeling_t5.T5ForConditionalGeneration.get_input_embeddings(self)
transformers.modeling_t5.T5ForConditionalGeneration.get_output_embeddings(self)
transformers.modeling_t5.T5ForConditionalGeneration.prepare_inputs_for_generation(self,input_ids,past,attention_mask,use_cache,encoder_outputs,**kwargs)
transformers.modeling_t5.T5ForConditionalGeneration.set_input_embeddings(self,new_embeddings)
transformers.modeling_t5.T5LayerCrossAttention(self,config,has_relative_attention_bias=False)
transformers.modeling_t5.T5LayerCrossAttention.__init__(self,config,has_relative_attention_bias=False)
transformers.modeling_t5.T5LayerCrossAttention.forward(self,hidden_states,kv,attention_mask=None,position_bias=None,head_mask=None,past_key_value_state=None,use_cache=False,query_length=None,output_attentions=False)
transformers.modeling_t5.T5LayerFF(self,config)
transformers.modeling_t5.T5LayerFF.__init__(self,config)
transformers.modeling_t5.T5LayerFF.forward(self,hidden_states)
transformers.modeling_t5.T5LayerNorm(self,hidden_size,eps=1e-06)
transformers.modeling_t5.T5LayerNorm.__init__(self,hidden_size,eps=1e-06)
transformers.modeling_t5.T5LayerNorm.forward(self,x)
transformers.modeling_t5.T5LayerSelfAttention(self,config,has_relative_attention_bias=False)
transformers.modeling_t5.T5LayerSelfAttention.__init__(self,config,has_relative_attention_bias=False)
transformers.modeling_t5.T5LayerSelfAttention.forward(self,hidden_states,attention_mask=None,position_bias=None,head_mask=None,past_key_value_state=None,use_cache=False,output_attentions=False)
transformers.modeling_t5.T5Model(self,config)
transformers.modeling_t5.T5Model.__init__(self,config)
transformers.modeling_t5.T5Model._prune_heads(self,heads_to_prune)
transformers.modeling_t5.T5Model.forward(self,input_ids=None,attention_mask=None,encoder_outputs=None,decoder_input_ids=None,decoder_attention_mask=None,past_key_values=None,use_cache=None,inputs_embeds=None,decoder_inputs_embeds=None,head_mask=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.modeling_t5.T5Model.get_decoder(self)
transformers.modeling_t5.T5Model.get_encoder(self)
transformers.modeling_t5.T5Model.get_input_embeddings(self)
transformers.modeling_t5.T5Model.set_input_embeddings(self,new_embeddings)
transformers.modeling_t5.T5PreTrainedModel(PreTrainedModel)
transformers.modeling_t5.T5PreTrainedModel._init_weights(self,module)
transformers.modeling_t5.T5PreTrainedModel._shift_right(self,input_ids)
transformers.modeling_t5.T5PreTrainedModel.dummy_inputs(self)
transformers.modeling_t5.T5Stack(self,config,embed_tokens=None)
transformers.modeling_t5.T5Stack.__init__(self,config,embed_tokens=None)
transformers.modeling_t5.T5Stack.forward(self,input_ids=None,attention_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,inputs_embeds=None,head_mask=None,past_key_value_states=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.modeling_t5.T5Stack.get_input_embeddings(self)
transformers.modeling_t5.T5Stack.get_output_embeddings(self)
transformers.modeling_t5.T5Stack.set_input_embeddings(self,new_embeddings)
transformers.modeling_t5.load_tf_weights_in_t5(model,config,tf_checkpoint_path)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/configuration_xlm.py----------------------------------------
A:transformers.configuration_xlm.logger->utils.logging.get_logger(__name__)
transformers.XLMConfig(self,vocab_size=30145,emb_dim=2048,n_layers=12,n_heads=16,dropout=0.1,attention_dropout=0.1,gelu_activation=True,sinusoidal_embeddings=False,causal=False,asm=False,n_langs=1,use_lang_emb=True,max_position_embeddings=512,embed_init_std=2048**(-0.5),layer_norm_eps=1e-12,init_std=0.02,bos_index=0,eos_index=1,pad_index=2,unk_index=3,mask_index=5,is_encoder=True,summary_type='first',summary_use_proj=True,summary_activation=None,summary_proj_to_labels=True,summary_first_dropout=0.1,start_n_top=5,end_n_top=5,mask_token_id=0,lang_id=0,pad_token_id=2,bos_token_id=0,**kwargs)
transformers.XLMConfig.hidden_size(self)
transformers.XLMConfig.n_words(self)
transformers.XLMConfig.n_words(self,value)
transformers.XLMConfig.num_attention_heads(self)
transformers.XLMConfig.num_hidden_layers(self)
transformers.configuration_xlm.XLMConfig(self,vocab_size=30145,emb_dim=2048,n_layers=12,n_heads=16,dropout=0.1,attention_dropout=0.1,gelu_activation=True,sinusoidal_embeddings=False,causal=False,asm=False,n_langs=1,use_lang_emb=True,max_position_embeddings=512,embed_init_std=2048**(-0.5),layer_norm_eps=1e-12,init_std=0.02,bos_index=0,eos_index=1,pad_index=2,unk_index=3,mask_index=5,is_encoder=True,summary_type='first',summary_use_proj=True,summary_activation=None,summary_proj_to_labels=True,summary_first_dropout=0.1,start_n_top=5,end_n_top=5,mask_token_id=0,lang_id=0,pad_token_id=2,bos_token_id=0,**kwargs)
transformers.configuration_xlm.XLMConfig.__init__(self,vocab_size=30145,emb_dim=2048,n_layers=12,n_heads=16,dropout=0.1,attention_dropout=0.1,gelu_activation=True,sinusoidal_embeddings=False,causal=False,asm=False,n_langs=1,use_lang_emb=True,max_position_embeddings=512,embed_init_std=2048**(-0.5),layer_norm_eps=1e-12,init_std=0.02,bos_index=0,eos_index=1,pad_index=2,unk_index=3,mask_index=5,is_encoder=True,summary_type='first',summary_use_proj=True,summary_activation=None,summary_proj_to_labels=True,summary_first_dropout=0.1,start_n_top=5,end_n_top=5,mask_token_id=0,lang_id=0,pad_token_id=2,bos_token_id=0,**kwargs)
transformers.configuration_xlm.XLMConfig.hidden_size(self)
transformers.configuration_xlm.XLMConfig.n_words(self)
transformers.configuration_xlm.XLMConfig.n_words(self,value)
transformers.configuration_xlm.XLMConfig.num_attention_heads(self)
transformers.configuration_xlm.XLMConfig.num_hidden_layers(self)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/modeling_flaubert.py----------------------------------------
A:transformers.modeling_flaubert.logger->utils.logging.get_logger(__name__)
A:transformers.modeling_flaubert.self.layerdrop->getattr(config, 'layerdrop', 0.0)
A:transformers.modeling_flaubert.self.pre_norm->getattr(config, 'pre_norm', False)
A:transformers.modeling_flaubert.(bs, slen)->input_ids.size()
A:transformers.modeling_flaubert.lengths->torch.tensor([slen] * bs, device=device)
A:transformers.modeling_flaubert.(mask, attn_mask)->get_masks(slen, lengths, self.causal, padding_mask=attention_mask)
A:transformers.modeling_flaubert.position_ids->position_ids.unsqueeze(0).expand((bs, slen)).unsqueeze(0).expand((bs, slen))
A:transformers.modeling_flaubert.head_mask->self.get_head_mask(head_mask, self.config.n_layers)
A:transformers.modeling_flaubert.inputs_embeds->self.embeddings(input_ids)
A:transformers.modeling_flaubert.tensor->self.layer_norm2[i](tensor)
A:transformers.modeling_flaubert.dropout_probability->random.uniform(0, 1)
A:transformers.modeling_flaubert.attn_outputs->self.attentions[i](tensor_normalized, attn_mask, cache=cache, head_mask=head_mask[i])
A:transformers.modeling_flaubert.attn->torch.nn.functional.dropout(attn, p=self.dropout, training=self.training)
A:transformers.modeling_flaubert.tensor_normalized->self.layer_norm2[i](tensor)
A:transformers.modeling_flaubert.self.transformer->FlaubertModel(config)
transformers.FlaubertForMultipleChoice(self,config)
transformers.FlaubertForQuestionAnswering(self,config)
transformers.FlaubertForQuestionAnsweringSimple(self,config)
transformers.FlaubertForSequenceClassification(self,config)
transformers.FlaubertForTokenClassification(self,config)
transformers.FlaubertModel(self,config)
transformers.FlaubertModel.forward(self,input_ids=None,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.FlaubertWithLMHeadModel(self,config)
transformers.modeling_flaubert.FlaubertForMultipleChoice(self,config)
transformers.modeling_flaubert.FlaubertForMultipleChoice.__init__(self,config)
transformers.modeling_flaubert.FlaubertForQuestionAnswering(self,config)
transformers.modeling_flaubert.FlaubertForQuestionAnswering.__init__(self,config)
transformers.modeling_flaubert.FlaubertForQuestionAnsweringSimple(self,config)
transformers.modeling_flaubert.FlaubertForQuestionAnsweringSimple.__init__(self,config)
transformers.modeling_flaubert.FlaubertForSequenceClassification(self,config)
transformers.modeling_flaubert.FlaubertForSequenceClassification.__init__(self,config)
transformers.modeling_flaubert.FlaubertForTokenClassification(self,config)
transformers.modeling_flaubert.FlaubertForTokenClassification.__init__(self,config)
transformers.modeling_flaubert.FlaubertModel(self,config)
transformers.modeling_flaubert.FlaubertModel.__init__(self,config)
transformers.modeling_flaubert.FlaubertModel.forward(self,input_ids=None,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.modeling_flaubert.FlaubertWithLMHeadModel(self,config)
transformers.modeling_flaubert.FlaubertWithLMHeadModel.__init__(self,config)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/configuration_bert.py----------------------------------------
A:transformers.configuration_bert.logger->utils.logging.get_logger(__name__)
transformers.BertConfig(self,vocab_size=30522,hidden_size=768,num_hidden_layers=12,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=512,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,pad_token_id=0,gradient_checkpointing=False,**kwargs)
transformers.configuration_bert.BertConfig(self,vocab_size=30522,hidden_size=768,num_hidden_layers=12,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=512,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,pad_token_id=0,gradient_checkpointing=False,**kwargs)
transformers.configuration_bert.BertConfig.__init__(self,vocab_size=30522,hidden_size=768,num_hidden_layers=12,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=512,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,pad_token_id=0,gradient_checkpointing=False,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/tokenization_gpt2.py----------------------------------------
A:transformers.tokenization_gpt2.logger->utils.logging.get_logger(__name__)
A:transformers.tokenization_gpt2.pairs->get_pairs(word)
A:transformers.tokenization_gpt2.self.encoder->json.load(vocab_handle)
A:transformers.tokenization_gpt2.self.byte_encoder->bytes_to_unicode()
A:transformers.tokenization_gpt2.self.bpe_ranks->dict(zip(bpe_merges, range(len(bpe_merges))))
A:transformers.tokenization_gpt2.self.pat->regex.compile("'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+")
A:transformers.tokenization_gpt2.word->' '.join(word)
A:transformers.tokenization_gpt2.bigram->min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float('inf')))
A:transformers.tokenization_gpt2.j->' '.join(word).index(first, i)
A:transformers.tokenization_gpt2.new_word->tuple(new_word)
A:transformers.tokenization_gpt2.token->''.join((self.byte_encoder[b] for b in token.encode('utf-8')))
A:transformers.tokenization_gpt2.text->bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=self.errors)
A:transformers.tokenization_gpt2.vocab_file->os.path.join(save_directory, VOCAB_FILES_NAMES['vocab_file'])
A:transformers.tokenization_gpt2.merge_file->os.path.join(save_directory, VOCAB_FILES_NAMES['merges_file'])
A:transformers.tokenization_gpt2.add_prefix_space->kwargs.pop('add_prefix_space', self.add_prefix_space)
A:transformers.tokenization_gpt2.is_pretokenized->kwargs.get('is_pretokenized', False)
transformers.GPT2Tokenizer(self,vocab_file,merges_file,errors='replace',unk_token='<|endoftext|>',bos_token='<|endoftext|>',eos_token='<|endoftext|>',add_prefix_space=False,**kwargs)
transformers.GPT2Tokenizer._convert_id_to_token(self,index)
transformers.GPT2Tokenizer._convert_token_to_id(self,token)
transformers.GPT2Tokenizer._tokenize(self,text)
transformers.GPT2Tokenizer.bpe(self,token)
transformers.GPT2Tokenizer.convert_tokens_to_string(self,tokens)
transformers.GPT2Tokenizer.get_vocab(self)
transformers.GPT2Tokenizer.prepare_for_tokenization(self,text,is_pretokenized=False,**kwargs)
transformers.GPT2Tokenizer.save_vocabulary(self,save_directory)
transformers.GPT2Tokenizer.vocab_size(self)
transformers.GPT2TokenizerFast(self,vocab_file,merges_file,unk_token='<|endoftext|>',bos_token='<|endoftext|>',eos_token='<|endoftext|>',add_prefix_space=False,trim_offsets=True,**kwargs)
transformers.GPT2TokenizerFast._batch_encode_plus(self,*args,**kwargs)->BatchEncoding
transformers.GPT2TokenizerFast._encode_plus(self,*args,**kwargs)->BatchEncoding
transformers.tokenization_gpt2.GPT2Tokenizer(self,vocab_file,merges_file,errors='replace',unk_token='<|endoftext|>',bos_token='<|endoftext|>',eos_token='<|endoftext|>',add_prefix_space=False,**kwargs)
transformers.tokenization_gpt2.GPT2Tokenizer.__init__(self,vocab_file,merges_file,errors='replace',unk_token='<|endoftext|>',bos_token='<|endoftext|>',eos_token='<|endoftext|>',add_prefix_space=False,**kwargs)
transformers.tokenization_gpt2.GPT2Tokenizer._convert_id_to_token(self,index)
transformers.tokenization_gpt2.GPT2Tokenizer._convert_token_to_id(self,token)
transformers.tokenization_gpt2.GPT2Tokenizer._tokenize(self,text)
transformers.tokenization_gpt2.GPT2Tokenizer.bpe(self,token)
transformers.tokenization_gpt2.GPT2Tokenizer.convert_tokens_to_string(self,tokens)
transformers.tokenization_gpt2.GPT2Tokenizer.get_vocab(self)
transformers.tokenization_gpt2.GPT2Tokenizer.prepare_for_tokenization(self,text,is_pretokenized=False,**kwargs)
transformers.tokenization_gpt2.GPT2Tokenizer.save_vocabulary(self,save_directory)
transformers.tokenization_gpt2.GPT2Tokenizer.vocab_size(self)
transformers.tokenization_gpt2.GPT2TokenizerFast(self,vocab_file,merges_file,unk_token='<|endoftext|>',bos_token='<|endoftext|>',eos_token='<|endoftext|>',add_prefix_space=False,trim_offsets=True,**kwargs)
transformers.tokenization_gpt2.GPT2TokenizerFast.__init__(self,vocab_file,merges_file,unk_token='<|endoftext|>',bos_token='<|endoftext|>',eos_token='<|endoftext|>',add_prefix_space=False,trim_offsets=True,**kwargs)
transformers.tokenization_gpt2.GPT2TokenizerFast._batch_encode_plus(self,*args,**kwargs)->BatchEncoding
transformers.tokenization_gpt2.GPT2TokenizerFast._encode_plus(self,*args,**kwargs)->BatchEncoding
transformers.tokenization_gpt2.bytes_to_unicode()
transformers.tokenization_gpt2.get_pairs(word)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/modeling_tf_mobilebert.py----------------------------------------
A:transformers.modeling_tf_mobilebert.logger->utils.logging.get_logger(__name__)
A:transformers.modeling_tf_mobilebert.self.bias->self.add_weight(shape=(self.vocab_size,), initializer='zeros', trainable=True, name='bias')
A:transformers.modeling_tf_mobilebert.self.weight->self.add_weight('weight', shape=[self.feat_size], initializer='ones')
A:transformers.modeling_tf_mobilebert.self.position_embeddings->tensorflow.keras.layers.Embedding(config.max_position_embeddings, config.hidden_size, embeddings_initializer=get_initializer(self.initializer_range), name='position_embeddings')
A:transformers.modeling_tf_mobilebert.self.token_type_embeddings->tensorflow.keras.layers.Embedding(config.type_vocab_size, config.hidden_size, embeddings_initializer=get_initializer(self.initializer_range), name='token_type_embeddings')
A:transformers.modeling_tf_mobilebert.self.embedding_transformation->tensorflow.keras.layers.Dense(config.hidden_size, name='embedding_transformation')
A:transformers.modeling_tf_mobilebert.self.LayerNorm->NORM2FN['layer_norm'](config.hidden_size, epsilon=config.layer_norm_eps, name='LayerNorm')
A:transformers.modeling_tf_mobilebert.self.dropout->tensorflow.keras.layers.Dropout(config.hidden_dropout_prob)
A:transformers.modeling_tf_mobilebert.self.word_embeddings->self.add_weight('weight', shape=[self.vocab_size, self.embedding_size], initializer=get_initializer(self.initializer_range))
A:transformers.modeling_tf_mobilebert.input_shape->shape_list(input_ids)
A:transformers.modeling_tf_mobilebert.token_type_ids->inputs.get('token_type_ids', token_type_ids)
A:transformers.modeling_tf_mobilebert.inputs_embeds->inputs.get('inputs_embeds', inputs_embeds)
A:transformers.modeling_tf_mobilebert.position_embeddings->self.position_embeddings(position_ids)
A:transformers.modeling_tf_mobilebert.token_type_embeddings->self.token_type_embeddings(token_type_ids)
A:transformers.modeling_tf_mobilebert.embeddings->self.dropout(embeddings, training=training)
A:transformers.modeling_tf_mobilebert.x->tensorflow.reshape(x, (batch_size, -1, self.num_attention_heads, self.attention_head_size))
A:transformers.modeling_tf_mobilebert.logits->self.classifier(sequence_output)
A:transformers.modeling_tf_mobilebert.self.attention_head_size->int(config.true_hidden_size / config.num_attention_heads)
A:transformers.modeling_tf_mobilebert.self.query->tensorflow.keras.layers.Dense(self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name='query')
A:transformers.modeling_tf_mobilebert.self.key->tensorflow.keras.layers.Dense(self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name='key')
A:transformers.modeling_tf_mobilebert.self.value->tensorflow.keras.layers.Dense(self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name='value')
A:transformers.modeling_tf_mobilebert.mixed_query_layer->self.query(query_tensor)
A:transformers.modeling_tf_mobilebert.mixed_key_layer->self.key(key_tensor)
A:transformers.modeling_tf_mobilebert.mixed_value_layer->self.value(value_tensor)
A:transformers.modeling_tf_mobilebert.query_layer->self.transpose_for_scores(mixed_query_layer, batch_size)
A:transformers.modeling_tf_mobilebert.key_layer->self.transpose_for_scores(mixed_key_layer, batch_size)
A:transformers.modeling_tf_mobilebert.value_layer->self.transpose_for_scores(mixed_value_layer, batch_size)
A:transformers.modeling_tf_mobilebert.attention_scores->tensorflow.matmul(query_layer, key_layer, transpose_b=True)
A:transformers.modeling_tf_mobilebert.dk->tensorflow.cast(shape_list(key_layer)[-1], tf.float32)
A:transformers.modeling_tf_mobilebert.attention_probs->self.dropout(attention_probs, training=training)
A:transformers.modeling_tf_mobilebert.context_layer->tensorflow.reshape(context_layer, (batch_size, -1, self.all_head_size))
A:transformers.modeling_tf_mobilebert.self.dense->self.add_weight(shape=(self.config.hidden_size - self.config.embedding_size, self.vocab_size), initializer='zeros', trainable=True, name='dense/weight')
A:transformers.modeling_tf_mobilebert.hidden_states->tensorflow.matmul(hidden_states, tf.concat([tf.transpose(self.decoder), self.dense], axis=0))
A:transformers.modeling_tf_mobilebert.self.self->TFMobileBertSelfAttention(config, name='self')
A:transformers.modeling_tf_mobilebert.self.mobilebert_output->TFMobileBertOutput(config, name='output')
A:transformers.modeling_tf_mobilebert.self_outputs->self.self(query_tensor, key_tensor, value_tensor, attention_mask, head_mask, output_attentions, training=training)
A:transformers.modeling_tf_mobilebert.attention_output->ffn_module(attention_output)
A:transformers.modeling_tf_mobilebert.layer_outputs->layer_module(hidden_states, attention_mask, head_mask[i], output_attentions, training=training)
A:transformers.modeling_tf_mobilebert.self.bottleneck->TFBottleneck(config, name='bottleneck')
A:transformers.modeling_tf_mobilebert.self.bottleneck_input->TFBottleneckLayer(config, name='input')
A:transformers.modeling_tf_mobilebert.self.attention->TFMobileBertAttention(config, name='attention')
A:transformers.modeling_tf_mobilebert.bottlenecked_hidden_states->self.bottleneck_input(hidden_states)
A:transformers.modeling_tf_mobilebert.shared_attention_input->self.attention(hidden_states)
A:transformers.modeling_tf_mobilebert.self.intermediate->TFMobileBertIntermediate(config, name='intermediate')
A:transformers.modeling_tf_mobilebert.intermediate_output->self.intermediate(attention_output)
A:transformers.modeling_tf_mobilebert.(query_tensor, key_tensor, value_tensor, layer_input)->self.bottleneck(hidden_states)
A:transformers.modeling_tf_mobilebert.attention_outputs->self.attention(query_tensor, key_tensor, value_tensor, layer_input, attention_mask, head_mask, output_attentions, training=training)
A:transformers.modeling_tf_mobilebert.layer_output->self.mobilebert_output(intermediate_output, attention_output, hidden_states, training=training)
A:transformers.modeling_tf_mobilebert.pooled_output->self.dropout(pooled_output, training=training)
A:transformers.modeling_tf_mobilebert.self.transform->TFMobileBertPredictionHeadTransform(config, name='transform')
A:transformers.modeling_tf_mobilebert.self.decoder->self.add_weight(shape=(self.config.vocab_size, self.config.embedding_size), initializer='zeros', trainable=True, name='decoder/weight')
A:transformers.modeling_tf_mobilebert.self.predictions->TFMobileBertMLMHead(config, name='predictions___cls')
A:transformers.modeling_tf_mobilebert.prediction_scores->self.mlm(sequence_output, training=training)
A:transformers.modeling_tf_mobilebert.self.embeddings->TFMobileBertEmbeddings(config, name='embeddings')
A:transformers.modeling_tf_mobilebert.self.encoder->TFMobileBertEncoder(config, name='encoder')
A:transformers.modeling_tf_mobilebert.self.pooler->TFMobileBertPooler(config, name='pooler')
A:transformers.modeling_tf_mobilebert.input_ids->inputs.get('input_ids')
A:transformers.modeling_tf_mobilebert.attention_mask->inputs.get('attention_mask', attention_mask)
A:transformers.modeling_tf_mobilebert.position_ids->inputs.get('position_ids', position_ids)
A:transformers.modeling_tf_mobilebert.head_mask->inputs.get('head_mask', head_mask)
A:transformers.modeling_tf_mobilebert.output_attentions->inputs.get('output_attentions', output_attentions)
A:transformers.modeling_tf_mobilebert.output_hidden_states->inputs.get('output_hidden_states', output_hidden_states)
A:transformers.modeling_tf_mobilebert.return_dict->inputs.get('return_dict', return_dict)
A:transformers.modeling_tf_mobilebert.extended_attention_mask->tensorflow.cast(extended_attention_mask, tf.float32)
A:transformers.modeling_tf_mobilebert.embedding_output->self.embeddings(input_ids, position_ids, token_type_ids, inputs_embeds, training=training)
A:transformers.modeling_tf_mobilebert.encoder_outputs->self.encoder(embedding_output, extended_attention_mask, head_mask, output_attentions, output_hidden_states, return_dict, training=training)
A:transformers.modeling_tf_mobilebert.self.mobilebert->TFMobileBertMainLayer(config, name='mobilebert')
A:transformers.modeling_tf_mobilebert.outputs->self.mobilebert(inputs, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, training=training)
A:transformers.modeling_tf_mobilebert.self.seq_relationship->tensorflow.keras.layers.Dense(2, name='seq_relationship')
A:transformers.modeling_tf_mobilebert.seq_relationship_score->self.cls(pooled_output)
A:transformers.modeling_tf_mobilebert.self.mlm->TFMobileBertMLMHead(config, name='mlm___cls')
A:transformers.modeling_tf_mobilebert.labels->inputs.pop('labels', labels)
A:transformers.modeling_tf_mobilebert.self.cls->TFMobileBertOnlyNSPHead(config, name='seq_relationship___cls')
A:transformers.modeling_tf_mobilebert.self.classifier->tensorflow.keras.layers.Dense(config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name='classifier')
A:transformers.modeling_tf_mobilebert.self.qa_outputs->tensorflow.keras.layers.Dense(config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name='qa_outputs')
A:transformers.modeling_tf_mobilebert.start_positions->inputs.pop('start_positions', start_positions)
A:transformers.modeling_tf_mobilebert.end_positions->inputs.pop('end_positions', start_positions)
A:transformers.modeling_tf_mobilebert.(start_logits, end_logits)->tensorflow.split(logits, 2, axis=-1)
A:transformers.modeling_tf_mobilebert.start_logits->tensorflow.squeeze(start_logits, axis=-1)
A:transformers.modeling_tf_mobilebert.end_logits->tensorflow.squeeze(end_logits, axis=-1)
A:transformers.modeling_tf_mobilebert.loss->self.compute_loss(labels, (start_logits, end_logits))
A:transformers.modeling_tf_mobilebert.reshaped_logits->tensorflow.reshape(logits, (-1, num_choices))
A:transformers.modeling_tf_mobilebert.sequence_output->self.dropout(sequence_output, training=training)
transformers.TFMobileBertForMaskedLM(self,config,*inputs,**kwargs)
transformers.TFMobileBertForMaskedLM.call(self,inputs=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.TFMobileBertForMaskedLM.get_output_embeddings(self)
transformers.TFMobileBertForMultipleChoice(self,config,*inputs,**kwargs)
transformers.TFMobileBertForMultipleChoice.call(self,inputs,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.TFMobileBertForMultipleChoice.dummy_inputs(self)
transformers.TFMobileBertForNextSentencePrediction(self,config,*inputs,**kwargs)
transformers.TFMobileBertForNextSentencePrediction.call(self,inputs,**kwargs)
transformers.TFMobileBertForPreTraining(self,config,*inputs,**kwargs)
transformers.TFMobileBertForPreTraining.call(self,inputs,**kwargs)
transformers.TFMobileBertForPreTraining.get_output_embeddings(self)
transformers.TFMobileBertForPreTrainingOutput(ModelOutput)
transformers.TFMobileBertForQuestionAnswering(self,config,*inputs,**kwargs)
transformers.TFMobileBertForQuestionAnswering.call(self,inputs=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,start_positions=None,end_positions=None,training=False)
transformers.TFMobileBertForSequenceClassification(self,config,*inputs,**kwargs)
transformers.TFMobileBertForSequenceClassification.call(self,inputs=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.TFMobileBertForTokenClassification(self,config,*inputs,**kwargs)
transformers.TFMobileBertForTokenClassification.call(self,inputs=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.TFMobileBertMainLayer(self,config,**kwargs)
transformers.TFMobileBertMainLayer._prune_heads(self,heads_to_prune)
transformers.TFMobileBertMainLayer._resize_token_embeddings(self,new_num_tokens)
transformers.TFMobileBertMainLayer.call(self,inputs,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False)
transformers.TFMobileBertMainLayer.get_input_embeddings(self)
transformers.TFMobileBertModel(self,config,*inputs,**kwargs)
transformers.TFMobileBertModel.call(self,inputs,**kwargs)
transformers.TFMobileBertPreTrainedModel(TFPreTrainedModel)
transformers.modeling_tf_mobilebert.TFBottleneck(self,config,**kwargs)
transformers.modeling_tf_mobilebert.TFBottleneck.__init__(self,config,**kwargs)
transformers.modeling_tf_mobilebert.TFBottleneck.call(self,hidden_states)
transformers.modeling_tf_mobilebert.TFBottleneckLayer(self,config,**kwargs)
transformers.modeling_tf_mobilebert.TFBottleneckLayer.__init__(self,config,**kwargs)
transformers.modeling_tf_mobilebert.TFBottleneckLayer.call(self,inputs)
transformers.modeling_tf_mobilebert.TFFFNLayer(self,config,**kwargs)
transformers.modeling_tf_mobilebert.TFFFNLayer.__init__(self,config,**kwargs)
transformers.modeling_tf_mobilebert.TFFFNLayer.call(self,hidden_states)
transformers.modeling_tf_mobilebert.TFFFNOutput(self,config,**kwargs)
transformers.modeling_tf_mobilebert.TFFFNOutput.__init__(self,config,**kwargs)
transformers.modeling_tf_mobilebert.TFFFNOutput.call(self,hidden_states,residual_tensor)
transformers.modeling_tf_mobilebert.TFLayerNorm(self,feat_size,*args,**kwargs)
transformers.modeling_tf_mobilebert.TFLayerNorm.__init__(self,feat_size,*args,**kwargs)
transformers.modeling_tf_mobilebert.TFMobileBertAttention(self,config,**kwargs)
transformers.modeling_tf_mobilebert.TFMobileBertAttention.__init__(self,config,**kwargs)
transformers.modeling_tf_mobilebert.TFMobileBertAttention.call(self,query_tensor,key_tensor,value_tensor,layer_input,attention_mask,head_mask,output_attentions,training=False)
transformers.modeling_tf_mobilebert.TFMobileBertAttention.prune_heads(self,heads)
transformers.modeling_tf_mobilebert.TFMobileBertEmbeddings(self,config,**kwargs)
transformers.modeling_tf_mobilebert.TFMobileBertEmbeddings.__init__(self,config,**kwargs)
transformers.modeling_tf_mobilebert.TFMobileBertEmbeddings._embedding(self,input_ids,position_ids,token_type_ids,inputs_embeds,training=False)
transformers.modeling_tf_mobilebert.TFMobileBertEmbeddings._linear(self,inputs)
transformers.modeling_tf_mobilebert.TFMobileBertEmbeddings.build(self,input_shape)
transformers.modeling_tf_mobilebert.TFMobileBertEmbeddings.call(self,input_ids=None,position_ids=None,token_type_ids=None,inputs_embeds=None,mode='embedding',training=False)
transformers.modeling_tf_mobilebert.TFMobileBertEncoder(self,config,**kwargs)
transformers.modeling_tf_mobilebert.TFMobileBertEncoder.__init__(self,config,**kwargs)
transformers.modeling_tf_mobilebert.TFMobileBertEncoder.call(self,hidden_states,attention_mask,head_mask,output_attentions,output_hidden_states,return_dict,training=False)
transformers.modeling_tf_mobilebert.TFMobileBertForMaskedLM(self,config,*inputs,**kwargs)
transformers.modeling_tf_mobilebert.TFMobileBertForMaskedLM.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_mobilebert.TFMobileBertForMaskedLM.call(self,inputs=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.modeling_tf_mobilebert.TFMobileBertForMaskedLM.get_output_embeddings(self)
transformers.modeling_tf_mobilebert.TFMobileBertForMultipleChoice(self,config,*inputs,**kwargs)
transformers.modeling_tf_mobilebert.TFMobileBertForMultipleChoice.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_mobilebert.TFMobileBertForMultipleChoice.call(self,inputs,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.modeling_tf_mobilebert.TFMobileBertForMultipleChoice.dummy_inputs(self)
transformers.modeling_tf_mobilebert.TFMobileBertForNextSentencePrediction(self,config,*inputs,**kwargs)
transformers.modeling_tf_mobilebert.TFMobileBertForNextSentencePrediction.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_mobilebert.TFMobileBertForNextSentencePrediction.call(self,inputs,**kwargs)
transformers.modeling_tf_mobilebert.TFMobileBertForPreTraining(self,config,*inputs,**kwargs)
transformers.modeling_tf_mobilebert.TFMobileBertForPreTraining.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_mobilebert.TFMobileBertForPreTraining.call(self,inputs,**kwargs)
transformers.modeling_tf_mobilebert.TFMobileBertForPreTraining.get_output_embeddings(self)
transformers.modeling_tf_mobilebert.TFMobileBertForPreTrainingOutput(ModelOutput)
transformers.modeling_tf_mobilebert.TFMobileBertForQuestionAnswering(self,config,*inputs,**kwargs)
transformers.modeling_tf_mobilebert.TFMobileBertForQuestionAnswering.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_mobilebert.TFMobileBertForQuestionAnswering.call(self,inputs=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,start_positions=None,end_positions=None,training=False)
transformers.modeling_tf_mobilebert.TFMobileBertForSequenceClassification(self,config,*inputs,**kwargs)
transformers.modeling_tf_mobilebert.TFMobileBertForSequenceClassification.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_mobilebert.TFMobileBertForSequenceClassification.call(self,inputs=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.modeling_tf_mobilebert.TFMobileBertForTokenClassification(self,config,*inputs,**kwargs)
transformers.modeling_tf_mobilebert.TFMobileBertForTokenClassification.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_mobilebert.TFMobileBertForTokenClassification.call(self,inputs=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.modeling_tf_mobilebert.TFMobileBertIntermediate(self,config,**kwargs)
transformers.modeling_tf_mobilebert.TFMobileBertIntermediate.__init__(self,config,**kwargs)
transformers.modeling_tf_mobilebert.TFMobileBertLMPredictionHead(self,config,**kwargs)
transformers.modeling_tf_mobilebert.TFMobileBertLMPredictionHead.__init__(self,config,**kwargs)
transformers.modeling_tf_mobilebert.TFMobileBertLMPredictionHead.build(self,input_shape)
transformers.modeling_tf_mobilebert.TFMobileBertLMPredictionHead.call(self,hidden_states)
transformers.modeling_tf_mobilebert.TFMobileBertLayer(self,config,**kwargs)
transformers.modeling_tf_mobilebert.TFMobileBertLayer.__init__(self,config,**kwargs)
transformers.modeling_tf_mobilebert.TFMobileBertLayer.call(self,hidden_states,attention_mask,head_mask,output_attentions,training=False)
transformers.modeling_tf_mobilebert.TFMobileBertMLMHead(self,config,**kwargs)
transformers.modeling_tf_mobilebert.TFMobileBertMLMHead.__init__(self,config,**kwargs)
transformers.modeling_tf_mobilebert.TFMobileBertMLMHead.call(self,sequence_output)
transformers.modeling_tf_mobilebert.TFMobileBertMainLayer(self,config,**kwargs)
transformers.modeling_tf_mobilebert.TFMobileBertMainLayer.__init__(self,config,**kwargs)
transformers.modeling_tf_mobilebert.TFMobileBertMainLayer._prune_heads(self,heads_to_prune)
transformers.modeling_tf_mobilebert.TFMobileBertMainLayer._resize_token_embeddings(self,new_num_tokens)
transformers.modeling_tf_mobilebert.TFMobileBertMainLayer.call(self,inputs,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False)
transformers.modeling_tf_mobilebert.TFMobileBertMainLayer.get_input_embeddings(self)
transformers.modeling_tf_mobilebert.TFMobileBertModel(self,config,*inputs,**kwargs)
transformers.modeling_tf_mobilebert.TFMobileBertModel.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_mobilebert.TFMobileBertModel.call(self,inputs,**kwargs)
transformers.modeling_tf_mobilebert.TFMobileBertOnlyNSPHead(self,config,**kwargs)
transformers.modeling_tf_mobilebert.TFMobileBertOnlyNSPHead.__init__(self,config,**kwargs)
transformers.modeling_tf_mobilebert.TFMobileBertOnlyNSPHead.call(self,pooled_output)
transformers.modeling_tf_mobilebert.TFMobileBertOutput(self,config,**kwargs)
transformers.modeling_tf_mobilebert.TFMobileBertOutput.__init__(self,config,**kwargs)
transformers.modeling_tf_mobilebert.TFMobileBertOutput.call(self,hidden_states,residual_tensor_1,residual_tensor_2,training=False)
transformers.modeling_tf_mobilebert.TFMobileBertPooler(self,config,**kwargs)
transformers.modeling_tf_mobilebert.TFMobileBertPooler.__init__(self,config,**kwargs)
transformers.modeling_tf_mobilebert.TFMobileBertPooler.call(self,hidden_states)
transformers.modeling_tf_mobilebert.TFMobileBertPreTrainedModel(TFPreTrainedModel)
transformers.modeling_tf_mobilebert.TFMobileBertPredictionHeadTransform(self,config,**kwargs)
transformers.modeling_tf_mobilebert.TFMobileBertPredictionHeadTransform.__init__(self,config,**kwargs)
transformers.modeling_tf_mobilebert.TFMobileBertPredictionHeadTransform.call(self,hidden_states)
transformers.modeling_tf_mobilebert.TFMobileBertSelfAttention(self,config,**kwargs)
transformers.modeling_tf_mobilebert.TFMobileBertSelfAttention.__init__(self,config,**kwargs)
transformers.modeling_tf_mobilebert.TFMobileBertSelfAttention.call(self,query_tensor,key_tensor,value_tensor,attention_mask,head_mask,output_attentions,training=False)
transformers.modeling_tf_mobilebert.TFMobileBertSelfAttention.transpose_for_scores(self,x,batch_size)
transformers.modeling_tf_mobilebert.TFMobileBertSelfOutput(self,config,**kwargs)
transformers.modeling_tf_mobilebert.TFMobileBertSelfOutput.__init__(self,config,**kwargs)
transformers.modeling_tf_mobilebert.TFMobileBertSelfOutput.call(self,hidden_states,residual_tensor,training=False)
transformers.modeling_tf_mobilebert.TFNoNorm(self,feat_size,epsilon=None,**kwargs)
transformers.modeling_tf_mobilebert.TFNoNorm.__init__(self,feat_size,epsilon=None,**kwargs)
transformers.modeling_tf_mobilebert.TFNoNorm.build(self,input_shape)
transformers.modeling_tf_mobilebert.TFNoNorm.call(self,inputs:tf.Tensor)
transformers.modeling_tf_mobilebert.TFOutputBottleneck(self,config,**kwargs)
transformers.modeling_tf_mobilebert.TFOutputBottleneck.__init__(self,config,**kwargs)
transformers.modeling_tf_mobilebert.TFOutputBottleneck.call(self,hidden_states,residual_tensor,training=False)
transformers.modeling_tf_mobilebert.mish(x)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/configuration_xlm_roberta.py----------------------------------------
A:transformers.configuration_xlm_roberta.logger->utils.logging.get_logger(__name__)
transformers.XLMRobertaConfig(RobertaConfig)
transformers.configuration_xlm_roberta.XLMRobertaConfig(RobertaConfig)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/modeling_dpr.py----------------------------------------
A:transformers.modeling_dpr.logger->utils.logging.get_logger(__name__)
A:transformers.modeling_dpr.self.bert_model->BertModel(config)
A:transformers.modeling_dpr.self.encode_proj->torch.nn.Linear(self.bert_model.config.hidden_size, config.projection_dim)
A:transformers.modeling_dpr.outputs->self.question_encoder(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.modeling_dpr.pooled_output->self.encode_proj(pooled_output)
A:transformers.modeling_dpr.self.encoder->DPREncoder(config)
A:transformers.modeling_dpr.self.qa_outputs->torch.nn.Linear(self.encoder.embeddings_size, 2)
A:transformers.modeling_dpr.self.qa_classifier->torch.nn.Linear(self.encoder.embeddings_size, 1)
A:transformers.modeling_dpr.logits->self.qa_outputs(sequence_output)
A:transformers.modeling_dpr.(start_logits, end_logits)->self.qa_outputs(sequence_output).split(1, dim=-1)
A:transformers.modeling_dpr.start_logits->start_logits.view(n_passages, sequence_length).view(n_passages, sequence_length)
A:transformers.modeling_dpr.end_logits->end_logits.view(n_passages, sequence_length).view(n_passages, sequence_length)
A:transformers.modeling_dpr.relevance_logits->relevance_logits.view(n_passages).view(n_passages)
A:transformers.modeling_dpr.self.ctx_encoder->DPREncoder(config)
A:transformers.modeling_dpr.input_shape->input_ids.size()
A:transformers.modeling_dpr.token_type_ids->torch.zeros(input_shape, dtype=torch.long, device=device)
A:transformers.modeling_dpr.self.question_encoder->DPREncoder(config)
A:transformers.modeling_dpr.self.span_predictor->DPRSpanPredictor(config)
A:transformers.modeling_dpr.attention_mask->torch.ones(input_shape, device=device)
transformers.DPRContextEncoder(self,config:DPRConfig)
transformers.DPRContextEncoder.forward(self,input_ids:Optional[Tensor]=None,attention_mask:Optional[Tensor]=None,token_type_ids:Optional[Tensor]=None,inputs_embeds:Optional[Tensor]=None,output_attentions=None,output_hidden_states=None,return_dict=None)->Union[DPRContextEncoderOutput, Tuple[Tensor, ...]]
transformers.DPRContextEncoderOutput(ModelOutput)
transformers.DPRPretrainedContextEncoder(PreTrainedModel)
transformers.DPRPretrainedContextEncoder.init_weights(self)
transformers.DPRPretrainedQuestionEncoder(PreTrainedModel)
transformers.DPRPretrainedQuestionEncoder.init_weights(self)
transformers.DPRPretrainedReader(PreTrainedModel)
transformers.DPRPretrainedReader.init_weights(self)
transformers.DPRQuestionEncoder(self,config:DPRConfig)
transformers.DPRQuestionEncoder.forward(self,input_ids:Optional[Tensor]=None,attention_mask:Optional[Tensor]=None,token_type_ids:Optional[Tensor]=None,inputs_embeds:Optional[Tensor]=None,output_attentions=None,output_hidden_states=None,return_dict=None)->Union[DPRQuestionEncoderOutput, Tuple[Tensor, ...]]
transformers.DPRQuestionEncoderOutput(ModelOutput)
transformers.DPRReader(self,config:DPRConfig)
transformers.DPRReader.forward(self,input_ids:Optional[Tensor]=None,attention_mask:Optional[Tensor]=None,inputs_embeds:Optional[Tensor]=None,output_attentions:bool=None,output_hidden_states:bool=None,return_dict=None)->Union[DPRReaderOutput, Tuple[Tensor, ...]]
transformers.DPRReaderOutput(ModelOutput)
transformers.modeling_dpr.DPRContextEncoder(self,config:DPRConfig)
transformers.modeling_dpr.DPRContextEncoder.__init__(self,config:DPRConfig)
transformers.modeling_dpr.DPRContextEncoder.forward(self,input_ids:Optional[Tensor]=None,attention_mask:Optional[Tensor]=None,token_type_ids:Optional[Tensor]=None,inputs_embeds:Optional[Tensor]=None,output_attentions=None,output_hidden_states=None,return_dict=None)->Union[DPRContextEncoderOutput, Tuple[Tensor, ...]]
transformers.modeling_dpr.DPRContextEncoderOutput(ModelOutput)
transformers.modeling_dpr.DPREncoder(self,config:DPRConfig)
transformers.modeling_dpr.DPREncoder.__init__(self,config:DPRConfig)
transformers.modeling_dpr.DPREncoder.embeddings_size(self)->int
transformers.modeling_dpr.DPREncoder.forward(self,input_ids:Tensor,attention_mask:Optional[Tensor]=None,token_type_ids:Optional[Tensor]=None,inputs_embeds:Optional[Tensor]=None,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=False)->Union[BaseModelOutputWithPooling, Tuple[Tensor, ...]]
transformers.modeling_dpr.DPREncoder.init_weights(self)
transformers.modeling_dpr.DPRPretrainedContextEncoder(PreTrainedModel)
transformers.modeling_dpr.DPRPretrainedContextEncoder.init_weights(self)
transformers.modeling_dpr.DPRPretrainedQuestionEncoder(PreTrainedModel)
transformers.modeling_dpr.DPRPretrainedQuestionEncoder.init_weights(self)
transformers.modeling_dpr.DPRPretrainedReader(PreTrainedModel)
transformers.modeling_dpr.DPRPretrainedReader.init_weights(self)
transformers.modeling_dpr.DPRQuestionEncoder(self,config:DPRConfig)
transformers.modeling_dpr.DPRQuestionEncoder.__init__(self,config:DPRConfig)
transformers.modeling_dpr.DPRQuestionEncoder.forward(self,input_ids:Optional[Tensor]=None,attention_mask:Optional[Tensor]=None,token_type_ids:Optional[Tensor]=None,inputs_embeds:Optional[Tensor]=None,output_attentions=None,output_hidden_states=None,return_dict=None)->Union[DPRQuestionEncoderOutput, Tuple[Tensor, ...]]
transformers.modeling_dpr.DPRQuestionEncoderOutput(ModelOutput)
transformers.modeling_dpr.DPRReader(self,config:DPRConfig)
transformers.modeling_dpr.DPRReader.__init__(self,config:DPRConfig)
transformers.modeling_dpr.DPRReader.forward(self,input_ids:Optional[Tensor]=None,attention_mask:Optional[Tensor]=None,inputs_embeds:Optional[Tensor]=None,output_attentions:bool=None,output_hidden_states:bool=None,return_dict=None)->Union[DPRReaderOutput, Tuple[Tensor, ...]]
transformers.modeling_dpr.DPRReaderOutput(ModelOutput)
transformers.modeling_dpr.DPRSpanPredictor(self,config:DPRConfig)
transformers.modeling_dpr.DPRSpanPredictor.__init__(self,config:DPRConfig)
transformers.modeling_dpr.DPRSpanPredictor.forward(self,input_ids:Tensor,attention_mask:Tensor,inputs_embeds:Optional[Tensor]=None,output_attentions:bool=False,output_hidden_states:bool=False,return_dict:bool=False)->Union[DPRReaderOutput, Tuple[Tensor, ...]]
transformers.modeling_dpr.DPRSpanPredictor.init_weights(self)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/modeling_openai.py----------------------------------------
A:transformers.modeling_openai.logger->utils.logging.get_logger(__name__)
A:transformers.modeling_openai.openai_checkpoint_folder_path->os.path.dirname(openai_checkpoint_folder_path)
A:transformers.modeling_openai.names->json.load(names_handle)
A:transformers.modeling_openai.shapes->json.load(shapes_handle)
A:transformers.modeling_openai.offsets->numpy.cumsum([np.prod(shape) for shape in shapes])
A:transformers.modeling_openai.model.tokens_embed.weight.data->torch.from_numpy(init_params[1])
A:transformers.modeling_openai.model.positions_embed.weight.data->torch.from_numpy(init_params[0])
A:transformers.modeling_openai.name->name.split('/').split('/')
A:transformers.modeling_openai.scope_names->re.split('(\\d+)', m_name)
A:transformers.modeling_openai.pointer->getattr(pointer, scope_names[0])
A:transformers.modeling_openai.num->int(scope_names[1])
A:transformers.modeling_openai.pointer.data->torch.from_numpy(array)
A:transformers.modeling_openai.self.c_attn->prune_conv1d_layer(self.c_attn, index_attn, dim=1)
A:transformers.modeling_openai.self.c_proj->Conv1D(nx, n_state)
A:transformers.modeling_openai.self.attn_dropout->torch.nn.Dropout(config.attn_pdrop)
A:transformers.modeling_openai.self.resid_dropout->torch.nn.Dropout(config.resid_pdrop)
A:transformers.modeling_openai.self.pruned_heads->self.pruned_heads.union(heads)
A:transformers.modeling_openai.(heads, index)->find_pruneable_heads_and_indices(heads, self.n_head, self.split_size // self.n_head, self.pruned_heads)
A:transformers.modeling_openai.index_attn->torch.cat([index, index + self.split_size, index + 2 * self.split_size])
A:transformers.modeling_openai.w->self.attn_dropout(w)
A:transformers.modeling_openai.x->self.c_attn(x)
A:transformers.modeling_openai.(query, key, value)->self.c_attn(x).split(self.split_size, dim=2)
A:transformers.modeling_openai.query->self.split_heads(query)
A:transformers.modeling_openai.key->self.split_heads(key, k=True)
A:transformers.modeling_openai.value->self.split_heads(value)
A:transformers.modeling_openai.attn_outputs->self.attn(x, attention_mask=attention_mask, head_mask=head_mask, output_attentions=output_attentions)
A:transformers.modeling_openai.a->self.resid_dropout(a)
A:transformers.modeling_openai.self.c_fc->Conv1D(n_state, nx)
A:transformers.modeling_openai.self.dropout->torch.nn.Dropout(config.resid_pdrop)
A:transformers.modeling_openai.h->self.ln_2(n + m)
A:transformers.modeling_openai.h2->self.c_proj(h)
A:transformers.modeling_openai.self.attn->Attention(nx, n_ctx, config, scale)
A:transformers.modeling_openai.self.ln_1->torch.nn.LayerNorm(nx, eps=config.layer_norm_epsilon)
A:transformers.modeling_openai.self.mlp->MLP(4 * nx, config)
A:transformers.modeling_openai.self.ln_2->torch.nn.LayerNorm(nx, eps=config.layer_norm_epsilon)
A:transformers.modeling_openai.n->self.ln_1(x + a)
A:transformers.modeling_openai.m->self.mlp(n)
A:transformers.modeling_openai.self.tokens_embed->torch.nn.Embedding(config.vocab_size, config.n_embd)
A:transformers.modeling_openai.self.positions_embed->torch.nn.Embedding(config.n_positions, config.n_embd)
A:transformers.modeling_openai.self.drop->torch.nn.Dropout(config.embd_pdrop)
A:transformers.modeling_openai.self.h->torch.nn.ModuleList([Block(config.n_ctx, config, scale=True) for _ in range(config.n_layer)])
A:transformers.modeling_openai.input_shape->input_ids.view(-1, input_shape[-1]).size()
A:transformers.modeling_openai.input_ids->input_ids.view(-1, input_shape[-1]).view(-1, input_shape[-1])
A:transformers.modeling_openai.attention_mask->attention_mask.to(dtype=next(self.parameters()).dtype).to(dtype=next(self.parameters()).dtype)
A:transformers.modeling_openai.head_mask->self.get_head_mask(head_mask, self.config.n_layer)
A:transformers.modeling_openai.inputs_embeds->self.tokens_embed(input_ids)
A:transformers.modeling_openai.position_embeds->self.positions_embed(position_ids)
A:transformers.modeling_openai.token_type_ids->token_type_ids.view(-1, token_type_ids.size(-1)).view(-1, token_type_ids.size(-1))
A:transformers.modeling_openai.token_type_embeds->self.tokens_embed(token_type_ids)
A:transformers.modeling_openai.hidden_states->hidden_states.view(*output_shape).view(*output_shape)
A:transformers.modeling_openai.outputs->block(hidden_states, attention_mask, head_mask[i], output_attentions=output_attentions)
A:transformers.modeling_openai.self.transformer->OpenAIGPTModel(config)
A:transformers.modeling_openai.self.lm_head->torch.nn.Linear(config.n_embd, config.vocab_size, bias=False)
A:transformers.modeling_openai.transformer_outputs->self.transformer(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.modeling_openai.lm_logits->self.lm_head(hidden_states)
A:transformers.modeling_openai.shift_logits->lm_logits[..., :-1, :].contiguous()
A:transformers.modeling_openai.shift_labels->labels[..., 1:].contiguous()
A:transformers.modeling_openai.loss_fct->CrossEntropyLoss()
A:transformers.modeling_openai.loss->loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
A:transformers.modeling_openai.self.multiple_choice_head->SequenceSummary(config)
A:transformers.modeling_openai.labels->kwargs.pop('lm_labels')
A:transformers.modeling_openai.mc_logits->self.multiple_choice_head(hidden_states, mc_token_ids).squeeze(-1)
A:transformers.modeling_openai.mc_loss->loss_fct(mc_logits.view(-1, mc_logits.size(-1)), mc_labels.view(-1))
A:transformers.modeling_openai.lm_loss->loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
transformers.OpenAIGPTDoubleHeadsModel(self,config)
transformers.OpenAIGPTDoubleHeadsModel.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,mc_token_ids=None,labels=None,mc_labels=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.OpenAIGPTDoubleHeadsModel.get_output_embeddings(self)
transformers.OpenAIGPTDoubleHeadsModelOutput(ModelOutput)
transformers.OpenAIGPTLMHeadModel(self,config)
transformers.OpenAIGPTLMHeadModel.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.OpenAIGPTLMHeadModel.get_output_embeddings(self)
transformers.OpenAIGPTModel(self,config)
transformers.OpenAIGPTModel._prune_heads(self,heads_to_prune)
transformers.OpenAIGPTModel.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.OpenAIGPTModel.get_input_embeddings(self)
transformers.OpenAIGPTModel.set_input_embeddings(self,new_embeddings)
transformers.OpenAIGPTPreTrainedModel(PreTrainedModel)
transformers.OpenAIGPTPreTrainedModel._init_weights(self,module)
transformers.load_tf_weights_in_openai_gpt(model,config,openai_checkpoint_folder_path)
transformers.modeling_openai.Attention(self,nx,n_ctx,config,scale=False)
transformers.modeling_openai.Attention.__init__(self,nx,n_ctx,config,scale=False)
transformers.modeling_openai.Attention._attn(self,q,k,v,attention_mask=None,head_mask=None,output_attentions=False)
transformers.modeling_openai.Attention.forward(self,x,attention_mask=None,head_mask=None,output_attentions=False)
transformers.modeling_openai.Attention.merge_heads(self,x)
transformers.modeling_openai.Attention.prune_heads(self,heads)
transformers.modeling_openai.Attention.split_heads(self,x,k=False)
transformers.modeling_openai.Block(self,n_ctx,config,scale=False)
transformers.modeling_openai.Block.__init__(self,n_ctx,config,scale=False)
transformers.modeling_openai.Block.forward(self,x,attention_mask=None,head_mask=None,output_attentions=False)
transformers.modeling_openai.MLP(self,n_state,config)
transformers.modeling_openai.MLP.__init__(self,n_state,config)
transformers.modeling_openai.MLP.forward(self,x)
transformers.modeling_openai.OpenAIGPTDoubleHeadsModel(self,config)
transformers.modeling_openai.OpenAIGPTDoubleHeadsModel.__init__(self,config)
transformers.modeling_openai.OpenAIGPTDoubleHeadsModel.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,mc_token_ids=None,labels=None,mc_labels=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.modeling_openai.OpenAIGPTDoubleHeadsModel.get_output_embeddings(self)
transformers.modeling_openai.OpenAIGPTDoubleHeadsModelOutput(ModelOutput)
transformers.modeling_openai.OpenAIGPTLMHeadModel(self,config)
transformers.modeling_openai.OpenAIGPTLMHeadModel.__init__(self,config)
transformers.modeling_openai.OpenAIGPTLMHeadModel.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.modeling_openai.OpenAIGPTLMHeadModel.get_output_embeddings(self)
transformers.modeling_openai.OpenAIGPTModel(self,config)
transformers.modeling_openai.OpenAIGPTModel.__init__(self,config)
transformers.modeling_openai.OpenAIGPTModel._prune_heads(self,heads_to_prune)
transformers.modeling_openai.OpenAIGPTModel.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.modeling_openai.OpenAIGPTModel.get_input_embeddings(self)
transformers.modeling_openai.OpenAIGPTModel.set_input_embeddings(self,new_embeddings)
transformers.modeling_openai.OpenAIGPTPreTrainedModel(PreTrainedModel)
transformers.modeling_openai.OpenAIGPTPreTrainedModel._init_weights(self,module)
transformers.modeling_openai.load_tf_weights_in_openai_gpt(model,config,openai_checkpoint_folder_path)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/training_args.py----------------------------------------
A:transformers.training_args.logger->utils.logging.get_logger(__name__)
A:transformers.training_args.current_time->datetime.datetime.now().strftime('%b%d_%H-%M-%S')
A:transformers.training_args.device->torch.device('cuda', self.local_rank)
A:transformers.training_args.n_gpu->torch.cuda.device_count()
A:transformers.training_args.d->dataclasses.asdict(self)
transformers.TrainingArguments
transformers.TrainingArguments.__post_init__(self)
transformers.TrainingArguments._setup_devices(self)->Tuple['torch.device', int]
transformers.TrainingArguments.device(self)->'torch.device'
transformers.TrainingArguments.eval_batch_size(self)->int
transformers.TrainingArguments.n_gpu(self)
transformers.TrainingArguments.to_json_string(self)
transformers.TrainingArguments.to_sanitized_dict(self)->Dict[str, Any]
transformers.TrainingArguments.train_batch_size(self)->int
transformers.training_args.TrainingArguments
transformers.training_args.TrainingArguments.__post_init__(self)
transformers.training_args.TrainingArguments._setup_devices(self)->Tuple['torch.device', int]
transformers.training_args.TrainingArguments.device(self)->'torch.device'
transformers.training_args.TrainingArguments.eval_batch_size(self)->int
transformers.training_args.TrainingArguments.n_gpu(self)
transformers.training_args.TrainingArguments.to_json_string(self)
transformers.training_args.TrainingArguments.to_sanitized_dict(self)->Dict[str, Any]
transformers.training_args.TrainingArguments.train_batch_size(self)->int
transformers.training_args.default_logdir()->str


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/configuration_bart.py----------------------------------------
A:transformers.configuration_bart.logger->utils.logging.get_logger(__name__)
transformers.BartConfig(self,activation_dropout=0.0,extra_pos_embeddings=2,activation_function='gelu',vocab_size=50265,d_model=1024,encoder_ffn_dim=4096,encoder_layers=12,encoder_attention_heads=16,decoder_ffn_dim=4096,decoder_layers=12,decoder_attention_heads=16,encoder_layerdrop=0.0,decoder_layerdrop=0.0,attention_dropout=0.0,dropout=0.1,max_position_embeddings=1024,init_std=0.02,classifier_dropout=0.0,num_labels=3,is_encoder_decoder=True,pad_token_id=1,bos_token_id=0,eos_token_id=2,normalize_before=False,add_final_layer_norm=False,scale_embedding=False,normalize_embedding=True,static_position_embeddings=False,add_bias_logits=False,force_bos_token_to_be_generated=False,**common_kwargs)
transformers.BartConfig.hidden_size(self)->int
transformers.BartConfig.is_valid_mbart(self)->bool
transformers.BartConfig.num_attention_heads(self)->int
transformers.configuration_bart.BartConfig(self,activation_dropout=0.0,extra_pos_embeddings=2,activation_function='gelu',vocab_size=50265,d_model=1024,encoder_ffn_dim=4096,encoder_layers=12,encoder_attention_heads=16,decoder_ffn_dim=4096,decoder_layers=12,decoder_attention_heads=16,encoder_layerdrop=0.0,decoder_layerdrop=0.0,attention_dropout=0.0,dropout=0.1,max_position_embeddings=1024,init_std=0.02,classifier_dropout=0.0,num_labels=3,is_encoder_decoder=True,pad_token_id=1,bos_token_id=0,eos_token_id=2,normalize_before=False,add_final_layer_norm=False,scale_embedding=False,normalize_embedding=True,static_position_embeddings=False,add_bias_logits=False,force_bos_token_to_be_generated=False,**common_kwargs)
transformers.configuration_bart.BartConfig.__init__(self,activation_dropout=0.0,extra_pos_embeddings=2,activation_function='gelu',vocab_size=50265,d_model=1024,encoder_ffn_dim=4096,encoder_layers=12,encoder_attention_heads=16,decoder_ffn_dim=4096,decoder_layers=12,decoder_attention_heads=16,encoder_layerdrop=0.0,decoder_layerdrop=0.0,attention_dropout=0.0,dropout=0.1,max_position_embeddings=1024,init_std=0.02,classifier_dropout=0.0,num_labels=3,is_encoder_decoder=True,pad_token_id=1,bos_token_id=0,eos_token_id=2,normalize_before=False,add_final_layer_norm=False,scale_embedding=False,normalize_embedding=True,static_position_embeddings=False,add_bias_logits=False,force_bos_token_to_be_generated=False,**common_kwargs)
transformers.configuration_bart.BartConfig.hidden_size(self)->int
transformers.configuration_bart.BartConfig.is_valid_mbart(self)->bool
transformers.configuration_bart.BartConfig.num_attention_heads(self)->int


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/convert_transfo_xl_original_tf_checkpoint_to_pytorch.py----------------------------------------
A:transformers.convert_transfo_xl_original_tf_checkpoint_to_pytorch.corpus->pickle.load(fp, encoding='latin1')
A:transformers.convert_transfo_xl_original_tf_checkpoint_to_pytorch.config_path->os.path.abspath(transfo_xl_config_file)
A:transformers.convert_transfo_xl_original_tf_checkpoint_to_pytorch.tf_path->os.path.abspath(tf_checkpoint_path)
A:transformers.convert_transfo_xl_original_tf_checkpoint_to_pytorch.config->transformers.TransfoXLConfig.from_json_file(transfo_xl_config_file)
A:transformers.convert_transfo_xl_original_tf_checkpoint_to_pytorch.model->load_tf_weights_in_transfo_xl(model, config, tf_path)
A:transformers.convert_transfo_xl_original_tf_checkpoint_to_pytorch.pytorch_weights_dump_path->os.path.join(pytorch_dump_folder_path, WEIGHTS_NAME)
A:transformers.convert_transfo_xl_original_tf_checkpoint_to_pytorch.pytorch_config_dump_path->os.path.join(pytorch_dump_folder_path, CONFIG_NAME)
A:transformers.convert_transfo_xl_original_tf_checkpoint_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.convert_transfo_xl_original_tf_checkpoint_to_pytorch.args->argparse.ArgumentParser().parse_args()
transformers.convert_transfo_xl_original_tf_checkpoint_to_pytorch.convert_transfo_xl_checkpoint_to_pytorch(tf_checkpoint_path,transfo_xl_config_file,pytorch_dump_folder_path,transfo_xl_dataset_file)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/activations.py----------------------------------------
A:transformers.activations.logger->utils.logging.get_logger(__name__)
transformers.activations._gelu_python(x)
transformers.activations.gelu_fast(x)
transformers.activations.gelu_new(x)
transformers.activations.get_activation(activation_string)
transformers.activations.swish(x)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/configuration_t5.py----------------------------------------
A:transformers.configuration_t5.logger->utils.logging.get_logger(__name__)
transformers.T5Config(self,vocab_size=32128,n_positions=512,d_model=512,d_kv=64,d_ff=2048,num_layers=6,num_heads=8,relative_attention_num_buckets=32,dropout_rate=0.1,layer_norm_epsilon=1e-06,initializer_factor=1.0,is_encoder_decoder=True,pad_token_id=0,eos_token_id=1,**kwargs)
transformers.T5Config.hidden_size(self)
transformers.T5Config.max_position_embeddings(self)
transformers.T5Config.num_attention_heads(self)
transformers.T5Config.num_hidden_layers(self)
transformers.configuration_t5.T5Config(self,vocab_size=32128,n_positions=512,d_model=512,d_kv=64,d_ff=2048,num_layers=6,num_heads=8,relative_attention_num_buckets=32,dropout_rate=0.1,layer_norm_epsilon=1e-06,initializer_factor=1.0,is_encoder_decoder=True,pad_token_id=0,eos_token_id=1,**kwargs)
transformers.configuration_t5.T5Config.__init__(self,vocab_size=32128,n_positions=512,d_model=512,d_kv=64,d_ff=2048,num_layers=6,num_heads=8,relative_attention_num_buckets=32,dropout_rate=0.1,layer_norm_epsilon=1e-06,initializer_factor=1.0,is_encoder_decoder=True,pad_token_id=0,eos_token_id=1,**kwargs)
transformers.configuration_t5.T5Config.hidden_size(self)
transformers.configuration_t5.T5Config.max_position_embeddings(self)
transformers.configuration_t5.T5Config.num_attention_heads(self)
transformers.configuration_t5.T5Config.num_hidden_layers(self)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/modeling_utils.py----------------------------------------
A:transformers.modeling_utils.logger->utils.logging.get_logger(__name__)
A:transformers.modeling_utils.mask->mask.view(-1).contiguous().eq(1).view(-1).contiguous().eq(1)
A:transformers.modeling_utils.process->psutil.Process(os.getpid())
A:transformers.modeling_utils.mem->psutil.Process(os.getpid()).memory_info()
A:transformers.modeling_utils.gen->self._named_members(get_members_fn=find_tensor_attributes)
A:transformers.modeling_utils.first_tuple->next(gen)
A:transformers.modeling_utils.encoder_extended_attention_mask->encoder_extended_attention_mask.to(dtype=self.dtype).to(dtype=self.dtype)
A:transformers.modeling_utils.seq_ids->torch.arange(seq_length, device=device)
A:transformers.modeling_utils.causal_mask->causal_mask.to(attention_mask.dtype).to(attention_mask.dtype)
A:transformers.modeling_utils.extended_attention_mask->extended_attention_mask.to(dtype=self.dtype).to(dtype=self.dtype)
A:transformers.modeling_utils.head_mask->head_mask.to(dtype=self.dtype).to(dtype=self.dtype)
A:transformers.modeling_utils.base_model->getattr(self, self.base_model_prefix, self)
A:transformers.modeling_utils.output_embeddings->self.get_output_embeddings()
A:transformers.modeling_utils.all_encoder_weights->set([module_name + '/' + sub_name for sub_name in encoder_modules.keys()])
A:transformers.modeling_utils.encoder_name->str(int(name) + encoder_layer_pos)
A:transformers.modeling_utils.output_embeddings.weight->torch.nn.Parameter(input_embeddings.weight.clone())
A:transformers.modeling_utils.output_embeddings.bias.data->torch.nn.functional.pad(output_embeddings.bias.data, (0, output_embeddings.weight.shape[0] - output_embeddings.bias.shape[0]), 'constant', 0)
A:transformers.modeling_utils.model_embeds->getattr(self, self.base_model_prefix, self)._resize_token_embeddings(new_num_tokens)
A:transformers.modeling_utils.old_embeddings->self.get_input_embeddings()
A:transformers.modeling_utils.new_embeddings->torch.nn.Embedding(new_num_tokens, old_embedding_dim)
A:transformers.modeling_utils.(old_num_tokens, old_embedding_dim)->self.get_input_embeddings().weight.size()
A:transformers.modeling_utils.num_tokens_to_copy->min(old_num_tokens, new_num_tokens)
A:transformers.modeling_utils.self.config.pruned_heads[layer]->list(union_heads)
A:transformers.modeling_utils.output_model_file->os.path.join(save_directory, WEIGHTS_NAME)
A:transformers.modeling_utils.config->kwargs.pop('config', None)
A:transformers.modeling_utils.state_dict->state_dict.copy().copy()
A:transformers.modeling_utils.cache_dir->kwargs.pop('cache_dir', None)
A:transformers.modeling_utils.from_tf->kwargs.pop('from_tf', False)
A:transformers.modeling_utils.force_download->kwargs.pop('force_download', False)
A:transformers.modeling_utils.resume_download->kwargs.pop('resume_download', False)
A:transformers.modeling_utils.proxies->kwargs.pop('proxies', None)
A:transformers.modeling_utils.output_loading_info->kwargs.pop('output_loading_info', False)
A:transformers.modeling_utils.local_files_only->kwargs.pop('local_files_only', False)
A:transformers.modeling_utils.use_cdn->kwargs.pop('use_cdn', True)
A:transformers.modeling_utils.(config, model_kwargs)->cls.config_class.from_pretrained(config_path, *model_args, cache_dir=cache_dir, return_unused_kwargs=True, force_download=force_download, resume_download=resume_download, proxies=proxies, local_files_only=local_files_only, **kwargs)
A:transformers.modeling_utils.archive_file->hf_bucket_url(pretrained_model_name_or_path, filename=TF2_WEIGHTS_NAME if from_tf else WEIGHTS_NAME, use_cdn=use_cdn)
A:transformers.modeling_utils.resolved_archive_file->cached_path(archive_file, cache_dir=cache_dir, force_download=force_download, proxies=proxies, resume_download=resume_download, local_files_only=local_files_only)
A:transformers.modeling_utils.model->torch_xla.core.xla_model.send_cpu_data_to_device(model, xm.xla_device())
A:transformers.modeling_utils.new_key->key.replace('beta', 'bias')
A:transformers.modeling_utils.state_dict[new_key]->state_dict.copy().copy().pop(old_key)
A:transformers.modeling_utils.metadata->getattr(state_dict, '_metadata', None)
A:transformers.modeling_utils.has_prefix_module->any((s.startswith(cls.base_model_prefix) for s in state_dict.keys()))
A:transformers.modeling_utils.model_to_load->getattr(model, cls.base_model_prefix)
A:transformers.modeling_utils.base_model_state_dict->getattr(model, cls.base_model_prefix).state_dict().keys()
A:transformers.modeling_utils.w->torch.empty(nx, nf)
A:transformers.modeling_utils.self.weight->torch.nn.Parameter(w)
A:transformers.modeling_utils.self.bias->torch.nn.Parameter(torch.zeros(nf))
A:transformers.modeling_utils.x->self.dense_1(x).squeeze(-1)
A:transformers.modeling_utils.self.dense->torch.nn.Linear(config.hidden_size, 1)
A:transformers.modeling_utils.self.dense_0->torch.nn.Linear(config.hidden_size * 2, config.hidden_size)
A:transformers.modeling_utils.self.activation->torch.nn.Tanh()
A:transformers.modeling_utils.self.LayerNorm->torch.nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
A:transformers.modeling_utils.self.dense_1->torch.nn.Linear(config.hidden_size, 1, bias=False)
A:transformers.modeling_utils.start_positions->start_positions[:, None, None].expand(-1, -1, hsz)
A:transformers.modeling_utils.start_states->torch.einsum('blh,bl->bh', hidden_states, start_log_probs)
A:transformers.modeling_utils.cls_index->cls_index.expand((-1,) * (cls_index.dim() - 1) + (hidden_states.size(-1),)).expand((-1,) * (cls_index.dim() - 1) + (hidden_states.size(-1),))
A:transformers.modeling_utils.cls_token_state->hidden_states.gather(-2, cls_index).squeeze(-2)
A:transformers.modeling_utils.self.start_logits->PoolerStartLogits(config)
A:transformers.modeling_utils.self.end_logits->PoolerEndLogits(config)
A:transformers.modeling_utils.self.answer_class->PoolerAnswerClass(config)
A:transformers.modeling_utils.start_logits->self.start_logits(hidden_states, p_mask=p_mask)
A:transformers.modeling_utils.end_logits->self.end_logits(hidden_states_expanded, start_states=start_states, p_mask=p_mask)
A:transformers.modeling_utils.loss_fct->CrossEntropyLoss()
A:transformers.modeling_utils.start_loss->loss_fct(start_logits, start_positions)
A:transformers.modeling_utils.end_loss->loss_fct(end_logits, end_positions)
A:transformers.modeling_utils.cls_logits->self.answer_class(hidden_states, start_states=start_states, cls_index=cls_index)
A:transformers.modeling_utils.loss_fct_cls->torch.nn.BCEWithLogitsLoss()
A:transformers.modeling_utils.cls_loss->loss_fct_cls(cls_logits, is_impossible)
A:transformers.modeling_utils.(bsz, slen, hsz)->hidden_states.size()
A:transformers.modeling_utils.start_log_probs->torch.nn.functional.softmax(start_logits, dim=-1)
A:transformers.modeling_utils.(start_top_log_probs, start_top_index)->torch.topk(start_log_probs, self.start_n_top, dim=-1)
A:transformers.modeling_utils.start_top_index_exp->start_top_index.unsqueeze(-1).expand(-1, -1, hsz)
A:transformers.modeling_utils.hidden_states_expanded->hidden_states.unsqueeze(2).expand_as(start_states)
A:transformers.modeling_utils.end_log_probs->torch.nn.functional.softmax(end_logits, dim=1)
A:transformers.modeling_utils.(end_top_log_probs, end_top_index)->torch.topk(end_log_probs, self.end_n_top, dim=1)
A:transformers.modeling_utils.end_top_log_probs->end_top_log_probs.view(-1, self.start_n_top * self.end_n_top).view(-1, self.start_n_top * self.end_n_top)
A:transformers.modeling_utils.end_top_index->end_top_index.view(-1, self.start_n_top * self.end_n_top).view(-1, self.start_n_top * self.end_n_top)
A:transformers.modeling_utils.self.summary_type->getattr(config, 'summary_type', 'last')
A:transformers.modeling_utils.self.summary->torch.nn.Linear(config.hidden_size, num_classes)
A:transformers.modeling_utils.activation_string->getattr(config, 'summary_activation', None)
A:transformers.modeling_utils.self.first_dropout->torch.nn.Dropout(config.summary_first_dropout)
A:transformers.modeling_utils.self.last_dropout->torch.nn.Dropout(config.summary_last_dropout)
A:transformers.modeling_utils.output->self.last_dropout(output)
A:transformers.modeling_utils.index->index.to(layer.weight.device).to(layer.weight.device)
A:transformers.modeling_utils.W->layer.weight.index_select(dim, index).clone().detach()
A:transformers.modeling_utils.b->layer.bias[index].clone().detach()
A:transformers.modeling_utils.new_size->list(layer.weight.size())
A:transformers.modeling_utils.new_size[dim]->len(index)
A:transformers.modeling_utils.new_layer->Conv1D(new_size[1], new_size[0]).to(layer.weight.device)
A:transformers.modeling_utils.num_args_in_forward_chunk_fn->len(inspect.signature(forward_fn).parameters)
A:transformers.modeling_utils.input_tensors_chunks->tuple((input_tensor.chunk(num_chunks, dim=chunk_dim) for input_tensor in input_tensors))
A:transformers.modeling_utils.output_chunks->tuple((forward_fn(*input_tensors_chunk) for input_tensors_chunk in zip(*input_tensors_chunks)))
transformers.Conv1D(self,nf,nx)
transformers.Conv1D.forward(self,x)
transformers.PreTrainedModel(self,config:PretrainedConfig,*inputs,**kwargs)
transformers.PreTrainedModel._get_resized_embeddings(self,old_embeddings:torch.nn.Embedding,new_num_tokens:Optional[int]=None)->torch.nn.Embedding
transformers.PreTrainedModel._resize_token_embeddings(self,new_num_tokens)
transformers.PreTrainedModel._tie_encoder_decoder_weights(encoder:nn.Module,decoder:nn.Module,base_model_prefix:str)
transformers.PreTrainedModel._tie_or_clone_weights(self,output_embeddings,input_embeddings)
transformers.PreTrainedModel.base_model(self)->nn.Module
transformers.PreTrainedModel.dummy_inputs(self)->Dict[str, torch.Tensor]
transformers.PreTrainedModel.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.PreTrainedModel.get_input_embeddings(self)->nn.Module
transformers.PreTrainedModel.get_output_embeddings(self)->nn.Module
transformers.PreTrainedModel.init_weights(self)
transformers.PreTrainedModel.prune_heads(self,heads_to_prune:Dict[int,List[int]])
transformers.PreTrainedModel.resize_token_embeddings(self,new_num_tokens:Optional[int]=None)->torch.nn.Embedding
transformers.PreTrainedModel.save_pretrained(self,save_directory)
transformers.PreTrainedModel.set_input_embeddings(self,value:nn.Module)
transformers.PreTrainedModel.tie_weights(self)
transformers.apply_chunking_to_forward(forward_fn:Callable[...,torch.Tensor],chunk_size:int,chunk_dim:int,*input_tensors)->torch.Tensor
transformers.modeling_utils.Conv1D(self,nf,nx)
transformers.modeling_utils.Conv1D.__init__(self,nf,nx)
transformers.modeling_utils.Conv1D.forward(self,x)
transformers.modeling_utils.ModuleUtilsMixin
transformers.modeling_utils.ModuleUtilsMixin._convert_head_mask_to_5d(self,head_mask,num_hidden_layers)
transformers.modeling_utils.ModuleUtilsMixin._hook_rss_memory_post_forward(module,*args,**kwargs)
transformers.modeling_utils.ModuleUtilsMixin._hook_rss_memory_pre_forward(module,*args,**kwargs)
transformers.modeling_utils.ModuleUtilsMixin.add_memory_hooks(self)
transformers.modeling_utils.ModuleUtilsMixin.device(self)->device
transformers.modeling_utils.ModuleUtilsMixin.dtype(self)->dtype
transformers.modeling_utils.ModuleUtilsMixin.get_extended_attention_mask(self,attention_mask:Tensor,input_shape:Tuple[int],device:device)->Tensor
transformers.modeling_utils.ModuleUtilsMixin.get_head_mask(self,head_mask:Optional[Tensor],num_hidden_layers:int,is_attention_chunked:bool=False)->Tensor
transformers.modeling_utils.ModuleUtilsMixin.invert_attention_mask(self,encoder_attention_mask:Tensor)->Tensor
transformers.modeling_utils.ModuleUtilsMixin.num_parameters(self,only_trainable:bool=False)->int
transformers.modeling_utils.ModuleUtilsMixin.reset_memory_hooks_state(self)
transformers.modeling_utils.PoolerAnswerClass(self,config)
transformers.modeling_utils.PoolerAnswerClass.__init__(self,config)
transformers.modeling_utils.PoolerAnswerClass.forward(self,hidden_states:torch.FloatTensor,start_states:Optional[torch.FloatTensor]=None,start_positions:Optional[torch.LongTensor]=None,cls_index:Optional[torch.LongTensor]=None)->torch.FloatTensor
transformers.modeling_utils.PoolerEndLogits(self,config:PretrainedConfig)
transformers.modeling_utils.PoolerEndLogits.__init__(self,config:PretrainedConfig)
transformers.modeling_utils.PoolerEndLogits.forward(self,hidden_states:torch.FloatTensor,start_states:Optional[torch.FloatTensor]=None,start_positions:Optional[torch.LongTensor]=None,p_mask:Optional[torch.FloatTensor]=None)->torch.FloatTensor
transformers.modeling_utils.PoolerStartLogits(self,config:PretrainedConfig)
transformers.modeling_utils.PoolerStartLogits.__init__(self,config:PretrainedConfig)
transformers.modeling_utils.PoolerStartLogits.forward(self,hidden_states:torch.FloatTensor,p_mask:Optional[torch.FloatTensor]=None)->torch.FloatTensor
transformers.modeling_utils.PreTrainedModel(self,config:PretrainedConfig,*inputs,**kwargs)
transformers.modeling_utils.PreTrainedModel.__init__(self,config:PretrainedConfig,*inputs,**kwargs)
transformers.modeling_utils.PreTrainedModel._get_resized_embeddings(self,old_embeddings:torch.nn.Embedding,new_num_tokens:Optional[int]=None)->torch.nn.Embedding
transformers.modeling_utils.PreTrainedModel._resize_token_embeddings(self,new_num_tokens)
transformers.modeling_utils.PreTrainedModel._tie_encoder_decoder_weights(encoder:nn.Module,decoder:nn.Module,base_model_prefix:str)
transformers.modeling_utils.PreTrainedModel._tie_or_clone_weights(self,output_embeddings,input_embeddings)
transformers.modeling_utils.PreTrainedModel.base_model(self)->nn.Module
transformers.modeling_utils.PreTrainedModel.dummy_inputs(self)->Dict[str, torch.Tensor]
transformers.modeling_utils.PreTrainedModel.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.modeling_utils.PreTrainedModel.get_input_embeddings(self)->nn.Module
transformers.modeling_utils.PreTrainedModel.get_output_embeddings(self)->nn.Module
transformers.modeling_utils.PreTrainedModel.init_weights(self)
transformers.modeling_utils.PreTrainedModel.prune_heads(self,heads_to_prune:Dict[int,List[int]])
transformers.modeling_utils.PreTrainedModel.resize_token_embeddings(self,new_num_tokens:Optional[int]=None)->torch.nn.Embedding
transformers.modeling_utils.PreTrainedModel.save_pretrained(self,save_directory)
transformers.modeling_utils.PreTrainedModel.set_input_embeddings(self,value:nn.Module)
transformers.modeling_utils.PreTrainedModel.tie_weights(self)
transformers.modeling_utils.SQuADHead(self,config)
transformers.modeling_utils.SQuADHead.__init__(self,config)
transformers.modeling_utils.SQuADHead.forward(self,hidden_states:torch.FloatTensor,start_positions:Optional[torch.LongTensor]=None,end_positions:Optional[torch.LongTensor]=None,cls_index:Optional[torch.LongTensor]=None,is_impossible:Optional[torch.LongTensor]=None,p_mask:Optional[torch.FloatTensor]=None,return_dict:bool=False)->Union[SquadHeadOutput, Tuple[torch.FloatTensor]]
transformers.modeling_utils.SequenceSummary(self,config:PretrainedConfig)
transformers.modeling_utils.SequenceSummary.__init__(self,config:PretrainedConfig)
transformers.modeling_utils.SequenceSummary.forward(self,hidden_states:torch.FloatTensor,cls_index:Optional[torch.LongTensor]=None)->torch.FloatTensor
transformers.modeling_utils.SquadHeadOutput(ModelOutput)
transformers.modeling_utils.apply_chunking_to_forward(forward_fn:Callable[...,torch.Tensor],chunk_size:int,chunk_dim:int,*input_tensors)->torch.Tensor
transformers.modeling_utils.find_pruneable_heads_and_indices(heads:List[int],n_heads:int,head_size:int,already_pruned_heads:Set[int])->Tuple[Set[int], torch.LongTensor]
transformers.modeling_utils.prune_conv1d_layer(layer:Conv1D,index:torch.LongTensor,dim:int=1)->Conv1D
transformers.modeling_utils.prune_layer(layer:Union[torch.nn.Linear,Conv1D],index:torch.LongTensor,dim:Optional[int]=None)->Union[torch.nn.Linear, Conv1D]
transformers.modeling_utils.prune_linear_layer(layer:torch.nn.Linear,index:torch.LongTensor,dim:int=0)->torch.nn.Linear
transformers.prune_layer(layer:Union[torch.nn.Linear,Conv1D],index:torch.LongTensor,dim:Optional[int]=None)->Union[torch.nn.Linear, Conv1D]


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/tokenization_t5.py----------------------------------------
A:transformers.tokenization_t5.logger->utils.logging.get_logger(__name__)
A:transformers.tokenization_t5.self.sp_model->sentencepiece.SentencePieceProcessor()
A:transformers.tokenization_t5.token_ids_0->self._add_eos_if_not_present(token_ids_0)
A:transformers.tokenization_t5.token_ids_1->self._add_eos_if_not_present(token_ids_1)
A:transformers.tokenization_t5.state->self.__dict__.copy()
A:transformers.tokenization_t5.pieces->self.sp_model.SampleEncodeAsPieces(text, 64, 0.1)
A:transformers.tokenization_t5.match->re.match('<extra_id_(\\d+)>', token)
A:transformers.tokenization_t5.num->int(match.group(1))
A:transformers.tokenization_t5.token->'<extra_id_{}>'.format(self.vocab_size - 1 - index)
A:transformers.tokenization_t5.out_string->self.sp_model.decode_pieces(tokens)
A:transformers.tokenization_t5.out_vocab_file->os.path.join(save_directory, VOCAB_FILES_NAMES['vocab_file'])
A:transformers.tokenization_t5.model_inputs->self(src_texts, add_special_tokens=True, return_tensors=return_tensors, max_length=max_length, padding=padding, truncation=truncation, **kwargs)
A:transformers.tokenization_t5.labels_and_decoder_mask->self(tgt_texts, add_special_tokens=True, return_tensors=return_tensors, padding=padding, max_length=max_target_length, truncation=truncation, **kwargs)
transformers.T5Tokenizer(self,vocab_file,eos_token='</s>',unk_token='<unk>',pad_token='<pad>',extra_ids=100,additional_special_tokens=None,**kwargs)
transformers.T5Tokenizer.__getstate__(self)
transformers.T5Tokenizer.__setstate__(self,d)
transformers.T5Tokenizer._add_eos_if_not_present(self,token_ids:List[int])->List[int]
transformers.T5Tokenizer._convert_id_to_token(self,index)
transformers.T5Tokenizer._convert_token_to_id(self,token)
transformers.T5Tokenizer._tokenize(self,text,sample=False)
transformers.T5Tokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.T5Tokenizer.convert_tokens_to_string(self,tokens)
transformers.T5Tokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.T5Tokenizer.get_vocab(self)
transformers.T5Tokenizer.prepare_seq2seq_batch(self,src_texts:List[str],tgt_texts:Optional[List[str]]=None,max_length:Optional[int]=None,max_target_length:Optional[int]=None,padding:str='longest',return_tensors:str=None,truncation:bool=True,**kwargs)->BatchEncoding
transformers.T5Tokenizer.save_vocabulary(self,save_directory)
transformers.T5Tokenizer.vocab_size(self)
transformers.tokenization_t5.T5Tokenizer(self,vocab_file,eos_token='</s>',unk_token='<unk>',pad_token='<pad>',extra_ids=100,additional_special_tokens=None,**kwargs)
transformers.tokenization_t5.T5Tokenizer.__getstate__(self)
transformers.tokenization_t5.T5Tokenizer.__init__(self,vocab_file,eos_token='</s>',unk_token='<unk>',pad_token='<pad>',extra_ids=100,additional_special_tokens=None,**kwargs)
transformers.tokenization_t5.T5Tokenizer.__setstate__(self,d)
transformers.tokenization_t5.T5Tokenizer._add_eos_if_not_present(self,token_ids:List[int])->List[int]
transformers.tokenization_t5.T5Tokenizer._convert_id_to_token(self,index)
transformers.tokenization_t5.T5Tokenizer._convert_token_to_id(self,token)
transformers.tokenization_t5.T5Tokenizer._tokenize(self,text,sample=False)
transformers.tokenization_t5.T5Tokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.tokenization_t5.T5Tokenizer.convert_tokens_to_string(self,tokens)
transformers.tokenization_t5.T5Tokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.tokenization_t5.T5Tokenizer.get_vocab(self)
transformers.tokenization_t5.T5Tokenizer.prepare_seq2seq_batch(self,src_texts:List[str],tgt_texts:Optional[List[str]]=None,max_length:Optional[int]=None,max_target_length:Optional[int]=None,padding:str='longest',return_tensors:str=None,truncation:bool=True,**kwargs)->BatchEncoding
transformers.tokenization_t5.T5Tokenizer.save_vocabulary(self,save_directory)
transformers.tokenization_t5.T5Tokenizer.vocab_size(self)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/modeling_tf_xlm.py----------------------------------------
A:transformers.modeling_tf_xlm.logger->utils.logging.get_logger(__name__)
A:transformers.modeling_tf_xlm.position_enc->numpy.array([[pos / np.power(10000, 2 * (j // 2) / dim) for j in range(dim)] for pos in range(n_pos)])
A:transformers.modeling_tf_xlm.out[:, 0::2]->tensorflow.constant(np.sin(position_enc[:, 0::2]))
A:transformers.modeling_tf_xlm.out[:, 1::2]->tensorflow.constant(np.cos(position_enc[:, 1::2]))
A:transformers.modeling_tf_xlm.alen->tensorflow.range(slen)
A:transformers.modeling_tf_xlm.mask->tensorflow.reshape(mask, mask_reshape)
A:transformers.modeling_tf_xlm.attn_mask->tensorflow.cast(attn_mask, dtype=dtype)
A:transformers.modeling_tf_xlm.NEW_ID->itertools.count()
A:transformers.modeling_tf_xlm.self.layer_id->next(TFMultiHeadAttention.NEW_ID)
A:transformers.modeling_tf_xlm.self.q_lin->tensorflow.keras.layers.Dense(dim, kernel_initializer=get_initializer(config.init_std), name='q_lin')
A:transformers.modeling_tf_xlm.self.k_lin->tensorflow.keras.layers.Dense(dim, kernel_initializer=get_initializer(config.init_std), name='k_lin')
A:transformers.modeling_tf_xlm.self.v_lin->tensorflow.keras.layers.Dense(dim, kernel_initializer=get_initializer(config.init_std), name='v_lin')
A:transformers.modeling_tf_xlm.self.out_lin->tensorflow.keras.layers.Dense(dim, kernel_initializer=get_initializer(config.init_std), name='out_lin')
A:transformers.modeling_tf_xlm.self.dropout->tensorflow.keras.layers.Dropout(config.dropout)
A:transformers.modeling_tf_xlm.self.pruned_heads->set()
A:transformers.modeling_tf_xlm.(bs, qlen, dim)->shape_list(input)
A:transformers.modeling_tf_xlm.q->shape(self.q_lin(input))
A:transformers.modeling_tf_xlm.k->tensorflow.concat([k_, k], axis=2)
A:transformers.modeling_tf_xlm.v->tensorflow.concat([v_, v], axis=2)
A:transformers.modeling_tf_xlm.scores->tensorflow.matmul(q, k, transpose_b=True)
A:transformers.modeling_tf_xlm.weights->self.dropout(weights, training=training)
A:transformers.modeling_tf_xlm.context->unshape(context)
A:transformers.modeling_tf_xlm.self.lin1->tensorflow.keras.layers.Dense(dim_hidden, kernel_initializer=get_initializer(config.init_std), name='lin1')
A:transformers.modeling_tf_xlm.self.lin2->tensorflow.keras.layers.Dense(out_dim, kernel_initializer=get_initializer(config.init_std), name='lin2')
A:transformers.modeling_tf_xlm.x->self.dropout(x, training=training)
A:transformers.modeling_tf_xlm.self.attention_dropout->tensorflow.keras.layers.Dropout(config.attention_dropout)
A:transformers.modeling_tf_xlm.self.position_embeddings->tensorflow.keras.layers.Embedding(config.max_position_embeddings, self.dim, embeddings_initializer=get_initializer(config.embed_init_std), name='position_embeddings')
A:transformers.modeling_tf_xlm.self.lang_embeddings->tensorflow.keras.layers.Embedding(self.n_langs, self.dim, embeddings_initializer=get_initializer(config.embed_init_std), name='lang_embeddings')
A:transformers.modeling_tf_xlm.self.embeddings->TFSharedEmbeddings(self.n_words, self.dim, initializer_range=config.embed_init_std, name='embeddings')
A:transformers.modeling_tf_xlm.self.layer_norm_emb->tensorflow.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name='layer_norm_emb')
A:transformers.modeling_tf_xlm.pruned_heads->config.pruned_heads.copy().items()
A:transformers.modeling_tf_xlm.input_ids->tensorflow.concat([inputs, mask_token], axis=1).get('input_ids')
A:transformers.modeling_tf_xlm.attention_mask->tensorflow.concat([inputs, mask_token], axis=1).get('attention_mask', attention_mask)
A:transformers.modeling_tf_xlm.langs->tensorflow.concat([inputs, mask_token], axis=1).get('langs', langs)
A:transformers.modeling_tf_xlm.token_type_ids->tensorflow.concat([inputs, mask_token], axis=1).get('token_type_ids', token_type_ids)
A:transformers.modeling_tf_xlm.position_ids->tensorflow.concat([inputs, mask_token], axis=1).get('position_ids', position_ids)
A:transformers.modeling_tf_xlm.lengths->tensorflow.concat([inputs, mask_token], axis=1).get('lengths', lengths)
A:transformers.modeling_tf_xlm.cache->tensorflow.concat([inputs, mask_token], axis=1).get('cache', cache)
A:transformers.modeling_tf_xlm.head_mask->tensorflow.concat([inputs, mask_token], axis=1).get('head_mask', head_mask)
A:transformers.modeling_tf_xlm.inputs_embeds->tensorflow.concat([inputs, mask_token], axis=1).get('inputs_embeds', inputs_embeds)
A:transformers.modeling_tf_xlm.output_attentions->tensorflow.concat([inputs, mask_token], axis=1).get('output_attentions', output_attentions)
A:transformers.modeling_tf_xlm.output_hidden_states->tensorflow.concat([inputs, mask_token], axis=1).get('output_hidden_states', output_hidden_states)
A:transformers.modeling_tf_xlm.return_dict->tensorflow.concat([inputs, mask_token], axis=1).get('return_dict', return_dict)
A:transformers.modeling_tf_xlm.(bs, slen)->shape_list(input_ids)
A:transformers.modeling_tf_xlm.(mask, attn_mask)->get_masks(slen, lengths, self.causal, padding_mask=attention_mask)
A:transformers.modeling_tf_xlm.tensor->self.layer_norm2[i](tensor)
A:transformers.modeling_tf_xlm.attn_outputs->self.attentions[i](tensor, attn_mask, None, cache, head_mask[i], output_attentions, training=training)
A:transformers.modeling_tf_xlm.attn->self.dropout(attn, training=training)
A:transformers.modeling_tf_xlm.inputs_list->tensorflow.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])
A:transformers.modeling_tf_xlm.attns_list->tensorflow.constant([[1, 1, 0, 0, 1], [1, 1, 1, 0, 0], [1, 0, 0, 1, 1]])
A:transformers.modeling_tf_xlm.langs_list->tensorflow.constant([[1, 1, 0, 0, 1], [1, 1, 1, 0, 0], [1, 0, 0, 1, 1]])
A:transformers.modeling_tf_xlm.self.transformer->TFXLMMainLayer(config, name='transformer')
A:transformers.modeling_tf_xlm.outputs->self.pred_layer(output)
A:transformers.modeling_tf_xlm.self.bias->self.add_weight(shape=(self.n_words,), initializer='zeros', trainable=True, name='bias')
A:transformers.modeling_tf_xlm.hidden_states->self.input_embeddings(hidden_states, mode='linear')
A:transformers.modeling_tf_xlm.self.pred_layer->TFXLMPredLayer(config, self.transformer.embeddings, name='pred_layer_._proj')
A:transformers.modeling_tf_xlm.inputs->tensorflow.concat([inputs, mask_token], axis=1)
A:transformers.modeling_tf_xlm.transformer_outputs->self.transformer(inputs, attention_mask=attention_mask, langs=langs, token_type_ids=token_type_ids, position_ids=position_ids, lengths=lengths, cache=cache, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, training=training)
A:transformers.modeling_tf_xlm.self.sequence_summary->TFSequenceSummary(config, initializer_range=config.init_std, name='sequence_summary')
A:transformers.modeling_tf_xlm.labels->tensorflow.concat([inputs, mask_token], axis=1).pop('labels', labels)
A:transformers.modeling_tf_xlm.logits->self.qa_outputs(sequence_output)
A:transformers.modeling_tf_xlm.self.logits_proj->tensorflow.keras.layers.Dense(1, kernel_initializer=get_initializer(config.initializer_range), name='logits_proj')
A:transformers.modeling_tf_xlm.reshaped_logits->tensorflow.reshape(logits, (-1, num_choices))
A:transformers.modeling_tf_xlm.self.classifier->tensorflow.keras.layers.Dense(config.num_labels, kernel_initializer=get_initializer(config.init_std), name='classifier')
A:transformers.modeling_tf_xlm.sequence_output->self.dropout(sequence_output, training=training)
A:transformers.modeling_tf_xlm.self.qa_outputs->tensorflow.keras.layers.Dense(config.num_labels, kernel_initializer=get_initializer(config.init_std), name='qa_outputs')
A:transformers.modeling_tf_xlm.start_positions->tensorflow.concat([inputs, mask_token], axis=1).pop('start_positions', start_positions)
A:transformers.modeling_tf_xlm.end_positions->tensorflow.concat([inputs, mask_token], axis=1).pop('end_positions', start_positions)
A:transformers.modeling_tf_xlm.(start_logits, end_logits)->tensorflow.split(logits, 2, axis=-1)
A:transformers.modeling_tf_xlm.start_logits->tensorflow.squeeze(start_logits, axis=-1)
A:transformers.modeling_tf_xlm.end_logits->tensorflow.squeeze(end_logits, axis=-1)
A:transformers.modeling_tf_xlm.loss->self.compute_loss(labels, (start_logits, end_logits))
transformers.TFXLMForMultipleChoice(self,config,*inputs,**kwargs)
transformers.TFXLMForMultipleChoice.call(self,inputs,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.TFXLMForMultipleChoice.dummy_inputs(self)
transformers.TFXLMForQuestionAnsweringSimple(self,config,*inputs,**kwargs)
transformers.TFXLMForQuestionAnsweringSimple.call(self,inputs=None,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,start_positions=None,end_positions=None,training=False)
transformers.TFXLMForSequenceClassification(self,config,*inputs,**kwargs)
transformers.TFXLMForSequenceClassification.call(self,inputs=None,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.TFXLMForTokenClassification(self,config,*inputs,**kwargs)
transformers.TFXLMForTokenClassification.call(self,inputs=None,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.TFXLMMainLayer(self,config,**kwargs)
transformers.TFXLMMainLayer._prune_heads(self,heads_to_prune)
transformers.TFXLMMainLayer._resize_token_embeddings(self,new_num_tokens)
transformers.TFXLMMainLayer.call(self,inputs,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False)
transformers.TFXLMMainLayer.get_input_embeddings(self)
transformers.TFXLMMainLayer.set_input_embeddings(self,value)
transformers.TFXLMModel(self,config,*inputs,**kwargs)
transformers.TFXLMModel.call(self,inputs,**kwargs)
transformers.TFXLMPreTrainedModel(TFPreTrainedModel)
transformers.TFXLMPreTrainedModel.dummy_inputs(self)
transformers.TFXLMWithLMHeadModel(self,config,*inputs,**kwargs)
transformers.TFXLMWithLMHeadModel.call(self,inputs,**kwargs)
transformers.TFXLMWithLMHeadModel.get_output_embeddings(self)
transformers.TFXLMWithLMHeadModel.prepare_inputs_for_generation(self,inputs,**kwargs)
transformers.TFXLMWithLMHeadModelOutput(ModelOutput)
transformers.modeling_tf_xlm.TFMultiHeadAttention(self,n_heads,dim,config,**kwargs)
transformers.modeling_tf_xlm.TFMultiHeadAttention.__init__(self,n_heads,dim,config,**kwargs)
transformers.modeling_tf_xlm.TFMultiHeadAttention.call(self,input,mask,kv,cache,head_mask,output_attentions,training=False)
transformers.modeling_tf_xlm.TFMultiHeadAttention.prune_heads(self,heads)
transformers.modeling_tf_xlm.TFTransformerFFN(self,in_dim,dim_hidden,out_dim,config,**kwargs)
transformers.modeling_tf_xlm.TFTransformerFFN.__init__(self,in_dim,dim_hidden,out_dim,config,**kwargs)
transformers.modeling_tf_xlm.TFTransformerFFN.call(self,input,training=False)
transformers.modeling_tf_xlm.TFXLMForMultipleChoice(self,config,*inputs,**kwargs)
transformers.modeling_tf_xlm.TFXLMForMultipleChoice.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_xlm.TFXLMForMultipleChoice.call(self,inputs,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.modeling_tf_xlm.TFXLMForMultipleChoice.dummy_inputs(self)
transformers.modeling_tf_xlm.TFXLMForQuestionAnsweringSimple(self,config,*inputs,**kwargs)
transformers.modeling_tf_xlm.TFXLMForQuestionAnsweringSimple.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_xlm.TFXLMForQuestionAnsweringSimple.call(self,inputs=None,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,start_positions=None,end_positions=None,training=False)
transformers.modeling_tf_xlm.TFXLMForSequenceClassification(self,config,*inputs,**kwargs)
transformers.modeling_tf_xlm.TFXLMForSequenceClassification.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_xlm.TFXLMForSequenceClassification.call(self,inputs=None,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.modeling_tf_xlm.TFXLMForTokenClassification(self,config,*inputs,**kwargs)
transformers.modeling_tf_xlm.TFXLMForTokenClassification.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_xlm.TFXLMForTokenClassification.call(self,inputs=None,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.modeling_tf_xlm.TFXLMMainLayer(self,config,**kwargs)
transformers.modeling_tf_xlm.TFXLMMainLayer.__init__(self,config,**kwargs)
transformers.modeling_tf_xlm.TFXLMMainLayer._prune_heads(self,heads_to_prune)
transformers.modeling_tf_xlm.TFXLMMainLayer._resize_token_embeddings(self,new_num_tokens)
transformers.modeling_tf_xlm.TFXLMMainLayer.call(self,inputs,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False)
transformers.modeling_tf_xlm.TFXLMMainLayer.get_input_embeddings(self)
transformers.modeling_tf_xlm.TFXLMMainLayer.set_input_embeddings(self,value)
transformers.modeling_tf_xlm.TFXLMModel(self,config,*inputs,**kwargs)
transformers.modeling_tf_xlm.TFXLMModel.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_xlm.TFXLMModel.call(self,inputs,**kwargs)
transformers.modeling_tf_xlm.TFXLMPreTrainedModel(TFPreTrainedModel)
transformers.modeling_tf_xlm.TFXLMPreTrainedModel.dummy_inputs(self)
transformers.modeling_tf_xlm.TFXLMPredLayer(self,config,input_embeddings,**kwargs)
transformers.modeling_tf_xlm.TFXLMPredLayer.__init__(self,config,input_embeddings,**kwargs)
transformers.modeling_tf_xlm.TFXLMPredLayer.build(self,input_shape)
transformers.modeling_tf_xlm.TFXLMPredLayer.call(self,hidden_states)
transformers.modeling_tf_xlm.TFXLMWithLMHeadModel(self,config,*inputs,**kwargs)
transformers.modeling_tf_xlm.TFXLMWithLMHeadModel.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_xlm.TFXLMWithLMHeadModel.call(self,inputs,**kwargs)
transformers.modeling_tf_xlm.TFXLMWithLMHeadModel.get_output_embeddings(self)
transformers.modeling_tf_xlm.TFXLMWithLMHeadModel.prepare_inputs_for_generation(self,inputs,**kwargs)
transformers.modeling_tf_xlm.TFXLMWithLMHeadModelOutput(ModelOutput)
transformers.modeling_tf_xlm.create_sinusoidal_embeddings(n_pos,dim,out)
transformers.modeling_tf_xlm.gelu(x)
transformers.modeling_tf_xlm.get_masks(slen,lengths,causal,padding_mask=None,dtype=tf.float32)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/configuration_longformer.py----------------------------------------
A:transformers.configuration_longformer.logger->utils.logging.get_logger(__name__)
transformers.LongformerConfig(self,attention_window:Union[List[int],int]=512,sep_token_id:int=2,**kwargs)
transformers.configuration_longformer.LongformerConfig(self,attention_window:Union[List[int],int]=512,sep_token_id:int=2,**kwargs)
transformers.configuration_longformer.LongformerConfig.__init__(self,attention_window:Union[List[int],int]=512,sep_token_id:int=2,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/configuration_transfo_xl.py----------------------------------------
A:transformers.configuration_transfo_xl.logger->utils.logging.get_logger(__name__)
transformers.TransfoXLConfig(self,vocab_size=267735,cutoffs=[20000,40000,200000],d_model=1024,d_embed=1024,n_head=16,d_head=64,d_inner=4096,div_val=4,pre_lnorm=False,n_layer=18,tgt_len=128,ext_len=0,mem_len=1600,clamp_len=1000,same_length=True,proj_share_all_but_first=True,attn_type=0,sample_softmax=-1,adaptive=True,dropout=0.1,dropatt=0.0,untie_r=True,init='normal',init_range=0.01,proj_init_std=0.01,init_std=0.02,layer_norm_epsilon=1e-05,eos_token_id=0,**kwargs)
transformers.TransfoXLConfig.hidden_size(self)
transformers.TransfoXLConfig.max_position_embeddings(self)
transformers.TransfoXLConfig.n_token(self)
transformers.TransfoXLConfig.n_token(self,value)
transformers.TransfoXLConfig.num_attention_heads(self)
transformers.TransfoXLConfig.num_hidden_layers(self)
transformers.configuration_transfo_xl.TransfoXLConfig(self,vocab_size=267735,cutoffs=[20000,40000,200000],d_model=1024,d_embed=1024,n_head=16,d_head=64,d_inner=4096,div_val=4,pre_lnorm=False,n_layer=18,tgt_len=128,ext_len=0,mem_len=1600,clamp_len=1000,same_length=True,proj_share_all_but_first=True,attn_type=0,sample_softmax=-1,adaptive=True,dropout=0.1,dropatt=0.0,untie_r=True,init='normal',init_range=0.01,proj_init_std=0.01,init_std=0.02,layer_norm_epsilon=1e-05,eos_token_id=0,**kwargs)
transformers.configuration_transfo_xl.TransfoXLConfig.__init__(self,vocab_size=267735,cutoffs=[20000,40000,200000],d_model=1024,d_embed=1024,n_head=16,d_head=64,d_inner=4096,div_val=4,pre_lnorm=False,n_layer=18,tgt_len=128,ext_len=0,mem_len=1600,clamp_len=1000,same_length=True,proj_share_all_but_first=True,attn_type=0,sample_softmax=-1,adaptive=True,dropout=0.1,dropatt=0.0,untie_r=True,init='normal',init_range=0.01,proj_init_std=0.01,init_std=0.02,layer_norm_epsilon=1e-05,eos_token_id=0,**kwargs)
transformers.configuration_transfo_xl.TransfoXLConfig.hidden_size(self)
transformers.configuration_transfo_xl.TransfoXLConfig.max_position_embeddings(self)
transformers.configuration_transfo_xl.TransfoXLConfig.n_token(self)
transformers.configuration_transfo_xl.TransfoXLConfig.n_token(self,value)
transformers.configuration_transfo_xl.TransfoXLConfig.num_attention_heads(self)
transformers.configuration_transfo_xl.TransfoXLConfig.num_hidden_layers(self)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/modeling_camembert.py----------------------------------------
A:transformers.modeling_camembert.logger->utils.logging.get_logger(__name__)
transformers.CamembertForCausalLM(RobertaForCausalLM)
transformers.CamembertForMaskedLM(RobertaForMaskedLM)
transformers.CamembertForMultipleChoice(RobertaForMultipleChoice)
transformers.CamembertForQuestionAnswering(RobertaForQuestionAnswering)
transformers.CamembertForSequenceClassification(RobertaForSequenceClassification)
transformers.CamembertForTokenClassification(RobertaForTokenClassification)
transformers.CamembertModel(RobertaModel)
transformers.modeling_camembert.CamembertForCausalLM(RobertaForCausalLM)
transformers.modeling_camembert.CamembertForMaskedLM(RobertaForMaskedLM)
transformers.modeling_camembert.CamembertForMultipleChoice(RobertaForMultipleChoice)
transformers.modeling_camembert.CamembertForQuestionAnswering(RobertaForQuestionAnswering)
transformers.modeling_camembert.CamembertForSequenceClassification(RobertaForSequenceClassification)
transformers.modeling_camembert.CamembertForTokenClassification(RobertaForTokenClassification)
transformers.modeling_camembert.CamembertModel(RobertaModel)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/convert_mobilebert_original_tf_checkpoint_to_pytorch.py----------------------------------------
A:transformers.convert_mobilebert_original_tf_checkpoint_to_pytorch.config->transformers.MobileBertConfig.from_json_file(mobilebert_config_file)
A:transformers.convert_mobilebert_original_tf_checkpoint_to_pytorch.model->load_tf_weights_in_mobilebert(model, config, tf_checkpoint_path)
A:transformers.convert_mobilebert_original_tf_checkpoint_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.convert_mobilebert_original_tf_checkpoint_to_pytorch.args->argparse.ArgumentParser().parse_args()
transformers.convert_mobilebert_original_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(tf_checkpoint_path,mobilebert_config_file,pytorch_dump_path)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/modeling_marian.py----------------------------------------
A:transformers.modeling_marian.logits[:, self.config.pad_token_id]->float('-inf')
transformers.MarianMTModel(BartForConditionalGeneration)
transformers.MarianMTModel.adjust_logits_during_generation(self,logits,cur_len,max_length)
transformers.modeling_marian.MarianMTModel(BartForConditionalGeneration)
transformers.modeling_marian.MarianMTModel.adjust_logits_during_generation(self,logits,cur_len,max_length)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/convert_dpr_original_checkpoint_to_pytorch.py----------------------------------------
A:transformers.convert_dpr_original_checkpoint_to_pytorch.CheckpointState->collections.namedtuple('CheckpointState', ['model_dict', 'optimizer_dict', 'scheduler_dict', 'offset', 'epoch', 'encoder_params'])
A:transformers.convert_dpr_original_checkpoint_to_pytorch.state_dict->torch.load(model_file, map_location=lambda s, l: default_restore_location(s, 'cpu'))
A:transformers.convert_dpr_original_checkpoint_to_pytorch.model->DPRState.from_type(comp_type, src_file=src_file).load_dpr_model()
A:transformers.convert_dpr_original_checkpoint_to_pytorch.saved_state->load_states_from_checkpoint(self.src_file)
A:transformers.convert_dpr_original_checkpoint_to_pytorch.dest_dir->Path(dest_dir)
A:transformers.convert_dpr_original_checkpoint_to_pytorch.dpr_state->DPRState.from_type(comp_type, src_file=src_file)
A:transformers.convert_dpr_original_checkpoint_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.convert_dpr_original_checkpoint_to_pytorch.args->argparse.ArgumentParser().parse_args()
A:transformers.convert_dpr_original_checkpoint_to_pytorch.src_file->Path(args.src)
transformers.convert_dpr_original_checkpoint_to_pytorch.DPRContextEncoderState(DPRState)
transformers.convert_dpr_original_checkpoint_to_pytorch.DPRContextEncoderState.load_dpr_model(self)
transformers.convert_dpr_original_checkpoint_to_pytorch.DPRQuestionEncoderState(DPRState)
transformers.convert_dpr_original_checkpoint_to_pytorch.DPRQuestionEncoderState.load_dpr_model(self)
transformers.convert_dpr_original_checkpoint_to_pytorch.DPRReaderState(DPRState)
transformers.convert_dpr_original_checkpoint_to_pytorch.DPRReaderState.load_dpr_model(self)
transformers.convert_dpr_original_checkpoint_to_pytorch.DPRState(self,src_file:Path)
transformers.convert_dpr_original_checkpoint_to_pytorch.DPRState.__init__(self,src_file:Path)
transformers.convert_dpr_original_checkpoint_to_pytorch.DPRState.from_type(comp_type:str,*args,**kwargs)->'DPRState'
transformers.convert_dpr_original_checkpoint_to_pytorch.DPRState.load_dpr_model(self)
transformers.convert_dpr_original_checkpoint_to_pytorch.convert(comp_type:str,src_file:Path,dest_dir:Path)
transformers.convert_dpr_original_checkpoint_to_pytorch.load_states_from_checkpoint(model_file:str)->CheckpointState


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/trainer.py----------------------------------------
A:transformers.trainer.logger->utils.logging.get_logger(__name__)
A:transformers.trainer.num_replicas->torch.distributed.get_world_size()
A:transformers.trainer.rank->torch.distributed.get_rank()
A:transformers.trainer.self.num_samples->int(math.ceil(len(self.dataset) * 1.0 / self.num_replicas))
A:transformers.trainer.indices->list(range(len(self.dataset)))
A:transformers.trainer.args->TrainingArguments('tmp_trainer')
A:transformers.trainer.model->torch.nn.DataParallel(model)
A:transformers.trainer.self.args.prediction_loss_only->kwargs.pop('prediction_loss_only')
A:transformers.trainer.self.tb_writer->SummaryWriter(log_dir=self.args.logging_dir)
A:transformers.trainer.self.scaler->torch.cuda.amp.GradScaler()
A:transformers.trainer.signature->inspect.signature(self.model.forward)
A:transformers.trainer.signature_columns->list(signature.parameters.keys())
A:transformers.trainer.ignored_columns->list(set(dataset.column_names) - set(signature_columns))
A:transformers.trainer.train_sampler->self._get_train_sampler()
A:transformers.trainer.eval_sampler->self._get_eval_sampler(eval_dataset)
A:transformers.trainer.test_sampler->self._get_eval_sampler(test_dataset)
A:transformers.trainer.self.optimizer->AdamW(optimizer_grouped_parameters, lr=self.args.learning_rate, betas=(self.args.adam_beta1, self.args.adam_beta2), eps=self.args.adam_epsilon)
A:transformers.trainer.self.lr_scheduler->get_linear_schedule_with_warmup(self.optimizer, num_warmup_steps=self.args.warmup_steps, num_training_steps=num_training_steps)
A:transformers.trainer.comet_mode->os.getenv('COMET_MODE', 'ONLINE').upper()
A:transformers.trainer.experiment->comet_ml.config.get_global_experiment()
A:transformers.trainer.args['offline_directory']->os.getenv('COMET_OFFLINE_DIRECTORY', './')
A:transformers.trainer.old_attr->getattr(self.args, key, None)
A:transformers.trainer.value->type(old_attr)(value)
A:transformers.trainer.self.objective->self.compute_objective(metrics)
A:transformers.trainer.output_dir->os.path.join(self.args.output_dir, checkpoint_folder)
A:transformers.trainer.self.model->torch.nn.DataParallel(model).to(self.args.device)
A:transformers.trainer.train_dataloader->self.get_train_dataloader()
A:transformers.trainer.t_total->int(len(train_dataloader) // self.args.gradient_accumulation_steps * self.args.num_train_epochs)
A:transformers.trainer.(model, self.optimizer)->apex.amp.initialize(model, self.optimizer, opt_level=self.args.fp16_opt_level)
A:transformers.trainer.self.global_step->int(model_path.split('-')[-1].split(os.path.sep)[0])
A:transformers.trainer.tr_loss->torch.tensor(0.0).to(self.args.device)
A:transformers.trainer.train_pbar->trange(epochs_trained, int(np.ceil(num_train_epochs)), desc='Epoch', disable=disable_tqdm)
A:transformers.trainer.parallel_loader->torch_xla.distributed.parallel_loader.ParallelLoader(train_dataloader, [self.args.device]).per_device_loader(self.args.device)
A:transformers.trainer.epoch_pbar->tqdm(epoch_iterator, desc='Iteration', disable=disable_tqdm)
A:transformers.trainer.tr_loss_scalar->torch.tensor(0.0).to(self.args.device).item()
A:transformers.trainer.metrics->self.compute_metrics(EvalPrediction(predictions=preds, label_ids=label_ids))
A:transformers.trainer.backend->HPSearchBackend(backend)
A:transformers.trainer.best_run->run_hp_search(self, n_trials, direction, **kwargs)
A:transformers.trainer.inputs[k]->v.to(self.args.device)
A:transformers.trainer.inputs->self._prepare_inputs(inputs)
A:transformers.trainer.outputs->model(**inputs)
A:transformers.trainer.loss->loss.mean().item().mean().item()
A:transformers.trainer.regex_match->re.match(f'.*{checkpoint_prefix}-([0-9]+)', path)
A:transformers.trainer.checkpoints_sorted->self._sorted_checkpoints(use_mtime=use_mtime)
A:transformers.trainer.number_of_checkpoints_to_delete->max(0, len(checkpoints_sorted) - self.args.save_total_limit)
A:transformers.trainer.eval_dataloader->self.get_eval_dataloader(eval_dataset)
A:transformers.trainer.output->self.prediction_loop(eval_dataloader, description='Evaluation')
A:transformers.trainer.test_dataloader->self.get_test_dataloader(test_dataset)
A:transformers.trainer.dataloader->torch_xla.distributed.parallel_loader.ParallelLoader(dataloader, [self.args.device]).per_device_loader(self.args.device)
A:transformers.trainer.(loss, logits, labels)->self.prediction_step(model, inputs, prediction_loss_only)
A:transformers.trainer.preds->preds.cpu().numpy().cpu().numpy()
A:transformers.trainer.label_ids->label_ids.cpu().numpy().cpu().numpy()
A:transformers.trainer.eval_losses->torch_xla.core.xla_model.mesh_reduce('eval_losses', torch.tensor(eval_losses), torch.cat).tolist()
A:transformers.trainer.samples_count->sum(xm.mesh_reduce('samples_count', torch.tensor([samples_count]), torch.cat).tolist())
A:transformers.trainer.metrics[f'eval_{key}']->self.compute_metrics(EvalPrediction(predictions=preds, label_ids=label_ids)).pop(key)
A:transformers.trainer.concat->torch.cat(output_tensors, dim=0)
A:transformers.trainer.has_labels->any((inputs.get(k) is not None for k in ['labels', 'lm_labels', 'masked_lm_labels']))
A:transformers.trainer.labels->labels.detach().detach()
transformers.Trainer(self,model:PreTrainedModel=None,args:TrainingArguments=None,data_collator:Optional[DataCollator]=None,train_dataset:Optional[Dataset]=None,eval_dataset:Optional[Dataset]=None,tokenizer:Optional['PreTrainedTokenizerBase']=None,model_init:Callable[[],PreTrainedModel]=None,compute_metrics:Optional[Callable[[EvalPrediction],Dict]]=None,tb_writer:Optional['SummaryWriter']=None,optimizers:Tuple[torch.optim.Optimizer,torch.optim.lr_scheduler.LambdaLR]=(None,None),**kwargs)
transformers.Trainer._get_eval_sampler(self,eval_dataset:Dataset)->Optional[torch.utils.data.sampler.Sampler]
transformers.Trainer._get_train_sampler(self)->Optional[torch.utils.data.sampler.Sampler]
transformers.Trainer._hp_search_setup(self,trial:Union['optuna.Trial',Dict[str,Any]])
transformers.Trainer._prepare_inputs(self,inputs:Dict[str,Union[torch.Tensor,Any]])->Dict[str, Union[torch.Tensor, Any]]
transformers.Trainer._remove_unused_columns(self,dataset:'nlp.Dataset',description:Optional[str]=None)
transformers.Trainer._report_to_hp_search(self,trial:Union['optuna.Trial',Dict[str,Any]],epoch:int,metrics:Dict[str,float])
transformers.Trainer._rotate_checkpoints(self,use_mtime=False)->None
transformers.Trainer._save(self,output_dir:Optional[str]=None)
transformers.Trainer._save_tpu(self,output_dir:Optional[str]=None)
transformers.Trainer._sorted_checkpoints(self,checkpoint_prefix=PREFIX_CHECKPOINT_DIR,use_mtime=False)->List[str]
transformers.Trainer._tune_save_checkpoint(self)
transformers.Trainer.create_optimizer_and_scheduler(self,num_training_steps:int)
transformers.Trainer.distributed_concat(self,tensor:torch.Tensor,num_total_examples:int)->torch.Tensor
transformers.Trainer.evaluate(self,eval_dataset:Optional[Dataset]=None)->Dict[str, float]
transformers.Trainer.get_eval_dataloader(self,eval_dataset:Optional[Dataset]=None)->DataLoader
transformers.Trainer.get_test_dataloader(self,test_dataset:Dataset)->DataLoader
transformers.Trainer.get_train_dataloader(self)->DataLoader
transformers.Trainer.hyperparameter_search(self,hp_space:Optional[Callable[['optuna.Trial'],Dict[str,float]]]=None,compute_objective:Optional[Callable[[Dict[str,float]],float]]=None,n_trials:int=20,direction:str='minimize',backend:Optional[Union['str',HPSearchBackend]]=None,**kwargs)->BestRun
transformers.Trainer.is_local_master(self)->bool
transformers.Trainer.is_local_process_zero(self)->bool
transformers.Trainer.is_world_master(self)->bool
transformers.Trainer.is_world_process_zero(self)->bool
transformers.Trainer.log(self,logs:Dict[str,float],iterator:Optional[tqdm]=None)->None
transformers.Trainer.num_examples(self,dataloader:DataLoader)->int
transformers.Trainer.predict(self,test_dataset:Dataset)->PredictionOutput
transformers.Trainer.prediction_loop(self,dataloader:DataLoader,description:str,prediction_loss_only:Optional[bool]=None)->PredictionOutput
transformers.Trainer.prediction_step(self,model:nn.Module,inputs:Dict[str,Union[torch.Tensor,Any]],prediction_loss_only:bool)->Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]
transformers.Trainer.save_model(self,output_dir:Optional[str]=None)
transformers.Trainer.setup_comet(self)
transformers.Trainer.setup_wandb(self)
transformers.Trainer.train(self,model_path:Optional[str]=None,trial:Union['optuna.Trial',Dict[str,Any]]=None)
transformers.Trainer.training_step(self,model:nn.Module,inputs:Dict[str,Union[torch.Tensor,Any]])->torch.Tensor
transformers.torch_distributed_zero_first(local_rank:int)
transformers.trainer.SequentialDistributedSampler(self,dataset,num_replicas=None,rank=None)
transformers.trainer.SequentialDistributedSampler.__init__(self,dataset,num_replicas=None,rank=None)
transformers.trainer.SequentialDistributedSampler.__iter__(self)
transformers.trainer.SequentialDistributedSampler.__len__(self)
transformers.trainer.Trainer(self,model:PreTrainedModel=None,args:TrainingArguments=None,data_collator:Optional[DataCollator]=None,train_dataset:Optional[Dataset]=None,eval_dataset:Optional[Dataset]=None,tokenizer:Optional['PreTrainedTokenizerBase']=None,model_init:Callable[[],PreTrainedModel]=None,compute_metrics:Optional[Callable[[EvalPrediction],Dict]]=None,tb_writer:Optional['SummaryWriter']=None,optimizers:Tuple[torch.optim.Optimizer,torch.optim.lr_scheduler.LambdaLR]=(None,None),**kwargs)
transformers.trainer.Trainer.__init__(self,model:PreTrainedModel=None,args:TrainingArguments=None,data_collator:Optional[DataCollator]=None,train_dataset:Optional[Dataset]=None,eval_dataset:Optional[Dataset]=None,tokenizer:Optional['PreTrainedTokenizerBase']=None,model_init:Callable[[],PreTrainedModel]=None,compute_metrics:Optional[Callable[[EvalPrediction],Dict]]=None,tb_writer:Optional['SummaryWriter']=None,optimizers:Tuple[torch.optim.Optimizer,torch.optim.lr_scheduler.LambdaLR]=(None,None),**kwargs)
transformers.trainer.Trainer._get_eval_sampler(self,eval_dataset:Dataset)->Optional[torch.utils.data.sampler.Sampler]
transformers.trainer.Trainer._get_train_sampler(self)->Optional[torch.utils.data.sampler.Sampler]
transformers.trainer.Trainer._hp_search_setup(self,trial:Union['optuna.Trial',Dict[str,Any]])
transformers.trainer.Trainer._prepare_inputs(self,inputs:Dict[str,Union[torch.Tensor,Any]])->Dict[str, Union[torch.Tensor, Any]]
transformers.trainer.Trainer._remove_unused_columns(self,dataset:'nlp.Dataset',description:Optional[str]=None)
transformers.trainer.Trainer._report_to_hp_search(self,trial:Union['optuna.Trial',Dict[str,Any]],epoch:int,metrics:Dict[str,float])
transformers.trainer.Trainer._rotate_checkpoints(self,use_mtime=False)->None
transformers.trainer.Trainer._save(self,output_dir:Optional[str]=None)
transformers.trainer.Trainer._save_tpu(self,output_dir:Optional[str]=None)
transformers.trainer.Trainer._sorted_checkpoints(self,checkpoint_prefix=PREFIX_CHECKPOINT_DIR,use_mtime=False)->List[str]
transformers.trainer.Trainer._tune_save_checkpoint(self)
transformers.trainer.Trainer.create_optimizer_and_scheduler(self,num_training_steps:int)
transformers.trainer.Trainer.distributed_concat(self,tensor:torch.Tensor,num_total_examples:int)->torch.Tensor
transformers.trainer.Trainer.evaluate(self,eval_dataset:Optional[Dataset]=None)->Dict[str, float]
transformers.trainer.Trainer.get_eval_dataloader(self,eval_dataset:Optional[Dataset]=None)->DataLoader
transformers.trainer.Trainer.get_test_dataloader(self,test_dataset:Dataset)->DataLoader
transformers.trainer.Trainer.get_train_dataloader(self)->DataLoader
transformers.trainer.Trainer.hyperparameter_search(self,hp_space:Optional[Callable[['optuna.Trial'],Dict[str,float]]]=None,compute_objective:Optional[Callable[[Dict[str,float]],float]]=None,n_trials:int=20,direction:str='minimize',backend:Optional[Union['str',HPSearchBackend]]=None,**kwargs)->BestRun
transformers.trainer.Trainer.is_local_master(self)->bool
transformers.trainer.Trainer.is_local_process_zero(self)->bool
transformers.trainer.Trainer.is_world_master(self)->bool
transformers.trainer.Trainer.is_world_process_zero(self)->bool
transformers.trainer.Trainer.log(self,logs:Dict[str,float],iterator:Optional[tqdm]=None)->None
transformers.trainer.Trainer.num_examples(self,dataloader:DataLoader)->int
transformers.trainer.Trainer.predict(self,test_dataset:Dataset)->PredictionOutput
transformers.trainer.Trainer.prediction_loop(self,dataloader:DataLoader,description:str,prediction_loss_only:Optional[bool]=None)->PredictionOutput
transformers.trainer.Trainer.prediction_step(self,model:nn.Module,inputs:Dict[str,Union[torch.Tensor,Any]],prediction_loss_only:bool)->Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]
transformers.trainer.Trainer.save_model(self,output_dir:Optional[str]=None)
transformers.trainer.Trainer.setup_comet(self)
transformers.trainer.Trainer.setup_wandb(self)
transformers.trainer.Trainer.train(self,model_path:Optional[str]=None,trial:Union['optuna.Trial',Dict[str,Any]]=None)
transformers.trainer.Trainer.training_step(self,model:nn.Module,inputs:Dict[str,Union[torch.Tensor,Any]])->torch.Tensor
transformers.trainer.get_tpu_sampler(dataset:Dataset)
transformers.trainer.torch_distributed_zero_first(local_rank:int)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/tokenization_retribert.py----------------------------------------
A:transformers.tokenization_retribert.logger->utils.logging.get_logger(__name__)
transformers.RetriBertTokenizer(BertTokenizer)
transformers.RetriBertTokenizerFast(BertTokenizerFast)
transformers.tokenization_retribert.RetriBertTokenizer(BertTokenizer)
transformers.tokenization_retribert.RetriBertTokenizerFast(BertTokenizerFast)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/modeling_albert.py----------------------------------------
A:transformers.modeling_albert.logger->utils.logging.get_logger(__name__)
A:transformers.modeling_albert.tf_path->os.path.abspath(tf_checkpoint_path)
A:transformers.modeling_albert.init_vars->tensorflow.train.list_variables(tf_path)
A:transformers.modeling_albert.array->numpy.transpose(array)
A:transformers.modeling_albert.name->name.split('/').split('/')
A:transformers.modeling_albert.scope_names->re.split('_(\\d+)', m_name)
A:transformers.modeling_albert.pointer->getattr(pointer, 'weight')
A:transformers.modeling_albert.num->int(scope_names[1])
A:transformers.modeling_albert.pointer.data->torch.from_numpy(array)
A:transformers.modeling_albert.self.word_embeddings->torch.nn.Embedding(config.vocab_size, config.embedding_size, padding_idx=config.pad_token_id)
A:transformers.modeling_albert.self.position_embeddings->torch.nn.Embedding(config.max_position_embeddings, config.embedding_size)
A:transformers.modeling_albert.self.token_type_embeddings->torch.nn.Embedding(config.type_vocab_size, config.embedding_size)
A:transformers.modeling_albert.self.LayerNorm->torch.nn.LayerNorm(config.embedding_size)
A:transformers.modeling_albert.self.attention_dropout->torch.nn.Dropout(config.attention_probs_dropout_prob)
A:transformers.modeling_albert.self.output_dropout->torch.nn.Dropout(config.hidden_dropout_prob)
A:transformers.modeling_albert.self.dense->torch.nn.Linear(config.hidden_size, config.embedding_size)
A:transformers.modeling_albert.self.pruned_heads->self.pruned_heads.union(heads)
A:transformers.modeling_albert.(heads, index)->find_pruneable_heads_and_indices(heads, self.num_attention_heads, self.attention_head_size, self.pruned_heads)
A:transformers.modeling_albert.self.query->prune_linear_layer(self.query, index)
A:transformers.modeling_albert.self.key->prune_linear_layer(self.key, index)
A:transformers.modeling_albert.self.value->prune_linear_layer(self.value, index)
A:transformers.modeling_albert.mixed_query_layer->self.query(input_ids)
A:transformers.modeling_albert.mixed_key_layer->self.key(input_ids)
A:transformers.modeling_albert.mixed_value_layer->self.value(input_ids)
A:transformers.modeling_albert.query_layer->self.transpose_for_scores(mixed_query_layer)
A:transformers.modeling_albert.key_layer->self.transpose_for_scores(mixed_key_layer)
A:transformers.modeling_albert.value_layer->self.transpose_for_scores(mixed_value_layer)
A:transformers.modeling_albert.attention_scores->torch.matmul(query_layer, key_layer.transpose(-1, -2))
A:transformers.modeling_albert.attention_probs->self.attention_dropout(attention_probs)
A:transformers.modeling_albert.context_layer->context_layer.permute(0, 2, 1, 3).contiguous().permute(0, 2, 1, 3).contiguous()
A:transformers.modeling_albert.w->self.dense.weight.t().view(self.num_attention_heads, self.attention_head_size, self.hidden_size).to(context_layer.dtype)
A:transformers.modeling_albert.b->self.dense.bias.to(context_layer.dtype)
A:transformers.modeling_albert.projected_context_layer_dropout->self.output_dropout(projected_context_layer)
A:transformers.modeling_albert.layernormed_context_layer->self.LayerNorm(input_ids + projected_context_layer_dropout)
A:transformers.modeling_albert.self.full_layer_layer_norm->torch.nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
A:transformers.modeling_albert.self.attention->AlbertAttention(config)
A:transformers.modeling_albert.self.ffn->torch.nn.Linear(config.hidden_size, config.intermediate_size)
A:transformers.modeling_albert.self.ffn_output->torch.nn.Linear(config.intermediate_size, config.hidden_size)
A:transformers.modeling_albert.self.dropout->torch.nn.Dropout(config.hidden_dropout_prob)
A:transformers.modeling_albert.attention_output->self.attention(hidden_states, attention_mask, head_mask, output_attentions)
A:transformers.modeling_albert.ffn_output->self.ffn_output(ffn_output)
A:transformers.modeling_albert.hidden_states->self.decoder(hidden_states)
A:transformers.modeling_albert.self.albert_layers->torch.nn.ModuleList([AlbertLayer(config) for _ in range(config.inner_group_num)])
A:transformers.modeling_albert.layer_output->albert_layer(hidden_states, attention_mask, head_mask[layer_index], output_attentions)
A:transformers.modeling_albert.self.embedding_hidden_mapping_in->torch.nn.Linear(config.embedding_size, config.hidden_size)
A:transformers.modeling_albert.self.albert_layer_groups->torch.nn.ModuleList([AlbertLayerGroup(config) for _ in range(config.num_hidden_groups)])
A:transformers.modeling_albert.layers_per_group->int(self.config.num_hidden_layers / self.config.num_hidden_groups)
A:transformers.modeling_albert.group_idx->int(layer / self.config.inner_group_num)
A:transformers.modeling_albert.layer_group_output->self.albert_layer_groups[group_idx](hidden_states, attention_mask, head_mask[group_idx * layers_per_group:(group_idx + 1) * layers_per_group], output_attentions, output_hidden_states)
A:transformers.modeling_albert.self.embeddings->AlbertEmbeddings(config)
A:transformers.modeling_albert.self.encoder->AlbertTransformer(config)
A:transformers.modeling_albert.self.pooler->torch.nn.Linear(config.hidden_size, config.hidden_size)
A:transformers.modeling_albert.self.pooler_activation->torch.nn.Tanh()
A:transformers.modeling_albert.new_embeddings->self._get_resized_embeddings(old_embeddings, new_num_tokens)
A:transformers.modeling_albert.inner_group_idx->int(layer - group_idx * self.config.inner_group_num)
A:transformers.modeling_albert.input_shape->input_ids.size()
A:transformers.modeling_albert.attention_mask->torch.ones(input_shape, device=device)
A:transformers.modeling_albert.token_type_ids->torch.zeros(input_shape, dtype=torch.long, device=device)
A:transformers.modeling_albert.extended_attention_mask->extended_attention_mask.to(dtype=self.dtype).to(dtype=self.dtype)
A:transformers.modeling_albert.head_mask->self.get_head_mask(head_mask, self.config.num_hidden_layers)
A:transformers.modeling_albert.embedding_output->self.embeddings(input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds)
A:transformers.modeling_albert.encoder_outputs->self.encoder(embedding_output, extended_attention_mask, head_mask=head_mask, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.modeling_albert.pooled_output->self.dropout(pooled_output)
A:transformers.modeling_albert.self.albert->AlbertModel(config)
A:transformers.modeling_albert.self.predictions->AlbertMLMHead(config)
A:transformers.modeling_albert.self.sop_classifier->AlbertSOPHead(config)
A:transformers.modeling_albert.labels->kwargs.pop('masked_lm_labels')
A:transformers.modeling_albert.outputs->self.albert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.modeling_albert.prediction_scores->self.predictions(sequence_outputs)
A:transformers.modeling_albert.sop_scores->self.sop_classifier(pooled_output)
A:transformers.modeling_albert.loss_fct->CrossEntropyLoss()
A:transformers.modeling_albert.masked_lm_loss->loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))
A:transformers.modeling_albert.sentence_order_loss->loss_fct(sop_scores.view(-1, 2), sentence_order_label.view(-1))
A:transformers.modeling_albert.self.bias->torch.nn.Parameter(torch.zeros(config.vocab_size))
A:transformers.modeling_albert.self.decoder->torch.nn.Linear(config.embedding_size, config.vocab_size)
A:transformers.modeling_albert.self.classifier->torch.nn.Linear(config.hidden_size, 1)
A:transformers.modeling_albert.dropout_pooled_output->self.dropout(pooled_output)
A:transformers.modeling_albert.logits->self.classifier(pooled_output)
A:transformers.modeling_albert.loss->loss_fct(reshaped_logits, labels)
A:transformers.modeling_albert.sequence_output->self.dropout(sequence_output)
A:transformers.modeling_albert.self.qa_outputs->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.modeling_albert.(start_logits, end_logits)->self.classifier(pooled_output).split(1, dim=-1)
A:transformers.modeling_albert.start_logits->start_logits.squeeze(-1).squeeze(-1)
A:transformers.modeling_albert.end_logits->end_logits.squeeze(-1).squeeze(-1)
A:transformers.modeling_albert.start_positions->start_positions.squeeze(-1).squeeze(-1)
A:transformers.modeling_albert.end_positions->end_positions.squeeze(-1).squeeze(-1)
A:transformers.modeling_albert.ignored_index->start_logits.squeeze(-1).squeeze(-1).size(1)
A:transformers.modeling_albert.start_loss->loss_fct(start_logits, start_positions)
A:transformers.modeling_albert.end_loss->loss_fct(end_logits, end_positions)
A:transformers.modeling_albert.reshaped_logits->self.classifier(pooled_output).view(-1, num_choices)
transformers.AlbertForMaskedLM(self,config)
transformers.AlbertForMaskedLM.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.AlbertForMaskedLM.get_input_embeddings(self)
transformers.AlbertForMaskedLM.get_output_embeddings(self)
transformers.AlbertForMultipleChoice(self,config)
transformers.AlbertForMultipleChoice.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.AlbertForPreTraining(self,config)
transformers.AlbertForPreTraining.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,sentence_order_label=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.AlbertForPreTraining.get_input_embeddings(self)
transformers.AlbertForPreTraining.get_output_embeddings(self)
transformers.AlbertForPreTrainingOutput(ModelOutput)
transformers.AlbertForQuestionAnswering(self,config)
transformers.AlbertForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,start_positions=None,end_positions=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.AlbertForSequenceClassification(self,config)
transformers.AlbertForSequenceClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.AlbertForTokenClassification(self,config)
transformers.AlbertForTokenClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.AlbertModel(self,config)
transformers.AlbertModel._prune_heads(self,heads_to_prune)
transformers.AlbertModel._resize_token_embeddings(self,new_num_tokens)
transformers.AlbertModel.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.AlbertModel.get_input_embeddings(self)
transformers.AlbertModel.set_input_embeddings(self,value)
transformers.AlbertPreTrainedModel(PreTrainedModel)
transformers.AlbertPreTrainedModel._init_weights(self,module)
transformers.load_tf_weights_in_albert(model,config,tf_checkpoint_path)
transformers.modeling_albert.AlbertAttention(self,config)
transformers.modeling_albert.AlbertAttention.__init__(self,config)
transformers.modeling_albert.AlbertAttention.forward(self,input_ids,attention_mask=None,head_mask=None,output_attentions=False)
transformers.modeling_albert.AlbertAttention.prune_heads(self,heads)
transformers.modeling_albert.AlbertEmbeddings(self,config)
transformers.modeling_albert.AlbertEmbeddings.__init__(self,config)
transformers.modeling_albert.AlbertForMaskedLM(self,config)
transformers.modeling_albert.AlbertForMaskedLM.__init__(self,config)
transformers.modeling_albert.AlbertForMaskedLM.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.modeling_albert.AlbertForMaskedLM.get_input_embeddings(self)
transformers.modeling_albert.AlbertForMaskedLM.get_output_embeddings(self)
transformers.modeling_albert.AlbertForMultipleChoice(self,config)
transformers.modeling_albert.AlbertForMultipleChoice.__init__(self,config)
transformers.modeling_albert.AlbertForMultipleChoice.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.modeling_albert.AlbertForPreTraining(self,config)
transformers.modeling_albert.AlbertForPreTraining.__init__(self,config)
transformers.modeling_albert.AlbertForPreTraining.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,sentence_order_label=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.modeling_albert.AlbertForPreTraining.get_input_embeddings(self)
transformers.modeling_albert.AlbertForPreTraining.get_output_embeddings(self)
transformers.modeling_albert.AlbertForPreTrainingOutput(ModelOutput)
transformers.modeling_albert.AlbertForQuestionAnswering(self,config)
transformers.modeling_albert.AlbertForQuestionAnswering.__init__(self,config)
transformers.modeling_albert.AlbertForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,start_positions=None,end_positions=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.modeling_albert.AlbertForSequenceClassification(self,config)
transformers.modeling_albert.AlbertForSequenceClassification.__init__(self,config)
transformers.modeling_albert.AlbertForSequenceClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.modeling_albert.AlbertForTokenClassification(self,config)
transformers.modeling_albert.AlbertForTokenClassification.__init__(self,config)
transformers.modeling_albert.AlbertForTokenClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.modeling_albert.AlbertLayer(self,config)
transformers.modeling_albert.AlbertLayer.__init__(self,config)
transformers.modeling_albert.AlbertLayer.ff_chunk(self,attention_output)
transformers.modeling_albert.AlbertLayer.forward(self,hidden_states,attention_mask=None,head_mask=None,output_attentions=False,output_hidden_states=False)
transformers.modeling_albert.AlbertLayerGroup(self,config)
transformers.modeling_albert.AlbertLayerGroup.__init__(self,config)
transformers.modeling_albert.AlbertLayerGroup.forward(self,hidden_states,attention_mask=None,head_mask=None,output_attentions=False,output_hidden_states=False)
transformers.modeling_albert.AlbertMLMHead(self,config)
transformers.modeling_albert.AlbertMLMHead.__init__(self,config)
transformers.modeling_albert.AlbertMLMHead.forward(self,hidden_states)
transformers.modeling_albert.AlbertModel(self,config)
transformers.modeling_albert.AlbertModel.__init__(self,config)
transformers.modeling_albert.AlbertModel._prune_heads(self,heads_to_prune)
transformers.modeling_albert.AlbertModel._resize_token_embeddings(self,new_num_tokens)
transformers.modeling_albert.AlbertModel.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.modeling_albert.AlbertModel.get_input_embeddings(self)
transformers.modeling_albert.AlbertModel.set_input_embeddings(self,value)
transformers.modeling_albert.AlbertPreTrainedModel(PreTrainedModel)
transformers.modeling_albert.AlbertPreTrainedModel._init_weights(self,module)
transformers.modeling_albert.AlbertSOPHead(self,config)
transformers.modeling_albert.AlbertSOPHead.__init__(self,config)
transformers.modeling_albert.AlbertSOPHead.forward(self,pooled_output)
transformers.modeling_albert.AlbertTransformer(self,config)
transformers.modeling_albert.AlbertTransformer.__init__(self,config)
transformers.modeling_albert.AlbertTransformer.forward(self,hidden_states,attention_mask=None,head_mask=None,output_attentions=False,output_hidden_states=False,return_dict=False)
transformers.modeling_albert.load_tf_weights_in_albert(model,config,tf_checkpoint_path)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/modeling_tf_transfo_xl.py----------------------------------------
A:transformers.modeling_tf_transfo_xl.logger->utils.logging.get_logger(__name__)
A:transformers.modeling_tf_transfo_xl.sinusoid_inp->tensorflow.einsum('i,j->ij', pos_seq, self.inv_freq)
A:transformers.modeling_tf_transfo_xl.pos_emb->self.drop(pos_emb, training=training)
A:transformers.modeling_tf_transfo_xl.self.layer_1->tensorflow.keras.layers.Dense(d_inner, kernel_initializer=get_initializer(init_std), activation=tf.nn.relu, name='CoreNet_._0')
A:transformers.modeling_tf_transfo_xl.self.drop_1->tensorflow.keras.layers.Dropout(dropout)
A:transformers.modeling_tf_transfo_xl.self.layer_2->tensorflow.keras.layers.Dense(d_model, kernel_initializer=get_initializer(init_std), name='CoreNet_._3')
A:transformers.modeling_tf_transfo_xl.self.drop_2->tensorflow.keras.layers.Dropout(dropout)
A:transformers.modeling_tf_transfo_xl.self.layer_norm->tensorflow.keras.layers.LayerNormalization(epsilon=layer_norm_epsilon, name='layer_norm')
A:transformers.modeling_tf_transfo_xl.core_out->tensorflow.transpose(core_out, perm=(1, 0, 2))
A:transformers.modeling_tf_transfo_xl.output->self.layer_norm(inp + core_out)
A:transformers.modeling_tf_transfo_xl.self.qkv_net->tensorflow.keras.layers.Dense(3 * n_head * d_head, kernel_initializer=get_initializer(init_std), use_bias=False, name='qkv_net')
A:transformers.modeling_tf_transfo_xl.self.drop->tensorflow.keras.layers.Dropout(config.dropout)
A:transformers.modeling_tf_transfo_xl.self.dropatt->tensorflow.keras.layers.Dropout(dropatt)
A:transformers.modeling_tf_transfo_xl.self.o_net->tensorflow.keras.layers.Dense(d_model, kernel_initializer=get_initializer(init_std), use_bias=False, name='o_net')
A:transformers.modeling_tf_transfo_xl.self.r_net->tensorflow.keras.layers.Dense(self.n_head * self.d_head, kernel_initializer=get_initializer(init_std), use_bias=False, name='r_net')
A:transformers.modeling_tf_transfo_xl.self.r_r_bias->self.add_weight(shape=(self.n_head, self.d_head), initializer='zeros', trainable=True, name='r_r_bias')
A:transformers.modeling_tf_transfo_xl.self.r_w_bias->self.add_weight(shape=(self.n_head, self.d_head), initializer='zeros', trainable=True, name='r_w_bias')
A:transformers.modeling_tf_transfo_xl.x_size->shape_list(x)
A:transformers.modeling_tf_transfo_xl.x->tensorflow.reshape(x, x_size)
A:transformers.modeling_tf_transfo_xl.cat->tensorflow.concat([mems[i], hids[i]], axis=0)
A:transformers.modeling_tf_transfo_xl.w_heads->self.qkv_net(w)
A:transformers.modeling_tf_transfo_xl.r_head_k->tensorflow.reshape(r_head_k, (rlen, self.n_head, self.d_head))
A:transformers.modeling_tf_transfo_xl.(w_head_q, w_head_k, w_head_v)->tensorflow.split(w_heads, 3, axis=-1)
A:transformers.modeling_tf_transfo_xl.w_head_q->tensorflow.reshape(w_head_q, (qlen, bsz, self.n_head, self.d_head))
A:transformers.modeling_tf_transfo_xl.w_head_k->tensorflow.reshape(w_head_k, (klen, bsz, self.n_head, self.d_head))
A:transformers.modeling_tf_transfo_xl.w_head_v->tensorflow.reshape(w_head_v, (klen, bsz, self.n_head, self.d_head))
A:transformers.modeling_tf_transfo_xl.AC->tensorflow.einsum('ibnd,jbnd->ijbn', rw_head_q, w_head_k)
A:transformers.modeling_tf_transfo_xl.BD->self._rel_shift(BD)
A:transformers.modeling_tf_transfo_xl.attn_prob->self.dropatt(attn_prob, training=training)
A:transformers.modeling_tf_transfo_xl.attn_vec->tensorflow.reshape(attn_vec, (attn_vec_sizes[0], attn_vec_sizes[1], self.n_head * self.d_head))
A:transformers.modeling_tf_transfo_xl.attn_vec_sizes->shape_list(attn_vec)
A:transformers.modeling_tf_transfo_xl.attn_out->self.drop(attn_out, training=training)
A:transformers.modeling_tf_transfo_xl.self.dec_attn->TFRelPartialLearnableMultiHeadAttn(n_head, d_model, d_head, dropout, tgt_len=tgt_len, ext_len=ext_len, mem_len=mem_len, dropatt=dropatt, pre_lnorm=pre_lnorm, r_w_bias=r_w_bias, r_r_bias=r_r_bias, init_std=init_std, layer_norm_epsilon=layer_norm_epsilon, output_attentions=output_attentions, name='dec_attn')
A:transformers.modeling_tf_transfo_xl.self.pos_ff->TFPositionwiseFF(d_model, d_inner, dropout, pre_lnorm=pre_lnorm, init_std=init_std, layer_norm_epsilon=layer_norm_epsilon, name='pos_ff')
A:transformers.modeling_tf_transfo_xl.attn_outputs->self.dec_attn(dec_inp, r, dec_attn_mask, mems, head_mask, output_attentions, training=training)
A:transformers.modeling_tf_transfo_xl.ff_output->self.pos_ff(attn_outputs[0], training=training)
A:transformers.modeling_tf_transfo_xl.inp_flat->tensorflow.reshape(inp, (-1,))
A:transformers.modeling_tf_transfo_xl.emb_flat->tensorflow.zeros([shape_list(inp_flat)[0], self.d_proj])
A:transformers.modeling_tf_transfo_xl.emb_i->tensorflow.einsum('id,de->ie', emb_i, self.emb_projs[i])
A:transformers.modeling_tf_transfo_xl.mask_idx->tensorflow.cast(tf.where(mask_i), dtype=tf.int64)
A:transformers.modeling_tf_transfo_xl.embed->tensorflow.reshape(emb_flat, embed_shape)
A:transformers.modeling_tf_transfo_xl.self.word_emb->TFAdaptiveEmbedding(config.vocab_size, config.d_embed, config.d_model, config.cutoffs, div_val=config.div_val, init_std=config.init_std, name='word_emb')
A:transformers.modeling_tf_transfo_xl.self.pos_emb->TFPositionalEmbedding(self.d_model, name='pos_emb')
A:transformers.modeling_tf_transfo_xl.empty->tensorflow.zeros([self.mem_len, bsz, self.d_model])
A:transformers.modeling_tf_transfo_xl.beg_idx->max(0, end_idx - self.mem_len)
A:transformers.modeling_tf_transfo_xl.input_ids->inputs.get('input_ids')
A:transformers.modeling_tf_transfo_xl.mems->inputs.get('mems', mems)
A:transformers.modeling_tf_transfo_xl.head_mask->inputs.get('head_mask', head_mask)
A:transformers.modeling_tf_transfo_xl.inputs_embeds->inputs.get('inputs_embeds', inputs_embeds)
A:transformers.modeling_tf_transfo_xl.output_attentions->inputs.get('output_attentions', output_attentions)
A:transformers.modeling_tf_transfo_xl.output_hidden_states->inputs.get('output_hidden_states', output_hidden_states)
A:transformers.modeling_tf_transfo_xl.return_dict->inputs.get('return_dict', return_dict)
A:transformers.modeling_tf_transfo_xl.(qlen, bsz)->shape_list(input_ids)
A:transformers.modeling_tf_transfo_xl.word_emb->self.word_emb(input_ids)
A:transformers.modeling_tf_transfo_xl.attn_mask->tensorflow.ones([qlen, qlen])
A:transformers.modeling_tf_transfo_xl.mask_u->tensorflow.linalg.band_part(attn_mask, 0, -1)
A:transformers.modeling_tf_transfo_xl.mask_dia->tensorflow.linalg.band_part(attn_mask, 0, 0)
A:transformers.modeling_tf_transfo_xl.attn_mask_pad->tensorflow.zeros([qlen, mlen])
A:transformers.modeling_tf_transfo_xl.dec_attn_mask->tensorflow.concat([dec_attn_mask[:, :qlen] + mask_l - mask_dia, dec_attn_mask[:, qlen:]], 1)
A:transformers.modeling_tf_transfo_xl.mask_l->tensorflow.linalg.band_part(attn_mask, -1, 0)
A:transformers.modeling_tf_transfo_xl.pos_seq->tensorflow.minimum(pos_seq, self.clamp_len)
A:transformers.modeling_tf_transfo_xl.layer_outputs->layer(core_out, pos_emb, dec_attn_mask, mems_i, head_mask[i], output_attentions, training=training)
A:transformers.modeling_tf_transfo_xl.new_mems->self._update_mems(hids, mems, mlen, qlen)
A:transformers.modeling_tf_transfo_xl.hids->tuple((tf.transpose(t, perm=(1, 0, 2)) for t in hids))
A:transformers.modeling_tf_transfo_xl.attentions->tuple((tf.transpose(t, perm=(2, 3, 0, 1)) for t in attentions))
A:transformers.modeling_tf_transfo_xl.self.transformer->TFTransfoXLMainLayer(config, name='transformer')
A:transformers.modeling_tf_transfo_xl.outputs->self.transformer(inputs, **kwargs)
A:transformers.modeling_tf_transfo_xl.self.bias->self.add_weight(shape=(self.vocab_size,), initializer='zeros', trainable=True, name='bias')
A:transformers.modeling_tf_transfo_xl.hidden_states->self.input_embeddings(hidden_states, mode='linear')
A:transformers.modeling_tf_transfo_xl.self.crit->TFAdaptiveSoftmaxMask(config.vocab_size, config.d_embed, config.d_model, config.cutoffs, div_val=config.div_val, name='crit')
A:transformers.modeling_tf_transfo_xl.labels->inputs.get('labels', labels)
A:transformers.modeling_tf_transfo_xl.transformer_outputs->self.transformer(input_ids, mems, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, training=training)
A:transformers.modeling_tf_transfo_xl.softmax_output->self.crit(pred_hid, labels, training=training)
transformers.TFAdaptiveEmbedding(self,n_token,d_embed,d_proj,cutoffs,div_val=1,init_std=0.02,sample_softmax=False,**kwargs)
transformers.TFAdaptiveEmbedding.build(self,input_shape)
transformers.TFAdaptiveEmbedding.call(self,inp)
transformers.TFTransfoXLLMHeadModel(self,config)
transformers.TFTransfoXLLMHeadModel.call(self,inputs,mems=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.TFTransfoXLLMHeadModel.get_output_embeddings(self)
transformers.TFTransfoXLLMHeadModel.init_mems(self,bsz)
transformers.TFTransfoXLLMHeadModel.prepare_inputs_for_generation(self,inputs,past,**model_kwargs)
transformers.TFTransfoXLLMHeadModel.reset_length(self,tgt_len,ext_len,mem_len)
transformers.TFTransfoXLLMHeadModelOutput(ModelOutput)
transformers.TFTransfoXLMainLayer(self,config,**kwargs)
transformers.TFTransfoXLMainLayer._prune_heads(self,heads)
transformers.TFTransfoXLMainLayer._resize_token_embeddings(self,new_num_tokens)
transformers.TFTransfoXLMainLayer._update_mems(self,hids,mems,mlen,qlen)
transformers.TFTransfoXLMainLayer.backward_compatible(self)
transformers.TFTransfoXLMainLayer.build(self,input_shape)
transformers.TFTransfoXLMainLayer.call(self,inputs,mems=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False)
transformers.TFTransfoXLMainLayer.get_input_embeddings(self)
transformers.TFTransfoXLMainLayer.init_mems(self,bsz)
transformers.TFTransfoXLMainLayer.reset_length(self,tgt_len,ext_len,mem_len)
transformers.TFTransfoXLMainLayer.set_input_embeddings(self,value)
transformers.TFTransfoXLModel(self,config,*inputs,**kwargs)
transformers.TFTransfoXLModel.call(self,inputs,**kwargs)
transformers.TFTransfoXLModelOutput(ModelOutput)
transformers.TFTransfoXLPreTrainedModel(TFPreTrainedModel)
transformers.modeling_tf_transfo_xl.TFAdaptiveEmbedding(self,n_token,d_embed,d_proj,cutoffs,div_val=1,init_std=0.02,sample_softmax=False,**kwargs)
transformers.modeling_tf_transfo_xl.TFAdaptiveEmbedding.__init__(self,n_token,d_embed,d_proj,cutoffs,div_val=1,init_std=0.02,sample_softmax=False,**kwargs)
transformers.modeling_tf_transfo_xl.TFAdaptiveEmbedding.build(self,input_shape)
transformers.modeling_tf_transfo_xl.TFAdaptiveEmbedding.call(self,inp)
transformers.modeling_tf_transfo_xl.TFPositionalEmbedding(self,demb,**kwargs)
transformers.modeling_tf_transfo_xl.TFPositionalEmbedding.__init__(self,demb,**kwargs)
transformers.modeling_tf_transfo_xl.TFPositionalEmbedding.call(self,pos_seq,bsz=None)
transformers.modeling_tf_transfo_xl.TFPositionwiseFF(self,d_model,d_inner,dropout,pre_lnorm=False,layer_norm_epsilon=1e-05,init_std=0.02,**kwargs)
transformers.modeling_tf_transfo_xl.TFPositionwiseFF.__init__(self,d_model,d_inner,dropout,pre_lnorm=False,layer_norm_epsilon=1e-05,init_std=0.02,**kwargs)
transformers.modeling_tf_transfo_xl.TFPositionwiseFF.call(self,inp,training=False)
transformers.modeling_tf_transfo_xl.TFRelPartialLearnableDecoderLayer(self,n_head,d_model,d_head,d_inner,dropout,tgt_len=None,ext_len=None,mem_len=None,dropatt=0.0,pre_lnorm=False,r_w_bias=None,r_r_bias=None,layer_norm_epsilon=1e-05,init_std=0.02,output_attentions=False,**kwargs)
transformers.modeling_tf_transfo_xl.TFRelPartialLearnableDecoderLayer.__init__(self,n_head,d_model,d_head,d_inner,dropout,tgt_len=None,ext_len=None,mem_len=None,dropatt=0.0,pre_lnorm=False,r_w_bias=None,r_r_bias=None,layer_norm_epsilon=1e-05,init_std=0.02,output_attentions=False,**kwargs)
transformers.modeling_tf_transfo_xl.TFRelPartialLearnableDecoderLayer.call(self,dec_inp,r,dec_attn_mask,mems,head_mask,output_attentions,training=False)
transformers.modeling_tf_transfo_xl.TFRelPartialLearnableMultiHeadAttn(self,n_head,d_model,d_head,dropout,dropatt=0,tgt_len=None,ext_len=None,mem_len=None,pre_lnorm=False,r_r_bias=None,r_w_bias=None,layer_norm_epsilon=1e-05,init_std=0.02,output_attentions=False,**kwargs)
transformers.modeling_tf_transfo_xl.TFRelPartialLearnableMultiHeadAttn.__init__(self,n_head,d_model,d_head,dropout,dropatt=0,tgt_len=None,ext_len=None,mem_len=None,pre_lnorm=False,r_r_bias=None,r_w_bias=None,layer_norm_epsilon=1e-05,init_std=0.02,output_attentions=False,**kwargs)
transformers.modeling_tf_transfo_xl.TFRelPartialLearnableMultiHeadAttn._rel_shift(self,x)
transformers.modeling_tf_transfo_xl.TFRelPartialLearnableMultiHeadAttn.build(self,input_shape)
transformers.modeling_tf_transfo_xl.TFRelPartialLearnableMultiHeadAttn.call(self,w,r,attn_mask,mems,head_mask,output_attentions,training=False)
transformers.modeling_tf_transfo_xl.TFTransfoXLLMHeadModel(self,config)
transformers.modeling_tf_transfo_xl.TFTransfoXLLMHeadModel.__init__(self,config)
transformers.modeling_tf_transfo_xl.TFTransfoXLLMHeadModel.call(self,inputs,mems=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.modeling_tf_transfo_xl.TFTransfoXLLMHeadModel.get_output_embeddings(self)
transformers.modeling_tf_transfo_xl.TFTransfoXLLMHeadModel.init_mems(self,bsz)
transformers.modeling_tf_transfo_xl.TFTransfoXLLMHeadModel.prepare_inputs_for_generation(self,inputs,past,**model_kwargs)
transformers.modeling_tf_transfo_xl.TFTransfoXLLMHeadModel.reset_length(self,tgt_len,ext_len,mem_len)
transformers.modeling_tf_transfo_xl.TFTransfoXLLMHeadModelOutput(ModelOutput)
transformers.modeling_tf_transfo_xl.TFTransfoXLMHead(self,config,input_embeddings,**kwargs)
transformers.modeling_tf_transfo_xl.TFTransfoXLMHead.__init__(self,config,input_embeddings,**kwargs)
transformers.modeling_tf_transfo_xl.TFTransfoXLMHead.build(self,input_shape)
transformers.modeling_tf_transfo_xl.TFTransfoXLMHead.call(self,hidden_states)
transformers.modeling_tf_transfo_xl.TFTransfoXLMainLayer(self,config,**kwargs)
transformers.modeling_tf_transfo_xl.TFTransfoXLMainLayer.__init__(self,config,**kwargs)
transformers.modeling_tf_transfo_xl.TFTransfoXLMainLayer._prune_heads(self,heads)
transformers.modeling_tf_transfo_xl.TFTransfoXLMainLayer._resize_token_embeddings(self,new_num_tokens)
transformers.modeling_tf_transfo_xl.TFTransfoXLMainLayer._update_mems(self,hids,mems,mlen,qlen)
transformers.modeling_tf_transfo_xl.TFTransfoXLMainLayer.backward_compatible(self)
transformers.modeling_tf_transfo_xl.TFTransfoXLMainLayer.build(self,input_shape)
transformers.modeling_tf_transfo_xl.TFTransfoXLMainLayer.call(self,inputs,mems=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False)
transformers.modeling_tf_transfo_xl.TFTransfoXLMainLayer.get_input_embeddings(self)
transformers.modeling_tf_transfo_xl.TFTransfoXLMainLayer.init_mems(self,bsz)
transformers.modeling_tf_transfo_xl.TFTransfoXLMainLayer.reset_length(self,tgt_len,ext_len,mem_len)
transformers.modeling_tf_transfo_xl.TFTransfoXLMainLayer.set_input_embeddings(self,value)
transformers.modeling_tf_transfo_xl.TFTransfoXLModel(self,config,*inputs,**kwargs)
transformers.modeling_tf_transfo_xl.TFTransfoXLModel.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_transfo_xl.TFTransfoXLModel.call(self,inputs,**kwargs)
transformers.modeling_tf_transfo_xl.TFTransfoXLModelOutput(ModelOutput)
transformers.modeling_tf_transfo_xl.TFTransfoXLPreTrainedModel(TFPreTrainedModel)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/configuration_camembert.py----------------------------------------
A:transformers.configuration_camembert.logger->utils.logging.get_logger(__name__)
transformers.CamembertConfig(RobertaConfig)
transformers.configuration_camembert.CamembertConfig(RobertaConfig)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/tokenization_openai.py----------------------------------------
A:transformers.tokenization_openai.logger->utils.logging.get_logger(__name__)
A:transformers.tokenization_openai.pairs->get_pairs(word)
A:transformers.tokenization_openai.text->self.nlp(text_standardize(self.fix_text(text)))
A:transformers.tokenization_openai._nlp->English()
A:transformers.tokenization_openai.self.nlp->BasicTokenizer(do_lower_case=True)
A:transformers.tokenization_openai.self.encoder->json.load(vocab_handle)
A:transformers.tokenization_openai.self.bpe_ranks->dict(zip(merges, range(len(merges))))
A:transformers.tokenization_openai.bigram->min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float('inf')))
A:transformers.tokenization_openai.j->' '.join(word).index(first, i)
A:transformers.tokenization_openai.new_word->tuple(new_word)
A:transformers.tokenization_openai.word->' '.join(word)
A:transformers.tokenization_openai.out_string->''.join(tokens).replace('</w>', ' ').strip()
A:transformers.tokenization_openai.vocab_file->os.path.join(save_directory, VOCAB_FILES_NAMES['vocab_file'])
A:transformers.tokenization_openai.merge_file->os.path.join(save_directory, VOCAB_FILES_NAMES['merges_file'])
transformers.OpenAIGPTTokenizer(self,vocab_file,merges_file,unk_token='<unk>',**kwargs)
transformers.OpenAIGPTTokenizer._convert_id_to_token(self,index)
transformers.OpenAIGPTTokenizer._convert_token_to_id(self,token)
transformers.OpenAIGPTTokenizer._tokenize(self,text)
transformers.OpenAIGPTTokenizer.bpe(self,token)
transformers.OpenAIGPTTokenizer.convert_tokens_to_string(self,tokens)
transformers.OpenAIGPTTokenizer.get_vocab(self)
transformers.OpenAIGPTTokenizer.save_vocabulary(self,save_directory)
transformers.OpenAIGPTTokenizer.vocab_size(self)
transformers.OpenAIGPTTokenizerFast(self,vocab_file,merges_file,unk_token='<unk>',**kwargs)
transformers.tokenization_openai.OpenAIGPTTokenizer(self,vocab_file,merges_file,unk_token='<unk>',**kwargs)
transformers.tokenization_openai.OpenAIGPTTokenizer.__init__(self,vocab_file,merges_file,unk_token='<unk>',**kwargs)
transformers.tokenization_openai.OpenAIGPTTokenizer._convert_id_to_token(self,index)
transformers.tokenization_openai.OpenAIGPTTokenizer._convert_token_to_id(self,token)
transformers.tokenization_openai.OpenAIGPTTokenizer._tokenize(self,text)
transformers.tokenization_openai.OpenAIGPTTokenizer.bpe(self,token)
transformers.tokenization_openai.OpenAIGPTTokenizer.convert_tokens_to_string(self,tokens)
transformers.tokenization_openai.OpenAIGPTTokenizer.get_vocab(self)
transformers.tokenization_openai.OpenAIGPTTokenizer.save_vocabulary(self,save_directory)
transformers.tokenization_openai.OpenAIGPTTokenizer.vocab_size(self)
transformers.tokenization_openai.OpenAIGPTTokenizerFast(self,vocab_file,merges_file,unk_token='<unk>',**kwargs)
transformers.tokenization_openai.OpenAIGPTTokenizerFast.__init__(self,vocab_file,merges_file,unk_token='<unk>',**kwargs)
transformers.tokenization_openai.get_pairs(word)
transformers.tokenization_openai.text_standardize(text)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/modeling_tf_pytorch_utils.py----------------------------------------
A:transformers.modeling_tf_pytorch_utils.logger->utils.logging.get_logger(__name__)
A:transformers.modeling_tf_pytorch_utils.tf_name->tf_name.replace(start_prefix_to_remove, '', 1).replace(start_prefix_to_remove, '', 1)
A:transformers.modeling_tf_pytorch_utils.transpose->bool(tf_name[-1] == 'kernel' or 'emb_projs' in tf_name or 'out_projs' in tf_name)
A:transformers.modeling_tf_pytorch_utils.pt_path->os.path.abspath(pytorch_checkpoint_path)
A:transformers.modeling_tf_pytorch_utils.pt_state_dict->pt_model.state_dict()
A:transformers.modeling_tf_pytorch_utils.new_key->key.replace('beta', 'bias')
A:transformers.modeling_tf_pytorch_utils.pt_state_dict[new_key]->pt_model.state_dict().pop(old_key)
A:transformers.modeling_tf_pytorch_utils.all_pytorch_weights->set(list(pt_state_dict.keys()))
A:transformers.modeling_tf_pytorch_utils.(name, transpose)->convert_tf_weight_name_to_pt_weight_name(sw_name, start_prefix_to_remove=start_prefix_to_remove)
A:transformers.modeling_tf_pytorch_utils.array->numpy.expand_dims(array, axis=0)
A:transformers.modeling_tf_pytorch_utils.missing_keys->list(all_pytorch_weights)
A:transformers.modeling_tf_pytorch_utils.tf_model_class->getattr(transformers, tf_model_class_name)
A:transformers.modeling_tf_pytorch_utils.tf_model->tf_model_class(pt_model.config)
A:transformers.modeling_tf_pytorch_utils.current_pt_params_dict->dict(pt_model.named_parameters())
A:transformers.modeling_tf_pytorch_utils.(pt_name, transpose)->convert_tf_weight_name_to_pt_weight_name(tf_weight.name, start_prefix_to_remove=start_prefix_to_remove)
A:transformers.modeling_tf_pytorch_utils.all_tf_weights->set(list(tf_weights_map.keys()))
A:transformers.modeling_tf_pytorch_utils.new_pt_params_dict[pt_weight_name]->torch.from_numpy(array)
A:transformers.modeling_tf_pytorch_utils.loaded_pt_weights_data_ptr[pt_weight.data_ptr()]->torch.from_numpy(array)
A:transformers.modeling_tf_pytorch_utils.(missing_keys, unexpected_keys)->pt_model.load_state_dict(new_pt_params_dict, strict=False)
transformers.convert_tf_weight_name_to_pt_weight_name(tf_name,start_prefix_to_remove='')
transformers.load_pytorch_checkpoint_in_tf2_model(tf_model,pytorch_checkpoint_path,tf_inputs=None,allow_missing_keys=False)
transformers.load_pytorch_model_in_tf2_model(tf_model,pt_model,tf_inputs=None,allow_missing_keys=False)
transformers.load_pytorch_weights_in_tf2_model(tf_model,pt_state_dict,tf_inputs=None,allow_missing_keys=False)
transformers.load_tf2_checkpoint_in_pytorch_model(pt_model,tf_checkpoint_path,tf_inputs=None,allow_missing_keys=False)
transformers.load_tf2_model_in_pytorch_model(pt_model,tf_model,allow_missing_keys=False)
transformers.load_tf2_weights_in_pytorch_model(pt_model,tf_weights,allow_missing_keys=False)
transformers.modeling_tf_pytorch_utils.convert_tf_weight_name_to_pt_weight_name(tf_name,start_prefix_to_remove='')
transformers.modeling_tf_pytorch_utils.load_pytorch_checkpoint_in_tf2_model(tf_model,pytorch_checkpoint_path,tf_inputs=None,allow_missing_keys=False)
transformers.modeling_tf_pytorch_utils.load_pytorch_model_in_tf2_model(tf_model,pt_model,tf_inputs=None,allow_missing_keys=False)
transformers.modeling_tf_pytorch_utils.load_pytorch_weights_in_tf2_model(tf_model,pt_state_dict,tf_inputs=None,allow_missing_keys=False)
transformers.modeling_tf_pytorch_utils.load_tf2_checkpoint_in_pytorch_model(pt_model,tf_checkpoint_path,tf_inputs=None,allow_missing_keys=False)
transformers.modeling_tf_pytorch_utils.load_tf2_model_in_pytorch_model(pt_model,tf_model,allow_missing_keys=False)
transformers.modeling_tf_pytorch_utils.load_tf2_weights_in_pytorch_model(pt_model,tf_weights,allow_missing_keys=False)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/convert_bert_original_tf_checkpoint_to_pytorch.py----------------------------------------
A:transformers.convert_bert_original_tf_checkpoint_to_pytorch.config->transformers.BertConfig.from_json_file(bert_config_file)
A:transformers.convert_bert_original_tf_checkpoint_to_pytorch.model->BertForPreTraining(config)
A:transformers.convert_bert_original_tf_checkpoint_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.convert_bert_original_tf_checkpoint_to_pytorch.args->argparse.ArgumentParser().parse_args()
transformers.convert_bert_original_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(tf_checkpoint_path,bert_config_file,pytorch_dump_path)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/configuration_mbart.py----------------------------------------
A:transformers.configuration_mbart.logger->utils.logging.get_logger(__name__)
transformers.MBartConfig(BartConfig)
transformers.configuration_mbart.MBartConfig(BartConfig)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/hf_argparser.py----------------------------------------
A:transformers.hf_argparser.DataClass->NewType('DataClass', Any)
A:transformers.hf_argparser.DataClassType->NewType('DataClassType', Any)
A:transformers.hf_argparser.kwargs->field.metadata.copy()
A:transformers.hf_argparser.typestring->str(field.type)
A:transformers.hf_argparser.kwargs['choices']->list(field.type)
A:transformers.hf_argparser.kwargs['default']->field.default_factory()
A:transformers.hf_argparser.args_file->Path(sys.argv[0]).with_suffix('.args')
A:transformers.hf_argparser.fargs->Path(sys.argv[0]).with_suffix('.args').read_text().split()
A:transformers.hf_argparser.(namespace, remaining_args)->self.parse_known_args(args=args)
A:transformers.hf_argparser.obj->dtype(**inputs)
A:transformers.hf_argparser.data->json.loads(Path(json_file).read_text())
transformers.HfArgumentParser(self,dataclass_types:Union[DataClassType,Iterable[DataClassType]],**kwargs)
transformers.HfArgumentParser._add_dataclass_arguments(self,dtype:DataClassType)
transformers.HfArgumentParser.parse_args_into_dataclasses(self,args=None,return_remaining_strings=False,look_for_args_file=True,args_filename=None)->Tuple[DataClass, ...]
transformers.HfArgumentParser.parse_dict(self,args:dict)->Tuple[DataClass, ...]
transformers.HfArgumentParser.parse_json_file(self,json_file:str)->Tuple[DataClass, ...]
transformers.hf_argparser.HfArgumentParser(self,dataclass_types:Union[DataClassType,Iterable[DataClassType]],**kwargs)
transformers.hf_argparser.HfArgumentParser.__init__(self,dataclass_types:Union[DataClassType,Iterable[DataClassType]],**kwargs)
transformers.hf_argparser.HfArgumentParser._add_dataclass_arguments(self,dtype:DataClassType)
transformers.hf_argparser.HfArgumentParser.parse_args_into_dataclasses(self,args=None,return_remaining_strings=False,look_for_args_file=True,args_filename=None)->Tuple[DataClass, ...]
transformers.hf_argparser.HfArgumentParser.parse_dict(self,args:dict)->Tuple[DataClass, ...]
transformers.hf_argparser.HfArgumentParser.parse_json_file(self,json_file:str)->Tuple[DataClass, ...]


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/convert_bart_original_pytorch_checkpoint_to_pytorch.py----------------------------------------
A:transformers.convert_bart_original_pytorch_checkpoint_to_pytorch.logger->utils.logging.get_logger(__name__)
A:transformers.convert_bart_original_pytorch_checkpoint_to_pytorch.val->dct.pop(old)
A:transformers.convert_bart_original_pytorch_checkpoint_to_pytorch.sd->torch.load(checkpoint_path, map_location='cpu')
A:transformers.convert_bart_original_pytorch_checkpoint_to_pytorch.hub_interface->torch.hub.load('pytorch/fairseq', 'bart.large.cnn').eval()
A:transformers.convert_bart_original_pytorch_checkpoint_to_pytorch.bart->load_xsum_checkpoint(checkpoint_path)
A:transformers.convert_bart_original_pytorch_checkpoint_to_pytorch.hf_checkpoint_name->checkpoint_path.replace('.', '-')
A:transformers.convert_bart_original_pytorch_checkpoint_to_pytorch.config->transformers.BartConfig.from_pretrained(hf_checkpoint_name)
A:transformers.convert_bart_original_pytorch_checkpoint_to_pytorch.tokens->load_xsum_checkpoint(checkpoint_path).encode(SAMPLE_TEXT).unsqueeze(0)
A:transformers.convert_bart_original_pytorch_checkpoint_to_pytorch.tokens2->transformers.BartTokenizer.from_pretrained(hf_checkpoint_name).encode(SAMPLE_TEXT, return_tensors='pt').unsqueeze(0)
A:transformers.convert_bart_original_pytorch_checkpoint_to_pytorch.state_dict->load_xsum_checkpoint(checkpoint_path).model.state_dict()
A:transformers.convert_bart_original_pytorch_checkpoint_to_pytorch.model->BartForConditionalGeneration(config).eval()
A:transformers.convert_bart_original_pytorch_checkpoint_to_pytorch.fairseq_output->load_xsum_checkpoint(checkpoint_path).extract_features(tokens)
A:transformers.convert_bart_original_pytorch_checkpoint_to_pytorch.model.lm_head->_make_linear_from_emb(model.model.shared)
A:transformers.convert_bart_original_pytorch_checkpoint_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.convert_bart_original_pytorch_checkpoint_to_pytorch.args->argparse.ArgumentParser().parse_args()
transformers.convert_bart_original_pytorch_checkpoint_to_pytorch.convert_bart_checkpoint(checkpoint_path,pytorch_dump_folder_path,hf_checkpoint_name=None)
transformers.convert_bart_original_pytorch_checkpoint_to_pytorch.load_xsum_checkpoint(checkpoint_path)
transformers.convert_bart_original_pytorch_checkpoint_to_pytorch.remove_ignore_keys_(state_dict)
transformers.convert_bart_original_pytorch_checkpoint_to_pytorch.rename_key(dct,old,new)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/modeling_tf_transfo_xl_utilities.py----------------------------------------
A:transformers.modeling_tf_transfo_xl_utilities.self.cluster_weight->self.add_weight(shape=(self.n_clusters, self.d_embed), initializer='zeros', trainable=True, name='cluster_weight')
A:transformers.modeling_tf_transfo_xl_utilities.self.cluster_bias->self.add_weight(shape=(self.n_clusters,), initializer='zeros', trainable=True, name='cluster_bias')
A:transformers.modeling_tf_transfo_xl_utilities.weight->self.add_weight(shape=(r_idx - l_idx, d_emb_i), initializer='zeros', trainable=True, name='out_layers_._{}_._weight'.format(i))
A:transformers.modeling_tf_transfo_xl_utilities.bias->self.add_weight(shape=(r_idx - l_idx,), initializer='zeros', trainable=True, name='out_layers_._{}_._bias'.format(i))
A:transformers.modeling_tf_transfo_xl_utilities.y->tensorflow.einsum('ibd,ed->ibe', y, proj)
A:transformers.modeling_tf_transfo_xl_utilities.lp_size->shape_list(logprob)
A:transformers.modeling_tf_transfo_xl_utilities.r->tensorflow.range(lp_size[0])
A:transformers.modeling_tf_transfo_xl_utilities.idx->tensorflow.stack([r, target], 1)
A:transformers.modeling_tf_transfo_xl_utilities.output->self._logit(hidden, self.out_layers[0][0], self.out_layers[0][1], self.out_projs[0])
A:transformers.modeling_tf_transfo_xl_utilities.loss->tensorflow.reduce_mean(loss)
A:transformers.modeling_tf_transfo_xl_utilities.out->tensorflow.concat(out, axis=-1)
A:transformers.modeling_tf_transfo_xl_utilities.hidden_sizes->shape_list(hidden)
A:transformers.modeling_tf_transfo_xl_utilities.mask_idx->tensorflow.where(mask)
A:transformers.modeling_tf_transfo_xl_utilities.cur_W->tensorflow.concat([cur_W, self.cluster_weight], 0)
A:transformers.modeling_tf_transfo_xl_utilities.cur_b->tensorflow.concat([cur_b, self.cluster_bias], 0)
A:transformers.modeling_tf_transfo_xl_utilities.head_logit->self._logit(hidden, cur_W, cur_b, self.out_projs[0])
A:transformers.modeling_tf_transfo_xl_utilities.head_logprob->tensorflow.nn.log_softmax(head_logit)
A:transformers.modeling_tf_transfo_xl_utilities.cur_head_logprob->tensorflow.boolean_mask(head_logprob, mask)
A:transformers.modeling_tf_transfo_xl_utilities.cur_logprob->self._gather_logprob(cur_tail_logprob, cur_target)
A:transformers.modeling_tf_transfo_xl_utilities.tail_logit->self._logit(hidden, cur_W, cur_b, self.out_projs[i])
A:transformers.modeling_tf_transfo_xl_utilities.tail_logprob->tensorflow.nn.log_softmax(tail_logit)
A:transformers.modeling_tf_transfo_xl_utilities.cur_tail_logprob->tensorflow.boolean_mask(tail_logprob, mask)
transformers.modeling_tf_transfo_xl_utilities.TFAdaptiveSoftmaxMask(self,vocab_size,d_embed,d_proj,cutoffs,div_val=1,keep_order=False,**kwargs)
transformers.modeling_tf_transfo_xl_utilities.TFAdaptiveSoftmaxMask.__init__(self,vocab_size,d_embed,d_proj,cutoffs,div_val=1,keep_order=False,**kwargs)
transformers.modeling_tf_transfo_xl_utilities.TFAdaptiveSoftmaxMask._gather_logprob(logprob,target)
transformers.modeling_tf_transfo_xl_utilities.TFAdaptiveSoftmaxMask._logit(x,W,b,proj=None)
transformers.modeling_tf_transfo_xl_utilities.TFAdaptiveSoftmaxMask.build(self,input_shape)
transformers.modeling_tf_transfo_xl_utilities.TFAdaptiveSoftmaxMask.call(self,hidden,target,return_mean=True,training=False)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/tokenization_xlm.py----------------------------------------
A:transformers.tokenization_xlm.logger->utils.logging.get_logger(__name__)
A:transformers.tokenization_xlm.pairs->get_pairs(word)
A:transformers.tokenization_xlm.text->lowercase_and_remove_accent(text)
A:transformers.tokenization_xlm.cat->unicodedata.category(char)
A:transformers.tokenization_xlm.self.cache_moses_punct_normalizer->dict()
A:transformers.tokenization_xlm.self.cache_moses_tokenizer->dict()
A:transformers.tokenization_xlm.self.lang_with_custom_tokenizer->set(['zh', 'th', 'ja'])
A:transformers.tokenization_xlm.self.encoder->json.load(vocab_handle)
A:transformers.tokenization_xlm.self.bpe_ranks->dict(zip(merges, range(len(merges))))
A:transformers.tokenization_xlm.punct_normalizer->sacremoses.MosesPunctNormalizer(lang=lang)
A:transformers.tokenization_xlm.moses_tokenizer->sacremoses.MosesTokenizer(lang=lang)
A:transformers.tokenization_xlm.self.ja_word_tokenizer->Mykytea.Mykytea('-model %s/local/share/kytea/model.bin' % os.path.expanduser('~'))
A:transformers.tokenization_xlm.bigram->min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float('inf')))
A:transformers.tokenization_xlm.j->' '.join(word).index(first, i)
A:transformers.tokenization_xlm.new_word->tuple(new_word)
A:transformers.tokenization_xlm.word->' '.join(word)
A:transformers.tokenization_xlm.out_string->''.join(tokens).replace('</w>', ' ').strip()
A:transformers.tokenization_xlm.vocab_file->os.path.join(save_directory, VOCAB_FILES_NAMES['vocab_file'])
A:transformers.tokenization_xlm.merge_file->os.path.join(save_directory, VOCAB_FILES_NAMES['merges_file'])
transformers.XLMTokenizer(self,vocab_file,merges_file,unk_token='<unk>',bos_token='<s>',sep_token='</s>',pad_token='<pad>',cls_token='</s>',mask_token='<special1>',additional_special_tokens=['<special0>','<special1>','<special2>','<special3>','<special4>','<special5>','<special6>','<special7>','<special8>','<special9>'],lang2id=None,id2lang=None,do_lowercase_and_remove_accent=True,**kwargs)
transformers.XLMTokenizer._convert_id_to_token(self,index)
transformers.XLMTokenizer._convert_token_to_id(self,token)
transformers.XLMTokenizer._tokenize(self,text,lang='en',bypass_tokenizer=False)
transformers.XLMTokenizer.bpe(self,token)
transformers.XLMTokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.XLMTokenizer.convert_tokens_to_string(self,tokens)
transformers.XLMTokenizer.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.XLMTokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.XLMTokenizer.get_vocab(self)
transformers.XLMTokenizer.ja_tokenize(self,text)
transformers.XLMTokenizer.moses_pipeline(self,text,lang)
transformers.XLMTokenizer.moses_punct_norm(self,text,lang)
transformers.XLMTokenizer.moses_tokenize(self,text,lang)
transformers.XLMTokenizer.save_vocabulary(self,save_directory)
transformers.XLMTokenizer.vocab_size(self)
transformers.tokenization_xlm.XLMTokenizer(self,vocab_file,merges_file,unk_token='<unk>',bos_token='<s>',sep_token='</s>',pad_token='<pad>',cls_token='</s>',mask_token='<special1>',additional_special_tokens=['<special0>','<special1>','<special2>','<special3>','<special4>','<special5>','<special6>','<special7>','<special8>','<special9>'],lang2id=None,id2lang=None,do_lowercase_and_remove_accent=True,**kwargs)
transformers.tokenization_xlm.XLMTokenizer.__init__(self,vocab_file,merges_file,unk_token='<unk>',bos_token='<s>',sep_token='</s>',pad_token='<pad>',cls_token='</s>',mask_token='<special1>',additional_special_tokens=['<special0>','<special1>','<special2>','<special3>','<special4>','<special5>','<special6>','<special7>','<special8>','<special9>'],lang2id=None,id2lang=None,do_lowercase_and_remove_accent=True,**kwargs)
transformers.tokenization_xlm.XLMTokenizer._convert_id_to_token(self,index)
transformers.tokenization_xlm.XLMTokenizer._convert_token_to_id(self,token)
transformers.tokenization_xlm.XLMTokenizer._tokenize(self,text,lang='en',bypass_tokenizer=False)
transformers.tokenization_xlm.XLMTokenizer.bpe(self,token)
transformers.tokenization_xlm.XLMTokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.tokenization_xlm.XLMTokenizer.convert_tokens_to_string(self,tokens)
transformers.tokenization_xlm.XLMTokenizer.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.tokenization_xlm.XLMTokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.tokenization_xlm.XLMTokenizer.get_vocab(self)
transformers.tokenization_xlm.XLMTokenizer.ja_tokenize(self,text)
transformers.tokenization_xlm.XLMTokenizer.moses_pipeline(self,text,lang)
transformers.tokenization_xlm.XLMTokenizer.moses_punct_norm(self,text,lang)
transformers.tokenization_xlm.XLMTokenizer.moses_tokenize(self,text,lang)
transformers.tokenization_xlm.XLMTokenizer.save_vocabulary(self,save_directory)
transformers.tokenization_xlm.XLMTokenizer.vocab_size(self)
transformers.tokenization_xlm.get_pairs(word)
transformers.tokenization_xlm.lowercase_and_remove_accent(text)
transformers.tokenization_xlm.remove_non_printing_char(text)
transformers.tokenization_xlm.replace_unicode_punct(text)
transformers.tokenization_xlm.romanian_preprocessing(text)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/configuration_mmbt.py----------------------------------------
A:transformers.configuration_mmbt.logger->utils.logging.get_logger(__name__)
transformers.MMBTConfig(self,config,num_labels=None,modal_hidden_size=2048)
transformers.configuration_mmbt.MMBTConfig(self,config,num_labels=None,modal_hidden_size=2048)
transformers.configuration_mmbt.MMBTConfig.__init__(self,config,num_labels=None,modal_hidden_size=2048)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/configuration_xlnet.py----------------------------------------
A:transformers.configuration_xlnet.logger->utils.logging.get_logger(__name__)
transformers.XLNetConfig(self,vocab_size=32000,d_model=1024,n_layer=24,n_head=16,d_inner=4096,ff_activation='gelu',untie_r=True,attn_type='bi',initializer_range=0.02,layer_norm_eps=1e-12,dropout=0.1,mem_len=None,reuse_len=None,bi_data=False,clamp_len=-1,same_length=False,summary_type='last',summary_use_proj=True,summary_activation='tanh',summary_last_dropout=0.1,start_n_top=5,end_n_top=5,pad_token_id=5,bos_token_id=1,eos_token_id=2,**kwargs)
transformers.XLNetConfig.hidden_size(self)
transformers.XLNetConfig.max_position_embeddings(self)
transformers.XLNetConfig.n_token(self)
transformers.XLNetConfig.n_token(self,value)
transformers.XLNetConfig.num_attention_heads(self)
transformers.XLNetConfig.num_hidden_layers(self)
transformers.configuration_xlnet.XLNetConfig(self,vocab_size=32000,d_model=1024,n_layer=24,n_head=16,d_inner=4096,ff_activation='gelu',untie_r=True,attn_type='bi',initializer_range=0.02,layer_norm_eps=1e-12,dropout=0.1,mem_len=None,reuse_len=None,bi_data=False,clamp_len=-1,same_length=False,summary_type='last',summary_use_proj=True,summary_activation='tanh',summary_last_dropout=0.1,start_n_top=5,end_n_top=5,pad_token_id=5,bos_token_id=1,eos_token_id=2,**kwargs)
transformers.configuration_xlnet.XLNetConfig.__init__(self,vocab_size=32000,d_model=1024,n_layer=24,n_head=16,d_inner=4096,ff_activation='gelu',untie_r=True,attn_type='bi',initializer_range=0.02,layer_norm_eps=1e-12,dropout=0.1,mem_len=None,reuse_len=None,bi_data=False,clamp_len=-1,same_length=False,summary_type='last',summary_use_proj=True,summary_activation='tanh',summary_last_dropout=0.1,start_n_top=5,end_n_top=5,pad_token_id=5,bos_token_id=1,eos_token_id=2,**kwargs)
transformers.configuration_xlnet.XLNetConfig.hidden_size(self)
transformers.configuration_xlnet.XLNetConfig.max_position_embeddings(self)
transformers.configuration_xlnet.XLNetConfig.n_token(self)
transformers.configuration_xlnet.XLNetConfig.n_token(self,value)
transformers.configuration_xlnet.XLNetConfig.num_attention_heads(self)
transformers.configuration_xlnet.XLNetConfig.num_hidden_layers(self)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/configuration_openai.py----------------------------------------
A:transformers.configuration_openai.logger->utils.logging.get_logger(__name__)
transformers.OpenAIGPTConfig(self,vocab_size=40478,n_positions=512,n_ctx=512,n_embd=768,n_layer=12,n_head=12,afn='gelu',resid_pdrop=0.1,embd_pdrop=0.1,attn_pdrop=0.1,layer_norm_epsilon=1e-05,initializer_range=0.02,predict_special_tokens=True,summary_type='cls_index',summary_use_proj=True,summary_activation=None,summary_proj_to_labels=True,summary_first_dropout=0.1,**kwargs)
transformers.OpenAIGPTConfig.hidden_size(self)
transformers.OpenAIGPTConfig.max_position_embeddings(self)
transformers.OpenAIGPTConfig.num_attention_heads(self)
transformers.OpenAIGPTConfig.num_hidden_layers(self)
transformers.configuration_openai.OpenAIGPTConfig(self,vocab_size=40478,n_positions=512,n_ctx=512,n_embd=768,n_layer=12,n_head=12,afn='gelu',resid_pdrop=0.1,embd_pdrop=0.1,attn_pdrop=0.1,layer_norm_epsilon=1e-05,initializer_range=0.02,predict_special_tokens=True,summary_type='cls_index',summary_use_proj=True,summary_activation=None,summary_proj_to_labels=True,summary_first_dropout=0.1,**kwargs)
transformers.configuration_openai.OpenAIGPTConfig.__init__(self,vocab_size=40478,n_positions=512,n_ctx=512,n_embd=768,n_layer=12,n_head=12,afn='gelu',resid_pdrop=0.1,embd_pdrop=0.1,attn_pdrop=0.1,layer_norm_epsilon=1e-05,initializer_range=0.02,predict_special_tokens=True,summary_type='cls_index',summary_use_proj=True,summary_activation=None,summary_proj_to_labels=True,summary_first_dropout=0.1,**kwargs)
transformers.configuration_openai.OpenAIGPTConfig.hidden_size(self)
transformers.configuration_openai.OpenAIGPTConfig.max_position_embeddings(self)
transformers.configuration_openai.OpenAIGPTConfig.num_attention_heads(self)
transformers.configuration_openai.OpenAIGPTConfig.num_hidden_layers(self)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/tokenization_albert.py----------------------------------------
A:transformers.tokenization_albert.logger->utils.logging.get_logger(__name__)
A:transformers.tokenization_albert.self.sp_model->sentencepiece.SentencePieceProcessor()
A:transformers.tokenization_albert.state->self.__dict__.copy()
A:transformers.tokenization_albert.outputs->outputs.lower().lower()
A:transformers.tokenization_albert.text->self.preprocess_text(text)
A:transformers.tokenization_albert.pieces->self.sp_model.SampleEncodeAsPieces(text, 64, 0.1)
A:transformers.tokenization_albert.cur_pieces->self.sp_model.EncodeAsPieces(piece[:-1].replace(SPIECE_UNDERLINE, ''))
A:transformers.tokenization_albert.out_string->''.join(tokens).replace(SPIECE_UNDERLINE, ' ').strip()
A:transformers.tokenization_albert.out_vocab_file->os.path.join(save_directory, VOCAB_FILES_NAMES['vocab_file'])
transformers.AlbertTokenizer(self,vocab_file,do_lower_case=True,remove_space=True,keep_accents=False,bos_token='[CLS]',eos_token='[SEP]',unk_token='<unk>',sep_token='[SEP]',pad_token='<pad>',cls_token='[CLS]',mask_token='[MASK]',**kwargs)
transformers.AlbertTokenizer.__getstate__(self)
transformers.AlbertTokenizer.__setstate__(self,d)
transformers.AlbertTokenizer._convert_id_to_token(self,index)
transformers.AlbertTokenizer._convert_token_to_id(self,token)
transformers.AlbertTokenizer._tokenize(self,text,sample=False)
transformers.AlbertTokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.AlbertTokenizer.convert_tokens_to_string(self,tokens)
transformers.AlbertTokenizer.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.AlbertTokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.AlbertTokenizer.get_vocab(self)
transformers.AlbertTokenizer.preprocess_text(self,inputs)
transformers.AlbertTokenizer.save_vocabulary(self,save_directory)
transformers.AlbertTokenizer.vocab_size(self)
transformers.tokenization_albert.AlbertTokenizer(self,vocab_file,do_lower_case=True,remove_space=True,keep_accents=False,bos_token='[CLS]',eos_token='[SEP]',unk_token='<unk>',sep_token='[SEP]',pad_token='<pad>',cls_token='[CLS]',mask_token='[MASK]',**kwargs)
transformers.tokenization_albert.AlbertTokenizer.__getstate__(self)
transformers.tokenization_albert.AlbertTokenizer.__init__(self,vocab_file,do_lower_case=True,remove_space=True,keep_accents=False,bos_token='[CLS]',eos_token='[SEP]',unk_token='<unk>',sep_token='[SEP]',pad_token='<pad>',cls_token='[CLS]',mask_token='[MASK]',**kwargs)
transformers.tokenization_albert.AlbertTokenizer.__setstate__(self,d)
transformers.tokenization_albert.AlbertTokenizer._convert_id_to_token(self,index)
transformers.tokenization_albert.AlbertTokenizer._convert_token_to_id(self,token)
transformers.tokenization_albert.AlbertTokenizer._tokenize(self,text,sample=False)
transformers.tokenization_albert.AlbertTokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.tokenization_albert.AlbertTokenizer.convert_tokens_to_string(self,tokens)
transformers.tokenization_albert.AlbertTokenizer.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.tokenization_albert.AlbertTokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.tokenization_albert.AlbertTokenizer.get_vocab(self)
transformers.tokenization_albert.AlbertTokenizer.preprocess_text(self,inputs)
transformers.tokenization_albert.AlbertTokenizer.save_vocabulary(self,save_directory)
transformers.tokenization_albert.AlbertTokenizer.vocab_size(self)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/configuration_flaubert.py----------------------------------------
A:transformers.configuration_flaubert.logger->utils.logging.get_logger(__name__)
transformers.FlaubertConfig(self,layerdrop=0.0,pre_norm=False,pad_token_id=2,bos_token_id=0,**kwargs)
transformers.configuration_flaubert.FlaubertConfig(self,layerdrop=0.0,pre_norm=False,pad_token_id=2,bos_token_id=0,**kwargs)
transformers.configuration_flaubert.FlaubertConfig.__init__(self,layerdrop=0.0,pre_norm=False,pad_token_id=2,bos_token_id=0,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/trainer_utils.py----------------------------------------
A:transformers.trainer_utils.loss->metrics.pop('eval_loss', None)
A:transformers.trainer_utils._->metrics.pop('epoch', None)
transformers.EvalPrediction(NamedTuple)
transformers.set_seed(seed:int)
transformers.trainer_utils.BestRun(NamedTuple)
transformers.trainer_utils.EvalPrediction(NamedTuple)
transformers.trainer_utils.HPSearchBackend(ExplicitEnum)
transformers.trainer_utils.PredictionOutput(NamedTuple)
transformers.trainer_utils.TrainOutput(NamedTuple)
transformers.trainer_utils.default_compute_objective(metrics:Dict[str,float])->float
transformers.trainer_utils.default_hp_space_optuna(trial)->Dict[str, float]
transformers.trainer_utils.default_hp_space_ray(trial)->Dict[str, float]
transformers.trainer_utils.set_seed(seed:int)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/modeling_tf_roberta.py----------------------------------------
A:transformers.modeling_tf_roberta.logger->utils.logging.get_logger(__name__)
A:transformers.modeling_tf_roberta.mask->tensorflow.cast(tf.math.not_equal(x, self.padding_idx), dtype=tf.int32)
A:transformers.modeling_tf_roberta.position_ids->inputs.get('position_ids', position_ids)
A:transformers.modeling_tf_roberta.self.embeddings->TFRobertaEmbeddings(config, name='embeddings')
A:transformers.modeling_tf_roberta.self.roberta->TFRobertaMainLayer(config, name='roberta')
A:transformers.modeling_tf_roberta.outputs->self.roberta(inputs, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, training=training)
A:transformers.modeling_tf_roberta.self.dense->tensorflow.keras.layers.Dense(config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), activation='tanh', name='dense')
A:transformers.modeling_tf_roberta.self.layer_norm->tensorflow.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name='layer_norm')
A:transformers.modeling_tf_roberta.self.act->tensorflow.keras.layers.Activation(gelu)
A:transformers.modeling_tf_roberta.self.bias->self.add_weight(shape=(self.vocab_size,), initializer='zeros', trainable=True, name='bias')
A:transformers.modeling_tf_roberta.x->self.out_proj(x)
A:transformers.modeling_tf_roberta.self.lm_head->TFRobertaLMHead(config, self.roberta.embeddings, name='lm_head')
A:transformers.modeling_tf_roberta.labels->inputs.pop('labels', labels)
A:transformers.modeling_tf_roberta.prediction_scores->self.lm_head(sequence_output)
A:transformers.modeling_tf_roberta.self.dropout->tensorflow.keras.layers.Dropout(config.hidden_dropout_prob)
A:transformers.modeling_tf_roberta.self.out_proj->tensorflow.keras.layers.Dense(config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name='out_proj')
A:transformers.modeling_tf_roberta.self.classifier->tensorflow.keras.layers.Dense(config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name='classifier')
A:transformers.modeling_tf_roberta.logits->self.qa_outputs(sequence_output)
A:transformers.modeling_tf_roberta.input_ids->inputs.get('input_ids')
A:transformers.modeling_tf_roberta.attention_mask->inputs.get('attention_mask', attention_mask)
A:transformers.modeling_tf_roberta.token_type_ids->inputs.get('token_type_ids', token_type_ids)
A:transformers.modeling_tf_roberta.head_mask->inputs.get('head_mask', head_mask)
A:transformers.modeling_tf_roberta.inputs_embeds->inputs.get('inputs_embeds', inputs_embeds)
A:transformers.modeling_tf_roberta.output_attentions->inputs.get('output_attentions', output_attentions)
A:transformers.modeling_tf_roberta.output_hidden_states->inputs.get('output_hidden_states', output_attentions)
A:transformers.modeling_tf_roberta.return_dict->inputs.get('return_dict', return_dict)
A:transformers.modeling_tf_roberta.pooled_output->self.dropout(pooled_output, training=training)
A:transformers.modeling_tf_roberta.reshaped_logits->tensorflow.reshape(logits, (-1, num_choices))
A:transformers.modeling_tf_roberta.sequence_output->self.dropout(sequence_output, training=training)
A:transformers.modeling_tf_roberta.self.qa_outputs->tensorflow.keras.layers.Dense(config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name='qa_outputs')
A:transformers.modeling_tf_roberta.start_positions->inputs.pop('start_positions', start_positions)
A:transformers.modeling_tf_roberta.end_positions->inputs.pop('end_positions', start_positions)
A:transformers.modeling_tf_roberta.(start_logits, end_logits)->tensorflow.split(logits, 2, axis=-1)
A:transformers.modeling_tf_roberta.start_logits->tensorflow.squeeze(start_logits, axis=-1)
A:transformers.modeling_tf_roberta.end_logits->tensorflow.squeeze(end_logits, axis=-1)
A:transformers.modeling_tf_roberta.loss->self.compute_loss(labels, (start_logits, end_logits))
transformers.TFRobertaForMaskedLM(self,config,*inputs,**kwargs)
transformers.TFRobertaForMaskedLM.call(self,inputs=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.TFRobertaForMaskedLM.get_output_embeddings(self)
transformers.TFRobertaForMultipleChoice(self,config,*inputs,**kwargs)
transformers.TFRobertaForMultipleChoice.call(self,inputs,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.TFRobertaForMultipleChoice.dummy_inputs(self)
transformers.TFRobertaForQuestionAnswering(self,config,*inputs,**kwargs)
transformers.TFRobertaForQuestionAnswering.call(self,inputs=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,start_positions=None,end_positions=None,training=False)
transformers.TFRobertaForSequenceClassification(self,config,*inputs,**kwargs)
transformers.TFRobertaForSequenceClassification.call(self,inputs=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.TFRobertaForTokenClassification(self,config,*inputs,**kwargs)
transformers.TFRobertaForTokenClassification.call(self,inputs=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.TFRobertaMainLayer(self,config,**kwargs)
transformers.TFRobertaModel(self,config,*inputs,**kwargs)
transformers.TFRobertaModel.call(self,inputs,**kwargs)
transformers.TFRobertaPreTrainedModel(TFPreTrainedModel)
transformers.modeling_tf_roberta.TFRobertaClassificationHead(self,config,**kwargs)
transformers.modeling_tf_roberta.TFRobertaClassificationHead.__init__(self,config,**kwargs)
transformers.modeling_tf_roberta.TFRobertaClassificationHead.call(self,features,training=False)
transformers.modeling_tf_roberta.TFRobertaEmbeddings(self,config,**kwargs)
transformers.modeling_tf_roberta.TFRobertaEmbeddings.__init__(self,config,**kwargs)
transformers.modeling_tf_roberta.TFRobertaEmbeddings._embedding(self,input_ids,position_ids,token_type_ids,inputs_embeds,training=False)
transformers.modeling_tf_roberta.TFRobertaEmbeddings.create_position_ids_from_input_ids(self,x)
transformers.modeling_tf_roberta.TFRobertaEmbeddings.create_position_ids_from_inputs_embeds(self,inputs_embeds)
transformers.modeling_tf_roberta.TFRobertaForMaskedLM(self,config,*inputs,**kwargs)
transformers.modeling_tf_roberta.TFRobertaForMaskedLM.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_roberta.TFRobertaForMaskedLM.call(self,inputs=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.modeling_tf_roberta.TFRobertaForMaskedLM.get_output_embeddings(self)
transformers.modeling_tf_roberta.TFRobertaForMultipleChoice(self,config,*inputs,**kwargs)
transformers.modeling_tf_roberta.TFRobertaForMultipleChoice.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_roberta.TFRobertaForMultipleChoice.call(self,inputs,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.modeling_tf_roberta.TFRobertaForMultipleChoice.dummy_inputs(self)
transformers.modeling_tf_roberta.TFRobertaForQuestionAnswering(self,config,*inputs,**kwargs)
transformers.modeling_tf_roberta.TFRobertaForQuestionAnswering.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_roberta.TFRobertaForQuestionAnswering.call(self,inputs=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,start_positions=None,end_positions=None,training=False)
transformers.modeling_tf_roberta.TFRobertaForSequenceClassification(self,config,*inputs,**kwargs)
transformers.modeling_tf_roberta.TFRobertaForSequenceClassification.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_roberta.TFRobertaForSequenceClassification.call(self,inputs=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.modeling_tf_roberta.TFRobertaForTokenClassification(self,config,*inputs,**kwargs)
transformers.modeling_tf_roberta.TFRobertaForTokenClassification.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_roberta.TFRobertaForTokenClassification.call(self,inputs=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.modeling_tf_roberta.TFRobertaLMHead(self,config,input_embeddings,**kwargs)
transformers.modeling_tf_roberta.TFRobertaLMHead.__init__(self,config,input_embeddings,**kwargs)
transformers.modeling_tf_roberta.TFRobertaLMHead.build(self,input_shape)
transformers.modeling_tf_roberta.TFRobertaLMHead.call(self,features)
transformers.modeling_tf_roberta.TFRobertaMainLayer(self,config,**kwargs)
transformers.modeling_tf_roberta.TFRobertaMainLayer.__init__(self,config,**kwargs)
transformers.modeling_tf_roberta.TFRobertaModel(self,config,*inputs,**kwargs)
transformers.modeling_tf_roberta.TFRobertaModel.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_roberta.TFRobertaModel.call(self,inputs,**kwargs)
transformers.modeling_tf_roberta.TFRobertaPreTrainedModel(TFPreTrainedModel)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/tokenization_mbart.py----------------------------------------
A:transformers.tokenization_mbart.logger->utils.logging.get_logger(__name__)
A:transformers.tokenization_mbart.self.sp_model_size->len(self.sp_model)
A:transformers.tokenization_mbart.self._additional_special_tokens->list(self.lang_code_to_id.keys())
transformers.MBartTokenizer(self,*args,**kwargs)
transformers.MBartTokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.MBartTokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.MBartTokenizer.prepare_seq2seq_batch(self,src_texts:List[str],src_lang:str='en_XX',tgt_texts:Optional[List[str]]=None,tgt_lang:str='ro_RO',max_length:Optional[int]=None,max_target_length:Optional[int]=None,truncation:bool=True,padding:str='longest',return_tensors:str='pt',**kwargs)->BatchEncoding
transformers.MBartTokenizer.set_src_lang_special_tokens(self,src_lang)->None
transformers.MBartTokenizer.set_tgt_lang_special_tokens(self,lang:str)->None
transformers.tokenization_mbart.MBartTokenizer(self,*args,**kwargs)
transformers.tokenization_mbart.MBartTokenizer.__init__(self,*args,**kwargs)
transformers.tokenization_mbart.MBartTokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.tokenization_mbart.MBartTokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.tokenization_mbart.MBartTokenizer.prepare_seq2seq_batch(self,src_texts:List[str],src_lang:str='en_XX',tgt_texts:Optional[List[str]]=None,tgt_lang:str='ro_RO',max_length:Optional[int]=None,max_target_length:Optional[int]=None,truncation:bool=True,padding:str='longest',return_tensors:str='pt',**kwargs)->BatchEncoding
transformers.tokenization_mbart.MBartTokenizer.set_src_lang_special_tokens(self,src_lang)->None
transformers.tokenization_mbart.MBartTokenizer.set_tgt_lang_special_tokens(self,lang:str)->None


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/convert_bert_pytorch_checkpoint_to_original_tf.py----------------------------------------
A:transformers.convert_bert_pytorch_checkpoint_to_original_tf.state_dict->transformers.BertModel.from_pretrained(pretrained_model_name_or_path=args.model_name, state_dict=torch.load(args.pytorch_model_path), cache_dir=args.cache_dir).state_dict()
A:transformers.convert_bert_pytorch_checkpoint_to_original_tf.name->name.replace(patt, repl).replace(patt, repl)
A:transformers.convert_bert_pytorch_checkpoint_to_original_tf.tf_dtype->tensorflow.dtypes.as_dtype(tensor.dtype)
A:transformers.convert_bert_pytorch_checkpoint_to_original_tf.tf_var->create_tf_var(tensor=torch_tensor, name=tf_name, session=session)
A:transformers.convert_bert_pytorch_checkpoint_to_original_tf.tf_name->to_tf_var_name(var_name)
A:transformers.convert_bert_pytorch_checkpoint_to_original_tf.torch_tensor->state_dict[var_name].numpy()
A:transformers.convert_bert_pytorch_checkpoint_to_original_tf.tf_weight->session.run(tf_var)
A:transformers.convert_bert_pytorch_checkpoint_to_original_tf.saver->tensorflow.train.Saver(tf.trainable_variables())
A:transformers.convert_bert_pytorch_checkpoint_to_original_tf.parser->argparse.ArgumentParser()
A:transformers.convert_bert_pytorch_checkpoint_to_original_tf.args->argparse.ArgumentParser().parse_args(raw_args)
A:transformers.convert_bert_pytorch_checkpoint_to_original_tf.model->transformers.BertModel.from_pretrained(pretrained_model_name_or_path=args.model_name, state_dict=torch.load(args.pytorch_model_path), cache_dir=args.cache_dir)
transformers.convert_bert_pytorch_checkpoint_to_original_tf.convert_pytorch_checkpoint_to_tf(model:BertModel,ckpt_dir:str,model_name:str)
transformers.convert_bert_pytorch_checkpoint_to_original_tf.main(raw_args=None)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/convert_openai_original_tf_checkpoint_to_pytorch.py----------------------------------------
A:transformers.convert_openai_original_tf_checkpoint_to_pytorch.config->transformers.OpenAIGPTConfig.from_json_file(openai_config_file)
A:transformers.convert_openai_original_tf_checkpoint_to_pytorch.model->OpenAIGPTModel(config)
A:transformers.convert_openai_original_tf_checkpoint_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.convert_openai_original_tf_checkpoint_to_pytorch.args->argparse.ArgumentParser().parse_args()
transformers.convert_openai_original_tf_checkpoint_to_pytorch.convert_openai_checkpoint_to_pytorch(openai_checkpoint_folder_path,openai_config_file,pytorch_dump_folder_path)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/modeling_auto.py----------------------------------------
A:transformers.modeling_auto.logger->utils.logging.get_logger(__name__)
A:transformers.modeling_auto.MODEL_MAPPING->OrderedDict([(RetriBertConfig, RetriBertModel), (T5Config, T5Model), (DistilBertConfig, DistilBertModel), (AlbertConfig, AlbertModel), (CamembertConfig, CamembertModel), (XLMRobertaConfig, XLMRobertaModel), (BartConfig, BartModel), (LongformerConfig, LongformerModel), (RobertaConfig, RobertaModel), (BertConfig, BertModel), (OpenAIGPTConfig, OpenAIGPTModel), (GPT2Config, GPT2Model), (MobileBertConfig, MobileBertModel), (TransfoXLConfig, TransfoXLModel), (XLNetConfig, XLNetModel), (FlaubertConfig, FlaubertModel), (XLMConfig, XLMModel), (CTRLConfig, CTRLModel), (ElectraConfig, ElectraModel), (ReformerConfig, ReformerModel)])
A:transformers.modeling_auto.MODEL_FOR_PRETRAINING_MAPPING->OrderedDict([(RetriBertConfig, RetriBertModel), (T5Config, T5ForConditionalGeneration), (DistilBertConfig, DistilBertForMaskedLM), (AlbertConfig, AlbertForPreTraining), (CamembertConfig, CamembertForMaskedLM), (XLMRobertaConfig, XLMRobertaForMaskedLM), (BartConfig, BartForConditionalGeneration), (LongformerConfig, LongformerForMaskedLM), (RobertaConfig, RobertaForMaskedLM), (BertConfig, BertForPreTraining), (OpenAIGPTConfig, OpenAIGPTLMHeadModel), (GPT2Config, GPT2LMHeadModel), (MobileBertConfig, MobileBertForPreTraining), (TransfoXLConfig, TransfoXLLMHeadModel), (XLNetConfig, XLNetLMHeadModel), (FlaubertConfig, FlaubertWithLMHeadModel), (XLMConfig, XLMWithLMHeadModel), (CTRLConfig, CTRLLMHeadModel), (ElectraConfig, ElectraForPreTraining)])
A:transformers.modeling_auto.MODEL_WITH_LM_HEAD_MAPPING->OrderedDict([(T5Config, T5ForConditionalGeneration), (DistilBertConfig, DistilBertForMaskedLM), (AlbertConfig, AlbertForMaskedLM), (CamembertConfig, CamembertForMaskedLM), (XLMRobertaConfig, XLMRobertaForMaskedLM), (MarianConfig, MarianMTModel), (BartConfig, BartForConditionalGeneration), (LongformerConfig, LongformerForMaskedLM), (RobertaConfig, RobertaForMaskedLM), (BertConfig, BertForMaskedLM), (OpenAIGPTConfig, OpenAIGPTLMHeadModel), (GPT2Config, GPT2LMHeadModel), (MobileBertConfig, MobileBertForMaskedLM), (TransfoXLConfig, TransfoXLLMHeadModel), (XLNetConfig, XLNetLMHeadModel), (FlaubertConfig, FlaubertWithLMHeadModel), (XLMConfig, XLMWithLMHeadModel), (CTRLConfig, CTRLLMHeadModel), (ElectraConfig, ElectraForMaskedLM), (EncoderDecoderConfig, EncoderDecoderModel), (ReformerConfig, ReformerModelWithLMHead)])
A:transformers.modeling_auto.MODEL_FOR_CAUSAL_LM_MAPPING->OrderedDict([(CamembertConfig, CamembertForCausalLM), (RobertaConfig, RobertaForCausalLM), (BertConfig, BertLMHeadModel), (OpenAIGPTConfig, OpenAIGPTLMHeadModel), (GPT2Config, GPT2LMHeadModel), (TransfoXLConfig, TransfoXLLMHeadModel), (XLNetConfig, XLNetLMHeadModel), (XLMConfig, XLMWithLMHeadModel), (CTRLConfig, CTRLLMHeadModel), (ReformerConfig, ReformerModelWithLMHead)])
A:transformers.modeling_auto.MODEL_FOR_MASKED_LM_MAPPING->OrderedDict([(DistilBertConfig, DistilBertForMaskedLM), (AlbertConfig, AlbertForMaskedLM), (BartConfig, BartForConditionalGeneration), (CamembertConfig, CamembertForMaskedLM), (XLMRobertaConfig, XLMRobertaForMaskedLM), (LongformerConfig, LongformerForMaskedLM), (RobertaConfig, RobertaForMaskedLM), (BertConfig, BertForMaskedLM), (MobileBertConfig, MobileBertForMaskedLM), (FlaubertConfig, FlaubertWithLMHeadModel), (XLMConfig, XLMWithLMHeadModel), (ElectraConfig, ElectraForMaskedLM), (ReformerConfig, ReformerForMaskedLM)])
A:transformers.modeling_auto.MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING->OrderedDict([(T5Config, T5ForConditionalGeneration), (PegasusConfig, PegasusForConditionalGeneration), (MarianConfig, MarianMTModel), (MBartConfig, MBartForConditionalGeneration), (BartConfig, BartForConditionalGeneration), (EncoderDecoderConfig, EncoderDecoderModel)])
A:transformers.modeling_auto.MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING->OrderedDict([(DistilBertConfig, DistilBertForSequenceClassification), (AlbertConfig, AlbertForSequenceClassification), (CamembertConfig, CamembertForSequenceClassification), (XLMRobertaConfig, XLMRobertaForSequenceClassification), (BartConfig, BartForSequenceClassification), (LongformerConfig, LongformerForSequenceClassification), (RobertaConfig, RobertaForSequenceClassification), (BertConfig, BertForSequenceClassification), (XLNetConfig, XLNetForSequenceClassification), (MobileBertConfig, MobileBertForSequenceClassification), (FlaubertConfig, FlaubertForSequenceClassification), (XLMConfig, XLMForSequenceClassification), (ElectraConfig, ElectraForSequenceClassification)])
A:transformers.modeling_auto.MODEL_FOR_QUESTION_ANSWERING_MAPPING->OrderedDict([(DistilBertConfig, DistilBertForQuestionAnswering), (AlbertConfig, AlbertForQuestionAnswering), (CamembertConfig, CamembertForQuestionAnswering), (BartConfig, BartForQuestionAnswering), (LongformerConfig, LongformerForQuestionAnswering), (XLMRobertaConfig, XLMRobertaForQuestionAnswering), (RobertaConfig, RobertaForQuestionAnswering), (BertConfig, BertForQuestionAnswering), (XLNetConfig, XLNetForQuestionAnsweringSimple), (FlaubertConfig, FlaubertForQuestionAnsweringSimple), (MobileBertConfig, MobileBertForQuestionAnswering), (XLMConfig, XLMForQuestionAnsweringSimple), (ElectraConfig, ElectraForQuestionAnswering), (ReformerConfig, ReformerForQuestionAnswering)])
A:transformers.modeling_auto.MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING->OrderedDict([(DistilBertConfig, DistilBertForTokenClassification), (CamembertConfig, CamembertForTokenClassification), (FlaubertConfig, FlaubertForTokenClassification), (XLMConfig, XLMForTokenClassification), (XLMRobertaConfig, XLMRobertaForTokenClassification), (LongformerConfig, LongformerForTokenClassification), (RobertaConfig, RobertaForTokenClassification), (BertConfig, BertForTokenClassification), (MobileBertConfig, MobileBertForTokenClassification), (XLNetConfig, XLNetForTokenClassification), (AlbertConfig, AlbertForTokenClassification), (ElectraConfig, ElectraForTokenClassification), (FlaubertConfig, FlaubertForTokenClassification)])
A:transformers.modeling_auto.MODEL_FOR_MULTIPLE_CHOICE_MAPPING->OrderedDict([(CamembertConfig, CamembertForMultipleChoice), (ElectraConfig, ElectraForMultipleChoice), (XLMRobertaConfig, XLMRobertaForMultipleChoice), (LongformerConfig, LongformerForMultipleChoice), (RobertaConfig, RobertaForMultipleChoice), (BertConfig, BertForMultipleChoice), (DistilBertConfig, DistilBertForMultipleChoice), (MobileBertConfig, MobileBertForMultipleChoice), (XLNetConfig, XLNetForMultipleChoice), (AlbertConfig, AlbertForMultipleChoice), (XLMConfig, XLMForMultipleChoice), (FlaubertConfig, FlaubertForMultipleChoice)])
A:transformers.modeling_auto.config->kwargs.pop('config', None)
A:transformers.modeling_auto.(config, kwargs)->configuration_auto.AutoConfig.from_pretrained(pretrained_model_name_or_path, return_unused_kwargs=True, **kwargs)
transformers.AutoModel(self)
transformers.AutoModel.from_config(cls,config)
transformers.AutoModel.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.AutoModelForCausalLM(self)
transformers.AutoModelForCausalLM.from_config(cls,config)
transformers.AutoModelForCausalLM.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.AutoModelForMaskedLM(self)
transformers.AutoModelForMaskedLM.from_config(cls,config)
transformers.AutoModelForMaskedLM.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.AutoModelForMultipleChoice(self)
transformers.AutoModelForMultipleChoice.from_config(cls,config)
transformers.AutoModelForMultipleChoice.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.AutoModelForPreTraining(self)
transformers.AutoModelForPreTraining.from_config(cls,config)
transformers.AutoModelForPreTraining.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.AutoModelForQuestionAnswering(self)
transformers.AutoModelForQuestionAnswering.from_config(cls,config)
transformers.AutoModelForQuestionAnswering.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.AutoModelForSeq2SeqLM(self)
transformers.AutoModelForSeq2SeqLM.from_config(cls,config)
transformers.AutoModelForSeq2SeqLM.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.AutoModelForSequenceClassification(self)
transformers.AutoModelForSequenceClassification.from_config(cls,config)
transformers.AutoModelForSequenceClassification.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.AutoModelForTokenClassification(self)
transformers.AutoModelForTokenClassification.from_config(cls,config)
transformers.AutoModelForTokenClassification.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.AutoModelWithLMHead(self)
transformers.AutoModelWithLMHead.from_config(cls,config)
transformers.AutoModelWithLMHead.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.modeling_auto.AutoModel(self)
transformers.modeling_auto.AutoModel.__init__(self)
transformers.modeling_auto.AutoModel.from_config(cls,config)
transformers.modeling_auto.AutoModel.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.modeling_auto.AutoModelForCausalLM(self)
transformers.modeling_auto.AutoModelForCausalLM.__init__(self)
transformers.modeling_auto.AutoModelForCausalLM.from_config(cls,config)
transformers.modeling_auto.AutoModelForCausalLM.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.modeling_auto.AutoModelForMaskedLM(self)
transformers.modeling_auto.AutoModelForMaskedLM.__init__(self)
transformers.modeling_auto.AutoModelForMaskedLM.from_config(cls,config)
transformers.modeling_auto.AutoModelForMaskedLM.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.modeling_auto.AutoModelForMultipleChoice(self)
transformers.modeling_auto.AutoModelForMultipleChoice.__init__(self)
transformers.modeling_auto.AutoModelForMultipleChoice.from_config(cls,config)
transformers.modeling_auto.AutoModelForMultipleChoice.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.modeling_auto.AutoModelForPreTraining(self)
transformers.modeling_auto.AutoModelForPreTraining.__init__(self)
transformers.modeling_auto.AutoModelForPreTraining.from_config(cls,config)
transformers.modeling_auto.AutoModelForPreTraining.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.modeling_auto.AutoModelForQuestionAnswering(self)
transformers.modeling_auto.AutoModelForQuestionAnswering.__init__(self)
transformers.modeling_auto.AutoModelForQuestionAnswering.from_config(cls,config)
transformers.modeling_auto.AutoModelForQuestionAnswering.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.modeling_auto.AutoModelForSeq2SeqLM(self)
transformers.modeling_auto.AutoModelForSeq2SeqLM.__init__(self)
transformers.modeling_auto.AutoModelForSeq2SeqLM.from_config(cls,config)
transformers.modeling_auto.AutoModelForSeq2SeqLM.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.modeling_auto.AutoModelForSequenceClassification(self)
transformers.modeling_auto.AutoModelForSequenceClassification.__init__(self)
transformers.modeling_auto.AutoModelForSequenceClassification.from_config(cls,config)
transformers.modeling_auto.AutoModelForSequenceClassification.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.modeling_auto.AutoModelForTokenClassification(self)
transformers.modeling_auto.AutoModelForTokenClassification.__init__(self)
transformers.modeling_auto.AutoModelForTokenClassification.from_config(cls,config)
transformers.modeling_auto.AutoModelForTokenClassification.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.modeling_auto.AutoModelWithLMHead(self)
transformers.modeling_auto.AutoModelWithLMHead.__init__(self)
transformers.modeling_auto.AutoModelWithLMHead.from_config(cls,config)
transformers.modeling_auto.AutoModelWithLMHead.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/configuration_pegasus.py----------------------------------------
A:transformers.configuration_pegasus.logger->utils.logging.get_logger(__name__)
A:transformers.configuration_pegasus.DEFAULTS->dict(vocab_size=96103, max_position_embeddings=512, d_model=1024, encoder_ffn_dim=4096, decoder_ffn_dim=4096, encoder_attention_heads=16, decoder_attention_heads=16, encoder_layers=16, decoder_layers=16, dropout=0.1, attention_dropout=0.1, activation_dropout=0.1, pad_token_id=0, eos_token_id=1, is_encoder_decoder=True, normalize_before=True, scale_embedding=True, normalize_embedding=False, add_final_layer_norm=True, static_position_embeddings=True, num_beams=8, activation_function='relu')
transformers.PegasusConfig(BartConfig)
transformers.configuration_pegasus.PegasusConfig(BartConfig)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/modeling_mmbt.py----------------------------------------
A:transformers.modeling_mmbt.logger->utils.logging.get_logger(__name__)
A:transformers.modeling_mmbt.self.proj_embeddings->torch.nn.Linear(config.modal_hidden_size, config.hidden_size)
A:transformers.modeling_mmbt.self.dropout->torch.nn.Dropout(config.hidden_dropout_prob)
A:transformers.modeling_mmbt.token_embeddings->torch.cat([token_embeddings, end_token_embeds.unsqueeze(1)], dim=1)
A:transformers.modeling_mmbt.seq_length->torch.cat([token_embeddings, end_token_embeds.unsqueeze(1)], dim=1).size(1)
A:transformers.modeling_mmbt.start_token_embeds->self.word_embeddings(start_token)
A:transformers.modeling_mmbt.end_token_embeds->self.word_embeddings(end_token)
A:transformers.modeling_mmbt.position_ids->position_ids.unsqueeze(0).expand(input_modal.size(0), seq_length).unsqueeze(0).expand(input_modal.size(0), seq_length)
A:transformers.modeling_mmbt.token_type_ids->torch.ones(input_txt_shape, dtype=torch.long, device=device)
A:transformers.modeling_mmbt.position_embeddings->self.position_embeddings(position_ids)
A:transformers.modeling_mmbt.token_type_embeddings->self.token_type_embeddings(token_type_ids)
A:transformers.modeling_mmbt.embeddings->self.dropout(embeddings)
A:transformers.modeling_mmbt.self.modal_encoder->ModalEmbeddings(config, encoder, transformer.embeddings)
A:transformers.modeling_mmbt.input_txt_shape->input_ids.size()
A:transformers.modeling_mmbt.modal_embeddings->self.modal_encoder(input_modal, start_token=modal_start_tokens, end_token=modal_end_tokens, position_ids=modal_position_ids, token_type_ids=modal_token_type_ids)
A:transformers.modeling_mmbt.txt_embeddings->self.transformer.embeddings(input_ids=input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds)
A:transformers.modeling_mmbt.embedding_output->torch.cat([modal_embeddings, txt_embeddings], 1)
A:transformers.modeling_mmbt.attention_mask->torch.cat([torch.ones(input_modal_shape, device=device, dtype=torch.long), attention_mask], dim=1)
A:transformers.modeling_mmbt.encoder_attention_mask->torch.cat([torch.ones(input_modal_shape, device=device), encoder_attention_mask], dim=1)
A:transformers.modeling_mmbt.extended_attention_mask->self.get_extended_attention_mask(attention_mask, input_shape, self.device)
A:transformers.modeling_mmbt.encoder_extended_attention_mask->self.invert_attention_mask(encoder_attention_mask)
A:transformers.modeling_mmbt.head_mask->self.get_head_mask(head_mask, self.config.num_hidden_layers)
A:transformers.modeling_mmbt.encoder_outputs->self.transformer.encoder(embedding_output, attention_mask=extended_attention_mask, head_mask=head_mask, encoder_hidden_states=encoder_hidden_states, encoder_attention_mask=encoder_extended_attention_mask, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.modeling_mmbt.pooled_output->self.dropout(pooled_output)
A:transformers.modeling_mmbt.self.mmbt->MMBTModel(config, transformer, encoder)
A:transformers.modeling_mmbt.self.classifier->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.modeling_mmbt.outputs->self.mmbt(input_modal=input_modal, input_ids=input_ids, modal_start_tokens=modal_start_tokens, modal_end_tokens=modal_end_tokens, attention_mask=attention_mask, token_type_ids=token_type_ids, modal_token_type_ids=modal_token_type_ids, position_ids=position_ids, modal_position_ids=modal_position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds, return_dict=return_dict)
A:transformers.modeling_mmbt.logits->self.classifier(pooled_output)
A:transformers.modeling_mmbt.loss_fct->CrossEntropyLoss()
A:transformers.modeling_mmbt.loss->loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
transformers.MMBTForClassification(self,config,transformer,encoder)
transformers.MMBTForClassification.forward(self,input_modal,input_ids=None,modal_start_tokens=None,modal_end_tokens=None,attention_mask=None,token_type_ids=None,modal_token_type_ids=None,position_ids=None,modal_position_ids=None,head_mask=None,inputs_embeds=None,labels=None,return_dict=None)
transformers.MMBTModel(self,config,transformer,encoder)
transformers.MMBTModel.forward(self,input_modal,input_ids=None,modal_start_tokens=None,modal_end_tokens=None,attention_mask=None,token_type_ids=None,modal_token_type_ids=None,position_ids=None,modal_position_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.MMBTModel.get_input_embeddings(self)
transformers.MMBTModel.set_input_embeddings(self,value)
transformers.ModalEmbeddings(self,config,encoder,embeddings)
transformers.ModalEmbeddings.forward(self,input_modal,start_token=None,end_token=None,position_ids=None,token_type_ids=None)
transformers.modeling_mmbt.MMBTForClassification(self,config,transformer,encoder)
transformers.modeling_mmbt.MMBTForClassification.__init__(self,config,transformer,encoder)
transformers.modeling_mmbt.MMBTForClassification.forward(self,input_modal,input_ids=None,modal_start_tokens=None,modal_end_tokens=None,attention_mask=None,token_type_ids=None,modal_token_type_ids=None,position_ids=None,modal_position_ids=None,head_mask=None,inputs_embeds=None,labels=None,return_dict=None)
transformers.modeling_mmbt.MMBTModel(self,config,transformer,encoder)
transformers.modeling_mmbt.MMBTModel.__init__(self,config,transformer,encoder)
transformers.modeling_mmbt.MMBTModel.forward(self,input_modal,input_ids=None,modal_start_tokens=None,modal_end_tokens=None,attention_mask=None,token_type_ids=None,modal_token_type_ids=None,position_ids=None,modal_position_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.modeling_mmbt.MMBTModel.get_input_embeddings(self)
transformers.modeling_mmbt.MMBTModel.set_input_embeddings(self,value)
transformers.modeling_mmbt.ModalEmbeddings(self,config,encoder,embeddings)
transformers.modeling_mmbt.ModalEmbeddings.__init__(self,config,encoder,embeddings)
transformers.modeling_mmbt.ModalEmbeddings.forward(self,input_modal,start_token=None,end_token=None,position_ids=None,token_type_ids=None)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/tokenization_bert_japanese.py----------------------------------------
A:transformers.tokenization_bert_japanese.logger->utils.logging.get_logger(__name__)
A:transformers.tokenization_bert_japanese.self.vocab->load_vocab(vocab_file)
A:transformers.tokenization_bert_japanese.self.ids_to_tokens->collections.OrderedDict([(ids, tok) for (tok, ids) in self.vocab.items()])
A:transformers.tokenization_bert_japanese.self.word_tokenizer->MecabTokenizer(do_lower_case=do_lower_case, never_split=never_split, **mecab_kwargs or {})
A:transformers.tokenization_bert_japanese.self.subword_tokenizer->CharacterTokenizer(vocab=self.vocab, unk_token=self.unk_token)
A:transformers.tokenization_bert_japanese.tokens->self.word_tokenizer.tokenize(text, never_split=self.all_special_tokens)
A:transformers.tokenization_bert_japanese.mecabrc->os.path.join(dic_dir, 'mecabrc')
A:transformers.tokenization_bert_japanese.self.mecab->fugashi.GenericTagger(mecab_option)
A:transformers.tokenization_bert_japanese.text->unicodedata.normalize('NFKC', text)
A:transformers.tokenization_bert_japanese.token->token.lower().lower()
transformers.BertJapaneseTokenizer(self,vocab_file,do_lower_case=False,do_word_tokenize=True,do_subword_tokenize=True,word_tokenizer_type='basic',subword_tokenizer_type='wordpiece',never_split=None,unk_token='[UNK]',sep_token='[SEP]',pad_token='[PAD]',cls_token='[CLS]',mask_token='[MASK]',mecab_kwargs=None,**kwargs)
transformers.BertJapaneseTokenizer._tokenize(self,text)
transformers.CharacterTokenizer(self,vocab,unk_token,normalize_text=True)
transformers.CharacterTokenizer.tokenize(self,text)
transformers.MecabTokenizer(self,do_lower_case=False,never_split=None,normalize_text=True,mecab_dic:Optional[str]='ipadic',mecab_option:Optional[str]=None)
transformers.MecabTokenizer.tokenize(self,text,never_split=None,**kwargs)
transformers.tokenization_bert_japanese.BertJapaneseTokenizer(self,vocab_file,do_lower_case=False,do_word_tokenize=True,do_subword_tokenize=True,word_tokenizer_type='basic',subword_tokenizer_type='wordpiece',never_split=None,unk_token='[UNK]',sep_token='[SEP]',pad_token='[PAD]',cls_token='[CLS]',mask_token='[MASK]',mecab_kwargs=None,**kwargs)
transformers.tokenization_bert_japanese.BertJapaneseTokenizer.__init__(self,vocab_file,do_lower_case=False,do_word_tokenize=True,do_subword_tokenize=True,word_tokenizer_type='basic',subword_tokenizer_type='wordpiece',never_split=None,unk_token='[UNK]',sep_token='[SEP]',pad_token='[PAD]',cls_token='[CLS]',mask_token='[MASK]',mecab_kwargs=None,**kwargs)
transformers.tokenization_bert_japanese.BertJapaneseTokenizer._tokenize(self,text)
transformers.tokenization_bert_japanese.CharacterTokenizer(self,vocab,unk_token,normalize_text=True)
transformers.tokenization_bert_japanese.CharacterTokenizer.__init__(self,vocab,unk_token,normalize_text=True)
transformers.tokenization_bert_japanese.CharacterTokenizer.tokenize(self,text)
transformers.tokenization_bert_japanese.MecabTokenizer(self,do_lower_case=False,never_split=None,normalize_text=True,mecab_dic:Optional[str]='ipadic',mecab_option:Optional[str]=None)
transformers.tokenization_bert_japanese.MecabTokenizer.__init__(self,do_lower_case=False,never_split=None,normalize_text=True,mecab_dic:Optional[str]='ipadic',mecab_option:Optional[str]=None)
transformers.tokenization_bert_japanese.MecabTokenizer.tokenize(self,text,never_split=None,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/modeling_tf_openai.py----------------------------------------
A:transformers.modeling_tf_openai.logger->utils.logging.get_logger(__name__)
A:transformers.modeling_tf_openai.self.c_attn->TFConv1D(n_state * 3, nx, initializer_range=config.initializer_range, name='c_attn')
A:transformers.modeling_tf_openai.self.c_proj->TFConv1D(nx, n_state, initializer_range=config.initializer_range, name='c_proj')
A:transformers.modeling_tf_openai.self.attn_dropout->tensorflow.keras.layers.Dropout(config.attn_pdrop)
A:transformers.modeling_tf_openai.self.resid_dropout->tensorflow.keras.layers.Dropout(config.resid_pdrop)
A:transformers.modeling_tf_openai.self.pruned_heads->set()
A:transformers.modeling_tf_openai.j->tensorflow.range(ns)
A:transformers.modeling_tf_openai.w->self.attn_dropout(w, training=training)
A:transformers.modeling_tf_openai.dk->tensorflow.cast(shape_list(k)[-1], tf.float32)
A:transformers.modeling_tf_openai.(_, _, nd, ns)->shape_list(w)
A:transformers.modeling_tf_openai.b->tensorflow.reshape(b, [1, 1, nd, ns])
A:transformers.modeling_tf_openai.x->self.c_attn(x)
A:transformers.modeling_tf_openai.x_shape->shape_list(x)
A:transformers.modeling_tf_openai.(query, key, value)->tensorflow.split(x, 3, axis=2)
A:transformers.modeling_tf_openai.query->self.split_heads(query)
A:transformers.modeling_tf_openai.key->self.split_heads(key)
A:transformers.modeling_tf_openai.value->self.split_heads(value)
A:transformers.modeling_tf_openai.attn_outputs->self._attn(query, key, value, attention_mask, head_mask, output_attentions, training=training)
A:transformers.modeling_tf_openai.a->self.resid_dropout(a, training=training)
A:transformers.modeling_tf_openai.self.c_fc->TFConv1D(n_state, nx, initializer_range=config.initializer_range, name='c_fc')
A:transformers.modeling_tf_openai.self.dropout->tensorflow.keras.layers.Dropout(config.resid_pdrop)
A:transformers.modeling_tf_openai.h->self.ln_2(n + m)
A:transformers.modeling_tf_openai.h2->self.dropout(h2, training=training)
A:transformers.modeling_tf_openai.self.attn->TFAttention(nx, n_ctx, config, scale, name='attn')
A:transformers.modeling_tf_openai.self.ln_1->tensorflow.keras.layers.LayerNormalization(epsilon=config.layer_norm_epsilon, name='ln_1')
A:transformers.modeling_tf_openai.self.mlp->TFMLP(4 * nx, config, name='mlp')
A:transformers.modeling_tf_openai.self.ln_2->tensorflow.keras.layers.LayerNormalization(epsilon=config.layer_norm_epsilon, name='ln_2')
A:transformers.modeling_tf_openai.output_attn->self.attn(x, attention_mask, head_mask, output_attentions, training=training)
A:transformers.modeling_tf_openai.n->self.ln_1(x + a)
A:transformers.modeling_tf_openai.m->self.mlp(n, training=training)
A:transformers.modeling_tf_openai.self.tokens_embed->TFSharedEmbeddings(config.vocab_size, config.n_embd, initializer_range=config.initializer_range, name='tokens_embed')
A:transformers.modeling_tf_openai.self.positions_embed->tensorflow.keras.layers.Embedding(config.n_positions, config.n_embd, embeddings_initializer=get_initializer(config.initializer_range), name='positions_embed')
A:transformers.modeling_tf_openai.self.drop->tensorflow.keras.layers.Dropout(config.embd_pdrop)
A:transformers.modeling_tf_openai.input_ids->inputs.get('input_ids')
A:transformers.modeling_tf_openai.attention_mask->inputs.get('attention_mask', attention_mask)
A:transformers.modeling_tf_openai.token_type_ids->inputs.get('token_type_ids', token_type_ids)
A:transformers.modeling_tf_openai.position_ids->inputs.get('position_ids', position_ids)
A:transformers.modeling_tf_openai.head_mask->inputs.get('head_mask', head_mask)
A:transformers.modeling_tf_openai.inputs_embeds->inputs.get('inputs_embeds', inputs_embeds)
A:transformers.modeling_tf_openai.output_attentions->inputs.get('output_attentions', output_attentions)
A:transformers.modeling_tf_openai.output_hidden_states->inputs.get('output_hidden_states', output_hidden_states)
A:transformers.modeling_tf_openai.return_dict->inputs.get('return_dict', return_dict)
A:transformers.modeling_tf_openai.input_shape->shape_list(input_ids)
A:transformers.modeling_tf_openai.position_embeds->self.positions_embed(position_ids)
A:transformers.modeling_tf_openai.token_type_embeds->self.tokens_embed(token_type_ids, mode='embedding')
A:transformers.modeling_tf_openai.hidden_states->tensorflow.reshape(hidden_states, input_shapes + shape_list(hidden_states)[-1:])
A:transformers.modeling_tf_openai.outputs->self.transformer(inputs, **kwargs)
A:transformers.modeling_tf_openai.all_attentions->tuple((tf.reshape(t, attention_output_shape) for t in all_attentions))
A:transformers.modeling_tf_openai.self.transformer->TFOpenAIGPTMainLayer(config, name='transformer')
A:transformers.modeling_tf_openai.labels->inputs.pop('labels', labels)
A:transformers.modeling_tf_openai.transformer_outputs->self.transformer(flat_input_ids, flat_attention_mask, flat_token_type_ids, flat_position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict=return_dict, training=training)
A:transformers.modeling_tf_openai.logits->self.transformer.tokens_embed(hidden_states, mode='linear')
A:transformers.modeling_tf_openai.loss->self.compute_loss(labels, logits)
A:transformers.modeling_tf_openai.self.multiple_choice_head->TFSequenceSummary(config, initializer_range=config.initializer_range, name='multiple_choice_head')
A:transformers.modeling_tf_openai.mc_token_ids->inputs.get('mc_token_ids', mc_token_ids)
A:transformers.modeling_tf_openai.input_shapes->shape_list(input_ids)
A:transformers.modeling_tf_openai.lm_logits->self.transformer.tokens_embed(hidden_states, mode='linear')
A:transformers.modeling_tf_openai.mc_logits->tensorflow.squeeze(mc_logits, axis=-1)
transformers.TFOpenAIGPTDoubleHeadsModel(self,config,*inputs,**kwargs)
transformers.TFOpenAIGPTDoubleHeadsModel.call(self,inputs,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,mc_token_ids=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False)
transformers.TFOpenAIGPTDoubleHeadsModel.get_output_embeddings(self)
transformers.TFOpenAIGPTDoubleHeadsModelOutput(ModelOutput)
transformers.TFOpenAIGPTLMHeadModel(self,config,*inputs,**kwargs)
transformers.TFOpenAIGPTLMHeadModel.call(self,inputs,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.TFOpenAIGPTLMHeadModel.get_output_embeddings(self)
transformers.TFOpenAIGPTMainLayer(self,config,*inputs,**kwargs)
transformers.TFOpenAIGPTMainLayer._prune_heads(self,heads_to_prune)
transformers.TFOpenAIGPTMainLayer.call(self,inputs,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False)
transformers.TFOpenAIGPTMainLayer.get_input_embeddings(self)
transformers.TFOpenAIGPTMainLayer.set_input_embeddings(self,value)
transformers.TFOpenAIGPTModel(self,config,*inputs,**kwargs)
transformers.TFOpenAIGPTModel.call(self,inputs,**kwargs)
transformers.TFOpenAIGPTPreTrainedModel(TFPreTrainedModel)
transformers.modeling_tf_openai.TFAttention(self,nx,n_ctx,config,scale=False,**kwargs)
transformers.modeling_tf_openai.TFAttention.__init__(self,nx,n_ctx,config,scale=False,**kwargs)
transformers.modeling_tf_openai.TFAttention._attn(self,q,k,v,attention_mask,head_mask,output_attentions,training=False)
transformers.modeling_tf_openai.TFAttention.call(self,x,attention_mask,head_mask,output_attentions,training=False)
transformers.modeling_tf_openai.TFAttention.causal_attention_mask(nd,ns,dtype)
transformers.modeling_tf_openai.TFAttention.merge_heads(self,x)
transformers.modeling_tf_openai.TFAttention.prune_heads(self,heads)
transformers.modeling_tf_openai.TFAttention.split_heads(self,x)
transformers.modeling_tf_openai.TFBlock(self,n_ctx,config,scale=False,**kwargs)
transformers.modeling_tf_openai.TFBlock.__init__(self,n_ctx,config,scale=False,**kwargs)
transformers.modeling_tf_openai.TFBlock.call(self,x,attention_mask,head_mask,output_attentions,training=False)
transformers.modeling_tf_openai.TFMLP(self,n_state,config,**kwargs)
transformers.modeling_tf_openai.TFMLP.__init__(self,n_state,config,**kwargs)
transformers.modeling_tf_openai.TFMLP.call(self,x,training=False)
transformers.modeling_tf_openai.TFOpenAIGPTDoubleHeadsModel(self,config,*inputs,**kwargs)
transformers.modeling_tf_openai.TFOpenAIGPTDoubleHeadsModel.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_openai.TFOpenAIGPTDoubleHeadsModel.call(self,inputs,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,mc_token_ids=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False)
transformers.modeling_tf_openai.TFOpenAIGPTDoubleHeadsModel.get_output_embeddings(self)
transformers.modeling_tf_openai.TFOpenAIGPTDoubleHeadsModelOutput(ModelOutput)
transformers.modeling_tf_openai.TFOpenAIGPTLMHeadModel(self,config,*inputs,**kwargs)
transformers.modeling_tf_openai.TFOpenAIGPTLMHeadModel.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_openai.TFOpenAIGPTLMHeadModel.call(self,inputs,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.modeling_tf_openai.TFOpenAIGPTLMHeadModel.get_output_embeddings(self)
transformers.modeling_tf_openai.TFOpenAIGPTMainLayer(self,config,*inputs,**kwargs)
transformers.modeling_tf_openai.TFOpenAIGPTMainLayer.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_openai.TFOpenAIGPTMainLayer._prune_heads(self,heads_to_prune)
transformers.modeling_tf_openai.TFOpenAIGPTMainLayer.call(self,inputs,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False)
transformers.modeling_tf_openai.TFOpenAIGPTMainLayer.get_input_embeddings(self)
transformers.modeling_tf_openai.TFOpenAIGPTMainLayer.set_input_embeddings(self,value)
transformers.modeling_tf_openai.TFOpenAIGPTModel(self,config,*inputs,**kwargs)
transformers.modeling_tf_openai.TFOpenAIGPTModel.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_openai.TFOpenAIGPTModel.call(self,inputs,**kwargs)
transformers.modeling_tf_openai.TFOpenAIGPTPreTrainedModel(TFPreTrainedModel)
transformers.modeling_tf_openai.gelu(x)
transformers.modeling_tf_openai.swish(x)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/testing_utils.py----------------------------------------
A:transformers.testing_utils._value->int(value)
A:transformers.testing_utils._run_slow_tests->parse_flag_from_env('RUN_SLOW', default=False)
A:transformers.testing_utils._run_custom_tokenizers->parse_flag_from_env('RUN_CUSTOM_TOKENIZERS', default=False)
A:transformers.testing_utils._tf_gpu_memory_limit->parse_int_from_env('TF_GPU_MEMORY_LIMIT', default=None)
A:transformers.testing_utils.test_case->unittest.skip('test requires TensorFlow')(test_case)
A:transformers.testing_utils.out_pr->apply_print_resets(out).lower()
A:transformers.testing_utils.match_str->apply_print_resets(out).lower().find(what.lower())
A:transformers.testing_utils.self.out_buf->StringIO()
A:transformers.testing_utils.self.err_buf->StringIO()
A:transformers.testing_utils.self.out->apply_print_resets(self.out_buf.getvalue())
A:transformers.testing_utils.self.err->self.err_buf.getvalue()
A:transformers.testing_utils.path->Path(tmp_dir).resolve()
A:transformers.testing_utils.tmp_dir->tempfile.mkdtemp()
transformers.testing_utils.CaptureStd(self,out=True,err=True)
transformers.testing_utils.CaptureStd.__enter__(self)
transformers.testing_utils.CaptureStd.__exit__(self,*exc)
transformers.testing_utils.CaptureStd.__init__(self,out=True,err=True)
transformers.testing_utils.CaptureStd.__repr__(self)
transformers.testing_utils.CaptureStderr(self)
transformers.testing_utils.CaptureStderr.__init__(self)
transformers.testing_utils.CaptureStdout(self)
transformers.testing_utils.CaptureStdout.__init__(self)
transformers.testing_utils.TestCasePlus(unittest.TestCase)
transformers.testing_utils.TestCasePlus.get_auto_remove_tmp_dir(self,tmp_dir=None,after=True,before=False)
transformers.testing_utils.TestCasePlus.setUp(self)
transformers.testing_utils.TestCasePlus.tearDown(self)
transformers.testing_utils.apply_print_resets(buf)
transformers.testing_utils.assert_screenout(out,what)
transformers.testing_utils.custom_tokenizers(test_case)
transformers.testing_utils.get_tests_dir()
transformers.testing_utils.parse_flag_from_env(key,default=False)
transformers.testing_utils.parse_int_from_env(key,default=None)
transformers.testing_utils.require_multigpu(test_case)
transformers.testing_utils.require_tf(test_case)
transformers.testing_utils.require_torch(test_case)
transformers.testing_utils.require_torch_and_cuda(test_case)
transformers.testing_utils.require_torch_tpu(test_case)
transformers.testing_utils.slow(test_case)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/configuration_retribert.py----------------------------------------
A:transformers.configuration_retribert.logger->utils.logging.get_logger(__name__)
transformers.RetriBertConfig(self,vocab_size=30522,hidden_size=768,num_hidden_layers=8,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=512,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,share_encoders=True,projection_dim=128,pad_token_id=0,**kwargs)
transformers.configuration_retribert.RetriBertConfig(self,vocab_size=30522,hidden_size=768,num_hidden_layers=8,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=512,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,share_encoders=True,projection_dim=128,pad_token_id=0,**kwargs)
transformers.configuration_retribert.RetriBertConfig.__init__(self,vocab_size=30522,hidden_size=768,num_hidden_layers=8,num_attention_heads=12,intermediate_size=3072,hidden_act='gelu',hidden_dropout_prob=0.1,attention_probs_dropout_prob=0.1,max_position_embeddings=512,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,share_encoders=True,projection_dim=128,pad_token_id=0,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/modeling_distilbert.py----------------------------------------
A:transformers.modeling_distilbert.logger->utils.logging.get_logger(__name__)
A:transformers.modeling_distilbert.position_enc->numpy.array([[pos / np.power(10000, 2 * (j // 2) / dim) for j in range(dim)] for pos in range(n_pos)])
A:transformers.modeling_distilbert.out[:, 0::2]->torch.FloatTensor(np.sin(position_enc[:, 0::2]))
A:transformers.modeling_distilbert.out[:, 1::2]->torch.FloatTensor(np.cos(position_enc[:, 1::2]))
A:transformers.modeling_distilbert.self.word_embeddings->torch.nn.Embedding(config.vocab_size, config.dim, padding_idx=config.pad_token_id)
A:transformers.modeling_distilbert.self.position_embeddings->torch.nn.Embedding(config.max_position_embeddings, config.dim)
A:transformers.modeling_distilbert.self.LayerNorm->torch.nn.LayerNorm(config.dim, eps=1e-12)
A:transformers.modeling_distilbert.self.dropout->torch.nn.Dropout(config.seq_classif_dropout)
A:transformers.modeling_distilbert.seq_length->input_ids.size(1)
A:transformers.modeling_distilbert.position_ids->position_ids.unsqueeze(0).expand_as(input_ids).unsqueeze(0).expand_as(input_ids)
A:transformers.modeling_distilbert.word_embeddings->self.word_embeddings(input_ids)
A:transformers.modeling_distilbert.position_embeddings->self.position_embeddings(position_ids)
A:transformers.modeling_distilbert.embeddings->self.dropout(embeddings)
A:transformers.modeling_distilbert.self.q_lin->prune_linear_layer(self.q_lin, index)
A:transformers.modeling_distilbert.self.k_lin->prune_linear_layer(self.k_lin, index)
A:transformers.modeling_distilbert.self.v_lin->prune_linear_layer(self.v_lin, index)
A:transformers.modeling_distilbert.self.out_lin->prune_linear_layer(self.out_lin, index, dim=1)
A:transformers.modeling_distilbert.self.pruned_heads->self.pruned_heads.union(heads)
A:transformers.modeling_distilbert.(heads, index)->find_pruneable_heads_and_indices(heads, self.n_heads, attention_head_size, self.pruned_heads)
A:transformers.modeling_distilbert.(bs, q_length, dim)->query.size()
A:transformers.modeling_distilbert.k_length->key.size(1)
A:transformers.modeling_distilbert.q->shape(self.q_lin(query))
A:transformers.modeling_distilbert.k->shape(self.k_lin(key))
A:transformers.modeling_distilbert.v->shape(self.v_lin(value))
A:transformers.modeling_distilbert.scores->torch.matmul(q, k.transpose(2, 3))
A:transformers.modeling_distilbert.mask->(mask == 0).view(mask_reshp).expand_as(scores)
A:transformers.modeling_distilbert.weights->self.dropout(weights)
A:transformers.modeling_distilbert.context->self.out_lin(context)
A:transformers.modeling_distilbert.self.lin1->torch.nn.Linear(in_features=config.dim, out_features=config.hidden_dim)
A:transformers.modeling_distilbert.self.lin2->torch.nn.Linear(in_features=config.hidden_dim, out_features=config.dim)
A:transformers.modeling_distilbert.x->self.dropout(x)
A:transformers.modeling_distilbert.self.attention->MultiHeadSelfAttention(config)
A:transformers.modeling_distilbert.self.sa_layer_norm->torch.nn.LayerNorm(normalized_shape=config.dim, eps=1e-12)
A:transformers.modeling_distilbert.self.ffn->FFN(config)
A:transformers.modeling_distilbert.self.output_layer_norm->torch.nn.LayerNorm(normalized_shape=config.dim, eps=1e-12)
A:transformers.modeling_distilbert.sa_output->self.sa_layer_norm(sa_output + x)
A:transformers.modeling_distilbert.ffn_output->self.output_layer_norm(ffn_output + sa_output)
A:transformers.modeling_distilbert.layer->TransformerBlock(config)
A:transformers.modeling_distilbert.self.layer->torch.nn.ModuleList([copy.deepcopy(layer) for _ in range(config.n_layers)])
A:transformers.modeling_distilbert.layer_outputs->layer_module(x=hidden_state, attn_mask=attn_mask, head_mask=head_mask[i], output_attentions=output_attentions)
A:transformers.modeling_distilbert.self.embeddings->Embeddings(config)
A:transformers.modeling_distilbert.self.transformer->Transformer(config)
A:transformers.modeling_distilbert.input_shape->input_ids.size()
A:transformers.modeling_distilbert.attention_mask->torch.ones(input_shape, device=device)
A:transformers.modeling_distilbert.head_mask->self.get_head_mask(head_mask, self.config.num_hidden_layers)
A:transformers.modeling_distilbert.inputs_embeds->self.embeddings(input_ids)
A:transformers.modeling_distilbert.self.distilbert->DistilBertModel(config)
A:transformers.modeling_distilbert.self.vocab_transform->torch.nn.Linear(config.dim, config.dim)
A:transformers.modeling_distilbert.self.vocab_layer_norm->torch.nn.LayerNorm(config.dim, eps=1e-12)
A:transformers.modeling_distilbert.self.vocab_projector->torch.nn.Linear(config.dim, config.vocab_size)
A:transformers.modeling_distilbert.self.mlm_loss_fct->torch.nn.CrossEntropyLoss()
A:transformers.modeling_distilbert.labels->kwargs.pop('masked_lm_labels')
A:transformers.modeling_distilbert.dlbrt_output->self.distilbert(input_ids=input_ids, attention_mask=attention_mask, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.modeling_distilbert.prediction_logits->self.vocab_projector(prediction_logits)
A:transformers.modeling_distilbert.mlm_loss->self.mlm_loss_fct(prediction_logits.view(-1, prediction_logits.size(-1)), labels.view(-1))
A:transformers.modeling_distilbert.self.pre_classifier->torch.nn.Linear(config.dim, config.dim)
A:transformers.modeling_distilbert.self.classifier->torch.nn.Linear(config.dim, 1)
A:transformers.modeling_distilbert.distilbert_output->self.distilbert(input_ids=input_ids, attention_mask=attention_mask, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.modeling_distilbert.pooled_output->self.dropout(pooled_output)
A:transformers.modeling_distilbert.logits->self.classifier(pooled_output)
A:transformers.modeling_distilbert.loss_fct->CrossEntropyLoss()
A:transformers.modeling_distilbert.loss->loss_fct(reshaped_logits, labels)
A:transformers.modeling_distilbert.self.qa_outputs->torch.nn.Linear(config.dim, config.num_labels)
A:transformers.modeling_distilbert.hidden_states->self.dropout(hidden_states)
A:transformers.modeling_distilbert.(start_logits, end_logits)->self.classifier(pooled_output).split(1, dim=-1)
A:transformers.modeling_distilbert.start_logits->start_logits.squeeze(-1).squeeze(-1)
A:transformers.modeling_distilbert.end_logits->end_logits.squeeze(-1).squeeze(-1)
A:transformers.modeling_distilbert.start_positions->start_positions.squeeze(-1).squeeze(-1)
A:transformers.modeling_distilbert.end_positions->end_positions.squeeze(-1).squeeze(-1)
A:transformers.modeling_distilbert.ignored_index->start_logits.squeeze(-1).squeeze(-1).size(1)
A:transformers.modeling_distilbert.start_loss->loss_fct(start_logits, start_positions)
A:transformers.modeling_distilbert.end_loss->loss_fct(end_logits, end_positions)
A:transformers.modeling_distilbert.outputs->self.distilbert(input_ids, attention_mask=attention_mask, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.modeling_distilbert.sequence_output->self.dropout(sequence_output)
A:transformers.modeling_distilbert.active_logits->self.classifier(pooled_output).view(-1, self.num_labels)
A:transformers.modeling_distilbert.active_labels->torch.where(active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels))
A:transformers.modeling_distilbert.reshaped_logits->self.classifier(pooled_output).view(-1, num_choices)
transformers.DistilBertForMaskedLM(self,config)
transformers.DistilBertForMaskedLM.forward(self,input_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.DistilBertForMaskedLM.get_output_embeddings(self)
transformers.DistilBertForMultipleChoice(self,config)
transformers.DistilBertForMultipleChoice.forward(self,input_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.DistilBertForQuestionAnswering(self,config)
transformers.DistilBertForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,start_positions=None,end_positions=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.DistilBertForSequenceClassification(self,config)
transformers.DistilBertForSequenceClassification.forward(self,input_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.DistilBertForTokenClassification(self,config)
transformers.DistilBertForTokenClassification.forward(self,input_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.DistilBertModel(self,config)
transformers.DistilBertModel._prune_heads(self,heads_to_prune)
transformers.DistilBertModel.forward(self,input_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.DistilBertModel.get_input_embeddings(self)
transformers.DistilBertModel.set_input_embeddings(self,new_embeddings)
transformers.DistilBertPreTrainedModel(PreTrainedModel)
transformers.DistilBertPreTrainedModel._init_weights(self,module)
transformers.modeling_distilbert.DistilBertForMaskedLM(self,config)
transformers.modeling_distilbert.DistilBertForMaskedLM.__init__(self,config)
transformers.modeling_distilbert.DistilBertForMaskedLM.forward(self,input_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.modeling_distilbert.DistilBertForMaskedLM.get_output_embeddings(self)
transformers.modeling_distilbert.DistilBertForMultipleChoice(self,config)
transformers.modeling_distilbert.DistilBertForMultipleChoice.__init__(self,config)
transformers.modeling_distilbert.DistilBertForMultipleChoice.forward(self,input_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.modeling_distilbert.DistilBertForQuestionAnswering(self,config)
transformers.modeling_distilbert.DistilBertForQuestionAnswering.__init__(self,config)
transformers.modeling_distilbert.DistilBertForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,start_positions=None,end_positions=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.modeling_distilbert.DistilBertForSequenceClassification(self,config)
transformers.modeling_distilbert.DistilBertForSequenceClassification.__init__(self,config)
transformers.modeling_distilbert.DistilBertForSequenceClassification.forward(self,input_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.modeling_distilbert.DistilBertForTokenClassification(self,config)
transformers.modeling_distilbert.DistilBertForTokenClassification.__init__(self,config)
transformers.modeling_distilbert.DistilBertForTokenClassification.forward(self,input_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.modeling_distilbert.DistilBertModel(self,config)
transformers.modeling_distilbert.DistilBertModel.__init__(self,config)
transformers.modeling_distilbert.DistilBertModel._prune_heads(self,heads_to_prune)
transformers.modeling_distilbert.DistilBertModel.forward(self,input_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.modeling_distilbert.DistilBertModel.get_input_embeddings(self)
transformers.modeling_distilbert.DistilBertModel.set_input_embeddings(self,new_embeddings)
transformers.modeling_distilbert.DistilBertPreTrainedModel(PreTrainedModel)
transformers.modeling_distilbert.DistilBertPreTrainedModel._init_weights(self,module)
transformers.modeling_distilbert.Embeddings(self,config)
transformers.modeling_distilbert.Embeddings.__init__(self,config)
transformers.modeling_distilbert.Embeddings.forward(self,input_ids)
transformers.modeling_distilbert.FFN(self,config)
transformers.modeling_distilbert.FFN.__init__(self,config)
transformers.modeling_distilbert.FFN.ff_chunk(self,input)
transformers.modeling_distilbert.FFN.forward(self,input)
transformers.modeling_distilbert.MultiHeadSelfAttention(self,config)
transformers.modeling_distilbert.MultiHeadSelfAttention.__init__(self,config)
transformers.modeling_distilbert.MultiHeadSelfAttention.forward(self,query,key,value,mask,head_mask=None,output_attentions=False)
transformers.modeling_distilbert.MultiHeadSelfAttention.prune_heads(self,heads)
transformers.modeling_distilbert.Transformer(self,config)
transformers.modeling_distilbert.Transformer.__init__(self,config)
transformers.modeling_distilbert.Transformer.forward(self,x,attn_mask=None,head_mask=None,output_attentions=False,output_hidden_states=False,return_dict=None)
transformers.modeling_distilbert.TransformerBlock(self,config)
transformers.modeling_distilbert.TransformerBlock.__init__(self,config)
transformers.modeling_distilbert.TransformerBlock.forward(self,x,attn_mask=None,head_mask=None,output_attentions=False)
transformers.modeling_distilbert.create_sinusoidal_embeddings(n_pos,dim,out)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/tokenization_bart.py----------------------------------------
A:transformers.tokenization_bart.logger->utils.logging.get_logger(__name__)
transformers.BartTokenizer(RobertaTokenizer)
transformers.BartTokenizer.prepare_seq2seq_batch(self,src_texts:List[str],tgt_texts:Optional[List[str]]=None,max_length:Optional[int]=None,max_target_length:Optional[int]=None,padding:str='longest',return_tensors:str='None',truncation=True,**kwargs)->BatchEncoding
transformers.BartTokenizerFast(RobertaTokenizerFast)
transformers.BartTokenizerFast.prepare_seq2seq_batch(self,src_texts:List[str],tgt_texts:Optional[List[str]]=None,max_length:Optional[int]=None,max_target_length:Optional[int]=None,padding:str='longest',return_tensors:str='None',truncation=True,**kwargs)->BatchEncoding
transformers.tokenization_bart.BartTokenizer(RobertaTokenizer)
transformers.tokenization_bart.BartTokenizer.prepare_seq2seq_batch(self,src_texts:List[str],tgt_texts:Optional[List[str]]=None,max_length:Optional[int]=None,max_target_length:Optional[int]=None,padding:str='longest',return_tensors:str='None',truncation=True,**kwargs)->BatchEncoding
transformers.tokenization_bart.BartTokenizerFast(RobertaTokenizerFast)
transformers.tokenization_bart.BartTokenizerFast.prepare_seq2seq_batch(self,src_texts:List[str],tgt_texts:Optional[List[str]]=None,max_length:Optional[int]=None,max_target_length:Optional[int]=None,padding:str='longest',return_tensors:str='None',truncation=True,**kwargs)->BatchEncoding


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/configuration_dpr.py----------------------------------------
A:transformers.configuration_dpr.logger->utils.logging.get_logger(__name__)
transformers.DPRConfig(self,projection_dim:int=0,**kwargs)
transformers.configuration_dpr.DPRConfig(self,projection_dim:int=0,**kwargs)
transformers.configuration_dpr.DPRConfig.__init__(self,projection_dim:int=0,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/configuration_utils.py----------------------------------------
A:transformers.configuration_utils.logger->utils.logging.get_logger(__name__)
A:transformers.configuration_utils.self.return_dict->kwargs.pop('return_dict', False)
A:transformers.configuration_utils.self.output_hidden_states->kwargs.pop('output_hidden_states', False)
A:transformers.configuration_utils.self.output_attentions->kwargs.pop('output_attentions', False)
A:transformers.configuration_utils.self.use_cache->kwargs.pop('use_cache', True)
A:transformers.configuration_utils.self.torchscript->kwargs.pop('torchscript', False)
A:transformers.configuration_utils.self.use_bfloat16->kwargs.pop('use_bfloat16', False)
A:transformers.configuration_utils.self.pruned_heads->kwargs.pop('pruned_heads', {})
A:transformers.configuration_utils.self.tie_word_embeddings->kwargs.pop('tie_word_embeddings', True)
A:transformers.configuration_utils.self.is_encoder_decoder->kwargs.pop('is_encoder_decoder', False)
A:transformers.configuration_utils.self.is_decoder->kwargs.pop('is_decoder', False)
A:transformers.configuration_utils.self.add_cross_attention->kwargs.pop('add_cross_attention', False)
A:transformers.configuration_utils.self.tie_encoder_decoder->kwargs.pop('tie_encoder_decoder', False)
A:transformers.configuration_utils.self.max_length->kwargs.pop('max_length', 20)
A:transformers.configuration_utils.self.min_length->kwargs.pop('min_length', 0)
A:transformers.configuration_utils.self.do_sample->kwargs.pop('do_sample', False)
A:transformers.configuration_utils.self.early_stopping->kwargs.pop('early_stopping', False)
A:transformers.configuration_utils.self.num_beams->kwargs.pop('num_beams', 1)
A:transformers.configuration_utils.self.temperature->kwargs.pop('temperature', 1.0)
A:transformers.configuration_utils.self.top_k->kwargs.pop('top_k', 50)
A:transformers.configuration_utils.self.top_p->kwargs.pop('top_p', 1.0)
A:transformers.configuration_utils.self.repetition_penalty->kwargs.pop('repetition_penalty', 1.0)
A:transformers.configuration_utils.self.length_penalty->kwargs.pop('length_penalty', 1.0)
A:transformers.configuration_utils.self.no_repeat_ngram_size->kwargs.pop('no_repeat_ngram_size', 0)
A:transformers.configuration_utils.self.bad_words_ids->kwargs.pop('bad_words_ids', None)
A:transformers.configuration_utils.self.num_return_sequences->kwargs.pop('num_return_sequences', 1)
A:transformers.configuration_utils.self.chunk_size_feed_forward->kwargs.pop('chunk_size_feed_forward', 0)
A:transformers.configuration_utils.self.architectures->kwargs.pop('architectures', None)
A:transformers.configuration_utils.self.finetuning_task->kwargs.pop('finetuning_task', None)
A:transformers.configuration_utils.self.id2label->dict(((int(key), value) for (key, value) in self.id2label.items()))
A:transformers.configuration_utils.self.label2id->dict(zip(self.id2label.values(), self.id2label.keys()))
A:transformers.configuration_utils.self.num_labels->kwargs.pop('num_labels', 2)
A:transformers.configuration_utils.self.prefix->kwargs.pop('prefix', None)
A:transformers.configuration_utils.self.bos_token_id->kwargs.pop('bos_token_id', None)
A:transformers.configuration_utils.self.pad_token_id->kwargs.pop('pad_token_id', None)
A:transformers.configuration_utils.self.eos_token_id->kwargs.pop('eos_token_id', None)
A:transformers.configuration_utils.self.decoder_start_token_id->kwargs.pop('decoder_start_token_id', None)
A:transformers.configuration_utils.self.task_specific_params->kwargs.pop('task_specific_params', None)
A:transformers.configuration_utils.self.xla_device->kwargs.pop('xla_device', None)
A:transformers.configuration_utils.output_config_file->os.path.join(save_directory, CONFIG_NAME)
A:transformers.configuration_utils.(config_dict, kwargs)->cls.get_config_dict(pretrained_model_name_or_path, **kwargs)
A:transformers.configuration_utils.cache_dir->kwargs.pop('cache_dir', None)
A:transformers.configuration_utils.force_download->kwargs.pop('force_download', False)
A:transformers.configuration_utils.resume_download->kwargs.pop('resume_download', False)
A:transformers.configuration_utils.proxies->kwargs.pop('proxies', None)
A:transformers.configuration_utils.local_files_only->kwargs.pop('local_files_only', False)
A:transformers.configuration_utils.config_file->hf_bucket_url(pretrained_model_name_or_path, filename=CONFIG_NAME, use_cdn=False)
A:transformers.configuration_utils.resolved_config_file->cached_path(config_file, cache_dir=cache_dir, force_download=force_download, proxies=proxies, resume_download=resume_download, local_files_only=local_files_only)
A:transformers.configuration_utils.config_dict->self.to_dict()
A:transformers.configuration_utils.msg->"Couldn't reach server at '{}' to download configuration file or configuration file is not a valid JSON file. Please check network or file content here: {}.".format(config_file, resolved_config_file)
A:transformers.configuration_utils.return_unused_kwargs->kwargs.pop('return_unused_kwargs', False)
A:transformers.configuration_utils.config->cls(**config_dict)
A:transformers.configuration_utils.config.pruned_heads->dict(((int(key), value) for (key, value) in config.pruned_heads.items()))
A:transformers.configuration_utils.text->reader.read()
A:transformers.configuration_utils.default_config_dict->PretrainedConfig().to_dict()
A:transformers.configuration_utils.output->copy.deepcopy(self.__dict__)
transformers.PretrainedConfig(self,**kwargs)
transformers.PretrainedConfig.__eq__(self,other)
transformers.PretrainedConfig.__repr__(self)
transformers.PretrainedConfig._dict_from_json_file(cls,json_file:str)
transformers.PretrainedConfig.from_dict(cls,config_dict:Dict[str,Any],**kwargs)->'PretrainedConfig'
transformers.PretrainedConfig.from_json_file(cls,json_file:str)->'PretrainedConfig'
transformers.PretrainedConfig.from_pretrained(cls,pretrained_model_name_or_path:str,**kwargs)->'PretrainedConfig'
transformers.PretrainedConfig.get_config_dict(cls,pretrained_model_name_or_path:str,**kwargs)->Tuple[Dict[str, Any], Dict[str, Any]]
transformers.PretrainedConfig.num_labels(self)->int
transformers.PretrainedConfig.num_labels(self,num_labels:int)
transformers.PretrainedConfig.save_pretrained(self,save_directory:str)
transformers.PretrainedConfig.to_dict(self)->Dict[str, Any]
transformers.PretrainedConfig.to_diff_dict(self)->Dict[str, Any]
transformers.PretrainedConfig.to_json_file(self,json_file_path:str,use_diff:bool=True)
transformers.PretrainedConfig.to_json_string(self,use_diff:bool=True)->str
transformers.PretrainedConfig.update(self,config_dict:Dict[str,Any])
transformers.PretrainedConfig.use_return_dict(self)->bool
transformers.configuration_utils.PretrainedConfig(self,**kwargs)
transformers.configuration_utils.PretrainedConfig.__eq__(self,other)
transformers.configuration_utils.PretrainedConfig.__init__(self,**kwargs)
transformers.configuration_utils.PretrainedConfig.__repr__(self)
transformers.configuration_utils.PretrainedConfig._dict_from_json_file(cls,json_file:str)
transformers.configuration_utils.PretrainedConfig.from_dict(cls,config_dict:Dict[str,Any],**kwargs)->'PretrainedConfig'
transformers.configuration_utils.PretrainedConfig.from_json_file(cls,json_file:str)->'PretrainedConfig'
transformers.configuration_utils.PretrainedConfig.from_pretrained(cls,pretrained_model_name_or_path:str,**kwargs)->'PretrainedConfig'
transformers.configuration_utils.PretrainedConfig.get_config_dict(cls,pretrained_model_name_or_path:str,**kwargs)->Tuple[Dict[str, Any], Dict[str, Any]]
transformers.configuration_utils.PretrainedConfig.num_labels(self)->int
transformers.configuration_utils.PretrainedConfig.num_labels(self,num_labels:int)
transformers.configuration_utils.PretrainedConfig.save_pretrained(self,save_directory:str)
transformers.configuration_utils.PretrainedConfig.to_dict(self)->Dict[str, Any]
transformers.configuration_utils.PretrainedConfig.to_diff_dict(self)->Dict[str, Any]
transformers.configuration_utils.PretrainedConfig.to_json_file(self,json_file_path:str,use_diff:bool=True)
transformers.configuration_utils.PretrainedConfig.to_json_string(self,use_diff:bool=True)->str
transformers.configuration_utils.PretrainedConfig.update(self,config_dict:Dict[str,Any])
transformers.configuration_utils.PretrainedConfig.use_return_dict(self)->bool


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/pipelines.py----------------------------------------
A:transformers.pipelines.logger->utils.logging.get_logger(__name__)
A:transformers.pipelines.output->'Conversation id: {} \n'.format(self.uuid)
A:transformers.pipelines.(path, _)->os.path.splitext(self.output_path)
A:transformers.pipelines.binary_path->os.path.extsep.join((path, 'pickle'))
A:transformers.pipelines.reader->csv.DictReader(f)
A:transformers.pipelines.writer->csv.DictWriter(f, list(data[0].keys()))
A:transformers.pipelines.self._entries->json.load(f)
A:transformers.pipelines.line->line.split('\t').split('\t')
A:transformers.pipelines.framework->get_framework(model)
A:transformers.pipelines.self.model->self.model.to(self.device)
A:transformers.pipelines.inputs->self.tokenizer.batch_encode_plus(inputs, add_special_tokens=False, padding=False).get('input_ids', [])
A:transformers.pipelines.predictions->self.model(**inputs)[0].cpu()
A:transformers.pipelines.text_inputs->self._args_parser(*args)
A:transformers.pipelines.padding->self._parse_and_tokenize(padding_text, padding=False, add_special_tokens=False)
A:transformers.pipelines.output_sequences->self.model.generate(input_ids=input_ids, **generate_kwargs)
A:transformers.pipelines.generated_sequence->generated_sequence.numpy().tolist().numpy().tolist()
A:transformers.pipelines.text->self.tokenizer.decode(generated_sequence, skip_special_tokens=True, clean_up_tokenization_spaces=clean_up_tokenization_spaces)
A:transformers.pipelines.prompt_length->len(self.tokenizer.decode(input_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=clean_up_tokenization_spaces))
A:transformers.pipelines.outputs->BatchEncoding({'input_ids': outputs, 'attention_mask': [[1] * len(outputs)]}, tensor_type=self.framework)
A:transformers.pipelines.labels->self._parse_labels(labels)
A:transformers.pipelines.candidate_labels->self._args_parser._parse_labels(candidate_labels)
A:transformers.pipelines.reshaped_outputs->BatchEncoding({'input_ids': outputs, 'attention_mask': [[1] * len(outputs)]}, tensor_type=self.framework).reshape((num_sequences, len(candidate_labels), -1))
A:transformers.pipelines.top_inds->list(reversed(scores[iseq].argsort()))
A:transformers.pipelines.numel->numpy.prod(masked_index.shape)
A:transformers.pipelines.target_enc->self.tokenizer.tokenize(target)
A:transformers.pipelines.target_inds->numpy.array(self.tokenizer.convert_tokens_to_ids(targets_proc))
A:transformers.pipelines.masked_index->torch.nonzero(input_ids == self.tokenizer.mask_token_id, as_tuple=False)
A:transformers.pipelines.probs->logits.softmax(dim=0)
A:transformers.pipelines.topk->tensorflow.math.top_k(probs, k=self.topk)
A:transformers.pipelines.values->tensorflow.gather_nd(values, tf.reshape(sort_inds, (-1, 1))).numpy()
A:transformers.pipelines.sort_inds->list(reversed(values.argsort(dim=-1)))
A:transformers.pipelines.(values, predictions)->logits.softmax(dim=0).topk(self.topk)
A:transformers.pipelines.tokens->self.ensure_tensor_on_device(**tokens)
A:transformers.pipelines.self._basic_tokenizer->BasicTokenizer(do_lower_case=False)
A:transformers.pipelines.entities->self.model(**tokens)[0][0].cpu().numpy()
A:transformers.pipelines.labels_idx->score.argmax(axis=-1)
A:transformers.pipelines.scores->numpy.mean([entity['score'] for entity in entities])
A:transformers.pipelines.kwargs['X']->list(args)
A:transformers.pipelines.inputs[i]->QuestionAnsweringPipeline.create_sample(**item)
A:transformers.pipelines.examples->self._args_parser(*args, **kwargs)
A:transformers.pipelines.start_->numpy.exp(start_ - np.log(np.sum(np.exp(start_), axis=-1, keepdims=True)))
A:transformers.pipelines.end_->numpy.exp(end_ - np.log(np.sum(np.exp(end_), axis=-1, keepdims=True)))
A:transformers.pipelines.min_null_score->min(min_null_score, (start_[0] * end_[0]).item())
A:transformers.pipelines.(starts, ends, scores)->self.decode(start_, end_, kwargs['topk'], kwargs['max_answer_len'])
A:transformers.pipelines.char_to_word->numpy.array(example.char_to_word_offset)
A:transformers.pipelines.outer->numpy.matmul(np.expand_dims(start, -1), np.expand_dims(end, 1))
A:transformers.pipelines.candidates->numpy.tril(np.triu(outer), max_answer_len - 1)
A:transformers.pipelines.scores_flat->numpy.tril(np.triu(outer), max_answer_len - 1).flatten()
A:transformers.pipelines.idx_sort->numpy.argsort(-scores_flat)
A:transformers.pipelines.token->self.tokenizer.tokenize(word)
A:transformers.pipelines.input_length->tensorflow.shape(inputs['input_ids'])[-1].numpy()
A:transformers.pipelines.min_length->generate_kwargs.get('min_length', self.model.config.min_length)
A:transformers.pipelines.max_length->generate_kwargs.get('max_length', self.model.config.max_length)
A:transformers.pipelines.summaries->self.model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], **generate_kwargs)
A:transformers.pipelines.record['summary_text']->self.tokenizer.decode(summary, skip_special_tokens=True, clean_up_tokenization_spaces=clean_up_tokenization_spaces)
A:transformers.pipelines.translations->self.model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], **generate_kwargs)
A:transformers.pipelines.record['translation_text']->self.tokenizer.decode(translation, skip_special_tokens=True, clean_up_tokenization_spaces=clean_up_tokenization_spaces)
A:transformers.pipelines.conversation_id->uuid.uuid4()
A:transformers.pipelines.generated_responses->self.model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], **generate_kwargs)
A:transformers.pipelines.cleaned_history->self._clean_padding_history(generated_responses)
A:transformers.pipelines.cutoff_eos_index->new_input[cutoff_eos_index:].index(self.tokenizer.eos_token_id)
A:transformers.pipelines.max_len->max([len(item) for item in outputs])
A:transformers.pipelines.tokenizer->tokenization_auto.AutoTokenizer.from_pretrained(tokenizer)
A:transformers.pipelines.config->configuration_auto.AutoConfig.from_pretrained(config)
A:transformers.pipelines.modelcard->modelcard.ModelCard.from_pretrained(modelcard)
A:transformers.pipelines.model->model_class.from_pretrained(model, config=config, **model_kwargs)
transformers.Conversation(self,text:str=None,conversation_id:UUID=None)
transformers.Conversation.__repr__(self)
transformers.Conversation.add_user_input(self,text:str,overwrite:bool=False)
transformers.Conversation.append_response(self,response:str)
transformers.Conversation.mark_processed(self)
transformers.Conversation.set_history(self,history:List[int])
transformers.ConversationalPipeline(self,min_length_for_response=32,*args,**kwargs)
transformers.ConversationalPipeline._clean_padding_history(self,generated_tensor)->List[List[int]]
transformers.ConversationalPipeline._concat_inputs_history(self,inputs:List[List[int]],histories:List[Optional[List[int]]],max_length:int)
transformers.ConversationalPipeline._parse_and_tokenize(self,*args,**kwargs)
transformers.CsvPipelineDataFormat(self,output_path:Optional[str],input_path:Optional[str],column:Optional[str],overwrite=False)
transformers.CsvPipelineDataFormat.__iter__(self)
transformers.CsvPipelineDataFormat.save(self,data:List[dict])
transformers.FeatureExtractionPipeline(self,model:Union['PreTrainedModel','TFPreTrainedModel'],tokenizer:PreTrainedTokenizer,modelcard:Optional[ModelCard]=None,framework:Optional[str]=None,args_parser:ArgumentHandler=None,device:int=-1,task:str='')
transformers.FillMaskPipeline(self,model:Union['PreTrainedModel','TFPreTrainedModel'],tokenizer:PreTrainedTokenizer,modelcard:Optional[ModelCard]=None,framework:Optional[str]=None,args_parser:ArgumentHandler=None,device:int=-1,topk=5,task:str='')
transformers.FillMaskPipeline.ensure_exactly_one_mask_token(self,masked_index:np.ndarray)
transformers.JsonPipelineDataFormat(self,output_path:Optional[str],input_path:Optional[str],column:Optional[str],overwrite=False)
transformers.JsonPipelineDataFormat.__iter__(self)
transformers.JsonPipelineDataFormat.save(self,data:dict)
transformers.PipedPipelineDataFormat(PipelineDataFormat)
transformers.PipedPipelineDataFormat.__iter__(self)
transformers.PipedPipelineDataFormat.save(self,data:dict)
transformers.PipedPipelineDataFormat.save_binary(self,data:Union[dict,List[dict]])->str
transformers.Pipeline(self,model:Union['PreTrainedModel','TFPreTrainedModel'],tokenizer:PreTrainedTokenizer,modelcard:Optional[ModelCard]=None,framework:Optional[str]=None,task:str='',args_parser:ArgumentHandler=None,device:int=-1,binary_output:bool=False)
transformers.Pipeline._forward(self,inputs,return_tensors=False)
transformers.Pipeline._parse_and_tokenize(self,*args,padding=True,add_special_tokens=True,**kwargs)
transformers.Pipeline.check_model_type(self,supported_models:Union[List[str],dict])
transformers.Pipeline.device_placement(self)
transformers.Pipeline.ensure_tensor_on_device(self,**inputs)
transformers.Pipeline.predict(self,X)
transformers.Pipeline.save_pretrained(self,save_directory:str)
transformers.Pipeline.transform(self,X)
transformers.PipelineDataFormat(self,output_path:Optional[str],input_path:Optional[str],column:Optional[str],overwrite:bool=False)
transformers.PipelineDataFormat.__iter__(self)
transformers.PipelineDataFormat.from_str(format:str,output_path:Optional[str],input_path:Optional[str],column:Optional[str],overwrite=False)->'PipelineDataFormat'
transformers.PipelineDataFormat.save(self,data:Union[dict,List[dict]])
transformers.PipelineDataFormat.save_binary(self,data:Union[dict,List[dict]])->str
transformers.PipelineException(self,task:str,model:str,reason:str)
transformers.QuestionAnsweringPipeline(self,model:Union['PreTrainedModel','TFPreTrainedModel'],tokenizer:PreTrainedTokenizer,modelcard:Optional[ModelCard]=None,framework:Optional[str]=None,device:int=-1,task:str='',**kwargs)
transformers.QuestionAnsweringPipeline.create_sample(question:Union[str,List[str]],context:Union[str,List[str]])->Union[SquadExample, List[SquadExample]]
transformers.QuestionAnsweringPipeline.decode(self,start:np.ndarray,end:np.ndarray,topk:int,max_answer_len:int)->Tuple
transformers.QuestionAnsweringPipeline.span_to_answer(self,text:str,start:int,end:int)->Dict[str, Union[str, int]]
transformers.SummarizationPipeline(self,*args,**kwargs)
transformers.TextClassificationPipeline(self,return_all_scores:bool=False,**kwargs)
transformers.TextGenerationPipeline(self,*args,**kwargs)
transformers.TextGenerationPipeline._parse_and_tokenize(self,*args,padding=True,add_special_tokens=True,**kwargs)
transformers.TokenClassificationPipeline(self,model:Union['PreTrainedModel','TFPreTrainedModel'],tokenizer:PreTrainedTokenizer,modelcard:Optional[ModelCard]=None,framework:Optional[str]=None,args_parser:ArgumentHandler=None,device:int=-1,binary_output:bool=False,ignore_labels=['O'],task:str='',grouped_entities:bool=False)
transformers.TokenClassificationPipeline.group_entities(self,entities:List[dict])->List[dict]
transformers.TokenClassificationPipeline.group_sub_entities(self,entities:List[dict])->dict
transformers.TranslationPipeline(self,*args,**kwargs)
transformers.ZeroShotClassificationPipeline(self,args_parser=ZeroShotClassificationArgumentHandler(),*args,**kwargs)
transformers.ZeroShotClassificationPipeline._parse_and_tokenize(self,*args,padding=True,add_special_tokens=True,**kwargs)
transformers.pipeline(task:str,model:Optional=None,config:Optional[Union[str,PretrainedConfig]]=None,tokenizer:Optional[Union[str,PreTrainedTokenizer]]=None,framework:Optional[str]=None,**kwargs)->Pipeline
transformers.pipelines.ArgumentHandler(self,*args,**kwargs)
transformers.pipelines.ArgumentHandler.__call__(self,*args,**kwargs)
transformers.pipelines.Conversation(self,text:str=None,conversation_id:UUID=None)
transformers.pipelines.Conversation.__init__(self,text:str=None,conversation_id:UUID=None)
transformers.pipelines.Conversation.__repr__(self)
transformers.pipelines.Conversation.add_user_input(self,text:str,overwrite:bool=False)
transformers.pipelines.Conversation.append_response(self,response:str)
transformers.pipelines.Conversation.mark_processed(self)
transformers.pipelines.Conversation.set_history(self,history:List[int])
transformers.pipelines.ConversationalPipeline(self,min_length_for_response=32,*args,**kwargs)
transformers.pipelines.ConversationalPipeline.__init__(self,min_length_for_response=32,*args,**kwargs)
transformers.pipelines.ConversationalPipeline._clean_padding_history(self,generated_tensor)->List[List[int]]
transformers.pipelines.ConversationalPipeline._concat_inputs_history(self,inputs:List[List[int]],histories:List[Optional[List[int]]],max_length:int)
transformers.pipelines.ConversationalPipeline._parse_and_tokenize(self,*args,**kwargs)
transformers.pipelines.CsvPipelineDataFormat(self,output_path:Optional[str],input_path:Optional[str],column:Optional[str],overwrite=False)
transformers.pipelines.CsvPipelineDataFormat.__init__(self,output_path:Optional[str],input_path:Optional[str],column:Optional[str],overwrite=False)
transformers.pipelines.CsvPipelineDataFormat.__iter__(self)
transformers.pipelines.CsvPipelineDataFormat.save(self,data:List[dict])
transformers.pipelines.DefaultArgumentHandler(self,*args,**kwargs)
transformers.pipelines.DefaultArgumentHandler.__call__(self,*args,**kwargs)
transformers.pipelines.DefaultArgumentHandler.handle_args(args:Sequence[Any])->List[str]
transformers.pipelines.DefaultArgumentHandler.handle_kwargs(kwargs:Dict)->List
transformers.pipelines.FeatureExtractionPipeline(self,model:Union['PreTrainedModel','TFPreTrainedModel'],tokenizer:PreTrainedTokenizer,modelcard:Optional[ModelCard]=None,framework:Optional[str]=None,args_parser:ArgumentHandler=None,device:int=-1,task:str='')
transformers.pipelines.FeatureExtractionPipeline.__init__(self,model:Union['PreTrainedModel','TFPreTrainedModel'],tokenizer:PreTrainedTokenizer,modelcard:Optional[ModelCard]=None,framework:Optional[str]=None,args_parser:ArgumentHandler=None,device:int=-1,task:str='')
transformers.pipelines.FillMaskPipeline(self,model:Union['PreTrainedModel','TFPreTrainedModel'],tokenizer:PreTrainedTokenizer,modelcard:Optional[ModelCard]=None,framework:Optional[str]=None,args_parser:ArgumentHandler=None,device:int=-1,topk=5,task:str='')
transformers.pipelines.FillMaskPipeline.__init__(self,model:Union['PreTrainedModel','TFPreTrainedModel'],tokenizer:PreTrainedTokenizer,modelcard:Optional[ModelCard]=None,framework:Optional[str]=None,args_parser:ArgumentHandler=None,device:int=-1,topk=5,task:str='')
transformers.pipelines.FillMaskPipeline.ensure_exactly_one_mask_token(self,masked_index:np.ndarray)
transformers.pipelines.JsonPipelineDataFormat(self,output_path:Optional[str],input_path:Optional[str],column:Optional[str],overwrite=False)
transformers.pipelines.JsonPipelineDataFormat.__init__(self,output_path:Optional[str],input_path:Optional[str],column:Optional[str],overwrite=False)
transformers.pipelines.JsonPipelineDataFormat.__iter__(self)
transformers.pipelines.JsonPipelineDataFormat.save(self,data:dict)
transformers.pipelines.PipedPipelineDataFormat(PipelineDataFormat)
transformers.pipelines.PipedPipelineDataFormat.__iter__(self)
transformers.pipelines.PipedPipelineDataFormat.save(self,data:dict)
transformers.pipelines.PipedPipelineDataFormat.save_binary(self,data:Union[dict,List[dict]])->str
transformers.pipelines.Pipeline(self,model:Union['PreTrainedModel','TFPreTrainedModel'],tokenizer:PreTrainedTokenizer,modelcard:Optional[ModelCard]=None,framework:Optional[str]=None,task:str='',args_parser:ArgumentHandler=None,device:int=-1,binary_output:bool=False)
transformers.pipelines.Pipeline.__init__(self,model:Union['PreTrainedModel','TFPreTrainedModel'],tokenizer:PreTrainedTokenizer,modelcard:Optional[ModelCard]=None,framework:Optional[str]=None,task:str='',args_parser:ArgumentHandler=None,device:int=-1,binary_output:bool=False)
transformers.pipelines.Pipeline._forward(self,inputs,return_tensors=False)
transformers.pipelines.Pipeline._parse_and_tokenize(self,*args,padding=True,add_special_tokens=True,**kwargs)
transformers.pipelines.Pipeline.check_model_type(self,supported_models:Union[List[str],dict])
transformers.pipelines.Pipeline.device_placement(self)
transformers.pipelines.Pipeline.ensure_tensor_on_device(self,**inputs)
transformers.pipelines.Pipeline.predict(self,X)
transformers.pipelines.Pipeline.save_pretrained(self,save_directory:str)
transformers.pipelines.Pipeline.transform(self,X)
transformers.pipelines.PipelineDataFormat(self,output_path:Optional[str],input_path:Optional[str],column:Optional[str],overwrite:bool=False)
transformers.pipelines.PipelineDataFormat.__init__(self,output_path:Optional[str],input_path:Optional[str],column:Optional[str],overwrite:bool=False)
transformers.pipelines.PipelineDataFormat.__iter__(self)
transformers.pipelines.PipelineDataFormat.from_str(format:str,output_path:Optional[str],input_path:Optional[str],column:Optional[str],overwrite=False)->'PipelineDataFormat'
transformers.pipelines.PipelineDataFormat.save(self,data:Union[dict,List[dict]])
transformers.pipelines.PipelineDataFormat.save_binary(self,data:Union[dict,List[dict]])->str
transformers.pipelines.PipelineException(self,task:str,model:str,reason:str)
transformers.pipelines.PipelineException.__init__(self,task:str,model:str,reason:str)
transformers.pipelines.QuestionAnsweringArgumentHandler(self,*args,**kwargs)
transformers.pipelines.QuestionAnsweringArgumentHandler.__call__(self,*args,**kwargs)
transformers.pipelines.QuestionAnsweringPipeline(self,model:Union['PreTrainedModel','TFPreTrainedModel'],tokenizer:PreTrainedTokenizer,modelcard:Optional[ModelCard]=None,framework:Optional[str]=None,device:int=-1,task:str='',**kwargs)
transformers.pipelines.QuestionAnsweringPipeline.__init__(self,model:Union['PreTrainedModel','TFPreTrainedModel'],tokenizer:PreTrainedTokenizer,modelcard:Optional[ModelCard]=None,framework:Optional[str]=None,device:int=-1,task:str='',**kwargs)
transformers.pipelines.QuestionAnsweringPipeline.create_sample(question:Union[str,List[str]],context:Union[str,List[str]])->Union[SquadExample, List[SquadExample]]
transformers.pipelines.QuestionAnsweringPipeline.decode(self,start:np.ndarray,end:np.ndarray,topk:int,max_answer_len:int)->Tuple
transformers.pipelines.QuestionAnsweringPipeline.span_to_answer(self,text:str,start:int,end:int)->Dict[str, Union[str, int]]
transformers.pipelines.SummarizationPipeline(self,*args,**kwargs)
transformers.pipelines.SummarizationPipeline.__init__(self,*args,**kwargs)
transformers.pipelines.TextClassificationPipeline(self,return_all_scores:bool=False,**kwargs)
transformers.pipelines.TextClassificationPipeline.__init__(self,return_all_scores:bool=False,**kwargs)
transformers.pipelines.TextGenerationPipeline(self,*args,**kwargs)
transformers.pipelines.TextGenerationPipeline.__init__(self,*args,**kwargs)
transformers.pipelines.TextGenerationPipeline._parse_and_tokenize(self,*args,padding=True,add_special_tokens=True,**kwargs)
transformers.pipelines.TokenClassificationPipeline(self,model:Union['PreTrainedModel','TFPreTrainedModel'],tokenizer:PreTrainedTokenizer,modelcard:Optional[ModelCard]=None,framework:Optional[str]=None,args_parser:ArgumentHandler=None,device:int=-1,binary_output:bool=False,ignore_labels=['O'],task:str='',grouped_entities:bool=False)
transformers.pipelines.TokenClassificationPipeline.__init__(self,model:Union['PreTrainedModel','TFPreTrainedModel'],tokenizer:PreTrainedTokenizer,modelcard:Optional[ModelCard]=None,framework:Optional[str]=None,args_parser:ArgumentHandler=None,device:int=-1,binary_output:bool=False,ignore_labels=['O'],task:str='',grouped_entities:bool=False)
transformers.pipelines.TokenClassificationPipeline.group_entities(self,entities:List[dict])->List[dict]
transformers.pipelines.TokenClassificationPipeline.group_sub_entities(self,entities:List[dict])->dict
transformers.pipelines.TranslationPipeline(self,*args,**kwargs)
transformers.pipelines.TranslationPipeline.__init__(self,*args,**kwargs)
transformers.pipelines.ZeroShotClassificationArgumentHandler(self,sequences,labels,hypothesis_template)
transformers.pipelines.ZeroShotClassificationArgumentHandler.__call__(self,sequences,labels,hypothesis_template)
transformers.pipelines.ZeroShotClassificationArgumentHandler._parse_labels(self,labels)
transformers.pipelines.ZeroShotClassificationPipeline(self,args_parser=ZeroShotClassificationArgumentHandler(),*args,**kwargs)
transformers.pipelines.ZeroShotClassificationPipeline.__init__(self,args_parser=ZeroShotClassificationArgumentHandler(),*args,**kwargs)
transformers.pipelines.ZeroShotClassificationPipeline._parse_and_tokenize(self,*args,padding=True,add_special_tokens=True,**kwargs)
transformers.pipelines._ScikitCompat(ABC)
transformers.pipelines._ScikitCompat.predict(self,X)
transformers.pipelines._ScikitCompat.transform(self,X)
transformers.pipelines.get_framework(model=None)
transformers.pipelines.pipeline(task:str,model:Optional=None,config:Optional[Union[str,PretrainedConfig]]=None,tokenizer:Optional[Union[str,PreTrainedTokenizer]]=None,framework:Optional[str]=None,**kwargs)->Pipeline


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/modeling_tf_gpt2.py----------------------------------------
A:transformers.modeling_tf_gpt2.logger->utils.logging.get_logger(__name__)
A:transformers.modeling_tf_gpt2.self.c_attn->TFConv1D(n_state * 3, nx, initializer_range=config.initializer_range, name='c_attn')
A:transformers.modeling_tf_gpt2.self.c_proj->TFConv1D(nx, n_state, initializer_range=config.initializer_range, name='c_proj')
A:transformers.modeling_tf_gpt2.self.attn_dropout->tensorflow.keras.layers.Dropout(config.attn_pdrop)
A:transformers.modeling_tf_gpt2.self.resid_dropout->tensorflow.keras.layers.Dropout(config.resid_pdrop)
A:transformers.modeling_tf_gpt2.self.pruned_heads->set()
A:transformers.modeling_tf_gpt2.j->tensorflow.range(ns)
A:transformers.modeling_tf_gpt2.w->self.attn_dropout(w, training=training)
A:transformers.modeling_tf_gpt2.dk->tensorflow.cast(shape_list(k)[-1], tf.float32)
A:transformers.modeling_tf_gpt2.(_, _, nd, ns)->shape_list(w)
A:transformers.modeling_tf_gpt2.b->tensorflow.reshape(b, [1, 1, nd, ns])
A:transformers.modeling_tf_gpt2.x->self.c_attn(x)
A:transformers.modeling_tf_gpt2.x_shape->shape_list(x)
A:transformers.modeling_tf_gpt2.(query, key, value)->tensorflow.split(x, 3, axis=2)
A:transformers.modeling_tf_gpt2.query->self.split_heads(query)
A:transformers.modeling_tf_gpt2.key->tensorflow.concat([past_key, key], axis=-2)
A:transformers.modeling_tf_gpt2.value->tensorflow.concat([past_value, value], axis=-2)
A:transformers.modeling_tf_gpt2.(past_key, past_value)->tensorflow.unstack(layer_past, axis=0)
A:transformers.modeling_tf_gpt2.present->tensorflow.stack([key, value], axis=0)
A:transformers.modeling_tf_gpt2.attn_outputs->self._attn(query, key, value, attention_mask, head_mask, output_attentions, training=training)
A:transformers.modeling_tf_gpt2.a->self.ln_1(x)
A:transformers.modeling_tf_gpt2.self.c_fc->TFConv1D(n_state, nx, initializer_range=config.initializer_range, name='c_fc')
A:transformers.modeling_tf_gpt2.self.dropout->tensorflow.keras.layers.Dropout(config.resid_pdrop)
A:transformers.modeling_tf_gpt2.h->self.act(self.c_fc(x))
A:transformers.modeling_tf_gpt2.h2->self.dropout(h2, training=training)
A:transformers.modeling_tf_gpt2.self.ln_1->tensorflow.keras.layers.LayerNormalization(epsilon=config.layer_norm_epsilon, name='ln_1')
A:transformers.modeling_tf_gpt2.self.attn->TFAttention(nx, n_ctx, config, scale, name='attn')
A:transformers.modeling_tf_gpt2.self.ln_2->tensorflow.keras.layers.LayerNormalization(epsilon=config.layer_norm_epsilon, name='ln_2')
A:transformers.modeling_tf_gpt2.self.mlp->TFMLP(inner_dim, config, name='mlp')
A:transformers.modeling_tf_gpt2.output_attn->self.attn(a, layer_past, attention_mask, head_mask, use_cache, output_attentions, training=training)
A:transformers.modeling_tf_gpt2.m->self.mlp(m, training=training)
A:transformers.modeling_tf_gpt2.self.wte->TFSharedEmbeddings(config.vocab_size, config.hidden_size, initializer_range=config.initializer_range, name='wte')
A:transformers.modeling_tf_gpt2.self.wpe->tensorflow.keras.layers.Embedding(config.n_positions, config.n_embd, embeddings_initializer=get_initializer(config.initializer_range), name='wpe')
A:transformers.modeling_tf_gpt2.self.drop->tensorflow.keras.layers.Dropout(config.embd_pdrop)
A:transformers.modeling_tf_gpt2.self.ln_f->tensorflow.keras.layers.LayerNormalization(epsilon=config.layer_norm_epsilon, name='ln_f')
A:transformers.modeling_tf_gpt2.input_ids->tensorflow.expand_dims(inputs[:, -1], -1).get('input_ids')
A:transformers.modeling_tf_gpt2.past->tensorflow.expand_dims(inputs[:, -1], -1).get('past', past)
A:transformers.modeling_tf_gpt2.attention_mask->tensorflow.expand_dims(inputs[:, -1], -1).get('attention_mask', attention_mask)
A:transformers.modeling_tf_gpt2.token_type_ids->tensorflow.expand_dims(inputs[:, -1], -1).get('token_type_ids', token_type_ids)
A:transformers.modeling_tf_gpt2.position_ids->tensorflow.expand_dims(inputs[:, -1], -1).get('position_ids', position_ids)
A:transformers.modeling_tf_gpt2.head_mask->tensorflow.expand_dims(inputs[:, -1], -1).get('head_mask', head_mask)
A:transformers.modeling_tf_gpt2.inputs_embeds->tensorflow.expand_dims(inputs[:, -1], -1).get('inputs_embeds', inputs_embeds)
A:transformers.modeling_tf_gpt2.use_cache->tensorflow.expand_dims(inputs[:, -1], -1).get('use_cache', use_cache)
A:transformers.modeling_tf_gpt2.output_attentions->tensorflow.expand_dims(inputs[:, -1], -1).get('output_attentions', output_attentions)
A:transformers.modeling_tf_gpt2.output_hidden_states->tensorflow.expand_dims(inputs[:, -1], -1).get('output_hidden_states', output_hidden_states)
A:transformers.modeling_tf_gpt2.return_dict->tensorflow.expand_dims(inputs[:, -1], -1).get('return_dict', return_dict)
A:transformers.modeling_tf_gpt2.input_shape->shape_list(input_ids)
A:transformers.modeling_tf_gpt2.position_embeds->self.wpe(position_ids)
A:transformers.modeling_tf_gpt2.token_type_embeds->self.wte(token_type_ids, mode='embedding')
A:transformers.modeling_tf_gpt2.hidden_states->tensorflow.reshape(hidden_states, input_shapes + shape_list(hidden_states)[-1:])
A:transformers.modeling_tf_gpt2.outputs->self.transformer(inputs, **kwargs)
A:transformers.modeling_tf_gpt2.all_attentions->tuple((tf.reshape(t, attention_output_shape) for t in all_attentions))
A:transformers.modeling_tf_gpt2.self.transformer->TFGPT2MainLayer(config, name='transformer')
A:transformers.modeling_tf_gpt2.inputs->tensorflow.expand_dims(inputs[:, -1], -1)
A:transformers.modeling_tf_gpt2.labels->tensorflow.expand_dims(inputs[:, -1], -1).pop('labels', labels)
A:transformers.modeling_tf_gpt2.transformer_outputs->self.transformer(flat_input_ids, past, flat_attention_mask, flat_token_type_ids, flat_position_ids, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict=return_dict, training=training)
A:transformers.modeling_tf_gpt2.logits->self.transformer.wte(hidden_states, mode='linear')
A:transformers.modeling_tf_gpt2.loss->self.compute_loss(labels, logits)
A:transformers.modeling_tf_gpt2.self.multiple_choice_head->TFSequenceSummary(config, initializer_range=config.initializer_range, name='multiple_choice_head')
A:transformers.modeling_tf_gpt2.mc_token_ids->tensorflow.expand_dims(inputs[:, -1], -1).get('mc_token_ids', mc_token_ids)
A:transformers.modeling_tf_gpt2.input_shapes->shape_list(input_ids)
A:transformers.modeling_tf_gpt2.lm_logits->self.transformer.wte(hidden_states, mode='linear')
A:transformers.modeling_tf_gpt2.mc_logits->tensorflow.squeeze(mc_logits, axis=-1)
transformers.TFGPT2DoubleHeadsModel(self,config,*inputs,**kwargs)
transformers.TFGPT2DoubleHeadsModel.call(self,inputs,past=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,mc_token_ids=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False)
transformers.TFGPT2DoubleHeadsModel.get_output_embeddings(self)
transformers.TFGPT2DoubleHeadsModelOutput(ModelOutput)
transformers.TFGPT2LMHeadModel(self,config,*inputs,**kwargs)
transformers.TFGPT2LMHeadModel.call(self,inputs,past=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.TFGPT2LMHeadModel.get_output_embeddings(self)
transformers.TFGPT2LMHeadModel.prepare_inputs_for_generation(self,inputs,past,**kwargs)
transformers.TFGPT2MainLayer(self,config,*inputs,**kwargs)
transformers.TFGPT2MainLayer._prune_heads(self,heads_to_prune)
transformers.TFGPT2MainLayer.call(self,inputs,past=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False)
transformers.TFGPT2MainLayer.get_input_embeddings(self)
transformers.TFGPT2MainLayer.set_input_embeddings(self,value)
transformers.TFGPT2Model(self,config,*inputs,**kwargs)
transformers.TFGPT2Model.call(self,inputs,**kwargs)
transformers.TFGPT2PreTrainedModel(TFPreTrainedModel)
transformers.modeling_tf_gpt2.TFAttention(self,nx,n_ctx,config,scale=False,**kwargs)
transformers.modeling_tf_gpt2.TFAttention.__init__(self,nx,n_ctx,config,scale=False,**kwargs)
transformers.modeling_tf_gpt2.TFAttention._attn(self,q,k,v,attention_mask,head_mask,output_attentions,training=False)
transformers.modeling_tf_gpt2.TFAttention.call(self,x,layer_past,attention_mask,head_mask,use_cache,output_attentions,training=False)
transformers.modeling_tf_gpt2.TFAttention.causal_attention_mask(nd,ns,dtype)
transformers.modeling_tf_gpt2.TFAttention.merge_heads(self,x)
transformers.modeling_tf_gpt2.TFAttention.prune_heads(self,heads)
transformers.modeling_tf_gpt2.TFAttention.split_heads(self,x)
transformers.modeling_tf_gpt2.TFBlock(self,n_ctx,config,scale=False,**kwargs)
transformers.modeling_tf_gpt2.TFBlock.__init__(self,n_ctx,config,scale=False,**kwargs)
transformers.modeling_tf_gpt2.TFBlock.call(self,x,layer_past,attention_mask,head_mask,use_cache,output_attentions,training=False)
transformers.modeling_tf_gpt2.TFGPT2DoubleHeadsModel(self,config,*inputs,**kwargs)
transformers.modeling_tf_gpt2.TFGPT2DoubleHeadsModel.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_gpt2.TFGPT2DoubleHeadsModel.call(self,inputs,past=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,mc_token_ids=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False)
transformers.modeling_tf_gpt2.TFGPT2DoubleHeadsModel.get_output_embeddings(self)
transformers.modeling_tf_gpt2.TFGPT2DoubleHeadsModelOutput(ModelOutput)
transformers.modeling_tf_gpt2.TFGPT2LMHeadModel(self,config,*inputs,**kwargs)
transformers.modeling_tf_gpt2.TFGPT2LMHeadModel.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_gpt2.TFGPT2LMHeadModel.call(self,inputs,past=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.modeling_tf_gpt2.TFGPT2LMHeadModel.get_output_embeddings(self)
transformers.modeling_tf_gpt2.TFGPT2LMHeadModel.prepare_inputs_for_generation(self,inputs,past,**kwargs)
transformers.modeling_tf_gpt2.TFGPT2MainLayer(self,config,*inputs,**kwargs)
transformers.modeling_tf_gpt2.TFGPT2MainLayer.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_gpt2.TFGPT2MainLayer._prune_heads(self,heads_to_prune)
transformers.modeling_tf_gpt2.TFGPT2MainLayer.call(self,inputs,past=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False)
transformers.modeling_tf_gpt2.TFGPT2MainLayer.get_input_embeddings(self)
transformers.modeling_tf_gpt2.TFGPT2MainLayer.set_input_embeddings(self,value)
transformers.modeling_tf_gpt2.TFGPT2Model(self,config,*inputs,**kwargs)
transformers.modeling_tf_gpt2.TFGPT2Model.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_gpt2.TFGPT2Model.call(self,inputs,**kwargs)
transformers.modeling_tf_gpt2.TFGPT2PreTrainedModel(TFPreTrainedModel)
transformers.modeling_tf_gpt2.TFMLP(self,n_state,config,**kwargs)
transformers.modeling_tf_gpt2.TFMLP.__init__(self,n_state,config,**kwargs)
transformers.modeling_tf_gpt2.TFMLP.call(self,x,training=False)
transformers.modeling_tf_gpt2.gelu(x)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/modeling_longformer.py----------------------------------------
A:transformers.modeling_longformer.logger->utils.logging.get_logger(__name__)
A:transformers.modeling_longformer.sep_token_indices->(input_ids == sep_token_id).nonzero()
A:transformers.modeling_longformer.question_end_index->question_end_index.unsqueeze(dim=1).unsqueeze(dim=1)
A:transformers.modeling_longformer.attention_mask->self._merge_to_attention_mask(attention_mask, global_attention_mask)
A:transformers.modeling_longformer.self.head_dim->int(config.hidden_size / config.num_attention_heads)
A:transformers.modeling_longformer.self.query->torch.nn.Linear(config.hidden_size, self.embed_dim)
A:transformers.modeling_longformer.self.key->torch.nn.Linear(config.hidden_size, self.embed_dim)
A:transformers.modeling_longformer.self.value->torch.nn.Linear(config.hidden_size, self.embed_dim)
A:transformers.modeling_longformer.self.query_global->torch.nn.Linear(config.hidden_size, self.embed_dim)
A:transformers.modeling_longformer.self.key_global->torch.nn.Linear(config.hidden_size, self.embed_dim)
A:transformers.modeling_longformer.self.value_global->torch.nn.Linear(config.hidden_size, self.embed_dim)
A:transformers.modeling_longformer.is_global_attn->is_index_global_attn.flatten().any().item()
A:transformers.modeling_longformer.hidden_states->self.dropout(hidden_states)
A:transformers.modeling_longformer.query_vectors->query_vectors.view(seq_len, batch_size, self.num_heads, self.head_dim).transpose(0, 1).view(seq_len, batch_size, self.num_heads, self.head_dim).transpose(0, 1)
A:transformers.modeling_longformer.key_vectors->key_vectors.view(seq_len, batch_size, self.num_heads, self.head_dim).transpose(0, 1).view(seq_len, batch_size, self.num_heads, self.head_dim).transpose(0, 1)
A:transformers.modeling_longformer.value_vectors->value_vectors.view(seq_len, batch_size, self.num_heads, self.head_dim).transpose(0, 1).view(seq_len, batch_size, self.num_heads, self.head_dim).transpose(0, 1)
A:transformers.modeling_longformer.(seq_len, batch_size, embed_dim)->self.dropout(hidden_states).size()
A:transformers.modeling_longformer.attn_scores->torch.cat((global_key_attn_scores, attn_scores), dim=-1)
A:transformers.modeling_longformer.float_mask->remove_from_windowed_attention_mask.type_as(query_vectors).masked_fill(remove_from_windowed_attention_mask, -10000.0)
A:transformers.modeling_longformer.diagonal_mask->self._sliding_chunks_query_key_matmul(float_mask.new_ones(size=float_mask.size()), float_mask, self.one_sided_attn_window_size)
A:transformers.modeling_longformer.(max_num_global_attn_indices, is_index_global_attn_nonzero, is_local_index_global_attn_nonzero, is_local_index_no_global_attn_nonzero)->self._get_global_attn_indices(is_index_global_attn)
A:transformers.modeling_longformer.global_key_attn_scores->self._concat_with_global_key_attn_probs(query_vectors=query_vectors, key_vectors=key_vectors, max_num_global_attn_indices=max_num_global_attn_indices, is_index_global_attn_nonzero=is_index_global_attn_nonzero, is_local_index_global_attn_nonzero=is_local_index_global_attn_nonzero, is_local_index_no_global_attn_nonzero=is_local_index_no_global_attn_nonzero)
A:transformers.modeling_longformer.attn_probs_fp32->torch.nn.functional.softmax(attn_scores, dim=-1, dtype=torch.float32)
A:transformers.modeling_longformer.attn_probs->attn_probs.permute(0, 2, 1, 3).permute(0, 2, 1, 3)
A:transformers.modeling_longformer.attn_output->self.output(self_outputs[0], hidden_states)
A:transformers.modeling_longformer.global_attn_output->global_attn_output.view(batch_size, self.num_heads, max_num_global_attn_indices, self.head_dim).view(batch_size, self.num_heads, max_num_global_attn_indices, self.head_dim)
A:transformers.modeling_longformer.attn_output[is_index_global_attn_nonzero[::-1]]->nonzero_global_attn_output.view(len(is_local_index_global_attn_nonzero[0]), -1)
A:transformers.modeling_longformer.hidden_states_padded->hidden_states_padded.view(*hidden_states_padded.size()[:-2], hidden_states_padded.size(-1), hidden_states_padded.size(-2)).view(*hidden_states_padded.size()[:-2], hidden_states_padded.size(-1), hidden_states_padded.size(-2))
A:transformers.modeling_longformer.(total_num_heads, num_chunks, window_overlap, hidden_dim)->chunked_hidden_states.view(total_num_heads, num_chunks, window_overlap, window_overlap + hidden_dim).size()
A:transformers.modeling_longformer.chunked_hidden_states->chunked_hidden_states.view(total_num_heads, num_chunks, window_overlap, window_overlap + hidden_dim).view(total_num_heads, num_chunks, window_overlap, window_overlap + hidden_dim)
A:transformers.modeling_longformer.chunk_size->list(hidden_states.size())
A:transformers.modeling_longformer.chunk_stride->list(hidden_states.stride())
A:transformers.modeling_longformer.beginning_mask_2d->input_tensor.new_ones(affected_seq_len, affected_seq_len + 1).tril().flip(dims=[0])
A:transformers.modeling_longformer.ending_mask->ending_mask.expand(ending_input.size()).expand(ending_input.size())
A:transformers.modeling_longformer.beginning_mask->beginning_mask.expand(beginning_input.size()).expand(beginning_input.size())
A:transformers.modeling_longformer.(batch_size, seq_len, num_heads, head_dim)->value.transpose(1, 2).reshape(batch_size * num_heads, seq_len, head_dim).size()
A:transformers.modeling_longformer.query->query.transpose(1, 2).reshape(batch_size * num_heads, seq_len, head_dim).transpose(1, 2).reshape(batch_size * num_heads, seq_len, head_dim)
A:transformers.modeling_longformer.key->key.transpose(1, 2).reshape(batch_size * num_heads, seq_len, head_dim).transpose(1, 2).reshape(batch_size * num_heads, seq_len, head_dim)
A:transformers.modeling_longformer.chunked_query->self._chunk(query, window_overlap)
A:transformers.modeling_longformer.chunked_key->self._chunk(key, window_overlap)
A:transformers.modeling_longformer.chunked_attention_scores->torch.einsum('bcxd,bcyd->bcxy', (chunked_query, chunked_key))
A:transformers.modeling_longformer.diagonal_chunked_attention_scores->self._pad_and_transpose_last_two_dims(chunked_attention_scores, padding=(0, 0, 0, 1))
A:transformers.modeling_longformer.diagonal_attention_scores->diagonal_attention_scores.view(batch_size, num_heads, seq_len, 2 * window_overlap + 1).transpose(2, 1).view(batch_size, num_heads, seq_len, 2 * window_overlap + 1).transpose(2, 1)
A:transformers.modeling_longformer.chunked_attn_probs->self._pad_and_diagonalize(chunked_attn_probs)
A:transformers.modeling_longformer.value->value.transpose(1, 2).reshape(batch_size * num_heads, seq_len, head_dim).transpose(1, 2).reshape(batch_size * num_heads, seq_len, head_dim)
A:transformers.modeling_longformer.padded_value->torch.nn.functional.pad(value, (0, 0, window_overlap, window_overlap), value=-1)
A:transformers.modeling_longformer.chunked_value_stride->torch.nn.functional.pad(value, (0, 0, window_overlap, window_overlap), value=-1).stride()
A:transformers.modeling_longformer.chunked_value->torch.nn.functional.pad(value, (0, 0, window_overlap, window_overlap), value=-1).as_strided(size=chunked_value_size, stride=chunked_value_stride)
A:transformers.modeling_longformer.context->torch.einsum('bcwd,bcdh->bcwh', (chunked_attn_probs, chunked_value))
A:transformers.modeling_longformer.num_global_attn_indices->is_index_global_attn.long().sum(dim=1)
A:transformers.modeling_longformer.max_num_global_attn_indices->is_index_global_attn.long().sum(dim=1).max()
A:transformers.modeling_longformer.is_index_global_attn_nonzero->is_index_global_attn.nonzero(as_tuple=True)
A:transformers.modeling_longformer.is_local_index_global_attn_nonzero->is_local_index_global_attn.nonzero(as_tuple=True)
A:transformers.modeling_longformer.is_local_index_no_global_attn_nonzero->(is_local_index_global_attn == 0).nonzero(as_tuple=True)
A:transformers.modeling_longformer.key_vectors_only_global->key_vectors.view(seq_len, batch_size, self.num_heads, self.head_dim).transpose(0, 1).view(seq_len, batch_size, self.num_heads, self.head_dim).transpose(0, 1).new_zeros(batch_size, max_num_global_attn_indices, self.num_heads, self.head_dim)
A:transformers.modeling_longformer.attn_probs_from_global_key->torch.einsum('blhd,bshd->blhs', (query_vectors, key_vectors_only_global))
A:transformers.modeling_longformer.attn_probs_only_global->attn_probs.permute(0, 2, 1, 3).permute(0, 2, 1, 3).narrow(-1, 0, max_num_global_attn_indices)
A:transformers.modeling_longformer.value_vectors_only_global->value_vectors.view(seq_len, batch_size, self.num_heads, self.head_dim).transpose(0, 1).view(seq_len, batch_size, self.num_heads, self.head_dim).transpose(0, 1).new_zeros(batch_size, max_num_global_attn_indices, self.num_heads, self.head_dim)
A:transformers.modeling_longformer.attn_output_only_global->torch.matmul(attn_probs_only_global.transpose(1, 2), value_vectors_only_global.transpose(1, 2)).transpose(1, 2)
A:transformers.modeling_longformer.attn_probs_without_global->attn_probs.permute(0, 2, 1, 3).permute(0, 2, 1, 3).narrow(-1, max_num_global_attn_indices, attn_probs.size(-1) - max_num_global_attn_indices).contiguous()
A:transformers.modeling_longformer.attn_output_without_global->self._sliding_chunks_matmul_attn_probs_value(attn_probs_without_global, value_vectors, self.one_sided_attn_window_size)
A:transformers.modeling_longformer.global_attn_hidden_states->self.dropout(hidden_states).new_zeros(max_num_global_attn_indices, batch_size, self.embed_dim)
A:transformers.modeling_longformer.global_query_vectors_only_global->global_query_vectors_only_global.contiguous().view(max_num_global_attn_indices, batch_size * self.num_heads, self.head_dim).transpose(0, 1).contiguous().view(max_num_global_attn_indices, batch_size * self.num_heads, self.head_dim).transpose(0, 1)
A:transformers.modeling_longformer.global_key_vectors->global_key_vectors.contiguous().view(-1, batch_size * self.num_heads, self.head_dim).transpose(0, 1).contiguous().view(-1, batch_size * self.num_heads, self.head_dim).transpose(0, 1)
A:transformers.modeling_longformer.global_value_vectors->global_value_vectors.contiguous().view(-1, batch_size * self.num_heads, self.head_dim).transpose(0, 1).contiguous().view(-1, batch_size * self.num_heads, self.head_dim).transpose(0, 1)
A:transformers.modeling_longformer.global_attn_scores->global_attn_scores.view(batch_size * self.num_heads, max_num_global_attn_indices, seq_len).view(batch_size * self.num_heads, max_num_global_attn_indices, seq_len)
A:transformers.modeling_longformer.global_attn_probs_float->torch.nn.functional.softmax(global_attn_scores, dim=-1, dtype=torch.float32)
A:transformers.modeling_longformer.global_attn_probs->torch.nn.functional.dropout(global_attn_probs_float.type_as(global_attn_scores), p=self.dropout, training=self.training)
A:transformers.modeling_longformer.self.self->LongformerSelfAttention(config, layer_id)
A:transformers.modeling_longformer.self.output->BertOutput(config)
A:transformers.modeling_longformer.self.pruned_heads->self.pruned_heads.union(heads)
A:transformers.modeling_longformer.(heads, index)->find_pruneable_heads_and_indices(heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads)
A:transformers.modeling_longformer.self.self.query->prune_linear_layer(self.self.query, index)
A:transformers.modeling_longformer.self.self.key->prune_linear_layer(self.self.key, index)
A:transformers.modeling_longformer.self.self.value->prune_linear_layer(self.self.value, index)
A:transformers.modeling_longformer.self.output.dense->prune_linear_layer(self.output.dense, index, dim=1)
A:transformers.modeling_longformer.self_outputs->self.self(hidden_states, attention_mask, output_attentions)
A:transformers.modeling_longformer.self.attention->LongformerAttention(config, layer_id)
A:transformers.modeling_longformer.self.intermediate->BertIntermediate(config)
A:transformers.modeling_longformer.self_attn_outputs->self.attention(hidden_states, attention_mask, output_attentions=output_attentions)
A:transformers.modeling_longformer.layer_output->self.output(intermediate_output, attn_output)
A:transformers.modeling_longformer.intermediate_output->self.intermediate(attn_output)
A:transformers.modeling_longformer.self.layer->torch.nn.ModuleList([LongformerLayer(config, layer_id=i) for i in range(config.num_hidden_layers)])
A:transformers.modeling_longformer.layer_outputs->layer_module(hidden_states, attention_mask, output_attentions)
A:transformers.modeling_longformer.self.embeddings->RobertaEmbeddings(config)
A:transformers.modeling_longformer.self.encoder->LongformerEncoder(config)
A:transformers.modeling_longformer.self.pooler->BertPooler(config)
A:transformers.modeling_longformer.input_ids->torch.nn.functional.pad(input_ids, (0, padding_len), value=pad_token_id)
A:transformers.modeling_longformer.position_ids->torch.nn.functional.pad(position_ids, (0, padding_len), value=pad_token_id)
A:transformers.modeling_longformer.input_ids_padding->torch.cat([inputs_embeds, inputs_embeds_padding], dim=-2).new_full((batch_size, padding_len), self.config.pad_token_id, dtype=torch.long)
A:transformers.modeling_longformer.inputs_embeds_padding->self.embeddings(input_ids_padding)
A:transformers.modeling_longformer.inputs_embeds->torch.cat([inputs_embeds, inputs_embeds_padding], dim=-2)
A:transformers.modeling_longformer.token_type_ids->torch.zeros(input_shape, dtype=torch.long, device=device)
A:transformers.modeling_longformer.input_shape->torch.nn.functional.pad(input_ids, (0, padding_len), value=pad_token_id).size()
A:transformers.modeling_longformer.(padding_len, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds)->self._pad_to_window_size(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, inputs_embeds=inputs_embeds, pad_token_id=self.config.pad_token_id)
A:transformers.modeling_longformer.embedding_output->self.embeddings(input_ids=input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds)
A:transformers.modeling_longformer.encoder_outputs->self.encoder(embedding_output, attention_mask=extended_attention_mask, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.modeling_longformer.pooled_output->self.dropout(pooled_output)
A:transformers.modeling_longformer.self.longformer->LongformerModel(config)
A:transformers.modeling_longformer.self.lm_head->RobertaLMHead(config)
A:transformers.modeling_longformer.labels->kwargs.pop('masked_lm_labels')
A:transformers.modeling_longformer.outputs->self.longformer(flat_input_ids, position_ids=flat_position_ids, token_type_ids=flat_token_type_ids, attention_mask=flat_attention_mask, global_attention_mask=flat_global_attention_mask, inputs_embeds=flat_inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.modeling_longformer.prediction_scores->self.lm_head(sequence_output)
A:transformers.modeling_longformer.loss_fct->CrossEntropyLoss()
A:transformers.modeling_longformer.masked_lm_loss->loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))
A:transformers.modeling_longformer.self.classifier->torch.nn.Linear(config.hidden_size, 1)
A:transformers.modeling_longformer.global_attention_mask->torch.stack([_compute_global_attention_mask(input_ids[:, i], self.config.sep_token_id, before_sep_token=False) for i in range(num_choices)], dim=1)
A:transformers.modeling_longformer.logits->self.classifier(pooled_output)
A:transformers.modeling_longformer.loss->loss_fct(reshaped_logits, labels)
A:transformers.modeling_longformer.self.dense->torch.nn.Linear(config.hidden_size, config.hidden_size)
A:transformers.modeling_longformer.self.dropout->torch.nn.Dropout(config.hidden_dropout_prob)
A:transformers.modeling_longformer.self.out_proj->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.modeling_longformer.output->self.out_proj(hidden_states)
A:transformers.modeling_longformer.self.qa_outputs->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.modeling_longformer.(start_logits, end_logits)->self.classifier(pooled_output).split(1, dim=-1)
A:transformers.modeling_longformer.start_logits->start_logits.squeeze(-1).squeeze(-1)
A:transformers.modeling_longformer.end_logits->end_logits.squeeze(-1).squeeze(-1)
A:transformers.modeling_longformer.start_positions->start_positions.squeeze(-1).squeeze(-1)
A:transformers.modeling_longformer.end_positions->end_positions.squeeze(-1).squeeze(-1)
A:transformers.modeling_longformer.ignored_index->start_logits.squeeze(-1).squeeze(-1).size(1)
A:transformers.modeling_longformer.start_loss->loss_fct(start_logits, start_positions)
A:transformers.modeling_longformer.end_loss->loss_fct(end_logits, end_positions)
A:transformers.modeling_longformer.sequence_output->self.dropout(sequence_output)
A:transformers.modeling_longformer.active_logits->self.classifier(pooled_output).view(-1, self.num_labels)
A:transformers.modeling_longformer.active_labels->torch.where(active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels))
A:transformers.modeling_longformer.reshaped_logits->self.classifier(pooled_output).view(-1, num_choices)
transformers.LongformerForMaskedLM(self,config)
transformers.LongformerForMaskedLM.forward(self,input_ids=None,attention_mask=None,global_attention_mask=None,token_type_ids=None,position_ids=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.LongformerForMaskedLM.get_output_embeddings(self)
transformers.LongformerForMultipleChoice(self,config)
transformers.LongformerForMultipleChoice.forward(self,input_ids=None,token_type_ids=None,attention_mask=None,global_attention_mask=None,labels=None,position_ids=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.LongformerForQuestionAnswering(self,config)
transformers.LongformerForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,global_attention_mask=None,token_type_ids=None,position_ids=None,inputs_embeds=None,start_positions=None,end_positions=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.LongformerForSequenceClassification(self,config)
transformers.LongformerForSequenceClassification.forward(self,input_ids=None,attention_mask=None,global_attention_mask=None,token_type_ids=None,position_ids=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.LongformerForTokenClassification(self,config)
transformers.LongformerForTokenClassification.forward(self,input_ids=None,attention_mask=None,global_attention_mask=None,token_type_ids=None,position_ids=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.LongformerModel(self,config)
transformers.LongformerModel._merge_to_attention_mask(self,attention_mask:torch.Tensor,global_attention_mask:torch.Tensor)
transformers.LongformerModel._pad_to_window_size(self,input_ids:torch.Tensor,attention_mask:torch.Tensor,token_type_ids:torch.Tensor,position_ids:torch.Tensor,inputs_embeds:torch.Tensor,pad_token_id:int)
transformers.LongformerModel._prune_heads(self,heads_to_prune)
transformers.LongformerModel.forward(self,input_ids=None,attention_mask=None,global_attention_mask=None,token_type_ids=None,position_ids=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.LongformerModel.get_input_embeddings(self)
transformers.LongformerModel.set_input_embeddings(self,value)
transformers.LongformerSelfAttention(self,config,layer_id)
transformers.LongformerSelfAttention._chunk(hidden_states,window_overlap)
transformers.LongformerSelfAttention._compute_attn_output_with_global_indices(self,value_vectors,attn_probs,max_num_global_attn_indices,is_index_global_attn_nonzero,is_local_index_global_attn_nonzero)
transformers.LongformerSelfAttention._compute_global_attn_output_from_hidden(self,hidden_states,max_num_global_attn_indices,is_local_index_global_attn_nonzero,is_index_global_attn_nonzero,is_local_index_no_global_attn_nonzero,is_index_masked)
transformers.LongformerSelfAttention._concat_with_global_key_attn_probs(self,key_vectors,query_vectors,max_num_global_attn_indices,is_index_global_attn_nonzero,is_local_index_global_attn_nonzero,is_local_index_no_global_attn_nonzero)
transformers.LongformerSelfAttention._get_global_attn_indices(is_index_global_attn)
transformers.LongformerSelfAttention._mask_invalid_locations(input_tensor,affected_seq_len)->torch.Tensor
transformers.LongformerSelfAttention._pad_and_diagonalize(chunked_hidden_states)
transformers.LongformerSelfAttention._pad_and_transpose_last_two_dims(hidden_states_padded,padding)
transformers.LongformerSelfAttention._sliding_chunks_matmul_attn_probs_value(self,attn_probs:torch.Tensor,value:torch.Tensor,window_overlap:int)
transformers.LongformerSelfAttention._sliding_chunks_query_key_matmul(self,query:torch.Tensor,key:torch.Tensor,window_overlap:int)
transformers.LongformerSelfAttention.forward(self,hidden_states,attention_mask=None,output_attentions=False)
transformers.modeling_longformer.LongformerAttention(self,config,layer_id=0)
transformers.modeling_longformer.LongformerAttention.__init__(self,config,layer_id=0)
transformers.modeling_longformer.LongformerAttention.forward(self,hidden_states,attention_mask=None,output_attentions=False)
transformers.modeling_longformer.LongformerAttention.prune_heads(self,heads)
transformers.modeling_longformer.LongformerClassificationHead(self,config)
transformers.modeling_longformer.LongformerClassificationHead.__init__(self,config)
transformers.modeling_longformer.LongformerClassificationHead.forward(self,hidden_states,**kwargs)
transformers.modeling_longformer.LongformerEncoder(self,config)
transformers.modeling_longformer.LongformerEncoder.__init__(self,config)
transformers.modeling_longformer.LongformerEncoder.forward(self,hidden_states,attention_mask=None,output_attentions=False,output_hidden_states=False,return_dict=False)
transformers.modeling_longformer.LongformerForMaskedLM(self,config)
transformers.modeling_longformer.LongformerForMaskedLM.__init__(self,config)
transformers.modeling_longformer.LongformerForMaskedLM.forward(self,input_ids=None,attention_mask=None,global_attention_mask=None,token_type_ids=None,position_ids=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.modeling_longformer.LongformerForMaskedLM.get_output_embeddings(self)
transformers.modeling_longformer.LongformerForMultipleChoice(self,config)
transformers.modeling_longformer.LongformerForMultipleChoice.__init__(self,config)
transformers.modeling_longformer.LongformerForMultipleChoice.forward(self,input_ids=None,token_type_ids=None,attention_mask=None,global_attention_mask=None,labels=None,position_ids=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.modeling_longformer.LongformerForQuestionAnswering(self,config)
transformers.modeling_longformer.LongformerForQuestionAnswering.__init__(self,config)
transformers.modeling_longformer.LongformerForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,global_attention_mask=None,token_type_ids=None,position_ids=None,inputs_embeds=None,start_positions=None,end_positions=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.modeling_longformer.LongformerForSequenceClassification(self,config)
transformers.modeling_longformer.LongformerForSequenceClassification.__init__(self,config)
transformers.modeling_longformer.LongformerForSequenceClassification.forward(self,input_ids=None,attention_mask=None,global_attention_mask=None,token_type_ids=None,position_ids=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.modeling_longformer.LongformerForTokenClassification(self,config)
transformers.modeling_longformer.LongformerForTokenClassification.__init__(self,config)
transformers.modeling_longformer.LongformerForTokenClassification.forward(self,input_ids=None,attention_mask=None,global_attention_mask=None,token_type_ids=None,position_ids=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.modeling_longformer.LongformerLayer(self,config,layer_id=0)
transformers.modeling_longformer.LongformerLayer.__init__(self,config,layer_id=0)
transformers.modeling_longformer.LongformerLayer.ff_chunk(self,attn_output)
transformers.modeling_longformer.LongformerLayer.forward(self,hidden_states,attention_mask=None,output_attentions=False)
transformers.modeling_longformer.LongformerModel(self,config)
transformers.modeling_longformer.LongformerModel.__init__(self,config)
transformers.modeling_longformer.LongformerModel._merge_to_attention_mask(self,attention_mask:torch.Tensor,global_attention_mask:torch.Tensor)
transformers.modeling_longformer.LongformerModel._pad_to_window_size(self,input_ids:torch.Tensor,attention_mask:torch.Tensor,token_type_ids:torch.Tensor,position_ids:torch.Tensor,inputs_embeds:torch.Tensor,pad_token_id:int)
transformers.modeling_longformer.LongformerModel._prune_heads(self,heads_to_prune)
transformers.modeling_longformer.LongformerModel.forward(self,input_ids=None,attention_mask=None,global_attention_mask=None,token_type_ids=None,position_ids=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.modeling_longformer.LongformerModel.get_input_embeddings(self)
transformers.modeling_longformer.LongformerModel.set_input_embeddings(self,value)
transformers.modeling_longformer.LongformerPreTrainedModel(PreTrainedModel)
transformers.modeling_longformer.LongformerPreTrainedModel._init_weights(self,module)
transformers.modeling_longformer.LongformerSelfAttention(self,config,layer_id)
transformers.modeling_longformer.LongformerSelfAttention.__init__(self,config,layer_id)
transformers.modeling_longformer.LongformerSelfAttention._chunk(hidden_states,window_overlap)
transformers.modeling_longformer.LongformerSelfAttention._compute_attn_output_with_global_indices(self,value_vectors,attn_probs,max_num_global_attn_indices,is_index_global_attn_nonzero,is_local_index_global_attn_nonzero)
transformers.modeling_longformer.LongformerSelfAttention._compute_global_attn_output_from_hidden(self,hidden_states,max_num_global_attn_indices,is_local_index_global_attn_nonzero,is_index_global_attn_nonzero,is_local_index_no_global_attn_nonzero,is_index_masked)
transformers.modeling_longformer.LongformerSelfAttention._concat_with_global_key_attn_probs(self,key_vectors,query_vectors,max_num_global_attn_indices,is_index_global_attn_nonzero,is_local_index_global_attn_nonzero,is_local_index_no_global_attn_nonzero)
transformers.modeling_longformer.LongformerSelfAttention._get_global_attn_indices(is_index_global_attn)
transformers.modeling_longformer.LongformerSelfAttention._mask_invalid_locations(input_tensor,affected_seq_len)->torch.Tensor
transformers.modeling_longformer.LongformerSelfAttention._pad_and_diagonalize(chunked_hidden_states)
transformers.modeling_longformer.LongformerSelfAttention._pad_and_transpose_last_two_dims(hidden_states_padded,padding)
transformers.modeling_longformer.LongformerSelfAttention._sliding_chunks_matmul_attn_probs_value(self,attn_probs:torch.Tensor,value:torch.Tensor,window_overlap:int)
transformers.modeling_longformer.LongformerSelfAttention._sliding_chunks_query_key_matmul(self,query:torch.Tensor,key:torch.Tensor,window_overlap:int)
transformers.modeling_longformer.LongformerSelfAttention.forward(self,hidden_states,attention_mask=None,output_attentions=False)
transformers.modeling_longformer._compute_global_attention_mask(input_ids,sep_token_id,before_sep_token=True)
transformers.modeling_longformer._get_question_end_index(input_ids,sep_token_id)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/hf_api.py----------------------------------------
A:transformers.hf_api.path->'{}/api/models'.format(self.endpoint)
A:transformers.hf_api.r->requests.get(path)
A:transformers.hf_api.d->requests.get(path).json()
A:transformers.hf_api.urls->self.presign(token, filename=filename, organization=organization)
A:transformers.hf_api.pf->TqdmProgressFileReader(f)
A:transformers.hf_api.self.pbar->tqdm(total=self.total_size, leave=False)
A:transformers.hf_api.path_token->expanduser('~/.huggingface/token')
transformers.hf_api.HfApi(self,endpoint=None)
transformers.hf_api.HfApi.__init__(self,endpoint=None)
transformers.hf_api.HfApi.delete_obj(self,token:str,filename:str,organization:Optional[str]=None)
transformers.hf_api.HfApi.list_objs(self,token:str,organization:Optional[str]=None)->List[S3Obj]
transformers.hf_api.HfApi.login(self,username:str,password:str)->str
transformers.hf_api.HfApi.logout(self,token:str)->None
transformers.hf_api.HfApi.model_list(self)->List[ModelInfo]
transformers.hf_api.HfApi.presign(self,token:str,filename:str,organization:Optional[str]=None)->PresignedUrl
transformers.hf_api.HfApi.presign_and_upload(self,token:str,filename:str,filepath:str,organization:Optional[str]=None)->str
transformers.hf_api.HfApi.whoami(self,token:str)->Tuple[str, List[str]]
transformers.hf_api.HfFolder
transformers.hf_api.HfFolder.delete_token(cls)
transformers.hf_api.HfFolder.get_token(cls)
transformers.hf_api.HfFolder.save_token(cls,token)
transformers.hf_api.ModelInfo(self,modelId:str,key:str,author:Optional[str]=None,downloads:Optional[int]=None,tags:List[str]=[],pipeline_tag:Optional[str]=None,siblings:Optional[List[Dict]]=None,**kwargs)
transformers.hf_api.ModelInfo.__init__(self,modelId:str,key:str,author:Optional[str]=None,downloads:Optional[int]=None,tags:List[str]=[],pipeline_tag:Optional[str]=None,siblings:Optional[List[Dict]]=None,**kwargs)
transformers.hf_api.PresignedUrl(self,write:str,access:str,type:str,**kwargs)
transformers.hf_api.PresignedUrl.__init__(self,write:str,access:str,type:str,**kwargs)
transformers.hf_api.S3Obj(self,filename:str,LastModified:str,ETag:str,Size:int,**kwargs)
transformers.hf_api.S3Obj.__init__(self,filename:str,LastModified:str,ETag:str,Size:int,**kwargs)
transformers.hf_api.S3Object(self,key:str,etag:str,lastModified:str,size:int,rfilename:str,**kwargs)
transformers.hf_api.S3Object.__init__(self,key:str,etag:str,lastModified:str,size:int,rfilename:str,**kwargs)
transformers.hf_api.TqdmProgressFileReader(self,f:io.BufferedReader)
transformers.hf_api.TqdmProgressFileReader.__init__(self,f:io.BufferedReader)
transformers.hf_api.TqdmProgressFileReader._read(self,n=-1)
transformers.hf_api.TqdmProgressFileReader.close(self)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/convert_xlm_original_pytorch_checkpoint_to_pytorch.py----------------------------------------
A:transformers.convert_xlm_original_pytorch_checkpoint_to_pytorch.chkpt->torch.load(xlm_checkpoint_path, map_location='cpu')
A:transformers.convert_xlm_original_pytorch_checkpoint_to_pytorch.config->dict(((n, v) for (n, v) in config.items() if not isinstance(v, (torch.FloatTensor, numpy.ndarray))))
A:transformers.convert_xlm_original_pytorch_checkpoint_to_pytorch.vocab->dict(((s + '</w>' if s.find('@@') == -1 and i > 13 else s.replace('@@', ''), i) for (s, i) in vocab.items()))
A:transformers.convert_xlm_original_pytorch_checkpoint_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.convert_xlm_original_pytorch_checkpoint_to_pytorch.args->argparse.ArgumentParser().parse_args()
transformers.convert_xlm_original_pytorch_checkpoint_to_pytorch.convert_xlm_checkpoint_to_pytorch(xlm_checkpoint_path,pytorch_dump_folder_path)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/modeling_tf_outputs.py----------------------------------------
transformers.modeling_tf_outputs.TFBaseModelOutput(ModelOutput)
transformers.modeling_tf_outputs.TFBaseModelOutputWithPast(ModelOutput)
transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling(ModelOutput)
transformers.modeling_tf_outputs.TFCausalLMOutput(ModelOutput)
transformers.modeling_tf_outputs.TFCausalLMOutputWithPast(ModelOutput)
transformers.modeling_tf_outputs.TFMaskedLMOutput(ModelOutput)
transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput(ModelOutput)
transformers.modeling_tf_outputs.TFNextSentencePredictorOutput(ModelOutput)
transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput(ModelOutput)
transformers.modeling_tf_outputs.TFSeq2SeqLMOutput(ModelOutput)
transformers.modeling_tf_outputs.TFSeq2SeqModelOutput(ModelOutput)
transformers.modeling_tf_outputs.TFSeq2SeqQuestionAnsweringModelOutput(ModelOutput)
transformers.modeling_tf_outputs.TFSeq2SeqSequenceClassifierOutput(ModelOutput)
transformers.modeling_tf_outputs.TFSequenceClassifierOutput(ModelOutput)
transformers.modeling_tf_outputs.TFTokenClassifierOutput(ModelOutput)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/optimization.py----------------------------------------
A:transformers.optimization.logger->utils.logging.get_logger(__name__)
A:transformers.optimization.defaults->dict(lr=lr, eps=eps, clip_threshold=clip_threshold, decay_rate=decay_rate, beta1=beta1, weight_decay=weight_decay, scale_parameter=scale_parameter, relative_step=relative_step, warmup_init=warmup_init)
A:transformers.optimization.loss->closure()
A:transformers.optimization.state['exp_avg']->state['exp_avg'].to(grad).to(grad)
A:transformers.optimization.state['exp_avg_sq']->state['exp_avg_sq'].to(grad).to(grad)
A:transformers.optimization.denom->exp_avg_sq.sqrt().add_(group['eps'])
A:transformers.optimization.rel_step_sz->min(min_step, 1.0 / math.sqrt(param_state['step']))
A:transformers.optimization.param_scale->max(param_group['eps'][1], param_state['RMS'])
A:transformers.optimization.r_factor->(exp_avg_sq_row / exp_avg_sq_row.mean(dim=-1, keepdim=True)).rsqrt_()
A:transformers.optimization.c_factor->exp_avg_sq_col.rsqrt()
A:transformers.optimization.grad->grad.float().float()
A:transformers.optimization.(factored, use_first_moment)->self._get_options(group, grad_shape)
A:transformers.optimization.state['exp_avg_sq_row']->state['exp_avg_sq_row'].to(grad).to(grad)
A:transformers.optimization.state['exp_avg_sq_col']->state['exp_avg_sq_col'].to(grad).to(grad)
A:transformers.optimization.p_data_fp32->p_data_fp32.float().float()
A:transformers.optimization.state['RMS']->self._rms(p_data_fp32)
A:transformers.optimization.group['lr']->self._get_lr(group, state)
A:transformers.optimization.update->exp_avg_sq.rsqrt().mul_(grad)
transformers.Adafactor(self,params,lr=None,eps=(1e-30,0.001),clip_threshold=1.0,decay_rate=-0.8,beta1=None,weight_decay=0.0,scale_parameter=True,relative_step=True,warmup_init=False)
transformers.Adafactor._approx_sq_grad(exp_avg_sq_row,exp_avg_sq_col)
transformers.Adafactor._get_lr(param_group,param_state)
transformers.Adafactor._get_options(param_group,param_shape)
transformers.Adafactor._rms(tensor)
transformers.Adafactor.step(self,closure=None)
transformers.AdamW(self,params:Iterable[torch.nn.parameter.Parameter],lr:float=0.001,betas:Tuple[float,float]=(0.9,0.999),eps:float=1e-06,weight_decay:float=0.0,correct_bias:bool=True)
transformers.AdamW.step(self,closure:Callable=None)
transformers.get_constant_schedule(optimizer:Optimizer,last_epoch:int=-1)
transformers.get_constant_schedule_with_warmup(optimizer:Optimizer,num_warmup_steps:int,last_epoch:int=-1)
transformers.get_cosine_schedule_with_warmup(optimizer:Optimizer,num_warmup_steps:int,num_training_steps:int,num_cycles:float=0.5,last_epoch:int=-1)
transformers.get_cosine_with_hard_restarts_schedule_with_warmup(optimizer:Optimizer,num_warmup_steps:int,num_training_steps:int,num_cycles:int=1,last_epoch:int=-1)
transformers.get_linear_schedule_with_warmup(optimizer,num_warmup_steps,num_training_steps,last_epoch=-1)
transformers.get_polynomial_decay_schedule_with_warmup(optimizer,num_warmup_steps,num_training_steps,lr_end=1e-07,power=1.0,last_epoch=-1)
transformers.optimization.Adafactor(self,params,lr=None,eps=(1e-30,0.001),clip_threshold=1.0,decay_rate=-0.8,beta1=None,weight_decay=0.0,scale_parameter=True,relative_step=True,warmup_init=False)
transformers.optimization.Adafactor.__init__(self,params,lr=None,eps=(1e-30,0.001),clip_threshold=1.0,decay_rate=-0.8,beta1=None,weight_decay=0.0,scale_parameter=True,relative_step=True,warmup_init=False)
transformers.optimization.Adafactor._approx_sq_grad(exp_avg_sq_row,exp_avg_sq_col)
transformers.optimization.Adafactor._get_lr(param_group,param_state)
transformers.optimization.Adafactor._get_options(param_group,param_shape)
transformers.optimization.Adafactor._rms(tensor)
transformers.optimization.Adafactor.step(self,closure=None)
transformers.optimization.AdamW(self,params:Iterable[torch.nn.parameter.Parameter],lr:float=0.001,betas:Tuple[float,float]=(0.9,0.999),eps:float=1e-06,weight_decay:float=0.0,correct_bias:bool=True)
transformers.optimization.AdamW.__init__(self,params:Iterable[torch.nn.parameter.Parameter],lr:float=0.001,betas:Tuple[float,float]=(0.9,0.999),eps:float=1e-06,weight_decay:float=0.0,correct_bias:bool=True)
transformers.optimization.AdamW.step(self,closure:Callable=None)
transformers.optimization.get_constant_schedule(optimizer:Optimizer,last_epoch:int=-1)
transformers.optimization.get_constant_schedule_with_warmup(optimizer:Optimizer,num_warmup_steps:int,last_epoch:int=-1)
transformers.optimization.get_cosine_schedule_with_warmup(optimizer:Optimizer,num_warmup_steps:int,num_training_steps:int,num_cycles:float=0.5,last_epoch:int=-1)
transformers.optimization.get_cosine_with_hard_restarts_schedule_with_warmup(optimizer:Optimizer,num_warmup_steps:int,num_training_steps:int,num_cycles:int=1,last_epoch:int=-1)
transformers.optimization.get_linear_schedule_with_warmup(optimizer,num_warmup_steps,num_training_steps,last_epoch=-1)
transformers.optimization.get_polynomial_decay_schedule_with_warmup(optimizer,num_warmup_steps,num_training_steps,lr_end=1e-07,power=1.0,last_epoch=-1)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/convert_t5_original_tf_checkpoint_to_pytorch.py----------------------------------------
A:transformers.convert_t5_original_tf_checkpoint_to_pytorch.config->transformers.T5Config.from_json_file(config_file)
A:transformers.convert_t5_original_tf_checkpoint_to_pytorch.model->T5Model(config)
A:transformers.convert_t5_original_tf_checkpoint_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.convert_t5_original_tf_checkpoint_to_pytorch.args->argparse.ArgumentParser().parse_args()
transformers.convert_t5_original_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(tf_checkpoint_path,config_file,pytorch_dump_path)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/configuration_auto.py----------------------------------------
A:transformers.configuration_auto.ALL_PRETRAINED_CONFIG_ARCHIVE_MAP->dict(((key, value) for pretrained_map in [BERT_PRETRAINED_CONFIG_ARCHIVE_MAP, BART_PRETRAINED_CONFIG_ARCHIVE_MAP, MBART_PRETRAINED_CONFIG_ARCHIVE_MAP, OPENAI_GPT_PRETRAINED_CONFIG_ARCHIVE_MAP, TRANSFO_XL_PRETRAINED_CONFIG_ARCHIVE_MAP, GPT2_PRETRAINED_CONFIG_ARCHIVE_MAP, CTRL_PRETRAINED_CONFIG_ARCHIVE_MAP, XLNET_PRETRAINED_CONFIG_ARCHIVE_MAP, XLM_PRETRAINED_CONFIG_ARCHIVE_MAP, ROBERTA_PRETRAINED_CONFIG_ARCHIVE_MAP, DISTILBERT_PRETRAINED_CONFIG_ARCHIVE_MAP, ALBERT_PRETRAINED_CONFIG_ARCHIVE_MAP, CAMEMBERT_PRETRAINED_CONFIG_ARCHIVE_MAP, T5_PRETRAINED_CONFIG_ARCHIVE_MAP, XLM_ROBERTA_PRETRAINED_CONFIG_ARCHIVE_MAP, FLAUBERT_PRETRAINED_CONFIG_ARCHIVE_MAP, ELECTRA_PRETRAINED_CONFIG_ARCHIVE_MAP, LONGFORMER_PRETRAINED_CONFIG_ARCHIVE_MAP, RETRIBERT_PRETRAINED_CONFIG_ARCHIVE_MAP] for (key, value) in pretrained_map.items()))
A:transformers.configuration_auto.CONFIG_MAPPING->OrderedDict([('retribert', RetriBertConfig), ('t5', T5Config), ('mobilebert', MobileBertConfig), ('distilbert', DistilBertConfig), ('albert', AlbertConfig), ('camembert', CamembertConfig), ('xlm-roberta', XLMRobertaConfig), ('pegasus', PegasusConfig), ('marian', MarianConfig), ('mbart', MBartConfig), ('bart', BartConfig), ('reformer', ReformerConfig), ('longformer', LongformerConfig), ('roberta', RobertaConfig), ('flaubert', FlaubertConfig), ('bert', BertConfig), ('openai-gpt', OpenAIGPTConfig), ('gpt2', GPT2Config), ('transfo-xl', TransfoXLConfig), ('xlnet', XLNetConfig), ('xlm', XLMConfig), ('ctrl', CTRLConfig), ('electra', ElectraConfig), ('encoder-decoder', EncoderDecoderConfig)])
A:transformers.configuration_auto.(config_dict, _)->configuration_utils.PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
transformers.AutoConfig(self)
transformers.AutoConfig.for_model(cls,model_type:str,*args,**kwargs)
transformers.AutoConfig.from_pretrained(cls,pretrained_model_name_or_path,**kwargs)
transformers.configuration_auto.AutoConfig(self)
transformers.configuration_auto.AutoConfig.__init__(self)
transformers.configuration_auto.AutoConfig.for_model(cls,model_type:str,*args,**kwargs)
transformers.configuration_auto.AutoConfig.from_pretrained(cls,pretrained_model_name_or_path,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/convert_albert_original_tf_checkpoint_to_pytorch.py----------------------------------------
A:transformers.convert_albert_original_tf_checkpoint_to_pytorch.config->transformers.AlbertConfig.from_json_file(albert_config_file)
A:transformers.convert_albert_original_tf_checkpoint_to_pytorch.model->AlbertForPreTraining(config)
A:transformers.convert_albert_original_tf_checkpoint_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.convert_albert_original_tf_checkpoint_to_pytorch.args->argparse.ArgumentParser().parse_args()
transformers.convert_albert_original_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(tf_checkpoint_path,albert_config_file,pytorch_dump_path)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/tokenization_reformer.py----------------------------------------
A:transformers.tokenization_reformer.logger->utils.logging.get_logger(__name__)
A:transformers.tokenization_reformer.self.sp_model->sentencepiece.SentencePieceProcessor()
A:transformers.tokenization_reformer.state->self.__dict__.copy()
A:transformers.tokenization_reformer.pieces->self.sp_model.SampleEncodeAsPieces(text, 64, 0.1)
A:transformers.tokenization_reformer.token->self.sp_model.IdToPiece(index)
A:transformers.tokenization_reformer.out_string->self.sp_model.decode_pieces(tokens)
A:transformers.tokenization_reformer.out_vocab_file->os.path.join(save_directory, VOCAB_FILES_NAMES['vocab_file'])
transformers.ReformerTokenizer(self,vocab_file,eos_token='</s>',unk_token='<unk>',pad_token='<pad>',additional_special_tokens=[],**kwargs)
transformers.ReformerTokenizer.__getstate__(self)
transformers.ReformerTokenizer.__setstate__(self,d)
transformers.ReformerTokenizer._convert_id_to_token(self,index)
transformers.ReformerTokenizer._convert_token_to_id(self,token)
transformers.ReformerTokenizer._tokenize(self,text,sample=False)
transformers.ReformerTokenizer.convert_tokens_to_string(self,tokens)
transformers.ReformerTokenizer.get_vocab(self)
transformers.ReformerTokenizer.save_vocabulary(self,save_directory)
transformers.ReformerTokenizer.vocab_size(self)
transformers.tokenization_reformer.ReformerTokenizer(self,vocab_file,eos_token='</s>',unk_token='<unk>',pad_token='<pad>',additional_special_tokens=[],**kwargs)
transformers.tokenization_reformer.ReformerTokenizer.__getstate__(self)
transformers.tokenization_reformer.ReformerTokenizer.__init__(self,vocab_file,eos_token='</s>',unk_token='<unk>',pad_token='<pad>',additional_special_tokens=[],**kwargs)
transformers.tokenization_reformer.ReformerTokenizer.__setstate__(self,d)
transformers.tokenization_reformer.ReformerTokenizer._convert_id_to_token(self,index)
transformers.tokenization_reformer.ReformerTokenizer._convert_token_to_id(self,token)
transformers.tokenization_reformer.ReformerTokenizer._tokenize(self,text,sample=False)
transformers.tokenization_reformer.ReformerTokenizer.convert_tokens_to_string(self,tokens)
transformers.tokenization_reformer.ReformerTokenizer.get_vocab(self)
transformers.tokenization_reformer.ReformerTokenizer.save_vocabulary(self,save_directory)
transformers.tokenization_reformer.ReformerTokenizer.vocab_size(self)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/convert_reformer_trax_checkpoint_to_pytorch.py----------------------------------------
A:transformers.convert_reformer_trax_checkpoint_to_pytorch.torch_layer.weight->torch.nn.Parameter(weight)
A:transformers.convert_reformer_trax_checkpoint_to_pytorch.torch_layer.bias->torch.nn.Parameter(bias)
A:transformers.convert_reformer_trax_checkpoint_to_pytorch.np_query_key->numpy.asarray(weights[0])
A:transformers.convert_reformer_trax_checkpoint_to_pytorch.np_value->numpy.asarray(weights[2])
A:transformers.convert_reformer_trax_checkpoint_to_pytorch.np_dense->numpy.asarray(weights[3])
A:transformers.convert_reformer_trax_checkpoint_to_pytorch.np_query->numpy.asarray(weights[0])
A:transformers.convert_reformer_trax_checkpoint_to_pytorch.np_key->numpy.asarray(weights[1])
A:transformers.convert_reformer_trax_checkpoint_to_pytorch.layer_norm_1_weight->numpy.asarray(layer_norm_1[0])
A:transformers.convert_reformer_trax_checkpoint_to_pytorch.layer_norm_1_bias->numpy.asarray(layer_norm_1[1])
A:transformers.convert_reformer_trax_checkpoint_to_pytorch.layer_norm_2_weight->numpy.asarray(intermediate_weights[0][0])
A:transformers.convert_reformer_trax_checkpoint_to_pytorch.layer_norm_2_bias->numpy.asarray(intermediate_weights[0][1])
A:transformers.convert_reformer_trax_checkpoint_to_pytorch.inter_dense_weight->numpy.asarray(intermediate_weights[1][0])
A:transformers.convert_reformer_trax_checkpoint_to_pytorch.inter_dense_bias->numpy.asarray(intermediate_weights[1][1])
A:transformers.convert_reformer_trax_checkpoint_to_pytorch.out_dense_weight->numpy.asarray(intermediate_weights[4][0])
A:transformers.convert_reformer_trax_checkpoint_to_pytorch.out_dense_bias->numpy.asarray(intermediate_weights[4][1])
A:transformers.convert_reformer_trax_checkpoint_to_pytorch.word_embeddings->numpy.asarray(weights[1])
A:transformers.convert_reformer_trax_checkpoint_to_pytorch.emb_weights->numpy.asarray(weights[3][emb_idx][0])
A:transformers.convert_reformer_trax_checkpoint_to_pytorch.position_embeddings.weights[emb_idx]->torch.nn.Parameter(torch.tensor(emb_weights))
A:transformers.convert_reformer_trax_checkpoint_to_pytorch.layer_norm_out_weight->numpy.asarray(weights[7][0])
A:transformers.convert_reformer_trax_checkpoint_to_pytorch.layer_norm_out_bias->numpy.asarray(weights[7][1])
A:transformers.convert_reformer_trax_checkpoint_to_pytorch.output_embed_weights->numpy.asarray(weights[9][0])
A:transformers.convert_reformer_trax_checkpoint_to_pytorch.output_embed_bias->numpy.asarray(weights[9][1])
A:transformers.convert_reformer_trax_checkpoint_to_pytorch.config->transformers.ReformerConfig.from_json_file(config_file)
A:transformers.convert_reformer_trax_checkpoint_to_pytorch.model->ReformerModelWithLMHead(config)
A:transformers.convert_reformer_trax_checkpoint_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.convert_reformer_trax_checkpoint_to_pytorch.args->argparse.ArgumentParser().parse_args()
transformers.convert_reformer_trax_checkpoint_to_pytorch.convert_trax_checkpoint_to_pytorch(trax_model_pkl_path,config_file,pytorch_dump_path)
transformers.convert_reformer_trax_checkpoint_to_pytorch.set_block_weights_in_torch(weights,torch_block,hidden_size)
transformers.convert_reformer_trax_checkpoint_to_pytorch.set_layer_weights_in_torch_local(weights,torch_layer,hidden_size)
transformers.convert_reformer_trax_checkpoint_to_pytorch.set_layer_weights_in_torch_lsh(weights,torch_layer,hidden_size)
transformers.convert_reformer_trax_checkpoint_to_pytorch.set_model_weights_in_torch(weights,torch_model,hidden_size)
transformers.convert_reformer_trax_checkpoint_to_pytorch.set_param(torch_layer,weight,bias=None)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/tokenization_ctrl.py----------------------------------------
A:transformers.tokenization_ctrl.logger->utils.logging.get_logger(__name__)
A:transformers.tokenization_ctrl.pairs->get_pairs(word)
A:transformers.tokenization_ctrl.self.encoder->json.load(vocab_handle)
A:transformers.tokenization_ctrl.self.bpe_ranks->dict(zip(merges, range(len(merges))))
A:transformers.tokenization_ctrl.word->'@@ '.join(word)
A:transformers.tokenization_ctrl.bigram->min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float('inf')))
A:transformers.tokenization_ctrl.j->'@@ '.join(word).index(first, i)
A:transformers.tokenization_ctrl.new_word->tuple(new_word)
A:transformers.tokenization_ctrl.words->regex.findall('\\S+\\n?', text)
A:transformers.tokenization_ctrl.out_string->' '.join(tokens).replace('@@ ', '').strip()
A:transformers.tokenization_ctrl.vocab_file->os.path.join(save_directory, VOCAB_FILES_NAMES['vocab_file'])
A:transformers.tokenization_ctrl.merge_file->os.path.join(save_directory, VOCAB_FILES_NAMES['merges_file'])
transformers.CTRLTokenizer(self,vocab_file,merges_file,unk_token='<unk>',**kwargs)
transformers.CTRLTokenizer._convert_id_to_token(self,index)
transformers.CTRLTokenizer._convert_token_to_id(self,token)
transformers.CTRLTokenizer._tokenize(self,text)
transformers.CTRLTokenizer.bpe(self,token)
transformers.CTRLTokenizer.convert_tokens_to_string(self,tokens)
transformers.CTRLTokenizer.get_vocab(self)
transformers.CTRLTokenizer.save_vocabulary(self,save_directory)
transformers.CTRLTokenizer.vocab_size(self)
transformers.tokenization_ctrl.CTRLTokenizer(self,vocab_file,merges_file,unk_token='<unk>',**kwargs)
transformers.tokenization_ctrl.CTRLTokenizer.__init__(self,vocab_file,merges_file,unk_token='<unk>',**kwargs)
transformers.tokenization_ctrl.CTRLTokenizer._convert_id_to_token(self,index)
transformers.tokenization_ctrl.CTRLTokenizer._convert_token_to_id(self,token)
transformers.tokenization_ctrl.CTRLTokenizer._tokenize(self,text)
transformers.tokenization_ctrl.CTRLTokenizer.bpe(self,token)
transformers.tokenization_ctrl.CTRLTokenizer.convert_tokens_to_string(self,tokens)
transformers.tokenization_ctrl.CTRLTokenizer.get_vocab(self)
transformers.tokenization_ctrl.CTRLTokenizer.save_vocabulary(self,save_directory)
transformers.tokenization_ctrl.CTRLTokenizer.vocab_size(self)
transformers.tokenization_ctrl.get_pairs(word)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/tokenization_roberta.py----------------------------------------
A:transformers.tokenization_roberta.logger->utils.logging.get_logger(__name__)
A:transformers.tokenization_roberta.add_prefix_space->kwargs.pop('add_prefix_space', self.add_prefix_space)
A:transformers.tokenization_roberta.self.backend_tokenizer._tokenizer.post_processor->RobertaProcessing(sep=(sep_token, self.sep_token_id), cls=(cls_token, self.cls_token_id), add_prefix_space=add_prefix_space, trim_offsets=trim_offsets)
transformers.RobertaTokenizer(self,vocab_file,merges_file,errors='replace',bos_token='<s>',eos_token='</s>',sep_token='</s>',cls_token='<s>',unk_token='<unk>',pad_token='<pad>',mask_token='<mask>',add_prefix_space=False,**kwargs)
transformers.RobertaTokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.RobertaTokenizer.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.RobertaTokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.RobertaTokenizer.prepare_for_tokenization(self,text,is_pretokenized=False,**kwargs)
transformers.RobertaTokenizerFast(self,vocab_file,merges_file,errors='replace',bos_token='<s>',eos_token='</s>',sep_token='</s>',cls_token='<s>',unk_token='<unk>',pad_token='<pad>',mask_token='<mask>',add_prefix_space=False,trim_offsets=True,**kwargs)
transformers.RobertaTokenizerFast.build_inputs_with_special_tokens(self,token_ids_0,token_ids_1=None)
transformers.RobertaTokenizerFast.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.tokenization_roberta.RobertaTokenizer(self,vocab_file,merges_file,errors='replace',bos_token='<s>',eos_token='</s>',sep_token='</s>',cls_token='<s>',unk_token='<unk>',pad_token='<pad>',mask_token='<mask>',add_prefix_space=False,**kwargs)
transformers.tokenization_roberta.RobertaTokenizer.__init__(self,vocab_file,merges_file,errors='replace',bos_token='<s>',eos_token='</s>',sep_token='</s>',cls_token='<s>',unk_token='<unk>',pad_token='<pad>',mask_token='<mask>',add_prefix_space=False,**kwargs)
transformers.tokenization_roberta.RobertaTokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.tokenization_roberta.RobertaTokenizer.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.tokenization_roberta.RobertaTokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.tokenization_roberta.RobertaTokenizer.prepare_for_tokenization(self,text,is_pretokenized=False,**kwargs)
transformers.tokenization_roberta.RobertaTokenizerFast(self,vocab_file,merges_file,errors='replace',bos_token='<s>',eos_token='</s>',sep_token='</s>',cls_token='<s>',unk_token='<unk>',pad_token='<pad>',mask_token='<mask>',add_prefix_space=False,trim_offsets=True,**kwargs)
transformers.tokenization_roberta.RobertaTokenizerFast.__init__(self,vocab_file,merges_file,errors='replace',bos_token='<s>',eos_token='</s>',sep_token='</s>',cls_token='<s>',unk_token='<unk>',pad_token='<pad>',mask_token='<mask>',add_prefix_space=False,trim_offsets=True,**kwargs)
transformers.tokenization_roberta.RobertaTokenizerFast.build_inputs_with_special_tokens(self,token_ids_0,token_ids_1=None)
transformers.tokenization_roberta.RobertaTokenizerFast.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/modeling_tf_electra.py----------------------------------------
A:transformers.modeling_tf_electra.logger->utils.logging.get_logger(__name__)
A:transformers.modeling_tf_electra.self.position_embeddings->tensorflow.keras.layers.Embedding(config.max_position_embeddings, config.embedding_size, embeddings_initializer=get_initializer(self.initializer_range), name='position_embeddings')
A:transformers.modeling_tf_electra.self.token_type_embeddings->tensorflow.keras.layers.Embedding(config.type_vocab_size, config.embedding_size, embeddings_initializer=get_initializer(self.initializer_range), name='token_type_embeddings')
A:transformers.modeling_tf_electra.self.LayerNorm->tensorflow.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name='LayerNorm')
A:transformers.modeling_tf_electra.self.dropout->tensorflow.keras.layers.Dropout(config.hidden_dropout_prob)
A:transformers.modeling_tf_electra.self.word_embeddings->self.add_weight('weight', shape=[self.vocab_size, self.embedding_size], initializer=get_initializer(self.initializer_range))
A:transformers.modeling_tf_electra.input_shape->shape_list(input_ids)
A:transformers.modeling_tf_electra.token_type_ids->inputs.get('token_type_ids', token_type_ids)
A:transformers.modeling_tf_electra.inputs_embeds->inputs.get('inputs_embeds', inputs_embeds)
A:transformers.modeling_tf_electra.position_embeddings->tensorflow.cast(self.position_embeddings(position_ids), inputs_embeds.dtype)
A:transformers.modeling_tf_electra.token_type_embeddings->tensorflow.cast(self.token_type_embeddings(token_type_ids), inputs_embeds.dtype)
A:transformers.modeling_tf_electra.embeddings->self.dropout(embeddings, training=training)
A:transformers.modeling_tf_electra.x->self.out_proj(x)
A:transformers.modeling_tf_electra.logits->self.qa_outputs(discriminator_sequence_output)
A:transformers.modeling_tf_electra.self.dense->tensorflow.keras.layers.Dense(config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name='dense')
A:transformers.modeling_tf_electra.self.dense_prediction->tensorflow.keras.layers.Dense(1, name='dense_prediction')
A:transformers.modeling_tf_electra.hidden_states->self.input_embeddings(hidden_states, mode='linear')
A:transformers.modeling_tf_electra.attention_mask->inputs.get('attention_mask', attention_mask)
A:transformers.modeling_tf_electra.extended_attention_mask->self.get_extended_attention_mask(attention_mask, input_shape, hidden_states.dtype)
A:transformers.modeling_tf_electra.self.embeddings->TFElectraEmbeddings(config, name='embeddings')
A:transformers.modeling_tf_electra.self.embeddings_project->tensorflow.keras.layers.Dense(config.hidden_size, name='embeddings_project')
A:transformers.modeling_tf_electra.self.encoder->TFBertEncoder(config, name='encoder')
A:transformers.modeling_tf_electra.input_ids->inputs.get('input_ids')
A:transformers.modeling_tf_electra.position_ids->inputs.get('position_ids', position_ids)
A:transformers.modeling_tf_electra.head_mask->inputs.get('head_mask', head_mask)
A:transformers.modeling_tf_electra.output_attentions->inputs.get('output_attentions', output_attentions)
A:transformers.modeling_tf_electra.output_hidden_states->inputs.get('output_hidden_states', output_hidden_states)
A:transformers.modeling_tf_electra.return_dict->inputs.get('return_dict', return_dict)
A:transformers.modeling_tf_electra.self.electra->TFElectraMainLayer(config, name='electra')
A:transformers.modeling_tf_electra.outputs->self.electra(flat_input_ids, flat_attention_mask, flat_token_type_ids, flat_position_ids, head_mask, flat_inputs_embeds, output_attentions, output_hidden_states, return_dict=return_dict, training=training)
A:transformers.modeling_tf_electra.self.discriminator_predictions->TFElectraDiscriminatorPredictions(config, name='discriminator_predictions')
A:transformers.modeling_tf_electra.discriminator_hidden_states->self.electra(inputs, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict=return_dict, training=training)
A:transformers.modeling_tf_electra.self.bias->self.add_weight(shape=(self.vocab_size,), initializer='zeros', trainable=True, name='bias')
A:transformers.modeling_tf_electra.self.generator_predictions->TFElectraGeneratorPredictions(config, name='generator_predictions')
A:transformers.modeling_tf_electra.self.generator_lm_head->TFElectraMaskedLMHead(config, self.electra.embeddings, name='generator_lm_head')
A:transformers.modeling_tf_electra.labels->inputs.pop('labels', labels)
A:transformers.modeling_tf_electra.generator_hidden_states->self.electra(input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, training=training)
A:transformers.modeling_tf_electra.prediction_scores->self.generator_lm_head(prediction_scores, training=training)
A:transformers.modeling_tf_electra.self.out_proj->tensorflow.keras.layers.Dense(config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name='out_proj')
A:transformers.modeling_tf_electra.self.classifier->tensorflow.keras.layers.Dense(config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name='classifier')
A:transformers.modeling_tf_electra.self.sequence_summary->TFSequenceSummary(config, initializer_range=config.initializer_range, name='sequence_summary')
A:transformers.modeling_tf_electra.reshaped_logits->tensorflow.reshape(logits, (-1, num_choices))
A:transformers.modeling_tf_electra.discriminator_sequence_output->self.dropout(discriminator_sequence_output)
A:transformers.modeling_tf_electra.self.qa_outputs->tensorflow.keras.layers.Dense(config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name='qa_outputs')
A:transformers.modeling_tf_electra.start_positions->inputs.pop('start_positions', start_positions)
A:transformers.modeling_tf_electra.end_positions->inputs.pop('end_positions', start_positions)
A:transformers.modeling_tf_electra.(start_logits, end_logits)->tensorflow.split(logits, 2, axis=-1)
A:transformers.modeling_tf_electra.start_logits->tensorflow.squeeze(start_logits, axis=-1)
A:transformers.modeling_tf_electra.end_logits->tensorflow.squeeze(end_logits, axis=-1)
A:transformers.modeling_tf_electra.loss->self.compute_loss(labels, (start_logits, end_logits))
transformers.TFElectraForMaskedLM(self,config,**kwargs)
transformers.TFElectraForMaskedLM.call(self,input_ids,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.TFElectraForMaskedLM.get_output_embeddings(self)
transformers.TFElectraForMultipleChoice(self,config,*inputs,**kwargs)
transformers.TFElectraForMultipleChoice.call(self,inputs,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.TFElectraForMultipleChoice.dummy_inputs(self)
transformers.TFElectraForPreTraining(self,config,**kwargs)
transformers.TFElectraForPreTraining.call(self,input_ids,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False)
transformers.TFElectraForPreTrainingOutput(ModelOutput)
transformers.TFElectraForQuestionAnswering(self,config,*inputs,**kwargs)
transformers.TFElectraForQuestionAnswering.call(self,inputs,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,start_positions=None,end_positions=None,training=False)
transformers.TFElectraForSequenceClassification(self,config,*inputs,**kwargs)
transformers.TFElectraForSequenceClassification.call(self,input_ids,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.TFElectraForTokenClassification(self,config,**kwargs)
transformers.TFElectraForTokenClassification.call(self,inputs,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.TFElectraModel(self,config,*inputs,**kwargs)
transformers.TFElectraModel.call(self,inputs,**kwargs)
transformers.TFElectraPreTrainedModel(TFBertPreTrainedModel)
transformers.TFElectraPreTrainedModel.get_extended_attention_mask(self,attention_mask,input_shape,dtype)
transformers.TFElectraPreTrainedModel.get_head_mask(self,head_mask)
transformers.modeling_tf_electra.TFElectraClassificationHead(self,config,**kwargs)
transformers.modeling_tf_electra.TFElectraClassificationHead.__init__(self,config,**kwargs)
transformers.modeling_tf_electra.TFElectraClassificationHead.call(self,inputs,**kwargs)
transformers.modeling_tf_electra.TFElectraDiscriminatorPredictions(self,config,**kwargs)
transformers.modeling_tf_electra.TFElectraDiscriminatorPredictions.__init__(self,config,**kwargs)
transformers.modeling_tf_electra.TFElectraDiscriminatorPredictions.call(self,discriminator_hidden_states,training=False)
transformers.modeling_tf_electra.TFElectraEmbeddings(self,config,**kwargs)
transformers.modeling_tf_electra.TFElectraEmbeddings.__init__(self,config,**kwargs)
transformers.modeling_tf_electra.TFElectraEmbeddings._embedding(self,input_ids,position_ids,token_type_ids,inputs_embeds,training=False)
transformers.modeling_tf_electra.TFElectraEmbeddings._linear(self,inputs)
transformers.modeling_tf_electra.TFElectraEmbeddings.build(self,input_shape)
transformers.modeling_tf_electra.TFElectraEmbeddings.call(self,input_ids,position_ids=None,token_type_ids=None,inputs_embeds=None,mode='embedding',training=False)
transformers.modeling_tf_electra.TFElectraForMaskedLM(self,config,**kwargs)
transformers.modeling_tf_electra.TFElectraForMaskedLM.__init__(self,config,**kwargs)
transformers.modeling_tf_electra.TFElectraForMaskedLM.call(self,input_ids,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.modeling_tf_electra.TFElectraForMaskedLM.get_output_embeddings(self)
transformers.modeling_tf_electra.TFElectraForMultipleChoice(self,config,*inputs,**kwargs)
transformers.modeling_tf_electra.TFElectraForMultipleChoice.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_electra.TFElectraForMultipleChoice.call(self,inputs,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.modeling_tf_electra.TFElectraForMultipleChoice.dummy_inputs(self)
transformers.modeling_tf_electra.TFElectraForPreTraining(self,config,**kwargs)
transformers.modeling_tf_electra.TFElectraForPreTraining.__init__(self,config,**kwargs)
transformers.modeling_tf_electra.TFElectraForPreTraining.call(self,input_ids,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False)
transformers.modeling_tf_electra.TFElectraForPreTrainingOutput(ModelOutput)
transformers.modeling_tf_electra.TFElectraForQuestionAnswering(self,config,*inputs,**kwargs)
transformers.modeling_tf_electra.TFElectraForQuestionAnswering.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_electra.TFElectraForQuestionAnswering.call(self,inputs,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,start_positions=None,end_positions=None,training=False)
transformers.modeling_tf_electra.TFElectraForSequenceClassification(self,config,*inputs,**kwargs)
transformers.modeling_tf_electra.TFElectraForSequenceClassification.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_electra.TFElectraForSequenceClassification.call(self,input_ids,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.modeling_tf_electra.TFElectraForTokenClassification(self,config,**kwargs)
transformers.modeling_tf_electra.TFElectraForTokenClassification.__init__(self,config,**kwargs)
transformers.modeling_tf_electra.TFElectraForTokenClassification.call(self,inputs,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.modeling_tf_electra.TFElectraGeneratorPredictions(self,config,**kwargs)
transformers.modeling_tf_electra.TFElectraGeneratorPredictions.__init__(self,config,**kwargs)
transformers.modeling_tf_electra.TFElectraGeneratorPredictions.call(self,generator_hidden_states,training=False)
transformers.modeling_tf_electra.TFElectraMainLayer(self,config,**kwargs)
transformers.modeling_tf_electra.TFElectraMainLayer.__init__(self,config,**kwargs)
transformers.modeling_tf_electra.TFElectraMainLayer._prune_heads(self,heads_to_prune)
transformers.modeling_tf_electra.TFElectraMainLayer._resize_token_embeddings(self,new_num_tokens)
transformers.modeling_tf_electra.TFElectraMainLayer.call(self,inputs,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False)
transformers.modeling_tf_electra.TFElectraMainLayer.get_input_embeddings(self)
transformers.modeling_tf_electra.TFElectraMainLayer.set_input_embeddings(self,value)
transformers.modeling_tf_electra.TFElectraMaskedLMHead(self,config,input_embeddings,**kwargs)
transformers.modeling_tf_electra.TFElectraMaskedLMHead.__init__(self,config,input_embeddings,**kwargs)
transformers.modeling_tf_electra.TFElectraMaskedLMHead.build(self,input_shape)
transformers.modeling_tf_electra.TFElectraMaskedLMHead.call(self,hidden_states,training=False)
transformers.modeling_tf_electra.TFElectraModel(self,config,*inputs,**kwargs)
transformers.modeling_tf_electra.TFElectraModel.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_electra.TFElectraModel.call(self,inputs,**kwargs)
transformers.modeling_tf_electra.TFElectraPreTrainedModel(TFBertPreTrainedModel)
transformers.modeling_tf_electra.TFElectraPreTrainedModel.get_extended_attention_mask(self,attention_mask,input_shape,dtype)
transformers.modeling_tf_electra.TFElectraPreTrainedModel.get_head_mask(self,head_mask)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/modeling_tf_t5.py----------------------------------------
A:transformers.modeling_tf_t5.logger->utils.logging.get_logger(__name__)
A:transformers.modeling_tf_t5.self.weight->self.add_weight('weight', shape=(input_shape[-1],), initializer='ones')
A:transformers.modeling_tf_t5.variance->tensorflow.math.reduce_mean(tf.math.square(x), axis=-1, keepdims=True)
A:transformers.modeling_tf_t5.self.wi->tensorflow.keras.layers.Dense(config.d_ff, use_bias=False, name='wi')
A:transformers.modeling_tf_t5.self.wo->tensorflow.keras.layers.Dense(config.d_model, use_bias=False, name='wo')
A:transformers.modeling_tf_t5.self.dropout->tensorflow.keras.layers.Dropout(config.dropout_rate)
A:transformers.modeling_tf_t5.h->self.wo(h)
A:transformers.modeling_tf_t5.self.DenseReluDense->TFT5DenseReluDense(config, name='DenseReluDense')
A:transformers.modeling_tf_t5.self.layer_norm->TFT5LayerNorm(epsilon=config.layer_norm_epsilon, name='layer_norm')
A:transformers.modeling_tf_t5.norm_x->self.layer_norm(hidden_states)
A:transformers.modeling_tf_t5.y->self.DenseReluDense(norm_x, training=training)
A:transformers.modeling_tf_t5.NEW_ID->itertools.count()
A:transformers.modeling_tf_t5.self.layer_id->next(TFT5Attention.NEW_ID)
A:transformers.modeling_tf_t5.self.q->tensorflow.keras.layers.Dense(self.inner_dim, use_bias=False, name='q')
A:transformers.modeling_tf_t5.self.k->tensorflow.keras.layers.Dense(self.inner_dim, use_bias=False, name='k')
A:transformers.modeling_tf_t5.self.v->tensorflow.keras.layers.Dense(self.inner_dim, use_bias=False, name='v')
A:transformers.modeling_tf_t5.self.o->tensorflow.keras.layers.Dense(self.d_model, use_bias=False, name='o')
A:transformers.modeling_tf_t5.self.relative_attention_bias->tensorflow.keras.layers.Embedding(self.relative_attention_num_buckets, self.n_heads, name='relative_attention_bias')
A:transformers.modeling_tf_t5.self.pruned_heads->set()
A:transformers.modeling_tf_t5.n->tensorflow.math.maximum(n, 0)
A:transformers.modeling_tf_t5.is_small->tensorflow.math.less(n, max_exact)
A:transformers.modeling_tf_t5.val_if_large->tensorflow.math.minimum(val_if_large, num_buckets - 1)
A:transformers.modeling_tf_t5.rp_bucket->self._relative_position_bucket(relative_position, bidirectional=not self.is_decoder, num_buckets=self.relative_attention_num_buckets)
A:transformers.modeling_tf_t5.values->tensorflow.expand_dims(tf.transpose(values, [2, 0, 1]), axis=0)
A:transformers.modeling_tf_t5.(bs, qlen, dim)->shape_list(input)
A:transformers.modeling_tf_t5.q->shape(self.q(input))
A:transformers.modeling_tf_t5.k->tensorflow.concat([k_, k], axis=2)
A:transformers.modeling_tf_t5.v->tensorflow.concat([v_, v], axis=2)
A:transformers.modeling_tf_t5.scores->tensorflow.einsum('bnqd,bnkd->bnqk', q, k)
A:transformers.modeling_tf_t5.position_bias->self.compute_bias(real_qlen, klen)
A:transformers.modeling_tf_t5.weights->self.dropout(weights, training=training)
A:transformers.modeling_tf_t5.context->self.o(context)
A:transformers.modeling_tf_t5.self.SelfAttention->TFT5Attention(config, has_relative_attention_bias=has_relative_attention_bias, name='SelfAttention')
A:transformers.modeling_tf_t5.attention_output->self.EncDecAttention(norm_x, mask=attention_mask, kv=kv, position_bias=position_bias, head_mask=head_mask, past_key_value_state=past_key_value_state, query_length=query_length, use_cache=use_cache, output_attentions=output_attentions, training=training)
A:transformers.modeling_tf_t5.self.EncDecAttention->TFT5Attention(config, has_relative_attention_bias=has_relative_attention_bias, name='EncDecAttention')
A:transformers.modeling_tf_t5.error_message->'There should be {} past states. 2 (past / key) for self attention.{} Got {} past key / value states'.format(expected_num_past_key_values, '2 (past / key) for cross attention' if expected_num_past_key_values == 4 else '', len(past_key_value_state))
A:transformers.modeling_tf_t5.self_attention_outputs->self.layer[0](hidden_states, attention_mask=attention_mask, position_bias=position_bias, head_mask=head_mask, past_key_value_state=self_attn_past_key_value_state, use_cache=use_cache, output_attentions=output_attentions, training=training)
A:transformers.modeling_tf_t5.cross_attention_outputs->self.layer[1](hidden_states, kv=encoder_hidden_states, attention_mask=encoder_attention_mask, position_bias=encoder_decoder_position_bias, head_mask=head_mask, past_key_value_state=cross_attn_past_key_value_state, query_length=query_length, use_cache=use_cache, output_attentions=output_attentions, training=training)
A:transformers.modeling_tf_t5.hidden_states->self.dropout(hidden_states, training=training)
A:transformers.modeling_tf_t5.self.final_layer_norm->TFT5LayerNorm(epsilon=config.layer_norm_epsilon, name='final_layer_norm')
A:transformers.modeling_tf_t5.input_ids->tensorflow.constant(DUMMY_INPUTS).get('input_ids')
A:transformers.modeling_tf_t5.attention_mask->tensorflow.constant(DUMMY_INPUTS).get('attention_mask', attention_mask)
A:transformers.modeling_tf_t5.encoder_hidden_states->tensorflow.constant(DUMMY_INPUTS).get('encoder_hidden_states', encoder_hidden_states)
A:transformers.modeling_tf_t5.encoder_attention_mask->tensorflow.cast(encoder_attention_mask, dtype=tf.float32)
A:transformers.modeling_tf_t5.inputs_embeds->tensorflow.constant(DUMMY_INPUTS).get('inputs_embeds', inputs_embeds)
A:transformers.modeling_tf_t5.head_mask->tensorflow.constant(DUMMY_INPUTS).get('head_mask', head_mask)
A:transformers.modeling_tf_t5.past_key_values->kwargs.pop('past_key_value_states')
A:transformers.modeling_tf_t5.use_cache->tensorflow.constant(DUMMY_INPUTS).get('use_cache', use_cache)
A:transformers.modeling_tf_t5.output_attentions->tensorflow.constant(DUMMY_INPUTS).get('output_attentions', output_attentions)
A:transformers.modeling_tf_t5.output_hidden_states->tensorflow.constant(DUMMY_INPUTS).get('output_hidden_states', output_hidden_states)
A:transformers.modeling_tf_t5.input_shape->shape_list(input_ids)
A:transformers.modeling_tf_t5.num_dims_attention_mask->len(shape_list(attention_mask))
A:transformers.modeling_tf_t5.seq_ids->tensorflow.range(mask_seq_length)
A:transformers.modeling_tf_t5.causal_mask->tensorflow.cast(causal_mask, dtype=tf.float32)
A:transformers.modeling_tf_t5.num_dims_encoder_attention_mask->len(shape_list(encoder_attention_mask))
A:transformers.modeling_tf_t5.layer_outputs->layer_module(hidden_states, attention_mask=extended_attention_mask, position_bias=position_bias, encoder_hidden_states=encoder_hidden_states, encoder_attention_mask=encoder_extended_attention_mask, encoder_decoder_position_bias=encoder_decoder_position_bias, head_mask=head_mask[i], past_key_value_state=past_key_value_state, use_cache=use_cache, output_attentions=output_attentions, training=training)
A:transformers.modeling_tf_t5.inputs->tensorflow.constant(DUMMY_INPUTS)
A:transformers.modeling_tf_t5.input_mask->tensorflow.constant(DUMMY_MASK)
A:transformers.modeling_tf_t5.shifted_input_ids->tensorflow.identity(shifted_input_ids)
A:transformers.modeling_tf_t5.start_tokens->tensorflow.fill((shape_list(shifted_input_ids)[0], 1), decoder_start_token_id)
A:transformers.modeling_tf_t5.assert_gte0->tensorflow.debugging.assert_greater_equal(shifted_input_ids, tf.cast(0, tf.int32))
A:transformers.modeling_tf_t5.self.shared->TFSharedEmbeddings(config.vocab_size, config.d_model, name='shared')
A:transformers.modeling_tf_t5.embed_tokens->self.get_output_embeddings()
A:transformers.modeling_tf_t5.encoder_config->copy.deepcopy(config)
A:transformers.modeling_tf_t5.self.encoder->TFT5MainLayer(encoder_config, embed_tokens, name='encoder')
A:transformers.modeling_tf_t5.decoder_config->copy.deepcopy(config)
A:transformers.modeling_tf_t5.self.decoder->TFT5MainLayer(decoder_config, embed_tokens, name='decoder')
A:transformers.modeling_tf_t5.encoder_outputs->self.encoder([input_ids, attention_mask, None, None, inputs_embeds, head_mask, None, False, output_attentions, output_hidden_states], training=training)
A:transformers.modeling_tf_t5.decoder_input_ids->self._shift_right(labels)
A:transformers.modeling_tf_t5.decoder_attention_mask->tensorflow.constant(DUMMY_INPUTS).get('decoder_attention_mask', decoder_attention_mask)
A:transformers.modeling_tf_t5.decoder_inputs_embeds->tensorflow.constant(DUMMY_INPUTS).get('decoder_inputs_embeds', decoder_inputs_embeds)
A:transformers.modeling_tf_t5.return_dict->tensorflow.constant(DUMMY_INPUTS).get('return_dict', return_dict)
A:transformers.modeling_tf_t5.decoder_outputs->self.decoder([decoder_input_ids, decoder_attention_mask, hidden_states, attention_mask, decoder_inputs_embeds, head_mask, past_key_values, use_cache, output_attentions, output_hidden_states], training=training)
A:transformers.modeling_tf_t5.labels->tensorflow.constant(DUMMY_INPUTS).get('labels', labels)
A:transformers.modeling_tf_t5.logits->embed_tokens(sequence_output, mode='linear')
transformers.TFT5ForConditionalGeneration(self,config,*inputs,**kwargs)
transformers.TFT5ForConditionalGeneration._reorder_cache(self,past,beam_idx)
transformers.TFT5ForConditionalGeneration.call(self,inputs,attention_mask=None,encoder_outputs=None,inputs_embeds=None,head_mask=None,past_key_values=None,decoder_input_ids=None,decoder_attention_mask=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.TFT5ForConditionalGeneration.get_decoder(self)
transformers.TFT5ForConditionalGeneration.get_encoder(self)
transformers.TFT5ForConditionalGeneration.get_input_embeddings(self)
transformers.TFT5ForConditionalGeneration.get_output_embeddings(self)
transformers.TFT5ForConditionalGeneration.prepare_inputs_for_generation(self,inputs,past,attention_mask,use_cache,**kwargs)
transformers.TFT5ForConditionalGeneration.set_input_embeddings(self,new_embeddings)
transformers.TFT5Model(self,config,*inputs,**kwargs)
transformers.TFT5Model.call(self,inputs,attention_mask=None,encoder_outputs=None,inputs_embeds=None,head_mask=None,past_key_values=None,decoder_input_ids=None,decoder_attention_mask=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.TFT5Model.get_decoder(self)
transformers.TFT5Model.get_encoder(self)
transformers.TFT5Model.get_input_embeddings(self)
transformers.TFT5Model.get_output_embeddings(self)
transformers.TFT5Model.set_input_embeddings(self,new_embeddings)
transformers.TFT5PreTrainedModel(TFPreTrainedModel)
transformers.TFT5PreTrainedModel._shift_right(self,input_ids)
transformers.TFT5PreTrainedModel.dummy_inputs(self)
transformers.modeling_tf_t5.TFT5Attention(self,config,has_relative_attention_bias=False,**kwargs)
transformers.modeling_tf_t5.TFT5Attention.__init__(self,config,has_relative_attention_bias=False,**kwargs)
transformers.modeling_tf_t5.TFT5Attention._relative_position_bucket(relative_position,bidirectional=True,num_buckets=32,max_distance=128)
transformers.modeling_tf_t5.TFT5Attention.call(self,input,mask=None,kv=None,position_bias=None,cache=None,past_key_value_state=None,head_mask=None,query_length=None,use_cache=False,training=False,output_attentions=False)
transformers.modeling_tf_t5.TFT5Attention.compute_bias(self,qlen,klen)
transformers.modeling_tf_t5.TFT5Attention.prune_heads(self,heads)
transformers.modeling_tf_t5.TFT5Block(self,config,has_relative_attention_bias=False,**kwargs)
transformers.modeling_tf_t5.TFT5Block.__init__(self,config,has_relative_attention_bias=False,**kwargs)
transformers.modeling_tf_t5.TFT5Block.call(self,hidden_states,attention_mask=None,position_bias=None,encoder_hidden_states=None,encoder_attention_mask=None,encoder_decoder_position_bias=None,head_mask=None,past_key_value_state=None,use_cache=False,output_attentions=False,training=False)
transformers.modeling_tf_t5.TFT5DenseReluDense(self,config,**kwargs)
transformers.modeling_tf_t5.TFT5DenseReluDense.__init__(self,config,**kwargs)
transformers.modeling_tf_t5.TFT5DenseReluDense.call(self,hidden_states,training=False)
transformers.modeling_tf_t5.TFT5ForConditionalGeneration(self,config,*inputs,**kwargs)
transformers.modeling_tf_t5.TFT5ForConditionalGeneration.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_t5.TFT5ForConditionalGeneration._reorder_cache(self,past,beam_idx)
transformers.modeling_tf_t5.TFT5ForConditionalGeneration.call(self,inputs,attention_mask=None,encoder_outputs=None,inputs_embeds=None,head_mask=None,past_key_values=None,decoder_input_ids=None,decoder_attention_mask=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False,**kwargs)
transformers.modeling_tf_t5.TFT5ForConditionalGeneration.get_decoder(self)
transformers.modeling_tf_t5.TFT5ForConditionalGeneration.get_encoder(self)
transformers.modeling_tf_t5.TFT5ForConditionalGeneration.get_input_embeddings(self)
transformers.modeling_tf_t5.TFT5ForConditionalGeneration.get_output_embeddings(self)
transformers.modeling_tf_t5.TFT5ForConditionalGeneration.prepare_inputs_for_generation(self,inputs,past,attention_mask,use_cache,**kwargs)
transformers.modeling_tf_t5.TFT5ForConditionalGeneration.set_input_embeddings(self,new_embeddings)
transformers.modeling_tf_t5.TFT5LayerCrossAttention(self,config,has_relative_attention_bias=False,**kwargs)
transformers.modeling_tf_t5.TFT5LayerCrossAttention.__init__(self,config,has_relative_attention_bias=False,**kwargs)
transformers.modeling_tf_t5.TFT5LayerCrossAttention.call(self,hidden_states,kv,attention_mask=None,position_bias=None,head_mask=None,past_key_value_state=None,query_length=None,use_cache=False,output_attentions=False,training=False)
transformers.modeling_tf_t5.TFT5LayerFF(self,config,**kwargs)
transformers.modeling_tf_t5.TFT5LayerFF.__init__(self,config,**kwargs)
transformers.modeling_tf_t5.TFT5LayerFF.call(self,hidden_states,training=False)
transformers.modeling_tf_t5.TFT5LayerNorm(self,epsilon=1e-06,**kwargs)
transformers.modeling_tf_t5.TFT5LayerNorm.__init__(self,epsilon=1e-06,**kwargs)
transformers.modeling_tf_t5.TFT5LayerNorm.build(self,input_shape)
transformers.modeling_tf_t5.TFT5LayerNorm.call(self,x)
transformers.modeling_tf_t5.TFT5LayerSelfAttention(self,config,has_relative_attention_bias=False,**kwargs)
transformers.modeling_tf_t5.TFT5LayerSelfAttention.__init__(self,config,has_relative_attention_bias=False,**kwargs)
transformers.modeling_tf_t5.TFT5LayerSelfAttention.call(self,hidden_states,attention_mask=None,position_bias=None,head_mask=None,past_key_value_state=None,use_cache=False,output_attentions=False,training=False)
transformers.modeling_tf_t5.TFT5MainLayer(self,config,embed_tokens=None,**kwargs)
transformers.modeling_tf_t5.TFT5MainLayer.__init__(self,config,embed_tokens=None,**kwargs)
transformers.modeling_tf_t5.TFT5MainLayer._prune_heads(self,heads_to_prune)
transformers.modeling_tf_t5.TFT5MainLayer._resize_token_embeddings(self,new_num_tokens)
transformers.modeling_tf_t5.TFT5MainLayer.call(self,inputs,attention_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,inputs_embeds=None,head_mask=None,past_key_values=None,use_cache=None,output_attentions=None,output_hidden_states=None,training=False,**kwargs)
transformers.modeling_tf_t5.TFT5MainLayer.get_input_embeddings(self)
transformers.modeling_tf_t5.TFT5MainLayer.get_output_embeddings(self)
transformers.modeling_tf_t5.TFT5MainLayer.set_embed_tokens(self,embed_tokens)
transformers.modeling_tf_t5.TFT5Model(self,config,*inputs,**kwargs)
transformers.modeling_tf_t5.TFT5Model.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_t5.TFT5Model.call(self,inputs,attention_mask=None,encoder_outputs=None,inputs_embeds=None,head_mask=None,past_key_values=None,decoder_input_ids=None,decoder_attention_mask=None,decoder_inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False,**kwargs)
transformers.modeling_tf_t5.TFT5Model.get_decoder(self)
transformers.modeling_tf_t5.TFT5Model.get_encoder(self)
transformers.modeling_tf_t5.TFT5Model.get_input_embeddings(self)
transformers.modeling_tf_t5.TFT5Model.get_output_embeddings(self)
transformers.modeling_tf_t5.TFT5Model.set_input_embeddings(self,new_embeddings)
transformers.modeling_tf_t5.TFT5PreTrainedModel(TFPreTrainedModel)
transformers.modeling_tf_t5.TFT5PreTrainedModel._shift_right(self,input_ids)
transformers.modeling_tf_t5.TFT5PreTrainedModel.dummy_inputs(self)
transformers.modeling_tf_t5._NoLayerEmbedTokens(self,layer,abs_scope_name=None)
transformers.modeling_tf_t5._NoLayerEmbedTokens.__init__(self,layer,abs_scope_name=None)
transformers.modeling_tf_t5._NoLayerEmbedTokens.call(self,inputs,mode='embedding')


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/modeling_pegasus.py----------------------------------------
transformers.PegasusForConditionalGeneration(BartForConditionalGeneration)
transformers.modeling_pegasus.PegasusForConditionalGeneration(BartForConditionalGeneration)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/modeling_xlm.py----------------------------------------
A:transformers.modeling_xlm.logger->utils.logging.get_logger(__name__)
A:transformers.modeling_xlm.position_enc->numpy.array([[pos / np.power(10000, 2 * (j // 2) / dim) for j in range(dim)] for pos in range(n_pos)])
A:transformers.modeling_xlm.out[:, 0::2]->torch.FloatTensor(np.sin(position_enc[:, 0::2]))
A:transformers.modeling_xlm.out[:, 1::2]->torch.FloatTensor(np.cos(position_enc[:, 1::2]))
A:transformers.modeling_xlm.alen->torch.arange(slen, dtype=torch.long, device=lengths.device)
A:transformers.modeling_xlm.bs->torch.tensor([slen] * bs, device=device).size(0)
A:transformers.modeling_xlm.NEW_ID->itertools.count()
A:transformers.modeling_xlm.self.layer_id->next(MultiHeadAttention.NEW_ID)
A:transformers.modeling_xlm.self.q_lin->prune_linear_layer(self.q_lin, index)
A:transformers.modeling_xlm.self.k_lin->prune_linear_layer(self.k_lin, index)
A:transformers.modeling_xlm.self.v_lin->prune_linear_layer(self.v_lin, index)
A:transformers.modeling_xlm.self.out_lin->prune_linear_layer(self.out_lin, index, dim=1)
A:transformers.modeling_xlm.self.pruned_heads->self.pruned_heads.union(heads)
A:transformers.modeling_xlm.(heads, index)->find_pruneable_heads_and_indices(heads, self.n_heads, attention_head_size, self.pruned_heads)
A:transformers.modeling_xlm.(bs, qlen, dim)->input.size()
A:transformers.modeling_xlm.klen->kv.size(1)
A:transformers.modeling_xlm.q->shape(self.q_lin(input))
A:transformers.modeling_xlm.k->torch.cat([k_, k], dim=2)
A:transformers.modeling_xlm.v->torch.cat([v_, v], dim=2)
A:transformers.modeling_xlm.scores->self.proj.log_prob(x)
A:transformers.modeling_xlm.mask->(mask == 0).view(mask_reshape).expand_as(scores)
A:transformers.modeling_xlm.weights->torch.nn.functional.dropout(weights, p=self.dropout, training=self.training)
A:transformers.modeling_xlm.context->unshape(context)
A:transformers.modeling_xlm.self.lin1->torch.nn.Linear(in_dim, dim_hidden)
A:transformers.modeling_xlm.self.lin2->torch.nn.Linear(dim_hidden, out_dim)
A:transformers.modeling_xlm.x->torch.nn.functional.dropout(x, p=self.dropout, training=self.training)
A:transformers.modeling_xlm.inputs_list->torch.tensor([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])
A:transformers.modeling_xlm.attns_list->torch.tensor([[1, 1, 0, 0, 1], [1, 1, 1, 0, 0], [1, 0, 0, 1, 1]])
A:transformers.modeling_xlm.langs_list->torch.tensor([[1, 1, 0, 0, 1], [1, 1, 1, 0, 0], [1, 0, 0, 1, 1]])
A:transformers.modeling_xlm.self.position_embeddings->torch.nn.Embedding(config.max_position_embeddings, self.dim)
A:transformers.modeling_xlm.self.lang_embeddings->torch.nn.Embedding(self.n_langs, self.dim)
A:transformers.modeling_xlm.self.embeddings->torch.nn.Embedding(self.n_words, self.dim, padding_idx=self.pad_index)
A:transformers.modeling_xlm.self.layer_norm_emb->torch.nn.LayerNorm(self.dim, eps=config.layer_norm_eps)
A:transformers.modeling_xlm.self.attentions->torch.nn.ModuleList()
A:transformers.modeling_xlm.self.layer_norm1->torch.nn.ModuleList()
A:transformers.modeling_xlm.self.ffns->torch.nn.ModuleList()
A:transformers.modeling_xlm.self.layer_norm2->torch.nn.ModuleList()
A:transformers.modeling_xlm.pruned_heads->config.pruned_heads.copy().items()
A:transformers.modeling_xlm.(bs, slen)->torch.cat([input_ids, mask_token], dim=1).size()
A:transformers.modeling_xlm.lengths->torch.tensor([slen] * bs, device=device)
A:transformers.modeling_xlm.(mask, attn_mask)->get_masks(slen, lengths, self.causal, padding_mask=attention_mask)
A:transformers.modeling_xlm.head_mask->self.get_head_mask(head_mask, self.config.n_layers)
A:transformers.modeling_xlm.inputs_embeds->self.embeddings(input_ids)
A:transformers.modeling_xlm.tensor->self.layer_norm2[i](tensor)
A:transformers.modeling_xlm.attn_outputs->self.attentions[i](tensor, attn_mask, cache=cache, head_mask=head_mask[i], output_attentions=output_attentions)
A:transformers.modeling_xlm.attn->torch.nn.functional.dropout(attn, p=self.dropout, training=self.training)
A:transformers.modeling_xlm.self.proj->torch.nn.AdaptiveLogSoftmaxWithLoss(in_features=dim, n_classes=config.n_words, cutoffs=config.asm_cutoffs, div_value=config.asm_div_value, head_bias=True)
A:transformers.modeling_xlm.loss->loss_fct(reshaped_logits, labels)
A:transformers.modeling_xlm.(_, loss)->self.proj(x, y)
A:transformers.modeling_xlm.self.transformer->XLMModel(config)
A:transformers.modeling_xlm.self.pred_layer->XLMPredLayer(config)
A:transformers.modeling_xlm.mask_token->torch.full((effective_batch_size, 1), mask_token_id, dtype=torch.long, device=input_ids.device)
A:transformers.modeling_xlm.input_ids->torch.cat([input_ids, mask_token], dim=1)
A:transformers.modeling_xlm.langs->torch.full_like(input_ids, lang_id)
A:transformers.modeling_xlm.transformer_outputs->self.transformer(input_ids=input_ids, attention_mask=attention_mask, langs=langs, token_type_ids=token_type_ids, position_ids=position_ids, lengths=lengths, cache=cache, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.modeling_xlm.outputs->self.transformer(input_ids, attention_mask=attention_mask, langs=langs, token_type_ids=token_type_ids, position_ids=position_ids, lengths=lengths, cache=cache, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.modeling_xlm.self.sequence_summary->SequenceSummary(config)
A:transformers.modeling_xlm.logits->self.logits_proj(logits)
A:transformers.modeling_xlm.loss_fct->CrossEntropyLoss()
A:transformers.modeling_xlm.self.qa_outputs->SQuADHead(config)
A:transformers.modeling_xlm.(start_logits, end_logits)->self.logits_proj(logits).split(1, dim=-1)
A:transformers.modeling_xlm.start_logits->start_logits.squeeze(-1).squeeze(-1)
A:transformers.modeling_xlm.end_logits->end_logits.squeeze(-1).squeeze(-1)
A:transformers.modeling_xlm.start_positions->start_positions.squeeze(-1).squeeze(-1)
A:transformers.modeling_xlm.end_positions->end_positions.squeeze(-1).squeeze(-1)
A:transformers.modeling_xlm.ignored_index->start_logits.squeeze(-1).squeeze(-1).size(1)
A:transformers.modeling_xlm.start_loss->loss_fct(start_logits, start_positions)
A:transformers.modeling_xlm.end_loss->loss_fct(end_logits, end_positions)
A:transformers.modeling_xlm.self.dropout->torch.nn.Dropout(config.dropout)
A:transformers.modeling_xlm.self.classifier->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.modeling_xlm.sequence_output->self.dropout(sequence_output)
A:transformers.modeling_xlm.active_logits->self.logits_proj(logits).view(-1, self.num_labels)
A:transformers.modeling_xlm.active_labels->torch.where(active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels))
A:transformers.modeling_xlm.self.logits_proj->torch.nn.Linear(config.num_labels, 1)
A:transformers.modeling_xlm.reshaped_logits->self.logits_proj(logits).view(-1, num_choices)
transformers.XLMForMultipleChoice(self,config,*inputs,**kwargs)
transformers.XLMForMultipleChoice.forward(self,input_ids=None,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.XLMForQuestionAnswering(self,config)
transformers.XLMForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,inputs_embeds=None,start_positions=None,end_positions=None,is_impossible=None,cls_index=None,p_mask=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.XLMForQuestionAnsweringOutput(ModelOutput)
transformers.XLMForQuestionAnsweringSimple(self,config)
transformers.XLMForQuestionAnsweringSimple.forward(self,input_ids=None,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,inputs_embeds=None,start_positions=None,end_positions=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.XLMForSequenceClassification(self,config)
transformers.XLMForSequenceClassification.forward(self,input_ids=None,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.XLMForTokenClassification(self,config)
transformers.XLMForTokenClassification.forward(self,input_ids=None,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.XLMModel(self,config)
transformers.XLMModel._prune_heads(self,heads_to_prune)
transformers.XLMModel.forward(self,input_ids=None,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.XLMModel.get_input_embeddings(self)
transformers.XLMModel.set_input_embeddings(self,new_embeddings)
transformers.XLMPreTrainedModel(self,*inputs,**kwargs)
transformers.XLMPreTrainedModel._init_weights(self,module)
transformers.XLMPreTrainedModel.dummy_inputs(self)
transformers.XLMWithLMHeadModel(self,config)
transformers.XLMWithLMHeadModel.forward(self,input_ids=None,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.XLMWithLMHeadModel.get_output_embeddings(self)
transformers.XLMWithLMHeadModel.prepare_inputs_for_generation(self,input_ids,**kwargs)
transformers.modeling_xlm.MultiHeadAttention(self,n_heads,dim,config)
transformers.modeling_xlm.MultiHeadAttention.__init__(self,n_heads,dim,config)
transformers.modeling_xlm.MultiHeadAttention.forward(self,input,mask,kv=None,cache=None,head_mask=None,output_attentions=False)
transformers.modeling_xlm.MultiHeadAttention.prune_heads(self,heads)
transformers.modeling_xlm.TransformerFFN(self,in_dim,dim_hidden,out_dim,config)
transformers.modeling_xlm.TransformerFFN.__init__(self,in_dim,dim_hidden,out_dim,config)
transformers.modeling_xlm.TransformerFFN.ff_chunk(self,input)
transformers.modeling_xlm.TransformerFFN.forward(self,input)
transformers.modeling_xlm.XLMForMultipleChoice(self,config,*inputs,**kwargs)
transformers.modeling_xlm.XLMForMultipleChoice.__init__(self,config,*inputs,**kwargs)
transformers.modeling_xlm.XLMForMultipleChoice.forward(self,input_ids=None,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.modeling_xlm.XLMForQuestionAnswering(self,config)
transformers.modeling_xlm.XLMForQuestionAnswering.__init__(self,config)
transformers.modeling_xlm.XLMForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,inputs_embeds=None,start_positions=None,end_positions=None,is_impossible=None,cls_index=None,p_mask=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.modeling_xlm.XLMForQuestionAnsweringOutput(ModelOutput)
transformers.modeling_xlm.XLMForQuestionAnsweringSimple(self,config)
transformers.modeling_xlm.XLMForQuestionAnsweringSimple.__init__(self,config)
transformers.modeling_xlm.XLMForQuestionAnsweringSimple.forward(self,input_ids=None,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,inputs_embeds=None,start_positions=None,end_positions=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.modeling_xlm.XLMForSequenceClassification(self,config)
transformers.modeling_xlm.XLMForSequenceClassification.__init__(self,config)
transformers.modeling_xlm.XLMForSequenceClassification.forward(self,input_ids=None,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.modeling_xlm.XLMForTokenClassification(self,config)
transformers.modeling_xlm.XLMForTokenClassification.__init__(self,config)
transformers.modeling_xlm.XLMForTokenClassification.forward(self,input_ids=None,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.modeling_xlm.XLMModel(self,config)
transformers.modeling_xlm.XLMModel.__init__(self,config)
transformers.modeling_xlm.XLMModel._prune_heads(self,heads_to_prune)
transformers.modeling_xlm.XLMModel.forward(self,input_ids=None,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.modeling_xlm.XLMModel.get_input_embeddings(self)
transformers.modeling_xlm.XLMModel.set_input_embeddings(self,new_embeddings)
transformers.modeling_xlm.XLMPreTrainedModel(self,*inputs,**kwargs)
transformers.modeling_xlm.XLMPreTrainedModel.__init__(self,*inputs,**kwargs)
transformers.modeling_xlm.XLMPreTrainedModel._init_weights(self,module)
transformers.modeling_xlm.XLMPreTrainedModel.dummy_inputs(self)
transformers.modeling_xlm.XLMPredLayer(self,config)
transformers.modeling_xlm.XLMPredLayer.__init__(self,config)
transformers.modeling_xlm.XLMPredLayer.forward(self,x,y=None)
transformers.modeling_xlm.XLMWithLMHeadModel(self,config)
transformers.modeling_xlm.XLMWithLMHeadModel.__init__(self,config)
transformers.modeling_xlm.XLMWithLMHeadModel.forward(self,input_ids=None,attention_mask=None,langs=None,token_type_ids=None,position_ids=None,lengths=None,cache=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.modeling_xlm.XLMWithLMHeadModel.get_output_embeddings(self)
transformers.modeling_xlm.XLMWithLMHeadModel.prepare_inputs_for_generation(self,input_ids,**kwargs)
transformers.modeling_xlm.create_sinusoidal_embeddings(n_pos,dim,out)
transformers.modeling_xlm.get_masks(slen,lengths,causal,padding_mask=None)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/convert_graph_to_onnx.py----------------------------------------
A:transformers.convert_graph_to_onnx.ORT_QUANTIZE_MINIMUM_VERSION->parse('1.4.0')
A:transformers.convert_graph_to_onnx.ort_version->parse(onnxruntime.__version__)
A:transformers.convert_graph_to_onnx.tokens->load_graph_from_args(pipeline_name, framework, model, tokenizer).tokenizer('This is a sample output', return_tensors=framework)
A:transformers.convert_graph_to_onnx.outputs->outputs.to_tuple().to_tuple()
A:transformers.convert_graph_to_onnx.input_vars->list(tokens.keys())
A:transformers.convert_graph_to_onnx.dynamic_axes->dict(input_dynamic_axes, **output_dynamic_axes)
A:transformers.convert_graph_to_onnx.(input_names, output_names, dynamic_axes, tokens)->infer_shapes(nlp, 'tf')
A:transformers.convert_graph_to_onnx.(ordered_input_names, model_args)->ensure_valid_input(nlp.model, tokens, input_names)
A:transformers.convert_graph_to_onnx.onnx_model->onnx.load(onnx_model_path.as_posix())
A:transformers.convert_graph_to_onnx.nlp->load_graph_from_args(pipeline_name, framework, model, tokenizer)
A:transformers.convert_graph_to_onnx.opt_model_path->generate_identified_filename(onnx_model_path, '-optimized')
A:transformers.convert_graph_to_onnx.sess_option->SessionOptions()
A:transformers.convert_graph_to_onnx.sess_option.optimized_model_filepath->generate_identified_filename(onnx_model_path, '-optimized').as_posix()
A:transformers.convert_graph_to_onnx._->InferenceSession(path.as_posix(), onnx_options, providers=['CPUExecutionProvider'])
A:transformers.convert_graph_to_onnx.quantized_model->quantize(model=onnx_model, quantization_mode=QuantizationMode.IntegerOps, force_fusions=True, symmetric_weight=True)
A:transformers.convert_graph_to_onnx.quantized_model_path->generate_identified_filename(onnx_model_path, '-quantized')
A:transformers.convert_graph_to_onnx.onnx_options->SessionOptions()
A:transformers.convert_graph_to_onnx.parser->OnnxConverterArgumentParser()
A:transformers.convert_graph_to_onnx.args->OnnxConverterArgumentParser().parse_args()
A:transformers.convert_graph_to_onnx.args.output->Path(args.output).absolute()
A:transformers.convert_graph_to_onnx.args.optimized_output->optimize(args.output)
A:transformers.convert_graph_to_onnx.args.quantized_output->quantize(args.optimized_output)
transformers.convert_graph_to_onnx.OnnxConverterArgumentParser(self)
transformers.convert_graph_to_onnx.OnnxConverterArgumentParser.__init__(self)
transformers.convert_graph_to_onnx.check_onnxruntime_requirements(minimum_version:Version)
transformers.convert_graph_to_onnx.convert(framework:str,model:str,output:Path,opset:int,tokenizer:Optional[str]=None,use_external_format:bool=False,pipeline_name:str='feature-extraction')
transformers.convert_graph_to_onnx.convert_pytorch(nlp:Pipeline,opset:int,output:Path,use_external_format:bool)
transformers.convert_graph_to_onnx.convert_tensorflow(nlp:Pipeline,opset:int,output:Path)
transformers.convert_graph_to_onnx.ensure_valid_input(model,tokens,input_names)
transformers.convert_graph_to_onnx.generate_identified_filename(filename:Path,identifier:str)->Path
transformers.convert_graph_to_onnx.infer_shapes(nlp:Pipeline,framework:str)->Tuple[List[str], List[str], Dict, BatchEncoding]
transformers.convert_graph_to_onnx.load_graph_from_args(pipeline_name:str,framework:str,model:str,tokenizer:Optional[str]=None)->Pipeline
transformers.convert_graph_to_onnx.optimize(onnx_model_path:Path)->Path
transformers.convert_graph_to_onnx.quantize(onnx_model_path:Path)->Path
transformers.convert_graph_to_onnx.verify(path:Path)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/tokenization_flaubert.py----------------------------------------
A:transformers.tokenization_flaubert.logger->utils.logging.get_logger(__name__)
A:transformers.tokenization_flaubert.text->self.moses_tokenize(text, lang=lang)
transformers.FlaubertTokenizer(self,do_lowercase=False,**kwargs)
transformers.FlaubertTokenizer._tokenize(self,text,bypass_tokenizer=False)
transformers.FlaubertTokenizer.preprocess_text(self,text)
transformers.tokenization_flaubert.FlaubertTokenizer(self,do_lowercase=False,**kwargs)
transformers.tokenization_flaubert.FlaubertTokenizer.__init__(self,do_lowercase=False,**kwargs)
transformers.tokenization_flaubert.FlaubertTokenizer._tokenize(self,text,bypass_tokenizer=False)
transformers.tokenization_flaubert.FlaubertTokenizer.preprocess_text(self,text)
transformers.tokenization_flaubert.convert_to_unicode(text)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/modeling_bart.py----------------------------------------
A:transformers.modeling_bart.logger->utils.logging.get_logger(__name__)
A:transformers.modeling_bart.decoder_input_ids->shift_tokens_right(labels, self.config.pad_token_id)
A:transformers.modeling_bart.(bsz, tgt_len)->shift_tokens_right(labels, self.config.pad_token_id).size()
A:transformers.modeling_bart.decoder_padding_mask->invert_mask(decoder_padding_mask)
A:transformers.modeling_bart.causal_mask->torch.triu(fill_with_neg_inf(torch.zeros(tgt_len, tgt_len)), 1).to(dtype=causal_mask_dtype, device=decoder_input_ids.device)
A:transformers.modeling_bart.input_ids->torch.tensor([[0, 6, 10, 4, 2], [0, 8, 12, 2, pad_token]], device=self.device)
A:transformers.modeling_bart.lin_layer->torch.nn.Linear(vocab_size, emb_size, bias=False)
A:transformers.modeling_bart.prev_output_tokens->torch.tensor([[0, 6, 10, 4, 2], [0, 8, 12, 2, pad_token]], device=self.device).clone()
A:transformers.modeling_bart.index_of_eos->(input_ids.ne(pad_token_id).sum(dim=1) - 1).unsqueeze(-1)
A:transformers.modeling_bart.prev_output_tokens[:, 0]->torch.tensor([[0, 6, 10, 4, 2], [0, 8, 12, 2, pad_token]], device=self.device).gather(1, index_of_eos).squeeze()
A:transformers.modeling_bart.padding_mask->torch.tensor([[0, 6, 10, 4, 2], [0, 8, 12, 2, pad_token]], device=self.device).eq(padding_idx)
A:transformers.modeling_bart.self.self_attn->Attention(embed_dim=self.embed_dim, num_heads=config.decoder_attention_heads, dropout=config.attention_dropout)
A:transformers.modeling_bart.self.self_attn_layer_norm->LayerNorm(self.embed_dim)
A:transformers.modeling_bart.self.fc1->torch.nn.Linear(self.embed_dim, config.decoder_ffn_dim)
A:transformers.modeling_bart.self.fc2->torch.nn.Linear(config.decoder_ffn_dim, self.embed_dim)
A:transformers.modeling_bart.self.final_layer_norm->LayerNorm(self.embed_dim)
A:transformers.modeling_bart.x->self.out_proj(x)
A:transformers.modeling_bart.(x, attn_weights)->self.self_attn(query=x, key=x, key_padding_mask=encoder_padding_mask, output_attentions=output_attentions)
A:transformers.modeling_bart.self.embed_positions->LearnedPositionalEmbedding(config.max_position_embeddings, config.d_model, self.padding_idx, config.extra_pos_embeddings)
A:transformers.modeling_bart.self.layers->torch.nn.ModuleList([DecoderLayer(config) for _ in range(config.decoder_layers)])
A:transformers.modeling_bart.attention_mask->invert_mask(attention_mask)
A:transformers.modeling_bart.embed_pos->self.embed_positions(input_ids)
A:transformers.modeling_bart.dropout_probability->random.uniform(0, 1)
A:transformers.modeling_bart.(x, attn)->encoder_layer(x, attention_mask, output_attentions=output_attentions)
A:transformers.modeling_bart.encoder_states->tuple((hidden_state.transpose(0, 1) for hidden_state in encoder_states))
A:transformers.modeling_bart.self.encoder_attn->Attention(self.embed_dim, config.decoder_attention_heads, dropout=config.attention_dropout, encoder_decoder_attention=True)
A:transformers.modeling_bart.self.encoder_attn_layer_norm->LayerNorm(self.embed_dim)
A:transformers.modeling_bart.(x, self_attn_weights)->self.self_attn(query=x, key=x, layer_state=layer_state, key_padding_mask=decoder_padding_mask, attn_mask=causal_mask, output_attentions=output_attentions)
A:transformers.modeling_bart.(x, _)->self.encoder_attn(query=x, key=encoder_hidden_states, key_padding_mask=encoder_attn_mask, layer_state=layer_state)
A:transformers.modeling_bart.past_key_values->unused.pop('decoder_past_key_values')
A:transformers.modeling_bart.encoder_padding_mask->invert_mask(encoder_padding_mask)
A:transformers.modeling_bart.positions->torch.arange(seq_len, dtype=torch.long, device=self.weight.device)
A:transformers.modeling_bart.encoder_hidden_states->encoder_hidden_states.transpose(0, 1).transpose(0, 1)
A:transformers.modeling_bart.(x, layer_self_attn, layer_past)->decoder_layer(x, encoder_hidden_states, encoder_attn_mask=encoder_padding_mask, decoder_padding_mask=decoder_padding_mask, layer_state=layer_state, causal_mask=decoder_causal_mask, output_attentions=output_attentions)
A:transformers.modeling_bart.all_hidden_states->tuple((hidden_state.transpose(0, 1) for hidden_state in all_hidden_states))
A:transformers.modeling_bart.attn_cache[k]->input_buffer_k.index_select(0, new_order)
A:transformers.modeling_bart.self.k_proj->torch.nn.Linear(embed_dim, embed_dim, bias=bias)
A:transformers.modeling_bart.self.v_proj->torch.nn.Linear(embed_dim, embed_dim, bias=bias)
A:transformers.modeling_bart.self.q_proj->torch.nn.Linear(embed_dim, embed_dim, bias=bias)
A:transformers.modeling_bart.self.out_proj->torch.nn.Linear(inner_dim, num_classes)
A:transformers.modeling_bart.(tgt_len, bsz, embed_dim)->query.size()
A:transformers.modeling_bart.saved_state->layer_state.get(self.cache_key, {})
A:transformers.modeling_bart.k->torch.cat([prev_key, k], dim=1)
A:transformers.modeling_bart.v->torch.cat([prev_value, v], dim=1)
A:transformers.modeling_bart.q->self._shape(q, tgt_len, bsz)
A:transformers.modeling_bart.(k, v, key_padding_mask)->self._use_saved_state(k, v, saved_state, key_padding_mask, static_kv, bsz)
A:transformers.modeling_bart.src_len->torch.cat([prev_key, k], dim=1).size(1)
A:transformers.modeling_bart.attn_weights->attn_weights.view(bsz, self.num_heads, tgt_len, src_len).view(bsz, self.num_heads, tgt_len, src_len)
A:transformers.modeling_bart.reshaped->key_padding_mask.unsqueeze(1).unsqueeze(2)
A:transformers.modeling_bart.attn_probs->torch.nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)
A:transformers.modeling_bart.attn_output->self.out_proj(attn_output)
A:transformers.modeling_bart.prev_key->_prev_key.view(bsz * self.num_heads, -1, self.head_dim)
A:transformers.modeling_bart.prev_value->_prev_value.view(bsz * self.num_heads, -1, self.head_dim)
A:transformers.modeling_bart.new_key_padding_mask->torch.cat([prev_key_padding_mask, key_padding_mask], dim=1)
A:transformers.modeling_bart.self.dense->torch.nn.Linear(input_dim, inner_dim)
A:transformers.modeling_bart.self.dropout->torch.nn.Dropout(p=pooler_dropout)
A:transformers.modeling_bart.self.shared->torch.nn.Embedding(vocab_size, config.d_model, padding_idx)
A:transformers.modeling_bart.self.encoder->BartEncoder(config, self.shared)
A:transformers.modeling_bart.self.decoder->BartDecoder(config, self.shared)
A:transformers.modeling_bart.(decoder_input_ids, decoder_padding_mask, causal_mask)->_prepare_bart_decoder_inputs(self.config, input_ids, decoder_input_ids=decoder_input_ids, decoder_padding_mask=decoder_attention_mask, causal_mask_dtype=self.shared.weight.dtype)
A:transformers.modeling_bart.encoder_outputs->BaseModelOutput(last_hidden_state=encoder_outputs[0], hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None, attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None)
A:transformers.modeling_bart.decoder_outputs->self.decoder(decoder_input_ids, encoder_outputs[0], attention_mask, decoder_padding_mask, decoder_causal_mask=causal_mask, past_key_values=past_key_values, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.modeling_bart.base_model->BartModel(config)
A:transformers.modeling_bart.new_embeddings->super().resize_token_embeddings(new_num_tokens)
A:transformers.modeling_bart.extra_bias->torch.zeros((1, new_num_tokens - old_num_tokens), device=self.final_logits_bias.device)
A:transformers.modeling_bart.new_bias->torch.cat([self.final_logits_bias, extra_bias], dim=1)
A:transformers.modeling_bart.labels->unused.pop('lm_labels')
A:transformers.modeling_bart.outputs->self.model(input_ids, attention_mask=attention_mask, decoder_input_ids=decoder_input_ids, decoder_attention_mask=decoder_attention_mask, encoder_outputs=encoder_outputs, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.modeling_bart.lm_logits->torch.nn.functional.linear(outputs[0], self.model.shared.weight, bias=self.final_logits_bias)
A:transformers.modeling_bart.loss_fct->CrossEntropyLoss(ignore_index=ignored_index)
A:transformers.modeling_bart.masked_lm_loss->loss_fct(lm_logits.view(-1, self.config.vocab_size), labels.view(-1))
A:transformers.modeling_bart.self.model->BartModel(config)
A:transformers.modeling_bart.self.classification_head->BartClassificationHead(config.d_model, config.d_model, config.num_labels, config.classif_dropout)
A:transformers.modeling_bart.eos_mask->torch.tensor([[0, 6, 10, 4, 2], [0, 8, 12, 2, pad_token]], device=self.device).eq(self.config.eos_token_id)
A:transformers.modeling_bart.logits->self.qa_outputs(sequence_output)
A:transformers.modeling_bart.loss->loss_fct(logits.view(-1, self.config.num_labels), labels.view(-1))
A:transformers.modeling_bart.self.qa_outputs->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.modeling_bart.(start_logits, end_logits)->self.qa_outputs(sequence_output).split(1, dim=-1)
A:transformers.modeling_bart.start_logits->start_logits.squeeze(-1).squeeze(-1)
A:transformers.modeling_bart.end_logits->end_logits.squeeze(-1).squeeze(-1)
A:transformers.modeling_bart.start_positions->start_positions.squeeze(-1).squeeze(-1)
A:transformers.modeling_bart.end_positions->end_positions.squeeze(-1).squeeze(-1)
A:transformers.modeling_bart.ignored_index->start_logits.squeeze(-1).squeeze(-1).size(1)
A:transformers.modeling_bart.start_loss->loss_fct(start_logits, start_positions)
A:transformers.modeling_bart.end_loss->loss_fct(end_logits, end_positions)
A:transformers.modeling_bart.self.weight->self._init_weight(self.weight)
A:transformers.modeling_bart.position_enc->numpy.array([[pos / np.power(10000, 2 * (j // 2) / dim) for j in range(dim)] for pos in range(n_pos)])
A:transformers.modeling_bart.out[:, 0:dim // 2]->torch.FloatTensor(np.sin(position_enc[:, 0::2]))
A:transformers.modeling_bart.out[:, dim // 2:]->torch.FloatTensor(np.cos(position_enc[:, 1::2]))
transformers.BartForConditionalGeneration(self,config:BartConfig)
transformers.BartForConditionalGeneration._force_token_ids_generation(self,scores,token_id)->None
transformers.BartForConditionalGeneration._reorder_cache(past,beam_idx)
transformers.BartForConditionalGeneration._resize_final_logits_bias(self,new_num_tokens:int,old_num_tokens:int)->None
transformers.BartForConditionalGeneration.adjust_logits_during_generation(self,logits,cur_len,max_length)
transformers.BartForConditionalGeneration.forward(self,input_ids,attention_mask=None,encoder_outputs=None,decoder_input_ids=None,decoder_attention_mask=None,past_key_values=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,**unused)
transformers.BartForConditionalGeneration.get_encoder(self)
transformers.BartForConditionalGeneration.get_output_embeddings(self)
transformers.BartForConditionalGeneration.prepare_inputs_for_generation(self,decoder_input_ids,past,attention_mask,use_cache,encoder_outputs,**kwargs)
transformers.BartForConditionalGeneration.resize_token_embeddings(self,new_num_tokens:int)->nn.Embedding
transformers.BartForQuestionAnswering(self,config)
transformers.BartForQuestionAnswering.forward(self,input_ids,attention_mask=None,encoder_outputs=None,decoder_input_ids=None,decoder_attention_mask=None,start_positions=None,end_positions=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.BartForSequenceClassification(self,config:BartConfig,**kwargs)
transformers.BartForSequenceClassification.forward(self,input_ids,attention_mask=None,encoder_outputs=None,decoder_input_ids=None,decoder_attention_mask=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.BartModel(self,config:BartConfig)
transformers.BartModel.forward(self,input_ids,attention_mask=None,decoder_input_ids=None,encoder_outputs:Optional[Tuple]=None,decoder_attention_mask=None,past_key_values=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.BartModel.get_input_embeddings(self)
transformers.BartModel.get_output_embeddings(self)
transformers.BartModel.set_input_embeddings(self,value)
transformers.PretrainedBartModel(PreTrainedModel)
transformers.PretrainedBartModel._init_weights(self,module)
transformers.PretrainedBartModel.dummy_inputs(self)
transformers.modeling_bart.Attention(self,embed_dim,num_heads,dropout=0.0,bias=True,encoder_decoder_attention=False)
transformers.modeling_bart.Attention.__init__(self,embed_dim,num_heads,dropout=0.0,bias=True,encoder_decoder_attention=False)
transformers.modeling_bart.Attention._shape(self,tensor,seq_len,bsz)
transformers.modeling_bart.Attention._use_saved_state(self,k,v,saved_state,key_padding_mask,static_kv,bsz)
transformers.modeling_bart.Attention.forward(self,query,key:Optional[Tensor],key_padding_mask:Optional[Tensor]=None,layer_state:Optional[Dict[str,Optional[Tensor]]]=None,attn_mask:Optional[Tensor]=None,output_attentions=False)->Tuple[Tensor, Optional[Tensor]]
transformers.modeling_bart.BartClassificationHead(self,input_dim,inner_dim,num_classes,pooler_dropout)
transformers.modeling_bart.BartClassificationHead.__init__(self,input_dim,inner_dim,num_classes,pooler_dropout)
transformers.modeling_bart.BartClassificationHead.forward(self,x)
transformers.modeling_bart.BartDecoder(self,config:BartConfig,embed_tokens:nn.Embedding)
transformers.modeling_bart.BartDecoder.__init__(self,config:BartConfig,embed_tokens:nn.Embedding)
transformers.modeling_bart.BartDecoder.forward(self,input_ids,encoder_hidden_states,encoder_padding_mask,decoder_padding_mask,decoder_causal_mask,past_key_values=None,use_cache=False,output_attentions=False,output_hidden_states=False,return_dict=False,**unused)
transformers.modeling_bart.BartEncoder(self,config:BartConfig,embed_tokens)
transformers.modeling_bart.BartEncoder.__init__(self,config:BartConfig,embed_tokens)
transformers.modeling_bart.BartEncoder.forward(self,input_ids,attention_mask=None,output_attentions=False,output_hidden_states=False,return_dict=False)
transformers.modeling_bart.BartForConditionalGeneration(self,config:BartConfig)
transformers.modeling_bart.BartForConditionalGeneration.__init__(self,config:BartConfig)
transformers.modeling_bart.BartForConditionalGeneration._force_token_ids_generation(self,scores,token_id)->None
transformers.modeling_bart.BartForConditionalGeneration._reorder_cache(past,beam_idx)
transformers.modeling_bart.BartForConditionalGeneration._resize_final_logits_bias(self,new_num_tokens:int,old_num_tokens:int)->None
transformers.modeling_bart.BartForConditionalGeneration.adjust_logits_during_generation(self,logits,cur_len,max_length)
transformers.modeling_bart.BartForConditionalGeneration.forward(self,input_ids,attention_mask=None,encoder_outputs=None,decoder_input_ids=None,decoder_attention_mask=None,past_key_values=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,**unused)
transformers.modeling_bart.BartForConditionalGeneration.get_encoder(self)
transformers.modeling_bart.BartForConditionalGeneration.get_output_embeddings(self)
transformers.modeling_bart.BartForConditionalGeneration.prepare_inputs_for_generation(self,decoder_input_ids,past,attention_mask,use_cache,encoder_outputs,**kwargs)
transformers.modeling_bart.BartForConditionalGeneration.resize_token_embeddings(self,new_num_tokens:int)->nn.Embedding
transformers.modeling_bart.BartForQuestionAnswering(self,config)
transformers.modeling_bart.BartForQuestionAnswering.__init__(self,config)
transformers.modeling_bart.BartForQuestionAnswering.forward(self,input_ids,attention_mask=None,encoder_outputs=None,decoder_input_ids=None,decoder_attention_mask=None,start_positions=None,end_positions=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.modeling_bart.BartForSequenceClassification(self,config:BartConfig,**kwargs)
transformers.modeling_bart.BartForSequenceClassification.__init__(self,config:BartConfig,**kwargs)
transformers.modeling_bart.BartForSequenceClassification.forward(self,input_ids,attention_mask=None,encoder_outputs=None,decoder_input_ids=None,decoder_attention_mask=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.modeling_bart.BartModel(self,config:BartConfig)
transformers.modeling_bart.BartModel.__init__(self,config:BartConfig)
transformers.modeling_bart.BartModel.forward(self,input_ids,attention_mask=None,decoder_input_ids=None,encoder_outputs:Optional[Tuple]=None,decoder_attention_mask=None,past_key_values=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.modeling_bart.BartModel.get_input_embeddings(self)
transformers.modeling_bart.BartModel.get_output_embeddings(self)
transformers.modeling_bart.BartModel.set_input_embeddings(self,value)
transformers.modeling_bart.DecoderLayer(self,config:BartConfig)
transformers.modeling_bart.DecoderLayer.__init__(self,config:BartConfig)
transformers.modeling_bart.DecoderLayer.forward(self,x,encoder_hidden_states,encoder_attn_mask=None,layer_state=None,causal_mask=None,decoder_padding_mask=None,output_attentions=False)
transformers.modeling_bart.EncoderLayer(self,config:BartConfig)
transformers.modeling_bart.EncoderLayer.__init__(self,config:BartConfig)
transformers.modeling_bart.EncoderLayer.forward(self,x,encoder_padding_mask,output_attentions=False)
transformers.modeling_bart.LayerNorm(normalized_shape,eps=1e-05,elementwise_affine=True)
transformers.modeling_bart.LearnedPositionalEmbedding(self,num_embeddings:int,embedding_dim:int,padding_idx:int,offset)
transformers.modeling_bart.LearnedPositionalEmbedding.__init__(self,num_embeddings:int,embedding_dim:int,padding_idx:int,offset)
transformers.modeling_bart.LearnedPositionalEmbedding.forward(self,input_ids,use_cache=False)
transformers.modeling_bart.PretrainedBartModel(PreTrainedModel)
transformers.modeling_bart.PretrainedBartModel._init_weights(self,module)
transformers.modeling_bart.PretrainedBartModel.dummy_inputs(self)
transformers.modeling_bart.SinusoidalPositionalEmbedding(self,num_positions,embedding_dim,padding_idx=None)
transformers.modeling_bart.SinusoidalPositionalEmbedding.__init__(self,num_positions,embedding_dim,padding_idx=None)
transformers.modeling_bart.SinusoidalPositionalEmbedding._init_weight(out:nn.Parameter)
transformers.modeling_bart.SinusoidalPositionalEmbedding.forward(self,input_ids,use_cache=False)
transformers.modeling_bart._check_shapes(shape_1,shape2)
transformers.modeling_bart._get_shape(t)
transformers.modeling_bart._make_linear_from_emb(emb)
transformers.modeling_bart._prepare_bart_decoder_inputs(config,input_ids,decoder_input_ids=None,decoder_padding_mask=None,causal_mask_dtype=torch.float32)
transformers.modeling_bart._reorder_buffer(attn_cache,new_order)
transformers.modeling_bart.fill_with_neg_inf(t)
transformers.modeling_bart.invert_mask(attention_mask)
transformers.modeling_bart.make_padding_mask(input_ids,padding_idx=1)
transformers.modeling_bart.shift_tokens_right(input_ids,pad_token_id)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/tokenization_camembert.py----------------------------------------
A:transformers.tokenization_camembert.logger->utils.logging.get_logger(__name__)
A:transformers.tokenization_camembert.self.sp_model->sentencepiece.SentencePieceProcessor()
A:transformers.tokenization_camembert.self.fairseq_offset->len(self.fairseq_tokens_to_ids)
A:transformers.tokenization_camembert.state->self.__dict__.copy()
A:transformers.tokenization_camembert.out_string->''.join(tokens).replace(SPIECE_UNDERLINE, ' ').strip()
A:transformers.tokenization_camembert.out_vocab_file->os.path.join(save_directory, VOCAB_FILES_NAMES['vocab_file'])
transformers.CamembertTokenizer(self,vocab_file,bos_token='<s>',eos_token='</s>',sep_token='</s>',cls_token='<s>',unk_token='<unk>',pad_token='<pad>',mask_token='<mask>',additional_special_tokens=['<s>NOTUSED','</s>NOTUSED'],**kwargs)
transformers.CamembertTokenizer.__getstate__(self)
transformers.CamembertTokenizer.__setstate__(self,d)
transformers.CamembertTokenizer._convert_id_to_token(self,index)
transformers.CamembertTokenizer._convert_token_to_id(self,token)
transformers.CamembertTokenizer._tokenize(self,text)
transformers.CamembertTokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.CamembertTokenizer.convert_tokens_to_string(self,tokens)
transformers.CamembertTokenizer.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.CamembertTokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.CamembertTokenizer.save_vocabulary(self,save_directory)
transformers.CamembertTokenizer.vocab_size(self)
transformers.tokenization_camembert.CamembertTokenizer(self,vocab_file,bos_token='<s>',eos_token='</s>',sep_token='</s>',cls_token='<s>',unk_token='<unk>',pad_token='<pad>',mask_token='<mask>',additional_special_tokens=['<s>NOTUSED','</s>NOTUSED'],**kwargs)
transformers.tokenization_camembert.CamembertTokenizer.__getstate__(self)
transformers.tokenization_camembert.CamembertTokenizer.__init__(self,vocab_file,bos_token='<s>',eos_token='</s>',sep_token='</s>',cls_token='<s>',unk_token='<unk>',pad_token='<pad>',mask_token='<mask>',additional_special_tokens=['<s>NOTUSED','</s>NOTUSED'],**kwargs)
transformers.tokenization_camembert.CamembertTokenizer.__setstate__(self,d)
transformers.tokenization_camembert.CamembertTokenizer._convert_id_to_token(self,index)
transformers.tokenization_camembert.CamembertTokenizer._convert_token_to_id(self,token)
transformers.tokenization_camembert.CamembertTokenizer._tokenize(self,text)
transformers.tokenization_camembert.CamembertTokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.tokenization_camembert.CamembertTokenizer.convert_tokens_to_string(self,tokens)
transformers.tokenization_camembert.CamembertTokenizer.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.tokenization_camembert.CamembertTokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.tokenization_camembert.CamembertTokenizer.save_vocabulary(self,save_directory)
transformers.tokenization_camembert.CamembertTokenizer.vocab_size(self)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/modeling_bert.py----------------------------------------
A:transformers.modeling_bert.logger->utils.logging.get_logger(__name__)
A:transformers.modeling_bert.tf_path->os.path.abspath(tf_checkpoint_path)
A:transformers.modeling_bert.init_vars->tensorflow.train.list_variables(tf_path)
A:transformers.modeling_bert.array->numpy.transpose(array)
A:transformers.modeling_bert.name->name.split('/').split('/')
A:transformers.modeling_bert.scope_names->re.split('_(\\d+)', m_name)
A:transformers.modeling_bert.pointer->getattr(pointer, 'weight')
A:transformers.modeling_bert.num->int(scope_names[1])
A:transformers.modeling_bert.pointer.data->torch.from_numpy(array)
A:transformers.modeling_bert.self.word_embeddings->torch.nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)
A:transformers.modeling_bert.self.position_embeddings->torch.nn.Embedding(config.max_position_embeddings, config.hidden_size)
A:transformers.modeling_bert.self.token_type_embeddings->torch.nn.Embedding(config.type_vocab_size, config.hidden_size)
A:transformers.modeling_bert.self.LayerNorm->BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)
A:transformers.modeling_bert.self.dropout->torch.nn.Dropout(config.hidden_dropout_prob)
A:transformers.modeling_bert.input_shape->torch.cat([input_ids, dummy_token], dim=1).size()
A:transformers.modeling_bert.token_type_ids->torch.zeros(input_shape, dtype=torch.long, device=device)
A:transformers.modeling_bert.inputs_embeds->self.word_embeddings(input_ids)
A:transformers.modeling_bert.position_embeddings->self.position_embeddings(position_ids)
A:transformers.modeling_bert.token_type_embeddings->self.token_type_embeddings(token_type_ids)
A:transformers.modeling_bert.embeddings->self.dropout(embeddings)
A:transformers.modeling_bert.self.attention_head_size->int(config.hidden_size / config.num_attention_heads)
A:transformers.modeling_bert.self.query->torch.nn.Linear(config.hidden_size, self.all_head_size)
A:transformers.modeling_bert.self.key->torch.nn.Linear(config.hidden_size, self.all_head_size)
A:transformers.modeling_bert.self.value->torch.nn.Linear(config.hidden_size, self.all_head_size)
A:transformers.modeling_bert.x->x.view(*new_x_shape).view(*new_x_shape)
A:transformers.modeling_bert.mixed_query_layer->self.query(hidden_states)
A:transformers.modeling_bert.mixed_key_layer->self.key(hidden_states)
A:transformers.modeling_bert.mixed_value_layer->self.value(hidden_states)
A:transformers.modeling_bert.query_layer->self.transpose_for_scores(mixed_query_layer)
A:transformers.modeling_bert.key_layer->self.transpose_for_scores(mixed_key_layer)
A:transformers.modeling_bert.value_layer->self.transpose_for_scores(mixed_value_layer)
A:transformers.modeling_bert.attention_scores->torch.matmul(query_layer, key_layer.transpose(-1, -2))
A:transformers.modeling_bert.attention_probs->self.dropout(attention_probs)
A:transformers.modeling_bert.context_layer->context_layer.view(*new_context_layer_shape).view(*new_context_layer_shape)
A:transformers.modeling_bert.self.dense->torch.nn.Linear(config.hidden_size, config.hidden_size)
A:transformers.modeling_bert.hidden_states->self.decoder(hidden_states)
A:transformers.modeling_bert.self.self->BertSelfAttention(config)
A:transformers.modeling_bert.self.output->BertOutput(config)
A:transformers.modeling_bert.self.pruned_heads->self.pruned_heads.union(heads)
A:transformers.modeling_bert.(heads, index)->find_pruneable_heads_and_indices(heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads)
A:transformers.modeling_bert.self.self.query->prune_linear_layer(self.self.query, index)
A:transformers.modeling_bert.self.self.key->prune_linear_layer(self.self.key, index)
A:transformers.modeling_bert.self.self.value->prune_linear_layer(self.self.value, index)
A:transformers.modeling_bert.self.output.dense->prune_linear_layer(self.output.dense, index, dim=1)
A:transformers.modeling_bert.self_outputs->self.self(hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)
A:transformers.modeling_bert.attention_output->self.output(self_outputs[0], hidden_states)
A:transformers.modeling_bert.self.attention->BertAttention(config)
A:transformers.modeling_bert.self.crossattention->BertAttention(config)
A:transformers.modeling_bert.self.intermediate->BertIntermediate(config)
A:transformers.modeling_bert.self_attention_outputs->self.attention(hidden_states, attention_mask, head_mask, output_attentions=output_attentions)
A:transformers.modeling_bert.cross_attention_outputs->self.crossattention(attention_output, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)
A:transformers.modeling_bert.layer_output->self.output(intermediate_output, attention_output)
A:transformers.modeling_bert.intermediate_output->self.intermediate(attention_output)
A:transformers.modeling_bert.self.layer->torch.nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])
A:transformers.modeling_bert.layer_outputs->layer_module(hidden_states, attention_mask, head_mask[i], encoder_hidden_states, encoder_attention_mask, output_attentions)
A:transformers.modeling_bert.self.activation->torch.nn.Tanh()
A:transformers.modeling_bert.pooled_output->self.dropout(pooled_output)
A:transformers.modeling_bert.self.transform->BertPredictionHeadTransform(config)
A:transformers.modeling_bert.self.decoder->torch.nn.Linear(config.hidden_size, config.vocab_size, bias=False)
A:transformers.modeling_bert.self.bias->torch.nn.Parameter(torch.zeros(config.vocab_size))
A:transformers.modeling_bert.self.predictions->BertLMPredictionHead(config)
A:transformers.modeling_bert.prediction_scores->self.cls(sequence_output)
A:transformers.modeling_bert.self.seq_relationship->torch.nn.Linear(config.hidden_size, 2)
A:transformers.modeling_bert.seq_relationship_score->self.seq_relationship(pooled_output)
A:transformers.modeling_bert.self.embeddings->BertEmbeddings(config)
A:transformers.modeling_bert.self.encoder->BertEncoder(config)
A:transformers.modeling_bert.self.pooler->BertPooler(config)
A:transformers.modeling_bert.attention_mask->torch.cat([attention_mask, attention_mask.new_zeros((attention_mask.shape[0], 1))], dim=-1)
A:transformers.modeling_bert.(encoder_batch_size, encoder_sequence_length, _)->encoder_hidden_states.size()
A:transformers.modeling_bert.encoder_attention_mask->torch.ones(encoder_hidden_shape, device=device)
A:transformers.modeling_bert.encoder_extended_attention_mask->self.invert_attention_mask(encoder_attention_mask)
A:transformers.modeling_bert.head_mask->self.get_head_mask(head_mask, self.config.num_hidden_layers)
A:transformers.modeling_bert.embedding_output->self.embeddings(input_ids=input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds)
A:transformers.modeling_bert.encoder_outputs->self.encoder(embedding_output, attention_mask=extended_attention_mask, head_mask=head_mask, encoder_hidden_states=encoder_hidden_states, encoder_attention_mask=encoder_extended_attention_mask, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.modeling_bert.self.bert->BertModel(config)
A:transformers.modeling_bert.self.cls->BertOnlyNSPHead(config)
A:transformers.modeling_bert.labels->kwargs.pop('masked_lm_labels')
A:transformers.modeling_bert.outputs->self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.modeling_bert.(prediction_scores, seq_relationship_score)->self.cls(sequence_output, pooled_output)
A:transformers.modeling_bert.loss_fct->CrossEntropyLoss(ignore_index=ignored_index)
A:transformers.modeling_bert.masked_lm_loss->loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))
A:transformers.modeling_bert.next_sentence_loss->loss_fct(seq_relationship_scores.view(-1, 2), next_sentence_label.view(-1))
A:transformers.modeling_bert.shifted_prediction_scores->prediction_scores[:, :-1, :].contiguous()
A:transformers.modeling_bert.lm_loss->loss_fct(shifted_prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))
A:transformers.modeling_bert.dummy_token->torch.full((effective_batch_size, 1), self.config.pad_token_id, dtype=torch.long, device=input_ids.device)
A:transformers.modeling_bert.input_ids->torch.cat([input_ids, dummy_token], dim=1)
A:transformers.modeling_bert.seq_relationship_scores->self.cls(pooled_output)
A:transformers.modeling_bert.self.classifier->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.modeling_bert.logits->self.qa_outputs(sequence_output)
A:transformers.modeling_bert.loss->loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
A:transformers.modeling_bert.reshaped_logits->self.qa_outputs(sequence_output).view(-1, num_choices)
A:transformers.modeling_bert.sequence_output->self.dropout(sequence_output)
A:transformers.modeling_bert.active_logits->self.qa_outputs(sequence_output).view(-1, self.num_labels)
A:transformers.modeling_bert.active_labels->torch.where(active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels))
A:transformers.modeling_bert.self.qa_outputs->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.modeling_bert.(start_logits, end_logits)->self.qa_outputs(sequence_output).split(1, dim=-1)
A:transformers.modeling_bert.start_logits->start_logits.squeeze(-1).squeeze(-1)
A:transformers.modeling_bert.end_logits->end_logits.squeeze(-1).squeeze(-1)
A:transformers.modeling_bert.start_positions->start_positions.squeeze(-1).squeeze(-1)
A:transformers.modeling_bert.end_positions->end_positions.squeeze(-1).squeeze(-1)
A:transformers.modeling_bert.ignored_index->start_logits.squeeze(-1).squeeze(-1).size(1)
A:transformers.modeling_bert.start_loss->loss_fct(start_logits, start_positions)
A:transformers.modeling_bert.end_loss->loss_fct(end_logits, end_positions)
transformers.BertForMaskedLM(self,config)
transformers.BertForMaskedLM.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.BertForMaskedLM.get_output_embeddings(self)
transformers.BertForMaskedLM.prepare_inputs_for_generation(self,input_ids,attention_mask=None,**model_kwargs)
transformers.BertForMultipleChoice(self,config)
transformers.BertForMultipleChoice.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.BertForNextSentencePrediction(self,config)
transformers.BertForNextSentencePrediction.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,next_sentence_label=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.BertForPreTraining(self,config)
transformers.BertForPreTraining.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,next_sentence_label=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.BertForPreTraining.get_output_embeddings(self)
transformers.BertForPreTrainingOutput(ModelOutput)
transformers.BertForQuestionAnswering(self,config)
transformers.BertForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,start_positions=None,end_positions=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.BertForSequenceClassification(self,config)
transformers.BertForSequenceClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.BertForTokenClassification(self,config)
transformers.BertForTokenClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.BertLMHeadModel(self,config)
transformers.BertLMHeadModel.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.BertLMHeadModel.get_output_embeddings(self)
transformers.BertLMHeadModel.prepare_inputs_for_generation(self,input_ids,attention_mask=None,**model_kwargs)
transformers.BertLayer(self,config)
transformers.BertLayer.feed_forward_chunk(self,attention_output)
transformers.BertLayer.forward(self,hidden_states,attention_mask=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,output_attentions=False)
transformers.BertModel(self,config)
transformers.BertModel._prune_heads(self,heads_to_prune)
transformers.BertModel.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.BertModel.get_input_embeddings(self)
transformers.BertModel.set_input_embeddings(self,value)
transformers.BertPreTrainedModel(PreTrainedModel)
transformers.BertPreTrainedModel._init_weights(self,module)
transformers.load_tf_weights_in_bert(model,config,tf_checkpoint_path)
transformers.modeling_bert.BertAttention(self,config)
transformers.modeling_bert.BertAttention.__init__(self,config)
transformers.modeling_bert.BertAttention.forward(self,hidden_states,attention_mask=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,output_attentions=False)
transformers.modeling_bert.BertAttention.prune_heads(self,heads)
transformers.modeling_bert.BertEmbeddings(self,config)
transformers.modeling_bert.BertEmbeddings.__init__(self,config)
transformers.modeling_bert.BertEmbeddings.forward(self,input_ids=None,token_type_ids=None,position_ids=None,inputs_embeds=None)
transformers.modeling_bert.BertEncoder(self,config)
transformers.modeling_bert.BertEncoder.__init__(self,config)
transformers.modeling_bert.BertEncoder.forward(self,hidden_states,attention_mask=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,output_attentions=False,output_hidden_states=False,return_dict=False)
transformers.modeling_bert.BertForMaskedLM(self,config)
transformers.modeling_bert.BertForMaskedLM.__init__(self,config)
transformers.modeling_bert.BertForMaskedLM.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.modeling_bert.BertForMaskedLM.get_output_embeddings(self)
transformers.modeling_bert.BertForMaskedLM.prepare_inputs_for_generation(self,input_ids,attention_mask=None,**model_kwargs)
transformers.modeling_bert.BertForMultipleChoice(self,config)
transformers.modeling_bert.BertForMultipleChoice.__init__(self,config)
transformers.modeling_bert.BertForMultipleChoice.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.modeling_bert.BertForNextSentencePrediction(self,config)
transformers.modeling_bert.BertForNextSentencePrediction.__init__(self,config)
transformers.modeling_bert.BertForNextSentencePrediction.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,next_sentence_label=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.modeling_bert.BertForPreTraining(self,config)
transformers.modeling_bert.BertForPreTraining.__init__(self,config)
transformers.modeling_bert.BertForPreTraining.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,next_sentence_label=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.modeling_bert.BertForPreTraining.get_output_embeddings(self)
transformers.modeling_bert.BertForPreTrainingOutput(ModelOutput)
transformers.modeling_bert.BertForQuestionAnswering(self,config)
transformers.modeling_bert.BertForQuestionAnswering.__init__(self,config)
transformers.modeling_bert.BertForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,start_positions=None,end_positions=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.modeling_bert.BertForSequenceClassification(self,config)
transformers.modeling_bert.BertForSequenceClassification.__init__(self,config)
transformers.modeling_bert.BertForSequenceClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.modeling_bert.BertForTokenClassification(self,config)
transformers.modeling_bert.BertForTokenClassification.__init__(self,config)
transformers.modeling_bert.BertForTokenClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.modeling_bert.BertIntermediate(self,config)
transformers.modeling_bert.BertIntermediate.__init__(self,config)
transformers.modeling_bert.BertIntermediate.forward(self,hidden_states)
transformers.modeling_bert.BertLMHeadModel(self,config)
transformers.modeling_bert.BertLMHeadModel.__init__(self,config)
transformers.modeling_bert.BertLMHeadModel.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.modeling_bert.BertLMHeadModel.get_output_embeddings(self)
transformers.modeling_bert.BertLMHeadModel.prepare_inputs_for_generation(self,input_ids,attention_mask=None,**model_kwargs)
transformers.modeling_bert.BertLMPredictionHead(self,config)
transformers.modeling_bert.BertLMPredictionHead.__init__(self,config)
transformers.modeling_bert.BertLMPredictionHead.forward(self,hidden_states)
transformers.modeling_bert.BertLayer(self,config)
transformers.modeling_bert.BertLayer.__init__(self,config)
transformers.modeling_bert.BertLayer.feed_forward_chunk(self,attention_output)
transformers.modeling_bert.BertLayer.forward(self,hidden_states,attention_mask=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,output_attentions=False)
transformers.modeling_bert.BertModel(self,config)
transformers.modeling_bert.BertModel.__init__(self,config)
transformers.modeling_bert.BertModel._prune_heads(self,heads_to_prune)
transformers.modeling_bert.BertModel.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.modeling_bert.BertModel.get_input_embeddings(self)
transformers.modeling_bert.BertModel.set_input_embeddings(self,value)
transformers.modeling_bert.BertOnlyMLMHead(self,config)
transformers.modeling_bert.BertOnlyMLMHead.__init__(self,config)
transformers.modeling_bert.BertOnlyMLMHead.forward(self,sequence_output)
transformers.modeling_bert.BertOnlyNSPHead(self,config)
transformers.modeling_bert.BertOnlyNSPHead.__init__(self,config)
transformers.modeling_bert.BertOnlyNSPHead.forward(self,pooled_output)
transformers.modeling_bert.BertOutput(self,config)
transformers.modeling_bert.BertOutput.__init__(self,config)
transformers.modeling_bert.BertOutput.forward(self,hidden_states,input_tensor)
transformers.modeling_bert.BertPooler(self,config)
transformers.modeling_bert.BertPooler.__init__(self,config)
transformers.modeling_bert.BertPooler.forward(self,hidden_states)
transformers.modeling_bert.BertPreTrainedModel(PreTrainedModel)
transformers.modeling_bert.BertPreTrainedModel._init_weights(self,module)
transformers.modeling_bert.BertPreTrainingHeads(self,config)
transformers.modeling_bert.BertPreTrainingHeads.__init__(self,config)
transformers.modeling_bert.BertPreTrainingHeads.forward(self,sequence_output,pooled_output)
transformers.modeling_bert.BertPredictionHeadTransform(self,config)
transformers.modeling_bert.BertPredictionHeadTransform.__init__(self,config)
transformers.modeling_bert.BertPredictionHeadTransform.forward(self,hidden_states)
transformers.modeling_bert.BertSelfAttention(self,config)
transformers.modeling_bert.BertSelfAttention.__init__(self,config)
transformers.modeling_bert.BertSelfAttention.forward(self,hidden_states,attention_mask=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,output_attentions=False)
transformers.modeling_bert.BertSelfAttention.transpose_for_scores(self,x)
transformers.modeling_bert.BertSelfOutput(self,config)
transformers.modeling_bert.BertSelfOutput.__init__(self,config)
transformers.modeling_bert.BertSelfOutput.forward(self,hidden_states,input_tensor)
transformers.modeling_bert.load_tf_weights_in_bert(model,config,tf_checkpoint_path)
transformers.modeling_bert.mish(x)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/modeling_electra.py----------------------------------------
A:transformers.modeling_electra.logger->utils.logging.get_logger(__name__)
A:transformers.modeling_electra.tf_path->os.path.abspath(tf_checkpoint_path)
A:transformers.modeling_electra.init_vars->tensorflow.train.list_variables(tf_path)
A:transformers.modeling_electra.array->numpy.transpose(array)
A:transformers.modeling_electra.name->name.split('/').split('/')
A:transformers.modeling_electra.scope_names->re.split('_(\\d+)', m_name)
A:transformers.modeling_electra.pointer->getattr(pointer, 'weight')
A:transformers.modeling_electra.num->int(scope_names[1])
A:transformers.modeling_electra.pointer.data->torch.from_numpy(array)
A:transformers.modeling_electra.self.word_embeddings->torch.nn.Embedding(config.vocab_size, config.embedding_size, padding_idx=config.pad_token_id)
A:transformers.modeling_electra.self.position_embeddings->torch.nn.Embedding(config.max_position_embeddings, config.embedding_size)
A:transformers.modeling_electra.self.token_type_embeddings->torch.nn.Embedding(config.type_vocab_size, config.embedding_size)
A:transformers.modeling_electra.self.LayerNorm->BertLayerNorm(config.embedding_size)
A:transformers.modeling_electra.self.dense->torch.nn.Linear(config.hidden_size, config.hidden_size)
A:transformers.modeling_electra.self.dense_prediction->torch.nn.Linear(config.hidden_size, 1)
A:transformers.modeling_electra.hidden_states->self.encoder(hidden_states, attention_mask=extended_attention_mask, head_mask=head_mask, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.modeling_electra.logits->self.classifier(pooled_output)
A:transformers.modeling_electra.self.embeddings->ElectraEmbeddings(config)
A:transformers.modeling_electra.self.embeddings_project->torch.nn.Linear(config.embedding_size, config.hidden_size)
A:transformers.modeling_electra.self.encoder->BertEncoder(config)
A:transformers.modeling_electra.input_shape->input_ids.size()
A:transformers.modeling_electra.attention_mask->torch.ones(input_shape, device=device)
A:transformers.modeling_electra.token_type_ids->torch.zeros(input_shape, dtype=torch.long, device=device)
A:transformers.modeling_electra.extended_attention_mask->self.get_extended_attention_mask(attention_mask, input_shape, device)
A:transformers.modeling_electra.head_mask->self.get_head_mask(head_mask, self.config.num_hidden_layers)
A:transformers.modeling_electra.self.dropout->torch.nn.Dropout(config.hidden_dropout_prob)
A:transformers.modeling_electra.self.out_proj->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.modeling_electra.x->self.out_proj(x)
A:transformers.modeling_electra.self.electra->ElectraModel(config)
A:transformers.modeling_electra.self.classifier->torch.nn.Linear(config.hidden_size, 1)
A:transformers.modeling_electra.discriminator_hidden_states->self.electra(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.modeling_electra.loss_fct->CrossEntropyLoss()
A:transformers.modeling_electra.loss->loss_fct(reshaped_logits, labels)
A:transformers.modeling_electra.self.discriminator_predictions->ElectraDiscriminatorPredictions(config)
A:transformers.modeling_electra.self.generator_predictions->ElectraGeneratorPredictions(config)
A:transformers.modeling_electra.self.generator_lm_head->torch.nn.Linear(config.embedding_size, config.vocab_size)
A:transformers.modeling_electra.labels->kwargs.pop('masked_lm_labels')
A:transformers.modeling_electra.generator_hidden_states->self.electra(input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)
A:transformers.modeling_electra.prediction_scores->self.generator_lm_head(prediction_scores)
A:transformers.modeling_electra.discriminator_sequence_output->self.dropout(discriminator_sequence_output)
A:transformers.modeling_electra.self.qa_outputs->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.modeling_electra.(start_logits, end_logits)->self.classifier(pooled_output).split(1, dim=-1)
A:transformers.modeling_electra.start_logits->start_logits.squeeze(-1).squeeze(-1)
A:transformers.modeling_electra.end_logits->end_logits.squeeze(-1).squeeze(-1)
A:transformers.modeling_electra.start_positions->start_positions.squeeze(-1).squeeze(-1)
A:transformers.modeling_electra.end_positions->end_positions.squeeze(-1).squeeze(-1)
A:transformers.modeling_electra.ignored_index->start_logits.squeeze(-1).squeeze(-1).size(1)
A:transformers.modeling_electra.start_loss->loss_fct(start_logits, start_positions)
A:transformers.modeling_electra.end_loss->loss_fct(end_logits, end_positions)
A:transformers.modeling_electra.self.sequence_summary->SequenceSummary(config)
A:transformers.modeling_electra.pooled_output->self.sequence_summary(sequence_output)
A:transformers.modeling_electra.reshaped_logits->self.classifier(pooled_output).view(-1, num_choices)
transformers.ElectraForMaskedLM(self,config)
transformers.ElectraForMaskedLM.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.ElectraForMaskedLM.get_output_embeddings(self)
transformers.ElectraForMultipleChoice(self,config)
transformers.ElectraForMultipleChoice.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.ElectraForPreTraining(self,config)
transformers.ElectraForPreTraining.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.ElectraForPreTrainingOutput(ModelOutput)
transformers.ElectraForQuestionAnswering(self,config)
transformers.ElectraForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,start_positions=None,end_positions=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.ElectraForSequenceClassification(self,config)
transformers.ElectraForSequenceClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.ElectraForTokenClassification(self,config)
transformers.ElectraForTokenClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.ElectraModel(self,config)
transformers.ElectraModel._prune_heads(self,heads_to_prune)
transformers.ElectraModel.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.ElectraModel.get_input_embeddings(self)
transformers.ElectraModel.set_input_embeddings(self,value)
transformers.ElectraPreTrainedModel(BertPreTrainedModel)
transformers.load_tf_weights_in_electra(model,config,tf_checkpoint_path,discriminator_or_generator='discriminator')
transformers.modeling_electra.ElectraClassificationHead(self,config)
transformers.modeling_electra.ElectraClassificationHead.__init__(self,config)
transformers.modeling_electra.ElectraClassificationHead.forward(self,features,**kwargs)
transformers.modeling_electra.ElectraDiscriminatorPredictions(self,config)
transformers.modeling_electra.ElectraDiscriminatorPredictions.__init__(self,config)
transformers.modeling_electra.ElectraDiscriminatorPredictions.forward(self,discriminator_hidden_states)
transformers.modeling_electra.ElectraEmbeddings(self,config)
transformers.modeling_electra.ElectraEmbeddings.__init__(self,config)
transformers.modeling_electra.ElectraForMaskedLM(self,config)
transformers.modeling_electra.ElectraForMaskedLM.__init__(self,config)
transformers.modeling_electra.ElectraForMaskedLM.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.modeling_electra.ElectraForMaskedLM.get_output_embeddings(self)
transformers.modeling_electra.ElectraForMultipleChoice(self,config)
transformers.modeling_electra.ElectraForMultipleChoice.__init__(self,config)
transformers.modeling_electra.ElectraForMultipleChoice.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.modeling_electra.ElectraForPreTraining(self,config)
transformers.modeling_electra.ElectraForPreTraining.__init__(self,config)
transformers.modeling_electra.ElectraForPreTraining.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.modeling_electra.ElectraForPreTrainingOutput(ModelOutput)
transformers.modeling_electra.ElectraForQuestionAnswering(self,config)
transformers.modeling_electra.ElectraForQuestionAnswering.__init__(self,config)
transformers.modeling_electra.ElectraForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,start_positions=None,end_positions=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.modeling_electra.ElectraForSequenceClassification(self,config)
transformers.modeling_electra.ElectraForSequenceClassification.__init__(self,config)
transformers.modeling_electra.ElectraForSequenceClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.modeling_electra.ElectraForTokenClassification(self,config)
transformers.modeling_electra.ElectraForTokenClassification.__init__(self,config)
transformers.modeling_electra.ElectraForTokenClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.modeling_electra.ElectraGeneratorPredictions(self,config)
transformers.modeling_electra.ElectraGeneratorPredictions.__init__(self,config)
transformers.modeling_electra.ElectraGeneratorPredictions.forward(self,generator_hidden_states)
transformers.modeling_electra.ElectraModel(self,config)
transformers.modeling_electra.ElectraModel.__init__(self,config)
transformers.modeling_electra.ElectraModel._prune_heads(self,heads_to_prune)
transformers.modeling_electra.ElectraModel.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.modeling_electra.ElectraModel.get_input_embeddings(self)
transformers.modeling_electra.ElectraModel.set_input_embeddings(self,value)
transformers.modeling_electra.ElectraPreTrainedModel(BertPreTrainedModel)
transformers.modeling_electra.load_tf_weights_in_electra(model,config,tf_checkpoint_path,discriminator_or_generator='discriminator')


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/configuration_roberta.py----------------------------------------
A:transformers.configuration_roberta.logger->utils.logging.get_logger(__name__)
transformers.RobertaConfig(self,pad_token_id=1,bos_token_id=0,eos_token_id=2,**kwargs)
transformers.configuration_roberta.RobertaConfig(self,pad_token_id=1,bos_token_id=0,eos_token_id=2,**kwargs)
transformers.configuration_roberta.RobertaConfig.__init__(self,pad_token_id=1,bos_token_id=0,eos_token_id=2,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/configuration_marian.py----------------------------------------
transformers.MarianConfig(BartConfig)
transformers.configuration_marian.MarianConfig(BartConfig)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/modeling_outputs.py----------------------------------------
transformers.modeling_outputs.BaseModelOutput(ModelOutput)
transformers.modeling_outputs.BaseModelOutputWithPast(ModelOutput)
transformers.modeling_outputs.BaseModelOutputWithPooling(ModelOutput)
transformers.modeling_outputs.CausalLMOutput(ModelOutput)
transformers.modeling_outputs.CausalLMOutputWithPast(ModelOutput)
transformers.modeling_outputs.MaskedLMOutput(ModelOutput)
transformers.modeling_outputs.MultipleChoiceModelOutput(ModelOutput)
transformers.modeling_outputs.NextSentencePredictorOutput(ModelOutput)
transformers.modeling_outputs.QuestionAnsweringModelOutput(ModelOutput)
transformers.modeling_outputs.Seq2SeqLMOutput(ModelOutput)
transformers.modeling_outputs.Seq2SeqModelOutput(ModelOutput)
transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput(ModelOutput)
transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput(ModelOutput)
transformers.modeling_outputs.SequenceClassifierOutput(ModelOutput)
transformers.modeling_outputs.TokenClassifierOutput(ModelOutput)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/configuration_distilbert.py----------------------------------------
A:transformers.configuration_distilbert.logger->utils.logging.get_logger(__name__)
transformers.DistilBertConfig(self,vocab_size=30522,max_position_embeddings=512,sinusoidal_pos_embds=False,n_layers=6,n_heads=12,dim=768,hidden_dim=4*768,dropout=0.1,attention_dropout=0.1,activation='gelu',initializer_range=0.02,qa_dropout=0.1,seq_classif_dropout=0.2,pad_token_id=0,**kwargs)
transformers.DistilBertConfig.hidden_size(self)
transformers.DistilBertConfig.num_attention_heads(self)
transformers.DistilBertConfig.num_hidden_layers(self)
transformers.configuration_distilbert.DistilBertConfig(self,vocab_size=30522,max_position_embeddings=512,sinusoidal_pos_embds=False,n_layers=6,n_heads=12,dim=768,hidden_dim=4*768,dropout=0.1,attention_dropout=0.1,activation='gelu',initializer_range=0.02,qa_dropout=0.1,seq_classif_dropout=0.2,pad_token_id=0,**kwargs)
transformers.configuration_distilbert.DistilBertConfig.__init__(self,vocab_size=30522,max_position_embeddings=512,sinusoidal_pos_embds=False,n_layers=6,n_heads=12,dim=768,hidden_dim=4*768,dropout=0.1,attention_dropout=0.1,activation='gelu',initializer_range=0.02,qa_dropout=0.1,seq_classif_dropout=0.2,pad_token_id=0,**kwargs)
transformers.configuration_distilbert.DistilBertConfig.hidden_size(self)
transformers.configuration_distilbert.DistilBertConfig.num_attention_heads(self)
transformers.configuration_distilbert.DistilBertConfig.num_hidden_layers(self)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/generation_utils.py----------------------------------------
A:transformers.generation_utils.logger->utils.logging.get_logger(__name__)
A:transformers.generation_utils.banned_batch_tokens->calc_banned_ngram_tokens(input_ids, num_batch_hypotheses, no_repeat_ngram_size, cur_len)
A:transformers.generation_utils.bad_words_ids->list(filter(lambda bad_token_seq: bad_token_seq != [eos_token_id], bad_words_ids))
A:transformers.generation_utils.banned_tokens->calc_banned_bad_words_ids(input_ids.tolist(), bad_words_ids)
A:transformers.generation_utils.input_ids->torch.cat([input_ids, beam_tokens.unsqueeze(1)], dim=-1)
A:transformers.generation_utils.attention_mask->torch.cat([attention_mask, attention_mask.new_ones((attention_mask.shape[0], 1))], dim=-1)
A:transformers.generation_utils.encoder->self.get_encoder()
A:transformers.generation_utils.expanded_batch_idxs->torch.arange(batch_size).view(-1, 1).repeat(1, num_beams * effective_batch_mult).view(-1).to(input_ids.device)
A:transformers.generation_utils.encoder_outputs['last_hidden_state']->encoder_outputs.last_hidden_state.index_select(0, expanded_batch_idxs)
A:transformers.generation_utils.output->self._generate_no_beam_search(input_ids, cur_len=cur_len, max_length=max_length, min_length=min_length, do_sample=do_sample, temperature=temperature, top_k=top_k, top_p=top_p, repetition_penalty=repetition_penalty, no_repeat_ngram_size=no_repeat_ngram_size, bad_words_ids=bad_words_ids, pad_token_id=pad_token_id, eos_token_id=eos_token_id, batch_size=effective_batch_size, attention_mask=attention_mask, use_cache=use_cache, model_kwargs=model_kwargs)
A:transformers.generation_utils.unfinished_sents->torch.cat([input_ids, beam_tokens.unsqueeze(1)], dim=-1).new(batch_size).fill_(1)
A:transformers.generation_utils.sent_lengths->torch.cat([input_ids, beam_tokens.unsqueeze(1)], dim=-1).new(output_batch_size)
A:transformers.generation_utils.model_inputs->self.prepare_inputs_for_generation(input_ids, past=past, attention_mask=attention_mask, use_cache=use_cache, **model_kwargs)
A:transformers.generation_utils.outputs->self(**model_inputs, return_dict=True)
A:transformers.generation_utils.scores->self.postprocess_next_token_scores(scores=scores, input_ids=input_ids, no_repeat_ngram_size=no_repeat_ngram_size, bad_words_ids=bad_words_ids, cur_len=cur_len, min_length=min_length, max_length=max_length, eos_token_id=eos_token_id, repetition_penalty=repetition_penalty, batch_size=batch_size, num_beams=num_beams)
A:transformers.generation_utils.next_token_logscores->top_k_top_p_filtering(scores, top_k=top_k, top_p=top_p)
A:transformers.generation_utils.probs->torch.nn.functional.softmax(_scores, dim=-1)
A:transformers.generation_utils.next_token->torch.argmax(next_token_logits, dim=-1)
A:transformers.generation_utils.is_sents_unfinished_and_token_to_add_is_eos->torch.cat([input_ids, beam_tokens.unsqueeze(1)], dim=-1).new(batch_size).fill_(1).mul(eos_in_sents.long()).bool()
A:transformers.generation_utils.beam_scores->beam_scores.new([x[0] for x in next_batch_beam]).new([x[0] for x in next_batch_beam])
A:transformers.generation_utils.next_token_logits->self.adjust_logits_during_generation(next_token_logits, cur_len=cur_len, max_length=max_length)
A:transformers.generation_utils._scores->_scores.contiguous().view(batch_size, num_beams * vocab_size).contiguous().view(batch_size, num_beams * vocab_size)
A:transformers.generation_utils.next_tokens->torch.gather(next_tokens, -1, next_scores_indices)
A:transformers.generation_utils.next_scores->next_scores.view(batch_size, num_beams * vocab_size).view(batch_size, num_beams * vocab_size)
A:transformers.generation_utils.(next_scores, next_scores_indices)->torch.sort(next_scores, descending=True, dim=1)
A:transformers.generation_utils.(next_scores, next_tokens)->torch.topk(next_scores, 2 * num_beams, dim=1, largest=True, sorted=True)
A:transformers.generation_utils.beam_tokens->torch.cat([input_ids, beam_tokens.unsqueeze(1)], dim=-1).new([x[1] for x in next_batch_beam])
A:transformers.generation_utils.beam_idx->torch.cat([input_ids, beam_tokens.unsqueeze(1)], dim=-1).new([x[2] for x in next_batch_beam])
A:transformers.generation_utils.past->self._reorder_cache(past, beam_idx)
A:transformers.generation_utils.final_score->beam_scores[effective_beam_id].item()
A:transformers.generation_utils.sorted_hyps->sorted(hypotheses.beams, key=lambda x: x[0])
A:transformers.generation_utils.sent_lengths[effective_batch_idx]->len(best_hyp)
A:transformers.generation_utils.sent_max_len->min(sent_lengths.max().item() + 1, max_length)
A:transformers.generation_utils.decoded->torch.stack(best).type(torch.long).to(next(self.parameters()).device)
A:transformers.generation_utils.gen_tokens->prev_input_ids[idx].tolist()
A:transformers.generation_utils.prev_ngram_tuple->tuple(ngram[:-1])
A:transformers.generation_utils.ngram_idx->tuple(prev_input_ids[hypo_idx, start_idx:cur_len].tolist())
A:transformers.generation_utils.banned_mask->torch.sparse.LongTensor(banned_mask.t(), indices, scores.size()).to(scores.device).to_dense().bool()
A:transformers.generation_utils.indices->torch.ones(len(banned_mask))
A:transformers.generation_utils.top_k->min(max(top_k, min_tokens_to_keep), logits.size(-1))
A:transformers.generation_utils.(sorted_logits, sorted_indices)->torch.sort(logits, descending=True)
A:transformers.generation_utils.cumulative_probs->torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
A:transformers.generation_utils.sorted_indices_to_remove[..., 1:]->sorted_indices_to_remove[..., :-1].clone()
A:transformers.generation_utils.indices_to_remove->sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)
A:transformers.generation_utils.sorted_scores->sorted([(s, idx) for (idx, (s, _)) in enumerate(self.beams)])
A:transformers.generation_utils.self.worst_score->min(score, self.worst_score)
transformers.generation_utils.BeamHypotheses(self,num_beams,max_length,length_penalty,early_stopping)
transformers.generation_utils.BeamHypotheses.__init__(self,num_beams,max_length,length_penalty,early_stopping)
transformers.generation_utils.BeamHypotheses.__len__(self)
transformers.generation_utils.BeamHypotheses.add(self,hyp,sum_logprobs)
transformers.generation_utils.BeamHypotheses.is_done(self,best_sum_logprobs,cur_len)
transformers.generation_utils.GenerationMixin
transformers.generation_utils.GenerationMixin._generate_beam_search(self,input_ids,cur_len,max_length,min_length,do_sample,early_stopping,temperature,top_k,top_p,repetition_penalty,no_repeat_ngram_size,bad_words_ids,pad_token_id,eos_token_id,batch_size,num_return_sequences,length_penalty,num_beams,vocab_size,attention_mask,use_cache,model_kwargs)
transformers.generation_utils.GenerationMixin._generate_no_beam_search(self,input_ids,cur_len,max_length,min_length,do_sample,temperature,top_k,top_p,repetition_penalty,no_repeat_ngram_size,bad_words_ids,pad_token_id,eos_token_id,batch_size,attention_mask,use_cache,model_kwargs)
transformers.generation_utils.GenerationMixin._reorder_cache(past:Tuple,beam_idx:Tensor)->Tuple[Tensor]
transformers.generation_utils.GenerationMixin.adjust_logits_during_generation(self,logits,**kwargs)
transformers.generation_utils.GenerationMixin.enforce_repetition_penalty_(self,lprobs,batch_size,num_beams,prev_output_tokens,repetition_penalty)
transformers.generation_utils.GenerationMixin.generate(self,input_ids:Optional[torch.LongTensor]=None,max_length:Optional[int]=None,min_length:Optional[int]=None,do_sample:Optional[bool]=None,early_stopping:Optional[bool]=None,num_beams:Optional[int]=None,temperature:Optional[float]=None,top_k:Optional[int]=None,top_p:Optional[float]=None,repetition_penalty:Optional[float]=None,bad_words_ids:Optional[Iterable[int]]=None,bos_token_id:Optional[int]=None,pad_token_id:Optional[int]=None,eos_token_id:Optional[int]=None,length_penalty:Optional[float]=None,no_repeat_ngram_size:Optional[int]=None,num_return_sequences:Optional[int]=None,attention_mask:Optional[torch.LongTensor]=None,decoder_start_token_id:Optional[int]=None,use_cache:Optional[bool]=None,**model_kwargs)->torch.LongTensor
transformers.generation_utils.GenerationMixin.postprocess_next_token_scores(self,scores,input_ids,no_repeat_ngram_size,bad_words_ids,cur_len,min_length,max_length,eos_token_id,repetition_penalty,batch_size,num_beams)
transformers.generation_utils.GenerationMixin.prepare_inputs_for_generation(self,input_ids,**kwargs)
transformers.generation_utils.calc_banned_bad_words_ids(prev_input_ids:Iterable[int],bad_words_ids:Iterable[int])->Iterable[int]
transformers.generation_utils.calc_banned_ngram_tokens(prev_input_ids:Tensor,num_hypos:int,no_repeat_ngram_size:int,cur_len:int)->None
transformers.generation_utils.set_scores_to_inf_for_banned_tokens(scores:torch.Tensor,banned_tokens:List[List[int]])->None
transformers.generation_utils.top_k_top_p_filtering(logits:Tensor,top_k:int=0,top_p:float=1.0,filter_value:float=-float('Inf'),min_tokens_to_keep:int=1)->Tensor
transformers.top_k_top_p_filtering(logits:Tensor,top_k:int=0,top_p:float=1.0,filter_value:float=-float('Inf'),min_tokens_to_keep:int=1)->Tensor


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/tokenization_utils_fast.py----------------------------------------
A:transformers.tokenization_utils_fast.logger->utils.logging.get_logger(__name__)
A:transformers.tokenization_utils_fast.base_vocab->self._tokenizer.get_vocab(with_added_tokens=False)
A:transformers.tokenization_utils_fast.full_vocab->self._tokenizer.get_vocab(with_added_tokens=True)
A:transformers.tokenization_utils_fast.added_vocab->dict(((tok, index) for (tok, index) in full_vocab.items() if tok not in base_vocab))
A:transformers.tokenization_utils_fast.encoding_dict->defaultdict(list)
A:transformers.tokenization_utils_fast.index->int(index)
A:transformers.tokenization_utils_fast.encodings->self._tokenizer.encode_batch(batch_text_or_text_pairs, add_special_tokens=add_special_tokens, is_pretokenized=is_pretokenized)
A:transformers.tokenization_utils_fast.batched_output->BatchEncoding({key: value[0] if len(value) > 0 and isinstance(value[0], list) else value for (key, value) in batched_output.items()}, batched_output.encodings)
A:transformers.tokenization_utils_fast.text->self._tokenizer.decode(token_ids, skip_special_tokens=skip_special_tokens)
A:transformers.tokenization_utils_fast.clean_text->self.clean_up_tokenization(text)
A:transformers.tokenization_utils_fast.files->self._tokenizer.save_model(folder, name=file)
A:transformers.tokenization_utils_fast.(folder, file)->os.path.split(os.path.abspath(save_directory))
transformers.PreTrainedTokenizerFast(self,tokenizer:BaseTokenizerFast,**kwargs)
transformers.PreTrainedTokenizerFast.__len__(self)->int
transformers.PreTrainedTokenizerFast._add_tokens(self,new_tokens:List[Union[str,AddedToken]],special_tokens=False)->int
transformers.PreTrainedTokenizerFast._batch_encode_plus(self,batch_text_or_text_pairs:Union[List[TextInput],List[TextInputPair],List[PreTokenizedInput],List[PreTokenizedInputPair]],add_special_tokens:bool=True,padding_strategy:PaddingStrategy=PaddingStrategy.DO_NOT_PAD,truncation_strategy:TruncationStrategy=TruncationStrategy.DO_NOT_TRUNCATE,max_length:Optional[int]=None,stride:int=0,is_pretokenized:bool=False,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[str]=None,return_token_type_ids:Optional[bool]=None,return_attention_mask:Optional[bool]=None,return_overflowing_tokens:bool=False,return_special_tokens_mask:bool=False,return_offsets_mapping:bool=False,return_length:bool=False,verbose:bool=True,**kwargs)->BatchEncoding
transformers.PreTrainedTokenizerFast._convert_encoding(self,encoding:EncodingFast,return_token_type_ids:Optional[bool]=None,return_attention_mask:Optional[bool]=None,return_overflowing_tokens:bool=False,return_special_tokens_mask:bool=False,return_offsets_mapping:bool=False,return_length:bool=False,verbose:bool=True)->Dict[str, Any]
transformers.PreTrainedTokenizerFast._convert_id_to_token(self,index:int)->Optional[str]
transformers.PreTrainedTokenizerFast._convert_token_to_id_with_added_voc(self,token:str)->int
transformers.PreTrainedTokenizerFast._encode_plus(self,text:Union[TextInput,PreTokenizedInput],text_pair:Optional[Union[TextInput,PreTokenizedInput]]=None,add_special_tokens:bool=True,padding_strategy:PaddingStrategy=PaddingStrategy.DO_NOT_PAD,truncation_strategy:TruncationStrategy=TruncationStrategy.DO_NOT_TRUNCATE,max_length:Optional[int]=None,stride:int=0,is_pretokenized:bool=False,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[bool]=None,return_token_type_ids:Optional[bool]=None,return_attention_mask:Optional[bool]=None,return_overflowing_tokens:bool=False,return_special_tokens_mask:bool=False,return_offsets_mapping:bool=False,return_length:bool=False,verbose:bool=True,**kwargs)->BatchEncoding
transformers.PreTrainedTokenizerFast.backend_tokenizer(self)->BaseTokenizerFast
transformers.PreTrainedTokenizerFast.convert_ids_to_tokens(self,ids:Union[int,List[int]],skip_special_tokens:bool=False)->Union[str, List[str]]
transformers.PreTrainedTokenizerFast.convert_tokens_to_ids(self,tokens:Union[str,List[str]])->Union[int, List[int]]
transformers.PreTrainedTokenizerFast.decode(self,token_ids:List[int],skip_special_tokens:bool=False,clean_up_tokenization_spaces:bool=True)->str
transformers.PreTrainedTokenizerFast.decoder(self)->DecoderFast
transformers.PreTrainedTokenizerFast.get_added_vocab(self)->Dict[str, int]
transformers.PreTrainedTokenizerFast.get_vocab(self)->Dict[str, int]
transformers.PreTrainedTokenizerFast.is_fast(self)->bool
transformers.PreTrainedTokenizerFast.num_special_tokens_to_add(self,pair:bool=False)->int
transformers.PreTrainedTokenizerFast.save_vocabulary(self,save_directory:str)->Tuple[str]
transformers.PreTrainedTokenizerFast.set_truncation_and_padding(self,padding_strategy:PaddingStrategy,truncation_strategy:TruncationStrategy,max_length:int,stride:int,pad_to_multiple_of:Optional[int])
transformers.PreTrainedTokenizerFast.tokenize(self,text:str,pair:Optional[str]=None,add_special_tokens:bool=False)->List[str]
transformers.PreTrainedTokenizerFast.vocab_size(self)->int
transformers.tokenization_utils_fast.PreTrainedTokenizerFast(self,tokenizer:BaseTokenizerFast,**kwargs)
transformers.tokenization_utils_fast.PreTrainedTokenizerFast.__init__(self,tokenizer:BaseTokenizerFast,**kwargs)
transformers.tokenization_utils_fast.PreTrainedTokenizerFast.__len__(self)->int
transformers.tokenization_utils_fast.PreTrainedTokenizerFast._add_tokens(self,new_tokens:List[Union[str,AddedToken]],special_tokens=False)->int
transformers.tokenization_utils_fast.PreTrainedTokenizerFast._batch_encode_plus(self,batch_text_or_text_pairs:Union[List[TextInput],List[TextInputPair],List[PreTokenizedInput],List[PreTokenizedInputPair]],add_special_tokens:bool=True,padding_strategy:PaddingStrategy=PaddingStrategy.DO_NOT_PAD,truncation_strategy:TruncationStrategy=TruncationStrategy.DO_NOT_TRUNCATE,max_length:Optional[int]=None,stride:int=0,is_pretokenized:bool=False,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[str]=None,return_token_type_ids:Optional[bool]=None,return_attention_mask:Optional[bool]=None,return_overflowing_tokens:bool=False,return_special_tokens_mask:bool=False,return_offsets_mapping:bool=False,return_length:bool=False,verbose:bool=True,**kwargs)->BatchEncoding
transformers.tokenization_utils_fast.PreTrainedTokenizerFast._convert_encoding(self,encoding:EncodingFast,return_token_type_ids:Optional[bool]=None,return_attention_mask:Optional[bool]=None,return_overflowing_tokens:bool=False,return_special_tokens_mask:bool=False,return_offsets_mapping:bool=False,return_length:bool=False,verbose:bool=True)->Dict[str, Any]
transformers.tokenization_utils_fast.PreTrainedTokenizerFast._convert_id_to_token(self,index:int)->Optional[str]
transformers.tokenization_utils_fast.PreTrainedTokenizerFast._convert_token_to_id_with_added_voc(self,token:str)->int
transformers.tokenization_utils_fast.PreTrainedTokenizerFast._encode_plus(self,text:Union[TextInput,PreTokenizedInput],text_pair:Optional[Union[TextInput,PreTokenizedInput]]=None,add_special_tokens:bool=True,padding_strategy:PaddingStrategy=PaddingStrategy.DO_NOT_PAD,truncation_strategy:TruncationStrategy=TruncationStrategy.DO_NOT_TRUNCATE,max_length:Optional[int]=None,stride:int=0,is_pretokenized:bool=False,pad_to_multiple_of:Optional[int]=None,return_tensors:Optional[bool]=None,return_token_type_ids:Optional[bool]=None,return_attention_mask:Optional[bool]=None,return_overflowing_tokens:bool=False,return_special_tokens_mask:bool=False,return_offsets_mapping:bool=False,return_length:bool=False,verbose:bool=True,**kwargs)->BatchEncoding
transformers.tokenization_utils_fast.PreTrainedTokenizerFast.backend_tokenizer(self)->BaseTokenizerFast
transformers.tokenization_utils_fast.PreTrainedTokenizerFast.convert_ids_to_tokens(self,ids:Union[int,List[int]],skip_special_tokens:bool=False)->Union[str, List[str]]
transformers.tokenization_utils_fast.PreTrainedTokenizerFast.convert_tokens_to_ids(self,tokens:Union[str,List[str]])->Union[int, List[int]]
transformers.tokenization_utils_fast.PreTrainedTokenizerFast.decode(self,token_ids:List[int],skip_special_tokens:bool=False,clean_up_tokenization_spaces:bool=True)->str
transformers.tokenization_utils_fast.PreTrainedTokenizerFast.decoder(self)->DecoderFast
transformers.tokenization_utils_fast.PreTrainedTokenizerFast.get_added_vocab(self)->Dict[str, int]
transformers.tokenization_utils_fast.PreTrainedTokenizerFast.get_vocab(self)->Dict[str, int]
transformers.tokenization_utils_fast.PreTrainedTokenizerFast.is_fast(self)->bool
transformers.tokenization_utils_fast.PreTrainedTokenizerFast.num_special_tokens_to_add(self,pair:bool=False)->int
transformers.tokenization_utils_fast.PreTrainedTokenizerFast.save_vocabulary(self,save_directory:str)->Tuple[str]
transformers.tokenization_utils_fast.PreTrainedTokenizerFast.set_truncation_and_padding(self,padding_strategy:PaddingStrategy,truncation_strategy:TruncationStrategy,max_length:int,stride:int,pad_to_multiple_of:Optional[int])
transformers.tokenization_utils_fast.PreTrainedTokenizerFast.tokenize(self,text:str,pair:Optional[str]=None,add_special_tokens:bool=False)->List[str]
transformers.tokenization_utils_fast.PreTrainedTokenizerFast.vocab_size(self)->int


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/tokenization_distilbert.py----------------------------------------
A:transformers.tokenization_distilbert.logger->utils.logging.get_logger(__name__)
transformers.DistilBertTokenizer(BertTokenizer)
transformers.DistilBertTokenizerFast(BertTokenizerFast)
transformers.tokenization_distilbert.DistilBertTokenizer(BertTokenizer)
transformers.tokenization_distilbert.DistilBertTokenizerFast(BertTokenizerFast)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/tokenization_electra.py----------------------------------------
transformers.ElectraTokenizer(BertTokenizer)
transformers.ElectraTokenizerFast(BertTokenizerFast)
transformers.tokenization_electra.ElectraTokenizer(BertTokenizer)
transformers.tokenization_electra.ElectraTokenizerFast(BertTokenizerFast)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/tokenization_xlnet.py----------------------------------------
A:transformers.tokenization_xlnet.logger->utils.logging.get_logger(__name__)
A:transformers.tokenization_xlnet.self.sp_model->sentencepiece.SentencePieceProcessor()
A:transformers.tokenization_xlnet.state->self.__dict__.copy()
A:transformers.tokenization_xlnet.outputs->outputs.lower().lower()
A:transformers.tokenization_xlnet.text->self.preprocess_text(text)
A:transformers.tokenization_xlnet.pieces->self.sp_model.SampleEncodeAsPieces(text, 64, 0.1)
A:transformers.tokenization_xlnet.cur_pieces->self.sp_model.EncodeAsPieces(piece[:-1].replace(SPIECE_UNDERLINE, ''))
A:transformers.tokenization_xlnet.out_string->''.join(tokens).replace(SPIECE_UNDERLINE, ' ').strip()
A:transformers.tokenization_xlnet.out_vocab_file->os.path.join(save_directory, VOCAB_FILES_NAMES['vocab_file'])
transformers.XLNetTokenizer(self,vocab_file,do_lower_case=False,remove_space=True,keep_accents=False,bos_token='<s>',eos_token='</s>',unk_token='<unk>',sep_token='<sep>',pad_token='<pad>',cls_token='<cls>',mask_token='<mask>',additional_special_tokens=['<eop>','<eod>'],**kwargs)
transformers.XLNetTokenizer.__getstate__(self)
transformers.XLNetTokenizer.__setstate__(self,d)
transformers.XLNetTokenizer._convert_id_to_token(self,index)
transformers.XLNetTokenizer._convert_token_to_id(self,token)
transformers.XLNetTokenizer._tokenize(self,text,sample=False)
transformers.XLNetTokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.XLNetTokenizer.convert_tokens_to_string(self,tokens)
transformers.XLNetTokenizer.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.XLNetTokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.XLNetTokenizer.get_vocab(self)
transformers.XLNetTokenizer.preprocess_text(self,inputs)
transformers.XLNetTokenizer.save_vocabulary(self,save_directory)
transformers.XLNetTokenizer.vocab_size(self)
transformers.tokenization_xlnet.XLNetTokenizer(self,vocab_file,do_lower_case=False,remove_space=True,keep_accents=False,bos_token='<s>',eos_token='</s>',unk_token='<unk>',sep_token='<sep>',pad_token='<pad>',cls_token='<cls>',mask_token='<mask>',additional_special_tokens=['<eop>','<eod>'],**kwargs)
transformers.tokenization_xlnet.XLNetTokenizer.__getstate__(self)
transformers.tokenization_xlnet.XLNetTokenizer.__init__(self,vocab_file,do_lower_case=False,remove_space=True,keep_accents=False,bos_token='<s>',eos_token='</s>',unk_token='<unk>',sep_token='<sep>',pad_token='<pad>',cls_token='<cls>',mask_token='<mask>',additional_special_tokens=['<eop>','<eod>'],**kwargs)
transformers.tokenization_xlnet.XLNetTokenizer.__setstate__(self,d)
transformers.tokenization_xlnet.XLNetTokenizer._convert_id_to_token(self,index)
transformers.tokenization_xlnet.XLNetTokenizer._convert_token_to_id(self,token)
transformers.tokenization_xlnet.XLNetTokenizer._tokenize(self,text,sample=False)
transformers.tokenization_xlnet.XLNetTokenizer.build_inputs_with_special_tokens(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.tokenization_xlnet.XLNetTokenizer.convert_tokens_to_string(self,tokens)
transformers.tokenization_xlnet.XLNetTokenizer.create_token_type_ids_from_sequences(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None)->List[int]
transformers.tokenization_xlnet.XLNetTokenizer.get_special_tokens_mask(self,token_ids_0:List[int],token_ids_1:Optional[List[int]]=None,already_has_special_tokens:bool=False)->List[int]
transformers.tokenization_xlnet.XLNetTokenizer.get_vocab(self)
transformers.tokenization_xlnet.XLNetTokenizer.preprocess_text(self,inputs)
transformers.tokenization_xlnet.XLNetTokenizer.save_vocabulary(self,save_directory)
transformers.tokenization_xlnet.XLNetTokenizer.vocab_size(self)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/configuration_reformer.py----------------------------------------
A:transformers.configuration_reformer.logger->utils.logging.get_logger(__name__)
A:transformers.configuration_reformer.self.num_hidden_layers->len(attn_layers)
A:transformers.configuration_reformer.self.axial_pos_shape->tuple(axial_pos_shape)
A:transformers.configuration_reformer.self.axial_pos_embds_dim->tuple(axial_pos_embds_dim)
transformers.ReformerConfig(self,attention_head_size=64,attn_layers=['local','lsh','local','lsh','local','lsh'],axial_norm_std=1.0,axial_pos_embds=True,axial_pos_shape=[64,64],axial_pos_embds_dim=[64,192],chunk_size_lm_head=0,eos_token_id=2,feed_forward_size=512,hash_seed=None,hidden_act='relu',hidden_dropout_prob=0.05,hidden_size=256,initializer_range=0.02,is_decoder=False,layer_norm_eps=1e-12,local_num_chunks_before=1,local_num_chunks_after=0,local_attention_probs_dropout_prob=0.05,local_attn_chunk_length=64,lsh_attn_chunk_length=64,lsh_attention_probs_dropout_prob=0.0,lsh_num_chunks_before=1,lsh_num_chunks_after=0,max_position_embeddings=4096,num_attention_heads=2,num_buckets=None,num_hashes=1,pad_token_id=0,vocab_size=320,tie_word_embeddings=False,**kwargs)
transformers.configuration_reformer.ReformerConfig(self,attention_head_size=64,attn_layers=['local','lsh','local','lsh','local','lsh'],axial_norm_std=1.0,axial_pos_embds=True,axial_pos_shape=[64,64],axial_pos_embds_dim=[64,192],chunk_size_lm_head=0,eos_token_id=2,feed_forward_size=512,hash_seed=None,hidden_act='relu',hidden_dropout_prob=0.05,hidden_size=256,initializer_range=0.02,is_decoder=False,layer_norm_eps=1e-12,local_num_chunks_before=1,local_num_chunks_after=0,local_attention_probs_dropout_prob=0.05,local_attn_chunk_length=64,lsh_attn_chunk_length=64,lsh_attention_probs_dropout_prob=0.0,lsh_num_chunks_before=1,lsh_num_chunks_after=0,max_position_embeddings=4096,num_attention_heads=2,num_buckets=None,num_hashes=1,pad_token_id=0,vocab_size=320,tie_word_embeddings=False,**kwargs)
transformers.configuration_reformer.ReformerConfig.__init__(self,attention_head_size=64,attn_layers=['local','lsh','local','lsh','local','lsh'],axial_norm_std=1.0,axial_pos_embds=True,axial_pos_shape=[64,64],axial_pos_embds_dim=[64,192],chunk_size_lm_head=0,eos_token_id=2,feed_forward_size=512,hash_seed=None,hidden_act='relu',hidden_dropout_prob=0.05,hidden_size=256,initializer_range=0.02,is_decoder=False,layer_norm_eps=1e-12,local_num_chunks_before=1,local_num_chunks_after=0,local_attention_probs_dropout_prob=0.05,local_attn_chunk_length=64,lsh_attn_chunk_length=64,lsh_attention_probs_dropout_prob=0.0,lsh_num_chunks_before=1,lsh_num_chunks_after=0,max_position_embeddings=4096,num_attention_heads=2,num_buckets=None,num_hashes=1,pad_token_id=0,vocab_size=320,tie_word_embeddings=False,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/__init__.py----------------------------------------
A:transformers.__init__.logger->utils.logging.get_logger(__name__)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/convert_pytorch_checkpoint_to_tf2.py----------------------------------------
A:transformers.convert_pytorch_checkpoint_to_tf2.config_file->cached_path(config_shortcut_name, force_download=not use_cached_models)
A:transformers.convert_pytorch_checkpoint_to_tf2.config->config_class.from_json_file(config_file)
A:transformers.convert_pytorch_checkpoint_to_tf2.tf_model->load_pytorch_checkpoint_in_tf2_model(tf_model, pytorch_checkpoint_path)
A:transformers.convert_pytorch_checkpoint_to_tf2.pytorch_checkpoint_url->hf_bucket_url(pytorch_checkpoint_path, filename=WEIGHTS_NAME)
A:transformers.convert_pytorch_checkpoint_to_tf2.pytorch_checkpoint_path->cached_path(pytorch_checkpoint_url, force_download=not use_cached_models)
A:transformers.convert_pytorch_checkpoint_to_tf2.tfo->tf_model(tf_model.dummy_inputs, training=False)
A:transformers.convert_pytorch_checkpoint_to_tf2.state_dict->torch.load(pytorch_checkpoint_path, map_location='cpu')
A:transformers.convert_pytorch_checkpoint_to_tf2.pt_model->pt_model_class.from_pretrained(pretrained_model_name_or_path=None, config=config, state_dict=state_dict)
A:transformers.convert_pytorch_checkpoint_to_tf2.pto->pt_model(**pt_model.dummy_inputs)
A:transformers.convert_pytorch_checkpoint_to_tf2.np_pt->pto[0].numpy()
A:transformers.convert_pytorch_checkpoint_to_tf2.np_tf->tfo[0].numpy()
A:transformers.convert_pytorch_checkpoint_to_tf2.diff->numpy.amax(np.abs(np_pt - np_tf))
A:transformers.convert_pytorch_checkpoint_to_tf2.model_types->list(MODEL_CLASSES.keys())
A:transformers.convert_pytorch_checkpoint_to_tf2.model_shortcut_names_or_path->list(aws_model_maps.keys())
A:transformers.convert_pytorch_checkpoint_to_tf2.model_file->cached_path(model_shortcut_name, force_download=not use_cached_models)
A:transformers.convert_pytorch_checkpoint_to_tf2.parser->argparse.ArgumentParser()
A:transformers.convert_pytorch_checkpoint_to_tf2.args->argparse.ArgumentParser().parse_args()
transformers.convert_pytorch_checkpoint_to_tf2.convert_all_pt_checkpoints_to_tf(args_model_type,tf_dump_path,model_shortcut_names_or_path=None,config_shortcut_names_or_path=None,compare_with_pt_model=False,use_cached_models=False,remove_cached_files=False,only_convert_finetuned_models=False)
transformers.convert_pytorch_checkpoint_to_tf2.convert_pt_checkpoint_to_tf(model_type,pytorch_checkpoint_path,config_file,tf_dump_path,compare_with_pt_model=False,use_cached_models=True)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/modeling_encoder_decoder.py----------------------------------------
A:transformers.modeling_encoder_decoder.logger->utils.logging.get_logger(__name__)
A:transformers.modeling_encoder_decoder.config->configuration_encoder_decoder.EncoderDecoderConfig.from_encoder_decoder_configs(encoder.config, decoder.config, **kwargs)
A:transformers.modeling_encoder_decoder.encoder->modeling_auto.AutoModel.from_pretrained(encoder_pretrained_model_name_or_path, *model_args, **kwargs_encoder)
A:transformers.modeling_encoder_decoder.decoder->modeling_auto.AutoModelForCausalLM.from_pretrained(decoder_pretrained_model_name_or_path, **kwargs_decoder)
A:transformers.modeling_encoder_decoder.decoder_config->configuration_auto.AutoConfig.from_pretrained(decoder_pretrained_model_name_or_path)
A:transformers.modeling_encoder_decoder.encoder_outputs->self.encoder(input_ids=input_ids, attention_mask=attention_mask, inputs_embeds=inputs_embeds, return_dict=return_dict, **kwargs_encoder)
A:transformers.modeling_encoder_decoder.decoder_outputs->self.decoder(input_ids=decoder_input_ids, inputs_embeds=decoder_inputs_embeds, attention_mask=decoder_attention_mask, encoder_hidden_states=hidden_states, encoder_attention_mask=attention_mask, labels=labels, return_dict=return_dict, **kwargs_decoder)
A:transformers.modeling_encoder_decoder.decoder_inputs->self.decoder.prepare_inputs_for_generation(input_ids)
transformers.EncoderDecoderModel(self,config:Optional[PretrainedConfig]=None,encoder:Optional[PreTrainedModel]=None,decoder:Optional[PreTrainedModel]=None)
transformers.EncoderDecoderModel._reorder_cache(self,past,beam_idx)
transformers.EncoderDecoderModel.forward(self,input_ids=None,inputs_embeds=None,attention_mask=None,encoder_outputs=None,decoder_input_ids=None,decoder_attention_mask=None,decoder_inputs_embeds=None,labels=None,return_dict=None,**kwargs)
transformers.EncoderDecoderModel.from_encoder_decoder_pretrained(cls,encoder_pretrained_model_name_or_path:str=None,decoder_pretrained_model_name_or_path:str=None,*model_args,**kwargs)->PreTrainedModel
transformers.EncoderDecoderModel.get_decoder(self)
transformers.EncoderDecoderModel.get_encoder(self)
transformers.EncoderDecoderModel.get_input_embeddings(self)
transformers.EncoderDecoderModel.get_output_embeddings(self)
transformers.EncoderDecoderModel.prepare_inputs_for_generation(self,input_ids,past,attention_mask,encoder_outputs,**kwargs)
transformers.EncoderDecoderModel.tie_weights(self)
transformers.modeling_encoder_decoder.EncoderDecoderModel(self,config:Optional[PretrainedConfig]=None,encoder:Optional[PreTrainedModel]=None,decoder:Optional[PreTrainedModel]=None)
transformers.modeling_encoder_decoder.EncoderDecoderModel.__init__(self,config:Optional[PretrainedConfig]=None,encoder:Optional[PreTrainedModel]=None,decoder:Optional[PreTrainedModel]=None)
transformers.modeling_encoder_decoder.EncoderDecoderModel._reorder_cache(self,past,beam_idx)
transformers.modeling_encoder_decoder.EncoderDecoderModel.forward(self,input_ids=None,inputs_embeds=None,attention_mask=None,encoder_outputs=None,decoder_input_ids=None,decoder_attention_mask=None,decoder_inputs_embeds=None,labels=None,return_dict=None,**kwargs)
transformers.modeling_encoder_decoder.EncoderDecoderModel.from_encoder_decoder_pretrained(cls,encoder_pretrained_model_name_or_path:str=None,decoder_pretrained_model_name_or_path:str=None,*model_args,**kwargs)->PreTrainedModel
transformers.modeling_encoder_decoder.EncoderDecoderModel.get_decoder(self)
transformers.modeling_encoder_decoder.EncoderDecoderModel.get_encoder(self)
transformers.modeling_encoder_decoder.EncoderDecoderModel.get_input_embeddings(self)
transformers.modeling_encoder_decoder.EncoderDecoderModel.get_output_embeddings(self)
transformers.modeling_encoder_decoder.EncoderDecoderModel.prepare_inputs_for_generation(self,input_ids,past,attention_mask,encoder_outputs,**kwargs)
transformers.modeling_encoder_decoder.EncoderDecoderModel.tie_weights(self)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/modeling_tf_bert.py----------------------------------------
A:transformers.modeling_tf_bert.logger->utils.logging.get_logger(__name__)
A:transformers.modeling_tf_bert.self.position_embeddings->tensorflow.keras.layers.Embedding(config.max_position_embeddings, config.hidden_size, embeddings_initializer=get_initializer(self.initializer_range), name='position_embeddings')
A:transformers.modeling_tf_bert.self.token_type_embeddings->tensorflow.keras.layers.Embedding(config.type_vocab_size, config.hidden_size, embeddings_initializer=get_initializer(self.initializer_range), name='token_type_embeddings')
A:transformers.modeling_tf_bert.self.LayerNorm->tensorflow.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name='LayerNorm')
A:transformers.modeling_tf_bert.self.dropout->tensorflow.keras.layers.Dropout(config.hidden_dropout_prob)
A:transformers.modeling_tf_bert.self.word_embeddings->self.add_weight('weight', shape=[self.vocab_size, self.hidden_size], initializer=get_initializer(self.initializer_range))
A:transformers.modeling_tf_bert.input_shape->shape_list(input_ids)
A:transformers.modeling_tf_bert.token_type_ids->inputs.get('token_type_ids', token_type_ids)
A:transformers.modeling_tf_bert.inputs_embeds->inputs.get('inputs_embeds', inputs_embeds)
A:transformers.modeling_tf_bert.position_embeddings->tensorflow.cast(self.position_embeddings(position_ids), inputs_embeds.dtype)
A:transformers.modeling_tf_bert.token_type_embeddings->tensorflow.cast(self.token_type_embeddings(token_type_ids), inputs_embeds.dtype)
A:transformers.modeling_tf_bert.embeddings->self.dropout(embeddings, training=training)
A:transformers.modeling_tf_bert.x->tensorflow.reshape(x, (batch_size, -1, self.num_attention_heads, self.attention_head_size))
A:transformers.modeling_tf_bert.logits->self.qa_outputs(sequence_output)
A:transformers.modeling_tf_bert.self.attention_head_size->int(config.hidden_size / config.num_attention_heads)
A:transformers.modeling_tf_bert.self.query->tensorflow.keras.layers.Dense(self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name='query')
A:transformers.modeling_tf_bert.self.key->tensorflow.keras.layers.Dense(self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name='key')
A:transformers.modeling_tf_bert.self.value->tensorflow.keras.layers.Dense(self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name='value')
A:transformers.modeling_tf_bert.mixed_query_layer->self.query(hidden_states)
A:transformers.modeling_tf_bert.mixed_key_layer->self.key(hidden_states)
A:transformers.modeling_tf_bert.mixed_value_layer->self.value(hidden_states)
A:transformers.modeling_tf_bert.query_layer->self.transpose_for_scores(mixed_query_layer, batch_size)
A:transformers.modeling_tf_bert.key_layer->self.transpose_for_scores(mixed_key_layer, batch_size)
A:transformers.modeling_tf_bert.value_layer->self.transpose_for_scores(mixed_value_layer, batch_size)
A:transformers.modeling_tf_bert.attention_scores->tensorflow.matmul(query_layer, key_layer, transpose_b=True)
A:transformers.modeling_tf_bert.dk->tensorflow.cast(shape_list(key_layer)[-1], attention_scores.dtype)
A:transformers.modeling_tf_bert.attention_probs->self.dropout(attention_probs, training=training)
A:transformers.modeling_tf_bert.context_layer->tensorflow.reshape(context_layer, (batch_size, -1, self.all_head_size))
A:transformers.modeling_tf_bert.self.dense->tensorflow.keras.layers.Dense(config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name='dense')
A:transformers.modeling_tf_bert.hidden_states->self.input_embeddings(hidden_states, mode='linear')
A:transformers.modeling_tf_bert.self.self_attention->TFBertSelfAttention(config, name='self')
A:transformers.modeling_tf_bert.self.dense_output->TFBertSelfOutput(config, name='output')
A:transformers.modeling_tf_bert.self_outputs->self.self_attention(input_tensor, attention_mask, head_mask, output_attentions, training=training)
A:transformers.modeling_tf_bert.attention_output->self.dense_output(self_outputs[0], input_tensor, training=training)
A:transformers.modeling_tf_bert.self.attention->TFBertAttention(config, name='attention')
A:transformers.modeling_tf_bert.self.intermediate->TFBertIntermediate(config, name='intermediate')
A:transformers.modeling_tf_bert.self.bert_output->TFBertOutput(config, name='output')
A:transformers.modeling_tf_bert.attention_outputs->self.attention(hidden_states, attention_mask, head_mask, output_attentions, training=training)
A:transformers.modeling_tf_bert.intermediate_output->self.intermediate(attention_output)
A:transformers.modeling_tf_bert.layer_output->self.bert_output(intermediate_output, attention_output, training=training)
A:transformers.modeling_tf_bert.layer_outputs->layer_module(hidden_states, attention_mask, head_mask[i], output_attentions, training=training)
A:transformers.modeling_tf_bert.pooled_output->self.dropout(pooled_output, training=training)
A:transformers.modeling_tf_bert.self.transform->TFBertPredictionHeadTransform(config, name='transform')
A:transformers.modeling_tf_bert.self.bias->self.add_weight(shape=(self.vocab_size,), initializer='zeros', trainable=True, name='bias')
A:transformers.modeling_tf_bert.self.predictions->TFBertLMPredictionHead(config, input_embeddings, name='predictions')
A:transformers.modeling_tf_bert.prediction_scores->self.mlm(sequence_output, training=training)
A:transformers.modeling_tf_bert.self.seq_relationship->tensorflow.keras.layers.Dense(2, kernel_initializer=get_initializer(config.initializer_range), name='seq_relationship')
A:transformers.modeling_tf_bert.seq_relationship_score->self.nsp(pooled_output)
A:transformers.modeling_tf_bert.self.embeddings->TFBertEmbeddings(config, name='embeddings')
A:transformers.modeling_tf_bert.self.encoder->TFBertEncoder(config, name='encoder')
A:transformers.modeling_tf_bert.self.pooler->TFBertPooler(config, name='pooler')
A:transformers.modeling_tf_bert.input_ids->inputs.get('input_ids')
A:transformers.modeling_tf_bert.attention_mask->inputs.get('attention_mask', attention_mask)
A:transformers.modeling_tf_bert.position_ids->inputs.get('position_ids', position_ids)
A:transformers.modeling_tf_bert.head_mask->inputs.get('head_mask', head_mask)
A:transformers.modeling_tf_bert.output_attentions->inputs.get('output_attentions', output_attentions)
A:transformers.modeling_tf_bert.output_hidden_states->inputs.get('output_hidden_states', output_hidden_states)
A:transformers.modeling_tf_bert.return_dict->inputs.get('return_dict', return_dict)
A:transformers.modeling_tf_bert.embedding_output->self.embeddings(input_ids, position_ids, token_type_ids, inputs_embeds, training=training)
A:transformers.modeling_tf_bert.extended_attention_mask->tensorflow.cast(extended_attention_mask, embedding_output.dtype)
A:transformers.modeling_tf_bert.encoder_outputs->self.encoder(embedding_output, extended_attention_mask, head_mask, output_attentions, output_hidden_states, return_dict, training=training)
A:transformers.modeling_tf_bert.self.bert->TFBertMainLayer(config, name='bert')
A:transformers.modeling_tf_bert.outputs->self.bert(inputs, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, training=training)
A:transformers.modeling_tf_bert.self.nsp->TFBertNSPHead(config, name='nsp___cls')
A:transformers.modeling_tf_bert.self.mlm->TFBertMLMHead(config, self.bert.embeddings, name='mlm___cls')
A:transformers.modeling_tf_bert.labels->inputs.pop('labels', labels)
A:transformers.modeling_tf_bert.loss->self.compute_loss(labels, (start_logits, end_logits))
A:transformers.modeling_tf_bert.self.classifier->tensorflow.keras.layers.Dense(config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name='classifier')
A:transformers.modeling_tf_bert.reshaped_logits->tensorflow.reshape(logits, (-1, num_choices))
A:transformers.modeling_tf_bert.sequence_output->self.dropout(sequence_output, training=training)
A:transformers.modeling_tf_bert.self.qa_outputs->tensorflow.keras.layers.Dense(config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name='qa_outputs')
A:transformers.modeling_tf_bert.start_positions->inputs.pop('start_positions', start_positions)
A:transformers.modeling_tf_bert.end_positions->inputs.pop('end_positions', start_positions)
A:transformers.modeling_tf_bert.(start_logits, end_logits)->tensorflow.split(logits, 2, axis=-1)
A:transformers.modeling_tf_bert.start_logits->tensorflow.squeeze(start_logits, axis=-1)
A:transformers.modeling_tf_bert.end_logits->tensorflow.squeeze(end_logits, axis=-1)
transformers.TFBertEmbeddings(self,config,**kwargs)
transformers.TFBertEmbeddings._embedding(self,input_ids,position_ids,token_type_ids,inputs_embeds,training=False)
transformers.TFBertEmbeddings._linear(self,inputs)
transformers.TFBertEmbeddings.build(self,input_shape)
transformers.TFBertEmbeddings.call(self,input_ids=None,position_ids=None,token_type_ids=None,inputs_embeds=None,mode='embedding',training=False)
transformers.TFBertForMaskedLM(self,config,*inputs,**kwargs)
transformers.TFBertForMaskedLM.call(self,inputs=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.TFBertForMaskedLM.get_output_embeddings(self)
transformers.TFBertForMultipleChoice(self,config,*inputs,**kwargs)
transformers.TFBertForMultipleChoice.call(self,inputs,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.TFBertForMultipleChoice.dummy_inputs(self)
transformers.TFBertForNextSentencePrediction(self,config,*inputs,**kwargs)
transformers.TFBertForNextSentencePrediction.call(self,inputs,**kwargs)
transformers.TFBertForPreTraining(self,config,*inputs,**kwargs)
transformers.TFBertForPreTraining.call(self,inputs,**kwargs)
transformers.TFBertForPreTraining.get_output_embeddings(self)
transformers.TFBertForPreTrainingOutput(ModelOutput)
transformers.TFBertForQuestionAnswering(self,config,*inputs,**kwargs)
transformers.TFBertForQuestionAnswering.call(self,inputs=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,start_positions=None,end_positions=None,training=False)
transformers.TFBertForSequenceClassification(self,config,*inputs,**kwargs)
transformers.TFBertForSequenceClassification.call(self,inputs=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.TFBertForTokenClassification(self,config,*inputs,**kwargs)
transformers.TFBertForTokenClassification.call(self,inputs=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.TFBertLMHeadModel(self,config,*inputs,**kwargs)
transformers.TFBertLMHeadModel.call(self,inputs=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.TFBertLMHeadModel.get_output_embeddings(self)
transformers.TFBertMainLayer(self,config,**kwargs)
transformers.TFBertMainLayer._prune_heads(self,heads_to_prune)
transformers.TFBertMainLayer.call(self,inputs,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False)
transformers.TFBertMainLayer.get_input_embeddings(self)
transformers.TFBertMainLayer.set_input_embeddings(self,value)
transformers.TFBertModel(self,config,*inputs,**kwargs)
transformers.TFBertModel.call(self,inputs,**kwargs)
transformers.TFBertPreTrainedModel(TFPreTrainedModel)
transformers.modeling_tf_bert.TFBertAttention(self,config,**kwargs)
transformers.modeling_tf_bert.TFBertAttention.__init__(self,config,**kwargs)
transformers.modeling_tf_bert.TFBertAttention.call(self,input_tensor,attention_mask,head_mask,output_attentions,training=False)
transformers.modeling_tf_bert.TFBertAttention.prune_heads(self,heads)
transformers.modeling_tf_bert.TFBertEmbeddings(self,config,**kwargs)
transformers.modeling_tf_bert.TFBertEmbeddings.__init__(self,config,**kwargs)
transformers.modeling_tf_bert.TFBertEmbeddings._embedding(self,input_ids,position_ids,token_type_ids,inputs_embeds,training=False)
transformers.modeling_tf_bert.TFBertEmbeddings._linear(self,inputs)
transformers.modeling_tf_bert.TFBertEmbeddings.build(self,input_shape)
transformers.modeling_tf_bert.TFBertEmbeddings.call(self,input_ids=None,position_ids=None,token_type_ids=None,inputs_embeds=None,mode='embedding',training=False)
transformers.modeling_tf_bert.TFBertEncoder(self,config,**kwargs)
transformers.modeling_tf_bert.TFBertEncoder.__init__(self,config,**kwargs)
transformers.modeling_tf_bert.TFBertEncoder.call(self,hidden_states,attention_mask,head_mask,output_attentions,output_hidden_states,return_dict,training=False)
transformers.modeling_tf_bert.TFBertForMaskedLM(self,config,*inputs,**kwargs)
transformers.modeling_tf_bert.TFBertForMaskedLM.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_bert.TFBertForMaskedLM.call(self,inputs=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.modeling_tf_bert.TFBertForMaskedLM.get_output_embeddings(self)
transformers.modeling_tf_bert.TFBertForMultipleChoice(self,config,*inputs,**kwargs)
transformers.modeling_tf_bert.TFBertForMultipleChoice.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_bert.TFBertForMultipleChoice.call(self,inputs,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.modeling_tf_bert.TFBertForMultipleChoice.dummy_inputs(self)
transformers.modeling_tf_bert.TFBertForNextSentencePrediction(self,config,*inputs,**kwargs)
transformers.modeling_tf_bert.TFBertForNextSentencePrediction.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_bert.TFBertForNextSentencePrediction.call(self,inputs,**kwargs)
transformers.modeling_tf_bert.TFBertForPreTraining(self,config,*inputs,**kwargs)
transformers.modeling_tf_bert.TFBertForPreTraining.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_bert.TFBertForPreTraining.call(self,inputs,**kwargs)
transformers.modeling_tf_bert.TFBertForPreTraining.get_output_embeddings(self)
transformers.modeling_tf_bert.TFBertForPreTrainingOutput(ModelOutput)
transformers.modeling_tf_bert.TFBertForQuestionAnswering(self,config,*inputs,**kwargs)
transformers.modeling_tf_bert.TFBertForQuestionAnswering.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_bert.TFBertForQuestionAnswering.call(self,inputs=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,start_positions=None,end_positions=None,training=False)
transformers.modeling_tf_bert.TFBertForSequenceClassification(self,config,*inputs,**kwargs)
transformers.modeling_tf_bert.TFBertForSequenceClassification.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_bert.TFBertForSequenceClassification.call(self,inputs=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.modeling_tf_bert.TFBertForTokenClassification(self,config,*inputs,**kwargs)
transformers.modeling_tf_bert.TFBertForTokenClassification.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_bert.TFBertForTokenClassification.call(self,inputs=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.modeling_tf_bert.TFBertIntermediate(self,config,**kwargs)
transformers.modeling_tf_bert.TFBertIntermediate.__init__(self,config,**kwargs)
transformers.modeling_tf_bert.TFBertIntermediate.call(self,hidden_states)
transformers.modeling_tf_bert.TFBertLMHeadModel(self,config,*inputs,**kwargs)
transformers.modeling_tf_bert.TFBertLMHeadModel.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_bert.TFBertLMHeadModel.call(self,inputs=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.modeling_tf_bert.TFBertLMHeadModel.get_output_embeddings(self)
transformers.modeling_tf_bert.TFBertLMPredictionHead(self,config,input_embeddings,**kwargs)
transformers.modeling_tf_bert.TFBertLMPredictionHead.__init__(self,config,input_embeddings,**kwargs)
transformers.modeling_tf_bert.TFBertLMPredictionHead.build(self,input_shape)
transformers.modeling_tf_bert.TFBertLMPredictionHead.call(self,hidden_states)
transformers.modeling_tf_bert.TFBertLayer(self,config,**kwargs)
transformers.modeling_tf_bert.TFBertLayer.__init__(self,config,**kwargs)
transformers.modeling_tf_bert.TFBertLayer.call(self,hidden_states,attention_mask,head_mask,output_attentions,training=False)
transformers.modeling_tf_bert.TFBertMLMHead(self,config,input_embeddings,**kwargs)
transformers.modeling_tf_bert.TFBertMLMHead.__init__(self,config,input_embeddings,**kwargs)
transformers.modeling_tf_bert.TFBertMLMHead.call(self,sequence_output)
transformers.modeling_tf_bert.TFBertMainLayer(self,config,**kwargs)
transformers.modeling_tf_bert.TFBertMainLayer.__init__(self,config,**kwargs)
transformers.modeling_tf_bert.TFBertMainLayer._prune_heads(self,heads_to_prune)
transformers.modeling_tf_bert.TFBertMainLayer.call(self,inputs,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False)
transformers.modeling_tf_bert.TFBertMainLayer.get_input_embeddings(self)
transformers.modeling_tf_bert.TFBertMainLayer.set_input_embeddings(self,value)
transformers.modeling_tf_bert.TFBertModel(self,config,*inputs,**kwargs)
transformers.modeling_tf_bert.TFBertModel.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_bert.TFBertModel.call(self,inputs,**kwargs)
transformers.modeling_tf_bert.TFBertNSPHead(self,config,**kwargs)
transformers.modeling_tf_bert.TFBertNSPHead.__init__(self,config,**kwargs)
transformers.modeling_tf_bert.TFBertNSPHead.call(self,pooled_output)
transformers.modeling_tf_bert.TFBertOutput(self,config,**kwargs)
transformers.modeling_tf_bert.TFBertOutput.__init__(self,config,**kwargs)
transformers.modeling_tf_bert.TFBertOutput.call(self,hidden_states,input_tensor,training=False)
transformers.modeling_tf_bert.TFBertPooler(self,config,**kwargs)
transformers.modeling_tf_bert.TFBertPooler.__init__(self,config,**kwargs)
transformers.modeling_tf_bert.TFBertPooler.call(self,hidden_states)
transformers.modeling_tf_bert.TFBertPreTrainedModel(TFPreTrainedModel)
transformers.modeling_tf_bert.TFBertPredictionHeadTransform(self,config,**kwargs)
transformers.modeling_tf_bert.TFBertPredictionHeadTransform.__init__(self,config,**kwargs)
transformers.modeling_tf_bert.TFBertPredictionHeadTransform.call(self,hidden_states)
transformers.modeling_tf_bert.TFBertSelfAttention(self,config,**kwargs)
transformers.modeling_tf_bert.TFBertSelfAttention.__init__(self,config,**kwargs)
transformers.modeling_tf_bert.TFBertSelfAttention.call(self,hidden_states,attention_mask,head_mask,output_attentions,training=False)
transformers.modeling_tf_bert.TFBertSelfAttention.transpose_for_scores(self,x,batch_size)
transformers.modeling_tf_bert.TFBertSelfOutput(self,config,**kwargs)
transformers.modeling_tf_bert.TFBertSelfOutput.__init__(self,config,**kwargs)
transformers.modeling_tf_bert.TFBertSelfOutput.call(self,hidden_states,input_tensor,training=False)
transformers.modeling_tf_bert.gelu(x)
transformers.modeling_tf_bert.gelu_new(x)
transformers.modeling_tf_bert.swish(x)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/training_args_tf.py----------------------------------------
A:transformers.training_args_tf.logger->utils.logging.get_logger(__name__)
A:transformers.training_args_tf.gpus->tensorflow.config.list_physical_devices('GPU')
A:transformers.training_args_tf.strategy->tensorflow.distribute.MirroredStrategy()
A:transformers.training_args_tf.tpu->tensorflow.distribute.cluster_resolver.TPUClusterResolver()
transformers.TFTrainingArguments(TrainingArguments)
transformers.TFTrainingArguments._setup_strategy(self)->Tuple['tf.distribute.Strategy', int]
transformers.TFTrainingArguments.eval_batch_size(self)->int
transformers.TFTrainingArguments.n_gpu(self)->int
transformers.TFTrainingArguments.n_replicas(self)->int
transformers.TFTrainingArguments.strategy(self)->'tf.distribute.Strategy'
transformers.TFTrainingArguments.train_batch_size(self)->int
transformers.training_args_tf.TFTrainingArguments(TrainingArguments)
transformers.training_args_tf.TFTrainingArguments._setup_strategy(self)->Tuple['tf.distribute.Strategy', int]
transformers.training_args_tf.TFTrainingArguments.eval_batch_size(self)->int
transformers.training_args_tf.TFTrainingArguments.n_gpu(self)->int
transformers.training_args_tf.TFTrainingArguments.n_replicas(self)->int
transformers.training_args_tf.TFTrainingArguments.strategy(self)->'tf.distribute.Strategy'
transformers.training_args_tf.TFTrainingArguments.train_batch_size(self)->int


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/convert_bert_original_tf2_checkpoint_to_pytorch.py----------------------------------------
A:transformers.convert_bert_original_tf2_checkpoint_to_pytorch.logger->utils.logging.get_logger(__name__)
A:transformers.convert_bert_original_tf2_checkpoint_to_pytorch.tf_path->os.path.abspath(tf_checkpoint_path)
A:transformers.convert_bert_original_tf2_checkpoint_to_pytorch.init_vars->tensorflow.train.list_variables(tf_path)
A:transformers.convert_bert_original_tf2_checkpoint_to_pytorch.name->full_name.split('/')
A:transformers.convert_bert_original_tf2_checkpoint_to_pytorch.array->array.transpose().transpose()
A:transformers.convert_bert_original_tf2_checkpoint_to_pytorch.layer_num->int(m_name.split('-')[-1])
A:transformers.convert_bert_original_tf2_checkpoint_to_pytorch.pointer->getattr(pointer, 'weight')
A:transformers.convert_bert_original_tf2_checkpoint_to_pytorch.trace->'.'.join(trace)
A:transformers.convert_bert_original_tf2_checkpoint_to_pytorch.pointer.data->torch.from_numpy(array)
A:transformers.convert_bert_original_tf2_checkpoint_to_pytorch.config->transformers.BertConfig.from_json_file(config_path)
A:transformers.convert_bert_original_tf2_checkpoint_to_pytorch.model->BertModel(config)
A:transformers.convert_bert_original_tf2_checkpoint_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.convert_bert_original_tf2_checkpoint_to_pytorch.args->argparse.ArgumentParser().parse_args()
transformers.convert_bert_original_tf2_checkpoint_to_pytorch.convert_tf2_checkpoint_to_pytorch(tf_checkpoint_path,config_path,pytorch_dump_path)
transformers.convert_bert_original_tf2_checkpoint_to_pytorch.load_tf2_weights_in_bert(model,tf_checkpoint_path,config)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/modeling_transfo_xl.py----------------------------------------
A:transformers.modeling_transfo_xl.logger->utils.logging.get_logger(__name__)
A:transformers.modeling_transfo_xl.tf_to_pt_map->build_tf_to_pytorch_map(model, config)
A:transformers.modeling_transfo_xl.init_vars->tensorflow.train.list_variables(tf_path)
A:transformers.modeling_transfo_xl.array->numpy.transpose(array)
A:transformers.modeling_transfo_xl.p_i.data->torch.from_numpy(arr_i)
A:transformers.modeling_transfo_xl.pointer.data->torch.from_numpy(array)
A:transformers.modeling_transfo_xl.sinusoid_inp->torch.ger(pos_seq, self.inv_freq)
A:transformers.modeling_transfo_xl.pos_emb->self.drop(pos_emb)
A:transformers.modeling_transfo_xl.self.CoreNet->torch.nn.Sequential(nn.Linear(d_model, d_inner), nn.ReLU(inplace=True), nn.Dropout(dropout), nn.Linear(d_inner, d_model), nn.Dropout(dropout))
A:transformers.modeling_transfo_xl.self.layer_norm->torch.nn.LayerNorm(d_model, eps=layer_norm_epsilon)
A:transformers.modeling_transfo_xl.core_out->core_out.transpose(0, 1).contiguous().transpose(0, 1).contiguous()
A:transformers.modeling_transfo_xl.output->self.layer_norm(inp + core_out)
A:transformers.modeling_transfo_xl.self.qkv_net->torch.nn.Linear(d_model, 3 * n_head * d_head, bias=False)
A:transformers.modeling_transfo_xl.self.drop->torch.nn.Dropout(config.dropout)
A:transformers.modeling_transfo_xl.self.dropatt->torch.nn.Dropout(dropatt)
A:transformers.modeling_transfo_xl.self.o_net->torch.nn.Linear(n_head * d_head, d_model, bias=False)
A:transformers.modeling_transfo_xl.self.r_r_bias->torch.nn.Parameter(torch.FloatTensor(self.n_head, self.d_head))
A:transformers.modeling_transfo_xl.self.r_w_bias->torch.nn.Parameter(torch.FloatTensor(self.n_head, self.d_head))
A:transformers.modeling_transfo_xl.self.r_net->torch.nn.Linear(self.d_model, self.n_head * self.d_head, bias=False)
A:transformers.modeling_transfo_xl.zero_pad->torch.zeros(zero_pad_shape, device=x.device, dtype=x.dtype)
A:transformers.modeling_transfo_xl.x_padded->x_padded.view(*x_padded_shape).view(*x_padded_shape)
A:transformers.modeling_transfo_xl.x->x_padded[1:].view_as(x)
A:transformers.modeling_transfo_xl.cat->torch.cat([mems[i], hids[i]], dim=0)
A:transformers.modeling_transfo_xl.w_heads->self.qkv_net(w)
A:transformers.modeling_transfo_xl.r_head_k->r_head_k.view(rlen, self.n_head, self.d_head).view(rlen, self.n_head, self.d_head)
A:transformers.modeling_transfo_xl.(w_head_q, w_head_k, w_head_v)->torch.chunk(w_heads, 3, dim=-1)
A:transformers.modeling_transfo_xl.klen->w_head_k.view(klen, bsz, self.n_head, self.d_head).size(0)
A:transformers.modeling_transfo_xl.w_head_q->w_head_q.view(qlen, bsz, self.n_head, self.d_head).view(qlen, bsz, self.n_head, self.d_head)
A:transformers.modeling_transfo_xl.w_head_k->w_head_k.view(klen, bsz, self.n_head, self.d_head).view(klen, bsz, self.n_head, self.d_head)
A:transformers.modeling_transfo_xl.w_head_v->w_head_v.view(klen, bsz, self.n_head, self.d_head).view(klen, bsz, self.n_head, self.d_head)
A:transformers.modeling_transfo_xl.AC->torch.einsum('ibnd,jbnd->ijbn', (rw_head_q, w_head_k))
A:transformers.modeling_transfo_xl.BD->self._rel_shift(BD)
A:transformers.modeling_transfo_xl.attn_score->attn_score.float().masked_fill(attn_mask[:, :, :, None], -1e+30).type_as(attn_score).float().masked_fill(attn_mask[:, :, :, None], -1e+30).type_as(attn_score)
A:transformers.modeling_transfo_xl.attn_prob->self.dropatt(attn_prob)
A:transformers.modeling_transfo_xl.attn_vec->attn_vec.contiguous().view(attn_vec.size(0), attn_vec.size(1), self.n_head * self.d_head).contiguous().view(attn_vec.size(0), attn_vec.size(1), self.n_head * self.d_head)
A:transformers.modeling_transfo_xl.attn_out->self.drop(attn_out)
A:transformers.modeling_transfo_xl.self.dec_attn->RelPartialLearnableMultiHeadAttn(n_head, d_model, d_head, dropout, layer_norm_epsilon=layer_norm_epsilon, **kwargs)
A:transformers.modeling_transfo_xl.self.pos_ff->PositionwiseFF(d_model, d_inner, dropout, pre_lnorm=kwargs.get('pre_lnorm'), layer_norm_epsilon=layer_norm_epsilon)
A:transformers.modeling_transfo_xl.attn_outputs->self.dec_attn(dec_inp, r, attn_mask=dec_attn_mask, mems=mems, head_mask=head_mask, output_attentions=output_attentions)
A:transformers.modeling_transfo_xl.ff_output->self.pos_ff(attn_outputs[0])
A:transformers.modeling_transfo_xl.self.emb_layers->torch.nn.ModuleList()
A:transformers.modeling_transfo_xl.self.emb_projs->torch.nn.ParameterList()
A:transformers.modeling_transfo_xl.embed->torch.zeros([inp_flat.size(0), self.d_proj], dtype=param.dtype, device=param.device).view(embed_shape)
A:transformers.modeling_transfo_xl.param->next(self.parameters())
A:transformers.modeling_transfo_xl.inp_flat->inp.view(-1)
A:transformers.modeling_transfo_xl.emb_flat->torch.zeros([inp_flat.size(0), self.d_proj], dtype=param.dtype, device=param.device)
A:transformers.modeling_transfo_xl.indices_i->mask_i.nonzero().squeeze()
A:transformers.modeling_transfo_xl.emb_i->torch.nn.functional.linear(emb_i, self.emb_projs[i])
A:transformers.modeling_transfo_xl.base_model->getattr(self, self.base_model_prefix, self)
A:transformers.modeling_transfo_xl.(new_num_tokens_layer, layer)->self._get_new_num_tokens_layer(new_num_tokens, layer)
A:transformers.modeling_transfo_xl.model_embeds->getattr(self, self.base_model_prefix, self)._resize_token_embeddings(new_num_tokens_layer, layer)
A:transformers.modeling_transfo_xl.new_embedding_shapes->self._get_embedding_shapes()
A:transformers.modeling_transfo_xl.embeddings->self.get_input_embeddings()
A:transformers.modeling_transfo_xl.new_embeddings_layer->self._get_resized_embeddings(embeddings.emb_layers[layer], new_num_tokens)
A:transformers.modeling_transfo_xl.embeddings.cutoffs[i]->sum(new_embedding_shapes[:i + 1])
A:transformers.modeling_transfo_xl.self.word_emb->AdaptiveEmbedding(config.vocab_size, config.d_embed, config.d_model, config.cutoffs, div_val=config.div_val)
A:transformers.modeling_transfo_xl.self.layers->torch.nn.ModuleList()
A:transformers.modeling_transfo_xl.self.pos_emb->PositionalEmbedding(self.d_model)
A:transformers.modeling_transfo_xl.empty->torch.zeros(self.mem_len, bsz, self.config.d_model, dtype=param.dtype, device=param.device)
A:transformers.modeling_transfo_xl.beg_idx->max(0, end_idx - self.mem_len)
A:transformers.modeling_transfo_xl.input_ids->input_ids.transpose(0, 1).contiguous().transpose(0, 1).contiguous()
A:transformers.modeling_transfo_xl.(qlen, bsz)->input_ids.transpose(0, 1).contiguous().transpose(0, 1).contiguous().size()
A:transformers.modeling_transfo_xl.inputs_embeds->inputs_embeds.transpose(0, 1).contiguous().transpose(0, 1).contiguous()
A:transformers.modeling_transfo_xl.mems->self.init_mems(bsz)
A:transformers.modeling_transfo_xl.head_mask->head_mask.to(dtype=next(self.parameters()).dtype).to(dtype=next(self.parameters()).dtype)
A:transformers.modeling_transfo_xl.word_emb->self.word_emb(input_ids)
A:transformers.modeling_transfo_xl.all_ones->self.word_emb(input_ids).new_ones((qlen, klen), dtype=torch.uint8)
A:transformers.modeling_transfo_xl.pos_seq->torch.arange(klen - 1, -1, -1.0, device=word_emb.device, dtype=word_emb.dtype)
A:transformers.modeling_transfo_xl.layer_outputs->layer(core_out, pos_emb, dec_attn_mask=dec_attn_mask, mems=mems_i, head_mask=head_mask[i], output_attentions=output_attentions)
A:transformers.modeling_transfo_xl.new_mems->self._update_mems(hids, mems, mlen, qlen)
A:transformers.modeling_transfo_xl.hids->tuple((t.transpose(0, 1).contiguous() for t in hids))
A:transformers.modeling_transfo_xl.attentions->tuple((t.permute(2, 3, 0, 1).contiguous() for t in attentions))
A:transformers.modeling_transfo_xl.self.transformer->TransfoXLModel(config)
A:transformers.modeling_transfo_xl.self.crit->ProjectedAdaptiveLogSoftmax(config.vocab_size, config.d_embed, config.d_model, config.cutoffs, div_val=config.div_val)
A:transformers.modeling_transfo_xl.self.crit.out_projs[i]->torch.nn.Parameter(self.transformer.word_emb.emb_projs[i].clone())
A:transformers.modeling_transfo_xl.transformer_outputs->self.transformer(input_ids, mems=mems, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.modeling_transfo_xl.softmax_output->self.crit(pred_hid, labels)
A:transformers.modeling_transfo_xl.inputs['input_ids']->input_ids[:, -1].unsqueeze(-1)
A:transformers.modeling_transfo_xl.new_cutoffs->super()._resize_cutoffs(new_num_tokens, new_emb_size, new_embedding_shapes, layer)
transformers.AdaptiveEmbedding(self,n_token,d_embed,d_proj,cutoffs,div_val=1,sample_softmax=False)
transformers.AdaptiveEmbedding.forward(self,inp)
transformers.TransfoXLLMHeadModel(self,config)
transformers.TransfoXLLMHeadModel._resize_cutoffs(self,new_num_tokens,new_emb_size,new_embedding_shapes,layer)
transformers.TransfoXLLMHeadModel.forward(self,input_ids=None,mems=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.TransfoXLLMHeadModel.get_output_embeddings(self)
transformers.TransfoXLLMHeadModel.init_mems(self,bsz)
transformers.TransfoXLLMHeadModel.prepare_inputs_for_generation(self,input_ids,past,**model_kwargs)
transformers.TransfoXLLMHeadModel.reset_length(self,tgt_len,ext_len,mem_len)
transformers.TransfoXLLMHeadModel.tie_weights(self)
transformers.TransfoXLLMHeadModelOutput(ModelOutput)
transformers.TransfoXLLMHeadModelOutput.logits(self)
transformers.TransfoXLModel(self,config)
transformers.TransfoXLModel._prune_heads(self,heads)
transformers.TransfoXLModel._update_mems(self,hids,mems,mlen,qlen)
transformers.TransfoXLModel.backward_compatible(self)
transformers.TransfoXLModel.forward(self,input_ids=None,mems=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.TransfoXLModel.get_input_embeddings(self)
transformers.TransfoXLModel.init_mems(self,bsz)
transformers.TransfoXLModel.reset_length(self,tgt_len,ext_len,mem_len)
transformers.TransfoXLModel.set_input_embeddings(self,new_embeddings)
transformers.TransfoXLModelOutput(ModelOutput)
transformers.TransfoXLPreTrainedModel(PreTrainedModel)
transformers.TransfoXLPreTrainedModel._get_embedding_shapes(self)
transformers.TransfoXLPreTrainedModel._get_new_num_tokens_layer(self,new_num_tokens,layer)
transformers.TransfoXLPreTrainedModel._init_bias(self,bias)
transformers.TransfoXLPreTrainedModel._init_weight(self,weight)
transformers.TransfoXLPreTrainedModel._init_weights(self,m)
transformers.TransfoXLPreTrainedModel._resize_cutoffs(self,new_num_tokens,new_emb_size,new_embedding_shapes,layer)
transformers.TransfoXLPreTrainedModel._resize_token_embeddings(self,new_num_tokens,layer=-1)
transformers.TransfoXLPreTrainedModel.resize_token_embeddings(self,new_num_tokens:Optional[int]=None,layer:Optional[int]=-1)
transformers.load_tf_weights_in_transfo_xl(model,config,tf_path)
transformers.modeling_transfo_xl.AdaptiveEmbedding(self,n_token,d_embed,d_proj,cutoffs,div_val=1,sample_softmax=False)
transformers.modeling_transfo_xl.AdaptiveEmbedding.__init__(self,n_token,d_embed,d_proj,cutoffs,div_val=1,sample_softmax=False)
transformers.modeling_transfo_xl.AdaptiveEmbedding.forward(self,inp)
transformers.modeling_transfo_xl.PositionalEmbedding(self,demb)
transformers.modeling_transfo_xl.PositionalEmbedding.__init__(self,demb)
transformers.modeling_transfo_xl.PositionalEmbedding.forward(self,pos_seq,bsz=None)
transformers.modeling_transfo_xl.PositionwiseFF(self,d_model,d_inner,dropout,pre_lnorm=False,layer_norm_epsilon=1e-05)
transformers.modeling_transfo_xl.PositionwiseFF.__init__(self,d_model,d_inner,dropout,pre_lnorm=False,layer_norm_epsilon=1e-05)
transformers.modeling_transfo_xl.PositionwiseFF.forward(self,inp)
transformers.modeling_transfo_xl.RelPartialLearnableDecoderLayer(self,n_head,d_model,d_head,d_inner,dropout,layer_norm_epsilon=1e-05,**kwargs)
transformers.modeling_transfo_xl.RelPartialLearnableDecoderLayer.__init__(self,n_head,d_model,d_head,d_inner,dropout,layer_norm_epsilon=1e-05,**kwargs)
transformers.modeling_transfo_xl.RelPartialLearnableDecoderLayer.forward(self,dec_inp,r,dec_attn_mask=None,mems=None,head_mask=None,output_attentions=False)
transformers.modeling_transfo_xl.RelPartialLearnableMultiHeadAttn(self,n_head,d_model,d_head,dropout,dropatt=0,tgt_len=None,ext_len=None,mem_len=None,pre_lnorm=False,r_r_bias=None,r_w_bias=None,layer_norm_epsilon=1e-05)
transformers.modeling_transfo_xl.RelPartialLearnableMultiHeadAttn.__init__(self,n_head,d_model,d_head,dropout,dropatt=0,tgt_len=None,ext_len=None,mem_len=None,pre_lnorm=False,r_r_bias=None,r_w_bias=None,layer_norm_epsilon=1e-05)
transformers.modeling_transfo_xl.RelPartialLearnableMultiHeadAttn._rel_shift(self,x)
transformers.modeling_transfo_xl.RelPartialLearnableMultiHeadAttn.forward(self,w,r,attn_mask=None,mems=None,head_mask=None,output_attentions=False)
transformers.modeling_transfo_xl.TransfoXLLMHeadModel(self,config)
transformers.modeling_transfo_xl.TransfoXLLMHeadModel.__init__(self,config)
transformers.modeling_transfo_xl.TransfoXLLMHeadModel._resize_cutoffs(self,new_num_tokens,new_emb_size,new_embedding_shapes,layer)
transformers.modeling_transfo_xl.TransfoXLLMHeadModel.forward(self,input_ids=None,mems=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.modeling_transfo_xl.TransfoXLLMHeadModel.get_output_embeddings(self)
transformers.modeling_transfo_xl.TransfoXLLMHeadModel.init_mems(self,bsz)
transformers.modeling_transfo_xl.TransfoXLLMHeadModel.prepare_inputs_for_generation(self,input_ids,past,**model_kwargs)
transformers.modeling_transfo_xl.TransfoXLLMHeadModel.reset_length(self,tgt_len,ext_len,mem_len)
transformers.modeling_transfo_xl.TransfoXLLMHeadModel.tie_weights(self)
transformers.modeling_transfo_xl.TransfoXLLMHeadModelOutput(ModelOutput)
transformers.modeling_transfo_xl.TransfoXLLMHeadModelOutput.logits(self)
transformers.modeling_transfo_xl.TransfoXLModel(self,config)
transformers.modeling_transfo_xl.TransfoXLModel.__init__(self,config)
transformers.modeling_transfo_xl.TransfoXLModel._prune_heads(self,heads)
transformers.modeling_transfo_xl.TransfoXLModel._update_mems(self,hids,mems,mlen,qlen)
transformers.modeling_transfo_xl.TransfoXLModel.backward_compatible(self)
transformers.modeling_transfo_xl.TransfoXLModel.forward(self,input_ids=None,mems=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.modeling_transfo_xl.TransfoXLModel.get_input_embeddings(self)
transformers.modeling_transfo_xl.TransfoXLModel.init_mems(self,bsz)
transformers.modeling_transfo_xl.TransfoXLModel.reset_length(self,tgt_len,ext_len,mem_len)
transformers.modeling_transfo_xl.TransfoXLModel.set_input_embeddings(self,new_embeddings)
transformers.modeling_transfo_xl.TransfoXLModelOutput(ModelOutput)
transformers.modeling_transfo_xl.TransfoXLPreTrainedModel(PreTrainedModel)
transformers.modeling_transfo_xl.TransfoXLPreTrainedModel._get_embedding_shapes(self)
transformers.modeling_transfo_xl.TransfoXLPreTrainedModel._get_new_num_tokens_layer(self,new_num_tokens,layer)
transformers.modeling_transfo_xl.TransfoXLPreTrainedModel._init_bias(self,bias)
transformers.modeling_transfo_xl.TransfoXLPreTrainedModel._init_weight(self,weight)
transformers.modeling_transfo_xl.TransfoXLPreTrainedModel._init_weights(self,m)
transformers.modeling_transfo_xl.TransfoXLPreTrainedModel._resize_cutoffs(self,new_num_tokens,new_emb_size,new_embedding_shapes,layer)
transformers.modeling_transfo_xl.TransfoXLPreTrainedModel._resize_token_embeddings(self,new_num_tokens,layer=-1)
transformers.modeling_transfo_xl.TransfoXLPreTrainedModel.resize_token_embeddings(self,new_num_tokens:Optional[int]=None,layer:Optional[int]=-1)
transformers.modeling_transfo_xl.build_tf_to_pytorch_map(model,config)
transformers.modeling_transfo_xl.load_tf_weights_in_transfo_xl(model,config,tf_path)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/modeling_mobilebert.py----------------------------------------
A:transformers.modeling_mobilebert.logger->utils.logging.get_logger(__name__)
A:transformers.modeling_mobilebert.tf_path->os.path.abspath(tf_checkpoint_path)
A:transformers.modeling_mobilebert.init_vars->tensorflow.train.list_variables(tf_path)
A:transformers.modeling_mobilebert.array->numpy.transpose(array)
A:transformers.modeling_mobilebert.name->name.split('/').split('/')
A:transformers.modeling_mobilebert.scope_names->re.split('_(\\d+)', m_name)
A:transformers.modeling_mobilebert.pointer->getattr(pointer, 'weight')
A:transformers.modeling_mobilebert.num->int(scope_names[1])
A:transformers.modeling_mobilebert.pointer.data->torch.from_numpy(array)
A:transformers.modeling_mobilebert.self.bias->torch.nn.Parameter(torch.zeros(config.vocab_size))
A:transformers.modeling_mobilebert.self.weight->torch.nn.Parameter(torch.ones(feat_size))
A:transformers.modeling_mobilebert.self.word_embeddings->torch.nn.Embedding(config.vocab_size, config.embedding_size, padding_idx=config.pad_token_id)
A:transformers.modeling_mobilebert.self.position_embeddings->torch.nn.Embedding(config.max_position_embeddings, config.hidden_size)
A:transformers.modeling_mobilebert.self.token_type_embeddings->torch.nn.Embedding(config.type_vocab_size, config.hidden_size)
A:transformers.modeling_mobilebert.self.embedding_transformation->torch.nn.Linear(embedded_input_size, config.hidden_size)
A:transformers.modeling_mobilebert.self.LayerNorm->NORM2FN['layer_norm'](config.hidden_size, eps=config.layer_norm_eps)
A:transformers.modeling_mobilebert.self.dropout->torch.nn.Dropout(config.hidden_dropout_prob)
A:transformers.modeling_mobilebert.input_shape->input_ids.size()
A:transformers.modeling_mobilebert.token_type_ids->torch.zeros(input_shape, dtype=torch.long, device=device)
A:transformers.modeling_mobilebert.inputs_embeds->self.embedding_transformation(inputs_embeds)
A:transformers.modeling_mobilebert.position_embeddings->self.position_embeddings(position_ids)
A:transformers.modeling_mobilebert.token_type_embeddings->self.token_type_embeddings(token_type_ids)
A:transformers.modeling_mobilebert.embeddings->self.dropout(embeddings)
A:transformers.modeling_mobilebert.self.attention_head_size->int(config.true_hidden_size / config.num_attention_heads)
A:transformers.modeling_mobilebert.self.query->torch.nn.Linear(config.true_hidden_size, self.all_head_size)
A:transformers.modeling_mobilebert.self.key->torch.nn.Linear(config.true_hidden_size, self.all_head_size)
A:transformers.modeling_mobilebert.self.value->torch.nn.Linear(config.true_hidden_size if config.use_bottleneck_attention else config.hidden_size, self.all_head_size)
A:transformers.modeling_mobilebert.x->x.view(*new_x_shape).view(*new_x_shape)
A:transformers.modeling_mobilebert.mixed_query_layer->self.query(query_tensor)
A:transformers.modeling_mobilebert.mixed_key_layer->self.key(key_tensor)
A:transformers.modeling_mobilebert.mixed_value_layer->self.value(value_tensor)
A:transformers.modeling_mobilebert.query_layer->self.transpose_for_scores(mixed_query_layer)
A:transformers.modeling_mobilebert.key_layer->self.transpose_for_scores(mixed_key_layer)
A:transformers.modeling_mobilebert.value_layer->self.transpose_for_scores(mixed_value_layer)
A:transformers.modeling_mobilebert.attention_scores->torch.matmul(query_layer, key_layer.transpose(-1, -2))
A:transformers.modeling_mobilebert.attention_probs->self.dropout(attention_probs)
A:transformers.modeling_mobilebert.context_layer->context_layer.view(*new_context_layer_shape).view(*new_context_layer_shape)
A:transformers.modeling_mobilebert.self.dense->torch.nn.Linear(config.vocab_size, config.hidden_size - config.embedding_size, bias=False)
A:transformers.modeling_mobilebert.layer_outputs->layer_module(hidden_states, attention_mask, head_mask[i], encoder_hidden_states, encoder_attention_mask, output_attentions)
A:transformers.modeling_mobilebert.self.self->MobileBertSelfAttention(config)
A:transformers.modeling_mobilebert.self.output->MobileBertOutput(config)
A:transformers.modeling_mobilebert.self.pruned_heads->self.pruned_heads.union(heads)
A:transformers.modeling_mobilebert.(heads, index)->find_pruneable_heads_and_indices(heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads)
A:transformers.modeling_mobilebert.self.self.query->prune_linear_layer(self.self.query, index)
A:transformers.modeling_mobilebert.self.self.key->prune_linear_layer(self.self.key, index)
A:transformers.modeling_mobilebert.self.self.value->prune_linear_layer(self.self.value, index)
A:transformers.modeling_mobilebert.self.output.dense->prune_linear_layer(self.output.dense, index, dim=1)
A:transformers.modeling_mobilebert.self_outputs->self.self(query_tensor, key_tensor, value_tensor, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)
A:transformers.modeling_mobilebert.attention_output->ffn_module(attention_output)
A:transformers.modeling_mobilebert.self.bottleneck->Bottleneck(config)
A:transformers.modeling_mobilebert.layer_output->self.output(intermediate_output, attention_output, hidden_states)
A:transformers.modeling_mobilebert.layer_input->self.LayerNorm(layer_input)
A:transformers.modeling_mobilebert.self.input->BottleneckLayer(config)
A:transformers.modeling_mobilebert.self.attention->MobileBertAttention(config)
A:transformers.modeling_mobilebert.bottlenecked_hidden_states->self.input(hidden_states)
A:transformers.modeling_mobilebert.shared_attention_input->self.attention(hidden_states)
A:transformers.modeling_mobilebert.self.intermediate->MobileBertIntermediate(config)
A:transformers.modeling_mobilebert.intermediate_output->self.intermediate(attention_output)
A:transformers.modeling_mobilebert.self.ffn->torch.nn.ModuleList([FFNLayer(config) for _ in range(config.num_feedforward_networks - 1)])
A:transformers.modeling_mobilebert.(query_tensor, key_tensor, value_tensor, layer_input)->self.bottleneck(hidden_states)
A:transformers.modeling_mobilebert.self_attention_outputs->self.attention(query_tensor, key_tensor, value_tensor, layer_input, attention_mask, head_mask, output_attentions=output_attentions)
A:transformers.modeling_mobilebert.self.layer->torch.nn.ModuleList([MobileBertLayer(config) for _ in range(config.num_hidden_layers)])
A:transformers.modeling_mobilebert.pooled_output->self.dropout(pooled_output)
A:transformers.modeling_mobilebert.hidden_states->hidden_states.matmul(torch.cat([self.decoder.weight.t(), self.dense.weight], dim=0)).matmul(torch.cat([self.decoder.weight.t(), self.dense.weight], dim=0))
A:transformers.modeling_mobilebert.self.transform->MobileBertPredictionHeadTransform(config)
A:transformers.modeling_mobilebert.self.decoder->torch.nn.Linear(config.embedding_size, config.vocab_size, bias=False)
A:transformers.modeling_mobilebert.self.predictions->MobileBertLMPredictionHead(config)
A:transformers.modeling_mobilebert.prediction_scores->self.cls(sequence_output)
A:transformers.modeling_mobilebert.self.seq_relationship->torch.nn.Linear(config.hidden_size, 2)
A:transformers.modeling_mobilebert.seq_relationship_score->self.cls(pooled_output)
A:transformers.modeling_mobilebert.self.embeddings->MobileBertEmbeddings(config)
A:transformers.modeling_mobilebert.self.encoder->MobileBertEncoder(config)
A:transformers.modeling_mobilebert.self.pooler->MobileBertPooler(config)
A:transformers.modeling_mobilebert.attention_mask->torch.ones(input_shape, device=device)
A:transformers.modeling_mobilebert.(encoder_batch_size, encoder_sequence_length, _)->encoder_hidden_states.size()
A:transformers.modeling_mobilebert.encoder_attention_mask->torch.ones(encoder_hidden_shape, device=device)
A:transformers.modeling_mobilebert.encoder_extended_attention_mask->self.invert_attention_mask(encoder_attention_mask)
A:transformers.modeling_mobilebert.head_mask->self.get_head_mask(head_mask, self.config.num_hidden_layers)
A:transformers.modeling_mobilebert.embedding_output->self.embeddings(input_ids=input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds)
A:transformers.modeling_mobilebert.encoder_outputs->self.encoder(embedding_output, attention_mask=extended_attention_mask, head_mask=head_mask, encoder_hidden_states=encoder_hidden_states, encoder_attention_mask=encoder_extended_attention_mask, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.modeling_mobilebert.self.mobilebert->MobileBertModel(config)
A:transformers.modeling_mobilebert.self.cls->MobileBertOnlyNSPHead(config)
A:transformers.modeling_mobilebert.output_embeddings->self.get_output_embeddings()
A:transformers.modeling_mobilebert.input_embeddings->self.get_input_embeddings()
A:transformers.modeling_mobilebert.resized_dense->torch.nn.Linear(input_embeddings.num_embeddings, self.config.hidden_size - self.config.embedding_size, bias=False)
A:transformers.modeling_mobilebert.outputs->self.mobilebert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.modeling_mobilebert.(prediction_scores, seq_relationship_score)->self.cls(sequence_output, pooled_output)
A:transformers.modeling_mobilebert.loss_fct->CrossEntropyLoss()
A:transformers.modeling_mobilebert.masked_lm_loss->loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))
A:transformers.modeling_mobilebert.next_sentence_loss->loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))
A:transformers.modeling_mobilebert.labels->kwargs.pop('masked_lm_labels')
A:transformers.modeling_mobilebert.self.classifier->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.modeling_mobilebert.logits->self.classifier(sequence_output)
A:transformers.modeling_mobilebert.loss->loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
A:transformers.modeling_mobilebert.self.qa_outputs->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.modeling_mobilebert.(start_logits, end_logits)->self.classifier(sequence_output).split(1, dim=-1)
A:transformers.modeling_mobilebert.start_logits->start_logits.squeeze(-1).squeeze(-1)
A:transformers.modeling_mobilebert.end_logits->end_logits.squeeze(-1).squeeze(-1)
A:transformers.modeling_mobilebert.start_positions->start_positions.squeeze(-1).squeeze(-1)
A:transformers.modeling_mobilebert.end_positions->end_positions.squeeze(-1).squeeze(-1)
A:transformers.modeling_mobilebert.ignored_index->start_logits.squeeze(-1).squeeze(-1).size(1)
A:transformers.modeling_mobilebert.start_loss->loss_fct(start_logits, start_positions)
A:transformers.modeling_mobilebert.end_loss->loss_fct(end_logits, end_positions)
A:transformers.modeling_mobilebert.reshaped_logits->self.classifier(sequence_output).view(-1, num_choices)
A:transformers.modeling_mobilebert.sequence_output->self.dropout(sequence_output)
A:transformers.modeling_mobilebert.active_logits->self.classifier(sequence_output).view(-1, self.num_labels)
A:transformers.modeling_mobilebert.active_labels->torch.where(active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels))
transformers.MobileBertForMaskedLM(self,config)
transformers.MobileBertForMaskedLM.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,encoder_hidden_states=None,encoder_attention_mask=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.MobileBertForMaskedLM.get_output_embeddings(self)
transformers.MobileBertForMaskedLM.tie_weights(self)
transformers.MobileBertForMultipleChoice(self,config)
transformers.MobileBertForMultipleChoice.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.MobileBertForNextSentencePrediction(self,config)
transformers.MobileBertForNextSentencePrediction.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,next_sentence_label=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.MobileBertForPreTraining(self,config)
transformers.MobileBertForPreTraining.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,next_sentence_label=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.MobileBertForPreTraining.get_output_embeddings(self)
transformers.MobileBertForPreTraining.tie_weights(self)
transformers.MobileBertForPreTrainingOutput(ModelOutput)
transformers.MobileBertForQuestionAnswering(self,config)
transformers.MobileBertForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,start_positions=None,end_positions=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.MobileBertForSequenceClassification(self,config)
transformers.MobileBertForSequenceClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.MobileBertForTokenClassification(self,config)
transformers.MobileBertForTokenClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.MobileBertLayer(self,config)
transformers.MobileBertLayer.forward(self,hidden_states,attention_mask=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,output_attentions=None)
transformers.MobileBertModel(self,config)
transformers.MobileBertModel._prune_heads(self,heads_to_prune)
transformers.MobileBertModel.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,output_hidden_states=None,output_attentions=None,return_dict=None)
transformers.MobileBertModel.get_input_embeddings(self)
transformers.MobileBertModel.set_input_embeddings(self,value)
transformers.MobileBertPreTrainedModel(PreTrainedModel)
transformers.MobileBertPreTrainedModel._init_weights(self,module)
transformers.load_tf_weights_in_mobilebert(model,config,tf_checkpoint_path)
transformers.modeling_mobilebert.Bottleneck(self,config)
transformers.modeling_mobilebert.Bottleneck.__init__(self,config)
transformers.modeling_mobilebert.Bottleneck.forward(self,hidden_states)
transformers.modeling_mobilebert.BottleneckLayer(self,config)
transformers.modeling_mobilebert.BottleneckLayer.__init__(self,config)
transformers.modeling_mobilebert.BottleneckLayer.forward(self,hidden_states)
transformers.modeling_mobilebert.FFNLayer(self,config)
transformers.modeling_mobilebert.FFNLayer.__init__(self,config)
transformers.modeling_mobilebert.FFNLayer.forward(self,hidden_states)
transformers.modeling_mobilebert.FFNOutput(self,config)
transformers.modeling_mobilebert.FFNOutput.__init__(self,config)
transformers.modeling_mobilebert.FFNOutput.forward(self,hidden_states,residual_tensor)
transformers.modeling_mobilebert.MobileBertAttention(self,config)
transformers.modeling_mobilebert.MobileBertAttention.__init__(self,config)
transformers.modeling_mobilebert.MobileBertAttention.forward(self,query_tensor,key_tensor,value_tensor,layer_input,attention_mask=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,output_attentions=None)
transformers.modeling_mobilebert.MobileBertAttention.prune_heads(self,heads)
transformers.modeling_mobilebert.MobileBertEmbeddings(self,config)
transformers.modeling_mobilebert.MobileBertEmbeddings.__init__(self,config)
transformers.modeling_mobilebert.MobileBertEmbeddings.forward(self,input_ids=None,token_type_ids=None,position_ids=None,inputs_embeds=None)
transformers.modeling_mobilebert.MobileBertEncoder(self,config)
transformers.modeling_mobilebert.MobileBertEncoder.__init__(self,config)
transformers.modeling_mobilebert.MobileBertEncoder.forward(self,hidden_states,attention_mask=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,output_attentions=False,output_hidden_states=False,return_dict=False)
transformers.modeling_mobilebert.MobileBertForMaskedLM(self,config)
transformers.modeling_mobilebert.MobileBertForMaskedLM.__init__(self,config)
transformers.modeling_mobilebert.MobileBertForMaskedLM.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,encoder_hidden_states=None,encoder_attention_mask=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.modeling_mobilebert.MobileBertForMaskedLM.get_output_embeddings(self)
transformers.modeling_mobilebert.MobileBertForMaskedLM.tie_weights(self)
transformers.modeling_mobilebert.MobileBertForMultipleChoice(self,config)
transformers.modeling_mobilebert.MobileBertForMultipleChoice.__init__(self,config)
transformers.modeling_mobilebert.MobileBertForMultipleChoice.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.modeling_mobilebert.MobileBertForNextSentencePrediction(self,config)
transformers.modeling_mobilebert.MobileBertForNextSentencePrediction.__init__(self,config)
transformers.modeling_mobilebert.MobileBertForNextSentencePrediction.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,next_sentence_label=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.modeling_mobilebert.MobileBertForPreTraining(self,config)
transformers.modeling_mobilebert.MobileBertForPreTraining.__init__(self,config)
transformers.modeling_mobilebert.MobileBertForPreTraining.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,next_sentence_label=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.modeling_mobilebert.MobileBertForPreTraining.get_output_embeddings(self)
transformers.modeling_mobilebert.MobileBertForPreTraining.tie_weights(self)
transformers.modeling_mobilebert.MobileBertForPreTrainingOutput(ModelOutput)
transformers.modeling_mobilebert.MobileBertForQuestionAnswering(self,config)
transformers.modeling_mobilebert.MobileBertForQuestionAnswering.__init__(self,config)
transformers.modeling_mobilebert.MobileBertForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,start_positions=None,end_positions=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.modeling_mobilebert.MobileBertForSequenceClassification(self,config)
transformers.modeling_mobilebert.MobileBertForSequenceClassification.__init__(self,config)
transformers.modeling_mobilebert.MobileBertForSequenceClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.modeling_mobilebert.MobileBertForTokenClassification(self,config)
transformers.modeling_mobilebert.MobileBertForTokenClassification.__init__(self,config)
transformers.modeling_mobilebert.MobileBertForTokenClassification.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,labels=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.modeling_mobilebert.MobileBertIntermediate(self,config)
transformers.modeling_mobilebert.MobileBertIntermediate.__init__(self,config)
transformers.modeling_mobilebert.MobileBertLMPredictionHead(self,config)
transformers.modeling_mobilebert.MobileBertLMPredictionHead.__init__(self,config)
transformers.modeling_mobilebert.MobileBertLMPredictionHead.forward(self,hidden_states)
transformers.modeling_mobilebert.MobileBertLayer(self,config)
transformers.modeling_mobilebert.MobileBertLayer.__init__(self,config)
transformers.modeling_mobilebert.MobileBertLayer.forward(self,hidden_states,attention_mask=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,output_attentions=None)
transformers.modeling_mobilebert.MobileBertModel(self,config)
transformers.modeling_mobilebert.MobileBertModel.__init__(self,config)
transformers.modeling_mobilebert.MobileBertModel._prune_heads(self,heads_to_prune)
transformers.modeling_mobilebert.MobileBertModel.forward(self,input_ids=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,output_hidden_states=None,output_attentions=None,return_dict=None)
transformers.modeling_mobilebert.MobileBertModel.get_input_embeddings(self)
transformers.modeling_mobilebert.MobileBertModel.set_input_embeddings(self,value)
transformers.modeling_mobilebert.MobileBertOnlyMLMHead(self,config)
transformers.modeling_mobilebert.MobileBertOnlyMLMHead.__init__(self,config)
transformers.modeling_mobilebert.MobileBertOnlyMLMHead.forward(self,sequence_output)
transformers.modeling_mobilebert.MobileBertOnlyNSPHead(self,config)
transformers.modeling_mobilebert.MobileBertOnlyNSPHead.__init__(self,config)
transformers.modeling_mobilebert.MobileBertOnlyNSPHead.forward(self,pooled_output)
transformers.modeling_mobilebert.MobileBertOutput(self,config)
transformers.modeling_mobilebert.MobileBertOutput.__init__(self,config)
transformers.modeling_mobilebert.MobileBertOutput.forward(self,intermediate_states,residual_tensor_1,residual_tensor_2)
transformers.modeling_mobilebert.MobileBertPooler(self,config)
transformers.modeling_mobilebert.MobileBertPooler.__init__(self,config)
transformers.modeling_mobilebert.MobileBertPooler.forward(self,hidden_states)
transformers.modeling_mobilebert.MobileBertPreTrainedModel(PreTrainedModel)
transformers.modeling_mobilebert.MobileBertPreTrainedModel._init_weights(self,module)
transformers.modeling_mobilebert.MobileBertPreTrainingHeads(self,config)
transformers.modeling_mobilebert.MobileBertPreTrainingHeads.__init__(self,config)
transformers.modeling_mobilebert.MobileBertPreTrainingHeads.forward(self,sequence_output,pooled_output)
transformers.modeling_mobilebert.MobileBertPredictionHeadTransform(self,config)
transformers.modeling_mobilebert.MobileBertPredictionHeadTransform.__init__(self,config)
transformers.modeling_mobilebert.MobileBertPredictionHeadTransform.forward(self,hidden_states)
transformers.modeling_mobilebert.MobileBertSelfAttention(self,config)
transformers.modeling_mobilebert.MobileBertSelfAttention.__init__(self,config)
transformers.modeling_mobilebert.MobileBertSelfAttention.forward(self,query_tensor,key_tensor,value_tensor,attention_mask=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,output_attentions=None)
transformers.modeling_mobilebert.MobileBertSelfAttention.transpose_for_scores(self,x)
transformers.modeling_mobilebert.MobileBertSelfOutput(self,config)
transformers.modeling_mobilebert.MobileBertSelfOutput.__init__(self,config)
transformers.modeling_mobilebert.MobileBertSelfOutput.forward(self,hidden_states,residual_tensor)
transformers.modeling_mobilebert.NoNorm(self,feat_size,eps=None)
transformers.modeling_mobilebert.NoNorm.__init__(self,feat_size,eps=None)
transformers.modeling_mobilebert.NoNorm.forward(self,input_tensor)
transformers.modeling_mobilebert.OutputBottleneck(self,config)
transformers.modeling_mobilebert.OutputBottleneck.__init__(self,config)
transformers.modeling_mobilebert.OutputBottleneck.forward(self,hidden_states,residual_tensor)
transformers.modeling_mobilebert.load_tf_weights_in_mobilebert(model,config,tf_checkpoint_path)
transformers.modeling_mobilebert.mish(x)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/tokenization_longformer.py----------------------------------------
A:transformers.tokenization_longformer.logger->utils.logging.get_logger(__name__)
transformers.LongformerTokenizer(RobertaTokenizer)
transformers.LongformerTokenizerFast(RobertaTokenizerFast)
transformers.tokenization_longformer.LongformerTokenizer(RobertaTokenizer)
transformers.tokenization_longformer.LongformerTokenizerFast(RobertaTokenizerFast)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/modeling_gpt2.py----------------------------------------
A:transformers.modeling_gpt2.logger->utils.logging.get_logger(__name__)
A:transformers.modeling_gpt2.tf_path->os.path.abspath(gpt2_checkpoint_path)
A:transformers.modeling_gpt2.init_vars->tensorflow.train.list_variables(tf_path)
A:transformers.modeling_gpt2.array->tensorflow.train.load_variable(tf_path, name)
A:transformers.modeling_gpt2.name->name.split('/').split('/')
A:transformers.modeling_gpt2.scope_names->re.split('(\\d+)', m_name)
A:transformers.modeling_gpt2.pointer->getattr(pointer, scope_names[0])
A:transformers.modeling_gpt2.num->int(scope_names[1])
A:transformers.modeling_gpt2.pointer.data->torch.from_numpy(array)
A:transformers.modeling_gpt2.self.c_attn->prune_conv1d_layer(self.c_attn, index_attn, dim=1)
A:transformers.modeling_gpt2.self.q_attn->Conv1D(n_state, nx)
A:transformers.modeling_gpt2.self.c_proj->Conv1D(nx, n_state)
A:transformers.modeling_gpt2.self.attn_dropout->torch.nn.Dropout(config.attn_pdrop)
A:transformers.modeling_gpt2.self.resid_dropout->torch.nn.Dropout(config.resid_pdrop)
A:transformers.modeling_gpt2.self.pruned_heads->self.pruned_heads.union(heads)
A:transformers.modeling_gpt2.(heads, index)->find_pruneable_heads_and_indices(heads, self.n_head, self.split_size // self.n_head, self.pruned_heads)
A:transformers.modeling_gpt2.index_attn->torch.cat([index, index + self.split_size, index + 2 * self.split_size])
A:transformers.modeling_gpt2.w->self.attn_dropout(w)
A:transformers.modeling_gpt2.x->x.view(*new_x_shape).view(*new_x_shape)
A:transformers.modeling_gpt2.query->self.split_heads(query)
A:transformers.modeling_gpt2.(key, value)->self.c_attn(encoder_hidden_states).split(self.split_size, dim=2)
A:transformers.modeling_gpt2.(query, key, value)->self.c_attn(hidden_states).split(self.split_size, dim=2)
A:transformers.modeling_gpt2.key->torch.cat((past_key, key), dim=-1)
A:transformers.modeling_gpt2.value->torch.cat((past_value, value), dim=-2)
A:transformers.modeling_gpt2.present->torch.stack((key.transpose(-2, -1), value))
A:transformers.modeling_gpt2.attn_outputs->self.attn(self.ln_1(hidden_states), layer_past=layer_past, attention_mask=attention_mask, head_mask=head_mask, use_cache=use_cache, output_attentions=output_attentions)
A:transformers.modeling_gpt2.a->self.resid_dropout(a)
A:transformers.modeling_gpt2.self.c_fc->Conv1D(n_state, nx)
A:transformers.modeling_gpt2.self.dropout->torch.nn.Dropout(config.resid_pdrop)
A:transformers.modeling_gpt2.h->self.act(self.c_fc(x))
A:transformers.modeling_gpt2.h2->self.c_proj(h)
A:transformers.modeling_gpt2.self.ln_1->torch.nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)
A:transformers.modeling_gpt2.self.attn->Attention(hidden_size, n_ctx, config, scale)
A:transformers.modeling_gpt2.self.ln_2->torch.nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)
A:transformers.modeling_gpt2.self.crossattention->Attention(hidden_size, n_ctx, config, scale, is_cross_attention=True)
A:transformers.modeling_gpt2.self.ln_cross_attn->torch.nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)
A:transformers.modeling_gpt2.self.mlp->MLP(inner_dim, config)
A:transformers.modeling_gpt2.cross_attn_outputs->self.crossattention(self.ln_cross_attn(hidden_states), attention_mask=attention_mask, head_mask=head_mask, encoder_hidden_states=encoder_hidden_states, encoder_attention_mask=encoder_attention_mask, output_attentions=output_attentions)
A:transformers.modeling_gpt2.feed_forward_hidden_states->self.mlp(self.ln_2(hidden_states))
A:transformers.modeling_gpt2.self.wte->torch.nn.Embedding(config.vocab_size, config.n_embd)
A:transformers.modeling_gpt2.self.wpe->torch.nn.Embedding(config.n_positions, config.n_embd)
A:transformers.modeling_gpt2.self.drop->torch.nn.Dropout(config.embd_pdrop)
A:transformers.modeling_gpt2.self.h->torch.nn.ModuleList([Block(config.n_ctx, config, scale=True) for _ in range(config.n_layer)])
A:transformers.modeling_gpt2.self.ln_f->torch.nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)
A:transformers.modeling_gpt2.past_key_values->kwargs.pop('past')
A:transformers.modeling_gpt2.input_shape->input_ids[:, -1].unsqueeze(-1).size()
A:transformers.modeling_gpt2.input_ids->input_ids[:, -1].unsqueeze(-1)
A:transformers.modeling_gpt2.token_type_ids->token_type_ids.view(-1, input_shape[-1]).view(-1, input_shape[-1])
A:transformers.modeling_gpt2.position_ids->position_ids.unsqueeze(0).view(-1, input_shape[-1]).unsqueeze(0).view(-1, input_shape[-1])
A:transformers.modeling_gpt2.past_length->past_key_values[0][0].size(-2)
A:transformers.modeling_gpt2.attention_mask->attention_mask.to(dtype=next(self.parameters()).dtype).to(dtype=next(self.parameters()).dtype)
A:transformers.modeling_gpt2.(encoder_batch_size, encoder_sequence_length, _)->encoder_hidden_states.size()
A:transformers.modeling_gpt2.encoder_attention_mask->self.invert_attention_mask(encoder_attention_mask)
A:transformers.modeling_gpt2.head_mask->self.get_head_mask(head_mask, self.config.n_layer)
A:transformers.modeling_gpt2.inputs_embeds->self.wte(input_ids)
A:transformers.modeling_gpt2.position_embeds->self.wpe(position_ids)
A:transformers.modeling_gpt2.token_type_embeds->self.wte(token_type_ids)
A:transformers.modeling_gpt2.hidden_states->hidden_states.view(*output_shape).view(*output_shape)
A:transformers.modeling_gpt2.outputs->block(hidden_states, layer_past=layer_past, attention_mask=attention_mask, head_mask=head_mask[i], encoder_hidden_states=encoder_hidden_states, encoder_attention_mask=encoder_attention_mask, use_cache=use_cache, output_attentions=output_attentions)
A:transformers.modeling_gpt2.self.transformer->GPT2Model(config)
A:transformers.modeling_gpt2.self.lm_head->torch.nn.Linear(config.n_embd, config.vocab_size, bias=False)
A:transformers.modeling_gpt2.transformer_outputs->self.transformer(input_ids, past_key_values=past_key_values, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.modeling_gpt2.lm_logits->self.lm_head(hidden_states)
A:transformers.modeling_gpt2.shift_logits->lm_logits[..., :-1, :].contiguous()
A:transformers.modeling_gpt2.shift_labels->labels[..., 1:].contiguous()
A:transformers.modeling_gpt2.loss_fct->CrossEntropyLoss()
A:transformers.modeling_gpt2.loss->loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
A:transformers.modeling_gpt2.self.multiple_choice_head->SequenceSummary(config)
A:transformers.modeling_gpt2.labels->kwargs.pop('lm_labels')
A:transformers.modeling_gpt2.mc_logits->self.multiple_choice_head(hidden_states, mc_token_ids).squeeze(-1)
A:transformers.modeling_gpt2.mc_loss->loss_fct(mc_logits.view(-1, mc_logits.size(-1)), mc_labels.view(-1))
A:transformers.modeling_gpt2.lm_loss->loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
transformers.GPT2DoubleHeadsModel(self,config)
transformers.GPT2DoubleHeadsModel.forward(self,input_ids=None,past_key_values=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,mc_token_ids=None,labels=None,mc_labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.GPT2DoubleHeadsModel.get_output_embeddings(self)
transformers.GPT2DoubleHeadsModel.prepare_inputs_for_generation(self,input_ids,past=None,**kwargs)
transformers.GPT2DoubleHeadsModelOutput(ModelOutput)
transformers.GPT2LMHeadModel(self,config)
transformers.GPT2LMHeadModel.forward(self,input_ids=None,past_key_values=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.GPT2LMHeadModel.get_output_embeddings(self)
transformers.GPT2LMHeadModel.prepare_inputs_for_generation(self,input_ids,past=None,**kwargs)
transformers.GPT2Model(self,config)
transformers.GPT2Model._prune_heads(self,heads_to_prune)
transformers.GPT2Model.forward(self,input_ids=None,past_key_values=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.GPT2Model.get_input_embeddings(self)
transformers.GPT2Model.set_input_embeddings(self,new_embeddings)
transformers.GPT2PreTrainedModel(self,*inputs,**kwargs)
transformers.GPT2PreTrainedModel._init_weights(self,module)
transformers.load_tf_weights_in_gpt2(model,config,gpt2_checkpoint_path)
transformers.modeling_gpt2.Attention(self,nx,n_ctx,config,scale=False,is_cross_attention=False)
transformers.modeling_gpt2.Attention.__init__(self,nx,n_ctx,config,scale=False,is_cross_attention=False)
transformers.modeling_gpt2.Attention._attn(self,q,k,v,attention_mask=None,head_mask=None,output_attentions=False)
transformers.modeling_gpt2.Attention.forward(self,hidden_states,layer_past=None,attention_mask=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,use_cache=False,output_attentions=False)
transformers.modeling_gpt2.Attention.merge_heads(self,x)
transformers.modeling_gpt2.Attention.prune_heads(self,heads)
transformers.modeling_gpt2.Attention.split_heads(self,x,k=False)
transformers.modeling_gpt2.Block(self,n_ctx,config,scale=False)
transformers.modeling_gpt2.Block.__init__(self,n_ctx,config,scale=False)
transformers.modeling_gpt2.Block.forward(self,hidden_states,layer_past=None,attention_mask=None,head_mask=None,encoder_hidden_states=None,encoder_attention_mask=None,use_cache=False,output_attentions=False)
transformers.modeling_gpt2.GPT2DoubleHeadsModel(self,config)
transformers.modeling_gpt2.GPT2DoubleHeadsModel.__init__(self,config)
transformers.modeling_gpt2.GPT2DoubleHeadsModel.forward(self,input_ids=None,past_key_values=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,mc_token_ids=None,labels=None,mc_labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.modeling_gpt2.GPT2DoubleHeadsModel.get_output_embeddings(self)
transformers.modeling_gpt2.GPT2DoubleHeadsModel.prepare_inputs_for_generation(self,input_ids,past=None,**kwargs)
transformers.modeling_gpt2.GPT2DoubleHeadsModelOutput(ModelOutput)
transformers.modeling_gpt2.GPT2LMHeadModel(self,config)
transformers.modeling_gpt2.GPT2LMHeadModel.__init__(self,config)
transformers.modeling_gpt2.GPT2LMHeadModel.forward(self,input_ids=None,past_key_values=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.modeling_gpt2.GPT2LMHeadModel.get_output_embeddings(self)
transformers.modeling_gpt2.GPT2LMHeadModel.prepare_inputs_for_generation(self,input_ids,past=None,**kwargs)
transformers.modeling_gpt2.GPT2Model(self,config)
transformers.modeling_gpt2.GPT2Model.__init__(self,config)
transformers.modeling_gpt2.GPT2Model._prune_heads(self,heads_to_prune)
transformers.modeling_gpt2.GPT2Model.forward(self,input_ids=None,past_key_values=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,encoder_hidden_states=None,encoder_attention_mask=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,**kwargs)
transformers.modeling_gpt2.GPT2Model.get_input_embeddings(self)
transformers.modeling_gpt2.GPT2Model.set_input_embeddings(self,new_embeddings)
transformers.modeling_gpt2.GPT2PreTrainedModel(self,*inputs,**kwargs)
transformers.modeling_gpt2.GPT2PreTrainedModel.__init__(self,*inputs,**kwargs)
transformers.modeling_gpt2.GPT2PreTrainedModel._init_weights(self,module)
transformers.modeling_gpt2.MLP(self,n_state,config)
transformers.modeling_gpt2.MLP.__init__(self,n_state,config)
transformers.modeling_gpt2.MLP.forward(self,x)
transformers.modeling_gpt2.load_tf_weights_in_gpt2(model,config,gpt2_checkpoint_path)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/convert_xlnet_original_tf_checkpoint_to_pytorch.py----------------------------------------
A:transformers.convert_xlnet_original_tf_checkpoint_to_pytorch.config->transformers.XLNetConfig.from_json_file(bert_config_file)
A:transformers.convert_xlnet_original_tf_checkpoint_to_pytorch.model->XLNetLMHeadModel(config)
A:transformers.convert_xlnet_original_tf_checkpoint_to_pytorch.pytorch_weights_dump_path->os.path.join(pytorch_dump_folder_path, WEIGHTS_NAME)
A:transformers.convert_xlnet_original_tf_checkpoint_to_pytorch.pytorch_config_dump_path->os.path.join(pytorch_dump_folder_path, CONFIG_NAME)
A:transformers.convert_xlnet_original_tf_checkpoint_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.convert_xlnet_original_tf_checkpoint_to_pytorch.args->argparse.ArgumentParser().parse_args()
transformers.convert_xlnet_original_tf_checkpoint_to_pytorch.convert_xlnet_checkpoint_to_pytorch(tf_checkpoint_path,bert_config_file,pytorch_dump_folder_path,finetuning_task=None)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/trainer_tf.py----------------------------------------
A:transformers.trainer_tf.logger->utils.logging.get_logger(__name__)
A:transformers.trainer_tf.self.gradient_accumulator->GradientAccumulator()
A:transformers.trainer_tf.self.args.prediction_loss_only->kwargs.pop('prediction_loss_only')
A:transformers.trainer_tf.self.tb_writer->tensorflow.summary.create_file_writer(self.args.logging_dir)
A:transformers.trainer_tf.self.num_train_examples->tensorflow.data.experimental.cardinality(self.train_dataset).numpy()
A:transformers.trainer_tf.ds->test_dataset.repeat().batch(self.args.eval_batch_size, drop_remainder=self.args.dataloader_drop_last).prefetch(tf.data.experimental.AUTOTUNE)
A:transformers.trainer_tf.num_examples->tensorflow.data.experimental.cardinality(test_dataset).numpy()
A:transformers.trainer_tf.steps->approx(num_examples / self.args.eval_batch_size)
A:transformers.trainer_tf.(self.optimizer, self.lr_scheduler)->create_optimizer(self.args.learning_rate, num_training_steps, self.args.warmup_steps, adam_beta1=self.args.adam_beta1, adam_beta2=self.args.adam_beta2, adam_epsilon=self.args.adam_epsilon, weight_decay_rate=self.args.weight_decay)
A:transformers.trainer_tf.comet_mode->os.getenv('COMET_MODE', 'ONLINE').upper()
A:transformers.trainer_tf.experiment->comet_ml.config.get_global_experiment()
A:transformers.trainer_tf.args['offline_directory']->os.getenv('COMET_OFFLINE_DIRECTORY', './')
A:transformers.trainer_tf.self.eval_loss->tensorflow.keras.metrics.Sum()
A:transformers.trainer_tf.logits->self.args.strategy.run(self.prediction_step, batch)
A:transformers.trainer_tf.preds->numpy.append(preds, logits.numpy(), axis=0)
A:transformers.trainer_tf.label_ids->numpy.append(label_ids, labels.numpy(), axis=0)
A:transformers.trainer_tf.metrics->self.compute_metrics(EvalPrediction(predictions=preds, label_ids=label_ids))
A:transformers.trainer_tf.metrics[f'eval_{key}']->self.compute_metrics(EvalPrediction(predictions=preds, label_ids=label_ids)).pop(key)
A:transformers.trainer_tf.(eval_ds, steps, num_examples)->self.get_eval_tfdataset(eval_dataset)
A:transformers.trainer_tf.output->self.prediction_loop(eval_ds, steps, num_examples, description='Evaluation')
A:transformers.trainer_tf.(per_example_loss, logits)->self.run_model(features, labels, False)
A:transformers.trainer_tf.train_ds->self.get_train_tfdataset()
A:transformers.trainer_tf.self.steps_per_epoch->approx(self.num_train_examples / self.total_train_batch_size)
A:transformers.trainer_tf.self.global_step->iterations.numpy()
A:transformers.trainer_tf.folder->os.path.join(self.args.output_dir, PREFIX_CHECKPOINT_DIR)
A:transformers.trainer_tf.ckpt->tensorflow.train.Checkpoint(optimizer=self.optimizer, model=self.model)
A:transformers.trainer_tf.self.model.ckpt_manager->tensorflow.train.CheckpointManager(ckpt, folder, max_to_keep=self.args.save_total_limit)
A:transformers.trainer_tf.policy->tensorflow.keras.mixed_precision.experimental.Policy('mixed_float16')
A:transformers.trainer_tf.self.train_loss->tensorflow.keras.metrics.Sum()
A:transformers.trainer_tf.start_time->datetime.datetime.now()
A:transformers.trainer_tf.logs['loss']->training_loss.numpy()
A:transformers.trainer_tf.logs['learning_rate']->self.lr_scheduler(self.global_step).numpy()
A:transformers.trainer_tf.ckpt_save_path->self.model.ckpt_manager.save()
A:transformers.trainer_tf.end_time->datetime.datetime.now()
A:transformers.trainer_tf.(per_example_loss, _)->self.run_model(features, labels, True)
A:transformers.trainer_tf.gradients->self.training_step(features, labels)
A:transformers.trainer_tf.labels->tensorflow.concat([labels[self.args.train_batch_size // self.args.n_replicas:], reduced_labels], axis=0)
A:transformers.trainer_tf.(test_ds, steps, num_examples)->self.get_test_tfdataset(test_dataset)
transformers.TFTrainer(self,model:TFPreTrainedModel,args:TFTrainingArguments,train_dataset:Optional[tf.data.Dataset]=None,eval_dataset:Optional[tf.data.Dataset]=None,compute_metrics:Optional[Callable[[EvalPrediction],Dict]]=None,tb_writer:Optional[tf.summary.SummaryWriter]=None,optimizers:Tuple[tf.keras.optimizers.Optimizer,tf.keras.optimizers.schedules.LearningRateSchedule]=(None,None),**kwargs)
transformers.TFTrainer.apply_gradients(self,features,labels)
transformers.TFTrainer.create_optimizer_and_scheduler(self,num_training_steps:int)
transformers.TFTrainer.distributed_prediction_steps(self,batch)
transformers.TFTrainer.distributed_training_steps(self,batch)
transformers.TFTrainer.evaluate(self,eval_dataset:Optional[tf.data.Dataset]=None)->Dict[str, float]
transformers.TFTrainer.get_eval_tfdataset(self,eval_dataset:Optional[tf.data.Dataset]=None)->tf.data.Dataset
transformers.TFTrainer.get_test_tfdataset(self,test_dataset:tf.data.Dataset)->tf.data.Dataset
transformers.TFTrainer.get_train_tfdataset(self)->tf.data.Dataset
transformers.TFTrainer.log(self,logs:Dict[str,float])->None
transformers.TFTrainer.predict(self,test_dataset:tf.data.Dataset)->PredictionOutput
transformers.TFTrainer.prediction_loop(self,dataset:tf.data.Dataset,steps:int,num_examples:int,description:str,prediction_loss_only:Optional[bool]=None)->PredictionOutput
transformers.TFTrainer.prediction_step(self,features:tf.Tensor,labels:tf.Tensor)->tf.Tensor
transformers.TFTrainer.run_model(self,features,labels,training)
transformers.TFTrainer.save_model(self,output_dir:Optional[str]=None)
transformers.TFTrainer.setup_comet(self)
transformers.TFTrainer.setup_wandb(self)
transformers.TFTrainer.train(self)->None
transformers.TFTrainer.training_step(self,features,labels)
transformers.trainer_tf.TFTrainer(self,model:TFPreTrainedModel,args:TFTrainingArguments,train_dataset:Optional[tf.data.Dataset]=None,eval_dataset:Optional[tf.data.Dataset]=None,compute_metrics:Optional[Callable[[EvalPrediction],Dict]]=None,tb_writer:Optional[tf.summary.SummaryWriter]=None,optimizers:Tuple[tf.keras.optimizers.Optimizer,tf.keras.optimizers.schedules.LearningRateSchedule]=(None,None),**kwargs)
transformers.trainer_tf.TFTrainer.__init__(self,model:TFPreTrainedModel,args:TFTrainingArguments,train_dataset:Optional[tf.data.Dataset]=None,eval_dataset:Optional[tf.data.Dataset]=None,compute_metrics:Optional[Callable[[EvalPrediction],Dict]]=None,tb_writer:Optional[tf.summary.SummaryWriter]=None,optimizers:Tuple[tf.keras.optimizers.Optimizer,tf.keras.optimizers.schedules.LearningRateSchedule]=(None,None),**kwargs)
transformers.trainer_tf.TFTrainer.apply_gradients(self,features,labels)
transformers.trainer_tf.TFTrainer.create_optimizer_and_scheduler(self,num_training_steps:int)
transformers.trainer_tf.TFTrainer.distributed_prediction_steps(self,batch)
transformers.trainer_tf.TFTrainer.distributed_training_steps(self,batch)
transformers.trainer_tf.TFTrainer.evaluate(self,eval_dataset:Optional[tf.data.Dataset]=None)->Dict[str, float]
transformers.trainer_tf.TFTrainer.get_eval_tfdataset(self,eval_dataset:Optional[tf.data.Dataset]=None)->tf.data.Dataset
transformers.trainer_tf.TFTrainer.get_test_tfdataset(self,test_dataset:tf.data.Dataset)->tf.data.Dataset
transformers.trainer_tf.TFTrainer.get_train_tfdataset(self)->tf.data.Dataset
transformers.trainer_tf.TFTrainer.log(self,logs:Dict[str,float])->None
transformers.trainer_tf.TFTrainer.predict(self,test_dataset:tf.data.Dataset)->PredictionOutput
transformers.trainer_tf.TFTrainer.prediction_loop(self,dataset:tf.data.Dataset,steps:int,num_examples:int,description:str,prediction_loss_only:Optional[bool]=None)->PredictionOutput
transformers.trainer_tf.TFTrainer.prediction_step(self,features:tf.Tensor,labels:tf.Tensor)->tf.Tensor
transformers.trainer_tf.TFTrainer.run_model(self,features,labels,training)
transformers.trainer_tf.TFTrainer.save_model(self,output_dir:Optional[str]=None)
transformers.trainer_tf.TFTrainer.setup_comet(self)
transformers.trainer_tf.TFTrainer.setup_wandb(self)
transformers.trainer_tf.TFTrainer.train(self)->None
transformers.trainer_tf.TFTrainer.training_step(self,features,labels)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/convert_roberta_original_pytorch_checkpoint_to_pytorch.py----------------------------------------
A:transformers.convert_roberta_original_pytorch_checkpoint_to_pytorch.logger->utils.logging.get_logger(__name__)
A:transformers.convert_roberta_original_pytorch_checkpoint_to_pytorch.roberta->fairseq.models.roberta.RobertaModel.from_pretrained(roberta_checkpoint_path)
A:transformers.convert_roberta_original_pytorch_checkpoint_to_pytorch.config->RobertaConfig(vocab_size=roberta_sent_encoder.embed_tokens.num_embeddings, hidden_size=roberta.args.encoder_embed_dim, num_hidden_layers=roberta.args.encoder_layers, num_attention_heads=roberta.args.encoder_attention_heads, intermediate_size=roberta.args.encoder_ffn_embed_dim, max_position_embeddings=514, type_vocab_size=1, layer_norm_eps=1e-05)
A:transformers.convert_roberta_original_pytorch_checkpoint_to_pytorch.model.roberta.embeddings.token_type_embeddings.weight.data->torch.zeros_like(model.roberta.embeddings.token_type_embeddings.weight)
A:transformers.convert_roberta_original_pytorch_checkpoint_to_pytorch.their_output->fairseq.models.roberta.RobertaModel.from_pretrained(roberta_checkpoint_path).model.classification_heads['mnli'](roberta.extract_features(input_ids))
A:transformers.convert_roberta_original_pytorch_checkpoint_to_pytorch.max_absolute_diff->torch.max(torch.abs(our_output - their_output)).item()
A:transformers.convert_roberta_original_pytorch_checkpoint_to_pytorch.success->torch.allclose(our_output, their_output, atol=0.001)
A:transformers.convert_roberta_original_pytorch_checkpoint_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.convert_roberta_original_pytorch_checkpoint_to_pytorch.args->argparse.ArgumentParser().parse_args()
transformers.convert_roberta_original_pytorch_checkpoint_to_pytorch.convert_roberta_checkpoint_to_pytorch(roberta_checkpoint_path:str,pytorch_dump_folder_path:str,classification_head:bool)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/configuration_mobilebert.py----------------------------------------
A:transformers.configuration_mobilebert.logger->utils.logging.get_logger(__name__)
transformers.MobileBertConfig(self,vocab_size=30522,hidden_size=512,num_hidden_layers=24,num_attention_heads=4,intermediate_size=512,hidden_act='relu',hidden_dropout_prob=0.0,attention_probs_dropout_prob=0.1,max_position_embeddings=512,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,pad_token_id=0,embedding_size=128,trigram_input=True,use_bottleneck=True,intra_bottleneck_size=128,use_bottleneck_attention=False,key_query_shared_bottleneck=True,num_feedforward_networks=4,normalization_type='no_norm',classifier_activation=True,**kwargs)
transformers.configuration_mobilebert.MobileBertConfig(self,vocab_size=30522,hidden_size=512,num_hidden_layers=24,num_attention_heads=4,intermediate_size=512,hidden_act='relu',hidden_dropout_prob=0.0,attention_probs_dropout_prob=0.1,max_position_embeddings=512,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,pad_token_id=0,embedding_size=128,trigram_input=True,use_bottleneck=True,intra_bottleneck_size=128,use_bottleneck_attention=False,key_query_shared_bottleneck=True,num_feedforward_networks=4,normalization_type='no_norm',classifier_activation=True,**kwargs)
transformers.configuration_mobilebert.MobileBertConfig.__init__(self,vocab_size=30522,hidden_size=512,num_hidden_layers=24,num_attention_heads=4,intermediate_size=512,hidden_act='relu',hidden_dropout_prob=0.0,attention_probs_dropout_prob=0.1,max_position_embeddings=512,type_vocab_size=2,initializer_range=0.02,layer_norm_eps=1e-12,pad_token_id=0,embedding_size=128,trigram_input=True,use_bottleneck=True,intra_bottleneck_size=128,use_bottleneck_attention=False,key_query_shared_bottleneck=True,num_feedforward_networks=4,normalization_type='no_norm',classifier_activation=True,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/convert_dialogpt_original_pytorch_checkpoint_to_pytorch.py----------------------------------------
A:transformers.convert_dialogpt_original_pytorch_checkpoint_to_pytorch.d->torch.load(checkpoint_path)
A:transformers.convert_dialogpt_original_pytorch_checkpoint_to_pytorch.d[NEW_KEY]->torch.load(checkpoint_path).pop(OLD_KEY)
A:transformers.convert_dialogpt_original_pytorch_checkpoint_to_pytorch.parser->argparse.ArgumentParser()
A:transformers.convert_dialogpt_original_pytorch_checkpoint_to_pytorch.args->argparse.ArgumentParser().parse_args()
A:transformers.convert_dialogpt_original_pytorch_checkpoint_to_pytorch.checkpoint_path->os.path.join(args.dialogpt_path, f'{MODEL}_ft.pkl')
transformers.convert_dialogpt_original_pytorch_checkpoint_to_pytorch.convert_dialogpt_checkpoint(checkpoint_path:str,pytorch_dump_folder_path:str)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/modeling_tf_distilbert.py----------------------------------------
A:transformers.modeling_tf_distilbert.logger->utils.logging.get_logger(__name__)
A:transformers.modeling_tf_distilbert.self.word_embeddings->self.add_weight('weight', shape=[self.vocab_size, self.dim], initializer=get_initializer(self.initializer_range))
A:transformers.modeling_tf_distilbert.self.position_embeddings->tensorflow.keras.layers.Embedding(config.max_position_embeddings, config.dim, embeddings_initializer=get_initializer(config.initializer_range), name='position_embeddings')
A:transformers.modeling_tf_distilbert.self.LayerNorm->tensorflow.keras.layers.LayerNormalization(epsilon=1e-12, name='LayerNorm')
A:transformers.modeling_tf_distilbert.self.dropout->tensorflow.keras.layers.Dropout(config.qa_dropout)
A:transformers.modeling_tf_distilbert.inputs_embeds->inputs.get('inputs_embeds', inputs_embeds)
A:transformers.modeling_tf_distilbert.position_embeddings->self.position_embeddings(position_ids)
A:transformers.modeling_tf_distilbert.embeddings->self.dropout(embeddings, training=training)
A:transformers.modeling_tf_distilbert.x->self.dropout(x, training=training)
A:transformers.modeling_tf_distilbert.logits->self.qa_outputs(hidden_states)
A:transformers.modeling_tf_distilbert.self.q_lin->tensorflow.keras.layers.Dense(config.dim, kernel_initializer=get_initializer(config.initializer_range), name='q_lin')
A:transformers.modeling_tf_distilbert.self.k_lin->tensorflow.keras.layers.Dense(config.dim, kernel_initializer=get_initializer(config.initializer_range), name='k_lin')
A:transformers.modeling_tf_distilbert.self.v_lin->tensorflow.keras.layers.Dense(config.dim, kernel_initializer=get_initializer(config.initializer_range), name='v_lin')
A:transformers.modeling_tf_distilbert.self.out_lin->tensorflow.keras.layers.Dense(config.dim, kernel_initializer=get_initializer(config.initializer_range), name='out_lin')
A:transformers.modeling_tf_distilbert.self.pruned_heads->set()
A:transformers.modeling_tf_distilbert.(bs, q_length, dim)->shape_list(query)
A:transformers.modeling_tf_distilbert.q->shape(self.q_lin(query))
A:transformers.modeling_tf_distilbert.k->shape(self.k_lin(key))
A:transformers.modeling_tf_distilbert.v->shape(self.v_lin(value))
A:transformers.modeling_tf_distilbert.scores->tensorflow.matmul(q, k, transpose_b=True)
A:transformers.modeling_tf_distilbert.mask->tensorflow.reshape(mask, mask_reshape)
A:transformers.modeling_tf_distilbert.weights->self.dropout(weights, training=training)
A:transformers.modeling_tf_distilbert.context->self.out_lin(context)
A:transformers.modeling_tf_distilbert.self.lin1->tensorflow.keras.layers.Dense(config.hidden_dim, kernel_initializer=get_initializer(config.initializer_range), name='lin1')
A:transformers.modeling_tf_distilbert.self.lin2->tensorflow.keras.layers.Dense(config.dim, kernel_initializer=get_initializer(config.initializer_range), name='lin2')
A:transformers.modeling_tf_distilbert.self.attention->TFMultiHeadSelfAttention(config, name='attention')
A:transformers.modeling_tf_distilbert.self.sa_layer_norm->tensorflow.keras.layers.LayerNormalization(epsilon=1e-12, name='sa_layer_norm')
A:transformers.modeling_tf_distilbert.self.ffn->TFFFN(config, name='ffn')
A:transformers.modeling_tf_distilbert.self.output_layer_norm->tensorflow.keras.layers.LayerNormalization(epsilon=1e-12, name='output_layer_norm')
A:transformers.modeling_tf_distilbert.sa_output->self.sa_layer_norm(sa_output + x)
A:transformers.modeling_tf_distilbert.ffn_output->self.output_layer_norm(ffn_output + sa_output)
A:transformers.modeling_tf_distilbert.layer_outputs->layer_module(hidden_state, attn_mask, head_mask[i], output_attentions, training=training)
A:transformers.modeling_tf_distilbert.self.embeddings->TFEmbeddings(config, name='embeddings')
A:transformers.modeling_tf_distilbert.self.transformer->TFTransformer(config, name='transformer')
A:transformers.modeling_tf_distilbert.input_ids->inputs.get('input_ids')
A:transformers.modeling_tf_distilbert.attention_mask->inputs.get('attention_mask', attention_mask)
A:transformers.modeling_tf_distilbert.head_mask->inputs.get('head_mask', head_mask)
A:transformers.modeling_tf_distilbert.output_attentions->inputs.get('output_attentions', output_attentions)
A:transformers.modeling_tf_distilbert.output_hidden_states->inputs.get('output_hidden_states', output_hidden_states)
A:transformers.modeling_tf_distilbert.return_dict->inputs.get('return_dict', return_dict)
A:transformers.modeling_tf_distilbert.input_shape->shape_list(input_ids)
A:transformers.modeling_tf_distilbert.embedding_output->self.embeddings(input_ids, inputs_embeds=inputs_embeds)
A:transformers.modeling_tf_distilbert.tfmr_output->self.transformer(embedding_output, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict, training=training)
A:transformers.modeling_tf_distilbert.self.distilbert->TFDistilBertMainLayer(config, name='distilbert')
A:transformers.modeling_tf_distilbert.outputs->self.distilbert(inputs, attention_mask=attention_mask, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, training=training)
A:transformers.modeling_tf_distilbert.self.bias->self.add_weight(shape=(self.vocab_size,), initializer='zeros', trainable=True, name='bias')
A:transformers.modeling_tf_distilbert.hidden_states->self.dropout(hidden_states, training=training)
A:transformers.modeling_tf_distilbert.self.vocab_transform->tensorflow.keras.layers.Dense(config.dim, kernel_initializer=get_initializer(config.initializer_range), name='vocab_transform')
A:transformers.modeling_tf_distilbert.self.act->tensorflow.keras.layers.Activation(gelu)
A:transformers.modeling_tf_distilbert.self.vocab_layer_norm->tensorflow.keras.layers.LayerNormalization(epsilon=1e-12, name='vocab_layer_norm')
A:transformers.modeling_tf_distilbert.self.vocab_projector->TFDistilBertLMHead(config, self.distilbert.embeddings, name='vocab_projector')
A:transformers.modeling_tf_distilbert.labels->inputs.get('labels', labels)
A:transformers.modeling_tf_distilbert.distilbert_output->self.distilbert(inputs, attention_mask=attention_mask, head_mask=head_mask, inputs_embeds=inputs_embeds, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, training=training)
A:transformers.modeling_tf_distilbert.prediction_logits->self.vocab_projector(prediction_logits)
A:transformers.modeling_tf_distilbert.self.pre_classifier->tensorflow.keras.layers.Dense(config.dim, kernel_initializer=get_initializer(config.initializer_range), activation='relu', name='pre_classifier')
A:transformers.modeling_tf_distilbert.self.classifier->tensorflow.keras.layers.Dense(1, kernel_initializer=get_initializer(config.initializer_range), name='classifier')
A:transformers.modeling_tf_distilbert.pooled_output->self.dropout(pooled_output, training=training)
A:transformers.modeling_tf_distilbert.sequence_output->self.dropout(sequence_output, training=training)
A:transformers.modeling_tf_distilbert.reshaped_logits->tensorflow.reshape(logits, (-1, num_choices))
A:transformers.modeling_tf_distilbert.self.qa_outputs->tensorflow.keras.layers.Dense(config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name='qa_outputs')
A:transformers.modeling_tf_distilbert.start_positions->inputs.pop('start_positions', start_positions)
A:transformers.modeling_tf_distilbert.end_positions->inputs.pop('end_positions', start_positions)
A:transformers.modeling_tf_distilbert.(start_logits, end_logits)->tensorflow.split(logits, 2, axis=-1)
A:transformers.modeling_tf_distilbert.start_logits->tensorflow.squeeze(start_logits, axis=-1)
A:transformers.modeling_tf_distilbert.end_logits->tensorflow.squeeze(end_logits, axis=-1)
A:transformers.modeling_tf_distilbert.loss->self.compute_loss(labels, (start_logits, end_logits))
transformers.TFDistilBertForMaskedLM(self,config,*inputs,**kwargs)
transformers.TFDistilBertForMaskedLM.call(self,inputs=None,attention_mask=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.TFDistilBertForMaskedLM.get_output_embeddings(self)
transformers.TFDistilBertForMultipleChoice(self,config,*inputs,**kwargs)
transformers.TFDistilBertForMultipleChoice.call(self,inputs,attention_mask=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.TFDistilBertForMultipleChoice.dummy_inputs(self)
transformers.TFDistilBertForQuestionAnswering(self,config,*inputs,**kwargs)
transformers.TFDistilBertForQuestionAnswering.call(self,inputs=None,attention_mask=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,start_positions=None,end_positions=None,training=False)
transformers.TFDistilBertForSequenceClassification(self,config,*inputs,**kwargs)
transformers.TFDistilBertForSequenceClassification.call(self,inputs=None,attention_mask=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.TFDistilBertForTokenClassification(self,config,*inputs,**kwargs)
transformers.TFDistilBertForTokenClassification.call(self,inputs=None,attention_mask=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.TFDistilBertMainLayer(self,config,**kwargs)
transformers.TFDistilBertMainLayer._prune_heads(self,heads_to_prune)
transformers.TFDistilBertMainLayer.call(self,inputs,attention_mask=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False)
transformers.TFDistilBertMainLayer.get_input_embeddings(self)
transformers.TFDistilBertMainLayer.set_input_embeddings(self,value)
transformers.TFDistilBertModel(self,config,*inputs,**kwargs)
transformers.TFDistilBertModel.call(self,inputs,**kwargs)
transformers.TFDistilBertPreTrainedModel(TFPreTrainedModel)
transformers.modeling_tf_distilbert.TFDistilBertForMaskedLM(self,config,*inputs,**kwargs)
transformers.modeling_tf_distilbert.TFDistilBertForMaskedLM.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_distilbert.TFDistilBertForMaskedLM.call(self,inputs=None,attention_mask=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.modeling_tf_distilbert.TFDistilBertForMaskedLM.get_output_embeddings(self)
transformers.modeling_tf_distilbert.TFDistilBertForMultipleChoice(self,config,*inputs,**kwargs)
transformers.modeling_tf_distilbert.TFDistilBertForMultipleChoice.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_distilbert.TFDistilBertForMultipleChoice.call(self,inputs,attention_mask=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.modeling_tf_distilbert.TFDistilBertForMultipleChoice.dummy_inputs(self)
transformers.modeling_tf_distilbert.TFDistilBertForQuestionAnswering(self,config,*inputs,**kwargs)
transformers.modeling_tf_distilbert.TFDistilBertForQuestionAnswering.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_distilbert.TFDistilBertForQuestionAnswering.call(self,inputs=None,attention_mask=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,start_positions=None,end_positions=None,training=False)
transformers.modeling_tf_distilbert.TFDistilBertForSequenceClassification(self,config,*inputs,**kwargs)
transformers.modeling_tf_distilbert.TFDistilBertForSequenceClassification.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_distilbert.TFDistilBertForSequenceClassification.call(self,inputs=None,attention_mask=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.modeling_tf_distilbert.TFDistilBertForTokenClassification(self,config,*inputs,**kwargs)
transformers.modeling_tf_distilbert.TFDistilBertForTokenClassification.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_distilbert.TFDistilBertForTokenClassification.call(self,inputs=None,attention_mask=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.modeling_tf_distilbert.TFDistilBertLMHead(self,config,input_embeddings,**kwargs)
transformers.modeling_tf_distilbert.TFDistilBertLMHead.__init__(self,config,input_embeddings,**kwargs)
transformers.modeling_tf_distilbert.TFDistilBertLMHead.build(self,input_shape)
transformers.modeling_tf_distilbert.TFDistilBertLMHead.call(self,hidden_states)
transformers.modeling_tf_distilbert.TFDistilBertMainLayer(self,config,**kwargs)
transformers.modeling_tf_distilbert.TFDistilBertMainLayer.__init__(self,config,**kwargs)
transformers.modeling_tf_distilbert.TFDistilBertMainLayer._prune_heads(self,heads_to_prune)
transformers.modeling_tf_distilbert.TFDistilBertMainLayer.call(self,inputs,attention_mask=None,head_mask=None,inputs_embeds=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False)
transformers.modeling_tf_distilbert.TFDistilBertMainLayer.get_input_embeddings(self)
transformers.modeling_tf_distilbert.TFDistilBertMainLayer.set_input_embeddings(self,value)
transformers.modeling_tf_distilbert.TFDistilBertModel(self,config,*inputs,**kwargs)
transformers.modeling_tf_distilbert.TFDistilBertModel.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_distilbert.TFDistilBertModel.call(self,inputs,**kwargs)
transformers.modeling_tf_distilbert.TFDistilBertPreTrainedModel(TFPreTrainedModel)
transformers.modeling_tf_distilbert.TFEmbeddings(self,config,**kwargs)
transformers.modeling_tf_distilbert.TFEmbeddings.__init__(self,config,**kwargs)
transformers.modeling_tf_distilbert.TFEmbeddings._embedding(self,input_ids,position_ids,inputs_embeds,training=False)
transformers.modeling_tf_distilbert.TFEmbeddings._linear(self,inputs)
transformers.modeling_tf_distilbert.TFEmbeddings.build(self,input_shape)
transformers.modeling_tf_distilbert.TFEmbeddings.call(self,input_ids=None,position_ids=None,inputs_embeds=None,mode='embedding',training=False)
transformers.modeling_tf_distilbert.TFFFN(self,config,**kwargs)
transformers.modeling_tf_distilbert.TFFFN.__init__(self,config,**kwargs)
transformers.modeling_tf_distilbert.TFFFN.call(self,input,training=False)
transformers.modeling_tf_distilbert.TFMultiHeadSelfAttention(self,config,**kwargs)
transformers.modeling_tf_distilbert.TFMultiHeadSelfAttention.__init__(self,config,**kwargs)
transformers.modeling_tf_distilbert.TFMultiHeadSelfAttention.call(self,query,key,value,mask,head_mask,output_attentions,training=False)
transformers.modeling_tf_distilbert.TFMultiHeadSelfAttention.prune_heads(self,heads)
transformers.modeling_tf_distilbert.TFTransformer(self,config,**kwargs)
transformers.modeling_tf_distilbert.TFTransformer.__init__(self,config,**kwargs)
transformers.modeling_tf_distilbert.TFTransformer.call(self,x,attn_mask,head_mask,output_attentions,output_hidden_states,return_dict,training=False)
transformers.modeling_tf_distilbert.TFTransformerBlock(self,config,**kwargs)
transformers.modeling_tf_distilbert.TFTransformerBlock.__init__(self,config,**kwargs)
transformers.modeling_tf_distilbert.TFTransformerBlock.call(self,x,attn_mask,head_mask,output_attentions,training=False)
transformers.modeling_tf_distilbert.gelu(x)
transformers.modeling_tf_distilbert.gelu_new(x)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/tokenization_transfo_xl.py----------------------------------------
A:transformers.tokenization_transfo_xl.logger->utils.logging.get_logger(__name__)
A:transformers.tokenization_transfo_xl.replaced->re.sub(reg, sub, text_array[i]).split()
A:transformers.tokenization_transfo_xl.text->tokenize_numbers(text)
A:transformers.tokenization_transfo_xl.self.counter->Counter()
A:transformers.tokenization_transfo_xl.self.punction_without_space_before_pattern->re.compile('[^\\s][{}]'.format(self.punctuation_symbols))
A:transformers.tokenization_transfo_xl.self.punctuation_with_space_around_pattern->self._compile_space_around_punctuation_pattern()
A:transformers.tokenization_transfo_xl.self.moses_punct_normalizer->sacremoses.MosesPunctNormalizer(language)
A:transformers.tokenization_transfo_xl.self.moses_tokenizer->sacremoses.MosesTokenizer(language)
A:transformers.tokenization_transfo_xl.self.moses_detokenizer->sacremoses.MosesDetokenizer(language)
A:transformers.tokenization_transfo_xl.vocab_dict->torch.load(pretrained_vocab_file)
A:transformers.tokenization_transfo_xl.look_ahead_for_special_token->'(?=[{}])'.format(self.punctuation_symbols)
A:transformers.tokenization_transfo_xl.symbols->self.moses_pipeline(line)
A:transformers.tokenization_transfo_xl.self.sym2idx->OrderedDict()
A:transformers.tokenization_transfo_xl.vocab_file->os.path.join(vocab_path, VOCAB_FILES_NAMES['pretrained_vocab_file'])
A:transformers.tokenization_transfo_xl.encoded->torch.cat(encoded)
A:transformers.tokenization_transfo_xl.out_string->self.moses_detokenizer.detokenize(tokens)
A:transformers.tokenization_transfo_xl.line->line.lower().lower()
A:transformers.tokenization_transfo_xl.tokenizer->Tokenizer(tokenizer)
A:transformers.tokenization_transfo_xl.tokenizer.post_processor->BertProcessing((eos_token, tokenizer.token_to_id(eos_token)), (eos_token, tokenizer.token_to_id(eos_token)))
A:transformers.tokenization_transfo_xl.data->torch.LongTensor(self.bptt, self.bsz)
A:transformers.tokenization_transfo_xl.self.data->torch.LongTensor(self.bptt, self.bsz).view(bsz, -1).t().contiguous().to(device)
A:transformers.tokenization_transfo_xl.seq_len->min(bptt, self.data.size(0) - 1 - i)
A:transformers.tokenization_transfo_xl.beg_idx->max(0, i - self.ext_len)
A:transformers.tokenization_transfo_xl.data_out->torch.LongTensor(self.bptt, self.bsz).transpose(0, 1).contiguous().to(self.device)
A:transformers.tokenization_transfo_xl.target_out->torch.LongTensor(self.bptt, self.bsz).transpose(0, 1).contiguous().to(self.device)
A:transformers.tokenization_transfo_xl.bptt->min(max_len, max(min_len, int(np.random.normal(bptt, std))))
A:transformers.tokenization_transfo_xl.(data, target, seq_len)->self.get_batch(i, bptt)
A:transformers.tokenization_transfo_xl.target->torch.LongTensor(self.bptt, self.bsz)
A:transformers.tokenization_transfo_xl.streams[i]->next(sent_stream)
A:transformers.tokenization_transfo_xl.n_new->min(len(streams[i]) - 1, self.bptt - n_filled)
A:transformers.tokenization_transfo_xl.n_retain->min(data.size(0), self.ext_len)
A:transformers.tokenization_transfo_xl.sent_stream->self.get_sent_stream(path)
A:transformers.tokenization_transfo_xl.sents->self.vocab.encode_file(path, add_double_eos=True)
A:transformers.tokenization_transfo_xl.vocab->TransfoXLTokenizer.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
A:transformers.tokenization_transfo_xl.corpus_file->os.path.join(pretrained_model_name_or_path, CORPUS_NAME)
A:transformers.tokenization_transfo_xl.resolved_corpus_file->cached_path(corpus_file, cache_dir=cache_dir)
A:transformers.tokenization_transfo_xl.corpus->TransfoXLCorpus(datadir, dataset, **kwargs)
A:transformers.tokenization_transfo_xl.corpus_dict->torch.load(resolved_corpus_file)
A:transformers.tokenization_transfo_xl.corpus.train->torch.tensor(corpus.train, dtype=torch.long)
A:transformers.tokenization_transfo_xl.corpus.valid->torch.tensor(corpus.valid, dtype=torch.long)
A:transformers.tokenization_transfo_xl.corpus.test->torch.tensor(corpus.test, dtype=torch.long)
A:transformers.tokenization_transfo_xl.self.vocab->TransfoXLTokenizer(*args, **kwargs)
A:transformers.tokenization_transfo_xl.train_path_pattern->os.path.join(path, '1-billion-word-language-modeling-benchmark-r13output', 'training-monolingual.tokenized.shuffled', 'news.en-*')
A:transformers.tokenization_transfo_xl.train_paths->glob.glob(train_path_pattern)
A:transformers.tokenization_transfo_xl.self.train->self.vocab.encode_file(os.path.join(path, 'train.txt'), ordered=True, add_eos=False)
A:transformers.tokenization_transfo_xl.self.valid->self.vocab.encode_file(os.path.join(path, 'valid.txt'), ordered=False, add_double_eos=True)
A:transformers.tokenization_transfo_xl.self.test->self.vocab.encode_file(os.path.join(path, 'test.txt'), ordered=False, add_double_eos=True)
A:transformers.tokenization_transfo_xl.data_iter->LMShuffledIterator(data, *args, **kwargs)
A:transformers.tokenization_transfo_xl.fn->os.path.join(datadir, 'cache.pt')
A:transformers.tokenization_transfo_xl.fn_pickle->os.path.join(datadir, 'cache.pkl')
A:transformers.tokenization_transfo_xl.kwargs['vocab_file']->os.path.join(datadir, '1b_word_vocab.txt')
transformers.TransfoXLCorpus(self,*args,**kwargs)
transformers.TransfoXLCorpus.build_corpus(self,path,dataset)
transformers.TransfoXLCorpus.from_pretrained(cls,pretrained_model_name_or_path,cache_dir=None,*inputs,**kwargs)
transformers.TransfoXLCorpus.get_iterator(self,split,*args,**kwargs)
transformers.TransfoXLTokenizer(self,special=None,min_freq=0,max_size=None,lower_case=False,delimiter=None,vocab_file=None,pretrained_vocab_file=None,never_split=None,unk_token='<unk>',eos_token='<eos>',additional_special_tokens=['<formula>'],language='en',**kwargs)
transformers.TransfoXLTokenizer._build_from_file(self,vocab_file)
transformers.TransfoXLTokenizer._compile_space_around_punctuation_pattern(self)
transformers.TransfoXLTokenizer._convert_id_to_token(self,idx)
transformers.TransfoXLTokenizer._convert_token_to_id(self,sym)
transformers.TransfoXLTokenizer._tokenize(self,line,add_eos=False,add_double_eos=False)
transformers.TransfoXLTokenizer.add_special(self,sym)
transformers.TransfoXLTokenizer.add_symbol(self,sym)
transformers.TransfoXLTokenizer.build_vocab(self)
transformers.TransfoXLTokenizer.convert_to_tensor(self,symbols)
transformers.TransfoXLTokenizer.convert_tokens_to_string(self,tokens)
transformers.TransfoXLTokenizer.count_file(self,path,verbose=False,add_eos=False)
transformers.TransfoXLTokenizer.count_sents(self,sents,verbose=False)
transformers.TransfoXLTokenizer.encode_file(self,path,ordered=False,verbose=False,add_eos=True,add_double_eos=False)
transformers.TransfoXLTokenizer.encode_sents(self,sents,ordered=False,verbose=False)
transformers.TransfoXLTokenizer.get_vocab(self)
transformers.TransfoXLTokenizer.moses_pipeline(self,text:str)->List[str]
transformers.TransfoXLTokenizer.moses_punct_norm(self,text)
transformers.TransfoXLTokenizer.moses_tokenize(self,text)
transformers.TransfoXLTokenizer.move_added_token(self,token:str,target_idx:int)
transformers.TransfoXLTokenizer.save_vocabulary(self,vocab_path)
transformers.TransfoXLTokenizer.vocab_size(self)
transformers.TransfoXLTokenizerFast(self,special=None,min_freq=0,max_size=None,lower_case=False,delimiter=None,vocab_file=None,pretrained_vocab_file=None,never_split=None,unk_token='<unk>',eos_token='<eos>',additional_special_tokens=['<formula>'],add_eos=False,add_double_eos=False,normalization=None,**kwargs)
transformers.TransfoXLTokenizerFast.save_pretrained(self,save_directory)
transformers.tokenization_transfo_xl.LMMultiFileIterator(self,paths,vocab,bsz,bptt,device='cpu',ext_len=None,shuffle=False)
transformers.tokenization_transfo_xl.LMMultiFileIterator.__init__(self,paths,vocab,bsz,bptt,device='cpu',ext_len=None,shuffle=False)
transformers.tokenization_transfo_xl.LMMultiFileIterator.__iter__(self)
transformers.tokenization_transfo_xl.LMMultiFileIterator.get_sent_stream(self,path)
transformers.tokenization_transfo_xl.LMOrderedIterator(self,data,bsz,bptt,device='cpu',ext_len=None)
transformers.tokenization_transfo_xl.LMOrderedIterator.__init__(self,data,bsz,bptt,device='cpu',ext_len=None)
transformers.tokenization_transfo_xl.LMOrderedIterator.__iter__(self)
transformers.tokenization_transfo_xl.LMOrderedIterator.get_batch(self,i,bptt=None)
transformers.tokenization_transfo_xl.LMOrderedIterator.get_fixlen_iter(self,start=0)
transformers.tokenization_transfo_xl.LMOrderedIterator.get_varlen_iter(self,start=0,std=5,min_len=5,max_deviation=3)
transformers.tokenization_transfo_xl.LMShuffledIterator(self,data,bsz,bptt,device='cpu',ext_len=None,shuffle=False)
transformers.tokenization_transfo_xl.LMShuffledIterator.__init__(self,data,bsz,bptt,device='cpu',ext_len=None,shuffle=False)
transformers.tokenization_transfo_xl.LMShuffledIterator.__iter__(self)
transformers.tokenization_transfo_xl.LMShuffledIterator.get_sent_stream(self)
transformers.tokenization_transfo_xl.LMShuffledIterator.stream_iterator(self,sent_stream)
transformers.tokenization_transfo_xl.TransfoXLCorpus(self,*args,**kwargs)
transformers.tokenization_transfo_xl.TransfoXLCorpus.__init__(self,*args,**kwargs)
transformers.tokenization_transfo_xl.TransfoXLCorpus.build_corpus(self,path,dataset)
transformers.tokenization_transfo_xl.TransfoXLCorpus.from_pretrained(cls,pretrained_model_name_or_path,cache_dir=None,*inputs,**kwargs)
transformers.tokenization_transfo_xl.TransfoXLCorpus.get_iterator(self,split,*args,**kwargs)
transformers.tokenization_transfo_xl.TransfoXLTokenizer(self,special=None,min_freq=0,max_size=None,lower_case=False,delimiter=None,vocab_file=None,pretrained_vocab_file=None,never_split=None,unk_token='<unk>',eos_token='<eos>',additional_special_tokens=['<formula>'],language='en',**kwargs)
transformers.tokenization_transfo_xl.TransfoXLTokenizer.__init__(self,special=None,min_freq=0,max_size=None,lower_case=False,delimiter=None,vocab_file=None,pretrained_vocab_file=None,never_split=None,unk_token='<unk>',eos_token='<eos>',additional_special_tokens=['<formula>'],language='en',**kwargs)
transformers.tokenization_transfo_xl.TransfoXLTokenizer._build_from_file(self,vocab_file)
transformers.tokenization_transfo_xl.TransfoXLTokenizer._compile_space_around_punctuation_pattern(self)
transformers.tokenization_transfo_xl.TransfoXLTokenizer._convert_id_to_token(self,idx)
transformers.tokenization_transfo_xl.TransfoXLTokenizer._convert_token_to_id(self,sym)
transformers.tokenization_transfo_xl.TransfoXLTokenizer._tokenize(self,line,add_eos=False,add_double_eos=False)
transformers.tokenization_transfo_xl.TransfoXLTokenizer.add_special(self,sym)
transformers.tokenization_transfo_xl.TransfoXLTokenizer.add_symbol(self,sym)
transformers.tokenization_transfo_xl.TransfoXLTokenizer.build_vocab(self)
transformers.tokenization_transfo_xl.TransfoXLTokenizer.convert_to_tensor(self,symbols)
transformers.tokenization_transfo_xl.TransfoXLTokenizer.convert_tokens_to_string(self,tokens)
transformers.tokenization_transfo_xl.TransfoXLTokenizer.count_file(self,path,verbose=False,add_eos=False)
transformers.tokenization_transfo_xl.TransfoXLTokenizer.count_sents(self,sents,verbose=False)
transformers.tokenization_transfo_xl.TransfoXLTokenizer.encode_file(self,path,ordered=False,verbose=False,add_eos=True,add_double_eos=False)
transformers.tokenization_transfo_xl.TransfoXLTokenizer.encode_sents(self,sents,ordered=False,verbose=False)
transformers.tokenization_transfo_xl.TransfoXLTokenizer.get_vocab(self)
transformers.tokenization_transfo_xl.TransfoXLTokenizer.moses_pipeline(self,text:str)->List[str]
transformers.tokenization_transfo_xl.TransfoXLTokenizer.moses_punct_norm(self,text)
transformers.tokenization_transfo_xl.TransfoXLTokenizer.moses_tokenize(self,text)
transformers.tokenization_transfo_xl.TransfoXLTokenizer.move_added_token(self,token:str,target_idx:int)
transformers.tokenization_transfo_xl.TransfoXLTokenizer.save_vocabulary(self,vocab_path)
transformers.tokenization_transfo_xl.TransfoXLTokenizer.vocab_size(self)
transformers.tokenization_transfo_xl.TransfoXLTokenizerFast(self,special=None,min_freq=0,max_size=None,lower_case=False,delimiter=None,vocab_file=None,pretrained_vocab_file=None,never_split=None,unk_token='<unk>',eos_token='<eos>',additional_special_tokens=['<formula>'],add_eos=False,add_double_eos=False,normalization=None,**kwargs)
transformers.tokenization_transfo_xl.TransfoXLTokenizerFast.__init__(self,special=None,min_freq=0,max_size=None,lower_case=False,delimiter=None,vocab_file=None,pretrained_vocab_file=None,never_split=None,unk_token='<unk>',eos_token='<eos>',additional_special_tokens=['<formula>'],add_eos=False,add_double_eos=False,normalization=None,**kwargs)
transformers.tokenization_transfo_xl.TransfoXLTokenizerFast.save_pretrained(self,save_directory)
transformers.tokenization_transfo_xl._TransfoXLDelimiterLookupTokenizer(self,vocab_file,delimiter,lowercase,unk_token,eos_token,add_eos=False,add_double_eos=False,normalization:Optional[str]=None)
transformers.tokenization_transfo_xl._TransfoXLDelimiterLookupTokenizer.__init__(self,vocab_file,delimiter,lowercase,unk_token,eos_token,add_eos=False,add_double_eos=False,normalization:Optional[str]=None)
transformers.tokenization_transfo_xl.detokenize_numbers(text:str)->str
transformers.tokenization_transfo_xl.get_lm_corpus(datadir,dataset)
transformers.tokenization_transfo_xl.tokenize_numbers(text_array:List[str])->List[str]


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/modeling_reformer.py----------------------------------------
A:transformers.modeling_reformer.logger->utils.logging.get_logger(__name__)
A:transformers.modeling_reformer.LSHSelfAttentionOutput->namedtuple('LSHSelfAttentionOutput', ['hidden_states', 'attention_probs', 'buckets'])
A:transformers.modeling_reformer.LocalSelfAttentionOutput->namedtuple('LocalSelfAttentionOutput', ['hidden_states', 'attention_probs'])
A:transformers.modeling_reformer.AttentionOutput->namedtuple('AttentionOutput', ['hidden_states', 'attention_probs', 'buckets'])
A:transformers.modeling_reformer.ReformerOutput->namedtuple('ReformerOutput', ['hidden_states', 'attn_output', 'attention_probs', 'buckets'])
A:transformers.modeling_reformer.ReformerBackwardOutput->namedtuple('ReformerBackwardOutput', ['attn_output', 'hidden_states', 'grad_attn_output', 'grad_hidden_states'])
A:transformers.modeling_reformer.ReformerEncoderOutput->namedtuple('ReformerEncoderOutput', ['hidden_states', 'all_hidden_states', 'all_attentions', 'past_buckets_states'])
A:transformers.modeling_reformer.scale_offset->scale_offset.expand(vector.shape).expand(vector.shape)
A:transformers.modeling_reformer.attn_types_set->set(attn_types)
A:transformers.modeling_reformer.self.least_common_mult_chunk_length->_get_least_common_mult_chunk_len(config)
A:transformers.modeling_reformer.self.weights->torch.nn.ParameterList()
A:transformers.modeling_reformer.weights->torch.cat(broadcasted_weights, dim=-1)
A:transformers.modeling_reformer.transposed_weights->torch.cat(broadcasted_weights, dim=-1).transpose(2, 1)
A:transformers.modeling_reformer.dropped_transposed_weights->torch.nn.functional.dropout2d(transposed_weights, p=self.dropout, training=self.training)
A:transformers.modeling_reformer.dropped_weights->torch.nn.functional.dropout2d(transposed_weights, p=self.dropout, training=self.training).transpose(2, 1)
A:transformers.modeling_reformer.position_encodings->torch.cat([torch.index_select(position_encodings[i], 0, position_ids[i]).unsqueeze(0) for i in range(batch_size)], dim=0)
A:transformers.modeling_reformer.max_position_id->torch.cat([position_ids, padded_position_ids], dim=-1).max().item()
A:transformers.modeling_reformer.self.embedding->torch.nn.Embedding(config.max_position_embeddings, config.hidden_size)
A:transformers.modeling_reformer.position_embeddings->self.position_embeddings(position_ids)
A:transformers.modeling_reformer.self.word_embeddings->torch.nn.Embedding(config.vocab_size, config.hidden_size)
A:transformers.modeling_reformer.input_shape->torch.cat([inputs_embeds, padded_inputs_embeds], dim=-2).size()
A:transformers.modeling_reformer.position_ids->torch.cat([position_ids, padded_position_ids], dim=-1)
A:transformers.modeling_reformer.inputs_embeds->torch.cat([inputs_embeds, padded_inputs_embeds], dim=-2)
A:transformers.modeling_reformer.embeddings->torch.nn.functional.dropout(inputs_embeds, p=self.dropout, training=self.training)
A:transformers.modeling_reformer.x->x.permute(0, 2, 1, 3).permute(0, 2, 1, 3)
A:transformers.modeling_reformer.self.query_key->torch.nn.Linear(self.hidden_size, self.all_head_size, bias=False)
A:transformers.modeling_reformer.self.value->torch.nn.Linear(self.hidden_size, self.all_head_size, bias=False)
A:transformers.modeling_reformer.query_vectors->self._split_seq_length_dim_to(query_vectors, -1, self.chunk_length, self.num_attention_heads, self.attention_head_size)
A:transformers.modeling_reformer.(key_value_hidden_states, sorted_bucket_idx, buckets)->self._get_relevant_hid_states_and_buckets(query_vectors=query_vectors, attention_mask=attention_mask, num_hashes=num_hashes, hidden_states=hidden_states, past_states=past_states, past_buckets=past_buckets)
A:transformers.modeling_reformer.query_key_vectors->torch.einsum('balh,ahr->balr', hidden_states, per_head_query_key)
A:transformers.modeling_reformer.value_vectors->self._look_adjacent(value_vectors, self.num_chunks_before, self.num_chunks_after)
A:transformers.modeling_reformer.key_value_hidden_states->torch.cat([key_value_hidden_states, hidden_states], dim=1)
A:transformers.modeling_reformer.buckets->torch.where(buckets_mask, buckets, torch.tensor(num_buckets - 1, dtype=torch.long, device=buckets.device))
A:transformers.modeling_reformer.(sorted_bucket_idx, undo_sorted_bucket_idx)->self._get_sorted_bucket_idx_and_undo_sorted_bucket_idx(sequence_length, buckets, num_hashes)
A:transformers.modeling_reformer.sorted_bucket_idx_per_hash->torch.arange(sequence_length, device=query_key_vectors.device).repeat(batch_size, self.num_attention_heads, 1)
A:transformers.modeling_reformer.key_vectors->self._look_adjacent(key_vectors, self.num_chunks_before, self.num_chunks_after)
A:transformers.modeling_reformer.(out_vectors, logits, attention_probs)->self._attend(query_vectors=query_vectors, key_vectors=key_vectors, value_vectors=value_vectors, sorted_bucket_idx_per_hash=sorted_bucket_idx_per_hash, attention_mask=attention_mask, head_mask=head_mask, do_standard_self_attention=do_standard_self_attention, do_cached_attention=do_cached_attention)
A:transformers.modeling_reformer.(out_vectors, logits)->ReverseSort.apply(out_vectors, logits, sorted_bucket_idx, undo_sorted_bucket_idx)
A:transformers.modeling_reformer.out_vectors->self._merge_hidden_size_dims(out_vectors, self.num_attention_heads, self.attention_head_size)
A:transformers.modeling_reformer.logits->self.qa_outputs(sequence_output)
A:transformers.modeling_reformer.probs_vectors->torch.exp(logits - torch.logsumexp(logits, dim=2, keepdim=True))
A:transformers.modeling_reformer.per_head_query_key->self.query_key.weight.reshape(self.num_attention_heads, self.attention_head_size, self.hidden_size).transpose(-2, -1)
A:transformers.modeling_reformer.per_head_value->self.value.weight.reshape(self.num_attention_heads, self.attention_head_size, self.hidden_size).transpose(-2, -1)
A:transformers.modeling_reformer.vectors->vectors.repeat(1, 1, num_hashes, 1).repeat(1, 1, num_hashes, 1)
A:transformers.modeling_reformer.random_rotations->torch.randn(rotations_shape, device=vectors.device, dtype=vectors.dtype)
A:transformers.modeling_reformer.rotated_vectors->torch.cat([rotated_vectors, -rotated_vectors], dim=-1)
A:transformers.modeling_reformer.rotated_vectors_factor->torch.cat([rotated_vectors_factor, -rotated_vectors_factor], dim=-1)
A:transformers.modeling_reformer.buckets_mask->torch.cat([torch.ones(input_shape, device=device, dtype=torch.uint8), torch.zeros((input_shape[0], padding_length), device=device, dtype=torch.uint8)], dim=-1).to(torch.uint8)[:, None, None, :].expand(buckets.shape)
A:transformers.modeling_reformer.offsets->offsets.expand((batch_size, self.num_attention_heads) + offsets.shape[-2:]).expand((batch_size, self.num_attention_heads) + offsets.shape[-2:])
A:transformers.modeling_reformer.offset_buckets->(buckets + offsets).flatten(start_dim=2, end_dim=3)
A:transformers.modeling_reformer.sorted_bucket_idx->_stable_argsort(buckets, dim=-1)
A:transformers.modeling_reformer.indices->torch.arange(sequence_length, device=query_vectors.device).repeat(batch_size, self.num_attention_heads, 1)
A:transformers.modeling_reformer.undo_sorted_bucket_idx->_stable_argsort(buckets, dim=-1).new(*sorted_bucket_idx.size())
A:transformers.modeling_reformer.query_key_dots->torch.where(mask, query_key_dots, mask_value)
A:transformers.modeling_reformer.query_bucket_idx->self._split_seq_length_dim_to(sorted_bucket_idx_per_hash, -1, self.chunk_length, self.num_attention_heads)
A:transformers.modeling_reformer.key_value_bucket_idx->torch.arange(query_key_dots.shape[-1], dtype=torch.long, device=query_key_dots.device)[None, None, :].expand(query_bucket_idx.shape[:2] + (-1,))
A:transformers.modeling_reformer.self_mask_value->self.self_mask_value_float16.half()
A:transformers.modeling_reformer.mask_value->self.mask_value_float16.half()
A:transformers.modeling_reformer.mask->self._compute_attn_mask(query_indices, key_indices, attention_mask, query_key_dots.shape, do_standard_self_attention)
A:transformers.modeling_reformer.self_mask->torch.ne(query_bucket_idx.unsqueeze(-1), key_value_bucket_idx.unsqueeze(-2)).to(query_bucket_idx.device)
A:transformers.modeling_reformer.attention_probs->torch.nn.functional.dropout(attention_probs, p=self.dropout, training=self.training)
A:transformers.modeling_reformer.attention_mask->torch.cat([torch.ones(input_shape, device=device, dtype=torch.uint8), torch.zeros((input_shape[0], padding_length), device=device, dtype=torch.uint8)], dim=-1)
A:transformers.modeling_reformer.causal_mask->torch.ge(query_indices.unsqueeze(-1), key_indices.unsqueeze(-2)).to(query_indices.device)
A:transformers.modeling_reformer.hidden_states->self.out_proj(hidden_states)
A:transformers.modeling_reformer.query_buckets->self._hash_vectors(query_vectors, num_hashes, attention_mask, increase_num_buckets=increase_num_buckets)
A:transformers.modeling_reformer.concat_buckets->torch.cat([past_buckets, query_buckets.unsqueeze(-1)], dim=-1)
A:transformers.modeling_reformer.bucket_idx->_stable_argsort(concat_buckets, dim=-1)
A:transformers.modeling_reformer.relevant_bucket_idx->(bucket_idx == bucket_idx.shape[-1] - 1).nonzero()
A:transformers.modeling_reformer.relevant_bucket_idx_chunk->relevant_bucket_idx_chunk.reshape(batch_size, self.num_attention_heads, num_hashes, -1).reshape(batch_size, self.num_attention_heads, num_hashes, -1)
A:transformers.modeling_reformer.relevant_hidden_states->relevant_hidden_states.reshape(batch_size, self.num_attention_heads, -1, self.hidden_size).reshape(batch_size, self.num_attention_heads, -1, self.hidden_size)
A:transformers.modeling_reformer.expanded_start_indices->start_indices_chunk.unsqueeze(-1).expand(indices.shape[0], total_chunk_size)
A:transformers.modeling_reformer.variance->torch.mean(x ** 2, -1, keepdim=True)
A:transformers.modeling_reformer.expanded_idxs->idxs.unsqueeze(-1).expand(-1, -1, -1, self.attention_head_size)
A:transformers.modeling_reformer.expanded_undo_sort_indices->_stable_argsort(buckets, dim=-1).new(*sorted_bucket_idx.size()).unsqueeze(-1).expand(out_vectors.shape)
A:transformers.modeling_reformer.expanded_sort_indices->_stable_argsort(buckets, dim=-1).unsqueeze(-1).expand(grad_out_vectors.shape)
A:transformers.modeling_reformer.grad_out_vectors->torch.gather(grad_out_vectors, 2, expanded_sort_indices)
A:transformers.modeling_reformer.grad_logits->torch.gather(grad_logits, 2, sorted_bucket_idx)
A:transformers.modeling_reformer.self.query->torch.nn.Linear(self.hidden_size, self.all_head_size, bias=False)
A:transformers.modeling_reformer.self.key->torch.nn.Linear(self.hidden_size, self.all_head_size, bias=False)
A:transformers.modeling_reformer.query_indices->self._split_seq_length_dim_to(indices, -1, self.chunk_length, self.num_attention_heads)
A:transformers.modeling_reformer.key_indices->self._look_adjacent(key_indices, self.num_chunks_before, self.num_chunks_after)
A:transformers.modeling_reformer.self.dense->torch.nn.Linear(2 * config.hidden_size, config.hidden_size)
A:transformers.modeling_reformer.self.layer_norm->torch.nn.LayerNorm(2 * config.hidden_size, eps=config.layer_norm_eps)
A:transformers.modeling_reformer.self.self_attention->LocalSelfAttention(config)
A:transformers.modeling_reformer.self.output->ReformerFeedForwardOutput(config)
A:transformers.modeling_reformer.self_attention_outputs->self.self_attention(hidden_states=hidden_states, head_mask=head_mask, attention_mask=attention_mask, num_hashes=num_hashes, past_buckets_states=past_buckets_states_layer, use_cache=use_cache, output_attentions=output_attentions, buckets=buckets)
A:transformers.modeling_reformer.past_buckets->torch.cat([past_buckets_states[self.layer_id][0], buckets], dim=-1)
A:transformers.modeling_reformer.past_states->torch.cat([past_buckets_states[self.layer_id][1], hidden_states], dim=1)
A:transformers.modeling_reformer.attention_output->self.output(self_attention_outputs.hidden_states)
A:transformers.modeling_reformer.self.attention->ReformerAttention(config, layer_id)
A:transformers.modeling_reformer.self.feed_forward->ChunkReformerFeedForward(config)
A:transformers.modeling_reformer.device_idx->torch.cuda.current_device()
A:transformers.modeling_reformer.self.attention_seed->int(torch.seed() % sys.maxsize)
A:transformers.modeling_reformer.self.feed_forward_seed->int(torch.seed() % sys.maxsize)
A:transformers.modeling_reformer.attn_outputs->self.attention(hidden_states=hidden_states, head_mask=head_mask, attention_mask=attention_mask, num_hashes=num_hashes, past_buckets_states=past_buckets_states, use_cache=use_cache, orig_sequence_length=orig_sequence_length, output_attentions=output_attentions)
A:transformers.modeling_reformer.res_hidden_states->self.feed_forward(next_attn_output)
A:transformers.modeling_reformer.(hidden_states, attn_output)->torch.chunk(hidden_states, 2, dim=-1)
A:transformers.modeling_reformer.layer_outputs->layer(prev_attn_output=attn_output, hidden_states=hidden_states, attention_mask=attention_mask, head_mask=layer_head_mask, num_hashes=num_hashes, past_buckets_states=past_buckets_states, use_cache=use_cache, orig_sequence_length=orig_sequence_length, output_attentions=output_attentions)
A:transformers.modeling_reformer.(grad_attn_output, grad_hidden_states)->torch.chunk(grad_hidden_states, 2, dim=-1)
A:transformers.modeling_reformer.output->layer.backward_pass(next_attn_output=output.attn_output, hidden_states=output.hidden_states, grad_attn_output=output.grad_attn_output, grad_hidden_states=output.grad_hidden_states, head_mask=head_mask[len(layers) - idx - 1], attention_mask=attention_mask, buckets=buckets)
A:transformers.modeling_reformer.grad_hidden_states->torch.cat([output.grad_attn_output, output.grad_hidden_states], dim=-1)
A:transformers.modeling_reformer.self.layers->torch.nn.ModuleList([ReformerLayer(config, i) for i in range(config.num_hidden_layers)])
A:transformers.modeling_reformer.self.decoder->torch.nn.Linear(2 * config.hidden_size, config.vocab_size, bias=False)
A:transformers.modeling_reformer.self.bias->torch.nn.Parameter(torch.zeros(config.vocab_size))
A:transformers.modeling_reformer.input_ids->torch.cat([input_ids, padded_input_ids], dim=-1)
A:transformers.modeling_reformer.input_mask->torch.tensor(DUMMY_MASK)
A:transformers.modeling_reformer.self.embeddings->ReformerEmbeddings(config)
A:transformers.modeling_reformer.self.encoder->ReformerEncoder(config)
A:transformers.modeling_reformer.head_mask->self.get_head_mask(head_mask, self.config.num_hidden_layers, is_attention_chunked=True)
A:transformers.modeling_reformer.least_common_mult_chunk_length->_get_least_common_mult_chunk_len(self.config)
A:transformers.modeling_reformer.min_chunk_length->_get_min_chunk_len(self.config)
A:transformers.modeling_reformer.(input_ids, inputs_embeds, attention_mask, position_ids, input_shape)->self._pad_to_mult_of_chunk_length(input_ids, inputs_embeds=inputs_embeds, attention_mask=attention_mask, position_ids=position_ids, input_shape=input_shape, padding_length=padding_length, padded_seq_length=least_common_mult_chunk_length, device=device)
A:transformers.modeling_reformer.embedding_output->self.embeddings(input_ids=input_ids, position_ids=position_ids, inputs_embeds=inputs_embeds, start_idx_pos_encodings=start_idx_pos_encodings)
A:transformers.modeling_reformer.encoder_outputs->self.encoder(hidden_states=embedding_output, head_mask=head_mask, attention_mask=attention_mask, num_hashes=num_hashes, past_buckets_states=past_buckets_states, use_cache=use_cache, orig_sequence_length=orig_sequence_length, output_hidden_states=output_hidden_states, output_attentions=output_attentions)
A:transformers.modeling_reformer.padded_input_ids->torch.full((input_shape[0], padding_length), self.config.pad_token_id, device=device, dtype=torch.long)
A:transformers.modeling_reformer.pad_attention_mask->torch.zeros(input_shape[0], padding_length, device=device, dtype=attention_mask.dtype)
A:transformers.modeling_reformer.padded_position_ids->torch.cat([position_ids, padded_position_ids], dim=-1).unsqueeze(0).expand(input_shape[0], padding_length)
A:transformers.modeling_reformer.padded_inputs_embeds->self.embeddings(padded_input_ids, position_ids)
A:transformers.modeling_reformer.self.reformer->ReformerModel(config)
A:transformers.modeling_reformer.self.lm_head->ReformerOnlyLMHead(config)
A:transformers.modeling_reformer.reformer_outputs->self.reformer(input_ids, position_ids=position_ids, attention_mask=attention_mask, head_mask=head_mask, inputs_embeds=inputs_embeds, num_hashes=num_hashes, use_cache=False, output_hidden_states=output_hidden_states, output_attentions=output_attentions, return_dict=return_dict)
A:transformers.modeling_reformer.shift_logits->logits[..., :-1, :].contiguous()
A:transformers.modeling_reformer.shift_labels->labels[..., 1:].contiguous()
A:transformers.modeling_reformer.loss_fct->CrossEntropyLoss(ignore_index=ignored_index)
A:transformers.modeling_reformer.loss->loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
A:transformers.modeling_reformer.reord_buckets->layer_past[0].index_select(0, beam_idx)
A:transformers.modeling_reformer.reord_hidden_states->layer_past[1].index_select(0, beam_idx)
A:transformers.modeling_reformer.masked_lm_loss->loss_fct(logits.view(-1, self.config.vocab_size), labels.view(-1))
A:transformers.modeling_reformer.self.classifier->ReformerClassificationHead(config)
A:transformers.modeling_reformer.outputs->self.reformer(input_ids, position_ids=position_ids, attention_mask=attention_mask, head_mask=head_mask, inputs_embeds=inputs_embeds, num_hashes=num_hashes, output_hidden_states=output_hidden_states, output_attentions=output_attentions, return_dict=return_dict)
A:transformers.modeling_reformer.self.dropout->torch.nn.Dropout(config.hidden_dropout_prob)
A:transformers.modeling_reformer.self.out_proj->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.modeling_reformer.self.qa_outputs->torch.nn.Linear(2 * config.hidden_size, config.num_labels)
A:transformers.modeling_reformer.(start_logits, end_logits)->self.qa_outputs(sequence_output).split(1, dim=-1)
A:transformers.modeling_reformer.start_logits->start_logits.squeeze(-1).squeeze(-1)
A:transformers.modeling_reformer.end_logits->end_logits.squeeze(-1).squeeze(-1)
A:transformers.modeling_reformer.start_positions->start_positions.squeeze(-1).squeeze(-1)
A:transformers.modeling_reformer.end_positions->end_positions.squeeze(-1).squeeze(-1)
A:transformers.modeling_reformer.ignored_index->start_logits.squeeze(-1).squeeze(-1).size(1)
A:transformers.modeling_reformer.start_loss->loss_fct(start_logits, start_positions)
A:transformers.modeling_reformer.end_loss->loss_fct(end_logits, end_positions)
transformers.ReformerAttention(self,config,layer_id=0)
transformers.ReformerAttention.forward(self,hidden_states,attention_mask=None,head_mask=None,num_hashes=None,past_buckets_states=None,use_cache=False,orig_sequence_length=None,output_attentions=False,buckets=None)
transformers.ReformerForMaskedLM(self,config)
transformers.ReformerForMaskedLM.forward(self,input_ids=None,position_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,num_hashes=None,labels=None,output_hidden_states=None,output_attentions=None,return_dict=None)
transformers.ReformerForMaskedLM.get_output_embeddings(self)
transformers.ReformerForQuestionAnswering(self,config)
transformers.ReformerForQuestionAnswering.forward(self,input_ids=None,position_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,num_hashes=None,start_positions=None,end_positions=None,output_hidden_states=None,output_attentions=None,return_dict=None)
transformers.ReformerForSequenceClassification(self,config)
transformers.ReformerForSequenceClassification.forward(self,input_ids=None,position_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,num_hashes=None,labels=None,output_hidden_states=None,output_attentions=None,return_dict=None)
transformers.ReformerLayer(self,config,layer_id=0)
transformers.ReformerLayer._init_attention_seed(self)
transformers.ReformerLayer._init_feed_forward_seed(self)
transformers.ReformerLayer.backward_pass(self,next_attn_output,hidden_states,grad_attn_output,grad_hidden_states,attention_mask=None,head_mask=None,buckets=None)
transformers.ReformerLayer.forward(self,prev_attn_output,hidden_states,attention_mask=None,head_mask=None,num_hashes=None,past_buckets_states=None,use_cache=False,orig_sequence_length=None,output_attentions=False)
transformers.ReformerModel(self,config)
transformers.ReformerModel._pad_to_mult_of_chunk_length(self,input_ids,inputs_embeds=None,attention_mask=None,position_ids=None,input_shape=None,padding_length=None,padded_seq_length=None,device=None)
transformers.ReformerModel._prune_heads(self,heads_to_prune)
transformers.ReformerModel.forward(self,input_ids=None,attention_mask=None,position_ids=None,head_mask=None,inputs_embeds=None,num_hashes=None,past_buckets_states=None,use_cache=None,output_hidden_states=None,output_attentions=None,return_dict=None)
transformers.ReformerModel.get_input_embeddings(self)
transformers.ReformerModel.set_input_embeddings(self,value)
transformers.ReformerModelOutput(ModelOutput)
transformers.ReformerModelWithLMHead(self,config)
transformers.ReformerModelWithLMHead._reorder_cache(self,past,beam_idx)
transformers.ReformerModelWithLMHead.forward(self,input_ids=None,position_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,num_hashes=None,past_buckets_states=None,use_cache=None,output_hidden_states=None,output_attentions=None,return_dict=None,labels=None)
transformers.ReformerModelWithLMHead.get_output_embeddings(self)
transformers.ReformerModelWithLMHead.prepare_inputs_for_generation(self,input_ids,past,**kwargs)
transformers.ReformerModelWithLMHeadOutput(ModelOutput)
transformers.modeling_reformer.AxialPositionEmbeddings(self,config)
transformers.modeling_reformer.AxialPositionEmbeddings.__init__(self,config)
transformers.modeling_reformer.AxialPositionEmbeddings.forward(self,position_ids)
transformers.modeling_reformer.ChunkReformerFeedForward(self,config)
transformers.modeling_reformer.ChunkReformerFeedForward.__init__(self,config)
transformers.modeling_reformer.ChunkReformerFeedForward.forward(self,attention_output)
transformers.modeling_reformer.ChunkReformerFeedForward.forward_chunk(self,hidden_states)
transformers.modeling_reformer.EfficientAttentionMixin
transformers.modeling_reformer.EfficientAttentionMixin._look_adjacent(self,vectors,num_chunks_before,num_chunks_after)
transformers.modeling_reformer.EfficientAttentionMixin._merge_hidden_size_dims(self,x,num_attn_heads,attn_head_size)
transformers.modeling_reformer.EfficientAttentionMixin._split_hidden_size_dim(self,x,num_attn_heads,attn_head_size)
transformers.modeling_reformer.EfficientAttentionMixin._split_seq_length_dim_to(self,vectors,dim_factor_1,dim_factor_2,num_attn_heads,attn_head_size=None)
transformers.modeling_reformer.LSHSelfAttention(self,config)
transformers.modeling_reformer.LSHSelfAttention.__init__(self,config)
transformers.modeling_reformer.LSHSelfAttention._attend(self,query_vectors,key_vectors,value_vectors,sorted_bucket_idx_per_hash,attention_mask,head_mask,do_standard_self_attention,do_cached_attention)
transformers.modeling_reformer.LSHSelfAttention._compute_attn_mask(self,query_indices,key_indices,attention_mask,query_key_dot_shape,do_standard_self_attention)
transformers.modeling_reformer.LSHSelfAttention._expand_to_indices_in_relevant_chunk(self,indices,sequence_length)
transformers.modeling_reformer.LSHSelfAttention._gather_by_expansion(self,vectors,idxs,num_hashes)
transformers.modeling_reformer.LSHSelfAttention._get_relevant_hid_states_and_buckets(self,query_vectors,attention_mask,num_hashes,hidden_states,past_states,past_buckets)
transformers.modeling_reformer.LSHSelfAttention._get_sorted_bucket_idx_and_undo_sorted_bucket_idx(self,sequence_length,buckets,num_hashes)
transformers.modeling_reformer.LSHSelfAttention._hash_vectors(self,vectors,num_hashes,attention_mask,increase_num_buckets=False)
transformers.modeling_reformer.LSHSelfAttention._len_and_dim_norm(self,vectors)
transformers.modeling_reformer.LSHSelfAttention._len_norm(self,x,epsilon=1e-06)
transformers.modeling_reformer.LSHSelfAttention._query_per_attn_head(self,hidden_states)
transformers.modeling_reformer.LSHSelfAttention._set_num_buckets(self,sequence_length)
transformers.modeling_reformer.LSHSelfAttention._value_per_attn_head(self,hidden_states)
transformers.modeling_reformer.LSHSelfAttention.forward(self,hidden_states,attention_mask=None,head_mask=None,num_hashes=None,buckets=None,past_buckets_states=None,use_cache=False,output_attentions=False,**kwargs)
transformers.modeling_reformer.LocalSelfAttention(self,config)
transformers.modeling_reformer.LocalSelfAttention.__init__(self,config)
transformers.modeling_reformer.LocalSelfAttention._compute_attn_mask(self,query_indices,key_indices,attention_mask,query_key_dots_shape,do_standard_self_attention)
transformers.modeling_reformer.LocalSelfAttention._retrieve_relevant_hidden_states(previous_hidden_states,chunk_length,num_chunks_before)
transformers.modeling_reformer.LocalSelfAttention.forward(self,hidden_states,attention_mask=None,head_mask=None,past_buckets_states=None,use_cache=False,output_attentions=False,**kwargs)
transformers.modeling_reformer.PositionEmbeddings(self,config)
transformers.modeling_reformer.PositionEmbeddings.__init__(self,config)
transformers.modeling_reformer.PositionEmbeddings.forward(self,position_ids)
transformers.modeling_reformer.ReformerAttention(self,config,layer_id=0)
transformers.modeling_reformer.ReformerAttention.__init__(self,config,layer_id=0)
transformers.modeling_reformer.ReformerAttention.forward(self,hidden_states,attention_mask=None,head_mask=None,num_hashes=None,past_buckets_states=None,use_cache=False,orig_sequence_length=None,output_attentions=False,buckets=None)
transformers.modeling_reformer.ReformerClassificationHead(self,config)
transformers.modeling_reformer.ReformerClassificationHead.__init__(self,config)
transformers.modeling_reformer.ReformerClassificationHead.forward(self,hidden_states,**kwargs)
transformers.modeling_reformer.ReformerEmbeddings(self,config)
transformers.modeling_reformer.ReformerEmbeddings.__init__(self,config)
transformers.modeling_reformer.ReformerEmbeddings.forward(self,input_ids=None,position_ids=None,inputs_embeds=None,start_idx_pos_encodings=0)
transformers.modeling_reformer.ReformerEncoder(self,config)
transformers.modeling_reformer.ReformerEncoder.__init__(self,config)
transformers.modeling_reformer.ReformerEncoder.forward(self,hidden_states,attention_mask=None,head_mask=None,num_hashes=None,past_buckets_states=None,use_cache=False,orig_sequence_length=None,output_hidden_states=False,output_attentions=False)
transformers.modeling_reformer.ReformerFeedForwardDense(self,config)
transformers.modeling_reformer.ReformerFeedForwardDense.__init__(self,config)
transformers.modeling_reformer.ReformerFeedForwardDense.forward(self,hidden_states)
transformers.modeling_reformer.ReformerFeedForwardOutput(self,config)
transformers.modeling_reformer.ReformerFeedForwardOutput.__init__(self,config)
transformers.modeling_reformer.ReformerFeedForwardOutput.forward(self,hidden_states)
transformers.modeling_reformer.ReformerForMaskedLM(self,config)
transformers.modeling_reformer.ReformerForMaskedLM.__init__(self,config)
transformers.modeling_reformer.ReformerForMaskedLM.forward(self,input_ids=None,position_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,num_hashes=None,labels=None,output_hidden_states=None,output_attentions=None,return_dict=None)
transformers.modeling_reformer.ReformerForMaskedLM.get_output_embeddings(self)
transformers.modeling_reformer.ReformerForQuestionAnswering(self,config)
transformers.modeling_reformer.ReformerForQuestionAnswering.__init__(self,config)
transformers.modeling_reformer.ReformerForQuestionAnswering.forward(self,input_ids=None,position_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,num_hashes=None,start_positions=None,end_positions=None,output_hidden_states=None,output_attentions=None,return_dict=None)
transformers.modeling_reformer.ReformerForSequenceClassification(self,config)
transformers.modeling_reformer.ReformerForSequenceClassification.__init__(self,config)
transformers.modeling_reformer.ReformerForSequenceClassification.forward(self,input_ids=None,position_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,num_hashes=None,labels=None,output_hidden_states=None,output_attentions=None,return_dict=None)
transformers.modeling_reformer.ReformerLayer(self,config,layer_id=0)
transformers.modeling_reformer.ReformerLayer.__init__(self,config,layer_id=0)
transformers.modeling_reformer.ReformerLayer._init_attention_seed(self)
transformers.modeling_reformer.ReformerLayer._init_feed_forward_seed(self)
transformers.modeling_reformer.ReformerLayer.backward_pass(self,next_attn_output,hidden_states,grad_attn_output,grad_hidden_states,attention_mask=None,head_mask=None,buckets=None)
transformers.modeling_reformer.ReformerLayer.forward(self,prev_attn_output,hidden_states,attention_mask=None,head_mask=None,num_hashes=None,past_buckets_states=None,use_cache=False,orig_sequence_length=None,output_attentions=False)
transformers.modeling_reformer.ReformerModel(self,config)
transformers.modeling_reformer.ReformerModel.__init__(self,config)
transformers.modeling_reformer.ReformerModel._pad_to_mult_of_chunk_length(self,input_ids,inputs_embeds=None,attention_mask=None,position_ids=None,input_shape=None,padding_length=None,padded_seq_length=None,device=None)
transformers.modeling_reformer.ReformerModel._prune_heads(self,heads_to_prune)
transformers.modeling_reformer.ReformerModel.forward(self,input_ids=None,attention_mask=None,position_ids=None,head_mask=None,inputs_embeds=None,num_hashes=None,past_buckets_states=None,use_cache=None,output_hidden_states=None,output_attentions=None,return_dict=None)
transformers.modeling_reformer.ReformerModel.get_input_embeddings(self)
transformers.modeling_reformer.ReformerModel.set_input_embeddings(self,value)
transformers.modeling_reformer.ReformerModelOutput(ModelOutput)
transformers.modeling_reformer.ReformerModelWithLMHead(self,config)
transformers.modeling_reformer.ReformerModelWithLMHead.__init__(self,config)
transformers.modeling_reformer.ReformerModelWithLMHead._reorder_cache(self,past,beam_idx)
transformers.modeling_reformer.ReformerModelWithLMHead.forward(self,input_ids=None,position_ids=None,attention_mask=None,head_mask=None,inputs_embeds=None,num_hashes=None,past_buckets_states=None,use_cache=None,output_hidden_states=None,output_attentions=None,return_dict=None,labels=None)
transformers.modeling_reformer.ReformerModelWithLMHead.get_output_embeddings(self)
transformers.modeling_reformer.ReformerModelWithLMHead.prepare_inputs_for_generation(self,input_ids,past,**kwargs)
transformers.modeling_reformer.ReformerModelWithLMHeadOutput(ModelOutput)
transformers.modeling_reformer.ReformerOnlyLMHead(self,config)
transformers.modeling_reformer.ReformerOnlyLMHead.__init__(self,config)
transformers.modeling_reformer.ReformerOnlyLMHead.forward(self,hidden_states)
transformers.modeling_reformer.ReformerOnlyLMHead.forward_chunk(self,hidden_states)
transformers.modeling_reformer.ReformerPreTrainedModel(PreTrainedModel)
transformers.modeling_reformer.ReformerPreTrainedModel._init_weights(self,module)
transformers.modeling_reformer.ReformerPreTrainedModel.dummy_inputs(self)
transformers.modeling_reformer.ReformerSelfOutput(self,config)
transformers.modeling_reformer.ReformerSelfOutput.__init__(self,config)
transformers.modeling_reformer.ReformerSelfOutput.forward(self,hidden_states)
transformers.modeling_reformer.ReverseSort(Function)
transformers.modeling_reformer.ReverseSort.backward(ctx,grad_out_vectors,grad_logits)
transformers.modeling_reformer.ReverseSort.forward(ctx,out_vectors,logits,sorted_bucket_idx,undo_sorted_bucket_idx)
transformers.modeling_reformer._ReversibleFunction(Function)
transformers.modeling_reformer._ReversibleFunction.backward(ctx,grad_hidden_states)
transformers.modeling_reformer._ReversibleFunction.forward(ctx,hidden_states,layers,attention_mask,head_mask,num_hashes,all_hidden_states,all_attentions,past_buckets_states,use_cache,orig_sequence_length,output_hidden_states,output_attentions)
transformers.modeling_reformer._get_least_common_mult_chunk_len(config)
transformers.modeling_reformer._get_min_chunk_len(config)
transformers.modeling_reformer._stable_argsort(vector,dim)
transformers.modeling_reformer.mish(x)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/modeling_tf_auto.py----------------------------------------
A:transformers.modeling_tf_auto.logger->utils.logging.get_logger(__name__)
A:transformers.modeling_tf_auto.TF_MODEL_MAPPING->OrderedDict([(T5Config, TFT5Model), (DistilBertConfig, TFDistilBertModel), (AlbertConfig, TFAlbertModel), (CamembertConfig, TFCamembertModel), (XLMRobertaConfig, TFXLMRobertaModel), (LongformerConfig, TFLongformerModel), (RobertaConfig, TFRobertaModel), (BertConfig, TFBertModel), (OpenAIGPTConfig, TFOpenAIGPTModel), (GPT2Config, TFGPT2Model), (MobileBertConfig, TFMobileBertModel), (TransfoXLConfig, TFTransfoXLModel), (XLNetConfig, TFXLNetModel), (FlaubertConfig, TFFlaubertModel), (XLMConfig, TFXLMModel), (CTRLConfig, TFCTRLModel), (ElectraConfig, TFElectraModel)])
A:transformers.modeling_tf_auto.TF_MODEL_FOR_PRETRAINING_MAPPING->OrderedDict([(T5Config, TFT5ForConditionalGeneration), (DistilBertConfig, TFDistilBertForMaskedLM), (AlbertConfig, TFAlbertForPreTraining), (CamembertConfig, TFCamembertForMaskedLM), (XLMRobertaConfig, TFXLMRobertaForMaskedLM), (RobertaConfig, TFRobertaForMaskedLM), (BertConfig, TFBertForPreTraining), (OpenAIGPTConfig, TFOpenAIGPTLMHeadModel), (GPT2Config, TFGPT2LMHeadModel), (MobileBertConfig, TFMobileBertForPreTraining), (TransfoXLConfig, TFTransfoXLLMHeadModel), (XLNetConfig, TFXLNetLMHeadModel), (FlaubertConfig, TFFlaubertWithLMHeadModel), (XLMConfig, TFXLMWithLMHeadModel), (CTRLConfig, TFCTRLLMHeadModel), (ElectraConfig, TFElectraForPreTraining)])
A:transformers.modeling_tf_auto.TF_MODEL_WITH_LM_HEAD_MAPPING->OrderedDict([(T5Config, TFT5ForConditionalGeneration), (DistilBertConfig, TFDistilBertForMaskedLM), (AlbertConfig, TFAlbertForMaskedLM), (CamembertConfig, TFCamembertForMaskedLM), (XLMRobertaConfig, TFXLMRobertaForMaskedLM), (LongformerConfig, TFLongformerForMaskedLM), (RobertaConfig, TFRobertaForMaskedLM), (BertConfig, TFBertForMaskedLM), (OpenAIGPTConfig, TFOpenAIGPTLMHeadModel), (GPT2Config, TFGPT2LMHeadModel), (MobileBertConfig, TFMobileBertForMaskedLM), (TransfoXLConfig, TFTransfoXLLMHeadModel), (XLNetConfig, TFXLNetLMHeadModel), (FlaubertConfig, TFFlaubertWithLMHeadModel), (XLMConfig, TFXLMWithLMHeadModel), (CTRLConfig, TFCTRLLMHeadModel), (ElectraConfig, TFElectraForMaskedLM)])
A:transformers.modeling_tf_auto.TF_MODEL_FOR_CAUSAL_LM_MAPPING->OrderedDict([(BertConfig, TFBertLMHeadModel), (OpenAIGPTConfig, TFOpenAIGPTLMHeadModel), (GPT2Config, TFGPT2LMHeadModel), (TransfoXLConfig, TFTransfoXLLMHeadModel), (XLNetConfig, TFXLNetLMHeadModel), (XLMConfig, TFXLMWithLMHeadModel), (CTRLConfig, TFCTRLLMHeadModel)])
A:transformers.modeling_tf_auto.TF_MODEL_FOR_MASKED_LM_MAPPING->OrderedDict([(DistilBertConfig, TFDistilBertForMaskedLM), (AlbertConfig, TFAlbertForMaskedLM), (CamembertConfig, TFCamembertForMaskedLM), (XLMRobertaConfig, TFXLMRobertaForMaskedLM), (LongformerConfig, TFLongformerForMaskedLM), (RobertaConfig, TFRobertaForMaskedLM), (BertConfig, TFBertForMaskedLM), (MobileBertConfig, TFMobileBertForMaskedLM), (FlaubertConfig, TFFlaubertWithLMHeadModel), (XLMConfig, TFXLMWithLMHeadModel), (ElectraConfig, TFElectraForMaskedLM)])
A:transformers.modeling_tf_auto.TF_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING->OrderedDict([(T5Config, TFT5ForConditionalGeneration)])
A:transformers.modeling_tf_auto.TF_MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING->OrderedDict([(DistilBertConfig, TFDistilBertForSequenceClassification), (AlbertConfig, TFAlbertForSequenceClassification), (CamembertConfig, TFCamembertForSequenceClassification), (XLMRobertaConfig, TFXLMRobertaForSequenceClassification), (RobertaConfig, TFRobertaForSequenceClassification), (BertConfig, TFBertForSequenceClassification), (XLNetConfig, TFXLNetForSequenceClassification), (MobileBertConfig, TFMobileBertForSequenceClassification), (FlaubertConfig, TFFlaubertForSequenceClassification), (XLMConfig, TFXLMForSequenceClassification), (ElectraConfig, TFElectraForSequenceClassification)])
A:transformers.modeling_tf_auto.TF_MODEL_FOR_QUESTION_ANSWERING_MAPPING->OrderedDict([(DistilBertConfig, TFDistilBertForQuestionAnswering), (AlbertConfig, TFAlbertForQuestionAnswering), (CamembertConfig, TFCamembertForQuestionAnswering), (XLMRobertaConfig, TFXLMRobertaForQuestionAnswering), (LongformerConfig, TFLongformerForQuestionAnswering), (RobertaConfig, TFRobertaForQuestionAnswering), (BertConfig, TFBertForQuestionAnswering), (XLNetConfig, TFXLNetForQuestionAnsweringSimple), (MobileBertConfig, TFMobileBertForQuestionAnswering), (FlaubertConfig, TFFlaubertForQuestionAnsweringSimple), (XLMConfig, TFXLMForQuestionAnsweringSimple), (ElectraConfig, TFElectraForQuestionAnswering)])
A:transformers.modeling_tf_auto.TF_MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING->OrderedDict([(DistilBertConfig, TFDistilBertForTokenClassification), (AlbertConfig, TFAlbertForTokenClassification), (CamembertConfig, TFCamembertForTokenClassification), (FlaubertConfig, TFFlaubertForTokenClassification), (XLMConfig, TFXLMForTokenClassification), (XLMRobertaConfig, TFXLMRobertaForTokenClassification), (RobertaConfig, TFRobertaForTokenClassification), (BertConfig, TFBertForTokenClassification), (MobileBertConfig, TFMobileBertForTokenClassification), (XLNetConfig, TFXLNetForTokenClassification), (ElectraConfig, TFElectraForTokenClassification)])
A:transformers.modeling_tf_auto.TF_MODEL_FOR_MULTIPLE_CHOICE_MAPPING->OrderedDict([(CamembertConfig, TFCamembertForMultipleChoice), (XLMConfig, TFXLMForMultipleChoice), (XLMRobertaConfig, TFXLMRobertaForMultipleChoice), (RobertaConfig, TFRobertaForMultipleChoice), (BertConfig, TFBertForMultipleChoice), (DistilBertConfig, TFDistilBertForMultipleChoice), (MobileBertConfig, TFMobileBertForMultipleChoice), (XLNetConfig, TFXLNetForMultipleChoice), (FlaubertConfig, TFFlaubertForMultipleChoice), (AlbertConfig, TFAlbertForMultipleChoice), (ElectraConfig, TFElectraForMultipleChoice)])
A:transformers.modeling_tf_auto.config->kwargs.pop('config', None)
A:transformers.modeling_tf_auto.(config, kwargs)->configuration_auto.AutoConfig.from_pretrained(pretrained_model_name_or_path, return_unused_kwargs=True, **kwargs)
transformers.TFAutoModel(self)
transformers.TFAutoModel.from_config(cls,config)
transformers.TFAutoModel.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.TFAutoModelForCausalLM(self)
transformers.TFAutoModelForCausalLM.from_config(cls,config)
transformers.TFAutoModelForCausalLM.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.TFAutoModelForMaskedLM(self)
transformers.TFAutoModelForMaskedLM.from_config(cls,config)
transformers.TFAutoModelForMaskedLM.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.TFAutoModelForMultipleChoice(self)
transformers.TFAutoModelForMultipleChoice.from_config(cls,config)
transformers.TFAutoModelForMultipleChoice.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.TFAutoModelForPreTraining(self)
transformers.TFAutoModelForPreTraining.from_config(cls,config)
transformers.TFAutoModelForPreTraining.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.TFAutoModelForQuestionAnswering(self)
transformers.TFAutoModelForQuestionAnswering.from_config(cls,config)
transformers.TFAutoModelForQuestionAnswering.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.TFAutoModelForSeq2SeqLM(self)
transformers.TFAutoModelForSeq2SeqLM.from_config(cls,config)
transformers.TFAutoModelForSeq2SeqLM.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.TFAutoModelForSequenceClassification(self)
transformers.TFAutoModelForSequenceClassification.from_config(cls,config)
transformers.TFAutoModelForSequenceClassification.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.TFAutoModelForTokenClassification(self)
transformers.TFAutoModelForTokenClassification.from_config(cls,config)
transformers.TFAutoModelForTokenClassification.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.TFAutoModelWithLMHead(self)
transformers.TFAutoModelWithLMHead.from_config(cls,config)
transformers.TFAutoModelWithLMHead.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.modeling_tf_auto.TFAutoModel(self)
transformers.modeling_tf_auto.TFAutoModel.__init__(self)
transformers.modeling_tf_auto.TFAutoModel.from_config(cls,config)
transformers.modeling_tf_auto.TFAutoModel.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.modeling_tf_auto.TFAutoModelForCausalLM(self)
transformers.modeling_tf_auto.TFAutoModelForCausalLM.__init__(self)
transformers.modeling_tf_auto.TFAutoModelForCausalLM.from_config(cls,config)
transformers.modeling_tf_auto.TFAutoModelForCausalLM.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.modeling_tf_auto.TFAutoModelForMaskedLM(self)
transformers.modeling_tf_auto.TFAutoModelForMaskedLM.__init__(self)
transformers.modeling_tf_auto.TFAutoModelForMaskedLM.from_config(cls,config)
transformers.modeling_tf_auto.TFAutoModelForMaskedLM.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.modeling_tf_auto.TFAutoModelForMultipleChoice(self)
transformers.modeling_tf_auto.TFAutoModelForMultipleChoice.__init__(self)
transformers.modeling_tf_auto.TFAutoModelForMultipleChoice.from_config(cls,config)
transformers.modeling_tf_auto.TFAutoModelForMultipleChoice.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.modeling_tf_auto.TFAutoModelForPreTraining(self)
transformers.modeling_tf_auto.TFAutoModelForPreTraining.__init__(self)
transformers.modeling_tf_auto.TFAutoModelForPreTraining.from_config(cls,config)
transformers.modeling_tf_auto.TFAutoModelForPreTraining.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.modeling_tf_auto.TFAutoModelForQuestionAnswering(self)
transformers.modeling_tf_auto.TFAutoModelForQuestionAnswering.__init__(self)
transformers.modeling_tf_auto.TFAutoModelForQuestionAnswering.from_config(cls,config)
transformers.modeling_tf_auto.TFAutoModelForQuestionAnswering.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.modeling_tf_auto.TFAutoModelForSeq2SeqLM(self)
transformers.modeling_tf_auto.TFAutoModelForSeq2SeqLM.__init__(self)
transformers.modeling_tf_auto.TFAutoModelForSeq2SeqLM.from_config(cls,config)
transformers.modeling_tf_auto.TFAutoModelForSeq2SeqLM.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.modeling_tf_auto.TFAutoModelForSequenceClassification(self)
transformers.modeling_tf_auto.TFAutoModelForSequenceClassification.__init__(self)
transformers.modeling_tf_auto.TFAutoModelForSequenceClassification.from_config(cls,config)
transformers.modeling_tf_auto.TFAutoModelForSequenceClassification.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.modeling_tf_auto.TFAutoModelForTokenClassification(self)
transformers.modeling_tf_auto.TFAutoModelForTokenClassification.__init__(self)
transformers.modeling_tf_auto.TFAutoModelForTokenClassification.from_config(cls,config)
transformers.modeling_tf_auto.TFAutoModelForTokenClassification.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.modeling_tf_auto.TFAutoModelWithLMHead(self)
transformers.modeling_tf_auto.TFAutoModelWithLMHead.__init__(self)
transformers.modeling_tf_auto.TFAutoModelWithLMHead.from_config(cls,config)
transformers.modeling_tf_auto.TFAutoModelWithLMHead.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/modeling_tf_ctrl.py----------------------------------------
A:transformers.modeling_tf_ctrl.logger->utils.logging.get_logger(__name__)
A:transformers.modeling_tf_ctrl.angle_rads->angle_defn(np.arange(position)[:, np.newaxis], np.arange(d_model_size)[np.newaxis, :], d_model_size)
A:transformers.modeling_tf_ctrl.sines->numpy.sin(angle_rads[:, 0::2])
A:transformers.modeling_tf_ctrl.cosines->numpy.cos(angle_rads[:, 1::2])
A:transformers.modeling_tf_ctrl.pos_encoding->tensorflow.cast(np.concatenate([sines, cosines], axis=-1), dtype=tf.float32)
A:transformers.modeling_tf_ctrl.matmul_qk->tensorflow.matmul(q, k, transpose_b=True)
A:transformers.modeling_tf_ctrl.dk->tensorflow.cast(shape_list(k)[-1], tf.float32)
A:transformers.modeling_tf_ctrl.attention_weights->tensorflow.nn.softmax(scaled_attention_logits, axis=-1)
A:transformers.modeling_tf_ctrl.output->self.dense(original_size_attention)
A:transformers.modeling_tf_ctrl.self.depth->int(d_model_size / self.num_heads)
A:transformers.modeling_tf_ctrl.self.Wq->tensorflow.keras.layers.Dense(d_model_size, name='Wq')
A:transformers.modeling_tf_ctrl.self.Wk->tensorflow.keras.layers.Dense(d_model_size, name='Wk')
A:transformers.modeling_tf_ctrl.self.Wv->tensorflow.keras.layers.Dense(d_model_size, name='Wv')
A:transformers.modeling_tf_ctrl.self.dense->tensorflow.keras.layers.Dense(d_model_size, name='dense')
A:transformers.modeling_tf_ctrl.x->tensorflow.reshape(x, (batch_size, -1, self.num_heads, self.depth))
A:transformers.modeling_tf_ctrl.q->self.split_into_heads(q, batch_size)
A:transformers.modeling_tf_ctrl.k->tensorflow.concat((past_key, k), axis=-2)
A:transformers.modeling_tf_ctrl.v->tensorflow.concat((past_value, v), axis=-2)
A:transformers.modeling_tf_ctrl.(past_key, past_value)->tensorflow.unstack(layer_past, axis=0)
A:transformers.modeling_tf_ctrl.present->tensorflow.stack((k, v), axis=0)
A:transformers.modeling_tf_ctrl.scaled_attention->tensorflow.transpose(output[0], perm=[0, 2, 1, 3])
A:transformers.modeling_tf_ctrl.original_size_attention->tensorflow.reshape(scaled_attention, (batch_size, -1, self.d_model_size))
A:transformers.modeling_tf_ctrl.self.dense_0->tensorflow.keras.layers.Dense(dff, activation='relu', name='0')
A:transformers.modeling_tf_ctrl.self.dense_2->tensorflow.keras.layers.Dense(d_model_size, name='2')
A:transformers.modeling_tf_ctrl.dense_0_output->self.dense_0(inputs)
A:transformers.modeling_tf_ctrl.dense_2_output->self.dense_2(dense_0_output)
A:transformers.modeling_tf_ctrl.self.multi_head_attention->TFMultiHeadAttention(d_model_size, num_heads, output_attentions=self.output_attentions, name='multi_head_attention')
A:transformers.modeling_tf_ctrl.self.ffn->TFPointWiseFeedForwardLayer(d_model_size, dff, name='ffn')
A:transformers.modeling_tf_ctrl.self.layernorm1->tensorflow.keras.layers.LayerNormalization(epsilon=layer_norm_epsilon, name='layernorm1')
A:transformers.modeling_tf_ctrl.self.layernorm2->tensorflow.keras.layers.LayerNormalization(epsilon=layer_norm_epsilon, name='layernorm2')
A:transformers.modeling_tf_ctrl.self.dropout1->tensorflow.keras.layers.Dropout(rate)
A:transformers.modeling_tf_ctrl.self.dropout2->tensorflow.keras.layers.Dropout(rate)
A:transformers.modeling_tf_ctrl.normed->self.layernorm1(x)
A:transformers.modeling_tf_ctrl.attn_outputs->self.multi_head_attention(normed, normed, normed, mask, layer_past, attention_mask, head_mask, use_cache, output_attentions, training=training)
A:transformers.modeling_tf_ctrl.attn_output->self.dropout1(attn_output, training=training)
A:transformers.modeling_tf_ctrl.out2->self.layernorm2(out1)
A:transformers.modeling_tf_ctrl.ffn_output->self.dropout2(ffn_output, training=training)
A:transformers.modeling_tf_ctrl.self.pos_encoding->positional_encoding(config.n_positions, self.d_model_size)
A:transformers.modeling_tf_ctrl.self.w->TFSharedEmbeddings(config.vocab_size, config.n_embd, initializer_range=config.initializer_range, name='w')
A:transformers.modeling_tf_ctrl.self.dropout->tensorflow.keras.layers.Dropout(config.embd_pdrop)
A:transformers.modeling_tf_ctrl.self.layernorm->tensorflow.keras.layers.LayerNormalization(epsilon=config.layer_norm_epsilon, name='layernorm')
A:transformers.modeling_tf_ctrl.input_ids->tensorflow.reshape(input_ids, [-1, input_shape[-1]])
A:transformers.modeling_tf_ctrl.past->tensorflow.expand_dims(inputs[:, -1], -1).get('past', past)
A:transformers.modeling_tf_ctrl.attention_mask->tensorflow.cast(attention_mask, tf.float32)
A:transformers.modeling_tf_ctrl.token_type_ids->tensorflow.reshape(token_type_ids, [-1, shape_list(token_type_ids)[-1]])
A:transformers.modeling_tf_ctrl.position_ids->tensorflow.reshape(position_ids, [-1, shape_list(position_ids)[-1]])
A:transformers.modeling_tf_ctrl.head_mask->tensorflow.expand_dims(inputs[:, -1], -1).get('head_mask', head_mask)
A:transformers.modeling_tf_ctrl.inputs_embeds->self.w(input_ids, mode='embedding')
A:transformers.modeling_tf_ctrl.use_cache->tensorflow.expand_dims(inputs[:, -1], -1).get('use_cache', use_cache)
A:transformers.modeling_tf_ctrl.output_attentions->tensorflow.expand_dims(inputs[:, -1], -1).get('output_attentions', output_attentions)
A:transformers.modeling_tf_ctrl.output_hidden_states->tensorflow.expand_dims(inputs[:, -1], -1).get('output_hidden_states', output_hidden_states)
A:transformers.modeling_tf_ctrl.return_dict->tensorflow.expand_dims(inputs[:, -1], -1).get('return_dict', return_dict)
A:transformers.modeling_tf_ctrl.input_shape->shape_list(input_ids)
A:transformers.modeling_tf_ctrl.token_type_embeds->self.w(token_type_ids, mode='embedding')
A:transformers.modeling_tf_ctrl.pos_embeds->tensorflow.gather(self.pos_encoding, position_ids)
A:transformers.modeling_tf_ctrl.hidden_states->self.input_embeddings(hidden_states, mode='linear')
A:transformers.modeling_tf_ctrl.outputs->self.transformer(inputs, **kwargs)
A:transformers.modeling_tf_ctrl.all_attentions->tuple((tf.reshape(t, attention_output_shape) for t in all_attentions))
A:transformers.modeling_tf_ctrl.self.transformer->TFCTRLMainLayer(config, name='transformer')
A:transformers.modeling_tf_ctrl.self.bias->self.add_weight(shape=(self.vocab_size,), initializer='zeros', trainable=True, name='bias')
A:transformers.modeling_tf_ctrl.self.lm_head->TFCTRLLMHead(config, self.transformer.w, name='lm_head')
A:transformers.modeling_tf_ctrl.inputs->tensorflow.expand_dims(inputs[:, -1], -1)
A:transformers.modeling_tf_ctrl.labels->tensorflow.expand_dims(inputs[:, -1], -1).pop('labels', labels)
A:transformers.modeling_tf_ctrl.transformer_outputs->self.transformer(inputs, past=past, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict, training=training)
A:transformers.modeling_tf_ctrl.logits->self.lm_head(hidden_states)
A:transformers.modeling_tf_ctrl.loss->self.compute_loss(labels, logits)
transformers.TFCTRLLMHeadModel(self,config,*inputs,**kwargs)
transformers.TFCTRLLMHeadModel.call(self,inputs,past=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.TFCTRLLMHeadModel.get_output_embeddings(self)
transformers.TFCTRLLMHeadModel.prepare_inputs_for_generation(self,inputs,past,**kwargs)
transformers.TFCTRLModel(self,config,*inputs,**kwargs)
transformers.TFCTRLModel.call(self,inputs,**kwargs)
transformers.TFCTRLPreTrainedModel(TFPreTrainedModel)
transformers.modeling_tf_ctrl.TFCTRLLMHead(self,config,input_embeddings,**kwargs)
transformers.modeling_tf_ctrl.TFCTRLLMHead.__init__(self,config,input_embeddings,**kwargs)
transformers.modeling_tf_ctrl.TFCTRLLMHead.build(self,input_shape)
transformers.modeling_tf_ctrl.TFCTRLLMHead.call(self,hidden_states)
transformers.modeling_tf_ctrl.TFCTRLLMHeadModel(self,config,*inputs,**kwargs)
transformers.modeling_tf_ctrl.TFCTRLLMHeadModel.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_ctrl.TFCTRLLMHeadModel.call(self,inputs,past=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,labels=None,training=False)
transformers.modeling_tf_ctrl.TFCTRLLMHeadModel.get_output_embeddings(self)
transformers.modeling_tf_ctrl.TFCTRLLMHeadModel.prepare_inputs_for_generation(self,inputs,past,**kwargs)
transformers.modeling_tf_ctrl.TFCTRLMainLayer(self,config,**kwargs)
transformers.modeling_tf_ctrl.TFCTRLMainLayer.__init__(self,config,**kwargs)
transformers.modeling_tf_ctrl.TFCTRLMainLayer._prune_heads(self,heads_to_prune)
transformers.modeling_tf_ctrl.TFCTRLMainLayer._resize_token_embeddings(self,new_num_tokens)
transformers.modeling_tf_ctrl.TFCTRLMainLayer.call(self,inputs,past=None,attention_mask=None,token_type_ids=None,position_ids=None,head_mask=None,inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None,training=False)
transformers.modeling_tf_ctrl.TFCTRLMainLayer.get_input_embeddings(self)
transformers.modeling_tf_ctrl.TFCTRLMainLayer.set_input_embeddings(self,value)
transformers.modeling_tf_ctrl.TFCTRLModel(self,config,*inputs,**kwargs)
transformers.modeling_tf_ctrl.TFCTRLModel.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_ctrl.TFCTRLModel.call(self,inputs,**kwargs)
transformers.modeling_tf_ctrl.TFCTRLPreTrainedModel(TFPreTrainedModel)
transformers.modeling_tf_ctrl.TFEncoderLayer(self,d_model_size,num_heads,dff,rate=0.1,layer_norm_epsilon=1e-06,output_attentions=False,**kwargs)
transformers.modeling_tf_ctrl.TFEncoderLayer.__init__(self,d_model_size,num_heads,dff,rate=0.1,layer_norm_epsilon=1e-06,output_attentions=False,**kwargs)
transformers.modeling_tf_ctrl.TFEncoderLayer.call(self,x,mask,layer_past,attention_mask,head_mask,use_cache,output_attentions,training=False)
transformers.modeling_tf_ctrl.TFMultiHeadAttention(self,d_model_size,num_heads,output_attentions=False,**kwargs)
transformers.modeling_tf_ctrl.TFMultiHeadAttention.__init__(self,d_model_size,num_heads,output_attentions=False,**kwargs)
transformers.modeling_tf_ctrl.TFMultiHeadAttention.call(self,v,k,q,mask,layer_past,attention_mask,head_mask,use_cache,output_attentions,training=False)
transformers.modeling_tf_ctrl.TFMultiHeadAttention.split_into_heads(self,x,batch_size)
transformers.modeling_tf_ctrl.TFPointWiseFeedForwardLayer(self,d_model_size,dff,**kwargs)
transformers.modeling_tf_ctrl.TFPointWiseFeedForwardLayer.__init__(self,d_model_size,dff,**kwargs)
transformers.modeling_tf_ctrl.TFPointWiseFeedForwardLayer.call(self,inputs,trainable=False)
transformers.modeling_tf_ctrl.angle_defn(pos,i,d_model_size)
transformers.modeling_tf_ctrl.positional_encoding(position,d_model_size)
transformers.modeling_tf_ctrl.scaled_dot_product_attention(q,k,v,mask,attention_mask=None,head_mask=None)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/tokenization_auto.py----------------------------------------
A:transformers.tokenization_auto.logger->utils.logging.get_logger(__name__)
A:transformers.tokenization_auto.TOKENIZER_MAPPING->OrderedDict([(RetriBertConfig, (RetriBertTokenizer, RetriBertTokenizerFast)), (T5Config, (T5Tokenizer, None)), (MobileBertConfig, (MobileBertTokenizer, MobileBertTokenizerFast)), (DistilBertConfig, (DistilBertTokenizer, DistilBertTokenizerFast)), (AlbertConfig, (AlbertTokenizer, None)), (CamembertConfig, (CamembertTokenizer, None)), (PegasusConfig, (PegasusTokenizer, None)), (MBartConfig, (MBartTokenizer, None)), (XLMRobertaConfig, (XLMRobertaTokenizer, None)), (MarianConfig, (MarianTokenizer, None)), (BartConfig, (BartTokenizer, BartTokenizerFast)), (LongformerConfig, (LongformerTokenizer, LongformerTokenizerFast)), (RobertaConfig, (RobertaTokenizer, RobertaTokenizerFast)), (ReformerConfig, (ReformerTokenizer, None)), (ElectraConfig, (ElectraTokenizer, ElectraTokenizerFast)), (BertConfig, (BertTokenizer, BertTokenizerFast)), (OpenAIGPTConfig, (OpenAIGPTTokenizer, OpenAIGPTTokenizerFast)), (GPT2Config, (GPT2Tokenizer, GPT2TokenizerFast)), (TransfoXLConfig, (TransfoXLTokenizer, TransfoXLTokenizerFast)), (XLNetConfig, (XLNetTokenizer, None)), (FlaubertConfig, (FlaubertTokenizer, None)), (XLMConfig, (XLMTokenizer, None)), (CTRLConfig, (CTRLTokenizer, None))])
A:transformers.tokenization_auto.config->configuration_auto.AutoConfig.from_pretrained(pretrained_model_name_or_path, **kwargs)
A:transformers.tokenization_auto.use_fast->kwargs.pop('use_fast', False)
transformers.AutoTokenizer(self)
transformers.AutoTokenizer.from_pretrained(cls,pretrained_model_name_or_path,*inputs,**kwargs)
transformers.tokenization_auto.AutoTokenizer(self)
transformers.tokenization_auto.AutoTokenizer.__init__(self)
transformers.tokenization_auto.AutoTokenizer.from_pretrained(cls,pretrained_model_name_or_path,*inputs,**kwargs)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/tokenization_pegasus.py----------------------------------------
A:transformers.tokenization_pegasus.sp_id->self.sp_model.piece_to_id(token)
A:transformers.tokenization_pegasus.token->self.sp_model.IdToPiece(index - self.offset)
A:transformers.tokenization_pegasus.all_special_ids->set(self.all_special_ids)
A:transformers.tokenization_pegasus.tokenizer_kwargs->dict(add_special_tokens=True, return_tensors=return_tensors, max_length=max_length, truncation=truncation, padding=padding)
transformers.PegasusTokenizer(self,*args,**kwargs)
transformers.PegasusTokenizer._convert_id_to_token(self,index:int)->str
transformers.PegasusTokenizer._convert_token_to_id(self,token:str)->int
transformers.PegasusTokenizer._special_token_mask(self,seq)
transformers.PegasusTokenizer.build_inputs_with_special_tokens(self,token_ids_0,token_ids_1=None)->List[int]
transformers.PegasusTokenizer.get_special_tokens_mask(self,token_ids_0:List,token_ids_1:Optional[List]=None,already_has_special_tokens:bool=False)->List[int]
transformers.PegasusTokenizer.get_vocab(self)->Dict[str, int]
transformers.PegasusTokenizer.num_special_tokens_to_add(self,pair=False)
transformers.PegasusTokenizer.prepare_seq2seq_batch(self,src_texts:List[str],tgt_texts:Optional[List[str]]=None,max_length:Optional[int]=None,max_target_length:Optional[int]=None,return_tensors:str='pt',truncation=True,padding='longest',**unused)->BatchEncoding
transformers.PegasusTokenizer.vocab_size(self)->int
transformers.tokenization_pegasus.PegasusTokenizer(self,*args,**kwargs)
transformers.tokenization_pegasus.PegasusTokenizer.__init__(self,*args,**kwargs)
transformers.tokenization_pegasus.PegasusTokenizer._convert_id_to_token(self,index:int)->str
transformers.tokenization_pegasus.PegasusTokenizer._convert_token_to_id(self,token:str)->int
transformers.tokenization_pegasus.PegasusTokenizer._special_token_mask(self,seq)
transformers.tokenization_pegasus.PegasusTokenizer.build_inputs_with_special_tokens(self,token_ids_0,token_ids_1=None)->List[int]
transformers.tokenization_pegasus.PegasusTokenizer.get_special_tokens_mask(self,token_ids_0:List,token_ids_1:Optional[List]=None,already_has_special_tokens:bool=False)->List[int]
transformers.tokenization_pegasus.PegasusTokenizer.get_vocab(self)->Dict[str, int]
transformers.tokenization_pegasus.PegasusTokenizer.num_special_tokens_to_add(self,pair=False)
transformers.tokenization_pegasus.PegasusTokenizer.prepare_seq2seq_batch(self,src_texts:List[str],tgt_texts:Optional[List[str]]=None,max_length:Optional[int]=None,max_target_length:Optional[int]=None,return_tensors:str='pt',truncation=True,padding='longest',**unused)->BatchEncoding
transformers.tokenization_pegasus.PegasusTokenizer.vocab_size(self)->int


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/modeling_xlnet.py----------------------------------------
A:transformers.modeling_xlnet.logger->utils.logging.get_logger(__name__)
A:transformers.modeling_xlnet.init_vars->tensorflow.train.list_variables(tf_path)
A:transformers.modeling_xlnet.array->numpy.transpose(array)
A:transformers.modeling_xlnet.tf_to_pt_map->build_tf_xlnet_to_pytorch_map(model, config, tf_weights)
A:transformers.modeling_xlnet.p_i.data->torch.from_numpy(arr_i)
A:transformers.modeling_xlnet.pointer.data->torch.from_numpy(array)
A:transformers.modeling_xlnet.self.q->torch.nn.Parameter(torch.FloatTensor(config.d_model, self.n_head, self.d_head))
A:transformers.modeling_xlnet.self.k->torch.nn.Parameter(torch.FloatTensor(config.d_model, self.n_head, self.d_head))
A:transformers.modeling_xlnet.self.v->torch.nn.Parameter(torch.FloatTensor(config.d_model, self.n_head, self.d_head))
A:transformers.modeling_xlnet.self.o->torch.nn.Parameter(torch.FloatTensor(config.d_model, self.n_head, self.d_head))
A:transformers.modeling_xlnet.self.r->torch.nn.Parameter(torch.FloatTensor(config.d_model, self.n_head, self.d_head))
A:transformers.modeling_xlnet.self.r_r_bias->torch.nn.Parameter(torch.FloatTensor(self.n_head, self.d_head))
A:transformers.modeling_xlnet.self.r_s_bias->torch.nn.Parameter(torch.FloatTensor(self.n_head, self.d_head))
A:transformers.modeling_xlnet.self.r_w_bias->torch.nn.Parameter(torch.FloatTensor(self.n_head, self.d_head))
A:transformers.modeling_xlnet.self.seg_embed->torch.nn.Parameter(torch.FloatTensor(2, self.n_head, self.d_head))
A:transformers.modeling_xlnet.self.layer_norm->XLNetLayerNorm(config.d_model, eps=config.layer_norm_eps)
A:transformers.modeling_xlnet.self.dropout->torch.nn.Dropout(config.dropout)
A:transformers.modeling_xlnet.x->torch.index_select(x, 3, torch.arange(klen, device=x.device, dtype=torch.long))
A:transformers.modeling_xlnet.ac->torch.einsum('ibnd,jbnd->bnij', q_head + self.r_w_bias, k_head_h)
A:transformers.modeling_xlnet.bd->self.rel_shift_bnij(bd, klen=ac.shape[3])
A:transformers.modeling_xlnet.ef->torch.einsum('ijbs,ibns->bnij', seg_mat, ef)
A:transformers.modeling_xlnet.attn_prob->self.dropout(attn_prob)
A:transformers.modeling_xlnet.attn_vec->self.rel_attn_core(q_head_h, k_head_h, v_head_h, k_head_r, seg_mat=seg_mat, attn_mask=attn_mask_h, head_mask=head_mask, output_attentions=output_attentions)
A:transformers.modeling_xlnet.attn_out->self.dropout(attn_out)
A:transformers.modeling_xlnet.output->self.sequence_summary(output)
A:transformers.modeling_xlnet.cat->torch.cat([mems, h], dim=0)
A:transformers.modeling_xlnet.k_head_h->torch.einsum('ibh,hnd->ibnd', cat, self.k)
A:transformers.modeling_xlnet.v_head_h->torch.einsum('ibh,hnd->ibnd', cat, self.v)
A:transformers.modeling_xlnet.k_head_r->torch.einsum('ibh,hnd->ibnd', r.type(self.r.dtype), self.r)
A:transformers.modeling_xlnet.q_head_h->torch.einsum('ibh,hnd->ibnd', h, self.q)
A:transformers.modeling_xlnet.attn_vec_h->self.rel_attn_core(q_head_h, k_head_h, v_head_h, k_head_r, seg_mat=seg_mat, attn_mask=attn_mask_h, head_mask=head_mask, output_attentions=output_attentions)
A:transformers.modeling_xlnet.output_h->self.dropout(word_emb_k)
A:transformers.modeling_xlnet.q_head_g->torch.einsum('mbnd,mlb->lbnd', q_head_g, target_mapping)
A:transformers.modeling_xlnet.attn_vec_g->self.rel_attn_core(q_head_g, k_head_h, v_head_h, k_head_r, seg_mat=seg_mat, attn_mask=attn_mask_g, head_mask=head_mask, output_attentions=output_attentions)
A:transformers.modeling_xlnet.output_g->self.dropout(word_emb_q)
A:transformers.modeling_xlnet.self.layer_1->torch.nn.Linear(config.d_model, config.d_inner)
A:transformers.modeling_xlnet.self.layer_2->torch.nn.Linear(config.d_inner, config.d_model)
A:transformers.modeling_xlnet.self.rel_attn->XLNetRelativeAttention(config)
A:transformers.modeling_xlnet.self.ff->XLNetFeedForward(config)
A:transformers.modeling_xlnet.outputs->self.transformer(input_ids, attention_mask=attention_mask, mems=mems, perm_mask=perm_mask, target_mapping=target_mapping, token_type_ids=token_type_ids, input_mask=input_mask, head_mask=head_mask, inputs_embeds=inputs_embeds, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.modeling_xlnet.output_x->self.ff(output_x)
A:transformers.modeling_xlnet.self.word_embedding->torch.nn.Embedding(config.vocab_size, config.d_model)
A:transformers.modeling_xlnet.self.mask_emb->torch.nn.Parameter(torch.FloatTensor(1, 1, config.d_model))
A:transformers.modeling_xlnet.self.layer->torch.nn.ModuleList([XLNetLayer(config) for _ in range(config.n_layer)])
A:transformers.modeling_xlnet.attn_mask->(attn_mask > 0).to(dtype_float)
A:transformers.modeling_xlnet.mask_up->torch.triu(attn_mask, diagonal=1)
A:transformers.modeling_xlnet.attn_mask_pad->torch.zeros([qlen, mlen])
A:transformers.modeling_xlnet.ret->ret.to(self.device).to(self.device)
A:transformers.modeling_xlnet.mask_lo->torch.tril(attn_mask, diagonal=-1)
A:transformers.modeling_xlnet.sinusoid_inp->torch.einsum('i,d->id', pos_seq, inv_freq)
A:transformers.modeling_xlnet.pos_emb->self.dropout(pos_emb)
A:transformers.modeling_xlnet.freq_seq->torch.arange(0, self.d_model, 2.0, dtype=torch.float)
A:transformers.modeling_xlnet.fwd_pos_seq->fwd_pos_seq.clamp(-self.clamp_len, self.clamp_len).clamp(-self.clamp_len, self.clamp_len)
A:transformers.modeling_xlnet.bwd_pos_seq->bwd_pos_seq.clamp(-self.clamp_len, self.clamp_len).clamp(-self.clamp_len, self.clamp_len)
A:transformers.modeling_xlnet.fwd_pos_emb->self.positional_embedding(fwd_pos_seq, inv_freq)
A:transformers.modeling_xlnet.bwd_pos_emb->self.positional_embedding(bwd_pos_seq, inv_freq)
A:transformers.modeling_xlnet.input_ids->torch.cat([input_ids, dummy_token], dim=1)
A:transformers.modeling_xlnet.inputs_embeds->inputs_embeds.transpose(0, 1).contiguous().transpose(0, 1).contiguous()
A:transformers.modeling_xlnet.mems_mask->torch.zeros([data_mask.shape[0], mlen, bsz]).to(data_mask)
A:transformers.modeling_xlnet.data_mask->torch.cat([mems_mask, data_mask], dim=1)
A:transformers.modeling_xlnet.non_tgt_mask->(attn_mask + non_tgt_mask[:, :, None, None] > 0).to(attn_mask)
A:transformers.modeling_xlnet.word_emb_k->self.word_embedding(input_ids)
A:transformers.modeling_xlnet.word_emb_q->self.mask_emb.expand(target_mapping.shape[0], bsz, -1)
A:transformers.modeling_xlnet.mem_pad->torch.zeros([mlen, bsz], dtype=torch.long, device=device)
A:transformers.modeling_xlnet.cat_ids->torch.cat([mem_pad, token_type_ids], dim=0)
A:transformers.modeling_xlnet.seg_mat->torch.nn.functional.one_hot(seg_mat, num_classes=2).to(dtype_float)
A:transformers.modeling_xlnet.head_mask->head_mask.to(dtype=next(self.parameters()).dtype).to(dtype=next(self.parameters()).dtype)
A:transformers.modeling_xlnet.hidden_states->tuple((hs.permute(1, 0, 2).contiguous() for hs in hidden_states))
A:transformers.modeling_xlnet.attentions->tuple((t.permute(2, 3, 0, 1).contiguous() for t in attentions))
A:transformers.modeling_xlnet.self.transformer->XLNetModel(config)
A:transformers.modeling_xlnet.self.lm_loss->torch.nn.Linear(config.d_model, config.vocab_size, bias=True)
A:transformers.modeling_xlnet.dummy_token->torch.zeros((effective_batch_size, 1), dtype=torch.long, device=input_ids.device)
A:transformers.modeling_xlnet.perm_mask->torch.zeros((effective_batch_size, sequence_length, sequence_length), dtype=torch.float, device=input_ids.device)
A:transformers.modeling_xlnet.target_mapping->torch.zeros((effective_batch_size, 1, sequence_length), dtype=torch.float, device=input_ids.device)
A:transformers.modeling_xlnet.inputs['mems']->tuple((layer_past[:-offset, :, :] for layer_past in past))
A:transformers.modeling_xlnet.transformer_outputs->self.transformer(input_ids, attention_mask=attention_mask, mems=mems, perm_mask=perm_mask, target_mapping=target_mapping, token_type_ids=token_type_ids, input_mask=input_mask, head_mask=head_mask, inputs_embeds=inputs_embeds, use_cache=use_cache, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)
A:transformers.modeling_xlnet.logits->self.qa_outputs(sequence_output)
A:transformers.modeling_xlnet.loss_fct->CrossEntropyLoss()
A:transformers.modeling_xlnet.loss->loss_fct(reshaped_logits, labels.view(-1))
A:transformers.modeling_xlnet.self.sequence_summary->SequenceSummary(config)
A:transformers.modeling_xlnet.self.logits_proj->torch.nn.Linear(config.d_model, 1)
A:transformers.modeling_xlnet.self.classifier->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.modeling_xlnet.active_logits->self.qa_outputs(sequence_output).view(-1, self.num_labels)
A:transformers.modeling_xlnet.active_labels->torch.where(active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels))
A:transformers.modeling_xlnet.reshaped_logits->self.qa_outputs(sequence_output).view(-1, num_choices)
A:transformers.modeling_xlnet.self.qa_outputs->torch.nn.Linear(config.hidden_size, config.num_labels)
A:transformers.modeling_xlnet.(start_logits, end_logits)->self.qa_outputs(sequence_output).split(1, dim=-1)
A:transformers.modeling_xlnet.start_logits->self.start_logits(hidden_states, p_mask=p_mask)
A:transformers.modeling_xlnet.end_logits->self.end_logits(hidden_states_expanded, start_states=start_states, p_mask=p_mask)
A:transformers.modeling_xlnet.start_positions->start_positions.squeeze(-1).squeeze(-1)
A:transformers.modeling_xlnet.end_positions->end_positions.squeeze(-1).squeeze(-1)
A:transformers.modeling_xlnet.ignored_index->self.start_logits(hidden_states, p_mask=p_mask).size(1)
A:transformers.modeling_xlnet.start_loss->loss_fct(start_logits, start_positions)
A:transformers.modeling_xlnet.end_loss->loss_fct(end_logits, end_positions)
A:transformers.modeling_xlnet.self.start_logits->PoolerStartLogits(config)
A:transformers.modeling_xlnet.self.end_logits->PoolerEndLogits(config)
A:transformers.modeling_xlnet.self.answer_class->PoolerAnswerClass(config)
A:transformers.modeling_xlnet.cls_logits->self.answer_class(hidden_states, start_states=start_states, cls_index=cls_index)
A:transformers.modeling_xlnet.loss_fct_cls->torch.nn.BCEWithLogitsLoss()
A:transformers.modeling_xlnet.cls_loss->loss_fct_cls(cls_logits, is_impossible)
A:transformers.modeling_xlnet.(bsz, slen, hsz)->tuple((hs.permute(1, 0, 2).contiguous() for hs in hidden_states)).size()
A:transformers.modeling_xlnet.start_log_probs->torch.nn.functional.softmax(start_logits, dim=-1)
A:transformers.modeling_xlnet.(start_top_log_probs, start_top_index)->torch.topk(start_log_probs, self.start_n_top, dim=-1)
A:transformers.modeling_xlnet.start_top_index_exp->start_top_index.unsqueeze(-1).expand(-1, -1, hsz)
A:transformers.modeling_xlnet.start_states->torch.einsum('blh,bl->bh', hidden_states, start_log_probs)
A:transformers.modeling_xlnet.hidden_states_expanded->tuple((hs.permute(1, 0, 2).contiguous() for hs in hidden_states)).unsqueeze(2).expand_as(start_states)
A:transformers.modeling_xlnet.end_log_probs->torch.nn.functional.softmax(end_logits, dim=1)
A:transformers.modeling_xlnet.(end_top_log_probs, end_top_index)->torch.topk(end_log_probs, self.end_n_top, dim=1)
A:transformers.modeling_xlnet.end_top_log_probs->end_top_log_probs.view(-1, self.start_n_top * self.end_n_top).view(-1, self.start_n_top * self.end_n_top)
A:transformers.modeling_xlnet.end_top_index->end_top_index.view(-1, self.start_n_top * self.end_n_top).view(-1, self.start_n_top * self.end_n_top)
transformers.XLNetForMultipleChoice(self,config)
transformers.XLNetForMultipleChoice.forward(self,input_ids=None,token_type_ids=None,input_mask=None,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,head_mask=None,inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.XLNetForMultipleChoiceOutput(ModelOutput)
transformers.XLNetForQuestionAnswering(self,config)
transformers.XLNetForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,token_type_ids=None,input_mask=None,head_mask=None,inputs_embeds=None,start_positions=None,end_positions=None,is_impossible=None,cls_index=None,p_mask=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.XLNetForQuestionAnsweringOutput(ModelOutput)
transformers.XLNetForQuestionAnsweringSimple(self,config)
transformers.XLNetForQuestionAnsweringSimple.forward(self,input_ids=None,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,token_type_ids=None,input_mask=None,head_mask=None,inputs_embeds=None,start_positions=None,end_positions=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.XLNetForQuestionAnsweringSimpleOutput(ModelOutput)
transformers.XLNetForSequenceClassification(self,config)
transformers.XLNetForSequenceClassification.forward(self,input_ids=None,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,token_type_ids=None,input_mask=None,head_mask=None,inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.XLNetForSequenceClassificationOutput(ModelOutput)
transformers.XLNetForTokenClassification(self,config)
transformers.XLNetForTokenClassification.forward(self,input_ids=None,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,token_type_ids=None,input_mask=None,head_mask=None,inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.XLNetForTokenClassificationOutput(ModelOutput)
transformers.XLNetLMHeadModel(self,config)
transformers.XLNetLMHeadModel.forward(self,input_ids=None,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,token_type_ids=None,input_mask=None,head_mask=None,inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.XLNetLMHeadModel.get_output_embeddings(self)
transformers.XLNetLMHeadModel.prepare_inputs_for_generation(self,input_ids,past,**kwargs)
transformers.XLNetLMHeadModelOutput(ModelOutput)
transformers.XLNetModel(self,config)
transformers.XLNetModel._prune_heads(self,heads_to_prune)
transformers.XLNetModel.cache_mem(self,curr_out,prev_mem)
transformers.XLNetModel.create_mask(self,qlen,mlen)
transformers.XLNetModel.forward(self,input_ids=None,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,token_type_ids=None,input_mask=None,head_mask=None,inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.XLNetModel.get_input_embeddings(self)
transformers.XLNetModel.positional_embedding(pos_seq,inv_freq,bsz=None)
transformers.XLNetModel.relative_positional_encoding(self,qlen,klen,bsz=None)
transformers.XLNetModel.set_input_embeddings(self,new_embeddings)
transformers.XLNetModelOutput(ModelOutput)
transformers.XLNetPreTrainedModel(PreTrainedModel)
transformers.XLNetPreTrainedModel._init_weights(self,module)
transformers.load_tf_weights_in_xlnet(model,config,tf_path)
transformers.modeling_xlnet.XLNetFeedForward(self,config)
transformers.modeling_xlnet.XLNetFeedForward.__init__(self,config)
transformers.modeling_xlnet.XLNetFeedForward.forward(self,inp)
transformers.modeling_xlnet.XLNetForMultipleChoice(self,config)
transformers.modeling_xlnet.XLNetForMultipleChoice.__init__(self,config)
transformers.modeling_xlnet.XLNetForMultipleChoice.forward(self,input_ids=None,token_type_ids=None,input_mask=None,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,head_mask=None,inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.modeling_xlnet.XLNetForMultipleChoiceOutput(ModelOutput)
transformers.modeling_xlnet.XLNetForQuestionAnswering(self,config)
transformers.modeling_xlnet.XLNetForQuestionAnswering.__init__(self,config)
transformers.modeling_xlnet.XLNetForQuestionAnswering.forward(self,input_ids=None,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,token_type_ids=None,input_mask=None,head_mask=None,inputs_embeds=None,start_positions=None,end_positions=None,is_impossible=None,cls_index=None,p_mask=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.modeling_xlnet.XLNetForQuestionAnsweringOutput(ModelOutput)
transformers.modeling_xlnet.XLNetForQuestionAnsweringSimple(self,config)
transformers.modeling_xlnet.XLNetForQuestionAnsweringSimple.__init__(self,config)
transformers.modeling_xlnet.XLNetForQuestionAnsweringSimple.forward(self,input_ids=None,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,token_type_ids=None,input_mask=None,head_mask=None,inputs_embeds=None,start_positions=None,end_positions=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.modeling_xlnet.XLNetForQuestionAnsweringSimpleOutput(ModelOutput)
transformers.modeling_xlnet.XLNetForSequenceClassification(self,config)
transformers.modeling_xlnet.XLNetForSequenceClassification.__init__(self,config)
transformers.modeling_xlnet.XLNetForSequenceClassification.forward(self,input_ids=None,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,token_type_ids=None,input_mask=None,head_mask=None,inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.modeling_xlnet.XLNetForSequenceClassificationOutput(ModelOutput)
transformers.modeling_xlnet.XLNetForTokenClassification(self,config)
transformers.modeling_xlnet.XLNetForTokenClassification.__init__(self,config)
transformers.modeling_xlnet.XLNetForTokenClassification.forward(self,input_ids=None,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,token_type_ids=None,input_mask=None,head_mask=None,inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.modeling_xlnet.XLNetForTokenClassificationOutput(ModelOutput)
transformers.modeling_xlnet.XLNetLMHeadModel(self,config)
transformers.modeling_xlnet.XLNetLMHeadModel.__init__(self,config)
transformers.modeling_xlnet.XLNetLMHeadModel.forward(self,input_ids=None,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,token_type_ids=None,input_mask=None,head_mask=None,inputs_embeds=None,labels=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.modeling_xlnet.XLNetLMHeadModel.get_output_embeddings(self)
transformers.modeling_xlnet.XLNetLMHeadModel.prepare_inputs_for_generation(self,input_ids,past,**kwargs)
transformers.modeling_xlnet.XLNetLMHeadModelOutput(ModelOutput)
transformers.modeling_xlnet.XLNetLayer(self,config)
transformers.modeling_xlnet.XLNetLayer.__init__(self,config)
transformers.modeling_xlnet.XLNetLayer.ff_chunk(self,output_x)
transformers.modeling_xlnet.XLNetLayer.forward(self,output_h,output_g,attn_mask_h,attn_mask_g,r,seg_mat,mems=None,target_mapping=None,head_mask=None,output_attentions=False)
transformers.modeling_xlnet.XLNetModel(self,config)
transformers.modeling_xlnet.XLNetModel.__init__(self,config)
transformers.modeling_xlnet.XLNetModel._prune_heads(self,heads_to_prune)
transformers.modeling_xlnet.XLNetModel.cache_mem(self,curr_out,prev_mem)
transformers.modeling_xlnet.XLNetModel.create_mask(self,qlen,mlen)
transformers.modeling_xlnet.XLNetModel.forward(self,input_ids=None,attention_mask=None,mems=None,perm_mask=None,target_mapping=None,token_type_ids=None,input_mask=None,head_mask=None,inputs_embeds=None,use_cache=None,output_attentions=None,output_hidden_states=None,return_dict=None)
transformers.modeling_xlnet.XLNetModel.get_input_embeddings(self)
transformers.modeling_xlnet.XLNetModel.positional_embedding(pos_seq,inv_freq,bsz=None)
transformers.modeling_xlnet.XLNetModel.relative_positional_encoding(self,qlen,klen,bsz=None)
transformers.modeling_xlnet.XLNetModel.set_input_embeddings(self,new_embeddings)
transformers.modeling_xlnet.XLNetModelOutput(ModelOutput)
transformers.modeling_xlnet.XLNetPreTrainedModel(PreTrainedModel)
transformers.modeling_xlnet.XLNetPreTrainedModel._init_weights(self,module)
transformers.modeling_xlnet.XLNetRelativeAttention(self,config)
transformers.modeling_xlnet.XLNetRelativeAttention.__init__(self,config)
transformers.modeling_xlnet.XLNetRelativeAttention.forward(self,h,g,attn_mask_h,attn_mask_g,r,seg_mat,mems=None,target_mapping=None,head_mask=None,output_attentions=False)
transformers.modeling_xlnet.XLNetRelativeAttention.post_attention(self,h,attn_vec,residual=True)
transformers.modeling_xlnet.XLNetRelativeAttention.prune_heads(self,heads)
transformers.modeling_xlnet.XLNetRelativeAttention.rel_attn_core(self,q_head,k_head_h,v_head_h,k_head_r,seg_mat=None,attn_mask=None,head_mask=None,output_attentions=False)
transformers.modeling_xlnet.XLNetRelativeAttention.rel_shift(x,klen=-1)
transformers.modeling_xlnet.XLNetRelativeAttention.rel_shift_bnij(x,klen=-1)
transformers.modeling_xlnet.build_tf_xlnet_to_pytorch_map(model,config,tf_weights=None)
transformers.modeling_xlnet.load_tf_weights_in_xlnet(model,config,tf_path)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/modeling_tf_utils.py----------------------------------------
A:transformers.modeling_tf_utils.logger->utils.logging.get_logger(__name__)
A:transformers.modeling_tf_utils.config_class->getattr(cls, 'config_class', None)
A:transformers.modeling_tf_utils.transformers_config->kwargs.pop('transformers_config', None)
A:transformers.modeling_tf_utils.config->kwargs.pop('config', None)
A:transformers.modeling_tf_utils.cfg->super(cls, self).get_config()
A:transformers.modeling_tf_utils.cfg['transformers_config']->self._transformers_config.to_dict()
A:transformers.modeling_tf_utils.cls->tensorflow.keras.utils.register_keras_serializable()(cls)
A:transformers.modeling_tf_utils.loss_fn->tensorflow.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)
A:transformers.modeling_tf_utils.active_loss->tensorflow.not_equal(tf.reshape(labels, (-1,)), -100)
A:transformers.modeling_tf_utils.reduced_logits->tensorflow.boolean_mask(tf.reshape(logits, (-1, shape_list(logits)[2])), active_loss)
A:transformers.modeling_tf_utils.labels->tensorflow.boolean_mask(tf.reshape(labels, (-1,)), active_loss)
A:transformers.modeling_tf_utils.start_loss->loss_fn(labels['start_position'], logits[0])
A:transformers.modeling_tf_utils.end_loss->loss_fn(labels['end_position'], logits[1])
A:transformers.modeling_tf_utils.base_model->getattr(self, self.base_model_prefix, self)
A:transformers.modeling_tf_utils.model_embeds->self._resize_token_embeddings(new_num_tokens)
A:transformers.modeling_tf_utils.old_embeddings->getattr(self, self.base_model_prefix, self).get_input_embeddings()
A:transformers.modeling_tf_utils.new_embeddings->self.add_weight('weight', shape=[new_num_tokens, old_embedding_dim], initializer=get_initializer(init_range), dtype=tf.float32)
A:transformers.modeling_tf_utils.word_embeddings->self._get_word_embeddings(old_embeddings)
A:transformers.modeling_tf_utils.init_range->getattr(self.config, 'initializer_range', 0.02)
A:transformers.modeling_tf_utils.init_weights->self.add_weight('weight', shape=[new_num_tokens, old_embedding_dim], initializer=get_initializer(init_range), dtype=tf.float32).numpy()
A:transformers.modeling_tf_utils.num_tokens_to_copy->min(old_num_tokens, new_num_tokens)
A:transformers.modeling_tf_utils.output_model_file->os.path.join(save_directory, TF2_WEIGHTS_NAME)
A:transformers.modeling_tf_utils.cache_dir->kwargs.pop('cache_dir', None)
A:transformers.modeling_tf_utils.from_pt->kwargs.pop('from_pt', False)
A:transformers.modeling_tf_utils.force_download->kwargs.pop('force_download', False)
A:transformers.modeling_tf_utils.resume_download->kwargs.pop('resume_download', False)
A:transformers.modeling_tf_utils.proxies->kwargs.pop('proxies', None)
A:transformers.modeling_tf_utils.output_loading_info->kwargs.pop('output_loading_info', False)
A:transformers.modeling_tf_utils.local_files_only->kwargs.pop('local_files_only', False)
A:transformers.modeling_tf_utils.use_cdn->kwargs.pop('use_cdn', True)
A:transformers.modeling_tf_utils.(config, model_kwargs)->tensorflow.keras.utils.register_keras_serializable()(cls).config_class.from_pretrained(config_path, *model_args, cache_dir=cache_dir, return_unused_kwargs=True, force_download=force_download, resume_download=resume_download, proxies=proxies, local_files_only=local_files_only, **kwargs)
A:transformers.modeling_tf_utils.archive_file->hf_bucket_url(pretrained_model_name_or_path, filename=WEIGHTS_NAME if from_pt else TF2_WEIGHTS_NAME, use_cdn=use_cdn)
A:transformers.modeling_tf_utils.resolved_archive_file->cached_path(archive_file, cache_dir=cache_dir, force_download=force_download, proxies=proxies, resume_download=resume_download, local_files_only=local_files_only)
A:transformers.modeling_tf_utils.model->cls(config, *model_args, **model_kwargs)
A:transformers.modeling_tf_utils.hdf5_layer_names->set(hdf5_format.load_attributes_from_hdf5_group(f, 'layer_names'))
A:transformers.modeling_tf_utils.model_layer_names->set((layer.name for layer in model.layers))
A:transformers.modeling_tf_utils.missing_keys->list(model_layer_names - hdf5_layer_names)
A:transformers.modeling_tf_utils.unexpected_keys->list(hdf5_layer_names - model_layer_names)
A:transformers.modeling_tf_utils.self.weight->self.add_weight('weight', shape=[self.vocab_size, self.hidden_size], initializer=get_initializer(self.initializer_range))
A:transformers.modeling_tf_utils.self.bias->self.add_weight('bias', shape=[1, self.nf], initializer=tf.zeros_initializer())
A:transformers.modeling_tf_utils.x->tensorflow.reshape(inputs, [-1, self.hidden_size])
A:transformers.modeling_tf_utils.base_config->super().get_config()
A:transformers.modeling_tf_utils.logits->tensorflow.matmul(x, self.weight, transpose_b=True)
A:transformers.modeling_tf_utils.self.summary->tensorflow.keras.layers.Dense(num_classes, kernel_initializer=get_initializer(initializer_range), name='summary')
A:transformers.modeling_tf_utils.self.first_dropout->tensorflow.keras.layers.Dropout(config.summary_first_dropout)
A:transformers.modeling_tf_utils.self.last_dropout->tensorflow.keras.layers.Dropout(config.summary_last_dropout)
A:transformers.modeling_tf_utils.hidden_states->inputs.get('hidden_states')
A:transformers.modeling_tf_utils.cls_index->tensorflow.fill(hidden_shape[:-2], hidden_shape[-2] - 1)
A:transformers.modeling_tf_utils.output->self.last_dropout(output, training=training)
A:transformers.modeling_tf_utils.hidden_shape->shape_list(hidden_states)
A:transformers.modeling_tf_utils.cls_shape->shape_list(cls_index)
A:transformers.modeling_tf_utils.static->tensorflow.reshape(inputs, [-1, self.hidden_size]).shape.as_list()
A:transformers.modeling_tf_utils.dynamic->tensorflow.shape(x)
transformers.TFPreTrainedModel(self,config,*inputs,**kwargs)
transformers.TFPreTrainedModel._get_resized_embeddings(self,old_embeddings,new_num_tokens=None)->tf.Variable
transformers.TFPreTrainedModel._get_word_embeddings(self,embeddings)
transformers.TFPreTrainedModel._resize_token_embeddings(self,new_num_tokens)
transformers.TFPreTrainedModel.dummy_inputs(self)->Dict[str, tf.Tensor]
transformers.TFPreTrainedModel.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.TFPreTrainedModel.get_input_embeddings(self)->tf.keras.layers.Layer
transformers.TFPreTrainedModel.get_output_embeddings(self)->tf.keras.layers.Layer
transformers.TFPreTrainedModel.prune_heads(self,heads_to_prune)
transformers.TFPreTrainedModel.resize_token_embeddings(self,new_num_tokens=None)->tf.Variable
transformers.TFPreTrainedModel.save_pretrained(self,save_directory)
transformers.TFPreTrainedModel.set_input_embeddings(self,value)
transformers.TFSequenceSummary(self,config:PretrainedConfig,initializer_range:float=0.02,**kwargs)
transformers.TFSequenceSummary.call(self,inputs,cls_index=None,training=False)
transformers.TFSharedEmbeddings(self,vocab_size:int,hidden_size:int,initializer_range:Optional[float]=None,**kwargs)
transformers.TFSharedEmbeddings._embedding(self,input_ids)
transformers.TFSharedEmbeddings._linear(self,inputs)
transformers.TFSharedEmbeddings.build(self,input_shape)
transformers.TFSharedEmbeddings.call(self,inputs:tf.Tensor,mode:str='embedding')->tf.Tensor
transformers.TFSharedEmbeddings.get_config(self)
transformers.modeling_tf_utils.TFCausalLanguageModelingLoss
transformers.modeling_tf_utils.TFCausalLanguageModelingLoss.compute_loss(self,labels,logits)
transformers.modeling_tf_utils.TFConv1D(self,nf,nx,initializer_range=0.02,**kwargs)
transformers.modeling_tf_utils.TFConv1D.__init__(self,nf,nx,initializer_range=0.02,**kwargs)
transformers.modeling_tf_utils.TFConv1D.build(self,input_shape)
transformers.modeling_tf_utils.TFConv1D.call(self,x)
transformers.modeling_tf_utils.TFMaskedLanguageModelingLoss(TFCausalLanguageModelingLoss)
transformers.modeling_tf_utils.TFModelUtilsMixin
transformers.modeling_tf_utils.TFModelUtilsMixin.num_parameters(self,only_trainable:bool=False)->int
transformers.modeling_tf_utils.TFMultipleChoiceLoss(TFSequenceClassificationLoss)
transformers.modeling_tf_utils.TFPreTrainedModel(self,config,*inputs,**kwargs)
transformers.modeling_tf_utils.TFPreTrainedModel.__init__(self,config,*inputs,**kwargs)
transformers.modeling_tf_utils.TFPreTrainedModel._get_resized_embeddings(self,old_embeddings,new_num_tokens=None)->tf.Variable
transformers.modeling_tf_utils.TFPreTrainedModel._get_word_embeddings(self,embeddings)
transformers.modeling_tf_utils.TFPreTrainedModel._resize_token_embeddings(self,new_num_tokens)
transformers.modeling_tf_utils.TFPreTrainedModel.dummy_inputs(self)->Dict[str, tf.Tensor]
transformers.modeling_tf_utils.TFPreTrainedModel.from_pretrained(cls,pretrained_model_name_or_path,*model_args,**kwargs)
transformers.modeling_tf_utils.TFPreTrainedModel.get_input_embeddings(self)->tf.keras.layers.Layer
transformers.modeling_tf_utils.TFPreTrainedModel.get_output_embeddings(self)->tf.keras.layers.Layer
transformers.modeling_tf_utils.TFPreTrainedModel.prune_heads(self,heads_to_prune)
transformers.modeling_tf_utils.TFPreTrainedModel.resize_token_embeddings(self,new_num_tokens=None)->tf.Variable
transformers.modeling_tf_utils.TFPreTrainedModel.save_pretrained(self,save_directory)
transformers.modeling_tf_utils.TFPreTrainedModel.set_input_embeddings(self,value)
transformers.modeling_tf_utils.TFQuestionAnsweringLoss
transformers.modeling_tf_utils.TFQuestionAnsweringLoss.compute_loss(self,labels,logits)
transformers.modeling_tf_utils.TFSequenceClassificationLoss
transformers.modeling_tf_utils.TFSequenceClassificationLoss.compute_loss(self,labels,logits)
transformers.modeling_tf_utils.TFSequenceSummary(self,config:PretrainedConfig,initializer_range:float=0.02,**kwargs)
transformers.modeling_tf_utils.TFSequenceSummary.__init__(self,config:PretrainedConfig,initializer_range:float=0.02,**kwargs)
transformers.modeling_tf_utils.TFSequenceSummary.call(self,inputs,cls_index=None,training=False)
transformers.modeling_tf_utils.TFSharedEmbeddings(self,vocab_size:int,hidden_size:int,initializer_range:Optional[float]=None,**kwargs)
transformers.modeling_tf_utils.TFSharedEmbeddings.__init__(self,vocab_size:int,hidden_size:int,initializer_range:Optional[float]=None,**kwargs)
transformers.modeling_tf_utils.TFSharedEmbeddings._embedding(self,input_ids)
transformers.modeling_tf_utils.TFSharedEmbeddings._linear(self,inputs)
transformers.modeling_tf_utils.TFSharedEmbeddings.build(self,input_shape)
transformers.modeling_tf_utils.TFSharedEmbeddings.call(self,inputs:tf.Tensor,mode:str='embedding')->tf.Tensor
transformers.modeling_tf_utils.TFSharedEmbeddings.get_config(self)
transformers.modeling_tf_utils.TFTokenClassificationLoss
transformers.modeling_tf_utils.TFTokenClassificationLoss.compute_loss(self,labels,logits)
transformers.modeling_tf_utils.cast_bool_to_primitive(bool_variable:Union[tf.Tensor,bool],default_tensor_to_true=False)->bool
transformers.modeling_tf_utils.get_initializer(initializer_range:float=0.02)->tf.initializers.TruncatedNormal
transformers.modeling_tf_utils.keras_serializable(cls)
transformers.modeling_tf_utils.shape_list(x:tf.Tensor)->List[int]
transformers.shape_list(x:tf.Tensor)->List[int]


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/tokenization_mobilebert.py----------------------------------------
A:transformers.tokenization_mobilebert.logger->utils.logging.get_logger(__name__)
transformers.MobileBertTokenizer(BertTokenizer)
transformers.MobileBertTokenizerFast(BertTokenizerFast)
transformers.tokenization_mobilebert.MobileBertTokenizer(BertTokenizer)
transformers.tokenization_mobilebert.MobileBertTokenizerFast(BertTokenizerFast)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/utils/logging.py----------------------------------------
A:transformers.utils.logging._lock->threading.Lock()
A:transformers.utils.logging._default_handler->logging.StreamHandler()
A:transformers.utils.logging.library_root_logger->_get_library_root_logger()
A:transformers.utils.logging.name->_get_library_name()
transformers.logging._configure_library_root_logger()->None
transformers.logging._get_library_name()->str
transformers.logging._get_library_root_logger()->logging.Logger
transformers.logging._reset_library_root_logger()->None
transformers.logging.disable_default_handler()->None
transformers.logging.disable_propagation()->None
transformers.logging.enable_default_handler()->None
transformers.logging.enable_propagation()->None
transformers.logging.get_logger(name:Optional[str]=None)->logging.Logger
transformers.logging.get_verbosity()->int
transformers.logging.set_verbosity(verbosity:int)->None
transformers.logging.set_verbosity_debug()
transformers.logging.set_verbosity_error()
transformers.logging.set_verbosity_info()
transformers.logging.set_verbosity_warning()
transformers.utils.logging._configure_library_root_logger()->None
transformers.utils.logging._get_library_name()->str
transformers.utils.logging._get_library_root_logger()->logging.Logger
transformers.utils.logging._reset_library_root_logger()->None
transformers.utils.logging.disable_default_handler()->None
transformers.utils.logging.disable_propagation()->None
transformers.utils.logging.enable_default_handler()->None
transformers.utils.logging.enable_propagation()->None
transformers.utils.logging.get_logger(name:Optional[str]=None)->logging.Logger
transformers.utils.logging.get_verbosity()->int
transformers.utils.logging.set_verbosity(verbosity:int)->None
transformers.utils.logging.set_verbosity_debug()
transformers.utils.logging.set_verbosity_error()
transformers.utils.logging.set_verbosity_info()
transformers.utils.logging.set_verbosity_warning()


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/utils/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/commands/serving.py----------------------------------------
A:transformers.commands.serving.logger->utils.logging.get_logger('transformers-cli/serving')
A:transformers.commands.serving.nlp->pipeline(task=args.task, model=args.model if args.model else None, config=args.config, tokenizer=args.tokenizer, device=args.device)
A:transformers.commands.serving.serve_parser->parser.add_parser('serve', help='CLI tool to run inference requests through REST and GraphQL endpoints.')
A:transformers.commands.serving.self._app->FastAPI(routes=[APIRoute('/', self.model_info, response_model=ServeModelInfoResult, response_class=JSONResponse, methods=['GET']), APIRoute('/tokenize', self.tokenize, response_model=ServeTokenizeResult, response_class=JSONResponse, methods=['POST']), APIRoute('/detokenize', self.detokenize, response_model=ServeDeTokenizeResult, response_class=JSONResponse, methods=['POST']), APIRoute('/forward', self.forward, response_model=ServeForwardResult, response_class=JSONResponse, methods=['POST'])], timeout=600)
A:transformers.commands.serving.tokens_txt->self._pipeline.tokenizer.tokenize(text_input)
A:transformers.commands.serving.tokens_ids->self._pipeline.tokenizer.convert_tokens_to_ids(tokens_txt)
A:transformers.commands.serving.decoded_str->self._pipeline.tokenizer.decode(tokens_ids, skip_special_tokens, cleanup_tokenization_spaces)
A:transformers.commands.serving.output->self._pipeline(inputs)
transformers.commands.serving.ServeCommand(self,pipeline:Pipeline,host:str,port:int,workers:int)
transformers.commands.serving.ServeCommand.__init__(self,pipeline:Pipeline,host:str,port:int,workers:int)
transformers.commands.serving.ServeCommand.detokenize(self,tokens_ids:List[int]=Body(None,embed=True),skip_special_tokens:bool=Body(False,embed=True),cleanup_tokenization_spaces:bool=Body(True,embed=True))
transformers.commands.serving.ServeCommand.model_info(self)
transformers.commands.serving.ServeCommand.register_subcommand(parser:ArgumentParser)
transformers.commands.serving.ServeCommand.run(self)
transformers.commands.serving.ServeCommand.tokenize(self,text_input:str=Body(None,embed=True),return_ids:bool=Body(False,embed=True))
transformers.commands.serving.ServeDeTokenizeResult(BaseModel)
transformers.commands.serving.ServeForwardResult(BaseModel)
transformers.commands.serving.ServeModelInfoResult(BaseModel)
transformers.commands.serving.ServeTokenizeResult(BaseModel)
transformers.commands.serving.serve_command_factory(args:Namespace)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/commands/run.py----------------------------------------
A:transformers.commands.run.logger->utils.logging.get_logger(__name__)
A:transformers.commands.run.nlp->pipeline(task=args.task, model=args.model if args.model else None, config=args.config, tokenizer=args.tokenizer, device=args.device)
A:transformers.commands.run.reader->transformers.pipelines.PipelineDataFormat.from_str(format=format, output_path=args.output, input_path=args.input, column=args.column if args.column else nlp.default_input_names, overwrite=args.overwrite)
A:transformers.commands.run.run_parser->parser.add_parser('run', help='Run a pipeline through the CLI')
A:transformers.commands.run.binary_path->self._reader.save_binary(outputs)
transformers.commands.run.RunCommand(self,nlp:Pipeline,reader:PipelineDataFormat)
transformers.commands.run.RunCommand.__init__(self,nlp:Pipeline,reader:PipelineDataFormat)
transformers.commands.run.RunCommand.register_subcommand(parser:ArgumentParser)
transformers.commands.run.RunCommand.run(self)
transformers.commands.run.run_command_factory(args)
transformers.commands.run.try_infer_format_from_ext(path:str)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/commands/user.py----------------------------------------
A:transformers.commands.user.login_parser->parser.add_parser('login', help='Log in using the same credentials as on huggingface.co')
A:transformers.commands.user.whoami_parser->parser.add_parser('whoami', help='Find out which huggingface.co account you are logged in as.')
A:transformers.commands.user.logout_parser->parser.add_parser('logout', help='Log out')
A:transformers.commands.user.s3_parser->parser.add_parser('s3', help='{ls, rm} Commands to interact with the files you upload on S3.')
A:transformers.commands.user.s3_subparsers->parser.add_parser('s3', help='{ls, rm} Commands to interact with the files you upload on S3.').add_subparsers(help='s3 related commands')
A:transformers.commands.user.ls_parser->parser.add_parser('s3', help='{ls, rm} Commands to interact with the files you upload on S3.').add_subparsers(help='s3 related commands').add_parser('ls')
A:transformers.commands.user.rm_parser->parser.add_parser('s3', help='{ls, rm} Commands to interact with the files you upload on S3.').add_subparsers(help='s3 related commands').add_parser('rm')
A:transformers.commands.user.upload_parser->parser.add_parser('upload', help='Upload a model to S3.')
A:transformers.commands.user.self._api->HfApi()
A:transformers.commands.user.username->input('Username: ')
A:transformers.commands.user.password->getpass()
A:transformers.commands.user.token->transformers.hf_api.HfFolder.get_token()
A:transformers.commands.user.(user, orgs)->self._api.whoami(token)
A:transformers.commands.user.row_format->('{{:{}}} ' * len(headers)).format(*col_widths)
A:transformers.commands.user.objs->self._api.list_objs(token, organization=self.args.organization)
A:transformers.commands.user.local_path->os.path.abspath(self.args.path)
A:transformers.commands.user.rel_path->os.path.basename(local_path)
A:transformers.commands.user.files->self.walk_dir(rel_path)
A:transformers.commands.user.(user, _)->self._api.whoami(token)
A:transformers.commands.user.choice->input('Proceed? [Y/n] ').lower()
A:transformers.commands.user.access_url->self._api.presign_and_upload(token=token, filename=filename, filepath=filepath, organization=self.args.organization)
transformers.commands.user.ANSI
transformers.commands.user.ANSI.bold(cls,s)
transformers.commands.user.ANSI.red(cls,s)
transformers.commands.user.BaseUserCommand(self,args)
transformers.commands.user.BaseUserCommand.__init__(self,args)
transformers.commands.user.DeleteObjCommand(BaseUserCommand)
transformers.commands.user.DeleteObjCommand.run(self)
transformers.commands.user.ListObjsCommand(BaseUserCommand)
transformers.commands.user.ListObjsCommand.run(self)
transformers.commands.user.ListObjsCommand.tabulate(self,rows:List[List[Union[str,int]]],headers:List[str])->str
transformers.commands.user.LoginCommand(BaseUserCommand)
transformers.commands.user.LoginCommand.run(self)
transformers.commands.user.LogoutCommand(BaseUserCommand)
transformers.commands.user.LogoutCommand.run(self)
transformers.commands.user.UploadCommand(BaseUserCommand)
transformers.commands.user.UploadCommand.run(self)
transformers.commands.user.UploadCommand.walk_dir(self,rel_path)
transformers.commands.user.UserCommands(BaseTransformersCLICommand)
transformers.commands.user.UserCommands.register_subcommand(parser:ArgumentParser)
transformers.commands.user.WhoamiCommand(BaseUserCommand)
transformers.commands.user.WhoamiCommand.run(self)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/commands/env.py----------------------------------------
A:transformers.commands.env.download_parser->parser.add_parser('env')
A:transformers.commands.env.pt_cuda_available->torch.cuda.is_available()
A:transformers.commands.env.tf_cuda_available->bool(tf.config.list_physical_devices('GPU'))
transformers.commands.env.EnvironmentCommand(BaseTransformersCLICommand)
transformers.commands.env.EnvironmentCommand.format_dict(d)
transformers.commands.env.EnvironmentCommand.register_subcommand(parser:ArgumentParser)
transformers.commands.env.EnvironmentCommand.run(self)
transformers.commands.env.info_command_factory(_)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/commands/train.py----------------------------------------
A:transformers.commands.train.train_parser->parser.add_parser('train', help='CLI tool to train a model on a task.')
A:transformers.commands.train.self.logger->utils.logging.get_logger('transformers-cli/training')
A:transformers.commands.train.self.pipeline->transformers.TextClassificationPipeline.from_pretrained(args.model)
A:transformers.commands.train.self.train_dataset->transformers.SingleSentenceClassificationProcessor.create_from_csv(args.train_data, column_label=args.column_label, column_text=args.column_text, column_id=args.column_id, skip_first_row=args.skip_first_row)
A:transformers.commands.train.self.valid_dataset->transformers.SingleSentenceClassificationProcessor.create_from_csv(args.validation_data, column_label=args.column_label, column_text=args.column_text, column_id=args.column_id, skip_first_row=args.skip_first_row)
transformers.commands.train.TrainCommand(self,args:Namespace)
transformers.commands.train.TrainCommand.__init__(self,args:Namespace)
transformers.commands.train.TrainCommand.register_subcommand(parser:ArgumentParser)
transformers.commands.train.TrainCommand.run(self)
transformers.commands.train.TrainCommand.run_tf(self)
transformers.commands.train.TrainCommand.run_torch(self)
transformers.commands.train.train_command_factory(args:Namespace)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/commands/download.py----------------------------------------
A:transformers.commands.download.download_parser->parser.add_parser('download')
transformers.commands.download.DownloadCommand(self,model:str,cache:str,force:bool)
transformers.commands.download.DownloadCommand.__init__(self,model:str,cache:str,force:bool)
transformers.commands.download.DownloadCommand.register_subcommand(parser:ArgumentParser)
transformers.commands.download.DownloadCommand.run(self)
transformers.commands.download.download_command_factory(args)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/commands/__init__.py----------------------------------------
transformers.commands.__init__.BaseTransformersCLICommand(ABC)
transformers.commands.__init__.BaseTransformersCLICommand.register_subcommand(parser:ArgumentParser)
transformers.commands.__init__.BaseTransformersCLICommand.run(self)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/commands/convert.py----------------------------------------
A:transformers.commands.convert.train_parser->parser.add_parser('convert', help='CLI tool to run convert model from original author checkpoints to Transformers PyTorch checkpoints.')
A:transformers.commands.convert.self._logger->utils.logging.get_logger('transformers-cli/converting')
transformers.commands.convert.ConvertCommand(self,model_type:str,tf_checkpoint:str,pytorch_dump_output:str,config:str,finetuning_task_name:str,*args)
transformers.commands.convert.ConvertCommand.__init__(self,model_type:str,tf_checkpoint:str,pytorch_dump_output:str,config:str,finetuning_task_name:str,*args)
transformers.commands.convert.ConvertCommand.register_subcommand(parser:ArgumentParser)
transformers.commands.convert.ConvertCommand.run(self)
transformers.commands.convert.convert_command_factory(args:Namespace)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/commands/transformers_cli.py----------------------------------------
A:transformers.commands.transformers_cli.parser->ArgumentParser('Transformers CLI tool', usage='transformers-cli <command> [<args>]')
A:transformers.commands.transformers_cli.commands_parser->ArgumentParser('Transformers CLI tool', usage='transformers-cli <command> [<args>]').add_subparsers(help='transformers-cli command helpers')
A:transformers.commands.transformers_cli.args->ArgumentParser('Transformers CLI tool', usage='transformers-cli <command> [<args>]').parse_args()
A:transformers.commands.transformers_cli.service->ArgumentParser('Transformers CLI tool', usage='transformers-cli <command> [<args>]').parse_args().func(args)
transformers.commands.transformers_cli.main()


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/benchmark/benchmark_tf.py----------------------------------------
A:transformers.benchmark.benchmark_tf.logger->utils.logging.get_logger(__name__)
A:transformers.benchmark.benchmark_tf.rng->random.Random()
A:transformers.benchmark.benchmark_tf._inference->self._prepare_inference_func(model_name, batch_size, sequence_length)
A:transformers.benchmark.benchmark_tf._train->self._prepare_train_func(model_name, batch_size, sequence_length)
A:transformers.benchmark.benchmark_tf.transformers_module->__import__('transformers', fromlist=[model_class])
A:transformers.benchmark.benchmark_tf.model_cls->getattr(transformers_module, model_class)
A:transformers.benchmark.benchmark_tf.model->TF_MODEL_WITH_LM_HEAD_MAPPING[config.__class__](config)
A:transformers.benchmark.benchmark_tf.input_ids->random_input_ids(batch_size, sequence_length, vocab_size)
A:transformers.benchmark.benchmark_tf.gradients->tensorflow.gradients(loss, model.trainable_variables)
A:transformers.benchmark.benchmark_tf.runtimes->timeit.repeat(func, repeat=self.args.repeat, number=10)
A:transformers.benchmark.benchmark_tf.trace->start_memory_tracing('transformers')
A:transformers.benchmark.benchmark_tf.handle->py3nvml.py3nvml.nvmlDeviceGetHandleByIndex(self.args.device_idx)
A:transformers.benchmark.benchmark_tf.meminfo->py3nvml.py3nvml.nvmlDeviceGetMemoryInfo(handle)
A:transformers.benchmark.benchmark_tf.memory->Memory(max_bytes_in_use)
A:transformers.benchmark.benchmark_tf.memory_bytes->measure_peak_memory_cpu(func)
A:transformers.benchmark.benchmark_tf.summary->stop_memory_tracing(trace)
transformers.TensorFlowBenchmark(Benchmark)
transformers.TensorFlowBenchmark._inference_memory(self,model_name:str,batch_size:int,sequence_length:int)->[Memory, Optional[MemorySummary]]
transformers.TensorFlowBenchmark._inference_speed(self,model_name:str,batch_size:int,sequence_length:int)->float
transformers.TensorFlowBenchmark._measure_memory(self,func:Callable[[],None])->[Memory, MemorySummary]
transformers.TensorFlowBenchmark._measure_speed(self,func)->float
transformers.TensorFlowBenchmark._prepare_inference_func(self,model_name:str,batch_size:int,sequence_length:int)->Callable[[], None]
transformers.TensorFlowBenchmark._prepare_train_func(self,model_name:str,batch_size:int,sequence_length:int)->Callable[[], None]
transformers.TensorFlowBenchmark._train_memory(self,model_name:str,batch_size:int,sequence_length:int)->[Memory, Optional[MemorySummary]]
transformers.TensorFlowBenchmark._train_speed(self,model_name:str,batch_size:int,sequence_length:int)->float
transformers.TensorFlowBenchmark.framework_version(self)
transformers.benchmark.benchmark_tf.TensorFlowBenchmark(Benchmark)
transformers.benchmark.benchmark_tf.TensorFlowBenchmark._inference_memory(self,model_name:str,batch_size:int,sequence_length:int)->[Memory, Optional[MemorySummary]]
transformers.benchmark.benchmark_tf.TensorFlowBenchmark._inference_speed(self,model_name:str,batch_size:int,sequence_length:int)->float
transformers.benchmark.benchmark_tf.TensorFlowBenchmark._measure_memory(self,func:Callable[[],None])->[Memory, MemorySummary]
transformers.benchmark.benchmark_tf.TensorFlowBenchmark._measure_speed(self,func)->float
transformers.benchmark.benchmark_tf.TensorFlowBenchmark._prepare_inference_func(self,model_name:str,batch_size:int,sequence_length:int)->Callable[[], None]
transformers.benchmark.benchmark_tf.TensorFlowBenchmark._prepare_train_func(self,model_name:str,batch_size:int,sequence_length:int)->Callable[[], None]
transformers.benchmark.benchmark_tf.TensorFlowBenchmark._train_memory(self,model_name:str,batch_size:int,sequence_length:int)->[Memory, Optional[MemorySummary]]
transformers.benchmark.benchmark_tf.TensorFlowBenchmark._train_speed(self,model_name:str,batch_size:int,sequence_length:int)->float
transformers.benchmark.benchmark_tf.TensorFlowBenchmark.framework_version(self)
transformers.benchmark.benchmark_tf.random_input_ids(batch_size:int,sequence_length:int,vocab_size:int)->['tf.Tensor']
transformers.benchmark.benchmark_tf.run_with_tf_optimizations(do_eager_mode:bool,use_xla:bool)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/benchmark/benchmark.py----------------------------------------
A:transformers.benchmark.benchmark.logger->utils.logging.get_logger(__name__)
A:transformers.benchmark.benchmark._inference->self._prepare_inference_func(model_name, batch_size, sequence_length)
A:transformers.benchmark.benchmark._train->self._prepare_train_func(model_name, batch_size, sequence_length)
A:transformers.benchmark.benchmark.transformers_module->__import__('transformers', fromlist=[model_class])
A:transformers.benchmark.benchmark.model_cls->getattr(transformers_module, model_class)
A:transformers.benchmark.benchmark.model->MODEL_WITH_LM_HEAD_MAPPING[config.__class__](config)
A:transformers.benchmark.benchmark.input_ids->torch.randint(vocab_size, (batch_size, sequence_length), dtype=torch.long, device=self.args.device)
A:transformers.benchmark.benchmark.inference_model->torch.jit.trace(model, input_ids)
A:transformers.benchmark.benchmark.outputs->inference_model(input_ids)
A:transformers.benchmark.benchmark.runtimes->timeit.repeat(func, repeat=self.args.repeat, number=10)
A:transformers.benchmark.benchmark.trace->start_memory_tracing('transformers')
A:transformers.benchmark.benchmark.handle->py3nvml.py3nvml.nvmlDeviceGetHandleByIndex(self.args.device_idx)
A:transformers.benchmark.benchmark.meminfo->py3nvml.py3nvml.nvmlDeviceGetMemoryInfo(handle)
A:transformers.benchmark.benchmark.memory->Memory(max_bytes_in_use)
A:transformers.benchmark.benchmark.memory_bytes->measure_peak_memory_cpu(func)
A:transformers.benchmark.benchmark.summary->stop_memory_tracing(trace)
transformers.PyTorchBenchmark(Benchmark)
transformers.PyTorchBenchmark._inference_memory(self,model_name:str,batch_size:int,sequence_length:int)->[Memory, Optional[MemorySummary]]
transformers.PyTorchBenchmark._inference_speed(self,model_name:str,batch_size:int,sequence_length:int)->float
transformers.PyTorchBenchmark._measure_memory(self,func:Callable[[],None])->[Memory, MemorySummary]
transformers.PyTorchBenchmark._measure_speed(self,func)->float
transformers.PyTorchBenchmark._prepare_inference_func(self,model_name:str,batch_size:int,sequence_length:int)->Callable[[], None]
transformers.PyTorchBenchmark._prepare_train_func(self,model_name:str,batch_size:int,sequence_length:int)->Callable[[], None]
transformers.PyTorchBenchmark._train_memory(self,model_name:str,batch_size:int,sequence_length:int)->[Memory, Optional[MemorySummary]]
transformers.PyTorchBenchmark._train_speed(self,model_name:str,batch_size:int,sequence_length:int)->float
transformers.PyTorchBenchmark.framework_version(self)
transformers.benchmark.benchmark.PyTorchBenchmark(Benchmark)
transformers.benchmark.benchmark.PyTorchBenchmark._inference_memory(self,model_name:str,batch_size:int,sequence_length:int)->[Memory, Optional[MemorySummary]]
transformers.benchmark.benchmark.PyTorchBenchmark._inference_speed(self,model_name:str,batch_size:int,sequence_length:int)->float
transformers.benchmark.benchmark.PyTorchBenchmark._measure_memory(self,func:Callable[[],None])->[Memory, MemorySummary]
transformers.benchmark.benchmark.PyTorchBenchmark._measure_speed(self,func)->float
transformers.benchmark.benchmark.PyTorchBenchmark._prepare_inference_func(self,model_name:str,batch_size:int,sequence_length:int)->Callable[[], None]
transformers.benchmark.benchmark.PyTorchBenchmark._prepare_train_func(self,model_name:str,batch_size:int,sequence_length:int)->Callable[[], None]
transformers.benchmark.benchmark.PyTorchBenchmark._train_memory(self,model_name:str,batch_size:int,sequence_length:int)->[Memory, Optional[MemorySummary]]
transformers.benchmark.benchmark.PyTorchBenchmark._train_speed(self,model_name:str,batch_size:int,sequence_length:int)->float
transformers.benchmark.benchmark.PyTorchBenchmark.framework_version(self)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/benchmark/benchmark_args.py----------------------------------------
A:transformers.benchmark.benchmark_args.logger->utils.logging.get_logger(__name__)
A:transformers.benchmark.benchmark_args.device->torch.device('cuda' if torch.cuda.is_available() else 'cpu')
A:transformers.benchmark.benchmark_args.n_gpu->torch.cuda.device_count()
transformers.PyTorchBenchmarkArguments(BenchmarkArguments)
transformers.PyTorchBenchmarkArguments._setup_devices(self)->Tuple['torch.device', int]
transformers.PyTorchBenchmarkArguments.device(self)->'torch.device'
transformers.PyTorchBenchmarkArguments.device_idx(self)->int
transformers.PyTorchBenchmarkArguments.is_gpu(self)
transformers.PyTorchBenchmarkArguments.is_tpu(self)
transformers.PyTorchBenchmarkArguments.n_gpu(self)
transformers.benchmark.benchmark_args.PyTorchBenchmarkArguments(BenchmarkArguments)
transformers.benchmark.benchmark_args.PyTorchBenchmarkArguments._setup_devices(self)->Tuple['torch.device', int]
transformers.benchmark.benchmark_args.PyTorchBenchmarkArguments.device(self)->'torch.device'
transformers.benchmark.benchmark_args.PyTorchBenchmarkArguments.device_idx(self)->int
transformers.benchmark.benchmark_args.PyTorchBenchmarkArguments.is_gpu(self)
transformers.benchmark.benchmark_args.PyTorchBenchmarkArguments.is_tpu(self)
transformers.benchmark.benchmark_args.PyTorchBenchmarkArguments.n_gpu(self)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/benchmark/benchmark_utils.py----------------------------------------
A:transformers.benchmark.benchmark_utils.logger->utils.logging.get_logger(__name__)
A:transformers.benchmark.benchmark_utils.BenchmarkOutput->namedtuple('BenchmarkOutput', ['time_inference_result', 'memory_inference_result', 'time_train_result', 'memory_train_result', 'inference_summary', 'train_summary'])
A:transformers.benchmark.benchmark_utils.result->str(result)
A:transformers.benchmark.benchmark_utils.queue->Queue()
A:transformers.benchmark.benchmark_utils.p->Process(target=wrapper_func, args=[queue] + list(args))
A:transformers.benchmark.benchmark_utils.process->psutil.Process(os.getpid())
A:transformers.benchmark.benchmark_utils.self.mem_usage->max(self.mem_usage, get_cpu_memory(self.process_id))
A:transformers.benchmark.benchmark_utils.stop->self.connection.poll(self.interval)
A:transformers.benchmark.benchmark_utils.(child_connection, parent_connection)->Pipe()
A:transformers.benchmark.benchmark_utils.mem_process->MemoryMeasureProcess(os.getpid(), child_connection, interval)
A:transformers.benchmark.benchmark_utils.max_memory->parent_connection.recv()
A:transformers.benchmark.benchmark_utils.num_measurements->parent_connection.recv()
A:transformers.benchmark.benchmark_utils.parent->psutil.Process(os.getpid())
A:transformers.benchmark.benchmark_utils.line->linecache.getline(filename, lineno).rstrip()
A:transformers.benchmark.benchmark_utils.traced_state->Frame(filename, name, lineno, event, line)
A:transformers.benchmark.benchmark_utils.mem->psutil.Process(os.getpid()).memory_info()
A:transformers.benchmark.benchmark_utils.handle->py3nvml.py3nvml.nvmlDeviceGetHandleByIndex(self.args.device_idx)
A:transformers.benchmark.benchmark_utils.meminfo->py3nvml.py3nvml.nvmlDeviceGetMemoryInfo(handle)
A:transformers.benchmark.benchmark_utils.mem_state->UsedMemoryState(traced_state, cpu_mem, gpu_mem)
A:transformers.benchmark.benchmark_utils.cumulative_memory_dict->defaultdict(lambda : [0, 0, 0])
A:transformers.benchmark.benchmark_utils.cumulative_memory->list((MemoryState(frame=frame, cpu=Memory(cpu_mem_inc), gpu=Memory(gpu_mem_inc), cpu_gpu=Memory(cpu_gpu_mem_inc)) for (frame, (cpu_mem_inc, gpu_mem_inc, cpu_gpu_mem_inc)) in cumulative_memory))
A:transformers.benchmark.benchmark_utils.memory_curr_trace->sorted(memory_curr_trace, key=lambda x: x.cpu_gpu.bytes, reverse=True)
A:transformers.benchmark.benchmark_utils.total_memory->Memory(total_memory)
A:transformers.benchmark.benchmark_utils.inference_result_time->copy.deepcopy(result_dict)
A:transformers.benchmark.benchmark_utils.inference_result_memory->copy.deepcopy(result_dict)
A:transformers.benchmark.benchmark_utils.train_result_time->copy.deepcopy(result_dict)
A:transformers.benchmark.benchmark_utils.train_result_memory->copy.deepcopy(result_dict)
A:transformers.benchmark.benchmark_utils.inference_result_time[model_name]->copy.deepcopy(model_dict)
A:transformers.benchmark.benchmark_utils.inference_result_memory[model_name]->copy.deepcopy(model_dict)
A:transformers.benchmark.benchmark_utils.train_result_time[model_name]->copy.deepcopy(model_dict)
A:transformers.benchmark.benchmark_utils.train_result_memory[model_name]->copy.deepcopy(model_dict)
A:transformers.benchmark.benchmark_utils.(memory, inference_summary)->self.inference_memory(model_name, batch_size, sequence_length)
A:transformers.benchmark.benchmark_utils.time->self.train_speed(model_name, batch_size, sequence_length)
A:transformers.benchmark.benchmark_utils.(memory, train_summary)->self.train_memory(model_name, batch_size, sequence_length)
A:transformers.benchmark.benchmark_utils.writer->csv.DictWriter(csv_file, fieldnames=fieldnames + ['result'])
A:transformers.benchmark.benchmark_utils.info['python_version']->platform.python_version()
A:transformers.benchmark.benchmark_utils.info['system']->platform.system()
A:transformers.benchmark.benchmark_utils.info['cpu']->platform.processor()
A:transformers.benchmark.benchmark_utils.info['date']->datetime.datetime.date(datetime.now())
A:transformers.benchmark.benchmark_utils.info['time']->datetime.datetime.time(datetime.now())
A:transformers.benchmark.benchmark_utils.info['cpu_ram_mb']->bytes_to_mega_bytes(psutil.virtual_memory().total)
A:transformers.benchmark.benchmark_utils.info['gpu']->py3nvml.py3nvml.nvmlDeviceGetName(handle)
A:transformers.benchmark.benchmark_utils.info['gpu_ram_mb']->bytes_to_mega_bytes(nvml.nvmlDeviceGetMemoryInfo(handle).total)
A:transformers.benchmark.benchmark_utils.info['gpu_performance_state']->py3nvml.py3nvml.nvmlDeviceGetPerformanceState(handle)
transformers.benchmark.benchmark_utils.Benchmark(self,args:BenchmarkArguments=None,configs:PretrainedConfig=None)
transformers.benchmark.benchmark_utils.Benchmark.__init__(self,args:BenchmarkArguments=None,configs:PretrainedConfig=None)
transformers.benchmark.benchmark_utils.Benchmark._inference_memory(self,model_name:str,batch_size:int,sequence_length:int)->[Memory, Optional[MemorySummary]]
transformers.benchmark.benchmark_utils.Benchmark._inference_speed(self,model_name:str,batch_size:int,sequence_length:int)->float
transformers.benchmark.benchmark_utils.Benchmark._train_memory(self,model_name:str,batch_size:int,sequence_length:int)->[Memory, Optional[MemorySummary]]
transformers.benchmark.benchmark_utils.Benchmark._train_speed(self,model_name:str,batch_size:int,sequence_length:int)->float
transformers.benchmark.benchmark_utils.Benchmark.environment_info(self)
transformers.benchmark.benchmark_utils.Benchmark.framework_version(self)
transformers.benchmark.benchmark_utils.Benchmark.inference_memory(self,*args,**kwargs)->[Memory, Optional[MemorySummary]]
transformers.benchmark.benchmark_utils.Benchmark.inference_speed(self,*args,**kwargs)->float
transformers.benchmark.benchmark_utils.Benchmark.print_fn(self)
transformers.benchmark.benchmark_utils.Benchmark.print_memory_trace_statistics(self,summary:MemorySummary)
transformers.benchmark.benchmark_utils.Benchmark.print_results(self,result_dict,type_label)
transformers.benchmark.benchmark_utils.Benchmark.run(self)
transformers.benchmark.benchmark_utils.Benchmark.save_to_csv(self,result_dict,filename)
transformers.benchmark.benchmark_utils.Benchmark.train_memory(self,*args,**kwargs)->[Memory, Optional[MemorySummary]]
transformers.benchmark.benchmark_utils.Benchmark.train_speed(self,*args,**kwargs)->float
transformers.benchmark.benchmark_utils.Frame(NamedTuple)
transformers.benchmark.benchmark_utils.Memory(NamedTuple)
transformers.benchmark.benchmark_utils.Memory.__repr__(self)->str
transformers.benchmark.benchmark_utils.MemoryState(NamedTuple)
transformers.benchmark.benchmark_utils.MemorySummary(NamedTuple)
transformers.benchmark.benchmark_utils.UsedMemoryState(NamedTuple)
transformers.benchmark.benchmark_utils.bytes_to_mega_bytes(memory_amount:int)->int
transformers.benchmark.benchmark_utils.is_memory_tracing_enabled()
transformers.benchmark.benchmark_utils.measure_peak_memory_cpu(function:Callable[[],None],interval=0.5,device_idx=None)->int
transformers.benchmark.benchmark_utils.separate_process_wrapper_fn(func:Callable[[],None],do_multi_processing:bool)->Callable[[], None]
transformers.benchmark.benchmark_utils.start_memory_tracing(modules_to_trace:Optional[Union[str,Iterable[str]]]=None,modules_not_to_trace:Optional[Union[str,Iterable[str]]]=None,events_to_trace:str='line',gpus_to_trace:Optional[List[int]]=None)->MemoryTrace
transformers.benchmark.benchmark_utils.stop_memory_tracing(memory_trace:Optional[MemoryTrace]=None,ignore_released_memory:bool=True)->Optional[MemorySummary]


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/benchmark/benchmark_args_tf.py----------------------------------------
A:transformers.benchmark.benchmark_args_tf.logger->utils.logging.get_logger(__name__)
A:transformers.benchmark.benchmark_args_tf.tpu->tensorflow.distribute.cluster_resolver.TPUClusterResolver()
A:transformers.benchmark.benchmark_args_tf.strategy->tensorflow.distribute.OneDeviceStrategy(device=f'/cpu:{self.device_idx}')
transformers.TensorFlowBenchmarkArguments(BenchmarkArguments)
transformers.TensorFlowBenchmarkArguments._setup_strategy(self)->Tuple['tf.distribute.Strategy', 'tf.distribute.cluster_resolver.TPUClusterResolver']
transformers.TensorFlowBenchmarkArguments._setup_tpu(self)->Tuple['tf.distribute.cluster_resolver.TPUClusterResolver']
transformers.TensorFlowBenchmarkArguments.gpu_list(self)
transformers.TensorFlowBenchmarkArguments.is_gpu(self)->bool
transformers.TensorFlowBenchmarkArguments.is_tpu(self)->bool
transformers.TensorFlowBenchmarkArguments.n_gpu(self)->int
transformers.TensorFlowBenchmarkArguments.strategy(self)->'tf.distribute.Strategy'
transformers.benchmark.benchmark_args_tf.TensorFlowBenchmarkArguments(BenchmarkArguments)
transformers.benchmark.benchmark_args_tf.TensorFlowBenchmarkArguments._setup_strategy(self)->Tuple['tf.distribute.Strategy', 'tf.distribute.cluster_resolver.TPUClusterResolver']
transformers.benchmark.benchmark_args_tf.TensorFlowBenchmarkArguments._setup_tpu(self)->Tuple['tf.distribute.cluster_resolver.TPUClusterResolver']
transformers.benchmark.benchmark_args_tf.TensorFlowBenchmarkArguments.gpu_list(self)
transformers.benchmark.benchmark_args_tf.TensorFlowBenchmarkArguments.is_gpu(self)->bool
transformers.benchmark.benchmark_args_tf.TensorFlowBenchmarkArguments.is_tpu(self)->bool
transformers.benchmark.benchmark_args_tf.TensorFlowBenchmarkArguments.n_gpu(self)->int
transformers.benchmark.benchmark_args_tf.TensorFlowBenchmarkArguments.strategy(self)->'tf.distribute.Strategy'


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/benchmark/benchmark_args_utils.py----------------------------------------
A:transformers.benchmark.benchmark_args_utils.logger->utils.logging.get_logger(__name__)
transformers.benchmark.benchmark_args_utils.BenchmarkArguments
transformers.benchmark.benchmark_args_utils.BenchmarkArguments.do_multi_processing(self)
transformers.benchmark.benchmark_args_utils.BenchmarkArguments.model_names(self)
transformers.benchmark.benchmark_args_utils.BenchmarkArguments.to_json_string(self)
transformers.benchmark.benchmark_args_utils.list_field(default=None,metadata=None)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/benchmark/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/data/data_collator.py----------------------------------------
A:transformers.data.data_collator.InputDataClass->NewType('InputDataClass', Any)
A:transformers.data.data_collator.DataCollator->NewType('DataCollator', Callable[[List[InputDataClass]], Dict[str, torch.Tensor]])
A:transformers.data.data_collator.batch['labels']->torch.tensor([f['label_ids'] for f in features], dtype=dtype)
A:transformers.data.data_collator.batch[k]->torch.tensor([f[k] for f in features])
A:transformers.data.data_collator.batch->self._tensorize_batch(examples)
A:transformers.data.data_collator.(inputs, labels)->self.mask_tokens(batch)
A:transformers.data.data_collator.labels->inputs.clone()
A:transformers.data.data_collator.length_of_first->examples[0].size(0)
A:transformers.data.data_collator.are_tensors_same_length->all((x.size(0) == length_of_first for x in examples))
A:transformers.data.data_collator.probability_matrix->torch.full(labels.shape, self.mlm_probability)
A:transformers.data.data_collator.padding_mask->inputs.clone().eq(self.tokenizer.pad_token_id)
A:transformers.data.data_collator.masked_indices->torch.bernoulli(probability_matrix).bool()
A:transformers.data.data_collator.inputs[indices_replaced]->self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)
A:transformers.data.data_collator.random_words->torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)
A:transformers.data.data_collator.(inputs, perm_mask, target_mapping, labels)->self.mask_tokens(batch)
A:transformers.data.data_collator.target_mapping->torch.zeros((labels.size(0), labels.size(1), labels.size(1)), dtype=torch.float32)
A:transformers.data.data_collator.max_len->inputs.clone().size(1)
A:transformers.data.data_collator.span_length->torch.randint(1, self.max_span_length + 1, (1,)).item()
A:transformers.data.data_collator.context_length->int(span_length / self.plm_probability)
A:transformers.data.data_collator.target_mapping[i]->torch.eye(labels.size(1))
A:transformers.data.data_collator.special_tokens_mask->torch.tensor([self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()], dtype=torch.bool)
A:transformers.data.data_collator.perm_mask->torch.zeros((labels.size(0), labels.size(1), labels.size(1)), dtype=torch.float32)
A:transformers.data.data_collator.perm_index->torch.flatten(perm_index.transpose(0, 1))
A:transformers.data.data_collator.(input_id, segment_id, attention_mask, label)->self.create_examples_from_document(doc, i, examples)
A:transformers.data.data_collator.(input_ids, mlm_labels)->self.mask_tokens(self._tensorize_batch(input_ids))
A:transformers.data.data_collator.input_ids->self._tensorize_batch(input_ids)
A:transformers.data.data_collator.target_seq_length->random.randint(2, max_num_tokens)
A:transformers.data.data_collator.a_end->random.randint(1, len(current_chunk) - 1)
A:transformers.data.data_collator.random_document_index->random.randint(0, len(examples) - 1)
A:transformers.data.data_collator.random_start->random.randint(0, len(random_document) - 1)
A:transformers.data.data_collator.(tokens_a, tokens_b, _)->self.tokenizer.truncate_sequences(tokens_a, tokens_b, num_tokens_to_remove=len(tokens_a) + len(tokens_b) - max_num_tokens, truncation_strategy='longest_first')
A:transformers.data.data_collator.input_id->self.tokenizer.build_inputs_with_special_tokens(tokens_a, tokens_b)
A:transformers.data.data_collator.segment_id->self.tokenizer.create_token_type_ids_from_sequences(tokens_a, tokens_b)
transformers.DataCollatorForLanguageModeling(self,examples:List[Union[List[int],torch.Tensor,Dict[str,torch.Tensor]]])
transformers.DataCollatorForLanguageModeling._tensorize_batch(self,examples:List[Union[List[int],torch.Tensor,Dict[str,torch.Tensor]]])->torch.Tensor
transformers.DataCollatorForLanguageModeling.mask_tokens(self,inputs:torch.Tensor)->Tuple[torch.Tensor, torch.Tensor]
transformers.DataCollatorForNextSentencePrediction(self,examples:List[Union[List[List[int]],Dict[str,torch.Tensor]]])
transformers.DataCollatorForNextSentencePrediction._tensorize_batch(self,examples:List[torch.Tensor])->torch.Tensor
transformers.DataCollatorForNextSentencePrediction.create_examples_from_document(self,document:List[List[int]],doc_index:int,examples:List[List[List[int]]])
transformers.DataCollatorForNextSentencePrediction.mask_tokens(self,inputs:torch.Tensor)->Tuple[torch.Tensor, torch.Tensor]
transformers.DataCollatorForPermutationLanguageModeling(self,examples:List[Union[List[int],torch.Tensor,Dict[str,torch.Tensor]]])
transformers.DataCollatorForPermutationLanguageModeling._tensorize_batch(self,examples:List[Union[List[int],torch.Tensor,Dict[str,torch.Tensor]]])->torch.Tensor
transformers.DataCollatorForPermutationLanguageModeling.mask_tokens(self,inputs:torch.Tensor)->Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]
transformers.DataCollatorWithPadding(self,features:List[Dict[str,Union[List[int],torch.Tensor]]])
transformers.data.data_collator.DataCollatorForLanguageModeling(self,examples:List[Union[List[int],torch.Tensor,Dict[str,torch.Tensor]]])
transformers.data.data_collator.DataCollatorForLanguageModeling.__call__(self,examples:List[Union[List[int],torch.Tensor,Dict[str,torch.Tensor]]])
transformers.data.data_collator.DataCollatorForLanguageModeling._tensorize_batch(self,examples:List[Union[List[int],torch.Tensor,Dict[str,torch.Tensor]]])->torch.Tensor
transformers.data.data_collator.DataCollatorForLanguageModeling.mask_tokens(self,inputs:torch.Tensor)->Tuple[torch.Tensor, torch.Tensor]
transformers.data.data_collator.DataCollatorForNextSentencePrediction(self,examples:List[Union[List[List[int]],Dict[str,torch.Tensor]]])
transformers.data.data_collator.DataCollatorForNextSentencePrediction.__call__(self,examples:List[Union[List[List[int]],Dict[str,torch.Tensor]]])
transformers.data.data_collator.DataCollatorForNextSentencePrediction._tensorize_batch(self,examples:List[torch.Tensor])->torch.Tensor
transformers.data.data_collator.DataCollatorForNextSentencePrediction.create_examples_from_document(self,document:List[List[int]],doc_index:int,examples:List[List[List[int]]])
transformers.data.data_collator.DataCollatorForNextSentencePrediction.mask_tokens(self,inputs:torch.Tensor)->Tuple[torch.Tensor, torch.Tensor]
transformers.data.data_collator.DataCollatorForPermutationLanguageModeling(self,examples:List[Union[List[int],torch.Tensor,Dict[str,torch.Tensor]]])
transformers.data.data_collator.DataCollatorForPermutationLanguageModeling.__call__(self,examples:List[Union[List[int],torch.Tensor,Dict[str,torch.Tensor]]])
transformers.data.data_collator.DataCollatorForPermutationLanguageModeling._tensorize_batch(self,examples:List[Union[List[int],torch.Tensor,Dict[str,torch.Tensor]]])->torch.Tensor
transformers.data.data_collator.DataCollatorForPermutationLanguageModeling.mask_tokens(self,inputs:torch.Tensor)->Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]
transformers.data.data_collator.DataCollatorWithPadding(self,features:List[Dict[str,Union[List[int],torch.Tensor]]])
transformers.data.data_collator.DataCollatorWithPadding.__call__(self,features:List[Dict[str,Union[List[int],torch.Tensor]]])
transformers.data.data_collator.default_data_collator(features:List[InputDataClass])->Dict[str, torch.Tensor]
transformers.default_data_collator(features:List[InputDataClass])->Dict[str, torch.Tensor]


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/data/test_generation_utils.py----------------------------------------
A:transformers.data.test_generation_utils.config->transformers.MarianConfig.from_pretrained('sshleifer/tiny-marian-en-de')
A:transformers.data.test_generation_utils.input_ids->torch.arange(0, 96, 1).view((8, 12))
A:transformers.data.test_generation_utils.scores->torch.rand((8, 300))
A:transformers.data.test_generation_utils.output->model.postprocess_next_token_scores(scores, input_ids, 0, bad_words_ids, 13, 15, config.max_length, config.eos_token_id, config.repetition_penalty, 32, 5)
A:transformers.data.test_generation_utils.length_bad_word->random.randint(1, 4)
A:transformers.data.test_generation_utils._->model.postprocess_next_token_scores(scores, input_ids, 0, bad_words_ids, 13, 15, config.max_length, config.eos_token_id, config.repetition_penalty, 32, 5)
transformers.data.test_generation_utils.GenerationUtilsTest(unittest.TestCase)
transformers.data.test_generation_utils.GenerationUtilsTest.config(self)
transformers.data.test_generation_utils.GenerationUtilsTest.model(self)
transformers.data.test_generation_utils.GenerationUtilsTest.test_postprocess_next_token_scores(self)
transformers.data.test_generation_utils.GenerationUtilsTest.test_postprocess_next_token_scores_large_bad_words_list(self)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/data/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/data/processors/glue.py----------------------------------------
A:transformers.data.processors.glue.logger->utils.logging.get_logger(__name__)
A:transformers.data.processors.glue.processor->glue_processors[task]()
A:transformers.data.processors.glue.features->glue_convert_examples_to_features(examples, tokenizer, max_length=max_length, task=task)
A:transformers.data.processors.glue.label->d.pop('label')
A:transformers.data.processors.glue.label_list->glue_processors[task]().get_labels()
A:transformers.data.processors.glue.batch_encoding->tokenizer([(example.text_a, example.text_b) for example in examples], max_length=max_length, padding='max_length', truncation=True)
A:transformers.data.processors.glue.feature->InputFeatures(**inputs, label=labels[i])
transformers.data.processors.glue.ColaProcessor(DataProcessor)
transformers.data.processors.glue.ColaProcessor._create_examples(self,lines,set_type)
transformers.data.processors.glue.ColaProcessor.get_dev_examples(self,data_dir)
transformers.data.processors.glue.ColaProcessor.get_example_from_tensor_dict(self,tensor_dict)
transformers.data.processors.glue.ColaProcessor.get_labels(self)
transformers.data.processors.glue.ColaProcessor.get_test_examples(self,data_dir)
transformers.data.processors.glue.ColaProcessor.get_train_examples(self,data_dir)
transformers.data.processors.glue.MnliMismatchedProcessor(MnliProcessor)
transformers.data.processors.glue.MnliMismatchedProcessor.get_dev_examples(self,data_dir)
transformers.data.processors.glue.MnliMismatchedProcessor.get_test_examples(self,data_dir)
transformers.data.processors.glue.MnliProcessor(DataProcessor)
transformers.data.processors.glue.MnliProcessor._create_examples(self,lines,set_type)
transformers.data.processors.glue.MnliProcessor.get_dev_examples(self,data_dir)
transformers.data.processors.glue.MnliProcessor.get_example_from_tensor_dict(self,tensor_dict)
transformers.data.processors.glue.MnliProcessor.get_labels(self)
transformers.data.processors.glue.MnliProcessor.get_test_examples(self,data_dir)
transformers.data.processors.glue.MnliProcessor.get_train_examples(self,data_dir)
transformers.data.processors.glue.MrpcProcessor(DataProcessor)
transformers.data.processors.glue.MrpcProcessor._create_examples(self,lines,set_type)
transformers.data.processors.glue.MrpcProcessor.get_dev_examples(self,data_dir)
transformers.data.processors.glue.MrpcProcessor.get_example_from_tensor_dict(self,tensor_dict)
transformers.data.processors.glue.MrpcProcessor.get_labels(self)
transformers.data.processors.glue.MrpcProcessor.get_test_examples(self,data_dir)
transformers.data.processors.glue.MrpcProcessor.get_train_examples(self,data_dir)
transformers.data.processors.glue.OutputMode(Enum)
transformers.data.processors.glue.QnliProcessor(DataProcessor)
transformers.data.processors.glue.QnliProcessor._create_examples(self,lines,set_type)
transformers.data.processors.glue.QnliProcessor.get_dev_examples(self,data_dir)
transformers.data.processors.glue.QnliProcessor.get_example_from_tensor_dict(self,tensor_dict)
transformers.data.processors.glue.QnliProcessor.get_labels(self)
transformers.data.processors.glue.QnliProcessor.get_test_examples(self,data_dir)
transformers.data.processors.glue.QnliProcessor.get_train_examples(self,data_dir)
transformers.data.processors.glue.QqpProcessor(DataProcessor)
transformers.data.processors.glue.QqpProcessor._create_examples(self,lines,set_type)
transformers.data.processors.glue.QqpProcessor.get_dev_examples(self,data_dir)
transformers.data.processors.glue.QqpProcessor.get_example_from_tensor_dict(self,tensor_dict)
transformers.data.processors.glue.QqpProcessor.get_labels(self)
transformers.data.processors.glue.QqpProcessor.get_test_examples(self,data_dir)
transformers.data.processors.glue.QqpProcessor.get_train_examples(self,data_dir)
transformers.data.processors.glue.RteProcessor(DataProcessor)
transformers.data.processors.glue.RteProcessor._create_examples(self,lines,set_type)
transformers.data.processors.glue.RteProcessor.get_dev_examples(self,data_dir)
transformers.data.processors.glue.RteProcessor.get_example_from_tensor_dict(self,tensor_dict)
transformers.data.processors.glue.RteProcessor.get_labels(self)
transformers.data.processors.glue.RteProcessor.get_test_examples(self,data_dir)
transformers.data.processors.glue.RteProcessor.get_train_examples(self,data_dir)
transformers.data.processors.glue.Sst2Processor(DataProcessor)
transformers.data.processors.glue.Sst2Processor._create_examples(self,lines,set_type)
transformers.data.processors.glue.Sst2Processor.get_dev_examples(self,data_dir)
transformers.data.processors.glue.Sst2Processor.get_example_from_tensor_dict(self,tensor_dict)
transformers.data.processors.glue.Sst2Processor.get_labels(self)
transformers.data.processors.glue.Sst2Processor.get_test_examples(self,data_dir)
transformers.data.processors.glue.Sst2Processor.get_train_examples(self,data_dir)
transformers.data.processors.glue.StsbProcessor(DataProcessor)
transformers.data.processors.glue.StsbProcessor._create_examples(self,lines,set_type)
transformers.data.processors.glue.StsbProcessor.get_dev_examples(self,data_dir)
transformers.data.processors.glue.StsbProcessor.get_example_from_tensor_dict(self,tensor_dict)
transformers.data.processors.glue.StsbProcessor.get_labels(self)
transformers.data.processors.glue.StsbProcessor.get_test_examples(self,data_dir)
transformers.data.processors.glue.StsbProcessor.get_train_examples(self,data_dir)
transformers.data.processors.glue.WnliProcessor(DataProcessor)
transformers.data.processors.glue.WnliProcessor._create_examples(self,lines,set_type)
transformers.data.processors.glue.WnliProcessor.get_dev_examples(self,data_dir)
transformers.data.processors.glue.WnliProcessor.get_example_from_tensor_dict(self,tensor_dict)
transformers.data.processors.glue.WnliProcessor.get_labels(self)
transformers.data.processors.glue.WnliProcessor.get_test_examples(self,data_dir)
transformers.data.processors.glue.WnliProcessor.get_train_examples(self,data_dir)
transformers.data.processors.glue._glue_convert_examples_to_features(examples:List[InputExample],tokenizer:PreTrainedTokenizer,max_length:Optional[int]=None,task=None,label_list=None,output_mode=None)
transformers.data.processors.glue.glue_convert_examples_to_features(examples:Union[List[InputExample],'tf.data.Dataset'],tokenizer:PreTrainedTokenizer,max_length:Optional[int]=None,task=None,label_list=None,output_mode=None)
transformers.glue_convert_examples_to_features(examples:Union[List[InputExample],'tf.data.Dataset'],tokenizer:PreTrainedTokenizer,max_length:Optional[int]=None,task=None,label_list=None,output_mode=None)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/data/processors/utils.py----------------------------------------
A:transformers.data.processors.utils.logger->utils.logging.get_logger(__name__)
A:transformers.data.processors.utils.processor->cls(**kwargs)
A:transformers.data.processors.utils.lines->self._read_tsv(file_name)
A:transformers.data.processors.utils.added_labels->set()
A:transformers.data.processors.utils.self.labels->list(set(self.labels).union(added_labels))
A:transformers.data.processors.utils.input_ids->tokenizer.encode(example.text_a, add_special_tokens=True, max_length=min(max_length, tokenizer.max_len))
A:transformers.data.processors.utils.batch_length->max((len(input_ids) for input_ids in all_input_ids))
A:transformers.data.processors.utils.label->float(example.label)
A:transformers.data.processors.utils.dataset->TensorDataset(all_input_ids, all_attention_mask, all_labels)
A:transformers.data.processors.utils.all_input_ids->torch.tensor([f.input_ids for f in features], dtype=torch.long)
A:transformers.data.processors.utils.all_attention_mask->torch.tensor([f.attention_mask for f in features], dtype=torch.long)
A:transformers.data.processors.utils.all_labels->torch.tensor([f.label for f in features], dtype=torch.float)
transformers.DataProcessor
transformers.DataProcessor._read_tsv(cls,input_file,quotechar=None)
transformers.DataProcessor.get_dev_examples(self,data_dir)
transformers.DataProcessor.get_example_from_tensor_dict(self,tensor_dict)
transformers.DataProcessor.get_labels(self)
transformers.DataProcessor.get_test_examples(self,data_dir)
transformers.DataProcessor.get_train_examples(self,data_dir)
transformers.DataProcessor.tfds_map(self,example)
transformers.InputExample
transformers.InputExample.to_json_string(self)
transformers.InputFeatures
transformers.InputFeatures.to_json_string(self)
transformers.SingleSentenceClassificationProcessor(self,labels=None,examples=None,mode='classification',verbose=False)
transformers.SingleSentenceClassificationProcessor.__getitem__(self,idx)
transformers.SingleSentenceClassificationProcessor.__len__(self)
transformers.SingleSentenceClassificationProcessor.add_examples(self,texts_or_text_and_labels,labels=None,ids=None,overwrite_labels=False,overwrite_examples=False)
transformers.SingleSentenceClassificationProcessor.add_examples_from_csv(self,file_name,split_name='',column_label=0,column_text=1,column_id=None,skip_first_row=False,overwrite_labels=False,overwrite_examples=False)
transformers.SingleSentenceClassificationProcessor.create_from_csv(cls,file_name,split_name='',column_label=0,column_text=1,column_id=None,skip_first_row=False,**kwargs)
transformers.SingleSentenceClassificationProcessor.create_from_examples(cls,texts_or_text_and_labels,labels=None,**kwargs)
transformers.SingleSentenceClassificationProcessor.get_features(self,tokenizer,max_length=None,pad_on_left=False,pad_token=0,mask_padding_with_zero=True,return_tensors=None)
transformers.data.processors.utils.DataProcessor
transformers.data.processors.utils.DataProcessor._read_tsv(cls,input_file,quotechar=None)
transformers.data.processors.utils.DataProcessor.get_dev_examples(self,data_dir)
transformers.data.processors.utils.DataProcessor.get_example_from_tensor_dict(self,tensor_dict)
transformers.data.processors.utils.DataProcessor.get_labels(self)
transformers.data.processors.utils.DataProcessor.get_test_examples(self,data_dir)
transformers.data.processors.utils.DataProcessor.get_train_examples(self,data_dir)
transformers.data.processors.utils.DataProcessor.tfds_map(self,example)
transformers.data.processors.utils.InputExample
transformers.data.processors.utils.InputExample.to_json_string(self)
transformers.data.processors.utils.InputFeatures
transformers.data.processors.utils.InputFeatures.to_json_string(self)
transformers.data.processors.utils.SingleSentenceClassificationProcessor(self,labels=None,examples=None,mode='classification',verbose=False)
transformers.data.processors.utils.SingleSentenceClassificationProcessor.__getitem__(self,idx)
transformers.data.processors.utils.SingleSentenceClassificationProcessor.__init__(self,labels=None,examples=None,mode='classification',verbose=False)
transformers.data.processors.utils.SingleSentenceClassificationProcessor.__len__(self)
transformers.data.processors.utils.SingleSentenceClassificationProcessor.add_examples(self,texts_or_text_and_labels,labels=None,ids=None,overwrite_labels=False,overwrite_examples=False)
transformers.data.processors.utils.SingleSentenceClassificationProcessor.add_examples_from_csv(self,file_name,split_name='',column_label=0,column_text=1,column_id=None,skip_first_row=False,overwrite_labels=False,overwrite_examples=False)
transformers.data.processors.utils.SingleSentenceClassificationProcessor.create_from_csv(cls,file_name,split_name='',column_label=0,column_text=1,column_id=None,skip_first_row=False,**kwargs)
transformers.data.processors.utils.SingleSentenceClassificationProcessor.create_from_examples(cls,texts_or_text_and_labels,labels=None,**kwargs)
transformers.data.processors.utils.SingleSentenceClassificationProcessor.get_features(self,tokenizer,max_length=None,pad_on_left=False,pad_token=0,mask_padding_with_zero=True,return_tensors=None)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/data/processors/squad.py----------------------------------------
A:transformers.data.processors.squad.logger->utils.logging.get_logger(__name__)
A:transformers.data.processors.squad.tok_answer_text->' '.join(tokenizer.tokenize(orig_answer_text))
A:transformers.data.processors.squad.text_span->' '.join(doc_tokens[new_start:new_end + 1])
A:transformers.data.processors.squad.actual_text->' '.join(example.doc_tokens[start_position:end_position + 1])
A:transformers.data.processors.squad.cleaned_answer_text->' '.join(whitespace_tokenize(example.answer_text))
A:transformers.data.processors.squad.sub_tokens->tokenizer.tokenize(token)
A:transformers.data.processors.squad.(tok_start_position, tok_end_position)->_improve_answer_span(all_doc_tokens, tok_start_position, tok_end_position, tokenizer, example.answer_text)
A:transformers.data.processors.squad.truncated_query->tokenizer.encode(example.question_text, add_special_tokens=False, truncation=True, max_length=max_query_length)
A:transformers.data.processors.squad.tokenizer_type->type(tokenizer).__name__.replace('Tokenizer', '').lower()
A:transformers.data.processors.squad.encoded_dict->tokenizer.encode_plus(texts, pairs, truncation=truncation, padding=padding_strategy, max_length=max_seq_length, return_overflowing_tokens=True, stride=max_seq_length - doc_stride - len(truncated_query) - sequence_pair_added_tokens, return_token_type_ids=True)
A:transformers.data.processors.squad.paragraph_len->min(len(all_doc_tokens) - len(spans) * doc_stride, max_seq_length - len(truncated_query) - sequence_pair_added_tokens)
A:transformers.data.processors.squad.tokens->tokenizer.convert_ids_to_tokens(non_padded_ids)
A:transformers.data.processors.squad.is_max_context->_new_check_is_max_context(spans, doc_span_index, doc_span_index * doc_stride + j)
A:transformers.data.processors.squad.cls_index->span['input_ids'].index(tokenizer.cls_token_id)
A:transformers.data.processors.squad.p_mask->numpy.ones_like(span['token_type_ids'])
A:transformers.data.processors.squad.pad_token_indices->numpy.where(span['input_ids'] == tokenizer.pad_token_id)
A:transformers.data.processors.squad.special_token_indices->numpy.asarray(tokenizer.get_special_tokens_mask(span['input_ids'], already_has_special_tokens=True)).nonzero()
A:transformers.data.processors.squad.threads->min(threads, cpu_count())
A:transformers.data.processors.squad.annotate_->partial(squad_convert_example_to_features, max_seq_length=max_seq_length, doc_stride=doc_stride, max_query_length=max_query_length, padding_strategy=padding_strategy, is_training=is_training)
A:transformers.data.processors.squad.features->list(tqdm(p.imap(annotate_, examples, chunksize=32), total=len(examples), desc='convert squad examples to features', disable=not tqdm_enabled))
A:transformers.data.processors.squad.all_input_ids->torch.tensor([f.input_ids for f in features], dtype=torch.long)
A:transformers.data.processors.squad.all_attention_masks->torch.tensor([f.attention_mask for f in features], dtype=torch.long)
A:transformers.data.processors.squad.all_token_type_ids->torch.tensor([f.token_type_ids for f in features], dtype=torch.long)
A:transformers.data.processors.squad.all_cls_index->torch.tensor([f.cls_index for f in features], dtype=torch.long)
A:transformers.data.processors.squad.all_p_mask->torch.tensor([f.p_mask for f in features], dtype=torch.float)
A:transformers.data.processors.squad.all_is_impossible->torch.tensor([f.is_impossible for f in features], dtype=torch.float)
A:transformers.data.processors.squad.all_feature_index->torch.arange(all_input_ids.size(0), dtype=torch.long)
A:transformers.data.processors.squad.dataset->TensorDataset(all_input_ids, all_attention_masks, all_token_type_ids, all_start_positions, all_end_positions, all_cls_index, all_p_mask, all_is_impossible)
A:transformers.data.processors.squad.all_start_positions->torch.tensor([f.start_position for f in features], dtype=torch.long)
A:transformers.data.processors.squad.all_end_positions->torch.tensor([f.end_position for f in features], dtype=torch.long)
A:transformers.data.processors.squad.answer->tensor_dict['answers']['text'][0].numpy().decode('utf-8')
A:transformers.data.processors.squad.answer_start->tensor_dict['answers']['answer_start'][0].numpy()
A:transformers.data.processors.squad.is_impossible->qa.get('is_impossible', False)
A:transformers.data.processors.squad.example->SquadExample(qas_id=qas_id, question_text=question_text, context_text=context_text, answer_text=answer_text, start_position_character=start_position_character, title=title, is_impossible=is_impossible, answers=answers)
transformers.SquadExample(self,qas_id,question_text,context_text,answer_text,start_position_character,title,answers=[],is_impossible=False)
transformers.SquadFeatures(self,input_ids,attention_mask,token_type_ids,cls_index,p_mask,example_index,unique_id,paragraph_len,token_is_max_context,tokens,token_to_orig_map,start_position,end_position,is_impossible,qas_id:str=None)
transformers.SquadV1Processor(SquadProcessor)
transformers.SquadV2Processor(SquadProcessor)
transformers.data.processors.squad.SquadExample(self,qas_id,question_text,context_text,answer_text,start_position_character,title,answers=[],is_impossible=False)
transformers.data.processors.squad.SquadExample.__init__(self,qas_id,question_text,context_text,answer_text,start_position_character,title,answers=[],is_impossible=False)
transformers.data.processors.squad.SquadFeatures(self,input_ids,attention_mask,token_type_ids,cls_index,p_mask,example_index,unique_id,paragraph_len,token_is_max_context,tokens,token_to_orig_map,start_position,end_position,is_impossible,qas_id:str=None)
transformers.data.processors.squad.SquadFeatures.__init__(self,input_ids,attention_mask,token_type_ids,cls_index,p_mask,example_index,unique_id,paragraph_len,token_is_max_context,tokens,token_to_orig_map,start_position,end_position,is_impossible,qas_id:str=None)
transformers.data.processors.squad.SquadProcessor(DataProcessor)
transformers.data.processors.squad.SquadProcessor._create_examples(self,input_data,set_type)
transformers.data.processors.squad.SquadProcessor._get_example_from_tensor_dict(self,tensor_dict,evaluate=False)
transformers.data.processors.squad.SquadProcessor.get_dev_examples(self,data_dir,filename=None)
transformers.data.processors.squad.SquadProcessor.get_examples_from_dataset(self,dataset,evaluate=False)
transformers.data.processors.squad.SquadProcessor.get_train_examples(self,data_dir,filename=None)
transformers.data.processors.squad.SquadResult(self,unique_id,start_logits,end_logits,start_top_index=None,end_top_index=None,cls_logits=None)
transformers.data.processors.squad.SquadResult.__init__(self,unique_id,start_logits,end_logits,start_top_index=None,end_top_index=None,cls_logits=None)
transformers.data.processors.squad.SquadV1Processor(SquadProcessor)
transformers.data.processors.squad.SquadV2Processor(SquadProcessor)
transformers.data.processors.squad._check_is_max_context(doc_spans,cur_span_index,position)
transformers.data.processors.squad._improve_answer_span(doc_tokens,input_start,input_end,tokenizer,orig_answer_text)
transformers.data.processors.squad._is_whitespace(c)
transformers.data.processors.squad._new_check_is_max_context(doc_spans,cur_span_index,position)
transformers.data.processors.squad.squad_convert_example_to_features(example,max_seq_length,doc_stride,max_query_length,padding_strategy,is_training)
transformers.data.processors.squad.squad_convert_example_to_features_init(tokenizer_for_convert)
transformers.data.processors.squad.squad_convert_examples_to_features(examples,tokenizer,max_seq_length,doc_stride,max_query_length,is_training,padding_strategy='max_length',return_dataset=False,threads=1,tqdm_enabled=True)
transformers.squad_convert_examples_to_features(examples,tokenizer,max_seq_length,doc_stride,max_query_length,is_training,padding_strategy='max_length',return_dataset=False,threads=1,tqdm_enabled=True)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/data/processors/xnli.py----------------------------------------
A:transformers.data.processors.xnli.logger->utils.logging.get_logger(__name__)
A:transformers.data.processors.xnli.lines->self._read_tsv(os.path.join(data_dir, 'XNLI-1.0/xnli.test.tsv'))
transformers.data.processors.xnli.XnliProcessor(self,language,train_language=None)
transformers.data.processors.xnli.XnliProcessor.__init__(self,language,train_language=None)
transformers.data.processors.xnli.XnliProcessor.get_labels(self)
transformers.data.processors.xnli.XnliProcessor.get_test_examples(self,data_dir)
transformers.data.processors.xnli.XnliProcessor.get_train_examples(self,data_dir)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/data/processors/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/data/metrics/__init__.py----------------------------------------
A:transformers.data.metrics.__init__.acc->simple_accuracy(preds, labels)
A:transformers.data.metrics.__init__.f1->f1_score(y_true=labels, y_pred=preds)
transformers.data.metrics.__init__.is_sklearn_available()


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/data/metrics/squad_metrics.py----------------------------------------
A:transformers.data.metrics.squad_metrics.logger->utils.logging.get_logger(__name__)
A:transformers.data.metrics.squad_metrics.regex->re.compile('\\b(a|an|the)\\b', re.UNICODE)
A:transformers.data.metrics.squad_metrics.exclude->set(string.punctuation)
A:transformers.data.metrics.squad_metrics.gold_toks->get_tokens(a_gold)
A:transformers.data.metrics.squad_metrics.pred_toks->get_tokens(a_pred)
A:transformers.data.metrics.squad_metrics.num_same->sum(common.values())
A:transformers.data.metrics.squad_metrics.exact_scores[qas_id]->max((compute_exact(a, prediction) for a in gold_answers))
A:transformers.data.metrics.squad_metrics.f1_scores[qas_id]->max((compute_f1(a, prediction) for a in gold_answers))
A:transformers.data.metrics.squad_metrics.new_scores[qid]->float(not qid_to_has_ans[qid])
A:transformers.data.metrics.squad_metrics.total->len(qid_list)
A:transformers.data.metrics.squad_metrics.num_no_ans->sum((1 for k in qid_to_has_ans if not qid_to_has_ans[k]))
A:transformers.data.metrics.squad_metrics.qid_list->sorted(na_probs, key=lambda k: na_probs[k])
A:transformers.data.metrics.squad_metrics.(best_exact, exact_thresh, has_ans_exact)->find_best_thresh_v2(preds, exact_raw, na_probs, qid_to_has_ans)
A:transformers.data.metrics.squad_metrics.(best_f1, f1_thresh, has_ans_f1)->find_best_thresh_v2(preds, f1_raw, na_probs, qid_to_has_ans)
A:transformers.data.metrics.squad_metrics.(best_exact, exact_thresh)->find_best_thresh(preds, exact_raw, na_probs, qid_to_has_ans)
A:transformers.data.metrics.squad_metrics.(best_f1, f1_thresh)->find_best_thresh(preds, f1_raw, na_probs, qid_to_has_ans)
A:transformers.data.metrics.squad_metrics.(exact, f1)->get_raw_scores(examples, preds)
A:transformers.data.metrics.squad_metrics.exact_threshold->apply_no_ans_threshold(exact, no_answer_probs, qas_id_to_has_answer, no_answer_probability_threshold)
A:transformers.data.metrics.squad_metrics.f1_threshold->apply_no_ans_threshold(f1, no_answer_probs, qas_id_to_has_answer, no_answer_probability_threshold)
A:transformers.data.metrics.squad_metrics.evaluation->make_eval_dict(exact_threshold, f1_threshold)
A:transformers.data.metrics.squad_metrics.has_ans_eval->make_eval_dict(exact_threshold, f1_threshold, qid_list=has_answer_qids)
A:transformers.data.metrics.squad_metrics.no_ans_eval->make_eval_dict(exact_threshold, f1_threshold, qid_list=no_answer_qids)
A:transformers.data.metrics.squad_metrics.ns_to_s_map->collections.OrderedDict()
A:transformers.data.metrics.squad_metrics.ns_text->''.join(ns_chars)
A:transformers.data.metrics.squad_metrics.tokenizer->BasicTokenizer(do_lower_case=do_lower_case)
A:transformers.data.metrics.squad_metrics.tok_text->' '.join(tok_text.split())
A:transformers.data.metrics.squad_metrics.start_position->' '.join(tok_text.split()).find(pred_text)
A:transformers.data.metrics.squad_metrics.(orig_ns_text, orig_ns_to_s_map)->_strip_spaces(orig_text)
A:transformers.data.metrics.squad_metrics.(tok_ns_text, tok_ns_to_s_map)->_strip_spaces(tok_text)
A:transformers.data.metrics.squad_metrics.index_and_score->sorted(enumerate(logits), key=lambda x: x[1], reverse=True)
A:transformers.data.metrics.squad_metrics.x->math.exp(score - max_score)
A:transformers.data.metrics.squad_metrics.example_index_to_features->collections.defaultdict(list)
A:transformers.data.metrics.squad_metrics._PrelimPrediction->collections.namedtuple('PrelimPrediction', ['feature_index', 'start_index', 'end_index', 'start_log_prob', 'end_log_prob'])
A:transformers.data.metrics.squad_metrics.all_predictions->collections.OrderedDict()
A:transformers.data.metrics.squad_metrics.all_nbest_json->collections.OrderedDict()
A:transformers.data.metrics.squad_metrics.scores_diff_json->collections.OrderedDict()
A:transformers.data.metrics.squad_metrics.start_indexes->_get_best_indexes(result.start_logits, n_best_size)
A:transformers.data.metrics.squad_metrics.end_indexes->_get_best_indexes(result.end_logits, n_best_size)
A:transformers.data.metrics.squad_metrics.prelim_predictions->sorted(prelim_predictions, key=lambda x: x.start_log_prob + x.end_log_prob, reverse=True)
A:transformers.data.metrics.squad_metrics._NbestPrediction->collections.namedtuple('NbestPrediction', ['text', 'start_log_prob', 'end_log_prob'])
A:transformers.data.metrics.squad_metrics.orig_text->' '.join(orig_tokens)
A:transformers.data.metrics.squad_metrics.final_text->get_final_text(tok_text, orig_text, do_lower_case, verbose_logging)
A:transformers.data.metrics.squad_metrics.probs->_compute_softmax(total_scores)
A:transformers.data.metrics.squad_metrics.output->collections.OrderedDict()
A:transformers.data.metrics.squad_metrics.score_null->min(score_null, cur_null_score)
transformers.data.metrics.squad_metrics._compute_softmax(scores)
transformers.data.metrics.squad_metrics._get_best_indexes(logits,n_best_size)
transformers.data.metrics.squad_metrics.apply_no_ans_threshold(scores,na_probs,qid_to_has_ans,na_prob_thresh)
transformers.data.metrics.squad_metrics.compute_exact(a_gold,a_pred)
transformers.data.metrics.squad_metrics.compute_f1(a_gold,a_pred)
transformers.data.metrics.squad_metrics.compute_predictions_log_probs(all_examples,all_features,all_results,n_best_size,max_answer_length,output_prediction_file,output_nbest_file,output_null_log_odds_file,start_n_top,end_n_top,version_2_with_negative,tokenizer,verbose_logging)
transformers.data.metrics.squad_metrics.compute_predictions_logits(all_examples,all_features,all_results,n_best_size,max_answer_length,do_lower_case,output_prediction_file,output_nbest_file,output_null_log_odds_file,verbose_logging,version_2_with_negative,null_score_diff_threshold,tokenizer)
transformers.data.metrics.squad_metrics.find_all_best_thresh(main_eval,preds,exact_raw,f1_raw,na_probs,qid_to_has_ans)
transformers.data.metrics.squad_metrics.find_all_best_thresh_v2(main_eval,preds,exact_raw,f1_raw,na_probs,qid_to_has_ans)
transformers.data.metrics.squad_metrics.find_best_thresh(preds,scores,na_probs,qid_to_has_ans)
transformers.data.metrics.squad_metrics.find_best_thresh_v2(preds,scores,na_probs,qid_to_has_ans)
transformers.data.metrics.squad_metrics.get_final_text(pred_text,orig_text,do_lower_case,verbose_logging=False)
transformers.data.metrics.squad_metrics.get_raw_scores(examples,preds)
transformers.data.metrics.squad_metrics.get_tokens(s)
transformers.data.metrics.squad_metrics.make_eval_dict(exact_scores,f1_scores,qid_list=None)
transformers.data.metrics.squad_metrics.merge_eval(main_eval,new_eval,prefix)
transformers.data.metrics.squad_metrics.normalize_answer(s)
transformers.data.metrics.squad_metrics.squad_evaluate(examples,preds,no_answer_probs=None,no_answer_probability_threshold=1.0)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/data/datasets/glue.py----------------------------------------
A:transformers.data.datasets.glue.logger->utils.logging.get_logger(__name__)
A:transformers.data.datasets.glue.self.task_name->self.task_name.lower()
A:transformers.data.datasets.glue.self.processor->glue_processors[args.task_name]()
A:transformers.data.datasets.glue.cached_features_file->os.path.join(cache_dir if cache_dir is not None else args.data_dir, 'cached_{}_{}_{}_{}'.format(mode.value, tokenizer.__class__.__name__, str(args.max_seq_length), args.task_name))
A:transformers.data.datasets.glue.label_list->self.processor.get_labels()
A:transformers.data.datasets.glue.start->time.time()
A:transformers.data.datasets.glue.self.features->glue_convert_examples_to_features(examples, tokenizer, max_length=args.max_seq_length, label_list=label_list, output_mode=self.output_mode)
A:transformers.data.datasets.glue.examples->self.processor.get_train_examples(args.data_dir)
transformers.GlueDataTrainingArguments
transformers.GlueDataTrainingArguments.__post_init__(self)
transformers.GlueDataset(self,args:GlueDataTrainingArguments,tokenizer:PreTrainedTokenizer,limit_length:Optional[int]=None,mode:Union[str,Split]=Split.train,cache_dir:Optional[str]=None)
transformers.GlueDataset.__getitem__(self,i)->InputFeatures
transformers.GlueDataset.__len__(self)
transformers.GlueDataset.get_labels(self)
transformers.data.datasets.glue.GlueDataTrainingArguments
transformers.data.datasets.glue.GlueDataTrainingArguments.__post_init__(self)
transformers.data.datasets.glue.GlueDataset(self,args:GlueDataTrainingArguments,tokenizer:PreTrainedTokenizer,limit_length:Optional[int]=None,mode:Union[str,Split]=Split.train,cache_dir:Optional[str]=None)
transformers.data.datasets.glue.GlueDataset.__getitem__(self,i)->InputFeatures
transformers.data.datasets.glue.GlueDataset.__init__(self,args:GlueDataTrainingArguments,tokenizer:PreTrainedTokenizer,limit_length:Optional[int]=None,mode:Union[str,Split]=Split.train,cache_dir:Optional[str]=None)
transformers.data.datasets.glue.GlueDataset.__len__(self)
transformers.data.datasets.glue.GlueDataset.get_labels(self)
transformers.data.datasets.glue.Split(Enum)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/data/datasets/language_modeling.py----------------------------------------
A:transformers.data.datasets.language_modeling.logger->utils.logging.get_logger(__name__)
A:transformers.data.datasets.language_modeling.(directory, filename)->os.path.split(file_path)
A:transformers.data.datasets.language_modeling.cached_features_file->os.path.join(directory, 'cached_nsp_{}_{}_{}'.format(tokenizer.__class__.__name__, str(block_size), filename))
A:transformers.data.datasets.language_modeling.start->time.time()
A:transformers.data.datasets.language_modeling.self.examples->pickle.load(handle)
A:transformers.data.datasets.language_modeling.text->f.read()
A:transformers.data.datasets.language_modeling.tokenized_text->tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))
A:transformers.data.datasets.language_modeling.batch_encoding->tokenizer(lines, add_special_tokens=True, truncation=True, max_length=block_size)
A:transformers.data.datasets.language_modeling.line->line.strip().strip()
A:transformers.data.datasets.language_modeling.tokens->tokenizer.convert_tokens_to_ids(tokens)
transformers.LineByLineTextDataset(self,tokenizer:PreTrainedTokenizer,file_path:str,block_size:int)
transformers.LineByLineTextDataset.__getitem__(self,i)->torch.Tensor
transformers.LineByLineTextDataset.__len__(self)
transformers.TextDataset(self,tokenizer:PreTrainedTokenizer,file_path:str,block_size:int,overwrite_cache=False)
transformers.TextDataset.__getitem__(self,i)->torch.Tensor
transformers.TextDataset.__len__(self)
transformers.TextDatasetForNextSentencePrediction(self,tokenizer:PreTrainedTokenizer,file_path:str,block_size:int,overwrite_cache=False)
transformers.TextDatasetForNextSentencePrediction.__getitem__(self,i)
transformers.TextDatasetForNextSentencePrediction.__len__(self)
transformers.data.datasets.language_modeling.LineByLineTextDataset(self,tokenizer:PreTrainedTokenizer,file_path:str,block_size:int)
transformers.data.datasets.language_modeling.LineByLineTextDataset.__getitem__(self,i)->torch.Tensor
transformers.data.datasets.language_modeling.LineByLineTextDataset.__init__(self,tokenizer:PreTrainedTokenizer,file_path:str,block_size:int)
transformers.data.datasets.language_modeling.LineByLineTextDataset.__len__(self)
transformers.data.datasets.language_modeling.TextDataset(self,tokenizer:PreTrainedTokenizer,file_path:str,block_size:int,overwrite_cache=False)
transformers.data.datasets.language_modeling.TextDataset.__getitem__(self,i)->torch.Tensor
transformers.data.datasets.language_modeling.TextDataset.__init__(self,tokenizer:PreTrainedTokenizer,file_path:str,block_size:int,overwrite_cache=False)
transformers.data.datasets.language_modeling.TextDataset.__len__(self)
transformers.data.datasets.language_modeling.TextDatasetForNextSentencePrediction(self,tokenizer:PreTrainedTokenizer,file_path:str,block_size:int,overwrite_cache=False)
transformers.data.datasets.language_modeling.TextDatasetForNextSentencePrediction.__getitem__(self,i)
transformers.data.datasets.language_modeling.TextDatasetForNextSentencePrediction.__init__(self,tokenizer:PreTrainedTokenizer,file_path:str,block_size:int,overwrite_cache=False)
transformers.data.datasets.language_modeling.TextDatasetForNextSentencePrediction.__len__(self)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/data/datasets/squad.py----------------------------------------
A:transformers.data.datasets.squad.logger->utils.logging.get_logger(__name__)
A:transformers.data.datasets.squad.MODEL_CONFIG_CLASSES->list(MODEL_FOR_QUESTION_ANSWERING_MAPPING.keys())
A:transformers.data.datasets.squad.MODEL_TYPES->tuple((conf.model_type for conf in MODEL_CONFIG_CLASSES))
A:transformers.data.datasets.squad.cached_features_file->os.path.join(cache_dir if cache_dir is not None else args.data_dir, 'cached_{}_{}_{}_{}'.format(mode.value, tokenizer.__class__.__name__, str(args.max_seq_length), version_tag))
A:transformers.data.datasets.squad.start->time.time()
A:transformers.data.datasets.squad.self.old_features->torch.load(cached_features_file)
A:transformers.data.datasets.squad.self.dataset->self.old_features.get('dataset', None)
A:transformers.data.datasets.squad.self.examples->self.processor.get_train_examples(args.data_dir)
A:transformers.data.datasets.squad.(self.features, self.dataset)->squad_convert_examples_to_features(examples=self.examples, tokenizer=tokenizer, max_seq_length=args.max_seq_length, doc_stride=args.doc_stride, max_query_length=args.max_query_length, is_training=mode == Split.train, threads=args.threads, return_dataset=dataset_format)
A:transformers.data.datasets.squad.input_ids->torch.tensor(feature.input_ids, dtype=torch.long)
A:transformers.data.datasets.squad.attention_mask->torch.tensor(feature.attention_mask, dtype=torch.long)
A:transformers.data.datasets.squad.token_type_ids->torch.tensor(feature.token_type_ids, dtype=torch.long)
A:transformers.data.datasets.squad.cls_index->torch.tensor(feature.cls_index, dtype=torch.long)
A:transformers.data.datasets.squad.p_mask->torch.tensor(feature.p_mask, dtype=torch.float)
A:transformers.data.datasets.squad.is_impossible->torch.tensor(feature.is_impossible, dtype=torch.float)
A:transformers.data.datasets.squad.start_positions->torch.tensor(feature.start_position, dtype=torch.long)
A:transformers.data.datasets.squad.end_positions->torch.tensor(feature.end_position, dtype=torch.long)
transformers.SquadDataTrainingArguments
transformers.SquadDataset(self,args:SquadDataTrainingArguments,tokenizer:PreTrainedTokenizer,limit_length:Optional[int]=None,mode:Union[str,Split]=Split.train,is_language_sensitive:Optional[bool]=False,cache_dir:Optional[str]=None,dataset_format:Optional[str]='pt')
transformers.SquadDataset.__getitem__(self,i)->Dict[str, torch.Tensor]
transformers.SquadDataset.__len__(self)
transformers.data.datasets.squad.Split(Enum)
transformers.data.datasets.squad.SquadDataTrainingArguments
transformers.data.datasets.squad.SquadDataset(self,args:SquadDataTrainingArguments,tokenizer:PreTrainedTokenizer,limit_length:Optional[int]=None,mode:Union[str,Split]=Split.train,is_language_sensitive:Optional[bool]=False,cache_dir:Optional[str]=None,dataset_format:Optional[str]='pt')
transformers.data.datasets.squad.SquadDataset.__getitem__(self,i)->Dict[str, torch.Tensor]
transformers.data.datasets.squad.SquadDataset.__init__(self,args:SquadDataTrainingArguments,tokenizer:PreTrainedTokenizer,limit_length:Optional[int]=None,mode:Union[str,Split]=Split.train,is_language_sensitive:Optional[bool]=False,cache_dir:Optional[str]=None,dataset_format:Optional[str]='pt')
transformers.data.datasets.squad.SquadDataset.__len__(self)


----------------------------------------/home/zhang/Packages/transformers/transformers3.1.0/data/datasets/__init__.py----------------------------------------

