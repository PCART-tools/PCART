
----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/__init__.py----------------------------------------
A:spacy.__init__.package->util.get_package_by_name(name, via=via)
spacy.__init__.load(name,via=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/lemmatizer.py----------------------------------------
A:spacy.lemmatizer.rules->pkg.load_json(('vocab', 'lemma_rules.json'), default={})
A:spacy.lemmatizer.lemmas->lemmatize(string, self.index.get(pos, {}), self.exc.get(pos, {}), self.rules.get(pos, []))
A:spacy.lemmatizer.string->string.lower().lower()
A:spacy.lemmatizer.index->set()
A:spacy.lemmatizer.pieces->line.split()
A:spacy.lemmatizer.exceptions[pieces[0]]->tuple(pieces[1:])
spacy.lemmatizer.Lemmatizer(self,index,exceptions,rules)
spacy.lemmatizer.Lemmatizer.__init__(self,index,exceptions,rules)
spacy.lemmatizer.Lemmatizer.adj(self,string)
spacy.lemmatizer.Lemmatizer.from_package(cls,pkg)
spacy.lemmatizer.Lemmatizer.load(cls,via)
spacy.lemmatizer.Lemmatizer.noun(self,string)
spacy.lemmatizer.Lemmatizer.punct(self,string)
spacy.lemmatizer.Lemmatizer.verb(self,string)
spacy.lemmatizer.lemmatize(string,index,exceptions,rules)
spacy.lemmatizer.read_exc(fileobj)
spacy.lemmatizer.read_index(fileobj)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/scorer.py----------------------------------------
A:spacy.scorer.self.tokens->PRFScore()
A:spacy.scorer.self.sbd->PRFScore()
A:spacy.scorer.self.unlabelled->PRFScore()
A:spacy.scorer.self.labelled->PRFScore()
A:spacy.scorer.self.tags->PRFScore()
A:spacy.scorer.self.ner->PRFScore()
A:spacy.scorer.gold_deps->set()
A:spacy.scorer.gold_tags->set()
A:spacy.scorer.gold_ents->set(tags_to_entities([annot[-1] for annot in gold.orig_annot]))
A:spacy.scorer.cand_deps->set()
A:spacy.scorer.cand_tags->set()
A:spacy.scorer.cand_ents->set()
spacy.scorer.PRFScore(self)
spacy.scorer.PRFScore.__init__(self)
spacy.scorer.PRFScore.fscore(self)
spacy.scorer.PRFScore.precision(self)
spacy.scorer.PRFScore.recall(self)
spacy.scorer.PRFScore.score_set(self,cand,gold)
spacy.scorer.Scorer(self,eval_punct=False)
spacy.scorer.Scorer.__init__(self,eval_punct=False)
spacy.scorer.Scorer.ents_f(self)
spacy.scorer.Scorer.ents_p(self)
spacy.scorer.Scorer.ents_r(self)
spacy.scorer.Scorer.las(self)
spacy.scorer.Scorer.score(self,tokens,gold,verbose=False)
spacy.scorer.Scorer.tags_acc(self)
spacy.scorer.Scorer.token_acc(self)
spacy.scorer.Scorer.uas(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/about.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/_.py----------------------------------------
A:spacy._.nlp->English()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/language.py----------------------------------------
A:spacy.language.get_lex_attr->cls.default_lex_attrs()
A:spacy.language.data_dir->util.get_package(data_dir).dir_path('ner')
A:spacy.language.package->util.get_package(data_dir)
A:spacy.language.vocab->self.default_vocab(package)
A:spacy.language.tokenizer->tokenizer.Tokenizer.from_package(package, self.vocab)
A:spacy.language.tagger->tagger.Tagger.from_package(package, self.vocab)
A:spacy.language.entity->self.default_entity(package, self.vocab)
A:spacy.language.parser->self.default_parser(package, self.vocab)
A:spacy.language.matcher->matcher.Matcher.from_package(package, self.vocab)
A:spacy.language.tokens->self.tokenizer(text)
A:spacy.language.strings_loc->os.path.join(data_dir, 'vocab', 'strings.json')
spacy.language.Language(self,data_dir=None,vocab=None,tokenizer=None,tagger=None,parser=None,entity=None,matcher=None,serializer=None,load_vectors=True,package=None)
spacy.language.Language.__init__(self,data_dir=None,vocab=None,tokenizer=None,tagger=None,parser=None,entity=None,matcher=None,serializer=None,load_vectors=True,package=None)
spacy.language.Language.__reduce__(self)
spacy.language.Language.cluster(string)
spacy.language.Language.default_dep_labels(cls)
spacy.language.Language.default_entity(cls,package,vocab)
spacy.language.Language.default_lex_attrs(cls)
spacy.language.Language.default_ner_labels(cls)
spacy.language.Language.default_parser(cls,package,vocab)
spacy.language.Language.default_vocab(cls,package,get_lex_attr=None)
spacy.language.Language.end_training(self,data_dir=None)
spacy.language.Language.is_alpha(string)
spacy.language.Language.is_ascii(string)
spacy.language.Language.is_digit(string)
spacy.language.Language.is_lower(string)
spacy.language.Language.is_punct(string)
spacy.language.Language.is_space(string)
spacy.language.Language.is_stop(string)
spacy.language.Language.is_title(string)
spacy.language.Language.is_upper(string)
spacy.language.Language.like_email(string)
spacy.language.Language.like_num(string)
spacy.language.Language.like_url(string)
spacy.language.Language.lower(string)
spacy.language.Language.norm(string)
spacy.language.Language.prefix(string)
spacy.language.Language.prob(string)
spacy.language.Language.shape(string)
spacy.language.Language.suffix(string)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/util.py----------------------------------------
A:spacy.util.start->min(length, max(0, start))
A:spacy.util.stop->min(length, max(start, stop))
A:spacy.util.tokenization->package.load_json(('tokenizer', 'specials.json'))
A:spacy.util.entries->fileobj.read().split('\n')
A:spacy.util.expression->'|'.join([piece for piece in entries if piece.strip()])
A:spacy.util.queue->list(indices)
A:spacy.util.string->string.replace(subtoks.replace('<SEP>', ' '), subtoks).replace(subtoks.replace('<SEP>', ' '), subtoks)
A:spacy.util.subtoks->chunk.split('<SEP>')
spacy.util.align_tokens(ref,indices)
spacy.util.detokenize(token_rules,words)
spacy.util.get_package(data_dir)
spacy.util.get_package_by_name(name=None,via=None)
spacy.util.normalize_slice(length,start,stop,step=None)
spacy.util.read_infix(fileobj)
spacy.util.read_lang_data(package)
spacy.util.read_prefix(fileobj)
spacy.util.read_suffix(fileobj)
spacy.util.utf8open(loc,mode='r')


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/multi_words.py----------------------------------------
spacy.multi_words.RegexMerger(self,regexes)
spacy.multi_words.RegexMerger.__init__(self,regexes)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/en/__init__.py----------------------------------------
A:spacy.en.__init__.STOPWORDS->set((w for w in STOPWORDS.split() if w))
A:spacy.en.__init__.LOCAL_DATA_DIR->os.path.join(path.dirname(__file__), 'data')
spacy.en.__init__.English(Language)
spacy.en.__init__.English.is_stop(string)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/en/regexes.py----------------------------------------
A:spacy.en.regexes.MW_PREPOSITIONS_RE->re.compile('|'.join(_mw_prepositions), flags=re.IGNORECASE)
A:spacy.en.regexes.TIME_RE->re.compile('{colon_digits}|{colon_digits} ?{am_pm}?|{one_two_digits} ?({am_pm})'.format(colon_digits='[0-2]?[0-9]:[0-5][0-9](?::[0-5][0-9])?', one_two_digits='[0-2]?[0-9]', am_pm='[ap]\\.?m\\.?'))
A:spacy.en.regexes.DATE_RE->re.compile('(?:this|last|next|the) (?:week|weekend|{days})'.format(days='Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday'))
A:spacy.en.regexes.MONEY_RE->re.compile('\\$\\d+(?:\\.\\d+)?|\\d+ dollars(?: \\d+ cents)?')
A:spacy.en.regexes.DAYS_RE->re.compile('Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday')


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/en/uget.py----------------------------------------
A:spacy.en.uget.self.start->time.time()
A:spacy.en.uget.self.time_left->math.ceil(elapsed * total_size / bytes_read - elapsed)
A:spacy.en.uget.percent->round(bytes_read * 100.0 / total_size, 2)
A:spacy.en.uget.content_range->read_request(request, offset=size, console=console, progress_func=progress, write_func=write).headers.get('Content-Range', '').strip()
A:spacy.en.uget.m->re.match('bytes (\\d+)-(\\d+)/(\\d+)', content_range)
A:spacy.en.uget.r->urlopen(HeadRequest(url))
A:spacy.en.uget.value->urlopen(HeadRequest(url)).headers.get(checksum_header)
A:spacy.en.uget.response->read_request(request, offset=size, console=console, progress_func=progress, write_func=write)
A:spacy.en.uget.(range_start, range_end, range_total)->get_content_range(response)
A:spacy.en.uget.eta->TimeEstimator()
A:spacy.en.uget.transfer_rate->RateSampler()
A:spacy.en.uget.chunk->read_request(request, offset=size, console=console, progress_func=progress, write_func=write).read(CHUNK_SIZE)
A:spacy.en.uget.path->os.path.abspath(path)
A:spacy.en.uget.size->f.tell()
A:spacy.en.uget.request->Request(url)
A:spacy.en.uget.origin_checksum->get_url_meta(url, checksum_header).get('checksum')
A:spacy.en.uget.meta->get_url_meta(url, checksum_header)
spacy.en.uget.InvalidChecksumException(Exception)
spacy.en.uget.InvalidOffsetException(Exception)
spacy.en.uget.MissingChecksumHeader(Exception)
spacy.en.uget.RateSampler(self,period=1)
spacy.en.uget.RateSampler.__enter__(self)
spacy.en.uget.RateSampler.__exit__(self,type,value,traceback)
spacy.en.uget.RateSampler.__init__(self,period=1)
spacy.en.uget.RateSampler.format(self,unit='MB')
spacy.en.uget.RateSampler.update(self,value)
spacy.en.uget.TimeEstimator(self,cooldown=1)
spacy.en.uget.TimeEstimator.__init__(self,cooldown=1)
spacy.en.uget.TimeEstimator.format(self)
spacy.en.uget.TimeEstimator.update(self,bytes_read,total_size)
spacy.en.uget.UnknownContentLengthException(Exception)
spacy.en.uget.UnsupportedHTTPCodeException(Exception)
spacy.en.uget.download(url,path='.',checksum=None,checksum_header=None,headers=None,console=None)
spacy.en.uget.format_bytes_read(bytes_read,unit='MB')
spacy.en.uget.format_percent(bytes_read,total_size)
spacy.en.uget.get_content_length(response)
spacy.en.uget.get_content_range(response)
spacy.en.uget.get_url_meta(url,checksum_header=None)
spacy.en.uget.progress(console,bytes_read,total_size,transfer_rate,eta)
spacy.en.uget.read_request(request,offset=0,console=None,progress_func=None,write_func=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/en/download.py----------------------------------------
A:spacy.en.download.data_path->os.path.join(path, 'data')
A:spacy.en.download.package->sputnik.install(about.__name__, about.__version__, about.__default_model__)
spacy.en.download.main(data_size='all',force=False)
spacy.en.download.migrate(path)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/test_pickle.py----------------------------------------
A:spacy.tests.test_pickle.file_->io.BytesIO()
A:spacy.tests.test_pickle.loaded->pickle.load(file_)
A:spacy.tests.test_pickle.f->tempfile.NamedTemporaryFile(delete=False)
A:spacy.tests.test_pickle.p->cloudpickle.CloudPickler(f)
A:spacy.tests.test_pickle.loaded_en->cloudpickle.load(open(f.name, 'rb'))
A:spacy.tests.test_pickle.doc->loaded_en(unicode('test parse'))
spacy.tests.test_pickle.test_cloudpickle_to_file(EN)
spacy.tests.test_pickle.test_pickle_english(EN)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/test_docs.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/prag_sbd.py----------------------------------------
A:spacy.tests.prag_sbd.EN->English()
A:spacy.tests.prag_sbd.tokens->EN(text)
A:spacy.tests.prag_sbd.sents->get_sent_strings('Hello world.Today is Tuesday.Mr. Smith went to the store and bought 1,000.That is a lot.')
spacy.tests.prag_sbd.get_sent_strings(text)
spacy.tests.prag_sbd.test_gr1()
spacy.tests.prag_sbd.test_gr10()
spacy.tests.prag_sbd.test_gr11()
spacy.tests.prag_sbd.test_gr12()
spacy.tests.prag_sbd.test_gr13()
spacy.tests.prag_sbd.test_gr14()
spacy.tests.prag_sbd.test_gr15()
spacy.tests.prag_sbd.test_gr16()
spacy.tests.prag_sbd.test_gr17()
spacy.tests.prag_sbd.test_gr18()
spacy.tests.prag_sbd.test_gr19()
spacy.tests.prag_sbd.test_gr2()
spacy.tests.prag_sbd.test_gr20()
spacy.tests.prag_sbd.test_gr21()
spacy.tests.prag_sbd.test_gr22()
spacy.tests.prag_sbd.test_gr23()
spacy.tests.prag_sbd.test_gr24()
spacy.tests.prag_sbd.test_gr25()
spacy.tests.prag_sbd.test_gr26()
spacy.tests.prag_sbd.test_gr27()
spacy.tests.prag_sbd.test_gr28()
spacy.tests.prag_sbd.test_gr29()
spacy.tests.prag_sbd.test_gr3()
spacy.tests.prag_sbd.test_gr30()
spacy.tests.prag_sbd.test_gr31()
spacy.tests.prag_sbd.test_gr32()
spacy.tests.prag_sbd.test_gr33()
spacy.tests.prag_sbd.test_gr34()
spacy.tests.prag_sbd.test_gr35()
spacy.tests.prag_sbd.test_gr36()
spacy.tests.prag_sbd.test_gr37()
spacy.tests.prag_sbd.test_gr38()
spacy.tests.prag_sbd.test_gr39()
spacy.tests.prag_sbd.test_gr4()
spacy.tests.prag_sbd.test_gr40()
spacy.tests.prag_sbd.test_gr41()
spacy.tests.prag_sbd.test_gr42()
spacy.tests.prag_sbd.test_gr43()
spacy.tests.prag_sbd.test_gr44()
spacy.tests.prag_sbd.test_gr45()
spacy.tests.prag_sbd.test_gr46()
spacy.tests.prag_sbd.test_gr47()
spacy.tests.prag_sbd.test_gr48()
spacy.tests.prag_sbd.test_gr49()
spacy.tests.prag_sbd.test_gr5()
spacy.tests.prag_sbd.test_gr50()
spacy.tests.prag_sbd.test_gr51()
spacy.tests.prag_sbd.test_gr52()
spacy.tests.prag_sbd.test_gr6()
spacy.tests.prag_sbd.test_gr7()
spacy.tests.prag_sbd.test_gr8()
spacy.tests.prag_sbd.test_gr9()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/test_matcher.py----------------------------------------
A:spacy.tests.test_matcher.tokens->EN('I like Google Now and java best')
A:spacy.tests.test_matcher.doc->EN.tokenizer('I like java')
spacy.tests.test_matcher.matcher(EN)
spacy.tests.test_matcher.test_compile(matcher)
spacy.tests.test_matcher.test_match_end(matcher,EN)
spacy.tests.test_matcher.test_match_middle(matcher,EN)
spacy.tests.test_matcher.test_match_multi(matcher,EN)
spacy.tests.test_matcher.test_match_preserved(matcher,EN)
spacy.tests.test_matcher.test_match_start(matcher,EN)
spacy.tests.test_matcher.test_no_match(matcher,EN)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/conftest.py----------------------------------------
A:spacy.tests.conftest.data_dir->os.environ.get('SPACY_DATA')
spacy.tests.conftest.EN()
spacy.tests.conftest.pytest_addoption(parser)
spacy.tests.conftest.pytest_runtest_setup(item)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/parser/test_ner.py----------------------------------------
A:spacy.tests.parser.test_ner.tokens->EN(u'Charity and other short-term aid have buoyed them so far, and a tax-relief bill working its way through Congress would help. But the September 11 Victim Compensation Fund, enacted by Congress to discourage people from filing lawsuits, will determine the shape of their lives for years to come.\n\n', entity=False)
A:spacy.tests.parser.test_ner.ents->matcher(doc)
A:spacy.tests.parser.test_ner.matcher->Matcher(EN.vocab, {'MemberNames': ('PERSON', {}, [[{LOWER: 'cal'}], [{LOWER: 'cal'}, {LOWER: 'henderson'}]])})
A:spacy.tests.parser.test_ner.doc->EN(u'who is cal the manager of?')
spacy.tests.parser.test_ner.test_consistency_bug(EN)
spacy.tests.parser.test_ner.test_simple_types(EN)
spacy.tests.parser.test_ner.test_unit_end_gazetteer(EN)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/parser/test_initial_actions_parse.py----------------------------------------
A:spacy.tests.parser.test_initial_actions_parse.doc->EN.tokenizer(u'I ate the pizza with anchovies.')
spacy.tests.parser.test_initial_actions_parse.test_initial(EN)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/parser/test_parser_pickle.py----------------------------------------
A:spacy.tests.parser.test_parser_pickle.file_->io.BytesIO()
A:spacy.tests.parser.test_parser_pickle.loaded->pickle.load(file_)
spacy.tests.parser.test_parser_pickle.test_pickle(EN)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/parser/test_parse.py----------------------------------------
A:spacy.tests.parser.test_parse.tokens->EN(u"i don't have other assistance")
spacy.tests.parser.test_parse.test_root(EN)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/parser/test_subtree.py----------------------------------------
A:spacy.tests.parser.test_subtree.sent->EN('The four wheels on the bus turned quickly')
spacy.tests.parser.test_subtree.test_subtrees(EN)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/parser/test_space_attachment.py----------------------------------------
A:spacy.tests.parser.test_space_attachment.doc->EN(text)
spacy.tests.parser.test_space_attachment.test_sentence_space(EN)
spacy.tests.parser.test_space_attachment.test_space_attachment(EN)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/parser/test_parse_navigate.py----------------------------------------
A:spacy.tests.parser.test_parse_navigate.text->file_.read()
A:spacy.tests.parser.test_parse_navigate.tokens->EN(sun_text)
A:spacy.tests.parser.test_parse_navigate.lefts[head.i]->set()
A:spacy.tests.parser.test_parse_navigate.rights[head.i]->set()
A:spacy.tests.parser.test_parse_navigate.subtree->list(token.subtree)
A:spacy.tests.parser.test_parse_navigate.debug->'\t'.join((token.orth_, token.right_edge.orth_, subtree[-1].orth_, token.right_edge.head.orth_))
spacy.tests.parser.test_parse_navigate.sun_text()
spacy.tests.parser.test_parse_navigate.test_child_consistency(EN,sun_text)
spacy.tests.parser.test_parse_navigate.test_consistency(EN,sun_text)
spacy.tests.parser.test_parse_navigate.test_edges(EN)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/parser/test_base_nps.py----------------------------------------
A:spacy.tests.parser.test_base_nps.sent->EN(u'A phrase with another phrase occurs')
A:spacy.tests.parser.test_base_nps.base_nps->list(sent.noun_chunks)
spacy.tests.parser.test_base_nps.test_coord(EN)
spacy.tests.parser.test_base_nps.test_merge_pp(EN)
spacy.tests.parser.test_base_nps.test_nsubj(EN)
spacy.tests.parser.test_base_nps.test_pp(EN)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/parser/test_sbd.py----------------------------------------
A:spacy.tests.parser.test_sbd.words->EN(string, tag=False, parse=False)
spacy.tests.parser.test_sbd.test_single_exclamation(EN)
spacy.tests.parser.test_sbd.test_single_no_period(EN)
spacy.tests.parser.test_sbd.test_single_period(EN)
spacy.tests.parser.test_sbd.test_single_question(EN)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/parser/test_conjuncts.py----------------------------------------
spacy.tests.parser.test_conjuncts.orths(tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/morphology/test_morphology_pickle.py----------------------------------------
A:spacy.tests.morphology.test_morphology_pickle.morphology->Morphology(StringStore(), {}, Lemmatizer({}, {}, {}))
A:spacy.tests.morphology.test_morphology_pickle.file_->io.BytesIO()
spacy.tests.morphology.test_morphology_pickle.test_pickle()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/spans/test_times.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/spans/test_span.py----------------------------------------
A:spacy.tests.spans.test_span.sents->list(doc.sents)
A:spacy.tests.spans.test_span.EN->English(parser=False)
A:spacy.tests.spans.test_span.doc->EN(text)
A:spacy.tests.spans.test_span.heads->numpy.asarray([[0, 3, -1, -2, -4]], dtype='int32')
spacy.tests.spans.test_span.doc(EN)
spacy.tests.spans.test_span.test_root(doc)
spacy.tests.spans.test_span.test_root2()
spacy.tests.spans.test_span.test_sent_spans(doc)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/spans/test_merge.py----------------------------------------
A:spacy.tests.spans.test_merge.tokens->EN(u'Stewart Lee is a stand up comedian who lives in England and loves Joe Pasquale')
A:spacy.tests.spans.test_merge.merged->EN(u'Stewart Lee is a stand up comedian who lives in England and loves Joe Pasquale').merge(start, end, label, lemma, label)
A:spacy.tests.spans.test_merge.(sent1, sent2)->list(tokens.sents)
A:spacy.tests.spans.test_merge.init_len->len(list(sent1.root.subtree))
A:spacy.tests.spans.test_merge.init_len2->len(sent2)
spacy.tests.spans.test_merge.test_entity_merge(EN)
spacy.tests.spans.test_merge.test_issue_54(EN)
spacy.tests.spans.test_merge.test_merge_heads(EN)
spacy.tests.spans.test_merge.test_merge_tokens(EN)
spacy.tests.spans.test_merge.test_np_merges(EN)
spacy.tests.spans.test_merge.test_sentence_update_after_merge(EN)
spacy.tests.spans.test_merge.test_subtree_size_check(EN)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/spans/conftest.py----------------------------------------
spacy.tests.spans.conftest.en_nlp()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/serialize/test_huffman.py----------------------------------------
A:spacy.tests.serialize.test_huffman.lo->heappop(heap)
A:spacy.tests.serialize.test_huffman.hi->heappop(heap)
A:spacy.tests.serialize.test_huffman.probs->numpy.zeros(shape=(10,), dtype=numpy.float32)
A:spacy.tests.serialize.test_huffman.codec->HuffmanCodec([(w.orth, numpy.exp(w.prob)) for w in EN.vocab])
A:spacy.tests.serialize.test_huffman.py_codes->list(py_codes.items())
A:spacy.tests.serialize.test_huffman.strings->list(codec.strings)
A:spacy.tests.serialize.test_huffman.bits->HuffmanCodec([(w.orth, numpy.exp(w.prob)) for w in EN.vocab]).encode(message)
A:spacy.tests.serialize.test_huffman.string->''.join(('{0:b}'.format(c).rjust(8, '0')[::-1] for c in bits.as_bytes()))
A:spacy.tests.serialize.test_huffman.symb2freq->defaultdict(int)
A:spacy.tests.serialize.test_huffman.by_freq->list(symb2freq.items())
A:spacy.tests.serialize.test_huffman.py_codec->py_encode(symb2freq)
A:spacy.tests.serialize.test_huffman.my_lengths->defaultdict(int)
A:spacy.tests.serialize.test_huffman.py_lengths->defaultdict(int)
A:spacy.tests.serialize.test_huffman.my_exp_len->sum((length * weight for (length, weight) in my_lengths.items()))
A:spacy.tests.serialize.test_huffman.py_exp_len->sum((length * weight for (length, weight) in py_lengths.items()))
spacy.tests.serialize.test_huffman.py_encode(symb2freq)
spacy.tests.serialize.test_huffman.test1()
spacy.tests.serialize.test_huffman.test_rosetta()
spacy.tests.serialize.test_huffman.test_round_trip()
spacy.tests.serialize.test_huffman.test_vocab(EN)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/serialize/test_codecs.py----------------------------------------
A:spacy.tests.serialize.test_codecs.codec->HuffmanCodec([(lex.orth, lex.prob) for lex in vocab])
A:spacy.tests.serialize.test_codecs.bits->BitArray()
A:spacy.tests.serialize.test_codecs.msg->numpy.array(ids, dtype=numpy.int32)
A:spacy.tests.serialize.test_codecs.result->numpy.array(range(len(msg)), dtype=numpy.int32)
A:spacy.tests.serialize.test_codecs.msg_list->list(msg)
A:spacy.tests.serialize.test_codecs.vocab->Vocab()
spacy.tests.serialize.test_codecs.test_attribute()
spacy.tests.serialize.test_codecs.test_binary()
spacy.tests.serialize.test_codecs.test_vocab_codec()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/serialize/test_io.py----------------------------------------
A:spacy.tests.serialize.test_io.doc1->EN(u'This is a simple test. With a couple of sentences.')
A:spacy.tests.serialize.test_io.doc2->EN(u'This is another test document.')
A:spacy.tests.serialize.test_io.tmp_dir->tempfile.mkdtemp()
A:spacy.tests.serialize.test_io.(bytes1, bytes2)->spacy.tokens.Doc.read_bytes(file_)
A:spacy.tests.serialize.test_io.r1->Doc(EN.vocab).from_bytes(bytes1)
A:spacy.tests.serialize.test_io.r2->Doc(EN.vocab).from_bytes(bytes2)
A:spacy.tests.serialize.test_io.orig->EN(u'The geese are flying')
A:spacy.tests.serialize.test_io.result->Doc(orig.vocab).from_bytes(orig.to_bytes())
spacy.tests.serialize.test_io.test_left_right(EN)
spacy.tests.serialize.test_io.test_lemmas(EN)
spacy.tests.serialize.test_io.test_read_write(EN)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/serialize/test_packer.py----------------------------------------
A:spacy.tests.serialize.test_packer.data_dir->os.environ.get('SPACY_DATA')
A:spacy.tests.serialize.test_packer.package->spacy.util.get_package(data_dir)
A:spacy.tests.serialize.test_packer.vocab->spacy.en.English.default_vocab(package=package)
A:spacy.tests.serialize.test_packer.null_re->re.compile('!!!!!!!!!')
A:spacy.tests.serialize.test_packer.tokenizer->Tokenizer(vocab, {}, null_re, null_re, null_re)
A:spacy.tests.serialize.test_packer.packer->Packer(tokenizer.vocab, [])
A:spacy.tests.serialize.test_packer.bits->Packer(tokenizer.vocab, []).pack(doc)
A:spacy.tests.serialize.test_packer.byte_str->bytearray(b'the dog jumped')
A:spacy.tests.serialize.test_packer.msg->tokenizer(u'the dog jumped')
A:spacy.tests.serialize.test_packer.result->Doc(EN.vocab).from_bytes(byte_string)
A:spacy.tests.serialize.test_packer.doc->EN(string)
A:spacy.tests.serialize.test_packer.byte_string->EN(string).to_bytes()
spacy.tests.serialize.test_packer.test_char_packer(vocab)
spacy.tests.serialize.test_packer.test_packer_annotated(tokenizer)
spacy.tests.serialize.test_packer.test_packer_bad_chars(EN)
spacy.tests.serialize.test_packer.test_packer_bad_chars(tokenizer)
spacy.tests.serialize.test_packer.test_packer_unannotated(tokenizer)
spacy.tests.serialize.test_packer.tokenizer(vocab)
spacy.tests.serialize.test_packer.vocab()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/matcher/test_matcher_bugfixes.py----------------------------------------
A:spacy.tests.matcher.test_matcher_bugfixes.doc->EN(u'get me a flight from SFO to LAX leaving 20 December and arriving on January 5th')
A:spacy.tests.matcher.test_matcher_bugfixes.matcher->Matcher(EN.vocab, {'BostonCeltics': ('ORG', {}, [[{LOWER: 'boston'}, {LOWER: 'celtics'}], [{LOWER: 'boston'}]])})
A:spacy.tests.matcher.test_matcher_bugfixes.matches->matcher(doc)
A:spacy.tests.matcher.test_matcher_bugfixes.ents->list(doc.ents)
spacy.tests.matcher.test_matcher_bugfixes.test_ner_interaction(EN)
spacy.tests.matcher.test_matcher_bugfixes.test_overlap_issue118(EN)
spacy.tests.matcher.test_matcher_bugfixes.test_overlap_prefix(EN)
spacy.tests.matcher.test_matcher_bugfixes.test_overlap_prefix_reorder(EN)
spacy.tests.matcher.test_matcher_bugfixes.test_overlap_reorder(EN)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/tokens/test_array.py----------------------------------------
A:spacy.tests.tokens.test_array.tokens->EN(text)
A:spacy.tests.tokens.test_array.feats_array->EN(text).to_array((attrs.ORTH, attrs.DEP))
spacy.tests.tokens.test_array.test_attr_of_token(EN)
spacy.tests.tokens.test_array.test_dep(EN)
spacy.tests.tokens.test_array.test_tag(EN)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/tokens/test_tokens_api.py----------------------------------------
A:spacy.tests.tokens.test_tokens_api.tokens->English(parser=False).tokenizer(u'I use goggle chrone to surf the web')
A:spacy.tests.tokens.test_tokens_api.packed->English(parser=False).tokenizer(u'I use goggle chrone to surf the web').to_bytes()
A:spacy.tests.tokens.test_tokens_api.new_tokens->Doc(EN.vocab).from_bytes(packed)
A:spacy.tests.tokens.test_tokens_api.doc->EN(text, tag=True)
A:spacy.tests.tokens.test_tokens_api.EN->English(parser=False)
A:spacy.tests.tokens.test_tokens_api.heads->numpy.asarray([[0, 3, -1, -2, -4]], dtype='int32')
spacy.tests.tokens.test_tokens_api.test_getitem(EN)
spacy.tests.tokens.test_tokens_api.test_merge(EN)
spacy.tests.tokens.test_tokens_api.test_merge_children(EN)
spacy.tests.tokens.test_tokens_api.test_merge_end_string(EN)
spacy.tests.tokens.test_tokens_api.test_merge_hang()
spacy.tests.tokens.test_tokens_api.test_serialize(EN)
spacy.tests.tokens.test_tokens_api.test_serialize_whitespace(EN)
spacy.tests.tokens.test_tokens_api.test_set_ents(EN)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/tokens/test_vec.py----------------------------------------
spacy.tests.tokens.test_vec.test_capitalized(EN)
spacy.tests.tokens.test_vec.test_vec(EN)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/tokens/test_noun_chunks.py----------------------------------------
A:spacy.tests.tokens.test_noun_chunks.nlp->English(parser=False)
A:spacy.tests.tokens.test_noun_chunks.sent->u'Peter has chronic command and control issues'.strip()
A:spacy.tests.tokens.test_noun_chunks.tokens->nlp(sent)
spacy.tests.tokens.test_noun_chunks.test_not_nested()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/tokens/test_token.py----------------------------------------
A:spacy.tests.tokens.test_token.tokens->EN(u'Give it back', parse=False)
spacy.tests.tokens.test_token.test_prob(EN)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/tokens/test_token_api.py----------------------------------------
A:spacy.tests.tokens.test_token_api.tokens->EN(u'foobar')
A:spacy.tests.tokens.test_token_api.(Hi, comma, my, email, is_, addr)->EN(u'Hi, my email is test@me.com')
A:spacy.tests.tokens.test_token_api.(apples, oranges, oov)->EN(u'apples oranges ldskbjlsdkbflzdfbl')
spacy.tests.tokens.test_token_api.test_flags(EN)
spacy.tests.tokens.test_token_api.test_is_properties(EN)
spacy.tests.tokens.test_token_api.test_single_token_string(EN)
spacy.tests.tokens.test_token_api.test_strings(EN)
spacy.tests.tokens.test_token_api.test_vectors(EN)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/munge/test_lev_align.py----------------------------------------
spacy.tests.munge.test_lev_align.test_align()
spacy.tests.munge.test_lev_align.test_edit_path()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/munge/test_align.py----------------------------------------
A:spacy.tests.munge.test_align.aligned->list(align_tokens(ref, indices))
spacy.tests.munge.test_align.test_align_continue()
spacy.tests.munge.test_align.test_hyphen_align()
spacy.tests.munge.test_align.test_perfect_align()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/munge/test_bad_periods.py----------------------------------------
A:spacy.tests.munge.test_bad_periods.hongbin_example->'\n1       2.      0.      LS      _       24      meta    _       _       _\n2       .       .       .       _       1       punct   _       _       _\n3       Wang    wang    NNP     _       4       compound        _       _       _\n4       Hongbin hongbin NNP     _       16      nsubj   _       _       _\n5       ,       ,       ,       _       4       punct   _       _       _\n6       the     the     DT      _       11      det     _       _       _\n7       "       "       ``      _       11      punct   _       _       _\n8       communist       communist       JJ      _       11      amod    _       _       _\n9       trail   trail   NN      _       11      compound        _       _       _\n10      -       -       HYPH    _       11      punct   _       _       _\n11      blazer  blazer  NN      _       4       appos   _       _       _\n12      ,       ,       ,       _       16      punct   _       _       _\n13      "       "       \'\'      _       16      punct   _       _       _\n14      has     have    VBZ     _       16      aux     _       _       _\n15      not     not     RB      _       16      neg     _       _       _\n16      turned  turn    VBN     _       24      ccomp   _       _       _\n17      into    into    IN      syn=CLR 16      prep    _       _       _\n18      a       a       DT      _       19      det     _       _       _\n19      capitalist      capitalist      NN      _       17      pobj    _       _       _\n20      (       (       -LRB-   _       24      punct   _       _       _\n21      he      he      PRP     _       24      nsubj   _       _       _\n22      does    do      VBZ     _       24      aux     _       _       _\n23      n\'t     not     RB      _       24      neg     _       _       _\n24      have    have    VB      _       0       root    _       _       _\n25      any     any     DT      _       26      det     _       _       _\n26      shares  share   NNS     _       24      dobj    _       _       _\n27      ,       ,       ,       _       24      punct   _       _       _\n28      does    do      VBZ     _       30      aux     _       _       _\n29      n\'t     not     RB      _       30      neg     _       _       _\n30      have    have    VB      _       24      conj    _       _       _\n31      any     any     DT      _       32      det     _       _       _\n32      savings saving  NNS     _       30      dobj    _       _       _\n33      ,       ,       ,       _       30      punct   _       _       _\n34      does    do      VBZ     _       36      aux     _       _       _\n35      n\'t     not     RB      _       36      neg     _       _       _\n36      have    have    VB      _       30      conj    _       _       _\n37      his     his     PRP$    _       39      poss    _       _       _\n38      own     own     JJ      _       39      amod    _       _       _\n39      car     car     NN      _       36      dobj    _       _       _\n40      ,       ,       ,       _       36      punct   _       _       _\n41      and     and     CC      _       36      cc      _       _       _\n42      does    do      VBZ     _       44      aux     _       _       _\n43      n\'t     not     RB      _       44      neg     _       _       _\n44      have    have    VB      _       36      conj    _       _       _\n45      a       a       DT      _       46      det     _       _       _\n46      mansion mansion NN      _       44      dobj    _       _       _\n47      ;       ;       .       _       24      punct   _       _       _\n'.strip()
A:spacy.tests.munge.test_bad_periods.(words, annot)->spacy.munge.read_conll.parse(hongbin_example, strip_bad_periods=True)
spacy.tests.munge.test_bad_periods.test_hongbin()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/munge/test_onto_ner.py----------------------------------------
spacy.tests.munge.test_onto_ner.test_get_tag()
spacy.tests.munge.test_onto_ner.test_get_text()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/munge/test_detokenize.py----------------------------------------
A:spacy.tests.munge.test_detokenize.tokens->"I ca n't !".split()
spacy.tests.munge.test_detokenize.test_contractions()
spacy.tests.munge.test_detokenize.test_contractions_punct()
spacy.tests.munge.test_detokenize.test_punct()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/vocab/test_intern.py----------------------------------------
A:spacy.tests.vocab.test_intern.string_file->io.BytesIO()
A:spacy.tests.vocab.test_intern.loaded->pickle.load(string_file)
A:spacy.tests.vocab.test_intern.new_store->StringStore()
spacy.tests.vocab.test_intern.sstore()
spacy.tests.vocab.test_intern.test_254_string(sstore)
spacy.tests.vocab.test_intern.test_255_string(sstore)
spacy.tests.vocab.test_intern.test_256_string(sstore)
spacy.tests.vocab.test_intern.test_dump_load(sstore)
spacy.tests.vocab.test_intern.test_long_string(sstore)
spacy.tests.vocab.test_intern.test_massive_strings(sstore)
spacy.tests.vocab.test_intern.test_med_string(sstore)
spacy.tests.vocab.test_intern.test_pickle_string_store(sstore)
spacy.tests.vocab.test_intern.test_retrieve_id(sstore)
spacy.tests.vocab.test_intern.test_save_bytes(sstore)
spacy.tests.vocab.test_intern.test_save_unicode(sstore)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/vocab/test_flag_features.py----------------------------------------
spacy.tests.vocab.test_flag_features.test_is_alpha(words)
spacy.tests.vocab.test_flag_features.test_is_digit(words)
spacy.tests.vocab.test_flag_features.words()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/vocab/test_shape.py----------------------------------------
spacy.tests.vocab.test_shape.test_capitalized()
spacy.tests.vocab.test_shape.test_digits()
spacy.tests.vocab.test_shape.test_mix()
spacy.tests.vocab.test_shape.test_punct()
spacy.tests.vocab.test_shape.test_punct_seq()
spacy.tests.vocab.test_shape.test_space()
spacy.tests.vocab.test_shape.test_truncate()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/vocab/test_vocab.py----------------------------------------
A:spacy.tests.vocab.test_vocab.file_->io.BytesIO()
A:spacy.tests.vocab.test_vocab.loaded->pickle.load(file_)
spacy.tests.vocab.test_vocab.test_case_neq(en_vocab)
spacy.tests.vocab.test_vocab.test_eq(en_vocab)
spacy.tests.vocab.test_vocab.test_neq(en_vocab)
spacy.tests.vocab.test_vocab.test_pickle_vocab(en_vocab)
spacy.tests.vocab.test_vocab.test_pickle_vocab_vectors(en_vocab)
spacy.tests.vocab.test_vocab.test_punct_neq(en_vocab)
spacy.tests.vocab.test_vocab.test_shape_attr(en_vocab)
spacy.tests.vocab.test_vocab.test_symbols(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/vocab/test_number.py----------------------------------------
spacy.tests.vocab.test_number.test_comma()
spacy.tests.vocab.test_number.test_digits()
spacy.tests.vocab.test_number.test_fraction()
spacy.tests.vocab.test_number.test_not_number()
spacy.tests.vocab.test_number.test_period()
spacy.tests.vocab.test_number.test_word()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/vocab/test_lexeme_flags.py----------------------------------------
spacy.tests.vocab.test_lexeme_flags.test_is_alpha(en_vocab)
spacy.tests.vocab.test_lexeme_flags.test_is_digit(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/vocab/test_urlish.py----------------------------------------
spacy.tests.vocab.test_urlish.test_basic_url()
spacy.tests.vocab.test_urlish.test_close_enough()
spacy.tests.vocab.test_urlish.test_non_match()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/vocab/test_is_punct.py----------------------------------------
spacy.tests.vocab.test_is_punct.test_comma()
spacy.tests.vocab.test_is_punct.test_letter()
spacy.tests.vocab.test_is_punct.test_space()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/vocab/conftest.py----------------------------------------
spacy.tests.vocab.conftest.en_vocab(EN)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/vocab/test_asciify.py----------------------------------------
spacy.tests.vocab.test_asciify.test_smart_quote()
spacy.tests.vocab.test_asciify.test_tilde()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/tokenizer/test_surround_punct.py----------------------------------------
A:spacy.tests.tokenizer.test_surround_punct.tokens->en_tokenizer(string)
spacy.tests.tokenizer.test_surround_punct.paired_puncts()
spacy.tests.tokenizer.test_surround_punct.test_token(paired_puncts,en_tokenizer)
spacy.tests.tokenizer.test_surround_punct.test_two_different(paired_puncts,en_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/tokenizer/test_whitespace.py----------------------------------------
A:spacy.tests.tokenizer.test_whitespace.tokens->en_tokenizer('hello \n possums')
spacy.tests.tokenizer.test_whitespace.test_double_space(en_tokenizer)
spacy.tests.tokenizer.test_whitespace.test_newline(en_tokenizer)
spacy.tests.tokenizer.test_whitespace.test_newline_double_space(en_tokenizer)
spacy.tests.tokenizer.test_whitespace.test_newline_space(en_tokenizer)
spacy.tests.tokenizer.test_whitespace.test_newline_space_wrap(en_tokenizer)
spacy.tests.tokenizer.test_whitespace.test_single_space(en_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/tokenizer/test_pre_punct.py----------------------------------------
A:spacy.tests.tokenizer.test_pre_punct.tokens->en_tokenizer(string)
spacy.tests.tokenizer.test_pre_punct.open_puncts()
spacy.tests.tokenizer.test_pre_punct.test_open(open_puncts,en_tokenizer)
spacy.tests.tokenizer.test_pre_punct.test_open_appostrophe(en_tokenizer)
spacy.tests.tokenizer.test_pre_punct.test_three_same_open(open_puncts,en_tokenizer)
spacy.tests.tokenizer.test_pre_punct.test_two_different_open(open_puncts,en_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/tokenizer/test_tokens_from_list.py----------------------------------------
A:spacy.tests.tokenizer.test_tokens_from_list.tokens->en_tokenizer.tokens_from_list(words)
spacy.tests.tokenizer.test_tokens_from_list.test1(en_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/tokenizer/test_indices.py----------------------------------------
A:spacy.tests.tokenizer.test_indices.tokens->en_tokenizer(text)
spacy.tests.tokenizer.test_indices.test_complex_punct(en_tokenizer)
spacy.tests.tokenizer.test_indices.test_simple_punct(en_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/tokenizer/test_string_loading.py----------------------------------------
A:spacy.tests.tokenizer.test_string_loading.tokens->en_tokenizer('Betty Botter bought a pound of butter.')
A:spacy.tests.tokenizer.test_string_loading.tokens2->en_tokenizer('Betty also bought a pound of butter.')
spacy.tests.tokenizer.test_string_loading.test_one(en_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/tokenizer/test_wiki_sun.py----------------------------------------
A:spacy.tests.tokenizer.test_wiki_sun.HERE->os.path.dirname(__file__)
A:spacy.tests.tokenizer.test_wiki_sun.loc->os.path.join(HERE, '..', 'sun.txt')
A:spacy.tests.tokenizer.test_wiki_sun.tokens->en_tokenizer(sun_txt)
spacy.tests.tokenizer.test_wiki_sun.sun_txt()
spacy.tests.tokenizer.test_wiki_sun.test_tokenize(sun_txt,en_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/tokenizer/test_post_punct.py----------------------------------------
A:spacy.tests.tokenizer.test_post_punct.tokens->en_tokenizer(string)
spacy.tests.tokenizer.test_post_punct.close_puncts()
spacy.tests.tokenizer.test_post_punct.test_close(close_puncts,en_tokenizer)
spacy.tests.tokenizer.test_post_punct.test_double_end_quote(en_tokenizer)
spacy.tests.tokenizer.test_post_punct.test_three_same_close(close_puncts,en_tokenizer)
spacy.tests.tokenizer.test_post_punct.test_two_different_close(close_puncts,en_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/tokenizer/test_tokenizer.py----------------------------------------
A:spacy.tests.tokenizer.test_tokenizer.file_->io.BytesIO()
A:spacy.tests.tokenizer.test_tokenizer.loaded->pickle.load(file_)
A:spacy.tests.tokenizer.test_tokenizer.tokens->en_tokenizer("Will this road take me to Puddleton?—No, you'll have to walk there.—Ariel.")
spacy.tests.tokenizer.test_tokenizer.test_bracket_period(en_tokenizer)
spacy.tests.tokenizer.test_tokenizer.test_cnts1(en_tokenizer)
spacy.tests.tokenizer.test_tokenizer.test_cnts2(en_tokenizer)
spacy.tests.tokenizer.test_tokenizer.test_cnts3(en_tokenizer)
spacy.tests.tokenizer.test_tokenizer.test_cnts4(en_tokenizer)
spacy.tests.tokenizer.test_tokenizer.test_cnts5(en_tokenizer)
spacy.tests.tokenizer.test_tokenizer.test_cnts6(en_tokenizer)
spacy.tests.tokenizer.test_tokenizer.test_contraction(en_tokenizer)
spacy.tests.tokenizer.test_tokenizer.test_contraction_punct(en_tokenizer)
spacy.tests.tokenizer.test_tokenizer.test_digits(en_tokenizer)
spacy.tests.tokenizer.test_tokenizer.test_em_dash_infix(en_tokenizer)
spacy.tests.tokenizer.test_tokenizer.test_ie(en_tokenizer)
spacy.tests.tokenizer.test_tokenizer.test_mr(en_tokenizer)
spacy.tests.tokenizer.test_tokenizer.test_no_word(en_tokenizer)
spacy.tests.tokenizer.test_tokenizer.test_pickle(en_tokenizer)
spacy.tests.tokenizer.test_tokenizer.test_punct(en_tokenizer)
spacy.tests.tokenizer.test_tokenizer.test_sample(en_tokenizer)
spacy.tests.tokenizer.test_tokenizer.test_single_word(en_tokenizer)
spacy.tests.tokenizer.test_tokenizer.test_two_whitespace(en_tokenizer)
spacy.tests.tokenizer.test_tokenizer.test_two_words(en_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/tokenizer/test_special_affix.py----------------------------------------
spacy.tests.tokenizer.test_special_affix.test_even_wrap_interact(en_tokenizer)
spacy.tests.tokenizer.test_special_affix.test_no_punct(en_tokenizer)
spacy.tests.tokenizer.test_special_affix.test_no_special(en_tokenizer)
spacy.tests.tokenizer.test_special_affix.test_prefix(en_tokenizer)
spacy.tests.tokenizer.test_special_affix.test_prefix_interact(en_tokenizer)
spacy.tests.tokenizer.test_special_affix.test_suffix(en_tokenizer)
spacy.tests.tokenizer.test_special_affix.test_suffix_interact(en_tokenizer)
spacy.tests.tokenizer.test_special_affix.test_uneven_wrap(en_tokenizer)
spacy.tests.tokenizer.test_special_affix.test_uneven_wrap_interact(en_tokenizer)
spacy.tests.tokenizer.test_special_affix.test_wrap(en_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/tokenizer/test_contractions.py----------------------------------------
A:spacy.tests.tokenizer.test_contractions.tokens->en_tokenizer("there'll")
spacy.tests.tokenizer.test_contractions.test_LL(en_tokenizer)
spacy.tests.tokenizer.test_contractions.test_aint(en_tokenizer)
spacy.tests.tokenizer.test_contractions.test_apostrophe(en_tokenizer)
spacy.tests.tokenizer.test_contractions.test_capitalized(en_tokenizer)
spacy.tests.tokenizer.test_contractions.test_possess(en_tokenizer)
spacy.tests.tokenizer.test_contractions.test_punct(en_tokenizer)
spacy.tests.tokenizer.test_contractions.test_therell(en_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/tokenizer/test_infix.py----------------------------------------
A:spacy.tests.tokenizer.test_infix.tokens->en_tokenizer('hi+there@gmail.it')
spacy.tests.tokenizer.test_infix.test_ellipsis(en_tokenizer)
spacy.tests.tokenizer.test_infix.test_email(en_tokenizer)
spacy.tests.tokenizer.test_infix.test_hyphen(en_tokenizer)
spacy.tests.tokenizer.test_infix.test_numeric_range(en_tokenizer)
spacy.tests.tokenizer.test_infix.test_period(en_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/tokenizer/conftest.py----------------------------------------
spacy.tests.tokenizer.conftest.en_tokenizer(EN)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/tokenizer/test_emoticons.py----------------------------------------
A:spacy.tests.tokenizer.test_emoticons.tokens->en_tokenizer(text)
spacy.tests.tokenizer.test_emoticons.test_false_positive(en_tokenizer)
spacy.tests.tokenizer.test_emoticons.test_tweebo_challenge(en_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/tokenizer/test_only_punct.py----------------------------------------
spacy.tests.tokenizer.test_only_punct.test_only_pre1(en_tokenizer)
spacy.tests.tokenizer.test_only_punct.test_only_pre2(en_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/vectors/test_vectors.py----------------------------------------
A:spacy.tests.vectors.test_vectors.doc->EN(u'apples orange juice')
A:spacy.tests.vectors.test_vectors.(apples, oranges)->EN(u'apples oranges')
A:spacy.tests.vectors.test_vectors.apples->EN(u'apples and apple pie')
A:spacy.tests.vectors.test_vectors.oranges->EN(u'orange juice')
spacy.tests.vectors.test_vectors.test_doc_doc_similarity(EN)
spacy.tests.vectors.test_vectors.test_doc_vector(EN)
spacy.tests.vectors.test_vectors.test_lexeme_doc_similarity(EN)
spacy.tests.vectors.test_vectors.test_lexeme_lexeme_similarity(EN)
spacy.tests.vectors.test_vectors.test_lexeme_span_similarity(EN)
spacy.tests.vectors.test_vectors.test_lexeme_vector(EN)
spacy.tests.vectors.test_vectors.test_span_doc_similarity(EN)
spacy.tests.vectors.test_vectors.test_span_span_similarity(EN)
spacy.tests.vectors.test_vectors.test_span_vector(EN)
spacy.tests.vectors.test_vectors.test_token_doc_similarity(EN)
spacy.tests.vectors.test_vectors.test_token_lexeme_similarity(EN)
spacy.tests.vectors.test_vectors.test_token_span_similarity(EN)
spacy.tests.vectors.test_vectors.test_token_token_similarity(EN)
spacy.tests.vectors.test_vectors.test_token_vector(EN)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/tagger/test_spaces.py----------------------------------------
A:spacy.tests.tagger.test_spaces.tokens->EN(string)
spacy.tests.tagger.test_spaces.tagged(EN)
spacy.tests.tagger.test_spaces.test_return_char(EN)
spacy.tests.tagger.test_spaces.test_spaces(tagged)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/tagger/test_lemmatizer.py----------------------------------------
A:spacy.tests.tagger.test_lemmatizer.data_dir->os.environ.get('SPACY_DATA')
A:spacy.tests.tagger.test_lemmatizer.index->read_index(file_)
A:spacy.tests.tagger.test_lemmatizer.exc->read_exc(file_)
A:spacy.tests.tagger.test_lemmatizer.file_->io.BytesIO()
A:spacy.tests.tagger.test_lemmatizer.loaded->pickle.load(file_)
spacy.tests.tagger.test_lemmatizer.lemmatizer(package)
spacy.tests.tagger.test_lemmatizer.package()
spacy.tests.tagger.test_lemmatizer.test_noun_lemmas(lemmatizer)
spacy.tests.tagger.test_lemmatizer.test_pickle_lemmatizer(lemmatizer)
spacy.tests.tagger.test_lemmatizer.test_read_exc(package)
spacy.tests.tagger.test_lemmatizer.test_read_index(package)
spacy.tests.tagger.test_lemmatizer.test_smart_quotes(lemmatizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/tagger/test_add_lemmas.py----------------------------------------
A:spacy.tests.tagger.test_add_lemmas.tokens->EN(u"I didn't do it")
spacy.tests.tagger.test_add_lemmas.lemmas(tagged)
spacy.tests.tagger.test_add_lemmas.tagged(EN)
spacy.tests.tagger.test_add_lemmas.test_didnt(EN)
spacy.tests.tagger.test_add_lemmas.test_lemmas(lemmas,tagged)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/tagger/test_morph_exceptions.py----------------------------------------
A:spacy.tests.tagger.test_morph_exceptions.nlp->English()
A:spacy.tests.tagger.test_morph_exceptions.tokens->nlp('I like his style.', tag=True, parse=False)
spacy.tests.tagger.test_morph_exceptions.morph_exc()
spacy.tests.tagger.test_morph_exceptions.test_load_exc(morph_exc)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tests/tagger/test_tag_names.py----------------------------------------
A:spacy.tests.tagger.test_tag_names.tokens->EN(u'I ate pizzas with anchovies.', parse=False, tag=True)
spacy.tests.tagger.test_tag_names.test_tag_names(EN)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/serialize/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/tokens/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/munge/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/munge/align_raw.py----------------------------------------
A:spacy.munge.align_raw.line->line.replace(find, replace).replace(find, replace)
A:spacy.munge.align_raw.ptb_sec_dir->Path(ptb_sec_dir)
A:spacy.munge.align_raw.text->file_.read()
A:spacy.munge.align_raw.(words, brackets)->spacy.munge.read_ptb.parse(parse_str, strip_bad_periods=True)
A:spacy.munge.align_raw.string->' '.join(words)
A:spacy.munge.align_raw.tok->tok.replace("'T-", "'T").replace("'T-", "'T")
A:spacy.munge.align_raw.raw_sents->_flatten(raw_by_para)
A:spacy.munge.align_raw.ptb_sents->list(_flatten(ptb_by_file))
A:spacy.munge.align_raw.alignment->align_chars(raw, ptb)
A:spacy.munge.align_raw.length->len(raw)
A:spacy.munge.align_raw.odc_loc->os.path.join(odc_dir, 'wsj%s.txt' % section)
A:spacy.munge.align_raw.ptb_sec->os.path.join(ptb_dir, section)
A:spacy.munge.align_raw.out_loc->os.path.join(out_dir, 'wsj%s.json' % section)
A:spacy.munge.align_raw.aligned->get_alignment(raw_paragraphs, ptb_files)
A:spacy.munge.align_raw.files->align_section(read_odc(odc_loc), read_ptb_sec(ptb_sec_dir))
A:spacy.munge.align_raw.mapping->dict((line.split() for line in open(path.join(onto_dir, 'map.txt')) if len(line.split()) == 2))
A:spacy.munge.align_raw.ptb_loc->os.path.join(onto_dir, annot_fn + '.parse')
A:spacy.munge.align_raw.src_loc->os.path.join(src_dir, src_fn + '.sgm')
A:spacy.munge.align_raw.src_doc->sgml_extract(open(src_loc).read())
A:spacy.munge.align_raw.subdir->os.path.join(*directories)
spacy.munge.align_raw._flatten(nested)
spacy.munge.align_raw._reform_ptb_word(tok)
spacy.munge.align_raw.align_chars(raw,ptb)
spacy.munge.align_raw.align_section(raw_paragraphs,ptb_files)
spacy.munge.align_raw.do_web(src_dir,onto_dir,out_dir)
spacy.munge.align_raw.do_wsj(odc_dir,ptb_dir,out_dir)
spacy.munge.align_raw.get_alignment(raw_by_para,ptb_by_file)
spacy.munge.align_raw.get_sections(odc_dir,ptb_dir,out_dir)
spacy.munge.align_raw.group_into_files(sents)
spacy.munge.align_raw.group_into_paras(sents)
spacy.munge.align_raw.main(odc_dir,onto_dir,out_dir)
spacy.munge.align_raw.may_mkdir(parent,*subdirs)
spacy.munge.align_raw.read_odc(section_loc)
spacy.munge.align_raw.read_ptb_sec(ptb_sec_dir)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/munge/read_ontonotes.py----------------------------------------
A:spacy.munge.read_ontonotes.docid_re->re.compile('<DOCID>([^>]+)</DOCID>')
A:spacy.munge.read_ontonotes.doctype_re->re.compile('<DOCTYPE SOURCE="[^"]+">([^>]+)</DOCTYPE>')
A:spacy.munge.read_ontonotes.datetime_re->re.compile('<DATETIME>([^>]+)</DATETIME>')
A:spacy.munge.read_ontonotes.headline_re->re.compile('<HEADLINE>(.+)</HEADLINE>', re.DOTALL)
A:spacy.munge.read_ontonotes.post_re->re.compile('<POST>(.+)</POST>', re.DOTALL)
A:spacy.munge.read_ontonotes.poster_re->re.compile('<POSTER>(.+)</POSTER>')
A:spacy.munge.read_ontonotes.postdate_re->re.compile('<POSTDATE>(.+)</POSTDATE>')
A:spacy.munge.read_ontonotes.tag_re->re.compile('<[^>]+>[^>]+</[^>]+>')
A:spacy.munge.read_ontonotes.matches->regex.search(text)
spacy.munge.read_ontonotes._get_one(regex,text,required=False)
spacy.munge.read_ontonotes._get_text(data)
spacy.munge.read_ontonotes.sgml_extract(text_data)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/munge/read_conll.py----------------------------------------
A:spacy.munge.read_conll.sent_text->sent_text.strip().strip()
A:spacy.munge.read_conll.(word, tag, head, dep)->_parse_line(line)
A:spacy.munge.read_conll.id_map[i]->len(words)
A:spacy.munge.read_conll.pieces->line.split()
spacy.munge.read_conll._is_bad_period(prev,period)
spacy.munge.read_conll._parse_line(line)
spacy.munge.read_conll.parse(sent_text,strip_bad_periods=False)
spacy.munge.read_conll.split(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/munge/read_ner.py----------------------------------------
A:spacy.munge.read_ner.string->string.replace('<ENAMEXTYPE="CARDINAL"><ENAMEXTYPE="CARDINAL">little</ENAMEX> drain</ENAMEX>', 'little drain').replace('<ENAMEXTYPE="CARDINAL"><ENAMEXTYPE="CARDINAL">little</ENAMEX> drain</ENAMEX>', 'little drain')
A:spacy.munge.read_ner.substr->re.compile('<ENAMEXTYPE="[^"]+">').sub('', substr)
A:spacy.munge.read_ner.(tag, open_tag)->_get_tag(substr, open_tag)
A:spacy.munge.read_ner.tag_re->re.compile('<ENAMEXTYPE="[^"]+">')
A:spacy.munge.read_ner.tags->re.compile('<ENAMEXTYPE="[^"]+">').findall(substr)
A:spacy.munge.read_ner.tok->tok.replace('-AMP-', '&').replace('-AMP-', '&')
spacy.munge.read_ner._fix_inner_entities(substr)
spacy.munge.read_ner._get_tag(substr,tag)
spacy.munge.read_ner._get_text(substr)
spacy.munge.read_ner.parse(string,strip_bad_periods=False)
spacy.munge.read_ner.reform_string(tok)
spacy.munge.read_ner.split(text)
spacy.munge.read_ner.tags_to_entities(tags)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/munge/read_ptb.py----------------------------------------
A:spacy.munge.read_ptb.sent_text->sent_text.replace('((', '( (', 1).replace('((', '( (', 1)
A:spacy.munge.read_ptb.bracketsRE->re.compile('(\\()([^\\s\\)\\(]+)|([^\\s\\)\\(]+)?(\\))')
A:spacy.munge.read_ptb.(open_, label, text, close)->match.groups()
A:spacy.munge.read_ptb.(label, start)->open_brackets.pop()
A:spacy.munge.read_ptb.line->line.rstrip().rstrip()
spacy.munge.read_ptb._is_bad_period(prev,period)
spacy.munge.read_ptb.parse(sent_text,strip_bad_periods=False)
spacy.munge.read_ptb.split(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/syntax/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy0.100.1/lib/python3.6/site-packages/spacy/syntax/util.py----------------------------------------
spacy.syntax.util.Config(self,**kwargs)
spacy.syntax.util.Config.__init__(self,**kwargs)
spacy.syntax.util.Config.get(self,attr,default=None)
spacy.syntax.util.Config.read(cls,model_dir,name)
spacy.syntax.util.Config.write(cls,model_dir,name,**kwargs)

