
----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/pipe_analysis.py----------------------------------------
A:spacy.pipe_analysis.data->dot_to_dict({value: True for value in values})
A:spacy.pipe_analysis.invalid_attrs->', '.join((a for a in values if a.startswith(obj_key)))
A:spacy.pipe_analysis.meta->nlp.get_pipe_meta(name)
A:spacy.pipe_analysis.prev_meta->nlp.get_pipe_meta(prev_name)
A:spacy.pipe_analysis.n_problems->sum((len(p) for p in analysis['problems'].values()))
spacy.pipe_analysis.analyze_pipes(nlp:'Language',*,keys:List[str]=DEFAULT_KEYS)->Dict[str, Dict[str, Union[List[str], Dict]]]
spacy.pipe_analysis.get_attr_info(nlp:'Language',attr:str)->Dict[str, List[str]]
spacy.pipe_analysis.print_pipe_analysis(analysis:Dict[str,Dict[str,Union[List[str],Dict]]],*,keys:List[str]=DEFAULT_KEYS)->None
spacy.pipe_analysis.validate_attrs(values:Iterable[str])->Iterable[str]


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/util.py----------------------------------------
A:spacy.util.logger->logging.getLogger('spacy')
A:spacy.util.logger_stream_handler->logging.StreamHandler()
A:spacy.util.languages->catalogue.create('spacy', 'languages', entry_points=True)
A:spacy.util.architectures->catalogue.create('spacy', 'architectures', entry_points=True)
A:spacy.util.tokenizers->catalogue.create('spacy', 'tokenizers', entry_points=True)
A:spacy.util.lemmatizers->catalogue.create('spacy', 'lemmatizers', entry_points=True)
A:spacy.util.lookups->catalogue.create('spacy', 'lookups', entry_points=True)
A:spacy.util.displacy_colors->catalogue.create('spacy', 'displacy_colors', entry_points=True)
A:spacy.util.misc->catalogue.create('spacy', 'misc', entry_points=True)
A:spacy.util.callbacks->catalogue.create('spacy', 'callbacks', entry_points=True)
A:spacy.util.batchers->catalogue.create('spacy', 'batchers', entry_points=True)
A:spacy.util.readers->catalogue.create('spacy', 'readers', entry_points=True)
A:spacy.util.augmenters->catalogue.create('spacy', 'augmenters', entry_points=True)
A:spacy.util.loggers->catalogue.create('spacy', 'loggers', entry_points=True)
A:spacy.util.scorers->catalogue.create('spacy', 'scorers', entry_points=True)
A:spacy.util._entry_point_factories->catalogue.create('spacy', 'factories', entry_points=True)
A:spacy.util.factories->catalogue.create('spacy', 'internal_factories')
A:spacy.util.models->catalogue.create('spacy', 'models', entry_points=True)
A:spacy.util.cli->catalogue.create('spacy', 'cli', entry_points=True)
A:spacy.util.reg->getattr(cls, registry_name)
A:spacy.util.func->getattr(cls, registry_name).get(func_name)
A:spacy.util.legacy_name->func_name.replace('spacy.', 'spacy-legacy.')
A:spacy.util.func_info->getattr(cls, registry_name).find(func_name)
A:spacy.util.match->find_matching_language(lang)
A:spacy.util.module->importlib.util.module_from_spec(spec)
A:spacy.util.path->path.setdefault(item, value if is_last else {}).setdefault(item, value if is_last else {})
A:spacy.util.file_path->Path(cast(os.PathLike, sys.modules[module.__module__].__file__))
A:spacy.util._DEFAULT_EMPTY_PIPES->SimpleFrozenList()
A:spacy.util.cls->importlib.import_module(name)
A:spacy.util.meta->srsly.read_json(path)
A:spacy.util.overrides->dict_to_dot(config)
A:spacy.util.config->Config(section_order=CONFIG_SECTION_ORDER)
A:spacy.util.nlp->get_lang_class(nlp_config['lang']).from_config(config, vocab=vocab, disable=disable, enable=enable, exclude=exclude, auto_fill=auto_fill, validate=validate, meta=meta)
A:spacy.util.lang_cls->get_lang_class(nlp_config['lang'])
A:spacy.util.result->sorted(result, key=lambda span: span.start)
A:spacy.util.config_path->ensure_path(path)
A:spacy.util.spec->importlib.util.spec_from_file_location(name, str(loc))
A:spacy.util.version->Version(version)
A:spacy.util.has_upper->any((sp.operator in ('<', '<=') for sp in specs))
A:spacy.util.has_lower->any((sp.operator in ('>', '>=') for sp in specs))
A:spacy.util.req->Requirement(requirement)
A:spacy.util.specset->SpecifierSet(constraint)
A:spacy.util.v->Version(version)
A:spacy.util.a->get_minor_version(version_a)
A:spacy.util.b->get_minor_version(version_b)
A:spacy.util.lower_version->get_minor_version(lower_version)
A:spacy.util.warn_msg->errors.Warnings.W094.format(model=f"{meta['lang']}_{meta['name']}", model_version=meta['version'], version=meta['spacy_version'], example=get_minor_version_range(about.__version__))
A:spacy.util.model_path->ensure_path(path)
A:spacy.util.pkg->importlib.import_module(name)
A:spacy.util.cmd_list->split_command(command)
A:spacy.util.cmd_str->' '.join(command)
A:spacy.util.ret->subprocess.run(cmd_list, env=os.environ.copy(), input=stdin, encoding='utf8', check=False, stdout=subprocess.PIPE if capture else None, stderr=subprocess.STDOUT if capture else None)
A:spacy.util.error->subprocess.SubprocessError(message)
A:spacy.util.prev_cwd->pathlib.Path.cwd()
A:spacy.util.current->Path(path).resolve()
A:spacy.util.d->Path(tempfile.mkdtemp())
A:spacy.util.ops->get_current_ops()
A:spacy.util.array->compat.cupy.ndarray(numpy_array.shape, order='C', dtype=numpy_array.dtype)
A:spacy.util.entries->file_.read().split('\n')
A:spacy.util.expression->'|'.join([piece for piece in entries if piece.strip()])
A:spacy.util.exc->expand_exc(exc, "'", 'â€™')
A:spacy.util.described_orth->''.join((attr[ORTH] for attr in token_attrs))
A:spacy.util.fixed->dict(token)
A:spacy.util.fixed[ORTH]->fixed[ORTH].replace(search, replace).replace(search, replace)
A:spacy.util.new_excs->dict(excs)
A:spacy.util.new_key->token_string.replace(search, replace)
A:spacy.util.start->min(length, max(0, start))
A:spacy.util.stop->min(length, max(start, stop))
A:spacy.util.sorted_spans->sorted(spans, key=get_sort_key, reverse=True)
A:spacy.util.serialized[key]->getter()
A:spacy.util.text->text.replace('"', '&quot;').replace('"', '&quot;')
A:spacy.util.word_start->text[text_pos:].index(word)
A:spacy.util.parts->section.split('.')
A:spacy.util.argspec->inspect.getfullargspec(func)
A:spacy.util.weight_sum->sum([v if v else 0.0 for v in result.values()])
A:spacy.util.result[key]->round(value / weight_sum, 2)
A:spacy.util.size_->itertools.repeat(size)
A:spacy.util.items->iter(items)
A:spacy.util.batch_size->next(size_)
A:spacy.util.batch->list(itertools.islice(items, int(batch_size)))
A:spacy.util.value->os.environ.get(env_var, False)
A:spacy.util.kwargs->dict(kwargs)
A:spacy.util.error_handler->proc.get_error_handler()
A:spacy.util.doc->proc(doc, **kwargs)
A:spacy.util.lexeme_norms->vocab.lookups.get_table('lexeme_norm', {})
A:spacy.util.langs->', '.join(LEXEME_NORM_LANGS)
A:spacy.util.pkg_to_dist->defaultdict(list)
A:spacy.util.g->itertools.groupby(iterable)
A:spacy.util.s->socket.socket(socket.AF_INET, socket.SOCK_STREAM)
spacy.registry(thinc.registry)
spacy.util.DummyTokenizer(self,text)
spacy.util.DummyTokenizer.from_bytes(self,data:bytes,**kwargs)->'DummyTokenizer'
spacy.util.DummyTokenizer.from_disk(self,path:Union[str,Path],**kwargs)->'DummyTokenizer'
spacy.util.DummyTokenizer.pipe(self,texts,**kwargs)
spacy.util.DummyTokenizer.to_bytes(self,**kwargs)
spacy.util.DummyTokenizer.to_disk(self,path:Union[str,Path],**kwargs)->None
spacy.util.ENV_VARS
spacy.util.SimpleFrozenDict(self,*args,error:str=Errors.E095,**kwargs)
spacy.util.SimpleFrozenDict.__setitem__(self,key,value)
spacy.util.SimpleFrozenDict.pop(self,key,default=None)
spacy.util.SimpleFrozenDict.update(self,other)
spacy.util.SimpleFrozenList(self,*args,error:str=Errors.E927)
spacy.util.SimpleFrozenList.append(self,*args,**kwargs)
spacy.util.SimpleFrozenList.clear(self,*args,**kwargs)
spacy.util.SimpleFrozenList.extend(self,*args,**kwargs)
spacy.util.SimpleFrozenList.insert(self,*args,**kwargs)
spacy.util.SimpleFrozenList.pop(self,*args,**kwargs)
spacy.util.SimpleFrozenList.remove(self,*args,**kwargs)
spacy.util.SimpleFrozenList.reverse(self,*args,**kwargs)
spacy.util.SimpleFrozenList.sort(self,*args,**kwargs)
spacy.util._get_attr_unless_lookup(default_func:Callable[[str],Any],lookups:Dict[str,Any],string:str)->Any
spacy.util._is_port_in_use(port:int,host:str='localhost')->bool
spacy.util._pipe(docs:Iterable['Doc'],proc:'PipeCallable',name:str,default_error_handler:Callable[[str,'PipeCallable',List['Doc'],Exception],NoReturn],kwargs:Mapping[str,Any])->Iterator['Doc']
spacy.util.add_lookups(default_func:Callable[[str],Any],*lookups)->Callable[[str], Any]
spacy.util.all_equal(iterable)
spacy.util.check_bool_env_var(env_var:str)->bool
spacy.util.check_lexeme_norms(vocab,component_name)
spacy.util.combine_score_weights(weights:List[Dict[str,Optional[float]]],overrides:Dict[str,Optional[float]]=SimpleFrozenDict())->Dict[str, Optional[float]]
spacy.util.compile_infix_regex(entries:Iterable[Union[str,Pattern]])->Pattern
spacy.util.compile_prefix_regex(entries:Iterable[Union[str,Pattern]])->Pattern
spacy.util.compile_suffix_regex(entries:Iterable[Union[str,Pattern]])->Pattern
spacy.util.copy_config(config:Union[Dict[str,Any],Config])->Config
spacy.util.create_default_optimizer()->Optimizer
spacy.util.dict_to_dot(obj:Dict[str,dict])->Dict[str, Any]
spacy.util.dot_to_dict(values:Dict[str,Any])->Dict[str, dict]
spacy.util.dot_to_object(config:Config,section:str)
spacy.util.ensure_path(path:Any)->Any
spacy.util.escape_html(text:str)->str
spacy.util.expand_exc(excs:Dict[str,List[dict]],search:str,replace:str)->Dict[str, List[dict]]
spacy.util.filter_chain_spans(*spans:Iterable['Span'])->List['Span']
spacy.util.filter_spans(spans:Iterable['Span'])->List['Span']
spacy.util.find_available_port(start:int,host:str,auto_select:bool=False)->int
spacy.util.find_matching_language(lang:str)->Optional[str]
spacy.util.from_bytes(bytes_data:bytes,setters:Dict[str,Callable[[bytes],Any]],exclude:Iterable[str])->None
spacy.util.from_dict(msg:Dict[str,Any],setters:Dict[str,Callable[[Any],Any]],exclude:Iterable[str])->Dict[str, Any]
spacy.util.from_disk(path:Union[str,Path],readers:Dict[str,Callable[[Path],None]],exclude:Iterable[str])->Path
spacy.util.get_arg_names(func:Callable)->List[str]
spacy.util.get_async(stream,numpy_array)
spacy.util.get_base_version(version:str)->str
spacy.util.get_cuda_stream(require:bool=False,non_blocking:bool=True)->Optional[CudaStream]
spacy.util.get_installed_models()->List[str]
spacy.util.get_lang_class(lang:str)->Type['Language']
spacy.util.get_minor_version(version:str)->Optional[str]
spacy.util.get_minor_version_range(version:str)->str
spacy.util.get_model_lower_version(constraint:str)->Optional[str]
spacy.util.get_model_meta(path:Union[str,Path])->Dict[str, Any]
spacy.util.get_module_path(module:ModuleType)->Path
spacy.util.get_object_name(obj:Any)->str
spacy.util.get_package_path(name:str)->Path
spacy.util.get_package_version(name:str)->Optional[str]
spacy.util.get_sourced_components(config:Union[Dict[str,Any],Config])->Dict[str, Dict[str, Any]]
spacy.util.get_words_and_spaces(words:Iterable[str],text:str)->Tuple[List[str], List[bool]]
spacy.util.ignore_error(proc_name,proc,docs,e)
spacy.util.import_file(name:str,loc:Union[str,Path])->ModuleType
spacy.util.is_compatible_version(version:str,constraint:str,prereleases:bool=True)->Optional[bool]
spacy.util.is_cwd(path:Union[Path,str])->bool
spacy.util.is_cython_func(func:Callable)->bool
spacy.util.is_in_jupyter()->bool
spacy.util.is_minor_version_match(version_a:str,version_b:str)->bool
spacy.util.is_package(name:str)->bool
spacy.util.is_prerelease_version(version:str)->bool
spacy.util.is_same_func(func1:Callable,func2:Callable)->bool
spacy.util.is_unconstrained_version(constraint:str,prereleases:bool=True)->Optional[bool]
spacy.util.join_command(command:List[str])->str
spacy.util.lang_class_is_loaded(lang:str)->bool
spacy.util.load_config(path:Union[str,Path],overrides:Dict[str,Any]=SimpleFrozenDict(),interpolate:bool=False)->Config
spacy.util.load_config_from_str(text:str,overrides:Dict[str,Any]=SimpleFrozenDict(),interpolate:bool=False)
spacy.util.load_language_data(path:Union[str,Path])->Union[dict, list]
spacy.util.load_meta(path:Union[str,Path])->Dict[str, Any]
spacy.util.load_model(name:Union[str,Path],*,vocab:Union['Vocab',bool]=True,disable:Union[str,Iterable[str]]=_DEFAULT_EMPTY_PIPES,enable:Union[str,Iterable[str]]=_DEFAULT_EMPTY_PIPES,exclude:Union[str,Iterable[str]]=_DEFAULT_EMPTY_PIPES,config:Union[Dict[str,Any],Config]=SimpleFrozenDict())->'Language'
spacy.util.load_model_from_config(config:Union[Dict[str,Any],Config],*,meta:Dict[str,Any]=SimpleFrozenDict(),vocab:Union['Vocab',bool]=True,disable:Union[str,Iterable[str]]=_DEFAULT_EMPTY_PIPES,enable:Union[str,Iterable[str]]=_DEFAULT_EMPTY_PIPES,exclude:Union[str,Iterable[str]]=_DEFAULT_EMPTY_PIPES,auto_fill:bool=False,validate:bool=True)->'Language'
spacy.util.load_model_from_init_py(init_file:Union[Path,str],*,vocab:Union['Vocab',bool]=True,disable:Union[str,Iterable[str]]=_DEFAULT_EMPTY_PIPES,enable:Union[str,Iterable[str]]=_DEFAULT_EMPTY_PIPES,exclude:Union[str,Iterable[str]]=_DEFAULT_EMPTY_PIPES,config:Union[Dict[str,Any],Config]=SimpleFrozenDict())->'Language'
spacy.util.load_model_from_package(name:str,*,vocab:Union['Vocab',bool]=True,disable:Union[str,Iterable[str]]=_DEFAULT_EMPTY_PIPES,enable:Union[str,Iterable[str]]=_DEFAULT_EMPTY_PIPES,exclude:Union[str,Iterable[str]]=_DEFAULT_EMPTY_PIPES,config:Union[Dict[str,Any],Config]=SimpleFrozenDict())->'Language'
spacy.util.load_model_from_path(model_path:Path,*,meta:Optional[Dict[str,Any]]=None,vocab:Union['Vocab',bool]=True,disable:Union[str,Iterable[str]]=_DEFAULT_EMPTY_PIPES,enable:Union[str,Iterable[str]]=_DEFAULT_EMPTY_PIPES,exclude:Union[str,Iterable[str]]=_DEFAULT_EMPTY_PIPES,config:Union[Dict[str,Any],Config]=SimpleFrozenDict())->'Language'
spacy.util.make_first_longest_spans_filter()
spacy.util.make_tempdir()->Generator[Path, None, None]
spacy.util.minibatch(items,size)
spacy.util.minify_html(html:str)->str
spacy.util.normalize_slice(length:int,start:int,stop:int,step:Optional[int]=None)->Tuple[int, int]
spacy.util.packages_distributions()->Dict[str, List[str]]
spacy.util.raise_error(proc_name,proc,docs,e)
spacy.util.read_regex(path:Union[str,Path])->Pattern
spacy.util.registry(thinc.registry)
spacy.util.registry.find(cls,registry_name:str,func_name:str)->Callable
spacy.util.registry.get(cls,registry_name:str,func_name:str)->Callable
spacy.util.registry.get_registry_names(cls)->List[str]
spacy.util.registry.has(cls,registry_name:str,func_name:str)->bool
spacy.util.replace_model_node(model:Model,target:Model,replacement:Model)->None
spacy.util.resolve_dot_names(config:Config,dot_names:List[Optional[str]])->Tuple[Any, ...]
spacy.util.run_command(command:Union[str,List[str]],*,stdin:Optional[Any]=None,capture:bool=False)->subprocess.CompletedProcess
spacy.util.set_dot_to_object(config:Config,section:str,value:Any)->None
spacy.util.set_lang_class(name:str,cls:Type['Language'])->None
spacy.util.split_command(command:str)->List[str]
spacy.util.split_requirement(requirement:str)->Tuple[str, str]
spacy.util.to_bytes(getters:Dict[str,Callable[[],bytes]],exclude:Iterable[str])->bytes
spacy.util.to_dict(getters:Dict[str,Callable[[],Any]],exclude:Iterable[str])->Dict[str, Any]
spacy.util.to_disk(path:Union[str,Path],writers:Dict[str,Callable[[Path],None]],exclude:Iterable[str])->Path
spacy.util.to_ternary_int(val)->int
spacy.util.update_exc(base_exceptions:Dict[str,List[dict]],*addition_dicts)->Dict[str, List[dict]]
spacy.util.walk_dict(node:Dict[str,Any],parent:List[str]=[])->Iterator[Tuple[List[str], Any]]
spacy.util.warn_if_jupyter_cupy()
spacy.util.working_dir(path:Union[str,Path])->Iterator[Path]


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lookups.py----------------------------------------
A:spacy.lookups.UNSET->object()
A:spacy.lookups.lookups->Lookups()
A:spacy.lookups.data->file_.read()
A:spacy.lookups.language_data->load_language_data(data[table])
A:spacy.lookups.self->cls(name=name)
A:spacy.lookups.self.bloom->BloomFilter().from_bytes(loaded['bloom'])
A:spacy.lookups.key->get_string_id(key)
A:spacy.lookups.loaded->srsly.msgpack_loads(bytes_data)
A:spacy.lookups.table->Table(name=name, data=data)
A:spacy.lookups.self._tables[key]->Table(key, value)
A:spacy.lookups.path->ensure_path(path)
spacy.lookups.Lookups(self)
spacy.lookups.Lookups.__contains__(self,name:str)->bool
spacy.lookups.Lookups.__len__(self)->int
spacy.lookups.Lookups.add_table(self,name:str,data:dict=SimpleFrozenDict())->Table
spacy.lookups.Lookups.from_bytes(self,bytes_data:bytes,**kwargs)->'Lookups'
spacy.lookups.Lookups.from_disk(self,path:Union[str,Path],filename:str='lookups.bin',**kwargs)->'Lookups'
spacy.lookups.Lookups.get_table(self,name:str,default:Any=UNSET)->Table
spacy.lookups.Lookups.has_table(self,name:str)->bool
spacy.lookups.Lookups.remove_table(self,name:str)->Table
spacy.lookups.Lookups.set_table(self,name:str,table:Table)->None
spacy.lookups.Lookups.tables(self)->List[str]
spacy.lookups.Lookups.to_bytes(self,**kwargs)->bytes
spacy.lookups.Lookups.to_disk(self,path:Union[str,Path],filename:str='lookups.bin',**kwargs)->None
spacy.lookups.Table(self,name:Optional[str]=None,data:Optional[dict]=None)
spacy.lookups.Table.__contains__(self,key:Union[str,int])->bool
spacy.lookups.Table.__getitem__(self,key:Union[str,int])->Any
spacy.lookups.Table.__setitem__(self,key:Union[str,int],value:Any)->None
spacy.lookups.Table.from_bytes(self,bytes_data:bytes)->'Table'
spacy.lookups.Table.from_dict(cls,data:dict,name:Optional[str]=None)->'Table'
spacy.lookups.Table.get(self,key:Union[str,int],default:Optional[Any]=None)->Any
spacy.lookups.Table.set(self,key:Union[str,int],value:Any)->None
spacy.lookups.Table.to_bytes(self)->bytes
spacy.lookups.load_lookups(lang:str,tables:List[str],strict:bool=True)->'Lookups'


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/about.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lexeme.pyi----------------------------------------
spacy.lexeme.Lexeme(self,vocab:Vocab,orth:int)
spacy.lexeme.Lexeme.__hash__(self)->int
spacy.lexeme.Lexeme.__richcmp__(self,other:Lexeme,op:int)->bool
spacy.lexeme.Lexeme.check_flag(self,flag_id:int)->bool
spacy.lexeme.Lexeme.has_vector(self)->bool
spacy.lexeme.Lexeme.is_oov(self)->bool
spacy.lexeme.Lexeme.orth_(self)->str
spacy.lexeme.Lexeme.set_attrs(self,**attrs:Any)->None
spacy.lexeme.Lexeme.set_flag(self,flag_id:int,value:bool)->None
spacy.lexeme.Lexeme.similarity(self,other:Union[Doc,Span,Token,Lexeme])->float
spacy.lexeme.Lexeme.text(self)->str
spacy.lexeme.Lexeme.vector_norm(self)->float


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/glossary.py----------------------------------------
spacy.explain(term)
spacy.glossary.explain(term)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/errors.py----------------------------------------
A:spacy.errors.msg->super().__getattribute__(code)
A:spacy.errors.pattern_errors->'\n'.join([f'- {e}' for e in error_msgs])
spacy.Errors(metaclass=ErrorsWithCodes)
spacy.errors.Errors(metaclass=ErrorsWithCodes)
spacy.errors.ErrorsWithCodes(type)
spacy.errors.ErrorsWithCodes.__getattribute__(self,code)
spacy.errors.MatchPatternError(self,key,errors)
spacy.errors.Warnings(metaclass=ErrorsWithCodes)
spacy.errors._escape_warning_msg(msg)
spacy.errors.filter_warning(action:Literal['default','error','ignore','always','module','once'],error_msg:str)
spacy.errors.setup_default_warnings()
spacy.setup_default_warnings()


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/ty.py----------------------------------------
spacy.ty.InitializableComponent(Protocol)
spacy.ty.InitializableComponent.initialize(self,get_examples:Callable[[],Iterable['Example']],nlp:Iterable['Example'],**kwargs:Any)
spacy.ty.ListenedToComponent(Protocol)
spacy.ty.ListenedToComponent.add_listener(self,listener:Model,component_name:str)->None
spacy.ty.ListenedToComponent.find_listeners(self,component)->None
spacy.ty.ListenedToComponent.remove_listener(self,listener:Model,component_name:str)->bool
spacy.ty.TrainableComponent(Protocol)
spacy.ty.TrainableComponent.finish_update(self,sgd:Optimizer)->None
spacy.ty.TrainableComponent.update(self,examples:Iterable['Example'],*,drop:float=0.0,sgd:Optional[Optimizer]=None,losses:Optional[Dict[str,float]]=None)->Dict[str, float]


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/language.py----------------------------------------
A:spacy.language.DEFAULT_CONFIG->util.load_config(DEFAULT_CONFIG_PATH)
A:spacy.language._AnyContext->TypeVar('_AnyContext')
A:spacy.language.lookups->load_lookups(lang=lang, tables=tables)
A:spacy.language.factories->SimpleFrozenDict(error=Errors.E957)
A:spacy.language.self._config->util.load_config(DEFAULT_CONFIG_PATH).merge(self.default_config)
A:spacy.language.self._meta->dict(meta)
A:spacy.language.vectors_name->meta.get('vectors', {}).get('name')
A:spacy.language.vocab->create_vocab(self.lang, self.Defaults, vectors_name=vectors_name)
A:spacy.language.self.tokenizer->create_tokenizer(self)
A:spacy.language.cls.default_config->util.load_config(DEFAULT_CONFIG_PATH).merge(cls.Defaults.config)
A:spacy.language.spacy_version->util.get_minor_version_range(about.__version__)
A:spacy.language.self._meta['labels']->dict(self.pipe_labels)
A:spacy.language.self._meta['pipeline']->list(self.pipe_names)
A:spacy.language.self._meta['components']->list(self.component_names)
A:spacy.language.self._meta['disabled']->list(self.disabled)
A:spacy.language.pipe_meta->self.get_factory_meta(factory_name)
A:spacy.language.pipe_config->util.copy_config(source_config['components'][source_name])
A:spacy.language.self._config['nlp']['pipeline']->list(self.component_names)
A:spacy.language.self._config['nlp']['disabled']->list(self.disabled)
A:spacy.language.prev_weights->self._config['training'].get('score_weights', {})
A:spacy.language.combined_score_weights->combine_score_weights(score_weights, prev_weights)
A:spacy.language.names->list(self.factories.keys())
A:spacy.language.labels[name]->list(pipe.labels)
A:spacy.language.internal_name->self.get_factory_name(factory_name)
A:spacy.language.err->errors.Errors.E886.format(name=pipe_name, tok2vec=tok2vec_name, path=listener_path)
A:spacy.language.existing_func->util.registry.factories.get(internal_name)
A:spacy.language.arg_names->util.get_arg_names(factory_func)
A:spacy.language.factory_meta->FactoryMeta(factory=name, default_config=default_config, assigns=validate_attrs(assigns), requires=validate_attrs(requires), scores=list(default_score_weights.keys()), default_score_weights=default_score_weights, retokenizes=retokenizes)
A:spacy.language.cls.factories->SimpleFrozenDict(registry.factories.get_all(), error=Errors.E957)
A:spacy.language.analysis->analyze_pipes(self, keys=keys)
A:spacy.language.config->util.copy_config(config)
A:spacy.language.resolved->util.registry.resolve(cfg, validate=validate)
A:spacy.language.filled->util.registry.fill(config, validate=validate, schema=ConfigSchema)
A:spacy.language.pipe->self.get_pipe(pipe_name)
A:spacy.language.source_config->source.config.interpolate()
A:spacy.language.bad_val->repr(factory_name)
A:spacy.language.(pipe_component, factory_name)->self.create_pipe_from_source(factory_name, source, name=name)
A:spacy.language.pipe_component->self.create_pipe(factory_name, name=name, config=config, raw_config=raw_config, validate=validate)
A:spacy.language.pipe_index->self.component_names.index(name)
A:spacy.language.self._pipe_meta[name]->self.get_factory_meta(factory_name)
A:spacy.language.i->self.component_names.index(old_name)
A:spacy.language.self._pipe_meta[new_name]->self._pipe_meta.pop(old_name)
A:spacy.language.self._pipe_configs[new_name]->self._pipe_configs.pop(old_name)
A:spacy.language.init_cfg->self._config['initialize']['components'].pop(old_name)
A:spacy.language.removed->self._components.pop(self.component_names.index(name))
A:spacy.language.doc->Doc(self.vocab).from_bytes(byte_doc)
A:spacy.language.error_handler->proc.get_error_handler()
A:spacy.language.examples->_copy_examples(examples)
A:spacy.language.self._optimizer->self.create_optimizer()
A:spacy.language.pipe_kwargs[name]->deepcopy(component_cfg[name])
A:spacy.language.pipes->list(self.pipeline)
A:spacy.language.I->util.registry.resolve(config['initialize'], schema=ConfigSchemaInit)
A:spacy.language.ops->get_current_ops()
A:spacy.language.tok_settings->validate_init_settings(self.tokenizer.initialize, I['tokenizer'], section='tokenizer', name='tokenizer')
A:spacy.language.p_settings->validate_init_settings(proc.initialize, p_settings, section='components', name=name)
A:spacy.language.pretrain_cfg->util.copy_config(config).get('pretraining')
A:spacy.language.P->util.registry.resolve(pretrain_cfg, schema=ConfigSchemaPretrain)
A:spacy.language.proc._rehearsal_model->deepcopy(proc.model)
A:spacy.language.kwargs->component_cfg.get(name, {})
A:spacy.language.scorer->Scorer(**kwargs)
A:spacy.language.start_time->timer()
A:spacy.language.docs->pipe(docs)
A:spacy.language.end_time->timer()
A:spacy.language.results->Scorer(**kwargs).score(examples)
A:spacy.language.n_words->sum((len(eg.predicted) for eg in examples))
A:spacy.language.texts->cast(Iterable[Union[str, Doc]], texts)
A:spacy.language.n_process->multiprocessing.cpu_count()
A:spacy.language.f->functools.partial(_pipe, proc=proc, name=name, kwargs=kwargs, default_error_handler=self.default_error_handler)
A:spacy.language.serialized_texts_with_ctx->prepare_input(texts)
A:spacy.language.(texts, raw_texts)->itertools.tee(serialized_texts_with_ctx)
A:spacy.language.(bytedocs_recv_ch, bytedocs_send_ch)->zip(*[mp.Pipe(False) for _ in range(n_process)])
A:spacy.language.batch_texts->util.minibatch(texts, batch_size)
A:spacy.language.sender->_Sender(batch_texts, texts_q, chunk_size=n_process)
A:spacy.language.byte_tuples->itertools.chain.from_iterable((recv.recv() for recv in cycle(bytedocs_recv_ch)))
A:spacy.language.error->srsly.msgpack_loads(byte_error)
A:spacy.language.config_lang->config['nlp'].get('lang')
A:spacy.language.orig_pipeline->util.copy_config(config).pop('components', {})
A:spacy.language.orig_pretraining->util.copy_config(config).pop('pretraining', None)
A:spacy.language.resolved_nlp->util.registry.resolve(filled['nlp'], validate=validate, schema=ConfigSchemaNlp)
A:spacy.language.lang_cls->before_creation(cls)
A:spacy.language.nlp->after_pipeline_creation(nlp)
A:spacy.language.pipeline->interpolated.get('components', {})
A:spacy.language.sourced->util.get_sourced_components(interpolated)
A:spacy.language.opts->', '.join(pipeline.keys())
A:spacy.language.pipe_cfg->util.copy_config(pipeline[pipe_name])
A:spacy.language.raw_config->Config(filled['components'][pipe_name])
A:spacy.language.factory->util.copy_config(pipeline[pipe_name]).pop('factory')
A:spacy.language.vocab_b->after_pipeline_creation(nlp).vocab.to_bytes(exclude=['lookups', 'strings'])
A:spacy.language.source_nlps[model]->util.load_model(model, vocab=nlp.vocab, exclude=['lookups'])
A:spacy.language.source_name->util.copy_config(pipeline[pipe_name]).get('component', pipe_name)
A:spacy.language.source_nlp_vectors_hashes[model]->hash(source_nlps[model].vocab.vectors.to_bytes(exclude=['strings']))
A:spacy.language.enabled->config['nlp'].get('enabled', [])
A:spacy.language.disabled_pipes->cls._resolve_component_status(list({*disable, *config['nlp'].get('disabled', [])}), enable, config['nlp']['pipeline'])
A:spacy.language.nlp._disabled->set((p for p in disabled_pipes if p not in exclude))
A:spacy.language.paths->util.get_sourced_components(interpolated).get(listener_name, {}).get('replace_listeners', [])
A:spacy.language.tok2vec->self.get_pipe(tok2vec_name)
A:spacy.language.tok2vec_cfg->self.get_pipe_config(tok2vec_name)
A:spacy.language.pipe_listeners->self.get_pipe(tok2vec_name).listener_map.get(pipe_name, [])
A:spacy.language.new_config->replace_func(tok2vec_cfg['model'], pipe_cfg['model']['tok2vec'])
A:spacy.language.new_model->tok2vec_model.attrs['replace_listener'](new_model)
A:spacy.language.path->util.ensure_path(path)
A:spacy.language.data->srsly.json_loads(b)
A:spacy.language.self.vocab.vectors.name->srsly.json_loads(b).get('vectors', {}).get('name')
A:spacy.language.texts_with_ctx->receiver.get()
A:spacy.language.self.data->iter(data)
A:spacy.language.self.queues->iter(cycle(queues))
spacy.Language(self,vocab:Union[Vocab,bool]=True,*,max_length:int=10**6,meta:Dict[str,Any]={},create_tokenizer:Optional[Callable[['Language'],Callable[[str],Doc]]]=None,batch_size:int=1000,**kwargs)
spacy.language.BaseDefaults
spacy.language.DisabledPipes(self,nlp:Language,names:List[str])
spacy.language.DisabledPipes.__enter__(self)
spacy.language.DisabledPipes.__exit__(self,*args)
spacy.language.DisabledPipes.restore(self)->None
spacy.language.FactoryMeta
spacy.language.Language(self,vocab:Union[Vocab,bool]=True,*,max_length:int=10**6,meta:Dict[str,Any]={},create_tokenizer:Optional[Callable[['Language'],Callable[[str],Doc]]]=None,batch_size:int=1000,**kwargs)
spacy.language.Language.__init_subclass__(cls,**kwargs)
spacy.language.Language._ensure_doc(self,doc_like:Union[str,Doc,bytes])->Doc
spacy.language.Language._ensure_doc_with_context(self,doc_like:Union[str,Doc,bytes],context:_AnyContext)->Doc
spacy.language.Language._get_pipe_index(self,before:Optional[Union[str,int]]=None,after:Optional[Union[str,int]]=None,first:Optional[bool]=None,last:Optional[bool]=None)->int
spacy.language.Language._has_gpu_model(self,disable:Iterable[str])
spacy.language.Language._link_components(self)->None
spacy.language.Language._multiprocessing_pipe(self,texts:Iterable[Union[str,Doc]],pipes:Iterable[Callable[...,Iterator[Doc]]],n_process:int,batch_size:int)->Iterator[Doc]
spacy.language.Language._resolve_component_status(disable:Union[str,Iterable[str]],enable:Union[str,Iterable[str]],pipe_names:Iterable[str])->Tuple[str, ...]
spacy.language.Language.add_pipe(self,factory_name:str,name:Optional[str]=None,*,before:Optional[Union[str,int]]=None,after:Optional[Union[str,int]]=None,first:Optional[bool]=None,last:Optional[bool]=None,source:Optional['Language']=None,config:Dict[str,Any]=SimpleFrozenDict(),raw_config:Optional[Config]=None,validate:bool=True)->PipeCallable
spacy.language.Language.analyze_pipes(self,*,keys:List[str]=['assigns','requires','scores','retokenizes'],pretty:bool=False)->Optional[Dict[str, Any]]
spacy.language.Language.begin_training(self,get_examples:Optional[Callable[[],Iterable[Example]]]=None,*,sgd:Optional[Optimizer]=None)->Optimizer
spacy.language.Language.component(cls,name:str,*,assigns:Iterable[str]=SimpleFrozenList(),requires:Iterable[str]=SimpleFrozenList(),retokenizes:bool=False,func:Optional[PipeCallable]=None)->Callable[..., Any]
spacy.language.Language.component_names(self)->List[str]
spacy.language.Language.components(self)->List[Tuple[str, PipeCallable]]
spacy.language.Language.config(self)->Config
spacy.language.Language.config(self,value:Config)->None
spacy.language.Language.create_optimizer(self)
spacy.language.Language.create_pipe(self,factory_name:str,name:Optional[str]=None,*,config:Dict[str,Any]=SimpleFrozenDict(),raw_config:Optional[Config]=None,validate:bool=True)->PipeCallable
spacy.language.Language.create_pipe_from_source(self,source_name:str,source:'Language',*,name:str)->Tuple[PipeCallable, str]
spacy.language.Language.disable_pipe(self,name:str)->None
spacy.language.Language.disable_pipes(self,*names)->'DisabledPipes'
spacy.language.Language.disabled(self)->List[str]
spacy.language.Language.enable_pipe(self,name:str)->None
spacy.language.Language.evaluate(self,examples:Iterable[Example],*,batch_size:Optional[int]=None,scorer:Optional[Scorer]=None,component_cfg:Optional[Dict[str,Dict[str,Any]]]=None,scorer_cfg:Optional[Dict[str,Any]]=None)->Dict[str, Any]
spacy.language.Language.factory(cls,name:str,*,default_config:Dict[str,Any]=SimpleFrozenDict(),assigns:Iterable[str]=SimpleFrozenList(),requires:Iterable[str]=SimpleFrozenList(),retokenizes:bool=False,default_score_weights:Dict[str,Optional[float]]=SimpleFrozenDict(),func:Optional[Callable]=None)->Callable
spacy.language.Language.factory_names(self)->List[str]
spacy.language.Language.from_bytes(self,bytes_data:bytes,*,exclude:Iterable[str]=SimpleFrozenList())->'Language'
spacy.language.Language.from_config(cls,config:Union[Dict[str,Any],Config]={},*,vocab:Union[Vocab,bool]=True,disable:Union[str,Iterable[str]]=_DEFAULT_EMPTY_PIPES,enable:Union[str,Iterable[str]]=_DEFAULT_EMPTY_PIPES,exclude:Union[str,Iterable[str]]=_DEFAULT_EMPTY_PIPES,meta:Dict[str,Any]=SimpleFrozenDict(),auto_fill:bool=True,validate:bool=True)->'Language'
spacy.language.Language.from_disk(self,path:Union[str,Path],*,exclude:Iterable[str]=SimpleFrozenList(),overrides:Dict[str,Any]=SimpleFrozenDict())->'Language'
spacy.language.Language.get_factory_meta(cls,name:str)->'FactoryMeta'
spacy.language.Language.get_factory_name(cls,name:str)->str
spacy.language.Language.get_pipe(self,name:str)->PipeCallable
spacy.language.Language.get_pipe_config(self,name:str)->Config
spacy.language.Language.get_pipe_meta(self,name:str)->'FactoryMeta'
spacy.language.Language.has_factory(cls,name:str)->bool
spacy.language.Language.has_pipe(self,name:str)->bool
spacy.language.Language.initialize(self,get_examples:Optional[Callable[[],Iterable[Example]]]=None,*,sgd:Optional[Optimizer]=None)->Optimizer
spacy.language.Language.make_doc(self,text:str)->Doc
spacy.language.Language.meta(self)->Dict[str, Any]
spacy.language.Language.meta(self,value:Dict[str,Any])->None
spacy.language.Language.path(self)
spacy.language.Language.pipe(self,texts:Union[Iterable[Union[str,Doc]],Iterable[Tuple[Union[str,Doc],_AnyContext]]],*,as_tuples:bool=False,batch_size:Optional[int]=None,disable:Iterable[str]=SimpleFrozenList(),component_cfg:Optional[Dict[str,Dict[str,Any]]]=None,n_process:int=1)->Union[Iterator[Doc], Iterator[Tuple[Doc, _AnyContext]]]
spacy.language.Language.pipe_factories(self)->Dict[str, str]
spacy.language.Language.pipe_labels(self)->Dict[str, List[str]]
spacy.language.Language.pipe_names(self)->List[str]
spacy.language.Language.pipeline(self)->List[Tuple[str, PipeCallable]]
spacy.language.Language.rehearse(self,examples:Iterable[Example],*,sgd:Optional[Optimizer]=None,losses:Optional[Dict[str,float]]=None,component_cfg:Optional[Dict[str,Dict[str,Any]]]=None,exclude:Iterable[str]=SimpleFrozenList())->Dict[str, float]
spacy.language.Language.remove_pipe(self,name:str)->Tuple[str, PipeCallable]
spacy.language.Language.rename_pipe(self,old_name:str,new_name:str)->None
spacy.language.Language.replace_listeners(self,tok2vec_name:str,pipe_name:str,listeners:Iterable[str])->None
spacy.language.Language.replace_pipe(self,name:str,factory_name:str,*,config:Dict[str,Any]=SimpleFrozenDict(),validate:bool=True)->PipeCallable
spacy.language.Language.resume_training(self,*,sgd:Optional[Optimizer]=None)->Optimizer
spacy.language.Language.select_pipes(self,*,disable:Optional[Union[str,Iterable[str]]]=None,enable:Optional[Union[str,Iterable[str]]]=None)->'DisabledPipes'
spacy.language.Language.set_error_handler(self,error_handler:Callable[[str,PipeCallable,List[Doc],Exception],NoReturn])
spacy.language.Language.set_factory_meta(cls,name:str,value:'FactoryMeta')->None
spacy.language.Language.to_bytes(self,*,exclude:Iterable[str]=SimpleFrozenList())->bytes
spacy.language.Language.to_disk(self,path:Union[str,Path],*,exclude:Iterable[str]=SimpleFrozenList())->None
spacy.language.Language.update(self,examples:Iterable[Example],_:Optional[Any]=None,*,drop:float=0.0,sgd:Optional[Optimizer]=None,losses:Optional[Dict[str,float]]=None,component_cfg:Optional[Dict[str,Dict[str,Any]]]=None,exclude:Iterable[str]=SimpleFrozenList(),annotates:Iterable[str]=SimpleFrozenList())
spacy.language.Language.use_params(self,params:Optional[dict])
spacy.language._Sender(self,data:Iterable[Any],queues:List[mp.Queue],chunk_size:int)
spacy.language._Sender.send(self)->None
spacy.language._Sender.step(self)->None
spacy.language._apply_pipes(ensure_doc:Callable[[Union[str,Doc,bytes],_AnyContext],Doc],pipes:Iterable[Callable[...,Iterator[Doc]]],receiver,sender,underscore_state:Tuple[dict,dict,dict])->None
spacy.language._copy_examples(examples:Iterable[Example])->List[Example]
spacy.language.create_tokenizer()->Callable[['Language'], Tokenizer]
spacy.language.load_lookups_data(lang,tables)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/vocab.pyi----------------------------------------
spacy.Vocab(self,lex_attr_getters:Optional[Dict[str,Callable[[str],Any]]]=...,strings:Optional[Union[List[str],StringStore]]=...,lookups:Optional[Lookups]=...,oov_prob:float=...,vectors_name:Optional[str]=...,writing_system:Dict[str,Any]=...,get_noun_chunks:Optional[Callable[[Union[Doc,Span]],Iterator[Span]]]=...)
spacy.vocab.Vocab(self,lex_attr_getters:Optional[Dict[str,Callable[[str],Any]]]=...,strings:Optional[Union[List[str],StringStore]]=...,lookups:Optional[Lookups]=...,oov_prob:float=...,vectors_name:Optional[str]=...,writing_system:Dict[str,Any]=...,get_noun_chunks:Optional[Callable[[Union[Doc,Span]],Iterator[Span]]]=...)
spacy.vocab.Vocab.__contains__(self,key:str)->bool
spacy.vocab.Vocab.__getitem__(self,id_or_string:Union[str,int])->Lexeme
spacy.vocab.Vocab.__iter__(self)->Iterator[Lexeme]
spacy.vocab.Vocab.__len__(self)->int
spacy.vocab.Vocab.add_flag(self,flag_getter:Callable[[str],bool],flag_id:int=...)->int
spacy.vocab.Vocab.deduplicate_vectors(self)->None
spacy.vocab.Vocab.from_bytes(self,bytes_data:bytes,*,exclude:Iterable[str]=...)->Vocab
spacy.vocab.Vocab.from_disk(self,path:Union[str,Path],*,exclude:Iterable[str]=...)->Vocab
spacy.vocab.Vocab.get_vector(self,orth:Union[int,str],minn:Optional[int]=...,maxn:Optional[int]=...)->FloatsXd
spacy.vocab.Vocab.has_vector(self,orth:Union[int,str])->bool
spacy.vocab.Vocab.lang(self)->str
spacy.vocab.Vocab.prune_vectors(self,nr_row:int,batch_size:int=...)->Dict[str, float]
spacy.vocab.Vocab.reset_vectors(self,*,width:Optional[int]=...,shape:Optional[int]=...)->None
spacy.vocab.Vocab.set_vector(self,orth:Union[int,str],vector:Floats1d)->None
spacy.vocab.Vocab.to_bytes(self,*,exclude:Iterable[str]=...)->bytes
spacy.vocab.Vocab.to_disk(self,path:Union[str,Path],*,exclude:Iterable[str]=...)->None
spacy.vocab.Vocab.vectors_length(self)->int
spacy.vocab.create_vocab(lang:Optional[str],defaults:Any,vectors_name:Optional[str]=...)->Vocab
spacy.vocab.pickle_vocab(vocab:Vocab)->Any
spacy.vocab.unpickle_vocab(sstore:StringStore,vectors:Any,morphology:Any,_unused_object:Any,lex_attr_getters:Any,lookups:Any,get_noun_chunks:Any)->Vocab


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/strings.pyi----------------------------------------
spacy.strings.StringStore(self,strings:Optional[Iterable[str]]=...,freeze:bool=...)
spacy.strings.StringStore.__contains__(self,string:str)->bool
spacy.strings.StringStore.__iter__(self)->Iterator[str]
spacy.strings.StringStore.__len__(self)->int
spacy.strings.StringStore.__reduce__(self)->Any
spacy.strings.StringStore._reset_and_load(self,strings:Iterable[str])->None
spacy.strings.StringStore.add(self,string:str)->int
spacy.strings.StringStore.as_int(self,key:Union[bytes,str,int])->int
spacy.strings.StringStore.as_string(self,key:Union[bytes,str,int])->str
spacy.strings.StringStore.from_bytes(self,bytes_data:bytes,**kwargs:Any)->StringStore
spacy.strings.StringStore.from_disk(self,path:Union[str,Path])->StringStore
spacy.strings.StringStore.to_bytes(self,**kwargs:Any)->bytes
spacy.strings.StringStore.to_disk(self,path:Union[str,Path])->None
spacy.strings.get_string_id(key:Union[str,int])->int


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/__main__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/compat.py----------------------------------------
A:spacy.compat.is_windows->sys.platform.startswith('win')
A:spacy.compat.is_linux->sys.platform.startswith('linux')


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/schemas.py----------------------------------------
A:spacy.schemas.ItemT->TypeVar('ItemT')
A:spacy.schemas.errors->e.errors()
A:spacy.schemas.data->defaultdict(list)
A:spacy.schemas.err_loc->' -> '.join([str(p) for p in error.get('loc', [])])
A:spacy.schemas.sig->inspect.signature(func)
A:spacy.schemas.schema->get_arg_model(func, exclude=exclude, name='InitArgModel')
A:spacy.schemas.regex->re.compile('^({\\d+}|{\\d+,\\d*}|{\\d*,\\d+})$')
spacy.schemas.ArgSchemaConfig
spacy.schemas.ArgSchemaConfigExtra
spacy.schemas.ConfigSchema(BaseModel)
spacy.schemas.ConfigSchema.Config
spacy.schemas.ConfigSchemaInit(BaseModel)
spacy.schemas.ConfigSchemaInit.Config
spacy.schemas.ConfigSchemaNlp(BaseModel)
spacy.schemas.ConfigSchemaNlp.Config
spacy.schemas.ConfigSchemaPretrain(BaseModel)
spacy.schemas.ConfigSchemaPretrain.Config
spacy.schemas.ConfigSchemaPretrainEmpty(BaseModel)
spacy.schemas.ConfigSchemaPretrainEmpty.Config
spacy.schemas.ConfigSchemaTraining(BaseModel)
spacy.schemas.ConfigSchemaTraining.Config
spacy.schemas.DocJSONSchema(BaseModel)
spacy.schemas.ModelMetaSchema(BaseModel)
spacy.schemas.ProjectConfigAssetGit(BaseModel)
spacy.schemas.ProjectConfigAssetGitItem(BaseModel)
spacy.schemas.ProjectConfigAssetURL(BaseModel)
spacy.schemas.ProjectConfigCommand(BaseModel)
spacy.schemas.ProjectConfigCommand.Config
spacy.schemas.ProjectConfigSchema(BaseModel)
spacy.schemas.ProjectConfigSchema.Config
spacy.schemas.RecommendationSchema(BaseModel)
spacy.schemas.RecommendationTrf(BaseModel)
spacy.schemas.RecommendationTrfItem(BaseModel)
spacy.schemas.TokenPattern(BaseModel)
spacy.schemas.TokenPattern.Config
spacy.schemas.TokenPattern.raise_for_none(cls,v)
spacy.schemas.TokenPatternNumber(BaseModel)
spacy.schemas.TokenPatternNumber.Config
spacy.schemas.TokenPatternNumber.raise_for_none(cls,v)
spacy.schemas.TokenPatternOperatorMinMax(ConstrainedStr)
spacy.schemas.TokenPatternOperatorSimple(str,Enum)
spacy.schemas.TokenPatternSchema(BaseModel)
spacy.schemas.TokenPatternSchema.Config
spacy.schemas.TokenPatternString(BaseModel)
spacy.schemas.TokenPatternString.Config
spacy.schemas.TokenPatternString.raise_for_none(cls,v)
spacy.schemas.get_arg_model(func:Callable,*,exclude:Iterable[str]=tuple(),name:str='ArgModel',strict:bool=True)->ModelMetaclass
spacy.schemas.validate(schema:Type[BaseModel],obj:Dict[str,Any])->List[str]
spacy.schemas.validate_init_settings(func:Callable,settings:Dict[str,Any],*,section:Optional[str]=None,name:str='',exclude:Iterable[str]=('get_examples','nlp'))->Dict[str, Any]
spacy.schemas.validate_token_pattern(obj:list)->List[str]


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/scorer.py----------------------------------------
A:spacy.scorer.MISSING_VALUES->frozenset([None, 0, ''])
A:spacy.scorer.self.saved_score->_roc_auc_score(self.golds, self.cands)
A:spacy.scorer.self.saved_score_at_len->len(self.golds)
A:spacy.scorer.nlp->get_lang_class(default_lang)()
A:spacy.scorer.acc_score->PRFScore()
A:spacy.scorer.prf_score->PRFScore()
A:spacy.scorer.gold_spans->set()
A:spacy.scorer.pred_spans->set()
A:spacy.scorer.tag_score->PRFScore()
A:spacy.scorer.gold_tags->set()
A:spacy.scorer.missing_indices->set()
A:spacy.scorer.value->getter(token, attr)
A:spacy.scorer.pred_tags->set()
A:spacy.scorer.micro_score->PRFScore()
A:spacy.scorer.(field, values)->feat.split(Morphology.FIELD_SEP)
A:spacy.scorer.per_feat[field]->PRFScore()
A:spacy.scorer.gold_per_feat[field]->set()
A:spacy.scorer.pred_per_feat[field]->set()
A:spacy.scorer.score->PRFScore()
A:spacy.scorer.score_per_type->defaultdict(PRFScore)
A:spacy.scorer.labels->set(labels)
A:spacy.scorer.score_per_type[label]->PRFScore()
A:spacy.scorer.pred_cats->getter(example.predicted, attr)
A:spacy.scorer.gold_cats->getter(example.reference, attr)
A:spacy.scorer.pred_score->getter(example.predicted, attr).get(label, 0.0)
A:spacy.scorer.gold_score->getter(example.reference, attr).get(label)
A:spacy.scorer.(pred_label, pred_score)->max(pred_cats.items(), key=lambda it: it[1])
A:spacy.scorer.(gold_label, gold_score)->max(gold_cats, key=lambda it: it[1])
A:spacy.scorer.micro_prf->PRFScore()
A:spacy.scorer.gold_span->gold_ent_by_offset.get((pred_ent.start_char, pred_ent.end_char), None)
A:spacy.scorer.f_per_type[label]->PRFScore()
A:spacy.scorer.unlabelled->PRFScore()
A:spacy.scorer.labelled->PRFScore()
A:spacy.scorer.labelled_per_dep->dict()
A:spacy.scorer.gold_deps->set()
A:spacy.scorer.dep->getter(token, attr)
A:spacy.scorer.head->head_getter(token, head_attr)
A:spacy.scorer.labelled_per_dep[dep]->PRFScore()
A:spacy.scorer.gold_deps_per_dep[dep]->set()
A:spacy.scorer.pred_deps->set()
A:spacy.scorer.pred_deps_per_dep[dep]->set()
A:spacy.scorer.score_per_type[pred_ent.label_]->PRFScore()
A:spacy.scorer.totals->PRFScore()
A:spacy.scorer.(fpr, tpr, _)->_roc_curve(y_true, y_score)
A:spacy.scorer.(fps, tps, thresholds)->_binary_clf_curve(y_true, y_score)
A:spacy.scorer.fpr->numpy.repeat(np.nan, fps.shape)
A:spacy.scorer.tpr->numpy.repeat(np.nan, tps.shape)
A:spacy.scorer.y_true->numpy.ravel(y_true)
A:spacy.scorer.y_score->numpy.ravel(y_score)
A:spacy.scorer.out->numpy.cumsum(arr, axis=axis, dtype=np.float64)
A:spacy.scorer.expected->numpy.sum(arr, axis=axis, dtype=np.float64)
A:spacy.scorer.x->numpy.ravel(x)
A:spacy.scorer.y->numpy.ravel(y)
A:spacy.scorer.dx->numpy.diff(x)
A:spacy.scorer.area->area.dtype.type(area).dtype.type(area)
spacy.scorer.PRFScore(self,*,tp:int=0,fp:int=0,fn:int=0)
spacy.scorer.PRFScore.__add__(self,other)
spacy.scorer.PRFScore.__iadd__(self,other)
spacy.scorer.PRFScore.__len__(self)->int
spacy.scorer.PRFScore.fscore(self)->float
spacy.scorer.PRFScore.precision(self)->float
spacy.scorer.PRFScore.recall(self)->float
spacy.scorer.PRFScore.score_set(self,cand:set,gold:set)->None
spacy.scorer.PRFScore.to_dict(self)->Dict[str, float]
spacy.scorer.ROCAUCScore(self)
spacy.scorer.ROCAUCScore.is_binary(self)
spacy.scorer.ROCAUCScore.score(self)
spacy.scorer.ROCAUCScore.score_set(self,cand,gold)->None
spacy.scorer.Scorer(self,nlp:Optional['Language']=None,default_lang:str='xx',default_pipeline:Iterable[str]=DEFAULT_PIPELINE,**cfg)
spacy.scorer.Scorer.score(self,examples:Iterable[Example])->Dict[str, Any]
spacy.scorer.Scorer.score_cats(examples:Iterable[Example],attr:str,*,getter:Callable[[Doc,str],Any]=getattr,labels:Iterable[str]=SimpleFrozenList(),multi_label:bool=True,positive_label:Optional[str]=None,threshold:Optional[float]=None,**cfg)->Dict[str, Any]
spacy.scorer.Scorer.score_deps(examples:Iterable[Example],attr:str,*,getter:Callable[[Token,str],Any]=getattr,head_attr:str='head',head_getter:Callable[[Token,str],Token]=getattr,ignore_labels:Iterable[str]=SimpleFrozenList(),missing_values:Set[Any]=MISSING_VALUES,**cfg)->Dict[str, Any]
spacy.scorer.Scorer.score_links(examples:Iterable[Example],*,negative_labels:Iterable[str],**cfg)->Dict[str, Any]
spacy.scorer.Scorer.score_spans(examples:Iterable[Example],attr:str,*,getter:Callable[[Doc,str],Iterable[Span]]=getattr,has_annotation:Optional[Callable[[Doc],bool]]=None,labeled:bool=True,allow_overlap:bool=False,**cfg)->Dict[str, Any]
spacy.scorer.Scorer.score_token_attr(examples:Iterable[Example],attr:str,*,getter:Callable[[Token,str],Any]=getattr,missing_values:Set[Any]=MISSING_VALUES,**cfg)->Dict[str, Any]
spacy.scorer.Scorer.score_token_attr_per_feat(examples:Iterable[Example],attr:str,*,getter:Callable[[Token,str],Any]=getattr,missing_values:Set[Any]=MISSING_VALUES,**cfg)->Dict[str, Any]
spacy.scorer.Scorer.score_tokenization(examples:Iterable[Example],**cfg)->Dict[str, Any]
spacy.scorer._auc(x,y)
spacy.scorer._binary_clf_curve(y_true,y_score)
spacy.scorer._roc_auc_score(y_true,y_score)
spacy.scorer._roc_curve(y_true,y_score)
spacy.scorer._stable_cumsum(arr,axis=None,rtol=1e-05,atol=1e-08)
spacy.scorer.get_ner_prf(examples:Iterable[Example],**kwargs)->Dict[str, Any]


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/__init__.py----------------------------------------
A:spacy.__init__.LangClass->util.get_lang_class(name)
A:spacy.__init__.config->util.dot_to_dict(config)
spacy.__init__.blank(name:str,*,vocab:Union[Vocab,bool]=True,config:Union[Dict[str,Any],Config]=util.SimpleFrozenDict(),meta:Dict[str,Any]=util.SimpleFrozenDict())->Language
spacy.__init__.load(name:Union[str,Path],*,vocab:Union[Vocab,bool]=True,disable:Union[str,Iterable[str]]=util._DEFAULT_EMPTY_PIPES,enable:Union[str,Iterable[str]]=util._DEFAULT_EMPTY_PIPES,exclude:Union[str,Iterable[str]]=util._DEFAULT_EMPTY_PIPES,config:Union[Dict[str,Any],Config]=util.SimpleFrozenDict())->Language


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/git_info.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/kb/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/pipeline/tok2vec.py----------------------------------------
A:spacy.pipeline.tok2vec.width->model.get_dim('nO')
A:spacy.pipeline.tok2vec.tokvecs->self.model.predict(docs)
A:spacy.pipeline.tok2vec.(tokvecs, bp_tokvecs)->self.model.begin_update(docs)
A:spacy.pipeline.tok2vec.d_docs->bp_tokvecs(d_tokvecs)
A:spacy.pipeline.tok2vec.batch_id->self.get_batch_id(inputs)
spacy.pipeline.Tok2Vec(self,vocab:Vocab,model:Model,name:str='tok2vec')
spacy.pipeline.tok2vec.Tok2Vec(self,vocab:Vocab,model:Model,name:str='tok2vec')
spacy.pipeline.tok2vec.Tok2Vec.add_label(self,label)
spacy.pipeline.tok2vec.Tok2Vec.add_listener(self,listener:'Tok2VecListener',component_name:str)->None
spacy.pipeline.tok2vec.Tok2Vec.find_listeners(self,component)->None
spacy.pipeline.tok2vec.Tok2Vec.get_loss(self,examples,scores)->None
spacy.pipeline.tok2vec.Tok2Vec.initialize(self,get_examples:Callable[[],Iterable[Example]],*,nlp:Optional[Language]=None)
spacy.pipeline.tok2vec.Tok2Vec.listeners(self)->List['Tok2VecListener']
spacy.pipeline.tok2vec.Tok2Vec.listening_components(self)->List[str]
spacy.pipeline.tok2vec.Tok2Vec.predict(self,docs:Iterable[Doc])
spacy.pipeline.tok2vec.Tok2Vec.remove_listener(self,listener:'Tok2VecListener',component_name:str)->bool
spacy.pipeline.tok2vec.Tok2Vec.set_annotations(self,docs:Sequence[Doc],tokvecses)->None
spacy.pipeline.tok2vec.Tok2Vec.update(self,examples:Iterable[Example],*,drop:float=0.0,sgd:Optional[Optimizer]=None,losses:Optional[Dict[str,float]]=None)
spacy.pipeline.tok2vec.Tok2VecListener(self,upstream_name:str,width:int)
spacy.pipeline.tok2vec.Tok2VecListener.get_batch_id(cls,inputs:Iterable[Doc])->int
spacy.pipeline.tok2vec.Tok2VecListener.receive(self,batch_id:int,outputs,backprop)->None
spacy.pipeline.tok2vec.Tok2VecListener.verify_inputs(self,inputs)->bool
spacy.pipeline.tok2vec._empty_backprop(dX)
spacy.pipeline.tok2vec.forward(model:Tok2VecListener,inputs,is_train:bool)
spacy.pipeline.tok2vec.make_tok2vec(nlp:Language,name:str,model:Model)->'Tok2Vec'


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/pipeline/edit_tree_lemmatizer.py----------------------------------------
A:spacy.pipeline.edit_tree_lemmatizer.self.trees->EditTrees(self.vocab.strings)
A:spacy.pipeline.edit_tree_lemmatizer.loss_func->SequenceCategoricalCrossentropy(normalize=False, missing_value=-1)
A:spacy.pipeline.edit_tree_lemmatizer.tree_id->self.trees.add(form, lemma)
A:spacy.pipeline.edit_tree_lemmatizer.label->self.tree2label.get(tree_id, 0)
A:spacy.pipeline.edit_tree_lemmatizer.(d_scores, loss)->loss_func(scores, truths)
A:spacy.pipeline.edit_tree_lemmatizer.n_docs->len(list(docs))
A:spacy.pipeline.edit_tree_lemmatizer.n_labels->len(self.cfg['labels'])
A:spacy.pipeline.edit_tree_lemmatizer.scores->self.model.predict(docs)
A:spacy.pipeline.edit_tree_lemmatizer.guesses->self._scores2guesses(docs, scores)
A:spacy.pipeline.edit_tree_lemmatizer.doc_guesses->doc_guesses.get().get()
A:spacy.pipeline.edit_tree_lemmatizer.doc_tree_ids->doc_tree_ids.get().get()
A:spacy.pipeline.edit_tree_lemmatizer.doc[j].lemma->getattr(doc[j], self.backoff)
A:spacy.pipeline.edit_tree_lemmatizer.lemma->self.trees.apply(tree_id, doc[j].text)
A:spacy.pipeline.edit_tree_lemmatizer.gold_label->self._pair2label(token.text, token.lemma_)
A:spacy.pipeline.edit_tree_lemmatizer.gold_labels->cast(Floats2d, gold_labels)
A:spacy.pipeline.edit_tree_lemmatizer.path->util.ensure_path(path)
A:spacy.pipeline.edit_tree_lemmatizer.self.cfg['labels']->list(labels['labels'])
A:spacy.pipeline.edit_tree_lemmatizer.errors->validate_edit_tree(tree)
A:spacy.pipeline.edit_tree_lemmatizer.tree->dict(tree)
A:spacy.pipeline.edit_tree_lemmatizer.tree['orig']->self.vocab.strings.add(tree['orig'])
A:spacy.pipeline.edit_tree_lemmatizer.tree['subst']->self.vocab.strings.add(tree['subst'])
A:spacy.pipeline.edit_tree_lemmatizer.vocab->Vocab()
A:spacy.pipeline.edit_tree_lemmatizer.trees->EditTrees(vocab.strings)
A:spacy.pipeline.edit_tree_lemmatizer.self.tree2label[tree_id]->len(self.cfg['labels'])
spacy.pipeline.EditTreeLemmatizer(self,vocab:Vocab,model:Model,name:str='trainable_lemmatizer',*,backoff:Optional[str]='orth',min_tree_freq:int=3,overwrite:bool=False,top_k:int=1,scorer:Optional[Callable]=lemmatizer_score)
spacy.pipeline.edit_tree_lemmatizer.EditTreeLemmatizer(self,vocab:Vocab,model:Model,name:str='trainable_lemmatizer',*,backoff:Optional[str]='orth',min_tree_freq:int=3,overwrite:bool=False,top_k:int=1,scorer:Optional[Callable]=lemmatizer_score)
spacy.pipeline.edit_tree_lemmatizer.EditTreeLemmatizer._add_labels(self,labels:Dict)
spacy.pipeline.edit_tree_lemmatizer.EditTreeLemmatizer._labels_from_data(self,get_examples:Callable[[],Iterable[Example]])
spacy.pipeline.edit_tree_lemmatizer.EditTreeLemmatizer._pair2label(self,form,lemma,add_label=False)
spacy.pipeline.edit_tree_lemmatizer.EditTreeLemmatizer._scores2guesses(self,docs,scores)
spacy.pipeline.edit_tree_lemmatizer.EditTreeLemmatizer.from_bytes(self,bytes_data,*,exclude=tuple())
spacy.pipeline.edit_tree_lemmatizer.EditTreeLemmatizer.from_disk(self,path,exclude=tuple())
spacy.pipeline.edit_tree_lemmatizer.EditTreeLemmatizer.get_loss(self,examples:Iterable[Example],scores:List[Floats2d])->Tuple[float, List[Floats2d]]
spacy.pipeline.edit_tree_lemmatizer.EditTreeLemmatizer.hide_labels(self)->bool
spacy.pipeline.edit_tree_lemmatizer.EditTreeLemmatizer.initialize(self,get_examples:Callable[[],Iterable[Example]],*,nlp:Optional[Language]=None,labels:Optional[Dict]=None)
spacy.pipeline.edit_tree_lemmatizer.EditTreeLemmatizer.label_data(self)->Dict
spacy.pipeline.edit_tree_lemmatizer.EditTreeLemmatizer.labels(self)->Tuple[int, ...]
spacy.pipeline.edit_tree_lemmatizer.EditTreeLemmatizer.predict(self,docs:Iterable[Doc])->List[Ints2d]
spacy.pipeline.edit_tree_lemmatizer.EditTreeLemmatizer.set_annotations(self,docs:Iterable[Doc],batch_tree_ids)
spacy.pipeline.edit_tree_lemmatizer.EditTreeLemmatizer.to_bytes(self,*,exclude=tuple())
spacy.pipeline.edit_tree_lemmatizer.EditTreeLemmatizer.to_disk(self,path,exclude=tuple())
spacy.pipeline.edit_tree_lemmatizer.make_edit_tree_lemmatizer(nlp:Language,name:str,model:Model,backoff:Optional[str],min_tree_freq:int,overwrite:bool,top_k:int,scorer:Optional[Callable])


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/pipeline/lemmatizer.py----------------------------------------
A:spacy.pipeline.lemmatizer.self.lookups->Lookups()
A:spacy.pipeline.lemmatizer.self.lemmatize->getattr(self, mode_attr)
A:spacy.pipeline.lemmatizer.error_handler->self.get_error_handler()
A:spacy.pipeline.lemmatizer.(required_tables, optional_tables)->self.get_lookups_config(self.mode)
A:spacy.pipeline.lemmatizer.lookups->load_lookups(lang=self.vocab.lang, tables=required_tables)
A:spacy.pipeline.lemmatizer.optional_lookups->load_lookups(lang=self.vocab.lang, tables=optional_tables, strict=False)
A:spacy.pipeline.lemmatizer.lookup_table->self.lookups.get_table('lemma_lookup', {})
A:spacy.pipeline.lemmatizer.result->self.lookups.get_table('lemma_lookup', {}).get(token.text, token.text)
A:spacy.pipeline.lemmatizer.univ_pos->token.pos_.lower()
A:spacy.pipeline.lemmatizer.index_table->self.lookups.get_table('lemma_index', {})
A:spacy.pipeline.lemmatizer.exc_table->self.lookups.get_table('lemma_exc', {})
A:spacy.pipeline.lemmatizer.rules_table->self.lookups.get_table('lemma_rules', {})
A:spacy.pipeline.lemmatizer.index->self.lookups.get_table('lemma_index', {}).get(univ_pos, {})
A:spacy.pipeline.lemmatizer.exceptions->self.lookups.get_table('lemma_exc', {}).get(univ_pos, {})
A:spacy.pipeline.lemmatizer.rules->self.lookups.get_table('lemma_rules', {}).get(univ_pos, {})
A:spacy.pipeline.lemmatizer.string->string.lower().lower()
A:spacy.pipeline.lemmatizer.forms->list(dict.fromkeys(forms))
spacy.pipeline.Lemmatizer(self,vocab:Vocab,model:Optional[Model],name:str='lemmatizer',*,mode:str='lookup',overwrite:bool=False,scorer:Optional[Callable]=lemmatizer_score)
spacy.pipeline.lemmatizer.Lemmatizer(self,vocab:Vocab,model:Optional[Model],name:str='lemmatizer',*,mode:str='lookup',overwrite:bool=False,scorer:Optional[Callable]=lemmatizer_score)
spacy.pipeline.lemmatizer.Lemmatizer._validate_tables(self,error_message:str=Errors.E912)->None
spacy.pipeline.lemmatizer.Lemmatizer.from_bytes(self,bytes_data:bytes,*,exclude:Iterable[str]=SimpleFrozenList())->'Lemmatizer'
spacy.pipeline.lemmatizer.Lemmatizer.from_disk(self,path:Union[str,Path],*,exclude:Iterable[str]=SimpleFrozenList())->'Lemmatizer'
spacy.pipeline.lemmatizer.Lemmatizer.get_lookups_config(cls,mode:str)->Tuple[List[str], List[str]]
spacy.pipeline.lemmatizer.Lemmatizer.initialize(self,get_examples:Optional[Callable[[],Iterable[Example]]]=None,*,nlp:Optional[Language]=None,lookups:Optional[Lookups]=None)
spacy.pipeline.lemmatizer.Lemmatizer.is_base_form(self,token:Token)->bool
spacy.pipeline.lemmatizer.Lemmatizer.lookup_lemmatize(self,token:Token)->List[str]
spacy.pipeline.lemmatizer.Lemmatizer.mode(self)
spacy.pipeline.lemmatizer.Lemmatizer.rule_lemmatize(self,token:Token)->List[str]
spacy.pipeline.lemmatizer.Lemmatizer.to_bytes(self,*,exclude:Iterable[str]=SimpleFrozenList())->bytes
spacy.pipeline.lemmatizer.Lemmatizer.to_disk(self,path:Union[str,Path],*,exclude:Iterable[str]=SimpleFrozenList())
spacy.pipeline.lemmatizer.lemmatizer_score(examples:Iterable[Example],**kwargs)->Dict[str, Any]
spacy.pipeline.lemmatizer.make_lemmatizer(nlp:Language,model:Optional[Model],name:str,mode:str,overwrite:bool,scorer:Optional[Callable])
spacy.pipeline.lemmatizer.make_lemmatizer_scorer()


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/pipeline/functions.py----------------------------------------
A:spacy.pipeline.functions.merger->Matcher(doc.vocab)
A:spacy.pipeline.functions.matches->merger(doc)
A:spacy.pipeline.functions.spans->util.filter_spans([doc[start:end + 1] for (_, start, end) in matches])
A:spacy.pipeline.functions.self.min_length->config.get('min_length', 0)
A:spacy.pipeline.functions.self.split_length->config.get('split_length', 0)
A:spacy.pipeline.functions.path->util.ensure_path(path)
A:spacy.pipeline.functions.parts->attr.split('.')
A:spacy.pipeline.functions.obj->getattr(obj, part)
spacy.pipeline.functions.DocCleaner(self,attrs:Dict[str,Any],*,silent:bool=True)
spacy.pipeline.functions.DocCleaner.from_bytes(self,data,**kwargs)
spacy.pipeline.functions.DocCleaner.from_disk(self,path,**kwargs)
spacy.pipeline.functions.DocCleaner.to_bytes(self,**kwargs)
spacy.pipeline.functions.DocCleaner.to_disk(self,path,**kwargs)
spacy.pipeline.functions.TokenSplitter(self,min_length:int=0,split_length:int=0)
spacy.pipeline.functions.TokenSplitter._get_config(self)->Dict[str, Any]
spacy.pipeline.functions.TokenSplitter._set_config(self,config:Dict[str,Any]={})->None
spacy.pipeline.functions.TokenSplitter.from_bytes(self,data,**kwargs)
spacy.pipeline.functions.TokenSplitter.from_disk(self,path,**kwargs)
spacy.pipeline.functions.TokenSplitter.to_bytes(self,**kwargs)
spacy.pipeline.functions.TokenSplitter.to_disk(self,path,**kwargs)
spacy.pipeline.functions.make_doc_cleaner(nlp:Language,name:str,*,attrs:Dict[str,Any],silent:bool)
spacy.pipeline.functions.make_token_splitter(nlp:Language,name:str,*,min_length:int=0,split_length:int=0)
spacy.pipeline.functions.merge_entities(doc:Doc)
spacy.pipeline.functions.merge_noun_chunks(doc:Doc)->Doc
spacy.pipeline.functions.merge_subtokens(doc:Doc,label:str='subtok')->Doc
spacy.pipeline.merge_entities(doc:Doc)
spacy.pipeline.merge_noun_chunks(doc:Doc)->Doc
spacy.pipeline.merge_subtokens(doc:Doc,label:str='subtok')->Doc


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/pipeline/attributeruler.py----------------------------------------
A:spacy.pipeline.attributeruler.self.matcher->Matcher(self.vocab, validate=self.validate)
A:spacy.pipeline.attributeruler.error_handler->self.get_error_handler()
A:spacy.pipeline.attributeruler.matches->self.matcher(doc, allow_missing=True, as_spans=False)
A:spacy.pipeline.attributeruler.span->Span(doc, start, end, label=match_id)
A:spacy.pipeline.attributeruler.(attrs, morph_attrs)->_split_morph_attrs(attrs)
A:spacy.pipeline.attributeruler.morph->self.vocab.morphology.add(morph_attrs)
A:spacy.pipeline.attributeruler.key->str(len(self.attrs))
A:spacy.pipeline.attributeruler.attrs->normalize_token_attrs(self.vocab, attrs)
spacy.pipeline.AttributeRuler(self,vocab:Vocab,name:str='attribute_ruler',*,validate:bool=False,scorer:Optional[Callable]=attribute_ruler_score)
spacy.pipeline.attributeruler.AttributeRuler(self,vocab:Vocab,name:str='attribute_ruler',*,validate:bool=False,scorer:Optional[Callable]=attribute_ruler_score)
spacy.pipeline.attributeruler.AttributeRuler.add(self,patterns:Iterable[MatcherPatternType],attrs:Dict,index:int=0)->None
spacy.pipeline.attributeruler.AttributeRuler.add_patterns(self,patterns:Iterable[AttributeRulerPatternType])->None
spacy.pipeline.attributeruler.AttributeRuler.clear(self)->None
spacy.pipeline.attributeruler.AttributeRuler.from_bytes(self,bytes_data:bytes,exclude:Iterable[str]=SimpleFrozenList())->'AttributeRuler'
spacy.pipeline.attributeruler.AttributeRuler.from_disk(self,path:Union[Path,str],exclude:Iterable[str]=SimpleFrozenList())->'AttributeRuler'
spacy.pipeline.attributeruler.AttributeRuler.initialize(self,get_examples:Optional[Callable[[],Iterable[Example]]],*,nlp:Optional[Language]=None,patterns:Optional[Iterable[AttributeRulerPatternType]]=None,tag_map:Optional[TagMapType]=None,morph_rules:Optional[MorphRulesType]=None)->None
spacy.pipeline.attributeruler.AttributeRuler.load_from_morph_rules(self,morph_rules:Dict[str,Dict[str,Dict[Union[int,str],Union[int,str]]]])->None
spacy.pipeline.attributeruler.AttributeRuler.load_from_tag_map(self,tag_map:Dict[str,Dict[Union[int,str],Union[int,str]]])->None
spacy.pipeline.attributeruler.AttributeRuler.match(self,doc:Doc)
spacy.pipeline.attributeruler.AttributeRuler.patterns(self)->List[AttributeRulerPatternType]
spacy.pipeline.attributeruler.AttributeRuler.set_annotations(self,doc,matches)
spacy.pipeline.attributeruler.AttributeRuler.to_bytes(self,exclude:Iterable[str]=SimpleFrozenList())->bytes
spacy.pipeline.attributeruler.AttributeRuler.to_disk(self,path:Union[Path,str],exclude:Iterable[str]=SimpleFrozenList())->None
spacy.pipeline.attributeruler._split_morph_attrs(attrs:dict)->Tuple[dict, dict]
spacy.pipeline.attributeruler.attribute_ruler_score(examples:Iterable[Example],**kwargs)->Dict[str, Any]
spacy.pipeline.attributeruler.make_attribute_ruler(nlp:Language,name:str,validate:bool,scorer:Optional[Callable])
spacy.pipeline.attributeruler.make_attribute_ruler_scorer()


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/pipeline/span_ruler.py----------------------------------------
A:spacy.pipeline.span_ruler.spans->self.ents_filter(spans, matches)
A:spacy.pipeline.span_ruler.entities->list(entities)
A:spacy.pipeline.span_ruler.kwargs->dict(kwargs)
A:spacy.pipeline.span_ruler.error_handler->self.get_error_handler()
A:spacy.pipeline.span_ruler.matches->cast(List[Tuple[int, int, int]], list(self.matcher(doc)) + list(self.phrase_matcher(doc)))
A:spacy.pipeline.span_ruler.deduplicated_matches->set((Span(doc, start, end, label=self._match_label_id_map[m_id]['label'], span_id=self._match_label_id_map[m_id]['id']) for (m_id, start, end) in matches if start != end))
A:spacy.pipeline.span_ruler.doc.ents->sorted(spans)
A:spacy.pipeline.span_ruler.p_label->cast(str, entry['label'])
A:spacy.pipeline.span_ruler.p_id->cast(str, entry.get('id', ''))
A:spacy.pipeline.span_ruler.label->repr((p_label, p_id))
A:spacy.pipeline.span_ruler.m_label_str->self.nlp.vocab.strings.as_string(m_label)
A:spacy.pipeline.span_ruler.orig_len->len(self)
A:spacy.pipeline.span_ruler.path->ensure_path(path)
spacy.pipeline.SpanRuler(self,nlp:Language,name:str='span_ruler',*,spans_key:Optional[str]=DEFAULT_SPANS_KEY,spans_filter:Optional[Callable[[Iterable[Span],Iterable[Span]],Iterable[Span]]]=None,annotate_ents:bool=False,ents_filter:Callable[[Iterable[Span],Iterable[Span]],Iterable[Span]]=util.filter_chain_spans,phrase_matcher_attr:Optional[Union[int,str]]=None,matcher_fuzzy_compare:Callable=levenshtein_compare,validate:bool=False,overwrite:bool=False,scorer:Optional[Callable]=partial(overlapping_labeled_spans_score,spans_key=DEFAULT_SPANS_KEY))
spacy.pipeline.span_ruler.SpanRuler(self,nlp:Language,name:str='span_ruler',*,spans_key:Optional[str]=DEFAULT_SPANS_KEY,spans_filter:Optional[Callable[[Iterable[Span],Iterable[Span]],Iterable[Span]]]=None,annotate_ents:bool=False,ents_filter:Callable[[Iterable[Span],Iterable[Span]],Iterable[Span]]=util.filter_chain_spans,phrase_matcher_attr:Optional[Union[int,str]]=None,matcher_fuzzy_compare:Callable=levenshtein_compare,validate:bool=False,overwrite:bool=False,scorer:Optional[Callable]=partial(overlapping_labeled_spans_score,spans_key=DEFAULT_SPANS_KEY))
spacy.pipeline.span_ruler.SpanRuler.__contains__(self,label:str)->bool
spacy.pipeline.span_ruler.SpanRuler.__len__(self)->int
spacy.pipeline.span_ruler.SpanRuler._require_patterns(self)->None
spacy.pipeline.span_ruler.SpanRuler.add_patterns(self,patterns:List[PatternType])->None
spacy.pipeline.span_ruler.SpanRuler.clear(self)->None
spacy.pipeline.span_ruler.SpanRuler.from_bytes(self,bytes_data:bytes,*,exclude:Iterable[str]=SimpleFrozenList())->'SpanRuler'
spacy.pipeline.span_ruler.SpanRuler.from_disk(self,path:Union[str,Path],*,exclude:Iterable[str]=SimpleFrozenList())->'SpanRuler'
spacy.pipeline.span_ruler.SpanRuler.ids(self)->Tuple[str, ...]
spacy.pipeline.span_ruler.SpanRuler.initialize(self,get_examples:Callable[[],Iterable[Example]],*,nlp:Optional[Language]=None,patterns:Optional[Sequence[PatternType]]=None)
spacy.pipeline.span_ruler.SpanRuler.key(self)->Optional[str]
spacy.pipeline.span_ruler.SpanRuler.labels(self)->Tuple[str, ...]
spacy.pipeline.span_ruler.SpanRuler.match(self,doc:Doc)
spacy.pipeline.span_ruler.SpanRuler.patterns(self)->List[PatternType]
spacy.pipeline.span_ruler.SpanRuler.remove(self,label:str)->None
spacy.pipeline.span_ruler.SpanRuler.remove_by_id(self,pattern_id:str)->None
spacy.pipeline.span_ruler.SpanRuler.set_annotations(self,doc,matches)
spacy.pipeline.span_ruler.SpanRuler.to_bytes(self,*,exclude:Iterable[str]=SimpleFrozenList())->bytes
spacy.pipeline.span_ruler.SpanRuler.to_disk(self,path:Union[str,Path],*,exclude:Iterable[str]=SimpleFrozenList())->None
spacy.pipeline.span_ruler.make_entity_ruler(nlp:Language,name:str,phrase_matcher_attr:Optional[Union[int,str]],matcher_fuzzy_compare:Callable,validate:bool,overwrite_ents:bool,scorer:Optional[Callable],ent_id_sep:str)
spacy.pipeline.span_ruler.make_overlapping_labeled_spans_scorer(spans_key:str=DEFAULT_SPANS_KEY)
spacy.pipeline.span_ruler.make_preserve_existing_ents_filter()
spacy.pipeline.span_ruler.make_prioritize_new_ents_filter()
spacy.pipeline.span_ruler.make_span_ruler(nlp:Language,name:str,spans_key:Optional[str],spans_filter:Optional[Callable[[Iterable[Span],Iterable[Span]],Iterable[Span]]],annotate_ents:bool,ents_filter:Callable[[Iterable[Span],Iterable[Span]],Iterable[Span]],phrase_matcher_attr:Optional[Union[int,str]],matcher_fuzzy_compare:Callable,validate:bool,overwrite:bool,scorer:Optional[Callable])
spacy.pipeline.span_ruler.overlapping_labeled_spans_score(examples:Iterable[Example],*,spans_key=DEFAULT_SPANS_KEY,**kwargs)->Dict[str, Any]
spacy.pipeline.span_ruler.prioritize_existing_ents_filter(entities:Iterable[Span],spans:Iterable[Span])->List[Span]
spacy.pipeline.span_ruler.prioritize_new_ents_filter(entities:Iterable[Span],spans:Iterable[Span])->List[Span]


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/pipeline/spancat.py----------------------------------------
A:spacy.pipeline.spancat.ops->get_current_ops()
A:spacy.pipeline.spancat.starts->starts.reshape((-1, 1)).reshape((-1, 1))
A:spacy.pipeline.spancat.lengths_array->get_current_ops().asarray1i(lengths)
A:spacy.pipeline.spancat.output->Ragged(ops.xp.zeros((0, 0), dtype='i'), lengths_array)
A:spacy.pipeline.spancat.sizes->list(range(min_size, max_size + 1))
A:spacy.pipeline.spancat.kwargs->dict(kwargs)
A:spacy.pipeline.spancat.indices->self.model.ops.to_numpy(indices)
A:spacy.pipeline.spancat.scores->self.model.predict((docs, indices))
A:spacy.pipeline.spancat.suggester_output->self.suggester(docs, ops=self.model.ops)
A:spacy.pipeline.spancat.doc.spans[self.key]->self._make_span_group(doc, indices_i, scores[offset:offset + indices.lengths[i]], labels)
A:spacy.pipeline.spancat.spans->SpanGroup(doc, name=self.key)
A:spacy.pipeline.spancat.(scores, backprop_scores)->self.model.begin_update((docs, spans))
A:spacy.pipeline.spancat.(loss, d_scores)->self.get_loss(examples, (spans, scores))
A:spacy.pipeline.spancat.target->self.model.ops.asarray(target, dtype='f')
A:spacy.pipeline.spancat.start->int(spans_i[j, 0])
A:spacy.pipeline.spancat.end->int(spans_i[j, 1])
A:spacy.pipeline.spancat.loss->float((d_scores ** 2).sum())
A:spacy.pipeline.spancat.Y->self.model.ops.alloc2f(spans.dataXd.shape[0], len(self.labels))
A:spacy.pipeline.spancat.ranked->(scores * -1).argsort()
A:spacy.pipeline.spancat.spans.attrs['scores']->scores[keeps].flatten()
A:spacy.pipeline.spancat.keeps->self.model.ops.to_numpy(keeps)
spacy.pipeline.SpanCategorizer(self,vocab:Vocab,model:Model[Tuple[List[Doc],Ragged],Floats2d],suggester:Suggester,name:str='spancat',*,spans_key:str='spans',threshold:float=0.5,max_positive:Optional[int]=None,scorer:Optional[Callable]=spancat_score)
spacy.pipeline.spancat.SpanCategorizer(self,vocab:Vocab,model:Model[Tuple[List[Doc],Ragged],Floats2d],suggester:Suggester,name:str='spancat',*,spans_key:str='spans',threshold:float=0.5,max_positive:Optional[int]=None,scorer:Optional[Callable]=spancat_score)
spacy.pipeline.spancat.SpanCategorizer._get_aligned_spans(self,eg:Example)
spacy.pipeline.spancat.SpanCategorizer._make_span_group(self,doc:Doc,indices:Ints2d,scores:Floats2d,labels:List[str])->SpanGroup
spacy.pipeline.spancat.SpanCategorizer._validate_categories(self,examples:Iterable[Example])
spacy.pipeline.spancat.SpanCategorizer.add_label(self,label:str)->int
spacy.pipeline.spancat.SpanCategorizer.get_loss(self,examples:Iterable[Example],spans_scores:Tuple[Ragged,Floats2d])->Tuple[float, float]
spacy.pipeline.spancat.SpanCategorizer.initialize(self,get_examples:Callable[[],Iterable[Example]],*,nlp:Optional[Language]=None,labels:Optional[List[str]]=None)->None
spacy.pipeline.spancat.SpanCategorizer.key(self)->str
spacy.pipeline.spancat.SpanCategorizer.label_data(self)->List[str]
spacy.pipeline.spancat.SpanCategorizer.labels(self)->Tuple[str]
spacy.pipeline.spancat.SpanCategorizer.predict(self,docs:Iterable[Doc])
spacy.pipeline.spancat.SpanCategorizer.set_annotations(self,docs:Iterable[Doc],indices_scores)->None
spacy.pipeline.spancat.SpanCategorizer.set_candidates(self,docs:Iterable[Doc],*,candidates_key:str='candidates')->None
spacy.pipeline.spancat.SpanCategorizer.update(self,examples:Iterable[Example],*,drop:float=0.0,sgd:Optional[Optimizer]=None,losses:Optional[Dict[str,float]]=None)->Dict[str, float]
spacy.pipeline.spancat.Suggester(self,docs:Iterable[Doc],*,ops:Optional[Ops]=None)
spacy.pipeline.spancat.build_ngram_range_suggester(min_size:int,max_size:int)->Suggester
spacy.pipeline.spancat.build_ngram_suggester(sizes:List[int])->Suggester
spacy.pipeline.spancat.make_spancat(nlp:Language,name:str,suggester:Suggester,model:Model[Tuple[List[Doc],Ragged],Floats2d],spans_key:str,scorer:Optional[Callable],threshold:float,max_positive:Optional[int])->'SpanCategorizer'
spacy.pipeline.spancat.make_spancat_scorer()
spacy.pipeline.spancat.spancat_score(examples:Iterable[Example],**kwargs)->Dict[str, Any]


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/pipeline/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/pipeline/pipe.pyi----------------------------------------
spacy.pipeline.Pipe(self,doc:Doc)
spacy.pipeline.pipe.Pipe(self,doc:Doc)
spacy.pipeline.pipe.Pipe._require_labels(self)->None
spacy.pipeline.pipe.Pipe.get_error_handler(self)->Callable[[str, 'Pipe', List[Doc], Exception], NoReturn]
spacy.pipeline.pipe.Pipe.hide_labels(self)->bool
spacy.pipeline.pipe.Pipe.initialize(self,get_examples:Callable[[],Iterable[Example]],*,nlp:Language=...)->None
spacy.pipeline.pipe.Pipe.is_trainable(self)->bool
spacy.pipeline.pipe.Pipe.label_data(self)->Any
spacy.pipeline.pipe.Pipe.labels(self)->Tuple[str, ...]
spacy.pipeline.pipe.Pipe.pipe(self,stream:Iterable[Doc],*,batch_size:int=...)->Iterator[Doc]
spacy.pipeline.pipe.Pipe.score(self,examples:Iterable[Example],**kwargs:Any)->Dict[str, Union[float, Dict[str, float]]]
spacy.pipeline.pipe.Pipe.set_error_handler(self,error_handler:Callable[[str,'Pipe',List[Doc],Exception],NoReturn])->None
spacy.pipeline.pipe.deserialize_config(path:Path)->Any


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/pipeline/entityruler.py----------------------------------------
A:spacy.pipeline.entityruler.self.token_patterns->defaultdict(list, {label: val for (label, val) in self.token_patterns.items() if label not in created_labels})
A:spacy.pipeline.entityruler.self.phrase_patterns->defaultdict(list, {label: val for (label, val) in self.phrase_patterns.items() if label not in created_labels})
A:spacy.pipeline.entityruler.self.matcher->Matcher(self.nlp.vocab, validate=self._validate, fuzzy_compare=self.matcher_fuzzy_compare)
A:spacy.pipeline.entityruler.self.phrase_matcher->PhraseMatcher(self.nlp.vocab, attr=self.phrase_matcher_attr)
A:spacy.pipeline.entityruler.self._ent_ids->defaultdict(tuple)
A:spacy.pipeline.entityruler.n_token_patterns->sum((len(p) for p in self.token_patterns.values()))
A:spacy.pipeline.entityruler.n_phrase_patterns->sum((len(p) for p in self.phrase_patterns.values()))
A:spacy.pipeline.entityruler.error_handler->self.get_error_handler()
A:spacy.pipeline.entityruler.matches->self.match(doc)
A:spacy.pipeline.entityruler.final_matches->sorted(final_matches, key=get_sort_key, reverse=True)
A:spacy.pipeline.entityruler.entities->list(doc.ents)
A:spacy.pipeline.entityruler.seen_tokens->set()
A:spacy.pipeline.entityruler.span->Span(doc, start, end, label=match_id)
A:spacy.pipeline.entityruler.keys->set(self.token_patterns.keys())
A:spacy.pipeline.entityruler.all_labels->set()
A:spacy.pipeline.entityruler.(label, _)->self._split_label(l)
A:spacy.pipeline.entityruler.all_ent_ids->set()
A:spacy.pipeline.entityruler.(_, ent_id)->self._split_label(l)
A:spacy.pipeline.entityruler.(ent_label, ent_id)->self._create_label(label, entry['id']).rsplit(self.ent_id_sep, 1)
A:spacy.pipeline.entityruler.label->self._create_label(label, entry['id'])
A:spacy.pipeline.entityruler.key->self.matcher._normalize_key(label)
A:spacy.pipeline.entityruler.cfg->srsly.msgpack_loads(patterns_bytes)
A:spacy.pipeline.entityruler.self.overwrite->srsly.msgpack_loads(patterns_bytes).get('overwrite', False)
A:spacy.pipeline.entityruler.self.phrase_matcher_attr->srsly.msgpack_loads(patterns_bytes).get('phrase_matcher_attr')
A:spacy.pipeline.entityruler.self.ent_id_sep->srsly.msgpack_loads(patterns_bytes).get('ent_id_sep', DEFAULT_ENT_ID_SEP)
A:spacy.pipeline.entityruler.path->ensure_path(path)
A:spacy.pipeline.entityruler.depr_patterns_path->ensure_path(path).with_suffix('.jsonl')
A:spacy.pipeline.entityruler.patterns->srsly.read_jsonl(depr_patterns_path)
spacy.pipeline.EntityRuler(self,nlp:Language,name:str='entity_ruler',*,phrase_matcher_attr:Optional[Union[int,str]]=None,matcher_fuzzy_compare:Callable=levenshtein_compare,validate:bool=False,overwrite_ents:bool=False,ent_id_sep:str=DEFAULT_ENT_ID_SEP,patterns:Optional[List[PatternType]]=None,scorer:Optional[Callable]=entity_ruler_score)
spacy.pipeline.entityruler.EntityRuler(self,nlp:Language,name:str='entity_ruler',*,phrase_matcher_attr:Optional[Union[int,str]]=None,matcher_fuzzy_compare:Callable=levenshtein_compare,validate:bool=False,overwrite_ents:bool=False,ent_id_sep:str=DEFAULT_ENT_ID_SEP,patterns:Optional[List[PatternType]]=None,scorer:Optional[Callable]=entity_ruler_score)
spacy.pipeline.entityruler.EntityRuler.__contains__(self,label:str)->bool
spacy.pipeline.entityruler.EntityRuler.__len__(self)->int
spacy.pipeline.entityruler.EntityRuler._create_label(self,label:Any,ent_id:Any)->str
spacy.pipeline.entityruler.EntityRuler._require_patterns(self)->None
spacy.pipeline.entityruler.EntityRuler._split_label(self,label:str)->Tuple[str, Optional[str]]
spacy.pipeline.entityruler.EntityRuler.add_patterns(self,patterns:List[PatternType])->None
spacy.pipeline.entityruler.EntityRuler.clear(self)->None
spacy.pipeline.entityruler.EntityRuler.ent_ids(self)->Tuple[Optional[str], ...]
spacy.pipeline.entityruler.EntityRuler.from_bytes(self,patterns_bytes:bytes,*,exclude:Iterable[str]=SimpleFrozenList())->'EntityRuler'
spacy.pipeline.entityruler.EntityRuler.from_disk(self,path:Union[str,Path],*,exclude:Iterable[str]=SimpleFrozenList())->'EntityRuler'
spacy.pipeline.entityruler.EntityRuler.initialize(self,get_examples:Callable[[],Iterable[Example]],*,nlp:Optional[Language]=None,patterns:Optional[Sequence[PatternType]]=None)
spacy.pipeline.entityruler.EntityRuler.labels(self)->Tuple[str, ...]
spacy.pipeline.entityruler.EntityRuler.match(self,doc:Doc)
spacy.pipeline.entityruler.EntityRuler.patterns(self)->List[PatternType]
spacy.pipeline.entityruler.EntityRuler.remove(self,ent_id:str)->None
spacy.pipeline.entityruler.EntityRuler.set_annotations(self,doc,matches)
spacy.pipeline.entityruler.EntityRuler.to_bytes(self,*,exclude:Iterable[str]=SimpleFrozenList())->bytes
spacy.pipeline.entityruler.EntityRuler.to_disk(self,path:Union[str,Path],*,exclude:Iterable[str]=SimpleFrozenList())->None
spacy.pipeline.entityruler.entity_ruler_score(examples,**kwargs)
spacy.pipeline.entityruler.make_entity_ruler(nlp:Language,name:str,phrase_matcher_attr:Optional[Union[int,str]],matcher_fuzzy_compare:Callable,validate:bool,overwrite_ents:bool,ent_id_sep:str,scorer:Optional[Callable])
spacy.pipeline.entityruler.make_entity_ruler_scorer()


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/pipeline/entity_linker.py----------------------------------------
A:spacy.pipeline.entity_linker.self.labels_discard->list(labels_discard)
A:spacy.pipeline.entity_linker.self.distance->CosineDistance(normalize=False)
A:spacy.pipeline.entity_linker.self.kb->kb_loader(self.vocab)
A:spacy.pipeline.entity_linker.(ents, _)->ex.get_aligned_ents_and_ner()
A:spacy.pipeline.entity_linker.has_annotations->any([doc.ents for doc in doc_sample])
A:spacy.pipeline.entity_linker.candidates->list(batch_candidates[j])
A:spacy.pipeline.entity_linker.doc.ents->ex.get_matching_ents()
A:spacy.pipeline.entity_linker.(sentence_encodings, bp_context)->self.model.begin_update(docs)
A:spacy.pipeline.entity_linker.(loss, d_scores)->self.get_loss(sentence_encodings=sentence_encodings, examples=examples)
A:spacy.pipeline.entity_linker.kb_ids->eg.get_aligned('ENT_KB_ID', as_string=True)
A:spacy.pipeline.entity_linker.entity_encoding->self.kb.get_vector(kb_id)
A:spacy.pipeline.entity_linker.entity_encodings->xp.asarray([c.entity_vector for c in candidates])
A:spacy.pipeline.entity_linker.out->self.model.ops.alloc2f(*sentence_encodings.shape)
A:spacy.pipeline.entity_linker.err->errors.Errors.E147.format(method='predict', msg='result variables not of equal length')
A:spacy.pipeline.entity_linker.gradients->self.distance.get_grad(selected_encodings, entity_encodings)
A:spacy.pipeline.entity_linker.loss->self.distance.get_loss(selected_encodings, entity_encodings)
A:spacy.pipeline.entity_linker.batch_candidates->list(self.get_candidates_batch(self.kb, [ent_batch[idx] for idx in valid_ent_idx]) if self.candidates_batch_size > 1 else [self.get_candidates(self.kb, ent_batch[idx]) for idx in valid_ent_idx])
A:spacy.pipeline.entity_linker.sent_index->sentences.index(ent.sent)
A:spacy.pipeline.entity_linker.start_sentence->max(0, sent_index - self.n_sents)
A:spacy.pipeline.entity_linker.end_sentence->min(len(sentences) - 1, sent_index + self.n_sents)
A:spacy.pipeline.entity_linker.sent_doc->doc[start_token:end_token].as_doc()
A:spacy.pipeline.entity_linker.sentence_norm->xp.linalg.norm(sentence_encoding_t)
A:spacy.pipeline.entity_linker.prior_probs->xp.asarray([0.0 for _ in candidates])
A:spacy.pipeline.entity_linker.entity_norm->xp.linalg.norm(entity_encodings, axis=1)
A:spacy.pipeline.entity_linker.count_ents->len([ent for doc in docs for ent in doc.ents])
spacy.pipeline.EntityLinker(self,vocab:Vocab,model:Model,name:str='entity_linker',*,labels_discard:Iterable[str],n_sents:int,incl_prior:bool,incl_context:bool,entity_vector_length:int,get_candidates:Callable[[KnowledgeBase,Span],Iterable[Candidate]],get_candidates_batch:Callable[[KnowledgeBase,Iterable[Span]],Iterable[Iterable[Candidate]]],overwrite:bool=BACKWARD_OVERWRITE,scorer:Optional[Callable]=entity_linker_score,use_gold_ents:bool,candidates_batch_size:int,threshold:Optional[float]=None)
spacy.pipeline.entity_linker.EntityLinker(self,vocab:Vocab,model:Model,name:str='entity_linker',*,labels_discard:Iterable[str],n_sents:int,incl_prior:bool,incl_context:bool,entity_vector_length:int,get_candidates:Callable[[KnowledgeBase,Span],Iterable[Candidate]],get_candidates_batch:Callable[[KnowledgeBase,Iterable[Span]],Iterable[Iterable[Candidate]]],overwrite:bool=BACKWARD_OVERWRITE,scorer:Optional[Callable]=entity_linker_score,use_gold_ents:bool,candidates_batch_size:int,threshold:Optional[float]=None)
spacy.pipeline.entity_linker.EntityLinker.add_label(self,label)
spacy.pipeline.entity_linker.EntityLinker.batch_has_learnable_example(self,examples)
spacy.pipeline.entity_linker.EntityLinker.from_bytes(self,bytes_data,*,exclude=tuple())
spacy.pipeline.entity_linker.EntityLinker.from_disk(self,path:Union[str,Path],*,exclude:Iterable[str]=SimpleFrozenList())->'EntityLinker'
spacy.pipeline.entity_linker.EntityLinker.get_loss(self,examples:Iterable[Example],sentence_encodings:Floats2d)
spacy.pipeline.entity_linker.EntityLinker.initialize(self,get_examples:Callable[[],Iterable[Example]],*,nlp:Optional[Language]=None,kb_loader:Optional[Callable[[Vocab],KnowledgeBase]]=None)
spacy.pipeline.entity_linker.EntityLinker.predict(self,docs:Iterable[Doc])->List[str]
spacy.pipeline.entity_linker.EntityLinker.rehearse(self,examples,*,sgd=None,losses=None,**config)
spacy.pipeline.entity_linker.EntityLinker.set_annotations(self,docs:Iterable[Doc],kb_ids:List[str])->None
spacy.pipeline.entity_linker.EntityLinker.set_kb(self,kb_loader:Callable[[Vocab],KnowledgeBase])
spacy.pipeline.entity_linker.EntityLinker.to_bytes(self,*,exclude=tuple())
spacy.pipeline.entity_linker.EntityLinker.to_disk(self,path:Union[str,Path],*,exclude:Iterable[str]=SimpleFrozenList())->None
spacy.pipeline.entity_linker.EntityLinker.update(self,examples:Iterable[Example],*,drop:float=0.0,sgd:Optional[Optimizer]=None,losses:Optional[Dict[str,float]]=None)->Dict[str, float]
spacy.pipeline.entity_linker.EntityLinker.validate_kb(self)->None
spacy.pipeline.entity_linker.entity_linker_score(examples,**kwargs)
spacy.pipeline.entity_linker.make_entity_linker(nlp:Language,name:str,model:Model,*,labels_discard:Iterable[str],n_sents:int,incl_prior:bool,incl_context:bool,entity_vector_length:int,get_candidates:Callable[[KnowledgeBase,Span],Iterable[Candidate]],get_candidates_batch:Callable[[KnowledgeBase,Iterable[Span]],Iterable[Iterable[Candidate]]],overwrite:bool,scorer:Optional[Callable],use_gold_ents:bool,candidates_batch_size:int,threshold:Optional[float]=None)
spacy.pipeline.entity_linker.make_entity_linker_scorer()


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/pipeline/textcat_multilabel.py----------------------------------------
A:spacy.pipeline.textcat_multilabel.self.cfg->dict(cfg)
A:spacy.pipeline.textcat_multilabel.subbatch->list(islice(get_examples(), 10))
A:spacy.pipeline.textcat_multilabel.(label_sample, _)->self._examples_to_truth(subbatch)
spacy.pipeline.MultiLabel_TextCategorizer(self,vocab:Vocab,model:Model,name:str='textcat_multilabel',*,threshold:float,scorer:Optional[Callable]=textcat_multilabel_score)
spacy.pipeline.textcat_multilabel.MultiLabel_TextCategorizer(self,vocab:Vocab,model:Model,name:str='textcat_multilabel',*,threshold:float,scorer:Optional[Callable]=textcat_multilabel_score)
spacy.pipeline.textcat_multilabel.MultiLabel_TextCategorizer._validate_categories(self,examples:Iterable[Example])
spacy.pipeline.textcat_multilabel.MultiLabel_TextCategorizer.initialize(self,get_examples:Callable[[],Iterable[Example]],*,nlp:Optional[Language]=None,labels:Optional[Iterable[str]]=None)
spacy.pipeline.textcat_multilabel.MultiLabel_TextCategorizer.support_missing_values(self)
spacy.pipeline.textcat_multilabel.make_multilabel_textcat(nlp:Language,name:str,model:Model[List[Doc],List[Floats2d]],threshold:float,scorer:Optional[Callable])->'MultiLabel_TextCategorizer'
spacy.pipeline.textcat_multilabel.make_textcat_multilabel_scorer()
spacy.pipeline.textcat_multilabel.textcat_multilabel_score(examples:Iterable[Example],**kwargs)->Dict[str, Any]


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/pipeline/textcat.py----------------------------------------
A:spacy.pipeline.textcat.self.cfg->dict(cfg)
A:spacy.pipeline.textcat.scores->self.model.ops.asarray(scores)
A:spacy.pipeline.textcat.doc.cats[label]->float(scores[i, j])
A:spacy.pipeline.textcat.(scores, bp_scores)->self.model.begin_update(docs)
A:spacy.pipeline.textcat.(loss, d_scores)->self.get_loss(examples, scores)
A:spacy.pipeline.textcat.(target, _)->self._rehearsal_model.begin_update(docs)
A:spacy.pipeline.textcat.nr_examples->len(list(examples))
A:spacy.pipeline.textcat.truths->self.model.ops.asarray(truths)
A:spacy.pipeline.textcat.not_missing->self.model.ops.asarray(not_missing)
A:spacy.pipeline.textcat.(truths, not_missing)->self._examples_to_truth(examples)
A:spacy.pipeline.textcat.mean_square_error->(d_scores ** 2).mean()
A:spacy.pipeline.textcat.self.model->self.model.attrs['resize_output'](self.model, len(self.labels))
A:spacy.pipeline.textcat.err->errors.Errors.E919.format(pos_label=positive_label, labels=self.labels)
A:spacy.pipeline.textcat.subbatch->list(islice(get_examples(), 10))
A:spacy.pipeline.textcat.(label_sample, _)->self._examples_to_truth(subbatch)
A:spacy.pipeline.textcat.vals->list(ex.reference.cats.values())
spacy.pipeline.TextCategorizer(self,vocab:Vocab,model:Model,name:str='textcat',*,threshold:float,scorer:Optional[Callable]=textcat_score)
spacy.pipeline.textcat.TextCategorizer(self,vocab:Vocab,model:Model,name:str='textcat',*,threshold:float,scorer:Optional[Callable]=textcat_score)
spacy.pipeline.textcat.TextCategorizer._examples_to_truth(self,examples:Iterable[Example])->Tuple[numpy.ndarray, numpy.ndarray]
spacy.pipeline.textcat.TextCategorizer._validate_categories(self,examples:Iterable[Example])
spacy.pipeline.textcat.TextCategorizer.add_label(self,label:str)->int
spacy.pipeline.textcat.TextCategorizer.get_loss(self,examples:Iterable[Example],scores)->Tuple[float, float]
spacy.pipeline.textcat.TextCategorizer.initialize(self,get_examples:Callable[[],Iterable[Example]],*,nlp:Optional[Language]=None,labels:Optional[Iterable[str]]=None,positive_label:Optional[str]=None)->None
spacy.pipeline.textcat.TextCategorizer.label_data(self)->List[str]
spacy.pipeline.textcat.TextCategorizer.labels(self)->Tuple[str]
spacy.pipeline.textcat.TextCategorizer.predict(self,docs:Iterable[Doc])
spacy.pipeline.textcat.TextCategorizer.rehearse(self,examples:Iterable[Example],*,drop:float=0.0,sgd:Optional[Optimizer]=None,losses:Optional[Dict[str,float]]=None)->Dict[str, float]
spacy.pipeline.textcat.TextCategorizer.set_annotations(self,docs:Iterable[Doc],scores)->None
spacy.pipeline.textcat.TextCategorizer.support_missing_values(self)
spacy.pipeline.textcat.TextCategorizer.update(self,examples:Iterable[Example],*,drop:float=0.0,sgd:Optional[Optimizer]=None,losses:Optional[Dict[str,float]]=None)->Dict[str, float]
spacy.pipeline.textcat.make_textcat(nlp:Language,name:str,model:Model[List[Doc],List[Floats2d]],threshold:float,scorer:Optional[Callable])->'TextCategorizer'
spacy.pipeline.textcat.make_textcat_scorer()
spacy.pipeline.textcat.textcat_score(examples:Iterable[Example],**kwargs)->Dict[str, Any]


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/pipeline/_edit_tree_internals/schemas.py----------------------------------------
A:spacy.pipeline._edit_tree_internals.schemas.errors->e.errors()
A:spacy.pipeline._edit_tree_internals.schemas.data->defaultdict(list)
A:spacy.pipeline._edit_tree_internals.schemas.err_loc->' -> '.join([str(p) for p in error.get('loc', [])])
spacy.pipeline._edit_tree_internals.schemas.EditTreeSchema(BaseModel)
spacy.pipeline._edit_tree_internals.schemas.MatchNodeSchema(BaseModel)
spacy.pipeline._edit_tree_internals.schemas.MatchNodeSchema.Config
spacy.pipeline._edit_tree_internals.schemas.SubstNodeSchema(BaseModel)
spacy.pipeline._edit_tree_internals.schemas.SubstNodeSchema.Config
spacy.pipeline._edit_tree_internals.schemas.validate_edit_tree(obj:Dict[str,Any])->List[str]


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/pipeline/_edit_tree_internals/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/pipeline/_parser_internals/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/pipeline/legacy/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/pipeline/legacy/entity_linker.py----------------------------------------
A:spacy.pipeline.legacy.entity_linker.self.labels_discard->list(labels_discard)
A:spacy.pipeline.legacy.entity_linker.self.distance->CosineDistance(normalize=False)
A:spacy.pipeline.legacy.entity_linker.self.kb->kb_loader(self.vocab)
A:spacy.pipeline.legacy.entity_linker.kb_ids->eg.get_aligned('ENT_KB_ID', as_string=True)
A:spacy.pipeline.legacy.entity_linker.sent_index->sentences.index(sent)
A:spacy.pipeline.legacy.entity_linker.start_sentence->max(0, sent_index - self.n_sents)
A:spacy.pipeline.legacy.entity_linker.end_sentence->min(len(sentences) - 1, sent_index + self.n_sents)
A:spacy.pipeline.legacy.entity_linker.sent_doc->doc[start_token:end_token].as_doc()
A:spacy.pipeline.legacy.entity_linker.(sentence_encodings, bp_context)->self.model.begin_update(sentence_docs)
A:spacy.pipeline.legacy.entity_linker.(loss, d_scores)->self.get_loss(sentence_encodings=sentence_encodings, examples=examples)
A:spacy.pipeline.legacy.entity_linker.entity_encoding->self.kb.get_vector(kb_id)
A:spacy.pipeline.legacy.entity_linker.entity_encodings->xp.asarray([c.entity_vector for c in candidates])
A:spacy.pipeline.legacy.entity_linker.err->errors.Errors.E147.format(method='predict', msg='result variables not of equal length')
A:spacy.pipeline.legacy.entity_linker.gradients->self.distance.get_grad(sentence_encodings, entity_encodings)
A:spacy.pipeline.legacy.entity_linker.loss->self.distance.get_loss(sentence_encodings, entity_encodings)
A:spacy.pipeline.legacy.entity_linker.sentence_norm->xp.linalg.norm(sentence_encoding_t)
A:spacy.pipeline.legacy.entity_linker.candidates->list(self.get_candidates(self.kb, ent))
A:spacy.pipeline.legacy.entity_linker.prior_probs->xp.asarray([0.0 for _ in candidates])
A:spacy.pipeline.legacy.entity_linker.entity_norm->xp.linalg.norm(entity_encodings, axis=1)
A:spacy.pipeline.legacy.entity_linker.best_index->scores.argmax().item()
A:spacy.pipeline.legacy.entity_linker.count_ents->len([ent for doc in docs for ent in doc.ents])
spacy.pipeline.legacy.EntityLinker_v1(self,vocab:Vocab,model:Model,name:str='entity_linker',*,labels_discard:Iterable[str],n_sents:int,incl_prior:bool,incl_context:bool,entity_vector_length:int,get_candidates:Callable[[KnowledgeBase,Span],Iterable[Candidate]],overwrite:bool=BACKWARD_OVERWRITE,scorer:Optional[Callable]=entity_linker_score)
spacy.pipeline.legacy.entity_linker.EntityLinker_v1(self,vocab:Vocab,model:Model,name:str='entity_linker',*,labels_discard:Iterable[str],n_sents:int,incl_prior:bool,incl_context:bool,entity_vector_length:int,get_candidates:Callable[[KnowledgeBase,Span],Iterable[Candidate]],overwrite:bool=BACKWARD_OVERWRITE,scorer:Optional[Callable]=entity_linker_score)
spacy.pipeline.legacy.entity_linker.EntityLinker_v1.add_label(self,label)
spacy.pipeline.legacy.entity_linker.EntityLinker_v1.from_bytes(self,bytes_data,*,exclude=tuple())
spacy.pipeline.legacy.entity_linker.EntityLinker_v1.from_disk(self,path:Union[str,Path],*,exclude:Iterable[str]=SimpleFrozenList())->'EntityLinker_v1'
spacy.pipeline.legacy.entity_linker.EntityLinker_v1.get_loss(self,examples:Iterable[Example],sentence_encodings:Floats2d)
spacy.pipeline.legacy.entity_linker.EntityLinker_v1.initialize(self,get_examples:Callable[[],Iterable[Example]],*,nlp:Optional[Language]=None,kb_loader:Optional[Callable[[Vocab],KnowledgeBase]]=None)
spacy.pipeline.legacy.entity_linker.EntityLinker_v1.predict(self,docs:Iterable[Doc])->List[str]
spacy.pipeline.legacy.entity_linker.EntityLinker_v1.rehearse(self,examples,*,sgd=None,losses=None,**config)
spacy.pipeline.legacy.entity_linker.EntityLinker_v1.set_annotations(self,docs:Iterable[Doc],kb_ids:List[str])->None
spacy.pipeline.legacy.entity_linker.EntityLinker_v1.set_kb(self,kb_loader:Callable[[Vocab],KnowledgeBase])
spacy.pipeline.legacy.entity_linker.EntityLinker_v1.to_bytes(self,*,exclude=tuple())
spacy.pipeline.legacy.entity_linker.EntityLinker_v1.to_disk(self,path:Union[str,Path],*,exclude:Iterable[str]=SimpleFrozenList())->None
spacy.pipeline.legacy.entity_linker.EntityLinker_v1.update(self,examples:Iterable[Example],*,drop:float=0.0,sgd:Optional[Optimizer]=None,losses:Optional[Dict[str,float]]=None)->Dict[str, float]
spacy.pipeline.legacy.entity_linker.EntityLinker_v1.validate_kb(self)->None
spacy.pipeline.legacy.entity_linker.entity_linker_score(examples,**kwargs)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/util.py----------------------------------------
A:spacy.tests.util.f->tempfile.TemporaryFile(mode=mode)
A:spacy.tests.util.vocab->Vocab()
A:spacy.tests.util.(move, label)->split_bilu_label(action_name)
A:spacy.tests.util.length->len(vectors[0][1])
A:spacy.tests.util.OPS->get_current_ops()
A:spacy.tests.util.v1->get_current_ops().to_numpy(OPS.asarray(vec1))
A:spacy.tests.util.v2->get_current_ops().to_numpy(OPS.asarray(vec2))
A:spacy.tests.util.msg1->srsly.msgpack_loads(b1)
A:spacy.tests.util.msg2->srsly.msgpack_loads(b2)
spacy.tests.util.add_vecs_to_vocab(vocab,vectors)
spacy.tests.util.apply_transition_sequence(parser,doc,sequence)
spacy.tests.util.assert_docs_equal(doc1,doc2)
spacy.tests.util.assert_packed_msg_equal(b1,b2)
spacy.tests.util.get_batch(batch_size)
spacy.tests.util.get_cosine(vec1,vec2)
spacy.tests.util.get_random_doc(n_words)
spacy.tests.util.make_tempfile(mode='r')


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/test_language.py----------------------------------------
A:spacy.tests.test_language.logger->logging.getLogger('spacy')
A:spacy.tests.test_language.nlp->English()
A:spacy.tests.test_language.textcat->English().add_pipe('textcat')
A:spacy.tests.test_language.doc->nlp(docs[0])
A:spacy.tests.test_language.example->spacy.training.Example.from_dict(doc, annots)
A:spacy.tests.test_language.scores->English().evaluate([example])
A:spacy.tests.test_language.textcat_multilabel->English().add_pipe('textcat_multilabel')
A:spacy.tests.test_language.span->Span(doc, 0, 1, label='FIRST')
A:spacy.tests.test_language.ops->get_current_ops()
A:spacy.tests.test_language.docs->nlp2.pipe(texts, n_process=2, batch_size=2)
A:spacy.tests.test_language.stream_texts->itertools.cycle(texts)
A:spacy.tests.test_language.(texts0, texts1)->itertools.tee(stream_texts)
A:spacy.tests.test_language.tuples->list(nlp.pipe(texts, as_tuples=True, n_process=n_process))
A:spacy.tests.test_language.words->text.split(' ')
A:spacy.tests.test_language.nlp.tokenizer->WhitespaceTokenizer(nlp.vocab)
A:spacy.tests.test_language.vectors_bytes->English().vocab.vectors.to_bytes()
spacy.tests.test_language.assert_sents_error(doc)
spacy.tests.test_language.evil_component(doc)
spacy.tests.test_language.ner_pipe(doc)
spacy.tests.test_language.nlp()
spacy.tests.test_language.nlp2(nlp,sample_vectors)
spacy.tests.test_language.perhaps_set_sentences(doc)
spacy.tests.test_language.sample_vectors()
spacy.tests.test_language.test_blank_languages(lang,target)
spacy.tests.test_language.test_component_return()
spacy.tests.test_language.test_dot_in_factory_names(nlp)
spacy.tests.test_language.test_evaluate_multiple_textcat_final(en_vocab)
spacy.tests.test_language.test_evaluate_multiple_textcat_separate(en_vocab)
spacy.tests.test_language.test_evaluate_no_pipe(nlp)
spacy.tests.test_language.test_evaluate_textcat_multilabel(en_vocab)
spacy.tests.test_language.test_invalid_arg_to_pipeline(nlp)
spacy.tests.test_language.test_language_custom_tokenizer()
spacy.tests.test_language.test_language_evaluate(nlp)
spacy.tests.test_language.test_language_from_config_before_after_init()
spacy.tests.test_language.test_language_from_config_before_after_init_invalid()
spacy.tests.test_language.test_language_from_config_invalid_lang()
spacy.tests.test_language.test_language_init_invalid_vocab(value)
spacy.tests.test_language.test_language_matching(lang,target)
spacy.tests.test_language.test_language_pipe(nlp2,n_process,texts)
spacy.tests.test_language.test_language_pipe_error_handler(n_process)
spacy.tests.test_language.test_language_pipe_error_handler_custom(en_vocab,n_process)
spacy.tests.test_language.test_language_pipe_error_handler_input_as_tuples(en_vocab,n_process)
spacy.tests.test_language.test_language_pipe_error_handler_make_doc_actual(n_process)
spacy.tests.test_language.test_language_pipe_error_handler_make_doc_preferred(n_process)
spacy.tests.test_language.test_language_pipe_error_handler_pipe(en_vocab,n_process)
spacy.tests.test_language.test_language_pipe_stream(nlp2,n_process,texts)
spacy.tests.test_language.test_language_source_and_vectors(nlp2)
spacy.tests.test_language.test_language_update(nlp)
spacy.tests.test_language.test_language_whitespace_tokenizer()
spacy.tests.test_language.test_multiprocessing_gpu_warning(nlp2,texts)
spacy.tests.test_language.test_pass_doc_to_pipeline(nlp,n_process)
spacy.tests.test_language.test_spacy_blank()
spacy.tests.test_language.texts()
spacy.tests.test_language.userdata_pipe(doc)
spacy.tests.test_language.vector_modification_pipe(doc)
spacy.tests.test_language.warn_error(proc_name,proc,docs,e)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/test_misc.py----------------------------------------
A:spacy.tests.test_misc.doc->en_tokenizer('zero one two three four five six')
A:spacy.tests.test_misc.result->spacy.util.resolve_dot_names(config, ['training.optimizer'])
A:spacy.tests.test_misc.path->spacy.util.get_package_path(package)
A:spacy.tests.test_misc.model->PrecomputableAffine(nO=nO, nI=nI, nF=nF, nP=nP).initialize()
A:spacy.tests.test_misc.tensor->PrecomputableAffine(nO=nO, nI=nI, nF=nF, nP=nP).initialize().ops.alloc((10, nI))
A:spacy.tests.test_misc.(Y, get_dX)->PrecomputableAffine(nO=nO, nI=nI, nF=nF, nP=nP).initialize().begin_update(tensor)
A:spacy.tests.test_misc.dY->PrecomputableAffine(nO=nO, nI=nI, nF=nF, nP=nP).initialize().ops.alloc((15, nO, nP))
A:spacy.tests.test_misc.ids->PrecomputableAffine(nO=nO, nI=nI, nF=nF, nP=nP).initialize().ops.alloc((15, nF))
A:spacy.tests.test_misc.d_pad->_backprop_precomputable_affine_padding(model, dY, ids)
A:spacy.tests.test_misc.current_ops->get_current_ops()
A:spacy.tests.test_misc.nlp->spacy.lang.en.English.from_config(config)
A:spacy.tests.test_misc.batches->list(minibatch_by_words(docs, size=batch_size, tolerance=tol, discard_oversize=False))
A:spacy.tests.test_misc.nlp_config->Config().from_str(cfg_string)
A:spacy.tests.test_misc.en_nlp->spacy.util.load_model_from_config(nlp_config, auto_fill=True)
A:spacy.tests.test_misc.default_config->Config().from_disk(DEFAULT_CONFIG_PATH)
A:spacy.tests.test_misc.nl_nlp->spacy.util.load_model_from_config(default_config, auto_fill=True)
A:spacy.tests.test_misc.T->spacy.util.registry.resolve(nl_nlp.config['training'], schema=ConfigSchemaTraining)
A:spacy.tests.test_misc.t->SimpleFrozenList(['foo', 'bar'], error='Error!')
A:spacy.tests.test_misc.code_path->os.path.join(temp_dir, 'code.py')
A:spacy.tests.test_misc.found_port->find_available_port(port, host, auto_select=True)
spacy.tests.test_misc.is_admin()
spacy.tests.test_misc.test_PrecomputableAffine(nO=4,nI=5,nF=3,nP=2)
spacy.tests.test_misc.test_ascii_filenames()
spacy.tests.test_misc.test_dot_to_dict(dot_notation,expected)
spacy.tests.test_misc.test_find_available_port()
spacy.tests.test_misc.test_import_code()
spacy.tests.test_misc.test_is_compatible_version(version,constraint,compatible)
spacy.tests.test_misc.test_is_unconstrained_version(constraint,expected)
spacy.tests.test_misc.test_issue6207(en_tokenizer)
spacy.tests.test_misc.test_issue6258()
spacy.tests.test_misc.test_load_model_blank_shortcut()
spacy.tests.test_misc.test_minor_version(a1,a2,b1,b2,is_match)
spacy.tests.test_misc.test_prefer_gpu()
spacy.tests.test_misc.test_require_cpu()
spacy.tests.test_misc.test_require_gpu()
spacy.tests.test_misc.test_resolve_dot_names()
spacy.tests.test_misc.test_set_dot_to_object()
spacy.tests.test_misc.test_simple_frozen_list()
spacy.tests.test_misc.test_to_ternary_int()
spacy.tests.test_misc.test_util_dot_section()
spacy.tests.test_misc.test_util_ensure_path_succeeds(text)
spacy.tests.test_misc.test_util_get_package_path(package)
spacy.tests.test_misc.test_util_is_package(package,result)
spacy.tests.test_misc.test_util_minibatch(doc_sizes,expected_batches)
spacy.tests.test_misc.test_util_minibatch_oversize(doc_sizes,expected_batches)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/test_scorer.py----------------------------------------
A:spacy.tests.test_scorer.nlp->English()
A:spacy.tests.test_scorer.doc->Doc(en_vocab, words=input_.split(' '), ents=['B-ORG', 'O', 'O', 'O', 'O', 'B-GPE', 'B-ORG', 'O', 'O', 'O'])
A:spacy.tests.test_scorer.scorer->Scorer()
A:spacy.tests.test_scorer.example->Example(pred_doc, gold_doc)
A:spacy.tests.test_scorer.scores->spacy.scorer.Scorer.score_cats([example], 'cats', labels=list(gold_doc.cats.keys()), multi_label=True, threshold=0.1)
A:spacy.tests.test_scorer.example.predicted->Doc(nlp.vocab, words=['One', 'sentence.', 'Two', 'sentences.', 'Three', 'sentences.'], spaces=[True, True, True, True, True, False])
A:spacy.tests.test_scorer.results->Scorer().score([example])
A:spacy.tests.test_scorer.entities->offsets_to_biluo_tags(doc, annot['entities'])
A:spacy.tests.test_scorer.pred_doc->en_tokenizer(text)
A:spacy.tests.test_scorer.ref_doc->en_tokenizer('a b c d e')
A:spacy.tests.test_scorer.(tpr, fpr, _)->_roc_curve(y_true, y_score)
A:spacy.tests.test_scorer.roc_auc->_roc_auc_score(y_true, y_score)
A:spacy.tests.test_scorer.score->ROCAUCScore()
A:spacy.tests.test_scorer.gold->English().make_doc(text)
A:spacy.tests.test_scorer.pred->English().make_doc(text)
A:spacy.tests.test_scorer.eg->Example(pred, gold)
A:spacy.tests.test_scorer.gold2->set()
A:spacy.tests.test_scorer.a->PRFScore()
A:spacy.tests.test_scorer.b->PRFScore()
A:spacy.tests.test_scorer.gold_doc->en_tokenizer(text)
A:spacy.tests.test_scorer.scores1->spacy.scorer.Scorer.score_cats([example], 'cats', labels=list(gold_doc.cats.keys()), multi_label=False, positive_label='POSITIVE', threshold=0.1)
A:spacy.tests.test_scorer.scores2->spacy.scorer.Scorer.score_cats([example], 'cats', labels=list(gold_doc.cats.keys()), multi_label=False, positive_label='POSITIVE', threshold=0.9)
spacy.tests.test_scorer.sented_doc()
spacy.tests.test_scorer.tagged_doc()
spacy.tests.test_scorer.test_las_per_type(en_vocab)
spacy.tests.test_scorer.test_ner_per_type(en_vocab)
spacy.tests.test_scorer.test_partial_annotation(en_tokenizer)
spacy.tests.test_scorer.test_prf_score()
spacy.tests.test_scorer.test_roc_auc_score()
spacy.tests.test_scorer.test_score_cats(en_tokenizer)
spacy.tests.test_scorer.test_score_spans()
spacy.tests.test_scorer.test_sents(sented_doc)
spacy.tests.test_scorer.test_tag_score(tagged_doc)
spacy.tests.test_scorer.test_tokenization(sented_doc)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/test_cli.py----------------------------------------
A:spacy.tests.test_cli.docs->list(conllu_to_docs(input_data))
A:spacy.tests.test_cli.nlp->spacy.blank('en')
A:spacy.tests.test_cli.example->spacy.training.Example.from_dict(nlp.make_doc(''), {})
A:spacy.tests.test_cli.source_nlp->spacy.lang.en.English.from_config(source_cfg)
A:spacy.tests.test_cli.base_cfg->Config(base_cfg)
A:spacy.tests.test_cli.filled_cfg->load_config(output_path)
A:spacy.tests.test_cli.cfg->load_project_config(d)
A:spacy.tests.test_cli.raw_data->info(tmp_dir, exclude=[''])
A:spacy.tests.test_cli.input_data->'\n'.join(lines)
A:spacy.tests.test_cli.converted_docs->list(conll_ner_to_docs(input_data, n_sents=10))
A:spacy.tests.test_cli.biluo_tags->offsets_to_biluo_tags(converted_docs[0], ent_offsets, missing='O')
A:spacy.tests.test_cli.converted->docs_to_json(converted_docs)
A:spacy.tests.test_cli.errors->validate(ProjectConfigSchema, config)
A:spacy.tests.test_cli.result->list(DocBin().from_disk(output).get_docs(nlp.vocab))
A:spacy.tests.test_cli.config->init_config(lang='nl', pipeline=[component_name], optimize='efficiency', gpu=False)
A:spacy.tests.test_cli.spec->SpecifierSet('==' + about.__version__)
A:spacy.tests.test_cli.compatibility->get_compatibility()
A:spacy.tests.test_cli.version->get_version(model_name, compatibility)
A:spacy.tests.test_cli.(model_pkgs, compat)->get_model_pkgs()
A:spacy.tests.test_cli.spacy_version->get_minor_version(about.__version__)
A:spacy.tests.test_cli.current_compat->compat.get(spacy_version, {})
A:spacy.tests.test_cli.component->spacy.blank('en').add_pipe(component_name)
A:spacy.tests.test_cli.nlp2->load_model_from_config(config, auto_fill=True)
A:spacy.tests.test_cli.pipe->spacy.blank('en').add_pipe(factory_name, name=pipe_name)
A:spacy.tests.test_cli.pred->Doc(nlp.vocab, words=['Welcome', 'to', 'the', 'Bank', 'of', 'China', '.'])
A:spacy.tests.test_cli.ref->Doc(nlp.vocab, words=['Welcome', 'to', 'the', 'Bank', 'of', 'China', '.'])
A:spacy.tests.test_cli.eg->Example(pred, ref)
A:spacy.tests.test_cli.data->_compile_gold(examples, ['spancat'], nlp, True)
A:spacy.tests.test_cli.expected->Counter({'china': 0.5, 'bank': 0.25, 'of': 0.25})
A:spacy.tests.test_cli.freq_distribution->_get_distribution(docs, normalize=True)
A:spacy.tests.test_cli.p->Counter({'a': 0.5, 'b': 0.25})
A:spacy.tests.test_cli.q->Counter({'a': 0.25, 'b': 0.5, 'c': 0.15, 'd': 0.1})
A:spacy.tests.test_cli.span_characteristics->_get_span_characteristics(examples=examples, compiled_gold=data, spans_key=spans_key)
A:spacy.tests.test_cli.span_freqs->_get_spans_length_freq_dist(sample_span_lengths, threshold)
A:spacy.tests.test_cli.doc->spacy.blank('en').make_doc(t[0])
A:spacy.tests.test_cli.docbin->DocBin(store_user_data=True)
A:spacy.tests.test_cli.remote->RemoteStorage(d / 'root', str(d / 'remote'))
A:spacy.tests.test_cli.thresholds->numpy.linspace(0, 1, 10)
A:spacy.tests.test_cli.new_nlp->English()
A:spacy.tests.test_cli.new_examples->make_examples(new_nlp)
A:spacy.tests.test_cli.(nlp, examples)->init_nlp()
A:spacy.tests.test_cli.res->find_threshold(model=nlp_dir, data_path=docs_dir / 'docs.spacy', pipe_name='spancat', threshold_key='threshold', scores_key='spans_sc_f', silent=True)
A:spacy.tests.test_cli.(nlp, _)->init_nlp()
spacy.tests.test_cli.test_applycli_docbin()
spacy.tests.test_cli.test_applycli_empty_dir()
spacy.tests.test_cli.test_applycli_jsonl()
spacy.tests.test_cli.test_applycli_mixed()
spacy.tests.test_cli.test_applycli_txt()
spacy.tests.test_cli.test_applycli_user_data()
spacy.tests.test_cli.test_cli_converters_conll_ner_to_docs()
spacy.tests.test_cli.test_cli_converters_conllu_empty_heads_ner()
spacy.tests.test_cli.test_cli_converters_conllu_to_docs()
spacy.tests.test_cli.test_cli_converters_conllu_to_docs_name_ner_map(lines)
spacy.tests.test_cli.test_cli_converters_conllu_to_docs_subtokens()
spacy.tests.test_cli.test_cli_converters_iob_to_docs()
spacy.tests.test_cli.test_cli_find_threshold(capsys)
spacy.tests.test_cli.test_cli_info()
spacy.tests.test_cli.test_debug_data_compile_gold()
spacy.tests.test_cli.test_debug_data_compile_gold_for_spans()
spacy.tests.test_cli.test_download_compatibility()
spacy.tests.test_cli.test_ensure_print_span_characteristics_wont_fail()
spacy.tests.test_cli.test_frequency_distribution_is_correct()
spacy.tests.test_cli.test_get_labels_from_model(factory_name,pipe_name)
spacy.tests.test_cli.test_get_span_characteristics_return_value()
spacy.tests.test_cli.test_get_third_party_dependencies()
spacy.tests.test_cli.test_init_config(lang,pipeline,optimize,pretraining)
spacy.tests.test_cli.test_init_labels(component_name)
spacy.tests.test_cli.test_is_subpath_of(parent,child,expected)
spacy.tests.test_cli.test_issue11235()
spacy.tests.test_cli.test_issue4924()
spacy.tests.test_cli.test_issue7055()
spacy.tests.test_cli.test_kl_divergence_computation_is_correct()
spacy.tests.test_cli.test_local_remote_storage()
spacy.tests.test_cli.test_local_remote_storage_pull_missing()
spacy.tests.test_cli.test_model_recommendations()
spacy.tests.test_cli.test_parse_cli_overrides()
spacy.tests.test_cli.test_parse_config_overrides(args,expected)
spacy.tests.test_cli.test_parse_config_overrides_invalid(args)
spacy.tests.test_cli.test_parse_config_overrides_invalid_2(args)
spacy.tests.test_cli.test_permitted_package_names()
spacy.tests.test_cli.test_project_check_requirements(reqs,output)
spacy.tests.test_cli.test_project_config_interpolation(int_value)
spacy.tests.test_cli.test_project_config_interpolation_env()
spacy.tests.test_cli.test_project_config_interpolation_override(greeting)
spacy.tests.test_cli.test_project_config_validation1(config)
spacy.tests.test_cli.test_project_config_validation2(config,n_errors)
spacy.tests.test_cli.test_project_config_validation_full()
spacy.tests.test_cli.test_span_length_freq_dist_output_must_be_correct()
spacy.tests.test_cli.test_span_length_freq_dist_threshold_must_be_correct(threshold)
spacy.tests.test_cli.test_string_to_list(value)
spacy.tests.test_cli.test_string_to_list_intify(value)
spacy.tests.test_cli.test_upload_download_local_file()
spacy.tests.test_cli.test_validate_compatibility_table()
spacy.tests.test_cli.test_walk_directory()


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/test_pickles.py----------------------------------------
A:spacy.tests.test_pickles.stringstore->StringStore()
A:spacy.tests.test_pickles.data->srsly.pickle_dumps(doc)
A:spacy.tests.test_pickles.unpickled->srsly.pickle_loads(data)
A:spacy.tests.test_pickles.vocab->Vocab(lex_attr_getters={int(NORM): lambda string: string[:-1]}, get_noun_chunks=English.Defaults.syntax_iterators.get('noun_chunks'))
A:spacy.tests.test_pickles.doc->Doc(en_vocab, words=words, deps=deps, heads=heads)
spacy.tests.test_pickles.test_pickle_doc(en_vocab)
spacy.tests.test_pickles.test_pickle_string_store(text1,text2)
spacy.tests.test_pickles.test_pickle_vocab(text1,text2)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/enable_gpu.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/test_errors.py----------------------------------------
spacy.tests.test_Errors(metaclass=ErrorsWithCodes)
spacy.tests.test_errors.Errors(metaclass=ErrorsWithCodes)
spacy.tests.test_errors.test_add_codes()


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/test_models.py----------------------------------------
A:spacy.tests.test_models.nlp->English()
A:spacy.tests.test_models.dY->get_gradient(model, Y)
A:spacy.tests.test_models.embed->MultiHashEmbed(width=32, rows=[1000, 50, 250], attrs=['NORM', 'PREFIX', 'SHAPE'], include_static_vectors=False)
A:spacy.tests.test_models.model1->get_updated_model()
A:spacy.tests.test_models.model2->get_updated_model()
A:spacy.tests.test_models.params1->get_all_params(model1)
A:spacy.tests.test_models.params2->get_all_params(model2)
A:spacy.tests.test_models.Y1->get_updated_model().ops.to_numpy(Y1)
A:spacy.tests.test_models.Y2->get_updated_model().ops.to_numpy(Y2)
A:spacy.tests.test_models.tok2vec1->get_updated_model().get_ref('tok2vec').predict(get_X())
A:spacy.tests.test_models.tok2vec2->get_updated_model().get_ref('tok2vec').predict(get_X())
A:spacy.tests.test_models.y1->get_updated_model().ops.to_numpy(y1)
A:spacy.tests.test_models.y2->get_updated_model().ops.to_numpy(y2)
A:spacy.tests.test_models.optimizer->Adam(0.001)
A:spacy.tests.test_models.model->build_spancat_model(tok2vec, reduce_mean(), chain(Relu(nO=nO), Logistic())).initialize(X=(docs, spans))
A:spacy.tests.test_models.initial_params->get_all_params(model)
A:spacy.tests.test_models.(Y, get_dX)->build_spancat_model(tok2vec, reduce_mean(), chain(Relu(nO=nO), Logistic())).initialize(X=(docs, spans)).begin_update(get_X())
A:spacy.tests.test_models.updated_params->get_all_params(model)
A:spacy.tests.test_models.(output, backprop)->build_spancat_model(tok2vec, reduce_mean(), chain(Relu(nO=nO), Logistic())).initialize(X=(docs, spans)).begin_update(docs)
A:spacy.tests.test_models.spans->Ragged(tok2vec.ops.asarray([[s.start, s.end] for s in spans_list], dtype='i'), tok2vec.ops.asarray(lengths, dtype='i'))
A:spacy.tests.test_models.x_lengths->build_spancat_model(tok2vec, reduce_mean(), chain(Relu(nO=nO), Logistic())).initialize(X=(docs, spans)).ops.asarray([5, 10], dtype='i')
A:spacy.tests.test_models.indices->_get_span_indices(model.ops, spans, x_lengths)
A:spacy.tests.test_models.X->Ragged(model.ops.alloc2f(15, 4), model.ops.asarray([5, 10], dtype='i'))
A:spacy.tests.test_models.(Y, backprop)->model((docs, spans), is_train=True)
A:spacy.tests.test_models.(dX, spans2)->backprop(Y)
A:spacy.tests.test_models.tok2vec->build_Tok2Vec_model(**get_tok2vec_kwargs())
A:spacy.tests.test_models.docs->get_docs()
spacy.tests.test_models.get_all_params(model)
spacy.tests.test_models.get_docs()
spacy.tests.test_models.get_gradient(model,Y)
spacy.tests.test_models.get_textcat_bow_kwargs()
spacy.tests.test_models.get_textcat_cnn_kwargs()
spacy.tests.test_models.get_tok2vec_kwargs()
spacy.tests.test_models.make_test_tok2vec()
spacy.tests.test_models.test_empty_docs(model_func,kwargs)
spacy.tests.test_models.test_extract_spans_forward_backward()
spacy.tests.test_models.test_extract_spans_span_indices()
spacy.tests.test_models.test_init_extract_spans()
spacy.tests.test_models.test_models_initialize_consistently(seed,model_func,kwargs)
spacy.tests.test_models.test_models_predict_consistently(seed,model_func,kwargs,get_X)
spacy.tests.test_models.test_models_update_consistently(seed,dropout,model_func,kwargs,get_X)
spacy.tests.test_models.test_multi_hash_embed()
spacy.tests.test_models.test_spancat_model_forward_backward(nO=5)
spacy.tests.test_models.test_spancat_model_init()


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/test_architectures.py----------------------------------------
A:spacy.tests.test_architectures.arch->spacy.registry.architectures.get('my_test_function')
spacy.tests.test_architectures.test_get_architecture()


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/test_ty.py----------------------------------------
A:spacy.tests.test_ty.nlp->spacy.blank('en')
A:spacy.tests.test_ty.tok2vec->spacy.blank('en').create_pipe('tok2vec')
A:spacy.tests.test_ty.tagger->spacy.blank('en').create_pipe('tagger')
A:spacy.tests.test_ty.entity_ruler->spacy.blank('en').create_pipe('entity_ruler')
spacy.tests.test_ty.test_component_types()


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/test_cli_app.py----------------------------------------
A:spacy.tests.test_cli_app.result->CliRunner().invoke(app, ['convert', str(d_in), str(d_out)])
A:spacy.tests.test_cli_app.out_files->os.listdir(d_out)
A:spacy.tests.test_cli_app.result_benchmark->CliRunner().invoke(app, ['benchmark', 'accuracy', '--help'])
A:spacy.tests.test_cli_app.result_evaluate->CliRunner().invoke(app, ['evaluate', '--help'])
spacy.tests.test_cli_app.test_benchmark_accuracy_alias()
spacy.tests.test_cli_app.test_convert_auto()
spacy.tests.test_cli_app.test_convert_auto_conflict()


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/test_displacy.py----------------------------------------
A:spacy.tests.test_displacy.doc->Doc(en_vocab, words=['But', 'Google', 'is', 'starting', 'from', 'behind'])
A:spacy.tests.test_displacy.html->spacy.displacy.render(doc, style='ent', manual=True)
A:spacy.tests.test_displacy.doc.tensor->numpy.zeros((len(words), 96), dtype='float32')
A:spacy.tests.test_displacy.dep_html->spacy.displacy.render(example_dep, style='dep', manual=True)
A:spacy.tests.test_displacy.ent_html->spacy.displacy.render(example_ent, style='ent', manual=True)
A:spacy.tests.test_displacy.doc.user_data['test']->set()
A:spacy.tests.test_displacy.renderer->EntityRenderer({'ents': ents, 'colors': colors})
A:spacy.tests.test_displacy.nlp->Persian()
A:spacy.tests.test_displacy.found->spacy.displacy.render(doc, style='ent', manual=True).count('</br>')
A:spacy.tests.test_displacy.spans->spacy.displacy.parse_spans(doc)
A:spacy.tests.test_displacy.ents->spacy.displacy.parse_ents(doc, {'kb_url_template': 'https://www.wikidata.org/wiki/{}'})
A:spacy.tests.test_displacy.deps->spacy.displacy.parse_deps(doc)
A:spacy.tests.test_displacy.result->EntityRenderer({'ents': ents, 'colors': colors}).render_ents('abcde', spans, None).split('\n\n')
spacy.tests.test_displacy.test_displacy_invalid_arcs()
spacy.tests.test_displacy.test_displacy_manual_sorted_entities()
spacy.tests.test_displacy.test_displacy_options_case()
spacy.tests.test_displacy.test_displacy_parse_deps(en_vocab)
spacy.tests.test_displacy.test_displacy_parse_empty_spans_key(en_vocab)
spacy.tests.test_displacy.test_displacy_parse_ents(en_vocab)
spacy.tests.test_displacy.test_displacy_parse_ents_with_kb_id_options(en_vocab)
spacy.tests.test_displacy.test_displacy_parse_spans(en_vocab)
spacy.tests.test_displacy.test_displacy_parse_spans_different_spans_key(en_vocab)
spacy.tests.test_displacy.test_displacy_parse_spans_with_kb_id_options(en_vocab)
spacy.tests.test_displacy.test_displacy_raises_for_wrong_type(en_vocab)
spacy.tests.test_displacy.test_displacy_render_wrapper(en_vocab)
spacy.tests.test_displacy.test_displacy_rtl()
spacy.tests.test_displacy.test_displacy_spans(en_vocab)
spacy.tests.test_displacy.test_issue2361(de_vocab)
spacy.tests.test_displacy.test_issue2728(en_vocab)
spacy.tests.test_displacy.test_issue3288(en_vocab)
spacy.tests.test_displacy.test_issue3531()
spacy.tests.test_displacy.test_issue3882(en_vocab)
spacy.tests.test_displacy.test_issue5447()
spacy.tests.test_displacy.test_issue5838()


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/conftest.py----------------------------------------
A:spacy.tests.conftest.issues->getopt('issue')
A:spacy.tests.conftest.nlp->get_lang_class('zh').from_config(config)
spacy.tests.conftest.af_tokenizer()
spacy.tests.conftest.am_tokenizer()
spacy.tests.conftest.ar_tokenizer()
spacy.tests.conftest.bg_tokenizer()
spacy.tests.conftest.bn_tokenizer()
spacy.tests.conftest.ca_tokenizer()
spacy.tests.conftest.cs_tokenizer()
spacy.tests.conftest.da_tokenizer()
spacy.tests.conftest.de_tokenizer()
spacy.tests.conftest.de_vocab()
spacy.tests.conftest.dsb_tokenizer()
spacy.tests.conftest.el_tokenizer()
spacy.tests.conftest.en_parser(en_vocab)
spacy.tests.conftest.en_tokenizer()
spacy.tests.conftest.en_vocab()
spacy.tests.conftest.es_tokenizer()
spacy.tests.conftest.es_vocab()
spacy.tests.conftest.et_tokenizer()
spacy.tests.conftest.eu_tokenizer()
spacy.tests.conftest.fa_tokenizer()
spacy.tests.conftest.fi_tokenizer()
spacy.tests.conftest.fr_tokenizer()
spacy.tests.conftest.fr_vocab()
spacy.tests.conftest.ga_tokenizer()
spacy.tests.conftest.grc_tokenizer()
spacy.tests.conftest.gu_tokenizer()
spacy.tests.conftest.he_tokenizer()
spacy.tests.conftest.hi_tokenizer()
spacy.tests.conftest.hr_tokenizer()
spacy.tests.conftest.hsb_tokenizer()
spacy.tests.conftest.hu_tokenizer()
spacy.tests.conftest.hy_tokenizer()
spacy.tests.conftest.id_tokenizer()
spacy.tests.conftest.is_tokenizer()
spacy.tests.conftest.it_tokenizer()
spacy.tests.conftest.it_vocab()
spacy.tests.conftest.ja_tokenizer()
spacy.tests.conftest.ko_tokenizer()
spacy.tests.conftest.ko_tokenizer_tokenizer()
spacy.tests.conftest.ky_tokenizer()
spacy.tests.conftest.la_tokenizer()
spacy.tests.conftest.lb_tokenizer()
spacy.tests.conftest.lg_tokenizer()
spacy.tests.conftest.lt_tokenizer()
spacy.tests.conftest.lv_tokenizer()
spacy.tests.conftest.mk_tokenizer()
spacy.tests.conftest.ml_tokenizer()
spacy.tests.conftest.nb_tokenizer()
spacy.tests.conftest.ne_tokenizer()
spacy.tests.conftest.nl_tokenizer()
spacy.tests.conftest.nl_vocab()
spacy.tests.conftest.pl_tokenizer()
spacy.tests.conftest.pt_tokenizer()
spacy.tests.conftest.pt_vocab()
spacy.tests.conftest.pytest_addoption(parser)
spacy.tests.conftest.pytest_runtest_setup(item)
spacy.tests.conftest.ro_tokenizer()
spacy.tests.conftest.ru_lemmatizer()
spacy.tests.conftest.ru_lookup_lemmatizer()
spacy.tests.conftest.ru_tokenizer()
spacy.tests.conftest.sa_tokenizer()
spacy.tests.conftest.sk_tokenizer()
spacy.tests.conftest.sl_tokenizer()
spacy.tests.conftest.sq_tokenizer()
spacy.tests.conftest.sr_tokenizer()
spacy.tests.conftest.sv_tokenizer()
spacy.tests.conftest.ta_tokenizer()
spacy.tests.conftest.th_tokenizer()
spacy.tests.conftest.ti_tokenizer()
spacy.tests.conftest.tl_tokenizer()
spacy.tests.conftest.tokenizer()
spacy.tests.conftest.tr_tokenizer()
spacy.tests.conftest.tt_tokenizer()
spacy.tests.conftest.uk_lemmatizer()
spacy.tests.conftest.uk_lookup_lemmatizer()
spacy.tests.conftest.uk_tokenizer()
spacy.tests.conftest.ur_tokenizer()
spacy.tests.conftest.vi_tokenizer()
spacy.tests.conftest.xx_tokenizer()
spacy.tests.conftest.yo_tokenizer()
spacy.tests.conftest.zh_tokenizer_char()
spacy.tests.conftest.zh_tokenizer_jieba()
spacy.tests.conftest.zh_tokenizer_pkuseg()


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/pipeline/test_attributeruler.py----------------------------------------
A:spacy.tests.pipeline.test_attributeruler.doc->nlp(text)
A:spacy.tests.pipeline.test_attributeruler.a->nlp.add_pipe('attribute_ruler')
A:spacy.tests.pipeline.test_attributeruler.ruler->nlp.add_pipe('attribute_ruler')
A:spacy.tests.pipeline.test_attributeruler.scores->nlp.evaluate(dev_examples, scorer_cfg={'weird_score': 0.23456})
A:spacy.tests.pipeline.test_attributeruler.a_reloaded->AttributeRuler(nlp.vocab).from_bytes(a.to_bytes())
A:spacy.tests.pipeline.test_attributeruler.doc1->a_reloaded(nlp.make_doc(text))
A:spacy.tests.pipeline.test_attributeruler.nlp2->spacy.util.load_model_from_path(tmp_dir)
A:spacy.tests.pipeline.test_attributeruler.doc2->nlp2(text)
spacy.tests.pipeline.test_attributeruler.check_morph_rules(ruler)
spacy.tests.pipeline.test_attributeruler.check_tag_map(ruler)
spacy.tests.pipeline.test_attributeruler.morph_rules()
spacy.tests.pipeline.test_attributeruler.nlp()
spacy.tests.pipeline.test_attributeruler.pattern_dicts()
spacy.tests.pipeline.test_attributeruler.tag_map()
spacy.tests.pipeline.test_attributeruler.test_attributeruler_indices(nlp)
spacy.tests.pipeline.test_attributeruler.test_attributeruler_init(nlp,pattern_dicts)
spacy.tests.pipeline.test_attributeruler.test_attributeruler_init_clear(nlp,pattern_dicts)
spacy.tests.pipeline.test_attributeruler.test_attributeruler_init_patterns(nlp,pattern_dicts)
spacy.tests.pipeline.test_attributeruler.test_attributeruler_morph_rules(nlp,morph_rules)
spacy.tests.pipeline.test_attributeruler.test_attributeruler_morph_rules_initialize(nlp,morph_rules)
spacy.tests.pipeline.test_attributeruler.test_attributeruler_patterns_prop(nlp,pattern_dicts)
spacy.tests.pipeline.test_attributeruler.test_attributeruler_rule_order(nlp)
spacy.tests.pipeline.test_attributeruler.test_attributeruler_score(nlp,pattern_dicts)
spacy.tests.pipeline.test_attributeruler.test_attributeruler_serialize(nlp,pattern_dicts)
spacy.tests.pipeline.test_attributeruler.test_attributeruler_tag_map(nlp,tag_map)
spacy.tests.pipeline.test_attributeruler.test_attributeruler_tag_map_initialize(nlp,tag_map)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/pipeline/test_morphologizer.py----------------------------------------
A:spacy.tests.pipeline.test_morphologizer.nlp->English()
A:spacy.tests.pipeline.test_morphologizer.morphologizer->English().get_pipe('morphologizer')
A:spacy.tests.pipeline.test_morphologizer.optimizer->English().initialize(get_examples=lambda : train_examples)
A:spacy.tests.pipeline.test_morphologizer.doc->nlp(test_text)
A:spacy.tests.pipeline.test_morphologizer.nlp2->spacy.util.load_model_from_path(tmp_dir)
A:spacy.tests.pipeline.test_morphologizer.doc2->nlp2(test_text)
spacy.tests.pipeline.test_morphologizer.test_implicit_label()
spacy.tests.pipeline.test_morphologizer.test_initialize_examples()
spacy.tests.pipeline.test_morphologizer.test_label_types()
spacy.tests.pipeline.test_morphologizer.test_no_label()
spacy.tests.pipeline.test_morphologizer.test_no_resize()
spacy.tests.pipeline.test_morphologizer.test_overfitting_IO()


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/pipeline/test_tok2vec.py----------------------------------------
A:spacy.tests.pipeline.test_tok2vec.vocab->Vocab()
A:spacy.tests.pipeline.test_tok2vec.doc->nlp(test_text)
A:spacy.tests.pipeline.test_tok2vec.tok2vec->spacy.util.load_model_from_config(orig_config, auto_fill=True, validate=True).get_pipe('tok2vec')
A:spacy.tests.pipeline.test_tok2vec.(vectors, backprop)->spacy.util.load_model_from_config(orig_config, auto_fill=True, validate=True).get_pipe('tok2vec').begin_update(docs)
A:spacy.tests.pipeline.test_tok2vec.batch->get_batch(batch_size)
A:spacy.tests.pipeline.test_tok2vec.embed->spacy.util.registry.get('architectures', embed_arch)
A:spacy.tests.pipeline.test_tok2vec.encode->spacy.util.registry.get('architectures', encode_arch)
A:spacy.tests.pipeline.test_tok2vec.tok2vec_model->spacy.util.registry.get('architectures', tok2vec_arch)
A:spacy.tests.pipeline.test_tok2vec.docs->list(nlp.pipe(['Eat blue ham', 'I like green eggs']))
A:spacy.tests.pipeline.test_tok2vec.nlp->spacy.util.load_model_from_config(orig_config, auto_fill=True, validate=True)
A:spacy.tests.pipeline.test_tok2vec.orig_config->Config().from_str(cfg_string_multi_textcat)
A:spacy.tests.pipeline.test_tok2vec.ops->get_current_ops()
A:spacy.tests.pipeline.test_tok2vec.tagger->spacy.util.load_model_from_config(orig_config, auto_fill=True, validate=True).get_pipe('tagger')
A:spacy.tests.pipeline.test_tok2vec.tagger_tok2vec->spacy.util.load_model_from_config(orig_config, auto_fill=True, validate=True).get_pipe('tagger').model.get_ref('tok2vec')
A:spacy.tests.pipeline.test_tok2vec.optimizer->spacy.util.load_model_from_config(orig_config, auto_fill=True, validate=True).initialize(lambda : train_examples)
A:spacy.tests.pipeline.test_tok2vec.(Y, get_dX)->spacy.util.load_model_from_config(orig_config, auto_fill=True, validate=True).get_pipe('tagger').model.begin_update(docs)
A:spacy.tests.pipeline.test_tok2vec.nlp2->spacy.util.load_model_from_path(tmp_dir)
A:spacy.tests.pipeline.test_tok2vec.doc2->nlp2(test_text)
A:spacy.tests.pipeline.test_tok2vec.ner->spacy.util.load_model_from_config(new_config, auto_fill=True).get_pipe('ner')
A:spacy.tests.pipeline.test_tok2vec.base_model->str(dir_path)
A:spacy.tests.pipeline.test_tok2vec.new_nlp->spacy.util.load_model_from_config(new_config, auto_fill=True)
A:spacy.tests.pipeline.test_tok2vec.textcat->spacy.util.load_model_from_config(orig_config, auto_fill=True, validate=True).get_pipe('textcat_multilabel')
A:spacy.tests.pipeline.test_tok2vec.textcat_tok2vec->spacy.util.load_model_from_config(orig_config, auto_fill=True, validate=True).get_pipe('textcat_multilabel').model.get_ref('tok2vec')
spacy.tests.pipeline.test_tok2vec.test_empty_doc()
spacy.tests.pipeline.test_tok2vec.test_init_tok2vec()
spacy.tests.pipeline.test_tok2vec.test_replace_listeners()
spacy.tests.pipeline.test_tok2vec.test_replace_listeners_from_config()
spacy.tests.pipeline.test_tok2vec.test_tok2vec_batch_sizes(batch_size,width,embed_size)
spacy.tests.pipeline.test_tok2vec.test_tok2vec_configs(width,tok2vec_arch,embed_arch,embed_config,encode_arch,encode_config)
spacy.tests.pipeline.test_tok2vec.test_tok2vec_frozen_not_annotating()
spacy.tests.pipeline.test_tok2vec.test_tok2vec_frozen_overfitting()
spacy.tests.pipeline.test_tok2vec.test_tok2vec_listener(with_vectors)
spacy.tests.pipeline.test_tok2vec.test_tok2vec_listener_callback()
spacy.tests.pipeline.test_tok2vec.test_tok2vec_listener_overfitting()
spacy.tests.pipeline.test_tok2vec.test_tok2vec_listeners_textcat()


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/pipeline/test_spancat.py----------------------------------------
A:spacy.tests.pipeline.test_spancat.OPS->get_current_ops()
A:spacy.tests.pipeline.test_spancat.eg->spacy.training.Example.from_dict(nlp.make_doc(t[0]), t[1])
A:spacy.tests.pipeline.test_spancat.nlp->Language()
A:spacy.tests.pipeline.test_spancat.spancat->Language().add_pipe('spancat', config={'spans_key': SPAN_KEY})
A:spacy.tests.pipeline.test_spancat.train_examples->make_examples(nlp)
A:spacy.tests.pipeline.test_spancat.doc->nlp(test_text)
A:spacy.tests.pipeline.test_spancat.ngram_suggester->spacy.util.registry.misc.get('spacy.ngram_suggester.v1')(sizes=[1])
A:spacy.tests.pipeline.test_spancat.scores->Language().evaluate(train_examples)
A:spacy.tests.pipeline.test_spancat.spangroup->Language().add_pipe('spancat', config={'spans_key': SPAN_KEY})._make_span_group(doc, indices, scores, labels)
A:spacy.tests.pipeline.test_spancat.ngrams->ngram_suggester(docs)
A:spacy.tests.pipeline.test_spancat.spans_set->set()
A:spacy.tests.pipeline.test_spancat.size_suggester->spacy.util.registry.misc.get('spacy.ngram_suggester.v1')(sizes=[1, 2, 3])
A:spacy.tests.pipeline.test_spancat.suggester_factory->spacy.util.registry.misc.get('spacy.ngram_range_suggester.v1')
A:spacy.tests.pipeline.test_spancat.range_suggester->suggester_factory(min_size=2, max_size=4)
A:spacy.tests.pipeline.test_spancat.ngrams_1->size_suggester(docs)
A:spacy.tests.pipeline.test_spancat.ngrams_2->range_suggester(docs)
A:spacy.tests.pipeline.test_spancat.ngrams_3->range_suggester(docs)
A:spacy.tests.pipeline.test_spancat.optimizer->Language().initialize(get_examples=lambda : train_examples)
A:spacy.tests.pipeline.test_spancat.nlp2->spacy.util.load_model_from_path(tmp_dir)
A:spacy.tests.pipeline.test_spancat.doc2->nlp2(test_text)
A:spacy.tests.pipeline.test_spancat.ops->get_current_ops()
A:spacy.tests.pipeline.test_spancat.spans->get_current_ops().asarray2i(spans)
A:spacy.tests.pipeline.test_spancat.lengths_array->get_current_ops().asarray1i(lengths)
A:spacy.tests.pipeline.test_spancat.output->Ragged(ops.xp.zeros((0, 0), dtype='i'), lengths_array)
spacy.tests.pipeline.test_spancat.make_examples(nlp,data=TRAIN_DATA)
spacy.tests.pipeline.test_spancat.test_doc_gc()
spacy.tests.pipeline.test_spancat.test_explicit_labels()
spacy.tests.pipeline.test_spancat.test_implicit_labels()
spacy.tests.pipeline.test_spancat.test_make_spangroup(max_positive,nr_results)
spacy.tests.pipeline.test_spancat.test_ngram_sizes(en_tokenizer)
spacy.tests.pipeline.test_spancat.test_ngram_suggester(en_tokenizer)
spacy.tests.pipeline.test_spancat.test_no_label()
spacy.tests.pipeline.test_spancat.test_no_resize()
spacy.tests.pipeline.test_spancat.test_overfitting_IO()
spacy.tests.pipeline.test_spancat.test_overfitting_IO_overlapping()
spacy.tests.pipeline.test_spancat.test_set_candidates()
spacy.tests.pipeline.test_spancat.test_zero_suggestions()


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/pipeline/test_tagger.py----------------------------------------
A:spacy.tests.pipeline.test_tagger.nlp->English()
A:spacy.tests.pipeline.test_tagger.example->spacy.training.Example.from_dict(nlp.make_doc(''), {'tags': []})
A:spacy.tests.pipeline.test_tagger.tagger->English().add_pipe('tagger')
A:spacy.tests.pipeline.test_tagger.optimizer->English().initialize(get_examples=lambda : train_examples)
A:spacy.tests.pipeline.test_tagger.batches->spacy.util.minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))
A:spacy.tests.pipeline.test_tagger.orig_tag_count->len(tagger.labels)
A:spacy.tests.pipeline.test_tagger.doc->nlp(test_text)
A:spacy.tests.pipeline.test_tagger.nlp2->spacy.util.load_model_from_path(tmp_dir)
A:spacy.tests.pipeline.test_tagger.doc2->nlp2(test_text)
A:spacy.tests.pipeline.test_tagger.neg_ex->spacy.training.Example.from_dict(nlp.make_doc(test_text), {'tags': ['!N', 'V', 'J', 'N']})
A:spacy.tests.pipeline.test_tagger.doc3->nlp(test_text)
spacy.tests.pipeline.test_tagger.test_implicit_label()
spacy.tests.pipeline.test_tagger.test_incomplete_data()
spacy.tests.pipeline.test_tagger.test_initialize_examples()
spacy.tests.pipeline.test_tagger.test_issue4348()
spacy.tests.pipeline.test_tagger.test_label_types()
spacy.tests.pipeline.test_tagger.test_no_data()
spacy.tests.pipeline.test_tagger.test_no_label()
spacy.tests.pipeline.test_tagger.test_no_resize()
spacy.tests.pipeline.test_tagger.test_overfitting_IO()
spacy.tests.pipeline.test_tagger.test_tagger_initialize_tag_map()
spacy.tests.pipeline.test_tagger.test_tagger_requires_labels()


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/pipeline/test_initialize.py----------------------------------------
A:spacy.tests.pipeline.test_initialize.nlp->English()
A:spacy.tests.pipeline.test_initialize.nlp.tokenizer->CustomTokenizer(nlp.tokenizer)
A:spacy.tests.pipeline.test_initialize.example->spacy.training.Example.from_dict(nlp('x'), {})
A:spacy.tests.pipeline.test_initialize.pipe->English().get_pipe(name)
spacy.tests.pipeline.test_initialize.test_initialize_arguments()


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/pipeline/test_lemmatizer.py----------------------------------------
A:spacy.tests.pipeline.test_lemmatizer.lookups->Lookups()
A:spacy.tests.pipeline.test_lemmatizer.nlp->English()
A:spacy.tests.pipeline.test_lemmatizer.lemmatizer->English().add_pipe('lemmatizer', config={'mode': 'rule'})
A:spacy.tests.pipeline.test_lemmatizer.lemmatizer.lookups->Lookups()
A:spacy.tests.pipeline.test_lemmatizer.doc->lemmatizer(doc)
A:spacy.tests.pipeline.test_lemmatizer.nlp2->spacy.util.load_model_from_path(tmp_dir)
A:spacy.tests.pipeline.test_lemmatizer.lemmatizer2->spacy.util.load_model_from_path(tmp_dir).add_pipe('lemmatizer', config={'mode': 'rule'})
A:spacy.tests.pipeline.test_lemmatizer.doc2->lemmatizer2(doc2)
spacy.tests.pipeline.test_lemmatizer.nlp()
spacy.tests.pipeline.test_lemmatizer.test_lemmatizer_config(nlp)
spacy.tests.pipeline.test_lemmatizer.test_lemmatizer_init(nlp)
spacy.tests.pipeline.test_lemmatizer.test_lemmatizer_serialize(nlp)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/pipeline/test_analysis.py----------------------------------------
A:spacy.tests.pipeline.test_analysis.nlp->Language()
A:spacy.tests.pipeline.test_analysis.test_component4_meta->Language().get_pipe_meta('c1')
A:spacy.tests.pipeline.test_analysis.mock->Mock()
A:spacy.tests.pipeline.test_analysis.mock.return_value->TestComponent5()
spacy.tests.pipeline.test_analysis.test_analysis_validate_attrs_invalid(attr)
spacy.tests.pipeline.test_analysis.test_analysis_validate_attrs_remove_pipe()
spacy.tests.pipeline.test_analysis.test_analysis_validate_attrs_valid()
spacy.tests.pipeline.test_analysis.test_component_decorator_assigns()
spacy.tests.pipeline.test_analysis.test_component_factories_class_func()


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/pipeline/test_annotates_on_update.py----------------------------------------
A:spacy.tests.pipeline.test_annotates_on_update.nlp->load_model_from_config(orig_config, auto_fill=True, validate=True)
A:spacy.tests.pipeline.test_annotates_on_update.doc->load_model_from_config(orig_config, auto_fill=True, validate=True).make_doc(text)
A:spacy.tests.pipeline.test_annotates_on_update.orig_config->Config().from_str(config_str)
spacy.tests.pipeline.test_annotates_on_update.config_str()
spacy.tests.pipeline.test_annotates_on_update.test_annotates_on_update()
spacy.tests.pipeline.test_annotates_on_update.test_annotating_components_from_config(config_str)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/pipeline/test_edit_tree_lemmatizer.py----------------------------------------
A:spacy.tests.pipeline.test_edit_tree_lemmatizer.nlp->English()
A:spacy.tests.pipeline.test_edit_tree_lemmatizer.lemmatizer->English().add_pipe('trainable_lemmatizer')
A:spacy.tests.pipeline.test_edit_tree_lemmatizer.nlp2->English()
A:spacy.tests.pipeline.test_edit_tree_lemmatizer.lemmatizer2->English().add_pipe('trainable_lemmatizer')
A:spacy.tests.pipeline.test_edit_tree_lemmatizer.optimizer->English().initialize(get_examples=lambda : train_examples)
A:spacy.tests.pipeline.test_edit_tree_lemmatizer.doc->nlp(test_text)
A:spacy.tests.pipeline.test_edit_tree_lemmatizer.(scores, _)->English().add_pipe('trainable_lemmatizer').model([eg.predicted for eg in train_examples], is_train=True)
A:spacy.tests.pipeline.test_edit_tree_lemmatizer.(_, dX)->English().add_pipe('trainable_lemmatizer').get_loss(train_examples, scores)
A:spacy.tests.pipeline.test_edit_tree_lemmatizer.doc2->nlp2(test_text)
A:spacy.tests.pipeline.test_edit_tree_lemmatizer.nlp_bytes->pickle.dumps(nlp)
A:spacy.tests.pipeline.test_edit_tree_lemmatizer.nlp3->English()
A:spacy.tests.pipeline.test_edit_tree_lemmatizer.doc3->nlp3(test_text)
A:spacy.tests.pipeline.test_edit_tree_lemmatizer.nlp4->pickle.loads(nlp_bytes)
A:spacy.tests.pipeline.test_edit_tree_lemmatizer.doc4->nlp4(test_text)
A:spacy.tests.pipeline.test_edit_tree_lemmatizer.strings->StringStore()
A:spacy.tests.pipeline.test_edit_tree_lemmatizer.trees->EditTrees(strings)
A:spacy.tests.pipeline.test_edit_tree_lemmatizer.tree->EditTrees(strings).add(form, lemma)
A:spacy.tests.pipeline.test_edit_tree_lemmatizer.b->EditTrees(strings).to_bytes()
A:spacy.tests.pipeline.test_edit_tree_lemmatizer.trees2->trees2.from_disk(trees_file).from_disk(trees_file)
A:spacy.tests.pipeline.test_edit_tree_lemmatizer.tree3->EditTrees(strings).add('deelt', 'delen')
A:spacy.tests.pipeline.test_edit_tree_lemmatizer.no_change->EditTrees(strings).add('xyz', 'xyz')
A:spacy.tests.pipeline.test_edit_tree_lemmatizer.empty->EditTrees(strings).add('', '')
spacy.tests.pipeline.test_edit_tree_lemmatizer.test_dutch()
spacy.tests.pipeline.test_edit_tree_lemmatizer.test_empty_strings()
spacy.tests.pipeline.test_edit_tree_lemmatizer.test_from_to_bytes()
spacy.tests.pipeline.test_edit_tree_lemmatizer.test_from_to_disk()
spacy.tests.pipeline.test_edit_tree_lemmatizer.test_incomplete_data()
spacy.tests.pipeline.test_edit_tree_lemmatizer.test_initialize_examples()
spacy.tests.pipeline.test_edit_tree_lemmatizer.test_initialize_from_labels()
spacy.tests.pipeline.test_edit_tree_lemmatizer.test_lemmatizer_label_data()
spacy.tests.pipeline.test_edit_tree_lemmatizer.test_lemmatizer_requires_labels()
spacy.tests.pipeline.test_edit_tree_lemmatizer.test_no_data()
spacy.tests.pipeline.test_edit_tree_lemmatizer.test_overfitting_IO()
spacy.tests.pipeline.test_edit_tree_lemmatizer.test_roundtrip(form,lemma)
spacy.tests.pipeline.test_edit_tree_lemmatizer.test_roundtrip_small_alphabet(form,lemma)
spacy.tests.pipeline.test_edit_tree_lemmatizer.test_unapplicable_trees()


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/pipeline/test_entity_linker.py----------------------------------------
A:spacy.tests.pipeline.test_entity_linker.nlp->English()
A:spacy.tests.pipeline.test_entity_linker.kb->InMemoryLookupKB(vocab, entity_vector_length=3)
A:spacy.tests.pipeline.test_entity_linker.dir_path->ensure_path(d)
A:spacy.tests.pipeline.test_entity_linker.kb2->InMemoryLookupKB(vocab=nlp.vocab, entity_vector_length=4)
A:spacy.tests.pipeline.test_entity_linker.ruler->English().add_pipe('entity_ruler', before='entity_linker')
A:spacy.tests.pipeline.test_entity_linker.doc->nlp(text)
A:spacy.tests.pipeline.test_entity_linker.example->spacy.training.Example.from_dict(doc, {'entities': entities, 'links': links, 'sent_starts': sent_starts})
A:spacy.tests.pipeline.test_entity_linker.mykb->InMemoryLookupKB(vocab, entity_vector_length=3)
A:spacy.tests.pipeline.test_entity_linker.entity_linker->English().add_pipe('entity_linker', last=True, config={'threshold': 0.99, 'model': config})
A:spacy.tests.pipeline.test_entity_linker.optimizer->English().initialize(get_examples=lambda : train_examples)
A:spacy.tests.pipeline.test_entity_linker.results->English().evaluate(train_examples)
A:spacy.tests.pipeline.test_entity_linker.kb1->InMemoryLookupKB(vocab=nlp.vocab, entity_vector_length=4)
A:spacy.tests.pipeline.test_entity_linker.mykb_new->InMemoryLookupKB(Vocab(), entity_vector_length=1)
A:spacy.tests.pipeline.test_entity_linker.q2_hash->InMemoryLookupKB(vocab, entity_vector_length=3).add_entity(entity='Q2', freq=12, entity_vector=[2])
A:spacy.tests.pipeline.test_entity_linker.adam_hash->InMemoryLookupKB(vocab, entity_vector_length=3).add_alias(alias='adam', entities=['Q2'], probabilities=[0.9])
A:spacy.tests.pipeline.test_entity_linker.candidates->InMemoryLookupKB(Vocab(), entity_vector_length=1).get_alias_candidates('adam')
A:spacy.tests.pipeline.test_entity_linker.kb_new_vocab->InMemoryLookupKB(Vocab(), entity_vector_length=1)
A:spacy.tests.pipeline.test_entity_linker.sent_doc->ent.sent.as_doc()
A:spacy.tests.pipeline.test_entity_linker.boston_ent->Span(doc, 3, 4, label='LOC', kb_id='Q1')
A:spacy.tests.pipeline.test_entity_linker.loc->nlp(text).vocab.strings.add('LOC')
A:spacy.tests.pipeline.test_entity_linker.q1->nlp(text).vocab.strings.add('Q1')
A:spacy.tests.pipeline.test_entity_linker.nlp2->English()
A:spacy.tests.pipeline.test_entity_linker.entity_linker2->English().get_pipe('entity_linker')
A:spacy.tests.pipeline.test_entity_linker.doc2->nlp('x y z')
A:spacy.tests.pipeline.test_entity_linker.nlp1->English()
A:spacy.tests.pipeline.test_entity_linker.kb_1->InMemoryLookupKB(nlp.vocab, entity_vector_length=3)
A:spacy.tests.pipeline.test_entity_linker.data->spacy.compat.pickle.dumps(nlp_1)
A:spacy.tests.pipeline.test_entity_linker.kb_2->kb_2.from_bytes(kb_bytes).from_bytes(kb_bytes)
A:spacy.tests.pipeline.test_entity_linker.nlp_1->English()
A:spacy.tests.pipeline.test_entity_linker.entity_linker_1->English().add_pipe('entity_linker', last=True)
A:spacy.tests.pipeline.test_entity_linker.nlp_2->nlp_2.from_bytes(nlp_bytes).from_bytes(nlp_bytes)
A:spacy.tests.pipeline.test_entity_linker.entity_linker_2->nlp_2.from_bytes(nlp_bytes).from_bytes(nlp_bytes).get_pipe('entity_linker')
A:spacy.tests.pipeline.test_entity_linker.kb_bytes->InMemoryLookupKB(nlp.vocab, entity_vector_length=3).to_bytes()
A:spacy.tests.pipeline.test_entity_linker.nlp_bytes->English().to_bytes()
A:spacy.tests.pipeline.test_entity_linker.ref1->nlp('Julia lives in London happily.')
A:spacy.tests.pipeline.test_entity_linker.pred1->nlp('Julia lives in London happily.')
A:spacy.tests.pipeline.test_entity_linker.ref2->nlp('She loves London.')
A:spacy.tests.pipeline.test_entity_linker.pred2->nlp('She loves London.')
A:spacy.tests.pipeline.test_entity_linker.ref3->nlp('London is great.')
A:spacy.tests.pipeline.test_entity_linker.pred3->nlp('London is great.')
A:spacy.tests.pipeline.test_entity_linker.scores->Scorer().score_links(train_examples, negative_labels=['NIL'])
A:spacy.tests.pipeline.test_entity_linker.eg.predicted->ruler(eg.predicted)
A:spacy.tests.pipeline.test_entity_linker.doc1->nlp('a b c')
A:spacy.tests.pipeline.test_entity_linker.eg->Example(doc1, doc2)
A:spacy.tests.pipeline.test_entity_linker.span_maker->build_span_maker()
spacy.tests.pipeline.test_entity_linker.assert_almost_equal(a,b)
spacy.tests.pipeline.test_entity_linker.nlp()
spacy.tests.pipeline.test_entity_linker.test_abstract_kb_instantiation()
spacy.tests.pipeline.test_entity_linker.test_append_alias(nlp)
spacy.tests.pipeline.test_entity_linker.test_append_invalid_alias(nlp)
spacy.tests.pipeline.test_entity_linker.test_candidate_generation(nlp)
spacy.tests.pipeline.test_entity_linker.test_el_pipe_configuration(nlp)
spacy.tests.pipeline.test_entity_linker.test_issue4674()
spacy.tests.pipeline.test_entity_linker.test_issue6730(en_vocab)
spacy.tests.pipeline.test_entity_linker.test_issue7065()
spacy.tests.pipeline.test_entity_linker.test_issue7065_b()
spacy.tests.pipeline.test_entity_linker.test_kb_custom_length(nlp)
spacy.tests.pipeline.test_entity_linker.test_kb_default(nlp)
spacy.tests.pipeline.test_entity_linker.test_kb_initialize_empty(nlp)
spacy.tests.pipeline.test_entity_linker.test_kb_invalid_combination(nlp)
spacy.tests.pipeline.test_entity_linker.test_kb_invalid_entities(nlp)
spacy.tests.pipeline.test_entity_linker.test_kb_invalid_entity_vector(nlp)
spacy.tests.pipeline.test_entity_linker.test_kb_invalid_probabilities(nlp)
spacy.tests.pipeline.test_entity_linker.test_kb_pickle()
spacy.tests.pipeline.test_entity_linker.test_kb_serialization()
spacy.tests.pipeline.test_entity_linker.test_kb_serialize(nlp)
spacy.tests.pipeline.test_entity_linker.test_kb_serialize_2(nlp)
spacy.tests.pipeline.test_entity_linker.test_kb_serialize_vocab(nlp)
spacy.tests.pipeline.test_entity_linker.test_kb_set_entities(nlp)
spacy.tests.pipeline.test_entity_linker.test_kb_to_bytes()
spacy.tests.pipeline.test_entity_linker.test_kb_valid_entities(nlp)
spacy.tests.pipeline.test_entity_linker.test_legacy_architectures(name,config)
spacy.tests.pipeline.test_entity_linker.test_nel_nsents(nlp)
spacy.tests.pipeline.test_entity_linker.test_nel_pickle()
spacy.tests.pipeline.test_entity_linker.test_nel_to_bytes()
spacy.tests.pipeline.test_entity_linker.test_no_entities()
spacy.tests.pipeline.test_entity_linker.test_no_gold_ents(patterns)
spacy.tests.pipeline.test_entity_linker.test_overfitting_IO()
spacy.tests.pipeline.test_entity_linker.test_partial_links()
spacy.tests.pipeline.test_entity_linker.test_preserving_links_asdoc(nlp)
spacy.tests.pipeline.test_entity_linker.test_preserving_links_ents(nlp)
spacy.tests.pipeline.test_entity_linker.test_preserving_links_ents_2(nlp)
spacy.tests.pipeline.test_entity_linker.test_scorer_links()
spacy.tests.pipeline.test_entity_linker.test_span_maker_forward_with_empty()
spacy.tests.pipeline.test_entity_linker.test_threshold(meet_threshold:bool,config:Dict[str,Any])
spacy.tests.pipeline.test_entity_linker.test_tokenization_mismatch()
spacy.tests.pipeline.test_entity_linker.test_vocab_serialization(nlp)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/pipeline/test_models.py----------------------------------------
A:spacy.tests.pipeline.test_models.OPS->get_current_ops()
A:spacy.tests.pipeline.test_models.array->get_current_ops().xp.asarray(l1, dtype='f')
A:spacy.tests.pipeline.test_models.ragged->Ragged(array, OPS.xp.asarray([2, 1], dtype='i'))
A:spacy.tests.pipeline.test_models.vocab->Vocab()
A:spacy.tests.pipeline.test_models.hash_id->Vocab().strings.add(word)
A:spacy.tests.pipeline.test_models.vector->numpy.random.uniform(-1, 1, (7,))
A:spacy.tests.pipeline.test_models.nlp->English()
A:spacy.tests.pipeline.test_models.proc->English().create_pipe(name)
A:spacy.tests.pipeline.test_models.Y_batched->model.predict(in_data).data.tolist()
spacy.tests.pipeline.test_models.get_docs()
spacy.tests.pipeline.test_models.test_components_batching_array(name)
spacy.tests.pipeline.test_models.test_components_batching_list(name)
spacy.tests.pipeline.test_models.test_layers_batching_all(model,in_data,out_data)
spacy.tests.pipeline.test_models.util_batch_unbatch_docs_array(model:Model[List[Doc],Array2d],in_data:List[Doc],out_data:Array2d)
spacy.tests.pipeline.test_models.util_batch_unbatch_docs_list(model:Model[List[Doc],List[Array2d]],in_data:List[Doc],out_data:List[Array2d])
spacy.tests.pipeline.test_models.util_batch_unbatch_docs_ragged(model:Model[List[Doc],Ragged],in_data:List[Doc],out_data:Ragged)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/pipeline/test_sentencizer.py----------------------------------------
A:spacy.tests.pipeline.test_sentencizer.doc->nlp(text)
A:spacy.tests.pipeline.test_sentencizer.sentencizer->Sentencizer(punct_chars=punct_chars)
A:spacy.tests.pipeline.test_sentencizer.nlp->spacy.blank(lang)
A:spacy.tests.pipeline.test_sentencizer.bytes_data->Sentencizer(punct_chars=punct_chars).to_bytes()
A:spacy.tests.pipeline.test_sentencizer.new_sentencizer->Sentencizer(punct_chars=None).from_bytes(bytes_data)
spacy.tests.pipeline.test_sentencizer.test_sentencizer(en_vocab)
spacy.tests.pipeline.test_sentencizer.test_sentencizer_across_scripts(lang,text)
spacy.tests.pipeline.test_sentencizer.test_sentencizer_complex(en_vocab,words,sent_starts,sent_ends,n_sents)
spacy.tests.pipeline.test_sentencizer.test_sentencizer_custom_punct(en_vocab,punct_chars,words,sent_starts,sent_ends,n_sents)
spacy.tests.pipeline.test_sentencizer.test_sentencizer_empty_docs()
spacy.tests.pipeline.test_sentencizer.test_sentencizer_pipe()
spacy.tests.pipeline.test_sentencizer.test_sentencizer_serialize_bytes(en_vocab)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/pipeline/test_functions.py----------------------------------------
A:spacy.tests.pipeline.test_functions.doc->nlp(doc)
A:spacy.tests.pipeline.test_functions.nlp->Language()
A:spacy.tests.pipeline.test_functions.merge_noun_chunks->Language().create_pipe('merge_noun_chunks')
A:spacy.tests.pipeline.test_functions.merge_entities->Language().create_pipe('merge_entities')
A:spacy.tests.pipeline.test_functions.token_splitter->Language().add_pipe('token_splitter', config=config)
spacy.tests.pipeline.test_functions.doc(en_vocab)
spacy.tests.pipeline.test_functions.doc2(en_vocab)
spacy.tests.pipeline.test_functions.test_factories_doc_cleaner()
spacy.tests.pipeline.test_functions.test_factories_merge_ents(doc2)
spacy.tests.pipeline.test_functions.test_factories_merge_noun_chunks(doc2)
spacy.tests.pipeline.test_functions.test_merge_subtokens(doc)
spacy.tests.pipeline.test_functions.test_token_splitter()


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/pipeline/test_span_ruler.py----------------------------------------
A:spacy.tests.pipeline.test_span_ruler.nlp->spacy.blank('xx')
A:spacy.tests.pipeline.test_span_ruler.ruler->spacy.blank('xx').add_pipe('span_ruler', config={'annotate_ents': True, 'ents_filter': {'@misc': 'test_pass_through_filter'}})
A:spacy.tests.pipeline.test_span_ruler.pattern_count->sum((len(mm) for mm in ruler.matcher._patterns.values()))
A:spacy.tests.pipeline.test_span_ruler.after_count->sum((len(mm) for mm in ruler.matcher._patterns.values()))
A:spacy.tests.pipeline.test_span_ruler.doc->ruler(doc)
A:spacy.tests.pipeline.test_span_ruler.ruler_bytes->spacy.blank('xx').add_pipe('span_ruler', config={'annotate_ents': True, 'ents_filter': {'@misc': 'test_pass_through_filter'}}).to_bytes()
A:spacy.tests.pipeline.test_span_ruler.new_nlp->spacy.blank('xx')
A:spacy.tests.pipeline.test_span_ruler.new_ruler->new_ruler.from_bytes(ruler_bytes).from_bytes(ruler_bytes)
A:spacy.tests.pipeline.test_span_ruler.validated_ruler->spacy.blank('xx').add_pipe('span_ruler', name='validated_span_ruler', config={'validate': True})
A:spacy.tests.pipeline.test_span_ruler.pred_doc->ruler(nlp.make_doc(text))
A:spacy.tests.pipeline.test_span_ruler.ref_doc->spacy.blank('xx').make_doc(text)
A:spacy.tests.pipeline.test_span_ruler.scores->spacy.blank('xx').evaluate([Example(pred_doc, ref_doc)])
spacy.tests.pipeline.test_span_ruler.overlapping_patterns()
spacy.tests.pipeline.test_span_ruler.patterns()
spacy.tests.pipeline.test_span_ruler.person_org_date_patterns(person_org_patterns)
spacy.tests.pipeline.test_span_ruler.person_org_patterns()
spacy.tests.pipeline.test_span_ruler.test_span_ruler_add_empty(patterns)
spacy.tests.pipeline.test_span_ruler.test_span_ruler_clear(patterns)
spacy.tests.pipeline.test_span_ruler.test_span_ruler_ents_bad_filter(overlapping_patterns)
spacy.tests.pipeline.test_span_ruler.test_span_ruler_ents_default_filter(overlapping_patterns)
spacy.tests.pipeline.test_span_ruler.test_span_ruler_ents_overwrite_filter(overlapping_patterns)
spacy.tests.pipeline.test_span_ruler.test_span_ruler_existing(patterns)
spacy.tests.pipeline.test_span_ruler.test_span_ruler_existing_overwrite(patterns)
spacy.tests.pipeline.test_span_ruler.test_span_ruler_init(patterns)
spacy.tests.pipeline.test_span_ruler.test_span_ruler_init_clear(patterns)
spacy.tests.pipeline.test_span_ruler.test_span_ruler_init_patterns(patterns)
spacy.tests.pipeline.test_span_ruler.test_span_ruler_multiprocessing(n_process)
spacy.tests.pipeline.test_span_ruler.test_span_ruler_no_patterns_warns()
spacy.tests.pipeline.test_span_ruler.test_span_ruler_overlapping_spans(overlapping_patterns)
spacy.tests.pipeline.test_span_ruler.test_span_ruler_properties(patterns)
spacy.tests.pipeline.test_span_ruler.test_span_ruler_remove_all_patterns(person_org_date_patterns)
spacy.tests.pipeline.test_span_ruler.test_span_ruler_remove_and_add()
spacy.tests.pipeline.test_span_ruler.test_span_ruler_remove_basic(person_org_patterns)
spacy.tests.pipeline.test_span_ruler.test_span_ruler_remove_nonexisting_pattern(person_org_patterns)
spacy.tests.pipeline.test_span_ruler.test_span_ruler_remove_patterns_in_a_row(person_org_date_patterns)
spacy.tests.pipeline.test_span_ruler.test_span_ruler_remove_several_patterns(person_org_patterns)
spacy.tests.pipeline.test_span_ruler.test_span_ruler_scorer(overlapping_patterns)
spacy.tests.pipeline.test_span_ruler.test_span_ruler_serialize_bytes(patterns)
spacy.tests.pipeline.test_span_ruler.test_span_ruler_serialize_dir(patterns)
spacy.tests.pipeline.test_span_ruler.test_span_ruler_spans_filter(overlapping_patterns)
spacy.tests.pipeline.test_span_ruler.test_span_ruler_validate()


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/pipeline/test_entity_ruler.py----------------------------------------
A:spacy.tests.pipeline.test_entity_ruler.nlp->English()
A:spacy.tests.pipeline.test_entity_ruler.doc->ruler(nlp.make_doc('I saw him last time we met, this time he brought some flowers, another time some chocolate.'))
A:spacy.tests.pipeline.test_entity_ruler.ruler->English().add_pipe(entity_ruler_factory, name='entity_ruler')
A:spacy.tests.pipeline.test_entity_ruler.ner->EntityRecognizer(doc.vocab, model)
A:spacy.tests.pipeline.test_entity_ruler.pattern_count->sum((len(mm) for mm in ruler.matcher._patterns.values()))
A:spacy.tests.pipeline.test_entity_ruler.after_count->sum((len(mm) for mm in ruler.matcher._patterns.values()))
A:spacy.tests.pipeline.test_entity_ruler.ruler_bytes->English().add_pipe(entity_ruler_factory, name='entity_ruler').to_bytes()
A:spacy.tests.pipeline.test_entity_ruler.new_ruler->new_ruler.from_bytes(ruler_bytes).from_bytes(ruler_bytes)
A:spacy.tests.pipeline.test_entity_ruler.validated_ruler->EntityRuler(nlp, validate=True)
spacy.tests.pipeline.test_entity_ruler.add_ent_component(doc)
spacy.tests.pipeline.test_entity_ruler.nlp()
spacy.tests.pipeline.test_entity_ruler.patterns()
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_cfg_ent_id_sep(nlp,patterns,entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_clear(nlp,patterns,entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_entity_id(nlp,patterns,entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_existing(nlp,patterns,entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_existing_complex(nlp,patterns,entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_existing_overwrite(nlp,patterns,entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_fix8216(nlp,patterns,entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_fuzzy(nlp,entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_fuzzy_disabled(nlp,entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_fuzzy_pipe(nlp,entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_init(nlp,patterns,entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_init_clear(nlp,patterns,entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_init_patterns(nlp,patterns,entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_multiprocessing(nlp,n_process,entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_no_patterns_warns(nlp,entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_overlapping_spans(nlp,entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_properties(nlp,patterns,entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_remove_all_patterns(nlp,entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_remove_and_add(nlp,entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_remove_basic(nlp,entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_remove_nonexisting_pattern(nlp,entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_remove_patterns_in_a_row(nlp,entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_remove_same_id_multiple_patterns(nlp,entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_remove_several_patterns(nlp,entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_serialize_bytes(nlp,patterns,entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_serialize_dir(nlp,patterns,entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_serialize_jsonl(nlp,patterns,entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_serialize_phrase_matcher_attr_bytes(nlp,patterns,entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_validate(nlp,entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_issue3345(entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_issue4849(entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_issue5918(entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_issue8168(entity_ruler_factory)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/pipeline/test_pipe_methods.py----------------------------------------
A:spacy.tests.pipeline.test_pipe_methods.nlp->spacy.load(tmp_dir, enable=to_enable, disable=[comp_name for comp_name in nlp.component_names if comp_name not in to_enable])
A:spacy.tests.pipeline.test_pipe_methods.nlp2->Language(Vocab())
A:spacy.tests.pipeline.test_pipe_methods.array1->numpy.asarray([0.1, 0.5, 0.8], dtype=numpy.float32)
A:spacy.tests.pipeline.test_pipe_methods.array2->numpy.asarray([-0.2, -0.6, -0.9], dtype=numpy.float32)
A:spacy.tests.pipeline.test_pipe_methods.array3->numpy.asarray([0.3, -0.1, 0.7], dtype=numpy.float32)
A:spacy.tests.pipeline.test_pipe_methods.array4->numpy.asarray([0.5, 0, 0.3], dtype=numpy.float32)
A:spacy.tests.pipeline.test_pipe_methods.array34->numpy.asarray([0.4, -0.05, 0.5], dtype=numpy.float32)
A:spacy.tests.pipeline.test_pipe_methods.ruler->spacy.load(tmp_dir, enable=to_enable, disable=[comp_name for comp_name in nlp.component_names if comp_name not in to_enable]).add_pipe('entity_ruler')
A:spacy.tests.pipeline.test_pipe_methods.ops->get_current_ops()
A:spacy.tests.pipeline.test_pipe_methods.vocab->Vocab(strings=words)
A:spacy.tests.pipeline.test_pipe_methods.en_doc->Doc(vocab, words=words, pos=pos, heads=heads, deps=deps)
A:spacy.tests.pipeline.test_pipe_methods.merge_nps->spacy.load(tmp_dir, enable=to_enable, disable=[comp_name for comp_name in nlp.component_names if comp_name not in to_enable]).create_pipe('merge_noun_chunks')
A:spacy.tests.pipeline.test_pipe_methods.doc->spacy.load(tmp_dir, enable=to_enable, disable=[comp_name for comp_name in nlp.component_names if comp_name not in to_enable]).make_doc('foo')
A:spacy.tests.pipeline.test_pipe_methods.dummy_pipe->DummyPipe()
A:spacy.tests.pipeline.test_pipe_methods.(removed_name, removed_component)->spacy.load(tmp_dir, enable=to_enable, disable=[comp_name for comp_name in nlp.component_names if comp_name not in to_enable]).remove_pipe(name)
A:spacy.tests.pipeline.test_pipe_methods.disabled->spacy.load(tmp_dir, enable=to_enable, disable=[comp_name for comp_name in nlp.component_names if comp_name not in to_enable]).select_pipes(disable=['c2'])
A:spacy.tests.pipeline.test_pipe_methods.pipe->spacy.load(tmp_dir, enable=to_enable, disable=[comp_name for comp_name in nlp.component_names if comp_name not in to_enable]).add_pipe(pipe)
A:spacy.tests.pipeline.test_pipe_methods.c1->spacy.language.Language.component(f'{name}1', func=make_component(f'{name}1'))
A:spacy.tests.pipeline.test_pipe_methods.c2->spacy.language.Language.component(f'{name}2', func=make_component(f'{name}2'))
A:spacy.tests.pipeline.test_pipe_methods.ner->spacy.load(tmp_dir, enable=to_enable, disable=[comp_name for comp_name in nlp.component_names if comp_name not in to_enable]).add_pipe('ner')
A:spacy.tests.pipeline.test_pipe_methods.initialize->getattr(pipe, 'initialize', None)
A:spacy.tests.pipeline.test_pipe_methods.components->set([f'{name}1', f'{name}2'])
A:spacy.tests.pipeline.test_pipe_methods.base_nlp->English()
spacy.tests.pipeline.test_pipe_methods.new_pipe(doc)
spacy.tests.pipeline.test_pipe_methods.nlp()
spacy.tests.pipeline.test_pipe_methods.other_pipe(doc)
spacy.tests.pipeline.test_pipe_methods.test_add_lots_of_pipes(nlp,n_pipes)
spacy.tests.pipeline.test_pipe_methods.test_add_pipe_before_after()
spacy.tests.pipeline.test_pipe_methods.test_add_pipe_duplicate_name(nlp)
spacy.tests.pipeline.test_pipe_methods.test_add_pipe_first(nlp,name)
spacy.tests.pipeline.test_pipe_methods.test_add_pipe_last(nlp,name1,name2)
spacy.tests.pipeline.test_pipe_methods.test_add_pipe_no_name(nlp)
spacy.tests.pipeline.test_pipe_methods.test_cant_add_pipe_first_and_last(nlp)
spacy.tests.pipeline.test_pipe_methods.test_disable_enable_pipes()
spacy.tests.pipeline.test_pipe_methods.test_disable_pipes_context(nlp,name)
spacy.tests.pipeline.test_pipe_methods.test_disable_pipes_context_restore(nlp,name)
spacy.tests.pipeline.test_pipe_methods.test_disable_pipes_method(nlp,name)
spacy.tests.pipeline.test_pipe_methods.test_enable_disable_conflict_with_config()
spacy.tests.pipeline.test_pipe_methods.test_enable_pipes_method(nlp,name)
spacy.tests.pipeline.test_pipe_methods.test_get_pipe(nlp,name)
spacy.tests.pipeline.test_pipe_methods.test_issue1506()
spacy.tests.pipeline.test_pipe_methods.test_issue1654()
spacy.tests.pipeline.test_pipe_methods.test_issue3880()
spacy.tests.pipeline.test_pipe_methods.test_issue5082()
spacy.tests.pipeline.test_pipe_methods.test_issue5458()
spacy.tests.pipeline.test_pipe_methods.test_load_disable_enable()
spacy.tests.pipeline.test_pipe_methods.test_multiple_predictions()
spacy.tests.pipeline.test_pipe_methods.test_pipe_base_class_add_label(nlp,component)
spacy.tests.pipeline.test_pipe_methods.test_pipe_label_data_exports_labels(pipe)
spacy.tests.pipeline.test_pipe_methods.test_pipe_label_data_no_labels(pipe)
spacy.tests.pipeline.test_pipe_methods.test_pipe_labels(nlp)
spacy.tests.pipeline.test_pipe_methods.test_pipe_methods_frozen()
spacy.tests.pipeline.test_pipe_methods.test_pipe_methods_initialize()
spacy.tests.pipeline.test_pipe_methods.test_raise_for_invalid_components(nlp,component)
spacy.tests.pipeline.test_pipe_methods.test_remove_pipe(nlp,name)
spacy.tests.pipeline.test_pipe_methods.test_rename_pipe(nlp,old_name,new_name)
spacy.tests.pipeline.test_pipe_methods.test_replace_last_pipe(nlp)
spacy.tests.pipeline.test_pipe_methods.test_replace_pipe(nlp,name,replacement,invalid_replacement)
spacy.tests.pipeline.test_pipe_methods.test_replace_pipe_config(nlp)
spacy.tests.pipeline.test_pipe_methods.test_select_pipes_errors(nlp)
spacy.tests.pipeline.test_pipe_methods.test_select_pipes_list_arg(nlp)
spacy.tests.pipeline.test_pipe_methods.test_update_with_annotates()
spacy.tests.pipeline.test_pipe_methods.test_warning_pipe_begin_training()


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/pipeline/test_pipe_factories.py----------------------------------------
A:spacy.tests.pipeline.test_pipe_factories.nlp->spacy.lang.en.English.from_config(config)
A:spacy.tests.pipeline.test_pipe_factories.my_component->spacy.lang.en.English.from_config(config).add_pipe(factory_name, name=pipe_name)
A:spacy.tests.pipeline.test_pipe_factories.nlp2->spacy.load(tmpdir, config=overrides)
A:spacy.tests.pipeline.test_pipe_factories.pipe->spacy.lang.en.English.from_config(config).get_pipe(name)
A:spacy.tests.pipeline.test_pipe_factories.nlp_en->English()
A:spacy.tests.pipeline.test_pipe_factories.nlp_de->German()
A:spacy.tests.pipeline.test_pipe_factories.result->combine_score_weights(weights, override)
A:spacy.tests.pipeline.test_pipe_factories.meta1->spacy.language.Language.get_factory_meta(f'{name}1')
A:spacy.tests.pipeline.test_pipe_factories.meta2->spacy.language.Language.get_factory_meta(f'{name}2')
A:spacy.tests.pipeline.test_pipe_factories.config->spacy.lang.en.English.from_config(config).config.copy()
A:spacy.tests.pipeline.test_pipe_factories.source_nlp->English()
A:spacy.tests.pipeline.test_pipe_factories.stop_words->set(['custom', 'stop'])
A:spacy.tests.pipeline.test_pipe_factories.meta->spacy.lang.en.English.from_config(config).get_pipe_meta('custom')
A:spacy.tests.pipeline.test_pipe_factories.pipe_cfg->spacy.lang.en.English.from_config(config).get_pipe_config(name)
spacy.tests.pipeline.test_pipe_factories.PipeFactoriesIdempotent(self,nlp,name)
spacy.tests.pipeline.test_pipe_factories.test_issue5137()
spacy.tests.pipeline.test_pipe_factories.test_language_factories_combine_score_weights(weights,override,expected)
spacy.tests.pipeline.test_pipe_factories.test_language_factories_invalid()
spacy.tests.pipeline.test_pipe_factories.test_language_factories_scores()
spacy.tests.pipeline.test_pipe_factories.test_pipe_class_component_config()
spacy.tests.pipeline.test_pipe_factories.test_pipe_class_component_defaults()
spacy.tests.pipeline.test_pipe_factories.test_pipe_class_component_init()
spacy.tests.pipeline.test_pipe_factories.test_pipe_class_component_model()
spacy.tests.pipeline.test_pipe_factories.test_pipe_class_component_model_custom()
spacy.tests.pipeline.test_pipe_factories.test_pipe_factories_config_excludes_nlp()
spacy.tests.pipeline.test_pipe_factories.test_pipe_factories_decorator_idempotent(i,func,func2)
spacy.tests.pipeline.test_pipe_factories.test_pipe_factories_empty_dict_default()
spacy.tests.pipeline.test_pipe_factories.test_pipe_factories_from_source()
spacy.tests.pipeline.test_pipe_factories.test_pipe_factories_from_source_config()
spacy.tests.pipeline.test_pipe_factories.test_pipe_factories_from_source_custom()
spacy.tests.pipeline.test_pipe_factories.test_pipe_factories_from_source_language_subclass()
spacy.tests.pipeline.test_pipe_factories.test_pipe_factories_language_specific()
spacy.tests.pipeline.test_pipe_factories.test_pipe_factories_wrong_formats()
spacy.tests.pipeline.test_pipe_factories.test_pipe_factory_meta_config_cleanup()
spacy.tests.pipeline.test_pipe_factories.test_pipe_function_component()


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/pipeline/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/pipeline/test_textcat.py----------------------------------------
A:spacy.tests.pipeline.test_textcat.nlp->English()
A:spacy.tests.pipeline.test_textcat.textcat->English().add_pipe('textcat')
A:spacy.tests.pipeline.test_textcat.optimizer->English().initialize()
A:spacy.tests.pipeline.test_textcat.batches->spacy.util.minibatch(train_data, size=compounding(4.0, 32.0, 1.001))
A:spacy.tests.pipeline.test_textcat.doc->nlp(test_text)
A:spacy.tests.pipeline.test_textcat.pipe_cfg->Config().from_str(textcat_config)
A:spacy.tests.pipeline.test_textcat.pipe->English().add_pipe(component, config=pipe_cfg, last=True)
A:spacy.tests.pipeline.test_textcat.result->English().add_pipe(component, config=pipe_cfg, last=True).model.predict([doc])
A:spacy.tests.pipeline.test_textcat.ops->get_current_ops()
A:spacy.tests.pipeline.test_textcat.out_data->DocBin(docs=[doc]).to_bytes()
A:spacy.tests.pipeline.test_textcat.config_str->config_str.replace('TRAIN_PLACEHOLDER', train_path.as_posix()).replace('TRAIN_PLACEHOLDER', train_path.as_posix())
A:spacy.tests.pipeline.test_textcat.config->spacy.util.load_config_from_str(config_str)
A:spacy.tests.pipeline.test_textcat.get_examples->make_get_examples_multi_label(nlp)
A:spacy.tests.pipeline.test_textcat.examples->example_getter()
A:spacy.tests.pipeline.test_textcat.scores->English().evaluate(train_examples)
A:spacy.tests.pipeline.test_textcat.example_getter->get_examples(nlp)
A:spacy.tests.pipeline.test_textcat.nlp2->spacy.util.load_model_from_path(tmp_dir)
A:spacy.tests.pipeline.test_textcat.doc2->nlp('two')
A:spacy.tests.pipeline.test_textcat.textcat_multilabel->English().add_pipe('textcat_multilabel')
A:spacy.tests.pipeline.test_textcat.ref1->nlp('one')
A:spacy.tests.pipeline.test_textcat.pred1->nlp('one')
A:spacy.tests.pipeline.test_textcat.ref2->nlp('two')
A:spacy.tests.pipeline.test_textcat.pred2->nlp('two')
A:spacy.tests.pipeline.test_textcat.doc1->nlp('one')
A:spacy.tests.pipeline.test_textcat.(loss, d_scores)->English().add_pipe('textcat').get_loss(train_examples, scores)
spacy.tests.pipeline.test_textcat.make_get_examples_multi_label(nlp)
spacy.tests.pipeline.test_textcat.make_get_examples_single_label(nlp)
spacy.tests.pipeline.test_textcat.test_error_with_multi_labels()
spacy.tests.pipeline.test_textcat.test_implicit_label(name,get_examples)
spacy.tests.pipeline.test_textcat.test_initialize_examples(name,get_examples,train_data)
spacy.tests.pipeline.test_textcat.test_invalid_label_value(name,get_examples)
spacy.tests.pipeline.test_textcat.test_issue3611()
spacy.tests.pipeline.test_textcat.test_issue4030()
spacy.tests.pipeline.test_textcat.test_issue5551(textcat_config)
spacy.tests.pipeline.test_textcat.test_issue6908(component_name)
spacy.tests.pipeline.test_textcat.test_issue7019()
spacy.tests.pipeline.test_textcat.test_issue9904()
spacy.tests.pipeline.test_textcat.test_label_types(name)
spacy.tests.pipeline.test_textcat.test_no_label(name)
spacy.tests.pipeline.test_textcat.test_no_resize(name,textcat_config)
spacy.tests.pipeline.test_textcat.test_overfitting_IO()
spacy.tests.pipeline.test_textcat.test_overfitting_IO_multi()
spacy.tests.pipeline.test_textcat.test_positive_class()
spacy.tests.pipeline.test_textcat.test_positive_class_not_binary()
spacy.tests.pipeline.test_textcat.test_positive_class_not_present()
spacy.tests.pipeline.test_textcat.test_resize(name,textcat_config)
spacy.tests.pipeline.test_textcat.test_resize_same_results(name,textcat_config)
spacy.tests.pipeline.test_textcat.test_simple_train()
spacy.tests.pipeline.test_textcat.test_textcat_configs(name,train_data,textcat_config)
spacy.tests.pipeline.test_textcat.test_textcat_eval_missing(multi_label:bool,spring_p:float)
spacy.tests.pipeline.test_textcat.test_textcat_evaluation()
spacy.tests.pipeline.test_textcat.test_textcat_learns_multilabel()
spacy.tests.pipeline.test_textcat.test_textcat_legacy_scorers(component_name,scorer)
spacy.tests.pipeline.test_textcat.test_textcat_loss(multi_label:bool,expected_loss:float)
spacy.tests.pipeline.test_textcat.test_textcat_multi_threshold()
spacy.tests.pipeline.test_textcat.test_textcat_multilabel_threshold()


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/pipeline/test_senter.py----------------------------------------
A:spacy.tests.pipeline.test_senter.nlp->English()
A:spacy.tests.pipeline.test_senter.senter->English().add_pipe('senter')
A:spacy.tests.pipeline.test_senter.optimizer->English().initialize()
A:spacy.tests.pipeline.test_senter.doc->nlp(test_text)
A:spacy.tests.pipeline.test_senter.nlp2->spacy.util.load_model_from_path(tmp_dir)
A:spacy.tests.pipeline.test_senter.doc2->nlp2(test_text)
spacy.tests.pipeline.test_senter.test_initialize_examples()
spacy.tests.pipeline.test_senter.test_label_types()
spacy.tests.pipeline.test_senter.test_overfitting_IO()


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/package/test_requirements.py----------------------------------------
A:spacy.tests.package.test_requirements.lines->f.readlines()
A:spacy.tests.package.test_requirements.line->line.strip().strip(',').strip('"').strip().strip(',').strip('"')
A:spacy.tests.package.test_requirements.(lib, v)->_parse_req(line)
A:spacy.tests.package.test_requirements.setup_keys->set()
A:spacy.tests.package.test_requirements.req_v->req_dict.get(lib, None)
A:spacy.tests.package.test_requirements.lib->re.match('^[a-z0-9\\-]*', line).group(0)
A:spacy.tests.package.test_requirements.v->line.strip().strip(',').strip('"').strip().strip(',').strip('"').replace(lib, '').strip()
spacy.tests.package.test_requirements._parse_req(line)
spacy.tests.package.test_requirements.test_build_dependencies()


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/package/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/test_lemmatizers.py----------------------------------------
A:spacy.tests.lang.test_lemmatizers.lookups->Lookups()
A:spacy.tests.lang.test_lemmatizers.lang_cls->get_lang_class(lang)
A:spacy.tests.lang.test_lemmatizers.nlp->lang_cls()
A:spacy.tests.lang.test_lemmatizers.lemmatizer->lang_cls().add_pipe('lemmatizer', config={'mode': 'lookup'})
A:spacy.tests.lang.test_lemmatizers.doc->nlp('x')
A:spacy.tests.lang.test_lemmatizers.captured->capfd.readouterr()
A:spacy.tests.lang.test_lemmatizers.(required, optional)->lang_cls().add_pipe('lemmatizer', config={'mode': 'lookup'}).get_lookups_config(mode)
spacy.tests.lang.test_lemmatizers.test_lemmatizer_initialize(lang,capfd)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/test_initialize.py----------------------------------------
A:spacy.tests.lang.test_initialize.nlp->get_lang_class(lang)()
A:spacy.tests.lang.test_initialize.doc->nlp('test')
A:spacy.tests.lang.test_initialize.captured->capfd.readouterr()
spacy.tests.lang.test_initialize.test_lang_initialize(lang,capfd)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/test_attrs.py----------------------------------------
A:spacy.tests.lang.test_attrs.int_attrs->intify_attrs({ENT_IOB: 'XX'})
spacy.tests.lang.test_attrs.test_attrs_do_deprecated(text)
spacy.tests.lang.test_attrs.test_attrs_ent_iob_intify()
spacy.tests.lang.test_attrs.test_attrs_idempotence(text)
spacy.tests.lang.test_attrs.test_attrs_key(text)
spacy.tests.lang.test_attrs.test_issue1889(word)
spacy.tests.lang.test_attrs.test_lex_attrs_is_ascii(text,match)
spacy.tests.lang.test_attrs.test_lex_attrs_is_currency(text,match)
spacy.tests.lang.test_attrs.test_lex_attrs_is_punct(text,match)
spacy.tests.lang.test_attrs.test_lex_attrs_like_url(text,match)
spacy.tests.lang.test_attrs.test_lex_attrs_word_shape(text,shape)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/is/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.is.test_tokenizer.tokens->is_tokenizer(text)
spacy.tests.lang.is.test_tokenizer.test_is_tokenizer_basic(is_tokenizer,text,expected_tokens)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/is/test_text.py----------------------------------------
A:spacy.tests.lang.is.test_text.tokens->is_tokenizer(text)
spacy.tests.lang.is.test_text.test_long_text(is_tokenizer)
spacy.tests.lang.is.test_text.test_ordinal_number(is_tokenizer)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/is/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/lg/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.lg.test_tokenizer.tokens->lg_tokenizer(text)
spacy.tests.lang.lg.test_tokenizer.test_lg_tokenizer_basic(lg_tokenizer,text,expected_tokens)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/lg/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/xx/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.xx.test_tokenizer.tokens->xx_tokenizer(text)
spacy.tests.lang.xx.test_tokenizer.test_xx_tokenizer_basic(xx_tokenizer,text,expected_tokens)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/xx/test_text.py----------------------------------------
A:spacy.tests.lang.xx.test_text.tokens->xx_tokenizer(text)
spacy.tests.lang.xx.test_text.test_long_text(xx_tokenizer)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/xx/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/th/test_tokenizer.py----------------------------------------
spacy.tests.lang.th.test_tokenizer.test_th_tokenizer(th_tokenizer,text,expected_tokens)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/th/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/th/test_serialize.py----------------------------------------
A:spacy.tests.lang.th.test_serialize.tokenizer_bytes->th_tokenizer.to_bytes()
A:spacy.tests.lang.th.test_serialize.nlp->Thai()
A:spacy.tests.lang.th.test_serialize.b->pickle.dumps(th_tokenizer)
A:spacy.tests.lang.th.test_serialize.th_tokenizer_re->pickle.loads(b)
spacy.tests.lang.th.test_serialize.test_th_tokenizer_pickle(th_tokenizer)
spacy.tests.lang.th.test_serialize.test_th_tokenizer_serialize(th_tokenizer)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/mk/test_text.py----------------------------------------
A:spacy.tests.lang.mk.test_text.tokens->mk_tokenizer(word)
spacy.tests.lang.mk.test_text.test_mk_lex_attrs_capitals(word)
spacy.tests.lang.mk.test_text.test_mk_lex_attrs_like_number(mk_tokenizer,word,match)
spacy.tests.lang.mk.test_text.test_mk_lex_attrs_like_number_for_ordinal(word)
spacy.tests.lang.mk.test_text.test_tokenizer_handles_long_text(mk_tokenizer)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/mk/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/pt/test_noun_chunks.py----------------------------------------
A:spacy.tests.lang.pt.test_noun_chunks.doc->pt_tokenizer('en Oxford este verano')
spacy.tests.lang.pt.test_noun_chunks.test_noun_chunks_is_parsed_pt(pt_tokenizer)
spacy.tests.lang.pt.test_noun_chunks.test_pt_noun_chunks(pt_vocab,words,heads,deps,pos,chunk_offsets)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/pt/test_text.py----------------------------------------
spacy.tests.lang.pt.test_text.test_pt_lex_attrs_capitals(word)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/pt/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/eu/test_text.py----------------------------------------
A:spacy.tests.lang.eu.test_text.tokens->eu_tokenizer(text)
spacy.tests.lang.eu.test_text.test_eu_tokenizer_handles_cnts(eu_tokenizer,text,length)
spacy.tests.lang.eu.test_text.test_eu_tokenizer_handles_long_text(eu_tokenizer)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/eu/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/ne/test_text.py----------------------------------------
A:spacy.tests.lang.ne.test_text.tokens->ne_tokenizer(text)
spacy.tests.lang.ne.test_text.test_ne_tokenizer_handlers_long_text(ne_tokenizer)
spacy.tests.lang.ne.test_text.test_ne_tokenizer_handles_cnts(ne_tokenizer,text,length)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/ne/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/fi/test_noun_chunks.py----------------------------------------
A:spacy.tests.lang.fi.test_noun_chunks.doc->Doc(tokens.vocab, words=[t.text for t in tokens], heads=[head + i for (i, head) in enumerate(heads)], deps=deps, pos=pos)
A:spacy.tests.lang.fi.test_noun_chunks.tokens->fi_tokenizer(text)
A:spacy.tests.lang.fi.test_noun_chunks.noun_chunks->list(doc.noun_chunks)
spacy.tests.lang.fi.test_noun_chunks.test_fi_noun_chunks(fi_tokenizer,text,pos,deps,heads,expected_noun_chunks)
spacy.tests.lang.fi.test_noun_chunks.test_noun_chunks_is_parsed(fi_tokenizer)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/fi/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.fi.test_tokenizer.tokens->fi_tokenizer(text)
spacy.tests.lang.fi.test_tokenizer.test_fi_tokenizer_abbreviation_inflections(fi_tokenizer,text,expected_tokens)
spacy.tests.lang.fi.test_tokenizer.test_fi_tokenizer_abbreviations(fi_tokenizer,text,expected_tokens)
spacy.tests.lang.fi.test_tokenizer.test_fi_tokenizer_contractions(fi_tokenizer,text,expected_tokens,expected_norms)
spacy.tests.lang.fi.test_tokenizer.test_fi_tokenizer_hyphenated_words(fi_tokenizer,text,expected_tokens)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/fi/test_text.py----------------------------------------
A:spacy.tests.lang.fi.test_text.tokens->fi_tokenizer(text)
spacy.tests.lang.fi.test_text.test_fi_lex_attrs_like_number(fi_tokenizer,text,match)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/fi/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/lb/test_prefix_suffix_infix.py----------------------------------------
A:spacy.tests.lang.lb.test_prefix_suffix_infix.tokens->lb_tokenizer(text)
spacy.tests.lang.lb.test_prefix_suffix_infix.test_lb_tokenizer_splits_even_wrap_interact(lb_tokenizer,text)
spacy.tests.lang.lb.test_prefix_suffix_infix.test_lb_tokenizer_splits_prefix_interact(lb_tokenizer,text,length)
spacy.tests.lang.lb.test_prefix_suffix_infix.test_lb_tokenizer_splits_suffix_interact(lb_tokenizer,text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/lb/test_text.py----------------------------------------
A:spacy.tests.lang.lb.test_text.tokens->lb_tokenizer(text)
spacy.tests.lang.lb.test_text.test_lb_tokenizer_handles_examples(lb_tokenizer,text,length)
spacy.tests.lang.lb.test_text.test_lb_tokenizer_handles_long_text(lb_tokenizer)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/lb/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/lb/test_exceptions.py----------------------------------------
A:spacy.tests.lang.lb.test_exceptions.tokens->lb_tokenizer(text)
spacy.tests.lang.lb.test_exceptions.test_lb_tokenizer_handles_abbr(lb_tokenizer,text)
spacy.tests.lang.lb.test_exceptions.test_lb_tokenizer_handles_exc_in_text(lb_tokenizer)
spacy.tests.lang.lb.test_exceptions.test_lb_tokenizer_splits_contractions(lb_tokenizer,text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/de/test_noun_chunks.py----------------------------------------
A:spacy.tests.lang.de.test_noun_chunks.doc->de_tokenizer('Er lag auf seinem')
spacy.tests.lang.de.test_noun_chunks.test_noun_chunks_is_parsed_de(de_tokenizer)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/de/test_prefix_suffix_infix.py----------------------------------------
A:spacy.tests.lang.de.test_prefix_suffix_infix.tokens->de_tokenizer('Viele Regeln--wie die Bindestrich-Regeln--sind kompliziert.')
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_keeps_hyphens(de_tokenizer,text)
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_splits_comma_infix(de_tokenizer,text)
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_splits_double_hyphen_infix(de_tokenizer)
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_splits_ellipsis_infix(de_tokenizer,text)
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_splits_even_wrap(de_tokenizer,text)
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_splits_even_wrap_interact(de_tokenizer,text)
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_splits_no_punct(de_tokenizer,text)
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_splits_no_special(de_tokenizer,text)
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_splits_numeric_range(de_tokenizer,text)
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_splits_period_infix(de_tokenizer,text)
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_splits_prefix_interact(de_tokenizer,text,length)
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_splits_prefix_punct(de_tokenizer,text)
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_splits_suffix_interact(de_tokenizer,text)
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_splits_suffix_punct(de_tokenizer,text)
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_splits_uneven_wrap(de_tokenizer,text)
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_splits_uneven_wrap_interact(de_tokenizer,text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/de/test_text.py----------------------------------------
A:spacy.tests.lang.de.test_text.tokens->de_tokenizer(text)
spacy.tests.lang.de.test_text.test_de_tokenizer_handles_examples(de_tokenizer,text,length)
spacy.tests.lang.de.test_text.test_de_tokenizer_handles_long_text(de_tokenizer)
spacy.tests.lang.de.test_text.test_de_tokenizer_handles_long_words(de_tokenizer,text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/de/test_parser.py----------------------------------------
A:spacy.tests.lang.de.test_parser.doc->Doc(de_vocab, words=words, pos=pos, deps=deps, heads=heads)
A:spacy.tests.lang.de.test_parser.chunks->list(doc.noun_chunks)
spacy.tests.lang.de.test_parser.test_de_extended_chunk(de_vocab)
spacy.tests.lang.de.test_parser.test_de_parser_noun_chunks_standard_de(de_vocab)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/de/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/de/test_exceptions.py----------------------------------------
A:spacy.tests.lang.de.test_exceptions.tokens->de_tokenizer(text)
spacy.tests.lang.de.test_exceptions.test_de_tokenizer_handles_abbr(de_tokenizer,text)
spacy.tests.lang.de.test_exceptions.test_de_tokenizer_handles_exc_in_text(de_tokenizer)
spacy.tests.lang.de.test_exceptions.test_de_tokenizer_splits_contractions(de_tokenizer,text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/ja/test_lemmatization.py----------------------------------------
spacy.tests.lang.ja.test_lemmatization.test_ja_lemmatizer_assigns(ja_tokenizer,word,lemma)
spacy.tests.lang.ja.test_lemmatization.test_ja_lemmatizer_norm(ja_tokenizer,word,norm)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/ja/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.ja.test_tokenizer.nlp->Japanese()
A:spacy.tests.lang.ja.test_tokenizer.doc->ja_tokenizer('\n\n\n \t\t \n\n\n')
A:spacy.tests.lang.ja.test_tokenizer.tokens->ja_tokenizer(text)
A:spacy.tests.lang.ja.test_tokenizer.nlp_a->spacy.lang.ja.Japanese.from_config({'nlp': {'tokenizer': {'split_mode': 'A'}}})
A:spacy.tests.lang.ja.test_tokenizer.nlp_b->spacy.lang.ja.Japanese.from_config({'nlp': {'tokenizer': {'split_mode': 'B'}}})
A:spacy.tests.lang.ja.test_tokenizer.nlp_c->spacy.lang.ja.Japanese.from_config({'nlp': {'tokenizer': {'split_mode': 'C'}}})
spacy.tests.lang.ja.test_tokenizer.test_issue2901()
spacy.tests.lang.ja.test_tokenizer.test_ja_tokenizer(ja_tokenizer,text,expected_tokens)
spacy.tests.lang.ja.test_tokenizer.test_ja_tokenizer_emptyish_texts(ja_tokenizer)
spacy.tests.lang.ja.test_tokenizer.test_ja_tokenizer_extra_spaces(ja_tokenizer)
spacy.tests.lang.ja.test_tokenizer.test_ja_tokenizer_inflections_reading_forms(ja_tokenizer,text,inflections,reading_forms)
spacy.tests.lang.ja.test_tokenizer.test_ja_tokenizer_naughty_strings(ja_tokenizer,text)
spacy.tests.lang.ja.test_tokenizer.test_ja_tokenizer_pos(ja_tokenizer,text,expected_pos)
spacy.tests.lang.ja.test_tokenizer.test_ja_tokenizer_sents(ja_tokenizer,text,expected_sents)
spacy.tests.lang.ja.test_tokenizer.test_ja_tokenizer_split_modes(ja_tokenizer,text,len_a,len_b,len_c)
spacy.tests.lang.ja.test_tokenizer.test_ja_tokenizer_sub_tokens(ja_tokenizer,text,sub_tokens_list_b,sub_tokens_list_c)
spacy.tests.lang.ja.test_tokenizer.test_ja_tokenizer_tags(ja_tokenizer,text,expected_tags)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/ja/test_morphologizer_factory.py----------------------------------------
A:spacy.tests.lang.ja.test_morphologizer_factory.nlp->Japanese()
A:spacy.tests.lang.ja.test_morphologizer_factory.morphologizer->Japanese().add_pipe('morphologizer')
spacy.tests.lang.ja.test_morphologizer_factory.test_ja_morphologizer_factory()


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/ja/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/ja/test_serialize.py----------------------------------------
A:spacy.tests.lang.ja.test_serialize.tokenizer_bytes->ja_tokenizer.to_bytes()
A:spacy.tests.lang.ja.test_serialize.nlp->spacy.lang.ja.Japanese.from_config({'nlp': {'tokenizer': {'split_mode': 'B'}}})
A:spacy.tests.lang.ja.test_serialize.nlp_r->Japanese()
A:spacy.tests.lang.ja.test_serialize.nlp_bytes->spacy.lang.ja.Japanese.from_config({'nlp': {'tokenizer': {'split_mode': 'B'}}}).to_bytes()
A:spacy.tests.lang.ja.test_serialize.b->pickle.dumps(ja_tokenizer)
A:spacy.tests.lang.ja.test_serialize.ja_tokenizer_re->pickle.loads(b)
spacy.tests.lang.ja.test_serialize.test_ja_tokenizer_pickle(ja_tokenizer)
spacy.tests.lang.ja.test_serialize.test_ja_tokenizer_serialize(ja_tokenizer)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/da/test_noun_chunks.py----------------------------------------
A:spacy.tests.lang.da.test_noun_chunks.doc->Doc(tokens.vocab, words=[t.text for t in tokens], heads=[head + i for (i, head) in enumerate(heads)], deps=deps, pos=pos)
A:spacy.tests.lang.da.test_noun_chunks.tokens->da_tokenizer(text)
A:spacy.tests.lang.da.test_noun_chunks.noun_chunks->list(doc.noun_chunks)
spacy.tests.lang.da.test_noun_chunks.test_da_noun_chunks(da_tokenizer,text,pos,deps,heads,expected_noun_chunks)
spacy.tests.lang.da.test_noun_chunks.test_noun_chunks_is_parsed(da_tokenizer)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/da/test_prefix_suffix_infix.py----------------------------------------
A:spacy.tests.lang.da.test_prefix_suffix_infix.tokens->da_tokenizer("'DBA's, Lars' og Liz' bil sku' sgu' ik' ha' en bule, det ka' han ik' li' mere', sagde hun.")
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_handles_no_punct(da_tokenizer,text)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_handles_numeric_range(da_tokenizer,text)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_handles_posessives_and_contractions(da_tokenizer)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_keeps_hyphens(da_tokenizer,text)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_splits_comma_infix(da_tokenizer,text)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_splits_double_hyphen_infix(da_tokenizer)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_splits_ellipsis_infix(da_tokenizer,text)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_splits_even_wrap(da_tokenizer,text,expected)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_splits_even_wrap_interact(da_tokenizer,text)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_splits_no_special(da_tokenizer,text)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_splits_period_infix(da_tokenizer,text)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_splits_prefix_interact(da_tokenizer,text,expected)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_splits_prefix_punct(da_tokenizer,text)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_splits_suffix_interact(da_tokenizer,text)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_splits_suffix_punct(da_tokenizer,text)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_splits_uneven_wrap(da_tokenizer,text)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_splits_uneven_wrap_interact(da_tokenizer,text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/da/test_text.py----------------------------------------
A:spacy.tests.lang.da.test_text.tokens->da_tokenizer(text)
spacy.tests.lang.da.test_text.test_da_lex_attrs_capitals(word)
spacy.tests.lang.da.test_text.test_da_tokenizer_handles_long_text(da_tokenizer)
spacy.tests.lang.da.test_text.test_lex_attrs_like_number(da_tokenizer,text,match)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/da/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/da/test_exceptions.py----------------------------------------
A:spacy.tests.lang.da.test_exceptions.tokens->da_tokenizer(text)
spacy.tests.lang.da.test_exceptions.test_da_tokenizer_handles_abbr(da_tokenizer,text)
spacy.tests.lang.da.test_exceptions.test_da_tokenizer_handles_ambiguous_abbr(da_tokenizer,text)
spacy.tests.lang.da.test_exceptions.test_da_tokenizer_handles_custom_base_exc(da_tokenizer)
spacy.tests.lang.da.test_exceptions.test_da_tokenizer_handles_dates(da_tokenizer,text)
spacy.tests.lang.da.test_exceptions.test_da_tokenizer_handles_exc_in_text(da_tokenizer)
spacy.tests.lang.da.test_exceptions.test_da_tokenizer_slash(da_tokenizer,text,n_tokens)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/es/test_exception.py----------------------------------------
A:spacy.tests.lang.es.test_exception.tokens->es_tokenizer(text)
spacy.tests.lang.es.test_exception.test_es_tokenizer_handles_abbr(es_tokenizer,text,lemma)
spacy.tests.lang.es.test_exception.test_es_tokenizer_handles_exc_in_text(es_tokenizer)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/es/test_noun_chunks.py----------------------------------------
A:spacy.tests.lang.es.test_noun_chunks.doc->es_tokenizer('en Oxford este verano')
spacy.tests.lang.es.test_noun_chunks.test_es_noun_chunks(es_vocab,words,heads,deps,pos,chunk_offsets)
spacy.tests.lang.es.test_noun_chunks.test_noun_chunks_is_parsed_es(es_tokenizer)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/es/test_text.py----------------------------------------
A:spacy.tests.lang.es.test_text.nlp->Spanish()
A:spacy.tests.lang.es.test_text.doc->nlp(text)
A:spacy.tests.lang.es.test_text.tokens->es_tokenizer(text)
spacy.tests.lang.es.test_text.test_es_lex_attrs_capitals(word)
spacy.tests.lang.es.test_text.test_es_tokenizer_handles_cnts(es_tokenizer,text,length)
spacy.tests.lang.es.test_text.test_es_tokenizer_handles_long_text(es_tokenizer)
spacy.tests.lang.es.test_text.test_issue3803()
spacy.tests.lang.es.test_text.test_lex_attrs_like_number(es_tokenizer,text,match)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/es/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/it/test_stopwords.py----------------------------------------
spacy.tests.lang.it.test_stopwords.test_stopwords_basic(it_tokenizer,word)
spacy.tests.lang.it.test_stopwords.test_stopwords_elided(it_tokenizer,word)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/it/test_noun_chunks.py----------------------------------------
A:spacy.tests.lang.it.test_noun_chunks.doc->it_tokenizer('Sei andato a Oxford')
spacy.tests.lang.it.test_noun_chunks.test_it_noun_chunks(it_vocab,words,heads,deps,pos,chunk_offsets)
spacy.tests.lang.it.test_noun_chunks.test_noun_chunks_is_parsed_it(it_tokenizer)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/it/test_prefix_suffix_infix.py----------------------------------------
A:spacy.tests.lang.it.test_prefix_suffix_infix.tokens->it_tokenizer(text)
spacy.tests.lang.it.test_prefix_suffix_infix.test_contractions(it_tokenizer,text,expected_tokens)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/it/test_text.py----------------------------------------
A:spacy.tests.lang.it.test_text.doc->it_tokenizer("Vuoi un po' di zucchero?")
spacy.tests.lang.it.test_text.test_issue2822(it_tokenizer)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/it/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/sa/test_text.py----------------------------------------
A:spacy.tests.lang.sa.test_text.tokens->sa_tokenizer(text)
spacy.tests.lang.sa.test_text.test_lex_attrs_like_number(sa_tokenizer,text,match)
spacy.tests.lang.sa.test_text.test_sa_tokenizer_handles_cnts(sa_tokenizer,text,length)
spacy.tests.lang.sa.test_text.test_sa_tokenizer_handles_long_text(sa_tokenizer)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/sa/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/zh/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.zh.test_tokenizer.user_dict->_get_pkuseg_trie_data(zh_tokenizer_pkuseg.pkuseg_seg.preprocesser.trie)
A:spacy.tests.lang.zh.test_tokenizer.updated_user_dict->_get_pkuseg_trie_data(zh_tokenizer_pkuseg.pkuseg_seg.preprocesser.trie)
A:spacy.tests.lang.zh.test_tokenizer.reset_user_dict->_get_pkuseg_trie_data(zh_tokenizer_pkuseg.pkuseg_seg.preprocesser.trie)
A:spacy.tests.lang.zh.test_tokenizer.tokens->zh_tokenizer_char('I   like cheese.')
A:spacy.tests.lang.zh.test_tokenizer.nlp->spacy.lang.zh.Chinese.from_config(config)
spacy.tests.lang.zh.test_tokenizer.test_zh_extra_spaces(zh_tokenizer_char)
spacy.tests.lang.zh.test_tokenizer.test_zh_tokenizer_char(zh_tokenizer_char,text)
spacy.tests.lang.zh.test_tokenizer.test_zh_tokenizer_jieba(zh_tokenizer_jieba,text,expected_tokens)
spacy.tests.lang.zh.test_tokenizer.test_zh_tokenizer_pkuseg(zh_tokenizer_pkuseg,text,expected_tokens)
spacy.tests.lang.zh.test_tokenizer.test_zh_tokenizer_pkuseg_user_dict(zh_tokenizer_pkuseg,zh_tokenizer_char)
spacy.tests.lang.zh.test_tokenizer.test_zh_uninitialized_pkuseg()
spacy.tests.lang.zh.test_tokenizer.test_zh_unsupported_segmenter()


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/zh/test_text.py----------------------------------------
A:spacy.tests.lang.zh.test_text.tokens->zh_tokenizer_jieba(text)
spacy.tests.lang.zh.test_text.test_lex_attrs_like_number(zh_tokenizer_jieba,text,match)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/zh/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/zh/test_serialize.py----------------------------------------
A:spacy.tests.lang.zh.test_serialize.tokenizer_bytes->zh_tokenizer.to_bytes()
A:spacy.tests.lang.zh.test_serialize.nlp->spacy.lang.zh.Chinese.from_config(config)
spacy.tests.lang.zh.test_serialize.test_zh_tokenizer_serialize_char(zh_tokenizer_char)
spacy.tests.lang.zh.test_serialize.test_zh_tokenizer_serialize_jieba(zh_tokenizer_jieba)
spacy.tests.lang.zh.test_serialize.test_zh_tokenizer_serialize_pkuseg_with_processors(zh_tokenizer_pkuseg)
spacy.tests.lang.zh.test_serialize.zh_tokenizer_serialize(zh_tokenizer)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/nl/test_noun_chunks.py----------------------------------------
A:spacy.tests.lang.nl.test_noun_chunks.doc->Doc(nl_vocab, words=['Dit', 'programma', 'wordt', 'beschouwd', 'als', "'s", 'werelds', 'eerste', 'computerprogramma'], deps=['det', 'nsubj:pass', 'aux:pass', 'ROOT', 'mark', 'det', 'fixed', 'amod', 'xcomp'], heads=[1, 3, 3, 3, 8, 8, 5, 8, 3], pos=['DET', 'NOUN', 'AUX', 'VERB', 'SCONJ', 'DET', 'NOUN', 'ADJ', 'NOUN'])
A:spacy.tests.lang.nl.test_noun_chunks.chunks->list(doc.noun_chunks)
spacy.tests.lang.nl.test_noun_chunks.nl_reference_chunking()
spacy.tests.lang.nl.test_noun_chunks.nl_sample(nl_vocab)
spacy.tests.lang.nl.test_noun_chunks.test_chunking(nl_sample,nl_reference_chunking)
spacy.tests.lang.nl.test_noun_chunks.test_need_dep(nl_tokenizer)
spacy.tests.lang.nl.test_noun_chunks.test_no_overlapping_chunks(nl_vocab)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/nl/test_text.py----------------------------------------
A:spacy.tests.lang.nl.test_text.tokens->nl_tokenizer(text)
spacy.tests.lang.nl.test_text.test_nl_lex_attrs_capitals(word)
spacy.tests.lang.nl.test_text.test_tokenizer_doesnt_split_hyphens(nl_tokenizer,text,num_tokens)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/nl/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/ky/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.ky.test_tokenizer.tokens->ky_tokenizer(text)
spacy.tests.lang.ky.test_tokenizer.test_ky_tokenizer_handles_norm_exceptions(ky_tokenizer,text,norms)
spacy.tests.lang.ky.test_tokenizer.test_ky_tokenizer_handles_testcases(ky_tokenizer,text,expected_tokens)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/ky/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/id/test_noun_chunks.py----------------------------------------
A:spacy.tests.lang.id.test_noun_chunks.doc->id_tokenizer('sebelas')
spacy.tests.lang.id.test_noun_chunks.test_noun_chunks_is_parsed_id(id_tokenizer)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/id/test_prefix_suffix_infix.py----------------------------------------
A:spacy.tests.lang.id.test_prefix_suffix_infix.tokens->id_tokenizer('Arsene Wenger--manajer Arsenal--melakukan konferensi pers.')
spacy.tests.lang.id.test_prefix_suffix_infix.test_id_tokenizer_splits_comma_infix(id_tokenizer,text)
spacy.tests.lang.id.test_prefix_suffix_infix.test_id_tokenizer_splits_double_hyphen_infix(id_tokenizer)
spacy.tests.lang.id.test_prefix_suffix_infix.test_id_tokenizer_splits_ellipsis_infix(id_tokenizer,text)
spacy.tests.lang.id.test_prefix_suffix_infix.test_id_tokenizer_splits_even_wrap(id_tokenizer,text)
spacy.tests.lang.id.test_prefix_suffix_infix.test_id_tokenizer_splits_even_wrap_interact(id_tokenizer,text)
spacy.tests.lang.id.test_prefix_suffix_infix.test_id_tokenizer_splits_hyphens(id_tokenizer,text,length)
spacy.tests.lang.id.test_prefix_suffix_infix.test_id_tokenizer_splits_no_punct(id_tokenizer,text)
spacy.tests.lang.id.test_prefix_suffix_infix.test_id_tokenizer_splits_no_special(id_tokenizer,text)
spacy.tests.lang.id.test_prefix_suffix_infix.test_id_tokenizer_splits_numeric_range(id_tokenizer,text)
spacy.tests.lang.id.test_prefix_suffix_infix.test_id_tokenizer_splits_period_infix(id_tokenizer,text)
spacy.tests.lang.id.test_prefix_suffix_infix.test_id_tokenizer_splits_prefix_interact(id_tokenizer,text,length)
spacy.tests.lang.id.test_prefix_suffix_infix.test_id_tokenizer_splits_prefix_punct(id_tokenizer,text)
spacy.tests.lang.id.test_prefix_suffix_infix.test_id_tokenizer_splits_suffix_interact(id_tokenizer,text)
spacy.tests.lang.id.test_prefix_suffix_infix.test_id_tokenizer_splits_suffix_punct(id_tokenizer,text)
spacy.tests.lang.id.test_prefix_suffix_infix.test_id_tokenizer_splits_uneven_wrap_interact(id_tokenizer,text)
spacy.tests.lang.id.test_prefix_suffix_infix.test_tokenizer_splits_uneven_wrap(id_tokenizer,text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/id/test_text.py----------------------------------------
spacy.tests.lang.id.test_text.test_id_lex_attrs_capitals(word)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/id/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/ar/test_text.py----------------------------------------
A:spacy.tests.lang.ar.test_text.tokens->ar_tokenizer(text)
spacy.tests.lang.ar.test_text.test_ar_tokenizer_handles_long_text(ar_tokenizer)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/ar/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/ar/test_exceptions.py----------------------------------------
A:spacy.tests.lang.ar.test_exceptions.tokens->ar_tokenizer(text)
spacy.tests.lang.ar.test_exceptions.test_ar_tokenizer_handles_abbr(ar_tokenizer,text)
spacy.tests.lang.ar.test_exceptions.test_ar_tokenizer_handles_exc_in_text(ar_tokenizer)
spacy.tests.lang.ar.test_exceptions.test_ar_tokenizer_handles_exc_in_text_2(ar_tokenizer)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/dsb/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.dsb.test_tokenizer.tokens->dsb_tokenizer(text)
spacy.tests.lang.dsb.test_tokenizer.test_dsb_tokenizer_basic(dsb_tokenizer,text,expected_tokens)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/dsb/test_text.py----------------------------------------
A:spacy.tests.lang.dsb.test_text.tokens->dsb_tokenizer(text)
spacy.tests.lang.dsb.test_text.test_lex_attrs_like_number(dsb_tokenizer,text,match)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/dsb/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/ro/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.ro.test_tokenizer.tokens->ro_tokenizer(text)
spacy.tests.lang.ro.test_tokenizer.test_ro_tokenizer_handles_testcases(ro_tokenizer,text,expected_tokens)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/ro/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/en/test_punct.py----------------------------------------
A:spacy.tests.lang.en.test_punct.tokens->en_tokenizer(text)
A:spacy.tests.lang.en.test_punct.tokens_punct->en_tokenizer("''")
A:spacy.tests.lang.en.test_punct.match->en_search_prefixes(text)
spacy.tests.lang.en.test_punct.test_en_tokenizer_handles_only_punct(en_tokenizer,text)
spacy.tests.lang.en.test_punct.test_en_tokenizer_splits_bracket_period(en_tokenizer)
spacy.tests.lang.en.test_punct.test_en_tokenizer_splits_close_punct(en_tokenizer,punct,text)
spacy.tests.lang.en.test_punct.test_en_tokenizer_splits_double_end_quote(en_tokenizer,text)
spacy.tests.lang.en.test_punct.test_en_tokenizer_splits_open_appostrophe(en_tokenizer,text)
spacy.tests.lang.en.test_punct.test_en_tokenizer_splits_open_close_punct(en_tokenizer,punct_open,punct_close,text)
spacy.tests.lang.en.test_punct.test_en_tokenizer_splits_open_punct(en_tokenizer,punct,text)
spacy.tests.lang.en.test_punct.test_en_tokenizer_splits_pre_punct_regex(text,punct)
spacy.tests.lang.en.test_punct.test_en_tokenizer_splits_same_close_punct(en_tokenizer,punct,text)
spacy.tests.lang.en.test_punct.test_en_tokenizer_splits_same_open_punct(en_tokenizer,punct,text)
spacy.tests.lang.en.test_punct.test_en_tokenizer_splits_two_diff_close_punct(en_tokenizer,punct,punct_add,text)
spacy.tests.lang.en.test_punct.test_en_tokenizer_splits_two_diff_open_punct(en_tokenizer,punct,punct_add,text)
spacy.tests.lang.en.test_punct.test_en_tokenizer_two_diff_punct(en_tokenizer,punct_open,punct_close,punct_open2,punct_close2,text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/en/test_noun_chunks.py----------------------------------------
A:spacy.tests.lang.en.test_noun_chunks.doc->en_tokenizer('This is a sentence')
A:spacy.tests.lang.en.test_noun_chunks.chunks->list(doc.noun_chunks)
A:spacy.tests.lang.en.test_noun_chunks.doc_chunks->list(doc.noun_chunks)
A:spacy.tests.lang.en.test_noun_chunks.span_chunks->list(span.noun_chunks)
spacy.tests.lang.en.test_noun_chunks.doc(en_vocab)
spacy.tests.lang.en.test_noun_chunks.test_en_noun_chunks_not_nested(doc,en_vocab)
spacy.tests.lang.en.test_noun_chunks.test_noun_chunks_is_parsed(en_tokenizer)
spacy.tests.lang.en.test_noun_chunks.test_noun_chunks_span(doc,en_tokenizer)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/en/test_prefix_suffix_infix.py----------------------------------------
A:spacy.tests.lang.en.test_prefix_suffix_infix.tokens->en_tokenizer(text)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_comma_infix(en_tokenizer,text)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_double_hyphen_infix(en_tokenizer)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_ellipsis_infix(en_tokenizer,text)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_em_dash_infix(en_tokenizer)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_even_wrap(en_tokenizer,text)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_even_wrap_interact(en_tokenizer,text)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_hyphens(en_tokenizer,text)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_no_punct(en_tokenizer,text)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_no_special(en_tokenizer,text)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_numeric_range(en_tokenizer,text)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_period_abbr(en_tokenizer)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_period_infix(en_tokenizer,text)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_prefix_interact(en_tokenizer,text,length)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_prefix_punct(en_tokenizer,text)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_suffix_interact(en_tokenizer,text)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_suffix_punct(en_tokenizer,text)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_uneven_wrap(en_tokenizer,text)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_uneven_wrap_interact(en_tokenizer,text)
spacy.tests.lang.en.test_prefix_suffix_infix.test_final_period(en_tokenizer,text,length)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/en/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.en.test_tokenizer.doc->es_tokenizer('â€”Yo me llamo... â€“murmurÃ³ el niÃ±oâ€“ Emilio SÃ¡nchez PÃ©rez.')
A:spacy.tests.lang.en.test_tokenizer.tokens->en_tokenizer(text)
spacy.tests.lang.en.test_tokenizer.test_control_issue792(en_tokenizer,text)
spacy.tests.lang.en.test_tokenizer.test_issue10699(en_tokenizer,text)
spacy.tests.lang.en.test_tokenizer.test_issue1698(en_tokenizer,text)
spacy.tests.lang.en.test_tokenizer.test_issue1758(en_tokenizer)
spacy.tests.lang.en.test_tokenizer.test_issue1773(en_tokenizer)
spacy.tests.lang.en.test_tokenizer.test_issue3277(es_tokenizer)
spacy.tests.lang.en.test_tokenizer.test_issue351(en_tokenizer)
spacy.tests.lang.en.test_tokenizer.test_issue3521(en_tokenizer,word)
spacy.tests.lang.en.test_tokenizer.test_issue360(en_tokenizer)
spacy.tests.lang.en.test_tokenizer.test_issue736(en_tokenizer,text,number)
spacy.tests.lang.en.test_tokenizer.test_issue740(en_tokenizer,text)
spacy.tests.lang.en.test_tokenizer.test_issue744(en_tokenizer,text)
spacy.tests.lang.en.test_tokenizer.test_issue759(en_tokenizer,text,is_num)
spacy.tests.lang.en.test_tokenizer.test_issue775(en_tokenizer,text)
spacy.tests.lang.en.test_tokenizer.test_issue792(en_tokenizer,text)
spacy.tests.lang.en.test_tokenizer.test_issue859(en_tokenizer,text)
spacy.tests.lang.en.test_tokenizer.test_issue886(en_tokenizer,text)
spacy.tests.lang.en.test_tokenizer.test_issue891(en_tokenizer,text)
spacy.tests.lang.en.test_tokenizer.test_issue957(en_tokenizer)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/en/test_indices.py----------------------------------------
A:spacy.tests.lang.en.test_indices.tokens->en_tokenizer(text)
spacy.tests.lang.en.test_indices.test_en_complex_punct(en_tokenizer)
spacy.tests.lang.en.test_indices.test_en_simple_punct(en_tokenizer)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/en/test_customized_tokenizer.py----------------------------------------
A:spacy.tests.lang.en.test_customized_tokenizer.prefix_re->compile_prefix_regex(English.Defaults.prefixes)
A:spacy.tests.lang.en.test_customized_tokenizer.suffix_re->compile_suffix_regex(English.Defaults.suffixes)
A:spacy.tests.lang.en.test_customized_tokenizer.infix_re->compile_infix_regex(custom_infixes)
A:spacy.tests.lang.en.test_customized_tokenizer.token_match_re->re.compile('a-b')
spacy.tests.lang.en.test_customized_tokenizer.custom_en_tokenizer(en_vocab)
spacy.tests.lang.en.test_customized_tokenizer.test_en_customized_tokenizer_handles_infixes(custom_en_tokenizer)
spacy.tests.lang.en.test_customized_tokenizer.test_en_customized_tokenizer_handles_rules(custom_en_tokenizer)
spacy.tests.lang.en.test_customized_tokenizer.test_en_customized_tokenizer_handles_rules_property(custom_en_tokenizer)
spacy.tests.lang.en.test_customized_tokenizer.test_en_customized_tokenizer_handles_token_match(custom_en_tokenizer)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/en/test_sbd.py----------------------------------------
A:spacy.tests.lang.en.test_sbd.doc->Doc(en_vocab, words=words, heads=heads, deps=deps)
A:spacy.tests.lang.en.test_sbd.sents->list(doc.sents)
spacy.tests.lang.en.test_sbd.test_en_sbd_single_punct(en_vocab,words,punct)
spacy.tests.lang.en.test_sbd.test_en_sentence_breaks(en_vocab,en_parser)
spacy.tests.lang.en.test_sbd.test_issue309(en_vocab)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/en/test_text.py----------------------------------------
A:spacy.tests.lang.en.test_text.tokens->en_tokenizer(text)
spacy.tests.lang.en.test_text.test_en_lex_attrs_capitals(word)
spacy.tests.lang.en.test_text.test_en_lex_attrs_like_number_for_ordinal(word)
spacy.tests.lang.en.test_text.test_en_tokenizer_handles_cnts(en_tokenizer,text,length)
spacy.tests.lang.en.test_text.test_en_tokenizer_handles_long_text(en_tokenizer)
spacy.tests.lang.en.test_text.test_lex_attrs_like_number(en_tokenizer,text,match)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/en/test_parser.py----------------------------------------
A:spacy.tests.lang.en.test_parser.doc->Doc(en_vocab, words=words, pos=pos, deps=deps, heads=heads)
A:spacy.tests.lang.en.test_parser.chunks->list(doc.noun_chunks)
spacy.tests.lang.en.test_parser.test_en_parser_noun_chunks_appositional_modifiers(en_vocab)
spacy.tests.lang.en.test_parser.test_en_parser_noun_chunks_coordinated(en_vocab)
spacy.tests.lang.en.test_parser.test_en_parser_noun_chunks_dative(en_vocab)
spacy.tests.lang.en.test_parser.test_en_parser_noun_chunks_pp_chunks(en_vocab)
spacy.tests.lang.en.test_parser.test_en_parser_noun_chunks_standard(en_vocab)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/en/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/en/test_exceptions.py----------------------------------------
A:spacy.tests.lang.en.test_exceptions.tokens->en_tokenizer(text)
A:spacy.tests.lang.en.test_exceptions.tokens_lower->en_tokenizer(text_lower)
A:spacy.tests.lang.en.test_exceptions.tokens_title->en_tokenizer(text_title)
spacy.tests.lang.en.test_exceptions.test_en_lex_attrs_norm_exceptions(en_tokenizer,text,norm)
spacy.tests.lang.en.test_exceptions.test_en_tokenizer_doesnt_split_apos_exc(en_tokenizer,text)
spacy.tests.lang.en.test_exceptions.test_en_tokenizer_excludes_ambiguous(en_tokenizer,exc)
spacy.tests.lang.en.test_exceptions.test_en_tokenizer_handles_abbr(en_tokenizer,text)
spacy.tests.lang.en.test_exceptions.test_en_tokenizer_handles_basic_contraction(en_tokenizer)
spacy.tests.lang.en.test_exceptions.test_en_tokenizer_handles_basic_contraction_punct(en_tokenizer,text)
spacy.tests.lang.en.test_exceptions.test_en_tokenizer_handles_capitalization(en_tokenizer,text_lower,text_title)
spacy.tests.lang.en.test_exceptions.test_en_tokenizer_handles_exc_in_text(en_tokenizer)
spacy.tests.lang.en.test_exceptions.test_en_tokenizer_handles_ll_contraction(en_tokenizer,text)
spacy.tests.lang.en.test_exceptions.test_en_tokenizer_handles_poss_contraction(en_tokenizer,text_poss,text)
spacy.tests.lang.en.test_exceptions.test_en_tokenizer_handles_times(en_tokenizer,text)
spacy.tests.lang.en.test_exceptions.test_en_tokenizer_keeps_title_case(en_tokenizer,pron,contraction)
spacy.tests.lang.en.test_exceptions.test_en_tokenizer_norm_exceptions(en_tokenizer,text,norms)
spacy.tests.lang.en.test_exceptions.test_en_tokenizer_splits_defined_punct(en_tokenizer,wo_punct,w_punct)
spacy.tests.lang.en.test_exceptions.test_en_tokenizer_splits_trailing_apos(en_tokenizer,text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/hsb/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.hsb.test_tokenizer.tokens->hsb_tokenizer(text)
spacy.tests.lang.hsb.test_tokenizer.test_hsb_tokenizer_basic(hsb_tokenizer,text,expected_tokens)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/hsb/test_text.py----------------------------------------
A:spacy.tests.lang.hsb.test_text.tokens->hsb_tokenizer(text)
spacy.tests.lang.hsb.test_text.test_lex_attrs_like_number(hsb_tokenizer,text,match)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/hsb/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/ru/test_lemmatizer.py----------------------------------------
A:spacy.tests.lang.ru.test_lemmatizer.pytestmark->pytest.mark.filterwarnings('ignore::DeprecationWarning')
A:spacy.tests.lang.ru.test_lemmatizer.doc->Doc(ru_lookup_lemmatizer.vocab, words=[word])
A:spacy.tests.lang.ru.test_lemmatizer.result_lemmas->ru_lemmatizer.pymorphy2_lemmatize(doc[0])
spacy.tests.lang.ru.test_lemmatizer.test_ru_doc_lemmatization(ru_lemmatizer)
spacy.tests.lang.ru.test_lemmatizer.test_ru_doc_lookup_lemmatization(ru_lookup_lemmatizer)
spacy.tests.lang.ru.test_lemmatizer.test_ru_lemmatizer_noun_lemmas(ru_lemmatizer,text,lemmas)
spacy.tests.lang.ru.test_lemmatizer.test_ru_lemmatizer_punct(ru_lemmatizer)
spacy.tests.lang.ru.test_lemmatizer.test_ru_lemmatizer_works_with_different_pos_homonyms(ru_lemmatizer,text,pos,morph,lemma)
spacy.tests.lang.ru.test_lemmatizer.test_ru_lemmatizer_works_with_noun_homonyms(ru_lemmatizer,text,morph,lemma)
spacy.tests.lang.ru.test_lemmatizer.test_ru_lookup_lemmatizer(ru_lookup_lemmatizer,word,lemma)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/ru/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.ru.test_tokenizer.tokens->ru_tokenizer(text)
A:spacy.tests.lang.ru.test_tokenizer.tokens_punct->ru_tokenizer("''")
spacy.tests.lang.ru.test_tokenizer.test_ru_tokenizer_handles_final_diacritic_and_period(ru_tokenizer,text)
spacy.tests.lang.ru.test_tokenizer.test_ru_tokenizer_handles_final_diacritics(ru_tokenizer,text)
spacy.tests.lang.ru.test_tokenizer.test_ru_tokenizer_handles_only_punct(ru_tokenizer,text)
spacy.tests.lang.ru.test_tokenizer.test_ru_tokenizer_splits_bracket_period(ru_tokenizer)
spacy.tests.lang.ru.test_tokenizer.test_ru_tokenizer_splits_close_punct(ru_tokenizer,punct,text)
spacy.tests.lang.ru.test_tokenizer.test_ru_tokenizer_splits_double_end_quote(ru_tokenizer,text)
spacy.tests.lang.ru.test_tokenizer.test_ru_tokenizer_splits_open_appostrophe(ru_tokenizer,text)
spacy.tests.lang.ru.test_tokenizer.test_ru_tokenizer_splits_open_close_punct(ru_tokenizer,punct_open,punct_close,text)
spacy.tests.lang.ru.test_tokenizer.test_ru_tokenizer_splits_open_punct(ru_tokenizer,punct,text)
spacy.tests.lang.ru.test_tokenizer.test_ru_tokenizer_splits_same_close_punct(ru_tokenizer,punct,text)
spacy.tests.lang.ru.test_tokenizer.test_ru_tokenizer_splits_same_open_punct(ru_tokenizer,punct,text)
spacy.tests.lang.ru.test_tokenizer.test_ru_tokenizer_splits_trailing_dot(ru_tokenizer,text)
spacy.tests.lang.ru.test_tokenizer.test_ru_tokenizer_splits_two_diff_close_punct(ru_tokenizer,punct,punct_add,text)
spacy.tests.lang.ru.test_tokenizer.test_ru_tokenizer_splits_two_diff_open_punct(ru_tokenizer,punct,punct_add,text)
spacy.tests.lang.ru.test_tokenizer.test_ru_tokenizer_two_diff_punct(ru_tokenizer,punct_open,punct_close,punct_open2,punct_close2,text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/ru/test_text.py----------------------------------------
spacy.tests.lang.ru.test_text.test_ru_lex_attrs_capitals(word)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/ru/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/ru/test_exceptions.py----------------------------------------
A:spacy.tests.lang.ru.test_exceptions.tokens->ru_tokenizer(text)
spacy.tests.lang.ru.test_exceptions.test_ru_tokenizer_abbrev_exceptions(ru_tokenizer,text,norms)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/cs/test_text.py----------------------------------------
A:spacy.tests.lang.cs.test_text.tokens->cs_tokenizer(text)
spacy.tests.lang.cs.test_text.test_lex_attrs_like_number(cs_tokenizer,text,match)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/cs/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/lv/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.lv.test_tokenizer.tokens->lv_tokenizer(text)
spacy.tests.lang.lv.test_tokenizer.test_lv_tokenizer_basic(lv_tokenizer,text,expected_tokens)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/lv/test_text.py----------------------------------------
A:spacy.tests.lang.lv.test_text.tokens->lv_tokenizer(text)
spacy.tests.lang.lv.test_text.test_long_text(lv_tokenizer)
spacy.tests.lang.lv.test_text.test_ordinal_number(lv_tokenizer)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/lv/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/la/test_exception.py----------------------------------------
A:spacy.tests.lang.la.test_exception.tokens->la_tokenizer(text)
spacy.tests.lang.la.test_exception.test_la_tokenizer_handles_exc_in_text(la_tokenizer)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/la/test_text.py----------------------------------------
A:spacy.tests.lang.la.test_text.tokens->la_tokenizer(text)
spacy.tests.lang.la.test_text.test_la_lex_attrs_capitals(word)
spacy.tests.lang.la.test_text.test_lex_attrs_like_number(la_tokenizer,text,match)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/la/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/ta/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.ta.test_tokenizer.tokens->nlp(text)
A:spacy.tests.lang.ta.test_tokenizer.nlp->Tamil()
spacy.tests.lang.ta.test_tokenizer.test_ta_tokenizer_basic(ta_tokenizer,text,expected_tokens)
spacy.tests.lang.ta.test_tokenizer.test_ta_tokenizer_special_case(text,expected_tokens)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/ta/test_text.py----------------------------------------
A:spacy.tests.lang.ta.test_text.tokens->ta_tokenizer(text)
A:spacy.tests.lang.ta.test_text.nlp->Tamil()
A:spacy.tests.lang.ta.test_text.doc->nlp(text)
spacy.tests.lang.ta.test_text.test_long_text(ta_tokenizer,text,num_tokens)
spacy.tests.lang.ta.test_text.test_ta_sentencizer(text,num_sents)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/ta/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/fr/test_noun_chunks.py----------------------------------------
A:spacy.tests.lang.fr.test_noun_chunks.doc->fr_tokenizer("Je suis allÃ© Ã  l'Ã©cole")
spacy.tests.lang.fr.test_noun_chunks.test_fr_noun_chunks(fr_vocab,words,heads,deps,pos,chunk_offsets)
spacy.tests.lang.fr.test_noun_chunks.test_noun_chunks_is_parsed_fr(fr_tokenizer)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/fr/test_prefix_suffix_infix.py----------------------------------------
A:spacy.tests.lang.fr.test_prefix_suffix_infix.SPLIT_INFIX->"(?<=[{a}]\\')(?=[{a}])".format(a=ALPHA)
A:spacy.tests.lang.fr.test_prefix_suffix_infix.tokens->fr_tokenizer_w_infix(text)
spacy.tests.lang.fr.test_prefix_suffix_infix.test_issue768(text,expected_tokens)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/fr/test_text.py----------------------------------------
A:spacy.tests.lang.fr.test_text.tokens->fr_tokenizer(text)
spacy.tests.lang.fr.test_text.test_fr_lex_attrs_capitals(word)
spacy.tests.lang.fr.test_text.test_tokenizer_handles_long_text(fr_tokenizer)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/fr/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/fr/test_exceptions.py----------------------------------------
A:spacy.tests.lang.fr.test_exceptions.tokens->fr_tokenizer(text)
spacy.tests.lang.fr.test_exceptions.test_fr_tokenizer_handles_abbr(fr_tokenizer,text)
spacy.tests.lang.fr.test_exceptions.test_fr_tokenizer_handles_exc_in_text(fr_tokenizer)
spacy.tests.lang.fr.test_exceptions.test_fr_tokenizer_handles_exc_in_text_2(fr_tokenizer)
spacy.tests.lang.fr.test_exceptions.test_fr_tokenizer_handles_title(fr_tokenizer)
spacy.tests.lang.fr.test_exceptions.test_fr_tokenizer_handles_title_2(fr_tokenizer)
spacy.tests.lang.fr.test_exceptions.test_fr_tokenizer_handles_title_3(fr_tokenizer)
spacy.tests.lang.fr.test_exceptions.test_fr_tokenizer_infix_exceptions(fr_tokenizer,text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/ur/test_prefix_suffix_infix.py----------------------------------------
A:spacy.tests.lang.ur.test_prefix_suffix_infix.tokens->ur_tokenizer(text)
spacy.tests.lang.ur.test_prefix_suffix_infix.test_contractions(ur_tokenizer,text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/ur/test_text.py----------------------------------------
A:spacy.tests.lang.ur.test_text.tokens->ur_tokenizer(text)
spacy.tests.lang.ur.test_text.test_ur_tokenizer_handles_cnts(ur_tokenizer,text,length)
spacy.tests.lang.ur.test_text.test_ur_tokenizer_handles_long_text(ur_tokenizer)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/ur/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/ml/test_text.py----------------------------------------
A:spacy.tests.lang.ml.test_text.tokens->ml_tokenizer(text)
spacy.tests.lang.ml.test_text.test_ml_tokenizer_handles_cnts(ml_tokenizer,text,length)
spacy.tests.lang.ml.test_text.test_ml_tokenizer_handles_long_text(ml_tokenizer)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/ml/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/tr/test_noun_chunks.py----------------------------------------
A:spacy.tests.lang.tr.test_noun_chunks.doc->tr_tokenizer('DÃ¼n seni gÃ¶rdÃ¼m.')
spacy.tests.lang.tr.test_noun_chunks.test_noun_chunks_is_parsed(tr_tokenizer)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/tr/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.tr.test_tokenizer.tokens->tr_tokenizer(text)
spacy.tests.lang.tr.test_tokenizer.test_tr_tokenizer_handles_allcases(tr_tokenizer,text,expected_tokens)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/tr/test_text.py----------------------------------------
A:spacy.tests.lang.tr.test_text.tokens->tr_tokenizer(text)
spacy.tests.lang.tr.test_text.test_tr_lex_attrs_capitals(word)
spacy.tests.lang.tr.test_text.test_tr_lex_attrs_like_number_cardinal_ordinal(word)
spacy.tests.lang.tr.test_text.test_tr_tokenizer_handles_long_text(tr_tokenizer)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/tr/test_parser.py----------------------------------------
A:spacy.tests.lang.tr.test_parser.tokens->tr_tokenizer(text)
A:spacy.tests.lang.tr.test_parser.doc->Doc(tokens.vocab, words=[t.text for t in tokens], pos=pos, heads=heads, deps=deps)
A:spacy.tests.lang.tr.test_parser.chunks->list(doc.noun_chunks)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_acl_nmod(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_acl_nmod2(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_acl_simple(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_acl_verb(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_amod_simple(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_chain_nmod_head_with_amod_acl(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_chain_nmod_with_acl(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_chain_nmod_with_adj(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_conj_and_adj_phrase(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_conj_fixed_adj_phrase(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_conj_noun_head_verb(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_conj_simple(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_conj_subject(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_conj_three(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_conj_three2(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_det_amod_nmod(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_determiner_simple(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_flat_and_chain_nmod(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_flat_in_nmod(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_flat_name_lastname_and_title(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_flat_names_and_title(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_flat_names_and_title2(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_flat_simple(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_nmod_amod(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_nmod_simple(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_nmod_three(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_nmod_two(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_np_recursive_four_nouns(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_np_recursive_long_two_acls(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_np_recursive_no_nmod(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_np_recursive_nsubj_attached_to_pron_root(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_np_recursive_nsubj_in_subnp(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_np_recursive_nsubj_to_root(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_np_recursive_two_nmods(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_one_det_one_adj_simple(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_one_det_two_adjs_simple(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_two_adjs_simple(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_two_flats_conjed(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_two_nouns_in_nmod(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_two_nouns_in_nmod2(tr_tokenizer)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/tr/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/tl/test_punct.py----------------------------------------
A:spacy.tests.lang.tl.test_punct.tokens->tl_tokenizer(text)
A:spacy.tests.lang.tl.test_punct.tokens_punct->tl_tokenizer("''")
A:spacy.tests.lang.tl.test_punct.match->tl_search_prefixes(text)
spacy.tests.lang.tl.test_punct.test_tl_tokenizer_handles_only_punct(tl_tokenizer,text)
spacy.tests.lang.tl.test_punct.test_tl_tokenizer_split_open_punct(tl_tokenizer,punct,text)
spacy.tests.lang.tl.test_punct.test_tl_tokenizer_splits_bracket_period(tl_tokenizer)
spacy.tests.lang.tl.test_punct.test_tl_tokenizer_splits_close_punct(tl_tokenizer,punct,text)
spacy.tests.lang.tl.test_punct.test_tl_tokenizer_splits_double_end_quote(tl_tokenizer,text)
spacy.tests.lang.tl.test_punct.test_tl_tokenizer_splits_open_apostrophe(tl_tokenizer,text)
spacy.tests.lang.tl.test_punct.test_tl_tokenizer_splits_open_close_punct(tl_tokenizer,punct_open,punct_close,text)
spacy.tests.lang.tl.test_punct.test_tl_tokenizer_splits_pre_punct_regex(text,punct)
spacy.tests.lang.tl.test_punct.test_tl_tokenizer_splits_same_close_punct(tl_tokenizer,punct,text)
spacy.tests.lang.tl.test_punct.test_tl_tokenizer_splits_same_open_punct(tl_tokenizer,punct,text)
spacy.tests.lang.tl.test_punct.test_tl_tokenizer_splits_two_diff_close_punct(tl_tokenizer,punct,punct_add,text)
spacy.tests.lang.tl.test_punct.test_tl_tokenizer_splits_two_diff_open_punct(tl_tokenizer,punct,punct_add,text)
spacy.tests.lang.tl.test_punct.test_tl_tokenizer_two_diff_punct(tl_tokenizer,punct_open,punct_close,punct_open2,punct_close2,text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/tl/test_indices.py----------------------------------------
A:spacy.tests.lang.tl.test_indices.tokens->tl_tokenizer(text)
spacy.tests.lang.tl.test_indices.test_tl_simple_punct(tl_tokenizer)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/tl/test_text.py----------------------------------------
A:spacy.tests.lang.tl.test_text.tokens->tl_tokenizer(text)
spacy.tests.lang.tl.test_text.test_lex_attrs_like_number(tl_tokenizer,text,match)
spacy.tests.lang.tl.test_text.test_tl_lex_attrs_capitals(word)
spacy.tests.lang.tl.test_text.test_tl_tokenizer_handles_cnts(tl_tokenizer,text,length)
spacy.tests.lang.tl.test_text.test_tl_tokenizer_handles_long_text(tl_tokenizer)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/tl/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/he/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.he.test_tokenizer.tokens->he_tokenizer(text)
spacy.tests.lang.he.test_tokenizer.test_he_lex_attrs_like_number_for_ordinal(word)
spacy.tests.lang.he.test_tokenizer.test_he_tokenizer_handles_abbreviation(he_tokenizer,text,expected_tokens)
spacy.tests.lang.he.test_tokenizer.test_he_tokenizer_handles_punct(he_tokenizer,text,expected_tokens)
spacy.tests.lang.he.test_tokenizer.test_lex_attrs_like_number(he_tokenizer,text,match)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/he/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/ko/test_lemmatization.py----------------------------------------
spacy.tests.lang.ko.test_lemmatization.test_ko_lemmatizer_assigns(ko_tokenizer,word,lemma)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/ko/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.ko.test_tokenizer.tokens->ko_tokenizer('ë¯¸ë‹› ë¦¬í”¼í„°')
spacy.tests.lang.ko.test_tokenizer.test_ko_empty_doc(ko_tokenizer)
spacy.tests.lang.ko.test_tokenizer.test_ko_spacy_tokenizer(ko_tokenizer_tokenizer,text,expected_tokens)
spacy.tests.lang.ko.test_tokenizer.test_ko_tokenizer(ko_tokenizer,text,expected_tokens)
spacy.tests.lang.ko.test_tokenizer.test_ko_tokenizer_full_tags(ko_tokenizer,text,expected_tags)
spacy.tests.lang.ko.test_tokenizer.test_ko_tokenizer_pos(ko_tokenizer,text,expected_pos)
spacy.tests.lang.ko.test_tokenizer.test_ko_tokenizer_tags(ko_tokenizer,text,expected_tags)
spacy.tests.lang.ko.test_tokenizer.test_ko_tokenizer_unknown_tag(ko_tokenizer)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/ko/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/ko/test_serialize.py----------------------------------------
A:spacy.tests.lang.ko.test_serialize.tokenizer_bytes->ko_tokenizer.to_bytes()
A:spacy.tests.lang.ko.test_serialize.nlp->Korean()
A:spacy.tests.lang.ko.test_serialize.b->pickle.dumps(ko_tokenizer)
A:spacy.tests.lang.ko.test_serialize.ko_tokenizer_re->pickle.loads(b)
spacy.tests.lang.ko.test_serialize.test_ko_tokenizer_pickle(ko_tokenizer)
spacy.tests.lang.ko.test_serialize.test_ko_tokenizer_serialize(ko_tokenizer)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/nb/test_noun_chunks.py----------------------------------------
A:spacy.tests.lang.nb.test_noun_chunks.doc->nb_tokenizer('SmÃ¸rsausen brukes bl.a. til')
spacy.tests.lang.nb.test_noun_chunks.test_noun_chunks_is_parsed_nb(nb_tokenizer)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/nb/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.nb.test_tokenizer.tokens->nb_tokenizer(text)
spacy.tests.lang.nb.test_tokenizer.test_nb_tokenizer_handles_exception_cases(nb_tokenizer,text,expected_tokens)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/nb/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/hy/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.hy.test_tokenizer.tokens->hy_tokenizer(text)
spacy.tests.lang.hy.test_tokenizer.test_ga_tokenizer_handles_exception_cases(hy_tokenizer,text,expected_tokens)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/hy/test_text.py----------------------------------------
spacy.tests.lang.hy.test_text.test_hy_lex_attrs_capitals(word)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/hy/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/sr/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.sr.test_tokenizer.tokens->sr_tokenizer(text)
A:spacy.tests.lang.sr.test_tokenizer.tokens_punct->sr_tokenizer("''")
spacy.tests.lang.sr.test_tokenizer.test_sr_tokenizer_handles_only_punct(sr_tokenizer,text)
spacy.tests.lang.sr.test_tokenizer.test_sr_tokenizer_splits_bracket_period(sr_tokenizer)
spacy.tests.lang.sr.test_tokenizer.test_sr_tokenizer_splits_close_punct(sr_tokenizer,punct,text)
spacy.tests.lang.sr.test_tokenizer.test_sr_tokenizer_splits_double_end_quote(sr_tokenizer,text)
spacy.tests.lang.sr.test_tokenizer.test_sr_tokenizer_splits_open_appostrophe(sr_tokenizer,text)
spacy.tests.lang.sr.test_tokenizer.test_sr_tokenizer_splits_open_close_punct(sr_tokenizer,punct_open,punct_close,text)
spacy.tests.lang.sr.test_tokenizer.test_sr_tokenizer_splits_open_punct(sr_tokenizer,punct,text)
spacy.tests.lang.sr.test_tokenizer.test_sr_tokenizer_splits_same_close_punct(sr_tokenizer,punct,text)
spacy.tests.lang.sr.test_tokenizer.test_sr_tokenizer_splits_same_open_punct(sr_tokenizer,punct,text)
spacy.tests.lang.sr.test_tokenizer.test_sr_tokenizer_splits_trailing_dot(sr_tokenizer,text)
spacy.tests.lang.sr.test_tokenizer.test_sr_tokenizer_splits_two_diff_close_punct(sr_tokenizer,punct,punct_add,text)
spacy.tests.lang.sr.test_tokenizer.test_sr_tokenizer_splits_two_diff_open_punct(sr_tokenizer,punct,punct_add,text)
spacy.tests.lang.sr.test_tokenizer.test_sr_tokenizer_two_diff_punct(sr_tokenizer,punct_open,punct_close,punct_open2,punct_close2,text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/sr/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/sr/test_exceptions.py----------------------------------------
A:spacy.tests.lang.sr.test_exceptions.tokens->sr_tokenizer(text)
spacy.tests.lang.sr.test_exceptions.test_sr_tokenizer_abbrev_exceptions(sr_tokenizer,text,norms,lemmas)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/bn/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.bn.test_tokenizer.tokens->bn_tokenizer(text)
spacy.tests.lang.bn.test_tokenizer.test_bn_tokenizer_handles_long_text(bn_tokenizer)
spacy.tests.lang.bn.test_tokenizer.test_bn_tokenizer_handles_testcases(bn_tokenizer,text,expected_tokens)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/bn/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/hu/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.hu.test_tokenizer.tokens->hu_tokenizer(text)
spacy.tests.lang.hu.test_tokenizer.test_hu_tokenizer_handles_testcases(hu_tokenizer,text,expected_tokens)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/hu/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/yo/test_text.py----------------------------------------
A:spacy.tests.lang.yo.test_text.tokens->yo_tokenizer(text)
spacy.tests.lang.yo.test_text.test_lex_attrs_like_number(yo_tokenizer,text,match)
spacy.tests.lang.yo.test_text.test_yo_lex_attrs_capitals(word)
spacy.tests.lang.yo.test_text.test_yo_tokenizer_handles_long_text(yo_tokenizer)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/yo/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/sk/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.sk.test_tokenizer.tokens->sk_tokenizer(text)
spacy.tests.lang.sk.test_tokenizer.test_sk_tokenizer_basic(sk_tokenizer,text,expected_tokens)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/sk/test_text.py----------------------------------------
A:spacy.tests.lang.sk.test_text.tokens->sk_tokenizer(text)
spacy.tests.lang.sk.test_text.test_lex_attrs_like_number(sk_tokenizer,text,match)
spacy.tests.lang.sk.test_text.test_long_text(sk_tokenizer)
spacy.tests.lang.sk.test_text.test_ordinal_number(sk_tokenizer)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/sk/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/el/test_exception.py----------------------------------------
A:spacy.tests.lang.el.test_exception.tokens->el_tokenizer(text)
spacy.tests.lang.el.test_exception.test_el_tokenizer_handles_abbr(el_tokenizer,text)
spacy.tests.lang.el.test_exception.test_el_tokenizer_handles_exc_in_text(el_tokenizer)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/el/test_noun_chunks.py----------------------------------------
A:spacy.tests.lang.el.test_noun_chunks.doc->el_tokenizer('ÎµÎ¯Î½Î±Î¹ Ï‡ÏŽÏÎ± Ï„Î·Ï‚ Î½Î¿Ï„Î¹Î¿Î±Î½Î±Ï„Î¿Î»Î¹ÎºÎ®Ï‚')
spacy.tests.lang.el.test_noun_chunks.test_noun_chunks_is_parsed_el(el_tokenizer)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/el/test_text.py----------------------------------------
A:spacy.tests.lang.el.test_text.tokens->el_tokenizer(text)
spacy.tests.lang.el.test_text.test_el_tokenizer_handles_cnts(el_tokenizer,text,length)
spacy.tests.lang.el.test_text.test_el_tokenizer_handles_long_text(el_tokenizer)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/el/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/ca/test_exception.py----------------------------------------
A:spacy.tests.lang.ca.test_exception.tokens->ca_tokenizer(text)
A:spacy.tests.lang.ca.test_exception.doc->ca_tokenizer(text)
spacy.tests.lang.ca.test_exception.test_ca_tokenizer_handles_abbr(ca_tokenizer,text,lemma)
spacy.tests.lang.ca.test_exception.test_ca_tokenizer_handles_exc_in_text(ca_tokenizer)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/ca/test_prefix_suffix_infix.py----------------------------------------
A:spacy.tests.lang.ca.test_prefix_suffix_infix.tokens->ca_tokenizer(text)
spacy.tests.lang.ca.test_prefix_suffix_infix.test_contractions(ca_tokenizer,text,expected_tokens)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/ca/test_text.py----------------------------------------
A:spacy.tests.lang.ca.test_text.tokens->ca_tokenizer(text)
spacy.tests.lang.ca.test_text.test_ca_lex_attrs_like_number(ca_tokenizer,text,match)
spacy.tests.lang.ca.test_text.test_ca_tokenizer_handles_cnts(ca_tokenizer,text,length)
spacy.tests.lang.ca.test_text.test_ca_tokenizer_handles_long_text(ca_tokenizer)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/ca/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/grc/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.grc.test_tokenizer.tokens->grc_tokenizer(text)
spacy.tests.lang.grc.test_tokenizer.test_grc_tokenizer(grc_tokenizer,text,expected_tokens)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/grc/test_text.py----------------------------------------
A:spacy.tests.lang.grc.test_text.tokens->grc_tokenizer(text)
spacy.tests.lang.grc.test_text.test_lex_attrs_like_number(grc_tokenizer,text,match)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/grc/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/am/test_exception.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/am/test_text.py----------------------------------------
A:spacy.tests.lang.am.test_text.tokens->am_tokenizer(text)
spacy.tests.lang.am.test_text.test_am_tokenizer_handles_cnts(am_tokenizer,text,length)
spacy.tests.lang.am.test_text.test_am_tokenizer_handles_long_text(am_tokenizer)
spacy.tests.lang.am.test_text.test_lex_attrs_like_number(am_tokenizer,text,match)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/am/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/fa/test_noun_chunks.py----------------------------------------
A:spacy.tests.lang.fa.test_noun_chunks.doc->fa_tokenizer('Ø§ÛŒÙ† ÛŒÚ© Ø¬Ù…Ù„Ù‡ Ù†Ù…ÙˆÙ†Ù‡ Ù…ÛŒ Ø¨Ø§Ø´Ø¯.')
spacy.tests.lang.fa.test_noun_chunks.test_noun_chunks_is_parsed_fa(fa_tokenizer)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/fa/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/tt/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.tt.test_tokenizer.tokens->tt_tokenizer(text)
spacy.tests.lang.tt.test_tokenizer.test_tt_tokenizer_handles_norm_exceptions(tt_tokenizer,text,norms)
spacy.tests.lang.tt.test_tokenizer.test_tt_tokenizer_handles_testcases(tt_tokenizer,text,expected_tokens)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/tt/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/ti/test_exception.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/ti/test_text.py----------------------------------------
A:spacy.tests.lang.ti.test_text.tokens->ti_tokenizer(text)
spacy.tests.lang.ti.test_text.test_lex_attrs_like_number(ti_tokenizer,text,match)
spacy.tests.lang.ti.test_text.test_ti_tokenizer_handles_cnts(ti_tokenizer,text,length)
spacy.tests.lang.ti.test_text.test_ti_tokenizer_handles_long_text(ti_tokenizer)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/ti/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/uk/test_lemmatizer.py----------------------------------------
A:spacy.tests.lang.uk.test_lemmatizer.pytestmark->pytest.mark.filterwarnings('ignore::DeprecationWarning')
A:spacy.tests.lang.uk.test_lemmatizer.doc->Doc(uk_lookup_lemmatizer.vocab, words=[word])
spacy.tests.lang.uk.test_lemmatizer.test_uk_lemmatizer(uk_lemmatizer)
spacy.tests.lang.uk.test_lemmatizer.test_uk_lookup_lemmatizer(uk_lookup_lemmatizer,word,lemma)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/uk/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.uk.test_tokenizer.tokens->uk_tokenizer(text)
A:spacy.tests.lang.uk.test_tokenizer.tokens_punct->uk_tokenizer("''")
spacy.tests.lang.uk.test_tokenizer.test_uk_tokenizer_handles_final_diacritics(uk_tokenizer)
spacy.tests.lang.uk.test_tokenizer.test_uk_tokenizer_handles_only_punct(uk_tokenizer,text)
spacy.tests.lang.uk.test_tokenizer.test_uk_tokenizer_splits_bracket_period(uk_tokenizer)
spacy.tests.lang.uk.test_tokenizer.test_uk_tokenizer_splits_close_punct(uk_tokenizer,punct,text)
spacy.tests.lang.uk.test_tokenizer.test_uk_tokenizer_splits_double_end_quote(uk_tokenizer,text)
spacy.tests.lang.uk.test_tokenizer.test_uk_tokenizer_splits_open_appostrophe(uk_tokenizer,text)
spacy.tests.lang.uk.test_tokenizer.test_uk_tokenizer_splits_open_close_punct(uk_tokenizer,punct_open,punct_close,text)
spacy.tests.lang.uk.test_tokenizer.test_uk_tokenizer_splits_open_punct(uk_tokenizer,punct,text)
spacy.tests.lang.uk.test_tokenizer.test_uk_tokenizer_splits_same_close_punct(uk_tokenizer,punct,text)
spacy.tests.lang.uk.test_tokenizer.test_uk_tokenizer_splits_same_open_punct(uk_tokenizer,punct,text)
spacy.tests.lang.uk.test_tokenizer.test_uk_tokenizer_splits_trailing_dot(uk_tokenizer,text)
spacy.tests.lang.uk.test_tokenizer.test_uk_tokenizer_splits_two_diff_close_punct(uk_tokenizer,punct,punct_add,text)
spacy.tests.lang.uk.test_tokenizer.test_uk_tokenizer_splits_two_diff_open_punct(uk_tokenizer,punct,punct_add,text)
spacy.tests.lang.uk.test_tokenizer.test_uk_tokenizer_two_diff_punct(uk_tokenizer,punct_open,punct_close,punct_open2,punct_close2,text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/uk/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/uk/test_tokenizer_exc.py----------------------------------------
A:spacy.tests.lang.uk.test_tokenizer_exc.tokens->uk_tokenizer(text)
spacy.tests.lang.uk.test_tokenizer_exc.test_uk_tokenizer_abbrev_exceptions(uk_tokenizer,text,norms,lemmas)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/hr/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.hr.test_tokenizer.tokens->hr_tokenizer(text)
spacy.tests.lang.hr.test_tokenizer.test_hr_tokenizer_basic(hr_tokenizer,text,expected_tokens)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/hr/test_text.py----------------------------------------
A:spacy.tests.lang.hr.test_text.tokens->hr_tokenizer(text)
spacy.tests.lang.hr.test_text.test_long_text(hr_tokenizer)
spacy.tests.lang.hr.test_text.test_ordinal_number(hr_tokenizer)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/hr/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/sl/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.sl.test_tokenizer.tokens->sl_tokenizer(text)
spacy.tests.lang.sl.test_tokenizer.test_sl_tokenizer_basic(sl_tokenizer,text,expected_tokens)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/sl/test_text.py----------------------------------------
A:spacy.tests.lang.sl.test_text.tokens->sl_tokenizer(text)
spacy.tests.lang.sl.test_text.test_long_text(sl_tokenizer)
spacy.tests.lang.sl.test_text.test_ordinal_number(sl_tokenizer)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/sl/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/et/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.et.test_tokenizer.tokens->et_tokenizer(text)
spacy.tests.lang.et.test_tokenizer.test_et_tokenizer_basic(et_tokenizer,text,expected_tokens)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/et/test_text.py----------------------------------------
A:spacy.tests.lang.et.test_text.tokens->et_tokenizer(text)
spacy.tests.lang.et.test_text.test_long_text(et_tokenizer)
spacy.tests.lang.et.test_text.test_ordinal_number(et_tokenizer)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/et/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/gu/test_text.py----------------------------------------
A:spacy.tests.lang.gu.test_text.tokens->gu_tokenizer(text)
spacy.tests.lang.gu.test_text.test_gu_tokenizer_handlers_long_text(gu_tokenizer)
spacy.tests.lang.gu.test_text.test_gu_tokenizer_handles_cnts(gu_tokenizer,text,length)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/gu/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/pl/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.pl.test_tokenizer.tokens->pl_tokenizer(text)
spacy.tests.lang.pl.test_tokenizer.test_tokenizer_handles_testcases(pl_tokenizer,text,expected_tokens)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/pl/test_text.py----------------------------------------
A:spacy.tests.lang.pl.test_text.tokens->pl_tokenizer(text)
spacy.tests.lang.pl.test_text.test_lex_attrs_like_number(pl_tokenizer,text,match)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/pl/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/hi/test_lex_attrs.py----------------------------------------
A:spacy.tests.lang.hi.test_lex_attrs.tokens->hi_tokenizer(text)
spacy.tests.lang.hi.test_lex_attrs.test_hi_like_num(word)
spacy.tests.lang.hi.test_lex_attrs.test_hi_like_num_ordinal_words(word)
spacy.tests.lang.hi.test_lex_attrs.test_hi_norm(word,word_norm)
spacy.tests.lang.hi.test_lex_attrs.test_hi_tokenizer_handles_long_text(hi_tokenizer)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/hi/test_text.py----------------------------------------
A:spacy.tests.lang.hi.test_text.nlp->Hindi()
A:spacy.tests.lang.hi.test_text.doc->nlp('hi. how à¤¹à¥à¤. à¤¹à¥‹à¤Ÿà¤², à¤¹à¥‹à¤Ÿà¤²')
spacy.tests.lang.hi.test_text.test_issue3625()


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/hi/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/sv/test_noun_chunks.py----------------------------------------
A:spacy.tests.lang.sv.test_noun_chunks.doc->Doc(tokens.vocab, words=words, heads=heads, deps=deps, pos=pos)
A:spacy.tests.lang.sv.test_noun_chunks.tokens->sv_tokenizer(text)
A:spacy.tests.lang.sv.test_noun_chunks.noun_chunks->list(doc.noun_chunks)
spacy.tests.lang.sv.test_noun_chunks.test_noun_chunks_is_parsed_sv(sv_tokenizer)
spacy.tests.lang.sv.test_noun_chunks.test_sv_noun_chunks(sv_tokenizer,text,pos,deps,heads,expected_noun_chunks)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/sv/test_prefix_suffix_infix.py----------------------------------------
A:spacy.tests.lang.sv.test_prefix_suffix_infix.tokens->sv_tokenizer(text)
spacy.tests.lang.sv.test_prefix_suffix_infix.test_tokenizer_handles_no_punct(sv_tokenizer,text)
spacy.tests.lang.sv.test_prefix_suffix_infix.test_tokenizer_splits_comma_infix(sv_tokenizer,text)
spacy.tests.lang.sv.test_prefix_suffix_infix.test_tokenizer_splits_ellipsis_infix(sv_tokenizer,text)
spacy.tests.lang.sv.test_prefix_suffix_infix.test_tokenizer_splits_no_special(sv_tokenizer,text)
spacy.tests.lang.sv.test_prefix_suffix_infix.test_tokenizer_splits_period_infix(sv_tokenizer,text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/sv/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.sv.test_tokenizer.tokens->sv_tokenizer(text)
spacy.tests.lang.sv.test_tokenizer.test_sv_tokenizer_handles_exception_cases(sv_tokenizer,text,expected_tokens)
spacy.tests.lang.sv.test_tokenizer.test_sv_tokenizer_handles_verb_exceptions(sv_tokenizer,text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/sv/test_lex_attrs.py----------------------------------------
A:spacy.tests.lang.sv.test_lex_attrs.tokens->sv_tokenizer(text)
spacy.tests.lang.sv.test_lex_attrs.test_lex_attrs_like_number(sv_tokenizer,text,match)
spacy.tests.lang.sv.test_lex_attrs.test_sv_lex_attrs_capitals(word)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/sv/test_text.py----------------------------------------
A:spacy.tests.lang.sv.test_text.tokens->sv_tokenizer(text)
spacy.tests.lang.sv.test_text.test_sv_tokenizer_handles_long_text(sv_tokenizer)
spacy.tests.lang.sv.test_text.test_sv_tokenizer_handles_trailing_dot_for_i_in_sentence(sv_tokenizer)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/sv/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/sv/test_exceptions.py----------------------------------------
A:spacy.tests.lang.sv.test_exceptions.tokens->sv_tokenizer(text)
spacy.tests.lang.sv.test_exceptions.test_issue805(sv_tokenizer,text,expected_tokens)
spacy.tests.lang.sv.test_exceptions.test_sv_tokenizer_handles_abbr(sv_tokenizer,text)
spacy.tests.lang.sv.test_exceptions.test_sv_tokenizer_handles_ambiguous_abbr(sv_tokenizer,text)
spacy.tests.lang.sv.test_exceptions.test_sv_tokenizer_handles_custom_base_exc(sv_tokenizer)
spacy.tests.lang.sv.test_exceptions.test_sv_tokenizer_handles_exc_in_text(sv_tokenizer)
spacy.tests.lang.sv.test_exceptions.test_sv_tokenizer_handles_exception_cases(sv_tokenizer,text,expected_tokens)
spacy.tests.lang.sv.test_exceptions.test_sv_tokenizer_handles_verb_exceptions(sv_tokenizer,text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/ga/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.ga.test_tokenizer.tokens->ga_tokenizer(text)
spacy.tests.lang.ga.test_tokenizer.test_ga_tokenizer_handles_exception_cases(ga_tokenizer,text,expected_tokens)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/ga/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/af/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.af.test_tokenizer.tokens->af_tokenizer(text)
spacy.tests.lang.af.test_tokenizer.test_af_tokenizer_basic(af_tokenizer,text,expected_tokens)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/af/test_text.py----------------------------------------
A:spacy.tests.lang.af.test_text.tokens->af_tokenizer(text)
spacy.tests.lang.af.test_text.test_indefinite_article(af_tokenizer)
spacy.tests.lang.af.test_text.test_long_text(af_tokenizer)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/af/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/lt/test_text.py----------------------------------------
A:spacy.tests.lang.lt.test_text.tokens->lt_tokenizer(text)
spacy.tests.lang.lt.test_text.test_lt_lex_attrs_like_number(lt_tokenizer,text,match)
spacy.tests.lang.lt.test_text.test_lt_tokenizer_abbrev_exceptions(lt_tokenizer,text)
spacy.tests.lang.lt.test_text.test_lt_tokenizer_handles_long_text(lt_tokenizer)
spacy.tests.lang.lt.test_text.test_lt_tokenizer_handles_punct_abbrev(lt_tokenizer,text,length)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/lt/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/sq/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.sq.test_tokenizer.tokens->sq_tokenizer(text)
spacy.tests.lang.sq.test_tokenizer.test_sq_tokenizer_basic(sq_tokenizer,text,expected_tokens)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/sq/test_text.py----------------------------------------
A:spacy.tests.lang.sq.test_text.tokens->sq_tokenizer(text)
spacy.tests.lang.sq.test_text.test_long_text(sq_tokenizer)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/sq/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/vi/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.vi.test_tokenizer.tokens->vi_tokenizer(text)
A:spacy.tests.lang.vi.test_tokenizer.doc->nlp(text)
A:spacy.tests.lang.vi.test_tokenizer.nlp->spacy.lang.vi.Vietnamese.from_config({'nlp': {'tokenizer': {'use_pyvi': False}}})
spacy.tests.lang.vi.test_tokenizer.test_vi_tokenizer(vi_tokenizer,text,expected_tokens)
spacy.tests.lang.vi.test_tokenizer.test_vi_tokenizer_emptyish_texts(vi_tokenizer)
spacy.tests.lang.vi.test_tokenizer.test_vi_tokenizer_extra_spaces(vi_tokenizer)
spacy.tests.lang.vi.test_tokenizer.test_vi_tokenizer_naughty_strings(vi_tokenizer,text)
spacy.tests.lang.vi.test_tokenizer.test_vi_tokenizer_no_pyvi()


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/vi/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/lang/vi/test_serialize.py----------------------------------------
A:spacy.tests.lang.vi.test_serialize.tokenizer_bytes->vi_tokenizer.to_bytes()
A:spacy.tests.lang.vi.test_serialize.nlp->spacy.lang.vi.Vietnamese.from_config({'nlp': {'tokenizer': {'use_pyvi': False}}})
A:spacy.tests.lang.vi.test_serialize.nlp_bytes->spacy.lang.vi.Vietnamese.from_config({'nlp': {'tokenizer': {'use_pyvi': False}}}).to_bytes()
A:spacy.tests.lang.vi.test_serialize.nlp_r->Vietnamese()
A:spacy.tests.lang.vi.test_serialize.b->pickle.dumps(vi_tokenizer)
A:spacy.tests.lang.vi.test_serialize.vi_tokenizer_re->pickle.loads(b)
spacy.tests.lang.vi.test_serialize.test_vi_tokenizer_pickle(vi_tokenizer)
spacy.tests.lang.vi.test_serialize.test_vi_tokenizer_serialize(vi_tokenizer)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/serialize/test_serialize_kb.py----------------------------------------
A:spacy.tests.serialize.test_serialize_kb.kb1->_get_dummy_kb(en_vocab)
A:spacy.tests.serialize.test_serialize_kb.dir_path->ensure_path(d)
A:spacy.tests.serialize.test_serialize_kb.kb2->InMemoryLookupKB(vocab=en_vocab, entity_vector_length=3)
A:spacy.tests.serialize.test_serialize_kb.kb->SubInMemoryLookupKB(vocab=vocab, entity_vector_length=entity_vector_length, custom_field=custom_field)
A:spacy.tests.serialize.test_serialize_kb.candidates->sorted(kb.get_alias_candidates('double07'), key=lambda x: x.entity_)
A:spacy.tests.serialize.test_serialize_kb.config->Config().from_str(config_string)
A:spacy.tests.serialize.test_serialize_kb.nlp->load_model_from_config(config, auto_fill=True)
A:spacy.tests.serialize.test_serialize_kb.entity_linker->load_model_from_config(config, auto_fill=True).get_pipe('entity_linker')
A:spacy.tests.serialize.test_serialize_kb.nlp2->spacy.util.load_model_from_path(tmp_dir)
A:spacy.tests.serialize.test_serialize_kb.entity_linker2->spacy.util.load_model_from_path(tmp_dir).get_pipe('entity_linker')
spacy.tests.serialize.test_serialize_kb._check_kb(kb)
spacy.tests.serialize.test_serialize_kb._get_dummy_kb(vocab)
spacy.tests.serialize.test_serialize_kb.test_serialize_kb_disk(en_vocab)
spacy.tests.serialize.test_serialize_kb.test_serialize_subclassed_kb()


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/serialize/test_serialize_docbin.py----------------------------------------
A:spacy.tests.serialize.test_serialize_docbin.doc->Doc(en_vocab, words=['hello', 'world'])
A:spacy.tests.serialize.test_serialize_docbin.doc_bin->DocBin().from_bytes(DocBin(docs=[doc1, doc2]).to_bytes())
A:spacy.tests.serialize.test_serialize_docbin.doc_bin_bytes->DocBin(store_user_data=writer_flag).to_bytes()
A:spacy.tests.serialize.test_serialize_docbin.new_doc_bin->DocBin(store_user_data=True).from_bytes(doc_bin_bytes)
A:spacy.tests.serialize.test_serialize_docbin.doc_bin_2->DocBin(store_user_data=reader_flag).from_bytes(doc_bin_bytes)
A:spacy.tests.serialize.test_serialize_docbin.nlp->spacy.blank('en')
A:spacy.tests.serialize.test_serialize_docbin.bytes_data->DocBin().from_bytes(DocBin(docs=[doc1, doc2]).to_bytes()).to_bytes()
A:spacy.tests.serialize.test_serialize_docbin.reloaded_docs->list(doc_bin.get_docs(nlp.vocab))
A:spacy.tests.serialize.test_serialize_docbin.doc1->Doc(en_vocab, words=['that', "'s"])
A:spacy.tests.serialize.test_serialize_docbin.doc2->Doc(en_vocab, words=['that', "'s"], spaces=[False, False])
A:spacy.tests.serialize.test_serialize_docbin.(re_doc1, re_doc2)->DocBin().from_bytes(DocBin(docs=[doc1, doc2]).to_bytes()).get_docs(en_vocab)
A:spacy.tests.serialize.test_serialize_docbin.doc_bin_1->DocBin(store_user_data=writer_flag)
spacy.tests.serialize.test_serialize_docbin.test_issue4367()
spacy.tests.serialize.test_serialize_docbin.test_issue4528(en_vocab)
spacy.tests.serialize.test_serialize_docbin.test_issue5141(en_vocab)
spacy.tests.serialize.test_serialize_docbin.test_serialize_custom_extension(en_vocab,writer_flag,reader_flag,reader_value)
spacy.tests.serialize.test_serialize_docbin.test_serialize_doc_bin()
spacy.tests.serialize.test_serialize_docbin.test_serialize_doc_bin_unknown_spaces(en_vocab)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/serialize/test_serialize_doc.py----------------------------------------
A:spacy.tests.serialize.test_serialize_doc.nlp->English()
A:spacy.tests.serialize.test_serialize_doc.data->Doc(en_vocab, words=['hello', 'world', '!']).to_bytes()
A:spacy.tests.serialize.test_serialize_doc.vectors->Vectors(data=data, keys=['I', 'am', 'Matt'])
A:spacy.tests.serialize.test_serialize_doc.tagger->English().add_pipe('tagger')
A:spacy.tests.serialize.test_serialize_doc.heads_deps->numpy.asarray([[1, 397], [4, 436], [2, 426], [1, 402], [0, 8206900633647566924], [18446744073709551615, 440], [18446744073709551614, 442]], dtype='uint64')
A:spacy.tests.serialize.test_serialize_doc.doc->Doc(en_vocab, words=['hello', 'world', '!'])
A:spacy.tests.serialize.test_serialize_doc.new_doc->Doc(en_vocab).from_bytes(doc.to_bytes())
A:spacy.tests.serialize.test_serialize_doc.matcher->PhraseMatcher(nlp.vocab)
A:spacy.tests.serialize.test_serialize_doc.new_matcher->pickle.loads(data)
A:spacy.tests.serialize.test_serialize_doc.docs->English().pipe(['hello', 'world'])
A:spacy.tests.serialize.test_serialize_doc.piped_doc->next(docs)
A:spacy.tests.serialize.test_serialize_doc.bytes_data->English().to_bytes()
A:spacy.tests.serialize.test_serialize_doc.new_nlp->English()
A:spacy.tests.serialize.test_serialize_doc.doc_bytes->Doc(en_vocab, words=['hello', 'world', '!']).to_bytes()
A:spacy.tests.serialize.test_serialize_doc.doc2->Doc(en_vocab)
A:spacy.tests.serialize.test_serialize_doc.doc_b->Doc(en_vocab, words=['hello', 'world', '!']).to_bytes()
A:spacy.tests.serialize.test_serialize_doc.doc_d->Doc(en_vocab).from_disk(file_path)
A:spacy.tests.serialize.test_serialize_doc.file_path->str(file_path)
spacy.tests.serialize.test_serialize_doc.test_issue1727()
spacy.tests.serialize.test_serialize_doc.test_issue1799()
spacy.tests.serialize.test_serialize_doc.test_issue1834()
spacy.tests.serialize.test_serialize_doc.test_issue1883()
spacy.tests.serialize.test_serialize_doc.test_issue2564()
spacy.tests.serialize.test_serialize_doc.test_issue3248_2()
spacy.tests.serialize.test_serialize_doc.test_issue3289()
spacy.tests.serialize.test_serialize_doc.test_issue3468()
spacy.tests.serialize.test_serialize_doc.test_issue3959()
spacy.tests.serialize.test_serialize_doc.test_serialize_doc_exclude(en_vocab)
spacy.tests.serialize.test_serialize_doc.test_serialize_doc_roundtrip_bytes(en_vocab)
spacy.tests.serialize.test_serialize_doc.test_serialize_doc_roundtrip_disk(en_vocab)
spacy.tests.serialize.test_serialize_doc.test_serialize_doc_roundtrip_disk_str_path(en_vocab)
spacy.tests.serialize.test_serialize_doc.test_serialize_doc_span_groups(en_vocab)
spacy.tests.serialize.test_serialize_doc.test_serialize_empty_doc(en_vocab)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/serialize/test_serialize_extension_attrs.py----------------------------------------
A:spacy.tests.serialize.test_serialize_extension_attrs.doc->Doc(Vocab()).from_bytes(doc_b)
A:spacy.tests.serialize.test_serialize_extension_attrs.doc_b->doc_w_attrs.to_bytes()
spacy.tests.serialize.test_serialize_extension_attrs.doc_w_attrs(en_tokenizer)
spacy.tests.serialize.test_serialize_extension_attrs.test_serialize_ext_attrs_from_bytes(doc_w_attrs)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/serialize/test_serialize_pipeline.py----------------------------------------
A:spacy.tests.serialize.test_serialize_pipeline.parser->Parser(en_vocab, model)
A:spacy.tests.serialize.test_serialize_pipeline.tagger1->tagger1.from_bytes(tagger1_b).from_bytes(tagger1_b)
A:spacy.tests.serialize.test_serialize_pipeline.tagger2->Tagger(de_vocab, model).from_disk(file_path)
A:spacy.tests.serialize.test_serialize_pipeline.nlp->spacy.blank('en')
A:spacy.tests.serialize.test_serialize_pipeline.tagger->spacy.blank('en').add_pipe('tagger')
A:spacy.tests.serialize.test_serialize_pipeline.ruler->spacy.blank('en').add_pipe('entity_ruler', before='ner')
A:spacy.tests.serialize.test_serialize_pipeline.ruler_bytes->spacy.blank('en').add_pipe('entity_ruler', before='ner').to_bytes()
A:spacy.tests.serialize.test_serialize_pipeline.new_ruler->spacy.lang.en.English.from_config(config).get_pipe('entity_ruler')
A:spacy.tests.serialize.test_serialize_pipeline.bytes_old_style->srsly.msgpack_dumps(ruler.patterns)
A:spacy.tests.serialize.test_serialize_pipeline.nlp2->spacy.lang.en.English.from_config(config)
A:spacy.tests.serialize.test_serialize_pipeline.ner->spacy.blank('en').create_pipe('ner', config=config)
A:spacy.tests.serialize.test_serialize_pipeline.doc1->nlp1('What do you think about Apple ?')
A:spacy.tests.serialize.test_serialize_pipeline.output_dir->ensure_path(d)
A:spacy.tests.serialize.test_serialize_pipeline.doc2->nlp2('What do you think about Apple ?')
A:spacy.tests.serialize.test_serialize_pipeline.nlp1->English()
A:spacy.tests.serialize.test_serialize_pipeline.ner1->English().add_pipe('ner')
A:spacy.tests.serialize.test_serialize_pipeline.apple_ent->Span(doc1, 5, 6, label='MY_ORG')
A:spacy.tests.serialize.test_serialize_pipeline.ner2->pickle.load(file_)
A:spacy.tests.serialize.test_serialize_pipeline.vocab->Vocab(vectors_name='test_vocab_add_vector')
A:spacy.tests.serialize.test_serialize_pipeline.new_parser->get_new_parser().from_bytes(parser.to_bytes(exclude=['cfg']), exclude=['vocab'])
A:spacy.tests.serialize.test_serialize_pipeline.bytes_2->get_new_parser().from_bytes(parser.to_bytes(exclude=['cfg']), exclude=['vocab']).to_bytes(exclude=['vocab'])
A:spacy.tests.serialize.test_serialize_pipeline.bytes_3->Parser(en_vocab, model).to_bytes(exclude=['vocab'])
A:spacy.tests.serialize.test_serialize_pipeline.vocab1->Vocab()
A:spacy.tests.serialize.test_serialize_pipeline.parser1->Parser(vocab1, model)
A:spacy.tests.serialize.test_serialize_pipeline.vocab2->Vocab()
A:spacy.tests.serialize.test_serialize_pipeline.parser2->parser2.from_bytes(parser1.to_bytes(exclude=['vocab'])).from_bytes(parser1.to_bytes(exclude=['vocab']))
A:spacy.tests.serialize.test_serialize_pipeline.parser_d->parser_d.from_disk(file_path).from_disk(file_path)
A:spacy.tests.serialize.test_serialize_pipeline.parser_bytes->Parser(en_vocab, model).to_bytes(exclude=['model', 'vocab'])
A:spacy.tests.serialize.test_serialize_pipeline.parser_d_bytes->parser_d.from_disk(file_path).from_disk(file_path).to_bytes(exclude=['model', 'vocab'])
A:spacy.tests.serialize.test_serialize_pipeline.bytes_data->Parser(en_vocab, model).to_bytes(exclude=['vocab'])
A:spacy.tests.serialize.test_serialize_pipeline.tagger1_b->tagger1.from_bytes(tagger1_b).from_bytes(tagger1_b).to_bytes()
A:spacy.tests.serialize.test_serialize_pipeline.new_tagger1->Tagger(en_vocab, model).from_bytes(tagger1_b)
A:spacy.tests.serialize.test_serialize_pipeline.new_tagger1_b->Tagger(en_vocab, model).from_bytes(tagger1_b).to_bytes()
A:spacy.tests.serialize.test_serialize_pipeline.tagger1_d->Tagger(en_vocab, model).from_disk(file_path1)
A:spacy.tests.serialize.test_serialize_pipeline.tagger2_d->Tagger(en_vocab, model).from_disk(file_path2)
A:spacy.tests.serialize.test_serialize_pipeline.textcat->TextCategorizer(en_vocab, model, threshold=0.5)
A:spacy.tests.serialize.test_serialize_pipeline.sr->SentenceRecognizer(en_vocab, model)
A:spacy.tests.serialize.test_serialize_pipeline.sr_b->SentenceRecognizer(en_vocab, model).to_bytes()
A:spacy.tests.serialize.test_serialize_pipeline.sr_d->SentenceRecognizer(en_vocab, model).from_bytes(sr_b)
A:spacy.tests.serialize.test_serialize_pipeline.config->spacy.blank('en').config.copy()
A:spacy.tests.serialize.test_serialize_pipeline.nlp3->spacy.load(d)
A:spacy.tests.serialize.test_serialize_pipeline.nlp4->spacy.load(d, disable=['ner'])
A:spacy.tests.serialize.test_serialize_pipeline.nlp5->spacy.load(d, exclude=['tagger'])
A:spacy.tests.serialize.test_serialize_pipeline.pipe->CustomPipe(Vocab(), Linear())
A:spacy.tests.serialize.test_serialize_pipeline.pipe_bytes->CustomPipe(Vocab(), Linear()).to_bytes()
A:spacy.tests.serialize.test_serialize_pipeline.new_pipe->CustomPipe(Vocab(), Linear()).from_disk(d)
A:spacy.tests.serialize.test_serialize_pipeline.orig_strings_length->len(nlp.vocab.strings)
A:spacy.tests.serialize.test_serialize_pipeline.reloaded_nlp->load(d, exclude=['strings'])
spacy.tests.serialize.test_serialize_pipeline.blank_parser(en_vocab)
spacy.tests.serialize.test_serialize_pipeline.parser(en_vocab)
spacy.tests.serialize.test_serialize_pipeline.taggers(en_vocab)
spacy.tests.serialize.test_serialize_pipeline.test_issue3456()
spacy.tests.serialize.test_serialize_pipeline.test_issue4042()
spacy.tests.serialize.test_serialize_pipeline.test_issue4042_bug2()
spacy.tests.serialize.test_serialize_pipeline.test_issue4725_1()
spacy.tests.serialize.test_serialize_pipeline.test_issue_3526_1(en_vocab)
spacy.tests.serialize.test_serialize_pipeline.test_issue_3526_2(en_vocab)
spacy.tests.serialize.test_serialize_pipeline.test_issue_3526_3(en_vocab)
spacy.tests.serialize.test_serialize_pipeline.test_issue_3526_4(en_vocab)
spacy.tests.serialize.test_serialize_pipeline.test_load_without_strings()
spacy.tests.serialize.test_serialize_pipeline.test_serialize_custom_trainable_pipe()
spacy.tests.serialize.test_serialize_pipeline.test_serialize_parser_roundtrip_bytes(en_vocab,Parser)
spacy.tests.serialize.test_serialize_pipeline.test_serialize_parser_roundtrip_disk(en_vocab,Parser)
spacy.tests.serialize.test_serialize_pipeline.test_serialize_parser_strings(Parser)
spacy.tests.serialize.test_serialize_pipeline.test_serialize_pipe_exclude(en_vocab,Parser)
spacy.tests.serialize.test_serialize_pipeline.test_serialize_pipeline_disable_enable()
spacy.tests.serialize.test_serialize_pipeline.test_serialize_sentencerecognizer(en_vocab)
spacy.tests.serialize.test_serialize_pipeline.test_serialize_tagger_roundtrip_bytes(en_vocab,taggers)
spacy.tests.serialize.test_serialize_pipeline.test_serialize_tagger_roundtrip_disk(en_vocab,taggers)
spacy.tests.serialize.test_serialize_pipeline.test_serialize_tagger_strings(en_vocab,de_vocab,taggers)
spacy.tests.serialize.test_serialize_pipeline.test_serialize_textcat_empty(en_vocab)
spacy.tests.serialize.test_serialize_pipeline.test_to_from_bytes(parser,blank_parser)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/serialize/test_serialize_tokenizer.py----------------------------------------
A:spacy.tests.serialize.test_serialize_tokenizer.doc->Doc(en_vocab, words=words, tags=tags, pos=pos, ents=ents)
A:spacy.tests.serialize.test_serialize_tokenizer.ent_array->Doc(en_vocab, words=words, tags=tags, pos=pos, ents=ents).to_array(header)
A:spacy.tests.serialize.test_serialize_tokenizer.doc_bytes->Doc(en_vocab, words=words, tags=tags, pos=pos, ents=ents).to_bytes()
A:spacy.tests.serialize.test_serialize_tokenizer.doc2->new_tokenizer(text)
A:spacy.tests.serialize.test_serialize_tokenizer.prefix_re->compile_prefix_regex(nlp.Defaults.prefixes)
A:spacy.tests.serialize.test_serialize_tokenizer.suffix_re->compile_suffix_regex(nlp.Defaults.suffixes)
A:spacy.tests.serialize.test_serialize_tokenizer.infix_re->compile_infix_regex(nlp.Defaults.infixes)
A:spacy.tests.serialize.test_serialize_tokenizer.new_tokenizer->load_tokenizer(tokenizer.to_bytes())
A:spacy.tests.serialize.test_serialize_tokenizer.nlp_1->English()
A:spacy.tests.serialize.test_serialize_tokenizer.doc_1a->nlp_1(test_string)
A:spacy.tests.serialize.test_serialize_tokenizer.doc_1b->nlp_1(test_string)
A:spacy.tests.serialize.test_serialize_tokenizer.nlp_2->load_model(model_dir)
A:spacy.tests.serialize.test_serialize_tokenizer.doc_2->nlp_2(test_string)
A:spacy.tests.serialize.test_serialize_tokenizer.tokenizer->Tokenizer(en_vocab, rules={'ABC.': [{'ORTH': 'ABC'}, {'ORTH': '.'}]})
A:spacy.tests.serialize.test_serialize_tokenizer.tokenizer_bytes->Tokenizer(en_vocab, rules={'ABC.': [{'ORTH': 'ABC'}, {'ORTH': '.'}]}).to_bytes()
A:spacy.tests.serialize.test_serialize_tokenizer.tokenizer_reloaded->Tokenizer(en_vocab).from_bytes(tokenizer_bytes)
A:spacy.tests.serialize.test_serialize_tokenizer.doc1->tokenizer(text)
A:spacy.tests.serialize.test_serialize_tokenizer.tokenizer_d->en_tokenizer.from_disk(file_path)
spacy.tests.serialize.test_serialize_tokenizer.load_tokenizer(b)
spacy.tests.serialize.test_serialize_tokenizer.test_issue2833(en_vocab)
spacy.tests.serialize.test_serialize_tokenizer.test_issue3012(en_vocab)
spacy.tests.serialize.test_serialize_tokenizer.test_issue4190()
spacy.tests.serialize.test_serialize_tokenizer.test_serialize_custom_tokenizer(en_vocab,en_tokenizer)
spacy.tests.serialize.test_serialize_tokenizer.test_serialize_tokenizer_roundtrip_bytes(en_tokenizer,text)
spacy.tests.serialize.test_serialize_tokenizer.test_serialize_tokenizer_roundtrip_disk(en_tokenizer)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/serialize/test_resource_warning.py----------------------------------------
A:spacy.tests.serialize.test_resource_warning.data->zeros((3, 1), dtype='f')
A:spacy.tests.serialize.test_resource_warning.self.model->SerializableDummy()
A:spacy.tests.serialize.test_resource_warning.nlp->Language()
A:spacy.tests.serialize.test_resource_warning.tagger->Language().add_pipe('tagger')
A:spacy.tests.serialize.test_resource_warning.kb->InMemoryLookupKB(nlp.vocab, entity_vector_length=1)
A:spacy.tests.serialize.test_resource_warning.entity_linker->Language().add_pipe('entity_linker')
A:spacy.tests.serialize.test_resource_warning.warnings_list->write_obj_and_catch_warnings(scenario[0])
A:spacy.tests.serialize.test_resource_warning.writer->Writer(path)
A:spacy.tests.serialize.test_resource_warning.kb_loaded->InMemoryLookupKB(nlp.vocab, entity_vector_length=1)
A:spacy.tests.serialize.test_resource_warning.scenarios->zip(*objects_to_test)
spacy.tests.serialize.test_resource_warning.TestToDiskResourceWarningUnittest(TestCase)
spacy.tests.serialize.test_resource_warning.TestToDiskResourceWarningUnittest.test_resource_warning(self)
spacy.tests.serialize.test_resource_warning.custom_pipe()
spacy.tests.serialize.test_resource_warning.entity_linker()
spacy.tests.serialize.test_resource_warning.nlp()
spacy.tests.serialize.test_resource_warning.tagger()
spacy.tests.serialize.test_resource_warning.test_save_and_load_knowledge_base()
spacy.tests.serialize.test_resource_warning.test_to_disk_resource_warning(obj)
spacy.tests.serialize.test_resource_warning.test_writer_with_path_py35()
spacy.tests.serialize.test_resource_warning.vectors()
spacy.tests.serialize.test_resource_warning.write_obj_and_catch_warnings(obj)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/serialize/test_serialize_span_groups.py----------------------------------------
A:spacy.tests.serialize.test_serialize_span_groups.doc->en_tokenizer('0 1 2 3 4 5 6')
A:spacy.tests.serialize.test_serialize_span_groups.doc.spans['test']->SpanGroup(doc, name='test', spans=[doc[0:1]])
A:spacy.tests.serialize.test_serialize_span_groups.doc.spans['test2']->SpanGroup(doc, name='test', spans=[doc[1:2]])
A:spacy.tests.serialize.test_serialize_span_groups.groups['key1']->SpanGroup(doc, name='key1', spans=[doc[0:1], doc[1:2]])
A:spacy.tests.serialize.test_serialize_span_groups.groups['key2']->SpanGroup(doc, name='too', spans=[doc[3:4], doc[4:5]])
A:spacy.tests.serialize.test_serialize_span_groups.groups['key3']->SpanGroup(doc, name='too', spans=[doc[1:2], doc[0:1]])
A:spacy.tests.serialize.test_serialize_span_groups.groups['key4']->SpanGroup(doc, name='key4', spans=[doc[0:1]])
A:spacy.tests.serialize.test_serialize_span_groups.groups['key5']->SpanGroup(doc, name='key4', spans=[doc[0:1]])
A:spacy.tests.serialize.test_serialize_span_groups.sg6->SpanGroup(doc, name='key6', spans=[doc[0:1]])
A:spacy.tests.serialize.test_serialize_span_groups.sg8->SpanGroup(doc, name='also', spans=[doc[1:2]])
A:spacy.tests.serialize.test_serialize_span_groups.regroups->SpanGroups(doc).from_bytes(groups.to_bytes())
A:spacy.tests.serialize.test_serialize_span_groups.span_groups->SpanGroups(doc)
A:spacy.tests.serialize.test_serialize_span_groups.sg1->SpanGroup(doc, spans=spans)
A:spacy.tests.serialize.test_serialize_span_groups.reloaded_span_groups->SpanGroups(doc).from_bytes(span_groups.to_bytes())
spacy.tests.serialize.test_serialize_span_groups.test_deserialize_span_groups_compat(en_tokenizer,spans_bytes,doc_text,expected_spangroups,expected_warning)
spacy.tests.serialize.test_serialize_span_groups.test_issue10685(en_tokenizer)
spacy.tests.serialize.test_serialize_span_groups.test_span_groups_serialization(en_tokenizer)
spacy.tests.serialize.test_serialize_span_groups.test_span_groups_serialization_mismatches(en_tokenizer)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/serialize/test_serialize_config.py----------------------------------------
A:spacy.tests.serialize.test_serialize_config.tok2vec->build_Tok2Vec_model(MultiHashEmbed(width=321, attrs=['LOWER', 'SHAPE'], rows=[5432, 5432], include_static_vectors=False), MaxoutWindowEncoder(width=321, window_size=3, maxout_pieces=4, depth=2))
A:spacy.tests.serialize.test_serialize_config.parser->spacy.lang.en.English.from_config(load_config_from_str(hyphen_config_str)).add_pipe('parser', config=model_config)
A:spacy.tests.serialize.test_serialize_config.source_nlp->spacy.lang.en.English.from_config(source_cfg)
A:spacy.tests.serialize.test_serialize_config.nlp->spacy.lang.en.English.from_config(load_config_from_str(hyphen_config_str))
A:spacy.tests.serialize.test_serialize_config.config->Config().from_str(parser_config_string)
A:spacy.tests.serialize.test_serialize_config.pretrain_config->load_config(DEFAULT_CONFIG_PRETRAIN_PATH)
A:spacy.tests.serialize.test_serialize_config.filled->spacy.util.registry.fill(config, schema=ConfigSchema, validate=False)
A:spacy.tests.serialize.test_serialize_config.config['nlp']['pipeline']->list(config['components'].keys())
A:spacy.tests.serialize.test_serialize_config.nlp_config->Config().from_str(nlp_config_string)
A:spacy.tests.serialize.test_serialize_config.nlp2->spacy.lang.en.English.from_config(interpolated)
A:spacy.tests.serialize.test_serialize_config.parser_cfg->dict()
A:spacy.tests.serialize.test_serialize_config.model_config->Config().from_str(parser_config_string)
A:spacy.tests.serialize.test_serialize_config.new_nlp->spacy.load(d)
A:spacy.tests.serialize.test_serialize_config.nlp_bytes->spacy.lang.en.English.from_config(load_config_from_str(hyphen_config_str)).to_bytes()
A:spacy.tests.serialize.test_serialize_config.base_config->Config().from_str(nlp_config_string)
A:spacy.tests.serialize.test_serialize_config.base_nlp->load_model_from_config(base_config, auto_fill=True)
A:spacy.tests.serialize.test_serialize_config.interpolated->Config().from_str(parser_config_string).interpolate()
A:spacy.tests.serialize.test_serialize_config.interpolated2->spacy.lang.en.English.from_config(load_config_from_str(hyphen_config_str)).config.interpolate()
A:spacy.tests.serialize.test_serialize_config.new_config->Config().from_str(filled.to_str())
spacy.tests.serialize.test_serialize_config.my_parser()
spacy.tests.serialize.test_serialize_config.test_config_auto_fill_extra_fields()
spacy.tests.serialize.test_serialize_config.test_config_interpolation()
spacy.tests.serialize.test_serialize_config.test_config_nlp_roundtrip()
spacy.tests.serialize.test_serialize_config.test_config_nlp_roundtrip_bytes_disk()
spacy.tests.serialize.test_serialize_config.test_config_only_resolve_relevant_blocks()
spacy.tests.serialize.test_serialize_config.test_config_optional_sections()
spacy.tests.serialize.test_serialize_config.test_config_overrides()
spacy.tests.serialize.test_serialize_config.test_config_validate_literal(parser_config_string)
spacy.tests.serialize.test_serialize_config.test_create_nlp_from_config()
spacy.tests.serialize.test_serialize_config.test_create_nlp_from_config_multiple_instances()
spacy.tests.serialize.test_serialize_config.test_create_nlp_from_pretraining_config()
spacy.tests.serialize.test_serialize_config.test_hyphen_in_config()
spacy.tests.serialize.test_serialize_config.test_issue8190()
spacy.tests.serialize.test_serialize_config.test_serialize_config_language_specific()
spacy.tests.serialize.test_serialize_config.test_serialize_config_missing_pipes()
spacy.tests.serialize.test_serialize_config.test_serialize_custom_nlp()
spacy.tests.serialize.test_serialize_config.test_serialize_nlp()
spacy.tests.serialize.test_serialize_config.test_serialize_parser(parser_config_string)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/serialize/test_serialize_language.py----------------------------------------
A:spacy.tests.serialize.test_serialize_language.nlp->Language(meta=meta_data)
A:spacy.tests.serialize.test_serialize_language.b->Language(meta=meta_data).to_bytes()
A:spacy.tests.serialize.test_serialize_language.language->Language(meta=meta_data)
A:spacy.tests.serialize.test_serialize_language.new_language->Language().from_disk(d)
A:spacy.tests.serialize.test_serialize_language.prefix_re->re.compile('1/|2/|:[0-9][0-9][A-K]:|:[0-9][0-9]:')
A:spacy.tests.serialize.test_serialize_language.suffix_re->re.compile('')
A:spacy.tests.serialize.test_serialize_language.infix_re->re.compile('[~]')
A:spacy.tests.serialize.test_serialize_language.nlp.tokenizer->custom_tokenizer(nlp)
A:spacy.tests.serialize.test_serialize_language.new_nlp->Language().from_bytes(nlp.to_bytes(exclude=['meta']))
spacy.tests.serialize.test_serialize_language.meta_data()
spacy.tests.serialize.test_serialize_language.test_issue2482()
spacy.tests.serialize.test_serialize_language.test_issue6950()
spacy.tests.serialize.test_serialize_language.test_serialize_language_exclude(meta_data)
spacy.tests.serialize.test_serialize_language.test_serialize_language_meta_disk(meta_data)
spacy.tests.serialize.test_serialize_language.test_serialize_with_custom_tokenizer()


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/serialize/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/serialize/test_serialize_vocab_strings.py----------------------------------------
A:spacy.tests.serialize.test_serialize_vocab_strings.doc->Doc(vocab).from_bytes(doc_bytes)
A:spacy.tests.serialize.test_serialize_vocab_strings.doc2->Doc(doc.vocab)
A:spacy.tests.serialize.test_serialize_vocab_strings.nlp1->English()
A:spacy.tests.serialize.test_serialize_vocab_strings.vocab_dir->ensure_path(d / 'vocab')
A:spacy.tests.serialize.test_serialize_vocab_strings.vocab2->vocab2.from_disk(file_path).from_disk(file_path)
A:spacy.tests.serialize.test_serialize_vocab_strings.nlp2->spacy.blank('en', vocab=vocab2)
A:spacy.tests.serialize.test_serialize_vocab_strings.nlp_dir->ensure_path(d / 'nlp')
A:spacy.tests.serialize.test_serialize_vocab_strings.nlp3->load_model(nlp_dir)
A:spacy.tests.serialize.test_serialize_vocab_strings.nlp->English()
A:spacy.tests.serialize.test_serialize_vocab_strings.vocab_bytes->en_vocab.to_bytes(exclude=['lookups'])
A:spacy.tests.serialize.test_serialize_vocab_strings.doc_bytes->Doc(vocab).from_bytes(doc_bytes).to_bytes()
A:spacy.tests.serialize.test_serialize_vocab_strings.vocab->Vocab(strings=strings)
A:spacy.tests.serialize.test_serialize_vocab_strings.text_hash->en_vocab.strings.add(text)
A:spacy.tests.serialize.test_serialize_vocab_strings.new_vocab->Vocab().from_bytes(vocab_bytes)
A:spacy.tests.serialize.test_serialize_vocab_strings.vocab1->Vocab(strings=strings)
A:spacy.tests.serialize.test_serialize_vocab_strings.vocab1_b->Vocab(strings=strings).to_bytes()
A:spacy.tests.serialize.test_serialize_vocab_strings.vocab2_b->vocab2.from_disk(file_path).from_disk(file_path).to_bytes()
A:spacy.tests.serialize.test_serialize_vocab_strings.new_vocab1->Vocab().from_bytes(vocab1_b)
A:spacy.tests.serialize.test_serialize_vocab_strings.vocab1_d->Vocab().from_disk(file_path1)
A:spacy.tests.serialize.test_serialize_vocab_strings.vocab2_d->Vocab().from_disk(file_path2)
A:spacy.tests.serialize.test_serialize_vocab_strings.sstore1->StringStore(strings=strings1)
A:spacy.tests.serialize.test_serialize_vocab_strings.sstore2->StringStore(strings=strings2)
A:spacy.tests.serialize.test_serialize_vocab_strings.sstore1_b->StringStore(strings=strings1).to_bytes()
A:spacy.tests.serialize.test_serialize_vocab_strings.sstore2_b->StringStore(strings=strings2).to_bytes()
A:spacy.tests.serialize.test_serialize_vocab_strings.new_sstore1->StringStore().from_bytes(sstore1_b)
A:spacy.tests.serialize.test_serialize_vocab_strings.sstore1_d->StringStore().from_disk(file_path1)
A:spacy.tests.serialize.test_serialize_vocab_strings.sstore2_d->StringStore().from_disk(file_path2)
A:spacy.tests.serialize.test_serialize_vocab_strings.ops->get_current_ops()
A:spacy.tests.serialize.test_serialize_vocab_strings.vectors->Vectors(data=ops.xp.zeros((10, 10)), mode='floret', hash_count=1)
A:spacy.tests.serialize.test_serialize_vocab_strings.vocab_pickled->pickle.dumps(vocab)
A:spacy.tests.serialize.test_serialize_vocab_strings.vocab_unpickled->pickle.loads(vocab_pickled)
spacy.tests.serialize.test_serialize_vocab_strings.test_deserialize_vocab_seen_entries(strings,lex_attr)
spacy.tests.serialize.test_serialize_vocab_strings.test_issue4054(en_vocab)
spacy.tests.serialize.test_serialize_vocab_strings.test_issue4133(en_vocab)
spacy.tests.serialize.test_serialize_vocab_strings.test_issue599(en_vocab)
spacy.tests.serialize.test_serialize_vocab_strings.test_pickle_vocab(strings,lex_attr)
spacy.tests.serialize.test_serialize_vocab_strings.test_serialize_stringstore_roundtrip_bytes(strings1,strings2)
spacy.tests.serialize.test_serialize_vocab_strings.test_serialize_stringstore_roundtrip_disk(strings1,strings2)
spacy.tests.serialize.test_serialize_vocab_strings.test_serialize_vocab(en_vocab,text)
spacy.tests.serialize.test_serialize_vocab_strings.test_serialize_vocab_lex_attrs_bytes(strings,lex_attr)
spacy.tests.serialize.test_serialize_vocab_strings.test_serialize_vocab_lex_attrs_disk(strings,lex_attr)
spacy.tests.serialize.test_serialize_vocab_strings.test_serialize_vocab_roundtrip_bytes(strings1,strings2)
spacy.tests.serialize.test_serialize_vocab_strings.test_serialize_vocab_roundtrip_disk(strings1,strings2)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/doc/test_creation.py----------------------------------------
A:spacy.tests.doc.test_creation.doc->Doc(vocab, words=words, spaces=spaces)
A:spacy.tests.doc.test_creation.(words, spaces)->spacy.util.get_words_and_spaces(words + ['away'], text)
A:spacy.tests.doc.test_creation.words->'I like ginger'.split()
A:spacy.tests.doc.test_creation.heads->list(range(len(words)))
A:spacy.tests.doc.test_creation.pos->'QQ ZZ XX'.split()
spacy.tests.doc.test_creation.test_create_from_words_and_text(vocab)
spacy.tests.doc.test_creation.test_create_invalid_pos(vocab)
spacy.tests.doc.test_creation.test_create_with_heads_and_no_deps(vocab)
spacy.tests.doc.test_creation.test_empty_doc(vocab)
spacy.tests.doc.test_creation.test_single_word(vocab)
spacy.tests.doc.test_creation.vocab()


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/doc/test_token_api.py----------------------------------------
A:spacy.tests.doc.test_token_api.doc->Doc(en_vocab, words=words, heads=heads, deps=deps)
A:spacy.tests.doc.test_token_api.tokens->en_tokenizer(text)
A:spacy.tests.doc.test_token_api.vocab->Vocab()
A:spacy.tests.doc.test_token_api.doc2->Doc(en_vocab, words=words, heads=heads, deps=['dep'] * len(heads))
A:spacy.tests.doc.test_token_api.example->spacy.training.Example.from_dict(doc, {'heads': heads, 'deps': deps})
A:spacy.tests.doc.test_token_api.(aligned_heads, aligned_deps)->spacy.training.Example.from_dict(doc, {'heads': heads, 'deps': deps}).get_aligned_parse(projectivize=True)
spacy.tests.doc.test_token_api.doc(en_vocab)
spacy.tests.doc.test_token_api.test_doc_token_api_ancestors(en_vocab)
spacy.tests.doc.test_token_api.test_doc_token_api_flags(en_tokenizer)
spacy.tests.doc.test_token_api.test_doc_token_api_head_setter(en_vocab)
spacy.tests.doc.test_token_api.test_doc_token_api_is_properties(en_vocab)
spacy.tests.doc.test_token_api.test_doc_token_api_prob_inherited_from_vocab(en_tokenizer,text)
spacy.tests.doc.test_token_api.test_doc_token_api_str_builtin(en_tokenizer,text)
spacy.tests.doc.test_token_api.test_doc_token_api_strings(en_vocab)
spacy.tests.doc.test_token_api.test_doc_token_api_vectors()
spacy.tests.doc.test_token_api.test_is_sent_end(en_tokenizer)
spacy.tests.doc.test_token_api.test_is_sent_start(en_tokenizer)
spacy.tests.doc.test_token_api.test_missing_head_dep(en_vocab)
spacy.tests.doc.test_token_api.test_set_invalid_pos()
spacy.tests.doc.test_token_api.test_set_pos()
spacy.tests.doc.test_token_api.test_token0_has_sent_start_true()
spacy.tests.doc.test_token_api.test_token_api_conjuncts_chain(en_vocab)
spacy.tests.doc.test_token_api.test_token_api_conjuncts_simple(en_vocab)
spacy.tests.doc.test_token_api.test_token_api_non_conjuncts(en_vocab)
spacy.tests.doc.test_token_api.test_tokenlast_has_sent_end_true()
spacy.tests.doc.test_token_api.test_tokens_sent(doc)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/doc/test_span.py----------------------------------------
A:spacy.tests.doc.test_span.tokens->en_tokenizer(text)
A:spacy.tests.doc.test_span.doc->en_tokenizer('Check span.sent raises error if doc is not sentencized.')
A:spacy.tests.doc.test_span.sents->list(doc.sents)
A:spacy.tests.doc.test_span.sent0->sents[0].as_doc()
A:spacy.tests.doc.test_span.sent1->sents[1].as_doc()
A:spacy.tests.doc.test_span.nlp->English()
A:spacy.tests.doc.test_span.text->nlp('Talk about being boring!')
A:spacy.tests.doc.test_span.text_var->nlp('Talk of being boring!')
A:spacy.tests.doc.test_span.y->nlp('Let')
A:spacy.tests.doc.test_span.span->Span(doc, 0, 1)
A:spacy.tests.doc.test_span.lca->doc[1:4].get_lca_matrix()
A:spacy.tests.doc.test_span.span2->en_tokenizer('Check span.sent raises error if doc is not sentencized.').char_span(span1.start_char + 1, span1.end_char, label='GPE', alignment_mode='unk')
A:spacy.tests.doc.test_span.arr->Span(doc, 0, 1).to_array([ORTH, LENGTH])
A:spacy.tests.doc.test_span.span_doc->doc[0:5].as_doc()
A:spacy.tests.doc.test_span.span_doc_with->Span(doc, 0, 1).as_doc(copy_user_data=True)
A:spacy.tests.doc.test_span.span_doc_without->Span(doc, 0, 1).as_doc()
A:spacy.tests.doc.test_span.sentences->list(doc.sents)
A:spacy.tests.doc.test_span.filtered->filter_spans(spans)
A:spacy.tests.doc.test_span.ops->get_current_ops()
A:spacy.tests.doc.test_span.doc_copy->en_tokenizer('Check span.sent raises error if doc is not sentencized.').copy()
spacy.tests.doc.test_span.doc(en_tokenizer)
spacy.tests.doc.test_span.doc_not_parsed(en_tokenizer)
spacy.tests.doc.test_span.test_char_span(doc,i_sent,i,j,text)
spacy.tests.doc.test_span.test_filter_spans(doc)
spacy.tests.doc.test_span.test_issue1537()
spacy.tests.doc.test_span.test_issue1612(en_tokenizer)
spacy.tests.doc.test_span.test_issue3199()
spacy.tests.doc.test_span.test_issue5152()
spacy.tests.doc.test_span.test_issue6755(en_tokenizer)
spacy.tests.doc.test_span.test_issue6815_1(sentence,start_idx,end_idx,label)
spacy.tests.doc.test_span.test_issue6815_2(sentence,start_idx,end_idx,kb_id)
spacy.tests.doc.test_span.test_issue6815_3(sentence,start_idx,end_idx,vector)
spacy.tests.doc.test_span.test_sent(en_tokenizer)
spacy.tests.doc.test_span.test_span_as_doc(doc)
spacy.tests.doc.test_span.test_span_as_doc_user_data(doc)
spacy.tests.doc.test_span.test_span_attrs_writable(doc)
spacy.tests.doc.test_span.test_span_boundaries(doc)
spacy.tests.doc.test_span.test_span_comparison(doc)
spacy.tests.doc.test_span.test_span_ents_property(doc)
spacy.tests.doc.test_span.test_span_eq_hash(doc,doc_not_parsed)
spacy.tests.doc.test_span.test_span_group_copy(doc)
spacy.tests.doc.test_span.test_span_lemma(doc)
spacy.tests.doc.test_span.test_span_sents(doc,start,end,expected_sentences,expected_sentences_with_hook)
spacy.tests.doc.test_span.test_span_sents_not_parsed(doc_not_parsed)
spacy.tests.doc.test_span.test_span_similarity_match()
spacy.tests.doc.test_span.test_span_string_label_id(doc)
spacy.tests.doc.test_span.test_span_string_label_kb_id(doc)
spacy.tests.doc.test_span.test_span_to_array(doc)
spacy.tests.doc.test_span.test_span_with_vectors(doc)
spacy.tests.doc.test_span.test_spans_are_hashable(en_tokenizer)
spacy.tests.doc.test_span.test_spans_by_character(doc)
spacy.tests.doc.test_span.test_spans_default_sentiment(en_tokenizer)
spacy.tests.doc.test_span.test_spans_lca_matrix(en_tokenizer)
spacy.tests.doc.test_span.test_spans_override_sentiment(en_tokenizer)
spacy.tests.doc.test_span.test_spans_root(doc)
spacy.tests.doc.test_span.test_spans_root2(en_tokenizer)
spacy.tests.doc.test_span.test_spans_sent_spans(doc)
spacy.tests.doc.test_span.test_spans_span_sent(doc,doc_not_parsed)
spacy.tests.doc.test_span.test_spans_span_sent_user_hooks(doc,start,end,expected_sentence)
spacy.tests.doc.test_span.test_spans_string_fn(doc)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/doc/test_pickle_doc.py----------------------------------------
A:spacy.tests.doc.test_pickle_doc.nlp->Language()
A:spacy.tests.doc.test_pickle_doc.doc->nlp('Hello')
A:spacy.tests.doc.test_pickle_doc.data->spacy.compat.pickle.dumps(doc, 1)
A:spacy.tests.doc.test_pickle_doc.doc2->spacy.compat.pickle.loads(b)
A:spacy.tests.doc.test_pickle_doc.one_pickled->spacy.compat.pickle.dumps(nlp('0'), -1)
A:spacy.tests.doc.test_pickle_doc.docs->list(nlp.pipe((str(i) for i in range(100))))
A:spacy.tests.doc.test_pickle_doc.many_pickled->spacy.compat.pickle.dumps(docs, -1)
A:spacy.tests.doc.test_pickle_doc.many_unpickled->spacy.compat.pickle.loads(many_pickled)
A:spacy.tests.doc.test_pickle_doc.b->spacy.compat.pickle.dumps(doc)
spacy.tests.doc.test_pickle_doc.test_hooks_unpickle()
spacy.tests.doc.test_pickle_doc.test_list_of_docs_pickles_efficiently()
spacy.tests.doc.test_pickle_doc.test_pickle_single_doc()
spacy.tests.doc.test_pickle_doc.test_user_data_from_disk()
spacy.tests.doc.test_pickle_doc.test_user_data_unpickles()


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/doc/test_retokenize_merge.py----------------------------------------
A:spacy.tests.doc.test_retokenize_merge.doc->Doc(tokens.vocab, words=[t.text for t in tokens], sent_starts=sent_starts)
A:spacy.tests.doc.test_retokenize_merge.new_doc->Doc(doc.vocab, words=['beach boys'])
A:spacy.tests.doc.test_retokenize_merge.tokens->en_tokenizer(text)
A:spacy.tests.doc.test_retokenize_merge.ent_type->max((w.ent_type_ for w in ent))
A:spacy.tests.doc.test_retokenize_merge.(sent1, sent2)->list(doc.sents)
A:spacy.tests.doc.test_retokenize_merge.init_len->len(list(sent1.root.subtree))
A:spacy.tests.doc.test_retokenize_merge.init_len2->len(sent2)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_lex_attrs(en_tokenizer)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_merge(en_tokenizer)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_merge_children(en_tokenizer)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_merge_extension_attrs(en_vocab)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_merge_extension_attrs_invalid(en_vocab,underscore_attrs)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_merge_hang(en_tokenizer)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_merge_without_parse_keeps_sents(en_tokenizer)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_retokenizer(en_tokenizer)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_retokenizer_attrs(en_tokenizer)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_span_np_merges(en_tokenizer)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_spans_entity_merge(en_tokenizer)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_spans_entity_merge_iob(en_vocab)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_spans_merge_heads(en_vocab)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_spans_merge_non_disjoint(en_tokenizer)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_spans_merge_tokens(en_tokenizer)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_spans_merge_tokens_default_attrs(en_vocab)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_spans_sentence_update_after_merge(en_tokenizer)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_spans_subtree_size_check(en_tokenizer)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenizer_merge_lex_attrs(en_vocab)
spacy.tests.doc.test_retokenize_merge.test_retokenize_disallow_zero_length(en_vocab)
spacy.tests.doc.test_retokenize_merge.test_retokenize_skip_duplicates(en_vocab)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/doc/test_json_doc_conversion.py----------------------------------------
A:spacy.tests.doc.test_json_doc_conversion.json_doc->spacy.blank('en')('This is just brilliant.').to_json()
A:spacy.tests.doc.test_json_doc_conversion.new_doc->Doc(doc.vocab).from_json(json_doc, validate=True)
A:spacy.tests.doc.test_json_doc_conversion.doc->spacy.blank('en')('This is just brilliant.')
A:spacy.tests.doc.test_json_doc_conversion.doc_json->spacy.blank('en')('This is just brilliant.').to_json(underscore=['text_length'])
spacy.tests.doc.test_json_doc_conversion.doc(en_vocab)
spacy.tests.doc.test_json_doc_conversion.doc_json()
spacy.tests.doc.test_json_doc_conversion.doc_without_deps(en_vocab)
spacy.tests.doc.test_json_doc_conversion.test_doc_to_json(doc)
spacy.tests.doc.test_json_doc_conversion.test_doc_to_json_span(doc)
spacy.tests.doc.test_json_doc_conversion.test_doc_to_json_underscore(doc)
spacy.tests.doc.test_json_doc_conversion.test_doc_to_json_underscore_error_attr(doc)
spacy.tests.doc.test_json_doc_conversion.test_doc_to_json_underscore_error_serialize(doc)
spacy.tests.doc.test_json_doc_conversion.test_doc_to_json_with_custom_user_data(doc)
spacy.tests.doc.test_json_doc_conversion.test_doc_to_json_with_token_attributes_missing(doc)
spacy.tests.doc.test_json_doc_conversion.test_doc_to_json_with_token_span_attributes(doc)
spacy.tests.doc.test_json_doc_conversion.test_doc_to_json_with_token_span_same_identifier(doc)
spacy.tests.doc.test_json_doc_conversion.test_json_to_doc(doc)
spacy.tests.doc.test_json_doc_conversion.test_json_to_doc_attribute_consistency(doc)
spacy.tests.doc.test_json_doc_conversion.test_json_to_doc_cats(doc)
spacy.tests.doc.test_json_doc_conversion.test_json_to_doc_compat(doc,doc_json)
spacy.tests.doc.test_json_doc_conversion.test_json_to_doc_sents(doc,doc_without_deps)
spacy.tests.doc.test_json_doc_conversion.test_json_to_doc_spaces()
spacy.tests.doc.test_json_doc_conversion.test_json_to_doc_spans(doc)
spacy.tests.doc.test_json_doc_conversion.test_json_to_doc_underscore(doc)
spacy.tests.doc.test_json_doc_conversion.test_json_to_doc_validation_error(doc)
spacy.tests.doc.test_json_doc_conversion.test_json_to_doc_with_token_span_attributes(doc)
spacy.tests.doc.test_json_doc_conversion.test_to_json_underscore_doc_getters(doc)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/doc/test_add_entities.py----------------------------------------
A:spacy.tests.doc.test_add_entities.doc->Doc(en_vocab, words=text)
A:spacy.tests.doc.test_add_entities.ner->EntityRecognizer(en_vocab, model)
A:spacy.tests.doc.test_add_entities.doc.ents->list(doc.ents)
A:spacy.tests.doc.test_add_entities.entity->Span(doc, 0, 4, label=391)
A:spacy.tests.doc.test_add_entities.new_entity->Span(doc, 0, 1, label=392)
spacy.tests.doc.test_add_entities._ner_example(ner)
spacy.tests.doc.test_add_entities.test_add_overlapping_entities(en_vocab)
spacy.tests.doc.test_add_entities.test_doc_add_entities_set_ents_iob(en_vocab)
spacy.tests.doc.test_add_entities.test_ents_reset(en_vocab)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/doc/test_span_group.py----------------------------------------
A:spacy.tests.doc.test_span_group.doc->en_tokenizer('0 1 2 3 4 5 6')
A:spacy.tests.doc.test_span_group.matcher->Matcher(en_tokenizer.vocab, validate=True)
A:spacy.tests.doc.test_span_group.matches->matcher(doc)
A:spacy.tests.doc.test_span_group.doc.spans['SPANS']->SpanGroup(doc, name='SPANS', attrs={'key': 'value'}, spans=spans)
A:spacy.tests.doc.test_span_group.clone->span_group.copy()
A:spacy.tests.doc.test_span_group.span->Span(other_doc, 0, 2)
A:spacy.tests.doc.test_span_group.span_group_2->SpanGroup(doc, name='MORE_SPANS', attrs={'key': 'new_value', 'new_key': 'new_value'}, spans=spans)
A:spacy.tests.doc.test_span_group.span_group_3->doc.spans['SPANS'].copy()._concat(span_group_2, inplace=True)
A:spacy.tests.doc.test_span_group.length->len(span_group)
A:spacy.tests.doc.test_span_group.span_group_3_expected->doc.spans['SPANS'].copy()._concat(span_group_2)
A:spacy.tests.doc.test_span_group.span_group_1->en_tokenizer('0 1 2 3 4 5 6').spans['SPANS'].copy()
A:spacy.tests.doc.test_span_group.span_group_1_expected->en_tokenizer('0 1 2 3 4 5 6').spans['SPANS'].copy()._concat(span_group_2)
spacy.tests.doc.test_span_group.doc(en_tokenizer)
spacy.tests.doc.test_span_group.other_doc(en_tokenizer)
spacy.tests.doc.test_span_group.span_group(en_tokenizer)
spacy.tests.doc.test_span_group.test_span_doc_delitem(doc)
spacy.tests.doc.test_span_group.test_span_group_add(doc)
spacy.tests.doc.test_span_group.test_span_group_concat(doc,other_doc)
spacy.tests.doc.test_span_group.test_span_group_copy(doc)
spacy.tests.doc.test_span_group.test_span_group_dealloc(span_group)
spacy.tests.doc.test_span_group.test_span_group_extend(doc)
spacy.tests.doc.test_span_group.test_span_group_has_overlap(doc)
spacy.tests.doc.test_span_group.test_span_group_iadd(doc)
spacy.tests.doc.test_span_group.test_span_group_set_item(doc,other_doc)
spacy.tests.doc.test_span_group.test_span_group_typing(doc:Doc)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/doc/test_graph.py----------------------------------------
A:spacy.tests.doc.test_graph.doc->Doc(Vocab(), words=['a', 'b', 'c', 'd'])
A:spacy.tests.doc.test_graph.graph->Graph(doc, name='hello', nodes=[(0,), (1,), (2,), (3,)], edges=[(0, 1), (0, 2), (0, 3), (3, 0)], labels=None, weights=None)
A:spacy.tests.doc.test_graph.node1->Graph(doc, name='hello', nodes=[(0,), (1,), (2,), (3,)], edges=[(0, 1), (0, 2), (0, 3), (3, 0)], labels=None, weights=None).add_node((0,))
A:spacy.tests.doc.test_graph.node2->Graph(doc, name='hello', nodes=[(0,), (1,), (2,), (3,)], edges=[(0, 1), (0, 2), (0, 3), (3, 0)], labels=None, weights=None).add_node((1, 3))
A:spacy.tests.doc.test_graph.(node0, node1, node2, node3)->list(graph.nodes)
spacy.tests.doc.test_graph.test_graph_edges_and_nodes()
spacy.tests.doc.test_graph.test_graph_init()
spacy.tests.doc.test_graph.test_graph_walk()


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/doc/test_array.py----------------------------------------
A:spacy.tests.doc.test_array.doc->Doc(en_vocab, words=words)
A:spacy.tests.doc.test_array.doc_array->Doc(en_vocab, words=words).to_array(['TAG', 'LEMMA'])
A:spacy.tests.doc.test_array.new_doc->Doc(doc.vocab, words=words).from_array(['TAG', 'LEMMA'], doc_array)
A:spacy.tests.doc.test_array.feats_array->Doc(en_vocab, words=words).to_array((ORTH, DEP))
A:spacy.tests.doc.test_array.feats_array_stringy->Doc(en_vocab, words=words).to_array(('ORTH', 'SHAPE'))
A:spacy.tests.doc.test_array.offsets->Doc(en_vocab, words=words).to_array('IDX')
A:spacy.tests.doc.test_array.arr->Doc(en_vocab, words=words).to_array(['HEAD'])
A:spacy.tests.doc.test_array.doc_from_array->Doc(en_vocab, words=words)
A:spacy.tests.doc.test_array.arr[0]->numpy.int32(5).astype(numpy.uint64)
spacy.tests.doc.test_array.test_doc_array_attr_of_token(en_vocab)
spacy.tests.doc.test_array.test_doc_array_dep(en_vocab)
spacy.tests.doc.test_array.test_doc_array_idx(en_vocab)
spacy.tests.doc.test_array.test_doc_array_morph(en_vocab)
spacy.tests.doc.test_array.test_doc_array_tag(en_vocab)
spacy.tests.doc.test_array.test_doc_array_to_from_string_attrs(en_vocab,attrs)
spacy.tests.doc.test_array.test_doc_from_array_heads_in_bounds(en_vocab)
spacy.tests.doc.test_array.test_doc_scalar_attr_of_token(en_vocab)
spacy.tests.doc.test_array.test_doc_stringy_array_attr_of_token(en_vocab)
spacy.tests.doc.test_array.test_issue2203(en_vocab)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/doc/test_doc_api.py----------------------------------------
A:spacy.tests.doc.test_doc_api.doc->en_tokenizer('Some text about Colombia and the Czech Republic')
A:spacy.tests.doc.test_doc_api.matrix->numpy.array([[0, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 2, 3, 3, 3], [1, 1, 3, 3, 3, 3], [1, 1, 3, 3, 4, 4], [1, 1, 3, 3, 4, 5]], dtype=numpy.int32)
A:spacy.tests.doc.test_doc_api.nlp->MultiLanguage()
A:spacy.tests.doc.test_doc_api.doc2->en_tokenizer('Some text about Colombia and the Czech Republic').copy()
A:spacy.tests.doc.test_doc_api.doc2_json->en_tokenizer('Some text about Colombia and the Czech Republic').copy().to_json()
A:spacy.tests.doc.test_doc_api.doc3->span3.as_doc()
A:spacy.tests.doc.test_doc_api.doc3_json->span3.as_doc().to_json()
A:spacy.tests.doc.test_doc_api.two_sent_doc->Doc(en_vocab, words=words, heads=heads, deps=deps)
A:spacy.tests.doc.test_doc_api.sents->list(doc.sents)
A:spacy.tests.doc.test_doc_api.sent_ext->self._get_my_ext(sent)
A:spacy.tests.doc.test_doc_api.docs->list(nlp.pipe(text, n_process=2))
A:spacy.tests.doc.test_doc_api.array->numpy.array(list(zip(pos, deps, tags)), dtype='uint64')
A:spacy.tests.doc.test_doc_api.tokens->en_tokenizer(text)
A:spacy.tests.doc.test_doc_api.new_tokens->Doc(tokens.vocab).from_bytes(tokens.to_bytes(exclude=['sentiment']), exclude=['sentiment'])
A:spacy.tests.doc.test_doc_api._->list(doc.noun_chunks)
A:spacy.tests.doc.test_doc_api.vocab->Vocab()
A:spacy.tests.doc.test_doc_api.lca->en_tokenizer('Some text about Colombia and the Czech Republic').get_lca_matrix()
A:spacy.tests.doc.test_doc_api.arr->en_tokenizer('Some text about Colombia and the Czech Republic').to_array(attrs)
A:spacy.tests.doc.test_doc_api.new_doc->Doc(en_vocab, words=words)
A:spacy.tests.doc.test_doc_api.attrs->en_tokenizer('Some text about Colombia and the Czech Republic')._get_array_attrs()
A:spacy.tests.doc.test_doc_api.span_group_texts->sorted([en_docs[0][1:4].text, en_docs[2][1:4].text, en_docs[4][0:1].text])
A:spacy.tests.doc.test_doc_api.de_doc->de_tokenizer(de_text)
A:spacy.tests.doc.test_doc_api.m_doc->spacy.tokens.Doc.from_docs(en_docs, exclude=['tensor'])
A:spacy.tests.doc.test_doc_api.ops->get_current_ops()
A:spacy.tests.doc.test_doc_api.doc.tensor->get_current_ops().asarray([[len(t.text), 0.0] for t in doc])
A:spacy.tests.doc.test_doc_api.doc1->en_tokenizer('Some text about Colombia and the Czech Republic')
A:spacy.tests.doc.test_doc_api.doc1b->en_tokenizer('c d')
spacy.tests.doc.test_doc_api.CustomPipe(self,nlp,name='my_pipe')
spacy.tests.doc.test_doc_api.CustomPipe._get_my_ext(span)
spacy.tests.doc.test_doc_api.test_doc_api_compare_by_string_position(en_vocab,text)
spacy.tests.doc.test_doc_api.test_doc_api_from_docs(en_tokenizer,de_tokenizer)
spacy.tests.doc.test_doc_api.test_doc_api_from_docs_ents(en_tokenizer)
spacy.tests.doc.test_doc_api.test_doc_api_getitem(en_tokenizer)
spacy.tests.doc.test_doc_api.test_doc_api_has_vector()
spacy.tests.doc.test_doc_api.test_doc_api_init(en_vocab)
spacy.tests.doc.test_doc_api.test_doc_api_right_edge(en_vocab)
spacy.tests.doc.test_doc_api.test_doc_api_runtime_error(en_tokenizer)
spacy.tests.doc.test_doc_api.test_doc_api_sents_empty_string(en_tokenizer)
spacy.tests.doc.test_doc_api.test_doc_api_serialize(en_tokenizer,text)
spacy.tests.doc.test_doc_api.test_doc_api_set_ents(en_tokenizer)
spacy.tests.doc.test_doc_api.test_doc_api_similarity_match()
spacy.tests.doc.test_doc_api.test_doc_ents_setter()
spacy.tests.doc.test_doc_api.test_doc_from_array_morph(en_vocab)
spacy.tests.doc.test_doc_api.test_doc_from_array_sent_starts(en_vocab)
spacy.tests.doc.test_doc_api.test_doc_init_iob()
spacy.tests.doc.test_doc_api.test_doc_is_nered(en_vocab)
spacy.tests.doc.test_doc_api.test_doc_lang(en_vocab)
spacy.tests.doc.test_doc_api.test_doc_morph_setter(en_tokenizer,de_tokenizer)
spacy.tests.doc.test_doc_api.test_doc_noun_chunks_not_implemented()
spacy.tests.doc.test_doc_api.test_doc_set_ents(en_tokenizer)
spacy.tests.doc.test_doc_api.test_doc_set_ents_invalid_spans(en_tokenizer)
spacy.tests.doc.test_doc_api.test_doc_spans_copy(en_tokenizer)
spacy.tests.doc.test_doc_api.test_doc_spans_setdefault(en_tokenizer)
spacy.tests.doc.test_doc_api.test_has_annotation(en_vocab)
spacy.tests.doc.test_doc_api.test_has_annotation_sents(en_vocab)
spacy.tests.doc.test_doc_api.test_init_args_unmodified(en_vocab)
spacy.tests.doc.test_doc_api.test_is_flags_deprecated(en_tokenizer)
spacy.tests.doc.test_doc_api.test_issue1547()
spacy.tests.doc.test_doc_api.test_issue1757()
spacy.tests.doc.test_doc_api.test_issue2396(en_vocab)
spacy.tests.doc.test_doc_api.test_issue2782(text,lang_cls)
spacy.tests.doc.test_doc_api.test_issue3869(sentence)
spacy.tests.doc.test_doc_api.test_issue3962(en_vocab)
spacy.tests.doc.test_doc_api.test_issue3962_long(en_vocab)
spacy.tests.doc.test_doc_api.test_issue4903()
spacy.tests.doc.test_doc_api.test_issue5048(en_vocab)
spacy.tests.doc.test_doc_api.test_lowest_common_ancestor(en_vocab,words,heads,lca_matrix)
spacy.tests.doc.test_doc_api.test_span_groups(en_tokenizer)
spacy.tests.doc.test_doc_api.test_token_lexeme(en_vocab)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/doc/test_retokenize_split.py----------------------------------------
A:spacy.tests.doc.test_retokenize_split.tensor->numpy.asarray([[1.0, 1.1], [2.0, 2.1], [3.0, 3.1], [4.0, 4.1], [5.0, 5.1], [6.0, 6.1]], dtype='f')
A:spacy.tests.doc.test_retokenize_split.doc->Doc(en_vocab, words=text.split())
A:spacy.tests.doc.test_retokenize_split.dep1->Doc(en_vocab, words=text.split()).vocab.strings.add('amod')
A:spacy.tests.doc.test_retokenize_split.dep2->Doc(en_vocab, words=text.split()).vocab.strings.add('subject')
A:spacy.tests.doc.test_retokenize_split.(sent1, sent2)->list(doc.sents)
A:spacy.tests.doc.test_retokenize_split.init_len->len(sent1)
A:spacy.tests.doc.test_retokenize_split.init_len2->len(sent2)
spacy.tests.doc.test_retokenize_split.test_doc_retokenize_spans_entity_split_iob()
spacy.tests.doc.test_retokenize_split.test_doc_retokenize_spans_sentence_update_after_split(en_vocab)
spacy.tests.doc.test_retokenize_split.test_doc_retokenize_split(en_vocab)
spacy.tests.doc.test_retokenize_split.test_doc_retokenize_split_dependencies(en_vocab)
spacy.tests.doc.test_retokenize_split.test_doc_retokenize_split_extension_attrs(en_vocab)
spacy.tests.doc.test_retokenize_split.test_doc_retokenize_split_extension_attrs_invalid(en_vocab,underscore_attrs)
spacy.tests.doc.test_retokenize_split.test_doc_retokenize_split_heads_error(en_vocab)
spacy.tests.doc.test_retokenize_split.test_doc_retokenize_split_lemmas(en_vocab)
spacy.tests.doc.test_retokenize_split.test_doc_retokenize_split_orths_mismatch(en_vocab)
spacy.tests.doc.test_retokenize_split.test_doc_retokenizer_realloc(en_vocab)
spacy.tests.doc.test_retokenize_split.test_doc_retokenizer_split_lex_attrs(en_vocab)
spacy.tests.doc.test_retokenize_split.test_doc_retokenizer_split_norm(en_vocab)
spacy.tests.doc.test_retokenize_split.test_issue3540(en_vocab)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/doc/test_morphanalysis.py----------------------------------------
A:spacy.tests.doc.test_morphanalysis.doc->tokenizer('a dog')
spacy.tests.doc.test_morphanalysis.i_has(en_tokenizer)
spacy.tests.doc.test_morphanalysis.test_morph_get(i_has)
spacy.tests.doc.test_morphanalysis.test_morph_iter(i_has)
spacy.tests.doc.test_morphanalysis.test_morph_property(tokenizer)
spacy.tests.doc.test_morphanalysis.test_morph_props(i_has)
spacy.tests.doc.test_morphanalysis.test_morph_set(i_has)
spacy.tests.doc.test_morphanalysis.test_morph_str(i_has)
spacy.tests.doc.test_morphanalysis.test_token_morph_eq(i_has)
spacy.tests.doc.test_morphanalysis.test_token_morph_key(i_has)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/doc/test_underscore.py----------------------------------------
A:spacy.tests.doc.test_underscore.doc->Doc(en_vocab, words=['hello', 'world'])
A:spacy.tests.doc.test_underscore.uscore->Underscore(Underscore.span_extensions, span, start=span.start, end=span.end)
A:spacy.tests.doc.test_underscore.doc._->Underscore(Underscore.doc_extensions, doc)
A:spacy.tests.doc.test_underscore.span->Mock(doc=Mock(), start=0, end=2)
A:spacy.tests.doc.test_underscore.span._->Underscore(Underscore.span_extensions, span, start=span.start, end=span.end)
A:spacy.tests.doc.test_underscore.token->Mock(doc=Mock(), idx=7, say_cheese=lambda token: 'cheese')
A:spacy.tests.doc.test_underscore.token._->Underscore(Underscore.token_extensions, token, start=token.idx)
A:spacy.tests.doc.test_underscore.doc1->Doc(en_vocab, words=['one'])
A:spacy.tests.doc.test_underscore.doc2->Doc(en_vocab, words=['two'])
spacy.tests.doc.test_underscore.clean_underscore()
spacy.tests.doc.test_underscore.test_create_doc_underscore()
spacy.tests.doc.test_underscore.test_create_span_underscore()
spacy.tests.doc.test_underscore.test_doc_underscore_getattr_setattr()
spacy.tests.doc.test_underscore.test_doc_underscore_remove_extension(obj)
spacy.tests.doc.test_underscore.test_span_underscore_getter_setter()
spacy.tests.doc.test_underscore.test_token_underscore_method()
spacy.tests.doc.test_underscore.test_underscore_accepts_valid(valid_kwargs)
spacy.tests.doc.test_underscore.test_underscore_dir(en_vocab)
spacy.tests.doc.test_underscore.test_underscore_docstring(en_vocab)
spacy.tests.doc.test_underscore.test_underscore_mutable_defaults_dict(en_vocab)
spacy.tests.doc.test_underscore.test_underscore_mutable_defaults_list(en_vocab)
spacy.tests.doc.test_underscore.test_underscore_raises_for_dup(obj)
spacy.tests.doc.test_underscore.test_underscore_raises_for_invalid(invalid_kwargs)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/doc/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/training/test_augmenters.py----------------------------------------
A:spacy.tests.training.test_augmenters.doc->Doc(nlp.vocab, words=words, spaces=spaces, tags=tags, lemmas=lemmas, heads=heads, deps=deps, ents=ents)
A:spacy.tests.training.test_augmenters.augmenter->create_lower_casing_augmenter(level=1.0)
A:spacy.tests.training.test_augmenters.reader->Corpus(output_file, augmenter=create_spongebob_augmenter())
A:spacy.tests.training.test_augmenters.corpus->list(reader(nlp))
A:spacy.tests.training.test_augmenters.example_dict->Example(nlp.make_doc(text), doc).to_dict()
A:spacy.tests.training.test_augmenters.example->Example(nlp.make_doc(text), doc)
A:spacy.tests.training.test_augmenters.mod_ex->make_whitespace_variant(nlp, example, ' ', 5)
A:spacy.tests.training.test_augmenters.mod_ex2->make_whitespace_variant(nlp, mod_ex, '\t\t\n', j)
spacy.tests.training.test_augmenters.doc(nlp)
spacy.tests.training.test_augmenters.make_docbin(docs,name='roundtrip.spacy')
spacy.tests.training.test_augmenters.nlp()
spacy.tests.training.test_augmenters.test_custom_data_augmentation(nlp,doc)
spacy.tests.training.test_augmenters.test_lowercase_augmenter(nlp,doc)
spacy.tests.training.test_augmenters.test_make_orth_variants(nlp)
spacy.tests.training.test_augmenters.test_make_whitespace_variant(nlp)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/training/test_pretraining.py----------------------------------------
A:spacy.tests.training.test_pretraining.config->Config().from_str(pretrain_string_internal)
A:spacy.tests.training.test_pretraining.nlp->English(vocab)
A:spacy.tests.training.test_pretraining.pretrain_config->util.load_config(DEFAULT_CONFIG_PRETRAIN_PATH)
A:spacy.tests.training.test_pretraining.filled->filled.interpolate().interpolate()
A:spacy.tests.training.test_pretraining.file_path->write_sample_jsonl(pretrain_dir)
A:spacy.tests.training.test_pretraining.nlp_path->write_vectors_model(tmp_dir)
A:spacy.tests.training.test_pretraining.train_config->util.load_config(DEFAULT_CONFIG_PATH)
A:spacy.tests.training.test_pretraining.(train_path, dev_path)->write_sample_training(train_dir)
A:spacy.tests.training.test_pretraining.nlp_base->init_nlp(filled)
A:spacy.tests.training.test_pretraining.model_base->init_nlp(filled).get_pipe(P['component']).model.get_ref(P['layer']).get_ref('embed')
A:spacy.tests.training.test_pretraining.pretrained_model->Path(pretrain_dir / 'model3.bin')
A:spacy.tests.training.test_pretraining.filled['initialize']['init_tok2vec']->str(pretrained_model)
A:spacy.tests.training.test_pretraining.model->English(vocab).get_pipe(P['component']).model.get_ref(P['layer']).get_ref('embed')
A:spacy.tests.training.test_pretraining.doc->Doc(English().vocab, words=words, tags=tags)
A:spacy.tests.training.test_pretraining.doc_bin->DocBin()
A:spacy.tests.training.test_pretraining.vocab->Vocab()
spacy.tests.training.test_pretraining.test_pretraining_default()
spacy.tests.training.test_pretraining.test_pretraining_tagger()
spacy.tests.training.test_pretraining.test_pretraining_tagger_tok2vec(config)
spacy.tests.training.test_pretraining.test_pretraining_tok2vec_characters(objective)
spacy.tests.training.test_pretraining.test_pretraining_tok2vec_vectors(objective)
spacy.tests.training.test_pretraining.test_pretraining_tok2vec_vectors_fail(objective)
spacy.tests.training.test_pretraining.test_pretraining_training()
spacy.tests.training.test_pretraining.write_sample_jsonl(tmp_dir)
spacy.tests.training.test_pretraining.write_sample_training(tmp_dir)
spacy.tests.training.test_pretraining.write_vectors_model(tmp_dir)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/training/test_readers.py----------------------------------------
A:spacy.tests.training.test_readers.doc->nlp('Quick test')
A:spacy.tests.training.test_readers.config->Config().from_str(nlp_config_string)
A:spacy.tests.training.test_readers.nlp->load_model_from_config(config, auto_fill=True)
A:spacy.tests.training.test_readers.T->spacy.util.registry.resolve(nlp.config['training'], schema=ConfigSchemaTraining)
A:spacy.tests.training.test_readers.(train_corpus, dev_corpus)->resolve_dot_names(nlp.config, dot_names)
A:spacy.tests.training.test_readers.scores->load_model_from_config(config, auto_fill=True).evaluate(dev_examples)
A:spacy.tests.training.test_readers.dev_examples->list(dev_corpus(nlp))
spacy.tests.training.test_readers.test_cat_readers(reader,additional_config)
spacy.tests.training.test_readers.test_readers()


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/training/test_logger.py----------------------------------------
A:spacy.tests.training.test_logger.nlp->spacy.blank('en')
A:spacy.tests.training.test_logger.console_logger->spacy.training.loggers.console_logger(progress_bar=True, console_output=True, output_file=None)
A:spacy.tests.training.test_logger.(log_step, finalize)->console_logger(nlp)
spacy.tests.training.test_logger.info()
spacy.tests.training.test_logger.nlp()
spacy.tests.training.test_logger.test_console_logger(nlp,info)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/training/test_rehearse.py----------------------------------------
A:spacy.tests.training.test_rehearse.pipe->_optimize(nlp, component, TRAIN_DATA, False).get_pipe(component)
A:spacy.tests.training.test_rehearse.optimizer->_optimize(nlp, component, TRAIN_DATA, False).initialize()
A:spacy.tests.training.test_rehearse.doc->_optimize(nlp, component, TRAIN_DATA, False).make_doc(text)
A:spacy.tests.training.test_rehearse.example->spacy.training.Example.from_dict(doc, annotation)
A:spacy.tests.training.test_rehearse.nlp->_optimize(nlp, component, TRAIN_DATA, False)
spacy.tests.training.test_rehearse._add_ner_label(ner,data)
spacy.tests.training.test_rehearse._add_parser_label(parser,data)
spacy.tests.training.test_rehearse._add_tagger_label(tagger,data)
spacy.tests.training.test_rehearse._add_textcat_label(textcat,data)
spacy.tests.training.test_rehearse._optimize(nlp,component:str,data:List,rehearse:bool)
spacy.tests.training.test_rehearse.test_rehearse(component)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/training/test_new_example.py----------------------------------------
A:spacy.tests.training.test_new_example.vocab->Vocab()
A:spacy.tests.training.test_new_example.example->spacy.training.example.Example.from_dict(predicted, annots)
A:spacy.tests.training.test_new_example.predicted->Doc(vocab, words=annots['words'])
A:spacy.tests.training.test_new_example.aligned_tags->spacy.training.example.Example.from_dict(predicted, annots).get_aligned('TAG', as_string=True)
A:spacy.tests.training.test_new_example.example1->spacy.training.example.Example.from_dict(predicted, annots)
A:spacy.tests.training.test_new_example.aligned_tags1->spacy.training.example.Example.from_dict(predicted, annots).get_aligned('TAG', as_string=True)
A:spacy.tests.training.test_new_example.example2->spacy.training.example.Example.from_dict(predicted, example1.to_dict())
A:spacy.tests.training.test_new_example.aligned_tags2->spacy.training.example.Example.from_dict(predicted, example1.to_dict()).get_aligned('TAG', as_string=True)
A:spacy.tests.training.test_new_example.ex->spacy.training.example.Example.from_dict(predicted, annots)
A:spacy.tests.training.test_new_example.example_1->spacy.training.example.Example.from_dict(predicted, annots_head_only)
A:spacy.tests.training.test_new_example.example_2->spacy.training.example.Example.from_dict(predicted, annots_head_dep)
A:spacy.tests.training.test_new_example.reference->Doc(en_vocab, words=words, tags=tags)
A:spacy.tests.training.test_new_example.output_dict->spacy.training.example.Example.from_dict(predicted, annots).to_dict()
A:spacy.tests.training.test_new_example.output_example->spacy.training.example.Example.from_dict(predicted, output_dict)
spacy.tests.training.test_new_example.test_Example_aligned_whitespace(en_vocab)
spacy.tests.training.test_new_example.test_Example_from_dict_basic()
spacy.tests.training.test_new_example.test_Example_from_dict_invalid(annots)
spacy.tests.training.test_new_example.test_Example_from_dict_sentences()
spacy.tests.training.test_new_example.test_Example_from_dict_with_cats(annots)
spacy.tests.training.test_new_example.test_Example_from_dict_with_empty_entities()
spacy.tests.training.test_new_example.test_Example_from_dict_with_entities(annots)
spacy.tests.training.test_new_example.test_Example_from_dict_with_entities_invalid(annots)
spacy.tests.training.test_new_example.test_Example_from_dict_with_entities_overlapping(annots)
spacy.tests.training.test_new_example.test_Example_from_dict_with_links(annots)
spacy.tests.training.test_new_example.test_Example_from_dict_with_links_invalid(annots)
spacy.tests.training.test_new_example.test_Example_from_dict_with_morphology(annots)
spacy.tests.training.test_new_example.test_Example_from_dict_with_parse(annots)
spacy.tests.training.test_new_example.test_Example_from_dict_with_sent_start(annots)
spacy.tests.training.test_new_example.test_Example_from_dict_with_spans(annots)
spacy.tests.training.test_new_example.test_Example_from_dict_with_spans_invalid(annots)
spacy.tests.training.test_new_example.test_Example_from_dict_with_spans_overlapping(annots)
spacy.tests.training.test_new_example.test_Example_from_dict_with_tags(pred_words,annots)
spacy.tests.training.test_new_example.test_Example_init_requires_doc_objects()
spacy.tests.training.test_new_example.test_Example_missing_deps()
spacy.tests.training.test_new_example.test_Example_missing_heads()
spacy.tests.training.test_new_example.test_aligned_tags()
spacy.tests.training.test_new_example.test_aligned_tags_multi()
spacy.tests.training.test_new_example.test_issue11260()


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/training/test_training.py----------------------------------------
A:spacy.tests.training.test_training.nlp->spacy.blank('en')
A:spacy.tests.training.test_training.doc->nlp('This is a sentence')
A:spacy.tests.training.test_training.ner->spacy.blank('en').add_pipe('ner')
A:spacy.tests.training.test_training.example->Example(doc1, doc2)
A:spacy.tests.training.test_training.nlp2->load_model_from_path(model_dir)
A:spacy.tests.training.test_training.docs->list(json_to_docs(data))
A:spacy.tests.training.test_training.data->DocBin(docs=docs, attrs=attrs).to_bytes()
A:spacy.tests.training.test_training.reader->Corpus(output_file)
A:spacy.tests.training.test_training.train_data->list(reader(nlp))
A:spacy.tests.training.test_training.optimizer->Adam()
A:spacy.tests.training.test_training.docs1->list(nlp.pipe(texts, batch_size=1))
A:spacy.tests.training.test_training.docs2->list(nlp.pipe(texts, batch_size=4))
A:spacy.tests.training.test_training.tags->Example(doc1, doc2).get_aligned('TAG', as_string=True)
A:spacy.tests.training.test_training.predicted->Doc(en_vocab, words=other_tokens, spaces=[True, False, False, True, False, False])
A:spacy.tests.training.test_training.reference->Doc(en_vocab, words=spacy_tokens, spaces=[True, True, True, False, True, False])
A:spacy.tests.training.test_training.ner_tags->Example(doc1, doc2).get_aligned_ner()
A:spacy.tests.training.test_training.eg->Example(Doc(doc.vocab, words=[w.text for w in doc], spaces=[bool(w.whitespace_) for w in doc]), doc)
A:spacy.tests.training.test_training.split_examples->Example(doc1, doc2).split_sents()
A:spacy.tests.training.test_training.(words, spaces)->get_words_and_spaces(['I', 'flew', 'to', 'San Francisco', 'Valley', '.'], 'I flew  to San Francisco Valley.')
A:spacy.tests.training.test_training.biluo_tags_converted->offsets_to_biluo_tags(doc, offsets)
A:spacy.tests.training.test_training.offsets_converted->biluo_tags_to_offsets(doc, biluo_tags)
A:spacy.tests.training.test_training.spans->biluo_tags_to_spans(doc, biluo_tags)
A:spacy.tests.training.test_training.ents_y2x->Example(doc1, doc2).get_aligned_spans_y2x(ents_ref)
A:spacy.tests.training.test_training.ruler->spacy.blank('en').add_pipe('entity_ruler')
A:spacy.tests.training.test_training.ents_x2y->Example(doc1, doc2).get_aligned_spans_x2y(ents_pred)
A:spacy.tests.training.test_training.gold_doc->spacy.blank('en').make_doc(text)
A:spacy.tests.training.test_training.spans_y2x_no_overlap->Example(doc1, doc2).get_aligned_spans_y2x(spans_gold, allow_overlap=False)
A:spacy.tests.training.test_training.spans_y2x_overlap->Example(doc1, doc2).get_aligned_spans_y2x(spans_gold, allow_overlap=True)
A:spacy.tests.training.test_training.(proj_heads, proj_labels)->Example(doc1, doc2).get_aligned_parse(projectivize=True)
A:spacy.tests.training.test_training.(nonproj_heads, nonproj_labels)->Example(doc1, doc2).get_aligned_parse(projectivize=False)
A:spacy.tests.training.test_training.doc_a->Doc(doc.vocab, words=['Double-Jointed'], spaces=[False], deps=['ROOT'], heads=[0])
A:spacy.tests.training.test_training.doc_b->Doc(doc.vocab, words=['Double', '-', 'Jointed'], spaces=[True, True, True], deps=['amod', 'punct', 'ROOT'], heads=[2, 2, 2])
A:spacy.tests.training.test_training.(proj_heads, proj_deps)->Example(doc1, doc2).get_aligned_parse(projectivize=True)
A:spacy.tests.training.test_training.converted_biluo->iob_to_biluo(good_iob)
A:spacy.tests.training.test_training.reloaded_nlp->English()
A:spacy.tests.training.test_training.reloaded_examples->list(reader(reloaded_nlp))
A:spacy.tests.training.test_training.reloaded_docs->DocBin().from_disk(output_file).get_docs(nlp.vocab)
A:spacy.tests.training.test_training.doc.user_data['check']->set()
A:spacy.tests.training.test_training.(a2b, b2a)->get_alignments(tokens_b, tokens_a)
A:spacy.tests.training.test_training.batches->minibatch(train_examples, size=compounding(4.0, 32.0, 1.001))
A:spacy.tests.training.test_training.align->spacy.training.Alignment.from_strings(other_tokens, spacy_tokens)
A:spacy.tests.training.test_training.a->nlp('This is a sentence').to_array(['TAG'])
A:spacy.tests.training.test_training.doc1->Doc(doc.vocab, words=[t.text for t in doc]).from_array(['TAG'], a)
A:spacy.tests.training.test_training.doc2->Doc(doc.vocab, words=[t.text for t in doc]).from_array(['TAG'], a)
A:spacy.tests.training.test_training.generator->train_while_improving(nlp, optimizer, generate_batch(), lambda : None, dropout=0.1, eval_frequency=100, accumulate_gradient=10, patience=10, max_steps=100, exclude=[], annotating_components=[], before_update=before_update)
spacy.tests.training.test_training._train_tuples(train_data)
spacy.tests.training.test_training.doc()
spacy.tests.training.test_training.merged_dict()
spacy.tests.training.test_training.test_align(tokens_a,tokens_b,expected)
spacy.tests.training.test_training.test_aligned_spans_x2y(en_vocab,en_tokenizer)
spacy.tests.training.test_training.test_aligned_spans_y2x(en_vocab,en_tokenizer)
spacy.tests.training.test_training.test_aligned_spans_y2x_overlap(en_vocab,en_tokenizer)
spacy.tests.training.test_training.test_alignment()
spacy.tests.training.test_training.test_alignment_array()
spacy.tests.training.test_training.test_alignment_case_insensitive()
spacy.tests.training.test_training.test_alignment_complex()
spacy.tests.training.test_training.test_alignment_complex_example(en_vocab)
spacy.tests.training.test_training.test_alignment_different_texts()
spacy.tests.training.test_training.test_alignment_spaces(en_vocab)
spacy.tests.training.test_training.test_biluo_spans(en_tokenizer)
spacy.tests.training.test_training.test_docbin_user_data_not_serialized(doc)
spacy.tests.training.test_training.test_docbin_user_data_serialized(doc)
spacy.tests.training.test_training.test_example_constructor(en_vocab)
spacy.tests.training.test_training.test_example_from_dict_no_ner(en_vocab)
spacy.tests.training.test_training.test_example_from_dict_some_ner(en_vocab)
spacy.tests.training.test_training.test_example_from_dict_tags(en_vocab)
spacy.tests.training.test_training.test_gold_biluo_4791(en_vocab,en_tokenizer)
spacy.tests.training.test_training.test_gold_biluo_BIL(en_vocab)
spacy.tests.training.test_training.test_gold_biluo_BL(en_vocab)
spacy.tests.training.test_training.test_gold_biluo_U(en_vocab)
spacy.tests.training.test_training.test_gold_biluo_additional_whitespace(en_vocab,en_tokenizer)
spacy.tests.training.test_training.test_gold_biluo_many_to_one(en_vocab,en_tokenizer)
spacy.tests.training.test_training.test_gold_biluo_misalign(en_vocab)
spacy.tests.training.test_training.test_gold_biluo_misaligned(en_vocab,en_tokenizer)
spacy.tests.training.test_training.test_gold_biluo_one_to_many(en_vocab,en_tokenizer)
spacy.tests.training.test_training.test_gold_biluo_overlap(en_vocab)
spacy.tests.training.test_training.test_gold_constructor()
spacy.tests.training.test_training.test_gold_ner_missing_tags(en_tokenizer)
spacy.tests.training.test_training.test_goldparse_endswith_space(en_tokenizer)
spacy.tests.training.test_training.test_goldparse_startswith_space(en_tokenizer)
spacy.tests.training.test_training.test_iob_to_biluo()
spacy.tests.training.test_training.test_issue4402()
spacy.tests.training.test_training.test_issue7029()
spacy.tests.training.test_training.test_issue999()
spacy.tests.training.test_training.test_json_to_docs_no_ner(en_vocab)
spacy.tests.training.test_training.test_projectivize(en_tokenizer)
spacy.tests.training.test_training.test_retokenized_docs(doc)
spacy.tests.training.test_training.test_roundtrip_docs_to_docbin(doc)
spacy.tests.training.test_training.test_roundtrip_offsets_biluo_conversion(en_tokenizer)
spacy.tests.training.test_training.test_split_sentences(en_vocab)
spacy.tests.training.test_training.test_split_sents(merged_dict)
spacy.tests.training.test_training.test_training_before_update(doc)
spacy.tests.training.test_training.test_tuple_format_implicit()
spacy.tests.training.test_training.test_tuple_format_implicit_invalid()
spacy.tests.training.test_training.vocab()


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/training/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/morphology/test_morph_features.py----------------------------------------
A:spacy.tests.morphology.test_morph_features.tag1->morphology.add({'Case': 'gen', 'Number': 'sing'})
A:spacy.tests.morphology.test_morph_features.tag2->morphology.add({'Number': 'sing', 'Case': 'gen'})
spacy.tests.morphology.test_morph_features.morphology()
spacy.tests.morphology.test_morph_features.test_add_morphology_with_int_ids(morphology)
spacy.tests.morphology.test_morph_features.test_add_morphology_with_mix_strings_and_ints(morphology)
spacy.tests.morphology.test_morph_features.test_add_morphology_with_string_names(morphology)
spacy.tests.morphology.test_morph_features.test_init(morphology)
spacy.tests.morphology.test_morph_features.test_morphology_tags_hash_distinctly(morphology)
spacy.tests.morphology.test_morph_features.test_morphology_tags_hash_independent_of_order(morphology)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/morphology/test_morph_pickle.py----------------------------------------
A:spacy.tests.morphology.test_morph_pickle.morphology->Morphology(StringStore())
A:spacy.tests.morphology.test_morph_pickle.b->pickle.dumps(morphology)
A:spacy.tests.morphology.test_morph_pickle.reloaded_morphology->pickle.loads(b)
A:spacy.tests.morphology.test_morph_pickle.feat->pickle.loads(b).get(morphology.strings['Feat3=Val3|Feat4=Val4'])
spacy.tests.morphology.test_morph_pickle.morphology()
spacy.tests.morphology.test_morph_pickle.test_morphology_pickle_roundtrip(morphology)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/morphology/test_morph_converters.py----------------------------------------
spacy.tests.morphology.test_morph_converters.test_feats_converters()


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/morphology/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/parser/test_state.py----------------------------------------
A:spacy.tests.parser.test_state.state->StateClass(doc)
spacy.tests.parser.test_state.doc(vocab)
spacy.tests.parser.test_state.test_H(doc)
spacy.tests.parser.test_state.test_L(doc)
spacy.tests.parser.test_state.test_R(doc)
spacy.tests.parser.test_state.test_init_state(doc)
spacy.tests.parser.test_state.test_push_pop(doc)
spacy.tests.parser.test_state.test_stack_depth(doc)
spacy.tests.parser.test_state.vocab()


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/parser/test_ner.py----------------------------------------
A:spacy.tests.parser.test_ner.actions->spacy.pipeline._parser_internals.ner.BiluoPushDown.get_actions(entity_types=entity_types)
A:spacy.tests.parser.test_ner.nlp->English()
A:spacy.tests.parser.test_ner.ner->English().add_pipe('beam_ner', config=config)
A:spacy.tests.parser.test_ner.example->spacy.training.Example.from_dict(doc, {'ner': iob})
A:spacy.tests.parser.test_ner.nlp2->spacy.util.load_model_from_path(tmp_dir)
A:spacy.tests.parser.test_ner.optimizer->English().initialize()
A:spacy.tests.parser.test_ner.ner2->spacy.util.load_model_from_path(tmp_dir).get_pipe('beam_ner')
A:spacy.tests.parser.test_ner.doc1->nlp1('I live in New York')
A:spacy.tests.parser.test_ner.ruler->English().add_pipe('entity_ruler')
A:spacy.tests.parser.test_ner.doc2->nlp2(test_text)
A:spacy.tests.parser.test_ner.doc->Doc(nlp.vocab, words=tokens)
A:spacy.tests.parser.test_ner.apple_ent->Span(doc, 5, 6, label='MY_ORG')
A:spacy.tests.parser.test_ner.act_classes->BiluoPushDown(vocab.strings, actions, incorrect_spans_key='non_entities').get_oracle_sequence(example)
A:spacy.tests.parser.test_ner.tsys->BiluoPushDown(vocab.strings, actions, incorrect_spans_key='non_entities')
A:spacy.tests.parser.test_ner.moves->BiluoPushDown(en_vocab.strings)
A:spacy.tests.parser.test_ner.(action, label)->split_bilu_label(tag)
A:spacy.tests.parser.test_ner.nlp1->English()
A:spacy.tests.parser.test_ner.ner1->English().create_pipe('ner', config=config)
A:spacy.tests.parser.test_ner.batches->spacy.util.minibatch(train_examples, size=8)
A:spacy.tests.parser.test_ner.untrained_ner->English().add_pipe('ner')
A:spacy.tests.parser.test_ner.doc3->nlp2(test_text)
A:spacy.tests.parser.test_ner.beams->English().add_pipe('beam_ner', config=config).predict(docs)
A:spacy.tests.parser.test_ner.beams2->spacy.util.load_model_from_path(tmp_dir).get_pipe('beam_ner').predict(docs2)
A:spacy.tests.parser.test_ner.neg_doc->English().make_doc(train_text)
A:spacy.tests.parser.test_ner.neg_ex->Example(neg_doc, neg_doc)
A:spacy.tests.parser.test_ner.neg_span->Span(doc, 50, 53, 'ORG')
A:spacy.tests.parser.test_ner.nlp.vocab.lookups->Lookups()
spacy.tests.parser.test_ner.BlockerComponent1(self,nlp,start,end,name='my_blocker')
spacy.tests.parser.test_ner.doc(vocab)
spacy.tests.parser.test_ner.entity_annots(doc)
spacy.tests.parser.test_ner.entity_types(entity_annots)
spacy.tests.parser.test_ner.neg_key()
spacy.tests.parser.test_ner.test_accept_blocked_token()
spacy.tests.parser.test_ner.test_beam_ner_scores()
spacy.tests.parser.test_ner.test_beam_overfitting_IO(neg_key)
spacy.tests.parser.test_ner.test_beam_valid_parse(neg_key)
spacy.tests.parser.test_ner.test_block_ner()
spacy.tests.parser.test_ner.test_empty_ner()
spacy.tests.parser.test_ner.test_get_oracle_moves(tsys,doc,entity_annots)
spacy.tests.parser.test_ner.test_issue1967(label)
spacy.tests.parser.test_ner.test_issue2179()
spacy.tests.parser.test_ner.test_issue2385()
spacy.tests.parser.test_ner.test_issue2800()
spacy.tests.parser.test_ner.test_issue3209()
spacy.tests.parser.test_ner.test_issue4267()
spacy.tests.parser.test_ner.test_issue4313()
spacy.tests.parser.test_ner.test_labels_from_BILUO()
spacy.tests.parser.test_ner.test_neg_annotation(neg_key)
spacy.tests.parser.test_ner.test_neg_annotation_conflict(neg_key)
spacy.tests.parser.test_ner.test_negative_sample_key_is_in_config(vocab,entity_types)
spacy.tests.parser.test_ner.test_negative_samples_U_entity(tsys,vocab,neg_key)
spacy.tests.parser.test_ner.test_negative_samples_three_word_input(tsys,vocab,neg_key)
spacy.tests.parser.test_ner.test_negative_samples_two_word_input(tsys,vocab,neg_key)
spacy.tests.parser.test_ner.test_ner_before_ruler()
spacy.tests.parser.test_ner.test_ner_constructor(en_vocab)
spacy.tests.parser.test_ner.test_ner_warns_no_lookups(caplog)
spacy.tests.parser.test_ner.test_oracle_moves_missing_B(en_vocab)
spacy.tests.parser.test_ner.test_oracle_moves_whitespace(en_vocab)
spacy.tests.parser.test_ner.test_overfitting_IO(use_upper)
spacy.tests.parser.test_ner.test_overwrite_token()
spacy.tests.parser.test_ner.test_ruler_before_ner()
spacy.tests.parser.test_ner.test_train_empty()
spacy.tests.parser.test_ner.test_train_negative_deprecated()
spacy.tests.parser.test_ner.tsys(vocab,entity_types)
spacy.tests.parser.test_ner.vocab()


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/parser/test_space_attachment.py----------------------------------------
A:spacy.tests.parser.test_space_attachment.doc->Doc(en_parser.vocab, words=text)
spacy.tests.parser.test_space_attachment.test_parser_sentence_space(en_vocab)
spacy.tests.parser.test_space_attachment.test_parser_space_attachment(en_vocab)
spacy.tests.parser.test_space_attachment.test_parser_space_attachment_intermediate_trailing(en_vocab,en_parser)
spacy.tests.parser.test_space_attachment.test_parser_space_attachment_leading(en_vocab,en_parser)
spacy.tests.parser.test_space_attachment.test_parser_space_attachment_space(en_parser,text,length)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/parser/test_nn_beam.py----------------------------------------
A:spacy.tests.parser.test_nn_beam.aeager->ArcEager(vocab.strings, {})
A:spacy.tests.parser.test_nn_beam.vec->numpy.random.uniform(-0.1, 0.1, (len(doc), vector_size))
A:spacy.tests.parser.test_nn_beam.(states, golds, _)->moves.init_gold_batch(examples)
A:spacy.tests.parser.test_nn_beam.n_state->sum((len(beam) for beam in beam))
A:spacy.tests.parser.test_nn_beam.nlp->Language()
A:spacy.tests.parser.test_nn_beam.parser->Language().add_pipe('beam_parser')
A:spacy.tests.parser.test_nn_beam.doc->Language().make_doc('Australia is a country')
A:spacy.tests.parser.test_nn_beam.beam_density->float(hyp.draw(hypothesis.strategies.floats(0.0, 1.0, width=32)))
A:spacy.tests.parser.test_nn_beam.beam->BeamBatch(moves, states, golds, width=beam_width, density=beam_density)
A:spacy.tests.parser.test_nn_beam.scores->hyp.draw(ndarrays_of_shape((n_state, moves.n_moves)))
spacy.tests.parser.test_nn_beam.batch_size(docs)
spacy.tests.parser.test_nn_beam.beam(moves,examples,beam_width)
spacy.tests.parser.test_nn_beam.beam_density(request)
spacy.tests.parser.test_nn_beam.beam_width()
spacy.tests.parser.test_nn_beam.docs(vocab)
spacy.tests.parser.test_nn_beam.examples(docs)
spacy.tests.parser.test_nn_beam.moves(vocab)
spacy.tests.parser.test_nn_beam.scores(moves,batch_size,beam_width)
spacy.tests.parser.test_nn_beam.states(docs)
spacy.tests.parser.test_nn_beam.test_beam_advance(beam,scores)
spacy.tests.parser.test_nn_beam.test_beam_advance_too_few_scores(beam,scores)
spacy.tests.parser.test_nn_beam.test_beam_density(moves,examples,beam_width,hyp)
spacy.tests.parser.test_nn_beam.test_beam_parse(examples,beam_width)
spacy.tests.parser.test_nn_beam.test_create_beam(beam)
spacy.tests.parser.test_nn_beam.tokvecs(docs,vector_size)
spacy.tests.parser.test_nn_beam.vector_size()
spacy.tests.parser.test_nn_beam.vocab()


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/parser/test_preset_sbd.py----------------------------------------
A:spacy.tests.parser.test_preset_sbd.doc->parser(doc)
A:spacy.tests.parser.test_preset_sbd.parser->DependencyParser(vocab, model)
A:spacy.tests.parser.test_preset_sbd.sgd->Adam(0.001)
A:spacy.tests.parser.test_preset_sbd.example->spacy.training.Example.from_dict(doc, {'heads': [1, 1, 3, 3], 'deps': ['left', 'ROOT', 'left', 'ROOT']})
spacy.tests.parser.test_preset_sbd._parser_example(parser)
spacy.tests.parser.test_preset_sbd.parser(vocab)
spacy.tests.parser.test_preset_sbd.test_no_sentences(parser)
spacy.tests.parser.test_preset_sbd.test_sents_1(parser)
spacy.tests.parser.test_preset_sbd.test_sents_1_2(parser)
spacy.tests.parser.test_preset_sbd.test_sents_1_3(parser)
spacy.tests.parser.test_preset_sbd.vocab()


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/parser/test_neural_parser.py----------------------------------------
A:spacy.tests.parser.test_neural_parser.actions->spacy.pipeline._parser_internals.arc_eager.ArcEager.get_actions(left_labels=['L'], right_labels=['R'])
A:spacy.tests.parser.test_neural_parser.example->spacy.training.Example.from_dict(doc, gold)
spacy.tests.parser.test_neural_parser.arc_eager(vocab)
spacy.tests.parser.test_neural_parser.doc(vocab)
spacy.tests.parser.test_neural_parser.gold(doc)
spacy.tests.parser.test_neural_parser.model(arc_eager,tok2vec,vocab)
spacy.tests.parser.test_neural_parser.parser(vocab,arc_eager)
spacy.tests.parser.test_neural_parser.test_build_model(parser,vocab)
spacy.tests.parser.test_neural_parser.test_can_init_nn_parser(parser)
spacy.tests.parser.test_neural_parser.test_predict_doc(parser,tok2vec,model,doc)
spacy.tests.parser.test_neural_parser.test_predict_doc_beam(parser,model,doc)
spacy.tests.parser.test_neural_parser.test_update_doc(parser,model,doc,gold)
spacy.tests.parser.test_neural_parser.test_update_doc_beam(parser,model,doc,gold)
spacy.tests.parser.test_neural_parser.tok2vec()
spacy.tests.parser.test_neural_parser.vocab()


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/parser/test_arc_eager_oracle.py----------------------------------------
A:spacy.tests.parser.test_arc_eager_oracle.doc->Doc(Vocab(), words=gold_words)
A:spacy.tests.parser.test_arc_eager_oracle.example->Example(predicted=predicted, reference=reference)
A:spacy.tests.parser.test_arc_eager_oracle.(states, golds, _)->M.init_gold_batch([example])
A:spacy.tests.parser.test_arc_eager_oracle.name->M.class_name(i)
A:spacy.tests.parser.test_arc_eager_oracle.state_costs[name]->M.get_cost(state, gold, i)
A:spacy.tests.parser.test_arc_eager_oracle.moves->ArcEager(vocab.strings, ArcEager.get_actions())
A:spacy.tests.parser.test_arc_eager_oracle.vocab->Vocab()
A:spacy.tests.parser.test_arc_eager_oracle.ae->ArcEager(vocab.strings, ArcEager.get_actions(left_labels=['amod'], right_labels=['pobj']))
A:spacy.tests.parser.test_arc_eager_oracle.(state, cost_history)->get_sequence_costs(arc_eager, words, heads, deps, actions)
A:spacy.tests.parser.test_arc_eager_oracle.parser->DependencyParser(doc.vocab, model)
A:spacy.tests.parser.test_arc_eager_oracle.(heads, deps)->projectivize(heads, deps)
A:spacy.tests.parser.test_arc_eager_oracle.line->line.strip().strip()
A:spacy.tests.parser.test_arc_eager_oracle.(word, dep, head)->line.strip().strip().split()
A:spacy.tests.parser.test_arc_eager_oracle.ae_oracle_actions->arc_eager.get_oracle_sequence(example, _debug=False)
A:spacy.tests.parser.test_arc_eager_oracle.reference->Doc(Vocab(), words=gold_words, deps=gold_deps, heads=gold_heads)
A:spacy.tests.parser.test_arc_eager_oracle.predicted->Doc(reference.vocab, words=['[', 'catalase', ']', ':', 'that', 'is', 'bad'])
spacy.tests.parser.test_arc_eager_oracle.arc_eager(vocab)
spacy.tests.parser.test_arc_eager_oracle.get_sequence_costs(M,words,heads,deps,transitions)
spacy.tests.parser.test_arc_eager_oracle.test_get_oracle_actions()
spacy.tests.parser.test_arc_eager_oracle.test_issue7056()
spacy.tests.parser.test_arc_eager_oracle.test_oracle_bad_tokenization(vocab,arc_eager)
spacy.tests.parser.test_arc_eager_oracle.test_oracle_dev_sentence(vocab,arc_eager)
spacy.tests.parser.test_arc_eager_oracle.test_oracle_four_words(arc_eager,vocab)
spacy.tests.parser.test_arc_eager_oracle.vocab()


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/parser/test_parse.py----------------------------------------
A:spacy.tests.parser.test_parse.parser->English().add_pipe('beam_parser', config=config)
A:spacy.tests.parser.test_parse.sgd->Adam(0.001)
A:spacy.tests.parser.test_parse.doc->English().make_doc(test_text)
A:spacy.tests.parser.test_parse.example->spacy.training.Example.from_dict(doc, {'heads': [1, 1, 3, 3], 'deps': ['left', 'ROOT', 'left', 'ROOT']})
A:spacy.tests.parser.test_parse.tokens->Doc(en_vocab, words=words)
A:spacy.tests.parser.test_parse.nlp->English()
A:spacy.tests.parser.test_parse.optimizer->English().initialize()
A:spacy.tests.parser.test_parse.nlp2->spacy.util.load_model_from_path(tmp_dir)
A:spacy.tests.parser.test_parse.doc2->nlp2(test_text)
A:spacy.tests.parser.test_parse.beams->English().add_pipe('beam_parser', config=config).predict(docs)
A:spacy.tests.parser.test_parse.(head_scores, label_scores)->English().add_pipe('beam_parser', config=config).scored_parses(beams)
A:spacy.tests.parser.test_parse.parser2->spacy.util.load_model_from_path(tmp_dir).get_pipe('beam_parser')
A:spacy.tests.parser.test_parse.beams2->spacy.util.load_model_from_path(tmp_dir).get_pipe('beam_parser').predict(docs2)
A:spacy.tests.parser.test_parse.(head_scores2, label_scores2)->spacy.util.load_model_from_path(tmp_dir).get_pipe('beam_parser').scored_parses(beams2)
spacy.tests.parser.test_parse._parser_example(parser)
spacy.tests.parser.test_parse.parser(vocab)
spacy.tests.parser.test_parse.test_beam_overfitting_IO()
spacy.tests.parser.test_parse.test_beam_parser_scores()
spacy.tests.parser.test_parse.test_incomplete_data(pipe_name)
spacy.tests.parser.test_parse.test_issue2772(en_vocab)
spacy.tests.parser.test_parse.test_issue3830_no_subtok()
spacy.tests.parser.test_parse.test_issue3830_with_subtok()
spacy.tests.parser.test_parse.test_overfitting_IO(pipe_name)
spacy.tests.parser.test_parse.test_parser_arc_eager_finalize_state(en_vocab,en_parser)
spacy.tests.parser.test_parse.test_parser_configs(pipe_name,parser_config)
spacy.tests.parser.test_parse.test_parser_constructor(en_vocab)
spacy.tests.parser.test_parse.test_parser_initial(en_vocab,en_parser)
spacy.tests.parser.test_parse.test_parser_merge_pp(en_vocab)
spacy.tests.parser.test_parse.test_parser_parse_one_word_sentence(en_vocab,en_parser,words)
spacy.tests.parser.test_parse.test_parser_parse_subtrees(en_vocab,en_parser)
spacy.tests.parser.test_parse.test_parser_root(en_vocab)
spacy.tests.parser.test_parse.test_parser_set_sent_starts(en_vocab)
spacy.tests.parser.test_parse.test_partial_annotation(parser)
spacy.tests.parser.test_parse.vocab()


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/parser/test_add_label.py----------------------------------------
A:spacy.tests.parser.test_add_label.parser->_train_parser(parser)
A:spacy.tests.parser.test_add_label.sgd->Adam(0.001)
A:spacy.tests.parser.test_add_label.doc->Doc(nlp.vocab, words=['hello', 'world'], ents=['B-D', 'O'])
A:spacy.tests.parser.test_add_label.example->Example(nlp.make_doc(doc.text), doc)
A:spacy.tests.parser.test_add_label.ner1->EntityRecognizer(Vocab(), model)
A:spacy.tests.parser.test_add_label.ner2->EntityRecognizer(Vocab(), model)
A:spacy.tests.parser.test_add_label.pipe->pipe_cls(Vocab(), model)
A:spacy.tests.parser.test_add_label.pipe_labels->sorted(list(pipe.labels))
A:spacy.tests.parser.test_add_label.nlp->Language()
A:spacy.tests.parser.test_add_label.ner->Language().add_pipe('ner')
spacy.tests.parser.test_add_label._ner_example(ner)
spacy.tests.parser.test_add_label._parser_example(parser)
spacy.tests.parser.test_add_label._train_parser(parser)
spacy.tests.parser.test_add_label.parser(vocab)
spacy.tests.parser.test_add_label.test_add_label(parser)
spacy.tests.parser.test_add_label.test_add_label_deserializes_correctly()
spacy.tests.parser.test_add_label.test_add_label_get_label(pipe_cls,n_moves,model_config)
spacy.tests.parser.test_add_label.test_init_parser(parser)
spacy.tests.parser.test_add_label.test_ner_labels_added_implicitly_on_beam_parse()
spacy.tests.parser.test_add_label.test_ner_labels_added_implicitly_on_greedy_parse()
spacy.tests.parser.test_add_label.test_ner_labels_added_implicitly_on_predict()
spacy.tests.parser.test_add_label.test_ner_labels_added_implicitly_on_update()
spacy.tests.parser.test_add_label.vocab()


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/parser/test_nonproj.py----------------------------------------
A:spacy.tests.parser.test_nonproj.doc->Doc(en_vocab, words=words, deps=deco_labels, heads=proj_heads)
A:spacy.tests.parser.test_nonproj.(proj_heads, deco_labels)->spacy.pipeline._parser_internals.nonproj.projectivize(nonproj_tree2, labels2)
A:spacy.tests.parser.test_nonproj.(deproj_heads, undeco_labels)->deprojectivize(proj_heads, deco_labels)
spacy.tests.parser.test_nonproj.cyclic_tree()
spacy.tests.parser.test_nonproj.multirooted_tree()
spacy.tests.parser.test_nonproj.nonproj_tree()
spacy.tests.parser.test_nonproj.partial_tree()
spacy.tests.parser.test_nonproj.proj_tree()
spacy.tests.parser.test_nonproj.test_parser_ancestors(tree,cyclic_tree,partial_tree,multirooted_tree)
spacy.tests.parser.test_nonproj.test_parser_contains_cycle(tree,cyclic_tree,partial_tree,multirooted_tree)
spacy.tests.parser.test_nonproj.test_parser_is_nonproj_arc(cyclic_tree,nonproj_tree,partial_tree,multirooted_tree)
spacy.tests.parser.test_nonproj.test_parser_is_nonproj_tree(proj_tree,cyclic_tree,nonproj_tree,partial_tree,multirooted_tree)
spacy.tests.parser.test_nonproj.test_parser_pseudoprojectivity(en_vocab)
spacy.tests.parser.test_nonproj.tree()


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/parser/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/parser/test_parse_navigate.py----------------------------------------
A:spacy.tests.parser.test_parse_navigate.doc->Doc(en_vocab, words=words, heads=heads, deps=['dep'] * len(heads))
A:spacy.tests.parser.test_parse_navigate.lefts[head.i]->set()
A:spacy.tests.parser.test_parse_navigate.rights[head.i]->set()
A:spacy.tests.parser.test_parse_navigate.subtree->list(token.subtree)
A:spacy.tests.parser.test_parse_navigate.debug->'\t'.join((token.text, token.right_edge.text, subtree[-1].text, token.right_edge.head.text))
spacy.tests.parser.test_parse_navigate.heads()
spacy.tests.parser.test_parse_navigate.test_parser_parse_navigate_child_consistency(en_vocab,words,heads)
spacy.tests.parser.test_parse_navigate.test_parser_parse_navigate_consistency(en_vocab,words,heads)
spacy.tests.parser.test_parse_navigate.test_parser_parse_navigate_edges(en_vocab,words,heads)
spacy.tests.parser.test_parse_navigate.words()


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/tokenizer/test_naughty_strings.py----------------------------------------
A:spacy.tests.tokenizer.test_naughty_strings.tokens->tokenizer(text)
spacy.tests.tokenizer.test_naughty_strings.test_tokenizer_naughty_strings(tokenizer,text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/tokenizer/test_urls.py----------------------------------------
A:spacy.tests.tokenizer.test_urls.tokens->tokenizer(url + suffix1 + suffix2)
spacy.tests.tokenizer.test_urls.test_should_match(en_tokenizer,url)
spacy.tests.tokenizer.test_urls.test_should_not_match(en_tokenizer,url)
spacy.tests.tokenizer.test_urls.test_tokenizer_handles_prefixed_url(tokenizer,prefix,url)
spacy.tests.tokenizer.test_urls.test_tokenizer_handles_simple_surround_url(tokenizer,url)
spacy.tests.tokenizer.test_urls.test_tokenizer_handles_simple_url(tokenizer,url)
spacy.tests.tokenizer.test_urls.test_tokenizer_handles_suffixed_url(tokenizer,url,suffix)
spacy.tests.tokenizer.test_urls.test_tokenizer_handles_surround_url(tokenizer,prefix,suffix,url)
spacy.tests.tokenizer.test_urls.test_tokenizer_handles_two_prefix_url(tokenizer,prefix1,prefix2,url)
spacy.tests.tokenizer.test_urls.test_tokenizer_handles_two_suffix_url(tokenizer,suffix1,suffix2,url)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/tokenizer/test_tokenizer.py----------------------------------------
A:spacy.tests.tokenizer.test_tokenizer.doc->en_tokenizer(text)
A:spacy.tests.tokenizer.test_tokenizer.s->set([token])
A:spacy.tests.tokenizer.test_tokenizer.items->list(s)
A:spacy.tests.tokenizer.test_tokenizer.doc.tensor->numpy.ones((len(doc), 128), dtype='f')
A:spacy.tests.tokenizer.test_tokenizer.nlp->English()
A:spacy.tests.tokenizer.test_tokenizer.docs->list(nlp.pipe(['', 'hello']))
A:spacy.tests.tokenizer.test_tokenizer.doc1->Doc(Vocab(), words=['a', 'b', 'c'])
A:spacy.tests.tokenizer.test_tokenizer.doc2->Doc(Vocab(), words=['a', 'c', 'e'])
A:spacy.tests.tokenizer.test_tokenizer.prefix_re->compile_prefix_regex(prefixes)
A:spacy.tests.tokenizer.test_tokenizer.suffix_re->compile_suffix_regex(suffixes)
A:spacy.tests.tokenizer.test_tokenizer.infix_re->compile_infix_regex(infixes)
A:spacy.tests.tokenizer.test_tokenizer.simple_url_re->re.compile('^https?://')
A:spacy.tests.tokenizer.test_tokenizer.nlp.tokenizer->new_tokenizer(nlp)
A:spacy.tests.tokenizer.test_tokenizer.a->en_tokenizer('a')
A:spacy.tests.tokenizer.test_tokenizer.am->en_tokenizer('am')
A:spacy.tests.tokenizer.test_tokenizer.t1->nlp(text1)
A:spacy.tests.tokenizer.test_tokenizer.t2->nlp(text2)
A:spacy.tests.tokenizer.test_tokenizer.t3->nlp(text3)
A:spacy.tests.tokenizer.test_tokenizer.tokens->tokenizer(text)
A:spacy.tests.tokenizer.test_tokenizer.text->infile.read()
A:spacy.tests.tokenizer.test_tokenizer.tokens1->tokenizer(text1)
A:spacy.tests.tokenizer.test_tokenizer.tokens2->tokenizer(text2)
A:spacy.tests.tokenizer.test_tokenizer.vocab->Vocab()
A:spacy.tests.tokenizer.test_tokenizer.tokenizer->Tokenizer(en_vocab, token_match=re.compile('^id$').match, rules={'id': [{'ORTH': 'i'}, {'ORTH': 'd'}]})
A:spacy.tests.tokenizer.test_tokenizer.tokenizer1->Tokenizer(en_vocab, suffix_search=suffix_re.search, rules=rules)
spacy.tests.tokenizer.test_tokenizer.test_gold_misaligned(en_tokenizer,text,words)
spacy.tests.tokenizer.test_tokenizer.test_issue10086(en_tokenizer)
spacy.tests.tokenizer.test_tokenizer.test_issue1061()
spacy.tests.tokenizer.test_tokenizer.test_issue1235()
spacy.tests.tokenizer.test_tokenizer.test_issue1242()
spacy.tests.tokenizer.test_tokenizer.test_issue1257()
spacy.tests.tokenizer.test_tokenizer.test_issue1375()
spacy.tests.tokenizer.test_tokenizer.test_issue1488()
spacy.tests.tokenizer.test_tokenizer.test_issue1494()
spacy.tests.tokenizer.test_tokenizer.test_issue1963(en_tokenizer)
spacy.tests.tokenizer.test_tokenizer.test_issue2070()
spacy.tests.tokenizer.test_tokenizer.test_issue2626_2835(en_tokenizer,text)
spacy.tests.tokenizer.test_tokenizer.test_issue2656(en_tokenizer)
spacy.tests.tokenizer.test_tokenizer.test_issue2754(en_tokenizer)
spacy.tests.tokenizer.test_tokenizer.test_issue2926(fr_tokenizer)
spacy.tests.tokenizer.test_tokenizer.test_issue3002()
spacy.tests.tokenizer.test_tokenizer.test_issue3449()
spacy.tests.tokenizer.test_tokenizer.test_issue743()
spacy.tests.tokenizer.test_tokenizer.test_issue801(en_tokenizer,text,tokens)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_add_special_case(tokenizer,text,tokens)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_add_special_case_tag(text,tokens)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_colons(tokenizer,text)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_flush_cache(en_vocab)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_flush_specials(en_vocab)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_handle_text_from_file(tokenizer,file_name)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_handles_digits(tokenizer)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_handles_long_text(tokenizer)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_handles_no_word(tokenizer)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_handles_punct(tokenizer)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_handles_punct_braces(tokenizer)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_handles_single_word(tokenizer,text)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_infix_prefix(en_vocab)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_initial_special_case_explain(en_vocab)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_keep_urls(tokenizer,text)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_keeps_email(tokenizer,text)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_prefix_suffix_overlap_lookbehind(en_vocab)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_special_cases_idx(tokenizer)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_special_cases_spaces(tokenizer)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_special_cases_with_affixes(tokenizer)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_special_cases_with_affixes_preserve_spacy()
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_special_cases_with_period(tokenizer)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_suspected_freeing_strings(tokenizer)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_validate_special_case(tokenizer,text,tokens)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/tokenizer/test_explain.py----------------------------------------
A:spacy.tests.tokenizer.test_explain.examples->pytest.importorskip(f'spacy.lang.{lang}.examples')
A:spacy.tests.tokenizer.test_explain.suffix_re->re.compile('[\\.]$')
A:spacy.tests.tokenizer.test_explain.infix_re->re.compile('[/]')
A:spacy.tests.tokenizer.test_explain.tokenizer->Tokenizer(en_vocab, rules=rules, suffix_search=suffix_re.search, infix_finditer=infix_re.finditer)
A:spacy.tests.tokenizer.test_explain.punctuation_and_space_regex->'|'.join([*[re.escape(p) for p in string.punctuation], '\\s'])
spacy.tests.tokenizer.test_explain.sentence_strategy(draw:hypothesis.strategies.DrawFn,max_n_words:int=4)->str
spacy.tests.tokenizer.test_explain.test_tokenizer_explain(lang)
spacy.tests.tokenizer.test_explain.test_tokenizer_explain_fuzzy(lang:str,sentence:str)->None
spacy.tests.tokenizer.test_explain.test_tokenizer_explain_special_matcher(en_vocab)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/tokenizer/test_whitespace.py----------------------------------------
A:spacy.tests.tokenizer.test_whitespace.tokens->tokenizer(text)
spacy.tests.tokenizer.test_whitespace.test_tokenizer_handles_double_trailing_ws(tokenizer,text)
spacy.tests.tokenizer.test_whitespace.test_tokenizer_splits_double_space(tokenizer,text)
spacy.tests.tokenizer.test_whitespace.test_tokenizer_splits_newline(tokenizer,text)
spacy.tests.tokenizer.test_whitespace.test_tokenizer_splits_newline_double_space(tokenizer,text)
spacy.tests.tokenizer.test_whitespace.test_tokenizer_splits_newline_space(tokenizer,text)
spacy.tests.tokenizer.test_whitespace.test_tokenizer_splits_newline_space_wrap(tokenizer,text)
spacy.tests.tokenizer.test_whitespace.test_tokenizer_splits_single_space(tokenizer,text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/tokenizer/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/tokenizer/test_exceptions.py----------------------------------------
A:spacy.tests.tokenizer.test_exceptions.tokens->tokenizer(text)
spacy.tests.tokenizer.test_exceptions.test_tokenizer_degree(tokenizer)
spacy.tests.tokenizer.test_exceptions.test_tokenizer_excludes_false_pos_emoticons(tokenizer,text,length)
spacy.tests.tokenizer.test_exceptions.test_tokenizer_handles_emoji(tokenizer,text,length)
spacy.tests.tokenizer.test_exceptions.test_tokenizer_handles_emoticons(tokenizer)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/matcher/test_levenshtein.py----------------------------------------
spacy.tests.matcher.test_levenshtein.test_levenshtein(dist,a,b)
spacy.tests.matcher.test_levenshtein.test_levenshtein_compare(a,b,fuzzy,expected)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/matcher/test_pattern_validation.py----------------------------------------
A:spacy.tests.matcher.test_pattern_validation.matcher->Matcher(en_vocab)
A:spacy.tests.matcher.test_pattern_validation.errors->validate_token_pattern(pattern)
spacy.tests.matcher.test_pattern_validation.test_matcher_pattern_validation(en_vocab,pattern)
spacy.tests.matcher.test_pattern_validation.test_minimal_pattern_validation(en_vocab,pattern,n_errors,n_min_errors)
spacy.tests.matcher.test_pattern_validation.test_pattern_errors(en_vocab)
spacy.tests.matcher.test_pattern_validation.test_pattern_validation(pattern,n_errors,_)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/matcher/test_phrase_matcher.py----------------------------------------
A:spacy.tests.matcher.test_phrase_matcher.nlp->English()
A:spacy.tests.matcher.test_phrase_matcher.matcher->PhraseMatcher(en_vocab)
A:spacy.tests.matcher.test_phrase_matcher.doc->Doc(en_vocab, words=words)
A:spacy.tests.matcher.test_phrase_matcher.matches->matcher(doc, as_spans=True)
A:spacy.tests.matcher.test_phrase_matcher.pattern1->Doc(en_vocab, words=['this'])
A:spacy.tests.matcher.test_phrase_matcher.pattern2->Doc(en_vocab, words=['this', 'is'])
A:spacy.tests.matcher.test_phrase_matcher.ruler->English().add_pipe('entity_ruler', config={'phrase_matcher_attr': 'LOWER'})
A:spacy.tests.matcher.test_phrase_matcher.nlp_reloaded->English()
A:spacy.tests.matcher.test_phrase_matcher.doc_reloaded->nlp_reloaded(text)
A:spacy.tests.matcher.test_phrase_matcher.pattern->Doc(en_vocab, words=['Spans', 'and', 'Docs'])
A:spacy.tests.matcher.test_phrase_matcher.new_matches->matcher(doc)
A:spacy.tests.matcher.test_phrase_matcher.no_matches->matcher(doc)
A:spacy.tests.matcher.test_phrase_matcher.on_match->Mock()
A:spacy.tests.matcher.test_phrase_matcher.doc1->Doc(en_vocab, words=['Test'])
A:spacy.tests.matcher.test_phrase_matcher.doc2->Doc(en_vocab, words=['Test'])
A:spacy.tests.matcher.test_phrase_matcher.doc3->Doc(en_vocab, words=['Test'])
A:spacy.tests.matcher.test_phrase_matcher.mock->Mock()
A:spacy.tests.matcher.test_phrase_matcher.pattern3->Doc(en_vocab, words=['this', 'is', 'a'])
A:spacy.tests.matcher.test_phrase_matcher.pattern4->Doc(en_vocab, words=['this', 'is', 'a', 'word'])
A:spacy.tests.matcher.test_phrase_matcher.b->srsly.pickle_dumps(matcher)
A:spacy.tests.matcher.test_phrase_matcher.matcher_unpickled->srsly.pickle_loads(b)
A:spacy.tests.matcher.test_phrase_matcher.matches_unpickled->matcher_unpickled(doc)
A:spacy.tests.matcher.test_phrase_matcher._->PhraseMatcher(en_vocab, attr=attr)
A:spacy.tests.matcher.test_phrase_matcher.matches_doc->matcher(doc)
A:spacy.tests.matcher.test_phrase_matcher.matches_span->matcher(span)
spacy.tests.matcher.test_phrase_matcher.test_attr_pipeline_checks(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_attr_validation(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_issue10643(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_issue3248_1()
spacy.tests.matcher.test_phrase_matcher.test_issue3331(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_issue3972(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_issue4002(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_issue4373()
spacy.tests.matcher.test_phrase_matcher.test_issue4651_with_phrase_matcher_attr()
spacy.tests.matcher.test_phrase_matcher.test_issue6839(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_matcher_phrase_matcher(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_phrase_matcher_add_new_api(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_phrase_matcher_as_spans(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_phrase_matcher_basic_check(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_phrase_matcher_bool_attrs(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_phrase_matcher_callback(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_phrase_matcher_contains(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_phrase_matcher_deprecated(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_phrase_matcher_length(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_phrase_matcher_overlapping_with_remove(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_phrase_matcher_pickle(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_phrase_matcher_remove(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_phrase_matcher_remove_overlapping_patterns(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_phrase_matcher_repeated_add(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_phrase_matcher_sent_start(en_vocab,attr)
spacy.tests.matcher.test_phrase_matcher.test_phrase_matcher_string_attrs(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_phrase_matcher_string_attrs_negative(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_phrase_matcher_validation(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_span_in_phrasematcher(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_span_v_doc_in_phrasematcher(en_vocab)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/matcher/test_matcher_api.py----------------------------------------
A:spacy.tests.matcher.test_matcher_api.matcher->Matcher(en_vocab)
A:spacy.tests.matcher.test_matcher_api.(on_match, patterns)->Matcher(en_vocab).get('Rule')
A:spacy.tests.matcher.test_matcher_api.doc->Doc(en_vocab, words=['foo', 'bar', 'foo', 'foo', 'bar', 'foo', 'foo', 'foo', 'bar', 'bar'])
A:spacy.tests.matcher.test_matcher_api.on_match->Mock()
A:spacy.tests.matcher.test_matcher_api.spans->matcher(doc, as_spans=True)
A:spacy.tests.matcher.test_matcher_api.matches->matcher(doc)
A:spacy.tests.matcher.test_matcher_api.words1->'He said , " some words " ...'.split()
A:spacy.tests.matcher.test_matcher_api.words2->'He said , " some three words " ...'.split()
A:spacy.tests.matcher.test_matcher_api.words->'He said , " some words " ...'.split()
A:spacy.tests.matcher.test_matcher_api.control->Matcher(matcher.vocab)
A:spacy.tests.matcher.test_matcher_api.m->matcher(doc)
A:spacy.tests.matcher.test_matcher_api.doc1->Doc(en_vocab, words=['I', 'visited', 'New', 'York', 'and', 'California'])
A:spacy.tests.matcher.test_matcher_api.doc2->Doc(en_vocab, words=['I', 'visited', 'my', 'friend', 'Alicia'])
A:spacy.tests.matcher.test_matcher_api.doc3->Doc(en_vocab, words=['Test'])
A:spacy.tests.matcher.test_matcher_api.mock->Mock()
spacy.tests.matcher.test_matcher_api.matcher(en_vocab)
spacy.tests.matcher.test_matcher_api.test_attr_pipeline_checks(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_add_new_api(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_any_token_operator(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_as_spans(matcher)
spacy.tests.matcher.test_matcher_api.test_matcher_basic_check(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_callback(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_callback_with_alignments(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_compare_length(en_vocab,cmp,bad)
spacy.tests.matcher.test_matcher_api.test_matcher_deprecated(matcher)
spacy.tests.matcher.test_matcher_api.test_matcher_empty_dict(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_empty_patterns_warns(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_ent_iob_key(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_extension_attribute(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_extension_in_set_predicate(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_extension_set_membership(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_from_api_docs(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_from_usage_docs(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_intersect_value_operator(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_len_contains(matcher)
spacy.tests.matcher.test_matcher_api.test_matcher_match_end(matcher)
spacy.tests.matcher.test_matcher_api.test_matcher_match_fuzzy(en_vocab,rules,match_locs)
spacy.tests.matcher.test_matcher_api.test_matcher_match_fuzzy_set_multiple(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_match_fuzzy_set_op_longest(en_vocab,set_op)
spacy.tests.matcher.test_matcher_api.test_matcher_match_fuzzyn_all_insertions(en_vocab,fuzzyn)
spacy.tests.matcher.test_matcher_api.test_matcher_match_fuzzyn_set_multiple(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_match_fuzzyn_set_op_longest(en_vocab,greedy,set_op)
spacy.tests.matcher.test_matcher_api.test_matcher_match_fuzzyn_various_edits(en_vocab,fuzzyn)
spacy.tests.matcher.test_matcher_api.test_matcher_match_middle(matcher)
spacy.tests.matcher.test_matcher_api.test_matcher_match_multi(matcher)
spacy.tests.matcher.test_matcher_api.test_matcher_match_one_plus(matcher)
spacy.tests.matcher.test_matcher_api.test_matcher_match_start(matcher)
spacy.tests.matcher.test_matcher_api.test_matcher_match_zero(matcher)
spacy.tests.matcher.test_matcher_api.test_matcher_match_zero_plus(matcher)
spacy.tests.matcher.test_matcher_api.test_matcher_min_max_operator(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_morph_handling(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_no_match(matcher)
spacy.tests.matcher.test_matcher_api.test_matcher_no_zero_length(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_operator_shadow(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_regex(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_regex_set_in(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_regex_set_not_in(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_regex_shape(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_remove_zero_operator(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_schema_token_attributes(en_vocab,pattern,text)
spacy.tests.matcher.test_matcher_api.test_matcher_set_value(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_set_value_operator(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_span(matcher)
spacy.tests.matcher.test_matcher_api.test_matcher_subset_value_operator(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_superset_value_operator(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_valid_callback(en_vocab)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/matcher/test_matcher_logic.py----------------------------------------
A:spacy.tests.matcher.test_matcher_logic.doc->Doc(matcher.vocab, words=list(string))
A:spacy.tests.matcher.test_matcher_logic.matcher->Matcher(en_vocab)
A:spacy.tests.matcher.test_matcher_logic.ents->list(doc.ents)
A:spacy.tests.matcher.test_matcher_logic.matches->matcher(doc, with_alignments=True)
A:spacy.tests.matcher.test_matcher_logic.entities->list(doc.ents)
A:spacy.tests.matcher.test_matcher_logic.vocab->Vocab(lex_attr_getters=LEX_ATTRS)
A:spacy.tests.matcher.test_matcher_logic.match->matcher(doc)
A:spacy.tests.matcher.test_matcher_logic.hello_world->Doc(vocab, words=['Hello', 'World'])
A:spacy.tests.matcher.test_matcher_logic.hello->Doc(vocab, words=['Hello'])
A:spacy.tests.matcher.test_matcher_logic.matched->sorted(matched, key=len, reverse=True)
A:spacy.tests.matcher.test_matcher_logic.nlp->English()
A:spacy.tests.matcher.test_matcher_logic.doc1->Doc(en_vocab, words=['a'])
A:spacy.tests.matcher.test_matcher_logic.doc2->Doc(en_vocab, words=['a', 'b', 'c'])
A:spacy.tests.matcher.test_matcher_logic.matches1->matcher(doc1)
A:spacy.tests.matcher.test_matcher_logic.matches2->matcher(doc2)
A:spacy.tests.matcher.test_matcher_logic.doc3->Doc(en_vocab, words=['a', 'b', 'b', 'c'])
A:spacy.tests.matcher.test_matcher_logic.doc4->Doc(en_vocab, words=['a', 'b', 'b', 'c'])
A:spacy.tests.matcher.test_matcher_logic.results1->matcher(nlp(text))
A:spacy.tests.matcher.test_matcher_logic.results2->matcher(nlp(text))
A:spacy.tests.matcher.test_matcher_logic.n_matches->len(matches)
spacy.tests.matcher.test_matcher_logic.doc(en_tokenizer,text)
spacy.tests.matcher.test_matcher_logic.test_greedy_matching_first(doc,text,pattern,re_pattern)
spacy.tests.matcher.test_matcher_logic.test_greedy_matching_longest(doc,text,pattern,longest)
spacy.tests.matcher.test_matcher_logic.test_greedy_matching_longest_first(en_tokenizer)
spacy.tests.matcher.test_matcher_logic.test_invalid_greediness(doc,text)
spacy.tests.matcher.test_matcher_logic.test_issue118(en_tokenizer,patterns)
spacy.tests.matcher.test_matcher_logic.test_issue118_prefix_reorder(en_tokenizer,patterns)
spacy.tests.matcher.test_matcher_logic.test_issue1434()
spacy.tests.matcher.test_matcher_logic.test_issue1450(string,start,end)
spacy.tests.matcher.test_matcher_logic.test_issue1945()
spacy.tests.matcher.test_matcher_logic.test_issue1971(en_vocab)
spacy.tests.matcher.test_matcher_logic.test_issue242(en_tokenizer)
spacy.tests.matcher.test_matcher_logic.test_issue2464(en_vocab)
spacy.tests.matcher.test_matcher_logic.test_issue2569(en_tokenizer)
spacy.tests.matcher.test_matcher_logic.test_issue2671()
spacy.tests.matcher.test_matcher_logic.test_issue3009(en_vocab)
spacy.tests.matcher.test_matcher_logic.test_issue3328(en_vocab)
spacy.tests.matcher.test_matcher_logic.test_issue3549(en_vocab)
spacy.tests.matcher.test_matcher_logic.test_issue3555(en_vocab)
spacy.tests.matcher.test_matcher_logic.test_issue3839(en_vocab)
spacy.tests.matcher.test_matcher_logic.test_issue3879(en_vocab)
spacy.tests.matcher.test_matcher_logic.test_issue3951(en_vocab)
spacy.tests.matcher.test_matcher_logic.test_issue4120(en_vocab)
spacy.tests.matcher.test_matcher_logic.test_issue587(en_tokenizer)
spacy.tests.matcher.test_matcher_logic.test_issue588(en_vocab)
spacy.tests.matcher.test_matcher_logic.test_issue590(en_vocab)
spacy.tests.matcher.test_matcher_logic.test_issue615(en_tokenizer)
spacy.tests.matcher.test_matcher_logic.test_issue850()
spacy.tests.matcher.test_matcher_logic.test_issue850_basic()
spacy.tests.matcher.test_matcher_logic.test_issue_1971_2(en_vocab)
spacy.tests.matcher.test_matcher_logic.test_issue_1971_3(en_vocab)
spacy.tests.matcher.test_matcher_logic.test_issue_1971_4(en_vocab)
spacy.tests.matcher.test_matcher_logic.test_match_consuming(doc,text,pattern,re_pattern)
spacy.tests.matcher.test_matcher_logic.test_matcher_end_zero_plus(en_vocab)
spacy.tests.matcher.test_matcher_logic.test_matcher_remove()
spacy.tests.matcher.test_matcher_logic.test_matcher_sets_return_correct_tokens(en_vocab)
spacy.tests.matcher.test_matcher_logic.test_matcher_with_alignments_greedy_longest(en_vocab)
spacy.tests.matcher.test_matcher_logic.test_matcher_with_alignments_non_greedy(en_vocab)
spacy.tests.matcher.test_matcher_logic.test_operator_combos(en_vocab)
spacy.tests.matcher.test_matcher_logic.text()


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/matcher/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/matcher/test_dependency_matcher.py----------------------------------------
A:spacy.tests.matcher.test_dependency_matcher.IS_BROWN_YELLOW->en_vocab.add_flag(is_brown_yellow)
A:spacy.tests.matcher.test_dependency_matcher.matcher->DependencyMatcher(en_tokenizer.vocab)
A:spacy.tests.matcher.test_dependency_matcher.mock->Mock()
A:spacy.tests.matcher.test_dependency_matcher.matches->matcher(doc)
A:spacy.tests.matcher.test_dependency_matcher.b->pickle.dumps(matcher)
A:spacy.tests.matcher.test_dependency_matcher.matcher_r->pickle.loads(b)
A:spacy.tests.matcher.test_dependency_matcher.pattern2->copy.deepcopy(pattern)
A:spacy.tests.matcher.test_dependency_matcher.matcher2->DependencyMatcher(en_vocab)
A:spacy.tests.matcher.test_dependency_matcher.matches2->matcher2(doc)
A:spacy.tests.matcher.test_dependency_matcher.doc->en_tokenizer('The red book')
A:spacy.tests.matcher.test_dependency_matcher.doc_matches->matcher(doc)
A:spacy.tests.matcher.test_dependency_matcher.span_matches->matcher(doc[offset:])
spacy.tests.matcher.test_dependency_matcher.dependency_matcher(en_vocab,patterns,doc)
spacy.tests.matcher.test_dependency_matcher.doc(en_vocab)
spacy.tests.matcher.test_dependency_matcher.patterns(en_vocab)
spacy.tests.matcher.test_dependency_matcher.test_dependency_matcher(dependency_matcher,doc,patterns)
spacy.tests.matcher.test_dependency_matcher.test_dependency_matcher_callback(en_vocab,doc)
spacy.tests.matcher.test_dependency_matcher.test_dependency_matcher_long_matches(en_vocab,doc)
spacy.tests.matcher.test_dependency_matcher.test_dependency_matcher_ops(en_vocab,doc,left,right,op,num_matches)
spacy.tests.matcher.test_dependency_matcher.test_dependency_matcher_order_issue(en_tokenizer)
spacy.tests.matcher.test_dependency_matcher.test_dependency_matcher_pattern_validation(en_vocab)
spacy.tests.matcher.test_dependency_matcher.test_dependency_matcher_pickle(en_vocab,patterns,doc)
spacy.tests.matcher.test_dependency_matcher.test_dependency_matcher_precedence_ops(en_vocab,op,num_matches)
spacy.tests.matcher.test_dependency_matcher.test_dependency_matcher_remove(en_tokenizer)
spacy.tests.matcher.test_dependency_matcher.test_dependency_matcher_span_user_data(en_tokenizer)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/vocab_vectors/test_vocab_api.py----------------------------------------
A:spacy.tests.vocab_vectors.test_vocab_api.vocab->Vocab()
A:spacy.tests.vocab_vectors.test_vocab_api.int_id->Vocab().strings.add('some string')
A:spacy.tests.vocab_vectors.test_vocab_api.nlp->English()
spacy.tests.vocab_vectors.test_vocab_api.test_issue1868()
spacy.tests.vocab_vectors.test_vocab_api.test_to_disk()
spacy.tests.vocab_vectors.test_vocab_api.test_to_disk_exclude()
spacy.tests.vocab_vectors.test_vocab_api.test_vocab_api_contains(en_vocab,text)
spacy.tests.vocab_vectors.test_vocab_api.test_vocab_api_eq(en_vocab,text)
spacy.tests.vocab_vectors.test_vocab_api.test_vocab_api_neq(en_vocab,text1,text2)
spacy.tests.vocab_vectors.test_vocab_api.test_vocab_api_shape_attr(en_vocab,text)
spacy.tests.vocab_vectors.test_vocab_api.test_vocab_api_symbols(en_vocab,string,symbol)
spacy.tests.vocab_vectors.test_vocab_api.test_vocab_writing_system(en_vocab)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/vocab_vectors/test_stringstore.py----------------------------------------
A:spacy.tests.vocab_vectors.test_stringstore.h->stringstore.add(heart)
A:spacy.tests.vocab_vectors.test_stringstore.apple_hash->stringstore.add('apple')
A:spacy.tests.vocab_vectors.test_stringstore.banana_hash->stringstore.add('banana')
A:spacy.tests.vocab_vectors.test_stringstore.key->stringstore.add(text)
A:spacy.tests.vocab_vectors.test_stringstore.store->stringstore.add(text)
A:spacy.tests.vocab_vectors.test_stringstore.serialized->stringstore.to_bytes()
A:spacy.tests.vocab_vectors.test_stringstore.new_stringstore->StringStore().from_bytes(serialized)
spacy.tests.vocab_vectors.test_stringstore.stringstore()
spacy.tests.vocab_vectors.test_stringstore.test_string_hash(stringstore)
spacy.tests.vocab_vectors.test_stringstore.test_stringstore_from_api_docs(stringstore)
spacy.tests.vocab_vectors.test_stringstore.test_stringstore_long_string(stringstore)
spacy.tests.vocab_vectors.test_stringstore.test_stringstore_massive_strings(stringstore)
spacy.tests.vocab_vectors.test_stringstore.test_stringstore_med_string(stringstore,text1,text2)
spacy.tests.vocab_vectors.test_stringstore.test_stringstore_multiply(stringstore,factor)
spacy.tests.vocab_vectors.test_stringstore.test_stringstore_retrieve_id(stringstore,text)
spacy.tests.vocab_vectors.test_stringstore.test_stringstore_save_bytes(stringstore,text1,text2,text3)
spacy.tests.vocab_vectors.test_stringstore.test_stringstore_save_unicode(stringstore,text1,text2,text3)
spacy.tests.vocab_vectors.test_stringstore.test_stringstore_to_bytes(stringstore,text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/vocab_vectors/test_lookups.py----------------------------------------
A:spacy.tests.vocab_vectors.test_lookups.lookups->Lookups()
A:spacy.tests.vocab_vectors.test_lookups.table->Vocab().lookups.get_table(table_name)
A:spacy.tests.vocab_vectors.test_lookups.table_bytes->Vocab().lookups.get_table(table_name).to_bytes()
A:spacy.tests.vocab_vectors.test_lookups.new_table->Table().from_bytes(table_bytes)
A:spacy.tests.vocab_vectors.test_lookups.new_table2->Table(data={'def': 456})
A:spacy.tests.vocab_vectors.test_lookups.lookups_bytes->Lookups().to_bytes()
A:spacy.tests.vocab_vectors.test_lookups.new_lookups->Lookups()
A:spacy.tests.vocab_vectors.test_lookups.table1->Lookups().get_table('table1')
A:spacy.tests.vocab_vectors.test_lookups.table2->Lookups().get_table('table2')
A:spacy.tests.vocab_vectors.test_lookups.vocab->Vocab()
A:spacy.tests.vocab_vectors.test_lookups.vocab_bytes->Vocab().to_bytes()
A:spacy.tests.vocab_vectors.test_lookups.new_vocab->Vocab()
spacy.tests.vocab_vectors.test_lookups.test_lookups_api()
spacy.tests.vocab_vectors.test_lookups.test_lookups_to_from_bytes()
spacy.tests.vocab_vectors.test_lookups.test_lookups_to_from_bytes_via_vocab()
spacy.tests.vocab_vectors.test_lookups.test_lookups_to_from_disk()
spacy.tests.vocab_vectors.test_lookups.test_lookups_to_from_disk_via_vocab()
spacy.tests.vocab_vectors.test_lookups.test_table_api()
spacy.tests.vocab_vectors.test_lookups.test_table_api_to_from_bytes()


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/vocab_vectors/test_vectors.py----------------------------------------
A:spacy.tests.vocab_vectors.test_vectors.OPS->get_current_ops()
A:spacy.tests.vocab_vectors.test_vectors.vectors->Vectors(data=data, keys=['A', 'B', 'C'])
A:spacy.tests.vocab_vectors.test_vectors.v->Vectors(data=data, keys=['a1', 'b1', 'c1', 'a2', 'c2'])
A:spacy.tests.vocab_vectors.test_vectors.vocab->Vocab()
A:spacy.tests.vocab_vectors.test_vectors.vector_data->numpy.zeros((3, 10), dtype='f')
A:spacy.tests.vocab_vectors.test_vectors.data->get_current_ops().asarray([[1, 1], [2, 2], [3, 4], [1, 1], [3, 4]], dtype='f')
A:spacy.tests.vocab_vectors.test_vectors.(keys, best_rows, scores)->Vectors(data=data, keys=['A', 'B', 'C']).most_similar(numpy.asarray([[9, 8, 7], [0, 0, 0]], dtype='f'))
A:spacy.tests.vocab_vectors.test_vectors.nlp->English()
A:spacy.tests.vocab_vectors.test_vectors.orig->get_current_ops().asarray([[1, 1], [2, 2], [3, 4], [1, 1], [3, 4]], dtype='f').copy()
A:spacy.tests.vocab_vectors.test_vectors.(_, best_rows, _)->Vectors(data=data, keys=['a1', 'b1', 'c1', 'a2', 'c2']).most_similar(v.data, batch_size=2, n=2, sort=True)
A:spacy.tests.vocab_vectors.test_vectors.(keys, _, scores)->Vectors(data=data, keys=['a1', 'b1', 'c1', 'a2', 'c2']).most_similar(numpy.asarray([[1, 2, 3]], dtype='f'))
A:spacy.tests.vocab_vectors.test_vectors.doc->Doc(vocab, words=text)
A:spacy.tests.vocab_vectors.test_vectors.token->tokenizer_v(text1)
A:spacy.tests.vocab_vectors.test_vectors.doc1->Doc(vocab, words=text1)
A:spacy.tests.vocab_vectors.test_vectors.doc2->Doc(vocab, words=text2)
A:spacy.tests.vocab_vectors.test_vectors.data[0]->get_current_ops().asarray([1.0, 1.2, 1.1])
A:spacy.tests.vocab_vectors.test_vectors.data[1]->get_current_ops().asarray([0.3, 1.3, 1.0])
A:spacy.tests.vocab_vectors.test_vectors.data[2]->get_current_ops().asarray([0.9, 1.22, 1.05])
A:spacy.tests.vocab_vectors.test_vectors.remap->Vocab().prune_vectors(2, batch_size=2)
A:spacy.tests.vocab_vectors.test_vectors.cosine->get_cosine(data[0], data[2])
A:spacy.tests.vocab_vectors.test_vectors.b->Vectors(data=data, keys=['a1', 'b1', 'c1', 'a2', 'c2']).to_bytes()
A:spacy.tests.vocab_vectors.test_vectors.v_r->Vectors()
A:spacy.tests.vocab_vectors.test_vectors.row->Vectors(data=data, keys=['a1', 'b1', 'c1', 'a2', 'c2']).add('D', vector=OPS.asarray([10, 20, 30, 40], dtype='f'))
A:spacy.tests.vocab_vectors.test_vectors.row_r->Vectors().add('D', vector=OPS.asarray([10, 20, 30, 40], dtype='f'))
A:spacy.tests.vocab_vectors.test_vectors.rows->get_current_ops().xp.asarray([h % nlp.vocab.vectors.shape[0] for ngram in ngrams for h in nlp.vocab.vectors._get_ngram_hashes(ngram)], dtype='uint32')
A:spacy.tests.vocab_vectors.test_vectors.vecs->get_current_ops().as_contig(v.data[rows])
A:spacy.tests.vocab_vectors.test_vectors.vocab_b->Vocab().to_bytes()
A:spacy.tests.vocab_vectors.test_vectors.nlp_plain->English()
A:spacy.tests.vocab_vectors.test_vectors.ngrams->English().vocab.vectors._get_ngrams(word)
A:spacy.tests.vocab_vectors.test_vectors.word->English().vocab.strings.as_string(word)
A:spacy.tests.vocab_vectors.test_vectors.single_vecs->get_current_ops().to_numpy(OPS.asarray([nlp.vocab[word].vector for word in words]))
A:spacy.tests.vocab_vectors.test_vectors.batch_vecs->get_current_ops().to_numpy(nlp.vocab.vectors.get_batch(words))
A:spacy.tests.vocab_vectors.test_vectors.vector->list(range(nlp.vocab.vectors.shape[1]))
A:spacy.tests.vocab_vectors.test_vectors.orig_bytes->English().vocab.vectors.to_bytes(exclude=['strings'])
A:spacy.tests.vocab_vectors.test_vectors.vocab_r->Vocab()
A:spacy.tests.vocab_vectors.test_vectors.vectors1->Vectors(shape=(10, 10))
A:spacy.tests.vocab_vectors.test_vectors.vectors2->Vectors(shape=(10, 10))
spacy.tests.vocab_vectors.test_vectors.data()
spacy.tests.vocab_vectors.test_vectors.floret_vectors_hashvec_str()
spacy.tests.vocab_vectors.test_vectors.floret_vectors_vec_str()
spacy.tests.vocab_vectors.test_vectors.most_similar_vectors_data()
spacy.tests.vocab_vectors.test_vectors.most_similar_vectors_keys()
spacy.tests.vocab_vectors.test_vectors.resize_data()
spacy.tests.vocab_vectors.test_vectors.strings()
spacy.tests.vocab_vectors.test_vectors.test_equality()
spacy.tests.vocab_vectors.test_vectors.test_floret_vectors(floret_vectors_vec_str,floret_vectors_hashvec_str)
spacy.tests.vocab_vectors.test_vectors.test_get_vector(strings,data)
spacy.tests.vocab_vectors.test_vectors.test_get_vector_resize(strings,data)
spacy.tests.vocab_vectors.test_vectors.test_init_vectors_unset()
spacy.tests.vocab_vectors.test_vectors.test_init_vectors_with_data(strings,data)
spacy.tests.vocab_vectors.test_vectors.test_init_vectors_with_resize_data(data,resize_data)
spacy.tests.vocab_vectors.test_vectors.test_init_vectors_with_resize_shape(strings,resize_data)
spacy.tests.vocab_vectors.test_vectors.test_init_vectors_with_shape(strings)
spacy.tests.vocab_vectors.test_vectors.test_issue1518()
spacy.tests.vocab_vectors.test_vectors.test_issue1539()
spacy.tests.vocab_vectors.test_vectors.test_issue1807()
spacy.tests.vocab_vectors.test_vectors.test_issue2871()
spacy.tests.vocab_vectors.test_vectors.test_issue3412()
spacy.tests.vocab_vectors.test_vectors.test_issue4725_2()
spacy.tests.vocab_vectors.test_vectors.test_set_vector(strings,data)
spacy.tests.vocab_vectors.test_vectors.test_vector_is_oov()
spacy.tests.vocab_vectors.test_vectors.test_vectors_clear()
spacy.tests.vocab_vectors.test_vectors.test_vectors_deduplicate()
spacy.tests.vocab_vectors.test_vectors.test_vectors_doc_doc_similarity(vocab,text1,text2)
spacy.tests.vocab_vectors.test_vectors.test_vectors_doc_vector(vocab,text)
spacy.tests.vocab_vectors.test_vectors.test_vectors_get_batch()
spacy.tests.vocab_vectors.test_vectors.test_vectors_lexeme_doc_similarity(vocab,text)
spacy.tests.vocab_vectors.test_vectors.test_vectors_lexeme_lexeme_similarity(vocab,text1,text2)
spacy.tests.vocab_vectors.test_vectors.test_vectors_lexeme_span_similarity(vocab,text)
spacy.tests.vocab_vectors.test_vectors.test_vectors_lexeme_vector(vocab,text)
spacy.tests.vocab_vectors.test_vectors.test_vectors_most_similar(most_similar_vectors_data,most_similar_vectors_keys)
spacy.tests.vocab_vectors.test_vectors.test_vectors_most_similar_identical()
spacy.tests.vocab_vectors.test_vectors.test_vectors_serialize()
spacy.tests.vocab_vectors.test_vectors.test_vectors_span_doc_similarity(vocab,text)
spacy.tests.vocab_vectors.test_vectors.test_vectors_span_span_similarity(vocab,text)
spacy.tests.vocab_vectors.test_vectors.test_vectors_span_vector(vocab,text)
spacy.tests.vocab_vectors.test_vectors.test_vectors_token_doc_similarity(vocab,text)
spacy.tests.vocab_vectors.test_vectors.test_vectors_token_lexeme_similarity(tokenizer_v,vocab,text1,text2)
spacy.tests.vocab_vectors.test_vectors.test_vectors_token_span_similarity(vocab,text)
spacy.tests.vocab_vectors.test_vectors.test_vectors_token_token_similarity(tokenizer_v,text)
spacy.tests.vocab_vectors.test_vectors.test_vectors_token_vector(tokenizer_v,vectors,text)
spacy.tests.vocab_vectors.test_vectors.test_vocab_add_vector()
spacy.tests.vocab_vectors.test_vectors.test_vocab_prune_vectors()
spacy.tests.vocab_vectors.test_vectors.tokenizer_v(vocab)
spacy.tests.vocab_vectors.test_vectors.vectors()
spacy.tests.vocab_vectors.test_vectors.vocab(en_vocab,vectors)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/vocab_vectors/test_lexeme.py----------------------------------------
A:spacy.tests.vocab_vectors.test_lexeme.vocab->Vocab(tag_map={'NN': {'pos': 'NOUN'}})
A:spacy.tests.vocab_vectors.test_lexeme.doc->Doc(vocab, words=['hello'])
A:spacy.tests.vocab_vectors.test_lexeme.is_len4->en_vocab.add_flag(lambda string: len(string) == 4, flag_id=IS_DIGIT)
spacy.tests.vocab_vectors.test_lexeme.test_issue361(en_vocab,text1,text2)
spacy.tests.vocab_vectors.test_lexeme.test_issue600()
spacy.tests.vocab_vectors.test_lexeme.test_vocab_lexeme_add_flag_auto_id(en_vocab)
spacy.tests.vocab_vectors.test_lexeme.test_vocab_lexeme_add_flag_provided_id(en_vocab)
spacy.tests.vocab_vectors.test_lexeme.test_vocab_lexeme_hash(en_vocab,text1,text2)
spacy.tests.vocab_vectors.test_lexeme.test_vocab_lexeme_is_alpha(en_vocab)
spacy.tests.vocab_vectors.test_lexeme.test_vocab_lexeme_is_digit(en_vocab)
spacy.tests.vocab_vectors.test_lexeme.test_vocab_lexeme_lt(en_vocab,text1,text2,prob1,prob2)
spacy.tests.vocab_vectors.test_lexeme.test_vocab_lexeme_oov_rank(en_vocab)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/vocab_vectors/test_similarity.py----------------------------------------
A:spacy.tests.vocab_vectors.test_similarity.doc->Doc(vocab, words=[word1, word2])
A:spacy.tests.vocab_vectors.test_similarity.doc1->Doc(vocab, words=['a', 'b'])
A:spacy.tests.vocab_vectors.test_similarity.doc2->Doc(vocab, words=['c', 'd', 'e'])
A:spacy.tests.vocab_vectors.test_similarity.vocab->Vocab()
spacy.tests.vocab_vectors.test_similarity.test_issue2219(en_vocab)
spacy.tests.vocab_vectors.test_similarity.test_vectors_similarity_DD(vocab,vectors)
spacy.tests.vocab_vectors.test_similarity.test_vectors_similarity_DS(vocab,vectors)
spacy.tests.vocab_vectors.test_similarity.test_vectors_similarity_LL(vocab,vectors)
spacy.tests.vocab_vectors.test_similarity.test_vectors_similarity_SS(vocab,vectors)
spacy.tests.vocab_vectors.test_similarity.test_vectors_similarity_TD(vocab,vectors)
spacy.tests.vocab_vectors.test_similarity.test_vectors_similarity_TS(vocab,vectors)
spacy.tests.vocab_vectors.test_similarity.test_vectors_similarity_TT(vocab,vectors)
spacy.tests.vocab_vectors.test_similarity.test_vectors_similarity_no_vectors()
spacy.tests.vocab_vectors.test_similarity.vectors()
spacy.tests.vocab_vectors.test_similarity.vocab(en_vocab,vectors)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tests/vocab_vectors/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/displacy/render.py----------------------------------------
A:spacy.displacy.render.colors->dict(DEFAULT_LABEL_COLORS)
A:spacy.displacy.render.user_colors->util.registry.displacy_colors.get_all()
A:spacy.displacy.render.user_color->user_color()
A:spacy.displacy.render.self.top_offset->options.get('top_offset', 40)
A:spacy.displacy.render.self.span_label_offset->options.get('span_label_offset', 20)
A:spacy.displacy.render.self.offset_step->options.get('top_offset_step', 17)
A:spacy.displacy.render.template->options.get('template')
A:spacy.displacy.render.settings->p.get('settings', {})
A:spacy.displacy.render.self.direction->p.get('settings', {}).get('direction', DEFAULT_DIR)
A:spacy.displacy.render.self.lang->p.get('settings', {}).get('lang', DEFAULT_LANG)
A:spacy.displacy.render.docs->''.join([TPL_FIGURE.format(content=doc) for doc in rendered])
A:spacy.displacy.render.markup->templates.TPL_ENTS.format(content=markup, dir=self.direction)
A:spacy.displacy.render.spans->sorted(spans, key=lambda s: (s['start_token'], -(s['end_token'] - s['start_token']), s['label']))
A:spacy.displacy.render.kb_id->span.get('kb_id', '')
A:spacy.displacy.render.kb_url->span.get('kb_url', '#')
A:spacy.displacy.render.entities->sorted(token['entities'], key=lambda d: d['render_slot'])
A:spacy.displacy.render.slices->self._get_span_slices(token['entities'])
A:spacy.displacy.render.starts->self._get_span_starts(token['entities'])
A:spacy.displacy.render.color->self.colors.get(label.upper(), self.default_color)
A:spacy.displacy.render.span_slice->self.span_slice_template.format(bg=color, top_offset=top_offset)
A:spacy.displacy.render.self.compact->options.get('compact', False)
A:spacy.displacy.render.self.word_spacing->options.get('word_spacing', 45)
A:spacy.displacy.render.self.arrow_spacing->options.get('arrow_spacing', 12 if self.compact else 20)
A:spacy.displacy.render.self.arrow_width->options.get('arrow_width', 6 if self.compact else 10)
A:spacy.displacy.render.self.arrow_stroke->options.get('arrow_stroke', 2)
A:spacy.displacy.render.self.distance->options.get('distance', 150 if self.compact else 175)
A:spacy.displacy.render.self.offset_x->options.get('offset_x', 50)
A:spacy.displacy.render.self.color->options.get('color', '#000000')
A:spacy.displacy.render.self.bg->options.get('bg', '#ffffff')
A:spacy.displacy.render.self.font->options.get('font', 'Arial')
A:spacy.displacy.render.svg->self.render_svg(render_id, p['words'], p['arcs'])
A:spacy.displacy.render.content->''.join([TPL_FIGURE.format(content=svg) for svg in rendered])
A:spacy.displacy.render.self.levels->self.get_levels(arcs)
A:spacy.displacy.render.self.highest_level->max(self.levels.values(), default=0)
A:spacy.displacy.render.html_text->escape_html(text)
A:spacy.displacy.render.error_args->dict(start=start, end=end, label=label, dir=direction)
A:spacy.displacy.render.arrowhead->self.get_arrowhead(direction, x_start, y, x_end)
A:spacy.displacy.render.arc->self.get_arc(x_start, y, y_curve, x_end)
A:spacy.displacy.render.length->max([arc['end'] for arc in arcs], default=0)
A:spacy.displacy.render.self.ents->options.get('ents', None)
A:spacy.displacy.render.additional_params->span.get('params', {})
A:spacy.displacy.render.entity->escape_html(text[start:end])
A:spacy.displacy.render.fragments->text[offset:].split('\n')
spacy.displacy.DependencyRenderer(self,options:Dict[str,Any]={})
spacy.displacy.EntityRenderer(self,options:Dict[str,Any]={})
spacy.displacy.SpanRenderer(self,options:Dict[str,Any]={})
spacy.displacy.render.DependencyRenderer(self,options:Dict[str,Any]={})
spacy.displacy.render.DependencyRenderer.get_arc(self,x_start:int,y:int,y_curve:int,x_end:int)->str
spacy.displacy.render.DependencyRenderer.get_arrowhead(self,direction:str,x:int,y:int,end:int)->str
spacy.displacy.render.DependencyRenderer.get_levels(self,arcs:List[Dict[str,Any]])->Dict[Tuple[int, int, str], int]
spacy.displacy.render.DependencyRenderer.render(self,parsed:List[Dict[str,Any]],page:bool=False,minify:bool=False)->str
spacy.displacy.render.DependencyRenderer.render_arrow(self,label:str,start:int,end:int,direction:str,i:int)->str
spacy.displacy.render.DependencyRenderer.render_svg(self,render_id:Union[int,str],words:List[Dict[str,Any]],arcs:List[Dict[str,Any]])->str
spacy.displacy.render.DependencyRenderer.render_word(self,text:str,tag:str,lemma:str,i:int)->str
spacy.displacy.render.EntityRenderer(self,options:Dict[str,Any]={})
spacy.displacy.render.EntityRenderer.render(self,parsed:List[Dict[str,Any]],page:bool=False,minify:bool=False)->str
spacy.displacy.render.EntityRenderer.render_ents(self,text:str,spans:List[Dict[str,Any]],title:Optional[str])->str
spacy.displacy.render.SpanRenderer(self,options:Dict[str,Any]={})
spacy.displacy.render.SpanRenderer._get_span_slices(self,entities:List[Dict])->str
spacy.displacy.render.SpanRenderer._get_span_starts(self,entities:List[Dict])->str
spacy.displacy.render.SpanRenderer._render_markup(self,per_token_info:List[Dict[str,Any]])->str
spacy.displacy.render.SpanRenderer.render(self,parsed:List[Dict[str,Any]],page:bool=False,minify:bool=False)->str
spacy.displacy.render.SpanRenderer.render_spans(self,tokens:List[str],spans:List[Dict[str,Any]],title:Optional[str])->str


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/displacy/templates.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/displacy/__init__.py----------------------------------------
A:spacy.displacy.__init__.renderer->renderer_func(options=options)
A:spacy.displacy.__init__.doc['ents']->sorted(doc['ents'], key=lambda x: (x['start'], x['end']))
A:spacy.displacy.__init__._html['parsed']->renderer_func(options=options).render(parsed, page=page, minify=minify).strip()
A:spacy.displacy.__init__.html->RENDER_WRAPPER(html)
A:spacy.displacy.__init__.port->find_available_port(port, host, auto_select_port)
A:spacy.displacy.__init__.httpd->wsgiref.simple_server.make_server(host, port, app)
A:spacy.displacy.__init__.res->renderer_func(options=options).render(parsed, page=page, minify=minify).strip().encode(encoding='utf-8')
A:spacy.displacy.__init__.doc->Doc(orig_doc.vocab).from_bytes(orig_doc.to_bytes(exclude=['user_data', 'user_hooks']))
A:spacy.displacy.__init__.fine_grained->options.get('fine_grained')
A:spacy.displacy.__init__.add_lemma->options.get('add_lemma')
A:spacy.displacy.__init__.kb_url_template->options.get('kb_url_template', None)
A:spacy.displacy.__init__.settings->get_doc_settings(doc)
A:spacy.displacy.__init__.spans_key->options.get('spans_key', 'sc')
A:spacy.displacy.__init__.keys->list(doc.spans.keys())
spacy.displacy.__init__.app(environ,start_response)
spacy.displacy.__init__.get_doc_settings(doc:Doc)->Dict[str, Any]
spacy.displacy.__init__.parse_deps(orig_doc:Doc,options:Dict[str,Any]={})->Dict[str, Any]
spacy.displacy.__init__.parse_ents(doc:Doc,options:Dict[str,Any]={})->Dict[str, Any]
spacy.displacy.__init__.parse_spans(doc:Doc,options:Dict[str,Any]={})->Dict[str, Any]
spacy.displacy.__init__.render(docs:Union[Iterable[Union[Doc,Span,dict]],Doc,Span,dict],style:str='dep',page:bool=False,minify:bool=False,jupyter:Optional[bool]=None,options:Dict[str,Any]={},manual:bool=False)->str
spacy.displacy.__init__.serve(docs:Union[Iterable[Doc],Doc],style:str='dep',page:bool=True,minify:bool=False,options:Dict[str,Any]={},manual:bool=False,port:int=5000,host:str='0.0.0.0',auto_select_port:bool=False)->None
spacy.displacy.__init__.set_render_wrapper(func:Callable[[str],str])->None


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/norm_exceptions.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.tokenizer_exceptions.URL_PATTERN->('^(?:(?:[\\w\\+\\-\\.]{2,})://)?(?:\\S+(?::\\S*)?@)?(?:(?!(?:10|127)(?:\\.\\d{1,3}){3})(?!(?:169\\.254|192\\.168)(?:\\.\\d{1,3}){2})(?!172\\.(?:1[6-9]|2\\d|3[0-1])(?:\\.\\d{1,3}){2})(?:[1-9]\\d?|1\\d\\d|2[01]\\d|22[0-3])(?:\\.(?:1?\\d{1,2}|2[0-4]\\d|25[0-5])){2}(?:\\.(?:[1-9]\\d?|1\\d\\d|2[0-4]\\d|25[0-4]))|(?:(?:[A-Za-z0-9\\u00a1-\\uffff][A-Za-z0-9\\u00a1-\\uffff_-]{0,62})?[A-Za-z0-9\\u00a1-\\uffff]\\.)+(?:[' + ALPHA_LOWER + ']{2,63}))(?::\\d{2,5})?(?:[/?#]\\S*)?$').strip()
A:spacy.lang.tokenizer_exceptions.emoticons->set("\n:)\n:-)\n:))\n:-))\n:)))\n:-)))\n(:\n(-:\n=)\n(=\n:]\n:-]\n[:\n[-:\n[=\n=]\n:o)\n(o:\n:}\n:-}\n8)\n8-)\n(-8\n;)\n;-)\n(;\n(-;\n:(\n:-(\n:((\n:-((\n:(((\n:-(((\n):\n)-:\n=(\n>:(\n:')\n:'-)\n:'(\n:'-(\n:/\n:-/\n=/\n=|\n:|\n:-|\n]=\n=[\n:1\n:P\n:-P\n:p\n:-p\n:O\n:-O\n:o\n:-o\n:0\n:-0\n:()\n>:o\n:*\n:-*\n:3\n:-3\n=3\n:>\n:->\n:X\n:-X\n:x\n:-x\n:D\n:-D\n;D\n;-D\n=D\nxD\nXD\nxDD\nXDD\n8D\n8-D\n\n^_^\n^__^\n^___^\n>.<\n>.>\n<.<\n._.\n;_;\n-_-\n-__-\nv.v\nV.V\nv_v\nV_V\no_o\no_O\nO_o\nO_O\n0_o\no_0\n0_0\no.O\nO.o\nO.O\no.o\n0.0\no.0\n0.o\n@_@\n<3\n<33\n<333\n</3\n(^_^)\n(-_-)\n(._.)\n(>_<)\n(*_*)\n(Â¬_Â¬)\nà² _à² \nà² ï¸µà² \n(à² _à² )\nÂ¯\\(ãƒ„)/Â¯\n(â•¯Â°â–¡Â°ï¼‰â•¯ï¸µâ”»â”â”»\n><(((*>\n".split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/lex_attrs.py----------------------------------------
A:spacy.lang.lex_attrs._tlds->set('com|org|edu|gov|net|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|pro|tel|travel|xyz|icu|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|sk|sl|sm|sn|so|sr|ss|st|su|sv|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|za|zm|zw'.split('|'))
A:spacy.lang.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.lex_attrs.get_lang(text:str,lang:str='')->str
spacy.lang.lex_attrs.is_alpha(string:str)->bool
spacy.lang.lex_attrs.is_ascii(text:str)->bool
spacy.lang.lex_attrs.is_bracket(text:str)->bool
spacy.lang.lex_attrs.is_currency(text:str)->bool
spacy.lang.lex_attrs.is_digit(string:str)->bool
spacy.lang.lex_attrs.is_left_punct(text:str)->bool
spacy.lang.lex_attrs.is_lower(string:str)->bool
spacy.lang.lex_attrs.is_punct(text:str)->bool
spacy.lang.lex_attrs.is_quote(text:str)->bool
spacy.lang.lex_attrs.is_right_punct(text:str)->bool
spacy.lang.lex_attrs.is_space(string:str)->bool
spacy.lang.lex_attrs.is_stop(string:str,stops:Set[str]=set())->bool
spacy.lang.lex_attrs.is_title(string:str)->bool
spacy.lang.lex_attrs.is_upper(string:str)->bool
spacy.lang.lex_attrs.like_email(text:str)->bool
spacy.lang.lex_attrs.like_num(text:str)->bool
spacy.lang.lex_attrs.like_url(text:str)->bool
spacy.lang.lex_attrs.lower(string:str)->str
spacy.lang.lex_attrs.prefix(string:str)->str
spacy.lang.lex_attrs.suffix(string:str)->str
spacy.lang.lex_attrs.word_shape(text:str)->str


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/char_classes.py----------------------------------------
A:spacy.lang.char_classes.ALPHA->group_chars(LATIN + _russian + _tatar + _greek + _ukrainian + _macedonian + _uncased)
A:spacy.lang.char_classes.ALPHA_LOWER->group_chars(_lower + _uncased)
A:spacy.lang.char_classes.ALPHA_UPPER->group_chars(_upper + _uncased)
A:spacy.lang.char_classes.UNITS->merge_chars(_units)
A:spacy.lang.char_classes.CURRENCY->merge_chars(_currency)
A:spacy.lang.char_classes.PUNCT->merge_chars(_punct)
A:spacy.lang.char_classes.HYPHENS->merge_chars(_hyphens)
A:spacy.lang.char_classes.LIST_UNITS->split_chars(_units)
A:spacy.lang.char_classes.LIST_CURRENCY->split_chars(_currency)
A:spacy.lang.char_classes.LIST_QUOTES->split_chars(_quotes)
A:spacy.lang.char_classes.LIST_PUNCT->split_chars(_punct)
A:spacy.lang.char_classes.LIST_HYPHENS->split_chars(_hyphens)
A:spacy.lang.char_classes.CONCAT_QUOTES->group_chars(_quotes)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/punctuation.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/is/stop_words.py----------------------------------------
A:spacy.lang.is.stop_words.STOP_WORDS->set('\nafhverju\naftan\naftur\nafÃ¾vÃ­\naldrei\nallir\nallt\nalveg\nannaÃ°\nannars\nbara\ndag\neÃ°a\neftir\neiga\neinhver\neinhverjir\neinhvers\neins\neinu\neitthvaÃ°\nekkert\nekki\nennÃ¾Ã¡\neru\nfara\nfer\nfinna\nfjÃ¶ldi\nfÃ³lk\nframan\nfrÃ¡\nfrekar\nfyrir\ngegnum\ngeta\ngetur\ngmg\ngott\nhann\nhafa\nhef\nhefur\nheyra\nhÃ©r\nhÃ©rna\nhjÃ¡\nhÃºn\nhvaÃ°\nhvar\nhver\nhverjir\nhverjum\nhvernig\nhvor\nhvort\nhÃ¦gt\nimg\ninn\nkannski\nkoma\nlÃ­ka\nlol\nmaÃ°ur\nmÃ¡tt\nmÃ©r\nmeÃ°\nmega\nmeira\nmig\nmikiÃ°\nminna\nminni\nmissa\nmjÃ¶g\nnei\nniÃ°ur\nnÃºna\noft\nokkar\nokkur\npÃ³st\npÃ³stur\nrofl\nsaman\nsem\nsÃ©r\nsig\nsinni\nsÃ­Ã°an\nsjÃ¡\nsmÃ¡\nsmÃ¡tt\nspurja\nspyrja\nstaÃ°ar\nstÃ³rt\nsvo\nsvona\nsÃ¦lir\nsÃ¦ll\ntaka\ntakk\ntil\ntilvitnun\ntitlar\nupp\nvar\nvel\nvelkomin\nvelkominn\nvera\nverÃ°ur\nveriÃ°\nvel\nviÃ°\nvil\nvilja\nvill\nvita\nvÃ¦ri\nyfir\nykkar\nÃ¾aÃ°\nÃ¾akka\nÃ¾akkir\nÃ¾annig\nÃ¾aÃ°\nÃ¾ar\nÃ¾arf\nÃ¾au\nÃ¾eim\nÃ¾eir\nÃ¾eirra\nÃ¾eirra\nÃ¾egar\nÃ¾ess\nÃ¾essa\nÃ¾essi\nÃ¾essu\nÃ¾essum\nÃ¾etta\nÃ¾Ã©r\nÃ¾iÃ°\nÃ¾inn\nÃ¾itt\nÃ¾Ã­n\nÃ¾rÃ¡Ã°\nÃ¾rÃ¡Ã°ur\nÃ¾vÃ­\nÃ¾Ã¦r\nÃ¦tti\n'.split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/is/__init__.py----------------------------------------
spacy.lang.is.__init__.Icelandic(Language)
spacy.lang.is.__init__.IcelandicDefaults(BaseDefaults)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/lg/examples.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/lg/stop_words.py----------------------------------------
A:spacy.lang.lg.stop_words.STOP_WORDS->set('\nabadde abalala abamu abangi abava ajja ali alina ani anti ateekeddwa atewamu\natya awamu aweebwa ayinza ba baali babadde babalina bajja\nbajjanewankubade bali balina bandi bangi bano bateekeddwa baweebwa bayina bebombi beera bibye\nbimu bingi bino bo bokka bonna buli bulijjo bulungi bwabwe bwaffe bwayo bwe bwonna bya byabwe\nbyaffe byebimu byonna ddaa ddala ddi e ebimu ebiri ebweruobulungi ebyo edda ejja ekirala ekyo\nendala engeri ennyo era erimu erina ffe ffenna ga gujja gumu gunno guno gwa gwe kaseera kati\nkennyini ki kiki kikino kikye kikyo kino kirungi kki ku kubangabyombi kubangaolwokuba kudda\nkuva kuwa kwegamba kyaffe kye kyekimuoyo kyekyo kyonna leero liryo lwa lwaki lyabwezaabwe\nlyaffe lyange mbadde mingi mpozzi mu mulinaoyina munda mwegyabwe nolwekyo nabadde nabo nandiyagadde\nnandiye nanti naye ne nedda neera nga nnyingi nnyini nnyinza nnyo nti nyinza nze oba ojja okudda\nokugenda okuggyako okutuusa okuva okuwa oli olina oluvannyuma olwekyobuva omuli ono osobola otya\noyina oyo seetaaga si sinakindi singa talina tayina tebaali tebaalina tebayina terina tetulina\ntetuteekeddwa tewali teyalina teyayina tolina tu tuyina tulina tuyina twafuna twetaaga wa wabula\nwabweru wadde waggulunnina wakati waliwobangi waliyo wandi wange wano wansi weebwa yabadde yaffe\nye yenna yennyini yina yonna ziba zijja zonna\n'.split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/lg/__init__.py----------------------------------------
spacy.lang.lg.__init__.Luganda(Language)
spacy.lang.lg.__init__.LugandaDefaults(BaseDefaults)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/lg/lex_attrs.py----------------------------------------
A:spacy.lang.lg.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.lg.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
A:spacy.lang.lg.lex_attrs.text_lower->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').lower()
spacy.lang.lg.lex_attrs.like_num(text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/lg/punctuation.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/xx/examples.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/xx/__init__.py----------------------------------------
spacy.lang.xx.__init__.MultiLanguage(Language)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/th/stop_words.py----------------------------------------
A:spacy.lang.th.stop_words.STOP_WORDS->set('\nà¸—à¸±à¹‰à¸‡à¸™à¸µà¹‰ à¸”à¸±à¸‡ à¸‚à¸­ à¸£à¸§à¸¡ à¸«à¸¥à¸±à¸‡à¸ˆà¸²à¸ à¹€à¸›à¹‡à¸™ à¸«à¸¥à¸±à¸‡ à¸«à¸£à¸·à¸­ à¹† à¹€à¸à¸µà¹ˆà¸¢à¸§à¸à¸±à¸š à¸‹à¸¶à¹ˆà¸‡à¹„à¸”à¹‰à¹à¸à¹ˆ à¸”à¹‰à¸§à¸¢à¹€à¸žà¸£à¸²à¸° à¸”à¹‰à¸§à¸¢à¸§à¹ˆà¸² à¸”à¹‰à¸§à¸¢à¹€à¸«à¸•à¸¸à¹€à¸žà¸£à¸²à¸°\nà¸”à¹‰à¸§à¸¢à¹€à¸«à¸•à¸¸à¸§à¹ˆà¸² à¸ªà¸¸à¸”à¹† à¹€à¸ªà¸£à¹‡à¸ˆà¹à¸¥à¹‰à¸§ à¹€à¸Šà¹ˆà¸™ à¹€à¸‚à¹‰à¸² à¸–à¹‰à¸² à¸–à¸¹à¸ à¸–à¸¶à¸‡ à¸•à¹ˆà¸²à¸‡à¹† à¹ƒà¸„à¸£ à¹€à¸›à¸´à¸”à¹€à¸œà¸¢ à¸„à¸£à¸² à¸£à¸·à¸­ à¸•à¸²à¸¡ à¹ƒà¸™ à¹„à¸”à¹‰à¹à¸à¹ˆ à¹„à¸”à¹‰à¹à¸•à¹ˆ\nà¹„à¸”à¹‰à¸—à¸µà¹ˆ à¸•à¸¥à¸­à¸”à¸–à¸¶à¸‡ à¸™à¸­à¸à¸ˆà¸²à¸à¸§à¹ˆà¸² à¸™à¸­à¸à¸™à¸±à¹‰à¸™ à¸ˆà¸£à¸´à¸‡ à¸­à¸¢à¹ˆà¸²à¸‡à¸”à¸µ à¸ªà¹ˆà¸§à¸™ à¹€à¸žà¸µà¸¢à¸‡à¹€à¸žà¸·à¹ˆà¸­ à¹€à¸”à¸µà¸¢à¸§ à¸ˆà¸±à¸” à¸—à¸±à¹‰à¸‡à¸—à¸µ à¸—à¸±à¹‰à¸‡à¸„à¸™ à¸—à¸±à¹‰à¸‡à¸•à¸±à¸§ à¹„à¸à¸¥à¹†\nà¸–à¸¶à¸‡à¹€à¸¡à¸·à¹ˆà¸­à¹ƒà¸” à¸„à¸‡à¸ˆà¸° à¸–à¸¹à¸à¹† à¹€à¸›à¹‡à¸™à¸—à¸µ à¸™à¸±à¸šà¹à¸•à¹ˆà¸—à¸µà¹ˆ à¸™à¸±à¸šà¹à¸•à¹ˆà¸™à¸±à¹‰à¸™ à¸£à¸±à¸šà¸£à¸­à¸‡ à¸”à¹‰à¸²à¸™ à¹€à¸›à¹‡à¸™à¸•à¹‰à¸™à¸¡à¸² à¸—à¸¸à¸ à¸à¸£à¸°à¸—à¸±à¹ˆà¸‡ à¸à¸£à¸°à¸—à¸³ à¸ˆà¸§à¸š à¸‹à¸¶à¹ˆà¸‡à¸à¹‡ à¸ˆà¸°\nà¸„à¸£à¸šà¸„à¸£à¸±à¸™ à¸™à¸±à¸šà¹à¸•à¹ˆ à¹€à¸¢à¸­à¸°à¹† à¹€à¸žà¸µà¸¢à¸‡à¹„à¸«à¸™ à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™à¹à¸›à¸¥à¸‡ à¹„à¸›à¹ˆ à¸œà¹ˆà¸²à¸™à¹† à¹€à¸žà¸·à¹ˆà¸­à¸—à¸µà¹ˆ à¸£à¸§à¸¡à¹† à¸à¸§à¹‰à¸²à¸‡à¸‚à¸§à¸²à¸‡ à¹€à¸ªà¸µà¸¢à¸¢à¸´à¹ˆà¸‡ à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™ à¸œà¹ˆà¸²à¸™\nà¸—à¸£à¸‡ à¸—à¸§à¹ˆà¸² à¸à¸±à¸™à¹€à¸–à¸­à¸° à¹€à¸à¸µà¹ˆà¸¢à¸§à¹† à¹ƒà¸”à¹† à¸„à¸£à¸±à¹‰à¸‡à¸—à¸µà¹ˆ à¸„à¸£à¸±à¹‰à¸‡à¸™à¸±à¹‰à¸™ à¸„à¸£à¸±à¹‰à¸‡à¸™à¸µà¹‰ à¸„à¸£à¸±à¹‰à¸‡à¸¥à¸° à¸„à¸£à¸±à¹‰à¸‡à¸«à¸¥à¸±à¸‡ à¸„à¸£à¸±à¹‰à¸‡à¸«à¸¥à¸±à¸‡à¸ªà¸¸à¸” à¸£à¹ˆà¸§à¸¡à¸à¸±à¸™ à¸£à¹ˆà¸§à¸¡à¸”à¹‰à¸§à¸¢ à¸à¹‡à¸•à¸²à¸¡à¸—à¸µ\nà¸—à¸µà¹ˆà¸ªà¸¸à¸” à¸œà¸´à¸”à¹† à¸¢à¸·à¸™à¸¢à¸‡ à¹€à¸¢à¸­à¸° à¸„à¸£à¸±à¹‰à¸‡à¹† à¹ƒà¸„à¸£à¹† à¸™à¸±à¹ˆà¸™à¹€à¸­à¸‡ à¹€à¸ªà¸¡à¸·à¸­à¸™à¸§à¹ˆà¸² à¹€à¸ªà¸£à¹‡à¸ˆ à¸•à¸¥à¸­à¸”à¸¨à¸ à¸—à¸±à¹‰à¸‡à¸—à¸µà¹ˆ à¸¢à¸·à¸™à¸¢à¸±à¸™ à¸”à¹‰à¸§à¸¢à¸—à¸µà¹ˆ à¸šà¸±à¸”à¸™à¸µà¹‰\nà¸”à¹‰à¸§à¸¢à¸›à¸£à¸°à¸à¸²à¸£à¸‰à¸°à¸™à¸µà¹‰ à¸‹à¸¶à¹ˆà¸‡à¸à¸±à¸™ à¸•à¸¥à¸­à¸”à¸—à¸±à¹ˆà¸§à¸–à¸¶à¸‡ à¸•à¸¥à¸­à¸”à¸—à¸±à¹ˆà¸§à¸—à¸±à¹‰à¸‡ à¸•à¸¥à¸­à¸”à¸›à¸µ à¹€à¸›à¹‡à¸™à¸à¸²à¸£ à¸™à¸±à¹ˆà¸™à¹à¸«à¸¥à¸° à¸žà¸£à¹‰à¸­à¸¡ à¹€à¸–à¸´à¸” à¸—à¸±à¹‰à¸‡ à¸ªà¸·à¸šà¹€à¸™à¸·à¹ˆà¸­à¸‡ à¸•à¸±à¹‰à¸‡à¹à¸•à¹ˆ\nà¸à¸¥à¸±à¸š à¸à¸¥à¹ˆà¸²à¸§à¸„à¸·à¸­ à¸à¸¥à¸¸à¹ˆà¸¡à¸à¹‰à¸­à¸™ à¸à¸¥à¸¸à¹ˆà¸¡à¹† à¸„à¸£à¸±à¹‰à¸‡à¸„à¸£à¸² à¸ªà¹ˆà¸‡ à¸£à¸§à¸”à¹€à¸£à¹‡à¸§ à¹€à¸ªà¸£à¹‡à¸ˆà¸ªà¸´à¹‰à¸™ à¹€à¸ªà¸µà¸¢ à¹€à¸ªà¸µà¸¢à¸à¹ˆà¸­à¸™ à¹€à¸ªà¸µà¸¢à¸ˆà¸™ à¸­à¸”à¸µà¸• à¸•à¸±à¹‰à¸‡ à¹€à¸à¸´à¸” à¸­à¸²à¸ˆ\nà¸­à¸µà¸ à¸•à¸¥à¸­à¸”à¹€à¸§à¸¥à¸² à¸ à¸²à¸¢à¸«à¸™à¹‰à¸² à¸ à¸²à¸¢à¸«à¸¥à¸±à¸‡ à¸¡à¸­à¸‡ à¸¡à¸±à¸™à¹† à¸¡à¸­à¸‡à¸§à¹ˆà¸² à¸¡à¸±à¸ à¸¡à¸±à¸à¸ˆà¸° à¸¡à¸±à¸™ à¸«à¸²à¸ à¸„à¸‡à¸­à¸¢à¸¹à¹ˆ à¹€à¸›à¹‡à¸™à¸—à¸µà¹ˆ à¹€à¸›à¹‡à¸™à¸—à¸µà¹ˆà¸ªà¸¸à¸”\nà¹€à¸›à¹‡à¸™à¹€à¸žà¸£à¸²à¸°à¹€à¸›à¹‡à¸™à¹€à¸žà¸£à¸²à¸°à¸§à¹ˆà¸² à¹€à¸à¸µà¹ˆà¸¢à¸§à¸à¸±à¸™ à¹€à¸žà¸µà¸¢à¸‡à¹„à¸£ à¹€à¸›à¹‡à¸™à¹à¸•à¹ˆà¹€à¸žà¸µà¸¢à¸‡ à¸à¸¥à¹ˆà¸²à¸§ à¸ˆà¸™à¸šà¸±à¸”à¸™à¸µà¹‰ à¹€à¸›à¹‡à¸™à¸­à¸±à¸™ à¸ˆà¸™ à¸ˆà¸™à¹€à¸¡à¸·à¹ˆà¸­ à¸ˆà¸™à¹à¸¡à¹‰ à¹ƒà¸à¸¥à¹‰\nà¹ƒà¸«à¸¡à¹ˆà¹† à¹€à¸›à¹‡à¸™à¹€à¸žà¸µà¸¢à¸‡ à¸­à¸¢à¹ˆà¸²à¸‡à¸—à¸µà¹ˆ à¸–à¸¹à¸à¸•à¹‰à¸­à¸‡ à¸—à¸±à¹‰à¸‡à¸™à¸±à¹‰à¸™ à¸—à¸±à¹‰à¸‡à¸™à¸±à¹‰à¸™à¸”à¹‰à¸§à¸¢ à¸à¸±à¸™à¸”à¸µà¸à¸§à¹ˆà¸² à¸à¸±à¸™à¸”à¸µà¹„à¸«à¸¡ à¸™à¸±à¹ˆà¸™à¹„à¸‡ à¸•à¸£à¸‡à¹† à¹à¸¢à¸°à¹† à¹€à¸›à¹‡à¸™à¸•à¹‰à¸™ à¹ƒà¸à¸¥à¹‰à¹†\nà¸‹à¸¶à¹ˆà¸‡à¹† à¸”à¹‰à¸§à¸¢à¸à¸±à¸™ à¸”à¸±à¸‡à¹€à¸„à¸¢ à¹€à¸–à¸­à¸° à¹€à¸ªà¸¡à¸·à¸­à¸™à¸à¸±à¸š à¹„à¸› à¸„à¸·à¸­ à¸‚à¸“à¸°à¸™à¸µà¹‰ à¸™à¸­à¸à¸ˆà¸²à¸ à¹€à¸žà¸·à¹ˆà¸­à¸—à¸µà¹ˆà¸ˆà¸° à¸‚à¸“à¸°à¸«à¸™à¸¶à¹ˆà¸‡ à¸‚à¸§à¸²à¸‡ à¸„à¸£à¸±à¸™ à¸­à¸¢à¸²à¸ à¹„à¸§à¹‰\nà¹à¸šà¸š à¸™à¸­à¸à¸ˆà¸²à¸à¸™à¸µà¹‰ à¹€à¸™à¸·à¹ˆà¸­à¸‡à¸ˆà¸²à¸ à¹€à¸”à¸µà¸¢à¸§à¸à¸±à¸™ à¸„à¸‡ à¹ƒà¸«à¹‰à¸¡à¸² à¸­à¸™à¸¶à¹ˆà¸‡ à¸à¹‡à¹à¸¥à¹‰à¸§à¹à¸•à¹ˆ à¸•à¹‰à¸­à¸‡ à¸‚à¹‰à¸²à¸‡ à¹€à¸žà¸·à¹ˆà¸­à¸§à¹ˆà¸² à¸ˆà¸™à¹à¸¡à¹‰à¸™ à¸„à¸£à¸±à¹‰à¸‡à¸«à¸™à¸¶à¹ˆà¸‡ à¸­à¸°à¹„à¸£ à¸‹à¸¶à¹ˆà¸‡\nà¹€à¸à¸´à¸™à¹† à¸”à¹‰à¸§à¸¢à¹€à¸«à¸•à¸¸à¸™à¸±à¹‰à¸™ à¸à¸±à¸™à¹à¸¥à¸°à¸à¸±à¸™ à¸£à¸±à¸š à¸£à¸°à¸«à¸§à¹ˆà¸²à¸‡ à¸„à¸£à¸±à¹‰à¸‡à¹„à¸«à¸™ à¹€à¸ªà¸£à¹‡à¸ˆà¸à¸±à¸™ à¸–à¸¶à¸‡à¸­à¸¢à¹ˆà¸²à¸‡à¹„à¸£ à¸‚à¸²à¸” à¸‚à¹‰à¸²à¸¯ à¹€à¸‚à¹‰à¸²à¹ƒà¸ˆ à¸„à¸£à¸š à¸„à¸£à¸±à¹‰à¸‡à¹ƒà¸”\nà¸„à¸£à¸šà¸–à¹‰à¸§à¸™ à¸£à¸°à¸¢à¸° à¹„à¸¡à¹ˆ à¹€à¸à¸·à¸­à¸š à¹€à¸à¸·à¸­à¸šà¸ˆà¸° à¹€à¸à¸·à¸­à¸šà¹† à¹à¸à¹ˆ à¹à¸ à¸­à¸¢à¹ˆà¸²à¸‡à¹‚à¸™à¹‰à¸™ à¸”à¸±à¸‡à¸à¸±à¸šà¸§à¹ˆà¸² à¸ˆà¸£à¸´à¸‡à¸ˆà¸±à¸‡ à¹€à¸¢à¸­à¸°à¹à¸¢à¸° à¸™à¸±à¹ˆà¸™ à¸”à¹‰à¸§à¸¢ à¸–à¸¶à¸‡à¹à¸¡à¹‰à¸§à¹ˆà¸²\nà¸¡à¸²à¸ à¸•à¸¥à¸­à¸”à¸à¸²à¸¥à¸™à¸²à¸™ à¸•à¸¥à¸­à¸”à¸£à¸°à¸¢à¸°à¹€à¸§à¸¥à¸² à¸•à¸¥à¸­à¸”à¸ˆà¸™ à¸•à¸¥à¸­à¸”à¹„à¸› à¹€à¸›à¹‡à¸™à¸­à¸±à¸™à¹† à¹€à¸›à¹‡à¸™à¸­à¸²à¸—à¸´ à¸à¹‡à¸•à¹ˆà¸­à¹€à¸¡à¸·à¹ˆà¸­ à¸ªà¸¹à¹ˆ à¹€à¸¡à¸·à¹ˆà¸­ à¹€à¸žà¸·à¹ˆà¸­ à¸à¹‡ à¸à¸±à¸š\nà¸”à¹‰à¸§à¸¢à¹€à¸«à¸¡à¸·à¸­à¸™à¸à¸±à¸™ à¸”à¹‰à¸§à¸¢à¹€à¸«à¸•à¸¸à¸™à¸µà¹‰ à¸„à¸£à¸±à¹‰à¸‡à¸„à¸£à¸²à¸§ à¸£à¸²à¸¢ à¸£à¹ˆà¸§à¸¡ à¹€à¸›à¹‡à¸™à¸­à¸±à¸™à¸¡à¸²à¸ à¸ªà¸¹à¸‡ à¸£à¸§à¸¡à¸à¸±à¸™ à¸£à¸§à¸¡à¸—à¸±à¹‰à¸‡ à¸£à¹ˆà¸§à¸¡à¸¡à¸·à¸­ à¹€à¸›à¹‡à¸™à¹€à¸žà¸µà¸¢à¸‡à¸§à¹ˆà¸² à¸£à¸§à¸¡à¸–à¸¶à¸‡\nà¸•à¹ˆà¸­ à¸™à¸° à¸à¸§à¹‰à¸²à¸‡ à¸¡à¸² à¸„à¸£à¸±à¸š à¸•à¸¥à¸­à¸”à¸—à¸±à¹‰à¸‡ à¸à¸²à¸£ à¸™à¸±à¹‰à¸™à¹† à¸™à¹ˆà¸² à¹€à¸›à¹‡à¸™à¸­à¸±à¸™à¸§à¹ˆà¸² à¹€à¸žà¸£à¸²à¸° à¸§à¸±à¸™ à¸ˆà¸™à¸‚à¸“à¸°à¸™à¸µà¹‰ à¸ˆà¸™à¸•à¸¥à¸­à¸” à¸ˆà¸™à¸–à¸¶à¸‡ à¸‚à¹‰à¸² à¸­à¸¢à¹ˆà¸²à¸‡à¹ƒà¸”\nà¹„à¸«à¸™à¹† à¸à¹ˆà¸­à¸™à¸«à¸™à¹‰à¸²à¸™à¸µà¹‰ à¸à¹ˆà¸­à¸™à¹† à¸ªà¸¹à¸‡à¸à¸§à¹ˆà¸² à¸ªà¸¹à¸‡à¸ªà¹ˆà¸‡ à¸ªà¸¹à¸‡à¸ªà¸¸à¸” à¸ªà¸¹à¸‡à¹† à¹€à¸ªà¸µà¸¢à¸”à¹‰à¸§à¸¢ à¹€à¸ªà¸µà¸¢à¸™à¸±à¹ˆà¸™ à¹€à¸ªà¸µà¸¢à¸™à¸µà¹ˆ à¹€à¸ªà¸µà¸¢à¸™à¸µà¹ˆà¸à¸£à¸°à¹„à¸£ à¹€à¸ªà¸µà¸¢à¸™à¸±à¹ˆà¸™à¹€à¸­à¸‡ à¸ªà¸¸à¸”\nà¸ªà¹à¸²à¸«à¸£à¸±à¸š à¸§à¹ˆà¸² à¸¥à¸‡ à¸ à¸²à¸¢à¹ƒà¸•à¹‰ à¹€à¸žà¸·à¹ˆà¸­à¹ƒà¸«à¹‰ à¸ à¸²à¸¢à¸™à¸­à¸ à¸ à¸²à¸¢à¹ƒà¸™ à¹€à¸‰à¸žà¸²à¸° à¸‹à¸¶à¹ˆà¸‡à¸à¸±à¸™à¹à¸¥à¸°à¸à¸±à¸™ à¸‡à¹ˆà¸²à¸¢ à¸‡à¹ˆà¸²à¸¢à¹† à¹„à¸‡ à¸–à¸¶à¸‡à¹à¸¡à¹‰à¸ˆà¸° à¸–à¸¶à¸‡à¹€à¸¡à¸·à¹ˆà¸­à¹„à¸£\nà¹€à¸à¸´à¸™ à¸à¹‡à¹„à¸”à¹‰ à¸„à¸£à¸²à¹ƒà¸” à¸„à¸£à¸²à¸—à¸µà¹ˆ à¸•à¸¥à¸­à¸”à¸§à¸±à¸™ à¸™à¸±à¸š à¸”à¸±à¸‡à¹€à¸à¹ˆà¸² à¸”à¸±à¹ˆà¸‡à¹€à¸à¹ˆà¸² à¸«à¸¥à¸²à¸¢ à¸«à¸™à¸¶à¹ˆà¸‡ à¸–à¸·à¸­à¸§à¹ˆà¸² à¸à¹ˆà¸­à¸™à¸«à¸™à¹‰à¸² à¸™à¸±à¸šà¸•à¸±à¹‰à¸‡à¹à¸•à¹ˆ à¸ˆà¸£à¸” à¸ˆà¸£à¸´à¸‡à¹†\nà¸ˆà¸§à¸™ à¸ˆà¸§à¸™à¹€à¸ˆà¸µà¸¢à¸™ à¸•à¸¥à¸­à¸”à¸¡à¸² à¸à¸¥à¸¸à¹ˆà¸¡ à¸à¸£à¸°à¸™à¸±à¹‰à¸™ à¸‚à¹‰à¸²à¸‡à¹† à¸•à¸£à¸‡ à¸‚à¹‰à¸²à¸žà¹€à¸ˆà¹‰à¸² à¸à¸§à¹ˆà¸² à¹€à¸à¸µà¹ˆà¸¢à¸§à¹€à¸™à¸·à¹ˆà¸­à¸‡ à¸‚à¸¶à¹‰à¸™ à¹ƒà¸«à¹‰à¹„à¸› à¸œà¸¥ à¹à¸•à¹ˆ à¹€à¸­à¸‡ à¹€à¸«à¹‡à¸™\nà¸ˆà¸¶à¸‡ à¹„à¸”à¹‰ à¹ƒà¸«à¹‰ à¹‚à¸”à¸¢ à¸ˆà¸£à¸´à¸‡à¹†à¸ˆà¸±à¸‡à¹† à¸”à¸±à¹ˆà¸‡à¸à¸±à¸šà¸§à¹ˆà¸² à¸—à¸±à¹‰à¸‡à¸™à¸±à¹‰à¸™à¹€à¸žà¸£à¸²à¸° à¸™à¸­à¸ à¸™à¸­à¸à¹€à¸«à¸™à¸·à¸­ à¸™à¹ˆà¸° à¸à¸±à¸™à¸™à¸° à¸‚à¸“à¸°à¹€à¸”à¸µà¸¢à¸§à¸à¸±à¸™ à¹à¸¢à¸°\nà¸™à¸­à¸à¹€à¸«à¸™à¸·à¸­à¸ˆà¸²à¸ à¸™à¹‰à¸­à¸¢ à¸à¹ˆà¸­à¸™ à¸ˆà¸§à¸™à¸ˆà¸° à¸‚à¹‰à¸²à¸‡à¹€à¸„à¸µà¸¢à¸‡ à¸à¹‡à¸•à¸²à¸¡à¹à¸•à¹ˆ à¸ˆà¸£à¸”à¸à¸±à¸š à¸™à¹‰à¸­à¸¢à¸à¸§à¹ˆà¸² à¸™à¸±à¹ˆà¸™à¹€à¸›à¹‡à¸™ à¸™à¸±à¸à¹† à¸„à¸£à¸±à¹‰à¸‡à¸à¸£à¸°à¸™à¸±à¹‰à¸™ à¹€à¸¥à¸¢ à¹„à¸à¸¥\nà¸ªà¸´à¹‰à¸™à¸à¸²à¸¥à¸™à¸²à¸™ à¸„à¸£à¸±à¹‰à¸‡ à¸£à¸·à¸­à¸§à¹ˆà¸² à¹€à¸à¹‡à¸š à¸­à¸¢à¹ˆà¸²à¸‡à¹€à¸Šà¹ˆà¸™ à¸šà¸²à¸‡ à¸”à¸±à¹ˆà¸‡ à¸”à¸±à¸‡à¸à¸¥à¹ˆà¸²à¸§ à¸”à¸±à¸‡à¸à¸±à¸š à¸£à¸¶ à¸£à¸¶à¸§à¹ˆà¸² à¸­à¸­à¸ à¹à¸£à¸ à¸ˆà¸‡ à¸¢à¸·à¸™à¸™à¸²à¸™ à¹„à¸”à¹‰à¸¡à¸² à¸•à¸™\nà¸•à¸™à¹€à¸­à¸‡ à¹„à¸”à¹‰à¸£à¸±à¸š à¸£à¸°à¸¢à¸°à¹† à¸à¸£à¸°à¸œà¸¡ à¸à¸±à¸™à¹„à¸«à¸¡ à¸à¸±à¸™à¹€à¸­à¸‡ à¸à¸³à¸¥à¸±à¸‡à¸ˆà¸° à¸à¸³à¸«à¸™à¸” à¸à¸¹ à¸à¸³à¸¥à¸±à¸‡ à¸„à¸§à¸²à¸¡ à¹à¸¥à¹‰à¸§ à¹à¸¥à¸° à¸•à¹ˆà¸²à¸‡ à¸­à¸¢à¹ˆà¸²à¸‡à¸™à¹‰à¸­à¸¢\nà¸­à¸¢à¹ˆà¸²à¸‡à¸™à¸±à¹‰à¸™ à¸­à¸¢à¹ˆà¸²à¸‡à¸™à¸µà¹‰ à¸à¹‡à¸„à¸·à¸­ à¸à¹‡à¹à¸„à¹ˆ à¸”à¹‰à¸§à¸¢à¹€à¸«à¸•à¸¸à¸—à¸µà¹ˆ à¹ƒà¸«à¸à¹ˆà¹† à¹ƒà¸«à¹‰à¸”à¸µ à¸¢à¸±à¸‡ à¹€à¸›à¹‡à¸™à¹€à¸žà¸·à¹ˆà¸­ à¸à¹‡à¸•à¸²à¸¡ à¸œà¸¹à¹‰ à¸•à¹ˆà¸­à¸à¸±à¸™ à¸–à¸·à¸­ à¸‹à¸¶à¹ˆà¸‡à¸à¹‡à¸„à¸·à¸­ à¸ à¸²à¸¢à¸ à¸²à¸„\nà¸ à¸²à¸¢à¸ à¸²à¸„à¸«à¸™à¹‰à¸² à¸à¹‡à¸”à¸µ à¸à¹‡à¸ˆà¸° à¸­à¸¢à¸¹à¹ˆ à¹€à¸ªà¸µà¸¢à¸¢à¸´à¹ˆà¸‡à¸™à¸±à¸ à¹ƒà¸«à¸¡à¹ˆ à¸‚à¸“à¸° à¹€à¸£à¸´à¹ˆà¸¡ à¹€à¸£à¸² à¸‚à¸§à¸²à¸‡à¹† à¹€à¸ªà¸µà¸¢à¹à¸¥à¹‰à¸§ à¹ƒà¸„à¸£à¹ˆ à¹ƒà¸„à¸£à¹ˆà¸ˆà¸° à¸•à¸™à¸¯ à¸‚à¸­à¸‡ à¹à¸«à¹ˆà¸‡\nà¸£à¸§à¸” à¸”à¸±à¹ˆà¸‡à¸à¸±à¸š à¸–à¸¶à¸‡à¹€à¸¡à¸·à¹ˆà¸­ à¸™à¹‰à¸­à¸¢à¹† à¸™à¸±à¸šà¸ˆà¸²à¸à¸™à¸±à¹‰à¸™ à¸•à¸¥à¸­à¸” à¸•à¸¥à¸­à¸”à¸à¸²à¸¥ à¹€à¸ªà¸£à¹‡à¸ˆà¸ªà¸¡à¸šà¸¹à¸£à¸“à¹Œ à¹€à¸‚à¸µà¸¢à¸™ à¸à¸§à¹‰à¸²à¸‡à¹† à¸¢à¸·à¸™à¸¢à¸²à¸§ à¸–à¸¶à¸‡à¹à¸à¹ˆ à¸‚à¸“à¸°à¹ƒà¸”\nà¸‚à¸“à¸°à¹ƒà¸”à¹† à¸‚à¸“à¸°à¸—à¸µà¹ˆ à¸‚à¸“à¸°à¸™à¸±à¹‰à¸™ à¸ˆà¸™à¸—à¸±à¹ˆà¸§ à¸ à¸²à¸„à¸¯ à¸ à¸²à¸¢ à¹€à¸›à¹‡à¸™à¹à¸•à¹ˆ à¸­à¸¢à¹ˆà¸²à¸‡ à¸žà¸š à¸ à¸²à¸„ à¹ƒà¸«à¹‰à¹à¸”à¹ˆ à¹€à¸ªà¸µà¸¢à¸ˆà¸™à¸à¸£à¸°à¸—à¸±à¹ˆà¸‡ à¹€à¸ªà¸µà¸¢à¸ˆà¸™à¸–à¸¶à¸‡\nà¸ˆà¸™à¸à¸£à¸°à¸—à¸±à¹ˆà¸‡ à¸ˆà¸™à¸à¸§à¹ˆà¸² à¸•à¸¥à¸­à¸”à¸—à¸±à¹ˆà¸§ à¹€à¸›à¹‡à¸™à¹† à¸™à¸­à¸à¸ˆà¸²à¸à¸™à¸±à¹‰à¸™ à¸œà¸´à¸” à¸„à¸£à¸±à¹‰à¸‡à¸à¹ˆà¸­à¸™ à¹à¸à¹‰à¹„à¸‚ à¸‚à¸±à¹‰à¸™ à¸à¸±à¸™ à¸Šà¹ˆà¸§à¸‡ à¸ˆà¸²à¸ à¸£à¸§à¸¡à¸”à¹‰à¸§à¸¢ à¹€à¸‚à¸²\nà¸”à¹‰à¸§à¸¢à¹€à¸Šà¹ˆà¸™à¸à¸±à¸™ à¸™à¸­à¸à¸ˆà¸²à¸à¸—à¸µà¹ˆ à¹€à¸›à¹‡à¸™à¸•à¹‰à¸™à¹„à¸› à¸‚à¹‰à¸²à¸‡à¸•à¹‰à¸™ à¸‚à¹‰à¸²à¸‡à¸šà¸™ à¸‚à¹‰à¸²à¸‡à¸¥à¹ˆà¸²à¸‡ à¸–à¸¶à¸‡à¸ˆà¸° à¸–à¸¶à¸‡à¸šà¸±à¸”à¸™à¸±à¹‰à¸™ à¸–à¸¶à¸‡à¹à¸¡à¹‰ à¸¡à¸µ à¸—à¸²à¸‡ à¹€à¸„à¸¢ à¸™à¸±à¸šà¸ˆà¸²à¸à¸™à¸µà¹‰\nà¸­à¸¢à¹ˆà¸²à¸‡à¹€à¸”à¸µà¸¢à¸§ à¹€à¸à¸µà¹ˆà¸¢à¸§à¸‚à¹‰à¸­à¸‡ à¸™à¸µà¹‰ à¸™à¹à¸² à¸™à¸±à¹‰à¸™ à¸—à¸µà¹ˆ à¸—à¹à¸²à¹ƒà¸«à¹‰ à¸—à¹à¸² à¸„à¸£à¸²à¸™à¸±à¹‰à¸™ à¸„à¸£à¸²à¸™à¸µà¹‰ à¸„à¸£à¸²à¸«à¸™à¸¶à¹ˆà¸‡ à¸„à¸£à¸²à¹„à¸«à¸™ à¸„à¸£à¸²à¸§ à¸„à¸£à¸²à¸§à¸à¹ˆà¸­à¸™ à¸„à¸£à¸²à¸§à¹ƒà¸”\nà¸„à¸£à¸²à¸§à¸—à¸µà¹ˆ à¸„à¸£à¸²à¸§à¸™à¸±à¹‰à¸™ à¸„à¸£à¸²à¸§à¸™à¸µà¹‰ à¸„à¸£à¸²à¸§à¹‚à¸™à¹‰à¸™ à¸„à¸£à¸²à¸§à¸¥à¸° à¸„à¸£à¸²à¸§à¸«à¸™à¹‰à¸² à¸„à¸£à¸²à¸§à¸«à¸™à¸¶à¹ˆà¸‡ à¸„à¸£à¸²à¸§à¸«à¸¥à¸±à¸‡ à¸„à¸£à¸²à¸§à¹„à¸«à¸™ à¸„à¸£à¸²à¸§à¹† à¸„à¸¥à¹‰à¸²à¸¢\nà¸„à¸¥à¹‰à¸²à¸¢à¸à¸±à¸™ à¸„à¸¥à¹‰à¸²à¸¢à¸à¸±à¸™à¸à¸±à¸š à¸„à¸¥à¹‰à¸²à¸¢à¸à¸±à¸š à¸„à¸¥à¹‰à¸²à¸¢à¸à¸±à¸šà¸§à¹ˆà¸² à¸„à¸¥à¹‰à¸²à¸¢à¸§à¹ˆà¸² à¸„à¸§à¸£ à¸„à¹ˆà¸­à¸™ à¸„à¹ˆà¸­à¸™à¸‚à¹‰à¸²à¸‡ à¸„à¹ˆà¸­à¸™à¸‚à¹‰à¸²à¸‡à¸ˆà¸° à¸„à¹ˆà¸­à¸¢à¹„à¸›à¸—à¸²à¸‡ à¸„à¹ˆà¸­à¸™à¸¡à¸²à¸—à¸²à¸‡ à¸„\nà¹ˆà¸­à¸¢ à¸„à¹ˆà¸­à¸¢à¹† à¸„à¸° à¸„à¹ˆà¸° à¸„à¸³ à¸„à¸´à¸” à¸„à¸´à¸”à¸§à¹ˆà¸² à¸„à¸¸à¸“ à¸„à¸¸à¸“à¹† à¹€à¸„à¸¢à¹† à¹à¸„à¹ˆ à¹à¸„à¹ˆà¸ˆà¸° à¹à¸„à¹ˆà¸™à¸±à¹‰à¸™ à¹à¸„à¹ˆà¸™à¸µà¹‰ à¹à¸„à¹ˆà¹€à¸žà¸µà¸¢à¸‡ à¹à¸„à¹ˆà¸§à¹ˆà¸² à¹à¸„à¹ˆà¹„à¸«à¸™ à¸ˆà¸±à¸‡à¹†\nà¸ˆà¸§à¸šà¸à¸±à¸š à¸ˆà¸§à¸šà¸ˆà¸™ à¸ˆà¹‰à¸° à¸ˆà¹Šà¸° à¸ˆà¸°à¹„à¸”à¹‰ à¸ˆà¸±à¸‡ à¸ˆà¸±à¸”à¸à¸²à¸£ à¸ˆà¸±à¸”à¸‡à¸²à¸™ à¸ˆà¸±à¸”à¹à¸ˆà¸‡ à¸ˆà¸±à¸”à¸•à¸±à¹‰à¸‡ à¸ˆà¸±à¸”à¸—à¸³ à¸ˆà¸±à¸”à¸«à¸² à¸ˆà¸±à¸”à¹ƒà¸«à¹‰ à¸ˆà¸±à¸š à¸ˆà¹‰à¸² à¸ˆà¹‹à¸² à¸ˆà¸²à¸à¸™à¸±à¹‰à¸™\nà¸ˆà¸²à¸à¸™à¸µà¹‰ à¸ˆà¸²à¸à¸™à¸µà¹‰à¹„à¸› à¸ˆà¸³ à¸ˆà¸³à¹€à¸›à¹‡à¸™ à¸ˆà¸³à¸žà¸§à¸ à¸ˆà¸¶à¸‡à¸ˆà¸° à¸ˆà¸¶à¸‡à¹€à¸›à¹‡à¸™ à¸ˆà¸¹à¹ˆà¹† à¸‰à¸°à¸™à¸±à¹‰à¸™ à¸‰à¸°à¸™à¸µà¹‰ à¸‰à¸±à¸™ à¹€à¸‰à¸à¹€à¸Šà¹ˆà¸™ à¹€à¸‰à¸¢ à¹€à¸‰à¸¢à¹† à¹„à¸‰à¸™ à¸Šà¹ˆà¸§à¸‡à¸à¹ˆà¸­à¸™ à¸Š\nà¹ˆà¸§à¸‡à¸•à¹ˆà¸­à¹„à¸› à¸Šà¹ˆà¸§à¸‡à¸–à¸±à¸”à¹„à¸› à¸Šà¹ˆà¸§à¸‡à¸—à¹‰à¸²à¸¢ à¸Šà¹ˆà¸§à¸‡à¸—à¸µà¹ˆ à¸Šà¹ˆà¸§à¸‡à¸™à¸±à¹‰à¸™ à¸Šà¹ˆà¸§à¸‡à¸™à¸µà¹‰ à¸Šà¹ˆà¸§à¸‡à¸£à¸°à¸«à¸§à¹ˆà¸²à¸‡ à¸Šà¹ˆà¸§à¸‡à¹à¸£à¸ à¸Šà¹ˆà¸§à¸‡à¸«à¸™à¹‰à¸² à¸Šà¹ˆà¸§à¸‡à¸«à¸¥à¸±à¸‡ à¸Šà¹ˆà¸§à¸‡à¹† à¸Šà¹ˆà¸§à¸¢ à¸Šà¹‰à¸²\nà¸Šà¹‰à¸²à¸™à¸²à¸™ à¸Šà¸²à¸§ à¸Šà¹‰à¸²à¹† à¹€à¸Šà¹ˆà¸™à¸à¹ˆà¸­à¸™ à¹€à¸Šà¹ˆà¸™à¸à¸±à¸™ à¹€à¸Šà¹ˆà¸™à¹€à¸„à¸¢ à¹€à¸Šà¹ˆà¸™à¸”à¸±à¸‡ à¹€à¸Šà¹ˆà¸™à¸”à¸±à¸‡à¸à¹ˆà¸­à¸™ à¹€à¸Šà¹ˆà¸™à¸”à¸±à¸‡à¹€à¸à¹ˆà¸² à¹€à¸Šà¹ˆà¸™à¸”à¸±à¸‡à¸—à¸µà¹ˆ à¹€à¸Šà¹ˆà¸™à¸”à¸±à¸‡à¸§à¹ˆà¸²\nà¹€à¸Šà¹ˆà¸™à¹€à¸”à¸µà¸¢à¸§à¸à¸±à¸™ à¹€à¸Šà¹ˆà¸™à¹€à¸”à¸µà¸¢à¸§à¸à¸±à¸š à¹€à¸Šà¹ˆà¸™à¹ƒà¸” à¹€à¸Šà¹ˆà¸™à¸—à¸µà¹ˆ à¹€à¸Šà¹ˆà¸™à¸—à¸µà¹ˆà¹€à¸„à¸¢ à¹€à¸Šà¹ˆà¸™à¸—à¸µà¹ˆà¸§à¹ˆà¸² à¹€à¸Šà¹ˆà¸™à¸™à¸±à¹‰à¸™ à¹€à¸Šà¹ˆà¸™à¸™à¸±à¹‰à¸™à¹€à¸­à¸‡ à¹€à¸Šà¹ˆà¸™à¸™à¸µà¹‰ à¹€à¸Šà¹ˆà¸™à¹€à¸¡à¸·à¹ˆà¸­ à¹€à¸Šà¹ˆà¸™à¹„à¸£\nà¹€à¸Šà¸·à¹ˆà¸­ à¹€à¸Šà¸·à¹ˆà¸­à¸–à¸·à¸­ à¹€à¸Šà¸·à¹ˆà¸­à¸¡à¸±à¹ˆà¸™ à¹€à¸Šà¸·à¹ˆà¸­à¸§à¹ˆà¸² à¹ƒà¸Šà¹ˆ à¹ƒà¸Šà¹‰ à¸‹à¸° à¸‹à¸°à¸à¹ˆà¸­à¸™ à¸‹à¸°à¸ˆà¸™ à¸‹à¸°à¸ˆà¸™à¸à¸£à¸°à¸—à¸±à¹ˆà¸‡ à¸‹à¸°à¸ˆà¸™à¸–à¸¶à¸‡ à¸”à¸±à¹ˆà¸‡à¹€à¸„à¸¢ à¸•à¹ˆà¸²à¸‡à¸à¹‡ à¸•à¹ˆà¸²à¸‡à¸«à¸²à¸\nà¸•à¸²à¸¡à¸”à¹‰à¸§à¸¢ à¸•à¸²à¸¡à¹à¸•à¹ˆ à¸•à¸²à¸¡à¸—à¸µà¹ˆ à¸•à¸²à¸¡à¹† à¹€à¸•à¹‡à¸¡à¹„à¸›à¸”à¹‰à¸§à¸¢ à¹€à¸•à¹‡à¸¡à¹„à¸›à¸«à¸¡à¸” à¹€à¸•à¹‡à¸¡à¹† à¹à¸•à¹ˆà¸à¹‡ à¹à¸•à¹ˆà¸à¹ˆà¸­à¸™ à¹à¸•à¹ˆà¸ˆà¸° à¹à¸•à¹ˆà¹€à¸”à¸´à¸¡ à¹à¸•à¹ˆà¸•à¹‰à¸­à¸‡ à¹à¸•à¹ˆà¸–à¹‰à¸²\nà¹à¸•à¹ˆà¸—à¸§à¹ˆà¸² à¹à¸•à¹ˆà¸—à¸µà¹ˆ à¹à¸•à¹ˆà¸™à¸±à¹‰à¸™ à¹à¸•à¹ˆà¹€à¸žà¸µà¸¢à¸‡ à¹à¸•à¹ˆà¹€à¸¡à¸·à¹ˆà¸­ à¹à¸•à¹ˆà¹„à¸£ à¹à¸•à¹ˆà¸¥à¸° à¹à¸•à¹ˆà¸§à¹ˆà¸² à¹à¸•à¹ˆà¹„à¸«à¸™ à¹à¸•à¹ˆà¸­à¸¢à¹ˆà¸²à¸‡à¹ƒà¸” à¹‚à¸• à¹‚à¸•à¹† à¹ƒà¸•à¹‰ à¸–à¹‰à¸²à¸ˆà¸° à¸–à¹‰à¸²à¸«à¸²à¸\nà¸—à¸±à¹‰à¸‡à¸›à¸§à¸‡ à¸—à¸±à¹‰à¸‡à¹€à¸›à¹‡à¸™ à¸—à¸±à¹‰à¸‡à¸¡à¸§à¸¥ à¸—à¸±à¹‰à¸‡à¸ªà¸´à¹‰à¸™ à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸” à¸—à¸±à¹‰à¸‡à¸«à¸¥à¸²à¸¢ à¸—à¸±à¹‰à¸‡à¹† à¸—à¸±à¸™ à¸—à¸±à¸™à¹ƒà¸”à¸™à¸±à¹‰à¸™ à¸—à¸±à¸™à¸—à¸µ à¸—à¸±à¸™à¸—à¸µà¸—à¸±à¸™à¹ƒà¸” à¸—à¸±à¹ˆà¸§ à¸—à¸³à¹ƒà¸«à¹‰ à¸—à¸³à¹† à¸—à¸µ à¸—à¸µà¹ˆà¸ˆà¸£à¸´à¸‡\nà¸—à¸µà¹ˆà¸‹à¸¶à¹ˆà¸‡ à¸—à¸µà¹€à¸”à¸µà¸¢à¸§ à¸—à¸µà¹ƒà¸” à¸—à¸µà¹ˆà¹ƒà¸” à¸—à¸µà¹ˆà¹„à¸”à¹‰ à¸—à¸µà¹€à¸–à¸­à¸° à¸—à¸µà¹ˆà¹à¸—à¹‰ à¸—à¸µà¹ˆà¹à¸—à¹‰à¸ˆà¸£à¸´à¸‡ à¸—à¸µà¹ˆà¸™à¸±à¹‰à¸™ à¸—à¸µà¹ˆà¸™à¸µà¹‰ à¸—à¸µà¹„à¸£ à¸—à¸µà¸¥à¸° à¸—à¸µà¹ˆà¸¥à¸° à¸—à¸µà¹ˆà¹à¸¥à¹‰à¸§ à¸—à¸µà¹ˆà¸§à¹ˆà¸² à¸—à¸µà¹ˆà¹à¸«à¹ˆà¸‡à¸™à¸±à¹‰à¸™ à¸—à¸µà¹† à¸—à¸µà¹ˆà¹†\nà¸—à¸¸à¸à¸„à¸™ à¸—à¸¸à¸à¸„à¸£à¸±à¹‰à¸‡ à¸—à¸¸à¸à¸„à¸£à¸² à¸—à¸¸à¸à¸„à¸£à¸²à¸§ à¸—à¸¸à¸à¸Šà¸´à¹‰à¸™ à¸—à¸¸à¸à¸•à¸±à¸§ à¸—à¸¸à¸à¸—à¸²à¸‡ à¸—à¸¸à¸à¸—à¸µ à¸—à¸¸à¸à¸—à¸µà¹ˆ à¸—à¸¸à¸à¹€à¸¡à¸·à¹ˆà¸­ à¸—à¸¸à¸à¸§à¸±à¸™ à¸—à¸¸à¸à¸§à¸±à¸™à¸™à¸µà¹‰ à¸—à¸¸à¸à¸ªà¸´à¹ˆà¸‡ à¸—à¸¸à¸à¸«à¸™ à¸—à¸¸à¸à¹à¸«à¹ˆà¸‡\nà¸—à¸¸à¸à¸­à¸¢à¹ˆà¸²à¸‡ à¸—à¸¸à¸à¸­à¸±à¸™ à¸—à¸¸à¸à¹† à¹€à¸—à¹ˆà¸² à¹€à¸—à¹ˆà¸²à¸à¸±à¸™ à¹€à¸—à¹ˆà¸²à¸à¸±à¸š à¹€à¸—à¹ˆà¸²à¹ƒà¸” à¹€à¸—à¹ˆà¸²à¸—à¸µà¹ˆ à¹€à¸—à¹ˆà¸²à¸™à¸±à¹‰à¸™ à¹€à¸—à¹ˆà¸²à¸™à¸µà¹‰ à¹à¸—à¹‰ à¹à¸—à¹‰à¸ˆà¸£à¸´à¸‡ à¹€à¸˜à¸­ à¸™à¸±à¹‰à¸™à¹„à¸§ à¸™à¸±à¸šà¹à¸•à¹ˆà¸™à¸µà¹‰\nà¸™à¸²à¸‡ à¸™à¸²à¸‡à¸ªà¸²à¸§ à¸™à¹ˆà¸²à¸ˆà¸° à¸™à¸²à¸™ à¸™à¸²à¸™à¹† à¸™à¸²à¸¢ à¸™à¸³ à¸™à¸³à¸žà¸² à¸™à¸³à¸¡à¸² à¸™à¸´à¸” à¸™à¸´à¸”à¸«à¸™à¹ˆà¸­à¸¢ à¸™à¸´à¸”à¹† à¸™à¸µà¹ˆ à¸™à¸µà¹ˆà¹„à¸‡ à¸™à¸µà¹ˆà¸™à¸² à¸™à¸µà¹ˆà¹à¸™à¹ˆà¸° à¸™à¸µà¹ˆà¹à¸«à¸¥à¸° à¸™à¸µà¹‰à¹à¸«à¸¥à¹ˆ\nà¸™à¸µà¹ˆà¹€à¸­à¸‡ à¸™à¸µà¹‰à¹€à¸­à¸‡ à¸™à¸¹à¹ˆà¸™ à¸™à¸¹à¹‰à¸™ à¹€à¸™à¹‰à¸™ à¹€à¸™à¸µà¹ˆà¸¢ à¹€à¸™à¸µà¹ˆà¸¢à¹€à¸­à¸‡ à¹ƒà¸™à¸Šà¹ˆà¸§à¸‡ à¹ƒà¸™à¸—à¸µà¹ˆ à¹ƒà¸™à¹€à¸¡à¸·à¹ˆà¸­ à¹ƒà¸™à¸£à¸°à¸«à¸§à¹ˆà¸²à¸‡ à¸šà¸™ à¸šà¸­à¸ à¸šà¸­à¸à¹à¸¥à¹‰à¸§ à¸šà¸­à¸à¸§à¹ˆà¸² à¸šà¹ˆà¸­à¸¢\nà¸šà¹ˆà¸­à¸¢à¸à¸§à¹ˆà¸² à¸šà¹ˆà¸­à¸¢à¸„à¸£à¸±à¹‰à¸‡ à¸šà¹ˆà¸­à¸¢à¹† à¸šà¸±à¸”à¸”à¸¥ à¸šà¸±à¸”à¹€à¸”à¸µà¹‹à¸¢à¸§à¸™à¸µà¹‰ à¸šà¸±à¸”à¸™à¸±à¹‰à¸™ à¸šà¹‰à¸²à¸‡ à¸šà¸²à¸‡à¸à¸§à¹ˆà¸² à¸šà¸²à¸‡à¸‚à¸“à¸° à¸šà¸²à¸‡à¸„à¸£à¸±à¹‰à¸‡ à¸šà¸²à¸‡à¸„à¸£à¸² à¸šà¸²à¸‡à¸„à¸£à¸²à¸§\nà¸šà¸²à¸‡à¸—à¸µ à¸šà¸²à¸‡à¸—à¸µà¹ˆ à¸šà¸²à¸‡à¹à¸«à¹ˆà¸‡ à¸šà¸²à¸‡à¹† à¸›à¸à¸´à¸šà¸±à¸•à¸´ à¸›à¸£à¸°à¸à¸­à¸š à¸›à¸£à¸°à¸à¸²à¸£ à¸›à¸£à¸°à¸à¸²à¸£à¸‰à¸°à¸™à¸µà¹‰ à¸›à¸£à¸°à¸à¸²à¸£à¹ƒà¸” à¸›à¸£à¸°à¸à¸²à¸£à¸«à¸™à¸¶à¹ˆà¸‡ à¸›à¸£à¸°à¸¡à¸²à¸“\nà¸›à¸£à¸°à¸ªà¸š à¸›à¸£à¸±à¸š à¸›à¸£à¸²à¸à¸ à¸›à¸£à¸²à¸à¸à¸§à¹ˆà¸² à¸›à¸±à¸ˆà¸ˆà¸¸à¸šà¸±à¸™ à¸›à¸´à¸” à¹€à¸›à¹‡à¸™à¸”à¹‰à¸§à¸¢ à¹€à¸›à¹‡à¸™à¸”à¸±à¸‡ à¸œà¸¹à¹‰à¹ƒà¸” à¹€à¸œà¸·à¹ˆà¸­ à¹€à¸œà¸·à¹ˆà¸­à¸ˆà¸° à¹€à¸œà¸·à¹ˆà¸­à¸—à¸µà¹ˆ à¹€à¸œà¸·à¹ˆà¸­à¸§à¹ˆà¸² à¸à¹ˆà¸²à¸¢ à¸à¹ˆà¸²à¸¢à¹ƒà¸”\nà¸žà¸šà¸§à¹ˆà¸² à¸žà¸¢à¸²à¸¢à¸²à¸¡ à¸žà¸£à¹‰à¸­à¸¡à¸à¸±à¸™ à¸žà¸£à¹‰à¸­à¸¡à¸à¸±à¸š à¸žà¸£à¹‰à¸­à¸¡à¸”à¹‰à¸§à¸¢ à¸žà¸£à¹‰à¸­à¸¡à¸—à¸±à¹‰à¸‡ à¸žà¸£à¹‰à¸­à¸¡à¸—à¸µà¹ˆ à¸žà¸£à¹‰à¸­à¸¡à¹€à¸žà¸µà¸¢à¸‡ à¸žà¸§à¸ à¸žà¸§à¸à¸à¸±à¸™ à¸žà¸§à¸à¸à¸¹ à¸žà¸§à¸à¹à¸\nà¸žà¸§à¸à¹€à¸‚à¸² à¸žà¸§à¸à¸„à¸¸à¸“ à¸žà¸§à¸à¸‰à¸±à¸™ à¸žà¸§à¸à¸—à¹ˆà¸²à¸™ à¸žà¸§à¸à¸—à¸µà¹ˆ à¸žà¸§à¸à¹€à¸˜à¸­ à¸žà¸§à¸à¸™à¸±à¹‰à¸™ à¸žà¸§à¸à¸™à¸µà¹‰ à¸žà¸§à¸à¸™à¸¹à¹‰à¸™ à¸žà¸§à¸à¹‚à¸™à¹‰à¸™ à¸žà¸§à¸à¸¡à¸±à¸™ à¸žà¸§à¸à¸¡à¸¶à¸‡ à¸žà¸­ à¸žà¸­à¸à¸±à¸™\nà¸žà¸­à¸„à¸§à¸£ à¸žà¸­à¸ˆà¸° à¸žà¸­à¸”à¸µ à¸žà¸­à¸•à¸±à¸§ à¸žà¸­à¸—à¸µ à¸žà¸­à¸—à¸µà¹ˆ à¸žà¸­à¹€à¸žà¸µà¸¢à¸‡ à¸žà¸­à¹à¸¥à¹‰à¸§ à¸žà¸­à¸ªà¸¡ à¸žà¸­à¸ªà¸¡à¸„à¸§à¸£ à¸žà¸­à¹€à¸«à¸¡à¸²à¸° à¸žà¸­à¹† à¸žà¸² à¸žà¸¶à¸‡ à¸žà¸¶à¹ˆà¸‡ à¸žà¸·à¹‰à¸™à¹† à¸žà¸¹à¸”\nà¹€à¸žà¸£à¸²à¸°à¸‰à¸°à¸™à¸±à¹‰à¸™ à¹€à¸žà¸£à¸²à¸°à¸§à¹ˆà¸² à¹€à¸žà¸´à¹ˆà¸‡ à¹€à¸žà¸´à¹ˆà¸‡à¸ˆà¸° à¹€à¸žà¸´à¹ˆà¸¡ à¹€à¸žà¸´à¹ˆà¸¡à¹€à¸•à¸´à¸¡ à¹€à¸žà¸µà¸¢à¸‡ à¹€à¸žà¸µà¸¢à¸‡à¹à¸„à¹ˆ à¹€à¸žà¸µà¸¢à¸‡à¹ƒà¸” à¹€à¸žà¸µà¸¢à¸‡à¹à¸•à¹ˆ à¹€à¸žà¸µà¸¢à¸‡à¸žà¸­ à¹€à¸žà¸µà¸¢à¸‡à¹€à¸žà¸£à¸²à¸°\nà¸¡à¸²à¸à¸à¸§à¹ˆà¸² à¸¡à¸²à¸à¸¡à¸²à¸¢ à¸¡à¸´ à¸¡à¸´à¸‰à¸°à¸™à¸±à¹‰à¸™ à¸¡à¸´à¹ƒà¸Šà¹ˆ à¸¡à¸´à¹„à¸”à¹‰ à¸¡à¸µà¹à¸•à¹ˆ à¸¡à¸¶à¸‡ à¸¡à¸¸à¹ˆà¸‡ à¸¡à¸¸à¹ˆà¸‡à¹€à¸™à¹‰à¸™ à¸¡à¸¸à¹ˆà¸‡à¸«à¸¡à¸²à¸¢ à¹€à¸¡à¸·à¹ˆà¸­à¸à¹ˆà¸­à¸™ à¹€à¸¡à¸·à¹ˆà¸­à¸„à¸£à¸±à¹‰à¸‡ à¹€à¸¡à¸·à¹ˆà¸­à¸„à¸£à¸±à¹‰à¸‡à¸à¹ˆà¸­à¸™\nà¹€à¸¡à¸·à¹ˆà¸­à¸„à¸£à¸²à¸§à¸à¹ˆà¸­à¸™ à¹€à¸¡à¸·à¹ˆà¸­à¸„à¸£à¸²à¸§à¸—à¸µà¹ˆ à¹€à¸¡à¸·à¹ˆà¸­à¸„à¸£à¸²à¸§ à¹€à¸¡à¸·à¹ˆà¸­à¸„à¸·à¸™ à¹€à¸¡à¸·à¹ˆà¸­à¹€à¸Šà¹‰à¸² à¹€à¸¡à¸·à¹ˆà¸­à¹ƒà¸” à¹€à¸¡à¸·à¹ˆà¸­à¸™à¸±à¹‰à¸™ à¹€à¸¡à¸·à¹ˆà¸­à¸™à¸µà¹‰ à¹€à¸¡à¸·à¹ˆà¸­à¹€à¸¢à¹‡à¸™ à¹€à¸¡à¸·à¹ˆà¸­à¸§à¸±à¸™à¸§à¸²à¸™ à¹€à¸¡à¸·à¹ˆà¸­à¸§à¸²à¸™\nà¹à¸¡à¹‰ à¹à¸¡à¹‰à¸à¸£à¸°à¸—à¸±à¹ˆà¸‡ à¹à¸¡à¹‰à¹à¸•à¹ˆ à¹à¸¡à¹‰à¸™à¸§à¹ˆà¸² à¹à¸¡à¹‰à¸§à¹ˆà¸² à¹„à¸¡à¹ˆà¸„à¹ˆà¸­à¸¢ à¹„à¸¡à¹ˆà¸„à¹ˆà¸­à¸¢à¸ˆà¸° à¹„à¸¡à¹ˆà¸„à¹ˆà¸­à¸¢à¹€à¸›à¹‡à¸™ à¹„à¸¡à¹ˆà¹ƒà¸Šà¹ˆ à¹„à¸¡à¹ˆà¹€à¸›à¹‡à¸™à¹„à¸£ à¹„à¸¡à¹ˆà¸§à¹ˆà¸² à¸¢à¸ à¸¢à¸à¹ƒà¸«à¹‰ à¸¢à¸­à¸¡\nà¸¢à¸­à¸¡à¸£à¸±à¸š à¸¢à¹ˆà¸­à¸¡ à¸¢à¹ˆà¸­à¸¢ à¸¢à¸±à¸‡à¸„à¸‡ à¸¢à¸±à¸‡à¸‡à¸±à¹‰à¸™ à¸¢à¸±à¸‡à¸‡à¸µà¹‰ à¸¢à¸±à¸‡à¹‚à¸‡à¹‰à¸™ à¸¢à¸±à¸‡à¹„à¸‡ à¸¢à¸±à¸‡à¸ˆà¸° à¸¢à¸±à¸‡à¹à¸•à¹ˆ à¸¢à¸²à¸ à¸¢à¸²à¸§ à¸¢à¸²à¸§à¸™à¸²à¸™ à¸¢à¸´à¹ˆà¸‡ à¸¢à¸´à¹ˆà¸‡à¸à¸§à¹ˆà¸² à¸¢à¸´à¹ˆà¸‡à¸‚à¸¶à¹‰à¸™\nà¸¢à¸´à¹ˆà¸‡à¸‚à¸¶à¹‰à¸™à¹„à¸› à¸¢à¸´à¹ˆà¸‡à¸ˆà¸™ à¸¢à¸´à¹ˆà¸‡à¸ˆà¸° à¸¢à¸´à¹ˆà¸‡à¸™à¸±à¸ à¸¢à¸´à¹ˆà¸‡à¹€à¸¡à¸·à¹ˆà¸­ à¸¢à¸´à¹ˆà¸‡à¹à¸¥à¹‰à¸§ à¸¢à¸´à¹ˆà¸‡à¹ƒà¸«à¸à¹ˆ à¹€à¸£à¹‡à¸§ à¹€à¸£à¹‡à¸§à¹† à¹€à¸£à¸²à¹† à¹€à¸£à¸µà¸¢à¸ à¹€à¸£à¸µà¸¢à¸š à¹€à¸£à¸·à¹ˆà¸­à¸¢ à¹€à¸£à¸·à¹ˆà¸­à¸¢à¹† à¸¥à¹‰à¸§à¸™\nà¸¥à¹‰à¸§à¸™à¸ˆà¸™ à¸¥à¹‰à¸§à¸™à¹à¸•à¹ˆ à¸¥à¸° à¸¥à¹ˆà¸²à¸ªà¸¸à¸” à¹€à¸¥à¹‡à¸ à¹€à¸¥à¹‡à¸à¸™à¹‰à¸­à¸¢ à¹€à¸¥à¹‡à¸à¹† à¹€à¸¥à¹ˆà¸²à¸§à¹ˆà¸² à¹à¸¥à¹‰à¸§à¸à¸±à¸™ à¹à¸¥à¹‰à¸§à¹à¸•à¹ˆ à¹à¸¥à¹‰à¸§à¹€à¸ªà¸£à¹‡à¸ˆ à¸§à¸±à¸™à¹ƒà¸” à¸§à¸±à¸™à¸™à¸±à¹‰à¸™ à¸§à¸±à¸™à¸™à¸µà¹‰ à¸§à¸±à¸™à¹„à¸«à¸™\nà¸ªà¸šà¸²à¸¢ à¸ªà¸¡à¸±à¸¢ à¸ªà¸¡à¸±à¸¢à¸à¹ˆà¸­à¸™ à¸ªà¸¡à¸±à¸¢à¸™à¸±à¹‰à¸™ à¸ªà¸¡à¸±à¸¢à¸™à¸µà¹‰ à¸ªà¸¡à¸±à¸¢à¹‚à¸™à¹‰à¸™ à¸ªà¹ˆà¸§à¸™à¹€à¸à¸´à¸™ à¸ªà¹ˆà¸§à¸™à¸”à¹‰à¸­à¸¢ à¸ªà¹ˆà¸§à¸™à¸”à¸µ à¸ªà¹ˆà¸§à¸™à¹ƒà¸” à¸ªà¹ˆà¸§à¸™à¸—à¸µà¹ˆ à¸ªà¹ˆà¸§à¸™à¸™à¹‰à¸­à¸¢ à¸ªà¹ˆà¸§à¸™à¸™à¸±à¹‰à¸™ à¸ª\nà¹ˆà¸§à¸™à¸¡à¸²à¸ à¸ªà¹ˆà¸§à¸™à¹ƒà¸«à¸à¹ˆ à¸ªà¸±à¹‰à¸™ à¸ªà¸±à¹‰à¸™à¹† à¸ªà¸²à¸¡à¸²à¸£à¸– à¸ªà¸³à¸„à¸±à¸ à¸ªà¸´à¹ˆà¸‡ à¸ªà¸´à¹ˆà¸‡à¹ƒà¸” à¸ªà¸´à¹ˆà¸‡à¸™à¸±à¹‰à¸™ à¸ªà¸´à¹ˆà¸‡à¸™à¸µà¹‰ à¸ªà¸´à¹ˆà¸‡à¹„à¸«à¸™ à¸ªà¸´à¹‰à¸™ à¹à¸ªà¸”à¸‡ à¹à¸ªà¸”à¸‡à¸§à¹ˆà¸² à¸«à¸™ à¸«à¸™à¸­ à¸«à¸™à¸­à¸¢\nà¸«à¸™à¹ˆà¸­à¸¢ à¸«à¸¡à¸” à¸«à¸¡à¸”à¸à¸±à¸™ à¸«à¸¡à¸”à¸ªà¸´à¹‰à¸™ à¸«à¸²à¸à¹à¸¡à¹‰ à¸«à¸²à¸à¹à¸¡à¹‰à¸™ à¸«à¸²à¸à¹à¸¡à¹‰à¸™à¸§à¹ˆà¸² à¸«à¸²à¸à¸§à¹ˆà¸² à¸«à¸²à¸„à¸§à¸²à¸¡ à¸«à¸²à¹ƒà¸Šà¹ˆ à¸«à¸²à¸£à¸·à¸­ à¹€à¸«à¸•à¸¸ à¹€à¸«à¸•à¸¸à¸œà¸¥ à¹€à¸«à¸•à¸¸à¸™à¸±à¹‰à¸™\nà¹€à¸«à¸•à¸¸à¸™à¸µà¹‰ à¹€à¸«à¸•à¸¸à¹„à¸£ à¹€à¸«à¹‡à¸™à¹à¸à¹ˆ à¹€à¸«à¹‡à¸™à¸„à¸§à¸£ à¹€à¸«à¹‡à¸™à¸ˆà¸° à¹€à¸«à¹‡à¸™à¸§à¹ˆà¸² à¹€à¸«à¸¥à¸·à¸­ à¹€à¸«à¸¥à¸·à¸­à¹€à¸à¸´à¸™ à¹€à¸«à¸¥à¹ˆà¸² à¹€à¸«à¸¥à¹ˆà¸²à¸™à¸±à¹‰à¸™ à¹€à¸«à¸¥à¹ˆà¸²à¸™à¸µà¹‰ à¹à¸«à¹ˆà¸‡à¹ƒà¸” à¹à¸«à¹ˆà¸‡à¸™à¸±à¹‰à¸™\nà¹à¸«à¹ˆà¸‡à¸™à¸µà¹‰ à¹à¸«à¹ˆà¸‡à¹‚à¸™à¹‰à¸™ à¹à¸«à¹ˆà¸‡à¹„à¸«à¸™ à¹à¸«à¸¥à¸° à¹ƒà¸«à¹‰à¹à¸à¹ˆ à¹ƒà¸«à¸à¹ˆ à¹ƒà¸«à¸à¹ˆà¹‚à¸• à¸­à¸¢à¹ˆà¸²à¸‡à¸¡à¸²à¸ à¸­à¸¢à¹ˆà¸²à¸‡à¸¢à¸´à¹ˆà¸‡ à¸­à¸¢à¹ˆà¸²à¸‡à¹„à¸£à¸à¹‡ à¸­à¸¢à¹ˆà¸²à¸‡à¹„à¸£à¸à¹‡à¹„à¸”à¹‰ à¸­à¸¢à¹ˆà¸²à¸‡à¹„à¸£à¹€à¸ªà¸µà¸¢\nà¸­à¸¢à¹ˆà¸²à¸‡à¸¥à¸° à¸­à¸¢à¹ˆà¸²à¸‡à¸«à¸™à¸¶à¹ˆà¸‡ à¸­à¸¢à¹ˆà¸²à¸‡à¹† à¸­à¸±à¸™ à¸­à¸±à¸™à¸ˆà¸° à¸­à¸±à¸™à¹„à¸”à¹‰à¹à¸à¹ˆ à¸­à¸±à¸™à¸—à¸µà¹ˆ à¸­à¸±à¸™à¸—à¸µà¹ˆà¸ˆà¸£à¸´à¸‡ à¸­à¸±à¸™à¸—à¸µà¹ˆà¸ˆà¸° à¸­à¸±à¸™à¹€à¸™à¸·à¹ˆà¸­à¸‡à¸¡à¸²à¸ˆà¸²à¸ à¸­à¸±à¸™à¸¥à¸° à¸­à¸±à¸™à¹† à¸­à¸²à¸ˆà¸ˆà¸°\nà¸­à¸²à¸ˆà¹€à¸›à¹‡à¸™ à¸­à¸²à¸ˆà¹€à¸›à¹‡à¸™à¸”à¹‰à¸§à¸¢ à¸­à¸·à¹ˆà¸™ à¸­à¸·à¹ˆà¸™à¹† à¹€à¸­à¹‡à¸‡ à¹€à¸­à¸² à¸¯ à¸¯à¸¥ à¸¯à¸¥à¸¯ 555 à¸à¸³ à¸‚à¸­à¹‚à¸—à¸© à¹€à¸¢à¸µà¹ˆà¸¢à¸¡ à¸™à¸µà¹ˆà¸„à¸·à¸­\n'.split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/th/tokenizer_exceptions.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/th/__init__.py----------------------------------------
A:spacy.lang.th.__init__.words->list(self.word_tokenize(text))
A:spacy.lang.th.__init__.config->load_config_from_str(DEFAULT_CONFIG)
spacy.lang.th.__init__.Thai(Language)
spacy.lang.th.__init__.ThaiDefaults(BaseDefaults)
spacy.lang.th.__init__.ThaiTokenizer(self,vocab:Vocab)
spacy.lang.th.__init__.create_thai_tokenizer()


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/th/lex_attrs.py----------------------------------------
A:spacy.lang.th.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.th.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.th.lex_attrs.like_num(text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/mk/lemmatizer.py----------------------------------------
A:spacy.lang.mk.lemmatizer.univ_pos->token.pos_.lower()
A:spacy.lang.mk.lemmatizer.index_table->self.lookups.get_table('lemma_index', {})
A:spacy.lang.mk.lemmatizer.exc_table->self.lookups.get_table('lemma_exc', {})
A:spacy.lang.mk.lemmatizer.rules_table->self.lookups.get_table('lemma_rules', {})
A:spacy.lang.mk.lemmatizer.index->self.lookups.get_table('lemma_index', {}).get(univ_pos, {})
A:spacy.lang.mk.lemmatizer.exceptions->self.lookups.get_table('lemma_exc', {}).get(univ_pos, {})
A:spacy.lang.mk.lemmatizer.rules->self.lookups.get_table('lemma_rules', {}).get(univ_pos, [])
A:spacy.lang.mk.lemmatizer.string->string.lower().lower()
A:spacy.lang.mk.lemmatizer.forms->list(OrderedDict.fromkeys(forms))
spacy.lang.mk.MacedonianLemmatizer(Lemmatizer)
spacy.lang.mk.lemmatizer.MacedonianLemmatizer(Lemmatizer)
spacy.lang.mk.lemmatizer.MacedonianLemmatizer.rule_lemmatize(self,token:Token)->List[str]


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/mk/stop_words.py----------------------------------------
A:spacy.lang.mk.stop_words.STOP_WORDS->set('\nÐ°\nÐ°Ð±Ñ€Ðµ\naÐ²\nÐ°Ð¸\nÐ°ÐºÐ¾\nÐ°Ð»Ð°Ð»Ð¾\nÐ°Ð¼\nÐ°Ð¼Ð°\nÐ°Ð¼Ð°Ð½\nÐ°Ð¼Ð¸\nÐ°Ð¼Ð¸Ð½\nÐ°Ð¿Ñ€Ð¸Ð»Ð¸-Ð»Ð¸-Ð»Ð¸\nÐ°Ñƒ\nÐ°ÑƒÑ…\nÐ°ÑƒÑ‡\nÐ°Ñ…\nÐ°Ñ…Ð°\nÐ°Ñ…Ð°-Ñ…Ð°\nÐ°Ñˆ\nÐ°ÑˆÐºÐ¾Ð»ÑÑƒÐ¼\nÐ°ÑˆÐºÐ¾Ð»ÑÑƒÐ½\nÐ°Ñ˜\nÐ°Ñ˜Ð´Ðµ\nÐ°Ñ˜Ñ\nÐ°ÑŸÐ°Ð±Ð°\nÐ±Ð°Ð²Ð½Ð¾\nÐ±Ð°Ð¼\nÐ±Ð°Ð¼-Ð±ÑƒÐ¼\nÐ±Ð°Ð¿\nÐ±Ð°Ñ€\nÐ±Ð°Ñ€Ðµ\nÐ±Ð°Ñ€ÐµÐ¼\nÐ±Ð°Ñƒ\nÐ±Ð°Ñƒ-Ð±Ð°Ñƒ\nÐ±Ð°Ñˆ\nÐ±Ð°Ñ˜\nÐ±Ðµ\nÐ±ÐµÐ°\nÐ±ÐµÐ²\nÐ±ÐµÐ²Ð¼Ðµ\nÐ±ÐµÐ²Ñ‚Ðµ\nÐ±ÐµÐ·\nÐ±ÐµÐ·Ð±ÐµÐ»Ð¸\nÐ±ÐµÐ·Ð´Ñ€ÑƒÐ³Ð¾\nÐ±ÐµÐ»ÐºÐ¸\nÐ±ÐµÑˆÐµ\nÐ±Ð¸\nÐ±Ð¸Ð´ÐµÑ˜ÑœÐ¸\nÐ±Ð¸Ð¼\nÐ±Ð¸Ñ\nÐ±Ð»Ð°\nÐ±Ð»Ð°Ð·Ðµ\nÐ±Ð¾Ð³Ð°Ð¼Ð¸\nÐ±Ð¾Ð¶ÐµÐ¼\nÐ±Ð¾Ñ†\nÐ±Ñ€Ð°Ð²Ð¾\nÐ±Ñ€Ð°Ð²Ð¾Ñ\nÐ±Ñ€Ðµ\nÐ±Ñ€ÐµÑ˜\nÐ±Ñ€Ð·Ð¾\nÐ±Ñ€Ð¸ÑˆÐºÐ°\nÐ±Ñ€Ñ€Ñ€\nÐ±Ñƒ\nÐ±ÑƒÐ¼\nÐ±ÑƒÑ„\nÐ±ÑƒÑ†\nÐ±ÑƒÑ˜Ñ€ÑƒÐ¼\nÐ²Ð°Ð°\nÐ²Ð°Ð¼\nÐ²Ð°Ñ€Ð°Ñ˜\nÐ²Ð°Ñ€Ð´Ð°\nÐ²Ð°Ñ\nÐ²Ð°Ñ˜\nÐ²Ðµ\nÐ²ÐµÐ»Ð°Ñ‚\nÐ²ÐµÐ»Ð¸\nÐ²ÐµÑ€ÑÑƒÑ\nÐ²ÐµÑœÐµ\nÐ²Ð¸\nÐ²Ð¸Ð°\nÐ²Ð¸Ð´Ð¸\nÐ²Ð¸Ðµ\nÐ²Ð¸ÑÑ‚Ð¸Ð½Ð°\nÐ²Ð¸Ñ‚Ð¾Ñ\nÐ²Ð½Ð°Ñ‚Ñ€Ðµ\nÐ²Ð¾\nÐ²Ð¾Ð·\nÐ²Ð¾Ð½\nÐ²Ð¿Ñ€Ð¾Ñ‡ÐµÐ¼\nÐ²Ñ€Ð²\nÐ²Ñ€ÐµÐ´\nÐ²Ñ€ÐµÐ¼Ðµ\nÐ²Ñ€Ð·\nÐ²ÑÑƒÑˆÐ½Ð¾ÑÑ‚\nÐ²Ñ‚Ð¾Ñ€\nÐ³Ð°Ð»Ð¸Ð±Ð°\nÐ³Ð¸\nÐ³Ð¸Ñ‚Ð»Ð°\nÐ³Ð¾\nÐ³Ð¾Ð´Ðµ\nÐ³Ð¾Ð´Ð¸ÑˆÐ½Ð¸Ðº\nÐ³Ð¾Ñ€Ðµ\nÐ³Ñ€Ð°\nÐ³ÑƒÑ†\nÐ³Ñ™Ñƒ\nÐ´Ð°\nÐ´Ð°Ð°Ð½\nÐ´Ð°Ð²Ð°\nÐ´Ð°Ð»\nÐ´Ð°Ð»Ð¸\nÐ´Ð°Ð½\nÐ´Ð²Ð°\nÐ´Ð²Ð°ÐµÑÐµÑ‚\nÐ´Ð²Ð°Ð½Ð°ÐµÑÐµÑ‚\nÐ´Ð²Ð°Ñ˜Ñ†Ð°\nÐ´Ð²Ðµ\nÐ´Ð²ÐµÑÑ‚Ðµ\nÐ´Ð²Ð¸Ð¶Ð°Ð¼\nÐ´Ð²Ð¸Ð¶Ð°Ñ‚\nÐ´Ð²Ð¸Ð¶Ð¸\nÐ´Ð²Ð¸Ð¶Ð¸Ð¼Ðµ\nÐ´Ð²Ð¸Ð¶Ð¸Ñ‚Ðµ\nÐ´Ð²Ð¸Ð¶Ð¸Ñˆ\nÐ´Ðµ\nÐ´ÐµÐ²ÐµÐ´ÐµÑÐµÑ‚\nÐ´ÐµÐ²ÐµÑ‚\nÐ´ÐµÐ²ÐµÑ‚Ð½Ð°ÐµÑÐµÑ‚\nÐ´ÐµÐ²ÐµÑ‚ÑÑ‚Ð¾Ñ‚Ð¸Ð½Ð¸\nÐ´ÐµÐ²ÐµÑ‚Ñ‚Ð¸\nÐ´ÐµÐºÐ°\nÐ´ÐµÐ»\nÐ´ÐµÐ»Ð¼Ð¸\nÐ´ÐµÐ¼ÐµÐº\nÐ´ÐµÑÐµÑ‚\nÐ´ÐµÑÐµÑ‚Ð¸Ð½Ð°\nÐ´ÐµÑÐµÑ‚Ñ‚Ð¸\nÐ´ÐµÑÐ¸Ñ‚Ð¸Ñ†Ð¸\nÐ´ÐµÑ˜Ð³Ð¸Ð´Ð¸\nÐ´ÐµÑ˜Ð´Ð¸\nÐ´Ð¸\nÐ´Ð¸Ð»Ð¼Ð¸\nÐ´Ð¸Ð½\nÐ´Ð¸Ð¿\nÐ´Ð½Ð¾\nÐ´Ð¾\nÐ´Ð¾Ð²Ð¾Ð»Ð½Ð¾\nÐ´Ð¾Ð´ÐµÐºÐ°\nÐ´Ð¾Ð´ÑƒÑˆÐ°\nÐ´Ð¾ÐºÐ°Ñ˜\nÐ´Ð¾ÐºÐ¾Ð»ÐºÑƒ\nÐ´Ð¾Ð¿Ñ€Ð°Ð²ÐµÐ½Ð¾\nÐ´Ð¾Ð¿Ñ€Ð°Ð²Ð¸\nÐ´Ð¾ÑÐ°Ð¼Ð¾Ñ‚Ð¸\nÐ´Ð¾ÑÑ‚Ð°\nÐ´Ñ€Ð¶Ð¸\nÐ´Ñ€Ð½\nÐ´Ñ€ÑƒÐ³\nÐ´Ñ€ÑƒÐ³Ð°\nÐ´Ñ€ÑƒÐ³Ð°Ñ‚Ð°\nÐ´Ñ€ÑƒÐ³Ð¸\nÐ´Ñ€ÑƒÐ³Ð¸Ð¾Ñ‚\nÐ´Ñ€ÑƒÐ³Ð¸Ñ‚Ðµ\nÐ´Ñ€ÑƒÐ³Ð¾\nÐ´Ñ€ÑƒÐ³Ð¾Ñ‚Ð¾\nÐ´ÑƒÐ¼\nÐ´ÑƒÑ€\nÐ´ÑƒÑ€Ð¸\nÐµ\nÐµÐ²Ð°Ð»Ð°\nÐµÐ²Ðµ\nÐµÐ²ÐµÑ‚\nÐµÐ³Ð°\nÐµÐ³Ð¸Ð´Ð¸\nÐµÐ´ÐµÐ½\nÐµÐ´Ð¸ÐºÐ¾Ñ˜ÑÐ¸\nÐµÐ´Ð¸Ð½Ð°ÐµÑÐµÑ‚\nÐµÐ´Ð¸Ð½ÑÑ‚Ð²ÐµÐ½Ð¾\nÐµÐ´Ð½Ð°Ñˆ\nÐµÐ´Ð½Ð¾\nÐµÐºÑÐ¸Ðº\nÐµÐ»Ð°\nÐµÐ»Ð±ÐµÑ‚Ðµ\nÐµÐ»ÐµÐ¼\nÐµÐ»Ð¸\nÐµÐ¼\nÐµÐ¼Ð¸\nÐµÐ½Ðµ\nÐµÑ‚Ðµ\nÐµÑƒÑ€ÐµÐºÐ°\nÐµÑ…\nÐµÑ˜\nÐ¶Ð¸Ð¼Ð¸\nÐ¶Ð¸Ñ‚Ð¸\nÐ·Ð°\nÐ·Ð°Ð²Ð°Ð»\nÐ·Ð°Ð²Ñ€ÑˆÐ¸\nÐ·Ð°Ð´\nÐ·Ð°Ð´ÐµÐºÐ°\nÐ·Ð°Ð´Ð¾Ð²Ð¾Ð»Ð½Ð°\nÐ·Ð°Ð´Ñ€Ð¶Ð¸\nÐ·Ð°ÐµÐ´Ð½Ð¾\nÐ·Ð°Ñ€\nÐ·Ð°Ñ€Ð°Ð´\nÐ·Ð°Ñ€Ð°Ð´Ð¸\nÐ·Ð°Ñ€Ðµ\nÐ·Ð°Ñ€ÐµÐ¼\nÐ·Ð°Ñ‚Ð¾Ð°\nÐ·Ð°ÑˆÑ‚Ð¾\nÐ·Ð³Ð¾Ñ€Ð°\nÐ·ÐµÐ¼Ð°\nÐ·ÐµÐ¼Ðµ\nÐ·ÐµÐ¼ÑƒÐ²Ð°\nÐ·ÐµÑ€\nÐ·Ð½Ð°Ñ‡Ð¸\nÐ·Ð¾ÑˆÑ‚Ð¾\nÐ·ÑƒÑ˜\nÐ¸\nÐ¸Ð°ÐºÐ¾\nÐ¸Ð·\nÐ¸Ð·Ð²ÐµÐ·ÐµÐ½\nÐ¸Ð·Ð³Ð»ÐµÐ´Ð°\nÐ¸Ð·Ð¼ÐµÑ“Ñƒ\nÐ¸Ð·Ð½Ð¾Ñ\nÐ¸Ð»Ð¸\nÐ¸Ð»Ð¸-Ð¸Ð»Ð¸\nÐ¸Ð»Ñ˜Ð°Ð´Ð°\nÐ¸Ð»Ñ˜Ð°Ð´Ð¸\nÐ¸Ð¼\nÐ¸Ð¼Ð°\nÐ¸Ð¼Ð°Ð°\nÐ¸Ð¼Ð°Ð°Ñ‚\nÐ¸Ð¼Ð°Ð²Ð¼Ðµ\nÐ¸Ð¼Ð°Ð²Ñ‚Ðµ\nÐ¸Ð¼Ð°Ð¼\nÐ¸Ð¼Ð°Ð¼Ðµ\nÐ¸Ð¼Ð°Ñ‚Ðµ\nÐ¸Ð¼Ð°Ñˆ\nÐ¸Ð¼Ð°ÑˆÐµ\nÐ¸Ð¼Ðµ\nÐ¸Ð¼ÐµÐ½Ð¾\nÐ¸Ð¼ÐµÐ½ÑƒÐ²Ð°\nÐ¸Ð¼Ð¿Ð»Ð¸Ñ†Ð¸Ñ€Ð°\nÐ¸Ð¼Ð¿Ð»Ð¸Ñ†Ð¸Ñ€Ð°Ð°Ñ‚\nÐ¸Ð¼Ð¿Ð»Ð¸Ñ†Ð¸Ñ€Ð°Ð¼\nÐ¸Ð¼Ð¿Ð»Ð¸Ñ†Ð¸Ñ€Ð°Ð¼Ðµ\nÐ¸Ð¼Ð¿Ð»Ð¸Ñ†Ð¸Ñ€Ð°Ñ‚Ðµ\nÐ¸Ð¼Ð¿Ð»Ð¸Ñ†Ð¸Ñ€Ð°Ñˆ\nÐ¸Ð½Ð°ÐºÑƒ\nÐ¸Ð½Ð´Ð¸Ñ†Ð¸Ñ€Ð°\nÐ¸ÑÐµÑ‡Ð¾Ðº\nÐ¸ÑÐºÐ»ÑƒÑ‡ÐµÐ½\nÐ¸ÑÐºÐ»ÑƒÑ‡ÐµÐ½Ð°\nÐ¸ÑÐºÐ»ÑƒÑ‡ÐµÐ½Ð¸\nÐ¸ÑÐºÐ»ÑƒÑ‡ÐµÐ½Ð¾\nÐ¸ÑÐºÐ¾Ñ€Ð¸ÑÑ‚ÐµÐ½\nÐ¸ÑÐºÐ¾Ñ€Ð¸ÑÑ‚ÐµÐ½Ð°\nÐ¸ÑÐºÐ¾Ñ€Ð¸ÑÑ‚ÐµÐ½Ð¸\nÐ¸ÑÐºÐ¾Ñ€Ð¸ÑÑ‚ÐµÐ½Ð¾\nÐ¸ÑÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¸\nÐ¸ÑÐºÑ€Ð°Ñ˜\nÐ¸ÑÑ‚Ð¸\nÐ¸ÑÑ‚Ð¾\nÐ¸Ñ‚Ð°ÐºÐ°\nÐ¸Ñ‚Ð½\nÐ¸Ñ…\nÐ¸Ñ…Ð°\nÐ¸Ñ…ÑƒÑƒ\nÐ¸Ñˆ\nÐ¸ÑˆÐ°Ð»Ð°\nÐ¸Ñ˜\nÐºÐ°\nÐºÐ°Ð´Ðµ\nÐºÐ°Ð¶ÑƒÐ²Ð°\nÐºÐ°ÐºÐ¾\nÐºÐ°ÐºÐ¾Ð²\nÐºÐ°Ð¼Ð¾Ð»Ð¸\nÐºÐ°Ñ˜\nÐºÐ²Ð°\nÐºÐ¸\nÐºÐ¸Ñ‚\nÐºÐ»Ð¾\nÐºÐ»ÑƒÐ¼\nÐºÐ¾Ð³Ð°\nÐºÐ¾Ð³Ð¾\nÐºÐ¾Ð³Ð¾-Ð³Ð¾Ð´Ðµ\nÐºÐ¾Ðµ\nÐºÐ¾Ð¸\nÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾\nÐºÐ¾Ð»Ð¸Ñ‡Ð¸Ð½Ð°\nÐºÐ¾Ð»ÐºÑƒ\nÐºÐ¾Ð¼Ñƒ\nÐºÐ¾Ð½\nÐºÐ¾Ñ€Ð¸ÑÑ‚ÐµÐ½Ð°\nÐºÐ¾Ñ€Ð¸ÑÑ‚ÐµÐ½Ð¸\nÐºÐ¾Ñ€Ð¸ÑÑ‚ÐµÐ½Ð¾\nÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¸\nÐºÐ¾Ñ‚\nÐºÐ¾Ñ‚Ñ€Ñ€\nÐºÐ¾Ñˆ-ÐºÐ¾Ñˆ\nÐºÐ¾Ñ˜\nÐºÐ¾Ñ˜Ð°\nÐºÐ¾Ñ˜Ð·Ð½Ð°Ðµ\nÐºÐ¾Ñ˜ÑˆÑ‚Ð¾\nÐºÑ€-ÐºÑ€-ÐºÑ€\nÐºÑ€Ð°Ñ˜\nÐºÑ€ÐµÐº\nÐºÑ€Ð·\nÐºÑ€Ðº\nÐºÑ€Ñ†\nÐºÑƒÐºÑƒ\nÐºÑƒÐºÑƒÑ€Ð¸Ð³Ñƒ\nÐºÑƒÑˆ\nÐ»Ðµ\nÐ»ÐµÐ±Ð°Ð¼Ð¸\nÐ»ÐµÐ»Ðµ\nÐ»ÐµÐ»Ð¸\nÐ»Ð¸\nÐ»Ð¸Ð´Ñƒ\nÐ»ÑƒÐ¿\nÐ¼Ð°\nÐ¼Ð°ÐºÐ°Ñ€\nÐ¼Ð°Ð»ÐºÑƒ\nÐ¼Ð°Ñ€Ñˆ\nÐ¼Ð°Ñ‚\nÐ¼Ð°Ñ†\nÐ¼Ð°ÑˆÐ°Ð»Ð°\nÐ¼Ðµ\nÐ¼ÐµÐ½Ðµ\nÐ¼ÐµÑÑ‚Ð¾\nÐ¼ÐµÑ“Ñƒ\nÐ¼ÐµÑ“ÑƒÐ²Ñ€ÐµÐ¼Ðµ\nÐ¼ÐµÑ“ÑƒÑ‚Ð¾Ð°\nÐ¼Ð¸\nÐ¼Ð¾Ðµ\nÐ¼Ð¾Ð¶Ðµ\nÐ¼Ð¾Ð¶ÐµÐ±Ð¸\nÐ¼Ð¾Ð»Ð°Ð¼\nÐ¼Ð¾Ð»Ð¸\nÐ¼Ð¾Ñ€\nÐ¼Ð¾Ñ€Ð°\nÐ¼Ð¾Ñ€Ðµ\nÐ¼Ð¾Ñ€Ð¸\nÐ¼Ñ€Ð°Ð·ÐµÑ†\nÐ¼Ñƒ\nÐ¼ÑƒÐºÐ»ÐµÑ†\nÐ¼ÑƒÑ‚Ð»Ð°Ðº\nÐ¼ÑƒÑ†\nÐ¼Ñ˜Ð°Ñƒ\nÐ½Ð°\nÐ½Ð°Ð²Ð¸Ð´ÑƒÐ¼\nÐ½Ð°Ð²Ð¸ÑÑ‚Ð¸Ð½Ð°\nÐ½Ð°Ð´\nÐ½Ð°Ð´Ð²Ð¾Ñ€\nÐ½Ð°Ð·Ð°Ð´\nÐ½Ð°ÐºÐ°Ñ˜\nÐ½Ð°ÐºÑ€Ð°Ñ˜\nÐ½Ð°Ð»Ð¸\nÐ½Ð°Ð¼\nÐ½Ð°Ð¼ÐµÑÑ‚Ð¾\nÐ½Ð°Ð¾ÐºÐ¾Ð»Ñƒ\nÐ½Ð°Ð¿Ñ€Ð°Ð²ÐµÐ½Ð¾\nÐ½Ð°Ð¿Ñ€Ð°Ð²Ð¸\nÐ½Ð°Ð¿Ñ€ÐµÐ´\nÐ½Ð°Ñ\nÐ½Ð°ÑÐ¿Ð¾Ñ€ÐµÐ´\nÐ½Ð°ÑÐ¿Ñ€ÐµÐ¼Ð°\nÐ½Ð°ÑÐ¿Ñ€Ð¾Ñ‚Ð¸\nÐ½Ð°ÑÑ€ÐµÐ´\nÐ½Ð°Ñ‚Ð°Ð¼Ñƒ\nÐ½Ð°Ñ‚ÐµÐ¼Ð°\nÐ½Ð°Ñ‡Ð¸Ð½\nÐ½Ð°Ñˆ\nÐ½Ð°ÑˆÐ°\nÐ½Ð°ÑˆÐµ\nÐ½Ð°ÑˆÐ¸\nÐ½Ð°Ñ˜\nÐ½Ð°Ñ˜Ð´Ð¾Ñ†Ð½Ð°\nÐ½Ð°Ñ˜Ð¼Ð°Ð»ÐºÑƒ\nÐ½Ð°Ñ˜Ð¼Ð½Ð¾Ð³Ñƒ\nÐ½Ðµ\nÐ½ÐµÐ°\nÐ½ÐµÐ³Ð¾\nÐ½ÐµÐ³Ð¾Ð²\nÐ½ÐµÐ³Ð¾Ð²Ð°\nÐ½ÐµÐ³Ð¾Ð²Ð¸\nÐ½ÐµÐ³Ð¾Ð²Ð¾\nÐ½ÐµÐ·Ðµ\nÐ½ÐµÐºÐ°\nÐ½ÐµÐºÐ°Ð´Ðµ\nÐ½ÐµÐºÐ°ÐºÐ¾\nÐ½ÐµÐºÐ°ÐºÐ¾Ð²\nÐ½ÐµÐºÐ¾Ð³Ð¾\nÐ½ÐµÐºÐ¾Ðµ\nÐ½ÐµÐºÐ¾Ð¸\nÐ½ÐµÐºÐ¾Ð»ÐºÑƒ\nÐ½ÐµÐºÐ¾Ð¼Ñƒ\nÐ½ÐµÐºÐ¾Ñ˜\nÐ½ÐµÐºÐ¾Ñ˜ÑÐ¸\nÐ½ÐµÐ»Ð¸\nÐ½ÐµÐ¼Ð¾Ñ˜\nÐ½ÐµÐ¼Ñƒ\nÐ½ÐµÐ¾Ñ‚Ð¸\nÐ½ÐµÑ‡Ð¸Ñ˜\nÐ½ÐµÑˆÑ‚Ð¾\nÐ½ÐµÑ˜Ð·Ðµ\nÐ½ÐµÑ˜Ð·Ð¸Ð½\nÐ½ÐµÑ˜Ð·Ð¸Ð½Ð¸\nÐ½ÐµÑ˜Ð·Ð¸Ð½Ð¾\nÐ½ÐµÑ˜ÑÐµ\nÐ½Ð¸\nÐ½Ð¸Ð²\nÐ½Ð¸Ð²ÐµÐ½\nÐ½Ð¸Ð²Ð½Ð°\nÐ½Ð¸Ð²Ð½Ð¸\nÐ½Ð¸Ð²Ð½Ð¾\nÐ½Ð¸Ðµ\nÐ½Ð¸Ð·\nÐ½Ð¸ÐºÐ°Ð´Ðµ\nÐ½Ð¸ÐºÐ°ÐºÐ¾\nÐ½Ð¸ÐºÐ¾Ð³Ð°Ñˆ\nÐ½Ð¸ÐºÐ¾Ð³Ð¾\nÐ½Ð¸ÐºÐ¾Ð¼Ñƒ\nÐ½Ð¸ÐºÐ¾Ñ˜\nÐ½Ð¸Ð¼\nÐ½Ð¸Ñ‚Ð¸\nÐ½Ð¸Ñ‚Ð¾\nÐ½Ð¸Ñ‚Ñƒ\nÐ½Ð¸Ñ‡Ð¸Ñ˜\nÐ½Ð¸ÑˆÑ‚Ð¾\nÐ½Ð¾\nÐ½Ñ\nÐ¾\nÐ¾Ð±Ñ€\nÐ¾Ð²Ð°\nÐ¾Ð²Ð°-Ð¾Ð½Ð°\nÐ¾Ð²Ð°Ð°\nÐ¾Ð²Ð°Ñ˜\nÐ¾Ð²Ð´Ðµ\nÐ¾Ð²ÐµÐ³Ð°\nÐ¾Ð²Ð¸Ðµ\nÐ¾Ð²Ð¾Ñ˜\nÐ¾Ð´\nÐ¾Ð´Ð°Ð²Ð´Ðµ\nÐ¾Ð´Ð¸\nÐ¾Ð´Ð½ÐµÑÑƒÐ²Ð°\nÐ¾Ð´Ð½Ð¾ÑÐ½Ð¾\nÐ¾Ð´Ð¾ÑˆÑ‚Ð¾\nÐ¾ÐºÐ¾Ð»Ñƒ\nÐ¾Ð»ÐµÐ»Ðµ\nÐ¾Ð»ÐºÐ°Ñ†Ð¾Ðº\nÐ¾Ð½\nÐ¾Ð½Ð°\nÐ¾Ð½Ð°Ð°\nÐ¾Ð½Ð°ÐºÐ°\nÐ¾Ð½Ð°ÐºÐ¾Ð²\nÐ¾Ð½Ð´Ðµ\nÐ¾Ð½Ð¸\nÐ¾Ð½Ð¸Ðµ\nÐ¾Ð½Ð¾\nÐ¾Ð½Ð¾Ñ˜\nÐ¾Ð¿\nÐ¾ÑÐ²ÐµÐ¼\nÐ¾ÑÐ²ÐµÐ½\nÐ¾ÑÐµÐ¼\nÐ¾ÑÐ¼Ð¸\nÐ¾ÑÑƒÐ¼\nÐ¾ÑÑƒÐ¼Ð´ÐµÑÐµÑ‚\nÐ¾ÑÑƒÐ¼Ð½Ð°ÐµÑÐµÑ‚\nÐ¾ÑÑƒÐ¼ÑÑ‚Ð¾Ñ‚Ð¸Ñ‚Ð½Ð¸\nÐ¾Ñ‚Ð°Ð´Ðµ\nÐ¾Ñ‚Ð¸\nÐ¾Ñ‚ÐºÐ°ÐºÐ¾\nÐ¾Ñ‚ÐºÐ°Ñ˜\nÐ¾Ñ‚ÐºÐ¾Ð³Ð°\nÐ¾Ñ‚ÐºÐ¾Ð»ÐºÑƒ\nÐ¾Ñ‚Ñ‚Ð°Ð¼Ñƒ\nÐ¾Ñ‚Ñ‚ÑƒÐºÐ°\nÐ¾Ñ„\nÐ¾Ñ…\nÐ¾Ñ˜\nÐ¿Ð°\nÐ¿Ð°Ðº\nÐ¿Ð°Ð¿Ð°\nÐ¿Ð°Ñ€Ð´Ð¾Ð½\nÐ¿Ð°Ñ‚Ðµ-ÑœÑƒÑ‚Ðµ\nÐ¿Ð°Ñ‚Ð¸\nÐ¿Ð°Ñƒ\nÐ¿Ð°Ñ‡Ðµ\nÐ¿ÐµÐµÑÐµÑ‚\nÐ¿ÐµÐºÐ¸\nÐ¿ÐµÑ‚\nÐ¿ÐµÑ‚Ð½Ð°ÐµÑÐµÑ‚\nÐ¿ÐµÑ‚ÑÑ‚Ð¾Ñ‚Ð¸Ð½Ð¸\nÐ¿ÐµÑ‚Ñ‚Ð¸\nÐ¿Ð¸\nÐ¿Ð¸-Ð¿Ð¸\nÐ¿Ð¸Ñ\nÐ¿Ð»Ð°Ñ\nÐ¿Ð»ÑƒÑ\nÐ¿Ð¾\nÐ¿Ð¾Ð±Ð°Ð²Ð½Ð¾\nÐ¿Ð¾Ð±Ð»Ð¸ÑÐºÑƒ\nÐ¿Ð¾Ð±Ñ€Ð·Ð¾\nÐ¿Ð¾Ð±ÑƒÐ½Ð¸\nÐ¿Ð¾Ð²ÐµÑœÐµ\nÐ¿Ð¾Ð²Ñ‚Ð¾Ñ€Ð½Ð¾\nÐ¿Ð¾Ð´\nÐ¿Ð¾Ð´Ð°Ð»ÐµÐºÑƒ\nÐ¿Ð¾Ð´Ð¾Ð»Ñƒ\nÐ¿Ð¾Ð´Ð¾Ñ†Ð½Ð°\nÐ¿Ð¾Ð´Ñ€ÑƒÐ³Ð¾\nÐ¿Ð¾Ð·Ð°Ð´Ð¸\nÐ¿Ð¾Ð¸Ð½Ð°ÐºÐ²Ð°\nÐ¿Ð¾Ð¸Ð½Ð°ÐºÐ²Ð¸\nÐ¿Ð¾Ð¸Ð½Ð°ÐºÐ²Ð¾\nÐ¿Ð¾Ð¸Ð½Ð°ÐºÐ¾Ð²\nÐ¿Ð¾Ð¸Ð½Ð°ÐºÑƒ\nÐ¿Ð¾ÐºÐ°Ð¶Ðµ\nÐ¿Ð¾ÐºÐ°Ð¶ÑƒÐ²Ð°\nÐ¿Ð¾ÐºÑ€Ð°Ñ˜\nÐ¿Ð¾Ð»Ð½Ð¾\nÐ¿Ð¾Ð¼Ð°Ð»ÐºÑƒ\nÐ¿Ð¾Ð¼ÐµÑ“Ñƒ\nÐ¿Ð¾Ð½Ð°Ñ‚Ð°Ð¼Ñƒ\nÐ¿Ð¾Ð½ÐµÐºÐ¾Ð³Ð°Ñˆ\nÐ¿Ð¾Ð½ÐµÐºÐ¾Ñ˜\nÐ¿Ð¾Ñ€Ð°Ð´Ð¸\nÐ¿Ð¾Ñ€Ð°Ð·Ð»Ð¸Ñ‡ÐµÐ½\nÐ¿Ð¾Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ð°\nÐ¿Ð¾Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ð¸\nÐ¿Ð¾Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ð¾\nÐ¿Ð¾ÑÐµÐ´ÑƒÐ²Ð°\nÐ¿Ð¾ÑÐ»Ðµ\nÐ¿Ð¾ÑÐ»ÐµÐ´ÐµÐ½\nÐ¿Ð¾ÑÐ»ÐµÐ´Ð½Ð°\nÐ¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸\nÐ¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¾\nÐ¿Ð¾ÑÐ¿Ð¾Ñ€Ð¾\nÐ¿Ð¾Ñ‚ÐµÐ³\nÐ¿Ð¾Ñ‚Ð¾Ð°\nÐ¿Ð¾ÑˆÐ¸Ñ€Ð¾ÐºÐ¾\nÐ¿Ñ€Ð°Ð²Ð¸\nÐ¿Ñ€Ð°Ð·Ð½Ð¾\nÐ¿Ñ€Ð²\nÐ¿Ñ€ÐµÐ´\nÐ¿Ñ€ÐµÐ·\nÐ¿Ñ€ÐµÐºÑƒ\nÐ¿Ñ€ÐµÑ‚ÐµÐ¶Ð½Ð¾\nÐ¿Ñ€ÐµÑ‚Ñ…Ð¾Ð´ÐµÐ½\nÐ¿Ñ€ÐµÑ‚Ñ…Ð¾Ð´Ð½Ð°\nÐ¿Ñ€ÐµÑ‚Ñ…Ð¾Ð´Ð½Ð¸\nÐ¿Ñ€ÐµÑ‚Ñ…Ð¾Ð´Ð½Ð¸Ðº\nÐ¿Ñ€ÐµÑ‚Ñ…Ð¾Ð´Ð½Ð¾\nÐ¿Ñ€Ð¸\nÐ¿Ñ€Ð¸ÑÐ²Ð¾Ð¸\nÐ¿Ñ€Ð¸Ñ‚Ð¾Ð°\nÐ¿Ñ€Ð¸Ñ‡Ð¸Ð½ÑƒÐ²Ð°\nÐ¿Ñ€Ð¸Ñ˜Ð°Ñ‚Ð½Ð¾\nÐ¿Ñ€Ð¾ÑÑ‚Ð¾\nÐ¿Ñ€Ð¾Ñ‚Ð¸Ð²\nÐ¿Ñ€Ñ€\nÐ¿ÑÑ‚\nÐ¿ÑƒÐº\nÐ¿ÑƒÑÑ‚Ð¾\nÐ¿ÑƒÑ„\nÐ¿ÑƒÑ˜\nÐ¿Ñ„ÑƒÑ˜\nÐ¿ÑˆÑ‚\nÑ€Ð°Ð´Ð¸\nÑ€Ð°Ð·Ð»Ð¸Ñ‡ÐµÐ½\nÑ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ð°\nÑ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ð¸\nÑ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ð¾\nÑ€Ð°Ð·Ð½Ð¸\nÑ€Ð°Ð·Ð¾Ñ€ÑƒÐ¶ÐµÐ½\nÑ€Ð°Ð·Ñ€ÐµÐ´Ð»Ð¸Ð²\nÑ€Ð°Ð¼ÐºÐ¸Ñ‚Ðµ\nÑ€Ð°Ð¼Ð½Ð¾Ð¾Ð±Ñ€Ð°Ð·Ð½Ð¾\nÑ€Ð°ÑÑ‚Ñ€ÐµÐ²Ð¾Ð¶ÐµÐ½Ð¾\nÑ€Ð°ÑÑ‚Ñ€ÐµÐ¿ÐµÑ€ÐµÐ½Ð¾\nÑ€Ð°ÑÑ‡ÑƒÐ²ÑÑ‚Ð²ÑƒÐ²Ð°Ð½Ð¾\nÑ€Ð°Ñ‚Ð¾Ð±Ð¾Ñ€Ð½Ð¾\nÑ€ÐµÑ‡Ðµ\nÑ€Ð¾Ð´ÐµÐ½\nÑ\nÑÐ°ÐºÐ°Ð½\nÑÐ°Ð¼\nÑÐ°Ð¼Ð°\nÑÐ°Ð¼Ð¸\nÑÐ°Ð¼Ð¸Ñ‚Ðµ\nÑÐ°Ð¼Ð¾\nÑÐ°Ð¼Ð¾Ñ‚Ð¸\nÑÐ²Ð¾Ðµ\nÑÐ²Ð¾Ð¸\nÑÐ²Ð¾Ñ˜\nÑÐ²Ð¾Ñ˜Ð°\nÑÐµ\nÑÐµÐ±Ðµ\nÑÐµÐ±ÐµÑÐ¸\nÑÐµÐ³Ð°\nÑÐµÐ´Ð¼Ð¸\nÑÐµÐ´ÑƒÐ¼\nÑÐµÐ´ÑƒÐ¼Ð´ÐµÑÐµÑ‚\nÑÐµÐ´ÑƒÐ¼Ð½Ð°ÐµÑÐµÑ‚\nÑÐµÐ´ÑƒÐ¼ÑÑ‚Ð¾Ñ‚Ð¸Ð½Ð¸\nÑÐµÐºÐ°Ð´Ðµ\nÑÐµÐºÐ°ÐºÐ¾Ð²\nÑÐµÐºÐ¸\nÑÐµÐºÐ¾Ð³Ð°Ñˆ\nÑÐµÐºÐ¾Ð³Ð¾\nÑÐµÐºÐ¾Ð¼Ñƒ\nÑÐµÐºÐ¾Ñ˜\nÑÐµÐºÐ¾Ñ˜Ð´Ð½ÐµÐ²Ð½Ð¾\nÑÐµÐ¼\nÑÐµÐ½ÐµÑˆÑ‚Ð¾\nÑÐµÐ¿Ð°Ðº\nÑÐµÑ€Ð¸Ð¾Ð·ÐµÐ½\nÑÐµÑ€Ð¸Ð¾Ð·Ð½Ð°\nÑÐµÑ€Ð¸Ð¾Ð·Ð½Ð¸\nÑÐµÑ€Ð¸Ð¾Ð·Ð½Ð¾\nÑÐµÑ‚\nÑÐµÑ‡Ð¸Ñ˜\nÑÐµÑˆÑ‚Ð¾\nÑÐ¸\nÑÐ¸ÐºÑ‚ÐµÑ€\nÑÐ¸Ð¾Ñ‚\nÑÐ¸Ð¿\nÑÐ¸Ñ€ÐµÑ‡\nÑÐ¸Ñ‚Ðµ\nÑÐ¸Ñ‡ÐºÐ¾\nÑÐºÐ¾Ðº\nÑÐºÐ¾Ñ€Ð¾\nÑÐºÑ€Ñ†\nÑÐ»ÐµÐ´Ð±ÐµÐ½Ð¸Ðº\nÑÐ»ÐµÐ´Ð±ÐµÐ½Ð¸Ñ‡ÐºÐ°\nÑÐ»ÐµÐ´ÐµÐ½\nÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»Ð½Ð¾\nÑÐ»ÐµÐ´ÑÑ‚Ð²ÐµÐ½Ð¾\nÑÐ¼Ðµ\nÑÐ¾\nÑÐ¾Ð½Ðµ\nÑÐ¾Ð¿ÑÑ‚Ð²ÐµÐ½\nÑÐ¾Ð¿ÑÑ‚Ð²ÐµÐ½Ð°\nÑÐ¾Ð¿ÑÑ‚Ð²ÐµÐ½Ð¸\nÑÐ¾Ð¿ÑÑ‚Ð²ÐµÐ½Ð¾\nÑÐ¾ÑÐµ\nÑÐ¾ÑÐµÐ¼\nÑÐ¿Ð¾Ð»Ð°Ñ˜\nÑÐ¿Ð¾Ñ€ÐµÐ´\nÑÐ¿Ð¾Ñ€Ð¾\nÑÐ¿Ñ€ÐµÐ¼Ð°\nÑÐ¿Ñ€Ð¾Ñ‚Ð¸\nÑÐ¿Ñ€Ð¾Ñ‚Ð¸Ð²\nÑÑ€ÐµÐ´\nÑÑ€ÐµÐ´Ðµ\nÑÑ€ÐµÑœÐ½Ð¾\nÑÑ€Ð¾Ñ‡ÐµÐ½\nÑÑÑ‚\nÑÑ‚Ð°Ð²Ð°\nÑÑ‚Ð°Ð²Ð°Ð°Ñ‚\nÑÑ‚Ð°Ð²Ð°Ð¼\nÑÑ‚Ð°Ð²Ð°Ð¼Ðµ\nÑÑ‚Ð°Ð²Ð°Ñ‚Ðµ\nÑÑ‚Ð°Ð²Ð°Ñˆ\nÑÑ‚Ð°Ð²Ð¸\nÑÑ‚Ðµ\nÑÑ‚Ð¾\nÑÑ‚Ð¾Ð¿\nÑÑ‚Ñ€Ð°Ð½Ð°\nÑÑƒÐ¼\nÑÑƒÐ¼Ð°\nÑÑƒÐ¿ÐµÑ€\nÑÑƒÑ\nÑÑ\nÑ‚Ð°\nÑ‚Ð°Ð°\nÑ‚Ð°ÐºÐ°\nÑ‚Ð°ÐºÐ²Ð°\nÑ‚Ð°ÐºÐ²Ð¸\nÑ‚Ð°ÐºÐ¾Ð²\nÑ‚Ð°Ð¼Ð°Ð¼\nÑ‚Ð°Ð¼Ñƒ\nÑ‚Ð°Ð½Ð³Ð°Ñ€-Ð¼Ð°Ð½Ð³Ð°Ñ€\nÑ‚Ð°Ð½Ð´Ð°Ñ€-Ð¼Ð°Ð½Ð´Ð°Ñ€\nÑ‚Ð°Ð¿\nÑ‚Ð²Ð¾Ðµ\nÑ‚Ðµ\nÑ‚ÐµÐ±Ðµ\nÑ‚ÐµÐ±ÐµÐºÐ°\nÑ‚ÐµÐº\nÑ‚ÐµÐºÐ¾Ñ‚\nÑ‚Ð¸\nÑ‚Ð¸Ðµ\nÑ‚Ð¸Ð·Ðµ\nÑ‚Ð¸Ðº-Ñ‚Ð°Ðº\nÑ‚Ð¸ÐºÐ¸\nÑ‚Ð¾Ð°\nÑ‚Ð¾Ð³Ð°Ñˆ\nÑ‚Ð¾Ñ˜\nÑ‚Ñ€Ð°Ðº\nÑ‚Ñ€Ð°ÐºÐ°-Ñ‚Ñ€ÑƒÐºÐ°\nÑ‚Ñ€Ð°Ñ\nÑ‚Ñ€ÐµÐ±Ð°\nÑ‚Ñ€ÐµÑ‚\nÑ‚Ñ€Ð¸\nÑ‚Ñ€Ð¸ÐµÑÐµÑ‚\nÑ‚Ñ€Ð¸Ð½Ð°ÐµÑÑ‚\nÑ‚Ñ€Ð¸ÑÑ‚Ð°\nÑ‚Ñ€ÑƒÐ¿\nÑ‚Ñ€ÑƒÐ¿Ð°\nÑ‚Ñ€ÑƒÑ\nÑ‚Ñƒ\nÑ‚ÑƒÐºÐ°\nÑ‚ÑƒÐºÑƒ\nÑ‚ÑƒÐºÑƒÑˆÑ‚Ð¾\nÑ‚ÑƒÑ„\nÑƒ\nÑƒÐ°\nÑƒÐ±Ð°Ð²Ð¾\nÑƒÐ²Ð¸\nÑƒÐ¶Ð°ÑÐ½Ð¾\nÑƒÐ·\nÑƒÑ€Ð°\nÑƒÑƒ\nÑƒÑ„\nÑƒÑ…Ð°\nÑƒÑˆ\nÑƒÑˆÑ‚Ðµ\nÑ„Ð°Ð·ÐµÐ½\nÑ„Ð°Ð»Ð°\nÑ„Ð¸Ð»\nÑ„Ð¸Ð»Ð°Ð½\nÑ„Ð¸Ñ\nÑ„Ð¸Ñƒ\nÑ„Ð¸Ñ™Ð°Ð½\nÑ„Ð¾Ð±\nÑ„Ð¾Ð½\nÑ…Ð°\nÑ…Ð°-Ñ…Ð°\nÑ…Ðµ\nÑ…ÐµÑ˜\nÑ…ÐµÑ˜\nÑ…Ð¸\nÑ…Ð¼\nÑ…Ð¾\nÑ†Ð°Ðº\nÑ†Ð°Ð¿\nÑ†ÐµÐ»Ð¸Ð½Ð°\nÑ†ÐµÐ»Ð¾\nÑ†Ð¸Ð³Ñƒ-Ð»Ð¸Ð³Ñƒ\nÑ†Ð¸Ñ†\nÑ‡ÐµÐºÐ°Ñ˜\nÑ‡ÐµÑÑ‚Ð¾\nÑ‡ÐµÑ‚Ð²Ñ€Ñ‚\nÑ‡ÐµÑ‚Ð¸Ñ€Ð¸\nÑ‡ÐµÑ‚Ð¸Ñ€Ð¸ÐµÑÐµÑ‚\nÑ‡ÐµÑ‚Ð¸Ñ€Ð¸Ð½Ð°ÐµÑÐµÑ‚\nÑ‡ÐµÑ‚Ð¸Ñ€ÑÑ‚Ð¾Ñ‚Ð¸Ð½Ð¸\nÑ‡Ð¸Ðµ\nÑ‡Ð¸Ð¸\nÑ‡Ð¸Ðº\nÑ‡Ð¸Ðº-Ñ‡Ð¸Ñ€Ð¸Ðº\nÑ‡Ð¸Ð½Ð¸\nÑ‡Ð¸Ñˆ\nÑ‡Ð¸Ñ˜\nÑ‡Ð¸Ñ˜Ð°\nÑ‡Ð¸Ñ˜ÑˆÑ‚Ð¾\nÑ‡ÐºÑ€Ð°Ð¿\nÑ‡Ð¾Ð¼Ñƒ\nÑ‡ÑƒÐº\nÑ‡ÑƒÐºÑˆ\nÑ‡ÑƒÐ¼Ñƒ\nÑ‡ÑƒÐ½ÐºÐ¸\nÑˆÐµÐµÑÐµÑ‚\nÑˆÐµÑÐ½Ð°ÐµÑÐµÑ‚\nÑˆÐµÑÑ‚\nÑˆÐµÑÑ‚Ð¸\nÑˆÐµÑÑ‚Ð¾Ñ‚Ð¸Ð½Ð¸\nÑˆÐ¸Ñ€ÑƒÐ¼\nÑˆÐ»Ð°Ðº\nÑˆÐ»Ð°Ð¿\nÑˆÐ»Ð°Ð¿Ð°-ÑˆÐ»ÑƒÐ¿Ð°\nÑˆÐ»ÑƒÐ¿\nÑˆÐ¼Ñ€Ðº\nÑˆÑ‚Ð¾\nÑˆÑ‚Ð¾Ð³Ð¾Ð´Ðµ\nÑˆÑ‚Ð¾Ð¼\nÑˆÑ‚Ð¾Ñ‚ÑƒÐºÑƒ\nÑˆÑ‚Ñ€Ð°Ðº\nÑˆÑ‚Ñ€Ð°Ð¿\nÑˆÑ‚Ñ€Ð°Ð¿-ÑˆÑ‚Ñ€ÑƒÐ¿\nÑˆÑƒÑœÑƒÑ€\nÑ“Ð¸Ð´Ð¸\nÑ“Ð¾Ð°\nÑ“Ð¾Ð°Ð¼Ð¸Ñ‚Ð¸\nÑ•Ð°Ð½\nÑ•Ðµ\nÑ•Ð¸Ð½\nÑ˜Ð°\nÑ˜Ð°Ð´ÐµÑ†\nÑ˜Ð°Ð·Ðµ\nÑ˜Ð°Ð»Ð¸\nÑ˜Ð°Ñ\nÑ˜Ð°ÑÐºÐ°\nÑ˜Ð¾Ðº\nÑœÐµ\nÑœÐµÑˆÐºÐ¸\nÑ\nÑŸÐ°Ð³Ð°Ñ€Ð°-Ð¼Ð°Ð³Ð°Ñ€Ð°\nÑŸÐ°Ð½Ð°Ð¼\nÑŸÐ¸Ð²-ÑŸÐ¸Ð²\n    '.split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/mk/tokenizer_exceptions.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/mk/__init__.py----------------------------------------
A:spacy.lang.mk.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.mk.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS, TOKENIZER_EXCEPTIONS)
A:spacy.lang.mk.__init__.lookups->Lookups()
spacy.lang.mk.__init__.Macedonian(Language)
spacy.lang.mk.__init__.MacedonianDefaults(BaseDefaults)
spacy.lang.mk.__init__.MacedonianDefaults.create_lemmatizer(cls,nlp=None,lookups=None)
spacy.lang.mk.__init__.make_lemmatizer(nlp:Language,model:Optional[Model],name:str,mode:str,overwrite:bool,scorer:Optional[Callable])


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/mk/lex_attrs.py----------------------------------------
A:spacy.lang.mk.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.mk.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
A:spacy.lang.mk.lex_attrs.text_lower->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').lower()
spacy.lang.mk.lex_attrs.like_num(text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/pt/examples.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/pt/stop_words.py----------------------------------------
A:spacy.lang.pt.stop_words.STOP_WORDS->set('\na Ã  Ã s Ã¡rea acerca ademais adeus agora ainda algo algumas alguns ali alÃ©m ambas ambos antes\nao aos apenas apoia apoio apontar apÃ³s aquela aquelas aquele aqueles aqui aquilo\nas assim atravÃ©s atrÃ¡s atÃ© aÃ­\n\nbaixo bastante bem boa bom breve\n\ncada caminho catorze cedo cento certamente certeza cima cinco coisa com como\ncomprida comprido conhecida conhecido conselho contra contudo corrente cuja\ncujo custa cÃ¡\n\nda daquela daquele dar das de debaixo demais dentro depois des desde dessa desse\ndesta deste deve devem deverÃ¡ dez dezanove dezasseis dezassete dezoito diante\ndireita disso diz dizem dizer do dois dos doze duas dÃ¡ dÃ£o\n\ne Ã© Ã©s ela elas ele eles em embora enquanto entre entÃ£o era essa essas esse esses esta\nestado estar estarÃ¡ estas estava este estes esteve estive estivemos estiveram\nestiveste estivestes estou estÃ¡ estÃ¡s estÃ£o eu eventual exemplo\n\nfalta farÃ¡ favor faz fazeis fazem fazemos fazer fazes fazia faÃ§o fez fim final\nfoi fomos for fora foram forma foste fostes fui\n\ngeral grande grandes grupo\n\ninclusive iniciar inicio ir irÃ¡ isso isto\n\njÃ¡\n\nlado lhe ligado local logo longe lugar lÃ¡\n\nmaior maioria maiorias mais mal mas me meio menor menos meses mesmo meu meus mil\nminha minhas momento muito muitos mÃ¡ximo mÃªs\n\nna nada naquela naquele nas nem nenhuma nessa nesse nesta neste no nos nossa\nnossas nosso nossos nova novas nove novo novos num numa nunca nuns nÃ£o nÃ­vel nÃ³s\nnÃºmero nÃºmeros\n\no obrigada obrigado oitava oitavo oito onde ontem onze ora os ou outra outras outros\n\npara parece parte partir pegar pela pelas pelo pelos perto pode podem poder poderÃ¡\npodia pois ponto pontos por porquanto porque porquÃª portanto porÃ©m posiÃ§Ã£o\npossivelmente posso possÃ­vel pouca pouco povo primeira primeiro prÃ³prio prÃ³xima\nprÃ³ximo puderam pÃ´de pÃµe pÃµem\n\nquais qual qualquer quando quanto quarta quarto quatro que quem quer querem quero\nquestÃ£o quieta quieto quinta quinto quinze quÃª\n\nrelaÃ§Ã£o\n\nsabe saber se segunda segundo sei seis sem sempre ser seria sete seu seus sexta\nsexto sim sistema sob sobre sois somente somos sou sua suas sÃ£o sÃ©tima sÃ©timo sÃ³\n\ntais tal talvez tambÃ©m tanta tanto tarde te tem temos tempo tendes tenho tens\ntentar tentaram tente tentei ter terceira terceiro teu teus teve tipo tive\ntivemos tiveram tiveste tivestes toda todas todo todos treze trÃªs tu tua tuas\ntudo tÃ£o tÃªm\n\num uma umas uns usa usar Ãºltimo\n\nvai vais valor veja vem vens ver vez vezes vinda vindo vinte vocÃª vocÃªs vos vossa\nvossas vosso vossos vÃ¡rios vÃ£o vÃªm vÃ³s\n\nzero\n'.split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/pt/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.pt.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/pt/__init__.py----------------------------------------
spacy.lang.pt.__init__.Portuguese(Language)
spacy.lang.pt.__init__.PortugueseDefaults(BaseDefaults)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/pt/lex_attrs.py----------------------------------------
A:spacy.lang.pt.lex_attrs.text->text.replace(',', '').replace('.', '').replace('Âº', '').replace('Âª', '').replace(',', '').replace('.', '').replace('Âº', '').replace('Âª', '')
A:spacy.lang.pt.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace('Âº', '').replace('Âª', '').replace(',', '').replace('.', '').replace('Âº', '').replace('Âª', '').split('/')
spacy.lang.pt.lex_attrs.like_num(text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/pt/syntax_iterators.py----------------------------------------
A:spacy.lang.pt.syntax_iterators.np_label->doc.vocab.strings.add('NP')
A:spacy.lang.pt.syntax_iterators.adj_label->doc.vocab.strings.add('amod')
A:spacy.lang.pt.syntax_iterators.det_label->doc.vocab.strings.add('det')
A:spacy.lang.pt.syntax_iterators.det_pos->doc.vocab.strings.add('DET')
A:spacy.lang.pt.syntax_iterators.adp_label->doc.vocab.strings.add('ADP')
A:spacy.lang.pt.syntax_iterators.conj->doc.vocab.strings.add('conj')
A:spacy.lang.pt.syntax_iterators.conj_pos->doc.vocab.strings.add('CCONJ')
A:spacy.lang.pt.syntax_iterators.right_childs->list(word.rights)
spacy.lang.pt.syntax_iterators.noun_chunks(doclike:Union[Doc,Span])->Iterator[Tuple[int, int, int]]


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/pt/punctuation.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/eu/examples.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/eu/stop_words.py----------------------------------------
A:spacy.lang.eu.stop_words.STOP_WORDS->set('\nal\nanitz\narabera\nasko\nbaina\nbat\nbatean\nbatek\nbati\nbatzuei\nbatzuek\nbatzuetan\nbatzuk\nbera\nberaiek\nberau\nberauek\nbere\nberori\nberoriek\nbeste\nbezala\nda\ndago\ndira\nditu\ndu\ndute\nedo\negin\nere\neta\neurak\nez\ngainera\ngu\ngutxi\nguzti\nhaiei\nhaiek\nhaietan\nhainbeste\nhala\nhan\nhandik\nhango\nhara\nhari\nhark\nhartan\nhau\nhauei\nhauek\nhauetan\nhemen\nhemendik\nhemengo\nhi\nhona\nhonek\nhonela\nhonetan\nhoni\nhor\nhori\nhoriei\nhoriek\nhorietan\nhorko\nhorra\nhorrek\nhorrela\nhorretan\nhorri\nhortik\nhura\nizan\nni\nnoiz\nnola\nnon\nnondik\nnongo\nnor\nnora\nze\nzein\nzen\nzenbait\nzenbat\nzer\nzergatik\nziren\nzituen\nzu\nzuek\nzuen\nzuten\n'.split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/eu/__init__.py----------------------------------------
spacy.lang.eu.__init__.Basque(Language)
spacy.lang.eu.__init__.BasqueDefaults(BaseDefaults)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/eu/lex_attrs.py----------------------------------------
A:spacy.lang.eu.lex_attrs._num_words->'\nbat\nbi\nhiru\nlau\nbost\nsei\nzazpi\nzortzi\nbederatzi\nhamar\nhamaika\nhamabi\nhamahiru\nhamalau\nhamabost\nhamasei\nhamazazpi\nHemezortzi\nhemeretzi\nhogei\nehun\nmila\nmilioi\n'.split()
A:spacy.lang.eu.lex_attrs._ordinal_words->'\nlehen\nbigarren\nhirugarren\nlaugarren\nbosgarren\nseigarren\nzazpigarren\nzortzigarren\nbederatzigarren\nhamargarren\nhamaikagarren\nhamabigarren\nhamahirugarren\nhamalaugarren\nhamabosgarren\nhamaseigarren\nhamazazpigarren\nhamazortzigarren\nhemeretzigarren\nhogeigarren\nbehin\n'.split()
A:spacy.lang.eu.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.eu.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.eu.lex_attrs.like_num(text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/eu/punctuation.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ne/examples.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ne/stop_words.py----------------------------------------
A:spacy.lang.ne.stop_words.STOP_WORDS->set('\nà¤…à¤•à¥à¤¸à¤°\nà¤…à¤—à¤¾à¤¡à¤¿\nà¤…à¤—à¤¾à¤¡à¥€\nà¤…à¤˜à¤¿\nà¤…à¤à¥ˆ\nà¤…à¤ à¤¾à¤°\nà¤…à¤¥à¤µà¤¾\nà¤…à¤¨à¤¿\nà¤…à¤¨à¥à¤¸à¤¾à¤°\nà¤…à¤¨à¥à¤¤à¤°à¥à¤—à¤¤\nà¤…à¤¨à¥à¤¯\nà¤…à¤¨à¥à¤¯à¤¤à¥à¤°\nà¤…à¤¨à¥à¤¯à¤¥à¤¾\nà¤…à¤¬\nà¤…à¤°à¥\nà¤…à¤°à¥à¤²à¤¾à¤ˆ\nà¤…à¤°à¥‚\nà¤…à¤°à¥à¤•à¥‹\nà¤…à¤°à¥à¤¥à¤¾à¤¤\nà¤…à¤°à¥à¤¥à¤¾à¤¤à¥\nà¤…à¤²à¤—\nà¤…à¤²à¤¿\nà¤…à¤µà¤¸à¥à¤¥à¤¾\nà¤…à¤¹à¤¿à¤²à¥‡\nà¤†à¤\nà¤†à¤à¤•à¤¾\nà¤†à¤à¤•à¥‹\nà¤†à¤œ\nà¤†à¤œà¤•à¥‹\nà¤†à¤ \nà¤†à¤¤à¥à¤®\nà¤†à¤¦à¤¿\nà¤†à¤¦à¤¿à¤²à¤¾à¤ˆ\nà¤†à¤«à¤¨à¥‹\nà¤†à¤«à¥‚\nà¤†à¤«à¥‚à¤²à¤¾à¤ˆ\nà¤†à¤«à¥ˆ\nà¤†à¤«à¥ˆà¤\nà¤†à¤«à¥à¤¨à¥ˆ\nà¤†à¤«à¥à¤¨à¥‹\nà¤†à¤¯à¥‹\nà¤‰\nà¤‰à¤•à¥à¤¤\nà¤‰à¤¦à¤¾à¤¹à¤°à¤£\nà¤‰à¤¨à¤•à¥‹\nà¤‰à¤¨à¤²à¤¾à¤ˆ\nà¤‰à¤¨à¤²à¥‡\nà¤‰à¤¨à¤¿\nà¤‰à¤¨à¥€\nà¤‰à¤¨à¥€à¤¹à¤°à¥à¤•à¥‹\nà¤‰à¤¨à¥à¤¨à¤¾à¤‡à¤¸\nà¤‰à¤ª\nà¤‰à¤¸à¤•à¥‹\nà¤‰à¤¸à¤²à¤¾à¤ˆ\nà¤‰à¤¸à¤²à¥‡\nà¤‰à¤¹à¤¾à¤²à¤¾à¤ˆ\nà¤Š\nà¤à¤‰à¤Ÿà¤¾\nà¤à¤‰à¤Ÿà¥ˆ\nà¤à¤•\nà¤à¤•à¤¦à¤®\nà¤à¤˜à¤¾à¤°\nà¤“à¤ \nà¤”\nà¤”à¤‚\nà¤•à¤¤à¤¾\nà¤•à¤¤à¤¿\nà¤•à¤¤à¥ˆ\nà¤•à¤®\nà¤•à¤®à¤¸à¥‡à¤•à¤®\nà¤•à¤¸à¤°à¤¿\nà¤•à¤¸à¤°à¥€\nà¤•à¤¸à¥ˆ\nà¤•à¤¸à¥ˆà¤•à¥‹\nà¤•à¤¸à¥ˆà¤²à¤¾à¤ˆ\nà¤•à¤¸à¥ˆà¤²à¥‡\nà¤•à¤¸à¥ˆà¤¸à¤à¤—\nà¤•à¤¸à¥à¤¤à¥‹\nà¤•à¤¹à¤¾à¤à¤¬à¤¾à¤Ÿ\nà¤•à¤¹à¤¿à¤²à¥‡à¤•à¤¾à¤¹à¥€à¤‚\nà¤•à¤¾\nà¤•à¤¾à¤®\nà¤•à¤¾à¤°à¤£\nà¤•à¤¿\nà¤•à¤¿à¤¨\nà¤•à¤¿à¤¨à¤­à¤¨à¥‡\nà¤•à¥à¤¨\nà¤•à¥à¤¨à¥ˆ\nà¤•à¥à¤¨à¥à¤¨à¥€\nà¤•à¥à¤°à¤¾\nà¤•à¥ƒà¤ªà¤¯à¤¾\nà¤•à¥‡\nà¤•à¥‡à¤¹à¤¿\nà¤•à¥‡à¤¹à¥€\nà¤•à¥‹\nà¤•à¥‹à¤¹à¤¿\nà¤•à¥‹à¤¹à¤¿à¤ªà¤¨à¤¿\nà¤•à¥‹à¤¹à¥€\nà¤•à¥‹à¤¹à¥€à¤ªà¤¨à¤¿\nà¤•à¥à¤°à¤®à¤¶à¤ƒ\nà¤—à¤\nà¤—à¤à¤•à¥‹\nà¤—à¤à¤°\nà¤—à¤¯à¥Œ\nà¤—à¤°à¤¿\nà¤—à¤°à¥€\nà¤—à¤°à¥‡\nà¤—à¤°à¥‡à¤•à¤¾\nà¤—à¤°à¥‡à¤•à¥‹\nà¤—à¤°à¥‡à¤°\nà¤—à¤°à¥Œà¤‚\nà¤—à¤°à¥à¤›\nà¤—à¤°à¥à¤›à¤¨à¥\nà¤—à¤°à¥à¤›à¥\nà¤—à¤°à¥à¤¦à¤¾\nà¤—à¤°à¥à¤¦à¥ˆ\nà¤—à¤°à¥à¤¨\nà¤—à¤°à¥à¤¨à¥\nà¤—à¤°à¥à¤¨à¥à¤ªà¤°à¥à¤›\nà¤—à¤°à¥à¤¨à¥‡\nà¤—à¥ˆà¤°\nà¤˜à¤°\nà¤šà¤¾à¤°\nà¤šà¤¾à¤²à¥‡\nà¤šà¤¾à¤¹à¤¨à¥à¤¹à¥à¤¨à¥à¤›\nà¤šà¤¾à¤¹à¤¨à¥à¤›à¥\nà¤šà¤¾à¤¹à¤¿à¤‚\nà¤šà¤¾à¤¹à¤¿à¤\nà¤šà¤¾à¤¹à¤¿à¤‚à¤²à¥‡\nà¤šà¤¾à¤¹à¥€à¤‚\nà¤šà¤¾à¤¹à¥‡à¤•à¥‹\nà¤šà¤¾à¤¹à¥‡à¤°\nà¤šà¥‹à¤Ÿà¥€\nà¤šà¥Œà¤¥à¥‹\nà¤šà¥Œà¤§\nà¤›\nà¤›à¤¨\nà¤›à¤¨à¥\nà¤›à¥\nà¤›à¥‚\nà¤›à¥ˆà¤¨\nà¤›à¥ˆà¤¨à¤¨à¥\nà¤›à¥Œ\nà¤›à¥Œà¤‚\nà¤œà¤¤à¤¾\nà¤œà¤¤à¤¾à¤¤à¤¤à¥ˆ\nà¤œà¤¨à¤¾\nà¤œà¤¨à¤¾à¤•à¥‹\nà¤œà¤¨à¤¾à¤²à¤¾à¤ˆ\nà¤œà¤¨à¤¾à¤²à¥‡\nà¤œà¤¬\nà¤œà¤¬à¤•à¤¿\nà¤œà¤¬à¤•à¥€\nà¤œà¤¸à¤•à¥‹\nà¤œà¤¸à¤¬à¤¾à¤Ÿ\nà¤œà¤¸à¤®à¤¾\nà¤œà¤¸à¤°à¥€\nà¤œà¤¸à¤²à¤¾à¤ˆ\nà¤œà¤¸à¤²à¥‡\nà¤œà¤¸à¥à¤¤à¤¾\nà¤œà¤¸à¥à¤¤à¥ˆ\nà¤œà¤¸à¥à¤¤à¥‹\nà¤œà¤¸à¥à¤¤à¥‹à¤¸à¥à¤•à¥ˆ\nà¤œà¤¹à¤¾à¤\nà¤œà¤¾à¤¨\nà¤œà¤¾à¤¨à¥‡\nà¤œà¤¾à¤¹à¤¿à¤°\nà¤œà¥à¤¨\nà¤œà¥à¤¨à¥ˆ\nà¤œà¥‡\nà¤œà¥‹\nà¤œà¥‹à¤ªà¤¨à¤¿\nà¤œà¥‹à¤ªà¤¨à¥€\nà¤à¥ˆà¤‚\nà¤ à¤¾à¤‰à¤à¤®à¤¾\nà¤ à¥€à¤•\nà¤ à¥‚à¤²à¥‹\nà¤¤\nà¤¤à¤¤à¤¾\nà¤¤à¤¤à¥à¤•à¤¾à¤²\nà¤¤à¤¥à¤¾\nà¤¤à¤¥à¤¾à¤ªà¤¿\nà¤¤à¤¥à¤¾à¤ªà¥€\nà¤¤à¤¦à¤¨à¥à¤¸à¤¾à¤°\nà¤¤à¤ªà¤¾à¤‡\nà¤¤à¤ªà¤¾à¤ˆ\nà¤¤à¤ªà¤¾à¤ˆà¤•à¥‹\nà¤¤à¤¬\nà¤¤à¤°\nà¤¤à¤°à¥à¤«\nà¤¤à¤²\nà¤¤à¤¸à¤°à¥€\nà¤¤à¤¾à¤ªà¤¨à¤¿\nà¤¤à¤¾à¤ªà¤¨à¥€\nà¤¤à¤¿à¤¨\nà¤¤à¤¿à¤¨à¤¿\nà¤¤à¤¿à¤¨à¤¿à¤¹à¤°à¥à¤²à¤¾à¤ˆ\nà¤¤à¤¿à¤¨à¥€\nà¤¤à¤¿à¤¨à¥€à¤¹à¤°à¥\nà¤¤à¤¿à¤¨à¥€à¤¹à¤°à¥à¤•à¥‹\nà¤¤à¤¿à¤¨à¥€à¤¹à¤°à¥‚\nà¤¤à¤¿à¤¨à¥€à¤¹à¤°à¥‚à¤•à¥‹\nà¤¤à¤¿à¤¨à¥ˆ\nà¤¤à¤¿à¤®à¥€\nà¤¤à¤¿à¤°\nà¤¤à¤¿à¤°à¤•à¥‹\nà¤¤à¥€\nà¤¤à¥€à¤¨\nà¤¤à¥à¤°à¤¨à¥à¤¤\nà¤¤à¥à¤°à¥à¤¨à¥à¤¤\nà¤¤à¥à¤°à¥à¤¨à¥à¤¤à¥ˆ\nà¤¤à¥‡à¤¶à¥à¤°à¥‹\nà¤¤à¥‡à¤¸à¥à¤•à¤¾à¤°à¤£\nà¤¤à¥‡à¤¸à¥à¤°à¥‹\nà¤¤à¥‡à¤¹à¥à¤°\nà¤¤à¥ˆà¤ªà¤¨à¤¿\nà¤¤à¥ˆà¤ªà¤¨à¥€\nà¤¤à¥à¤¯à¤¤à¥à¤¤à¤¿à¤•à¥ˆ\nà¤¤à¥à¤¯à¤¤à¥à¤¤à¤¿à¤•à¥ˆà¤®à¤¾\nà¤¤à¥à¤¯à¤¸\nà¤¤à¥à¤¯à¤¸à¤•à¤¾à¤°à¤£\nà¤¤à¥à¤¯à¤¸à¤•à¥‹\nà¤¤à¥à¤¯à¤¸à¤²à¥‡\nà¤¤à¥à¤¯à¤¸à¥ˆà¤²à¥‡\nà¤¤à¥à¤¯à¤¸à¥‹\nà¤¤à¥à¤¯à¤¸à¥à¤¤à¥ˆ\nà¤¤à¥à¤¯à¤¸à¥à¤¤à¥‹\nà¤¤à¥à¤¯à¤¹à¤¾à¤\nà¤¤à¥à¤¯à¤¹à¤¿à¤\nà¤¤à¥à¤¯à¤¹à¥€\nà¤¤à¥à¤¯à¤¹à¥€à¤\nà¤¤à¥à¤¯à¤¹à¥€à¤‚\nà¤¤à¥à¤¯à¥‹\nà¤¤à¥à¤¸à¤ªà¤›à¤¿\nà¤¤à¥à¤¸à¥ˆà¤²à¥‡\nà¤¥à¤ª\nà¤¥à¤°à¤¿\nà¤¥à¤°à¥€\nà¤¥à¤¾à¤¹à¤¾\nà¤¥à¤¿à¤\nà¤¥à¤¿à¤à¤\nà¤¥à¤¿à¤à¤¨\nà¤¥à¤¿à¤¯à¥‹\nà¤¦à¤°à¥à¤¤à¤¾\nà¤¦à¤¶\nà¤¦à¤¿à¤\nà¤¦à¤¿à¤à¤•à¥‹\nà¤¦à¤¿à¤¨\nà¤¦à¤¿à¤¨à¥à¤­à¤à¤•à¥‹\nà¤¦à¤¿à¤¨à¥à¤¹à¥à¤¨à¥à¤›\nà¤¦à¥à¤‡\nà¤¦à¥à¤‡à¤µà¤Ÿà¤¾\nà¤¦à¥à¤ˆ\nà¤¦à¥‡à¤–à¤¿\nà¤¦à¥‡à¤–à¤¿à¤¨à¥à¤›\nà¤¦à¥‡à¤–à¤¿à¤¯à¥‹\nà¤¦à¥‡à¤–à¥‡\nà¤¦à¥‡à¤–à¥‡à¤•à¥‹\nà¤¦à¥‡à¤–à¥‡à¤°\nà¤¦à¥‹à¤¶à¥à¤°à¥€\nà¤¦à¥‹à¤¶à¥à¤°à¥‹\nà¤¦à¥‹à¤¸à¥à¤°à¥‹\nà¤¦à¥à¤µà¤¾à¤°à¤¾\nà¤§à¤¨à¥à¤¨\nà¤§à¥‡à¤°à¥ˆ\nà¤§à¥Œ\nà¤¨\nà¤¨à¤—à¤°à¥à¤¨à¥\nà¤¨à¤—à¤°à¥à¤¨à¥‚\nà¤¨à¤œà¤¿à¤•à¥ˆ\nà¤¨à¤¤à¥à¤°\nà¤¨à¤¤à¥à¤°à¤­à¤¨à¥‡\nà¤¨à¤­à¤ˆ\nà¤¨à¤­à¤à¤•à¥‹\nà¤¨à¤­à¤¨à¥‡à¤°\nà¤¨à¤¯à¤¾à¤\nà¤¨à¤¿\nà¤¨à¤¿à¤•à¥ˆ\nà¤¨à¤¿à¤®à¥à¤¤à¤¿\nà¤¨à¤¿à¤®à¥à¤¨\nà¤¨à¤¿à¤®à¥à¤¨à¤¾à¤¨à¥à¤¸à¤¾à¤°\nà¤¨à¤¿à¤°à¥à¤¦à¤¿à¤·à¥à¤Ÿ\nà¤¨à¥ˆ\nà¤¨à¥Œ\nà¤ªà¤•à¥à¤•à¤¾\nà¤ªà¤•à¥à¤•à¥ˆ\nà¤ªà¤›à¤¾à¤¡à¤¿\nà¤ªà¤›à¤¾à¤¡à¥€\nà¤ªà¤›à¤¿\nà¤ªà¤›à¤¿à¤²à¥à¤²à¥‹\nà¤ªà¤›à¥€\nà¤ªà¤Ÿà¤•\nà¤ªà¤¨à¤¿\nà¤ªà¤¨à¥à¤§à¥à¤°\nà¤ªà¤°à¥à¤›\nà¤ªà¤°à¥à¤¥à¥à¤¯à¥‹\nà¤ªà¤°à¥à¤¦à¥ˆà¤¨\nà¤ªà¤°à¥à¤¨à¥‡\nà¤ªà¤°à¥à¤¨à¥‡à¤®à¤¾\nà¤ªà¤°à¥à¤¯à¤¾à¤ªà¥à¤¤\nà¤ªà¤¹à¤¿à¤²à¥‡\nà¤ªà¤¹à¤¿à¤²à¥‹\nà¤ªà¤¹à¤¿à¤²à¥à¤¯à¥ˆ\nà¤ªà¤¾à¤à¤š\nà¤ªà¤¾à¤‚à¤š\nà¤ªà¤¾à¤šà¥Œà¤\nà¤ªà¤¾à¤à¤šà¥Œà¤‚\nà¤ªà¤¿à¤šà¥à¤›à¥‡\nà¤ªà¥‚à¤°à¥à¤µ\nà¤ªà¥‹\nà¤ªà¥à¤°à¤¤à¤¿\nà¤ªà¥à¤°à¤¤à¥‡à¤•\nà¤ªà¥à¤°à¤¤à¥à¤¯à¤•\nà¤ªà¥à¤°à¤¾à¤¯\nà¤ªà¥à¤²à¤¸\nà¤«à¤°à¤•\nà¤«à¥‡à¤°à¤¿\nà¤«à¥‡à¤°à¥€\nà¤¬à¤¢à¥€\nà¤¬à¤¤à¤¾à¤\nà¤¬à¤¨à¥‡\nà¤¬à¤°à¥\nà¤¬à¤¾à¤Ÿ\nà¤¬à¤¾à¤°à¥‡\nà¤¬à¤¾à¤¹à¤¿à¤°\nà¤¬à¤¾à¤¹à¥‡à¤•\nà¤¬à¤¾à¤¹à¥à¤°\nà¤¬à¤¿à¤š\nà¤¬à¤¿à¤šà¤®à¤¾\nà¤¬à¤¿à¤°à¥à¤¦à¥à¤§\nà¤¬à¤¿à¤¶à¥‡à¤·\nà¤¬à¤¿à¤¸\nà¤¬à¥€à¤š\nà¤¬à¥€à¤šà¤®à¤¾\nà¤¬à¥€à¤¸\nà¤­à¤\nà¤­à¤à¤\nà¤­à¤à¤•à¤¾\nà¤­à¤à¤•à¤¾à¤²à¤¾à¤ˆ\nà¤­à¤à¤•à¥‹\nà¤­à¤à¤¨\nà¤­à¤à¤°\nà¤­à¤¨\nà¤­à¤¨à¥‡\nà¤­à¤¨à¥‡à¤•à¥‹\nà¤­à¤¨à¥‡à¤°\nà¤­à¤¨à¥\nà¤­à¤¨à¥à¤›à¤¨à¥\nà¤­à¤¨à¥à¤›à¥\nà¤­à¤¨à¥à¤¦à¤¾\nà¤­à¤¨à¥à¤¦à¥ˆ\nà¤­à¤¨à¥à¤¨à¥à¤­à¤¯à¥‹\nà¤­à¤¨à¥à¤¨à¥‡\nà¤­à¤¨à¥à¤¯à¤¾\nà¤­à¤¯à¥‡à¤¨\nà¤­à¤¯à¥‹\nà¤­à¤°\nà¤­à¤°à¤¿\nà¤­à¤°à¥€\nà¤­à¤¾\nà¤­à¤¿à¤¤à¥à¤°\nà¤­à¤¿à¤¤à¥à¤°à¥€\nà¤­à¥€à¤¤à¥à¤°\nà¤®\nà¤®à¤§à¥à¤¯\nà¤®à¤§à¥à¤¯à¥‡\nà¤®à¤²à¤¾à¤ˆ\nà¤®à¤¾\nà¤®à¤¾à¤¤à¥à¤°\nà¤®à¤¾à¤¤à¥à¤°à¥ˆ\nà¤®à¤¾à¤¥à¤¿\nà¤®à¤¾à¤¥à¥€\nà¤®à¥à¤–à¥à¤¯\nà¤®à¥à¤¨à¤¿\nà¤®à¥à¤¨à¥à¤¤à¤¿à¤°\nà¤®à¥‡à¤°à¥‹\nà¤®à¥ˆà¤²à¥‡\nà¤¯à¤¤à¤¿\nà¤¯à¤¥à¥‹à¤šà¤¿à¤¤\nà¤¯à¤¦à¤¿\nà¤¯à¤¦à¥à¤§à¥à¤¯à¤ªà¤¿\nà¤¯à¤¦à¥à¤¯à¤ªà¤¿\nà¤¯à¤¸\nà¤¯à¤¸à¤•à¤¾\nà¤¯à¤¸à¤•à¥‹\nà¤¯à¤¸à¤ªà¤›à¤¿\nà¤¯à¤¸à¤¬à¤¾à¤¹à¥‡à¤•\nà¤¯à¤¸à¤®à¤¾\nà¤¯à¤¸à¤°à¥€\nà¤¯à¤¸à¤²à¥‡\nà¤¯à¤¸à¥‹\nà¤¯à¤¸à¥à¤¤à¥ˆ\nà¤¯à¤¸à¥à¤¤à¥‹\nà¤¯à¤¹à¤¾à¤\nà¤¯à¤¹à¤¾à¤à¤¸à¤®à¥à¤®\nà¤¯à¤¹à¥€\nà¤¯à¤¾\nà¤¯à¥€\nà¤¯à¥‹\nà¤°\nà¤°à¤¹à¥€\nà¤°à¤¹à¥‡à¤•à¤¾\nà¤°à¤¹à¥‡à¤•à¥‹\nà¤°à¤¹à¥‡à¤›\nà¤°à¤¾à¤–à¥‡\nà¤°à¤¾à¤–à¥à¤›\nà¤°à¤¾à¤®à¥à¤°à¥‹\nà¤°à¥à¤ªà¤®à¤¾\nà¤°à¥‚à¤ª\nà¤°à¥‡\nà¤²à¤—à¤­à¤—\nà¤²à¤—à¤¾à¤¯à¤¤\nà¤²à¤¾à¤ˆ\nà¤²à¤¾à¤–\nà¤²à¤¾à¤—à¤¿\nà¤²à¤¾à¤—à¥‡à¤•à¥‹\nà¤²à¥‡\nà¤µà¤Ÿà¤¾\nà¤µà¤°à¥€à¤ªà¤°à¥€\nà¤µà¤¾\nà¤µà¤¾à¤Ÿ\nà¤µà¤¾à¤ªà¤¤\nà¤µà¤¾à¤¸à¥à¤¤à¤µà¤®à¤¾\nà¤¶à¤¾à¤¯à¤¦\nà¤¸à¤•à¥à¤›\nà¤¸à¤•à¥à¤¨à¥‡\nà¤¸à¤à¤—\nà¤¸à¤‚à¤—\nà¤¸à¤à¤—à¤•à¥‹\nà¤¸à¤à¤—à¤¸à¤à¤—à¥ˆ\nà¤¸à¤à¤—à¥ˆ\nà¤¸à¤‚à¤—à¥ˆ\nà¤¸à¤™à¥à¤—\nà¤¸à¤™à¥à¤—à¤•à¥‹\nà¤¸à¤Ÿà¥à¤Ÿà¤¾\nà¤¸à¤¤à¥à¤°\nà¤¸à¤§à¥ˆ\nà¤¸à¤¬à¥ˆ\nà¤¸à¤¬à¥ˆà¤•à¥‹\nà¤¸à¤¬à¥ˆà¤²à¤¾à¤ˆ\nà¤¸à¤®à¤¯\nà¤¸à¤®à¥‡à¤¤\nà¤¸à¤®à¥à¤­à¤µ\nà¤¸à¤®à¥à¤®\nà¤¸à¤¯\nà¤¸à¤°à¤¹\nà¤¸à¤¹à¤¿à¤¤\nà¤¸à¤¹à¤¿à¤¤à¥ˆ\nà¤¸à¤¹à¥€\nà¤¸à¤¾à¤à¤šà¥à¤šà¥ˆ\nà¤¸à¤¾à¤¤\nà¤¸à¤¾à¤¥\nà¤¸à¤¾à¤¥à¥ˆ\nà¤¸à¤¾à¤¯à¤¦\nà¤¸à¤¾à¤°à¤¾\nà¤¸à¥à¤¨à¥‡à¤•à¥‹\nà¤¸à¥à¤¨à¥‡à¤°\nà¤¸à¥à¤°à¥\nà¤¸à¥à¤°à¥à¤•à¥‹\nà¤¸à¥à¤°à¥à¤®à¥ˆ\nà¤¸à¥‹\nà¤¸à¥‹à¤šà¥‡à¤•à¥‹\nà¤¸à¥‹à¤šà¥‡à¤°\nà¤¸à¥‹à¤¹à¥€\nà¤¸à¥‹à¤¹à¥à¤°\nà¤¸à¥à¤¥à¤¿à¤¤\nà¤¸à¥à¤ªà¤·à¥à¤Ÿ\nà¤¹à¤œà¤¾à¤°\nà¤¹à¤°à¥‡\nà¤¹à¤°à¥‡à¤•\nà¤¹à¤¾à¤®à¥€\nà¤¹à¤¾à¤®à¥€à¤²à¥‡\nà¤¹à¤¾à¤®à¥à¤°à¤¾\nà¤¹à¤¾à¤®à¥à¤°à¥‹\nà¤¹à¥à¤à¤¦à¥ˆà¤¨\nà¤¹à¥à¤¨\nà¤¹à¥à¤¨à¤¤\nà¤¹à¥à¤¨à¥\nà¤¹à¥à¤¨à¥‡\nà¤¹à¥à¤¨à¥‡à¤›\nà¤¹à¥à¤¨à¥\nà¤¹à¥à¤¨à¥à¤›\nà¤¹à¥à¤¨à¥à¤¥à¥à¤¯à¥‹\nà¤¹à¥ˆà¤¨\nà¤¹à¥‹\nà¤¹à¥‹à¤‡à¤¨\nà¤¹à¥‹à¤•à¤¿\nà¤¹à¥‹à¤²à¤¾\n'.split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ne/__init__.py----------------------------------------
spacy.lang.ne.__init__.Nepali(Language)
spacy.lang.ne.__init__.NepaliDefaults(BaseDefaults)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ne/lex_attrs.py----------------------------------------
A:spacy.lang.ne.lex_attrs.length->len(suffix_group[0])
A:spacy.lang.ne.lex_attrs.text->text.replace(', ', '').replace('.', '').replace(', ', '').replace('.', '')
A:spacy.lang.ne.lex_attrs.(num, denom)->text.replace(', ', '').replace('.', '').replace(', ', '').replace('.', '').split('/')
spacy.lang.ne.lex_attrs.like_num(text)
spacy.lang.ne.lex_attrs.norm(string)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/fi/examples.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/fi/stop_words.py----------------------------------------
A:spacy.lang.fi.stop_words.STOP_WORDS->set('\naiemmin aika aikaa aikaan aikaisemmin aikaisin aikana aikoina aikoo aikovat\naina ainakaan ainakin ainoa ainoat aiomme aion aiotte aivan ajan alas alemmas\nalkuisin alkuun alla alle aloitamme aloitan aloitat aloitatte aloitattivat\naloitettava aloitettavaksi aloitettu aloitimme aloitin aloitit aloititte\naloittaa aloittamatta aloitti aloittivat alta aluksi alussa alusta annettavaksi\nannettava annettu ansiosta antaa antamatta antoi apu asia asiaa asian asiasta\nasiat asioiden asioihin asioita asti avuksi avulla avun avutta\n\nedelle edelleen edellÃ¤ edeltÃ¤ edemmÃ¤s edes edessÃ¤ edestÃ¤ ehkÃ¤ ei eikÃ¤ eilen\neivÃ¤t eli ellei elleivÃ¤t ellemme ellen ellet ellette emme en enemmÃ¤n eniten\nennen ensi ensimmÃ¤inen ensimmÃ¤iseksi ensimmÃ¤isen ensimmÃ¤isenÃ¤ ensimmÃ¤iset\nensimmÃ¤isiksi ensimmÃ¤isinÃ¤ ensimmÃ¤isiÃ¤ ensimmÃ¤istÃ¤ ensin entinen entisen\nentisiÃ¤ entisten entistÃ¤ enÃ¤Ã¤ eri erittÃ¤in erityisesti erÃ¤iden erÃ¤s erÃ¤Ã¤t esi\nesiin esillÃ¤ esimerkiksi et eteen etenkin ette ettei ettÃ¤\n\nhalua haluaa haluamatta haluamme haluan haluat haluatte haluavat halunnut\nhalusi halusimme halusin halusit halusitte halusivat halutessa haluton he hei\nheidÃ¤n heidÃ¤t heihin heille heillÃ¤ heiltÃ¤ heissÃ¤ heistÃ¤ heitÃ¤ helposti heti\nhetkellÃ¤ hieman hitaasti huolimatta huomenna hyvien hyviin hyviksi hyville\nhyviltÃ¤ hyvin hyvinÃ¤ hyvissÃ¤ hyvistÃ¤ hyviÃ¤ hyvÃ¤ hyvÃ¤t hyvÃ¤Ã¤ hÃ¤n hÃ¤neen hÃ¤nelle\nhÃ¤nellÃ¤ hÃ¤neltÃ¤ hÃ¤nen hÃ¤nessÃ¤ hÃ¤nestÃ¤ hÃ¤net hÃ¤ntÃ¤\n\nihan ilman ilmeisesti itse itsensÃ¤ itseÃ¤Ã¤n\n\nja jo johon joiden joihin joiksi joilla joille joilta joina joissa joista joita\njoka jokainen jokin joko joksi joku jolla jolle jolloin jolta jompikumpi jona\njonka jonkin jonne joo jopa jos joskus jossa josta jota jotain joten jotenkin\njotenkuten jotka jotta jouduimme jouduin jouduit jouduitte joudumme joudun\njoudutte joukkoon joukossa joukosta joutua joutui joutuivat joutumaan joutuu\njoutuvat juuri jÃ¤lkeen jÃ¤lleen jÃ¤Ã¤\n\nkahdeksan kahdeksannen kahdella kahdelle kahdelta kahden kahdessa kahdesta\nkahta kahteen kai kaiken kaikille kaikilta kaikkea kaikki kaikkia kaikkiaan\nkaikkialla kaikkialle kaikkialta kaikkien kaikkiin kaksi kannalta kannattaa\nkanssa kanssaan kanssamme kanssani kanssanne kanssasi kauan kauemmas kaukana\nkautta kehen keiden keihin keiksi keille keillÃ¤ keiltÃ¤ keinÃ¤ keissÃ¤ keistÃ¤\nkeitten keittÃ¤ keitÃ¤ keneen keneksi kenelle kenellÃ¤ keneltÃ¤ kenen kenenÃ¤\nkenessÃ¤ kenestÃ¤ kenet kenettÃ¤ kenties kerran kerta kertaa keskellÃ¤ kesken\nkeskimÃ¤Ã¤rin ketkÃ¤ ketÃ¤ kiitos kohti koko kokonaan kolmas kolme kolmen kolmesti\nkoska koskaan kovin kuin kuinka kuinkaan kuitenkaan kuitenkin kuka kukaan kukin\nkumpainen kumpainenkaan kumpi kumpikaan kumpikin kun kuten kuuden kuusi kuutta\nkylliksi kyllÃ¤ kymmenen kyse\n\nliian liki lisÃ¤ksi lisÃ¤Ã¤ lla luo luona lÃ¤hekkÃ¤in lÃ¤helle lÃ¤hellÃ¤ lÃ¤heltÃ¤\nlÃ¤hemmÃ¤s lÃ¤hes lÃ¤hinnÃ¤ lÃ¤htien lÃ¤pi\n\nmahdollisimman mahdollista me meidÃ¤n meidÃ¤t meihin meille meillÃ¤ meiltÃ¤ meissÃ¤\nmeistÃ¤ meitÃ¤ melkein melko menee menemme menen menet menette menevÃ¤t meni\nmenimme menin menit menivÃ¤t mennessÃ¤ mennyt menossa mihin miksi mikÃ¤ mikÃ¤li\nmikÃ¤Ã¤n mille milloin milloinkan millÃ¤ miltÃ¤ minkÃ¤ minne minua minulla minulle\nminulta minun minussa minusta minut minuun minÃ¤ missÃ¤ mistÃ¤ miten mitkÃ¤ mitÃ¤\nmitÃ¤Ã¤n moi molemmat mones monesti monet moni moniaalla moniaalle moniaalta\nmonta muassa muiden muita muka mukaan mukaansa mukana mutta muu muualla muualle\nmuualta muuanne muulloin muun muut muuta muutama muutaman muuten myÃ¶hemmin myÃ¶s\nmyÃ¶skin myÃ¶skÃ¤Ã¤n myÃ¶tÃ¤\n\nne neljÃ¤ neljÃ¤n neljÃ¤Ã¤ niiden niihin niiksi niille niillÃ¤ niiltÃ¤ niin niinÃ¤\nniissÃ¤ niistÃ¤ niitÃ¤ noiden noihin noiksi noilla noille noilta noin noina noissa\nnoista noita nopeammin nopeasti nopeiten nro nuo nyt nÃ¤iden nÃ¤ihin nÃ¤iksi\nnÃ¤ille nÃ¤illÃ¤ nÃ¤iltÃ¤ nÃ¤in nÃ¤inÃ¤ nÃ¤issÃ¤ nÃ¤istÃ¤ nÃ¤itÃ¤ nÃ¤mÃ¤\n\nohi oikea oikealla oikein ole olemme olen olet olette oleva olevan olevat oli\nolimme olin olisi olisimme olisin olisit olisitte olisivat olit olitte olivat\nolla olleet ollut oma omaa omaan omaksi omalle omalta oman omassa omat omia\nomien omiin omiksi omille omilta omissa omista on onkin onko ovat\n\npaikoittain paitsi pakosti paljon paremmin parempi parhaillaan parhaiten\nperusteella perÃ¤ti pian pieneen pieneksi pienelle pienellÃ¤ pieneltÃ¤ pienempi\npienestÃ¤ pieni pienin poikki puolesta puolestaan pÃ¤Ã¤lle\n\nrunsaasti\n\nsaakka sama samaa samaan samalla saman samat samoin satojen se\nseitsemÃ¤n sekÃ¤ sen seuraavat siellÃ¤ sieltÃ¤ siihen siinÃ¤ siis siitÃ¤ sijaan siksi\nsille silloin sillÃ¤ silti siltÃ¤ sinne sinua sinulla sinulle sinulta sinun\nsinussa sinusta sinut sinuun sinÃ¤ sisÃ¤kkÃ¤in sisÃ¤llÃ¤ siten sitten sitÃ¤ ssa sta\nsuoraan suuntaan suuren suuret suuri suuria suurin suurten\n\ntaa taas taemmas tahansa tai takaa takaisin takana takia tallÃ¤ tapauksessa\ntarpeeksi tavalla tavoitteena te teidÃ¤n teidÃ¤t teihin teille teillÃ¤ teiltÃ¤\nteissÃ¤ teistÃ¤ teitÃ¤ tietysti todella toinen toisaalla toisaalle toisaalta\ntoiseen toiseksi toisella toiselle toiselta toisemme toisen toisensa toisessa\ntoisesta toista toistaiseksi toki tosin tule tulee tulemme tulen\ntulet tulette tulevat tulimme tulin tulisi tulisimme tulisin tulisit tulisitte\ntulisivat tulit tulitte tulivat tulla tulleet tullut tuntuu tuo tuohon tuoksi\ntuolla tuolle tuolloin tuolta tuon tuona tuonne tuossa tuosta tuota tuskin tykÃ¶\ntÃ¤hÃ¤n tÃ¤ksi tÃ¤lle tÃ¤llÃ¤ tÃ¤llÃ¶in tÃ¤ltÃ¤ tÃ¤mÃ¤ tÃ¤mÃ¤n tÃ¤nne tÃ¤nÃ¤ tÃ¤nÃ¤Ã¤n tÃ¤ssÃ¤ tÃ¤stÃ¤\ntÃ¤ten tÃ¤tÃ¤ tÃ¤ysin tÃ¤ytyvÃ¤t tÃ¤ytyy tÃ¤Ã¤llÃ¤ tÃ¤Ã¤ltÃ¤\n\nulkopuolella usea useasti useimmiten usein useita uudeksi uudelleen uuden uudet\nuusi uusia uusien uusinta uuteen uutta\n\nvaan vai vaiheessa vaikea vaikean vaikeat vaikeilla vaikeille vaikeilta\nvaikeissa vaikeista vaikka vain varmasti varsin varsinkin varten vasen\nvasemmalla vasta vastaan vastakkain vastan verran vielÃ¤ vierekkÃ¤in vieressÃ¤\nvieri viiden viime viimeinen viimeisen viimeksi viisi voi voidaan voimme voin\nvoisi voit voitte voivat vuoden vuoksi vuosi vuosien vuosina vuotta vÃ¤hemmÃ¤n\nvÃ¤hintÃ¤Ã¤n vÃ¤hiten vÃ¤hÃ¤n vÃ¤lillÃ¤\n\nyhdeksÃ¤n yhden yhdessÃ¤ yhteen yhteensÃ¤ yhteydessÃ¤ yhteyteen yhtÃ¤ yhtÃ¤Ã¤lle\nyhtÃ¤Ã¤llÃ¤ yhtÃ¤Ã¤ltÃ¤ yhtÃ¤Ã¤n yhÃ¤ yksi yksin yksittÃ¤in yleensÃ¤ ylemmÃ¤s yli ylÃ¶s\nympÃ¤ri\n\nÃ¤lkÃ¶Ã¶n Ã¤lÃ¤\n'.split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/fi/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.fi.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/fi/__init__.py----------------------------------------
spacy.lang.fi.__init__.Finnish(Language)
spacy.lang.fi.__init__.FinnishDefaults(BaseDefaults)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/fi/lex_attrs.py----------------------------------------
A:spacy.lang.fi.lex_attrs.text->text.replace('.', '').replace(',', '').replace('.', '').replace(',', '')
A:spacy.lang.fi.lex_attrs.(num, denom)->text.replace('.', '').replace(',', '').replace('.', '').replace(',', '').split('/')
spacy.lang.fi.lex_attrs.like_num(text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/fi/syntax_iterators.py----------------------------------------
A:spacy.lang.fi.syntax_iterators.np_label->doc.vocab.strings.add('NP')
A:spacy.lang.fi.syntax_iterators.conj_label->doc.vocab.strings.add('conj')
spacy.lang.fi.syntax_iterators.noun_chunks(doclike:Union[Doc,Span])->Iterator[Tuple[int, int, int]]


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/fi/punctuation.py----------------------------------------
A:spacy.lang.fi.punctuation._quotes->char_classes.CONCAT_QUOTES.replace("'", '')
A:spacy.lang.fi.punctuation.DASHES->'|'.join((x for x in LIST_HYPHENS if x != '-'))


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/lb/examples.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/lb/stop_words.py----------------------------------------
A:spacy.lang.lb.stop_words.STOP_WORDS->set("\na\nÃ \nÃ¤is\nÃ¤r\nÃ¤rt\nÃ¤ert\nÃ¤ren\nall\nallem\nalles\nalleguer\nals\nalso\nam\nan\nanerefalls\nass\naus\nawer\nbei\nbeim\nbis\nbis\nd'\ndach\ndatt\ndÃ¤in\ndÃ¤r\ndat\nde\ndee\nden\ndeel\ndeem\ndeen\ndeene\ndÃ©i\nden\ndeng\ndenger\ndem\nder\ndÃ«sem\ndi\ndir\ndo\nda\ndann\ndomat\ndozou\ndrop\ndu\nduerch\nduerno\ne\nee\nem\neen\neent\nÃ«\nen\nÃ«nner\nÃ«m\nech\neis\neise\neisen\neiser\neises\neisereen\nesou\neen\neng\nenger\nengem\nentweder\net\nerÃ©ischt\nfalls\nfir\ngÃ©int\ngÃ©if\ngÃ«tt\ngÃ«t\ngeet\ngi\nginn\ngouf\ngouff\ngoung\nhat\nhaten\nhatt\nhÃ¤tt\nhei\nhu\nhuet\nhun\nhunn\nhiren\nhien\nhin\nhier\nhir\njidderen\njiddereen\njiddwereen\njiddereng\njiddwerengen\njo\nins\niech\niwwer\nkann\nkee\nkeen\nkÃ«nne\nkÃ«nnt\nkÃ©ng\nkÃ©ngen\nkÃ©ngem\nkoum\nkuckt\nmam\nmat\nma\nmÃ¤\nmech\nmÃ©i\nmÃ©cht\nmeng\nmenger\nmer\nmir\nmuss\nnach\nnÃ¤mmlech\nnÃ¤mmelech\nnÃ¤ischt\nnawell\nnÃ«mme\nnÃ«mmen\nnet\nnees\nnee\nno\nnu\nnom\noch\noder\nons\nonsen\nonser\nonsereen\nonst\nom\nop\nouni\nsÃ¤i\nsÃ¤in\nschonn\nschonns\nsi\nsid\nsie\nse\nsech\nseng\nsenge\nsengem\nsenger\nselwecht\nselwer\nsinn\nsollten\nsouguer\nsou\nsoss\nsot\n't\ntÃ«scht\nu\nun\num\nvirdrun\nvu\nvum\nvun\nwann\nwar\nwaren\nwas\nwat\nwÃ«llt\nweider\nwÃ©i\nwÃ©ini\nwÃ©inst\nwi\nwollt\nwou\nwouhin\nzanter\nze\nzu\nzum\nzwar\n".split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/lb/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.lb.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/lb/__init__.py----------------------------------------
spacy.lang.lb.__init__.Luxembourgish(Language)
spacy.lang.lb.__init__.LuxembourgishDefaults(BaseDefaults)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/lb/lex_attrs.py----------------------------------------
A:spacy.lang.lb.lex_attrs._num_words->set('\nnull eent zwee drÃ¤i vÃ©ier fÃ«nnef sechs ziwen aacht nÃ©ng zÃ©ng eelef zwielef drÃ¤izÃ©ng\nvÃ©ierzÃ©ng foffzÃ©ng siechzÃ©ng siwwenzÃ©ng uechtzeng uechzeng nonnzÃ©ng nongzÃ©ng zwanzeg drÃ«sseg vÃ©ierzeg foffzeg sechzeg siechzeg siwenzeg achtzeg achzeg uechtzeg uechzeg nonnzeg\nhonnert dausend millioun milliard billioun billiard trillioun triliard\n'.split())
A:spacy.lang.lb.lex_attrs._ordinal_words->set('\nÃ©ischten zweeten drÃ«tten vÃ©ierten fÃ«nneften sechsten siwenten aachten nÃ©ngten zÃ©ngten eeleften\nzwieleften drÃ¤izÃ©ngten vÃ©ierzÃ©ngten foffzÃ©ngten siechzÃ©ngten uechtzÃ©ngen uechzÃ©ngten nonnzÃ©ngten nongzÃ©ngten zwanzegsten\ndrÃ«ssegsten vÃ©ierzegsten foffzegsten siechzegsten siwenzegsten uechzegsten nonnzegsten\nhonnertsten dausendsten milliounsten\nmilliardsten billiounsten billiardsten trilliounsten trilliardsten\n'.split())
A:spacy.lang.lb.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.lb.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.lb.lex_attrs.like_num(text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/lb/punctuation.py----------------------------------------
A:spacy.lang.lb.punctuation.ELISION->" ' â€™ ".strip().replace(' ', '')


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/de/examples.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/de/stop_words.py----------------------------------------
A:spacy.lang.de.stop_words.STOP_WORDS->set('\nÃ¡ a ab aber ach acht achte achten achter achtes ag alle allein allem allen\naller allerdings alles allgemeinen als also am an andere anderen anderem andern\nanders auch auf aus ausser auÃŸer ausserdem auÃŸerdem\n\nbald bei beide beiden beim beispiel bekannt bereits besonders besser besten bin\nbis bisher bist\n\nda dabei dadurch dafÃ¼r dagegen daher dahin dahinter damals damit danach daneben\ndank dann daran darauf daraus darf darfst darin darÃ¼ber darum darunter das\ndasein daselbst dass daÃŸ dasselbe davon davor dazu dazwischen dein deine deinem\ndeiner dem dementsprechend demgegenÃ¼ber demgemÃ¤ss demgemÃ¤ÃŸ demselben demzufolge\nden denen denn denselben der deren derjenige derjenigen dermassen dermaÃŸen\nderselbe derselben des deshalb desselben dessen deswegen dich die diejenige\ndiejenigen dies diese dieselbe dieselben diesem diesen dieser dieses dir doch\ndort drei drin dritte dritten dritter drittes du durch durchaus dÃ¼rfen dÃ¼rft\ndurfte durften\n\neben ebenso ehrlich eigen eigene eigenen eigener eigenes ein einander eine\neinem einen einer eines einige einigen einiger einiges einmal einmaleins elf en\nende endlich entweder er erst erste ersten erster erstes es etwa etwas euch\n\nfrÃ¼her fÃ¼nf fÃ¼nfte fÃ¼nften fÃ¼nfter fÃ¼nftes fÃ¼r\n\ngab ganz ganze ganzen ganzer ganzes gar gedurft gegen gegenÃ¼ber gehabt gehen\ngeht gekannt gekonnt gemacht gemocht gemusst genug gerade gern gesagt geschweige\ngewesen gewollt geworden gibt ging gleich gross groÃŸ grosse groÃŸe grossen\ngroÃŸen grosser groÃŸer grosses groÃŸes gut gute guter gutes\n\nhabe haben habt hast hat hatte hÃ¤tte hatten hÃ¤tten heisst heiÃŸt her heute hier\nhin hinter hoch\n\nich ihm ihn ihnen ihr ihre ihrem ihren ihrer ihres im immer in indem\ninfolgedessen ins irgend ist\n\nja jahr jahre jahren je jede jedem jeden jeder jedermann jedermanns jedoch\njemand jemandem jemanden jene jenem jenen jener jenes jetzt\n\nkam kann kannst kaum kein keine keinem keinen keiner kleine kleinen kleiner\nkleines kommen kommt kÃ¶nnen kÃ¶nnt konnte kÃ¶nnte konnten kurz\n\nlang lange leicht leider lieber los\n\nmachen macht machte mag magst man manche manchem manchen mancher manches mehr\nmein meine meinem meinen meiner meines mich mir mit mittel mochte mÃ¶chte mochten\nmÃ¶gen mÃ¶glich mÃ¶gt morgen muss muÃŸ mÃ¼ssen musst mÃ¼sst musste mussten\n\nna nach nachdem nahm natÃ¼rlich neben nein neue neuen neun neunte neunten neunter\nneuntes nicht nichts nie niemand niemandem niemanden noch nun nur\n\nob oben oder offen oft ohne\n\nrecht rechte rechten rechter rechtes richtig rund\n\nsagt sagte sah satt schlecht schon sechs sechste sechsten sechster sechstes\nsehr sei seid seien sein seine seinem seinen seiner seines seit seitdem selbst\nselbst sich sie sieben siebente siebenten siebenter siebentes siebte siebten\nsiebter siebtes sind so solang solche solchem solchen solcher solches soll\nsollen sollte sollten sondern sonst sowie spÃ¤ter statt\n\ntag tage tagen tat teil tel trotzdem tun\n\nÃ¼ber Ã¼berhaupt Ã¼brigens uhr um und uns unser unsere unserer unter\n\nvergangene vergangenen viel viele vielem vielen vielleicht vier vierte vierten\nvierter viertes vom von vor\n\nwahr wÃ¤hrend wÃ¤hrenddem wÃ¤hrenddessen wann war wÃ¤re waren wart warum was wegen\nweil weit weiter weitere weiteren weiteres welche welchem welchen welcher\nwelches wem wen wenig wenige weniger weniges wenigstens wenn wer werde werden\nwerdet wessen wie wieder will willst wir wird wirklich wirst wo wohl wollen\nwollt wollte wollten worden wurde wÃ¼rde wurden wÃ¼rden\n\nzehn zehnte zehnten zehnter zehntes zeit zu zuerst zugleich zum zunÃ¤chst zur\nzurÃ¼ck zusammen zwanzig zwar zwei zweite zweiten zweiter zweites zwischen\n'.split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/de/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.de.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/de/__init__.py----------------------------------------
spacy.lang.de.__init__.German(Language)
spacy.lang.de.__init__.GermanDefaults(BaseDefaults)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/de/syntax_iterators.py----------------------------------------
A:spacy.lang.de.syntax_iterators.np_label->doc.vocab.strings.add('NP')
A:spacy.lang.de.syntax_iterators.np_deps->set((doc.vocab.strings.add(label) for label in labels))
A:spacy.lang.de.syntax_iterators.close_app->doc.vocab.strings.add('nk')
spacy.lang.de.syntax_iterators.noun_chunks(doclike:Union[Doc,Span])->Iterator[Tuple[int, int, int]]


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/de/punctuation.py----------------------------------------
A:spacy.lang.de.punctuation._quotes->char_classes.CONCAT_QUOTES.replace("'", '')


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/tn/examples.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/tn/stop_words.py----------------------------------------
A:spacy.lang.tn.stop_words.STOP_WORDS->set('\nke gareng ga selekanyo tlhwatlhwa yo mongwe se\nsengwe fa go le jalo gongwe ba na mo tikologong\njaaka kwa morago nna gonne ka sa pele nako teng\ntlase fela ntle magareng tsona feta bobedi kgabaganya\nmoo gape kgatlhanong botlhe tsotlhe bokana e esi\nsetseng mororo dinako golo kgolo nnye wena gago\no ntse ntle tla goreng gangwe mang yotlhe gore\neo yona tseraganyo eng ne sentle re rona thata\ngodimo fitlha pedi masomamabedi lesomepedi mmogo\ntharo tseo boraro tseno yone jaanong bobona bona\nlesome tsaya tsamaiso nngwe masomethataro thataro\ntsa mmatota tota sale thoko supa dira tshwanetse di mmalwa masisi\nbonala e tshwanang bogolo tsenya tsweetswee karolo\nsepe tlhalosa dirwa robedi robongwe lesomenngwe gaisa\ntlhano lesometlhano botlalo lekgolo\n'.split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/tn/__init__.py----------------------------------------
spacy.lang.tn.__init__.Setswana(Language)
spacy.lang.tn.__init__.SetswanaDefaults(BaseDefaults)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/tn/lex_attrs.py----------------------------------------
A:spacy.lang.tn.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.tn.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
A:spacy.lang.tn.lex_attrs.text_lower->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').lower()
spacy.lang.tn.lex_attrs.like_num(text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/tn/punctuation.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ja/examples.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ja/stop_words.py----------------------------------------
A:spacy.lang.ja.stop_words.STOP_WORDS->set('\nã‚ ã‚ã£ ã‚ã¾ã‚Š ã‚ã‚Š ã‚ã‚‹ ã‚ã‚‹ã„ã¯ ã‚ã‚Œ\nã„ ã„ã„ ã„ã† ã„ã ã„ãšã‚Œ ã„ã£ ã„ã¤ ã„ã‚‹ ã„ã‚\nã†ã¡\nãˆ\nãŠ ãŠã„ ãŠã‘ ãŠã‚ˆã³ ãŠã‚‰ ãŠã‚Š\nã‹ ã‹ã‘ ã‹ã¤ ã‹ã¤ã¦ ã‹ãªã‚Š ã‹ã‚‰ ãŒ\nã ãã£ã‹ã‘\nãã‚‹ ãã‚“\nã“ ã“ã† ã“ã“ ã“ã¨ ã“ã® ã“ã‚Œ ã” ã”ã¨\nã• ã•ã‚‰ã« ã•ã‚“\nã— ã—ã‹ ã—ã‹ã— ã—ã¾ã† ã—ã¾ã£ ã—ã‚ˆã†\nã™ ã™ã ã™ã¹ã¦ ã™ã‚‹ ãš\nã› ã›ã„ ã›ã‚‹\nãã† ãã“ ãã—ã¦ ãã® ãã‚Œ ãã‚Œãžã‚Œ\nãŸ ãŸã„ ãŸã ã— ãŸã¡ ãŸã‚ ãŸã‚‰ ãŸã‚Š ã  ã ã‘ ã ã£\nã¡ ã¡ã‚ƒã‚“\nã¤ ã¤ã„ ã¤ã‘ ã¤ã¤\nã¦ ã§ ã§ã ã§ãã‚‹ ã§ã™\nã¨ ã¨ã ã¨ã“ã‚ ã¨ã£ ã¨ã‚‚ ã©ã†\nãª ãªã„ ãªãŠ ãªã‹ã£ ãªãŒã‚‰ ãªã ãªã‘ã‚Œ ãªã— ãªã£ ãªã© ãªã‚‰ ãªã‚Š ãªã‚‹\nã« ã«ã¦\nã¬\nã­\nã® ã®ã¡ ã®ã¿\nã¯ ã¯ã˜ã‚ ã°\nã²ã¨\nã¶ã‚Š\nã¸ ã¹ã\nã»ã‹ ã»ã¨ã‚“ã© ã»ã© ã»ã¼\nã¾ ã¾ã™ ã¾ãŸ ã¾ã§ ã¾ã¾\nã¿\nã‚‚ ã‚‚ã† ã‚‚ã£ ã‚‚ã¨ ã‚‚ã®\nã‚„ ã‚„ã£\nã‚ˆ ã‚ˆã† ã‚ˆã ã‚ˆã£ ã‚ˆã‚Š ã‚ˆã‚‹ ã‚ˆã‚Œ\nã‚‰ ã‚‰ã—ã„ ã‚‰ã‚Œ ã‚‰ã‚Œã‚‹\nã‚‹\nã‚Œ ã‚Œã‚‹\nã‚’\nã‚“\nä¸€\n'.split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ja/tag_bigram_map.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ja/tag_orth_map.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ja/tag_map.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ja/__init__.py----------------------------------------
A:spacy.lang.ja.__init__.self.tokenizer->try_sudachi_import(self.split_mode)
A:spacy.lang.ja.__init__.sudachipy_tokens->self.tokenizer.tokenize(text)
A:spacy.lang.ja.__init__.dtokens->self._get_dtokens(sub_a, False)
A:spacy.lang.ja.__init__.(dtokens, spaces)->get_dtokens_and_spaces(dtokens, text)
A:spacy.lang.ja.__init__.sub_tokens_list->list(sub_tokens_list)
A:spacy.lang.ja.__init__.doc->Doc(self.vocab, words=words, spaces=spaces)
A:spacy.lang.ja.__init__.(token.pos, next_pos)->resolve_pos(token.orth_, dtoken.tag, tags[idx + 1] if idx + 1 < len(tags) else None)
A:spacy.lang.ja.__init__.morph['Reading']->re.sub('[=|]', '_', dtoken.reading)
A:spacy.lang.ja.__init__.token.morph->MorphAnalysis(self.vocab, morph)
A:spacy.lang.ja.__init__.sub_a->token.split(self.tokenizer.SplitMode.A)
A:spacy.lang.ja.__init__.sub_b->token.split(self.tokenizer.SplitMode.B)
A:spacy.lang.ja.__init__.self.split_mode->load_config_from_str(DEFAULT_CONFIG).get('split_mode', None)
A:spacy.lang.ja.__init__.path->util.ensure_path(path)
A:spacy.lang.ja.__init__.config->load_config_from_str(DEFAULT_CONFIG)
A:spacy.lang.ja.__init__.DetailedToken->namedtuple('DetailedToken', ['surface', 'tag', 'inf', 'lemma', 'norm', 'reading', 'sub_tokens'])
A:spacy.lang.ja.__init__.tok->sudachipy.dictionary.Dictionary().create(mode=split_mode)
A:spacy.lang.ja.__init__.word_start->text[text_pos:].index(word)
spacy.lang.ja.__init__.Japanese(Language)
spacy.lang.ja.__init__.JapaneseDefaults(BaseDefaults)
spacy.lang.ja.__init__.JapaneseTokenizer(self,vocab:Vocab,split_mode:Optional[str]=None)
spacy.lang.ja.__init__.JapaneseTokenizer.__reduce__(self)
spacy.lang.ja.__init__.JapaneseTokenizer._get_config(self)->Dict[str, Any]
spacy.lang.ja.__init__.JapaneseTokenizer._get_dtokens(self,sudachipy_tokens,need_sub_tokens:bool=True)
spacy.lang.ja.__init__.JapaneseTokenizer._get_sub_tokens(self,sudachipy_tokens)
spacy.lang.ja.__init__.JapaneseTokenizer._set_config(self,config:Dict[str,Any]={})->None
spacy.lang.ja.__init__.JapaneseTokenizer.from_bytes(self,data:bytes,**kwargs)->'JapaneseTokenizer'
spacy.lang.ja.__init__.JapaneseTokenizer.from_disk(self,path:Union[str,Path],**kwargs)->'JapaneseTokenizer'
spacy.lang.ja.__init__.JapaneseTokenizer.score(self,examples)
spacy.lang.ja.__init__.JapaneseTokenizer.to_bytes(self,**kwargs)->bytes
spacy.lang.ja.__init__.JapaneseTokenizer.to_disk(self,path:Union[str,Path],**kwargs)->None
spacy.lang.ja.__init__.create_tokenizer(split_mode:Optional[str]=None)
spacy.lang.ja.__init__.get_dtokens_and_spaces(dtokens,text,gap_tag='ç©ºç™½')
spacy.lang.ja.__init__.make_morphologizer(nlp:Language,model:Model,name:str,overwrite:bool,extend:bool,scorer:Optional[Callable])
spacy.lang.ja.__init__.resolve_pos(orth,tag,next_tag)
spacy.lang.ja.__init__.try_sudachi_import(split_mode='A')


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ja/syntax_iterators.py----------------------------------------
A:spacy.lang.ja.syntax_iterators.np_label->doc.vocab.strings.add('NP')
spacy.lang.ja.syntax_iterators.noun_chunks(doclike:Union[Doc,Span])->Iterator[Tuple[int, int, int]]


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/da/examples.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/da/stop_words.py----------------------------------------
A:spacy.lang.da.stop_words.STOP_WORDS->set('\naf aldrig alene alle allerede alligevel alt altid anden andet andre at\n\nbag begge blandt blev blive bliver burde bÃ¸r\n\nda de dem den denne dens der derefter deres derfor derfra deri dermed derpÃ¥ derved det dette dig din dine disse dog du\n\nefter egen eller ellers en end endnu ene eneste enhver ens enten er et\n\nflere flest fleste for foran fordi forrige fra fÃ¥ fÃ¸r fÃ¸rst\n\ngennem gjorde gjort god gÃ¸r gÃ¸re gÃ¸rende\n\nham han hans har havde have hel heller hen hende hendes henover her herefter heri hermed herpÃ¥ hun hvad hvem hver hvilke hvilken hvilkes hvis hvor hvordan hvorefter hvorfor hvorfra hvorhen hvori hvorimod hvornÃ¥r hvorved\n\ni igen igennem ikke imellem imens imod ind indtil ingen intet\n\njeg jer jeres jo\n\nkan kom kommer kun kunne\n\nlad langs lav lave lavet lidt lige ligesom lille lÃ¦ngere\n\nman mange med meget mellem men mens mere mest mig min mindre mindst mine mit mÃ¥ mÃ¥ske\n\nned nemlig nogen nogensinde noget nogle nok nu ny nyt nÃ¦r nÃ¦ste nÃ¦sten\n\nog ogsÃ¥ om omkring op os over overalt\n\npÃ¥\n\nsamme sammen selv selvom senere ses siden sig sige skal skulle som stadig synes syntes sÃ¥ sÃ¥dan sÃ¥ledes\n\ntemmelig tidligere til tilbage tit\n\nud uden udover under undtagen\n\nvar ved vi via vil ville vore vores vÃ¦r vÃ¦re vÃ¦ret\n\nÃ¸vrigt\n'.split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/da/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.da.tokenizer_exceptions.capitalized->orth.capitalize()
A:spacy.lang.da.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/da/__init__.py----------------------------------------
spacy.lang.da.__init__.Danish(Language)
spacy.lang.da.__init__.DanishDefaults(BaseDefaults)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/da/lex_attrs.py----------------------------------------
A:spacy.lang.da.lex_attrs._num_words->'nul\nen et to tre fire fem seks syv otte ni ti\nelleve tolv tretten fjorten femten seksten sytten atten nitten tyve\nenogtyve toogtyve treogtyve fireogtyve femogtyve seksogtyve syvogtyve otteogtyve niogtyve tredive\nenogtredive toogtredive treogtredive fireogtredive femogtredive seksogtredive syvogtredive otteogtredive niogtredive fyrre\nenogfyrre toogfyrre treogfyrre fireogfyrre femgogfyrre seksogfyrre syvogfyrre otteogfyrre niogfyrre halvtreds\nenoghalvtreds tooghalvtreds treoghalvtreds fireoghalvtreds femoghalvtreds seksoghalvtreds syvoghalvtreds otteoghalvtreds nioghalvtreds tres\nenogtres toogtres treogtres fireogtres femogtres seksogtres syvogtres otteogtres niogtres halvfjerds\nenoghalvfjerds tooghalvfjerds treoghalvfjerds fireoghalvfjerds femoghalvfjerds seksoghalvfjerds syvoghalvfjerds otteoghalvfjerds nioghalvfjerds firs\nenogfirs toogfirs treogfirs fireogfirs femogfirs seksogfirs syvogfirs otteogfirs niogfirs halvfems\nenoghalvfems tooghalvfems treoghalvfems fireoghalvfems femoghalvfems seksoghalvfems syvoghalvfems otteoghalvfems nioghalvfems hundrede\nmillion milliard billion billiard trillion trilliard\n'.split()
A:spacy.lang.da.lex_attrs._ordinal_words->'nulte\nfÃ¸rste anden tredje fjerde femte sjette syvende ottende niende tiende\nelfte tolvte trettende fjortende femtende sekstende syttende attende nittende tyvende\nenogtyvende toogtyvende treogtyvende fireogtyvende femogtyvende seksogtyvende syvogtyvende otteogtyvende niogtyvende tredivte enogtredivte toogtredivte treogtredivte fireogtredivte femogtredivte seksogtredivte syvogtredivte otteogtredivte niogtredivte fyrretyvende\nenogfyrretyvende toogfyrretyvende treogfyrretyvende fireogfyrretyvende femogfyrretyvende seksogfyrretyvende syvogfyrretyvende otteogfyrretyvende niogfyrretyvende halvtredsindstyvende enoghalvtredsindstyvende\ntooghalvtredsindstyvende treoghalvtredsindstyvende fireoghalvtredsindstyvende femoghalvtredsindstyvende seksoghalvtredsindstyvende syvoghalvtredsindstyvende otteoghalvtredsindstyvende nioghalvtredsindstyvende\ntresindstyvende enogtresindstyvende toogtresindstyvende treogtresindstyvende fireogtresindstyvende femogtresindstyvende seksogtresindstyvende syvogtresindstyvende otteogtresindstyvende niogtresindstyvende halvfjerdsindstyvende\nenoghalvfjerdsindstyvende tooghalvfjerdsindstyvende treoghalvfjerdsindstyvende fireoghalvfjerdsindstyvende femoghalvfjerdsindstyvende seksoghalvfjerdsindstyvende syvoghalvfjerdsindstyvende otteoghalvfjerdsindstyvende nioghalvfjerdsindstyvende firsindstyvende\nenogfirsindstyvende toogfirsindstyvende treogfirsindstyvende fireogfirsindstyvende femogfirsindstyvende seksogfirsindstyvende syvogfirsindstyvende otteogfirsindstyvende niogfirsindstyvende halvfemsindstyvende\nenoghalvfemsindstyvende tooghalvfemsindstyvende treoghalvfemsindstyvende fireoghalvfemsindstyvende femoghalvfemsindstyvende seksoghalvfemsindstyvende syvoghalvfemsindstyvende otteoghalvfemsindstyvende nioghalvfemsindstyvende\n'.split()
A:spacy.lang.da.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.da.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.da.lex_attrs.like_num(text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/da/syntax_iterators.py----------------------------------------
A:spacy.lang.da.syntax_iterators.right->get_right_bound(doc, tok)
A:spacy.lang.da.syntax_iterators.np_label->doc.vocab.strings.add('NP')
A:spacy.lang.da.syntax_iterators.(left, right)->get_bounds(doc, token)
spacy.lang.da.syntax_iterators.noun_chunks(doclike:Union[Doc,Span])->Iterator[Tuple[int, int, int]]


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/da/punctuation.py----------------------------------------
A:spacy.lang.da.punctuation._quotes->char_classes.CONCAT_QUOTES.replace("'", '')


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/mr/stop_words.py----------------------------------------
A:spacy.lang.mr.stop_words.STOP_WORDS->set('\nà¤¨\nà¤…à¤¤à¤°à¥€\nà¤¤à¥‹\nà¤¹à¥‡à¤‚\nà¤¤à¥‡à¤‚\nà¤•à¤¾à¤‚\nà¤†à¤£à¤¿\nà¤œà¥‡à¤‚\nà¤œà¥‡\nà¤®à¤—\nà¤¤à¥‡\nà¤®à¥€\nà¤œà¥‹\nà¤ªà¤°à¥€\nà¤—à¤¾\nà¤¹à¥‡\nà¤à¤¸à¥‡à¤‚\nà¤†à¤¤à¤¾à¤‚\nà¤¨à¤¾à¤¹à¥€à¤‚\nà¤¤à¥‡à¤¥\nà¤¹à¤¾\nà¤¤à¤¯à¤¾\nà¤…à¤¸à¥‡\nà¤®à¥à¤¹à¤£à¥‡\nà¤•à¤¾à¤¯\nà¤•à¥€à¤‚\nà¤œà¥ˆà¤¸à¥‡à¤‚\nà¤¤à¤‚à¤µ\nà¤¤à¥‚à¤‚\nà¤¹à¥‹à¤¯\nà¤œà¥ˆà¤¸à¤¾\nà¤†à¤¹à¥‡\nà¤ªà¥ˆà¤‚\nà¤¤à¥ˆà¤¸à¤¾\nà¤œà¤°à¥€\nà¤®à¥à¤¹à¤£à¥‹à¤¨à¤¿\nà¤à¤•\nà¤à¤¸à¤¾\nà¤œà¥€\nà¤¨à¤¾\nà¤®à¤œ\nà¤à¤¥\nà¤¯à¤¾\nà¤œà¥‡à¤¥\nà¤œà¤¯à¤¾\nà¤¤à¥à¤œ\nà¤¤à¥‡à¤£à¥‡à¤‚\nà¤¤à¥ˆà¤‚\nà¤ªà¤¾à¤‚\nà¤…à¤¸à¥‹\nà¤•à¤°à¥€\nà¤à¤¸à¥€\nà¤¯à¥‡à¤£à¥‡à¤‚\nà¤œà¤¾à¤¹à¤²à¤¾\nà¤¤à¥‡à¤‚à¤šà¤¿\nà¤†à¤˜à¤µà¥‡à¤‚\nà¤¹à¥‹à¤¤à¥€\nà¤•à¤¾à¤‚à¤¹à¥€à¤‚\nà¤¹à¥‹à¤Šà¤¨à¤¿\nà¤à¤•à¥‡à¤‚\nà¤®à¤¾à¤¤à¥‡à¤‚\nà¤ à¤¾à¤¯à¥€à¤‚\nà¤¯à¥‡\nà¤¸à¤•à¤³\nà¤•à¥‡à¤²à¥‡à¤‚\nà¤œà¥‡à¤£à¥‡à¤‚\nà¤œà¤¾à¤£\nà¤œà¥ˆà¤¸à¥€\nà¤¹à¥‹à¤¯à¥‡\nà¤œà¥‡à¤µà¥€à¤‚\nà¤à¤±à¥à¤¹à¤µà¥€à¤‚\nà¤®à¥€à¤šà¤¿\nà¤•à¤¿à¤°à¥€à¤Ÿà¥€\nà¤¦à¤¿à¤¸à¥‡\nà¤¦à¥‡à¤µà¤¾\nà¤¹à¥‹\nà¤¤à¤°à¤¿\nà¤•à¥€à¤œà¥‡\nà¤¤à¥ˆà¤¸à¥‡\nà¤†à¤ªà¤£\nà¤¤à¤¿à¤¯à¥‡\nà¤•à¤°à¥à¤®\nà¤¨à¥‹à¤¹à¥‡\nà¤‡à¤¯à¥‡\nà¤ªà¤¡à¥‡\nà¤®à¤¾à¤à¥‡à¤‚\nà¤¤à¥ˆà¤¸à¥€\nà¤²à¤¾à¤—à¥‡\nà¤¨à¤¾à¤¨à¤¾\nà¤œà¤‚à¤µ\nà¤•à¥€à¤°\nà¤…à¤§à¤¿à¤•\nà¤…à¤¨à¥‡à¤•\nà¤…à¤¶à¥€\nà¤…à¤¸à¤²à¤¯à¤¾à¤šà¥‡\nà¤…à¤¸à¤²à¥‡à¤²à¥à¤¯à¤¾\nà¤…à¤¸à¤¾\nà¤…à¤¸à¥‚à¤¨\nà¤…à¤¸à¥‡\nà¤†à¤œ\nà¤†à¤£à¤¿\nà¤†à¤¤à¤¾\nà¤†à¤ªà¤²à¥à¤¯à¤¾\nà¤†à¤²à¤¾\nà¤†à¤²à¥€\nà¤†à¤²à¥‡\nà¤†à¤¹à¥‡\nà¤†à¤¹à¥‡à¤¤\nà¤à¤•\nà¤à¤•à¤¾\nà¤•à¤®à¥€\nà¤•à¤°à¤£à¤¯à¤¾à¤¤\nà¤•à¤°à¥‚à¤¨\nà¤•à¤¾\nà¤•à¤¾à¤®\nà¤•à¤¾à¤¯\nà¤•à¤¾à¤¹à¥€\nà¤•à¤¿à¤µà¤¾\nà¤•à¥€\nà¤•à¥‡à¤²à¤¾\nà¤•à¥‡à¤²à¥€\nà¤•à¥‡à¤²à¥‡\nà¤•à¥‹à¤Ÿà¥€\nà¤—à¥‡à¤²à¥à¤¯à¤¾\nà¤˜à¥‡à¤Šà¤¨\nà¤œà¤¾à¤¤\nà¤à¤¾à¤²à¤¾\nà¤à¤¾à¤²à¥€\nà¤à¤¾à¤²à¥‡\nà¤à¤¾à¤²à¥‡à¤²à¥à¤¯à¤¾\nà¤Ÿà¤¾\nà¤¤à¤°\nà¤¤à¤°à¥€\nà¤¤à¤¸à¥‡à¤š\nà¤¤à¤¾\nà¤¤à¥€\nà¤¤à¥€à¤¨\nà¤¤à¥‡\nà¤¤à¥‹\nà¤¤à¥à¤¯à¤¾\nà¤¤à¥à¤¯à¤¾à¤šà¤¾\nà¤¤à¥à¤¯à¤¾à¤šà¥€\nà¤¤à¥à¤¯à¤¾à¤šà¥à¤¯à¤¾\nà¤¤à¥à¤¯à¤¾à¤¨à¤¾\nà¤¤à¥à¤¯à¤¾à¤¨à¥€\nà¤¤à¥à¤¯à¤¾à¤®à¥à¤³à¥‡\nà¤¤à¥à¤°à¥€\nà¤¦à¤¿à¤²à¥€\nà¤¦à¥‹à¤¨\nà¤¨\nà¤ªà¤£\nà¤ªà¤®\nà¤ªà¤°à¤¯à¤¤à¤¨\nà¤ªà¤¾à¤Ÿà¥€à¤²\nà¤®\nà¤®à¤¾à¤¤à¥à¤°\nà¤®à¤¾à¤¹à¤¿à¤¤à¥€\nà¤®à¥€\nà¤®à¥à¤¬à¥€\nà¤®à¥à¤¹à¤£à¤œà¥‡\nà¤®à¥à¤¹à¤£à¤¾à¤²à¥‡\nà¤®à¥à¤¹à¤£à¥‚à¤¨\nà¤¯à¤¾\nà¤¯à¤¾à¤šà¤¾\nà¤¯à¤¾à¤šà¥€\nà¤¯à¤¾à¤šà¥à¤¯à¤¾\nà¤¯à¤¾à¤¨à¤¾\nà¤¯à¤¾à¤¨à¥€\nà¤¯à¥‡à¤£à¤¾à¤°\nà¤¯à¥‡à¤¤\nà¤¯à¥‡à¤¥à¥€à¤²\nà¤¯à¥‡à¤¥à¥‡\nà¤²à¤¾à¤–\nà¤µ\nà¤µà¥à¤¯à¤•à¤¤\nà¤¸à¤°à¥à¤µ\nà¤¸à¤¾à¤—à¤¿à¤¤à¥à¤²à¥‡\nà¤¸à¥à¤°à¥‚\nà¤¹à¤œà¤¾à¤°\nà¤¹à¤¾\nà¤¹à¥€\nà¤¹à¥‡\nà¤¹à¥‹à¤£à¤¾à¤°\nà¤¹à¥‹à¤¤\nà¤¹à¥‹à¤¤à¤¾\nà¤¹à¥‹à¤¤à¥€\nà¤¹à¥‹à¤¤à¥‡\n'.split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/mr/__init__.py----------------------------------------
spacy.lang.mr.__init__.Marathi(Language)
spacy.lang.mr.__init__.MarathiDefaults(BaseDefaults)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/es/examples.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/es/lemmatizer.py----------------------------------------
A:spacy.lang.es.lemmatizer.pos->token.pos_.lower()
A:spacy.lang.es.lemmatizer.features->set(token.morph)
A:spacy.lang.es.lemmatizer.string->string.lower().lower()
A:spacy.lang.es.lemmatizer.exc->self.lookups.get_table('lemma_exc').get('pron', {}).get(pron)
A:spacy.lang.es.lemmatizer.lemmas->list(dict.fromkeys(lemmas))
A:spacy.lang.es.lemmatizer.rule->self.select_rule('pron', features)
A:spacy.lang.es.lemmatizer.index->self.lookups.get_table('lemma_index').get(rule_pos, [])
A:spacy.lang.es.lemmatizer.groups->self.lookups.get_table('lemma_rules_groups')
A:spacy.lang.es.lemmatizer.possible_lemma->re.sub(old + '$', new, word)
A:spacy.lang.es.lemmatizer.splitted_word->re.sub(',', '.', word).split(',')
A:spacy.lang.es.lemmatizer.word->re.sub(',', '.', word)
A:spacy.lang.es.lemmatizer.voc_alt_lemma->lemma.replace(old, new, 1)
A:spacy.lang.es.lemmatizer.m->re.search(pron_patt, verb)
A:spacy.lang.es.lemmatizer.verb->re.sub(old, new, verb)
spacy.lang.es.SpanishLemmatizer(Lemmatizer)
spacy.lang.es.lemmatizer.SpanishLemmatizer(Lemmatizer)
spacy.lang.es.lemmatizer.SpanishLemmatizer.get_lookups_config(cls,mode:str)->Tuple[List[str], List[str]]
spacy.lang.es.lemmatizer.SpanishLemmatizer.lemmatize_adj(self,word:str,features:List[str],rule:str,index:List[str])->List[str]
spacy.lang.es.lemmatizer.SpanishLemmatizer.lemmatize_adv(self,word:str,features:List[str],rule:str,index:List[str])->List[str]
spacy.lang.es.lemmatizer.SpanishLemmatizer.lemmatize_det(self,word:str,features:List[str],rule:str,index:List[str])->List[str]
spacy.lang.es.lemmatizer.SpanishLemmatizer.lemmatize_noun(self,word:str,features:List[str],rule:str,index:List[str])->List[str]
spacy.lang.es.lemmatizer.SpanishLemmatizer.lemmatize_num(self,word:str,features:List[str],rule:str,index:List[str])->List[str]
spacy.lang.es.lemmatizer.SpanishLemmatizer.lemmatize_pron(self,word:str,features:List[str],rule:Optional[str],index:List[str])->List[str]
spacy.lang.es.lemmatizer.SpanishLemmatizer.lemmatize_verb(self,word:str,features:List[str],rule:Optional[str],index:List[str])->List[str]
spacy.lang.es.lemmatizer.SpanishLemmatizer.lemmatize_verb_pron(self,word:str,features:List[str],rule:Optional[str],index:List[str])->List[str]
spacy.lang.es.lemmatizer.SpanishLemmatizer.rule_lemmatize(self,token:Token)->List[str]
spacy.lang.es.lemmatizer.SpanishLemmatizer.select_rule(self,pos:str,features:List[str])->Optional[str]


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/es/stop_words.py----------------------------------------
A:spacy.lang.es.stop_words.STOP_WORDS->set('\na acuerdo adelante ademas ademÃ¡s afirmÃ³ agregÃ³ ahi ahora ahÃ­ al algo alguna\nalgunas alguno algunos algÃºn alli allÃ­ alrededor ambos ante anterior antes\napenas aproximadamente aquel aquella aquellas aquello aquellos aqui aquÃ©l\naquÃ©lla aquÃ©llas aquÃ©llos aquÃ­ arriba asegurÃ³ asi asÃ­ atras aun aunque aÃ±adiÃ³\naÃºn\n\nbajo bastante bien breve buen buena buenas bueno buenos\n\ncada casi cierta ciertas cierto ciertos cinco claro comentÃ³ como con conmigo\nconocer conseguimos conseguir considera considerÃ³ consigo consigue consiguen\nconsigues contigo contra creo cual cuales cualquier cuando cuanta cuantas\ncuanto cuantos cuatro cuenta cuÃ¡l cuÃ¡les cuÃ¡ndo cuÃ¡nta cuÃ¡ntas cuÃ¡nto cuÃ¡ntos\ncÃ³mo\n\nda dado dan dar de debajo debe deben debido decir dejÃ³ del delante demasiado\ndemÃ¡s dentro deprisa desde despacio despues despuÃ©s detras detrÃ¡s dia dias dice\ndicen dicho dieron diez diferente diferentes dijeron dijo dio doce donde dos\ndurante dÃ­a dÃ­as dÃ³nde\n\ne el ella ellas ello ellos embargo en encima encuentra enfrente enseguida\nentonces entre era eramos eran eras eres es esa esas ese eso esos esta estaba\nestaban estado estados estais estamos estan estar estarÃ¡ estas este esto estos\nestoy estuvo estÃ¡ estÃ¡n excepto existe existen explicÃ³ expresÃ³ Ã©l Ã©sa Ã©sas Ã©se\nÃ©sos Ã©sta Ã©stas Ã©ste Ã©stos\n\nfin final fue fuera fueron fui fuimos\n\ngran grande grandes\n\nha haber habia habla hablan habrÃ¡ habÃ­a habÃ­an hace haceis hacemos hacen hacer\nhacerlo haces hacia haciendo hago han hasta hay haya he hecho hemos hicieron\nhizo hoy hubo\n\nigual incluso indicÃ³ informo informÃ³ ir\n\njunto\n\nla lado largo las le les llegÃ³ lleva llevar lo los luego\n\nmal manera manifestÃ³ mas mayor me mediante medio mejor mencionÃ³ menos menudo mi\nmia mias mientras mio mios mis misma mismas mismo mismos modo mucha muchas\nmucho muchos muy mÃ¡s mÃ­ mÃ­a mÃ­as mÃ­o mÃ­os\n\nnada nadie ni ninguna ningunas ninguno ningunos ningÃºn no nos nosotras nosotros\nnuestra nuestras nuestro nuestros nueva nuevas nueve nuevo nuevos nunca\n\no ocho once os otra otras otro otros\n\npara parece parte partir pasada pasado paÃ¬s peor pero pesar poca pocas poco\npocos podeis podemos poder podria podriais podriamos podrian podrias podrÃ¡\npodrÃ¡n podrÃ­a podrÃ­an poner por porque posible primer primera primero primeros\npronto propia propias propio propios proximo prÃ³ximo prÃ³ximos pudo pueda puede\npueden puedo pues\n\nqeu que quedÃ³ queremos quien quienes quiere quiza quizas quizÃ¡ quizÃ¡s quiÃ©n\nquiÃ©nes quÃ©\n\nrealizado realizar realizÃ³ repente respecto\n\nsabe sabeis sabemos saben saber sabes salvo se sea sean segun segunda segundo\nsegÃºn seis ser sera serÃ¡ serÃ¡n serÃ­a seÃ±alÃ³ si sido siempre siendo siete sigue\nsiguiente sin sino sobre sois sola solamente solas solo solos somos son soy su\nsupuesto sus suya suyas suyo suyos sÃ© sÃ­ sÃ³lo\n\ntal tambien tambiÃ©n tampoco tan tanto tarde te temprano tendrÃ¡ tendrÃ¡n teneis\ntenemos tener tenga tengo tenido tenÃ­a tercera tercero ti tiene tienen toda\ntodas todavia todavÃ­a todo todos total tras trata travÃ©s tres tu tus tuvo tuya\ntuyas tuyo tuyos tÃº\n\nu ultimo un una unas uno unos usa usais usamos usan usar usas uso usted ustedes\nÃºltima Ãºltimas Ãºltimo Ãºltimos\n\nva vais vamos van varias varios vaya veces ver verdad verdadera verdadero vez\nvosotras vosotros voy vuestra vuestras vuestro vuestros\n\ny ya yo\n'.split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/es/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.es.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/es/__init__.py----------------------------------------
spacy.lang.es.__init__.Spanish(Language)
spacy.lang.es.__init__.SpanishDefaults(BaseDefaults)
spacy.lang.es.__init__.make_lemmatizer(nlp:Language,model:Optional[Model],name:str,mode:str,overwrite:bool,scorer:Optional[Callable])


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/es/lex_attrs.py----------------------------------------
A:spacy.lang.es.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.es.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
A:spacy.lang.es.lex_attrs.text_lower->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').lower()
spacy.lang.es.lex_attrs.like_num(text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/es/syntax_iterators.py----------------------------------------
A:spacy.lang.es.syntax_iterators.np_label->doc.vocab.strings.add('NP')
A:spacy.lang.es.syntax_iterators.adj_label->doc.vocab.strings.add('amod')
A:spacy.lang.es.syntax_iterators.adp_label->doc.vocab.strings.add('ADP')
A:spacy.lang.es.syntax_iterators.conj->doc.vocab.strings.add('conj')
A:spacy.lang.es.syntax_iterators.conj_pos->doc.vocab.strings.add('CCONJ')
A:spacy.lang.es.syntax_iterators.right_childs->list(word.rights)
spacy.lang.es.syntax_iterators.noun_chunks(doclike:Union[Doc,Span])->Iterator[Tuple[int, int, int]]


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/es/punctuation.py----------------------------------------
A:spacy.lang.es.punctuation._units->merge_chars(' '.join(_list_units))


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/it/examples.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/it/lemmatizer.py----------------------------------------
A:spacy.lang.it.lemmatizer.morphology->token.morph.to_dict()
A:spacy.lang.it.lemmatizer.lookup_pos->univ_pos.lower()
A:spacy.lang.it.lemmatizer.lookup_table->self.lookups.get_table('lemma_lookup')
A:spacy.lang.it.lemmatizer.string->string.lower().lower()
A:spacy.lang.it.lemmatizer.lemma->self.lookups.get_table('lemma_lookup').get(string, string)
spacy.lang.it.ItalianLemmatizer(Lemmatizer)
spacy.lang.it.lemmatizer.ItalianLemmatizer(Lemmatizer)
spacy.lang.it.lemmatizer.ItalianLemmatizer.get_lookups_config(cls,mode:str)->Tuple[List[str], List[str]]
spacy.lang.it.lemmatizer.ItalianLemmatizer.lemmatize_adj(self,string:str,morphology:dict,lookup_table:Dict[str,str])->List[str]
spacy.lang.it.lemmatizer.ItalianLemmatizer.lemmatize_adp(self,string:str,morphology:dict,lookup_table:Dict[str,str])->List[str]
spacy.lang.it.lemmatizer.ItalianLemmatizer.lemmatize_det(self,string:str,morphology:dict,lookup_table:Dict[str,str])->List[str]
spacy.lang.it.lemmatizer.ItalianLemmatizer.lemmatize_noun(self,string:str,morphology:dict,lookup_table:Dict[str,str])->List[str]
spacy.lang.it.lemmatizer.ItalianLemmatizer.lemmatize_pron(self,string:str,morphology:dict,lookup_table:Dict[str,str])->List[str]
spacy.lang.it.lemmatizer.ItalianLemmatizer.pos_lookup_lemmatize(self,token:Token)->List[str]


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/it/stop_words.py----------------------------------------
A:spacy.lang.it.stop_words.STOP_WORDS->set("\na abbastanza abbia abbiamo abbiano abbiate accidenti ad adesso affinche agl\nagli ahime ahimÃ¨ ai al alcuna alcuni alcuno all alla alle allo allora altri\naltrimenti altro altrove altrui anche ancora anni anno ansa anticipo assai\nattesa attraverso avanti avemmo avendo avente aver avere averlo avesse\navessero avessi avessimo aveste avesti avete aveva avevamo avevano avevate\navevi avevo avrai avranno avrebbe avrebbero avrei avremmo avremo avreste\navresti avrete avrÃ  avrÃ² avuta avute avuti avuto\n\nbasta bene benissimo brava bravo\n\ncasa caso cento certa certe certi certo che chi chicchessia chiunque ci c'\nciascuna ciascuno cima cio cioe circa citta cittÃ  co codesta codesti codesto\ncogli coi col colei coll coloro colui come cominci comunque con concernente\nconciliarsi conclusione consiglio contro cortesia cos cosa cosi cosÃ¬ cui\n\nd' da dagl dagli dai dal dall dall' dalla dalle dallo dappertutto davanti degl degli\ndei del dell dell' della delle dello dentro detto deve di dice dietro dire\ndirimpetto diventa diventare diventato dopo dov dove dovra dovrÃ  dovunque due\ndunque durante\n\ne ebbe ebbero ebbi ecc ecco ed effettivamente egli ella entrambi eppure era\nerano eravamo eravate eri ero esempio esse essendo esser essere essi ex Ã¨\n\nfa faccia facciamo facciano facciate faccio facemmo facendo facesse facessero\nfacessi facessimo faceste facesti faceva facevamo facevano facevate facevi\nfacevo fai fanno farai faranno fare farebbe farebbero farei faremmo faremo\nfareste faresti farete farÃ  farÃ² fatto favore fece fecero feci fin finalmente\nfinche fine fino forse forza fosse fossero fossi fossimo foste fosti fra\nfrattempo fu fui fummo fuori furono futuro generale\n\ngia giÃ  giacche giorni giorno gli gl' gliela gliele glieli glielo gliene governo\ngrande grazie gruppo\n\nha haha hai hanno ho\n\nieri il improvviso in inc infatti inoltre insieme intanto intorno invece io\n\nl' la lÃ  lasciato lato lavoro le lei li lo lontano loro lui lungo luogo\n\nm' ma macche magari maggior mai male malgrado malissimo mancanza marche me\nmedesimo mediante meglio meno mentre mesi mezzo mi mia mie miei mila miliardi\nmilioni minimi ministro mio modo molti moltissimo molto momento mondo mosto\n\nnazionale ne negl negli nei nel nell nella nelle nello nemmeno neppure nessun nessun'\nnessuna nessuno nient' niente no noi non nondimeno nonostante nonsia nostra nostre\nnostri nostro novanta nove nulla nuovo\n\nod oggi ogni ognuna ognuno oltre oppure ora ore osi ossia ottanta otto\n\npaese parecchi parecchie parecchio parte partendo peccato peggio per perche\nperchÃ© percio perciÃ² perfino pero persino persone perÃ² piedi pieno piglia piu\npiuttosto piÃ¹ po pochissimo poco poi poiche possa possedere posteriore posto\npotrebbe preferibilmente presa press prima primo principalmente probabilmente\nproprio puo puÃ² pure purtroppo\n\nqualche qualcosa qualcuna qualcuno quale quali qualunque quando quanta quante\nquanti quanto quantunque quasi quattro quel quel' quella quelle quelli quello quest quest'\nquesta queste questi questo qui quindi\n\nrealmente recente recentemente registrazione relativo riecco salvo\n\ns' sara sarÃ  sarai saranno sarebbe sarebbero sarei saremmo saremo sareste\nsaresti sarete saro sarÃ² scola scopo scorso se secondo seguente seguito sei\nsembra sembrare sembrato sembri sempre senza sette si sia siamo siano siate\nsiete sig solito solo soltanto sono sopra sotto spesso srl sta stai stando\nstanno starai staranno starebbe starebbero starei staremmo staremo stareste\nstaresti starete starÃ  starÃ² stata state stati stato stava stavamo stavano\nstavate stavi stavo stemmo stessa stesse stessero stessi stessimo stesso\nsteste stesti stette stettero stetti stia stiamo stiano stiate sto su sua\nsubito successivamente successivo sue sugl sugli sui sul sull sulla sulle\nsullo suo suoi\n\nt' tale tali talvolta tanto te tempo ti titolo tra tranne tre trenta\ntroppo trovato tu tua tue tuo tuoi tutta tuttavia tutte tutti tutto\n\nuguali ulteriore ultimo un un' una uno uomo\n\nv' va vale vari varia varie vario verso vi via vicino visto vita voi volta volte\nvostra vostre vostri vostro\n".split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/it/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.it.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/it/__init__.py----------------------------------------
spacy.lang.it.__init__.Italian(Language)
spacy.lang.it.__init__.ItalianDefaults(BaseDefaults)
spacy.lang.it.__init__.make_lemmatizer(nlp:Language,model:Optional[Model],name:str,mode:str,overwrite:bool,scorer:Optional[Callable])


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/it/syntax_iterators.py----------------------------------------
A:spacy.lang.it.syntax_iterators.np_label->doc.vocab.strings.add('NP')
A:spacy.lang.it.syntax_iterators.adj_label->doc.vocab.strings.add('amod')
A:spacy.lang.it.syntax_iterators.det_pos->doc.vocab.strings.add('DET')
A:spacy.lang.it.syntax_iterators.adp_label->doc.vocab.strings.add('ADP')
A:spacy.lang.it.syntax_iterators.conj->doc.vocab.strings.add('conj')
A:spacy.lang.it.syntax_iterators.conj_pos->doc.vocab.strings.add('CCONJ')
A:spacy.lang.it.syntax_iterators.right_childs->list(word.rights)
spacy.lang.it.syntax_iterators.noun_chunks(doclike:Union[Doc,Span])->Iterator[Tuple[int, int, int]]


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/it/punctuation.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/sa/examples.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/sa/stop_words.py----------------------------------------
A:spacy.lang.sa.stop_words.STOP_WORDS->set('\nà¤…à¤¹à¤®à¥\nà¤†à¤µà¤¾à¤®à¥\nà¤µà¤¯à¤®à¥\nà¤®à¤¾à¤®à¥  à¤®à¤¾\nà¤†à¤µà¤¾à¤®à¥\nà¤…à¤¸à¥à¤®à¤¾à¤¨à¥  à¤¨à¤ƒ\nà¤®à¤¯à¤¾\nà¤†à¤µà¤¾à¤­à¥à¤¯à¤¾à¤®à¥\nà¤…à¤¸à¥à¤®à¤¾à¤­à¤¿à¤¸à¥\nà¤®à¤¹à¥à¤¯à¤®à¥  à¤®à¥‡\nà¤†à¤µà¤¾à¤­à¥à¤¯à¤¾à¤®à¥  à¤¨à¥Œ\nà¤…à¤¸à¥à¤®à¤­à¥à¤¯à¤®à¥  à¤¨à¤ƒ\nà¤®à¤¤à¥\nà¤†à¤µà¤¾à¤­à¥à¤¯à¤¾à¤®à¥\nà¤…à¤¸à¥à¤®à¤¤à¥\nà¤®à¤®  à¤®à¥‡\nà¤†à¤µà¤¯à¥‹à¤ƒ\nà¤…à¤¸à¥à¤®à¤¾à¤•à¤®à¥  à¤¨à¤ƒ\nà¤®à¤¯à¤¿\nà¤†à¤µà¤¯à¥‹à¤ƒ\nà¤…à¤¸à¥à¤®à¤¾à¤¸à¥\nà¤¤à¥à¤µà¤®à¥\nà¤¯à¥à¤µà¤¾à¤®à¥\nà¤¯à¥‚à¤¯à¤®à¥\nà¤¤à¥à¤µà¤¾à¤®à¥  à¤¤à¥à¤µà¤¾\nà¤¯à¥à¤µà¤¾à¤®à¥  à¤µà¤¾à¤®à¥\nà¤¯à¥à¤·à¥à¤®à¤¾à¤¨à¥  à¤µà¤ƒ\nà¤¤à¥à¤µà¤¯à¤¾\nà¤¯à¥à¤µà¤¾à¤­à¥à¤¯à¤¾à¤®à¥\nà¤¯à¥à¤·à¥à¤®à¤¾à¤­à¤¿à¤ƒ\nà¤¤à¥à¤­à¥à¤¯à¤®à¥  à¤¤à¥‡\nà¤¯à¥à¤µà¤¾à¤­à¥à¤¯à¤¾à¤®à¥  à¤µà¤¾à¤®à¥\nà¤¯à¥à¤·à¥à¤®à¤­à¥à¤¯à¤®à¥  à¤µà¤ƒ\nà¤¤à¥à¤µà¤¤à¥\nà¤¯à¥à¤µà¤¾à¤­à¥à¤¯à¤¾à¤®à¥\nà¤¯à¥à¤·à¥à¤®à¤¤à¥\nà¤¤à¤µ  à¤¤à¥‡\nà¤¯à¥à¤µà¤¯à¥‹à¤ƒ  à¤µà¤¾à¤®à¥\nà¤¯à¥à¤·à¥à¤®à¤¾à¤•à¤®à¥  à¤µà¤ƒ\nà¤¤à¥à¤µà¤¯à¤¿\nà¤¯à¥à¤µà¤¯à¥‹à¤ƒ\nà¤¯à¥à¤·à¥à¤®à¤¾à¤¸à¥\nà¤¸à¤ƒ\nà¤¤à¥Œ\nà¤¤à¥‡\nà¤¤à¤®à¥\nà¤¤à¥Œ\nà¤¤à¤¾à¤¨à¥\nà¤¤à¥‡à¤¨\nà¤¤à¤¾à¤­à¥à¤¯à¤¾à¤®à¥\nà¤¤à¥ˆà¤ƒ\nà¤¤à¤¸à¥à¤®à¥ˆ\nà¤¤à¤¾à¤­à¥à¤¯à¤¾à¤®à¥\nà¤¤à¥‡à¤­à¥à¤¯à¤ƒ\nà¤¤à¤¸à¥à¤®à¤¾à¤¤à¥\nà¤¤à¤¾à¤­à¥à¤¯à¤¾à¤®à¥\nà¤¤à¥‡à¤­à¥à¤¯à¤ƒ\nà¤¤à¤¸à¥à¤¯\nà¤¤à¤¯à¥‹à¤ƒ\nà¤¤à¥‡à¤·à¤¾à¤®à¥\nà¤¤à¤¸à¥à¤®à¤¿à¤¨à¥\nà¤¤à¤¯à¥‹à¤ƒ\nà¤¤à¥‡à¤·à¥\nà¤¸à¤¾\nà¤¤à¥‡\nà¤¤à¤¾à¤ƒ\nà¤¤à¤¾à¤®à¥\nà¤¤à¥‡\nà¤¤à¤¾à¤ƒ\nà¤¤à¤¯à¤¾\nà¤¤à¤¾à¤­à¥à¤¯à¤¾à¤®à¥\nà¤¤à¤¾à¤­à¤¿à¤ƒ\nà¤¤à¤¸à¥à¤¯à¥ˆ\nà¤¤à¤¾à¤­à¥à¤¯à¤¾à¤®à¥\nà¤¤à¤¾à¤­à¥à¤¯à¤ƒ\nà¤¤à¤¸à¥à¤¯à¤¾à¤ƒ\nà¤¤à¤¾à¤­à¥à¤¯à¤¾à¤®à¥\nà¤¤à¤¾à¤­à¥à¤¯à¤ƒ\nà¤¤à¤¸à¥à¤¯\nà¤¤à¤¯à¥‹à¤ƒ\nà¤¤à¤¾à¤¸à¤¾à¤®à¥\nà¤¤à¤¸à¥à¤¯à¤¾à¤®à¥\nà¤¤à¤¯à¥‹à¤ƒ\nà¤¤à¤¾à¤¸à¥\nà¤¤à¤¤à¥\nà¤¤à¥‡\nà¤¤à¤¾à¤¨à¤¿\nà¤¤à¤¤à¥\nà¤¤à¥‡\nà¤¤à¤¾à¤¨à¤¿\nà¤¤à¤¯à¤¾\nà¤¤à¤¾à¤­à¥à¤¯à¤¾à¤®à¥\nà¤¤à¤¾à¤­à¤¿à¤ƒ\nà¤¤à¤¸à¥à¤¯à¥ˆ\nà¤¤à¤¾à¤­à¥à¤¯à¤¾à¤®à¥\nà¤¤à¤¾à¤­à¥à¤¯à¤ƒ\nà¤¤à¤¸à¥à¤¯à¤¾à¤ƒ\nà¤¤à¤¾à¤­à¥à¤¯à¤¾à¤®à¥\nà¤¤à¤¾à¤­à¥à¤¯à¤ƒ\nà¤¤à¤¸à¥à¤¯\nà¤¤à¤¯à¥‹à¤ƒ\nà¤¤à¤¾à¤¸à¤¾à¤®à¥\nà¤¤à¤¸à¥à¤¯à¤¾à¤®à¥\nà¤¤à¤¯à¥‹à¤ƒ\nà¤¤à¤¾à¤¸à¥\nà¤…à¤¯à¤®à¥\nà¤‡à¤®à¥Œ\nà¤‡à¤®à¥‡\nà¤‡à¤®à¤®à¥\nà¤‡à¤®à¥Œ\nà¤‡à¤®à¤¾à¤¨à¥\nà¤…à¤¨à¥‡à¤¨\nà¤†à¤­à¥à¤¯à¤¾à¤®à¥\nà¤à¤­à¤¿à¤ƒ\nà¤…à¤¸à¥à¤®à¥ˆ\nà¤†à¤­à¥à¤¯à¤¾à¤®à¥\nà¤à¤­à¥à¤¯à¤ƒ\nà¤…à¤¸à¥à¤®à¤¾à¤¤à¥\nà¤†à¤­à¥à¤¯à¤¾à¤®à¥\nà¤à¤­à¥à¤¯à¤ƒ\nà¤…à¤¸à¥à¤¯\nà¤…à¤¨à¤¯à¥‹à¤ƒ\nà¤à¤·à¤¾à¤®à¥\nà¤…à¤¸à¥à¤®à¤¿à¤¨à¥\nà¤…à¤¨à¤¯à¥‹à¤ƒ\nà¤à¤·à¥\nà¤‡à¤¯à¤®à¥\nà¤‡à¤®à¥‡\nà¤‡à¤®à¤¾à¤ƒ\nà¤‡à¤®à¤¾à¤®à¥\nà¤‡à¤®à¥‡\nà¤‡à¤®à¤¾à¤ƒ\nà¤…à¤¨à¤¯à¤¾\nà¤†à¤­à¥à¤¯à¤¾à¤®à¥\nà¤†à¤­à¤¿à¤ƒ\nà¤…à¤¸à¥à¤¯à¥ˆ\nà¤†à¤­à¥à¤¯à¤¾à¤®à¥\nà¤†à¤­à¥à¤¯à¤ƒ\nà¤…à¤¸à¥à¤¯à¤¾à¤ƒ\nà¤†à¤­à¥à¤¯à¤¾à¤®à¥\nà¤†à¤­à¥à¤¯à¤ƒ\nà¤…à¤¸à¥à¤¯à¤¾à¤ƒ\nà¤…à¤¨à¤¯à¥‹à¤ƒ\nà¤†à¤¸à¤¾à¤®à¥\nà¤…à¤¸à¥à¤¯à¤¾à¤®à¥\nà¤…à¤¨à¤¯à¥‹à¤ƒ\nà¤†à¤¸à¥\nà¤‡à¤¦à¤®à¥\nà¤‡à¤®à¥‡\nà¤‡à¤®à¤¾à¤¨à¤¿\nà¤‡à¤¦à¤®à¥\nà¤‡à¤®à¥‡\nà¤‡à¤®à¤¾à¤¨à¤¿\nà¤…à¤¨à¥‡à¤¨\nà¤†à¤­à¥à¤¯à¤¾à¤®à¥\nà¤à¤­à¤¿à¤ƒ\nà¤…à¤¸à¥à¤®à¥ˆ\nà¤†à¤­à¥à¤¯à¤¾à¤®à¥\nà¤à¤­à¥à¤¯à¤ƒ\nà¤…à¤¸à¥à¤®à¤¾à¤¤à¥\nà¤†à¤­à¥à¤¯à¤¾à¤®à¥\nà¤à¤­à¥à¤¯à¤ƒ\nà¤…à¤¸à¥à¤¯\nà¤…à¤¨à¤¯à¥‹à¤ƒ\nà¤à¤·à¤¾à¤®à¥\nà¤…à¤¸à¥à¤®à¤¿à¤¨à¥\nà¤…à¤¨à¤¯à¥‹à¤ƒ\nà¤à¤·à¥\nà¤à¤·à¤ƒ\nà¤à¤¤à¥Œ\nà¤à¤¤à¥‡\nà¤à¤¤à¤®à¥  à¤à¤¨à¤®à¥\nà¤à¤¤à¥Œ  à¤à¤¨à¥Œ\nà¤à¤¤à¤¾à¤¨à¥  à¤à¤¨à¤¾à¤¨à¥\nà¤à¤¤à¥‡à¤¨\nà¤à¤¤à¤¾à¤­à¥à¤¯à¤¾à¤®à¥\nà¤à¤¤à¥ˆà¤ƒ\nà¤à¤¤à¤¸à¥à¤®à¥ˆ\nà¤à¤¤à¤¾à¤­à¥à¤¯à¤¾à¤®à¥\nà¤à¤¤à¥‡à¤­à¥à¤¯à¤ƒ\nà¤à¤¤à¤¸à¥à¤®à¤¾à¤¤à¥\nà¤à¤¤à¤¾à¤­à¥à¤¯à¤¾à¤®à¥\nà¤à¤¤à¥‡à¤­à¥à¤¯à¤ƒ\nà¤à¤¤à¤¸à¥à¤¯\nà¤à¤¤à¤¸à¥à¤®à¤¿à¤¨à¥\nà¤à¤¤à¥‡à¤·à¤¾à¤®à¥\nà¤à¤¤à¤¸à¥à¤®à¤¿à¤¨à¥\nà¤à¤¤à¤¸à¥à¤®à¤¿à¤¨à¥\nà¤à¤¤à¥‡à¤·à¥\nà¤à¤·à¤¾\nà¤à¤¤à¥‡\nà¤à¤¤à¤¾à¤ƒ\nà¤à¤¤à¤¾à¤®à¥  à¤à¤¨à¤¾à¤®à¥\nà¤à¤¤à¥‡  à¤à¤¨à¥‡\nà¤à¤¤à¤¾à¤ƒ  à¤à¤¨à¤¾à¤ƒ\nà¤à¤¤à¤¯à¤¾  à¤à¤¨à¤¯à¤¾\nà¤à¤¤à¤¾à¤­à¥à¤¯à¤¾à¤®à¥\nà¤à¤¤à¤¾à¤­à¤¿à¤ƒ\nà¤à¤¤à¤¸à¥à¤¯à¥ˆ\nà¤à¤¤à¤¾à¤­à¥à¤¯à¤¾à¤®à¥\nà¤à¤¤à¤¾à¤­à¥à¤¯à¤ƒ\nà¤à¤¤à¤¸à¥à¤¯à¤¾à¤ƒ\nà¤à¤¤à¤¾à¤­à¥à¤¯à¤¾à¤®à¥\nà¤à¤¤à¤¾à¤­à¥à¤¯à¤ƒ\nà¤à¤¤à¤¸à¥à¤¯à¤¾à¤ƒ\nà¤à¤¤à¤¯à¥‹à¤ƒ  à¤à¤¨à¤¯à¥‹à¤ƒ\nà¤à¤¤à¤¾à¤¸à¤¾à¤®à¥\nà¤à¤¤à¤¸à¥à¤¯à¤¾à¤®à¥\nà¤à¤¤à¤¯à¥‹à¤ƒ  à¤à¤¨à¤¯à¥‹à¤ƒ\nà¤à¤¤à¤¾à¤¸à¥\nà¤à¤¤à¤¤à¥  à¤à¤¤à¤¦à¥\nà¤à¤¤à¥‡\nà¤à¤¤à¤¾à¤¨à¤¿\nà¤à¤¤à¤¤à¥  à¤à¤¤à¤¦à¥  à¤à¤¨à¤¤à¥  à¤à¤¨à¤¦à¥\nà¤à¤¤à¥‡  à¤à¤¨à¥‡\nà¤à¤¤à¤¾à¤¨à¤¿  à¤à¤¨à¤¾à¤¨à¤¿\nà¤à¤¤à¥‡à¤¨  à¤à¤¨à¥‡à¤¨\nà¤à¤¤à¤¾à¤­à¥à¤¯à¤¾à¤®à¥\nà¤à¤¤à¥ˆà¤ƒ\nà¤à¤¤à¤¸à¥à¤®à¥ˆ\nà¤à¤¤à¤¾à¤­à¥à¤¯à¤¾à¤®à¥\nà¤à¤¤à¥‡à¤­à¥à¤¯à¤ƒ\nà¤à¤¤à¤¸à¥à¤®à¤¾à¤¤à¥\nà¤à¤¤à¤¾à¤­à¥à¤¯à¤¾à¤®à¥\nà¤à¤¤à¥‡à¤­à¥à¤¯à¤ƒ\nà¤à¤¤à¤¸à¥à¤¯\nà¤à¤¤à¤¯à¥‹à¤ƒ  à¤à¤¨à¤¯à¥‹à¤ƒ\nà¤à¤¤à¥‡à¤·à¤¾à¤®à¥\nà¤à¤¤à¤¸à¥à¤®à¤¿à¤¨à¥\nà¤à¤¤à¤¯à¥‹à¤ƒ  à¤à¤¨à¤¯à¥‹à¤ƒ\nà¤à¤¤à¥‡à¤·à¥\nà¤…à¤¸à¥Œ\nà¤…à¤®à¥‚\nà¤…à¤®à¥€\nà¤…à¤®à¥‚à¤®à¥\nà¤…à¤®à¥‚\nà¤…à¤®à¥‚à¤¨à¥\nà¤…à¤®à¥à¤¨à¤¾\nà¤…à¤®à¥‚à¤­à¥à¤¯à¤¾à¤®à¥\nà¤…à¤®à¥€à¤­à¤¿à¤ƒ\nà¤…à¤®à¥à¤·à¥à¤®à¥ˆ\nà¤…à¤®à¥‚à¤­à¥à¤¯à¤¾à¤®à¥\nà¤…à¤®à¥€à¤­à¥à¤¯à¤ƒ\nà¤…à¤®à¥à¤·à¥à¤®à¤¾à¤¤à¥\nà¤…à¤®à¥‚à¤­à¥à¤¯à¤¾à¤®à¥\nà¤…à¤®à¥€à¤­à¥à¤¯à¤ƒ\nà¤…à¤®à¥à¤·à¥à¤¯\nà¤…à¤®à¥à¤¯à¥‹à¤ƒ\nà¤…à¤®à¥€à¤·à¤¾à¤®à¥\nà¤…à¤®à¥à¤·à¥à¤®à¤¿à¤¨à¥\nà¤…à¤®à¥à¤¯à¥‹à¤ƒ\nà¤…à¤®à¥€à¤·à¥\nà¤…à¤¸à¥Œ\nà¤…à¤®à¥‚\nà¤…à¤®à¥‚à¤ƒ\nà¤…à¤®à¥‚à¤®à¥\nà¤…à¤®à¥‚\nà¤…à¤®à¥‚à¤ƒ\nà¤…à¤®à¥à¤¯à¤¾\nà¤…à¤®à¥‚à¤­à¥à¤¯à¤¾à¤®à¥\nà¤…à¤®à¥‚à¤­à¤¿à¤ƒ\nà¤…à¤®à¥à¤·à¥à¤¯à¥ˆ\nà¤…à¤®à¥‚à¤­à¥à¤¯à¤¾à¤®à¥\nà¤…à¤®à¥‚à¤­à¥à¤¯à¤ƒ\nà¤…à¤®à¥à¤·à¥à¤¯à¤¾à¤ƒ\nà¤…à¤®à¥‚à¤­à¥à¤¯à¤¾à¤®à¥\nà¤…à¤®à¥‚à¤­à¥à¤¯à¤ƒ\nà¤…à¤®à¥à¤·à¥à¤¯à¤¾à¤ƒ\nà¤…à¤®à¥à¤¯à¥‹à¤ƒ\nà¤…à¤®à¥‚à¤·à¤¾à¤®à¥\nà¤…à¤®à¥à¤·à¥à¤¯à¤¾à¤®à¥\nà¤…à¤®à¥à¤¯à¥‹à¤ƒ\nà¤…à¤®à¥‚à¤·à¥\nà¤…à¤®à¥\nà¤…à¤®à¥à¤¨à¥€\nà¤…à¤®à¥‚à¤¨à¤¿\nà¤…à¤®à¥\nà¤…à¤®à¥à¤¨à¥€\nà¤…à¤®à¥‚à¤¨à¤¿\nà¤…à¤®à¥à¤¨à¤¾\nà¤…à¤®à¥‚à¤­à¥à¤¯à¤¾à¤®à¥\nà¤…à¤®à¥€à¤­à¤¿à¤ƒ\nà¤…à¤®à¥à¤·à¥à¤®à¥ˆ\nà¤…à¤®à¥‚à¤­à¥à¤¯à¤¾à¤®à¥\nà¤…à¤®à¥€à¤­à¥à¤¯à¤ƒ\nà¤…à¤®à¥à¤·à¥à¤®à¤¾à¤¤à¥\nà¤…à¤®à¥‚à¤­à¥à¤¯à¤¾à¤®à¥\nà¤…à¤®à¥€à¤­à¥à¤¯à¤ƒ\nà¤…à¤®à¥à¤·à¥à¤¯\nà¤…à¤®à¥à¤¯à¥‹à¤ƒ\nà¤…à¤®à¥€à¤·à¤¾à¤®à¥\nà¤…à¤®à¥à¤·à¥à¤®à¤¿à¤¨à¥\nà¤…à¤®à¥à¤¯à¥‹à¤ƒ\nà¤…à¤®à¥€à¤·à¥\nà¤•à¤ƒ\nà¤•à¥Œ\nà¤•à¥‡\nà¤•à¤®à¥\nà¤•à¥Œ\nà¤•à¤¾à¤¨à¥\nà¤•à¥‡à¤¨\nà¤•à¤¾à¤­à¥à¤¯à¤¾à¤®à¥\nà¤•à¥ˆà¤ƒ\nà¤•à¤¸à¥à¤®à¥ˆ\nà¤•à¤¾à¤­à¥à¤¯à¤¾à¤®à¥\nà¤•à¥‡à¤­à¥à¤¯\nà¤•à¤¸à¥à¤®à¤¾à¤¤à¥\nà¤•à¤¾à¤­à¥à¤¯à¤¾à¤®à¥\nà¤•à¥‡à¤­à¥à¤¯\nà¤•à¤¸à¥à¤¯\nà¤•à¤¯à¥‹à¤ƒ\nà¤•à¥‡à¤·à¤¾à¤®à¥\nà¤•à¤¸à¥à¤®à¤¿à¤¨à¥\nà¤•à¤¯à¥‹à¤ƒ\nà¤•à¥‡à¤·à¥\nà¤•à¤¾\nà¤•à¥‡\nà¤•à¤¾à¤ƒ\nà¤•à¤¾à¤®à¥\nà¤•à¥‡\nà¤•à¤¾à¤ƒ\nà¤•à¤¯à¤¾\nà¤•à¤¾à¤­à¥à¤¯à¤¾à¤®à¥\nà¤•à¤¾à¤­à¤¿à¤ƒ\nà¤•à¤¸à¥à¤¯à¥ˆ\nà¤•à¤¾à¤­à¥à¤¯à¤¾à¤®à¥\nà¤•à¤¾à¤­à¥à¤¯à¤ƒ\nà¤•à¤¸à¥à¤¯à¤¾à¤ƒ\nà¤•à¤¾à¤­à¥à¤¯à¤¾à¤®à¥\nà¤•à¤¾à¤­à¥à¤¯à¤ƒ\nà¤•à¤¸à¥à¤¯à¤¾à¤ƒ\nà¤•à¤¯à¥‹à¤ƒ\nà¤•à¤¾à¤¸à¤¾à¤®à¥\nà¤•à¤¸à¥à¤¯à¤¾à¤®à¥\nà¤•à¤¯à¥‹à¤ƒ\nà¤•à¤¾à¤¸à¥\nà¤•à¤¿à¤®à¥\nà¤•à¥‡\nà¤•à¤¾à¤¨à¤¿\nà¤•à¤¿à¤®à¥\nà¤•à¥‡\nà¤•à¤¾à¤¨à¤¿\nà¤•à¥‡à¤¨\nà¤•à¤¾à¤­à¥à¤¯à¤¾à¤®à¥\nà¤•à¥ˆà¤ƒ\nà¤•à¤¸à¥à¤®à¥ˆ\nà¤•à¤¾à¤­à¥à¤¯à¤¾à¤®à¥\nà¤•à¥‡à¤­à¥à¤¯\nà¤•à¤¸à¥à¤®à¤¾à¤¤à¥\nà¤•à¤¾à¤­à¥à¤¯à¤¾à¤®à¥\nà¤•à¥‡à¤­à¥à¤¯\nà¤•à¤¸à¥à¤¯\nà¤•à¤¯à¥‹à¤ƒ\nà¤•à¥‡à¤·à¤¾à¤®à¥\nà¤•à¤¸à¥à¤®à¤¿à¤¨à¥\nà¤•à¤¯à¥‹à¤ƒ\nà¤•à¥‡à¤·à¥\nà¤­à¤µà¤¾à¤¨à¥\nà¤­à¤µà¤¨à¥à¤¤à¥Œ\nà¤­à¤µà¤¨à¥à¤¤à¤ƒ\nà¤­à¤µà¤¨à¥à¤¤à¤®à¥\nà¤­à¤µà¤¨à¥à¤¤à¥Œ\nà¤­à¤µà¤¤à¤ƒ\nà¤­à¤µà¤¤à¤¾\nà¤­à¤µà¤¦à¥à¤­à¥à¤¯à¤¾à¤®à¥\nà¤­à¤µà¤¦à¥à¤­à¤¿à¤ƒ\nà¤­à¤µà¤¤à¥‡\nà¤­à¤µà¤¦à¥à¤­à¥à¤¯à¤¾à¤®à¥\nà¤­à¤µà¤¦à¥à¤­à¥à¤¯à¤ƒ\nà¤­à¤µà¤¤à¤ƒ\nà¤­à¤µà¤¦à¥à¤­à¥à¤¯à¤¾à¤®à¥\nà¤­à¤µà¤¦à¥à¤­à¥à¤¯à¤ƒ\nà¤­à¤µà¤¤à¤ƒ\nà¤­à¤µà¤¤à¥‹à¤ƒ\nà¤­à¤µà¤¤à¤¾à¤®à¥\nà¤­à¤µà¤¤à¤¿\nà¤­à¤µà¤¤à¥‹à¤ƒ\nà¤­à¤µà¤¤à¥à¤¸à¥\nà¤­à¤µà¤¤à¥€\nà¤­à¤µà¤¤à¥à¤¯à¥Œ\nà¤­à¤µà¤¤à¥à¤¯à¤ƒ\nà¤­à¤µà¤¤à¥€à¤®à¥\nà¤­à¤µà¤¤à¥à¤¯à¥Œ\nà¤­à¤µà¤¤à¥€à¤ƒ\nà¤­à¤µà¤¤à¥à¤¯à¤¾\nà¤­à¤µà¤¤à¥€à¤­à¥à¤¯à¤¾à¤®à¥\nà¤­à¤µà¤¤à¥€à¤­à¤¿à¤ƒ\nà¤­à¤µà¤¤à¥à¤¯à¥ˆ\nà¤­à¤µà¤¤à¥€à¤­à¥à¤¯à¤¾à¤®à¥\nà¤­à¤µà¤¤à¥€à¤­à¤¿à¤ƒ\nà¤­à¤µà¤¤à¥à¤¯à¤¾à¤ƒ\nà¤­à¤µà¤¤à¥€à¤­à¥à¤¯à¤¾à¤®à¥\nà¤­à¤µà¤¤à¥€à¤­à¤¿à¤ƒ\nà¤­à¤µà¤¤à¥à¤¯à¤¾à¤ƒ\nà¤­à¤µà¤¤à¥à¤¯à¥‹à¤ƒ\nà¤­à¤µà¤¤à¥€à¤¨à¤¾à¤®à¥\nà¤­à¤µà¤¤à¥à¤¯à¤¾à¤®à¥\nà¤­à¤µà¤¤à¥à¤¯à¥‹à¤ƒ\nà¤­à¤µà¤¤à¥€à¤·à¥\nà¤­à¤µà¤¤à¥\nà¤­à¤µà¤¤à¥€\nà¤­à¤µà¤¨à¥à¤¤à¤¿\nà¤­à¤µà¤¤à¥\nà¤­à¤µà¤¤à¥€\nà¤­à¤µà¤¨à¥à¤¤à¤¿\nà¤­à¤µà¤¤à¤¾\nà¤­à¤µà¤¦à¥à¤­à¥à¤¯à¤¾à¤®à¥\nà¤­à¤µà¤¦à¥à¤­à¤¿à¤ƒ\nà¤­à¤µà¤¤à¥‡\nà¤­à¤µà¤¦à¥à¤­à¥à¤¯à¤¾à¤®à¥\nà¤­à¤µà¤¦à¥à¤­à¥à¤¯à¤ƒ\nà¤­à¤µà¤¤à¤ƒ\nà¤­à¤µà¤¦à¥à¤­à¥à¤¯à¤¾à¤®à¥\nà¤­à¤µà¤¦à¥à¤­à¥à¤¯à¤ƒ\nà¤­à¤µà¤¤à¤ƒ\nà¤­à¤µà¤¤à¥‹à¤ƒ\nà¤­à¤µà¤¤à¤¾à¤®à¥\nà¤­à¤µà¤¤à¤¿\nà¤­à¤µà¤¤à¥‹à¤ƒ\nà¤­à¤µà¤¤à¥à¤¸à¥\nà¤…à¤¯à¥‡\nà¤…à¤°à¥‡\nà¤…à¤°à¥‡à¤°à¥‡\nà¤…à¤µà¤¿à¤§à¤¾\nà¤…à¤¸à¤¾à¤§à¥à¤¨à¤¾\nà¤…à¤¸à¥à¤¤à¥‹à¤­\nà¤…à¤¹à¤¹\nà¤…à¤¹à¤¾à¤µà¤¸à¥\nà¤†à¤®à¥\nà¤†à¤°à¥à¤¯à¤¹à¤²à¤®à¥\nà¤†à¤¹\nà¤†à¤¹à¥‹\nà¤‡à¤¸à¥\nà¤‰à¤®à¥\nà¤‰à¤µà¥‡\nà¤•à¤¾à¤®à¥\nà¤•à¥à¤®à¥\nà¤šà¤®à¤¤à¥\nà¤Ÿà¤¸à¤¤à¥\nà¤¦à¥ƒà¤¨à¥\nà¤§à¤¿à¤•à¥\nà¤ªà¤¾à¤Ÿà¥\nà¤«à¤¤à¥\nà¤«à¤¾à¤Ÿà¥\nà¤«à¥à¤¡à¥à¤¤à¥\nà¤¬à¤¤\nà¤¬à¤¾à¤²à¥\nà¤µà¤Ÿà¥\nà¤µà¥à¤¯à¤µà¤¸à¥à¤¤à¥‹à¤­à¤¤à¤¿ à¤µà¥à¤¯à¤µà¤¸à¥à¤¤à¥à¤­à¥\nà¤·à¤¾à¤Ÿà¥\nà¤¸à¥à¤¤à¥‹à¤­\nà¤¹à¥à¤®à¥à¤®à¤¾\nà¤¹à¥‚à¤®à¥\nà¤…à¤¤à¤¿\nà¤…à¤§à¤¿\nà¤…à¤¨à¥\nà¤…à¤ª\nà¤…à¤ªà¤¿\nà¤…à¤­à¤¿\nà¤…à¤µ\nà¤†\nà¤‰à¤¦à¥\nà¤‰à¤ª\nà¤¨à¤¿\nà¤¨à¤¿à¤°à¥\nà¤ªà¤°à¤¾\nà¤ªà¤°à¤¿\nà¤ªà¥à¤°\nà¤ªà¥à¤°à¤¤à¤¿\nà¤µà¤¿\nà¤¸à¤®à¥\nà¤…à¤¥à¤µà¤¾ à¤‰à¤¤\nà¤…à¤¨à¥à¤¯à¤¥à¤¾\nà¤‡à¤µ\nà¤š\nà¤šà¥‡à¤¤à¥ à¤¯à¤¦à¤¿\nà¤¤à¥ à¤ªà¤°à¤¨à¥à¤¤à¥\nà¤¯à¤¤à¤ƒ à¤•à¤°à¤£à¥‡à¤¨ à¤¹à¤¿ à¤¯à¤¤à¤¸à¥ à¤¯à¤¦à¤°à¥à¤¥à¤®à¥ à¤¯à¤¦à¤°à¥à¤¥à¥‡ à¤¯à¤°à¥à¤¹à¤¿ à¤¯à¤¥à¤¾ à¤¯à¤¤à¥à¤•à¤¾à¤°à¤£à¤®à¥ à¤¯à¥‡à¤¨ à¤¹à¥€ à¤¹à¤¿à¤¨\nà¤¯à¤¥à¤¾ à¤¯à¤¤à¤¸à¥\nà¤¯à¤¦à¥à¤¯à¤ªà¤¿\nà¤¯à¤¾à¤¤à¥ à¤…à¤µà¤§à¥‡à¤¸à¥ à¤¯à¤¾à¤µà¤¤à¤¿\nà¤¯à¥‡à¤¨ à¤ªà¥à¤°à¤•à¤¾à¤°à¥‡à¤£\nà¤¸à¥à¤¥à¤¾à¤¨à¥‡\nà¤…à¤¹\nà¤à¤µ\nà¤à¤µà¤®à¥\nà¤•à¤šà¥à¤šà¤¿à¤¤à¥\nà¤•à¥\nà¤•à¥à¤µà¤¿à¤¤à¥\nà¤•à¥‚à¤ªà¤¤à¥\nà¤š\nà¤šà¤£à¥\nà¤šà¥‡à¤¤à¥\nà¤¤à¤¤à¥à¤°\nà¤¨à¤•à¤¿à¤®à¥\nà¤¨à¤¹\nà¤¨à¥à¤¨à¤®à¥\nà¤¨à¥‡à¤¤à¥\nà¤­à¥‚à¤¯à¤¸à¥\nà¤®à¤•à¤¿à¤®à¥\nà¤®à¤•à¤¿à¤°à¥\nà¤¯à¤¤à¥à¤°\nà¤¯à¥à¤—à¤ªà¤¤à¥\nà¤µà¤¾\nà¤¶à¤¶à¥à¤µà¤¤à¥\nà¤¸à¥‚à¤ªà¤¤à¥\nà¤¹\nà¤¹à¤¨à¥à¤¤\nà¤¹à¤¿\n'.split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/sa/__init__.py----------------------------------------
spacy.lang.sa.__init__.Sanskrit(Language)
spacy.lang.sa.__init__.SanskritDefaults(BaseDefaults)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/sa/lex_attrs.py----------------------------------------
A:spacy.lang.sa.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.sa.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.sa.lex_attrs.like_num(text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/te/examples.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/te/stop_words.py----------------------------------------
A:spacy.lang.te.stop_words.STOP_WORDS->set('\nà°…à°‚à°¦à°°à±‚\nà°…à°‚à°¦à±à°¬à°¾à°Ÿà±à°²à±‹\nà°…à°¡à°—à°‚à°¡à°¿\nà°…à°¡à°—à°¡à°‚\nà°…à°¡à±à°¡à°‚à°—à°¾\nà°…à°¨à±à°—à±à°£à°‚à°—à°¾\nà°…à°¨à±à°®à°¤à°¿à°‚à°šà±\nà°…à°¨à±à°®à°¤à°¿à°¸à±à°¤à±à°‚à°¦à°¿\nà°…à°¯à°¿à°¤à±‡\nà°‡à°ªà±à°ªà°Ÿà°¿à°•à±‡\nà°‰à°¨à±à°¨à°¾à°°à±\nà°Žà°•à±à°•à°¡à±ˆà°¨à°¾\nà°Žà°ªà±à°ªà±à°¡à±\nà°Žà°µà°°à±ˆà°¨à°¾\nà°Žà°µà°°à±‹ à°’à°•à°°à±\nà°\nà°à°¦à±ˆà°¨à°¾\nà°à°®à±ˆà°¨à°ªà±à°ªà°Ÿà°¿à°•à°¿\nà°à°®à±ˆà°¨à°ªà±à°ªà°Ÿà°¿à°•à°¿\nà°’à°•\nà°’à°• à°ªà±à°°à°•à±à°•à°¨\nà°•à°¨à°¿à°ªà°¿à°¸à±à°¤à°¾à°¯à°¿\nà°•à°¾à°¦à±\nà°•à°¾à°¦à±\nà°•à±‚à°¡à°¾\nà°—à°¾\nà°—à±à°°à°¿à°‚à°šà°¿\nà°šà±à°Ÿà±à°Ÿà±‚\nà°šà±‡à°¯à°—à°²à°¿à°—à°¿à°‚à°¦à°¿\nà°¤à°—à°¿à°¨\nà°¤à°°à±à°µà°¾à°¤\nà°¤à°°à±à°µà°¾à°¤\nà°¦à°¾à°¦à°¾à°ªà±\nà°¦à±‚à°°à°‚à°—à°¾\nà°¨à°¿à°œà°‚à°—à°¾\nà°ªà±ˆ\nà°ªà±à°°à°•à°¾à°°à°‚\nà°®à°§à±à°¯\nà°®à°§à±à°¯\nà°®à°°à°¿à°¯à±\nà°®à°°à±Šà°•\nà°®à°³à±à°³à±€\nà°®à°¾à°¤à±à°°à°®à±‡\nà°®à±†à°šà±à°šà±à°•à±‹\nà°µà°¦à±à°¦\nà°µà°¦à±à°¦\nà°µà±†à°‚à°Ÿ\nà°µà±‡à°°à±à°—à°¾\nà°µà±à°¯à°¤à°¿à°°à±‡à°•à°‚à°—à°¾\nà°¸à°‚à°¬à°‚à°§à°‚\n'.split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/te/__init__.py----------------------------------------
spacy.lang.te.__init__.Telugu(Language)
spacy.lang.te.__init__.TeluguDefaults(BaseDefaults)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/te/lex_attrs.py----------------------------------------
A:spacy.lang.te.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.te.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.te.lex_attrs.like_num(text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/zh/examples.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/zh/stop_words.py----------------------------------------
A:spacy.lang.zh.stop_words.STOP_WORDS->set('\n!\n"\n#\n$\n%\n&\n\'\n(\n)\n*\n+\n,\n-\n--\n.\n..\n...\n......\n...................\n./\n.ä¸€\n.æ•°\n.æ—¥\n/\n//\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n:\n://\n::\n;\n<\n=\n>\n>>\n?\n@\nA\nLex\n[\n]\n^\n_\n`\nexp\nsub\nsup\n|\n}\n~\n~~~~\nÂ·\nÃ—\nÃ—Ã—Ã—\nÎ”\nÎ¨\nÎ³\nÎ¼\nÏ†\nÏ†ï¼Ž\nÐ’\nâ€”\nâ€”â€”\nâ€”â€”â€”\nâ€˜\nâ€™\nâ€™â€˜\nâ€œ\nâ€\nâ€ï¼Œ\nâ€¦\nâ€¦â€¦\nâ€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â‘¢\nâ€²âˆˆ\nâ€²ï½œ\nâ„ƒ\nâ…¢\nâ†‘\nâ†’\nâˆˆï¼»\nâˆªÏ†âˆˆ\nâ‰ˆ\nâ‘ \nâ‘¡\nâ‘¡ï½ƒ\nâ‘¢\nâ‘¢ï¼½\nâ‘£\nâ‘¤\nâ‘¥\nâ‘¦\nâ‘§\nâ‘¨\nâ‘©\nâ”€â”€\nâ– \nâ–²\n\u3000\nã€\nã€‚\nã€ˆ\nã€‰\nã€Š\nã€‹\nã€‹ï¼‰ï¼Œ\nã€\nã€Ž\nã€\nã€\nã€‘\nã€”\nã€•\nã€•ã€”\nãˆ§\nä¸€\nä¸€.\nä¸€ä¸€\nä¸€ä¸‹\nä¸€ä¸ª\nä¸€äº›\nä¸€ä½•\nä¸€åˆ‡\nä¸€åˆ™\nä¸€åˆ™é€šè¿‡\nä¸€å¤©\nä¸€å®š\nä¸€æ–¹é¢\nä¸€æ—¦\nä¸€æ—¶\nä¸€æ¥\nä¸€æ ·\nä¸€æ¬¡\nä¸€ç‰‡\nä¸€ç•ª\nä¸€ç›´\nä¸€è‡´\nä¸€èˆ¬\nä¸€èµ·\nä¸€è½¬çœ¼\nä¸€è¾¹\nä¸€é¢\nä¸ƒ\nä¸‡ä¸€\nä¸‰\nä¸‰å¤©ä¸¤å¤´\nä¸‰ç•ªä¸¤æ¬¡\nä¸‰ç•ªäº”æ¬¡\nä¸Š\nä¸Šä¸‹\nä¸Šå‡\nä¸ŠåŽ»\nä¸Šæ¥\nä¸Šè¿°\nä¸Šé¢\nä¸‹\nä¸‹åˆ—\nä¸‹åŽ»\nä¸‹æ¥\nä¸‹é¢\nä¸\nä¸ä¸€\nä¸ä¸‹\nä¸ä¹…\nä¸äº†\nä¸äº¦ä¹ä¹Ž\nä¸ä»…\nä¸ä»…...è€Œä¸”\nä¸ä»…ä»…\nä¸ä»…ä»…æ˜¯\nä¸ä¼š\nä¸ä½†\nä¸ä½†...è€Œä¸”\nä¸å…‰\nä¸å…\nä¸å†\nä¸åŠ›\nä¸å•\nä¸å˜\nä¸åª\nä¸å¯\nä¸å¯å¼€äº¤\nä¸å¯æŠ—æ‹’\nä¸åŒ\nä¸å¤–\nä¸å¤–ä¹Ž\nä¸å¤Ÿ\nä¸å¤§\nä¸å¦‚\nä¸å¦¨\nä¸å®š\nä¸å¯¹\nä¸å°‘\nä¸å°½\nä¸å°½ç„¶\nä¸å·§\nä¸å·²\nä¸å¸¸\nä¸å¾—\nä¸å¾—ä¸\nä¸å¾—äº†\nä¸å¾—å·²\nä¸å¿…\nä¸æ€Žä¹ˆ\nä¸æ€•\nä¸æƒŸ\nä¸æˆ\nä¸æ‹˜\nä¸æ‹©æ‰‹æ®µ\nä¸æ•¢\nä¸æ–™\nä¸æ–­\nä¸æ—¥\nä¸æ—¶\nä¸æ˜¯\nä¸æ›¾\nä¸æ­¢\nä¸æ­¢ä¸€æ¬¡\nä¸æ¯”\nä¸æ¶ˆ\nä¸æ»¡\nä¸ç„¶\nä¸ç„¶çš„è¯\nä¸ç‰¹\nä¸ç‹¬\nä¸ç”±å¾—\nä¸çŸ¥ä¸è§‰\nä¸ç®¡\nä¸ç®¡æ€Žæ ·\nä¸ç»æ„\nä¸èƒœ\nä¸èƒ½\nä¸èƒ½ä¸\nä¸è‡³äºŽ\nä¸è‹¥\nä¸è¦\nä¸è®º\nä¸èµ·\nä¸è¶³\nä¸è¿‡\nä¸è¿­\nä¸é—®\nä¸é™\nä¸Ž\nä¸Žå…¶\nä¸Žå…¶è¯´\nä¸Žå¦\nä¸Žæ­¤åŒæ—¶\nä¸“é—¨\nä¸”\nä¸”ä¸è¯´\nä¸”è¯´\nä¸¤è€…\nä¸¥æ ¼\nä¸¥é‡\nä¸ª\nä¸ªäºº\nä¸ªåˆ«\nä¸­å°\nä¸­é—´\nä¸°å¯Œ\nä¸²è¡Œ\nä¸´\nä¸´åˆ°\nä¸º\nä¸ºä¸»\nä¸ºäº†\nä¸ºä»€ä¹ˆ\nä¸ºä»€éº½\nä¸ºä½•\nä¸ºæ­¢\nä¸ºæ­¤\nä¸ºç€\nä¸»å¼ \nä¸»è¦\nä¸¾å‡¡\nä¸¾è¡Œ\nä¹ƒ\nä¹ƒè‡³\nä¹ƒè‡³äºŽ\nä¹ˆ\nä¹‹\nä¹‹ä¸€\nä¹‹å‰\nä¹‹åŽ\nä¹‹å¾Œ\nä¹‹æ‰€ä»¥\nä¹‹ç±»\nä¹Œä¹Ž\nä¹Ž\nä¹’\nä¹˜\nä¹˜åŠ¿\nä¹˜æœº\nä¹˜èƒœ\nä¹˜è™š\nä¹˜éš™\nä¹\nä¹Ÿ\nä¹Ÿå¥½\nä¹Ÿå°±æ˜¯è¯´\nä¹Ÿæ˜¯\nä¹Ÿç½¢\näº†\näº†è§£\näº‰å–\näºŒ\näºŒæ¥\näºŒè¯ä¸è¯´\näºŒè¯æ²¡è¯´\näºŽ\näºŽæ˜¯\näºŽæ˜¯ä¹Ž\näº‘äº‘\näº‘å°”\näº’\näº’ç›¸\näº”\näº›\näº¤å£\näº¦\näº§ç”Ÿ\näº²å£\näº²æ‰‹\näº²çœ¼\näº²è‡ª\näº²èº«\näºº\näººäºº\näººä»¬\näººå®¶\näººæ°‘\nä»€ä¹ˆ\nä»€ä¹ˆæ ·\nä»€éº½\nä»…\nä»…ä»…\nä»Š\nä»ŠåŽ\nä»Šå¤©\nä»Šå¹´\nä»Šå¾Œ\nä»‹äºŽ\nä»\nä»æ—§\nä»ç„¶\nä»Ž\nä»Žä¸\nä»Žä¸¥\nä»Žä¸­\nä»Žäº‹\nä»Žä»Šä»¥åŽ\nä»Žä¼˜\nä»Žå¤åˆ°ä»Š\nä»Žå¤è‡³ä»Š\nä»Žå¤´\nä»Žå®½\nä»Žå°\nä»Žæ–°\nä»Žæ— åˆ°æœ‰\nä»Žæ—©åˆ°æ™š\nä»Žæœª\nä»Žæ¥\nä»Žæ­¤\nä»Žæ­¤ä»¥åŽ\nä»Žè€Œ\nä»Žè½»\nä»Žé€Ÿ\nä»Žé‡\nä»–\nä»–äºº\nä»–ä»¬\nä»–æ˜¯\nä»–çš„\nä»£æ›¿\nä»¥\nä»¥ä¸Š\nä»¥ä¸‹\nä»¥ä¸º\nä»¥ä¾¿\nä»¥å…\nä»¥å‰\nä»¥åŠ\nä»¥åŽ\nä»¥å¤–\nä»¥å¾Œ\nä»¥æ•…\nä»¥æœŸ\nä»¥æ¥\nä»¥è‡³\nä»¥è‡³äºŽ\nä»¥è‡´\nä»¬\nä»»\nä»»ä½•\nä»»å‡­\nä»»åŠ¡\nä¼å›¾\nä¼™åŒ\nä¼š\nä¼Ÿå¤§\nä¼ \nä¼ è¯´\nä¼ é—»\nä¼¼ä¹Ž\nä¼¼çš„\nä½†\nä½†å‡¡\nä½†æ„¿\nä½†æ˜¯\nä½•\nä½•ä¹è€Œä¸ä¸º\nä½•ä»¥\nä½•å†µ\nä½•å¤„\nä½•å¦¨\nä½•å°\nä½•å¿…\nä½•æ—¶\nä½•æ­¢\nä½•è‹¦\nä½•é¡»\nä½™å¤–\nä½œä¸º\nä½ \nä½ ä»¬\nä½ æ˜¯\nä½ çš„\nä½¿\nä½¿å¾—\nä½¿ç”¨\nä¾‹å¦‚\nä¾\nä¾æ®\nä¾ç…§\nä¾é \nä¾¿\nä¾¿äºŽ\nä¿ƒè¿›\nä¿æŒ\nä¿ç®¡\nä¿é™©\nä¿º\nä¿ºä»¬\nå€åŠ \nå€æ„Ÿ\nå€’ä¸å¦‚\nå€’ä¸å¦‚è¯´\nå€’æ˜¯\nå€˜\nå€˜ä½¿\nå€˜æˆ–\nå€˜ç„¶\nå€˜è‹¥\nå€Ÿ\nå€Ÿä»¥\nå€Ÿæ­¤\nå‡ä½¿\nå‡å¦‚\nå‡è‹¥\nåå\nåšåˆ°\nå¶å°”\nå¶è€Œ\nå‚¥ç„¶\nåƒ\nå„¿\nå…è®¸\nå…ƒï¼å¨\nå……å…¶æž\nå……å…¶é‡\nå……åˆ†\nå…ˆä¸å…ˆ\nå…ˆåŽ\nå…ˆå¾Œ\nå…ˆç”Ÿ\nå…‰\nå…‰æ˜¯\nå…¨ä½“\nå…¨åŠ›\nå…¨å¹´\nå…¨ç„¶\nå…¨èº«å¿ƒ\nå…¨éƒ¨\nå…¨éƒ½\nå…¨é¢\nå…«\nå…«æˆ\nå…¬ç„¶\nå…­\nå…®\nå…±\nå…±åŒ\nå…±æ€»\nå…³äºŽ\nå…¶\nå…¶ä¸€\nå…¶ä¸­\nå…¶äºŒ\nå…¶ä»–\nå…¶ä½™\nå…¶åŽ\nå…¶å®ƒ\nå…¶å®ž\nå…¶æ¬¡\nå…·ä½“\nå…·ä½“åœ°è¯´\nå…·ä½“æ¥è¯´\nå…·ä½“è¯´æ¥\nå…·æœ‰\nå…¼ä¹‹\nå†…\nå†\nå†å…¶æ¬¡\nå†åˆ™\nå†æœ‰\nå†æ¬¡\nå†è€…\nå†è€…è¯´\nå†è¯´\nå†’\nå†²\nå†³ä¸\nå†³å®š\nå†³éž\nå†µä¸”\nå‡†å¤‡\nå‡‘å·§\nå‡ç¥ž\nå‡ \nå‡ ä¹Ž\nå‡ åº¦\nå‡ æ—¶\nå‡ ç•ª\nå‡ ç»\nå‡¡\nå‡¡æ˜¯\nå‡­\nå‡­å€Ÿ\nå‡º\nå‡ºäºŽ\nå‡ºåŽ»\nå‡ºæ¥\nå‡ºçŽ°\nåˆ†åˆ«\nåˆ†å¤´\nåˆ†æœŸ\nåˆ†æœŸåˆ†æ‰¹\nåˆ‡\nåˆ‡ä¸å¯\nåˆ‡åˆ‡\nåˆ‡å‹¿\nåˆ‡èŽ«\nåˆ™\nåˆ™ç”š\nåˆš\nåˆšå¥½\nåˆšå·§\nåˆšæ‰\nåˆ\nåˆ«\nåˆ«äºº\nåˆ«å¤„\nåˆ«æ˜¯\nåˆ«çš„\nåˆ«ç®¡\nåˆ«è¯´\nåˆ°\nåˆ°äº†å„¿\nåˆ°å¤„\nåˆ°å¤´\nåˆ°å¤´æ¥\nåˆ°åº•\nåˆ°ç›®å‰ä¸ºæ­¢\nå‰åŽ\nå‰æ­¤\nå‰è€…\nå‰è¿›\nå‰é¢\nåŠ ä¸Š\nåŠ ä¹‹\nåŠ ä»¥\nåŠ å…¥\nåŠ å¼º\nåŠ¨ä¸åŠ¨\nåŠ¨è¾„\nå‹ƒç„¶\nåŒ†åŒ†\nååˆ†\nåƒ\nåƒä¸‡\nåƒä¸‡åƒä¸‡\nåŠ\nå•\nå•å•\nå•çº¯\nå³\nå³ä»¤\nå³ä½¿\nå³ä¾¿\nå³åˆ»\nå³å¦‚\nå³å°†\nå³æˆ–\nå³æ˜¯è¯´\nå³è‹¥\nå´\nå´ä¸\nåŽ†\nåŽŸæ¥\nåŽ»\nåˆ\nåˆåŠ\nåŠ\nåŠå…¶\nåŠæ—¶\nåŠè‡³\nåŒæ–¹\nåä¹‹\nåä¹‹äº¦ç„¶\nåä¹‹åˆ™\nåå€’\nåå€’æ˜¯\nååº”\nåæ‰‹\nåæ˜ \nåè€Œ\nåè¿‡æ¥\nåè¿‡æ¥è¯´\nå–å¾—\nå–é“\nå—åˆ°\nå˜æˆ\nå¤æ¥\nå¦\nå¦ä¸€ä¸ª\nå¦ä¸€æ–¹é¢\nå¦å¤–\nå¦æ‚‰\nå¦æ–¹é¢\nå¦è¡Œ\nåª\nåªå½“\nåªæ€•\nåªæ˜¯\nåªæœ‰\nåªæ¶ˆ\nåªè¦\nåªé™\nå«\nå«åš\nå¬å¼€\nå®å’š\nå®å½“\nå¯\nå¯ä»¥\nå¯å¥½\nå¯æ˜¯\nå¯èƒ½\nå¯è§\nå„\nå„ä¸ª\nå„äºº\nå„ä½\nå„åœ°\nå„å¼\nå„ç§\nå„çº§\nå„è‡ª\nåˆç†\nåŒ\nåŒä¸€\nåŒæ—¶\nåŒæ ·\nåŽ\nåŽæ¥\nåŽè€…\nåŽé¢\nå‘\nå‘ä½¿\nå‘ç€\nå“\nå—\nå¦åˆ™\nå§\nå§å“’\nå±\nå‘€\nå‘ƒ\nå‘†å‘†åœ°\nå‘\nå‘•\nå‘—\nå‘œ\nå‘œå‘¼\nå‘¢\nå‘¨å›´\nå‘µ\nå‘µå‘µ\nå‘¸\nå‘¼å“§\nå‘¼å•¦\nå’‹\nå’Œ\nå’š\nå’¦\nå’§\nå’±\nå’±ä»¬\nå’³\nå“‡\nå“ˆ\nå“ˆå“ˆ\nå“‰\nå“Ž\nå“Žå‘€\nå“Žå“Ÿ\nå“—\nå“—å•¦\nå“Ÿ\nå“¦\nå“©\nå“ª\nå“ªä¸ª\nå“ªäº›\nå“ªå„¿\nå“ªå¤©\nå“ªå¹´\nå“ªæ€•\nå“ªæ ·\nå“ªè¾¹\nå“ªé‡Œ\nå“¼\nå“¼å”·\nå”‰\nå”¯æœ‰\nå•Š\nå•Šå‘€\nå•Šå“ˆ\nå•Šå“Ÿ\nå•\nå•¥\nå•¦\nå•ªè¾¾\nå•·å½“\nå–€\nå–‚\nå–\nå–”å”·\nå–½\nå—¡\nå—¡å—¡\nå—¬\nå—¯\nå—³\nå˜Ž\nå˜Žå˜Ž\nå˜Žç™»\nå˜˜\nå˜›\nå˜»\nå˜¿\nå˜¿å˜¿\nå››\nå› \nå› ä¸º\nå› äº†\nå› æ­¤\nå› ç€\nå› è€Œ\nå›º\nå›ºç„¶\nåœ¨\nåœ¨ä¸‹\nåœ¨äºŽ\nåœ°\nå‡\nåšå†³\nåšæŒ\nåŸºäºŽ\nåŸºæœ¬\nåŸºæœ¬ä¸Š\nå¤„åœ¨\nå¤„å¤„\nå¤„ç†\nå¤æ‚\nå¤š\nå¤šä¹ˆ\nå¤šäº\nå¤šå¤š\nå¤šå¤šå°‘å°‘\nå¤šå¤šç›Šå–„\nå¤šå°‘\nå¤šå¹´å‰\nå¤šå¹´æ¥\nå¤šæ•°\nå¤šæ¬¡\nå¤Ÿçž§çš„\nå¤§\nå¤§ä¸äº†\nå¤§ä¸¾\nå¤§äº‹\nå¤§ä½“\nå¤§ä½“ä¸Š\nå¤§å‡¡\nå¤§åŠ›\nå¤§å¤š\nå¤§å¤šæ•°\nå¤§å¤§\nå¤§å®¶\nå¤§å¼ æ——é¼“\nå¤§æ‰¹\nå¤§æŠµ\nå¤§æ¦‚\nå¤§ç•¥\nå¤§çº¦\nå¤§è‡´\nå¤§éƒ½\nå¤§é‡\nå¤§é¢å„¿ä¸Š\nå¤±åŽ»\nå¥‡\nå¥ˆ\nå¥‹å‹‡\nå¥¹\nå¥¹ä»¬\nå¥¹æ˜¯\nå¥¹çš„\nå¥½\nå¥½åœ¨\nå¥½çš„\nå¥½è±¡\nå¦‚\nå¦‚ä¸Š\nå¦‚ä¸Šæ‰€è¿°\nå¦‚ä¸‹\nå¦‚ä»Š\nå¦‚ä½•\nå¦‚å…¶\nå¦‚å‰æ‰€è¿°\nå¦‚åŒ\nå¦‚å¸¸\nå¦‚æ˜¯\nå¦‚æœŸ\nå¦‚æžœ\nå¦‚æ¬¡\nå¦‚æ­¤\nå¦‚æ­¤ç­‰ç­‰\nå¦‚è‹¥\nå§‹è€Œ\nå§‘ä¸”\nå­˜åœ¨\nå­˜å¿ƒ\nå­°æ–™\nå­°çŸ¥\nå®\nå®å¯\nå®æ„¿\nå®è‚¯\nå®ƒ\nå®ƒä»¬\nå®ƒä»¬çš„\nå®ƒæ˜¯\nå®ƒçš„\nå®‰å…¨\nå®Œå…¨\nå®Œæˆ\nå®š\nå®žçŽ°\nå®žé™…\nå®£å¸ƒ\nå®¹æ˜“\nå¯†åˆ‡\nå¯¹\nå¯¹äºŽ\nå¯¹åº”\nå¯¹å¾…\nå¯¹æ–¹\nå¯¹æ¯”\nå°†\nå°†æ‰\nå°†è¦\nå°†è¿‘\nå°\nå°‘æ•°\nå°”\nå°”åŽ\nå°”å°”\nå°”ç­‰\nå°šä¸”\nå°¤å…¶\nå°±\nå°±åœ°\nå°±æ˜¯\nå°±æ˜¯äº†\nå°±æ˜¯è¯´\nå°±æ­¤\nå°±ç®—\nå°±è¦\nå°½\nå°½å¯èƒ½\nå°½å¦‚äººæ„\nå°½å¿ƒå°½åŠ›\nå°½å¿ƒç«­åŠ›\nå°½å¿«\nå°½æ—©\nå°½ç„¶\nå°½ç®¡\nå°½ç®¡å¦‚æ­¤\nå°½é‡\nå±€å¤–\nå±…ç„¶\nå±Šæ—¶\nå±žäºŽ\nå±¡\nå±¡å±¡\nå±¡æ¬¡\nå±¡æ¬¡ä¸‰ç•ª\nå²‚\nå²‚ä½†\nå²‚æ­¢\nå²‚éž\nå·æµä¸æ¯\nå·¦å³\nå·¨å¤§\nå·©å›º\nå·®ä¸€ç‚¹\nå·®ä¸å¤š\nå·±\nå·²\nå·²çŸ£\nå·²ç»\nå·´\nå·´å·´\nå¸¦\nå¸®åŠ©\nå¸¸\nå¸¸å¸¸\nå¸¸è¨€è¯´\nå¸¸è¨€è¯´å¾—å¥½\nå¸¸è¨€é“\nå¹³ç´ \nå¹´å¤ä¸€å¹´\nå¹¶\nå¹¶ä¸\nå¹¶ä¸æ˜¯\nå¹¶ä¸”\nå¹¶æŽ’\nå¹¶æ— \nå¹¶æ²¡\nå¹¶æ²¡æœ‰\nå¹¶è‚©\nå¹¶éž\nå¹¿å¤§\nå¹¿æ³›\nåº”å½“\nåº”ç”¨\nåº”è¯¥\nåº¶ä¹Ž\nåº¶å‡ \nå¼€å¤–\nå¼€å§‹\nå¼€å±•\nå¼•èµ·\nå¼—\nå¼¹æŒ‡ä¹‹é—´\nå¼ºçƒˆ\nå¼ºè°ƒ\nå½’\nå½’æ ¹åˆ°åº•\nå½’æ ¹ç»“åº•\nå½’é½\nå½“\nå½“ä¸‹\nå½“ä¸­\nå½“å„¿\nå½“å‰\nå½“å³\nå½“å£å„¿\nå½“åœ°\nå½“åœº\nå½“å¤´\nå½“åº­\nå½“æ—¶\nå½“ç„¶\nå½“çœŸ\nå½“ç€\nå½¢æˆ\nå½»å¤œ\nå½»åº•\nå½¼\nå½¼æ—¶\nå½¼æ­¤\nå¾€\nå¾€å¾€\nå¾…\nå¾…åˆ°\nå¾ˆ\nå¾ˆå¤š\nå¾ˆå°‘\nå¾Œæ¥\nå¾Œé¢\nå¾—\nå¾—äº†\nå¾—å‡º\nå¾—åˆ°\nå¾—å¤©ç‹¬åŽš\nå¾—èµ·\nå¿ƒé‡Œ\nå¿…\nå¿…å®š\nå¿…å°†\nå¿…ç„¶\nå¿…è¦\nå¿…é¡»\nå¿«\nå¿«è¦\nå¿½åœ°\nå¿½ç„¶\næ€Ž\næ€Žä¹ˆ\næ€Žä¹ˆåŠž\næ€Žä¹ˆæ ·\næ€Žå¥ˆ\næ€Žæ ·\næ€Žéº½\næ€•\næ€¥åŒ†åŒ†\næ€ª\næ€ªä¸å¾—\næ€»ä¹‹\næ€»æ˜¯\næ€»çš„æ¥çœ‹\næ€»çš„æ¥è¯´\næ€»çš„è¯´æ¥\næ€»ç»“\næ€»è€Œè¨€ä¹‹\næç„¶\nææ€•\næ°ä¼¼\næ°å¥½\næ°å¦‚\næ°å·§\næ°æ°\næ°æ°ç›¸å\næ°é€¢\næ‚¨\næ‚¨ä»¬\næ‚¨æ˜¯\næƒŸå…¶\næƒ¯å¸¸\næ„æ€\næ„¤ç„¶\næ„¿æ„\næ…¢è¯´\næˆä¸º\næˆå¹´\næˆå¹´ç´¯æœˆ\næˆå¿ƒ\næˆ‘\næˆ‘ä»¬\næˆ‘æ˜¯\næˆ‘çš„\næˆ–\næˆ–åˆ™\næˆ–å¤šæˆ–å°‘\næˆ–æ˜¯\næˆ–æ›°\næˆ–è€…\næˆ–è®¸\næˆ˜æ–—\næˆªç„¶\næˆªè‡³\næ‰€\næ‰€ä»¥\næ‰€åœ¨\næ‰€å¹¸\næ‰€æœ‰\næ‰€è°“\næ‰\næ‰èƒ½\næ‰‘é€š\næ‰“\næ‰“ä»Ž\næ‰“å¼€å¤©çª—è¯´äº®è¯\næ‰©å¤§\næŠŠ\næŠ‘æˆ–\næŠ½å†·å­\næ‹¦è…°\næ‹¿\næŒ‰\næŒ‰æ—¶\næŒ‰æœŸ\næŒ‰ç…§\næŒ‰ç†\næŒ‰è¯´\næŒ¨ä¸ª\næŒ¨å®¶æŒ¨æˆ·\næŒ¨æ¬¡\næŒ¨ç€\næŒ¨é—¨æŒ¨æˆ·\næŒ¨é—¨é€æˆ·\næ¢å¥è¯è¯´\næ¢è¨€ä¹‹\næ®\næ®å®ž\næ®æ‚‰\næ®æˆ‘æ‰€çŸ¥\næ®æ­¤\næ®ç§°\næ®è¯´\næŽŒæ¡\næŽ¥ä¸‹æ¥\næŽ¥ç€\næŽ¥è‘—\næŽ¥è¿žä¸æ–­\næ”¾é‡\næ•…\næ•…æ„\næ•…æ­¤\næ•…è€Œ\næ•žå¼€å„¿\næ•¢\næ•¢äºŽ\næ•¢æƒ…\næ•°/\næ•´ä¸ª\næ–­ç„¶\næ–¹\næ–¹ä¾¿\næ–¹æ‰\næ–¹èƒ½\næ–¹é¢\næ—äºº\næ— \næ— å®\næ— æ³•\næ— è®º\næ—¢\næ—¢...åˆ\næ—¢å¾€\næ—¢æ˜¯\næ—¢ç„¶\næ—¥å¤ä¸€æ—¥\næ—¥æ¸\næ—¥ç›Š\næ—¥è‡»\næ—¥è§\næ—¶å€™\næ˜‚ç„¶\næ˜Žæ˜¾\næ˜Žç¡®\næ˜¯\næ˜¯ä¸æ˜¯\næ˜¯ä»¥\næ˜¯å¦\næ˜¯çš„\næ˜¾ç„¶\næ˜¾è‘—\næ™®é€š\næ™®é\næš—ä¸­\næš—åœ°é‡Œ\næš—è‡ª\næ›´\næ›´ä¸º\næ›´åŠ \næ›´è¿›ä¸€æ­¥\næ›¾\næ›¾ç»\næ›¿\næ›¿ä»£\næœ€\næœ€åŽ\næœ€å¤§\næœ€å¥½\næœ€å¾Œ\næœ€è¿‘\næœ€é«˜\næœ‰\næœ‰äº›\næœ‰å…³\næœ‰åˆ©\næœ‰åŠ›\næœ‰åŠ\næœ‰æ‰€\næœ‰æ•ˆ\næœ‰æ—¶\næœ‰ç‚¹\næœ‰çš„\næœ‰çš„æ˜¯\næœ‰ç€\næœ‰è‘—\næœ›\næœ\næœç€\næœ«##æœ«\næœ¬\næœ¬äºº\næœ¬åœ°\næœ¬ç€\næœ¬èº«\næƒæ—¶\næ¥\næ¥ä¸åŠ\næ¥å¾—åŠ\næ¥çœ‹\næ¥ç€\næ¥è‡ª\næ¥è®²\næ¥è¯´\næž\næžä¸º\næžäº†\næžå…¶\næžåŠ›\næžå¤§\næžåº¦\næžç«¯\næž„æˆ\næžœç„¶\næžœçœŸ\næŸ\næŸä¸ª\næŸäº›\næŸæŸ\næ ¹æ®\næ ¹æœ¬\næ ¼å¤–\næ¢†\næ¦‚\næ¬¡ç¬¬\næ¬¢è¿Ž\næ¬¤\næ­£å€¼\næ­£åœ¨\næ­£å¦‚\næ­£å·§\næ­£å¸¸\næ­£æ˜¯\næ­¤\næ­¤ä¸­\næ­¤åŽ\næ­¤åœ°\næ­¤å¤„\næ­¤å¤–\næ­¤æ—¶\næ­¤æ¬¡\næ­¤é—´\næ®†\næ¯‹å®\næ¯\næ¯ä¸ª\næ¯å¤©\næ¯å¹´\næ¯å½“\næ¯æ—¶æ¯åˆ»\næ¯æ¯\næ¯é€¢\næ¯”\næ¯”åŠ\næ¯”å¦‚\næ¯”å¦‚è¯´\næ¯”æ–¹\næ¯”ç…§\næ¯”èµ·\næ¯”è¾ƒ\næ¯•ç«Ÿ\næ¯«ä¸\næ¯«æ— \næ¯«æ— ä¾‹å¤–\næ¯«æ— ä¿ç•™åœ°\næ±\næ²™æ²™\næ²¡\næ²¡å¥ˆä½•\næ²¡æœ‰\næ²¿\næ²¿ç€\næ³¨æ„\næ´»\næ·±å…¥\næ¸…æ¥š\næ»¡\næ»¡è¶³\næ¼«è¯´\nç„‰\nç„¶\nç„¶åˆ™\nç„¶åŽ\nç„¶å¾Œ\nç„¶è€Œ\nç…§\nç…§ç€\nç‰¢ç‰¢\nç‰¹åˆ«æ˜¯\nç‰¹æ®Š\nç‰¹ç‚¹\nçŠ¹ä¸”\nçŠ¹è‡ª\nç‹¬\nç‹¬è‡ª\nçŒ›ç„¶\nçŒ›ç„¶é—´\nçŽ‡å°”\nçŽ‡ç„¶\nçŽ°ä»£\nçŽ°åœ¨\nç†åº”\nç†å½“\nç†è¯¥\nç‘Ÿç‘Ÿ\nç”šä¸”\nç”šä¹ˆ\nç”šæˆ–\nç”šè€Œ\nç”šè‡³\nç”šè‡³äºŽ\nç”¨\nç”¨æ¥\nç”«\nç”­\nç”±\nç”±äºŽ\nç”±æ˜¯\nç”±æ­¤\nç”±æ­¤å¯è§\nç•¥\nç•¥ä¸º\nç•¥åŠ \nç•¥å¾®\nç™½\nç™½ç™½\nçš„\nçš„ç¡®\nçš„è¯\nçš†å¯\nç›®å‰\nç›´åˆ°\nç›´æŽ¥\nç›¸ä¼¼\nç›¸ä¿¡\nç›¸å\nç›¸åŒ\nç›¸å¯¹\nç›¸å¯¹è€Œè¨€\nç›¸åº”\nç›¸å½“\nç›¸ç­‰\nçœå¾—\nçœ‹\nçœ‹ä¸ŠåŽ»\nçœ‹å‡º\nçœ‹åˆ°\nçœ‹æ¥\nçœ‹æ ·å­\nçœ‹çœ‹\nçœ‹è§\nçœ‹èµ·æ¥\nçœŸæ˜¯\nçœŸæ­£\nçœ¨çœ¼\nç€\nç€å‘¢\nçŸ£\nçŸ£ä¹Ž\nçŸ£å“‰\nçŸ¥é“\nç °\nç¡®å®š\nç¢°å·§\nç¤¾ä¼šä¸»ä¹‰\nç¦»\nç§\nç§¯æž\nç§»åŠ¨\nç©¶ç«Ÿ\nç©·å¹´ç´¯æœˆ\nçªå‡º\nçªç„¶\nçªƒ\nç«‹\nç«‹åˆ»\nç«‹å³\nç«‹åœ°\nç«‹æ—¶\nç«‹é©¬\nç«Ÿ\nç«Ÿç„¶\nç«Ÿè€Œ\nç¬¬\nç¬¬äºŒ\nç­‰\nç­‰åˆ°\nç­‰ç­‰\nç­–ç•¥åœ°\nç®€ç›´\nç®€è€Œè¨€ä¹‹\nç®€è¨€ä¹‹\nç®¡\nç±»å¦‚\nç²—\nç²¾å…‰\nç´§æŽ¥ç€\nç´¯å¹´\nç´¯æ¬¡\nçº¯\nçº¯ç²¹\nçºµ\nçºµä»¤\nçºµä½¿\nçºµç„¶\nç»ƒä¹ \nç»„æˆ\nç»\nç»å¸¸\nç»è¿‡\nç»“åˆ\nç»“æžœ\nç»™\nç»\nç»ä¸\nç»å¯¹\nç»éž\nç»é¡¶\nç»§ä¹‹\nç»§åŽ\nç»§ç»­\nç»§è€Œ\nç»´æŒ\nç»¼ä¸Šæ‰€è¿°\nç¼•ç¼•\nç½¢äº†\nè€\nè€å¤§\nè€æ˜¯\nè€è€å®žå®ž\nè€ƒè™‘\nè€…\nè€Œ\nè€Œä¸”\nè€Œå†µ\nè€Œåˆ\nè€ŒåŽ\nè€Œå¤–\nè€Œå·²\nè€Œæ˜¯\nè€Œè¨€\nè€Œè®º\nè”ç³»\nè”è¢‚\nèƒŒåœ°é‡Œ\nèƒŒé èƒŒ\nèƒ½\nèƒ½å¦\nèƒ½å¤Ÿ\nè…¾\nè‡ª\nè‡ªä¸ªå„¿\nè‡ªä»Ž\nè‡ªå„å„¿\nè‡ªåŽ\nè‡ªå®¶\nè‡ªå·±\nè‡ªæ‰“\nè‡ªèº«\nè‡­\nè‡³\nè‡³äºŽ\nè‡³ä»Š\nè‡³è‹¥\nè‡´\nèˆ¬çš„\nè‰¯å¥½\nè‹¥\nè‹¥å¤«\nè‹¥æ˜¯\nè‹¥æžœ\nè‹¥éž\nèŒƒå›´\nèŽ«\nèŽ«ä¸\nèŽ«ä¸ç„¶\nèŽ«å¦‚\nèŽ«è‹¥\nèŽ«éž\nèŽ·å¾—\nè—‰ä»¥\nè™½\nè™½åˆ™\nè™½ç„¶\nè™½è¯´\nè›®\nè¡Œä¸º\nè¡ŒåŠ¨\nè¡¨æ˜Ž\nè¡¨ç¤º\nè¢«\nè¦\nè¦ä¸\nè¦ä¸æ˜¯\nè¦ä¸ç„¶\nè¦ä¹ˆ\nè¦æ˜¯\nè¦æ±‚\nè§\nè§„å®š\nè§‰å¾—\nè­¬å–»\nè­¬å¦‚\nè®¤ä¸º\nè®¤çœŸ\nè®¤è¯†\nè®©\nè®¸å¤š\nè®º\nè®ºè¯´\nè®¾ä½¿\nè®¾æˆ–\nè®¾è‹¥\nè¯šå¦‚\nè¯šç„¶\nè¯è¯´\nè¯¥\nè¯¥å½“\nè¯´æ˜Ž\nè¯´æ¥\nè¯´è¯´\nè¯·å‹¿\nè¯¸\nè¯¸ä½\nè¯¸å¦‚\nè°\nè°äºº\nè°æ–™\nè°çŸ¥\nè°¨\nè±ç„¶\nè´¼æ­»\nèµ–ä»¥\nèµ¶\nèµ¶å¿«\nèµ¶æ—©ä¸èµ¶æ™š\nèµ·\nèµ·å…ˆ\nèµ·åˆ\nèµ·å¤´\nèµ·æ¥\nèµ·è§\nèµ·é¦–\nè¶\nè¶ä¾¿\nè¶åŠ¿\nè¶æ—©\nè¶æœº\nè¶çƒ­\nè¶ç€\nè¶Šæ˜¯\nè·\nè·Ÿ\nè·¯ç»\nè½¬åŠ¨\nè½¬å˜\nè½¬è´´\nè½°ç„¶\nè¾ƒ\nè¾ƒä¸º\nè¾ƒä¹‹\nè¾ƒæ¯”\nè¾¹\nè¾¾åˆ°\nè¾¾æ—¦\nè¿„\nè¿…é€Ÿ\nè¿‡\nè¿‡äºŽ\nè¿‡åŽ»\nè¿‡æ¥\nè¿ç”¨\nè¿‘\nè¿‘å‡ å¹´æ¥\nè¿‘å¹´æ¥\nè¿‘æ¥\nè¿˜\nè¿˜æ˜¯\nè¿˜æœ‰\nè¿˜è¦\nè¿™\nè¿™ä¸€æ¥\nè¿™ä¸ª\nè¿™ä¹ˆ\nè¿™ä¹ˆäº›\nè¿™ä¹ˆæ ·\nè¿™ä¹ˆç‚¹å„¿\nè¿™äº›\nè¿™ä¼šå„¿\nè¿™å„¿\nè¿™å°±æ˜¯è¯´\nè¿™æ—¶\nè¿™æ ·\nè¿™æ¬¡\nè¿™ç‚¹\nè¿™ç§\nè¿™èˆ¬\nè¿™è¾¹\nè¿™é‡Œ\nè¿™éº½\nè¿›å…¥\nè¿›åŽ»\nè¿›æ¥\nè¿›æ­¥\nè¿›è€Œ\nè¿›è¡Œ\nè¿ž\nè¿žåŒ\nè¿žå£°\nè¿žæ—¥\nè¿žæ—¥æ¥\nè¿žè¢‚\nè¿žè¿ž\nè¿Ÿæ—©\nè¿«äºŽ\né€‚åº”\né€‚å½“\né€‚ç”¨\né€æ­¥\né€æ¸\né€šå¸¸\né€šè¿‡\né€ æˆ\né€¢\né‡åˆ°\né­åˆ°\néµå¾ª\néµç…§\né¿å…\né‚£\né‚£ä¸ª\né‚£ä¹ˆ\né‚£ä¹ˆäº›\né‚£ä¹ˆæ ·\né‚£äº›\né‚£ä¼šå„¿\né‚£å„¿\né‚£æ—¶\né‚£æœ«\né‚£æ ·\né‚£èˆ¬\né‚£è¾¹\né‚£é‡Œ\né‚£éº½\néƒ¨åˆ†\néƒ½\né„™äºº\né‡‡å–\né‡Œé¢\né‡å¤§\né‡æ–°\né‡è¦\né‰´äºŽ\né’ˆå¯¹\né•¿æœŸä»¥æ¥\né•¿æ­¤ä¸‹åŽ»\né•¿çº¿\né•¿è¯çŸ­è¯´\né—®é¢˜\né—´æˆ–\né˜²æ­¢\né˜¿\né™„è¿‘\né™ˆå¹´\né™åˆ¶\né™¡ç„¶\né™¤\né™¤äº†\né™¤å´\né™¤åŽ»\né™¤å¤–\né™¤å¼€\né™¤æ­¤\né™¤æ­¤ä¹‹å¤–\né™¤æ­¤ä»¥å¤–\né™¤æ­¤è€Œå¤–\né™¤éž\néš\néšåŽ\néšæ—¶\néšç€\néšè‘—\néš”å¤œ\néš”æ—¥\néš¾å¾—\néš¾æ€ª\néš¾è¯´\néš¾é“\néš¾é“è¯´\né›†ä¸­\né›¶\néœ€è¦\néžä½†\néžå¸¸\néžå¾’\néžå¾—\néžç‰¹\néžç‹¬\né \né¡¶å¤š\né¡·\né¡·åˆ»\né¡·åˆ»ä¹‹é—´\né¡·åˆ»é—´\né¡º\né¡ºç€\né¡¿æ—¶\né¢‡\né£Žé›¨æ— é˜»\né¥±\né¦–å…ˆ\né©¬ä¸Š\né«˜ä½Ž\né«˜å…´\né»˜ç„¶\né»˜é»˜åœ°\né½\nï¸¿\nï¼\nï¼ƒ\nï¼„\nï¼…\nï¼†\nï¼‡\nï¼ˆ\nï¼‰\nï¼‰Ã·ï¼ˆï¼‘ï¼\nï¼‰ã€\nï¼Š\nï¼‹\nï¼‹Î¾\nï¼‹ï¼‹\nï¼Œ\nï¼Œä¹Ÿ\nï¼\nï¼Î²\nï¼ï¼\nï¼ï¼»ï¼Šï¼½ï¼\nï¼Ž\nï¼\nï¼\nï¼ï¼šï¼’\nï¼‘\nï¼‘ï¼Ž\nï¼‘ï¼’ï¼…\nï¼’\nï¼’ï¼Žï¼“ï¼…\nï¼“\nï¼”\nï¼•\nï¼•ï¼šï¼\nï¼–\nï¼—\nï¼˜\nï¼™\nï¼š\nï¼›\nï¼œ\nï¼œÂ±\nï¼œÎ”\nï¼œÎ»\nï¼œÏ†\nï¼œï¼œ\nï¼\nï¼â€³\nï¼â˜†\nï¼ï¼ˆ\nï¼ï¼\nï¼ï¼»\nï¼ï½›\nï¼ž\nï¼žÎ»\nï¼Ÿ\nï¼ \nï¼¡\nï¼¬ï¼©\nï¼²ï¼Žï¼¬ï¼Ž\nï¼ºï¼¸ï¼¦ï¼©ï¼´ï¼¬\nï¼»\nï¼»â‘ â‘ ï¼½\nï¼»â‘ â‘¡ï¼½\nï¼»â‘ â‘¢ï¼½\nï¼»â‘ â‘£ï¼½\nï¼»â‘ â‘¤ï¼½\nï¼»â‘ â‘¥ï¼½\nï¼»â‘ â‘¦ï¼½\nï¼»â‘ â‘§ï¼½\nï¼»â‘ â‘¨ï¼½\nï¼»â‘ ï¼¡ï¼½\nï¼»â‘ ï¼¢ï¼½\nï¼»â‘ ï¼£ï¼½\nï¼»â‘ ï¼¤ï¼½\nï¼»â‘ ï¼¥ï¼½\nï¼»â‘ ï¼½\nï¼»â‘ ï½ï¼½\nï¼»â‘ ï½ƒï¼½\nï¼»â‘ ï½„ï¼½\nï¼»â‘ ï½…ï¼½\nï¼»â‘ ï½†ï¼½\nï¼»â‘ ï½‡ï¼½\nï¼»â‘ ï½ˆï¼½\nï¼»â‘ ï½‰ï¼½\nï¼»â‘ ï½ï¼½\nï¼»â‘¡\nï¼»â‘¡â‘ ï¼½\nï¼»â‘¡â‘¡ï¼½\nï¼»â‘¡â‘¢ï¼½\nï¼»â‘¡â‘£\nï¼»â‘¡â‘¤ï¼½\nï¼»â‘¡â‘¥ï¼½\nï¼»â‘¡â‘¦ï¼½\nï¼»â‘¡â‘§ï¼½\nï¼»â‘¡â‘©ï¼½\nï¼»â‘¡ï¼¢ï¼½\nï¼»â‘¡ï¼§ï¼½\nï¼»â‘¡ï¼½\nï¼»â‘¡ï½ï¼½\nï¼»â‘¡ï½‚ï¼½\nï¼»â‘¡ï½ƒï¼½\nï¼»â‘¡ï½„ï¼½\nï¼»â‘¡ï½…ï¼½\nï¼»â‘¡ï½†ï¼½\nï¼»â‘¡ï½‡ï¼½\nï¼»â‘¡ï½ˆï¼½\nï¼»â‘¡ï½‰ï¼½\nï¼»â‘¡ï½Šï¼½\nï¼»â‘¢â‘ ï¼½\nï¼»â‘¢â‘©ï¼½\nï¼»â‘¢ï¼¦ï¼½\nï¼»â‘¢ï¼½\nï¼»â‘¢ï½ï¼½\nï¼»â‘¢ï½‚ï¼½\nï¼»â‘¢ï½ƒï¼½\nï¼»â‘¢ï½„ï¼½\nï¼»â‘¢ï½…ï¼½\nï¼»â‘¢ï½‡ï¼½\nï¼»â‘¢ï½ˆï¼½\nï¼»â‘£ï¼½\nï¼»â‘£ï½ï¼½\nï¼»â‘£ï½‚ï¼½\nï¼»â‘£ï½ƒï¼½\nï¼»â‘£ï½„ï¼½\nï¼»â‘£ï½…ï¼½\nï¼»â‘¤ï¼½\nï¼»â‘¤ï¼½ï¼½\nï¼»â‘¤ï½ï¼½\nï¼»â‘¤ï½‚ï¼½\nï¼»â‘¤ï½„ï¼½\nï¼»â‘¤ï½…ï¼½\nï¼»â‘¤ï½†ï¼½\nï¼»â‘¥ï¼½\nï¼»â‘¦ï¼½\nï¼»â‘§ï¼½\nï¼»â‘¨ï¼½\nï¼»â‘©ï¼½\nï¼»ï¼Šï¼½\nï¼»ï¼\nï¼»ï¼½\nï¼½\nï¼½âˆ§â€²ï¼ï¼»\nï¼½ï¼»\nï¼¿\nï½ï¼½\nï½‚ï¼½\nï½ƒï¼½\nï½…ï¼½\nï½†ï¼½\nï½Žï½‡æ˜‰\nï½›\nï½›ï¼\nï½œ\nï½\nï½ï¼ž\nï½ž\nï½žÂ±\nï½žï¼‹\nï¿¥\n'.split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/zh/__init__.py----------------------------------------
A:spacy.lang.zh.__init__.warn_msg->errors.Warnings.W104.format(target='pkuseg', current=self.segmenter)
A:spacy.lang.zh.__init__.self.jieba_seg->try_jieba_import()
A:spacy.lang.zh.__init__.self.pkuseg_seg->spacy_pkuseg.pkuseg(path)
A:spacy.lang.zh.__init__.words->list(text)
A:spacy.lang.zh.__init__.(words, spaces)->util.get_words_and_spaces(words, text)
A:spacy.lang.zh.__init__.self.pkuseg_seg.preprocesser->spacy_pkuseg.Preprocesser(user_dict)
A:spacy.lang.zh.__init__.self.segmenter->load_config_from_str(DEFAULT_CONFIG).get('segmenter', Segmenter.char)
A:spacy.lang.zh.__init__.tempdir->Path(tempdir)
A:spacy.lang.zh.__init__.pkuseg_features_b->fileh.read()
A:spacy.lang.zh.__init__.pkuseg_weights_b->fileh.read()
A:spacy.lang.zh.__init__.pkuseg_data['processors_data']->srsly.msgpack_loads(b)
A:spacy.lang.zh.__init__.self.pkuseg_seg.postprocesser.common_words->set(common_words)
A:spacy.lang.zh.__init__.self.pkuseg_seg.postprocesser.other_words->set(other_words)
A:spacy.lang.zh.__init__.path->util.ensure_path(path)
A:spacy.lang.zh.__init__.data->srsly.read_msgpack(path)
A:spacy.lang.zh.__init__.config->load_config_from_str(DEFAULT_CONFIG)
spacy.lang.zh.__init__.Chinese(Language)
spacy.lang.zh.__init__.ChineseDefaults(BaseDefaults)
spacy.lang.zh.__init__.ChineseTokenizer(self,vocab:Vocab,segmenter:Segmenter=Segmenter.char)
spacy.lang.zh.__init__.ChineseTokenizer._get_config(self)->Dict[str, Any]
spacy.lang.zh.__init__.ChineseTokenizer._set_config(self,config:Dict[str,Any]={})->None
spacy.lang.zh.__init__.ChineseTokenizer.from_bytes(self,data,**kwargs)
spacy.lang.zh.__init__.ChineseTokenizer.from_disk(self,path,**kwargs)
spacy.lang.zh.__init__.ChineseTokenizer.initialize(self,get_examples:Optional[Callable[[],Iterable[Example]]]=None,*,nlp:Optional[Language]=None,pkuseg_model:Optional[str]=None,pkuseg_user_dict:Optional[str]='default')
spacy.lang.zh.__init__.ChineseTokenizer.pkuseg_update_user_dict(self,words:List[str],reset:bool=False)
spacy.lang.zh.__init__.ChineseTokenizer.score(self,examples)
spacy.lang.zh.__init__.ChineseTokenizer.to_bytes(self,**kwargs)
spacy.lang.zh.__init__.ChineseTokenizer.to_disk(self,path,**kwargs)
spacy.lang.zh.__init__.Segmenter(str,Enum)
spacy.lang.zh.__init__.Segmenter.values(cls)
spacy.lang.zh.__init__._get_pkuseg_trie_data(node,path='')
spacy.lang.zh.__init__.create_chinese_tokenizer(segmenter:Segmenter=Segmenter.char)
spacy.lang.zh.__init__.try_jieba_import()
spacy.lang.zh.__init__.try_pkuseg_import(pkuseg_model:Optional[str],pkuseg_user_dict:Optional[str])


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/zh/lex_attrs.py----------------------------------------
A:spacy.lang.zh.lex_attrs.text->text.replace(',', '').replace('.', '').replace('ï¼Œ', '').replace('ã€‚', '').replace(',', '').replace('.', '').replace('ï¼Œ', '').replace('ã€‚', '')
A:spacy.lang.zh.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace('ï¼Œ', '').replace('ã€‚', '').replace(',', '').replace('.', '').replace('ï¼Œ', '').replace('ã€‚', '').split('/')
spacy.lang.zh.lex_attrs.like_num(text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/nl/examples.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/nl/lemmatizer.py----------------------------------------
A:spacy.lang.nl.lemmatizer.lookup_table->self.lookups.get_table('lemma_lookup', {})
A:spacy.lang.nl.lemmatizer.string->string.lower().lower()
A:spacy.lang.nl.lemmatizer.univ_pos->token.pos_.lower()
A:spacy.lang.nl.lemmatizer.index_table->self.lookups.get_table('lemma_index', {})
A:spacy.lang.nl.lemmatizer.exc_table->self.lookups.get_table('lemma_exc', {})
A:spacy.lang.nl.lemmatizer.rules_table->self.lookups.get_table('lemma_rules', {})
A:spacy.lang.nl.lemmatizer.index->self.lookups.get_table('lemma_index', {}).get(univ_pos, {})
A:spacy.lang.nl.lemmatizer.exceptions->self.lookups.get_table('lemma_exc', {}).get(univ_pos, {})
A:spacy.lang.nl.lemmatizer.rules->self.lookups.get_table('lemma_rules', {}).get(univ_pos, {})
A:spacy.lang.nl.lemmatizer.lemma_index->self.lookups.get_table('lemma_index', {}).get(univ_pos, {})
A:spacy.lang.nl.lemmatizer.looked_up_lemma->self.lookups.get_table('lemma_lookup', {}).get(string)
A:spacy.lang.nl.lemmatizer.forms->list(dict.fromkeys(oov_forms))
spacy.lang.nl.DutchLemmatizer(Lemmatizer)
spacy.lang.nl.lemmatizer.DutchLemmatizer(Lemmatizer)
spacy.lang.nl.lemmatizer.DutchLemmatizer.get_lookups_config(cls,mode:str)->Tuple[List[str], List[str]]
spacy.lang.nl.lemmatizer.DutchLemmatizer.lookup_lemmatize(self,token:Token)->List[str]
spacy.lang.nl.lemmatizer.DutchLemmatizer.rule_lemmatize(self,token:Token)->List[str]


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/nl/stop_words.py----------------------------------------
A:spacy.lang.nl.stop_words.STOP_WORDS->set("\naan af al alle alles allebei alleen allen als altijd ander anders andere anderen aangaande aangezien achter achterna\nafgelopen aldus alhoewel anderzijds\n\nben bij bijna bijvoorbeeld behalve beide beiden beneden bent bepaald beter betere betreffende binnen binnenin boven\nbovenal bovendien bovenstaand buiten\n\ndaar dan dat de der den deze die dit doch doen door dus daarheen daarin daarna daarnet daarom daarop des dezelfde dezen\ndien dikwijls doet doorgaand doorgaans\n\neen eens en er echter enige eerder eerst eerste eersten effe eigen elk elke enkel enkele enz erdoor etc even eveneens\nevenwel\n\nff\n\nge geen geweest gauw gedurende gegeven gehad geheel gekund geleden gelijk gemogen geven geweest gewoon gewoonweg\ngeworden gij\n\nhaar had heb hebben heeft hem het hier hij hoe hun hadden hare hebt hele hen hierbeneden hierboven hierin hoewel hun\n\niemand iets ik in is idd ieder ikke ikzelf indien inmiddels inz inzake\n\nja je jou jouw jullie jezelf jij jijzelf jouwe juist\n\nkan kon kunnen klaar konden krachtens kunnen kunt\n\nlang later liet liever\n\nmaar me meer men met mij mijn moet mag mede meer meesten mezelf mijzelf min minder misschien mocht mochten moest moesten\nmoet moeten mogelijk mogen\n\nna naar niet niets nog nu nabij nadat net nogal nooit nr nu\n\nof om omdat ons ook op over omhoog omlaag omstreeks omtrent omver onder ondertussen ongeveer onszelf onze ooit opdat\nopnieuw opzij over overigens\n\npas pp precies prof publ\n\nreeds rond rondom\n\nsedert sinds sindsdien slechts sommige spoedig steeds\n\nâ€˜t 't te tegen toch toen tot tamelijk ten tenzij ter terwijl thans tijdens toe totdat tussen\n\nu uit uw uitgezonderd uwe uwen\n\nvan veel voor vaak vanaf vandaan vanuit vanwege veeleer verder verre vervolgens vgl volgens vooraf vooral vooralsnog\nvoorbij voordat voordien voorheen voorop voort voorts vooruit vrij vroeg\n\nwant waren was wat we wel werd wezen wie wij wil worden waar waarom wanneer want weer weg wegens weinig weinige weldra\nwelk welke welken werd werden wiens wier wilde wordt\n\nzal ze zei zelf zich zij zijn zo zonder zou zeer zeker zekere zelfde zelfs zichzelf zijnde zijne zoâ€™n zoals zodra zouden\n zoveel zowat zulk zulke zulks zullen zult\n".split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/nl/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.nl.tokenizer_exceptions.uppered->orth.upper()
A:spacy.lang.nl.tokenizer_exceptions.capsed->orth.capitalize()
A:spacy.lang.nl.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/nl/__init__.py----------------------------------------
spacy.lang.nl.__init__.Dutch(Language)
spacy.lang.nl.__init__.DutchDefaults(BaseDefaults)
spacy.lang.nl.__init__.make_lemmatizer(nlp:Language,model:Optional[Model],name:str,mode:str,overwrite:bool,scorer:Optional[Callable])


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/nl/lex_attrs.py----------------------------------------
A:spacy.lang.nl.lex_attrs._num_words->set('\nnul een Ã©Ã©n twee drie vier vijf zes zeven acht negen tien elf twaalf dertien\nveertien twintig dertig veertig vijftig zestig zeventig tachtig negentig honderd\nduizend miljoen miljard biljoen biljard triljoen triljard\n'.split())
A:spacy.lang.nl.lex_attrs._ordinal_words->set('\neerste tweede derde vierde vijfde zesde zevende achtste negende tiende elfde\ntwaalfde dertiende veertiende twintigste dertigste veertigste vijftigste\nzestigste zeventigste tachtigste negentigste honderdste duizendste miljoenste\nmiljardste biljoenste biljardste triljoenste triljardste\n'.split())
A:spacy.lang.nl.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.nl.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.nl.lex_attrs.like_num(text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/nl/syntax_iterators.py----------------------------------------
A:spacy.lang.nl.syntax_iterators.span_label->doc.vocab.strings.add('NP')
A:spacy.lang.nl.syntax_iterators.nsubjs->filter(lambda x: x.dep == doc.vocab.strings['nsubj'], word.children)
A:spacy.lang.nl.syntax_iterators.next_word->next(nsubjs, None)
A:spacy.lang.nl.syntax_iterators.children->filter(lambda x: x.dep in noun_deps, word.children)
A:spacy.lang.nl.syntax_iterators.start_span->min(children_i)
spacy.lang.nl.syntax_iterators.noun_chunks(doclike:Union[Doc,Span])->Iterator[Tuple[int, int, int]]


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/nl/punctuation.py----------------------------------------
A:spacy.lang.nl.punctuation._quotes->char_classes.CONCAT_QUOTES.replace("'", '')
A:spacy.lang.nl.punctuation._units->merge_chars(' '.join(_list_units))


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/si/examples.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/si/stop_words.py----------------------------------------
A:spacy.lang.si.stop_words.STOP_WORDS->set('\nà·ƒà·„\nà·ƒà¶¸à¶œ\nà·ƒà¶¸à¶Ÿ\nà¶…à·„à·\nà¶†à·„à·Š\nà¶†\nà¶•à·„à·\nà¶…à¶±à·š\nà¶…à¶³à·\nà¶…à¶´à·œà¶ºà·’\nà¶…à¶´à·\nà¶…à¶ºà·’à¶ºà·\nà¶†à¶ºà·’\nà¶Œà¶ºà·’\nà¶ à·“\nà¶ à·’à·„à·Š\nà¶ à·’à¶šà·Š\nà·„à·\u200d\nà¶¯à·\nà¶¯à·à·„à·\nà¶¸à·™à¶±à·Š\nà·ƒà·š\nà·€à·à¶±à·’\nà¶¶à¶³à·”\nà·€à¶±à·Š\nà¶…à¶ºà·”à¶»à·”\nà¶…à¶ºà·”à¶»à·’à¶±à·Š\nà¶½à·™à·ƒ\nà·€à·à¶©à·’\nà·à·Š\u200dà¶»à·“\nà·„à·\nà¶º\nà¶±à·’à·ƒà·\nà¶±à·’à·ƒà·à·€à·™à¶±à·Š\nà¶¶à·€à¶§\nà¶¶à·€\nà¶¶à·€à·™à¶±à·Š\nà¶±à¶¸à·Š\nà·€à·à¶©à·’\nà·ƒà·’à¶§\nà¶¯à·“\nà¶¸à·„à·\nà¶¸à·„\nà¶´à¶¸à¶«\nà¶´à¶¸à¶«à·’à¶±à·Š\nà¶´à¶¸à¶±\nà·€à¶±\nà·€à·’à¶§\nà·€à·’à¶§à·’à¶±à·Š\nà¶¸à·š\nà¶¸à·™à¶½à·™à·ƒ\nà¶¸à·™à¶ºà·’à¶±à·Š\nà¶‡à¶­à·’\nà¶½à·™à·ƒ\nà·ƒà·’à¶¯à·”\nà·€à·à¶ºà·™à¶±à·Š\nà¶ºà¶±\nà·ƒà¶³à·„à·\nà¶¸à¶œà·’à¶±à·Š\nà·„à·\u200d\nà¶‰à¶­à·\nà¶’\nà¶‘à¶¸\nà¶¯\nà¶…à¶­à¶»\nà·€à·’à·ƒà·’à¶±à·Š\nà·ƒà¶¸à¶œ\nà¶´à·’à·…à·’à¶¶à¶³à·€\nà¶´à·’à·…à·’à¶¶à¶³\nà¶­à·”à·…\nà¶¶à·€\nà·€à·à¶±à·’\nà¶¸à·„\nà¶¸à·™à¶¸\nà¶¸à·™à·„à·’\nà¶¸à·š\nà·€à·™à¶­\nà·€à·™à¶­à·’à¶±à·Š\nà·€à·™à¶­à¶§\nà·€à·™à¶±à·”à·€à·™à¶±à·Š\nà·€à·™à¶±à·”à·€à¶§\nà·€à·™à¶±\nà¶œà·à¶±\nà¶±à·‘\nà¶…à¶±à·”à·€\nà¶±à·€\nà¶´à·’à·…à·’à¶¶à¶³\nà·€à·’à·à·šà·‚\nà¶¯à·à¶±à¶§\nà¶‘à·„à·™à¶±à·Š\nà¶¸à·™à·„à·™à¶±à·Š\nà¶‘à·„à·š\nà¶¸à·™à·„à·š\nà¶¸\nà¶­à·€à¶­à·Š\nà¶­à·€ \nà·ƒà·„\nà¶¯à¶šà·Šà·€à·\nà¶§\nà¶œà·š\nà¶‘\nà¶š\nà¶šà·Š\nà¶¶à·€à¶­à·Š\nà¶¶à·€à¶¯\nà¶¸à¶­\nà¶‡à¶­à·”à¶½à·”\nà¶‡à¶­à·”à·…à·”\nà¶¸à·™à·ƒà·š\nà·€à¶©à·\nà·€à¶©à·à¶­à·Šà¶¸\nà¶±à·’à¶­à·’\nà¶±à·’à¶­à·’à¶­à·Š\nà¶±à·’à¶­à·œà¶»\nà¶±à·’à¶­à¶»\nà¶‰à¶šà·Šà¶¶à·’à¶­à·’\nà¶¯à·à¶±à·Š\nà¶ºà¶½à·’\nà¶´à·”à¶±\nà¶‰à¶­à·’à¶±à·Š\nà·ƒà·’à¶§\nà·ƒà·’à¶§à¶±à·Š\nà¶´à¶§à¶±à·Š\nà¶­à·™à¶šà·Š\nà¶¯à¶šà·Šà·€à·\nà·ƒà·\nà¶­à·à¶šà·Š\nà¶­à·”à·€à¶šà·Š\nà¶´à·€à·\nà¶¯\nà·„à·\u200d\nà·€à¶­à·Š\nà·€à·’à¶±à·\nà·„à·à¶»\nà¶¸à·’à·ƒ\nà¶¸à·”à¶­à·Š\nà¶šà·’à¶¸\nà¶šà·’à¶¸à·Š\nà¶‡à¶ºà·’\nà¶¸à¶±à·Šà¶¯\nà·„à·™à·€à¶­à·Š\nà¶±à·œà·„à·œà¶­à·Š\nà¶´à¶­à·\nà¶´à·à·ƒà·\nà¶œà·à¶±à·™\nà¶­à·€\nà¶‰à¶­à·\nà¶¶à·œà·„à·\nà·€à·„à·\nà·ƒà·™à¶¯\nà·ƒà·à¶±à·’à¶±à·Š\nà·„à¶±à·’à¶š\nà¶‘à¶¸à·Šà¶¶à·\nà¶‘à¶¸à·Šà¶¶à¶½\nà¶¶à·œà¶½\nà¶±à¶¸à·Š\nà·€à¶±à·à·„à·’\nà¶šà¶½à·“\nà¶‰à¶³à·”à¶»à·\nà¶…à¶±à·Šà¶±\nà¶”à¶±à·Šà¶±\nà¶¸à·™à¶±à·Šà¶±\nà¶‹à¶¯à·™à·ƒà·\nà¶´à·’à¶«à·’à·ƒ\nà·ƒà¶³à·„à·\nà¶…à¶»à¶¶à¶ºà·\nà¶±à·’à·ƒà·\nà¶‘à¶±à·’à·ƒà·\nà¶‘à¶¶à·à·€à·’à¶±à·Š\nà¶¶à·à·€à·’à¶±à·Š\nà·„à·™à¶ºà·’à¶±à·Š\nà·ƒà·šà¶šà·Š\nà·ƒà·šà¶š\nà¶œà·à¶±\nà¶…à¶±à·”à·€\nà¶´à¶»à·’à¶¯à·’\nà·€à·’à¶§\nà¶­à·™à¶šà·Š\nà¶¸à·™à¶­à·™à¶šà·Š\nà¶¸à·šà¶­à·à¶šà·Š\nà¶­à·”à¶»à·”\nà¶­à·”à¶»à·\nà¶­à·”à¶»à·à·€à¶§\nà¶­à·”à¶½à·’à¶±à·Š\nà¶±à¶¸à·”à¶­à·Š\nà¶‘à¶±à¶¸à·”à¶­à·Š\nà·€à·ƒà·Š\nà¶¸à·™à¶±à·Š\nà¶½à·™à·ƒ\nà¶´à¶»à·’à¶¯à·’\nà¶‘à·„à·™à¶­à·Š\n'.split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/si/__init__.py----------------------------------------
spacy.lang.si.__init__.Sinhala(Language)
spacy.lang.si.__init__.SinhalaDefaults(BaseDefaults)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/si/lex_attrs.py----------------------------------------
A:spacy.lang.si.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.si.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.si.lex_attrs.like_num(text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/bg/examples.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/bg/stop_words.py----------------------------------------
A:spacy.lang.bg.stop_words.STOP_WORDS->set('\nÐ° Ð°Ð²Ñ‚ÐµÐ½Ñ‚Ð¸Ñ‡ÐµÐ½ Ð°Ð· Ð°ÐºÐ¾ Ð°Ð»Ð°\n\nÐ±Ðµ Ð±ÐµÐ· Ð±ÐµÑˆÐµ Ð±Ð¸ Ð±Ð¸Ð²Ñˆ Ð±Ð¸Ð²ÑˆÐ° Ð±Ð¸Ð²ÑˆÐ¾ Ð±Ð¸Ð²ÑˆÐ¸ Ð±Ð¸Ð» Ð±Ð¸Ð»Ð° Ð±Ð¸Ð»Ð¸ Ð±Ð¸Ð»Ð¾ Ð±Ð»Ð°Ð³Ð¾Ð´Ð°Ñ€Ñ Ð±Ð»Ð¸Ð·Ð¾ Ð±ÑŠÐ´Ð°Ñ‚\nÐ±ÑŠÐ´Ðµ Ð±ÑŠÐ´Ð° Ð±ÑÑ…Ð°\n\nÐ² Ð²Ð°Ñ Ð²Ð°Ñˆ Ð²Ð°ÑˆÐ° Ð²Ð°ÑˆÐ°Ñ‚Ð° Ð²Ð°ÑˆÐ¸ÑÑ‚ Ð²ÐµÑ€Ð¾ÑÑ‚Ð½Ð¾ Ð²ÐµÑ‡Ðµ Ð²Ð·ÐµÐ¼Ð° Ð²Ð¸ Ð²Ð¸Ðµ Ð²Ð¸Ð½Ð°Ð³Ð¸ Ð²Ð½Ð¸Ð¼Ð°Ð²Ð° Ð²Ñ€ÐµÐ¼Ðµ Ð²ÑÐµ \nÐ²ÑÐµÐºÐ¸ Ð²ÑÐ¸Ñ‡ÐºÐ¸ Ð²Ð¼ÐµÑÑ‚Ð¾ Ð²ÑÐ¸Ñ‡ÐºÐ¾ Ð²ÑÐ»ÐµÐ´ÑÑ‚Ð²Ð¸Ðµ Ð²ÑÑŠÑ‰Ð½Ð¾ÑÑ‚ Ð²ÑÑÐºÐ° Ð²Ñ‚Ð¾Ñ€Ð¸ Ð²ÑŠÐ² Ð²ÑŠÐ¿Ñ€ÐµÐºÐ¸ Ð²ÑŠÑ€Ñ…Ñƒ\nÐ²ÑŠÑ‚Ñ€Ðµ Ð²ÐµÐ´Ð½ÑŠÐ¶ \n\nÐ³ Ð³Ð¸ Ð³Ð»Ð°Ð²ÐµÐ½ Ð³Ð»Ð°Ð²Ð½Ð° Ð³Ð»Ð°Ð²Ð½Ð¾ Ð³Ð»Ð°Ñ Ð³Ð¾ Ð³Ð¾Ð´Ð½Ð¾ Ð³Ð¾Ð´Ð¸Ð½Ð° Ð³Ð¾Ð´Ð¸Ð½Ð¸ Ð³Ð¾Ð´Ð¸ÑˆÐµÐ½\n\nÐ´ Ð´Ð° Ð´Ð°Ð»Ð¸ Ð´Ð°Ð»ÐµÑ‡ Ð´Ð°Ð»ÐµÑ‡Ðµ Ð´Ð²Ð° Ð´Ð²Ð°Ð¼Ð° Ð´Ð²Ð°Ð¼Ð°Ñ‚Ð° Ð´Ð²Ðµ Ð´Ð²ÐµÑ‚Ðµ Ð´ÐµÐ½ Ð´Ð½ÐµÑ Ð´Ð½Ð¸ Ð´Ð¾ Ð´Ð¾Ð±Ñ€Ð° Ð´Ð¾Ð±Ñ€Ðµ \nÐ´Ð¾Ð±Ñ€Ð¾ Ð´Ð¾Ð±ÑŠÑ€ Ð´Ð¾ÑÑ‚Ð°Ñ‚ÑŠÑ‡Ð½Ð¾ Ð´Ð¾ÐºÐ°Ñ‚Ð¾ Ð´Ð¾ÐºÐ¾Ð³Ð° Ð´Ð¾Ñ€Ð¸ Ð´Ð¾ÑÐµÐ³Ð° Ð´Ð¾ÑÑ‚Ð° Ð´Ñ€ÑƒÐ³ Ð´Ñ€ÑƒÐ³Ð° Ð´Ñ€ÑƒÐ³Ð°Ð´Ðµ Ð´Ñ€ÑƒÐ³Ð¸\n\nÐµ ÐµÐ²Ñ‚Ð¸Ð½ ÐµÐ´Ð²Ð° ÐµÐ´Ð¸Ð½ ÐµÐ´Ð½Ð° ÐµÐ´Ð½Ð°ÐºÐ²Ð° ÐµÐ´Ð½Ð°ÐºÐ²Ð¸ ÐµÐ´Ð½Ð°ÐºÑŠÐ² ÐµÐ´Ð½Ð¾ ÐµÐºÐ¸Ð¿ ÐµÑ‚Ð¾\n\nÐ¶Ð¸Ð²Ð¾Ñ‚ Ð¶Ð¸Ð²\n\nÐ·Ð° Ð·Ð´Ñ€Ð°Ð²ÐµÐ¹ Ð·Ð´Ñ€Ð°ÑÑ‚Ð¸ Ð·Ð½Ð°Ðµ Ð·Ð½Ð°Ñ Ð·Ð°Ð±Ð°Ð²ÑÐ¼ Ð·Ð°Ð´ Ð·Ð°Ð´Ð°Ð´ÐµÐ½Ð¸ Ð·Ð°ÐµÐ´Ð½Ð¾ Ð·Ð°Ñ€Ð°Ð´Ð¸ Ð·Ð°ÑÐµÐ³Ð° Ð·Ð°ÑÐ¿Ð°Ð» \nÐ·Ð°Ñ‚Ð¾Ð²Ð° Ð·Ð°Ð¿Ð°Ð·Ð²Ð° Ð·Ð°Ð¿Ð¾Ñ‡Ð²Ð°Ð¼ Ð·Ð°Ñ‰Ð¾ Ð·Ð°Ñ‰Ð¾Ñ‚Ð¾ Ð·Ð°Ð²Ð¸Ð½Ð°Ð³Ð¸\n\nÐ¸ Ð¸Ð· Ð¸Ð»Ð¸ Ð¸Ð¼ Ð¸Ð¼Ð° Ð¸Ð¼Ð°Ñ‚ Ð¸ÑÐºÐ° Ð¸ÑÐºÐ°Ð¼ Ð¸Ð·Ð¿Ð¾Ð»Ð·Ð²Ð°Ð¹ÐºÐ¸ Ð¸Ð·Ð³Ð»ÐµÐ¶Ð´Ð° Ð¸Ð·Ð³Ð»ÐµÐ¶Ð´Ð°ÑˆÐµ Ð¸Ð·Ð³Ð»ÐµÐ¶Ð´Ð°Ð¹ÐºÐ¸ \nÐ¸Ð·Ð²ÑŠÐ½ Ð¸Ð¼Ð°Ð¹ÐºÐ¸\n\nÐ¹ Ð¹Ð¾ \n\nÐºÐ°Ð·Ð° ÐºÐ°Ð·Ð²Ð° ÐºÐ°Ð·Ð²Ð°Ð¹ÐºÐ¸ ÐºÐ°Ð·Ð²Ð°Ð¼ ÐºÐ°Ðº ÐºÐ°ÐºÐ²Ð° ÐºÐ°ÐºÐ²Ð¾ ÐºÐ°ÐºÑ‚Ð¾ ÐºÐ°ÐºÑŠÐ² ÐºÐ°Ñ‚Ð¾ ÐºÐ¾Ð³Ð° ÐºÐ°ÑƒÐ·Ð° ÐºÐ°ÑƒÐ·Ð¸ \nÐºÐ¾Ð³Ð°Ñ‚Ð¾ ÐºÐ¾Ð³Ð¾Ñ‚Ð¾ ÐºÐ¾ÐµÑ‚Ð¾ ÐºÐ¾Ð¸Ñ‚Ð¾ ÐºÐ¾Ð¹ ÐºÐ¾Ð¹Ñ‚Ð¾ ÐºÐ¾Ð»ÐºÐ¾ ÐºÐ¾ÑÑ‚Ð¾ ÐºÑŠÐ´Ðµ ÐºÑŠÐ´ÐµÑ‚Ð¾ ÐºÑŠÐ¼ ÐºÑ€Ð°Ð¹ ÐºÑ€Ð°Ñ‚ÑŠÐº \nÐºÑ€ÑŠÐ³ÑŠÐ»\n\nÐ»ÐµÑÐµÐ½ Ð»ÐµÑÐ½Ð¾ Ð»Ð¸ Ð»ÐµÑ‚Ñ Ð»ÐµÑ‚Ð¸Ñˆ Ð»ÐµÑ‚Ð¸Ð¼ Ð»Ð¾Ñˆ\n\nÐ¼ Ð¼Ð°Ð¹ Ð¼Ð°Ð»ÐºÐ¾ Ð¼Ð°ÐºÐ°Ñ€ Ð¼Ð°Ð»Ñ†Ð¸Ð½Ð° Ð¼ÐµÐ¶Ð´ÑƒÐ²Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾ Ð¼Ð¸Ð½ÑƒÑ Ð¼Ðµ Ð¼ÐµÐ¶Ð´Ñƒ Ð¼ÐµÐº Ð¼ÐµÐ½ Ð¼ÐµÑÐµÑ† Ð¼Ð¸ Ð¼Ð¸Ñ \nÐ¼Ð¸ÑÐ»Ñ Ð¼Ð½Ð¾Ð³Ð¾ Ð¼Ð½Ð¾Ð·Ð¸Ð½Ð° Ð¼Ð¾Ð³Ð° Ð¼Ð¾Ð³Ð°Ñ‚ Ð¼Ð¾Ð¶Ðµ Ð¼Ð¾Ð¹ Ð¼Ð¾Ð¶ÐµÐ¼ Ð¼Ð¾ÐºÑŠÑ€ Ð¼Ð¾Ð»Ñ Ð¼Ð¾Ð¼ÐµÐ½Ñ‚Ð° Ð¼Ñƒ\n\nÐ½ Ð½Ð° Ð½Ð°Ð´ Ð½Ð°Ð·Ð°Ð´ Ð½Ð°Ð¹ Ð½Ð°Ñˆ Ð½Ð°Ð²ÑÑÐºÑŠÐ´Ðµ Ð½Ð°Ð²ÑŠÑ‚Ñ€Ðµ Ð½Ð°Ð³Ð¾Ñ€Ðµ Ð½Ð°Ð¿Ñ€Ð°Ð²Ð¸ Ð½Ð°Ð¿Ñ€ÐµÐ´ Ð½Ð°Ð´Ð¾Ð»Ñƒ Ð½Ð°Ð¸ÑÑ‚Ð¸Ð½Ð° \nÐ½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€ Ð½Ð°Ð¾Ð¿Ð°ÐºÐ¸ Ð½Ð°Ð¿Ð¾Ð»Ð¾Ð²Ð¸Ð½Ð° Ð½Ð°Ð¿Ð¾ÑÐ»ÐµÐ´ÑŠÐº Ð½ÐµÐºÐ° Ð½ÐµÐ·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ Ð½Ð°Ñ Ð½Ð°ÑÐ°Ð¼ Ð½Ð°ÑÐºÐ¾Ñ€Ð¾ \nÐ½Ð°ÑÑ‚Ñ€Ð°Ð½Ð° Ð½ÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ð¾ Ð½ÐµÐ³Ð¾ Ð½ÐµÐ³Ð¾Ð² Ð½ÐµÑ‰Ð¾ Ð½ÐµÑ Ð½Ð¸ Ð½Ð¸Ðµ Ð½Ð¸ÐºÐ¾Ð¹ Ð½Ð¸Ñ‚Ð¾ Ð½Ð¸Ñ‰Ð¾ Ð½Ð¾ Ð½Ð¾Ð² Ð½ÑÐºÐ°Ðº Ð½Ð¾Ð²Ð° \nÐ½Ð¾Ð²Ð¸ Ð½Ð¾Ð²Ð¸Ð½Ð° Ð½ÑÐºÐ¾Ð¸ Ð½ÑÐºÐ¾Ð¹ Ð½ÑÐºÐ¾Ð³Ð° Ð½ÑÐºÑŠÐ´Ðµ Ð½ÑÐºÐ¾Ð»ÐºÐ¾ Ð½ÑÐ¼Ð°\n\nÐ¾ Ð¾Ð±Ð°Ñ‡Ðµ Ð¾ÐºÐ¾Ð»Ð¾ Ð¾Ð¿Ð¸ÑÐ°Ð½ Ð¾Ð¿Ð¸Ñ‚Ð°Ñ… Ð¾Ð¿Ð¸Ñ‚Ð²Ð° Ð¾Ð¿Ð¸Ñ‚Ð²Ð°Ð¹ÐºÐ¸ Ð¾Ð¿Ð¸Ñ‚Ð²Ð°Ð¼ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¾ Ð¾ÑÐ²ÐµÐ½ \nÐ¾Ð±Ð¸ÐºÐ½Ð¾Ð²ÐµÐ½Ð¾ Ð¾ÑÐ¸Ð³ÑƒÑ€ÑÐ²Ð° Ð¾Ð±Ñ€Ð°Ñ‚Ð½Ð¾ Ð¾Ð·Ð½Ð°Ñ‡Ð°Ð²Ð° Ð¾ÑÐ¾Ð±ÐµÐ½ Ð¾ÑÐ¾Ð±ÐµÐ½Ð¾ Ð¾Ñ‚ Ð¾Ñ… Ð¾Ñ‚Ð²ÑŠÐ´ Ð¾Ñ‚Ð³Ð¾Ñ€Ðµ Ð¾Ñ‚Ð´Ð¾Ð»Ñƒ \nÐ¾Ñ‚Ð½Ð¾Ð²Ð¾ Ð¾Ñ‚Ð¸Ð²Ð° Ð¾Ñ‚Ð¸Ð²Ð°Ð¼ Ð¾Ñ‚Ð¸Ð´Ð¾Ñ… Ð¾Ñ‚ÑÐµÐ³Ð° Ð¾Ñ‚Ð´ÐµÐ»Ð½Ð¾ Ð¾Ñ‚ÐºÐ¾Ð»ÐºÐ¾Ñ‚Ð¾ Ð¾Ñ‚ÐºÑŠÐ´ÐµÑ‚Ð¾ Ð¾Ñ‡ÐµÐ²Ð¸Ð´Ð½Ð¾ Ð¾Ñ‚Ñ‚Ð°Ð¼ \nÐ¾Ñ‚Ð½Ð¾ÑÐ½Ð¾ Ð¾Ñ‰Ðµ\n\nÐ¿ Ð¿Ð°Ðº Ð¿Ð¾ Ð¿Ð¾Ð²ÐµÑ‡Ðµ Ð¿Ð¾Ð²ÐµÑ‡ÐµÑ‚Ð¾ Ð¿Ð¾Ð´ Ð¿Ð¾Ð½Ðµ Ð¿Ñ€Ð¾ÑÑ‚Ð¾ Ð¿Ñ€ÑÐºÐ¾ Ð¿Ð¾Ñ€Ð°Ð´Ð¸ Ð¿Ð¾ÑÐ»Ðµ Ð¿Ð¾ÑÐ»ÐµÐ´ÐµÐ½ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¾ \nÐ¿Ð¾ÑÐ¾Ñ‡ÐµÐ½ Ð¿Ð¾Ñ‡Ñ‚Ð¸ Ð¿Ñ€Ð°Ð²Ð¸ Ð¿Ñ€Ð°Ð² Ð¿Ñ€Ð°Ð²Ð¸ Ð¿Ñ€Ð°Ð²Ñ Ð¿Ñ€ÐµÐ´ Ð¿Ñ€ÐµÐ´Ð¸ Ð¿Ñ€ÐµÐ· Ð¿Ñ€Ð¸ Ð¿ÑŠÐº Ð¿ÑŠÑ€Ð²Ð°Ñ‚Ð° Ð¿ÑŠÑ€Ð²Ð¸ Ð¿ÑŠÑ€Ð²Ð¾ \nÐ¿ÑŠÑ‚ Ð¿ÑŠÑ‚Ð¸ Ð¿Ð»ÑŽÑ\n\nÑ€Ð°Ð²ÐµÐ½ Ñ€Ð°Ð²Ð½Ð° Ñ€Ð°Ð·Ð»Ð¸Ñ‡ÐµÐ½ Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ð¸ Ñ€Ð°Ð·ÑƒÐ¼ÐµÐ½ Ñ€Ð°Ð·ÑƒÐ¼Ð½Ð¾\n\nÑ ÑÐ° ÑÐ°Ð¼ ÑÐ°Ð¼Ð¾ ÑÐµÐ±Ðµ ÑÐµÑ€Ð¸Ð¾Ð·Ð½Ð¾ ÑÐ¸Ð³ÑƒÑ€ÐµÐ½ ÑÐ¸Ð³ÑƒÑ€Ð½Ð¾ ÑÐµ ÑÐµÐ³Ð° ÑÐ¸ ÑÐ¸Ð½ ÑÐºÐ¾Ñ€Ð¾ ÑÐºÐ¾Ñ€Ð¾ÑˆÐµÐ½ ÑÐ»ÐµÐ´ \nÑÐ»ÐµÐ´Ð²Ð°Ñ‰ ÑÐ»ÐµÐ´Ð²Ð°Ñ‰Ð¸Ñ ÑÐ»ÐµÐ´Ð²Ð° ÑÐ»ÐµÐ´Ð½Ð¾Ñ‚Ð¾ ÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»Ð½Ð¾ ÑÐ»ÑƒÑ‡Ð²Ð° ÑÐ¼Ðµ ÑÐ¼ÑÑ… ÑÐ¾Ð±ÑÑ‚Ð²ÐµÐ½ \nÑÑ€Ð°Ð²Ð½Ð¸Ñ‚ÐµÐ»Ð½Ð¾ ÑÐ¼ÐµÑ ÑÐ¿Ð¾Ñ€ÐµÐ´ ÑÑ€ÐµÐ´ ÑÑ‚Ð°Ð²Ð° ÑÑ€ÐµÑ‰Ñƒ ÑÑŠÐ²ÑÐµÐ¼ ÑÑŠÐ´ÑŠÑ€Ð¶Ð° ÑÑŠÐ´ÑŠÑ€Ð¶Ð°Ñ‰ ÑÑŠÐ¶Ð°Ð»ÑÐ²Ð°Ð¼ \nÑÑŠÐ¾Ñ‚Ð²ÐµÑ‚ÐµÐ½ ÑÑŠÐ¾Ñ‚Ð²ÐµÑ‚Ð½Ð¾ ÑÑ‚Ðµ ÑÑŠÐ¼ ÑÑŠÑ ÑÑŠÑ‰Ð¾\n\nÑ‚ Ñ‚Ð°ÐºÐ° Ñ‚ÐµÑ…ÐµÐ½ Ñ‚ÐµÑ…Ð½Ð¸ Ñ‚Ð°ÐºÐ¸Ð²Ð° Ñ‚Ð°ÐºÑŠÐ² Ñ‚Ð²ÑŠÑ€Ð´Ðµ Ñ‚Ð°Ð¼ Ñ‚Ñ€ÐµÑ‚Ð° Ñ‚Ð²Ð¾Ð¹ Ñ‚Ðµ Ñ‚ÐµÐ·Ð¸ Ñ‚Ð¸ Ñ‚Ð¾ Ñ‚Ð¾Ð²Ð° \nÑ‚Ð¾Ð³Ð°Ð²Ð° Ñ‚Ð¾Ð·Ð¸ Ñ‚Ð¾Ð¹ Ñ‚ÑŠÑ€ÑÐ¸ Ñ‚Ð¾Ð»ÐºÐ¾Ð²Ð° Ñ‚Ð¾Ñ‡Ð½Ð¾ Ñ‚Ñ€Ð¸ Ñ‚Ñ€ÑÐ±Ð²Ð° Ñ‚ÑƒÐº Ñ‚ÑŠÐ¹ Ñ‚Ñ Ñ‚ÑÑ…\n\nÑƒ ÑƒÑ‚Ñ€Ðµ ÑƒÐ¶Ð°ÑÐ½Ð¾ ÑƒÐ¿Ð¾Ñ‚Ñ€ÐµÐ±Ð° ÑƒÑÐ¿Ð¾Ñ€ÐµÐ´Ð½Ð¾ ÑƒÑ‚Ð¾Ñ‡Ð½ÐµÐ½ ÑƒÑ‚Ð¾Ñ‡Ð½ÑÐ²Ð°Ð½Ðµ\n\nÑ…Ð°Ñ€ÐµÑÐ²Ð° Ñ…Ð°Ñ€ÐµÑÐ°Ð»Ð¸ Ñ…Ð¸Ð»ÑÐ´Ð¸\n\nÑ‡ Ñ‡Ð°ÑÐ° Ñ†ÐµÐ½Ñ Ñ†ÑÐ»Ð¾ Ñ†ÑÐ»Ð¾ÑÑ‚ÐµÐ½ Ñ‡Ðµ Ñ‡ÐµÑÑ‚Ð¾ Ñ‡Ñ€ÐµÐ· Ñ‡ÑƒÐ´Ñ\n\nÑ‰Ðµ Ñ‰ÐµÑˆÐµ Ñ‰Ð¾Ð¼ Ñ‰ÑÑ…Ð°\n\nÑŽÐ¼Ñ€ÑƒÐº\n\nÑ ÑÐº\n'.split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/bg/tokenizer_exceptions.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/bg/__init__.py----------------------------------------
A:spacy.lang.bg.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.bg.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS, TOKENIZER_EXCEPTIONS)
spacy.lang.bg.__init__.Bulgarian(Language)
spacy.lang.bg.__init__.BulgarianDefaults(BaseDefaults)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/bg/lex_attrs.py----------------------------------------
A:spacy.lang.bg.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.bg.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.bg.lex_attrs.like_num(text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ky/examples.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ky/stop_words.py----------------------------------------
A:spacy.lang.ky.stop_words.STOP_WORDS->set('\nÐ°Ð³Ð° Ð°Ð´Ð°Ð¼ Ð°Ð¹Ñ‚Ñ‚Ñ‹ Ð°Ð¹Ñ‚Ñ‹Ð¼Ñ‹Ð½Ð´Ð° Ð°Ð¹Ñ‚Ñ‹Ð¿ Ð°Ð» Ð°Ð»Ð°Ñ€\nÐ°Ð»Ð°Ñ€Ð´Ñ‹Ð½ Ð°Ð»Ð³Ð°Ð½ Ð°Ð»ÑƒÑƒ Ð°Ð»Ñ‹Ð¿ Ð°Ð½Ð´Ð° Ð°Ð½Ð´Ð°Ð½ Ð°Ð½Ñ‹\nÐ°Ð½Ñ‹Ð½ Ð°Ñ€\n\nÐ±Ð°Ñ€ Ð±Ð°ÑÐ¼Ð° Ð±Ð°Ñˆ Ð±Ð°ÑˆÐºÐ° Ð±Ð°ÑˆÐºÑ‹ Ð±Ð°ÑˆÑ‡Ñ‹ÑÑ‹ Ð±ÐµÑ€Ð³ÐµÐ½\nÐ±Ð¸Ð· Ð±Ð¸Ð»Ð´Ð¸Ñ€Ð³ÐµÐ½ Ð±Ð¸Ð»Ð´Ð¸Ñ€Ð´Ð¸ Ð±Ð¸Ñ€ Ð±Ð¸Ñ€Ð¸Ð½Ñ‡Ð¸ Ð±Ð¸Ñ€Ð¾Ðº\nÐ±Ð¸ÑˆÐºÐµÐº Ð±Ð¾Ð»Ð³Ð¾Ð½ Ð±Ð¾Ð»Ð¾Ñ‚ Ð±Ð¾Ð»ÑÐ¾ Ð±Ð¾Ð»ÑƒÐ¿ Ð±Ð¾ÑŽÐ½Ñ‡Ð°\nÐ±ÑƒÐ³Ð° Ð±ÑƒÐ»\n\nÐ³Ð°Ð½Ð°\n\nÐ´Ð° Ð´Ð°Ð³Ñ‹ Ð´ÐµÐ³ÐµÐ½ Ð´ÐµÐ´Ð¸ Ð´ÐµÐ¿\n\nÐ¶Ð°Ð½Ð° Ð¶Ð°Ñ‚Ð°Ñ‚ Ð¶Ð°Ñ‚ÐºÐ°Ð½ Ð¶Ð°Ò£Ñ‹ Ð¶Ðµ Ð¶Ð¾Ð³Ð¾Ñ€ÐºÑƒ Ð¶Ð¾Ðº Ð¶Ð¾Ð»\nÐ¶Ð¾Ð»Ñƒ\n\nÐºÐ°Ð±Ñ‹Ð» ÐºÐ°Ð»Ð³Ð°Ð½ ÐºÐ°Ð½Ð´Ð°Ð¹ ÐºÐ°Ñ€Ð°Ñ‚Ð° ÐºÐ°Ñ€ÑˆÑ‹ ÐºÐ°Ñ‚Ð°Ñ€Ñ‹\nÐºÐµÐ»Ð³ÐµÐ½ ÐºÐµÑ€ÐµÐº ÐºÐ¸Ð¹Ð¸Ð½ ÐºÐ¾Ð» ÐºÑ‹Ð»Ð¼Ñ‹Ñˆ ÐºÑ‹Ñ€Ð³Ñ‹Ð·\nÐºÒ¯Ð½Ò¯ ÐºÓ©Ð¿\n\nÐ¼Ð°Ð°Ð»Ñ‹Ð¼Ð°Ñ‚ Ð¼Ð°Ð¼Ð»ÐµÐºÐµÑ‚Ñ‚Ð¸Ðº Ð¼ÐµÐ½ Ð¼ÐµÐ½ÐµÐ½ Ð¼Ð¸Ò£\nÐ¼ÑƒÑ€Ð´Ð°Ð³Ñ‹ Ð¼Ñ‹Ð¹Ð·Ð°Ð¼ Ð¼Ñ‹Ð½Ð´Ð°Ð¹ Ð¼Ò¯Ð¼ÐºÒ¯Ð½\n\nÐ¾ÑˆÐ¾Ð» Ð¾ÑˆÐ¾Ð½Ð´Ð¾Ð¹\n\nÑÒ¯Ñ€Ó©Ñ‚ ÑÓ©Ð·\n\nÑ‚Ð°Ñ€Ð°Ð±Ñ‹Ð½Ð°Ð½ Ñ‚ÑƒÑ€Ð³Ð°Ð½ Ñ‚ÑƒÑƒÑ€Ð°Ð»ÑƒÑƒ\n\nÑƒÐºÑƒÐº ÑƒÑ‡ÑƒÑ€Ð´Ð°\n\nÑ‡ÐµÐ¹Ð¸Ð½ Ñ‡ÐµÐº\n\nÑÐºÐµÐ½Ð¸Ð½ ÑÐºÐ¸ ÑÐ» ÑÐ»Ðµ ÑÐ¼ÐµÑ ÑÐ¼Ð¸ ÑÑ‡\n\nÒ¯Ñ‡ Ò¯Ñ‡Ò¯Ð½\n\nÓ©Ð·\n'.split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ky/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.ky.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ky/__init__.py----------------------------------------
spacy.lang.ky.__init__.Kyrgyz(Language)
spacy.lang.ky.__init__.KyrgyzDefaults(BaseDefaults)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ky/lex_attrs.py----------------------------------------
A:spacy.lang.ky.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.ky.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.ky.lex_attrs.like_num(text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ky/punctuation.py----------------------------------------
A:spacy.lang.ky.punctuation._hyphens_no_dash->char_classes.HYPHENS.replace('-', '').strip('|').replace('||', '')


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/id/examples.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/id/stop_words.py----------------------------------------
A:spacy.lang.id.stop_words.STOP_WORDS->set('\nada adalah adanya adapun agak agaknya agar akan akankah akhir akhiri akhirnya\naku akulah amat amatlah anda andalah antar antara antaranya apa apaan apabila\napakah apalagi apatah artinya asal asalkan atas atau ataukah ataupun awal\nawalnya\n\nbagai bagaikan bagaimana bagaimanakah bagaimanapun bagi bagian bahkan bahwa\nbahwasanya baik bakal bakalan balik banyak bapak baru bawah beberapa begini\nbeginian beginikah beginilah begitu begitukah begitulah begitupun bekerja\nbelakang belakangan belum belumlah benar benarkah benarlah berada berakhir\nberakhirlah berakhirnya berapa berapakah berapalah berapapun berarti berawal\nberbagai berdatangan beri berikan berikut berikutnya berjumlah berkali-kali\nberkata berkehendak berkeinginan berkenaan berlainan berlalu berlangsung\nberlebihan bermacam bermacam-macam bermaksud bermula bersama bersama-sama\nbersiap bersiap-siap bertanya bertanya-tanya berturut berturut-turut bertutur\nberujar berupa besar betul betulkah biasa biasanya bila bilakah bisa bisakah\nboleh bolehkah bolehlah buat bukan bukankah bukanlah bukannya bulan bung\n\ncara caranya cukup cukupkah cukuplah cuma\n\ndahulu dalam dan dapat dari daripada datang dekat demi demikian demikianlah\ndengan depan di dia diakhiri diakhirinya dialah diantara diantaranya diberi\ndiberikan diberikannya dibuat dibuatnya didapat didatangkan digunakan\ndiibaratkan diibaratkannya diingat diingatkan diinginkan dijawab dijelaskan\ndijelaskannya dikarenakan dikatakan dikatakannya dikerjakan diketahui\ndiketahuinya dikira dilakukan dilalui dilihat dimaksud dimaksudkan\ndimaksudkannya dimaksudnya diminta dimintai dimisalkan dimulai dimulailah\ndimulainya dimungkinkan dini dipastikan diperbuat diperbuatnya dipergunakan\ndiperkirakan diperlihatkan diperlukan diperlukannya dipersoalkan dipertanyakan\ndipunyai diri dirinya disampaikan disebut disebutkan disebutkannya disini\ndisinilah ditambahkan ditandaskan ditanya ditanyai ditanyakan ditegaskan\nditujukan ditunjuk ditunjuki ditunjukkan ditunjukkannya ditunjuknya dituturkan\ndituturkannya diucapkan diucapkannya diungkapkan dong dua dulu\n\nempat enggak enggaknya entah entahlah\n\nguna gunakan\n\nhal hampir hanya hanyalah hari harus haruslah harusnya hendak hendaklah\nhendaknya hingga\n\nia ialah ibarat ibaratkan ibaratnya ibu ikut ingat ingat-ingat ingin inginkah\ninginkan ini inikah inilah itu itukah itulah\n\njadi jadilah jadinya jangan jangankan janganlah jauh jawab jawaban jawabnya\njelas jelaskan jelaslah jelasnya jika jikalau juga jumlah jumlahnya justru\n\nkala kalau kalaulah kalaupun kalian kami kamilah kamu kamulah kan kapan\nkapankah kapanpun karena karenanya kasus kata katakan katakanlah katanya ke\nkeadaan kebetulan kecil kedua keduanya keinginan kelamaan kelihatan\nkelihatannya kelima keluar kembali kemudian kemungkinan kemungkinannya kenapa\nkepada kepadanya kesampaian keseluruhan keseluruhannya keterlaluan ketika\nkhususnya kini kinilah kira kira-kira kiranya kita kitalah kok kurang\n\nlagi lagian lah lain lainnya lalu lama lamanya lanjut lanjutnya lebih lewat\nlima luar\n\nmacam maka makanya makin malah malahan mampu mampukah mana manakala manalagi\nmasa masalah masalahnya masih masihkah masing masing-masing mau maupun\nmelainkan melakukan melalui melihat melihatnya memang memastikan memberi\nmemberikan membuat memerlukan memihak meminta memintakan memisalkan memperbuat\nmempergunakan memperkirakan memperlihatkan mempersiapkan mempersoalkan\nmempertanyakan mempunyai memulai memungkinkan menaiki menambahkan menandaskan\nmenanti menanti-nanti menantikan menanya menanyai menanyakan mendapat\nmendapatkan mendatang mendatangi mendatangkan menegaskan mengakhiri mengapa\nmengatakan mengatakannya mengenai mengerjakan mengetahui menggunakan\nmenghendaki mengibaratkan mengibaratkannya mengingat mengingatkan menginginkan\nmengira mengucapkan mengucapkannya mengungkapkan menjadi menjawab menjelaskan\nmenuju menunjuk menunjuki menunjukkan menunjuknya menurut menuturkan\nmenyampaikan menyangkut menyatakan menyebutkan menyeluruh menyiapkan merasa\nmereka merekalah merupakan meski meskipun meyakini meyakinkan minta mirip\nmisal misalkan misalnya mula mulai mulailah mulanya mungkin mungkinkah\n\nnah naik namun nanti nantinya nyaris nyatanya\n\noleh olehnya\n\npada padahal padanya pak paling panjang pantas para pasti pastilah penting\npentingnya per percuma perlu perlukah perlunya pernah persoalan pertama\npertama-tama pertanyaan pertanyakan pihak pihaknya pukul pula pun punya\n\nrasa rasanya rata rupanya\n\nsaat saatnya saja sajalah saling sama sama-sama sambil sampai sampai-sampai\nsampaikan sana sangat sangatlah satu saya sayalah se sebab sebabnya sebagai\nsebagaimana sebagainya sebagian sebaik sebaik-baiknya sebaiknya sebaliknya\nsebanyak sebegini sebegitu sebelum sebelumnya sebenarnya seberapa sebesar\nsebetulnya sebisanya sebuah sebut sebutlah sebutnya secara secukupnya sedang\nsedangkan sedemikian sedikit sedikitnya seenaknya segala segalanya segera\nseharusnya sehingga seingat sejak sejauh sejenak sejumlah sekadar sekadarnya\nsekali sekali-kali sekalian sekaligus sekalipun sekarang sekarang sekecil\nseketika sekiranya sekitar sekitarnya sekurang-kurangnya sekurangnya sela\nselain selaku selalu selama selama-lamanya selamanya selanjutnya seluruh\nseluruhnya semacam semakin semampu semampunya semasa semasih semata semata-mata\nsemaunya sementara semisal semisalnya sempat semua semuanya semula sendiri\nsendirian sendirinya seolah seolah-olah seorang sepanjang sepantasnya\nsepantasnyalah seperlunya seperti sepertinya sepihak sering seringnya serta\nserupa sesaat sesama sesampai sesegera sesekali seseorang sesuatu sesuatunya\nsesudah sesudahnya setelah setempat setengah seterusnya setiap setiba setibanya\nsetidak-tidaknya setidaknya setinggi seusai sewaktu siap siapa siapakah\nsiapapun sini sinilah soal soalnya suatu sudah sudahkah sudahlah supaya\n\ntadi tadinya tahu tahun tak tambah tambahnya tampak tampaknya tandas tandasnya\ntanpa tanya tanyakan tanyanya tapi tegas tegasnya telah tempat tengah tentang\ntentu tentulah tentunya tepat terakhir terasa terbanyak terdahulu terdapat\nterdiri terhadap terhadapnya teringat teringat-ingat terjadi terjadilah\nterjadinya terkira terlalu terlebih terlihat termasuk ternyata tersampaikan\ntersebut tersebutlah tertentu tertuju terus terutama tetap tetapi tiap tiba\ntiba-tiba tidak tidakkah tidaklah tiga tinggi toh tunjuk turut tutur tuturnya\n\nucap ucapnya ujar ujarnya umum umumnya ungkap ungkapnya untuk usah usai\n\nwaduh wah wahai waktu waktunya walau walaupun wong\n\nyaitu yakin yakni yang\n'.split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/id/_tokenizer_exceptions_list.py----------------------------------------
A:spacy.lang.id._tokenizer_exceptions_list.ID_BASE_EXCEPTIONS->set('\naba-aba\nabah-abah\nabal-abal\nabang-abang\nabar-abar\nabong-abong\nabrit-abrit\nabrit-abritan\nabu-abu\nabuh-abuhan\nabuk-abuk\nabun-abun\nacak-acak\nacak-acakan\nacang-acang\nacap-acap\naci-aci\naci-acian\naci-acinya\naco-acoan\nad-blocker\nad-interim\nada-ada\nada-adanya\nada-adanyakah\nadang-adang\nadap-adapan\nadd-on\nadd-ons\nadik-adik\nadik-beradik\naduk-adukan\nafter-sales\nagak-agak\nagak-agih\nagama-agama\nagar-agar\nage-related\nagut-agut\nair-air\nair-cooled\nair-to-air\najak-ajak\najar-ajar\naji-aji\nakal-akal\nakal-akalan\nakan-akan\nakar-akar\nakar-akaran\nakhir-akhir\nakhir-akhirnya\naki-aki\naksi-aksi\nalah-mengalahi\nalai-belai\nalan-alan\nalang-alang\nalang-alangan\nalap-alap\nalat-alat\nali-ali\nalif-alifan\nalih-alih\naling-aling\naling-alingan\nalip-alipan\nall-electric\nall-in-one\nall-out\nall-time\nalon-alon\nalt-right\nalt-text\nalu-alu\nalu-aluan\nalun-alun\nalur-alur\nalur-aluran\nalways-on\namai-amai\namatir-amatiran\nambah-ambah\nambai-ambai\nambil-mengambil\nambreng-ambrengan\nambring-ambringan\nambu-ambu\nambung-ambung\namin-amin\namit-amit\nampai-ampai\namprung-amprungan\namung-amung\nanai-anai\nanak-anak\nanak-anakan\nanak-beranak\nanak-cucu\nanak-istri\nancak-ancak\nancang-ancang\nancar-ancar\nandang-andang\nandeng-andeng\naneh-aneh\nangan-angan\nanggar-anggar\nanggaran-red\nanggota-anggota\nanggung-anggip\nangin-angin\nangin-anginan\nangkal-angkal\nangkul-angkul\nangkup-angkup\nangkut-angkut\nani-ani\naning-aning\nanjang-anjang\nanjing-anjing\nanjung-anjung\nanjung-anjungan\nantah-berantah\nantar-antar\nantar-mengantar\nante-mortem\nantek-antek\nanter-anter\nantihuru-hara\nanting-anting\nantung-antung\nanyam-menganyam\nanyang-anyang\napa-apa\napa-apaan\napel-apel\napi-api\napit-apit\naplikasi-aplikasi\napotek-apotek\naprit-apritan\napu-apu\napung-apung\narah-arah\narak-arak\narak-arakan\naram-aram\narek-arek\narem-arem\nari-ari\nartis-artis\naru-aru\narung-arungan\nasa-asaan\nasal-asalan\nasal-muasal\nasal-usul\nasam-asaman\nasas-asas\naset-aset\nasmaul-husna\nasosiasi-asosiasi\nasuh-asuh\nasyik-asyiknya\natas-mengatasi\nati-ati\natung-atung\naturan-aturan\naudio-video\naudio-visual\nauto-brightness\nauto-complete\nauto-focus\nauto-play\nauto-update\navant-garde\nawan-awan\nawan-berawan\nawang-awang\nawang-gemawang\nawar-awar\nawat-awat\nawik-awik\nawut-awutan\nayah-anak\nayak-ayak\nayam-ayam\nayam-ayaman\nayang-ayang\nayat-ayat\nayeng-ayengan\nayun-temayun\nayut-ayutan\nba-bi-bu\nback-to-back\nback-up\nbadan-badan\nbade-bade\nbadut-badut\nbagi-bagi\nbahan-bahan\nbahu-membahu\nbaik-baik\nbail-out\nbajang-bajang\nbaji-baji\nbalai-balai\nbalam-balam\nbalas-berbalas\nbalas-membalas\nbale-bale\nbaling-baling\nball-playing\nbalon-balon\nbalut-balut\nband-band\nbandara-bandara\nbangsa-bangsa\nbangun-bangun\nbangunan-bangunan\nbank-bank\nbantah-bantah\nbantahan-bantahan\nbantal-bantal\nbanyak-banyak\nbapak-anak\nbapak-bapak\nbapak-ibu\nbapak-ibunya\nbarang-barang\nbarat-barat\nbarat-daya\nbarat-laut\nbarau-barau\nbare-bare\nbareng-bareng\nbari-bari\nbarik-barik\nbaris-berbaris\nbaru-baru\nbaru-batu\nbarung-barung\nbasa-basi\nbata-bata\nbatalyon-batalyon\nbatang-batang\nbatas-batas\nbatir-batir\nbatu-batu\nbatuk-batuk\nbatung-batung\nbau-bauan\nbawa-bawa\nbayan-bayan\nbayang-bayang\nbayi-bayi\nbea-cukai\nbedeng-bedeng\nbedil-bedal\nbedil-bedilan\nbegana-begini\nbek-bek\nbekal-bekalan\nbekerdom-kerdom\nbekertak-kertak\nbelang-belang\nbelat-belit\nbeliau-beliau\nbelu-belai\nbelum-belum\nbenar-benar\nbenda-benda\nbengang-bengut\nbenggal-benggil\nbengkal-bengkil\nbengkang-bengkok\nbengkang-bengkong\nbengkang-bengkung\nbenteng-benteng\nbentuk-bentuk\nbenua-benua\nber-selfie\nberabad-abad\nberabun-rabun\nberacah-acah\nberada-ada\nberadik-berkakak\nberagah-agah\nberagak-agak\nberagam-ragam\nberaja-raja\nberakit-rakit\nberaku-akuan\nberalu-aluan\nberalun-alun\nberamah-ramah\nberamah-ramahan\nberamah-tamah\nberamai-ramai\nberambai-ambai\nberambal-ambalan\nberambil-ambil\nberamuk-amuk\nberamuk-amukan\nberandai-andai\nberandai-randai\nberaneh-aneh\nberang-berang\nberangan-angan\nberanggap-anggapan\nberangguk-angguk\nberangin-angin\nberangka-angka\nberangka-angkaan\nberangkai-rangkai\nberangkap-rangkapan\nberani-berani\nberanja-anja\nberantai-rantai\nberapi-api\nberapung-apung\nberarak-arakan\nberas-beras\nberasak-asak\nberasak-asakan\nberasap-asap\nberasing-asingan\nberatus-ratus\nberawa-rawa\nberawas-awas\nberayal-ayalan\nberayun-ayun\nberbagai-bagai\nberbahas-bahasan\nberbahasa-bahasa\nberbaik-baikan\nberbait-bait\nberbala-bala\nberbalas-balasan\nberbalik-balik\nberbalun-balun\nberbanjar-banjar\nberbantah-bantah\nberbanyak-banyak\nberbarik-barik\nberbasa-basi\nberbasah-basah\nberbatu-batu\nberbayang-bayang\nberbecak-becak\nberbeda-beda\nberbedil-bedilan\nberbega-bega\nberbeka-beka\nberbelah-belah\nberbelakang-belakangan\nberbelang-belang\nberbelau-belauan\nberbeli-beli\nberbeli-belian\nberbelit-belit\nberbelok-belok\nberbenang-benang\nberbenar-benar\nberbencah-bencah\nberbencol-bencol\nberbenggil-benggil\nberbentol-bentol\nberbentong-bentong\nberberani-berani\nberbesar-besar\nberbidai-bidai\nberbiduk-biduk\nberbiku-biku\nberbilik-bilik\nberbinar-binar\nberbincang-bincang\nberbingkah-bingkah\nberbintang-bintang\nberbintik-bintik\nberbintil-bintil\nberbisik-bisik\nberbolak-balik\nberbolong-bolong\nberbondong-bondong\nberbongkah-bongkah\nberbuai-buai\nberbual-bual\nberbudak-budak\nberbukit-bukit\nberbulan-bulan\nberbunga-bunga\nberbuntut-buntut\nberbunuh-bunuhan\nberburu-buru\nberburuk-buruk\nberbutir-butir\nbercabang-cabang\nbercaci-cacian\nbercakap-cakap\nbercakar-cakaran\nbercamping-camping\nbercantik-cantik\nbercari-cari\nbercari-carian\nbercarik-carik\nbercarut-carut\nbercebar-cebur\nbercepat-cepat\nbercerai-berai\nbercerai-cerai\nbercetai-cetai\nberciap-ciap\nbercikun-cikun\nbercinta-cintaan\nbercita-cita\nberciut-ciut\nbercompang-camping\nberconteng-conteng\nbercoreng-coreng\nbercoreng-moreng\nbercuang-caing\nbercuit-cuit\nbercumbu-cumbu\nbercumbu-cumbuan\nbercura-bura\nbercura-cura\nberdada-dadaan\nberdahulu-dahuluan\nberdalam-dalam\nberdalih-dalih\nberdampung-dampung\nberdebar-debar\nberdecak-decak\nberdecap-decap\nberdecup-decup\nberdecut-decut\nberdedai-dedai\nberdegap-degap\nberdegar-degar\nberdeham-deham\nberdekah-dekah\nberdekak-dekak\nberdekap-dekapan\nberdekat-dekat\nberdelat-delat\nberdembai-dembai\nberdembun-dembun\nberdempang-dempang\nberdempet-dempet\nberdencing-dencing\nberdendam-dendaman\nberdengkang-dengkang\nberdengut-dengut\nberdentang-dentang\nberdentum-dentum\nberdentung-dentung\nberdenyar-denyar\nberdenyut-denyut\nberdepak-depak\nberdepan-depan\nberderai-derai\nberderak-derak\nberderam-deram\nberderau-derau\nberderik-derik\nberdering-dering\nberderung-derung\nberderus-derus\nberdesak-desakan\nberdesik-desik\nberdesing-desing\nberdesus-desus\nberdikit-dikit\nberdingkit-dingkit\nberdua-dua\nberduri-duri\nberduru-duru\nberduyun-duyun\nberebut-rebut\nberebut-rebutan\nberegang-regang\nberek-berek\nberembut-rembut\nberempat-empat\nberenak-enak\nberencel-encel\nbereng-bereng\nberenggan-enggan\nberenteng-renteng\nberesa-esaan\nberesah-resah\nberfoya-foya\nbergagah-gagahan\nbergagap-gagap\nbergagau-gagau\nbergalur-galur\nberganda-ganda\nberganjur-ganjur\nberganti-ganti\nbergarah-garah\nbergaruk-garuk\nbergaya-gaya\nbergegas-gegas\nbergelang-gelang\nbergelap-gelap\nbergelas-gelasan\nbergeleng-geleng\nbergemal-gemal\nbergembar-gembor\nbergembut-gembut\nbergepok-gepok\nbergerek-gerek\nbergesa-gesa\nbergilir-gilir\nbergolak-golak\nbergolek-golek\nbergolong-golong\nbergores-gores\nbergotong-royong\nbergoyang-goyang\nbergugus-gugus\nbergulung-gulung\nbergulut-gulut\nbergumpal-gumpal\nbergunduk-gunduk\nbergunung-gunung\nberhadap-hadapan\nberhamun-hamun\nberhandai-handai\nberhanyut-hanyut\nberhari-hari\nberhati-hati\nberhati-hatilah\nberhektare-hektare\nberhilau-hilau\nberhormat-hormat\nberhujan-hujan\nberhura-hura\nberi-beri\nberi-memberi\nberia-ia\nberia-ria\nberiak-riak\nberiba-iba\nberibu-ribu\nberigi-rigi\nberimpit-impit\nberindap-indap\nbering-bering\nberingat-ingat\nberinggit-ringgit\nberintik-rintik\nberiring-iring\nberiring-iringan\nberita-berita\nberjabir-jabir\nberjaga-jaga\nberjagung-jagung\nberjalan-jalan\nberjalar-jalar\nberjalin-jalin\nberjalur-jalur\nberjam-jam\nberjari-jari\nberjauh-jauhan\nberjegal-jegalan\nberjejal-jejal\nberjela-jela\nberjengkek-jengkek\nberjenis-jenis\nberjenjang-jenjang\nberjilid-jilid\nberjinak-jinak\nberjingkat-jingkat\nberjingkik-jingkik\nberjingkrak-jingkrak\nberjongkok-jongkok\nberjubel-jubel\nberjujut-jujutan\nberjulai-julai\nberjumbai-jumbai\nberjumbul-jumbul\nberjuntai-juntai\nberjurai-jurai\nberjurus-jurus\nberjuta-juta\nberka-li-kali\nberkabu-kabu\nberkaca-kaca\nberkaing-kaing\nberkait-kaitan\nberkala-kala\nberkali-kali\nberkamit-kamit\nberkanjar-kanjar\nberkaok-kaok\nberkarung-karung\nberkasak-kusuk\nberkasih-kasihan\nberkata-kata\nberkatak-katak\nberkecai-kecai\nberkecek-kecek\nberkecil-kecil\nberkecil-kecilan\nberkedip-kedip\nberkejang-kejang\nberkejap-kejap\nberkejar-kejaran\nberkelar-kelar\nberkelepai-kelepai\nberkelip-kelip\nberkelit-kelit\nberkelok-kelok\nberkelompok-kelompok\nberkelun-kelun\nberkembur-kembur\nberkempul-kempul\nberkena-kenaan\nberkenal-kenalan\nberkendur-kendur\nberkeok-keok\nberkepak-kepak\nberkepal-kepal\nberkeping-keping\nberkepul-kepul\nberkeras-kerasan\nberkering-kering\nberkeritik-keritik\nberkeruit-keruit\nberkerut-kerut\nberketai-ketai\nberketak-ketak\nberketak-ketik\nberketap-ketap\nberketap-ketip\nberketar-ketar\nberketi-keti\nberketil-ketil\nberketuk-ketak\nberketul-ketul\nberkial-kial\nberkian-kian\nberkias-kias\nberkias-kiasan\nberkibar-kibar\nberkilah-kilah\nberkilap-kilap\nberkilat-kilat\nberkilau-kilauan\nberkilo-kilo\nberkimbang-kimbang\nberkinja-kinja\nberkipas-kipas\nberkira-kira\nberkirim-kiriman\nberkisar-kisar\nberkoak-koak\nberkoar-koar\nberkobar-kobar\nberkobok-kobok\nberkocak-kocak\nberkodi-kodi\nberkolek-kolek\nberkomat-kamit\nberkopah-kopah\nberkoper-koper\nberkotak-kotak\nberkuat-kuat\nberkuat-kuatan\nberkumur-kumur\nberkunang-kunang\nberkunar-kunar\nberkunjung-kunjungan\nberkurik-kurik\nberkurun-kurun\nberkusau-kusau\nberkusu-kusu\nberkusut-kusut\nberkuting-kuting\nberkutu-kutuan\nberlabun-labun\nberlain-lainan\nberlaju-laju\nberlalai-lalai\nberlama-lama\nberlambai-lambai\nberlambak-lambak\nberlampang-lampang\nberlanggar-langgar\nberlapang-lapang\nberlapis-lapis\nberlapuk-lapuk\nberlarah-larah\nberlarat-larat\nberlari-lari\nberlari-larian\nberlarih-larih\nberlarik-larik\nberlarut-larut\nberlawak-lawak\nberlayap-layapan\nberlebih-lebih\nberlebih-lebihan\nberleha-leha\nberlekas-lekas\nberlekas-lekasan\nberlekat-lekat\nberlekuk-lekuk\nberlempar-lemparan\nberlena-lena\nberlengah-lengah\nberlenggak-lenggok\nberlenggek-lenggek\nberlenggok-lenggok\nberleret-leret\nberletih-letih\nberliang-liuk\nberlibat-libat\nberligar-ligar\nberliku-liku\nberlikur-likur\nberlimbak-limbak\nberlimpah-limpah\nberlimpap-limpap\nberlimpit-limpit\nberlinang-linang\nberlindak-lindak\nberlipat-lipat\nberlomba-lomba\nberlompok-lompok\nberloncat-loncatan\nberlopak-lopak\nberlubang-lubang\nberlusin-lusin\nbermaaf-maafan\nbermabuk-mabukan\nbermacam-macam\nbermain-main\nbermalam-malam\nbermalas-malas\nbermalas-malasan\nbermanik-manik\nbermanis-manis\nbermanja-manja\nbermasak-masak\nbermati-mati\nbermegah-megah\nbermemek-memek\nbermenung-menung\nbermesra-mesraan\nbermewah-mewah\nbermewah-mewahan\nberminggu-minggu\nberminta-minta\nberminyak-minyak\nbermuda-muda\nbermudah-mudah\nbermuka-muka\nbermula-mula\nbermuluk-muluk\nbermulut-mulut\nbernafsi-nafsi\nbernaka-naka\nbernala-nala\nbernanti-nanti\nberniat-niat\nbernyala-nyala\nberogak-ogak\nberoleng-oleng\nberolok-olok\nberomong-omong\nberoncet-roncet\nberonggok-onggok\nberorang-orang\nberoyal-royal\nberpada-pada\nberpadu-padu\nberpahit-pahit\nberpair-pair\nberpal-pal\nberpalu-palu\nberpalu-paluan\nberpalun-palun\nberpanas-panas\nberpandai-pandai\nberpandang-pandangan\nberpangkat-pangkat\nberpanjang-panjang\nberpantun-pantun\nberpasang-pasang\nberpasang-pasangan\nberpasuk-pasuk\nberpayah-payah\nberpeluh-peluh\nberpeluk-pelukan\nberpenat-penat\nberpencar-pencar\nberpendar-pendar\nberpenggal-penggal\nberperai-perai\nberperang-perangan\nberpesai-pesai\nberpesta-pesta\nberpesuk-pesuk\nberpetak-petak\nberpeti-peti\nberpihak-pihak\nberpijar-pijar\nberpikir-pikir\nberpikul-pikul\nberpilih-pilih\nberpilin-pilin\nberpindah-pindah\nberpintal-pintal\nberpirau-pirau\nberpisah-pisah\nberpolah-polah\nberpolok-polok\nberpongah-pongah\nberpontang-panting\nberporah-porah\nberpotong-potong\nberpotong-potongan\nberpuak-puak\nberpual-pual\nberpugak-pugak\nberpuing-puing\nberpukas-pukas\nberpuluh-puluh\nberpulun-pulun\nberpuntal-puntal\nberpura-pura\nberpusar-pusar\nberpusing-pusing\nberpusu-pusu\nberputar-putar\nberrumpun-rumpun\nbersaf-saf\nbersahut-sahutan\nbersakit-sakit\nbersalah-salahan\nbersalam-salaman\nbersalin-salin\nbersalip-salipan\nbersama-sama\nbersambar-sambaran\nbersambut-sambutan\nbersampan-sampan\nbersantai-santai\nbersapa-sapaan\nbersarang-sarang\nbersedan-sedan\nbersedia-sedia\nbersedu-sedu\nbersejuk-sejuk\nbersekat-sekat\nberselang-selang\nberselang-seli\nberselang-seling\nberselang-tenggang\nberselit-selit\nberseluk-beluk\nbersembunyi-sembunyi\nbersembunyi-sembunyian\nbersembur-semburan\nbersempit-sempit\nbersenang-senang\nbersenang-senangkan\nbersenda-senda\nbersendi-sendi\nbersenggang-senggang\nbersenggau-senggau\nbersepah-sepah\nbersepak-sepakan\nbersepi-sepi\nberserak-serak\nberseri-seri\nberseru-seru\nbersesak-sesak\nbersetai-setai\nbersia-sia\nbersiap-siap\nbersiar-siar\nbersih-bersih\nbersikut-sikutan\nbersilir-silir\nbersimbur-simburan\nbersinau-sinau\nbersopan-sopan\nbersorak-sorai\nbersuap-suapan\nbersudah-sudah\nbersuka-suka\nbersuka-sukaan\nbersuku-suku\nbersulang-sulang\nbersumpah-sumpahan\nbersungguh-sungguh\nbersungut-sungut\nbersunyi-sunyi\nbersuruk-surukan\nbersusah-susah\nbersusuk-susuk\nbersusuk-susukan\nbersutan-sutan\nbertabur-tabur\nbertahan-tahan\nbertahu-tahu\nbertahun-tahun\nbertajuk-tajuk\nbertakik-takik\nbertala-tala\nbertalah-talah\nbertali-tali\nbertalu-talu\nbertalun-talun\nbertambah-tambah\nbertanda-tandaan\nbertangis-tangisan\nbertangkil-tangkil\nbertanya-tanya\nbertarik-tarikan\nbertatai-tatai\nbertatap-tatapan\nbertatih-tatih\nbertawan-tawan\nbertawar-tawaran\nbertebu-tebu\nbertebu-tebukan\nberteguh-teguh\nberteguh-teguhan\nberteka-teki\nbertelang-telang\nbertelau-telau\nbertele-tele\nbertembuk-tembuk\nbertempat-tempat\nbertempuh-tempuh\nbertenang-tenang\nbertenggang-tenggangan\nbertentu-tentu\nbertepek-tepek\nberterang-terang\nberterang-terangan\nberteriak-teriak\nbertikam-tikaman\nbertimbal-timbalan\nbertimbun-timbun\nbertimpa-timpa\nbertimpas-timpas\nbertingkah-tingkah\nbertingkat-tingkat\nbertinjau-tinjauan\nbertiras-tiras\nbertitar-titar\nbertitik-titik\nbertoboh-toboh\nbertolak-tolak\nbertolak-tolakan\nbertolong-tolongan\nbertonjol-tonjol\nbertruk-truk\nbertua-tua\nbertua-tuaan\nbertual-tual\nbertubi-tubi\nbertukar-tukar\nbertukar-tukaran\nbertukas-tukas\nbertumpak-tumpak\nbertumpang-tindih\nbertumpuk-tumpuk\nbertunda-tunda\nbertunjuk-tunjukan\nbertura-tura\nberturut-turut\nbertutur-tutur\nberuas-ruas\nberubah-ubah\nberulang-alik\nberulang-ulang\nberumbai-rumbai\nberundak-undak\nberundan-undan\nberundung-undung\nberunggas-runggas\nberunggun-unggun\nberunggut-unggut\nberungkur-ungkuran\nberuntai-untai\nberuntun-runtun\nberuntung-untung\nberunyai-unyai\nberupa-rupa\nberura-ura\nberuris-uris\nberurut-urutan\nberwarna-warna\nberwarna-warni\nberwindu-windu\nberwiru-wiru\nberyang-yang\nbesar-besar\nbesar-besaran\nbetak-betak\nbeti-beti\nbetik-betik\nbetul-betul\nbiang-biang\nbiar-biar\nbiaya-biaya\nbicu-bicu\nbidadari-bidadari\nbidang-bidang\nbijak-bijaklah\nbiji-bijian\nbila-bila\nbilang-bilang\nbincang-bincang\nbincang-bincut\nbingkah-bingkah\nbini-binian\nbintang-bintang\nbintik-bintik\nbio-oil\nbiri-biri\nbiru-biru\nbiru-hitam\nbiru-kuning\nbisik-bisik\nbiti-biti\nblak-blakan\nblok-blok\nbocah-bocah\nbohong-bohong\nbohong-bohongan\nbola-bola\nbolak-balik\nbolang-baling\nboleh-boleh\nbom-bom\nbomber-bomber\nbonek-bonek\nbongkar-bangkir\nbongkar-membongkar\nbongkar-pasang\nboro-boro\nbos-bos\nbottom-up\nbox-to-box\nboyo-boyo\nbuah-buahan\nbuang-buang\nbuat-buatan\nbuaya-buaya\nbubun-bubun\nbugi-bugi\nbuild-up\nbuilt-in\nbuilt-up\nbuka-buka\nbuka-bukaan\nbuka-tutup\nbukan-bukan\nbukti-bukti\nbuku-buku\nbulan-bulan\nbulan-bulanan\nbulang-baling\nbulang-bulang\nbulat-bulat\nbuli-buli\nbulu-bulu\nbuluh-buluh\nbulus-bulus\nbunga-bunga\nbunga-bungaan\nbunuh-membunuh\nbunyi-bunyian\nbupati-bupati\nbupati-wakil\nburu-buru\nburung-burung\nburung-burungan\nbus-bus\nbusiness-to-business\nbusur-busur\nbutir-butir\nby-pass\nbye-bye\ncabang-cabang\ncabik-cabik\ncabik-mencabik\ncabup-cawabup\ncaci-maki\ncagub-cawagub\ncaing-caing\ncakar-mencakar\ncakup-mencakup\ncalak-calak\ncalar-balar\ncaleg-caleg\ncalo-calo\ncalon-calon\ncampang-camping\ncampur-campur\ncapres-cawapres\ncara-cara\ncari-cari\ncari-carian\ncarut-marut\ncatch-up\ncawali-cawawali\ncawe-cawe\ncawi-cawi\ncebar-cebur\ncelah-celah\ncelam-celum\ncelangak-celinguk\ncelas-celus\nceledang-celedok\ncelengkak-celengkok\ncelingak-celinguk\ncelung-celung\ncemas-cemas\ncenal-cenil\ncengar-cengir\ncengir-cengir\ncengis-cengis\ncengking-mengking\ncentang-perenang\ncepat-cepat\nceplas-ceplos\ncerai-berai\ncerita-cerita\nceruk-menceruk\nceruk-meruk\ncetak-biru\ncetak-mencetak\ncetar-ceter\ncheck-in\ncheck-ins\ncheck-up\nchit-chat\nchoki-choki\ncingak-cinguk\ncipika-cipiki\nciri-ciri\nciri-cirinya\ncirit-birit\ncita-cita\ncita-citaku\nclose-up\nclosed-circuit\ncoba-coba\ncobak-cabik\ncobar-cabir\ncola-cala\ncolang-caling\ncomat-comot\ncomot-comot\ncompang-camping\ncomputer-aided\ncomputer-generated\ncondong-mondong\ncongak-cangit\nconggah-canggih\ncongkah-cangkih\ncongkah-mangkih\ncopak-capik\ncopy-paste\ncorak-carik\ncorat-coret\ncoreng-moreng\ncoret-coret\ncrat-crit\ncross-border\ncross-dressing\ncrypto-ransomware\ncuang-caing\ncublak-cublak\ncubung-cubung\nculik-culik\ncuma-cuma\ncumi-cumi\ncungap-cangip\ncupu-cupu\ndabu-dabu\ndaerah-daerah\ndag-dag\ndag-dig-dug\ndaging-dagingan\ndahulu-mendahului\ndalam-dalam\ndali-dali\ndam-dam\ndanau-danau\ndansa-dansi\ndapil-dapil\ndapur-dapur\ndari-dari\ndaru-daru\ndasar-dasar\ndatang-datang\ndatang-mendatangi\ndaun-daun\ndaun-daunan\ndawai-dawai\ndayang-dayang\ndayung-mayung\ndebak-debuk\ndebu-debu\ndeca-core\ndecision-making\ndeep-lying\ndeg-degan\ndegap-degap\ndekak-dekak\ndekat-dekat\ndengar-dengaran\ndengking-mendengking\ndepartemen-departemen\ndepo-depo\ndeputi-deputi\ndesa-desa\ndesa-kota\ndesas-desus\ndetik-detik\ndewa-dewa\ndewa-dewi\ndewan-dewan\ndewi-dewi\ndial-up\ndiam-diam\ndibayang-bayangi\ndibuat-buat\ndiiming-imingi\ndilebih-lebihkan\ndimana-mana\ndimata-matai\ndinas-dinas\ndinul-Islam\ndiobok-obok\ndiolok-olok\ndireksi-direksi\ndirektorat-direktorat\ndirjen-dirjen\ndirut-dirut\nditunggu-tunggu\ndivisi-divisi\ndo-it-yourself\ndoa-doa\ndog-dog\ndoggy-style\ndokok-dokok\ndolak-dalik\ndor-doran\ndorong-mendorong\ndosa-dosa\ndress-up\ndrive-in\ndua-dua\ndua-duaan\ndua-duanya\ndubes-dubes\nduduk-duduk\ndugaan-dugaan\ndulang-dulang\nduri-duri\nduta-duta\ndwi-kewarganegaraan\ne-arena\ne-billing\ne-budgeting\ne-cctv\ne-class\ne-commerce\ne-counting\ne-elektronik\ne-entertainment\ne-evolution\ne-faktur\ne-filing\ne-fin\ne-form\ne-government\ne-govt\ne-hakcipta\ne-id\ne-info\ne-katalog\ne-ktp\ne-leadership\ne-lhkpn\ne-library\ne-loket\ne-m1\ne-money\ne-news\ne-nisn\ne-npwp\ne-paspor\ne-paten\ne-pay\ne-perda\ne-perizinan\ne-planning\ne-polisi\ne-power\ne-punten\ne-retribusi\ne-samsat\ne-sport\ne-store\ne-tax\ne-ticketing\ne-tilang\ne-toll\ne-visa\ne-voting\ne-wallet\ne-warong\necek-ecek\neco-friendly\neco-park\nedan-edanan\neditor-editor\neditor-in-chief\nefek-efek\nekonomi-ekonomi\neksekutif-legislatif\nekspor-impor\nelang-elang\nelemen-elemen\nemak-emak\nembuh-embuhan\nempat-empat\nempek-empek\nempet-empetan\nempok-empok\nempot-empotan\nenak-enak\nencal-encal\nend-to-end\nend-user\nendap-endap\nendut-endut\nendut-endutan\nengah-engah\nengap-engap\nenggan-enggan\nengkah-engkah\nengket-engket\nentah-berentah\nenten-enten\nentry-level\nequity-linked\nerang-erot\nerat-erat\nerek-erek\nereng-ereng\nerong-erong\nesek-esek\nex-officio\nexchange-traded\nexercise-induced\nextra-time\nface-down\nface-to-face\nfair-play\nfakta-fakta\nfaktor-faktor\nfakultas-fakultas\nfase-fase\nfast-food\nfeed-in\nfifty-fifty\nfile-file\nfirst-leg\nfirst-team\nfitur-fitur\nfitur-fiturnya\nfixed-income\nflip-flop\nflip-plop\nfly-in\nfollow-up\nfoto-foto\nfoya-foya\nfraksi-fraksi\nfree-to-play\nfront-end\nfungsi-fungsi\ngaba-gaba\ngabai-gabai\ngada-gada\ngading-gading\ngadis-gadis\ngado-gado\ngail-gail\ngajah-gajah\ngajah-gajahan\ngala-gala\ngaleri-galeri\ngali-gali\ngali-galian\ngaling-galing\ngalu-galu\ngamak-gamak\ngambar-gambar\ngambar-menggambar\ngamit-gamitan\ngampang-gampangan\ngana-gini\nganal-ganal\nganda-berganda\nganjal-mengganjal\nganjil-genap\nganteng-ganteng\ngantung-gantung\ngapah-gopoh\ngara-gara\ngarah-garah\ngaris-garis\ngasak-gasakan\ngatal-gatal\ngaun-gaun\ngawar-gawar\ngaya-gayanya\ngayang-gayang\nge-er\ngebyah-uyah\ngebyar-gebyar\ngedana-gedini\ngedebak-gedebuk\ngedebar-gedebur\ngedung-gedung\ngelang-gelang\ngelap-gelapan\ngelar-gelar\ngelas-gelas\ngelembung-gelembungan\ngeleng-geleng\ngeli-geli\ngeliang-geliut\ngeliat-geliut\ngembar-gembor\ngembrang-gembreng\ngempul-gempul\ngempur-menggempur\ngendang-gendang\ngengsi-gengsian\ngenjang-genjot\ngenjot-genjotan\ngenjrang-genjreng\ngenome-wide\ngeo-politik\ngerabak-gerubuk\ngerak-gerik\ngerak-geriknya\ngerakan-gerakan\ngerbas-gerbus\ngereja-gereja\ngereng-gereng\ngeriak-geriuk\ngerit-gerit\ngerot-gerot\ngeruh-gerah\ngetak-getuk\ngetem-getem\ngeti-geti\ngial-gial\ngial-giul\ngila-gila\ngila-gilaan\ngilang-gemilang\ngilap-gemilap\ngili-gili\ngiling-giling\ngilir-bergilir\nginang-ginang\ngirap-girap\ngirik-girik\ngiring-giring\ngo-auto\ngo-bills\ngo-bluebird\ngo-box\ngo-car\ngo-clean\ngo-food\ngo-glam\ngo-jek\ngo-kart\ngo-mart\ngo-massage\ngo-med\ngo-points\ngo-pulsa\ngo-ride\ngo-send\ngo-shop\ngo-tix\ngo-to-market\ngoak-goak\ngoal-line\ngol-gol\ngolak-galik\ngondas-gandes\ngonjang-ganjing\ngonjlang-ganjling\ngonta-ganti\ngontok-gontokan\ngorap-gorap\ngorong-gorong\ngotong-royong\ngresek-gresek\ngua-gua\ngual-gail\ngubernur-gubernur\ngudu-gudu\ngula-gula\ngulang-gulang\ngulung-menggulung\nguna-ganah\nguna-guna\ngundala-gundala\nguntang-guntang\ngunung-ganang\ngunung-gemunung\ngunung-gunungan\nguru-guru\nhabis-habis\nhabis-habisan\nhak-hak\nhak-hal\nhakim-hakim\nhal-hal\nhalai-balai\nhalf-time\nhama-hama\nhampir-hampir\nhancur-hancuran\nhancur-menghancurkan\nhands-free\nhands-on\nhang-out\nhantu-hantu\nhappy-happy\nharap-harap\nharap-harapan\nhard-disk\nharga-harga\nhari-hari\nharimau-harimau\nharum-haruman\nhasil-hasil\nhasta-wara\nhat-trick\nhati-hati\nhati-hatilah\nhead-mounted\nhead-to-head\nhead-up\nheads-up\nheavy-duty\nhebat-hebatan\nhewan-hewan\nhexa-core\nhidup-hidup\nhidup-mati\nhila-hila\nhilang-hilang\nhina-menghinakan\nhip-hop\nhiru-biru\nhiru-hara\nhiruk-pikuk\nhitam-putih\nhitung-hitung\nhitung-hitungan\nhormat-menghormati\nhot-swappable\nhotel-hotel\nhow-to\nhubar-habir\nhubaya-hubaya\nhukum-red\nhukuman-hukuman\nhula-hoop\nhula-hula\nhulu-hilir\nhumas-humas\nhura-hura\nhuru-hara\nibar-ibar\nibu-anak\nibu-ibu\nicak-icak\nicip-icip\nidam-idam\nide-ide\nigau-igauan\nikan-ikan\nikut-ikut\nikut-ikutan\nilam-ilam\nilat-ilatan\nilmu-ilmu\nimbang-imbangan\niming-iming\nimut-imut\ninang-inang\ninca-binca\nincang-incut\nindustri-industri\ningar-bingar\ningar-ingar\ningat-ingat\ningat-ingatan\ningau-ingauan\ninggang-inggung\ninjak-injak\ninput-output\ninstansi-instansi\ninstant-on\ninstrumen-instrumen\ninter-governmental\nira-ira\nirah-irahan\niras-iras\niring-iringan\niris-irisan\nisak-isak\nisat-bb\niseng-iseng\nistana-istana\nistri-istri\nisu-isu\niya-iya\njabatan-jabatan\njadi-jadian\njagoan-jagoan\njaja-jajaan\njaksa-jaksa\njala-jala\njalan-jalan\njali-jali\njalin-berjalin\njalin-menjalin\njam-jam\njamah-jamahan\njambak-jambakan\njambu-jambu\njampi-jampi\njanda-janda\njangan-jangan\njanji-janji\njarang-jarang\njari-jari\njaring-jaring\njarum-jarum\njasa-jasa\njatuh-bangun\njauh-dekat\njauh-jauh\njawi-jawi\njebar-jebur\njebat-jebatan\njegal-jegalan\njejak-jejak\njelang-menjelang\njelas-jelas\njelur-jelir\njembatan-jembatan\njenazah-jenazah\njendal-jendul\njenderal-jenderal\njenggar-jenggur\njenis-jenis\njenis-jenisnya\njentik-jentik\njerah-jerih\njinak-jinak\njiwa-jiwa\njoli-joli\njolong-jolong\njongkang-jangking\njongkar-jangkir\njongkat-jangkit\njor-joran\njotos-jotosan\njuak-juak\njual-beli\njuang-juang\njulo-julo\njulung-julung\njulur-julur\njumbai-jumbai\njungkang-jungkit\njungkat-jungkit\njurai-jurai\nkabang-kabang\nkabar-kabari\nkabir-kabiran\nkabruk-kabrukan\nkabu-kabu\nkabupaten-kabupaten\nkabupaten-kota\nkaca-kaca\nkacang-kacang\nkacang-kacangan\nkacau-balau\nkadang-kadang\nkader-kader\nkades-kades\nkadis-kadis\nkail-kail\nkain-kain\nkait-kait\nkakak-adik\nkakak-beradik\nkakak-kakak\nkakek-kakek\nkakek-nenek\nkaki-kaki\nkala-kala\nkalau-kalau\nkaleng-kalengan\nkali-kalian\nkalimat-kalimat\nkalung-kalung\nkalut-malut\nkambing-kambing\nkamit-kamit\nkampung-kampung\nkampus-kampus\nkanak-kanak\nkanak-kanan\nkanan-kanak\nkanan-kiri\nkangen-kangenan\nkanwil-kanwil\nkapa-kapa\nkapal-kapal\nkapan-kapan\nkapolda-kapolda\nkapolres-kapolres\nkapolsek-kapolsek\nkapu-kapu\nkarang-karangan\nkarang-mengarang\nkareseh-peseh\nkarut-marut\nkarya-karya\nkasak-kusuk\nkasus-kasus\nkata-kata\nkatang-katang\nkava-kava\nkawa-kawa\nkawan-kawan\nkawin-cerai\nkawin-mawin\nkayu-kayu\nkayu-kayuan\nke-Allah-an\nkeabu-abuan\nkearab-araban\nkeasyik-asyikan\nkebarat-baratan\nkebasah-basahan\nkebat-kebit\nkebata-bataan\nkebayi-bayian\nkebelanda-belandaan\nkeberlarut-larutan\nkebesar-hatian\nkebiasaan-kebiasaan\nkebijakan-kebijakan\nkebiru-biruan\nkebudak-budakan\nkebun-kebun\nkebut-kebutan\nkecamatan-kecamatan\nkecentang-perenangan\nkecil-kecil\nkecil-kecilan\nkecil-mengecil\nkecokelat-cokelatan\nkecomak-kecimik\nkecuh-kecah\nkedek-kedek\nkedekak-kedekik\nkedesa-desaan\nkedubes-kedubes\nkedutaan-kedutaan\nkeempat-empatnya\nkegadis-gadisan\nkegelap-gelapan\nkegiatan-kegiatan\nkegila-gilaan\nkegirang-girangan\nkehati-hatian\nkeheran-heranan\nkehijau-hijauan\nkehitam-hitaman\nkeinggris-inggrisan\nkejaga-jagaan\nkejahatan-kejahatan\nkejang-kejang\nkejar-kejar\nkejar-kejaran\nkejar-mengejar\nkejingga-jinggaan\nkejut-kejut\nkejutan-kejutan\nkekabur-kaburan\nkekanak-kanakan\nkekoboi-koboian\nkekota-kotaan\nkekuasaan-kekuasaan\nkekuning-kuningan\nkelak-kelik\nkelak-keluk\nkelaki-lakian\nkelang-kelok\nkelap-kelip\nkelasah-kelusuh\nkelek-kelek\nkelek-kelekan\nkelemak-kelemek\nkelik-kelik\nkelip-kelip\nkelompok-kelompok\nkelontang-kelantung\nkeluar-masuk\nkelurahan-kelurahan\nkelusuh-kelasah\nkelut-melut\nkemak-kemik\nkemalu-maluan\nkemana-mana\nkemanja-manjaan\nkemarah-marahan\nkemasam-masaman\nkemati-matian\nkembang-kembang\nkemenpan-rb\nkementerian-kementerian\nkemerah-merahan\nkempang-kempis\nkempas-kempis\nkemuda-mudaan\nkena-mengena\nkenal-mengenal\nkenang-kenangan\nkencang-kencung\nkencing-mengencingi\nkencrang-kencring\nkendang-kendang\nkendang-kendangan\nkeningrat-ningratan\nkentung-kentung\nkenyat-kenyit\nkepala-kepala\nkepala-kepalaan\nkepandir-pandiran\nkepang-kepot\nkeperak-perakan\nkepetah-lidahan\nkepilu-piluan\nkeping-keping\nkepucat-pucatan\nkepuh-kepuh\nkepura-puraan\nkeputih-putihan\nkerah-kerahan\nkerancak-rancakan\nkerang-kerangan\nkerang-keroh\nkerang-kerot\nkerang-keruk\nkerang-kerung\nkerap-kerap\nkeras-mengerasi\nkercap-kercip\nkercap-kercup\nkeriang-keriut\nkerja-kerja\nkernyat-kernyut\nkerobak-kerabit\nkerobak-kerobek\nkerobak-kerobik\nkerobat-kerabit\nkerong-kerong\nkeropas-kerapis\nkertak-kertuk\nkertap-kertap\nkeruntang-pungkang\nkesalahan-kesalahan\nkesap-kesip\nkesemena-menaan\nkesenak-senakan\nkesewenang-wenangan\nkesia-siaan\nkesik-kesik\nkesipu-sipuan\nkesu-kesi\nkesuh-kesih\nkesuk-kesik\nketakar-keteker\nketakutan-ketakutan\nketap-ketap\nketap-ketip\nketar-ketir\nketentuan-ketentuan\nketergesa-gesaan\nketi-keti\nketidur-tiduran\nketiga-tiganya\nketir-ketir\nketua-ketua\nketua-tuaan\nketuan-tuanan\nkeungu-unguan\nkewangi-wangian\nki-ka\nkia-kia\nkiai-kiai\nkiak-kiak\nkial-kial\nkiang-kiut\nkiat-kiat\nkibang-kibut\nkicang-kecoh\nkicang-kicu\nkick-off\nkida-kida\nkijang-kijang\nkilau-mengilau\nkili-kili\nkilik-kilik\nkincir-kincir\nkios-kios\nkira-kira\nkira-kiraan\nkiri-kanan\nkirim-berkirim\nkisah-kisah\nkisi-kisi\nkitab-kitab\nkitang-kitang\nkiu-kiu\nklaim-klaim\nklik-klikan\nklip-klip\nklub-klub\nkluntang-klantung\nknock-knock\nknock-on\nknock-out\nko-as\nko-pilot\nkoak-koak\nkoboi-koboian\nkocah-kacih\nkocar-kacir\nkodam-kodam\nkode-kode\nkodim-kodim\nkodok-kodok\nkolang-kaling\nkole-kole\nkoleh-koleh\nkolong-kolong\nkoma-koma\nkomat-kamit\nkomisaris-komisaris\nkomisi-komisi\nkomite-komite\nkomoditas-komoditas\nkongko-kongko\nkonsulat-konsulat\nkonsultan-konsultan\nkontal-kantil\nkontang-kanting\nkontra-terorisme\nkontrak-kontrak\nkonvensi-konvensi\nkopat-kapit\nkoperasi-koperasi\nkopi-kopi\nkoran-koran\nkoreng-koreng\nkos-kosan\nkosak-kasik\nkota-kota\nkota-wakil\nkotak-katik\nkotak-kotak\nkoyak-koyak\nkuas-kuas\nkuat-kuat\nkubu-kubuan\nkucar-kacir\nkucing-kucing\nkucing-kucingan\nkuda-kuda\nkuda-kudaan\nkudap-kudap\nkue-kue\nkulah-kulah\nkulak-kulak\nkulik-kulik\nkulum-kulum\nkumat-kamit\nkumpul-kumpul\nkunang-kunang\nkunar-kunar\nkung-fu\nkuning-hitam\nkupat-kapit\nkupu-kupu\nkura-kura\nkurang-kurang\nkusat-mesat\nkutat-kutet\nkuti-kuti\nkuwung-kuwung\nkyai-kyai\nlaba-laba\nlabi-labi\nlabu-labu\nlaga-laga\nlagi-lagi\nlagu-lagu\nlaguh-lagah\nlain-lain\nlaki-laki\nlalu-lalang\nlalu-lintas\nlama-kelamaan\nlama-lama\nlamat-lamat\nlambat-lambat\nlampion-lampion\nlampu-lampu\nlancang-lancang\nlancar-lancar\nlangak-longok\nlanggar-melanggar\nlangit-langit\nlangkah-langka\nlangkah-langkah\nlanja-lanjaan\nlapas-lapas\nlapat-lapat\nlaporan-laporan\nlaptop-tablet\nlarge-scale\nlari-lari\nlari-larian\nlaskar-laskar\nlauk-pauk\nlaun-laun\nlaut-timur\nlawah-lawah\nlawak-lawak\nlawan-lawan\nlawi-lawi\nlayang-layang\nlayu-layuan\nlebih-lebih\nlecet-lecet\nlegak-legok\nlegum-legum\nlegup-legup\nleha-leha\nlekak-lekuk\nlekap-lekup\nlekas-lekas\nlekat-lekat\nlekuh-lekih\nlekum-lekum\nlekup-lekap\nlembaga-lembaga\nlempar-lemparan\nlenggak-lenggok\nlenggok-lenggok\nlenggut-lenggut\nlengket-lengket\nlentam-lentum\nlentang-lentok\nlentang-lentung\nlepa-lepa\nlerang-lerang\nlereng-lereng\nlese-majeste\nletah-letai\nlete-lete\nletuk-letuk\nletum-letum\nletup-letup\nleyeh-leyeh\nliang-liuk\nliang-liut\nliar-liar\nliat-liut\nlidah-lidah\nlife-toxins\nliga-liga\nlight-emitting\nlika-liku\nlil-alamin\nlilin-lilin\nline-up\nlintas-selat\nlipat-melipat\nliquid-cooled\nlithium-ion\nlithium-polymer\nliuk-liuk\nliung-liung\nlobi-lobi\nlock-up\nlocked-in\nlokasi-lokasi\nlong-term\nlongak-longok\nlontang-lanting\nlontang-lantung\nlopak-lapik\nlopak-lopak\nlow-cost\nlow-density\nlow-end\nlow-light\nlow-multi\nlow-pass\nlucu-lucu\nluka-luka\nlukisan-lukisan\nlumba-lumba\nlumi-lumi\nluntang-lantung\nlupa-lupa\nlupa-lupaan\nlurah-camat\nmaaf-memaafkan\nmabuk-mabukan\nmabul-mabul\nmacam-macam\nmacan-macanan\nmachine-to-machine\nmafia-mafia\nmahasiswa-mahasiswi\nmahasiswa/i\nmahi-mahi\nmain-main\nmain-mainan\nmain-mainlah\nmajelis-majelis\nmaju-mundur\nmakam-makam\nmakan-makan\nmakan-makanan\nmakanan-red\nmake-up\nmaki-maki\nmaki-makian\nmal-mal\nmalai-malai\nmalam-malam\nmalar-malar\nmalas-malasan\nmali-mali\nmalu-malu\nmama-mama\nman-in-the-middle\nmana-mana\nmanajer-manajer\nmanik-manik\nmanis-manis\nmanis-manisan\nmarah-marah\nmark-up\nmas-mas\nmasa-masa\nmasak-masak\nmasalah-masalah\nmash-up\nmasing-masing\nmasjid-masjid\nmasuk-keluar\nmat-matan\nmata-mata\nmatch-fixing\nmati-mati\nmati-matian\nmaya-maya\nmayat-mayat\nmayday-mayday\nmedia-media\nmega-bintang\nmega-tsunami\nmegal-megol\nmegap-megap\nmeger-meger\nmegrek-megrek\nmelak-melak\nmelambai-lambai\nmelambai-lambaikan\nmelambat-lambatkan\nmelaun-laun\nmelawak-lawak\nmelayang-layang\nmelayap-layap\nmelayap-layapkan\nmelebih-lebihi\nmelebih-lebihkan\nmelejang-lejangkan\nmelek-melekan\nmeleleh-leleh\nmelengah-lengah\nmelihat-lihat\nmelimpah-limpah\nmelincah-lincah\nmeliuk-liuk\nmelolong-lolong\nmelompat-lompat\nmeloncat-loncat\nmelonco-lonco\nmelongak-longok\nmelonjak-lonjak\nmemacak-macak\nmemada-madai\nmemadan-madan\nmemaki-maki\nmemaksa-maksa\nmemanas-manasi\nmemancit-mancitkan\nmemandai-mandai\nmemanggil-manggil\nmemanis-manis\nmemanjut-manjut\nmemantas-mantas\nmemasak-masak\nmemata-matai\nmematah-matah\nmematuk-matuk\nmematut-matut\nmemau-mau\nmemayah-mayahkan\nmembaca-baca\nmembacah-bacah\nmembagi-bagikan\nmembalik-balik\nmembangkit-bangkit\nmembarut-barut\nmembawa-bawa\nmembayang-bayangi\nmembayang-bayangkan\nmembeda-bedakan\nmembelai-belai\nmembeli-beli\nmembelit-belitkan\nmembelu-belai\nmembenar-benar\nmembenar-benari\nmemberai-beraikan\nmembesar-besar\nmembesar-besarkan\nmembikin-bikin\nmembilah-bilah\nmembolak-balikkan\nmembongkar-bangkir\nmembongkar-bongkar\nmembuang-buang\nmembuat-buat\nmembulan-bulani\nmembunga-bungai\nmembungkuk-bungkuk\nmemburu-buru\nmemburu-burukan\nmemburuk-burukkan\nmemelintir-melintir\nmemencak-mencak\nmemencar-mencar\nmemercik-mercik\nmemetak-metak\nmemetang-metangkan\nmemetir-metir\nmemijar-mijar\nmemikir-mikir\nmemikir-mikirkan\nmemilih-milih\nmemilin-milin\nmeminang-minang\nmeminta-minta\nmemisah-misahkan\nmemontang-mantingkan\nmemorak-perandakan\nmemorak-porandakan\nmemotong-motong\nmemperamat-amat\nmemperamat-amatkan\nmemperbagai-bagaikan\nmemperganda-gandakan\nmemperganduh-ganduhkan\nmemperimpit-impitkan\nmemperkuda-kudakan\nmemperlengah-lengah\nmemperlengah-lengahkan\nmempermacam-macamkan\nmemperolok-olok\nmemperolok-olokkan\nmempersama-samakan\nmempertubi-tubi\nmempertubi-tubikan\nmemperturut-turutkan\nmemuja-muja\nmemukang-mukang\nmemulun-mulun\nmemundi-mundi\nmemundi-mundikan\nmemutar-mutar\nmemuyu-muyu\nmen-tweet\nmenagak-nagak\nmenakut-nakuti\nmenang-kalah\nmenanjur-nanjur\nmenanti-nanti\nmenari-nari\nmencabik-cabik\nmencabik-cabikkan\nmencacah-cacah\nmencaing-caing\nmencak-mencak\nmencakup-cakup\nmencapak-capak\nmencari-cari\nmencarik-carik\nmencarik-carikkan\nmencarut-carut\nmencengis-cengis\nmencepak-cepak\nmencepuk-cepuk\nmencerai-beraikan\nmencetai-cetai\nmenciak-ciak\nmenciap-ciap\nmenciar-ciar\nmencita-citakan\nmencium-cium\nmenciut-ciut\nmencla-mencle\nmencoang-coang\nmencoba-coba\nmencocok-cocok\nmencolek-colek\nmenconteng-conteng\nmencubit-cubit\nmencucuh-cucuh\nmencucuh-cucuhkan\nmencuri-curi\nmendecap-decap\nmendegam-degam\nmendengar-dengar\nmendengking-dengking\nmendengus-dengus\nmendengut-dengut\nmenderai-deraikan\nmenderak-derakkan\nmenderau-derau\nmenderu-deru\nmendesas-desuskan\nmendesus-desus\nmendetap-detap\nmendewa-dewakan\nmendudu-dudu\nmenduga-duga\nmenebu-nebu\nmenegur-neguri\nmenepak-nepak\nmenepak-nepakkan\nmengabung-ngabung\nmengaci-acikan\nmengacu-acu\nmengada-ada\nmengada-ngada\nmengadang-adangi\nmengaduk-aduk\nmengagak-agak\nmengagak-agihkan\nmengagut-agut\nmengais-ngais\nmengalang-alangi\nmengali-ali\nmengalur-alur\nmengamang-amang\nmengamat-amati\nmengambai-ambaikan\nmengambang-ambang\nmengambung-ambung\nmengambung-ambungkan\nmengamit-ngamitkan\nmengancai-ancaikan\nmengancak-ancak\nmengancar-ancar\nmengangan-angan\nmengangan-angankan\nmengangguk-angguk\nmenganggut-anggut\nmengangin-anginkan\nmengangkat-angkat\nmenganjung-anjung\nmenganjung-anjungkan\nmengap-mengap\nmengapa-apai\nmengapi-apikan\nmengarah-arahi\nmengarang-ngarang\nmengata-ngatai\nmengatup-ngatupkan\nmengaum-aum\nmengaum-aumkan\nmengejan-ejan\nmengejar-ngejar\nmengejut-ngejuti\nmengelai-ngelai\nmengelepik-ngelepik\nmengelip-ngelip\nmengelu-elukan\nmengelus-elus\nmengembut-embut\nmengempas-empaskan\nmengenap-enapkan\nmengendap-endap\nmengenjak-enjak\nmengentak-entak\nmengentak-entakkan\nmengepak-ngepak\nmengepak-ngepakkan\nmengepal-ngepalkan\nmengerjap-ngerjap\nmengerling-ngerling\nmengertak-ngertakkan\nmengesot-esot\nmenggaba-gabai\nmenggali-gali\nmenggalur-galur\nmenggamak-gamak\nmenggamit-gamitkan\nmenggapai-gapai\nmenggapai-gapaikan\nmenggaruk-garuk\nmenggebu-gebu\nmenggebyah-uyah\nmenggeleng-gelengkan\nmenggelepar-gelepar\nmenggelepar-geleparkan\nmenggeliang-geliutkan\nmenggelinding-gelinding\nmenggemak-gemak\nmenggembar-gemborkan\nmenggerak-gerakkan\nmenggerecak-gerecak\nmenggesa-gesakan\nmenggili-gili\nmenggodot-godot\nmenggolak-galikkan\nmenggorek-gorek\nmenggoreng-goreng\nmenggosok-gosok\nmenggoyang-goyangkan\nmengguit-guit\nmenghalai-balaikan\nmenghalang-halangi\nmenghambur-hamburkan\nmenghinap-hinap\nmenghitam-memutihkan\nmenghitung-hitung\nmenghubung-hubungkan\nmenghujan-hujankan\nmengiang-ngiang\nmengibar-ngibarkan\nmengibas-ngibas\nmengibas-ngibaskan\nmengidam-idamkan\nmengilah-ngilahkan\nmengilai-ilai\nmengilat-ngilatkan\nmengilik-ngilik\nmengimak-imak\nmengimbak-imbak\nmengiming-iming\nmengincrit-incrit\nmengingat-ingat\nmenginjak-injak\nmengipas-ngipas\nmengira-ngira\nmengira-ngirakan\nmengiras-iras\nmengiras-irasi\nmengiris-iris\nmengitar-ngitar\nmengitik-ngitik\nmengodol-odol\nmengogok-ogok\nmengolak-alik\nmengolak-alikkan\nmengolang-aling\nmengolang-alingkan\nmengoleng-oleng\nmengolok-olok\nmengombang-ambing\nmengombang-ambingkan\nmengongkang-ongkang\nmengongkok-ongkok\nmengonyah-anyih\nmengopak-apik\nmengorak-arik\nmengorat-oret\nmengorek-ngorek\nmengoret-oret\nmengorok-orok\nmengotak-atik\nmengotak-ngatikkan\nmengotak-ngotakkan\nmengoyak-ngoyak\nmengoyak-ngoyakkan\nmengoyak-oyak\nmenguar-nguarkan\nmenguar-uarkan\nmengubah-ubah\nmengubek-ubek\nmenguber-uber\nmengubit-ubit\nmengubrak-abrik\nmengucar-ngacirkan\nmengucek-ngucek\nmengucek-ucek\nmenguik-uik\nmenguis-uis\nmengulang-ulang\nmengulas-ulas\nmengulit-ulit\nmengulum-ngulum\nmengulur-ulur\nmenguman-uman\nmengumbang-ambingkan\nmengumpak-umpak\nmengungkat-ungkat\nmengungkit-ungkit\nmengupa-upa\nmengurik-urik\nmengusil-usil\nmengusil-usilkan\nmengutak-atik\nmengutak-ngatikkan\nmengutik-ngutik\nmengutik-utik\nmenika-nika\nmenimang-nimang\nmenimbang-nimbang\nmenimbun-nimbun\nmenimpang-nimpangkan\nmeningkat-ningkat\nmeniru-niru\nmenit-menit\nmenitar-nitarkan\nmeniup-niup\nmenjadi-jadi\nmenjadi-jadikan\nmenjedot-jedotkan\nmenjelek-jelekkan\nmenjengek-jengek\nmenjengit-jengit\nmenjerit-jerit\nmenjilat-jilat\nmenjungkat-jungkit\nmenko-menko\nmenlu-menlu\nmenonjol-nonjolkan\nmentah-mentah\nmentang-mentang\nmenteri-menteri\nmentul-mentul\nmenuding-nuding\nmenumpah-numpahkan\nmenunda-nunda\nmenunduk-nunduk\nmenusuk-nusuk\nmenyala-nyala\nmenyama-nyama\nmenyama-nyamai\nmenyambar-nyambar\nmenyangkut-nyangkutkan\nmenyanjung-nyanjung\nmenyanjung-nyanjungkan\nmenyapu-nyapu\nmenyarat-nyarat\nmenyayat-nyayat\nmenyedang-nyedang\nmenyedang-nyedangkan\nmenyelang-nyelangkan\nmenyelang-nyeling\nmenyelang-nyelingkan\nmenyenak-nyenak\nmenyendi-nyendi\nmenyentak-nyentak\nmenyentuh-nyentuh\nmenyepak-nyepakkan\nmenyerak-nyerakkan\nmenyeret-nyeret\nmenyeru-nyerukan\nmenyetel-nyetel\nmenyia-nyiakan\nmenyibak-nyibak\nmenyobek-nyobek\nmenyorong-nyorongkan\nmenyungguh-nyungguhi\nmenyuruk-nyuruk\nmeraba-raba\nmerah-hitam\nmerah-merah\nmerambang-rambang\nmerangkak-rangkak\nmerasa-rasai\nmerata-ratakan\nmeraung-raung\nmeraung-raungkan\nmerayau-rayau\nmerayu-rayu\nmercak-mercik\nmercedes-benz\nmerek-merek\nmereka-mereka\nmereka-reka\nmerelap-relap\nmerem-merem\nmeremah-remah\nmeremas-remas\nmeremeh-temehkan\nmerempah-rempah\nmerempah-rempahi\nmerengek-rengek\nmerengeng-rengeng\nmerenik-renik\nmerenta-renta\nmerenyai-renyai\nmeresek-resek\nmerintang-rintang\nmerintik-rintik\nmerobek-robek\nmeronta-ronta\nmeruap-ruap\nmerubu-rubu\nmerungus-rungus\nmerungut-rungut\nmeta-analysis\nmetode-metode\nmewanti-wanti\nmewarna-warnikan\nmeyakin-yakini\nmid-range\nmid-size\nmiju-miju\nmikro-kecil\nmimpi-mimpi\nminggu-minggu\nminta-minta\nminuman-minuman\nmixed-use\nmobil-mobil\nmobile-first\nmobile-friendly\nmoga-moga\nmola-mola\nmomen-momen\nmondar-mandir\nmonyet-monyet\nmorak-marik\nmorat-marit\nmove-on\nmuda-muda\nmuda-mudi\nmuda/i\nmudah-mudahan\nmuka-muka\nmula-mula\nmultiple-output\nmuluk-muluk\nmulut-mulutan\nmumi-mumi\nmundur-mundur\nmuntah-muntah\nmurid-muridnya\nmusda-musda\nmuseum-museum\nmuslim-muslimah\nmusuh-musuh\nmusuh-musuhnya\nnabi-nabi\nnada-nadanya\nnaga-naga\nnaga-naganya\nnaik-naik\nnaik-turun\nnakal-nakalan\nnama-nama\nnanti-nantian\nnanya-nanya\nnasi-nasi\nnasib-nasiban\nnear-field\nnegara-negara\nnegera-negara\nnegeri-negeri\nnegeri-red\nneka-neka\nnekat-nekat\nneko-neko\nnenek-nenek\nneo-liberalisme\nnext-gen\nnext-generation\nngeang-ngeang\nngeri-ngeri\nnggak-nggak\nngobrol-ngobrol\nngumpul-ngumpul\nnilai-nilai\nnine-dash\nnipa-nipa\nnong-nong\nnorma-norma\nnovel-novel\nnyai-nyai\nnyolong-nyolong\nnyut-nyutan\nob-gyn\nobat-obat\nobat-obatan\nobjek-objek\nobok-obok\nobrak-abrik\nocta-core\nodong-odong\noedipus-kompleks\noff-road\nogah-agih\nogah-ogah\nogah-ogahan\nogak-agik\nogak-ogak\nogoh-ogoh\nolak-alik\nolak-olak\nolang-aling\nolang-alingan\nole-ole\noleh-oleh\nolok-olok\nolok-olokan\nolong-olong\nom-om\nombang-ambing\nomni-channel\non-board\non-demand\non-fire\non-line\non-off\non-premises\non-roll\non-screen\non-the-go\nonde-onde\nondel-ondel\nondos-ondos\none-click\none-to-one\none-touch\none-two\noneng-oneng\nongkang-ongkang\nongol-ongol\nonline-to-offline\nontran-ontran\nonyah-anyih\nonyak-anyik\nopak-apik\nopsi-opsi\nopt-in\norak-arik\norang-aring\norang-orang\norang-orangan\norat-oret\norganisasi-organisasi\normas-ormas\norok-orok\norong-orong\noseng-oseng\notak-atik\notak-otak\notak-otakan\nover-heating\nover-the-air\nover-the-top\npa-pa\npabrik-pabrik\npadi-padian\npagi-pagi\npagi-sore\npajak-pajak\npaket-paket\npalas-palas\npalato-alveolar\npaling-paling\npalu-arit\npalu-memalu\npanas-dingin\npanas-panas\npandai-pandai\npandang-memandang\npanel-panel\npangeran-pangeran\npanggung-panggung\npangkalan-pangkalan\npanja-panja\npanji-panji\npansus-pansus\npantai-pantai\npao-pao\npara-para\nparang-parang\nparpol-parpol\npartai-partai\nparu-paru\npas-pasan\npasal-pasal\npasang-memasang\npasang-surut\npasar-pasar\npasu-pasu\npaus-paus\npaut-memaut\npay-per-click\npaya-paya\npdi-p\npecah-pecah\npecat-pecatan\npeer-to-peer\npejabat-pejabat\npekak-pekak\npekik-pekuk\npelabuhan-pelabuhan\npelacur-pelacur\npelajar-pelajar\npelan-pelan\npelangi-pelangi\npem-bully\npemain-pemain\npemata-mataan\npemda-pemda\npemeluk-pemeluknya\npemerintah-pemerintah\npemerintah-red\npemerintah-swasta\npemetang-metangan\npemilu-pemilu\npemimpin-pemimpin\npeminta-minta\npemuda-pemuda\npemuda-pemudi\npenanggung-jawab\npengali-ali\npengaturan-pengaturan\npenggembar-gemboran\npengorak-arik\npengotak-ngotakan\npengundang-undang\npengusaha-pengusaha\npentung-pentungan\npenyakit-penyakit\nperak-perak\nperang-perangan\nperas-perus\nperaturan-peraturan\nperda-perda\nperempat-final\nperempuan-perempuan\npergi-pergi\npergi-pulang\nperintang-rintang\nperkereta-apian\nperlahan-lahan\nperlip-perlipan\npermen-permen\npernak-pernik\npernik-pernik\npertama-tama\npertandingan-pertandingan\npertimbangan-pertimbangan\nperudang-undangan\nperundang-undangan\nperundangan-undangan\nperusahaan-perusahaan\nperusahaan-perusahan\nperwakilan-perwakilan\npesan-pesan\npesawat-pesawat\npeta-jalan\npetang-petang\npetantang-petenteng\npetatang-peteteng\npete-pete\npiala-piala\npiat-piut\npick-up\npicture-in-picture\npihak-pihak\npijak-pijak\npijar-pijar\npijat-pijat\npikir-pikir\npil-pil\npilah-pilih\npilih-pilih\npilihan-pilihan\npilin-memilin\npilkada-pilkada\npina-pina\npindah-pindah\nping-pong\npinjam-meminjam\npintar-pintarlah\npisang-pisang\npistol-pistolan\npiting-memiting\nplanet-planet\nplay-off\nplin-plan\nplintat-plintut\nplonga-plongo\nplug-in\nplus-minus\nplus-plus\npoco-poco\npohon-pohonan\npoin-poin\npoint-of-sale\npoint-of-sales\npokemon-pokemon\npokja-pokja\npokok-pokok\npokrol-pokrolan\npolang-paling\npolda-polda\npoleng-poleng\npolong-polongan\npolres-polres\npolsek-polsek\npolwan-polwan\npoma-poma\npondok-pondok\nponpes-ponpes\npontang-panting\npop-up\nporak-parik\nporak-peranda\nporak-poranda\npos-pos\nposko-posko\npotong-memotong\npraktek-praktek\npraktik-praktik\nproduk-produk\nprogram-program\npromosi-degradasi\nprovinsi-provinsi\nproyek-proyek\npuing-puing\npuisi-puisi\npuji-pujian\npukang-pukang\npukul-memukul\npulang-pergi\npulau-pulai\npulau-pulau\npull-up\npulut-pulut\npundi-pundi\npungak-pinguk\npunggung-memunggung\npura-pura\npuruk-parak\npusar-pusar\npusat-pusat\npush-to-talk\npush-up\npush-ups\npusing-pusing\npuskesmas-puskesmas\nputar-putar\nputera-puteri\nputih-hitam\nputih-putih\nputra-putra\nputra-putri\nputra/i\nputri-putri\nputus-putus\nputusan-putusan\npuvi-puvi\nquad-core\nraba-rabaan\nraba-rubu\nrada-rada\nradio-frequency\nragu-ragu\nrahasia-rahasiaan\nraja-raja\nrama-rama\nramai-ramai\nramalan-ramalan\nrambeh-rambeh\nrambu-rambu\nrame-rame\nramu-ramuan\nranda-rondo\nrangkul-merangkul\nrango-rango\nrap-rap\nrasa-rasanya\nrata-rata\nraun-raun\nread-only\nreal-life\nreal-time\nrebah-rebah\nrebah-rebahan\nrebas-rebas\nred-eye\nredam-redam\nredep-redup\nrehab-rekon\nreja-reja\nreka-reka\nreka-rekaan\nrekan-rekan\nrekan-rekannya\nrekor-rekor\nrelief-relief\nremah-remah\nremang-remang\nrembah-rembah\nrembah-rembih\nremeh-cemeh\nremeh-temeh\nrempah-rempah\nrencana-rencana\nrenyai-renyai\nrep-repan\nrepot-repot\nrepuh-repuh\nrestoran-restoran\nretak-retak\nriang-riang\nribu-ribu\nribut-ribut\nrica-rica\nride-sharing\nrigi-rigi\nrinai-rinai\nrintik-rintik\nritual-ritual\nrobak-rabik\nrobat-rabit\nrobot-robot\nrole-play\nrole-playing\nroll-on\nrombang-rambing\nromol-romol\nrompang-romping\nrondah-rondih\nropak-rapik\nroyal-royalan\nroyo-royo\nruak-ruak\nruba-ruba\nrudal-rudal\nruji-ruji\nruku-ruku\nrumah-rumah\nrumah-rumahan\nrumbai-rumbai\nrumput-rumputan\nrunding-merunding\nrundu-rundu\nrunggu-rangga\nrunner-up\nruntang-runtung\nrupa-rupa\nrupa-rupanya\nrusun-rusun\nrute-rute\nsaat-saat\nsaban-saban\nsabu-sabu\nsabung-menyabung\nsah-sah\nsahabat-sahabat\nsaham-saham\nsahut-menyahut\nsaing-menyaing\nsaji-sajian\nsakit-sakitan\nsaksi-saksi\nsaku-saku\nsalah-salah\nsama-sama\nsamar-samar\nsambar-menyambar\nsambung-bersambung\nsambung-menyambung\nsambut-menyambut\nsamo-samo\nsampah-sampah\nsampai-sampai\nsamping-menyamping\nsana-sini\nsandar-menyandar\nsandi-sandi\nsangat-sangat\nsangkut-menyangkut\nsapa-menyapa\nsapai-sapai\nsapi-sapi\nsapu-sapu\nsaran-saran\nsarana-prasarana\nsari-sari\nsarit-sarit\nsatu-dua\nsatu-satu\nsatu-satunya\nsatuan-satuan\nsaudara-saudara\nsauk-menyauk\nsauk-sauk\nsayang-sayang\nsayap-sayap\nsayup-menyayup\nsayup-sayup\nsayur-mayur\nsayur-sayuran\nsci-fi\nseagak-agak\nseakal-akal\nseakan-akan\nsealak-alak\nseari-arian\nsebaik-baiknya\nsebelah-menyebelah\nsebentar-sebentar\nseberang-menyeberang\nseberuntung-beruntungnya\nsebesar-besarnya\nseboleh-bolehnya\nsedalam-dalamnya\nsedam-sedam\nsedang-menyedang\nsedang-sedang\nsedap-sedapan\nsedapat-dapatnya\nsedikit-dikitnya\nsedikit-sedikit\nsedikit-sedikitnya\nsedini-dininya\nseelok-eloknya\nsegala-galanya\nsegan-menyegan\nsegan-menyegani\nsegan-segan\nsehabis-habisnya\nsehari-hari\nsehari-harian\nsehari-harinya\nsejadi-jadinya\nsekali-kali\nsekali-sekali\nsekenyang-kenyangnya\nsekira-kira\nsekolah-sekolah\nsekonyong-konyong\nsekosong-kosongnya\nsektor-sektor\nsekuasa-kuasanya\nsekuat-kuatnya\nsekurang-kurangnya\nsel-sel\nsela-menyela\nsela-sela\nselak-seluk\nselama-lamanya\nselambat-lambatnya\nselang-seli\nselang-seling\nselar-belar\nselat-latnya\nselatan-tenggara\nselekas-lekasnya\nselentang-selenting\nselepas-lepas\nself-driving\nself-esteem\nself-healing\nself-help\nselir-menyelir\nseloyong-seloyong\nseluk-beluk\nseluk-semeluk\nsema-sema\nsemah-semah\nsemak-semak\nsemaksimal-maksimalnya\nsemalam-malaman\nsemang-semang\nsemanis-manisnya\nsemasa-masa\nsemata-mata\nsemau-maunya\nsembunyi-sembunyi\nsembunyi-sembunyian\nsembur-sembur\nsemena-mena\nsemenda-menyemenda\nsemengga-mengga\nsemenggah-menggah\nsementang-mentang\nsemerdeka-merdekanya\nsemi-final\nsemi-permanen\nsempat-sempatnya\nsemu-semu\nsemua-muanya\nsemujur-mujurnya\nsemut-semutan\nsen-senan\nsendiri-sendiri\nsengal-sengal\nsengar-sengir\nsengau-sengauan\nsenggak-sengguk\nsenggang-tenggang\nsenggol-menyenggol\nsenior-junior\nsenjata-senjata\nsenyum-senyum\nseolah-olah\nsepala-pala\nsepandai-pandai\nsepetang-petangan\nsepoi-sepoi\nsepraktis-praktisnya\nsepuas-puasnya\nserak-serak\nserak-serik\nserang-menyerang\nserang-serangan\nserangan-serangan\nseraya-menyeraya\nserba-serbi\nserbah-serbih\nserembah-serembih\nserigala-serigala\nsering-sering\nserobot-serobotan\nserong-menyerong\nserta-menyertai\nserta-merta\nserta-serta\nseru-seruan\nservice-oriented\nsesak-menyesak\nsesal-menyesali\nsesayup-sayup\nsesi-sesi\nsesuang-suang\nsesudah-sudah\nsesudah-sudahnya\nsesuka-suka\nsesuka-sukanya\nset-piece\nsetempat-setempat\nsetengah-setengah\nsetidak-tidaknya\nsetinggi-tingginya\nseupaya-upaya\nseupaya-upayanya\nsewa-menyewa\nsewaktu-waktu\nsewenang-wenang\nsewot-sewotan\nshabu-shabu\nshort-term\nshort-throw\nsia-sia\nsiang-siang\nsiap-siap\nsiapa-siapa\nsibar-sibar\nsibur-sibur\nsida-sida\nside-by-side\nsign-in\nsiku-siku\nsikut-sikutan\nsilah-silah\nsilang-menyilang\nsilir-semilir\nsimbol-simbol\nsimpan-pinjam\nsinar-menyinar\nsinar-seminar\nsinar-suminar\nsindir-menyindir\nsinga-singa\nsinggah-menyinggah\nsingle-core\nsipil-militer\nsir-siran\nsirat-sirat\nsisa-sisa\nsisi-sisi\nsiswa-siswa\nsiswa-siswi\nsiswa/i\nsiswi-siswi\nsitu-situ\nsitus-situs\nsix-core\nsix-speed\nslintat-slintut\nslo-mo\nslow-motion\nsnap-on\nsobek-sobekan\nsodok-sodokan\nsok-sokan\nsolek-menyolek\nsolid-state\nsorak-sorai\nsorak-sorak\nsore-sore\nsosio-ekonomi\nsoya-soya\nspill-resistant\nsplit-screen\nsponsor-sponsor\nsponsor-sponsoran\nsrikandi-srikandi\nstaf-staf\nstand-by\nstand-up\nstart-up\nstasiun-stasiun\nstate-owned\nstriker-striker\nstudi-studi\nsuam-suam\nsuami-isteri\nsuami-istri\nsuami-suami\nsuang-suang\nsuara-suara\nsudin-sudin\nsudu-sudu\nsudung-sudung\nsugi-sugi\nsuka-suka\nsuku-suku\nsulang-menyulang\nsulat-sulit\nsulur-suluran\nsum-sum\nsumber-sumber\nsumpah-sumpah\nsumpit-sumpit\nsundut-bersundut\nsungai-sungai\nsungguh-sungguh\nsungut-sungut\nsunting-menyunting\nsuper-damai\nsuper-rahasia\nsuper-sub\nsupply-demand\nsupply-side\nsuram-suram\nsurat-menyurat\nsurat-surat\nsuruh-suruhan\nsuruk-surukan\nsusul-menyusul\nsuwir-suwir\nsyarat-syarat\nsystem-on-chip\nt-shirt\nt-shirts\ntabar-tabar\ntabir-mabir\ntabrak-tubruk\ntabuh-tabuhan\ntabun-menabun\ntahu-menahu\ntahu-tahu\ntahun-tahun\ntakah-takahnya\ntakang-takik\ntake-off\ntakut-takut\ntakut-takutan\ntali-bertali\ntali-tali\ntalun-temalun\ntaman-taman\ntampak-tampak\ntanak-tanakan\ntanam-menanam\ntanam-tanaman\ntanda-tanda\ntangan-menangan\ntangan-tangan\ntangga-tangga\ntanggal-tanggal\ntanggul-tanggul\ntanggung-menanggung\ntanggung-tanggung\ntank-tank\ntante-tante\ntanya-jawab\ntapa-tapa\ntapak-tapak\ntari-menari\ntari-tarian\ntarik-menarik\ntarik-ulur\ntata-tertib\ntatah-tatah\ntau-tau\ntawa-tawa\ntawak-tawak\ntawang-tawang\ntawar-menawar\ntawar-tawar\ntayum-temayum\ntebak-tebakan\ntebu-tebu\ntedong-tedong\ntegak-tegak\ntegerbang-gerbang\nteh-tehan\ntek-tek\nteka-teki\nteknik-teknik\nteman-teman\nteman-temanku\ntemas-temas\ntembak-menembak\ntemeh-temeh\ntempa-menempa\ntempat-tempat\ntempo-tempo\ntemut-temut\ntenang-tenang\ntengah-tengah\ntenggang-menenggang\ntengok-menengok\nteori-teori\nteraba-raba\nteralang-alang\nterambang-ambang\nterambung-ambung\nterang-terang\nterang-terangan\nteranggar-anggar\nterangguk-angguk\nteranggul-anggul\nterangin-angin\nterangkup-angkup\nteranja-anja\nterapung-apung\nterayan-rayan\nterayap-rayap\nterbada-bada\nterbahak-bahak\nterbang-terbang\nterbata-bata\nterbatuk-batuk\nterbayang-bayang\nterbeda-bedakan\nterbengkil-bengkil\nterbengong-bengong\nterbirit-birit\nterbuai-buai\nterbuang-buang\nterbungkuk-bungkuk\nterburu-buru\ntercangak-cangak\ntercengang-cengang\ntercilap-cilap\ntercongget-congget\ntercoreng-moreng\ntercungap-cungap\nterdangka-dangka\nterdengih-dengih\nterduga-duga\nterekeh-ekeh\nterembut-embut\nterembut-rembut\nterempas-empas\nterengah-engah\nteresak-esak\ntergagap-gagap\ntergagau-gagau\ntergaguk-gaguk\ntergapai-gapai\ntergegap-gegap\ntergegas-gegas\ntergelak-gelak\ntergelang-gelang\ntergeleng-geleng\ntergelung-gelung\ntergerai-gerai\ntergerenyeng-gerenyeng\ntergesa-gesa\ntergila-gila\ntergolek-golek\ntergontai-gontai\ntergudik-gudik\ntergugu-gugu\nterguling-guling\ntergulut-gulut\nterhambat-hambat\nterharak-harak\nterharap-harap\nterhengit-hengit\nterheran-heran\nterhinggut-hinggut\nterigau-igau\nterimpi-impi\nterincut-incut\nteringa-inga\nteringat-ingat\nterinjak-injak\nterisak-isak\nterjembak-jembak\nterjerit-jerit\nterkadang-kadang\nterkagum-kagum\nterkaing-kaing\nterkakah-kakah\nterkakak-kakak\nterkampul-kampul\nterkanjar-kanjar\nterkantuk-kantuk\nterkapah-kapah\nterkapai-kapai\nterkapung-kapung\nterkatah-katah\nterkatung-katung\nterkecap-kecap\nterkedek-kedek\nterkedip-kedip\nterkejar-kejar\nterkekau-kekau\nterkekeh-kekeh\nterkekek-kekek\nterkelinjat-kelinjat\nterkelip-kelip\nterkempul-kempul\nterkemut-kemut\nterkencar-kencar\nterkencing-kencing\nterkentut-kentut\nterkepak-kepak\nterkesot-kesot\nterkesut-kesut\nterkial-kial\nterkijai-kijai\nterkikih-kikih\nterkikik-kikik\nterkincak-kincak\nterkindap-kindap\nterkinja-kinja\nterkirai-kirai\nterkitar-kitar\nterkocoh-kocoh\nterkojol-kojol\nterkokol-kokol\nterkosel-kosel\nterkotak-kotak\nterkoteng-koteng\nterkuai-kuai\nterkumpal-kumpal\nterlara-lara\nterlayang-layang\nterlebih-lebih\nterlincah-lincah\nterliuk-liuk\nterlolong-lolong\nterlongong-longong\nterlunta-lunta\ntermangu-mangu\ntermanja-manja\ntermata-mata\ntermengah-mengah\ntermenung-menung\ntermimpi-mimpi\ntermonyong-monyong\nternanti-nanti\nterngiang-ngiang\nteroleng-oleng\nterombang-ambing\nterpalit-palit\nterpandang-pandang\nterpecah-pecah\nterpekik-pekik\nterpencar-pencar\nterpereh-pereh\nterpijak-pijak\nterpikau-pikau\nterpilah-pilah\nterpinga-pinga\nterpingkal-pingkal\nterpingkau-pingkau\nterpontang-panting\nterpusing-pusing\nterputus-putus\ntersanga-sanga\ntersaruk-saruk\ntersedan-sedan\ntersedih-sedih\ntersedu-sedu\nterseduh-seduh\ntersendat-sendat\ntersendeng-sendeng\ntersengal-sengal\ntersengguk-sengguk\ntersengut-sengut\nterseok-seok\ntersera-sera\nterserak-serak\ntersetai-setai\ntersia-sia\ntersipu-sipu\ntersoja-soja\ntersungkuk-sungkuk\ntersuruk-suruk\ntertagak-tagak\ntertahan-tahan\ntertatih-tatih\ntertegun-tegun\ntertekan-tekan\nterteleng-teleng\ntertendang-tendang\ntertimpang-timpang\ntertitar-titar\nterumbang-ambing\nterumbang-umbang\nterungkap-ungkap\nterus-menerus\nterus-terusan\ntete-a-tete\ntext-to-speech\nthink-tank\nthink-thank\nthird-party\nthird-person\nthree-axis\nthree-point\ntiap-tiap\ntiba-tiba\ntidak-tidak\ntidur-tidur\ntidur-tiduran\ntie-dye\ntie-in\ntiga-tiganya\ntikam-menikam\ntiki-taka\ntikus-tikus\ntilik-menilik\ntim-tim\ntimah-timah\ntimang-timangan\ntimbang-menimbang\ntime-lapse\ntimpa-menimpa\ntimu-timu\ntimun-timunan\ntimur-barat\ntimur-laut\ntimur-tenggara\ntindih-bertindih\ntindih-menindih\ntinjau-meninjau\ntinju-meninju\ntip-off\ntipu-tipu\ntiru-tiruan\ntitik-titik\ntitik-titiknya\ntiup-tiup\nto-do\ntokak-takik\ntoko-toko\ntokoh-tokoh\ntokok-menokok\ntolak-menolak\ntolong-menolong\ntong-tong\ntop-level\ntop-up\ntotol-totol\ntouch-screen\ntrade-in\ntraining-camp\ntrans-nasional\ntreble-winner\ntri-band\ntrik-trik\ntriple-core\ntruk-truk\ntua-tua\ntuan-tuan\ntuang-tuang\ntuban-tuban\ntubuh-tubuh\ntujuan-tujuan\ntuk-tuk\ntukang-menukang\ntukar-menukar\ntulang-belulang\ntulang-tulangan\ntuli-tuli\ntulis-menulis\ntumbuh-tumbuhan\ntumpang-tindih\ntune-up\ntunggang-tunggik\ntunggang-tungging\ntunggang-tunggit\ntunggul-tunggul\ntunjuk-menunjuk\ntupai-tupai\ntupai-tupaian\nturi-turian\nturn-based\nturnamen-turnamen\nturun-temurun\nturut-menurut\nturut-turutan\ntuyuk-tuyuk\ntwin-cam\ntwin-turbocharged\ntwo-state\ntwo-step\ntwo-tone\nu-shape\nuang-uangan\nuar-uar\nubek-ubekan\nubel-ubel\nubrak-abrik\nubun-ubun\nubur-ubur\nuci-uci\nudang-undang\nudap-udapan\nugal-ugalan\nuget-uget\nuir-uir\nujar-ujar\nuji-coba\nujung-ujung\nujung-ujungnya\nuka-uka\nukir-mengukir\nukir-ukiran\nula-ula\nulak-ulak\nulam-ulam\nulang-alik\nulang-aling\nulang-ulang\nulap-ulap\nular-ular\nular-ularan\nulek-ulek\nulu-ulu\nulung-ulung\numang-umang\numbang-ambing\numbi-umbian\numbul-umbul\numbut-umbut\nuncang-uncit\nundak-undakan\nundang-undang\nundang-undangnya\nunduk-unduk\nundung-undung\nundur-undur\nunek-unek\nungah-angih\nunggang-anggit\nunggat-unggit\nunggul-mengungguli\nungkit-ungkit\nunit-unit\nuniversitas-universitas\nunsur-unsur\nuntang-anting\nunting-unting\nuntung-untung\nuntung-untungan\nupah-mengupah\nupih-upih\nupside-down\nura-ura\nuran-uran\nurat-urat\nuring-uringan\nurup-urup\nurup-urupan\nurus-urus\nusaha-usaha\nuser-user\nuser-useran\nutak-atik\nutang-piutang\nutang-utang\nutar-utar\nutara-jauh\nutara-selatan\nuter-uter\nutusan-utusan\nv-belt\nv-neck\nvalue-added\nvery-very\nvideo-video\nvisi-misi\nvisi-misinya\nvoa-islam\nvoice-over\nvolt-ampere\nwajah-wajah\nwajar-wajar\nwake-up\nwakil-wakil\nwalk-in\nwalk-out\nwangi-wangian\nwanita-wanita\nwanti-wanti\nwara-wara\nwara-wiri\nwarna-warna\nwarna-warni\nwas-was\nwater-cooled\nweb-based\nwide-angle\nwilayah-wilayah\nwin-win\nwira-wiri\nwora-wari\nwork-life\nworld-class\nyang-yang\nyayasan-yayasan\nyear-on-year\nyel-yel\nyo-yo\nzam-zam\nzig-zag\n'.split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/id/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.id.tokenizer_exceptions.orth_title->'-'.join([part.title() for part in orth.split('-')])
A:spacy.lang.id.tokenizer_exceptions.orth_caps->'-'.join([part.upper() for part in orth.split('-')])
A:spacy.lang.id.tokenizer_exceptions.orth_lower->orth.lower()
A:spacy.lang.id.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/id/__init__.py----------------------------------------
spacy.lang.id.__init__.Indonesian(Language)
spacy.lang.id.__init__.IndonesianDefaults(BaseDefaults)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/id/lex_attrs.py----------------------------------------
A:spacy.lang.id.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.id.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
A:spacy.lang.id.lex_attrs.(_, num)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('-')
spacy.lang.id.lex_attrs.is_currency(text)
spacy.lang.id.lex_attrs.like_num(text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/id/syntax_iterators.py----------------------------------------
A:spacy.lang.id.syntax_iterators.conj->doc.vocab.strings.add('conj')
A:spacy.lang.id.syntax_iterators.np_label->doc.vocab.strings.add('NP')
spacy.lang.id.syntax_iterators.noun_chunks(doclike:Union[Doc,Span])->Iterator[Tuple[int, int, int]]


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/id/punctuation.py----------------------------------------
A:spacy.lang.id.punctuation.UNITS->merge_chars(_units)
A:spacy.lang.id.punctuation.CURRENCY->merge_chars(_currency)
A:spacy.lang.id.punctuation.MONTHS->merge_chars(_months)
A:spacy.lang.id.punctuation.LIST_CURRENCY->split_chars(_currency)
A:spacy.lang.id.punctuation._prefixes->list(TOKENIZER_PREFIXES)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ar/examples.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ar/stop_words.py----------------------------------------
A:spacy.lang.ar.stop_words.STOP_WORDS->set('\nÙ…Ù†\nÙ†Ø­Ùˆ\nÙ„Ø¹Ù„\nØ¨Ù…Ø§\nØ¨ÙŠÙ†\nÙˆØ¨ÙŠÙ†\nØ§ÙŠØ¶Ø§\nÙˆØ¨ÙŠÙ†Ù…Ø§\nØªØ­Øª\nÙ…Ø«Ù„Ø§\nÙ„Ø¯ÙŠ\nØ¹Ù†Ù‡\nÙ…Ø¹\nÙ‡ÙŠ\nÙˆÙ‡Ø°Ø§\nÙˆØ§Ø°Ø§\nÙ‡Ø°Ø§Ù†\nØ§Ù†Ù‡\nØ¨ÙŠÙ†Ù…Ø§\nØ£Ù…Ø³Ù‰\nÙˆØ³ÙˆÙ\nÙˆÙ„Ù…\nÙ„Ø°Ù„Ùƒ\nØ¥Ù„Ù‰\nÙ…Ù†Ù‡\nÙ…Ù†Ù‡Ø§\nÙƒÙ…Ø§\nØ¸Ù„\nÙ‡Ù†Ø§\nØ¨Ù‡\nÙƒØ°Ù„Ùƒ\nØ§Ù…Ø§\nÙ‡Ù…Ø§\nØ¨Ø¹Ø¯\nØ¨ÙŠÙ†Ù‡Ù…\nØ§Ù„ØªÙŠ\nØ£Ø¨Ùˆ\nØ§Ø°Ø§\nØ¨Ø¯Ù„Ø§\nÙ„Ù‡Ø§\nØ£Ù…Ø§Ù…\nÙŠÙ„ÙŠ\nØ­ÙŠÙ†\nØ¶Ø¯\nØ§Ù„Ø°ÙŠ\nÙ‚Ø¯\nØµØ§Ø±\nØ¥Ø°Ø§\nÙ…Ø§Ø¨Ø±Ø­\nÙ‚Ø¨Ù„\nÙƒÙ„\nÙˆÙ„ÙŠØ³Øª\nØ§Ù„Ø°ÙŠÙ†\nÙ„Ù‡Ø°Ø§\nÙˆØ«ÙŠ\nØ§Ù†Ù‡Ù…\nØ¨Ø§Ù„Ù„ØªÙŠ\nÙ…Ø§ÙØªØ¦\nÙˆÙ„Ø§\nØ¨Ù‡Ø°Ù‡\nØ¨Ø­ÙŠØ«\nÙƒÙŠÙ\nÙˆÙ„Ù‡\nØ¹Ù„ÙŠ\nØ¨Ø§Øª\nÙ„Ø§Ø³ÙŠÙ…Ø§\nØ­ØªÙ‰\nÙˆÙ‚Ø¯\nÙˆ\nØ£Ù…Ø§\nÙÙŠÙ‡Ø§\nØ¨Ù‡Ø°Ø§\nÙ„Ø°Ø§\nØ­ÙŠØ«\nÙ„Ù‚Ø¯\nØ¥Ù†\nÙØ¥Ù†\nØ§ÙˆÙ„\nÙ„ÙŠØª\nÙØ§Ù„Ù„ØªÙŠ\nÙˆÙ„Ù‚Ø¯\nÙ„Ø³ÙˆÙ\nÙ‡Ø°Ù‡\nÙˆÙ„Ù…Ø§Ø°Ø§\nÙ…Ø¹Ù‡\nØ§Ù„Ø­Ø§Ù„ÙŠ\nØ¨Ø¥Ù†\nØ­ÙˆÙ„\nÙÙŠ\nØ¹Ù„ÙŠÙ‡\nÙ…Ø§ÙŠØ²Ø§Ù„\nÙˆÙ„Ø¹Ù„\nØ£Ù†Ù‡\nØ£Ø¶Ø­Ù‰\nØ§ÙŠ\nØ³ØªÙƒÙˆÙ†\nÙ„Ù†\nØ£Ù†\nØ¶Ù…Ù†\nÙˆØ¹Ù„Ù‰\nØ§Ù…Ø³Ù‰\nØ§Ù„ÙŠ\nØ°Ø§Øª\nÙˆÙ„Ø§ÙŠØ²Ø§Ù„\nØ°Ù„Ùƒ\nÙÙ‚Ø¯\nÙ‡Ù…\nØ£ÙŠ\nØ¹Ù†Ø¯\nØ§Ø¨Ù†\nØ£Ùˆ\nÙÙ‡Ùˆ\nÙØ§Ù†Ù‡\nØ³ÙˆÙ\nÙ…Ø§\nØ¢Ù„\nÙƒÙ„Ø§\nØ¹Ù†Ù‡Ø§\nÙˆÙƒØ°Ù„Ùƒ\nÙ„ÙŠØ³Øª\nÙ„Ù…\nÙˆØ£Ù†\nÙ…Ø§Ø°Ø§\nÙ„Ùˆ\nÙˆÙ‡Ù„\nØ§Ù„Ù„ØªÙŠ\nÙˆÙ„Ø°Ø§\nÙŠÙ…ÙƒÙ†\nÙÙŠÙ‡\nØ§Ù„Ø§\nØ¹Ù„ÙŠÙ‡Ø§\nÙˆØ¨ÙŠÙ†Ù‡Ù…\nÙŠÙˆÙ…\nÙˆØ¨Ù…Ø§\nÙ„Ù…Ø§\nÙÙƒØ§Ù†\nØ§Ø¶Ø­Ù‰\nØ§ØµØ¨Ø­\nÙ„Ù‡Ù…\nØ¨Ù‡Ø§\nØ§Ùˆ\nØ§Ù„Ø°Ù‰\nØ§Ù„Ù‰\nØ¥Ù„ÙŠ\nÙ‚Ø§Ù„\nÙˆØ§Ù„ØªÙŠ\nÙ„Ø§Ø²Ø§Ù„\nØ£ØµØ¨Ø­\nÙˆÙ„Ù‡Ø°Ø§\nÙ…Ø«Ù„\nÙˆÙƒØ§Ù†Øª\nÙ„ÙƒÙ†Ù‡\nØ¨Ø°Ù„Ùƒ\nÙ‡Ø°Ø§\nÙ„Ù…Ø§Ø°Ø§\nÙ‚Ø§Ù„Øª\nÙÙ‚Ø·\nÙ„ÙƒÙ†\nÙ…Ù…Ø§\nÙˆÙƒÙ„\nÙˆØ§Ù†\nÙˆØ£Ø¨Ùˆ\nÙˆÙ…Ù†\nÙƒØ§Ù†\nÙ…Ø§Ø²Ø§Ù„\nÙ‡Ù„\nØ¨ÙŠÙ†Ù‡Ù†\nÙ‡Ùˆ\nÙˆÙ…Ø§\nØ¹Ù„Ù‰\nÙˆÙ‡Ùˆ\nÙ„Ø£Ù†\nÙˆØ§Ù„Ù„ØªÙŠ\nÙˆØ§Ù„Ø°ÙŠ\nØ¯ÙˆÙ†\nØ¹Ù†\nÙˆØ§ÙŠØ¶Ø§\nÙ‡Ù†Ø§Ùƒ\nØ¨Ù„Ø§\nØ¬Ø¯Ø§\nØ«Ù…\nÙ…Ù†Ø°\nØ§Ù„Ù„Ø°ÙŠÙ†\nÙ„Ø§ÙŠØ²Ø§Ù„\nØ¨Ø¹Ø¶\nÙ…Ø³Ø§Ø¡\nØªÙƒÙˆÙ†\nÙÙ„Ø§\nØ¨ÙŠÙ†Ù†Ø§\nÙ„Ø§\nÙˆÙ„ÙƒÙ†\nØ¥Ø°\nÙˆØ£Ø«Ù†Ø§Ø¡\nÙ„ÙŠØ³\nÙˆÙ…Ø¹\nÙÙŠÙ‡Ù…\nÙˆÙ„Ø³ÙˆÙ\nØ¨Ù„\nØªÙ„Ùƒ\nØ£Ø­Ø¯\nÙˆÙ‡ÙŠ\nÙˆÙƒØ§Ù†\nÙˆÙ…Ù†Ù‡Ø§\nÙˆÙÙŠ\nÙ…Ø§Ø§Ù†ÙÙƒ\nØ§Ù„ÙŠÙˆÙ…\nÙˆÙ…Ø§Ø°Ø§\nÙ‡Ø¤Ù„Ø§Ø¡\nÙˆÙ„ÙŠØ³\nÙ„Ù‡\nØ£Ø«Ù†Ø§Ø¡\nØ¨Ø¯\nØ§Ù„ÙŠÙ‡\nÙƒØ£Ù†\nØ§Ù„ÙŠÙ‡Ø§\nØ¨ØªÙ„Ùƒ\nÙŠÙƒÙˆÙ†\nÙˆÙ„Ù…Ø§\nÙ‡Ù†\nÙˆØ§Ù„Ù‰\nÙƒØ§Ù†Øª\nÙˆÙ‚Ø¨Ù„\nØ§Ù†\nÙ„Ø¯Ù‰\nØ¥Ø°Ù…Ø§\nØ¥Ø°Ù†\nØ£Ù\nØ£Ù‚Ù„\nØ£ÙƒØ«Ø±\nØ£Ù„Ø§\nØ¥Ù„Ø§\nØ§Ù„Ù„Ø§ØªÙŠ\nØ§Ù„Ù„Ø§Ø¦ÙŠ\nØ§Ù„Ù„ØªØ§Ù†\nØ§Ù„Ù„ØªÙŠØ§\nØ§Ù„Ù„ØªÙŠÙ†\nØ§Ù„Ù„Ø°Ø§Ù†\nØ§Ù„Ù„ÙˆØ§ØªÙŠ\nØ¥Ù„ÙŠÙƒ\nØ¥Ù„ÙŠÙƒÙ…\nØ¥Ù„ÙŠÙƒÙ…Ø§\nØ¥Ù„ÙŠÙƒÙ†\nØ£Ù…\nØ£Ù…Ø§\nØ¥Ù…Ø§\nØ¥Ù†Ø§\nØ£Ù†Ø§\nØ£Ù†Øª\nØ£Ù†ØªÙ…\nØ£Ù†ØªÙ…Ø§\nØ£Ù†ØªÙ†\nØ¥Ù†Ù…Ø§\nØ¥Ù†Ù‡\nØ£Ù†Ù‰\nØ£Ù†Ù‰\nØ¢Ù‡\nØ¢Ù‡Ø§\nØ£ÙˆÙ„Ø§Ø¡\nØ£ÙˆÙ„Ø¦Ùƒ\nØ£ÙˆÙ‡\nØ¢ÙŠ\nØ£ÙŠÙ‡Ø§\nØ¥ÙŠ\nØ£ÙŠÙ†\nØ£ÙŠÙ†\nØ£ÙŠÙ†Ù…Ø§\nØ¥ÙŠÙ‡\nØ¨Ø®\nØ¨Ø³\nØ¨Ùƒ\nØ¨ÙƒÙ…\nØ¨ÙƒÙ…\nØ¨ÙƒÙ…Ø§\nØ¨ÙƒÙ†\nØ¨Ù„Ù‰\nØ¨Ù…Ø§Ø°Ø§\nØ¨Ù…Ù†\nØ¨Ù†Ø§\nØ¨Ù‡Ù…\nØ¨Ù‡Ù…Ø§\nØ¨Ù‡Ù†\nØ¨ÙŠ\nØ¨ÙŠØ¯\nØªÙ„ÙƒÙ…\nØªÙ„ÙƒÙ…Ø§\nØªÙ‡\nØªÙŠ\nØªÙŠÙ†\nØªÙŠÙ†Ùƒ\nØ«Ù…Ø©\nØ­Ø§Ø´Ø§\nØ­Ø¨Ø°Ø§\nØ­ÙŠØ«Ù…Ø§\nØ®Ù„Ø§\nØ°Ø§\nØ°Ø§Ùƒ\nØ°Ø§Ù†\nØ°Ø§Ù†Ùƒ\nØ°Ù„ÙƒÙ…\nØ°Ù„ÙƒÙ…Ø§\nØ°Ù„ÙƒÙ†\nØ°Ù‡\nØ°Ùˆ\nØ°ÙˆØ§\nØ°ÙˆØ§ØªØ§\nØ°ÙˆØ§ØªÙŠ\nØ°ÙŠ\nØ°ÙŠÙ†\nØ°ÙŠÙ†Ùƒ\nØ±ÙŠØ«\nØ³ÙˆÙ‰\nØ´ØªØ§Ù†\nØ¹Ø¯Ø§\nØ¹Ø³Ù‰\nØ¹Ù„\nØ¹Ù„ÙŠÙƒ\nØ¹Ù…Ø§\nØºÙŠØ±\nÙØ¥Ø°Ø§\nÙÙ…Ù†\nÙÙŠÙ…\nÙÙŠÙ…Ø§\nÙƒØ£Ù†Ù…Ø§\nÙƒØ£ÙŠ\nÙƒØ£ÙŠÙ†\nÙƒØ°Ø§\nÙƒÙ„Ø§Ù‡Ù…Ø§\nÙƒÙ„ØªØ§\nÙƒÙ„Ù…Ø§\nÙƒÙ„ÙŠÙƒÙ…Ø§\nÙƒÙ„ÙŠÙ‡Ù…Ø§\nÙƒÙ…\nÙƒÙ…\nÙƒÙŠ\nÙƒÙŠØª\nÙƒÙŠÙÙ…Ø§\nÙ„Ø³Øª\nÙ„Ø³ØªÙ…\nÙ„Ø³ØªÙ…Ø§\nÙ„Ø³ØªÙ†\nÙ„Ø³Ù†\nÙ„Ø³Ù†Ø§\nÙ„Ùƒ\nÙ„ÙƒÙ…\nÙ„ÙƒÙ…Ø§\nÙ„ÙƒÙ†Ù…Ø§\nÙ„ÙƒÙŠ\nÙ„ÙƒÙŠÙ„Ø§\nÙ„Ù†Ø§\nÙ„Ù‡Ù…Ø§\nÙ„Ù‡Ù†\nÙ„ÙˆÙ„Ø§\nÙ„ÙˆÙ…Ø§\nÙ„ÙŠ\nÙ„Ø¦Ù†\nÙ„ÙŠØ³Ø§\nÙ„ÙŠØ³ØªØ§\nÙ„ÙŠØ³ÙˆØ§\nÙ…ØªÙ‰\nÙ…Ø°\nÙ…Ù…Ù†\nÙ…Ù‡\nÙ…Ù‡Ù…Ø§\nÙ†Ø­Ù†\nÙ†Ø¹Ù…\nÙ‡Ø§\nÙ‡Ø§ØªØ§Ù†\nÙ‡Ø§ØªÙ‡\nÙ‡Ø§ØªÙŠ\nÙ‡Ø§ØªÙŠÙ†\nÙ‡Ø§Ùƒ\nÙ‡Ø§Ù‡Ù†Ø§\nÙ‡Ø°ÙŠ\nÙ‡Ø°ÙŠÙ†\nÙ‡ÙƒØ°Ø§\nÙ‡Ù„Ø§\nÙ‡Ù†Ø§Ù„Ùƒ\nÙ‡ÙŠØ§\nÙ‡ÙŠØª\nÙ‡ÙŠÙ‡Ø§Øª\nÙˆØ§Ù„Ø°ÙŠÙ†\nÙˆØ¥Ø°\nÙˆØ¥Ø°Ø§\nÙˆØ¥Ù†\nÙˆÙ„Ùˆ\nÙŠØ§\n'.split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ar/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.ar.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ar/__init__.py----------------------------------------
spacy.lang.ar.__init__.Arabic(Language)
spacy.lang.ar.__init__.ArabicDefaults(BaseDefaults)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ar/lex_attrs.py----------------------------------------
A:spacy.lang.ar.lex_attrs._num_words->set('\nØµÙØ±\nÙˆØ§Ø­Ø¯\nØ¥Ø«Ù†Ø§Ù†\nØ§Ø«Ù†Ø§Ù†\nØ«Ù„Ø§Ø«Ø©\nØ«Ù„Ø§Ø«Ù‡\nØ£Ø±Ø¨Ø¹Ø©\nØ£Ø±Ø¨Ø¹Ù‡\nØ®Ù…Ø³Ø©\nØ®Ù…Ø³Ù‡\nØ³ØªØ©\nØ³ØªÙ‡\nØ³Ø¨Ø¹Ø©\nØ³Ø¨Ø¹Ù‡\nØ«Ù…Ø§Ù†ÙŠØ©\nØ«Ù…Ø§Ù†ÙŠÙ‡\nØªØ³Ø¹Ø©\nØªØ³Ø¹Ù‡\nï»‹ïº¸ïº®ïº“\nï»‹ïº¸ïº®Ù‡\nØ¹Ø´Ø±ÙˆÙ†\nØ¹Ø´Ø±ÙŠÙ†\nØ«Ù„Ø§Ø«ÙˆÙ†\nØ«Ù„Ø§Ø«ÙŠÙ†\nØ§Ø±Ø¨Ø¹ÙˆÙ†\nØ§Ø±Ø¨Ø¹ÙŠÙ†\nØ£Ø±Ø¨Ø¹ÙˆÙ†\nØ£Ø±Ø¨Ø¹ÙŠÙ†\nØ®Ù…Ø³ÙˆÙ†\nØ®Ù…Ø³ÙŠÙ†\nØ³ØªÙˆÙ†\nØ³ØªÙŠÙ†\nØ³Ø¨Ø¹ÙˆÙ†\nØ³Ø¨Ø¹ÙŠÙ†\nØ«Ù…Ø§Ù†ÙˆÙ†\nØ«Ù…Ø§Ù†ÙŠÙ†\nØªØ³Ø¹ÙˆÙ†\nØªØ³Ø¹ÙŠÙ†\nÙ…Ø§Ø¦ØªÙŠÙ†\nÙ…Ø§Ø¦ØªØ§Ù†\nØ«Ù„Ø§Ø«Ù…Ø§Ø¦Ø©\nØ®Ù…Ø³Ù…Ø§Ø¦Ø©\nØ³Ø¨Ø¹Ù…Ø§Ø¦Ø©\nØ§Ù„Ù\nØ¢Ù„Ø§Ù\nÙ…Ù„Ø§ÙŠÙŠÙ†\nÙ…Ù„ÙŠÙˆÙ†\nÙ…Ù„ÙŠØ§Ø±\nÙ…Ù„ÙŠØ§Ø±Ø§Øª\n'.split())
A:spacy.lang.ar.lex_attrs._ordinal_words->set('\nØ§ÙˆÙ„\nØ£ÙˆÙ„\nØ­Ø§Ø¯\nÙˆØ§Ø­Ø¯\nØ«Ø§Ù†\nØ«Ø§Ù†ÙŠ\nØ«Ø§Ù„Ø«\nØ±Ø§Ø¨Ø¹\nØ®Ø§Ù…Ø³\nØ³Ø§Ø¯Ø³\nØ³Ø§Ø¨Ø¹\nØ«Ø§Ù…Ù†\nØªØ§Ø³Ø¹\nØ¹Ø§Ø´Ø±\n'.split())
A:spacy.lang.ar.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.ar.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.ar.lex_attrs.like_num(text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ar/punctuation.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/lij/examples.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/lij/stop_words.py----------------------------------------
A:spacy.lang.lij.stop_words.STOP_WORDS->set("\na Ã  Ã¢ a-a a-e a-i a-o aiva aloa an ancheu ancon apreuvo ascÃ¬ atra atre atri atro avanti avei\n\nbella belle belli bello ben\n\nch' che chÃ¬ chi ciÃ¹ co-a co-e co-i co-o comm' comme con cÃ¶sa coscÃ¬ cÃ¶se\n\nd' da da-a da-e da-i da-o dapeu de delongo derÃª di do doe doÃ® donde dÃ²ppo\n\nÃ© e Ãª ea ean emmo en Ã«se\n\nfin fiÃ±a\n\ngh' ghe guÃ¦ei\n\ni Ã® in insemme int' inta inte inti into\n\nl' lÃª lÃ¬ lÃ´\n\nm' ma manco me megio meno mezo mi\n\nna n' ne ni ninte nisciun nisciuÃ±a no\n\no Ã² Ã´ oua\n\nparte pe pe-a pe-i pe-e pe-o perchÃ© pittin pÃ¶ primma prÃ²pio\n\nquÃ¦ quand' quande quarche quella quelle quelli quello\n\ns' sce scÃª sci sciÃ¢ sciÃ´ sciÃ¹ se segge seu sÃ² solo son sott' sta stÃ¦ta stÃ¦te stÃ¦ti stÃ¦to ste sti sto\n\ntanta tante tanti tanto te ti torna tra trÃ²ppo tutta tutte tutti tutto\n\nun uÃ±a unn' unna\n\nza zu\n".split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/lij/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.lij.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/lij/__init__.py----------------------------------------
spacy.lang.lij.__init__.Ligurian(Language)
spacy.lang.lij.__init__.LigurianDefaults(BaseDefaults)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/lij/punctuation.py----------------------------------------
A:spacy.lang.lij.punctuation.ELISION->" ' â€™ ".strip().replace(' ', '').replace('\n', '')


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/dsb/examples.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/dsb/stop_words.py----------------------------------------
A:spacy.lang.dsb.stop_words.STOP_WORDS->set('\na abo aby ako ale aÅ¾\n\ndaniÅ¾ dokulaÅ¾\n\ngaÅ¾\n\njolic\n\npak pÃ³tom\n\nteke togodla\n'.split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/dsb/__init__.py----------------------------------------
spacy.lang.dsb.__init__.LowerSorbian(Language)
spacy.lang.dsb.__init__.LowerSorbianDefaults(BaseDefaults)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/dsb/lex_attrs.py----------------------------------------
A:spacy.lang.dsb.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.dsb.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
A:spacy.lang.dsb.lex_attrs.text_lower->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').lower()
spacy.lang.dsb.lex_attrs.like_num(text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ro/examples.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ro/stop_words.py----------------------------------------
A:spacy.lang.ro.stop_words.STOP_WORDS->set('\na\nabia\nacea\naceasta\naceastÄƒ\naceea\naceeasi\naceeaÈ™i\nacei\naceia\nacel\nacela\nacelasi\nacelaÈ™i\nacele\nacelea\nacest\nacesta\naceste\nacestea\nacestei\nacestia\nacestui\naceÅŸti\naceÅŸtia\nacolo\nacord\nacum\nadica\nadicÄƒ\nai\naia\naibÄƒ\naici\naiurea\nal\nala\nalaturi\nale\nalea\nalt\nalta\naltceva\naltcineva\nalte\naltfel\nalti\naltii\naltul\nalÄƒturi\nam\nanume\napoi\napai\napÄƒi\nar\nare\nas\nasa\nasemenea\nasta\nastazi\nastea\nastfel\nastÄƒzi\nasupra\natare\natat\natata\natatea\natatia\nati\natit\natita\natitea\natitia\natunci\nau\navea\navem\naveÅ£i\naveÈ›i\navut\nazi\naÅŸ\naÅŸadar\naÅ£i\naÈ™\naÈ™adar\naÈ›i\nb\nba\nbine\nbucur\nbunÄƒ\nc\nca\ncam\ncand\ncapat\ncare\ncareia\ncarora\ncaruia\ncat\ncatre\ncaut\nce\ncea\nceea\ncei\nceilalti\ncel\ncele\ncelor\nceva\nchiar\nci\ncinci\ncind\ncine\ncineva\ncit\ncita\ncite\nciteva\nciti\ncitiva\nconform\ncontra\ncu\ncui\ncum\ncumva\ncurÃ¢nd\ncurÃ®nd\ncÃ¢nd\ncÃ¢t\ncÃ¢te\ncÃ¢tva\ncÃ¢Å£i\ncÃ¢È›i\ncÃ®nd\ncÃ®t\ncÃ®te\ncÃ®tva\ncÃ®Å£i\ncÃ®È›i\ncÄƒ\ncÄƒci\ncÄƒrei\ncÄƒror\ncÄƒrora\ncÄƒrui\ncÄƒruia\ncÄƒtre\nd\nda\ndaca\ndacÄƒ\ndar\ndat\ndatoritÄƒ\ndatÄƒ\ndau\nde\ndeasupra\ndeci\ndecit\ndegraba\ndeja\ndeoarece\ndeparte\ndesi\ndespre\ndeÅŸi\ndeÈ™i\ndin\ndinaintea\ndincolo\ndincoace\ndintr\ndintr-\ndintre\ndoar\ndoi\ndoilea\ndouÄƒ\ndrept\ndupa\ndupÄƒ\ndÄƒ\ndeunaseara\ndeunÄƒsearÄƒ\ndeunazi\ndeunÄƒzi\ne\nea\nei\nel\nele\nera\neram\neste\neu\nexact\neÅŸti\neÈ™ti\nf\nface\nfara\nfata\nfel\nfi\nfie\nfiecare\nfii\nfim\nfiu\nfiÅ£i\nfiÈ›i\nfoarte\nfost\nfrumos\nfÄƒrÄƒ\ng\ngeaba\ngraÅ£ie\ngraÈ›ie\nh\ni\nia\niar\nieri\nii\nil\nimi\nin\ninainte\ninapoi\ninca\nincotro\nincit\ninsa\nintr\nintre\nisi\niti\nj\nk\nl\nla\nle\nli\nlor\nlui\nlÃ¢ngÄƒ\nlÃ®ngÄƒ\nm\nma\nmai\nmare\nmacar\nmÄƒcar\nmata\nmatale\nmea\nmei\nmele\nmereu\nmeu\nmi\nmie\nmine\nmod\nmult\nmulta\nmulte\nmulti\nmultÄƒ\nmulÅ£i\nmulÅ£umesc\nmulÈ›i\nmulÈ›umesc\nmÃ¢ine\nmÃ®ine\nmÄƒ\nn\nna\nne\nneincetat\nneÃ®ncetat\nnevoie\nni\nnici\nnicidecum\nnicidecat\nnicidecÃ¢t\nniciodata\nniciodatÄƒ\nnicÄƒieri\nnimeni\nnimeri\nnimic\nniste\nniÅŸte\nniÈ™te\nnoastre\nnoastrÄƒ\nnoi\nnoroc\nnostri\nnostru\nnou\nnoua\nnouÄƒ\nnoÅŸtri\nnoÈ™tri\nnu\nnumai\no\nodata\nodatÄƒ\nodinioara\nodinioarÄƒ\nopt\nor\nori\noricare\norice\noricine\noricum\noricÃ¢nd\noricÃ¢t\noricÃ®nd\noricÃ®t\noriunde\np\npai\npÄƒi\nparca\nparcÄƒ\npatra\npatru\npatrulea\npe\npentru\npeste\npic\npina\nplus\npoate\npot\nprea\nprima\nprimul\nprin\nprintr-\nprintre\nputini\npuÅ£in\npuÅ£ina\npuÅ£inÄƒ\npÃ¢nÄƒ\npÃ®nÄƒ\nr\nrog\ns\nsa\nsa-mi\nsa-ti\nsai\nsale\nsau\nse\nsi\nsint\nsintem\nspate\nspre\nsub\nsunt\nsuntem\nsunteÅ£i\nsunteÈ›i\nsus\nsutÄƒ\nsÃ®nt\nsÃ®ntem\nsÃ®nteÅ£i\nsÃ®nteÈ›i\nsÄƒ\nsÄƒi\nsÄƒu\nt\nta\ntale\nte\nti\ntimp\ntine\ntoata\ntoate\ntoatÄƒ\ntocmai\ntot\ntoti\ntotul\ntotusi\ntotuÅŸi\ntotuÈ™i\ntoÅ£i\ntoÈ›i\ntrei\ntreia\ntreilea\ntu\ntuturor\ntÄƒi\ntÄƒu\nu\nul\nului\nun\nuna\nunde\nundeva\nunei\nuneia\nunele\nuneori\nunii\nunor\nunora\nunu\nunui\nunuia\nunul\nv\nva\nvai\nvi\nvoastre\nvoastrÄƒ\nvoi\nvom\nvor\nvostru\nvouÄƒ\nvoÅŸtri\nvoÈ™tri\nvreme\nvreo\nvreun\nvÄƒ\nx\nz\nzece\nzero\nzi\nzice\nÃ®i\nÃ®l\nÃ®mi\nÃ®mpotriva\nÃ®n\nÃ®nainte\nÃ®naintea\nÃ®ncotro\nÃ®ncÃ¢t\nÃ®ncÃ®t\nÃ®ntre\nÃ®ntrucÃ¢t\nÃ®ntrucÃ®t\nÃ®Å£i\nÃ®È›i\nÄƒla\nÄƒlea\nÄƒsta\nÄƒstea\nÄƒÅŸtia\nÄƒÈ™tia\nÅŸapte\nÅŸase\nÅŸi\nÅŸtiu\nÅ£i\nÅ£ie\nÈ™apte\nÈ™ase\nÈ™i\nÈ™tiu\nÈ›i\nÈ›ie\n'.split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ro/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.ro.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ro/__init__.py----------------------------------------
spacy.lang.ro.__init__.Romanian(Language)
spacy.lang.ro.__init__.RomanianDefaults(BaseDefaults)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ro/lex_attrs.py----------------------------------------
A:spacy.lang.ro.lex_attrs._num_words->set('\nzero unu doi douÄƒ trei patru cinci È™ase È™apte opt nouÄƒ zece\nunsprezece doisprezece douÄƒsprezece treisprezece patrusprezece cincisprezece È™aisprezece È™aptesprezece optsprezece nouÄƒsprezece\ndouÄƒzeci treizeci patruzeci cincizeci È™aizeci È™aptezeci optzeci nouÄƒzeci\nsutÄƒ mie milion miliard bilion trilion cvadrilion catralion cvintilion sextilion septilion enÈ™pemii\n'.split())
A:spacy.lang.ro.lex_attrs._ordinal_words->set('\nprimul doilea treilea patrulea cincilea È™aselea È™aptelea optulea nouÄƒlea zecelea\nprima doua treia patra cincia È™asea È™aptea opta noua zecea\nunsprezecelea doisprezecelea treisprezecelea patrusprezecelea cincisprezecelea È™aisprezecelea È™aptesprezecelea optsprezecelea nouÄƒsprezecelea\nunsprezecea douÄƒsprezecea treisprezecea patrusprezecea cincisprezecea È™aisprezecea È™aptesprezecea optsprezecea nouÄƒsprezecea\ndouÄƒzecilea treizecilea patruzecilea cincizecilea È™aizecilea È™aptezecilea optzecilea nouÄƒzecilea sutÄƒlea\ndouÄƒzecea treizecea patruzecea cincizecea È™aizecea È™aptezecea optzecea nouÄƒzecea suta\nmiilea mielea mia milionulea milioana miliardulea miliardelea miliarda enÈ™pemia\n'.split())
A:spacy.lang.ro.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.ro.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.ro.lex_attrs.like_num(text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ro/punctuation.py----------------------------------------
A:spacy.lang.ro.punctuation.upper_token->token.upper()
A:spacy.lang.ro.punctuation._ud_rrt_prefix_variants->_make_ro_variants(_ud_rrt_prefixes)
A:spacy.lang.ro.punctuation._ud_rrt_suffix_variants->_make_ro_variants(_ud_rrt_suffixes)
spacy.lang.ro.punctuation._make_ro_variants(tokens)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/en/examples.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/en/lemmatizer.py----------------------------------------
A:spacy.lang.en.lemmatizer.univ_pos->token.pos_.lower()
A:spacy.lang.en.lemmatizer.morphology->token.morph.to_dict()
spacy.lang.en.EnglishLemmatizer(Lemmatizer)
spacy.lang.en.lemmatizer.EnglishLemmatizer(Lemmatizer)
spacy.lang.en.lemmatizer.EnglishLemmatizer.is_base_form(self,token:Token)->bool


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/en/stop_words.py----------------------------------------
A:spacy.lang.en.stop_words.STOP_WORDS->set('\na about above across after afterwards again against all almost alone along\nalready also although always am among amongst amount an and another any anyhow\nanyone anything anyway anywhere are around as at\n\nback be became because become becomes becoming been before beforehand behind\nbeing below beside besides between beyond both bottom but by\n\ncall can cannot ca could\n\ndid do does doing done down due during\n\neach eight either eleven else elsewhere empty enough even ever every\neveryone everything everywhere except\n\nfew fifteen fifty first five for former formerly forty four from front full\nfurther\n\nget give go\n\nhad has have he hence her here hereafter hereby herein hereupon hers herself\nhim himself his how however hundred\n\ni if in indeed into is it its itself\n\nkeep\n\nlast latter latterly least less\n\njust\n\nmade make many may me meanwhile might mine more moreover most mostly move much\nmust my myself\n\nname namely neither never nevertheless next nine no nobody none noone nor not\nnothing now nowhere\n\nof off often on once one only onto or other others otherwise our ours ourselves\nout over own\n\npart per perhaps please put\n\nquite\n\nrather re really regarding\n\nsame say see seem seemed seeming seems serious several she should show side\nsince six sixty so some somehow someone something sometime sometimes somewhere\nstill such\n\ntake ten than that the their them themselves then thence there thereafter\nthereby therefore therein thereupon these they third this those though three\nthrough throughout thru thus to together too top toward towards twelve twenty\ntwo\n\nunder until up unless upon us used using\n\nvarious very very via was we well were what whatever when whence whenever where\nwhereafter whereas whereby wherein whereupon wherever whether which while\nwhither who whoever whole whom whose why will with within without would\n\nyet you your yours yourself yourselves\n'.split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/en/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.en.tokenizer_exceptions.verb_data_tc->dict(verb_data)
A:spacy.lang.en.tokenizer_exceptions.verb_data_tc[ORTH]->verb_data_tc[ORTH].title().title()
A:spacy.lang.en.tokenizer_exceptions.exc_data_tc->dict(exc_data)
A:spacy.lang.en.tokenizer_exceptions.exc_data_tc[ORTH]->exc_data_tc[ORTH].title().title()
A:spacy.lang.en.tokenizer_exceptions.data_apos->dict(data)
A:spacy.lang.en.tokenizer_exceptions.exc_data_apos->dict(exc_data)
A:spacy.lang.en.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/en/__init__.py----------------------------------------
spacy.lang.en.__init__.English(Language)
spacy.lang.en.__init__.EnglishDefaults(BaseDefaults)
spacy.lang.en.__init__.make_lemmatizer(nlp:Language,model:Optional[Model],name:str,mode:str,overwrite:bool,scorer:Optional[Callable])


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/en/lex_attrs.py----------------------------------------
A:spacy.lang.en.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.en.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
A:spacy.lang.en.lex_attrs.text_lower->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').lower()
spacy.lang.en.lex_attrs.like_num(text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/en/syntax_iterators.py----------------------------------------
A:spacy.lang.en.syntax_iterators.conj->doc.vocab.strings.add('conj')
A:spacy.lang.en.syntax_iterators.np_label->doc.vocab.strings.add('NP')
spacy.lang.en.syntax_iterators.noun_chunks(doclike:Union[Doc,Span])->Iterator[Tuple[int, int, int]]


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/en/punctuation.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/hsb/examples.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/hsb/stop_words.py----------------------------------------
A:spacy.lang.hsb.stop_words.STOP_WORDS->set('\na abo ale ani\n\ndokelÅ¾\n\nhdyÅ¾\n\njeli jelizo\n\nkaÅ¾\n\npak potom\n\nteÅ¾ tohodla\n\nzo zoby\n'.split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/hsb/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.hsb.tokenizer_exceptions._exc->dict()
A:spacy.lang.hsb.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/hsb/__init__.py----------------------------------------
spacy.lang.hsb.__init__.UpperSorbian(Language)
spacy.lang.hsb.__init__.UpperSorbianDefaults(BaseDefaults)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/hsb/lex_attrs.py----------------------------------------
A:spacy.lang.hsb.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.hsb.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
A:spacy.lang.hsb.lex_attrs.text_lower->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').lower()
spacy.lang.hsb.lex_attrs.like_num(text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ru/examples.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ru/lemmatizer.py----------------------------------------
A:spacy.lang.ru.lemmatizer.self._morph->MorphAnalyzer(lang='ru')
A:spacy.lang.ru.lemmatizer.morphology->dict()
A:spacy.lang.ru.lemmatizer.analyses->self._morph.parse(string)
A:spacy.lang.ru.lemmatizer.(analysis_pos, _)->oc2ud(str(analysis.tag))
A:spacy.lang.ru.lemmatizer.(_, analysis_morph)->oc2ud(str(analysis.tag))
A:spacy.lang.ru.lemmatizer.normal_forms->set([an.normal_form for an in analyses])
A:spacy.lang.ru.lemmatizer.unmatched->set()
A:spacy.lang.ru.lemmatizer.grams->oc_tag.replace(' ', ',').split(',')
A:spacy.lang.ru.lemmatizer.gram->set().pop()
spacy.lang.ru.RussianLemmatizer(self,vocab:Vocab,model:Optional[Model],name:str='lemmatizer',*,mode:str='pymorphy3',overwrite:bool=False,scorer:Optional[Callable]=lemmatizer_score)
spacy.lang.ru.lemmatizer.RussianLemmatizer(self,vocab:Vocab,model:Optional[Model],name:str='lemmatizer',*,mode:str='pymorphy3',overwrite:bool=False,scorer:Optional[Callable]=lemmatizer_score)
spacy.lang.ru.lemmatizer.RussianLemmatizer._pymorphy_lemmatize(self,token:Token)->List[str]
spacy.lang.ru.lemmatizer.RussianLemmatizer._pymorphy_lookup_lemmatize(self,token:Token)->List[str]
spacy.lang.ru.lemmatizer.RussianLemmatizer.pymorphy2_lemmatize(self,token:Token)->List[str]
spacy.lang.ru.lemmatizer.RussianLemmatizer.pymorphy2_lookup_lemmatize(self,token:Token)->List[str]
spacy.lang.ru.lemmatizer.RussianLemmatizer.pymorphy3_lemmatize(self,token:Token)->List[str]
spacy.lang.ru.lemmatizer.RussianLemmatizer.pymorphy3_lookup_lemmatize(self,token:Token)->List[str]
spacy.lang.ru.lemmatizer.oc2ud(oc_tag:str)->Tuple[str, Dict[str, str]]


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ru/stop_words.py----------------------------------------
A:spacy.lang.ru.stop_words.STOP_WORDS->set('\nÐ° Ð°Ð²Ð¾ÑÑŒ Ð°Ð³Ð° Ð°Ð³Ñƒ Ð°Ð¶ Ð°Ð¹ Ð°Ð»Ð¸ Ð°Ð»Ð»Ð¾ Ð°Ñƒ Ð°Ñ… Ð°Ñ\n\nÐ± Ð±ÑƒÐ´ÐµÐ¼ Ð±ÑƒÐ´ÐµÑ‚ Ð±ÑƒÐ´ÐµÑ‚Ðµ Ð±ÑƒÐ´ÐµÑˆÑŒ Ð±ÑƒÐ´Ñƒ Ð±ÑƒÐ´ÑƒÑ‚ Ð±ÑƒÐ´ÑƒÑ‡Ð¸ Ð±ÑƒÐ´ÑŒ Ð±ÑƒÐ´ÑŒÑ‚Ðµ Ð±Ñ‹ Ð±Ñ‹Ð» Ð±Ñ‹Ð»Ð° Ð±Ñ‹Ð»Ð¸ Ð±Ñ‹Ð»Ð¾\nÐ±Ñ‹Ñ‚ÑŒ Ð±Ð°Ñ† Ð±ÐµÐ· Ð±ÐµÐ·ÑƒÑÐ»Ð¾Ð²Ð½Ð¾ Ð±Ð¸ÑˆÑŒ Ð±Ð»Ð°Ð³Ð¾ Ð±Ð»Ð°Ð³Ð¾Ð´Ð°Ñ€Ñ Ð±Ð»Ð¸Ð¶Ð°Ð¹ÑˆÐ¸Ðµ Ð±Ð»Ð¸Ð·ÐºÐ¾ Ð±Ð¾Ð»ÐµÐµ Ð±Ð¾Ð»ÑŒÑˆÐµ\nÐ±ÑƒÐ´Ñ‚Ð¾ Ð±Ñ‹Ð²Ð°ÐµÑ‚ Ð±Ñ‹Ð²Ð°Ð»Ð° Ð±Ñ‹Ð²Ð°Ð»Ð¸ Ð±Ñ‹Ð²Ð°ÑŽ Ð±Ñ‹Ð²Ð°ÑŽÑ‚ Ð±Ñ‹Ñ‚ÑƒÐµÑ‚\n\nÐ² Ð²Ð°Ð¼ Ð²Ð°Ð¼Ð¸ Ð²Ð°Ñ Ð²ÐµÑÑŒ Ð²Ð¾ Ð²Ð¾Ñ‚ Ð²ÑÐµ Ð²ÑÑ‘ Ð²ÑÐµÐ³Ð¾ Ð²ÑÐµÐ¹ Ð²ÑÐµÐ¼ Ð²ÑÑ‘Ð¼ Ð²ÑÐµÐ¼Ð¸ Ð²ÑÐµÐ¼Ñƒ Ð²ÑÐµÑ… Ð²ÑÐµÑŽ\nÐ²ÑÐµÑ Ð²ÑÑŽ Ð²ÑÑ Ð²Ñ‹ Ð²Ð°Ñˆ Ð²Ð°ÑˆÐ° Ð²Ð°ÑˆÐµ Ð²Ð°ÑˆÐ¸ Ð²Ð´Ð°Ð»Ð¸ Ð²Ð´Ð¾Ð±Ð°Ð²Ð¾Ðº Ð²Ð´Ñ€ÑƒÐ³ Ð²ÐµÐ´ÑŒ Ð²ÐµÐ·Ð´Ðµ Ð²ÐµÑ€Ð½ÐµÐµ\nÐ²Ð·Ð°Ð¸Ð¼Ð½Ð¾ Ð²Ð·Ð°Ð¿Ñ€Ð°Ð²Ð´Ñƒ Ð²Ð¸Ð´Ð½Ð¾ Ð²Ð¸ÑˆÑŒ Ð²ÐºÐ»ÑŽÑ‡Ð°Ñ Ð²Ð¼ÐµÑÑ‚Ð¾ Ð²Ð½Ð°ÐºÐ»Ð°Ð´Ðµ Ð²Ð½Ð°Ñ‡Ð°Ð»Ðµ Ð²Ð½Ðµ Ð²Ð½Ð¸Ð· Ð²Ð½Ð¸Ð·Ñƒ\nÐ²Ð½Ð¾Ð²ÑŒ Ð²Ð¾Ð²ÑÐµ Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾ Ð²Ð¾Ð¸ÑÑ‚Ð¸Ð½Ñƒ Ð²Ð¾ÐºÑ€ÑƒÐ³ Ð²Ð¾Ð½ Ð²Ð¾Ð¾Ð±Ñ‰Ðµ Ð²Ð¾Ð¿Ñ€ÐµÐºÐ¸ Ð²Ð¿ÐµÑ€ÐµÐºÐ¾Ñ€ Ð²Ð¿Ð»Ð¾Ñ‚ÑŒ\nÐ²Ð¿Ð¾Ð»Ð½Ðµ Ð²Ð¿Ñ€Ð°Ð²Ð´Ñƒ Ð²Ð¿Ñ€Ð°Ð²Ðµ Ð²Ð¿Ñ€Ð¾Ñ‡ÐµÐ¼ Ð²Ð¿Ñ€ÑÐ¼ÑŒ Ð²Ñ€ÐµÑÐ½Ð¾Ñ‚Ñƒ Ð²Ñ€Ð¾Ð´Ðµ Ð²Ñ€ÑÐ´ Ð²ÑÐµÐ³Ð´Ð° Ð²ÑÑŽÐ´Ñƒ\nÐ²ÑÑÐºÐ¸Ð¹ Ð²ÑÑÐºÐ¾Ð³Ð¾ Ð²ÑÑÐºÐ¾Ð¹ Ð²ÑÑÑ‡ÐµÑÐºÐ¸ Ð²Ñ‡ÐµÑ€ÐµÐ´\n\nÐ³ Ð³Ð¾ Ð³Ð´Ðµ Ð³Ð¾Ñ€Ð°Ð·Ð´Ð¾ Ð³Ð°Ð²\n\nÐ´ Ð´Ð° Ð´Ð»Ñ Ð´Ð¾ Ð´Ð°Ð±Ñ‹ Ð´Ð°Ð²Ð°Ð¹Ñ‚Ðµ Ð´Ð°Ð²Ð½Ð¾ Ð´Ð°Ð²Ð½Ñ‹Ð¼ Ð´Ð°Ð¶Ðµ Ð´Ð°Ð»ÐµÐµ Ð´Ð°Ð»ÐµÐºÐ¾ Ð´Ð°Ð»ÑŒÑˆÐµ Ð´Ð°Ð½Ð½Ð°Ñ\nÐ´Ð°Ð½Ð½Ð¾Ð³Ð¾ Ð´Ð°Ð½Ð½Ð¾Ðµ Ð´Ð°Ð½Ð½Ð¾Ð¹ Ð´Ð°Ð½Ð½Ð¾Ð¼ Ð´Ð°Ð½Ð½Ð¾Ð¼Ñƒ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ð¹ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð°Ð½Ñƒ Ð´Ð°Ð½ÑƒÐ½Ð°Ñ…\nÐ´Ð°Ñ€Ð¾Ð¼ Ð´Ðµ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ð´Ð¾Ð²Ð¾Ð»ÑŒÐ½Ð¾ Ð´Ð¾ÐºÐ¾Ð»Ðµ Ð´Ð¾ÐºÐ¾Ð»ÑŒ Ð´Ð¾Ð»Ð³Ð¾ Ð´Ð¾Ð»Ð¶ÐµÐ½ Ð´Ð¾Ð»Ð¶Ð½Ð°\nÐ´Ð¾Ð»Ð¶Ð½Ð¾ Ð´Ð¾Ð»Ð¶Ð½Ñ‹ Ð´Ð¾Ð»Ð¶Ð½Ñ‹Ð¹ Ð´Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ð´Ñ€ÑƒÐ³Ð°Ñ Ð´Ñ€ÑƒÐ³Ð¸Ðµ Ð´Ñ€ÑƒÐ³Ð¸Ð¼ Ð´Ñ€ÑƒÐ³Ð¸Ð¼Ð¸\nÐ´Ñ€ÑƒÐ³Ð¸Ñ… Ð´Ñ€ÑƒÐ³Ð¾Ðµ Ð´Ñ€ÑƒÐ³Ð¾Ð¹\n\nÐµ ÐµÐ³Ð¾ ÐµÐ´Ð¸Ð¼ ÐµÐ´ÑÑ‚ ÐµÐµ ÐµÑ‘ ÐµÐ¹ ÐµÐ» ÐµÐ»Ð° ÐµÐ¼ ÐµÐ¼Ñƒ ÐµÐ¼ÑŠ ÐµÑÐ»Ð¸ ÐµÑÑ‚ ÐµÑÑ‚ÑŒ ÐµÑˆÑŒ ÐµÑ‰Ðµ ÐµÑ‰Ñ‘ ÐµÑŽ ÐµÐ´Ð²Ð°\nÐµÐ¶ÐµÐ»Ð¸ ÐµÐ»Ðµ\n\nÐ¶ Ð¶Ðµ\n\nÐ· Ð·Ð° Ð·Ð°Ñ‚ÐµÐ¼ Ð·Ð°Ñ‚Ð¾ Ð·Ð°Ñ‡ÐµÐ¼ Ð·Ð´ÐµÑÑŒ Ð·Ð½Ð°Ñ‡Ð¸Ñ‚ Ð·Ñ€Ñ\n\nÐ¸ Ð¸Ð· Ð¸Ð»Ð¸ Ð¸Ð¼ Ð¸Ð¼Ð¸ Ð¸Ð¼ÑŠ Ð¸Ñ… Ð¸Ð±Ð¾ Ð¸Ð»ÑŒ Ð¸Ð¼ÐµÐµÑ‚ Ð¸Ð¼ÐµÐ» Ð¸Ð¼ÐµÐ»Ð° Ð¸Ð¼ÐµÐ»Ð¾ Ð¸Ð¼ÐµÐ½Ð½Ð¾ Ð¸Ð¼ÐµÑ‚ÑŒ Ð¸Ð½Ð°Ñ‡Ðµ\nÐ¸Ð½Ð¾Ð³Ð´Ð° Ð¸Ð½Ñ‹Ð¼ Ð¸Ð½Ñ‹Ð¼Ð¸ Ð¸Ñ‚Ð°Ðº Ð¸ÑˆÑŒ\n\nÐ¹\n\nÐº ÐºÐ°Ðº ÐºÐµÐ¼ ÐºÐ¾ ÐºÐ¾Ð³Ð´Ð° ÐºÐ¾Ð³Ð¾ ÐºÐ¾Ð¼ ÐºÐ¾Ð¼Ñƒ ÐºÐ¾Ð¼ÑŒÑ ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ ÐºÐ¾Ñ‚Ð¾Ñ€Ð¾Ð³Ð¾ ÐºÐ¾Ñ‚Ð¾Ñ€Ð¾Ðµ ÐºÐ¾Ñ‚Ð¾Ñ€Ð¾Ð¹ ÐºÐ¾Ñ‚Ð¾Ñ€Ð¾Ð¼\nÐºÐ¾Ñ‚Ð¾Ñ€Ð¾Ð¼Ñƒ ÐºÐ¾Ñ‚Ð¾Ñ€Ð¾ÑŽ ÐºÐ¾Ñ‚Ð¾Ñ€ÑƒÑŽ ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¼ ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¼Ð¸ ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ñ… ÐºÑ‚Ð¾ ÐºÐ° ÐºÐ°Ð±Ñ‹\nÐºÐ°Ð¶Ð´Ð°Ñ ÐºÐ°Ð¶Ð´Ð¾Ðµ ÐºÐ°Ð¶Ð´Ñ‹Ðµ ÐºÐ°Ð¶Ð´Ñ‹Ð¹ ÐºÐ°Ð¶ÐµÑ‚ÑÑ ÐºÐ°Ð·Ð°Ð»Ð°ÑÑŒ ÐºÐ°Ð·Ð°Ð»Ð¸ÑÑŒ ÐºÐ°Ð·Ð°Ð»Ð¾ÑÑŒ ÐºÐ°Ð·Ð°Ð»ÑÑ ÐºÐ°Ð·Ð°Ñ‚ÑŒÑÑ\nÐºÐ°ÐºÐ°Ñ ÐºÐ°ÐºÐ¸Ðµ ÐºÐ°ÐºÐ¸Ð¼ ÐºÐ°ÐºÐ¸Ð¼Ð¸ ÐºÐ°ÐºÐ¾Ð² ÐºÐ°ÐºÐ¾Ð³Ð¾ ÐºÐ°ÐºÐ¾Ð¹ ÐºÐ°ÐºÐ¾Ð¼Ñƒ ÐºÐ°ÐºÐ¾ÑŽ ÐºÐ°ÑÐ°Ñ‚ÐµÐ»ÑŒÐ½Ð¾ ÐºÐ¾Ð¹ ÐºÐ¾Ð»Ð¸\nÐºÐ¾Ð»ÑŒ ÐºÐ¾Ð½ÐµÑ‡Ð½Ð¾ ÐºÐ¾Ñ€Ð¾Ñ‡Ðµ ÐºÑ€Ð¾Ð¼Ðµ ÐºÑÑ‚Ð°Ñ‚Ð¸ ÐºÑƒ ÐºÑƒÐ´Ð°\n\nÐ» Ð»Ð¸ Ð»Ð¸Ð±Ð¾ Ð»Ð¸ÑˆÑŒ Ð»ÑŽÐ±Ð°Ñ Ð»ÑŽÐ±Ð¾Ð³Ð¾ Ð»ÑŽÐ±Ð¾Ðµ Ð»ÑŽÐ±Ð¾Ð¹ Ð»ÑŽÐ±Ð¾Ð¼ Ð»ÑŽÐ±ÑƒÑŽ Ð»ÑŽÐ±Ñ‹Ð¼Ð¸ Ð»ÑŽÐ±Ñ‹Ñ…\n\nÐ¼ Ð¼ÐµÐ½Ñ Ð¼Ð½Ðµ Ð¼Ð½Ð¾Ð¹ Ð¼Ð½Ð¾ÑŽ Ð¼Ð¾Ð³ Ð¼Ð¾Ð³Ð¸ Ð¼Ð¾Ð³Ð¸Ñ‚Ðµ Ð¼Ð¾Ð³Ð»Ð° Ð¼Ð¾Ð³Ð»Ð¸ Ð¼Ð¾Ð³Ð»Ð¾ Ð¼Ð¾Ð³Ñƒ Ð¼Ð¾Ð³ÑƒÑ‚ Ð¼Ð¾Ðµ Ð¼Ð¾Ñ‘ Ð¼Ð¾ÐµÐ³Ð¾\nÐ¼Ð¾ÐµÐ¹ Ð¼Ð¾ÐµÐ¼ Ð¼Ð¾Ñ‘Ð¼ Ð¼Ð¾ÐµÐ¼Ñƒ Ð¼Ð¾ÐµÑŽ Ð¼Ð¾Ð¶ÐµÐ¼ Ð¼Ð¾Ð¶ÐµÑ‚ Ð¼Ð¾Ð¶ÐµÑ‚Ðµ Ð¼Ð¾Ð¶ÐµÑˆÑŒ Ð¼Ð¾Ð¸ Ð¼Ð¾Ð¹ Ð¼Ð¾Ð¸Ð¼ Ð¼Ð¾Ð¸Ð¼Ð¸ Ð¼Ð¾Ð¸Ñ…\nÐ¼Ð¾Ñ‡ÑŒ Ð¼Ð¾ÑŽ Ð¼Ð¾Ñ Ð¼Ñ‹ Ð¼Ð°Ð»Ð¾ Ð¼ÐµÐ¶ Ð¼ÐµÐ¶Ð´Ñƒ Ð¼ÐµÐ½ÐµÐµ Ð¼ÐµÐ½ÑŒÑˆÐµ Ð¼Ð¸Ð¼Ð¾ Ð¼Ð½Ð¾Ð³Ð¸Ðµ Ð¼Ð½Ð¾Ð³Ð¾ Ð¼Ð½Ð¾Ð³Ð¾Ð³Ð¾ Ð¼Ð½Ð¾Ð³Ð¾Ðµ\nÐ¼Ð½Ð¾Ð³Ð¾Ð¼ Ð¼Ð½Ð¾Ð³Ð¾Ð¼Ñƒ Ð¼Ð¾Ð¶Ð½Ð¾ Ð¼Ð¾Ð» Ð¼Ñƒ\n\nÐ½ Ð½Ð° Ð½Ð°Ð¼ Ð½Ð°Ð¼Ð¸ Ð½Ð°Ñ Ð½Ð°ÑÐ° Ð½Ð°Ñˆ Ð½Ð°ÑˆÐ° Ð½Ð°ÑˆÐµ Ð½Ð°ÑˆÐµÐ³Ð¾ Ð½Ð°ÑˆÐµÐ¹ Ð½Ð°ÑˆÐµÐ¼ Ð½Ð°ÑˆÐµÐ¼Ñƒ Ð½Ð°ÑˆÐµÑŽ Ð½Ð°ÑˆÐ¸ Ð½Ð°ÑˆÐ¸Ð¼\nÐ½Ð°ÑˆÐ¸Ð¼Ð¸ Ð½Ð°ÑˆÐ¸Ñ… Ð½Ð°ÑˆÑƒ Ð½Ðµ Ð½ÐµÐ³Ð¾ Ð½ÐµÐµ Ð½ÐµÑ‘ Ð½ÐµÐ¹ Ð½ÐµÐ¼ Ð½Ñ‘Ð¼ Ð½ÐµÐ¼Ñƒ Ð½ÐµÑ‚ Ð½ÐµÑŽ Ð½Ð¸Ð¼ Ð½Ð¸Ð¼Ð¸ Ð½Ð¸Ñ… Ð½Ð¾\nÐ½Ð°Ð²ÐµÑ€Ð½ÑÐºÐ° Ð½Ð°Ð²ÐµÑ€Ñ…Ñƒ Ð½Ð°Ð²Ñ€ÑÐ´ Ð½Ð°Ð²Ñ‹Ð²Ð¾Ñ€Ð¾Ñ‚ Ð½Ð°Ð´ Ð½Ð°Ð´Ð¾ Ð½Ð°Ð·Ð°Ð´ Ð½Ð°Ð¸Ð±Ð¾Ð»ÐµÐµ Ð½Ð°Ð¸Ð·Ð²Ð¾Ñ€Ð¾Ñ‚\nÐ½Ð°Ð¸Ð·Ð½Ð°Ð½ÐºÑƒ Ð½Ð°Ð¸Ð¿Ð°Ñ‡Ðµ Ð½Ð°ÐºÐ°Ð½ÑƒÐ½Ðµ Ð½Ð°ÐºÐ¾Ð½ÐµÑ† Ð½Ð°Ð¾Ð±Ð¾Ñ€Ð¾Ñ‚ Ð½Ð°Ð¿ÐµÑ€ÐµÐ´ Ð½Ð°Ð¿ÐµÑ€ÐµÐºÐ¾Ñ€ Ð½Ð°Ð¿Ð¾Ð´Ð¾Ð±Ð¸Ðµ\nÐ½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€ Ð½Ð°Ð¿Ñ€Ð¾Ñ‚Ð¸Ð² Ð½Ð°Ð¿Ñ€ÑÐ¼ÑƒÑŽ Ð½Ð°ÑÐ¸Ð»Ñƒ Ð½Ð°ÑÑ‚Ð¾ÑÑ‰Ð°Ñ Ð½Ð°ÑÑ‚Ð¾ÑÑ‰ÐµÐµ Ð½Ð°ÑÑ‚Ð¾ÑÑ‰Ð¸Ðµ Ð½Ð°ÑÑ‚Ð¾ÑÑ‰Ð¸Ð¹\nÐ½Ð°ÑÑ‡ÐµÑ‚ Ð½Ð°Ñ‚Ðµ Ð½Ð°Ñ…Ð¾Ð´Ð¸Ñ‚ÑŒÑÑ Ð½Ð°Ñ‡Ð°Ð»Ð° Ð½Ð°Ñ‡Ð°Ð»Ðµ Ð½ÐµÐ²Ð°Ð¶Ð½Ð¾ Ð½ÐµÐ³Ð´Ðµ Ð½ÐµÐ´Ð°Ð²Ð½Ð¾ Ð½ÐµÐ´Ð°Ð»ÐµÐºÐ¾ Ð½ÐµÐ·Ð°Ñ‡ÐµÐ¼\nÐ½ÐµÐºÐµÐ¼ Ð½ÐµÐºÐ¾Ð³Ð´Ð° Ð½ÐµÐºÐ¾Ð¼Ñƒ Ð½ÐµÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ Ð½ÐµÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð½ÐµÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð½ÐµÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ñ… Ð½ÐµÐºÑ‚Ð¾ Ð½ÐµÐºÑƒÐ´Ð°\nÐ½ÐµÐ»ÑŒÐ·Ñ Ð½ÐµÐ¼Ð½Ð¾Ð³Ð¸Ðµ Ð½ÐµÐ¼Ð½Ð¾Ð³Ð¸Ð¼ Ð½ÐµÐ¼Ð½Ð¾Ð³Ð¾ Ð½ÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ð¾ Ð½ÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ð¾ÑÑ‚Ð¸ Ð½ÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ñ‹Ðµ\nÐ½ÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ñ‹Ð¼ Ð½ÐµÐ¾Ñ‚ÐºÑƒÐ´Ð° Ð½ÐµÐ¿Ñ€ÐµÑ€Ñ‹Ð²Ð½Ð¾ Ð½ÐµÑ€ÐµÐ´ÐºÐ¾ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¾ Ð½ÐµÑ‚Ñƒ Ð½ÐµÑƒÐ¶ÐµÐ»Ð¸ Ð½ÐµÑ‡ÐµÐ³Ð¾\nÐ½ÐµÑ‡ÐµÐ¼ Ð½ÐµÑ‡ÐµÐ¼Ñƒ Ð½ÐµÑ‡Ñ‚Ð¾ Ð½ÐµÑˆÑ‚Ð¾ Ð½Ð¸Ð±ÑƒÐ´ÑŒ Ð½Ð¸Ð³Ð´Ðµ Ð½Ð¸Ð¶Ðµ Ð½Ð¸Ð·ÐºÐ¾ Ð½Ð¸ÐºÐ°Ðº Ð½Ð¸ÐºÐ°ÐºÐ¾Ð¹ Ð½Ð¸ÐºÐµÐ¼\nÐ½Ð¸ÐºÐ¾Ð³Ð´Ð° Ð½Ð¸ÐºÐ¾Ð³Ð¾ Ð½Ð¸ÐºÐ¾Ð¼Ñƒ Ð½Ð¸ÐºÑ‚Ð¾ Ð½Ð¸ÐºÑƒÐ´Ð° Ð½Ð¸Ð¾Ñ‚ÐºÑƒÐ´Ð° Ð½Ð¸Ð¿Ð¾Ñ‡ÐµÐ¼ Ð½Ð¸Ñ‡ÐµÐ³Ð¾ Ð½Ð¸Ñ‡ÐµÐ¼ Ð½Ð¸Ñ‡ÐµÐ¼Ñƒ\nÐ½Ð¸Ñ‡Ñ‚Ð¾ Ð½Ñƒ Ð½ÑƒÐ¶Ð½Ð°Ñ Ð½ÑƒÐ¶Ð½Ð¾ Ð½ÑƒÐ¶Ð½Ð¾Ð³Ð¾ Ð½ÑƒÐ¶Ð½Ñ‹Ðµ Ð½ÑƒÐ¶Ð½Ñ‹Ð¹ Ð½ÑƒÐ¶Ð½Ñ‹Ñ… Ð½Ñ‹Ð½Ðµ Ð½Ñ‹Ð½ÐµÑˆÐ½ÐµÐµ Ð½Ñ‹Ð½ÐµÑˆÐ½ÐµÐ¹\nÐ½Ñ‹Ð½ÐµÑˆÐ½Ð¸Ñ… Ð½Ñ‹Ð½Ñ‡Ðµ\n\nÐ¾ Ð¾Ð± Ð¾Ð´Ð¸Ð½ Ð¾Ð´Ð½Ð° Ð¾Ð´Ð½Ð¸ Ð¾Ð´Ð½Ð¸Ð¼ Ð¾Ð´Ð½Ð¸Ð¼Ð¸ Ð¾Ð´Ð½Ð¸Ñ… Ð¾Ð´Ð½Ð¾ Ð¾Ð´Ð½Ð¾Ð³Ð¾ Ð¾Ð´Ð½Ð¾Ð¹ Ð¾Ð´Ð½Ð¾Ð¼ Ð¾Ð´Ð½Ð¾Ð¼Ñƒ Ð¾Ð´Ð½Ð¾ÑŽ\nÐ¾Ð´Ð½Ñƒ Ð¾Ð½ Ð¾Ð½Ð° Ð¾Ð½Ðµ Ð¾Ð½Ð¸ Ð¾Ð½Ð¾ Ð¾Ñ‚ Ð¾Ð±Ð° Ð¾Ð±Ñ‰ÑƒÑŽ Ð¾Ð±Ñ‹Ñ‡Ð½Ð¾ Ð¾Ð³Ð¾ Ð¾Ð´Ð½Ð°Ð¶Ð´Ñ‹ Ð¾Ð´Ð½Ð°ÐºÐ¾ Ð¾Ð¹ Ð¾ÐºÐ¾Ð»Ð¾ Ð¾Ð½Ñ‹Ð¹\nÐ¾Ð¿ Ð¾Ð¿ÑÑ‚ÑŒ Ð¾ÑÐ¾Ð±ÐµÐ½Ð½Ð¾ Ð¾ÑÐ¾Ð±Ð¾ Ð¾ÑÐ¾Ð±ÑƒÑŽ Ð¾ÑÐ¾Ð±Ñ‹Ðµ Ð¾Ñ‚ÐºÑƒÐ´Ð° Ð¾Ñ‚Ð½ÐµÐ»Ð¸Ð¶Ð° Ð¾Ñ‚Ð½ÐµÐ»Ð¸Ð¶Ðµ Ð¾Ñ‚Ð¾Ð²ÑÑŽÐ´Ñƒ\nÐ¾Ñ‚ÑÑŽÐ´Ð° Ð¾Ñ‚Ñ‚Ð¾Ð³Ð¾ Ð¾Ñ‚Ñ‚Ð¾Ñ‚ Ð¾Ñ‚Ñ‚ÑƒÐ´Ð° Ð¾Ñ‚Ñ‡ÐµÐ³Ð¾ Ð¾Ñ‚Ñ‡ÐµÐ¼Ñƒ Ð¾Ñ… Ð¾Ñ‡ÐµÐ²Ð¸Ð´Ð½Ð¾ Ð¾Ñ‡ÐµÐ½ÑŒ Ð¾Ð¼\n\nÐ¿ Ð¿Ð¾ Ð¿Ñ€Ð¸ Ð¿Ð°Ñ‡Ðµ Ð¿ÐµÑ€ÐµÐ´ Ð¿Ð¾Ð´ Ð¿Ð¾Ð´Ð°Ð²Ð½Ð¾ Ð¿Ð¾Ð´Ð¸ Ð¿Ð¾Ð´Ð¾Ð±Ð½Ð°Ñ Ð¿Ð¾Ð´Ð¾Ð±Ð½Ð¾ Ð¿Ð¾Ð´Ð¾Ð±Ð½Ð¾Ð³Ð¾ Ð¿Ð¾Ð´Ð¾Ð±Ð½Ñ‹Ðµ\nÐ¿Ð¾Ð´Ð¾Ð±Ð½Ñ‹Ð¹ Ð¿Ð¾Ð´Ð¾Ð±Ð½Ñ‹Ð¼ Ð¿Ð¾Ð´Ð¾Ð±Ð½Ñ‹Ñ… Ð¿Ð¾ÐµÐ»Ð¸ÐºÑƒ Ð¿Ð¾Ð¶Ð°Ð»ÑƒÐ¹ Ð¿Ð¾Ð¶Ð°Ð»ÑƒÐ¹ÑÑ‚Ð° Ð¿Ð¾Ð·Ð¶Ðµ Ð¿Ð¾Ð¸ÑÑ‚Ð¸Ð½Ðµ\nÐ¿Ð¾ÐºÐ° Ð¿Ð¾ÐºÐ°Ð¼ÐµÑÑ‚ Ð¿Ð¾ÐºÐ¾Ð»Ðµ Ð¿Ð¾ÐºÐ¾Ð»ÑŒ Ð¿Ð¾ÐºÑƒÐ´Ð° Ð¿Ð¾ÐºÑƒÐ´Ð¾Ð²Ð° Ð¿Ð¾Ð¼Ð¸Ð¼Ð¾ Ð¿Ð¾Ð½ÐµÐ¶Ðµ Ð¿Ð¾Ð¿Ñ€Ð¸Ñ‰Ðµ Ð¿Ð¾Ñ€\nÐ¿Ð¾Ñ€Ð° Ð¿Ð¾ÑÐµÐ¼Ñƒ Ð¿Ð¾ÑÐºÐ¾Ð»ÑŒÐºÑƒ Ð¿Ð¾ÑÐ»Ðµ Ð¿Ð¾ÑÑ€ÐµÐ´Ð¸ Ð¿Ð¾ÑÑ€ÐµÐ´ÑÑ‚Ð²Ð¾Ð¼ Ð¿Ð¾Ñ‚Ð¾Ð¼ Ð¿Ð¾Ñ‚Ð¾Ð¼Ñƒ Ð¿Ð¾Ñ‚Ð¾Ð¼ÑƒÑˆÑ‚Ð°\nÐ¿Ð¾Ñ…Ð¾Ð¶ÐµÐ¼ Ð¿Ð¾Ñ‡ÐµÐ¼Ñƒ Ð¿Ð¾Ñ‡Ñ‚Ð¸ Ð¿Ð¾ÑÑ‚Ð¾Ð¼Ñƒ Ð¿Ñ€ÐµÐ¶Ð´Ðµ Ð¿Ñ€Ð¸Ñ‚Ð¾Ð¼ Ð¿Ñ€Ð¸Ñ‡ÐµÐ¼ Ð¿Ñ€Ð¾ Ð¿Ñ€Ð¾ÑÑ‚Ð¾ Ð¿Ñ€Ð¾Ñ‡ÐµÐ³Ð¾\nÐ¿Ñ€Ð¾Ñ‡ÐµÐµ Ð¿Ñ€Ð¾Ñ‡ÐµÐ¼Ñƒ Ð¿Ñ€Ð¾Ñ‡Ð¸Ð¼Ð¸ Ð¿Ñ€Ð¾Ñ‰Ðµ Ð¿Ñ€ÑÐ¼ Ð¿ÑƒÑÑ‚ÑŒ\n\nÑ€ Ñ€Ð°Ð´Ð¸ Ñ€Ð°Ð·Ð²Ðµ Ñ€Ð°Ð½ÐµÐµ Ñ€Ð°Ð½Ð¾ Ñ€Ð°Ð½ÑŒÑˆÐµ Ñ€ÑÐ´Ð¾Ð¼\n\nÑ ÑÐ°Ð¼ ÑÐ°Ð¼Ð° ÑÐ°Ð¼Ð¸ ÑÐ°Ð¼Ð¸Ð¼ ÑÐ°Ð¼Ð¸Ð¼Ð¸ ÑÐ°Ð¼Ð¸Ñ… ÑÐ°Ð¼Ð¾ ÑÐ°Ð¼Ð¾Ð³Ð¾ ÑÐ°Ð¼Ð¾Ð¼ ÑÐ°Ð¼Ð¾Ð¼Ñƒ ÑÐ°Ð¼Ñƒ ÑÐ²Ð¾Ðµ ÑÐ²Ð¾Ñ‘\nÑÐ²Ð¾ÐµÐ³Ð¾ ÑÐ²Ð¾ÐµÐ¹ ÑÐ²Ð¾ÐµÐ¼ ÑÐ²Ð¾Ñ‘Ð¼ ÑÐ²Ð¾ÐµÐ¼Ñƒ ÑÐ²Ð¾ÐµÑŽ ÑÐ²Ð¾Ð¸ ÑÐ²Ð¾Ð¹ ÑÐ²Ð¾Ð¸Ð¼ ÑÐ²Ð¾Ð¸Ð¼Ð¸ ÑÐ²Ð¾Ð¸Ñ… ÑÐ²Ð¾ÑŽ ÑÐ²Ð¾Ñ\nÑÐµÐ±Ðµ ÑÐµÐ±Ñ ÑÐ¾Ð±Ð¾Ð¹ ÑÐ¾Ð±Ð¾ÑŽ ÑÐ°Ð¼Ð°Ñ ÑÐ°Ð¼Ð¾Ðµ ÑÐ°Ð¼Ð¾Ð¹ ÑÐ°Ð¼Ñ‹Ð¹ ÑÐ°Ð¼Ñ‹Ñ… ÑÐ²ÐµÑ€Ñ… ÑÐ²Ñ‹ÑˆÐµ ÑÐµ ÑÐµÐ³Ð¾ ÑÐµÐ¹\nÑÐµÐ¹Ñ‡Ð°Ñ ÑÐ¸Ðµ ÑÐ¸Ñ… ÑÐºÐ²Ð¾Ð·ÑŒ ÑÐºÐ¾Ð»ÑŒÐºÐ¾ ÑÐºÐ¾Ñ€ÐµÐµ ÑÐºÐ¾Ñ€Ð¾ ÑÐ»ÐµÐ´ÑƒÐµÑ‚ ÑÐ»Ð¸ÑˆÐºÐ¾Ð¼ ÑÐ¼Ð¾Ð³ÑƒÑ‚ ÑÐ¼Ð¾Ð¶ÐµÑ‚\nÑÐ½Ð°Ñ‡Ð°Ð»Ð° ÑÐ½Ð¾Ð²Ð° ÑÐ¾ ÑÐ¾Ð±ÑÑ‚Ð²ÐµÐ½Ð½Ð¾ ÑÐ¾Ð²ÑÐµÐ¼ ÑÐ¿ÐµÑ€Ð²Ð° ÑÐ¿Ð¾ÐºÐ¾Ð½Ñƒ ÑÐ¿ÑƒÑÑ‚Ñ ÑÑ€Ð°Ð·Ñƒ ÑÑ€ÐµÐ´Ð¸ ÑÑ€Ð¾Ð´Ð½Ð¸\nÑÑ‚Ð°Ð» ÑÑ‚Ð°Ð»Ð° ÑÑ‚Ð°Ð»Ð¸ ÑÑ‚Ð°Ð»Ð¾ ÑÑ‚Ð°Ñ‚ÑŒ ÑÑƒÑ‚ÑŒ ÑÑ‹Ð·Ð½Ð¾Ð²Ð°\n\nÑ‚Ð° Ñ‚Ð¾ Ñ‚Ñƒ Ñ‚Ñ‹ Ñ‚Ð¸ Ñ‚Ð°Ðº Ñ‚Ð°ÐºÐ°Ñ Ñ‚Ð°ÐºÐ¸Ðµ Ñ‚Ð°ÐºÐ¸Ð¼ Ñ‚Ð°ÐºÐ¸Ð¼Ð¸ Ñ‚Ð°ÐºÐ¸Ñ… Ñ‚Ð°ÐºÐ¾Ð³Ð¾ Ñ‚Ð°ÐºÐ¾Ðµ Ñ‚Ð°ÐºÐ¾Ð¹ Ñ‚Ð°ÐºÐ¾Ð¼ Ñ‚Ð°ÐºÐ¾Ð¼Ñƒ Ñ‚Ð°ÐºÐ¾ÑŽ\nÑ‚Ð°ÐºÑƒÑŽ Ñ‚Ðµ Ñ‚ÐµÐ±Ðµ Ñ‚ÐµÐ±Ñ Ñ‚ÐµÐ¼ Ñ‚ÐµÐ¼Ð¸ Ñ‚ÐµÑ… Ñ‚Ð¾Ð±Ð¾Ð¹ Ñ‚Ð¾Ð±Ð¾ÑŽ Ñ‚Ð¾Ð³Ð¾ Ñ‚Ð¾Ð¹ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ñ‚Ð¾Ð¼ Ñ‚Ð¾Ð¼Ð°Ñ… Ñ‚Ð¾Ð¼Ñƒ\nÑ‚Ð¾Ñ‚ Ñ‚Ð¾ÑŽ Ñ‚Ð°ÐºÐ¶Ðµ Ñ‚Ð°ÐºÐ¸ Ñ‚Ð°ÐºÐ¾Ð² Ñ‚Ð°ÐºÐ¾Ð²Ð° Ñ‚Ð°Ð¼ Ñ‚Ð²Ð¾Ð¸ Ñ‚Ð²Ð¾Ð¸Ð¼ Ñ‚Ð²Ð¾Ð¸Ñ… Ñ‚Ð²Ð¾Ð¹ Ñ‚Ð²Ð¾Ñ Ñ‚Ð²Ð¾Ñ‘\nÑ‚ÐµÐ¿ÐµÑ€ÑŒ Ñ‚Ð¾Ð³Ð´Ð° Ñ‚Ð¾Ð¶Ðµ Ñ‚Ð¾Ñ‚Ñ‡Ð°Ñ Ñ‚Ð¾Ñ‡Ð½Ð¾ Ñ‚ÑƒÐ´Ð° Ñ‚ÑƒÑ‚ Ñ‚ÑŒÑ„Ñƒ Ñ‚Ð°Ñ\n\nÑƒ ÑƒÐ¶Ðµ ÑƒÐ²Ñ‹ ÑƒÐ¶ ÑƒÑ€Ð° ÑƒÑ… ÑƒÑŽ\n\nÑ„ Ñ„Ñƒ\n\nÑ… Ñ…Ð° Ñ…Ðµ Ñ…Ð¾Ñ€Ð¾ÑˆÐ¾ Ñ…Ð¾Ñ‚ÐµÐ» Ñ…Ð¾Ñ‚ÐµÐ»Ð° Ñ…Ð¾Ñ‚ÐµÐ»Ð¾ÑÑŒ Ñ…Ð¾Ñ‚ÐµÑ‚ÑŒ Ñ…Ð¾Ñ‚ÑŒ Ñ…Ð¾Ñ‚Ñ Ñ…Ð¾Ñ‡ÐµÑˆÑŒ Ñ…Ð¾Ñ‡Ñƒ Ñ…ÑƒÐ¶Ðµ\n\nÑ‡ Ñ‡ÐµÐ³Ð¾ Ñ‡ÐµÐ¼ Ñ‡Ñ‘Ð¼ Ñ‡ÐµÐ¼Ñƒ Ñ‡Ñ‚Ð¾ Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ñ‡Ð°ÑÑ‚Ð¾ Ñ‡Ð°Ñ‰Ðµ Ñ‡ÐµÐ¹ Ñ‡ÐµÑ€ÐµÐ· Ñ‡Ñ‚Ð¾Ð± Ñ‡ÑƒÑ‚ÑŒ Ñ‡Ñ…Ð°Ñ‚ÑŒ Ñ‡ÑŒÐ¸Ð¼\nÑ‡ÑŒÐ¸Ñ… Ñ‡ÑŒÑ‘ Ñ‡Ñ‘\n\nÑˆ ÑˆÐ°\n\nÑ‰ Ñ‰Ð° Ñ‰Ð°Ñ\n\nÑ‹ Ñ‹Ñ… Ñ‹Ðµ Ñ‹Ð¹\n\nÑ ÑÑ‚Ð° ÑÑ‚Ð¸ ÑÑ‚Ð¸Ð¼ ÑÑ‚Ð¸Ð¼Ð¸ ÑÑ‚Ð¸Ñ… ÑÑ‚Ð¾ ÑÑ‚Ð¾Ð³Ð¾ ÑÑ‚Ð¾Ð¹ ÑÑ‚Ð¾Ð¼ ÑÑ‚Ð¾Ð¼Ñƒ ÑÑ‚Ð¾Ñ‚ ÑÑ‚Ð¾ÑŽ ÑÑ‚Ñƒ ÑÐ´Ð°Ðº ÑÐ´Ð°ÐºÐ¸Ð¹\nÑÐ¹ ÑÐºÐ° ÑÐºÐ¸Ð¹ ÑÑ‚Ð°Ðº ÑÑ‚Ð°ÐºÐ¸Ð¹ ÑÑ…\n\nÑŽ\n\nÑ ÑÐ²Ð½Ð¾ ÑÐ²Ð½Ñ‹Ñ… ÑÐºÐ¾ ÑÐºÐ¾Ð±Ñ‹ ÑÐºÐ¾Ð¶Ðµ\n'.split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ru/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.ru.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ru/__init__.py----------------------------------------
spacy.lang.ru.__init__.Russian(Language)
spacy.lang.ru.__init__.RussianDefaults(BaseDefaults)
spacy.lang.ru.__init__.make_lemmatizer(nlp:Language,model:Optional[Model],name:str,mode:str,overwrite:bool,scorer:Optional[Callable])


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ru/lex_attrs.py----------------------------------------
A:spacy.lang.ru.lex_attrs._num_words->list(set('\nÐ½Ð¾Ð»ÑŒ Ð½Ð¾Ð»Ñ Ð½Ð¾Ð»ÑŽ Ð½Ð¾Ð»Ñ‘Ð¼ Ð½Ð¾Ð»Ðµ Ð½ÑƒÐ»ÐµÐ²Ð¾Ð¹ Ð½ÑƒÐ»ÐµÐ²Ð¾Ð³Ð¾ Ð½ÑƒÐ»ÐµÐ²Ð¾Ð¼Ñƒ Ð½ÑƒÐ»ÐµÐ²Ñ‹Ð¼ Ð½ÑƒÐ»ÐµÐ²Ð¾Ð¼ Ð½ÑƒÐ»ÐµÐ²Ð°Ñ Ð½ÑƒÐ»ÐµÐ²ÑƒÑŽ Ð½ÑƒÐ»ÐµÐ²Ð¾Ðµ Ð½ÑƒÐ»ÐµÐ²Ñ‹Ðµ Ð½ÑƒÐ»ÐµÐ²Ñ‹Ñ… Ð½ÑƒÐ»ÐµÐ²Ñ‹Ð¼Ð¸ \n\nÑ‡ÐµÑ‚Ð²ÐµÑ€Ñ‚ÑŒ Ñ‡ÐµÑ‚Ð²ÐµÑ€Ñ‚Ð¸ Ñ‡ÐµÑ‚Ð²ÐµÑ€Ñ‚ÑŒÑŽ Ñ‡ÐµÑ‚Ð²ÐµÑ€Ñ‚ÐµÐ¹ Ñ‡ÐµÑ‚Ð²ÐµÑ€Ñ‚ÑÐ¼ Ñ‡ÐµÑ‚Ð²ÐµÑ€Ñ‚ÑÐ¼Ð¸ Ñ‡ÐµÑ‚Ð²ÐµÑ€Ñ‚ÑÑ… \n\nÑ‚Ñ€ÐµÑ‚ÑŒ Ñ‚Ñ€ÐµÑ‚Ð¸ Ñ‚Ñ€ÐµÑ‚ÑŒÑŽ Ñ‚Ñ€ÐµÑ‚ÐµÐ¹ Ñ‚Ñ€ÐµÑ‚ÑÐ¼ Ñ‚Ñ€ÐµÑ‚ÑÐ¼Ð¸ Ñ‚Ñ€ÐµÑ‚ÑÑ… \n\nÐ¿Ð¾Ð»Ð¾Ð²Ð¸Ð½Ð° Ð¿Ð¾Ð»Ð¾Ð²Ð¸Ð½Ñ‹ Ð¿Ð¾Ð»Ð¾Ð²Ð¸Ð½Ðµ Ð¿Ð¾Ð»Ð¾Ð²Ð¸Ð½Ñƒ Ð¿Ð¾Ð»Ð¾Ð²Ð¸Ð½Ð¾Ð¹ Ð¿Ð¾Ð»Ð¾Ð²Ð¸Ð½ Ð¿Ð¾Ð»Ð¾Ð²Ð¸Ð½Ð°Ð¼ Ð¿Ð¾Ð»Ð¾Ð²Ð¸Ð½Ð°Ð¼Ð¸ Ð¿Ð¾Ð»Ð¾Ð²Ð¸Ð½Ð°Ñ… Ð¿Ð¾Ð»Ð¾Ð²Ð¸Ð½Ð¾ÑŽ \n\nÐ¾Ð´Ð¸Ð½ Ð¾Ð´Ð½Ð¾Ð³Ð¾ Ð¾Ð´Ð½Ð¾Ð¼Ñƒ Ð¾Ð´Ð½Ð¸Ð¼ Ð¾Ð´Ð½Ð¾Ð¼ \nÐ¿ÐµÑ€Ð²Ð¾Ð¹ Ð¿ÐµÑ€Ð²Ð¾Ð³Ð¾ Ð¿ÐµÑ€Ð²Ð¾Ð¼Ñƒ Ð¿ÐµÑ€Ð²Ð¾Ð¼ Ð¿ÐµÑ€Ð²Ñ‹Ð¹ Ð¿ÐµÑ€Ð²Ñ‹Ð¼ Ð¿ÐµÑ€Ð²Ñ‹Ñ… \nÐ²Ð¾-Ð¿ÐµÑ€Ð²Ñ‹Ñ… \nÐµÐ´Ð¸Ð½Ð¸Ñ†Ð° ÐµÐ´Ð¸Ð½Ð¸Ñ†Ñ‹ ÐµÐ´Ð¸Ð½Ð¸Ñ†Ðµ ÐµÐ´Ð¸Ð½Ð¸Ñ†Ñƒ ÐµÐ´Ð¸Ð½Ð¸Ñ†ÐµÐ¹ ÐµÐ´Ð¸Ð½Ð¸Ñ† ÐµÐ´Ð¸Ð½Ð¸Ñ†Ð°Ð¼ ÐµÐ´Ð¸Ð½Ð¸Ñ†Ð°Ð¼Ð¸ ÐµÐ´Ð¸Ð½Ð¸Ñ†Ð°Ñ… ÐµÐ´Ð¸Ð½Ð¸Ñ†ÐµÑŽ \n\nÐ´Ð²Ð° Ð´Ð²ÑƒÐ¼Ñ Ð´Ð²ÑƒÐ¼ Ð´Ð²ÑƒÑ… Ð´Ð²Ð¾Ð¸Ñ… Ð´Ð²Ð¾Ðµ Ð´Ð²Ðµ \nÐ²Ñ‚Ð¾Ñ€Ð¾Ð³Ð¾ Ð²Ñ‚Ð¾Ñ€Ð¾Ð¼Ñƒ Ð²Ñ‚Ð¾Ñ€Ð¾Ð¹ Ð²Ñ‚Ð¾Ñ€Ð¾Ð¼ Ð²Ñ‚Ð¾Ñ€Ñ‹Ð¼ Ð²Ñ‚Ð¾Ñ€Ñ‹Ñ… \nÐ´Ð²Ð¾Ð¹ÐºÐ° Ð´Ð²Ð¾Ð¹ÐºÐ¸ Ð´Ð²Ð¾Ð¹ÐºÐµ Ð´Ð²Ð¾Ð¹ÐºÑƒ Ð´Ð²Ð¾Ð¹ÐºÐ¾Ð¹ Ð´Ð²Ð¾ÐµÐº Ð´Ð²Ð¾Ð¹ÐºÐ°Ð¼ Ð´Ð²Ð¾Ð¹ÐºÐ°Ð¼Ð¸ Ð´Ð²Ð¾Ð¹ÐºÐ°Ñ… Ð´Ð²Ð¾Ð¹ÐºÐ¾ÑŽ  \nÐ²Ð¾-Ð²Ñ‚Ð¾Ñ€Ñ‹Ñ… \nÐ¾Ð±Ð° Ð¾Ð±Ðµ Ð¾Ð±ÐµÐ¸Ð¼ Ð¾Ð±ÐµÐ¸Ð¼Ð¸ Ð¾Ð±ÐµÐ¸Ñ… Ð¾Ð±Ð¾Ð¸Ð¼ Ð¾Ð±Ð¾Ð¸Ð¼Ð¸ Ð¾Ð±Ð¾Ð¸Ñ… \n\nÐ¿Ð¾Ð»Ñ‚Ð¾Ñ€Ð° Ð¿Ð¾Ð»Ñ‚Ð¾Ñ€Ñ‹ Ð¿Ð¾Ð»ÑƒÑ‚Ð¾Ñ€Ð° \n\nÑ‚Ñ€Ð¸ Ñ‚Ñ€ÐµÑ‚ÑŒÐµÐ³Ð¾ Ñ‚Ñ€ÐµÑ‚ÑŒÐµÐ¼Ñƒ Ñ‚Ñ€ÐµÑ‚ÑŒÐµÐ¼ Ñ‚Ñ€ÐµÑ‚ÑŒÐ¸Ð¼ Ñ‚Ñ€ÐµÑ‚Ð¸Ð¹ Ñ‚Ñ€ÐµÐ¼Ñ Ñ‚Ñ€ÐµÐ¼ Ñ‚Ñ€ÐµÑ… Ñ‚Ñ€Ð¾Ðµ Ñ‚Ñ€Ð¾Ð¸Ñ… Ñ‚Ñ€Ñ‘Ñ… \nÑ‚Ñ€Ð¾Ð¹ÐºÐ° Ñ‚Ñ€Ð¾Ð¹ÐºÐ¸ Ñ‚Ñ€Ð¾Ð¹ÐºÐµ Ñ‚Ñ€Ð¾Ð¹ÐºÑƒ Ñ‚Ñ€Ð¾Ð¹ÐºÐ¾ÑŽ Ñ‚Ñ€Ð¾ÐµÐº Ñ‚Ñ€Ð¾Ð¹ÐºÐ°Ð¼ Ñ‚Ñ€Ð¾Ð¹ÐºÐ°Ð¼Ð¸ Ñ‚Ñ€Ð¾Ð¹ÐºÐ°Ñ… Ñ‚Ñ€Ð¾Ð¹ÐºÐ¾Ð¹ \nÑ‚Ñ€Ð¾ÐµÑ‡ÐºÐ° Ñ‚Ñ€Ð¾ÐµÑ‡ÐºÐ¸ Ñ‚Ñ€Ð¾ÐµÑ‡ÐºÐµ Ñ‚Ñ€Ð¾ÐµÑ‡ÐºÑƒ Ñ‚Ñ€Ð¾ÐµÑ‡ÐºÐ¾Ð¹ Ñ‚Ñ€Ð¾ÐµÑ‡ÐµÐº Ñ‚Ñ€Ð¾ÐµÑ‡ÐºÐ°Ð¼ Ñ‚Ñ€Ð¾ÐµÑ‡ÐºÐ°Ð¼Ð¸ Ñ‚Ñ€Ð¾ÐµÑ‡ÐºÐ°Ñ… Ñ‚Ñ€Ð¾ÐµÑ‡ÐºÐ¾Ð¹ \nÑ‚Ñ€ÐµÑˆÐºÐ° Ñ‚Ñ€ÐµÑˆÐºÐ¸ Ñ‚Ñ€ÐµÑˆÐºÐµ Ñ‚Ñ€ÐµÑˆÐºÑƒ Ñ‚Ñ€ÐµÑˆÐºÐ¾Ð¹ Ñ‚Ñ€ÐµÑˆÐµÐº Ñ‚Ñ€ÐµÑˆÐºÐ°Ð¼ Ñ‚Ñ€ÐµÑˆÐºÐ°Ð¼Ð¸ Ñ‚Ñ€ÐµÑˆÐºÐ°Ñ… Ñ‚Ñ€ÐµÑˆÐºÐ¾ÑŽ \nÑ‚Ñ€Ñ‘ÑˆÐºÐ° Ñ‚Ñ€Ñ‘ÑˆÐºÐ¸ Ñ‚Ñ€Ñ‘ÑˆÐºÐµ Ñ‚Ñ€Ñ‘ÑˆÐºÑƒ Ñ‚Ñ€Ñ‘ÑˆÐºÐ¾Ð¹ Ñ‚Ñ€Ñ‘ÑˆÐµÐº Ñ‚Ñ€Ñ‘ÑˆÐºÐ°Ð¼ Ñ‚Ñ€Ñ‘ÑˆÐºÐ°Ð¼Ð¸ Ñ‚Ñ€Ñ‘ÑˆÐºÐ°Ñ… Ñ‚Ñ€Ñ‘ÑˆÐºÐ¾ÑŽ \nÑ‚Ñ€Ð¾ÑÐº Ñ‚Ñ€Ð¾ÑÐºÐ° Ñ‚Ñ€Ð¾ÑÐºÑƒ Ñ‚Ñ€Ð¾ÑÐºÐ¾Ð¼ Ñ‚Ñ€Ð¾ÑÐºÐµ Ñ‚Ñ€Ð¾ÑÐºÐ¸ Ñ‚Ñ€Ð¾ÑÐºÐ¾Ð² Ñ‚Ñ€Ð¾ÑÐºÐ°Ð¼ Ñ‚Ñ€Ð¾ÑÐºÐ°Ð¼Ð¸ Ñ‚Ñ€Ð¾ÑÐºÐ°Ñ…  \nÑ‚Ñ€ÐµÑ…Ð° Ñ‚Ñ€ÐµÑ…Ñƒ Ñ‚Ñ€ÐµÑ…Ð¾Ð¹ \nÑ‚Ñ€Ñ‘Ñ…Ð° Ñ‚Ñ€Ñ‘Ñ…Ñƒ Ñ‚Ñ€Ñ‘Ñ…Ð¾Ð¹ \nÐ²Ñ‚Ñ€Ð¾ÐµÐ¼ Ð²Ñ‚Ñ€Ð¾Ñ‘Ð¼ \n\nÑ‡ÐµÑ‚Ñ‹Ñ€Ðµ Ñ‡ÐµÑ‚Ð²ÐµÑ€Ñ‚Ð¾Ð³Ð¾ Ñ‡ÐµÑ‚Ð²ÐµÑ€Ñ‚Ð¾Ð¼Ñƒ Ñ‡ÐµÑ‚Ð²ÐµÑ€Ñ‚Ð¾Ð¼ Ñ‡ÐµÑ‚Ð²ÐµÑ€Ñ‚Ñ‹Ð¹ Ñ‡ÐµÑ‚Ð²ÐµÑ€Ñ‚Ñ‹Ð¼ Ñ‡ÐµÑ‚Ð²ÐµÑ€ÐºÐ° Ñ‡ÐµÑ‚Ñ‹Ñ€ÑŒÐ¼Ñ Ñ‡ÐµÑ‚Ñ‹Ñ€ÐµÐ¼ Ñ‡ÐµÑ‚Ñ‹Ñ€ÐµÑ… Ñ‡ÐµÑ‚Ð²ÐµÑ€Ð¾ Ñ‡ÐµÑ‚Ñ‹Ñ€Ñ‘Ñ… Ñ‡ÐµÑ‚Ð²ÐµÑ€Ñ‹Ð¼ \nÑ‡ÐµÑ‚Ð²ÐµÑ€Ñ‹Ñ… \nÐ²Ñ‡ÐµÑ‚Ð²ÐµÑ€Ð¾Ð¼ \n\nÐ¿ÑÑ‚ÑŒ Ð¿ÑÑ‚Ð¾Ð³Ð¾ Ð¿ÑÑ‚Ð¾Ð¼Ñƒ Ð¿ÑÑ‚Ð¾Ð¼ Ð¿ÑÑ‚Ñ‹Ð¹ Ð¿ÑÑ‚Ñ‹Ð¼ Ð¿ÑÑ‚ÑŒÑŽ Ð¿ÑÑ‚Ð¸ Ð¿ÑÑ‚ÐµÑ€Ð¾ Ð¿ÑÑ‚ÐµÑ€Ñ‹Ñ… Ð¿ÑÑ‚ÐµÑ€Ñ‹Ð¼Ð¸ \nÐ²Ð¿ÑÑ‚ÐµÑ€Ð¾Ð¼ \nÐ¿ÑÑ‚ÐµÑ€Ð¾Ñ‡ÐºÐ° Ð¿ÑÑ‚ÐµÑ€Ð¾Ñ‡ÐºÐ¸ Ð¿ÑÑ‚ÐµÑ€Ð¾Ñ‡ÐºÐµ Ð¿ÑÑ‚ÐµÑ€Ð¾Ñ‡ÐºÐ°Ð¼Ð¸ Ð¿ÑÑ‚ÐµÑ€Ð¾Ñ‡ÐºÐ¾Ð¹ Ð¿ÑÑ‚ÐµÑ€Ð¾Ñ‡ÐºÑƒ Ð¿ÑÑ‚ÐµÑ€Ð¾Ñ‡ÐºÐ¾Ð¹ Ð¿ÑÑ‚ÐµÑ€Ð¾Ñ‡ÐºÐ°Ð¼Ð¸ \nÐ¿ÑÑ‚Ñ‘Ñ€Ð¾Ñ‡ÐºÐ° Ð¿ÑÑ‚Ñ‘Ñ€Ð¾Ñ‡ÐºÐ¸ Ð¿ÑÑ‚Ñ‘Ñ€Ð¾Ñ‡ÐºÐµ Ð¿ÑÑ‚Ñ‘Ñ€Ð¾Ñ‡ÐºÐ°Ð¼Ð¸ Ð¿ÑÑ‚Ñ‘Ñ€Ð¾Ñ‡ÐºÐ¾Ð¹ Ð¿ÑÑ‚Ñ‘Ñ€Ð¾Ñ‡ÐºÑƒ Ð¿ÑÑ‚Ñ‘Ñ€Ð¾Ñ‡ÐºÐ¾Ð¹ Ð¿ÑÑ‚Ñ‘Ñ€Ð¾Ñ‡ÐºÐ°Ð¼Ð¸ \nÐ¿ÑÑ‚ÐµÑ€ÐºÐ° Ð¿ÑÑ‚ÐµÑ€ÐºÐ¸ Ð¿ÑÑ‚ÐµÑ€ÐºÐµ Ð¿ÑÑ‚ÐµÑ€ÐºÐ°Ð¼Ð¸ Ð¿ÑÑ‚ÐµÑ€ÐºÐ¾Ð¹ Ð¿ÑÑ‚ÐµÑ€ÐºÑƒ Ð¿ÑÑ‚ÐµÑ€ÐºÐ°Ð¼Ð¸ \nÐ¿ÑÑ‚Ñ‘Ñ€ÐºÐ° Ð¿ÑÑ‚Ñ‘Ñ€ÐºÐ¸ Ð¿ÑÑ‚Ñ‘Ñ€ÐºÐµ Ð¿ÑÑ‚Ñ‘Ñ€ÐºÐ°Ð¼Ð¸ Ð¿ÑÑ‚Ñ‘Ñ€ÐºÐ¾Ð¹ Ð¿ÑÑ‚Ñ‘Ñ€ÐºÑƒ Ð¿ÑÑ‚Ñ‘Ñ€ÐºÐ°Ð¼Ð¸ \nÐ¿ÑÑ‚Ñ‘Ñ€Ð° Ð¿ÑÑ‚Ñ‘Ñ€Ñ‹ Ð¿ÑÑ‚Ñ‘Ñ€Ðµ Ð¿ÑÑ‚Ñ‘Ñ€Ð°Ð¼Ð¸ Ð¿ÑÑ‚Ñ‘Ñ€Ð¾Ð¹ Ð¿ÑÑ‚Ñ‘Ñ€Ñƒ Ð¿ÑÑ‚Ñ‘Ñ€Ð°Ð¼Ð¸ \nÐ¿ÑÑ‚ÐµÑ€Ð° Ð¿ÑÑ‚ÐµÑ€Ñ‹ Ð¿ÑÑ‚ÐµÑ€Ðµ Ð¿ÑÑ‚ÐµÑ€Ð°Ð¼Ð¸ Ð¿ÑÑ‚ÐµÑ€Ð¾Ð¹ Ð¿ÑÑ‚ÐµÑ€Ñƒ Ð¿ÑÑ‚ÐµÑ€Ð°Ð¼Ð¸ \nÐ¿ÑÑ‚Ð°Ðº Ð¿ÑÑ‚Ð°ÐºÐ¸ Ð¿ÑÑ‚Ð°ÐºÐµ Ð¿ÑÑ‚Ð°ÐºÐ°Ð¼Ð¸ Ð¿ÑÑ‚Ð°ÐºÐ¾Ð¼ Ð¿ÑÑ‚Ð°ÐºÑƒ Ð¿ÑÑ‚Ð°ÐºÐ°Ð¼Ð¸ \n\nÑˆÐµÑÑ‚ÑŒ ÑˆÐµÑÑ‚ÐµÑ€ÐºÐ° ÑˆÐµÑÑ‚Ð¾Ð³Ð¾ ÑˆÐµÑÑ‚Ð¾Ð¼Ñƒ ÑˆÐµÑÑ‚Ð¾Ð¹ ÑˆÐµÑÑ‚Ð¾Ð¼ ÑˆÐµÑÑ‚Ñ‹Ð¼ ÑˆÐµÑÑ‚ÑŒÑŽ ÑˆÐµÑÑ‚Ð¸ ÑˆÐµÑÑ‚ÐµÑ€Ð¾ ÑˆÐµÑÑ‚ÐµÑ€Ñ‹Ñ… \nÐ²ÑˆÐµÑÑ‚ÐµÑ€Ð¾Ð¼ \n\nÑÐµÐ¼ÑŒ ÑÐµÐ¼ÐµÑ€ÐºÐ° ÑÐµÐ´ÑŒÐ¼Ð¾Ð³Ð¾ ÑÐµÐ´ÑŒÐ¼Ð¾Ð¼Ñƒ ÑÐµÐ´ÑŒÐ¼Ð¾Ð¹ ÑÐµÐ´ÑŒÐ¼Ð¾Ð¼ ÑÐµÐ´ÑŒÐ¼Ñ‹Ð¼ ÑÐµÐ¼ÑŒÑŽ ÑÐµÐ¼Ð¸ ÑÐµÐ¼ÐµÑ€Ð¾ ÑÐµÐ´ÑŒÐ¼Ñ‹Ñ… \nÐ²ÑÐµÐ¼ÐµÑ€Ð¾Ð¼ \n\nÐ²Ð¾ÑÐµÐ¼ÑŒ Ð²Ð¾ÑÑŒÐ¼ÐµÑ€ÐºÐ° Ð²Ð¾ÑÑŒÐ¼Ð¾Ð³Ð¾ Ð²Ð¾ÑÑŒÐ¼Ð¾Ð¼Ñƒ Ð²Ð¾ÑÐµÐ¼ÑŒÑŽ Ð²Ð¾ÑÑŒÐ¼Ð¾Ð¹ Ð²Ð¾ÑÑŒÐ¼Ð¾Ð¼ Ð²Ð¾ÑÑŒÐ¼Ñ‹Ð¼ Ð²Ð¾ÑÐµÐ¼Ð¸ Ð²Ð¾ÑÑŒÐ¼ÐµÑ€Ð¾Ð¼ Ð²Ð¾ÑÑŒÐ¼Ð¸ Ð²Ð¾ÑÑŒÐ¼ÑŒÑŽ \nÐ²Ð¾ÑÑŒÐ¼ÐµÑ€Ñ‹Ñ… \nÐ²Ð²Ð¾ÑÑŒÐ¼ÐµÑ€Ð¾Ð¼ \n\nÐ´ÐµÐ²ÑÑ‚ÑŒ Ð´ÐµÐ²ÑÑ‚Ð¾Ð³Ð¾ Ð´ÐµÐ²ÑÑ‚Ð¾Ð¼Ñƒ Ð´ÐµÐ²ÑÑ‚ÐºÐ° Ð´ÐµÐ²ÑÑ‚Ð¾Ð¼ Ð´ÐµÐ²ÑÑ‚Ñ‹Ð¹ Ð´ÐµÐ²ÑÑ‚Ñ‹Ð¼ Ð´ÐµÐ²ÑÑ‚ÑŒÑŽ Ð´ÐµÐ²ÑÑ‚Ð¸ Ð´ÐµÐ²ÑÑ‚ÐµÑ€Ð¾Ð¼ Ð²Ð´ÐµÐ²ÑÑ‚ÐµÑ€Ð¾Ð¼ Ð´ÐµÐ²ÑÑ‚ÐµÑ€Ñ‹Ñ… \nÐ²Ð´ÐµÐ²ÑÑ‚ÐµÑ€Ð¾Ð¼ \n\nÐ´ÐµÑÑÑ‚ÑŒ Ð´ÐµÑÑÑ‚Ð¾Ð³Ð¾ Ð´ÐµÑÑÑ‚Ð¾Ð¼Ñƒ Ð´ÐµÑÑÑ‚ÐºÐ° Ð´ÐµÑÑÑ‚Ð¾Ð¼ Ð´ÐµÑÑÑ‚Ñ‹Ð¹ Ð´ÐµÑÑÑ‚Ñ‹Ð¼ Ð´ÐµÑÑÑ‚ÑŒÑŽ Ð´ÐµÑÑÑ‚Ð¸ Ð´ÐµÑÑÑ‚ÐµÑ€Ð¾Ð¼ Ð´ÐµÑÑÑ‚Ñ‹Ñ… \nÐ²Ð´ÐµÑÑÑ‚ÐµÑ€Ð¾Ð¼ \n\nÐ¾Ð´Ð¸Ð½Ð½Ð°Ð´Ñ†Ð°Ñ‚ÑŒ Ð¾Ð´Ð¸Ð½Ð½Ð°Ð´Ñ†Ð°Ñ‚Ð¾Ð³Ð¾ Ð¾Ð´Ð¸Ð½Ð½Ð°Ð´Ñ†Ð°Ñ‚Ð¾Ð¼Ñƒ Ð¾Ð´Ð¸Ð½Ð½Ð°Ð´Ñ†Ð°Ñ‚Ð¾Ð¼ Ð¾Ð´Ð¸Ð½Ð½Ð°Ð´Ñ†Ð°Ñ‚Ñ‹Ð¹ Ð¾Ð´Ð¸Ð½Ð½Ð°Ð´Ñ†Ð°Ñ‚Ñ‹Ð¼ Ð¾Ð´Ð¸Ð½Ð½Ð°Ð´Ñ†Ð°Ñ‚ÑŒÑŽ Ð¾Ð´Ð¸Ð½Ð½Ð°Ð´Ñ†Ð°Ñ‚Ð¸ \nÐ¾Ð´Ð¸Ð½Ð½Ð°Ð´Ñ†Ð°Ñ‚Ñ‹Ñ… \n\nÐ´Ð²ÐµÐ½Ð°Ð´Ñ†Ð°Ñ‚ÑŒ Ð´Ð²ÐµÐ½Ð°Ð´Ñ†Ð°Ñ‚Ð¾Ð³Ð¾ Ð´Ð²ÐµÐ½Ð°Ð´Ñ†Ð°Ñ‚Ð¾Ð¼Ñƒ Ð´Ð²ÐµÐ½Ð°Ð´Ñ†Ð°Ñ‚Ð¾Ð¼ Ð´Ð²ÐµÐ½Ð°Ð´Ñ†Ð°Ñ‚Ñ‹Ð¹ Ð´Ð²ÐµÐ½Ð°Ð´Ñ†Ð°Ñ‚Ñ‹Ð¼ Ð´Ð²ÐµÐ½Ð°Ð´Ñ†Ð°Ñ‚ÑŒÑŽ Ð´Ð²ÐµÐ½Ð°Ð´Ñ†Ð°Ñ‚Ð¸ \nÐ´Ð²ÐµÐ½Ð°Ð´Ñ†Ð°Ñ‚Ñ‹Ñ… \n\nÑ‚Ñ€Ð¸Ð½Ð°Ð´Ñ†Ð°Ñ‚ÑŒ Ñ‚Ñ€Ð¸Ð½Ð°Ð´Ñ†Ð°Ñ‚Ð¾Ð³Ð¾ Ñ‚Ñ€Ð¸Ð½Ð°Ð´Ñ†Ð°Ñ‚Ð¾Ð¼Ñƒ Ñ‚Ñ€Ð¸Ð½Ð°Ð´Ñ†Ð°Ñ‚Ð¾Ð¼ Ñ‚Ñ€Ð¸Ð½Ð°Ð´Ñ†Ð°Ñ‚Ñ‹Ð¹ Ñ‚Ñ€Ð¸Ð½Ð°Ð´Ñ†Ð°Ñ‚Ñ‹Ð¼ Ñ‚Ñ€Ð¸Ð½Ð°Ð´Ñ†Ð°Ñ‚ÑŒÑŽ Ñ‚Ñ€Ð¸Ð½Ð°Ð´Ñ†Ð°Ñ‚Ð¸ \nÑ‚Ñ€Ð¸Ð½Ð°Ð´Ñ†Ð°Ñ‚Ñ‹Ñ… \n\nÑ‡ÐµÑ‚Ñ‹Ñ€Ð½Ð°Ð´Ñ†Ð°Ñ‚ÑŒ Ñ‡ÐµÑ‚Ñ‹Ñ€Ð½Ð°Ð´Ñ†Ð°Ñ‚Ð¾Ð³Ð¾ Ñ‡ÐµÑ‚Ñ‹Ñ€Ð½Ð°Ð´Ñ†Ð°Ñ‚Ð¾Ð¼Ñƒ Ñ‡ÐµÑ‚Ñ‹Ñ€Ð½Ð°Ð´Ñ†Ð°Ñ‚Ð¾Ð¼ Ñ‡ÐµÑ‚Ñ‹Ñ€Ð½Ð°Ð´Ñ†Ð°Ñ‚Ñ‹Ð¹ Ñ‡ÐµÑ‚Ñ‹Ñ€Ð½Ð°Ð´Ñ†Ð°Ñ‚Ñ‹Ð¼ Ñ‡ÐµÑ‚Ñ‹Ñ€Ð½Ð°Ð´Ñ†Ð°Ñ‚ÑŒÑŽ Ñ‡ÐµÑ‚Ñ‹Ñ€Ð½Ð°Ð´Ñ†Ð°Ñ‚Ð¸ \nÑ‡ÐµÑ‚Ñ‹Ñ€Ð½Ð°Ð´Ñ†Ð°Ñ‚Ñ‹Ñ… \n\nÐ¿ÑÑ‚Ð½Ð°Ð´Ñ†Ð°Ñ‚ÑŒ Ð¿ÑÑ‚Ð½Ð°Ð´Ñ†Ð°Ñ‚Ð¾Ð³Ð¾ Ð¿ÑÑ‚Ð½Ð°Ð´Ñ†Ð°Ñ‚Ð¾Ð¼Ñƒ Ð¿ÑÑ‚Ð½Ð°Ð´Ñ†Ð°Ñ‚Ð¾Ð¼ Ð¿ÑÑ‚Ð½Ð°Ð´Ñ†Ð°Ñ‚Ñ‹Ð¹ Ð¿ÑÑ‚Ð½Ð°Ð´Ñ†Ð°Ñ‚Ñ‹Ð¼ Ð¿ÑÑ‚Ð½Ð°Ð´Ñ†Ð°Ñ‚ÑŒÑŽ Ð¿ÑÑ‚Ð½Ð°Ð´Ñ†Ð°Ñ‚Ð¸ \nÐ¿ÑÑ‚Ð½Ð°Ð´Ñ†Ð°Ñ‚Ñ‹Ñ… \nÐ¿ÑÑ‚Ð½Ð°Ñ€Ð¸Ðº Ð¿ÑÑ‚Ð½Ð°Ñ€Ð¸ÐºÑƒ Ð¿ÑÑ‚Ð½Ð°Ñ€Ð¸ÐºÐ¾Ð¼ Ð¿ÑÑ‚Ð½Ð°Ñ€Ð¸ÐºÐ¸ \n\nÑˆÐµÑÑ‚Ð½Ð°Ð´Ñ†Ð°Ñ‚ÑŒ ÑˆÐµÑÑ‚Ð½Ð°Ð´Ñ†Ð°Ñ‚Ð¾Ð³Ð¾ ÑˆÐµÑÑ‚Ð½Ð°Ð´Ñ†Ð°Ñ‚Ð¾Ð¼Ñƒ ÑˆÐµÑÑ‚Ð½Ð°Ð´Ñ†Ð°Ñ‚Ð¾Ð¼ ÑˆÐµÑÑ‚Ð½Ð°Ð´Ñ†Ð°Ñ‚Ñ‹Ð¹ ÑˆÐµÑÑ‚Ð½Ð°Ð´Ñ†Ð°Ñ‚Ñ‹Ð¼ ÑˆÐµÑÑ‚Ð½Ð°Ð´Ñ†Ð°Ñ‚ÑŒÑŽ ÑˆÐµÑÑ‚Ð½Ð°Ð´Ñ†Ð°Ñ‚Ð¸ \nÑˆÐµÑÑ‚Ð½Ð°Ð´Ñ†Ð°Ñ‚Ñ‹Ñ… \n\nÑÐµÐ¼Ð½Ð°Ð´Ñ†Ð°Ñ‚ÑŒ ÑÐµÐ¼Ð½Ð°Ð´Ñ†Ð°Ñ‚Ð¾Ð³Ð¾ ÑÐµÐ¼Ð½Ð°Ð´Ñ†Ð°Ñ‚Ð¾Ð¼Ñƒ ÑÐµÐ¼Ð½Ð°Ð´Ñ†Ð°Ñ‚Ð¾Ð¼ ÑÐµÐ¼Ð½Ð°Ð´Ñ†Ð°Ñ‚Ñ‹Ð¹ ÑÐµÐ¼Ð½Ð°Ð´Ñ†Ð°Ñ‚Ñ‹Ð¼ ÑÐµÐ¼Ð½Ð°Ð´Ñ†Ð°Ñ‚ÑŒÑŽ ÑÐµÐ¼Ð½Ð°Ð´Ñ†Ð°Ñ‚Ð¸ ÑÐµÐ¼Ð½Ð°Ð´Ñ†Ð°Ñ‚Ñ‹Ñ… \n\nÐ²Ð¾ÑÐµÐ¼Ð½Ð°Ð´Ñ†Ð°Ñ‚ÑŒ Ð²Ð¾ÑÐµÐ¼Ð½Ð°Ð´Ñ†Ð°Ñ‚Ð¾Ð³Ð¾ Ð²Ð¾ÑÐµÐ¼Ð½Ð°Ð´Ñ†Ð°Ñ‚Ð¾Ð¼Ñƒ Ð²Ð¾ÑÐµÐ¼Ð½Ð°Ð´Ñ†Ð°Ñ‚Ð¾Ð¼ Ð²Ð¾ÑÐµÐ¼Ð½Ð°Ð´Ñ†Ð°Ñ‚Ñ‹Ð¹ Ð²Ð¾ÑÐµÐ¼Ð½Ð°Ð´Ñ†Ð°Ñ‚Ñ‹Ð¼ Ð²Ð¾ÑÐµÐ¼Ð½Ð°Ð´Ñ†Ð°Ñ‚ÑŒÑŽ Ð²Ð¾ÑÐµÐ¼Ð½Ð°Ð´Ñ†Ð°Ñ‚Ð¸ \nÐ²Ð¾ÑÐµÐ¼Ð½Ð°Ð´Ñ†Ð°Ñ‚Ñ‹Ñ… \n\nÐ´ÐµÐ²ÑÑ‚Ð½Ð°Ð´Ñ†Ð°Ñ‚ÑŒ Ð´ÐµÐ²ÑÑ‚Ð½Ð°Ð´Ñ†Ð°Ñ‚Ð¾Ð³Ð¾ Ð´ÐµÐ²ÑÑ‚Ð½Ð°Ð´Ñ†Ð°Ñ‚Ð¾Ð¼Ñƒ Ð´ÐµÐ²ÑÑ‚Ð½Ð°Ð´Ñ†Ð°Ñ‚Ð¾Ð¼ Ð´ÐµÐ²ÑÑ‚Ð½Ð°Ð´Ñ†Ð°Ñ‚Ñ‹Ð¹ Ð´ÐµÐ²ÑÑ‚Ð½Ð°Ð´Ñ†Ð°Ñ‚Ñ‹Ð¼ Ð´ÐµÐ²ÑÑ‚Ð½Ð°Ð´Ñ†Ð°Ñ‚ÑŒÑŽ Ð´ÐµÐ²ÑÑ‚Ð½Ð°Ð´Ñ†Ð°Ñ‚Ð¸ \nÐ´ÐµÐ²ÑÑ‚Ð½Ð°Ð´Ñ†Ð°Ñ‚Ñ‹Ñ… \n\nÐ´Ð²Ð°Ð´Ñ†Ð°Ñ‚ÑŒ Ð´Ð²Ð°Ð´Ñ†Ð°Ñ‚Ð¾Ð³Ð¾ Ð´Ð²Ð°Ð´Ñ†Ð°Ñ‚Ð¾Ð¼Ñƒ Ð´Ð²Ð°Ð´Ñ†Ð°Ñ‚Ð¾Ð¼ Ð´Ð²Ð°Ð´Ñ†Ð°Ñ‚Ñ‹Ð¹ Ð´Ð²Ð°Ð´Ñ†Ð°Ñ‚Ñ‹Ð¼ Ð´Ð²Ð°Ð´Ñ†Ð°Ñ‚ÑŒÑŽ Ð´Ð²Ð°Ð´Ñ†Ð°Ñ‚Ð¸ Ð´Ð²Ð°Ð´Ñ†Ð°Ñ‚Ñ‹Ñ… \n\nÑ‡ÐµÑ‚Ð²ÐµÑ€Ñ‚Ð°Ðº Ñ‡ÐµÑ‚Ð²ÐµÑ€Ñ‚Ð°ÐºÐ° Ñ‡ÐµÑ‚Ð²ÐµÑ€Ñ‚Ð°ÐºÐµ Ñ‡ÐµÑ‚Ð²ÐµÑ€Ñ‚Ð°ÐºÑƒ Ñ‡ÐµÑ‚Ð²ÐµÑ€Ñ‚Ð°ÐºÐ¸ Ñ‡ÐµÑ‚Ð²ÐµÑ€Ñ‚Ð°ÐºÐ¾Ð¼ Ñ‡ÐµÑ‚Ð²ÐµÑ€Ñ‚Ð°ÐºÐ°Ð¼Ð¸ \n\nÑ‚Ñ€Ð¸Ð´Ñ†Ð°Ñ‚ÑŒ Ñ‚Ñ€Ð¸Ð´Ñ†Ð°Ñ‚Ð¾Ð³Ð¾ Ñ‚Ñ€Ð¸Ð´Ñ†Ð°Ñ‚Ð¾Ð¼Ñƒ Ñ‚Ñ€Ð¸Ð´Ñ†Ð°Ñ‚Ð¾Ð¼ Ñ‚Ñ€Ð¸Ð´Ñ†Ð°Ñ‚Ñ‹Ð¹ Ñ‚Ñ€Ð¸Ð´Ñ†Ð°Ñ‚Ñ‹Ð¼ Ñ‚Ñ€Ð¸Ð´Ñ†Ð°Ñ‚ÑŒÑŽ Ñ‚Ñ€Ð¸Ð´Ñ†Ð°Ñ‚Ð¸ Ñ‚Ñ€Ð¸Ð´Ñ†Ð°Ñ‚Ñ‹Ñ… \nÑ‚Ñ€Ð¸Ð´Ñ†Ð°Ð´ÐºÐ° Ñ‚Ñ€Ð¸Ð´Ñ†Ð°Ð´ÐºÑƒ Ñ‚Ñ€Ð¸Ð´Ñ†Ð°Ð´ÐºÐµ Ñ‚Ñ€Ð¸Ð´Ñ†Ð°Ð´ÐºÐ¸ Ñ‚Ñ€Ð¸Ð´Ñ†Ð°Ð´ÐºÐ¾Ð¹ Ñ‚Ñ€Ð¸Ð´Ñ†Ð°Ð´ÐºÐ¾ÑŽ Ñ‚Ñ€Ð¸Ð´Ñ†Ð°Ð´ÐºÐ°Ð¼Ð¸ \n\nÑ‚Ñ€Ð¸Ð´ÐµÐ²ÑÑ‚ÑŒ Ñ‚Ñ€Ð¸Ð´ÐµÐ²ÑÑ‚Ð¸ Ñ‚Ñ€Ð¸Ð´ÐµÐ²ÑÑ‚ÑŒÑŽ \n\nÑÐ¾Ñ€Ð¾Ðº ÑÐ¾Ñ€Ð¾ÐºÐ¾Ð²Ð¾Ð³Ð¾ ÑÐ¾Ñ€Ð¾ÐºÐ¾Ð²Ð¾Ð¼Ñƒ ÑÐ¾Ñ€Ð¾ÐºÐ¾Ð²Ð¾Ð¼ ÑÐ¾Ñ€Ð¾ÐºÐ¾Ð²Ñ‹Ð¼ ÑÐ¾Ñ€Ð¾ÐºÐ¾Ð²Ð¾Ð¹ ÑÐ¾Ñ€Ð¾ÐºÐ¾Ð²Ñ‹Ñ… \nÑÐ¾Ñ€Ð¾ÐºÐµÑ‚ ÑÐ¾Ñ€Ð¾ÐºÐµÑ‚Ð° ÑÐ¾Ñ€Ð¾ÐºÐµÑ‚Ñƒ ÑÐ¾Ñ€Ð¾ÐºÐµÑ‚Ðµ ÑÐ¾Ñ€Ð¾ÐºÐµÑ‚Ñ‹ ÑÐ¾Ñ€Ð¾ÐºÐµÑ‚Ð¾Ð¼ ÑÐ¾Ñ€Ð¾ÐºÐµÑ‚Ð°Ð¼Ð¸ ÑÐ¾Ñ€Ð¾ÐºÐµÑ‚Ð°Ð¼ \n\nÐ¿ÑÑ‚ÑŒÐ´ÐµÑÑÑ‚ Ð¿ÑÑ‚ÑŒÐ´ÐµÑÑÑ‚Ð¾Ð³Ð¾ Ð¿ÑÑ‚ÑŒÐ´ÐµÑÑÑ‚Ð¾Ð¼Ñƒ Ð¿ÑÑ‚ÑŒÑŽÐ´ÐµÑÑÑ‚ÑŒÑŽ Ð¿ÑÑ‚ÑŒÐ´ÐµÑÑÑ‚Ð¾Ð¼ Ð¿ÑÑ‚ÑŒÐ´ÐµÑÑÑ‚Ñ‹Ð¹ Ð¿ÑÑ‚ÑŒÐ´ÐµÑÑÑ‚Ñ‹Ð¼ Ð¿ÑÑ‚Ð¸Ð´ÐµÑÑÑ‚Ð¸ Ð¿ÑÑ‚ÑŒÐ´ÐµÑÑÑ‚Ñ‹Ñ… \nÐ¿Ð¾Ð»Ñ‚Ð¸Ð½Ð½Ð¸Ðº Ð¿Ð¾Ð»Ñ‚Ð¸Ð½Ð½Ð¸ÐºÐ° Ð¿Ð¾Ð»Ñ‚Ð¸Ð½Ð½Ð¸ÐºÐµ Ð¿Ð¾Ð»Ñ‚Ð¸Ð½Ð½Ð¸ÐºÑƒ Ð¿Ð¾Ð»Ñ‚Ð¸Ð½Ð½Ð¸ÐºÐ¸ Ð¿Ð¾Ð»Ñ‚Ð¸Ð½Ð½Ð¸ÐºÐ¾Ð¼ Ð¿Ð¾Ð»Ñ‚Ð¸Ð½Ð½Ð¸ÐºÐ°Ð¼Ð¸ Ð¿Ð¾Ð»Ñ‚Ð¸Ð½Ð½Ð¸ÐºÐ°Ð¼ Ð¿Ð¾Ð»Ñ‚Ð¸Ð½Ð½Ð¸ÐºÐ°Ñ… \nÐ¿ÑÑ‚Ð¸Ð´ÐµÑÑÑ‚ÐºÐ° Ð¿ÑÑ‚Ð¸Ð´ÐµÑÑÑ‚ÐºÐµ Ð¿ÑÑ‚Ð¸Ð´ÐµÑÑÑ‚ÐºÑƒ Ð¿ÑÑ‚Ð¸Ð´ÐµÑÑÑ‚ÐºÐ¸ Ð¿ÑÑ‚Ð¸Ð´ÐµÑÑÑ‚ÐºÐ¾Ð¹ Ð¿ÑÑ‚Ð¸Ð´ÐµÑÑÑ‚ÐºÐ°Ð¼Ð¸ Ð¿ÑÑ‚Ð¸Ð´ÐµÑÑÑ‚ÐºÐ°Ð¼ Ð¿ÑÑ‚Ð¸Ð´ÐµÑÑÑ‚ÐºÐ°Ñ… \nÐ¿Ð¾Ð»Ñ‚Ð¾Ñ Ð¿Ð¾Ð»Ñ‚Ð¾ÑÐ° Ð¿Ð¾Ð»Ñ‚Ð¾ÑÐµ Ð¿Ð¾Ð»Ñ‚Ð¾ÑÑƒ Ð¿Ð¾Ð»Ñ‚Ð¾ÑÑ‹ Ð¿Ð¾Ð»Ñ‚Ð¾ÑÐ¾Ð¼ Ð¿Ð¾Ð»Ñ‚Ð¾ÑÐ°Ð¼Ð¸ Ð¿Ð¾Ð»Ñ‚Ð¾ÑÐ°Ð¼ Ð¿Ð¾Ð»Ñ‚Ð¾ÑÐ°Ñ… \n\nÑˆÐµÑÑ‚ÑŒÐ´ÐµÑÑÑ‚ ÑˆÐµÑÑ‚ÑŒÐ´ÐµÑÑÑ‚Ð¾Ð³Ð¾ ÑˆÐµÑÑ‚ÑŒÐ´ÐµÑÑÑ‚Ð¾Ð¼Ñƒ ÑˆÐµÑÑ‚ÑŒÑŽÐ´ÐµÑÑÑ‚ÑŒÑŽ ÑˆÐµÑÑ‚ÑŒÐ´ÐµÑÑÑ‚Ð¾Ð¼ ÑˆÐµÑÑ‚ÑŒÐ´ÐµÑÑÑ‚Ñ‹Ð¹ ÑˆÐµÑÑ‚ÑŒÐ´ÐµÑÑÑ‚Ñ‹Ð¼ ÑˆÐµÑÑ‚Ð¸Ð´ÐµÑÑÑ‚Ñ‹Ðµ ÑˆÐµÑÑ‚Ð¸Ð´ÐµÑÑÑ‚Ð¸ \nÑˆÐµÑÑ‚ÑŒÐ´ÐµÑÑÑ‚Ñ‹Ñ… \n\nÑÐµÐ¼ÑŒÐ´ÐµÑÑÑ‚ ÑÐµÐ¼ÑŒÐ´ÐµÑÑÑ‚Ð¾Ð³Ð¾ ÑÐµÐ¼ÑŒÐ´ÐµÑÑÑ‚Ð¾Ð¼Ñƒ ÑÐµÐ¼ÑŒÑŽÐ´ÐµÑÑÑ‚ÑŒÑŽ ÑÐµÐ¼ÑŒÐ´ÐµÑÑÑ‚Ð¾Ð¼ ÑÐµÐ¼ÑŒÐ´ÐµÑÑÑ‚Ñ‹Ð¹ ÑÐµÐ¼ÑŒÐ´ÐµÑÑÑ‚Ñ‹Ð¼ ÑÐµÐ¼Ð¸Ð´ÐµÑÑÑ‚Ð¸ ÑÐµÐ¼ÑŒÐ´ÐµÑÑÑ‚Ñ‹Ñ… \n\nÐ²Ð¾ÑÐµÐ¼ÑŒÐ´ÐµÑÑÑ‚ Ð²Ð¾ÑÐµÐ¼ÑŒÐ´ÐµÑÑÑ‚Ð¾Ð³Ð¾ Ð²Ð¾ÑÐµÐ¼ÑŒÐ´ÐµÑÑÑ‚Ð¾Ð¼Ñƒ Ð²Ð¾ÑÐµÐ¼ÑŒÑŽÐ´ÐµÑÑÑ‚ÑŒÑŽ Ð²Ð¾ÑÐµÐ¼ÑŒÐ´ÐµÑÑÑ‚Ð¾Ð¼ Ð²Ð¾ÑÐµÐ¼ÑŒÐ´ÐµÑÑÑ‚Ñ‹Ð¹ Ð²Ð¾ÑÐµÐ¼ÑŒÐ´ÐµÑÑÑ‚Ñ‹Ð¼ Ð²Ð¾ÑÐµÐ¼Ð¸Ð´ÐµÑÑÑ‚Ð¸ \nÐ²Ð¾ÑÑŒÐ¼Ð¸Ð´ÐµÑÑÑ‚Ð¸ Ð²Ð¾ÑÑŒÐ¼Ð¸Ð´ÐµÑÑÑ‚Ñ‹Ñ… \n\nÐ´ÐµÐ²ÑÐ½Ð¾ÑÑ‚Ð¾ Ð´ÐµÐ²ÑÐ½Ð¾ÑÑ‚Ð¾Ð³Ð¾ Ð´ÐµÐ²ÑÐ½Ð¾ÑÑ‚Ð¾Ð¼Ñƒ Ð´ÐµÐ²ÑÐ½Ð¾ÑÑ‚Ð¾Ð¼ Ð´ÐµÐ²ÑÐ½Ð¾ÑÑ‚Ñ‹Ð¹ Ð´ÐµÐ²ÑÐ½Ð¾ÑÑ‚Ñ‹Ð¼ Ð´ÐµÐ²ÑÐ½Ð¾ÑÑ‚Ð° Ð´ÐµÐ²ÑÐ½Ð¾ÑÑ‚Ñ‹Ñ… \n\nÑÑ‚Ð¾ ÑÐ¾Ñ‚Ð¾Ð³Ð¾ ÑÐ¾Ñ‚Ð¾Ð¼Ñƒ ÑÐ¾Ñ‚Ð¾Ð¼ ÑÐ¾Ñ‚ÐµÐ½ ÑÐ¾Ñ‚Ñ‹Ð¹ ÑÐ¾Ñ‚Ñ‹Ð¼ ÑÑ‚Ð° \nÑÑ‚Ð¾Ð»ÑŒÐ½Ð¸Ðº ÑÑ‚Ð¾Ð»ÑŒÐ½Ð¸ÐºÐ° ÑÑ‚Ð¾Ð»ÑŒÐ½Ð¸ÐºÑƒ ÑÑ‚Ð¾Ð»ÑŒÐ½Ð¸ÐºÐµ ÑÑ‚Ð¾Ð»ÑŒÐ½Ð¸ÐºÐ¸ ÑÑ‚Ð¾Ð»ÑŒÐ½Ð¸ÐºÐ¾Ð¼ ÑÑ‚Ð¾Ð»ÑŒÐ½Ð¸ÐºÐ°Ð¼Ð¸ \nÑÐ¾Ñ‚ÐºÐ° ÑÐ¾Ñ‚ÐºÐ¸ ÑÐ¾Ñ‚ÐºÐµ ÑÐ¾Ñ‚ÐºÐ¾Ð¹ ÑÐ¾Ñ‚ÐºÐ°Ð¼Ð¸ ÑÐ¾Ñ‚ÐºÐ°Ð¼ ÑÐ¾Ñ‚ÐºÐ°Ñ… \nÑÐ¾Ñ‚Ð½Ñ ÑÐ¾Ñ‚Ð½Ð¸ ÑÐ¾Ñ‚Ð½Ðµ ÑÐ¾Ñ‚Ð½ÐµÐ¹ ÑÐ¾Ñ‚Ð½ÑÐ¼Ð¸ ÑÐ¾Ñ‚Ð½ÑÐ¼ ÑÐ¾Ñ‚Ð½ÑÑ… \n\nÐ´Ð²ÐµÑÑ‚Ð¸ Ð´Ð²ÑƒÐ¼ÑÑÑ‚Ð°Ð¼Ð¸ Ð´Ð²ÑƒÑ…ÑÐ¾Ñ‚Ð¾Ð³Ð¾ Ð´Ð²ÑƒÑ…ÑÐ¾Ñ‚Ð¾Ð¼Ñƒ Ð´Ð²ÑƒÑ…ÑÐ¾Ñ‚Ð¾Ð¼ Ð´Ð²ÑƒÑ…ÑÐ¾Ñ‚Ñ‹Ð¹ Ð´Ð²ÑƒÑ…ÑÐ¾Ñ‚Ñ‹Ð¼ Ð´Ð²ÑƒÐ¼ÑÑ‚Ð°Ð¼ Ð´Ð²ÑƒÑ…ÑÑ‚Ð°Ñ… Ð´Ð²ÑƒÑ…ÑÐ¾Ñ‚ \n\nÑ‚Ñ€Ð¸ÑÑ‚Ð° Ñ‚Ñ€ÐµÐ¼ÑÑÑ‚Ð°Ð¼Ð¸ Ñ‚Ñ€ÐµÑ…ÑÐ¾Ñ‚Ð¾Ð³Ð¾ Ñ‚Ñ€ÐµÑ…ÑÐ¾Ñ‚Ð¾Ð¼Ñƒ Ñ‚Ñ€ÐµÑ…ÑÐ¾Ñ‚Ð¾Ð¼ Ñ‚Ñ€ÐµÑ…ÑÐ¾Ñ‚Ñ‹Ð¹ Ñ‚Ñ€ÐµÑ…ÑÐ¾Ñ‚Ñ‹Ð¼ Ñ‚Ñ€ÐµÐ¼ÑÑ‚Ð°Ð¼ Ñ‚Ñ€ÐµÑ…ÑÑ‚Ð°Ñ… Ñ‚Ñ€ÐµÑ…ÑÐ¾Ñ‚ \n\nÑ‡ÐµÑ‚Ñ‹Ñ€ÐµÑÑ‚Ð° Ñ‡ÐµÑ‚Ñ‹Ñ€ÐµÑ…ÑÐ¾Ñ‚Ð¾Ð³Ð¾ Ñ‡ÐµÑ‚Ñ‹Ñ€ÐµÑ…ÑÐ¾Ñ‚Ð¾Ð¼Ñƒ Ñ‡ÐµÑ‚Ñ‹Ñ€ÑŒÐ¼ÑÑÑ‚Ð°Ð¼Ð¸ Ñ‡ÐµÑ‚Ñ‹Ñ€ÐµÑ…ÑÐ¾Ñ‚Ð¾Ð¼ Ñ‡ÐµÑ‚Ñ‹Ñ€ÐµÑ…ÑÐ¾Ñ‚Ñ‹Ð¹ Ñ‡ÐµÑ‚Ñ‹Ñ€ÐµÑ…ÑÐ¾Ñ‚Ñ‹Ð¼ Ñ‡ÐµÑ‚Ñ‹Ñ€ÐµÐ¼ÑÑ‚Ð°Ð¼ Ñ‡ÐµÑ‚Ñ‹Ñ€ÐµÑ…ÑÑ‚Ð°Ñ… \nÑ‡ÐµÑ‚Ñ‹Ñ€ÐµÑ…ÑÐ¾Ñ‚ \n\nÐ¿ÑÑ‚ÑŒÑÐ¾Ñ‚ Ð¿ÑÑ‚Ð¸ÑÐ¾Ñ‚Ð¾Ð³Ð¾ Ð¿ÑÑ‚Ð¸ÑÐ¾Ñ‚Ð¾Ð¼Ñƒ Ð¿ÑÑ‚ÑŒÑŽÑÑ‚Ð°Ð¼Ð¸ Ð¿ÑÑ‚Ð¸ÑÐ¾Ñ‚Ð¾Ð¼ Ð¿ÑÑ‚Ð¸ÑÐ¾Ñ‚Ñ‹Ð¹ Ð¿ÑÑ‚Ð¸ÑÐ¾Ñ‚Ñ‹Ð¼ Ð¿ÑÑ‚Ð¸ÑÑ‚Ð°Ð¼ Ð¿ÑÑ‚Ð¸ÑÑ‚Ð°Ñ… Ð¿ÑÑ‚Ð¸ÑÐ¾Ñ‚ \nÐ¿ÑÑ‚Ð¸ÑÐ¾Ñ‚ÐºÐ° Ð¿ÑÑ‚Ð¸ÑÐ¾Ñ‚ÐºÐ¸ Ð¿ÑÑ‚Ð¸ÑÐ¾Ñ‚ÐºÐµ Ð¿ÑÑ‚Ð¸ÑÐ¾Ñ‚ÐºÐ¾Ð¹ Ð¿ÑÑ‚Ð¸ÑÐ¾Ñ‚ÐºÐ°Ð¼Ð¸ Ð¿ÑÑ‚Ð¸ÑÐ¾Ñ‚ÐºÐ°Ð¼ Ð¿ÑÑ‚Ð¸ÑÐ¾Ñ‚ÐºÐ¾ÑŽ Ð¿ÑÑ‚Ð¸ÑÐ¾Ñ‚ÐºÐ°Ñ… \nÐ¿ÑÑ‚Ð¸Ñ…Ð°Ñ‚ÐºÐ° Ð¿ÑÑ‚Ð¸Ñ…Ð°Ñ‚ÐºÐ¸ Ð¿ÑÑ‚Ð¸Ñ…Ð°Ñ‚ÐºÐµ Ð¿ÑÑ‚Ð¸Ñ…Ð°Ñ‚ÐºÐ¾Ð¹ Ð¿ÑÑ‚Ð¸Ñ…Ð°Ñ‚ÐºÐ°Ð¼Ð¸ Ð¿ÑÑ‚Ð¸Ñ…Ð°Ñ‚ÐºÐ°Ð¼ Ð¿ÑÑ‚Ð¸Ñ…Ð°Ñ‚ÐºÐ¾ÑŽ Ð¿ÑÑ‚Ð¸Ñ…Ð°Ñ‚ÐºÐ°Ñ… \nÐ¿ÑÑ‚Ð¸Ñ„Ð°Ð½ Ð¿ÑÑ‚Ð¸Ñ„Ð°Ð½Ñ‹ Ð¿ÑÑ‚Ð¸Ñ„Ð°Ð½Ðµ Ð¿ÑÑ‚Ð¸Ñ„Ð°Ð½Ð¾Ð¼ Ð¿ÑÑ‚Ð¸Ñ„Ð°Ð½Ð°Ð¼Ð¸ Ð¿ÑÑ‚Ð¸Ñ„Ð°Ð½Ð°Ñ… \n\nÑˆÐµÑÑ‚ÑŒÑÐ¾Ñ‚ ÑˆÐµÑÑ‚Ð¸ÑÐ¾Ñ‚Ð¾Ð³Ð¾ ÑˆÐµÑÑ‚Ð¸ÑÐ¾Ñ‚Ð¾Ð¼Ñƒ ÑˆÐµÑÑ‚ÑŒÑŽÑÑ‚Ð°Ð¼Ð¸ ÑˆÐµÑÑ‚Ð¸ÑÐ¾Ñ‚Ð¾Ð¼ ÑˆÐµÑÑ‚Ð¸ÑÐ¾Ñ‚Ñ‹Ð¹ ÑˆÐµÑÑ‚Ð¸ÑÐ¾Ñ‚Ñ‹Ð¼ ÑˆÐµÑÑ‚Ð¸ÑÑ‚Ð°Ð¼ ÑˆÐµÑÑ‚Ð¸ÑÑ‚Ð°Ñ… ÑˆÐµÑÑ‚Ð¸ÑÐ¾Ñ‚ \n\nÑÐµÐ¼ÑŒÑÐ¾Ñ‚ ÑÐµÐ¼Ð¸ÑÐ¾Ñ‚Ð¾Ð³Ð¾ ÑÐµÐ¼Ð¸ÑÐ¾Ñ‚Ð¾Ð¼Ñƒ ÑÐµÐ¼ÑŒÑŽÑÑ‚Ð°Ð¼Ð¸ ÑÐµÐ¼Ð¸ÑÐ¾Ñ‚Ð¾Ð¼ ÑÐµÐ¼Ð¸ÑÐ¾Ñ‚Ñ‹Ð¹ ÑÐµÐ¼Ð¸ÑÐ¾Ñ‚Ñ‹Ð¼ ÑÐµÐ¼Ð¸ÑÑ‚Ð°Ð¼ ÑÐµÐ¼Ð¸ÑÑ‚Ð°Ñ… ÑÐµÐ¼Ð¸ÑÐ¾Ñ‚ \n\nÐ²Ð¾ÑÐµÐ¼ÑŒÑÐ¾Ñ‚ Ð²Ð¾ÑÐµÐ¼Ð¸ÑÐ¾Ñ‚Ð¾Ð³Ð¾ Ð²Ð¾ÑÐµÐ¼Ð¸ÑÐ¾Ñ‚Ð¾Ð¼Ñƒ Ð²Ð¾ÑÐµÐ¼Ð¸ÑÐ¾Ñ‚Ð¾Ð¼ Ð²Ð¾ÑÐµÐ¼Ð¸ÑÐ¾Ñ‚Ñ‹Ð¹ Ð²Ð¾ÑÐµÐ¼Ð¸ÑÐ¾Ñ‚Ñ‹Ð¼ Ð²Ð¾ÑÑŒÐ¼Ð¸ÑÑ‚Ð°Ð¼Ð¸ Ð²Ð¾ÑÑŒÐ¼Ð¸ÑÑ‚Ð°Ð¼ Ð²Ð¾ÑÑŒÐ¼Ð¸ÑÑ‚Ð°Ñ… Ð²Ð¾ÑÑŒÐ¼Ð¸ÑÐ¾Ñ‚ \n\nÐ´ÐµÐ²ÑÑ‚ÑŒÑÐ¾Ñ‚ Ð´ÐµÐ²ÑÑ‚Ð¸ÑÐ¾Ñ‚Ð¾Ð³Ð¾ Ð´ÐµÐ²ÑÑ‚Ð¸ÑÐ¾Ñ‚Ð¾Ð¼Ñƒ Ð´ÐµÐ²ÑÑ‚ÑŒÑŽÑÑ‚Ð°Ð¼Ð¸ Ð´ÐµÐ²ÑÑ‚Ð¸ÑÐ¾Ñ‚Ð¾Ð¼ Ð´ÐµÐ²ÑÑ‚Ð¸ÑÐ¾Ñ‚Ñ‹Ð¹ Ð´ÐµÐ²ÑÑ‚Ð¸ÑÐ¾Ñ‚Ñ‹Ð¼ Ð´ÐµÐ²ÑÑ‚Ð¸ÑÑ‚Ð°Ð¼ Ð´ÐµÐ²ÑÑ‚Ð¸ÑÑ‚Ð°Ñ… Ð´ÐµÐ²ÑÑ‚Ð¸ÑÐ¾Ñ‚ \n\nÑ‚Ñ‹ÑÑÑ‡Ð° Ñ‚Ñ‹ÑÑÑ‡Ð½Ð¾Ð³Ð¾ Ñ‚Ñ‹ÑÑÑ‡Ð½Ð¾Ð¼Ñƒ Ñ‚Ñ‹ÑÑÑ‡Ð½Ð¾Ð¼ Ñ‚Ñ‹ÑÑÑ‡Ð½Ñ‹Ð¹ Ñ‚Ñ‹ÑÑÑ‡Ð½Ñ‹Ð¼ Ñ‚Ñ‹ÑÑÑ‡Ð°Ð¼ Ñ‚Ñ‹ÑÑÑ‡Ð°Ñ… Ñ‚Ñ‹ÑÑÑ‡ÐµÐ¹ Ñ‚Ñ‹ÑÑÑ‡ Ñ‚Ñ‹ÑÑÑ‡Ð¸ Ñ‚Ñ‹Ñ \nÐºÐ¾ÑÐ°Ñ€ÑŒ ÐºÐ¾ÑÐ°Ñ€Ñ ÐºÐ¾ÑÐ°Ñ€Ñƒ ÐºÐ¾ÑÐ°Ñ€ÐµÐ¼ ÐºÐ¾ÑÐ°Ñ€ÑÐ¼Ð¸ ÐºÐ¾ÑÐ°Ñ€ÑÑ… ÐºÐ¾ÑÐ°Ñ€ÑÐ¼ ÐºÐ¾ÑÐ°Ñ€ÐµÐ¹ \n\nÐ´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ñ‹Ð¹ Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ð¾Ð³Ð¾ Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ð¾Ð¼Ñƒ Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ñ‹Ð¼ Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ð¾Ð¼ Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ð°Ñ Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ð¾Ð¹ \nÐ´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½ÑƒÑŽ Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ð¾ÑŽ Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ð¾Ðµ Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ñ‹Ðµ Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ñ‹Ñ… Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ñ‹Ð¼Ð¸ \n\nÐ´Ð²Ð°Ð´Ñ†Ð°Ñ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ñ‹Ð¹ Ð´Ð²Ð°Ð´Ñ†Ð°Ñ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ð¾Ð³Ð¾ Ð´Ð²Ð°Ð´Ñ†Ð°Ñ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ð¾Ð¼Ñƒ Ð´Ð²Ð°Ð´Ñ†Ð°Ñ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ñ‹Ð¼ Ð´Ð²Ð°Ð´Ñ†Ð°Ñ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ð¾Ð¼ Ð´Ð²Ð°Ð´Ñ†Ð°Ñ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ð°Ñ \nÐ´Ð²Ð°Ð´Ñ†Ð°Ñ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ð¾Ð¹ Ð´Ð²Ð°Ð´Ñ†Ð°Ñ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½ÑƒÑŽ Ð´Ð²Ð°Ð´Ñ†Ð°Ñ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ð¾ÑŽ Ð´Ð²Ð°Ð´Ñ†Ð°Ñ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ð¾Ðµ Ð´Ð²Ð°Ð´Ñ†Ð°Ñ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ñ‹Ðµ Ð´Ð²Ð°Ð´Ñ†Ð°Ñ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ñ‹Ñ… \nÐ´Ð²Ð°Ð´Ñ†Ð°Ñ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ñ‹Ð¼Ð¸ \n\nÑ‚Ñ€Ð¸Ð´Ñ†Ð°Ñ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ñ‹Ð¹ Ñ‚Ñ€Ð¸Ð´Ñ†Ð°Ñ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ð¾Ð³Ð¾ Ñ‚Ñ€Ð¸Ð´Ñ†Ð°Ñ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ð¾Ð¼Ñƒ Ñ‚Ñ€Ð¸Ð´Ñ†Ð°Ñ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ñ‹Ð¼ Ñ‚Ñ€Ð¸Ð´Ñ†Ð°Ñ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ð¾Ð¼ Ñ‚Ñ€Ð¸Ð´Ñ†Ð°Ñ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ð°Ñ \nÑ‚Ñ€Ð¸Ð´Ñ†Ð°Ñ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ð¾Ð¹ Ñ‚Ñ€Ð¸Ð´Ñ†Ð°Ñ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½ÑƒÑŽ Ñ‚Ñ€Ð¸Ð´Ñ†Ð°Ñ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ð¾ÑŽ Ñ‚Ñ€Ð¸Ð´Ñ†Ð°Ñ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ð¾Ðµ Ñ‚Ñ€Ð¸Ð´Ñ†Ð°Ñ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ñ‹Ðµ Ñ‚Ñ€Ð¸Ð´Ñ†Ð°Ñ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ñ‹Ñ… \nÑ‚Ñ€Ð¸Ð´Ñ†Ð°Ñ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ñ‹Ð¼Ð¸ \n\nÑÐ¾Ñ€Ð¾ÐºÐ°Ñ‚Ñ‹ÑÑÑ‡Ð½Ñ‹Ð¹ ÑÐ¾Ñ€Ð¾ÐºÐ°Ñ‚Ñ‹ÑÑÑ‡Ð½Ð¾Ð³Ð¾ ÑÐ¾Ñ€Ð¾ÐºÐ°Ñ‚Ñ‹ÑÑÑ‡Ð½Ð¾Ð¼Ñƒ ÑÐ¾Ñ€Ð¾ÐºÐ°Ñ‚Ñ‹ÑÑÑ‡Ð½Ñ‹Ð¼ ÑÐ¾Ñ€Ð¾ÐºÐ°Ñ‚Ñ‹ÑÑÑ‡Ð½Ð¾Ð¼ ÑÐ¾Ñ€Ð¾ÐºÐ°Ñ‚Ñ‹ÑÑÑ‡Ð½Ð°Ñ \nÑÐ¾Ñ€Ð¾ÐºÐ°Ñ‚Ñ‹ÑÑÑ‡Ð½Ð¾Ð¹ ÑÐ¾Ñ€Ð¾ÐºÐ°Ñ‚Ñ‹ÑÑÑ‡Ð½ÑƒÑŽ ÑÐ¾Ñ€Ð¾ÐºÐ°Ñ‚Ñ‹ÑÑÑ‡Ð½Ð¾ÑŽ ÑÐ¾Ñ€Ð¾ÐºÐ°Ñ‚Ñ‹ÑÑÑ‡Ð½Ð¾Ðµ ÑÐ¾Ñ€Ð¾ÐºÐ°Ñ‚Ñ‹ÑÑÑ‡Ð½Ñ‹Ðµ ÑÐ¾Ñ€Ð¾ÐºÐ°Ñ‚Ñ‹ÑÑÑ‡Ð½Ñ‹Ñ… \nÑÐ¾Ñ€Ð¾ÐºÐ°Ñ‚Ñ‹ÑÑÑ‡Ð½Ñ‹Ð¼Ð¸ \n\nÐ¿ÑÑ‚Ð¸Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ñ‹Ð¹ Ð¿ÑÑ‚Ð¸Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ð¾Ð³Ð¾ Ð¿ÑÑ‚Ð¸Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ð¾Ð¼Ñƒ Ð¿ÑÑ‚Ð¸Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ñ‹Ð¼ Ð¿ÑÑ‚Ð¸Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ð¾Ð¼ Ð¿ÑÑ‚Ð¸Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ð°Ñ \nÐ¿ÑÑ‚Ð¸Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ð¾Ð¹ Ð¿ÑÑ‚Ð¸Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½ÑƒÑŽ Ð¿ÑÑ‚Ð¸Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ð¾ÑŽ Ð¿ÑÑ‚Ð¸Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ð¾Ðµ Ð¿ÑÑ‚Ð¸Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ñ‹Ðµ Ð¿ÑÑ‚Ð¸Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ñ‹Ñ… \nÐ¿ÑÑ‚Ð¸Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ñ‹Ð¼Ð¸ \n\nÑˆÐµÑÑ‚Ð¸Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ñ‹Ð¹ ÑˆÐµÑÑ‚Ð¸Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ð¾Ð³Ð¾ ÑˆÐµÑÑ‚Ð¸Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ð¾Ð¼Ñƒ ÑˆÐµÑÑ‚Ð¸Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ñ‹Ð¼ ÑˆÐµÑÑ‚Ð¸Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ð¾Ð¼ ÑˆÐµÑÑ‚Ð¸Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ð°Ñ \nÑˆÐµÑÑ‚Ð¸Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ð¾Ð¹ ÑˆÐµÑÑ‚Ð¸Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½ÑƒÑŽ ÑˆÐµÑÑ‚Ð¸Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ð¾ÑŽ ÑˆÐµÑÑ‚Ð¸Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ð¾Ðµ ÑˆÐµÑÑ‚Ð¸Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ñ‹Ðµ ÑˆÐµÑÑ‚Ð¸Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ñ‹Ñ… \nÑˆÐµÑÑ‚Ð¸Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ñ‹Ð¼Ð¸ \n\nÑÐµÐ¼Ð¸Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ñ‹Ð¹ ÑÐµÐ¼Ð¸Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ð¾Ð³Ð¾ ÑÐµÐ¼Ð¸Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ð¾Ð¼Ñƒ ÑÐµÐ¼Ð¸Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ñ‹Ð¼ ÑÐµÐ¼Ð¸Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ð¾Ð¼ ÑÐµÐ¼Ð¸Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ð°Ñ \nÑÐµÐ¼Ð¸Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ð¾Ð¹ ÑÐµÐ¼Ð¸Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½ÑƒÑŽ ÑÐµÐ¼Ð¸Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ð¾ÑŽ ÑÐµÐ¼Ð¸Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ð¾Ðµ ÑÐµÐ¼Ð¸Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ñ‹Ðµ ÑÐµÐ¼Ð¸Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ñ‹Ñ… \nÑÐµÐ¼Ð¸Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ñ‹Ð¼Ð¸ \n\nÐ²Ð¾ÑÑŒÐ¼Ð¸Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ñ‹Ð¹ Ð²Ð¾ÑÑŒÐ¼Ð¸Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ð¾Ð³Ð¾ Ð²Ð¾ÑÑŒÐ¼Ð¸Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ð¾Ð¼Ñƒ Ð²Ð¾ÑÑŒÐ¼Ð¸Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ñ‹Ð¼ Ð²Ð¾ÑÑŒÐ¼Ð¸Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ð¾Ð¼ Ð²Ð¾ÑÑŒÐ¼Ð¸Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ð°Ñ \nÐ²Ð¾ÑÑŒÐ¼Ð¸Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ð¾Ð¹ Ð²Ð¾ÑÑŒÐ¼Ð¸Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½ÑƒÑŽ Ð²Ð¾ÑÑŒÐ¼Ð¸Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ð¾ÑŽ Ð²Ð¾ÑÑŒÐ¼Ð¸Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ð¾Ðµ Ð²Ð¾ÑÑŒÐ¼Ð¸Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ñ‹Ðµ Ð²Ð¾ÑÑŒÐ¼Ð¸Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ñ‹Ñ… \nÐ²Ð¾ÑÑŒÐ¼Ð¸Ð´ÐµÑÑÑ‚Ð¸Ñ‚Ñ‹ÑÑÑ‡Ð½Ñ‹Ð¼Ð¸ \n\nÑÑ‚Ð¾Ñ‚Ñ‹ÑÑÑ‡Ð½Ñ‹Ð¹ ÑÑ‚Ð¾Ñ‚Ñ‹ÑÑÑ‡Ð½Ð¾Ð³Ð¾ ÑÑ‚Ð¾Ñ‚Ñ‹ÑÑÑ‡Ð½Ð¾Ð¼Ñƒ ÑÑ‚Ð¾Ñ‚Ñ‹ÑÑÑ‡Ð½Ñ‹Ð¼ ÑÑ‚Ð¾Ñ‚Ñ‹ÑÑÑ‡Ð½Ð¾Ð¼ ÑÑ‚Ð¾Ñ‚Ñ‹ÑÑÑ‡Ð½Ð°Ñ ÑÑ‚Ð¾Ñ‚Ñ‹ÑÑÑ‡Ð½Ð¾Ð¹ ÑÑ‚Ð¾Ñ‚Ñ‹ÑÑÑ‡Ð½ÑƒÑŽ ÑÑ‚Ð¾Ñ‚Ñ‹ÑÑÑ‡Ð½Ð¾Ðµ \nÑÑ‚Ð¾Ñ‚Ñ‹ÑÑÑ‡Ð½Ñ‹Ðµ ÑÑ‚Ð¾Ñ‚Ñ‹ÑÑÑ‡Ð½Ñ‹Ñ… ÑÑ‚Ð¾Ñ‚Ñ‹ÑÑÑ‡Ð½Ñ‹Ð¼Ð¸ ÑÑ‚Ð¾Ñ‚Ñ‹ÑÑÑ‡Ð½Ð¾ÑŽ \n\nÐ¼Ð¸Ð»Ð»Ð¸Ð¾Ð½ Ð¼Ð¸Ð»Ð»Ð¸Ð¾Ð½Ð½Ð¾Ð³Ð¾ Ð¼Ð¸Ð»Ð»Ð¸Ð¾Ð½Ð¾Ð² Ð¼Ð¸Ð»Ð»Ð¸Ð¾Ð½Ð½Ð¾Ð¼Ñƒ Ð¼Ð¸Ð»Ð»Ð¸Ð¾Ð½Ð½Ð¾Ð¼ Ð¼Ð¸Ð»Ð»Ð¸Ð¾Ð½Ð½Ñ‹Ð¹ Ð¼Ð¸Ð»Ð»Ð¸Ð¾Ð½Ð½Ñ‹Ð¼ Ð¼Ð¸Ð»Ð»Ð¸Ð¾Ð½Ð¾Ð¼ Ð¼Ð¸Ð»Ð»Ð¸Ð¾Ð½Ð° Ð¼Ð¸Ð»Ð»Ð¸Ð¾Ð½Ðµ Ð¼Ð¸Ð»Ð»Ð¸Ð¾Ð½Ñƒ \nÐ¼Ð¸Ð»Ð»Ð¸Ð¾Ð½Ð¾Ð² \nÐ»ÑÐ¼ Ð»ÑÐ¼Ð° Ð»ÑÐ¼Ñ‹ Ð»ÑÐ¼Ð¾Ð¼ Ð»ÑÐ¼Ð°Ð¼Ð¸ Ð»ÑÐ¼Ð°Ñ… Ð»ÑÐ¼Ð¾Ð² \nÐ¼Ð»Ð½ \n\nÐ´ÐµÑÑÑ‚Ð¸Ð¼Ð¸Ð»Ð»Ð¸Ð¾Ð½Ð½Ð°Ñ Ð´ÐµÑÑÑ‚Ð¸Ð¼Ð¸Ð»Ð»Ð¸Ð¾Ð½Ð½Ð¾Ð¹ Ð´ÐµÑÑÑ‚Ð¸Ð¼Ð¸Ð»Ð»Ð¸Ð¾Ð½Ð½Ñ‹Ð¼Ð¸ Ð´ÐµÑÑÑ‚Ð¸Ð¼Ð¸Ð»Ð»Ð¸Ð¾Ð½Ð½Ñ‹Ð¹ Ð´ÐµÑÑÑ‚Ð¸Ð¼Ð¸Ð»Ð»Ð¸Ð¾Ð½Ð½Ñ‹Ð¼ Ð´ÐµÑÑÑ‚Ð¸Ð¼Ð¸Ð»Ð»Ð¸Ð¾Ð½Ð½Ð¾Ð¼Ñƒ \nÐ´ÐµÑÑÑ‚Ð¸Ð¼Ð¸Ð»Ð»Ð¸Ð¾Ð½Ð½Ñ‹Ð¼Ð¸ Ð´ÐµÑÑÑ‚Ð¸Ð¼Ð¸Ð»Ð»Ð¸Ð¾Ð½Ð½ÑƒÑŽ Ð´ÐµÑÑÑ‚Ð¸Ð¼Ð¸Ð»Ð»Ð¸Ð¾Ð½Ð½Ð¾Ðµ  Ð´ÐµÑÑÑ‚Ð¸Ð¼Ð¸Ð»Ð»Ð¸Ð¾Ð½Ð½Ñ‹Ðµ Ð´ÐµÑÑÑ‚Ð¸Ð¼Ð¸Ð»Ð»Ð¸Ð¾Ð½Ð½Ñ‹Ñ… Ð´ÐµÑÑÑ‚Ð¸Ð¼Ð¸Ð»Ð»Ð¸Ð¾Ð½Ð½Ð¾ÑŽ \n\nÐ¼Ð¸Ð»Ð»Ð¸Ð°Ñ€Ð´ Ð¼Ð¸Ð»Ð»Ð¸Ð°Ñ€Ð´Ð½Ð¾Ð³Ð¾ Ð¼Ð¸Ð»Ð»Ð¸Ð°Ñ€Ð´Ð½Ð¾Ð¼Ñƒ Ð¼Ð¸Ð»Ð»Ð¸Ð°Ñ€Ð´Ð½Ð¾Ð¼ Ð¼Ð¸Ð»Ð»Ð¸Ð°Ñ€Ð´Ð½Ñ‹Ð¹ Ð¼Ð¸Ð»Ð»Ð¸Ð°Ñ€Ð´Ð½Ñ‹Ð¼ Ð¼Ð¸Ð»Ð»Ð¸Ð°Ñ€Ð´Ð¾Ð¼ Ð¼Ð¸Ð»Ð»Ð¸Ð°Ñ€Ð´Ð° Ð¼Ð¸Ð»Ð»Ð¸Ð°Ñ€Ð´Ðµ Ð¼Ð¸Ð»Ð»Ð¸Ð°Ñ€Ð´Ñƒ \nÐ¼Ð¸Ð»Ð»Ð¸Ð°Ñ€Ð´Ð¾Ð² \nÐ»ÑÑ€Ð´ Ð»ÑÑ€Ð´Ð° Ð»ÑÑ€Ð´Ñ‹ Ð»ÑÑ€Ð´Ð¾Ð¼ Ð»ÑÑ€Ð´Ð°Ð¼Ð¸ Ð»ÑÑ€Ð´Ð°Ñ… Ð»ÑÑ€Ð´Ð¾Ð² \nÐ¼Ð»Ñ€Ð´ \n\nÑ‚Ñ€Ð¸Ð»Ð»Ð¸Ð¾Ð½ Ñ‚Ñ€Ð¸Ð»Ð»Ð¸Ð¾Ð½Ð½Ð¾Ð³Ð¾ Ñ‚Ñ€Ð¸Ð»Ð»Ð¸Ð¾Ð½Ð½Ð¾Ð¼Ñƒ Ñ‚Ñ€Ð¸Ð»Ð»Ð¸Ð¾Ð½Ð½Ð¾Ð¼ Ñ‚Ñ€Ð¸Ð»Ð»Ð¸Ð¾Ð½Ð½Ñ‹Ð¹ Ñ‚Ñ€Ð¸Ð»Ð»Ð¸Ð¾Ð½Ð½Ñ‹Ð¼ Ñ‚Ñ€Ð¸Ð»Ð»Ð¸Ð¾Ð½Ð¾Ð¼ Ñ‚Ñ€Ð¸Ð»Ð»Ð¸Ð¾Ð½Ð° Ñ‚Ñ€Ð¸Ð»Ð»Ð¸Ð¾Ð½Ðµ Ñ‚Ñ€Ð¸Ð»Ð»Ð¸Ð¾Ð½Ñƒ \nÑ‚Ñ€Ð¸Ð»Ð»Ð¸Ð¾Ð½Ð¾Ð² Ñ‚Ñ€Ð»Ð½ \n\nÐºÐ²Ð°Ð´Ñ€Ð¸Ð»Ð»Ð¸Ð¾Ð½ ÐºÐ²Ð°Ð´Ñ€Ð¸Ð»Ð»Ð¸Ð¾Ð½Ð½Ð¾Ð³Ð¾ ÐºÐ²Ð°Ð´Ñ€Ð¸Ð»Ð»Ð¸Ð¾Ð½Ð½Ð¾Ð¼Ñƒ ÐºÐ²Ð°Ð´Ñ€Ð¸Ð»Ð»Ð¸Ð¾Ð½Ð½Ñ‹Ð¹ ÐºÐ²Ð°Ð´Ñ€Ð¸Ð»Ð»Ð¸Ð¾Ð½Ð½Ñ‹Ð¼ ÐºÐ²Ð°Ð´Ñ€Ð¸Ð»Ð»Ð¸Ð¾Ð½Ð¾Ð¼ ÐºÐ²Ð°Ð´Ñ€Ð¸Ð»Ð»Ð¸Ð¾Ð½Ð° ÐºÐ²Ð°Ð´Ñ€Ð¸Ð»Ð»Ð¸Ð¾Ð½Ðµ \nÐºÐ²Ð°Ð´Ñ€Ð¸Ð»Ð»Ð¸Ð¾Ð½Ñƒ ÐºÐ²Ð°Ð´Ñ€Ð¸Ð»Ð»Ð¸Ð¾Ð½Ð¾Ð² ÐºÐ²Ð°Ð´Ñ€Ð»Ð½ \n\nÐºÐ²Ð¸Ð½Ñ‚Ð¸Ð»Ð»Ð¸Ð¾Ð½ ÐºÐ²Ð¸Ð½Ñ‚Ð¸Ð»Ð»Ð¸Ð¾Ð½Ð½Ð¾Ð³Ð¾ ÐºÐ²Ð¸Ð½Ñ‚Ð¸Ð»Ð»Ð¸Ð¾Ð½Ð½Ð¾Ð¼Ñƒ ÐºÐ²Ð¸Ð½Ñ‚Ð¸Ð»Ð»Ð¸Ð¾Ð½Ð½Ñ‹Ð¹ ÐºÐ²Ð¸Ð½Ñ‚Ð¸Ð»Ð»Ð¸Ð¾Ð½Ð½Ñ‹Ð¼ ÐºÐ²Ð¸Ð½Ñ‚Ð¸Ð»Ð»Ð¸Ð¾Ð½Ð¾Ð¼ ÐºÐ²Ð¸Ð½Ñ‚Ð¸Ð»Ð»Ð¸Ð¾Ð½Ð° ÐºÐ²Ð¸Ð½Ñ‚Ð¸Ð»Ð»Ð¸Ð¾Ð½Ðµ \nÐºÐ²Ð¸Ð½Ñ‚Ð¸Ð»Ð»Ð¸Ð¾Ð½Ñƒ ÐºÐ²Ð¸Ð½Ñ‚Ð¸Ð»Ð»Ð¸Ð¾Ð½Ð¾Ð² ÐºÐ²Ð¸Ð½Ñ‚Ð»Ð½ \n\ni ii iii iv v vi vii viii ix x xi xii xiii xiv xv xvi xvii xviii xix xx xxi xxii xxiii xxiv xxv xxvi xxvii xxvii xxix\n'.split()))
A:spacy.lang.ru.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.ru.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.ru.lex_attrs.like_num(text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/cs/examples.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/cs/stop_words.py----------------------------------------
A:spacy.lang.cs.stop_words.STOP_WORDS->set('\na\naby\nahoj\naÄkoli\nale\nalespoÅˆ\nanebo\nani\naniÅ¾\nano\natd.\natp.\nasi\naspoÅˆ\naÅ¾\nbÄ›hem\nbez\nbeze\nblÃ­zko\nbohuÅ¾el\nbrzo\nbude\nbudeme\nbudeÅ¡\nbudete\nbudou\nbudu\nby\nbyl\nbyla\nbyli\nbylo\nbyly\nbys\nbÃ½t\nÄau\nchce\nchceme\nchceÅ¡\nchcete\nchci\nchtÄ›jÃ­\nchtÃ­t\nchuÅ¥\nchuti\nco\ncoÅ¾\ncz\nÄi\nÄlÃ¡nek\nÄlÃ¡nku\nÄlÃ¡nky\nÄtrnÃ¡ct\nÄtyÅ™i\ndÃ¡l\ndÃ¡le\ndaleko\ndalÅ¡Ã­\ndÄ›kovat\ndÄ›kujeme\ndÄ›kuji\nden\ndeset\ndevatenÃ¡ct\ndevÄ›t\ndnes\ndo\ndobrÃ½\ndocela\ndva\ndvacet\ndvanÃ¡ct\ndvÄ›\nemail\nho\nhodnÄ›\ni\njÃ¡\njak\njakmile\njako\njakoÅ¾\njde\nje\njeden\njedenÃ¡ct\njedna\njedno\njednou\njedou\njeho\njehoÅ¾\njej\njejÃ­\njejich\njejichÅ¾\njehoÅ¾\njelikoÅ¾\njemu\njen\njenom\njenÅ¾\njeÅ¾\njeÅ¡tÄ›\njestli\njestliÅ¾e\njeÅ¡tÄ›\nji\njÃ­\njich\njÃ­m\njim\njimi\njinak\njinÃ©\njiÅ¾\njsi\njsme\njsem\njsou\njste\nk\nkam\nkaÅ¾dÃ½\nkde\nkdo\nkdy\nkdyÅ¾\nke\nkolik\nkromÄ›\nkterÃ¡\nkterak\nkterou\nkterÃ©\nkteÅ™Ã­\nkterÃ½\nkvÅ¯li\nku\nmÃ¡\nmajÃ­\nmÃ¡lo\nmÃ¡m\nmÃ¡me\nmÃ¡Å¡\nmÃ¡te\nmÃ©\nmÄ›\nmezi\nmi\nmÃ­\nmÃ­t\nmne\nmnÄ›\nmnou\nmoc\nmohl\nmohou\nmoje\nmoji\nmoÅ¾nÃ¡\nmÅ¯j\nmusÃ­\nmÅ¯Å¾e\nmy\nna\nnad\nnade\nnÃ¡m\nnÃ¡mi\nnaproti\nnÃ¡s\nnÃ¡Å¡\nnaÅ¡e\nnaÅ¡i\nnaÄeÅ¾\nne\nnÄ›\nnebo\nnebyl\nnebyla\nnebyli\nnebyly\nnechÅ¥\nnÄ›co\nnedÄ›lÃ¡\nnedÄ›lajÃ­\nnedÄ›lÃ¡m\nnedÄ›lÃ¡me\nnedÄ›lÃ¡Å¡\nnedÄ›lÃ¡te\nnÄ›jak\nnejsi\nnejsou\nnÄ›kde\nnÄ›kdo\nnemajÃ­\nnemÃ¡me\nnemÃ¡te\nnemÄ›l\nnÄ›mu\nnÄ›muÅ¾\nnenÃ­\nnestaÄÃ­\nnÄ›\nnevadÃ­\nnovÃ©\nnovÃ½\nnovÃ­\nneÅ¾\nnic\nnich\nnÃ­\nnÃ­m\nnimi\nnula\no\nod\node\non\nona\noni\nono\nony\nosm\nosmnÃ¡ct\npak\npatnÃ¡ct\npÄ›t\npo\npod\npokud\npoÅ™Ã¡d\npouze\npotom\npozdÄ›\npravÃ©\npÅ™ed\npÅ™ede\npÅ™es\npÅ™ece\npro\nproÄ\nprosÃ­m\nprostÄ›\nproto\nproti\nprvnÃ­\nprÃ¡vÄ›\nprotoÅ¾e\npÅ™i\npÅ™iÄemÅ¾\nrovnÄ›\ns\nse\nsedm\nsedmnÃ¡ct\nsi\nsice\nskoro\nsic\nÅ¡est\nÅ¡estnÃ¡ct\nskoro\nsmÄ›jÃ­\nsmÃ­\nsnad\nspolu\nsta\nsvÅ¯j\nsvÃ©\nsvÃ¡\nsvÃ½ch\nsvÃ½m\nsvÃ½mi\nsvÅ¯j\nstÃ©\nsto\nstrana\nta\ntady\ntak\ntakhle\ntaky\ntakÃ©\ntakÅ¾e\ntam\ntÃ¡mhle\ntÃ¡mhleto\ntamto\ntÄ›\ntebe\ntebou\nteÄ\ntedy\nten\ntento\ntÃ©to\nti\ntÃ­m\ntÃ­mto\ntisÃ­c\ntisÃ­ce\nto\ntobÄ›\ntohle\ntohoto\ntom\ntomto\ntomu\ntomuto\ntoto\ntÅ™eba\ntÅ™i\ntÅ™inÃ¡ct\ntroÅ¡ku\ntrochu\ntu\ntuto\ntvÃ¡\ntvÃ©\ntvoje\ntvÅ¯j\nty\ntyto\ntÄ›m\ntÄ›ma\ntÄ›mi\nu\nurÄitÄ›\nuÅ¾\nv\nvÃ¡m\nvÃ¡mi\nvÃ¡s\nvÃ¡Å¡\nvaÅ¡e\nvaÅ¡i\nve\nveÄer\nvedle\nvÃ­ce\nvlastnÄ›\nvÅ¡ak\nvÅ¡echen\nvÅ¡echno\nvÅ¡ichni\nvÅ¯bec\nvy\nvÅ¾dy\nz\nzda\nza\nzde\nzaÄ\nzatÃ­mco\nze\nÅ¾e\n'.split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/cs/__init__.py----------------------------------------
spacy.lang.cs.__init__.Czech(Language)
spacy.lang.cs.__init__.CzechDefaults(BaseDefaults)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/cs/lex_attrs.py----------------------------------------
A:spacy.lang.cs.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.cs.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.cs.lex_attrs.like_num(text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/lv/stop_words.py----------------------------------------
A:spacy.lang.lv.stop_words.STOP_WORDS->set('\naiz\nap\napakÅ¡\napakÅ¡pus\nar\narÄ«\naugÅ¡pus\nbet\nbez\nbija\nbiji\nbiju\nbijÄm\nbijÄt\nbÅ«s\nbÅ«si\nbÅ«siet\nbÅ«sim\nbÅ«t\nbÅ«Å¡u\ncaur\ndiemÅ¾Ä“l\ndiezin\ndroÅ¡i\ndÄ“Ä¼\nesam\nesat\nesi\nesmu\ngan\ngar\niekam\niekams\niekÄm\niekÄms\niekÅ¡\niekÅ¡pus\nik\nir\nit\nitin\niz\nja\njau\njeb\njebÅ¡u\njel\njo\njÄ\nka\nkamÄ“r\nkaut\nkolÄ«dz\nkopÅ¡\nkÄ\nkÄ¼uva\nkÄ¼uvi\nkÄ¼uvu\nkÄ¼uvÄm\nkÄ¼uvÄt\nkÄ¼Å«s\nkÄ¼Å«si\nkÄ¼Å«siet\nkÄ¼Å«sim\nkÄ¼Å«st\nkÄ¼Å«stam\nkÄ¼Å«stat\nkÄ¼Å«sti\nkÄ¼Å«stu\nkÄ¼Å«t\nkÄ¼Å«Å¡u\nlabad\nlai\nlejpus\nlÄ«dz\nlÄ«dzko\nne\nnebÅ«t\nnedz\nnekÄ\nnevis\nnezin\nno\nnu\nnÄ“\notrpus\npa\npar\npat\npie\npirms\npret\npriekÅ¡\npÄr\npÄ“c\nstarp\ntad\ntak\ntapi\ntaps\ntapsi\ntapsiet\ntapsim\ntapt\ntapÄt\ntapÅ¡u\ntaÄu\nte\ntiec\ntiek\ntiekam\ntiekat\ntieku\ntik\ntika\ntikai\ntiki\ntikko\ntiklab\ntiklÄ«dz\ntiks\ntiksiet\ntiksim\ntikt\ntiku\ntikvien\ntikÄm\ntikÄt\ntikÅ¡u\ntomÄ“r\ntopat\nturpretim\nturpretÄ«\ntÄ\ntÄdÄ“Ä¼\ntÄlab\ntÄpÄ“c\nun\nuz\nvai\nvar\nvarat\nvarÄ“ja\nvarÄ“ji\nvarÄ“ju\nvarÄ“jÄm\nvarÄ“jÄt\nvarÄ“s\nvarÄ“si\nvarÄ“siet\nvarÄ“sim\nvarÄ“t\nvarÄ“Å¡u\nvien\nvirs\nvirspus\nvis\nviÅ†pus\nzem\nÄrpus\nÅ¡aipus\n'.split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/lv/__init__.py----------------------------------------
spacy.lang.lv.__init__.Latvian(Language)
spacy.lang.lv.__init__.LatvianDefaults(BaseDefaults)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/la/stop_words.py----------------------------------------
A:spacy.lang.la.stop_words.STOP_WORDS->set('\nab ac ad adhuc aliqui aliquis an ante apud at atque aut autem \n\ncum cur \n\nde deinde dum \n\nego enim ergo es est et etiam etsi ex \n\nfio \n\nhaud hic \n\niam idem igitur ille in infra inter interim ipse is ita \n\nmagis modo mox \n\nnam ne nec necque neque nisi non nos \n\no ob \n\nper possum post pro \n\nquae quam quare qui quia quicumque quidem quilibet quis quisnam quisquam quisque quisquis quo quoniam \n\nsed si sic sive sub sui sum super suus \n\ntam tamen trans tu tum \n\nubi uel uero\n\nvel vero\n'.split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/la/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.la.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/la/__init__.py----------------------------------------
spacy.lang.la.__init__.Latin(Language)
spacy.lang.la.__init__.LatinDefaults(BaseDefaults)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/la/lex_attrs.py----------------------------------------
A:spacy.lang.la.lex_attrs.roman_numerals_compile->re.compile('(?i)^(?=[MDCLXVI])M*(C[MD]|D?C{0,4})(X[CL]|L?X{0,4})(I[XV]|V?I{0,4})$')
A:spacy.lang.la.lex_attrs._num_words->set('\nunus una unum duo duae tres tria quattuor quinque sex septem octo novem decem\n'.split())
A:spacy.lang.la.lex_attrs._ordinal_words->set('\nprimus prima primum secundus secunda secundum tertius tertia tertium\n'.split())
spacy.lang.la.lex_attrs.like_num(text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ta/examples.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ta/stop_words.py----------------------------------------
A:spacy.lang.ta.stop_words.STOP_WORDS->set('\nà®’à®°à¯\nà®Žà®©à¯à®±à¯\nà®®à®±à¯à®±à¯à®®à¯\nà®‡à®¨à¯à®¤\nà®‡à®¤à¯\nà®Žà®©à¯à®±\nà®•à¯Šà®£à¯à®Ÿà¯\nà®Žà®©à¯à®ªà®¤à¯\nà®ªà®²\nà®†à®•à¯à®®à¯\nà®…à®²à¯à®²à®¤à¯\nà®…à®µà®°à¯\nà®¨à®¾à®©à¯\nà®‰à®³à¯à®³\nà®…à®¨à¯à®¤\nà®‡à®µà®°à¯\nà®Žà®©\nà®®à¯à®¤à®²à¯\nà®Žà®©à¯à®©\nà®‡à®°à¯à®¨à¯à®¤à¯\nà®šà®¿à®²\nà®Žà®©à¯\nà®ªà¯‹à®©à¯à®±\nà®µà¯‡à®£à¯à®Ÿà¯à®®à¯\nà®µà®¨à¯à®¤à¯\nà®‡à®¤à®©à¯\nà®…à®¤à¯\nà®…à®µà®©à¯\nà®¤à®¾à®©à¯\nà®ªà®²à®°à¯à®®à¯\nà®Žà®©à¯à®©à¯à®®à¯\nà®®à¯‡à®²à¯à®®à¯\nà®ªà®¿à®©à¯à®©à®°à¯\nà®•à¯Šà®£à¯à®Ÿ\nà®‡à®°à¯à®•à¯à®•à¯à®®à¯\nà®¤à®©à®¤à¯\nà®‰à®³à¯à®³à®¤à¯\nà®ªà¯‹à®¤à¯\nà®Žà®©à¯à®±à¯à®®à¯\nà®…à®¤à®©à¯\nà®¤à®©à¯\nà®ªà®¿à®±à®•à¯\nà®…à®µà®°à¯à®•à®³à¯\nà®µà®°à¯ˆ\nà®…à®µà®³à¯\nà®¨à¯€\nà®†à®•à®¿à®¯\nà®‡à®°à¯à®¨à¯à®¤à®¤à¯\nà®‰à®³à¯à®³à®©\nà®µà®¨à¯à®¤\nà®‡à®°à¯à®¨à¯à®¤\nà®®à®¿à®•à®µà¯à®®à¯\nà®‡à®™à¯à®•à¯\nà®®à¯€à®¤à¯\nà®“à®°à¯\nà®‡à®µà¯ˆ\nà®‡à®¨à¯à®¤à®•à¯\nà®ªà®±à¯à®±à®¿\nà®µà®°à¯à®®à¯\nà®µà¯‡à®±à¯\nà®‡à®°à¯\nà®‡à®¤à®¿à®²à¯\nà®ªà¯‹à®²à¯\nà®‡à®ªà¯à®ªà¯‹à®¤à¯\nà®…à®µà®°à®¤à¯\nà®®à®Ÿà¯à®Ÿà¯à®®à¯\nà®‡à®¨à¯à®¤à®ªà¯\nà®Žà®©à¯à®®à¯\nà®®à¯‡à®²à¯\nà®ªà®¿à®©à¯\nà®šà¯‡à®°à¯à®¨à¯à®¤\nà®†à®•à®¿à®¯à¯‹à®°à¯\nà®Žà®©à®•à¯à®•à¯\nà®‡à®©à¯à®©à¯à®®à¯\nà®…à®¨à¯à®¤à®ªà¯\nà®…à®©à¯à®±à¯\nà®’à®°à¯‡\nà®®à®¿à®•\nà®…à®™à¯à®•à¯\nà®ªà®²à¯à®µà¯‡à®±à¯\nà®µà®¿à®Ÿà¯à®Ÿà¯\nà®ªà¯†à®°à¯à®®à¯\nà®…à®¤à¯ˆ\nà®ªà®±à¯à®±à®¿à®¯\nà®‰à®©à¯\nà®…à®¤à®¿à®•\nà®…à®¨à¯à®¤à®•à¯\nà®ªà¯‡à®°à¯\nà®‡à®¤à®©à®¾à®²à¯\nà®…à®µà¯ˆ\nà®…à®¤à¯‡\nà®à®©à¯\nà®®à¯à®±à¯ˆ\nà®¯à®¾à®°à¯\nà®Žà®©à¯à®ªà®¤à¯ˆ\nà®Žà®²à¯à®²à®¾à®®à¯\nà®®à®Ÿà¯à®Ÿà¯à®®à¯‡\nà®‡à®™à¯à®•à¯‡\nà®…à®™à¯à®•à¯‡\nà®‡à®Ÿà®®à¯\nà®‡à®Ÿà®¤à¯à®¤à®¿à®²à¯\nà®…à®¤à®¿à®²à¯\nà®¨à®¾à®®à¯\nà®…à®¤à®±à¯à®•à¯\nà®Žà®©à®µà¯‡\nà®ªà®¿à®±\nà®šà®¿à®±à¯\nà®®à®±à¯à®±\nà®µà®¿à®Ÿ\nà®Žà®¨à¯à®¤\nà®Žà®©à®µà¯à®®à¯\nà®Žà®©à®ªà¯à®ªà®Ÿà¯à®®à¯\nà®Žà®©à®¿à®©à¯à®®à¯\nà®…à®Ÿà¯à®¤à¯à®¤\nà®‡à®¤à®©à¯ˆ\nà®‡à®¤à¯ˆ\nà®•à¯Šà®³à¯à®³\nà®‡à®¨à¯à®¤à®¤à¯\nà®‡à®¤à®±à¯à®•à¯\nà®…à®¤à®©à®¾à®²à¯\nà®¤à®µà®¿à®°\nà®ªà¯‹à®²\nà®µà®°à¯ˆà®¯à®¿à®²à¯\nà®šà®±à¯à®±à¯\nà®Žà®©à®•à¯\n'.split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ta/__init__.py----------------------------------------
spacy.lang.ta.__init__.Tamil(Language)
spacy.lang.ta.__init__.TamilDefaults(BaseDefaults)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ta/lex_attrs.py----------------------------------------
A:spacy.lang.ta.lex_attrs.length->len(num_suffix)
A:spacy.lang.ta.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.ta.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.ta.lex_attrs.like_num(text)
spacy.lang.ta.lex_attrs.suffix_filter(text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/fr/examples.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/fr/lemmatizer.py----------------------------------------
A:spacy.lang.fr.lemmatizer.univ_pos->token.pos_.lower()
A:spacy.lang.fr.lemmatizer.index_table->self.lookups.get_table('lemma_index', {})
A:spacy.lang.fr.lemmatizer.exc_table->self.lookups.get_table('lemma_exc', {})
A:spacy.lang.fr.lemmatizer.rules_table->self.lookups.get_table('lemma_rules', {})
A:spacy.lang.fr.lemmatizer.lookup_table->self.lookups.get_table('lemma_lookup', {})
A:spacy.lang.fr.lemmatizer.index->self.lookups.get_table('lemma_index', {}).get(univ_pos, {})
A:spacy.lang.fr.lemmatizer.exceptions->self.lookups.get_table('lemma_exc', {}).get(univ_pos, {})
A:spacy.lang.fr.lemmatizer.rules->self.lookups.get_table('lemma_rules', {}).get(univ_pos, [])
A:spacy.lang.fr.lemmatizer.string->string.lower().lower()
A:spacy.lang.fr.lemmatizer.forms->list(dict.fromkeys(forms))
spacy.lang.fr.FrenchLemmatizer(Lemmatizer)
spacy.lang.fr.lemmatizer.FrenchLemmatizer(Lemmatizer)
spacy.lang.fr.lemmatizer.FrenchLemmatizer.get_lookups_config(cls,mode:str)->Tuple[List[str], List[str]]
spacy.lang.fr.lemmatizer.FrenchLemmatizer.rule_lemmatize(self,token:Token)->List[str]


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/fr/stop_words.py----------------------------------------
A:spacy.lang.fr.stop_words.STOP_WORDS->set("\na Ã  Ã¢ abord afin ah ai aie ainsi ait allaient allons\nalors anterieur anterieure anterieures antÃ©rieur antÃ©rieure antÃ©rieures\napres aprÃ¨s as assez attendu au\naupres auquel aura auraient aurait auront\naussi autre autrement autres autrui aux auxquelles auxquels avaient\navais avait avant avec avoir avons ayant\n\nbas basee bat\n\nc' câ€™ Ã§a car ce ceci cela celle celle-ci celle-la celle-lÃ  celles celles-ci celles-la celles-lÃ \ncelui celui-ci celui-la celui-lÃ  cent cependant certain certaine certaines certains certes ces\ncet cette ceux ceux-ci ceux-lÃ  chacun chacune chaque chez ci cinq cinquantaine cinquante\ncinquantiÃ¨me cinquiÃ¨me combien comme comment compris concernant\n\nd' dâ€™ da dans de debout dedans dehors deja dejÃ  delÃ  depuis derriere\nderriÃ¨re des desormais desquelles desquels dessous dessus deux deuxiÃ¨me\ndeuxiÃ¨mement devant devers devra different differente differentes differents diffÃ©rent\ndiffÃ©rente diffÃ©rentes diffÃ©rents dire directe directement dit dite dits divers\ndiverse diverses dix dix-huit dix-neuf dix-sept dixiÃ¨me doit doivent donc dont\ndouze douziÃ¨me du duquel durant dÃ¨s dÃ©ja dÃ©jÃ  dÃ©sormais\n\neffet egalement eh elle elle-meme elle-mÃªme elles elles-memes elles-mÃªmes en encore\nenfin entre envers environ es Ã¨s est et etaient Ã©taient etais Ã©tais etait Ã©tait\netant Ã©tant etc etre Ãªtre eu eux eux-mÃªmes exactement exceptÃ© Ã©galement\n\nfais faisaient faisant fait facon faÃ§on feront font\n\ngens\n\nha hem hep hi ho hormis hors hou houp hue hui huit huitiÃ¨me\nhÃ© i il ils importe\n\nj' jâ€™ je jusqu jusque juste\n\nl' lâ€™ la laisser laquelle le lequel les lesquelles lesquels leur leurs longtemps\nlors lorsque lui lui-meme lui-mÃªme lÃ  lÃ¨s\n\nm' mâ€™ ma maint maintenant mais malgre malgrÃ© me meme memes merci mes mien\nmienne miennes miens mille moi moi-meme moi-mÃªme moindres moins\nmon mÃªme mÃªmes\n\nn' nâ€™ na ne neanmoins neuviÃ¨me ni nombreuses nombreux nos notamment\nnotre nous nous-mÃªmes nouveau nul nÃ©anmoins nÃ´tre nÃ´tres\n\no Ã´ on ont onze onziÃ¨me or ou ouias ouste outre\nouvert ouverte ouverts oÃ¹\n\npar parce parfois parle parlent parler parmi partant\npas pendant pense permet personne peu peut peuvent peux plus\nplusieurs plutot plutÃ´t possible possibles pour pourquoi\npourrais pourrait pouvait prealable precisement\npremier premiÃ¨re premiÃ¨rement\npres procedant proche prÃ¨s prÃ©alable prÃ©cisement pu puis puisque\n\nqu' quâ€™ quand quant quant-Ã -soi quarante quatorze quatre quatre-vingt\nquatriÃ¨me quatriÃ¨mement que quel quelconque quelle quelles quelqu'un quelque\nquelques quels qui quiconque quinze quoi quoique\n\nrelative relativement rend rendre restant reste\nrestent retour revoici revoila revoilÃ \n\ns' sâ€™ sa sait sans sauf se seize selon semblable semblaient\nsemble semblent sent sept septiÃ¨me sera seraient serait seront ses seul seule\nseulement seuls seules si sien sienne siennes siens sinon six sixiÃ¨me soi soi-meme soi-mÃªme soit\nsoixante son sont sous souvent specifique specifiques spÃ©cifique spÃ©cifiques stop\nsuffisant suffisante suffit suis suit suivant suivante\nsuivantes suivants suivre sur surtout\n\nt' tâ€™ ta tant te tel telle tellement telles tels tenant tend tenir tente\ntes tien tienne tiennes tiens toi toi-meme toi-mÃªme ton touchant toujours tous\ntout toute toutes treize trente tres trois troisiÃ¨me troisiÃ¨mement trÃ¨s\ntu tÃ©\n\nun une unes uns\n\nva vais vas vers via vingt voici voila voilÃ  vont vos\nvotre votres vous vous-mÃªmes vu vÃ© vÃ´tre vÃ´tres\n\ny\n\n".split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/fr/_tokenizer_exceptions_list.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/fr/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.fr.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)
spacy.lang.fr.tokenizer_exceptions.lower_first_letter(text)
spacy.lang.fr.tokenizer_exceptions.upper_first_letter(text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/fr/__init__.py----------------------------------------
spacy.lang.fr.__init__.French(Language)
spacy.lang.fr.__init__.FrenchDefaults(BaseDefaults)
spacy.lang.fr.__init__.make_lemmatizer(nlp:Language,model:Optional[Model],name:str,mode:str,overwrite:bool,scorer:Optional[Callable])


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/fr/lex_attrs.py----------------------------------------
A:spacy.lang.fr.lex_attrs._num_words->set('\nzero un une deux trois quatre cinq six sept huit neuf dix\nonze douze treize quatorze quinze seize dix-sept dix-huit dix-neuf\nvingt trente quarante cinquante soixante soixante-dix septante quatre-vingt huitante quatre-vingt-dix nonante\ncent mille mil million milliard billion quadrillion quintillion\nsextillion septillion octillion nonillion decillion\n'.split())
A:spacy.lang.fr.lex_attrs._ordinal_words->set('\npremier premiÃ¨re deuxiÃ¨me second seconde troisiÃ¨me quatriÃ¨me cinquiÃ¨me sixiÃ¨me septiÃ¨me huitiÃ¨me neuviÃ¨me dixiÃ¨me\nonziÃ¨me douziÃ¨me treiziÃ¨me quatorziÃ¨me quinziÃ¨me seiziÃ¨me dix-septiÃ¨me dix-huitiÃ¨me dix-neuviÃ¨me\nvingtiÃ¨me trentiÃ¨me quarantiÃ¨me cinquantiÃ¨me soixantiÃ¨me soixante-dixiÃ¨me septantiÃ¨me quatre-vingtiÃ¨me huitantiÃ¨me quatre-vingt-dixiÃ¨me nonantiÃ¨me\ncentiÃ¨me milliÃ¨me millionniÃ¨me milliardiÃ¨me billionniÃ¨me quadrillionniÃ¨me quintillionniÃ¨me\nsextillionniÃ¨me septillionniÃ¨me octillionniÃ¨me nonillionniÃ¨me decillionniÃ¨me\n'.split())
A:spacy.lang.fr.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.fr.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.fr.lex_attrs.like_num(text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/fr/syntax_iterators.py----------------------------------------
A:spacy.lang.fr.syntax_iterators.np_label->doc.vocab.strings.add('NP')
A:spacy.lang.fr.syntax_iterators.adj_label->doc.vocab.strings.add('amod')
A:spacy.lang.fr.syntax_iterators.det_label->doc.vocab.strings.add('det')
A:spacy.lang.fr.syntax_iterators.det_pos->doc.vocab.strings.add('DET')
A:spacy.lang.fr.syntax_iterators.adp_pos->doc.vocab.strings.add('ADP')
A:spacy.lang.fr.syntax_iterators.conj_label->doc.vocab.strings.add('conj')
A:spacy.lang.fr.syntax_iterators.conj_pos->doc.vocab.strings.add('CCONJ')
A:spacy.lang.fr.syntax_iterators.right_childs->list(word.rights)
spacy.lang.fr.syntax_iterators.noun_chunks(doclike:Union[Doc,Span])->Iterator[Tuple[int, int, int]]


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/fr/punctuation.py----------------------------------------
A:spacy.lang.fr.punctuation.ELISION->"' â€™".replace(' ', '')
A:spacy.lang.fr.punctuation.HYPHENS->'- â€“ â€” â€ â€‘'.replace(' ', '')


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ur/examples.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ur/stop_words.py----------------------------------------
A:spacy.lang.ur.stop_words.STOP_WORDS->set('\nØ«Ú¾ÛŒ\nØ®Ùˆ\nÚ¯ÛŒ\nØ§Ù¾ÙŒÛ’\nÚ¯Ø¦Û’\nØ«ÛØª\nØ·Ø±Ù\nÛÙˆØ¨Ø±ÛŒ\nÙ¾Ø¨Ø¦Û’\nØ§Ù¾ÙŒØ¨\nØ¯ÙˆØ¶Ø±ÛŒ\nÚ¯ÛŒØ¨\nÚ©Øª\nÚ¯Ø¨\nØ«Ú¾ÛŒ\nØ¶Û’\nÛØ±\nÙ¾Ø±\nØ§Ø´\nØ¯ÛŒ\nÚ¯Û’\nÙ„Ú¯ÛŒÚº\nÛÛ’\nØ«Ø¹Ø°\nØ¶Ú©ØªÛ’\nØªÚ¾ÛŒ\nØ§Ù‰\nØ¯ÛŒØ¨\nÙ„Ø¦Û’\nÙˆØ§Ù„Û’\nÛŒÛ\nØ«Ø¯Ø¨Ø¦Û’\nØ¶Ú©ØªÛŒ\nØªÚ¾Ø¨\nØ§Ù‹Ø°Ø±\nØ±Ø±ÛŒØ¹Û’\nÙ„Ú¯ÛŒ\nÛÙˆØ¨Ø±Ø§\nÛÙˆÙ‹Û’\nØ«Ø¨ÛØ±\nØ¶Ú©ØªØ¨\nÙ‹ÛÛŒÚº\nØªÙˆ\nØ§ÙˆØ±\nØ±ÛØ¨\nÙ„Ú¯Û’\nÛÙˆØ¶Ú©ØªØ¨\nÛÙˆÚº\nÚ©Ø¨\nÛÙˆØ¨Ø±Û’\nØªÙˆØ¨Ù…\nÚ©ÛŒØ¨\nØ§ÛŒØ·Û’\nØ±ÛÛŒ\nÙ‡Ú¯Ø±\nÛÙˆØ¶Ú©ØªÛŒ\nÛÛŒÚº\nÚ©Ø±ÛŒÚº\nÛÙˆ\nØªÚ©\nÚ©ÛŒ\nØ§ÛŒÚ©\nØ±ÛÛ’\nÙ‡ÛŒÚº\nÛÙˆØ¶Ú©ØªÛ’\nÚ©ÛŒØ·Û’\nÛÙˆÙ‹Ø¨\nØªØª\nÚ©Û\nÛÙˆØ§\nØ¢Ø¦Û’\nØ¶Ø¨Øª\nØªÚ¾Û’\nÚ©ÛŒÙˆÚº\nÛÙˆ\nØªØ¨\nÚ©Û’\nÙ¾Ú¾Ø±\nØ«ØºÛŒØ±\nØ®Ø¨Ø±\nÛÛ’\nØ±Ú©Ú¾\nÚ©ÛŒ\nØ·Ø¨\nÚ©ÙˆØ¦ÛŒ\n  Ø±Ø±ÛŒØ¹Û’\nØ«Ø¨Ø±Û’\nØ®Ø¨\nØ§Ø¶Ø·Ø±Ø°\nØ«Ù„Ú©Û\nØ®Ø¬Ú©Û\nØ±Ú©Ú¾\nØªØ¨\nÚ©ÛŒ\nØ·Ø±Ù\nØ«Ø±Ø§Úº\nØ®Ø¨Ø±\nØ±Ø±ÛŒØ¹Û\nØ§Ø¶Ú©Ø¨\nØ«ÙŒØ°\nØ®Øµ\nÚ©ÛŒ\nÙ„Ø¦Û’\nØªÙˆÛÛŒÚº\nØ¯ÙˆØ¶Ø±Û’\nÚ©Ø±Ø±ÛÛŒ\nØ§Ø¶Ú©ÛŒ\nØ«ÛŒÚ†\nØ®ÙˆÚ©Û\nØ±Ú©Ú¾ØªÛŒ\nÚ©ÛŒÙˆÙ‹Ú©Û\nØ¯ÙˆÙ‹ÙˆÚº\nÚ©Ø±\nØ±ÛÛ’\nØ®Ø¨Ø±\nÛÛŒ\nØ«Ø±Ø¢Úº\nØ§Ø¶Ú©Û’\nÙ¾Ú†Ú¾Ù„Ø§\nØ®ÛŒØ·Ø¨\nØ±Ú©Ú¾ØªÛ’\nÚ©Û’\nØ«Ø¹Ø°\nØªÙˆ\nÛÛŒ\n  Ø¯ÙˆØ±Ù‰\nÚ©Ø±\nÛŒÛØ¨Úº\nØ¢Ø´\nØªÚ¾ÙˆÚ‘Ø§\nÚ†Ú©Û’\nØ²Ú©ÙˆÛŒÛ\nØ¯ÙˆØ¶Ø±ÙˆÚº\nØ¶Ú©Ø¨\nØ§ÙˆÙ‹Ú†Ø¨\nØ«ÙŒØ¨\nÙ¾Ù„\nØªÚ¾ÙˆÚ‘ÛŒ\nÚ†Ù„Ø§\nØ®Ø¨Ù‡ÙˆØ¸\nØ¯ÛŒØªØ¨\nØ¶Ú©ÙŒØ¨\nØ§Ø®Ø¨Ø²Øª\nØ§ÙˆÙ‹Ú†Ø¨Ø¦ÛŒ\nØ«ÙŒØ¨Ø±ÛØ¨\nÙ¾ÙˆÚ†Ú¾Ø¨\nØªÚ¾ÙˆÚ‘Û’\nÚ†Ù„Ùˆ\nØ®ØªÙ†\nØ¯ÛŒØªÛŒ\nØ¶Ú©ÛŒ\nØ§Ú†Ú¾Ø¨\nØ§ÙˆÙ‹Ú†ÛŒ\nØ«ÙŒØ¨Ø±ÛÛŒ\nÙ¾ÙˆÚ†Ú¾ØªØ¨\nØªÛŒÙŠ\nÚ†Ù„ÛŒÚº\nØ¯Ø±\nØ¯ÛŒØªÛ’\nØ¶Ú©Û’\nØ§Ú†Ú¾ÛŒ\nØ§ÙˆÙ‹Ú†Û’\nØ«ÙŒØ¨Ø±ÛÛ’\nÙ¾ÙˆÚ†Ú¾ØªÛŒ\nØ®Ø¨Ù‹Ø¨\nÚ†Ù„Û’\nØ¯Ø±Ø®Ø¨Øª\nØ¯ÛŒØ±\nØ¶Ù„Ø·Ù„Û\nØ§Ú†Ú¾Û’\nØ§Ù¹Ú¾Ø¨Ù‹Ø¨\nØ«ÙŒØ¨Ù‹Ø¨\nÙ¾ÙˆÚ†Ú¾ØªÛ’\nØ®Ø¨Ù‹ØªØ¨\nÚ†Ú¾ÙˆÙ¹Ø¨\nØ¯Ø±Ø®Û\nØ¯ÛŒÚ©Ú¾ÙŒØ¨\nØ¶ÙˆÚ†\nØ§Ø®ØªØªØ¨Ù…\nØ§ÛÙ†\nØ«ÙŒØ°\nÙ¾ÙˆÚ†Ú¾ÙŒØ¨\nØ®Ø¨Ù‹ØªÛŒ\nÚ†Ú¾ÙˆÙ¹ÙˆÚº\nØ¯Ø±Ø®Û’\nØ¯ÛŒÚ©Ú¾Ùˆ\nØ¶ÙˆÚ†Ø¨\nØ§Ø¯Ú¾Ø±\nØ¢Ø¦ÛŒ\nØ«ÙŒØ°Ú©Ø±Ù‹Ø¨\nÙ¾ÙˆÚ†Ú¾Ùˆ\nØ®Ø¨Ù‹ØªÛ’\nÚ†Ú¾ÙˆÙ¹ÛŒ\nØ¯Ø±Ø²Ù‚ÛŒÙ‚Øª\nØ¯ÛŒÚ©Ú¾ÛŒ\nØ¶ÙˆÚ†ØªØ¨\nØ§Ø±Ø¯\nØ¢Ø¦Û’\nØ«ÙŒØ°Ú©Ø±Ùˆ\nÙ¾ÙˆÚ†Ú¾ÙˆÚº\nØ®Ø¨Ù‹ÙŒØ¨\nÚ†Ú¾ÙˆÙ¹Û’\nØ¯Ø±Ø¶Øª\nØ¯ÛŒÚ©Ú¾ÛŒÚº\nØ¶ÙˆÚ†ØªÛŒ\nØ§Ø±Ø¯Ú¯Ø±Ø¯\nØ¢Ø¬\nØ«ÙŒØ°ÛŒ\nÙ¾ÙˆÚ†Ú¾ÛŒÚº\nØ®Ø·Ø·Ø±Ø°\nÚ†Ú¾Û\nØ¯Ø´\nØ¯ÛŒÙŒØ¨\nØ¶ÙˆÚ†ØªÛ’\nØ§Ø±Ú©Ø¨Ù‰\nØ¢Ø®Ø±\nØ«Ú‘Ø§\nÙ¾ÙˆØ±Ø§\nØ®Ú¯Û\nÚ†ÛŒØ³ÛŒÚº\nØ¯ÙØ¹Û\nØ¯Û’\nØ¶ÙˆÚ†ÙŒØ¨\nØ§Ø¶ØªØ¹ÙˆØ¨Ù„\nØ¢Ø®Ø±\nÙ¾ÛÙ„Ø§\nØ®Ú¯ÛÙˆÚº\nØ²Ø¨ØµÙ„\nØ¯Ú©Ú¾Ø¨Ø¦ÛŒÚº\nØ±Ø§Ø¶ØªÙˆÚº\nØ¶ÙˆÚ†Ùˆ\nØ§Ø¶ØªØ¹ÙˆØ¨Ù„Ø§Øª\nØ¢Ø¯Ù‡ÛŒ\nØ«Ú‘ÛŒ\nÙ¾ÛÙ„ÛŒ\nØ®Ú¯ÛÛŒÚº\nØ²Ø¨Ø¶Ø±\nØ¯Ú©Ú¾Ø¨ØªØ¨\nØ±Ø§Ø¶ØªÛ\nØ¶ÙˆÚ†ÛŒ\nØ§ØºÛŒØ¨\nØ¢Ù‹Ø¨\nØ«Ú‘Û’\nÙ¾ÛÙ„Û’Ø¶ÛŒ\nØ®Ù„Ø°ÛŒ\nØ²Ø¨Ù„\nØ¯Ú©Ú¾Ø¨ØªÛŒ\nØ±Ø§Ø¶ØªÛ’\nØ¶ÙˆÚ†ÛŒÚº\nØ§Ø·Ø±Ø§Ù\nØ¢Ù¹Ú¾\nØ«Ú¾Ø±\nØ®ÙŒØ¨Ø©\nØ²Ø¨Ù„\nØ¯Ú©Ú¾Ø¨ØªÛ’\nØ±Ú©ÙŠ\nØ¶ÛŒØ°Ú¾Ø¨\nØ§ÙØ±Ø§Ø¯\nØ¢ÛŒØ¨\nØ«Ú¾Ø±Ø§\nÙ¾ÛÙ„Û’\nØ®ÙˆØ§Ù‰\nØ²Ø¨Ù„Ø§Øª\nØ¯Ú©Ú¾Ø¨Ù‹Ø¨\nØ±Ú©Ú¾Ø¨\nØ¶ÛŒØ°Ú¾ÛŒ\nØ§Ú©Ø«Ø±\nØ«Ø¨\nÛÙˆØ§\nÙ¾ÛŒØ¹\nØ®ÙˆÙ‹ÛÛŒ\nØ²Ø¨Ù„ÛŒÛ\nØ¯Ú©Ú¾Ø¨Ùˆ\nØ±Ú©Ú¾ÛŒ\nØ¶ÛŒØ°Ú¾Û’\nØ§Ú©Ù¹Ú¾Ø¨\nØ«Ú¾Ø±Ù¾ÙˆØ±\nØªØ¨Ø²Ù\nØ®ÛŒØ·Ø¨Ú©Û\nØ²ØµÙˆÚº\nØ±Ú©Ú¾Û’\nØ¶ÛŒÚ©ÙŒÚˆ\nØ§Ú©Ù¹Ú¾ÛŒ\nØ«Ø¨Ø±ÛŒ\nØ«ÛØªØ±\nØªØ±\nÚ†Ø¨Ø±\nØ²ØµÛ\nØ¯Ù„Ú†Ø·Ù¾\nØ²ÛŒØ¨Ø¯Ù\nØºØ¨ÛŒØ°\nØ§Ú©Ù¹Ú¾Û’\nØ«Ø¨Ù„Ø§\nØ«ÛØªØ±ÛŒ\nØªØ±ØªÛŒØª\nÚ†Ø¨ÛØ¨\nØ²ØµÛ’\nØ¯Ù„Ú†Ø·Ù¾ÛŒ\nØ¶Ø¨Øª\nØºØ®Øµ\nØ§Ú©ÛŒÙ„Ø§\nØ«Ø¨Ù„ØªØ±ØªÛŒØª\nØ«ÛØªØ±ÛŒÙŠ\nØªØ±ÛŒÙŠ\nÚ†Ø¨ÛÙŒØ¨\nØ²Ù‚Ø¨Ø¦Ù‚\nØ¯Ù„Ú†Ø·Ù¾ÛŒØ¨Úº\nØ¶Ø¨Ø¯Ù\nØºØ°\nØ§Ú©ÛŒÙ„ÛŒ\nØ«Ø±Ø´\nÙ¾Ø¨Ø´\nØªØ¹Ø°Ø§Ø¯\nÚ†Ø¨ÛÛ’\nØ²Ù‚ÛŒØªÛŒÚº\nÙ‡ÙŒØ¨Ø¶Øª\nØ¶Ø¨Ø±Ø§\nØºØ±ÙˆØ¹\nØ§Ú©ÛŒÙ„Û’\nØ«ØºÛŒØ±\nÙ¾Ø¨Ù‹Ø¨\nÚ†Ú©Ø¨\nØ²Ù‚ÛŒÙ‚Øª\nØ¯Ùˆ\nØ¶Ø¨Ø±Û’\nØºØ±ÙˆØ¹Ø¨Øª\nØ§Ú¯Ø±Ú†Û\nØ«Ù„ÙŒØ°\nÙ¾Ø¨Ù‹Ú†\nØªÙ†\nÚ†Ú©ÛŒ\nØ²Ú©Ù†\nØ¯ÙˆØ±\nØ¶Ø¨Ù„\nØºÛ’\nØ§Ù„Ú¯\nÙ¾Ø±Ø§Ù‹Ø¨\nØªÙŒÛØ¨\nÚ†Ú©ÛŒÚº\nØ¯ÙˆØ¶Ø±Ø§\nØ¶Ø¨Ù„ÙˆÚº\nØµØ¨Ù\nØµØ³ÛŒØ±\nÙ‚Ø¬ÛŒÙ„Û\nÚ©ÙˆÙ‹Ø·Û’\nÙ„Ø§Ø²Ù‡ÛŒ\nÙ‡Ø·Ø¦Ù„Û’\nÙ‹ÛŒØ¨\nØ·Ø±ÛŒÙ‚\nÚ©Ø±ØªÛŒ\nÚ©ÛØªÛ’\nØµÙØ±\nÙ‚Ø·Ù†\nÚ©Ú¾ÙˆÙ„Ø§\nÙ„Ú¯ØªØ¨\nÙ‡Ø·Ø¨Ø¦Ù„\nÙˆØ§Ø±\nØ·Ø±ÛŒÙ‚ÙˆÚº\nÚ©Ø±ØªÛ’\nÚ©ÛÙŒØ¨\nØµÙˆØ±Øª\nÚ©Ø¦ÛŒ\nÚ©Ú¾ÙˆÙ„ÙŒØ¨\nÙ„Ú¯ØªÛŒ\nÙ‡Ø·ØªØ¹ÙˆÙ„\nÙˆØ§Ø±\nØ·Ø±ÛŒÙ‚Û\nÚ©Ø±ØªÛ’\nÛÙˆ\nÚ©ÛÙŒØ¨\nØµÙˆØ±ØªØ³Ø¨Ù„\nÚ©Ø¦Û’\nÚ©Ú¾ÙˆÙ„Ùˆ\nÙ„Ú¯ØªÛ’\nÙ‡Ø»ØªÙˆÙ„\nÙ¹Ú¾ÛŒÚ©\nØ·Ø±ÛŒÙ‚Û’\nÚ©Ø±Ù‹Ø¨\nÚ©ÛÙˆ\nØµÙˆØ±ØªÙˆÚº\nÚ©Ø¨ÙÛŒ\nÙ‡Ø·Ù„Ù‚\nÚˆÚ¾ÙˆÙ‹ÚˆØ§\nØ·ÙˆØ±\nÚ©Ø±Ùˆ\nÚ©ÛÙˆÚº\nØµÙˆØ±ØªÛŒÚº\nÚ©Ø¨Ù…\nÚ©Ú¾ÙˆÙ„ÛŒÚº\nÙ„Ú¯ÛŒ\nÙ‡Ø¹Ù„ÙˆÙ…\nÚˆÚ¾ÙˆÙ‹ÚˆÙ„ÛŒØ¨\nØ·ÙˆØ±Ù¾Ø±\nÚ©Ø±ÛŒÚº\nÚ©ÛÛŒ\nØ¶Ø±ÙˆØ±\nÚ©Ø¬Ú¾ÛŒ\nÚ©Ú¾ÙˆÙ„Û’\nÙ„Ú¯Û’\nÙ‡Ú©ÙˆÙ„\nÚˆÚ¾ÙˆÙ‹ÚˆÙ‹Ø¨\nØ¸Ø¨ÛØ±\nÚ©Ø±Û’\nÚ©ÛÛŒÚº\nØ¶Ø±ÙˆØ±Øª\nÚ©Ø±Ø§\nÚ©ÛØ¨\nÙ„ÙˆØ¬Ø¨\nÙ‡Ù„Ø§\nÚˆÚ¾ÙˆÙ‹ÚˆÙˆ\nØ¹Ø°Ø¯\nÚ©Ù„\nÚ©ÛÛŒÚº\nÚ©Ø±ØªØ¨\nÚ©ÛØªØ¨\nÙ„ÙˆØ¬ÛŒ\nÙ‡ÙˆÚ©ÙŠ\nÚˆÚ¾ÙˆÙ‹ÚˆÛŒ\nØ¹Ø¸ÛŒÙ†\nÚ©Ù†\nÚ©ÛÛ’\nØ¶Ø±ÙˆØ±ÛŒ\nÚ©Ø±ØªØ¨ÛÙˆÚº\nÚ©ÛØªÛŒ\nÙ„ÙˆØ¬Û’\nÙ‡ÙˆÚ©ÙŒØ¨Øª\nÚˆÚ¾ÙˆÙ‹ÚˆÛŒÚº\nØ¹Ù„Ø§Ù‚ÙˆÚº\nÚ©ÙˆØªØ±\nÚ©ÛŒÛ’\nÙ„ÙˆØ³Ø¨Øª\nÙ‡ÙˆÚ©ÙŒÛ\nÛÙ†\nÙ„Û’\nÙ‹Ø¨Ù¾Ø·ÙŒØ°\nÛÙˆØ±ÛÛ’\nØ¹Ù„Ø§Ù‚Û\nÚ©ÙˆØ±Ø§\nÚ©Û’\nØ±Ø±ÛŒØ¹Û’\nÙ„ÙˆØ³Û\nÙ‡Ú‘Ø§\nÛÙˆØ¦ÛŒ\nÙ‡ØªØ¹Ù„Ù‚\nÙ‹Ø¨Ú¯Ø³ÛŒØ±\nÛÙˆÚ¯Ø¦ÛŒ\nØ¹Ù„Ø§Ù‚Û’\nÚ©ÙˆØ±ÙˆÚº\nÚ¯Ø¦ÛŒ\nÙ„Ùˆ\nÙ‡Ú‘Ù‹Ø¨\nÛÙˆØ¦Û’\nÙ‡Ø³ØªØ±Ù…\nÙ‹Ø·Ø¬Øª\nÛÙˆ\nÚ¯Ø¦Û’\nØ¹Ù„Ø§ÙˆÙ\nÚ©ÙˆØ±Ù\nÚ¯Ø±Ø¯\nÙ„ÙˆÚ¯\nÙ‡Ú‘Û’\nÛÙˆØªÛŒ\nÙ‡Ø³ØªØ±Ù‡Û\nÙ‹Ù‚Ø·Û\nÛÙˆÚ¯ÛŒØ¨\nÚ©ÙˆØ±Û’\nÚ¯Ø±ÙˆÙ¾\nÙ„ÙˆÚ¯ÙˆÚº\nÙ‡ÛØ±Ø«Ø¨Ù‰\nÛÙˆØªÛ’\nÙ‡Ø³Ø·ÙˆØ´\nÙ‹Ú©Ø¨Ù„ÙŒØ¨\nÛÙˆÙ‹ÛŒ\nØ¹ÙˆÙˆÙ‡ÛŒ\nÚ©ÙˆØ·ÙŠ\nÚ¯Ø±ÙˆÙ\nÙ„Ú‘Ú©Ù¾ÙŠ\nÙ‡ÛŒØ±Ø§\nÛÙˆÚ†Ú©Ø¨\nÙ‡Ø®ØªÙ„Ù\nÙ‹Ú©ØªÛ\nÛÛŒ\nÙØ±Ø¯\nÚ©ÙˆÙ‰\nÚ¯Ø±ÙˆÛÙˆÚº\nÙ„ÛŒ\nÙ‡ÛŒØ±ÛŒ\nÛÙˆÚ†Ú©ÛŒ\nÙ‡Ø³ÛŒØ°\nÙÛŒ\nÚ©ÙˆÙ‹Ø·Ø¨\nÚ¯ÙŒØªÛŒ\nÙ„ÛŒØ¨\nÙ‡ÛŒØ±Û’\nÛÙˆÚ†Ú©Û’\nÙ‡Ø·Ø¦Ù„Û\nÙ‹ÙˆØ®ÙˆØ§Ù‰\nÛŒÙ‚ÛŒÙŒÛŒ\nÙ‚Ø¬Ù„\nÚ©ÙˆÙ‹Ø·ÛŒ\nÙ„ÛŒÙŒØ¨\nÙ‹Ø¦ÛŒ\nÛÙˆØ±ÛØ¨\nÙ„ÛŒÚº\nÙ‹Ø¦Û’\nÛÙˆØ±ÛÛŒ\nØ«Ø¨Ø¹Ø«\nØ¶Øª\n'.split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ur/__init__.py----------------------------------------
spacy.lang.ur.__init__.Urdu(Language)
spacy.lang.ur.__init__.UrduDefaults(BaseDefaults)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ur/lex_attrs.py----------------------------------------
A:spacy.lang.ur.lex_attrs._num_words->'Ø§ÛŒÚ© Ø¯Ùˆ ØªÛŒÙ† Ú†Ø§Ø± Ù¾Ø§Ù†Ú† Ú†Ú¾ Ø³Ø§Øª Ø¢Ù¹Ú¾ Ù†Ùˆ Ø¯Ø³ Ú¯ÛŒØ§Ø±Û Ø¨Ø§Ø±Û ØªÛŒØ±Û Ú†ÙˆØ¯Û Ù¾Ù†Ø¯Ø±Û Ø³ÙˆÙ„Û Ø³ØªØ±Û\n Ø§Ù¹Ù‡Ø§Ø±Ø§ Ø§Ù†ÛŒØ³ Ø¨ÛŒØ³ Ø§Ú©ÛŒØ³ Ø¨Ø§Ø¦ÛŒØ³ ØªØ¦ÛŒØ³ Ú†ÙˆØ¨ÛŒØ³ Ù¾Ú†ÛŒØ³ Ú†Ú¾Ø¨Ø¨ÛŒØ³\nØ³ØªØ§ÛŒØ³ Ø§Ù¹Ú¾Ø§Ø¦Ø³ Ø§Ù†ØªÙŠØ³ ØªÛŒØ³ Ø§Ú©ØªÛŒØ³ Ø¨ØªÛŒØ³ ØªÛŒÙ†ØªÛŒØ³ Ú†ÙˆÙ†ØªÛŒØ³ Ù¾ÛŒÙ†ØªÛŒØ³\n Ú†Ú¾ØªÛŒØ³ Ø³ÛŒÙ†ØªÛŒØ³ Ø§Ø±ØªÛŒØ³ Ø§Ù†ØªØ§Ù„ÛŒØ³ Ú†Ø§Ù„ÛŒØ³ Ø§Ú©ØªØ§Ù„ÛŒØ³ Ø¨ÛŒØ§Ù„ÛŒØ³ ØªÛŒØªØ§Ù„ÛŒØ³\nÚ†ÙˆØ§Ù„ÛŒØ³ Ù¾ÛŒØªØ§Ù„ÛŒØ³ Ú†Ú¾ÛŒØ§Ù„ÛŒØ³ Ø³ÛŒÙ†ØªØ§Ù„ÛŒØ³ Ø§Ú‘ØªØ§Ù„ÛŒØ³ Ø§Ù†Ú†Ø§Ù„ÛŒØ³ Ù¾Ú†Ø§Ø³ Ø§Ú©Ø§ÙˆÙ† Ø¨Ø§ÙˆÙ†\n ØªØ±ÛŒÙ¾Ù† Ú†ÙˆÙ† Ù¾Ú†Ù¾Ù† Ú†Ú¾Ù¾Ù† Ø³ØªØ§ÙˆÙ† Ø§Ù¹Ú¾Ø§ÙˆÙ† Ø§Ù†Ø³Ù¹Ú¾ Ø³Ø§Ø«Ú¾\nØ§Ú©Ø³Ù¹Ú¾ Ø¨Ø§Ø³Ù¹Ú¾ ØªØ±ÛŒØ³Ù¹Ú¾ Ú†ÙˆØ³Ù¹Ú¾ Ù¾ÛŒØ³Ù¹Ú¾ Ú†Ú¾ÛŒØ§Ø³Ù¹Ú¾ Ø³Ú‘Ø³Ù¹Ú¾ Ø§Ú‘Ø³Ù¹Ú¾\nØ§Ù†Ú¾ØªØ± Ø³ØªØ± Ø§Ú©Ú¾ØªØ± Ø¨Ú¾ØªØªØ± ØªÛŒÚ¾ØªØ± Ú†ÙˆÚ¾ØªØ± ØªÚ†Ú¾ØªØ± Ú†Ú¾ÛŒØªØ± Ø³ØªØªØ±\nØ§Ù¹Ú¾ØªØ± Ø§Ù†ÛŒØ§Ø³ÛŒ Ø§Ø³ÛŒ Ø§Ú©ÛŒØ§Ø³ÛŒ Ø¨ÛŒØ§Ø³ÛŒ ØªÛŒØ±Ø§Ø³ÛŒ Ú†ÙˆØ±Ø§Ø³ÛŒ Ù¾Ú†ÛŒØ§Ø³ÛŒ Ú†Ú¾ÛŒØ§Ø³ÛŒ\n Ø³Ù¹ÛŒØ§Ø³ÛŒ Ø§Ù¹Ú¾ÛŒØ§Ø³ÛŒ Ù†ÙˆØ§Ø³ÛŒ Ù†ÙˆÛ’ Ø§Ú©Ø§Ù†ÙˆÛ’ Ø¨Ø§Ù†ÙˆÛ’ ØªØ±Ø§Ù†ÙˆÛ’\nÚ†ÙˆØ±Ø§Ù†ÙˆÛ’ Ù¾Ú†Ø§Ù†ÙˆÛ’ Ú†Ú¾ÛŒØ§Ù†ÙˆÛ’ Ø³ØªØ§Ù†ÙˆÛ’ Ø§Ù¹Ú¾Ø§Ù†ÙˆÛ’ Ù†Ù†Ø§Ù†ÙˆÛ’ Ø³Ùˆ\n'.split()
A:spacy.lang.ur.lex_attrs._ordinal_words->'Ù¾ÛÙ„Ø§ Ø¯ÙˆØ³Ø±Ø§ ØªÛŒØ³Ø±Ø§ Ú†ÙˆØªÚ¾Ø§ Ù¾Ø§Ù†Ú†ÙˆØ§Úº Ú†Ú¾Ù¹Ø§ Ø³Ø§ØªÙˆØ§Úº Ø¢Ù¹Ú¾ÙˆØ§Úº Ù†ÙˆØ§Úº Ø¯Ø³ÙˆØ§Úº Ú¯ÛŒØ§Ø±ÛÙˆØ§Úº Ø¨Ø§Ø±ÛÙˆØ§Úº ØªÛŒØ±Ú¾ÙˆØ§Úº Ú†ÙˆØ¯Ú¾ÙˆØ§Úº\n Ù¾Ù†Ø¯Ø±Ú¾ÙˆØ§Úº Ø³ÙˆÙ„ÛÙˆØ§Úº Ø³ØªØ±Ú¾ÙˆØ§Úº Ø§Ù¹Ú¾Ø§Ø±ÙˆØ§Úº Ø§Ù†ÛŒØ³ÙˆØ§Úº Ø¨Ø³ÛŒÙˆØ§Úº\n'.split()
A:spacy.lang.ur.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.ur.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.ur.lex_attrs.like_num(text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ur/punctuation.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ml/examples.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ml/stop_words.py----------------------------------------
A:spacy.lang.ml.stop_words.STOP_WORDS->set('\nà´…à´¤àµ\nà´‡à´¤àµ\nà´†à´¯à´¿à´°àµà´¨àµà´¨àµ\nà´†à´•àµà´¨àµà´¨àµ\nà´µà´°àµ†\nà´…à´¨àµà´¨àµ‡à´°à´‚\nà´…à´¨àµà´¨àµ\nà´‡à´¨àµà´¨àµ\nà´†à´£àµ\n'.split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ml/__init__.py----------------------------------------
spacy.lang.ml.__init__.Malayalam(Language)
spacy.lang.ml.__init__.MalayalamDefaults(BaseDefaults)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ml/lex_attrs.py----------------------------------------
A:spacy.lang.ml.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.ml.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.ml.lex_attrs.like_num(text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/tr/examples.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/tr/stop_words.py----------------------------------------
A:spacy.lang.tr.stop_words.STOP_WORDS->set('\nacaba\nacep\nadamakÄ±llÄ±\nadeta\nait\nama\namma\nanca\nancak\narada\nartÄ±k\naslÄ±nda\naynen\nayrÄ±ca\naz\naÃ§Ä±kÃ§a\naÃ§Ä±kÃ§asÄ±\nbana\nbari\nbazen\nbazÄ±\nbazÄ±sÄ±\nbazÄ±sÄ±na\nbazÄ±sÄ±nda\nbazÄ±sÄ±ndan\nbazÄ±sÄ±nÄ±\nbazÄ±sÄ±nÄ±n\nbaÅŸkasÄ±\nbaÅŸkasÄ±na\nbaÅŸkasÄ±nda\nbaÅŸkasÄ±ndan\nbaÅŸkasÄ±nÄ±\nbaÅŸkasÄ±nÄ±n\nbaÅŸka\nbelki\nben\nbende\nbenden\nbeni\nbenim\nberi\nberiki\nberikinin\nberikiyi\nberisi\nbilcÃ¼mle\nbile\nbinaen\nbinaenaleyh\nbiraz\nbirazdan\nbirbiri\nbirbirine\nbirbirini\nbirbirinin\nbirbirinde\nbirbirinden\nbirden\nbirdenbire\nbiri\nbirine\nbirini\nbirinin\nbirinde\nbirinden\nbirice\nbirileri\nbirilerinde\nbirilerinden\nbirilerine\nbirilerini\nbirilerinin\nbirisi\nbirisine\nbirisini\nbirisinin\nbirisinde\nbirisinden\nbirkaÃ§\nbirkaÃ§Ä±\nbirkaÃ§Ä±na\nbirkaÃ§Ä±nÄ±\nbirkaÃ§Ä±nÄ±n\nbirkaÃ§Ä±nda\nbirkaÃ§Ä±ndan\nbirkez\nbirlikte\nbirÃ§ok\nbirÃ§oÄŸu\nbirÃ§oÄŸuna\nbirÃ§oÄŸunda\nbirÃ§oÄŸundan\nbirÃ§oÄŸunu\nbirÃ§oÄŸunun\nbirÅŸey\nbirÅŸeyi\nbitevi\nbiteviye\nbittabi\nbiz\nbizatihi\nbizce\nbizcileyin\nbizden\nbize\nbizi\nbizim\nbizimki\nbizzat\nboÅŸuna\nbu\nbuna\nbunda\nbundan\nbunlar\nbunlarÄ±\nbunlarÄ±n\nbunu\nbunun\nburacÄ±kta\nburada\nburadan\nburasÄ±\nburasÄ±na\nburasÄ±nÄ±\nburasÄ±nÄ±n\nburasÄ±nda\nburasÄ±ndan\nbÃ¶yle\nbÃ¶ylece\nbÃ¶ylecene\nbÃ¶ylelikle\nbÃ¶ylemesine\nbÃ¶ylesine\nbÃ¼sbÃ¼tÃ¼n\nbÃ¼tÃ¼n\ncuk\ncÃ¼mlesi\ncÃ¼mlesine\ncÃ¼mlesini\ncÃ¼mlesinin\ncÃ¼mlesinden\ncÃ¼mlemize\ncÃ¼mlemizi\ncÃ¼mlemizden\nÃ§abuk\nÃ§abukÃ§a\nÃ§eÅŸitli\nÃ§ok\nÃ§oklarÄ±\nÃ§oklarÄ±nca\nÃ§okluk\nÃ§oklukla\nÃ§okÃ§a\nÃ§oÄŸu\nÃ§oÄŸun\nÃ§oÄŸunca\nÃ§oÄŸunda\nÃ§oÄŸundan\nÃ§oÄŸunlukla\nÃ§oÄŸunu\nÃ§oÄŸunun\nÃ§Ã¼nkÃ¼\nda\ndaha\ndahasÄ±\ndahi\ndahil\ndahilen\ndaima\ndair\ndayanarak\nde\ndefa\ndek\ndemin\ndemincek\ndeminden\ndenli\nderakap\nderhal\nderken\ndeÄŸil\ndeÄŸin\ndiye\ndiÄŸer\ndiÄŸeri\ndiÄŸerine\ndiÄŸerini\ndiÄŸerinden\ndolayÄ±\ndolayÄ±sÄ±yla\ndoÄŸru\nedecek\neden\nederek\nedilecek\nediliyor\nedilmesi\nediyor\nelbet\nelbette\nemme\nen\nenikonu\nepey\nepeyce\nepeyi\nesasen\nesnasÄ±nda\netmesi\netraflÄ±\netraflÄ±ca\netti\nettiÄŸi\nettiÄŸini\nevleviyetle\nevvel\nevvela\nevvelce\nevvelden\nevvelemirde\nevveli\neÄŸer\nfakat\nfilanca\nfilancanÄ±n\ngah\ngayet\ngayetle\ngayri\ngayrÄ±\ngelgelelim\ngene\ngerek\ngerÃ§i\ngeÃ§ende\ngeÃ§enlerde\ngibi\ngibilerden\ngibisinden\ngine\ngÃ¶re\ngÄ±rla\nhakeza\nhalbuki\nhalen\nhalihazÄ±rda\nhaliyle\nhandiyse\nhangi\nhangisi\nhangisine\nhangisine\nhangisinde\nhangisinden\nhani\nhariÃ§\nhasebiyle\nhasÄ±lÄ±\nhatta\nhele\nhem\nhenÃ¼z\nhep\nhepsi\nhepsini\nhepsinin\nhepsinde\nhepsinden\nher\nherhangi\nherkes\nherkesi\nherkesin\nherkesten\nhiÃ§\nhiÃ§bir\nhiÃ§biri\nhiÃ§birine\nhiÃ§birini\nhiÃ§birinin\nhiÃ§birinde\nhiÃ§birinden\nhoÅŸ\nhulasaten\niken\nila\nile\nilen\nilgili\nilk\nilla\nillaki\nimdi\nindinde\ninen\ninsermi\nise\nister\nitibaren\nitibariyle\nitibarÄ±yla\niyi\niyice\niyicene\niÃ§in\niÅŸ\niÅŸte\nkadar\nkaffesi\nkah\nkala\nkanÄ±mca\nkarÅŸÄ±n\nkaynak\nkaÃ§Ä±\nkaÃ§Ä±na\nkaÃ§Ä±nda\nkaÃ§Ä±ndan\nkaÃ§Ä±nÄ±\nkaÃ§Ä±nÄ±n\nkelli\nkendi\nkendilerinde\nkendilerinden\nkendilerine\nkendilerini\nkendilerinin\nkendini\nkendisi\nkendisinde\nkendisinden\nkendisine\nkendisini\nkendisinin\nkere\nkez\nkeza\nkezalik\nkeÅŸke\nki\nkim\nkimden\nkime\nkimi\nkiminin\nkimisi\nkimisinde\nkimisinden\nkimisine\nkimisinin\nkimse\nkimsecik\nkimsecikler\nkÃ¼lliyen\nkÄ±saca\nkÄ±sacasÄ±\nlakin\nleh\nlÃ¼tfen\nmaada\nmadem\nmademki\nmamafih\nmebni\nmeÄ‘er\nmeÄŸer\nmeÄŸerki\nmeÄŸerse\nmu\nmÃ¼\nmÄ±\nmi\nnasÄ±l\nnasÄ±lsa\nnazaran\nnaÅŸi\nne\nneden\nnedeniyle\nnedenle\nnedenler\nnedenlerden\nnedense\nnerde\nnerden\nnerdeyse\nnere\nnerede\nnereden\nneredeyse\nneresi\nnereye\nnetekim\nneye\nneyi\nneyse\nnice\nnihayet\nnihayetinde\nnitekim\nniye\nniÃ§in\no\nolan\nolarak\noldu\nolduklarÄ±nÄ±\noldukÃ§a\nolduÄŸu\nolduÄŸunu\nolmak\nolmasÄ±\nolsa\nolsun\nolup\nolur\nolursa\noluyor\nona\nonca\nonculayÄ±n\nonda\nondan\nonlar\nonlara\nonlardan\nonlarÄ±\nonlarÄ±n\nonu\nonun\nora\noracÄ±k\noracÄ±kta\norada\noradan\noranca\noranla\noraya\noysa\noysaki\nÃ¶bÃ¼r\nÃ¶bÃ¼rkÃ¼\nÃ¶bÃ¼rÃ¼\nÃ¶bÃ¼rÃ¼nde\nÃ¶bÃ¼rÃ¼nden\nÃ¶bÃ¼rÃ¼ne\nÃ¶bÃ¼rÃ¼nÃ¼\nÃ¶nce\nÃ¶nceden\nÃ¶nceleri\nÃ¶ncelikle\nÃ¶teki\nÃ¶tekisi\nÃ¶yle\nÃ¶ylece\nÃ¶ylelikle\nÃ¶ylemesine\nÃ¶z\npek\npekala\npeki\npekÃ§e\npeyderpey\nraÄŸmen\nsadece\nsahi\nsahiden\nsana\nsanki\nsen\nsenden\nseni\nsenin\nsiz\nsizden\nsizi\nsizin\nsonra\nsonradan\nsonralarÄ±\nsonunda\nÅŸayet\nÅŸey\nÅŸeyden\nÅŸeyi\nÅŸeyler\nÅŸu\nÅŸuna\nÅŸuncacÄ±k\nÅŸunda\nÅŸundan\nÅŸunlar\nÅŸunlarÄ±\nÅŸunlarÄ±n\nÅŸunu\nÅŸunun\nÅŸura\nÅŸuracÄ±k\nÅŸuracÄ±kta\nÅŸurasÄ±\nÅŸÃ¶yle\nÅŸimdi\ntabii\ntam\ntamam\ntamamen\ntamamÄ±yla\ntarafÄ±ndan\ntek\ntÃ¼m\nÃ¼zere\nvar\nvardÄ±\nvasÄ±tasÄ±yla\nve\nvelev\nvelhasÄ±l\nvelhasÄ±lÄ±kelam\nveya\nveyahut\nya\nyahut\nyakinen\nyakÄ±nda\nyakÄ±ndan\nyakÄ±nlarda\nyalnÄ±z\nyalnÄ±zca\nyani\nyapacak\nyapmak\nyaptÄ±\nyaptÄ±klarÄ±\nyaptÄ±ÄŸÄ±\nyaptÄ±ÄŸÄ±nÄ±\nyapÄ±lan\nyapÄ±lmasÄ±\nyapÄ±yor\nyeniden\nyenilerde\nyerine\nyine\nyok\nyoksa\nyoluyla\nyÃ¼zÃ¼nden\nzarfÄ±nda\nzaten\nzati\nzira\n'.split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/tr/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.tr.tokenizer_exceptions._dash_num->'(([{al}\\d]+/\\d+)|(\\d+/[{al}]))'.format(al=ALPHA)
A:spacy.lang.tr.tokenizer_exceptions._roman_ord->'({rn})\\.'.format(rn=_roman_num)
A:spacy.lang.tr.tokenizer_exceptions._inflections->"'[{al}]+".format(al=ALPHA_LOWER)
A:spacy.lang.tr.tokenizer_exceptions._abbrev_inflected->"[{a}]+\\.'[{al}]+".format(a=ALPHA, al=ALPHA_LOWER)
A:spacy.lang.tr.tokenizer_exceptions._nums->'(({d})|({dn})|({te})|({on})|({n})|({ro})|({rn}))({inf})?'.format(d=_date, dn=_dash_num, te=_time_exp, on=_ord_num, n=_num, ro=_roman_ord, rn=_roman_num, inf=_inflections)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/tr/__init__.py----------------------------------------
spacy.lang.tr.__init__.Turkish(Language)
spacy.lang.tr.__init__.TurkishDefaults(BaseDefaults)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/tr/lex_attrs.py----------------------------------------
A:spacy.lang.tr.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.tr.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
A:spacy.lang.tr.lex_attrs.text_lower->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').lower()
spacy.lang.tr.lex_attrs.like_num(text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/tr/syntax_iterators.py----------------------------------------
A:spacy.lang.tr.syntax_iterators.conj->doc.vocab.strings.add('conj')
A:spacy.lang.tr.syntax_iterators.flat->doc.vocab.strings.add('flat')
A:spacy.lang.tr.syntax_iterators.np_label->doc.vocab.strings.add('NP')
spacy.lang.tr.syntax_iterators.noun_chunks(doclike:Union[Doc,Span])->Iterator[Tuple[int, int, int]]


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/tl/stop_words.py----------------------------------------
A:spacy.lang.tl.stop_words.STOP_WORDS->set('\nakin\naking\nako\nalin\nam\namin\naming\nang\nano\nanumang\napat\nat\natin\nating\nay\nbababa\nbago\nbakit\nbawat\nbilang\ndahil\ndalawa\ndapat\ndin\ndito\ndoon\ngagawin\ngayunman\nginagawa\nginawa\nginawang\ngumawa\ngusto\nhabang\nhanggang\nhindi\nhuwag\niba\nibaba\nibabaw\nibig\nikaw\nilagay\nilalim\nilan\ninyong\nisa\nisang\nitaas\nito\niyo\niyon\niyong\nka\nkahit\nkailangan\nkailanman\nkami\nkanila\nkanilang\nkanino\nkanya\nkanyang\nkapag\nkapwa\nkaramihan\nkatiyakan\nkatulad\nkaya\nkaysa\nko\nkong\nkulang\nkumuha\nkung\nlaban\nlahat\nlamang\nlikod\nlima\nmaaari\nmaaaring\nmaging\nmahusay\nmakita\nmarami\nmarapat\nmasyado\nmay\nmayroon\nmga\nminsan\nmismo\nmula\nmuli\nna\nnabanggit\nnaging\nnagkaroon\nnais\nnakita\nnamin\nnapaka\nnarito\nnasaan\nng\nngayon\nni\nnila\nnilang\nnito\nniya\nniyang\nnoon\no\npa\npaano\npababa\npaggawa\npagitan\npagkakaroon\npagkatapos\npalabas\npamamagitan\npanahon\npangalawa\npara\nparaan\npareho\npataas\npero\npumunta\npumupunta\nsa\nsaan\nsabi\nsabihin\nsarili\nsila\nsino\nsiya\ntatlo\ntayo\ntulad\ntungkol\nuna\nwalang\n'.split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/tl/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.tl.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/tl/__init__.py----------------------------------------
spacy.lang.tl.__init__.Tagalog(Language)
spacy.lang.tl.__init__.TagalogDefaults(BaseDefaults)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/tl/lex_attrs.py----------------------------------------
A:spacy.lang.tl.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.tl.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.tl.lex_attrs.like_num(text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/he/examples.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/he/stop_words.py----------------------------------------
A:spacy.lang.he.stop_words.STOP_WORDS->set('\n×× ×™\n××ª\n××ª×”\n×× ×—× ×•\n××ª×Ÿ\n××ª×\n×”×\n×”×Ÿ\n×”×™×\n×”×•×\n×©×œ×™\n×©×œ×•\n×©×œ×š\n×©×œ×”\n×©×œ× ×•\n×©×œ×›×\n×©×œ×›×Ÿ\n×©×œ×”×\n×©×œ×”×Ÿ\n×œ×™\n×œ×•\n×œ×”\n×œ× ×•\n×œ×›×\n×œ×›×Ÿ\n×œ×”×\n×œ×”×Ÿ\n××•×ª×”\n××•×ª×•\n×–×”\n×–××ª\n××œ×”\n××œ×•\n×ª×—×ª\n×ž×ª×—×ª\n×ž×¢×œ\n×‘×™×Ÿ\n×¢×\n×¢×“\n×¢×œ\n××œ\n×ž×•×œ\n×©×œ\n××¦×œ\n×›×ž×•\n××—×¨\n××•×ª×•\n×‘×œ×™\n×œ×¤× ×™\n××—×¨×™\n×ž××—×•×¨×™\n×¢×œ×™\n×¢×œ×™×•\n×¢×œ×™×”\n×¢×œ×™×š\n×¢×œ×™× ×•\n×¢×œ×™×›×\n×¢×œ×™×›×Ÿ\n×¢×œ×™×”×\n×¢×œ×™×”×Ÿ\n×›×œ\n×›×•×œ×\n×›×•×œ×Ÿ\n×›×š\n×›×›×”\n×›×–×”\n×›×–××ª\n×–×”\n××•×ª×™\n××•×ª×”\n××•×ª×\n××•×ª×š\n××•×ª×•\n××•×ª×Ÿ\n××•×ª× ×•\n×•××ª\n××ª\n××ª×›×\n××ª×›×Ÿ\n××™×ª×™\n××™×ª×•\n××™×ª×š\n××™×ª×”\n××™×ª×\n××™×ª×Ÿ\n××™×ª× ×•\n××™×ª×›×\n××™×ª×›×Ÿ\n×™×”×™×”\n×ª×”×™×”\n×”×™×™×ª×™\n×”×™×ª×”\n×”×™×”\n×œ×”×™×•×ª\n×¢×¦×ž×™\n×¢×¦×ž×•\n×¢×¦×ž×”\n×¢×¦×ž×\n×¢×¦×ž×Ÿ\n×¢×¦×ž× ×•\n×ž×™\n×ž×”\n××™×¤×”\n×”×™×›×Ÿ\n×‘×ž×§×•× ×©×‘×•\n××\n×œ××Ÿ\n×œ×ž×§×•× ×©×‘×•\n×ž×§×•× ×‘×•\n××™×–×”\n×ž×”×™×›×Ÿ\n××™×š\n×›×™×¦×“\n×‘××™×–×• ×ž×™×“×”\n×ž×ª×™\n×‘×©×¢×” ×©\n×›××©×¨\n×›×©\n×œ×ž×¨×•×ª\n×œ×¤× ×™\n××—×¨×™\n×ž××™×–×• ×¡×™×‘×”\n×”×¡×™×‘×” ×©×‘×’×œ×œ×”\n×œ×ž×”\n×ž×“×•×¢\n×œ××™×–×• ×ª×›×œ×™×ª\n×›×™\n×™×©\n××™×Ÿ\n××š\n×ž× ×™×Ÿ\n×ž××™×Ÿ\n×ž××™×¤×”\n×™×›×œ\n×™×›×œ×”\n×™×›×œ×•\n×™×›×•×œ\n×™×›×•×œ×”\n×™×›×•×œ×™×\n×™×›×•×œ×•×ª\n×™×•×›×œ×•\n×™×•×›×œ\n×ž×¡×•×’×œ\n×œ×\n×¨×§\n××•×œ×™\n××™×Ÿ\n×œ××•\n××™\n×›×œ×œ\n×‘×¢×“\n× ×’×“\n××\n×¢×\n××œ\n××œ×”\n××œ×•\n××£\n×¢×œ\n×ž×¢×œ\n×ž×ª×—×ª\n×ž×¦×“\n×‘×©×‘×™×œ\n×œ×‘×™×Ÿ\n×‘××ž×¦×¢\n×‘×ª×•×š\n×“×¨×š\n×ž×‘×¢×“\n×‘××ž×¦×¢×•×ª\n×œ×ž×¢×œ×”\n×œ×ž×˜×”\n×ž×—×•×¥\n×ž×Ÿ\n×œ×¢×‘×¨\n×ž×›××Ÿ\n×›××Ÿ\n×”× ×”\n×”×¨×™\n×¤×”\n×©×\n××š\n×‘×¨×\n×©×•×‘\n××‘×œ\n×ž×‘×œ×™\n×‘×œ×™\n×ž×œ×‘×“\n×¨×§\n×‘×’×œ×œ\n×ž×›×™×•×•×Ÿ\n×¢×“\n××©×¨\n×•××™×œ×•\n×œ×ž×¨×•×ª\n×›×ž×•\n×›×¤×™\n××–\n××—×¨×™\n×›×Ÿ\n×œ×›×Ÿ\n×œ×¤×™×›×š\n×¢×–\n×ž××•×“\n×ž×¢×˜\n×ž×¢×˜×™×\n×‘×ž×™×“×”\n×©×•×‘\n×™×•×ª×¨\n×ž×“×™\n×’×\n×›×Ÿ\n× ×•\n××—×¨\n××—×¨×ª\n××—×¨×™×\n××—×¨×•×ª\n××©×¨\n××•\n'.split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/he/__init__.py----------------------------------------
spacy.lang.he.__init__.Hebrew(Language)
spacy.lang.he.__init__.HebrewDefaults(BaseDefaults)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/he/lex_attrs.py----------------------------------------
A:spacy.lang.he.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.he.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.he.lex_attrs.like_num(text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ko/examples.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ko/stop_words.py----------------------------------------
A:spacy.lang.ko.stop_words.STOP_WORDS->set('\nì´\nìžˆ\ní•˜\nê²ƒ\në“¤\nê·¸\në˜\nìˆ˜\nì´\në³´\nì•Š\nì—†\në‚˜\nì£¼\nì•„ë‹ˆ\në“±\nê°™\në•Œ\në…„\nê°€\ní•œ\nì§€\nì˜¤\në§\nì¼\nê·¸ë ‡\nìœ„í•˜\në•Œë¬¸\nê·¸ê²ƒ\në‘\në§í•˜\nì•Œ\nê·¸ëŸ¬ë‚˜\në°›\nëª»í•˜\nì¼\nê·¸ëŸ°\në˜\në”\në§Ž\nê·¸ë¦¬ê³ \nì¢‹\ní¬\nì‹œí‚¤\nê·¸ëŸ¬\ní•˜ë‚˜\nì‚´\në°\nì•ˆ\nì–´ë–¤\në²ˆ\në‚˜\në‹¤ë¥¸\nì–´ë–»\në“¤\nì´ë ‡\nì \nì‹¶\në§\nì¢€\nì›\nìž˜\në†“\n'.split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ko/tag_map.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ko/__init__.py----------------------------------------
A:spacy.lang.ko.__init__.self._mecab->try_mecab_import()
A:spacy.lang.ko.__init__.self._mecab_tokenizer->self._mecab('-F%f[0],%f[7]')
A:spacy.lang.ko.__init__.dtokens->list(self.detailed_tokens(text))
A:spacy.lang.ko.__init__.doc->Doc(self.vocab, words=surfaces, spaces=list(check_spaces(text, surfaces)))
A:spacy.lang.ko.__init__.(first_tag, sep, eomi_tags)->dtoken['tag'].partition('+')
A:spacy.lang.ko.__init__.(tag, _, expr)->feature.partition(',')
A:spacy.lang.ko.__init__.(lemma, _, remainder)->expr.partition('/')
A:spacy.lang.ko.__init__.config->load_config_from_str(DEFAULT_CONFIG)
A:spacy.lang.ko.__init__.idx->text.find(token, start)
spacy.lang.ko.__init__.Korean(Language)
spacy.lang.ko.__init__.KoreanDefaults(BaseDefaults)
spacy.lang.ko.__init__.KoreanTokenizer(self,vocab:Vocab)
spacy.lang.ko.__init__.KoreanTokenizer.__reduce__(self)
spacy.lang.ko.__init__.KoreanTokenizer.detailed_tokens(self,text:str)->Iterator[Dict[str, Any]]
spacy.lang.ko.__init__.KoreanTokenizer.mecab_tokenizer(self)
spacy.lang.ko.__init__.KoreanTokenizer.score(self,examples)
spacy.lang.ko.__init__.check_spaces(text,tokens)
spacy.lang.ko.__init__.create_tokenizer()
spacy.lang.ko.__init__.try_mecab_import()->None


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ko/lex_attrs.py----------------------------------------
A:spacy.lang.ko.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.ko.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.ko.lex_attrs.like_num(text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ko/punctuation.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/nb/examples.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/nb/stop_words.py----------------------------------------
A:spacy.lang.nb.stop_words.STOP_WORDS->set('\nalle allerede alt and andre annen annet at av\n\nbak bare bedre beste blant ble bli blir blitt bris by bÃ¥de\n\nda dag de del dem den denne der dermed det dette disse du\n\neller en enn er et ett etter\n\nfem fikk fire fjor flere folk for fortsatt fra fram\nfunnet fÃ¥ fÃ¥r fÃ¥tt fÃ¸r fÃ¸rst fÃ¸rste\n\ngang gi gikk gjennom gjorde gjort gjÃ¸r gjÃ¸re god godt grunn gÃ¥ gÃ¥r\n\nha hadde ham han hans har hele helt henne hennes her hun\n\ni ifÃ¸lge igjen ikke ingen inn\n\nja jeg\n\nkamp kampen kan kl klart kom komme kommer kontakt kort kroner kunne kveld\n\nla laget land landet langt leder ligger like litt lÃ¸pet\n\nman mange med meg mellom men mener mennesker mens mer mot mye mÃ¥ mÃ¥l mÃ¥tte\n\nned neste noe noen nok ny nye nÃ¥ nÃ¥r\n\nog ogsÃ¥ om opp opplyser oss over\n\npersoner plass poeng pÃ¥\n\nrunde rundt\n\nsa saken samme sammen samtidig satt se seg seks selv senere ser sett\nsiden sier sin sine siste sitt skal skriver skulle slik som sted stedet stor\nstore stÃ¥r svÃ¦rt sÃ¥\n\nta tatt tid tidligere til tilbake tillegg tok tror\n\nunder ut uten utenfor\n\nvant var ved veldig vi videre viktig vil ville viser vÃ¥r vÃ¦re vÃ¦rt\n\nÃ¥ Ã¥r\n\nÃ¸nsker\n'.split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/nb/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.nb.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/nb/__init__.py----------------------------------------
spacy.lang.nb.__init__.Norwegian(Language)
spacy.lang.nb.__init__.NorwegianDefaults(BaseDefaults)
spacy.lang.nb.__init__.make_lemmatizer(nlp:Language,model:Optional[Model],name:str,mode:str,overwrite:bool,scorer:Optional[Callable])


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/nb/syntax_iterators.py----------------------------------------
A:spacy.lang.nb.syntax_iterators.conj->doc.vocab.strings.add('conj')
A:spacy.lang.nb.syntax_iterators.np_label->doc.vocab.strings.add('NP')
spacy.lang.nb.syntax_iterators.noun_chunks(doclike:Union[Doc,Span])->Iterator[Tuple[int, int, int]]


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/nb/punctuation.py----------------------------------------
A:spacy.lang.nb.punctuation._quotes->char_classes.CONCAT_QUOTES.replace("'", '')


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/hy/examples.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/hy/stop_words.py----------------------------------------
A:spacy.lang.hy.stop_words.STOP_WORDS->set('\nÕ¶Õ¡\nÕ¸Õ²Õ»Õ¨\nÕ¡ÕµÕ½Õ¿Õ¥Õ²\nÕ¥Õ¶Ö„\nÕ¶Õ¡\nÕ§Õ«Ö€\nÕ¸Ö€ÕºÕ¥Õ½\nÕ¸Ö‚Ö€Õ«Õ·\nÕ¢Õ¸Õ¬Õ¸Ö€Õ¨\nÕ¡ÕµÕ¶\nÕ¡ÕµÕ¬\nÕ¶Õ¸Ö‚ÕµÕ¶Õ¹Õ¡Öƒ\nÕ§Õ«\nÕ´Õ«\nÖ‡\nÕ¸Õ²Õ»\nÕ¥Õ½\nÕ¸Õ´Õ¶\nÕ°Õ¥Õ¿\nÕ¶Ö€Õ¡Õ¶Ö„\nÕ¡Õ´Õ¥Õ¶Ö„Õ¨\nÕ¨Õ½Õ¿\nÕ«Õ¶Õ¹-Õ«Õ¶Õ¹\nÕ¡ÕµÕ½ÕºÕ¥Õ½\nÕ°Õ¡Õ´Õ¡ÕµÕ¶\nÕ´Õ«\nÕ¶Õ¡Ö‡\nÕ¶Õ¸Ö‚ÕµÕ¶Ö„Õ¡Õ¶\nÕ¤Õ¡\nÕ¸Õ¾Ö‡Õ§\nÕ°Õ¡Õ´Õ¡Ö€\nÕ¡ÕµÕ¶Õ¿Õ¥Õ²\nÕ§Õ«Õ¶\nÕ¸Ö€Õ¸Õ¶Ö„\nÕ½Õ¸Ö‚ÕµÕ¶\nÕ«Õ¶Õ¹-Õ¸Ö€\nÕ¡Õ´Õ¥Õ¶Õ¨\nÕ¶Õ¸Ö‚ÕµÕ¶ÕºÕ«Õ½Õ«\nÕ¸Ö‚\nÕ«Ö€\nÕ¸Ö€Õ¸Õ·\nÕ´Õ«Ö‡Õ¶Õ¸Ö‚ÕµÕ¶\nÕ«\nÕ¡ÕµÕ¶ÕºÕ«Õ½Õ«\nÕ´Õ¥Õ¶Ö„\nÕ¡Õ´Õ¥Õ¶ Õ¸Ö„\nÕ¶Õ¸Ö‚ÕµÕ¶\nÕ¥Ö€Õ¢Ö‡Õ§\nÕ¡ÕµÕ¶\nÕ¸Ö€Ö‡Õ§\nÕ«Õ¶\nÕ¡ÕµÕ¤ÕºÕ¥Õ½\nÕ¶Ö€Õ¡\nÕ¸Ö€Õ¨\nÕ¾Ö€Õ¡\nÕ¤Õ¸Ö‚\nÕ§Õ«Õ¶Ö„\nÕ¡ÕµÕ¤ÕºÕ«Õ½Õ«\nÕ§Õ«Ö„\nÕµÕ¸Ö‚Ö€Õ¡Ö„Õ¡Õ¶Õ¹ÕµÕ¸Ö‚Ö€Õ¨\nÕ¥Õ´\nÕºÕ«Õ¿Õ«\nÕ¡ÕµÕ¤\nÕ¡Õ´Õ¢Õ¸Õ²Õ»Õ¨\nÕ°Õ¥Õ¿Õ¸\nÕ¥Ö„\nÕ¡Õ´Õ¥Õ¶\nÕ¡ÕµÕ¬\nÕ¯Õ¡Õ´\nÕ¡ÕµÕ½Ö„Õ¡Õ¶\nÕ¸Ö€\nÕ¡ÕµÕ¶ÕºÕ¥Õ½\nÕ¡ÕµÕ½Õ«Õ¶Õ¹\nÕ¢Õ¸Õ¬Õ¸Ö€\nÕ§\nÕ´Õ¥Õ¯Õ¶Õ¸Ö‚Õ´Õ¥Õ¯Õ¨\nÕ¡ÕµÕ¤Õ¹Õ¡Öƒ\nÕ¡ÕµÕ¶Ö„Õ¡Õ¶\nÕ¡Õ´Õ¢Õ¸Õ²Õ»\nÕ¥Ö€Õ¢Ö‡Õ«ÖÕ¥\nÕ¡ÕµÕ¶Õ¹Õ¡Öƒ\nÕ¡Õ´Õ¥Õ¶Õ¡ÕµÕ¶\nÕ´ÕµÕ¸Ö‚Õ½\nÕ¡ÕµÕ¶Õ«Õ¶Õ¹\nÕ«Õ½Õ¯\nÕ¡ÕµÕ¤Õ¿Õ¥Õ²\nÕ¡ÕµÕ½\nÕ½Õ¡\nÕ¥Õ¶\nÕ¡Õ´Õ¥Õ¶ Õ«Õ¶Õ¹\nÕ¸Ö€Ö‡Õ«ÖÕ¥\nÕ¸Ö‚Õ´\nÕ´Õ¥Õ¯Õ¨\nÕ¡ÕµÕ¤\nÕ¤Õ¸Ö‚Ö„\nÕ¡ÕµÕ½Õ¹Õ¡Öƒ\nÕ¡ÕµÕ¤Ö„Õ¡Õ¶\nÕ¡ÕµÕ½ÕºÕ«Õ½Õ«\nÕ§Ö€\nÕµÕ¸Ö‚Ö€Õ¡Ö„Õ¡Õ¶Õ¹ÕµÕ¸Ö‚Ö€\nÕ¡ÕµÕ½\nÕ´Õ¥Õ»\nÕ©\n'.split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/hy/__init__.py----------------------------------------
spacy.lang.hy.__init__.Armenian(Language)
spacy.lang.hy.__init__.ArmenianDefaults(BaseDefaults)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/hy/lex_attrs.py----------------------------------------
A:spacy.lang.hy.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.hy.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.hy.lex_attrs.like_num(text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/sr/examples.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/sr/stop_words.py----------------------------------------
A:spacy.lang.sr.stop_words.STOP_WORDS->set('\nÐ°\nÐ°Ð²Ð°Ñ˜\nÐ°ÐºÐ¾\nÐ°Ð»\nÐ°Ð»Ð¸\nÐ°Ñ€Ñ…\nÐ°Ñƒ\nÐ°Ñ…\nÐ°Ñ…Ð°\nÐ°Ñ˜\nÐ±Ð°Ñ€\nÐ±Ð¸\nÐ±Ð¸Ð»Ð°\nÐ±Ð¸Ð»Ð¸\nÐ±Ð¸Ð»Ð¾\nÐ±Ð¸ÑÐ¼Ð¾\nÐ±Ð¸ÑÑ‚Ðµ\nÐ±Ð¸Ñ…\nÐ±Ð¸Ñ˜Ð°ÑÐ¼Ð¾\nÐ±Ð¸Ñ˜Ð°ÑÑ‚Ðµ\nÐ±Ð¸Ñ˜Ð°Ñ…\nÐ±Ð¸Ñ˜Ð°Ñ…Ñƒ\nÐ±Ð¸Ñ˜Ð°ÑˆÐµ\nÐ±Ð¸Ñ›Ðµ\nÐ±Ð»Ð¸Ð·Ñƒ\nÐ±Ñ€Ð¾Ñ˜\nÐ±Ñ€Ñ€\nÐ±ÑƒÐ´Ðµ\nÐ±ÑƒÐ´Ð¸Ð¼Ð¾\nÐ±ÑƒÐ´Ð¸Ñ‚Ðµ\nÐ±ÑƒÐ´Ñƒ\nÐ±ÑƒÐ´ÑƒÑ›Ð¸\nÐ±ÑƒÐ¼\nÐ±ÑƒÑ›\nÐ²Ð°Ð¼\nÐ²Ð°Ð¼Ð°\nÐ²Ð°Ñ\nÐ²Ð°ÑˆÐ°\nÐ²Ð°ÑˆÐµ\nÐ²Ð°ÑˆÐ¸Ð¼\nÐ²Ð°ÑˆÐ¸Ð¼Ð°\nÐ²Ð°Ñ™Ð´Ð°\nÐ²ÐµÐ¾Ð¼Ð°\nÐ²ÐµÑ€Ð¾Ð²Ð°Ñ‚Ð½Ð¾\nÐ²ÐµÑ›\nÐ²ÐµÑ›Ð¸Ð½Ð°\nÐ²Ð¸\nÐ²Ð¸Ð´ÐµÐ¾\nÐ²Ð¸ÑˆÐµ\nÐ²Ñ€Ð»Ð¾\nÐ²Ñ€Ñ…\nÐ³Ð°\nÐ³Ð´Ðµ\nÐ³Ð¸Ñ†\nÐ³Ð¾Ð´\nÐ³Ð¾Ñ€Ðµ\nÐ³Ñ’ÐµÐºÐ¾Ñ˜Ðµ\nÐ´Ð°\nÐ´Ð°ÐºÐ»Ðµ\nÐ´Ð°Ð½Ð°\nÐ´Ð°Ð½Ð°Ñ\nÐ´Ð°Ñ˜\nÐ´Ð²Ð°\nÐ´Ðµ\nÐ´ÐµÐ´ÐµÑ€\nÐ´ÐµÐ»Ð¸Ð¼Ð¸Ñ†Ðµ\nÐ´ÐµÐ»Ð¸Ð¼Ð¸Ñ‡Ð½Ð¾\nÐ´ÐµÐ¼\nÐ´Ð¾\nÐ´Ð¾Ð±Ð°Ñ€\nÐ´Ð¾Ð±Ð¸Ñ‚Ð¸\nÐ´Ð¾Ð²ÐµÑ‡ÐµÑ€\nÐ´Ð¾ÐºÐ»Ðµ\nÐ´Ð¾Ð»Ðµ\nÐ´Ð¾Ð½ÐµÐºÐ»Ðµ\nÐ´Ð¾ÑÐ°Ð´\nÐ´Ð¾ÑÐºÐ¾Ñ€Ð¾\nÐ´Ð¾Ñ‚Ð°Ð´\nÐ´Ð¾Ñ‚Ð»Ðµ\nÐ´Ð¾ÑˆÐ°Ð¾\nÐ´Ð¾Ñ›Ð¸\nÐ´Ñ€ÑƒÐ³Ð°Ð¼Ð¾\nÐ´Ñ€ÑƒÐ³Ð´Ðµ\nÐ´Ñ€ÑƒÐ³Ð¸\nÐµ\nÐµÐ²Ð¾\nÐµÐ½Ð¾\nÐµÑ‚Ð¾\nÐµÑ…\nÐµÑ…Ðµ\nÐµÑ˜\nÐ¶ÐµÐ»ÐµÐ»Ð°\nÐ¶ÐµÐ»ÐµÐ»Ðµ\nÐ¶ÐµÐ»ÐµÐ»Ð¸\nÐ¶ÐµÐ»ÐµÐ»Ð¾\nÐ¶ÐµÐ»ÐµÑ…\nÐ¶ÐµÐ»ÐµÑ›Ð¸\nÐ¶ÐµÐ»Ð¸\nÐ·Ð°\nÐ·Ð°Ð¸ÑÑ‚Ð°\nÐ·Ð°Ñ€\nÐ·Ð°Ñ‚Ð¸Ð¼\nÐ·Ð°Ñ‚Ð¾\nÐ·Ð°Ñ…Ð²Ð°Ð»Ð¸Ñ‚Ð¸\nÐ·Ð°ÑˆÑ‚Ð¾\nÐ·Ð±Ð¸Ñ™Ð°\nÐ·Ð¸Ð¼ÑƒÑ\nÐ·Ð½Ð°Ñ‚Ð¸\nÐ·ÑƒÐ¼\nÐ¸\nÐ¸Ð´Ðµ\nÐ¸Ð·\nÐ¸Ð·Ð²Ð°Ð½\nÐ¸Ð·Ð²Ð¾Ð»Ð¸\nÐ¸Ð·Ð¼ÐµÑ’Ñƒ\nÐ¸Ð·Ð½Ð°Ð´\nÐ¸ÐºÐ°Ð´Ð°\nÐ¸ÐºÐ°ÐºÐ°Ð²\nÐ¸ÐºÐ°ÐºÐ²Ð°\nÐ¸ÐºÐ°ÐºÐ²Ðµ\nÐ¸ÐºÐ°ÐºÐ²Ð¸\nÐ¸ÐºÐ°ÐºÐ²Ð¸Ð¼\nÐ¸ÐºÐ°ÐºÐ²Ð¸Ð¼Ð°\nÐ¸ÐºÐ°ÐºÐ²Ð¸Ñ…\nÐ¸ÐºÐ°ÐºÐ²Ð¾\nÐ¸ÐºÐ°ÐºÐ²Ð¾Ð³\nÐ¸ÐºÐ°ÐºÐ²Ð¾Ð³Ð°\nÐ¸ÐºÐ°ÐºÐ²Ð¾Ð¼\nÐ¸ÐºÐ°ÐºÐ²Ð¾Ð¼Ðµ\nÐ¸ÐºÐ°ÐºÐ²Ð¾Ñ˜\nÐ¸Ð»Ð¸\nÐ¸Ð¼\nÐ¸Ð¼Ð°\nÐ¸Ð¼Ð°Ð¼\nÐ¸Ð¼Ð°Ð¾\nÐ¸ÑÐ¿Ð¾Ð´\nÐ¸Ñ…\nÐ¸Ñ˜Ñƒ\nÐ¸Ñ›Ð¸\nÐºÐ°Ð´\nÐºÐ°Ð´Ð°\nÐºÐ¾Ð³Ð°\nÐºÐ¾Ñ˜ÐµÐºÐ°ÐºÐ°Ð²\nÐºÐ¾Ñ˜Ð¸Ð¼Ð°\nÐºÐ¾Ñ˜Ñƒ\nÐºÑ€Ð¸ÑˆÐ¾Ð¼\nÐ»Ð°Ð½Ð¸\nÐ»Ð¸\nÐ¼Ð°Ð»Ð¸\nÐ¼Ð°ÑšÐ¸\nÐ¼Ðµ\nÐ¼ÐµÐ½Ðµ\nÐ¼ÐµÐ½Ð¸\nÐ¼Ð¸\nÐ¼Ð¸Ð¼Ð¾\nÐ¼Ð¸ÑÐ»Ð¸\nÐ¼Ð½Ð¾Ð³Ð¾\nÐ¼Ð¾Ð³Ñƒ\nÐ¼Ð¾Ñ€Ð°\nÐ¼Ð¾Ñ€Ð°Ð¾\nÐ¼Ð¾Ñ˜\nÐ¼Ð¾Ñ˜Ð°\nÐ¼Ð¾Ñ˜Ðµ\nÐ¼Ð¾Ñ˜Ð¸\nÐ¼Ð¾Ñ˜Ñƒ\nÐ¼Ð¾Ñ›Ð¸\nÐ¼Ñƒ\nÐ½Ð°\nÐ½Ð°Ð´\nÐ½Ð°ÐºÐ¾Ð½\nÐ½Ð°Ð¼\nÐ½Ð°Ð¼Ð°\nÐ½Ð°Ñ\nÐ½Ð°ÑˆÐ°\nÐ½Ð°ÑˆÐµ\nÐ½Ð°ÑˆÐµÐ³\nÐ½Ð°ÑˆÐ¸\nÐ½Ð°Ñ›Ð¸\nÐ½Ðµ\nÐ½ÐµÐ³Ð´Ðµ\nÐ½ÐµÐºÐ°\nÐ½ÐµÐºÐ°Ð´\nÐ½ÐµÐºÐµ\nÐ½ÐµÐºÐ¾Ð³\nÐ½ÐµÐºÑƒ\nÐ½ÐµÐ¼Ð°\nÐ½ÐµÐ¼Ð°Ð¼\nÐ½ÐµÐºÐ¾\nÐ½ÐµÑ›Ðµ\nÐ½ÐµÑ›ÐµÐ¼Ð¾\nÐ½ÐµÑ›ÐµÑ‚Ðµ\nÐ½ÐµÑ›ÐµÑˆ\nÐ½ÐµÑ›Ñƒ\nÐ½Ð¸\nÐ½Ð¸ÐºÐ°Ð´Ð°\nÐ½Ð¸ÐºÐ¾Ð³Ð°\nÐ½Ð¸ÐºÐ¾Ñ˜Ðµ\nÐ½Ð¸ÐºÐ¾Ñ˜Ð¸\nÐ½Ð¸ÐºÐ¾Ñ˜Ñƒ\nÐ½Ð¸ÑÐ°Ð¼\nÐ½Ð¸ÑÐ¸\nÐ½Ð¸ÑÑ‚Ðµ\nÐ½Ð¸ÑÑƒ\nÐ½Ð¸ÑˆÑ‚Ð°\nÐ½Ð¸Ñ˜ÐµÐ´Ð°Ð½\nÐ½Ð¾\nÐ¾\nÐ¾Ð²Ð°\nÐ¾Ð²Ð°ÐºÐ¾\nÐ¾Ð²Ð°Ð¼Ð¾\nÐ¾Ð²Ð°Ñ˜\nÐ¾Ð²Ð´Ðµ\nÐ¾Ð²Ðµ\nÐ¾Ð²Ð¸Ð¼\nÐ¾Ð²Ð¸Ð¼Ð°\nÐ¾Ð²Ð¾\nÐ¾Ð²Ð¾Ñ˜\nÐ¾Ð´\nÐ¾Ð´Ð¼Ð°Ñ…\nÐ¾ÐºÐ¾\nÐ¾ÐºÐ¾Ð»Ð¾\nÐ¾Ð½\nÐ¾Ð½Ð°Ñ˜\nÐ¾Ð½Ðµ\nÐ¾Ð½Ð¸Ð¼\nÐ¾Ð½Ð¸Ð¼Ð°\nÐ¾Ð½Ð¾Ð¼\nÐ¾Ð½Ð¾Ñ˜\nÐ¾Ð½Ñƒ\nÐ¾ÑÐ¸Ð¼\nÐ¾ÑÑ‚Ð°Ð»Ð¸\nÐ¾Ñ‚Ð¸ÑˆÐ°Ð¾\nÐ¿Ð°\nÐ¿Ð°Ðº\nÐ¿Ð¸Ñ‚Ð°Ñ‚Ð¸\nÐ¿Ð¾\nÐ¿Ð¾Ð²Ð¾Ð´Ð¾Ð¼\nÐ¿Ð¾Ð´\nÐ¿Ð¾Ð´Ð°Ñ™Ðµ\nÐ¿Ð¾Ð¶ÐµÑ™Ð°Ð½\nÐ¿Ð¾Ð¶ÐµÑ™Ð½Ð°\nÐ¿Ð¾Ð¸Ð·Ð´Ð°Ñ™Ðµ\nÐ¿Ð¾Ð¸Ð¼ÐµÐ½Ñ†Ðµ\nÐ¿Ð¾Ð½ÐµÐºÐ°Ð´\nÐ¿Ð¾Ð¿Ñ€ÐµÐºÐ¾\nÐ¿Ð¾Ñ€ÐµÐ´\nÐ¿Ð¾ÑÐ»Ðµ\nÐ¿Ð¾Ñ‚Ð°Ð¼Ð°Ð½\nÐ¿Ð¾Ñ‚Ñ€Ð±ÑƒÑˆÐºÐµ\nÐ¿Ð¾ÑƒÐ·Ð´Ð°Ð½Ð¾\nÐ¿Ð¾Ñ‡ÐµÑ‚Ð°Ðº\nÐ¿Ð¾Ñ˜ÐµÐ´Ð¸Ð½Ð¸\nÐ¿Ñ€Ð°Ð²Ð¸Ñ‚Ð¸\nÐ¿Ñ€Ð²Ð¸\nÐ¿Ñ€ÐµÐºÐ¾\nÐ¿Ñ€ÐµÐ¼Ð°\nÐ¿Ñ€Ð¸Ñ˜Ðµ\nÐ¿ÑƒÑ‚\nÐ¿Ñ™ÑƒÑ\nÑ€Ð°Ð´Ð¸Ñ˜Ðµ\nÑ\nÑÐ°\nÑÐ°Ð²\nÑÐ°Ð´Ð°\nÑÐ°Ð¼\nÑÐ°Ð¼Ð¾\nÑÐ°ÑÐ²Ð¸Ð¼\nÑÐ²Ð°\nÑÐ²Ð°ÐºÐ¸\nÑÐ²Ð¸\nÑÐ²Ð¸Ð¼\nÑÐ²Ð¾Ð³\nÑÐ²Ð¾Ð¼\nÑÐ²Ð¾Ñ˜\nÑÐ²Ð¾Ñ˜Ð°\nÑÐ²Ð¾Ñ˜Ðµ\nÑÐ²Ð¾Ñ˜Ñƒ\nÑÐ²Ñƒ\nÑÐ²ÑƒÐ³Ð´Ðµ\nÑÐµ\nÑÐµÐ±Ðµ\nÑÐµÐ±Ð¸\nÑÐ¸\nÑÐ¼ÐµÑ‚Ð¸\nÑÐ¼Ð¾\nÑÑ‚Ð²Ð°Ñ€\nÑÑ‚Ð²Ð°Ñ€Ð½Ð¾\nÑÑ‚Ðµ\nÑÑƒ\nÑÑƒÑ‚Ñ€Ð°\nÑ‚Ð°\nÑ‚Ð°Ã¨Ð½Ð¾\nÑ‚Ð°ÐºÐ¾\nÑ‚Ð°ÐºÐ¾Ñ’Ðµ\nÑ‚Ð°Ð¼Ð¾\nÑ‚Ð²Ð¾Ñ˜\nÑ‚Ð²Ð¾Ñ˜Ð°\nÑ‚Ð²Ð¾Ñ˜Ðµ\nÑ‚Ð²Ð¾Ñ˜Ð¸\nÑ‚Ð²Ð¾Ñ˜Ñƒ\nÑ‚Ðµ\nÑ‚ÐµÐ±Ðµ\nÑ‚ÐµÐ±Ð¸\nÑ‚Ð¸\nÑ‚Ð¸Ð¼Ð°\nÑ‚Ð¾\nÑ‚Ð¾Ð¼Ðµ\nÑ‚Ð¾Ñ˜\nÑ‚Ñƒ\nÑƒ\nÑƒÐ²ÐµÐº\nÑƒÐ²Ð¸Ñ˜ÐµÐº\nÑƒÐ·\nÑƒÐ·Ð°\nÑƒÐ·Ð°Ð»ÑƒÐ´\nÑƒÐ·Ð´ÑƒÐ¶\nÑƒÐ·ÐµÑ‚Ð¸\nÑƒÐ¼Ð°Ð»Ð¾\nÑƒÐ½ÑƒÑ‚Ñ€Ð°\nÑƒÐ¿Ð¾Ñ‚Ñ€ÐµÐ±Ð¸Ñ‚Ð¸\nÑƒÐ¿Ñ€ÐºÐ¾Ñ\nÑƒÑ‡Ð¸Ð½Ð¸Ð¾\nÑƒÑ‡Ð¸Ð½Ð¸Ñ‚Ð¸\nÑ…Ð°Ð»Ð¾\nÑ…Ð²Ð°Ð»Ð°\nÑ…ÐµÑ˜\nÑ…Ð¼\nÑ…Ð¾Ð¿\nÑ…Ð¾Ñ›Ðµ\nÑ…Ð¾Ñ›ÐµÐ¼Ð¾\nÑ…Ð¾Ñ›ÐµÑ‚Ðµ\nÑ…Ð¾Ñ›ÐµÑˆ\nÑ…Ð¾Ñ›Ñƒ\nÑ…Ñ‚ÐµÐ´Ð¾ÑÑ‚Ðµ\nÑ…Ñ‚ÐµÐ´Ð¾Ñ…\nÑ…Ñ‚ÐµÐ´Ð¾ÑˆÐµ\nÑ…Ñ‚ÐµÐ»Ð°\nÑ…Ñ‚ÐµÐ»Ðµ\nÑ…Ñ‚ÐµÐ»Ð¸\nÑ…Ñ‚ÐµÐ¾\nÑ…Ñ‚ÐµÑ˜Ð°ÑÐ¼Ð¾\nÑ…Ñ‚ÐµÑ˜Ð°ÑÑ‚Ðµ\nÑ…Ñ‚ÐµÑ˜Ð°Ñ…Ñƒ\nÑ…ÑƒÑ€Ð°\nÑ‡ÐµÑÑ‚Ð¾\nÑ‡Ð¸Ñ˜ÐµÐ¼\nÑ‡Ð¸Ñ˜Ð¸\nÑ‡Ð¸Ñ˜Ð¸Ð¼\nÑ‡Ð¸Ñ˜Ð¸Ð¼Ð°\nÑˆÐ¸Ñ†\nÑˆÑ‚Ð°Ð³Ð¾Ð´\nÑˆÑ‚Ð¾\nÑˆÑ‚Ð¾Ð³Ð¾Ð´\nÑ˜Ð°\nÑ˜Ðµ\nÑ˜ÐµÐ´Ð°Ð½\nÑ˜ÐµÐ´Ð¸Ð½Ð¸\nÑ˜ÐµÐ´Ð½Ð°\nÑ˜ÐµÐ´Ð½Ðµ\nÑ˜ÐµÐ´Ð½Ð¸\nÑ˜ÐµÐ´Ð½Ð¾\nÑ˜ÐµÐ´Ð½Ð¾Ð¼\nÑ˜ÐµÑ€\nÑ˜ÐµÑÐ°Ð¼\nÑ˜ÐµÑÐ¸\nÑ˜ÐµÑÐ¼Ð¾\nÑ˜ÐµÑÑƒ\nÑ˜Ð¸Ð¼\nÑ˜Ð¾Ñ˜\nÑ˜Ñƒ\nÑ˜ÑƒÑ‡Ðµ\nÑšÐµÐ³Ð¾Ð²Ð°\nÑšÐµÐ³Ð¾Ð²Ð¾\nÑšÐµÐ·Ð¸Ð½\nÑšÐµÐ·Ð¸Ð½Ð°\nÑšÐµÐ·Ð¸Ð½Ð¾\nÑšÐµÐ¼Ñƒ\nÑšÐµÐ½\nÑšÐ¸Ð¼\nÑšÐ¸Ð¼Ð°\nÑšÐ¸Ñ…Ð¾Ð²Ð°\nÑšÐ¸Ñ…Ð¾Ð²Ð¾\nÑšÐ¾Ñ˜\nÑšÑƒ\nÑ›Ðµ\nÑ›ÐµÐ¼Ð¾\nÑ›ÐµÑ‚Ðµ\nÑ›ÐµÑˆ\nÑ›Ñƒ\n'.split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/sr/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.sr.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/sr/__init__.py----------------------------------------
spacy.lang.sr.__init__.Serbian(Language)
spacy.lang.sr.__init__.SerbianDefaults(BaseDefaults)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/sr/lex_attrs.py----------------------------------------
A:spacy.lang.sr.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.sr.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.sr.lex_attrs.like_num(text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/bn/examples.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/bn/stop_words.py----------------------------------------
A:spacy.lang.bn.stop_words.STOP_WORDS->set('\nà¦…à¦¤à¦à¦¬ à¦…à¦¥à¦š à¦…à¦¥à¦¬à¦¾ à¦…à¦¨à§à¦¯à¦¾à¦¯à¦¼à§€ à¦…à¦¨à§‡à¦• à¦…à¦¨à§‡à¦•à§‡ à¦…à¦¨à§‡à¦•à§‡à¦‡ à¦…à¦¨à§à¦¤à¦¤  à¦…à¦¬à¦§à¦¿ à¦…à¦¬à¦¶à§à¦¯ à¦…à¦°à§à¦¥à¦¾à§Ž à¦…à¦¨à§à¦¯ à¦…à¦¨à§à¦¯à¦¾à§Ÿà§€ à¦…à¦°à§à¦§à¦­à¦¾à¦—à§‡\nà¦†à¦—à¦¾à¦®à§€ à¦†à¦—à§‡ à¦†à¦—à§‡à¦‡ à¦†à¦›à§‡ à¦†à¦œ à¦†à¦¦à§à¦¯à¦­à¦¾à¦—à§‡ à¦†à¦ªà¦¨à¦¾à¦° à¦†à¦ªà¦¨à¦¿ à¦†à¦¬à¦¾à¦° à¦†à¦®à¦°à¦¾ à¦†à¦®à¦¾à¦•à§‡ à¦†à¦®à¦¾à¦¦à§‡à¦° à¦†à¦®à¦¾à¦°  à¦†à¦®à¦¿ à¦†à¦° à¦†à¦°à¦“\nà¦‡à¦¤à§à¦¯à¦¾à¦¦à¦¿ à¦‡à¦¹à¦¾\nà¦‰à¦šà¦¿à¦¤ à¦‰à¦¨à¦¿ à¦‰à¦ªà¦° à¦‰à¦ªà¦°à§‡ à¦‰à¦¤à§à¦¤à¦°\nà¦ à¦à¦à¦¦à§‡à¦° à¦à¦à¦°à¦¾ à¦à¦‡ à¦à¦• à¦à¦•à¦‡ à¦à¦•à¦œà¦¨ à¦à¦•à¦Ÿà¦¾ à¦à¦•à¦Ÿà¦¿  à¦à¦•à¦¬à¦¾à¦° à¦à¦•à§‡ à¦à¦–à¦¨ à¦à¦–à¦¨à¦“ à¦à¦–à¦¾à¦¨à§‡ à¦à¦–à¦¾à¦¨à§‡à¦‡ à¦à¦Ÿà¦¾ à¦à¦¸à§‹\nà¦à¦Ÿà¦¾à¦‡ à¦à¦Ÿà¦¿ à¦à¦¤ à¦à¦¤à¦Ÿà¦¾à¦‡ à¦à¦¤à§‡ à¦à¦¦à§‡à¦° à¦à¦¬à¦‚ à¦à¦¬à¦¾à¦° à¦à¦®à¦¨ à¦à¦®à¦¨à¦¿ à¦à¦®à¦¨à¦•à¦¿ à¦à¦° à¦à¦°à¦¾ à¦à¦²à§‹ à¦à¦¸ à¦à¦¸à§‡\nà¦\nà¦“ à¦“à¦à¦¦à§‡à¦° à¦“à¦à¦° à¦“à¦à¦°à¦¾ à¦“à¦‡ à¦“à¦•à§‡ à¦“à¦–à¦¾à¦¨à§‡ à¦“à¦¦à§‡à¦° à¦“à¦° à¦“à¦°à¦¾\nà¦•à¦–à¦¨à¦“ à¦•à¦¤ à¦•à¦¥à¦¾ à¦•à¦¬à§‡ à¦•à¦¯à¦¼à§‡à¦•  à¦•à¦¯à¦¼à§‡à¦•à¦Ÿà¦¿ à¦•à¦°à¦›à§‡ à¦•à¦°à¦›à§‡à¦¨ à¦•à¦°à¦¤à§‡  à¦•à¦°à¦¬à§‡ à¦•à¦°à¦¬à§‡à¦¨ à¦•à¦°à¦²à§‡ à¦•à§Ÿà§‡à¦•  à¦•à§Ÿà§‡à¦•à¦Ÿà¦¿ à¦•à¦°à¦¿à§Ÿà§‡ à¦•à¦°à¦¿à§Ÿà¦¾ à¦•à¦°à¦¾à§Ÿ\nà¦•à¦°à¦²à§‡à¦¨ à¦•à¦°à¦¾ à¦•à¦°à¦¾à¦‡ à¦•à¦°à¦¾à¦¯à¦¼ à¦•à¦°à¦¾à¦° à¦•à¦°à¦¿ à¦•à¦°à¦¿à¦¤à§‡ à¦•à¦°à¦¿à¦¯à¦¼à¦¾ à¦•à¦°à¦¿à¦¯à¦¼à§‡ à¦•à¦°à§‡ à¦•à¦°à§‡à¦‡ à¦•à¦°à§‡à¦›à¦¿à¦²à§‡à¦¨ à¦•à¦°à§‡à¦›à§‡ à¦•à¦°à§‡à¦›à§‡à¦¨ à¦•à¦°à§‡à¦¨ à¦•à¦¾à¦‰à¦•à§‡\nà¦•à¦¾à¦› à¦•à¦¾à¦›à§‡ à¦•à¦¾à¦œ à¦•à¦¾à¦œà§‡ à¦•à¦¾à¦°à¦“ à¦•à¦¾à¦°à¦£ à¦•à¦¿ à¦•à¦¿à¦‚à¦¬à¦¾ à¦•à¦¿à¦›à§ à¦•à¦¿à¦›à§à¦‡ à¦•à¦¿à¦¨à§à¦¤à§ à¦•à§€ à¦•à§‡ à¦•à§‡à¦‰ à¦•à§‡à¦‰à¦‡ à¦•à§‡à¦¨ à¦•à§‹à¦¨ à¦•à§‹à¦¨à¦“ à¦•à§‹à¦¨à§‹ à¦•à§‡à¦®à¦¨à§‡ à¦•à§‹à¦Ÿà¦¿\nà¦•à§à¦·à§‡à¦¤à§à¦°à§‡ à¦–à§à¦¬\nà¦—à¦¿à¦¯à¦¼à§‡ à¦—à¦¿à¦¯à¦¼à§‡à¦›à§‡ à¦—à§à¦²à¦¿ à¦—à§‡à¦›à§‡ à¦—à§‡à¦² à¦—à§‡à¦²à§‡ à¦—à§‹à¦Ÿà¦¾ à¦—à¦¿à§Ÿà§‡ à¦—à¦¿à§Ÿà§‡à¦›à§‡\nà¦šà¦²à§‡ à¦šà¦¾à¦¨ à¦šà¦¾à¦¯à¦¼ à¦šà§‡à¦¯à¦¼à§‡ à¦šà¦¾à§Ÿ à¦šà§‡à§Ÿà§‡ à¦šà¦¾à¦° à¦šà¦¾à¦²à§ à¦šà§‡à¦·à§à¦Ÿà¦¾\nà¦›à¦¾à¦¡à¦¼à¦¾ à¦›à¦¾à¦¡à¦¼à¦¾à¦“ à¦›à¦¿à¦² à¦›à¦¿à¦²à§‡à¦¨ à¦›à¦¾à§œà¦¾ à¦›à¦¾à§œà¦¾à¦“\nà¦œà¦¨ à¦œà¦¨à¦•à§‡ à¦œà¦¨à§‡à¦° à¦œà¦¨à§à¦¯ à¦œà¦¨à§à¦¯à§‡ à¦œà¦¾à¦¨à¦¤à§‡ à¦œà¦¾à¦¨à¦¾ à¦œà¦¾à¦¨à¦¾à¦¨à§‹ à¦œà¦¾à¦¨à¦¾à¦¯à¦¼  à¦œà¦¾à¦¨à¦¿à¦¯à¦¼à§‡  à¦œà¦¾à¦¨à¦¿à¦¯à¦¼à§‡à¦›à§‡ à¦œà¦¾à¦¨à¦¾à§Ÿ à¦œà¦¾à¦¾à¦¨à¦¿à§Ÿà§‡ à¦œà¦¾à¦¨à¦¿à§Ÿà§‡à¦›à§‡\nà¦Ÿà¦¿\nà¦ à¦¿à¦•\nà¦¤à¦–à¦¨ à¦¤à¦¤ à¦¤à¦¥à¦¾ à¦¤à¦¬à§ à¦¤à¦¬à§‡ à¦¤à¦¾ à¦¤à¦¾à¦à¦•à§‡ à¦¤à¦¾à¦à¦¦à§‡à¦° à¦¤à¦¾à¦à¦° à¦¤à¦¾à¦à¦°à¦¾ à¦¤à¦¾à¦à¦¹à¦¾à¦°à¦¾ à¦¤à¦¾à¦‡ à¦¤à¦¾à¦“ à¦¤à¦¾à¦•à§‡ à¦¤à¦¾à¦¤à§‡ à¦¤à¦¾à¦¦à§‡à¦° à¦¤à¦¾à¦° à¦¤à¦¾à¦°à¦ªà¦° à¦¤à¦¾à¦°à¦¾ à¦¤à¦¾à¦°à¦‡ à¦¤à¦¾à¦¹à¦²à§‡ à¦¤à¦¾à¦¹à¦¾ à¦¤à¦¾à¦¹à¦¾à¦¤à§‡ à¦¤à¦¾à¦¹à¦¾à¦° à¦¤à¦¿à¦¨à¦‡\nà¦¤à¦¿à¦¨à¦¿ à¦¤à¦¿à¦¨à¦¿à¦“ à¦¤à§à¦®à¦¿ à¦¤à§à¦²à§‡ à¦¤à§‡à¦®à¦¨ à¦¤à§‹ à¦¤à§‹à¦®à¦¾à¦° à¦¤à§à¦‡ à¦¤à§‹à¦°à¦¾ à¦¤à§‹à¦° à¦¤à§‹à¦®à¦¾à¦¦à§‡à¦° à¦¤à§‹à¦¦à§‡à¦°\nà¦¥à¦¾à¦•à¦¬à§‡ à¦¥à¦¾à¦•à¦¬à§‡à¦¨ à¦¥à¦¾à¦•à¦¾ à¦¥à¦¾à¦•à¦¾à¦¯à¦¼ à¦¥à¦¾à¦•à§‡ à¦¥à¦¾à¦•à§‡à¦¨ à¦¥à§‡à¦•à§‡ à¦¥à§‡à¦•à§‡à¦‡  à¦¥à§‡à¦•à§‡à¦“ à¦¥à¦¾à¦•à¦¾à§Ÿ\nà¦¦à¦¿à¦•à§‡ à¦¦à¦¿à¦¤à§‡ à¦¦à¦¿à¦¯à¦¼à§‡ à¦¦à¦¿à¦¯à¦¼à§‡à¦›à§‡ à¦¦à¦¿à¦¯à¦¼à§‡à¦›à§‡à¦¨ à¦¦à¦¿à¦²à§‡à¦¨ à¦¦à¦¿à§Ÿà§‡ à¦¦à§  à¦¦à§à¦Ÿà¦¿  à¦¦à§à¦Ÿà§‹ à¦¦à§‡à¦“à¦¯à¦¼à¦¾ à¦¦à§‡à¦“à¦¯à¦¼à¦¾à¦° à¦¦à§‡à¦–à¦¤à§‡ à¦¦à§‡à¦–à¦¾ à¦¦à§‡à¦–à§‡ à¦¦à§‡à¦¨ à¦¦à§‡à¦¯à¦¼  à¦¦à§‡à¦¶à§‡à¦°\nà¦¦à§à¦¬à¦¾à¦°à¦¾ à¦¦à¦¿à§Ÿà§‡à¦›à§‡ à¦¦à¦¿à§Ÿà§‡à¦›à§‡à¦¨ à¦¦à§‡à§Ÿ à¦¦à§‡à¦“à§Ÿà¦¾ à¦¦à§‡à¦“à§Ÿà¦¾à¦° à¦¦à¦¿à¦¨ à¦¦à§à¦‡\nà¦§à¦°à¦¾ à¦§à¦°à§‡\nà¦¨à¦¯à¦¼ à¦¨à¦¾ à¦¨à¦¾à¦‡ à¦¨à¦¾à¦•à¦¿ à¦¨à¦¾à¦—à¦¾à¦¦ à¦¨à¦¾à¦¨à¦¾ à¦¨à¦¿à¦œà§‡ à¦¨à¦¿à¦œà§‡à¦‡ à¦¨à¦¿à¦œà§‡à¦¦à§‡à¦° à¦¨à¦¿à¦œà§‡à¦° à¦¨à¦¿à¦¤à§‡ à¦¨à¦¿à¦¯à¦¼à§‡ à¦¨à¦¿à§Ÿà§‡ à¦¨à§‡à¦‡ à¦¨à§‡à¦“à§Ÿà¦¾ à¦¨à§‡à¦“à¦¯à¦¼à¦¾à¦° à¦¨à§Ÿ à¦¨à¦¤à§à¦¨\nà¦ªà¦•à§à¦·à§‡ à¦ªà¦° à¦ªà¦°à§‡ à¦ªà¦°à§‡à¦‡ à¦ªà¦°à§‡à¦“ à¦ªà¦°à§à¦¯à¦¨à§à¦¤ à¦ªà¦¾à¦“à¦¯à¦¼à¦¾ à¦ªà¦¾à¦°à¦¿ à¦ªà¦¾à¦°à§‡ à¦ªà¦¾à¦°à§‡à¦¨ à¦ªà§‡à¦¯à¦¼à§‡ à¦ªà§à¦°à¦¤à¦¿ à¦ªà§à¦°à¦­à§ƒà¦¤à¦¿ à¦ªà§à¦°à¦¾à¦¯à¦¼ à¦ªà¦¾à¦“à§Ÿà¦¾ à¦ªà§‡à§Ÿà§‡ à¦ªà§à¦°à¦¾à§Ÿ à¦ªà¦¾à¦à¦š à¦ªà§à¦°à¦¥à¦® à¦ªà§à¦°à¦¾à¦¥à¦®à¦¿à¦•\nà¦«à¦²à§‡ à¦«à¦¿à¦°à§‡ à¦«à§‡à¦°\nà¦¬à¦›à¦° à¦¬à¦¦à¦²à§‡ à¦¬à¦°à¦‚ à¦¬à¦²à¦¤à§‡ à¦¬à¦²à¦² à¦¬à¦²à¦²à§‡à¦¨ à¦¬à¦²à¦¾ à¦¬à¦²à§‡ à¦¬à¦²à§‡à¦›à§‡à¦¨ à¦¬à¦²à§‡à¦¨  à¦¬à¦¸à§‡ à¦¬à¦¹à§ à¦¬à¦¾ à¦¬à¦¾à¦¦à§‡ à¦¬à¦¾à¦° à¦¬à¦¿à¦¨à¦¾ à¦¬à¦¿à¦­à¦¿à¦¨à§à¦¨ à¦¬à¦¿à¦¶à§‡à¦· à¦¬à¦¿à¦·à¦¯à¦¼à¦Ÿà¦¿ à¦¬à§‡à¦¶ à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦¬à§à¦¯à¦¾à¦ªà¦¾à¦°à§‡ à¦¬à¦•à§à¦¤à¦¬à§à¦¯ à¦¬à¦¨ à¦¬à§‡à¦¶à¦¿\nà¦­à¦¾à¦¬à§‡  à¦­à¦¾à¦¬à§‡à¦‡\nà¦®à¦¤ à¦®à¦¤à§‹ à¦®à¦¤à§‹à¦‡ à¦®à¦§à§à¦¯à¦­à¦¾à¦—à§‡ à¦®à¦§à§à¦¯à§‡ à¦®à¦§à§à¦¯à§‡à¦‡  à¦®à¦§à§à¦¯à§‡à¦“ à¦®à¦¨à§‡ à¦®à¦¾à¦¤à§à¦° à¦®à¦¾à¦§à§à¦¯à¦®à§‡ à¦®à¦¾à¦¨à§à¦· à¦®à¦¾à¦¨à§à¦·à§‡à¦° à¦®à§‹à¦Ÿ à¦®à§‹à¦Ÿà§‡à¦‡ à¦®à§‹à¦¦à§‡à¦° à¦®à§‹à¦°\nà¦¯à¦–à¦¨ à¦¯à¦¤ à¦¯à¦¤à¦Ÿà¦¾ à¦¯à¦¥à§‡à¦·à§à¦Ÿ à¦¯à¦¦à¦¿ à¦¯à¦¦à¦¿à¦“ à¦¯à¦¾ à¦¯à¦¾à¦à¦° à¦¯à¦¾à¦à¦°à¦¾ à¦¯à¦¾à¦“à¦¯à¦¼à¦¾  à¦¯à¦¾à¦“à¦¯à¦¼à¦¾à¦° à¦¯à¦¾à¦•à§‡ à¦¯à¦¾à¦šà§à¦›à§‡ à¦¯à¦¾à¦¤à§‡ à¦¯à¦¾à¦¦à§‡à¦° à¦¯à¦¾à¦¨ à¦¯à¦¾à¦¬à§‡ à¦¯à¦¾à¦¯à¦¼ à¦¯à¦¾à¦°  à¦¯à¦¾à¦°à¦¾ à¦¯à¦¾à§Ÿ à¦¯à¦¿à¦¨à¦¿ à¦¯à§‡ à¦¯à§‡à¦–à¦¾à¦¨à§‡ à¦¯à§‡à¦¤à§‡ à¦¯à§‡à¦¨\nà¦¯à§‡à¦®à¦¨\nà¦°à¦•à¦® à¦°à¦¯à¦¼à§‡à¦›à§‡ à¦°à¦¾à¦–à¦¾ à¦°à§‡à¦–à§‡ à¦°à§Ÿà§‡à¦›à§‡\nà¦²à¦•à§à¦·\nà¦¶à§à¦§à§ à¦¶à§à¦°à§\nà¦¸à¦¾à¦§à¦¾à¦°à¦£ à¦¸à¦¾à¦®à¦¨à§‡ à¦¸à¦™à§à¦—à§‡ à¦¸à¦™à§à¦—à§‡à¦“ à¦¸à¦¬ à¦¸à¦¬à¦¾à¦° à¦¸à¦®à¦¸à§à¦¤ à¦¸à¦®à§à¦ªà§à¦°à¦¤à¦¿ à¦¸à¦®à§Ÿ à¦¸à¦¹ à¦¸à¦¹à¦¿à¦¤ à¦¸à¦¾à¦¥à§‡ à¦¸à§à¦¤à¦°à¦¾à¦‚ à¦¸à§‡  à¦¸à§‡à¦‡ à¦¸à§‡à¦–à¦¾à¦¨ à¦¸à§‡à¦–à¦¾à¦¨à§‡  à¦¸à§‡à¦Ÿà¦¾ à¦¸à§‡à¦Ÿà¦¾à¦‡ à¦¸à§‡à¦Ÿà¦¾à¦“ à¦¸à§‡à¦Ÿà¦¿ à¦¸à§à¦ªà¦·à§à¦Ÿ à¦¸à§à¦¬à¦¯à¦¼à¦‚\nà¦¹à¦‡à¦¤à§‡ à¦¹à¦‡à¦¬à§‡ à¦¹à¦‡à¦¯à¦¼à¦¾ à¦¹à¦“à¦¯à¦¼à¦¾ à¦¹à¦“à¦¯à¦¼à¦¾à¦¯à¦¼ à¦¹à¦“à¦¯à¦¼à¦¾à¦° à¦¹à¦šà§à¦›à§‡ à¦¹à¦¤ à¦¹à¦¤à§‡ à¦¹à¦¤à§‡à¦‡ à¦¹à¦¨ à¦¹à¦¬à§‡ à¦¹à¦¬à§‡à¦¨ à¦¹à¦¯à¦¼ à¦¹à¦¯à¦¼à¦¤à§‹ à¦¹à¦¯à¦¼à¦¨à¦¿ à¦¹à¦¯à¦¼à§‡ à¦¹à¦¯à¦¼à§‡à¦‡ à¦¹à¦¯à¦¼à§‡à¦›à¦¿à¦² à¦¹à¦¯à¦¼à§‡à¦›à§‡ à¦¹à¦¾à¦œà¦¾à¦°\nà¦¹à§Ÿà§‡à¦›à§‡à¦¨ à¦¹à¦² à¦¹à¦²à§‡ à¦¹à¦²à§‡à¦‡ à¦¹à¦²à§‡à¦“ à¦¹à¦²à§‹ à¦¹à¦¿à¦¸à¦¾à¦¬à§‡ à¦¹à¦¿à¦¸à§‡à¦¬à§‡ à¦¹à§ˆà¦²à§‡ à¦¹à§‹à¦• à¦¹à§Ÿ à¦¹à§Ÿà§‡ à¦¹à§Ÿà§‡à¦›à§‡ à¦¹à§ˆà¦¤à§‡ à¦¹à¦‡à§Ÿà¦¾  à¦¹à§Ÿà§‡à¦›à¦¿à¦² à¦¹à§Ÿà§‡à¦›à§‡à¦¨ à¦¹à§Ÿà¦¨à¦¿ à¦¹à§Ÿà§‡à¦‡ à¦¹à§Ÿà¦¤à§‹ à¦¹à¦“à§Ÿà¦¾ à¦¹à¦“à§Ÿà¦¾à¦° à¦¹à¦“à§Ÿà¦¾à§Ÿ\n'.split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/bn/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.bn.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/bn/__init__.py----------------------------------------
spacy.lang.bn.__init__.Bengali(Language)
spacy.lang.bn.__init__.BengaliDefaults(BaseDefaults)
spacy.lang.bn.__init__.make_lemmatizer(nlp:Language,model:Optional[Model],name:str,mode:str,overwrite:bool,scorer:Optional[Callable])


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/bn/punctuation.py----------------------------------------
A:spacy.lang.bn.punctuation._quotes->char_classes.CONCAT_QUOTES.replace("'", '')


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/hu/examples.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/hu/stop_words.py----------------------------------------
A:spacy.lang.hu.stop_words.STOP_WORDS->set('\na abban ahhoz ahogy ahol aki akik akkor akÃ¡r alatt amely amelyek amelyekben\namelyeket amelyet amelynek ami amikor amit amolyan amÃ­g annak arra arrÃ³l az\nazok azon azonban azt aztÃ¡n azutÃ¡n azzal azÃ©rt\n\nbe belÃ¼l benne bÃ¡r\n\ncikk cikkek cikkeket csak\n\nde\n\ne ebben eddig egy egyes egyetlen egyik egyre egyÃ©b egÃ©sz ehhez ekkor el ellen\nelo eloszÃ¶r elott elso elÃ©g elÅ‘tt emilyen ennek erre ez ezek ezen ezt ezzel\nezÃ©rt\n\nfel felÃ©\n\nha hanem hiszen hogy hogyan hÃ¡t\n\nide igen ill ill. illetve ilyen ilyenkor inkÃ¡bb is ismÃ©t ison itt\n\njobban jÃ³ jÃ³l\n\nkell kellett keressÃ¼nk keresztÃ¼l ki kÃ­vÃ¼l kÃ¶zÃ¶tt kÃ¶zÃ¼l\n\nle legalÃ¡bb legyen lehet lehetett lenne lenni lesz lett\n\nma maga magÃ¡t majd meg mellett mely melyek mert mi miatt mikor milyen minden\nmindenki mindent mindig mint mintha mit mivel miÃ©rt mondta most mÃ¡r mÃ¡s mÃ¡sik\nmÃ©g mÃ­g\n\nnagy nagyobb nagyon ne nekem neki nem nincs nÃ©ha nÃ©hÃ¡ny nÃ©lkÃ¼l\n\no oda ok oket olyan ott\n\npedig persze pÃ©ldÃ¡ul\n\nrÃ¡\n\ns sajÃ¡t sem semmi sok sokat sokkal stb. szemben szerint szinte szÃ¡mÃ¡ra szÃ©t\n\ntalÃ¡n te tehÃ¡t teljes ti tovÃ¡bb tovÃ¡bbÃ¡ tÃ¶bb tÃºl ugyanis\n\nutolsÃ³ utÃ¡n utÃ¡na\n\nvagy vagyis vagyok valaki valami valamint valÃ³ van vannak vele vissza viszont\nvolna volt voltak voltam voltunk\n\nÃ¡ltal Ã¡ltalÃ¡ban Ã¡t\n\nÃ©n Ã©ppen Ã©s\n\nÃ­gy\n\nÃ¶n Ã¶ssze\n\nÃºgy Ãºj Ãºjabb Ãºjra\n\nÅ‘ Å‘ket\n'.split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/hu/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.hu.tokenizer_exceptions._suffixes->'-[{al}]+'.format(al=ALPHA_LOWER)
A:spacy.lang.hu.tokenizer_exceptions._numeric_exp->'({n})(({o})({n}))*[%]?'.format(n=_num, o=_ops)
A:spacy.lang.hu.tokenizer_exceptions._nums->'(({ne})|({t})|({on})|({c}))({s})?'.format(ne=_numeric_exp, t=_time_exp, on=_ord_num_or_date, c=CURRENCY, s=_suffixes)
A:spacy.lang.hu.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/hu/__init__.py----------------------------------------
spacy.lang.hu.__init__.Hungarian(Language)
spacy.lang.hu.__init__.HungarianDefaults(BaseDefaults)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/hu/punctuation.py----------------------------------------
A:spacy.lang.hu.punctuation._concat_icons->char_classes.CONCAT_ICONS.replace('Â°', '')
A:spacy.lang.hu.punctuation._quotes->char_classes.CONCAT_QUOTES.replace("'", '')
A:spacy.lang.hu.punctuation._units->char_classes.UNITS.replace('%', '')


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/yo/examples.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/yo/stop_words.py----------------------------------------
A:spacy.lang.yo.stop_words.STOP_WORDS->set('a an b bÃ¡ bÃ­ báº¹Ì€ráº¹Ì€ d e f fÃºn fáº¹Ì g gbogbo i inÃº j jÃ¹ jáº¹ jáº¹Ì k kan kÃ¬ kÃ­ kÃ² l lÃ¡ti lÃ¨ lá» m mi mo mÃ¡a má»Ì€ n ni nÃ¡Ã  nÃ­ nÃ­gbÃ  nÃ­torÃ­ nÇ¹kan o p padÃ  pÃ© pÃºpá»Ì€ páº¹Ì€lÃº r ráº¹Ì€ s sÃ¬ sÃ­ sÃ­nÃº t ti tÃ­ u w wÃ  wÃ¡ wá»n wá»Ìn y yÃ¬Ã­ Ã  Ã ti Ã wá»n Ã¡ Ã¨ Ã© Ã¬ Ã­ Ã² Ã²un Ã³ Ã¹ Ãº Å„ Å„lÃ¡ Ç¹ Ì€ Ì Ì£ á¹£ á¹£e á¹£Ã© á¹£Ã¹gbá»Ìn áº¹ áº¹má»Ì á» á»já»Ì á»Ì€pá»Ì€lá»pá»Ì€'.split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/yo/__init__.py----------------------------------------
spacy.lang.yo.__init__.Yoruba(Language)
spacy.lang.yo.__init__.YorubaDefaults(BaseDefaults)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/yo/lex_attrs.py----------------------------------------
A:spacy.lang.yo.lex_attrs.text->strip_accents_text(text)
spacy.lang.yo.lex_attrs.like_num(text)
spacy.lang.yo.lex_attrs.strip_accents_text(text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/sk/examples.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/sk/stop_words.py----------------------------------------
A:spacy.lang.sk.stop_words.STOP_WORDS->set('\na\naby\naj\nak\nakej\nakejÅ¾e\nako\nakom\nakomÅ¾e\nakou\nakouÅ¾e\nakoÅ¾e\nakÃ¡\nakÃ¡Å¾e\nakÃ©\nakÃ©ho\nakÃ©hoÅ¾e\nakÃ©mu\nakÃ©muÅ¾e\nakÃ©Å¾e\nakÃº\nakÃºÅ¾e\nakÃ½\nakÃ½ch\nakÃ½chÅ¾e\nakÃ½m\nakÃ½mi\nakÃ½miÅ¾e\nakÃ½mÅ¾e\nakÃ½Å¾e\nale\nalebo\nani\nasi\navÅ¡ak\naÅ¾\nba\nbez\nbezo\nbol\nbola\nboli\nbolo\nbude\nbudem\nbudeme\nbudete\nbudeÅ¡\nbudÃº\nbuÄ\nby\nbyÅ¥\ncez\ncezo\ndnes\ndo\neÅ¡te\nho\nhoci\ni\niba\nich\nim\ninej\ninom\ninÃ¡\ninÃ©\ninÃ©ho\ninÃ©mu\ninÃ­\ninÃº\ninÃ½\ninÃ½ch\ninÃ½m\ninÃ½mi\nja\nje\njeho\njej\njemu\nju\nk\nkam\nkamÅ¾e\nkaÅ¾dou\nkaÅ¾dÃ¡\nkaÅ¾dÃ©\nkaÅ¾dÃ©ho\nkaÅ¾dÃ©mu\nkaÅ¾dÃ­\nkaÅ¾dÃº\nkaÅ¾dÃ½\nkaÅ¾dÃ½ch\nkaÅ¾dÃ½m\nkaÅ¾dÃ½mi\nkde\nkej\nkejÅ¾e\nkeÄ\nkeÄÅ¾e\nkie\nkieho\nkiehoÅ¾e\nkiemu\nkiemuÅ¾e\nkieÅ¾e\nkoho\nkom\nkomu\nkou\nkouÅ¾e\nkto\nktorej\nktorou\nktorÃ¡\nktorÃ©\nktorÃ­\nktorÃº\nktorÃ½\nktorÃ½ch\nktorÃ½m\nktorÃ½mi\nku\nkÃ¡\nkÃ¡Å¾e\nkÃ©\nkÃ©Å¾e\nkÃº\nkÃºÅ¾e\nkÃ½\nkÃ½ho\nkÃ½hoÅ¾e\nkÃ½m\nkÃ½mu\nkÃ½muÅ¾e\nkÃ½Å¾e\nlebo\nleda\nledaÅ¾e\nlen\nma\nmajÃº\nmal\nmala\nmali\nmaÅ¥\nmedzi\nmi\nmne\nmnou\nmoja\nmoje\nmojej\nmojich\nmojim\nmojimi\nmojou\nmoju\nmoÅ¾no\nmu\nmusia\nmusieÅ¥\nmusÃ­\nmusÃ­m\nmusÃ­me\nmusÃ­te\nmusÃ­Å¡\nmy\nmÃ¡\nmÃ¡m\nmÃ¡me\nmÃ¡te\nmÃ¡Å¡\nmÃ´cÅ¥\nmÃ´j\nmÃ´jho\nmÃ´Å¾e\nmÃ´Å¾em\nmÃ´Å¾eme\nmÃ´Å¾ete\nmÃ´Å¾eÅ¡\nmÃ´Å¾u\nmÅˆa\nna\nnad\nnado\nnajmÃ¤\nnami\nnaÅ¡a\nnaÅ¡e\nnaÅ¡ej\nnaÅ¡i\nnaÅ¡ich\nnaÅ¡im\nnaÅ¡imi\nnaÅ¡ou\nne\nnech\nneho\nnej\nnejakej\nnejakom\nnejakou\nnejakÃ¡\nnejakÃ©\nnejakÃ©ho\nnejakÃ©mu\nnejakÃº\nnejakÃ½\nnejakÃ½ch\nnejakÃ½m\nnejakÃ½mi\nnemu\nneÅ¾\nnich\nnie\nniektorej\nniektorom\nniektorou\nniektorÃ¡\nniektorÃ©\nniektorÃ©ho\nniektorÃ©mu\nniektorÃº\nniektorÃ½\nniektorÃ½ch\nniektorÃ½m\nniektorÃ½mi\nnielen\nnieÄo\nnim\nnimi\nniÄ\nniÄoho\nniÄom\nniÄomu\nniÄÃ­m\nno\nnÃ¡m\nnÃ¡s\nnÃ¡Å¡\nnÃ¡Å¡ho\nnÃ­m\no\nod\nodo\non\nona\noni\nono\nony\noÅˆ\noÅˆho\npo\npod\npodo\npodÄ¾a\npokiaÄ¾\npopod\npopri\npotom\npoza\npre\npred\npredo\npreto\npretoÅ¾e\npreÄo\npri\nprÃ¡ve\ns\nsa\nseba\nsebe\nsebou\nsem\nsi\nsme\nso\nsom\nste\nsvoj\nsvoja\nsvoje\nsvojho\nsvojich\nsvojim\nsvojimi\nsvojou\nsvoju\nsvojÃ­m\nsÃº\nta\ntak\ntakej\ntakejto\ntakÃ¡\ntakÃ¡to\ntakÃ©\ntakÃ©ho\ntakÃ©hoto\ntakÃ©mu\ntakÃ©muto\ntakÃ©to\ntakÃ­\ntakÃº\ntakÃºto\ntakÃ½\ntakÃ½to\ntakÅ¾e\ntam\nteba\ntebe\ntebou\nteda\ntej\ntejto\nten\ntento\nti\ntie\ntieto\ntieÅ¾\nto\ntoho\ntohoto\ntohto\ntom\ntomto\ntomu\ntomuto\ntoto\ntou\ntouto\ntu\ntvoj\ntvoja\ntvoje\ntvojej\ntvojho\ntvoji\ntvojich\ntvojim\ntvojimi\ntvojÃ­m\nty\ntÃ¡\ntÃ¡to\ntÃ­\ntÃ­to\ntÃº\ntÃºto\ntÃ½ch\ntÃ½m\ntÃ½mi\ntÃ½mto\nu\nuÅ¾\nv\nvami\nvaÅ¡a\nvaÅ¡e\nvaÅ¡ej\nvaÅ¡i\nvaÅ¡ich\nvaÅ¡im\nvaÅ¡Ã­m\nveÄ\nviac\nvo\nvy\nvÃ¡m\nvÃ¡s\nvÃ¡Å¡\nvÃ¡Å¡ho\nvÅ¡ak\nvÅ¡etci\nvÅ¡etka\nvÅ¡etko\nvÅ¡etky\nvÅ¡etok\nz\nza\nzaÄo\nzaÄoÅ¾e\nzo\nÃ¡no\nÄej\nÄi\nÄia\nÄie\nÄieho\nÄiemu\nÄiu\nÄo\nÄoho\nÄom\nÄomu\nÄou\nÄoÅ¾e\nÄÃ­\nÄÃ­m\nÄÃ­mi\nÄalÅ¡ia\nÄalÅ¡ie\nÄalÅ¡ieho\nÄalÅ¡iemu\nÄalÅ¡iu\nÄalÅ¡om\nÄalÅ¡ou\nÄalÅ¡Ã­\nÄalÅ¡Ã­ch\nÄalÅ¡Ã­m\nÄalÅ¡Ã­mi\nÅˆom\nÅˆou\nÅˆu\nÅ¾e\n'.split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/sk/__init__.py----------------------------------------
spacy.lang.sk.__init__.Slovak(Language)
spacy.lang.sk.__init__.SlovakDefaults(BaseDefaults)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/sk/lex_attrs.py----------------------------------------
A:spacy.lang.sk.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.sk.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.sk.lex_attrs.like_num(text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/el/examples.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/el/lemmatizer.py----------------------------------------
A:spacy.lang.el.lemmatizer.univ_pos->token.pos_.lower()
A:spacy.lang.el.lemmatizer.index_table->self.lookups.get_table('lemma_index', {})
A:spacy.lang.el.lemmatizer.exc_table->self.lookups.get_table('lemma_exc', {})
A:spacy.lang.el.lemmatizer.rules_table->self.lookups.get_table('lemma_rules', {})
A:spacy.lang.el.lemmatizer.index->self.lookups.get_table('lemma_index', {}).get(univ_pos, {})
A:spacy.lang.el.lemmatizer.exceptions->self.lookups.get_table('lemma_exc', {}).get(univ_pos, {})
A:spacy.lang.el.lemmatizer.rules->self.lookups.get_table('lemma_rules', {}).get(univ_pos, {})
A:spacy.lang.el.lemmatizer.string->string.lower().lower()
A:spacy.lang.el.lemmatizer.forms->list(dict.fromkeys(forms))
spacy.lang.el.GreekLemmatizer(Lemmatizer)
spacy.lang.el.lemmatizer.GreekLemmatizer(Lemmatizer)
spacy.lang.el.lemmatizer.GreekLemmatizer.rule_lemmatize(self,token:Token)->List[str]


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/el/stop_words.py----------------------------------------
A:spacy.lang.el.stop_words.STOP_WORDS->set('\nÎ±Î´Î¹Î¬ÎºÎ¿Ï€Î± Î±Î¹ Î±ÎºÏŒÎ¼Î± Î±ÎºÏŒÎ¼Î· Î±ÎºÏÎ¹Î²ÏŽÏ‚ Î¬Î»Î»Î± Î±Î»Î»Î¬ Î±Î»Î»Î±Ï‡Î¿Ï Î¬Î»Î»ÎµÏ‚ Î¬Î»Î»Î· Î¬Î»Î»Î·Î½\nÎ¬Î»Î»Î·Ï‚ Î±Î»Î»Î¹ÏŽÏ‚ Î±Î»Î»Î¹ÏŽÏ„Î¹ÎºÎ± Î¬Î»Î»Î¿ Î¬Î»Î»Î¿Î¹ Î±Î»Î»Î¿Î¹ÏŽÏ‚ Î±Î»Î»Î¿Î¹ÏŽÏ„Î¹ÎºÎ± Î¬Î»Î»Î¿Î½ Î¬Î»Î»Î¿Ï‚ Î¬Î»Î»Î¿Ï„Îµ Î±Î»Î»Î¿Ï\nÎ¬Î»Î»Î¿Ï…Ï‚ Î¬Î»Î»Ï‰Î½ Î¬Î¼Î± Î¬Î¼ÎµÏƒÎ± Î±Î¼Î­ÏƒÏ‰Ï‚ Î±Î½ Î±Î½Î¬ Î±Î½Î¬Î¼ÎµÏƒÎ± Î±Î½Î±Î¼ÎµÏ„Î±Î¾Ï Î¬Î½ÎµÏ… Î±Î½Ï„Î¯ Î±Î½Ï„Î¯Ï€ÎµÏÎ± Î±Î½Ï„Î¯Ï‚\nÎ¬Î½Ï‰ Î±Î½Ï‰Ï„Î­ÏÏ‰ Î¬Î¾Î±Ï†Î½Î± Î±Ï€ Î±Ï€Î­Î½Î±Î½Ï„Î¹ Î±Ï€ÏŒ Î±Ï€ÏŒÏˆÎµ Î¬ÏÎ± Î¬ÏÎ±Î³Îµ Î±ÏÎºÎµÏ„Î¬ Î±ÏÎºÎµÏ„Î­Ï‚\nÎ±ÏÏ‡Î¹ÎºÎ¬ Î±Ï‚ Î±ÏÏÎ¹Î¿ Î±Ï…Ï„Î¬ Î±Ï…Ï„Î­Ï‚ Î±Ï…Ï„Î® Î±Ï…Ï„Î®Î½ Î±Ï…Ï„Î®Ï‚ Î±Ï…Ï„ÏŒ Î±Ï…Ï„Î¿Î¯ Î±Ï…Ï„ÏŒÎ½ Î±Ï…Ï„ÏŒÏ‚ Î±Ï…Ï„Î¿Ï Î±Ï…Ï„Î¿ÏÏ‚\nÎ±Ï…Ï„ÏŽÎ½ Î±Ï†ÏŒÏ„Î¿Ï… Î±Ï†Î¿Ï\n\nÎ²Î­Î²Î±Î¹Î± Î²ÎµÎ²Î±Î¹ÏŒÏ„Î±Ï„Î±\n\nÎ³Î¹ Î³Î¹Î± Î³Î¹Î±Ï„Î¯ Î³ÏÎ®Î³Î¿ÏÎ± Î³ÏÏÏ‰\n\nÎ´Î± Î´Îµ Î´ÎµÎ¯Î½Î± Î´ÎµÎ½ Î´ÎµÎ¾Î¹Î¬ Î´Î®Î¸ÎµÎ½ Î´Î·Î»Î±Î´Î® Î´Î¹ Î´Î¹Î± Î´Î¹Î±ÏÎºÏŽÏ‚ Î´Î¹ÎºÎ¬ Î´Î¹ÎºÏŒ Î´Î¹ÎºÎ¿Î¯ Î´Î¹ÎºÏŒÏ‚ Î´Î¹ÎºÎ¿Ï\nÎ´Î¹ÎºÎ¿ÏÏ‚ Î´Î¹ÏŒÎ»Î¿Ï… Î´Î¯Ï€Î»Î± Î´Î¯Ï‡Ï‰Ï‚\n\nÎµÎ¬Î½ ÎµÎ±Ï…Ï„ÏŒ ÎµÎ±Ï…Ï„ÏŒÎ½ ÎµÎ±Ï…Ï„Î¿Ï ÎµÎ±Ï…Ï„Î¿ÏÏ‚ ÎµÎ±Ï…Ï„ÏŽÎ½ Î­Î³ÎºÎ±Î¹ÏÎ± ÎµÎ³ÎºÎ±Î¯ÏÏ‰Ï‚ ÎµÎ³ÏŽ ÎµÎ´ÏŽ ÎµÎ¹Î´ÎµÎ¼Î® ÎµÎ¯Î¸Îµ ÎµÎ¯Î¼Î±Î¹\nÎµÎ¯Î¼Î±ÏƒÏ„Îµ ÎµÎ¯Î½Î±Î¹ ÎµÎ¹Ï‚ ÎµÎ¯ÏƒÎ±Î¹ ÎµÎ¯ÏƒÎ±ÏƒÏ„Îµ ÎµÎ¯ÏƒÏ„Îµ ÎµÎ¯Ï„Îµ ÎµÎ¯Ï‡Î± ÎµÎ¯Ï‡Î±Î¼Îµ ÎµÎ¯Ï‡Î±Î½ ÎµÎ¯Ï‡Î±Ï„Îµ ÎµÎ¯Ï‡Îµ ÎµÎ¯Ï‡ÎµÏ‚ Î­ÎºÎ±ÏƒÏ„Î±\nÎ­ÎºÎ±ÏƒÏ„ÎµÏ‚ Î­ÎºÎ±ÏƒÏ„Î· Î­ÎºÎ±ÏƒÏ„Î·Î½ Î­ÎºÎ±ÏƒÏ„Î·Ï‚ Î­ÎºÎ±ÏƒÏ„Î¿ Î­ÎºÎ±ÏƒÏ„Î¿Î¹ Î­ÎºÎ±ÏƒÏ„Î¿Î½ Î­ÎºÎ±ÏƒÏ„Î¿Ï‚ ÎµÎºÎ¬ÏƒÏ„Î¿Ï… ÎµÎºÎ¬ÏƒÏ„Î¿Ï…Ï‚ ÎµÎºÎ¬ÏƒÏ„Ï‰Î½\nÎµÎºÎµÎ¯ ÎµÎºÎµÎ¯Î½Î± ÎµÎºÎµÎ¯Î½ÎµÏ‚ ÎµÎºÎµÎ¯Î½Î· ÎµÎºÎµÎ¯Î½Î·Î½ ÎµÎºÎµÎ¯Î½Î·Ï‚ ÎµÎºÎµÎ¯Î½Î¿ ÎµÎºÎµÎ¯Î½Î¿Î¹ ÎµÎºÎµÎ¯Î½Î¿Î½ ÎµÎºÎµÎ¯Î½Î¿Ï‚ ÎµÎºÎµÎ¯Î½Î¿Ï…\nÎµÎºÎµÎ¯Î½Î¿Ï…Ï‚ ÎµÎºÎµÎ¯Î½Ï‰Î½ ÎµÎºÏ„ÏŒÏ‚ ÎµÎ¼Î¬Ï‚ ÎµÎ¼ÎµÎ¯Ï‚ ÎµÎ¼Î­Î½Î± ÎµÎ¼Ï€ÏÏŒÏ‚ ÎµÎ½ Î­Î½Î± Î­Î½Î±Î½ Î­Î½Î±Ï‚ ÎµÎ½ÏŒÏ‚ ÎµÎ½Ï„ÎµÎ»ÏŽÏ‚ ÎµÎ½Ï„ÏŒÏ‚\nÎµÎ½Î±Î½Ï„Î¯Î¿Î½  ÎµÎ¾Î®Ï‚  ÎµÎ¾Î±Î¹Ï„Î¯Î±Ï‚  ÎµÏ€Î¹Ï€Î»Î­Î¿Î½ ÎµÏ€ÏŒÎ¼ÎµÎ½Î· ÎµÎ½Ï„Ï‰Î¼ÎµÏ„Î±Î¾Ï ÎµÎ½ÏŽ ÎµÎ¾ Î­Î¾Î±Ï†Î½Î± ÎµÎ¾Î®Ïƒ ÎµÎ¾Î¯ÏƒÎ¿Ï… Î­Î¾Ï‰ ÎµÏ€Î¬Î½Ï‰\nÎµÏ€ÎµÎ¹Î´Î® Î­Ï€ÎµÎ¹Ï„Î± ÎµÏ€Î¯ ÎµÏ€Î¯ÏƒÎ·Ï‚ ÎµÏ€Î¿Î¼Î­Î½Ï‰Ï‚ ÎµÏƒÎ¬Ï‚ ÎµÏƒÎµÎ¯Ï‚ ÎµÏƒÎ­Î½Î± Î­ÏƒÏ„Ï‰ ÎµÏƒÏ ÎµÏ„Î­ÏÎ± ÎµÏ„Î­ÏÎ±Î¹ ÎµÏ„Î­ÏÎ±Ï‚ Î­Ï„ÎµÏÎµÏ‚\nÎ­Ï„ÎµÏÎ· Î­Ï„ÎµÏÎ·Ï‚ Î­Ï„ÎµÏÎ¿ Î­Ï„ÎµÏÎ¿Î¹ Î­Ï„ÎµÏÎ¿Î½ Î­Ï„ÎµÏÎ¿Ï‚ ÎµÏ„Î­ÏÎ¿Ï… Î­Ï„ÎµÏÎ¿Ï…Ï‚ ÎµÏ„Î­ÏÏ‰Î½ ÎµÏ„Î¿ÏÏ„Î± ÎµÏ„Î¿ÏÏ„ÎµÏ‚ ÎµÏ„Î¿ÏÏ„Î· ÎµÏ„Î¿ÏÏ„Î·Î½\nÎµÏ„Î¿ÏÏ„Î·Ï‚ ÎµÏ„Î¿ÏÏ„Î¿ ÎµÏ„Î¿ÏÏ„Î¿Î¹ ÎµÏ„Î¿ÏÏ„Î¿Î½ ÎµÏ„Î¿ÏÏ„Î¿Ï‚ ÎµÏ„Î¿ÏÏ„Î¿Ï… ÎµÏ„Î¿ÏÏ„Î¿Ï…Ï‚ ÎµÏ„Î¿ÏÏ„Ï‰Î½ Î­Ï„ÏƒÎ¹ ÎµÏÎ³Îµ ÎµÏ…Î¸ÏÏ‚ ÎµÏ…Ï„Ï…Ï‡ÏŽÏ‚ ÎµÏ†ÎµÎ¾Î®Ï‚\nÎ­Ï‡ÎµÎ¹ Î­Ï‡ÎµÎ¹Ï‚ Î­Ï‡ÎµÏ„Îµ Î­Ï‡Î¿Î¼Îµ Î­Ï‡Î¿Ï…Î¼Îµ Î­Ï‡Î¿Ï…Î½ ÎµÏ‡Ï„Î­Ï‚ Î­Ï‡Ï‰ Î­Ï‰Ï‚ Î­Î³Î¹Î½Î±Î½  Î­Î³Î¹Î½Îµ  Î­ÎºÎ±Î½Îµ  Î­Î¾Î¹  Î­Ï‡Î¿Î½Ï„Î±Ï‚\n\nÎ· Î®Î´Î· Î®Î¼Î±ÏƒÏ„Î±Î½ Î®Î¼Î±ÏƒÏ„Îµ Î®Î¼Î¿Ï…Î½ Î®ÏƒÎ±ÏƒÏ„Î±Î½ Î®ÏƒÎ±ÏƒÏ„Îµ Î®ÏƒÎ¿Ï…Î½ Î®Ï„Î±Î½ Î®Ï„Î±Î½Îµ Î®Ï„Î¿Î¹ Î®Ï„Ï„Î¿Î½\n\nÎ¸Î±\n\nÎ¹ Î¹Î´Î¯Î± Î¯Î´Î¹Î± Î¯Î´Î¹Î±Î½ Î¹Î´Î¯Î±Ï‚ Î¯Î´Î¹ÎµÏ‚ Î¯Î´Î¹Î¿ Î¯Î´Î¹Î¿Î¹ Î¯Î´Î¹Î¿Î½ Î¯Î´Î¹Î¿Ïƒ Î¯Î´Î¹Î¿Ï‚ Î¹Î´Î¯Î¿Ï… Î¯Î´Î¹Î¿Ï…Ï‚ Î¯Î´Î¹Ï‰Î½ Î¹Î´Î¯Ï‰Ï‚ Î¹Î¹ Î¹Î¹Î¹\nÎ¯ÏƒÎ±Î¼Îµ Î¯ÏƒÎ¹Î± Î¯ÏƒÏ‰Ï‚\n\nÎºÎ¬Î¸Îµ ÎºÎ±Î¸ÎµÎ¼Î¯Î± ÎºÎ±Î¸ÎµÎ¼Î¯Î±Ï‚ ÎºÎ±Î¸Î­Î½Î± ÎºÎ±Î¸Î­Î½Î±Ï‚ ÎºÎ±Î¸ÎµÎ½ÏŒÏ‚ ÎºÎ±Î¸ÎµÏ„Î¯ ÎºÎ±Î¸ÏŒÎ»Î¿Ï… ÎºÎ±Î¸ÏŽÏ‚ ÎºÎ±Î¹ ÎºÎ±ÎºÎ¬ ÎºÎ±ÎºÏŽÏ‚ ÎºÎ±Î»Î¬\nÎºÎ±Î»ÏŽÏ‚ ÎºÎ±Î¼Î¯Î± ÎºÎ±Î¼Î¯Î±Î½ ÎºÎ±Î¼Î¯Î±Ï‚ ÎºÎ¬Î¼Ï€Î¿ÏƒÎ± ÎºÎ¬Î¼Ï€Î¿ÏƒÎµÏ‚ ÎºÎ¬Î¼Ï€Î¿ÏƒÎ· ÎºÎ¬Î¼Ï€Î¿ÏƒÎ·Î½ ÎºÎ¬Î¼Ï€Î¿ÏƒÎ·Ï‚ ÎºÎ¬Î¼Ï€Î¿ÏƒÎ¿ ÎºÎ¬Î¼Ï€Î¿ÏƒÎ¿Î¹\nÎºÎ¬Î¼Ï€Î¿ÏƒÎ¿Î½ ÎºÎ¬Î¼Ï€Î¿ÏƒÎ¿Ï‚ ÎºÎ¬Î¼Ï€Î¿ÏƒÎ¿Ï… ÎºÎ¬Î¼Ï€Î¿ÏƒÎ¿Ï…Ï‚ ÎºÎ¬Î¼Ï€Î¿ÏƒÏ‰Î½ ÎºÎ±Î½ÎµÎ¯Ï‚ ÎºÎ¬Î½ÎµÎ½ ÎºÎ±Î½Î­Î½Î± ÎºÎ±Î½Î­Î½Î±Î½ ÎºÎ±Î½Î­Î½Î±Ï‚\nÎºÎ±Î½ÎµÎ½ÏŒÏ‚ ÎºÎ¬Ï€Î¿Î¹Î± ÎºÎ¬Ï€Î¿Î¹Î±Î½ ÎºÎ¬Ï€Î¿Î¹Î±Ï‚ ÎºÎ¬Ï€Î¿Î¹ÎµÏ‚ ÎºÎ¬Ï€Î¿Î¹Î¿ ÎºÎ¬Ï€Î¿Î¹Î¿Î¹ ÎºÎ¬Ï€Î¿Î¹Î¿Î½ ÎºÎ¬Ï€Î¿Î¹Î¿Ï‚ ÎºÎ¬Ï€Î¿Î¹Î¿Ï… ÎºÎ¬Ï€Î¿Î¹Î¿Ï…Ï‚\nÎºÎ¬Ï€Î¿Î¹Ï‰Î½ ÎºÎ¬Ï€Î¿Ï„Îµ ÎºÎ¬Ï€Î¿Ï… ÎºÎ¬Ï€Ï‰Ï‚ ÎºÎ±Ï„ ÎºÎ±Ï„Î¬ ÎºÎ¬Ï„Î¹ ÎºÎ±Ï„Î¹Ï„Î¯ ÎºÎ±Ï„ÏŒÏ€Î¹Î½ ÎºÎ¬Ï„Ï‰ ÎºÎ¹ÏŒÎ»Î±Ï‚ ÎºÎ»Ï€ ÎºÎ¿Î½Ï„Î¬ ÎºÏ„Î» ÎºÏ…ÏÎ¯Ï‰Ï‚\n\nÎ»Î¹Î³Î¬ÎºÎ¹ Î»Î¯Î³Î¿ Î»Î¹Î³ÏŒÏ„ÎµÏÎ¿ Î»ÏŒÎ³Ï‰ Î»Î¿Î¹Ï€Î¬ Î»Î¿Î¹Ï€ÏŒÎ½\n\nÎ¼Î± Î¼Î±Î¶Î¯ Î¼Î±ÎºÎ¬ÏÎ¹ Î¼Î±ÎºÏÏ…Î¬ Î¼Î¬Î»Î¹ÏƒÏ„Î± Î¼Î¬Î»Î»Î¿Î½ Î¼Î±Ï‚ Î¼Îµ Î¼ÎµÎ¸Î±ÏÏÎ¹Î¿ Î¼ÎµÎ¯Î¿Î½ Î¼Î­Î»ÎµÎ¹ Î¼Î­Î»Î»ÎµÏ„Î±Î¹ Î¼ÎµÎ¼Î¹Î¬Ï‚ Î¼ÎµÎ½\nÎ¼ÎµÏÎ¹ÎºÎ¬ Î¼ÎµÏÎ¹ÎºÎ­Ï‚ Î¼ÎµÏÎ¹ÎºÎ¿Î¯ Î¼ÎµÏÎ¹ÎºÎ¿ÏÏ‚ Î¼ÎµÏÎ¹ÎºÏŽÎ½ Î¼Î­ÏƒÎ± Î¼ÎµÏ„ Î¼ÎµÏ„Î¬ Î¼ÎµÏ„Î±Î¾Ï Î¼Î­Ï‡ÏÎ¹ Î¼Î· Î¼Î®Î´Îµ Î¼Î·Î½ Î¼Î®Ï€Ï‰Ï‚\nÎ¼Î®Ï„Îµ Î¼Î¹Î± Î¼Î¹Î±Î½ Î¼Î¹Î±Ï‚ Î¼ÏŒÎ»Î¹Ï‚ Î¼Î¿Î»Î¿Î½ÏŒÏ„Î¹ Î¼Î¿Î½Î¬Ï‡Î± Î¼ÏŒÎ½ÎµÏ‚ Î¼ÏŒÎ½Î· Î¼ÏŒÎ½Î·Î½ Î¼ÏŒÎ½Î·Ï‚ Î¼ÏŒÎ½Î¿ Î¼ÏŒÎ½Î¿Î¹ Î¼Î¿Î½Î¿Î¼Î¹Î¬Ï‚\nÎ¼ÏŒÎ½Î¿Ï‚ Î¼ÏŒÎ½Î¿Ï… Î¼ÏŒÎ½Î¿Ï…Ï‚ Î¼ÏŒÎ½Ï‰Î½ Î¼Î¿Ï… Î¼Ï€Î¿ÏÎµÎ¯ Î¼Ï€Î¿ÏÎ¿ÏÎ½ Î¼Ï€ÏÎ¿Ï‚ Î¼Î­ÏƒÏ‰  Î¼Î¯Î±  Î¼ÎµÏƒÏŽ\n\nÎ½Î± Î½Î±Î¹ Î½Ï‰ÏÎ¯Ï‚\n\nÎ¾Î±Î½Î¬ Î¾Î±Ï†Î½Î¹ÎºÎ¬\n\nÎ¿ Î¿Î¹ ÏŒÎ»Î± ÏŒÎ»ÎµÏ‚ ÏŒÎ»Î· ÏŒÎ»Î·Î½ ÏŒÎ»Î·Ï‚ ÏŒÎ»Î¿ Î¿Î»ÏŒÎ³Ï…ÏÎ± ÏŒÎ»Î¿Î¹ ÏŒÎ»Î¿Î½ Î¿Î»Î¿Î½Î­Î½ ÏŒÎ»Î¿Ï‚ Î¿Î»ÏŒÏ„ÎµÎ»Î± ÏŒÎ»Î¿Ï… ÏŒÎ»Î¿Ï…Ï‚ ÏŒÎ»Ï‰Î½\nÏŒÎ»Ï‰Ï‚ Î¿Î»Ï‰ÏƒÎ´Î¹ÏŒÎ»Î¿Ï… ÏŒÎ¼Ï‰Ï‚ ÏŒÏ€Î¿Î¹Î± Î¿Ï€Î¿Î¹Î±Î´Î®Ï€Î¿Ï„Îµ Î¿Ï€Î¿Î¯Î±Î½ Î¿Ï€Î¿Î¹Î±Î½Î´Î®Ï€Î¿Ï„Îµ Î¿Ï€Î¿Î¯Î±Ï‚ Î¿Ï€Î¿Î¯Î¿Ï‚ Î¿Ï€Î¿Î¹Î±ÏƒÎ´Î®Ï€Î¿Ï„Îµ Î¿Ï€Î¿Î¹Î´Î®Ï€Î¿Ï„Îµ\nÏŒÏ€Î¿Î¹ÎµÏ‚ Î¿Ï€Î¿Î¹ÎµÏƒÎ´Î®Ï€Î¿Ï„Îµ ÏŒÏ€Î¿Î¹Î¿ Î¿Ï€Î¿Î¹Î¿Î´Î·Î®Ï€Î¿Ï„Îµ ÏŒÏ€Î¿Î¹Î¿Î¹ ÏŒÏ€Î¿Î¹Î¿Î½ Î¿Ï€Î¿Î¹Î¿Î½Î´Î®Ï€Î¿Ï„Îµ ÏŒÏ€Î¿Î¹Î¿Ï‚ Î¿Ï€Î¿Î¹Î¿ÏƒÎ´Î®Ï€Î¿Ï„Îµ\nÎ¿Ï€Î¿Î¯Î¿Ï… Î¿Ï€Î¿Î¹Î¿Ï…Î´Î®Ï€Î¿Ï„Îµ Î¿Ï€Î¿Î¯Î¿Ï…Ï‚ Î¿Ï€Î¿Î¹Î¿Ï…ÏƒÎ´Î®Ï€Î¿Ï„Îµ Î¿Ï€Î¿Î¯Ï‰Î½ Î¿Ï€Î¿Î¹Ï‰Î½Î´Î®Ï€Î¿Ï„Îµ ÏŒÏ€Î¿Ï„Îµ Î¿Ï€Î¿Ï„ÎµÎ´Î®Ï€Î¿Ï„Îµ ÏŒÏ€Î¿Ï…\nÎ¿Ï€Î¿Ï…Î´Î®Ï€Î¿Ï„Îµ ÏŒÏ€Ï‰Ï‚ Î¿ÏÎ¹ÏƒÎ¼Î­Î½Î± Î¿ÏÎ¹ÏƒÎ¼Î­Î½ÎµÏ‚ Î¿ÏÎ¹ÏƒÎ¼Î­Î½Ï‰Î½ Î¿ÏÎ¹ÏƒÎ¼Î­Î½Ï‰Ï‚ ÏŒÏƒÎ± Î¿ÏƒÎ±Î´Î®Ï€Î¿Ï„Îµ ÏŒÏƒÎµÏ‚ Î¿ÏƒÎµÏƒÎ´Î®Ï€Î¿Ï„Îµ\nÏŒÏƒÎ· Î¿ÏƒÎ·Î´Î®Ï€Î¿Ï„Îµ ÏŒÏƒÎ·Î½ Î¿ÏƒÎ·Î½Î´Î®Ï€Î¿Ï„Îµ ÏŒÏƒÎ·Ï‚ Î¿ÏƒÎ·ÏƒÎ´Î®Ï€Î¿Ï„Îµ ÏŒÏƒÎ¿ Î¿ÏƒÎ¿Î´Î®Ï€Î¿Ï„Îµ ÏŒÏƒÎ¿Î¹ Î¿ÏƒÎ¿Î¹Î´Î®Ï€Î¿Ï„Îµ ÏŒÏƒÎ¿Î½ Î¿ÏƒÎ¿Î½Î´Î®Ï€Î¿Ï„Îµ\nÏŒÏƒÎ¿Ï‚ Î¿ÏƒÎ¿ÏƒÎ´Î®Ï€Î¿Ï„Îµ ÏŒÏƒÎ¿Ï… Î¿ÏƒÎ¿Ï…Î´Î®Ï€Î¿Ï„Îµ ÏŒÏƒÎ¿Ï…Ï‚ Î¿ÏƒÎ¿Ï…ÏƒÎ´Î®Ï€Î¿Ï„Îµ ÏŒÏƒÏ‰Î½ Î¿ÏƒÏ‰Î½Î´Î®Ï€Î¿Ï„Îµ ÏŒÏ„Î±Î½ ÏŒÏ„Î¹ Î¿Ï„Î¹Î´Î®Ï€Î¿Ï„Îµ\nÏŒÏ„Î¿Ï… Î¿Ï… Î¿Ï…Î´Î­ Î¿ÏÏ„Îµ ÏŒÏ‡Î¹ Î¿Ï€Î¿Î¯Î±  Î¿Ï€Î¿Î¯ÎµÏ‚  Î¿Ï€Î¿Î¯Î¿  Î¿Ï€Î¿Î¯Î¿Î¹  Î¿Ï€ÏŒÏ„Îµ  Î¿Ï‚\n\nÏ€Î¬Î½Ï‰  Ï€Î±ÏÎ¬  Ï€ÎµÏÎ¯  Ï€Î¿Î»Î»Î¬  Ï€Î¿Î»Î»Î­Ï‚  Ï€Î¿Î»Î»Î¿Î¯  Ï€Î¿Î»Î»Î¿ÏÏ‚  Ï€Î¿Ï…  Ï€ÏÏŽÏ„Î±  Ï€ÏÏŽÏ„ÎµÏ‚  Ï€ÏÏŽÏ„Î·  Ï€ÏÏŽÏ„Î¿  Ï€ÏÏŽÏ„Î¿Ï‚  Ï€Ï‰Ï‚\nÏ€Î¬Î»Î¹ Ï€Î¬Î½Ï„Î± Ï€Î¬Î½Ï„Î¿Ï„Îµ Ï€Î±Î½Ï„Î¿Ï Ï€Î¬Î½Ï„Ï‰Ï‚ Ï€Î¬ÏÎ± Ï€Î­ÏÎ± Ï€Î­ÏÎ¹ Ï€ÎµÏÎ¯Ï€Î¿Ï… Ï€ÎµÏÎ¹ÏƒÏƒÏŒÏ„ÎµÏÎ¿ Ï€Î­ÏÏƒÎ¹ Ï€Î­ÏÏ…ÏƒÎ¹ Ï€Î¹Î± Ï€Î¹Î¸Î±Î½ÏŒÎ½\nÏ€Î¹Î¿ Ï€Î¯ÏƒÏ‰ Ï€Î»Î¬Î¹ Ï€Î»Î­Î¿Î½ Ï€Î»Î·Î½ Ï€Î¿Î¹Î¬ Ï€Î¿Î¹Î¬Î½ Ï€Î¿Î¹Î¬Ï‚ Ï€Î¿Î¹Î­Ï‚ Ï€Î¿Î¹ÏŒ Ï€Î¿Î¹Î¿Î¯ Ï€Î¿Î¹ÏŒÎ½ Ï€Î¿Î¹ÏŒÏ‚ Ï€Î¿Î¹Î¿Ï Ï€Î¿Î¹Î¿ÏÏ‚\nÏ€Î¿Î¹ÏŽÎ½ Ï€Î¿Î»Ï Ï€ÏŒÏƒÎµÏ‚ Ï€ÏŒÏƒÎ· Ï€ÏŒÏƒÎ·Î½ Ï€ÏŒÏƒÎ·Ï‚ Ï€ÏŒÏƒÎ¿Î¹ Ï€ÏŒÏƒÎ¿Ï‚ Ï€ÏŒÏƒÎ¿Ï…Ï‚ Ï€ÏŒÏ„Îµ Ï€Î¿Ï„Î­ Ï€Î¿Ï Ï€Î¿ÏÎ¸Îµ Ï€Î¿Ï…Î¸ÎµÎ½Î¬ Ï€ÏÎ­Ï€ÎµÎ¹\nÏ€ÏÎ¹Î½ Ï€ÏÎ¿ Ï€ÏÎ¿ÎºÎµÎ¹Î¼Î­Î½Î¿Ï… Ï€ÏÏŒÎºÎµÎ¹Ï„Î±Î¹ Ï€ÏÏŒÏ€ÎµÏÏƒÎ¹ Ï€ÏÎ¿Ï‚ Ï€ÏÎ¿Ï„Î¿Ï Ï€ÏÎ¿Ï‡Î¸Î­Ï‚ Ï€ÏÎ¿Ï‡Ï„Î­Ï‚ Ï€ÏÏ‰Ï„ÏÏ„ÎµÏÎ± Ï€ÏŽÏ‚\n\nÏƒÎ±Î½ ÏƒÎ±Ï‚ ÏƒÎµ ÏƒÎµÎ¹Ï‚ ÏƒÎ¿Ï… ÏƒÏ„Î± ÏƒÏ„Î· ÏƒÏ„Î·Î½ ÏƒÏ„Î·Ï‚ ÏƒÏ„Î¹Ï‚ ÏƒÏ„Î¿ ÏƒÏ„Î¿Î½ ÏƒÏ„Î¿Ï… ÏƒÏ„Î¿Ï…Ï‚ ÏƒÏ„Ï‰Î½ ÏƒÏ…Î³Ï‡ÏÏŒÎ½Ï‰Ï‚\nÏƒÏ…Î½ ÏƒÏ…Î½Î¬Î¼Î± ÏƒÏ…Î½ÎµÏ€ÏŽÏ‚ ÏƒÏ…Ï‡Î½Î¬Ï‚ ÏƒÏ…Ï‡Î½Î­Ï‚ ÏƒÏ…Ï‡Î½Î® ÏƒÏ…Ï‡Î½Î®Î½ ÏƒÏ…Ï‡Î½Î®Ï‚ ÏƒÏ…Ï‡Î½ÏŒ ÏƒÏ…Ï‡Î½Î¿Î¯ ÏƒÏ…Ï‡Î½ÏŒÎ½\nÏƒÏ…Ï‡Î½ÏŒÏ‚ ÏƒÏ…Ï‡Î½Î¿Ï ÏƒÏ…Ï‡Î½Î¿ÏÏ‚ ÏƒÏ…Ï‡Î½ÏŽÎ½ ÏƒÏ…Ï‡Î½ÏŽÏ‚ ÏƒÏ‡ÎµÎ´ÏŒÎ½\n\nÏ„Î± Ï„Î¬Î´Îµ Ï„Î±ÏÏ„Î± Ï„Î±ÏÏ„ÎµÏ‚ Ï„Î±ÏÏ„Î· Ï„Î±ÏÏ„Î·Î½ Ï„Î±ÏÏ„Î·Ï‚ Ï„Î±ÏÏ„Î¿Ï„Î±ÏÏ„Î¿Î½ Ï„Î±ÏÏ„Î¿Ï‚ Ï„Î±ÏÏ„Î¿Ï… Ï„Î±ÏÏ„Ï‰Î½ Ï„Î¬Ï‡Î± Ï„Î¬Ï‡Î±Ï„Îµ\nÏ„ÎµÎ»ÎµÏ…Ï„Î±Î¯Î±  Ï„ÎµÎ»ÎµÏ…Ï„Î±Î¯Î¿  Ï„ÎµÎ»ÎµÏ…Ï„Î±Î¯Î¿Ï‚  Ï„Î¿Ï  Ï„ÏÎ¯Î±  Ï„ÏÎ¯Ï„Î·  Ï„ÏÎµÎ¹Ï‚ Ï„ÎµÎ»Î¹ÎºÎ¬ Ï„ÎµÎ»Î¹ÎºÏŽÏ‚ Ï„ÎµÏ‚ Ï„Î­Ï„Î¿Î¹Î± Ï„Î­Ï„Î¿Î¹Î±Î½\nÏ„Î­Ï„Î¿Î¹Î±Ï‚ Ï„Î­Ï„Î¿Î¹ÎµÏ‚ Ï„Î­Ï„Î¿Î¹Î¿ Ï„Î­Ï„Î¿Î¹Î¿Î¹ Ï„Î­Ï„Î¿Î¹Î¿Î½ Ï„Î­Ï„Î¿Î¹Î¿Ï‚ Ï„Î­Ï„Î¿Î¹Î¿Ï…\nÏ„Î­Ï„Î¿Î¹Î¿Ï…Ï‚ Ï„Î­Ï„Î¿Î¹Ï‰Î½ Ï„Î· Ï„Î·Î½ Ï„Î·Ï‚ Ï„Î¹ Ï„Î¯Ï€Î¿Ï„Î± Ï„Î¯Ï€Î¿Ï„Îµ Ï„Î¹Ï‚ Ï„Î¿ Ï„Î¿Î¹ Ï„Î¿Î½ Ï„Î¿Ïƒ Ï„ÏŒÏƒÎ± Ï„ÏŒÏƒÎµÏ‚ Ï„ÏŒÏƒÎ· Ï„ÏŒÏƒÎ·Î½\nÏ„ÏŒÏƒÎ·Ï‚ Ï„ÏŒÏƒÎ¿ Ï„ÏŒÏƒÎ¿Î¹ Ï„ÏŒÏƒÎ¿Î½ Ï„ÏŒÏƒÎ¿Ï‚ Ï„ÏŒÏƒÎ¿Ï… Ï„ÏŒÏƒÎ¿Ï…Ï‚ Ï„ÏŒÏƒÏ‰Î½ Ï„ÏŒÏ„Îµ Ï„Î¿Ï… Ï„Î¿Ï…Î»Î¬Ï‡Î¹ÏƒÏ„Î¿ Ï„Î¿Ï…Î»Î¬Ï‡Î¹ÏƒÏ„Î¿Î½ Ï„Î¿Ï…Ï‚ Ï„Î¿ÏÏ‚ Ï„Î¿ÏÏ„Î±\nÏ„Î¿ÏÏ„ÎµÏ‚ Ï„Î¿ÏÏ„Î· Ï„Î¿ÏÏ„Î·Î½ Ï„Î¿ÏÏ„Î·Ï‚ Ï„Î¿ÏÏ„Î¿ Ï„Î¿ÏÏ„Î¿Î¹ Ï„Î¿ÏÏ„Î¿Î¹Ï‚ Ï„Î¿ÏÏ„Î¿Î½ Ï„Î¿ÏÏ„Î¿Ï‚ Ï„Î¿ÏÏ„Î¿Ï… Ï„Î¿ÏÏ„Î¿Ï…Ï‚ Ï„Î¿ÏÏ„Ï‰Î½ Ï„Ï…Ï‡ÏŒÎ½\nÏ„Ï‰Î½ Ï„ÏŽÏÎ±\n\nÏ…Ï€ Ï…Ï€Î­Ï Ï…Ï€ÏŒ Ï…Ï€ÏŒÏˆÎ· Ï…Ï€ÏŒÏˆÎ¹Î½ ÏÏƒÏ„ÎµÏÎ±\n\nÏ‡Ï‰ÏÎ¯Ï‚ Ï‡Ï‰ÏÎ¹ÏƒÏ„Î¬\n\nÏ‰ Ï‰Ï‚ Ï‰ÏƒÎ¬Î½ Ï‰ÏƒÏŒÏ„Î¿Ï… ÏŽÏƒÏ€Î¿Ï… ÏŽÏƒÏ„Îµ Ï‰ÏƒÏ„ÏŒÏƒÎ¿ Ï‰Ï‡\n'.split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/el/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.el.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/el/__init__.py----------------------------------------
spacy.lang.el.__init__.Greek(Language)
spacy.lang.el.__init__.GreekDefaults(BaseDefaults)
spacy.lang.el.__init__.make_lemmatizer(nlp:Language,model:Optional[Model],name:str,mode:str,overwrite:bool,scorer:Optional[Callable])


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/el/lex_attrs.py----------------------------------------
A:spacy.lang.el.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.el.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('^')
spacy.lang.el.lex_attrs.like_num(text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/el/get_pos_from_wiktionary.py----------------------------------------
A:spacy.lang.el.get_pos_from_wiktionary.regex->re.compile('==={{(\\w+)\\|el}}===')
A:spacy.lang.el.get_pos_from_wiktionary.regex2->re.compile('==={{(\\w+ \\w+)\\|el}}===')
A:spacy.lang.el.get_pos_from_wiktionary.title->title.lower().lower()
A:spacy.lang.el.get_pos_from_wiktionary.all_regex->re.compile('==={{(\\w+)\\|el}}===').findall(text)
A:spacy.lang.el.get_pos_from_wiktionary.words->sorted(expected_parts_dict[i])
spacy.lang.el.get_pos_from_wiktionary.get_pos_from_wiktionary()


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/el/syntax_iterators.py----------------------------------------
A:spacy.lang.el.syntax_iterators.conj->doc.vocab.strings.add('conj')
A:spacy.lang.el.syntax_iterators.nmod->doc.vocab.strings.add('nmod')
A:spacy.lang.el.syntax_iterators.np_label->doc.vocab.strings.add('NP')
spacy.lang.el.syntax_iterators.noun_chunks(doclike:Union[Doc,Span])->Iterator[Tuple[int, int, int]]


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/el/punctuation.py----------------------------------------
A:spacy.lang.el.punctuation.UNITS->merge_chars(_units)
spacy.lang.el.punctuation.merge_chars(char)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ca/examples.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ca/lemmatizer.py----------------------------------------
A:spacy.lang.ca.lemmatizer.univ_pos->token.pos_.lower()
A:spacy.lang.ca.lemmatizer.index_table->self.lookups.get_table('lemma_index', {})
A:spacy.lang.ca.lemmatizer.exc_table->self.lookups.get_table('lemma_exc', {})
A:spacy.lang.ca.lemmatizer.rules_table->self.lookups.get_table('lemma_rules', {})
A:spacy.lang.ca.lemmatizer.lookup_table->self.lookups.get_table('lemma_lookup', {})
A:spacy.lang.ca.lemmatizer.index->self.lookups.get_table('lemma_index', {}).get(univ_pos, {})
A:spacy.lang.ca.lemmatizer.exceptions->self.lookups.get_table('lemma_exc', {}).get(univ_pos, {})
A:spacy.lang.ca.lemmatizer.rules->self.lookups.get_table('lemma_rules', {}).get(univ_pos, [])
A:spacy.lang.ca.lemmatizer.string->string.lower().lower()
A:spacy.lang.ca.lemmatizer.forms->list(dict.fromkeys(forms))
spacy.lang.ca.CatalanLemmatizer(Lemmatizer)
spacy.lang.ca.lemmatizer.CatalanLemmatizer(Lemmatizer)
spacy.lang.ca.lemmatizer.CatalanLemmatizer.get_lookups_config(cls,mode:str)->Tuple[List[str], List[str]]
spacy.lang.ca.lemmatizer.CatalanLemmatizer.rule_lemmatize(self,token:Token)->List[str]


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ca/stop_words.py----------------------------------------
A:spacy.lang.ca.stop_words.STOP_WORDS->set("\na abans acÃ­ ah aixÃ­ aixÃ² al aleshores algun alguna algunes alguns alhora allÃ  allÃ­ allÃ²\nals altra altre altres amb ambdues ambdÃ³s anar ans apa aquell aquella aquelles aquells\naquest aquesta aquestes aquests aquÃ­\n\nbaix bastant bÃ©\n\ncada cadascuna cadascunes cadascuns cadascÃº com consegueixo conseguim conseguir\nconsigueix consigueixen consigueixes contra\n\nd'un d'una d'unes d'uns dalt de del dels des des de desprÃ©s dins dintre donat doncs durant\n\ne eh el elles ells els em en encara ens entre era erem eren eres es esta estan estat\nestava estaven estem esteu estic estÃ  estÃ vem estÃ veu et etc ets Ã©rem Ã©reu Ã©s Ã©ssent\n\nfa faig fan fas fem fer feu fi fins fora\n\ngairebÃ©\n\nha han has haver havia he hem heu hi ho\n\ni igual iguals inclÃ²s\n\nja jo\n\nl'hi la les li li'n llarg llavors\n\nm'he ma mal malgrat mateix mateixa mateixes mateixos me mentre meu meus meva\nmeves mode molt molta moltes molts mon mons mÃ©s\n\nn'he n'hi ne ni no nogensmenys nomÃ©s nosaltres nostra nostre nostres\n\no oh oi on\n\npas pel pels per per que perquÃ¨ perÃ² poc poca pocs podem poden poder\npodeu poques potser primer propi puc\n\nqual quals quan quant que quelcom qui quin quina quines quins quÃ¨\n\ns'ha s'han sa sabem saben saber sabeu sap saps semblant semblants sense ser ses\nseu seus seva seves si sobre sobretot soc solament sols som son sons sota sou sÃ³c sÃ³n\n\nt'ha t'han t'he ta tal tambÃ© tampoc tan tant tanta tantes te tene tenim tenir teniu\nteu teus teva teves tinc ton tons tot tota totes tots\n\nun una unes uns us Ãºltim Ãºs\n\nva vaig vam van vas veu vosaltres vostra vostre vostres\n\n".split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ca/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.ca.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ca/__init__.py----------------------------------------
spacy.lang.ca.__init__.Catalan(Language)
spacy.lang.ca.__init__.CatalanDefaults(BaseDefaults)
spacy.lang.ca.__init__.make_lemmatizer(nlp:Language,model:Optional[Model],name:str,mode:str,overwrite:bool,scorer:Optional[Callable])


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ca/lex_attrs.py----------------------------------------
A:spacy.lang.ca.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.ca.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.ca.lex_attrs.like_num(text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ca/syntax_iterators.py----------------------------------------
A:spacy.lang.ca.syntax_iterators.np_label->doc.vocab.strings.add('NP')
spacy.lang.ca.syntax_iterators.noun_chunks(doclike:Union[Doc,Span])->Iterator[Tuple[int, int, int]]


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ca/punctuation.py----------------------------------------
A:spacy.lang.ca.punctuation.ELISION->" ' â€™ ".strip().replace(' ', '').replace('\n', '')
A:spacy.lang.ca.punctuation._units->char_classes._units.replace('% ', '').replace('% ', '')
A:spacy.lang.ca.punctuation.UNITS->merge_chars(_units)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/grc/examples.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/grc/stop_words.py----------------------------------------
A:spacy.lang.grc.stop_words.STOP_WORDS->set("\nÎ±á½Ï„á¿· Î±á½Ï„Î¿á¿¦ Î±á½Ï„á¿†Ï‚ Î±á½Ï„ÏŒÎ½ Î±á½Ï„á½¸Î½ Î±á½Ï„á¿¶Î½ Î±á½Ï„á½¸Ï‚ Î±á½Ï„á½¸ Î±á½Ï„ÏŒ Î±á½Ï„ÏŒÏ‚ Î±á½Ï„á½´Î½ Î±á½Ï„Î¿á¿–Ï‚ Î±á½Ï„Î¿á½ºÏ‚ Î±á½”Ï„' Î±á½Ï„á½° Î±á½Ï„á¿‡ Î±á½Ï„á½´\nÎ±á½Ï„á½¼ Î±á½‘Ï„Î±á½¶ ÎºÎ±á½Ï„á½¸Ï‚ Î±á½Ï„Î¬ Î±á½‘Ï„ÏŒÏ‚ Î±á½Ï„Î¿á¿–ÏƒÎ¹ Î±á½Ï„Î¿á¿–ÏƒÎ¹Î½ Î±á½‘Ï„á½¸Ï‚ Î±á½Ï„Î®Î½ Î±á½Ï„Î¿á¿–ÏƒÎ¯ Î±á½Ï„Î¿Î¯ Î±á½Ï„Î¿á½¶ Î±á½Ï„Î¿á¿–Î¿ Î±á½Ï„Î¬Ï‰Î½ Î±á½Ï„á½°Ï‚\nÎ±á½Ï„Î­Ï‰Î½ Î±á½Ï„ÏŽ Î±á½Ï„Î¬Ï‚ Î±á½Ï„Î¿ÏÏ‚ Î±á½Ï„Î® Î±á½Ï„Î±Î¯ Î±á½Ï„Î±á½¶ Î±á½Ï„á¿‡ÏƒÎ¹Î½ Ï„á½ Ï…Ï„á¿· Ï„á½ Ï…Ï„á½¸ Ï„Î±á½Ï„á½° Ï„Î±ÏÏ„á¿ƒ Î±á½Ï„á¿‡ÏƒÎ¹ Î±á½Ï„á¿‡Ï‚ Î±á½Ï„Î±á¿–Ï‚ Î±á½Ï„á¾¶Ï‚ Î±á½Ï„á½°Î½ Ï„Î±á½Ï„á½¸Î½\n\nÎ³Îµ Î³' Î³Î­ Î³á½°Ï Î³Î¬Ï Î´Î±á¿–Ï„Î± Î´Î±Î¹Ï„á½¸Ï‚ Î´Î±Î¹Ï„á½¶ Î´Î±á½¶ Î´Î±Î¹Ï„Î¯ Î´Î±á¿–Ï„' Î´Î±ÎÎ´Î±Ï‚ Î´Î±ÎÎ´Ï‰Î½ Î´á¼° Î´Î¹á½° Î´Î¹Î¬ Î´á½² Î´' Î´Î­ Î´á½´ Î´Î® Îµá¼° Îµá¼´ ÎºÎµá¼° ÎºÎµá¼´ Î±á¼´ Î±á¼² Îµá¼² Î±á¼°\n\ná¼ÏƒÏ„Î¯ á¼ÏƒÏ„Î¹Î½ á½¢Î½ á¼¦Î½ á¼ÏƒÏ„á½¶Î½ á½¦ÏƒÎ¹Î½ Îµá¼¶Î½Î±Î¹ á½„Î½Ï„Î¹ Îµá¼°ÏƒÎ¹Î½ á¼ÏƒÏ„Î¹ á½„Î½Ï„Î± Î¿á½–ÏƒÎ±Î½ á¼¦ÏƒÎ±Î½ á¼”ÏƒÏ„Î¹ á½„Î½Ï„Î±Ï‚ á¼ÏƒÏ„á½² Îµá¼°Ïƒá½¶ Îµá¼¶ á½¤Î½ á¼¦ Î¿á½–ÏƒÎ±Î¹ á¼”ÏƒÏ„Î±Î¹ á¼ÏƒÎ¼á½²Î½ á¼ÏƒÏ„' á¼ÏƒÏ„Î¯Î½ á¼”ÏƒÏ„' á½¦ á¼”ÏƒÎµÎ¹ á¼¦Î¼ÎµÎ½ Îµá¼°Î¼Î¹ Îµá¼°Ïƒá½¶Î½ á¼¦ÏƒÎ¸' \ná¼ÏƒÏ„á½¶ á¾– Î¿á½–Ïƒ' á¼”ÏƒÏ„Î¹Î½ Îµá¼°Î¼á½¶ Îµá¼´Î¼' á¼ÏƒÎ¸' á¾–Ï‚ ÏƒÏ„Î¯ Îµá¼´Î·Î½ Îµá¼¶Î½Î±Î¯ Î¿á½–ÏƒÎ± Îºá¼„ÏƒÏ„' Îµá¼´Î· á¼¦ÏƒÎ¸Î± Îµá¼°Î¼' á¼”ÏƒÏ„Ï‰ á½„Î½Ï„' á¼”ÏƒÎ¸' á¼”Î¼Î¼ÎµÎ½Î±Î¹ á¼”Ï‰ á¼á½¼Î½ á¼ÏƒÏƒÎ¹ á¼”ÏƒÏƒÎµÏ„Î±Î¹ á¼ÏƒÏ„á½¸Î½ á¼”ÏƒÎ±Î½ á¼”ÏƒÏ„Ï‰Î½ á¼ÏŒÎ½Ï„Î± á¼¦ÎµÎ½ á¼Î¿á¿¦ÏƒÎ±Î½ á¼”Î·Î½ \ná¼”ÏƒÏƒÎ¿Î¼Î±Î¹ Îµá¼°ÏƒÎ¯ á¼ÏƒÏ„ÏŒÎ½ á¼”ÏƒÎºÎµÎ½ á¼ÏŒÎ½Ï„' á¼ÏŽÎ½ á¼”ÏƒÏƒÎµÏƒÎ¸' Îµá¼°Ïƒ' á¼ÏŒÎ½Ï„ÎµÏ‚ á¼ÏŒÎ½Ï„Îµ á¼ÏƒÏƒÎµá¿–Ï„Î±Î¹ Îµá¼°Î¼ÎµÎ½ á¼”Î±ÏƒÎ¹Î½ á¼”ÏƒÎºÎµ á¼”Î¼ÎµÎ½Î±Î¹ á¼”ÏƒÎµÏƒÎ¸Î±Î¹ á¼”á¿ƒ Îµá¼°Î¼á½²Î½ Îµá¼°ÏƒÎ¹ á¼ÏŒÎ½Ï„Î±Ï‚ á¼”ÏƒÏ„Îµ Îµá¼°Ï‚ á¼¦Ï„Îµ Îµá¼°Î¼Î¯ á¼”ÏƒÏƒÎµÎ±Î¹ á¼”Î¼Î¼ÎµÎ½ \ná¼Î¿á¿¦ÏƒÎ± á¼”Î¼ÎµÎ½ á¾–ÏƒÎ¹Î½ á¼ÏƒÏ„Îµ á¼ÏŒÎ½Ï„Î¹ Îµá¼¶ÎµÎ½ á¼”ÏƒÏƒÎ¿Î½Ï„Î±Î¹ á¼”Î·ÏƒÎ¸Î± á¼”ÏƒÎµÏƒÎ¸Îµ á¼ÏƒÏƒÎ¯ á¼Î¿á¿¦Ïƒ' á¼”Î±ÏƒÎ¹ á¼”Î± á¼¦Î± á¼ÏŒÎ½ á¼”ÏƒÏƒÎµÏƒÎ¸Î±Î¹ á¼”ÏƒÎ¿Î¼Î±Î¹ á¼”ÏƒÎºÎ¿Î½ Îµá¼´Î·Ï‚ á¼”Ï‰ÏƒÎ¹Î½ Îµá¼´Î·ÏƒÎ±Î½ á¼á½¸Î½ á¼Î¿Ï…ÏƒÎ­Ï‰Î½ á¼”ÏƒÏƒá¿ƒ á¼Î¿ÏÏƒÎ·Ï‚ á¼”ÏƒÎ¿Î½Ï„Î±Î¹ \ná¼Î¿ÏÏƒÎ±Ï‚ á¼ÏŒÎ½Ï„Ï‰Î½ á¼ÏŒÎ½Ï„Î¿Ï‚ á¼ÏƒÎ¿Î¼Î­Î½Î·Î½ á¼”ÏƒÏ„Ï‰ÏƒÎ±Î½ á¼”Ï‰ÏƒÎ¹ á¼”Î±Ï‚ á¼Î¿á¿¦ÏƒÎ±Î¹ á¼£Î½ Îµá¼°ÏƒÎ¯Î½ á¼¤ÏƒÏ„Î·Î½ á½„Î½Ï„ÎµÏ‚ á½„Î½Ï„Ï‰Î½ Î¿á½”ÏƒÎ±Ï‚ Î¿á½”ÏƒÎ±Î¹Ï‚ á½„Î½Ï„Î¿Ï‚ Î¿á½–ÏƒÎ¹ Î¿á½”ÏƒÎ·Ï‚ á¼”Ïƒá¿ƒ á½‚Î½ á¼ÏƒÎ¼ÎµÎ½ á¼ÏƒÎ¼Î­Î½ Î¿á½–ÏƒÎ¹Î½ á¼ÏƒÎ¿Î¼Î­Î½Î¿Ï…Ï‚ á¼ÏƒÏƒÏŒÎ¼ÎµÏƒÎ¸Î±\n\ná¼’Ï‚ á¼Ï‚ á¼”Ï‚ á¼Î½ ÎºÎµá¼°Ï‚ Îµá¼²Ï‚ Îºá¼€Î½ á¼”Î½ ÎºÎ±Ï„á½° ÎºÎ±Ï„' ÎºÎ±Î¸' ÎºÎ±Ï„Î¬ ÎºÎ¬Ï„Î± Îºá½°Ï€ Îºá½°Îº Îºá½°Î´ Îºá½°Ï ÎºÎ¬Ï Îºá½°Î³ Îºá½°Î¼ ÎºÎ±á½¶ ÎºÎ±Î¯ Î¼ÎµÏ„á½° Î¼ÎµÎ¸' Î¼ÎµÏ„' Î¼Î­Ï„Î± Î¼ÎµÏ„Î¬ Î¼Î­Î¸' Î¼Î­Ï„' Î¼á½²Î½ Î¼Î­Î½ Î¼á½´\n\nÎ¼Î® Î¼Î· Î¿á½Îº Î¿á½’ Î¿á½ Î¿á½Ï‡ Î¿á½Ï‡á½¶ ÎºÎ¿á½ ÎºÎ¿á½Ï‡ Î¿á½” ÎºÎ¿á½Îº Î¿á½Ï‡Î¯ Î¿á½Îºá½¶ Î¿á½Î´á½²Î½ Î¿á½Î´Îµá½¶Ï‚ Î¿á½Î´Î­Î½ ÎºÎ¿á½Î´Îµá½¶Ï‚ ÎºÎ¿á½Î´á½²Î½ Î¿á½Î´Î­Î½Î± Î¿á½Î´ÎµÎ½á½¸Ï‚ Î¿á½Î´Î­Î½' Î¿á½Î´ÎµÎ½ÏŒÏ‚ Î¿á½Î´ÎµÎ½á½¶\nÎ¿á½Î´ÎµÎ¼Î¯Î± Î¿á½Î´ÎµÎ¯Ï‚ Î¿á½Î´ÎµÎ¼Î¯Î±Î½ Î¿á½Î´á½² Î¿á½Î´' ÎºÎ¿á½Î´' Î¿á½Î´Î­ Î¿á½”Ï„Îµ Î¿á½”Î¸' Î¿á½”Ï„Î­ Ï„Îµ Î¿á½”Ï„' Î¿á½•Ï„Ï‰Ï‚ Î¿á½•Ï„Ï‰ Î¿á½•Ï„á¿¶ Ï‡Î¿á½”Ï„Ï‰Ï‚ Î¿á½–Î½ á½¦Î½ á½§Î½ Ï„Î¿á¿¦Ï„Î¿ Ï„Î¿á¿¦Î¸' Ï„Î¿á¿¦Ï„Î¿Î½ Ï„Î¿ÏÏ„á¿³\nÏ„Î¿ÏÏ„Î¿Î¹Ï‚ Ï„Î±ÏÏ„Î±Ï‚ Î±á½•Ï„Î· Ï„Î±á¿¦Ï„Î± Î¿á½—Ï„Î¿Ï‚ Ï„Î±ÏÏ„Î·Ï‚ Ï„Î±ÏÏ„Î·Î½ Ï„Î¿ÏÏ„Ï‰Î½ Ï„Î±á¿¦Ï„' Ï„Î¿á¿¦Ï„' Ï„Î¿ÏÏ„Î¿Ï… Î±á½—Ï„Î±Î¹ Ï„Î¿ÏÏ„Î¿Ï…Ï‚ Ï„Î¿á¿¦Ï„ÏŒ Ï„Î±á¿¦Ï„Î¬ Ï„Î¿ÏÏ„Î¿Î¹ÏƒÎ¹ Ï‡Î±á½”Ï„Î· Ï„Î±á¿¦Î¸' Ï‡Î¿á½–Ï„Î¿Î¹\nÏ„Î¿ÏÏ„Î¿Î¹ÏƒÎ¹Î½ Î¿á½—Ï„ÏŒÏ‚ Î¿á½—Ï„Î¿Î¹ Ï„Î¿ÏÏ„Ï‰ Ï„Î¿Ï…Ï„Î­Ï‰Î½ Ï„Î¿á¿¦Ï„á½¸Î½ Î¿á½—Ï„Î¿Î¯ Ï„Î¿á¿¦Ï„Î¿Ï… Î¿á½—Ï„Î¿á½¶ Ï„Î±ÏÏ„á¿ƒÏƒÎ¹ Ï„Î±ÏÏ„Î±Î¹Ï‚ Ï„Î±Ï…Ï„á½¶ Ï€Î±Ïá½° Ï€Î±Ï' Ï€Î¬ÏÎ± Ï€Î±ÏÎ¬ Ï€á½°Ï Ï€Î±ÏÎ±á½¶ Ï€Î¬Ï' Ï€ÎµÏá½¶\nÏ€Î­ÏÎ¹ Ï€ÎµÏÎ¯ Ï€Ïá½¸Ï‚ Ï€ÏÏŒÏ‚ Ï€Î¿Ï„' Ï€Î¿Ï„á½¶ Ï€ÏÎ¿Ï„á½¶ Ï€ÏÎ¿Ï„Î¯ Ï€ÏŒÏ„Î¹\n\nÏƒá½¸Ï‚ ÏƒÎ®Î½ Ïƒá½´Î½ Ïƒá½¸Î½ ÏƒÏŒÎ½ Ïƒá½° Ïƒá¿¶Î½ ÏƒÎ¿á¿–ÏƒÎ¹Î½ ÏƒÏŒÏ‚ Ïƒá¿†Ï‚ Ïƒá¿· ÏƒÎ±á¿–Ï‚ Ïƒá¿‡ ÏƒÎ¿á¿–Ï‚ ÏƒÎ¿á¿¦ Ïƒ' Ïƒá½°Î½ ÏƒÎ¬ Ïƒá½´ Ïƒá½°Ï‚\nÏƒá¾· ÏƒÎ¿á½ºÏ‚ ÏƒÎ¿ÏÏ‚ ÏƒÎ¿á¿–ÏƒÎ¹ Ïƒá¿‡Ï‚ Ïƒá¿‡ÏƒÎ¹ ÏƒÎ® Ïƒá¿‡ÏƒÎ¹Î½ ÏƒÎ¿á½¶ ÏƒÎ¿Ï… á½‘Î¼Îµá¿–Ï‚ Ïƒá½² ÏƒÏ ÏƒÎ¿Î¹ á½‘Î¼á¾¶Ï‚ á½‘Î¼á¿¶Î½ á½‘Î¼á¿–Î½ ÏƒÎµ\nÏƒÎ­ Ïƒá½º ÏƒÎ­Î¸ÎµÎ½ ÏƒÎ¿Î¯ á½‘Î¼á½¶Î½ ÏƒÏ†á¿·Î½ á½‘Î¼Î¯Î½ Ï„Î¿Î¹ Ï„Î¿á½¶ ÏƒÏ†á½¼ á½”Î¼Î¼' ÏƒÏ†á¿¶ÏŠ ÏƒÎµá¿–Î¿ Ï„' ÏƒÏ†á¿¶ÏŠÎ½ á½”Î¼Î¼Î¹Î½ ÏƒÎ­Î¿ ÏƒÎµÏ… ÏƒÎµá¿¦\ná½”Î¼Î¼Î¹ á½‘Î¼Î­Ï‰Î½ Ï„ÏÎ½Î· á½‘Î¼ÎµÎ¯Ï‰Î½ Ï„Î¿Î¯ á½”Î¼Î¼ÎµÏ‚ ÏƒÎµÎ¿ Ï„Î­ Ï„ÎµÎ¿á¿–Î¿ á½‘Î¼Î­Î±Ï‚ Ïƒá½ºÎ½ Î¾á½ºÎ½ ÏƒÏÎ½ \n\nÎ¸' Ï„Î¯ Ï„Î¹ Ï„Î¹Ï‚ Ï„Î¹Î½ÎµÏ‚ Ï„Î¹Î½Î± Ï„Î¹Î½Î¿Ï‚ Ï„Î¹Î½á½¸Ï‚ Ï„Î¹Î½á½¶ Ï„Î¹Î½á¿¶Î½ Ï„Î¯Ï‚ Ï„Î¯Î½ÎµÏ‚ Ï„Î¹Î½á½°Ï‚ Ï„Î¹Î½' Ï„á¿³ Ï„Î¿Ï… Ï„Î¯Î½Î± Ï„Î¿á¿¦ Ï„á¿· Ï„Î¹Î½Î¯ Ï„Î¹Î½Î¬ Ï„Î¯Î½Î¿Ï‚ Ï„Î¹Î½Î¹ Ï„Î¹Î½Î±Ï‚ Ï„Î¹Î½á½° Ï„Î¹Î½Ï‰Î½\nÏ„Î¯Î½' Ï„ÎµÏ… Ï„Î­Î¿ Ï„Î¹Î½Î­Ï‚ Ï„ÎµÎ¿ Ï„Î¹Î½á½²Ï‚ Ï„Îµá¿· Ï„Î­á¿³ Ï„Î¹Î½ÏŒÏ‚ Ï„Îµá¿³ Ï„Î¹Ïƒá½¶ \n\nÏ„Î¿Î¹Î±á¿¦Ï„Î± Ï„Î¿Î¹Î¿á¿¦Ï„Î¿Î½ Ï„Î¿Î¹Î¿á¿¦Î¸' Ï„Î¿Î¹Î¿á¿¦Ï„Î¿Ï‚ Ï„Î¿Î¹Î±ÏÏ„Î·Î½ Ï„Î¿Î¹Î±á¿¦Ï„' Ï„Î¿Î¹Î¿ÏÏ„Î¿Ï… Ï„Î¿Î¹Î±á¿¦Î¸' Ï„Î¿Î¹Î±ÏÏ„á¿ƒ Ï„Î¿Î¹Î¿ÏÏ„Î¿Î¹Ï‚ Ï„Î¿Î¹Î±á¿¦Ï„Î±Î¹ Ï„Î¿Î¹Î±á¿¦Ï„Î¬ Ï„Î¿Î¹Î±ÏÏ„Î· Ï„Î¿Î¹Î¿á¿¦Ï„Î¿Î¹ Ï„Î¿Î¹Î¿ÏÏ„Ï‰Î½ Ï„Î¿Î¹Î¿ÏÏ„Î¿Î¹ÏƒÎ¹\nÏ„Î¿Î¹Î¿á¿¦Ï„Î¿ Ï„Î¿Î¹Î¿ÏÏ„Î¿Ï…Ï‚ Ï„Î¿Î¹Î¿ÏÏ„á¿³ Ï„Î¿Î¹Î±ÏÏ„Î·Ï‚ Ï„Î¿Î¹Î±ÏÏ„Î±Î¹Ï‚ Ï„Î¿Î¹Î±ÏÏ„Î±Ï‚ Ï„Î¿Î¹Î¿á¿¦Ï„ÏŒÏ‚ Ï„Î¯Î½Î¹ Ï„Î¿á¿–ÏƒÎ¹ Ï„Î¯Î½Ï‰Î½ Ï„Î­Ï‰Î½ Ï„Î­Î¿Î¹ÏƒÎ¯ Ï„á½° Ï„á¿‡ Ï„ÏŽ Ï„á½¼\n\ná¼€Î»Î»á½° á¼€Î»Î»' á¼€Î»Î»Î¬ á¼€Ï€' á¼€Ï€á½¸ Îºá¼€Ï€' á¼€Ï†' Ï„á¼€Ï€á½¸ Îºá¼€Ï†' á¼„Ï€Î¿ á¼€Ï€ÏŒ Ï„á½ Ï€á½¸ Ï„á¼€Ï€' á¼„Î»Î»Ï‰Î½ á¼„Î»Î»á¿³ á¼„Î»Î»Î· á¼„Î»Î»Î·Ï‚ á¼„Î»Î»Î¿Ï…Ï‚ á¼„Î»Î»Î¿Î¹Ï‚ á¼„Î»Î»Î¿Î½ á¼„Î»Î»Î¿ á¼„Î»Î»Î¿Ï… Ï„á¼„Î»Î»Î± á¼„Î»Î»Î± \ná¼„Î»Î»á¾³ á¼„Î»Î»Î¿Î¹ÏƒÎ¹Î½ Ï„á¼„Î»Î»' á¼„Î»Î»' á¼„Î»Î»Î¿Ï‚ á¼„Î»Î»Î¿Î¹ÏƒÎ¹ Îºá¼„Î»Î»' á¼„Î»Î»Î¿Î¹ á¼„Î»Î»á¿ƒÏƒÎ¹ á¼„Î»Î»ÏŒÎ½ á¼„Î»Î»Î·Î½ á¼„Î»Î»Î¬ á¼„Î»Î»Î±Î¹ á¼„Î»Î»Î¿Î¹ÏƒÎ¯Î½ á½§Î»Î»Î¿Î¹ á¼„Î»Î»á¿ƒ á¼„Î»Î»Î±Ï‚ á¼€Î»Î»Î­Ï‰Î½ Ï„á¼†Î»Î»Î± á¼„Î»Î»Ï‰Ï‚\ná¼€Î»Î»Î¬Ï‰Î½ á¼„Î»Î»Î±Î¹Ï‚ Ï„á¼†Î»Î»'\n\ná¼‚Î½ á¼„Î½ Îºá¼‚Î½ Ï„á¼‚Î½ á¼ƒÎ½ ÎºÎµÎ½ Îº' ÎºÎ­Î½ ÎºÎ­ ÎºÎµ Ï‡' á¼„ÏÎ± Ï„á¼„ÏÎ± á¼„Ï' Ï„á¼„Ï' á¼„Ï á¿¥Î± á¿¥Î¬ á¿¥ Ï„á½°Ï á¼„ÏÎ¬ á¼‚Ï\n\ná¼¡Î¼á¾¶Ï‚ Î¼Îµ á¼Î³á½¼ á¼Î¼á½² Î¼Î¿Î¹ Îºá¼€Î³á½¼ á¼¡Î¼á¿¶Î½ á¼¡Î¼Îµá¿–Ï‚ á¼Î¼Î¿á½¶ á¼”Î³Ï‰Î³' á¼Î¼Î¿á½¶ á¼¡Î¼á¿–Î½ Î¼' á¼”Î³Ï‰Î³Î­ á¼Î³ÏŽ á¼Î¼Î¿Î¯ á¼Î¼Î¿á¿¦ Îºá¼€Î¼Î¿á¿¦ á¼”Î¼' Îºá¼€Î¼á½² á¼¡Î¼á½¶Î½ Î¼Î¿Ï… á¼Î¼Î­ á¼”Î³Ï‰Î³Îµ Î½á¿·Î½ Î½á½¼ Ï‡á¼ Î¼Îµá¿–Ï‚ á¼Î¼á½² Îºá¼€Î³ÏŽ Îºá¼€Î¼Î¿á½¶ Ï‡á¼ Î¼á¾¶Ï‚\ná¼Î³á½¼ á¼¡Î¼Î¯Î½ Îºá¼„Î¼' á¼”Î¼Î¿Î¹Î³' Î¼Î¿Î¯ Ï„Î¿á½Î¼á½² á¼„Î¼Î¼Îµ á¼Î³á½¼Î½ á¼Î¼Îµá¿¦ á¼Î¼Îµá¿–Î¿ Î¼ÎµÏ… á¼”Î¼Î¿Î¹Î³Îµ á¼„Î¼Î¼Î¹ Î¼Î­ á¼¡Î¼Î­Î±Ï‚ Î½á¿¶ÏŠ á¼„Î¼Î¼Î¹Î½ á¼§Î¼Î¹Î½ á¼Î³ÏŽÎ½ Î½á¿¶Î á¼Î¼Î­Î¸ÎµÎ½ á¼¥Î¼Î¹Î½ á¼„Î¼Î¼ÎµÏ‚ Î½á¿¶Î¹ á¼¡Î¼ÎµÎ¯Ï‰Î½ á¼„Î¼Î¼' á¼¡Î¼Î­Ï‰Î½ á¼Î¼Î­Î¿\ná¼Îº á¼”Îº á¼Î¾ Îºá¼€Îº Îº á¼ƒÎº Îºá¼€Î¾ á¼”Î¾ ÎµÎ¾ á¼˜Îº Ï„á¼€Î¼á½° á¼Î¼Î¿á¿–Ï‚ Ï„Î¿á½Î¼ÏŒÎ½ á¼Î¼á¾¶Ï‚ Ï„Î¿á½Î¼á½¸Î½ á¼Î¼á¿¶Î½ á¼Î¼á½¸Ï‚ á¼Î¼á¿†Ï‚ á¼Î¼á¿· Ï„á½ Î¼á¿· á¼Î¼á½¸Î½ Ï„á¼„Î¼' á¼Î¼á½´ á¼Î¼á½°Ï‚ á¼Î¼Î±á¿–Ï‚ á¼Î¼á½´Î½ á¼Î¼ÏŒÎ½ á¼Î¼á½° á¼Î¼ÏŒÏ‚ á¼Î¼Î¿á½ºÏ‚ á¼Î¼á¿‡ á¼Î¼á¾·\nÎ¿á½‘Î¼á½¸Ï‚ á¼Î¼Î¿á¿–Î½ Î¿á½‘Î¼ÏŒÏ‚ Îºá¼€Î¼á½¸Î½ á¼Î¼Î±á½¶ á¼Î¼Î® á¼Î¼Î¬Ï‚ á¼Î¼Î¿á¿–ÏƒÎ¹ á¼Î¼Î¿á¿–ÏƒÎ¹Î½ á¼Î¼á¿‡ÏƒÎ¹Î½ á¼Î¼á¿‡ÏƒÎ¹ á¼Î¼á¿‡Ï‚ á¼Î¼Î®Î½ \n\ná¼”Î½Î¹ á¼Î½á½¶ Îµá¼°Î½á½¶ Îµá¼°Î½ á¼Î¼ á¼Ï€á½¶ á¼Ï€' á¼”Ï€Î¹ á¼Ï†' Îºá¼€Ï€á½¶ Ï„á¼€Ï€á½¶ á¼Ï€Î¯ á¼”Ï†' á¼”Ï€' á¼á½°Î½ á¼¢Î½ á¼Î¬Î½ á¼¤Î½ á¼„Î½Ï€ÎµÏ\n\nÎ±á½‘Ï„Î¿á¿–Ï‚ Î±á½‘Ï„á½¸Î½ Î±á½‘Ï„á¿· á¼‘Î±Ï…Ï„Î¿á¿¦ Î±á½‘Ï„ÏŒÎ½ Î±á½‘Ï„á¿†Ï‚ Î±á½‘Ï„á¿¶Î½ Î±á½‘Ï„Î¿á¿¦ Î±á½‘Ï„á½´Î½ Î±á½‘Ï„Î¿á¿–Î½ Ï‡Î±á½Ï„Î¿á¿¦ Î±á½‘Ï„Î±á¿–Ï‚ á¼‘Ï‰Ï…Ï„Î¿á¿¦ á¼‘Ï‰Ï…Ï„á¿‡ á¼‘Ï‰Ï…Ï„á½¸Î½ á¼Ï‰Ï…Ï„á¿· á¼‘Ï‰Ï…Ï„á¿†Ï‚ á¼‘Ï‰Ï…Ï„ÏŒÎ½ á¼‘Ï‰Ï…Ï„á¿·\ná¼‘Ï‰Ï…Ï„Î¬Ï‚ á¼‘Ï‰Ï…Ï„á¿¶Î½ á¼‘Ï‰Ï…Ï„Î¿á½ºÏ‚ á¼‘Ï‰Ï…Ï„Î¿á¿–ÏƒÎ¹ á¼‘Î±Ï…Ï„á¿‡ á¼‘Î±Ï…Ï„Î¿ÏÏ‚ Î±á½‘Ï„Î¿á½ºÏ‚ á¼‘Î±Ï…Ï„á¿¶Î½ á¼‘Î±Ï…Ï„Î¿á½ºÏ‚ á¼‘Î±Ï…Ï„á½¸Î½ á¼‘Î±Ï…Ï„á¿· á¼‘Î±Ï…Ï„Î¿á¿–Ï‚ á¼‘Î±Ï…Ï„á½´Î½ á¼‘Î±Ï…Ï„á¿†Ï‚ \n\ná¼”Ï„Î¹ á¼”Ï„' á¼”Î¸' Îºá¼„Ï„Î¹ á¼¢ á¼¤ á¼ Î­ á¼ á½² á¼¦Îµ á¼¦Î­ á¼¡ Ï„Î¿á½ºÏ‚ Ï„á½´Î½ Ï„á½¸ Ï„á¿¶Î½ Ï„á½¸Î½ á½ á¼ Î¿á¼± Ï„Î¿á¿–Ï‚ Ï„Î±á¿–Ï‚ Ï„á¿†Ï‚ Ï„á½°Ï‚ Î±á¼± Ï„ÏŒ Ï„á½°Î½ Ï„á¾¶Ï‚ Ï„Î¿á¿–ÏƒÎ¹Î½ Î±á¼³ Ï‡á½  Ï„Î®Î½ Ï„Î¬ Ï„Î¿á¿–Î½ Ï„Î¬Ï‚ á½…\nÏ‡Î¿á¼° á¼£ á¼¥ Ï‡á¼  Ï„Î¬Î½ Ï„á¾¶Î½ á½ƒ Î¿á¼³ Î¿á¼µ Ï„Î¿á¿–Î¿ Ï„ÏŒÎ½ Ï„Î¿á¿–Î¹Î½ Ï„Î¿ÏÏ‚ Ï„Î¬Ï‰Î½ Ï„Î±á½¶ Ï„á¿‡Ï‚ Ï„á¿‡ÏƒÎ¹ Ï„á¿‡ÏƒÎ¹Î½ Î±á¼µ Ï„Î¿á¿–ÏŒ Ï„Î¿á¿–ÏƒÎ¯Î½ á½…Ï„Ï„Î¯ Ï„Î±Î¯ Î¤á½´Î½ Ï„á¿† Ï„á¿¶ Ï„Î¬Î´Îµ á½…Î´Îµ Ï„Î¿á¿¦Î´Îµ Ï„ÏŒÎ´Îµ Ï„ÏŒÎ½Î´'\nÏ„Î¬Î´' Ï„á¿†ÏƒÎ´Îµ Ï„á¿·Î´Îµ á½…Î´' Ï„á¿¶Î½Î´' Ï„á¿‡Î´' Ï„Î¿á¿¦Î´Î­ Ï„á¿¶Î½Î´Îµ Ï„ÏŒÎ½Î´Îµ Ï„ÏŒÎ´' Ï„Î¿á¿¦Î´' Ï„Î¬ÏƒÎ´Îµ Ï„Î®Î½Î´Îµ Ï„Î¬ÏƒÎ´' Ï„Î®Î½Î´' Ï„Î±á¿–ÏƒÎ´Î­ Ï„á¿‡Î´Îµ Ï„á¿†ÏƒÎ´' Ï„Î¬Î½Î´' Ï„á¿·Î´' Ï„Î¬Î½Î´Îµ á¼…Î´Îµ Ï„Î¿á¿–ÏƒÎ´' á¼¥Î´'\nÏ„á¾·Î´Î­ Ï„Î¿á¿–ÏƒÎ´Îµ Ï„Î¿ÏÏƒÎ´' á¼¥Î´Îµ Ï„Î¿ÏÏƒÎ´Îµ Ï„ÏŽÎ´' á¼…Î´' Î¿á¼µÎ´' Ï„á¿¶Î½Î´Î­ Î¿á¼µÎ´Îµ Ï„á¾·Î´Îµ Ï„Î¿á¿–ÏƒÎ´ÎµÏƒÏƒÎ¹ Ï„ÏŽÎ´Îµ Ï„á¿‡Î´Î­ Ï„Î¿á¿–ÏƒÎ¹Î´Îµ Î±á¼µÎ´Îµ Ï„Î¿á¿¦Î´á½² Ï„á¿†Î´' Î±á¼µÎ´' Ï„Î¿á¿–ÏƒÎ´ÎµÏƒÎ¹ á½ƒÎ½ á¼ƒ á½ƒÏ‚ á¾§ Î¿á½— á¼…Ï€ÎµÏ\nÎ¿á½“Ï‚ á¼§Ï‚ Î¿á¼·Ï‚ á¼…ÏƒÏ€ÎµÏ á¾— á¼… Ï‡á½¦Î½Ï€ÎµÏ á½£ Î±á¼·Ï‚ á¾‡ á½…Ï‚ á¼¥Ï€ÎµÏ á¼ƒÏ‚ á½…ÏƒÏ€ÎµÏ á½…Î½Ï€ÎµÏ á½§Î½Ï€ÎµÏ á¾§Ï€ÎµÏ á½…Î½ Î±á¼·Î½ Î¿á¼·ÏƒÎ¹ á¼‡Ï‚ á¼…Ï‚ á½¥ Î¿á½•Ï‚ á¼¥Î½ Î¿á¼·ÏƒÎ¹Î½ á¼•Î·Ï‚ á½…Î¿Ï… á¾—Ï‚ Î¿á¼·ÏƒÎ¯ Î¿á¼·ÏƒÎ¯Î½ Ï„Î¿á¿–ÏƒÎ¯ á¾—ÏƒÎ¹Î½ Î¿á¼µÏ€ÎµÏ Î±á¼·ÏƒÏ€ÎµÏ\ná½…ÏƒÏ„Î¹Ï‚ á¼¥Ï„Î¹Ï‚ á½…Ï„Î¿Ï… á½…Ï„Î¿Î¹ÏƒÎ¹ á¼¥Î½Ï„Î¹Î½' á½…Ï„á¿³ á½…Î½Ï„Î¹Î½' á½…Ï„Ï„Î¹ á¼…ÏƒÏƒÎ¬ á½…Ï„Îµá¿³ á½…Ï„Î¹Ï‚ á½…Ï„Î¹Î½' á½…Ï„ÎµÏ… á¼¥Î½Ï„Î¹Î½Î± Î±á¼µÏ„Î¹Î½Î­Ï‚ á½…Î½Ï„Î¹Î½Î± á¼…ÏƒÏƒÎ± á¾§Ï„Î¹Î½Î¹ Î¿á¼µÏ„Î¹Î½ÎµÏ‚ á½…Ï„Î¹ á¼…Ï„Î¹Ï‚ á½…Ï„' á½‘Î¼á½´\ná½‘Î¼Î®Î½ á½‘Î¼á½¸Î½ á½‘Ï€á½²Ï á½•Ï€ÎµÏ á½‘Ï€Î­ÏÏ„ÎµÏÎ¿Î½ á½‘Ï€Îµá½¶Ï á½‘Ï€Î­ÏÏ„Î±Ï„Î¿Ï‚ á½‘Ï€á½¸ á½‘Ï€' á½‘Ï†' á½•Ï€Î¿ á½‘Ï€Î±á½¶ á½‘Ï€ÏŒ á½•Ï€' á½•Ï†'\n\n á½£Ï‚ á½¡Ï‚ á½¥Ï‚ á½§Ï‚ á½¥ÏƒÏ„' á½¥ÏƒÏ„Îµ á½¥ÏƒÎ¸' á½¤ á½¢ \n \n ".split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/grc/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.grc.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/grc/__init__.py----------------------------------------
spacy.lang.grc.__init__.AncientGreek(Language)
spacy.lang.grc.__init__.AncientGreekDefaults(BaseDefaults)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/grc/lex_attrs.py----------------------------------------
spacy.lang.grc.lex_attrs.like_num(text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/grc/punctuation.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/am/examples.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/am/stop_words.py----------------------------------------
A:spacy.lang.am.stop_words.STOP_WORDS->set('\náŒáŠ• áŠ áŠ•á‰º áŠ áŠ•á‰° áŠ¥áŠ“áŠ•á‰° á‹«áŠ•á‰° á‹«áŠ•á‰º á‹¨áŠ“áŠ•á‰° áˆ«áˆµáˆ…áŠ• áˆ«áˆµáˆ½áŠ• áˆ«áˆ³á‰½áˆáŠ•\náˆáˆ‰ áŠ‹áˆ‹ á‰ áˆ°áˆžáŠ‘ áŠ áˆ‰ á‰ áŠ‹áˆ‹ áˆáŠ”á‰³ á‰ áŠ©áˆ áŠ áˆµá‰³á‹á‰€á‹‹áˆ áˆ†áŠ á‰ á‹áˆµáŒ¥\náŠ áˆµá‰³á‹áˆ°á‹‹áˆ áˆ†áŠ‘ á‰£áŒ£áˆ áŠ¥áˆµáŠ«áˆáŠ• áˆ†áŠ–áˆ á‰ á‰°áˆˆá‹­ áŠ áˆ³áˆ°á‰  áˆáˆ á‰ á‰°áˆ˜áˆˆáŠ¨á‰°\náŠ áˆ³áˆµá‰ á‹‹áˆ áˆ‹á‹­ á‰ á‰°áˆ˜áˆ³áˆ³á‹­ áŠ áˆµáˆáˆ‹áŒŠ áˆŒáˆ‹ á‹¨á‰°áˆˆá‹«á‹¨ áŠ áˆµáŒˆáŠá‹˜á‰¡ áˆŒáˆŽá‰½ á‹¨á‰°áˆˆá‹«á‹©\náŠ áˆµáŒˆáŠ•á‹á‰ á‹‹áˆ áˆá‹© á‰°á‰£áˆˆ áŠ á‰¥áˆ«áˆ­á‰°á‹‹áˆ áˆ˜áˆ†áŠ‘ á‰°áŒˆáˆˆáŒ¸ áŠ áˆµáˆ¨á‹µá‰°á‹‹áˆ  á‰°áŒˆáˆáŒ¿áˆ\náˆ›áˆˆá‰± á‰°áŒ¨áˆ›áˆª áŠ¥á‰£áŠ­áˆ… á‹¨áˆšáŒˆáŠ á‰°áŠ¨áŠ“á‹ˆáŠ áŠ¥á‰£áŠ­áˆ½ áˆ›á‹µáˆ¨áŒ á‰½áŒáˆ­ áŠ áŠ•áŒ»áˆ­ áˆ›áŠ•\ná‰µáŠ“áŠ•á‰µ áŠ¥áˆµáŠªá‹°áˆ­áˆµ áŠá‰ áˆ¨á‰½ áŠ¥áŠ•áŠ³ áˆ°áˆžáŠ‘áŠ• áŠá‰ áˆ© áŠ¥áŠ•áŠ³áŠ• áˆ²áˆ†áŠ• áŠá‰ áˆ­ áŠ¥á‹šáˆ áˆ²áˆ\náŠá‹ áŠ¥áŠ•á‹°áŒˆáˆˆáŒ¹á‰µ áŠ áˆˆ áŠ“ áŠ¥áŠ•á‹°á‰°áŠ“áŒˆáˆ©á‰µ á‰¢áˆ†áŠ• áŠáŒˆáˆ­ áŠ¥áŠ•á‹³áˆµáˆ¨á‹±á‰µ á‰¥áˆˆá‹‹áˆ áŠáŒˆáˆ®á‰½\náŠ¥áŠ•á‹°áŒˆáŠ“ á‰¥á‹™ áŠ“á‰µ á‹ˆá‰…á‰µ á‰¦á‰³ áŠ“á‰¸á‹ áŠ¥áŠ•á‹²áˆáˆ á‰ áˆ­áŠ«á‰³ áŠ áˆáŠ• áŠ¥áŠ•áŒ‚ áŠ¥áˆµáŠ¨\náˆ›áˆˆá‰µ á‹¨áˆšáˆ†áŠ‘á‰µ áˆµáˆˆáˆ›áŠ“á‰¸á‹áˆ á‹áˆµáŒ¥ á‹­áˆ†áŠ“áˆ‰ áˆ²á‰£áˆ áŠ¨áˆ†áŠá‹ áˆµáˆˆá‹šáˆ áŠ¨áŠ áŠ•á‹µ\ná‹«áˆáˆ†áŠ áˆ³áˆˆ á‹¨áŠá‰ áˆ¨á‹áŠ• áŠ¨áŠ áŠ•á‹³áŠ•á‹µ á‰ áˆ›áŠ“á‰¸á‹áˆ á‰ áˆ™áˆ‰ á‹¨áˆ†áŠá‹ á‹«áˆ‰ á‰ áŠ¥áŠá‹šáˆ\ná‹ˆáˆ­ áˆ˜áˆ†áŠ“á‰¸á‹ áŠ¨áˆŒáˆŽá‰½ á‰ á‹‹áŠ“ áŠ áŠ•á‹²á‰µ á‹ˆá‹­áˆ\ná‰ áˆ‹á‹­ áŠ¥áŠ•á‹° á‰ áˆ›á‰€á‹µ áˆˆáˆŒáˆŽá‰½ á‰ áˆ†áŠ‘ á‰¢áˆ†áŠ•áˆ áŒŠá‹œáŠ“  á‹­áˆ†áŠ‘á‰ á‰³áˆ á‰ áˆ†áŠ áŠ áŠ•á‹±\náˆˆá‹šáˆ… áˆˆáˆ†áŠá‹ áˆˆáŠá‹šáˆ… áŠ¨á‹šáˆ… á‹¨áˆŒáˆ‹á‹áŠ• áˆ¶áˆµá‰°áŠ› áŠ áŠ•á‹³áŠ•á‹µ áˆˆáˆ›áŠ•áŠ›á‹áˆ á‹¨áˆ†áŠ áŠ¨áˆáˆˆá‰µ\ná‹¨áŠáŒˆáˆ© áˆ°áŠ£á‰µ áŠ áŠ•á‹°áŠ› áŠ¥áŠ•á‹²áˆ†áŠ• áŠ¥áŠ•á‹°áŠá‹šáˆ… áˆ›áŠ•áŠ›á‹áˆ áŠ«áˆáˆ†áŠ á‹¨áˆ†áŠ‘á‰µ  áŒ‹áˆ­ á‰¢á‹«áŠ•áˆµ\ná‹­áˆ…áŠ•áŠ•áˆ áŠ¥áŠá‹°áˆ†áŠ áŠ¥áŠá‹šáˆ…áŠ• á‹­áŠ¸á‹  á‹¨áˆ›áŠ“á‰¸á‹áˆ\ná‰ áˆ™áˆ‰áˆ á‹­áˆ…á‰½á‹ á‰ á‰°áˆˆá‹­áˆ áŠ áŠ•á‹±áŠ• á‹¨áˆšá‰½áˆˆá‹áŠ• á‰ áŠá‹šáˆ… áŠ¨áŠ¥áŠá‹šáˆ… á‰ áˆŒáˆ‹\ná‹¨á‹šáˆ áŠ¨áŠ¥áŠá‹šáˆ áˆˆá‹šáˆ á‰ áˆšáŒˆá‰£ áˆˆáŠ¥á‹«áŠ•á‹³áŠ•á‹± á‹¨áŠ áŠ•á‰€áŒ¹ á‹ˆá‹° á‹­áˆ…áˆ áˆµáˆˆáˆ†áŠ á‹ˆá‹­\náˆ›áŠ“á‰¸á‹áŠ•áˆ á‰°á‰¥áˆŽ áŠ¥áŠá‹šáˆ… áˆ˜áˆ†áŠ“á‰¸á‹áŠ• á‹¨áˆ†áŠá‰½áŠ• áŠ¨áŠ áˆµáˆ­ áˆ³á‹­áˆ†áŠ• áŠ¨á‹šá‹« á‹¨áˆˆá‹áˆ\ná‹¨áˆ›á‹­á‰ áˆáŒ¥ áŠ¥áŠ•á‹°áˆ†áŠáŠ“ áŠ¥áŠ•á‹²áˆ†áŠ‘  á‰ áˆšá‰½áˆ‰ á‰¥á‰» á‰¥áˆŽ áŠ¨áˆŒáˆ‹ á‹¨áˆŒáˆ‹á‰¸á‹áŠ•\náˆˆáˆ†áŠ á‰ áˆŒáˆŽá‰½ áˆáˆˆá‰±áŠ•áˆ á‰ á‰€áˆ­ á‹­áˆ… á‰ á‰³á‰½ áŠ áŠ•á‹°áˆ†áŠ á‰ áŠáˆ±\ná‹­áˆ…áŠ• á‹¨áˆŒáˆ‹ áŠ¥áŠ•á‹²áˆ… áŠ¨áˆ†áŠ á‹«áˆ‹á‰¸á‹ á‰ áŠá‹šáˆ á‰ áˆšáˆ á‹¨á‹šáˆ… á‹­áˆ…áŠ•áŠ‘\ná‰ áŠ¥áŠ•á‹°á‹šáˆ… á‰áŒ¥áˆ­ áˆ›áŠ“á‰¸á‹áˆ áˆ†áŠá‹ á‰£áˆ‰ á‰ á‹šáˆ… á‰ áˆµá‰°á‰€áˆ­ áˆ²áˆ†áŠ•áŠ“\ná‰ á‹šáˆ…áˆ áˆ˜áˆ†áŠ• áˆáŠ•áŒŠá‹œáˆ áŠ¥áŠá‹šáˆ…áˆ á‰ á‹šáˆ…áŠ“ á‹«áˆˆ áˆµáˆ\náˆ²áŠ–áˆ­ áŠ¨á‹šáˆ…áˆ áˆ˜áˆ†áŠ‘áŠ• á‰ áˆáŠ”á‰³á‹ á‹¨áˆ›á‹«áŠ•áˆµ áŠ¥áŠá‹šáˆ…áŠ‘ áˆ›áŠ•áˆ áŠ¨áŠá‹šáˆ\ná‹«áˆ‹á‰¸á‹áŠ• áŠ¥áŒ…áŒ áˆ²áˆ†áŠ‘ áˆˆáˆ†áŠ‘ áˆŠáˆ†áŠ•  áˆˆáˆ›áŠ“á‰¸á‹áˆ\n'.split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/am/tokenizer_exceptions.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/am/__init__.py----------------------------------------
A:spacy.lang.am.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.am.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS, TOKENIZER_EXCEPTIONS)
spacy.lang.am.__init__.Amharic(Language)
spacy.lang.am.__init__.AmharicDefaults(BaseDefaults)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/am/lex_attrs.py----------------------------------------
A:spacy.lang.am.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.am.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
A:spacy.lang.am.lex_attrs.text_lower->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').lower()
spacy.lang.am.lex_attrs.like_num(text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/am/punctuation.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/fa/examples.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/fa/stop_words.py----------------------------------------
A:spacy.lang.fa.stop_words.STOP_WORDS->set('\nÙˆ\nØ¯Ø±\nØ¨Ù‡\nØ§Ø²\nÚ©Ù‡\nØ§ÛŒÙ†\nØ±Ø§\nØ¨Ø§\nØ§Ø³Øª\nØ¨Ø±Ø§ÛŒ\nØ¢Ù†\nÛŒÚ©\nØ®ÙˆØ¯\nØªØ§\nÚ©Ø±Ø¯\nØ¨Ø±\nÙ‡Ù…\nÙ†ÛŒØ²\nÚ¯ÙØª\nÙ…ÛŒ\u200cØ´ÙˆØ¯\nÙˆÛŒ\nØ´Ø¯\nØ¯Ø§Ø±Ø¯\nÙ…Ø§\nØ§Ù…Ø§\nÛŒØ§\nØ´Ø¯Ù‡\nØ¨Ø§ÛŒØ¯\nÙ‡Ø±\nØ¢Ù†Ù‡Ø§\nØ¨ÙˆØ¯\nØ§Ùˆ\nØ¯ÛŒÚ¯Ø±\nØ¯Ùˆ\nÙ…ÙˆØ±Ø¯\nÙ…ÛŒ\u200cÚ©Ù†Ø¯\nØ´ÙˆØ¯\nÚ©Ù†Ø¯\nÙˆØ¬ÙˆØ¯\nØ¨ÛŒÙ†\nÙ¾ÛŒØ´\nØ´Ø¯Ù‡\u200cØ§Ø³Øª\nÙ¾Ø³\nÙ†Ø¸Ø±\nØ§Ú¯Ø±\nÙ‡Ù…Ù‡\nÛŒÚ©ÛŒ\nØ­Ø§Ù„\nÙ‡Ø³ØªÙ†Ø¯\nÙ…Ù†\nÚ©Ù†Ù†Ø¯\nÙ†ÛŒØ³Øª\nØ¨Ø§Ø´Ø¯\nÚ†Ù‡\nØ¨ÛŒ\nÙ…ÛŒ\nØ¨Ø®Ø´\nÙ…ÛŒ\u200cÚ©Ù†Ù†Ø¯\nÙ‡Ù…ÛŒÙ†\nØ§ÙØ²ÙˆØ¯\nÙ‡Ø§ÛŒÛŒ\nØ¯Ø§Ø±Ù†Ø¯\nØ±Ø§Ù‡\nÙ‡Ù…Ú†Ù†ÛŒÙ†\nØ±ÙˆÛŒ\nØ¯Ø§Ø¯\nØ¨ÛŒØ´ØªØ±\nØ¨Ø³ÛŒØ§Ø±\nØ³Ù‡\nØ¯Ø§Ø´Øª\nÚ†Ù†Ø¯\nØ³ÙˆÛŒ\nØªÙ†Ù‡Ø§\nÙ‡ÛŒÚ†\nÙ…ÛŒØ§Ù†\nØ§ÛŒÙ†Ú©Ù‡\nØ´Ø¯Ù†\nØ¨Ø¹Ø¯\nØ¬Ø¯ÛŒØ¯\nÙˆÙ„ÛŒ\nØ­ØªÛŒ\nÚ©Ø±Ø¯Ù†\nØ¨Ø±Ø®ÛŒ\nÚ©Ø±Ø¯Ù†Ø¯\nÙ…ÛŒ\u200cØ¯Ù‡Ø¯\nØ§ÙˆÙ„\nÙ†Ù‡\nÚ©Ø±Ø¯Ù‡\u200cØ§Ø³Øª\nÙ†Ø³Ø¨Øª\nØ¨ÛŒØ´\nØ´Ù…Ø§\nÚ†Ù†ÛŒÙ†\nØ·ÙˆØ±\nØ§ÙØ±Ø§Ø¯\nØªÙ…Ø§Ù…\nØ¯Ø±Ø¨Ø§Ø±Ù‡\nØ¨Ø§Ø±\nØ¨Ø³ÛŒØ§Ø±ÛŒ\nÙ…ÛŒ\u200cØªÙˆØ§Ù†Ø¯\nÚ©Ø±Ø¯Ù‡\nÚ†ÙˆÙ†\nÙ†Ø¯Ø§Ø±Ø¯\nØ¯ÙˆÙ…\nØ¨Ø²Ø±Ú¯\nØ·ÛŒ\nØ­Ø¯ÙˆØ¯\nÙ‡Ù…Ø§Ù†\nØ¨Ø¯ÙˆÙ†\nØ§Ù„Ø¨ØªÙ‡\nØ¢Ù†Ø§Ù†\nÙ…ÛŒ\u200cÚ¯ÙˆÛŒØ¯\nØ¯ÛŒÚ¯Ø±ÛŒ\nØ®ÙˆØ§Ù‡Ø¯\u200cØ´Ø¯\nÚ©Ù†ÛŒÙ…\nÙ‚Ø§Ø¨Ù„\nÛŒØ¹Ù†ÛŒ\nØ±Ø´Ø¯\nÙ…ÛŒ\u200cØªÙˆØ§Ù†\nÙˆØ§Ø±Ø¯\nÚ©Ù„\nÙˆÛŒÚ˜Ù‡\nÙ‚Ø¨Ù„\nØ¨Ø±Ø§Ø³Ø§Ø³\nÙ†ÛŒØ§Ø²\nÚ¯Ø°Ø§Ø±ÛŒ\nÙ‡Ù†ÙˆØ²\nÙ„Ø§Ø²Ù…\nØ³Ø§Ø²ÛŒ\nØ¨ÙˆØ¯Ù‡\u200cØ§Ø³Øª\nÚ†Ø±Ø§\nÙ…ÛŒ\u200cØ´ÙˆÙ†Ø¯\nÙˆÙ‚ØªÛŒ\nÚ¯Ø±ÙØª\nÚ©Ù…\nØ¬Ø§ÛŒ\nØ­Ø§Ù„ÛŒ\nØªØºÛŒÛŒØ±\nÙ¾ÛŒØ¯Ø§\nØ§Ú©Ù†ÙˆÙ†\nØªØ­Øª\nØ¨Ø§Ø¹Ø«\nÙ…Ø¯Øª\nÙÙ‚Ø·\nØ²ÛŒØ§Ø¯ÛŒ\nØªØ¹Ø¯Ø§Ø¯\nØ¢ÛŒØ§\nØ¨ÛŒØ§Ù†\nØ±Ùˆ\nØ´Ø¯Ù†Ø¯\nØ¹Ø¯Ù…\nÚ©Ø±Ø¯Ù‡\u200cØ§Ù†Ø¯\nØ¨ÙˆØ¯Ù†\nÙ†ÙˆØ¹\nØ¨Ù„Ú©Ù‡\nØ¬Ø§Ø±ÛŒ\nØ¯Ù‡Ø¯\nØ¨Ø±Ø§Ø¨Ø±\nÙ…Ù‡Ù…\nØ¨ÙˆØ¯Ù‡\nØ§Ø®ÛŒØ±\nÙ…Ø±Ø¨ÙˆØ·\nØ§Ù…Ø±\nØ²ÛŒØ±\nÚ¯ÛŒØ±ÛŒ\nØ´Ø§ÛŒØ¯\nØ®ØµÙˆØµ\nØ¢Ù‚Ø§ÛŒ\nØ§Ø«Ø±\nÚ©Ù†Ù†Ø¯Ù‡\nØ¨ÙˆØ¯Ù†Ø¯\nÙÚ©Ø±\nÚ©Ù†Ø§Ø±\nØ§ÙˆÙ„ÛŒÙ†\nØ³ÙˆÙ…\nØ³Ø§ÛŒØ±\nÚ©Ù†ÛŒØ¯\nØ¶Ù…Ù†\nÙ…Ø§Ù†Ù†Ø¯\nØ¨Ø§Ø²\nÙ…ÛŒ\u200cÚ¯ÛŒØ±Ø¯\nÙ…Ù…Ú©Ù†\nØ­Ù„\nØ¯Ø§Ø±Ø§ÛŒ\nÙ¾ÛŒ\nÙ…Ø«Ù„\nÙ…ÛŒ\u200cØ±Ø³Ø¯\nØ§Ø¬Ø±Ø§\nØ¯ÙˆØ±\nÙ…Ù†Ø¸ÙˆØ±\nÚ©Ø³ÛŒ\nÙ…ÙˆØ¬Ø¨\nØ·ÙˆÙ„\nØ§Ù…Ú©Ø§Ù†\nØ¢Ù†Ú†Ù‡\nØªØ¹ÛŒÛŒÙ†\nÚ¯ÙØªÙ‡\nØ´ÙˆÙ†Ø¯\nØ¬Ù…Ø¹\nØ®ÛŒÙ„ÛŒ\nØ¹Ù„Ø§ÙˆÙ‡\nÚ¯ÙˆÙ†Ù‡\nØªØ§Ú©Ù†ÙˆÙ†\nØ±Ø³ÛŒØ¯\nØ³Ø§Ù„Ù‡\nÚ¯Ø±ÙØªÙ‡\nØ´Ø¯Ù‡\u200cØ§Ù†Ø¯\nØ¹Ù„Øª\nÚ†Ù‡Ø§Ø±\nØ¯Ø§Ø´ØªÙ‡\u200cØ¨Ø§Ø´Ø¯\nØ®ÙˆØ§Ù‡Ø¯\u200cØ¨ÙˆØ¯\nØ·Ø±Ù\nØªÙ‡ÛŒÙ‡\nØªØ¨Ø¯ÛŒÙ„\nÙ…Ù†Ø§Ø³Ø¨\nØ²ÛŒØ±Ø§\nÙ…Ø´Ø®Øµ\nÙ…ÛŒ\u200cØªÙˆØ§Ù†Ù†Ø¯\nÙ†Ø²Ø¯ÛŒÚ©\nØ¬Ø±ÛŒØ§Ù†\nØ±ÙˆÙ†Ø¯\nØ¨Ù†Ø§Ø¨Ø±Ø§ÛŒÙ†\nÙ…ÛŒ\u200cØ¯Ù‡Ù†Ø¯\nÛŒØ§ÙØª\nÙ†Ø®Ø³ØªÛŒÙ†\nØ¨Ø§Ù„Ø§\nÙ¾Ù†Ø¬\nØ±ÛŒØ²ÛŒ\nØ¹Ø§Ù„ÛŒ\nÚ†ÛŒØ²ÛŒ\nÙ†Ø®Ø³Øª\nØ¨ÛŒØ´ØªØ±ÛŒ\nØªØ±ØªÛŒØ¨\nØ´Ø¯Ù‡\u200cØ¨ÙˆØ¯\nØ®Ø§Øµ\nØ®ÙˆØ¨ÛŒ\nØ®ÙˆØ¨\nØ´Ø±ÙˆØ¹\nÙØ±Ø¯\nÚ©Ø§Ù…Ù„\nØºÛŒØ±\nÙ…ÛŒ\u200cØ±ÙˆØ¯\nØ¯Ù‡Ù†Ø¯\nØ¢Ø®Ø±ÛŒÙ†\nØ¯Ø§Ø¯Ù†\nØ¬Ø¯ÛŒ\nØ¨Ù‡ØªØ±ÛŒÙ†\nØ´Ø§Ù…Ù„\nÚ¯ÛŒØ±Ø¯\nØ¨Ø®Ø´ÛŒ\nØ¨Ø§Ø´Ù†Ø¯\nØªÙ…Ø§Ù…ÛŒ\nØ¨Ù‡ØªØ±\nØ¯Ø§Ø¯Ù‡\u200cØ§Ø³Øª\nØ­Ø¯\nÙ†Ø¨ÙˆØ¯\nÚ©Ø³Ø§Ù†ÛŒ\nÙ…ÛŒ\u200cÚ©Ø±Ø¯\nØ¯Ø§Ø±ÛŒÙ…\nØ¹Ù„ÛŒÙ‡\nÙ…ÛŒ\u200cØ¨Ø§Ø´Ø¯\nØ¯Ø§Ù†Ø³Øª\nÙ†Ø§Ø´ÛŒ\nØ¯Ø§Ø´ØªÙ†Ø¯\nØ¯Ù‡Ù‡\nÙ…ÛŒ\u200cØ´Ø¯\nØ§ÛŒØ´Ø§Ù†\nØ¢Ù†Ø¬Ø§\nÚ¯Ø±ÙØªÙ‡\u200cØ§Ø³Øª\nØ¯Ú†Ø§Ø±\nÙ…ÛŒ\u200cØ¢ÛŒØ¯\nÙ„Ø­Ø§Ø¸\nØ¢Ù†Ú©Ù‡\nØ¯Ø§Ø¯Ù‡\nØ¨Ø¹Ø¶ÛŒ\nÙ‡Ø³ØªÛŒÙ…\nØ§Ù†Ø¯\nØ¨Ø±Ø¯Ø§Ø±ÛŒ\nÙ†Ø¨Ø§ÛŒØ¯\nÙ…ÛŒ\u200cÚ©Ù†ÛŒÙ…\nÙ†Ø´Ø³Øª\nØ³Ù‡Ù…\nÙ‡Ù…ÛŒØ´Ù‡\nØ¢Ù…Ø¯\nØ§Ø´\nÙˆÚ¯Ùˆ\nÙ…ÛŒ\u200cÚ©Ù†Ù…\nØ­Ø¯Ø§Ù‚Ù„\nØ·Ø¨Ù‚\nØ¬Ø§\nØ®ÙˆØ§Ù‡Ø¯\u200cÚ©Ø±Ø¯\nÙ†ÙˆØ¹ÛŒ\nÚ†Ú¯ÙˆÙ†Ù‡\nØ±ÙØª\nÙ‡Ù†Ú¯Ø§Ù…\nÙÙˆÙ‚\nØ±ÙˆØ´\nÙ†Ø¯Ø§Ø±Ù†Ø¯\nØ³Ø¹ÛŒ\nØ¨Ù†Ø¯ÛŒ\nØ´Ù…Ø§Ø±\nÚ©Ù„ÛŒ\nÚ©Ø§ÙÛŒ\nÙ…ÙˆØ§Ø¬Ù‡\nÙ‡Ù…Ú†Ù†Ø§Ù†\nØ²ÛŒØ§Ø¯\nØ³Ù…Øª\nÚ©ÙˆÚ†Ú©\nØ¯Ø§Ø´ØªÙ‡\u200cØ§Ø³Øª\nÚ†ÛŒØ²\nÙ¾Ø´Øª\nØ¢ÙˆØ±Ø¯\nØ­Ø§Ù„Ø§\nØ±ÙˆØ¨Ù‡\nØ³Ø§Ù„\u200cÙ‡Ø§ÛŒ\nØ¯Ø§Ø¯Ù†Ø¯\nÙ…ÛŒ\u200cÚ©Ø±Ø¯Ù†Ø¯\nØ¹Ù‡Ø¯Ù‡\nÙ†ÛŒÙ…Ù‡\nØ¬Ø§ÛŒÛŒ\nØ¯ÛŒÚ¯Ø±Ø§Ù†\nØ³ÛŒ\nØ¨Ø±ÙˆØ²\nÛŒÚ©Ø¯ÛŒÚ¯Ø±\nØ¢Ù…Ø¯Ù‡\u200cØ§Ø³Øª\nØ¬Ø²\nÚ©Ù†Ù…\nØ³Ù¾Ø³\nÚ©Ù†Ù†Ø¯Ú¯Ø§Ù†\nØ®ÙˆØ¯Ø´\nÙ‡Ù…ÙˆØ§Ø±Ù‡\nÛŒØ§ÙØªÙ‡\nØ´Ø§Ù†\nØµØ±Ù\nÙ†Ù…ÛŒ\u200cØ´ÙˆØ¯\nØ±Ø³ÛŒØ¯Ù†\nÚ†Ù‡Ø§Ø±Ù…\nÛŒØ§Ø¨Ø¯\nÙ…ØªØ±\nØ³Ø§Ø²\nØ¯Ø§Ø´ØªÙ‡\nÚ©Ø±Ø¯Ù‡\u200cØ¨ÙˆØ¯\nØ¨Ø§Ø±Ù‡\nÙ†Ø­ÙˆÙ‡\nÚ©Ø±Ø¯Ù…\nØªÙˆ\nØ´Ø®ØµÛŒ\nØ¯Ø§Ø´ØªÙ‡\u200cØ¨Ø§Ø´Ù†Ø¯\nÙ…Ø­Ø³ÙˆØ¨\nÙ¾Ø®Ø´\nÚ©Ù…ÛŒ\nÙ…ØªÙØ§ÙˆØª\nØ³Ø±Ø§Ø³Ø±\nÚ©Ø§Ù…Ù„Ø§\nØ¯Ø§Ø´ØªÙ†\nÙ†Ø¸ÛŒØ±\nØ¢Ù…Ø¯Ù‡\nÚ¯Ø±ÙˆÙ‡ÛŒ\nÙØ±Ø¯ÛŒ\nØ¹\nÙ‡Ù…Ú†ÙˆÙ†\nØ®Ø·Ø±\nØ®ÙˆÛŒØ´\nÚ©Ø¯Ø§Ù…\nØ¯Ø³ØªÙ‡\nØ³Ø¨Ø¨\nØ¹ÛŒÙ†\nØ¢ÙˆØ±ÛŒ\nÙ…ØªØ§Ø³ÙØ§Ù†Ù‡\nØ¨ÛŒØ±ÙˆÙ†\nØ¯Ø§Ø±\nØ§Ø¨ØªØ¯Ø§\nØ´Ø´\nØ§ÙØ±Ø§Ø¯ÛŒ\nÙ…ÛŒ\u200cÚ¯ÙˆÛŒÙ†Ø¯\nØ³Ø§Ù„Ù‡Ø§ÛŒ\nØ¯Ø±ÙˆÙ†\nÙ†ÛŒØ³ØªÙ†Ø¯\nÛŒØ§ÙØªÙ‡\u200cØ§Ø³Øª\nÙ¾Ø±\nØ®Ø§Ø·Ø±Ù†Ø´Ø§Ù†\nÚ¯Ø§Ù‡\nØ¬Ù…Ø¹ÛŒ\nØ§ØºÙ„Ø¨\nØ¯ÙˆØ¨Ø§Ø±Ù‡\nÙ…ÛŒ\u200cÛŒØ§Ø¨Ø¯\nÙ„Ø°Ø§\nØ²Ø§Ø¯Ù‡\nÚ¯Ø±Ø¯Ø¯\nØ§ÛŒÙ†Ø¬Ø§'.split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/fa/generate_verbs_exc.py----------------------------------------
A:spacy.lang.fa.generate_verbs_exc.verb_roots->'\n#Ù‡Ø³Øª\nØ¢Ø®Øª#Ø¢Ù‡Ù†Ø¬\nØ¢Ø±Ø§Ø³Øª#Ø¢Ø±Ø§\nØ¢Ø±Ø§Ù…Ø§Ù†Ø¯#Ø¢Ø±Ø§Ù…Ø§Ù†\nØ¢Ø±Ø§Ù…ÛŒØ¯#Ø¢Ø±Ø§Ù…\nØ¢Ø±Ù…ÛŒØ¯#Ø¢Ø±Ø§Ù…\nØ¢Ø²Ø±Ø¯#Ø¢Ø²Ø§Ø±\nØ¢Ø²Ù…ÙˆØ¯#Ø¢Ø²Ù…Ø§\nØ¢Ø³ÙˆØ¯#Ø¢Ø³Ø§\nØ¢Ø´Ø§Ù…ÛŒØ¯#Ø¢Ø´Ø§Ù…\nØ¢Ø´ÙØª#Ø¢Ø´ÙˆØ¨\nØ¢Ø´ÙˆØ¨ÛŒØ¯#Ø¢Ø´ÙˆØ¨\nØ¢ØºØ§Ø²ÛŒØ¯#Ø¢ØºØ§Ø²\nØ¢ØºØ´Øª#Ø¢Ù…ÛŒØ²\nØ¢ÙØ±ÛŒØ¯#Ø¢ÙØ±ÛŒÙ†\nØ¢Ù„ÙˆØ¯#Ø¢Ù„Ø§\nØ¢Ù…Ø¯#Ø¢\nØ¢Ù…Ø±Ø²ÛŒØ¯#Ø¢Ù…Ø±Ø²\nØ¢Ù…ÙˆØ®Øª#Ø¢Ù…ÙˆØ²\nØ¢Ù…ÙˆØ²Ø§Ù†Ø¯#Ø¢Ù…ÙˆØ²Ø§Ù†\nØ¢Ù…ÛŒØ®Øª#Ø¢Ù…ÛŒØ²\nØ¢ÙˆØ±Ø¯#Ø¢Ø±\nØ¢ÙˆØ±Ø¯#Ø¢ÙˆØ±\nØ¢ÙˆÛŒØ®Øª#Ø¢ÙˆÛŒØ²\nØ¢Ú©Ù†Ø¯#Ø¢Ú©Ù†\nØ¢Ú¯Ø§Ù‡Ø§Ù†ÛŒØ¯#Ø¢Ú¯Ø§Ù‡Ø§Ù†\nØ§Ø±Ø²ÛŒØ¯#Ø§Ø±Ø²\nØ§ÙØªØ§Ø¯#Ø§ÙØª\nØ§ÙØ±Ø§Ø®Øª#Ø§ÙØ±Ø§Ø²\nØ§ÙØ±Ø§Ø´Øª#Ø§ÙØ±Ø§Ø²\nØ§ÙØ±ÙˆØ®Øª#Ø§ÙØ±ÙˆØ²\nØ§ÙØ±ÙˆØ²ÛŒØ¯#Ø§ÙØ±ÙˆØ²\nØ§ÙØ²ÙˆØ¯#Ø§ÙØ²Ø§\nØ§ÙØ³Ø±Ø¯#Ø§ÙØ³Ø±\nØ§ÙØ´Ø§Ù†Ø¯#Ø§ÙØ´Ø§Ù†\nØ§ÙÚ©Ù†Ø¯#Ø§ÙÚ©Ù†\nØ§ÙÚ¯Ù†Ø¯#Ø§ÙÚ¯Ù†\nØ§Ù†Ø¨Ø§Ø´Øª#Ø§Ù†Ø¨Ø§Ø±\nØ§Ù†Ø¬Ø§Ù…ÛŒØ¯#Ø§Ù†Ø¬Ø§Ù…\nØ§Ù†Ø¯Ø§Ø®Øª#Ø§Ù†Ø¯Ø§Ø²\nØ§Ù†Ø¯ÙˆØ®Øª#Ø§Ù†Ø¯ÙˆØ²\nØ§Ù†Ø¯ÙˆØ¯#Ø§Ù†Ø¯Ø§\nØ§Ù†Ø¯ÛŒØ´ÛŒØ¯#Ø§Ù†Ø¯ÛŒØ´\nØ§Ù†Ú¯Ø§Ø´Øª#Ø§Ù†Ú¯Ø§Ø±\nØ§Ù†Ú¯ÛŒØ®Øª#Ø§Ù†Ú¯ÛŒØ²\nØ§Ù†Ú¯ÛŒØ²Ø§Ù†Ø¯#Ø§Ù†Ú¯ÛŒØ²Ø§Ù†\nØ§ÛŒØ³ØªØ§Ø¯#Ø§ÛŒØ³Øª\nØ§ÛŒØ³ØªØ§Ù†Ø¯#Ø§ÛŒØ³ØªØ§Ù†\nØ¨Ø§Ø®Øª#Ø¨Ø§Ø²\nØ¨Ø§Ø±Ø§Ù†Ø¯#Ø¨Ø§Ø±Ø§Ù†\nØ¨Ø§Ø±Ú¯Ø°Ø§Ø´Øª#Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±\nØ¨Ø§Ø±ÛŒØ¯#Ø¨Ø§Ø±\nØ¨Ø§Ø²#Ø¨Ø§Ø²Ø®ÙˆØ§Ù‡\nØ¨Ø§Ø²Ø¢ÙØ±ÛŒØ¯#Ø¨Ø§Ø²Ø¢ÙØ±ÛŒÙ†\nØ¨Ø§Ø²Ø¢Ù…Ø¯#Ø¨Ø§Ø²Ø¢\nØ¨Ø§Ø²Ø¢Ù…ÙˆØ®Øª#Ø¨Ø§Ø²Ø¢Ù…ÙˆØ²\nØ¨Ø§Ø²Ø¢ÙˆØ±Ø¯#Ø¨Ø§Ø²Ø¢ÙˆØ±\nØ¨Ø§Ø²Ø§ÛŒØ³ØªØ§Ø¯#Ø¨Ø§Ø²Ø§ÛŒØ³Øª\nØ¨Ø§Ø²ØªØ§Ø¨ÛŒØ¯#Ø¨Ø§Ø²ØªØ§Ø¨\nØ¨Ø§Ø²Ø¬Ø³Øª#Ø¨Ø§Ø²Ø¬Ùˆ\nØ¨Ø§Ø²Ø®ÙˆØ§Ù†Ø¯#Ø¨Ø§Ø²Ø®ÙˆØ§Ù†\nØ¨Ø§Ø²Ø®ÙˆØ±Ø§Ù†Ø¯#Ø¨Ø§Ø²Ø®ÙˆØ±Ø§Ù†\nØ¨Ø§Ø²Ø¯Ø§Ø¯#Ø¨Ø§Ø²Ø¯Ù‡\nØ¨Ø§Ø²Ø¯Ø§Ø´Øª#Ø¨Ø§Ø²Ø¯Ø§Ø±\nØ¨Ø§Ø²Ø±Ø³Ø§Ù†Ø¯#Ø¨Ø§Ø²Ø±Ø³Ø§Ù†\nØ¨Ø§Ø²Ø±Ø³Ø§Ù†ÛŒØ¯#Ø¨Ø§Ø²Ø±Ø³Ø§Ù†\nØ¨Ø§Ø²Ø²Ø¯#Ø¨Ø§Ø²Ø²Ù†\nØ¨Ø§Ø²Ø³ØªØ§Ù†Ø¯#Ø¨Ø§Ø²Ø³ØªØ§Ù†\nØ¨Ø§Ø²Ø´Ù…Ø§Ø±Ø¯#Ø¨Ø§Ø²Ø´Ù…Ø§Ø±\nØ¨Ø§Ø²Ø´Ù…Ø±Ø¯#Ø¨Ø§Ø²Ø´Ù…Ø§Ø±\nØ¨Ø§Ø²Ø´Ù…Ø±Ø¯#Ø¨Ø§Ø²Ø´Ù…Ø±\nØ¨Ø§Ø²Ø´Ù†Ø§Ø®Øª#Ø¨Ø§Ø²Ø´Ù†Ø§Ø³\nØ¨Ø§Ø²Ø´Ù†Ø§Ø³Ø§Ù†Ø¯#Ø¨Ø§Ø²Ø´Ù†Ø§Ø³Ø§Ù†\nØ¨Ø§Ø²ÙØ±Ø³ØªØ§Ø¯#Ø¨Ø§Ø²ÙØ±Ø³Øª\nØ¨Ø§Ø²Ù…Ø§Ù†Ø¯#Ø¨Ø§Ø²Ù…Ø§Ù†\nØ¨Ø§Ø²Ù†Ø´Ø³Øª#Ø¨Ø§Ø²Ù†Ø´ÛŒÙ†\nØ¨Ø§Ø²Ù†Ù…Ø§ÛŒØ§Ù†Ø¯#Ø¨Ø§Ø²Ù†Ù…Ø§ÛŒØ§Ù†\nØ¨Ø§Ø²Ù†Ù‡Ø§Ø¯#Ø¨Ø§Ø²Ù†Ù‡\nØ¨Ø§Ø²Ù†Ú¯Ø±ÛŒØ³Øª#Ø¨Ø§Ø²Ù†Ú¯Ø±\nØ¨Ø§Ø²Ù¾Ø±Ø³ÛŒØ¯#Ø¨Ø§Ø²Ù¾Ø±Ø³\nØ¨Ø§Ø²Ú¯Ø°Ø§Ø±Ø¯#Ø¨Ø§Ø²Ú¯Ø°Ø§Ø±\nØ¨Ø§Ø²Ú¯Ø°Ø§Ø´Øª#Ø¨Ø§Ø²Ú¯Ø°Ø§Ø±\nØ¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†Ø¯#Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†\nØ¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†ÛŒØ¯#Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†\nØ¨Ø§Ø²Ú¯Ø±Ø¯ÛŒØ¯#Ø¨Ø§Ø²Ú¯Ø±Ø¯\nØ¨Ø§Ø²Ú¯Ø±ÙØª#Ø¨Ø§Ø²Ú¯ÛŒØ±\nØ¨Ø§Ø²Ú¯Ø´Øª#Ø¨Ø§Ø²Ú¯Ø±Ø¯\nØ¨Ø§Ø²Ú¯Ø´ÙˆØ¯#Ø¨Ø§Ø²Ú¯Ø´Ø§\nØ¨Ø§Ø²Ú¯ÙØª#Ø¨Ø§Ø²Ú¯Ùˆ\nØ¨Ø§Ø²ÛŒØ§ÙØª#Ø¨Ø§Ø²ÛŒØ§Ø¨\nØ¨Ø§ÙØª#Ø¨Ø§Ù\nØ¨Ø§Ù„ÛŒØ¯#Ø¨Ø§Ù„\nØ¨Ø§ÙˆØ±Ø§Ù†Ø¯#Ø¨Ø§ÙˆØ±Ø§Ù†\nØ¨Ø§ÛŒØ³Øª#Ø¨Ø§ÛŒØ¯\nØ¨Ø®Ø´ÙˆØ¯#Ø¨Ø®Ø´\nØ¨Ø®Ø´ÙˆØ¯#Ø¨Ø®Ø´Ø§\nØ¨Ø®Ø´ÛŒØ¯#Ø¨Ø®Ø´\nØ¨Ø±#Ø¨Ø±Ø®ÙˆØ§Ù‡\nØ¨Ø±Ø¢Ø´ÙØª#Ø¨Ø±Ø¢Ø´ÙˆØ¨\nØ¨Ø±Ø¢Ù…Ø¯#Ø¨Ø±Ø¢\nØ¨Ø±Ø¢ÙˆØ±Ø¯#Ø¨Ø±Ø¢ÙˆØ±\nØ¨Ø±Ø§Ø²ÛŒØ¯#Ø¨Ø±Ø§Ø²\nØ¨Ø±Ø§ÙØªØ§Ø¯#Ø¨Ø±Ø§ÙØª\nØ¨Ø±Ø§ÙØ±Ø§Ø®Øª#Ø¨Ø±Ø§ÙØ±Ø§Ø²\nØ¨Ø±Ø§ÙØ±Ø§Ø´Øª#Ø¨Ø±Ø§ÙØ±Ø§Ø²\nØ¨Ø±Ø§ÙØ±ÙˆØ®Øª#Ø¨Ø±Ø§ÙØ±ÙˆØ²\nØ¨Ø±Ø§ÙØ´Ø§Ù†Ø¯#Ø¨Ø±Ø§ÙØ´Ø§Ù†\nØ¨Ø±Ø§ÙÚ©Ù†Ø¯#Ø¨Ø±Ø§ÙÚ©Ù†\nØ¨Ø±Ø§Ù†Ø¯#Ø¨Ø±Ø§Ù†\nØ¨Ø±Ø§Ù†Ø¯Ø§Ø®Øª#Ø¨Ø±Ø§Ù†Ø¯Ø§Ø²\nØ¨Ø±Ø§Ù†Ú¯ÛŒØ®Øª#Ø¨Ø±Ø§Ù†Ú¯ÛŒØ²\nØ¨Ø±Ø¨Ø³Øª#Ø¨Ø±Ø¨Ù†Ø¯\nØ¨Ø±ØªØ§Ø¨Ø§Ù†Ø¯#Ø¨Ø±ØªØ§Ø¨Ø§Ù†\nØ¨Ø±ØªØ§Ø¨ÛŒØ¯#Ø¨Ø±ØªØ§Ø¨\nØ¨Ø±ØªØ§ÙØª#Ø¨Ø±ØªØ§Ø¨\nØ¨Ø±ØªÙ†ÛŒØ¯#Ø¨Ø±ØªÙ†\nØ¨Ø±Ø¬Ù‡ÛŒØ¯#Ø¨Ø±Ø¬Ù‡\nØ¨Ø±Ø®Ø§Ø³Øª#Ø¨Ø±Ø®ÛŒØ²\nØ¨Ø±Ø®ÙˆØ±Ø¯#Ø¨Ø±Ø®ÙˆØ±\nØ¨Ø±Ø¯#Ø¨Ø±\nØ¨Ø±Ø¯Ø§Ø´Øª#Ø¨Ø±Ø¯Ø§Ø±\nØ¨Ø±Ø¯Ù…ÛŒØ¯#Ø¨Ø±Ø¯Ù…\nØ¨Ø±Ø²Ø¯#Ø¨Ø±Ø²Ù†\nØ¨Ø±Ø´Ø¯#Ø¨Ø±Ø´Ùˆ\nØ¨Ø±Ø´Ù…Ø§Ø±Ø¯#Ø¨Ø±Ø´Ù…Ø§Ø±\nØ¨Ø±Ø´Ù…Ø±Ø¯#Ø¨Ø±Ø´Ù…Ø§Ø±\nØ¨Ø±Ø´Ù…Ø±Ø¯#Ø¨Ø±Ø´Ù…Ø±\nØ¨Ø±Ù†Ø´Ø§Ù†Ø¯#Ø¨Ø±Ù†Ø´Ø§Ù†\nØ¨Ø±Ù†Ø´Ø§Ù†ÛŒØ¯#Ø¨Ø±Ù†Ø´Ø§Ù†\nØ¨Ø±Ù†Ø´Ø³Øª#Ø¨Ø±Ù†Ø´ÛŒÙ†\nØ¨Ø±Ù†Ù‡Ø§Ø¯#Ø¨Ø±Ù†Ù‡\nØ¨Ø±Ú†ÛŒØ¯#Ø¨Ø±Ú†ÛŒÙ†\nØ¨Ø±Ú©Ø±Ø¯#Ø¨Ø±Ú©Ù†\nØ¨Ø±Ú©Ø´ÛŒØ¯#Ø¨Ø±Ú©Ø´\nØ¨Ø±Ú©Ù†Ø¯#Ø¨Ø±Ú©Ù†\nØ¨Ø±Ú¯Ø°Ø´Øª#Ø¨Ø±Ú¯Ø°Ø±\nØ¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯#Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†\nØ¨Ø±Ú¯Ø±Ø¯Ø§Ù†ÛŒØ¯#Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†\nØ¨Ø±Ú¯Ø±Ø¯ÛŒØ¯#Ø¨Ø±Ú¯Ø±Ø¯\nØ¨Ø±Ú¯Ø±ÙØª#Ø¨Ø±Ú¯ÛŒØ±\nØ¨Ø±Ú¯Ø²ÛŒØ¯#Ø¨Ø±Ú¯Ø²ÛŒÙ†\nØ¨Ø±Ú¯Ø´Øª#Ø¨Ø±Ú¯Ø±Ø¯\nØ¨Ø±Ú¯Ø´ÙˆØ¯#Ø¨Ø±Ú¯Ø´Ø§\nØ¨Ø±Ú¯Ù…Ø§Ø±Ø¯#Ø¨Ø±Ú¯Ù…Ø§Ø±\nØ¨Ø±Ú¯Ù…Ø§Ø±ÛŒØ¯#Ø¨Ø±Ú¯Ù…Ø§Ø±\nØ¨Ø±Ú¯Ù…Ø§Ø´Øª#Ø¨Ø±Ú¯Ù…Ø§Ø±\nØ¨Ø±ÛŒØ¯#Ø¨Ø±\nØ¨Ø³Øª#Ø¨Ù†Ø¯\nØ¨Ù„Ø¹ÛŒØ¯#Ø¨Ù„Ø¹\nØ¨ÙˆØ¯#Ø¨Ø§Ø´\nØ¨ÙˆØ³ÛŒØ¯#Ø¨ÙˆØ³\nØ¨ÙˆÛŒÛŒØ¯#Ø¨Ùˆ\nØ¨ÛŒØ®Øª#Ø¨ÛŒØ²\nØ¨ÛŒØ®Øª#Ø¨ÙˆØ²\nØªØ§Ø¨Ø§Ù†Ø¯#ØªØ§Ø¨Ø§Ù†\nØªØ§Ø¨ÛŒØ¯#ØªØ§Ø¨\nØªØ§Ø®Øª#ØªØ§Ø²\nØªØ§Ø±Ø§Ù†Ø¯#ØªØ§Ø±Ø§Ù†\nØªØ§Ø²Ø§Ù†Ø¯#ØªØ§Ø²Ø§Ù†\nØªØ§Ø²ÛŒØ¯#ØªØ§Ø²\nØªØ§ÙØª#ØªØ§Ø¨\nØªØ±Ø§Ø¯ÛŒØ³ÛŒØ¯#ØªØ±Ø§Ø¯ÛŒØ³\nØªØ±Ø§Ø´Ø§Ù†Ø¯#ØªØ±Ø§Ø´Ø§Ù†\nØªØ±Ø§Ø´ÛŒØ¯#ØªØ±Ø§Ø´\nØªØ±Ø§ÙˆÛŒØ¯#ØªØ±Ø§Ùˆ\nØªØ±Ø³Ø§Ù†Ø¯#ØªØ±Ø³Ø§Ù†\nØªØ±Ø³ÛŒØ¯#ØªØ±Ø³\nØªØ±Ø´Ø§Ù†Ø¯#ØªØ±Ø´Ø§Ù†\nØªØ±Ø´ÛŒØ¯#ØªØ±Ø´\nØªØ±Ú©Ø§Ù†Ø¯#ØªØ±Ú©Ø§Ù†\nØªØ±Ú©ÛŒØ¯#ØªØ±Ú©\nØªÙØªÛŒØ¯#ØªÙØª\nØªÙ…Ø±Ú¯ÛŒØ¯#ØªÙ…Ø±Ú¯\nØªÙ†ÛŒØ¯#ØªÙ†\nØªÙˆØ§Ù†Ø³Øª#ØªÙˆØ§Ù†\nØªÙˆÙÛŒØ¯#ØªÙˆÙ\nØªÙ¾Ø§Ù†Ø¯#ØªÙ¾Ø§Ù†\nØªÙ¾ÛŒØ¯#ØªÙ¾\nØªÚ©Ø§Ù†Ø¯#ØªÚ©Ø§Ù†\nØªÚ©Ø§Ù†ÛŒØ¯#ØªÚ©Ø§Ù†\nØ¬Ø³Øª#Ø¬Ù‡\nØ¬Ø³Øª#Ø¬Ùˆ\nØ¬Ù†Ø¨Ø§Ù†Ø¯#Ø¬Ù†Ø¨Ø§Ù†\nØ¬Ù†Ø¨ÛŒØ¯#Ø¬Ù†Ø¨\nØ¬Ù†Ú¯ÛŒØ¯#Ø¬Ù†Ú¯\nØ¬Ù‡Ø§Ù†Ø¯#Ø¬Ù‡Ø§Ù†\nØ¬Ù‡ÛŒØ¯#Ø¬Ù‡\nØ¬ÙˆØ´Ø§Ù†Ø¯#Ø¬ÙˆØ´Ø§Ù†\nØ¬ÙˆØ´Ø§Ù†ÛŒØ¯#Ø¬ÙˆØ´Ø§Ù†\nØ¬ÙˆØ´ÛŒØ¯#Ø¬ÙˆØ´\nØ¬ÙˆÙŠØ¯#Ø¬Ùˆ\nØ¬ÙˆÛŒØ¯#Ø¬Ùˆ\nØ®Ø§Ø±Ø§Ù†Ø¯#Ø®Ø§Ø±Ø§Ù†\nØ®Ø§Ø±ÛŒØ¯#Ø®Ø§Ø±\nØ®Ø§Ø³Øª#Ø®ÛŒØ²\nØ®Ø§ÛŒÛŒØ¯#Ø®Ø§\nØ®Ø±Ø§Ø´Ø§Ù†Ø¯#Ø®Ø±Ø§Ø´Ø§Ù†\nØ®Ø±Ø§Ø´ÛŒØ¯#Ø®Ø±Ø§Ø´\nØ®Ø±Ø§Ù…ÛŒØ¯#Ø®Ø±Ø§Ù…\nØ®Ø±ÙˆØ´ÛŒØ¯#Ø®Ø±ÙˆØ´\nØ®Ø±ÛŒØ¯#Ø®Ø±\nØ®Ø²ÛŒØ¯#Ø®Ø²\nØ®Ø³Ø¨ÛŒØ¯#Ø®Ø³Ø¨\nØ®Ø´Ú©Ø§Ù†Ø¯#Ø®Ø´Ú©Ø§Ù†\nØ®Ø´Ú©ÛŒØ¯#Ø®Ø´Ú©\nØ®ÙØª#Ø®ÙˆØ§Ø¨\nØ®Ù„ÛŒØ¯#Ø®Ù„\nØ®Ù…Ø§Ù†Ø¯#Ø®Ù…Ø§Ù†\nØ®Ù…ÛŒØ¯#Ø®Ù…\nØ®Ù†Ø¯Ø§Ù†Ø¯#Ø®Ù†Ø¯Ø§Ù†\nØ®Ù†Ø¯Ø§Ù†ÛŒØ¯#Ø®Ù†Ø¯Ø§Ù†\nØ®Ù†Ø¯ÛŒØ¯#Ø®Ù†Ø¯\nØ®ÙˆØ§Ø¨Ø§Ù†Ø¯#Ø®ÙˆØ§Ø¨Ø§Ù†\nØ®ÙˆØ§Ø¨Ø§Ù†ÛŒØ¯#Ø®ÙˆØ§Ø¨Ø§Ù†\nØ®ÙˆØ§Ø¨ÛŒØ¯#Ø®ÙˆØ§Ø¨\nØ®ÙˆØ§Ø³Øª#Ø®ÙˆØ§Ù‡\nØ®ÙˆØ§Ø³Øª#Ø®ÛŒØ²\nØ®ÙˆØ§Ù†Ø¯#Ø®ÙˆØ§Ù†\nØ®ÙˆØ±Ø§Ù†Ø¯#Ø®ÙˆØ±Ø§Ù†\nØ®ÙˆØ±Ø¯#Ø®ÙˆØ±\nØ®ÛŒØ²Ø§Ù†Ø¯#Ø®ÛŒØ²Ø§Ù†\nØ®ÛŒØ³Ø§Ù†Ø¯#Ø®ÛŒØ³Ø§Ù†\nØ¯Ø§Ø¯#Ø¯Ù‡\nØ¯Ø§Ø´Øª#Ø¯Ø§Ø±\nØ¯Ø§Ù†Ø³Øª#Ø¯Ø§Ù†\nØ¯Ø±#Ø¯Ø±Ø®ÙˆØ§Ù‡\nØ¯Ø±Ø¢Ù…Ø¯#Ø¯Ø±Ø¢\nØ¯Ø±Ø¢Ù…ÛŒØ®Øª#Ø¯Ø±Ø¢Ù…ÛŒØ²\nØ¯Ø±Ø¢ÙˆØ±Ø¯#Ø¯Ø±Ø¢ÙˆØ±\nØ¯Ø±Ø¢ÙˆÛŒØ®Øª#Ø¯Ø±Ø¢ÙˆÛŒØ²\nØ¯Ø±Ø§ÙØªØ§Ø¯#Ø¯Ø±Ø§ÙØª\nØ¯Ø±Ø§ÙÚ©Ù†Ø¯#Ø¯Ø±Ø§ÙÚ©Ù†\nØ¯Ø±Ø§Ù†Ø¯Ø§Ø®Øª#Ø¯Ø±Ø§Ù†Ø¯Ø§Ø²\nØ¯Ø±Ø§Ù†ÛŒØ¯#Ø¯Ø±Ø§Ù†\nØ¯Ø±Ø¨Ø±Ø¯#Ø¯Ø±Ø¨Ø±\nØ¯Ø±Ø¨Ø±Ú¯Ø±ÙØª#Ø¯Ø±Ø¨Ø±Ú¯ÛŒØ±\nØ¯Ø±Ø®Ø´Ø§Ù†Ø¯#Ø¯Ø±Ø®Ø´Ø§Ù†\nØ¯Ø±Ø®Ø´Ø§Ù†ÛŒØ¯#Ø¯Ø±Ø®Ø´Ø§Ù†\nØ¯Ø±Ø®Ø´ÛŒØ¯#Ø¯Ø±Ø®Ø´\nØ¯Ø±Ø¯Ø§Ø¯#Ø¯Ø±Ø¯Ù‡\nØ¯Ø±Ø±ÙØª#Ø¯Ø±Ø±Ùˆ\nØ¯Ø±Ù…Ø§Ù†Ø¯#Ø¯Ø±Ù…Ø§Ù†\nØ¯Ø±Ù†Ù…ÙˆØ¯#Ø¯Ø±Ù†Ù…Ø§\nØ¯Ø±Ù†ÙˆØ±Ø¯ÛŒØ¯#Ø¯Ø±Ù†ÙˆØ±Ø¯\nØ¯Ø±ÙˆØ¯#Ø¯Ø±Ùˆ\nØ¯Ø±ÙˆÛŒØ¯#Ø¯Ø±Ùˆ\nØ¯Ø±Ú©Ø±Ø¯#Ø¯Ø±Ú©Ù†\nØ¯Ø±Ú©Ø´ÛŒØ¯#Ø¯Ø±Ú©Ø´\nØ¯Ø±Ú¯Ø°Ø´Øª#Ø¯Ø±Ú¯Ø°Ø±\nØ¯Ø±Ú¯Ø±ÙØª#Ø¯Ø±Ú¯ÛŒØ±\nØ¯Ø±ÛŒØ§ÙØª#Ø¯Ø±ÛŒØ§Ø¨\nØ¯Ø±ÛŒØ¯#Ø¯Ø±\nØ¯Ø²Ø¯ÛŒØ¯#Ø¯Ø²Ø¯\nØ¯Ù…ÛŒØ¯#Ø¯Ù…\nØ¯ÙˆØ§Ù†Ø¯#Ø¯ÙˆØ§Ù†\nØ¯ÙˆØ®Øª#Ø¯ÙˆØ²\nØ¯ÙˆØ´ÛŒØ¯#Ø¯ÙˆØ´\nØ¯ÙˆÛŒØ¯#Ø¯Ùˆ\nØ¯ÛŒØ¯#Ø¨ÛŒÙ†\nØ±Ø§Ù†Ø¯#Ø±Ø§Ù†\nØ±Ø¨ÙˆØ¯#Ø±Ø¨Ø§\nØ±Ø¨ÙˆØ¯#Ø±ÙˆØ¨\nØ±Ø®Ø´ÛŒØ¯#Ø±Ø®Ø´\nØ±Ø³Ø§Ù†Ø¯#Ø±Ø³Ø§Ù†\nØ±Ø³Ø§Ù†ÛŒØ¯#Ø±Ø³Ø§Ù†\nØ±Ø³Øª#Ø±Ù‡\nØ±Ø³Øª#Ø±Ùˆ\nØ±Ø³ÛŒØ¯#Ø±Ø³\nØ±Ø´Øª#Ø±ÛŒØ³\nØ±ÙØª#Ø±Ùˆ\nØ±ÙØª#Ø±ÙˆØ¨\nØ±Ù‚ØµØ§Ù†Ø¯#Ø±Ù‚ØµØ§Ù†\nØ±Ù‚ØµÛŒØ¯#Ø±Ù‚Øµ\nØ±Ù…Ø§Ù†Ø¯#Ø±Ù…Ø§Ù†\nØ±Ù…Ø§Ù†ÛŒØ¯#Ø±Ù…Ø§Ù†\nØ±Ù…ÛŒØ¯#Ø±Ù…\nØ±Ù†Ø¬Ø§Ù†Ø¯#Ø±Ù†Ø¬Ø§Ù†\nØ±Ù†Ø¬Ø§Ù†ÛŒØ¯#Ø±Ù†Ø¬Ø§Ù†\nØ±Ù†Ø¬ÛŒØ¯#Ø±Ù†Ø¬\nØ±Ù†Ø¯ÛŒØ¯#Ø±Ù†Ø¯\nØ±Ù‡Ø§Ù†Ø¯#Ø±Ù‡Ø§Ù†\nØ±Ù‡Ø§Ù†ÛŒØ¯#Ø±Ù‡Ø§Ù†\nØ±Ù‡ÛŒØ¯#Ø±Ù‡\nØ±ÙˆØ¨ÛŒØ¯#Ø±ÙˆØ¨\nØ±ÙˆÙØª#Ø±ÙˆØ¨\nØ±ÙˆÛŒØ§Ù†Ø¯#Ø±ÙˆÛŒØ§Ù†\nØ±ÙˆÛŒØ§Ù†ÛŒØ¯#Ø±ÙˆÛŒØ§Ù†\nØ±ÙˆÛŒÛŒØ¯#Ø±Ùˆ\nØ±ÙˆÛŒÛŒØ¯#Ø±ÙˆÛŒ\nØ±ÛŒØ®Øª#Ø±ÛŒØ²\nØ±ÛŒØ¯#Ø±ÛŒÙ†\nØ±ÛŒØ¯Ù†#Ø±ÛŒÙ†\nØ±ÛŒØ³ÛŒØ¯#Ø±ÛŒØ³\nØ²Ø§Ø¯#Ø²Ø§\nØ²Ø§Ø±ÛŒØ¯#Ø²Ø§Ø±\nØ²Ø§ÛŒØ§Ù†Ø¯#Ø²Ø§ÛŒØ§Ù†\nØ²Ø§ÛŒÛŒØ¯#Ø²Ø§\nØ²Ø¯#Ø²Ù†\nØ²Ø¯ÙˆØ¯#Ø²Ø¯Ø§\nØ²ÛŒØ³Øª#Ø²ÛŒ\nØ³Ø§Ø¨Ø§Ù†Ø¯#Ø³Ø§Ø¨Ø§Ù†\nØ³Ø§Ø¨ÛŒØ¯#Ø³Ø§Ø¨\nØ³Ø§Ø®Øª#Ø³Ø§Ø²\nØ³Ø§ÛŒÛŒØ¯#Ø³Ø§\nØ³ØªØ§Ø¯#Ø³ØªØ§Ù†\nØ³ØªØ§Ù†Ø¯#Ø³ØªØ§Ù†\nØ³ØªØ±Ø¯#Ø³ØªØ±\nØ³ØªÙˆØ¯#Ø³ØªØ§\nØ³ØªÛŒØ²ÛŒØ¯#Ø³ØªÛŒØ²\nØ³Ø±Ø§Ù†Ø¯#Ø³Ø±Ø§Ù†\nØ³Ø±Ø§ÛŒÛŒØ¯#Ø³Ø±Ø§\nØ³Ø±Ø´Øª#Ø³Ø±Ø´\nØ³Ø±ÙˆØ¯#Ø³Ø±Ø§\nØ³Ø±Ú©Ø´ÛŒØ¯#Ø³Ø±Ú©Ø´\nØ³Ø±Ú¯Ø±ÙØª#Ø³Ø±Ú¯ÛŒØ±\nØ³Ø±ÛŒØ¯#Ø³Ø±\nØ³Ø²ÛŒØ¯#Ø³Ø²\nØ³ÙØª#Ø³Ù†Ø¨\nØ³Ù†Ø¬ÛŒØ¯#Ø³Ù†Ø¬\nØ³ÙˆØ®Øª#Ø³ÙˆØ²\nØ³ÙˆØ¯#Ø³Ø§\nØ³ÙˆØ²Ø§Ù†Ø¯#Ø³ÙˆØ²Ø§Ù†\nØ³Ù¾Ø§Ø±Ø¯#Ø³Ù¾Ø§Ø±\nØ³Ù¾Ø±Ø¯#Ø³Ù¾Ø§Ø±\nØ³Ù¾Ø±Ø¯#Ø³Ù¾Ø±\nØ³Ù¾ÙˆØ®Øª#Ø³Ù¾ÙˆØ²\nØ³Ú¯Ø§Ù„ÛŒØ¯#Ø³Ú¯Ø§Ù„\nØ´Ø§Ø´ÛŒØ¯#Ø´Ø§Ø´\nØ´Ø§ÛŒØ³Øª#\nØ´Ø§ÛŒØ³Øª#Ø´Ø§ÛŒØ¯\nØ´ØªØ§Ø¨Ø§Ù†Ø¯#Ø´ØªØ§Ø¨Ø§Ù†\nØ´ØªØ§Ø¨ÛŒØ¯#Ø´ØªØ§Ø¨\nØ´ØªØ§ÙØª#Ø´ØªØ§Ø¨\nØ´Ø¯#Ø´Ùˆ\nØ´Ø³Øª#Ø´Ùˆ\nØ´Ø³Øª#Ø´ÙˆÛŒ\nØ´Ù„ÛŒØ¯#Ø´Ù„\nØ´Ù…Ø§Ø±#Ø´Ù…Ø±\nØ´Ù…Ø§Ø±Ø¯#Ø´Ù…Ø§Ø±\nØ´Ù…Ø±Ø¯#Ø´Ù…Ø§Ø±\nØ´Ù…Ø±Ø¯#Ø´Ù…Ø±\nØ´Ù†Ø§Ø®Øª#Ø´Ù†Ø§Ø³\nØ´Ù†Ø§Ø³Ø§Ù†Ø¯#Ø´Ù†Ø§Ø³Ø§Ù†\nØ´Ù†ÙØª#Ø´Ù†Ùˆ\nØ´Ù†ÛŒØ¯#Ø´Ù†Ùˆ\nØ´ÙˆØªÛŒØ¯#Ø´ÙˆØª\nØ´ÙˆØ±Ø§Ù†Ø¯#Ø´ÙˆØ±Ø§Ù†\nØ´ÙˆØ±ÛŒØ¯#Ø´ÙˆØ±\nØ´Ú©Ø§ÙØª#Ø´Ú©Ø§Ù\nØ´Ú©Ø§Ù†Ø¯#Ø´Ú©Ø§Ù†\nØ´Ú©Ø§Ù†Ø¯#Ø´Ú©Ù†\nØ´Ú©Ø³Øª#Ø´Ú©Ù†\nØ´Ú©ÙØª#Ø´Ú©Ù\nØ·Ù„Ø¨ÛŒØ¯#Ø·Ù„Ø¨\nØ·Ù¾ÛŒØ¯#Ø·Ù¾\nØºØ±Ø§Ù†Ø¯#ØºØ±Ø§Ù†\nØºØ±ÛŒØ¯#ØºØ±\nØºÙ„ØªØ§Ù†Ø¯#ØºÙ„ØªØ§Ù†\nØºÙ„ØªØ§Ù†ÛŒØ¯#ØºÙ„ØªØ§Ù†\nØºÙ„ØªÛŒØ¯#ØºÙ„Øª\nØºÙ„Ø·Ø§Ù†Ø¯#ØºÙ„Ø·Ø§Ù†\nØºÙ„Ø·Ø§Ù†ÛŒØ¯#ØºÙ„Ø·Ø§Ù†\nØºÙ„Ø·ÛŒØ¯#ØºÙ„Ø·\nÙØ±Ø§#ÙØ±Ø§Ø®ÙˆØ§Ù‡\nÙØ±Ø§Ø®ÙˆØ§Ù†Ø¯#ÙØ±Ø§Ø®ÙˆØ§Ù†\nÙØ±Ø§Ø¯Ø§Ø´Øª#ÙØ±Ø§Ø¯Ø§Ø±\nÙØ±Ø§Ø±Ø³ÛŒØ¯#ÙØ±Ø§Ø±Ø³\nÙØ±Ø§Ù†Ù…ÙˆØ¯#ÙØ±Ø§Ù†Ù…Ø§\nÙØ±Ø§Ú¯Ø±ÙØª#ÙØ±Ø§Ú¯ÛŒØ±\nÙØ±Ø³ØªØ§Ø¯#ÙØ±Ø³Øª\nÙØ±Ø³ÙˆØ¯#ÙØ±Ø³Ø§\nÙØ±Ù…ÙˆØ¯#ÙØ±Ù…Ø§\nÙØ±Ù‡ÛŒØ®Øª#ÙØ±Ù‡ÛŒØ²\nÙØ±Ùˆ#ÙØ±ÙˆØ®ÙˆØ§Ù‡\nÙØ±ÙˆØ¢Ù…Ø¯#ÙØ±ÙˆØ¢\nÙØ±ÙˆØ¢ÙˆØ±Ø¯#ÙØ±ÙˆØ¢ÙˆØ±\nÙØ±ÙˆØ§ÙØªØ§Ø¯#ÙØ±ÙˆØ§ÙØª\nÙØ±ÙˆØ§ÙÚ©Ù†Ø¯#ÙØ±ÙˆØ§ÙÚ©Ù†\nÙØ±ÙˆØ¨Ø±Ø¯#ÙØ±ÙˆØ¨Ø±\nÙØ±ÙˆØ¨Ø³Øª#ÙØ±ÙˆØ¨Ù†Ø¯\nÙØ±ÙˆØ®Øª#ÙØ±ÙˆØ´\nÙØ±ÙˆØ®ÙØª#ÙØ±ÙˆØ®ÙˆØ§Ø¨\nÙØ±ÙˆØ®ÙˆØ±Ø¯#ÙØ±ÙˆØ®ÙˆØ±\nÙØ±ÙˆØ¯Ø§Ø¯#ÙØ±ÙˆØ¯Ù‡\nÙØ±ÙˆØ¯ÙˆØ®Øª#ÙØ±ÙˆØ¯ÙˆØ²\nÙØ±ÙˆØ±ÙØª#ÙØ±ÙˆØ±Ùˆ\nÙØ±ÙˆØ±ÛŒØ®Øª#ÙØ±ÙˆØ±ÛŒØ²\nÙØ±ÙˆØ´Ú©Ø³Øª#ÙØ±ÙˆØ´Ú©Ù†\nÙØ±ÙˆÙØ±Ø³ØªØ§Ø¯#ÙØ±ÙˆÙØ±Ø³Øª\nÙØ±ÙˆÙ…Ø§Ù†Ø¯#ÙØ±ÙˆÙ…Ø§Ù†\nÙØ±ÙˆÙ†Ø´Ø§Ù†Ø¯#ÙØ±ÙˆÙ†Ø´Ø§Ù†\nÙØ±ÙˆÙ†Ø´Ø§Ù†ÛŒØ¯#ÙØ±ÙˆÙ†Ø´Ø§Ù†\nÙØ±ÙˆÙ†Ø´Ø³Øª#ÙØ±ÙˆÙ†Ø´ÛŒÙ†\nÙØ±ÙˆÙ†Ù…ÙˆØ¯#ÙØ±ÙˆÙ†Ù…Ø§\nÙØ±ÙˆÙ†Ù‡Ø§Ø¯#ÙØ±ÙˆÙ†Ù‡\nÙØ±ÙˆÙ¾Ø§Ø´Ø§Ù†Ø¯#ÙØ±ÙˆÙ¾Ø§Ø´Ø§Ù†\nÙØ±ÙˆÙ¾Ø§Ø´ÛŒØ¯#ÙØ±ÙˆÙ¾Ø§Ø´\nÙØ±ÙˆÚ†Ú©ÛŒØ¯#ÙØ±ÙˆÚ†Ú©\nÙØ±ÙˆÚ©Ø±Ø¯#ÙØ±ÙˆÚ©Ù†\nÙØ±ÙˆÚ©Ø´ÛŒØ¯#ÙØ±ÙˆÚ©Ø´\nÙØ±ÙˆÚ©ÙˆØ¨ÛŒØ¯#ÙØ±ÙˆÚ©ÙˆØ¨\nÙØ±ÙˆÚ©ÙˆÙØª#ÙØ±ÙˆÚ©ÙˆØ¨\nÙØ±ÙˆÚ¯Ø°Ø§Ø±Ø¯#ÙØ±ÙˆÚ¯Ø°Ø§Ø±\nÙØ±ÙˆÚ¯Ø°Ø§Ø´Øª#ÙØ±ÙˆÚ¯Ø°Ø§Ø±\nÙØ±ÙˆÚ¯Ø±ÙØª#ÙØ±ÙˆÚ¯ÛŒØ±\nÙØ±ÛŒÙØª#ÙØ±ÛŒØ¨\nÙØ´Ø§Ù†Ø¯#ÙØ´Ø§Ù†\nÙØ´Ø±Ø¯#ÙØ´Ø§Ø±\nÙØ´Ø±Ø¯#ÙØ´Ø±\nÙÙ„Ø³ÙÛŒØ¯#ÙÙ„Ø³Ù\nÙÙ‡Ù…Ø§Ù†Ø¯#ÙÙ‡Ù…Ø§Ù†\nÙÙ‡Ù…ÛŒØ¯#ÙÙ‡Ù…\nÙ‚Ø§Ù¾ÛŒØ¯#Ù‚Ø§Ù¾\nÙ‚Ø¨ÙˆÙ„Ø§Ù†Ø¯#Ù‚Ø¨ÙˆÙ„\nÙ‚Ø¨ÙˆÙ„Ø§Ù†Ø¯#Ù‚Ø¨ÙˆÙ„Ø§Ù†\nÙ„Ø§Ø³ÛŒØ¯#Ù„Ø§Ø³\nÙ„Ø±Ø²Ø§Ù†Ø¯#Ù„Ø±Ø²Ø§Ù†\nÙ„Ø±Ø²ÛŒØ¯#Ù„Ø±Ø²\nÙ„ØºØ²Ø§Ù†Ø¯#Ù„ØºØ²Ø§Ù†\nÙ„ØºØ²ÛŒØ¯#Ù„ØºØ²\nÙ„Ù…Ø¨Ø§Ù†Ø¯#Ù„Ù…Ø¨Ø§Ù†\nÙ„Ù…ÛŒØ¯#Ù„Ù…\nÙ„Ù†Ú¯ÛŒØ¯#Ù„Ù†Ú¯\nÙ„ÙˆÙ„ÛŒØ¯#Ù„ÙˆÙ„\nÙ„ÛŒØ³ÛŒØ¯#Ù„ÛŒØ³\nÙ…Ø§Ø³ÛŒØ¯#Ù…Ø§Ø³\nÙ…Ø§Ù„Ø§Ù†Ø¯#Ù…Ø§Ù„Ø§Ù†\nÙ…Ø§Ù„ÛŒØ¯#Ù…Ø§Ù„\nÙ…Ø§Ù†Ø¯#Ù…Ø§Ù†\nÙ…Ø§Ù†Ø³Øª#Ù…Ø§Ù†\nÙ…Ø±Ø¯#Ù…ÛŒØ±\nÙ…ÙˆÛŒÛŒØ¯#Ù…Ùˆ\nÙ…Ú©ÛŒØ¯#Ù…Ú©\nÙ†Ø§Ø²ÛŒØ¯#Ù†Ø§Ø²\nÙ†Ø§Ù„Ø§Ù†Ø¯#Ù†Ø§Ù„Ø§Ù†\nÙ†Ø§Ù„ÛŒØ¯#Ù†Ø§Ù„\nÙ†Ø§Ù…ÛŒØ¯#Ù†Ø§Ù…\nÙ†Ø´Ø§Ù†Ø¯#Ù†Ø´Ø§Ù†\nÙ†Ø´Ø³Øª#Ù†Ø´ÛŒÙ†\nÙ†Ù…Ø§ÛŒØ§Ù†Ø¯#Ù†Ù…Ø§\nÙ†Ù…Ø§ÛŒØ§Ù†Ø¯#Ù†Ù…Ø§ÛŒØ§Ù†\nÙ†Ù…ÙˆØ¯#Ù†Ù…Ø§\nÙ†Ù‡Ø§Ø¯#Ù†Ù‡\nÙ†Ù‡ÙØª#Ù†Ù‡Ù†Ø¨\nÙ†ÙˆØ§Ø®Øª#Ù†ÙˆØ§Ø²\nÙ†ÙˆØ§Ø²ÛŒØ¯#Ù†ÙˆØ§Ø²\nÙ†ÙˆØ±Ø¯ÛŒØ¯#Ù†ÙˆØ±Ø¯\nÙ†ÙˆØ´Ø§Ù†Ø¯#Ù†ÙˆØ´Ø§Ù†\nÙ†ÙˆØ´Ø§Ù†ÛŒØ¯#Ù†ÙˆØ´Ø§Ù†\nÙ†ÙˆØ´Øª#Ù†ÙˆÛŒØ³\nÙ†ÙˆØ´ÛŒØ¯#Ù†ÙˆØ´\nÙ†Ú©ÙˆÙ‡ÛŒØ¯#Ù†Ú©ÙˆÙ‡\nÙ†Ú¯Ø§Ø´Øª#Ù†Ú¯Ø§Ø±\nÙ†Ú¯Ø±ÛŒØ¯#\nÙ†Ú¯Ø±ÛŒØ³Øª#Ù†Ú¯Ø±\nÙ‡Ø±Ø§Ø³Ø§Ù†Ø¯#Ù‡Ø±Ø§Ø³Ø§Ù†\nÙ‡Ø±Ø§Ø³Ø§Ù†ÛŒØ¯#Ù‡Ø±Ø§Ø³Ø§Ù†\nÙ‡Ø±Ø§Ø³ÛŒØ¯#Ù‡Ø±Ø§Ø³\nÙ‡Ø´Øª#Ù‡Ù„\nÙˆØ§#ÙˆØ§Ø®ÙˆØ§Ù‡\nÙˆØ§Ø¯Ø§Ø´Øª#ÙˆØ§Ø¯Ø§Ø±\nÙˆØ§Ø±ÙØª#ÙˆØ§Ø±Ùˆ\nÙˆØ§Ø±Ù‡Ø§Ù†Ø¯#ÙˆØ§Ø±Ù‡Ø§Ù†\nÙˆØ§Ù…Ø§Ù†Ø¯#ÙˆØ§Ù…Ø§Ù†\nÙˆØ§Ù†Ù‡Ø§Ø¯#ÙˆØ§Ù†Ù‡\nÙˆØ§Ú©Ø±Ø¯#ÙˆØ§Ú©Ù†\nÙˆØ§Ú¯Ø°Ø§Ø±Ø¯#ÙˆØ§Ú¯Ø°Ø§Ø±\nÙˆØ§Ú¯Ø°Ø§Ø´Øª#ÙˆØ§Ú¯Ø°Ø§Ø±\nÙˆØ±#ÙˆØ±Ø®ÙˆØ§Ù‡\nÙˆØ±Ø¢Ù…Ø¯#ÙˆØ±Ø¢\nÙˆØ±Ø§ÙØªØ§Ø¯#ÙˆØ±Ø§ÙØª\nÙˆØ±Ø±ÙØª#ÙˆØ±Ø±Ùˆ\nÙˆØ±Ø²ÛŒØ¯#ÙˆØ±Ø²\nÙˆØ²Ø§Ù†Ø¯#ÙˆØ²Ø§Ù†\nÙˆØ²ÛŒØ¯#ÙˆØ²\nÙˆÛŒØ±Ø§Ø³Øª#ÙˆÛŒØ±Ø§\nÙ¾Ø§Ø´Ø§Ù†Ø¯#Ù¾Ø§Ø´Ø§Ù†\nÙ¾Ø§Ø´ÛŒØ¯#Ù¾Ø§Ø´\nÙ¾Ø§Ù„ÙˆØ¯#Ù¾Ø§Ù„Ø§\nÙ¾Ø§ÛŒÛŒØ¯#Ù¾Ø§\nÙ¾Ø®Øª#Ù¾Ø²\nÙ¾Ø°ÛŒØ±Ø§Ù†Ø¯#Ù¾Ø°ÛŒØ±Ø§Ù†\nÙ¾Ø°ÛŒØ±ÙØª#Ù¾Ø°ÛŒØ±\nÙ¾Ø±Ø§Ù†Ø¯#Ù¾Ø±Ø§Ù†\nÙ¾Ø±Ø§Ú©Ù†Ø¯#Ù¾Ø±Ø§Ú©Ù†\nÙ¾Ø±Ø¯Ø§Ø®Øª#Ù¾Ø±Ø¯Ø§Ø²\nÙ¾Ø±Ø³ØªÛŒØ¯#Ù¾Ø±Ø³Øª\nÙ¾Ø±Ø³ÛŒØ¯#Ù¾Ø±Ø³\nÙ¾Ø±Ù‡ÛŒØ®Øª#Ù¾Ø±Ù‡ÛŒØ²\nÙ¾Ø±Ù‡ÛŒØ²ÛŒØ¯#Ù¾Ø±Ù‡ÛŒØ²\nÙ¾Ø±ÙˆØ±Ø§Ù†Ø¯#Ù¾Ø±ÙˆØ±Ø§Ù†\nÙ¾Ø±ÙˆØ±Ø¯#Ù¾Ø±ÙˆØ±\nÙ¾Ø±ÛŒØ¯#Ù¾Ø±\nÙ¾Ø³Ù†Ø¯ÛŒØ¯#Ù¾Ø³Ù†Ø¯\nÙ¾Ù„Ø§Ø³Ø§Ù†Ø¯#Ù¾Ù„Ø§Ø³Ø§Ù†\nÙ¾Ù„Ø§Ø³ÛŒØ¯#Ù¾Ù„Ø§Ø³\nÙ¾Ù„Ú©ÛŒØ¯#Ù¾Ù„Ú©\nÙ¾Ù†Ø§Ù‡Ø§Ù†Ø¯#Ù¾Ù†Ø§Ù‡Ø§Ù†\nÙ¾Ù†Ø§Ù‡ÛŒØ¯#Ù¾Ù†Ø§Ù‡\nÙ¾Ù†Ø¯Ø§Ø´Øª#Ù¾Ù†Ø¯Ø§Ø±\nÙ¾ÙˆØ³Ø§Ù†Ø¯#Ù¾ÙˆØ³Ø§Ù†\nÙ¾ÙˆØ³ÛŒØ¯#Ù¾ÙˆØ³\nÙ¾ÙˆØ´Ø§Ù†Ø¯#Ù¾ÙˆØ´Ø§Ù†\nÙ¾ÙˆØ´ÛŒØ¯#Ù¾ÙˆØ´\nÙ¾ÙˆÛŒÛŒØ¯#Ù¾Ùˆ\nÙ¾Ú˜Ù…Ø±Ø¯#Ù¾Ú˜Ù…Ø±\nÙ¾Ú˜ÙˆÙ‡ÛŒØ¯#Ù¾Ú˜ÙˆÙ‡\nÙ¾Ú©ÛŒØ¯#Ù¾Ú©\nÙ¾ÛŒØ±Ø§Ø³Øª#Ù¾ÛŒØ±Ø§\nÙ¾ÛŒÙ…ÙˆØ¯#Ù¾ÛŒÙ…Ø§\nÙ¾ÛŒÙˆØ³Øª#Ù¾ÛŒÙˆÙ†Ø¯\nÙ¾ÛŒÚ†Ø§Ù†Ø¯#Ù¾ÛŒÚ†Ø§Ù†\nÙ¾ÛŒÚ†Ø§Ù†ÛŒØ¯#Ù¾ÛŒÚ†Ø§Ù†\nÙ¾ÛŒÚ†ÛŒØ¯#Ù¾ÛŒÚ†\nÚ†Ø§Ù¾ÛŒØ¯#Ú†Ø§Ù¾\nÚ†Ø§ÛŒÛŒØ¯#Ú†Ø§\nÚ†Ø±Ø§Ù†Ø¯#Ú†Ø±Ø§Ù†\nÚ†Ø±Ø§Ù†ÛŒØ¯#Ú†Ø±Ø§Ù†\nÚ†Ø±Ø¨Ø§Ù†Ø¯#Ú†Ø±Ø¨Ø§Ù†\nÚ†Ø±Ø¨ÛŒØ¯#Ú†Ø±Ø¨\nÚ†Ø±Ø®Ø§Ù†Ø¯#Ú†Ø±Ø®Ø§Ù†\nÚ†Ø±Ø®Ø§Ù†ÛŒØ¯#Ú†Ø±Ø®Ø§Ù†\nÚ†Ø±Ø®ÛŒØ¯#Ú†Ø±Ø®\nÚ†Ø±ÙˆÚ©ÛŒØ¯#Ú†Ø±ÙˆÚ©\nÚ†Ø±ÛŒØ¯#Ú†Ø±\nÚ†Ø²Ø§Ù†Ø¯#Ú†Ø²Ø§Ù†\nÚ†Ø³Ø¨Ø§Ù†Ø¯#Ú†Ø³Ø¨Ø§Ù†\nÚ†Ø³Ø¨ÛŒØ¯#Ú†Ø³Ø¨\nÚ†Ø³ÛŒØ¯#Ú†Ø³\nÚ†Ø´Ø§Ù†Ø¯#Ú†Ø´Ø§Ù†\nÚ†Ø´ÛŒØ¯#Ú†Ø´\nÚ†Ù„Ø§Ù†Ø¯#Ú†Ù„Ø§Ù†\nÚ†Ù„Ø§Ù†ÛŒØ¯#Ú†Ù„Ø§Ù†\nÚ†Ù¾Ø§Ù†Ø¯#Ú†Ù¾Ø§Ù†\nÚ†Ù¾ÛŒØ¯#Ú†Ù¾\nÚ†Ú©Ø§Ù†Ø¯#Ú†Ú©Ø§Ù†\nÚ†Ú©ÛŒØ¯#Ú†Ú©\nÚ†ÛŒØ¯#Ú†ÛŒÙ†\nÚ©Ø§Ø³Øª#Ú©Ø§Ù‡\nÚ©Ø§Ø´Øª#Ú©Ø§Ø±\nÚ©Ø§ÙˆÛŒØ¯#Ú©Ø§Ùˆ\nÚ©Ø±Ø¯#Ú©Ù†\nÚ©Ø´Ø§Ù†Ø¯#Ú©Ø´Ø§Ù†\nÚ©Ø´Ø§Ù†ÛŒØ¯#Ú©Ø´Ø§Ù†\nÚ©Ø´Øª#Ú©Ø§Ø±\nÚ©Ø´Øª#Ú©Ø´\nÚ©Ø´ÛŒØ¯#Ú©Ø´\nÚ©Ù†Ø¯#Ú©Ù†\nÚ©ÙˆØ¨Ø§Ù†Ø¯#Ú©ÙˆØ¨Ø§Ù†\nÚ©ÙˆØ¨ÛŒØ¯#Ú©ÙˆØ¨\nÚ©ÙˆØ´ÛŒØ¯#Ú©ÙˆØ´\nÚ©ÙˆÙØª#Ú©ÙˆØ¨\nÚ©ÙˆÚ†Ø§Ù†ÛŒØ¯#Ú©ÙˆÚ†Ø§Ù†\nÚ©ÙˆÚ†ÛŒØ¯#Ú©ÙˆÚ†\nÚ¯Ø§ÛŒÛŒØ¯#Ú¯Ø§\nÚ¯Ø¯Ø§Ø®Øª#Ú¯Ø¯Ø§Ø²\nÚ¯Ø°Ø§Ø±Ø¯#Ú¯Ø°Ø§Ø±\nÚ¯Ø°Ø§Ø´Øª#Ú¯Ø°Ø§Ø±\nÚ¯Ø°Ø±Ø§Ù†Ø¯#Ú¯Ø°Ø±Ø§Ù†\nÚ¯Ø°Ø´Øª#Ú¯Ø°Ø±\nÚ¯Ø±Ø§Ø²ÛŒØ¯#Ú¯Ø±Ø§Ø²\nÚ¯Ø±Ø§Ù†ÛŒØ¯#Ú¯Ø±Ø§Ù†\nÚ¯Ø±Ø§ÛŒÛŒØ¯#Ú¯Ø±Ø§\nÚ¯Ø±Ø¯Ø§Ù†Ø¯#Ú¯Ø±Ø¯Ø§Ù†\nÚ¯Ø±Ø¯Ø§Ù†ÛŒØ¯#Ú¯Ø±Ø¯Ø§Ù†\nÚ¯Ø±Ø¯ÛŒØ¯#Ú¯Ø±Ø¯\nÚ¯Ø±ÙØª#Ú¯ÛŒØ±\nÚ¯Ø±ÙˆÛŒØ¯#Ú¯Ø±Ùˆ\nÚ¯Ø±ÛŒØ§Ù†Ø¯#Ú¯Ø±ÛŒØ§Ù†\nÚ¯Ø±ÛŒØ®Øª#Ú¯Ø±ÛŒØ²\nÚ¯Ø±ÛŒØ²Ø§Ù†Ø¯#Ú¯Ø±ÛŒØ²Ø§Ù†\nÚ¯Ø±ÛŒØ³Øª#Ú¯Ø±\nÚ¯Ø±ÛŒØ³Øª#Ú¯Ø±ÛŒ\nÚ¯Ø²Ø§Ø±Ø¯#Ú¯Ø²Ø§Ø±\nÚ¯Ø²Ø§Ø´Øª#Ú¯Ø²Ø§Ø±\nÚ¯Ø²ÛŒØ¯#Ú¯Ø²ÛŒÙ†\nÚ¯Ø³Ø§Ø±Ø¯#Ú¯Ø³Ø§Ø±\nÚ¯Ø³ØªØ±Ø§Ù†Ø¯#Ú¯Ø³ØªØ±Ø§Ù†\nÚ¯Ø³ØªØ±Ø§Ù†ÛŒØ¯#Ú¯Ø³ØªØ±Ø§Ù†\nÚ¯Ø³ØªØ±Ø¯#Ú¯Ø³ØªØ±\nÚ¯Ø³Ø³Øª#Ú¯Ø³Ù„\nÚ¯Ø³Ù„Ø§Ù†Ø¯#Ú¯Ø³Ù„\nÚ¯Ø³ÛŒØ®Øª#Ú¯Ø³Ù„\nÚ¯Ø´Ø§Ø¯#Ú¯Ø´Ø§\nÚ¯Ø´Øª#Ú¯Ø±Ø¯\nÚ¯Ø´ÙˆØ¯#Ú¯Ø´Ø§\nÚ¯ÙØª#Ú¯Ùˆ\nÚ¯Ù…Ø§Ø±Ø¯#Ú¯Ù…Ø§Ø±\nÚ¯Ù…Ø§Ø´Øª#Ú¯Ù…Ø§Ø±\nÚ¯Ù†Ø¬Ø§Ù†Ø¯#Ú¯Ù†Ø¬Ø§Ù†\nÚ¯Ù†Ø¬Ø§Ù†ÛŒØ¯#Ú¯Ù†Ø¬Ø§Ù†\nÚ¯Ù†Ø¬ÛŒØ¯#Ú¯Ù†Ø¬\nÚ¯Ù†Ø¯Ø§Ù†Ø¯#Ú¯Ù†Ø¯Ø§Ù†\nÚ¯Ù†Ø¯ÛŒØ¯#Ú¯Ù†Ø¯\nÚ¯ÙˆØ§Ø±ÛŒØ¯#Ú¯ÙˆØ§Ø±\nÚ¯ÙˆØ²ÛŒØ¯#Ú¯ÙˆØ²\nÚ¯ÛŒØ±Ø§Ù†Ø¯#Ú¯ÛŒØ±Ø§Ù†\nÛŒØ§Ø²ÛŒØ¯#ÛŒØ§Ø²\nÛŒØ§ÙØª#ÛŒØ§Ø¨\nÛŒÙˆÙ†ÛŒØ¯#ÛŒÙˆÙ†\n'.strip().split()
A:spacy.lang.fa.generate_verbs_exc.(past, present)->verb_root.split('#')
A:spacy.lang.fa.generate_verbs_exc.conjugations->list(set(map(lambda item: item.replace('Ø¨Ø¢', 'Ø¨ÛŒØ§').replace('Ù†Ø¢', 'Ù†ÛŒØ§'), conjugations)))


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/fa/tokenizer_exceptions.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/fa/__init__.py----------------------------------------
spacy.lang.fa.__init__.Persian(Language)
spacy.lang.fa.__init__.PersianDefaults(BaseDefaults)
spacy.lang.fa.__init__.make_lemmatizer(nlp:Language,model:Optional[Model],name:str,mode:str,overwrite:bool,scorer:Optional[Callable])


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/fa/lex_attrs.py----------------------------------------
A:spacy.lang.fa.lex_attrs._num_words->set('\nØµÙØ±\nÛŒÚ©\nØ¯Ùˆ\nØ³Ù‡\nÚ†Ù‡Ø§Ø±\nÙ¾Ù†Ø¬\nØ´Ø´\nØ´ÛŒØ´\nÙ‡ÙØª\nÙ‡Ø´Øª\nÙ†Ù‡\nØ¯Ù‡\nÛŒØ§Ø²Ø¯Ù‡\nØ¯ÙˆØ§Ø²Ø¯Ù‡\nØ³ÛŒØ²Ø¯Ù‡\nÚ†Ù‡Ø§Ø±Ø¯Ù‡\nÙ¾Ø§Ù†Ø²Ø¯Ù‡\nÙ¾ÙˆÙ†Ø²Ø¯Ù‡\nØ´Ø§Ù†Ø²Ø¯Ù‡\nØ´ÙˆÙ†Ø²Ø¯Ù‡\nÙ‡ÙØ¯Ù‡\nÙ‡Ø¬Ø¯Ù‡\nÙ‡ÛŒØ¬Ø¯Ù‡\nÙ†ÙˆØ²Ø¯Ù‡\nØ¨ÛŒØ³Øª\nØ³ÛŒ\nÚ†Ù‡Ù„\nÙ¾Ù†Ø¬Ø§Ù‡\nØ´ØµØª\nÙ‡ÙØªØ§Ø¯\nÙ‡Ø´ØªØ§Ø¯\nÙ†ÙˆØ¯\nØµØ¯\nÛŒÚ©ØµØ¯\nÛŒÚ©\u200cØµØ¯\nØ¯ÙˆÛŒØ³Øª\nØ³ÛŒØµØ¯\nÚ†Ù‡Ø§Ø±ØµØ¯\nÙ¾Ø§Ù†ØµØ¯\nÙ¾ÙˆÙ†ØµØ¯\nØ´Ø´ØµØ¯\nØ´ÛŒØ´ØµØ¯\nÙ‡ÙØªØµØ¯\nÙ‡ÙØµØ¯\nÙ‡Ø´ØªØµØ¯\nÙ†Ù‡ØµØ¯\nÙ‡Ø²Ø§Ø±\nÙ…ÛŒÙ„ÛŒÙˆÙ†\nÙ…ÛŒÙ„ÛŒØ§Ø±Ø¯\nØ¨ÛŒÙ„ÛŒÙˆÙ†\nØ¨ÛŒÙ„ÛŒØ§Ø±Ø¯\nØªØ±ÛŒÙ„ÛŒÙˆÙ†\nØªØ±ÛŒÙ„ÛŒØ§Ø±Ø¯\nÚ©ÙˆØ§Ø¯Ø±ÛŒÙ„ÛŒÙˆÙ†\nÚ©Ø§Ø¯Ø±ÛŒÙ„ÛŒØ§Ø±Ø¯\nÚ©ÙˆÛŒÙ†ØªÛŒÙ„ÛŒÙˆÙ†\n'.split())
A:spacy.lang.fa.lex_attrs._ordinal_words->set('\nØ§ÙˆÙ„\nØ³ÙˆÙ…\nØ³ÛŒ\u200cØ§Ù…'.split())
A:spacy.lang.fa.lex_attrs.text->text.replace(',', '').replace('.', '').replace('ØŒ', '').replace('Ù«', '').replace('/', '').replace(',', '').replace('.', '').replace('ØŒ', '').replace('Ù«', '').replace('/', '')
spacy.lang.fa.lex_attrs.like_num(text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/fa/syntax_iterators.py----------------------------------------
A:spacy.lang.fa.syntax_iterators.conj->doc.vocab.strings.add('conj')
A:spacy.lang.fa.syntax_iterators.np_label->doc.vocab.strings.add('NP')
spacy.lang.fa.syntax_iterators.noun_chunks(doclike:Union[Doc,Span])->Iterator[Tuple[int, int, int]]


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/fa/punctuation.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/tt/examples.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/tt/stop_words.py----------------------------------------
A:spacy.lang.tt.stop_words.STOP_WORDS->set('Ð°Ð»Ð°Ð¹ Ð°Ð»Ð°Ð¹ÑÐ° Ð°Ð»Ð°Ñ€ Ð°Ð»Ð°Ñ€Ð³Ð° Ð°Ð»Ð°Ñ€Ð´Ð° Ð°Ð»Ð°Ñ€Ð´Ð°Ð½ Ð°Ð»Ð°Ñ€Ð½Ñ‹ Ð°Ð»Ð°Ñ€Ð½Ñ‹Ò£ Ð°Ð»Ð°Ñ€Ñ‡Ð°\nÐ°Ð»Ð°Ñ€Ñ‹ Ð°Ð»Ð°Ñ€Ñ‹Ð½ Ð°Ð»Ð°Ñ€Ñ‹Ð½Ð³Ð° Ð°Ð»Ð°Ñ€Ñ‹Ð½Ð´Ð° Ð°Ð»Ð°Ñ€Ñ‹Ð½Ð½Ð°Ð½ Ð°Ð»Ð°Ñ€Ñ‹Ð½Ñ‹Ò£ Ð°Ð»Ñ‚Ð¼Ñ‹Ñˆ Ð°Ð»Ñ‚Ð¼Ñ‹ÑˆÑ‹Ð½Ñ‡Ñ‹ Ð°Ð»Ñ‚Ð¼Ñ‹ÑˆÑ‹Ð½Ñ‡Ñ‹Ð³Ð°\nÐ°Ð»Ñ‚Ð¼Ñ‹ÑˆÑ‹Ð½Ñ‡Ñ‹Ð´Ð° Ð°Ð»Ñ‚Ð¼Ñ‹ÑˆÑ‹Ð½Ñ‡Ñ‹Ð´Ð°Ð½ Ð°Ð»Ñ‚Ð¼Ñ‹ÑˆÑ‹Ð½Ñ‡Ñ‹Ð»Ð°Ñ€ Ð°Ð»Ñ‚Ð¼Ñ‹ÑˆÑ‹Ð½Ñ‡Ñ‹Ð»Ð°Ñ€Ð³Ð° Ð°Ð»Ñ‚Ð¼Ñ‹ÑˆÑ‹Ð½Ñ‡Ñ‹Ð»Ð°Ñ€Ð´Ð°\nÐ°Ð»Ñ‚Ð¼Ñ‹ÑˆÑ‹Ð½Ñ‡Ñ‹Ð»Ð°Ñ€Ð´Ð°Ð½ Ð°Ð»Ñ‚Ð¼Ñ‹ÑˆÑ‹Ð½Ñ‡Ñ‹Ð»Ð°Ñ€Ð½Ñ‹ Ð°Ð»Ñ‚Ð¼Ñ‹ÑˆÑ‹Ð½Ñ‡Ñ‹Ð»Ð°Ñ€Ð½Ñ‹Ò£ Ð°Ð»Ñ‚Ð¼Ñ‹ÑˆÑ‹Ð½Ñ‡Ñ‹Ð½Ñ‹ Ð°Ð»Ñ‚Ð¼Ñ‹ÑˆÑ‹Ð½Ñ‡Ñ‹Ð½Ñ‹Ò£\nÐ°Ð»Ñ‚Ñ‹ Ð°Ð»Ñ‚Ñ‹Ð»Ð°Ð¿ Ð°Ð»Ñ‚Ñ‹Ð½Ñ‡Ñ‹ Ð°Ð»Ñ‚Ñ‹Ð½Ñ‡Ñ‹Ð³Ð° Ð°Ð»Ñ‚Ñ‹Ð½Ñ‡Ñ‹Ð´Ð° Ð°Ð»Ñ‚Ñ‹Ð½Ñ‡Ñ‹Ð´Ð°Ð½ Ð°Ð»Ñ‚Ñ‹Ð½Ñ‡Ñ‹Ð»Ð°Ñ€ Ð°Ð»Ñ‚Ñ‹Ð½Ñ‡Ñ‹Ð»Ð°Ñ€Ð³Ð°\nÐ°Ð»Ñ‚Ñ‹Ð½Ñ‡Ñ‹Ð»Ð°Ñ€Ð´Ð° Ð°Ð»Ñ‚Ñ‹Ð½Ñ‡Ñ‹Ð»Ð°Ñ€Ð´Ð°Ð½ Ð°Ð»Ñ‚Ñ‹Ð½Ñ‡Ñ‹Ð»Ð°Ñ€Ð½Ñ‹ Ð°Ð»Ñ‚Ñ‹Ð½Ñ‡Ñ‹Ð»Ð°Ñ€Ð½Ñ‹Ò£ Ð°Ð»Ñ‚Ñ‹Ð½Ñ‡Ñ‹Ð½Ñ‹ Ð°Ð»Ñ‚Ñ‹Ð½Ñ‡Ñ‹Ð½Ñ‹Ò£\nÐ°Ð»Ñ‚Ñ‹ÑˆÐ°Ñ€ Ð°Ð½Ð´Ð° Ð°Ð½Ð´Ð°Ð³Ñ‹ Ð°Ð½Ð´Ð°Ð¹ Ð°Ð½Ð´Ñ‹Ð¹ Ð°Ð½Ð´Ñ‹Ð¹Ð³Ð° Ð°Ð½Ð´Ñ‹Ð¹Ð´Ð° Ð°Ð½Ð´Ñ‹Ð¹Ð´Ð°Ð½ Ð°Ð½Ð´Ñ‹Ð¹Ð½Ñ‹ Ð°Ð½Ð´Ñ‹Ð¹Ð½Ñ‹Ò£ Ð°Ð½Ð½Ð°Ð½\nÐ°Ð½ÑÑ‹ Ð°Ð½Ñ‡Ð° Ð°Ð½Ñ‹ Ð°Ð½Ñ‹ÐºÑ‹ Ð°Ð½Ñ‹ÐºÑ‹Ð½ Ð°Ð½Ñ‹ÐºÑ‹Ð½Ð³Ð° Ð°Ð½Ñ‹ÐºÑ‹Ð½Ð´Ð° Ð°Ð½Ñ‹ÐºÑ‹Ð½Ð½Ð°Ð½ Ð°Ð½Ñ‹ÐºÑ‹Ð½Ñ‹Ò£ Ð°Ð½Ñ‹ÑÑ‹ Ð°Ð½Ñ‹ÑÑ‹Ð½\nÐ°Ð½Ñ‹ÑÑ‹Ð½Ð³Ð° Ð°Ð½Ñ‹ÑÑ‹Ð½Ð´Ð° Ð°Ð½Ñ‹ÑÑ‹Ð½Ð½Ð°Ð½ Ð°Ð½Ñ‹ÑÑ‹Ð½Ñ‹Ò£ Ð°Ð½Ñ‹Ò£ Ð°Ð½Ñ‹Ò£Ñ‡Ð° Ð°Ñ€ÐºÑ‹Ð»Ñ‹ Ð°Ñ€Ñ‹ Ð°ÑˆÐ° Ð°Ò£Ð° Ð°Ò£Ð°Ñ€ Ð°Ò£Ð°Ñ€Ð³Ð°\nÐ°Ò£Ð°Ñ€Ð´Ð° Ð°Ò£Ð°Ñ€Ð´Ð°Ð³Ñ‹ Ð°Ò£Ð°Ñ€Ð´Ð°Ð½\n\nÐ±Ð°Ñ€ Ð±Ð°Ñ€Ð° Ð±Ð°Ñ€Ð»Ñ‹Ðº Ð±Ð°Ñ€Ñ‡Ð° Ð±Ð°Ñ€Ñ‡Ð°ÑÑ‹ Ð±Ð°Ñ€Ñ‡Ð°ÑÑ‹Ð½ Ð±Ð°Ñ€Ñ‡Ð°ÑÑ‹Ð½Ð° Ð±Ð°Ñ€Ñ‡Ð°ÑÑ‹Ð½Ð´Ð° Ð±Ð°Ñ€Ñ‡Ð°ÑÑ‹Ð½Ð½Ð°Ð½\nÐ±Ð°Ñ€Ñ‡Ð°ÑÑ‹Ð½Ñ‹Ò£ Ð±Ð°Ñ€Ñ‹ Ð±Ð°ÑˆÐºÐ° Ð±Ð°ÑˆÐºÐ°Ñ‡Ð° Ð±Ðµ\xadÐ»Ó™Ð½ Ð±ÐµÐ· Ð±ÐµÐ·Ð³Ó™ Ð±ÐµÐ·Ð´Ó™ Ð±ÐµÐ·Ð´Ó™Ð½ Ð±ÐµÐ·Ð½Ðµ Ð±ÐµÐ·Ð½ÐµÒ£ Ð±ÐµÐ·Ð½ÐµÒ£Ñ‡Ó™\nÐ±ÐµÐ»Ð´ÐµÑ€Ò¯ÐµÐ½Ñ‡Ó™ Ð±ÐµÐ»Ó™Ð½ Ð±ÐµÑ€ Ð±ÐµÑ€Ð³Ó™ Ð±ÐµÑ€ÐµÐ½Ñ‡Ðµ Ð±ÐµÑ€ÐµÐ½Ñ‡ÐµÐ³Ó™ Ð±ÐµÑ€ÐµÐ½Ñ‡ÐµÐ´Ó™ Ð±ÐµÑ€ÐµÐ½Ñ‡ÐµÐ´Ó™Ð½ Ð±ÐµÑ€ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€\nÐ±ÐµÑ€ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð³Ó™ Ð±ÐµÑ€ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð´Ó™ Ð±ÐµÑ€ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð´Ó™Ð½ Ð±ÐµÑ€ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð½Ðµ Ð±ÐµÑ€ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð½ÐµÒ£ Ð±ÐµÑ€ÐµÐ½Ñ‡ÐµÐ½Ðµ\nÐ±ÐµÑ€ÐµÐ½Ñ‡ÐµÐ½ÐµÒ£ Ð±ÐµÑ€ÐºÐ°Ð¹Ð´Ð° Ð±ÐµÑ€ÐºÐ°Ð¹ÑÑ‹ Ð±ÐµÑ€ÐºÐ°Ñ Ð±ÐµÑ€ÐºÐ°ÑÐ½ Ð±ÐµÑ€ÐºÐµÐ¼ Ð±ÐµÑ€ÐºÐµÐ¼Ð³Ó™ Ð±ÐµÑ€ÐºÐµÐ¼Ð´Ó™ Ð±ÐµÑ€ÐºÐµÐ¼Ð½Ðµ\nÐ±ÐµÑ€ÐºÐµÐ¼Ð½ÐµÒ£ Ð±ÐµÑ€ÐºÐµÐ¼Ð½Ó™Ð½ Ð±ÐµÑ€Ð»Ó™Ð½ Ð±ÐµÑ€Ð½Ð¸ Ð±ÐµÑ€Ð½Ð¸Ð³Ó™ Ð±ÐµÑ€Ð½Ð¸Ð´Ó™ Ð±ÐµÑ€Ð½Ð¸Ð´Ó™Ð½ Ð±ÐµÑ€Ð½Ð¸Ð½Ð´Ð¸ Ð±ÐµÑ€Ð½Ð¸Ð½Ðµ\nÐ±ÐµÑ€Ð½Ð¸Ð½ÐµÒ£ Ð±ÐµÑ€Ð½Ð¸Ñ‡ÐµÐº Ð±ÐµÑ€Ð½Ð¸Ñ‡Ó™ Ð±ÐµÑ€Ð½Ó™Ñ€ÑÓ™ Ð±ÐµÑ€Ð½Ó™Ñ€ÑÓ™Ð³Ó™ Ð±ÐµÑ€Ð½Ó™Ñ€ÑÓ™Ð´Ó™ Ð±ÐµÑ€Ð½Ó™Ñ€ÑÓ™Ð´Ó™Ð½ Ð±ÐµÑ€Ð½Ó™Ñ€ÑÓ™Ð½Ðµ\nÐ±ÐµÑ€Ð½Ó™Ñ€ÑÓ™Ð½ÐµÒ£ Ð±ÐµÑ€Ñ€Ó™Ñ‚Ñ‚Ó™Ð½ Ð±ÐµÑ€ÑÐµ Ð±ÐµÑ€ÑÐµÐ½ Ð±ÐµÑ€ÑÐµÐ½Ð³Ó™ Ð±ÐµÑ€ÑÐµÐ½Ð´Ó™ Ð±ÐµÑ€ÑÐµÐ½ÐµÒ£ Ð±ÐµÑ€ÑÐµÐ½Ð½Ó™Ð½ Ð±ÐµÑ€Ó™Ñ€\nÐ±ÐµÑ€Ó™Ñ€ÑÐµ Ð±ÐµÑ€Ó™Ñ€ÑÐµÐ½ Ð±ÐµÑ€Ó™Ñ€ÑÐµÐ½Ð´Ó™ Ð±ÐµÑ€Ó™Ñ€ÑÐµÐ½ÐµÒ£ Ð±ÐµÑ€Ó™Ñ€ÑÐµÐ½Ð½Ó™Ð½ Ð±ÐµÑ€Ó™Ñ€ÑÐµÐ½Ó™ Ð±ÐµÑ€Ó™Ò¯ Ð±Ð¸Ð³Ñ€Ó™Ðº Ð±Ð¸Ðº\nÐ±Ð¸Ñ€Ð»Ðµ Ð±Ð¸Ñ‚ Ð±Ð¸Ñˆ Ð±Ð¸ÑˆÐµÐ½Ñ‡Ðµ Ð±Ð¸ÑˆÐµÐ½Ñ‡ÐµÐ³Ó™ Ð±Ð¸ÑˆÐµÐ½Ñ‡ÐµÐ´Ó™ Ð±Ð¸ÑˆÐµÐ½Ñ‡ÐµÐ´Ó™Ð½ Ð±Ð¸ÑˆÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€ Ð±Ð¸ÑˆÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð³Ó™\nÐ±Ð¸ÑˆÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð´Ó™ Ð±Ð¸ÑˆÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð´Ó™Ð½ Ð±Ð¸ÑˆÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð½Ðµ Ð±Ð¸ÑˆÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð½ÐµÒ£ Ð±Ð¸ÑˆÐµÐ½Ñ‡ÐµÐ½Ðµ Ð±Ð¸ÑˆÐµÐ½Ñ‡ÐµÐ½ÐµÒ£\nÐ±Ð¸ÑˆÐ»Ó™Ð¿ Ð±Ð¾Ð»Ð°Ð¹ Ð±Ð¾Ð»Ð°Ñ€ Ð±Ð¾Ð»Ð°Ñ€Ð³Ð° Ð±Ð¾Ð»Ð°Ñ€Ð´Ð° Ð±Ð¾Ð»Ð°Ñ€Ð´Ð°Ð½ Ð±Ð¾Ð»Ð°Ñ€Ð½Ñ‹ Ð±Ð¾Ð»Ð°Ñ€Ð½Ñ‹Ò£ Ð±Ð¾Ð»Ð°Ñ€Ñ‹ Ð±Ð¾Ð»Ð°Ñ€Ñ‹Ð½\nÐ±Ð¾Ð»Ð°Ñ€Ñ‹Ð½Ð³Ð° Ð±Ð¾Ð»Ð°Ñ€Ñ‹Ð½Ð´Ð° Ð±Ð¾Ð»Ð°Ñ€Ñ‹Ð½Ð½Ð°Ð½ Ð±Ð¾Ð»Ð°Ñ€Ñ‹Ð½Ñ‹Ò£ Ð±Ñƒ Ð±ÑƒÐµ Ð±ÑƒÐµÐ½Ð° Ð±ÑƒÐµÐ½Ð´Ð° Ð±ÑƒÐµÐ½Ñ‡Ð° Ð±ÑƒÐ¹Ð»Ð°Ð¿\nÐ±ÑƒÐ»Ð°Ñ€Ð°Ðº Ð±ÑƒÐ»Ð°Ñ‡Ð°Ðº Ð±ÑƒÐ»Ð´Ñ‹ Ð±ÑƒÐ»Ð¼Ñ‹Ð¹ Ð±ÑƒÐ»ÑÐ° Ð±ÑƒÐ»Ñ‹Ð¿ Ð±ÑƒÐ»Ñ‹Ñ€ Ð±ÑƒÐ»Ñ‹Ñ€Ð³Ð° Ð±ÑƒÑÑ‹ Ð±Ò¯Ñ‚Ó™Ð½ Ð±Ó™Ð»ÐºÐ¸ Ð±Ó™Ð½\nÐ±Ó™Ñ€Ð°Ð±Ó™Ñ€ÐµÐ½Ó™ Ð±Ó©Ñ‚ÐµÐ½ Ð±Ó©Ñ‚ÐµÐ½ÐµÑÐµ Ð±Ó©Ñ‚ÐµÐ½ÐµÑÐµÐ½ Ð±Ó©Ñ‚ÐµÐ½ÐµÑÐµÐ½Ð´Ó™ Ð±Ó©Ñ‚ÐµÐ½ÐµÑÐµÐ½ÐµÒ£ Ð±Ó©Ñ‚ÐµÐ½ÐµÑÐµÐ½Ð½Ó™Ð½\nÐ±Ó©Ñ‚ÐµÐ½ÐµÑÐµÐ½Ó™\n\nÐ²Ó™\n\nÐ³ÐµÐ» Ð³ÐµÐ½Ó™ Ð³Ñ‹Ð½Ð° Ð³Ò¯Ñ Ð³Ò¯ÑÐºÐ¸ Ð³Ó™Ñ€Ñ‡Ó™\n\nÐ´Ð° Ð´Ð¸ Ð´Ð¸Ð³Ó™Ð½ Ð´Ð¸Ð´Ðµ Ð´Ð¸Ð¿ Ð´Ð¸ÑÑ‚Ó™Ð»Ó™Ð³Ó™Ð½ Ð´Ð¸ÑÑ‚Ó™Ð»Ó™Ñ€Ñ‡Ó™ Ð´Ò¯Ñ€Ñ‚ Ð´Ò¯Ñ€Ñ‚ÐµÐ½Ñ‡Ðµ Ð´Ò¯Ñ€Ñ‚ÐµÐ½Ñ‡ÐµÐ³Ó™ Ð´Ò¯Ñ€Ñ‚ÐµÐ½Ñ‡ÐµÐ´Ó™\nÐ´Ò¯Ñ€Ñ‚ÐµÐ½Ñ‡ÐµÐ´Ó™Ð½ Ð´Ò¯Ñ€Ñ‚ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€ Ð´Ò¯Ñ€Ñ‚ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð³Ó™ Ð´Ò¯Ñ€Ñ‚ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð´Ó™ Ð´Ò¯Ñ€Ñ‚ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð´Ó™Ð½ Ð´Ò¯Ñ€Ñ‚ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð½Ðµ\nÐ´Ò¯Ñ€Ñ‚ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð½ÐµÒ£ Ð´Ò¯Ñ€Ñ‚ÐµÐ½Ñ‡ÐµÐ½Ðµ Ð´Ò¯Ñ€Ñ‚ÐµÐ½Ñ‡ÐµÐ½ÐµÒ£ Ð´Ò¯Ñ€Ñ‚Ð»Ó™Ð¿ Ð´Ó™\n\nÐµÐ³ÐµÑ€Ð¼Ðµ ÐµÐ³ÐµÑ€Ð¼ÐµÐ½Ñ‡Ðµ ÐµÐ³ÐµÑ€Ð¼ÐµÐ½Ñ‡ÐµÐ³Ó™ ÐµÐ³ÐµÑ€Ð¼ÐµÐ½Ñ‡ÐµÐ´Ó™ ÐµÐ³ÐµÑ€Ð¼ÐµÐ½Ñ‡ÐµÐ´Ó™Ð½ ÐµÐ³ÐµÑ€Ð¼ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€\nÐµÐ³ÐµÑ€Ð¼ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð³Ó™ ÐµÐ³ÐµÑ€Ð¼ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð´Ó™ ÐµÐ³ÐµÑ€Ð¼ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð´Ó™Ð½ ÐµÐ³ÐµÑ€Ð¼ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð½Ðµ ÐµÐ³ÐµÑ€Ð¼ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð½ÐµÒ£\nÐµÐ³ÐµÑ€Ð¼ÐµÐ½Ñ‡ÐµÐ½Ðµ ÐµÐ³ÐµÑ€Ð¼ÐµÐ½Ñ‡ÐµÐ½ÐµÒ£ ÐµÐ» ÐµÐ»Ð´Ð°\n\nÐ¸Ð´Ðµ Ð¸Ð´ÐµÐº Ð¸Ð´ÐµÐ¼ Ð¸ÐºÐµ Ð¸ÐºÐµÐ½Ñ‡Ðµ Ð¸ÐºÐµÐ½Ñ‡ÐµÐ³Ó™ Ð¸ÐºÐµÐ½Ñ‡ÐµÐ´Ó™ Ð¸ÐºÐµÐ½Ñ‡ÐµÐ´Ó™Ð½ Ð¸ÐºÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€ Ð¸ÐºÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð³Ó™\nÐ¸ÐºÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð´Ó™ Ð¸ÐºÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð´Ó™Ð½ Ð¸ÐºÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð½Ðµ Ð¸ÐºÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð½ÐµÒ£ Ð¸ÐºÐµÐ½Ñ‡ÐµÐ½Ðµ Ð¸ÐºÐµÐ½Ñ‡ÐµÐ½ÐµÒ£ Ð¸ÐºÐµÑˆÓ™Ñ€ Ð¸ÐºÓ™Ð½\nÐ¸Ð»Ð»Ðµ Ð¸Ð»Ð»ÐµÐ½Ñ‡Ðµ Ð¸Ð»Ð»ÐµÐ½Ñ‡ÐµÐ³Ó™ Ð¸Ð»Ð»ÐµÐ½Ñ‡ÐµÐ´Ó™ Ð¸Ð»Ð»ÐµÐ½Ñ‡ÐµÐ´Ó™Ð½ Ð¸Ð»Ð»ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€ Ð¸Ð»Ð»ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð³Ó™\nÐ¸Ð»Ð»ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð´Ó™ Ð¸Ð»Ð»ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð´Ó™Ð½ Ð¸Ð»Ð»ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð½Ðµ Ð¸Ð»Ð»ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð½ÐµÒ£ Ð¸Ð»Ð»ÐµÐ½Ñ‡ÐµÐ½Ðµ Ð¸Ð»Ð»ÐµÐ½Ñ‡ÐµÐ½ÐµÒ£ Ð¸Ð»Ó™\nÐ¸Ð»Ó™Ð½ Ð¸Ð½Ð´Ðµ Ð¸ÑÓ™ Ð¸Ñ‚ÐµÐ¿ Ð¸Ñ‚ÐºÓ™Ð½ Ð¸Ñ‚Ñ‚Ðµ Ð¸Ñ‚Ò¯ Ð¸Ñ‚Ó™ Ð¸Ñ‚Ó™Ñ€Ð³Ó™ Ð¸Ò£\n\nÐ¹Ó©Ð· Ð¹Ó©Ð·ÐµÐ½Ñ‡Ðµ Ð¹Ó©Ð·ÐµÐ½Ñ‡ÐµÐ³Ó™ Ð¹Ó©Ð·ÐµÐ½Ñ‡ÐµÐ´Ó™ Ð¹Ó©Ð·ÐµÐ½Ñ‡ÐµÐ´Ó™Ð½ Ð¹Ó©Ð·ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€ Ð¹Ó©Ð·ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð³Ó™ Ð¹Ó©Ð·ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð´Ó™\nÐ¹Ó©Ð·ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð´Ó™Ð½ Ð¹Ó©Ð·ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð½Ðµ Ð¹Ó©Ð·ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð½ÐµÒ£ Ð¹Ó©Ð·ÐµÐ½Ñ‡ÐµÐ½Ðµ Ð¹Ó©Ð·ÐµÐ½Ñ‡ÐµÐ½ÐµÒ£ Ð¹Ó©Ð·Ð»Ó™Ð³Ó™Ð½ Ð¹Ó©Ð·Ð»Ó™Ñ€Ñ‡Ó™\nÐ¹Ó©Ð·Ó™Ñ€Ð»Ó™Ð³Ó™Ð½\n\nÐºÐ°Ð´Ó™Ñ€ ÐºÐ°Ð¹ ÐºÐ°Ð¹Ð±ÐµÑ€ ÐºÐ°Ð¹Ð±ÐµÑ€Ð»Ó™Ñ€Ðµ ÐºÐ°Ð¹Ð±ÐµÑ€ÑÐµ ÐºÐ°Ð¹Ð±ÐµÑ€Ó™Ò¯ ÐºÐ°Ð¹Ð±ÐµÑ€Ó™Ò¯Ð³Ó™ ÐºÐ°Ð¹Ð±ÐµÑ€Ó™Ò¯Ð´Ó™ ÐºÐ°Ð¹Ð±ÐµÑ€Ó™Ò¯Ð´Ó™Ð½\nÐºÐ°Ð¹Ð±ÐµÑ€Ó™Ò¯Ð½Ðµ ÐºÐ°Ð¹Ð±ÐµÑ€Ó™Ò¯Ð½ÐµÒ£ ÐºÐ°Ð¹Ð´Ð°Ð³Ñ‹ ÐºÐ°Ð¹ÑÑ‹ ÐºÐ°Ð¹ÑÑ‹Ð±ÐµÑ€ ÐºÐ°Ð¹ÑÑ‹Ð½ ÐºÐ°Ð¹ÑÑ‹Ð½Ð° ÐºÐ°Ð¹ÑÑ‹Ð½Ð´Ð° ÐºÐ°Ð¹ÑÑ‹Ð½Ð½Ð°Ð½\nÐºÐ°Ð¹ÑÑ‹Ð½Ñ‹Ò£ ÐºÐ°Ð¹Ñ‡Ð°Ð½Ð³Ñ‹ ÐºÐ°Ð¹Ñ‡Ð°Ð½Ð´Ð°Ð³Ñ‹ ÐºÐ°Ð¹Ñ‡Ð°Ð½Ð½Ð°Ð½ ÐºÐ°Ñ€Ð°Ð³Ð°Ð½Ð´Ð° ÐºÐ°Ñ€Ð°Ð¼Ð°ÑÑ‚Ð°Ð½ ÐºÐ°Ñ€Ð°Ð¼Ñ‹Ð¹ ÐºÐ°Ñ€Ð°Ñ‚Ð° ÐºÐ°Ñ€ÑˆÑ‹\nÐºÐ°Ñ€ÑˆÑ‹Ð½Ð° ÐºÐ°Ñ€ÑˆÑ‹Ð½Ð´Ð° ÐºÐ°Ñ€ÑˆÑ‹Ð½Ð´Ð°Ð³Ñ‹ ÐºÐµÐ±ÐµÐº ÐºÐµÐ¼ ÐºÐµÐ¼Ð³Ó™ ÐºÐµÐ¼Ð´Ó™ ÐºÐµÐ¼Ð½Ðµ ÐºÐµÐ¼Ð½ÐµÒ£ ÐºÐµÐ¼Ð½Ó™Ð½ ÐºÐµÐ½Ó™ ÐºÐ¸\nÐºÐ¸Ð»ÐµÐ¿ ÐºÐ¸Ð»Ó™ ÐºÐ¸Ñ€Ó™Ðº ÐºÑ‹Ð½Ð° ÐºÑ‹Ñ€Ñ‹Ð³Ñ‹Ð½Ñ‡Ñ‹ ÐºÑ‹Ñ€Ñ‹Ð³Ñ‹Ð½Ñ‡Ñ‹Ð³Ð° ÐºÑ‹Ñ€Ñ‹Ð³Ñ‹Ð½Ñ‡Ñ‹Ð´Ð° ÐºÑ‹Ñ€Ñ‹Ð³Ñ‹Ð½Ñ‡Ñ‹Ð´Ð°Ð½\nÐºÑ‹Ñ€Ñ‹Ð³Ñ‹Ð½Ñ‡Ñ‹Ð»Ð°Ñ€ ÐºÑ‹Ñ€Ñ‹Ð³Ñ‹Ð½Ñ‡Ñ‹Ð»Ð°Ñ€Ð³Ð° ÐºÑ‹Ñ€Ñ‹Ð³Ñ‹Ð½Ñ‡Ñ‹Ð»Ð°Ñ€Ð´Ð° ÐºÑ‹Ñ€Ñ‹Ð³Ñ‹Ð½Ñ‡Ñ‹Ð»Ð°Ñ€Ð´Ð°Ð½ ÐºÑ‹Ñ€Ñ‹Ð³Ñ‹Ð½Ñ‡Ñ‹Ð»Ð°Ñ€Ð½Ñ‹\nÐºÑ‹Ñ€Ñ‹Ð³Ñ‹Ð½Ñ‡Ñ‹Ð»Ð°Ñ€Ð½Ñ‹Ò£ ÐºÑ‹Ñ€Ñ‹Ð³Ñ‹Ð½Ñ‡Ñ‹Ð½Ñ‹ ÐºÑ‹Ñ€Ñ‹Ð³Ñ‹Ð½Ñ‡Ñ‹Ð½Ñ‹Ò£ ÐºÑ‹Ñ€Ñ‹Ðº ÐºÒ¯Ðº ÐºÒ¯Ð¿Ð»Ó™Ð³Ó™Ð½ ÐºÒ¯Ð¿Ð¼Ðµ ÐºÒ¯Ð¿Ð¼ÐµÐ»Ó™Ð¿\nÐºÒ¯Ð¿Ð¼ÐµÑˆÓ™Ñ€ ÐºÒ¯Ð¿Ð¼ÐµÑˆÓ™Ñ€Ð»Ó™Ð¿ ÐºÒ¯Ð¿Ñ‚Ó™Ð½ ÐºÒ¯Ñ€Ó™\n\nÐ»Ó™ÐºÐ¸Ð½\n\nÐ¼Ð°ÐºÑÐ°Ñ‚Ñ‹Ð½Ð´Ð° Ð¼ÐµÐ½Ó™ Ð¼ÐµÒ£ Ð¼ÐµÒ£ÐµÐ½Ñ‡Ðµ Ð¼ÐµÒ£ÐµÐ½Ñ‡ÐµÐ³Ó™ Ð¼ÐµÒ£ÐµÐ½Ñ‡ÐµÐ´Ó™ Ð¼ÐµÒ£ÐµÐ½Ñ‡ÐµÐ´Ó™Ð½ Ð¼ÐµÒ£ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€\nÐ¼ÐµÒ£ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð³Ó™ Ð¼ÐµÒ£ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð´Ó™ Ð¼ÐµÒ£ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð´Ó™Ð½ Ð¼ÐµÒ£ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð½Ðµ Ð¼ÐµÒ£ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð½ÐµÒ£ Ð¼ÐµÒ£ÐµÐ½Ñ‡ÐµÐ½Ðµ\nÐ¼ÐµÒ£ÐµÐ½Ñ‡ÐµÐ½ÐµÒ£ Ð¼ÐµÒ£Ð»Ó™Ð³Ó™Ð½ Ð¼ÐµÒ£Ð»Ó™Ð¿ Ð¼ÐµÒ£Ð½Ó™Ñ€Ñ‡Ó™ Ð¼ÐµÒ£Ó™Ñ€Ð»Ó™Ð³Ó™Ð½ Ð¼ÐµÒ£Ó™Ñ€Ð»Ó™Ð¿ Ð¼Ð¸Ð»Ð»Ð¸Ð°Ñ€Ð´ Ð¼Ð¸Ð»Ð»Ð¸Ð°Ñ€Ð´Ð»Ð°Ð³Ð°Ð½\nÐ¼Ð¸Ð»Ð»Ð¸Ð°Ñ€Ð´Ð»Ð°Ñ€Ñ‡Ð° Ð¼Ð¸Ð»Ð»Ð¸Ð¾Ð½ Ð¼Ð¸Ð»Ð»Ð¸Ð¾Ð½Ð»Ð°Ð³Ð°Ð½ Ð¼Ð¸Ð»Ð»Ð¸Ð¾Ð½Ð½Ð°Ñ€Ñ‡Ð° Ð¼Ð¸Ð»Ð»Ð¸Ð¾Ð½Ñ‹Ð½Ñ‡Ñ‹ Ð¼Ð¸Ð»Ð»Ð¸Ð¾Ð½Ñ‹Ð½Ñ‡Ñ‹Ð³Ð°\nÐ¼Ð¸Ð»Ð»Ð¸Ð¾Ð½Ñ‹Ð½Ñ‡Ñ‹Ð´Ð° Ð¼Ð¸Ð»Ð»Ð¸Ð¾Ð½Ñ‹Ð½Ñ‡Ñ‹Ð´Ð°Ð½ Ð¼Ð¸Ð»Ð»Ð¸Ð¾Ð½Ñ‹Ð½Ñ‡Ñ‹Ð»Ð°Ñ€ Ð¼Ð¸Ð»Ð»Ð¸Ð¾Ð½Ñ‹Ð½Ñ‡Ñ‹Ð»Ð°Ñ€Ð³Ð° Ð¼Ð¸Ð»Ð»Ð¸Ð¾Ð½Ñ‹Ð½Ñ‡Ñ‹Ð»Ð°Ñ€Ð´Ð°\nÐ¼Ð¸Ð»Ð»Ð¸Ð¾Ð½Ñ‹Ð½Ñ‡Ñ‹Ð»Ð°Ñ€Ð´Ð°Ð½ Ð¼Ð¸Ð»Ð»Ð¸Ð¾Ð½Ñ‹Ð½Ñ‡Ñ‹Ð»Ð°Ñ€Ð½Ñ‹ Ð¼Ð¸Ð»Ð»Ð¸Ð¾Ð½Ñ‹Ð½Ñ‡Ñ‹Ð»Ð°Ñ€Ð½Ñ‹Ò£ Ð¼Ð¸Ð»Ð»Ð¸Ð¾Ð½Ñ‹Ð½Ñ‡Ñ‹Ð½Ñ‹\nÐ¼Ð¸Ð»Ð»Ð¸Ð¾Ð½Ñ‹Ð½Ñ‡Ñ‹Ð½Ñ‹Ò£ Ð¼Ð¸Ð½ Ð¼Ð¸Ð½Ð´Ó™ Ð¼Ð¸Ð½Ðµ Ð¼Ð¸Ð½ÐµÐ¼ Ð¼Ð¸Ð½ÐµÐ¼Ñ‡Ó™ Ð¼Ð¸Ð½Ð½Ó™Ð½ Ð¼Ð¸Ò£Ð° Ð¼Ð¾Ð½Ð´Ð° Ð¼Ð¾Ð½Ð´Ð°Ð³Ñ‹ Ð¼Ð¾Ð½Ð´Ñ‹Ðµ\nÐ¼Ð¾Ð½Ð´Ñ‹ÐµÐ½ Ð¼Ð¾Ð½Ð´Ñ‹ÐµÐ½Ð³Ó™ Ð¼Ð¾Ð½Ð´Ñ‹ÐµÐ½Ð´Ó™ Ð¼Ð¾Ð½Ð´Ñ‹ÐµÐ½Ð½Ó™Ð½ Ð¼Ð¾Ð½Ð´Ñ‹ÐµÐ½Ñ‹Ò£ Ð¼Ð¾Ð½Ð´Ñ‹Ð¹ Ð¼Ð¾Ð½Ð´Ñ‹Ð¹Ð³Ð° Ð¼Ð¾Ð½Ð´Ñ‹Ð¹Ð´Ð°\nÐ¼Ð¾Ð½Ð´Ñ‹Ð¹Ð´Ð°Ð½ Ð¼Ð¾Ð½Ð´Ñ‹Ð¹Ð»Ð°Ñ€ Ð¼Ð¾Ð½Ð´Ñ‹Ð¹Ð»Ð°Ñ€Ð³Ð° Ð¼Ð¾Ð½Ð´Ñ‹Ð¹Ð»Ð°Ñ€Ð´Ð° Ð¼Ð¾Ð½Ð´Ñ‹Ð¹Ð»Ð°Ñ€Ð´Ð°Ð½ Ð¼Ð¾Ð½Ð´Ñ‹Ð¹Ð»Ð°Ñ€Ð½Ñ‹\nÐ¼Ð¾Ð½Ð´Ñ‹Ð¹Ð»Ð°Ñ€Ð½Ñ‹Ò£ Ð¼Ð¾Ð½Ð´Ñ‹Ð¹Ð»Ð°Ñ€Ñ‹ Ð¼Ð¾Ð½Ð´Ñ‹Ð¹Ð»Ð°Ñ€Ñ‹Ð½ Ð¼Ð¾Ð½Ð´Ñ‹Ð¹Ð»Ð°Ñ€Ñ‹Ð½Ð³Ð° Ð¼Ð¾Ð½Ð´Ñ‹Ð¹Ð»Ð°Ñ€Ñ‹Ð½Ð´Ð° Ð¼Ð¾Ð½Ð´Ñ‹Ð¹Ð»Ð°Ñ€Ñ‹Ð½Ð½Ð°Ð½\nÐ¼Ð¾Ð½Ð´Ñ‹Ð¹Ð»Ð°Ñ€Ñ‹Ð½Ñ‹Ò£ Ð¼Ð¾Ð½Ð´Ñ‹Ð¹Ð½Ñ‹ Ð¼Ð¾Ð½Ð´Ñ‹Ð¹Ð½Ñ‹Ò£ Ð¼Ð¾Ð½Ð½Ð°Ð½ Ð¼Ð¾Ð½ÑÑ‹Ð· Ð¼Ð¾Ð½Ñ‡Ð° Ð¼Ð¾Ð½Ñ‹ Ð¼Ð¾Ð½Ñ‹ÐºÑ‹ Ð¼Ð¾Ð½Ñ‹ÐºÑ‹Ð½\nÐ¼Ð¾Ð½Ñ‹ÐºÑ‹Ð½Ð³Ð° Ð¼Ð¾Ð½Ñ‹ÐºÑ‹Ð½Ð´Ð° Ð¼Ð¾Ð½Ñ‹ÐºÑ‹Ð½Ð½Ð°Ð½ Ð¼Ð¾Ð½Ñ‹ÐºÑ‹Ð½Ñ‹Ò£ Ð¼Ð¾Ð½Ñ‹ÑÑ‹ Ð¼Ð¾Ð½Ñ‹ÑÑ‹Ð½ Ð¼Ð¾Ð½Ñ‹ÑÑ‹Ð½Ð³Ð° Ð¼Ð¾Ð½Ñ‹ÑÑ‹Ð½Ð´Ð°\nÐ¼Ð¾Ð½Ñ‹ÑÑ‹Ð½Ð½Ð°Ð½ Ð¼Ð¾Ð½Ñ‹ÑÑ‹Ð½Ñ‹Ò£ Ð¼Ð¾Ð½Ñ‹Ò£ Ð¼Ð¾Ò£Ð° Ð¼Ð¾Ò£Ð°Ñ€ Ð¼Ð¾Ò£Ð°Ñ€Ð³Ð° Ð¼Ó™Ð³ÑŠÐ»Ò¯Ð¼Ð°Ñ‚Ñ‹Ð½Ñ‡Ð° Ð¼Ó™Ð³Ó™Ñ€ Ð¼Ó™Ð½ Ð¼Ó©Ð¼ÐºÐ¸Ð½\n\nÐ½Ð¸ Ð½Ð¸Ð±Ð°Ñ€Ñ‹ÑÑ‹ Ð½Ð¸ÐºÐ°Ð´Ó™Ñ€Ðµ Ð½Ð¸Ð½Ð´Ð¸ Ð½Ð¸Ð½Ð´Ð¸Ðµ Ð½Ð¸Ð½Ð´Ð¸ÐµÐ½ Ð½Ð¸Ð½Ð´Ð¸ÐµÐ½Ð³Ó™ Ð½Ð¸Ð½Ð´Ð¸ÐµÐ½Ð´Ó™ Ð½Ð¸Ð½Ð´Ð¸ÐµÐ½ÐµÒ£\nÐ½Ð¸Ð½Ð´Ð¸ÐµÐ½Ð½Ó™Ð½ Ð½Ð¸Ð½Ð´Ð¸Ð»Ó™Ñ€ Ð½Ð¸Ð½Ð´Ð¸Ð»Ó™Ñ€Ð³Ó™ Ð½Ð¸Ð½Ð´Ð¸Ð»Ó™Ñ€Ð´Ó™ Ð½Ð¸Ð½Ð´Ð¸Ð»Ó™Ñ€Ð´Ó™Ð½ Ð½Ð¸Ð½Ð´Ð¸Ð»Ó™Ñ€ÐµÐ½ Ð½Ð¸Ð½Ð´Ð¸Ð»Ó™Ñ€ÐµÐ½Ð½\nÐ½Ð¸Ð½Ð´Ð¸Ð»Ó™Ñ€ÐµÐ½Ð½Ð³Ó™ Ð½Ð¸Ð½Ð´Ð¸Ð»Ó™Ñ€ÐµÐ½Ð½Ð´Ó™ Ð½Ð¸Ð½Ð´Ð¸Ð»Ó™Ñ€ÐµÐ½Ð½ÐµÒ£ Ð½Ð¸Ð½Ð´Ð¸Ð»Ó™Ñ€ÐµÐ½Ð½Ð½Ó™Ð½ Ð½Ð¸Ð½Ð´Ð¸Ð»Ó™Ñ€Ð½Ðµ Ð½Ð¸Ð½Ð´Ð¸Ð»Ó™Ñ€Ð½ÐµÒ£\nÐ½Ð¸Ð½Ð´Ð¸Ñ€Ó™Ðº Ð½Ð¸Ñ…Ó™Ñ‚Ð»Ðµ Ð½Ð¸Ñ‡Ð°ÐºÐ»Ñ‹ Ð½Ð¸Ñ‡ÐµÐº Ð½Ð¸Ñ‡Ó™ÑˆÓ™Ñ€ Ð½Ð¸Ñ‡Ó™ÑˆÓ™Ñ€Ð»Ó™Ð¿ Ð½ÑƒÐ»ÑŒ Ð½Ñ‡Ðµ Ð½Ñ‡Ñ‹ Ð½Ó™Ñ€ÑÓ™ Ð½Ó™Ñ€ÑÓ™Ð³Ó™\nÐ½Ó™Ñ€ÑÓ™Ð´Ó™ Ð½Ó™Ñ€ÑÓ™Ð´Ó™Ð½ Ð½Ó™Ñ€ÑÓ™Ð½Ðµ Ð½Ó™Ñ€ÑÓ™Ð½ÐµÒ£\n\nÑÐ°ÐµÐ½ ÑÐµÐ· ÑÐµÐ·Ð³Ó™ ÑÐµÐ·Ð´Ó™ ÑÐµÐ·Ð´Ó™Ð½ ÑÐµÐ·Ð½Ðµ ÑÐµÐ·Ð½ÐµÒ£ ÑÐµÐ·Ð½ÐµÒ£Ñ‡Ó™ ÑÐ¸Ð³ÐµÐ· ÑÐ¸Ð³ÐµÐ·ÐµÐ½Ñ‡Ðµ ÑÐ¸Ð³ÐµÐ·ÐµÐ½Ñ‡ÐµÐ³Ó™\nÑÐ¸Ð³ÐµÐ·ÐµÐ½Ñ‡ÐµÐ´Ó™ ÑÐ¸Ð³ÐµÐ·ÐµÐ½Ñ‡ÐµÐ´Ó™Ð½ ÑÐ¸Ð³ÐµÐ·ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€ ÑÐ¸Ð³ÐµÐ·ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð³Ó™ ÑÐ¸Ð³ÐµÐ·ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð´Ó™\nÑÐ¸Ð³ÐµÐ·ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð´Ó™Ð½ ÑÐ¸Ð³ÐµÐ·ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð½Ðµ ÑÐ¸Ð³ÐµÐ·ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð½ÐµÒ£ ÑÐ¸Ð³ÐµÐ·ÐµÐ½Ñ‡ÐµÐ½Ðµ ÑÐ¸Ð³ÐµÐ·ÐµÐ½Ñ‡ÐµÐ½ÐµÒ£\nÑÐ¸ÐºÑÓ™Ð½ ÑÐ¸Ð½ ÑÐ¸Ð½Ð´Ó™ ÑÐ¸Ð½Ðµ ÑÐ¸Ð½ÐµÒ£ ÑÐ¸Ð½ÐµÒ£Ñ‡Ó™ ÑÐ¸Ð½Ð½Ó™Ð½ ÑÐ¸Ò£Ð° ÑÐ¾Ò£ ÑÑ‹Ð¼Ð°Ð½ ÑÒ¯Ð·ÐµÐ½Ñ‡Ó™ ÑÒ¯Ð·Ð»Ó™Ñ€ÐµÐ½Ñ‡Ó™\n\nÑ‚Ð° Ñ‚Ð°Ð±Ð° Ñ‚ÐµÐ³Ðµ Ñ‚ÐµÐ³ÐµÐ»Ó™Ð¹ Ñ‚ÐµÐ³ÐµÐ»Ó™Ñ€ Ñ‚ÐµÐ³ÐµÐ»Ó™Ñ€Ð³Ó™ Ñ‚ÐµÐ³ÐµÐ»Ó™Ñ€Ð´Ó™ Ñ‚ÐµÐ³ÐµÐ»Ó™Ñ€Ð´Ó™Ð½ Ñ‚ÐµÐ³ÐµÐ»Ó™Ñ€Ðµ Ñ‚ÐµÐ³ÐµÐ»Ó™Ñ€ÐµÐ½\nÑ‚ÐµÐ³ÐµÐ»Ó™Ñ€ÐµÐ½Ð³Ó™ Ñ‚ÐµÐ³ÐµÐ»Ó™Ñ€ÐµÐ½Ð´Ó™ Ñ‚ÐµÐ³ÐµÐ»Ó™Ñ€ÐµÐ½ÐµÒ£ Ñ‚ÐµÐ³ÐµÐ»Ó™Ñ€ÐµÐ½Ð½Ó™Ð½ Ñ‚ÐµÐ³ÐµÐ»Ó™Ñ€Ð½Ðµ Ñ‚ÐµÐ³ÐµÐ»Ó™Ñ€Ð½ÐµÒ£ Ñ‚ÐµÐ³ÐµÐ½Ð´Ð¸\nÑ‚ÐµÐ³ÐµÐ½Ð´Ð¸Ð³Ó™ Ñ‚ÐµÐ³ÐµÐ½Ð´Ð¸Ð´Ó™ Ñ‚ÐµÐ³ÐµÐ½Ð´Ð¸Ð´Ó™Ð½ Ñ‚ÐµÐ³ÐµÐ½Ð´Ð¸Ð½Ðµ Ñ‚ÐµÐ³ÐµÐ½Ð´Ð¸Ð½ÐµÒ£ Ñ‚ÐµÐ³ÐµÐ½Ð´Ó™ Ñ‚ÐµÐ³ÐµÐ½Ð´Ó™Ð³Ðµ Ñ‚ÐµÐ³ÐµÐ½Ðµ\nÑ‚ÐµÐ³ÐµÐ½ÐµÐºÐµ Ñ‚ÐµÐ³ÐµÐ½ÐµÐºÐµÐ½ Ñ‚ÐµÐ³ÐµÐ½ÐµÐºÐµÐ½Ð³Ó™ Ñ‚ÐµÐ³ÐµÐ½ÐµÐºÐµÐ½Ð´Ó™ Ñ‚ÐµÐ³ÐµÐ½ÐµÐºÐµÐ½ÐµÒ£ Ñ‚ÐµÐ³ÐµÐ½ÐµÐºÐµÐ½Ð½Ó™Ð½ Ñ‚ÐµÐ³ÐµÐ½ÐµÒ£\nÑ‚ÐµÐ³ÐµÐ½Ð½Ó™Ð½ Ñ‚ÐµÐ³ÐµÑÐµ Ñ‚ÐµÐ³ÐµÑÐµÐ½ Ñ‚ÐµÐ³ÐµÑÐµÐ½Ð³Ó™ Ñ‚ÐµÐ³ÐµÑÐµÐ½Ð´Ó™ Ñ‚ÐµÐ³ÐµÑÐµÐ½ÐµÒ£ Ñ‚ÐµÐ³ÐµÑÐµÐ½Ð½Ó™Ð½ Ñ‚ÐµÐ³ÐµÒ£Ó™ Ñ‚Ð¸ÐµÑˆ Ñ‚Ð¸Ðº\nÑ‚Ð¸ÐºÐ»Ðµ Ñ‚Ð¾Ñ€Ð° Ñ‚Ñ€Ð¸Ð»Ð»Ð¸Ð°Ñ€Ð´ Ñ‚Ñ€Ð¸Ð»Ð»Ð¸Ð¾Ð½ Ñ‚ÑƒÐ³Ñ‹Ð· Ñ‚ÑƒÐ³Ñ‹Ð·Ð»Ð°Ð¿ Ñ‚ÑƒÐ³Ñ‹Ð·Ð»Ð°ÑˆÑ‹Ð¿ Ñ‚ÑƒÐ³Ñ‹Ð·Ñ‹Ð½Ñ‡Ñ‹ Ñ‚ÑƒÐ³Ñ‹Ð·Ñ‹Ð½Ñ‡Ñ‹Ð³Ð°\nÑ‚ÑƒÐ³Ñ‹Ð·Ñ‹Ð½Ñ‡Ñ‹Ð´Ð° Ñ‚ÑƒÐ³Ñ‹Ð·Ñ‹Ð½Ñ‡Ñ‹Ð´Ð°Ð½ Ñ‚ÑƒÐ³Ñ‹Ð·Ñ‹Ð½Ñ‡Ñ‹Ð»Ð°Ñ€ Ñ‚ÑƒÐ³Ñ‹Ð·Ñ‹Ð½Ñ‡Ñ‹Ð»Ð°Ñ€Ð³Ð° Ñ‚ÑƒÐ³Ñ‹Ð·Ñ‹Ð½Ñ‡Ñ‹Ð»Ð°Ñ€Ð´Ð°\nÑ‚ÑƒÐ³Ñ‹Ð·Ñ‹Ð½Ñ‡Ñ‹Ð»Ð°Ñ€Ð´Ð°Ð½ Ñ‚ÑƒÐ³Ñ‹Ð·Ñ‹Ð½Ñ‡Ñ‹Ð»Ð°Ñ€Ð½Ñ‹ Ñ‚ÑƒÐ³Ñ‹Ð·Ñ‹Ð½Ñ‡Ñ‹Ð»Ð°Ñ€Ð½Ñ‹Ò£ Ñ‚ÑƒÐ³Ñ‹Ð·Ñ‹Ð½Ñ‡Ñ‹Ð½Ñ‹ Ñ‚ÑƒÐ³Ñ‹Ð·Ñ‹Ð½Ñ‡Ñ‹Ð½Ñ‹Ò£ Ñ‚ÑƒÐºÑÐ°Ð½\nÑ‚ÑƒÐºÑÐ°Ð½Ñ‹Ð½Ñ‡Ñ‹ Ñ‚ÑƒÐºÑÐ°Ð½Ñ‹Ð½Ñ‡Ñ‹Ð³Ð° Ñ‚ÑƒÐºÑÐ°Ð½Ñ‹Ð½Ñ‡Ñ‹Ð´Ð° Ñ‚ÑƒÐºÑÐ°Ð½Ñ‹Ð½Ñ‡Ñ‹Ð´Ð°Ð½ Ñ‚ÑƒÐºÑÐ°Ð½Ñ‹Ð½Ñ‡Ñ‹Ð»Ð°Ñ€ Ñ‚ÑƒÐºÑÐ°Ð½Ñ‹Ð½Ñ‡Ñ‹Ð»Ð°Ñ€Ð³Ð°\nÑ‚ÑƒÐºÑÐ°Ð½Ñ‹Ð½Ñ‡Ñ‹Ð»Ð°Ñ€Ð´Ð° Ñ‚ÑƒÐºÑÐ°Ð½Ñ‹Ð½Ñ‡Ñ‹Ð»Ð°Ñ€Ð´Ð°Ð½ Ñ‚ÑƒÐºÑÐ°Ð½Ñ‹Ð½Ñ‡Ñ‹Ð»Ð°Ñ€Ð½Ñ‹ Ñ‚ÑƒÐºÑÐ°Ð½Ñ‹Ð½Ñ‡Ñ‹Ð»Ð°Ñ€Ð½Ñ‹Ò£ Ñ‚ÑƒÐºÑÐ°Ð½Ñ‹Ð½Ñ‡Ñ‹Ð½Ñ‹\nÑ‚ÑƒÐºÑÐ°Ð½Ñ‹Ð½Ñ‡Ñ‹Ð½Ñ‹Ò£ Ñ‚ÑƒÑ€Ñ‹Ð½Ð´Ð° Ñ‚Ñ‹Ñˆ Ñ‚Ò¯Ð³ÐµÐ» Ñ‚Ó™ Ñ‚Ó™Ð³Ð°ÐµÐ½Ð»Ó™Ð½Ð³Ó™Ð½ Ñ‚Ó©Ð¼Ó™Ð½\n\nÑƒÐµÐ½Ñ‡Ð° ÑƒÐ¹Ð»Ð°Ð²Ñ‹Ð½Ñ‡Ð° ÑƒÐº ÑƒÐ» ÑƒÐ½ ÑƒÐ½Ð°Ð»Ñ‚Ñ‹ ÑƒÐ½Ð°Ð»Ñ‚Ñ‹Ð½Ñ‡Ñ‹ ÑƒÐ½Ð°Ð»Ñ‚Ñ‹Ð½Ñ‡Ñ‹Ð³Ð° ÑƒÐ½Ð°Ð»Ñ‚Ñ‹Ð½Ñ‡Ñ‹Ð´Ð° ÑƒÐ½Ð°Ð»Ñ‚Ñ‹Ð½Ñ‡Ñ‹Ð´Ð°Ð½\nÑƒÐ½Ð°Ð»Ñ‚Ñ‹Ð½Ñ‡Ñ‹Ð»Ð°Ñ€ ÑƒÐ½Ð°Ð»Ñ‚Ñ‹Ð½Ñ‡Ñ‹Ð»Ð°Ñ€Ð³Ð° ÑƒÐ½Ð°Ð»Ñ‚Ñ‹Ð½Ñ‡Ñ‹Ð»Ð°Ñ€Ð´Ð° ÑƒÐ½Ð°Ð»Ñ‚Ñ‹Ð½Ñ‡Ñ‹Ð»Ð°Ñ€Ð´Ð°Ð½ ÑƒÐ½Ð°Ð»Ñ‚Ñ‹Ð½Ñ‡Ñ‹Ð»Ð°Ñ€Ð½Ñ‹\nÑƒÐ½Ð°Ð»Ñ‚Ñ‹Ð½Ñ‡Ñ‹Ð»Ð°Ñ€Ð½Ñ‹Ò£ ÑƒÐ½Ð°Ð»Ñ‚Ñ‹Ð½Ñ‡Ñ‹Ð½Ñ‹ ÑƒÐ½Ð°Ð»Ñ‚Ñ‹Ð½Ñ‡Ñ‹Ð½Ñ‹Ò£ ÑƒÐ½Ð°Ñ€Ð»Ð°Ð³Ð°Ð½ ÑƒÐ½Ð°Ñ€Ð»Ð°Ð¿ ÑƒÐ½Ð°ÑƒÐ»Ð° ÑƒÐ½Ð°ÑƒÐ»Ð°Ð¿ ÑƒÐ½Ð±ÐµÑ€\nÑƒÐ½Ð±ÐµÑ€ÐµÐ½Ñ‡Ðµ ÑƒÐ½Ð±ÐµÑ€ÐµÐ½Ñ‡ÐµÐ³Ó™ ÑƒÐ½Ð±ÐµÑ€ÐµÐ½Ñ‡ÐµÐ´Ó™ ÑƒÐ½Ð±ÐµÑ€ÐµÐ½Ñ‡ÐµÐ´Ó™Ð½ ÑƒÐ½Ð±ÐµÑ€ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€ ÑƒÐ½Ð±ÐµÑ€ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð³Ó™\nÑƒÐ½Ð±ÐµÑ€ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð´Ó™ ÑƒÐ½Ð±ÐµÑ€ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð´Ó™Ð½ ÑƒÐ½Ð±ÐµÑ€ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð½Ðµ ÑƒÐ½Ð±ÐµÑ€ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð½ÐµÒ£ ÑƒÐ½Ð±ÐµÑ€ÐµÐ½Ñ‡ÐµÐ½Ðµ\nÑƒÐ½Ð±ÐµÑ€ÐµÐ½Ñ‡ÐµÐ½ÐµÒ£ ÑƒÐ½Ð±Ð¸Ñˆ ÑƒÐ½Ð±Ð¸ÑˆÐµÐ½Ñ‡Ðµ ÑƒÐ½Ð±Ð¸ÑˆÐµÐ½Ñ‡ÐµÐ³Ó™ ÑƒÐ½Ð±Ð¸ÑˆÐµÐ½Ñ‡ÐµÐ´Ó™ ÑƒÐ½Ð±Ð¸ÑˆÐµÐ½Ñ‡ÐµÐ´Ó™Ð½ ÑƒÐ½Ð±Ð¸ÑˆÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€\nÑƒÐ½Ð±Ð¸ÑˆÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð³Ó™ ÑƒÐ½Ð±Ð¸ÑˆÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð´Ó™ ÑƒÐ½Ð±Ð¸ÑˆÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð´Ó™Ð½ ÑƒÐ½Ð±Ð¸ÑˆÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð½Ðµ ÑƒÐ½Ð±Ð¸ÑˆÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð½ÐµÒ£\nÑƒÐ½Ð±Ð¸ÑˆÐµÐ½Ñ‡ÐµÐ½Ðµ ÑƒÐ½Ð±Ð¸ÑˆÐµÐ½Ñ‡ÐµÐ½ÐµÒ£ ÑƒÐ½Ð´Ò¯Ñ€Ñ‚ ÑƒÐ½Ð´Ò¯Ñ€Ñ‚ÐµÐ½Ñ‡Ðµ ÑƒÐ½Ð´Ò¯Ñ€Ñ‚ÐµÐ½Ñ‡ÐµÐ³Ó™ ÑƒÐ½Ð´Ò¯Ñ€Ñ‚ÐµÐ½Ñ‡ÐµÐ´Ó™\nÑƒÐ½Ð´Ò¯Ñ€Ñ‚ÐµÐ½Ñ‡ÐµÐ´Ó™Ð½ ÑƒÐ½Ð´Ò¯Ñ€Ñ‚ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€ ÑƒÐ½Ð´Ò¯Ñ€Ñ‚ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð³Ó™ ÑƒÐ½Ð´Ò¯Ñ€Ñ‚ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð´Ó™ ÑƒÐ½Ð´Ò¯Ñ€Ñ‚ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð´Ó™Ð½\nÑƒÐ½Ð´Ò¯Ñ€Ñ‚ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð½Ðµ ÑƒÐ½Ð´Ò¯Ñ€Ñ‚ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð½ÐµÒ£ ÑƒÐ½Ð´Ò¯Ñ€Ñ‚ÐµÐ½Ñ‡ÐµÐ½Ðµ ÑƒÐ½Ð´Ò¯Ñ€Ñ‚ÐµÐ½Ñ‡ÐµÐ½ÐµÒ£ ÑƒÐ½Ð¸ÐºÐµ ÑƒÐ½Ð¸ÐºÐµÐ½Ñ‡Ðµ\nÑƒÐ½Ð¸ÐºÐµÐ½Ñ‡ÐµÐ³Ó™ ÑƒÐ½Ð¸ÐºÐµÐ½Ñ‡ÐµÐ´Ó™ ÑƒÐ½Ð¸ÐºÐµÐ½Ñ‡ÐµÐ´Ó™Ð½ ÑƒÐ½Ð¸ÐºÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€ ÑƒÐ½Ð¸ÐºÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð³Ó™ ÑƒÐ½Ð¸ÐºÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð´Ó™\nÑƒÐ½Ð¸ÐºÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð´Ó™Ð½ ÑƒÐ½Ð¸ÐºÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð½Ðµ ÑƒÐ½Ð¸ÐºÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð½ÐµÒ£ ÑƒÐ½Ð¸ÐºÐµÐ½Ñ‡ÐµÐ½Ðµ ÑƒÐ½Ð¸ÐºÐµÐ½Ñ‡ÐµÐ½ÐµÒ£ ÑƒÐ½Ð»Ð°Ð³Ð°Ð½\nÑƒÐ½Ð»Ð°Ð¿ ÑƒÐ½Ð½Ð°Ñ€Ñ‡Ð° ÑƒÐ½ÑÐ¸Ð³ÐµÐ· ÑƒÐ½ÑÐ¸Ð³ÐµÐ·ÐµÐ½Ñ‡Ðµ ÑƒÐ½ÑÐ¸Ð³ÐµÐ·ÐµÐ½Ñ‡ÐµÐ³Ó™ ÑƒÐ½ÑÐ¸Ð³ÐµÐ·ÐµÐ½Ñ‡ÐµÐ´Ó™ ÑƒÐ½ÑÐ¸Ð³ÐµÐ·ÐµÐ½Ñ‡ÐµÐ´Ó™Ð½\nÑƒÐ½ÑÐ¸Ð³ÐµÐ·ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€ ÑƒÐ½ÑÐ¸Ð³ÐµÐ·ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð³Ó™ ÑƒÐ½ÑÐ¸Ð³ÐµÐ·ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð´Ó™ ÑƒÐ½ÑÐ¸Ð³ÐµÐ·ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð´Ó™Ð½\nÑƒÐ½ÑÐ¸Ð³ÐµÐ·ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð½Ðµ ÑƒÐ½ÑÐ¸Ð³ÐµÐ·ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð½ÐµÒ£ ÑƒÐ½ÑÐ¸Ð³ÐµÐ·ÐµÐ½Ñ‡ÐµÐ½Ðµ ÑƒÐ½ÑÐ¸Ð³ÐµÐ·ÐµÐ½Ñ‡ÐµÐ½ÐµÒ£ ÑƒÐ½Ñ‚ÑƒÐ³Ñ‹Ð·\nÑƒÐ½Ñ‚ÑƒÐ³Ñ‹Ð·Ñ‹Ð½Ñ‡Ñ‹ ÑƒÐ½Ñ‚ÑƒÐ³Ñ‹Ð·Ñ‹Ð½Ñ‡Ñ‹Ð³Ð° ÑƒÐ½Ñ‚ÑƒÐ³Ñ‹Ð·Ñ‹Ð½Ñ‡Ñ‹Ð´Ð° ÑƒÐ½Ñ‚ÑƒÐ³Ñ‹Ð·Ñ‹Ð½Ñ‡Ñ‹Ð´Ð°Ð½ ÑƒÐ½Ñ‚ÑƒÐ³Ñ‹Ð·Ñ‹Ð½Ñ‡Ñ‹Ð»Ð°Ñ€\nÑƒÐ½Ñ‚ÑƒÐ³Ñ‹Ð·Ñ‹Ð½Ñ‡Ñ‹Ð»Ð°Ñ€Ð³Ð° ÑƒÐ½Ñ‚ÑƒÐ³Ñ‹Ð·Ñ‹Ð½Ñ‡Ñ‹Ð»Ð°Ñ€Ð´Ð° ÑƒÐ½Ñ‚ÑƒÐ³Ñ‹Ð·Ñ‹Ð½Ñ‡Ñ‹Ð»Ð°Ñ€Ð´Ð°Ð½ ÑƒÐ½Ñ‚ÑƒÐ³Ñ‹Ð·Ñ‹Ð½Ñ‡Ñ‹Ð»Ð°Ñ€Ð½Ñ‹\nÑƒÐ½Ñ‚ÑƒÐ³Ñ‹Ð·Ñ‹Ð½Ñ‡Ñ‹Ð»Ð°Ñ€Ð½Ñ‹Ò£ ÑƒÐ½Ñ‚ÑƒÐ³Ñ‹Ð·Ñ‹Ð½Ñ‡Ñ‹Ð½Ñ‹ ÑƒÐ½Ñ‚ÑƒÐ³Ñ‹Ð·Ñ‹Ð½Ñ‡Ñ‹Ð½Ñ‹Ò£ ÑƒÐ½Ñ‹Ð½Ñ‡Ñ‹ ÑƒÐ½Ñ‹Ð½Ñ‡Ñ‹Ð³Ð° ÑƒÐ½Ñ‹Ð½Ñ‡Ñ‹Ð´Ð°\nÑƒÐ½Ñ‹Ð½Ñ‡Ñ‹Ð´Ð°Ð½ ÑƒÐ½Ñ‹Ð½Ñ‡Ñ‹Ð»Ð°Ñ€ ÑƒÐ½Ñ‹Ð½Ñ‡Ñ‹Ð»Ð°Ñ€Ð³Ð° ÑƒÐ½Ñ‹Ð½Ñ‡Ñ‹Ð»Ð°Ñ€Ð´Ð° ÑƒÐ½Ñ‹Ð½Ñ‡Ñ‹Ð»Ð°Ñ€Ð´Ð°Ð½ ÑƒÐ½Ñ‹Ð½Ñ‡Ñ‹Ð»Ð°Ñ€Ð½Ñ‹\nÑƒÐ½Ñ‹Ð½Ñ‡Ñ‹Ð»Ð°Ñ€Ð½Ñ‹Ò£ ÑƒÐ½Ñ‹Ð½Ñ‡Ñ‹Ð½Ñ‹ ÑƒÐ½Ñ‹Ð½Ñ‡Ñ‹Ð½Ñ‹Ò£ ÑƒÐ½Ò—Ð¸Ð´Ðµ ÑƒÐ½Ò—Ð¸Ð´ÐµÐ½Ñ‡Ðµ ÑƒÐ½Ò—Ð¸Ð´ÐµÐ½Ñ‡ÐµÐ³Ó™ ÑƒÐ½Ò—Ð¸Ð´ÐµÐ½Ñ‡ÐµÐ´Ó™\nÑƒÐ½Ò—Ð¸Ð´ÐµÐ½Ñ‡ÐµÐ´Ó™Ð½ ÑƒÐ½Ò—Ð¸Ð´ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€ ÑƒÐ½Ò—Ð¸Ð´ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð³Ó™ ÑƒÐ½Ò—Ð¸Ð´ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð´Ó™ ÑƒÐ½Ò—Ð¸Ð´ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð´Ó™Ð½\nÑƒÐ½Ò—Ð¸Ð´ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð½Ðµ ÑƒÐ½Ò—Ð¸Ð´ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð½ÐµÒ£ ÑƒÐ½Ò—Ð¸Ð´ÐµÐ½Ñ‡ÐµÐ½Ðµ ÑƒÐ½Ò—Ð¸Ð´ÐµÐ½Ñ‡ÐµÐ½ÐµÒ£ ÑƒÐ½Ó©Ñ‡ ÑƒÐ½Ó©Ñ‡ÐµÐ½Ñ‡Ðµ ÑƒÐ½Ó©Ñ‡ÐµÐ½Ñ‡ÐµÐ³Ó™\nÑƒÐ½Ó©Ñ‡ÐµÐ½Ñ‡ÐµÐ´Ó™ ÑƒÐ½Ó©Ñ‡ÐµÐ½Ñ‡ÐµÐ´Ó™Ð½ ÑƒÐ½Ó©Ñ‡ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€ ÑƒÐ½Ó©Ñ‡ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð³Ó™ ÑƒÐ½Ó©Ñ‡ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð´Ó™ ÑƒÐ½Ó©Ñ‡ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð´Ó™Ð½\nÑƒÐ½Ó©Ñ‡ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð½Ðµ ÑƒÐ½Ó©Ñ‡ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð½ÐµÒ£ ÑƒÐ½Ó©Ñ‡ÐµÐ½Ñ‡ÐµÐ½Ðµ ÑƒÐ½Ó©Ñ‡ÐµÐ½Ñ‡ÐµÐ½ÐµÒ£ ÑƒÑ‚Ñ‹Ð· ÑƒÑ‚Ñ‹Ð·Ñ‹Ð½Ñ‡Ñ‹ ÑƒÑ‚Ñ‹Ð·Ñ‹Ð½Ñ‡Ñ‹Ð³Ð°\nÑƒÑ‚Ñ‹Ð·Ñ‹Ð½Ñ‡Ñ‹Ð´Ð° ÑƒÑ‚Ñ‹Ð·Ñ‹Ð½Ñ‡Ñ‹Ð´Ð°Ð½ ÑƒÑ‚Ñ‹Ð·Ñ‹Ð½Ñ‡Ñ‹Ð»Ð°Ñ€ ÑƒÑ‚Ñ‹Ð·Ñ‹Ð½Ñ‡Ñ‹Ð»Ð°Ñ€Ð³Ð° ÑƒÑ‚Ñ‹Ð·Ñ‹Ð½Ñ‡Ñ‹Ð»Ð°Ñ€Ð´Ð° ÑƒÑ‚Ñ‹Ð·Ñ‹Ð½Ñ‡Ñ‹Ð»Ð°Ñ€Ð´Ð°Ð½\nÑƒÑ‚Ñ‹Ð·Ñ‹Ð½Ñ‡Ñ‹Ð»Ð°Ñ€Ð½Ñ‹ ÑƒÑ‚Ñ‹Ð·Ñ‹Ð½Ñ‡Ñ‹Ð»Ð°Ñ€Ð½Ñ‹Ò£ ÑƒÑ‚Ñ‹Ð·Ñ‹Ð½Ñ‡Ñ‹Ð½Ñ‹ ÑƒÑ‚Ñ‹Ð·Ñ‹Ð½Ñ‡Ñ‹Ð½Ñ‹Ò£\n\nÑ„Ð¸ÐºÐµÑ€ÐµÐ½Ñ‡Ó™ Ñ„Ó™ÐºÐ°Ñ‚ÑŒ\n\nÑ…Ð°ÐºÑ‹Ð½Ð´Ð° Ñ…Ó™Ð±Ó™Ñ€ Ñ…Ó™Ð»Ð±ÑƒÐºÐ¸ Ñ…Ó™Ñ‚Ð»Ðµ Ñ…Ó™Ñ‚Ñ‚Ð°\n\nÑ‡Ð°ÐºÐ»Ñ‹ Ñ‡Ð°ÐºÑ‚Ð° Ñ‡Ó©Ð½ÐºÐ¸\n\nÑˆÐ¸ÐºÐµÐ»Ð»Ðµ ÑˆÑƒÐ» ÑˆÑƒÐ»Ð°Ð¹ ÑˆÑƒÐ»Ð°Ñ€ ÑˆÑƒÐ»Ð°Ñ€Ð³Ð° ÑˆÑƒÐ»Ð°Ñ€Ð´Ð° ÑˆÑƒÐ»Ð°Ñ€Ð´Ð°Ð½ ÑˆÑƒÐ»Ð°Ñ€Ð½Ñ‹ ÑˆÑƒÐ»Ð°Ñ€Ð½Ñ‹Ò£ ÑˆÑƒÐ»Ð°Ñ€Ñ‹ ÑˆÑƒÐ»Ð°Ñ€Ñ‹Ð½\nÑˆÑƒÐ»Ð°Ñ€Ñ‹Ð½Ð³Ð° ÑˆÑƒÐ»Ð°Ñ€Ñ‹Ð½Ð´Ð° ÑˆÑƒÐ»Ð°Ñ€Ñ‹Ð½Ð½Ð°Ð½ ÑˆÑƒÐ»Ð°Ñ€Ñ‹Ð½Ñ‹Ò£ ÑˆÑƒÐ»ÐºÐ°Ð´Ó™Ñ€ ÑˆÑƒÐ»Ñ‚Ð¸ÐºÐ»Ðµ ÑˆÑƒÐ»Ñ‚Ð¸ÐºÐ»ÐµÐ¼ ÑˆÑƒÐ»Ñ…Ó™Ñ‚Ð»Ðµ\nÑˆÑƒÐ»Ñ‡Ð°ÐºÐ»Ñ‹ ÑˆÑƒÐ½Ð´Ð° ÑˆÑƒÐ½Ð´Ð°Ð³Ñ‹ ÑˆÑƒÐ½Ð´Ñ‹Ð¹ ÑˆÑƒÐ½Ð´Ñ‹Ð¹Ð³Ð° ÑˆÑƒÐ½Ð´Ñ‹Ð¹Ð´Ð° ÑˆÑƒÐ½Ð´Ñ‹Ð¹Ð´Ð°Ð½ ÑˆÑƒÐ½Ð´Ñ‹Ð¹Ð½Ñ‹ ÑˆÑƒÐ½Ð´Ñ‹Ð¹Ð½Ñ‹Ò£\nÑˆÑƒÐ½Ð»Ñ‹ÐºÑ‚Ð°Ð½ ÑˆÑƒÐ½Ð½Ð°Ð½ ÑˆÑƒÐ½ÑÑ‹ ÑˆÑƒÐ½Ñ‡Ð° ÑˆÑƒÐ½Ñ‹ ÑˆÑƒÐ½Ñ‹ÐºÑ‹ ÑˆÑƒÐ½Ñ‹ÐºÑ‹Ð½ ÑˆÑƒÐ½Ñ‹ÐºÑ‹Ð½Ð³Ð° ÑˆÑƒÐ½Ñ‹ÐºÑ‹Ð½Ð´Ð° ÑˆÑƒÐ½Ñ‹ÐºÑ‹Ð½Ð½Ð°Ð½\nÑˆÑƒÐ½Ñ‹ÐºÑ‹Ð½Ñ‹Ò£ ÑˆÑƒÐ½Ñ‹ÑÑ‹ ÑˆÑƒÐ½Ñ‹ÑÑ‹Ð½ ÑˆÑƒÐ½Ñ‹ÑÑ‹Ð½Ð³Ð° ÑˆÑƒÐ½Ñ‹ÑÑ‹Ð½Ð´Ð° ÑˆÑƒÐ½Ñ‹ÑÑ‹Ð½Ð½Ð°Ð½ ÑˆÑƒÐ½Ñ‹ÑÑ‹Ð½Ñ‹Ò£ ÑˆÑƒÐ½Ñ‹Ò£ ÑˆÑƒÑˆÑ‹\nÑˆÑƒÑˆÑ‹Ð½Ð´Ð° ÑˆÑƒÑˆÑ‹Ð½Ð½Ð°Ð½ ÑˆÑƒÑˆÑ‹Ð½Ñ‹ ÑˆÑƒÑˆÑ‹Ð½Ñ‹Ò£ ÑˆÑƒÑˆÑ‹Ò£Ð° ÑˆÑƒÒ£Ð° ÑˆÑƒÒ£Ð°Ñ€ ÑˆÑƒÒ£Ð°Ñ€Ð³Ð°\n\nÑÐ»ÐµÐº\n\nÑŽÐ³Ñ‹Ð¹ÑÓ™ ÑŽÐº ÑŽÐºÑÐ°\n\nÑ ÑÐ³ÑŠÐ½Ð¸ ÑÐ·ÑƒÑ‹Ð½Ñ‡Ð° ÑÐ¸ÑÓ™ ÑÐºÐ¸ ÑÐºÑ‚Ð°Ð½ ÑÐºÑ‹Ð½ ÑÑ€Ð°ÑˆÐ»Ñ‹ ÑÑ…ÑƒÑ‚ ÑÑˆÑŒ ÑÑˆÑŒÐ»ÐµÐº\n\nÒ—Ð¸Ð´Ðµ Ò—Ð¸Ð´ÐµÐ»Ó™Ð¿ Ò—Ð¸Ð´ÐµÐ½Ñ‡Ðµ Ò—Ð¸Ð´ÐµÐ½Ñ‡ÐµÐ³Ó™ Ò—Ð¸Ð´ÐµÐ½Ñ‡ÐµÐ´Ó™ Ò—Ð¸Ð´ÐµÐ½Ñ‡ÐµÐ´Ó™Ð½ Ò—Ð¸Ð´ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€ Ò—Ð¸Ð´ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð³Ó™\nÒ—Ð¸Ð´ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð´Ó™ Ò—Ð¸Ð´ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð´Ó™Ð½ Ò—Ð¸Ð´ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð½Ðµ Ò—Ð¸Ð´ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð½ÐµÒ£ Ò—Ð¸Ð´ÐµÐ½Ñ‡ÐµÐ½Ðµ Ò—Ð¸Ð´ÐµÐ½Ñ‡ÐµÐ½ÐµÒ£\nÒ—Ð¸Ð´ÐµÑˆÓ™Ñ€ Ò—Ð¸Ñ‚Ð¼ÐµÑˆ Ò—Ð¸Ñ‚Ð¼ÐµÑˆÐµÐ½Ñ‡Ðµ Ò—Ð¸Ñ‚Ð¼ÐµÑˆÐµÐ½Ñ‡ÐµÐ³Ó™ Ò—Ð¸Ñ‚Ð¼ÐµÑˆÐµÐ½Ñ‡ÐµÐ´Ó™ Ò—Ð¸Ñ‚Ð¼ÐµÑˆÐµÐ½Ñ‡ÐµÐ´Ó™Ð½ Ò—Ð¸Ñ‚Ð¼ÐµÑˆÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€\nÒ—Ð¸Ñ‚Ð¼ÐµÑˆÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð³Ó™ Ò—Ð¸Ñ‚Ð¼ÐµÑˆÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð´Ó™ Ò—Ð¸Ñ‚Ð¼ÐµÑˆÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð´Ó™Ð½ Ò—Ð¸Ñ‚Ð¼ÐµÑˆÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð½Ðµ\nÒ—Ð¸Ñ‚Ð¼ÐµÑˆÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð½ÐµÒ£ Ò—Ð¸Ñ‚Ð¼ÐµÑˆÐµÐ½Ñ‡ÐµÐ½Ðµ Ò—Ð¸Ñ‚Ð¼ÐµÑˆÐµÐ½Ñ‡ÐµÐ½ÐµÒ£ Ò—Ñ‹ÐµÐ½Ñ‹ÑÑ‹\n\nÒ¯Ð· Ò¯Ð·Ðµ Ò¯Ð·ÐµÐ¼ Ò¯Ð·ÐµÐ¼Ð´Ó™ Ò¯Ð·ÐµÐ¼Ð½Ðµ Ò¯Ð·ÐµÐ¼Ð½ÐµÒ£ Ò¯Ð·ÐµÐ¼Ð½Ó™Ð½ Ò¯Ð·ÐµÐ¼Ó™ Ò¯Ð·ÐµÐ½ Ò¯Ð·ÐµÐ½Ð´Ó™ Ò¯Ð·ÐµÐ½ÐµÒ£ Ò¯Ð·ÐµÐ½Ð½Ó™Ð½ Ò¯Ð·ÐµÐ½Ó™\nÒ¯Ðº\n\nÒ»Ð¸Ñ‡Ð±ÐµÑ€ Ò»Ð¸Ñ‡Ð±ÐµÑ€Ðµ Ò»Ð¸Ñ‡Ð±ÐµÑ€ÐµÐ½ Ò»Ð¸Ñ‡Ð±ÐµÑ€ÐµÐ½Ð´Ó™ Ò»Ð¸Ñ‡Ð±ÐµÑ€ÐµÐ½ÐµÒ£ Ò»Ð¸Ñ‡Ð±ÐµÑ€ÐµÐ½Ð½Ó™Ð½ Ò»Ð¸Ñ‡Ð±ÐµÑ€ÐµÐ½Ó™ Ò»Ð¸Ñ‡Ð±ÐµÑ€ÑÐµ\nÒ»Ð¸Ñ‡Ð±ÐµÑ€ÑÐµÐ½ Ò»Ð¸Ñ‡Ð±ÐµÑ€ÑÐµÐ½Ð´Ó™ Ò»Ð¸Ñ‡Ð±ÐµÑ€ÑÐµÐ½ÐµÒ£ Ò»Ð¸Ñ‡Ð±ÐµÑ€ÑÐµÐ½Ð½Ó™Ð½ Ò»Ð¸Ñ‡Ð±ÐµÑ€ÑÐµÐ½Ó™ Ò»Ð¸Ñ‡Ð±ÐµÑ€Ó™Ò¯ Ò»Ð¸Ñ‡Ð±ÐµÑ€Ó™Ò¯Ð³Ó™\nÒ»Ð¸Ñ‡Ð±ÐµÑ€Ó™Ò¯Ð´Ó™ Ò»Ð¸Ñ‡Ð±ÐµÑ€Ó™Ò¯Ð´Ó™Ð½ Ò»Ð¸Ñ‡Ð±ÐµÑ€Ó™Ò¯Ð½Ðµ Ò»Ð¸Ñ‡Ð±ÐµÑ€Ó™Ò¯Ð½ÐµÒ£ Ò»Ð¸Ñ‡ÐºÐ°Ð¹ÑÑ‹ Ò»Ð¸Ñ‡ÐºÐ°Ð¹ÑÑ‹Ð³Ð° Ò»Ð¸Ñ‡ÐºÐ°Ð¹ÑÑ‹Ð´Ð°\nÒ»Ð¸Ñ‡ÐºÐ°Ð¹ÑÑ‹Ð´Ð°Ð½ Ò»Ð¸Ñ‡ÐºÐ°Ð¹ÑÑ‹Ð½Ñ‹ Ò»Ð¸Ñ‡ÐºÐ°Ð¹ÑÑ‹Ð½Ñ‹Ò£ Ò»Ð¸Ñ‡ÐºÐµÐ¼ Ò»Ð¸Ñ‡ÐºÐµÐ¼Ð³Ó™ Ò»Ð¸Ñ‡ÐºÐµÐ¼Ð´Ó™ Ò»Ð¸Ñ‡ÐºÐµÐ¼Ð½Ðµ Ò»Ð¸Ñ‡ÐºÐµÐ¼Ð½ÐµÒ£\nÒ»Ð¸Ñ‡ÐºÐµÐ¼Ð½Ó™Ð½ Ò»Ð¸Ñ‡Ð½Ð¸ Ò»Ð¸Ñ‡Ð½Ð¸Ð³Ó™ Ò»Ð¸Ñ‡Ð½Ð¸Ð´Ó™ Ò»Ð¸Ñ‡Ð½Ð¸Ð´Ó™Ð½ Ò»Ð¸Ñ‡Ð½Ð¸Ð½Ð´Ð¸ Ò»Ð¸Ñ‡Ð½Ð¸Ð½Ðµ Ò»Ð¸Ñ‡Ð½Ð¸Ð½ÐµÒ£ Ò»Ð¸Ñ‡Ð½Ó™Ñ€ÑÓ™\nÒ»Ð¸Ñ‡Ð½Ó™Ñ€ÑÓ™Ð³Ó™ Ò»Ð¸Ñ‡Ð½Ó™Ñ€ÑÓ™Ð´Ó™ Ò»Ð¸Ñ‡Ð½Ó™Ñ€ÑÓ™Ð´Ó™Ð½ Ò»Ð¸Ñ‡Ð½Ó™Ñ€ÑÓ™Ð½Ðµ Ò»Ð¸Ñ‡Ð½Ó™Ñ€ÑÓ™Ð½ÐµÒ£ Ò»Ó™Ð¼ Ò»Ó™Ð¼Ð¼Ó™ Ò»Ó™Ð¼Ð¼Ó™ÑÐµ\nÒ»Ó™Ð¼Ð¼Ó™ÑÐµÐ½ Ò»Ó™Ð¼Ð¼Ó™ÑÐµÐ½Ð´Ó™ Ò»Ó™Ð¼Ð¼Ó™ÑÐµÐ½ÐµÒ£ Ò»Ó™Ð¼Ð¼Ó™ÑÐµÐ½Ð½Ó™Ð½ Ò»Ó™Ð¼Ð¼Ó™ÑÐµÐ½Ó™ Ò»Ó™Ñ€ Ò»Ó™Ñ€Ð±ÐµÑ€ Ò»Ó™Ñ€Ð±ÐµÑ€Ðµ Ò»Ó™Ñ€Ð±ÐµÑ€ÑÐµ\nÒ»Ó™Ñ€ÐºÐ°Ð¹ÑÑ‹ Ò»Ó™Ñ€ÐºÐ°Ð¹ÑÑ‹Ð³Ð° Ò»Ó™Ñ€ÐºÐ°Ð¹ÑÑ‹Ð´Ð° Ò»Ó™Ñ€ÐºÐ°Ð¹ÑÑ‹Ð´Ð°Ð½ Ò»Ó™Ñ€ÐºÐ°Ð¹ÑÑ‹Ð½Ñ‹ Ò»Ó™Ñ€ÐºÐ°Ð¹ÑÑ‹Ð½Ñ‹Ò£ Ò»Ó™Ñ€ÐºÐµÐ¼\nÒ»Ó™Ñ€ÐºÐµÐ¼Ð³Ó™ Ò»Ó™Ñ€ÐºÐµÐ¼Ð´Ó™ Ò»Ó™Ñ€ÐºÐµÐ¼Ð½Ðµ Ò»Ó™Ñ€ÐºÐµÐ¼Ð½ÐµÒ£ Ò»Ó™Ñ€ÐºÐµÐ¼Ð½Ó™Ð½ Ò»Ó™Ñ€Ð½Ð¸ Ò»Ó™Ñ€Ð½Ó™Ñ€ÑÓ™ Ò»Ó™Ñ€Ð½Ó™Ñ€ÑÓ™Ð³Ó™\nÒ»Ó™Ñ€Ð½Ó™Ñ€ÑÓ™Ð´Ó™ Ò»Ó™Ñ€Ð½Ó™Ñ€ÑÓ™Ð´Ó™Ð½ Ò»Ó™Ñ€Ð½Ó™Ñ€ÑÓ™Ð½Ðµ Ò»Ó™Ñ€Ð½Ó™Ñ€ÑÓ™Ð½ÐµÒ£ Ò»Ó™Ñ€Ñ‚Ó©Ñ€Ð»Ðµ\n\nÓ™ Ó™Ð³Ó™Ñ€ Ó™Ð¹Ñ‚Ò¯ÐµÐ½Ñ‡Ó™ Ó™Ð¹Ñ‚Ò¯Ð»Ó™Ñ€ÐµÐ½Ñ‡Ó™ Ó™Ð»Ð±Ó™Ñ‚Ñ‚Ó™ Ó™Ð»Ðµ Ó™Ð»ÐµÐ³Ðµ Ó™Ð»Ð»Ó™ Ó™Ð¼Ð¼Ð° Ó™Ð½Ó™\n\nÓ©ÑÑ‚Ó™Ð¿ Ó©Ñ‡ Ó©Ñ‡ÐµÐ½ Ó©Ñ‡ÐµÐ½Ñ‡Ðµ Ó©Ñ‡ÐµÐ½Ñ‡ÐµÐ³Ó™ Ó©Ñ‡ÐµÐ½Ñ‡ÐµÐ´Ó™ Ó©Ñ‡ÐµÐ½Ñ‡ÐµÐ´Ó™Ð½ Ó©Ñ‡ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€ Ó©Ñ‡ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð³Ó™\nÓ©Ñ‡ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð´Ó™ Ó©Ñ‡ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð´Ó™Ð½ Ó©Ñ‡ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð½Ðµ Ó©Ñ‡ÐµÐ½Ñ‡ÐµÐ»Ó™Ñ€Ð½ÐµÒ£ Ó©Ñ‡ÐµÐ½Ñ‡ÐµÐ½Ðµ Ó©Ñ‡ÐµÐ½Ñ‡ÐµÐ½ÐµÒ£ Ó©Ñ‡Ð»Ó™Ð¿\nÓ©Ñ‡Ó™Ñ€Ð»Ó™Ð¿'.split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/tt/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.tt.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/tt/__init__.py----------------------------------------
spacy.lang.tt.__init__.Tatar(Language)
spacy.lang.tt.__init__.TatarDefaults(BaseDefaults)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/tt/lex_attrs.py----------------------------------------
A:spacy.lang.tt.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.tt.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.tt.lex_attrs.like_num(text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/tt/punctuation.py----------------------------------------
A:spacy.lang.tt.punctuation._hyphens_no_dash->char_classes.HYPHENS.replace('-', '').strip('|').replace('||', '')


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ti/examples.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ti/stop_words.py----------------------------------------
A:spacy.lang.ti.stop_words.STOP_WORDS->set("\n'áˆá‰ áˆ­ 'áˆž 'á‰² 'á‰³ 'áŠ³ 'á‹áŠ• 'á‹š 'á‹¨ 'á‹© 'á‹« 'á‹®áˆ 'á‹®áŠ•\náˆá‹•áˆŠ áˆ’á‹™ áˆ’á‹› áˆ•áŒ‚ áˆ˜á‰ áˆ áˆ˜áŠ• áˆ˜áŠ•áŒŽ áˆ˜áŒ áŠ• áˆ›áˆˆá‰µ áˆáˆµ áˆá‰£áˆ\náˆáŠ¥áŠ•á‰² áˆáŠ½áŠ•á‹«á‰± áˆáŠ½áŠ•á‹«á‰µ áˆá‹ƒáŠ‘ áˆá‹ƒáŠ•áŠ“ áˆá‹ƒáŠ–áˆ\náˆµáˆˆ áˆµáˆˆá‹š áˆµáˆˆá‹á‰ áˆ‹ áˆ½á‹‘ á‰…á‹µáˆš á‰ áˆˆ á‰ á‰² á‰ á‹š á‰¥áˆá‰£áˆ á‰¥á‰°á‹ˆáˆ³áŠº á‰¥áŠ¸áˆ˜á‹­\ná‰¥á‹˜á‹­ á‰¥á‹˜á‹­áŠ« á‰¥á‹™áˆ• á‰¥á‹›á‹•á‰£ á‰¥ááˆ‹á‹­ á‰°á‰£áˆ‚áˆ‰ áŠá‰ áˆ¨ áŠá‰² áŠá‰³ áŠá‰¶áˆ\náŠá‹š áŠá‹­áˆ© áŠáŒˆáˆ«á‰µ áŠáŒˆáˆ­ áŠ“á‰¥ áŠ“á‰¥á‰² áŠ“á‰µáŠ©áˆ áŠ“á‰µáŠª áŠ“á‰µáŠ« áŠ“á‰µáŠ­áŠ•\náŠ“á‹­ áŠ“á‹­á‰² áŠ•áˆ•áŠ“ áŠ•áˆ± áŠ•áˆ³ áŠ•áˆ³á‰¶áˆ áŠ•áˆµáŠº áŠ•áˆµáŠ» áŠ•áˆµáŠ»á‰µáŠ©áˆ áŠ•áˆµáŠ»á‰µáŠ­áŠ• áŠ•á‹“á‹­\náŠ¢áˆˆ áŠ¢áˆ‰ áŠ¢áˆ‹ áŠ¢áˆáŠ« áŠ¢áˆŽáˆ áŠ¢áŠ“ áŠ¢áŠ» áŠ¢á‹© áŠ£áˆˆáŠ¹\náŠ£áˆˆá‹‰ áŠ£áˆˆá‹Ž áŠ£áˆŽ áŠ£á‰¥ áŠ£á‰¥á‰² áŠ£á‰¥á‰³ áŠ£á‰¥áŠ¡ áŠ£á‰¥á‹š áŠ£áŠ áŠ£á‹á‹© áŠ£á‹­áŠ®áŠáŠ• áŠ£á‹­áŠ°áŠáŠ•\náŠ¥áˆá‰ áˆ­ áŠ¥áˆž áŠ¥á‰°áŠ• áŠ¥á‰² áŠ¥á‰³ áŠ¥á‰¶áˆ áŠ¥áŠ•á‰° áŠ¥áŠ•á‰°áˆŽ\náŠ£áˆ‹ áŠ¥áŠ•á‰°áŠ¾áŠ áŠ¥áŠ•á‰³á‹­ áŠ¥áŠ•áŠ¨áˆŽ áŠ¥áŠ³ áŠ¥á‹‹áŠ• áŠ¥á‹áŠ• áŠ¥á‹š áŠ¥á‹› áŠ¥á‹žáˆ\náŠ¥á‹¨ áŠ¥á‹¨áŠ• áŠ¥á‹© áŠ¥á‹« áŠ¥á‹®áˆ\náŠ¨áˆŽ áŠ¨áˆ˜á‹­ áŠ¨áˆ áŠ¨áˆá‰² áŠ¨áˆáŠ¡ áŠ¨áˆá‹˜áˆŽ\náŠ¨áˆá‹š áŠ¨áŠ£ áŠ©áˆ‰ áŠ«áˆáŠ¥ áŠ«áˆáŠ¦á‰µ áŠ«á‰¥ áŠ«á‰¥á‰² áŠ«á‰¥á‰¶áˆ áŠ­áˆ³á‰¥ áŠ­áˆ³á‹• áŠ­á‰¥áˆ\náŠ­áŠ•á‹°á‹­ áŠ­áŠ•á‹² áŠ­áŠ¸á‹áŠ• áŠ®á‹­áŠ‘ áŠ°á‹­áŠ‘ áŠµáˆ‰ áŠ¸áˆ áŠ¸áŠ£ á‹ˆá‹­\ná‹‹áˆ‹ á‹˜áˆˆáŠ“ á‹˜áˆˆá‹‰ á‹˜áˆˆá‹‹ á‹˜áˆˆá‹Ž á‹˜áˆˆá‹Žáˆ á‹˜áˆ‹ á‹˜áˆŽ á‹˜á‹­á‰¥áˆ‰  \ná‹áˆ­áŠ¨á‰¥ á‹á‰ áˆƒáˆ á‹á‰ áˆˆ á‹á‰¥áˆ á‹á‰°á‰£áˆ…áˆˆ á‹á‰°áŠ»á‹¨á‹° á‹á‰°áˆáˆ‹áˆˆá‹¨ á‹á‰°áˆáˆ‹áˆˆá‹©\ná‹áŠá‰ áˆ¨ á‹áŠá‰ áˆ¨á‰µ á‹áŠá‰ áˆ© á‹áŠ«á‹¨á‹µ á‹áŠ¸á‹áŠ• á‹áŠ½áŠ¥áˆ á‹áŠ¾áŠ á‹á‹€áŠ\ná‹¨áˆˆáŠ• á‹­á‰•áˆ¨á‰¥ á‹­á‰¥áˆ á‹­áŠ¸á‹áŠ• á‹­áŠ¹áŠ• á‹­áŠ½áŠ¥áˆ á‹°áŠ£ á‹µáˆ•áˆª á‹µáˆ›\náŒˆáˆˆ áŒˆáˆŠáŒ¹ áŒˆáŠ“ áŒˆá‹­áˆ© áŒáŠ“ áŒáŠ• áŒ¥áˆ«á‹­\n".split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ti/tokenizer_exceptions.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ti/__init__.py----------------------------------------
A:spacy.lang.ti.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.ti.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS, TOKENIZER_EXCEPTIONS)
spacy.lang.ti.__init__.Tigrinya(Language)
spacy.lang.ti.__init__.TigrinyaDefaults(BaseDefaults)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ti/lex_attrs.py----------------------------------------
A:spacy.lang.ti.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.ti.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
A:spacy.lang.ti.lex_attrs.text_lower->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').lower()
spacy.lang.ti.lex_attrs.like_num(text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ti/punctuation.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/uk/examples.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/uk/lemmatizer.py----------------------------------------
A:spacy.lang.uk.lemmatizer.self._morph->MorphAnalyzer(lang='uk')
spacy.lang.uk.UkrainianLemmatizer(self,vocab:Vocab,model:Optional[Model],name:str='lemmatizer',*,mode:str='pymorphy3',overwrite:bool=False,scorer:Optional[Callable]=lemmatizer_score)
spacy.lang.uk.lemmatizer.UkrainianLemmatizer(self,vocab:Vocab,model:Optional[Model],name:str='lemmatizer',*,mode:str='pymorphy3',overwrite:bool=False,scorer:Optional[Callable]=lemmatizer_score)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/uk/stop_words.py----------------------------------------
A:spacy.lang.uk.stop_words.STOP_WORDS->set("Ð°\nÐ°Ð±Ð¾\nÐ°Ð´Ð¶Ðµ\nÐ°Ð¶\nÐ°Ð»Ðµ\nÐ°Ð»Ð»Ð¾\nÐ±\nÐ±Ð°Ð³Ð°Ñ‚Ð¾\nÐ±ÐµÐ·\nÐ±ÐµÐ·Ð¿ÐµÑ€ÐµÑ€Ð²Ð½Ð¾\nÐ±Ð¸\nÐ±Ñ–Ð»ÑŒÑˆ\nÐ±Ñ–Ð»ÑŒÑˆÐµ\nÐ±Ñ–Ð»Ñ\nÐ±Ð»Ð¸Ð·ÑŒÐºÐ¾\nÐ±Ð¾\nÐ±ÑƒÐ²\nÐ±ÑƒÐ²Ð°Ñ”\nÐ±ÑƒÐ´Ðµ\nÐ±ÑƒÐ´ÐµÐ¼Ð¾\nÐ±ÑƒÐ´ÐµÑ‚Ðµ\nÐ±ÑƒÐ´ÐµÑˆ\nÐ±ÑƒÐ´Ñƒ\nÐ±ÑƒÐ´ÑƒÑ‚ÑŒ\nÐ±ÑƒÐ´ÑŒ\nÐ±ÑƒÐ»Ð°\nÐ±ÑƒÐ»Ð¸\nÐ±ÑƒÐ»Ð¾\nÐ±ÑƒÑ‚Ð¸\nÐ²\nÐ²Ð°Ð¼\nÐ²Ð°Ð¼Ð¸\nÐ²Ð°Ñ\nÐ²Ð°Ñˆ\nÐ²Ð°ÑˆÐ°\nÐ²Ð°ÑˆÐµ\nÐ²Ð°ÑˆÐ¸Ð¼\nÐ²Ð°ÑˆÐ¸Ð¼Ð¸\nÐ²Ð°ÑˆÐ¸Ñ…\nÐ²Ð°ÑˆÑ–\nÐ²Ð°ÑˆÑ–Ð¹\nÐ²Ð°ÑˆÐ¾Ð³Ð¾\nÐ²Ð°ÑˆÐ¾Ñ—\nÐ²Ð°ÑˆÐ¾Ð¼Ñƒ\nÐ²Ð°ÑˆÐ¾ÑŽ\nÐ²Ð°ÑˆÑƒ\nÐ²Ð³Ð¾Ñ€Ñ–\nÐ²Ð³Ð¾Ñ€Ñƒ\nÐ²Ð´Ð°Ð»Ð¸Ð½Ñ–\nÐ²ÐµÑÑŒ\nÐ²Ð¶Ðµ\nÐ²Ð¸\nÐ²Ñ–Ð´\nÐ²Ñ–Ð´ÑÐ¾Ñ‚ÐºÑ–Ð²\nÐ²Ñ–Ð½\nÐ²Ñ–ÑÑ–Ð¼\nÐ²Ñ–ÑÑ–Ð¼Ð½Ð°Ð´Ñ†ÑÑ‚Ð¸Ð¹\nÐ²Ñ–ÑÑ–Ð¼Ð½Ð°Ð´Ñ†ÑÑ‚ÑŒ\nÐ²Ð½Ð¸Ð·\nÐ²Ð½Ð¸Ð·Ñƒ\nÐ²Ð¾Ð½Ð°\nÐ²Ð¾Ð½Ð¸\nÐ²Ð¾Ð½Ð¾\nÐ²Ð¾ÑÑŒÐ¼Ð¸Ð¹\nÐ²ÑÐµ\nÐ²ÑÐµÑŽ\nÐ²ÑÑ–\nÐ²ÑÑ–Ð¼\nÐ²ÑÑ–Ñ…\nÐ²ÑÑŒÐ¾Ð³Ð¾\nÐ²ÑÑŒÐ¾Ð¼Ñƒ\nÐ²ÑÑŽ\nÐ²ÑÑ\nÐ²Ñ‚Ñ–Ð¼\nÐ³\nÐ³ÐµÑ‚ÑŒ\nÐ³Ð¾Ð²Ð¾Ñ€Ð¸Ð²\nÐ³Ð¾Ð²Ð¾Ñ€Ð¸Ñ‚ÑŒ\nÐ´Ð°Ð²Ð½Ð¾\nÐ´Ð°Ð»ÐµÐºÐ¾\nÐ´Ð°Ð»Ñ–\nÐ´Ð°Ñ€Ð¼Ð°\nÐ´Ð²Ð°\nÐ´Ð²Ð°Ð´Ñ†ÑÑ‚Ð¸Ð¹\nÐ´Ð²Ð°Ð´Ñ†ÑÑ‚ÑŒ\nÐ´Ð²Ð°Ð½Ð°Ð´Ñ†ÑÑ‚Ð¸Ð¹\nÐ´Ð²Ð°Ð½Ð°Ð´Ñ†ÑÑ‚ÑŒ\nÐ´Ð²Ñ–\nÐ´Ð²Ð¾Ñ…\nÐ´Ðµ\nÐ´ÐµÐ²'ÑÑ‚Ð¸Ð¹\nÐ´ÐµÐ²'ÑÑ‚Ð½Ð°Ð´Ñ†ÑÑ‚Ð¸Ð¹\nÐ´ÐµÐ²'ÑÑ‚Ð½Ð°Ð´Ñ†ÑÑ‚ÑŒ\nÐ´ÐµÐ²'ÑÑ‚ÑŒ\nÐ´ÐµÐºÑ–Ð»ÑŒÐºÐ°\nÐ´ÐµÐ½ÑŒ\nÐ´ÐµÑÑÑ‚Ð¸Ð¹\nÐ´ÐµÑÑÑ‚ÑŒ\nÐ´Ñ–Ð¹ÑÐ½Ð¾\nÐ´Ð»Ñ\nÐ´Ð½Ñ\nÐ´Ð¾\nÐ´Ð¾Ð±Ñ€Ðµ\nÐ´Ð¾Ð²Ð³Ð¾\nÐ´Ð¾ÐºÐ¸\nÐ´Ð¾ÑÐ¸Ñ‚ÑŒ\nÐ´Ñ€ÑƒÐ³Ð¸Ð¹\nÐ´ÑƒÐ¶Ðµ\nÐ´ÑÐºÑƒÑŽ\nÐµ\nÑ”\nÐ¶\nÐ¶Ðµ\nÐ·\nÐ·Ð°\nÐ·Ð°Ð²Ð¶Ð´Ð¸\nÐ·Ð°Ð·Ð²Ð¸Ñ‡Ð°Ð¹\nÐ·Ð°Ð½Ð°Ð´Ñ‚Ð¾\nÐ·Ð°Ñ€Ð°Ð·\nÐ·Ð°Ñ‚Ðµ\nÐ·Ð²Ð¸Ñ‡Ð°Ð¹Ð½Ð¾\nÐ·Ð²Ñ–Ð´ÑÐ¸\nÐ·Ð²Ñ–Ð´ÑƒÑÑ–Ð»ÑŒ\nÐ·Ð´Ð°Ñ”Ñ‚ÑŒÑÑ\nÐ·Ñ–\nÐ·Ð½Ð°Ñ‡Ð¸Ñ‚ÑŒ\nÐ·Ð½Ð¾Ð²Ñƒ\nÐ·Ð¾Ð²ÑÑ–Ð¼\nÑ–\nÑ–Ð·\nÑ—Ñ—\nÑ—Ð¹\nÑ—Ð¼\nÑ–Ð½Ð¾Ð´Ñ–\nÑ–Ð½ÑˆÐ°\nÑ–Ð½ÑˆÐµ\nÑ–Ð½ÑˆÐ¸Ð¹\nÑ–Ð½ÑˆÐ¸Ñ…\nÑ–Ð½ÑˆÑ–\nÑ—Ñ…\nÐ¹\nÐ¹Ð¾Ð³Ð¾\nÐ¹Ð¾Ð¼Ñƒ\nÐºÐ°Ð¶Ðµ\nÐºÐ¸Ð¼\nÐºÑ–Ð»ÑŒÐºÐ°\nÐºÐ¾Ð³Ð¾\nÐºÐ¾Ð¶ÐµÐ½\nÐºÐ¾Ð¶Ð½Ð°\nÐºÐ¾Ð¶Ð½Ðµ\nÐºÐ¾Ð¶Ð½Ñ–\nÐºÐ¾Ð»Ð¸\nÐºÐ¾Ð¼Ñƒ\nÐºÑ€Ð°Ñ‰Ðµ\nÐºÑ€Ñ–Ð¼\nÐºÑƒÐ´Ð¸\nÐ»Ð°ÑÐºÐ°\nÐ»ÐµÐ´Ð²Ðµ\nÐ»Ð¸ÑˆÐµ\nÐ¼\nÐ¼Ð°Ñ”\nÐ¼Ð°Ð¹Ð¶Ðµ\nÐ¼Ð°Ð»Ð¾\nÐ¼Ð°Ñ‚Ð¸\nÐ¼ÐµÐ½Ðµ\nÐ¼ÐµÐ½Ñ–\nÐ¼ÐµÐ½Ñˆ\nÐ¼ÐµÐ½ÑˆÐµ\nÐ¼Ð¸\nÐ¼Ð¸Ð¼Ð¾\nÐ¼Ñ–Ð³\nÐ¼Ñ–Ð¶\nÐ¼Ñ–Ð¹\nÐ¼Ñ–Ð»ÑŒÐ¹Ð¾Ð½Ñ–Ð²\nÐ¼Ð½Ð¾ÑŽ\nÐ¼Ð¾Ð³Ð¾\nÐ¼Ð¾Ð³Ñ‚Ð¸\nÐ¼Ð¾Ñ”\nÐ¼Ð¾Ñ”Ñ—\nÐ¼Ð¾Ñ”Ð¼Ñƒ\nÐ¼Ð¾Ñ”ÑŽ\nÐ¼Ð¾Ð¶Ðµ\nÐ¼Ð¾Ð¶Ð½Ð°\nÐ¼Ð¾Ð¶Ð½Ð¾\nÐ¼Ð¾Ð¶ÑƒÑ‚ÑŒ\nÐ¼Ð¾Ñ—\nÐ¼Ð¾Ñ—Ð¹\nÐ¼Ð¾Ñ—Ð¼\nÐ¼Ð¾Ñ—Ð¼Ð¸\nÐ¼Ð¾Ñ—Ñ…\nÐ¼Ð¾ÑŽ\nÐ¼Ð¾Ñ\nÐ½Ð°\nÐ½Ð°Ð²Ñ–Ñ‚ÑŒ\nÐ½Ð°Ð²Ñ–Ñ‰Ð¾\nÐ½Ð°Ð²ÐºÐ¾Ð»Ð¾\nÐ½Ð°Ð²ÐºÑ€ÑƒÐ³Ð¸\nÐ½Ð°Ð³Ð¾Ñ€Ñ–\nÐ½Ð°Ð´\nÐ½Ð°Ð·Ð°Ð´\nÐ½Ð°Ð¹Ð±Ñ–Ð»ÑŒÑˆ\nÐ½Ð°Ð¼\nÐ½Ð°Ð¼Ð¸\nÐ½Ð°Ñ€ÐµÑˆÑ‚Ñ–\nÐ½Ð°Ñ\nÐ½Ð°Ñˆ\nÐ½Ð°ÑˆÐ°\nÐ½Ð°ÑˆÐµ\nÐ½Ð°ÑˆÐ¸Ð¼\nÐ½Ð°ÑˆÐ¸Ð¼Ð¸\nÐ½Ð°ÑˆÐ¸Ñ…\nÐ½Ð°ÑˆÑ–\nÐ½Ð°ÑˆÑ–Ð¹\nÐ½Ð°ÑˆÐ¾Ð³Ð¾\nÐ½Ð°ÑˆÐ¾Ñ—\nÐ½Ð°ÑˆÐ¾Ð¼Ñƒ\nÐ½Ð°ÑˆÐ¾ÑŽ\nÐ½Ð°ÑˆÑƒ\nÐ½Ðµ\nÐ½ÐµÐ±Ð°Ð³Ð°Ñ‚Ð¾\nÐ½ÐµÐ±ÑƒÐ´ÑŒ\nÐ½ÐµÐ´Ð°Ð»ÐµÐºÐ¾\nÐ½ÐµÑ—\nÐ½ÐµÐ¼Ð°Ñ”\nÐ½ÐµÑ€Ñ–Ð´ÐºÐ¾\nÐ½ÐµÑ‰Ð¾Ð´Ð°Ð²Ð½Ð¾\nÐ½ÐµÑŽ\nÐ½Ð¸Ð±ÑƒÐ´ÑŒ\nÐ½Ð¸Ð¶Ñ‡Ðµ\nÐ½Ð¸Ð·ÑŒÐºÐ¾\nÐ½Ð¸Ð¼\nÐ½Ð¸Ð¼Ð¸\nÐ½Ð¸Ñ…\nÐ½Ñ–\nÐ½Ñ–Ð±Ð¸\nÐ½Ñ–Ð¶\nÐ½Ñ–Ð¹\nÐ½Ñ–ÐºÐ¾Ð»Ð¸\nÐ½Ñ–ÐºÑƒÐ´Ð¸\nÐ½Ñ–Ð¼\nÐ½Ñ–Ñ‡Ð¾Ð³Ð¾\nÐ½Ñƒ\nÐ½ÑŒÐ¾Ð³Ð¾\nÐ½ÑŒÐ¾Ð¼Ñƒ\nÐ¾\nÐ¾Ð±Ð¸Ð´Ð²Ð°\nÐ¾Ð±Ð¾Ñ”\nÐ¾Ð´Ð¸Ð½\nÐ¾Ð´Ð¸Ð½Ð°Ð´Ñ†ÑÑ‚Ð¸Ð¹\nÐ¾Ð´Ð¸Ð½Ð°Ð´Ñ†ÑÑ‚ÑŒ\nÐ¾Ð´Ð½Ð°Ðº\nÐ¾Ð´Ð½Ñ–Ñ”Ñ—\nÐ¾Ð´Ð½Ñ–Ð¹\nÐ¾Ð´Ð½Ð¾Ð³Ð¾\nÐ¾Ð·Ð½Ð°Ñ‡Ð°Ñ”\nÐ¾ÐºÑ€Ñ–Ð¼\nÐ¾Ð½\nÐ¾ÑÐ¾Ð±Ð»Ð¸Ð²Ð¾\nÐ¾ÑÑŒ\nÐ¿'ÑÑ‚Ð¸Ð¹\nÐ¿'ÑÑ‚Ð½Ð°Ð´Ñ†ÑÑ‚Ð¸Ð¹\nÐ¿'ÑÑ‚Ð½Ð°Ð´Ñ†ÑÑ‚ÑŒ\nÐ¿'ÑÑ‚ÑŒ\nÐ¿ÐµÑ€ÐµÐ´\nÐ¿ÐµÑ€ÑˆÐ¸Ð¹\nÐ¿Ñ–Ð´\nÐ¿Ñ–Ð·Ð½Ñ–ÑˆÐµ\nÐ¿Ñ–Ñ€\nÐ¿Ñ–ÑÐ»Ñ\nÐ¿Ð¾\nÐ¿Ð¾Ð²Ð¸Ð½Ð½Ð¾\nÐ¿Ð¾Ð´Ñ–Ð²\nÐ¿Ð¾ÐºÐ¸\nÐ¿Ð¾Ñ€Ð°\nÐ¿Ð¾Ñ€ÑƒÑ‡\nÐ¿Ð¾ÑÐµÑ€ÐµÐ´\nÐ¿Ð¾Ñ‚Ñ–Ð¼\nÐ¿Ð¾Ñ‚Ñ€Ñ–Ð±Ð½Ð¾\nÐ¿Ð¾Ñ‡Ð°Ð»Ð°\nÐ¿Ð¾Ñ‡Ð°Ñ‚ÐºÑƒ\nÐ¿Ñ€Ð¸\nÐ¿Ñ€Ð¾\nÐ¿Ñ€Ð¾ÑÑ‚Ð¾\nÐ¿Ñ€Ð¾Ñ‚Ðµ\nÐ¿Ñ€Ð¾Ñ‚Ð¸\nÑ€Ð°Ð·\nÑ€Ð°Ð·Ñƒ\nÑ€Ð°Ð½Ñ–ÑˆÐµ\nÑ€Ð°Ð½Ð¾\nÑ€Ð°Ð¿Ñ‚Ð¾Ð¼\nÑ€Ñ–Ðº\nÑ€Ð¾ÐºÐ¸\nÑ€Ð¾ÐºÑ–Ð²\nÑ€Ð¾ÐºÑƒ\nÑ€Ð¾Ñ†Ñ–\nÑÐ°Ð¼\nÑÐ°Ð¼Ð°\nÑÐ°Ð¼Ðµ\nÑÐ°Ð¼Ð¸Ð¼\nÑÐ°Ð¼Ð¸Ð¼Ð¸\nÑÐ°Ð¼Ð¸Ñ…\nÑÐ°Ð¼Ñ–\nÑÐ°Ð¼Ñ–Ð¹\nÑÐ°Ð¼Ð¾\nÑÐ°Ð¼Ð¾Ð³Ð¾\nÑÐ°Ð¼Ð¾Ð¼Ñƒ\nÑÐ°Ð¼Ñƒ\nÑÐ²Ð¾Ð³Ð¾\nÑÐ²Ð¾Ñ”\nÑÐ²Ð¾Ñ”Ñ—\nÑÐ²Ð¾Ñ—\nÑÐ²Ð¾Ñ—Ð¹\nÑÐ²Ð¾Ñ—Ñ…\nÑÐ²Ð¾ÑŽ\nÑÐµÐ±Ðµ\nÑÐ¸Ñ…\nÑÑ–Ð¼\nÑÑ–Ð¼Ð½Ð°Ð´Ñ†ÑÑ‚Ð¸Ð¹\nÑÑ–Ð¼Ð½Ð°Ð´Ñ†ÑÑ‚ÑŒ\nÑÐºÐ°Ð·Ð°Ð²\nÑÐºÐ°Ð·Ð°Ð»Ð°\nÑÐºÐ°Ð·Ð°Ñ‚Ð¸\nÑÐºÑ–Ð»ÑŒÐºÐ¸\nÑÐºÑ€Ñ–Ð·ÑŒ\nÑÐ¾Ð±Ñ–\nÑÐ¾Ð±Ð¾ÑŽ\nÑÐ¿Ð°ÑÐ¸Ð±Ñ–\nÑÐ¿Ð¾Ñ‡Ð°Ñ‚ÐºÑƒ\nÑÐ¿Ñ€Ð°Ð²\nÑÑ‚Ð°Ð²\nÑÑƒÑ‚ÑŒ\nÑÑŒÐ¾Ð³Ð¾Ð´Ð½Ñ–\nÑÑŒÐ¾Ð¼Ð¸Ð¹\nÑ‚\nÑ‚Ð°\nÑ‚Ð°Ðº\nÑ‚Ð°ÐºÐ°\nÑ‚Ð°ÐºÐµ\nÑ‚Ð°ÐºÐ¸Ð¹\nÑ‚Ð°ÐºÑ–\nÑ‚Ð°ÐºÐ¾Ð¶\nÑ‚Ð°Ð¼\nÑ‚Ð²Ñ–Ð¹\nÑ‚Ð²Ð¾Ð³Ð¾\nÑ‚Ð²Ð¾Ñ”\nÑ‚Ð²Ð¾Ñ”Ñ—\nÑ‚Ð²Ð¾Ñ”Ð¼Ñƒ\nÑ‚Ð²Ð¾Ñ”ÑŽ\nÑ‚Ð²Ð¾Ñ—\nÑ‚Ð²Ð¾Ñ—Ð¹\nÑ‚Ð²Ð¾Ñ—Ð¼\nÑ‚Ð²Ð¾Ñ—Ð¼Ð¸\nÑ‚Ð²Ð¾Ñ—Ñ…\nÑ‚Ð²Ð¾ÑŽ\nÑ‚Ð²Ð¾Ñ\nÑ‚Ðµ\nÑ‚ÐµÐ±Ðµ\nÑ‚ÐµÐ¶\nÑ‚ÐµÐ¿ÐµÑ€\nÑ‚Ð¸\nÑ‚Ð¸Ð¼\nÑ‚Ð¸Ð¼Ð¸\nÑ‚Ð¸ÑÑÑ‡\nÑ‚Ð¸Ñ…\nÑ‚Ñ–\nÑ‚Ñ–Ñ”Ñ—\nÑ‚Ñ–Ñ”ÑŽ\nÑ‚Ñ–Ð¹\nÑ‚Ñ–Ð»ÑŒÐºÐ¸\nÑ‚Ñ–Ð¼\nÑ‚Ð¾\nÑ‚Ð¾Ð±Ñ–\nÑ‚Ð¾Ð±Ð¾ÑŽ\nÑ‚Ð¾Ð³Ð¾\nÑ‚Ð¾Ð´Ñ–\nÑ‚Ð¾Ð¹\nÑ‚Ð¾Ð¼Ñƒ\nÑ‚Ð¾ÑŽ\nÑ‚Ñ€ÐµÐ±Ð°\nÑ‚Ñ€ÐµÑ‚Ñ–Ð¹\nÑ‚Ñ€Ð¸\nÑ‚Ñ€Ð¸Ð½Ð°Ð´Ñ†ÑÑ‚Ð¸Ð¹\nÑ‚Ñ€Ð¸Ð½Ð°Ð´Ñ†ÑÑ‚ÑŒ\nÑ‚Ñ€Ð¾Ñ…Ð¸\nÑ‚Ñƒ\nÑ‚ÑƒÐ´Ð¸\nÑ‚ÑƒÑ‚\nÑƒ\nÑƒÐ²ÐµÑÑŒ\nÑƒÐ¼Ñ–Ñ‚Ð¸\nÑƒÑÐµ\nÑƒÑÑ–\nÑƒÑÑ–Ð¼\nÑƒÑÑ–Ð¼Ð°\nÑƒÑÑ–Ñ…\nÑƒÑÑŒÐ¾Ð³Ð¾\nÑƒÑÑŒÐ¾Ð¼Ñƒ\nÑƒÑÑŽ\nÑƒÑÑŽÐ´Ð¸\nÑƒÑÑ\nÑ…Ñ–Ð±Ð°\nÑ…Ð¾Ñ‚Ñ–Ñ‚Ð¸\nÑ…Ð¾Ñ‡\nÑ…Ð¾Ñ‡Ð°\nÑ…Ð¾Ñ‡ÐµÑˆ\nÑ…Ñ‚Ð¾\nÑ†Ðµ\nÑ†ÐµÐ¹\nÑ†Ð¸Ð¼\nÑ†Ð¸Ð¼Ð¸\nÑ†Ð¸Ñ…\nÑ†Ñ–\nÑ†Ñ–Ñ”Ñ—\nÑ†Ñ–Ð¹\nÑ†ÑŒÐ¾Ð³Ð¾\nÑ†ÑŒÐ¾Ð¼Ñƒ\nÑ†ÑŽ\nÑ†Ñ\nÑ‡Ð°Ñ\nÑ‡Ð°ÑÑ‚Ñ–ÑˆÐµ\nÑ‡Ð°ÑÑ‚Ð¾\nÑ‡Ð°ÑÑƒ\nÑ‡ÐµÑ€ÐµÐ·\nÑ‡ÐµÑ‚Ð²ÐµÑ€Ñ‚Ð¸Ð¹\nÑ‡Ð¸\nÑ‡Ð¸Ñ”\nÑ‡Ð¸Ñ”Ñ—\nÑ‡Ð¸Ñ”Ð¼Ñƒ\nÑ‡Ð¸Ñ—\nÑ‡Ð¸Ñ—Ð¹\nÑ‡Ð¸Ñ—Ð¼\nÑ‡Ð¸Ñ—Ð¼Ð¸\nÑ‡Ð¸Ñ—Ñ…\nÑ‡Ð¸Ð¹\nÑ‡Ð¸Ð¹Ð¾Ð³Ð¾\nÑ‡Ð¸Ð¹Ð¾Ð¼Ñƒ\nÑ‡Ð¸Ð¼\nÑ‡Ð¸ÑÐ»ÐµÐ½Ð½Ð°\nÑ‡Ð¸ÑÐ»ÐµÐ½Ð½Ðµ\nÑ‡Ð¸ÑÐ»ÐµÐ½Ð½Ð¸Ð¹\nÑ‡Ð¸ÑÐ»ÐµÐ½Ð½Ñ–\nÑ‡Ð¸ÑŽ\nÑ‡Ð¸Ñ\nÑ‡Ð¾Ð³Ð¾\nÑ‡Ð¾Ð¼Ñƒ\nÑ‡Ð¾Ñ‚Ð¸Ñ€Ð¸\nÑ‡Ð¾Ñ‚Ð¸Ñ€Ð½Ð°Ð´Ñ†ÑÑ‚Ð¸Ð¹\nÑ‡Ð¾Ñ‚Ð¸Ñ€Ð½Ð°Ð´Ñ†ÑÑ‚ÑŒ\nÑˆÑ–ÑÑ‚Ð½Ð°Ð´Ñ†ÑÑ‚Ð¸Ð¹\nÑˆÑ–ÑÑ‚Ð½Ð°Ð´Ñ†ÑÑ‚ÑŒ\nÑˆÑ–ÑÑ‚ÑŒ\nÑˆÐ¾ÑÑ‚Ð¸Ð¹\nÑ‰Ðµ\nÑ‰Ð¾\nÑ‰Ð¾Ð±\nÑ‰Ð¾Ð´Ð¾\nÑ‰Ð¾ÑÑŒ\nÑ\nÑÐº\nÑÐºÐ°\nÑÐºÐ¸Ð¹\nÑÐºÐ¸Ñ…\nÑÐºÑ–\nÑÐºÑ–Ð¹\nÑÐºÐ¾Ð³Ð¾\nÑÐºÐ¾Ñ—\nÑÐºÑ‰Ð¾".split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/uk/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.uk.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/uk/__init__.py----------------------------------------
spacy.lang.uk.__init__.Ukrainian(Language)
spacy.lang.uk.__init__.UkrainianDefaults(BaseDefaults)
spacy.lang.uk.__init__.make_lemmatizer(nlp:Language,model:Optional[Model],name:str,mode:str,overwrite:bool,scorer:Optional[Callable])


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/uk/lex_attrs.py----------------------------------------
A:spacy.lang.uk.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.uk.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.uk.lex_attrs.like_num(text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/hr/examples.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/hr/stop_words.py----------------------------------------
A:spacy.lang.hr.stop_words.STOP_WORDS->set('\na\nah\naha\naj\nako\nal\nali\narh\nau\navaj\nbar\nbaÅ¡\nbez\nbi\nbih\nbijah\nbijahu\nbijaÅ¡e\nbijasmo\nbijaste\nbila\nbili\nbilo\nbio\nbismo\nbiste\nbiti\nbrr\nbuÄ‡\nbudavÅ¡i\nbude\nbudimo\nbudite\nbudu\nbuduÄ‡i\nbum\nbumo\nÄ‡e\nÄ‡emo\nÄ‡eÅ¡\nÄ‡ete\nÄijem\nÄijim\nÄijima\nÄ‡u\nda\ndaj\ndakle\nde\ndeder\ndem\ndjelomice\ndjelomiÄno\ndo\ndoista\ndok\ndokle\ndonekle\ndosad\ndoskoro\ndotad\ndotle\ndoveÄer\ndrugamo\ndrugdje\nduÅ¾\ne\neh\nehe\nej\neno\neto\nevo\nga\ngdjekakav\ngdjekoje\ngic\ngod\nhalo\nhej\nhm\nhoÄ‡e\nhoÄ‡emo\nhoÄ‡eÅ¡\nhoÄ‡ete\nhoÄ‡u\nhop\nhtijahu\nhtijasmo\nhtijaste\nhtio\nhtjedoh\nhtjedoÅ¡e\nhtjedoste\nhtjela\nhtjele\nhtjeli\nhura\ni\niako\nih\niju\nijuju\nikada\nikakav\nikakva\nikakve\nikakvi\nikakvih\nikakvim\nikakvima\nikakvo\nikakvog\nikakvoga\nikakvoj\nikakvom\nikakvome\nili\nim\niz\nja\nje\njedna\njedne\njedni\njedno\njer\njesam\njesi\njesmo\njest\njeste\njesu\njim\njoj\njoÅ¡\nju\nkada\nkako\nkao\nkoja\nkoje\nkoji\nkojima\nkoju\nkroz\nlani\nli\nme\nmene\nmeni\nmi\nmimo\nmoj\nmoja\nmoje\nmoji\nmoju\nmu\nna\nnad\nnakon\nnam\nnama\nnas\nnaÅ¡\nnaÅ¡a\nnaÅ¡e\nnaÅ¡eg\nnaÅ¡i\nne\nneÄ‡e\nneÄ‡emo\nneÄ‡eÅ¡\nneÄ‡ete\nneÄ‡u\nnego\nneka\nneke\nneki\nnekog\nneku\nnema\nneÅ¡to\nnetko\nni\nnije\nnikoga\nnikoje\nnikoji\nnikoju\nnisam\nnisi\nnismo\nniste\nnisu\nnjega\nnjegov\nnjegova\nnjegovo\nnjemu\nnjezin\nnjezina\nnjezino\nnjih\nnjihov\nnjihova\nnjihovo\nnjim\nnjima\nnjoj\nnju\nno\no\nod\nodmah\non\nona\none\noni\nono\nonu\nonoj\nonom\nonim\nonima\nova\novaj\novim\novima\novoj\npa\npak\npljus\npo\npod\npodalje\npoimence\npoizdalje\nponekad\npored\npostrance\npotajice\npotrbuÅ¡ke\npouzdano\nprije\ns\nsa\nsam\nsamo\nsasvim\nsav\nse\nsebe\nsebi\nsi\nÅ¡ic\nsmo\nste\nÅ¡to\nÅ¡ta\nÅ¡togod\nÅ¡tagod\nsu\nsva\nsve\nsvi\nsvi\nsvog\nsvoj\nsvoja\nsvoje\nsvoju\nsvom\nsvu\nta\ntada\ntaj\ntako\nte\ntebe\ntebi\nti\ntim\ntima\nto\ntoj\ntome\ntu\ntvoj\ntvoja\ntvoje\ntvoji\ntvoju\nu\nusprkos\nutaman\nuvijek\nuz\nuza\nuzagrapce\nuzalud\nuzduÅ¾\nvaljda\nvam\nvama\nvas\nvaÅ¡\nvaÅ¡a\nvaÅ¡e\nvaÅ¡im\nvaÅ¡ima\nveÄ‡\nvi\nvjerojatno\nvjerovatno\nvrh\nvrlo\nza\nzaista\nzar\nzatim\nzato\nzbija\nzbog\nÅ¾eleÄ‡i\nÅ¾eljah\nÅ¾eljela\nÅ¾eljele\nÅ¾eljeli\nÅ¾eljelo\nÅ¾eljen\nÅ¾eljena\nÅ¾eljene\nÅ¾eljeni\nÅ¾eljenu\nÅ¾eljeo\nzimus\nzum\n'.split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/hr/__init__.py----------------------------------------
spacy.lang.hr.__init__.Croatian(Language)
spacy.lang.hr.__init__.CroatianDefaults(BaseDefaults)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/sl/examples.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/sl/stop_words.py----------------------------------------
A:spacy.lang.sl.stop_words.STOP_WORDS->set('\na ali \n\nb bi bil bila bile bili bilo biti blizu bo bodo bojo bolj bom bomo \nboste bova boÅ¡ brez\n\nc cel cela celi celo\n\nÄ Äe Äesto Äetrta Äetrtek Äetrti Äetrto Äez Äigav\n\nd da daleÄ dan danes datum deset deseta deseti deseto devet\ndeveta deveti deveto do dober dobra dobri dobro dokler dol dolg\ndolga dolgi dovolj drug druga drugi drugo dva dve\n\ne eden en ena ene eni enkrat eno etc.\n\nf\n\ng g. ga ga. gor gospa gospod \n\nh halo \n\ni idr. ii iii in iv ix iz\n\nj jaz je ji jih jim jo jutri\n\nk kadarkoli kaj kajti kako kakor kamor kamorkoli kar karkoli\nkaterikoli kdaj kdo kdorkoli ker ki kje kjer kjerkoli\nko koder koderkoli koga komu kot kratek kratka kratke kratki\n\nl lahka lahke lahki lahko le lep lepa lepe lepi lepo leto\n\nm majhen majhna majhni malce malo manj me med medtem mene\nmesec mi midva midve mnogo moj moja moje mora morajo moram\nmoramo morate moraÅ¡ morem mu\n\nn na nad naj najina najino najmanj naju najveÄ nam narobe\nnas nato nazaj naÅ¡ naÅ¡a naÅ¡e ne nedavno nedelja nek neka\nnekaj nekatere nekateri nekatero nekdo neke nekega neki\nnekje neko nekoga nekoÄ ni nikamor nikdar nikjer nikoli\nniÄ nje njega njegov njegova njegovo njej njemu njen\nnjena njeno nji njih njihov njihova njihovo njiju njim\nnjo njun njuna njuno no nocoj npr.\n\no ob oba obe oboje od odprt odprta odprti okoli on\nonadva one oni onidve osem osma osmi osmo oz.\n\np pa pet peta petek peti peto po pod pogosto poleg poln\npolna polni polno ponavadi ponedeljek ponovno potem\npovsod pozdravljen pozdravljeni prav prava prave pravi\npravo prazen prazna prazno prbl. precej pred prej preko\npri pribl. pribliÅ¾no primer pripravljen pripravljena\npripravljeni proti prva prvi prvo\n\nr ravno redko res reÄ\n\ns saj sam sama same sami samo se sebe sebi sedaj sedem\nsedma sedmi sedmo sem seveda si sicer skoraj skozi slab sm\nso sobota spet sreda srednja srednji sta ste stran stvar sva\n\nÅ¡ Å¡est Å¡esta Å¡esti Å¡esto Å¡tiri \n\nt ta tak taka take taki tako takoj tam te tebe tebi tega\nteÅ¾ak teÅ¾ka teÅ¾ki teÅ¾ko ti tista tiste tisti tisto tj.\ntja to toda torek tretja tretje tretji tri tu tudi tukaj\ntvoj tvoja tvoje\n\nu\n\nv vaju vam vas vaÅ¡ vaÅ¡a vaÅ¡e ve vedno velik velika veliki\nveliko vendar ves veÄ vi vidva vii viii visok visoka visoke\nvisoki vsa vsaj vsak vsaka vsakdo vsake vsaki vsakomur vse\nvsega vsi vso vÄasih vÄeraj \n\nx \n\nz za zadaj zadnji zakaj zaprta zaprti zaprto zdaj zelo zunaj\n\nÅ¾ Å¾e\n'.split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/sl/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.sl.tokenizer_exceptions.abbrv->'\nCo. Ch. DIPL. DR. Dr. Ev. Inc. Jr. Kr. Mag. M. MR. Mr. Mt. Murr. Npr. OZ. \nOpr. Osn. Prim. Roj. ST. Sim. Sp. Sred. St. Sv. Å kofl. Tel. UR. Zb. \na. aa. ab. abc. abit. abl. abs. abt. acc. accel. add. adj. adv. aet. afr. akad. al. alban. all. alleg. \nalp. alt. alter. alÅ¾ir. am. an. andr. ang. anh. anon. ans. antrop. apoc. app. approx. apt. ar. arc. arch. \narh. arr. as. asist. assist. assoc. asst. astr. attn. aug. avstral. az. b. bab. bal. bbl. bd. belg. bioinf. \nbiomed. bk. bl. bn. borg. bp. br. braz. brit. bros. broÅ¡. bt. bu. c. ca. cal. can. cand. cantab. cap. capt.\ncat. cath. cc. cca. cd. cdr. cdre. cent. cerkv. cert. cf. cfr. ch. chap. chem. chr. chs. cic. circ. civ. cl.\ncm. cmd. cnr. co. cod. col. coll. colo. com. comp. con. conc. cond. conn. cons. cont. coop. corr. cost. cp.\ncpl. cr. crd. cres. cresc. ct. cu. d. dan. dat. davÄ. ddr. dec. ded. def. dem. dent. dept. dia. dip. dipl. \ndir. disp. diss. div. do. doc. dok. dol. doo. dop. dott. dr. dram. druÅ¾. druÅ¾b. drÅ¾. dt. duh. dur. dvr. dwt. e.\nea. ecc. eccl. eccles. econ. edn. egipt. egr. ekon. eksp. el. em. enc. eng. eo. ep. err. esp. esq. est.\net. etc. etnogr. etnol. ev. evfem. evr. ex. exc. excl. exp. expl. ext. exx. f. fa. facs. fak. faks. fas.\nfasc. fco. fcp. feb. febr. fec. fed. fem. ff. fff. fid. fig. fil. film. fiziol. fiziot. flam. fm. fo. fol. folk.\nfrag. fran. franc. fsc. g. ga. gal. gdÄ. ge. gen. geod. geog. geotehnol. gg. gimn. glas. glav. gnr. go. gor.\ngosp. gp. graf. gram. gren. grÅ¡. gs. h. hab. hf. hist. ho. hort. i. ia. ib. ibid. id. idr. idridr. ill. imen.\nimp. impf. impr. in. inc. incl. ind. indus. inf. inform. ing. init. ins. int. inv. inÅ¡p. inÅ¡tr. inÅ¾. is. islam.\nist. ital. iur. iz. izbr. izd. izg. izgr. izr. izv. j. jak. jam. jan. jav. je. jez. jr. jsl. jud. jug.\njugoslovan. jur. juÅ¾. jv. jz. k. kal. kan. kand. kat. kdo. kem. kip. kmet. kol. kom. komp. konf. kont. kost. kov. \nkp. kpfw. kr. kraj. krat. kub. kult. kv. kval. l. la. lab. lb. ld. let. lib. lik. litt. lj. ljud. ll. loc. log. \nloÄ. lt. ma. madÅ¾. mag. manag. manjÅ¡. masc. mass. mater. max. maxmax. mb. md. mech. medic. medij. medn. \nmehÄ. mem. menedÅ¾. mes. mess. metal. meteor. meteorol. mex. mi. mikr. mil. minn. mio. misc. miss. mit. mk. \nmkt. ml. mlad. mlle. mlr. mm. mme. mnoÅ¾. mo. moj. moÅ¡. moÅ¾n. mr. mrd. mrs. ms. msc. msgr. mt. murr. mus. mut. \nn. na. nad. nadalj. nadom. nagl. nakl. namer. nan. naniz. nasl. nat. navt. naÄ. ned. nem. nik. nizoz. nm. nn. \nno. nom. norv. notr. nov. novogr. ns. o. ob. obd. obj. oblaÄ. obl. oblik. obr. obraz. obs. obst. obt. obÄ. oc. \noct. od. odd. odg. odn. odst. odv. oec. off. ok. okla. okr. ont. oo. op. opis. opp. opr. orch. ord. ore. oreg. \norg. orient. orig. ork. ort. oseb. osn. ot. ozir. oÅ¡k. p. pag. par. para. parc. parl. part. past. pat. pdk. \npen. perf. pert. perz. pesn. pet. pev. pf. pfc. ph. pharm. phil. pis. pl. po. pod. podr. podaljÅ¡. pogl. pogoj. pojm. \npok. pokr. pol. poljed. poljub. polu. pom. pomen. pon. ponov. pop. por. port. pos. posl. posn. pov. pp. ppl. pr. \npraet. prav. pravopis. pravosl. preb. pred. predl. predm. predp. preds. pref. pregib. prel. prem. premen. prep. \npres. pret. prev. pribl. prih. pril. primerj. primor. prip. pripor. prir. prist. priv. proc. prof. prog. proiz. \nprom. pron. prop. prot. protest. prov. ps. pss. pt. publ. pz. q. qld. qu. quad. que. r. racc. rastl. razgl. \nrazl. razv. rd. red. ref. reg. rel. relig. rep. repr. rer. resp. rest. ret. rev. revol. reÅ¾. rim. rist. rkp. rm. \nroj. rom. romun. rp. rr. rt. rud. ruÅ¡. ry. sal. samogl. san. sc. scen. sci. scr. sdv. seg. sek. sen. sept. ser. \nsev. sg. sgt. sh. sig. sigg. sign. sim. sin. sing. sinh. skand. skl. sklad. sklanj. sklep. skr. sl. slik. slov. \nslovak. slovn. sn. so. sob. soc. sociol. sod. sopomen. sopr. sor. sov. sovj. sp. spec. spl. spr. spreg. sq. sr. \nsre. sred. sredoz. srh. ss. ssp. st. sta. stan. stanstar. stcsl. ste. stim. stol. stom. str. stroj. strok. stsl. \nstud. sup. supl. suppl. svet. sz. t. tab. tech. ted. tehn. tehnol. tek. teks. tekst. tel. temp. ten. teol. ter. \nterm. test. th. theol. tim. tip. tisoÄl. tit. tl. tol. tolmaÄ. tom. tor. tov. tr. trad. traj. trans. tren. \ntrib. tril. trop. trp. trÅ¾. ts. tt. tu. tur. turiz. tvor. tvorb. tÄ. u. ul. umet. un. univ. up. upr. ur. urad. \nus. ust. utr. v. va. val. var. varn. ven. ver. verb. vest. vezal. vic. vis. viv. viz. viÅ¡. vod. vok. vol. vpr. \nvrst. vrstil. vs. vv. vzd. vzg. vzh. vzor. w. wed. wg. wk. x. y. z. zah. zaim. zak. zap. zasl. zavar. zaÄ. zb. \nzdruÅ¾. zg. zn. znan. znanstv. zoot. zun. zv. zvd. Ã¡. Ã©. Ä‡. Ä. Äas. Äet. Äl. Älen. Äustv. Ä‘. Ä¾. Å‚. ÅŸ. Å T. Å¡. Å¡ir. \nÅ¡kofl. Å¡kot. Å¡ol. Å¡t. Å¡tevil. Å¡tud. Å¯. Å±. Å¾en. Å¾ival. \n'.split()
A:spacy.lang.sl.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/sl/__init__.py----------------------------------------
spacy.lang.sl.__init__.Slovenian(Language)
spacy.lang.sl.__init__.SlovenianDefaults(BaseDefaults)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/sl/lex_attrs.py----------------------------------------
A:spacy.lang.sl.lex_attrs._num_words->set('\n\tnula niÄla niÄ ena dva tri Å¡tiri pet Å¡est sedem osem\n\tdevet deset enajst dvanajst trinajst Å¡tirinajst petnajst\n\tÅ¡estnajst sedemnajst osemnajst devetnajst dvajset trideset Å¡tirideset\n\tpetdeset Å¡estdest sedemdeset osemdeset devedeset sto tisoÄ\n\tmilijon bilijon trilijon kvadrilijon neÅ¡teto\n\t\n\ten eden enega enemu ennem enim enih enima enimi ene eni eno\n\tdveh dvema dvem dvoje trije treh trem tremi troje Å¡tirje Å¡tirih Å¡tirim Å¡tirimi\n\tpetih petim petimi Å¡estih Å¡estim Å¡estimi sedmih sedmim sedmimi osmih osmim osmimi\n\tdevetih devetim devetimi desetih desetim desetimi enajstih enajstim enajstimi\n\tdvanajstih dvanajstim dvanajstimi trinajstih trinajstim trinajstimi\n\tÅ¡estnajstih Å¡estnajstim Å¡estnajstimi petnajstih petnajstim petnajstimi\n\tsedemnajstih sedemnajstim sedemnajstimi osemnajstih osemnajstim osemnajstimi\n\tdevetnajstih devetnajstim devetnajstimi dvajsetih dvajsetim dvajsetimi  \n\t'.split())
A:spacy.lang.sl.lex_attrs._ordinal_words->set('\n\tprvi drugi tretji Äetrti peti Å¡esti sedmi osmi\n\tdeveti deseti enajsti dvanajsti trinajsti Å¡tirinajsti\n\tpetnajsti Å¡estnajsti sedemnajsti osemnajsti devetnajsti\n\tdvajseti trideseti Å¡tirideseti petdeseti Å¡estdeseti sedemdeseti\n\tosemdeseti devetdeseti stoti tisoÄi milijonti bilijonti\n\ttrilijonti kvadrilijonti neÅ¡teti\n\t\n\tprva druga tretja Äetrta peta Å¡esta sedma osma\n\tdeveta deseta enajsta dvanajsta trinajsta Å¡tirnajsta\n\tpetnajsta Å¡estnajsta sedemnajsta osemnajsta devetnajsta\n\tdvajseta trideseta Å¡tirideseta petdeseta Å¡estdeseta sedemdeseta\n\tosemdeseta devetdeseta stota tisoÄa milijonta bilijonta\n\ttrilijonta kvadrilijonta neÅ¡teta\n\t\n\tprvo drugo tretje Äetrto peto Å¡estro sedmo osmo\n\tdeveto deseto enajsto dvanajsto trinajsto Å¡tirnajsto\n\tpetnajsto Å¡estnajsto sedemnajsto osemnajsto devetnajsto\n\tdvajseto trideseto Å¡tirideseto petdeseto Å¡estdeseto sedemdeseto\n\tosemdeseto devetdeseto stoto tisoÄo milijonto bilijonto\n\ttrilijonto kvadrilijonto neÅ¡teto\n\t\n\tprvega drugega tretjega Äetrtega petega Å¡estega sedmega osmega \n\tdevega desetega enajstega dvanajstega trinajstega Å¡tirnajstega\n\tpetnajstega Å¡estnajstega sedemnajstega osemnajstega devetnajstega\n\tdvajsetega tridesetega Å¡tiridesetega petdesetega Å¡estdesetega sedemdesetega\n\tosemdesetega devetdesetega stotega tisoÄega milijontega bilijontega\n\ttrilijontega kvadrilijontega neÅ¡tetega\n\t\n\tprvemu drugemu tretjemu Äetrtemu petemu Å¡estemu sedmemu osmemu devetemu desetemu \n\tenajstemu dvanajstemu trinajstemu Å¡tirnajstemu petnajstemu Å¡estnajstemu sedemnajstemu\n\tosemnajstemu devetnajstemu dvajsetemu tridesetemu Å¡tiridesetemu petdesetemu Å¡estdesetemu\n\tsedemdesetemu osemdesetemu devetdesetemu stotemu tisoÄemu milijontemu bilijontemu\n\ttrilijontemu kvadrilijontemu neÅ¡tetemu\n\t\n\tprvem drugem tretjem Äetrtem petem Å¡estem sedmem osmem devetem desetem\n\tenajstem dvanajstem trinajstem Å¡tirnajstem petnajstem Å¡estnajstem sedemnajstem\n\tosemnajstem devetnajstem dvajsetem tridesetem Å¡tiridesetem petdesetem Å¡estdesetem\n\tsedemdesetem osemdesetem devetdesetem stotem tisoÄem milijontem bilijontem\n\ttrilijontem kvadrilijontem neÅ¡tetem\n\t\n\tprvim drugim tretjim Äetrtim petim Å¡estim sedtim osmim devetim desetim\n\tenajstim dvanajstim trinajstim Å¡tirnajstim petnajstim Å¡estnajstim sedemnajstim\n\tosemnajstim devetnajstim dvajsetim tridesetim Å¡tiridesetim petdesetim Å¡estdesetim\n\tsedemdesetim osemdesetim devetdesetim stotim tisoÄim milijontim bilijontim\n\ttrilijontim kvadrilijontim neÅ¡tetim\n\t    \n\tprvih drugih tretjih Äetrthih petih Å¡estih sedmih osmih deveth desetih\n\tenajstih dvanajstih trinajstih Å¡tirnajstih petnajstih Å¡estnajstih sedemnajstih\n\tosemnajstih devetnajstih dvajsetih tridesetih Å¡tiridesetih petdesetih Å¡estdesetih\n\tsedemdesetih osemdesetih devetdesetih stotih tisoÄih milijontih bilijontih\n\ttrilijontih kvadrilijontih neÅ¡teth\n\t\n\tprvima drugima tretjima Äetrtima petima Å¡estima sedmima osmima devetima desetima\n\tenajstima dvanajstima trinajstima Å¡tirnajstima petnajstima Å¡estnajstima sedemnajstima\n\tosemnajstima devetnajstima dvajsetima tridesetima Å¡tiridesetima petdesetima Å¡estdesetima\n\tsedemdesetima osemdesetima devetdesetima stotima tisoÄima milijontima bilijontima\n\ttrilijontima kvadrilijontima neÅ¡tetima\n\t\n\tprve druge Äetrte pete Å¡este sedme osme devete desete\n\tenajste dvanajste trinajste Å¡tirnajste petnajste Å¡estnajste sedemnajste\n\tosemnajste devetnajste dvajsete tridesete Å¡tiridesete petdesete Å¡estdesete\n\tsedemdesete osemdesete devetdesete stote tisoÄe milijonte bilijonte \n\ttrilijonte kvadrilijonte neÅ¡tete\n\t\n\tprvimi drugimi tretjimi Äetrtimi petimi Å¡estimi sedtimi osmimi devetimi desetimi\n\tenajstimi dvanajstimi trinajstimi Å¡tirnajstimi petnajstimi Å¡estnajstimi sedemnajstimi\n\tosemnajstimi devetnajstimi dvajsetimi tridesetimi Å¡tiridesetimi petdesetimi Å¡estdesetimi\n\tsedemdesetimi osemdesetimi devetdesetimi stotimi tisoÄimi milijontimi bilijontimi\n\ttrilijontimi kvadrilijontimi neÅ¡tetimi\n\t'.split())
A:spacy.lang.sl.lex_attrs._currency_words->set('\n\tevro evra evru evrom evrov evroma evrih evrom evre evri evr eur\n\tcent centa centu cenom centov centoma centih centom cente centi\n\tdolar dolarja dolarji dolarju dolarjem dolarjev dolarjema dolarjih dolarje usd\n\ttolar tolarja tolarji tolarju tolarjem tolarjev tolarjema tolarjih tolarje tol\n\tdinar dinarja dinarji dinarju dinarjem dinarjev dinarjema dinarjih dinarje din\n\tfunt funta funti funtu funtom funtov funtoma funtih funte gpb\n\tforint forinta forinti forintu forintom forintov forintoma forintih forinte\n\tzlot zlota zloti zlotu zlotom zlotov zlotoma zlotih zlote \n\trupij rupija rupiji rupiju rupijem rupijev rupijema rupijih rupije\n\tjen jena jeni jenu jenom jenov jenoma jenih jene\n\tkuna kuni kune kuno kun kunama kunah kunam kunami\n\tmarka marki marke markama markah markami \n\t'.split())
A:spacy.lang.sl.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.sl.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
A:spacy.lang.sl.lex_attrs.text_lower->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').lower()
spacy.lang.sl.lex_attrs.is_currency(text)
spacy.lang.sl.lex_attrs.like_num(text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/sl/punctuation.py----------------------------------------
A:spacy.lang.sl.punctuation.CONCAT_QUOTES->char_classes.CONCAT_QUOTES.replace("'", '').replace("'", '')


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/et/stop_words.py----------------------------------------
A:spacy.lang.et.stop_words.STOP_WORDS->set('\naga\nei\net\nja\njah\nkas\nkui\nkÃµik\nma\nme\nmida\nmidagi\nmind\nminu\nmis\nmu\nmul\nmulle\nnad\nnii\noled\nolen\noli\noma\non\npole\nsa\nseda\nsee\nselle\nsiin\nsiis\nta\nte\nÃ¤ra\n'.split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/et/__init__.py----------------------------------------
spacy.lang.et.__init__.Estonian(Language)
spacy.lang.et.__init__.EstonianDefaults(BaseDefaults)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/gu/examples.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/gu/stop_words.py----------------------------------------
A:spacy.lang.gu.stop_words.STOP_WORDS->set('\nàªàª®\nàª†\nàª\nàª°àª¹à«€\nàª›à«‡\nàª›à«‹\nàª¹àª¤àª¾\nàª¹àª¤à«àª‚\nàª¹àª¤à«€\nàª¹à«‹àª¯\nàª¹àª¤à«‹\nàª¶àª•à«‡\nàª¤à«‡\nàª¤à«‡àª¨àª¾\nàª¤à«‡àª¨à«àª‚\nàª¤à«‡àª¨à«‡\nàª¤à«‡àª¨à«€\nàª¤à«‡àª“\nàª¤à«‡àª®àª¨à«‡\nàª¤à«‡àª®àª¨àª¾\nàª¤à«‡àª®àª£à«‡\nàª¤à«‡àª®àª¨à«àª‚\nàª¤à«‡àª®àª¾àª‚\nàª…àª¨à«‡\nàª…àª¹à«€àª‚\nàª¥à«€\nàª¥àªˆ\nàª¥àª¾àª¯\nàªœà«‡\n àª¨à«‡\nàª•à«‡\nàª¨àª¾\nàª¨à«€\nàª¨à«‹\nàª¨à«‡\nàª¨à«àª‚\nàª¶à«àª‚\nàª®àª¾àª‚\nàªªàª£\nàªªàª°\nàªœà«‡àªµàª¾\nàªœà«‡àªµà«àª‚\nàªœàª¾àª¯\nàªœà«‡àª®\nàªœà«‡àª¥à«€\nàª®àª¾àª¤à«àª°\nàª®àª¾àªŸà«‡\nàªªàª°àª¥à«€\nàª†àªµà«àª¯à«àª‚\nàªàªµà«€\nàª†àªµà«€\nàª°à«€àª¤à«‡\nàª¸à«àª§à«€\nàª¥àª¾àª¯\nàª¥àªˆ\nàª¸àª¾àª¥à«‡\nàª²àª¾àª—à«‡\nàª¹à«‹àªµàª¾\nàª›àª¤àª¾àª‚\nàª°àª¹à«‡àª²àª¾\nàª•àª°à«€\nàª•àª°à«‡\nàª•à«‡àªŸàª²àª¾\nàª•à«‹àªˆ\nàª•à«‡àª®\nàª•àª°à«àª¯à«‹\nàª•àª°à«àª¯à«\nàª•àª°à«‡\nàª¸à«Œàª¥à«€\nàª¤à«àª¯àª¾àª°àª¬àª¾àª¦\nàª¤àª¥àª¾\nàª¦à«àªµàª¾àª°àª¾\nàªœà«àª“\nàªœàª¾àª“\nàªœà«àª¯àª¾àª°à«‡\nàª¤à«àª¯àª¾àª°à«‡\nàª¶àª•à«‹\nàª¨àª¥à«€\nàª¹àªµà«‡\nàª…àª¥àªµàª¾\nàª¥àª¤à«‹\nàª¦àª°\nàªàªŸàª²à«‹\nàªªàª°àª‚àª¤à«\n'.split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/gu/__init__.py----------------------------------------
spacy.lang.gu.__init__.Gujarati(Language)
spacy.lang.gu.__init__.GujaratiDefaults(BaseDefaults)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/pl/examples.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/pl/lemmatizer.py----------------------------------------
A:spacy.lang.pl.lemmatizer.morphology->token.morph.to_dict()
A:spacy.lang.pl.lemmatizer.lookup_pos->univ_pos.lower()
A:spacy.lang.pl.lemmatizer.lookup_table->self.lookups.get_table('lemma_lookup_' + lookup_pos, {})
A:spacy.lang.pl.lemmatizer.string->string.lower().lower()
spacy.lang.pl.PolishLemmatizer(Lemmatizer)
spacy.lang.pl.lemmatizer.PolishLemmatizer(Lemmatizer)
spacy.lang.pl.lemmatizer.PolishLemmatizer.get_lookups_config(cls,mode:str)->Tuple[List[str], List[str]]
spacy.lang.pl.lemmatizer.PolishLemmatizer.lemmatize_adj(self,string:str,morphology:dict,lookup_table:Dict[str,str])->List[str]
spacy.lang.pl.lemmatizer.PolishLemmatizer.lemmatize_noun(self,string:str,morphology:dict,lookup_table:Dict[str,str])->List[str]
spacy.lang.pl.lemmatizer.PolishLemmatizer.lemmatize_verb(self,string:str,morphology:dict,lookup_table:Dict[str,str])->List[str]
spacy.lang.pl.lemmatizer.PolishLemmatizer.pos_lookup_lemmatize(self,token:Token)->List[str]


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/pl/stop_words.py----------------------------------------
A:spacy.lang.pl.stop_words.STOP_WORDS->set('\na aby ach acz aczkolwiek aj albo ale alez\naleÅ¼ ani az aÅ¼\n\nbardziej bardzo beda bede bedzie bez bo bowiem by\nbyc byl byla byli bylo byly bym bynajmniej byÄ‡ byÅ‚\nbyÅ‚a byÅ‚o byÅ‚y bÄ™dzie bÄ™dÄ… bÄ™dÄ™\n\ncala cali caly caÅ‚a caÅ‚y chce choÄ‡ ci cie\nciebie ciÄ™ co cokolwiek coraz cos coÅ› czasami czasem czemu\nczy czyli czÄ™sto\n\ndaleko dla dlaczego dlatego do dobrze dokad dokÄ…d\ndosc doÅ›Ä‡ duzo duÅ¼o dwa dwaj dwie dwoje dzis\ndzisiaj dziÅ›\n\ngdy gdyby gdyz gdyÅ¼ gdzie gdziekolwiek gdzies gdzieÅ› go\ngodz\n\ni ich ile im inna inne inny\ninnych iv ix iz iÅ¼\n\nja jak jakas jakaÅ› jakby jaki jakichs jakichÅ› jakie\njakis jakiz jakiÅ› jakiÅ¼ jakkolwiek jako jakos jakoÅ› je jeden\njedna jednak jednakze jednakÅ¼e jedno jednym jedynie jego jej jemu\njesli jest jestem jeszcze jezeli jeÅ›li jeÅ¼eli juz juÅ¼ jÄ…\n\nkazdy kaÅ¼dy kiedy kierunku kilka kilku kims kimÅ› kto\nktokolwiek ktora ktore ktorego ktorej ktory ktorych ktorym ktorzy ktos\nktoÅ› ktÃ³ra ktÃ³re ktÃ³rego ktÃ³rej ktÃ³ry ktÃ³rych ktÃ³rym ktÃ³rzy ku\n\nlecz lub\n\nma majÄ… mam mamy maÅ‚o mi miaÅ‚ miedzy\nmimo miÄ™dzy mna mnie mnÄ… moga mogÄ… moi moim moj\nmoja moje moze mozliwe mozna moÅ¼e moÅ¼liwe moÅ¼na mu musi\nmy mÃ³j\n\nna nad nam nami nas nasi nasz nasza nasze\nnaszego naszych natomiast natychmiast nawet nia nic nich nie niech\nniego niej niemu nigdy nim nimi niz niÄ… niÅ¼ no\n\no obok od ok okoÅ‚o on ona one\noni ono oraz oto owszem\n\npan pana pani po pod podczas pomimo ponad\nponiewaz poniewaÅ¼ powinien powinna powinni powinno poza prawie przeciez\nprzecieÅ¼ przed przede przedtem przez przy\n\nraz razie roku rowniez rÃ³wnieÅ¼\n\nsam sama sie siÄ™ skad skÄ…d soba sobie sobÄ…\nsposob sposÃ³b swoje sÄ…\n\nta tak taka taki takich takie takze takÅ¼e tam\nte tego tej tel temu ten teraz teÅ¼ to toba\ntobie tobÄ… totez toteÅ¼ totobÄ… trzeba tu tutaj twoi twoim\ntwoj twoja twoje twym twÃ³j ty tych tylko tym tys\ntzw tÄ™\n\nu\n\nvi vii viii\n\nw wam wami was wasi wasz wasza wasze we\nwedÅ‚ug wie wiele wielu wiÄ™c wiÄ™cej wlasnie wszyscy wszystkich wszystkie\nwszystkim wszystko wtedy wy wÅ‚aÅ›nie wÅ›rÃ³d\n\nxi xii xiii xiv xv\n\nz za zaden zadna zadne zadnych zapewne zawsze zaÅ›\nze zeby znow znowu znÃ³w zostal zostaÅ‚\n\nÅ¼aden Å¼adna Å¼adne Å¼adnych Å¼e Å¼eby'.split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/pl/__init__.py----------------------------------------
spacy.lang.pl.__init__.Polish(Language)
spacy.lang.pl.__init__.PolishDefaults(BaseDefaults)
spacy.lang.pl.__init__.make_lemmatizer(nlp:Language,model:Optional[Model],name:str,mode:str,overwrite:bool,scorer:Optional[Callable])


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/pl/lex_attrs.py----------------------------------------
A:spacy.lang.pl.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.pl.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.pl.lex_attrs.like_num(text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/pl/punctuation.py----------------------------------------
A:spacy.lang.pl.punctuation._quotes->char_classes.CONCAT_QUOTES.replace("'", '')


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/az/examples.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/az/stop_words.py----------------------------------------
A:spacy.lang.az.stop_words.STOP_WORDS->set('\namma\narasÄ±nda\nartÄ±q\nay\naz\nbax\nbelÉ™\nbeÅŸ\nbilÉ™r\nbir\nbiraz\nbiri\nbirÅŸey\nbiz\nbizim\nbizlÉ™r\nbu\nbuna\nbundan\nbunlarÄ±n\nbunu\nbunun\nburadan\nbÃ¼tÃ¼n\nbÉ™li\nbÉ™lkÉ™\nbÉ™y\nbÉ™zi\nbÉ™zÉ™n\ndaha\ndedi\ndeyil\ndir\ndÃ¼z\ndÉ™\ndÉ™k\ndÉ™n\ndÉ™qiqÉ™\nedir\nedÉ™n\nelÉ™\net\netdi\netmÉ™\netmÉ™k\nfaiz\ngilÉ™\ngÃ¶rÉ™\nha\nhaqqÄ±nda\nharada\nheÃ§\nhÉ™\nhÉ™m\nhÉ™min\nhÉ™miÅŸÉ™\nhÉ™r\nidi\nil\nildÉ™\nilk\nilÉ™\nin\nindi\nistifadÉ™\nisÉ™\nki\nkim\nkimi\nkimÉ™\nlakin\nlap\nmirÅŸey\nmÉ™hz\nmÉ™n\nmÉ™nÉ™\nniyÉ™\nnÉ™\nnÉ™hayÉ™t\no\nobirisi\nof\nolan\nolar\nolaraq\noldu\nolduÄŸu\nolmadÄ±\nolmaz\nolmuÅŸdur\nolsun\nolur\non\nona\nondan\nonlar\nonlardan\nonlarÄ±n\nonsuzda\nonu\nonun\noradan\nqarÅŸÄ±\nqÉ™dÉ™r\nsaat\nsadÉ™cÉ™\nsaniyÉ™\nsiz\nsizin\nsizlÉ™r\nsonra\nsÉ™hv\nsÉ™n\nsÉ™nin\nsÉ™nÉ™\ntÉ™É™ssÃ¼f\nvar\nvÉ™\nxan\nxanÄ±m\nxeyr\nya\nyalnÄ±z\nyaxÅŸÄ±\nyeddi\nyenÉ™\nyox\nyoxdur\nyoxsa\nyÉ™ni\nzaman\nÃ§ox\nÃ§Ã¼nki\nÃ¶z\nÃ¶zÃ¼\nÃ¼Ã§Ã¼n\nÉ™gÉ™r\nÉ™lbÉ™ttÉ™\nÉ™n\nÉ™slindÉ™\n'.split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/az/__init__.py----------------------------------------
spacy.lang.az.__init__.Azerbaijani(Language)
spacy.lang.az.__init__.AzerbaijaniDefaults(BaseDefaults)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/az/lex_attrs.py----------------------------------------
A:spacy.lang.az.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.az.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
A:spacy.lang.az.lex_attrs.text_lower->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').lower()
spacy.lang.az.lex_attrs.like_num(text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/hi/examples.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/hi/stop_words.py----------------------------------------
A:spacy.lang.hi.stop_words.STOP_WORDS->set('\nà¤…à¤‚à¤¦à¤°\nà¤…à¤¤\nà¤…à¤¦à¤¿\nà¤…à¤ª\nà¤…à¤ªà¤¨à¤¾\nà¤…à¤ªà¤¨à¤¿\nà¤…à¤ªà¤¨à¥€\nà¤…à¤ªà¤¨à¥‡\nà¤…à¤­à¤¿\nà¤…à¤­à¥€\nà¤…à¤‚à¤¦à¤°\nà¤†à¤¦à¤¿\nà¤†à¤ª\nà¤…à¤—à¤°\nà¤‡à¤‚à¤¹à¤¿à¤‚\nà¤‡à¤‚à¤¹à¥‡à¤‚\nà¤‡à¤‚à¤¹à¥‹à¤‚\nà¤‡à¤¤à¤¯à¤¾à¤¦à¤¿\nà¤‡à¤¤à¥à¤¯à¤¾à¤¦à¤¿\nà¤‡à¤¨\nà¤‡à¤¨à¤•à¤¾\nà¤‡à¤¨à¥à¤¹à¥€à¤‚\nà¤‡à¤¨à¥à¤¹à¥‡à¤‚\nà¤‡à¤¨à¥à¤¹à¥‹à¤‚\nà¤‡à¤¸\nà¤‡à¤¸à¤•à¤¾\nà¤‡à¤¸à¤•à¤¿\nà¤‡à¤¸à¤•à¥€\nà¤‡à¤¸à¤•à¥‡\nà¤‡à¤¸à¤®à¥‡à¤‚\nà¤‡à¤¸à¤¿\nà¤‡à¤¸à¥€\nà¤‡à¤¸à¥‡\nà¤‰à¤‚à¤¹à¤¿à¤‚\nà¤‰à¤‚à¤¹à¥‡à¤‚\nà¤‰à¤‚à¤¹à¥‹à¤‚\nà¤‰à¤¨\nà¤‰à¤¨à¤•à¤¾\nà¤‰à¤¨à¤•à¤¿\nà¤‰à¤¨à¤•à¥€\nà¤‰à¤¨à¤•à¥‡\nà¤‰à¤¨à¤•à¥‹\nà¤‰à¤¨à¥à¤¹à¥€à¤‚\nà¤‰à¤¨à¥à¤¹à¥‡à¤‚\nà¤‰à¤¨à¥à¤¹à¥‹à¤‚\nà¤‰à¤¸\nà¤‰à¤¸à¤•à¥‡\nà¤‰à¤¸à¤¿\nà¤‰à¤¸à¥€\nà¤‰à¤¸à¥‡\nà¤à¤•\nà¤à¤µà¤‚\nà¤à¤¸\nà¤à¤¸à¥‡\nà¤à¤¸à¥‡\nà¤“à¤°\nà¤”à¤°\nà¤•à¤‡\nà¤•à¤ˆ\nà¤•à¤°\nà¤•à¤°à¤¤à¤¾\nà¤•à¤°à¤¤à¥‡\nà¤•à¤°à¤¨à¤¾\nà¤•à¤°à¤¨à¥‡\nà¤•à¤°à¥‡à¤‚\nà¤•à¤¹à¤¤à¥‡\nà¤•à¤¹à¤¾\nà¤•à¤¾\nà¤•à¤¾à¤«à¤¿\nà¤•à¤¾à¥žà¥€\nà¤•à¤¿\nà¤•à¤¿à¤‚à¤¹à¥‡à¤‚\nà¤•à¤¿à¤‚à¤¹à¥‹à¤‚\nà¤•à¤¿à¤¤à¤¨à¤¾\nà¤•à¤¿à¤¨à¥à¤¹à¥‡à¤‚\nà¤•à¤¿à¤¨à¥à¤¹à¥‹à¤‚\nà¤•à¤¿à¤¯à¤¾\nà¤•à¤¿à¤°\nà¤•à¤¿à¤¸\nà¤•à¤¿à¤¸à¤¿\nà¤•à¤¿à¤¸à¥€\nà¤•à¤¿à¤¸à¥‡\nà¤•à¥€\nà¤•à¥à¤›\nà¤•à¥à¤²\nà¤•à¥‡\nà¤•à¥‹\nà¤•à¥‹à¤‡\nà¤•à¥‹à¤ˆ\nà¤•à¥‹à¤¨\nà¤•à¥‹à¤¨à¤¸à¤¾\nà¤•à¥Œà¤¨\nà¤•à¥Œà¤¨à¤¸à¤¾\nà¤—à¤¯à¤¾\nà¤˜à¤°\nà¤œà¤¬\nà¤œà¤¹à¤¾à¤\nà¤œà¤¹à¤¾à¤‚\nà¤œà¤¾\nà¤œà¤¿à¤‚à¤¹à¥‡à¤‚\nà¤œà¤¿à¤‚à¤¹à¥‹à¤‚\nà¤œà¤¿à¤¤à¤¨à¤¾\nà¤œà¤¿à¤§à¤°\nà¤œà¤¿à¤¨\nà¤œà¤¿à¤¨à¥à¤¹à¥‡à¤‚\nà¤œà¤¿à¤¨à¥à¤¹à¥‹à¤‚\nà¤œà¤¿à¤¸\nà¤œà¤¿à¤¸à¥‡\nà¤œà¥€à¤§à¤°\nà¤œà¥‡à¤¸à¤¾\nà¤œà¥‡à¤¸à¥‡\nà¤œà¥ˆà¤¸à¤¾\nà¤œà¥ˆà¤¸à¥‡\nà¤œà¥‹\nà¤¤à¤•\nà¤¤à¤¬\nà¤¤à¤°à¤¹\nà¤¤à¤¿à¤‚à¤¹à¥‡à¤‚\nà¤¤à¤¿à¤‚à¤¹à¥‹à¤‚\nà¤¤à¤¿à¤¨\nà¤¤à¤¿à¤¨à¥à¤¹à¥‡à¤‚\nà¤¤à¤¿à¤¨à¥à¤¹à¥‹à¤‚\nà¤¤à¤¿à¤¸\nà¤¤à¤¿à¤¸à¥‡\nà¤¤à¥‹\nà¤¥à¤¾\nà¤¥à¤¿\nà¤¥à¥€\nà¤¥à¥‡\nà¤¦à¤¬à¤¾à¤°à¤¾\nà¤¦à¤µà¤¾à¤°à¤¾\nà¤¦à¤¿à¤¯à¤¾\nà¤¦à¥à¤¸à¤°à¤¾\nà¤¦à¥à¤¸à¤°à¥‡\nà¤¦à¥‚à¤¸à¤°à¥‡\nà¤¦à¥‹\nà¤¦à¥à¤µà¤¾à¤°à¤¾\nà¤¨\nà¤¨à¤¹à¤¿à¤‚\nà¤¨à¤¹à¥€à¤‚\nà¤¨à¤¾\nà¤¨à¤¿à¤šà¥‡\nà¤¨à¤¿à¤¹à¤¾à¤¯à¤¤\nà¤¨à¥€à¤šà¥‡\nà¤¨à¥‡\nà¤ªà¤°\nà¤ªà¤¹à¤²à¥‡\nà¤ªà¥à¤°à¤¾\nà¤ªà¥‚à¤°à¤¾\nà¤ªà¥‡\nà¤«à¤¿à¤°\nà¤¬à¤¨à¤¿\nà¤¬à¤¨à¥€\nà¤¬à¤¹à¤¿\nà¤¬à¤¹à¥€\nà¤¬à¤¹à¥à¤¤\nà¤¬à¤¾à¤¦\nà¤¬à¤¾à¤²à¤¾\nà¤¬à¤¿à¤²à¤•à¥à¤²\nà¤­à¤¿\nà¤­à¤¿à¤¤à¤°\nà¤­à¥€\nà¤­à¥€à¤¤à¤°\nà¤®à¤—à¤°\nà¤®à¤¾à¤¨à¥‹\nà¤®à¥‡\nà¤®à¥‡à¤‚\nà¤®à¥ˆà¤‚\nà¤®à¥à¤à¤•à¥‹\nà¤®à¥‡à¤°à¤¾\nà¤¯à¤¦à¤¿\nà¤¯à¤¹\nà¤¯à¤¹à¤¾à¤\nà¤¯à¤¹à¤¾à¤‚\nà¤¯à¤¹à¤¿\nà¤¯à¤¹à¥€\nà¤¯à¤¾\nà¤¯à¤¿à¤¹\nà¤¯à¥‡\nà¤°à¤–à¥‡à¤‚\nà¤°à¤µà¤¾à¤¸à¤¾\nà¤°à¤¹à¤¾\nà¤°à¤¹à¥‡\nà¤±à¥à¤µà¤¾à¤¸à¤¾\nà¤²à¤¿à¤\nà¤²à¤¿à¤¯à¥‡\nà¤²à¥‡à¤•à¤¿à¤¨\nà¤µ\nà¤µà¤—à¥‡à¤°à¤¹\nà¤µà¥šà¥ˆà¤°à¤¹\nà¤µà¤°à¤—\nà¤µà¤°à¥à¤—\nà¤µà¤¹\nà¤µà¤¹à¤¾à¤\nà¤µà¤¹à¤¾à¤‚\nà¤µà¤¹à¤¿à¤‚\nà¤µà¤¹à¥€à¤‚\nà¤µà¤¾à¤²à¥‡\nà¤µà¥à¤¹\nà¤µà¥‡\nà¤µà¥šà¥ˆà¤°à¤¹\nà¤¸à¤‚à¤—\nà¤¸à¤•à¤¤à¤¾\nà¤¸à¤•à¤¤à¥‡\nà¤¸à¤¬à¤¸à¥‡\nà¤¸à¤­à¤¿\nà¤¸à¤­à¥€\nà¤¸à¤¾à¤¥\nà¤¸à¤¾à¤¬à¥à¤¤\nà¤¸à¤¾à¤­\nà¤¸à¤¾à¤°à¤¾\nà¤¸à¥‡\nà¤¸à¥‹\nà¤¸à¤‚à¤—\nà¤¹à¤¿\nà¤¹à¥€\nà¤¹à¥à¤…\nà¤¹à¥à¤†\nà¤¹à¥à¤‡\nà¤¹à¥à¤ˆ\nà¤¹à¥à¤\nà¤¹à¥‡\nà¤¹à¥‡à¤‚\nà¤¹à¥ˆ\nà¤¹à¥ˆà¤‚\nà¤¹à¥‹\nà¤¹à¥‚à¤\nà¤¹à¥‹à¤¤à¤¾\nà¤¹à¥‹à¤¤à¤¿\nà¤¹à¥‹à¤¤à¥€\nà¤¹à¥‹à¤¤à¥‡\nà¤¹à¥‹à¤¨à¤¾\nà¤¹à¥‹à¤¨à¥‡\n'.split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/hi/__init__.py----------------------------------------
spacy.lang.hi.__init__.Hindi(Language)
spacy.lang.hi.__init__.HindiDefaults(BaseDefaults)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/hi/lex_attrs.py----------------------------------------
A:spacy.lang.hi.lex_attrs.length->len(suffix_group[0])
A:spacy.lang.hi.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.hi.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.hi.lex_attrs.like_num(text)
spacy.lang.hi.lex_attrs.norm(string)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/sv/examples.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/sv/stop_words.py----------------------------------------
A:spacy.lang.sv.stop_words.STOP_WORDS->set('\naderton adertonde adjÃ¶ aldrig alla allas allt alltid alltsÃ¥ Ã¤n andra andras\nannan annat Ã¤nnu artonde arton Ã¥tminstone att Ã¥tta Ã¥ttio Ã¥ttionde Ã¥ttonde av\nÃ¤ven\n\nbÃ¥da bÃ¥das bakom bara bÃ¤st bÃ¤ttre behÃ¶va behÃ¶vas behÃ¶vde behÃ¶vt beslut beslutat\nbeslutit bland blev bli blir blivit bort borta bra\n\ndÃ¥ dag dagar dagarna dagen dÃ¤r dÃ¤rfÃ¶r de del delen dem den deras dess det detta\ndig din dina dit ditt dock du\n\nefter eftersom elfte eller elva en enkel enkelt enkla enligt er era ert ett\nettusen\n\nfÃ¥ fanns fÃ¥r fÃ¥tt fem femte femtio femtionde femton femtonde fick fin finnas\nfinns fjÃ¤rde fjorton fjortonde fler flera flesta fÃ¶ljande fÃ¶r fÃ¶re fÃ¶rlÃ¥t fÃ¶rra\nfÃ¶rsta fram framfÃ¶r frÃ¥n fyra fyrtio fyrtionde\n\ngÃ¥ gÃ¤lla gÃ¤ller gÃ¤llt gÃ¥r gÃ¤rna gÃ¥tt genast genom gick gjorde gjort god goda\ngodare godast gÃ¶r gÃ¶ra gott\n\nha hade haft han hans har hÃ¤r heller hellre helst helt henne hennes hit hÃ¶g\nhÃ¶ger hÃ¶gre hÃ¶gst hon honom hundra hundraen hundraett hur\n\ni ibland idag igÃ¥r igen imorgon in infÃ¶r inga ingen ingenting inget innan inne\ninom inte inuti\n\nja jag jÃ¤mfÃ¶rt\n\nkan kanske knappast kom komma kommer kommit kr kunde kunna kunnat kvar\n\nlÃ¤nge lÃ¤ngre lÃ¥ngsam lÃ¥ngsammare lÃ¥ngsammast lÃ¥ngsamt lÃ¤ngst lÃ¥ngt lÃ¤tt lÃ¤ttare\nlÃ¤ttast legat ligga ligger lika likstÃ¤lld likstÃ¤llda lilla lite liten litet\n\nman mÃ¥nga mÃ¥ste med mellan men mer mera mest mig min mina mindre minst mitt\nmittemot mÃ¶jlig mÃ¶jligen mÃ¶jligt mÃ¶jligtvis mot mycket\n\nnÃ¥gon nÃ¥gonting nÃ¥got nÃ¥gra nÃ¤r nÃ¤sta ned nederst nedersta nedre nej ner ni nio\nnionde nittio nittionde nitton nittonde nÃ¶dvÃ¤ndig nÃ¶dvÃ¤ndiga nÃ¶dvÃ¤ndigt\nnÃ¶dvÃ¤ndigtvis nog noll nr nu nummer\n\noch ocksÃ¥ ofta oftast olika olikt om oss\n\nÃ¶ver Ã¶vermorgon Ã¶verst Ã¶vre\n\npÃ¥\n\nrakt rÃ¤tt redan\n\nsÃ¥ sade sÃ¤ga sÃ¤ger sagt samma sÃ¤mre sÃ¤mst sedan senare senast sent sex sextio\nsextionde sexton sextonde sig sin sina sist sista siste sitt sjÃ¤tte sju sjunde\nsjuttio sjuttionde sjutton sjuttonde ska skall skulle slutligen smÃ¥ smÃ¥tt snart\nsom stor stora stÃ¶rre stÃ¶rst stort\n\ntack tidig tidigare tidigast tidigt till tills tillsammans tio tionde tjugo\ntjugoen tjugoett tjugonde tjugotre tjugotvÃ¥ tjungo tolfte tolv tre tredje\ntrettio trettionde tretton trettonde tvÃ¥ tvÃ¥hundra\n\nunder upp ur ursÃ¤kt ut utan utanfÃ¶r ute\n\nvad vÃ¤nster vÃ¤nstra var vÃ¥r vara vÃ¥ra varfÃ¶r varifrÃ¥n varit varken vÃ¤rre\nvarsÃ¥god vart vÃ¥rt vem vems verkligen vi vid vidare viktig viktigare viktigast\nviktigt vilka vilken vilket vill\n'.split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/sv/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.sv.tokenizer_exceptions.verb_data_tc->dict(verb_data)
A:spacy.lang.sv.tokenizer_exceptions.verb_data_tc[ORTH]->verb_data_tc[ORTH].title().title()
A:spacy.lang.sv.tokenizer_exceptions.capitalized->orth.capitalize()
A:spacy.lang.sv.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/sv/__init__.py----------------------------------------
spacy.lang.sv.__init__.Swedish(Language)
spacy.lang.sv.__init__.SwedishDefaults(BaseDefaults)
spacy.lang.sv.__init__.make_lemmatizer(nlp:Language,model:Optional[Model],name:str,mode:str,overwrite:bool,scorer:Optional[Callable])


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/sv/lex_attrs.py----------------------------------------
A:spacy.lang.sv.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.sv.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.sv.lex_attrs.like_num(text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/sv/syntax_iterators.py----------------------------------------
A:spacy.lang.sv.syntax_iterators.conj->doc.vocab.strings.add('conj')
A:spacy.lang.sv.syntax_iterators.np_label->doc.vocab.strings.add('NP')
spacy.lang.sv.syntax_iterators.noun_chunks(doclike:Union[Doc,Span])->Iterator[Tuple[int, int, int]]


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ga/lemmatizer.py----------------------------------------
A:spacy.lang.ga.lemmatizer.string->unponc(token.text)
A:spacy.lang.ga.lemmatizer.demutated->demutate(string)
A:spacy.lang.ga.lemmatizer.lookup_pos->univ_pos.lower()
A:spacy.lang.ga.lemmatizer.lookup_table->self.lookups.get_table('lemma_lookup_' + lookup_pos, {})
A:spacy.lang.ga.lemmatizer.lc->word.lower()
spacy.lang.ga.IrishLemmatizer(Lemmatizer)
spacy.lang.ga.lemmatizer.IrishLemmatizer(Lemmatizer)
spacy.lang.ga.lemmatizer.IrishLemmatizer.get_lookups_config(cls,mode:str)->Tuple[List[str], List[str]]
spacy.lang.ga.lemmatizer.IrishLemmatizer.pos_lookup_lemmatize(self,token:Token)->List[str]
spacy.lang.ga.lemmatizer.demutate(word:str,is_hpref:bool=False)->str
spacy.lang.ga.lemmatizer.unponc(word:str)->str


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ga/stop_words.py----------------------------------------
A:spacy.lang.ga.stop_words.STOP_WORDS->set('\na ach ag agus an aon ar arna as\n\nba beirt bhÃºr\n\ncaoga ceathair ceathrar chomh chuig chun cois cÃ©ad cÃºig cÃºigear\n\ndaichead dar de deich deichniÃºr den dhÃ¡ do don dtÃ­ dÃ¡ dÃ¡r dÃ³\n\nfaoi faoin faoina faoinÃ¡r fara fiche\n\ngach gan go gur\n\nhaon hocht\n\ni iad idir in ina ins inÃ¡r is\n\nle leis lena lenÃ¡r\n\nmar mo muid mÃ©\n\nna nach naoi naonÃºr nÃ¡ nÃ­ nÃ­or nÃ³ nÃ³cha\n\nocht ochtar ochtÃ³ os\n\nroimh\n\nsa seacht seachtar seachtÃ³ seasca seisear siad sibh sinn sna sÃ© sÃ­\n\ntar thar thÃº triÃºr trÃ­ trÃ­na trÃ­nÃ¡r trÃ­ocha tÃº\n\num\n\nÃ¡r\n\nÃ© Ã©is\n\nÃ­\n\nÃ³ Ã³n Ã³na Ã³nÃ¡r\n'.split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ga/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.ga.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/ga/__init__.py----------------------------------------
spacy.lang.ga.__init__.Irish(Language)
spacy.lang.ga.__init__.IrishDefaults(BaseDefaults)
spacy.lang.ga.__init__.make_lemmatizer(nlp:Language,model:Optional[Model],name:str,mode:str,overwrite:bool)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/kn/examples.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/kn/stop_words.py----------------------------------------
A:spacy.lang.kn.stop_words.STOP_WORDS->set('\nà²¹à²²à²µà³\nà²®à³‚à²²à²•\nà²¹à²¾à²—à³‚\nà²…à²¦à³\nà²¨à³€à²¡à²¿à²¦à³à²¦à²¾à²°à³†\nà²¯à²¾à²µ\nà²Žà²‚à²¦à²°à³\nà²…à²µà²°à³\nà²ˆà²—\nà²Žà²‚à²¬\nà²¹à²¾à²—à²¾à²—à²¿\nà²…à²·à³à²Ÿà³‡\nà²¨à²¾à²µà³\nà²‡à²¦à³‡\nà²¹à³‡à²³à²¿\nà²¤à²®à³à²®\nà²¹à³€à²—à³†\nà²¨à²®à³à²®\nà²¬à³‡à²°à³†\nà²¨à³€à²¡à²¿à²¦à²°à³\nà²®à²¤à³à²¤à³†\nà²‡à²¦à³\nà²ˆ\nà²¨à³€à²µà³\nà²¨à²¾à²¨à³\nà²‡à²¤à³à²¤à³\nà²Žà²²à³à²²à²¾\nà²¯à²¾à²µà³à²¦à³‡\nà²¨à²¡à³†à²¦\nà²…à²¦à²¨à³à²¨à³\nà²Žà²‚à²¦à²°à³†\nà²¨à³€à²¡à²¿à²¦à³†\nà²¹à³€à²—à²¾à²—à²¿\nà²œà³†à³‚à²¤à³†à²—à³†\nà²‡à²¦à²°à²¿à²‚à²¦\nà²¨à²¨à²—à³†\nà²…à²²à³à²²à²¦à³†\nà²Žà²·à³à²Ÿà³\nà²‡à²¦à²°\nà²‡à²²à³à²²\nà²•à²³à³†à²¦\nà²¤à³à²‚à²¬à²¾\nà²ˆà²—à²¾à²—à²²à³‡\nà²®à²¾à²¡à²¿\nà²…à²¦à²•à³à²•à³†\nà²¬à²—à³à²—à³†\nà²…à²µà²°\nà²‡à²¦à²¨à³à²¨à³\nà²†\nà²‡à²¦à³†\nà²¹à³†à²šà³à²šà³\nà²‡à²¨à³à²¨à³\nà²Žà²²à³à²²\nà²‡à²°à³à²µ\nà²…à²µà²°à²¿à²—à³†\nà²¨à²¿à²®à³à²®\nà²à²¨à³\nà²•à³‚à²¡\nà²‡à²²à³à²²à²¿\nà²¨à²¨à³à²¨à²¨à³à²¨à³\nà²•à³†à²²à²µà³\nà²®à²¾à²¤à³à²°\nà²¬à²³à²¿à²•\nà²…à²‚à²¤\nà²¤à²¨à³à²¨\nà²†à²—\nà²…à²¥à²µà²¾\nà²…à²²à³à²²\nà²•à³‡à²µà²²\nà²†à²¦à²°à³†\nà²®à²¤à³à²¤à³\nà²‡à²¨à³à²¨à³‚\nà²…à²¦à³‡\nà²†à²—à²¿\nà²…à²µà²°à²¨à³à²¨à³\nà²¹à³‡à²³à²¿à²¦à³à²¦à²¾à²°à³†\nà²¨à²¡à³†à²¦à²¿à²¦à³†\nà²‡à²¦à²•à³à²•à³†\nà²Žà²‚à²¬à³à²¦à³\nà²Žà²‚à²¦à³\nà²¨à²¨à³à²¨\nà²®à³‡à²²à³†\n'.split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/kn/__init__.py----------------------------------------
spacy.lang.kn.__init__.Kannada(Language)
spacy.lang.kn.__init__.KannadaDefaults(BaseDefaults)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/af/stop_words.py----------------------------------------
A:spacy.lang.af.stop_words.STOP_WORDS->set("\n'n\naan\naf\nal\nas\nbaie\nby\ndaar\ndag\ndat\ndie\ndit\neen\nek\nen\ngaan\ngesÃª\nhaar\nhet\nhom\nhulle\nhy\nin\nis\njou\njy\nkan\nkom\nma\nmaar\nmet\nmy\nna\nnie\nom\nons\nop\nsaam\nsal\nse\nsien\nso\nsy\nte\ntoe\nuit\nvan\nvir\nwas\nwat\nÅ‰\n".split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/af/__init__.py----------------------------------------
spacy.lang.af.__init__.Afrikaans(Language)
spacy.lang.af.__init__.AfrikaansDefaults(BaseDefaults)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/lt/examples.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/lt/stop_words.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/lt/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.lt.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(mod_base_exceptions, _exc)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/lt/__init__.py----------------------------------------
spacy.lang.lt.__init__.Lithuanian(Language)
spacy.lang.lt.__init__.LithuanianDefaults(BaseDefaults)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/lt/lex_attrs.py----------------------------------------
A:spacy.lang.lt.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.lt.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.lt.lex_attrs.like_num(text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/lt/punctuation.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/sq/examples.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/sq/stop_words.py----------------------------------------
A:spacy.lang.sq.stop_words.STOP_WORDS->set('\na\nafert\nai\najo\nandej\nanes\naq\nas\nasaj\nashtu\nata\nate\natij\natje\nato\naty\natyre\nb\nbe\nbehem\nbehet\nbej\nbeje\nbejne\nben\nbene\nbere\nberi\nbie\nc\nca\ncdo\ncfare\ncila\ncilat\ncilave\ncilen\nciles\ncilet\ncili\ncilin\ncilit\nderi\ndhe\ndic\ndicka\ndickaje\ndike\ndikujt\ndikush\ndisa\ndo\ndot\ndrejt\nduke\ndy\ne\nedhe\nende\neshte\netj\nfare\ngjate\ngje\ngjitha\ngjithcka\ngjithe\ngjithnje\nhere\ni\nia\nishin\nishte\niu\nja\njam\njane\njap\nje\njemi\njo\nju\nk\nka\nkam\nkane\nkem\nkemi\nkeq\nkesaj\nkeshtu\nkete\nketej\nketij\nketo\nketu\nketyre\nkishin\nkishte\nkjo\nkrejt\nkryer\nkryesisht\nkryhet\nku\nkudo\nkundrejt\nkur\nkurre\nkush\nky\nla\nle\nlloj\nm\nma\nmadhe\nmarr\nmarre\nmban\nmbi\nme\nmenjehere\nmerr\nmerret\nmes\nmi\nmidis\nmire\nmjaft\nmori\nmos\nmua\nmund\nna\nndaj\nnder\nndermjet\nndersa\nndonje\nndryshe\nne\nnen\nneper\nnepermjet\nnese\nnga\nnje\nnjera\nnuk\nose\npa\npak\npapritur\npara\npas\npasi\npasur\nper\nperbashket\nperpara\npo\npor\nprane\nprapa\nprej\npse\nqe\nqene\nqenet\nrralle\nrreth\nrri\ns\nsa\nsaj\nsapo\nse\nsecila\nsepse\nsh\nshih\nshume\nsi\nsic\nsikur\nsipas\nsiper\nsone\nt\nta\ntani\nte\ntej\ntek\nteper\ntere\nti\ntij\ntilla\ntille\ntjera\ntjeret\ntjeter\ntjetren\nto\ntone\nty\ntyre\nu\nua\nune\nvazhdimisht\nvend\nvet\nveta\nvete\nvetem\nveten\nvetes\nvjen\nyne\nzakonisht\n'.split())


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/sq/__init__.py----------------------------------------
spacy.lang.sq.__init__.Albanian(Language)
spacy.lang.sq.__init__.AlbanianDefaults(BaseDefaults)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/vi/examples.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/vi/stop_words.py----------------------------------------
A:spacy.lang.vi.stop_words.STOP_WORDS->set('\na_lÃ´\na_ha\nai\nai_ai\nai_náº¥y\nai_Ä‘Ã³\nalÃ´\namen\nanh\nanh_áº¥y\nba\nba_bau\nba_báº£n\nba_cÃ¹ng\nba_há»\nba_ngÃ y\nba_ngÃ´i\nba_tÄƒng\nbao_giá»\nbao_lÃ¢u\nbao_nhiÃªu\nbao_náº£\nbay_biáº¿n\nbiáº¿t\nbiáº¿t_bao\nbiáº¿t_bao_nhiÃªu\nbiáº¿t_cháº¯c\nbiáº¿t_chá»«ng_nÃ o\nbiáº¿t_mÃ¬nh\nbiáº¿t_máº¥y\nbiáº¿t_tháº¿\nbiáº¿t_trÆ°á»›c\nbiáº¿t_viá»‡c\nbiáº¿t_Ä‘Ã¢u\nbiáº¿t_Ä‘Ã¢u_chá»«ng\nbiáº¿t_Ä‘Ã¢u_Ä‘áº¥y\nbiáº¿t_Ä‘Æ°á»£c\nbuá»•i\nbuá»•i_lÃ m\nbuá»•i_má»›i\nbuá»•i_ngÃ y\nbuá»•i_sá»›m\nbÃ \nbÃ _áº¥y\nbÃ i\nbÃ i_bÃ¡c\nbÃ i_bá»\nbÃ i_cÃ¡i\nbÃ¡c\nbÃ¡n\nbÃ¡n_cáº¥p\nbÃ¡n_dáº¡\nbÃ¡n_tháº¿\nbÃ¢y_báº©y\nbÃ¢y_chá»«\nbÃ¢y_giá»\nbÃ¢y_nhiÃªu\nbÃ¨n\nbÃ©ng\nbÃªn\nbÃªn_bá»‹\nbÃªn_cÃ³\nbÃªn_cáº¡nh\nbÃ´ng\nbÆ°á»›c\nbÆ°á»›c_khá»i\nbÆ°á»›c_tá»›i\nbÆ°á»›c_Ä‘i\nbáº¡n\nbáº£n\nbáº£n_bá»™\nbáº£n_riÃªng\nbáº£n_thÃ¢n\nbáº£n_Ã½\nbáº¥t_chá»£t\nbáº¥t_cá»©\nbáº¥t_giÃ¡c\nbáº¥t_kÃ¬\nbáº¥t_ká»ƒ\nbáº¥t_ká»³\nbáº¥t_luáº­n\nbáº¥t_ngá»\nbáº¥t_nhÆ°á»£c\nbáº¥t_quÃ¡\nbáº¥t_quÃ¡_chá»‰\nbáº¥t_thÃ¬nh_lÃ¬nh\nbáº¥t_tá»­\nbáº¥t_Ä‘á»“\nbáº¥y\nbáº¥y_cháº§y\nbáº¥y_chá»«\nbáº¥y_giá»\nbáº¥y_lÃ¢u\nbáº¥y_lÃ¢u_nay\nbáº¥y_nay\nbáº¥y_nhiÃªu\nbáº­p_bÃ _báº­p_bÃµm\nbáº­p_bÃµm\nbáº¯t_Ä‘áº§u\nbáº¯t_Ä‘áº§u_tá»«\nbáº±ng\nbáº±ng_cá»©\nbáº±ng_khÃ´ng\nbáº±ng_ngÆ°á»i\nbáº±ng_nhau\nbáº±ng_nhÆ°\nbáº±ng_nÃ o\nbáº±ng_náº¥y\nbáº±ng_vÃ o\nbáº±ng_Ä‘Æ°á»£c\nbáº±ng_áº¥y\nbá»ƒn\nbá»‡t\nbá»‹\nbá»‹_chÃº\nbá»‹_vÃ¬\nbá»\nbá»_bÃ \nbá»_cha\nbá»_cuá»™c\nbá»_khÃ´ng\nbá»_láº¡i\nbá»_mÃ¬nh\nbá»_máº¥t\nbá»_máº¹\nbá»_nhá»\nbá»_quÃ¡\nbá»_ra\nbá»_riÃªng\nbá»_viá»‡c\nbá»_xa\nbá»—ng\nbá»—ng_chá»‘c\nbá»—ng_dÆ°ng\nbá»—ng_khÃ´ng\nbá»—ng_nhiÃªn\nbá»—ng_nhÆ°ng\nbá»—ng_tháº¥y\nbá»—ng_Ä‘Ã¢u\nbá»™\nbá»™_thuá»™c\nbá»™_Ä‘iá»u\nbá»™i_pháº§n\nbá»›\nbá»Ÿi\nbá»Ÿi_ai\nbá»Ÿi_chÆ°ng\nbá»Ÿi_nhÆ°ng\nbá»Ÿi_sao\nbá»Ÿi_tháº¿\nbá»Ÿi_tháº¿_cho_nÃªn\nbá»Ÿi_táº¡i\nbá»Ÿi_vÃ¬\nbá»Ÿi_váº­y\nbá»Ÿi_Ä‘Ã¢u\nbá»©c\ncao\ncao_lÃ¢u\ncao_rÃ¡o\ncao_rÄƒng\ncao_sang\ncao_sá»‘\ncao_tháº¥p\ncao_tháº¿\ncao_xa\ncha\ncha_cháº£\nchao_Ã´i\nchia_sáº»\nchiáº¿c\ncho\ncho_biáº¿t\ncho_cháº¯c\ncho_hay\ncho_nhau\ncho_nÃªn\ncho_ráº±ng\ncho_rá»“i\ncho_tháº¥y\ncho_tin\ncho_tá»›i\ncho_tá»›i_khi\ncho_vá»\ncho_Äƒn\ncho_Ä‘ang\ncho_Ä‘Æ°á»£c\ncho_Ä‘áº¿n\ncho_Ä‘áº¿n_khi\ncho_Ä‘áº¿n_ná»—i\nchoa\nchu_cha\nchui_cha\nchung\nchung_cho\nchung_chung\nchung_cuá»™c\nchung_cá»¥c\nchung_nhau\nchung_qui\nchung_quy\nchung_quy_láº¡i\nchung_Ã¡i\nchuyá»ƒn\nchuyá»ƒn_tá»±\nchuyá»ƒn_Ä‘áº¡t\nchuyá»‡n\nchuáº©n_bá»‹\nchÃ nh_cháº¡nh\nchÃ­_cháº¿t\nchÃ­nh\nchÃ­nh_báº£n\nchÃ­nh_giá»¯a\nchÃ­nh_lÃ \nchÃ­nh_thá»‹\nchÃ­nh_Ä‘iá»ƒm\nchÃ¹n_chÃ¹n\nchÃ¹n_chÅ©n\nchÃº\nchÃº_dáº«n\nchÃº_khÃ¡ch\nchÃº_mÃ y\nchÃº_mÃ¬nh\nchÃºng\nchÃºng_mÃ¬nh\nchÃºng_ta\nchÃºng_tÃ´i\nchÃºng_Ã´ng\nchÄƒn_cháº¯n\nchÄƒng\nchÄƒng_cháº¯c\nchÄƒng_ná»¯a\nchÆ¡i\nchÆ¡i_há»\nchÆ°a\nchÆ°a_bao_giá»\nchÆ°a_cháº¯c\nchÆ°a_cÃ³\nchÆ°a_cáº§n\nchÆ°a_dÃ¹ng\nchÆ°a_dá»…\nchÆ°a_ká»ƒ\nchÆ°a_tÃ­nh\nchÆ°a_tá»«ng\ncháº§m_cháº­p\ncháº­c\ncháº¯c\ncháº¯c_cháº¯n\ncháº¯c_dáº¡\ncháº¯c_háº³n\ncháº¯c_lÃ²ng\ncháº¯c_ngÆ°á»i\ncháº¯c_vÃ o\ncháº¯c_Äƒn\ncháº³ng_láº½\ncháº³ng_nhá»¯ng\ncháº³ng_ná»¯a\ncháº³ng_pháº£i\ncháº¿t_ná»—i\ncháº¿t_tháº­t\ncháº¿t_tiá»‡t\nchá»‰\nchá»‰_chÃ­nh\nchá»‰_cÃ³\nchá»‰_lÃ \nchá»‰_tÃªn\nchá»‰n\nchá»‹\nchá»‹_bá»™\nchá»‹_áº¥y\nchá»‹u\nchá»‹u_chÆ°a\nchá»‹u_lá»i\nchá»‹u_tá»‘t\nchá»‹u_Äƒn\nchá»n\nchá»n_bÃªn\nchá»n_ra\nchá»‘c_chá»‘c\nchá»›\nchá»›_chi\nchá»›_gÃ¬\nchá»›_khÃ´ng\nchá»›_ká»ƒ\nchá»›_nhÆ°\nchá»£t\nchá»£t_nghe\nchá»£t_nhÃ¬n\nchá»§n\nchá»©\nchá»©_ai\nchá»©_cÃ²n\nchá»©_gÃ¬\nchá»©_khÃ´ng\nchá»©_khÃ´ng_pháº£i\nchá»©_láº¡i\nchá»©_lá»‹\nchá»©_nhÆ°\nchá»©_sao\ncoi_bá»™\ncoi_mÃ²i\ncon\ncon_con\ncon_dáº¡\ncon_nhÃ \ncon_tÃ­nh\ncu_cáº­u\ncuá»‘i\ncuá»‘i_cÃ¹ng\ncuá»‘i_Ä‘iá»ƒm\ncuá»‘n\ncuá»™c\ncÃ ng\ncÃ ng_cÃ ng\ncÃ ng_hay\ncÃ¡_nhÃ¢n\ncÃ¡c\ncÃ¡c_cáº­u\ncÃ¡ch\ncÃ¡ch_bá»©c\ncÃ¡ch_khÃ´ng\ncÃ¡ch_nhau\ncÃ¡ch_Ä‘á»u\ncÃ¡i\ncÃ¡i_gÃ¬\ncÃ¡i_há»\ncÃ¡i_Ä‘Ã£\ncÃ¡i_Ä‘Ã³\ncÃ¡i_áº¥y\ncÃ¢u_há»i\ncÃ¢y\ncÃ¢y_nÆ°á»›c\ncÃ²n\ncÃ²n_nhÆ°\ncÃ²n_ná»¯a\ncÃ²n_thá»i_gian\ncÃ²n_vá»\ncÃ³\ncÃ³_ai\ncÃ³_chuyá»‡n\ncÃ³_chÄƒng\ncÃ³_chÄƒng_lÃ \ncÃ³_chá»©\ncÃ³_cÆ¡\ncÃ³_dá»…\ncÃ³_há»\ncÃ³_khi\ncÃ³_ngÃ y\ncÃ³_ngÆ°á»i\ncÃ³_nhiá»u\ncÃ³_nhÃ \ncÃ³_pháº£i\ncÃ³_sá»‘\ncÃ³_thÃ¡ng\ncÃ³_tháº¿\ncÃ³_thá»ƒ\ncÃ³_váº»\ncÃ³_Ã½\ncÃ³_Äƒn\ncÃ³_Ä‘iá»u\ncÃ³_Ä‘iá»u_kiá»‡n\ncÃ³_Ä‘Ã¡ng\ncÃ³_Ä‘Ã¢u\ncÃ³_Ä‘Æ°á»£c\ncÃ³c_khÃ´\ncÃ´\ncÃ´_mÃ¬nh\ncÃ´_quáº£\ncÃ´_tÄƒng\ncÃ´_áº¥y\ncÃ´ng_nhiÃªn\ncÃ¹ng\ncÃ¹ng_chung\ncÃ¹ng_cá»±c\ncÃ¹ng_nhau\ncÃ¹ng_tuá»•i\ncÃ¹ng_tá»™t\ncÃ¹ng_vá»›i\ncÃ¹ng_Äƒn\ncÄƒn\ncÄƒn_cÃ¡i\ncÄƒn_cáº¯t\ncÄƒn_tÃ­nh\ncÅ©ng\ncÅ©ng_nhÆ°\ncÅ©ng_nÃªn\ncÅ©ng_tháº¿\ncÅ©ng_váº­y\ncÅ©ng_váº­y_thÃ´i\ncÅ©ng_Ä‘Æ°á»£c\ncÆ¡\ncÆ¡_chá»‰\ncÆ¡_chá»«ng\ncÆ¡_cÃ¹ng\ncÆ¡_dáº«n\ncÆ¡_há»“\ncÆ¡_há»™i\ncÆ¡_mÃ \ncÆ¡n\ncáº£\ncáº£_nghe\ncáº£_nghÄ©\ncáº£_ngÃ y\ncáº£_ngÆ°á»i\ncáº£_nhÃ \ncáº£_nÄƒm\ncáº£_tháº£y\ncáº£_thá»ƒ\ncáº£_tin\ncáº£_Äƒn\ncáº£_Ä‘áº¿n\ncáº£m_tháº¥y\ncáº£m_Æ¡n\ncáº¥p\ncáº¥p_sá»‘\ncáº¥p_trá»±c_tiáº¿p\ncáº§n\ncáº§n_cáº¥p\ncáº§n_gÃ¬\ncáº§n_sá»‘\ncáº­t_lá»±c\ncáº­t_sá»©c\ncáº­u\ncá»•_lai\ncá»¥_thá»ƒ\ncá»¥_thá»ƒ_lÃ \ncá»¥_thá»ƒ_nhÆ°\ncá»§a\ncá»§a_ngá»t\ncá»§a_tin\ncá»©\ncá»©_nhÆ°\ncá»©_viá»‡c\ncá»©_Ä‘iá»ƒm\ncá»±c_lá»±c\ndo\ndo_vÃ¬\ndo_váº­y\ndo_Ä‘Ã³\nduy\nduy_chá»‰\nduy_cÃ³\ndÃ i\ndÃ i_lá»i\ndÃ i_ra\ndÃ nh\ndÃ nh_dÃ nh\ndÃ o\ndÃ¬\ndÃ¹\ndÃ¹_cho\ndÃ¹_dÃ¬\ndÃ¹_gÃ¬\ndÃ¹_ráº±ng\ndÃ¹_sao\ndÃ¹ng\ndÃ¹ng_cho\ndÃ¹ng_háº¿t\ndÃ¹ng_lÃ m\ndÃ¹ng_Ä‘áº¿n\ndÆ°á»›i\ndÆ°á»›i_nÆ°á»›c\ndáº¡\ndáº¡_bÃ¡n\ndáº¡_con\ndáº¡_dÃ i\ndáº¡_dáº¡\ndáº¡_khÃ¡ch\ndáº§n_dÃ \ndáº§n_dáº§n\ndáº§u_sao\ndáº«n\ndáº«u\ndáº«u_mÃ \ndáº«u_ráº±ng\ndáº«u_sao\ndá»…\ndá»…_dÃ¹ng\ndá»…_gÃ¬\ndá»…_khiáº¿n\ndá»…_nghe\ndá»…_ngÆ°Æ¡i\ndá»…_nhÆ°_chÆ¡i\ndá»…_sá»£\ndá»…_sá»­_dá»¥ng\ndá»…_thÆ°á»ng\ndá»…_tháº¥y\ndá»…_Äƒn\ndá»…_Ä‘Ã¢u\ndá»Ÿ_chá»«ng\ndá»¯\ndá»¯_cÃ¡ch\nem\nem_em\ngiÃ¡_trá»‹\ngiÃ¡_trá»‹_thá»±c_táº¿\ngiáº£m\ngiáº£m_chÃ­nh\ngiáº£m_tháº¥p\ngiáº£m_tháº¿\ngiá»‘ng\ngiá»‘ng_ngÆ°á»i\ngiá»‘ng_nhau\ngiá»‘ng_nhÆ°\ngiá»\ngiá»_lÃ¢u\ngiá»_nÃ y\ngiá»_Ä‘i\ngiá»_Ä‘Ã¢y\ngiá»_Ä‘áº¿n\ngiá»¯\ngiá»¯_láº¥y\ngiá»¯_Ã½\ngiá»¯a\ngiá»¯a_lÃºc\ngÃ¢y\ngÃ¢y_cho\ngÃ¢y_giá»‘ng\ngÃ¢y_ra\ngÃ¢y_thÃªm\ngÃ¬\ngÃ¬_gÃ¬\ngÃ¬_Ä‘Ã³\ngáº§n\ngáº§n_bÃªn\ngáº§n_háº¿t\ngáº§n_ngÃ y\ngáº§n_nhÆ°\ngáº§n_xa\ngáº§n_Ä‘Ã¢y\ngáº§n_Ä‘áº¿n\ngáº·p\ngáº·p_khÃ³_khÄƒn\ngáº·p_pháº£i\ngá»“m\nhay\nhay_biáº¿t\nhay_hay\nhay_khÃ´ng\nhay_lÃ \nhay_lÃ m\nhay_nhá»‰\nhay_nÃ³i\nhay_sao\nhay_tin\nhay_Ä‘Ã¢u\nhiá»ƒu\nhiá»‡n_nay\nhiá»‡n_táº¡i\nhoÃ n_toÃ n\nhoáº·c\nhoáº·c_lÃ \nhÃ£y\nhÃ£y_cÃ²n\nhÆ¡n\nhÆ¡n_cáº£\nhÆ¡n_háº¿t\nhÆ¡n_lÃ \nhÆ¡n_ná»¯a\nhÆ¡n_trÆ°á»›c\nháº§u_háº¿t\nháº¿t\nháº¿t_chuyá»‡n\nháº¿t_cáº£\nháº¿t_cá»§a\nháº¿t_nÃ³i\nháº¿t_rÃ¡o\nháº¿t_rá»“i\nháº¿t_Ã½\nhá»\nhá»_gáº§n\nhá»_xa\nhá»i\nhá»i_láº¡i\nhá»i_xem\nhá»i_xin\nhá»—_trá»£\nkhi\nkhi_khÃ¡c\nkhi_khÃ´ng\nkhi_nÃ o\nkhi_nÃªn\nkhi_trÆ°á»›c\nkhiáº¿n\nkhoáº£ng\nkhoáº£ng_cÃ¡ch\nkhoáº£ng_khÃ´ng\nkhÃ¡\nkhÃ¡_tá»‘t\nkhÃ¡c\nkhÃ¡c_gÃ¬\nkhÃ¡c_khÃ¡c\nkhÃ¡c_nhau\nkhÃ¡c_nÃ o\nkhÃ¡c_thÆ°á»ng\nkhÃ¡c_xa\nkhÃ¡ch\nkhÃ³\nkhÃ³_biáº¿t\nkhÃ³_chÆ¡i\nkhÃ³_khÄƒn\nkhÃ³_lÃ m\nkhÃ³_má»Ÿ\nkhÃ³_nghe\nkhÃ³_nghÄ©\nkhÃ³_nÃ³i\nkhÃ³_tháº¥y\nkhÃ³_trÃ¡nh\nkhÃ´ng\nkhÃ´ng_ai\nkhÃ´ng_bao_giá»\nkhÃ´ng_bao_lÃ¢u\nkhÃ´ng_biáº¿t\nkhÃ´ng_bÃ¡n\nkhÃ´ng_chá»‰\nkhÃ´ng_cÃ²n\nkhÃ´ng_cÃ³\nkhÃ´ng_cÃ³_gÃ¬\nkhÃ´ng_cÃ¹ng\nkhÃ´ng_cáº§n\nkhÃ´ng_cá»©\nkhÃ´ng_dÃ¹ng\nkhÃ´ng_gÃ¬\nkhÃ´ng_hay\nkhÃ´ng_khá»i\nkhÃ´ng_ká»ƒ\nkhÃ´ng_ngoÃ i\nkhÃ´ng_nháº­n\nkhÃ´ng_nhá»¯ng\nkhÃ´ng_pháº£i\nkhÃ´ng_pháº£i_khÃ´ng\nkhÃ´ng_thá»ƒ\nkhÃ´ng_tÃ­nh\nkhÃ´ng_Ä‘iá»u_kiá»‡n\nkhÃ´ng_Ä‘Æ°á»£c\nkhÃ´ng_Ä‘áº§y\nkhÃ´ng_Ä‘á»ƒ\nkháº³ng_Ä‘á»‹nh\nkhá»i\nkhá»i_nÃ³i\nká»ƒ\nká»ƒ_cáº£\nká»ƒ_nhÆ°\nká»ƒ_tá»›i\nká»ƒ_tá»«\nliÃªn_quan\nloáº¡i\nloáº¡i_tá»«\nluÃ´n\nluÃ´n_cáº£\nluÃ´n_luÃ´n\nluÃ´n_tay\nlÃ \nlÃ _cÃ¹ng\nlÃ _lÃ \nlÃ _nhiá»u\nlÃ _pháº£i\nlÃ _tháº¿_nÃ o\nlÃ _vÃ¬\nlÃ _Ã­t\nlÃ m\nlÃ m_báº±ng\nlÃ m_cho\nlÃ m_dáº§n_dáº§n\nlÃ m_gÃ¬\nlÃ m_lÃ²ng\nlÃ m_láº¡i\nlÃ m_láº¥y\nlÃ m_máº¥t\nlÃ m_ngay\nlÃ m_nhÆ°\nlÃ m_nÃªn\nlÃ m_ra\nlÃ m_riÃªng\nlÃ m_sao\nlÃ m_theo\nlÃ m_tháº¿_nÃ o\nlÃ m_tin\nlÃ m_tÃ´i\nlÃ m_tÄƒng\nlÃ m_táº¡i\nlÃ m_táº¯p_lá»±\nlÃ m_vÃ¬\nlÃ m_Ä‘Ãºng\nlÃ m_Ä‘Æ°á»£c\nlÃ¢u\nlÃ¢u_cÃ¡c\nlÃ¢u_lÃ¢u\nlÃ¢u_nay\nlÃ¢u_ngÃ y\nlÃªn\nlÃªn_cao\nlÃªn_cÆ¡n\nlÃªn_máº¡nh\nlÃªn_ngÃ´i\nlÃªn_nÆ°á»›c\nlÃªn_sá»‘\nlÃªn_xuá»‘ng\nlÃªn_Ä‘áº¿n\nlÃ²ng\nlÃ²ng_khÃ´ng\nlÃºc\nlÃºc_khÃ¡c\nlÃºc_lÃ¢u\nlÃºc_nÃ o\nlÃºc_nÃ y\nlÃºc_sÃ¡ng\nlÃºc_trÆ°á»›c\nlÃºc_Ä‘i\nlÃºc_Ä‘Ã³\nlÃºc_Ä‘áº¿n\nlÃºc_áº¥y\nlÃ½_do\nlÆ°á»£ng\nlÆ°á»£ng_cáº£\nlÆ°á»£ng_sá»‘\nlÆ°á»£ng_tá»«\nláº¡i\nláº¡i_bá»™\nláº¡i_cÃ¡i\nláº¡i_cÃ²n\nláº¡i_giá»‘ng\nláº¡i_lÃ m\nláº¡i_ngÆ°á»i\nláº¡i_nÃ³i\nláº¡i_ná»¯a\nláº¡i_quáº£\nláº¡i_thÃ´i\nláº¡i_Äƒn\nláº¡i_Ä‘Ã¢y\nláº¥y\nláº¥y_cÃ³\nláº¥y_cáº£\nláº¥y_giá»‘ng\nláº¥y_lÃ m\nláº¥y_lÃ½_do\nláº¥y_láº¡i\nláº¥y_ra\nláº¥y_rÃ¡o\nláº¥y_sau\nláº¥y_sá»‘\nláº¥y_thÃªm\nláº¥y_tháº¿\nláº¥y_vÃ o\nláº¥y_xuá»‘ng\nláº¥y_Ä‘Æ°á»£c\nláº¥y_Ä‘á»ƒ\nláº§n\nláº§n_khÃ¡c\nláº§n_láº§n\nláº§n_nÃ o\nláº§n_nÃ y\nláº§n_sang\nláº§n_sau\nláº§n_theo\nláº§n_trÆ°á»›c\nláº§n_tÃ¬m\nlá»›n\nlá»›n_lÃªn\nlá»›n_nhá»\nlá»i\nlá»i_chÃº\nlá»i_nÃ³i\nmang\nmang_láº¡i\nmang_mang\nmang_náº·ng\nmang_vá»\nmuá»‘n\nmÃ \nmÃ _cáº£\nmÃ _khÃ´ng\nmÃ _láº¡i\nmÃ _thÃ´i\nmÃ _váº«n\nmÃ¬nh\nmáº¡nh\nmáº¥t\nmáº¥t_cÃ²n\nmá»i\nmá»i_giá»\nmá»i_khi\nmá»i_lÃºc\nmá»i_ngÆ°á»i\nmá»i_nÆ¡i\nmá»i_sá»±\nmá»i_thá»©\nmá»i_viá»‡c\nmá»‘i\nmá»—i\nmá»—i_lÃºc\nmá»—i_láº§n\nmá»—i_má»™t\nmá»—i_ngÃ y\nmá»—i_ngÆ°á»i\nmá»™t\nmá»™t_cÃ¡ch\nmá»™t_cÆ¡n\nmá»™t_khi\nmá»™t_lÃºc\nmá»™t_sá»‘\nmá»™t_vÃ i\nmá»™t_Ã­t\nmá»›i\nmá»›i_hay\nmá»›i_rá»“i\nmá»›i_Ä‘Ã¢y\nmá»Ÿ\nmá»Ÿ_mang\nmá»Ÿ_nÆ°á»›c\nmá»Ÿ_ra\nmá»£\nmá»©c\nnay\nngay\nngay_bÃ¢y_giá»\nngay_cáº£\nngay_khi\nngay_khi_Ä‘áº¿n\nngay_lÃºc\nngay_lÃºc_nÃ y\nngay_láº­p_tá»©c\nngay_tháº­t\nngay_tá»©c_kháº¯c\nngay_tá»©c_thÃ¬\nngay_tá»«\nnghe\nnghe_chá»«ng\nnghe_hiá»ƒu\nnghe_khÃ´ng\nnghe_láº¡i\nnghe_nhÃ¬n\nnghe_nhÆ°\nnghe_nÃ³i\nnghe_ra\nnghe_rÃµ\nnghe_tháº¥y\nnghe_tin\nnghe_trá»±c_tiáº¿p\nnghe_Ä‘Ã¢u\nnghe_Ä‘Ã¢u_nhÆ°\nnghe_Ä‘Æ°á»£c\nnghen\nnghiá»…m_nhiÃªn\nnghÄ©\nnghÄ©_láº¡i\nnghÄ©_ra\nnghÄ©_tá»›i\nnghÄ©_xa\nnghÄ©_Ä‘áº¿n\nnghá»‰m\nngoÃ i\nngoÃ i_nÃ y\nngoÃ i_ra\nngoÃ i_xa\nngoáº£i\nnguá»“n\nngÃ y\nngÃ y_cÃ ng\nngÃ y_cáº¥p\nngÃ y_giá»\nngÃ y_ngÃ y\nngÃ y_nÃ o\nngÃ y_nÃ y\nngÃ y_ná»\nngÃ y_qua\nngÃ y_rÃ y\nngÃ y_thÃ¡ng\nngÃ y_xÆ°a\nngÃ y_xá»­a\nngÃ y_Ä‘áº¿n\nngÃ y_áº¥y\nngÃ´i\nngÃ´i_nhÃ \nngÃ´i_thá»©\nngÃµ_háº§u\nngÄƒn_ngáº¯t\nngÆ°Æ¡i\nngÆ°á»i\nngÆ°á»i_há»i\nngÆ°á»i_khÃ¡c\nngÆ°á»i_khÃ¡ch\nngÆ°á»i_mÃ¬nh\nngÆ°á»i_nghe\nngÆ°á»i_ngÆ°á»i\nngÆ°á»i_nháº­n\nngá»n\nngá»n_nguá»“n\nngá»t\nngá»“i\nngá»“i_bá»‡t\nngá»“i_khÃ´ng\nngá»“i_sau\nngá»“i_trá»‡t\nngá»™_nhá»¡\nnhanh\nnhanh_lÃªn\nnhanh_tay\nnhau\nnhiÃªn_háº­u\nnhiá»u\nnhiá»u_Ã­t\nnhiá»‡t_liá»‡t\nnhung_nhÄƒng\nnhÃ \nnhÃ _chung\nnhÃ _khÃ³\nnhÃ _lÃ m\nnhÃ _ngoÃ i\nnhÃ _ngÆ°Æ¡i\nnhÃ _tÃ´i\nnhÃ _viá»‡c\nnhÃ¢n_dá»‹p\nnhÃ¢n_tiá»‡n\nnhÃ©\nnhÃ¬n\nnhÃ¬n_chung\nnhÃ¬n_láº¡i\nnhÃ¬n_nháº­n\nnhÃ¬n_theo\nnhÃ¬n_tháº¥y\nnhÃ¬n_xuá»‘ng\nnhÃ³m\nnhÃ³n_nhÃ©n\nnhÆ°\nnhÆ°_ai\nnhÆ°_chÆ¡i\nnhÆ°_khÃ´ng\nnhÆ°_lÃ \nnhÆ°_nhau\nnhÆ°_quáº£\nnhÆ°_sau\nnhÆ°_thÆ°á»ng\nnhÆ°_tháº¿\nnhÆ°_tháº¿_nÃ o\nnhÆ°_thá»ƒ\nnhÆ°_trÃªn\nnhÆ°_trÆ°á»›c\nnhÆ°_tuá»“ng\nnhÆ°_váº­y\nnhÆ°_Ã½\nnhÆ°ng\nnhÆ°ng_mÃ \nnhÆ°á»£c_báº±ng\nnháº¥t\nnháº¥t_loáº¡t\nnháº¥t_luáº­t\nnháº¥t_lÃ \nnháº¥t_má»±c\nnháº¥t_nháº¥t\nnháº¥t_quyáº¿t\nnháº¥t_sinh\nnháº¥t_thiáº¿t\nnháº¥t_thÃ¬\nnháº¥t_tÃ¢m\nnháº¥t_tá»\nnháº¥t_Ä‘Ã¡n\nnháº¥t_Ä‘á»‹nh\nnháº­n\nnháº­n_biáº¿t\nnháº­n_há»\nnháº­n_lÃ m\nnháº­n_nhau\nnháº­n_ra\nnháº­n_tháº¥y\nnháº­n_viá»‡c\nnháº­n_Ä‘Æ°á»£c\nnháº±m\nnháº±m_khi\nnháº±m_lÃºc\nnháº±m_vÃ o\nnháº±m_Ä‘á»ƒ\nnhá»‰\nnhá»\nnhá»_ngÆ°á»i\nnhá»›\nnhá»›_báº­p_bÃµm\nnhá»›_láº¡i\nnhá»›_láº¥y\nnhá»›_ra\nnhá»\nnhá»_chuyá»ƒn\nnhá»_cÃ³\nnhá»_nhá»\nnhá»_Ä‘Ã³\nnhá»¡_ra\nnhá»¯ng\nnhá»¯ng_ai\nnhá»¯ng_khi\nnhá»¯ng_lÃ \nnhá»¯ng_lÃºc\nnhá»¯ng_muá»‘n\nnhá»¯ng_nhÆ°\nnÃ o\nnÃ o_cÅ©ng\nnÃ o_hay\nnÃ o_lÃ \nnÃ o_pháº£i\nnÃ o_Ä‘Ã¢u\nnÃ o_Ä‘Ã³\nnÃ y\nnÃ y_ná»\nnÃªn\nnÃªn_chi\nnÃªn_chÄƒng\nnÃªn_lÃ m\nnÃªn_ngÆ°á»i\nnÃªn_trÃ¡nh\nnÃ³\nnÃ³c\nnÃ³i\nnÃ³i_bÃ´ng\nnÃ³i_chung\nnÃ³i_khÃ³\nnÃ³i_lÃ \nnÃ³i_lÃªn\nnÃ³i_láº¡i\nnÃ³i_nhá»\nnÃ³i_pháº£i\nnÃ³i_qua\nnÃ³i_ra\nnÃ³i_riÃªng\nnÃ³i_rÃµ\nnÃ³i_thÃªm\nnÃ³i_tháº­t\nnÃ³i_toáº¹t\nnÃ³i_trÆ°á»›c\nnÃ³i_tá»‘t\nnÃ³i_vá»›i\nnÃ³i_xa\nnÃ³i_Ã½\nnÃ³i_Ä‘áº¿n\nnÃ³i_Ä‘á»§\nnÄƒm\nnÄƒm_thÃ¡ng\nnÆ¡i\nnÆ¡i_nÆ¡i\nnÆ°á»›c\nnÆ°á»›c_bÃ i\nnÆ°á»›c_cÃ¹ng\nnÆ°á»›c_lÃªn\nnÆ°á»›c_náº·ng\nnÆ°á»›c_quáº£\nnÆ°á»›c_xuá»‘ng\nnÆ°á»›c_Äƒn\nnÆ°á»›c_Ä‘áº¿n\nnáº¥y\nnáº·ng\nnáº·ng_cÄƒn\nnáº·ng_mÃ¬nh\nnáº·ng_vá»\nnáº¿u\nnáº¿u_cÃ³\nnáº¿u_cáº§n\nnáº¿u_khÃ´ng\nnáº¿u_mÃ \nnáº¿u_nhÆ°\nnáº¿u_tháº¿\nnáº¿u_váº­y\nnáº¿u_Ä‘Æ°á»£c\nná»n\nná»\nná»›\nná»©c_ná»Ÿ\nná»¯a\nná»¯a_khi\nná»¯a_lÃ \nná»¯a_rá»“i\noai_oÃ¡i\noÃ¡i\npho\nphÃ¨\nphÃ¨_phÃ¨\nphÃ­a\nphÃ­a_bÃªn\nphÃ­a_báº¡n\nphÃ­a_dÆ°á»›i\nphÃ­a_sau\nphÃ­a_trong\nphÃ­a_trÃªn\nphÃ­a_trÆ°á»›c\nphÃ³c\nphÃ³t\nphÃ¹_há»£p\nphÄƒn_pháº¯t\nphÆ°Æ¡ng_chi\npháº£i\npháº£i_biáº¿t\npháº£i_chi\npháº£i_chÄƒng\npháº£i_cÃ¡ch\npháº£i_cÃ¡i\npháº£i_giá»\npháº£i_khi\npháº£i_khÃ´ng\npháº£i_láº¡i\npháº£i_lá»i\npháº£i_ngÆ°á»i\npháº£i_nhÆ°\npháº£i_rá»“i\npháº£i_tay\npháº§n\npháº§n_lá»›n\npháº§n_nhiá»u\npháº§n_nÃ o\npháº§n_sau\npháº§n_viá»‡c\npháº¯t\nphá»‰_phui\nphá»ng\nphá»ng_nhÆ°\nphá»ng_nÆ°á»›c\nphá»ng_theo\nphá»ng_tÃ­nh\nphá»‘c\nphá»¥t\nphá»©t\nqua\nqua_chuyá»‡n\nqua_khá»i\nqua_láº¡i\nqua_láº§n\nqua_ngÃ y\nqua_tay\nqua_thÃ¬\nqua_Ä‘i\nquan_trá»ng\nquan_trá»ng_váº¥n_Ä‘á»\nquan_tÃ¢m\nquay\nquay_bÆ°á»›c\nquay_láº¡i\nquay_sá»‘\nquay_Ä‘i\nquÃ¡\nquÃ¡_bÃ¡n\nquÃ¡_bá»™\nquÃ¡_giá»\nquÃ¡_lá»i\nquÃ¡_má»©c\nquÃ¡_nhiá»u\nquÃ¡_tay\nquÃ¡_thÃ¬\nquÃ¡_tin\nquÃ¡_trÃ¬nh\nquÃ¡_tuá»•i\nquÃ¡_Ä‘Ã¡ng\nquÃ¡_Æ°\nquáº£\nquáº£_lÃ \nquáº£_tháº­t\nquáº£_tháº¿\nquáº£_váº­y\nquáº­n\nra\nra_bÃ i\nra_bá»™\nra_chÆ¡i\nra_gÃ¬\nra_láº¡i\nra_lá»i\nra_ngÃ´i\nra_ngÆ°á»i\nra_sao\nra_tay\nra_vÃ o\nra_Ã½\nra_Ä‘iá»u\nra_Ä‘Ã¢y\nren_rÃ©n\nriu_rÃ­u\nriÃªng\nriÃªng_tá»«ng\nriá»‡t\nrÃ y\nrÃ¡o\nrÃ¡o_cáº£\nrÃ¡o_nÆ°á»›c\nrÃ¡o_trá»i\nrÃ©n\nrÃ©n_bÆ°á»›c\nrÃ­ch\nrÃ³n_rÃ©n\nrÃµ\nrÃµ_lÃ \nrÃµ_tháº­t\nrÃºt_cá»¥c\nrÄƒng\nrÄƒng_rÄƒng\nráº¥t\nráº¥t_lÃ¢u\nráº±ng\nráº±ng_lÃ \nrá»‘t_cuá»™c\nrá»‘t_cá»¥c\nrá»“i\nrá»“i_ná»¯a\nrá»“i_ra\nrá»“i_sao\nrá»“i_sau\nrá»“i_tay\nrá»“i_thÃ¬\nrá»“i_xem\nrá»“i_Ä‘Ã¢y\nrá»©a\nsa_sáº£\nsang\nsang_nÄƒm\nsang_sÃ¡ng\nsang_tay\nsao\nsao_báº£n\nsao_báº±ng\nsao_cho\nsao_váº­y\nsao_Ä‘ang\nsau\nsau_chÃ³t\nsau_cuá»‘i\nsau_cÃ¹ng\nsau_háº¿t\nsau_nÃ y\nsau_ná»¯a\nsau_sau\nsau_Ä‘Ã¢y\nsau_Ä‘Ã³\nso\nso_vá»›i\nsong_le\nsuÃ½t\nsuÃ½t_ná»¯a\nsÃ¡ng\nsÃ¡ng_ngÃ y\nsÃ¡ng_rÃµ\nsÃ¡ng_tháº¿\nsÃ¡ng_Ã½\nsÃ¬\nsÃ¬_sÃ¬\nsáº¥t\nsáº¯p\nsáº¯p_Ä‘áº·t\nsáº½\nsáº½_biáº¿t\nsáº½_hay\nsá»‘\nsá»‘_cho_biáº¿t\nsá»‘_cá»¥_thá»ƒ\nsá»‘_loáº¡i\nsá»‘_lÃ \nsá»‘_ngÆ°á»i\nsá»‘_pháº§n\nsá»‘_thiáº¿u\nsá»‘t_sá»™t\nsá»›m\nsá»›m_ngÃ y\nsá»Ÿ_dÄ©\nsá»­_dá»¥ng\nsá»±\nsá»±_tháº¿\nsá»±_viá»‡c\ntanh\ntanh_tanh\ntay\ntay_quay\ntha_há»“\ntha_há»“_chÆ¡i\ntha_há»“_Äƒn\nthan_Ã´i\nthanh\nthanh_ba\nthanh_chuyá»ƒn\nthanh_khÃ´ng\nthanh_thanh\nthanh_tÃ­nh\nthanh_Ä‘iá»u_kiá»‡n\nthanh_Ä‘iá»ƒm\nthay_Ä‘á»•i\nthay_Ä‘á»•i_tÃ¬nh_tráº¡ng\ntheo\ntheo_bÆ°á»›c\ntheo_nhÆ°\ntheo_tin\nthi_thoáº£ng\nthiáº¿u\nthiáº¿u_gÃ¬\nthiáº¿u_Ä‘iá»ƒm\nthoáº¡t\nthoáº¡t_nghe\nthoáº¡t_nhiÃªn\nthoáº¯t\nthuáº§n\nthuáº§n_Ã¡i\nthuá»™c\nthuá»™c_bÃ i\nthuá»™c_cÃ¡ch\nthuá»™c_láº¡i\nthuá»™c_tá»«\nthÃ \nthÃ _lÃ \nthÃ _ráº±ng\nthÃ nh_ra\nthÃ nh_thá»­\nthÃ¡i_quÃ¡\nthÃ¡ng\nthÃ¡ng_ngÃ y\nthÃ¡ng_nÄƒm\nthÃ¡ng_thÃ¡ng\nthÃªm\nthÃªm_chuyá»‡n\nthÃªm_giá»\nthÃªm_vÃ o\nthÃ¬\nthÃ¬_giá»\nthÃ¬_lÃ \nthÃ¬_pháº£i\nthÃ¬_ra\nthÃ¬_thÃ´i\nthÃ¬nh_lÃ¬nh\nthÃ­ch\nthÃ­ch_cá»©\nthÃ­ch_thuá»™c\nthÃ­ch_tá»±\nthÃ­ch_Ã½\nthÃ­m\nthÃ´i\nthÃ´i_viá»‡c\nthÃºng_tháº¯ng\nthÆ°Æ¡ng_Ã´i\nthÆ°á»ng\nthÆ°á»ng_bá»‹\nthÆ°á»ng_hay\nthÆ°á»ng_khi\nthÆ°á»ng_sá»‘\nthÆ°á»ng_sá»±\nthÆ°á»ng_thÃ´i\nthÆ°á»ng_thÆ°á»ng\nthÆ°á»ng_tÃ­nh\nthÆ°á»ng_táº¡i\nthÆ°á»ng_xuáº¥t_hiá»‡n\nthÆ°á»ng_Ä‘áº¿n\ntháº£o_hÃ¨n\ntháº£o_nÃ o\ntháº¥p\ntháº¥p_cÆ¡\ntháº¥p_thá»m\ntháº¥p_xuá»‘ng\ntháº¥y\ntháº¥y_thÃ¡ng\ntháº©y\ntháº­m\ntháº­m_chÃ­\ntháº­m_cáº¥p\ntháº­m_tá»«\ntháº­t\ntháº­t_cháº¯c\ntháº­t_lÃ \ntháº­t_lá»±c\ntháº­t_quáº£\ntháº­t_ra\ntháº­t_sá»±\ntháº­t_thÃ \ntháº­t_tá»‘t\ntháº­t_váº­y\ntháº¿\ntháº¿_chuáº©n_bá»‹\ntháº¿_lÃ \ntháº¿_láº¡i\ntháº¿_mÃ \ntháº¿_nÃ o\ntháº¿_nÃªn\ntháº¿_ra\ntháº¿_sá»±\ntháº¿_thÃ¬\ntháº¿_thÃ´i\ntháº¿_thÆ°á»ng\ntháº¿_tháº¿\ntháº¿_Ã \ntháº¿_Ä‘Ã³\ntháº¿ch\nthá»‰nh_thoáº£ng\nthá»m\nthá»‘c\nthá»‘c_thÃ¡o\nthá»‘t\nthá»‘t_nhiÃªn\nthá»‘t_nÃ³i\nthá»‘t_thÃ´i\nthá»™c\nthá»i_gian\nthá»i_gian_sá»­_dá»¥ng\nthá»i_gian_tÃ­nh\nthá»i_Ä‘iá»ƒm\nthá»¥c_máº¡ng\nthá»©\nthá»©_báº£n\nthá»©_Ä‘áº¿n\nthá»­a\nthá»±c_hiá»‡n\nthá»±c_hiá»‡n_Ä‘Ãºng\nthá»±c_ra\nthá»±c_sá»±\nthá»±c_táº¿\nthá»±c_váº­y\ntin\ntin_thÃªm\ntin_vÃ o\ntiáº¿p_theo\ntiáº¿p_tá»¥c\ntiáº¿p_Ä‘Ã³\ntiá»‡n_thá»ƒ\ntoÃ \ntoÃ©_khÃ³i\ntoáº¹t\ntrong\ntrong_khi\ntrong_lÃºc\ntrong_mÃ¬nh\ntrong_ngoÃ i\ntrong_nÃ y\ntrong_sá»‘\ntrong_vÃ¹ng\ntrong_Ä‘Ã³\ntrong_áº¥y\ntrÃ¡nh\ntrÃ¡nh_khá»i\ntrÃ¡nh_ra\ntrÃ¡nh_tÃ¬nh_tráº¡ng\ntrÃ¡nh_xa\ntrÃªn\ntrÃªn_bá»™\ntrÃªn_dÆ°á»›i\ntrÆ°á»›c\ntrÆ°á»›c_háº¿t\ntrÆ°á»›c_khi\ntrÆ°á»›c_kia\ntrÆ°á»›c_nay\ntrÆ°á»›c_ngÃ y\ntrÆ°á»›c_nháº¥t\ntrÆ°á»›c_sau\ntrÆ°á»›c_tiÃªn\ntrÆ°á»›c_tuá»•i\ntrÆ°á»›c_Ä‘Ã¢y\ntrÆ°á»›c_Ä‘Ã³\ntráº£\ntráº£_cá»§a\ntráº£_láº¡i\ntráº£_ngay\ntráº£_trÆ°á»›c\ntráº¿u_trÃ¡o\ntrá»ƒn\ntrá»‡t\ntrá»‡u_tráº¡o\ntrá»ng\ntrá»i_Ä‘áº¥t_Æ¡i\ntrá»Ÿ_thÃ nh\ntrá»«_phi\ntrá»±c_tiáº¿p\ntrá»±c_tiáº¿p_lÃ m\ntuy\ntuy_cÃ³\ntuy_lÃ \ntuy_nhiÃªn\ntuy_ráº±ng\ntuy_tháº¿\ntuy_váº­y\ntuy_Ä‘Ã£\ntuyá»‡t_nhiÃªn\ntuáº§n_tá»±\ntuá»‘t_luá»‘t\ntuá»‘t_tuá»“n_tuá»™t\ntuá»‘t_tuá»™t\ntuá»•i\ntuá»•i_cáº£\ntuá»•i_tÃ´i\ntÃ _tÃ \ntÃªn\ntÃªn_chÃ­nh\ntÃªn_cÃ¡i\ntÃªn_há»\ntÃªn_tá»±\ntÃªnh\ntÃªnh_tÃªnh\ntÃ¬m\ntÃ¬m_báº¡n\ntÃ¬m_cÃ¡ch\ntÃ¬m_hiá»ƒu\ntÃ¬m_ra\ntÃ¬m_viá»‡c\ntÃ¬nh_tráº¡ng\ntÃ­nh\ntÃ­nh_cÃ¡ch\ntÃ­nh_cÄƒn\ntÃ­nh_ngÆ°á»i\ntÃ­nh_phá»ng\ntÃ­nh_tá»«\ntÃ­t_mÃ¹\ntÃ²_te\ntÃ´i\ntÃ´i_con\ntÃ´ng_tá»‘c\ntÃ¹_tÃ¬\ntÄƒm_táº¯p\ntÄƒng\ntÄƒng_chÃºng\ntÄƒng_cáº¥p\ntÄƒng_giáº£m\ntÄƒng_thÃªm\ntÄƒng_tháº¿\ntáº¡i\ntáº¡i_lÃ²ng\ntáº¡i_nÆ¡i\ntáº¡i_sao\ntáº¡i_tÃ´i\ntáº¡i_vÃ¬\ntáº¡i_Ä‘Ã¢u\ntáº¡i_Ä‘Ã¢y\ntáº¡i_Ä‘Ã³\ntáº¡o\ntáº¡o_cÆ¡_há»™i\ntáº¡o_nÃªn\ntáº¡o_ra\ntáº¡o_Ã½\ntáº¡o_Ä‘iá»u_kiá»‡n\ntáº¥m\ntáº¥m_báº£n\ntáº¥m_cÃ¡c\ntáº¥n\ntáº¥n_tá»›i\ntáº¥t_cáº£\ntáº¥t_cáº£_bao_nhiÃªu\ntáº¥t_tháº£y\ntáº¥t_táº§n_táº­t\ntáº¥t_táº­t\ntáº­p_trung\ntáº¯p\ntáº¯p_lá»±\ntáº¯p_táº¯p\ntá»t\ntá»_ra\ntá»_váº»\ntá»‘c_táº£\ntá»‘i_Æ°\ntá»‘t\ntá»‘t_báº¡n\ntá»‘t_bá»™\ntá»‘t_hÆ¡n\ntá»‘t_má»‘i\ntá»‘t_ngÃ y\ntá»™t\ntá»™t_cÃ¹ng\ntá»›\ntá»›i\ntá»›i_gáº§n\ntá»›i_má»©c\ntá»›i_nÆ¡i\ntá»›i_thÃ¬\ntá»©c_thÃ¬\ntá»©c_tá»‘c\ntá»«\ntá»«_cÄƒn\ntá»«_giá»\ntá»«_khi\ntá»«_loáº¡i\ntá»«_nay\ntá»«_tháº¿\ntá»«_tÃ­nh\ntá»«_táº¡i\ntá»«_tá»«\ntá»«_Ã¡i\ntá»«_Ä‘iá»u\ntá»«_Ä‘Ã³\ntá»«_áº¥y\ntá»«ng\ntá»«ng_cÃ¡i\ntá»«ng_giá»\ntá»«ng_nhÃ \ntá»«ng_pháº§n\ntá»«ng_thá»i_gian\ntá»«ng_Ä‘Æ¡n_vá»‹\ntá»«ng_áº¥y\ntá»±\ntá»±_cao\ntá»±_khi\ntá»±_lÆ°á»£ng\ntá»±_tÃ­nh\ntá»±_táº¡o\ntá»±_vÃ¬\ntá»±_Ã½\ntá»±_Äƒn\ntá»±u_trung\nveo\nveo_veo\nviá»‡c\nviá»‡c_gÃ¬\nvung_thiÃªn_Ä‘á»‹a\nvung_tÃ n_tÃ¡n\nvung_tÃ¡n_tÃ n\nvÃ \nvÃ i\nvÃ i_ba\nvÃ i_ngÆ°á»i\nvÃ i_nhÃ \nvÃ i_nÆ¡i\nvÃ i_tÃªn\nvÃ i_Ä‘iá»u\nvÃ o\nvÃ o_gáº·p\nvÃ o_khoáº£ng\nvÃ o_lÃºc\nvÃ o_vÃ¹ng\nvÃ o_Ä‘áº¿n\nvÃ¢ng\nvÃ¢ng_chá»‹u\nvÃ¢ng_dáº¡\nvÃ¢ng_vÃ¢ng\nvÃ¢ng_Ã½\nvÃ¨o\nvÃ¨o_vÃ¨o\nvÃ¬\nvÃ¬_chÆ°ng\nvÃ¬_ráº±ng\nvÃ¬_sao\nvÃ¬_tháº¿\nvÃ¬_váº­y\nvÃ­_báº±ng\nvÃ­_dÃ¹\nvÃ­_phá»ng\nvÃ­_thá»­\nvÃ´_hÃ¬nh_trung\nvÃ´_ká»ƒ\nvÃ´_luáº­n\nvÃ´_vÃ n\nvÃ¹ng\nvÃ¹ng_lÃªn\nvÃ¹ng_nÆ°á»›c\nvÄƒng_tÃª\nvÆ°á»£t\nvÆ°á»£t_khá»i\nvÆ°á»£t_quÃ¡\nváº¡n_nháº¥t\nváº£_chÄƒng\nváº£_láº¡i\nváº¥n_Ä‘á»\nváº¥n_Ä‘á»_quan_trá»ng\nváº«n\nváº«n_tháº¿\nváº­y\nváº­y_lÃ \nváº­y_mÃ \nváº­y_nÃªn\nváº­y_ra\nváº­y_thÃ¬\nváº­y_Æ°\nvá»\nvá»_khÃ´ng\nvá»_nÆ°á»›c\nvá»_pháº§n\nvá»_sau\nvá»_tay\nvá»‹_trÃ­\nvá»‹_táº¥t\nvá»‘n_dÄ©\nvá»›i\nvá»›i_láº¡i\nvá»›i_nhau\nvá»Ÿ\nvá»¥t\nvá»«a\nvá»«a_khi\nvá»«a_lÃºc\nvá»«a_má»›i\nvá»«a_qua\nvá»«a_rá»“i\nvá»«a_vá»«a\nxa\nxa_cÃ¡ch\nxa_gáº§n\nxa_nhÃ \nxa_tanh\nxa_táº¯p\nxa_xa\nxa_xáº£\nxem\nxem_láº¡i\nxem_ra\nxem_sá»‘\nxin\nxin_gáº·p\nxin_vÃ¢ng\nxiáº¿t_bao\nxon_xÃ³n\nxoÃ nh_xoáº¡ch\nxoÃ©t\nxoáº³n\nxoáº¹t\nxuáº¥t_hiá»‡n\nxuáº¥t_kÃ¬_báº¥t_Ã½\nxuáº¥t_ká»³_báº¥t_Ã½\nxuá»ƒ\nxuá»‘ng\nxÄƒm_xÃºi\nxÄƒm_xÄƒm\nxÄƒm_xáº¯m\nxáº£y_ra\nxá»nh_xá»‡ch\nxá»‡p\nxá»­_lÃ½\nyÃªu_cáº§u\nÃ \nÃ _nÃ y\nÃ _Æ¡i\nÃ o\nÃ o_vÃ o\nÃ o_Ã o\nÃ¡\nÃ¡_Ã \nÃ¡i\nÃ¡i_chÃ \nÃ¡i_dÃ \nÃ¡ng\nÃ¡ng_nhÆ°\nÃ¢u_lÃ \nÃ­t\nÃ­t_biáº¿t\nÃ­t_cÃ³\nÃ­t_hÆ¡n\nÃ­t_khi\nÃ­t_lÃ¢u\nÃ­t_nhiá»u\nÃ­t_nháº¥t\nÃ­t_ná»¯a\nÃ­t_quÃ¡\nÃ­t_ra\nÃ­t_thÃ´i\nÃ­t_tháº¥y\nÃ´_hay\nÃ´_hÃ´\nÃ´_kÃª\nÃ´_kÃ¬a\nÃ´i_chao\nÃ´i_thÃ´i\nÃ´ng\nÃ´ng_nhá»\nÃ´ng_táº¡o\nÃ´ng_tá»«\nÃ´ng_áº¥y\nÃ´ng_á»•ng\nÃºi\nÃºi_chÃ \nÃºi_dÃ o\nÃ½\nÃ½_chá»«ng\nÃ½_da\nÃ½_hoáº·c\nÄƒn\nÄƒn_chung\nÄƒn_cháº¯c\nÄƒn_chá»‹u\nÄƒn_cuá»™c\nÄƒn_háº¿t\nÄƒn_há»i\nÄƒn_lÃ m\nÄƒn_ngÆ°á»i\nÄƒn_ngá»“i\nÄƒn_quÃ¡\nÄƒn_riÃªng\nÄƒn_sÃ¡ng\nÄƒn_tay\nÄƒn_trÃªn\nÄƒn_vá»\nÄ‘ang\nÄ‘ang_tay\nÄ‘ang_thÃ¬\nÄ‘iá»u\nÄ‘iá»u_gÃ¬\nÄ‘iá»u_kiá»‡n\nÄ‘iá»ƒm\nÄ‘iá»ƒm_chÃ­nh\nÄ‘iá»ƒm_gáº·p\nÄ‘iá»ƒm_Ä‘áº§u_tiÃªn\nÄ‘Ã nh_Ä‘áº¡ch\nÄ‘Ã¡ng\nÄ‘Ã¡ng_ká»ƒ\nÄ‘Ã¡ng_lÃ­\nÄ‘Ã¡ng_lÃ½\nÄ‘Ã¡ng_láº½\nÄ‘Ã¡ng_sá»‘\nÄ‘Ã¡nh_giÃ¡\nÄ‘Ã¡nh_Ä‘Ã¹ng\nÄ‘Ã¡o_Ä‘á»ƒ\nÄ‘Ã¢u\nÄ‘Ã¢u_cÃ³\nÄ‘Ã¢u_cÅ©ng\nÄ‘Ã¢u_nhÆ°\nÄ‘Ã¢u_nÃ o\nÄ‘Ã¢u_pháº£i\nÄ‘Ã¢u_Ä‘Ã¢u\nÄ‘Ã¢u_Ä‘Ã¢y\nÄ‘Ã¢u_Ä‘Ã³\nÄ‘Ã¢y\nÄ‘Ã¢y_nÃ y\nÄ‘Ã¢y_rá»“i\nÄ‘Ã¢y_Ä‘Ã³\nÄ‘Ã£\nÄ‘Ã£_hay\nÄ‘Ã£_khÃ´ng\nÄ‘Ã£_lÃ \nÄ‘Ã£_lÃ¢u\nÄ‘Ã£_tháº¿\nÄ‘Ã£_váº­y\nÄ‘Ã£_Ä‘á»§\nÄ‘Ã³\nÄ‘Ã³_Ä‘Ã¢y\nÄ‘Ãºng\nÄ‘Ãºng_ngÃ y\nÄ‘Ãºng_ra\nÄ‘Ãºng_tuá»•i\nÄ‘Ãºng_vá»›i\nÄ‘Æ¡n_vá»‹\nÄ‘Æ°a\nÄ‘Æ°a_cho\nÄ‘Æ°a_chuyá»‡n\nÄ‘Æ°a_em\nÄ‘Æ°a_ra\nÄ‘Æ°a_tay\nÄ‘Æ°a_tin\nÄ‘Æ°a_tá»›i\nÄ‘Æ°a_vÃ o\nÄ‘Æ°a_vá»\nÄ‘Æ°a_xuá»‘ng\nÄ‘Æ°a_Ä‘áº¿n\nÄ‘Æ°á»£c\nÄ‘Æ°á»£c_cÃ¡i\nÄ‘Æ°á»£c_lá»i\nÄ‘Æ°á»£c_nÆ°á»›c\nÄ‘Æ°á»£c_tin\nÄ‘áº¡i_loáº¡i\nÄ‘áº¡i_nhÃ¢n\nÄ‘áº¡i_phÃ m\nÄ‘áº¡i_Ä‘á»ƒ\nÄ‘áº¡t\nÄ‘áº£m_báº£o\nÄ‘áº§u_tiÃªn\nÄ‘áº§y\nÄ‘áº§y_nÄƒm\nÄ‘áº§y_phÃ¨\nÄ‘áº§y_tuá»•i\nÄ‘áº·c_biá»‡t\nÄ‘áº·t\nÄ‘áº·t_lÃ m\nÄ‘áº·t_mÃ¬nh\nÄ‘áº·t_má»©c\nÄ‘áº·t_ra\nÄ‘áº·t_trÆ°á»›c\nÄ‘áº·t_Ä‘á»ƒ\nÄ‘áº¿n\nÄ‘áº¿n_bao_giá»\nÄ‘áº¿n_cÃ¹ng\nÄ‘áº¿n_cÃ¹ng_cá»±c\nÄ‘áº¿n_cáº£\nÄ‘áº¿n_giá»\nÄ‘áº¿n_gáº§n\nÄ‘áº¿n_hay\nÄ‘áº¿n_khi\nÄ‘áº¿n_lÃºc\nÄ‘áº¿n_lá»i\nÄ‘áº¿n_nay\nÄ‘áº¿n_ngÃ y\nÄ‘áº¿n_nÆ¡i\nÄ‘áº¿n_ná»—i\nÄ‘áº¿n_thÃ¬\nÄ‘áº¿n_tháº¿\nÄ‘áº¿n_tuá»•i\nÄ‘áº¿n_xem\nÄ‘áº¿n_Ä‘iá»u\nÄ‘áº¿n_Ä‘Ã¢u\nÄ‘á»u\nÄ‘á»u_bÆ°á»›c\nÄ‘á»u_nhau\nÄ‘á»u_Ä‘á»u\nÄ‘á»ƒ\nÄ‘á»ƒ_cho\nÄ‘á»ƒ_giá»‘ng\nÄ‘á»ƒ_khÃ´ng\nÄ‘á»ƒ_lÃ²ng\nÄ‘á»ƒ_láº¡i\nÄ‘á»ƒ_mÃ \nÄ‘á»ƒ_pháº§n\nÄ‘á»ƒ_Ä‘Æ°á»£c\nÄ‘á»ƒ_Ä‘áº¿n_ná»—i\nÄ‘á»‘i_vá»›i\nÄ‘á»“ng_thá»i\nÄ‘á»§\nÄ‘á»§_dÃ¹ng\nÄ‘á»§_nÆ¡i\nÄ‘á»§_sá»‘\nÄ‘á»§_Ä‘iá»u\nÄ‘á»§_Ä‘iá»ƒm\nÆ¡\nÆ¡_hay\nÆ¡_kÃ¬a\nÆ¡i\nÆ¡i_lÃ \nÆ°\náº¡\náº¡_Æ¡i\náº¥y\náº¥y_lÃ \náº§u_Æ¡\náº¯t\náº¯t_háº³n\náº¯t_lÃ \náº¯t_pháº£i\náº¯t_tháº­t\ná»‘i_dÃ o\ná»‘i_giá»i\ná»‘i_giá»i_Æ¡i\ná»“\ná»“_á»“\ná»•ng\ná»›\ná»›_nÃ y\ná»\ná»_á»\ná»Ÿ\ná»Ÿ_láº¡i\ná»Ÿ_nhÆ°\ná»Ÿ_nhá»\ná»Ÿ_nÄƒm\ná»Ÿ_trÃªn\ná»Ÿ_vÃ o\ná»Ÿ_Ä‘Ã¢y\ná»Ÿ_Ä‘Ã³\ná»Ÿ_Ä‘Æ°á»£c\ná»§a\ná»©_há»±\ná»©_á»«\ná»«\ná»«_nhÃ©\ná»«_thÃ¬\ná»«_Ã o\ná»«_á»«\ná»­\n'.split('\n'))


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/vi/__init__.py----------------------------------------
A:spacy.lang.vi.__init__.words->self.pyvi_tokenize(text)
A:spacy.lang.vi.__init__.(words, spaces)->util.get_words_and_spaces(text.split(), text)
A:spacy.lang.vi.__init__.tokens->re.findall(patterns, text, re.UNICODE)
A:spacy.lang.vi.__init__.segs->self.pyvi_sylabelize_with_ws(text)
A:spacy.lang.vi.__init__.labels->self.ViTokenizer.ViTokenizer.model.predict([self.ViTokenizer.ViTokenizer.sent2features(words, False)])
A:spacy.lang.vi.__init__.self.use_pyvi->load_config_from_str(DEFAULT_CONFIG).get('use_pyvi', False)
A:spacy.lang.vi.__init__.path->util.ensure_path(path)
A:spacy.lang.vi.__init__.config->load_config_from_str(DEFAULT_CONFIG)
spacy.lang.vi.__init__.Vietnamese(Language)
spacy.lang.vi.__init__.VietnameseDefaults(BaseDefaults)
spacy.lang.vi.__init__.VietnameseTokenizer(self,vocab:Vocab,use_pyvi:bool=False)
spacy.lang.vi.__init__.VietnameseTokenizer.__reduce__(self)
spacy.lang.vi.__init__.VietnameseTokenizer._get_config(self)->Dict[str, Any]
spacy.lang.vi.__init__.VietnameseTokenizer._set_config(self,config:Dict[str,Any]={})->None
spacy.lang.vi.__init__.VietnameseTokenizer.from_bytes(self,data:bytes,**kwargs)->'VietnameseTokenizer'
spacy.lang.vi.__init__.VietnameseTokenizer.from_disk(self,path:Union[str,Path],**kwargs)->'VietnameseTokenizer'
spacy.lang.vi.__init__.VietnameseTokenizer.pyvi_sylabelize_with_ws(self,text)
spacy.lang.vi.__init__.VietnameseTokenizer.pyvi_tokenize(self,text)
spacy.lang.vi.__init__.VietnameseTokenizer.to_bytes(self,**kwargs)->bytes
spacy.lang.vi.__init__.VietnameseTokenizer.to_disk(self,path:Union[str,Path],**kwargs)->None
spacy.lang.vi.__init__.create_vietnamese_tokenizer(use_pyvi:bool=True)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/lang/vi/lex_attrs.py----------------------------------------
A:spacy.lang.vi.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.vi.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.vi.lex_attrs.like_num(text)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/training/initialize.py----------------------------------------
A:spacy.training.initialize.config->load_model_from_config(raw_config, auto_fill=True).config.interpolate()
A:spacy.training.initialize.sourced->get_sourced_components(config)
A:spacy.training.initialize.nlp->load_model_from_config(raw_config, auto_fill=True)
A:spacy.training.initialize.T->util.registry.resolve(config['training'], schema=ConfigSchemaTraining)
A:spacy.training.initialize.(train_corpus, dev_corpus)->resolve_dot_names(config, dot_names)
A:spacy.training.initialize.data_path->ensure_path(data)
A:spacy.training.initialize.lex_attrs->srsly.read_jsonl(data_path)
A:spacy.training.initialize.sourced_vectors_hashes->load_model_from_config(raw_config, auto_fill=True).meta.pop('_sourced_vectors_hashes', {})
A:spacy.training.initialize.vectors_hash->hash(nlp.vocab.vectors.to_bytes(exclude=['strings']))
A:spacy.training.initialize.vectors_nlp->load_model(name, vocab=nlp.vocab, exclude=exclude)
A:spacy.training.initialize.err->thinc.api.ConfigValidationError.from_error(e, title=title, desc=desc)
A:spacy.training.initialize.lex.rank->load_model_from_config(raw_config, auto_fill=True).vocab.vectors.key2row.get(lex.orth, OOV_RANK)
A:spacy.training.initialize.init_tok2vec->ensure_path(I['init_tok2vec'])
A:spacy.training.initialize.weights_data->zip_file.open(names[0]).read()
A:spacy.training.initialize.layer->get_tok2vec_ref(nlp, P)
A:spacy.training.initialize.vectors_loc->ensure_path(vectors_loc)
A:spacy.training.initialize.nlp.vocab.vectors->Vectors(strings=nlp.vocab.strings, data=vectors_data, keys=vector_keys)
A:spacy.training.initialize.(vectors_data, vector_keys, floret_settings)->read_vectors(vectors_loc, truncate, mode=mode)
A:spacy.training.initialize.f->ensure_shape(vectors_loc)
A:spacy.training.initialize.header_parts->next(f).split()
A:spacy.training.initialize.shape->tuple((int(size) for size in first_line.split()[:2]))
A:spacy.training.initialize.vectors_data->numpy.zeros(shape=shape, dtype='f')
A:spacy.training.initialize.line->line.rstrip().rstrip()
A:spacy.training.initialize.pieces->line.rstrip().rstrip().rsplit(' ', vectors_data.shape[1])
A:spacy.training.initialize.word->line.rstrip().rstrip().rsplit(' ', vectors_data.shape[1]).pop(0)
A:spacy.training.initialize.vectors_data[i]->numpy.asarray(pieces, dtype='f')
A:spacy.training.initialize.loc->ensure_path(loc)
A:spacy.training.initialize.zip_file->zipfile.ZipFile(str(loc))
A:spacy.training.initialize.names->zipfile.ZipFile(str(loc)).namelist()
A:spacy.training.initialize.file_->zipfile.ZipFile(str(loc)).open(names[0])
A:spacy.training.initialize.lines->open_file(vectors_loc)
A:spacy.training.initialize.first_line->next(lines)
A:spacy.training.initialize.lines2->open_file(vectors_loc)
spacy.training.initialize.convert_vectors(nlp:'Language',vectors_loc:Optional[Path],*,truncate:int,prune:int,name:Optional[str]=None,mode:str=VectorsMode.default)->None
spacy.training.initialize.ensure_shape(vectors_loc)
spacy.training.initialize.init_nlp(config:Config,*,use_gpu:int=-1)->'Language'
spacy.training.initialize.init_tok2vec(nlp:'Language',pretrain_config:Dict[str,Any],init_config:Dict[str,Any])->bool
spacy.training.initialize.init_vocab(nlp:'Language',*,data:Optional[Path]=None,lookups:Optional[Lookups]=None,vectors:Optional[str]=None)->None
spacy.training.initialize.load_vectors_into_model(nlp:'Language',name:Union[str,Path],*,add_strings:bool=True)->None
spacy.training.initialize.open_file(loc:Union[str,Path])->IO
spacy.training.initialize.read_vectors(vectors_loc:Path,truncate_vectors:int,*,mode:str=VectorsMode.default)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/training/pretrain.py----------------------------------------
A:spacy.training.pretrain.msg->Printer(no_print=silent)
A:spacy.training.pretrain.nlp->load_model_from_config(config)
A:spacy.training.pretrain._config->load_model_from_config(config).config.interpolate()
A:spacy.training.pretrain.P->util.registry.resolve(_config['pretraining'], schema=ConfigSchemaPretrain)
A:spacy.training.pretrain.corpus->dot_to_object(_config, P['corpus'])
A:spacy.training.pretrain.model->create_function(nlp.vocab, tok2vec)
A:spacy.training.pretrain.epoch_resume->_resume_model(model, resume_path, epoch_resume, silent=silent)
A:spacy.training.pretrain.tracker->ProgressTracker(frequency=10000)
A:spacy.training.pretrain.docs->ensure_docs(batch)
A:spacy.training.pretrain.loss->make_update(model, docs, optimizer, objective)
A:spacy.training.pretrain.progress->ProgressTracker(frequency=10000).update(epoch, loss, docs)
A:spacy.training.pretrain.weights_data->file_.read()
A:spacy.training.pretrain.model_name->re.search('model\\d+\\.bin', str(resume_path))
A:spacy.training.pretrain.(predictions, backprop)->create_function(nlp.vocab, tok2vec).begin_update(docs)
A:spacy.training.pretrain.(loss, gradients)->objective_func(model.ops, docs, predictions)
A:spacy.training.pretrain.tok2vec->get_tok2vec_ref(nlp, pretrain_config)
A:spacy.training.pretrain.layer->layer.get_ref(pretrain_config['layer']).get_ref(pretrain_config['layer'])
A:spacy.training.pretrain.self.words_per_epoch->Counter()
A:spacy.training.pretrain.self.last_time->time.time()
A:spacy.training.pretrain.words_in_batch->sum((len(doc) for doc in docs))
A:spacy.training.pretrain.self.prev_loss->float(self.loss)
A:spacy.training.pretrain.n_digits->len(str(int(figure)))
A:spacy.training.pretrain.n_decimal->min(n_decimal, max_decimal)
spacy.training.pretrain.ProgressTracker(self,frequency=1000000)
spacy.training.pretrain.ProgressTracker.update(self,epoch,loss,docs)
spacy.training.pretrain._resume_model(model:Model,resume_path:Path,epoch_resume:Optional[int],silent:bool=True)->int
spacy.training.pretrain._smart_round(figure:Union[float,int],width:int=10,max_decimal:int=4)->str
spacy.training.pretrain.create_pretraining_model(nlp,pretrain_config)
spacy.training.pretrain.ensure_docs(examples_or_docs:Iterable[Union[Doc,Example]])->List[Doc]
spacy.training.pretrain.get_tok2vec_ref(nlp,pretrain_config)
spacy.training.pretrain.make_update(model:Model,docs:Iterable[Doc],optimizer:Optimizer,objective_func:Callable)->float
spacy.training.pretrain.pretrain(config:Config,output_dir:Path,resume_path:Optional[Path]=None,epoch_resume:Optional[int]=None,use_gpu:int=-1,silent:bool=True)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/training/batchers.py----------------------------------------
A:spacy.training.batchers.ItemT->TypeVar('ItemT')
A:spacy.training.batchers.size_->iter(size)
A:spacy.training.batchers.outer_batch->list(outer_batch)
A:spacy.training.batchers.target_size->next(size_)
A:spacy.training.batchers.n_words->get_length(seq)
spacy.training.batchers._batch_by_length(seqs:Sequence[Any],max_words:int,get_length=len)->List[List[Any]]
spacy.training.batchers.configure_minibatch(size:Sizing,get_length:Optional[Callable[[ItemT],int]]=None)->BatcherT
spacy.training.batchers.configure_minibatch_by_padded_size(*,size:Sizing,buffer:int,discard_oversize:bool,get_length:Optional[Callable[[ItemT],int]]=None)->BatcherT
spacy.training.batchers.configure_minibatch_by_words(*,size:Sizing,tolerance:float,discard_oversize:bool,get_length:Optional[Callable[[ItemT],int]]=None)->BatcherT
spacy.training.batchers.minibatch_by_padded_size(seqs:Iterable[ItemT],size:Sizing,buffer:int=256,discard_oversize:bool=False,get_length:Callable=len)->Iterable[List[ItemT]]
spacy.training.batchers.minibatch_by_words(seqs:Iterable[ItemT],size:Sizing,tolerance=0.2,discard_oversize=False,get_length=len)->Iterable[List[ItemT]]
spacy.training.minibatch_by_padded_size(seqs:Iterable[ItemT],size:Sizing,buffer:int=256,discard_oversize:bool=False,get_length:Callable=len)->Iterable[List[ItemT]]
spacy.training.minibatch_by_words(seqs:Iterable[ItemT],size:Sizing,tolerance=0.2,discard_oversize=False,get_length=len)->Iterable[List[ItemT]]


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/training/iob_utils.py----------------------------------------
A:spacy.training.iob_utils.tags->list(tags)
A:spacy.training.iob_utils.tag->list(tags).pop(0)
A:spacy.training.iob_utils.ents->doc_to_biluo_tags(doc, missing='-')
A:spacy.training.iob_utils.start_token->starts.get(start_char)
A:spacy.training.iob_utils.end_token->ends.get(end_char)
A:spacy.training.iob_utils.entity_chars->set()
A:spacy.training.iob_utils.ent_str->str(entities)
A:spacy.training.iob_utils.token_offsets->tags_to_entities(tags)
A:spacy.training.iob_utils.span->Span(doc, start_idx, end_idx + 1, label=label)
A:spacy.training.iob_utils.spans->biluo_tags_to_spans(doc, tags)
spacy.training.biluo_tags_to_offsets(doc:Doc,tags:Iterable[str])->List[Tuple[int, int, Union[str, int]]]
spacy.training.biluo_tags_to_spans(doc:Doc,tags:Iterable[str])->List[Span]
spacy.training.biluo_to_iob(tags:Iterable[str])->List[str]
spacy.training.iob_to_biluo(tags:Iterable[str])->List[str]
spacy.training.iob_utils._consume_ent(tags:List[str])->List[str]
spacy.training.iob_utils._consume_os(tags:List[str])->Iterator[str]
spacy.training.iob_utils._doc_to_biluo_tags_with_partial(doc:Doc)->List[str]
spacy.training.iob_utils.biluo_tags_to_offsets(doc:Doc,tags:Iterable[str])->List[Tuple[int, int, Union[str, int]]]
spacy.training.iob_utils.biluo_tags_to_spans(doc:Doc,tags:Iterable[str])->List[Span]
spacy.training.iob_utils.biluo_to_iob(tags:Iterable[str])->List[str]
spacy.training.iob_utils.doc_to_biluo_tags(doc:Doc,missing:str='O')
spacy.training.iob_utils.iob_to_biluo(tags:Iterable[str])->List[str]
spacy.training.iob_utils.offsets_to_biluo_tags(doc:Doc,entities:Iterable[Tuple[int,int,Union[str,int]]],missing:str='O')->List[str]
spacy.training.iob_utils.remove_bilu_prefix(label:str)->str
spacy.training.iob_utils.split_bilu_label(label:str)->Tuple[str, str]
spacy.training.iob_utils.tags_to_entities(tags:Iterable[str])->List[Tuple[str, int, int]]
spacy.training.offsets_to_biluo_tags(doc:Doc,entities:Iterable[Tuple[int,int,Union[str,int]]],missing:str='O')->List[str]
spacy.training.remove_bilu_prefix(label:str)->str
spacy.training.split_bilu_label(label:str)->Tuple[str, str]
spacy.training.tags_to_entities(tags:Iterable[str])->List[Tuple[str, int, int]]


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/training/loggers.py----------------------------------------
A:spacy.training.loggers.output_file->util.ensure_path(output_file)
A:spacy.training.loggers.msg->Printer(no_print=True)
A:spacy.training.loggers.output_stream->open(output_file, 'w', encoding='utf-8')
A:spacy.training.loggers.(table_header, table_widths, table_aligns)->setup_table(cols=['E', '#'] + loss_cols + score_cols + ['Score'], widths=[3, 6] + [8 for _ in loss_cols] + [6 for _ in score_cols] + [6])
A:spacy.training.loggers.log_losses[pipe_name]->float(info['losses'][pipe_name])
A:spacy.training.loggers.score->float(score)
A:spacy.training.loggers.err->errors.Errors.E916.format(name=col, score_type=type(score))
A:spacy.training.loggers.progress->tqdm.tqdm(total=total, disable=None, leave=False, file=stderr, initial=initial)
spacy.training.console_logger(progress_bar:bool=False,console_output:bool=True,output_file:Optional[Union[str,Path]]=None)
spacy.training.loggers.console_logger(progress_bar:bool=False,console_output:bool=True,output_file:Optional[Union[str,Path]]=None)
spacy.training.loggers.console_logger_v3(progress_bar:Optional[str]=None,console_output:bool=True,output_file:Optional[Union[str,Path]]=None)
spacy.training.loggers.setup_table(*,cols:List[str],widths:List[int],max_width:int=13)->Tuple[List[str], List[int], List[str]]


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/training/loop.py----------------------------------------
A:spacy.training.loop.msg->Printer(no_print=True)
A:spacy.training.loop.config->nlp.config.interpolate()
A:spacy.training.loop.T->util.registry.resolve(config['training'], schema=ConfigSchemaTraining)
A:spacy.training.loop.(train_corpus, dev_corpus)->resolve_dot_names(config, dot_names)
A:spacy.training.loop.before_to_disk->create_before_to_disk_callback(T['before_to_disk'])
A:spacy.training.loop.training_step_iterator->train_while_improving(nlp, optimizer, create_train_batches(nlp, train_corpus, batcher, T['max_epochs']), create_evaluation_callback(nlp, dev_corpus, score_weights), dropout=T['dropout'], accumulate_gradient=T['accumulate_gradient'], patience=T['patience'], max_steps=T['max_steps'], eval_frequency=T['eval_frequency'], exclude=frozen_components, annotating_components=annotating_components, before_update=before_update)
A:spacy.training.loop.(log_step, finalize_logger)->train_logger(nlp, stdout, stderr)
A:spacy.training.loop.info['output_path']->str(output_path / DIR_MODEL_LAST)
A:spacy.training.loop.dropouts->constant(dropout)
A:spacy.training.loop.start_time->timer()
A:spacy.training.loop.dropout->next(dropouts)
A:spacy.training.loop.(score, other_scores)->evaluate()
A:spacy.training.loop.best_result->max(((r_score, -r_step) for (r_score, r_step) in results))
A:spacy.training.loop.batch->list(batch)
A:spacy.training.loop.scores->nlp.evaluate(dev_corpus(nlp))
A:spacy.training.loop.weighted_score->sum((scores.get(s, 0.0) * weights.get(s, 0.0) for s in weights))
A:spacy.training.loop.keys->list(scores.keys())
A:spacy.training.loop.err->errors.Errors.E914.format(name='before_to_disk', value=type(modified_nlp))
A:spacy.training.loop.examples->corpus(nlp)
A:spacy.training.loop.nlp.meta['performance'][metric]->info['other_scores'].get(metric, 0.0)
A:spacy.training.loop.modified_nlp->callback(nlp)
spacy.training.loop.clean_output_dir(path:Optional[Path])->None
spacy.training.loop.create_before_to_disk_callback(callback:Optional[Callable[['Language'],'Language']])->Callable[['Language'], 'Language']
spacy.training.loop.create_evaluation_callback(nlp:'Language',dev_corpus:Callable,weights:Dict[str,float])->Callable[[], Tuple[float, Dict[str, float]]]
spacy.training.loop.create_train_batches(nlp:'Language',corpus:Callable[['Language'],Iterable[Example]],batcher:Callable[[Iterable[Example]],Iterable[Example]],max_epochs:int)
spacy.training.loop.subdivide_batch(batch,accumulate_gradient)
spacy.training.loop.train(nlp:'Language',output_path:Optional[Path]=None,*,use_gpu:int=-1,stdout:IO=sys.stdout,stderr:IO=sys.stderr)->Tuple['Language', Optional[Path]]
spacy.training.loop.train_while_improving(nlp:'Language',optimizer:Optimizer,train_data,evaluate,*,dropout:float,eval_frequency:int,accumulate_gradient:int,patience:int,max_steps:int,exclude:List[str],annotating_components:List[str],before_update:Optional[Callable[['Language',Dict[str,Any]],None]])
spacy.training.loop.update_meta(training:Union[Dict[str,Any],Config],nlp:'Language',info:Dict[str,Any])->None


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/training/corpus.py----------------------------------------
A:spacy.training.corpus.path->util.ensure_path(path)
A:spacy.training.corpus.seen->set()
A:spacy.training.corpus.self.path->util.ensure_path(path)
A:spacy.training.corpus.ref_docs->list(ref_docs)
A:spacy.training.corpus.examples->self.make_examples(nlp, ref_docs)
A:spacy.training.corpus.eg->self._make_example(nlp, ref_sent, True)
A:spacy.training.corpus.loc->util.ensure_path(loc)
A:spacy.training.corpus.doc_bin->DocBin().from_disk(loc)
A:spacy.training.corpus.docs->DocBin().from_disk(loc).get_docs(vocab)
A:spacy.training.corpus.records->srsly.read_jsonl(loc)
A:spacy.training.corpus.doc->nlp.make_doc(record['text'])
spacy.training.Corpus(self,path:Union[str,Path],*,limit:int=0,gold_preproc:bool=False,max_length:int=0,augmenter:Optional[Callable]=None,shuffle:bool=False)
spacy.training.JsonlCorpus(self,path:Optional[Union[str,Path]],*,limit:int=0,min_length:int=0,max_length:int=0)
spacy.training.corpus.Corpus(self,path:Union[str,Path],*,limit:int=0,gold_preproc:bool=False,max_length:int=0,augmenter:Optional[Callable]=None,shuffle:bool=False)
spacy.training.corpus.Corpus._make_example(self,nlp:'Language',reference:Doc,gold_preproc:bool)->Example
spacy.training.corpus.Corpus.make_examples(self,nlp:'Language',reference_docs:Iterable[Doc])->Iterator[Example]
spacy.training.corpus.Corpus.make_examples_gold_preproc(self,nlp:'Language',reference_docs:Iterable[Doc])->Iterator[Example]
spacy.training.corpus.Corpus.read_docbin(self,vocab:Vocab,locs:Iterable[Union[str,Path]])->Iterator[Doc]
spacy.training.corpus.JsonlCorpus(self,path:Optional[Union[str,Path]],*,limit:int=0,min_length:int=0,max_length:int=0)
spacy.training.corpus.create_docbin_reader(path:Optional[Path],gold_preproc:bool,max_length:int=0,limit:int=0,augmenter:Optional[Callable]=None)->Callable[['Language'], Iterable[Example]]
spacy.training.corpus.create_jsonl_reader(path:Optional[Union[str,Path]],min_length:int=0,max_length:int=0,limit:int=0)->Callable[['Language'], Iterable[Example]]
spacy.training.corpus.read_labels(path:Path,*,require:bool=False)
spacy.training.corpus.walk_corpus(path:Union[str,Path],file_type)->List[Path]


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/training/callbacks.py----------------------------------------
A:spacy.training.callbacks.base_nlp->load_model(vocab)
spacy.training.callbacks.create_copy_from_base_model(tokenizer:Optional[str]=None,vocab:Optional[str]=None)->Callable[[Language], Language]
spacy.training.create_copy_from_base_model(tokenizer:Optional[str]=None,vocab:Optional[str]=None)->Callable[[Language], Language]


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/training/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/training/alignment.py----------------------------------------
A:spacy.training.alignment.x2y->AlignmentArray(x2y)
A:spacy.training.alignment.y2x->AlignmentArray(y2x)
A:spacy.training.alignment.(x2y, y2x)->get_alignments(A, B)
spacy.training.Alignment
spacy.training.alignment.Alignment
spacy.training.alignment.Alignment.from_indices(cls,x2y:List[List[int]],y2x:List[List[int]])->'Alignment'
spacy.training.alignment.Alignment.from_strings(cls,A:List[str],B:List[str])->'Alignment'


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/training/augment.py----------------------------------------
A:spacy.training.augment.example->make_whitespace_variant(nlp, example, random.choice(whitespace_variants), random.randrange(0, len(example.reference)))
A:spacy.training.augment.orig_dict->make_whitespace_variant(nlp, example, random.choice(whitespace_variants), random.randrange(0, len(example.reference))).to_dict()
A:spacy.training.augment.orig_dict['doc_annotation']['entities']->_doc_to_biluo_tags_with_partial(example.reference)
A:spacy.training.augment.(variant_text, variant_token_annot)->make_orth_variants(nlp, raw_text, orig_dict['token_annotation'], orth_variants, lower=raw_text is not None and random.random() < lower)
A:spacy.training.augment.example_dict->make_whitespace_variant(nlp, example, random.choice(whitespace_variants), random.randrange(0, len(example.reference))).to_dict()
A:spacy.training.augment.example_dict['doc_annotation']['entities']->_doc_to_biluo_tags_with_partial(example.reference)
A:spacy.training.augment.doc->nlp.make_doc(example.text.lower())
A:spacy.training.augment.words->example_dict.get('token_annotation', {}).get('ORTH', [])
A:spacy.training.augment.tags->example_dict.get('token_annotation', {}).get('TAG', [])
A:spacy.training.augment.raw->construct_modified_raw_text(token_dict)
A:spacy.training.augment.ndsv->orth_variants.get('single', [])
A:spacy.training.augment.ndpv->orth_variants.get('paired', [])
A:spacy.training.augment.pair_idx->pair.index(words[word_idx])
A:spacy.training.augment.doc_dict->make_whitespace_variant(nlp, example, random.choice(whitespace_variants), random.randrange(0, len(example.reference))).to_dict().get('doc_annotation', {})
A:spacy.training.augment.token_dict->make_whitespace_variant(nlp, example, random.choice(whitespace_variants), random.randrange(0, len(example.reference))).to_dict().get('token_annotation', {})
A:spacy.training.augment.length->len(words)
A:spacy.training.augment.(ent_iob_prev, ent_type_prev)->split_bilu_label(ent_prev)
A:spacy.training.augment.(ent_iob_next, ent_type_next)->split_bilu_label(ent_next)
spacy.training.augment.combined_augmenter(nlp:'Language',example:Example,*,lower_level:float=0.0,orth_level:float=0.0,orth_variants:Optional[Dict[str,List[Dict]]]=None,whitespace_level:float=0.0,whitespace_per_token:float=0.0,whitespace_variants:Optional[List[str]]=None)->Iterator[Example]
spacy.training.augment.construct_modified_raw_text(token_dict)
spacy.training.augment.create_combined_augmenter(lower_level:float,orth_level:float,orth_variants:Optional[Dict[str,List[Dict]]],whitespace_level:float,whitespace_per_token:float,whitespace_variants:Optional[List[str]])->Callable[['Language', Example], Iterator[Example]]
spacy.training.augment.create_lower_casing_augmenter(level:float)->Callable[['Language', Example], Iterator[Example]]
spacy.training.augment.create_orth_variants_augmenter(level:float,lower:float,orth_variants:Dict[str,List[Dict]])->Callable[['Language', Example], Iterator[Example]]
spacy.training.augment.dont_augment(nlp:'Language',example:Example)->Iterator[Example]
spacy.training.augment.lower_casing_augmenter(nlp:'Language',example:Example,*,level:float)->Iterator[Example]
spacy.training.augment.make_lowercase_variant(nlp:'Language',example:Example)
spacy.training.augment.make_orth_variants(nlp:'Language',raw:str,token_dict:Dict[str,List[str]],orth_variants:Dict[str,List[Dict[str,List[str]]]],*,lower:bool=False)->Tuple[str, Dict[str, List[str]]]
spacy.training.augment.make_whitespace_variant(nlp:'Language',example:Example,whitespace:str,position:int)->Example
spacy.training.augment.orth_variants_augmenter(nlp:'Language',example:Example,orth_variants:Dict[str,List[Dict]],*,level:float=0.0,lower:float=0.0)->Iterator[Example]
spacy.training.dont_augment(nlp:'Language',example:Example)->Iterator[Example]
spacy.training.orth_variants_augmenter(nlp:'Language',example:Example,orth_variants:Dict[str,List[Dict]],*,level:float=0.0,lower:float=0.0)->Iterator[Example]


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/training/converters/conll_ner_to_docs.py----------------------------------------
A:spacy.training.converters.conll_ner_to_docs.msg->Printer(no_print=no_print)
A:spacy.training.converters.conll_ner_to_docs.input_data->segment_sents_and_docs(input_data, n_sents, doc_delimiter, model=model, msg=msg)
A:spacy.training.converters.conll_ner_to_docs.nlp->get_lang_class('xx')()
A:spacy.training.converters.conll_ner_to_docs.conll_doc->conll_doc.strip().strip()
A:spacy.training.converters.conll_ner_to_docs.conll_sent->conll_sent.strip().strip()
A:spacy.training.converters.conll_ner_to_docs.cols->list(zip(*[line.split() for line in lines]))
A:spacy.training.converters.conll_ner_to_docs.length->len(cols[0])
A:spacy.training.converters.conll_ner_to_docs.doc->Doc(nlp.vocab, words=words)
A:spacy.training.converters.conll_ner_to_docs.entities->tags_to_entities(biluo_tags)
A:spacy.training.converters.conll_ner_to_docs.sentencizer->get_lang_class('xx')().create_pipe('sentencizer')
A:spacy.training.converters.conll_ner_to_docs.lines->Doc(nlp.vocab, words=words).strip().split('\n')
A:spacy.training.converters.conll_ner_to_docs.nlpdoc->Doc(nlp.vocab, words=words)
A:spacy.training.converters.conll_ner_to_docs.sents->segment_sents_and_docs(input_data, n_sents, doc_delimiter, model=model, msg=msg).split(sent_delimiter)
spacy.training.converters.conll_ner_to_docs(input_data,n_sents=10,seg_sents=False,model=None,no_print=False,**kwargs)
spacy.training.converters.conll_ner_to_docs.conll_ner_to_docs(input_data,n_sents=10,seg_sents=False,model=None,no_print=False,**kwargs)
spacy.training.converters.conll_ner_to_docs.n_sents_info(msg,n_sents)
spacy.training.converters.conll_ner_to_docs.segment_docs(input_data,n_sents,doc_delimiter)
spacy.training.converters.conll_ner_to_docs.segment_sents_and_docs(doc,n_sents,doc_delimiter,model=None,msg=None)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/training/converters/conllu_to_docs.py----------------------------------------
A:spacy.training.converters.conllu_to_docs.msg->Printer(no_print=no_print)
A:spacy.training.converters.conllu_to_docs.sent_docs->read_conllx(input_data, append_morphology=append_morphology, ner_tag_pattern=MISC_NER_PATTERN, ner_map=ner_map, merge_subtokens=merge_subtokens)
A:spacy.training.converters.conllu_to_docs.lines->sent.strip().split('\n')
A:spacy.training.converters.conllu_to_docs.parts->line.split('\t')
A:spacy.training.converters.conllu_to_docs.vocab->Vocab()
A:spacy.training.converters.conllu_to_docs.set_ents->has_ner(input_data, ner_tag_pattern)
A:spacy.training.converters.conllu_to_docs.doc->merge_conllu_subtokens(lines, doc)
A:spacy.training.converters.conllu_to_docs.tag_match->re.match(tag_pattern, misc_part)
A:spacy.training.converters.conllu_to_docs.prefix->re.match(tag_pattern, misc_part).group(2)
A:spacy.training.converters.conllu_to_docs.suffix->ner_map.get(suffix, suffix)
A:spacy.training.converters.conllu_to_docs.(subtok_start, subtok_end)->id_.split('-')
A:spacy.training.converters.conllu_to_docs.ents->get_entities(lines, ner_tag_pattern, ner_map)
A:spacy.training.converters.conllu_to_docs.doc.ents->biluo_tags_to_spans(doc, ents)
A:spacy.training.converters.conllu_to_docs.doc_x->Doc(vocab, words=words, spaces=spaces, tags=tags, morphs=morphs, lemmas=lemmas, pos=poses, deps=deps, heads=heads)
A:spacy.training.converters.conllu_to_docs.(field, values)->feature.split('=', 1)
A:spacy.training.converters.conllu_to_docs.morphs[field]->set()
A:spacy.training.converters.conllu_to_docs.token._.merged_lemma->' '.join(lemmas)
A:spacy.training.converters.conllu_to_docs.token.tag_->'_'.join(tags)
A:spacy.training.converters.conllu_to_docs.token._.merged_morph->'|'.join(sorted(morphs.values()))
spacy.training.converters.conllu_to_docs(input_data,n_sents=10,append_morphology=False,ner_map=None,merge_subtokens=False,no_print=False,**_)
spacy.training.converters.conllu_to_docs.conllu_sentence_to_doc(vocab,lines,ner_tag_pattern,merge_subtokens=False,append_morphology=False,ner_map=None,set_ents=False)
spacy.training.converters.conllu_to_docs.conllu_to_docs(input_data,n_sents=10,append_morphology=False,ner_map=None,merge_subtokens=False,no_print=False,**_)
spacy.training.converters.conllu_to_docs.get_entities(lines,tag_pattern,ner_map=None)
spacy.training.converters.conllu_to_docs.has_ner(input_data,ner_tag_pattern)
spacy.training.converters.conllu_to_docs.merge_conllu_subtokens(lines,doc)
spacy.training.converters.conllu_to_docs.read_conllx(input_data,append_morphology=False,merge_subtokens=False,ner_tag_pattern='',ner_map=None)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/training/converters/iob_to_docs.py----------------------------------------
A:spacy.training.converters.iob_to_docs.vocab->Vocab()
A:spacy.training.converters.iob_to_docs.msg->Printer(no_print=no_print)
A:spacy.training.converters.iob_to_docs.(sent_words, sent_tags, sent_iob)->zip(*sent_tokens)
A:spacy.training.converters.iob_to_docs.(sent_words, sent_iob)->zip(*sent_tokens)
A:spacy.training.converters.iob_to_docs.doc->Doc(vocab, words=words)
A:spacy.training.converters.iob_to_docs.biluo->iob_to_biluo(iob)
A:spacy.training.converters.iob_to_docs.entities->tags_to_entities(biluo)
spacy.training.converters.iob_to_docs(input_data,n_sents=10,no_print=False,*args,**kwargs)
spacy.training.converters.iob_to_docs.iob_to_docs(input_data,n_sents=10,no_print=False,*args,**kwargs)
spacy.training.converters.iob_to_docs.read_iob(raw_sents,vocab,n_sents)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/training/converters/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/training/converters/json_to_docs.py----------------------------------------
A:spacy.training.converters.json_to_docs.input_data->input_data.encode('utf8').encode('utf8')
A:spacy.training.converters.json_to_docs.example_dict->_fix_legacy_dict_data(json_para)
A:spacy.training.converters.json_to_docs.(tok_dict, doc_dict)->_parse_example_dict_data(example_dict)
A:spacy.training.converters.json_to_docs.doc->annotations_to_doc(nlp.vocab, tok_dict, doc_dict)
spacy.training.converters.json_to_docs(input_data,model=None,**kwargs)
spacy.training.converters.json_to_docs.json_to_docs(input_data,model=None,**kwargs)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/ml/extract_spans.py----------------------------------------
A:spacy.ml.extract_spans.indices->_get_span_indices(ops, spans, X.lengths)
A:spacy.ml.extract_spans.Y->Ragged(ops.xp.zeros(X.dataXd.shape, dtype=X.dataXd.dtype), ops.xp.zeros((len(X.lengths),), dtype='i'))
A:spacy.ml.extract_spans.dX->Ragged(ops.alloc2f(*x_shape), x_lengths)
A:spacy.ml.extract_spans.(spans, lengths)->_ensure_cpu(spans, lengths)
spacy.ml.extract_spans._ensure_cpu(spans:Ragged,lengths:Ints1d)->Tuple[Ragged, Ints1d]
spacy.ml.extract_spans._get_span_indices(ops,spans:Ragged,lengths:Ints1d)->Ints1d
spacy.ml.extract_spans.extract_spans()->Model[Tuple[Ragged, Ragged], Ragged]
spacy.ml.extract_spans.forward(model:Model,source_spans:Tuple[Ragged,Ragged],is_train:bool)->Tuple[Ragged, Callable]
spacy.ml.extract_spans.init(model,X=None,Y=None)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/ml/tb_framework.py----------------------------------------
A:spacy.ml.tb_framework.upper->noop()
A:spacy.ml.tb_framework.step_model->ParserStepModel(X, model.layers, unseen_classes=model.attrs['unseen_classes'], train=is_train, has_upper=model.attrs['has_upper'])
A:spacy.ml.tb_framework.lower->model.get_ref('lower')
A:spacy.ml.tb_framework.statevecs->model.ops.alloc2f(2, lower.get_dim('nO'))
spacy.ml.tb_framework.TransitionModel(tok2vec,lower,upper,resize_output,dropout=0.2,unseen_classes=set())
spacy.ml.tb_framework.forward(model,X,is_train)
spacy.ml.tb_framework.init(model,X=None,Y=None)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/ml/staticvectors.py----------------------------------------
A:spacy.ml.staticvectors.token_count->sum((len(doc) for doc in docs))
A:spacy.ml.staticvectors.keys->model.ops.flatten([cast(Ints1d, doc.to_array(key_attr)) for doc in docs])
A:spacy.ml.staticvectors.W->cast(Floats2d, model.ops.as_contig(model.get_param('W')))
A:spacy.ml.staticvectors.V->model.ops.as_contig(V)
A:spacy.ml.staticvectors.rows->vocab.vectors.find(keys=keys)
A:spacy.ml.staticvectors.vectors_data->model.ops.gemm(V, W, trans2=True)
A:spacy.ml.staticvectors.output->Ragged(vectors_data, model.ops.asarray1i([len(doc) for doc in docs]))
A:spacy.ml.staticvectors.mask->ops.get_dropout_mask((nO,), rate)
spacy.ml.staticvectors.StaticVectors(nO:Optional[int]=None,nM:Optional[int]=None,*,dropout:Optional[float]=None,init_W:Callable=glorot_uniform_init,key_attr:str='ORTH')->Model[List[Doc], Ragged]
spacy.ml.staticvectors._get_drop_mask(ops:Ops,nO:int,rate:Optional[float])->Optional[Floats1d]
spacy.ml.staticvectors._handle_empty(ops:Ops,nO:int)
spacy.ml.staticvectors.forward(model:Model[List[Doc],Ragged],docs:List[Doc],is_train:bool)->Tuple[Ragged, Callable]
spacy.ml.staticvectors.init(init_W:Callable,model:Model[List[Doc],Ragged],X:Optional[List[Doc]]=None,Y:Optional[Ragged]=None)->Model[List[Doc], Ragged]


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/ml/_precomputable_affine.py----------------------------------------
A:spacy.ml._precomputable_affine.model->Model('precomputable_affine', forward, init=init, dims={'nO': nO, 'nI': nI, 'nF': nF, 'nP': nP}, params={'W': None, 'b': None, 'pad': None}, attrs={'dropout_rate': dropout})
A:spacy.ml._precomputable_affine.nF->Model('precomputable_affine', forward, init=init, dims={'nO': nO, 'nI': nI, 'nF': nF, 'nP': nP}, params={'W': None, 'b': None, 'pad': None}, attrs={'dropout_rate': dropout}).get_dim('nF')
A:spacy.ml._precomputable_affine.nO->Model('precomputable_affine', forward, init=init, dims={'nO': nO, 'nI': nI, 'nF': nF, 'nP': nP}, params={'W': None, 'b': None, 'pad': None}, attrs={'dropout_rate': dropout}).get_dim('nO')
A:spacy.ml._precomputable_affine.nP->Model('precomputable_affine', forward, init=init, dims={'nO': nO, 'nI': nI, 'nF': nF, 'nP': nP}, params={'W': None, 'b': None, 'pad': None}, attrs={'dropout_rate': dropout}).get_dim('nP')
A:spacy.ml._precomputable_affine.nI->Model('precomputable_affine', forward, init=init, dims={'nO': nO, 'nI': nI, 'nF': nF, 'nP': nP}, params={'W': None, 'b': None, 'pad': None}, attrs={'dropout_rate': dropout}).get_dim('nI')
A:spacy.ml._precomputable_affine.W->Model('precomputable_affine', forward, init=init, dims={'nO': nO, 'nI': nI, 'nF': nF, 'nP': nP}, params={'W': None, 'b': None, 'pad': None}, attrs={'dropout_rate': dropout}).get_param('W').copy()
A:spacy.ml._precomputable_affine.Yf->Yf.reshape((Yf.shape[0], nF, nO, nP)).reshape((Yf.shape[0], nF, nO, nP))
A:spacy.ml._precomputable_affine.Yf[0]->Model('precomputable_affine', forward, init=init, dims={'nO': nO, 'nI': nI, 'nF': nF, 'nP': nP}, params={'W': None, 'b': None, 'pad': None}, attrs={'dropout_rate': dropout}).ops.xp.squeeze(model.get_param('pad'), 0)
A:spacy.ml._precomputable_affine.Xf->Xf.reshape((Xf.shape[0], nF * nI)).reshape((Xf.shape[0], nF * nI))
A:spacy.ml._precomputable_affine.dY->dY.reshape((dY.shape[0], nO * nP)).reshape((dY.shape[0], nO * nP))
A:spacy.ml._precomputable_affine.Wopfi->Wopfi.reshape((nO * nP, nF * nI)).reshape((nO * nP, nF * nI))
A:spacy.ml._precomputable_affine.dXf->Model('precomputable_affine', forward, init=init, dims={'nO': nO, 'nI': nI, 'nF': nF, 'nP': nP}, params={'W': None, 'b': None, 'pad': None}, attrs={'dropout_rate': dropout}).ops.gemm(dY.reshape((dY.shape[0], nO * nP)), Wopfi)
A:spacy.ml._precomputable_affine.dWopfi->dWopfi.transpose((2, 0, 1, 3)).transpose((2, 0, 1, 3))
A:spacy.ml._precomputable_affine.mask->Model('precomputable_affine', forward, init=init, dims={'nO': nO, 'nI': nI, 'nF': nF, 'nP': nP}, params={'W': None, 'b': None, 'pad': None}, attrs={'dropout_rate': dropout}).ops.asarray(ids < 0, dtype='f')
A:spacy.ml._precomputable_affine.d_pad->Model('precomputable_affine', forward, init=init, dims={'nO': nO, 'nI': nI, 'nF': nF, 'nP': nP}, params={'W': None, 'b': None, 'pad': None}, attrs={'dropout_rate': dropout}).ops.gemm(mask, dY.reshape(nB, nO * nP), trans1=True)
A:spacy.ml._precomputable_affine.b->Model('precomputable_affine', forward, init=init, dims={'nO': nO, 'nI': nI, 'nF': nF, 'nP': nP}, params={'W': None, 'b': None, 'pad': None}, attrs={'dropout_rate': dropout}).get_param('b').copy()
A:spacy.ml._precomputable_affine.pad->normal_init(ops, pad.shape, mean=1.0)
A:spacy.ml._precomputable_affine.ids->ops.asarray(ids, dtype='i')
A:spacy.ml._precomputable_affine.tokvecs->ops.alloc((5000, nI), dtype='f')
A:spacy.ml._precomputable_affine.hiddens->hiddens.reshape((hiddens.shape[0] * nF, nO * nP)).reshape((hiddens.shape[0] * nF, nO * nP))
A:spacy.ml._precomputable_affine.vectors->Model('precomputable_affine', forward, init=init, dims={'nO': nO, 'nI': nI, 'nF': nF, 'nP': nP}, params={'W': None, 'b': None, 'pad': None}, attrs={'dropout_rate': dropout}).ops.asarray(vectors)
A:spacy.ml._precomputable_affine.acts1->predict(ids, tokvecs)
A:spacy.ml._precomputable_affine.var->Model('precomputable_affine', forward, init=init, dims={'nO': nO, 'nI': nI, 'nF': nF, 'nP': nP}, params={'W': None, 'b': None, 'pad': None}, attrs={'dropout_rate': dropout}).ops.xp.var(acts1)
A:spacy.ml._precomputable_affine.mean->Model('precomputable_affine', forward, init=init, dims={'nO': nO, 'nI': nI, 'nF': nF, 'nP': nP}, params={'W': None, 'b': None, 'pad': None}, attrs={'dropout_rate': dropout}).ops.xp.mean(acts1)
spacy.ml._precomputable_affine.PrecomputableAffine(nO,nI,nF,nP,dropout=0.1)
spacy.ml._precomputable_affine._backprop_precomputable_affine_padding(model,dY,ids)
spacy.ml._precomputable_affine.forward(model,X,is_train)
spacy.ml._precomputable_affine.init(model,X=None,Y=None)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/ml/extract_ngrams.py----------------------------------------
A:spacy.ml.extract_ngrams.unigrams->model.ops.asarray(doc.to_array([model.attrs['attr']]))
A:spacy.ml.extract_ngrams.keys->model.ops.xp.concatenate(ngrams)
A:spacy.ml.extract_ngrams.(keys, vals)->model.ops.xp.unique(keys, return_counts=True)
A:spacy.ml.extract_ngrams.lengths->model.ops.asarray([arr.shape[0] for arr in batch_keys], dtype='int32')
A:spacy.ml.extract_ngrams.batch_keys->model.ops.xp.concatenate(batch_keys)
A:spacy.ml.extract_ngrams.batch_vals->model.ops.asarray(model.ops.xp.concatenate(batch_vals), dtype='f')
spacy.ml.extract_ngrams.extract_ngrams(ngram_size:int,attr:int=LOWER)->Model
spacy.ml.extract_ngrams.forward(model:Model,docs,is_train:bool)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/ml/featureextractor.py----------------------------------------
A:spacy.ml.featureextractor.attrs->attrs.reshape((attrs.shape[0], 1)).reshape((attrs.shape[0], 1))
spacy.ml.featureextractor.FeatureExtractor(columns:List[Union[int,str]])->Model[List[Doc], List[Ints2d]]
spacy.ml.featureextractor.forward(model:Model[List[Doc],List[Ints2d]],docs,is_train:bool)->Tuple[List[Ints2d], Callable]


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/ml/_character_embed.py----------------------------------------
A:spacy.ml._character_embed.vectors_table->model.ops.alloc3f(model.get_dim('nC'), model.get_dim('nV'), model.get_dim('nM'))
A:spacy.ml._character_embed.E->model.get_param('E')
A:spacy.ml._character_embed.nC->model.get_dim('nC')
A:spacy.ml._character_embed.nM->model.get_dim('nM')
A:spacy.ml._character_embed.nO->model.get_dim('nO')
A:spacy.ml._character_embed.nCv->model.ops.xp.arange(nC)
A:spacy.ml._character_embed.doc_ids->model.ops.asarray(doc.to_utf8_array(nr_char=nC))
A:spacy.ml._character_embed.doc_vectors->model.ops.alloc3f(len(doc), nC, nM)
A:spacy.ml._character_embed.dE->model.ops.alloc(E.shape, dtype=E.dtype)
A:spacy.ml._character_embed.d_doc_vectors->d_doc_vectors.reshape((len(doc_ids), nC, nM)).reshape((len(doc_ids), nC, nM))
spacy.ml._character_embed.CharacterEmbed(nM:int,nC:int)->Model[List[Doc], List[Floats2d]]
spacy.ml._character_embed.forward(model:Model,docs:List[Doc],is_train:bool)
spacy.ml._character_embed.init(model:Model,X=None,Y=None)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/ml/callbacks.py----------------------------------------
A:spacy.ml.callbacks.extra_funcs->additional_pipe_functions.get(pipe.name, [])
A:spacy.ml.callbacks.func->getattr(pipe, name, None)
A:spacy.ml.callbacks.wrapped_func->functools.partial(types.MethodType(nvtx_range_wrapper_for_pipe_method, pipe), func)
A:spacy.ml.callbacks.wrapped_func.__signature__->inspect.signature(func)
A:spacy.ml.callbacks.nlp->pipes_with_nvtx_range(nlp, additional_pipe_functions)
spacy.ml.callbacks.create_models_and_pipes_with_nvtx_range(forward_color:int=-1,backprop_color:int=-1,additional_pipe_functions:Optional[Dict[str,List[str]]]=None)->Callable[['Language'], 'Language']
spacy.ml.callbacks.create_models_with_nvtx_range(forward_color:int=-1,backprop_color:int=-1)->Callable[['Language'], 'Language']
spacy.ml.callbacks.models_with_nvtx_range(nlp,forward_color:int,backprop_color:int)
spacy.ml.callbacks.nvtx_range_wrapper_for_pipe_method(self,func,*args,**kwargs)
spacy.ml.callbacks.pipes_with_nvtx_range(nlp,additional_pipe_functions:Optional[Dict[str,List[str]]])
spacy.ml.create_models_with_nvtx_range(forward_color:int=-1,backprop_color:int=-1)->Callable[['Language'], 'Language']


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/ml/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/ml/models/tok2vec.py----------------------------------------
A:spacy.ml.models.tok2vec.tok2vec->chain(embed, encode)
A:spacy.ml.models.tok2vec.nO->chain(embed, encode).get_ref('listener').get_dim('nO')
A:spacy.ml.models.tok2vec.model->clone(residual(cnn), depth)
A:spacy.ml.models.tok2vec.feature->intify_attr(feature)
A:spacy.ml.models.tok2vec.char_embed->chain(_character_embed.CharacterEmbed(nM=nM, nC=nC), cast(Model[List[Floats2d], Ragged], list2ragged()))
A:spacy.ml.models.tok2vec.max_out->with_array(Maxout(width, nM * nC + width, nP=3, normalize=True, dropout=0.0))
A:spacy.ml.models.tok2vec.cnn->chain(expand_window(window_size=window_size), Mish(nO=width, nI=width * (window_size * 2 + 1), dropout=0.0, normalize=True))
spacy.ml.BiLSTMEncoder(width:int,depth:int,dropout:float)->Model[List[Floats2d], List[Floats2d]]
spacy.ml.CharacterEmbed(width:int,rows:int,nM:int,nC:int,include_static_vectors:bool,feature:Union[int,str]='LOWER')->Model[List[Doc], List[Floats2d]]
spacy.ml.MaxoutWindowEncoder(width:int,window_size:int,maxout_pieces:int,depth:int)->Model[List[Floats2d], List[Floats2d]]
spacy.ml.MishWindowEncoder(width:int,window_size:int,depth:int)->Model[List[Floats2d], List[Floats2d]]
spacy.ml.MultiHashEmbed(width:int,attrs:List[Union[str,int]],rows:List[int],include_static_vectors:bool)->Model[List[Doc], List[Floats2d]]
spacy.ml.build_Tok2Vec_model(embed:Model[List[Doc],List[Floats2d]],encode:Model[List[Floats2d],List[Floats2d]])->Model[List[Doc], List[Floats2d]]
spacy.ml.build_hash_embed_cnn_tok2vec(*,width:int,depth:int,embed_size:int,window_size:int,maxout_pieces:int,subword_features:bool,pretrained_vectors:Optional[bool])->Model[List[Doc], List[Floats2d]]
spacy.ml.get_tok2vec_width(model:Model)
spacy.ml.models.tok2vec.BiLSTMEncoder(width:int,depth:int,dropout:float)->Model[List[Floats2d], List[Floats2d]]
spacy.ml.models.tok2vec.CharacterEmbed(width:int,rows:int,nM:int,nC:int,include_static_vectors:bool,feature:Union[int,str]='LOWER')->Model[List[Doc], List[Floats2d]]
spacy.ml.models.tok2vec.MaxoutWindowEncoder(width:int,window_size:int,maxout_pieces:int,depth:int)->Model[List[Floats2d], List[Floats2d]]
spacy.ml.models.tok2vec.MishWindowEncoder(width:int,window_size:int,depth:int)->Model[List[Floats2d], List[Floats2d]]
spacy.ml.models.tok2vec.MultiHashEmbed(width:int,attrs:List[Union[str,int]],rows:List[int],include_static_vectors:bool)->Model[List[Doc], List[Floats2d]]
spacy.ml.models.tok2vec.build_Tok2Vec_model(embed:Model[List[Doc],List[Floats2d]],encode:Model[List[Floats2d],List[Floats2d]])->Model[List[Doc], List[Floats2d]]
spacy.ml.models.tok2vec.build_hash_embed_cnn_tok2vec(*,width:int,depth:int,embed_size:int,window_size:int,maxout_pieces:int,subword_features:bool,pretrained_vectors:Optional[bool])->Model[List[Doc], List[Floats2d]]
spacy.ml.models.tok2vec.get_tok2vec_width(model:Model)
spacy.ml.models.tok2vec.tok2vec_listener_v1(width:int,upstream:str='*')
spacy.ml.tok2vec_listener_v1(width:int,upstream:str='*')


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/ml/models/parser.py----------------------------------------
A:spacy.ml.models.parser.tok2vec->chain(tok2vec, list2array(), Linear(hidden_width, t2v_width))
A:spacy.ml.models.parser.lower->model.get_ref('lower')
A:spacy.ml.models.parser.upper->model.get_ref('upper')
A:spacy.ml.models.parser.nI->smaller.maybe_get_dim('nI')
A:spacy.ml.models.parser.larger->_define_lower(nO=new_nO, nI=nI, nF=nF, nP=nP)
A:spacy.ml.models.parser.larger_W->_define_lower(nO=new_nO, nI=nI, nF=nF, nP=nP).ops.alloc4f(nF, new_nO, nP, nI)
A:spacy.ml.models.parser.larger_b->_define_lower(nO=new_nO, nI=nI, nF=nF, nP=nP).ops.alloc2f(new_nO, nP)
A:spacy.ml.models.parser.smaller_W->smaller.get_param('W')
A:spacy.ml.models.parser.smaller_b->smaller.get_param('b')
A:spacy.ml.models.parser.old_nO->smaller.get_dim('nO')
A:spacy.ml.models.parser.nF->smaller.maybe_get_dim('nF')
A:spacy.ml.models.parser.nP->smaller.maybe_get_dim('nP')
A:spacy.ml.models.parser.larger_pad->_define_lower(nO=new_nO, nI=nI, nF=nF, nP=nP).ops.alloc4f(1, nF, new_nO, nP)
A:spacy.ml.models.parser.smaller_pad->smaller.get_param('pad')
spacy.ml._define_lower(nO,nF,nI,nP)
spacy.ml._define_upper(nO,nI)
spacy.ml._resize_lower(model,new_nO)
spacy.ml._resize_upper(model,new_nO)
spacy.ml.build_tb_parser_model(tok2vec:Model[List[Doc],List[Floats2d]],state_type:Literal['parser','ner'],extra_state_tokens:bool,hidden_width:int,maxout_pieces:int,use_upper:bool,nO:Optional[int]=None)->Model
spacy.ml.models.parser._define_lower(nO,nF,nI,nP)
spacy.ml.models.parser._define_upper(nO,nI)
spacy.ml.models.parser._resize_lower(model,new_nO)
spacy.ml.models.parser._resize_upper(model,new_nO)
spacy.ml.models.parser.build_tb_parser_model(tok2vec:Model[List[Doc],List[Floats2d]],state_type:Literal['parser','ner'],extra_state_tokens:bool,hidden_width:int,maxout_pieces:int,use_upper:bool,nO:Optional[int]=None)->Model
spacy.ml.models.parser.resize_output(model,new_nO)
spacy.ml.resize_output(model,new_nO)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/ml/models/tagger.py----------------------------------------
A:spacy.ml.models.tagger.output_layer->Softmax_v2(nO, t2v_width, init_W=zero_init, normalize_outputs=normalize)
A:spacy.ml.models.tagger.softmax->with_array(output_layer)
A:spacy.ml.models.tagger.model->chain(tok2vec, softmax)
spacy.ml.build_tagger_model(tok2vec:Model[List[Doc],List[Floats2d]],nO:Optional[int]=None,normalize=False)->Model[List[Doc], List[Floats2d]]
spacy.ml.models.tagger.build_tagger_model(tok2vec:Model[List[Doc],List[Floats2d]],nO:Optional[int]=None,normalize=False)->Model[List[Doc], List[Floats2d]]


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/ml/models/spancat.py----------------------------------------
A:spacy.ml.models.spancat.model->chain(cast(Model[Tuple[List[Doc], Ragged], Tuple[Ragged, Ragged]], with_getitem(0, chain(tok2vec, cast(Model[List[Floats2d], Ragged], list2ragged())))), extract_spans(), reducer, scorer)
spacy.ml.build_linear_logistic(nO=None,nI=None)->Model[Floats2d, Floats2d]
spacy.ml.build_mean_max_reducer(hidden_size:int)->Model[Ragged, Floats2d]
spacy.ml.build_spancat_model(tok2vec:Model[List[Doc],List[Floats2d]],reducer:Model[Ragged,Floats2d],scorer:Model[Floats2d,Floats2d])->Model[Tuple[List[Doc], Ragged], Floats2d]
spacy.ml.models.spancat.build_linear_logistic(nO=None,nI=None)->Model[Floats2d, Floats2d]
spacy.ml.models.spancat.build_mean_max_reducer(hidden_size:int)->Model[Ragged, Floats2d]
spacy.ml.models.spancat.build_spancat_model(tok2vec:Model[List[Doc],List[Floats2d]],reducer:Model[Ragged,Floats2d],scorer:Model[Floats2d,Floats2d])->Model[Tuple[List[Doc], Ragged], Floats2d]


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/ml/models/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/ml/models/multi_task.py----------------------------------------
A:spacy.ml.models.multi_task.model->build_masked_language_model(vocab, chain(tok2vec, output_layer))
A:spacy.ml.models.multi_task.model.attrs['loss']->partial(get_characters_loss, nr_char=n_characters)
A:spacy.ml.models.multi_task.distance->L2Distance(normalize=True)
A:spacy.ml.models.multi_task.ids->ops.flatten([doc.to_array(ID).ravel() for doc in docs])
A:spacy.ml.models.multi_task.(d_target, loss)->distance(prediction, target)
A:spacy.ml.models.multi_task.target_ids->target_ids.reshape((-1,)).reshape((-1,))
A:spacy.ml.models.multi_task.target->target.reshape((-1, 256 * nr_char)).reshape((-1, 256 * nr_char))
A:spacy.ml.models.multi_task.loss->(diff ** 2).sum()
A:spacy.ml.models.multi_task.softmax->Softmax(nO=nO, nI=token_vector_width * 2)
A:spacy.ml.models.multi_task.output_layer->chain(cast(Model[List['Floats2d'], Floats2d], list2array()), Maxout(nO=hidden_size, nP=maxout_pieces), LayerNorm(nI=hidden_size), MultiSoftmax([256] * nr_char, nI=hidden_size))
A:spacy.ml.models.multi_task.random_words->_RandomWords(vocab)
A:spacy.ml.models.multi_task.(mask, docs)->_apply_mask(docs, random_words, mask_prob=mask_prob)
A:spacy.ml.models.multi_task.mask->numpy.random.uniform(0.0, 1.0, (N,))
A:spacy.ml.models.multi_task.(output, backprop)->build_masked_language_model(vocab, chain(tok2vec, output_layer)).layers[0](docs, is_train)
A:spacy.ml.models.multi_task.index->self._cache.pop()
A:spacy.ml.models.multi_task.N->sum((len(doc) for doc in docs))
A:spacy.ml.models.multi_task.word->_replace_word(token.text, random_words)
A:spacy.ml.models.multi_task.roll->numpy.random.random()
spacy.ml._RandomWords(self,vocab:'Vocab')
spacy.ml._RandomWords.next(self)->str
spacy.ml._apply_mask(docs:Iterable['Doc'],random_words:_RandomWords,mask_prob:float=0.15)->Tuple[numpy.ndarray, List['Doc']]
spacy.ml._replace_word(word:str,random_words:_RandomWords,mask:str='[MASK]')->str
spacy.ml.build_cloze_characters_multi_task_model(vocab:'Vocab',tok2vec:Model,maxout_pieces:int,hidden_size:int,nr_char:int)->Model
spacy.ml.build_cloze_multi_task_model(vocab:'Vocab',tok2vec:Model,maxout_pieces:int,hidden_size:int)->Model
spacy.ml.build_masked_language_model(vocab:'Vocab',wrapped_model:Model,mask_prob:float=0.15)->Model
spacy.ml.build_multi_task_model(tok2vec:Model,maxout_pieces:int,token_vector_width:int,nO:Optional[int]=None)->Model
spacy.ml.create_pretrain_characters(maxout_pieces:int,hidden_size:int,n_characters:int)->Callable[['Vocab', Model], Model]
spacy.ml.create_pretrain_vectors(maxout_pieces:int,hidden_size:int,loss:str)->Callable[['Vocab', Model], Model]
spacy.ml.get_characters_loss(ops,docs,prediction,nr_char)
spacy.ml.get_vectors_loss(ops,docs,prediction,distance)
spacy.ml.models.multi_task._RandomWords(self,vocab:'Vocab')
spacy.ml.models.multi_task._RandomWords.next(self)->str
spacy.ml.models.multi_task._apply_mask(docs:Iterable['Doc'],random_words:_RandomWords,mask_prob:float=0.15)->Tuple[numpy.ndarray, List['Doc']]
spacy.ml.models.multi_task._replace_word(word:str,random_words:_RandomWords,mask:str='[MASK]')->str
spacy.ml.models.multi_task.build_cloze_characters_multi_task_model(vocab:'Vocab',tok2vec:Model,maxout_pieces:int,hidden_size:int,nr_char:int)->Model
spacy.ml.models.multi_task.build_cloze_multi_task_model(vocab:'Vocab',tok2vec:Model,maxout_pieces:int,hidden_size:int)->Model
spacy.ml.models.multi_task.build_masked_language_model(vocab:'Vocab',wrapped_model:Model,mask_prob:float=0.15)->Model
spacy.ml.models.multi_task.build_multi_task_model(tok2vec:Model,maxout_pieces:int,token_vector_width:int,nO:Optional[int]=None)->Model
spacy.ml.models.multi_task.create_pretrain_characters(maxout_pieces:int,hidden_size:int,n_characters:int)->Callable[['Vocab', Model], Model]
spacy.ml.models.multi_task.create_pretrain_vectors(maxout_pieces:int,hidden_size:int,loss:str)->Callable[['Vocab', Model], Model]
spacy.ml.models.multi_task.get_characters_loss(ops,docs,prediction,nr_char)
spacy.ml.models.multi_task.get_vectors_loss(ops,docs,prediction,distance)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/ml/models/entity_linker.py----------------------------------------
A:spacy.ml.models.entity_linker.token_width->tok2vec.maybe_get_dim('nO')
A:spacy.ml.models.entity_linker.output_layer->Linear(nO=nO, nI=token_width)
A:spacy.ml.models.entity_linker.sent_index->sentences.index(ent.sent)
A:spacy.ml.models.entity_linker.start_sentence->max(0, sent_index - n_sents)
A:spacy.ml.models.entity_linker.end_sentence->min(len(sentences) - 1, sent_index + n_sents)
A:spacy.ml.models.entity_linker.lengths->model.ops.asarray1i([len(cands) for cands in candidates])
A:spacy.ml.models.entity_linker.out->Ragged(model.ops.flatten(candidates), lengths)
A:spacy.ml.models.entity_linker.kb->InMemoryLookupKB(vocab, entity_vector_length=1)
spacy.ml.build_nel_encoder(tok2vec:Model,nO:Optional[int]=None)->Model[List[Doc], Floats2d]
spacy.ml.build_span_maker(n_sents:int=0)->Model
spacy.ml.create_candidates()->Callable[[KnowledgeBase, Span], Iterable[Candidate]]
spacy.ml.create_candidates_batch()->Callable[[KnowledgeBase, Iterable[Span]], Iterable[Iterable[Candidate]]]
spacy.ml.empty_kb(entity_vector_length:int)->Callable[[Vocab], KnowledgeBase]
spacy.ml.load_kb(kb_path:Path)->Callable[[Vocab], KnowledgeBase]
spacy.ml.models.entity_linker.build_nel_encoder(tok2vec:Model,nO:Optional[int]=None)->Model[List[Doc], Floats2d]
spacy.ml.models.entity_linker.build_span_maker(n_sents:int=0)->Model
spacy.ml.models.entity_linker.create_candidates()->Callable[[KnowledgeBase, Span], Iterable[Candidate]]
spacy.ml.models.entity_linker.create_candidates_batch()->Callable[[KnowledgeBase, Iterable[Span]], Iterable[Iterable[Candidate]]]
spacy.ml.models.entity_linker.empty_kb(entity_vector_length:int)->Callable[[Vocab], KnowledgeBase]
spacy.ml.models.entity_linker.load_kb(kb_path:Path)->Callable[[Vocab], KnowledgeBase]
spacy.ml.models.entity_linker.span_maker_forward(model,docs:List[Doc],is_train)->Tuple[Ragged, Callable]
spacy.ml.span_maker_forward(model,docs:List[Doc],is_train)->Tuple[Ragged, Callable]


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/ml/models/textcat.py----------------------------------------
A:spacy.ml.models.textcat.nI->tok2vec.maybe_get_dim('nO')
A:spacy.ml.models.textcat.output_layer->Softmax(nO=nO, nI=nO_double)
A:spacy.ml.models.textcat.resizable_layer->resize_model(resizable_layer, new_nO)
A:spacy.ml.models.textcat.model.attrs['resize_output']->partial(resize_and_set_ref, resizable_layer=resizable_layer)
A:spacy.ml.models.textcat.sparse_linear->SparseLinear(nO=nO)
A:spacy.ml.models.textcat.model->with_cpu(model, model.ops)
A:spacy.ml.models.textcat.width->tok2vec.maybe_get_dim('nO')
A:spacy.ml.models.textcat.attention_layer->ParametricAttention(width)
A:spacy.ml.models.textcat.maxout_layer->Maxout(nO=width, nI=width)
A:spacy.ml.models.textcat.norm_layer->LayerNorm(nI=width)
A:spacy.ml.models.textcat.tok2vec_width->get_tok2vec_width(model)
spacy.ml.build_bow_text_classifier(exclusive_classes:bool,ngram_size:int,no_output_layer:bool,nO:Optional[int]=None)->Model[List[Doc], Floats2d]
spacy.ml.build_simple_cnn_text_classifier(tok2vec:Model,exclusive_classes:bool,nO:Optional[int]=None)->Model[List[Doc], Floats2d]
spacy.ml.build_text_classifier_lowdata(width:int,dropout:Optional[float],nO:Optional[int]=None)->Model[List[Doc], Floats2d]
spacy.ml.build_text_classifier_v2(tok2vec:Model[List[Doc],List[Floats2d]],linear_model:Model[List[Doc],Floats2d],nO:Optional[int]=None)->Model[List[Doc], Floats2d]
spacy.ml.init_ensemble_textcat(model,X,Y)->Model
spacy.ml.models.textcat.build_bow_text_classifier(exclusive_classes:bool,ngram_size:int,no_output_layer:bool,nO:Optional[int]=None)->Model[List[Doc], Floats2d]
spacy.ml.models.textcat.build_simple_cnn_text_classifier(tok2vec:Model,exclusive_classes:bool,nO:Optional[int]=None)->Model[List[Doc], Floats2d]
spacy.ml.models.textcat.build_text_classifier_lowdata(width:int,dropout:Optional[float],nO:Optional[int]=None)->Model[List[Doc], Floats2d]
spacy.ml.models.textcat.build_text_classifier_v2(tok2vec:Model[List[Doc],List[Floats2d]],linear_model:Model[List[Doc],Floats2d],nO:Optional[int]=None)->Model[List[Doc], Floats2d]
spacy.ml.models.textcat.init_ensemble_textcat(model,X,Y)->Model
spacy.ml.models.textcat.resize_and_set_ref(model,new_nO,resizable_layer)
spacy.ml.resize_and_set_ref(model,new_nO,resizable_layer)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/matcher/phrasematcher.pyi----------------------------------------
spacy.matcher.PhraseMatcher(self,vocab:Vocab,attr:Optional[Union[int,str]],validate:bool=...)
spacy.matcher.phrasematcher.PhraseMatcher(self,vocab:Vocab,attr:Optional[Union[int,str]],validate:bool=...)
spacy.matcher.phrasematcher.PhraseMatcher.__contains__(self,key:str)->bool
spacy.matcher.phrasematcher.PhraseMatcher.__len__(self)->int
spacy.matcher.phrasematcher.PhraseMatcher.__reduce__(self)->Any
spacy.matcher.phrasematcher.PhraseMatcher.add(self,key:str,docs:List[Doc],*,on_match:Optional[Callable[[Matcher,Doc,int,List[Tuple[Any,...]]],Any]]=...)->None
spacy.matcher.phrasematcher.PhraseMatcher.remove(self,key:str)->None


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/matcher/dependencymatcher.pyi----------------------------------------
spacy.matcher.DependencyMatcher(self,vocab:Vocab,*,validate:bool=...)
spacy.matcher.dependencymatcher.DependencyMatcher(self,vocab:Vocab,*,validate:bool=...)
spacy.matcher.dependencymatcher.DependencyMatcher.__contains__(self,key:Union[str,int])->bool
spacy.matcher.dependencymatcher.DependencyMatcher.__len__(self)->int
spacy.matcher.dependencymatcher.DependencyMatcher.__reduce__(self)->Tuple[Callable[[Vocab, Dict[str, Any], Dict[str, Callable[..., Any]]], DependencyMatcher], Tuple[Vocab, Dict[str, List[Any]], Dict[str, Callable[[DependencyMatcher, Doc, int, List[Tuple[int, List[int]]]], Any]]], None, None]
spacy.matcher.dependencymatcher.DependencyMatcher.add(self,key:Union[str,int],patterns:List[List[Dict[str,Any]]],*,on_match:Optional[Callable[[DependencyMatcher,Doc,int,List[Tuple[int,List[int]]]],Any]]=...)->None
spacy.matcher.dependencymatcher.DependencyMatcher.get(self,key:Union[str,int],default:Optional[Any]=...)->Tuple[Optional[Callable[[DependencyMatcher, Doc, int, List[Tuple[int, List[int]]]], Any]], List[List[Dict[str, Any]]]]
spacy.matcher.dependencymatcher.DependencyMatcher.has_key(self,key:Union[str,int])->bool
spacy.matcher.dependencymatcher.DependencyMatcher.remove(self,key:Union[str,int])->None
spacy.matcher.dependencymatcher.unpickle_matcher(vocab:Vocab,patterns:Dict[str,Any],callbacks:Dict[str,Callable[...,Any]])->DependencyMatcher


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/matcher/matcher.pyi----------------------------------------
spacy.matcher.Matcher(self,vocab:Vocab,validate:bool=...,fuzzy_compare:Callable[[str,str,int],bool]=...)
spacy.matcher.matcher.Matcher(self,vocab:Vocab,validate:bool=...,fuzzy_compare:Callable[[str,str,int],bool]=...)
spacy.matcher.matcher.Matcher.__contains__(self,key:str)->bool
spacy.matcher.matcher.Matcher.__len__(self)->int
spacy.matcher.matcher.Matcher.__reduce__(self)->Any
spacy.matcher.matcher.Matcher._normalize_key(self,key:Any)->Any
spacy.matcher.matcher.Matcher.add(self,key:Union[str,int],patterns:List[List[Dict[str,Any]]],*,on_match:Optional[Callable[[Matcher,Doc,int,List[Tuple[Any,...]]],Any]]=...,greedy:Optional[str]=...)->None
spacy.matcher.matcher.Matcher.get(self,key:Union[str,int],default:Optional[Any]=...)->Tuple[Optional[Callable[[Any], Any]], List[List[Dict[Any, Any]]]]
spacy.matcher.matcher.Matcher.has_key(self,key:Union[str,int])->bool
spacy.matcher.matcher.Matcher.pipe(self,docs:Iterable[Tuple[Doc,Any]],batch_size:int=...,return_matches:bool=...,as_tuples:bool=...)->Union[Iterator[Tuple[Tuple[Doc, Any], Any]], Iterator[Tuple[Doc, Any]], Iterator[Doc]]
spacy.matcher.matcher.Matcher.remove(self,key:str)->None


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/matcher/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/cli/pretrain.py----------------------------------------
A:spacy.cli.pretrain.config_overrides->parse_config_overrides(ctx.args)
A:spacy.cli.pretrain.raw_config->load_config(config_path, overrides=config_overrides, interpolate=False)
A:spacy.cli.pretrain.config->load_config(config_path, overrides=config_overrides, interpolate=False).interpolate()
A:spacy.cli.pretrain.model_name->re.search('model\\d+\\.bin', str(resume_path))
spacy.cli.pretrain.pretrain_cli(ctx:typer.Context,config_path:Path=Arg(...,help='Pathtoconfigfile',exists=True,dir_okay=False,allow_dash=True),output_dir:Path=Arg(...,help='Directorytowriteweightstooneachepoch'),code_path:Optional[Path]=Opt(None,'--code','-c',help='PathtoPythonfilewithadditionalcode(registeredfunctions)tobeimported'),resume_path:Optional[Path]=Opt(None,'--resume-path','-r',help='Pathtopretrainedweightsfromwhichtoresumepretraining'),epoch_resume:Optional[int]=Opt(None,'--epoch-resume','-er',help='Theepochtoresumecountingfromwhenusing--resume-path.Preventsunintendedoverwritingofexistingweightfiles.'),use_gpu:int=Opt(-1,'--gpu-id','-g',help='GPUIDor-1forCPU'))
spacy.cli.pretrain.verify_cli_args(config_path,output_dir,resume_path,epoch_resume)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/cli/find_threshold.py----------------------------------------
A:spacy.cli.find_threshold.data_path->util.ensure_path(data_path)
A:spacy.cli.find_threshold.nlp->util.load_model(model, config=set_nested_item(filter_config(nlp.config, config_keys_full, '.'.join(config_keys_full)).copy(), config_keys_full, threshold))
A:spacy.cli.find_threshold.pipe->util.load_model(model, config=set_nested_item(filter_config(nlp.config, config_keys_full, '.'.join(config_keys_full)).copy(), config_keys_full, threshold)).get_pipe(pipe_name)
A:spacy.cli.find_threshold.corpus->Corpus(data_path, gold_preproc=gold_preproc)
A:spacy.cli.find_threshold.dev_dataset->list(corpus(nlp))
A:spacy.cli.find_threshold.config_keys->threshold_key.split('.')
A:spacy.cli.find_threshold.thresholds->numpy.linspace(0, 1, n_trials)
A:spacy.cli.find_threshold.eval_scores->util.load_model(model, config=set_nested_item(filter_config(nlp.config, config_keys_full, '.'.join(config_keys_full)).copy(), config_keys_full, threshold)).evaluate(dev_dataset)
A:spacy.cli.find_threshold.best_threshold->max(scores.keys(), key=lambda key: scores[key])
spacy.cli.find_threshold(model:str,data_path:Path,pipe_name:str,threshold_key:str,scores_key:str,*,n_trials:int=_DEFAULTS['n_trials'],use_gpu:int=_DEFAULTS['use_gpu'],gold_preproc:bool=_DEFAULTS['gold_preproc'],silent:bool=True)->Tuple[float, float, Dict[float, float]]
spacy.cli.find_threshold.find_threshold(model:str,data_path:Path,pipe_name:str,threshold_key:str,scores_key:str,*,n_trials:int=_DEFAULTS['n_trials'],use_gpu:int=_DEFAULTS['use_gpu'],gold_preproc:bool=_DEFAULTS['gold_preproc'],silent:bool=True)->Tuple[float, float, Dict[float, float]]
spacy.cli.find_threshold.find_threshold_cli(model:str=Arg(...,help='Modelnameorpath'),data_path:Path=Arg(...,help='Locationofbinaryevaluationdatain.spacyformat',exists=True),pipe_name:str=Arg(...,help='Nameofpipetoexaminethresholdsfor'),threshold_key:str=Arg(...,help="Keyofthresholdattributeincomponent'sconfiguration"),scores_key:str=Arg(...,help='Metrictooptimize'),n_trials:int=Opt(_DEFAULTS['n_trials'],'--n_trials','-n',help='Numberoftrialstodetermineoptimalthresholds'),code_path:Optional[Path]=Opt(None,'--code','-c',help='PathtoPythonfilewithadditionalcode(registeredfunctions)tobeimported'),use_gpu:int=Opt(_DEFAULTS['use_gpu'],'--gpu-id','-g',help='GPUIDor-1forCPU'),gold_preproc:bool=Opt(_DEFAULTS['gold_preproc'],'--gold-preproc','-G',help='Usegoldpreprocessing'),verbose:bool=Opt(False,'--silent','-V','-VV',help='Displaymoreinformationfordebuggingpurposes'))


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/cli/init_pipeline.py----------------------------------------
A:spacy.cli.init_pipeline.nlp->init_nlp(config, use_gpu=use_gpu)
A:spacy.cli.init_pipeline.lex_attrs->srsly.read_jsonl(jsonl_loc)
A:spacy.cli.init_pipeline.overrides->parse_config_overrides(ctx.args)
A:spacy.cli.init_pipeline.config->util.load_config(config_path, overrides=overrides)
spacy.cli.init_pipeline._init_labels(nlp,output_path)
spacy.cli.init_pipeline.init_labels_cli(ctx:typer.Context,config_path:Path=Arg(...,help='Pathtoconfigfile',exists=True,allow_dash=True),output_path:Path=Arg(...,help='Outputdirectoryforthelabels'),code_path:Optional[Path]=Opt(None,'--code','-c',help='PathtoPythonfilewithadditionalcode(registeredfunctions)tobeimported'),verbose:bool=Opt(False,'--verbose','-V','-VV',help='Displaymoreinformationfordebuggingpurposes'),use_gpu:int=Opt(-1,'--gpu-id','-g',help='GPUIDor-1forCPU'))
spacy.cli.init_pipeline.init_pipeline_cli(ctx:typer.Context,config_path:Path=Arg(...,help='Pathtoconfigfile',exists=True,allow_dash=True),output_path:Path=Arg(...,help='Outputdirectoryfortheprepareddata'),code_path:Optional[Path]=Opt(None,'--code','-c',help='PathtoPythonfilewithadditionalcode(registeredfunctions)tobeimported'),verbose:bool=Opt(False,'--verbose','-V','-VV',help='Displaymoreinformationfordebuggingpurposes'),use_gpu:int=Opt(-1,'--gpu-id','-g',help='GPUIDor-1forCPU'))
spacy.cli.init_pipeline.init_vectors_cli(lang:str=Arg(...,help='Thelanguageofthenlpobjecttocreate'),vectors_loc:Path=Arg(...,help='VectorsfileinWord2Vecformat',exists=True),output_dir:Path=Arg(...,help='Pipelineoutputdirectory'),prune:int=Opt(-1,'--prune','-p',help='Optionalnumberofvectorstopruneto'),truncate:int=Opt(0,'--truncate','-t',help='Optionalnumberofvectorstotruncatetowhenreadinginvectorsfile'),mode:str=Opt('default','--mode','-m',help='Vectorsmode:defaultorfloret'),name:Optional[str]=Opt(None,'--name','-n',help='Optionalnameforthewordvectors,e.g.en_core_web_lg.vectors'),verbose:bool=Opt(False,'--verbose','-V','-VV',help='Displaymoreinformationfordebuggingpurposes'),jsonl_loc:Optional[Path]=Opt(None,'--lexemes-jsonl','-j',help='LocationofJSONL-formattedattributesfile',hidden=True))
spacy.cli.init_pipeline.update_lexemes(nlp:Language,jsonl_loc:Path)->None
spacy.cli.init_pipeline_cli(ctx:typer.Context,config_path:Path=Arg(...,help='Pathtoconfigfile',exists=True,allow_dash=True),output_path:Path=Arg(...,help='Outputdirectoryfortheprepareddata'),code_path:Optional[Path]=Opt(None,'--code','-c',help='PathtoPythonfilewithadditionalcode(registeredfunctions)tobeimported'),verbose:bool=Opt(False,'--verbose','-V','-VV',help='Displaymoreinformationfordebuggingpurposes'),use_gpu:int=Opt(-1,'--gpu-id','-g',help='GPUIDor-1forCPU'))


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/cli/_util.py----------------------------------------
A:spacy.cli._util.app->typer.Typer(name=NAME, help=HELP)
A:spacy.cli._util.benchmark_cli->typer.Typer(name='benchmark', help=BENCHMARK_HELP, no_args_is_help=True)
A:spacy.cli._util.project_cli->typer.Typer(name='project', help=PROJECT_HELP, no_args_is_help=True)
A:spacy.cli._util.debug_cli->typer.Typer(name='debug', help=DEBUG_HELP, no_args_is_help=True)
A:spacy.cli._util.init_cli->typer.Typer(name='init', help=INIT_HELP, no_args_is_help=True)
A:spacy.cli._util.command->get_command(app)
A:spacy.cli._util.env_overrides->_parse_overrides(split_arg_string(env_string))
A:spacy.cli._util.cli_overrides->_parse_overrides(args, is_cli=True)
A:spacy.cli._util.opt->opt.replace('-', '_').replace('-', '_')
A:spacy.cli._util.(opt, value)->opt.replace('-', '_').replace('-', '_').split('=', 1)
A:spacy.cli._util.value->args.pop(0)
A:spacy.cli._util.result[opt]->_parse_override(value)
A:spacy.cli._util.config->substitute_project_variables(config, overrides)
A:spacy.cli._util.errors->validate(ProjectConfigSchema, config)
A:spacy.cli._util.config[env_key][config_var]->_parse_override(os.environ.get(env_var, ''))
A:spacy.cli._util.cfg->Config().from_str(cfg.to_str(), overrides=overrides)
A:spacy.cli._util.interpolated->Config().from_str(cfg.to_str(), overrides=overrides).interpolate()
A:spacy.cli._util.spacy_version->substitute_project_variables(config, overrides).get('spacy_version', None)
A:spacy.cli._util.workflows->substitute_project_variables(config, overrides).get('workflows', {})
A:spacy.cli._util.duplicates->set([cmd for cmd in command_names if command_names.count(cmd) > 1])
A:spacy.cli._util.data_str->srsly.json_dumps(data, sort_keys=True).encode('utf8')
A:spacy.cli._util.path->Path(path)
A:spacy.cli._util.dir_checksum->hashlib.md5()
A:spacy.cli._util.err->e.from_error(e, title='', desc=desc, show_config=show_config)
A:spacy.cli._util.dest->str(dest)
A:spacy.cli._util.src->str(src)
A:spacy.cli._util.git_version->get_git_version()
A:spacy.cli._util.ret->run_command('git --version', capture=True)
A:spacy.cli._util.git_repo->_http_to_git(repo)
A:spacy.cli._util.missings->' '.join([x[1:] for x in ret.stdout.split() if x.startswith('?')])
A:spacy.cli._util.stdout->run_command('git --version', capture=True).stdout.strip()
A:spacy.cli._util.version->stdout[11:].strip().split('.')
A:spacy.cli._util.repo->repo.replace('https://', 'git@').replace('/', ':', 1).replace('https://', 'git@').replace('/', ':', 1)
A:spacy.cli._util.parent_realpath->os.path.realpath(parent)
A:spacy.cli._util.child_realpath->os.path.realpath(child)
A:spacy.cli._util.p->int(p)
A:spacy.cli._util.local_msg->Printer(no_print=silent, pretty=not silent)
A:spacy.cli._util.seen->set()
spacy.cli._util._format_number(number:Union[int,float],ndigits:int=2)->str
spacy.cli._util._http_to_git(repo:str)->str
spacy.cli._util._parse_override(value:Any)->Any
spacy.cli._util._parse_overrides(args:List[str],is_cli:bool=False)->Dict[str, Any]
spacy.cli._util.download_file(src:Union[str,'FluidPath'],dest:Path,*,force:bool=False)->None
spacy.cli._util.ensure_pathy(path)
spacy.cli._util.get_checksum(path:Union[Path,str])->str
spacy.cli._util.get_git_version(error:str="Couldnotrun'git'.Makesureit'sinstalledandtheexecutableisavailable.")->Tuple[int, int]
spacy.cli._util.get_hash(data,exclude:Iterable[str]=tuple())->str
spacy.cli._util.git_checkout(repo:str,subpath:str,dest:Path,*,branch:str='master',sparse:bool=False)
spacy.cli._util.git_repo_branch_exists(repo:str,branch:str)->bool
spacy.cli._util.git_sparse_checkout(repo,subpath,dest,branch)
spacy.cli._util.import_code(code_path:Optional[Union[Path,str]])->None
spacy.cli._util.is_subpath_of(parent,child)
spacy.cli._util.load_project_config(path:Path,interpolate:bool=True,overrides:Dict[str,Any]=SimpleFrozenDict())->Dict[str, Any]
spacy.cli._util.parse_config_overrides(args:List[str],env_var:Optional[str]=ENV_VARS.CONFIG_OVERRIDES)->Dict[str, Any]
spacy.cli._util.setup_cli()->None
spacy.cli._util.setup_gpu(use_gpu:int,silent=None)->None
spacy.cli._util.show_validation_error(file_path:Optional[Union[str,Path]]=None,*,title:Optional[str]=None,desc:str='',show_config:Optional[bool]=None,hint_fill:bool=True)
spacy.cli._util.string_to_list(value:str,intify:bool=False)->Union[List[str], List[int]]
spacy.cli._util.substitute_project_variables(config:Dict[str,Any],overrides:Dict[str,Any]=SimpleFrozenDict(),key:str='vars',env_key:str='env')->Dict[str, Any]
spacy.cli._util.upload_file(src:Path,dest:Union[str,'FluidPath'])->None
spacy.cli._util.validate_project_commands(config:Dict[str,Any])->None
spacy.cli._util.validate_project_version(config:Dict[str,Any])->None
spacy.cli._util.walk_directory(path:Path,suffix:Optional[str]=None)->List[Path]
spacy.cli.setup_cli()->None


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/cli/package.py----------------------------------------
A:spacy.cli.package.(create_sdist, create_wheel)->get_build_formats(string_to_list(build))
A:spacy.cli.package.msg->Printer()
A:spacy.cli.package.input_path->util.ensure_path(input_dir)
A:spacy.cli.package.output_path->util.ensure_path(output_dir)
A:spacy.cli.package.meta_path->util.ensure_path(meta_path)
A:spacy.cli.package.meta->generate_meta(meta, msg)
A:spacy.cli.package.errors->validate(ModelMetaSchema, meta)
A:spacy.cli.package.readme->generate_readme(meta)
A:spacy.cli.package.init_py->"\nfrom pathlib import Path\nfrom spacy.util import load_model_from_init_py, get_model_meta\n\n{imports}\n\n__version__ = get_model_meta(Path(__file__).parent)['version']\n\n\ndef load(**overrides):\n    return load_model_from_init_py(__file__, **overrides)\n".lstrip().format(imports='\n'.join((f'from . import {m}' for m in imports)))
A:spacy.cli.package.wheel_name_squashed->re.sub('_+', '_', model_name_v)
A:spacy.cli.package.distributions->util.packages_distributions()
A:spacy.cli.package.funcs->defaultdict(set)
A:spacy.cli.package.modules->set()
A:spacy.cli.package.func_info->util.registry.find(reg_name, func_name)
A:spacy.cli.package.module_name->util.registry.find(reg_name, func_name).get('module')
A:spacy.cli.package.dist->util.packages_distributions().get(module_name)
A:spacy.cli.package.version->util.get_package_version(pkg)
A:spacy.cli.package.version_range->util.get_minor_version_range(version)
A:spacy.cli.package.nlp->util.load_model_from_path(Path(model_path))
A:spacy.cli.package.meta['spacy_version']->util.get_minor_version_range(about.__version__)
A:spacy.cli.package.reqs->get_third_party_dependencies(nlp.config, exclude=existing_reqs)
A:spacy.cli.package.response->get_raw_input(desc, default)
A:spacy.cli.package.md->MarkdownRenderer()
A:spacy.cli.package.pipeline->', '.join([md.code(p) for p in meta.get('pipeline', [])])
A:spacy.cli.package.components->', '.join([md.code(p) for p in meta.get('components', [])])
A:spacy.cli.package.vecs->generate_meta(meta, msg).get('vectors', {})
A:spacy.cli.package.notes->generate_meta(meta, msg).get('notes', '')
A:spacy.cli.package.license_name->generate_meta(meta, msg).get('license')
A:spacy.cli.package.sources->_format_sources(meta.get('sources'))
A:spacy.cli.package.description->generate_meta(meta, msg).get('description')
A:spacy.cli.package.label_scheme->_format_label_scheme(cast(Dict[str, Any], meta.get('labels')))
A:spacy.cli.package.accuracy->_format_accuracy(cast(Dict[str, Any], meta.get('performance')))
A:spacy.cli.package.name->source.get('name')
A:spacy.cli.package.url->source.get('url')
A:spacy.cli.package.author->source.get('author')
A:spacy.cli.package.col1->MarkdownRenderer().bold(md.code(pipe))
A:spacy.cli.package.col2->', '.join([md.code(str(label).replace('|', '\\|')) for label in labels])
A:spacy.cli.package.permitted_match->re.search('^([A-Z0-9]|[A-Z0-9][A-Z0-9._-]*[A-Z0-9])$', package_name, re.IGNORECASE)
A:spacy.cli.package.TEMPLATE_SETUP->'\n#!/usr/bin/env python\nimport io\nimport json\nfrom os import path, walk\nfrom shutil import copy\nfrom setuptools import setup\n\n\ndef load_meta(fp):\n    with io.open(fp, encoding=\'utf8\') as f:\n        return json.load(f)\n\n\ndef load_readme(fp):\n    if path.exists(fp):\n        with io.open(fp, encoding=\'utf8\') as f:\n            return f.read()\n    return ""\n\n\ndef list_files(data_dir):\n    output = []\n    for root, _, filenames in walk(data_dir):\n        for filename in filenames:\n            if not filename.startswith(\'.\'):\n                output.append(path.join(root, filename))\n    output = [path.relpath(p, path.dirname(data_dir)) for p in output]\n    output.append(\'meta.json\')\n    return output\n\n\ndef list_requirements(meta):\n    parent_package = meta.get(\'parent_package\', \'spacy\')\n    requirements = [parent_package + meta[\'spacy_version\']]\n    if \'setup_requires\' in meta:\n        requirements += meta[\'setup_requires\']\n    if \'requirements\' in meta:\n        requirements += meta[\'requirements\']\n    return requirements\n\n\ndef setup_package():\n    root = path.abspath(path.dirname(__file__))\n    meta_path = path.join(root, \'meta.json\')\n    meta = load_meta(meta_path)\n    readme_path = path.join(root, \'README.md\')\n    readme = load_readme(readme_path)\n    model_name = str(meta[\'lang\'] + \'_\' + meta[\'name\'])\n    model_dir = path.join(model_name, model_name + \'-\' + meta[\'version\'])\n\n    copy(meta_path, path.join(model_name))\n    copy(meta_path, model_dir)\n\n    setup(\n        name=model_name,\n        description=meta.get(\'description\'),\n        long_description=readme,\n        author=meta.get(\'author\'),\n        author_email=meta.get(\'email\'),\n        url=meta.get(\'url\'),\n        version=meta[\'version\'],\n        license=meta.get(\'license\'),\n        packages=[model_name],\n        package_data={model_name: list_files(model_dir)},\n        install_requires=list_requirements(meta),\n        zip_safe=False,\n        entry_points={\'spacy_models\': [\'{m} = {m}\'.format(m=model_name)]}\n    )\n\n\nif __name__ == \'__main__\':\n    setup_package()\n'.lstrip()
A:spacy.cli.package.TEMPLATE_MANIFEST->'\ninclude meta.json\ninclude LICENSE\ninclude LICENSES_SOURCES\ninclude README.md\n'.strip()
A:spacy.cli.package.TEMPLATE_INIT->"\nfrom pathlib import Path\nfrom spacy.util import load_model_from_init_py, get_model_meta\n\n{imports}\n\n__version__ = get_model_meta(Path(__file__).parent)['version']\n\n\ndef load(**overrides):\n    return load_model_from_init_py(__file__, **overrides)\n".lstrip()
spacy.cli.package(input_dir:Path,output_dir:Path,meta_path:Optional[Path]=None,code_paths:List[Path]=[],name:Optional[str]=None,version:Optional[str]=None,create_meta:bool=False,create_sdist:bool=True,create_wheel:bool=False,force:bool=False,silent:bool=True)->None
spacy.cli.package._format_accuracy(data:Dict[str,Any],exclude:List[str]=['speed'])->str
spacy.cli.package._format_label_scheme(data:Dict[str,Any])->str
spacy.cli.package._format_sources(data:Any)->str
spacy.cli.package._is_permitted_package_name(package_name:str)->bool
spacy.cli.package.create_file(file_path:Path,contents:str)->None
spacy.cli.package.generate_meta(existing_meta:Dict[str,Any],msg:Printer)->Dict[str, Any]
spacy.cli.package.generate_readme(meta:Dict[str,Any])->str
spacy.cli.package.get_build_formats(formats:List[str])->Tuple[bool, bool]
spacy.cli.package.get_meta(model_path:Union[str,Path],existing_meta:Dict[str,Any])->Dict[str, Any]
spacy.cli.package.get_third_party_dependencies(config:Config,exclude:List[str]=util.SimpleFrozenList())->List[str]
spacy.cli.package.has_wheel()->bool
spacy.cli.package.package(input_dir:Path,output_dir:Path,meta_path:Optional[Path]=None,code_paths:List[Path]=[],name:Optional[str]=None,version:Optional[str]=None,create_meta:bool=False,create_sdist:bool=True,create_wheel:bool=False,force:bool=False,silent:bool=True)->None
spacy.cli.package.package_cli(input_dir:Path=Arg(...,help='Directorywithpipelinedata',exists=True,file_okay=False),output_dir:Path=Arg(...,help='Outputparentdirectory',exists=True,file_okay=False),code_paths:str=Opt('','--code','-c',help='Comma-separatedpathstoPythonfilewithadditionalcode(registeredfunctions)tobeincludedinthepackage'),meta_path:Optional[Path]=Opt(None,'--meta-path','--meta','-m',help='Pathtometa.json',exists=True,dir_okay=False),create_meta:bool=Opt(False,'--create-meta','-C',help='Createmeta.json,evenifoneexists'),name:Optional[str]=Opt(None,'--name','-n',help='Packagenametooverridemeta'),version:Optional[str]=Opt(None,'--version','-v',help='Packageversiontooverridemeta'),build:str=Opt('sdist','--build','-b',help='Comma-separatedformatstobuild:sdistand/orwheel,ornone.'),force:bool=Opt(False,'--force','-f','-F',help='Forceoverwritingexistingdatainoutputdirectory'))


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/cli/init_config.py----------------------------------------
A:spacy.cli.init_config.RECOMMENDATIONS->srsly.read_yaml(ROOT / 'quickstart_training_recommendations.yml')
A:spacy.cli.init_config.pipeline->string_to_list(pipeline)
A:spacy.cli.init_config.msg->Printer(no_print=no_print)
A:spacy.cli.init_config.config->util.load_config(DEFAULT_CONFIG_PRETRAIN_PATH).merge(config)
A:spacy.cli.init_config.nlp->util.load_model_from_config(config, auto_fill=True)
A:spacy.cli.init_config.sourced->util.get_sourced_components(config)
A:spacy.cli.init_config.pretrain_config->util.load_config(DEFAULT_CONFIG_PRETRAIN_PATH)
A:spacy.cli.init_config.filled->util.load_config(DEFAULT_CONFIG_PRETRAIN_PATH).merge(filled)
A:spacy.cli.init_config.before->util.load_config(DEFAULT_CONFIG_PRETRAIN_PATH).merge(config).to_str()
A:spacy.cli.init_config.after->util.load_config(DEFAULT_CONFIG_PRETRAIN_PATH).merge(filled).to_str()
A:spacy.cli.init_config.template->Template(f.read())
A:spacy.cli.init_config.reco->RecommendationSchema(**RECOMMENDATIONS.get(lang, defaults)).dict()
A:spacy.cli.init_config.base_template->re.sub('\\n\\n\\n+', '\n\n', base_template)
A:spacy.cli.init_config.template_vars->Template(f.read()).make_module(variables)
spacy.cli.fill_config(output_file:Path,base_path:Path,*,pretraining:bool=False,diff:bool=False,silent:bool=False)->Tuple[Config, Config]
spacy.cli.init_config(*,lang:str=InitValues.lang,pipeline:List[str]=InitValues.pipeline,optimize:str=InitValues.optimize,gpu:bool=InitValues.gpu,pretraining:bool=InitValues.pretraining,silent:bool=True)->Config
spacy.cli.init_config.InitValues
spacy.cli.init_config.Optimizations(str,Enum)
spacy.cli.init_config.fill_config(output_file:Path,base_path:Path,*,pretraining:bool=False,diff:bool=False,silent:bool=False)->Tuple[Config, Config]
spacy.cli.init_config.has_spacy_transformers()->bool
spacy.cli.init_config.init_config(*,lang:str=InitValues.lang,pipeline:List[str]=InitValues.pipeline,optimize:str=InitValues.optimize,gpu:bool=InitValues.gpu,pretraining:bool=InitValues.pretraining,silent:bool=True)->Config
spacy.cli.init_config.init_config_cli(output_file:Path=Arg(...,help='Filetosavetheconfigtoor-forstdout(willonlyoutputconfigandnoadditionallogginginfo)',allow_dash=True),lang:str=Opt(InitValues.lang,'--lang','-l',help='Two-lettercodeofthelanguagetouse'),pipeline:str=Opt(','.join(InitValues.pipeline),'--pipeline','-p',help="Comma-separatednamesoftrainablepipelinecomponentstoinclude(without'tok2vec'or'transformer')"),optimize:Optimizations=Opt(InitValues.optimize,'--optimize','-o',help='Whethertooptimizeforefficiency(fasterinference,smallermodel,lowermemoryconsumption)orhigheraccuracy(potentiallylargerandslowermodel).Thiswillimpactthechoiceofarchitecture,pretrainedweightsandrelatedhyperparameters.'),gpu:bool=Opt(InitValues.gpu,'--gpu','-G',help='WhetherthemodelcanrunonGPU.Thiswillimpactthechoiceofarchitecture,pretrainedweightsandrelatedhyperparameters.'),pretraining:bool=Opt(InitValues.pretraining,'--pretraining','-pt',help="Includeconfigforpretraining(with'spacypretrain')"),force_overwrite:bool=Opt(InitValues.force_overwrite,'--force','-F',help='Forceoverwritingtheoutputfile'))
spacy.cli.init_config.init_fill_config_cli(base_path:Path=Arg(...,help='Pathtobaseconfigtofill',exists=True,dir_okay=False),output_file:Path=Arg('-',help='Pathtooutput.cfgfile(or-forstdout)',allow_dash=True),pretraining:bool=Opt(False,'--pretraining','-pt',help="Includeconfigforpretraining(with'spacypretrain')"),diff:bool=Opt(False,'--diff','-D',help='Printavisualdiffhighlightingthechanges'),code_path:Optional[Path]=Opt(None,'--code-path','--code','-c',help='PathtoPythonfilewithadditionalcode(registeredfunctions)tobeimported'))
spacy.cli.init_config.save_config(config:Config,output_file:Path,is_stdout:bool=False,silent:bool=False)->None
spacy.cli.init_config.validate_config_for_pretrain(config:Config,msg:Printer)->None


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/cli/debug_config.py----------------------------------------
A:spacy.cli.debug_config.overrides->parse_config_overrides(ctx.args)
A:spacy.cli.debug_config.config->util.load_model_from_config(config).config.interpolate()
A:spacy.cli.debug_config.nlp->util.load_model_from_config(config)
A:spacy.cli.debug_config.T->getattr(util.registry, reg_name).resolve(config['training'], schema=ConfigSchemaTraining)
A:spacy.cli.debug_config.variables->get_variables(config)
A:spacy.cli.debug_config.funcs->get_registered_funcs(config)
A:spacy.cli.debug_config.registry->getattr(util.registry, reg_name)
A:spacy.cli.debug_config.path->variable[2:-1].replace(':', '.')
A:spacy.cli.debug_config.info->getattr(util.registry, reg_name).find(value)
A:spacy.cli.debug_config.value->util.dot_to_object(config, path)
A:spacy.cli.debug_config.result[variable]->repr(value)
spacy.cli.debug_config(config_path:Path,*,overrides:Dict[str,Any]={},show_funcs:bool=False,show_vars:bool=False)
spacy.cli.debug_config.debug_config(config_path:Path,*,overrides:Dict[str,Any]={},show_funcs:bool=False,show_vars:bool=False)
spacy.cli.debug_config.debug_config_cli(ctx:typer.Context,config_path:Path=Arg(...,help='Pathtoconfigfile',exists=True,allow_dash=True),code_path:Optional[Path]=Opt(None,'--code-path','--code','-c',help='PathtoPythonfilewithadditionalcode(registeredfunctions)tobeimported'),show_funcs:bool=Opt(False,'--show-functions','-F',help='Showanoverviewofallregisteredfunctionsusedintheconfigandwheretheycomefrom(modules,filesetc.)'),show_vars:bool=Opt(False,'--show-variables','-V',help='Showanoverviewofallvariablesreferencedintheconfigandtheirvalues.ThiswillalsoreflectvariablesoverwrittenontheCLI.'))
spacy.cli.debug_config.get_registered_funcs(config:Config)->List[Dict[str, Optional[Union[str, int]]]]
spacy.cli.debug_config.get_variables(config:Config)->Dict[str, Any]


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/cli/debug_diff.py----------------------------------------
A:spacy.cli.debug_diff.msg->Printer()
A:spacy.cli.debug_diff.user_config->load_config(config_path)
A:spacy.cli.debug_diff.other_config->init_config(lang=lang, pipeline=pipeline, optimize=optimize.value, gpu=gpu, pretraining=pretraining, silent=True)
A:spacy.cli.debug_diff.pipeline->list(user_config['nlp']['pipeline'])
A:spacy.cli.debug_diff.user->load_config(config_path).to_str()
A:spacy.cli.debug_diff.other->init_config(lang=lang, pipeline=pipeline, optimize=optimize.value, gpu=gpu, pretraining=pretraining, silent=True).to_str()
A:spacy.cli.debug_diff.diff_text->diff_strings(other, user, add_symbols=markdown)
A:spacy.cli.debug_diff.md->MarkdownRenderer()
spacy.cli.debug_diff(config_path:Path,compare_to:Optional[Path],gpu:bool,optimize:Optimizations,pretraining:bool,markdown:bool)
spacy.cli.debug_diff.debug_diff(config_path:Path,compare_to:Optional[Path],gpu:bool,optimize:Optimizations,pretraining:bool,markdown:bool)
spacy.cli.debug_diff.debug_diff_cli(ctx:typer.Context,config_path:Path=Arg(...,help='Pathtoconfigfile',exists=True,allow_dash=True),compare_to:Optional[Path]=Opt(None,help='Pathtoaconfigfiletodiffagainst,or`None`tocompareagainstdefaultsettings',exists=True,allow_dash=True),optimize:Optimizations=Opt(Optimizations.efficiency.value,'--optimize','-o',help='Whethertheuserconfigwasoptimizedforefficiencyoraccuracy.Onlyrelevantwhencomparingagainstthedefaultconfig.'),gpu:bool=Opt(False,'--gpu','-G',help='WhethertheoriginalconfigcanrunonaGPU.Onlyrelevantwhencomparingagainstthedefaultconfig.'),pretraining:bool=Opt(False,'--pretraining','--pt',help='Whethertocompareonaconfigwithpretraininginvolved.Onlyrelevantwhencomparingagainstthedefaultconfig.'),markdown:bool=Opt(False,'--markdown','-md',help='GenerateMarkdownforGitHubissues'))


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/cli/profile.py----------------------------------------
A:spacy.cli.profile.texts->list(itertools.islice(texts, n_texts))
A:spacy.cli.profile.(imdb_train, _)->ml_datasets.imdb(train_limit=n_texts, dev_limit=0)
A:spacy.cli.profile.(texts, _)->zip(*imdb_train)
A:spacy.cli.profile.nlp->load_model(model)
A:spacy.cli.profile.s->pstats.Stats('Profile.prof')
A:spacy.cli.profile.input_path->Path(loc)
A:spacy.cli.profile.file_->Path(loc).open()
A:spacy.cli.profile.data->srsly.json_loads(line)
spacy.cli.profile(model:str,inputs:Optional[Path]=None,n_texts:int=10000)->None
spacy.cli.profile._read_inputs(loc:Union[Path,str],msg:Printer)->Iterator[str]
spacy.cli.profile.parse_texts(nlp:Language,texts:Sequence[str])->None
spacy.cli.profile.profile(model:str,inputs:Optional[Path]=None,n_texts:int=10000)->None
spacy.cli.profile.profile_cli(ctx:typer.Context,model:str=Arg(...,help='Trainedpipelinetoload'),inputs:Optional[Path]=Arg(None,help="Locationofinputfile.'-'forstdin.",exists=True,allow_dash=True),n_texts:int=Opt(10000,'--n-texts','-n',help='Maximumnumberoftextstouseifavailable'))


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/cli/assemble.py----------------------------------------
A:spacy.cli.assemble.overrides->parse_config_overrides(ctx.args)
A:spacy.cli.assemble.config->config.interpolate().interpolate()
A:spacy.cli.assemble.nlp->load_model_from_config(config, auto_fill=True)
A:spacy.cli.assemble.sourced->get_sourced_components(config)
spacy.cli.assemble.assemble_cli(ctx:typer.Context,config_path:Path=Arg(...,help='Pathtoconfigfile',exists=True,allow_dash=True),output_path:Path=Arg(...,help='Outputdirectorytostoreassembledpipelinein'),code_path:Optional[Path]=Opt(None,'--code','-c',help='PathtoPythonfilewithadditionalcode(registeredfunctions)tobeimported'),verbose:bool=Opt(False,'--verbose','-V','-VV',help='Displaymoreinformationfordebuggingpurposes'))
spacy.cli.assemble_cli(ctx:typer.Context,config_path:Path=Arg(...,help='Pathtoconfigfile',exists=True,allow_dash=True),output_path:Path=Arg(...,help='Outputdirectorytostoreassembledpipelinein'),code_path:Optional[Path]=Opt(None,'--code','-c',help='PathtoPythonfilewithadditionalcode(registeredfunctions)tobeimported'),verbose:bool=Opt(False,'--verbose','-V','-VV',help='Displaymoreinformationfordebuggingpurposes'))


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/cli/info.py----------------------------------------
A:spacy.cli.info.exclude->string_to_list(exclude)
A:spacy.cli.info.msg->Printer(no_print=silent, pretty=not silent)
A:spacy.cli.info.data->json.loads(dist.get_metadata('direct_url.json'))
A:spacy.cli.info.data['Pipelines']->', '.join((f'{n} ({v})' for (n, v) in data['Pipelines'].items()))
A:spacy.cli.info.markdown_data->get_markdown(data, title=title, exclude=exclude)
A:spacy.cli.info.package->pkg_name.replace('-', '_')
A:spacy.cli.info.all_models[package]->util.get_package_version(pkg_name)
A:spacy.cli.info.model_path->Path(model)
A:spacy.cli.info.meta->srsly.read_json(meta_path)
A:spacy.cli.info.meta['source']->str(model_path)
A:spacy.cli.info.download_url->info_installed_model_url(model)
A:spacy.cli.info.dist->pkg_resources.get_distribution(model)
A:spacy.cli.info.version->get_latest_version(model)
A:spacy.cli.info.filename->get_model_filename(model, version)
A:spacy.cli.info.release_url->release_tpl.format(m=model, v=version)
A:spacy.cli.info.md->MarkdownRenderer()
A:spacy.cli.info.existing_path->Path(value).exists()
spacy.cli.info(model:Optional[str]=None,*,markdown:bool=False,silent:bool=True,exclude:Optional[List[str]]=None,url:bool=False)->Union[str, dict]
spacy.cli.info.get_markdown(data:Dict[str,Any],title:Optional[str]=None,exclude:Optional[List[str]]=None)->str
spacy.cli.info.info(model:Optional[str]=None,*,markdown:bool=False,silent:bool=True,exclude:Optional[List[str]]=None,url:bool=False)->Union[str, dict]
spacy.cli.info.info_cli(model:Optional[str]=Arg(None,help='OptionalloadablespaCypipeline'),markdown:bool=Opt(False,'--markdown','-md',help='GenerateMarkdownforGitHubissues'),silent:bool=Opt(False,'--silent','-s','-S',help="Don'tprintanything(justreturn)"),exclude:str=Opt('labels','--exclude','-e',help='Comma-separatedkeystoexcludefromtheprint-out'),url:bool=Opt(False,'--url','-u',help='PrinttheURLtodownloadthemostrecentcompatibleversionofthepipeline'))
spacy.cli.info.info_installed_model_url(model:str)->Optional[str]
spacy.cli.info.info_model(model:str,*,silent:bool=True)->Dict[str, Any]
spacy.cli.info.info_model_url(model:str)->Dict[str, Any]
spacy.cli.info.info_spacy()->Dict[str, Any]


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/cli/benchmark_speed.py----------------------------------------
A:spacy.cli.benchmark_speed.nlp->util.load_model(model)
A:spacy.cli.benchmark_speed.corpus->Corpus(data_path)
A:spacy.cli.benchmark_speed.wps->benchmark(nlp, docs, n_batches, batch_size, not no_shuffle)
A:spacy.cli.benchmark_speed.self.start->time.perf_counter()
A:spacy.cli.benchmark_speed.self.q1->numpy.quantile(sample, 0.25)
A:spacy.cli.benchmark_speed.self.q2->numpy.quantile(sample, 0.5)
A:spacy.cli.benchmark_speed.self.q3->numpy.quantile(sample, 0.75)
A:spacy.cli.benchmark_speed.docs->util.load_model(model).pipe(tqdm(docs, unit='doc'), batch_size=batch_size)
A:spacy.cli.benchmark_speed.batch_docs->list(islice(docs, batch_size if batch_size else nlp.batch_size))
A:spacy.cli.benchmark_speed.n_tokens->count_tokens(batch_docs)
A:spacy.cli.benchmark_speed.mean->numpy.mean(sample)
A:spacy.cli.benchmark_speed.bootstrap_means->bootstrap(sample)
A:spacy.cli.benchmark_speed.quartiles->Quartiles(sample)
A:spacy.cli.benchmark_speed.n_outliers->numpy.sum((sample < quartiles.q1 - 1.5 * quartiles.iqr) | (sample > quartiles.q3 + 1.5 * quartiles.iqr))
A:spacy.cli.benchmark_speed.n_extreme_outliers->numpy.sum((sample < quartiles.q1 - 3.0 * quartiles.iqr) | (sample > quartiles.q3 + 3.0 * quartiles.iqr))
spacy.cli.benchmark_speed.Quartiles(self,sample:numpy.ndarray)
spacy.cli.benchmark_speed.annotate(nlp:Language,docs:List[Doc],batch_size:Optional[int])->numpy.ndarray
spacy.cli.benchmark_speed.benchmark(nlp:Language,docs:List[Doc],n_batches:int,batch_size:int,shuffle:bool)->numpy.ndarray
spacy.cli.benchmark_speed.benchmark_speed_cli(ctx:typer.Context,model:str=Arg(...,help='Modelnameorpath'),data_path:Path=Arg(...,help='Locationofbinaryevaluationdatain.spacyformat',exists=True),batch_size:Optional[int]=Opt(None,'--batch-size','-b',min=1,help='Overridethepipelinebatchsize'),no_shuffle:bool=Opt(False,'--no-shuffle',help='Donotshufflebenchmarkdata'),use_gpu:int=Opt(-1,'--gpu-id','-g',help='GPUIDor-1forCPU'),n_batches:int=Opt(50,'--batches',help='Minimumnumberofbatchestobenchmark',min=30),warmup_epochs:int=Opt(3,'--warmup','-w',min=0,help='Numberofiterationsoverthedataforwarmup'))
spacy.cli.benchmark_speed.bootstrap(x,statistic=numpy.mean,iterations=10000)->numpy.ndarray
spacy.cli.benchmark_speed.count_tokens(docs:Iterable[Doc])->int
spacy.cli.benchmark_speed.print_mean_with_ci(sample:numpy.ndarray)
spacy.cli.benchmark_speed.print_outliers(sample:numpy.ndarray)
spacy.cli.benchmark_speed.time_context
spacy.cli.benchmark_speed.time_context.__enter__(self)
spacy.cli.benchmark_speed.time_context.__exit__(self,type,value,traceback)
spacy.cli.benchmark_speed.warmup(nlp:Language,docs:List[Doc],warmup_epochs:int,batch_size:Optional[int])->numpy.ndarray
spacy.cli.benchmark_speed_cli(ctx:typer.Context,model:str=Arg(...,help='Modelnameorpath'),data_path:Path=Arg(...,help='Locationofbinaryevaluationdatain.spacyformat',exists=True),batch_size:Optional[int]=Opt(None,'--batch-size','-b',min=1,help='Overridethepipelinebatchsize'),no_shuffle:bool=Opt(False,'--no-shuffle',help='Donotshufflebenchmarkdata'),use_gpu:int=Opt(-1,'--gpu-id','-g',help='GPUIDor-1forCPU'),n_batches:int=Opt(50,'--batches',help='Minimumnumberofbatchestobenchmark',min=30),warmup_epochs:int=Opt(3,'--warmup','-w',min=0,help='Numberofiterationsoverthedataforwarmup'))


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/cli/debug_data.py----------------------------------------
A:spacy.cli.debug_data.overrides->parse_config_overrides(ctx.args)
A:spacy.cli.debug_data.msg->Printer(no_print=silent, pretty=not no_format, ignore_warnings=ignore_warnings)
A:spacy.cli.debug_data.cfg->util.load_config(config_path, overrides=config_overrides)
A:spacy.cli.debug_data.nlp->util.load_model_from_config(cfg)
A:spacy.cli.debug_data.config->util.load_model_from_config(cfg).config.interpolate()
A:spacy.cli.debug_data.T->util.registry.resolve(config['training'], schema=ConfigSchemaTraining)
A:spacy.cli.debug_data.sourced_components->get_sourced_components(cfg)
A:spacy.cli.debug_data.(train_corpus, dev_corpus)->resolve_dot_names(config, dot_names)
A:spacy.cli.debug_data.train_dataset->list(train_corpus(nlp))
A:spacy.cli.debug_data.dev_dataset->list(dev_corpus(nlp))
A:spacy.cli.debug_data.gold_train_data->_compile_gold(train_dataset, factory_names, nlp, make_proj=True)
A:spacy.cli.debug_data.gold_train_unpreprocessed_data->_compile_gold(train_dataset, factory_names, nlp, make_proj=False)
A:spacy.cli.debug_data.gold_dev_data->_compile_gold(dev_dataset, factory_names, nlp, make_proj=True)
A:spacy.cli.debug_data.overlap->len(train_texts.intersection(dev_texts))
A:spacy.cli.debug_data.most_common_words->gold_train_data['words'].most_common(10)
A:spacy.cli.debug_data.n_missing_vectors->sum(gold_train_data['words_missing_vectors'].values())
A:spacy.cli.debug_data.model_labels_spancat->_get_labels_from_spancat(nlp)
A:spacy.cli.debug_data.neg_docs->_get_examples_without_label(train_dataset, label, 'ner')
A:spacy.cli.debug_data.span_characteristics->_get_span_characteristics(train_dataset, gold_train_data, spans_key)
A:spacy.cli.debug_data._span_freqs->_get_spans_length_freq_dist(gold_train_data['spans_length'][spans_key])
A:spacy.cli.debug_data._filtered_span_freqs->_filter_spans_length_freq_dist(_span_freqs, threshold=SPAN_LENGTH_THRESHOLD_PERCENTAGE)
A:spacy.cli.debug_data.p_spans->span_characteristics['p_spans'].values()
A:spacy.cli.debug_data.p_bounds->span_characteristics['p_bounds'].values()
A:spacy.cli.debug_data.labels->set(label_list)
A:spacy.cli.debug_data.model_labels->_get_labels_from_model(nlp, 'morphologizer')
A:spacy.cli.debug_data.labels_with_counts->_format_labels(gold_train_unpreprocessed_data['deps'].most_common(), counts=True)
A:spacy.cli.debug_data.data->srsly.read_jsonl(file_path)
A:spacy.cli.debug_data.sent_starts->eg.get_aligned_sent_starts()
A:spacy.cli.debug_data.combined_label->remove_bilu_prefix(label)
A:spacy.cli.debug_data.data['spancat'][spans_key]->Counter()
A:spacy.cli.debug_data.data['spans_length'][spans_key]->dict()
A:spacy.cli.debug_data.data['spans_per_type'][spans_key]->dict()
A:spacy.cli.debug_data.data['sb_per_type'][spans_key]->dict()
A:spacy.cli.debug_data.tags->eg.get_aligned('TAG', as_string=True)
A:spacy.cli.debug_data.pos_tags->eg.get_aligned('POS', as_string=True)
A:spacy.cli.debug_data.morphs->eg.get_aligned('MORPH', as_string=True)
A:spacy.cli.debug_data.label_dict->morphology.Morphology.feats_to_dict(morph)
A:spacy.cli.debug_data.(aligned_heads, aligned_deps)->eg.get_aligned_parse(projectivize=make_proj)
A:spacy.cli.debug_data.freqs->dict(sorted(freqs.items()))
A:spacy.cli.debug_data.pipe->util.load_model_from_config(cfg).get_pipe(pipe_name)
A:spacy.cli.debug_data.labels[pipe.key]->set()
A:spacy.cli.debug_data.total->sum(word_counts.values(), 0.0)
A:spacy.cli.debug_data.t->token.text.lower().replace('``', '"').replace("''", '"')
A:spacy.cli.debug_data.word_counts->Counter({k: v / total for (k, v) in word_counts.items()})
A:spacy.cli.debug_data.p_corpus->_get_distribution([eg.reference for eg in examples], normalize=True)
A:spacy.cli.debug_data.max_col->max(30, max((len(label) for label in span_characteristics['labels'])))
A:spacy.cli.debug_data.table->_format_span_row(span_data=table_data, labels=span_characteristics['labels'])
A:spacy.cli.debug_data.percentage->round(percentage, 2)
spacy.cli.debug_data(config_path:Path,*,config_overrides:Dict[str,Any]={},ignore_warnings:bool=False,verbose:bool=False,no_format:bool=True,silent:bool=True)
spacy.cli.debug_data._compile_gold(examples:Sequence[Example],factory_names:List[str],nlp:Language,make_proj:bool)->Dict[str, Any]
spacy.cli.debug_data._filter_spans_length_freq_dist(freq_dist:Dict[int,float],threshold:int)->Dict[int, float]
spacy.cli.debug_data._format_freqs(freqs:Dict[int,float],sort:bool=True)->str
spacy.cli.debug_data._format_labels(labels:Union[Iterable[str],Iterable[Tuple[str,int]]],counts:bool=False)->str
spacy.cli.debug_data._format_span_row(span_data:List[Dict],labels:List[str])->List[Any]
spacy.cli.debug_data._get_distribution(docs,normalize:bool=True)->Counter
spacy.cli.debug_data._get_examples_without_label(data:Sequence[Example],label:str,component:Literal['ner','spancat']='ner',spans_key:Optional[str]='sc')->int
spacy.cli.debug_data._get_kl_divergence(p:Counter,q:Counter)->float
spacy.cli.debug_data._get_labels_from_model(nlp:Language,factory_name:str)->Set[str]
spacy.cli.debug_data._get_labels_from_spancat(nlp:Language)->Dict[str, Set[str]]
spacy.cli.debug_data._get_span_characteristics(examples:List[Example],compiled_gold:Dict[str,Any],spans_key:str)->Dict[str, Any]
spacy.cli.debug_data._get_spans_length_freq_dist(length_dict:Dict,threshold=SPAN_LENGTH_THRESHOLD_PERCENTAGE)->Dict[int, float]
spacy.cli.debug_data._gmean(l:List)->float
spacy.cli.debug_data._load_file(file_path:Path,msg:Printer)->None
spacy.cli.debug_data._print_span_characteristics(span_characteristics:Dict[str,Any])
spacy.cli.debug_data._wgt_average(metric:Dict[str,float],frequencies:Counter)->float
spacy.cli.debug_data.debug_data(config_path:Path,*,config_overrides:Dict[str,Any]={},ignore_warnings:bool=False,verbose:bool=False,no_format:bool=True,silent:bool=True)
spacy.cli.debug_data.debug_data_cli(ctx:typer.Context,config_path:Path=Arg(...,help='Pathtoconfigfile',exists=True,allow_dash=True),code_path:Optional[Path]=Opt(None,'--code-path','--code','-c',help='PathtoPythonfilewithadditionalcode(registeredfunctions)tobeimported'),ignore_warnings:bool=Opt(False,'--ignore-warnings','-IW',help='Ignorewarnings,onlyshowstatsanderrors'),verbose:bool=Opt(False,'--verbose','-V',help='Printadditionalinformationandexplanations'),no_format:bool=Opt(False,'--no-format','-NF',help="Don'tpretty-printtheresults"))


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/cli/apply.py----------------------------------------
A:spacy.cli.apply.docbin->DocBin(store_user_data=True)
A:spacy.cli.apply.text->fin.read()
A:spacy.cli.apply.data_path->ensure_path(data_path)
A:spacy.cli.apply.output_file->output_file.with_suffix('.spacy').with_suffix('.spacy')
A:spacy.cli.apply.code_path->ensure_path(code_path)
A:spacy.cli.apply.paths->walk_directory(data_path)
A:spacy.cli.apply.nlp->load_model(model)
A:spacy.cli.apply.datagen->cast(DocOrStrStream, chain(*streams))
spacy.cli.apply(data_path:Path,output_file:Path,model:str,json_field:str,batch_size:int,n_process:int)
spacy.cli.apply._stream_docbin(path:Path,vocab:Vocab)->Iterable[Doc]
spacy.cli.apply._stream_jsonl(path:Path,field:str)->Iterable[str]
spacy.cli.apply._stream_texts(paths:Iterable[Path])->Iterable[str]
spacy.cli.apply.apply(data_path:Path,output_file:Path,model:str,json_field:str,batch_size:int,n_process:int)
spacy.cli.apply.apply_cli(model:str=Arg(...,help='Modelnameorpath'),data_path:Path=Arg(...,help=path_help,exists=True),output_file:Path=Arg(...,help=out_help,dir_okay=False),code_path:Optional[Path]=Opt(None,'--code','-c',help=code_help),text_key:str=Opt('text','--text-key','-tk',help='KeycontainingtextstringforJSONL'),force_overwrite:bool=Opt(False,'--force','-F',help='Forceoverwritingtheoutputfile'),use_gpu:int=Opt(-1,'--gpu-id','-g',help='GPUIDor-1forCPU.'),batch_size:int=Opt(1,'--batch-size','-b',help='Batchsize.'),n_process:int=Opt(1,'--n-process','-n',help='numberofprocessorstouse.'))


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/cli/validate.py----------------------------------------
A:spacy.cli.validate.(model_pkgs, compat)->get_model_pkgs()
A:spacy.cli.validate.spacy_version->get_model_meta(model_path).get('spacy_version', 'n/a')
A:spacy.cli.validate.current_compat->compat.get(spacy_version, {})
A:spacy.cli.validate.comp->Printer(no_print=silent, pretty=not silent).text('', color='green', icon='good', no_print=True)
A:spacy.cli.validate.version->get_package_version(pkg_name)
A:spacy.cli.validate.msg->Printer(no_print=silent, pretty=not silent)
A:spacy.cli.validate.r->requests.get(about.__compatibility__)
A:spacy.cli.validate.all_models->set()
A:spacy.cli.validate.installed_models->get_installed_models()
A:spacy.cli.validate.package->pkg_name.replace('-', '_')
A:spacy.cli.validate.model_path->get_package_path(package)
A:spacy.cli.validate.model_meta->get_model_meta(model_path)
A:spacy.cli.validate.is_compat->is_compatible_version(about.__version__, spacy_version)
spacy.cli.validate()->None
spacy.cli.validate.get_model_pkgs(silent:bool=False)->Tuple[dict, dict]
spacy.cli.validate.reformat_version(version:str)->str
spacy.cli.validate.validate()->None
spacy.cli.validate.validate_cli()


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/cli/train.py----------------------------------------
A:spacy.cli.train.overrides->parse_config_overrides(ctx.args)
A:spacy.cli.train.config_path->util.ensure_path(config_path)
A:spacy.cli.train.output_path->util.ensure_path(output_path)
A:spacy.cli.train.config->util.load_config(config_path, overrides=overrides, interpolate=False)
A:spacy.cli.train.nlp->init_nlp(config, use_gpu=use_gpu)
spacy.cli.train.train(config_path:Union[str,Path],output_path:Optional[Union[str,Path]]=None,*,use_gpu:int=-1,overrides:Dict[str,Any]=util.SimpleFrozenDict())
spacy.cli.train.train_cli(ctx:typer.Context,config_path:Path=Arg(...,help='Pathtoconfigfile',exists=True,allow_dash=True),output_path:Optional[Path]=Opt(None,'--output','--output-path','-o',help='Outputdirectorytostoretrainedpipelinein'),code_path:Optional[Path]=Opt(None,'--code','-c',help='PathtoPythonfilewithadditionalcode(registeredfunctions)tobeimported'),verbose:bool=Opt(False,'--verbose','-V','-VV',help='Displaymoreinformationfordebuggingpurposes'),use_gpu:int=Opt(-1,'--gpu-id','-g',help='GPUIDor-1forCPU'))
spacy.cli.train_cli(ctx:typer.Context,config_path:Path=Arg(...,help='Pathtoconfigfile',exists=True,allow_dash=True),output_path:Optional[Path]=Opt(None,'--output','--output-path','-o',help='Outputdirectorytostoretrainedpipelinein'),code_path:Optional[Path]=Opt(None,'--code','-c',help='PathtoPythonfilewithadditionalcode(registeredfunctions)tobeimported'),verbose:bool=Opt(False,'--verbose','-V','-VV',help='Displaymoreinformationfordebuggingpurposes'),use_gpu:int=Opt(-1,'--gpu-id','-g',help='GPUIDor-1forCPU'))


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/cli/download.py----------------------------------------
A:spacy.cli.download.components->model.split('-')
A:spacy.cli.download.model_name->''.join(components[:-1])
A:spacy.cli.download.compatibility->get_compatibility()
A:spacy.cli.download.version->get_minor_version(about.__version__)
A:spacy.cli.download.filename->dl_tpl.format(m=model_name, v=version, s=suffix)
A:spacy.cli.download.r->requests.get(about.__compatibility__)
A:spacy.cli.download.comp_table->requests.get(about.__compatibility__).json()
A:spacy.cli.download.comp->get_compatibility()
spacy.cli.download(model:str,direct:bool=False,sdist:bool=False,*pip_args)->None
spacy.cli.download.download(model:str,direct:bool=False,sdist:bool=False,*pip_args)->None
spacy.cli.download.download_cli(ctx:typer.Context,model:str=Arg(...,help='Nameofpipelinepackagetodownload'),direct:bool=Opt(False,'--direct','-d','-D',help='Forcedirectdownloadofname+version'),sdist:bool=Opt(False,'--sdist','-S',help='Downloadsdist(.tar.gz)archiveinsteadofpre-builtbinarywheel'))
spacy.cli.download.download_model(filename:str,user_pip_args:Optional[Sequence[str]]=None)->None
spacy.cli.download.get_compatibility()->dict
spacy.cli.download.get_latest_version(model:str)->str
spacy.cli.download.get_model_filename(model_name:str,version:str,sdist:bool=False)->str
spacy.cli.download.get_version(model:str,comp:dict)->str


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/cli/__init__.py----------------------------------------
spacy.cli.__init__.link(*args,**kwargs)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/cli/evaluate.py----------------------------------------
A:spacy.cli.evaluate.msg->Printer(no_print=silent, pretty=not silent)
A:spacy.cli.evaluate.data_path->util.ensure_path(data_path)
A:spacy.cli.evaluate.output_path->util.ensure_path(output)
A:spacy.cli.evaluate.displacy_path->util.ensure_path(displacy_path)
A:spacy.cli.evaluate.corpus->Corpus(data_path, gold_preproc=gold_preproc)
A:spacy.cli.evaluate.nlp->util.load_model(model)
A:spacy.cli.evaluate.dev_dataset->list(corpus(nlp))
A:spacy.cli.evaluate.scores->util.load_model(model).evaluate(dev_dataset)
A:spacy.cli.evaluate.data->handle_scores_per_type(scores, data, spans_key=spans_key, silent=silent)
A:spacy.cli.evaluate.docs->list(nlp.pipe((ex.reference.text for ex in dev_dataset[:displacy_limit])))
A:spacy.cli.evaluate.html->displacy.render(docs[:limit], style='dep', page=True, options={'compact': True})
spacy.cli.evaluate(model:str,data_path:Path,output:Optional[Path]=None,use_gpu:int=-1,gold_preproc:bool=False,displacy_path:Optional[Path]=None,displacy_limit:int=25,silent:bool=True,spans_key:str='sc')->Dict[str, Any]
spacy.cli.evaluate.evaluate(model:str,data_path:Path,output:Optional[Path]=None,use_gpu:int=-1,gold_preproc:bool=False,displacy_path:Optional[Path]=None,displacy_limit:int=25,silent:bool=True,spans_key:str='sc')->Dict[str, Any]
spacy.cli.evaluate.evaluate_cli(model:str=Arg(...,help='Modelnameorpath'),data_path:Path=Arg(...,help='Locationofbinaryevaluationdatain.spacyformat',exists=True),output:Optional[Path]=Opt(None,'--output','-o',help='OutputJSONfileformetrics',dir_okay=False),code_path:Optional[Path]=Opt(None,'--code','-c',help='PathtoPythonfilewithadditionalcode(registeredfunctions)tobeimported'),use_gpu:int=Opt(-1,'--gpu-id','-g',help='GPUIDor-1forCPU'),gold_preproc:bool=Opt(False,'--gold-preproc','-G',help='Usegoldpreprocessing'),displacy_path:Optional[Path]=Opt(None,'--displacy-path','-dp',help='DirectorytooutputrenderedparsesasHTML',exists=True,file_okay=False),displacy_limit:int=Opt(25,'--displacy-limit','-dl',help='LimitofparsestorenderasHTML'))
spacy.cli.evaluate.handle_scores_per_type(scores:Dict[str,Any],data:Dict[str,Any]={},*,spans_key:str='sc',silent:bool=False)->Dict[str, Any]
spacy.cli.evaluate.print_prf_per_type(msg:Printer,scores:Dict[str,Dict[str,float]],name:str,type:str)->None
spacy.cli.evaluate.print_textcats_auc_per_cat(msg:Printer,scores:Dict[str,Dict[str,float]])->None
spacy.cli.evaluate.render_parses(docs:List[Doc],output_path:Path,model_name:str='',limit:int=250,deps:bool=True,ents:bool=True)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/cli/debug_model.py----------------------------------------
A:spacy.cli.debug_model.layers->print_settings.get('layers', '')
A:spacy.cli.debug_model.config_overrides->parse_config_overrides(ctx.args)
A:spacy.cli.debug_model.raw_config->util.load_config(config_path, overrides=config_overrides, interpolate=False)
A:spacy.cli.debug_model.config->util.get_lang_class(lang)().config.interpolate()
A:spacy.cli.debug_model.nlp->util.get_lang_class(lang)()
A:spacy.cli.debug_model.T->util.registry.resolve(config['training'], schema=ConfigSchemaTraining)
A:spacy.cli.debug_model.pipe->util.get_lang_class(lang)().get_pipe(component)
A:spacy.cli.debug_model.(train_corpus,)->resolve_dot_names(config, dot_names)
A:spacy.cli.debug_model.examples->list(itertools.islice(train_corpus(nlp), 5))
A:spacy.cli.debug_model.upstream_component->util.get_lang_class(lang)().get_pipe('transformer')
A:spacy.cli.debug_model.prediction->model.predict([ex.predicted for ex in examples])
A:spacy.cli.debug_model.parameters->print_settings.get('parameters', False)
A:spacy.cli.debug_model.dimensions->print_settings.get('dimensions', False)
A:spacy.cli.debug_model.gradients->print_settings.get('gradients', False)
A:spacy.cli.debug_model.attributes->print_settings.get('attributes', False)
A:spacy.cli.debug_model.print_value->_print_matrix(node.get_grad(name))
spacy.cli.debug_model(config,resolved_train_config,nlp,pipe,*,print_settings:Optional[Dict[str,Any]]=None)
spacy.cli.debug_model._get_docs(lang:str='en')
spacy.cli.debug_model._print_matrix(value)
spacy.cli.debug_model._print_model(model,print_settings)
spacy.cli.debug_model._sentences()
spacy.cli.debug_model._set_output_dim(model,nO)
spacy.cli.debug_model.debug_model(config,resolved_train_config,nlp,pipe,*,print_settings:Optional[Dict[str,Any]]=None)
spacy.cli.debug_model.debug_model_cli(ctx:typer.Context,config_path:Path=Arg(...,help='Pathtoconfigfile',exists=True,allow_dash=True),component:str=Arg(...,help='Nameofthepipelinecomponentofwhichthemodelshouldbeanalysed'),layers:str=Opt('','--layers','-l',help='Comma-separatednamesoflayerIDstoprint'),dimensions:bool=Opt(False,'--dimensions','-DIM',help='Showdimensions'),parameters:bool=Opt(False,'--parameters','-PAR',help='Showparameters'),gradients:bool=Opt(False,'--gradients','-GRAD',help='Showgradients'),attributes:bool=Opt(False,'--attributes','-ATTR',help='Showattributes'),P0:bool=Opt(False,'--print-step0','-P0',help='Printmodelbeforetraining'),P1:bool=Opt(False,'--print-step1','-P1',help='Printmodelafterinitialization'),P2:bool=Opt(False,'--print-step2','-P2',help='Printmodelaftertraining'),P3:bool=Opt(False,'--print-step3','-P3',help='Printfinalpredictions'),use_gpu:int=Opt(-1,'--gpu-id','-g',help='GPUIDor-1forCPU'))


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/cli/convert.py----------------------------------------
A:spacy.cli.convert.input_path->Path(input_path)
A:spacy.cli.convert.msg->Printer(no_print=silent)
A:spacy.cli.convert.converter->_get_converter(msg, converter, input_path)
A:spacy.cli.convert.input_data->file_.read()
A:spacy.cli.convert.docs->func(input_data, n_sents=n_sents, seg_sents=seg_sents, append_morphology=morphology, merge_subtokens=merge_subtokens, lang=lang, model=model, no_print=silent, ner_map=ner_map)
A:spacy.cli.convert.all_docs->itertools.chain.from_iterable([docs for (_, docs) in doc_files])
A:spacy.cli.convert.len_docs->len(db)
A:spacy.cli.convert.db->DocBin(docs=docs, store_user_data=True)
A:spacy.cli.convert.data->DocBin(docs=docs, store_user_data=True).to_bytes()
A:spacy.cli.convert.subpath->input_loc.relative_to(input_path)
A:spacy.cli.convert.output_file->output_file.with_suffix(f'.{file_type}').with_suffix(f'.{file_type}')
A:spacy.cli.convert.iob_re->re.compile('\\S+\\|(O|[IB]-\\S+)')
A:spacy.cli.convert.ner_re->re.compile('\\S+\\s+(O|[IB]-\\S+)$')
A:spacy.cli.convert.line->line.strip().strip()
A:spacy.cli.convert.input_locs->walk_directory(input_path, suffix=None)
A:spacy.cli.convert.file_types->list(set([loc.suffix[1:] for loc in input_locs]))
A:spacy.cli.convert.file_types_str->','.join(file_types)
A:spacy.cli.convert.converter_autodetect->autodetect_ner_format(input_data)
spacy.cli.convert(input_path:Path,output_dir:Union[str,Path],*,file_type:str='json',n_sents:int=1,seg_sents:bool=False,model:Optional[str]=None,morphology:bool=False,merge_subtokens:bool=False,converter:str,ner_map:Optional[Path]=None,lang:Optional[str]=None,concatenate:bool=False,silent:bool=True,msg:Optional[Printer]=None)->None
spacy.cli.convert.FileTypes(str,Enum)
spacy.cli.convert._get_converter(msg,converter,input_path:Path)
spacy.cli.convert._print_docs_to_stdout(data:Any,output_type:str)->None
spacy.cli.convert._write_docs_to_file(data:Any,output_file:Path,output_type:str)->None
spacy.cli.convert.autodetect_ner_format(input_data:str)->Optional[str]
spacy.cli.convert.convert(input_path:Path,output_dir:Union[str,Path],*,file_type:str='json',n_sents:int=1,seg_sents:bool=False,model:Optional[str]=None,morphology:bool=False,merge_subtokens:bool=False,converter:str,ner_map:Optional[Path]=None,lang:Optional[str]=None,concatenate:bool=False,silent:bool=True,msg:Optional[Printer]=None)->None
spacy.cli.convert.convert_cli(input_path:str=Arg(...,help='Inputfileordirectory',exists=True),output_dir:Path=Arg('-',help="Outputdirectory.'-'forstdout.",allow_dash=True,exists=True),file_type:FileTypes=Opt('spacy','--file-type','-t',help='Typeofdatatoproduce'),n_sents:int=Opt(1,'--n-sents','-n',help='Numberofsentencesperdoc(0todisable)'),seg_sents:bool=Opt(False,'--seg-sents','-s',help='Segmentsentences(for-cner)'),model:Optional[str]=Opt(None,'--model','--base','-b',help='TrainedspaCypipelineforsentencesegmentationtouseasbase(for--seg-sents)'),morphology:bool=Opt(False,'--morphology','-m',help='Enableappendingmorphologytotags'),merge_subtokens:bool=Opt(False,'--merge-subtokens','-T',help='MergeCoNLL-Usubtokens'),converter:str=Opt(AUTO,'--converter','-c',help=f'Converter:{tuple(CONVERTERS.keys())}'),ner_map:Optional[Path]=Opt(None,'--ner-map','-nm',help='NERtagmapping(asJSON-encodeddictofentitytypes)',exists=True),lang:Optional[str]=Opt(None,'--lang','-l',help='Language(iftokenizerrequired)'),concatenate:bool=Opt(None,'--concatenate','-C',help='Concatenateoutputtoasinglefile'))
spacy.cli.convert.verify_cli_args(msg:Printer,input_path:Path,output_dir:Union[str,Path],file_type:str,converter:str,ner_map:Optional[Path])


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/cli/project/push.py----------------------------------------
A:spacy.cli.project.push.config->load_project_config(project_dir)
A:spacy.cli.project.push.storage->RemoteStorage(project_dir, remote)
A:spacy.cli.project.push.cmd_hash->get_command_hash('', '', [project_dir / dep for dep in cmd.get('deps', [])], cmd['script'])
A:spacy.cli.project.push.url->RemoteStorage(project_dir, remote).push(output_path, command_hash=cmd_hash, content_hash=get_content_hash(output_loc))
spacy.cli.project.push._is_not_empty_dir(loc:Path)
spacy.cli.project.push.project_push(project_dir:Path,remote:str)
spacy.cli.project.push.project_push_cli(remote:str=Arg('default',help='Nameorpathofremotestorage'),project_dir:Path=Arg(Path.cwd(),help='Locationofprojectdirectory.Defaultstocurrentworkingdirectory.',exists=True,file_okay=False))
spacy.cli.project_push(project_dir:Path,remote:str)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/cli/project/document.py----------------------------------------
A:spacy.cli.project.document.config->load_project_config(project_dir)
A:spacy.cli.project.document.md->MarkdownRenderer(no_emoji=no_emoji)
A:spacy.cli.project.document.title->load_project_config(project_dir).get('title')
A:spacy.cli.project.document.description->load_project_config(project_dir).get('description')
A:spacy.cli.project.document.cmds->load_project_config(project_dir).get('commands', [])
A:spacy.cli.project.document.wfs->load_project_config(project_dir).get('workflows', {}).items()
A:spacy.cli.project.document.assets->load_project_config(project_dir).get('assets', [])
A:spacy.cli.project.document.dest->MarkdownRenderer(no_emoji=no_emoji).link(dest, dest_path)
A:spacy.cli.project.document.existing->f.read()
spacy.cli.project.document.project_document(project_dir:Path,output_file:Path,*,no_emoji:bool=False)->None
spacy.cli.project.document.project_document_cli(project_dir:Path=Arg(Path.cwd(),help='Pathtoclonedproject.Defaultstocurrentworkingdirectory.',exists=True,file_okay=False),output_file:Path=Opt('-','--output','-o',help='PathtooutputMarkdownfileforoutput.Defaultsto-forstandardoutput'),no_emoji:bool=Opt(False,'--no-emoji','-NE',help="Don'tuseemoji"))
spacy.cli.project_document(project_dir:Path,output_file:Path,*,no_emoji:bool=False)->None


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/cli/project/pull.py----------------------------------------
A:spacy.cli.project.pull.config->load_project_config(project_dir)
A:spacy.cli.project.pull.storage->RemoteStorage(project_dir, remote)
A:spacy.cli.project.pull.commands->list(config.get('commands', []))
A:spacy.cli.project.pull.cmd_hash->get_command_hash('', '', deps, cmd['script'])
A:spacy.cli.project.pull.url->RemoteStorage(project_dir, remote).pull(output_path, command_hash=cmd_hash)
spacy.cli.project.pull.project_pull(project_dir:Path,remote:str,*,verbose:bool=False)
spacy.cli.project.pull.project_pull_cli(remote:str=Arg('default',help='Nameorpathofremotestorage'),project_dir:Path=Arg(Path.cwd(),help='Locationofprojectdirectory.Defaultstocurrentworkingdirectory.',exists=True,file_okay=False))
spacy.cli.project_pull(project_dir:Path,remote:str,*,verbose:bool=False)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/cli/project/assets.py----------------------------------------
A:spacy.cli.project.assets.overrides->parse_config_overrides(ctx.args)
A:spacy.cli.project.assets.project_path->ensure_path(project_dir)
A:spacy.cli.project.assets.config->load_project_config(project_path, overrides=overrides)
A:spacy.cli.project.assets.dest->(project_dir / asset['dest']).resolve()
A:spacy.cli.project.assets.checksum->asset.get('checksum')
A:spacy.cli.project.assets.url->convert_asset_url(url)
A:spacy.cli.project.assets.dest_path->(project_path / dest).resolve()
A:spacy.cli.project.assets.converted->re.sub('/(tree|blob)/', '/', converted)
spacy.cli.project.assets.check_private_asset(dest:Path,checksum:Optional[str]=None)->None
spacy.cli.project.assets.convert_asset_url(url:str)->str
spacy.cli.project.assets.fetch_asset(project_path:Path,url:str,dest:Path,checksum:Optional[str]=None)->None
spacy.cli.project.assets.project_assets(project_dir:Path,*,overrides:Dict[str,Any]=SimpleFrozenDict(),sparse_checkout:bool=False,extra:bool=False)->None
spacy.cli.project.assets.project_assets_cli(ctx:typer.Context,project_dir:Path=Arg(Path.cwd(),help='Pathtoclonedproject.Defaultstocurrentworkingdirectory.',exists=True,file_okay=False),sparse_checkout:bool=Opt(False,'--sparse','-S',help='UsesparsecheckoutforassetsprovidedviaGit,toonlycheckoutandclonethefilesneeded.RequiresGitv22.2+.'),extra:bool=Opt(False,'--extra','-e',help="Downloadallassets,includingthosemarkedas'extra'."))
spacy.cli.project_assets(project_dir:Path,*,overrides:Dict[str,Any]=SimpleFrozenDict(),sparse_checkout:bool=False,extra:bool=False)->None


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/cli/project/run.py----------------------------------------
A:spacy.cli.project.run.overrides->parse_config_overrides(ctx.args)
A:spacy.cli.project.run.config->load_project_config(project_dir)
A:spacy.cli.project.run.workflows->load_project_config(project_dir).get('workflows', {})
A:spacy.cli.project.run.check_spacy_commit->check_bool_env_var(ENV_VARS.PROJECT_USE_GIT_VERSION)
A:spacy.cli.project.run.rerun->check_rerun(current_dir, cmd, check_spacy_commit=check_spacy_commit)
A:spacy.cli.project.run.config_commands->load_project_config(project_dir).get('commands', [])
A:spacy.cli.project.run.help_text->commands[subcommand].get('help')
A:spacy.cli.project.run.title->load_project_config(project_dir).get('title')
A:spacy.cli.project.run.command->split_command(c)
A:spacy.cli.project.run.data->srsly.read_yaml(lock_path)
A:spacy.cli.project.run.spacy_v->entry.get('spacy_version')
A:spacy.cli.project.run.commit->entry.get('spacy_git_version')
A:spacy.cli.project.run.lock_entry->get_lock_entry(project_dir, command)
A:spacy.cli.project.run.data[command['name']]->get_lock_entry(project_dir, command)
A:spacy.cli.project.run.deps->get_fileinfo(project_dir, command.get('deps', []))
A:spacy.cli.project.run.outs->get_fileinfo(project_dir, command.get('outputs', []))
A:spacy.cli.project.run.outs_nc->get_fileinfo(project_dir, command.get('outputs_no_cache', []))
spacy.cli.project.run._check_requirements(requirements:List[str])->Tuple[bool, bool]
spacy.cli.project.run.check_rerun(project_dir:Path,command:Dict[str,Any],*,check_spacy_version:bool=True,check_spacy_commit:bool=False)->bool
spacy.cli.project.run.get_fileinfo(project_dir:Path,paths:List[str])->List[Dict[str, Optional[str]]]
spacy.cli.project.run.get_lock_entry(project_dir:Path,command:Dict[str,Any])->Dict[str, Any]
spacy.cli.project.run.print_run_help(project_dir:Path,subcommand:Optional[str]=None)->None
spacy.cli.project.run.project_run(project_dir:Path,subcommand:str,*,overrides:Dict[str,Any]=SimpleFrozenDict(),force:bool=False,dry:bool=False,capture:bool=False,skip_requirements_check:bool=False)->None
spacy.cli.project.run.project_run_cli(ctx:typer.Context,subcommand:str=Arg(None,help=f'Nameofcommanddefinedinthe{PROJECT_FILE}'),project_dir:Path=Arg(Path.cwd(),help='Locationofprojectdirectory.Defaultstocurrentworkingdirectory.',exists=True,file_okay=False),force:bool=Opt(False,'--force','-F',help='Forcere-runningsteps,evenifnothingchanged'),dry:bool=Opt(False,'--dry','-D',help="Performadryrunanddon'texecutescripts"),show_help:bool=Opt(False,'--help',help='Showhelpmessageandavailablesubcommands'))
spacy.cli.project.run.run_commands(commands:Iterable[str]=SimpleFrozenList(),silent:bool=False,dry:bool=False,capture:bool=False)->None
spacy.cli.project.run.update_lockfile(project_dir:Path,command:Dict[str,Any])->None
spacy.cli.project.run.validate_subcommand(commands:Sequence[str],workflows:Sequence[str],subcommand:str)->None
spacy.cli.project_run(project_dir:Path,subcommand:str,*,overrides:Dict[str,Any]=SimpleFrozenDict(),force:bool=False,dry:bool=False,capture:bool=False,skip_requirements_check:bool=False)->None


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/cli/project/dvc.py----------------------------------------
A:spacy.cli.project.dvc.config->load_project_config(project_dir)
A:spacy.cli.project.dvc.updated->update_dvc_config(project_dir, config, workflow, verbose=verbose, quiet=quiet, force=force)
A:spacy.cli.project.dvc.workflows->load_project_config(project_dir).get('workflows', {})
A:spacy.cli.project.dvc.workflow_names->list(workflows.keys())
A:spacy.cli.project.dvc.config_hash->get_hash(config)
A:spacy.cli.project.dvc.path->path.resolve().resolve()
A:spacy.cli.project.dvc.ref_hash->f.readline().strip().replace('# ', '')
A:spacy.cli.project.dvc.deps->command.get('deps', [])
A:spacy.cli.project.dvc.outputs->command.get('outputs', [])
A:spacy.cli.project.dvc.outputs_no_cache->command.get('outputs_no_cache', [])
A:spacy.cli.project.dvc.content->f.read()
spacy.cli.project.dvc.check_workflows(workflows:List[str],workflow:Optional[str]=None)->None
spacy.cli.project.dvc.ensure_dvc(project_dir:Path)->None
spacy.cli.project.dvc.project_update_dvc(project_dir:Path,workflow:Optional[str]=None,*,verbose:bool=False,quiet:bool=False,force:bool=False)->None
spacy.cli.project.dvc.project_update_dvc_cli(project_dir:Path=Arg(Path.cwd(),help='Locationofprojectdirectory.Defaultstocurrentworkingdirectory.',exists=True,file_okay=False),workflow:Optional[str]=Arg(None,help=f'Nameofworkflowdefinedin{PROJECT_FILE}.Defaultstofirstworkflowifnotset.'),verbose:bool=Opt(False,'--verbose','-V',help='Printmoreinfo'),quiet:bool=Opt(False,'--quiet','-q',help='Printlessinfo'),force:bool=Opt(False,'--force','-F',help='ForceupdateDVCconfig'))
spacy.cli.project.dvc.update_dvc_config(path:Path,config:Dict[str,Any],workflow:Optional[str]=None,verbose:bool=False,quiet:bool=False,force:bool=False)->bool
spacy.cli.project_update_dvc(project_dir:Path,workflow:Optional[str]=None,*,verbose:bool=False,quiet:bool=False,force:bool=False)->None


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/cli/project/remote_storage.py----------------------------------------
A:spacy.cli.project.remote_storage.self.url->ensure_pathy(url)
A:spacy.cli.project.remote_storage.url->self.find(path, command_hash=command_hash, content_hash=content_hash)
A:spacy.cli.project.remote_storage.abs_directory->os.path.abspath(directory)
A:spacy.cli.project.remote_storage.abs_target->os.path.abspath(target)
A:spacy.cli.project.remote_storage.prefix->os.path.commonprefix([abs_directory, abs_target])
A:spacy.cli.project.remote_storage.member_path->os.path.join(path, member.name)
A:spacy.cli.project.remote_storage.name->self.encode_name(str(path))
A:spacy.cli.project.remote_storage.urls->list((self.url / name / command_hash).iterdir())
A:spacy.cli.project.remote_storage.spacy_v->str(get_minor_version(about.__version__) or '')
A:spacy.cli.project.remote_storage.creation_bytes->''.join(hashes).encode('utf8')
A:spacy.cli.project.remote_storage.site_dirs->site.getsitepackages()
A:spacy.cli.project.remote_storage.packages->set()
A:spacy.cli.project.remote_storage.site_dir->Path(site_dir)
A:spacy.cli.project.remote_storage.package_bytes->''.join(sorted(packages)).encode('utf8')
A:spacy.cli.project.remote_storage.env_vars[key]->os.environ.get(value[1:], '')
spacy.cli.project.remote_storage.RemoteStorage(self,project_root:Path,url:str,*,compression='gz')
spacy.cli.project.remote_storage.RemoteStorage.encode_name(self,name:str)->str
spacy.cli.project.remote_storage.RemoteStorage.find(self,path:Path,*,command_hash:Optional[str]=None,content_hash:Optional[str]=None)->Optional['FluidPath']
spacy.cli.project.remote_storage.RemoteStorage.make_url(self,path:Path,command_hash:str,content_hash:str)->'FluidPath'
spacy.cli.project.remote_storage.RemoteStorage.pull(self,path:Path,*,command_hash:Optional[str]=None,content_hash:Optional[str]=None)->Optional['FluidPath']
spacy.cli.project.remote_storage.RemoteStorage.push(self,path:Path,command_hash:str,content_hash:str)->'FluidPath'
spacy.cli.project.remote_storage.get_command_hash(site_hash:str,env_hash:str,deps:List[Path],cmd:List[str])->str
spacy.cli.project.remote_storage.get_content_hash(loc:Path)->str
spacy.cli.project.remote_storage.get_env_hash(env:Dict[str,str])->str
spacy.cli.project.remote_storage.get_site_hash()


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/cli/project/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/cli/project/clone.py----------------------------------------
A:spacy.cli.project.clone.default_branches_msg->', '.join((f"'{b}'" for b in DEFAULT_BRANCHES))
A:spacy.cli.project.clone.dest->ensure_path(dest)
A:spacy.cli.project.clone.project_dir->ensure_path(dest).resolve()
A:spacy.cli.project.clone.repo_name->re.sub('(http(s?)):\\/\\/github.com/', '', repo)
spacy.cli.project.clone.check_clone(name:str,dest:Path,repo:str)->None
spacy.cli.project.clone.project_clone(name:str,dest:Path,*,repo:str=about.__projects__,branch:str=about.__projects_branch__,sparse_checkout:bool=False)->None
spacy.cli.project.clone.project_clone_cli(name:str=Arg(...,help='Thenameofthetemplatetoclone'),dest:Optional[Path]=Arg(None,help='Wheretoclonetheproject.Defaultstocurrentworkingdirectory',exists=False),repo:str=Opt(DEFAULT_REPO,'--repo','-r',help='Therepositorytoclonefrom'),branch:Optional[str]=Opt(None,'--branch','-b',help=f"Thebranchtoclonefrom.Ifnotprovided,willattempt{','.join(DEFAULT_BRANCHES)}"),sparse_checkout:bool=Opt(False,'--sparse','-S',help='UsesparseGitcheckouttoonlycheckoutandclonethefilesneeded.RequiresGitv22.2+.'))
spacy.cli.project_clone(name:str,dest:Path,*,repo:str=about.__projects__,branch:str=about.__projects_branch__,sparse_checkout:bool=False)->None


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tokens/morphanalysis.pyi----------------------------------------
spacy.tokens.MorphAnalysis(self,vocab:Vocab,features:Union[Dict[str,str],str]=...)
spacy.tokens.morphanalysis.MorphAnalysis(self,vocab:Vocab,features:Union[Dict[str,str],str]=...)
spacy.tokens.morphanalysis.MorphAnalysis.__contains__(self,feature:str)->bool
spacy.tokens.morphanalysis.MorphAnalysis.__eq__(self,other:MorphAnalysis)->bool
spacy.tokens.morphanalysis.MorphAnalysis.__hash__(self)->int
spacy.tokens.morphanalysis.MorphAnalysis.__iter__(self)->Iterator[str]
spacy.tokens.morphanalysis.MorphAnalysis.__len__(self)->int
spacy.tokens.morphanalysis.MorphAnalysis.__ne__(self,other:MorphAnalysis)->bool
spacy.tokens.morphanalysis.MorphAnalysis.__repr__(self)->str
spacy.tokens.morphanalysis.MorphAnalysis.__str__(self)->str
spacy.tokens.morphanalysis.MorphAnalysis.from_id(cls,vocab:Vocab,key:Any)->MorphAnalysis
spacy.tokens.morphanalysis.MorphAnalysis.get(self,field:Any)->List[str]
spacy.tokens.morphanalysis.MorphAnalysis.to_dict(self)->Dict[str, str]
spacy.tokens.morphanalysis.MorphAnalysis.to_json(self)->str


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tokens/underscore.py----------------------------------------
A:spacy.tokens.underscore.extensions->list(self._extensions.keys())
A:spacy.tokens.underscore.method_partial->functools.partial(method, self._obj)
A:spacy.tokens.underscore.key->self._get_key(name)
A:spacy.tokens.underscore.new_default->copy.copy(default)
A:spacy.tokens.underscore.default->kwargs.get('default')
A:spacy.tokens.underscore.getter->kwargs.get('getter')
A:spacy.tokens.underscore.setter->kwargs.get('setter')
A:spacy.tokens.underscore.method->kwargs.get('method')
A:spacy.tokens.underscore.nr_defined->sum((t is True for t in valid_opts))
spacy.tokens.underscore.Underscore(self,extensions:Dict[str,Any],obj:Union['Doc','Span','Token'],start:Optional[int]=None,end:Optional[int]=None)
spacy.tokens.underscore.Underscore.__dir__(self)->List[str]
spacy.tokens.underscore.Underscore.__getattr__(self,name:str)->Any
spacy.tokens.underscore.Underscore.__setattr__(self,name:str,value:Any)
spacy.tokens.underscore.Underscore._get_key(self,name:str)->Tuple[str, str, Optional[int], Optional[int]]
spacy.tokens.underscore.Underscore.get(self,name:str)->Any
spacy.tokens.underscore.Underscore.get_state(cls)->Tuple[Dict[Any, Any], Dict[Any, Any], Dict[Any, Any]]
spacy.tokens.underscore.Underscore.has(self,name:str)->bool
spacy.tokens.underscore.Underscore.load_state(cls,state:Tuple[Dict[Any,Any],Dict[Any,Any],Dict[Any,Any]])->None
spacy.tokens.underscore.Underscore.set(self,name:str,value:Any)
spacy.tokens.underscore.get_ext_args(**kwargs:Any)
spacy.tokens.underscore.is_writable_attr(ext)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tokens/doc.pyi----------------------------------------
spacy.tokens.Doc(self,vocab:Vocab,words:Optional[List[str]]=...,spaces:Optional[List[bool]]=...,user_data:Optional[Dict[Any,Any]]=...,tags:Optional[List[str]]=...,pos:Optional[List[str]]=...,morphs:Optional[List[str]]=...,lemmas:Optional[List[str]]=...,heads:Optional[List[int]]=...,deps:Optional[List[str]]=...,sent_starts:Optional[List[Union[bool,int,None]]]=...,ents:Optional[List[str]]=...)
spacy.tokens.doc.Doc(self,vocab:Vocab,words:Optional[List[str]]=...,spaces:Optional[List[bool]]=...,user_data:Optional[Dict[Any,Any]]=...,tags:Optional[List[str]]=...,pos:Optional[List[str]]=...,morphs:Optional[List[str]]=...,lemmas:Optional[List[str]]=...,heads:Optional[List[int]]=...,deps:Optional[List[str]]=...,sent_starts:Optional[List[Union[bool,int,None]]]=...,ents:Optional[List[str]]=...)
spacy.tokens.doc.Doc._(self)->Underscore
spacy.tokens.doc.Doc.__bytes__(self)->bytes
spacy.tokens.doc.Doc.__iter__(self)->Iterator[Token]
spacy.tokens.doc.Doc.__len__(self)->int
spacy.tokens.doc.Doc.__repr__(self)->str
spacy.tokens.doc.Doc.__str__(self)->str
spacy.tokens.doc.Doc.__unicode__(self)->str
spacy.tokens.doc.Doc._get_array_attrs()->Tuple[Any]
spacy.tokens.doc.Doc.char_span(self,start_idx:int,end_idx:int,label:Union[int,str]=...,kb_id:Union[int,str]=...,vector:Optional[Floats1d]=...,alignment_mode:str=...)->Span
spacy.tokens.doc.Doc.copy(self)->Doc
spacy.tokens.doc.Doc.count_by(self,attr_id:int,exclude:Optional[Any]=...,counts:Optional[Any]=...)->Dict[Any, int]
spacy.tokens.doc.Doc.doc(self)->Doc
spacy.tokens.doc.Doc.extend_tensor(self,tensor:Floats2d)->None
spacy.tokens.doc.Doc.from_array(self,attrs:Union[int,str,List[Union[int,str]]],array:Ints2d)->Doc
spacy.tokens.doc.Doc.from_bytes(self,bytes_data:bytes,*,exclude:Union[List[str],Tuple[str]]=...)->Doc
spacy.tokens.doc.Doc.from_dict(self,msg:bytes,*,exclude:Union[List[str],Tuple[str]]=...)->Doc
spacy.tokens.doc.Doc.from_disk(self,path:Union[str,Path],*,exclude:Union[List[str],Tuple[str]]=...)->Doc
spacy.tokens.doc.Doc.from_docs(docs:List[Doc],ensure_whitespace:bool=...,attrs:Optional[Union[Tuple[Union[str,int]],List[Union[int,str]]]]=...)->Doc
spacy.tokens.doc.Doc.from_json(self,doc_json:Dict[str,Any]=...,validate:bool=False)->Doc
spacy.tokens.doc.Doc.get_extension(cls,name:str)->Tuple[Optional[Any], Optional[DocMethod], Optional[Callable[[Doc], Any]], Optional[Callable[[Doc, Any], None]]]
spacy.tokens.doc.Doc.get_lca_matrix(self)->Ints2d
spacy.tokens.doc.Doc.has_annotation(self,attr:Union[int,str],*,require_complete:bool=...)->bool
spacy.tokens.doc.Doc.has_extension(cls,name:str)->bool
spacy.tokens.doc.Doc.has_vector(self)->bool
spacy.tokens.doc.Doc.is_nered(self)->bool
spacy.tokens.doc.Doc.is_parsed(self)->bool
spacy.tokens.doc.Doc.is_sentenced(self)->bool
spacy.tokens.doc.Doc.is_tagged(self)->bool
spacy.tokens.doc.Doc.lang(self)->int
spacy.tokens.doc.Doc.lang_(self)->str
spacy.tokens.doc.Doc.noun_chunks(self)->Iterator[Span]
spacy.tokens.doc.Doc.remove_extension(cls,name:str)->Tuple[Optional[Any], Optional[DocMethod], Optional[Callable[[Doc], Any]], Optional[Callable[[Doc, Any], None]]]
spacy.tokens.doc.Doc.retokenize(self)->Retokenizer
spacy.tokens.doc.Doc.sents(self)->Iterator[Span]
spacy.tokens.doc.Doc.set_ents(self,entities:List[Span],*,blocked:Optional[List[Span]]=...,missing:Optional[List[Span]]=...,outside:Optional[List[Span]]=...,default:str=...)->None
spacy.tokens.doc.Doc.set_extension(cls,name:str,default:Optional[Any]=...,getter:Optional[Callable[[Doc],Any]]=...,setter:Optional[Callable[[Doc,Any],None]]=...,method:Optional[DocMethod]=...,force:bool=...)->None
spacy.tokens.doc.Doc.similarity(self,other:Union[Doc,Span,Token,Lexeme])->float
spacy.tokens.doc.Doc.text(self)->str
spacy.tokens.doc.Doc.text_with_ws(self)->str
spacy.tokens.doc.Doc.to_array(self,py_attr_ids:Union[int,str,List[Union[int,str]]])->np.ndarray[Any, np.dtype[np.float_]]
spacy.tokens.doc.Doc.to_bytes(self,*,exclude:Union[List[str],Tuple[str]]=...)->bytes
spacy.tokens.doc.Doc.to_dict(self,*,exclude:Union[List[str],Tuple[str]]=...)->bytes
spacy.tokens.doc.Doc.to_disk(self,path:Union[str,Path],*,exclude:Iterable[str]=...)->None
spacy.tokens.doc.Doc.to_json(self,underscore:Optional[List[str]]=...)->Dict[str, Any]
spacy.tokens.doc.Doc.to_utf8_array(self,nr_char:int=...)->Ints2d
spacy.tokens.doc.DocMethod(self:Doc,*args:Any,**kwargs:Any)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tokens/_serialize.py----------------------------------------
A:spacy.tokens._serialize.attrs->sorted(int_attrs)
A:spacy.tokens._serialize.array->array.reshape((array.shape[0], 1)).reshape((array.shape[0], 1))
A:spacy.tokens._serialize.spaces->spaces.reshape((spaces.shape[0], 1)).reshape((spaces.shape[0], 1))
A:spacy.tokens._serialize.orth_col->self.attrs.index(ORTH)
A:spacy.tokens._serialize.doc->doc.from_array(self.attrs, tokens).from_array(self.attrs, tokens)
A:spacy.tokens._serialize.user_data->srsly.msgpack_loads(self.user_data[i], use_list=False)
A:spacy.tokens._serialize.msg->srsly.msgpack_loads(zlib.decompress(bytes_data))
A:spacy.tokens._serialize.self.strings->set(msg['strings'])
A:spacy.tokens._serialize.lengths->numpy.frombuffer(msg['lengths'], dtype='int32')
A:spacy.tokens._serialize.flat_spaces->flat_spaces.reshape((flat_spaces.size, 1)).reshape((flat_spaces.size, 1))
A:spacy.tokens._serialize.flat_tokens->flat_tokens.reshape(shape).reshape(shape)
A:spacy.tokens._serialize.self.tokens->NumpyOps().unflatten(flat_tokens, lengths)
A:spacy.tokens._serialize.self.spaces->NumpyOps().unflatten(flat_spaces, lengths)
A:spacy.tokens._serialize.self.span_groups->srsly.msgpack_loads(zlib.decompress(bytes_data)).get('span_groups', [b'' for _ in lengths])
A:spacy.tokens._serialize.self.flags->srsly.msgpack_loads(zlib.decompress(bytes_data)).get('flags', [{} for _ in lengths])
A:spacy.tokens._serialize.self.user_data->list(msg['user_data'])
A:spacy.tokens._serialize.path->ensure_path(path)
A:spacy.tokens._serialize.doc_bin->DocBin(store_user_data=True).from_bytes(byte_string)
spacy.tokens.DocBin(self,attrs:Iterable[str]=ALL_ATTRS,store_user_data:bool=False,docs:Iterable[Doc]=SimpleFrozenList())
spacy.tokens._serialize.DocBin(self,attrs:Iterable[str]=ALL_ATTRS,store_user_data:bool=False,docs:Iterable[Doc]=SimpleFrozenList())
spacy.tokens._serialize.DocBin.__len__(self)->int
spacy.tokens._serialize.DocBin.add(self,doc:Doc)->None
spacy.tokens._serialize.DocBin.from_bytes(self,bytes_data:bytes)->'DocBin'
spacy.tokens._serialize.DocBin.from_disk(self,path:Union[str,Path])->'DocBin'
spacy.tokens._serialize.DocBin.get_docs(self,vocab:Vocab)->Iterator[Doc]
spacy.tokens._serialize.DocBin.merge(self,other:'DocBin')->None
spacy.tokens._serialize.DocBin.to_bytes(self)->bytes
spacy.tokens._serialize.DocBin.to_disk(self,path:Union[str,Path])->None
spacy.tokens._serialize.merge_bins(bins)
spacy.tokens._serialize.pickle_bin(doc_bin)
spacy.tokens._serialize.unpickle_bin(byte_string)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tokens/span.pyi----------------------------------------
spacy.tokens.Span(self,doc:Doc,start:int,end:int,label:Union[str,int]=...,vector:Optional[Floats1d]=...,vector_norm:Optional[float]=...,kb_id:Union[str,int]=...,span_id:Union[str,int]=...)
spacy.tokens.span.Span(self,doc:Doc,start:int,end:int,label:Union[str,int]=...,vector:Optional[Floats1d]=...,vector_norm:Optional[float]=...,kb_id:Union[str,int]=...,span_id:Union[str,int]=...)
spacy.tokens.span.Span._(self)->Underscore
spacy.tokens.span.Span.__hash__(self)->int
spacy.tokens.span.Span.__iter__(self)->Iterator[Token]
spacy.tokens.span.Span.__len__(self)->int
spacy.tokens.span.Span.__repr__(self)->str
spacy.tokens.span.Span.__richcmp__(self,other:Span,op:int)->bool
spacy.tokens.span.Span.as_doc(self,*,copy_user_data:bool=...)->Doc
spacy.tokens.span.Span.char_span(self,start_idx:int,end_idx:int,label:Union[int,str]=...,kb_id:Union[int,str]=...,vector:Optional[Floats1d]=...)->Span
spacy.tokens.span.Span.conjuncts(self)->Tuple[Token]
spacy.tokens.span.Span.doc(self)->Doc
spacy.tokens.span.Span.ents(self)->Tuple[Span]
spacy.tokens.span.Span.get_extension(cls,name:str)->Tuple[Optional[Any], Optional[SpanMethod], Optional[Callable[[Span], Any]], Optional[Callable[[Span, Any], None]]]
spacy.tokens.span.Span.get_lca_matrix(self)->Ints2d
spacy.tokens.span.Span.has_extension(cls,name:str)->bool
spacy.tokens.span.Span.has_vector(self)->bool
spacy.tokens.span.Span.lefts(self)->Iterator[Token]
spacy.tokens.span.Span.lemma_(self)->str
spacy.tokens.span.Span.n_lefts(self)->int
spacy.tokens.span.Span.n_rights(self)->int
spacy.tokens.span.Span.noun_chunks(self)->Iterator[Span]
spacy.tokens.span.Span.orth_(self)->str
spacy.tokens.span.Span.remove_extension(cls,name:str)->Tuple[Optional[Any], Optional[SpanMethod], Optional[Callable[[Span], Any]], Optional[Callable[[Span, Any], None]]]
spacy.tokens.span.Span.rights(self)->Iterator[Token]
spacy.tokens.span.Span.root(self)->Token
spacy.tokens.span.Span.sent(self)->Span
spacy.tokens.span.Span.sentiment(self)->float
spacy.tokens.span.Span.set_extension(cls,name:str,default:Optional[Any]=...,getter:Optional[Callable[[Span],Any]]=...,setter:Optional[Callable[[Span,Any],None]]=...,method:Optional[SpanMethod]=...,force:bool=...)->None
spacy.tokens.span.Span.similarity(self,other:Union[Doc,Span,Token,Lexeme])->float
spacy.tokens.span.Span.subtree(self)->Iterator[Token]
spacy.tokens.span.Span.tensor(self)->FloatsXd
spacy.tokens.span.Span.text(self)->str
spacy.tokens.span.Span.text_with_ws(self)->str
spacy.tokens.span.Span.vector(self)->Floats1d
spacy.tokens.span.Span.vector_norm(self)->float
spacy.tokens.span.Span.vocab(self)->Vocab
spacy.tokens.span.SpanMethod(self:Span,*args:Any,**kwargs:Any)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tokens/_retokenize.pyi----------------------------------------
spacy.tokens._retokenize.Retokenizer(self,doc:Doc)
spacy.tokens._retokenize.Retokenizer.__enter__(self)->Retokenizer
spacy.tokens._retokenize.Retokenizer.__exit__(self,*args:Any)->None
spacy.tokens._retokenize.Retokenizer.merge(self,span:Span,attrs:Dict[Union[str,int],Any]=...)->None
spacy.tokens._retokenize.Retokenizer.split(self,token:Token,orths:List[str],heads:List[Union[Token,Tuple[Token,int]]],attrs:Dict[Union[str,int],List[Any]]=...)->None
spacy.tokens._retokenize.normalize_token_attrs(vocab:Vocab,attrs:Dict)
spacy.tokens._retokenize.set_token_attrs(py_token:Token,attrs:Dict)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tokens/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tokens/token.pyi----------------------------------------
spacy.tokens.Token(self,vocab:Vocab,doc:Doc,offset:int)
spacy.tokens.token.Token(self,vocab:Vocab,doc:Doc,offset:int)
spacy.tokens.token.Token._(self)->Underscore
spacy.tokens.token.Token.__bytes__(self)->bytes
spacy.tokens.token.Token.__hash__(self)->int
spacy.tokens.token.Token.__len__(self)->int
spacy.tokens.token.Token.__repr__(self)->str
spacy.tokens.token.Token.__richcmp__(self,other:Token,op:int)->bool
spacy.tokens.token.Token.__str__(self)->str
spacy.tokens.token.Token.__unicode__(self)->str
spacy.tokens.token.Token.ancestors(self)->Iterator[Token]
spacy.tokens.token.Token.children(self)->Iterator[Token]
spacy.tokens.token.Token.cluster(self)->int
spacy.tokens.token.Token.conjuncts(self)->Tuple[Token]
spacy.tokens.token.Token.ent_iob(self)->int
spacy.tokens.token.Token.ent_iob_(self)->str
spacy.tokens.token.Token.get_extension(cls,name:str)->Tuple[Optional[Any], Optional[TokenMethod], Optional[Callable[[Token], Any]], Optional[Callable[[Token, Any], None]]]
spacy.tokens.token.Token.has_dep(self)->bool
spacy.tokens.token.Token.has_extension(cls,name:str)->bool
spacy.tokens.token.Token.has_head(self)->bool
spacy.tokens.token.Token.has_morph(self)->bool
spacy.tokens.token.Token.has_vector(self)->bool
spacy.tokens.token.Token.idx(self)->int
spacy.tokens.token.Token.iob_strings(cls)->Tuple[str]
spacy.tokens.token.Token.is_alpha(self)->bool
spacy.tokens.token.Token.is_ancestor(self,descendant:Token)->bool
spacy.tokens.token.Token.is_ascii(self)->bool
spacy.tokens.token.Token.is_bracket(self)->bool
spacy.tokens.token.Token.is_currency(self)->bool
spacy.tokens.token.Token.is_digit(self)->bool
spacy.tokens.token.Token.is_left_punct(self)->bool
spacy.tokens.token.Token.is_lower(self)->bool
spacy.tokens.token.Token.is_oov(self)->bool
spacy.tokens.token.Token.is_punct(self)->bool
spacy.tokens.token.Token.is_quote(self)->bool
spacy.tokens.token.Token.is_right_punct(self)->bool
spacy.tokens.token.Token.is_space(self)->bool
spacy.tokens.token.Token.is_stop(self)->bool
spacy.tokens.token.Token.is_title(self)->bool
spacy.tokens.token.Token.is_upper(self)->bool
spacy.tokens.token.Token.lang(self)->int
spacy.tokens.token.Token.lang_(self)->str
spacy.tokens.token.Token.left_edge(self)->Token
spacy.tokens.token.Token.lefts(self)->Iterator[Token]
spacy.tokens.token.Token.lex(self)->Lexeme
spacy.tokens.token.Token.lex_id(self)->int
spacy.tokens.token.Token.like_email(self)->bool
spacy.tokens.token.Token.like_num(self)->bool
spacy.tokens.token.Token.like_url(self)->bool
spacy.tokens.token.Token.lower(self)->int
spacy.tokens.token.Token.lower_(self)->str
spacy.tokens.token.Token.n_lefts(self)->int
spacy.tokens.token.Token.n_rights(self)->int
spacy.tokens.token.Token.nbor(self,i:int=...)->Token
spacy.tokens.token.Token.norm(self)->int
spacy.tokens.token.Token.orth(self)->int
spacy.tokens.token.Token.orth_(self)->str
spacy.tokens.token.Token.prefix(self)->int
spacy.tokens.token.Token.prefix_(self)->str
spacy.tokens.token.Token.prob(self)->float
spacy.tokens.token.Token.rank(self)->int
spacy.tokens.token.Token.remove_extension(cls,name:str)->Tuple[Optional[Any], Optional[TokenMethod], Optional[Callable[[Token], Any]], Optional[Callable[[Token, Any], None]]]
spacy.tokens.token.Token.right_edge(self)->Token
spacy.tokens.token.Token.rights(self)->Iterator[Token]
spacy.tokens.token.Token.sent(self)->Span
spacy.tokens.token.Token.sentiment(self)->float
spacy.tokens.token.Token.set_extension(cls,name:str,default:Optional[Any]=...,getter:Optional[Callable[[Token],Any]]=...,setter:Optional[Callable[[Token,Any],None]]=...,method:Optional[TokenMethod]=...,force:bool=...)->None
spacy.tokens.token.Token.shape(self)->int
spacy.tokens.token.Token.shape_(self)->str
spacy.tokens.token.Token.similarity(self,other:Union[Doc,Span,Token,Lexeme])->float
spacy.tokens.token.Token.subtree(self)->Iterator[Token]
spacy.tokens.token.Token.suffix(self)->int
spacy.tokens.token.Token.suffix_(self)->str
spacy.tokens.token.Token.tensor(self)->Optional[FloatsXd]
spacy.tokens.token.Token.text(self)->str
spacy.tokens.token.Token.text_with_ws(self)->str
spacy.tokens.token.Token.vector(self)->Floats1d
spacy.tokens.token.Token.vector_norm(self)->float
spacy.tokens.token.Token.whitespace_(self)->str
spacy.tokens.token.TokenMethod(self:Token,*args:Any,**kwargs:Any)


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tokens/span_group.pyi----------------------------------------
spacy.tokens.SpanGroup(self,doc:Doc,*,name:str=...,attrs:Dict[str,Any]=...,spans:Iterable[Span]=...)
spacy.tokens.span_group.SpanGroup(self,doc:Doc,*,name:str=...,attrs:Dict[str,Any]=...,spans:Iterable[Span]=...)
spacy.tokens.span_group.SpanGroup.__getitem__(self,i:int)->Span
spacy.tokens.span_group.SpanGroup.__iter__(self)
spacy.tokens.span_group.SpanGroup.__len__(self)->int
spacy.tokens.span_group.SpanGroup.__repr__(self)->str
spacy.tokens.span_group.SpanGroup.append(self,span:Span)->None
spacy.tokens.span_group.SpanGroup.copy(self,doc:Optional[Doc]=...)->SpanGroup
spacy.tokens.span_group.SpanGroup.doc(self)->Doc
spacy.tokens.span_group.SpanGroup.extend(self,spans:Iterable[Span])->None
spacy.tokens.span_group.SpanGroup.from_bytes(self,bytes_data:bytes)->SpanGroup
spacy.tokens.span_group.SpanGroup.has_overlap(self)->bool
spacy.tokens.span_group.SpanGroup.to_bytes(self)->bytes


----------------------------------------/home/zhang/Packages/spacy/spacy3.5.0/tokens/_dict_proxies.py----------------------------------------
A:spacy.tokens._dict_proxies._EMPTY_BYTES->srsly.msgpack_dumps([])
A:spacy.tokens._dict_proxies.self.doc_ref->weakref.ref(doc)
A:spacy.tokens._dict_proxies.value->self._make_span_group(key, value)
A:spacy.tokens._dict_proxies.doc->self.doc_ref()
A:spacy.tokens._dict_proxies.default->self._make_span_group(key, spans)
A:spacy.tokens._dict_proxies.group->SpanGroup(doc).from_bytes(value_bytes)
A:spacy.tokens._dict_proxies.self[key]->SpanGroup(doc).from_bytes(value_bytes).copy()
spacy.tokens._dict_proxies.SpanGroups(self,doc:'Doc',items:Iterable[Tuple[str,SpanGroup]]=tuple())
spacy.tokens._dict_proxies.SpanGroups.__setitem__(self,key:str,value:Union[SpanGroup,Iterable['Span']])->None
spacy.tokens._dict_proxies.SpanGroups._ensure_doc(self)->'Doc'
spacy.tokens._dict_proxies.SpanGroups._make_span_group(self,name:str,spans:Iterable['Span'])->SpanGroup
spacy.tokens._dict_proxies.SpanGroups.copy(self,doc:Optional['Doc']=None)->'SpanGroups'
spacy.tokens._dict_proxies.SpanGroups.from_bytes(self,bytes_data:bytes)->'SpanGroups'
spacy.tokens._dict_proxies.SpanGroups.setdefault(self,key,default=None)
spacy.tokens._dict_proxies.SpanGroups.to_bytes(self)->bytes

