
----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/__main__.py----------------------------------------
A:spacy.__main__.msg->Printer()
A:spacy.__main__.command->sys.argv.pop(1)
A:spacy.__main__.available->'Available: {}'.format(', '.join(commands))


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/__init__.py----------------------------------------
A:spacy.__init__.depr_path->overrides.get('path')
A:spacy.__init__.LangClass->util.get_lang_class(name)
spacy.__init__.blank(name,**kwargs)
spacy.__init__.info(model=None,markdown=False,silent=False)
spacy.__init__.load(name,**overrides)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lemmatizer.py----------------------------------------
A:spacy.lemmatizer.lookup_table->self.lookups.get_table('lemma_lookup', {})
A:spacy.lemmatizer.index_table->self.lookups.get_table('lemma_index', {})
A:spacy.lemmatizer.exc_table->self.lookups.get_table('lemma_exc', {})
A:spacy.lemmatizer.rules_table->self.lookups.get_table('lemma_rules', {})
A:spacy.lemmatizer.lemmas->self.lemmatize(string, index_table.get(univ_pos, {}), exc_table.get(univ_pos, {}), rules_table.get(univ_pos, []))
A:spacy.lemmatizer.string->string.lower().lower()
A:spacy.lemmatizer.forms->list(OrderedDict.fromkeys(forms))
spacy.lemmatizer.Lemmatizer(self,lookups,*args,**kwargs)
spacy.lemmatizer.Lemmatizer.__init__(self,lookups,*args,**kwargs)
spacy.lemmatizer.Lemmatizer.adj(self,string,morphology=None)
spacy.lemmatizer.Lemmatizer.adp(self,string,morphology=None)
spacy.lemmatizer.Lemmatizer.det(self,string,morphology=None)
spacy.lemmatizer.Lemmatizer.is_base_form(self,univ_pos,morphology=None)
spacy.lemmatizer.Lemmatizer.lemmatize(self,string,index,exceptions,rules)
spacy.lemmatizer.Lemmatizer.load(cls,*args,**kwargs)
spacy.lemmatizer.Lemmatizer.lookup(self,string,orth=None)
spacy.lemmatizer.Lemmatizer.noun(self,string,morphology=None)
spacy.lemmatizer.Lemmatizer.num(self,string,morphology=None)
spacy.lemmatizer.Lemmatizer.pron(self,string,morphology=None)
spacy.lemmatizer.Lemmatizer.punct(self,string,morphology=None)
spacy.lemmatizer.Lemmatizer.verb(self,string,morphology=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/_ml.py----------------------------------------
A:spacy._ml.xp->get_array_module(yh)
A:spacy._ml.norm1->get_array_module(yh).linalg.norm(vec1)
A:spacy._ml.norm2->get_array_module(yh).linalg.norm(vec2)
A:spacy._ml.learn_rate->util.env_opt('learn_rate', 0.001)
A:spacy._ml.beta1->util.env_opt('optimizer_B1', 0.9)
A:spacy._ml.beta2->util.env_opt('optimizer_B2', 0.999)
A:spacy._ml.eps->util.env_opt('optimizer_eps', 1e-08)
A:spacy._ml.L2->util.env_opt('L2_penalty', 1e-06)
A:spacy._ml.max_grad_norm->util.env_opt('grad_norm_clip', 1.0)
A:spacy._ml.optimizer->Adam(ops, learn_rate, L2=L2, beta1=beta1, beta2=beta2, eps=eps)
A:spacy._ml.lengths->NumpyOps().asarray([len(X) for X in Xs], dtype='i')
A:spacy._ml.X->NumpyOps().flatten(seqs, pad=0)
A:spacy._ml.(cpu_outputs, backprop)->wrap(concatenate_lists_fwd, concat).begin_update(_to_cpu(inputs), drop=drop)
A:spacy._ml.gpu_outputs->_to_device(ops, cpu_outputs)
A:spacy._ml.cpu_d_outputs->_to_cpu(d_outputs)
A:spacy._ml.unigrams->doc.to_array([self.attr])
A:spacy._ml.keys->self.ops.xp.concatenate(ngrams)
A:spacy._ml.(keys, vals)->self.ops.xp.unique(keys, return_counts=True)
A:spacy._ml.batch_keys->self.ops.xp.concatenate(batch_keys)
A:spacy._ml.batch_vals->self.ops.asarray(self.ops.xp.concatenate(batch_vals), dtype='f')
A:spacy._ml.Yf->self._add_padding(Yf)
A:spacy._ml.(dY, ids)->self._backprop_padding(dY, ids)
A:spacy._ml.Xf->Xf.reshape((Xf.shape[0], self.nF * self.nI)).reshape((Xf.shape[0], self.nF * self.nI))
A:spacy._ml.dY->backprop(dY, sgd=sgd)
A:spacy._ml.Wopfi->Wopfi.reshape((self.nO * self.nP, self.nF * self.nI)).reshape((self.nO * self.nP, self.nF * self.nI))
A:spacy._ml.dXf->self.ops.gemm(dY.reshape((dY.shape[0], self.nO * self.nP)), Wopfi)
A:spacy._ml.dWopfi->dWopfi.reshape((self.nO, self.nP, self.nF, self.nI)).reshape((self.nO, self.nP, self.nF, self.nI))
A:spacy._ml.Yf_padded->self.ops.xp.vstack((self.pad, Yf))
A:spacy._ml.mask->numpy.random.uniform(0.0, 1.0, (N,))
A:spacy._ml.ids->NumpyOps().asarray(ids, dtype='i')
A:spacy._ml.tokvecs->NumpyOps().allocate((5000, model.nI), dtype='f')
A:spacy._ml.hiddens->hiddens.reshape((hiddens.shape[0] * model.nF, model.nO * model.nP)).reshape((hiddens.shape[0] * model.nF, model.nO * model.nP))
A:spacy._ml.vectors->concatenate_lists(trained_vectors, static_vectors)
A:spacy._ml.acts1->predict(ids, tokvecs)
A:spacy._ml.var->wrap(concatenate_lists_fwd, concat).ops.xp.var(acts1)
A:spacy._ml.mean->wrap(concatenate_lists_fwd, concat).ops.xp.mean(acts1)
A:spacy._ml.data->NumpyOps().asarray(vectors.data)
A:spacy._ml.model->wrap(concatenate_lists_fwd, concat)
A:spacy._ml.pretrained_vectors->cfg.get('pretrained_vectors', None)
A:spacy._ml.cnn_maxout_pieces->cfg.get('cnn_maxout_pieces', 3)
A:spacy._ml.subword_features->cfg.get('subword_features', True)
A:spacy._ml.char_embed->cfg.get('char_embed', True)
A:spacy._ml.conv_depth->cfg.get('conv_depth', 2)
A:spacy._ml.bilstm_depth->kwargs.get('bilstm_depth', 0)
A:spacy._ml.norm->HashEmbed(width, embed_size, column=cols.index(NORM), name='embed_norm')
A:spacy._ml.prefix->HashEmbed(width // 2, nr_vector, column=2)
A:spacy._ml.suffix->HashEmbed(width // 2, nr_vector, column=3)
A:spacy._ml.shape->HashEmbed(width // 2, nr_vector, column=4)
A:spacy._ml.glove->StaticVectors(pretrained_vectors, width, column=cols.index(ID))
A:spacy._ml.embed->concatenate_lists(CharacterEmbed(nM=64, nC=8), FeatureExtracter(cols) >> with_flatten(norm))
A:spacy._ml.reduce_dimensions->LN(Maxout(width, 64 * 8 + width, pieces=cnn_maxout_pieces))
A:spacy._ml.convolution->Residual(ExtractWindow(nW=1) >> LN(Maxout(width, width * 3, pieces=cnn_maxout_pieces)))
A:spacy._ml.(Y, backprop)->layer.begin_update(X, drop=drop)
A:spacy._ml.ops->NumpyOps()
A:spacy._ml.output->NumpyOps().xp.ascontiguousarray(X[:, idx], dtype=X.dtype)
A:spacy._ml.dX->NumpyOps().allocate(X.shape)
A:spacy._ml.self.nO->sum(out_sizes)
A:spacy._ml.output__BO->self.predict(input__BI)
A:spacy._ml.grad__BI->self.ops.gemm(grad__BO, self.W)
A:spacy._ml.embed_size->util.env_opt('embed_size', 7000)
A:spacy._ml.token_vector_width->util.env_opt('token_vector_width', 128)
A:spacy._ml.tok2vec->Tok2Vec(width=hidden_width, embed_size=embed_width, pretrained_vectors=pretrained_vectors, cnn_maxout_pieces=cnn_maxout_pieces, subword_features=True, conv_depth=conv_depth, bilstm_depth=0)
A:spacy._ml.softmax->with_flatten(MultiSoftmax(class_nums, token_vector_width))
A:spacy._ml.indices->numpy.zeros((len(doc),), dtype='i')
A:spacy._ml.depth->cfg.get('depth', 2)
A:spacy._ml.nr_vector->cfg.get('nr_vector', 5000)
A:spacy._ml.pretrained_dims->cfg.get('pretrained_dims', 0)
A:spacy._ml.lower->HashEmbed(width, nr_vector, column=1)
A:spacy._ml.linear_model->build_bow_text_classifier(nr_class, ngram_size=cfg.get('ngram_size', 1), exclusive_classes=False)
A:spacy._ml.output_layer->Softmax(nr_class, tok2vec.nO)
A:spacy._ml.model.tok2vec->chain(tok2vec, flatten)
A:spacy._ml.context_width->cfg.get('entity_width')
A:spacy._ml.drop_factor->kwargs.get('drop_factor', 1.0)
A:spacy._ml.concat->concatenate(*layers)
A:spacy._ml.(flat_y, bp_flat_y)->concatenate(*layers).begin_update(Xs, drop=drop)
A:spacy._ml.ys->NumpyOps().unflatten(flat_y, lengths)
A:spacy._ml.random_words->_RandomWords(vocab)
A:spacy._ml.(mask, docs)->_apply_mask(docs, random_words, mask_prob=mask_prob)
A:spacy._ml.(output, backprop)->wrap(concatenate_lists_fwd, concat).begin_update(docs, drop=drop)
A:spacy._ml.self.probs->numpy.exp(numpy.array(self.probs, dtype='f'))
A:spacy._ml.index->self._cache.pop()
A:spacy._ml.N->sum((len(doc) for doc in docs))
A:spacy._ml.word->_replace_word(token.text, random_words)
A:spacy._ml.roll->numpy.random.random()
A:spacy._ml.nCv->self.ops.xp.arange(self.nC)
A:spacy._ml.doc_ids->doc.to_utf8_array(nr_char=self.nC)
A:spacy._ml.doc_vectors->self.ops.allocate((len(doc), self.nC, self.nM))
A:spacy._ml.d_doc_vectors->d_doc_vectors.reshape((len(doc_ids), self.nC, self.nM)).reshape((len(doc_ids), self.nC, self.nM))
A:spacy._ml.norm_yh->get_array_module(yh).linalg.norm(yh, axis=1, keepdims=True)
A:spacy._ml.norm_y->get_array_module(yh).linalg.norm(y, axis=1, keepdims=True)
A:spacy._ml.loss->get_array_module(yh).abs(cosine - 1).sum()
spacy._ml.CharacterEmbed(self,nM=None,nC=None,**kwargs)
spacy._ml.CharacterEmbed.__init__(self,nM=None,nC=None,**kwargs)
spacy._ml.CharacterEmbed.begin_update(self,docs,drop=0.0)
spacy._ml.CharacterEmbed.nO(self)
spacy._ml.CharacterEmbed.nV(self)
spacy._ml.MultiSoftmax(self,out_sizes,nI=None,**kwargs)
spacy._ml.MultiSoftmax.__init__(self,out_sizes,nI=None,**kwargs)
spacy._ml.MultiSoftmax.begin_update(self,input__BI,drop=0.0)
spacy._ml.MultiSoftmax.predict(self,input__BI)
spacy._ml.PrecomputableAffine(self,nO=None,nI=None,nF=None,nP=None,**kwargs)
spacy._ml.PrecomputableAffine.__init__(self,nO=None,nI=None,nF=None,nP=None,**kwargs)
spacy._ml.PrecomputableAffine._add_padding(self,Yf)
spacy._ml.PrecomputableAffine._backprop_padding(self,dY,ids)
spacy._ml.PrecomputableAffine.begin_update(self,X,drop=0.0)
spacy._ml.PrecomputableAffine.init_weights(model)
spacy._ml.PyTorchBiLSTM(nO,nI,depth,dropout=0.2)
spacy._ml.SpacyVectors(docs,drop=0.0)
spacy._ml.Tok2Vec(width,embed_size,**kwargs)
spacy._ml._RandomWords(self,vocab)
spacy._ml._RandomWords.__init__(self,vocab)
spacy._ml._RandomWords.next(self)
spacy._ml._apply_mask(docs,random_words,mask_prob=0.15)
spacy._ml._divide_array(X,size)
spacy._ml._flatten_add_lengths(seqs,pad=0,drop=0.0)
spacy._ml._replace_word(word,random_words,mask='[MASK]')
spacy._ml._to_cpu(X)
spacy._ml._to_device(ops,X)
spacy._ml._uniform_init(lo,hi)
spacy._ml._zero_init(model)
spacy._ml.asarray(ops,dtype)
spacy._ml.build_bow_text_classifier(nr_class,ngram_size=1,exclusive_classes=False,no_output_layer=False,**cfg)
spacy._ml.build_morphologizer_model(class_nums,**cfg)
spacy._ml.build_nel_encoder(embed_width,hidden_width,ner_types,**cfg)
spacy._ml.build_simple_cnn_text_classifier(tok2vec,nr_class,exclusive_classes=False,**cfg)
spacy._ml.build_tagger_model(nr_class,**cfg)
spacy._ml.build_text_classifier(nr_class,width=64,**cfg)
spacy._ml.concatenate_lists(*layers,**kwargs)
spacy._ml.cosine(vec1,vec2)
spacy._ml.cpu_softmax(X,drop=0.0)
spacy._ml.create_default_optimizer(ops,**cfg)
spacy._ml.doc2feats(cols=None)
spacy._ml.extract_ngrams(self,ngram_size,attr=LOWER)
spacy._ml.extract_ngrams.__init__(self,ngram_size,attr=LOWER)
spacy._ml.extract_ngrams.begin_update(self,docs,drop=0.0)
spacy._ml.flatten(seqs,drop=0.0)
spacy._ml.get_col(idx)
spacy._ml.get_cossim_loss(yh,y)
spacy._ml.get_token_vectors(tokens_attrs_vectors,drop=0.0)
spacy._ml.getitem(i)
spacy._ml.link_vectors_to_models(vocab)
spacy._ml.logistic(X,drop=0.0)
spacy._ml.masked_language_model(vocab,model,mask_prob=0.15)
spacy._ml.print_shape(prefix)
spacy._ml.reapply(layer,n_times)
spacy._ml.with_cpu(ops,model)
spacy._ml.zero_init(model)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/scorer.py----------------------------------------
A:spacy.scorer.self.saved_score->_roc_auc_score(self.golds, self.cands)
A:spacy.scorer.self.saved_score_at_len->len(self.golds)
A:spacy.scorer.self.tokens->PRFScore()
A:spacy.scorer.self.sbd->PRFScore()
A:spacy.scorer.self.unlabelled->PRFScore()
A:spacy.scorer.self.labelled->PRFScore()
A:spacy.scorer.self.tags->PRFScore()
A:spacy.scorer.self.ner->PRFScore()
A:spacy.scorer.self.ner_per_ents->dict()
A:spacy.scorer.self.textcat_per_cat->dict()
A:spacy.scorer.self.textcat_positive_label->model.cfg.get('positive_label', None)
A:spacy.scorer.self.textcat->PRFScore()
A:spacy.scorer.self.textcat_per_cat[label]->PRFScore()
A:spacy.scorer.gold->gold.GoldParse.from_annot_tuples(doc, zip(*gold.orig_annot))
A:spacy.scorer.gold_deps->set()
A:spacy.scorer.gold_tags->set()
A:spacy.scorer.gold_ents->set(tags_to_entities([annot[-1] for annot in gold.orig_annot]))
A:spacy.scorer.cand_deps->set()
A:spacy.scorer.cand_tags->set()
A:spacy.scorer.ent_labels->set([x[0] for x in gold_ents] + [k.label_ for k in doc.ents])
A:spacy.scorer.self.ner_per_ents[ent_label]->PRFScore()
A:spacy.scorer.cand_ents->set()
A:spacy.scorer.goldcat->max(gold.cats, key=gold.cats.get)
A:spacy.scorer.candcat->max(doc.cats, key=doc.cats.get)
A:spacy.scorer.model_labels->set(self.textcat_per_cat)
A:spacy.scorer.eval_labels->set(gold.cats)
A:spacy.scorer.(fpr, tpr, _)->_roc_curve(y_true, y_score)
A:spacy.scorer.(fps, tps, thresholds)->_binary_clf_curve(y_true, y_score)
A:spacy.scorer.fpr->numpy.repeat(np.nan, fps.shape)
A:spacy.scorer.tpr->numpy.repeat(np.nan, tps.shape)
A:spacy.scorer.y_true->numpy.ravel(y_true)
A:spacy.scorer.y_score->numpy.ravel(y_score)
A:spacy.scorer.out->numpy.cumsum(arr, axis=axis, dtype=np.float64)
A:spacy.scorer.expected->numpy.sum(arr, axis=axis, dtype=np.float64)
A:spacy.scorer.x->numpy.ravel(x)
A:spacy.scorer.y->numpy.ravel(y)
A:spacy.scorer.dx->numpy.diff(x)
A:spacy.scorer.area->area.dtype.type(area).dtype.type(area)
spacy.scorer.PRFScore(self)
spacy.scorer.PRFScore.__init__(self)
spacy.scorer.PRFScore.fscore(self)
spacy.scorer.PRFScore.precision(self)
spacy.scorer.PRFScore.recall(self)
spacy.scorer.PRFScore.score_set(self,cand,gold)
spacy.scorer.ROCAUCScore(self)
spacy.scorer.ROCAUCScore.__init__(self)
spacy.scorer.ROCAUCScore.score(self)
spacy.scorer.ROCAUCScore.score_set(self,cand,gold)
spacy.scorer.Scorer(self,eval_punct=False,pipeline=None)
spacy.scorer.Scorer.__init__(self,eval_punct=False,pipeline=None)
spacy.scorer.Scorer.ents_f(self)
spacy.scorer.Scorer.ents_p(self)
spacy.scorer.Scorer.ents_per_type(self)
spacy.scorer.Scorer.ents_r(self)
spacy.scorer.Scorer.las(self)
spacy.scorer.Scorer.score(self,doc,gold,verbose=False,punct_labels=('p','punct'))
spacy.scorer.Scorer.scores(self)
spacy.scorer.Scorer.tags_acc(self)
spacy.scorer.Scorer.textcat_score(self)
spacy.scorer.Scorer.textcats_per_cat(self)
spacy.scorer.Scorer.token_acc(self)
spacy.scorer.Scorer.uas(self)
spacy.scorer._auc(x,y)
spacy.scorer._binary_clf_curve(y_true,y_score)
spacy.scorer._roc_auc_score(y_true,y_score)
spacy.scorer._roc_curve(y_true,y_score)
spacy.scorer._stable_cumsum(arr,axis=None,rtol=1e-05,atol=1e-08)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/glossary.py----------------------------------------
spacy.explain(term)
spacy.glossary.explain(term)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/about.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/language.py----------------------------------------
A:spacy.language.lookups->cls.create_lookups(nlp)
A:spacy.language.root->util.get_module_path(cls)
A:spacy.language.lang->cls.lex_attr_getters[LANG](None)
A:spacy.language.user_lookups->util.get_entry_point(util.ENTRY_POINTS.lookups, lang, {})
A:spacy.language.data->util.load_language_data(filename)
A:spacy.language.lemmatizer->cls.create_lemmatizer(nlp, lookups=lookups)
A:spacy.language.lex_attr_getters->dict(cls.lex_attr_getters)
A:spacy.language.lex_attr_getters[IS_STOP]->functools.partial(is_stop, stops=cls.stop_words)
A:spacy.language.vocab->factory(self, **meta.get('vocab', {}))
A:spacy.language.prefixes->tuple(TOKENIZER_PREFIXES)
A:spacy.language.suffixes->tuple(TOKENIZER_SUFFIXES)
A:spacy.language.infixes->tuple(TOKENIZER_INFIXES)
A:spacy.language.tag_map->dict(TAG_MAP)
A:spacy.language.stop_words->set()
A:spacy.language.user_factories->util.get_entry_points(util.ENTRY_POINTS.factories)
A:spacy.language.self._meta->dict(meta)
A:spacy.language.vocab.vectors.name->meta.get('vectors', {}).get('name')
A:spacy.language.make_doc->factory(self, **meta.get('tokenizer', {}))
A:spacy.language.labels->OrderedDict()
A:spacy.language.labels[name]->list(pipe.labels)
A:spacy.language.msg->errors.Errors.E003.format(component=repr(component), name=name)
A:spacy.language.name->repr(component)
A:spacy.language.i->self.pipe_names.index(old_name)
A:spacy.language.doc->func(doc, **kwargs)
A:spacy.language.err->errors.Errors.E151.format(unexp=unexpected, exp=expected_keys)
A:spacy.language.gold->GoldParse(doc, **gold)
A:spacy.language.self._optimizer->create_default_optimizer(Model.ops)
A:spacy.language.(docs, golds)->zip(*docs_golds)
A:spacy.language.pipes->list(self.pipeline)
A:spacy.language.kwargs->dict(kwargs)
A:spacy.language.docs->_pipe(proc, docs, kwargs)
A:spacy.language.docs[i]->self.make_doc(doc)
A:spacy.language.docs_golds->proc.preprocess_gold(docs_golds)
A:spacy.language._->annots_brackets.pop()
A:spacy.language.self.vocab.vectors.data->thinc.neural.Model.ops.asarray(self.vocab.vectors.data)
A:spacy.language.sgd->create_default_optimizer(Model.ops)
A:spacy.language.proc._rehearsal_model->deepcopy(proc.model)
A:spacy.language.scorer->Scorer(pipeline=self.pipeline)
A:spacy.language.golds->list(golds)
A:spacy.language.(text_context1, text_context2)->itertools.tee(texts)
A:spacy.language.recent_refs->weakref.WeakSet()
A:spacy.language.old_refs->weakref.WeakSet()
A:spacy.language.original_strings_data->list(self.vocab.strings)
A:spacy.language.(keys, strings)->self.vocab.strings._cleanup_stale_strings(original_strings_data)
A:spacy.language.path->util.ensure_path(path)
A:spacy.language.serializers->OrderedDict()
A:spacy.language.deserializers->OrderedDict()
A:spacy.language.exclude->util.get_serialization_exclude(deserializers, exclude, kwargs)
A:spacy.language.self.original_pipeline->copy(nlp.pipeline)
spacy.language.BaseDefaults(object)
spacy.language.BaseDefaults.create_lemmatizer(cls,nlp=None,lookups=None)
spacy.language.BaseDefaults.create_lookups(cls,nlp=None)
spacy.language.BaseDefaults.create_tokenizer(cls,nlp=None)
spacy.language.BaseDefaults.create_vocab(cls,nlp=None)
spacy.language.DisabledPipes(self,nlp,*names)
spacy.language.DisabledPipes.__enter__(self)
spacy.language.DisabledPipes.__exit__(self,*args)
spacy.language.DisabledPipes.__init__(self,nlp,*names)
spacy.language.DisabledPipes.restore(self)
spacy.language.Language(self,vocab=True,make_doc=True,max_length=10**6,meta={},**kwargs)
spacy.language.Language.__init__(self,vocab=True,make_doc=True,max_length=10**6,meta={},**kwargs)
spacy.language.Language._format_docs_and_golds(self,docs,golds)
spacy.language.Language.add_pipe(self,component,name=None,before=None,after=None,first=None,last=None)
spacy.language.Language.begin_training(self,get_gold_tuples=None,sgd=None,component_cfg=None,**cfg)
spacy.language.Language.create_pipe(self,name,config=dict())
spacy.language.Language.disable_pipes(self,*names)
spacy.language.Language.entity(self)
spacy.language.Language.evaluate(self,docs_golds,verbose=False,batch_size=256,scorer=None,component_cfg=None)
spacy.language.Language.from_bytes(self,bytes_data,exclude=tuple(),disable=None,**kwargs)
spacy.language.Language.from_disk(self,path,exclude=tuple(),disable=None)
spacy.language.Language.get_pipe(self,name)
spacy.language.Language.has_pipe(self,name)
spacy.language.Language.linker(self)
spacy.language.Language.make_doc(self,text)
spacy.language.Language.matcher(self)
spacy.language.Language.meta(self)
spacy.language.Language.meta(self,value)
spacy.language.Language.parser(self)
spacy.language.Language.path(self)
spacy.language.Language.pipe(self,texts,as_tuples=False,n_threads=-1,batch_size=1000,disable=[],cleanup=False,component_cfg=None)
spacy.language.Language.pipe_labels(self)
spacy.language.Language.pipe_names(self)
spacy.language.Language.preprocess_gold(self,docs_golds)
spacy.language.Language.rehearse(self,docs,sgd=None,losses=None,config=None)
spacy.language.Language.remove_pipe(self,name)
spacy.language.Language.rename_pipe(self,old_name,new_name)
spacy.language.Language.replace_pipe(self,name,component)
spacy.language.Language.resume_training(self,sgd=None,**cfg)
spacy.language.Language.tagger(self)
spacy.language.Language.tensorizer(self)
spacy.language.Language.to_bytes(self,exclude=tuple(),disable=None,**kwargs)
spacy.language.Language.to_disk(self,path,exclude=tuple(),disable=None)
spacy.language.Language.update(self,docs,golds,drop=0.0,sgd=None,losses=None,component_cfg=None)
spacy.language.Language.use_params(self,params,**cfg)
spacy.language._fix_pretrained_vectors_name(nlp)
spacy.language._pipe(func,docs,kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/compat.py----------------------------------------
A:spacy.compat.izip->getattr(itertools, 'izip', zip)
A:spacy.compat.is_windows->sys.platform.startswith('win')
A:spacy.compat.is_linux->sys.platform.startswith('linux')
A:spacy.compat.loc->path2str(loc)
A:spacy.compat.spec->importlib.util.spec_from_file_location(name, str(loc))
A:spacy.compat.module->importlib.util.module_from_spec(spec)
A:spacy.compat.string->string.replace('\\\\U', '\\U').replace('\\\\U', '\\U')
spacy.compat.b_to_str(b_str)
spacy.compat.import_file(name,loc)
spacy.compat.is_config(python2=None,python3=None,windows=None,linux=None,osx=None)
spacy.compat.symlink_remove(link)
spacy.compat.symlink_to(orig,dest)
spacy.compat.unescape_unicode(string)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/errors.py----------------------------------------
A:spacy.errors.msg->"Invalid token patterns for matcher rule '{}'\n".format(key)
A:spacy.errors.pattern_errors->'\n'.join(['- {}'.format(e) for e in error_msgs])
A:spacy.errors.SPACY_WARNING_FILTER->os.environ.get('SPACY_WARNING_FILTER')
A:spacy.errors.SPACY_WARNING_TYPES->_get_warn_types(os.environ.get('SPACY_WARNING_TYPES'))
A:spacy.errors.SPACY_WARNING_IGNORE->_get_warn_excl(os.environ.get('SPACY_WARNING_IGNORE'))
spacy.Errors(object)
spacy.Warnings(object)
spacy.deprecation_warning(message)
spacy.errors.Errors(object)
spacy.errors.MatchPatternError(self,key,errors)
spacy.errors.MatchPatternError.__init__(self,key,errors)
spacy.errors.ModelsWarning(UserWarning)
spacy.errors.TempErrors(object)
spacy.errors.Warnings(object)
spacy.errors._get_warn_excl(arg)
spacy.errors._get_warn_types(arg)
spacy.errors._warn(message,warn_type='user')
spacy.errors.add_codes(err_cls)
spacy.errors.deprecation_warning(message)
spacy.errors.models_warning(message)
spacy.errors.user_warning(message)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lookups.py----------------------------------------
A:spacy.lookups.UNSET->object()
A:spacy.lookups.self._tables->OrderedDict()
A:spacy.lookups.table->Table(name=name, data=data)
A:spacy.lookups.self._tables[key]->Table(key)
A:spacy.lookups.path->ensure_path(path)
A:spacy.lookups.data->srsly.msgpack_loads(bytes_data).get('dict', {})
A:spacy.lookups.self->cls(name=name)
A:spacy.lookups.self.bloom->BloomFilter().from_bytes(loaded['bloom'])
A:spacy.lookups.key->get_string_id(key)
A:spacy.lookups.loaded->srsly.msgpack_loads(bytes_data)
spacy.lookups.Lookups(self)
spacy.lookups.Lookups.__contains__(self,name)
spacy.lookups.Lookups.__init__(self)
spacy.lookups.Lookups.__len__(self)
spacy.lookups.Lookups.add_table(self,name,data=SimpleFrozenDict())
spacy.lookups.Lookups.from_bytes(self,bytes_data,**kwargs)
spacy.lookups.Lookups.from_disk(self,path,**kwargs)
spacy.lookups.Lookups.get_table(self,name,default=UNSET)
spacy.lookups.Lookups.has_table(self,name)
spacy.lookups.Lookups.remove_table(self,name)
spacy.lookups.Lookups.tables(self)
spacy.lookups.Lookups.to_bytes(self,**kwargs)
spacy.lookups.Lookups.to_disk(self,path,**kwargs)
spacy.lookups.Table(self,name=None,data=None)
spacy.lookups.Table.__contains__(self,key)
spacy.lookups.Table.__getitem__(self,key)
spacy.lookups.Table.__init__(self,name=None,data=None)
spacy.lookups.Table.__setitem__(self,key,value)
spacy.lookups.Table.from_bytes(self,bytes_data)
spacy.lookups.Table.from_dict(cls,data,name=None)
spacy.lookups.Table.get(self,key,default=None)
spacy.lookups.Table.set(self,key,value)
spacy.lookups.Table.to_bytes(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/util.py----------------------------------------
A:spacy.util.entry_point->get_entry_point(ENTRY_POINTS.languages, lang)
A:spacy.util.module->importlib.import_module('.lang.%s' % lang, 'spacy')
A:spacy.util.LANGUAGES[lang]->getattr(module, module.__all__[0])
A:spacy.util._data_path->ensure_path(path)
A:spacy.util.path->ensure_path(path)
A:spacy.util.data_path->get_data_path()
A:spacy.util.cls->get_lang_class(lang)
A:spacy.util.meta->srsly.read_json(meta_path)
A:spacy.util.lang->srsly.read_json(meta_path).get('lang_factory', meta['lang'])
A:spacy.util.nlp->cls(meta=meta, **overrides)
A:spacy.util.pipeline->srsly.read_json(meta_path).get('pipeline', [])
A:spacy.util.disable->overrides.get('disable', [])
A:spacy.util.config->srsly.read_json(meta_path).get('pipeline_args', {}).get(name, {})
A:spacy.util.component->cls(meta=meta, **overrides).create_pipe(name, config=config)
A:spacy.util.model_path->ensure_path(path)
A:spacy.util.name->name.lower().lower()
A:spacy.util.packages->pkg_resources.working_set.by_key.keys()
A:spacy.util.pkg->importlib.import_module(name)
A:spacy.util.result[entry_point.name]->get_entry_point(ENTRY_POINTS.languages, lang).load()
A:spacy.util.array->compat.cupy.ndarray(numpy_array.shape, order='C', dtype=numpy_array.dtype)
A:spacy.util.value->type_convert(os.environ[name])
A:spacy.util.entries->file_.read().split('\n')
A:spacy.util.expression->'|'.join([piece for piece in entries if piece.strip()])
A:spacy.util.exc->expand_exc(exc, "'", '’')
A:spacy.util.described_orth->''.join((attr[ORTH] for attr in token_attrs))
A:spacy.util.fixed->dict(token)
A:spacy.util.fixed[ORTH]->fixed[ORTH].replace(search, replace).replace(search, replace)
A:spacy.util.new_excs->dict(excs)
A:spacy.util.new_key->token_string.replace(search, replace)
A:spacy.util.start->min(length, max(0, start))
A:spacy.util.stop->min(length, max(start, stop))
A:spacy.util.size_->itertools.repeat(size)
A:spacy.util.items->iter(items)
A:spacy.util.batch_size->next(size_)
A:spacy.util.batch->list(itertools.islice(items, int(batch_size)))
A:spacy.util.curr->float(start)
A:spacy.util.(doc, gold)->next(items)
A:spacy.util.doc->next(items)
A:spacy.util.iterable->iter(iterable)
A:spacy.util.sorted_spans->sorted(spans, key=get_sort_key, reverse=True)
A:spacy.util.seen_tokens->set()
A:spacy.util.result->sorted(result, key=lambda span: span.start)
A:spacy.util.serialized->OrderedDict()
A:spacy.util.serialized[key]->getter()
A:spacy.util.msg->srsly.msgpack_loads(bytes_data)
A:spacy.util.text->text.replace('"', '&quot;').replace('"', '&quot;')
A:spacy.util.device->compat.cupy.cuda.device.Device(gpu_id)
A:spacy.util.Model.ops->CupyOps()
A:spacy.util.validator->get_json_validator(schema)
A:spacy.util.err_path->'[{}]'.format(' -> '.join([str(p) for p in err.path]))
A:spacy.util.exclude->list(exclude)
spacy.util.DummyTokenizer(object)
spacy.util.DummyTokenizer.from_bytes(self,_bytes_data,**kwargs)
spacy.util.DummyTokenizer.from_disk(self,_path,**kwargs)
spacy.util.DummyTokenizer.to_bytes(self,**kwargs)
spacy.util.DummyTokenizer.to_disk(self,_path,**kwargs)
spacy.util.ENTRY_POINTS(object)
spacy.util.SimpleFrozenDict(dict)
spacy.util.SimpleFrozenDict.__setitem__(self,key,value)
spacy.util.SimpleFrozenDict.pop(self,key,default=None)
spacy.util.SimpleFrozenDict.update(self,other)
spacy.util._get_attr_unless_lookup(default_func,lookups,string)
spacy.util.add_lookups(default_func,*lookups)
spacy.util.compile_infix_regex(entries)
spacy.util.compile_prefix_regex(entries)
spacy.util.compile_suffix_regex(entries)
spacy.util.compounding(start,stop,compound)
spacy.util.decaying(start,stop,decay)
spacy.util.ensure_path(path)
spacy.util.env_opt(name,default=None)
spacy.util.escape_html(text)
spacy.util.expand_exc(excs,search,replace)
spacy.util.filter_spans(spans)
spacy.util.fix_random_seed(seed=0)
spacy.util.from_bytes(bytes_data,setters,exclude)
spacy.util.from_disk(path,readers,exclude)
spacy.util.get_async(stream,numpy_array)
spacy.util.get_cuda_stream(require=False)
spacy.util.get_data_path(require_exists=True)
spacy.util.get_entry_point(key,value,default=None)
spacy.util.get_entry_points(key)
spacy.util.get_json_validator(schema)
spacy.util.get_lang_class(lang)
spacy.util.get_model_meta(path)
spacy.util.get_module_path(module)
spacy.util.get_package_path(name)
spacy.util.get_serialization_exclude(serializers,exclude,kwargs)
spacy.util.is_in_jupyter()
spacy.util.is_package(name)
spacy.util.itershuffle(iterable,bufsize=1000)
spacy.util.lang_class_is_loaded(lang)
spacy.util.load_language_data(path)
spacy.util.load_model(name,**overrides)
spacy.util.load_model_from_init_py(init_file,**overrides)
spacy.util.load_model_from_link(name,**overrides)
spacy.util.load_model_from_package(name,**overrides)
spacy.util.load_model_from_path(model_path,meta=False,**overrides)
spacy.util.minibatch(items,size=8)
spacy.util.minibatch_by_words(items,size,tuples=True,count_words=len)
spacy.util.minify_html(html)
spacy.util.normalize_slice(length,start,stop,step=None)
spacy.util.read_regex(path)
spacy.util.set_data_path(path)
spacy.util.set_env_log(value)
spacy.util.set_lang_class(name,cls)
spacy.util.stepping(start,stop,steps)
spacy.util.to_bytes(getters,exclude)
spacy.util.to_disk(path,writers,exclude)
spacy.util.update_exc(base_exceptions,*addition_dicts)
spacy.util.use_gpu(gpu_id)
spacy.util.validate_json(data,validator)
spacy.util.validate_schema(schema)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/pipeline/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/pipeline/entityruler.py----------------------------------------
A:spacy.pipeline.entityruler.self.overwrite->srsly.msgpack_loads(patterns_bytes).get('overwrite', False)
A:spacy.pipeline.entityruler.self.token_patterns->defaultdict(list)
A:spacy.pipeline.entityruler.self.phrase_patterns->defaultdict(list)
A:spacy.pipeline.entityruler.self.matcher->Matcher(nlp.vocab, validate=validate)
A:spacy.pipeline.entityruler.self.phrase_matcher->PhraseMatcher(self.nlp.vocab, attr=self.phrase_matcher_attr)
A:spacy.pipeline.entityruler.self.ent_id_sep->srsly.msgpack_loads(patterns_bytes).get('ent_id_sep', DEFAULT_ENT_ID_SEP)
A:spacy.pipeline.entityruler.patterns->srsly.read_jsonl(depr_patterns_path)
A:spacy.pipeline.entityruler.n_token_patterns->sum((len(p) for p in self.token_patterns.values()))
A:spacy.pipeline.entityruler.n_phrase_patterns->sum((len(p) for p in self.phrase_patterns.values()))
A:spacy.pipeline.entityruler.matches->sorted(matches, key=get_sort_key, reverse=True)
A:spacy.pipeline.entityruler.entities->list(doc.ents)
A:spacy.pipeline.entityruler.seen_tokens->set()
A:spacy.pipeline.entityruler.(ent_label, ent_id)->'{}{}{}'.format(label, self.ent_id_sep, ent_id).rsplit(self.ent_id_sep, 1)
A:spacy.pipeline.entityruler.span->Span(doc, start, end, label=match_id)
A:spacy.pipeline.entityruler.all_labels->set(self.token_patterns.keys())
A:spacy.pipeline.entityruler.all_ent_ids->set()
A:spacy.pipeline.entityruler.(_, ent_id)->self._split_label(l)
A:spacy.pipeline.entityruler.current_index->self.nlp.pipe_names.index(self.name)
A:spacy.pipeline.entityruler.label->'{}{}{}'.format(label, self.ent_id_sep, ent_id)
A:spacy.pipeline.entityruler.cfg->srsly.msgpack_loads(patterns_bytes)
A:spacy.pipeline.entityruler.self.phrase_matcher_attr->srsly.msgpack_loads(patterns_bytes).get('phrase_matcher_attr')
A:spacy.pipeline.entityruler.serial->OrderedDict((('overwrite', self.overwrite), ('ent_id_sep', self.ent_id_sep), ('phrase_matcher_attr', self.phrase_matcher_attr), ('patterns', self.patterns)))
A:spacy.pipeline.entityruler.path->ensure_path(path)
A:spacy.pipeline.entityruler.depr_patterns_path->ensure_path(path).with_suffix('.jsonl')
spacy.pipeline.EntityRuler(self,nlp,phrase_matcher_attr=None,validate=False,**cfg)
spacy.pipeline.EntityRuler.__contains__(self,label)
spacy.pipeline.EntityRuler.__len__(self)
spacy.pipeline.EntityRuler._create_label(self,label,ent_id)
spacy.pipeline.EntityRuler._split_label(self,label)
spacy.pipeline.EntityRuler.add_patterns(self,patterns)
spacy.pipeline.EntityRuler.ent_ids(self)
spacy.pipeline.EntityRuler.from_bytes(self,patterns_bytes,**kwargs)
spacy.pipeline.EntityRuler.from_disk(self,path,**kwargs)
spacy.pipeline.EntityRuler.labels(self)
spacy.pipeline.EntityRuler.patterns(self)
spacy.pipeline.EntityRuler.to_bytes(self,**kwargs)
spacy.pipeline.EntityRuler.to_disk(self,path,**kwargs)
spacy.pipeline.entityruler.EntityRuler(self,nlp,phrase_matcher_attr=None,validate=False,**cfg)
spacy.pipeline.entityruler.EntityRuler.__contains__(self,label)
spacy.pipeline.entityruler.EntityRuler.__init__(self,nlp,phrase_matcher_attr=None,validate=False,**cfg)
spacy.pipeline.entityruler.EntityRuler.__len__(self)
spacy.pipeline.entityruler.EntityRuler._create_label(self,label,ent_id)
spacy.pipeline.entityruler.EntityRuler._split_label(self,label)
spacy.pipeline.entityruler.EntityRuler.add_patterns(self,patterns)
spacy.pipeline.entityruler.EntityRuler.ent_ids(self)
spacy.pipeline.entityruler.EntityRuler.from_bytes(self,patterns_bytes,**kwargs)
spacy.pipeline.entityruler.EntityRuler.from_disk(self,path,**kwargs)
spacy.pipeline.entityruler.EntityRuler.labels(self)
spacy.pipeline.entityruler.EntityRuler.patterns(self)
spacy.pipeline.entityruler.EntityRuler.to_bytes(self,**kwargs)
spacy.pipeline.entityruler.EntityRuler.to_disk(self,path,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/pipeline/functions.py----------------------------------------
A:spacy.pipeline.functions.merger->Matcher(doc.vocab)
A:spacy.pipeline.functions.matches->merger(doc)
spacy.pipeline.functions.merge_entities(doc)
spacy.pipeline.functions.merge_noun_chunks(doc)
spacy.pipeline.functions.merge_subtokens(doc,label='subtok')
spacy.pipeline.merge_entities(doc)
spacy.pipeline.merge_noun_chunks(doc)
spacy.pipeline.merge_subtokens(doc,label='subtok')


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/pipeline/hooks.py----------------------------------------
A:spacy.pipeline.hooks.self.cfg->dict(cfg)
A:spacy.pipeline.hooks.(sims, bp_sims)->self.model.begin_update(doc1_doc2, drop=drop)
A:spacy.pipeline.hooks.self.model->self.Model(pipeline[0].model.nO)
A:spacy.pipeline.hooks.sgd->self.create_optimizer()
spacy.pipeline.SentenceSegmenter(self,vocab,strategy=None)
spacy.pipeline.SentenceSegmenter.split_on_punct(doc)
spacy.pipeline.SimilarityHook(self,vocab,model=True,**cfg)
spacy.pipeline.SimilarityHook.Model(cls,length)
spacy.pipeline.SimilarityHook.begin_training(self,_=tuple(),pipeline=None,sgd=None,**kwargs)
spacy.pipeline.SimilarityHook.pipe(self,docs,**kwargs)
spacy.pipeline.SimilarityHook.predict(self,doc1,doc2)
spacy.pipeline.SimilarityHook.update(self,doc1_doc2,golds,sgd=None,drop=0.0)
spacy.pipeline.hooks.SentenceSegmenter(self,vocab,strategy=None)
spacy.pipeline.hooks.SentenceSegmenter.__init__(self,vocab,strategy=None)
spacy.pipeline.hooks.SentenceSegmenter.split_on_punct(doc)
spacy.pipeline.hooks.SimilarityHook(self,vocab,model=True,**cfg)
spacy.pipeline.hooks.SimilarityHook.Model(cls,length)
spacy.pipeline.hooks.SimilarityHook.__init__(self,vocab,model=True,**cfg)
spacy.pipeline.hooks.SimilarityHook.begin_training(self,_=tuple(),pipeline=None,sgd=None,**kwargs)
spacy.pipeline.hooks.SimilarityHook.pipe(self,docs,**kwargs)
spacy.pipeline.hooks.SimilarityHook.predict(self,doc1,doc2)
spacy.pipeline.hooks.SimilarityHook.update(self,doc1_doc2,golds,sgd=None,drop=0.0)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/test_lemmatizer.py----------------------------------------
A:spacy.tests.test_lemmatizer.nlp->Language()
A:spacy.tests.test_lemmatizer.table->Language().vocab.lookups.add_table('lemma_lookup')
A:spacy.tests.test_lemmatizer.new_nlp->Language()
A:spacy.tests.test_lemmatizer.nlp_bytes->Language().to_bytes()
A:spacy.tests.test_lemmatizer.nlp.vocab.lookups->Lookups()
A:spacy.tests.test_lemmatizer.tagger->Language().create_pipe('tagger')
spacy.tests.test_lemmatizer.test_lemmatizer_reflects_lookups_changes()
spacy.tests.test_lemmatizer.test_tagger_warns_no_lemma_lookups()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/test_scorer.py----------------------------------------
A:spacy.tests.test_scorer.scorer->Scorer()
A:spacy.tests.test_scorer.doc->get_doc(en_vocab, words=input_.split(' '), ents=[[0, 1, 'ORG'], [5, 6, 'GPE'], [6, 7, 'ORG']])
A:spacy.tests.test_scorer.gold->GoldParse(doc, entities=annot['entities'])
A:spacy.tests.test_scorer.(tpr, fpr, _)->_roc_curve(y_true, y_score)
A:spacy.tests.test_scorer.roc_auc->_roc_auc_score(y_true, y_score)
A:spacy.tests.test_scorer.score->ROCAUCScore()
spacy.tests.test_scorer.test_ner_per_type(en_vocab)
spacy.tests.test_scorer.test_roc_auc_score()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/test_misc.py----------------------------------------
A:spacy.tests.test_misc.path->spacy.util.get_package_path(package)
A:spacy.tests.test_misc.model->PrecomputableAffine(nO=nO, nI=nI, nF=nF, nP=nP)
A:spacy.tests.test_misc.tensor->PrecomputableAffine(nO=nO, nI=nI, nF=nF, nP=nP).ops.allocate((10, nI))
A:spacy.tests.test_misc.(Y, get_dX)->PrecomputableAffine(nO=nO, nI=nI, nF=nF, nP=nP).begin_update(tensor)
A:spacy.tests.test_misc.dY->PrecomputableAffine(nO=nO, nI=nI, nF=nF, nP=nP).ops.allocate((15, nO, nP))
A:spacy.tests.test_misc.ids->PrecomputableAffine(nO=nO, nI=nI, nF=nF, nP=nP).ops.allocate((15, nF))
spacy.tests.test_misc.is_admin()
spacy.tests.test_misc.symlink()
spacy.tests.test_misc.symlink_setup_target(request,symlink_target,symlink)
spacy.tests.test_misc.symlink_target()
spacy.tests.test_misc.test_PrecomputableAffine(nO=4,nI=5,nF=3,nP=2)
spacy.tests.test_misc.test_ascii_filenames()
spacy.tests.test_misc.test_create_symlink_windows(symlink_setup_target,symlink_target,symlink,is_admin)
spacy.tests.test_misc.test_prefer_gpu()
spacy.tests.test_misc.test_require_gpu()
spacy.tests.test_misc.test_util_ensure_path_succeeds(text)
spacy.tests.test_misc.test_util_get_package_path(package)
spacy.tests.test_misc.test_util_is_package(package)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/test_language.py----------------------------------------
A:spacy.tests.test_language.nlp->Language(Vocab())
A:spacy.tests.test_language.textcat->Language(Vocab()).create_pipe('textcat')
A:spacy.tests.test_language.doc->Doc(nlp.vocab, words=text.split(' '))
A:spacy.tests.test_language.gold->GoldParse(doc, **annots)
spacy.tests.test_language.nlp()
spacy.tests.test_language.test_language_evaluate(nlp)
spacy.tests.test_language.test_language_update(nlp)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/test_gold.py----------------------------------------
A:spacy.tests.test_gold.doc->nlp(text)
A:spacy.tests.test_gold.tags->biluo_tags_from_offsets(doc, entities)
A:spacy.tests.test_gold.biluo_tags_converted->biluo_tags_from_offsets(doc, offsets)
A:spacy.tests.test_gold.offsets_converted->offsets_from_biluo_tags(doc, biluo_tags)
A:spacy.tests.test_gold.spans->spans_from_biluo_tags(doc, biluo_tags)
A:spacy.tests.test_gold.gold->GoldParse(doc, entities=biluo_tags)
A:spacy.tests.test_gold.nlp->English()
A:spacy.tests.test_gold.goldcorpus->GoldCorpus(str(json_file), str(json_file))
A:spacy.tests.test_gold.(reloaded_doc, goldparse)->next(goldcorpus.train_docs(nlp))
spacy.tests.test_gold.test_biluo_spans(en_tokenizer)
spacy.tests.test_gold.test_gold_biluo_BIL(en_vocab)
spacy.tests.test_gold.test_gold_biluo_BL(en_vocab)
spacy.tests.test_gold.test_gold_biluo_U(en_vocab)
spacy.tests.test_gold.test_gold_biluo_misalign(en_vocab)
spacy.tests.test_gold.test_gold_biluo_overlap(en_vocab)
spacy.tests.test_gold.test_gold_ner_missing_tags(en_tokenizer)
spacy.tests.test_gold.test_roundtrip_docs_to_json()
spacy.tests.test_gold.test_roundtrip_offsets_biluo_conversion(en_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/test_align.py----------------------------------------
A:spacy.tests.test_align.(output_cost, i2j, j2i, matrix)->align(string1, string2)
A:spacy.tests.test_align.(output_cost, output_i2j, j2i, matrix)->align(string1, string2)
A:spacy.tests.test_align.(output_cost, output_i2j, output_j2i, matrix)->align(string1, string2)
A:spacy.tests.test_align.(cost, i2j, j2i, matrix)->align(words1, words2)
A:spacy.tests.test_align.(i2j_multi, j2i_multi)->multi_align(i2j, j2i, lengths1, lengths2)
spacy.tests.test_align.test_align_costs(string1,string2,cost)
spacy.tests.test_align.test_align_i2j(string1,string2,i2j)
spacy.tests.test_align.test_align_i2j_2(string1,string2,j2i)
spacy.tests.test_align.test_align_many_to_one()
spacy.tests.test_align.test_align_strings()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/test_cli.py----------------------------------------
A:spacy.tests.test_cli.input_data->'\n'.join(lines)
A:spacy.tests.test_cli.converted->conll_ner2json(input_data, n_sents=10)
A:spacy.tests.test_cli.nlp->English()
A:spacy.tests.test_cli.(docs, skip_count)->make_docs(nlp, [too_long_jsonl], 1, 5)
spacy.tests.test_cli.test_cli_converters_conll_ner2json()
spacy.tests.test_cli.test_cli_converters_conllu2json()
spacy.tests.test_cli.test_cli_converters_iob2json()
spacy.tests.test_cli.test_pretrain_make_docs()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/test_json_schemas.py----------------------------------------
A:spacy.tests.test_json_schemas.errors->validate_json([data], training_schema_validator)
spacy.tests.test_json_schemas.test_json_schema_training_invalid(data,n_errors,training_schema_validator)
spacy.tests.test_json_schemas.test_json_schema_training_valid(data,training_schema_validator)
spacy.tests.test_json_schemas.test_schemas(schema)
spacy.tests.test_json_schemas.test_validate_schema()
spacy.tests.test_json_schemas.training_schema_validator()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/test_displacy.py----------------------------------------
A:spacy.tests.test_displacy.doc->get_doc(en_vocab, words=['But', 'Google', 'is', 'starting', 'from', 'behind'])
A:spacy.tests.test_displacy.ents->spacy.displacy.parse_ents(doc)
A:spacy.tests.test_displacy.deps->spacy.displacy.parse_deps(doc)
A:spacy.tests.test_displacy.renderer->DependencyRenderer()
A:spacy.tests.test_displacy.html->spacy.displacy.render(doc, style='ent')
A:spacy.tests.test_displacy.nlp->Persian()
spacy.tests.test_displacy.test_displacy_invalid_arcs()
spacy.tests.test_displacy.test_displacy_parse_deps(en_vocab)
spacy.tests.test_displacy.test_displacy_parse_ents(en_vocab)
spacy.tests.test_displacy.test_displacy_raises_for_wrong_type(en_vocab)
spacy.tests.test_displacy.test_displacy_render_wrapper(en_vocab)
spacy.tests.test_displacy.test_displacy_rtl()
spacy.tests.test_displacy.test_displacy_spans(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/util.py----------------------------------------
A:spacy.tests.util.f->tempfile.TemporaryFile(mode=mode)
A:spacy.tests.util.d->Path(tempfile.mkdtemp())
A:spacy.tests.util.doc->Doc(vocab, words=words)
A:spacy.tests.util.attrs->Doc(vocab, words=words).to_array([POS, HEAD, DEP])
A:spacy.tests.util.(move, label)->action_name.split('-')
A:spacy.tests.util.length->len(vectors[0][1])
A:spacy.tests.util.msg1->srsly.msgpack_loads(b1)
A:spacy.tests.util.msg2->srsly.msgpack_loads(b2)
spacy.tests.util.add_vecs_to_vocab(vocab,vectors)
spacy.tests.util.apply_transition_sequence(parser,doc,sequence)
spacy.tests.util.assert_docs_equal(doc1,doc2)
spacy.tests.util.assert_packed_msg_equal(b1,b2)
spacy.tests.util.get_cosine(vec1,vec2)
spacy.tests.util.get_doc(vocab,words=[],pos=None,heads=None,deps=None,tags=None,ents=None)
spacy.tests.util.make_tempdir()
spacy.tests.util.make_tempfile(mode='r')


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/conftest.py----------------------------------------
A:spacy.tests.conftest.nlp->get_lang_class('en')(en_vocab)
spacy.tests.conftest.ar_tokenizer()
spacy.tests.conftest.bn_tokenizer()
spacy.tests.conftest.ca_tokenizer()
spacy.tests.conftest.da_tokenizer()
spacy.tests.conftest.de_tokenizer()
spacy.tests.conftest.el_tokenizer()
spacy.tests.conftest.en_parser(en_vocab)
spacy.tests.conftest.en_tokenizer()
spacy.tests.conftest.en_vocab()
spacy.tests.conftest.es_tokenizer()
spacy.tests.conftest.fi_tokenizer()
spacy.tests.conftest.fr_tokenizer()
spacy.tests.conftest.ga_tokenizer()
spacy.tests.conftest.he_tokenizer()
spacy.tests.conftest.hr_tokenizer()
spacy.tests.conftest.hu_tokenizer()
spacy.tests.conftest.id_tokenizer()
spacy.tests.conftest.it_tokenizer()
spacy.tests.conftest.ja_tokenizer()
spacy.tests.conftest.ko_tokenizer()
spacy.tests.conftest.lt_tokenizer()
spacy.tests.conftest.nb_tokenizer()
spacy.tests.conftest.nl_tokenizer()
spacy.tests.conftest.pl_tokenizer()
spacy.tests.conftest.pt_tokenizer()
spacy.tests.conftest.pytest_addoption(parser)
spacy.tests.conftest.pytest_runtest_setup(item)
spacy.tests.conftest.ro_tokenizer()
spacy.tests.conftest.ru_lemmatizer()
spacy.tests.conftest.ru_tokenizer()
spacy.tests.conftest.sr_tokenizer()
spacy.tests.conftest.sv_tokenizer()
spacy.tests.conftest.th_tokenizer()
spacy.tests.conftest.tokenizer()
spacy.tests.conftest.tr_tokenizer()
spacy.tests.conftest.tt_tokenizer()
spacy.tests.conftest.uk_tokenizer()
spacy.tests.conftest.ur_tokenizer()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/test_pickles.py----------------------------------------
A:spacy.tests.test_pickles.stringstore->StringStore()
A:spacy.tests.test_pickles.data->srsly.pickle_dumps(vocab)
A:spacy.tests.test_pickles.unpickled->srsly.pickle_loads(data)
A:spacy.tests.test_pickles.vocab->Vocab(lex_attr_getters={int(NORM): lambda string: string[:-1]})
spacy.tests.test_pickles.test_pickle_string_store(text1,text2)
spacy.tests.test_pickles.test_pickle_vocab(text1,text2)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/parser/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/parser/test_ner.py----------------------------------------
A:spacy.tests.parser.test_ner.actions->spacy.syntax.ner.BiluoPushDown.get_actions(entity_types=entity_types)
A:spacy.tests.parser.test_ner.gold->GoldParse(doc, words=words, entities=biluo_tags)
A:spacy.tests.parser.test_ner.act_classes->tsys.get_oracle_sequence(doc, gold)
A:spacy.tests.parser.test_ner.doc->nlp('This is Antti L Korhonen speaking in Finland')
A:spacy.tests.parser.test_ner.moves->BiluoPushDown(en_vocab.strings)
A:spacy.tests.parser.test_ner.(action, label)->tag.split('-')
A:spacy.tests.parser.test_ner.seq->BiluoPushDown(en_vocab.strings).get_oracle_sequence(doc, gold)
A:spacy.tests.parser.test_ner.nlp1->English()
A:spacy.tests.parser.test_ner.doc1->nlp1('I live in New York')
A:spacy.tests.parser.test_ner.ner1->English().create_pipe('ner')
A:spacy.tests.parser.test_ner.nlp2->English()
A:spacy.tests.parser.test_ner.doc2->nlp2('I live in New York')
A:spacy.tests.parser.test_ner.ner2->EntityRecognizer(doc.vocab)
A:spacy.tests.parser.test_ner.nlp->English()
A:spacy.tests.parser.test_ner.ruler->EntityRuler(nlp)
A:spacy.tests.parser.test_ner.untrained_ner->English().create_pipe('ner')
spacy.tests.parser.test_ner.BlockerComponent1(self,start,end)
spacy.tests.parser.test_ner.BlockerComponent1.__init__(self,start,end)
spacy.tests.parser.test_ner.doc(vocab)
spacy.tests.parser.test_ner.entity_annots(doc)
spacy.tests.parser.test_ner.entity_types(entity_annots)
spacy.tests.parser.test_ner.test_accept_blocked_token()
spacy.tests.parser.test_ner.test_block_ner()
spacy.tests.parser.test_ner.test_get_oracle_moves(tsys,doc,entity_annots)
spacy.tests.parser.test_ner.test_get_oracle_moves_negative_O(tsys,vocab)
spacy.tests.parser.test_ner.test_get_oracle_moves_negative_entities(tsys,doc,entity_annots)
spacy.tests.parser.test_ner.test_get_oracle_moves_negative_entities2(tsys,vocab)
spacy.tests.parser.test_ner.test_ner_before_ruler()
spacy.tests.parser.test_ner.test_oracle_moves_missing_B(en_vocab)
spacy.tests.parser.test_ner.test_oracle_moves_whitespace(en_vocab)
spacy.tests.parser.test_ner.test_overwrite_token()
spacy.tests.parser.test_ner.test_ruler_before_ner()
spacy.tests.parser.test_ner.tsys(vocab,entity_types)
spacy.tests.parser.test_ner.vocab()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/parser/test_arc_eager_oracle.py----------------------------------------
A:spacy.tests.parser.test_arc_eager_oracle.doc->Doc(Vocab(), words=[t[1] for t in annot_tuples])
A:spacy.tests.parser.test_arc_eager_oracle.gold->GoldParse(doc, words=words, tags=tags, heads=heads, deps=deps)
A:spacy.tests.parser.test_arc_eager_oracle.state->StateClass(doc)
A:spacy.tests.parser.test_arc_eager_oracle.name->M.class_name(i)
A:spacy.tests.parser.test_arc_eager_oracle.state_costs[name]->M.get_cost(state, gold, i)
A:spacy.tests.parser.test_arc_eager_oracle.moves->ArcEager(vocab.strings, ArcEager.get_actions())
A:spacy.tests.parser.test_arc_eager_oracle.vocab->Vocab()
A:spacy.tests.parser.test_arc_eager_oracle.(state, cost_history)->get_sequence_costs(arc_eager, words, heads, deps, actions)
A:spacy.tests.parser.test_arc_eager_oracle.parser->DependencyParser(doc.vocab)
A:spacy.tests.parser.test_arc_eager_oracle.(ids, words, tags, heads, deps, ents)->zip(*annot_tuples)
A:spacy.tests.parser.test_arc_eager_oracle.(heads, deps)->projectivize(heads, deps)
spacy.tests.parser.test_arc_eager_oracle.arc_eager(vocab)
spacy.tests.parser.test_arc_eager_oracle.doc(words,vocab)
spacy.tests.parser.test_arc_eager_oracle.get_sequence_costs(M,words,heads,deps,transitions)
spacy.tests.parser.test_arc_eager_oracle.gold(doc,words)
spacy.tests.parser.test_arc_eager_oracle.test_get_oracle_actions()
spacy.tests.parser.test_arc_eager_oracle.test_oracle_four_words(arc_eager,vocab)
spacy.tests.parser.test_arc_eager_oracle.vocab()
spacy.tests.parser.test_arc_eager_oracle.words()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/parser/test_parse.py----------------------------------------
A:spacy.tests.parser.test_parse.tokens->en_tokenizer(text)
A:spacy.tests.parser.test_parse.doc->get_doc(tokens.vocab, words=[t.text for t in tokens], deps=deps, heads=heads, tags=tags)
spacy.tests.parser.test_parse.test_parser_arc_eager_finalize_state(en_tokenizer,en_parser)
spacy.tests.parser.test_parse.test_parser_initial(en_tokenizer,en_parser)
spacy.tests.parser.test_parse.test_parser_merge_pp(en_tokenizer)
spacy.tests.parser.test_parse.test_parser_parse_one_word_sentence(en_tokenizer,en_parser,text)
spacy.tests.parser.test_parse.test_parser_parse_subtrees(en_tokenizer,en_parser)
spacy.tests.parser.test_parse.test_parser_root(en_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/parser/test_nn_beam.py----------------------------------------
A:spacy.tests.parser.test_nn_beam.aeager->ArcEager(vocab.strings, {})
A:spacy.tests.parser.test_nn_beam.vec->numpy.random.uniform(-0.1, 0.1, (len(doc), vector_size))
A:spacy.tests.parser.test_nn_beam.nlp->Language()
A:spacy.tests.parser.test_nn_beam.doc->Language().make_doc('Australia is a country')
spacy.tests.parser.test_nn_beam.batch_size(docs)
spacy.tests.parser.test_nn_beam.beam(moves,states,golds,beam_width)
spacy.tests.parser.test_nn_beam.beam_width()
spacy.tests.parser.test_nn_beam.docs(vocab)
spacy.tests.parser.test_nn_beam.golds(docs)
spacy.tests.parser.test_nn_beam.moves(vocab)
spacy.tests.parser.test_nn_beam.scores(moves,batch_size,beam_width)
spacy.tests.parser.test_nn_beam.states(docs)
spacy.tests.parser.test_nn_beam.test_beam_advance(beam,scores)
spacy.tests.parser.test_nn_beam.test_beam_advance_too_few_scores(beam,scores)
spacy.tests.parser.test_nn_beam.test_beam_parse()
spacy.tests.parser.test_nn_beam.test_create_beam(beam)
spacy.tests.parser.test_nn_beam.tokvecs(docs,vector_size)
spacy.tests.parser.test_nn_beam.vector_size()
spacy.tests.parser.test_nn_beam.vocab()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/parser/test_neural_parser.py----------------------------------------
A:spacy.tests.parser.test_neural_parser.actions->spacy.syntax.arc_eager.ArcEager.get_actions(left_labels=['L'], right_labels=['R'])
spacy.tests.parser.test_neural_parser.arc_eager(vocab)
spacy.tests.parser.test_neural_parser.doc(vocab)
spacy.tests.parser.test_neural_parser.gold(doc)
spacy.tests.parser.test_neural_parser.model(arc_eager,tok2vec)
spacy.tests.parser.test_neural_parser.parser(vocab,arc_eager)
spacy.tests.parser.test_neural_parser.test_build_model(parser)
spacy.tests.parser.test_neural_parser.test_can_init_nn_parser(parser)
spacy.tests.parser.test_neural_parser.test_predict_doc(parser,tok2vec,model,doc)
spacy.tests.parser.test_neural_parser.test_predict_doc_beam(parser,model,doc)
spacy.tests.parser.test_neural_parser.test_update_doc(parser,model,doc,gold)
spacy.tests.parser.test_neural_parser.test_update_doc_beam(parser,model,doc,gold)
spacy.tests.parser.test_neural_parser.tok2vec()
spacy.tests.parser.test_neural_parser.vocab()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/parser/test_space_attachment.py----------------------------------------
A:spacy.tests.parser.test_space_attachment.tokens->en_tokenizer(text)
A:spacy.tests.parser.test_space_attachment.doc->Doc(en_parser.vocab, words=text)
spacy.tests.parser.test_space_attachment.test_parser_sentence_space(en_tokenizer)
spacy.tests.parser.test_space_attachment.test_parser_space_attachment(en_tokenizer)
spacy.tests.parser.test_space_attachment.test_parser_space_attachment_intermediate_trailing(en_tokenizer,en_parser)
spacy.tests.parser.test_space_attachment.test_parser_space_attachment_leading(en_tokenizer,en_parser)
spacy.tests.parser.test_space_attachment.test_parser_space_attachment_space(en_tokenizer,en_parser,text,length)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/parser/test_preset_sbd.py----------------------------------------
A:spacy.tests.parser.test_preset_sbd.parser->DependencyParser(vocab)
A:spacy.tests.parser.test_preset_sbd.sgd->Adam(NumpyOps(), 0.001)
A:spacy.tests.parser.test_preset_sbd.doc->parser(doc)
A:spacy.tests.parser.test_preset_sbd.gold->GoldParse(doc, heads=[1, 1, 3, 3], deps=['left', 'ROOT', 'left', 'ROOT'])
spacy.tests.parser.test_preset_sbd.parser(vocab)
spacy.tests.parser.test_preset_sbd.test_no_sentences(parser)
spacy.tests.parser.test_preset_sbd.test_sents_1(parser)
spacy.tests.parser.test_preset_sbd.test_sents_1_2(parser)
spacy.tests.parser.test_preset_sbd.test_sents_1_3(parser)
spacy.tests.parser.test_preset_sbd.vocab()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/parser/test_parse_navigate.py----------------------------------------
A:spacy.tests.parser.test_parse_navigate.tokens->en_tokenizer(text)
A:spacy.tests.parser.test_parse_navigate.doc->get_doc(tokens.vocab, words=[t.text for t in tokens], heads=heads)
A:spacy.tests.parser.test_parse_navigate.lefts[head.i]->set()
A:spacy.tests.parser.test_parse_navigate.rights[head.i]->set()
A:spacy.tests.parser.test_parse_navigate.subtree->list(token.subtree)
A:spacy.tests.parser.test_parse_navigate.debug->'\t'.join((token.text, token.right_edge.text, subtree[-1].text, token.right_edge.head.text))
spacy.tests.parser.test_parse_navigate.heads()
spacy.tests.parser.test_parse_navigate.test_parser_parse_navigate_child_consistency(en_tokenizer,text,heads)
spacy.tests.parser.test_parse_navigate.test_parser_parse_navigate_consistency(en_tokenizer,text,heads)
spacy.tests.parser.test_parse_navigate.test_parser_parse_navigate_edges(en_tokenizer,text,heads)
spacy.tests.parser.test_parse_navigate.text()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/parser/test_add_label.py----------------------------------------
A:spacy.tests.parser.test_add_label.parser->_train_parser(parser)
A:spacy.tests.parser.test_add_label.sgd->Adam(NumpyOps(), 0.001)
A:spacy.tests.parser.test_add_label.doc->parser(doc)
A:spacy.tests.parser.test_add_label.gold->GoldParse(doc, heads=[1, 1, 3, 3], deps=['right', 'ROOT', 'left', 'ROOT'])
A:spacy.tests.parser.test_add_label.ner1->EntityRecognizer(Vocab())
A:spacy.tests.parser.test_add_label.ner2->EntityRecognizer(Vocab()).from_bytes(ner1.to_bytes())
A:spacy.tests.parser.test_add_label.pipe->pipe_cls(Vocab())
A:spacy.tests.parser.test_add_label.pipe_labels->sorted(list(pipe.labels))
spacy.tests.parser.test_add_label._train_parser(parser)
spacy.tests.parser.test_add_label.parser(vocab)
spacy.tests.parser.test_add_label.test_add_label(parser)
spacy.tests.parser.test_add_label.test_add_label_deserializes_correctly()
spacy.tests.parser.test_add_label.test_add_label_get_label(pipe_cls,n_moves)
spacy.tests.parser.test_add_label.test_init_parser(parser)
spacy.tests.parser.test_add_label.vocab()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/parser/test_nonproj.py----------------------------------------
A:spacy.tests.parser.test_nonproj.tokens->en_tokenizer('whatever ' * len(proj_heads))
A:spacy.tests.parser.test_nonproj.doc->get_doc(tokens.vocab, words=[t.text for t in tokens], deps=deco_labels, heads=rel_proj_heads)
A:spacy.tests.parser.test_nonproj.(proj_heads, deco_labels)->spacy.syntax.nonproj.projectivize(nonproj_tree2, labels2)
A:spacy.tests.parser.test_nonproj.(deproj_heads, undeco_labels)->deprojectivize(proj_heads, deco_labels)
spacy.tests.parser.test_nonproj.cyclic_tree()
spacy.tests.parser.test_nonproj.multirooted_tree()
spacy.tests.parser.test_nonproj.nonproj_tree()
spacy.tests.parser.test_nonproj.partial_tree()
spacy.tests.parser.test_nonproj.proj_tree()
spacy.tests.parser.test_nonproj.test_parser_ancestors(tree,cyclic_tree,partial_tree,multirooted_tree)
spacy.tests.parser.test_nonproj.test_parser_contains_cycle(tree,cyclic_tree,partial_tree,multirooted_tree)
spacy.tests.parser.test_nonproj.test_parser_is_nonproj_arc(nonproj_tree,partial_tree,multirooted_tree)
spacy.tests.parser.test_nonproj.test_parser_is_nonproj_tree(proj_tree,nonproj_tree,partial_tree,multirooted_tree)
spacy.tests.parser.test_nonproj.test_parser_pseudoprojectivity(en_tokenizer)
spacy.tests.parser.test_nonproj.tree()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/pipeline/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/pipeline/test_entity_ruler.py----------------------------------------
A:spacy.tests.pipeline.test_entity_ruler.ruler->EntityRuler(nlp)
A:spacy.tests.pipeline.test_entity_ruler.doc->nlp('Apple is a technology company')
A:spacy.tests.pipeline.test_entity_ruler.ruler_bytes->EntityRuler(nlp).to_bytes()
A:spacy.tests.pipeline.test_entity_ruler.new_ruler->new_ruler.from_bytes(ruler_bytes).from_bytes(ruler_bytes)
A:spacy.tests.pipeline.test_entity_ruler.validated_ruler->EntityRuler(nlp, validate=True)
spacy.tests.pipeline.test_entity_ruler.add_ent()
spacy.tests.pipeline.test_entity_ruler.nlp()
spacy.tests.pipeline.test_entity_ruler.patterns()
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_cfg_ent_id_sep(nlp,patterns)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_entity_id(nlp,patterns)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_existing(nlp,patterns,add_ent)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_existing_complex(nlp,patterns,add_ent)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_existing_overwrite(nlp,patterns,add_ent)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_init(nlp,patterns)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_serialize_bytes(nlp,patterns)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_serialize_phrase_matcher_attr_bytes(nlp,patterns)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_validate(nlp)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/pipeline/test_pipe_methods.py----------------------------------------
A:spacy.tests.pipeline.test_pipe_methods.(removed_name, removed_component)->nlp.remove_pipe(name)
A:spacy.tests.pipeline.test_pipe_methods.disabled->nlp.disable_pipes(name)
A:spacy.tests.pipeline.test_pipe_methods.pipe->nlp.create_pipe(name)
spacy.tests.pipeline.test_pipe_methods.new_pipe(doc)
spacy.tests.pipeline.test_pipe_methods.nlp()
spacy.tests.pipeline.test_pipe_methods.test_add_lots_of_pipes(nlp,n_pipes)
spacy.tests.pipeline.test_pipe_methods.test_add_pipe_duplicate_name(nlp)
spacy.tests.pipeline.test_pipe_methods.test_add_pipe_first(nlp,name)
spacy.tests.pipeline.test_pipe_methods.test_add_pipe_last(nlp,name1,name2)
spacy.tests.pipeline.test_pipe_methods.test_add_pipe_no_name(nlp)
spacy.tests.pipeline.test_pipe_methods.test_cant_add_pipe_first_and_last(nlp)
spacy.tests.pipeline.test_pipe_methods.test_disable_pipes_context(nlp,name)
spacy.tests.pipeline.test_pipe_methods.test_disable_pipes_method(nlp,name)
spacy.tests.pipeline.test_pipe_methods.test_get_pipe(nlp,name)
spacy.tests.pipeline.test_pipe_methods.test_pipe_base_class_add_label(nlp,component)
spacy.tests.pipeline.test_pipe_methods.test_pipe_labels(nlp)
spacy.tests.pipeline.test_pipe_methods.test_raise_for_invalid_components(nlp,component)
spacy.tests.pipeline.test_pipe_methods.test_remove_pipe(nlp,name)
spacy.tests.pipeline.test_pipe_methods.test_rename_pipe(nlp,old_name,new_name)
spacy.tests.pipeline.test_pipe_methods.test_replace_pipe(nlp,name,replacement,not_callable)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/pipeline/test_sentencizer.py----------------------------------------
A:spacy.tests.pipeline.test_sentencizer.doc->sentencizer(doc)
A:spacy.tests.pipeline.test_sentencizer.sentencizer->Sentencizer(punct_chars=punct_chars)
A:spacy.tests.pipeline.test_sentencizer.bytes_data->Sentencizer(punct_chars=punct_chars).to_bytes()
A:spacy.tests.pipeline.test_sentencizer.new_sentencizer->Sentencizer().from_bytes(bytes_data)
spacy.tests.pipeline.test_sentencizer.test_sentencizer(en_vocab)
spacy.tests.pipeline.test_sentencizer.test_sentencizer_complex(en_vocab,words,sent_starts,n_sents)
spacy.tests.pipeline.test_sentencizer.test_sentencizer_custom_punct(en_vocab,punct_chars,words,sent_starts,n_sents)
spacy.tests.pipeline.test_sentencizer.test_sentencizer_serialize_bytes(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/pipeline/test_entity_linker.py----------------------------------------
A:spacy.tests.pipeline.test_entity_linker.mykb->KnowledgeBase(nlp.vocab, entity_vector_length=1)
A:spacy.tests.pipeline.test_entity_linker.sentencizer->nlp.create_pipe('sentencizer')
A:spacy.tests.pipeline.test_entity_linker.ruler->EntityRuler(nlp)
A:spacy.tests.pipeline.test_entity_linker.el_pipe->nlp.create_pipe(name='entity_linker')
A:spacy.tests.pipeline.test_entity_linker.doc->nlp(text)
A:spacy.tests.pipeline.test_entity_linker.sent_doc->ent.sent.as_doc()
A:spacy.tests.pipeline.test_entity_linker.boston_ent->Span(doc, 3, 4, label='LOC', kb_id='Q1')
A:spacy.tests.pipeline.test_entity_linker.loc->nlp(text).vocab.strings.add('LOC')
A:spacy.tests.pipeline.test_entity_linker.q1->nlp(text).vocab.strings.add('Q1')
spacy.tests.pipeline.test_entity_linker.assert_almost_equal(a,b)
spacy.tests.pipeline.test_entity_linker.nlp()
spacy.tests.pipeline.test_entity_linker.test_candidate_generation(nlp)
spacy.tests.pipeline.test_entity_linker.test_kb_invalid_combination(nlp)
spacy.tests.pipeline.test_entity_linker.test_kb_invalid_entities(nlp)
spacy.tests.pipeline.test_entity_linker.test_kb_invalid_entity_vector(nlp)
spacy.tests.pipeline.test_entity_linker.test_kb_invalid_probabilities(nlp)
spacy.tests.pipeline.test_entity_linker.test_kb_valid_entities(nlp)
spacy.tests.pipeline.test_entity_linker.test_preserving_links_asdoc(nlp)
spacy.tests.pipeline.test_entity_linker.test_preserving_links_ents(nlp)
spacy.tests.pipeline.test_entity_linker.test_preserving_links_ents_2(nlp)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/pipeline/test_textcat.py----------------------------------------
A:spacy.tests.pipeline.test_textcat.nlp->Language()
A:spacy.tests.pipeline.test_textcat.doc->Doc(nlp.vocab, words=['d'] * 3 + [w1, w2] + ['d'] * 3)
A:spacy.tests.pipeline.test_textcat.model->TextCategorizer(nlp.vocab, width=8)
A:spacy.tests.pipeline.test_textcat.optimizer->TextCategorizer(nlp.vocab, width=8).begin_training()
spacy.tests.pipeline.test_textcat.test_simple_train()
spacy.tests.pipeline.test_textcat.test_textcat_learns_multilabel()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/pipeline/test_factories.py----------------------------------------
A:spacy.tests.pipeline.test_factories.tokens->en_tokenizer(text)
A:spacy.tests.pipeline.test_factories.doc->get_doc(tokens.vocab, words=[t.text for t in tokens], heads=heads, tags=tags, pos=pos, deps=deps)
A:spacy.tests.pipeline.test_factories.nlp->Language()
A:spacy.tests.pipeline.test_factories.merge_noun_chunks->Language().create_pipe('merge_noun_chunks')
A:spacy.tests.pipeline.test_factories.merge_entities->Language().create_pipe('merge_entities')
spacy.tests.pipeline.test_factories.doc(en_tokenizer)
spacy.tests.pipeline.test_factories.test_factories_merge_ents(doc)
spacy.tests.pipeline.test_factories.test_factories_merge_noun_chunks(doc)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/morphology/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/morphology/test_morph_features.py----------------------------------------
A:spacy.tests.morphology.test_morph_features.lemmatizer->Lemmatizer(Lookups())
A:spacy.tests.morphology.test_morph_features.tag1->morphology.add({'Case_gen'})
A:spacy.tests.morphology.test_morph_features.tag2->morphology.update(tag1, {'Number_sing'})
A:spacy.tests.morphology.test_morph_features.tag3->morphology.add({'Number_sing', 'Case_gen'})
spacy.tests.morphology.test_morph_features.morphology()
spacy.tests.morphology.test_morph_features.test_add_morphology_with_int_ids(morphology)
spacy.tests.morphology.test_morph_features.test_add_morphology_with_mix_strings_and_ints(morphology)
spacy.tests.morphology.test_morph_features.test_add_morphology_with_string_names(morphology)
spacy.tests.morphology.test_morph_features.test_init(morphology)
spacy.tests.morphology.test_morph_features.test_morphology_tags_hash_distinctly(morphology)
spacy.tests.morphology.test_morph_features.test_morphology_tags_hash_independent_of_order(morphology)
spacy.tests.morphology.test_morph_features.test_update_morphology_tag(morphology)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/test_initialize.py----------------------------------------
A:spacy.tests.lang.test_initialize.nlp->get_lang_class(lang)()
A:spacy.tests.lang.test_initialize.doc->nlp('test')
A:spacy.tests.lang.test_initialize.captured->capfd.readouterr()
spacy.tests.lang.test_initialize.test_lang_initialize(lang,capfd)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/test_attrs.py----------------------------------------
A:spacy.tests.lang.test_attrs.int_attrs->intify_attrs({'F': text, 'is_alpha': True}, strings_map={text: 10}, _do_deprecated=True)
spacy.tests.lang.test_attrs.test_attrs_do_deprecated(text)
spacy.tests.lang.test_attrs.test_attrs_idempotence(text)
spacy.tests.lang.test_attrs.test_attrs_key(text)
spacy.tests.lang.test_attrs.test_lex_attrs_is_ascii(text,match)
spacy.tests.lang.test_attrs.test_lex_attrs_is_currency(text,match)
spacy.tests.lang.test_attrs.test_lex_attrs_is_punct(text,match)
spacy.tests.lang.test_attrs.test_lex_attrs_like_url(text,match)
spacy.tests.lang.test_attrs.test_lex_attrs_word_shape(text,shape)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/ga/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/ga/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.ga.test_tokenizer.tokens->ga_tokenizer(text)
spacy.tests.lang.ga.test_tokenizer.test_ga_tokenizer_handles_exception_cases(ga_tokenizer,text,expected_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/it/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/it/test_prefix_suffix_infix.py----------------------------------------
A:spacy.tests.lang.it.test_prefix_suffix_infix.tokens->it_tokenizer(text)
spacy.tests.lang.it.test_prefix_suffix_infix.test_contractions(it_tokenizer,text,expected_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/en/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/en/test_indices.py----------------------------------------
A:spacy.tests.lang.en.test_indices.tokens->en_tokenizer(text)
spacy.tests.lang.en.test_indices.test_en_complex_punct(en_tokenizer)
spacy.tests.lang.en.test_indices.test_en_simple_punct(en_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/en/test_exceptions.py----------------------------------------
A:spacy.tests.lang.en.test_exceptions.tokens->en_tokenizer(text)
A:spacy.tests.lang.en.test_exceptions.tokens_lower->en_tokenizer(text_lower)
A:spacy.tests.lang.en.test_exceptions.tokens_title->en_tokenizer(text_title)
spacy.tests.lang.en.test_exceptions.test_en_lex_attrs_norm_exceptions(en_tokenizer,text,norm)
spacy.tests.lang.en.test_exceptions.test_en_tokenizer_doesnt_split_apos_exc(en_tokenizer,text)
spacy.tests.lang.en.test_exceptions.test_en_tokenizer_excludes_ambiguous(en_tokenizer,exc)
spacy.tests.lang.en.test_exceptions.test_en_tokenizer_handles_abbr(en_tokenizer,text)
spacy.tests.lang.en.test_exceptions.test_en_tokenizer_handles_basic_contraction(en_tokenizer)
spacy.tests.lang.en.test_exceptions.test_en_tokenizer_handles_basic_contraction_punct(en_tokenizer,text)
spacy.tests.lang.en.test_exceptions.test_en_tokenizer_handles_capitalization(en_tokenizer,text_lower,text_title)
spacy.tests.lang.en.test_exceptions.test_en_tokenizer_handles_exc_in_text(en_tokenizer)
spacy.tests.lang.en.test_exceptions.test_en_tokenizer_handles_ll_contraction(en_tokenizer,text)
spacy.tests.lang.en.test_exceptions.test_en_tokenizer_handles_poss_contraction(en_tokenizer,text_poss,text)
spacy.tests.lang.en.test_exceptions.test_en_tokenizer_handles_times(en_tokenizer,text)
spacy.tests.lang.en.test_exceptions.test_en_tokenizer_keeps_title_case(en_tokenizer,pron,contraction)
spacy.tests.lang.en.test_exceptions.test_en_tokenizer_norm_exceptions(en_tokenizer,text,norms)
spacy.tests.lang.en.test_exceptions.test_en_tokenizer_splits_defined_punct(en_tokenizer,wo_punct,w_punct)
spacy.tests.lang.en.test_exceptions.test_en_tokenizer_splits_trailing_apos(en_tokenizer,text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/en/test_parser.py----------------------------------------
A:spacy.tests.lang.en.test_parser.tokens->en_tokenizer(text)
A:spacy.tests.lang.en.test_parser.doc->get_doc(tokens.vocab, words=[t.text for t in tokens], tags=tags, deps=deps, heads=heads)
A:spacy.tests.lang.en.test_parser.chunks->list(doc.noun_chunks)
spacy.tests.lang.en.test_parser.test_en_parser_noun_chunks_appositional_modifiers(en_tokenizer)
spacy.tests.lang.en.test_parser.test_en_parser_noun_chunks_coordinated(en_tokenizer)
spacy.tests.lang.en.test_parser.test_en_parser_noun_chunks_dative(en_tokenizer)
spacy.tests.lang.en.test_parser.test_en_parser_noun_chunks_pp_chunks(en_tokenizer)
spacy.tests.lang.en.test_parser.test_en_parser_noun_chunks_standard(en_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/en/test_text.py----------------------------------------
A:spacy.tests.lang.en.test_text.tokens->en_tokenizer(text)
spacy.tests.lang.en.test_text.test_en_lex_attrs_capitals(word)
spacy.tests.lang.en.test_text.test_en_tokenizer_handles_cnts(en_tokenizer,text,length)
spacy.tests.lang.en.test_text.test_en_tokenizer_handles_long_text(en_tokenizer)
spacy.tests.lang.en.test_text.test_lex_attrs_like_number(en_tokenizer,text,match)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/en/test_punct.py----------------------------------------
A:spacy.tests.lang.en.test_punct.tokens->en_tokenizer(text)
A:spacy.tests.lang.en.test_punct.tokens_punct->en_tokenizer("''")
A:spacy.tests.lang.en.test_punct.match->en_search_prefixes(text)
spacy.tests.lang.en.test_punct.test_en_tokenizer_handles_only_punct(en_tokenizer,text)
spacy.tests.lang.en.test_punct.test_en_tokenizer_splits_bracket_period(en_tokenizer)
spacy.tests.lang.en.test_punct.test_en_tokenizer_splits_close_punct(en_tokenizer,punct,text)
spacy.tests.lang.en.test_punct.test_en_tokenizer_splits_double_end_quote(en_tokenizer,text)
spacy.tests.lang.en.test_punct.test_en_tokenizer_splits_open_appostrophe(en_tokenizer,text)
spacy.tests.lang.en.test_punct.test_en_tokenizer_splits_open_close_punct(en_tokenizer,punct_open,punct_close,text)
spacy.tests.lang.en.test_punct.test_en_tokenizer_splits_open_punct(en_tokenizer,punct,text)
spacy.tests.lang.en.test_punct.test_en_tokenizer_splits_pre_punct_regex(text,punct)
spacy.tests.lang.en.test_punct.test_en_tokenizer_splits_same_close_punct(en_tokenizer,punct,text)
spacy.tests.lang.en.test_punct.test_en_tokenizer_splits_same_open_punct(en_tokenizer,punct,text)
spacy.tests.lang.en.test_punct.test_en_tokenizer_splits_two_diff_close_punct(en_tokenizer,punct,punct_add,text)
spacy.tests.lang.en.test_punct.test_en_tokenizer_splits_two_diff_open_punct(en_tokenizer,punct,punct_add,text)
spacy.tests.lang.en.test_punct.test_en_tokenizer_two_diff_punct(en_tokenizer,punct_open,punct_close,punct_open2,punct_close2,text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/en/test_customized_tokenizer.py----------------------------------------
A:spacy.tests.lang.en.test_customized_tokenizer.prefix_re->compile_prefix_regex(English.Defaults.prefixes)
A:spacy.tests.lang.en.test_customized_tokenizer.suffix_re->compile_suffix_regex(English.Defaults.suffixes)
A:spacy.tests.lang.en.test_customized_tokenizer.infix_re->compile_infix_regex(custom_infixes)
spacy.tests.lang.en.test_customized_tokenizer.custom_en_tokenizer(en_vocab)
spacy.tests.lang.en.test_customized_tokenizer.test_en_customized_tokenizer_handles_infixes(custom_en_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/en/test_noun_chunks.py----------------------------------------
A:spacy.tests.lang.en.test_noun_chunks.doc->get_doc(en_vocab, words=words, heads=heads, deps=deps)
spacy.tests.lang.en.test_noun_chunks.test_en_noun_chunks_not_nested(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/en/test_prefix_suffix_infix.py----------------------------------------
A:spacy.tests.lang.en.test_prefix_suffix_infix.tokens->en_tokenizer(text)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_comma_infix(en_tokenizer,text)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_double_hyphen_infix(en_tokenizer)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_ellipsis_infix(en_tokenizer,text)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_em_dash_infix(en_tokenizer)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_even_wrap(en_tokenizer,text)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_even_wrap_interact(en_tokenizer,text)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_hyphens(en_tokenizer,text)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_no_punct(en_tokenizer,text)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_no_special(en_tokenizer,text)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_numeric_range(en_tokenizer,text)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_period_abbr(en_tokenizer)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_period_infix(en_tokenizer,text)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_prefix_interact(en_tokenizer,text,length)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_prefix_punct(en_tokenizer,text)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_suffix_interact(en_tokenizer,text)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_suffix_punct(en_tokenizer,text)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_uneven_wrap(en_tokenizer,text)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_uneven_wrap_interact(en_tokenizer,text)
spacy.tests.lang.en.test_prefix_suffix_infix.test_final_period(en_tokenizer,text,length)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/en/test_sbd.py----------------------------------------
A:spacy.tests.lang.en.test_sbd.tokens->en_tokenizer(text)
A:spacy.tests.lang.en.test_sbd.doc->get_doc(tokens.vocab, words=[t.text for t in tokens], heads=heads, deps=deps)
spacy.tests.lang.en.test_sbd.test_en_sbd_single_punct(en_tokenizer,text,punct)
spacy.tests.lang.en.test_sbd.test_en_sentence_breaks(en_tokenizer,en_parser)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/en/test_tagger.py----------------------------------------
A:spacy.tests.lang.en.test_tagger.tokens->en_tokenizer(text)
A:spacy.tests.lang.en.test_tagger.doc->get_doc(tokens.vocab, words=[t.text for t in tokens], tags=tags)
spacy.tests.lang.en.test_tagger.test_en_tagger_load_morph_exc(en_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/fi/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/fi/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.fi.test_tokenizer.tokens->fi_tokenizer(text)
spacy.tests.lang.fi.test_tokenizer.test_fi_tokenizer_handles_testcases(fi_tokenizer,text,expected_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/ca/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/ca/test_text.py----------------------------------------
A:spacy.tests.lang.ca.test_text.tokens->ca_tokenizer(text)
spacy.tests.lang.ca.test_text.test_ca_lex_attrs_like_number(ca_tokenizer,text,match)
spacy.tests.lang.ca.test_text.test_ca_tokenizer_handles_cnts(ca_tokenizer,text,length)
spacy.tests.lang.ca.test_text.test_ca_tokenizer_handles_long_text(ca_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/ca/test_exception.py----------------------------------------
A:spacy.tests.lang.ca.test_exception.tokens->ca_tokenizer(text)
spacy.tests.lang.ca.test_exception.test_ca_tokenizer_handles_abbr(ca_tokenizer,text,lemma)
spacy.tests.lang.ca.test_exception.test_ca_tokenizer_handles_exc_in_text(ca_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/ca/test_prefix_suffix_infix.py----------------------------------------
A:spacy.tests.lang.ca.test_prefix_suffix_infix.tokens->ca_tokenizer(text)
spacy.tests.lang.ca.test_prefix_suffix_infix.test_contractions(ca_tokenizer,text,expected_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/nb/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/nb/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.nb.test_tokenizer.tokens->nb_tokenizer(text)
spacy.tests.lang.nb.test_tokenizer.test_nb_tokenizer_handles_exception_cases(nb_tokenizer,text,expected_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/bn/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/bn/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.bn.test_tokenizer.tokens->bn_tokenizer(text)
spacy.tests.lang.bn.test_tokenizer.test_bn_tokenizer_handles_long_text(bn_tokenizer)
spacy.tests.lang.bn.test_tokenizer.test_bn_tokenizer_handles_testcases(bn_tokenizer,text,expected_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/lt/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/lt/test_text.py----------------------------------------
A:spacy.tests.lang.lt.test_text.tokens->lt_tokenizer(text)
spacy.tests.lang.lt.test_text.test_lt_lex_attrs_like_number(lt_tokenizer,text,match)
spacy.tests.lang.lt.test_text.test_lt_tokenizer_abbrev_exceptions(lt_tokenizer,text)
spacy.tests.lang.lt.test_text.test_lt_tokenizer_handles_long_text(lt_tokenizer)
spacy.tests.lang.lt.test_text.test_lt_tokenizer_handles_punct_abbrev(lt_tokenizer,text,length)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/da/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/da/test_exceptions.py----------------------------------------
A:spacy.tests.lang.da.test_exceptions.tokens->da_tokenizer(text)
spacy.tests.lang.da.test_exceptions.test_da_tokenizer_handles_abbr(da_tokenizer,text)
spacy.tests.lang.da.test_exceptions.test_da_tokenizer_handles_ambiguous_abbr(da_tokenizer,text)
spacy.tests.lang.da.test_exceptions.test_da_tokenizer_handles_custom_base_exc(da_tokenizer)
spacy.tests.lang.da.test_exceptions.test_da_tokenizer_handles_dates(da_tokenizer,text)
spacy.tests.lang.da.test_exceptions.test_da_tokenizer_handles_exc_in_text(da_tokenizer)
spacy.tests.lang.da.test_exceptions.test_da_tokenizer_norm_exceptions(da_tokenizer,text,norm)
spacy.tests.lang.da.test_exceptions.test_da_tokenizer_slash(da_tokenizer,text,n_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/da/test_text.py----------------------------------------
A:spacy.tests.lang.da.test_text.tokens->da_tokenizer(text)
spacy.tests.lang.da.test_text.test_da_lex_attrs_capitals(word)
spacy.tests.lang.da.test_text.test_da_tokenizer_handles_long_text(da_tokenizer)
spacy.tests.lang.da.test_text.test_lex_attrs_like_number(da_tokenizer,text,match)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/da/test_prefix_suffix_infix.py----------------------------------------
A:spacy.tests.lang.da.test_prefix_suffix_infix.tokens->da_tokenizer("'DBA's, Lars' og Liz' bil sku' sgu' ik' ha' en bule, det ka' han ik' li' mere', sagde hun.")
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_handles_no_punct(da_tokenizer,text)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_handles_numeric_range(da_tokenizer,text)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_handles_posessives_and_contractions(da_tokenizer)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_keeps_hyphens(da_tokenizer,text)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_splits_comma_infix(da_tokenizer,text)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_splits_double_hyphen_infix(da_tokenizer)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_splits_ellipsis_infix(da_tokenizer,text)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_splits_even_wrap(da_tokenizer,text,expected)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_splits_even_wrap_interact(da_tokenizer,text)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_splits_no_special(da_tokenizer,text)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_splits_period_infix(da_tokenizer,text)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_splits_prefix_interact(da_tokenizer,text,expected)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_splits_prefix_punct(da_tokenizer,text)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_splits_suffix_interact(da_tokenizer,text)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_splits_suffix_punct(da_tokenizer,text)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_splits_uneven_wrap(da_tokenizer,text)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_splits_uneven_wrap_interact(da_tokenizer,text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/uk/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/uk/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.uk.test_tokenizer.tokens->uk_tokenizer(text)
A:spacy.tests.lang.uk.test_tokenizer.tokens_punct->uk_tokenizer("''")
spacy.tests.lang.uk.test_tokenizer.test_uk_tokenizer_handles_only_punct(uk_tokenizer,text)
spacy.tests.lang.uk.test_tokenizer.test_uk_tokenizer_splits_bracket_period(uk_tokenizer)
spacy.tests.lang.uk.test_tokenizer.test_uk_tokenizer_splits_close_punct(uk_tokenizer,punct,text)
spacy.tests.lang.uk.test_tokenizer.test_uk_tokenizer_splits_double_end_quote(uk_tokenizer,text)
spacy.tests.lang.uk.test_tokenizer.test_uk_tokenizer_splits_open_appostrophe(uk_tokenizer,text)
spacy.tests.lang.uk.test_tokenizer.test_uk_tokenizer_splits_open_close_punct(uk_tokenizer,punct_open,punct_close,text)
spacy.tests.lang.uk.test_tokenizer.test_uk_tokenizer_splits_open_punct(uk_tokenizer,punct,text)
spacy.tests.lang.uk.test_tokenizer.test_uk_tokenizer_splits_same_close_punct(uk_tokenizer,punct,text)
spacy.tests.lang.uk.test_tokenizer.test_uk_tokenizer_splits_same_open_punct(uk_tokenizer,punct,text)
spacy.tests.lang.uk.test_tokenizer.test_uk_tokenizer_splits_trailing_dot(uk_tokenizer,text)
spacy.tests.lang.uk.test_tokenizer.test_uk_tokenizer_splits_two_diff_close_punct(uk_tokenizer,punct,punct_add,text)
spacy.tests.lang.uk.test_tokenizer.test_uk_tokenizer_splits_two_diff_open_punct(uk_tokenizer,punct,punct_add,text)
spacy.tests.lang.uk.test_tokenizer.test_uk_tokenizer_two_diff_punct(uk_tokenizer,punct_open,punct_close,punct_open2,punct_close2,text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/uk/test_tokenizer_exc.py----------------------------------------
A:spacy.tests.lang.uk.test_tokenizer_exc.tokens->uk_tokenizer(text)
spacy.tests.lang.uk.test_tokenizer_exc.test_uk_tokenizer_abbrev_exceptions(uk_tokenizer,text,norms,lemmas)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/ru/test_lemmatizer.py----------------------------------------
A:spacy.tests.lang.ru.test_lemmatizer.doc->get_doc(ru_tokenizer.vocab, words=words, tags=tags)
spacy.tests.lang.ru.test_lemmatizer.test_ru_doc_lemmatization(ru_tokenizer)
spacy.tests.lang.ru.test_lemmatizer.test_ru_lemmatizer_noun_lemmas(ru_lemmatizer,text,lemmas)
spacy.tests.lang.ru.test_lemmatizer.test_ru_lemmatizer_punct(ru_lemmatizer)
spacy.tests.lang.ru.test_lemmatizer.test_ru_lemmatizer_works_with_different_pos_homonyms(ru_lemmatizer,text,pos,morphology,lemma)
spacy.tests.lang.ru.test_lemmatizer.test_ru_lemmatizer_works_with_noun_homonyms(ru_lemmatizer,text,morphology,lemma)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/ru/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/ru/test_exceptions.py----------------------------------------
A:spacy.tests.lang.ru.test_exceptions.tokens->ru_tokenizer(text)
spacy.tests.lang.ru.test_exceptions.test_ru_tokenizer_abbrev_exceptions(ru_tokenizer,text,norms)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/ru/test_text.py----------------------------------------
spacy.tests.lang.ru.test_text.test_ru_lex_attrs_capitals(word)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/ru/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.ru.test_tokenizer.tokens->ru_tokenizer(text)
A:spacy.tests.lang.ru.test_tokenizer.tokens_punct->ru_tokenizer("''")
spacy.tests.lang.ru.test_tokenizer.test_ru_tokenizer_handles_only_punct(ru_tokenizer,text)
spacy.tests.lang.ru.test_tokenizer.test_ru_tokenizer_splits_bracket_period(ru_tokenizer)
spacy.tests.lang.ru.test_tokenizer.test_ru_tokenizer_splits_close_punct(ru_tokenizer,punct,text)
spacy.tests.lang.ru.test_tokenizer.test_ru_tokenizer_splits_double_end_quote(ru_tokenizer,text)
spacy.tests.lang.ru.test_tokenizer.test_ru_tokenizer_splits_open_appostrophe(ru_tokenizer,text)
spacy.tests.lang.ru.test_tokenizer.test_ru_tokenizer_splits_open_close_punct(ru_tokenizer,punct_open,punct_close,text)
spacy.tests.lang.ru.test_tokenizer.test_ru_tokenizer_splits_open_punct(ru_tokenizer,punct,text)
spacy.tests.lang.ru.test_tokenizer.test_ru_tokenizer_splits_same_close_punct(ru_tokenizer,punct,text)
spacy.tests.lang.ru.test_tokenizer.test_ru_tokenizer_splits_same_open_punct(ru_tokenizer,punct,text)
spacy.tests.lang.ru.test_tokenizer.test_ru_tokenizer_splits_trailing_dot(ru_tokenizer,text)
spacy.tests.lang.ru.test_tokenizer.test_ru_tokenizer_splits_two_diff_close_punct(ru_tokenizer,punct,punct_add,text)
spacy.tests.lang.ru.test_tokenizer.test_ru_tokenizer_splits_two_diff_open_punct(ru_tokenizer,punct,punct_add,text)
spacy.tests.lang.ru.test_tokenizer.test_ru_tokenizer_two_diff_punct(ru_tokenizer,punct_open,punct_close,punct_open2,punct_close2,text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/ar/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/ar/test_exceptions.py----------------------------------------
A:spacy.tests.lang.ar.test_exceptions.tokens->ar_tokenizer(text)
spacy.tests.lang.ar.test_exceptions.test_ar_tokenizer_handles_abbr(ar_tokenizer,text)
spacy.tests.lang.ar.test_exceptions.test_ar_tokenizer_handles_exc_in_text(ar_tokenizer)
spacy.tests.lang.ar.test_exceptions.test_ar_tokenizer_handles_exc_in_text_2(ar_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/ar/test_text.py----------------------------------------
A:spacy.tests.lang.ar.test_text.tokens->ar_tokenizer(text)
spacy.tests.lang.ar.test_text.test_ar_tokenizer_handles_long_text(ar_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/ja/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/ja/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.ja.test_tokenizer.tokens->ja_tokenizer('I   like cheese.')
spacy.tests.lang.ja.test_tokenizer.test_extra_spaces(ja_tokenizer)
spacy.tests.lang.ja.test_tokenizer.test_ja_tokenizer(ja_tokenizer,text,expected_tokens)
spacy.tests.lang.ja.test_tokenizer.test_ja_tokenizer_pos(ja_tokenizer,text,expected_pos)
spacy.tests.lang.ja.test_tokenizer.test_ja_tokenizer_tags(ja_tokenizer,text,expected_tags)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/ja/test_lemmatization.py----------------------------------------
spacy.tests.lang.ja.test_lemmatization.test_ja_lemmatizer_assigns(ja_tokenizer,word,lemma)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/el/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/el/test_text.py----------------------------------------
A:spacy.tests.lang.el.test_text.tokens->el_tokenizer(text)
spacy.tests.lang.el.test_text.test_el_tokenizer_handles_cnts(el_tokenizer,text,length)
spacy.tests.lang.el.test_text.test_el_tokenizer_handles_long_text(el_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/el/test_exception.py----------------------------------------
A:spacy.tests.lang.el.test_exception.tokens->el_tokenizer(text)
spacy.tests.lang.el.test_exception.test_el_tokenizer_handles_abbr(el_tokenizer,text)
spacy.tests.lang.el.test_exception.test_el_tokenizer_handles_exc_in_text(el_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/ur/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/ur/test_text.py----------------------------------------
A:spacy.tests.lang.ur.test_text.tokens->ur_tokenizer(text)
spacy.tests.lang.ur.test_text.test_ur_tokenizer_handles_cnts(ur_tokenizer,text,length)
spacy.tests.lang.ur.test_text.test_ur_tokenizer_handles_long_text(ur_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/ur/test_prefix_suffix_infix.py----------------------------------------
A:spacy.tests.lang.ur.test_prefix_suffix_infix.tokens->ur_tokenizer(text)
spacy.tests.lang.ur.test_prefix_suffix_infix.test_contractions(ur_tokenizer,text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/es/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/es/test_text.py----------------------------------------
A:spacy.tests.lang.es.test_text.tokens->es_tokenizer(text)
spacy.tests.lang.es.test_text.test_es_tokenizer_handles_cnts(es_tokenizer,text,length)
spacy.tests.lang.es.test_text.test_es_tokenizer_handles_long_text(es_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/es/test_exception.py----------------------------------------
A:spacy.tests.lang.es.test_exception.tokens->es_tokenizer(text)
spacy.tests.lang.es.test_exception.test_es_tokenizer_handles_abbr(es_tokenizer,text,lemma)
spacy.tests.lang.es.test_exception.test_es_tokenizer_handles_exc_in_text(es_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/tt/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/tt/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.tt.test_tokenizer.tokens->tt_tokenizer(text)
spacy.tests.lang.tt.test_tokenizer.test_tt_tokenizer_handles_norm_exceptions(tt_tokenizer,text,norms)
spacy.tests.lang.tt.test_tokenizer.test_tt_tokenizer_handles_testcases(tt_tokenizer,text,expected_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/sv/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/sv/test_exceptions.py----------------------------------------
A:spacy.tests.lang.sv.test_exceptions.tokens->sv_tokenizer(text)
spacy.tests.lang.sv.test_exceptions.test_sv_tokenizer_handles_abbr(sv_tokenizer,text)
spacy.tests.lang.sv.test_exceptions.test_sv_tokenizer_handles_ambiguous_abbr(sv_tokenizer,text)
spacy.tests.lang.sv.test_exceptions.test_sv_tokenizer_handles_custom_base_exc(sv_tokenizer)
spacy.tests.lang.sv.test_exceptions.test_sv_tokenizer_handles_exc_in_text(sv_tokenizer)
spacy.tests.lang.sv.test_exceptions.test_sv_tokenizer_handles_exception_cases(sv_tokenizer,text,expected_tokens)
spacy.tests.lang.sv.test_exceptions.test_sv_tokenizer_handles_verb_exceptions(sv_tokenizer,text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/sv/test_text.py----------------------------------------
A:spacy.tests.lang.sv.test_text.tokens->sv_tokenizer(text)
spacy.tests.lang.sv.test_text.test_sv_tokenizer_handles_long_text(sv_tokenizer)
spacy.tests.lang.sv.test_text.test_sv_tokenizer_handles_trailing_dot_for_i_in_sentence(sv_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/sv/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.sv.test_tokenizer.tokens->sv_tokenizer(text)
spacy.tests.lang.sv.test_tokenizer.test_sv_tokenizer_handles_exception_cases(sv_tokenizer,text,expected_tokens)
spacy.tests.lang.sv.test_tokenizer.test_sv_tokenizer_handles_verb_exceptions(sv_tokenizer,text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/sv/test_prefix_suffix_infix.py----------------------------------------
A:spacy.tests.lang.sv.test_prefix_suffix_infix.tokens->sv_tokenizer(text)
spacy.tests.lang.sv.test_prefix_suffix_infix.test_tokenizer_handles_no_punct(sv_tokenizer,text)
spacy.tests.lang.sv.test_prefix_suffix_infix.test_tokenizer_splits_comma_infix(sv_tokenizer,text)
spacy.tests.lang.sv.test_prefix_suffix_infix.test_tokenizer_splits_ellipsis_infix(sv_tokenizer,text)
spacy.tests.lang.sv.test_prefix_suffix_infix.test_tokenizer_splits_no_special(sv_tokenizer,text)
spacy.tests.lang.sv.test_prefix_suffix_infix.test_tokenizer_splits_period_infix(sv_tokenizer,text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/id/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/id/test_text.py----------------------------------------
spacy.tests.lang.id.test_text.test_id_lex_attrs_capitals(word)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/id/test_prefix_suffix_infix.py----------------------------------------
A:spacy.tests.lang.id.test_prefix_suffix_infix.tokens->id_tokenizer('Arsene Wenger--manajer Arsenal--melakukan konferensi pers.')
spacy.tests.lang.id.test_prefix_suffix_infix.test_id_tokenizer_splits_comma_infix(id_tokenizer,text)
spacy.tests.lang.id.test_prefix_suffix_infix.test_id_tokenizer_splits_double_hyphen_infix(id_tokenizer)
spacy.tests.lang.id.test_prefix_suffix_infix.test_id_tokenizer_splits_ellipsis_infix(id_tokenizer,text)
spacy.tests.lang.id.test_prefix_suffix_infix.test_id_tokenizer_splits_even_wrap(id_tokenizer,text)
spacy.tests.lang.id.test_prefix_suffix_infix.test_id_tokenizer_splits_even_wrap_interact(id_tokenizer,text)
spacy.tests.lang.id.test_prefix_suffix_infix.test_id_tokenizer_splits_hyphens(id_tokenizer,text,length)
spacy.tests.lang.id.test_prefix_suffix_infix.test_id_tokenizer_splits_no_punct(id_tokenizer,text)
spacy.tests.lang.id.test_prefix_suffix_infix.test_id_tokenizer_splits_no_special(id_tokenizer,text)
spacy.tests.lang.id.test_prefix_suffix_infix.test_id_tokenizer_splits_numeric_range(id_tokenizer,text)
spacy.tests.lang.id.test_prefix_suffix_infix.test_id_tokenizer_splits_period_infix(id_tokenizer,text)
spacy.tests.lang.id.test_prefix_suffix_infix.test_id_tokenizer_splits_prefix_interact(id_tokenizer,text,length)
spacy.tests.lang.id.test_prefix_suffix_infix.test_id_tokenizer_splits_prefix_punct(id_tokenizer,text)
spacy.tests.lang.id.test_prefix_suffix_infix.test_id_tokenizer_splits_suffix_interact(id_tokenizer,text)
spacy.tests.lang.id.test_prefix_suffix_infix.test_id_tokenizer_splits_suffix_punct(id_tokenizer,text)
spacy.tests.lang.id.test_prefix_suffix_infix.test_id_tokenizer_splits_uneven_wrap_interact(id_tokenizer,text)
spacy.tests.lang.id.test_prefix_suffix_infix.test_tokenizer_splits_uneven_wrap(id_tokenizer,text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/sr/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/sr/test_exceptions.py----------------------------------------
A:spacy.tests.lang.sr.test_exceptions.tokens->sr_tokenizer(text)
spacy.tests.lang.sr.test_exceptions.test_sr_tokenizer_abbrev_exceptions(sr_tokenizer,text,norms,lemmas)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/sr/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.sr.test_tokenizer.tokens->sr_tokenizer(text)
A:spacy.tests.lang.sr.test_tokenizer.tokens_punct->sr_tokenizer("''")
spacy.tests.lang.sr.test_tokenizer.test_sr_tokenizer_handles_only_punct(sr_tokenizer,text)
spacy.tests.lang.sr.test_tokenizer.test_sr_tokenizer_splits_bracket_period(sr_tokenizer)
spacy.tests.lang.sr.test_tokenizer.test_sr_tokenizer_splits_close_punct(sr_tokenizer,punct,text)
spacy.tests.lang.sr.test_tokenizer.test_sr_tokenizer_splits_double_end_quote(sr_tokenizer,text)
spacy.tests.lang.sr.test_tokenizer.test_sr_tokenizer_splits_open_appostrophe(sr_tokenizer,text)
spacy.tests.lang.sr.test_tokenizer.test_sr_tokenizer_splits_open_close_punct(sr_tokenizer,punct_open,punct_close,text)
spacy.tests.lang.sr.test_tokenizer.test_sr_tokenizer_splits_open_punct(sr_tokenizer,punct,text)
spacy.tests.lang.sr.test_tokenizer.test_sr_tokenizer_splits_same_close_punct(sr_tokenizer,punct,text)
spacy.tests.lang.sr.test_tokenizer.test_sr_tokenizer_splits_same_open_punct(sr_tokenizer,punct,text)
spacy.tests.lang.sr.test_tokenizer.test_sr_tokenizer_splits_trailing_dot(sr_tokenizer,text)
spacy.tests.lang.sr.test_tokenizer.test_sr_tokenizer_splits_two_diff_close_punct(sr_tokenizer,punct,punct_add,text)
spacy.tests.lang.sr.test_tokenizer.test_sr_tokenizer_splits_two_diff_open_punct(sr_tokenizer,punct,punct_add,text)
spacy.tests.lang.sr.test_tokenizer.test_sr_tokenizer_two_diff_punct(sr_tokenizer,punct_open,punct_close,punct_open2,punct_close2,text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/de/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/de/test_exceptions.py----------------------------------------
A:spacy.tests.lang.de.test_exceptions.tokens->de_tokenizer(text)
spacy.tests.lang.de.test_exceptions.test_de_lex_attrs_norm_exceptions(de_tokenizer,text,norm)
spacy.tests.lang.de.test_exceptions.test_de_tokenizer_handles_abbr(de_tokenizer,text)
spacy.tests.lang.de.test_exceptions.test_de_tokenizer_handles_exc_in_text(de_tokenizer)
spacy.tests.lang.de.test_exceptions.test_de_tokenizer_norm_exceptions(de_tokenizer,text,norms)
spacy.tests.lang.de.test_exceptions.test_de_tokenizer_splits_contractions(de_tokenizer,text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/de/test_parser.py----------------------------------------
A:spacy.tests.lang.de.test_parser.tokens->de_tokenizer(text)
A:spacy.tests.lang.de.test_parser.doc->get_doc(tokens.vocab, words=[t.text for t in tokens], tags=tags, deps=deps, heads=heads)
A:spacy.tests.lang.de.test_parser.chunks->list(doc.noun_chunks)
spacy.tests.lang.de.test_parser.test_de_extended_chunk(de_tokenizer)
spacy.tests.lang.de.test_parser.test_de_parser_noun_chunks_standard_de(de_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/de/test_text.py----------------------------------------
A:spacy.tests.lang.de.test_text.tokens->de_tokenizer(text)
spacy.tests.lang.de.test_text.test_de_tokenizer_handles_examples(de_tokenizer,text,length)
spacy.tests.lang.de.test_text.test_de_tokenizer_handles_long_text(de_tokenizer)
spacy.tests.lang.de.test_text.test_de_tokenizer_handles_long_words(de_tokenizer,text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/de/test_prefix_suffix_infix.py----------------------------------------
A:spacy.tests.lang.de.test_prefix_suffix_infix.tokens->de_tokenizer('Viele Regeln--wie die Bindestrich-Regeln--sind kompliziert.')
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_keeps_hyphens(de_tokenizer,text)
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_splits_comma_infix(de_tokenizer,text)
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_splits_double_hyphen_infix(de_tokenizer)
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_splits_ellipsis_infix(de_tokenizer,text)
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_splits_even_wrap(de_tokenizer,text)
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_splits_even_wrap_interact(de_tokenizer,text)
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_splits_no_punct(de_tokenizer,text)
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_splits_no_special(de_tokenizer,text)
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_splits_numeric_range(de_tokenizer,text)
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_splits_period_infix(de_tokenizer,text)
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_splits_prefix_interact(de_tokenizer,text,length)
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_splits_prefix_punct(de_tokenizer,text)
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_splits_suffix_interact(de_tokenizer,text)
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_splits_suffix_punct(de_tokenizer,text)
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_splits_uneven_wrap(de_tokenizer,text)
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_splits_uneven_wrap_interact(de_tokenizer,text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/he/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/he/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.he.test_tokenizer.tokens->he_tokenizer(text)
spacy.tests.lang.he.test_tokenizer.test_he_tokenizer_handles_abbreviation(he_tokenizer,text,expected_tokens)
spacy.tests.lang.he.test_tokenizer.test_he_tokenizer_handles_punct(he_tokenizer,text,expected_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/ko/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/ko/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.ko.test_tokenizer.tokens->ko_tokenizer('')
spacy.tests.lang.ko.test_tokenizer.test_ko_empty_doc(ko_tokenizer)
spacy.tests.lang.ko.test_tokenizer.test_ko_tokenizer(ko_tokenizer,text,expected_tokens)
spacy.tests.lang.ko.test_tokenizer.test_ko_tokenizer_full_tags(ko_tokenizer,text,expected_tags)
spacy.tests.lang.ko.test_tokenizer.test_ko_tokenizer_pos(ko_tokenizer,text,expected_pos)
spacy.tests.lang.ko.test_tokenizer.test_ko_tokenizer_tags(ko_tokenizer,text,expected_tags)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/ko/test_lemmatization.py----------------------------------------
spacy.tests.lang.ko.test_lemmatization.test_ko_lemmatizer_assigns(ko_tokenizer,word,lemma)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/nl/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/nl/test_text.py----------------------------------------
A:spacy.tests.lang.nl.test_text.tokens->nl_tokenizer(text)
spacy.tests.lang.nl.test_text.test_nl_lex_attrs_capitals(word)
spacy.tests.lang.nl.test_text.test_tokenizer_doesnt_split_hyphens(nl_tokenizer,text,num_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/ro/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/ro/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.ro.test_tokenizer.tokens->ro_tokenizer(text)
spacy.tests.lang.ro.test_tokenizer.test_ro_tokenizer_handles_testcases(ro_tokenizer,text,expected_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/pl/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/pl/test_text.py----------------------------------------
A:spacy.tests.lang.pl.test_text.tokens->pl_tokenizer(text)
spacy.tests.lang.pl.test_text.test_lex_attrs_like_number(pl_tokenizer,text,match)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/pl/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.pl.test_tokenizer.tokens->pl_tokenizer(text)
spacy.tests.lang.pl.test_tokenizer.test_tokenizer_handles_testcases(pl_tokenizer,text,expected_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/th/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/th/test_tokenizer.py----------------------------------------
spacy.tests.lang.th.test_tokenizer.test_th_tokenizer(th_tokenizer,text,expected_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/tr/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/hu/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/hu/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.hu.test_tokenizer.tokens->hu_tokenizer(text)
spacy.tests.lang.hu.test_tokenizer.test_hu_tokenizer_handles_testcases(hu_tokenizer,text,expected_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/fr/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/fr/test_exceptions.py----------------------------------------
A:spacy.tests.lang.fr.test_exceptions.tokens->fr_tokenizer(text)
spacy.tests.lang.fr.test_exceptions.test_fr_tokenizer_handles_abbr(fr_tokenizer,text,lemma)
spacy.tests.lang.fr.test_exceptions.test_fr_tokenizer_handles_exc_in_text(fr_tokenizer)
spacy.tests.lang.fr.test_exceptions.test_fr_tokenizer_handles_exc_in_text_2(fr_tokenizer)
spacy.tests.lang.fr.test_exceptions.test_fr_tokenizer_handles_title(fr_tokenizer)
spacy.tests.lang.fr.test_exceptions.test_fr_tokenizer_handles_title_2(fr_tokenizer)
spacy.tests.lang.fr.test_exceptions.test_fr_tokenizer_handles_title_3(fr_tokenizer)
spacy.tests.lang.fr.test_exceptions.test_fr_tokenizer_infix_exceptions(fr_tokenizer,text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/fr/test_text.py----------------------------------------
A:spacy.tests.lang.fr.test_text.tokens->fr_tokenizer(text)
spacy.tests.lang.fr.test_text.test_fr_lex_attrs_capitals(word)
spacy.tests.lang.fr.test_text.test_tokenizer_handles_long_text(fr_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/fr/test_prefix_suffix_infix.py----------------------------------------
A:spacy.tests.lang.fr.test_prefix_suffix_infix.SPLIT_INFIX->"(?<=[{a}]\\')(?=[{a}])".format(a=ALPHA)
A:spacy.tests.lang.fr.test_prefix_suffix_infix.fr_tokenizer_w_infix->FrenchTest.Defaults.create_tokenizer()
A:spacy.tests.lang.fr.test_prefix_suffix_infix.tokens->fr_tokenizer_w_infix(text)
spacy.tests.lang.fr.test_prefix_suffix_infix.test_issue768(text,expected_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/pt/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/pt/test_text.py----------------------------------------
spacy.tests.lang.pt.test_text.test_pt_lex_attrs_capitals(word)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/serialize/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/serialize/test_serialize_doc.py----------------------------------------
A:spacy.tests.serialize.test_serialize_doc.doc->Doc(en_vocab, words=['hello', 'world'])
A:spacy.tests.serialize.test_serialize_doc.data->Doc(en_vocab, words=['hello', 'world']).to_bytes()
A:spacy.tests.serialize.test_serialize_doc.doc2->Doc(en_vocab)
A:spacy.tests.serialize.test_serialize_doc.doc_b->Doc(en_vocab, words=['hello', 'world']).to_bytes()
A:spacy.tests.serialize.test_serialize_doc.new_doc->Doc(en_vocab).from_bytes(doc.to_bytes(exclude=['user_data']))
A:spacy.tests.serialize.test_serialize_doc.doc_d->Doc(en_vocab).from_disk(file_path)
A:spacy.tests.serialize.test_serialize_doc.file_path->path2str(file_path)
spacy.tests.serialize.test_serialize_doc.test_serialize_doc_exclude(en_vocab)
spacy.tests.serialize.test_serialize_doc.test_serialize_doc_roundtrip_bytes(en_vocab)
spacy.tests.serialize.test_serialize_doc.test_serialize_doc_roundtrip_disk(en_vocab)
spacy.tests.serialize.test_serialize_doc.test_serialize_doc_roundtrip_disk_str_path(en_vocab)
spacy.tests.serialize.test_serialize_doc.test_serialize_empty_doc(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/serialize/test_serialize_kb.py----------------------------------------
A:spacy.tests.serialize.test_serialize_kb.kb1->_get_dummy_kb(en_vocab)
A:spacy.tests.serialize.test_serialize_kb.dir_path->ensure_path(d)
A:spacy.tests.serialize.test_serialize_kb.kb2->KnowledgeBase(vocab=en_vocab, entity_vector_length=3)
A:spacy.tests.serialize.test_serialize_kb.kb->KnowledgeBase(vocab=vocab, entity_vector_length=3)
A:spacy.tests.serialize.test_serialize_kb.candidates->sorted(kb.get_candidates('double07'), key=lambda x: x.entity_)
spacy.tests.serialize.test_serialize_kb._check_kb(kb)
spacy.tests.serialize.test_serialize_kb._get_dummy_kb(vocab)
spacy.tests.serialize.test_serialize_kb.test_serialize_kb_disk(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/serialize/test_serialize_language.py----------------------------------------
A:spacy.tests.serialize.test_serialize_language.language->Language(meta=meta_data)
A:spacy.tests.serialize.test_serialize_language.new_language->Language().from_disk(d)
A:spacy.tests.serialize.test_serialize_language.prefix_re->re.compile('1/|2/|:[0-9][0-9][A-K]:|:[0-9][0-9]:')
A:spacy.tests.serialize.test_serialize_language.suffix_re->re.compile('')
A:spacy.tests.serialize.test_serialize_language.infix_re->re.compile('[~]')
A:spacy.tests.serialize.test_serialize_language.nlp->Language(meta=meta_data)
A:spacy.tests.serialize.test_serialize_language.nlp.tokenizer->custom_tokenizer(nlp)
A:spacy.tests.serialize.test_serialize_language.new_nlp->Language().from_bytes(nlp.to_bytes(exclude=['meta']))
spacy.tests.serialize.test_serialize_language.meta_data()
spacy.tests.serialize.test_serialize_language.test_serialize_language_exclude(meta_data)
spacy.tests.serialize.test_serialize_language.test_serialize_language_meta_disk(meta_data)
spacy.tests.serialize.test_serialize_language.test_serialize_with_custom_tokenizer()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/serialize/test_serialize_extension_attrs.py----------------------------------------
A:spacy.tests.serialize.test_serialize_extension_attrs.doc->Doc(Vocab()).from_bytes(doc_b)
A:spacy.tests.serialize.test_serialize_extension_attrs.doc_b->doc_w_attrs.to_bytes()
spacy.tests.serialize.test_serialize_extension_attrs.doc_w_attrs(en_tokenizer)
spacy.tests.serialize.test_serialize_extension_attrs.test_serialize_ext_attrs_from_bytes(doc_w_attrs)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/serialize/test_serialize_tokenizer.py----------------------------------------
A:spacy.tests.serialize.test_serialize_tokenizer.tok->get_lang_class('en').Defaults.create_tokenizer()
A:spacy.tests.serialize.test_serialize_tokenizer.tokenizer->Tokenizer(en_vocab, suffix_search=en_tokenizer.suffix_search)
A:spacy.tests.serialize.test_serialize_tokenizer.tokenizer_bytes->Tokenizer(en_vocab, suffix_search=en_tokenizer.suffix_search).to_bytes()
A:spacy.tests.serialize.test_serialize_tokenizer.new_tokenizer->load_tokenizer(tokenizer.to_bytes())
A:spacy.tests.serialize.test_serialize_tokenizer.doc1->tokenizer(text)
A:spacy.tests.serialize.test_serialize_tokenizer.doc2->new_tokenizer(text)
A:spacy.tests.serialize.test_serialize_tokenizer.tokenizer_d->en_tokenizer.from_disk(file_path)
spacy.tests.serialize.test_serialize_tokenizer.load_tokenizer(b)
spacy.tests.serialize.test_serialize_tokenizer.test_serialize_custom_tokenizer(en_vocab,en_tokenizer)
spacy.tests.serialize.test_serialize_tokenizer.test_serialize_tokenizer_roundtrip_bytes(en_tokenizer,text)
spacy.tests.serialize.test_serialize_tokenizer.test_serialize_tokenizer_roundtrip_disk(en_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/serialize/test_serialize_pipeline.py----------------------------------------
A:spacy.tests.serialize.test_serialize_pipeline.parser->Parser(en_vocab)
A:spacy.tests.serialize.test_serialize_pipeline.(parser.model, cfg)->Parser(en_vocab).Model(parser.moves.n_moves)
A:spacy.tests.serialize.test_serialize_pipeline.tagger1->tagger1.from_bytes(tagger1_b).from_bytes(tagger1_b)
A:spacy.tests.serialize.test_serialize_pipeline.tagger2->Tagger(en_vocab)
A:spacy.tests.serialize.test_serialize_pipeline.tagger1.model->tagger1.from_bytes(tagger1_b).from_bytes(tagger1_b).Model(8)
A:spacy.tests.serialize.test_serialize_pipeline.(parser.model, _)->Parser(en_vocab).Model(0)
A:spacy.tests.serialize.test_serialize_pipeline.new_parser->get_new_parser().from_bytes(parser.to_bytes(exclude=['cfg']), exclude=['vocab'])
A:spacy.tests.serialize.test_serialize_pipeline.(new_parser.model, _)->get_new_parser().from_bytes(parser.to_bytes(exclude=['cfg']), exclude=['vocab']).Model(0)
A:spacy.tests.serialize.test_serialize_pipeline.parser_d->parser_d.from_disk(file_path).from_disk(file_path)
A:spacy.tests.serialize.test_serialize_pipeline.(parser_d.model, _)->parser_d.from_disk(file_path).from_disk(file_path).Model(0)
A:spacy.tests.serialize.test_serialize_pipeline.parser_bytes->Parser(en_vocab).to_bytes(exclude=['model', 'vocab'])
A:spacy.tests.serialize.test_serialize_pipeline.parser_d_bytes->parser_d.from_disk(file_path).from_disk(file_path).to_bytes(exclude=['model', 'vocab'])
A:spacy.tests.serialize.test_serialize_pipeline.bytes_data->Parser(en_vocab).to_bytes(exclude=['vocab'])
A:spacy.tests.serialize.test_serialize_pipeline.tagger1_b->tagger1.from_bytes(tagger1_b).from_bytes(tagger1_b).to_bytes()
A:spacy.tests.serialize.test_serialize_pipeline.new_tagger1->Tagger(en_vocab).from_bytes(tagger1_b)
A:spacy.tests.serialize.test_serialize_pipeline.tagger1_d->Tagger(en_vocab).from_disk(file_path1)
A:spacy.tests.serialize.test_serialize_pipeline.tagger2_d->Tagger(en_vocab).from_disk(file_path2)
A:spacy.tests.serialize.test_serialize_pipeline.tensorizer->Tensorizer(en_vocab)
A:spacy.tests.serialize.test_serialize_pipeline.tensorizer.model->Tensorizer(en_vocab).Model()
A:spacy.tests.serialize.test_serialize_pipeline.tensorizer_b->Tensorizer(en_vocab).to_bytes(exclude=['vocab'])
A:spacy.tests.serialize.test_serialize_pipeline.new_tensorizer->Tensorizer(en_vocab).from_bytes(tensorizer_b)
A:spacy.tests.serialize.test_serialize_pipeline.tensorizer_d->Tensorizer(en_vocab).from_disk(file_path)
A:spacy.tests.serialize.test_serialize_pipeline.textcat->TextCategorizer(en_vocab, labels=['ENTITY', 'ACTION', 'MODIFIER'])
spacy.tests.serialize.test_serialize_pipeline.blank_parser(en_vocab)
spacy.tests.serialize.test_serialize_pipeline.parser(en_vocab)
spacy.tests.serialize.test_serialize_pipeline.taggers(en_vocab)
spacy.tests.serialize.test_serialize_pipeline.test_serialize_parser_roundtrip_bytes(en_vocab,Parser)
spacy.tests.serialize.test_serialize_pipeline.test_serialize_parser_roundtrip_disk(en_vocab,Parser)
spacy.tests.serialize.test_serialize_pipeline.test_serialize_pipe_exclude(en_vocab,Parser)
spacy.tests.serialize.test_serialize_pipeline.test_serialize_tagger_roundtrip_bytes(en_vocab,taggers)
spacy.tests.serialize.test_serialize_pipeline.test_serialize_tagger_roundtrip_disk(en_vocab,taggers)
spacy.tests.serialize.test_serialize_pipeline.test_serialize_tensorizer_roundtrip_bytes(en_vocab)
spacy.tests.serialize.test_serialize_pipeline.test_serialize_tensorizer_roundtrip_disk(en_vocab)
spacy.tests.serialize.test_serialize_pipeline.test_serialize_textcat_empty(en_vocab)
spacy.tests.serialize.test_serialize_pipeline.test_to_from_bytes(parser,blank_parser)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/serialize/test_serialize_vocab_strings.py----------------------------------------
A:spacy.tests.serialize.test_serialize_vocab_strings.text_hash->en_vocab.strings.add(text)
A:spacy.tests.serialize.test_serialize_vocab_strings.vocab_bytes->en_vocab.to_bytes(exclude=['lookups'])
A:spacy.tests.serialize.test_serialize_vocab_strings.new_vocab->Vocab().from_bytes(vocab_bytes)
A:spacy.tests.serialize.test_serialize_vocab_strings.vocab1->Vocab(strings=strings)
A:spacy.tests.serialize.test_serialize_vocab_strings.vocab2->vocab2.from_disk(file_path).from_disk(file_path)
A:spacy.tests.serialize.test_serialize_vocab_strings.vocab1_b->Vocab(strings=strings).to_bytes()
A:spacy.tests.serialize.test_serialize_vocab_strings.vocab2_b->vocab2.from_disk(file_path).from_disk(file_path).to_bytes()
A:spacy.tests.serialize.test_serialize_vocab_strings.new_vocab1->Vocab().from_bytes(vocab1_b)
A:spacy.tests.serialize.test_serialize_vocab_strings.vocab1_d->Vocab().from_disk(file_path1)
A:spacy.tests.serialize.test_serialize_vocab_strings.vocab2_d->Vocab().from_disk(file_path2)
A:spacy.tests.serialize.test_serialize_vocab_strings.vocab->Vocab(strings=strings)
A:spacy.tests.serialize.test_serialize_vocab_strings.length->len(vocab)
A:spacy.tests.serialize.test_serialize_vocab_strings.sstore1->StringStore(strings=strings1)
A:spacy.tests.serialize.test_serialize_vocab_strings.sstore2->StringStore(strings=strings2)
A:spacy.tests.serialize.test_serialize_vocab_strings.sstore1_b->StringStore(strings=strings1).to_bytes()
A:spacy.tests.serialize.test_serialize_vocab_strings.sstore2_b->StringStore(strings=strings2).to_bytes()
A:spacy.tests.serialize.test_serialize_vocab_strings.new_sstore1->StringStore().from_bytes(sstore1_b)
A:spacy.tests.serialize.test_serialize_vocab_strings.sstore1_d->StringStore().from_disk(file_path1)
A:spacy.tests.serialize.test_serialize_vocab_strings.sstore2_d->StringStore().from_disk(file_path2)
spacy.tests.serialize.test_serialize_vocab_strings.test_deserialize_vocab_seen_entries(strings,lex_attr)
spacy.tests.serialize.test_serialize_vocab_strings.test_serialize_stringstore_roundtrip_bytes(strings1,strings2)
spacy.tests.serialize.test_serialize_vocab_strings.test_serialize_stringstore_roundtrip_disk(strings1,strings2)
spacy.tests.serialize.test_serialize_vocab_strings.test_serialize_vocab(en_vocab,text)
spacy.tests.serialize.test_serialize_vocab_strings.test_serialize_vocab_lex_attrs_bytes(strings,lex_attr)
spacy.tests.serialize.test_serialize_vocab_strings.test_serialize_vocab_lex_attrs_disk(strings,lex_attr)
spacy.tests.serialize.test_serialize_vocab_strings.test_serialize_vocab_roundtrip_bytes(strings1,strings2)
spacy.tests.serialize.test_serialize_vocab_strings.test_serialize_vocab_roundtrip_disk(strings1,strings2)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/vocab_vectors/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/vocab_vectors/test_vectors.py----------------------------------------
A:spacy.tests.vocab_vectors.test_vectors.v->Vectors(data=data)
A:spacy.tests.vocab_vectors.test_vectors.orig->numpy.ndarray((5, 3), dtype='f').copy()
A:spacy.tests.vocab_vectors.test_vectors.doc->Doc(vocab, words=text)
A:spacy.tests.vocab_vectors.test_vectors.truth->list(ngrams_vocab.get_vector(text, 1, 6))
A:spacy.tests.vocab_vectors.test_vectors.test->list([(ngrams_vectors[1][1][i] + ngrams_vectors[2][1][i] + ngrams_vectors[3][1][i]) / 3 for i in range(len(ngrams_vectors[1][1]))])
A:spacy.tests.vocab_vectors.test_vectors.token->tokenizer_v(text1)
A:spacy.tests.vocab_vectors.test_vectors.doc1->Doc(vocab, words=text1)
A:spacy.tests.vocab_vectors.test_vectors.doc2->Doc(vocab, words=text2)
A:spacy.tests.vocab_vectors.test_vectors.vocab->Vocab(vectors_name='test_vocab_prune_vectors')
A:spacy.tests.vocab_vectors.test_vectors.data->numpy.ndarray((5, 3), dtype='f')
A:spacy.tests.vocab_vectors.test_vectors.remap->Vocab(vectors_name='test_vocab_prune_vectors').prune_vectors(2)
spacy.tests.vocab_vectors.test_vectors.data()
spacy.tests.vocab_vectors.test_vectors.ngrams_vectors()
spacy.tests.vocab_vectors.test_vectors.ngrams_vocab(en_vocab,ngrams_vectors)
spacy.tests.vocab_vectors.test_vectors.resize_data()
spacy.tests.vocab_vectors.test_vectors.strings()
spacy.tests.vocab_vectors.test_vectors.test_get_vector(strings,data)
spacy.tests.vocab_vectors.test_vectors.test_get_vector_resize(strings,data,resize_data)
spacy.tests.vocab_vectors.test_vectors.test_init_vectors_with_data(strings,data)
spacy.tests.vocab_vectors.test_vectors.test_init_vectors_with_resize_data(data,resize_data)
spacy.tests.vocab_vectors.test_vectors.test_init_vectors_with_resize_shape(strings,resize_data)
spacy.tests.vocab_vectors.test_vectors.test_init_vectors_with_shape(strings)
spacy.tests.vocab_vectors.test_vectors.test_set_vector(strings,data)
spacy.tests.vocab_vectors.test_vectors.test_vectors__ngrams_subword(ngrams_vocab,ngrams_vectors,text)
spacy.tests.vocab_vectors.test_vectors.test_vectors__ngrams_word(ngrams_vocab,ngrams_vectors,text)
spacy.tests.vocab_vectors.test_vectors.test_vectors_doc_doc_similarity(vocab,text1,text2)
spacy.tests.vocab_vectors.test_vectors.test_vectors_doc_vector(vocab,text)
spacy.tests.vocab_vectors.test_vectors.test_vectors_lexeme_doc_similarity(vocab,text)
spacy.tests.vocab_vectors.test_vectors.test_vectors_lexeme_lexeme_similarity(vocab,text1,text2)
spacy.tests.vocab_vectors.test_vectors.test_vectors_lexeme_span_similarity(vocab,text)
spacy.tests.vocab_vectors.test_vectors.test_vectors_lexeme_vector(vocab,text)
spacy.tests.vocab_vectors.test_vectors.test_vectors_span_doc_similarity(vocab,text)
spacy.tests.vocab_vectors.test_vectors.test_vectors_span_span_similarity(vocab,text)
spacy.tests.vocab_vectors.test_vectors.test_vectors_span_vector(vocab,text)
spacy.tests.vocab_vectors.test_vectors.test_vectors_token_doc_similarity(vocab,text)
spacy.tests.vocab_vectors.test_vectors.test_vectors_token_lexeme_similarity(tokenizer_v,vocab,text1,text2)
spacy.tests.vocab_vectors.test_vectors.test_vectors_token_span_similarity(vocab,text)
spacy.tests.vocab_vectors.test_vectors.test_vectors_token_token_similarity(tokenizer_v,text)
spacy.tests.vocab_vectors.test_vectors.test_vectors_token_vector(tokenizer_v,vectors,text)
spacy.tests.vocab_vectors.test_vectors.test_vocab_add_vector()
spacy.tests.vocab_vectors.test_vectors.test_vocab_prune_vectors()
spacy.tests.vocab_vectors.test_vectors.tokenizer_v(vocab)
spacy.tests.vocab_vectors.test_vectors.vectors()
spacy.tests.vocab_vectors.test_vectors.vocab(en_vocab,vectors)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/vocab_vectors/test_lookups.py----------------------------------------
A:spacy.tests.vocab_vectors.test_lookups.lookups->Lookups()
A:spacy.tests.vocab_vectors.test_lookups.table->Vocab().lookups.get_table(table_name)
A:spacy.tests.vocab_vectors.test_lookups.table_bytes->Vocab().lookups.get_table(table_name).to_bytes()
A:spacy.tests.vocab_vectors.test_lookups.new_table->Table().from_bytes(table_bytes)
A:spacy.tests.vocab_vectors.test_lookups.new_table2->Table(data={'def': 456})
A:spacy.tests.vocab_vectors.test_lookups.lookups_bytes->Lookups().to_bytes()
A:spacy.tests.vocab_vectors.test_lookups.new_lookups->Lookups()
A:spacy.tests.vocab_vectors.test_lookups.table1->Lookups().get_table('table1')
A:spacy.tests.vocab_vectors.test_lookups.table2->Lookups().get_table('table2')
A:spacy.tests.vocab_vectors.test_lookups.vocab->Vocab()
A:spacy.tests.vocab_vectors.test_lookups.vocab_bytes->Vocab().to_bytes()
A:spacy.tests.vocab_vectors.test_lookups.new_vocab->Vocab()
spacy.tests.vocab_vectors.test_lookups.test_lookups_api()
spacy.tests.vocab_vectors.test_lookups.test_lookups_to_from_bytes()
spacy.tests.vocab_vectors.test_lookups.test_lookups_to_from_bytes_via_vocab()
spacy.tests.vocab_vectors.test_lookups.test_lookups_to_from_disk()
spacy.tests.vocab_vectors.test_lookups.test_lookups_to_from_disk_via_vocab()
spacy.tests.vocab_vectors.test_lookups.test_table_api()
spacy.tests.vocab_vectors.test_lookups.test_table_api_to_from_bytes()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/vocab_vectors/test_lexeme.py----------------------------------------
A:spacy.tests.vocab_vectors.test_lexeme.is_len4->en_vocab.add_flag(lambda string: len(string) == 4, flag_id=IS_DIGIT)
spacy.tests.vocab_vectors.test_lexeme.test_lexeme_bytes_roundtrip(en_vocab)
spacy.tests.vocab_vectors.test_lexeme.test_vocab_lexeme_add_flag_auto_id(en_vocab)
spacy.tests.vocab_vectors.test_lexeme.test_vocab_lexeme_add_flag_provided_id(en_vocab)
spacy.tests.vocab_vectors.test_lexeme.test_vocab_lexeme_hash(en_vocab,text1,text2)
spacy.tests.vocab_vectors.test_lexeme.test_vocab_lexeme_is_alpha(en_vocab)
spacy.tests.vocab_vectors.test_lexeme.test_vocab_lexeme_is_digit(en_vocab)
spacy.tests.vocab_vectors.test_lexeme.test_vocab_lexeme_lt(en_vocab,text1,text2,prob1,prob2)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/vocab_vectors/test_stringstore.py----------------------------------------
A:spacy.tests.vocab_vectors.test_stringstore.h->stringstore.add(heart)
A:spacy.tests.vocab_vectors.test_stringstore.apple_hash->stringstore.add('apple')
A:spacy.tests.vocab_vectors.test_stringstore.banana_hash->stringstore.add('banana')
A:spacy.tests.vocab_vectors.test_stringstore.key->stringstore.add(text)
A:spacy.tests.vocab_vectors.test_stringstore.store->stringstore.add(text)
A:spacy.tests.vocab_vectors.test_stringstore.serialized->stringstore.to_bytes()
A:spacy.tests.vocab_vectors.test_stringstore.new_stringstore->StringStore().from_bytes(serialized)
spacy.tests.vocab_vectors.test_stringstore.stringstore()
spacy.tests.vocab_vectors.test_stringstore.test_string_hash(stringstore)
spacy.tests.vocab_vectors.test_stringstore.test_stringstore_freeze_oov(stringstore,text)
spacy.tests.vocab_vectors.test_stringstore.test_stringstore_from_api_docs(stringstore)
spacy.tests.vocab_vectors.test_stringstore.test_stringstore_long_string(stringstore)
spacy.tests.vocab_vectors.test_stringstore.test_stringstore_massive_strings(stringstore)
spacy.tests.vocab_vectors.test_stringstore.test_stringstore_med_string(stringstore,text1,text2)
spacy.tests.vocab_vectors.test_stringstore.test_stringstore_multiply(stringstore,factor)
spacy.tests.vocab_vectors.test_stringstore.test_stringstore_retrieve_id(stringstore,text)
spacy.tests.vocab_vectors.test_stringstore.test_stringstore_save_bytes(stringstore,text1,text2,text3)
spacy.tests.vocab_vectors.test_stringstore.test_stringstore_save_unicode(stringstore,text1,text2,text3)
spacy.tests.vocab_vectors.test_stringstore.test_stringstore_to_bytes(stringstore,text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/vocab_vectors/test_vocab_api.py----------------------------------------
spacy.tests.vocab_vectors.test_vocab_api.test_vocab_api_contains(en_vocab,text)
spacy.tests.vocab_vectors.test_vocab_api.test_vocab_api_eq(en_vocab,text)
spacy.tests.vocab_vectors.test_vocab_api.test_vocab_api_neq(en_vocab,text1,text2)
spacy.tests.vocab_vectors.test_vocab_api.test_vocab_api_shape_attr(en_vocab,text)
spacy.tests.vocab_vectors.test_vocab_api.test_vocab_api_symbols(en_vocab,string,symbol)
spacy.tests.vocab_vectors.test_vocab_api.test_vocab_writing_system(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/vocab_vectors/test_similarity.py----------------------------------------
A:spacy.tests.vocab_vectors.test_similarity.doc->Doc(vocab, words=[word1, word2])
spacy.tests.vocab_vectors.test_similarity.test_vectors_similarity_DS(vocab,vectors)
spacy.tests.vocab_vectors.test_similarity.test_vectors_similarity_LL(vocab,vectors)
spacy.tests.vocab_vectors.test_similarity.test_vectors_similarity_TD(vocab,vectors)
spacy.tests.vocab_vectors.test_similarity.test_vectors_similarity_TS(vocab,vectors)
spacy.tests.vocab_vectors.test_similarity.test_vectors_similarity_TT(vocab,vectors)
spacy.tests.vocab_vectors.test_similarity.vectors()
spacy.tests.vocab_vectors.test_similarity.vocab(en_vocab,vectors)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/doc/test_underscore.py----------------------------------------
A:spacy.tests.doc.test_underscore.doc->Doc(en_vocab, words=['hello', 'world'])
A:spacy.tests.doc.test_underscore.uscore->Underscore(Underscore.span_extensions, span, start=span.start, end=span.end)
A:spacy.tests.doc.test_underscore.doc._->Underscore(Underscore.doc_extensions, doc)
A:spacy.tests.doc.test_underscore.span->Mock(doc=Mock(), start=0, end=2)
A:spacy.tests.doc.test_underscore.span._->Underscore(Underscore.span_extensions, span, start=span.start, end=span.end)
A:spacy.tests.doc.test_underscore.token->Mock(doc=Mock(), idx=7, say_cheese=lambda token: 'cheese')
A:spacy.tests.doc.test_underscore.token._->Underscore(Underscore.token_extensions, token, start=token.idx)
A:spacy.tests.doc.test_underscore.doc1->Doc(en_vocab, words=['one'])
A:spacy.tests.doc.test_underscore.doc2->Doc(en_vocab, words=['two'])
spacy.tests.doc.test_underscore.test_create_doc_underscore()
spacy.tests.doc.test_underscore.test_create_span_underscore()
spacy.tests.doc.test_underscore.test_doc_underscore_getattr_setattr()
spacy.tests.doc.test_underscore.test_doc_underscore_remove_extension(obj)
spacy.tests.doc.test_underscore.test_span_underscore_getter_setter()
spacy.tests.doc.test_underscore.test_token_underscore_method()
spacy.tests.doc.test_underscore.test_underscore_accepts_valid(valid_kwargs)
spacy.tests.doc.test_underscore.test_underscore_dir(en_vocab)
spacy.tests.doc.test_underscore.test_underscore_docstring(en_vocab)
spacy.tests.doc.test_underscore.test_underscore_mutable_defaults_dict(en_vocab)
spacy.tests.doc.test_underscore.test_underscore_mutable_defaults_list(en_vocab)
spacy.tests.doc.test_underscore.test_underscore_raises_for_dup(obj)
spacy.tests.doc.test_underscore.test_underscore_raises_for_invalid(invalid_kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/doc/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/doc/test_retokenize_merge.py----------------------------------------
A:spacy.tests.doc.test_retokenize_merge.doc->Doc(en_vocab, words=['hello', 'world', '!'])
A:spacy.tests.doc.test_retokenize_merge.new_doc->Doc(doc.vocab, words=['beach boys'])
A:spacy.tests.doc.test_retokenize_merge.tokens->en_tokenizer(text)
A:spacy.tests.doc.test_retokenize_merge.ent_type->max((w.ent_type_ for w in ent))
A:spacy.tests.doc.test_retokenize_merge.(sent1, sent2)->list(doc.sents)
A:spacy.tests.doc.test_retokenize_merge.init_len->len(list(sent1.root.subtree))
A:spacy.tests.doc.test_retokenize_merge.init_len2->len(sent2)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_lex_attrs(en_tokenizer)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_merge(en_tokenizer)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_merge_children(en_tokenizer)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_merge_extension_attrs(en_vocab)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_merge_extension_attrs_invalid(en_vocab,underscore_attrs)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_merge_hang(en_tokenizer)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_retokenizer(en_tokenizer)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_retokenizer_attrs(en_tokenizer)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_span_np_merges(en_tokenizer)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_spans_entity_merge(en_tokenizer)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_spans_entity_merge_iob(en_vocab)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_spans_merge_heads(en_tokenizer)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_spans_merge_non_disjoint(en_tokenizer)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_spans_merge_tokens(en_tokenizer)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_spans_merge_tokens_default_attrs(en_tokenizer)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_spans_sentence_update_after_merge(en_tokenizer)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_spans_subtree_size_check(en_tokenizer)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenizer_merge_lex_attrs(en_vocab)
spacy.tests.doc.test_retokenize_merge.test_retokenize_skip_duplicates(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/doc/test_pickle_doc.py----------------------------------------
A:spacy.tests.doc.test_pickle_doc.nlp->Language()
A:spacy.tests.doc.test_pickle_doc.doc->nlp('Hello')
A:spacy.tests.doc.test_pickle_doc.data->spacy.compat.pickle.dumps(doc, 1)
A:spacy.tests.doc.test_pickle_doc.doc2->spacy.compat.pickle.loads(b)
A:spacy.tests.doc.test_pickle_doc.one_pickled->spacy.compat.pickle.dumps(nlp('0'), -1)
A:spacy.tests.doc.test_pickle_doc.docs->list(nlp.pipe((unicode_(i) for i in range(100))))
A:spacy.tests.doc.test_pickle_doc.many_pickled->spacy.compat.pickle.dumps(docs, -1)
A:spacy.tests.doc.test_pickle_doc.many_unpickled->spacy.compat.pickle.loads(many_pickled)
A:spacy.tests.doc.test_pickle_doc.b->spacy.compat.pickle.dumps(doc)
spacy.tests.doc.test_pickle_doc.test_hooks_unpickle()
spacy.tests.doc.test_pickle_doc.test_list_of_docs_pickles_efficiently()
spacy.tests.doc.test_pickle_doc.test_pickle_single_doc()
spacy.tests.doc.test_pickle_doc.test_user_data_from_disk()
spacy.tests.doc.test_pickle_doc.test_user_data_unpickles()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/doc/test_doc_api.py----------------------------------------
A:spacy.tests.doc.test_doc_api.doc->Doc(en_vocab, words=['Hello', 'world'])
A:spacy.tests.doc.test_doc_api.tokens->en_tokenizer(sentence)
A:spacy.tests.doc.test_doc_api.new_tokens->Doc(tokens.vocab).from_bytes(tokens.to_bytes(exclude=['sentiment']), exclude=['sentiment'])
A:spacy.tests.doc.test_doc_api.sents->list(doc.sents)
A:spacy.tests.doc.test_doc_api.vocab->Vocab()
A:spacy.tests.doc.test_doc_api.doc2->Doc(doc.vocab, words=['a', 'b', 'c'])
A:spacy.tests.doc.test_doc_api.lca->Doc(en_vocab, words=['Hello', 'world']).get_lca_matrix()
A:spacy.tests.doc.test_doc_api.arr->numpy.array([[0, 0], [0, 0], [0, 0], [384, 3], [384, 1]], dtype='uint64')
A:spacy.tests.doc.test_doc_api.new_doc->Doc(en_vocab).from_bytes(doc.to_bytes())
spacy.tests.doc.test_doc_api.test_doc_api_compare_by_string_position(en_vocab,text)
spacy.tests.doc.test_doc_api.test_doc_api_getitem(en_tokenizer)
spacy.tests.doc.test_doc_api.test_doc_api_has_vector()
spacy.tests.doc.test_doc_api.test_doc_api_right_edge(en_tokenizer)
spacy.tests.doc.test_doc_api.test_doc_api_runtime_error(en_tokenizer)
spacy.tests.doc.test_doc_api.test_doc_api_sents_empty_string(en_tokenizer)
spacy.tests.doc.test_doc_api.test_doc_api_serialize(en_tokenizer,text)
spacy.tests.doc.test_doc_api.test_doc_api_set_ents(en_tokenizer)
spacy.tests.doc.test_doc_api.test_doc_api_similarity_match()
spacy.tests.doc.test_doc_api.test_doc_is_nered(en_vocab)
spacy.tests.doc.test_doc_api.test_doc_lang(en_vocab)
spacy.tests.doc.test_doc_api.test_lowest_common_ancestor(en_tokenizer,sentence,heads,lca_matrix)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/doc/test_creation.py----------------------------------------
A:spacy.tests.doc.test_creation.lookups->Lookups()
A:spacy.tests.doc.test_creation.doc->Doc(vocab, words=['dogs', 'dogses'])
spacy.tests.doc.test_creation.lemmatizer()
spacy.tests.doc.test_creation.test_empty_doc(vocab)
spacy.tests.doc.test_creation.test_lookup_lemmatization(vocab)
spacy.tests.doc.test_creation.test_single_word(vocab)
spacy.tests.doc.test_creation.vocab(lemmatizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/doc/test_span.py----------------------------------------
A:spacy.tests.doc.test_span.tokens->en_tokenizer(text)
A:spacy.tests.doc.test_span.doc->Doc(tokens.vocab, words=[t.text for t in tokens])
A:spacy.tests.doc.test_span.sents->list(doc.sents)
A:spacy.tests.doc.test_span.lca->doc[2:].get_lca_matrix()
A:spacy.tests.doc.test_span.span2->Doc(tokens.vocab, words=[t.text for t in tokens]).char_span(span1.start_char, span1.end_char, label='GPE')
A:spacy.tests.doc.test_span.arr->Span(doc, 0, 1).to_array([ORTH, LENGTH])
A:spacy.tests.doc.test_span.span_doc->Span(doc, 0, 1).as_doc()
A:spacy.tests.doc.test_span.span_doc_with->Span(doc, 0, 1).as_doc(copy_user_data=True)
A:spacy.tests.doc.test_span.span_doc_without->Span(doc, 0, 1).as_doc()
A:spacy.tests.doc.test_span.span->Span(doc, 0, 1)
A:spacy.tests.doc.test_span.sentences->list(doc.sents)
A:spacy.tests.doc.test_span.filtered->filter_spans(spans)
spacy.tests.doc.test_span.doc(en_tokenizer)
spacy.tests.doc.test_span.doc_not_parsed(en_tokenizer)
spacy.tests.doc.test_span.test_filter_spans(doc)
spacy.tests.doc.test_span.test_span_as_doc(doc)
spacy.tests.doc.test_span.test_span_as_doc_user_data(doc)
spacy.tests.doc.test_span.test_span_ents_property(doc)
spacy.tests.doc.test_span.test_span_kb_id_readonly(doc)
spacy.tests.doc.test_span.test_span_label_readonly(doc)
spacy.tests.doc.test_span.test_span_similarity_match()
spacy.tests.doc.test_span.test_span_string_label_kb_id(doc)
spacy.tests.doc.test_span.test_span_to_array(doc)
spacy.tests.doc.test_span.test_spans_are_hashable(en_tokenizer)
spacy.tests.doc.test_span.test_spans_by_character(doc)
spacy.tests.doc.test_span.test_spans_default_sentiment(en_tokenizer)
spacy.tests.doc.test_span.test_spans_lca_matrix(en_tokenizer)
spacy.tests.doc.test_span.test_spans_override_sentiment(en_tokenizer)
spacy.tests.doc.test_span.test_spans_root(doc)
spacy.tests.doc.test_span.test_spans_root2(en_tokenizer)
spacy.tests.doc.test_span.test_spans_sent_spans(doc)
spacy.tests.doc.test_span.test_spans_span_sent(doc,doc_not_parsed)
spacy.tests.doc.test_span.test_spans_string_fn(doc)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/doc/test_array.py----------------------------------------
A:spacy.tests.doc.test_array.doc->Doc(en_vocab, words=words)
A:spacy.tests.doc.test_array.feats_array->Doc(en_vocab, words=words).to_array((ORTH, DEP))
A:spacy.tests.doc.test_array.feats_array_stringy->Doc(en_vocab, words=words).to_array(('ORTH', 'SHAPE'))
spacy.tests.doc.test_array.test_doc_array_attr_of_token(en_vocab)
spacy.tests.doc.test_array.test_doc_array_dep(en_vocab)
spacy.tests.doc.test_array.test_doc_array_tag(en_vocab)
spacy.tests.doc.test_array.test_doc_array_to_from_string_attrs(en_vocab,attrs)
spacy.tests.doc.test_array.test_doc_scalar_attr_of_token(en_vocab)
spacy.tests.doc.test_array.test_doc_stringy_array_attr_of_token(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/doc/test_to_json.py----------------------------------------
A:spacy.tests.doc.test_to_json.json_doc->doc.to_json()
A:spacy.tests.doc.test_to_json.validator->get_json_validator(TRAINING_SCHEMA)
A:spacy.tests.doc.test_to_json.errors->validate_json([json_doc], validator)
spacy.tests.doc.test_to_json.doc(en_vocab)
spacy.tests.doc.test_to_json.test_doc_to_json(doc)
spacy.tests.doc.test_to_json.test_doc_to_json_underscore(doc)
spacy.tests.doc.test_to_json.test_doc_to_json_underscore_error_attr(doc)
spacy.tests.doc.test_to_json.test_doc_to_json_underscore_error_serialize(doc)
spacy.tests.doc.test_to_json.test_doc_to_json_valid_training(doc)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/doc/test_retokenize_split.py----------------------------------------
A:spacy.tests.doc.test_retokenize_split.doc->Doc(en_vocab, words=['LosAngeles', 'start'])
A:spacy.tests.doc.test_retokenize_split.dep1->Doc(en_vocab, words=['LosAngeles', 'start']).vocab.strings.add('amod')
A:spacy.tests.doc.test_retokenize_split.dep2->Doc(en_vocab, words=['LosAngeles', 'start']).vocab.strings.add('subject')
A:spacy.tests.doc.test_retokenize_split.(sent1, sent2)->list(doc.sents)
A:spacy.tests.doc.test_retokenize_split.init_len->len(sent1)
A:spacy.tests.doc.test_retokenize_split.init_len2->len(sent2)
spacy.tests.doc.test_retokenize_split.test_doc_retokenize_spans_entity_split_iob()
spacy.tests.doc.test_retokenize_split.test_doc_retokenize_spans_sentence_update_after_split(en_vocab)
spacy.tests.doc.test_retokenize_split.test_doc_retokenize_split(en_vocab)
spacy.tests.doc.test_retokenize_split.test_doc_retokenize_split_dependencies(en_vocab)
spacy.tests.doc.test_retokenize_split.test_doc_retokenize_split_extension_attrs(en_vocab)
spacy.tests.doc.test_retokenize_split.test_doc_retokenize_split_extension_attrs_invalid(en_vocab,underscore_attrs)
spacy.tests.doc.test_retokenize_split.test_doc_retokenize_split_heads_error(en_vocab)
spacy.tests.doc.test_retokenize_split.test_doc_retokenize_split_orths_mismatch(en_vocab)
spacy.tests.doc.test_retokenize_split.test_doc_retokenizer_split_lex_attrs(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/doc/test_add_entities.py----------------------------------------
A:spacy.tests.doc.test_add_entities.doc->get_doc(en_vocab, text)
A:spacy.tests.doc.test_add_entities.ner->EntityRecognizer(en_vocab)
A:spacy.tests.doc.test_add_entities.doc.ents->list(doc.ents)
A:spacy.tests.doc.test_add_entities.entity->Span(doc, 0, 4, label=391)
A:spacy.tests.doc.test_add_entities.new_entity->Span(doc, 0, 1, label=392)
spacy.tests.doc.test_add_entities.test_add_overlapping_entities(en_vocab)
spacy.tests.doc.test_add_entities.test_doc_add_entities_set_ents_iob(en_vocab)
spacy.tests.doc.test_add_entities.test_ents_reset(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/doc/test_morphanalysis.py----------------------------------------
A:spacy.tests.doc.test_morphanalysis.doc->en_tokenizer('I has')
spacy.tests.doc.test_morphanalysis.i_has(en_tokenizer)
spacy.tests.doc.test_morphanalysis.test_morph_get(i_has)
spacy.tests.doc.test_morphanalysis.test_morph_iter(i_has)
spacy.tests.doc.test_morphanalysis.test_morph_props(i_has)
spacy.tests.doc.test_morphanalysis.test_token_morph_id(i_has)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/doc/test_token_api.py----------------------------------------
A:spacy.tests.doc.test_token_api.tokens->en_tokenizer(text)
A:spacy.tests.doc.test_token_api.doc->get_doc(en_vocab, words=words, heads=heads, deps=deps)
A:spacy.tests.doc.test_token_api.vocab->Vocab()
A:spacy.tests.doc.test_token_api.words->'They came .'.split()
spacy.tests.doc.test_token_api.doc(en_tokenizer)
spacy.tests.doc.test_token_api.test_doc_token_api_ancestors(en_tokenizer)
spacy.tests.doc.test_token_api.test_doc_token_api_flags(en_tokenizer)
spacy.tests.doc.test_token_api.test_doc_token_api_head_setter(en_tokenizer)
spacy.tests.doc.test_token_api.test_doc_token_api_is_properties(en_vocab)
spacy.tests.doc.test_token_api.test_doc_token_api_prob_inherited_from_vocab(en_tokenizer,text)
spacy.tests.doc.test_token_api.test_doc_token_api_str_builtin(en_tokenizer,text)
spacy.tests.doc.test_token_api.test_doc_token_api_strings(en_tokenizer)
spacy.tests.doc.test_token_api.test_doc_token_api_vectors()
spacy.tests.doc.test_token_api.test_is_sent_start(en_tokenizer)
spacy.tests.doc.test_token_api.test_set_pos()
spacy.tests.doc.test_token_api.test_token0_has_sent_start_true()
spacy.tests.doc.test_token_api.test_token_api_conjuncts_chain(en_vocab)
spacy.tests.doc.test_token_api.test_token_api_conjuncts_simple(en_vocab)
spacy.tests.doc.test_token_api.test_token_api_non_conjuncts(en_vocab)
spacy.tests.doc.test_token_api.test_tokens_sent(doc)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/matcher/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/matcher/test_pattern_validation.py----------------------------------------
A:spacy.tests.matcher.test_pattern_validation.matcher->Matcher(en_vocab)
A:spacy.tests.matcher.test_pattern_validation.errors->validate_json(pattern, validator)
spacy.tests.matcher.test_pattern_validation.test_matcher_pattern_validation(en_vocab,pattern)
spacy.tests.matcher.test_pattern_validation.test_minimal_pattern_validation(en_vocab,pattern,n_errors,n_min_errors)
spacy.tests.matcher.test_pattern_validation.test_pattern_validation(validator,pattern,n_errors,_)
spacy.tests.matcher.test_pattern_validation.test_xfail_pattern_validation(validator,pattern,n_errors,_)
spacy.tests.matcher.test_pattern_validation.validator()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/matcher/test_matcher_api.py----------------------------------------
A:spacy.tests.matcher.test_matcher_api.matcher->Matcher(en_vocab)
A:spacy.tests.matcher.test_matcher_api.(on_match, patterns)->Matcher(en_vocab).get('Rule')
A:spacy.tests.matcher.test_matcher_api.doc->Doc(en_vocab, words=text.split(' '))
A:spacy.tests.matcher.test_matcher_api.matches->matcher(doc)
A:spacy.tests.matcher.test_matcher_api.words1->'He said , " some words " ...'.split()
A:spacy.tests.matcher.test_matcher_api.words2->'He said , " some three words " ...'.split()
A:spacy.tests.matcher.test_matcher_api.words->'He said , " some words " ...'.split()
A:spacy.tests.matcher.test_matcher_api.control->Matcher(matcher.vocab)
A:spacy.tests.matcher.test_matcher_api.m->matcher(doc)
A:spacy.tests.matcher.test_matcher_api.IS_BROWN_YELLOW->en_vocab.add_flag(is_brown_yellow)
A:spacy.tests.matcher.test_matcher_api.doc1->Doc(en_vocab, words=['Test'])
A:spacy.tests.matcher.test_matcher_api.doc2->Doc(en_vocab, words=['Test'])
A:spacy.tests.matcher.test_matcher_api.doc3->Doc(en_vocab, words=['Test'])
spacy.tests.matcher.test_matcher_api.dependency_matcher(en_vocab)
spacy.tests.matcher.test_matcher_api.deps()
spacy.tests.matcher.test_matcher_api.heads()
spacy.tests.matcher.test_matcher_api.matcher(en_vocab)
spacy.tests.matcher.test_matcher_api.test_attr_pipeline_checks(en_vocab)
spacy.tests.matcher.test_matcher_api.test_dependency_matcher_compile(dependency_matcher)
spacy.tests.matcher.test_matcher_api.test_matcher_any_token_operator(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_compare_length(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_empty_dict(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_extension_attribute(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_extension_set_membership(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_from_api_docs(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_from_usage_docs(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_len_contains(matcher)
spacy.tests.matcher.test_matcher_api.test_matcher_match_end(matcher)
spacy.tests.matcher.test_matcher_api.test_matcher_match_middle(matcher)
spacy.tests.matcher.test_matcher_api.test_matcher_match_multi(matcher)
spacy.tests.matcher.test_matcher_api.test_matcher_match_one_plus(matcher)
spacy.tests.matcher.test_matcher_api.test_matcher_match_start(matcher)
spacy.tests.matcher.test_matcher_api.test_matcher_match_zero(matcher)
spacy.tests.matcher.test_matcher_api.test_matcher_match_zero_plus(matcher)
spacy.tests.matcher.test_matcher_api.test_matcher_no_match(matcher)
spacy.tests.matcher.test_matcher_api.test_matcher_operator_shadow(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_regex(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_regex_shape(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_schema_token_attributes(en_vocab,pattern,text)
spacy.tests.matcher.test_matcher_api.test_matcher_set_value(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_set_value_operator(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_valid_callback(en_vocab)
spacy.tests.matcher.test_matcher_api.text()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/matcher/test_matcher_logic.py----------------------------------------
A:spacy.tests.matcher.test_matcher_logic.doc->Doc(en_vocab, words='zero one two three'.split())
A:spacy.tests.matcher.test_matcher_logic.matcher->Matcher(en_vocab)
A:spacy.tests.matcher.test_matcher_logic.matches->matcher(doc)
spacy.tests.matcher.test_matcher_logic.doc(en_tokenizer,text)
spacy.tests.matcher.test_matcher_logic.test_greedy_matching(doc,text,pattern,re_pattern)
spacy.tests.matcher.test_matcher_logic.test_match_consuming(doc,text,pattern,re_pattern)
spacy.tests.matcher.test_matcher_logic.test_matcher_end_zero_plus(en_vocab)
spacy.tests.matcher.test_matcher_logic.test_matcher_sets_return_correct_tokens(en_vocab)
spacy.tests.matcher.test_matcher_logic.test_operator_combos(en_vocab)
spacy.tests.matcher.test_matcher_logic.text()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/matcher/test_phrase_matcher.py----------------------------------------
A:spacy.tests.matcher.test_phrase_matcher.doc->Doc(en_vocab, words=words2)
A:spacy.tests.matcher.test_phrase_matcher.pattern->Doc(en_vocab, words=words1)
A:spacy.tests.matcher.test_phrase_matcher.matcher->PhraseMatcher(en_vocab, attr='TEXT')
A:spacy.tests.matcher.test_phrase_matcher.matches->matcher(doc)
A:spacy.tests.matcher.test_phrase_matcher.doc1->Doc(en_vocab, words=['Test'])
A:spacy.tests.matcher.test_phrase_matcher.doc2->Doc(en_vocab, words=['Test'])
A:spacy.tests.matcher.test_phrase_matcher.doc3->Doc(en_vocab, words=['Test'])
spacy.tests.matcher.test_phrase_matcher.test_attr_pipeline_checks(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_attr_validation(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_matcher_phrase_matcher(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_phrase_matcher_bool_attrs(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_phrase_matcher_contains(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_phrase_matcher_length(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_phrase_matcher_overlapping_with_remove(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_phrase_matcher_remove(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_phrase_matcher_repeated_add(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_phrase_matcher_string_attrs(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_phrase_matcher_string_attrs_negative(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_phrase_matcher_validation(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue4042.py----------------------------------------
A:spacy.tests.regression.test_issue4042.nlp->English()
A:spacy.tests.regression.test_issue4042.ner->English().create_pipe('ner')
A:spacy.tests.regression.test_issue4042.ruler->EntityRuler(nlp)
A:spacy.tests.regression.test_issue4042.doc1->nlp1('What do you think about Apple ?')
A:spacy.tests.regression.test_issue4042.output_dir->ensure_path(d)
A:spacy.tests.regression.test_issue4042.nlp2->English(vocab)
A:spacy.tests.regression.test_issue4042.doc2->nlp2('What do you think about Apple ?')
A:spacy.tests.regression.test_issue4042.nlp1->English()
A:spacy.tests.regression.test_issue4042.ner1->English().create_pipe('ner')
A:spacy.tests.regression.test_issue4042.apple_ent->Span(doc1, 5, 6, label='MY_ORG')
A:spacy.tests.regression.test_issue4042.ner2->EntityRecognizer(vocab)
spacy.tests.regression.test_issue4042.test_issue4042()
spacy.tests.regression.test_issue4042.test_issue4042_bug2()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue3625.py----------------------------------------
A:spacy.tests.regression.test_issue3625.nlp->Hindi()
A:spacy.tests.regression.test_issue3625.doc->nlp('hi. how हुए. होटल, होटल')
spacy.tests.regression.test_issue3625.test_issue3625()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue3972.py----------------------------------------
A:spacy.tests.regression.test_issue3972.matcher->PhraseMatcher(en_vocab)
A:spacy.tests.regression.test_issue3972.doc->Doc(en_vocab, words=['I', 'live', 'in', 'New', 'York'])
A:spacy.tests.regression.test_issue3972.matches->matcher(doc)
spacy.tests.regression.test_issue3972.test_issue3972(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue1001-1500.py----------------------------------------
A:spacy.tests.regression.test_issue1001-1500.tokenizer->spacy.lang.en.English.Defaults.create_tokenizer()
A:spacy.tests.regression.test_issue1001-1500.doc->nlp('This is a test.')
A:spacy.tests.regression.test_issue1001-1500.nlp->English()
A:spacy.tests.regression.test_issue1001-1500.docs->list(nlp.pipe(['', 'hello']))
A:spacy.tests.regression.test_issue1001-1500.doc1->Doc(Vocab(), words=['a', 'b', 'c'])
A:spacy.tests.regression.test_issue1001-1500.doc2->Doc(Vocab(), words=['a', 'c', 'e'])
A:spacy.tests.regression.test_issue1001-1500.lookups->Lookups()
A:spacy.tests.regression.test_issue1001-1500.lemmatizer->Lemmatizer(lookups)
A:spacy.tests.regression.test_issue1001-1500.vocab->Vocab(lex_attr_getters=LEX_ATTRS)
A:spacy.tests.regression.test_issue1001-1500.hello_world->Doc(vocab, words=['Hello', 'World'])
A:spacy.tests.regression.test_issue1001-1500.hello->Doc(vocab, words=['Hello'])
A:spacy.tests.regression.test_issue1001-1500.matcher->Matcher(Vocab())
A:spacy.tests.regression.test_issue1001-1500.matches->matcher(doc)
A:spacy.tests.regression.test_issue1001-1500.prefix_re->re.compile('[\\[\\("\']')
A:spacy.tests.regression.test_issue1001-1500.suffix_re->re.compile('[\\]\\)"\']')
A:spacy.tests.regression.test_issue1001-1500.infix_re->re.compile('[^a-z]')
A:spacy.tests.regression.test_issue1001-1500.simple_url_re->re.compile('^https?://')
A:spacy.tests.regression.test_issue1001-1500.nlp.tokenizer->new_tokenizer(nlp)
spacy.tests.regression.test_issue1001-1500.test_issue1061()
spacy.tests.regression.test_issue1001-1500.test_issue1235()
spacy.tests.regression.test_issue1001-1500.test_issue1242()
spacy.tests.regression.test_issue1001-1500.test_issue1250()
spacy.tests.regression.test_issue1001-1500.test_issue1257()
spacy.tests.regression.test_issue1001-1500.test_issue1375()
spacy.tests.regression.test_issue1001-1500.test_issue1387()
spacy.tests.regression.test_issue1001-1500.test_issue1434()
spacy.tests.regression.test_issue1001-1500.test_issue1450(string,start,end)
spacy.tests.regression.test_issue1001-1500.test_issue1488()
spacy.tests.regression.test_issue1001-1500.test_issue1494()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue3611.py----------------------------------------
A:spacy.tests.regression.test_issue3611.pos_cats->list()
A:spacy.tests.regression.test_issue3611.train_data->list(zip(x_train, [{'cats': cats} for cats in pos_cats]))
A:spacy.tests.regression.test_issue3611.nlp->spacy.blank('en')
A:spacy.tests.regression.test_issue3611.textcat->spacy.blank('en').create_pipe('textcat', config={'exclusive_classes': True, 'architecture': 'bow', 'ngram_size': 2})
A:spacy.tests.regression.test_issue3611.optimizer->spacy.blank('en').begin_training()
A:spacy.tests.regression.test_issue3611.batches->minibatch(train_data, size=compounding(4.0, 32.0, 1.001))
A:spacy.tests.regression.test_issue3611.(texts, annotations)->zip(*batch)
spacy.tests.regression.test_issue3611.test_issue3611()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue4278.py----------------------------------------
A:spacy.tests.regression.test_issue4278.doc->nlp.make_doc('foo')
A:spacy.tests.regression.test_issue4278.dummy_pipe->DummyPipe()
spacy.tests.regression.test_issue4278.DummyPipe(self)
spacy.tests.regression.test_issue4278.DummyPipe.__init__(self)
spacy.tests.regression.test_issue4278.DummyPipe.predict(self,docs)
spacy.tests.regression.test_issue4278.DummyPipe.set_annotations(self,docs,scores,tensors=None)
spacy.tests.regression.test_issue4278.nlp()
spacy.tests.regression.test_issue4278.test_multiple_predictions(nlp)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue3001-3500.py----------------------------------------
A:spacy.tests.regression.test_issue3001-3500.nlp->English()
A:spacy.tests.regression.test_issue3001-3500.doc->nlp('Hello world')
A:spacy.tests.regression.test_issue3001-3500.matcher->Matcher(nlp.vocab)
A:spacy.tests.regression.test_issue3001-3500.matches->matcher(doc)
A:spacy.tests.regression.test_issue3001-3500.ent_array->nlp('Hello world').to_array(header)
A:spacy.tests.regression.test_issue3001-3500.doc_bytes->nlp('Hello world').to_bytes()
A:spacy.tests.regression.test_issue3001-3500.doc2->Doc(en_vocab).from_bytes(doc_bytes)
A:spacy.tests.regression.test_issue3001-3500.ner->EntityRecognizer(doc.vocab)
A:spacy.tests.regression.test_issue3001-3500.nlp2->English()
A:spacy.tests.regression.test_issue3001-3500.data->spacy.compat.pickle.dumps(matcher)
A:spacy.tests.regression.test_issue3001-3500.new_matcher->spacy.compat.pickle.loads(data)
A:spacy.tests.regression.test_issue3001-3500.doc.tensor->numpy.zeros((len(words), 96), dtype='float32')
A:spacy.tests.regression.test_issue3001-3500.bytes_data->English().to_bytes()
A:spacy.tests.regression.test_issue3001-3500.new_nlp->English()
A:spacy.tests.regression.test_issue3001-3500.ruler->EntityRuler(nlp, patterns=[{'label': 'GPE', 'pattern': 'New York'}])
A:spacy.tests.regression.test_issue3001-3500.pattern->re.compile(unescape_unicode(prefix_search.decode('utf8')))
A:spacy.tests.regression.test_issue3001-3500.phrasematcher->PhraseMatcher(nlp.vocab)
A:spacy.tests.regression.test_issue3001-3500.docs->list(nlp.tokenizer.pipe(texts, n_threads=4))
A:spacy.tests.regression.test_issue3001-3500.sizes->decaying(10.0, 1.0, 0.5)
A:spacy.tests.regression.test_issue3001-3500.size->next(sizes)
A:spacy.tests.regression.test_issue3001-3500.t1->nlp(text1)
A:spacy.tests.regression.test_issue3001-3500.t2->nlp(text2)
A:spacy.tests.regression.test_issue3001-3500.t3->nlp(text3)
A:spacy.tests.regression.test_issue3001-3500.new_doc->Doc(nlp.vocab).from_bytes(doc_bytes)
spacy.tests.regression.test_issue3001-3500.test_issue3002()
spacy.tests.regression.test_issue3001-3500.test_issue3009(en_vocab)
spacy.tests.regression.test_issue3001-3500.test_issue3012(en_vocab)
spacy.tests.regression.test_issue3001-3500.test_issue3199()
spacy.tests.regression.test_issue3001-3500.test_issue3209()
spacy.tests.regression.test_issue3001-3500.test_issue3248_1()
spacy.tests.regression.test_issue3001-3500.test_issue3248_2()
spacy.tests.regression.test_issue3001-3500.test_issue3277(es_tokenizer)
spacy.tests.regression.test_issue3001-3500.test_issue3288(en_vocab)
spacy.tests.regression.test_issue3001-3500.test_issue3289()
spacy.tests.regression.test_issue3001-3500.test_issue3328(en_vocab)
spacy.tests.regression.test_issue3001-3500.test_issue3331(en_vocab)
spacy.tests.regression.test_issue3001-3500.test_issue3345()
spacy.tests.regression.test_issue3001-3500.test_issue3410()
spacy.tests.regression.test_issue3001-3500.test_issue3447()
spacy.tests.regression.test_issue3001-3500.test_issue3449()
spacy.tests.regression.test_issue3001-3500.test_issue3456()
spacy.tests.regression.test_issue3001-3500.test_issue3468()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue2501-3000.py----------------------------------------
A:spacy.tests.regression.test_issue2501-3000.nlp->Japanese()
A:spacy.tests.regression.test_issue2501-3000.tagger->Japanese().create_pipe('tagger')
A:spacy.tests.regression.test_issue2501-3000.doc->fr_tokenizer('Learn html5/css3/javascript/jquery')
A:spacy.tests.regression.test_issue2501-3000.docs->Japanese().pipe(['hello', 'world'])
A:spacy.tests.regression.test_issue2501-3000.piped_doc->next(docs)
A:spacy.tests.regression.test_issue2501-3000.matcher->Matcher(nlp.vocab)
A:spacy.tests.regression.test_issue2501-3000.matched->sorted(matched, key=len, reverse=True)
A:spacy.tests.regression.test_issue2501-3000.doc1->nlp('This is a high-adrenaline situation.')
A:spacy.tests.regression.test_issue2501-3000.doc2->nlp('This is a high adrenaline situation.')
A:spacy.tests.regression.test_issue2501-3000.matches1->matcher(doc1)
A:spacy.tests.regression.test_issue2501-3000.matches2->matcher(doc2)
A:spacy.tests.regression.test_issue2501-3000.html->spacy.displacy.render(doc, style='ent')
A:spacy.tests.regression.test_issue2501-3000.a->en_tokenizer('a')
A:spacy.tests.regression.test_issue2501-3000.am->en_tokenizer('am')
A:spacy.tests.regression.test_issue2501-3000.words->'When we write or communicate virtually , we can hide our true feelings .'.split()
A:spacy.tests.regression.test_issue2501-3000.ner->Japanese().create_pipe('ner')
A:spacy.tests.regression.test_issue2501-3000.optimizer->Japanese().begin_training()
A:spacy.tests.regression.test_issue2501-3000.vocab->Vocab(vectors_name='test_issue2871')
A:spacy.tests.regression.test_issue2501-3000.vector_data->numpy.zeros((3, 10), dtype='f')
spacy.tests.regression.test_issue2501-3000.test_issue2564()
spacy.tests.regression.test_issue2501-3000.test_issue2569(en_tokenizer)
spacy.tests.regression.test_issue2501-3000.test_issue2626_2835(en_tokenizer,text)
spacy.tests.regression.test_issue2501-3000.test_issue2656(en_tokenizer)
spacy.tests.regression.test_issue2501-3000.test_issue2671()
spacy.tests.regression.test_issue2501-3000.test_issue2728(en_vocab)
spacy.tests.regression.test_issue2501-3000.test_issue2754(en_tokenizer)
spacy.tests.regression.test_issue2501-3000.test_issue2772(en_vocab)
spacy.tests.regression.test_issue2501-3000.test_issue2782(text,lang_cls)
spacy.tests.regression.test_issue2501-3000.test_issue2800()
spacy.tests.regression.test_issue2501-3000.test_issue2822(it_tokenizer)
spacy.tests.regression.test_issue2501-3000.test_issue2833(en_vocab)
spacy.tests.regression.test_issue2501-3000.test_issue2871()
spacy.tests.regression.test_issue2501-3000.test_issue2901()
spacy.tests.regression.test_issue2501-3000.test_issue2926(fr_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue3526.py----------------------------------------
A:spacy.tests.regression.test_issue3526.nlp->Language(vocab=en_vocab)
A:spacy.tests.regression.test_issue3526.ruler->Language(vocab=en_vocab).get_pipe('entity_ruler')
A:spacy.tests.regression.test_issue3526.ruler_bytes->Language(vocab=en_vocab).get_pipe('entity_ruler').to_bytes()
A:spacy.tests.regression.test_issue3526.new_ruler->load(tmpdir).get_pipe('entity_ruler')
A:spacy.tests.regression.test_issue3526.bytes_old_style->srsly.msgpack_dumps(ruler.patterns)
A:spacy.tests.regression.test_issue3526.nlp2->load(tmpdir)
spacy.tests.regression.test_issue3526.add_ent()
spacy.tests.regression.test_issue3526.patterns()
spacy.tests.regression.test_issue3526.test_entity_ruler_existing_bytes_old_format_safe(patterns,en_vocab)
spacy.tests.regression.test_issue3526.test_entity_ruler_existing_overwrite_serialize_bytes(patterns,en_vocab)
spacy.tests.regression.test_issue3526.test_entity_ruler_from_disk_old_format_safe(patterns,en_vocab)
spacy.tests.regression.test_issue3526.test_entity_ruler_in_pipeline_from_issue(patterns,en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue3803.py----------------------------------------
A:spacy.tests.regression.test_issue3803.nlp->Spanish()
A:spacy.tests.regression.test_issue3803.doc->nlp(text)
spacy.tests.regression.test_issue3803.test_issue3803()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue1501-2000.py----------------------------------------
A:spacy.tests.regression.test_issue1501-2000.nlp->Language()
A:spacy.tests.regression.test_issue1501-2000.vectors->Vectors(data=data, keys=['I', 'am', 'Matt'])
A:spacy.tests.regression.test_issue1501-2000.doc->Doc(en_vocab, words=['this', 'is', 'text'])
A:spacy.tests.regression.test_issue1501-2000.sents->list(doc.sents)
A:spacy.tests.regression.test_issue1501-2000.sent0->sents[0].as_doc()
A:spacy.tests.regression.test_issue1501-2000.sent1->sents[1].as_doc()
A:spacy.tests.regression.test_issue1501-2000.v->Vectors(shape=(10, 10), keys=[5, 3, 98, 100])
A:spacy.tests.regression.test_issue1501-2000.nlp2->Language(Vocab())
A:spacy.tests.regression.test_issue1501-2000.data->numpy.ones((3, 300), dtype='f')
A:spacy.tests.regression.test_issue1501-2000.tagger->Tagger(Vocab()).from_disk(path)
A:spacy.tests.regression.test_issue1501-2000.tokens->en_tokenizer("would've")
A:spacy.tests.regression.test_issue1501-2000.heads_deps->numpy.asarray([[1, 397], [4, 436], [2, 426], [1, 402], [0, 8206900633647566924], [18446744073709551615, 440], [18446744073709551614, 442]], dtype='uint64')
A:spacy.tests.regression.test_issue1501-2000.vocab->Vocab()
A:spacy.tests.regression.test_issue1501-2000.new_doc->Doc(new_matcher.vocab, words=['hello'])
A:spacy.tests.regression.test_issue1501-2000.int_id->Vocab().strings.add('some string')
A:spacy.tests.regression.test_issue1501-2000.matcher->Matcher(en_vocab)
A:spacy.tests.regression.test_issue1501-2000.new_matcher->copy.deepcopy(matcher)
A:spacy.tests.regression.test_issue1501-2000.matches->matcher(doc)
A:spacy.tests.regression.test_issue1501-2000.doc.tensor->numpy.ones((len(doc), 128), dtype='f')
A:spacy.tests.regression.test_issue1501-2000.ner->EntityRecognizer(Vocab())
spacy.tests.regression.test_issue1501-2000.test_issue1506()
spacy.tests.regression.test_issue1501-2000.test_issue1518()
spacy.tests.regression.test_issue1501-2000.test_issue1537()
spacy.tests.regression.test_issue1501-2000.test_issue1539()
spacy.tests.regression.test_issue1501-2000.test_issue1547()
spacy.tests.regression.test_issue1501-2000.test_issue1612(en_tokenizer)
spacy.tests.regression.test_issue1501-2000.test_issue1654()
spacy.tests.regression.test_issue1501-2000.test_issue1698(en_tokenizer,text)
spacy.tests.regression.test_issue1501-2000.test_issue1727()
spacy.tests.regression.test_issue1501-2000.test_issue1757()
spacy.tests.regression.test_issue1501-2000.test_issue1758(en_tokenizer)
spacy.tests.regression.test_issue1501-2000.test_issue1773(en_tokenizer)
spacy.tests.regression.test_issue1501-2000.test_issue1799()
spacy.tests.regression.test_issue1501-2000.test_issue1807()
spacy.tests.regression.test_issue1501-2000.test_issue1834()
spacy.tests.regression.test_issue1501-2000.test_issue1868()
spacy.tests.regression.test_issue1501-2000.test_issue1883()
spacy.tests.regression.test_issue1501-2000.test_issue1889(word)
spacy.tests.regression.test_issue1501-2000.test_issue1915()
spacy.tests.regression.test_issue1501-2000.test_issue1945()
spacy.tests.regression.test_issue1501-2000.test_issue1963(en_tokenizer)
spacy.tests.regression.test_issue1501-2000.test_issue1967(label)
spacy.tests.regression.test_issue1501-2000.test_issue1971(en_vocab)
spacy.tests.regression.test_issue1501-2000.test_issue_1971_2(en_vocab)
spacy.tests.regression.test_issue1501-2000.test_issue_1971_3(en_vocab)
spacy.tests.regression.test_issue1501-2000.test_issue_1971_4(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue3555.py----------------------------------------
A:spacy.tests.regression.test_issue3555.matcher->Matcher(en_vocab)
A:spacy.tests.regression.test_issue3555.doc->Doc(en_vocab, words=['have', 'apple'])
spacy.tests.regression.test_issue3555.test_issue3555(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue4030.py----------------------------------------
A:spacy.tests.regression.test_issue4030.pos_cats->list()
A:spacy.tests.regression.test_issue4030.train_data->list(zip(x_train, [{'cats': cats} for cats in pos_cats]))
A:spacy.tests.regression.test_issue4030.nlp->spacy.blank('en')
A:spacy.tests.regression.test_issue4030.textcat->spacy.blank('en').create_pipe('textcat', config={'exclusive_classes': True, 'architecture': 'bow', 'ngram_size': 2})
A:spacy.tests.regression.test_issue4030.optimizer->spacy.blank('en').begin_training()
A:spacy.tests.regression.test_issue4030.batches->minibatch(train_data, size=compounding(4.0, 32.0, 1.001))
A:spacy.tests.regression.test_issue4030.(texts, annotations)->zip(*batch)
A:spacy.tests.regression.test_issue4030.doc->nlp('')
spacy.tests.regression.test_issue4030.test_issue4030()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue4313.py----------------------------------------
A:spacy.tests.regression.test_issue4313.nlp->English()
A:spacy.tests.regression.test_issue4313.ner->EntityRecognizer(nlp.vocab)
A:spacy.tests.regression.test_issue4313.doc->nlp('What do you think about Apple ?')
A:spacy.tests.regression.test_issue4313.apple_ent->Span(doc, 5, 6, label='MY_ORG')
A:spacy.tests.regression.test_issue4313.beams->English().entity.beam_parse(docs, beam_width=beam_width, beam_density=beam_density)
A:spacy.tests.regression.test_issue4313.entity_scores->defaultdict(float)
spacy.tests.regression.test_issue4313.test_issue4313()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue3882.py----------------------------------------
A:spacy.tests.regression.test_issue3882.doc->Doc(en_vocab, words=['Hello', 'world'])
A:spacy.tests.regression.test_issue3882.doc.user_data['test']->set()
spacy.tests.regression.test_issue3882.test_issue3882(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue3879.py----------------------------------------
A:spacy.tests.regression.test_issue3879.doc->Doc(en_vocab, words=['This', 'is', 'a', 'test', '.'])
A:spacy.tests.regression.test_issue3879.matcher->Matcher(en_vocab)
spacy.tests.regression.test_issue3879.test_issue3879(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue3830.py----------------------------------------
A:spacy.tests.regression.test_issue3830.parser->DependencyParser(Vocab(), learn_tokens=True)
spacy.tests.regression.test_issue3830.test_issue3830_no_subtok()
spacy.tests.regression.test_issue3830.test_issue3830_with_subtok()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue4272.py----------------------------------------
A:spacy.tests.regression.test_issue4272.nlp->Greek()
A:spacy.tests.regression.test_issue4272.doc->nlp('Χθες')
spacy.tests.regression.test_issue4272.test_issue4272()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue4348.py----------------------------------------
A:spacy.tests.regression.test_issue4348.nlp->English()
A:spacy.tests.regression.test_issue4348.tagger->English().create_pipe('tagger')
A:spacy.tests.regression.test_issue4348.optimizer->English().begin_training()
A:spacy.tests.regression.test_issue4348.batches->minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))
A:spacy.tests.regression.test_issue4348.(texts, annotations)->zip(*batch)
spacy.tests.regression.test_issue4348.test_issue4348()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue3531.py----------------------------------------
A:spacy.tests.regression.test_issue3531.dep_html->spacy.displacy.render(example_dep, style='dep', manual=True)
A:spacy.tests.regression.test_issue3531.ent_html->spacy.displacy.render(example_ent, style='ent', manual=True)
spacy.tests.regression.test_issue3531.test_issue3531()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue4120.py----------------------------------------
A:spacy.tests.regression.test_issue4120.matcher->Matcher(en_vocab)
A:spacy.tests.regression.test_issue4120.doc1->Doc(en_vocab, words=['a'])
A:spacy.tests.regression.test_issue4120.doc2->Doc(en_vocab, words=['a', 'b', 'c'])
A:spacy.tests.regression.test_issue4120.doc3->Doc(en_vocab, words=['a', 'b', 'b', 'c'])
A:spacy.tests.regression.test_issue4120.doc4->Doc(en_vocab, words=['a', 'b', 'b', 'c'])
spacy.tests.regression.test_issue4120.test_issue4120(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue3959.py----------------------------------------
A:spacy.tests.regression.test_issue3959.nlp->English()
A:spacy.tests.regression.test_issue3959.doc->nlp('displaCy uses JavaScript, SVG and CSS to show you how computers understand language')
A:spacy.tests.regression.test_issue3959.doc2->nlp('')
spacy.tests.regression.test_issue3959.test_issue3959()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue3549.py----------------------------------------
A:spacy.tests.regression.test_issue3549.matcher->Matcher(en_vocab, validate=True)
spacy.tests.regression.test_issue3549.test_issue3549(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue3880.py----------------------------------------
A:spacy.tests.regression.test_issue3880.nlp->English()
spacy.tests.regression.test_issue3880.test_issue3880()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue3869.py----------------------------------------
A:spacy.tests.regression.test_issue3869.nlp->English()
A:spacy.tests.regression.test_issue3869.doc->nlp(sentence)
spacy.tests.regression.test_issue3869.test_issue3869(sentence)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue3521.py----------------------------------------
spacy.tests.regression.test_issue3521.test_issue3521(en_tokenizer,word)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue3951.py----------------------------------------
A:spacy.tests.regression.test_issue3951.matcher->Matcher(en_vocab)
A:spacy.tests.regression.test_issue3951.doc->Doc(en_vocab, words=['Hello', 'my', 'new', 'world'])
A:spacy.tests.regression.test_issue3951.matches->matcher(doc)
spacy.tests.regression.test_issue3951.test_issue3951(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue2001-2500.py----------------------------------------
A:spacy.tests.regression.test_issue2001-2500.nlp->Italian()
A:spacy.tests.regression.test_issue2001-2500.doc->Doc(en_vocab, words=['a', 'b'])
A:spacy.tests.regression.test_issue2001-2500.ner->Italian().create_pipe('ner')
A:spacy.tests.regression.test_issue2001-2500.nlp2->Italian()
A:spacy.tests.regression.test_issue2001-2500.doc_array->Doc(en_vocab, words=['a', 'b']).to_array(['TAG', 'LEMMA'])
A:spacy.tests.regression.test_issue2001-2500.new_doc->Doc(doc.vocab, words=words).from_array(['TAG', 'LEMMA'], doc_array)
A:spacy.tests.regression.test_issue2001-2500.html->render(doc)
A:spacy.tests.regression.test_issue2001-2500.matrix->numpy.array([[0, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 2, 3, 3, 3], [1, 1, 3, 3, 3, 3], [1, 1, 3, 3, 4, 4], [1, 1, 3, 3, 4, 5]], dtype=numpy.int32)
A:spacy.tests.regression.test_issue2001-2500.matcher->Matcher(en_vocab)
A:spacy.tests.regression.test_issue2001-2500.matches->matcher(doc)
A:spacy.tests.regression.test_issue2001-2500.b->Italian().to_bytes()
spacy.tests.regression.test_issue2001-2500.test_issue2070()
spacy.tests.regression.test_issue2001-2500.test_issue2179()
spacy.tests.regression.test_issue2001-2500.test_issue2203(en_vocab)
spacy.tests.regression.test_issue2001-2500.test_issue2219(en_vocab)
spacy.tests.regression.test_issue2001-2500.test_issue2361(de_tokenizer)
spacy.tests.regression.test_issue2001-2500.test_issue2385()
spacy.tests.regression.test_issue2001-2500.test_issue2385_biluo(tags)
spacy.tests.regression.test_issue2001-2500.test_issue2396(en_vocab)
spacy.tests.regression.test_issue2001-2500.test_issue2464(en_vocab)
spacy.tests.regression.test_issue2001-2500.test_issue2482()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue4133.py----------------------------------------
A:spacy.tests.regression.test_issue4133.nlp->English()
A:spacy.tests.regression.test_issue4133.vocab_bytes->English().vocab.to_bytes()
A:spacy.tests.regression.test_issue4133.doc->Doc(vocab).from_bytes(doc_bytes)
A:spacy.tests.regression.test_issue4133.doc_bytes->Doc(vocab).from_bytes(doc_bytes).to_bytes()
A:spacy.tests.regression.test_issue4133.vocab->vocab.from_bytes(vocab_bytes).from_bytes(vocab_bytes)
spacy.tests.regression.test_issue4133.test_issue4133(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue3839.py----------------------------------------
A:spacy.tests.regression.test_issue3839.doc->Doc(en_vocab, words=['terrific', 'group', 'of', 'people'])
A:spacy.tests.regression.test_issue3839.matcher->Matcher(en_vocab)
A:spacy.tests.regression.test_issue3839.matches->matcher(doc)
spacy.tests.regression.test_issue3839.test_issue3839(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue4267.py----------------------------------------
A:spacy.tests.regression.test_issue4267.nlp->English()
A:spacy.tests.regression.test_issue4267.ner->English().create_pipe('ner')
A:spacy.tests.regression.test_issue4267.doc1->nlp('hi')
A:spacy.tests.regression.test_issue4267.ruler->EntityRuler(nlp)
A:spacy.tests.regression.test_issue4267.doc2->nlp('hi')
spacy.tests.regression.test_issue4267.test_issue4267()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue3540.py----------------------------------------
A:spacy.tests.regression.test_issue3540.tensor->numpy.asarray([[1.0, 1.1], [2.0, 2.1], [3.0, 3.1], [4.0, 4.1], [5.0, 5.1], [6.0, 6.1]], dtype='f')
A:spacy.tests.regression.test_issue3540.doc->Doc(en_vocab, words=words)
spacy.tests.regression.test_issue3540.test_issue3540(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue4002.py----------------------------------------
A:spacy.tests.regression.test_issue4002.matcher->PhraseMatcher(en_vocab, attr='NORM')
A:spacy.tests.regression.test_issue4002.pattern1->Doc(en_vocab, words=['c', 'd'])
A:spacy.tests.regression.test_issue4002.doc->Doc(en_vocab, words=['a', 'b', 'c', 'd'])
A:spacy.tests.regression.test_issue4002.matches->matcher(doc)
A:spacy.tests.regression.test_issue4002.pattern2->Doc(en_vocab, words=['1', '2'])
spacy.tests.regression.test_issue4002.test_issue4002(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue4054.py----------------------------------------
A:spacy.tests.regression.test_issue4054.nlp1->English()
A:spacy.tests.regression.test_issue4054.vocab_dir->ensure_path(d / 'vocab')
A:spacy.tests.regression.test_issue4054.vocab2->Vocab().from_disk(vocab_dir)
A:spacy.tests.regression.test_issue4054.nlp2->spacy.blank('en', vocab=vocab2)
A:spacy.tests.regression.test_issue4054.nlp_dir->ensure_path(d / 'nlp')
A:spacy.tests.regression.test_issue4054.nlp3->spacy.load(nlp_dir)
spacy.tests.regression.test_issue4054.test_issue4054(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue1-1000.py----------------------------------------
A:spacy.tests.regression.test_issue1-1000.doc->nlp2(raw_text)
A:spacy.tests.regression.test_issue1-1000.matcher->Matcher(vocab)
A:spacy.tests.regression.test_issue1-1000.ents->list(doc.ents)
A:spacy.tests.regression.test_issue1-1000.tokens->en_tokenizer(text)
A:spacy.tests.regression.test_issue1-1000.sents->list(doc.sents)
A:spacy.tests.regression.test_issue1-1000.matches->matcher(doc)
A:spacy.tests.regression.test_issue1-1000.vocab->Vocab(lex_attr_getters={LOWER: lambda string: string.lower()})
A:spacy.tests.regression.test_issue1-1000.lookups->Lookups()
A:spacy.tests.regression.test_issue1-1000.lemmatizer->Lemmatizer(lookups)
A:spacy.tests.regression.test_issue1-1000.doc2->Doc(doc.vocab)
A:spacy.tests.regression.test_issue1-1000.entities->list(doc.ents)
A:spacy.tests.regression.test_issue1-1000.s->set([token])
A:spacy.tests.regression.test_issue1-1000.items->list(s)
A:spacy.tests.regression.test_issue1-1000.match->matcher(doc)
A:spacy.tests.regression.test_issue1-1000.nlp->Language()
A:spacy.tests.regression.test_issue1-1000.ner->Language().create_pipe('ner')
A:spacy.tests.regression.test_issue1-1000.nlp2->Language().from_disk(model_dir)
spacy.tests.regression.test_issue1-1000.test_control_issue792(en_tokenizer,text)
spacy.tests.regression.test_issue1-1000.test_issue118(en_tokenizer,patterns)
spacy.tests.regression.test_issue1-1000.test_issue118_prefix_reorder(en_tokenizer,patterns)
spacy.tests.regression.test_issue1-1000.test_issue242(en_tokenizer)
spacy.tests.regression.test_issue1-1000.test_issue309(en_tokenizer)
spacy.tests.regression.test_issue1-1000.test_issue351(en_tokenizer)
spacy.tests.regression.test_issue1-1000.test_issue360(en_tokenizer)
spacy.tests.regression.test_issue1-1000.test_issue361(en_vocab,text1,text2)
spacy.tests.regression.test_issue1-1000.test_issue587(en_tokenizer)
spacy.tests.regression.test_issue1-1000.test_issue588(en_vocab)
spacy.tests.regression.test_issue1-1000.test_issue589()
spacy.tests.regression.test_issue1-1000.test_issue590(en_vocab)
spacy.tests.regression.test_issue1-1000.test_issue595()
spacy.tests.regression.test_issue1-1000.test_issue599(en_vocab)
spacy.tests.regression.test_issue1-1000.test_issue600()
spacy.tests.regression.test_issue1-1000.test_issue615(en_tokenizer)
spacy.tests.regression.test_issue1-1000.test_issue736(en_tokenizer,text,number)
spacy.tests.regression.test_issue1-1000.test_issue740(en_tokenizer,text)
spacy.tests.regression.test_issue1-1000.test_issue743()
spacy.tests.regression.test_issue1-1000.test_issue744(en_tokenizer,text)
spacy.tests.regression.test_issue1-1000.test_issue759(en_tokenizer,text,is_num)
spacy.tests.regression.test_issue1-1000.test_issue775(en_tokenizer,text)
spacy.tests.regression.test_issue1-1000.test_issue792(en_tokenizer,text)
spacy.tests.regression.test_issue1-1000.test_issue801(en_tokenizer,text,tokens)
spacy.tests.regression.test_issue1-1000.test_issue805(sv_tokenizer,text,expected_tokens)
spacy.tests.regression.test_issue1-1000.test_issue850()
spacy.tests.regression.test_issue1-1000.test_issue850_basic()
spacy.tests.regression.test_issue1-1000.test_issue852(fr_tokenizer,text)
spacy.tests.regression.test_issue1-1000.test_issue859(en_tokenizer,text)
spacy.tests.regression.test_issue1-1000.test_issue886(en_tokenizer,text)
spacy.tests.regression.test_issue1-1000.test_issue891(en_tokenizer,text)
spacy.tests.regression.test_issue1-1000.test_issue912(en_vocab,text,tag,lemma)
spacy.tests.regression.test_issue1-1000.test_issue957(en_tokenizer)
spacy.tests.regression.test_issue1-1000.test_issue999(train_data)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue4190.py----------------------------------------
A:spacy.tests.regression.test_issue4190.nlp_1->English()
A:spacy.tests.regression.test_issue4190.doc_1a->nlp_1(test_string)
A:spacy.tests.regression.test_issue4190.doc_1b->nlp_1(test_string)
A:spacy.tests.regression.test_issue4190.nlp_2->spacy.util.load_model(model_dir)
A:spacy.tests.regression.test_issue4190.doc_2->nlp_2(test_string)
A:spacy.tests.regression.test_issue4190.prefix_re->spacy.util.compile_prefix_regex(nlp.Defaults.prefixes)
A:spacy.tests.regression.test_issue4190.suffix_re->spacy.util.compile_suffix_regex(nlp.Defaults.suffixes)
A:spacy.tests.regression.test_issue4190.infix_re->spacy.util.compile_infix_regex(nlp.Defaults.infixes)
A:spacy.tests.regression.test_issue4190.new_tokenizer->Tokenizer(nlp.vocab, exceptions, prefix_search=prefix_re.search, suffix_search=suffix_re.search, infix_finditer=infix_re.finditer, token_match=nlp.tokenizer.token_match)
spacy.tests.regression.test_issue4190.customize_tokenizer(nlp)
spacy.tests.regression.test_issue4190.test_issue4190()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue3962.py----------------------------------------
A:spacy.tests.regression.test_issue3962.tokens->en_tokenizer(text)
A:spacy.tests.regression.test_issue3962.doc2->span2.as_doc()
A:spacy.tests.regression.test_issue3962.doc2_json->span2.as_doc().to_json()
A:spacy.tests.regression.test_issue3962.doc3->span3.as_doc()
A:spacy.tests.regression.test_issue3962.doc3_json->span3.as_doc().to_json()
A:spacy.tests.regression.test_issue3962.sents->list(doc2.sents)
spacy.tests.regression.test_issue3962.doc(en_tokenizer)
spacy.tests.regression.test_issue3962.test_issue3962(doc)
spacy.tests.regression.test_issue3962.test_issue3962_long(two_sent_doc)
spacy.tests.regression.test_issue3962.two_sent_doc(en_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/tokenizer/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/tokenizer/test_whitespace.py----------------------------------------
A:spacy.tests.tokenizer.test_whitespace.tokens->tokenizer(text)
spacy.tests.tokenizer.test_whitespace.test_tokenizer_handles_double_trainling_ws(tokenizer,text)
spacy.tests.tokenizer.test_whitespace.test_tokenizer_splits_double_space(tokenizer,text)
spacy.tests.tokenizer.test_whitespace.test_tokenizer_splits_newline(tokenizer,text)
spacy.tests.tokenizer.test_whitespace.test_tokenizer_splits_newline_double_space(tokenizer,text)
spacy.tests.tokenizer.test_whitespace.test_tokenizer_splits_newline_space(tokenizer,text)
spacy.tests.tokenizer.test_whitespace.test_tokenizer_splits_newline_space_wrap(tokenizer,text)
spacy.tests.tokenizer.test_whitespace.test_tokenizer_splits_single_space(tokenizer,text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/tokenizer/test_exceptions.py----------------------------------------
A:spacy.tests.tokenizer.test_exceptions.tokens->tokenizer(text)
spacy.tests.tokenizer.test_exceptions.test_tokenizer_excludes_false_pos_emoticons(tokenizer,text,length)
spacy.tests.tokenizer.test_exceptions.test_tokenizer_handles_emoji(tokenizer,text,length)
spacy.tests.tokenizer.test_exceptions.test_tokenizer_handles_emoticons(tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/tokenizer/test_naughty_strings.py----------------------------------------
A:spacy.tests.tokenizer.test_naughty_strings.tokens->tokenizer(text)
spacy.tests.tokenizer.test_naughty_strings.test_tokenizer_naughty_strings(tokenizer,text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/tokenizer/test_tokenizer.py----------------------------------------
A:spacy.tests.tokenizer.test_tokenizer.tokens->tokenizer(text)
A:spacy.tests.tokenizer.test_tokenizer.text->loc.open('r', encoding='utf8').read()
A:spacy.tests.tokenizer.test_tokenizer.tokens1->tokenizer(text1)
A:spacy.tests.tokenizer.test_tokenizer.tokens2->tokenizer(text2)
A:spacy.tests.tokenizer.test_tokenizer.doc->tokenizer(text)
A:spacy.tests.tokenizer.test_tokenizer.vocab->Vocab(tag_map={'NN': {'pos': 'NOUN'}})
A:spacy.tests.tokenizer.test_tokenizer.tokenizer->Tokenizer(vocab, {}, None, None, None)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_add_special_case(tokenizer,text,tokens)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_add_special_case_tag(text,tokens)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_colons(tokenizer,text)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_handle_text_from_file(tokenizer,file_name)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_handles_digits(tokenizer)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_handles_long_text(tokenizer)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_handles_no_word(tokenizer)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_handles_punct(tokenizer)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_handles_punct_braces(tokenizer)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_handles_single_word(tokenizer,text)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_keep_urls(tokenizer,text)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_keeps_email(tokenizer,text)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_suspected_freeing_strings(tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/tokenizer/test_urls.py----------------------------------------
A:spacy.tests.tokenizer.test_urls.tokens->tokenizer(url + suffix1 + suffix2)
spacy.tests.tokenizer.test_urls.test_should_match(en_tokenizer,url)
spacy.tests.tokenizer.test_urls.test_should_not_match(en_tokenizer,url)
spacy.tests.tokenizer.test_urls.test_tokenizer_handles_prefixed_url(tokenizer,prefix,url)
spacy.tests.tokenizer.test_urls.test_tokenizer_handles_simple_surround_url(tokenizer,url)
spacy.tests.tokenizer.test_urls.test_tokenizer_handles_simple_url(tokenizer,url)
spacy.tests.tokenizer.test_urls.test_tokenizer_handles_suffixed_url(tokenizer,url,suffix)
spacy.tests.tokenizer.test_urls.test_tokenizer_handles_surround_url(tokenizer,prefix,suffix,url)
spacy.tests.tokenizer.test_urls.test_tokenizer_handles_two_prefix_url(tokenizer,prefix1,prefix2,url)
spacy.tests.tokenizer.test_urls.test_tokenizer_handles_two_suffix_url(tokenizer,suffix1,suffix2,url)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/char_classes.py----------------------------------------
A:spacy.lang.char_classes.ALPHA->group_chars(LATIN + _russian + _tatar + _greek + _ukrainian + _uncased)
A:spacy.lang.char_classes.ALPHA_LOWER->group_chars(_lower + _uncased)
A:spacy.lang.char_classes.ALPHA_UPPER->group_chars(_upper + _uncased)
A:spacy.lang.char_classes.UNITS->merge_chars(_units)
A:spacy.lang.char_classes.CURRENCY->merge_chars(_currency)
A:spacy.lang.char_classes.PUNCT->merge_chars(_punct)
A:spacy.lang.char_classes.HYPHENS->merge_chars(_hyphens)
A:spacy.lang.char_classes.LIST_UNITS->split_chars(_units)
A:spacy.lang.char_classes.LIST_CURRENCY->split_chars(_currency)
A:spacy.lang.char_classes.LIST_QUOTES->split_chars(_quotes)
A:spacy.lang.char_classes.LIST_PUNCT->split_chars(_punct)
A:spacy.lang.char_classes.LIST_HYPHENS->split_chars(_hyphens)
A:spacy.lang.char_classes.CONCAT_QUOTES->group_chars(_quotes)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/lex_attrs.py----------------------------------------
A:spacy.lang.lex_attrs._tlds->set('com|org|edu|gov|net|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|sk|sl|sm|sn|so|sr|ss|st|su|sv|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|za|zm|zw'.split('|'))
A:spacy.lang.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.lex_attrs.cluster(string)
spacy.lang.lex_attrs.get_prob(string)
spacy.lang.lex_attrs.is_alpha(string)
spacy.lang.lex_attrs.is_ascii(text)
spacy.lang.lex_attrs.is_bracket(text)
spacy.lang.lex_attrs.is_currency(text)
spacy.lang.lex_attrs.is_digit(string)
spacy.lang.lex_attrs.is_left_punct(text)
spacy.lang.lex_attrs.is_lower(string)
spacy.lang.lex_attrs.is_oov(string)
spacy.lang.lex_attrs.is_punct(text)
spacy.lang.lex_attrs.is_quote(text)
spacy.lang.lex_attrs.is_right_punct(text)
spacy.lang.lex_attrs.is_space(string)
spacy.lang.lex_attrs.is_stop(string,stops=set())
spacy.lang.lex_attrs.is_title(string)
spacy.lang.lex_attrs.is_upper(string)
spacy.lang.lex_attrs.like_email(text)
spacy.lang.lex_attrs.like_num(text)
spacy.lang.lex_attrs.like_url(text)
spacy.lang.lex_attrs.lower(string)
spacy.lang.lex_attrs.prefix(string)
spacy.lang.lex_attrs.suffix(string)
spacy.lang.lex_attrs.word_shape(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.tokenizer_exceptions.URL_PATTERN->'^(?=[\\w])(?:(?:https?|ftp|mailto)://)?(?:\\S+(?::\\S*)?@)?(?:(?!(?:10|127)(?:\\.\\d{1,3}){3})(?!(?:169\\.254|192\\.168)(?:\\.\\d{1,3}){2})(?!172\\.(?:1[6-9]|2\\d|3[0-1])(?:\\.\\d{1,3}){2})(?:[1-9]\\d?|1\\d\\d|2[01]\\d|22[0-3])(?:\\.(?:1?\\d{1,2}|2[0-4]\\d|25[0-5])){2}(?:\\.(?:[1-9]\\d?|1\\d\\d|2[0-4]\\d|25[0-4]))|(?:(?:[a-z0-9\\-]*)?[a-z0-9]+)(?:\\.(?:[a-z0-9])(?:[a-z0-9\\-])*[a-z0-9])?(?:\\.(?:[a-z]{2,})))(?::\\d{2,5})?(?:/\\S*)?\\??(:?\\S*)?(?<=[\\w/])$'.strip()
A:spacy.lang.tokenizer_exceptions.emoticons->set('\n:)\n:-)\n:))\n:-))\n:)))\n:-)))\n(:\n(-:\n=)\n(=\n")\n:]\n:-]\n[:\n[-:\n:o)\n(o:\n:}\n:-}\n8)\n8-)\n(-8\n;)\n;-)\n(;\n(-;\n:(\n:-(\n:((\n:-((\n:(((\n:-(((\n):\n)-:\n=(\n>:(\n:\')\n:\'-)\n:\'(\n:\'-(\n:/\n:-/\n=/\n=|\n:|\n:-|\n:1\n:P\n:-P\n:p\n:-p\n:O\n:-O\n:o\n:-o\n:0\n:-0\n:()\n>:o\n:*\n:-*\n:3\n:-3\n=3\n:>\n:->\n:X\n:-X\n:x\n:-x\n:D\n:-D\n;D\n;-D\n=D\nxD\nXD\nxDD\nXDD\n8D\n8-D\n\n^_^\n^__^\n^___^\n>.<\n>.>\n<.<\n._.\n;_;\n-_-\n-__-\nv.v\nV.V\nv_v\nV_V\no_o\no_O\nO_o\nO_O\n0_o\no_0\n0_0\no.O\nO.o\nO.O\no.o\n0.0\no.0\n0.o\n@_@\n<3\n<33\n<333\n</3\n(^_^)\n(-_-)\n(._.)\n(>_<)\n(*_*)\n(¬_¬)\nಠ_ಠ\nಠ︵ಠ\n(ಠ_ಠ)\n¯\\(ツ)/¯\n(╯°□°）╯︵┻━┻\n><(((*>\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/norm_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/tag_map.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/punctuation.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/vi/__init__.py----------------------------------------
A:spacy.lang.vi.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.vi.__init__.lex_attr_getters[NORM]->add_lookups(Language.Defaults.lex_attr_getters[NORM], BASE_NORMS)
A:spacy.lang.vi.__init__.(words, spaces)->pyvi.ViTokenizer.spacy_tokenize(text)
A:spacy.lang.vi.__init__.spaces[-1]->bool(token.whitespace_)
spacy.lang.vi.__init__.Vietnamese(Language)
spacy.lang.vi.__init__.Vietnamese.make_doc(self,text)
spacy.lang.vi.__init__.VietnameseDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/vi/lex_attrs.py----------------------------------------
A:spacy.lang.vi.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.vi.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.vi.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/vi/stop_words.py----------------------------------------
A:spacy.lang.vi.stop_words.STOP_WORDS->set('\na_lô\na_ha\nai\nai_ai\nai_nấy\nai_đó\nalô\namen\nanh\nanh_ấy\nba\nba_bau\nba_bản\nba_cùng\nba_họ\nba_ngày\nba_ngôi\nba_tăng\nbao_giờ\nbao_lâu\nbao_nhiêu\nbao_nả\nbay_biến\nbiết\nbiết_bao\nbiết_bao_nhiêu\nbiết_chắc\nbiết_chừng_nào\nbiết_mình\nbiết_mấy\nbiết_thế\nbiết_trước\nbiết_việc\nbiết_đâu\nbiết_đâu_chừng\nbiết_đâu_đấy\nbiết_được\nbuổi\nbuổi_làm\nbuổi_mới\nbuổi_ngày\nbuổi_sớm\nbà\nbà_ấy\nbài\nbài_bác\nbài_bỏ\nbài_cái\nbác\nbán\nbán_cấp\nbán_dạ\nbán_thế\nbây_bẩy\nbây_chừ\nbây_giờ\nbây_nhiêu\nbèn\nbéng\nbên\nbên_bị\nbên_có\nbên_cạnh\nbông\nbước\nbước_khỏi\nbước_tới\nbước_đi\nbạn\nbản\nbản_bộ\nbản_riêng\nbản_thân\nbản_ý\nbất_chợt\nbất_cứ\nbất_giác\nbất_kì\nbất_kể\nbất_kỳ\nbất_luận\nbất_ngờ\nbất_nhược\nbất_quá\nbất_quá_chỉ\nbất_thình_lình\nbất_tử\nbất_đồ\nbấy\nbấy_chầy\nbấy_chừ\nbấy_giờ\nbấy_lâu\nbấy_lâu_nay\nbấy_nay\nbấy_nhiêu\nbập_bà_bập_bõm\nbập_bõm\nbắt_đầu\nbắt_đầu_từ\nbằng\nbằng_cứ\nbằng_không\nbằng_người\nbằng_nhau\nbằng_như\nbằng_nào\nbằng_nấy\nbằng_vào\nbằng_được\nbằng_ấy\nbển\nbệt\nbị\nbị_chú\nbị_vì\nbỏ\nbỏ_bà\nbỏ_cha\nbỏ_cuộc\nbỏ_không\nbỏ_lại\nbỏ_mình\nbỏ_mất\nbỏ_mẹ\nbỏ_nhỏ\nbỏ_quá\nbỏ_ra\nbỏ_riêng\nbỏ_việc\nbỏ_xa\nbỗng\nbỗng_chốc\nbỗng_dưng\nbỗng_không\nbỗng_nhiên\nbỗng_nhưng\nbỗng_thấy\nbỗng_đâu\nbộ\nbộ_thuộc\nbộ_điều\nbội_phần\nbớ\nbởi\nbởi_ai\nbởi_chưng\nbởi_nhưng\nbởi_sao\nbởi_thế\nbởi_thế_cho_nên\nbởi_tại\nbởi_vì\nbởi_vậy\nbởi_đâu\nbức\ncao\ncao_lâu\ncao_ráo\ncao_răng\ncao_sang\ncao_số\ncao_thấp\ncao_thế\ncao_xa\ncha\ncha_chả\nchao_ôi\nchia_sẻ\nchiếc\ncho\ncho_biết\ncho_chắc\ncho_hay\ncho_nhau\ncho_nên\ncho_rằng\ncho_rồi\ncho_thấy\ncho_tin\ncho_tới\ncho_tới_khi\ncho_về\ncho_ăn\ncho_đang\ncho_được\ncho_đến\ncho_đến_khi\ncho_đến_nỗi\nchoa\nchu_cha\nchui_cha\nchung\nchung_cho\nchung_chung\nchung_cuộc\nchung_cục\nchung_nhau\nchung_qui\nchung_quy\nchung_quy_lại\nchung_ái\nchuyển\nchuyển_tự\nchuyển_đạt\nchuyện\nchuẩn_bị\nchành_chạnh\nchí_chết\nchính\nchính_bản\nchính_giữa\nchính_là\nchính_thị\nchính_điểm\nchùn_chùn\nchùn_chũn\nchú\nchú_dẫn\nchú_khách\nchú_mày\nchú_mình\nchúng\nchúng_mình\nchúng_ta\nchúng_tôi\nchúng_ông\nchăn_chắn\nchăng\nchăng_chắc\nchăng_nữa\nchơi\nchơi_họ\nchưa\nchưa_bao_giờ\nchưa_chắc\nchưa_có\nchưa_cần\nchưa_dùng\nchưa_dễ\nchưa_kể\nchưa_tính\nchưa_từng\nchầm_chập\nchậc\nchắc\nchắc_chắn\nchắc_dạ\nchắc_hẳn\nchắc_lòng\nchắc_người\nchắc_vào\nchắc_ăn\nchẳng_lẽ\nchẳng_những\nchẳng_nữa\nchẳng_phải\nchết_nỗi\nchết_thật\nchết_tiệt\nchỉ\nchỉ_chính\nchỉ_có\nchỉ_là\nchỉ_tên\nchỉn\nchị\nchị_bộ\nchị_ấy\nchịu\nchịu_chưa\nchịu_lời\nchịu_tốt\nchịu_ăn\nchọn\nchọn_bên\nchọn_ra\nchốc_chốc\nchớ\nchớ_chi\nchớ_gì\nchớ_không\nchớ_kể\nchớ_như\nchợt\nchợt_nghe\nchợt_nhìn\nchủn\nchứ\nchứ_ai\nchứ_còn\nchứ_gì\nchứ_không\nchứ_không_phải\nchứ_lại\nchứ_lị\nchứ_như\nchứ_sao\ncoi_bộ\ncoi_mòi\ncon\ncon_con\ncon_dạ\ncon_nhà\ncon_tính\ncu_cậu\ncuối\ncuối_cùng\ncuối_điểm\ncuốn\ncuộc\ncàng\ncàng_càng\ncàng_hay\ncá_nhân\ncác\ncác_cậu\ncách\ncách_bức\ncách_không\ncách_nhau\ncách_đều\ncái\ncái_gì\ncái_họ\ncái_đã\ncái_đó\ncái_ấy\ncâu_hỏi\ncây\ncây_nước\ncòn\ncòn_như\ncòn_nữa\ncòn_thời_gian\ncòn_về\ncó\ncó_ai\ncó_chuyện\ncó_chăng\ncó_chăng_là\ncó_chứ\ncó_cơ\ncó_dễ\ncó_họ\ncó_khi\ncó_ngày\ncó_người\ncó_nhiều\ncó_nhà\ncó_phải\ncó_số\ncó_tháng\ncó_thế\ncó_thể\ncó_vẻ\ncó_ý\ncó_ăn\ncó_điều\ncó_điều_kiện\ncó_đáng\ncó_đâu\ncó_được\ncóc_khô\ncô\ncô_mình\ncô_quả\ncô_tăng\ncô_ấy\ncông_nhiên\ncùng\ncùng_chung\ncùng_cực\ncùng_nhau\ncùng_tuổi\ncùng_tột\ncùng_với\ncùng_ăn\ncăn\ncăn_cái\ncăn_cắt\ncăn_tính\ncũng\ncũng_như\ncũng_nên\ncũng_thế\ncũng_vậy\ncũng_vậy_thôi\ncũng_được\ncơ\ncơ_chỉ\ncơ_chừng\ncơ_cùng\ncơ_dẫn\ncơ_hồ\ncơ_hội\ncơ_mà\ncơn\ncả\ncả_nghe\ncả_nghĩ\ncả_ngày\ncả_người\ncả_nhà\ncả_năm\ncả_thảy\ncả_thể\ncả_tin\ncả_ăn\ncả_đến\ncảm_thấy\ncảm_ơn\ncấp\ncấp_số\ncấp_trực_tiếp\ncần\ncần_cấp\ncần_gì\ncần_số\ncật_lực\ncật_sức\ncậu\ncổ_lai\ncụ_thể\ncụ_thể_là\ncụ_thể_như\ncủa\ncủa_ngọt\ncủa_tin\ncứ\ncứ_như\ncứ_việc\ncứ_điểm\ncực_lực\ndo\ndo_vì\ndo_vậy\ndo_đó\nduy\nduy_chỉ\nduy_có\ndài\ndài_lời\ndài_ra\ndành\ndành_dành\ndào\ndì\ndù\ndù_cho\ndù_dì\ndù_gì\ndù_rằng\ndù_sao\ndùng\ndùng_cho\ndùng_hết\ndùng_làm\ndùng_đến\ndưới\ndưới_nước\ndạ\ndạ_bán\ndạ_con\ndạ_dài\ndạ_dạ\ndạ_khách\ndần_dà\ndần_dần\ndầu_sao\ndẫn\ndẫu\ndẫu_mà\ndẫu_rằng\ndẫu_sao\ndễ\ndễ_dùng\ndễ_gì\ndễ_khiến\ndễ_nghe\ndễ_ngươi\ndễ_như_chơi\ndễ_sợ\ndễ_sử_dụng\ndễ_thường\ndễ_thấy\ndễ_ăn\ndễ_đâu\ndở_chừng\ndữ\ndữ_cách\nem\nem_em\ngiá_trị\ngiá_trị_thực_tế\ngiảm\ngiảm_chính\ngiảm_thấp\ngiảm_thế\ngiống\ngiống_người\ngiống_nhau\ngiống_như\ngiờ\ngiờ_lâu\ngiờ_này\ngiờ_đi\ngiờ_đây\ngiờ_đến\ngiữ\ngiữ_lấy\ngiữ_ý\ngiữa\ngiữa_lúc\ngây\ngây_cho\ngây_giống\ngây_ra\ngây_thêm\ngì\ngì_gì\ngì_đó\ngần\ngần_bên\ngần_hết\ngần_ngày\ngần_như\ngần_xa\ngần_đây\ngần_đến\ngặp\ngặp_khó_khăn\ngặp_phải\ngồm\nhay\nhay_biết\nhay_hay\nhay_không\nhay_là\nhay_làm\nhay_nhỉ\nhay_nói\nhay_sao\nhay_tin\nhay_đâu\nhiểu\nhiện_nay\nhiện_tại\nhoàn_toàn\nhoặc\nhoặc_là\nhãy\nhãy_còn\nhơn\nhơn_cả\nhơn_hết\nhơn_là\nhơn_nữa\nhơn_trước\nhầu_hết\nhết\nhết_chuyện\nhết_cả\nhết_của\nhết_nói\nhết_ráo\nhết_rồi\nhết_ý\nhọ\nhọ_gần\nhọ_xa\nhỏi\nhỏi_lại\nhỏi_xem\nhỏi_xin\nhỗ_trợ\nkhi\nkhi_khác\nkhi_không\nkhi_nào\nkhi_nên\nkhi_trước\nkhiến\nkhoảng\nkhoảng_cách\nkhoảng_không\nkhá\nkhá_tốt\nkhác\nkhác_gì\nkhác_khác\nkhác_nhau\nkhác_nào\nkhác_thường\nkhác_xa\nkhách\nkhó\nkhó_biết\nkhó_chơi\nkhó_khăn\nkhó_làm\nkhó_mở\nkhó_nghe\nkhó_nghĩ\nkhó_nói\nkhó_thấy\nkhó_tránh\nkhông\nkhông_ai\nkhông_bao_giờ\nkhông_bao_lâu\nkhông_biết\nkhông_bán\nkhông_chỉ\nkhông_còn\nkhông_có\nkhông_có_gì\nkhông_cùng\nkhông_cần\nkhông_cứ\nkhông_dùng\nkhông_gì\nkhông_hay\nkhông_khỏi\nkhông_kể\nkhông_ngoài\nkhông_nhận\nkhông_những\nkhông_phải\nkhông_phải_không\nkhông_thể\nkhông_tính\nkhông_điều_kiện\nkhông_được\nkhông_đầy\nkhông_để\nkhẳng_định\nkhỏi\nkhỏi_nói\nkể\nkể_cả\nkể_như\nkể_tới\nkể_từ\nliên_quan\nloại\nloại_từ\nluôn\nluôn_cả\nluôn_luôn\nluôn_tay\nlà\nlà_cùng\nlà_là\nlà_nhiều\nlà_phải\nlà_thế_nào\nlà_vì\nlà_ít\nlàm\nlàm_bằng\nlàm_cho\nlàm_dần_dần\nlàm_gì\nlàm_lòng\nlàm_lại\nlàm_lấy\nlàm_mất\nlàm_ngay\nlàm_như\nlàm_nên\nlàm_ra\nlàm_riêng\nlàm_sao\nlàm_theo\nlàm_thế_nào\nlàm_tin\nlàm_tôi\nlàm_tăng\nlàm_tại\nlàm_tắp_lự\nlàm_vì\nlàm_đúng\nlàm_được\nlâu\nlâu_các\nlâu_lâu\nlâu_nay\nlâu_ngày\nlên\nlên_cao\nlên_cơn\nlên_mạnh\nlên_ngôi\nlên_nước\nlên_số\nlên_xuống\nlên_đến\nlòng\nlòng_không\nlúc\nlúc_khác\nlúc_lâu\nlúc_nào\nlúc_này\nlúc_sáng\nlúc_trước\nlúc_đi\nlúc_đó\nlúc_đến\nlúc_ấy\nlý_do\nlượng\nlượng_cả\nlượng_số\nlượng_từ\nlại\nlại_bộ\nlại_cái\nlại_còn\nlại_giống\nlại_làm\nlại_người\nlại_nói\nlại_nữa\nlại_quả\nlại_thôi\nlại_ăn\nlại_đây\nlấy\nlấy_có\nlấy_cả\nlấy_giống\nlấy_làm\nlấy_lý_do\nlấy_lại\nlấy_ra\nlấy_ráo\nlấy_sau\nlấy_số\nlấy_thêm\nlấy_thế\nlấy_vào\nlấy_xuống\nlấy_được\nlấy_để\nlần\nlần_khác\nlần_lần\nlần_nào\nlần_này\nlần_sang\nlần_sau\nlần_theo\nlần_trước\nlần_tìm\nlớn\nlớn_lên\nlớn_nhỏ\nlời\nlời_chú\nlời_nói\nmang\nmang_lại\nmang_mang\nmang_nặng\nmang_về\nmuốn\nmà\nmà_cả\nmà_không\nmà_lại\nmà_thôi\nmà_vẫn\nmình\nmạnh\nmất\nmất_còn\nmọi\nmọi_giờ\nmọi_khi\nmọi_lúc\nmọi_người\nmọi_nơi\nmọi_sự\nmọi_thứ\nmọi_việc\nmối\nmỗi\nmỗi_lúc\nmỗi_lần\nmỗi_một\nmỗi_ngày\nmỗi_người\nmột\nmột_cách\nmột_cơn\nmột_khi\nmột_lúc\nmột_số\nmột_vài\nmột_ít\nmới\nmới_hay\nmới_rồi\nmới_đây\nmở\nmở_mang\nmở_nước\nmở_ra\nmợ\nmức\nnay\nngay\nngay_bây_giờ\nngay_cả\nngay_khi\nngay_khi_đến\nngay_lúc\nngay_lúc_này\nngay_lập_tức\nngay_thật\nngay_tức_khắc\nngay_tức_thì\nngay_từ\nnghe\nnghe_chừng\nnghe_hiểu\nnghe_không\nnghe_lại\nnghe_nhìn\nnghe_như\nnghe_nói\nnghe_ra\nnghe_rõ\nnghe_thấy\nnghe_tin\nnghe_trực_tiếp\nnghe_đâu\nnghe_đâu_như\nnghe_được\nnghen\nnghiễm_nhiên\nnghĩ\nnghĩ_lại\nnghĩ_ra\nnghĩ_tới\nnghĩ_xa\nnghĩ_đến\nnghỉm\nngoài\nngoài_này\nngoài_ra\nngoài_xa\nngoải\nnguồn\nngày\nngày_càng\nngày_cấp\nngày_giờ\nngày_ngày\nngày_nào\nngày_này\nngày_nọ\nngày_qua\nngày_rày\nngày_tháng\nngày_xưa\nngày_xửa\nngày_đến\nngày_ấy\nngôi\nngôi_nhà\nngôi_thứ\nngõ_hầu\nngăn_ngắt\nngươi\nngười\nngười_hỏi\nngười_khác\nngười_khách\nngười_mình\nngười_nghe\nngười_người\nngười_nhận\nngọn\nngọn_nguồn\nngọt\nngồi\nngồi_bệt\nngồi_không\nngồi_sau\nngồi_trệt\nngộ_nhỡ\nnhanh\nnhanh_lên\nnhanh_tay\nnhau\nnhiên_hậu\nnhiều\nnhiều_ít\nnhiệt_liệt\nnhung_nhăng\nnhà\nnhà_chung\nnhà_khó\nnhà_làm\nnhà_ngoài\nnhà_ngươi\nnhà_tôi\nnhà_việc\nnhân_dịp\nnhân_tiện\nnhé\nnhìn\nnhìn_chung\nnhìn_lại\nnhìn_nhận\nnhìn_theo\nnhìn_thấy\nnhìn_xuống\nnhóm\nnhón_nhén\nnhư\nnhư_ai\nnhư_chơi\nnhư_không\nnhư_là\nnhư_nhau\nnhư_quả\nnhư_sau\nnhư_thường\nnhư_thế\nnhư_thế_nào\nnhư_thể\nnhư_trên\nnhư_trước\nnhư_tuồng\nnhư_vậy\nnhư_ý\nnhưng\nnhưng_mà\nnhược_bằng\nnhất\nnhất_loạt\nnhất_luật\nnhất_là\nnhất_mực\nnhất_nhất\nnhất_quyết\nnhất_sinh\nnhất_thiết\nnhất_thì\nnhất_tâm\nnhất_tề\nnhất_đán\nnhất_định\nnhận\nnhận_biết\nnhận_họ\nnhận_làm\nnhận_nhau\nnhận_ra\nnhận_thấy\nnhận_việc\nnhận_được\nnhằm\nnhằm_khi\nnhằm_lúc\nnhằm_vào\nnhằm_để\nnhỉ\nnhỏ\nnhỏ_người\nnhớ\nnhớ_bập_bõm\nnhớ_lại\nnhớ_lấy\nnhớ_ra\nnhờ\nnhờ_chuyển\nnhờ_có\nnhờ_nhờ\nnhờ_đó\nnhỡ_ra\nnhững\nnhững_ai\nnhững_khi\nnhững_là\nnhững_lúc\nnhững_muốn\nnhững_như\nnào\nnào_cũng\nnào_hay\nnào_là\nnào_phải\nnào_đâu\nnào_đó\nnày\nnày_nọ\nnên\nnên_chi\nnên_chăng\nnên_làm\nnên_người\nnên_tránh\nnó\nnóc\nnói\nnói_bông\nnói_chung\nnói_khó\nnói_là\nnói_lên\nnói_lại\nnói_nhỏ\nnói_phải\nnói_qua\nnói_ra\nnói_riêng\nnói_rõ\nnói_thêm\nnói_thật\nnói_toẹt\nnói_trước\nnói_tốt\nnói_với\nnói_xa\nnói_ý\nnói_đến\nnói_đủ\nnăm\nnăm_tháng\nnơi\nnơi_nơi\nnước\nnước_bài\nnước_cùng\nnước_lên\nnước_nặng\nnước_quả\nnước_xuống\nnước_ăn\nnước_đến\nnấy\nnặng\nnặng_căn\nnặng_mình\nnặng_về\nnếu\nnếu_có\nnếu_cần\nnếu_không\nnếu_mà\nnếu_như\nnếu_thế\nnếu_vậy\nnếu_được\nnền\nnọ\nnớ\nnức_nở\nnữa\nnữa_khi\nnữa_là\nnữa_rồi\noai_oái\noái\npho\nphè\nphè_phè\nphía\nphía_bên\nphía_bạn\nphía_dưới\nphía_sau\nphía_trong\nphía_trên\nphía_trước\nphóc\nphót\nphù_hợp\nphăn_phắt\nphương_chi\nphải\nphải_biết\nphải_chi\nphải_chăng\nphải_cách\nphải_cái\nphải_giờ\nphải_khi\nphải_không\nphải_lại\nphải_lời\nphải_người\nphải_như\nphải_rồi\nphải_tay\nphần\nphần_lớn\nphần_nhiều\nphần_nào\nphần_sau\nphần_việc\nphắt\nphỉ_phui\nphỏng\nphỏng_như\nphỏng_nước\nphỏng_theo\nphỏng_tính\nphốc\nphụt\nphứt\nqua\nqua_chuyện\nqua_khỏi\nqua_lại\nqua_lần\nqua_ngày\nqua_tay\nqua_thì\nqua_đi\nquan_trọng\nquan_trọng_vấn_đề\nquan_tâm\nquay\nquay_bước\nquay_lại\nquay_số\nquay_đi\nquá\nquá_bán\nquá_bộ\nquá_giờ\nquá_lời\nquá_mức\nquá_nhiều\nquá_tay\nquá_thì\nquá_tin\nquá_trình\nquá_tuổi\nquá_đáng\nquá_ư\nquả\nquả_là\nquả_thật\nquả_thế\nquả_vậy\nquận\nra\nra_bài\nra_bộ\nra_chơi\nra_gì\nra_lại\nra_lời\nra_ngôi\nra_người\nra_sao\nra_tay\nra_vào\nra_ý\nra_điều\nra_đây\nren_rén\nriu_ríu\nriêng\nriêng_từng\nriệt\nrày\nráo\nráo_cả\nráo_nước\nráo_trọi\nrén\nrén_bước\nrích\nrón_rén\nrõ\nrõ_là\nrõ_thật\nrút_cục\nrăng\nrăng_răng\nrất\nrất_lâu\nrằng\nrằng_là\nrốt_cuộc\nrốt_cục\nrồi\nrồi_nữa\nrồi_ra\nrồi_sao\nrồi_sau\nrồi_tay\nrồi_thì\nrồi_xem\nrồi_đây\nrứa\nsa_sả\nsang\nsang_năm\nsang_sáng\nsang_tay\nsao\nsao_bản\nsao_bằng\nsao_cho\nsao_vậy\nsao_đang\nsau\nsau_chót\nsau_cuối\nsau_cùng\nsau_hết\nsau_này\nsau_nữa\nsau_sau\nsau_đây\nsau_đó\nso\nso_với\nsong_le\nsuýt\nsuýt_nữa\nsáng\nsáng_ngày\nsáng_rõ\nsáng_thế\nsáng_ý\nsì\nsì_sì\nsất\nsắp\nsắp_đặt\nsẽ\nsẽ_biết\nsẽ_hay\nsố\nsố_cho_biết\nsố_cụ_thể\nsố_loại\nsố_là\nsố_người\nsố_phần\nsố_thiếu\nsốt_sột\nsớm\nsớm_ngày\nsở_dĩ\nsử_dụng\nsự\nsự_thế\nsự_việc\ntanh\ntanh_tanh\ntay\ntay_quay\ntha_hồ\ntha_hồ_chơi\ntha_hồ_ăn\nthan_ôi\nthanh\nthanh_ba\nthanh_chuyển\nthanh_không\nthanh_thanh\nthanh_tính\nthanh_điều_kiện\nthanh_điểm\nthay_đổi\nthay_đổi_tình_trạng\ntheo\ntheo_bước\ntheo_như\ntheo_tin\nthi_thoảng\nthiếu\nthiếu_gì\nthiếu_điểm\nthoạt\nthoạt_nghe\nthoạt_nhiên\nthoắt\nthuần\nthuần_ái\nthuộc\nthuộc_bài\nthuộc_cách\nthuộc_lại\nthuộc_từ\nthà\nthà_là\nthà_rằng\nthành_ra\nthành_thử\nthái_quá\ntháng\ntháng_ngày\ntháng_năm\ntháng_tháng\nthêm\nthêm_chuyện\nthêm_giờ\nthêm_vào\nthì\nthì_giờ\nthì_là\nthì_phải\nthì_ra\nthì_thôi\nthình_lình\nthích\nthích_cứ\nthích_thuộc\nthích_tự\nthích_ý\nthím\nthôi\nthôi_việc\nthúng_thắng\nthương_ôi\nthường\nthường_bị\nthường_hay\nthường_khi\nthường_số\nthường_sự\nthường_thôi\nthường_thường\nthường_tính\nthường_tại\nthường_xuất_hiện\nthường_đến\nthảo_hèn\nthảo_nào\nthấp\nthấp_cơ\nthấp_thỏm\nthấp_xuống\nthấy\nthấy_tháng\nthẩy\nthậm\nthậm_chí\nthậm_cấp\nthậm_từ\nthật\nthật_chắc\nthật_là\nthật_lực\nthật_quả\nthật_ra\nthật_sự\nthật_thà\nthật_tốt\nthật_vậy\nthế\nthế_chuẩn_bị\nthế_là\nthế_lại\nthế_mà\nthế_nào\nthế_nên\nthế_ra\nthế_sự\nthế_thì\nthế_thôi\nthế_thường\nthế_thế\nthế_à\nthế_đó\nthếch\nthỉnh_thoảng\nthỏm\nthốc\nthốc_tháo\nthốt\nthốt_nhiên\nthốt_nói\nthốt_thôi\nthộc\nthời_gian\nthời_gian_sử_dụng\nthời_gian_tính\nthời_điểm\nthục_mạng\nthứ\nthứ_bản\nthứ_đến\nthửa\nthực_hiện\nthực_hiện_đúng\nthực_ra\nthực_sự\nthực_tế\nthực_vậy\ntin\ntin_thêm\ntin_vào\ntiếp_theo\ntiếp_tục\ntiếp_đó\ntiện_thể\ntoà\ntoé_khói\ntoẹt\ntrong\ntrong_khi\ntrong_lúc\ntrong_mình\ntrong_ngoài\ntrong_này\ntrong_số\ntrong_vùng\ntrong_đó\ntrong_ấy\ntránh\ntránh_khỏi\ntránh_ra\ntránh_tình_trạng\ntránh_xa\ntrên\ntrên_bộ\ntrên_dưới\ntrước\ntrước_hết\ntrước_khi\ntrước_kia\ntrước_nay\ntrước_ngày\ntrước_nhất\ntrước_sau\ntrước_tiên\ntrước_tuổi\ntrước_đây\ntrước_đó\ntrả\ntrả_của\ntrả_lại\ntrả_ngay\ntrả_trước\ntrếu_tráo\ntrển\ntrệt\ntrệu_trạo\ntrỏng\ntrời_đất_ơi\ntrở_thành\ntrừ_phi\ntrực_tiếp\ntrực_tiếp_làm\ntuy\ntuy_có\ntuy_là\ntuy_nhiên\ntuy_rằng\ntuy_thế\ntuy_vậy\ntuy_đã\ntuyệt_nhiên\ntuần_tự\ntuốt_luốt\ntuốt_tuồn_tuột\ntuốt_tuột\ntuổi\ntuổi_cả\ntuổi_tôi\ntà_tà\ntên\ntên_chính\ntên_cái\ntên_họ\ntên_tự\ntênh\ntênh_tênh\ntìm\ntìm_bạn\ntìm_cách\ntìm_hiểu\ntìm_ra\ntìm_việc\ntình_trạng\ntính\ntính_cách\ntính_căn\ntính_người\ntính_phỏng\ntính_từ\ntít_mù\ntò_te\ntôi\ntôi_con\ntông_tốc\ntù_tì\ntăm_tắp\ntăng\ntăng_chúng\ntăng_cấp\ntăng_giảm\ntăng_thêm\ntăng_thế\ntại\ntại_lòng\ntại_nơi\ntại_sao\ntại_tôi\ntại_vì\ntại_đâu\ntại_đây\ntại_đó\ntạo\ntạo_cơ_hội\ntạo_nên\ntạo_ra\ntạo_ý\ntạo_điều_kiện\ntấm\ntấm_bản\ntấm_các\ntấn\ntấn_tới\ntất_cả\ntất_cả_bao_nhiêu\ntất_thảy\ntất_tần_tật\ntất_tật\ntập_trung\ntắp\ntắp_lự\ntắp_tắp\ntọt\ntỏ_ra\ntỏ_vẻ\ntốc_tả\ntối_ư\ntốt\ntốt_bạn\ntốt_bộ\ntốt_hơn\ntốt_mối\ntốt_ngày\ntột\ntột_cùng\ntớ\ntới\ntới_gần\ntới_mức\ntới_nơi\ntới_thì\ntức_thì\ntức_tốc\ntừ\ntừ_căn\ntừ_giờ\ntừ_khi\ntừ_loại\ntừ_nay\ntừ_thế\ntừ_tính\ntừ_tại\ntừ_từ\ntừ_ái\ntừ_điều\ntừ_đó\ntừ_ấy\ntừng\ntừng_cái\ntừng_giờ\ntừng_nhà\ntừng_phần\ntừng_thời_gian\ntừng_đơn_vị\ntừng_ấy\ntự\ntự_cao\ntự_khi\ntự_lượng\ntự_tính\ntự_tạo\ntự_vì\ntự_ý\ntự_ăn\ntựu_trung\nveo\nveo_veo\nviệc\nviệc_gì\nvung_thiên_địa\nvung_tàn_tán\nvung_tán_tàn\nvà\nvài\nvài_ba\nvài_người\nvài_nhà\nvài_nơi\nvài_tên\nvài_điều\nvào\nvào_gặp\nvào_khoảng\nvào_lúc\nvào_vùng\nvào_đến\nvâng\nvâng_chịu\nvâng_dạ\nvâng_vâng\nvâng_ý\nvèo\nvèo_vèo\nvì\nvì_chưng\nvì_rằng\nvì_sao\nvì_thế\nvì_vậy\nví_bằng\nví_dù\nví_phỏng\nví_thử\nvô_hình_trung\nvô_kể\nvô_luận\nvô_vàn\nvùng\nvùng_lên\nvùng_nước\nvăng_tê\nvượt\nvượt_khỏi\nvượt_quá\nvạn_nhất\nvả_chăng\nvả_lại\nvấn_đề\nvấn_đề_quan_trọng\nvẫn\nvẫn_thế\nvậy\nvậy_là\nvậy_mà\nvậy_nên\nvậy_ra\nvậy_thì\nvậy_ư\nvề\nvề_không\nvề_nước\nvề_phần\nvề_sau\nvề_tay\nvị_trí\nvị_tất\nvốn_dĩ\nvới\nvới_lại\nvới_nhau\nvở\nvụt\nvừa\nvừa_khi\nvừa_lúc\nvừa_mới\nvừa_qua\nvừa_rồi\nvừa_vừa\nxa\nxa_cách\nxa_gần\nxa_nhà\nxa_tanh\nxa_tắp\nxa_xa\nxa_xả\nxem\nxem_lại\nxem_ra\nxem_số\nxin\nxin_gặp\nxin_vâng\nxiết_bao\nxon_xón\nxoành_xoạch\nxoét\nxoẳn\nxoẹt\nxuất_hiện\nxuất_kì_bất_ý\nxuất_kỳ_bất_ý\nxuể\nxuống\nxăm_xúi\nxăm_xăm\nxăm_xắm\nxảy_ra\nxềnh_xệch\nxệp\nxử_lý\nyêu_cầu\nà\nà_này\nà_ơi\nào\nào_vào\nào_ào\ná\ná_à\nái\nái_chà\nái_dà\náng\náng_như\nâu_là\nít\nít_biết\nít_có\nít_hơn\nít_khi\nít_lâu\nít_nhiều\nít_nhất\nít_nữa\nít_quá\nít_ra\nít_thôi\nít_thấy\nô_hay\nô_hô\nô_kê\nô_kìa\nôi_chao\nôi_thôi\nông\nông_nhỏ\nông_tạo\nông_từ\nông_ấy\nông_ổng\núi\núi_chà\núi_dào\ný\ný_chừng\ný_da\ný_hoặc\năn\năn_chung\năn_chắc\năn_chịu\năn_cuộc\năn_hết\năn_hỏi\năn_làm\năn_người\năn_ngồi\năn_quá\năn_riêng\năn_sáng\năn_tay\năn_trên\năn_về\nđang\nđang_tay\nđang_thì\nđiều\nđiều_gì\nđiều_kiện\nđiểm\nđiểm_chính\nđiểm_gặp\nđiểm_đầu_tiên\nđành_đạch\nđáng\nđáng_kể\nđáng_lí\nđáng_lý\nđáng_lẽ\nđáng_số\nđánh_giá\nđánh_đùng\nđáo_để\nđâu\nđâu_có\nđâu_cũng\nđâu_như\nđâu_nào\nđâu_phải\nđâu_đâu\nđâu_đây\nđâu_đó\nđây\nđây_này\nđây_rồi\nđây_đó\nđã\nđã_hay\nđã_không\nđã_là\nđã_lâu\nđã_thế\nđã_vậy\nđã_đủ\nđó\nđó_đây\nđúng\nđúng_ngày\nđúng_ra\nđúng_tuổi\nđúng_với\nđơn_vị\nđưa\nđưa_cho\nđưa_chuyện\nđưa_em\nđưa_ra\nđưa_tay\nđưa_tin\nđưa_tới\nđưa_vào\nđưa_về\nđưa_xuống\nđưa_đến\nđược\nđược_cái\nđược_lời\nđược_nước\nđược_tin\nđại_loại\nđại_nhân\nđại_phàm\nđại_để\nđạt\nđảm_bảo\nđầu_tiên\nđầy\nđầy_năm\nđầy_phè\nđầy_tuổi\nđặc_biệt\nđặt\nđặt_làm\nđặt_mình\nđặt_mức\nđặt_ra\nđặt_trước\nđặt_để\nđến\nđến_bao_giờ\nđến_cùng\nđến_cùng_cực\nđến_cả\nđến_giờ\nđến_gần\nđến_hay\nđến_khi\nđến_lúc\nđến_lời\nđến_nay\nđến_ngày\nđến_nơi\nđến_nỗi\nđến_thì\nđến_thế\nđến_tuổi\nđến_xem\nđến_điều\nđến_đâu\nđều\nđều_bước\nđều_nhau\nđều_đều\nđể\nđể_cho\nđể_giống\nđể_không\nđể_lòng\nđể_lại\nđể_mà\nđể_phần\nđể_được\nđể_đến_nỗi\nđối_với\nđồng_thời\nđủ\nđủ_dùng\nđủ_nơi\nđủ_số\nđủ_điều\nđủ_điểm\nơ\nơ_hay\nơ_kìa\nơi\nơi_là\nư\nạ\nạ_ơi\nấy\nấy_là\nầu_ơ\nắt\nắt_hẳn\nắt_là\nắt_phải\nắt_thật\nối_dào\nối_giời\nối_giời_ơi\nồ\nồ_ồ\nổng\nớ\nớ_này\nờ\nờ_ờ\nở\nở_lại\nở_như\nở_nhờ\nở_năm\nở_trên\nở_vào\nở_đây\nở_đó\nở_được\nủa\nứ_hự\nứ_ừ\nừ\nừ_nhé\nừ_thì\nừ_ào\nừ_ừ\nử\n'.split('\n'))


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/vi/tag_map.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ga/__init__.py----------------------------------------
A:spacy.lang.ga.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.ga.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS, TOKENIZER_EXCEPTIONS)
A:spacy.lang.ga.__init__.stop_words->set(STOP_WORDS)
spacy.lang.ga.__init__.Irish(Language)
spacy.lang.ga.__init__.IrishDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ga/irish_morphology_helpers.py----------------------------------------
spacy.lang.ga.irish_morphology_helpers.deduplicate(word)
spacy.lang.ga.irish_morphology_helpers.devoice(word)
spacy.lang.ga.irish_morphology_helpers.ends_dentals(word)
spacy.lang.ga.irish_morphology_helpers.ends_with_vowel(word)
spacy.lang.ga.irish_morphology_helpers.starts_with_vowel(word)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ga/tokenizer_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ga/stop_words.py----------------------------------------
A:spacy.lang.ga.stop_words.STOP_WORDS->set('\na ach ag agus an aon ar arna as\n\nba beirt bhúr\n\ncaoga ceathair ceathrar chomh chuig chun cois céad cúig cúigear\n\ndaichead dar de deich deichniúr den dhá do don dtí dá dár dó\n\nfaoi faoin faoina faoinár fara fiche\n\ngach gan go gur\n\nhaon hocht\n\ni iad idir in ina ins inár is\n\nle leis lena lenár\n\nmar mo muid mé\n\nna nach naoi naonúr ná ní níor nó nócha\n\nocht ochtar ochtó os\n\nroimh\n\nsa seacht seachtar seachtó seasca seisear siad sibh sinn sna sé sí\n\ntar thar thú triúr trí trína trínár tríocha tú\n\num\n\nár\n\né éis\n\ní\n\nó ón óna ónár\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ga/tag_map.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/te/__init__.py----------------------------------------
A:spacy.lang.te.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
spacy.lang.te.__init__.Telugu(Language)
spacy.lang.te.__init__.TeluguDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/te/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/te/lex_attrs.py----------------------------------------
A:spacy.lang.te.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.te.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.te.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/te/stop_words.py----------------------------------------
A:spacy.lang.te.stop_words.STOP_WORDS->set('\nఅందరూ\nఅందుబాటులో\nఅడగండి\nఅడగడం\nఅడ్డంగా\nఅనుగుణంగా\nఅనుమతించు\nఅనుమతిస్తుంది\nఅయితే\nఇప్పటికే\nఉన్నారు\nఎక్కడైనా\nఎప్పుడు\nఎవరైనా\nఎవరో ఒకరు\nఏ\nఏదైనా\nఏమైనప్పటికి\nఏమైనప్పటికి\nఒక\nఒక ప్రక్కన\nకనిపిస్తాయి\nకాదు\nకాదు\nకూడా\nగా\nగురించి\nచుట్టూ\nచేయగలిగింది\nతగిన\nతర్వాత\nతర్వాత\nదాదాపు\nదూరంగా\nనిజంగా\nపై\nప్రకారం\nమధ్య\nమధ్య\nమరియు\nమరొక\nమళ్ళీ\nమాత్రమే\nమెచ్చుకో\nవద్ద\nవద్ద\nవెంట\nవేరుగా\nవ్యతిరేకంగా\nసంబంధం\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/si/__init__.py----------------------------------------
A:spacy.lang.si.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
spacy.lang.si.__init__.Sinhala(Language)
spacy.lang.si.__init__.SinhalaDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/si/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/si/lex_attrs.py----------------------------------------
A:spacy.lang.si.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.si.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.si.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/si/stop_words.py----------------------------------------
A:spacy.lang.si.stop_words.STOP_WORDS->set('\nඅතර\nඑච්චර\nඑපමණ\nඑලෙස\nඑවිට\nඒ\nකට\nකදී\nකින්\nක්\nට\nතුර\nත්\nද\nනමුත්\nනොහොත්\nපමණ\nපමණි\nම\nමෙච්චර\nමෙපමණ\nමෙලෙස\nමෙවිට\nමේ\nය\nයි\nලදී\nලෙස\nවගේ\nවන\nවිට\nවිටෙක\nවිතර\nවිය\nවුව\nවුවත්\nවුවද\nවූ\nසමඟ\nසහ\nහා\nහෙවත්\nහෝ\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/it/__init__.py----------------------------------------
A:spacy.lang.it.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.it.__init__.lex_attr_getters[NORM]->add_lookups(Language.Defaults.lex_attr_getters[NORM], BASE_NORMS)
A:spacy.lang.it.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS, TOKENIZER_EXCEPTIONS)
spacy.lang.it.__init__.Italian(Language)
spacy.lang.it.__init__.ItalianDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/it/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/it/tokenizer_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/it/stop_words.py----------------------------------------
A:spacy.lang.it.stop_words.STOP_WORDS->set('\na abbastanza abbia abbiamo abbiano abbiate accidenti ad adesso affinche agl\nagli ahime ahimè ai al alcuna alcuni alcuno all alla alle allo allora altri\naltrimenti altro altrove altrui anche ancora anni anno ansa anticipo assai\nattesa attraverso avanti avemmo avendo avente aver avere averlo avesse\navessero avessi avessimo aveste avesti avete aveva avevamo avevano avevate\navevi avevo avrai avranno avrebbe avrebbero avrei avremmo avremo avreste\navresti avrete avrà avrò avuta avute avuti avuto\n\nbasta bene benissimo brava bravo\n\ncasa caso cento certa certe certi certo che chi chicchessia chiunque ci\nciascuna ciascuno cima cio cioe circa citta città co codesta codesti codesto\ncogli coi col colei coll coloro colui come cominci comunque con concernente\nconciliarsi conclusione consiglio contro cortesia cos cosa cosi così cui\n\nda dagl dagli dai dal dall dalla dalle dallo dappertutto davanti degl degli\ndei del dell della delle dello dentro detto deve di dice dietro dire\ndirimpetto diventa diventare diventato dopo dov dove dovra dovrà dovunque due\ndunque durante\n\nebbe ebbero ebbi ecc ecco ed effettivamente egli ella entrambi eppure era\nerano eravamo eravate eri ero esempio esse essendo esser essere essi ex\n\nfa faccia facciamo facciano facciate faccio facemmo facendo facesse facessero\nfacessi facessimo faceste facesti faceva facevamo facevano facevate facevi\nfacevo fai fanno farai faranno fare farebbe farebbero farei faremmo faremo\nfareste faresti farete farà farò fatto favore fece fecero feci fin finalmente\nfinche fine fino forse forza fosse fossero fossi fossimo foste fosti fra\nfrattempo fu fui fummo fuori furono futuro generale\n\ngia già giacche giorni giorno gli gliela gliele glieli glielo gliene governo\ngrande grazie gruppo\n\nha haha hai hanno ho\n\nieri il improvviso in inc infatti inoltre insieme intanto intorno invece io\n\nla là lasciato lato lavoro le lei li lo lontano loro lui lungo luogo\n\nma macche magari maggior mai male malgrado malissimo mancanza marche me\nmedesimo mediante meglio meno mentre mesi mezzo mi mia mie miei mila miliardi\nmilioni minimi ministro mio modo molti moltissimo molto momento mondo mosto\n\nnazionale ne negl negli nei nel nell nella nelle nello nemmeno neppure nessun\nnessuna nessuno niente no noi non nondimeno nonostante nonsia nostra nostre\nnostri nostro novanta nove nulla nuovo\n\nod oggi ogni ognuna ognuno oltre oppure ora ore osi ossia ottanta otto\n\npaese parecchi parecchie parecchio parte partendo peccato peggio per perche\nperché percio perciò perfino pero persino persone però piedi pieno piglia piu\npiuttosto più po pochissimo poco poi poiche possa possedere posteriore posto\npotrebbe preferibilmente presa press prima primo principalmente probabilmente\nproprio puo può pure purtroppo\n\nqualche qualcosa qualcuna qualcuno quale quali qualunque quando quanta quante\nquanti quanto quantunque quasi quattro quel quella quelle quelli quello quest\nquesta queste questi questo qui quindi\n\nrealmente recente recentemente registrazione relativo riecco salvo\n\nsara sarà sarai saranno sarebbe sarebbero sarei saremmo saremo sareste\nsaresti sarete saro sarò scola scopo scorso se secondo seguente seguito sei\nsembra sembrare sembrato sembri sempre senza sette si sia siamo siano siate\nsiete sig solito solo soltanto sono sopra sotto spesso srl sta stai stando\nstanno starai staranno starebbe starebbero starei staremmo staremo stareste\nstaresti starete starà starò stata state stati stato stava stavamo stavano\nstavate stavi stavo stemmo stessa stesse stessero stessi stessimo stesso\nsteste stesti stette stettero stetti stia stiamo stiano stiate sto su sua\nsubito successivamente successivo sue sugl sugli sui sul sull sulla sulle\nsullo suo suoi\n\ntale tali talvolta tanto te tempo ti titolo torino tra tranne tre trenta\ntroppo trovato tu tua tue tuo tuoi tutta tuttavia tutte tutti tutto\n\nuguali ulteriore ultimo un una uno uomo\n\nva vale vari varia varie vario verso vi via vicino visto vita voi volta volte\nvostra vostre vostri vostro\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/it/tag_map.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/it/punctuation.py----------------------------------------
A:spacy.lang.it.punctuation.ELISION->" ' ’ ".strip().replace(' ', '').replace('\n', '')


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/en/__init__.py----------------------------------------
A:spacy.lang.en.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.en.__init__.lex_attr_getters[NORM]->add_lookups(Language.Defaults.lex_attr_getters[NORM], BASE_NORMS, NORM_EXCEPTIONS)
A:spacy.lang.en.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS, TOKENIZER_EXCEPTIONS)
spacy.lang.en.__init__.English(Language)
spacy.lang.en.__init__.EnglishDefaults(Language.Defaults)
spacy.lang.en.__init__._return_en(_)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/en/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/en/lex_attrs.py----------------------------------------
A:spacy.lang.en.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.en.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.en.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/en/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.en.tokenizer_exceptions.verb_data_tc->dict(verb_data)
A:spacy.lang.en.tokenizer_exceptions.verb_data_tc[ORTH]->verb_data_tc[ORTH].title().title()
A:spacy.lang.en.tokenizer_exceptions.exc_data_tc->dict(exc_data)
A:spacy.lang.en.tokenizer_exceptions.exc_data_tc[ORTH]->exc_data_tc[ORTH].title().title()
A:spacy.lang.en.tokenizer_exceptions.data_apos->dict(data)
A:spacy.lang.en.tokenizer_exceptions.exc_data_apos->dict(exc_data)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/en/syntax_iterators.py----------------------------------------
A:spacy.lang.en.syntax_iterators.conj->doc.vocab.strings.add('conj')
A:spacy.lang.en.syntax_iterators.np_label->doc.vocab.strings.add('NP')
A:spacy.lang.en.syntax_iterators.seen->set()
spacy.lang.en.syntax_iterators.noun_chunks(obj)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/en/morph_rules.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/en/stop_words.py----------------------------------------
A:spacy.lang.en.stop_words.STOP_WORDS->set('\na about above across after afterwards again against all almost alone along\nalready also although always am among amongst amount an and another any anyhow\nanyone anything anyway anywhere are around as at\n\nback be became because become becomes becoming been before beforehand behind\nbeing below beside besides between beyond both bottom but by\n\ncall can cannot ca could\n\ndid do does doing done down due during\n\neach eight either eleven else elsewhere empty enough even ever every\neveryone everything everywhere except\n\nfew fifteen fifty first five for former formerly forty four from front full\nfurther\n\nget give go\n\nhad has have he hence her here hereafter hereby herein hereupon hers herself\nhim himself his how however hundred\n\ni if in indeed into is it its itself\n\nkeep\n\nlast latter latterly least less\n\njust\n\nmade make many may me meanwhile might mine more moreover most mostly move much\nmust my myself\n\nname namely neither never nevertheless next nine no nobody none noone nor not\nnothing now nowhere\n\nof off often on once one only onto or other others otherwise our ours ourselves\nout over own\n\npart per perhaps please put\n\nquite\n\nrather re really regarding\n\nsame say see seem seemed seeming seems serious several she should show side\nsince six sixty so some somehow someone something sometime sometimes somewhere\nstill such\n\ntake ten than that the their them themselves then thence there thereafter\nthereby therefore therein thereupon these they third this those though three\nthrough throughout thru thus to together too top toward towards twelve twenty\ntwo\n\nunder until up unless upon us used using\n\nvarious very very via was we well were what whatever when whence whenever where\nwhereafter whereas whereby wherein whereupon wherever whether which while\nwhither who whoever whole whom whose why will with within without would\n\nyet you your yours yourself yourselves\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/en/norm_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/en/tag_map.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/fi/__init__.py----------------------------------------
A:spacy.lang.fi.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.fi.__init__.lex_attr_getters[NORM]->add_lookups(Language.Defaults.lex_attr_getters[NORM], BASE_NORMS)
A:spacy.lang.fi.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS, TOKENIZER_EXCEPTIONS)
spacy.lang.fi.__init__.Finnish(Language)
spacy.lang.fi.__init__.FinnishDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/fi/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/fi/lex_attrs.py----------------------------------------
A:spacy.lang.fi.lex_attrs.text->text.replace('.', '').replace(',', '').replace('.', '').replace(',', '')
A:spacy.lang.fi.lex_attrs.(num, denom)->text.replace('.', '').replace(',', '').replace('.', '').replace(',', '').split('/')
spacy.lang.fi.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/fi/tokenizer_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/fi/stop_words.py----------------------------------------
A:spacy.lang.fi.stop_words.STOP_WORDS->set('\naiemmin aika aikaa aikaan aikaisemmin aikaisin aikana aikoina aikoo aikovat\naina ainakaan ainakin ainoa ainoat aiomme aion aiotte aivan ajan alas alemmas\nalkuisin alkuun alla alle aloitamme aloitan aloitat aloitatte aloitattivat\naloitettava aloitettavaksi aloitettu aloitimme aloitin aloitit aloititte\naloittaa aloittamatta aloitti aloittivat alta aluksi alussa alusta annettavaksi\nannettava annettu ansiosta antaa antamatta antoi apu asia asiaa asian asiasta\nasiat asioiden asioihin asioita asti avuksi avulla avun avutta\n\nedelle edelleen edellä edeltä edemmäs edes edessä edestä ehkä ei eikä eilen\neivät eli ellei elleivät ellemme ellen ellet ellette emme en enemmän eniten\nennen ensi ensimmäinen ensimmäiseksi ensimmäisen ensimmäisenä ensimmäiset\nensimmäisiksi ensimmäisinä ensimmäisiä ensimmäistä ensin entinen entisen\nentisiä entisten entistä enää eri erittäin erityisesti eräiden eräs eräät esi\nesiin esillä esimerkiksi et eteen etenkin ette ettei että\n\nhalua haluaa haluamatta haluamme haluan haluat haluatte haluavat halunnut\nhalusi halusimme halusin halusit halusitte halusivat halutessa haluton he hei\nheidän heidät heihin heille heillä heiltä heissä heistä heitä helposti heti\nhetkellä hieman hitaasti huolimatta huomenna hyvien hyviin hyviksi hyville\nhyviltä hyvin hyvinä hyvissä hyvistä hyviä hyvä hyvät hyvää hän häneen hänelle\nhänellä häneltä hänen hänessä hänestä hänet häntä\n\nihan ilman ilmeisesti itse itsensä itseään\n\nja jo johon joiden joihin joiksi joilla joille joilta joina joissa joista joita\njoka jokainen jokin joko joksi joku jolla jolle jolloin jolta jompikumpi jona\njonka jonkin jonne joo jopa jos joskus jossa josta jota jotain joten jotenkin\njotenkuten jotka jotta jouduimme jouduin jouduit jouduitte joudumme joudun\njoudutte joukkoon joukossa joukosta joutua joutui joutuivat joutumaan joutuu\njoutuvat juuri jälkeen jälleen jää\n\nkahdeksan kahdeksannen kahdella kahdelle kahdelta kahden kahdessa kahdesta\nkahta kahteen kai kaiken kaikille kaikilta kaikkea kaikki kaikkia kaikkiaan\nkaikkialla kaikkialle kaikkialta kaikkien kaikkiin kaksi kannalta kannattaa\nkanssa kanssaan kanssamme kanssani kanssanne kanssasi kauan kauemmas kaukana\nkautta kehen keiden keihin keiksi keille keillä keiltä keinä keissä keistä\nkeitten keittä keitä keneen keneksi kenelle kenellä keneltä kenen kenenä\nkenessä kenestä kenet kenettä kenties kerran kerta kertaa keskellä kesken\nkeskimäärin ketkä ketä kiitos kohti koko kokonaan kolmas kolme kolmen kolmesti\nkoska koskaan kovin kuin kuinka kuinkaan kuitenkaan kuitenkin kuka kukaan kukin\nkumpainen kumpainenkaan kumpi kumpikaan kumpikin kun kuten kuuden kuusi kuutta\nkylliksi kyllä kymmenen kyse\n\nliian liki lisäksi lisää lla luo luona lähekkäin lähelle lähellä läheltä\nlähemmäs lähes lähinnä lähtien läpi\n\nmahdollisimman mahdollista me meidän meidät meihin meille meillä meiltä meissä\nmeistä meitä melkein melko menee menemme menen menet menette menevät meni\nmenimme menin menit menivät mennessä mennyt menossa mihin miksi mikä mikäli\nmikään mille milloin milloinkan millä miltä minkä minne minua minulla minulle\nminulta minun minussa minusta minut minuun minä missä mistä miten mitkä mitä\nmitään moi molemmat mones monesti monet moni moniaalla moniaalle moniaalta\nmonta muassa muiden muita muka mukaan mukaansa mukana mutta muu muualla muualle\nmuualta muuanne muulloin muun muut muuta muutama muutaman muuten myöhemmin myös\nmyöskin myöskään myötä\n\nne neljä neljän neljää niiden niihin niiksi niille niillä niiltä niin niinä\nniissä niistä niitä noiden noihin noiksi noilla noille noilta noin noina noissa\nnoista noita nopeammin nopeasti nopeiten nro nuo nyt näiden näihin näiksi\nnäille näillä näiltä näin näinä näissä näistä näitä nämä\n\nohi oikea oikealla oikein ole olemme olen olet olette oleva olevan olevat oli\nolimme olin olisi olisimme olisin olisit olisitte olisivat olit olitte olivat\nolla olleet ollut oma omaa omaan omaksi omalle omalta oman omassa omat omia\nomien omiin omiksi omille omilta omissa omista on onkin onko ovat\n\npaikoittain paitsi pakosti paljon paremmin parempi parhaillaan parhaiten\nperusteella peräti pian pieneen pieneksi pienelle pienellä pieneltä pienempi\npienestä pieni pienin poikki puolesta puolestaan päälle\n\nrunsaasti\n\nsaakka sama samaa samaan samalla saman samat samoin satojen se\nseitsemän sekä sen seuraavat siellä sieltä siihen siinä siis siitä sijaan siksi\nsille silloin sillä silti siltä sinne sinua sinulla sinulle sinulta sinun\nsinussa sinusta sinut sinuun sinä sisäkkäin sisällä siten sitten sitä ssa sta\nsuoraan suuntaan suuren suuret suuri suuria suurin suurten\n\ntaa taas taemmas tahansa tai takaa takaisin takana takia tallä tapauksessa\ntarpeeksi tavalla tavoitteena te teidän teidät teihin teille teillä teiltä\nteissä teistä teitä tietysti todella toinen toisaalla toisaalle toisaalta\ntoiseen toiseksi toisella toiselle toiselta toisemme toisen toisensa toisessa\ntoisesta toista toistaiseksi toki tosin tule tulee tulemme tulen\ntulet tulette tulevat tulimme tulin tulisi tulisimme tulisin tulisit tulisitte\ntulisivat tulit tulitte tulivat tulla tulleet tullut tuntuu tuo tuohon tuoksi\ntuolla tuolle tuolloin tuolta tuon tuona tuonne tuossa tuosta tuota tuskin tykö\ntähän täksi tälle tällä tällöin tältä tämä tämän tänne tänä tänään tässä tästä\ntäten tätä täysin täytyvät täytyy täällä täältä\n\nulkopuolella usea useasti useimmiten usein useita uudeksi uudelleen uuden uudet\nuusi uusia uusien uusinta uuteen uutta\n\nvaan vai vaiheessa vaikea vaikean vaikeat vaikeilla vaikeille vaikeilta\nvaikeissa vaikeista vaikka vain varmasti varsin varsinkin varten vasen\nvasemmalla vasta vastaan vastakkain vastan verran vielä vierekkäin vieressä\nvieri viiden viime viimeinen viimeisen viimeksi viisi voi voidaan voimme voin\nvoisi voit voitte voivat vuoden vuoksi vuosi vuosien vuosina vuotta vähemmän\nvähintään vähiten vähän välillä\n\nyhdeksän yhden yhdessä yhteen yhteensä yhteydessä yhteyteen yhtä yhtäälle\nyhtäällä yhtäältä yhtään yhä yksi yksin yksittäin yleensä ylemmäs yli ylös\nympäri\n\nälköön älä\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/sq/__init__.py----------------------------------------
A:spacy.lang.sq.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
spacy.lang.sq.__init__.Albanian(Language)
spacy.lang.sq.__init__.AlbanianDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/sq/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/sq/stop_words.py----------------------------------------
A:spacy.lang.sq.stop_words.STOP_WORDS->set('\na\nafert\nai\najo\nandej\nanes\naq\nas\nasaj\nashtu\nata\nate\natij\natje\nato\naty\natyre\nb\nbe\nbehem\nbehet\nbej\nbeje\nbejne\nben\nbene\nbere\nberi\nbie\nc\nca\ncdo\ncfare\ncila\ncilat\ncilave\ncilen\nciles\ncilet\ncili\ncilin\ncilit\nderi\ndhe\ndic\ndicka\ndickaje\ndike\ndikujt\ndikush\ndisa\ndo\ndot\ndrejt\nduke\ndy\ne\nedhe\nende\neshte\netj\nfare\ngjate\ngje\ngjitha\ngjithcka\ngjithe\ngjithnje\nhere\ni\nia\nishin\nishte\niu\nja\njam\njane\njap\nje\njemi\njo\nju\nk\nka\nkam\nkane\nkem\nkemi\nkeq\nkesaj\nkeshtu\nkete\nketej\nketij\nketo\nketu\nketyre\nkishin\nkishte\nkjo\nkrejt\nkryer\nkryesisht\nkryhet\nku\nkudo\nkundrejt\nkur\nkurre\nkush\nky\nla\nle\nlloj\nm\nma\nmadhe\nmarr\nmarre\nmban\nmbi\nme\nmenjehere\nmerr\nmerret\nmes\nmi\nmidis\nmire\nmjaft\nmori\nmos\nmua\nmund\nna\nndaj\nnder\nndermjet\nndersa\nndonje\nndryshe\nne\nnen\nneper\nnepermjet\nnese\nnga\nnje\nnjera\nnuk\nose\npa\npak\npapritur\npara\npas\npasi\npasur\nper\nperbashket\nperpara\npo\npor\nprane\nprapa\nprej\npse\nqe\nqene\nqenet\nrralle\nrreth\nrri\ns\nsa\nsaj\nsapo\nse\nsecila\nsepse\nsh\nshih\nshume\nsi\nsic\nsikur\nsipas\nsiper\nsone\nt\nta\ntani\nte\ntej\ntek\nteper\ntere\nti\ntij\ntilla\ntille\ntjera\ntjeret\ntjeter\ntjetren\nto\ntone\nty\ntyre\nu\nua\nune\nvazhdimisht\nvend\nvet\nveta\nvete\nvetem\nveten\nvetes\nvjen\nyne\nzakonisht\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/is/__init__.py----------------------------------------
A:spacy.lang.is.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
spacy.lang.is.__init__.Icelandic(Language)
spacy.lang.is.__init__.IcelandicDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/is/stop_words.py----------------------------------------
A:spacy.lang.is.stop_words.STOP_WORDS->set('\nafhverju\naftan\naftur\nafþví\naldrei\nallir\nallt\nalveg\nannað\nannars\nbara\ndag\neða\neftir\neiga\neinhver\neinhverjir\neinhvers\neins\neinu\neitthvað\nekkert\nekki\nennþá\neru\nfara\nfer\nfinna\nfjöldi\nfólk\nframan\nfrá\nfrekar\nfyrir\ngegnum\ngeta\ngetur\ngmg\ngott\nhann\nhafa\nhef\nhefur\nheyra\nhér\nhérna\nhjá\nhún\nhvað\nhvar\nhver\nhverjir\nhverjum\nhvernig\nhvor\nhvort\nhægt\nimg\ninn\nkannski\nkoma\nlíka\nlol\nmaður\nmátt\nmér\nmeð\nmega\nmeira\nmig\nmikið\nminna\nminni\nmissa\nmjög\nnei\nniður\nnúna\noft\nokkar\nokkur\npóst\npóstur\nrofl\nsaman\nsem\nsér\nsig\nsinni\nsíðan\nsjá\nsmá\nsmátt\nspurja\nspyrja\nstaðar\nstórt\nsvo\nsvona\nsælir\nsæll\ntaka\ntakk\ntil\ntilvitnun\ntitlar\nupp\nvar\nvel\nvelkomin\nvelkominn\nvera\nverður\nverið\nvel\nvið\nvil\nvilja\nvill\nvita\nværi\nyfir\nykkar\nþað\nþakka\nþakkir\nþannig\nþað\nþar\nþarf\nþau\nþeim\nþeir\nþeirra\nþeirra\nþegar\nþess\nþessa\nþessi\nþessu\nþessum\nþetta\nþér\nþið\nþinn\nþitt\nþín\nþráð\nþráður\nþví\nþær\nætti\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ca/__init__.py----------------------------------------
A:spacy.lang.ca.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.ca.__init__.lex_attr_getters[NORM]->add_lookups(Language.Defaults.lex_attr_getters[NORM], BASE_NORMS)
A:spacy.lang.ca.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS, TOKENIZER_EXCEPTIONS)
spacy.lang.ca.__init__.Catalan(Language)
spacy.lang.ca.__init__.CatalanDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ca/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ca/lex_attrs.py----------------------------------------
A:spacy.lang.ca.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.ca.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.ca.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ca/tokenizer_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ca/stop_words.py----------------------------------------
A:spacy.lang.ca.stop_words.STOP_WORDS->set("\na abans ací ah així això al aleshores algun alguna algunes alguns alhora allà allí allò\nals altra altre altres amb ambdues ambdós anar ans apa aquell aquella aquelles aquells\naquest aquesta aquestes aquests aquí\n\nbaix bastant bé\n\ncada cadascuna cadascunes cadascuns cadascú com consegueixo conseguim conseguir\nconsigueix consigueixen consigueixes contra\n\nd'un d'una d'unes d'uns dalt de del dels des des de després dins dintre donat doncs durant\n\ne eh el elles ells els em en encara ens entre era erem eren eres es esta estan estat\nestava estaven estem esteu estic està estàvem estàveu et etc ets érem éreu és éssent\n\nfa faig fan fas fem fer feu fi fins fora\n\ngairebé\n\nha han has haver havia he hem heu hi ho\n\ni igual iguals inclòs\n\nja jo\n\nl'hi la les li li'n llarg llavors\n\nm'he ma mal malgrat mateix mateixa mateixes mateixos me mentre meu meus meva\nmeves mode molt molta moltes molts mon mons més\n\nn'he n'hi ne ni no nogensmenys només nosaltres nostra nostre nostres\n\no oh oi on\n\npas pel pels per per que perquè però poc poca pocs podem poden poder\npodeu poques potser primer propi puc\n\nqual quals quan quant que quelcom qui quin quina quines quins què\n\ns'ha s'han sa sabem saben saber sabeu sap saps semblant semblants sense ser ses\nseu seus seva seves si sobre sobretot soc solament sols som son sons sota sou sóc són\n\nt'ha t'han t'he ta tal també tampoc tan tant tanta tantes te tene tenim tenir teniu\nteu teus teva teves tinc ton tons tot tota totes tots\n\nun una unes uns us últim ús\n\nva vaig vam van vas veu vosaltres vostra vostre vostres\n\n".split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ca/tag_map.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ca/punctuation.py----------------------------------------
A:spacy.lang.ca.punctuation.ELISION->" ' ’ ".strip().replace(' ', '').replace('\n', '')


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/nb/__init__.py----------------------------------------
A:spacy.lang.nb.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.nb.__init__.lex_attr_getters[NORM]->add_lookups(Language.Defaults.lex_attr_getters[NORM], BASE_NORMS)
A:spacy.lang.nb.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS, TOKENIZER_EXCEPTIONS)
spacy.lang.nb.__init__.Norwegian(Language)
spacy.lang.nb.__init__.NorwegianDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/nb/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/nb/tokenizer_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/nb/syntax_iterators.py----------------------------------------
A:spacy.lang.nb.syntax_iterators.conj->doc.vocab.strings.add('conj')
A:spacy.lang.nb.syntax_iterators.np_label->doc.vocab.strings.add('NP')
A:spacy.lang.nb.syntax_iterators.seen->set()
spacy.lang.nb.syntax_iterators.noun_chunks(obj)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/nb/morph_rules.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/nb/stop_words.py----------------------------------------
A:spacy.lang.nb.stop_words.STOP_WORDS->set('\nalle allerede alt and andre annen annet at av\n\nbak bare bedre beste blant ble bli blir blitt bris by både\n\nda dag de del dem den denne der dermed det dette disse drept du\n\neller en enn er et ett etter\n\nfem fikk fire fjor flere folk for fortsatt fotball fra fram frankrike fredag\nfunnet få får fått før først første\n\ngang gi gikk gjennom gjorde gjort gjør gjøre god godt grunn gå går\n\nha hadde ham han hans har hele helt henne hennes her hun hva hvor hvordan\nhvorfor\n\ni ifølge igjen ikke ingen inn\n\nja jeg\n\nkamp kampen kan kl klart kom komme kommer kontakt kort kroner kunne kveld\nkvinner\n\nla laget land landet langt leder ligger like litt løpet lørdag\n\nman mandag mange mannen mars med meg mellom men mener menn mennesker mens mer\nmillioner minutter mot msci mye må mål måtte\n\nned neste noe noen nok norge norsk norske ntb ny nye nå når\n\nog også om onsdag opp opplyser oslo oss over\n\npersoner plass poeng politidistrikt politiet president prosent på\n\nregjeringen runde rundt russland\n\nsa saken samme sammen samtidig satt se seg seks selv senere september ser sett\nsiden sier sin sine siste sitt skal skriver skulle slik som sted stedet stor\nstore står sverige svært så søndag\n\nta tatt tid tidligere til tilbake tillegg tirsdag to tok torsdag tre tror\ntyskland\n\nunder usa ut uten utenfor\n\nvant var ved veldig vi videre viktig vil ville viser vår være vært\n\nå år\n\nønsker\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/nb/tag_map.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/nb/punctuation.py----------------------------------------
A:spacy.lang.nb.punctuation._quotes->char_classes.CONCAT_QUOTES.replace("'", '')


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/xx/__init__.py----------------------------------------
A:spacy.lang.xx.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.xx.__init__.lex_attr_getters[NORM]->add_lookups(Language.Defaults.lex_attr_getters[NORM], BASE_NORMS)
A:spacy.lang.xx.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS)
spacy.lang.xx.__init__.MultiLanguage(Language)
spacy.lang.xx.__init__.MultiLanguageDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/bn/__init__.py----------------------------------------
A:spacy.lang.bn.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.bn.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS, TOKENIZER_EXCEPTIONS)
spacy.lang.bn.__init__.Bengali(Language)
spacy.lang.bn.__init__.BengaliDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/bn/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/bn/tokenizer_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/bn/morph_rules.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/bn/stop_words.py----------------------------------------
A:spacy.lang.bn.stop_words.STOP_WORDS->set('\nঅতএব অথচ অথবা অনুযায়ী অনেক অনেকে অনেকেই অন্তত  অবধি অবশ্য অর্থাৎ অন্য অনুযায়ী অর্ধভাগে\nআগামী আগে আগেই আছে আজ আদ্যভাগে আপনার আপনি আবার আমরা আমাকে আমাদের আমার  আমি আর আরও\nইত্যাদি ইহা\nউচিত উনি উপর উপরে উত্তর\nএ এঁদের এঁরা এই এক একই একজন একটা একটি  একবার একে এখন এখনও এখানে এখানেই এটা এসো\nএটাই এটি এত এতটাই এতে এদের এবং এবার এমন এমনি এমনকি এর এরা এলো এস এসে\nঐ\nও ওঁদের ওঁর ওঁরা ওই ওকে ওখানে ওদের ওর ওরা\nকখনও কত কথা কবে কয়েক  কয়েকটি করছে করছেন করতে  করবে করবেন করলে কয়েক  কয়েকটি করিয়ে করিয়া করায়\nকরলেন করা করাই করায় করার করি করিতে করিয়া করিয়ে করে করেই করেছিলেন করেছে করেছেন করেন কাউকে\nকাছ কাছে কাজ কাজে কারও কারণ কি কিংবা কিছু কিছুই কিন্তু কী কে কেউ কেউই কেন কোন কোনও কোনো কেমনে কোটি\nক্ষেত্রে খুব\nগিয়ে গিয়েছে গুলি গেছে গেল গেলে গোটা গিয়ে গিয়েছে\nচলে চান চায় চেয়ে চায় চেয়ে চার চালু চেষ্টা\nছাড়া ছাড়াও ছিল ছিলেন ছাড়া ছাড়াও\nজন জনকে জনের জন্য জন্যে জানতে জানা জানানো জানায়  জানিয়ে  জানিয়েছে জানায় জাানিয়ে জানিয়েছে\nটি\nঠিক\nতখন তত তথা তবু তবে তা তাঁকে তাঁদের তাঁর তাঁরা তাঁহারা তাই তাও তাকে তাতে তাদের তার তারপর তারা তারই তাহলে তাহা তাহাতে তাহার তিনই\nতিনি তিনিও তুমি তুলে তেমন তো তোমার তুই তোরা তোর তোমাদের তোদের\nথাকবে থাকবেন থাকা থাকায় থাকে থাকেন থেকে থেকেই  থেকেও থাকায়\nদিকে দিতে দিয়ে দিয়েছে দিয়েছেন দিলেন দিয়ে দু  দুটি  দুটো দেওয়া দেওয়ার দেখতে দেখা দেখে দেন দেয়  দেশের\nদ্বারা দিয়েছে দিয়েছেন দেয় দেওয়া দেওয়ার দিন দুই\nধরা ধরে\nনয় না নাই নাকি নাগাদ নানা নিজে নিজেই নিজেদের নিজের নিতে নিয়ে নিয়ে নেই নেওয়া নেওয়ার নয় নতুন\nপক্ষে পর পরে পরেই পরেও পর্যন্ত পাওয়া পারি পারে পারেন পেয়ে প্রতি প্রভৃতি প্রায় পাওয়া পেয়ে প্রায় পাঁচ প্রথম প্রাথমিক\nফলে ফিরে ফের\nবছর বদলে বরং বলতে বলল বললেন বলা বলে বলেছেন বলেন  বসে বহু বা বাদে বার বিনা বিভিন্ন বিশেষ বিষয়টি বেশ ব্যবহার ব্যাপারে বক্তব্য বন বেশি\nভাবে  ভাবেই\nমত মতো মতোই মধ্যভাগে মধ্যে মধ্যেই  মধ্যেও মনে মাত্র মাধ্যমে মানুষ মানুষের মোট মোটেই মোদের মোর\nযখন যত যতটা যথেষ্ট যদি যদিও যা যাঁর যাঁরা যাওয়া  যাওয়ার যাকে যাচ্ছে যাতে যাদের যান যাবে যায় যার  যারা যায় যিনি যে যেখানে যেতে যেন\nযেমন\nরকম রয়েছে রাখা রেখে রয়েছে\nলক্ষ\nশুধু শুরু\nসাধারণ সামনে সঙ্গে সঙ্গেও সব সবার সমস্ত সম্প্রতি সময় সহ সহিত সাথে সুতরাং সে  সেই সেখান সেখানে  সেটা সেটাই সেটাও সেটি স্পষ্ট স্বয়ং\nহইতে হইবে হইয়া হওয়া হওয়ায় হওয়ার হচ্ছে হত হতে হতেই হন হবে হবেন হয় হয়তো হয়নি হয়ে হয়েই হয়েছিল হয়েছে হাজার\nহয়েছেন হল হলে হলেই হলেও হলো হিসাবে হিসেবে হৈলে হোক হয় হয়ে হয়েছে হৈতে হইয়া  হয়েছিল হয়েছেন হয়নি হয়েই হয়তো হওয়া হওয়ার হওয়ায়\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/bn/tag_map.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/bn/punctuation.py----------------------------------------
A:spacy.lang.bn.punctuation._quotes->char_classes.CONCAT_QUOTES.replace("'", '')


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/bg/__init__.py----------------------------------------
A:spacy.lang.bg.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
spacy.lang.bg.__init__.Bulgarian(Language)
spacy.lang.bg.__init__.BulgarianDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/bg/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/bg/stop_words.py----------------------------------------
A:spacy.lang.bg.stop_words.STOP_WORDS->set('\nа\nавтентичен\nаз\nако\nала\nбе\nбез\nбеше\nби\nбивш\nбивша\nбившо\nбил\nбила\nбили\nбило\nблагодаря\nблизо\nбъдат\nбъде\nбяха\nв\nвас\nваш\nваша\nвероятно\nвече\nвзема\nви\nвие\nвинаги\nвнимава\nвреме\nвсе\nвсеки\nвсички\nвсичко\nвсяка\nвъв\nвъпреки\nвърху\nг\nги\nглавен\nглавна\nглавно\nглас\nго\nгодина\nгодини\nгодишен\nд\nда\nдали\nдва\nдвама\nдвамата\nдве\nдвете\nден\nднес\nдни\nдо\nдобра\nдобре\nдобро\nдобър\nдокато\nдокога\nдори\nдосега\nдоста\nдруг\nдруга\nдруги\nе\nевтин\nедва\nедин\nедна\nеднаква\nеднакви\nеднакъв\nедно\nекип\nето\nживот\nза\nзабавям\nзад\nзаедно\nзаради\nзасега\nзаспал\nзатова\nзащо\nзащото\nи\nиз\nили\nим\nима\nимат\nиска\nй\nказа\nкак\nкаква\nкакво\nкакто\nкакъв\nкато\nкога\nкогато\nкоето\nкоито\nкой\nкойто\nколко\nкоято\nкъде\nкъдето\nкъм\nлесен\nлесно\nли\nлош\nм\nмай\nмалко\nме\nмежду\nмек\nмен\nмесец\nми\nмного\nмнозина\nмога\nмогат\nможе\nмокър\nмоля\nмомента\nму\nн\nна\nнад\nназад\nнай\nнаправи\nнапред\nнапример\nнас\nне\nнего\nнещо\nнея\nни\nние\nникой\nнито\nнищо\nно\nнов\nнова\nнови\nновина\nнякои\nнякой\nняколко\nняма\nобаче\nоколо\nосвен\nособено\nот\nотгоре\nотново\nоще\nпак\nпо\nповече\nповечето\nпод\nпоне\nпоради\nпосле\nпочти\nправи\nпред\nпреди\nпрез\nпри\nпък\nпървата\nпърви\nпърво\nпъти\nравен\nравна\nс\nса\nсам\nсамо\nсе\nсега\nси\nсин\nскоро\nслед\nследващ\nсме\nсмях\nспоред\nсред\nсрещу\nсте\nсъм\nсъс\nсъщо\nт\nтази\nтака\nтакива\nтакъв\nтам\nтвой\nте\nтези\nти\nт.н.\nто\nтова\nтогава\nтози\nтой\nтолкова\nточно\nтри\nтрябва\nтук\nтъй\nтя\nтях\nу\nутре\nхаресва\nхиляди\nч\nчаса\nче\nчесто\nчрез\nще\nщом\nюмрук\nя\nяк\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/lt/__init__.py----------------------------------------
A:spacy.lang.lt.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.lt.__init__.lex_attr_getters[NORM]->add_lookups(Language.Defaults.lex_attr_getters[NORM], BASE_NORMS)
A:spacy.lang.lt.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS, TOKENIZER_EXCEPTIONS)
spacy.lang.lt.__init__.Lithuanian(Language)
spacy.lang.lt.__init__.LithuanianDefaults(Language.Defaults)
spacy.lang.lt.__init__._return_lt(_)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/lt/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/lt/lex_attrs.py----------------------------------------
A:spacy.lang.lt.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.lt.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.lt.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/lt/tokenizer_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/lt/morph_rules.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/lt/stop_words.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/lt/tag_map.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/da/__init__.py----------------------------------------
A:spacy.lang.da.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.da.__init__.lex_attr_getters[NORM]->add_lookups(Language.Defaults.lex_attr_getters[NORM], BASE_NORMS, NORM_EXCEPTIONS)
A:spacy.lang.da.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS, TOKENIZER_EXCEPTIONS)
spacy.lang.da.__init__.Danish(Language)
spacy.lang.da.__init__.DanishDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/da/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/da/lex_attrs.py----------------------------------------
A:spacy.lang.da.lex_attrs._num_words->'nul\nen et to tre fire fem seks syv otte ni ti\nelleve tolv tretten fjorten femten seksten sytten atten nitten tyve\nenogtyve toogtyve treogtyve fireogtyve femogtyve seksogtyve syvogtyve otteogtyve niogtyve tredive\nenogtredive toogtredive treogtredive fireogtredive femogtredive seksogtredive syvogtredive otteogtredive niogtredive fyrre\nenogfyrre toogfyrre treogfyrre fireogfyrre femgogfyrre seksogfyrre syvogfyrre otteogfyrre niogfyrre halvtreds\nenoghalvtreds tooghalvtreds treoghalvtreds fireoghalvtreds femoghalvtreds seksoghalvtreds syvoghalvtreds otteoghalvtreds nioghalvtreds tres\nenogtres toogtres treogtres fireogtres femogtres seksogtres syvogtres otteogtres niogtres halvfjerds\nenoghalvfjerds tooghalvfjerds treoghalvfjerds fireoghalvfjerds femoghalvfjerds seksoghalvfjerds syvoghalvfjerds otteoghalvfjerds nioghalvfjerds firs\nenogfirs toogfirs treogfirs fireogfirs femogfirs seksogfirs syvogfirs otteogfirs niogfirs halvfems\nenoghalvfems tooghalvfems treoghalvfems fireoghalvfems femoghalvfems seksoghalvfems syvoghalvfems otteoghalvfems nioghalvfems hundrede\nmillion milliard billion billiard trillion trilliard\n'.split()
A:spacy.lang.da.lex_attrs._ordinal_words->'nulte\nførste anden tredje fjerde femte sjette syvende ottende niende tiende\nelfte tolvte trettende fjortende femtende sekstende syttende attende nittende tyvende\nenogtyvende toogtyvende treogtyvende fireogtyvende femogtyvende seksogtyvende syvogtyvende otteogtyvende niogtyvende tredivte enogtredivte toogtredivte treogtredivte fireogtredivte femogtredivte seksogtredivte syvogtredivte otteogtredivte niogtredivte fyrretyvende\nenogfyrretyvende toogfyrretyvende treogfyrretyvende fireogfyrretyvende femogfyrretyvende seksogfyrretyvende syvogfyrretyvende otteogfyrretyvende niogfyrretyvende halvtredsindstyvende enoghalvtredsindstyvende\ntooghalvtredsindstyvende treoghalvtredsindstyvende fireoghalvtredsindstyvende femoghalvtredsindstyvende seksoghalvtredsindstyvende syvoghalvtredsindstyvende otteoghalvtredsindstyvende nioghalvtredsindstyvende\ntresindstyvende enogtresindstyvende toogtresindstyvende treogtresindstyvende fireogtresindstyvende femogtresindstyvende seksogtresindstyvende syvogtresindstyvende otteogtresindstyvende niogtresindstyvende halvfjerdsindstyvende\nenoghalvfjerdsindstyvende tooghalvfjerdsindstyvende treoghalvfjerdsindstyvende fireoghalvfjerdsindstyvende femoghalvfjerdsindstyvende seksoghalvfjerdsindstyvende syvoghalvfjerdsindstyvende otteoghalvfjerdsindstyvende nioghalvfjerdsindstyvende firsindstyvende\nenogfirsindstyvende toogfirsindstyvende treogfirsindstyvende fireogfirsindstyvende femogfirsindstyvende seksogfirsindstyvende syvogfirsindstyvende otteogfirsindstyvende niogfirsindstyvende halvfemsindstyvende\nenoghalvfemsindstyvende tooghalvfemsindstyvende treoghalvfemsindstyvende fireoghalvfemsindstyvende femoghalvfemsindstyvende seksoghalvfemsindstyvende syvoghalvfemsindstyvende otteoghalvfemsindstyvende nioghalvfemsindstyvende\n'.split()
A:spacy.lang.da.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.da.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.da.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/da/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.da.tokenizer_exceptions.capitalized->orth.capitalize()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/da/morph_rules.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/da/stop_words.py----------------------------------------
A:spacy.lang.da.stop_words.STOP_WORDS->set('\naf aldrig alene alle allerede alligevel alt altid anden andet andre at\n\nbag begge blandt blev blive bliver burde bør\n\nda de dem den denne dens der derefter deres derfor derfra deri dermed derpå derved det dette dig din dine disse dog du\n\nefter egen eller ellers en end endnu ene eneste enhver ens enten er et\n\nflere flest fleste for foran fordi forrige fra få før først\n\ngennem gjorde gjort god gør gøre gørende\n\nham han hans har havde have hel heller hen hende hendes henover her herefter heri hermed herpå hun hvad hvem hver hvilke hvilken hvilkes hvis hvor hvordan hvorefter hvorfor hvorfra hvorhen hvori hvorimod hvornår hvorved\n\ni igen igennem ikke imellem imens imod ind indtil ingen intet\n\njeg jer jeres jo\n\nkan kom kommer kun kunne\n\nlad langs lav lave lavet lidt lige ligesom lille længere\n\nman mange med meget mellem men mens mere mest mig min mindre mindst mine mit må måske\n\nned nemlig nogen nogensinde noget nogle nok nu ny nyt nær næste næsten\n\nog også om omkring op os over overalt\n\npå\n\nsamme sammen selv selvom senere ses siden sig sige skal skulle som stadig synes syntes så sådan således\n\ntemmelig tidligere til tilbage tit\n\nud uden udover under undtagen\n\nvar ved vi via vil ville vore vores vær være været\n\nøvrigt\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/da/norm_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/da/punctuation.py----------------------------------------
A:spacy.lang.da.punctuation._quotes->char_classes.CONCAT_QUOTES.replace("'", '')


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/uk/__init__.py----------------------------------------
A:spacy.lang.uk.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.uk.__init__.lex_attr_getters[NORM]->add_lookups(Language.Defaults.lex_attr_getters[NORM], BASE_NORMS)
A:spacy.lang.uk.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS, TOKENIZER_EXCEPTIONS)
A:spacy.lang.uk.__init__.lookups->Lookups()
spacy.lang.uk.__init__.Ukrainian(Language)
spacy.lang.uk.__init__.UkrainianDefaults(Language.Defaults)
spacy.lang.uk.__init__.UkrainianDefaults.create_lemmatizer(cls,nlp=None,lookups=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/uk/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/uk/lemmatizer.py----------------------------------------
A:spacy.lang.uk.lemmatizer.UkrainianLemmatizer._morph->MorphAnalyzer(lang='uk')
A:spacy.lang.uk.lemmatizer.univ_pos->self.normalize_univ_pos(univ_pos)
A:spacy.lang.uk.lemmatizer.analyses->self._morph.parse(string)
A:spacy.lang.uk.lemmatizer.(analysis_pos, _)->oc2ud(str(analysis.tag))
A:spacy.lang.uk.lemmatizer.(_, analysis_morph)->oc2ud(str(analysis.tag))
A:spacy.lang.uk.lemmatizer.morphology->dict()
A:spacy.lang.uk.lemmatizer.unmatched->set()
A:spacy.lang.uk.lemmatizer.grams->oc_tag.replace(' ', ',').split(',')
A:spacy.lang.uk.lemmatizer.gram->set().pop()
spacy.lang.uk.UkrainianLemmatizer(self,lookups=None)
spacy.lang.uk.UkrainianLemmatizer.lookup(self,string,orth=None)
spacy.lang.uk.UkrainianLemmatizer.normalize_univ_pos(univ_pos)
spacy.lang.uk.lemmatizer.UkrainianLemmatizer(self,lookups=None)
spacy.lang.uk.lemmatizer.UkrainianLemmatizer.__init__(self,lookups=None)
spacy.lang.uk.lemmatizer.UkrainianLemmatizer.lookup(self,string,orth=None)
spacy.lang.uk.lemmatizer.UkrainianLemmatizer.normalize_univ_pos(univ_pos)
spacy.lang.uk.lemmatizer.oc2ud(oc_tag)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/uk/lex_attrs.py----------------------------------------
A:spacy.lang.uk.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.uk.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.uk.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/uk/tokenizer_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/uk/stop_words.py----------------------------------------
A:spacy.lang.uk.stop_words.STOP_WORDS->set("а\nабо\nадже\nаж\nале\nалло\nб\nбагато\nбез\nбезперервно\nби\nбільш\nбільше\nбіля\nблизько\nбо\nбув\nбуває\nбуде\nбудемо\nбудете\nбудеш\nбуду\nбудуть\nбудь\nбула\nбули\nбуло\nбути\nв\nвам\nвами\nвас\nваш\nваша\nваше\nвашим\nвашими\nваших\nваші\nвашій\nвашого\nвашої\nвашому\nвашою\nвашу\nвгорі\nвгору\nвдалині\nвесь\nвже\nви\nвід\nвідсотків\nвін\nвісім\nвісімнадцятий\nвісімнадцять\nвниз\nвнизу\nвона\nвони\nвоно\nвосьмий\nвсе\nвсею\nвсі\nвсім\nвсіх\nвсього\nвсьому\nвсю\nвся\nвтім\nг\nгеть\nговорив\nговорить\nдавно\nдалеко\nдалі\nдарма\nдва\nдвадцятий\nдвадцять\nдванадцятий\nдванадцять\nдві\nдвох\nде\nдев'ятий\nдев'ятнадцятий\nдев'ятнадцять\nдев'ять\nдекілька\nдень\nдесятий\nдесять\nдійсно\nдля\nдня\nдо\nдобре\nдовго\nдоки\nдосить\nдругий\nдуже\nдякую\nе\nє\nж\nже\nз\nза\nзавжди\nзазвичай\nзанадто\nзараз\nзате\nзвичайно\nзвідси\nзвідусіль\nздається\nзі\nзначить\nзнову\nзовсім\nі\nіз\nїї\nїй\nїм\nіноді\nінша\nінше\nінший\nінших\nінші\nїх\nй\nйого\nйому\nкаже\nким\nкілька\nкого\nкожен\nкожна\nкожне\nкожні\nколи\nкому\nкраще\nкрім\nкуди\nласка\nледве\nлише\nм\nмає\nмайже\nмало\nмати\nмене\nмені\nменш\nменше\nми\nмимо\nміг\nміж\nмій\nмільйонів\nмною\nмого\nмогти\nмоє\nмоєї\nмоєму\nмоєю\nможе\nможна\nможно\nможуть\nмої\nмоїй\nмоїм\nмоїми\nмоїх\nмою\nмоя\nна\nнавіть\nнавіщо\nнавколо\nнавкруги\nнагорі\nнад\nназад\nнайбільш\nнам\nнами\nнарешті\nнас\nнаш\nнаша\nнаше\nнашим\nнашими\nнаших\nнаші\nнашій\nнашого\nнашої\nнашому\nнашою\nнашу\nне\nнебагато\nнебудь\nнедалеко\nнеї\nнемає\nнерідко\nнещодавно\nнею\nнибудь\nнижче\nнизько\nним\nними\nних\nні\nніби\nніж\nній\nніколи\nнікуди\nнім\nнічого\nну\nнього\nньому\nо\nобидва\nобоє\nодин\nодинадцятий\nодинадцять\nоднак\nоднієї\nодній\nодного\nозначає\nокрім\nон\nособливо\nось\nп'ятий\nп'ятнадцятий\nп'ятнадцять\nп'ять\nперед\nперший\nпід\nпізніше\nпір\nпісля\nпо\nповинно\nподів\nпоки\nпора\nпоруч\nпосеред\nпотім\nпотрібно\nпочала\nпочатку\nпри\nпро\nпросто\nпроте\nпроти\nраз\nразу\nраніше\nрано\nраптом\nрік\nроки\nроків\nроку\nроці\nсам\nсама\nсаме\nсамим\nсамими\nсамих\nсамі\nсамій\nсамо\nсамого\nсамому\nсаму\nсвого\nсвоє\nсвоєї\nсвої\nсвоїй\nсвоїх\nсвою\nсебе\nсих\nсім\nсімнадцятий\nсімнадцять\nсказав\nсказала\nсказати\nскільки\nскрізь\nсобі\nсобою\nспасибі\nспочатку\nсправ\nстав\nсуть\nсьогодні\nсьомий\nт\nта\nтак\nтака\nтаке\nтакий\nтакі\nтакож\nтам\nтвій\nтвого\nтвоє\nтвоєї\nтвоєму\nтвоєю\nтвої\nтвоїй\nтвоїм\nтвоїми\nтвоїх\nтвою\nтвоя\nте\nтебе\nтеж\nтепер\nти\nтим\nтими\nтисяч\nтих\nті\nтієї\nтією\nтій\nтільки\nтім\nто\nтобі\nтобою\nтого\nтоді\nтой\nтому\nтою\nтреба\nтретій\nтри\nтринадцятий\nтринадцять\nтрохи\nту\nтуди\nтут\nу\nувесь\nуміти\nусе\nусі\nусім\nусіма\nусіх\nусього\nусьому\nусю\nусюди\nуся\nхіба\nхотіти\nхоч\nхоча\nхочеш\nхто\nце\nцей\nцим\nцими\nцих\nці\nцієї\nцій\nцього\nцьому\nцю\nця\nчас\nчастіше\nчасто\nчасу\nчерез\nчетвертий\nчи\nчиє\nчиєї\nчиєму\nчиї\nчиїй\nчиїм\nчиїми\nчиїх\nчий\nчийого\nчийому\nчим\nчисленна\nчисленне\nчисленний\nчисленні\nчию\nчия\nчого\nчому\nчотири\nчотирнадцятий\nчотирнадцять\nшістнадцятий\nшістнадцять\nшість\nшостий\nще\nщо\nщоб\nщодо\nщось\nя\nяк\nяка\nякий\nяких\nякі\nякій\nякого\nякої\nякщо".split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/uk/tag_map.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/hi/__init__.py----------------------------------------
A:spacy.lang.hi.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
spacy.lang.hi.__init__.Hindi(Language)
spacy.lang.hi.__init__.HindiDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/hi/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/hi/lex_attrs.py----------------------------------------
A:spacy.lang.hi.lex_attrs.length->len(suffix_group[0])
A:spacy.lang.hi.lex_attrs.text->text.replace(', ', '').replace('.', '').replace(', ', '').replace('.', '')
A:spacy.lang.hi.lex_attrs.(num, denom)->text.replace(', ', '').replace('.', '').replace(', ', '').replace('.', '').split('/')
spacy.lang.hi.lex_attrs.like_num(text)
spacy.lang.hi.lex_attrs.norm(string)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/hi/stop_words.py----------------------------------------
A:spacy.lang.hi.stop_words.STOP_WORDS->set('\nअंदर\nअत\nअदि\nअप\nअपना\nअपनि\nअपनी\nअपने\nअभि\nअभी\nअंदर\nआदि\nआप\nअगर\nइंहिं\nइंहें\nइंहों\nइतयादि\nइत्यादि\nइन\nइनका\nइन्हीं\nइन्हें\nइन्हों\nइस\nइसका\nइसकि\nइसकी\nइसके\nइसमें\nइसि\nइसी\nइसे\nउंहिं\nउंहें\nउंहों\nउन\nउनका\nउनकि\nउनकी\nउनके\nउनको\nउन्हीं\nउन्हें\nउन्हों\nउस\nउसके\nउसि\nउसी\nउसे\nएक\nएवं\nएस\nएसे\nऐसे\nओर\nऔर\nकइ\nकई\nकर\nकरता\nकरते\nकरना\nकरने\nकरें\nकहते\nकहा\nका\nकाफि\nकाफ़ी\nकि\nकिंहें\nकिंहों\nकितना\nकिन्हें\nकिन्हों\nकिया\nकिर\nकिस\nकिसि\nकिसी\nकिसे\nकी\nकुछ\nकुल\nके\nको\nकोइ\nकोई\nकोन\nकोनसा\nकौन\nकौनसा\nगया\nघर\nजब\nजहाँ\nजहां\nजा\nजिंहें\nजिंहों\nजितना\nजिधर\nजिन\nजिन्हें\nजिन्हों\nजिस\nजिसे\nजीधर\nजेसा\nजेसे\nजैसा\nजैसे\nजो\nतक\nतब\nतरह\nतिंहें\nतिंहों\nतिन\nतिन्हें\nतिन्हों\nतिस\nतिसे\nतो\nथा\nथि\nथी\nथे\nदबारा\nदवारा\nदिया\nदुसरा\nदुसरे\nदूसरे\nदो\nद्वारा\nन\nनहिं\nनहीं\nना\nनिचे\nनिहायत\nनीचे\nने\nपर\nपहले\nपुरा\nपूरा\nपे\nफिर\nबनि\nबनी\nबहि\nबही\nबहुत\nबाद\nबाला\nबिलकुल\nभि\nभितर\nभी\nभीतर\nमगर\nमानो\nमे\nमें\nमैं\nमुझको\nमेरा\nयदि\nयह\nयहाँ\nयहां\nयहि\nयही\nया\nयिह\nये\nरखें\nरवासा\nरहा\nरहे\nऱ्वासा\nलिए\nलिये\nलेकिन\nव\nवगेरह\nवग़ैरह\nवरग\nवर्ग\nवह\nवहाँ\nवहां\nवहिं\nवहीं\nवाले\nवुह\nवे\nवग़ैरह\nसंग\nसकता\nसकते\nसबसे\nसभि\nसभी\nसाथ\nसाबुत\nसाभ\nसारा\nसे\nसो\nसंग\nहि\nही\nहुअ\nहुआ\nहुइ\nहुई\nहुए\nहे\nहें\nहै\nहैं\nहो\nहूँ\nहोता\nहोति\nहोती\nहोते\nहोना\nहोने\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ru/__init__.py----------------------------------------
A:spacy.lang.ru.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.ru.__init__.lex_attr_getters[NORM]->add_lookups(Language.Defaults.lex_attr_getters[NORM], BASE_NORMS, NORM_EXCEPTIONS)
A:spacy.lang.ru.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS, TOKENIZER_EXCEPTIONS)
A:spacy.lang.ru.__init__.lookups->Lookups()
spacy.lang.ru.__init__.Russian(Language)
spacy.lang.ru.__init__.RussianDefaults(Language.Defaults)
spacy.lang.ru.__init__.RussianDefaults.create_lemmatizer(cls,nlp=None,lookups=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ru/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ru/lemmatizer.py----------------------------------------
A:spacy.lang.ru.lemmatizer.RussianLemmatizer._morph->MorphAnalyzer()
A:spacy.lang.ru.lemmatizer.univ_pos->self.normalize_univ_pos(univ_pos)
A:spacy.lang.ru.lemmatizer.analyses->self._morph.parse(string)
A:spacy.lang.ru.lemmatizer.(analysis_pos, _)->oc2ud(str(analysis.tag))
A:spacy.lang.ru.lemmatizer.(_, analysis_morph)->oc2ud(str(analysis.tag))
A:spacy.lang.ru.lemmatizer.morphology->dict()
A:spacy.lang.ru.lemmatizer.unmatched->set()
A:spacy.lang.ru.lemmatizer.grams->oc_tag.replace(' ', ',').split(',')
A:spacy.lang.ru.lemmatizer.gram->set().pop()
spacy.lang.ru.RussianLemmatizer(self,lookups=None)
spacy.lang.ru.RussianLemmatizer.lookup(self,string,orth=None)
spacy.lang.ru.RussianLemmatizer.normalize_univ_pos(univ_pos)
spacy.lang.ru.lemmatizer.RussianLemmatizer(self,lookups=None)
spacy.lang.ru.lemmatizer.RussianLemmatizer.__init__(self,lookups=None)
spacy.lang.ru.lemmatizer.RussianLemmatizer.lookup(self,string,orth=None)
spacy.lang.ru.lemmatizer.RussianLemmatizer.normalize_univ_pos(univ_pos)
spacy.lang.ru.lemmatizer.oc2ud(oc_tag)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ru/lex_attrs.py----------------------------------------
A:spacy.lang.ru.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.ru.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.ru.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ru/tokenizer_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ru/stop_words.py----------------------------------------
A:spacy.lang.ru.stop_words.STOP_WORDS->set('\nа\n\nбудем будет будете будешь буду будут будучи будь будьте бы был была были было\nбыть\n\nв вам вами вас весь во вот все всё всего всей всем всём всеми всему всех всею\nвсея всю вся вы\n\nда для до\n\nего едим едят ее её ей ел ела ем ему емъ если ест есть ешь еще ещё ею\n\nже\n\nза\n\nи из или им ими имъ их\n\nк как кем ко когда кого ком кому комья которая которого которое которой котором\nкоторому которою которую которые который которым которыми которых кто\n\nменя мне мной мною мог моги могите могла могли могло могу могут мое моё моего\nмоей моем моём моему моею можем может можете можешь мои мой моим моими моих\nмочь мою моя мы\n\nна нам нами нас наса наш наша наше нашего нашей нашем нашему нашею наши нашим\nнашими наших нашу не него нее неё ней нем нём нему нет нею ним ними них но\n\nо об один одна одни одним одними одних одно одного одной одном одному одною\nодну он она оне они оно от\n\nпо при\n\nс сам сама сами самим самими самих само самого самом самому саму свое своё\nсвоего своей своем своём своему своею свои свой своим своими своих свою своя\nсебе себя собой собою\n\nта так такая такие таким такими таких такого такое такой таком такому такою\nтакую те тебе тебя тем теми тех то тобой тобою того той только том томах тому\nтот тою ту ты\n\nу уже\n\nчего чем чём чему что чтобы\n\nэта эти этим этими этих это этого этой этом этому этот этою эту\n\nя\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ru/norm_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ru/tag_map.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/zh/__init__.py----------------------------------------
A:spacy.lang.zh.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.zh.__init__.words->list(jieba.cut(text, cut_all=False))
A:spacy.lang.zh.__init__.spaces[-1]->bool(token.whitespace_)
spacy.lang.zh.__init__.Chinese(Language)
spacy.lang.zh.__init__.Chinese.make_doc(self,text)
spacy.lang.zh.__init__.ChineseDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/zh/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/zh/lex_attrs.py----------------------------------------
A:spacy.lang.zh.lex_attrs.text->text.replace(',', '').replace('.', '').replace('，', '').replace('。', '').replace(',', '').replace('.', '').replace('，', '').replace('。', '')
A:spacy.lang.zh.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace('，', '').replace('。', '').replace(',', '').replace('.', '').replace('，', '').replace('。', '').split('/')
spacy.lang.zh.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/zh/stop_words.py----------------------------------------
A:spacy.lang.zh.stop_words.STOP_WORDS->set('\n!\n"\n#\n$\n%\n&\n\'\n(\n)\n*\n+\n,\n-\n--\n.\n..\n...\n......\n...................\n./\n.一\n.数\n.日\n/\n//\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n:\n://\n::\n;\n<\n=\n>\n>>\n?\n@\nA\nLex\n[\n]\n^\n_\n`\nexp\nsub\nsup\n|\n}\n~\n~~~~\n·\n×\n×××\nΔ\nΨ\nγ\nμ\nφ\nφ．\nВ\n—\n——\n———\n‘\n’\n’‘\n“\n”\n”，\n…\n……\n…………………………………………………③\n′∈\n′｜\n℃\nⅢ\n↑\n→\n∈［\n∪φ∈\n≈\n①\n②\n②ｃ\n③\n③］\n④\n⑤\n⑥\n⑦\n⑧\n⑨\n⑩\n──\n■\n▲\n\u3000\n、\n。\n〈\n〉\n《\n》\n》），\n」\n『\n』\n【\n】\n〔\n〕\n〕〔\n㈧\n一\n一.\n一一\n一下\n一个\n一些\n一何\n一切\n一则\n一则通过\n一天\n一定\n一方面\n一旦\n一时\n一来\n一样\n一次\n一片\n一番\n一直\n一致\n一般\n一起\n一转眼\n一边\n一面\n七\n万一\n三\n三天两头\n三番两次\n三番五次\n上\n上下\n上升\n上去\n上来\n上述\n上面\n下\n下列\n下去\n下来\n下面\n不\n不一\n不下\n不久\n不了\n不亦乐乎\n不仅\n不仅...而且\n不仅仅\n不仅仅是\n不会\n不但\n不但...而且\n不光\n不免\n不再\n不力\n不单\n不变\n不只\n不可\n不可开交\n不可抗拒\n不同\n不外\n不外乎\n不够\n不大\n不如\n不妨\n不定\n不对\n不少\n不尽\n不尽然\n不巧\n不已\n不常\n不得\n不得不\n不得了\n不得已\n不必\n不怎么\n不怕\n不惟\n不成\n不拘\n不择手段\n不敢\n不料\n不断\n不日\n不时\n不是\n不曾\n不止\n不止一次\n不比\n不消\n不满\n不然\n不然的话\n不特\n不独\n不由得\n不知不觉\n不管\n不管怎样\n不经意\n不胜\n不能\n不能不\n不至于\n不若\n不要\n不论\n不起\n不足\n不过\n不迭\n不问\n不限\n与\n与其\n与其说\n与否\n与此同时\n专门\n且\n且不说\n且说\n两者\n严格\n严重\n个\n个人\n个别\n中小\n中间\n丰富\n串行\n临\n临到\n为\n为主\n为了\n为什么\n为什麽\n为何\n为止\n为此\n为着\n主张\n主要\n举凡\n举行\n乃\n乃至\n乃至于\n么\n之\n之一\n之前\n之后\n之後\n之所以\n之类\n乌乎\n乎\n乒\n乘\n乘势\n乘机\n乘胜\n乘虚\n乘隙\n九\n也\n也好\n也就是说\n也是\n也罢\n了\n了解\n争取\n二\n二来\n二话不说\n二话没说\n于\n于是\n于是乎\n云云\n云尔\n互\n互相\n五\n些\n交口\n亦\n产生\n亲口\n亲手\n亲眼\n亲自\n亲身\n人\n人人\n人们\n人家\n人民\n什么\n什么样\n什麽\n仅\n仅仅\n今\n今后\n今天\n今年\n今後\n介于\n仍\n仍旧\n仍然\n从\n从不\n从严\n从中\n从事\n从今以后\n从优\n从古到今\n从古至今\n从头\n从宽\n从小\n从新\n从无到有\n从早到晚\n从未\n从来\n从此\n从此以后\n从而\n从轻\n从速\n从重\n他\n他人\n他们\n他是\n他的\n代替\n以\n以上\n以下\n以为\n以便\n以免\n以前\n以及\n以后\n以外\n以後\n以故\n以期\n以来\n以至\n以至于\n以致\n们\n任\n任何\n任凭\n任务\n企图\n伙同\n会\n伟大\n传\n传说\n传闻\n似乎\n似的\n但\n但凡\n但愿\n但是\n何\n何乐而不为\n何以\n何况\n何处\n何妨\n何尝\n何必\n何时\n何止\n何苦\n何须\n余外\n作为\n你\n你们\n你是\n你的\n使\n使得\n使用\n例如\n依\n依据\n依照\n依靠\n便\n便于\n促进\n保持\n保管\n保险\n俺\n俺们\n倍加\n倍感\n倒不如\n倒不如说\n倒是\n倘\n倘使\n倘或\n倘然\n倘若\n借\n借以\n借此\n假使\n假如\n假若\n偏偏\n做到\n偶尔\n偶而\n傥然\n像\n儿\n允许\n元／吨\n充其极\n充其量\n充分\n先不先\n先后\n先後\n先生\n光\n光是\n全体\n全力\n全年\n全然\n全身心\n全部\n全都\n全面\n八\n八成\n公然\n六\n兮\n共\n共同\n共总\n关于\n其\n其一\n其中\n其二\n其他\n其余\n其后\n其它\n其实\n其次\n具体\n具体地说\n具体来说\n具体说来\n具有\n兼之\n内\n再\n再其次\n再则\n再有\n再次\n再者\n再者说\n再说\n冒\n冲\n决不\n决定\n决非\n况且\n准备\n凑巧\n凝神\n几\n几乎\n几度\n几时\n几番\n几经\n凡\n凡是\n凭\n凭借\n出\n出于\n出去\n出来\n出现\n分别\n分头\n分期\n分期分批\n切\n切不可\n切切\n切勿\n切莫\n则\n则甚\n刚\n刚好\n刚巧\n刚才\n初\n别\n别人\n别处\n别是\n别的\n别管\n别说\n到\n到了儿\n到处\n到头\n到头来\n到底\n到目前为止\n前后\n前此\n前者\n前进\n前面\n加上\n加之\n加以\n加入\n加强\n动不动\n动辄\n勃然\n匆匆\n十分\n千\n千万\n千万千万\n半\n单\n单单\n单纯\n即\n即令\n即使\n即便\n即刻\n即如\n即将\n即或\n即是说\n即若\n却\n却不\n历\n原来\n去\n又\n又及\n及\n及其\n及时\n及至\n双方\n反之\n反之亦然\n反之则\n反倒\n反倒是\n反应\n反手\n反映\n反而\n反过来\n反过来说\n取得\n取道\n受到\n变成\n古来\n另\n另一个\n另一方面\n另外\n另悉\n另方面\n另行\n只\n只当\n只怕\n只是\n只有\n只消\n只要\n只限\n叫\n叫做\n召开\n叮咚\n叮当\n可\n可以\n可好\n可是\n可能\n可见\n各\n各个\n各人\n各位\n各地\n各式\n各种\n各级\n各自\n合理\n同\n同一\n同时\n同样\n后\n后来\n后者\n后面\n向\n向使\n向着\n吓\n吗\n否则\n吧\n吧哒\n吱\n呀\n呃\n呆呆地\n呐\n呕\n呗\n呜\n呜呼\n呢\n周围\n呵\n呵呵\n呸\n呼哧\n呼啦\n咋\n和\n咚\n咦\n咧\n咱\n咱们\n咳\n哇\n哈\n哈哈\n哉\n哎\n哎呀\n哎哟\n哗\n哗啦\n哟\n哦\n哩\n哪\n哪个\n哪些\n哪儿\n哪天\n哪年\n哪怕\n哪样\n哪边\n哪里\n哼\n哼唷\n唉\n唯有\n啊\n啊呀\n啊哈\n啊哟\n啐\n啥\n啦\n啪达\n啷当\n喀\n喂\n喏\n喔唷\n喽\n嗡\n嗡嗡\n嗬\n嗯\n嗳\n嘎\n嘎嘎\n嘎登\n嘘\n嘛\n嘻\n嘿\n嘿嘿\n四\n因\n因为\n因了\n因此\n因着\n因而\n固\n固然\n在\n在下\n在于\n地\n均\n坚决\n坚持\n基于\n基本\n基本上\n处在\n处处\n处理\n复杂\n多\n多么\n多亏\n多多\n多多少少\n多多益善\n多少\n多年前\n多年来\n多数\n多次\n够瞧的\n大\n大不了\n大举\n大事\n大体\n大体上\n大凡\n大力\n大多\n大多数\n大大\n大家\n大张旗鼓\n大批\n大抵\n大概\n大略\n大约\n大致\n大都\n大量\n大面儿上\n失去\n奇\n奈\n奋勇\n她\n她们\n她是\n她的\n好\n好在\n好的\n好象\n如\n如上\n如上所述\n如下\n如今\n如何\n如其\n如前所述\n如同\n如常\n如是\n如期\n如果\n如次\n如此\n如此等等\n如若\n始而\n姑且\n存在\n存心\n孰料\n孰知\n宁\n宁可\n宁愿\n宁肯\n它\n它们\n它们的\n它是\n它的\n安全\n完全\n完成\n定\n实现\n实际\n宣布\n容易\n密切\n对\n对于\n对应\n对待\n对方\n对比\n将\n将才\n将要\n将近\n小\n少数\n尔\n尔后\n尔尔\n尔等\n尚且\n尤其\n就\n就地\n就是\n就是了\n就是说\n就此\n就算\n就要\n尽\n尽可能\n尽如人意\n尽心尽力\n尽心竭力\n尽快\n尽早\n尽然\n尽管\n尽管如此\n尽量\n局外\n居然\n届时\n属于\n屡\n屡屡\n屡次\n屡次三番\n岂\n岂但\n岂止\n岂非\n川流不息\n左右\n巨大\n巩固\n差一点\n差不多\n己\n已\n已矣\n已经\n巴\n巴巴\n带\n帮助\n常\n常常\n常言说\n常言说得好\n常言道\n平素\n年复一年\n并\n并不\n并不是\n并且\n并排\n并无\n并没\n并没有\n并肩\n并非\n广大\n广泛\n应当\n应用\n应该\n庶乎\n庶几\n开外\n开始\n开展\n引起\n弗\n弹指之间\n强烈\n强调\n归\n归根到底\n归根结底\n归齐\n当\n当下\n当中\n当儿\n当前\n当即\n当口儿\n当地\n当场\n当头\n当庭\n当时\n当然\n当真\n当着\n形成\n彻夜\n彻底\n彼\n彼时\n彼此\n往\n往往\n待\n待到\n很\n很多\n很少\n後来\n後面\n得\n得了\n得出\n得到\n得天独厚\n得起\n心里\n必\n必定\n必将\n必然\n必要\n必须\n快\n快要\n忽地\n忽然\n怎\n怎么\n怎么办\n怎么样\n怎奈\n怎样\n怎麽\n怕\n急匆匆\n怪\n怪不得\n总之\n总是\n总的来看\n总的来说\n总的说来\n总结\n总而言之\n恍然\n恐怕\n恰似\n恰好\n恰如\n恰巧\n恰恰\n恰恰相反\n恰逢\n您\n您们\n您是\n惟其\n惯常\n意思\n愤然\n愿意\n慢说\n成为\n成年\n成年累月\n成心\n我\n我们\n我是\n我的\n或\n或则\n或多或少\n或是\n或曰\n或者\n或许\n战斗\n截然\n截至\n所\n所以\n所在\n所幸\n所有\n所谓\n才\n才能\n扑通\n打\n打从\n打开天窗说亮话\n扩大\n把\n抑或\n抽冷子\n拦腰\n拿\n按\n按时\n按期\n按照\n按理\n按说\n挨个\n挨家挨户\n挨次\n挨着\n挨门挨户\n挨门逐户\n换句话说\n换言之\n据\n据实\n据悉\n据我所知\n据此\n据称\n据说\n掌握\n接下来\n接着\n接著\n接连不断\n放量\n故\n故意\n故此\n故而\n敞开儿\n敢\n敢于\n敢情\n数/\n整个\n断然\n方\n方便\n方才\n方能\n方面\n旁人\n无\n无宁\n无法\n无论\n既\n既...又\n既往\n既是\n既然\n日复一日\n日渐\n日益\n日臻\n日见\n时候\n昂然\n明显\n明确\n是\n是不是\n是以\n是否\n是的\n显然\n显著\n普通\n普遍\n暗中\n暗地里\n暗自\n更\n更为\n更加\n更进一步\n曾\n曾经\n替\n替代\n最\n最后\n最大\n最好\n最後\n最近\n最高\n有\n有些\n有关\n有利\n有力\n有及\n有所\n有效\n有时\n有点\n有的\n有的是\n有着\n有著\n望\n朝\n朝着\n末##末\n本\n本人\n本地\n本着\n本身\n权时\n来\n来不及\n来得及\n来看\n来着\n来自\n来讲\n来说\n极\n极为\n极了\n极其\n极力\n极大\n极度\n极端\n构成\n果然\n果真\n某\n某个\n某些\n某某\n根据\n根本\n格外\n梆\n概\n次第\n欢迎\n欤\n正值\n正在\n正如\n正巧\n正常\n正是\n此\n此中\n此后\n此地\n此处\n此外\n此时\n此次\n此间\n殆\n毋宁\n每\n每个\n每天\n每年\n每当\n每时每刻\n每每\n每逢\n比\n比及\n比如\n比如说\n比方\n比照\n比起\n比较\n毕竟\n毫不\n毫无\n毫无例外\n毫无保留地\n汝\n沙沙\n没\n没奈何\n没有\n沿\n沿着\n注意\n活\n深入\n清楚\n满\n满足\n漫说\n焉\n然\n然则\n然后\n然後\n然而\n照\n照着\n牢牢\n特别是\n特殊\n特点\n犹且\n犹自\n独\n独自\n猛然\n猛然间\n率尔\n率然\n现代\n现在\n理应\n理当\n理该\n瑟瑟\n甚且\n甚么\n甚或\n甚而\n甚至\n甚至于\n用\n用来\n甫\n甭\n由\n由于\n由是\n由此\n由此可见\n略\n略为\n略加\n略微\n白\n白白\n的\n的确\n的话\n皆可\n目前\n直到\n直接\n相似\n相信\n相反\n相同\n相对\n相对而言\n相应\n相当\n相等\n省得\n看\n看上去\n看出\n看到\n看来\n看样子\n看看\n看见\n看起来\n真是\n真正\n眨眼\n着\n着呢\n矣\n矣乎\n矣哉\n知道\n砰\n确定\n碰巧\n社会主义\n离\n种\n积极\n移动\n究竟\n穷年累月\n突出\n突然\n窃\n立\n立刻\n立即\n立地\n立时\n立马\n竟\n竟然\n竟而\n第\n第二\n等\n等到\n等等\n策略地\n简直\n简而言之\n简言之\n管\n类如\n粗\n精光\n紧接着\n累年\n累次\n纯\n纯粹\n纵\n纵令\n纵使\n纵然\n练习\n组成\n经\n经常\n经过\n结合\n结果\n给\n绝\n绝不\n绝对\n绝非\n绝顶\n继之\n继后\n继续\n继而\n维持\n综上所述\n缕缕\n罢了\n老\n老大\n老是\n老老实实\n考虑\n者\n而\n而且\n而况\n而又\n而后\n而外\n而已\n而是\n而言\n而论\n联系\n联袂\n背地里\n背靠背\n能\n能否\n能够\n腾\n自\n自个儿\n自从\n自各儿\n自后\n自家\n自己\n自打\n自身\n臭\n至\n至于\n至今\n至若\n致\n般的\n良好\n若\n若夫\n若是\n若果\n若非\n范围\n莫\n莫不\n莫不然\n莫如\n莫若\n莫非\n获得\n藉以\n虽\n虽则\n虽然\n虽说\n蛮\n行为\n行动\n表明\n表示\n被\n要\n要不\n要不是\n要不然\n要么\n要是\n要求\n见\n规定\n觉得\n譬喻\n譬如\n认为\n认真\n认识\n让\n许多\n论\n论说\n设使\n设或\n设若\n诚如\n诚然\n话说\n该\n该当\n说明\n说来\n说说\n请勿\n诸\n诸位\n诸如\n谁\n谁人\n谁料\n谁知\n谨\n豁然\n贼死\n赖以\n赶\n赶快\n赶早不赶晚\n起\n起先\n起初\n起头\n起来\n起见\n起首\n趁\n趁便\n趁势\n趁早\n趁机\n趁热\n趁着\n越是\n距\n跟\n路经\n转动\n转变\n转贴\n轰然\n较\n较为\n较之\n较比\n边\n达到\n达旦\n迄\n迅速\n过\n过于\n过去\n过来\n运用\n近\n近几年来\n近年来\n近来\n还\n还是\n还有\n还要\n这\n这一来\n这个\n这么\n这么些\n这么样\n这么点儿\n这些\n这会儿\n这儿\n这就是说\n这时\n这样\n这次\n这点\n这种\n这般\n这边\n这里\n这麽\n进入\n进去\n进来\n进步\n进而\n进行\n连\n连同\n连声\n连日\n连日来\n连袂\n连连\n迟早\n迫于\n适应\n适当\n适用\n逐步\n逐渐\n通常\n通过\n造成\n逢\n遇到\n遭到\n遵循\n遵照\n避免\n那\n那个\n那么\n那么些\n那么样\n那些\n那会儿\n那儿\n那时\n那末\n那样\n那般\n那边\n那里\n那麽\n部分\n都\n鄙人\n采取\n里面\n重大\n重新\n重要\n鉴于\n针对\n长期以来\n长此下去\n长线\n长话短说\n问题\n间或\n防止\n阿\n附近\n陈年\n限制\n陡然\n除\n除了\n除却\n除去\n除外\n除开\n除此\n除此之外\n除此以外\n除此而外\n除非\n随\n随后\n随时\n随着\n随著\n隔夜\n隔日\n难得\n难怪\n难说\n难道\n难道说\n集中\n零\n需要\n非但\n非常\n非徒\n非得\n非特\n非独\n靠\n顶多\n顷\n顷刻\n顷刻之间\n顷刻间\n顺\n顺着\n顿时\n颇\n风雨无阻\n饱\n首先\n马上\n高低\n高兴\n默然\n默默地\n齐\n︿\n！\n＃\n＄\n％\n＆\n＇\n（\n）\n）÷（１－\n）、\n＊\n＋\n＋ξ\n＋＋\n，\n，也\n－\n－β\n－－\n－［＊］－\n．\n／\n０\n０：２\n１\n１．\n１２％\n２\n２．３％\n３\n４\n５\n５：０\n６\n７\n８\n９\n：\n；\n＜\n＜±\n＜Δ\n＜λ\n＜φ\n＜＜\n＝\n＝″\n＝☆\n＝（\n＝－\n＝［\n＝｛\n＞\n＞λ\n？\n＠\nＡ\nＬＩ\nＲ．Ｌ．\nＺＸＦＩＴＬ\n［\n［①①］\n［①②］\n［①③］\n［①④］\n［①⑤］\n［①⑥］\n［①⑦］\n［①⑧］\n［①⑨］\n［①Ａ］\n［①Ｂ］\n［①Ｃ］\n［①Ｄ］\n［①Ｅ］\n［①］\n［①ａ］\n［①ｃ］\n［①ｄ］\n［①ｅ］\n［①ｆ］\n［①ｇ］\n［①ｈ］\n［①ｉ］\n［①ｏ］\n［②\n［②①］\n［②②］\n［②③］\n［②④\n［②⑤］\n［②⑥］\n［②⑦］\n［②⑧］\n［②⑩］\n［②Ｂ］\n［②Ｇ］\n［②］\n［②ａ］\n［②ｂ］\n［②ｃ］\n［②ｄ］\n［②ｅ］\n［②ｆ］\n［②ｇ］\n［②ｈ］\n［②ｉ］\n［②ｊ］\n［③①］\n［③⑩］\n［③Ｆ］\n［③］\n［③ａ］\n［③ｂ］\n［③ｃ］\n［③ｄ］\n［③ｅ］\n［③ｇ］\n［③ｈ］\n［④］\n［④ａ］\n［④ｂ］\n［④ｃ］\n［④ｄ］\n［④ｅ］\n［⑤］\n［⑤］］\n［⑤ａ］\n［⑤ｂ］\n［⑤ｄ］\n［⑤ｅ］\n［⑤ｆ］\n［⑥］\n［⑦］\n［⑧］\n［⑨］\n［⑩］\n［＊］\n［－\n［］\n］\n］∧′＝［\n］［\n＿\nａ］\nｂ］\nｃ］\nｅ］\nｆ］\nｎｇ昉\n｛\n｛－\n｜\n｝\n｝＞\n～\n～±\n～＋\n￥\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/zh/tag_map.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ar/__init__.py----------------------------------------
A:spacy.lang.ar.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.ar.__init__.lex_attr_getters[NORM]->add_lookups(Language.Defaults.lex_attr_getters[NORM], BASE_NORMS)
A:spacy.lang.ar.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS, TOKENIZER_EXCEPTIONS)
spacy.lang.ar.__init__.Arabic(Language)
spacy.lang.ar.__init__.ArabicDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ar/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ar/lex_attrs.py----------------------------------------
A:spacy.lang.ar.lex_attrs._num_words->set('\nصفر\nواحد\nإثنان\nاثنان\nثلاثة\nثلاثه\nأربعة\nأربعه\nخمسة\nخمسه\nستة\nسته\nسبعة\nسبعه\nثمانية\nثمانيه\nتسعة\nتسعه\nﻋﺸﺮﺓ\nﻋﺸﺮه\nعشرون\nعشرين\nثلاثون\nثلاثين\nاربعون\nاربعين\nأربعون\nأربعين\nخمسون\nخمسين\nستون\nستين\nسبعون\nسبعين\nثمانون\nثمانين\nتسعون\nتسعين\nمائتين\nمائتان\nثلاثمائة\nخمسمائة\nسبعمائة\nالف\nآلاف\nملايين\nمليون\nمليار\nمليارات\n'.split())
A:spacy.lang.ar.lex_attrs._ordinal_words->set('\nاول\nأول\nحاد\nواحد\nثان\nثاني\nثالث\nرابع\nخامس\nسادس\nسابع\nثامن\nتاسع\nعاشر\n'.split())
A:spacy.lang.ar.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.ar.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.ar.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ar/tokenizer_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ar/stop_words.py----------------------------------------
A:spacy.lang.ar.stop_words.STOP_WORDS->set('\nمن\nنحو\nلعل\nبما\nبين\nوبين\nايضا\nوبينما\nتحت\nمثلا\nلدي\nعنه\nمع\nهي\nوهذا\nواذا\nهذان\nانه\nبينما\nأمسى\nوسوف\nولم\nلذلك\nإلى\nمنه\nمنها\nكما\nظل\nهنا\nبه\nكذلك\nاما\nهما\nبعد\nبينهم\nالتي\nأبو\nاذا\nبدلا\nلها\nأمام\nيلي\nحين\nضد\nالذي\nقد\nصار\nإذا\nمابرح\nقبل\nكل\nوليست\nالذين\nلهذا\nوثي\nانهم\nباللتي\nمافتئ\nولا\nبهذه\nبحيث\nكيف\nوله\nعلي\nبات\nلاسيما\nحتى\nوقد\nو\nأما\nفيها\nبهذا\nلذا\nحيث\nلقد\nإن\nفإن\nاول\nليت\nفاللتي\nولقد\nلسوف\nهذه\nولماذا\nمعه\nالحالي\nبإن\nحول\nفي\nعليه\nمايزال\nولعل\nأنه\nأضحى\nاي\nستكون\nلن\nأن\nضمن\nوعلى\nامسى\nالي\nذات\nولايزال\nذلك\nفقد\nهم\nأي\nعند\nابن\nأو\nفهو\nفانه\nسوف\nما\nآل\nكلا\nعنها\nوكذلك\nليست\nلم\nوأن\nماذا\nلو\nوهل\nاللتي\nولذا\nيمكن\nفيه\nالا\nعليها\nوبينهم\nيوم\nوبما\nلما\nفكان\nاضحى\nاصبح\nلهم\nبها\nاو\nالذى\nالى\nإلي\nقال\nوالتي\nلازال\nأصبح\nولهذا\nمثل\nوكانت\nلكنه\nبذلك\nهذا\nلماذا\nقالت\nفقط\nلكن\nمما\nوكل\nوان\nوأبو\nومن\nكان\nمازال\nهل\nبينهن\nهو\nوما\nعلى\nوهو\nلأن\nواللتي\nوالذي\nدون\nعن\nوايضا\nهناك\nبلا\nجدا\nثم\nمنذ\nاللذين\nلايزال\nبعض\nمساء\nتكون\nفلا\nبيننا\nلا\nولكن\nإذ\nوأثناء\nليس\nومع\nفيهم\nولسوف\nبل\nتلك\nأحد\nوهي\nوكان\nومنها\nوفي\nماانفك\nاليوم\nوماذا\nهؤلاء\nوليس\nله\nأثناء\nبد\nاليه\nكأن\nاليها\nبتلك\nيكون\nولما\nهن\nوالى\nكانت\nوقبل\nان\nلدى\nإذما\nإذن\nأف\nأقل\nأكثر\nألا\nإلا\nاللاتي\nاللائي\nاللتان\nاللتيا\nاللتين\nاللذان\nاللواتي\nإليك\nإليكم\nإليكما\nإليكن\nأم\nأما\nإما\nإنا\nأنا\nأنت\nأنتم\nأنتما\nأنتن\nإنما\nإنه\nأنى\nأنى\nآه\nآها\nأولاء\nأولئك\nأوه\nآي\nأيها\nإي\nأين\nأين\nأينما\nإيه\nبخ\nبس\nبك\nبكم\nبكم\nبكما\nبكن\nبلى\nبماذا\nبمن\nبنا\nبهم\nبهما\nبهن\nبي\nبيد\nتلكم\nتلكما\nته\nتي\nتين\nتينك\nثمة\nحاشا\nحبذا\nحيثما\nخلا\nذا\nذاك\nذان\nذانك\nذلكم\nذلكما\nذلكن\nذه\nذو\nذوا\nذواتا\nذواتي\nذي\nذين\nذينك\nريث\nسوى\nشتان\nعدا\nعسى\nعل\nعليك\nعما\nغير\nفإذا\nفمن\nفيم\nفيما\nكأنما\nكأي\nكأين\nكذا\nكلاهما\nكلتا\nكلما\nكليكما\nكليهما\nكم\nكم\nكي\nكيت\nكيفما\nلست\nلستم\nلستما\nلستن\nلسن\nلسنا\nلك\nلكم\nلكما\nلكنما\nلكي\nلكيلا\nلنا\nلهما\nلهن\nلولا\nلوما\nلي\nلئن\nليسا\nليستا\nليسوا\nمتى\nمذ\nممن\nمه\nمهما\nنحن\nنعم\nها\nهاتان\nهاته\nهاتي\nهاتين\nهاك\nهاهنا\nهذي\nهذين\nهكذا\nهلا\nهنالك\nهيا\nهيت\nهيهات\nوالذين\nوإذ\nوإذا\nوإن\nولو\nيا\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ar/punctuation.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ja/__init__.py----------------------------------------
A:spacy.lang.ja.__init__.ShortUnitWord->namedtuple('ShortUnitWord', ['surface', 'lemma', 'pos'])
A:spacy.lang.ja.__init__.node->tokenizer.parseToNode(text)
A:spacy.lang.ja.__init__.parts->tokenizer.parseToNode(text).feature.split(',')
A:spacy.lang.ja.__init__.pos->','.join(parts[0:4])
A:spacy.lang.ja.__init__.self.tokenizer->try_mecab_import().Tagger()
A:spacy.lang.ja.__init__.(dtokens, spaces)->detailed_tokens(self.tokenizer, text)
A:spacy.lang.ja.__init__.doc->Doc(self.vocab, words=words, spaces=spaces)
A:spacy.lang.ja.__init__.token.tag_->resolve_pos(dtoken)
A:spacy.lang.ja.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
spacy.lang.ja.__init__.Japanese(Language)
spacy.lang.ja.__init__.Japanese.make_doc(self,text)
spacy.lang.ja.__init__.JapaneseDefaults(Language.Defaults)
spacy.lang.ja.__init__.JapaneseDefaults.create_tokenizer(cls,nlp=None)
spacy.lang.ja.__init__.JapaneseTokenizer(self,cls,nlp=None)
spacy.lang.ja.__init__.JapaneseTokenizer.__init__(self,cls,nlp=None)
spacy.lang.ja.__init__.detailed_tokens(tokenizer,text)
spacy.lang.ja.__init__.pickle_japanese(instance)
spacy.lang.ja.__init__.resolve_pos(token)
spacy.lang.ja.__init__.try_mecab_import()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ja/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ja/stop_words.py----------------------------------------
A:spacy.lang.ja.stop_words.STOP_WORDS->set('\nあ あっ あまり あり ある あるいは あれ\nい いい いう いく いずれ いっ いつ いる いわ\nうち\nえ\nお おい おけ および おら おり\nか かけ かつ かつて かなり から が\nき きっかけ\nくる くん\nこ こう ここ こと この これ ご ごと\nさ さらに さん\nし しか しかし しまう しまっ しよう\nす すぐ すべて する ず\nせ せい せる\nそう そこ そして その それ それぞれ\nた たい ただし たち ため たら たり だ だけ だっ\nち ちゃん\nつ つい つけ つつ\nて で でき できる です\nと とき ところ とっ とも どう\nな ない なお なかっ ながら なく なけれ なし なっ など なら なり なる\nに にて\nぬ\nね\nの のち のみ\nは はじめ ば\nひと\nぶり\nへ べき\nほか ほとんど ほど ほぼ\nま ます また まで まま\nみ\nも もう もっ もと もの\nや やっ\nよ よう よく よっ より よる よれ\nら らしい られ られる\nる\nれ れる\nを\nん\n一\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ja/tag_map.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/lv/__init__.py----------------------------------------
A:spacy.lang.lv.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
spacy.lang.lv.__init__.Latvian(Language)
spacy.lang.lv.__init__.LatvianDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/lv/stop_words.py----------------------------------------
A:spacy.lang.lv.stop_words.STOP_WORDS->set('\naiz\nap\napakš\napakšpus\nar\narī\naugšpus\nbet\nbez\nbija\nbiji\nbiju\nbijām\nbijāt\nbūs\nbūsi\nbūsiet\nbūsim\nbūt\nbūšu\ncaur\ndiemžēl\ndiezin\ndroši\ndēļ\nesam\nesat\nesi\nesmu\ngan\ngar\niekam\niekams\niekām\niekāms\niekš\niekšpus\nik\nir\nit\nitin\niz\nja\njau\njeb\njebšu\njel\njo\njā\nka\nkamēr\nkaut\nkolīdz\nkopš\nkā\nkļuva\nkļuvi\nkļuvu\nkļuvām\nkļuvāt\nkļūs\nkļūsi\nkļūsiet\nkļūsim\nkļūst\nkļūstam\nkļūstat\nkļūsti\nkļūstu\nkļūt\nkļūšu\nlabad\nlai\nlejpus\nlīdz\nlīdzko\nne\nnebūt\nnedz\nnekā\nnevis\nnezin\nno\nnu\nnē\notrpus\npa\npar\npat\npie\npirms\npret\npriekš\npār\npēc\nstarp\ntad\ntak\ntapi\ntaps\ntapsi\ntapsiet\ntapsim\ntapt\ntapāt\ntapšu\ntaču\nte\ntiec\ntiek\ntiekam\ntiekat\ntieku\ntik\ntika\ntikai\ntiki\ntikko\ntiklab\ntiklīdz\ntiks\ntiksiet\ntiksim\ntikt\ntiku\ntikvien\ntikām\ntikāt\ntikšu\ntomēr\ntopat\nturpretim\nturpretī\ntā\ntādēļ\ntālab\ntāpēc\nun\nuz\nvai\nvar\nvarat\nvarēja\nvarēji\nvarēju\nvarējām\nvarējāt\nvarēs\nvarēsi\nvarēsiet\nvarēsim\nvarēt\nvarēšu\nvien\nvirs\nvirspus\nvis\nviņpus\nzem\nārpus\nšaipus\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/et/__init__.py----------------------------------------
A:spacy.lang.et.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
spacy.lang.et.__init__.Estonian(Language)
spacy.lang.et.__init__.EstonianDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/et/stop_words.py----------------------------------------
A:spacy.lang.et.stop_words.STOP_WORDS->set('\naga\nei\net\nja\njah\nkas\nkui\nkõik\nma\nme\nmida\nmidagi\nmind\nminu\nmis\nmu\nmul\nmulle\nnad\nnii\noled\nolen\noli\noma\non\npole\nsa\nseda\nsee\nselle\nsiin\nsiis\nta\nte\nära\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/af/__init__.py----------------------------------------
A:spacy.lang.af.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
spacy.lang.af.__init__.Afrikaans(Language)
spacy.lang.af.__init__.AfrikaansDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/af/stop_words.py----------------------------------------
A:spacy.lang.af.stop_words.STOP_WORDS->set("\n'n\naan\naf\nal\nas\nbaie\nby\ndaar\ndag\ndat\ndie\ndit\neen\nek\nen\ngaan\ngesê\nhaar\nhet\nhom\nhulle\nhy\nin\nis\njou\njy\nkan\nkom\nma\nmaar\nmet\nmy\nna\nnie\nom\nons\nop\nsaam\nsal\nse\nsien\nso\nsy\nte\ntoe\nuit\nvan\nvir\nwas\nwat\nŉ\n".split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/tl/__init__.py----------------------------------------
A:spacy.lang.tl.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.tl.__init__.lex_attr_getters[NORM]->add_lookups(Language.Defaults.lex_attr_getters[NORM], BASE_NORMS)
A:spacy.lang.tl.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS, TOKENIZER_EXCEPTIONS)
spacy.lang.tl.__init__.Tagalog(Language)
spacy.lang.tl.__init__.TagalogDefaults(Language.Defaults)
spacy.lang.tl.__init__._return_tl(_)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/tl/lex_attrs.py----------------------------------------
A:spacy.lang.tl.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.tl.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.tl.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/tl/tokenizer_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/tl/stop_words.py----------------------------------------
A:spacy.lang.tl.stop_words.STOP_WORDS->set('\nakin\naking\nako\nalin\nam\namin\naming\nang\nano\nanumang\napat\nat\natin\nating\nay\nbababa\nbago\nbakit\nbawat\nbilang\ndahil\ndalawa\ndapat\ndin\ndito\ndoon\ngagawin\ngayunman\nginagawa\nginawa\nginawang\ngumawa\ngusto\nhabang\nhanggang\nhindi\nhuwag\niba\nibaba\nibabaw\nibig\nikaw\nilagay\nilalim\nilan\ninyong\nisa\nisang\nitaas\nito\niyo\niyon\niyong\nka\nkahit\nkailangan\nkailanman\nkami\nkanila\nkanilang\nkanino\nkanya\nkanyang\nkapag\nkapwa\nkaramihan\nkatiyakan\nkatulad\nkaya\nkaysa\nko\nkong\nkulang\nkumuha\nkung\nlaban\nlahat\nlamang\nlikod\nlima\nmaaari\nmaaaring\nmaging\nmahusay\nmakita\nmarami\nmarapat\nmasyado\nmay\nmayroon\nmga\nminsan\nmismo\nmula\nmuli\nna\nnabanggit\nnaging\nnagkaroon\nnais\nnakita\nnamin\nnapaka\nnarito\nnasaan\nng\nngayon\nni\nnila\nnilang\nnito\nniya\nniyang\nnoon\no\npa\npaano\npababa\npaggawa\npagitan\npagkakaroon\npagkatapos\npalabas\npamamagitan\npanahon\npangalawa\npara\nparaan\npareho\npataas\npero\npumunta\npumupunta\nsa\nsaan\nsabi\nsabihin\nsarili\nsila\nsino\nsiya\ntatlo\ntayo\ntulad\ntungkol\nuna\nwalang\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/el/get_pos_from_wiktionary.py----------------------------------------
A:spacy.lang.el.get_pos_from_wiktionary.regex->re.compile('==={{(\\w+)\\|el}}===')
A:spacy.lang.el.get_pos_from_wiktionary.regex2->re.compile('==={{(\\w+ \\w+)\\|el}}===')
A:spacy.lang.el.get_pos_from_wiktionary.title->title.lower().lower()
A:spacy.lang.el.get_pos_from_wiktionary.all_regex->re.compile('==={{(\\w+)\\|el}}===').findall(text)
A:spacy.lang.el.get_pos_from_wiktionary.words->sorted(expected_parts_dict[i])
spacy.lang.el.get_pos_from_wiktionary.get_pos_from_wiktionary()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/el/__init__.py----------------------------------------
A:spacy.lang.el.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.el.__init__.lex_attr_getters[NORM]->add_lookups(Language.Defaults.lex_attr_getters[NORM], BASE_NORMS, NORM_EXCEPTIONS)
A:spacy.lang.el.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS, TOKENIZER_EXCEPTIONS)
A:spacy.lang.el.__init__.lookups->Lookups()
spacy.lang.el.__init__.Greek(Language)
spacy.lang.el.__init__.GreekDefaults(Language.Defaults)
spacy.lang.el.__init__.GreekDefaults.create_lemmatizer(cls,nlp=None,lookups=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/el/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/el/lemmatizer.py----------------------------------------
A:spacy.lang.el.lemmatizer.string->string.lower().lower()
spacy.lang.el.GreekLemmatizer(Lemmatizer)
spacy.lang.el.GreekLemmatizer.lemmatize(self,string,index,exceptions,rules)
spacy.lang.el.lemmatizer.GreekLemmatizer(Lemmatizer)
spacy.lang.el.lemmatizer.GreekLemmatizer.lemmatize(self,string,index,exceptions,rules)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/el/tag_map_general.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/el/lex_attrs.py----------------------------------------
A:spacy.lang.el.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.el.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('^')
spacy.lang.el.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/el/tokenizer_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/el/syntax_iterators.py----------------------------------------
A:spacy.lang.el.syntax_iterators.conj->doc.vocab.strings.add('conj')
A:spacy.lang.el.syntax_iterators.nmod->doc.vocab.strings.add('nmod')
A:spacy.lang.el.syntax_iterators.np_label->doc.vocab.strings.add('NP')
A:spacy.lang.el.syntax_iterators.seen->set()
spacy.lang.el.syntax_iterators.noun_chunks(obj)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/el/stop_words.py----------------------------------------
A:spacy.lang.el.stop_words.STOP_WORDS->set('\nαδιάκοπα αι ακόμα ακόμη ακριβώς άλλα αλλά αλλαχού άλλες άλλη άλλην\nάλλης αλλιώς αλλιώτικα άλλο άλλοι αλλοιώς αλλοιώτικα άλλον άλλος άλλοτε αλλού\nάλλους άλλων άμα άμεσα αμέσως αν ανά ανάμεσα αναμεταξύ άνευ αντί αντίπερα αντίς\nάνω ανωτέρω άξαφνα απ απέναντι από απόψε άρα άραγε αρκετά αρκετές\nαρχικά ας αύριο αυτά αυτές αυτή αυτήν αυτής αυτό αυτοί αυτόν αυτός αυτού αυτούς\nαυτών αφότου αφού\n\nβέβαια βεβαιότατα\n\nγι για γιατί γρήγορα γύρω\n\nδα δε δείνα δεν δεξιά δήθεν δηλαδή δι δια διαρκώς δικά δικό δικοί δικός δικού\nδικούς διόλου δίπλα δίχως\n\nεάν εαυτό εαυτόν εαυτού εαυτούς εαυτών έγκαιρα εγκαίρως εγώ εδώ ειδεμή είθε είμαι\nείμαστε είναι εις είσαι είσαστε είστε είτε είχα είχαμε είχαν είχατε είχε είχες έκαστα\nέκαστες έκαστη έκαστην έκαστης έκαστο έκαστοι έκαστον έκαστος εκάστου εκάστους εκάστων\nεκεί εκείνα εκείνες εκείνη εκείνην εκείνης εκείνο εκείνοι εκείνον εκείνος εκείνου\nεκείνους εκείνων εκτός εμάς εμείς εμένα εμπρός εν ένα έναν ένας ενός εντελώς εντός\nεναντίον  εξής  εξαιτίας  επιπλέον επόμενη εντωμεταξύ ενώ εξ έξαφνα εξήσ εξίσου έξω επάνω\nεπειδή έπειτα επί επίσης επομένως εσάς εσείς εσένα έστω εσύ ετέρα ετέραι ετέρας έτερες\nέτερη έτερης έτερο έτεροι έτερον έτερος ετέρου έτερους ετέρων ετούτα ετούτες ετούτη ετούτην\nετούτης ετούτο ετούτοι ετούτον ετούτος ετούτου ετούτους ετούτων έτσι εύγε ευθύς ευτυχώς εφεξής\nέχει έχεις έχετε έχομε έχουμε έχουν εχτές έχω έως έγιναν  έγινε  έκανε  έξι  έχοντας\n\nη ήδη ήμασταν ήμαστε ήμουν ήσασταν ήσαστε ήσουν ήταν ήτανε ήτοι ήττον\n\nθα\n\nι ιδία ίδια ίδιαν ιδίας ίδιες ίδιο ίδιοι ίδιον ίδιοσ ίδιος ιδίου ίδιους ίδιων ιδίως ιι ιιι\nίσαμε ίσια ίσως\n\nκάθε καθεμία καθεμίας καθένα καθένας καθενός καθετί καθόλου καθώς και κακά κακώς καλά\nκαλώς καμία καμίαν καμίας κάμποσα κάμποσες κάμποση κάμποσην κάμποσης κάμποσο κάμποσοι\nκάμποσον κάμποσος κάμποσου κάμποσους κάμποσων κανείς κάνεν κανένα κανέναν κανένας\nκανενός κάποια κάποιαν κάποιας κάποιες κάποιο κάποιοι κάποιον κάποιος κάποιου κάποιους\nκάποιων κάποτε κάπου κάπως κατ κατά κάτι κατιτί κατόπιν κάτω κιόλας κλπ κοντά κτλ κυρίως\n\nλιγάκι λίγο λιγότερο λόγω λοιπά λοιπόν\n\nμα μαζί μακάρι μακρυά μάλιστα μάλλον μας με μεθαύριο μείον μέλει μέλλεται μεμιάς μεν\nμερικά μερικές μερικοί μερικούς μερικών μέσα μετ μετά μεταξύ μέχρι μη μήδε μην μήπως\nμήτε μια μιαν μιας μόλις μολονότι μονάχα μόνες μόνη μόνην μόνης μόνο μόνοι μονομιάς\nμόνος μόνου μόνους μόνων μου μπορεί μπορούν μπρος μέσω  μία  μεσώ\n\nνα ναι νωρίς\n\nξανά ξαφνικά\n\nο οι όλα όλες όλη όλην όλης όλο ολόγυρα όλοι όλον ολονέν όλος ολότελα όλου όλους όλων\nόλως ολωσδιόλου όμως όποια οποιαδήποτε οποίαν οποιανδήποτε οποίας οποίος οποιασδήποτε οποιδήποτε\nόποιες οποιεσδήποτε όποιο οποιοδηήποτε όποιοι όποιον οποιονδήποτε όποιος οποιοσδήποτε\nοποίου οποιουδήποτε οποίους οποιουσδήποτε οποίων οποιωνδήποτε όποτε οποτεδήποτε όπου\nοπουδήποτε όπως ορισμένα ορισμένες ορισμένων ορισμένως όσα οσαδήποτε όσες οσεσδήποτε\nόση οσηδήποτε όσην οσηνδήποτε όσης οσησδήποτε όσο οσοδήποτε όσοι οσοιδήποτε όσον οσονδήποτε\nόσος οσοσδήποτε όσου οσουδήποτε όσους οσουσδήποτε όσων οσωνδήποτε όταν ότι οτιδήποτε\nότου ου ουδέ ούτε όχι οποία  οποίες  οποίο  οποίοι  οπότε  ος\n\nπάνω  παρά  περί  πολλά  πολλές  πολλοί  πολλούς  που  πρώτα  πρώτες  πρώτη  πρώτο  πρώτος  πως\nπάλι πάντα πάντοτε παντού πάντως πάρα πέρα πέρι περίπου περισσότερο πέρσι πέρυσι πια πιθανόν\nπιο πίσω πλάι πλέον πλην ποιά ποιάν ποιάς ποιές ποιό ποιοί ποιόν ποιός ποιού ποιούς\nποιών πολύ πόσες πόση πόσην πόσης πόσοι πόσος πόσους πότε ποτέ πού πούθε πουθενά πρέπει\nπριν προ προκειμένου πρόκειται πρόπερσι προς προτού προχθές προχτές πρωτύτερα πώς\n\nσαν σας σε σεις σου στα στη στην στης στις στο στον στου στους στων συγχρόνως\nσυν συνάμα συνεπώς συχνάς συχνές συχνή συχνήν συχνής συχνό συχνοί συχνόν\nσυχνός συχνού συχνούς συχνών συχνώς σχεδόν\n\nτα τάδε ταύτα ταύτες ταύτη ταύτην ταύτης ταύτοταύτον ταύτος ταύτου ταύτων τάχα τάχατε\nτελευταία  τελευταίο  τελευταίος  τού  τρία  τρίτη  τρεις τελικά τελικώς τες τέτοια τέτοιαν\nτέτοιας τέτοιες τέτοιο τέτοιοι τέτοιον τέτοιος τέτοιου\nτέτοιους τέτοιων τη την της τι τίποτα τίποτε τις το τοι τον τοσ τόσα τόσες τόση τόσην\nτόσης τόσο τόσοι τόσον τόσος τόσου τόσους τόσων τότε του τουλάχιστο τουλάχιστον τους τούς τούτα\nτούτες τούτη τούτην τούτης τούτο τούτοι τούτοις τούτον τούτος τούτου τούτους τούτων τυχόν\nτων τώρα\n\nυπ υπέρ υπό υπόψη υπόψιν ύστερα\n\nχωρίς χωριστά\n\nω ως ωσάν ωσότου ώσπου ώστε ωστόσο ωχ\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/el/norm_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/el/tag_map.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/el/punctuation.py----------------------------------------
A:spacy.lang.el.punctuation.UNITS->merge_chars(_units)
spacy.lang.el.punctuation.merge_chars(char)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/mr/__init__.py----------------------------------------
A:spacy.lang.mr.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
spacy.lang.mr.__init__.Marathi(Language)
spacy.lang.mr.__init__.MarathiDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/mr/stop_words.py----------------------------------------
A:spacy.lang.mr.stop_words.STOP_WORDS->set('\nन\nअतरी\nतो\nहें\nतें\nकां\nआणि\nजें\nजे\nमग\nते\nमी\nजो\nपरी\nगा\nहे\nऐसें\nआतां\nनाहीं\nतेथ\nहा\nतया\nअसे\nम्हणे\nकाय\nकीं\nजैसें\nतंव\nतूं\nहोय\nजैसा\nआहे\nपैं\nतैसा\nजरी\nम्हणोनि\nएक\nऐसा\nजी\nना\nमज\nएथ\nया\nजेथ\nजया\nतुज\nतेणें\nतैं\nपां\nअसो\nकरी\nऐसी\nयेणें\nजाहला\nतेंचि\nआघवें\nहोती\nकांहीं\nहोऊनि\nएकें\nमातें\nठायीं\nये\nसकळ\nकेलें\nजेणें\nजाण\nजैसी\nहोये\nजेवीं\nएऱ्हवीं\nमीचि\nकिरीटी\nदिसे\nदेवा\nहो\nतरि\nकीजे\nतैसे\nआपण\nतिये\nकर्म\nनोहे\nइये\nपडे\nमाझें\nतैसी\nलागे\nनाना\nजंव\nकीर\nअधिक\nअनेक\nअशी\nअसलयाचे\nअसलेल्या\nअसा\nअसून\nअसे\nआज\nआणि\nआता\nआपल्या\nआला\nआली\nआले\nआहे\nआहेत\nएक\nएका\nकमी\nकरणयात\nकरून\nका\nकाम\nकाय\nकाही\nकिवा\nकी\nकेला\nकेली\nकेले\nकोटी\nगेल्या\nघेऊन\nजात\nझाला\nझाली\nझाले\nझालेल्या\nटा\nतर\nतरी\nतसेच\nता\nती\nतीन\nते\nतो\nत्या\nत्याचा\nत्याची\nत्याच्या\nत्याना\nत्यानी\nत्यामुळे\nत्री\nदिली\nदोन\nन\nपण\nपम\nपरयतन\nपाटील\nम\nमात्र\nमाहिती\nमी\nमुबी\nम्हणजे\nम्हणाले\nम्हणून\nया\nयाचा\nयाची\nयाच्या\nयाना\nयानी\nयेणार\nयेत\nयेथील\nयेथे\nलाख\nव\nव्यकत\nसर्व\nसागित्ले\nसुरू\nहजार\nहा\nही\nहे\nहोणार\nहोत\nहोता\nहोती\nहोते\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/kn/__init__.py----------------------------------------
A:spacy.lang.kn.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
spacy.lang.kn.__init__.Kannada(Language)
spacy.lang.kn.__init__.KannadaDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/kn/stop_words.py----------------------------------------
A:spacy.lang.kn.stop_words.STOP_WORDS->set('\nಹಲವು\nಮೂಲಕ\nಹಾಗೂ\nಅದು\nನೀಡಿದ್ದಾರೆ\nಯಾವ\nಎಂದರು\nಅವರು\nಈಗ\nಎಂಬ\nಹಾಗಾಗಿ\nಅಷ್ಟೇ\nನಾವು\nಇದೇ\nಹೇಳಿ\nತಮ್ಮ\nಹೀಗೆ\nನಮ್ಮ\nಬೇರೆ\nನೀಡಿದರು\nಮತ್ತೆ\nಇದು\nಈ\nನೀವು\nನಾನು\nಇತ್ತು\nಎಲ್ಲಾ\nಯಾವುದೇ\nನಡೆದ\nಅದನ್ನು\nಎಂದರೆ\nನೀಡಿದೆ\nಹೀಗಾಗಿ\nಜೊತೆಗೆ\nಇದರಿಂದ\nನನಗೆ\nಅಲ್ಲದೆ\nಎಷ್ಟು\nಇದರ\nಇಲ್ಲ\nಕಳೆದ\nತುಂಬಾ\nಈಗಾಗಲೇ\nಮಾಡಿ\nಅದಕ್ಕೆ\nಬಗ್ಗೆ\nಅವರ\nಇದನ್ನು\nಆ\nಇದೆ\nಹೆಚ್ಚು\nಇನ್ನು\nಎಲ್ಲ\nಇರುವ\nಅವರಿಗೆ\nನಿಮ್ಮ\nಏನು\nಕೂಡ\nಇಲ್ಲಿ\nನನ್ನನ್ನು\nಕೆಲವು\nಮಾತ್ರ\nಬಳಿಕ\nಅಂತ\nತನ್ನ\nಆಗ\nಅಥವಾ\nಅಲ್ಲ\nಕೇವಲ\nಆದರೆ\nಮತ್ತು\nಇನ್ನೂ\nಅದೇ\nಆಗಿ\nಅವರನ್ನು\nಹೇಳಿದ್ದಾರೆ\nನಡೆದಿದೆ\nಇದಕ್ಕೆ\nಎಂಬುದು\nಎಂದು\nನನ್ನ\nಮೇಲೆ\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ur/__init__.py----------------------------------------
A:spacy.lang.ur.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
spacy.lang.ur.__init__.Urdu(Language)
spacy.lang.ur.__init__.UrduDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ur/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ur/lex_attrs.py----------------------------------------
A:spacy.lang.ur.lex_attrs._num_words->'ایک دو تین چار پانچ چھ سات آٹھ نو دس گیارہ بارہ تیرہ چودہ پندرہ سولہ سترہ\n اٹهارا انیس بیس اکیس بائیس تئیس چوبیس پچیس چھببیس\nستایس اٹھائس انتيس تیس اکتیس بتیس تینتیس چونتیس پینتیس\n چھتیس سینتیس ارتیس انتالیس چالیس اکتالیس بیالیس تیتالیس\nچوالیس پیتالیس چھیالیس سینتالیس اڑتالیس انچالیس پچاس اکاون باون\n تریپن چون پچپن چھپن ستاون اٹھاون انسٹھ ساثھ\nاکسٹھ باسٹھ تریسٹھ چوسٹھ پیسٹھ چھیاسٹھ سڑسٹھ اڑسٹھ\nانھتر ستر اکھتر بھتتر تیھتر چوھتر تچھتر چھیتر ستتر\nاٹھتر انیاسی اسی اکیاسی بیاسی تیراسی چوراسی پچیاسی چھیاسی\n سٹیاسی اٹھیاسی نواسی نوے اکانوے بانوے ترانوے\nچورانوے پچانوے چھیانوے ستانوے اٹھانوے ننانوے سو\n'.split()
A:spacy.lang.ur.lex_attrs._ordinal_words->'پہلا دوسرا تیسرا چوتھا پانچواں چھٹا ساتواں آٹھواں نواں دسواں گیارہواں بارہواں تیرھواں چودھواں\n پندرھواں سولہواں سترھواں اٹھارواں انیسواں بسیواں\n'.split()
A:spacy.lang.ur.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.ur.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.ur.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ur/stop_words.py----------------------------------------
A:spacy.lang.ur.stop_words.STOP_WORDS->set('\nثھی\nخو\nگی\nاپٌے\nگئے\nثہت\nطرف\nہوبری\nپبئے\nاپٌب\nدوضری\nگیب\nکت\nگب\nثھی\nضے\nہر\nپر\nاش\nدی\nگے\nلگیں\nہے\nثعذ\nضکتے\nتھی\nاى\nدیب\nلئے\nوالے\nیہ\nثدبئے\nضکتی\nتھب\nاًذر\nرریعے\nلگی\nہوبرا\nہوًے\nثبہر\nضکتب\nًہیں\nتو\nاور\nرہب\nلگے\nہوضکتب\nہوں\nکب\nہوبرے\nتوبم\nکیب\nایطے\nرہی\nهگر\nہوضکتی\nہیں\nکریں\nہو\nتک\nکی\nایک\nرہے\nهیں\nہوضکتے\nکیطے\nہوًب\nتت\nکہ\nہوا\nآئے\nضبت\nتھے\nکیوں\nہو\nتب\nکے\nپھر\nثغیر\nخبر\nہے\nرکھ\nکی\nطب\nکوئی\n  رریعے\nثبرے\nخب\nاضطرذ\nثلکہ\nخجکہ\nرکھ\nتب\nکی\nطرف\nثراں\nخبر\nرریعہ\nاضکب\nثٌذ\nخص\nکی\nلئے\nتوہیں\nدوضرے\nکررہی\nاضکی\nثیچ\nخوکہ\nرکھتی\nکیوًکہ\nدوًوں\nکر\nرہے\nخبر\nہی\nثرآں\nاضکے\nپچھلا\nخیطب\nرکھتے\nکے\nثعذ\nتو\nہی\n  دورى\nکر\nیہبں\nآش\nتھوڑا\nچکے\nزکویہ\nدوضروں\nضکب\nاوًچب\nثٌب\nپل\nتھوڑی\nچلا\nخبهوظ\nدیتب\nضکٌب\nاخبزت\nاوًچبئی\nثٌبرہب\nپوچھب\nتھوڑے\nچلو\nختن\nدیتی\nضکی\nاچھب\nاوًچی\nثٌبرہی\nپوچھتب\nتیي\nچلیں\nدر\nدیتے\nضکے\nاچھی\nاوًچے\nثٌبرہے\nپوچھتی\nخبًب\nچلے\nدرخبت\nدیر\nضلطلہ\nاچھے\nاٹھبًب\nثٌبًب\nپوچھتے\nخبًتب\nچھوٹب\nدرخہ\nدیکھٌب\nضوچ\nاختتبم\nاہن\nثٌذ\nپوچھٌب\nخبًتی\nچھوٹوں\nدرخے\nدیکھو\nضوچب\nادھر\nآئی\nثٌذکرًب\nپوچھو\nخبًتے\nچھوٹی\nدرزقیقت\nدیکھی\nضوچتب\nارد\nآئے\nثٌذکرو\nپوچھوں\nخبًٌب\nچھوٹے\nدرضت\nدیکھیں\nضوچتی\nاردگرد\nآج\nثٌذی\nپوچھیں\nخططرذ\nچھہ\nدش\nدیٌب\nضوچتے\nارکبى\nآخر\nثڑا\nپورا\nخگہ\nچیسیں\nدفعہ\nدے\nضوچٌب\nاضتعوبل\nآخر\nپہلا\nخگہوں\nزبصل\nدکھبئیں\nراضتوں\nضوچو\nاضتعوبلات\nآدهی\nثڑی\nپہلی\nخگہیں\nزبضر\nدکھبتب\nراضتہ\nضوچی\nاغیب\nآًب\nثڑے\nپہلےضی\nخلذی\nزبل\nدکھبتی\nراضتے\nضوچیں\nاطراف\nآٹھ\nثھر\nخٌبة\nزبل\nدکھبتے\nرکي\nضیذھب\nافراد\nآیب\nثھرا\nپہلے\nخواى\nزبلات\nدکھبًب\nرکھب\nضیذھی\nاکثر\nثب\nہوا\nپیع\nخوًہی\nزبلیہ\nدکھبو\nرکھی\nضیذھے\nاکٹھب\nثھرپور\nتبزٍ\nخیطبکہ\nزصوں\nرکھے\nضیکٌڈ\nاکٹھی\nثبری\nثہتر\nتر\nچبر\nزصہ\nدلچطپ\nزیبدٍ\nغبیذ\nاکٹھے\nثبلا\nثہتری\nترتیت\nچبہب\nزصے\nدلچطپی\nضبت\nغخص\nاکیلا\nثبلترتیت\nثہتریي\nتریي\nچبہٌب\nزقبئق\nدلچطپیبں\nضبدٍ\nغذ\nاکیلی\nثرش\nپبش\nتعذاد\nچبہے\nزقیتیں\nهٌبضت\nضبرا\nغروع\nاکیلے\nثغیر\nپبًب\nچکب\nزقیقت\nدو\nضبرے\nغروعبت\nاگرچہ\nثلٌذ\nپبًچ\nتن\nچکی\nزکن\nدور\nضبل\nغے\nالگ\nپراًب\nتٌہب\nچکیں\nدوضرا\nضبلوں\nصبف\nصسیر\nقجیلہ\nکوًطے\nلازهی\nهطئلے\nًیب\nطریق\nکرتی\nکہتے\nصفر\nقطن\nکھولا\nلگتب\nهطبئل\nوار\nطریقوں\nکرتے\nکہٌب\nصورت\nکئی\nکھولٌب\nلگتی\nهطتعول\nوار\nطریقہ\nکرتے\nہو\nکہٌب\nصورتسبل\nکئے\nکھولو\nلگتے\nهػتول\nٹھیک\nطریقے\nکرًب\nکہو\nصورتوں\nکبفی\nهطلق\nڈھوًڈا\nطور\nکرو\nکہوں\nصورتیں\nکبم\nکھولیں\nلگی\nهعلوم\nڈھوًڈلیب\nطورپر\nکریں\nکہی\nضرور\nکجھی\nکھولے\nلگے\nهکول\nڈھوًڈًب\nظبہر\nکرے\nکہیں\nضرورت\nکرا\nکہب\nلوجب\nهلا\nڈھوًڈو\nعذد\nکل\nکہیں\nکرتب\nکہتب\nلوجی\nهوکي\nڈھوًڈی\nعظین\nکن\nکہے\nضروری\nکرتبہوں\nکہتی\nلوجے\nهوکٌبت\nڈھوًڈیں\nعلاقوں\nکوتر\nکیے\nلوسبت\nهوکٌہ\nہن\nلے\nًبپطٌذ\nہورہے\nعلاقہ\nکورا\nکے\nرریعے\nلوسہ\nهڑا\nہوئی\nهتعلق\nًبگسیر\nہوگئی\nعلاقے\nکوروں\nگئی\nلو\nهڑًب\nہوئے\nهسترم\nًطجت\nہو\nگئے\nعلاوٍ\nکورٍ\nگرد\nلوگ\nهڑے\nہوتی\nهسترهہ\nًقطہ\nہوگیب\nکورے\nگروپ\nلوگوں\nهہرثبى\nہوتے\nهسطوش\nًکبلٌب\nہوًی\nعووهی\nکوطي\nگروٍ\nلڑکپي\nهیرا\nہوچکب\nهختلف\nًکتہ\nہی\nفرد\nکوى\nگروہوں\nلی\nهیری\nہوچکی\nهسیذ\nفی\nکوًطب\nگٌتی\nلیب\nهیرے\nہوچکے\nهطئلہ\nًوخواى\nیقیٌی\nقجل\nکوًطی\nلیٌب\nًئی\nہورہب\nلیں\nًئے\nہورہی\nثبعث\nضت\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ur/tag_map.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ur/punctuation.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/hr/__init__.py----------------------------------------
A:spacy.lang.hr.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.hr.__init__.lex_attr_getters[NORM]->add_lookups(Language.Defaults.lex_attr_getters[NORM], BASE_NORMS)
A:spacy.lang.hr.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS)
spacy.lang.hr.__init__.Croatian(Language)
spacy.lang.hr.__init__.CroatianDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/hr/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/hr/stop_words.py----------------------------------------
A:spacy.lang.hr.stop_words.STOP_WORDS->set('\na\nah\naha\naj\nako\nal\nali\narh\nau\navaj\nbar\nbaš\nbez\nbi\nbih\nbijah\nbijahu\nbijaše\nbijasmo\nbijaste\nbila\nbili\nbilo\nbio\nbismo\nbiste\nbiti\nbrr\nbuć\nbudavši\nbude\nbudimo\nbudite\nbudu\nbudući\nbum\nbumo\nće\nćemo\nćeš\nćete\nčijem\nčijim\nčijima\nću\nda\ndaj\ndakle\nde\ndeder\ndem\ndjelomice\ndjelomično\ndo\ndoista\ndok\ndokle\ndonekle\ndosad\ndoskoro\ndotad\ndotle\ndovečer\ndrugamo\ndrugdje\nduž\ne\neh\nehe\nej\neno\neto\nevo\nga\ngdjekakav\ngdjekoje\ngic\ngod\nhalo\nhej\nhm\nhoće\nhoćemo\nhoćeš\nhoćete\nhoću\nhop\nhtijahu\nhtijasmo\nhtijaste\nhtio\nhtjedoh\nhtjedoše\nhtjedoste\nhtjela\nhtjele\nhtjeli\nhura\ni\niako\nih\niju\nijuju\nikada\nikakav\nikakva\nikakve\nikakvi\nikakvih\nikakvim\nikakvima\nikakvo\nikakvog\nikakvoga\nikakvoj\nikakvom\nikakvome\nili\nim\niz\nja\nje\njedna\njedne\njedni\njedno\njer\njesam\njesi\njesmo\njest\njeste\njesu\njim\njoj\njoš\nju\nkada\nkako\nkao\nkoja\nkoje\nkoji\nkojima\nkoju\nkroz\nlani\nli\nme\nmene\nmeni\nmi\nmimo\nmoj\nmoja\nmoje\nmoji\nmoju\nmu\nna\nnad\nnakon\nnam\nnama\nnas\nnaš\nnaša\nnaše\nnašeg\nnaši\nne\nneće\nnećemo\nnećeš\nnećete\nneću\nnego\nneka\nneke\nneki\nnekog\nneku\nnema\nnešto\nnetko\nni\nnije\nnikoga\nnikoje\nnikoji\nnikoju\nnisam\nnisi\nnismo\nniste\nnisu\nnjega\nnjegov\nnjegova\nnjegovo\nnjemu\nnjezin\nnjezina\nnjezino\nnjih\nnjihov\nnjihova\nnjihovo\nnjim\nnjima\nnjoj\nnju\nno\no\nod\nodmah\non\nona\none\noni\nono\nonu\nonoj\nonom\nonim\nonima\nova\novaj\novim\novima\novoj\npa\npak\npljus\npo\npod\npodalje\npoimence\npoizdalje\nponekad\npored\npostrance\npotajice\npotrbuške\npouzdano\nprije\ns\nsa\nsam\nsamo\nsasvim\nsav\nse\nsebe\nsebi\nsi\nšic\nsmo\nste\nšto\nšta\nštogod\nštagod\nsu\nsva\nsve\nsvi\nsvi\nsvog\nsvoj\nsvoja\nsvoje\nsvoju\nsvom\nsvu\nta\ntada\ntaj\ntako\nte\ntebe\ntebi\nti\ntim\ntima\nto\ntoj\ntome\ntu\ntvoj\ntvoja\ntvoje\ntvoji\ntvoju\nu\nusprkos\nutaman\nuvijek\nuz\nuza\nuzagrapce\nuzalud\nuzduž\nvaljda\nvam\nvama\nvas\nvaš\nvaša\nvaše\nvašim\nvašima\nveć\nvi\nvjerojatno\nvjerovatno\nvrh\nvrlo\nza\nzaista\nzar\nzatim\nzato\nzbija\nzbog\nželeći\nželjah\nželjela\nželjele\nželjeli\nželjelo\nželjen\nželjena\nželjene\nželjeni\nželjenu\nželjeo\nzimus\nzum\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/es/__init__.py----------------------------------------
A:spacy.lang.es.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.es.__init__.lex_attr_getters[NORM]->add_lookups(Language.Defaults.lex_attr_getters[NORM], BASE_NORMS)
A:spacy.lang.es.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS, TOKENIZER_EXCEPTIONS)
spacy.lang.es.__init__.Spanish(Language)
spacy.lang.es.__init__.SpanishDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/es/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/es/lex_attrs.py----------------------------------------
A:spacy.lang.es.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.es.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.es.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/es/tokenizer_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/es/syntax_iterators.py----------------------------------------
A:spacy.lang.es.syntax_iterators.np_label->doc.vocab.strings.add('NP')
A:spacy.lang.es.syntax_iterators.(left, right)->noun_bounds(doc, token, np_left_deps, np_right_deps, stop_deps)
A:spacy.lang.es.syntax_iterators.token->next_token(token)
spacy.lang.es.syntax_iterators.is_verb_token(token)
spacy.lang.es.syntax_iterators.next_token(token)
spacy.lang.es.syntax_iterators.noun_bounds(doc,root,np_left_deps,np_right_deps,stop_deps)
spacy.lang.es.syntax_iterators.noun_chunks(obj)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/es/stop_words.py----------------------------------------
A:spacy.lang.es.stop_words.STOP_WORDS->set('\nactualmente acuerdo adelante ademas además adrede afirmó agregó ahi ahora ahí\nal algo alguna algunas alguno algunos algún alli allí alrededor ambos ampleamos\nantano antaño ante anterior antes apenas aproximadamente aquel aquella aquellas\naquello aquellos aqui aquél aquélla aquéllas aquéllos aquí arriba arribaabajo\naseguró asi así atras aun aunque ayer añadió aún\n\nbajo bastante bien breve buen buena buenas bueno buenos\n\ncada casi cerca cierta ciertas cierto ciertos cinco claro comentó como con\nconmigo conocer conseguimos conseguir considera consideró consigo consigue\nconsiguen consigues contigo contra cosas creo cual cuales cualquier cuando\ncuanta cuantas cuanto cuantos cuatro cuenta cuál cuáles cuándo cuánta cuántas\ncuánto cuántos cómo\n\nda dado dan dar de debajo debe deben debido decir dejó del delante demasiado\ndemás dentro deprisa desde despacio despues después detras detrás dia dias dice\ndicen dicho dieron diferente diferentes dijeron dijo dio donde dos durante día\ndías dónde\n\nejemplo el ella ellas ello ellos embargo empleais emplean emplear empleas\nempleo en encima encuentra enfrente enseguida entonces entre era eramos eran\neras eres es esa esas ese eso esos esta estaba estaban estado estados estais\nestamos estan estar estará estas este esto estos estoy estuvo está están ex\nexcepto existe existen explicó expresó él ésa ésas ése ésos ésta éstas éste\néstos\n\nfin final fue fuera fueron fui fuimos\n\ngeneral gran grandes gueno\n\nha haber habia habla hablan habrá había habían hace haceis hacemos hacen hacer\nhacerlo haces hacia haciendo hago han hasta hay haya he hecho hemos hicieron\nhizo horas hoy hubo\n\nigual incluso indicó informo informó intenta intentais intentamos intentan\nintentar intentas intento ir\n\njunto\n\nla lado largo las le lejos les llegó lleva llevar lo los luego lugar\n\nmal manera manifestó mas mayor me mediante medio mejor mencionó menos menudo mi\nmia mias mientras mio mios mis misma mismas mismo mismos modo momento mucha\nmuchas mucho muchos muy más mí mía mías mío míos\n\nnada nadie ni ninguna ningunas ninguno ningunos ningún no nos nosotras nosotros\nnuestra nuestras nuestro nuestros nueva nuevas nuevo nuevos nunca\n\nocho os otra otras otro otros\n\npais para parece parte partir pasada pasado paìs peor pero pesar poca pocas\npoco pocos podeis podemos poder podria podriais podriamos podrian podrias podrá\npodrán podría podrían poner por porque posible primer primera primero primeros\nprincipalmente pronto propia propias propio propios proximo próximo próximos\npudo pueda puede pueden puedo pues\n\nqeu que quedó queremos quien quienes quiere quiza quizas quizá quizás quién quiénes qué\n\nraras realizado realizar realizó repente respecto\n\nsabe sabeis sabemos saben saber sabes salvo se sea sean segun segunda segundo\nsegún seis ser sera será serán sería señaló si sido siempre siendo siete sigue\nsiguiente sin sino sobre sois sola solamente solas solo solos somos son soy\nsoyos su supuesto sus suya suyas suyo sé sí sólo\n\ntal tambien también tampoco tan tanto tarde te temprano tendrá tendrán teneis\ntenemos tener tenga tengo tenido tenía tercera ti tiempo tiene tienen toda\ntodas todavia todavía todo todos total trabaja trabajais trabajamos trabajan\ntrabajar trabajas trabajo tras trata través tres tu tus tuvo tuya tuyas tuyo\ntuyos tú\n\nultimo un una unas uno unos usa usais usamos usan usar usas uso usted ustedes\núltima últimas último últimos\n\nva vais valor vamos van varias varios vaya veces ver verdad verdadera verdadero\nvez vosotras vosotros voy vuestra vuestras vuestro vuestros\n\nya yo\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/es/tag_map.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/tt/__init__.py----------------------------------------
A:spacy.lang.tt.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.tt.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS, TOKENIZER_EXCEPTIONS)
A:spacy.lang.tt.__init__.infixes->tuple(TOKENIZER_INFIXES)
spacy.lang.tt.__init__.Tatar(Language)
spacy.lang.tt.__init__.TatarDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/tt/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/tt/lex_attrs.py----------------------------------------
A:spacy.lang.tt.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.tt.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.tt.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/tt/tokenizer_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/tt/stop_words.py----------------------------------------
A:spacy.lang.tt.stop_words.STOP_WORDS->set('алай алайса алар аларга аларда алардан аларны аларның аларча\nалары аларын аларынга аларында аларыннан аларының алтмыш алтмышынчы алтмышынчыга\nалтмышынчыда алтмышынчыдан алтмышынчылар алтмышынчыларга алтмышынчыларда\nалтмышынчылардан алтмышынчыларны алтмышынчыларның алтмышынчыны алтмышынчының\nалты алтылап алтынчы алтынчыга алтынчыда алтынчыдан алтынчылар алтынчыларга\nалтынчыларда алтынчылардан алтынчыларны алтынчыларның алтынчыны алтынчының\nалтышар анда андагы андай андый андыйга андыйда андыйдан андыйны андыйның аннан\nансы анча аны аныкы аныкын аныкынга аныкында аныкыннан аныкының анысы анысын\nанысынга анысында анысыннан анысының аның аныңча аркылы ары аша аңа аңар аңарга\nаңарда аңардагы аңардан\n\nбар бара барлык барча барчасы барчасын барчасына барчасында барчасыннан\nбарчасының бары башка башкача бе\xadлән без безгә бездә бездән безне безнең безнеңчә\nбелдерүенчә белән бер бергә беренче беренчегә беренчедә беренчедән беренчеләр\nберенчеләргә беренчеләрдә беренчеләрдән беренчеләрне беренчеләрнең беренчене\nберенченең беркайда беркайсы беркая беркаян беркем беркемгә беркемдә беркемне\nберкемнең беркемнән берлән берни бернигә бернидә бернидән бернинди бернине\nбернинең берничек берничә бернәрсә бернәрсәгә бернәрсәдә бернәрсәдән бернәрсәне\nбернәрсәнең беррәттән берсе берсен берсенгә берсендә берсенең берсеннән берәр\nберәрсе берәрсен берәрсендә берәрсенең берәрсеннән берәрсенә берәү бигрәк бик\nбирле бит биш бишенче бишенчегә бишенчедә бишенчедән бишенчеләр бишенчеләргә\nбишенчеләрдә бишенчеләрдән бишенчеләрне бишенчеләрнең бишенчене бишенченең\nбишләп болай болар боларга боларда болардан боларны боларның болары боларын\nболарынга боларында боларыннан боларының бу буе буена буенда буенча буйлап\nбуларак булачак булды булмый булса булып булыр булырга бусы бүтән бәлки бән\nбәрабәренә бөтен бөтенесе бөтенесен бөтенесендә бөтенесенең бөтенесеннән\nбөтенесенә\n\nвә\n\nгел генә гына гүя гүяки гәрчә\n\nда ди дигән диде дип дистәләгән дистәләрчә дүрт дүртенче дүртенчегә дүртенчедә\nдүртенчедән дүртенчеләр дүртенчеләргә дүртенчеләрдә дүртенчеләрдән дүртенчеләрне\nдүртенчеләрнең дүртенчене дүртенченең дүртләп дә\n\nегерме егерменче егерменчегә егерменчедә егерменчедән егерменчеләр\nегерменчеләргә егерменчеләрдә егерменчеләрдән егерменчеләрне егерменчеләрнең\nегерменчене егерменченең ел елда\n\nиде идек идем ике икенче икенчегә икенчедә икенчедән икенчеләр икенчеләргә\nикенчеләрдә икенчеләрдән икенчеләрне икенчеләрнең икенчене икенченең икешәр икән\nилле илленче илленчегә илленчедә илленчедән илленчеләр илленчеләргә\nилленчеләрдә илленчеләрдән илленчеләрне илленчеләрнең илленчене илленченең илә\nилән инде исә итеп иткән итте итү итә итәргә иң\n\nйөз йөзенче йөзенчегә йөзенчедә йөзенчедән йөзенчеләр йөзенчеләргә йөзенчеләрдә\nйөзенчеләрдән йөзенчеләрне йөзенчеләрнең йөзенчене йөзенченең йөзләгән йөзләрчә\nйөзәрләгән\n\nкадәр кай кайбер кайберләре кайберсе кайберәү кайберәүгә кайберәүдә кайберәүдән\nкайберәүне кайберәүнең кайдагы кайсы кайсыбер кайсын кайсына кайсында кайсыннан\nкайсының кайчангы кайчандагы кайчаннан караганда карамастан карамый карата каршы\nкаршына каршында каршындагы кебек кем кемгә кемдә кемне кемнең кемнән кенә ки\nкилеп килә кирәк кына кырыгынчы кырыгынчыга кырыгынчыда кырыгынчыдан\nкырыгынчылар кырыгынчыларга кырыгынчыларда кырыгынчылардан кырыгынчыларны\nкырыгынчыларның кырыгынчыны кырыгынчының кырык күк күпләгән күпме күпмеләп\nкүпмешәр күпмешәрләп күптән күрә\n\nләкин\n\nмаксатында менә мең меңенче меңенчегә меңенчедә меңенчедән меңенчеләр\nмеңенчеләргә меңенчеләрдә меңенчеләрдән меңенчеләрне меңенчеләрнең меңенчене\nмеңенченең меңләгән меңләп меңнәрчә меңәрләгән меңәрләп миллиард миллиардлаган\nмиллиардларча миллион миллионлаган миллионнарча миллионынчы миллионынчыга\nмиллионынчыда миллионынчыдан миллионынчылар миллионынчыларга миллионынчыларда\nмиллионынчылардан миллионынчыларны миллионынчыларның миллионынчыны\nмиллионынчының мин миндә мине минем минемчә миннән миңа монда мондагы мондые\nмондыен мондыенгә мондыендә мондыеннән мондыеның мондый мондыйга мондыйда\nмондыйдан мондыйлар мондыйларга мондыйларда мондыйлардан мондыйларны\nмондыйларның мондыйлары мондыйларын мондыйларынга мондыйларында мондыйларыннан\nмондыйларының мондыйны мондыйның моннан монсыз монча моны моныкы моныкын\nмоныкынга моныкында моныкыннан моныкының монысы монысын монысынга монысында\nмонысыннан монысының моның моңа моңар моңарга мәгълүматынча мәгәр мән мөмкин\n\nни нибарысы никадәре нинди ниндие ниндиен ниндиенгә ниндиендә ниндиенең\nниндиеннән ниндиләр ниндиләргә ниндиләрдә ниндиләрдән ниндиләрен ниндиләренн\nниндиләреннгә ниндиләренндә ниндиләреннең ниндиләренннән ниндиләрне ниндиләрнең\nниндирәк нихәтле ничаклы ничек ничәшәр ничәшәрләп нуль нче нчы нәрсә нәрсәгә\nнәрсәдә нәрсәдән нәрсәне нәрсәнең\n\nсаен сез сезгә сездә сездән сезне сезнең сезнеңчә сигез сигезенче сигезенчегә\nсигезенчедә сигезенчедән сигезенчеләр сигезенчеләргә сигезенчеләрдә\nсигезенчеләрдән сигезенчеләрне сигезенчеләрнең сигезенчене сигезенченең\nсиксән син синдә сине синең синеңчә синнән сиңа соң сыман сүзенчә сүзләренчә\n\nта таба теге тегеләй тегеләр тегеләргә тегеләрдә тегеләрдән тегеләре тегеләрен\nтегеләренгә тегеләрендә тегеләренең тегеләреннән тегеләрне тегеләрнең тегенди\nтегендигә тегендидә тегендидән тегендине тегендинең тегендә тегендәге тегене\nтегенеке тегенекен тегенекенгә тегенекендә тегенекенең тегенекеннән тегенең\nтегеннән тегесе тегесен тегесенгә тегесендә тегесенең тегесеннән тегеңә тиеш тик\nтикле тора триллиард триллион тугыз тугызлап тугызлашып тугызынчы тугызынчыга\nтугызынчыда тугызынчыдан тугызынчылар тугызынчыларга тугызынчыларда\nтугызынчылардан тугызынчыларны тугызынчыларның тугызынчыны тугызынчының туксан\nтуксанынчы туксанынчыга туксанынчыда туксанынчыдан туксанынчылар туксанынчыларга\nтуксанынчыларда туксанынчылардан туксанынчыларны туксанынчыларның туксанынчыны\nтуксанынчының турында тыш түгел тә тәгаенләнгән төмән\n\nуенча уйлавынча ук ул ун уналты уналтынчы уналтынчыга уналтынчыда уналтынчыдан\nуналтынчылар уналтынчыларга уналтынчыларда уналтынчылардан уналтынчыларны\nуналтынчыларның уналтынчыны уналтынчының унарлаган унарлап унаула унаулап унбер\nунберенче унберенчегә унберенчедә унберенчедән унберенчеләр унберенчеләргә\nунберенчеләрдә унберенчеләрдән унберенчеләрне унберенчеләрнең унберенчене\nунберенченең унбиш унбишенче унбишенчегә унбишенчедә унбишенчедән унбишенчеләр\nунбишенчеләргә унбишенчеләрдә унбишенчеләрдән унбишенчеләрне унбишенчеләрнең\nунбишенчене унбишенченең ундүрт ундүртенче ундүртенчегә ундүртенчедә\nундүртенчедән ундүртенчеләр ундүртенчеләргә ундүртенчеләрдә ундүртенчеләрдән\nундүртенчеләрне ундүртенчеләрнең ундүртенчене ундүртенченең унике уникенче\nуникенчегә уникенчедә уникенчедән уникенчеләр уникенчеләргә уникенчеләрдә\nуникенчеләрдән уникенчеләрне уникенчеләрнең уникенчене уникенченең унлаган\nунлап уннарча унсигез унсигезенче унсигезенчегә унсигезенчедә унсигезенчедән\nунсигезенчеләр унсигезенчеләргә унсигезенчеләрдә унсигезенчеләрдән\nунсигезенчеләрне унсигезенчеләрнең унсигезенчене унсигезенченең унтугыз\nунтугызынчы унтугызынчыга унтугызынчыда унтугызынчыдан унтугызынчылар\nунтугызынчыларга унтугызынчыларда унтугызынчылардан унтугызынчыларны\nунтугызынчыларның унтугызынчыны унтугызынчының унынчы унынчыга унынчыда\nунынчыдан унынчылар унынчыларга унынчыларда унынчылардан унынчыларны\nунынчыларның унынчыны унынчының унҗиде унҗиденче унҗиденчегә унҗиденчедә\nунҗиденчедән унҗиденчеләр унҗиденчеләргә унҗиденчеләрдә унҗиденчеләрдән\nунҗиденчеләрне унҗиденчеләрнең унҗиденчене унҗиденченең унөч унөченче унөченчегә\nунөченчедә унөченчедән унөченчеләр унөченчеләргә унөченчеләрдә унөченчеләрдән\nунөченчеләрне унөченчеләрнең унөченчене унөченченең утыз утызынчы утызынчыга\nутызынчыда утызынчыдан утызынчылар утызынчыларга утызынчыларда утызынчылардан\nутызынчыларны утызынчыларның утызынчыны утызынчының\n\nфикеренчә фәкать\n\nхакында хәбәр хәлбуки хәтле хәтта\n\nчаклы чакта чөнки\n\nшикелле шул шулай шулар шуларга шуларда шулардан шуларны шуларның шулары шуларын\nшуларынга шуларында шуларыннан шуларының шулкадәр шултикле шултиклем шулхәтле\nшулчаклы шунда шундагы шундый шундыйга шундыйда шундыйдан шундыйны шундыйның\nшунлыктан шуннан шунсы шунча шуны шуныкы шуныкын шуныкынга шуныкында шуныкыннан\nшуныкының шунысы шунысын шунысынга шунысында шунысыннан шунысының шуның шушы\nшушында шушыннан шушыны шушының шушыңа шуңа шуңар шуңарга\n\nэлек\n\nюгыйсә юк юкса\n\nя ягъни язуынча яисә яки яктан якын ярашлы яхут яшь яшьлек\n\nҗиде җиделәп җиденче җиденчегә җиденчедә җиденчедән җиденчеләр җиденчеләргә\nҗиденчеләрдә җиденчеләрдән җиденчеләрне җиденчеләрнең җиденчене җиденченең\nҗидешәр җитмеш җитмешенче җитмешенчегә җитмешенчедә җитмешенчедән җитмешенчеләр\nҗитмешенчеләргә җитмешенчеләрдә җитмешенчеләрдән җитмешенчеләрне\nҗитмешенчеләрнең җитмешенчене җитмешенченең җыенысы\n\nүз үзе үзем үземдә үземне үземнең үземнән үземә үзен үзендә үзенең үзеннән үзенә\nүк\n\nһичбер һичбере һичберен һичберендә һичберенең һичбереннән һичберенә һичберсе\nһичберсен һичберсендә һичберсенең һичберсеннән һичберсенә һичберәү һичберәүгә\nһичберәүдә һичберәүдән һичберәүне һичберәүнең һичкайсы һичкайсыга һичкайсыда\nһичкайсыдан һичкайсыны һичкайсының һичкем һичкемгә һичкемдә һичкемне һичкемнең\nһичкемнән һични һичнигә һичнидә һичнидән һичнинди һичнине һичнинең һичнәрсә\nһичнәрсәгә һичнәрсәдә һичнәрсәдән һичнәрсәне һичнәрсәнең һәм һәммә һәммәсе\nһәммәсен һәммәсендә һәммәсенең һәммәсеннән һәммәсенә һәр һәрбер һәрбере һәрберсе\nһәркайсы һәркайсыга һәркайсыда һәркайсыдан һәркайсыны һәркайсының һәркем\nһәркемгә һәркемдә һәркемне һәркемнең һәркемнән һәрни һәрнәрсә һәрнәрсәгә\nһәрнәрсәдә һәрнәрсәдән һәрнәрсәне һәрнәрсәнең һәртөрле\n\nә әгәр әйтүенчә әйтүләренчә әлбәттә әле әлеге әллә әмма әнә\n\nөстәп өч өчен өченче өченчегә өченчедә өченчедән өченчеләр өченчеләргә\nөченчеләрдә өченчеләрдән өченчеләрне өченчеләрнең өченчене өченченең өчләп\nөчәрләп'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/tt/punctuation.py----------------------------------------
A:spacy.lang.tt.punctuation._hyphens_no_dash->char_classes.HYPHENS.replace('-', '').strip('|').replace('||', '')


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/sv/__init__.py----------------------------------------
A:spacy.lang.sv.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.sv.__init__.lex_attr_getters[NORM]->add_lookups(Language.Defaults.lex_attr_getters[NORM], BASE_NORMS)
A:spacy.lang.sv.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS, TOKENIZER_EXCEPTIONS)
spacy.lang.sv.__init__.Swedish(Language)
spacy.lang.sv.__init__.SwedishDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/sv/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/sv/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.sv.tokenizer_exceptions.verb_data_tc->dict(verb_data)
A:spacy.lang.sv.tokenizer_exceptions.verb_data_tc[ORTH]->verb_data_tc[ORTH].title().title()
A:spacy.lang.sv.tokenizer_exceptions.capitalized->orth.capitalize()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/sv/morph_rules.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/sv/stop_words.py----------------------------------------
A:spacy.lang.sv.stop_words.STOP_WORDS->set('\naderton adertonde adjö aldrig alla allas allt alltid alltså än andra andras\nannan annat ännu artonde arton åtminstone att åtta åttio åttionde åttonde av\näven\n\nbåda bådas bakom bara bäst bättre behöva behövas behövde behövt beslut beslutat\nbeslutit bland blev bli blir blivit bort borta bra\n\ndå dag dagar dagarna dagen där därför de del delen dem den deras dess det detta\ndig din dina dit ditt dock du\n\nefter eftersom elfte eller elva en enkel enkelt enkla enligt er era ert ett\nettusen\n\nfå fanns får fått fem femte femtio femtionde femton femtonde fick fin finnas\nfinns fjärde fjorton fjortonde fler flera flesta följande för före förlåt förra\nförsta fram framför från fyra fyrtio fyrtionde\n\ngå gälla gäller gällt går gärna gått genast genom gick gjorde gjort god goda\ngodare godast gör göra gott\n\nha hade haft han hans har här heller hellre helst helt henne hennes hit hög\nhöger högre högst hon honom hundra hundraen hundraett hur\n\ni ibland idag igår igen imorgon in inför inga ingen ingenting inget innan inne\ninom inte inuti\n\nja jag jämfört\n\nkan kanske knappast kom komma kommer kommit kr kunde kunna kunnat kvar\n\nlänge längre långsam långsammare långsammast långsamt längst långt lätt lättare\nlättast legat ligga ligger lika likställd likställda lilla lite liten litet\n\nman många måste med mellan men mer mera mest mig min mina mindre minst mitt\nmittemot möjlig möjligen möjligt möjligtvis mot mycket\n\nnågon någonting något några när nästa ned nederst nedersta nedre nej ner ni nio\nnionde nittio nittionde nitton nittonde nödvändig nödvändiga nödvändigt\nnödvändigtvis nog noll nr nu nummer\n\noch också ofta oftast olika olikt om oss\n\növer övermorgon överst övre\n\npå\n\nrakt rätt redan\n\nså sade säga säger sagt samma sämre sämst sedan senare senast sent sex sextio\nsextionde sexton sextonde sig sin sina sist sista siste sitt sjätte sju sjunde\nsjuttio sjuttionde sjutton sjuttonde ska skall skulle slutligen små smått snart\nsom stor stora större störst stort\n\ntack tidig tidigare tidigast tidigt till tills tillsammans tio tionde tjugo\ntjugoen tjugoett tjugonde tjugotre tjugotvå tjungo tolfte tolv tre tredje\ntrettio trettionde tretton trettonde två tvåhundra\n\nunder upp ur ursäkt ut utan utanför ute\n\nvad vänster vänstra var vår vara våra varför varifrån varit varken värre\nvarsågod vart vårt vem vems verkligen vi vid vidare viktig viktigare viktigast\nviktigt vilka vilken vilket vill\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/sv/tag_map.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/sl/__init__.py----------------------------------------
A:spacy.lang.sl.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
spacy.lang.sl.__init__.Slovenian(Language)
spacy.lang.sl.__init__.SlovenianDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/sl/stop_words.py----------------------------------------
A:spacy.lang.sl.stop_words.STOP_WORDS->set('\na\nali\napril\navgust\nb\nbi\nbil\nbila\nbile\nbili\nbilo\nbiti\nblizu\nbo\nbodo\nbojo\nbolj\nbom\nbomo\nboste\nbova\nboš\nbrez\nc\ncel\ncela\nceli\ncelo\nd\nda\ndaleč\ndan\ndanes\ndatum\ndecember\ndeset\ndeseta\ndeseti\ndeseto\ndevet\ndeveta\ndeveti\ndeveto\ndo\ndober\ndobra\ndobri\ndobro\ndokler\ndol\ndolg\ndolga\ndolgi\ndovolj\ndrug\ndruga\ndrugi\ndrugo\ndva\ndve\ne\neden\nen\nena\nene\neni\nenkrat\neno\netc.\nf\nfebruar\ng\ng.\nga\nga.\ngor\ngospa\ngospod\nh\nhalo\ni\nidr.\nii\niii\nin\niv\nix\niz\nj\njanuar\njaz\nje\nji\njih\njim\njo\njulij\njunij\njutri\nk\nkadarkoli\nkaj\nkajti\nkako\nkakor\nkamor\nkamorkoli\nkar\nkarkoli\nkaterikoli\nkdaj\nkdo\nkdorkoli\nker\nki\nkje\nkjer\nkjerkoli\nko\nkoder\nkoderkoli\nkoga\nkomu\nkot\nkratek\nkratka\nkratke\nkratki\nl\nlahka\nlahke\nlahki\nlahko\nle\nlep\nlepa\nlepe\nlepi\nlepo\nleto\nm\nmaj\nmajhen\nmajhna\nmajhni\nmalce\nmalo\nmanj\nmarec\nme\nmed\nmedtem\nmene\nmesec\nmi\nmidva\nmidve\nmnogo\nmoj\nmoja\nmoje\nmora\nmorajo\nmoram\nmoramo\nmorate\nmoraš\nmorem\nmu\nn\nna\nnad\nnaj\nnajina\nnajino\nnajmanj\nnaju\nnajveč\nnam\nnarobe\nnas\nnato\nnazaj\nnaš\nnaša\nnaše\nne\nnedavno\nnedelja\nnek\nneka\nnekaj\nnekatere\nnekateri\nnekatero\nnekdo\nneke\nnekega\nneki\nnekje\nneko\nnekoga\nnekoč\nni\nnikamor\nnikdar\nnikjer\nnikoli\nnič\nnje\nnjega\nnjegov\nnjegova\nnjegovo\nnjej\nnjemu\nnjen\nnjena\nnjeno\nnji\nnjih\nnjihov\nnjihova\nnjihovo\nnjiju\nnjim\nnjo\nnjun\nnjuna\nnjuno\nno\nnocoj\nnovember\nnpr.\no\nob\noba\nobe\noboje\nod\nodprt\nodprta\nodprti\nokoli\noktober\non\nonadva\none\noni\nonidve\nosem\nosma\nosmi\nosmo\noz.\np\npa\npet\npeta\npetek\npeti\npeto\npo\npod\npogosto\npoleg\npoln\npolna\npolni\npolno\nponavadi\nponedeljek\nponovno\npotem\npovsod\npozdravljen\npozdravljeni\nprav\nprava\nprave\npravi\npravo\nprazen\nprazna\nprazno\nprbl.\nprecej\npred\nprej\npreko\npri\npribl.\npribližno\nprimer\npripravljen\npripravljena\npripravljeni\nproti\nprva\nprvi\nprvo\nr\nravno\nredko\nres\nreč\ns\nsaj\nsam\nsama\nsame\nsami\nsamo\nse\nsebe\nsebi\nsedaj\nsedem\nsedma\nsedmi\nsedmo\nsem\nseptember\nseveda\nsi\nsicer\nskoraj\nskozi\nslab\nsmo\nso\nsobota\nspet\nsreda\nsrednja\nsrednji\nsta\nste\nstran\nstvar\nsva\nt\nta\ntak\ntaka\ntake\ntaki\ntako\ntakoj\ntam\nte\ntebe\ntebi\ntega\ntežak\ntežka\ntežki\ntežko\nti\ntista\ntiste\ntisti\ntisto\ntj.\ntja\nto\ntoda\ntorek\ntretja\ntretje\ntretji\ntri\ntu\ntudi\ntukaj\ntvoj\ntvoja\ntvoje\nu\nv\nvaju\nvam\nvas\nvaš\nvaša\nvaše\nve\nvedno\nvelik\nvelika\nveliki\nveliko\nvendar\nves\nveč\nvi\nvidva\nvii\nviii\nvisok\nvisoka\nvisoke\nvisoki\nvsa\nvsaj\nvsak\nvsaka\nvsakdo\nvsake\nvsaki\nvsakomur\nvse\nvsega\nvsi\nvso\nvčasih\nvčeraj\nx\nz\nza\nzadaj\nzadnji\nzakaj\nzaprta\nzaprti\nzaprto\nzdaj\nzelo\nzunaj\nč\nče\nčesto\nčetrta\nčetrtek\nčetrti\nčetrto\nčez\nčigav\nš\nšest\nšesta\nšesti\nšesto\nštiri\nž\nže\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/cs/__init__.py----------------------------------------
A:spacy.lang.cs.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
spacy.lang.cs.__init__.Czech(Language)
spacy.lang.cs.__init__.CzechDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/cs/stop_words.py----------------------------------------
A:spacy.lang.cs.stop_words.STOP_WORDS->set("\načkoli\nahoj\nale\nanebo\nano\nasi\naspoň\nběhem\nbez\nbeze\nblízko\nbohužel\nbrzo\nbude\nbudeme\nbudeš\nbudete\nbudou\nbudu\nbyl\nbyla\nbyli\nbylo\nbyly\nbys\nčau\nchce\nchceme\nchceš\nchcete\nchci\nchtějí\nchtít\nchut'\nchuti\nco\nčtrnáct\nčtyři\ndál\ndále\ndaleko\nděkovat\nděkujeme\nděkuji\nden\ndeset\ndevatenáct\ndevět\ndo\ndobrý\ndocela\ndva\ndvacet\ndvanáct\ndvě\nhodně\njá\njak\njde\nje\njeden\njedenáct\njedna\njedno\njednou\njedou\njeho\njejí\njejich\njemu\njen\njenom\nještě\njestli\njestliže\njí\njich\njím\njimi\njinak\njsem\njsi\njsme\njsou\njste\nkam\nkde\nkdo\nkdy\nkdyž\nke\nkolik\nkromě\nkterá\nkteré\nkteří\nkterý\nkvůli\nmá\nmají\nmálo\nmám\nmáme\nmáš\nmáte\nmé\nmě\nmezi\nmí\nmít\nmně\nmnou\nmoc\nmohl\nmohou\nmoje\nmoji\nmožná\nmůj\nmusí\nmůže\nmy\nna\nnad\nnade\nnám\nnámi\nnaproti\nnás\nnáš\nnaše\nnaši\nne\nně\nnebo\nnebyl\nnebyla\nnebyli\nnebyly\nněco\nnedělá\nnedělají\nnedělám\nneděláme\nneděláš\nneděláte\nnějak\nnejsi\nněkde\nněkdo\nnemají\nnemáme\nnemáte\nneměl\nněmu\nnení\nnestačí\nnevadí\nnež\nnic\nnich\nním\nnimi\nnula\nod\node\non\nona\noni\nono\nony\nosm\nosmnáct\npak\npatnáct\npět\npo\npořád\npotom\npozdě\npřed\npřes\npřese\npro\nproč\nprosím\nprostě\nproti\nprotože\nrovně\nse\nsedm\nsedmnáct\nšest\nšestnáct\nskoro\nsmějí\nsmí\nsnad\nspolu\nsta\nsté\nsto\nta\ntady\ntak\ntakhle\ntaky\ntam\ntamhle\ntamhleto\ntamto\ntě\ntebe\ntebou\nted'\ntedy\nten\nti\ntisíc\ntisíce\nto\ntobě\ntohle\ntoto\ntřeba\ntři\ntřináct\ntrošku\ntvá\ntvé\ntvoje\ntvůj\nty\nurčitě\nuž\nvám\nvámi\nvás\nváš\nvaše\nvaši\nve\nvečer\nvedle\nvlastně\nvšechno\nvšichni\nvůbec\nvy\nvždy\nza\nzač\nzatímco\nze\nže\n".split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/sk/__init__.py----------------------------------------
A:spacy.lang.sk.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
spacy.lang.sk.__init__.Slovak(Language)
spacy.lang.sk.__init__.SlovakDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/sk/stop_words.py----------------------------------------
A:spacy.lang.sk.stop_words.STOP_WORDS->set('\na\naby\naj\nak\nako\naký\nale\nalebo\nand\nani\nasi\navšak\naž\nba\nbez\nbol\nbola\nboli\nbolo\nbude\nbudem\nbudeme\nbudete\nbudeš\nbudú\nbuï\nbuď\nby\nbyť\ncez\ndnes\ndo\nešte\nfor\nho\nhoci\ni\niba\nich\nim\niné\niný\nja\nje\njeho\njej\njemu\nju\nk\nkam\nkaždá\nkaždé\nkaždí\nkaždý\nkde\nkedže\nkeï\nkeď\nkto\nktorou\nktorá\nktoré\nktorí\nktorý\nku\nlebo\nlen\nma\nmať\nmedzi\nmenej\nmi\nmna\nmne\nmnou\nmoja\nmoje\nmu\nmusieť\nmy\nmá\nmáte\nmòa\nmôcť\nmôj\nmôže\nna\nnad\nnami\nnaši\nnech\nneho\nnej\nnemu\nnež\nnich\nnie\nniektorý\nnielen\nnim\nnič\nno\nnová\nnové\nnoví\nnový\nnám\nnás\nnáš\nním\no\nod\nodo\nof\non\nona\noni\nono\nony\npo\npod\npodľa\npokiaľ\npotom\npre\npred\npredo\npreto\npretože\nprečo\npri\nprvá\nprvé\nprví\nprvý\npráve\npýta\ns\nsa\nseba\nsem\nsi\nsme\nso\nsom\nspäť\nste\nsvoj\nsvoje\nsvojich\nsvojím\nsvojími\nsú\nta\ntak\ntaký\ntakže\ntam\nte\nteba\ntebe\ntebou\nteda\ntej\nten\ntento\nthe\nti\ntie\ntieto\ntiež\nto\ntoho\ntohoto\ntom\ntomto\ntomu\ntomuto\ntoto\ntou\ntu\ntvoj\ntvojími\nty\ntá\ntáto\ntú\ntúto\ntým\ntýmto\ntě\nuž\nv\nvami\nvaše\nveï\nviac\nvo\nvy\nvám\nvás\nváš\nvšak\nvšetok\nz\nza\nzo\n\x9da\náno\nèi\nèo\nèí\nòom\nòou\nòu\nči\nčo\nďalšia\nďalšie\nďalší\nže\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/id/__init__.py----------------------------------------
A:spacy.lang.id.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.id.__init__.lex_attr_getters[NORM]->add_lookups(Language.Defaults.lex_attr_getters[NORM], BASE_NORMS, NORM_EXCEPTIONS)
A:spacy.lang.id.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS, TOKENIZER_EXCEPTIONS)
spacy.lang.id.__init__.Indonesian(Language)
spacy.lang.id.__init__.IndonesianDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/id/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/id/lex_attrs.py----------------------------------------
A:spacy.lang.id.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.id.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
A:spacy.lang.id.lex_attrs.(_, num)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('-')
spacy.lang.id.lex_attrs.is_currency(text)
spacy.lang.id.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/id/_tokenizer_exceptions_list.py----------------------------------------
A:spacy.lang.id._tokenizer_exceptions_list.ID_BASE_EXCEPTIONS->set('\naba-aba\nabah-abah\nabal-abal\nabang-abang\nabar-abar\nabong-abong\nabrit-abrit\nabrit-abritan\nabu-abu\nabuh-abuhan\nabuk-abuk\nabun-abun\nacak-acak\nacak-acakan\nacang-acang\nacap-acap\naci-aci\naci-acian\naci-acinya\naco-acoan\nad-blocker\nad-interim\nada-ada\nada-adanya\nada-adanyakah\nadang-adang\nadap-adapan\nadd-on\nadd-ons\nadik-adik\nadik-beradik\naduk-adukan\nafter-sales\nagak-agak\nagak-agih\nagama-agama\nagar-agar\nage-related\nagut-agut\nair-air\nair-cooled\nair-to-air\najak-ajak\najar-ajar\naji-aji\nakal-akal\nakal-akalan\nakan-akan\nakar-akar\nakar-akaran\nakhir-akhir\nakhir-akhirnya\naki-aki\naksi-aksi\nalah-mengalahi\nalai-belai\nalan-alan\nalang-alang\nalang-alangan\nalap-alap\nalat-alat\nali-ali\nalif-alifan\nalih-alih\naling-aling\naling-alingan\nalip-alipan\nall-electric\nall-in-one\nall-out\nall-time\nalon-alon\nalt-right\nalt-text\nalu-alu\nalu-aluan\nalun-alun\nalur-alur\nalur-aluran\nalways-on\namai-amai\namatir-amatiran\nambah-ambah\nambai-ambai\nambil-mengambil\nambreng-ambrengan\nambring-ambringan\nambu-ambu\nambung-ambung\namin-amin\namit-amit\nampai-ampai\namprung-amprungan\namung-amung\nanai-anai\nanak-anak\nanak-anakan\nanak-beranak\nanak-cucu\nanak-istri\nancak-ancak\nancang-ancang\nancar-ancar\nandang-andang\nandeng-andeng\naneh-aneh\nangan-angan\nanggar-anggar\nanggaran-red\nanggota-anggota\nanggung-anggip\nangin-angin\nangin-anginan\nangkal-angkal\nangkul-angkul\nangkup-angkup\nangkut-angkut\nani-ani\naning-aning\nanjang-anjang\nanjing-anjing\nanjung-anjung\nanjung-anjungan\nantah-berantah\nantar-antar\nantar-mengantar\nante-mortem\nantek-antek\nanter-anter\nantihuru-hara\nanting-anting\nantung-antung\nanyam-menganyam\nanyang-anyang\napa-apa\napa-apaan\napel-apel\napi-api\napit-apit\naplikasi-aplikasi\napotek-apotek\naprit-apritan\napu-apu\napung-apung\narah-arah\narak-arak\narak-arakan\naram-aram\narek-arek\narem-arem\nari-ari\nartis-artis\naru-aru\narung-arungan\nasa-asaan\nasal-asalan\nasal-muasal\nasal-usul\nasam-asaman\nasas-asas\naset-aset\nasmaul-husna\nasosiasi-asosiasi\nasuh-asuh\nasyik-asyiknya\natas-mengatasi\nati-ati\natung-atung\naturan-aturan\naudio-video\naudio-visual\nauto-brightness\nauto-complete\nauto-focus\nauto-play\nauto-update\navant-garde\nawan-awan\nawan-berawan\nawang-awang\nawang-gemawang\nawar-awar\nawat-awat\nawik-awik\nawut-awutan\nayah-anak\nayak-ayak\nayam-ayam\nayam-ayaman\nayang-ayang\nayat-ayat\nayeng-ayengan\nayun-temayun\nayut-ayutan\nba-bi-bu\nback-to-back\nback-up\nbadan-badan\nbade-bade\nbadut-badut\nbagi-bagi\nbahan-bahan\nbahu-membahu\nbaik-baik\nbail-out\nbajang-bajang\nbaji-baji\nbalai-balai\nbalam-balam\nbalas-berbalas\nbalas-membalas\nbale-bale\nbaling-baling\nball-playing\nbalon-balon\nbalut-balut\nband-band\nbandara-bandara\nbangsa-bangsa\nbangun-bangun\nbangunan-bangunan\nbank-bank\nbantah-bantah\nbantahan-bantahan\nbantal-bantal\nbanyak-banyak\nbapak-anak\nbapak-bapak\nbapak-ibu\nbapak-ibunya\nbarang-barang\nbarat-barat\nbarat-daya\nbarat-laut\nbarau-barau\nbare-bare\nbareng-bareng\nbari-bari\nbarik-barik\nbaris-berbaris\nbaru-baru\nbaru-batu\nbarung-barung\nbasa-basi\nbata-bata\nbatalyon-batalyon\nbatang-batang\nbatas-batas\nbatir-batir\nbatu-batu\nbatuk-batuk\nbatung-batung\nbau-bauan\nbawa-bawa\nbayan-bayan\nbayang-bayang\nbayi-bayi\nbea-cukai\nbedeng-bedeng\nbedil-bedal\nbedil-bedilan\nbegana-begini\nbek-bek\nbekal-bekalan\nbekerdom-kerdom\nbekertak-kertak\nbelang-belang\nbelat-belit\nbeliau-beliau\nbelu-belai\nbelum-belum\nbenar-benar\nbenda-benda\nbengang-bengut\nbenggal-benggil\nbengkal-bengkil\nbengkang-bengkok\nbengkang-bengkong\nbengkang-bengkung\nbenteng-benteng\nbentuk-bentuk\nbenua-benua\nber-selfie\nberabad-abad\nberabun-rabun\nberacah-acah\nberada-ada\nberadik-berkakak\nberagah-agah\nberagak-agak\nberagam-ragam\nberaja-raja\nberakit-rakit\nberaku-akuan\nberalu-aluan\nberalun-alun\nberamah-ramah\nberamah-ramahan\nberamah-tamah\nberamai-ramai\nberambai-ambai\nberambal-ambalan\nberambil-ambil\nberamuk-amuk\nberamuk-amukan\nberandai-andai\nberandai-randai\nberaneh-aneh\nberang-berang\nberangan-angan\nberanggap-anggapan\nberangguk-angguk\nberangin-angin\nberangka-angka\nberangka-angkaan\nberangkai-rangkai\nberangkap-rangkapan\nberani-berani\nberanja-anja\nberantai-rantai\nberapi-api\nberapung-apung\nberarak-arakan\nberas-beras\nberasak-asak\nberasak-asakan\nberasap-asap\nberasing-asingan\nberatus-ratus\nberawa-rawa\nberawas-awas\nberayal-ayalan\nberayun-ayun\nberbagai-bagai\nberbahas-bahasan\nberbahasa-bahasa\nberbaik-baikan\nberbait-bait\nberbala-bala\nberbalas-balasan\nberbalik-balik\nberbalun-balun\nberbanjar-banjar\nberbantah-bantah\nberbanyak-banyak\nberbarik-barik\nberbasa-basi\nberbasah-basah\nberbatu-batu\nberbayang-bayang\nberbecak-becak\nberbeda-beda\nberbedil-bedilan\nberbega-bega\nberbeka-beka\nberbelah-belah\nberbelakang-belakangan\nberbelang-belang\nberbelau-belauan\nberbeli-beli\nberbeli-belian\nberbelit-belit\nberbelok-belok\nberbenang-benang\nberbenar-benar\nberbencah-bencah\nberbencol-bencol\nberbenggil-benggil\nberbentol-bentol\nberbentong-bentong\nberberani-berani\nberbesar-besar\nberbidai-bidai\nberbiduk-biduk\nberbiku-biku\nberbilik-bilik\nberbinar-binar\nberbincang-bincang\nberbingkah-bingkah\nberbintang-bintang\nberbintik-bintik\nberbintil-bintil\nberbisik-bisik\nberbolak-balik\nberbolong-bolong\nberbondong-bondong\nberbongkah-bongkah\nberbuai-buai\nberbual-bual\nberbudak-budak\nberbukit-bukit\nberbulan-bulan\nberbunga-bunga\nberbuntut-buntut\nberbunuh-bunuhan\nberburu-buru\nberburuk-buruk\nberbutir-butir\nbercabang-cabang\nbercaci-cacian\nbercakap-cakap\nbercakar-cakaran\nbercamping-camping\nbercantik-cantik\nbercari-cari\nbercari-carian\nbercarik-carik\nbercarut-carut\nbercebar-cebur\nbercepat-cepat\nbercerai-berai\nbercerai-cerai\nbercetai-cetai\nberciap-ciap\nbercikun-cikun\nbercinta-cintaan\nbercita-cita\nberciut-ciut\nbercompang-camping\nberconteng-conteng\nbercoreng-coreng\nbercoreng-moreng\nbercuang-caing\nbercuit-cuit\nbercumbu-cumbu\nbercumbu-cumbuan\nbercura-bura\nbercura-cura\nberdada-dadaan\nberdahulu-dahuluan\nberdalam-dalam\nberdalih-dalih\nberdampung-dampung\nberdebar-debar\nberdecak-decak\nberdecap-decap\nberdecup-decup\nberdecut-decut\nberdedai-dedai\nberdegap-degap\nberdegar-degar\nberdeham-deham\nberdekah-dekah\nberdekak-dekak\nberdekap-dekapan\nberdekat-dekat\nberdelat-delat\nberdembai-dembai\nberdembun-dembun\nberdempang-dempang\nberdempet-dempet\nberdencing-dencing\nberdendam-dendaman\nberdengkang-dengkang\nberdengut-dengut\nberdentang-dentang\nberdentum-dentum\nberdentung-dentung\nberdenyar-denyar\nberdenyut-denyut\nberdepak-depak\nberdepan-depan\nberderai-derai\nberderak-derak\nberderam-deram\nberderau-derau\nberderik-derik\nberdering-dering\nberderung-derung\nberderus-derus\nberdesak-desakan\nberdesik-desik\nberdesing-desing\nberdesus-desus\nberdikit-dikit\nberdingkit-dingkit\nberdua-dua\nberduri-duri\nberduru-duru\nberduyun-duyun\nberebut-rebut\nberebut-rebutan\nberegang-regang\nberek-berek\nberembut-rembut\nberempat-empat\nberenak-enak\nberencel-encel\nbereng-bereng\nberenggan-enggan\nberenteng-renteng\nberesa-esaan\nberesah-resah\nberfoya-foya\nbergagah-gagahan\nbergagap-gagap\nbergagau-gagau\nbergalur-galur\nberganda-ganda\nberganjur-ganjur\nberganti-ganti\nbergarah-garah\nbergaruk-garuk\nbergaya-gaya\nbergegas-gegas\nbergelang-gelang\nbergelap-gelap\nbergelas-gelasan\nbergeleng-geleng\nbergemal-gemal\nbergembar-gembor\nbergembut-gembut\nbergepok-gepok\nbergerek-gerek\nbergesa-gesa\nbergilir-gilir\nbergolak-golak\nbergolek-golek\nbergolong-golong\nbergores-gores\nbergotong-royong\nbergoyang-goyang\nbergugus-gugus\nbergulung-gulung\nbergulut-gulut\nbergumpal-gumpal\nbergunduk-gunduk\nbergunung-gunung\nberhadap-hadapan\nberhamun-hamun\nberhandai-handai\nberhanyut-hanyut\nberhari-hari\nberhati-hati\nberhati-hatilah\nberhektare-hektare\nberhilau-hilau\nberhormat-hormat\nberhujan-hujan\nberhura-hura\nberi-beri\nberi-memberi\nberia-ia\nberia-ria\nberiak-riak\nberiba-iba\nberibu-ribu\nberigi-rigi\nberimpit-impit\nberindap-indap\nbering-bering\nberingat-ingat\nberinggit-ringgit\nberintik-rintik\nberiring-iring\nberiring-iringan\nberita-berita\nberjabir-jabir\nberjaga-jaga\nberjagung-jagung\nberjalan-jalan\nberjalar-jalar\nberjalin-jalin\nberjalur-jalur\nberjam-jam\nberjari-jari\nberjauh-jauhan\nberjegal-jegalan\nberjejal-jejal\nberjela-jela\nberjengkek-jengkek\nberjenis-jenis\nberjenjang-jenjang\nberjilid-jilid\nberjinak-jinak\nberjingkat-jingkat\nberjingkik-jingkik\nberjingkrak-jingkrak\nberjongkok-jongkok\nberjubel-jubel\nberjujut-jujutan\nberjulai-julai\nberjumbai-jumbai\nberjumbul-jumbul\nberjuntai-juntai\nberjurai-jurai\nberjurus-jurus\nberjuta-juta\nberka-li-kali\nberkabu-kabu\nberkaca-kaca\nberkaing-kaing\nberkait-kaitan\nberkala-kala\nberkali-kali\nberkamit-kamit\nberkanjar-kanjar\nberkaok-kaok\nberkarung-karung\nberkasak-kusuk\nberkasih-kasihan\nberkata-kata\nberkatak-katak\nberkecai-kecai\nberkecek-kecek\nberkecil-kecil\nberkecil-kecilan\nberkedip-kedip\nberkejang-kejang\nberkejap-kejap\nberkejar-kejaran\nberkelar-kelar\nberkelepai-kelepai\nberkelip-kelip\nberkelit-kelit\nberkelok-kelok\nberkelompok-kelompok\nberkelun-kelun\nberkembur-kembur\nberkempul-kempul\nberkena-kenaan\nberkenal-kenalan\nberkendur-kendur\nberkeok-keok\nberkepak-kepak\nberkepal-kepal\nberkeping-keping\nberkepul-kepul\nberkeras-kerasan\nberkering-kering\nberkeritik-keritik\nberkeruit-keruit\nberkerut-kerut\nberketai-ketai\nberketak-ketak\nberketak-ketik\nberketap-ketap\nberketap-ketip\nberketar-ketar\nberketi-keti\nberketil-ketil\nberketuk-ketak\nberketul-ketul\nberkial-kial\nberkian-kian\nberkias-kias\nberkias-kiasan\nberkibar-kibar\nberkilah-kilah\nberkilap-kilap\nberkilat-kilat\nberkilau-kilauan\nberkilo-kilo\nberkimbang-kimbang\nberkinja-kinja\nberkipas-kipas\nberkira-kira\nberkirim-kiriman\nberkisar-kisar\nberkoak-koak\nberkoar-koar\nberkobar-kobar\nberkobok-kobok\nberkocak-kocak\nberkodi-kodi\nberkolek-kolek\nberkomat-kamit\nberkopah-kopah\nberkoper-koper\nberkotak-kotak\nberkuat-kuat\nberkuat-kuatan\nberkumur-kumur\nberkunang-kunang\nberkunar-kunar\nberkunjung-kunjungan\nberkurik-kurik\nberkurun-kurun\nberkusau-kusau\nberkusu-kusu\nberkusut-kusut\nberkuting-kuting\nberkutu-kutuan\nberlabun-labun\nberlain-lainan\nberlaju-laju\nberlalai-lalai\nberlama-lama\nberlambai-lambai\nberlambak-lambak\nberlampang-lampang\nberlanggar-langgar\nberlapang-lapang\nberlapis-lapis\nberlapuk-lapuk\nberlarah-larah\nberlarat-larat\nberlari-lari\nberlari-larian\nberlarih-larih\nberlarik-larik\nberlarut-larut\nberlawak-lawak\nberlayap-layapan\nberlebih-lebih\nberlebih-lebihan\nberleha-leha\nberlekas-lekas\nberlekas-lekasan\nberlekat-lekat\nberlekuk-lekuk\nberlempar-lemparan\nberlena-lena\nberlengah-lengah\nberlenggak-lenggok\nberlenggek-lenggek\nberlenggok-lenggok\nberleret-leret\nberletih-letih\nberliang-liuk\nberlibat-libat\nberligar-ligar\nberliku-liku\nberlikur-likur\nberlimbak-limbak\nberlimpah-limpah\nberlimpap-limpap\nberlimpit-limpit\nberlinang-linang\nberlindak-lindak\nberlipat-lipat\nberlomba-lomba\nberlompok-lompok\nberloncat-loncatan\nberlopak-lopak\nberlubang-lubang\nberlusin-lusin\nbermaaf-maafan\nbermabuk-mabukan\nbermacam-macam\nbermain-main\nbermalam-malam\nbermalas-malas\nbermalas-malasan\nbermanik-manik\nbermanis-manis\nbermanja-manja\nbermasak-masak\nbermati-mati\nbermegah-megah\nbermemek-memek\nbermenung-menung\nbermesra-mesraan\nbermewah-mewah\nbermewah-mewahan\nberminggu-minggu\nberminta-minta\nberminyak-minyak\nbermuda-muda\nbermudah-mudah\nbermuka-muka\nbermula-mula\nbermuluk-muluk\nbermulut-mulut\nbernafsi-nafsi\nbernaka-naka\nbernala-nala\nbernanti-nanti\nberniat-niat\nbernyala-nyala\nberogak-ogak\nberoleng-oleng\nberolok-olok\nberomong-omong\nberoncet-roncet\nberonggok-onggok\nberorang-orang\nberoyal-royal\nberpada-pada\nberpadu-padu\nberpahit-pahit\nberpair-pair\nberpal-pal\nberpalu-palu\nberpalu-paluan\nberpalun-palun\nberpanas-panas\nberpandai-pandai\nberpandang-pandangan\nberpangkat-pangkat\nberpanjang-panjang\nberpantun-pantun\nberpasang-pasang\nberpasang-pasangan\nberpasuk-pasuk\nberpayah-payah\nberpeluh-peluh\nberpeluk-pelukan\nberpenat-penat\nberpencar-pencar\nberpendar-pendar\nberpenggal-penggal\nberperai-perai\nberperang-perangan\nberpesai-pesai\nberpesta-pesta\nberpesuk-pesuk\nberpetak-petak\nberpeti-peti\nberpihak-pihak\nberpijar-pijar\nberpikir-pikir\nberpikul-pikul\nberpilih-pilih\nberpilin-pilin\nberpindah-pindah\nberpintal-pintal\nberpirau-pirau\nberpisah-pisah\nberpolah-polah\nberpolok-polok\nberpongah-pongah\nberpontang-panting\nberporah-porah\nberpotong-potong\nberpotong-potongan\nberpuak-puak\nberpual-pual\nberpugak-pugak\nberpuing-puing\nberpukas-pukas\nberpuluh-puluh\nberpulun-pulun\nberpuntal-puntal\nberpura-pura\nberpusar-pusar\nberpusing-pusing\nberpusu-pusu\nberputar-putar\nberrumpun-rumpun\nbersaf-saf\nbersahut-sahutan\nbersakit-sakit\nbersalah-salahan\nbersalam-salaman\nbersalin-salin\nbersalip-salipan\nbersama-sama\nbersambar-sambaran\nbersambut-sambutan\nbersampan-sampan\nbersantai-santai\nbersapa-sapaan\nbersarang-sarang\nbersedan-sedan\nbersedia-sedia\nbersedu-sedu\nbersejuk-sejuk\nbersekat-sekat\nberselang-selang\nberselang-seli\nberselang-seling\nberselang-tenggang\nberselit-selit\nberseluk-beluk\nbersembunyi-sembunyi\nbersembunyi-sembunyian\nbersembur-semburan\nbersempit-sempit\nbersenang-senang\nbersenang-senangkan\nbersenda-senda\nbersendi-sendi\nbersenggang-senggang\nbersenggau-senggau\nbersepah-sepah\nbersepak-sepakan\nbersepi-sepi\nberserak-serak\nberseri-seri\nberseru-seru\nbersesak-sesak\nbersetai-setai\nbersia-sia\nbersiap-siap\nbersiar-siar\nbersih-bersih\nbersikut-sikutan\nbersilir-silir\nbersimbur-simburan\nbersinau-sinau\nbersopan-sopan\nbersorak-sorai\nbersuap-suapan\nbersudah-sudah\nbersuka-suka\nbersuka-sukaan\nbersuku-suku\nbersulang-sulang\nbersumpah-sumpahan\nbersungguh-sungguh\nbersungut-sungut\nbersunyi-sunyi\nbersuruk-surukan\nbersusah-susah\nbersusuk-susuk\nbersusuk-susukan\nbersutan-sutan\nbertabur-tabur\nbertahan-tahan\nbertahu-tahu\nbertahun-tahun\nbertajuk-tajuk\nbertakik-takik\nbertala-tala\nbertalah-talah\nbertali-tali\nbertalu-talu\nbertalun-talun\nbertambah-tambah\nbertanda-tandaan\nbertangis-tangisan\nbertangkil-tangkil\nbertanya-tanya\nbertarik-tarikan\nbertatai-tatai\nbertatap-tatapan\nbertatih-tatih\nbertawan-tawan\nbertawar-tawaran\nbertebu-tebu\nbertebu-tebukan\nberteguh-teguh\nberteguh-teguhan\nberteka-teki\nbertelang-telang\nbertelau-telau\nbertele-tele\nbertembuk-tembuk\nbertempat-tempat\nbertempuh-tempuh\nbertenang-tenang\nbertenggang-tenggangan\nbertentu-tentu\nbertepek-tepek\nberterang-terang\nberterang-terangan\nberteriak-teriak\nbertikam-tikaman\nbertimbal-timbalan\nbertimbun-timbun\nbertimpa-timpa\nbertimpas-timpas\nbertingkah-tingkah\nbertingkat-tingkat\nbertinjau-tinjauan\nbertiras-tiras\nbertitar-titar\nbertitik-titik\nbertoboh-toboh\nbertolak-tolak\nbertolak-tolakan\nbertolong-tolongan\nbertonjol-tonjol\nbertruk-truk\nbertua-tua\nbertua-tuaan\nbertual-tual\nbertubi-tubi\nbertukar-tukar\nbertukar-tukaran\nbertukas-tukas\nbertumpak-tumpak\nbertumpang-tindih\nbertumpuk-tumpuk\nbertunda-tunda\nbertunjuk-tunjukan\nbertura-tura\nberturut-turut\nbertutur-tutur\nberuas-ruas\nberubah-ubah\nberulang-alik\nberulang-ulang\nberumbai-rumbai\nberundak-undak\nberundan-undan\nberundung-undung\nberunggas-runggas\nberunggun-unggun\nberunggut-unggut\nberungkur-ungkuran\nberuntai-untai\nberuntun-runtun\nberuntung-untung\nberunyai-unyai\nberupa-rupa\nberura-ura\nberuris-uris\nberurut-urutan\nberwarna-warna\nberwarna-warni\nberwindu-windu\nberwiru-wiru\nberyang-yang\nbesar-besar\nbesar-besaran\nbetak-betak\nbeti-beti\nbetik-betik\nbetul-betul\nbiang-biang\nbiar-biar\nbiaya-biaya\nbicu-bicu\nbidadari-bidadari\nbidang-bidang\nbijak-bijaklah\nbiji-bijian\nbila-bila\nbilang-bilang\nbincang-bincang\nbincang-bincut\nbingkah-bingkah\nbini-binian\nbintang-bintang\nbintik-bintik\nbio-oil\nbiri-biri\nbiru-biru\nbiru-hitam\nbiru-kuning\nbisik-bisik\nbiti-biti\nblak-blakan\nblok-blok\nbocah-bocah\nbohong-bohong\nbohong-bohongan\nbola-bola\nbolak-balik\nbolang-baling\nboleh-boleh\nbom-bom\nbomber-bomber\nbonek-bonek\nbongkar-bangkir\nbongkar-membongkar\nbongkar-pasang\nboro-boro\nbos-bos\nbottom-up\nbox-to-box\nboyo-boyo\nbuah-buahan\nbuang-buang\nbuat-buatan\nbuaya-buaya\nbubun-bubun\nbugi-bugi\nbuild-up\nbuilt-in\nbuilt-up\nbuka-buka\nbuka-bukaan\nbuka-tutup\nbukan-bukan\nbukti-bukti\nbuku-buku\nbulan-bulan\nbulan-bulanan\nbulang-baling\nbulang-bulang\nbulat-bulat\nbuli-buli\nbulu-bulu\nbuluh-buluh\nbulus-bulus\nbunga-bunga\nbunga-bungaan\nbunuh-membunuh\nbunyi-bunyian\nbupati-bupati\nbupati-wakil\nburu-buru\nburung-burung\nburung-burungan\nbus-bus\nbusiness-to-business\nbusur-busur\nbutir-butir\nby-pass\nbye-bye\ncabang-cabang\ncabik-cabik\ncabik-mencabik\ncabup-cawabup\ncaci-maki\ncagub-cawagub\ncaing-caing\ncakar-mencakar\ncakup-mencakup\ncalak-calak\ncalar-balar\ncaleg-caleg\ncalo-calo\ncalon-calon\ncampang-camping\ncampur-campur\ncapres-cawapres\ncara-cara\ncari-cari\ncari-carian\ncarut-marut\ncatch-up\ncawali-cawawali\ncawe-cawe\ncawi-cawi\ncebar-cebur\ncelah-celah\ncelam-celum\ncelangak-celinguk\ncelas-celus\nceledang-celedok\ncelengkak-celengkok\ncelingak-celinguk\ncelung-celung\ncemas-cemas\ncenal-cenil\ncengar-cengir\ncengir-cengir\ncengis-cengis\ncengking-mengking\ncentang-perenang\ncepat-cepat\nceplas-ceplos\ncerai-berai\ncerita-cerita\nceruk-menceruk\nceruk-meruk\ncetak-biru\ncetak-mencetak\ncetar-ceter\ncheck-in\ncheck-ins\ncheck-up\nchit-chat\nchoki-choki\ncingak-cinguk\ncipika-cipiki\nciri-ciri\nciri-cirinya\ncirit-birit\ncita-cita\ncita-citaku\nclose-up\nclosed-circuit\ncoba-coba\ncobak-cabik\ncobar-cabir\ncola-cala\ncolang-caling\ncomat-comot\ncomot-comot\ncompang-camping\ncomputer-aided\ncomputer-generated\ncondong-mondong\ncongak-cangit\nconggah-canggih\ncongkah-cangkih\ncongkah-mangkih\ncopak-capik\ncopy-paste\ncorak-carik\ncorat-coret\ncoreng-moreng\ncoret-coret\ncrat-crit\ncross-border\ncross-dressing\ncrypto-ransomware\ncuang-caing\ncublak-cublak\ncubung-cubung\nculik-culik\ncuma-cuma\ncumi-cumi\ncungap-cangip\ncupu-cupu\ndabu-dabu\ndaerah-daerah\ndag-dag\ndag-dig-dug\ndaging-dagingan\ndahulu-mendahului\ndalam-dalam\ndali-dali\ndam-dam\ndanau-danau\ndansa-dansi\ndapil-dapil\ndapur-dapur\ndari-dari\ndaru-daru\ndasar-dasar\ndatang-datang\ndatang-mendatangi\ndaun-daun\ndaun-daunan\ndawai-dawai\ndayang-dayang\ndayung-mayung\ndebak-debuk\ndebu-debu\ndeca-core\ndecision-making\ndeep-lying\ndeg-degan\ndegap-degap\ndekak-dekak\ndekat-dekat\ndengar-dengaran\ndengking-mendengking\ndepartemen-departemen\ndepo-depo\ndeputi-deputi\ndesa-desa\ndesa-kota\ndesas-desus\ndetik-detik\ndewa-dewa\ndewa-dewi\ndewan-dewan\ndewi-dewi\ndial-up\ndiam-diam\ndibayang-bayangi\ndibuat-buat\ndiiming-imingi\ndilebih-lebihkan\ndimana-mana\ndimata-matai\ndinas-dinas\ndinul-Islam\ndiobok-obok\ndiolok-olok\ndireksi-direksi\ndirektorat-direktorat\ndirjen-dirjen\ndirut-dirut\nditunggu-tunggu\ndivisi-divisi\ndo-it-yourself\ndoa-doa\ndog-dog\ndoggy-style\ndokok-dokok\ndolak-dalik\ndor-doran\ndorong-mendorong\ndosa-dosa\ndress-up\ndrive-in\ndua-dua\ndua-duaan\ndua-duanya\ndubes-dubes\nduduk-duduk\ndugaan-dugaan\ndulang-dulang\nduri-duri\nduta-duta\ndwi-kewarganegaraan\ne-arena\ne-billing\ne-budgeting\ne-cctv\ne-class\ne-commerce\ne-counting\ne-elektronik\ne-entertainment\ne-evolution\ne-faktur\ne-filing\ne-fin\ne-form\ne-government\ne-govt\ne-hakcipta\ne-id\ne-info\ne-katalog\ne-ktp\ne-leadership\ne-lhkpn\ne-library\ne-loket\ne-m1\ne-money\ne-news\ne-nisn\ne-npwp\ne-paspor\ne-paten\ne-pay\ne-perda\ne-perizinan\ne-planning\ne-polisi\ne-power\ne-punten\ne-retribusi\ne-samsat\ne-sport\ne-store\ne-tax\ne-ticketing\ne-tilang\ne-toll\ne-visa\ne-voting\ne-wallet\ne-warong\necek-ecek\neco-friendly\neco-park\nedan-edanan\neditor-editor\neditor-in-chief\nefek-efek\nekonomi-ekonomi\neksekutif-legislatif\nekspor-impor\nelang-elang\nelemen-elemen\nemak-emak\nembuh-embuhan\nempat-empat\nempek-empek\nempet-empetan\nempok-empok\nempot-empotan\nenak-enak\nencal-encal\nend-to-end\nend-user\nendap-endap\nendut-endut\nendut-endutan\nengah-engah\nengap-engap\nenggan-enggan\nengkah-engkah\nengket-engket\nentah-berentah\nenten-enten\nentry-level\nequity-linked\nerang-erot\nerat-erat\nerek-erek\nereng-ereng\nerong-erong\nesek-esek\nex-officio\nexchange-traded\nexercise-induced\nextra-time\nface-down\nface-to-face\nfair-play\nfakta-fakta\nfaktor-faktor\nfakultas-fakultas\nfase-fase\nfast-food\nfeed-in\nfifty-fifty\nfile-file\nfirst-leg\nfirst-team\nfitur-fitur\nfitur-fiturnya\nfixed-income\nflip-flop\nflip-plop\nfly-in\nfollow-up\nfoto-foto\nfoya-foya\nfraksi-fraksi\nfree-to-play\nfront-end\nfungsi-fungsi\ngaba-gaba\ngabai-gabai\ngada-gada\ngading-gading\ngadis-gadis\ngado-gado\ngail-gail\ngajah-gajah\ngajah-gajahan\ngala-gala\ngaleri-galeri\ngali-gali\ngali-galian\ngaling-galing\ngalu-galu\ngamak-gamak\ngambar-gambar\ngambar-menggambar\ngamit-gamitan\ngampang-gampangan\ngana-gini\nganal-ganal\nganda-berganda\nganjal-mengganjal\nganjil-genap\nganteng-ganteng\ngantung-gantung\ngapah-gopoh\ngara-gara\ngarah-garah\ngaris-garis\ngasak-gasakan\ngatal-gatal\ngaun-gaun\ngawar-gawar\ngaya-gayanya\ngayang-gayang\nge-er\ngebyah-uyah\ngebyar-gebyar\ngedana-gedini\ngedebak-gedebuk\ngedebar-gedebur\ngedung-gedung\ngelang-gelang\ngelap-gelapan\ngelar-gelar\ngelas-gelas\ngelembung-gelembungan\ngeleng-geleng\ngeli-geli\ngeliang-geliut\ngeliat-geliut\ngembar-gembor\ngembrang-gembreng\ngempul-gempul\ngempur-menggempur\ngendang-gendang\ngengsi-gengsian\ngenjang-genjot\ngenjot-genjotan\ngenjrang-genjreng\ngenome-wide\ngeo-politik\ngerabak-gerubuk\ngerak-gerik\ngerak-geriknya\ngerakan-gerakan\ngerbas-gerbus\ngereja-gereja\ngereng-gereng\ngeriak-geriuk\ngerit-gerit\ngerot-gerot\ngeruh-gerah\ngetak-getuk\ngetem-getem\ngeti-geti\ngial-gial\ngial-giul\ngila-gila\ngila-gilaan\ngilang-gemilang\ngilap-gemilap\ngili-gili\ngiling-giling\ngilir-bergilir\nginang-ginang\ngirap-girap\ngirik-girik\ngiring-giring\ngo-auto\ngo-bills\ngo-bluebird\ngo-box\ngo-car\ngo-clean\ngo-food\ngo-glam\ngo-jek\ngo-kart\ngo-mart\ngo-massage\ngo-med\ngo-points\ngo-pulsa\ngo-ride\ngo-send\ngo-shop\ngo-tix\ngo-to-market\ngoak-goak\ngoal-line\ngol-gol\ngolak-galik\ngondas-gandes\ngonjang-ganjing\ngonjlang-ganjling\ngonta-ganti\ngontok-gontokan\ngorap-gorap\ngorong-gorong\ngotong-royong\ngresek-gresek\ngua-gua\ngual-gail\ngubernur-gubernur\ngudu-gudu\ngula-gula\ngulang-gulang\ngulung-menggulung\nguna-ganah\nguna-guna\ngundala-gundala\nguntang-guntang\ngunung-ganang\ngunung-gemunung\ngunung-gunungan\nguru-guru\nhabis-habis\nhabis-habisan\nhak-hak\nhak-hal\nhakim-hakim\nhal-hal\nhalai-balai\nhalf-time\nhama-hama\nhampir-hampir\nhancur-hancuran\nhancur-menghancurkan\nhands-free\nhands-on\nhang-out\nhantu-hantu\nhappy-happy\nharap-harap\nharap-harapan\nhard-disk\nharga-harga\nhari-hari\nharimau-harimau\nharum-haruman\nhasil-hasil\nhasta-wara\nhat-trick\nhati-hati\nhati-hatilah\nhead-mounted\nhead-to-head\nhead-up\nheads-up\nheavy-duty\nhebat-hebatan\nhewan-hewan\nhexa-core\nhidup-hidup\nhidup-mati\nhila-hila\nhilang-hilang\nhina-menghinakan\nhip-hop\nhiru-biru\nhiru-hara\nhiruk-pikuk\nhitam-putih\nhitung-hitung\nhitung-hitungan\nhormat-menghormati\nhot-swappable\nhotel-hotel\nhow-to\nhubar-habir\nhubaya-hubaya\nhukum-red\nhukuman-hukuman\nhula-hoop\nhula-hula\nhulu-hilir\nhumas-humas\nhura-hura\nhuru-hara\nibar-ibar\nibu-anak\nibu-ibu\nicak-icak\nicip-icip\nidam-idam\nide-ide\nigau-igauan\nikan-ikan\nikut-ikut\nikut-ikutan\nilam-ilam\nilat-ilatan\nilmu-ilmu\nimbang-imbangan\niming-iming\nimut-imut\ninang-inang\ninca-binca\nincang-incut\nindustri-industri\ningar-bingar\ningar-ingar\ningat-ingat\ningat-ingatan\ningau-ingauan\ninggang-inggung\ninjak-injak\ninput-output\ninstansi-instansi\ninstant-on\ninstrumen-instrumen\ninter-governmental\nira-ira\nirah-irahan\niras-iras\niring-iringan\niris-irisan\nisak-isak\nisat-bb\niseng-iseng\nistana-istana\nistri-istri\nisu-isu\niya-iya\njabatan-jabatan\njadi-jadian\njagoan-jagoan\njaja-jajaan\njaksa-jaksa\njala-jala\njalan-jalan\njali-jali\njalin-berjalin\njalin-menjalin\njam-jam\njamah-jamahan\njambak-jambakan\njambu-jambu\njampi-jampi\njanda-janda\njangan-jangan\njanji-janji\njarang-jarang\njari-jari\njaring-jaring\njarum-jarum\njasa-jasa\njatuh-bangun\njauh-dekat\njauh-jauh\njawi-jawi\njebar-jebur\njebat-jebatan\njegal-jegalan\njejak-jejak\njelang-menjelang\njelas-jelas\njelur-jelir\njembatan-jembatan\njenazah-jenazah\njendal-jendul\njenderal-jenderal\njenggar-jenggur\njenis-jenis\njenis-jenisnya\njentik-jentik\njerah-jerih\njinak-jinak\njiwa-jiwa\njoli-joli\njolong-jolong\njongkang-jangking\njongkar-jangkir\njongkat-jangkit\njor-joran\njotos-jotosan\njuak-juak\njual-beli\njuang-juang\njulo-julo\njulung-julung\njulur-julur\njumbai-jumbai\njungkang-jungkit\njungkat-jungkit\njurai-jurai\nkabang-kabang\nkabar-kabari\nkabir-kabiran\nkabruk-kabrukan\nkabu-kabu\nkabupaten-kabupaten\nkabupaten-kota\nkaca-kaca\nkacang-kacang\nkacang-kacangan\nkacau-balau\nkadang-kadang\nkader-kader\nkades-kades\nkadis-kadis\nkail-kail\nkain-kain\nkait-kait\nkakak-adik\nkakak-beradik\nkakak-kakak\nkakek-kakek\nkakek-nenek\nkaki-kaki\nkala-kala\nkalau-kalau\nkaleng-kalengan\nkali-kalian\nkalimat-kalimat\nkalung-kalung\nkalut-malut\nkambing-kambing\nkamit-kamit\nkampung-kampung\nkampus-kampus\nkanak-kanak\nkanak-kanan\nkanan-kanak\nkanan-kiri\nkangen-kangenan\nkanwil-kanwil\nkapa-kapa\nkapal-kapal\nkapan-kapan\nkapolda-kapolda\nkapolres-kapolres\nkapolsek-kapolsek\nkapu-kapu\nkarang-karangan\nkarang-mengarang\nkareseh-peseh\nkarut-marut\nkarya-karya\nkasak-kusuk\nkasus-kasus\nkata-kata\nkatang-katang\nkava-kava\nkawa-kawa\nkawan-kawan\nkawin-cerai\nkawin-mawin\nkayu-kayu\nkayu-kayuan\nke-Allah-an\nkeabu-abuan\nkearab-araban\nkeasyik-asyikan\nkebarat-baratan\nkebasah-basahan\nkebat-kebit\nkebata-bataan\nkebayi-bayian\nkebelanda-belandaan\nkeberlarut-larutan\nkebesar-hatian\nkebiasaan-kebiasaan\nkebijakan-kebijakan\nkebiru-biruan\nkebudak-budakan\nkebun-kebun\nkebut-kebutan\nkecamatan-kecamatan\nkecentang-perenangan\nkecil-kecil\nkecil-kecilan\nkecil-mengecil\nkecokelat-cokelatan\nkecomak-kecimik\nkecuh-kecah\nkedek-kedek\nkedekak-kedekik\nkedesa-desaan\nkedubes-kedubes\nkedutaan-kedutaan\nkeempat-empatnya\nkegadis-gadisan\nkegelap-gelapan\nkegiatan-kegiatan\nkegila-gilaan\nkegirang-girangan\nkehati-hatian\nkeheran-heranan\nkehijau-hijauan\nkehitam-hitaman\nkeinggris-inggrisan\nkejaga-jagaan\nkejahatan-kejahatan\nkejang-kejang\nkejar-kejar\nkejar-kejaran\nkejar-mengejar\nkejingga-jinggaan\nkejut-kejut\nkejutan-kejutan\nkekabur-kaburan\nkekanak-kanakan\nkekoboi-koboian\nkekota-kotaan\nkekuasaan-kekuasaan\nkekuning-kuningan\nkelak-kelik\nkelak-keluk\nkelaki-lakian\nkelang-kelok\nkelap-kelip\nkelasah-kelusuh\nkelek-kelek\nkelek-kelekan\nkelemak-kelemek\nkelik-kelik\nkelip-kelip\nkelompok-kelompok\nkelontang-kelantung\nkeluar-masuk\nkelurahan-kelurahan\nkelusuh-kelasah\nkelut-melut\nkemak-kemik\nkemalu-maluan\nkemana-mana\nkemanja-manjaan\nkemarah-marahan\nkemasam-masaman\nkemati-matian\nkembang-kembang\nkemenpan-rb\nkementerian-kementerian\nkemerah-merahan\nkempang-kempis\nkempas-kempis\nkemuda-mudaan\nkena-mengena\nkenal-mengenal\nkenang-kenangan\nkencang-kencung\nkencing-mengencingi\nkencrang-kencring\nkendang-kendang\nkendang-kendangan\nkeningrat-ningratan\nkentung-kentung\nkenyat-kenyit\nkepala-kepala\nkepala-kepalaan\nkepandir-pandiran\nkepang-kepot\nkeperak-perakan\nkepetah-lidahan\nkepilu-piluan\nkeping-keping\nkepucat-pucatan\nkepuh-kepuh\nkepura-puraan\nkeputih-putihan\nkerah-kerahan\nkerancak-rancakan\nkerang-kerangan\nkerang-keroh\nkerang-kerot\nkerang-keruk\nkerang-kerung\nkerap-kerap\nkeras-mengerasi\nkercap-kercip\nkercap-kercup\nkeriang-keriut\nkerja-kerja\nkernyat-kernyut\nkerobak-kerabit\nkerobak-kerobek\nkerobak-kerobik\nkerobat-kerabit\nkerong-kerong\nkeropas-kerapis\nkertak-kertuk\nkertap-kertap\nkeruntang-pungkang\nkesalahan-kesalahan\nkesap-kesip\nkesemena-menaan\nkesenak-senakan\nkesewenang-wenangan\nkesia-siaan\nkesik-kesik\nkesipu-sipuan\nkesu-kesi\nkesuh-kesih\nkesuk-kesik\nketakar-keteker\nketakutan-ketakutan\nketap-ketap\nketap-ketip\nketar-ketir\nketentuan-ketentuan\nketergesa-gesaan\nketi-keti\nketidur-tiduran\nketiga-tiganya\nketir-ketir\nketua-ketua\nketua-tuaan\nketuan-tuanan\nkeungu-unguan\nkewangi-wangian\nki-ka\nkia-kia\nkiai-kiai\nkiak-kiak\nkial-kial\nkiang-kiut\nkiat-kiat\nkibang-kibut\nkicang-kecoh\nkicang-kicu\nkick-off\nkida-kida\nkijang-kijang\nkilau-mengilau\nkili-kili\nkilik-kilik\nkincir-kincir\nkios-kios\nkira-kira\nkira-kiraan\nkiri-kanan\nkirim-berkirim\nkisah-kisah\nkisi-kisi\nkitab-kitab\nkitang-kitang\nkiu-kiu\nklaim-klaim\nklik-klikan\nklip-klip\nklub-klub\nkluntang-klantung\nknock-knock\nknock-on\nknock-out\nko-as\nko-pilot\nkoak-koak\nkoboi-koboian\nkocah-kacih\nkocar-kacir\nkodam-kodam\nkode-kode\nkodim-kodim\nkodok-kodok\nkolang-kaling\nkole-kole\nkoleh-koleh\nkolong-kolong\nkoma-koma\nkomat-kamit\nkomisaris-komisaris\nkomisi-komisi\nkomite-komite\nkomoditas-komoditas\nkongko-kongko\nkonsulat-konsulat\nkonsultan-konsultan\nkontal-kantil\nkontang-kanting\nkontra-terorisme\nkontrak-kontrak\nkonvensi-konvensi\nkopat-kapit\nkoperasi-koperasi\nkopi-kopi\nkoran-koran\nkoreng-koreng\nkos-kosan\nkosak-kasik\nkota-kota\nkota-wakil\nkotak-katik\nkotak-kotak\nkoyak-koyak\nkuas-kuas\nkuat-kuat\nkubu-kubuan\nkucar-kacir\nkucing-kucing\nkucing-kucingan\nkuda-kuda\nkuda-kudaan\nkudap-kudap\nkue-kue\nkulah-kulah\nkulak-kulak\nkulik-kulik\nkulum-kulum\nkumat-kamit\nkumpul-kumpul\nkunang-kunang\nkunar-kunar\nkung-fu\nkuning-hitam\nkupat-kapit\nkupu-kupu\nkura-kura\nkurang-kurang\nkusat-mesat\nkutat-kutet\nkuti-kuti\nkuwung-kuwung\nkyai-kyai\nlaba-laba\nlabi-labi\nlabu-labu\nlaga-laga\nlagi-lagi\nlagu-lagu\nlaguh-lagah\nlain-lain\nlaki-laki\nlalu-lalang\nlalu-lintas\nlama-kelamaan\nlama-lama\nlamat-lamat\nlambat-lambat\nlampion-lampion\nlampu-lampu\nlancang-lancang\nlancar-lancar\nlangak-longok\nlanggar-melanggar\nlangit-langit\nlangkah-langka\nlangkah-langkah\nlanja-lanjaan\nlapas-lapas\nlapat-lapat\nlaporan-laporan\nlaptop-tablet\nlarge-scale\nlari-lari\nlari-larian\nlaskar-laskar\nlauk-pauk\nlaun-laun\nlaut-timur\nlawah-lawah\nlawak-lawak\nlawan-lawan\nlawi-lawi\nlayang-layang\nlayu-layuan\nlebih-lebih\nlecet-lecet\nlegak-legok\nlegum-legum\nlegup-legup\nleha-leha\nlekak-lekuk\nlekap-lekup\nlekas-lekas\nlekat-lekat\nlekuh-lekih\nlekum-lekum\nlekup-lekap\nlembaga-lembaga\nlempar-lemparan\nlenggak-lenggok\nlenggok-lenggok\nlenggut-lenggut\nlengket-lengket\nlentam-lentum\nlentang-lentok\nlentang-lentung\nlepa-lepa\nlerang-lerang\nlereng-lereng\nlese-majeste\nletah-letai\nlete-lete\nletuk-letuk\nletum-letum\nletup-letup\nleyeh-leyeh\nliang-liuk\nliang-liut\nliar-liar\nliat-liut\nlidah-lidah\nlife-toxins\nliga-liga\nlight-emitting\nlika-liku\nlil-alamin\nlilin-lilin\nline-up\nlintas-selat\nlipat-melipat\nliquid-cooled\nlithium-ion\nlithium-polymer\nliuk-liuk\nliung-liung\nlobi-lobi\nlock-up\nlocked-in\nlokasi-lokasi\nlong-term\nlongak-longok\nlontang-lanting\nlontang-lantung\nlopak-lapik\nlopak-lopak\nlow-cost\nlow-density\nlow-end\nlow-light\nlow-multi\nlow-pass\nlucu-lucu\nluka-luka\nlukisan-lukisan\nlumba-lumba\nlumi-lumi\nluntang-lantung\nlupa-lupa\nlupa-lupaan\nlurah-camat\nmaaf-memaafkan\nmabuk-mabukan\nmabul-mabul\nmacam-macam\nmacan-macanan\nmachine-to-machine\nmafia-mafia\nmahasiswa-mahasiswi\nmahasiswa/i\nmahi-mahi\nmain-main\nmain-mainan\nmain-mainlah\nmajelis-majelis\nmaju-mundur\nmakam-makam\nmakan-makan\nmakan-makanan\nmakanan-red\nmake-up\nmaki-maki\nmaki-makian\nmal-mal\nmalai-malai\nmalam-malam\nmalar-malar\nmalas-malasan\nmali-mali\nmalu-malu\nmama-mama\nman-in-the-middle\nmana-mana\nmanajer-manajer\nmanik-manik\nmanis-manis\nmanis-manisan\nmarah-marah\nmark-up\nmas-mas\nmasa-masa\nmasak-masak\nmasalah-masalah\nmash-up\nmasing-masing\nmasjid-masjid\nmasuk-keluar\nmat-matan\nmata-mata\nmatch-fixing\nmati-mati\nmati-matian\nmaya-maya\nmayat-mayat\nmayday-mayday\nmedia-media\nmega-bintang\nmega-tsunami\nmegal-megol\nmegap-megap\nmeger-meger\nmegrek-megrek\nmelak-melak\nmelambai-lambai\nmelambai-lambaikan\nmelambat-lambatkan\nmelaun-laun\nmelawak-lawak\nmelayang-layang\nmelayap-layap\nmelayap-layapkan\nmelebih-lebihi\nmelebih-lebihkan\nmelejang-lejangkan\nmelek-melekan\nmeleleh-leleh\nmelengah-lengah\nmelihat-lihat\nmelimpah-limpah\nmelincah-lincah\nmeliuk-liuk\nmelolong-lolong\nmelompat-lompat\nmeloncat-loncat\nmelonco-lonco\nmelongak-longok\nmelonjak-lonjak\nmemacak-macak\nmemada-madai\nmemadan-madan\nmemaki-maki\nmemaksa-maksa\nmemanas-manasi\nmemancit-mancitkan\nmemandai-mandai\nmemanggil-manggil\nmemanis-manis\nmemanjut-manjut\nmemantas-mantas\nmemasak-masak\nmemata-matai\nmematah-matah\nmematuk-matuk\nmematut-matut\nmemau-mau\nmemayah-mayahkan\nmembaca-baca\nmembacah-bacah\nmembagi-bagikan\nmembalik-balik\nmembangkit-bangkit\nmembarut-barut\nmembawa-bawa\nmembayang-bayangi\nmembayang-bayangkan\nmembeda-bedakan\nmembelai-belai\nmembeli-beli\nmembelit-belitkan\nmembelu-belai\nmembenar-benar\nmembenar-benari\nmemberai-beraikan\nmembesar-besar\nmembesar-besarkan\nmembikin-bikin\nmembilah-bilah\nmembolak-balikkan\nmembongkar-bangkir\nmembongkar-bongkar\nmembuang-buang\nmembuat-buat\nmembulan-bulani\nmembunga-bungai\nmembungkuk-bungkuk\nmemburu-buru\nmemburu-burukan\nmemburuk-burukkan\nmemelintir-melintir\nmemencak-mencak\nmemencar-mencar\nmemercik-mercik\nmemetak-metak\nmemetang-metangkan\nmemetir-metir\nmemijar-mijar\nmemikir-mikir\nmemikir-mikirkan\nmemilih-milih\nmemilin-milin\nmeminang-minang\nmeminta-minta\nmemisah-misahkan\nmemontang-mantingkan\nmemorak-perandakan\nmemorak-porandakan\nmemotong-motong\nmemperamat-amat\nmemperamat-amatkan\nmemperbagai-bagaikan\nmemperganda-gandakan\nmemperganduh-ganduhkan\nmemperimpit-impitkan\nmemperkuda-kudakan\nmemperlengah-lengah\nmemperlengah-lengahkan\nmempermacam-macamkan\nmemperolok-olok\nmemperolok-olokkan\nmempersama-samakan\nmempertubi-tubi\nmempertubi-tubikan\nmemperturut-turutkan\nmemuja-muja\nmemukang-mukang\nmemulun-mulun\nmemundi-mundi\nmemundi-mundikan\nmemutar-mutar\nmemuyu-muyu\nmen-tweet\nmenagak-nagak\nmenakut-nakuti\nmenang-kalah\nmenanjur-nanjur\nmenanti-nanti\nmenari-nari\nmencabik-cabik\nmencabik-cabikkan\nmencacah-cacah\nmencaing-caing\nmencak-mencak\nmencakup-cakup\nmencapak-capak\nmencari-cari\nmencarik-carik\nmencarik-carikkan\nmencarut-carut\nmencengis-cengis\nmencepak-cepak\nmencepuk-cepuk\nmencerai-beraikan\nmencetai-cetai\nmenciak-ciak\nmenciap-ciap\nmenciar-ciar\nmencita-citakan\nmencium-cium\nmenciut-ciut\nmencla-mencle\nmencoang-coang\nmencoba-coba\nmencocok-cocok\nmencolek-colek\nmenconteng-conteng\nmencubit-cubit\nmencucuh-cucuh\nmencucuh-cucuhkan\nmencuri-curi\nmendecap-decap\nmendegam-degam\nmendengar-dengar\nmendengking-dengking\nmendengus-dengus\nmendengut-dengut\nmenderai-deraikan\nmenderak-derakkan\nmenderau-derau\nmenderu-deru\nmendesas-desuskan\nmendesus-desus\nmendetap-detap\nmendewa-dewakan\nmendudu-dudu\nmenduga-duga\nmenebu-nebu\nmenegur-neguri\nmenepak-nepak\nmenepak-nepakkan\nmengabung-ngabung\nmengaci-acikan\nmengacu-acu\nmengada-ada\nmengada-ngada\nmengadang-adangi\nmengaduk-aduk\nmengagak-agak\nmengagak-agihkan\nmengagut-agut\nmengais-ngais\nmengalang-alangi\nmengali-ali\nmengalur-alur\nmengamang-amang\nmengamat-amati\nmengambai-ambaikan\nmengambang-ambang\nmengambung-ambung\nmengambung-ambungkan\nmengamit-ngamitkan\nmengancai-ancaikan\nmengancak-ancak\nmengancar-ancar\nmengangan-angan\nmengangan-angankan\nmengangguk-angguk\nmenganggut-anggut\nmengangin-anginkan\nmengangkat-angkat\nmenganjung-anjung\nmenganjung-anjungkan\nmengap-mengap\nmengapa-apai\nmengapi-apikan\nmengarah-arahi\nmengarang-ngarang\nmengata-ngatai\nmengatup-ngatupkan\nmengaum-aum\nmengaum-aumkan\nmengejan-ejan\nmengejar-ngejar\nmengejut-ngejuti\nmengelai-ngelai\nmengelepik-ngelepik\nmengelip-ngelip\nmengelu-elukan\nmengelus-elus\nmengembut-embut\nmengempas-empaskan\nmengenap-enapkan\nmengendap-endap\nmengenjak-enjak\nmengentak-entak\nmengentak-entakkan\nmengepak-ngepak\nmengepak-ngepakkan\nmengepal-ngepalkan\nmengerjap-ngerjap\nmengerling-ngerling\nmengertak-ngertakkan\nmengesot-esot\nmenggaba-gabai\nmenggali-gali\nmenggalur-galur\nmenggamak-gamak\nmenggamit-gamitkan\nmenggapai-gapai\nmenggapai-gapaikan\nmenggaruk-garuk\nmenggebu-gebu\nmenggebyah-uyah\nmenggeleng-gelengkan\nmenggelepar-gelepar\nmenggelepar-geleparkan\nmenggeliang-geliutkan\nmenggelinding-gelinding\nmenggemak-gemak\nmenggembar-gemborkan\nmenggerak-gerakkan\nmenggerecak-gerecak\nmenggesa-gesakan\nmenggili-gili\nmenggodot-godot\nmenggolak-galikkan\nmenggorek-gorek\nmenggoreng-goreng\nmenggosok-gosok\nmenggoyang-goyangkan\nmengguit-guit\nmenghalai-balaikan\nmenghalang-halangi\nmenghambur-hamburkan\nmenghinap-hinap\nmenghitam-memutihkan\nmenghitung-hitung\nmenghubung-hubungkan\nmenghujan-hujankan\nmengiang-ngiang\nmengibar-ngibarkan\nmengibas-ngibas\nmengibas-ngibaskan\nmengidam-idamkan\nmengilah-ngilahkan\nmengilai-ilai\nmengilat-ngilatkan\nmengilik-ngilik\nmengimak-imak\nmengimbak-imbak\nmengiming-iming\nmengincrit-incrit\nmengingat-ingat\nmenginjak-injak\nmengipas-ngipas\nmengira-ngira\nmengira-ngirakan\nmengiras-iras\nmengiras-irasi\nmengiris-iris\nmengitar-ngitar\nmengitik-ngitik\nmengodol-odol\nmengogok-ogok\nmengolak-alik\nmengolak-alikkan\nmengolang-aling\nmengolang-alingkan\nmengoleng-oleng\nmengolok-olok\nmengombang-ambing\nmengombang-ambingkan\nmengongkang-ongkang\nmengongkok-ongkok\nmengonyah-anyih\nmengopak-apik\nmengorak-arik\nmengorat-oret\nmengorek-ngorek\nmengoret-oret\nmengorok-orok\nmengotak-atik\nmengotak-ngatikkan\nmengotak-ngotakkan\nmengoyak-ngoyak\nmengoyak-ngoyakkan\nmengoyak-oyak\nmenguar-nguarkan\nmenguar-uarkan\nmengubah-ubah\nmengubek-ubek\nmenguber-uber\nmengubit-ubit\nmengubrak-abrik\nmengucar-ngacirkan\nmengucek-ngucek\nmengucek-ucek\nmenguik-uik\nmenguis-uis\nmengulang-ulang\nmengulas-ulas\nmengulit-ulit\nmengulum-ngulum\nmengulur-ulur\nmenguman-uman\nmengumbang-ambingkan\nmengumpak-umpak\nmengungkat-ungkat\nmengungkit-ungkit\nmengupa-upa\nmengurik-urik\nmengusil-usil\nmengusil-usilkan\nmengutak-atik\nmengutak-ngatikkan\nmengutik-ngutik\nmengutik-utik\nmenika-nika\nmenimang-nimang\nmenimbang-nimbang\nmenimbun-nimbun\nmenimpang-nimpangkan\nmeningkat-ningkat\nmeniru-niru\nmenit-menit\nmenitar-nitarkan\nmeniup-niup\nmenjadi-jadi\nmenjadi-jadikan\nmenjedot-jedotkan\nmenjelek-jelekkan\nmenjengek-jengek\nmenjengit-jengit\nmenjerit-jerit\nmenjilat-jilat\nmenjungkat-jungkit\nmenko-menko\nmenlu-menlu\nmenonjol-nonjolkan\nmentah-mentah\nmentang-mentang\nmenteri-menteri\nmentul-mentul\nmenuding-nuding\nmenumpah-numpahkan\nmenunda-nunda\nmenunduk-nunduk\nmenusuk-nusuk\nmenyala-nyala\nmenyama-nyama\nmenyama-nyamai\nmenyambar-nyambar\nmenyangkut-nyangkutkan\nmenyanjung-nyanjung\nmenyanjung-nyanjungkan\nmenyapu-nyapu\nmenyarat-nyarat\nmenyayat-nyayat\nmenyedang-nyedang\nmenyedang-nyedangkan\nmenyelang-nyelangkan\nmenyelang-nyeling\nmenyelang-nyelingkan\nmenyenak-nyenak\nmenyendi-nyendi\nmenyentak-nyentak\nmenyentuh-nyentuh\nmenyepak-nyepakkan\nmenyerak-nyerakkan\nmenyeret-nyeret\nmenyeru-nyerukan\nmenyetel-nyetel\nmenyia-nyiakan\nmenyibak-nyibak\nmenyobek-nyobek\nmenyorong-nyorongkan\nmenyungguh-nyungguhi\nmenyuruk-nyuruk\nmeraba-raba\nmerah-hitam\nmerah-merah\nmerambang-rambang\nmerangkak-rangkak\nmerasa-rasai\nmerata-ratakan\nmeraung-raung\nmeraung-raungkan\nmerayau-rayau\nmerayu-rayu\nmercak-mercik\nmercedes-benz\nmerek-merek\nmereka-mereka\nmereka-reka\nmerelap-relap\nmerem-merem\nmeremah-remah\nmeremas-remas\nmeremeh-temehkan\nmerempah-rempah\nmerempah-rempahi\nmerengek-rengek\nmerengeng-rengeng\nmerenik-renik\nmerenta-renta\nmerenyai-renyai\nmeresek-resek\nmerintang-rintang\nmerintik-rintik\nmerobek-robek\nmeronta-ronta\nmeruap-ruap\nmerubu-rubu\nmerungus-rungus\nmerungut-rungut\nmeta-analysis\nmetode-metode\nmewanti-wanti\nmewarna-warnikan\nmeyakin-yakini\nmid-range\nmid-size\nmiju-miju\nmikro-kecil\nmimpi-mimpi\nminggu-minggu\nminta-minta\nminuman-minuman\nmixed-use\nmobil-mobil\nmobile-first\nmobile-friendly\nmoga-moga\nmola-mola\nmomen-momen\nmondar-mandir\nmonyet-monyet\nmorak-marik\nmorat-marit\nmove-on\nmuda-muda\nmuda-mudi\nmuda/i\nmudah-mudahan\nmuka-muka\nmula-mula\nmultiple-output\nmuluk-muluk\nmulut-mulutan\nmumi-mumi\nmundur-mundur\nmuntah-muntah\nmurid-muridnya\nmusda-musda\nmuseum-museum\nmuslim-muslimah\nmusuh-musuh\nmusuh-musuhnya\nnabi-nabi\nnada-nadanya\nnaga-naga\nnaga-naganya\nnaik-naik\nnaik-turun\nnakal-nakalan\nnama-nama\nnanti-nantian\nnanya-nanya\nnasi-nasi\nnasib-nasiban\nnear-field\nnegara-negara\nnegera-negara\nnegeri-negeri\nnegeri-red\nneka-neka\nnekat-nekat\nneko-neko\nnenek-nenek\nneo-liberalisme\nnext-gen\nnext-generation\nngeang-ngeang\nngeri-ngeri\nnggak-nggak\nngobrol-ngobrol\nngumpul-ngumpul\nnilai-nilai\nnine-dash\nnipa-nipa\nnong-nong\nnorma-norma\nnovel-novel\nnyai-nyai\nnyolong-nyolong\nnyut-nyutan\nob-gyn\nobat-obat\nobat-obatan\nobjek-objek\nobok-obok\nobrak-abrik\nocta-core\nodong-odong\noedipus-kompleks\noff-road\nogah-agih\nogah-ogah\nogah-ogahan\nogak-agik\nogak-ogak\nogoh-ogoh\nolak-alik\nolak-olak\nolang-aling\nolang-alingan\nole-ole\noleh-oleh\nolok-olok\nolok-olokan\nolong-olong\nom-om\nombang-ambing\nomni-channel\non-board\non-demand\non-fire\non-line\non-off\non-premises\non-roll\non-screen\non-the-go\nonde-onde\nondel-ondel\nondos-ondos\none-click\none-to-one\none-touch\none-two\noneng-oneng\nongkang-ongkang\nongol-ongol\nonline-to-offline\nontran-ontran\nonyah-anyih\nonyak-anyik\nopak-apik\nopsi-opsi\nopt-in\norak-arik\norang-aring\norang-orang\norang-orangan\norat-oret\norganisasi-organisasi\normas-ormas\norok-orok\norong-orong\noseng-oseng\notak-atik\notak-otak\notak-otakan\nover-heating\nover-the-air\nover-the-top\npa-pa\npabrik-pabrik\npadi-padian\npagi-pagi\npagi-sore\npajak-pajak\npaket-paket\npalas-palas\npalato-alveolar\npaling-paling\npalu-arit\npalu-memalu\npanas-dingin\npanas-panas\npandai-pandai\npandang-memandang\npanel-panel\npangeran-pangeran\npanggung-panggung\npangkalan-pangkalan\npanja-panja\npanji-panji\npansus-pansus\npantai-pantai\npao-pao\npara-para\nparang-parang\nparpol-parpol\npartai-partai\nparu-paru\npas-pasan\npasal-pasal\npasang-memasang\npasang-surut\npasar-pasar\npasu-pasu\npaus-paus\npaut-memaut\npay-per-click\npaya-paya\npdi-p\npecah-pecah\npecat-pecatan\npeer-to-peer\npejabat-pejabat\npekak-pekak\npekik-pekuk\npelabuhan-pelabuhan\npelacur-pelacur\npelajar-pelajar\npelan-pelan\npelangi-pelangi\npem-bully\npemain-pemain\npemata-mataan\npemda-pemda\npemeluk-pemeluknya\npemerintah-pemerintah\npemerintah-red\npemerintah-swasta\npemetang-metangan\npemilu-pemilu\npemimpin-pemimpin\npeminta-minta\npemuda-pemuda\npemuda-pemudi\npenanggung-jawab\npengali-ali\npengaturan-pengaturan\npenggembar-gemboran\npengorak-arik\npengotak-ngotakan\npengundang-undang\npengusaha-pengusaha\npentung-pentungan\npenyakit-penyakit\nperak-perak\nperang-perangan\nperas-perus\nperaturan-peraturan\nperda-perda\nperempat-final\nperempuan-perempuan\npergi-pergi\npergi-pulang\nperintang-rintang\nperkereta-apian\nperlahan-lahan\nperlip-perlipan\npermen-permen\npernak-pernik\npernik-pernik\npertama-tama\npertandingan-pertandingan\npertimbangan-pertimbangan\nperudang-undangan\nperundang-undangan\nperundangan-undangan\nperusahaan-perusahaan\nperusahaan-perusahan\nperwakilan-perwakilan\npesan-pesan\npesawat-pesawat\npeta-jalan\npetang-petang\npetantang-petenteng\npetatang-peteteng\npete-pete\npiala-piala\npiat-piut\npick-up\npicture-in-picture\npihak-pihak\npijak-pijak\npijar-pijar\npijat-pijat\npikir-pikir\npil-pil\npilah-pilih\npilih-pilih\npilihan-pilihan\npilin-memilin\npilkada-pilkada\npina-pina\npindah-pindah\nping-pong\npinjam-meminjam\npintar-pintarlah\npisang-pisang\npistol-pistolan\npiting-memiting\nplanet-planet\nplay-off\nplin-plan\nplintat-plintut\nplonga-plongo\nplug-in\nplus-minus\nplus-plus\npoco-poco\npohon-pohonan\npoin-poin\npoint-of-sale\npoint-of-sales\npokemon-pokemon\npokja-pokja\npokok-pokok\npokrol-pokrolan\npolang-paling\npolda-polda\npoleng-poleng\npolong-polongan\npolres-polres\npolsek-polsek\npolwan-polwan\npoma-poma\npondok-pondok\nponpes-ponpes\npontang-panting\npop-up\nporak-parik\nporak-peranda\nporak-poranda\npos-pos\nposko-posko\npotong-memotong\npraktek-praktek\npraktik-praktik\nproduk-produk\nprogram-program\npromosi-degradasi\nprovinsi-provinsi\nproyek-proyek\npuing-puing\npuisi-puisi\npuji-pujian\npukang-pukang\npukul-memukul\npulang-pergi\npulau-pulai\npulau-pulau\npull-up\npulut-pulut\npundi-pundi\npungak-pinguk\npunggung-memunggung\npura-pura\npuruk-parak\npusar-pusar\npusat-pusat\npush-to-talk\npush-up\npush-ups\npusing-pusing\npuskesmas-puskesmas\nputar-putar\nputera-puteri\nputih-hitam\nputih-putih\nputra-putra\nputra-putri\nputra/i\nputri-putri\nputus-putus\nputusan-putusan\npuvi-puvi\nquad-core\nraba-rabaan\nraba-rubu\nrada-rada\nradio-frequency\nragu-ragu\nrahasia-rahasiaan\nraja-raja\nrama-rama\nramai-ramai\nramalan-ramalan\nrambeh-rambeh\nrambu-rambu\nrame-rame\nramu-ramuan\nranda-rondo\nrangkul-merangkul\nrango-rango\nrap-rap\nrasa-rasanya\nrata-rata\nraun-raun\nread-only\nreal-life\nreal-time\nrebah-rebah\nrebah-rebahan\nrebas-rebas\nred-eye\nredam-redam\nredep-redup\nrehab-rekon\nreja-reja\nreka-reka\nreka-rekaan\nrekan-rekan\nrekan-rekannya\nrekor-rekor\nrelief-relief\nremah-remah\nremang-remang\nrembah-rembah\nrembah-rembih\nremeh-cemeh\nremeh-temeh\nrempah-rempah\nrencana-rencana\nrenyai-renyai\nrep-repan\nrepot-repot\nrepuh-repuh\nrestoran-restoran\nretak-retak\nriang-riang\nribu-ribu\nribut-ribut\nrica-rica\nride-sharing\nrigi-rigi\nrinai-rinai\nrintik-rintik\nritual-ritual\nrobak-rabik\nrobat-rabit\nrobot-robot\nrole-play\nrole-playing\nroll-on\nrombang-rambing\nromol-romol\nrompang-romping\nrondah-rondih\nropak-rapik\nroyal-royalan\nroyo-royo\nruak-ruak\nruba-ruba\nrudal-rudal\nruji-ruji\nruku-ruku\nrumah-rumah\nrumah-rumahan\nrumbai-rumbai\nrumput-rumputan\nrunding-merunding\nrundu-rundu\nrunggu-rangga\nrunner-up\nruntang-runtung\nrupa-rupa\nrupa-rupanya\nrusun-rusun\nrute-rute\nsaat-saat\nsaban-saban\nsabu-sabu\nsabung-menyabung\nsah-sah\nsahabat-sahabat\nsaham-saham\nsahut-menyahut\nsaing-menyaing\nsaji-sajian\nsakit-sakitan\nsaksi-saksi\nsaku-saku\nsalah-salah\nsama-sama\nsamar-samar\nsambar-menyambar\nsambung-bersambung\nsambung-menyambung\nsambut-menyambut\nsamo-samo\nsampah-sampah\nsampai-sampai\nsamping-menyamping\nsana-sini\nsandar-menyandar\nsandi-sandi\nsangat-sangat\nsangkut-menyangkut\nsapa-menyapa\nsapai-sapai\nsapi-sapi\nsapu-sapu\nsaran-saran\nsarana-prasarana\nsari-sari\nsarit-sarit\nsatu-dua\nsatu-satu\nsatu-satunya\nsatuan-satuan\nsaudara-saudara\nsauk-menyauk\nsauk-sauk\nsayang-sayang\nsayap-sayap\nsayup-menyayup\nsayup-sayup\nsayur-mayur\nsayur-sayuran\nsci-fi\nseagak-agak\nseakal-akal\nseakan-akan\nsealak-alak\nseari-arian\nsebaik-baiknya\nsebelah-menyebelah\nsebentar-sebentar\nseberang-menyeberang\nseberuntung-beruntungnya\nsebesar-besarnya\nseboleh-bolehnya\nsedalam-dalamnya\nsedam-sedam\nsedang-menyedang\nsedang-sedang\nsedap-sedapan\nsedapat-dapatnya\nsedikit-dikitnya\nsedikit-sedikit\nsedikit-sedikitnya\nsedini-dininya\nseelok-eloknya\nsegala-galanya\nsegan-menyegan\nsegan-menyegani\nsegan-segan\nsehabis-habisnya\nsehari-hari\nsehari-harian\nsehari-harinya\nsejadi-jadinya\nsekali-kali\nsekali-sekali\nsekenyang-kenyangnya\nsekira-kira\nsekolah-sekolah\nsekonyong-konyong\nsekosong-kosongnya\nsektor-sektor\nsekuasa-kuasanya\nsekuat-kuatnya\nsekurang-kurangnya\nsel-sel\nsela-menyela\nsela-sela\nselak-seluk\nselama-lamanya\nselambat-lambatnya\nselang-seli\nselang-seling\nselar-belar\nselat-latnya\nselatan-tenggara\nselekas-lekasnya\nselentang-selenting\nselepas-lepas\nself-driving\nself-esteem\nself-healing\nself-help\nselir-menyelir\nseloyong-seloyong\nseluk-beluk\nseluk-semeluk\nsema-sema\nsemah-semah\nsemak-semak\nsemaksimal-maksimalnya\nsemalam-malaman\nsemang-semang\nsemanis-manisnya\nsemasa-masa\nsemata-mata\nsemau-maunya\nsembunyi-sembunyi\nsembunyi-sembunyian\nsembur-sembur\nsemena-mena\nsemenda-menyemenda\nsemengga-mengga\nsemenggah-menggah\nsementang-mentang\nsemerdeka-merdekanya\nsemi-final\nsemi-permanen\nsempat-sempatnya\nsemu-semu\nsemua-muanya\nsemujur-mujurnya\nsemut-semutan\nsen-senan\nsendiri-sendiri\nsengal-sengal\nsengar-sengir\nsengau-sengauan\nsenggak-sengguk\nsenggang-tenggang\nsenggol-menyenggol\nsenior-junior\nsenjata-senjata\nsenyum-senyum\nseolah-olah\nsepala-pala\nsepandai-pandai\nsepetang-petangan\nsepoi-sepoi\nsepraktis-praktisnya\nsepuas-puasnya\nserak-serak\nserak-serik\nserang-menyerang\nserang-serangan\nserangan-serangan\nseraya-menyeraya\nserba-serbi\nserbah-serbih\nserembah-serembih\nserigala-serigala\nsering-sering\nserobot-serobotan\nserong-menyerong\nserta-menyertai\nserta-merta\nserta-serta\nseru-seruan\nservice-oriented\nsesak-menyesak\nsesal-menyesali\nsesayup-sayup\nsesi-sesi\nsesuang-suang\nsesudah-sudah\nsesudah-sudahnya\nsesuka-suka\nsesuka-sukanya\nset-piece\nsetempat-setempat\nsetengah-setengah\nsetidak-tidaknya\nsetinggi-tingginya\nseupaya-upaya\nseupaya-upayanya\nsewa-menyewa\nsewaktu-waktu\nsewenang-wenang\nsewot-sewotan\nshabu-shabu\nshort-term\nshort-throw\nsia-sia\nsiang-siang\nsiap-siap\nsiapa-siapa\nsibar-sibar\nsibur-sibur\nsida-sida\nside-by-side\nsign-in\nsiku-siku\nsikut-sikutan\nsilah-silah\nsilang-menyilang\nsilir-semilir\nsimbol-simbol\nsimpan-pinjam\nsinar-menyinar\nsinar-seminar\nsinar-suminar\nsindir-menyindir\nsinga-singa\nsinggah-menyinggah\nsingle-core\nsipil-militer\nsir-siran\nsirat-sirat\nsisa-sisa\nsisi-sisi\nsiswa-siswa\nsiswa-siswi\nsiswa/i\nsiswi-siswi\nsitu-situ\nsitus-situs\nsix-core\nsix-speed\nslintat-slintut\nslo-mo\nslow-motion\nsnap-on\nsobek-sobekan\nsodok-sodokan\nsok-sokan\nsolek-menyolek\nsolid-state\nsorak-sorai\nsorak-sorak\nsore-sore\nsosio-ekonomi\nsoya-soya\nspill-resistant\nsplit-screen\nsponsor-sponsor\nsponsor-sponsoran\nsrikandi-srikandi\nstaf-staf\nstand-by\nstand-up\nstart-up\nstasiun-stasiun\nstate-owned\nstriker-striker\nstudi-studi\nsuam-suam\nsuami-isteri\nsuami-istri\nsuami-suami\nsuang-suang\nsuara-suara\nsudin-sudin\nsudu-sudu\nsudung-sudung\nsugi-sugi\nsuka-suka\nsuku-suku\nsulang-menyulang\nsulat-sulit\nsulur-suluran\nsum-sum\nsumber-sumber\nsumpah-sumpah\nsumpit-sumpit\nsundut-bersundut\nsungai-sungai\nsungguh-sungguh\nsungut-sungut\nsunting-menyunting\nsuper-damai\nsuper-rahasia\nsuper-sub\nsupply-demand\nsupply-side\nsuram-suram\nsurat-menyurat\nsurat-surat\nsuruh-suruhan\nsuruk-surukan\nsusul-menyusul\nsuwir-suwir\nsyarat-syarat\nsystem-on-chip\nt-shirt\nt-shirts\ntabar-tabar\ntabir-mabir\ntabrak-tubruk\ntabuh-tabuhan\ntabun-menabun\ntahu-menahu\ntahu-tahu\ntahun-tahun\ntakah-takahnya\ntakang-takik\ntake-off\ntakut-takut\ntakut-takutan\ntali-bertali\ntali-tali\ntalun-temalun\ntaman-taman\ntampak-tampak\ntanak-tanakan\ntanam-menanam\ntanam-tanaman\ntanda-tanda\ntangan-menangan\ntangan-tangan\ntangga-tangga\ntanggal-tanggal\ntanggul-tanggul\ntanggung-menanggung\ntanggung-tanggung\ntank-tank\ntante-tante\ntanya-jawab\ntapa-tapa\ntapak-tapak\ntari-menari\ntari-tarian\ntarik-menarik\ntarik-ulur\ntata-tertib\ntatah-tatah\ntau-tau\ntawa-tawa\ntawak-tawak\ntawang-tawang\ntawar-menawar\ntawar-tawar\ntayum-temayum\ntebak-tebakan\ntebu-tebu\ntedong-tedong\ntegak-tegak\ntegerbang-gerbang\nteh-tehan\ntek-tek\nteka-teki\nteknik-teknik\nteman-teman\nteman-temanku\ntemas-temas\ntembak-menembak\ntemeh-temeh\ntempa-menempa\ntempat-tempat\ntempo-tempo\ntemut-temut\ntenang-tenang\ntengah-tengah\ntenggang-menenggang\ntengok-menengok\nteori-teori\nteraba-raba\nteralang-alang\nterambang-ambang\nterambung-ambung\nterang-terang\nterang-terangan\nteranggar-anggar\nterangguk-angguk\nteranggul-anggul\nterangin-angin\nterangkup-angkup\nteranja-anja\nterapung-apung\nterayan-rayan\nterayap-rayap\nterbada-bada\nterbahak-bahak\nterbang-terbang\nterbata-bata\nterbatuk-batuk\nterbayang-bayang\nterbeda-bedakan\nterbengkil-bengkil\nterbengong-bengong\nterbirit-birit\nterbuai-buai\nterbuang-buang\nterbungkuk-bungkuk\nterburu-buru\ntercangak-cangak\ntercengang-cengang\ntercilap-cilap\ntercongget-congget\ntercoreng-moreng\ntercungap-cungap\nterdangka-dangka\nterdengih-dengih\nterduga-duga\nterekeh-ekeh\nterembut-embut\nterembut-rembut\nterempas-empas\nterengah-engah\nteresak-esak\ntergagap-gagap\ntergagau-gagau\ntergaguk-gaguk\ntergapai-gapai\ntergegap-gegap\ntergegas-gegas\ntergelak-gelak\ntergelang-gelang\ntergeleng-geleng\ntergelung-gelung\ntergerai-gerai\ntergerenyeng-gerenyeng\ntergesa-gesa\ntergila-gila\ntergolek-golek\ntergontai-gontai\ntergudik-gudik\ntergugu-gugu\nterguling-guling\ntergulut-gulut\nterhambat-hambat\nterharak-harak\nterharap-harap\nterhengit-hengit\nterheran-heran\nterhinggut-hinggut\nterigau-igau\nterimpi-impi\nterincut-incut\nteringa-inga\nteringat-ingat\nterinjak-injak\nterisak-isak\nterjembak-jembak\nterjerit-jerit\nterkadang-kadang\nterkagum-kagum\nterkaing-kaing\nterkakah-kakah\nterkakak-kakak\nterkampul-kampul\nterkanjar-kanjar\nterkantuk-kantuk\nterkapah-kapah\nterkapai-kapai\nterkapung-kapung\nterkatah-katah\nterkatung-katung\nterkecap-kecap\nterkedek-kedek\nterkedip-kedip\nterkejar-kejar\nterkekau-kekau\nterkekeh-kekeh\nterkekek-kekek\nterkelinjat-kelinjat\nterkelip-kelip\nterkempul-kempul\nterkemut-kemut\nterkencar-kencar\nterkencing-kencing\nterkentut-kentut\nterkepak-kepak\nterkesot-kesot\nterkesut-kesut\nterkial-kial\nterkijai-kijai\nterkikih-kikih\nterkikik-kikik\nterkincak-kincak\nterkindap-kindap\nterkinja-kinja\nterkirai-kirai\nterkitar-kitar\nterkocoh-kocoh\nterkojol-kojol\nterkokol-kokol\nterkosel-kosel\nterkotak-kotak\nterkoteng-koteng\nterkuai-kuai\nterkumpal-kumpal\nterlara-lara\nterlayang-layang\nterlebih-lebih\nterlincah-lincah\nterliuk-liuk\nterlolong-lolong\nterlongong-longong\nterlunta-lunta\ntermangu-mangu\ntermanja-manja\ntermata-mata\ntermengah-mengah\ntermenung-menung\ntermimpi-mimpi\ntermonyong-monyong\nternanti-nanti\nterngiang-ngiang\nteroleng-oleng\nterombang-ambing\nterpalit-palit\nterpandang-pandang\nterpecah-pecah\nterpekik-pekik\nterpencar-pencar\nterpereh-pereh\nterpijak-pijak\nterpikau-pikau\nterpilah-pilah\nterpinga-pinga\nterpingkal-pingkal\nterpingkau-pingkau\nterpontang-panting\nterpusing-pusing\nterputus-putus\ntersanga-sanga\ntersaruk-saruk\ntersedan-sedan\ntersedih-sedih\ntersedu-sedu\nterseduh-seduh\ntersendat-sendat\ntersendeng-sendeng\ntersengal-sengal\ntersengguk-sengguk\ntersengut-sengut\nterseok-seok\ntersera-sera\nterserak-serak\ntersetai-setai\ntersia-sia\ntersipu-sipu\ntersoja-soja\ntersungkuk-sungkuk\ntersuruk-suruk\ntertagak-tagak\ntertahan-tahan\ntertatih-tatih\ntertegun-tegun\ntertekan-tekan\nterteleng-teleng\ntertendang-tendang\ntertimpang-timpang\ntertitar-titar\nterumbang-ambing\nterumbang-umbang\nterungkap-ungkap\nterus-menerus\nterus-terusan\ntete-a-tete\ntext-to-speech\nthink-tank\nthink-thank\nthird-party\nthird-person\nthree-axis\nthree-point\ntiap-tiap\ntiba-tiba\ntidak-tidak\ntidur-tidur\ntidur-tiduran\ntie-dye\ntie-in\ntiga-tiganya\ntikam-menikam\ntiki-taka\ntikus-tikus\ntilik-menilik\ntim-tim\ntimah-timah\ntimang-timangan\ntimbang-menimbang\ntime-lapse\ntimpa-menimpa\ntimu-timu\ntimun-timunan\ntimur-barat\ntimur-laut\ntimur-tenggara\ntindih-bertindih\ntindih-menindih\ntinjau-meninjau\ntinju-meninju\ntip-off\ntipu-tipu\ntiru-tiruan\ntitik-titik\ntitik-titiknya\ntiup-tiup\nto-do\ntokak-takik\ntoko-toko\ntokoh-tokoh\ntokok-menokok\ntolak-menolak\ntolong-menolong\ntong-tong\ntop-level\ntop-up\ntotol-totol\ntouch-screen\ntrade-in\ntraining-camp\ntrans-nasional\ntreble-winner\ntri-band\ntrik-trik\ntriple-core\ntruk-truk\ntua-tua\ntuan-tuan\ntuang-tuang\ntuban-tuban\ntubuh-tubuh\ntujuan-tujuan\ntuk-tuk\ntukang-menukang\ntukar-menukar\ntulang-belulang\ntulang-tulangan\ntuli-tuli\ntulis-menulis\ntumbuh-tumbuhan\ntumpang-tindih\ntune-up\ntunggang-tunggik\ntunggang-tungging\ntunggang-tunggit\ntunggul-tunggul\ntunjuk-menunjuk\ntupai-tupai\ntupai-tupaian\nturi-turian\nturn-based\nturnamen-turnamen\nturun-temurun\nturut-menurut\nturut-turutan\ntuyuk-tuyuk\ntwin-cam\ntwin-turbocharged\ntwo-state\ntwo-step\ntwo-tone\nu-shape\nuang-uangan\nuar-uar\nubek-ubekan\nubel-ubel\nubrak-abrik\nubun-ubun\nubur-ubur\nuci-uci\nudang-undang\nudap-udapan\nugal-ugalan\nuget-uget\nuir-uir\nujar-ujar\nuji-coba\nujung-ujung\nujung-ujungnya\nuka-uka\nukir-mengukir\nukir-ukiran\nula-ula\nulak-ulak\nulam-ulam\nulang-alik\nulang-aling\nulang-ulang\nulap-ulap\nular-ular\nular-ularan\nulek-ulek\nulu-ulu\nulung-ulung\numang-umang\numbang-ambing\numbi-umbian\numbul-umbul\numbut-umbut\nuncang-uncit\nundak-undakan\nundang-undang\nundang-undangnya\nunduk-unduk\nundung-undung\nundur-undur\nunek-unek\nungah-angih\nunggang-anggit\nunggat-unggit\nunggul-mengungguli\nungkit-ungkit\nunit-unit\nuniversitas-universitas\nunsur-unsur\nuntang-anting\nunting-unting\nuntung-untung\nuntung-untungan\nupah-mengupah\nupih-upih\nupside-down\nura-ura\nuran-uran\nurat-urat\nuring-uringan\nurup-urup\nurup-urupan\nurus-urus\nusaha-usaha\nuser-user\nuser-useran\nutak-atik\nutang-piutang\nutang-utang\nutar-utar\nutara-jauh\nutara-selatan\nuter-uter\nutusan-utusan\nv-belt\nv-neck\nvalue-added\nvery-very\nvideo-video\nvisi-misi\nvisi-misinya\nvoa-islam\nvoice-over\nvolt-ampere\nwajah-wajah\nwajar-wajar\nwake-up\nwakil-wakil\nwalk-in\nwalk-out\nwangi-wangian\nwanita-wanita\nwanti-wanti\nwara-wara\nwara-wiri\nwarna-warna\nwarna-warni\nwas-was\nwater-cooled\nweb-based\nwide-angle\nwilayah-wilayah\nwin-win\nwira-wiri\nwora-wari\nwork-life\nworld-class\nyang-yang\nyayasan-yayasan\nyear-on-year\nyel-yel\nyo-yo\nzam-zam\nzig-zag\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/id/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.id.tokenizer_exceptions.orth_title->'-'.join([part.title() for part in orth.split('-')])
A:spacy.lang.id.tokenizer_exceptions.orth_caps->'-'.join([part.upper() for part in orth.split('-')])
A:spacy.lang.id.tokenizer_exceptions.orth_lower->orth.lower()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/id/syntax_iterators.py----------------------------------------
A:spacy.lang.id.syntax_iterators.conj->doc.vocab.strings.add('conj')
A:spacy.lang.id.syntax_iterators.np_label->doc.vocab.strings.add('NP')
A:spacy.lang.id.syntax_iterators.seen->set()
spacy.lang.id.syntax_iterators.noun_chunks(obj)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/id/stop_words.py----------------------------------------
A:spacy.lang.id.stop_words.STOP_WORDS->set('\nada adalah adanya adapun agak agaknya agar akan akankah akhir akhiri akhirnya\naku akulah amat amatlah anda andalah antar antara antaranya apa apaan apabila\napakah apalagi apatah artinya asal asalkan atas atau ataukah ataupun awal\nawalnya\n\nbagai bagaikan bagaimana bagaimanakah bagaimanapun bagi bagian bahkan bahwa\nbahwasanya baik bakal bakalan balik banyak bapak baru bawah beberapa begini\nbeginian beginikah beginilah begitu begitukah begitulah begitupun bekerja\nbelakang belakangan belum belumlah benar benarkah benarlah berada berakhir\nberakhirlah berakhirnya berapa berapakah berapalah berapapun berarti berawal\nberbagai berdatangan beri berikan berikut berikutnya berjumlah berkali-kali\nberkata berkehendak berkeinginan berkenaan berlainan berlalu berlangsung\nberlebihan bermacam bermacam-macam bermaksud bermula bersama bersama-sama\nbersiap bersiap-siap bertanya bertanya-tanya berturut berturut-turut bertutur\nberujar berupa besar betul betulkah biasa biasanya bila bilakah bisa bisakah\nboleh bolehkah bolehlah buat bukan bukankah bukanlah bukannya bulan bung\n\ncara caranya cukup cukupkah cukuplah cuma\n\ndahulu dalam dan dapat dari daripada datang dekat demi demikian demikianlah\ndengan depan di dia diakhiri diakhirinya dialah diantara diantaranya diberi\ndiberikan diberikannya dibuat dibuatnya didapat didatangkan digunakan\ndiibaratkan diibaratkannya diingat diingatkan diinginkan dijawab dijelaskan\ndijelaskannya dikarenakan dikatakan dikatakannya dikerjakan diketahui\ndiketahuinya dikira dilakukan dilalui dilihat dimaksud dimaksudkan\ndimaksudkannya dimaksudnya diminta dimintai dimisalkan dimulai dimulailah\ndimulainya dimungkinkan dini dipastikan diperbuat diperbuatnya dipergunakan\ndiperkirakan diperlihatkan diperlukan diperlukannya dipersoalkan dipertanyakan\ndipunyai diri dirinya disampaikan disebut disebutkan disebutkannya disini\ndisinilah ditambahkan ditandaskan ditanya ditanyai ditanyakan ditegaskan\nditujukan ditunjuk ditunjuki ditunjukkan ditunjukkannya ditunjuknya dituturkan\ndituturkannya diucapkan diucapkannya diungkapkan dong dua dulu\n\nempat enggak enggaknya entah entahlah\n\nguna gunakan\n\nhal hampir hanya hanyalah hari harus haruslah harusnya hendak hendaklah\nhendaknya hingga\n\nia ialah ibarat ibaratkan ibaratnya ibu ikut ingat ingat-ingat ingin inginkah\ninginkan ini inikah inilah itu itukah itulah\n\njadi jadilah jadinya jangan jangankan janganlah jauh jawab jawaban jawabnya\njelas jelaskan jelaslah jelasnya jika jikalau juga jumlah jumlahnya justru\n\nkala kalau kalaulah kalaupun kalian kami kamilah kamu kamulah kan kapan\nkapankah kapanpun karena karenanya kasus kata katakan katakanlah katanya ke\nkeadaan kebetulan kecil kedua keduanya keinginan kelamaan kelihatan\nkelihatannya kelima keluar kembali kemudian kemungkinan kemungkinannya kenapa\nkepada kepadanya kesampaian keseluruhan keseluruhannya keterlaluan ketika\nkhususnya kini kinilah kira kira-kira kiranya kita kitalah kok kurang\n\nlagi lagian lah lain lainnya lalu lama lamanya lanjut lanjutnya lebih lewat\nlima luar\n\nmacam maka makanya makin malah malahan mampu mampukah mana manakala manalagi\nmasa masalah masalahnya masih masihkah masing masing-masing mau maupun\nmelainkan melakukan melalui melihat melihatnya memang memastikan memberi\nmemberikan membuat memerlukan memihak meminta memintakan memisalkan memperbuat\nmempergunakan memperkirakan memperlihatkan mempersiapkan mempersoalkan\nmempertanyakan mempunyai memulai memungkinkan menaiki menambahkan menandaskan\nmenanti menanti-nanti menantikan menanya menanyai menanyakan mendapat\nmendapatkan mendatang mendatangi mendatangkan menegaskan mengakhiri mengapa\nmengatakan mengatakannya mengenai mengerjakan mengetahui menggunakan\nmenghendaki mengibaratkan mengibaratkannya mengingat mengingatkan menginginkan\nmengira mengucapkan mengucapkannya mengungkapkan menjadi menjawab menjelaskan\nmenuju menunjuk menunjuki menunjukkan menunjuknya menurut menuturkan\nmenyampaikan menyangkut menyatakan menyebutkan menyeluruh menyiapkan merasa\nmereka merekalah merupakan meski meskipun meyakini meyakinkan minta mirip\nmisal misalkan misalnya mula mulai mulailah mulanya mungkin mungkinkah\n\nnah naik namun nanti nantinya nyaris nyatanya\n\noleh olehnya\n\npada padahal padanya pak paling panjang pantas para pasti pastilah penting\npentingnya per percuma perlu perlukah perlunya pernah persoalan pertama\npertama-tama pertanyaan pertanyakan pihak pihaknya pukul pula pun punya\n\nrasa rasanya rata rupanya\n\nsaat saatnya saja sajalah saling sama sama-sama sambil sampai sampai-sampai\nsampaikan sana sangat sangatlah satu saya sayalah se sebab sebabnya sebagai\nsebagaimana sebagainya sebagian sebaik sebaik-baiknya sebaiknya sebaliknya\nsebanyak sebegini sebegitu sebelum sebelumnya sebenarnya seberapa sebesar\nsebetulnya sebisanya sebuah sebut sebutlah sebutnya secara secukupnya sedang\nsedangkan sedemikian sedikit sedikitnya seenaknya segala segalanya segera\nseharusnya sehingga seingat sejak sejauh sejenak sejumlah sekadar sekadarnya\nsekali sekali-kali sekalian sekaligus sekalipun sekarang sekarang sekecil\nseketika sekiranya sekitar sekitarnya sekurang-kurangnya sekurangnya sela\nselain selaku selalu selama selama-lamanya selamanya selanjutnya seluruh\nseluruhnya semacam semakin semampu semampunya semasa semasih semata semata-mata\nsemaunya sementara semisal semisalnya sempat semua semuanya semula sendiri\nsendirian sendirinya seolah seolah-olah seorang sepanjang sepantasnya\nsepantasnyalah seperlunya seperti sepertinya sepihak sering seringnya serta\nserupa sesaat sesama sesampai sesegera sesekali seseorang sesuatu sesuatunya\nsesudah sesudahnya setelah setempat setengah seterusnya setiap setiba setibanya\nsetidak-tidaknya setidaknya setinggi seusai sewaktu siap siapa siapakah\nsiapapun sini sinilah soal soalnya suatu sudah sudahkah sudahlah supaya\n\ntadi tadinya tahu tahun tak tambah tambahnya tampak tampaknya tandas tandasnya\ntanpa tanya tanyakan tanyanya tapi tegas tegasnya telah tempat tengah tentang\ntentu tentulah tentunya tepat terakhir terasa terbanyak terdahulu terdapat\nterdiri terhadap terhadapnya teringat teringat-ingat terjadi terjadilah\nterjadinya terkira terlalu terlebih terlihat termasuk ternyata tersampaikan\ntersebut tersebutlah tertentu tertuju terus terutama tetap tetapi tiap tiba\ntiba-tiba tidak tidakkah tidaklah tiga tinggi toh tunjuk turut tutur tuturnya\n\nucap ucapnya ujar ujarnya umum umumnya ungkap ungkapnya untuk usah usai\n\nwaduh wah wahai waktu waktunya walau walaupun wong\n\nyaitu yakin yakni yang\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/id/norm_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/id/tag_map.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/id/punctuation.py----------------------------------------
A:spacy.lang.id.punctuation.UNITS->merge_chars(_units)
A:spacy.lang.id.punctuation.CURRENCY->merge_chars(_currency)
A:spacy.lang.id.punctuation.MONTHS->merge_chars(_months)
A:spacy.lang.id.punctuation.LIST_CURRENCY->split_chars(_currency)
A:spacy.lang.id.punctuation._prefixes->list(TOKENIZER_PREFIXES)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/sr/__init__.py----------------------------------------
A:spacy.lang.sr.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.sr.__init__.lex_attr_getters[NORM]->add_lookups(Language.Defaults.lex_attr_getters[NORM], BASE_NORMS, NORM_EXCEPTIONS)
A:spacy.lang.sr.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS, TOKENIZER_EXCEPTIONS)
spacy.lang.sr.__init__.Serbian(Language)
spacy.lang.sr.__init__.SerbianDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/sr/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/sr/lex_attrs.py----------------------------------------
A:spacy.lang.sr.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.sr.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.sr.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/sr/tokenizer_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/sr/stop_words.py----------------------------------------
A:spacy.lang.sr.stop_words.STOP_WORDS->set('\nа\nавај\nако\nал\nали\nарх\nау\nах\nаха\nај\nбар\nби\nбила\nбили\nбило\nбисмо\nбисте\nбих\nбијасмо\nбијасте\nбијах\nбијаху\nбијаше\nбиће\nблизу\nброј\nбрр\nбуде\nбудимо\nбудите\nбуду\nбудући\nбум\nбућ\nвам\nвама\nвас\nваша\nваше\nвашим\nвашима\nваљда\nвеома\nвероватно\nвећ\nвећина\nви\nвидео\nвише\nврло\nврх\nга\nгде\nгиц\nгод\nгоре\nгђекоје\nда\nдакле\nдана\nданас\nдај\nдва\nде\nдедер\nделимице\nделимично\nдем\nдо\nдобар\nдобити\nдовечер\nдокле\nдоле\nдонекле\nдосад\nдоскоро\nдотад\nдотле\nдошао\nдоћи\nдругамо\nдругде\nдруги\nе\nево\nено\nето\nех\nехе\nеј\nжелела\nжелеле\nжелели\nжелело\nжелех\nжелећи\nжели\nза\nзаиста\nзар\nзатим\nзато\nзахвалити\nзашто\nзбиља\nзимус\nзнати\nзум\nи\nиде\nиз\nизван\nизволи\nизмеђу\nизнад\nикада\nикакав\nикаква\nикакве\nикакви\nикаквим\nикаквима\nикаквих\nикакво\nикаквог\nикаквога\nикаквом\nикаквоме\nикаквој\nили\nим\nима\nимам\nимао\nиспод\nих\nију\nићи\nкад\nкада\nкога\nкојекакав\nкојима\nкоју\nкришом\nлани\nли\nмали\nмањи\nме\nмене\nмени\nми\nмимо\nмисли\nмного\nмогу\nмора\nморао\nмој\nмоја\nмоје\nмоји\nмоју\nмоћи\nму\nна\nнад\nнакон\nнам\nнама\nнас\nнаша\nнаше\nнашег\nнаши\nнаћи\nне\nнегде\nнека\nнекад\nнеке\nнеког\nнеку\nнема\nнемам\nнеко\nнеће\nнећемо\nнећете\nнећеш\nнећу\nни\nникада\nникога\nникоје\nникоји\nникоју\nнисам\nниси\nнисте\nнису\nништа\nниједан\nно\nо\nова\nовако\nовамо\nовај\nовде\nове\nовим\nовима\nово\nовој\nод\nодмах\nоко\nоколо\nон\nонај\nоне\nоним\nонима\nоном\nоној\nону\nосим\nостали\nотишао\nпа\nпак\nпитати\nпо\nповодом\nпод\nподаље\nпожељан\nпожељна\nпоиздаље\nпоименце\nпонекад\nпопреко\nпоред\nпосле\nпотаман\nпотрбушке\nпоуздано\nпочетак\nпоједини\nправити\nпрви\nпреко\nпрема\nприје\nпут\nпљус\nрадије\nс\nса\nсав\nсада\nсам\nсамо\nсасвим\nсва\nсваки\nсви\nсвим\nсвог\nсвом\nсвој\nсвоја\nсвоје\nсвоју\nсву\nсвугде\nсе\nсебе\nсеби\nси\nсмети\nсмо\nствар\nстварно\nсте\nсу\nсутра\nта\nтаèно\nтако\nтакође\nтамо\nтвој\nтвоја\nтвоје\nтвоји\nтвоју\nте\nтебе\nтеби\nти\nтима\nто\nтоме\nтој\nту\nу\nувек\nувијек\nуз\nуза\nузалуд\nуздуж\nузети\nумало\nунутра\nупотребити\nупркос\nучинио\nучинити\nхало\nхвала\nхеј\nхм\nхоп\nхоће\nхоћемо\nхоћете\nхоћеш\nхоћу\nхтедосте\nхтедох\nхтедоше\nхтела\nхтеле\nхтели\nхтео\nхтејасмо\nхтејасте\nхтејаху\nхура\nчесто\nчијем\nчији\nчијим\nчијима\nшиц\nштагод\nшто\nштогод\nја\nје\nједан\nједини\nједна\nједне\nједни\nједно\nједном\nјер\nјесам\nјеси\nјесмо\nјесу\nјим\nјој\nју\nјуче\nњегова\nњегово\nњезин\nњезина\nњезино\nњему\nњен\nњим\nњима\nњихова\nњихово\nњој\nњу\nће\nћемо\nћете\nћеш\nћу\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/sr/norm_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/de/__init__.py----------------------------------------
A:spacy.lang.de.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.de.__init__.lex_attr_getters[NORM]->add_lookups(Language.Defaults.lex_attr_getters[NORM], NORM_EXCEPTIONS, BASE_NORMS)
A:spacy.lang.de.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS, TOKENIZER_EXCEPTIONS)
spacy.lang.de.__init__.German(Language)
spacy.lang.de.__init__.GermanDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/de/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/de/tokenizer_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/de/syntax_iterators.py----------------------------------------
A:spacy.lang.de.syntax_iterators.np_label->doc.vocab.strings.add('NP')
A:spacy.lang.de.syntax_iterators.np_deps->set((doc.vocab.strings.add(label) for label in labels))
A:spacy.lang.de.syntax_iterators.close_app->doc.vocab.strings.add('nk')
spacy.lang.de.syntax_iterators.noun_chunks(obj)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/de/stop_words.py----------------------------------------
A:spacy.lang.de.stop_words.STOP_WORDS->set('\ná a ab aber ach acht achte achten achter achtes ag alle allein allem allen\naller allerdings alles allgemeinen als also am an andere anderen anderem andern\nanders auch auf aus ausser außer ausserdem außerdem\n\nbald bei beide beiden beim beispiel bekannt bereits besonders besser besten bin\nbis bisher bist\n\nda dabei dadurch dafür dagegen daher dahin dahinter damals damit danach daneben\ndank dann daran darauf daraus darf darfst darin darüber darum darunter das\ndasein daselbst dass daß dasselbe davon davor dazu dazwischen dein deine deinem\ndeiner dem dementsprechend demgegenüber demgemäss demgemäß demselben demzufolge\nden denen denn denselben der deren derjenige derjenigen dermassen dermaßen\nderselbe derselben des deshalb desselben dessen deswegen dich die diejenige\ndiejenigen dies diese dieselbe dieselben diesem diesen dieser dieses dir doch\ndort drei drin dritte dritten dritter drittes du durch durchaus dürfen dürft\ndurfte durften\n\neben ebenso ehrlich eigen eigene eigenen eigener eigenes ein einander eine\neinem einen einer eines einigeeinigen einiger einiges einmal einmaleins elf en\nende endlich entweder er erst erste ersten erster erstes es etwa etwas euch\n\nfrüher fünf fünfte fünften fünfter fünftes für\n\ngab ganz ganze ganzen ganzer ganzes gar gedurft gegen gegenüber gehabt gehen\ngeht gekannt gekonnt gemacht gemocht gemusst genug gerade gern gesagt geschweige\ngewesen gewollt geworden gibt ging gleich gott gross groß grosse große grossen\ngroßen grosser großer grosses großes gut gute guter gutes\n\nhabe haben habt hast hat hatte hätte hatten hätten heisst heißt her heute hier\nhin hinter hoch\n\nich ihm ihn ihnen ihr ihre ihrem ihren ihrer ihres im immer in indem\ninfolgedessen ins irgend ist\n\nja jahr jahre jahren je jede jedem jeden jeder jedermann jedermanns jedoch\njemand jemandem jemanden jene jenem jenen jener jenes jetzt\n\nkam kann kannst kaum kein keine keinem keinen keiner kleine kleinen kleiner\nkleines kommen kommt können könnt konnte könnte konnten kurz\n\nlang lange leicht leider lieber los\n\nmachen macht machte mag magst man manche manchem manchen mancher manches mehr\nmein meine meinem meinen meiner meines mensch menschen mich mir mit mittel\nmochte möchte mochten mögen möglich mögt morgen muss muß müssen musst müsst\nmusste mussten\n\nna nach nachdem nahm natürlich neben nein neue neuen neun neunte neunten neunter\nneuntes nicht nichts nie niemand niemandem niemanden noch nun nur\n\nob oben oder offen oft ohne\n\nrecht rechte rechten rechter rechtes richtig rund\n\nsagt sagte sah satt schlecht schon sechs sechste sechsten sechster sechstes\nsehr sei seid seien sein seine seinem seinen seiner seines seit seitdem selbst\nselbst sich sie sieben siebente siebenten siebenter siebentes siebte siebten\nsiebter siebtes sind so solang solche solchem solchen solcher solches soll\nsollen sollte sollten sondern sonst sowie später statt\n\ntag tage tagen tat teil tel trotzdem tun\n\nüber überhaupt übrigens uhr um und uns unser unsere unserer unter\n\nvergangene vergangenen viel viele vielem vielen vielleicht vier vierte vierten\nvierter viertes vom von vor\n\nwahr während währenddem währenddessen wann war wäre waren wart warum was wegen\nweil weit weiter weitere weiteren weiteres welche welchem welchen welcher\nwelches wem wen wenig wenige weniger weniges wenigstens wenn wer werde werden\nwerdet wessen wie wieder will willst wir wird wirklich wirst wo wohl wollen\nwollt wollte wollten worden wurde würde wurden würden\n\nzehn zehnte zehnten zehnter zehntes zeit zu zuerst zugleich zum zunächst zur\nzurück zusammen zwanzig zwar zwei zweite zweiten zweiter zweites zwischen\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/de/norm_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/de/tag_map.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/de/punctuation.py----------------------------------------
A:spacy.lang.de.punctuation._quotes->char_classes.CONCAT_QUOTES.replace("'", '')


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/he/__init__.py----------------------------------------
A:spacy.lang.he.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.he.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS)
spacy.lang.he.__init__.Hebrew(Language)
spacy.lang.he.__init__.HebrewDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/he/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/he/stop_words.py----------------------------------------
A:spacy.lang.he.stop_words.STOP_WORDS->set('\nאני\nאת\nאתה\nאנחנו\nאתן\nאתם\nהם\nהן\nהיא\nהוא\nשלי\nשלו\nשלך\nשלה\nשלנו\nשלכם\nשלכן\nשלהם\nשלהן\nלי\nלו\nלה\nלנו\nלכם\nלכן\nלהם\nלהן\nאותה\nאותו\nזה\nזאת\nאלה\nאלו\nתחת\nמתחת\nמעל\nבין\nעם\nעד\nנגר\nעל\nאל\nמול\nשל\nאצל\nכמו\nאחר\nאותו\nבלי\nלפני\nאחרי\nמאחורי\nעלי\nעליו\nעליה\nעליך\nעלינו\nעליכם\nלעיכן\nעליהם\nעליהן\nכל\nכולם\nכולן\nכך\nככה\nכזה\nזה\nזות\nאותי\nאותה\nאותם\nאותך\nאותו\nאותן\nאותנו\nואת\nאת\nאתכם\nאתכן\nאיתי\nאיתו\nאיתך\nאיתה\nאיתם\nאיתן\nאיתנו\nאיתכם\nאיתכן\nיהיה\nתהיה\nהיתי\nהיתה\nהיה\nלהיות\nעצמי\nעצמו\nעצמה\nעצמם\nעצמן\nעצמנו\nעצמהם\nעצמהן\nמי\nמה\nאיפה\nהיכן\nבמקום שבו\nאם\nלאן\nלמקום שבו\nמקום בו\nאיזה\nמהיכן\nאיך\nכיצד\nבאיזו מידה\nמתי\nבשעה ש\nכאשר\nכש\nלמרות\nלפני\nאחרי\nמאיזו סיבה\nהסיבה שבגללה\nלמה\nמדוע\nלאיזו תכלית\nכי\nיש\nאין\nאך\nמנין\nמאין\nמאיפה\nיכל\nיכלה\nיכלו\nיכול\nיכולה\nיכולים\nיכולות\nיוכלו\nיוכל\nמסוגל\nלא\nרק\nאולי\nאין\nלאו\nאי\nכלל\nנגד\nאם\nעם\nאל\nאלה\nאלו\nאף\nעל\nמעל\nמתחת\nמצד\nבשביל\nלבין\nבאמצע\nבתוך\nדרך\nמבעד\nבאמצעות\nלמעלה\nלמטה\nמחוץ\nמן\nלעבר\nמכאן\nכאן\nהנה\nהרי\nפה\nשם\nאך\nברם\nשוב\nאבל\nמבלי\nבלי\nמלבד\nרק\nבגלל\nמכיוון\nעד\nאשר\nואילו\nלמרות\nאס\nכמו\nכפי\nאז\nאחרי\nכן\nלכן\nלפיכך\nמאד\nעז\nמעט\nמעטים\nבמידה\nשוב\nיותר\nמדי\nגם\nכן\nנו\nאחר\nאחרת\nאחרים\nאחרות\nאשר\nאו\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/fa/__init__.py----------------------------------------
A:spacy.lang.fa.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.fa.__init__.lex_attr_getters[NORM]->add_lookups(Language.Defaults.lex_attr_getters[NORM], BASE_NORMS)
A:spacy.lang.fa.__init__.tokenizer_exceptions->update_exc(TOKENIZER_EXCEPTIONS)
spacy.lang.fa.__init__.Persian(Language)
spacy.lang.fa.__init__.PersianDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/fa/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/fa/lex_attrs.py----------------------------------------
A:spacy.lang.fa.lex_attrs._num_words->set('\nصفر\nیک\nدو\nسه\nچهار\nپنج\nشش\nشیش\nهفت\nهشت\nنه\nده\nیازده\nدوازده\nسیزده\nچهارده\nپانزده\nپونزده\nشانزده\nشونزده\nهفده\nهجده\nهیجده\nنوزده\nبیست\nسی\nچهل\nپنجاه\nشصت\nهفتاد\nهشتاد\nنود\nصد\nیکصد\nیک\u200cصد\nدویست\nسیصد\nچهارصد\nپانصد\nپونصد\nششصد\nشیشصد\nهفتصد\nهفصد\nهشتصد\nنهصد\nهزار\nمیلیون\nمیلیارد\nبیلیون\nبیلیارد\nتریلیون\nتریلیارد\nکوادریلیون\nکادریلیارد\nکوینتیلیون\n'.split())
A:spacy.lang.fa.lex_attrs._ordinal_words->set('\nاول\nسوم\nسی\u200cام'.split())
A:spacy.lang.fa.lex_attrs.text->text.replace(',', '').replace('.', '').replace('،', '').replace('٫', '').replace('/', '').replace(',', '').replace('.', '').replace('،', '').replace('٫', '').replace('/', '')
spacy.lang.fa.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/fa/tokenizer_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/fa/syntax_iterators.py----------------------------------------
A:spacy.lang.fa.syntax_iterators.conj->doc.vocab.strings.add('conj')
A:spacy.lang.fa.syntax_iterators.np_label->doc.vocab.strings.add('NP')
A:spacy.lang.fa.syntax_iterators.seen->set()
spacy.lang.fa.syntax_iterators.noun_chunks(obj)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/fa/stop_words.py----------------------------------------
A:spacy.lang.fa.stop_words.STOP_WORDS->set('\nو\nدر\nبه\nاز\nکه\nاین\nرا\nبا\nاست\nبرای\nآن\nیک\nخود\nتا\nکرد\nبر\nهم\nنیز\nگفت\nمی\u200cشود\nوی\nشد\nدارد\nما\nاما\nیا\nشده\nباید\nهر\nآنها\nبود\nاو\nدیگر\nدو\nمورد\nمی\u200cکند\nشود\nکند\nوجود\nبین\nپیش\nشده\u200cاست\nپس\nنظر\nاگر\nهمه\nیکی\nحال\nهستند\nمن\nکنند\nنیست\nباشد\nچه\nبی\nمی\nبخش\nمی\u200cکنند\nهمین\nافزود\nهایی\nدارند\nراه\nهمچنین\nروی\nداد\nبیشتر\nبسیار\nسه\nداشت\nچند\nسوی\nتنها\nهیچ\nمیان\nاینکه\nشدن\nبعد\nجدید\nولی\nحتی\nکردن\nبرخی\nکردند\nمی\u200cدهد\nاول\nنه\nکرده\u200cاست\nنسبت\nبیش\nشما\nچنین\nطور\nافراد\nتمام\nدرباره\nبار\nبسیاری\nمی\u200cتواند\nکرده\nچون\nندارد\nدوم\nبزرگ\nطی\nحدود\nهمان\nبدون\nالبته\nآنان\nمی\u200cگوید\nدیگری\nخواهد\u200cشد\nکنیم\nقابل\nیعنی\nرشد\nمی\u200cتوان\nوارد\nکل\nویژه\nقبل\nبراساس\nنیاز\nگذاری\nهنوز\nلازم\nسازی\nبوده\u200cاست\nچرا\nمی\u200cشوند\nوقتی\nگرفت\nکم\nجای\nحالی\nتغییر\nپیدا\nاکنون\nتحت\nباعث\nمدت\nفقط\nزیادی\nتعداد\nآیا\nبیان\nرو\nشدند\nعدم\nکرده\u200cاند\nبودن\nنوع\nبلکه\nجاری\nدهد\nبرابر\nمهم\nبوده\nاخیر\nمربوط\nامر\nزیر\nگیری\nشاید\nخصوص\nآقای\nاثر\nکننده\nبودند\nفکر\nکنار\nاولین\nسوم\nسایر\nکنید\nضمن\nمانند\nباز\nمی\u200cگیرد\nممکن\nحل\nدارای\nپی\nمثل\nمی\u200cرسد\nاجرا\nدور\nمنظور\nکسی\nموجب\nطول\nامکان\nآنچه\nتعیین\nگفته\nشوند\nجمع\nخیلی\nعلاوه\nگونه\nتاکنون\nرسید\nساله\nگرفته\nشده\u200cاند\nعلت\nچهار\nداشته\u200cباشد\nخواهد\u200cبود\nطرف\nتهیه\nتبدیل\nمناسب\nزیرا\nمشخص\nمی\u200cتوانند\nنزدیک\nجریان\nروند\nبنابراین\nمی\u200cدهند\nیافت\nنخستین\nبالا\nپنج\nریزی\nعالی\nچیزی\nنخست\nبیشتری\nترتیب\nشده\u200cبود\nخاص\nخوبی\nخوب\nشروع\nفرد\nکامل\nغیر\nمی\u200cرود\nدهند\nآخرین\nدادن\nجدی\nبهترین\nشامل\nگیرد\nبخشی\nباشند\nتمامی\nبهتر\nداده\u200cاست\nحد\nنبود\nکسانی\nمی\u200cکرد\nداریم\nعلیه\nمی\u200cباشد\nدانست\nناشی\nداشتند\nدهه\nمی\u200cشد\nایشان\nآنجا\nگرفته\u200cاست\nدچار\nمی\u200cآید\nلحاظ\nآنکه\nداده\nبعضی\nهستیم\nاند\nبرداری\nنباید\nمی\u200cکنیم\nنشست\nسهم\nهمیشه\nآمد\nاش\nوگو\nمی\u200cکنم\nحداقل\nطبق\nجا\nخواهد\u200cکرد\nنوعی\nچگونه\nرفت\nهنگام\nفوق\nروش\nندارند\nسعی\nبندی\nشمار\nکلی\nکافی\nمواجه\nهمچنان\nزیاد\nسمت\nکوچک\nداشته\u200cاست\nچیز\nپشت\nآورد\nحالا\nروبه\nسال\u200cهای\nدادند\nمی\u200cکردند\nعهده\nنیمه\nجایی\nدیگران\nسی\nبروز\nیکدیگر\nآمده\u200cاست\nجز\nکنم\nسپس\nکنندگان\nخودش\nهمواره\nیافته\nشان\nصرف\nنمی\u200cشود\nرسیدن\nچهارم\nیابد\nمتر\nساز\nداشته\nکرده\u200cبود\nباره\nنحوه\nکردم\nتو\nشخصی\nداشته\u200cباشند\nمحسوب\nپخش\nکمی\nمتفاوت\nسراسر\nکاملا\nداشتن\nنظیر\nآمده\nگروهی\nفردی\nع\nهمچون\nخطر\nخویش\nکدام\nدسته\nسبب\nعین\nآوری\nمتاسفانه\nبیرون\nدار\nابتدا\nشش\nافرادی\nمی\u200cگویند\nسالهای\nدرون\nنیستند\nیافته\u200cاست\nپر\nخاطرنشان\nگاه\nجمعی\nاغلب\nدوباره\nمی\u200cیابد\nلذا\nزاده\nگردد\nاینجا'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/fa/tag_map.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/fa/punctuation.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/fa/generate_verbs_exc.py----------------------------------------
A:spacy.lang.fa.generate_verbs_exc.verb_roots->'\n#هست\nآخت#آهنج\nآراست#آرا\nآراماند#آرامان\nآرامید#آرام\nآرمید#آرام\nآزرد#آزار\nآزمود#آزما\nآسود#آسا\nآشامید#آشام\nآشفت#آشوب\nآشوبید#آشوب\nآغازید#آغاز\nآغشت#آمیز\nآفرید#آفرین\nآلود#آلا\nآمد#آ\nآمرزید#آمرز\nآموخت#آموز\nآموزاند#آموزان\nآمیخت#آمیز\nآورد#آر\nآورد#آور\nآویخت#آویز\nآکند#آکن\nآگاهانید#آگاهان\nارزید#ارز\nافتاد#افت\nافراخت#افراز\nافراشت#افراز\nافروخت#افروز\nافروزید#افروز\nافزود#افزا\nافسرد#افسر\nافشاند#افشان\nافکند#افکن\nافگند#افگن\nانباشت#انبار\nانجامید#انجام\nانداخت#انداز\nاندوخت#اندوز\nاندود#اندا\nاندیشید#اندیش\nانگاشت#انگار\nانگیخت#انگیز\nانگیزاند#انگیزان\nایستاد#ایست\nایستاند#ایستان\nباخت#باز\nباراند#باران\nبارگذاشت#بارگذار\nبارید#بار\nباز#بازخواه\nبازآفرید#بازآفرین\nبازآمد#بازآ\nبازآموخت#بازآموز\nبازآورد#بازآور\nبازایستاد#بازایست\nبازتابید#بازتاب\nبازجست#بازجو\nبازخواند#بازخوان\nبازخوراند#بازخوران\nبازداد#بازده\nبازداشت#بازدار\nبازرساند#بازرسان\nبازرسانید#بازرسان\nباززد#باززن\nبازستاند#بازستان\nبازشمارد#بازشمار\nبازشمرد#بازشمار\nبازشمرد#بازشمر\nبازشناخت#بازشناس\nبازشناساند#بازشناسان\nبازفرستاد#بازفرست\nبازماند#بازمان\nبازنشست#بازنشین\nبازنمایاند#بازنمایان\nبازنهاد#بازنه\nبازنگریست#بازنگر\nبازپرسید#بازپرس\nبازگذارد#بازگذار\nبازگذاشت#بازگذار\nبازگرداند#بازگردان\nبازگردانید#بازگردان\nبازگردید#بازگرد\nبازگرفت#بازگیر\nبازگشت#بازگرد\nبازگشود#بازگشا\nبازگفت#بازگو\nبازیافت#بازیاب\nبافت#باف\nبالید#بال\nباوراند#باوران\nبایست#باید\nبخشود#بخش\nبخشود#بخشا\nبخشید#بخش\nبر#برخواه\nبرآشفت#برآشوب\nبرآمد#برآ\nبرآورد#برآور\nبرازید#براز\nبرافتاد#برافت\nبرافراخت#برافراز\nبرافراشت#برافراز\nبرافروخت#برافروز\nبرافشاند#برافشان\nبرافکند#برافکن\nبراند#بران\nبرانداخت#برانداز\nبرانگیخت#برانگیز\nبربست#بربند\nبرتاباند#برتابان\nبرتابید#برتاب\nبرتافت#برتاب\nبرتنید#برتن\nبرجهید#برجه\nبرخاست#برخیز\nبرخورد#برخور\nبرد#بر\nبرداشت#بردار\nبردمید#بردم\nبرزد#برزن\nبرشد#برشو\nبرشمارد#برشمار\nبرشمرد#برشمار\nبرشمرد#برشمر\nبرنشاند#برنشان\nبرنشانید#برنشان\nبرنشست#برنشین\nبرنهاد#برنه\nبرچید#برچین\nبرکرد#برکن\nبرکشید#برکش\nبرکند#برکن\nبرگذشت#برگذر\nبرگرداند#برگردان\nبرگردانید#برگردان\nبرگردید#برگرد\nبرگرفت#برگیر\nبرگزید#برگزین\nبرگشت#برگرد\nبرگشود#برگشا\nبرگمارد#برگمار\nبرگمارید#برگمار\nبرگماشت#برگمار\nبرید#بر\nبست#بند\nبلعید#بلع\nبود#باش\nبوسید#بوس\nبویید#بو\nبیخت#بیز\nبیخت#بوز\nتاباند#تابان\nتابید#تاب\nتاخت#تاز\nتاراند#تاران\nتازاند#تازان\nتازید#تاز\nتافت#تاب\nترادیسید#ترادیس\nتراشاند#تراشان\nتراشید#تراش\nتراوید#تراو\nترساند#ترسان\nترسید#ترس\nترشاند#ترشان\nترشید#ترش\nترکاند#ترکان\nترکید#ترک\nتفتید#تفت\nتمرگید#تمرگ\nتنید#تن\nتوانست#توان\nتوفید#توف\nتپاند#تپان\nتپید#تپ\nتکاند#تکان\nتکانید#تکان\nجست#جه\nجست#جو\nجنباند#جنبان\nجنبید#جنب\nجنگید#جنگ\nجهاند#جهان\nجهید#جه\nجوشاند#جوشان\nجوشانید#جوشان\nجوشید#جوش\nجويد#جو\nجوید#جو\nخاراند#خاران\nخارید#خار\nخاست#خیز\nخایید#خا\nخراشاند#خراشان\nخراشید#خراش\nخرامید#خرام\nخروشید#خروش\nخرید#خر\nخزید#خز\nخسبید#خسب\nخشکاند#خشکان\nخشکید#خشک\nخفت#خواب\nخلید#خل\nخماند#خمان\nخمید#خم\nخنداند#خندان\nخندانید#خندان\nخندید#خند\nخواباند#خوابان\nخوابانید#خوابان\nخوابید#خواب\nخواست#خواه\nخواست#خیز\nخواند#خوان\nخوراند#خوران\nخورد#خور\nخیزاند#خیزان\nخیساند#خیسان\nداد#ده\nداشت#دار\nدانست#دان\nدر#درخواه\nدرآمد#درآ\nدرآمیخت#درآمیز\nدرآورد#درآور\nدرآویخت#درآویز\nدرافتاد#درافت\nدرافکند#درافکن\nدرانداخت#درانداز\nدرانید#دران\nدربرد#دربر\nدربرگرفت#دربرگیر\nدرخشاند#درخشان\nدرخشانید#درخشان\nدرخشید#درخش\nدرداد#درده\nدررفت#دررو\nدرماند#درمان\nدرنمود#درنما\nدرنوردید#درنورد\nدرود#درو\nدروید#درو\nدرکرد#درکن\nدرکشید#درکش\nدرگذشت#درگذر\nدرگرفت#درگیر\nدریافت#دریاب\nدرید#در\nدزدید#دزد\nدمید#دم\nدواند#دوان\nدوخت#دوز\nدوشید#دوش\nدوید#دو\nدید#بین\nراند#ران\nربود#ربا\nربود#روب\nرخشید#رخش\nرساند#رسان\nرسانید#رسان\nرست#ره\nرست#رو\nرسید#رس\nرشت#ریس\nرفت#رو\nرفت#روب\nرقصاند#رقصان\nرقصید#رقص\nرماند#رمان\nرمانید#رمان\nرمید#رم\nرنجاند#رنجان\nرنجانید#رنجان\nرنجید#رنج\nرندید#رند\nرهاند#رهان\nرهانید#رهان\nرهید#ره\nروبید#روب\nروفت#روب\nرویاند#رویان\nرویانید#رویان\nرویید#رو\nرویید#روی\nریخت#ریز\nرید#رین\nریدن#رین\nریسید#ریس\nزاد#زا\nزارید#زار\nزایاند#زایان\nزایید#زا\nزد#زن\nزدود#زدا\nزیست#زی\nساباند#سابان\nسابید#ساب\nساخت#ساز\nسایید#سا\nستاد#ستان\nستاند#ستان\nسترد#ستر\nستود#ستا\nستیزید#ستیز\nسراند#سران\nسرایید#سرا\nسرشت#سرش\nسرود#سرا\nسرکشید#سرکش\nسرگرفت#سرگیر\nسرید#سر\nسزید#سز\nسفت#سنب\nسنجید#سنج\nسوخت#سوز\nسود#سا\nسوزاند#سوزان\nسپارد#سپار\nسپرد#سپار\nسپرد#سپر\nسپوخت#سپوز\nسگالید#سگال\nشاشید#شاش\nشایست#\nشایست#شاید\nشتاباند#شتابان\nشتابید#شتاب\nشتافت#شتاب\nشد#شو\nشست#شو\nشست#شوی\nشلید#شل\nشمار#شمر\nشمارد#شمار\nشمرد#شمار\nشمرد#شمر\nشناخت#شناس\nشناساند#شناسان\nشنفت#شنو\nشنید#شنو\nشوتید#شوت\nشوراند#شوران\nشورید#شور\nشکافت#شکاف\nشکاند#شکان\nشکاند#شکن\nشکست#شکن\nشکفت#شکف\nطلبید#طلب\nطپید#طپ\nغراند#غران\nغرید#غر\nغلتاند#غلتان\nغلتانید#غلتان\nغلتید#غلت\nغلطاند#غلطان\nغلطانید#غلطان\nغلطید#غلط\nفرا#فراخواه\nفراخواند#فراخوان\nفراداشت#فرادار\nفرارسید#فرارس\nفرانمود#فرانما\nفراگرفت#فراگیر\nفرستاد#فرست\nفرسود#فرسا\nفرمود#فرما\nفرهیخت#فرهیز\nفرو#فروخواه\nفروآمد#فروآ\nفروآورد#فروآور\nفروافتاد#فروافت\nفروافکند#فروافکن\nفروبرد#فروبر\nفروبست#فروبند\nفروخت#فروش\nفروخفت#فروخواب\nفروخورد#فروخور\nفروداد#فروده\nفرودوخت#فرودوز\nفرورفت#فرورو\nفروریخت#فروریز\nفروشکست#فروشکن\nفروفرستاد#فروفرست\nفروماند#فرومان\nفرونشاند#فرونشان\nفرونشانید#فرونشان\nفرونشست#فرونشین\nفرونمود#فرونما\nفرونهاد#فرونه\nفروپاشاند#فروپاشان\nفروپاشید#فروپاش\nفروچکید#فروچک\nفروکرد#فروکن\nفروکشید#فروکش\nفروکوبید#فروکوب\nفروکوفت#فروکوب\nفروگذارد#فروگذار\nفروگذاشت#فروگذار\nفروگرفت#فروگیر\nفریفت#فریب\nفشاند#فشان\nفشرد#فشار\nفشرد#فشر\nفلسفید#فلسف\nفهماند#فهمان\nفهمید#فهم\nقاپید#قاپ\nقبولاند#قبول\nقبولاند#قبولان\nلاسید#لاس\nلرزاند#لرزان\nلرزید#لرز\nلغزاند#لغزان\nلغزید#لغز\nلمباند#لمبان\nلمید#لم\nلنگید#لنگ\nلولید#لول\nلیسید#لیس\nماسید#ماس\nمالاند#مالان\nمالید#مال\nماند#مان\nمانست#مان\nمرد#میر\nمویید#مو\nمکید#مک\nنازید#ناز\nنالاند#نالان\nنالید#نال\nنامید#نام\nنشاند#نشان\nنشست#نشین\nنمایاند#نما\nنمایاند#نمایان\nنمود#نما\nنهاد#نه\nنهفت#نهنب\nنواخت#نواز\nنوازید#نواز\nنوردید#نورد\nنوشاند#نوشان\nنوشانید#نوشان\nنوشت#نویس\nنوشید#نوش\nنکوهید#نکوه\nنگاشت#نگار\nنگرید#\nنگریست#نگر\nهراساند#هراسان\nهراسانید#هراسان\nهراسید#هراس\nهشت#هل\nوا#واخواه\nواداشت#وادار\nوارفت#وارو\nوارهاند#وارهان\nواماند#وامان\nوانهاد#وانه\nواکرد#واکن\nواگذارد#واگذار\nواگذاشت#واگذار\nور#ورخواه\nورآمد#ورآ\nورافتاد#ورافت\nوررفت#وررو\nورزید#ورز\nوزاند#وزان\nوزید#وز\nویراست#ویرا\nپاشاند#پاشان\nپاشید#پاش\nپالود#پالا\nپایید#پا\nپخت#پز\nپذیراند#پذیران\nپذیرفت#پذیر\nپراند#پران\nپراکند#پراکن\nپرداخت#پرداز\nپرستید#پرست\nپرسید#پرس\nپرهیخت#پرهیز\nپرهیزید#پرهیز\nپروراند#پروران\nپرورد#پرور\nپرید#پر\nپسندید#پسند\nپلاساند#پلاسان\nپلاسید#پلاس\nپلکید#پلک\nپناهاند#پناهان\nپناهید#پناه\nپنداشت#پندار\nپوساند#پوسان\nپوسید#پوس\nپوشاند#پوشان\nپوشید#پوش\nپویید#پو\nپژمرد#پژمر\nپژوهید#پژوه\nپکید#پک\nپیراست#پیرا\nپیمود#پیما\nپیوست#پیوند\nپیچاند#پیچان\nپیچانید#پیچان\nپیچید#پیچ\nچاپید#چاپ\nچایید#چا\nچراند#چران\nچرانید#چران\nچرباند#چربان\nچربید#چرب\nچرخاند#چرخان\nچرخانید#چرخان\nچرخید#چرخ\nچروکید#چروک\nچرید#چر\nچزاند#چزان\nچسباند#چسبان\nچسبید#چسب\nچسید#چس\nچشاند#چشان\nچشید#چش\nچلاند#چلان\nچلانید#چلان\nچپاند#چپان\nچپید#چپ\nچکاند#چکان\nچکید#چک\nچید#چین\nکاست#کاه\nکاشت#کار\nکاوید#کاو\nکرد#کن\nکشاند#کشان\nکشانید#کشان\nکشت#کار\nکشت#کش\nکشید#کش\nکند#کن\nکوباند#کوبان\nکوبید#کوب\nکوشید#کوش\nکوفت#کوب\nکوچانید#کوچان\nکوچید#کوچ\nگایید#گا\nگداخت#گداز\nگذارد#گذار\nگذاشت#گذار\nگذراند#گذران\nگذشت#گذر\nگرازید#گراز\nگرانید#گران\nگرایید#گرا\nگرداند#گردان\nگردانید#گردان\nگردید#گرد\nگرفت#گیر\nگروید#گرو\nگریاند#گریان\nگریخت#گریز\nگریزاند#گریزان\nگریست#گر\nگریست#گری\nگزارد#گزار\nگزاشت#گزار\nگزید#گزین\nگسارد#گسار\nگستراند#گستران\nگسترانید#گستران\nگسترد#گستر\nگسست#گسل\nگسلاند#گسل\nگسیخت#گسل\nگشاد#گشا\nگشت#گرد\nگشود#گشا\nگفت#گو\nگمارد#گمار\nگماشت#گمار\nگنجاند#گنجان\nگنجانید#گنجان\nگنجید#گنج\nگنداند#گندان\nگندید#گند\nگوارید#گوار\nگوزید#گوز\nگیراند#گیران\nیازید#یاز\nیافت#یاب\nیونید#یون\n'.strip().split()
A:spacy.lang.fa.generate_verbs_exc.(past, present)->verb_root.split('#')
A:spacy.lang.fa.generate_verbs_exc.conjugations->set(map(lambda item: item.replace('بآ', 'بیا').replace('نآ', 'نیا'), conjugations))


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ko/__init__.py----------------------------------------
A:spacy.lang.ko.__init__.idx->text.find(token, start)
A:spacy.lang.ko.__init__.self.Tokenizer->try_mecab_import()
A:spacy.lang.ko.__init__.dtokens->list(self.detailed_tokens(text))
A:spacy.lang.ko.__init__.doc->Doc(self.vocab, words=surfaces, spaces=list(check_spaces(text, surfaces)))
A:spacy.lang.ko.__init__.(first_tag, sep, eomi_tags)->dtoken['tag'].partition('+')
A:spacy.lang.ko.__init__.(tag, _, expr)->feature.partition(',')
A:spacy.lang.ko.__init__.(lemma, _, remainder)->expr.partition('/')
A:spacy.lang.ko.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
spacy.lang.ko.__init__.Korean(Language)
spacy.lang.ko.__init__.Korean.make_doc(self,text)
spacy.lang.ko.__init__.KoreanDefaults(Language.Defaults)
spacy.lang.ko.__init__.KoreanDefaults.create_tokenizer(cls,nlp=None)
spacy.lang.ko.__init__.KoreanTokenizer(self,cls,nlp=None)
spacy.lang.ko.__init__.KoreanTokenizer.__init__(self,cls,nlp=None)
spacy.lang.ko.__init__.KoreanTokenizer.detailed_tokens(self,text)
spacy.lang.ko.__init__.check_spaces(text,tokens)
spacy.lang.ko.__init__.pickle_korean(instance)
spacy.lang.ko.__init__.try_mecab_import()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ko/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ko/stop_words.py----------------------------------------
A:spacy.lang.ko.stop_words.STOP_WORDS->set('\n이\n있\n하\n것\n들\n그\n되\n수\n이\n보\n않\n없\n나\n주\n아니\n등\n같\n때\n년\n가\n한\n지\n오\n말\n일\n그렇\n위하\n때문\n그것\n두\n말하\n알\n그러나\n받\n못하\n일\n그런\n또\n더\n많\n그리고\n좋\n크\n시키\n그러\n하나\n살\n데\n안\n어떤\n번\n나\n다른\n어떻\n들\n이렇\n점\n싶\n말\n좀\n원\n잘\n놓\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ko/tag_map.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/nl/__init__.py----------------------------------------
A:spacy.lang.nl.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.nl.__init__.lex_attr_getters[NORM]->add_lookups(Language.Defaults.lex_attr_getters[NORM], BASE_NORMS)
A:spacy.lang.nl.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS, TOKENIZER_EXCEPTIONS)
A:spacy.lang.nl.__init__.lookups->Lookups()
spacy.lang.nl.__init__.Dutch(Language)
spacy.lang.nl.__init__.DutchDefaults(Language.Defaults)
spacy.lang.nl.__init__.DutchDefaults.create_lemmatizer(cls,nlp=None,lookups=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/nl/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/nl/lemmatizer.py----------------------------------------
A:spacy.lang.nl.lemmatizer.string->string.lower().lower()
A:spacy.lang.nl.lemmatizer.index_table->self.lookups.get_table('lemma_index', {})
A:spacy.lang.nl.lemmatizer.lemma_index->self.lookups.get_table('lemma_index', {}).get(univ_pos, {})
A:spacy.lang.nl.lemmatizer.exc_table->self.lookups.get_table('lemma_exc', {})
A:spacy.lang.nl.lemmatizer.exceptions->self.lookups.get_table('lemma_exc', {}).get(univ_pos, {})
A:spacy.lang.nl.lemmatizer.lookup_table->self.lookups.get_table('lemma_lookup', {})
A:spacy.lang.nl.lemmatizer.looked_up_lemma->self.lookups.get_table('lemma_lookup', {}).get(string)
A:spacy.lang.nl.lemmatizer.rules_table->self.lookups.get_table('lemma_rules', {})
A:spacy.lang.nl.lemmatizer.(forms, is_known)->self.lemmatize(string, lemma_index, exceptions, rules_table.get(univ_pos, []))
spacy.lang.nl.DutchLemmatizer(self,string,univ_pos,morphology=None)
spacy.lang.nl.DutchLemmatizer.lemmatize(self,string,index,exceptions,rules)
spacy.lang.nl.DutchLemmatizer.lookup(self,string,orth=None)
spacy.lang.nl.lemmatizer.DutchLemmatizer(self,string,univ_pos,morphology=None)
spacy.lang.nl.lemmatizer.DutchLemmatizer.__call__(self,string,univ_pos,morphology=None)
spacy.lang.nl.lemmatizer.DutchLemmatizer.lemmatize(self,string,index,exceptions,rules)
spacy.lang.nl.lemmatizer.DutchLemmatizer.lookup(self,string,orth=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/nl/lex_attrs.py----------------------------------------
A:spacy.lang.nl.lex_attrs._num_words->set('\nnul een één twee drie vier vijf zes zeven acht negen tien elf twaalf dertien\nveertien twintig dertig veertig vijftig zestig zeventig tachtig negentig honderd\nduizend miljoen miljard biljoen biljard triljoen triljard\n'.split())
A:spacy.lang.nl.lex_attrs._ordinal_words->set('\neerste tweede derde vierde vijfde zesde zevende achtste negende tiende elfde\ntwaalfde dertiende veertiende twintigste dertigste veertigste vijftigste\nzestigste zeventigste tachtigste negentigste honderdste duizendste miljoenste\nmiljardste biljoenste biljardste triljoenste triljardste\n'.split())
A:spacy.lang.nl.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.nl.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.nl.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/nl/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.nl.tokenizer_exceptions.uppered->orth.upper()
A:spacy.lang.nl.tokenizer_exceptions.capsed->orth.capitalize()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/nl/stop_words.py----------------------------------------
A:spacy.lang.nl.stop_words.STOP_WORDS->set("\naan af al alle alles allebei alleen allen als altijd ander anders andere anderen aangaangde aangezien achter achterna\nafgelopen aldus alhoewel anderzijds\n\nben bij bijna bijvoorbeeld behalve beide beiden beneden bent bepaald beter betere betreffende binnen binnenin boven\nbovenal bovendien bovenstaand buiten\n\ndaar dan dat de der den deze die dit doch doen door dus daarheen daarin daarna daarnet daarom daarop des dezelfde dezen\ndien dikwijls doet doorgaand doorgaans\n\neen eens en er echter enige eerder eerst eerste eersten effe eigen elk elke enkel enkele enz erdoor etc even eveneens\nevenwel\n\nff\n\nge geen geweest gauw gedurende gegeven gehad geheel gekund geleden gelijk gemogen geven geweest gewoon gewoonweg\ngeworden gij\n\nhaar had heb hebben heeft hem het hier hij hoe hun hadden hare hebt hele hen hierbeneden hierboven hierin hoewel hun\n\niemand iets ik in is idd ieder ikke ikzelf indien inmiddels inz inzake\n\nja je jou jouw jullie jezelf jij jijzelf jouwe juist\n\nkan kon kunnen klaar konden krachtens kunnen kunt\n\nlang later liet liever\n\nmaar me meer men met mij mijn moet mag mede meer meesten mezelf mijzelf min minder misschien mocht mochten moest moesten\nmoet moeten mogelijk mogen\n\nna naar niet niets nog nu nabij nadat net nogal nooit nr nu\n\nof om omdat ons ook op over omhoog omlaag omstreeks omtrent omver onder ondertussen ongeveer onszelf onze ooit opdat\nopnieuw opzij over overigens\n\npas pp precies prof publ\n\nreeds rond rondom\n\nsedert sinds sindsdien slechts sommige spoedig steeds\n\n‘t 't te tegen toch toen tot tamelijk ten tenzij ter terwijl thans tijdens toe totdat tussen\n\nu uit uw uitgezonderd uwe uwen\n\nvan veel voor vaak vanaf vandaan vanuit vanwege veeleer verder verre vervolgens vgl volgens vooraf vooral vooralsnog\nvoorbij voordat voordien voorheen voorop voort voorts vooruit vrij vroeg\n\nwant waren was wat we wel werd wezen wie wij wil worden waar waarom wanneer want weer weg wegens weinig weinige weldra\nwelk welke welken werd werden wiens wier wilde wordt\n\nzal ze zei zelf zich zij zijn zo zonder zou zeer zeker zekere zelfde zelfs zichzelf zijnde zijne zo’n zoals zodra zouden\n zoveel zowat zulk zulke zulks zullen zult\n".split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/nl/tag_map.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/nl/punctuation.py----------------------------------------
A:spacy.lang.nl.punctuation._quotes->char_classes.CONCAT_QUOTES.replace("'", '')


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ro/__init__.py----------------------------------------
A:spacy.lang.ro.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.ro.__init__.lex_attr_getters[NORM]->add_lookups(Language.Defaults.lex_attr_getters[NORM], BASE_NORMS)
A:spacy.lang.ro.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS, TOKENIZER_EXCEPTIONS)
spacy.lang.ro.__init__.Romanian(Language)
spacy.lang.ro.__init__.RomanianDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ro/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ro/lex_attrs.py----------------------------------------
A:spacy.lang.ro.lex_attrs._num_words->set('\nzero unu doi două trei patru cinci șase șapte opt nouă zece\nunsprezece doisprezece douăsprezece treisprezece patrusprezece cincisprezece șaisprezece șaptesprezece optsprezece nouăsprezece\ndouăzeci treizeci patruzeci cincizeci șaizeci șaptezeci optzeci nouăzeci\nsută mie milion miliard bilion trilion cvadrilion catralion cvintilion sextilion septilion enșpemii\n'.split())
A:spacy.lang.ro.lex_attrs._ordinal_words->set('\nprimul doilea treilea patrulea cincilea șaselea șaptelea optulea nouălea zecelea\nprima doua treia patra cincia șasea șaptea opta noua zecea\nunsprezecelea doisprezecelea treisprezecelea patrusprezecelea cincisprezecelea șaisprezecelea șaptesprezecelea optsprezecelea nouăsprezecelea\nunsprezecea douăsprezecea treisprezecea patrusprezecea cincisprezecea șaisprezecea șaptesprezecea optsprezecea nouăsprezecea\ndouăzecilea treizecilea patruzecilea cincizecilea șaizecilea șaptezecilea optzecilea nouăzecilea sutălea\ndouăzecea treizecea patruzecea cincizecea șaizecea șaptezecea optzecea nouăzecea suta\nmiilea mielea mia milionulea milioana miliardulea miliardelea miliarda enșpemia\n'.split())
A:spacy.lang.ro.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.ro.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.ro.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ro/tokenizer_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ro/stop_words.py----------------------------------------
A:spacy.lang.ro.stop_words.STOP_WORDS->set('\na\nabia\nacea\naceasta\naceastă\naceea\naceeasi\nacei\naceia\nacel\nacela\nacelasi\nacele\nacelea\nacest\nacesta\naceste\nacestea\nacestei\nacestia\nacestui\naceşti\naceştia\nacești\naceștia\nacolo\nacord\nacum\nadica\nai\naia\naibă\naici\naiurea\nal\nala\nalaturi\nale\nalea\nalt\nalta\naltceva\naltcineva\nalte\naltfel\nalti\naltii\naltul\nalături\nam\nanume\napoi\nar\nare\nas\nasa\nasemenea\nasta\nastazi\nastea\nastfel\nastăzi\nasupra\natare\natat\natata\natatea\natatia\nati\natit\natita\natitea\natitia\natunci\nau\navea\navem\naveţi\naveți\navut\nazi\naş\naşadar\naţi\naș\nașadar\nați\nb\nba\nbine\nbucur\nbună\nc\nca\ncam\ncand\ncapat\ncare\ncareia\ncarora\ncaruia\ncat\ncatre\ncaut\nce\ncea\nceea\ncei\nceilalti\ncel\ncele\ncelor\nceva\nchiar\nci\ncinci\ncind\ncine\ncineva\ncit\ncita\ncite\nciteva\nciti\ncitiva\nconform\ncontra\ncu\ncui\ncum\ncumva\ncurând\ncurînd\ncând\ncât\ncâte\ncâtva\ncâţi\ncâți\ncînd\ncît\ncîte\ncîtva\ncîţi\ncîți\ncă\ncăci\ncărei\ncăror\ncărui\ncătre\nd\nda\ndaca\ndacă\ndar\ndat\ndatorită\ndată\ndau\nde\ndeasupra\ndeci\ndecit\ndegraba\ndeja\ndeoarece\ndeparte\ndesi\ndespre\ndeşi\ndeși\ndin\ndinaintea\ndintr\ndintr-\ndintre\ndoar\ndoi\ndoilea\ndouă\ndrept\ndupa\ndupă\ndă\ne\nea\nei\nel\nele\nera\neram\neste\neu\nexact\neşti\nești\nf\nface\nfara\nfata\nfel\nfi\nfie\nfiecare\nfii\nfim\nfiu\nfiţi\nfiți\nfoarte\nfost\nfrumos\nfără\ng\ngeaba\ngraţie\ngrație\nh\nhalbă\ni\nia\niar\nieri\nii\nil\nimi\nin\ninainte\ninapoi\ninca\nincit\ninsa\nintr\nintre\nisi\niti\nj\nk\nl\nla\nle\nli\nlor\nlui\nlângă\nlîngă\nm\nma\nmai\nmare\nmea\nmei\nmele\nmereu\nmeu\nmi\nmie\nmine\nmod\nmult\nmulta\nmulte\nmulti\nmultă\nmulţi\nmulţumesc\nmulți\nmulțumesc\nmâine\nmîine\nmă\nn\nne\nnevoie\nni\nnici\nniciodata\nnicăieri\nnimeni\nnimeri\nnimic\nniste\nnişte\nniște\nnoastre\nnoastră\nnoi\nnoroc\nnostri\nnostru\nnou\nnoua\nnouă\nnoştri\nnoștri\nnu\nnumai\no\nopt\nor\nori\noricare\norice\noricine\noricum\noricând\noricât\noricînd\noricît\noriunde\np\npai\nparca\npatra\npatru\npatrulea\npe\npentru\npeste\npic\npina\nplus\npoate\npot\nprea\nprima\nprimul\nprin\nprintr-\nputini\npuţin\npuţina\npuţină\npuțin\npuțina\npuțină\npână\npînă\nr\nrog\ns\nsa\nsa-mi\nsa-ti\nsai\nsale\nsau\nse\nsi\nsint\nsintem\nspate\nspre\nsub\nsunt\nsuntem\nsunteţi\nsunteți\nsus\nsută\nsînt\nsîntem\nsînteţi\nsînteți\nsă\nsăi\nsău\nt\nta\ntale\nte\nti\ntimp\ntine\ntoata\ntoate\ntoată\ntocmai\ntot\ntoti\ntotul\ntotusi\ntotuşi\ntotuși\ntoţi\ntoți\ntrei\ntreia\ntreilea\ntu\ntuturor\ntăi\ntău\nu\nul\nului\nun\nuna\nunde\nundeva\nunei\nuneia\nunele\nuneori\nunii\nunor\nunora\nunu\nunui\nunuia\nunul\nv\nva\nvi\nvoastre\nvoastră\nvoi\nvom\nvor\nvostru\nvouă\nvoştri\nvoștri\nvreme\nvreo\nvreun\nvă\nx\nz\nzece\nzero\nzi\nzice\nîi\nîl\nîmi\nîmpotriva\nîn\nînainte\nînaintea\nîncotro\nîncât\nîncît\nîntre\nîntrucât\nîntrucît\nîţi\nîți\năla\nălea\năsta\năstea\năştia\năștia\nşapte\nşase\nşi\nştiu\nţi\nţie\nșapte\nșase\nși\nștiu\nți\nție\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ro/tag_map.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/pl/__init__.py----------------------------------------
A:spacy.lang.pl.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.pl.__init__.lex_attr_getters[NORM]->add_lookups(Language.Defaults.lex_attr_getters[NORM], BASE_NORMS)
A:spacy.lang.pl.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS, TOKENIZER_EXCEPTIONS)
spacy.lang.pl.__init__.Polish(Language)
spacy.lang.pl.__init__.PolishDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/pl/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/pl/lex_attrs.py----------------------------------------
A:spacy.lang.pl.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.pl.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.pl.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/pl/_tokenizer_exceptions_list.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/pl/tokenizer_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/pl/stop_words.py----------------------------------------
A:spacy.lang.pl.stop_words.STOP_WORDS->set('\na aby ach acz aczkolwiek aj albo ale alez\należ ani az aż\n\nbardziej bardzo beda bede bedzie bez bo bowiem by\nbyc byl byla byli bylo byly bym bynajmniej być był\nbyła było były będzie będą będę\n\ncala cali caly cała cały chce choć ci cie\nciebie cię co cokolwiek coraz cos coś czasami czasem czemu\nczy czyli często\n\ndaleko dla dlaczego dlatego do dobrze dokad dokąd\ndosc dość duzo dużo dwa dwaj dwie dwoje dzis\ndzisiaj dziś\n\ngdy gdyby gdyz gdyż gdzie gdziekolwiek gdzies gdzieś go\ngodz\n\ni ich ile im inna inne inny\ninnych iv ix iz iż\n\nja jak jakas jakaś jakby jaki jakichs jakichś jakie\njakis jakiz jakiś jakiż jakkolwiek jako jakos jakoś je jeden\njedna jednak jednakze jednakże jedno jednym jedynie jego jej jemu\njesli jest jestem jeszcze jezeli jeśli jeżeli juz już ją\n\nkazdy każdy kiedy kierunku kilka kilku kims kimś kto\nktokolwiek ktora ktore ktorego ktorej ktory ktorych ktorym ktorzy ktos\nktoś która które którego której który których którym którzy ku\n\nlecz lub\n\nma mają mam mamy mało mi miał miedzy\nmimo między mna mnie mną moga mogą moi moim moj\nmoja moje moze mozliwe mozna może możliwe można mu musi\nmy mój\n\nna nad nam nami nas nasi nasz nasza nasze\nnaszego naszych natomiast natychmiast nawet nia nic nich nie niech\nniego niej niemu nigdy nim nimi niz nią niż no\n\no obok od ok około on ona one\noni ono oraz oto owszem\n\npan pana pani po pod podczas pomimo ponad\nponiewaz ponieważ powinien powinna powinni powinno poza prawie przeciez\nprzecież przed przede przedtem przez przy\n\nraz razie roku rowniez również\n\nsam sama sie się skad skąd soba sobie sobą\nsposob sposób swoje są\n\nta tak taka taki takich takie takze także tam\nte tego tej tel temu ten teraz też to toba\ntobie tobą totez toteż totobą trzeba tu tutaj twoi twoim\ntwoj twoja twoje twym twój ty tych tylko tym tys\ntzw tę\n\nu\n\nvi vii viii\n\nw wam wami was wasi wasz wasza wasze we\nwedług wie wiele wielu więc więcej wlasnie wszyscy wszystkich wszystkie\nwszystkim wszystko wtedy wy właśnie wśród\n\nxi xii xiii xiv xv\n\nz za zaden zadna zadne zadnych zapewne zawsze zaś\nze zeby znow znowu znów zostal został\n\nżaden żadna żadne żadnych że żeby'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/pl/tag_map.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/pl/punctuation.py----------------------------------------
A:spacy.lang.pl.punctuation._quotes->char_classes.CONCAT_QUOTES.replace("'", '')


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/th/__init__.py----------------------------------------
A:spacy.lang.th.__init__.words->list(self.word_tokenize(text))
A:spacy.lang.th.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.th.__init__.lex_attr_getters[NORM]->add_lookups(Language.Defaults.lex_attr_getters[NORM], BASE_NORMS, NORM_EXCEPTIONS)
A:spacy.lang.th.__init__.tokenizer_exceptions->dict(TOKENIZER_EXCEPTIONS)
spacy.lang.th.__init__.Thai(Language)
spacy.lang.th.__init__.Thai.make_doc(self,text)
spacy.lang.th.__init__.ThaiDefaults(Language.Defaults)
spacy.lang.th.__init__.ThaiDefaults.create_tokenizer(cls,nlp=None)
spacy.lang.th.__init__.ThaiTokenizer(self,cls,nlp=None)
spacy.lang.th.__init__.ThaiTokenizer.__init__(self,cls,nlp=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/th/lex_attrs.py----------------------------------------
A:spacy.lang.th.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.th.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.th.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/th/tokenizer_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/th/stop_words.py----------------------------------------
A:spacy.lang.th.stop_words.STOP_WORDS->set('\nทั้งนี้ ดัง ขอ รวม หลังจาก เป็น หลัง หรือ ๆ เกี่ยวกับ ซึ่งได้แก่ ด้วยเพราะ ด้วยว่า ด้วยเหตุเพราะ\nด้วยเหตุว่า สุดๆ เสร็จแล้ว เช่น เข้า ถ้า ถูก ถึง ต่างๆ ใคร เปิดเผย ครา รือ ตาม ใน ได้แก่ ได้แต่\nได้ที่ ตลอดถึง นอกจากว่า นอกนั้น จริง อย่างดี ส่วน เพียงเพื่อ เดียว จัด ทั้งที ทั้งคน ทั้งตัว ไกลๆ\nถึงเมื่อใด คงจะ ถูกๆ เป็นที นับแต่ที่ นับแต่นั้น รับรอง ด้าน เป็นต้นมา ทุก กระทั่ง กระทำ จวบ ซึ่งก็ จะ\nครบครัน นับแต่ เยอะๆ เพียงไหน เปลี่ยนแปลง ไป่ ผ่านๆ เพื่อที่ รวมๆ กว้างขวาง เสียยิ่ง เปลี่ยน ผ่าน\nทรง ทว่า กันเถอะ เกี่ยวๆ ใดๆ ครั้งที่ ครั้งนั้น ครั้งนี้ ครั้งละ ครั้งหลัง ครั้งหลังสุด ร่วมกัน ร่วมด้วย ก็ตามที\nที่สุด ผิดๆ ยืนยง เยอะ ครั้งๆ ใครๆ นั่นเอง เสมือนว่า เสร็จ ตลอดศก ทั้งที่ ยืนยัน ด้วยที่ บัดนี้\nด้วยประการฉะนี้ ซึ่งกัน ตลอดทั่วถึง ตลอดทั่วทั้ง ตลอดปี เป็นการ นั่นแหละ พร้อม เถิด ทั้ง สืบเนื่อง ตั้งแต่\nกลับ กล่าวคือ กลุ่มก้อน กลุ่มๆ ครั้งครา ส่ง รวดเร็ว เสร็จสิ้น เสีย เสียก่อน เสียจน อดีต ตั้ง เกิด อาจ\nอีก ตลอดเวลา ภายหน้า ภายหลัง มอง มันๆ มองว่า มัก มักจะ มัน หาก คงอยู่ เป็นที่ เป็นที่สุด\nเป็นเพราะเป็นเพราะว่า เกี่ยวกัน เพียงไร เป็นแต่เพียง กล่าว จนบัดนี้ เป็นอัน จน จนเมื่อ จนแม้ ใกล้\nใหม่ๆ เป็นเพียง อย่างที่ ถูกต้อง ทั้งนั้น ทั้งนั้นด้วย กันดีกว่า กันดีไหม นั่นไง ตรงๆ แยะๆ เป็นต้น ใกล้ๆ\nซึ่งๆ ด้วยกัน ดังเคย เถอะ เสมือนกับ ไป คือ ขณะนี้ นอกจาก เพื่อที่จะ ขณะหนึ่ง ขวาง ครัน อยาก ไว้\nแบบ นอกจากนี้ เนื่องจาก เดียวกัน คง ให้มา อนึ่ง ก็แล้วแต่ ต้อง ข้าง เพื่อว่า จนแม้น ครั้งหนึ่ง อะไร ซึ่ง\nเกินๆ ด้วยเหตุนั้น กันและกัน รับ ระหว่าง ครั้งไหน เสร็จกัน ถึงอย่างไร ขาด ข้าฯ เข้าใจ ครบ ครั้งใด\nครบถ้วน ระยะ ไม่ เกือบ เกือบจะ เกือบๆ แก่ แก อย่างโน้น ดังกับว่า จริงจัง เยอะแยะ นั่น ด้วย ถึงแม้ว่า\nมาก ตลอดกาลนาน ตลอดระยะเวลา ตลอดจน ตลอดไป เป็นอันๆ เป็นอาทิ ก็ต่อเมื่อ สู่ เมื่อ เพื่อ ก็ กับ\nด้วยเหมือนกัน ด้วยเหตุนี้ ครั้งคราว ราย ร่วม เป็นอันมาก สูง รวมกัน รวมทั้ง ร่วมมือ เป็นเพียงว่า รวมถึง\nต่อ นะ กว้าง มา ครับ ตลอดทั้ง การ นั้นๆ น่า เป็นอันว่า เพราะ วัน จนขณะนี้ จนตลอด จนถึง ข้า อย่างใด\nไหนๆ ก่อนหน้านี้ ก่อนๆ สูงกว่า สูงส่ง สูงสุด สูงๆ เสียด้วย เสียนั่น เสียนี่ เสียนี่กระไร เสียนั่นเอง สุด\nสําหรับ ว่า ลง ภายใต้ เพื่อให้ ภายนอก ภายใน เฉพาะ ซึ่งกันและกัน ง่าย ง่ายๆ ไง ถึงแม้จะ ถึงเมื่อไร\nเกิน ก็ได้ คราใด คราที่ ตลอดวัน นับ ดังเก่า ดั่งเก่า หลาย หนึ่ง ถือว่า ก่อนหน้า นับตั้งแต่ จรด จริงๆ\nจวน จวนเจียน ตลอดมา กลุ่ม กระนั้น ข้างๆ ตรง ข้าพเจ้า กว่า เกี่ยวเนื่อง ขึ้น ให้ไป ผล แต่ เอง เห็น\nจึง ได้ ให้ โดย จริงๆจังๆ ดั่งกับว่า ทั้งนั้นเพราะ นอก นอกเหนือ น่ะ กันนะ ขณะเดียวกัน แยะ\nนอกเหนือจาก น้อย ก่อน จวนจะ ข้างเคียง ก็ตามแต่ จรดกับ น้อยกว่า นั่นเป็น นักๆ ครั้งกระนั้น เลย ไกล\nสิ้นกาลนาน ครั้ง รือว่า เก็บ อย่างเช่น บาง ดั่ง ดังกล่าว ดังกับ รึ รึว่า ออก แรก จง ยืนนาน ได้มา ตน\nตนเอง ได้รับ ระยะๆ กระผม กันไหม กันเอง กำลังจะ กำหนด กู กำลัง ความ แล้ว และ ต่าง อย่างน้อย\nอย่างนั้น อย่างนี้ ก็คือ ก็แค่ ด้วยเหตุที่ ใหญ่ๆ ให้ดี ยัง เป็นเพื่อ ก็ตาม ผู้ ต่อกัน ถือ ซึ่งก็คือ ภายภาค\nภายภาคหน้า ก็ดี ก็จะ อยู่ เสียยิ่งนัก ใหม่ ขณะ เริ่ม เรา ขวางๆ เสียแล้ว ใคร่ ใคร่จะ ตนฯ ของ แห่ง\nรวด ดั่งกับ ถึงเมื่อ น้อยๆ นับจากนั้น ตลอด ตลอดกาล เสร็จสมบูรณ์ เขียน กว้างๆ ยืนยาว ถึงแก่ ขณะใด\nขณะใดๆ ขณะที่ ขณะนั้น จนทั่ว ภาคฯ ภาย เป็นแต่ อย่าง พบ ภาค ให้แด่ เสียจนกระทั่ง เสียจนถึง\nจนกระทั่ง จนกว่า ตลอดทั่ว เป็นๆ นอกจากนั้น ผิด ครั้งก่อน แก้ไข ขั้น กัน ช่วง จาก รวมด้วย เขา\nด้วยเช่นกัน นอกจากที่ เป็นต้นไป ข้างต้น ข้างบน ข้างล่าง ถึงจะ ถึงบัดนั้น ถึงแม้ มี ทาง เคย นับจากนี้\nอย่างเดียว เกี่ยวข้อง นี้ นํา นั้น ที่ ทําให้ ทํา ครานั้น ครานี้ คราหนึ่ง คราไหน คราว คราวก่อน คราวใด\nคราวที่ คราวนั้น คราวนี้ คราวโน้น คราวละ คราวหน้า คราวหนึ่ง คราวหลัง คราวไหน คราวๆ คล้าย\nคล้ายกัน คล้ายกันกับ คล้ายกับ คล้ายกับว่า คล้ายว่า ควร ค่อน ค่อนข้าง ค่อนข้างจะ ค่อยไปทาง ค่อนมาทาง ค\n่อย ค่อยๆ คะ ค่ะ คำ คิด คิดว่า คุณ คุณๆ เคยๆ แค่ แค่จะ แค่นั้น แค่นี้ แค่เพียง แค่ว่า แค่ไหน จังๆ\nจวบกับ จวบจน จ้ะ จ๊ะ จะได้ จัง จัดการ จัดงาน จัดแจง จัดตั้ง จัดทำ จัดหา จัดให้ จับ จ้า จ๋า จากนั้น\nจากนี้ จากนี้ไป จำ จำเป็น จำพวก จึงจะ จึงเป็น จู่ๆ ฉะนั้น ฉะนี้ ฉัน เฉกเช่น เฉย เฉยๆ ไฉน ช่วงก่อน ช\n่วงต่อไป ช่วงถัดไป ช่วงท้าย ช่วงที่ ช่วงนั้น ช่วงนี้ ช่วงระหว่าง ช่วงแรก ช่วงหน้า ช่วงหลัง ช่วงๆ ช่วย ช้า\nช้านาน ชาว ช้าๆ เช่นก่อน เช่นกัน เช่นเคย เช่นดัง เช่นดังก่อน เช่นดังเก่า เช่นดังที่ เช่นดังว่า\nเช่นเดียวกัน เช่นเดียวกับ เช่นใด เช่นที่ เช่นที่เคย เช่นที่ว่า เช่นนั้น เช่นนั้นเอง เช่นนี้ เช่นเมื่อ เช่นไร\nเชื่อ เชื่อถือ เชื่อมั่น เชื่อว่า ใช่ ใช้ ซะ ซะก่อน ซะจน ซะจนกระทั่ง ซะจนถึง ดั่งเคย ต่างก็ ต่างหาก\nตามด้วย ตามแต่ ตามที่ ตามๆ เต็มไปด้วย เต็มไปหมด เต็มๆ แต่ก็ แต่ก่อน แต่จะ แต่เดิม แต่ต้อง แต่ถ้า\nแต่ทว่า แต่ที่ แต่นั้น แต่เพียง แต่เมื่อ แต่ไร แต่ละ แต่ว่า แต่ไหน แต่อย่างใด โต โตๆ ใต้ ถ้าจะ ถ้าหาก\nทั้งปวง ทั้งเป็น ทั้งมวล ทั้งสิ้น ทั้งหมด ทั้งหลาย ทั้งๆ ทัน ทันใดนั้น ทันที ทันทีทันใด ทั่ว ทำให้ ทำๆ ที ที่จริง\nที่ซึ่ง ทีเดียว ทีใด ที่ใด ที่ได้ ทีเถอะ ที่แท้ ที่แท้จริง ที่นั้น ที่นี้ ทีไร ทีละ ที่ละ ที่แล้ว ที่ว่า ที่แห่งนั้น ทีๆ ที่ๆ\nทุกคน ทุกครั้ง ทุกครา ทุกคราว ทุกชิ้น ทุกตัว ทุกทาง ทุกที ทุกที่ ทุกเมื่อ ทุกวัน ทุกวันนี้ ทุกสิ่ง ทุกหน ทุกแห่ง\nทุกอย่าง ทุกอัน ทุกๆ เท่า เท่ากัน เท่ากับ เท่าใด เท่าที่ เท่านั้น เท่านี้ แท้ แท้จริง เธอ นั้นไว นับแต่นี้\nนาง นางสาว น่าจะ นาน นานๆ นาย นำ นำพา นำมา นิด นิดหน่อย นิดๆ นี่ นี่ไง นี่นา นี่แน่ะ นี่แหละ นี้แหล่\nนี่เอง นี้เอง นู่น นู้น เน้น เนี่ย เนี่ยเอง ในช่วง ในที่ ในเมื่อ ในระหว่าง บน บอก บอกแล้ว บอกว่า บ่อย\nบ่อยกว่า บ่อยครั้ง บ่อยๆ บัดดล บัดเดี๋ยวนี้ บัดนั้น บ้าง บางกว่า บางขณะ บางครั้ง บางครา บางคราว\nบางที บางที่ บางแห่ง บางๆ ปฏิบัติ ประกอบ ประการ ประการฉะนี้ ประการใด ประการหนึ่ง ประมาณ\nประสบ ปรับ ปรากฏ ปรากฏว่า ปัจจุบัน ปิด เป็นด้วย เป็นดัง ผู้ใด เผื่อ เผื่อจะ เผื่อที่ เผื่อว่า ฝ่าย ฝ่ายใด\nพบว่า พยายาม พร้อมกัน พร้อมกับ พร้อมด้วย พร้อมทั้ง พร้อมที่ พร้อมเพียง พวก พวกกัน พวกกู พวกแก\nพวกเขา พวกคุณ พวกฉัน พวกท่าน พวกที่ พวกเธอ พวกนั้น พวกนี้ พวกนู้น พวกโน้น พวกมัน พวกมึง พอ พอกัน\nพอควร พอจะ พอดี พอตัว พอที พอที่ พอเพียง พอแล้ว พอสม พอสมควร พอเหมาะ พอๆ พา พึง พึ่ง พื้นๆ พูด\nเพราะฉะนั้น เพราะว่า เพิ่ง เพิ่งจะ เพิ่ม เพิ่มเติม เพียง เพียงแค่ เพียงใด เพียงแต่ เพียงพอ เพียงเพราะ\nมากกว่า มากมาย มิ มิฉะนั้น มิใช่ มิได้ มีแต่ มึง มุ่ง มุ่งเน้น มุ่งหมาย เมื่อก่อน เมื่อครั้ง เมื่อครั้งก่อน\nเมื่อคราวก่อน เมื่อคราวที่ เมื่อคราว เมื่อคืน เมื่อเช้า เมื่อใด เมื่อนั้น เมื่อนี้ เมื่อเย็น เมื่อวันวาน เมื่อวาน\nแม้ แม้กระทั่ง แม้แต่ แม้นว่า แม้ว่า ไม่ค่อย ไม่ค่อยจะ ไม่ค่อยเป็น ไม่ใช่ ไม่เป็นไร ไม่ว่า ยก ยกให้ ยอม\nยอมรับ ย่อม ย่อย ยังคง ยังงั้น ยังงี้ ยังโง้น ยังไง ยังจะ ยังแต่ ยาก ยาว ยาวนาน ยิ่ง ยิ่งกว่า ยิ่งขึ้น\nยิ่งขึ้นไป ยิ่งจน ยิ่งจะ ยิ่งนัก ยิ่งเมื่อ ยิ่งแล้ว ยิ่งใหญ่ เร็ว เร็วๆ เราๆ เรียก เรียบ เรื่อย เรื่อยๆ ล้วน\nล้วนจน ล้วนแต่ ละ ล่าสุด เล็ก เล็กน้อย เล็กๆ เล่าว่า แล้วกัน แล้วแต่ แล้วเสร็จ วันใด วันนั้น วันนี้ วันไหน\nสบาย สมัย สมัยก่อน สมัยนั้น สมัยนี้ สมัยโน้น ส่วนเกิน ส่วนด้อย ส่วนดี ส่วนใด ส่วนที่ ส่วนน้อย ส่วนนั้น ส\n่วนมาก ส่วนใหญ่ สั้น สั้นๆ สามารถ สำคัญ สิ่ง สิ่งใด สิ่งนั้น สิ่งนี้ สิ่งไหน สิ้น แสดง แสดงว่า หน หนอ หนอย\nหน่อย หมด หมดกัน หมดสิ้น หากแม้ หากแม้น หากแม้นว่า หากว่า หาความ หาใช่ หารือ เหตุ เหตุผล เหตุนั้น\nเหตุนี้ เหตุไร เห็นแก่ เห็นควร เห็นจะ เห็นว่า เหลือ เหลือเกิน เหล่า เหล่านั้น เหล่านี้ แห่งใด แห่งนั้น\nแห่งนี้ แห่งโน้น แห่งไหน แหละ ให้แก่ ใหญ่ ใหญ่โต อย่างมาก อย่างยิ่ง อย่างไรก็ อย่างไรก็ได้ อย่างไรเสีย\nอย่างละ อย่างหนึ่ง อย่างๆ อัน อันจะ อันได้แก่ อันที่ อันที่จริง อันที่จะ อันเนื่องมาจาก อันละ อันๆ อาจจะ\nอาจเป็น อาจเป็นด้วย อื่น อื่นๆ เอ็ง เอา ฯ ฯล ฯลฯ 555 กำ ขอโทษ เยี่ยม นี่คือ\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/th/norm_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/th/tag_map.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/tr/__init__.py----------------------------------------
A:spacy.lang.tr.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.tr.__init__.lex_attr_getters[NORM]->add_lookups(Language.Defaults.lex_attr_getters[NORM], BASE_NORMS)
A:spacy.lang.tr.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS, TOKENIZER_EXCEPTIONS)
spacy.lang.tr.__init__.Turkish(Language)
spacy.lang.tr.__init__.TurkishDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/tr/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/tr/lex_attrs.py----------------------------------------
A:spacy.lang.tr.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.tr.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.tr.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/tr/tokenizer_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/tr/stop_words.py----------------------------------------
A:spacy.lang.tr.stop_words.STOP_WORDS->set('\nacaba\nacep\nadamakıllı\nadeta\nait\nama\namma\nanca\nancak\narada\nartık\naslında\naynen\nayrıca\naz\naçıkça\naçıkçası\nbana\nbari\nbazen\nbazı\nbazısı\nbazısına\nbazısında\nbazısından\nbazısını\nbazısının\nbaşkası\nbaşkasına\nbaşkasında\nbaşkasından\nbaşkasını\nbaşkasının\nbaşka\nbelki\nben\nbende\nbenden\nbeni\nbenim\nberi\nberiki\nberikinin\nberikiyi\nberisi\nbilcümle\nbile\nbinaen\nbinaenaleyh\nbiraz\nbirazdan\nbirbiri\nbirbirine\nbirbirini\nbirbirinin\nbirbirinde\nbirbirinden\nbirden\nbirdenbire\nbiri\nbirine\nbirini\nbirinin\nbirinde\nbirinden\nbirice\nbirileri\nbirilerinde\nbirilerinden\nbirilerine\nbirilerini\nbirilerinin\nbirisi\nbirisine\nbirisini\nbirisinin\nbirisinde\nbirisinden\nbirkaç\nbirkaçı\nbirkaçına\nbirkaçını\nbirkaçının\nbirkaçında\nbirkaçından\nbirkez\nbirlikte\nbirçok\nbirçoğu\nbirçoğuna\nbirçoğunda\nbirçoğundan\nbirçoğunu\nbirçoğunun\nbirşey\nbirşeyi\nbitevi\nbiteviye\nbittabi\nbiz\nbizatihi\nbizce\nbizcileyin\nbizden\nbize\nbizi\nbizim\nbizimki\nbizzat\nboşuna\nbu\nbuna\nbunda\nbundan\nbunlar\nbunları\nbunların\nbunu\nbunun\nburacıkta\nburada\nburadan\nburası\nburasına\nburasını\nburasının\nburasında\nburasından\nböyle\nböylece\nböylecene\nböylelikle\nböylemesine\nböylesine\nbüsbütün\nbütün\ncuk\ncümlesi\ncümlesine\ncümlesini\ncümlesinin\ncümlesinden\ncümlemize\ncümlemizi\ncümlemizden\nçabuk\nçabukça\nçeşitli\nçok\nçokları\nçoklarınca\nçokluk\nçoklukla\nçokça\nçoğu\nçoğun\nçoğunca\nçoğunda\nçoğundan\nçoğunlukla\nçoğunu\nçoğunun\nçünkü\nda\ndaha\ndahası\ndahi\ndahil\ndahilen\ndaima\ndair\ndayanarak\nde\ndefa\ndek\ndemin\ndemincek\ndeminden\ndenli\nderakap\nderhal\nderken\ndeğil\ndeğin\ndiye\ndiğer\ndiğeri\ndiğerine\ndiğerini\ndiğerinden\ndolayı\ndolayısıyla\ndoğru\nedecek\neden\nederek\nedilecek\nediliyor\nedilmesi\nediyor\nelbet\nelbette\nemme\nen\nenikonu\nepey\nepeyce\nepeyi\nesasen\nesnasında\netmesi\netraflı\netraflıca\netti\nettiği\nettiğini\nevleviyetle\nevvel\nevvela\nevvelce\nevvelden\nevvelemirde\nevveli\neğer\nfakat\nfilanca\nfilancanın\ngah\ngayet\ngayetle\ngayri\ngayrı\ngelgelelim\ngene\ngerek\ngerçi\ngeçende\ngeçenlerde\ngibi\ngibilerden\ngibisinden\ngine\ngöre\ngırla\nhakeza\nhalbuki\nhalen\nhalihazırda\nhaliyle\nhandiyse\nhangi\nhangisi\nhangisine\nhangisine\nhangisinde\nhangisinden\nhani\nhariç\nhasebiyle\nhasılı\nhatta\nhele\nhem\nhenüz\nhep\nhepsi\nhepsini\nhepsinin\nhepsinde\nhepsinden\nher\nherhangi\nherkes\nherkesi\nherkesin\nherkesten\nhiç\nhiçbir\nhiçbiri\nhiçbirine\nhiçbirini\nhiçbirinin\nhiçbirinde\nhiçbirinden\nhoş\nhulasaten\niken\nila\nile\nilen\nilgili\nilk\nilla\nillaki\nimdi\nindinde\ninen\ninsermi\nise\nister\nitibaren\nitibariyle\nitibarıyla\niyi\niyice\niyicene\niçin\niş\nişte\nkadar\nkaffesi\nkah\nkala\nkanımca\nkarşın\nkaynak\nkaçı\nkaçına\nkaçında\nkaçından\nkaçını\nkaçının\nkelli\nkendi\nkendilerinde\nkendilerinden\nkendilerine\nkendilerini\nkendilerinin\nkendini\nkendisi\nkendisinde\nkendisinden\nkendisine\nkendisini\nkendisinin\nkere\nkez\nkeza\nkezalik\nkeşke\nki\nkim\nkimden\nkime\nkimi\nkiminin\nkimisi\nkimisinde\nkimisinden\nkimisine\nkimisinin\nkimse\nkimsecik\nkimsecikler\nkülliyen\nkısaca\nkısacası\nlakin\nleh\nlütfen\nmaada\nmadem\nmademki\nmamafih\nmebni\nmeđer\nmeğer\nmeğerki\nmeğerse\nmu\nmü\nmı\nmi\nnasıl\nnasılsa\nnazaran\nnaşi\nne\nneden\nnedeniyle\nnedenle\nnedenler\nnedenlerden\nnedense\nnerde\nnerden\nnerdeyse\nnere\nnerede\nnereden\nneredeyse\nneresi\nnereye\nnetekim\nneye\nneyi\nneyse\nnice\nnihayet\nnihayetinde\nnitekim\nniye\nniçin\no\nolan\nolarak\noldu\nolduklarını\noldukça\nolduğu\nolduğunu\nolmak\nolması\nolsa\nolsun\nolup\nolur\nolursa\noluyor\nona\nonca\nonculayın\nonda\nondan\nonlar\nonlara\nonlardan\nonları\nonların\nonu\nonun\nora\noracık\noracıkta\norada\noradan\noranca\noranla\noraya\noysa\noysaki\nöbür\nöbürkü\nöbürü\nöbüründe\nöbüründen\nöbürüne\nöbürünü\nönce\nönceden\nönceleri\nöncelikle\nöteki\nötekisi\nöyle\nöylece\nöylelikle\nöylemesine\nöz\npek\npekala\npeki\npekçe\npeyderpey\nrağmen\nsadece\nsahi\nsahiden\nsana\nsanki\nsen\nsenden\nseni\nsenin\nsiz\nsizden\nsizi\nsizin\nsonra\nsonradan\nsonraları\nsonunda\nşayet\nşey\nşeyden\nşeyi\nşeyler\nşu\nşuna\nşuncacık\nşunda\nşundan\nşunlar\nşunları\nşunların\nşunu\nşunun\nşura\nşuracık\nşuracıkta\nşurası\nşöyle\nşimdi\ntabii\ntam\ntamam\ntamamen\ntamamıyla\ntarafından\ntek\ntüm\nüzere\nvar\nvardı\nvasıtasıyla\nve\nvelev\nvelhasıl\nvelhasılıkelam\nveya\nveyahut\nya\nyahut\nyakinen\nyakında\nyakından\nyakınlarda\nyalnız\nyalnızca\nyani\nyapacak\nyapmak\nyaptı\nyaptıkları\nyaptığı\nyaptığını\nyapılan\nyapılması\nyapıyor\nyeniden\nyenilerde\nyerine\nyine\nyok\nyoksa\nyoluyla\nyüzünden\nzarfında\nzaten\nzati\nzira\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ta/__init__.py----------------------------------------
A:spacy.lang.ta.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
spacy.lang.ta.__init__.Tamil(Language)
spacy.lang.ta.__init__.TamilDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ta/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ta/lex_attrs.py----------------------------------------
A:spacy.lang.ta.lex_attrs.length->len(num_suffix)
A:spacy.lang.ta.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.ta.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.ta.lex_attrs.like_num(text)
spacy.lang.ta.lex_attrs.suffix_filter(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ta/stop_words.py----------------------------------------
A:spacy.lang.ta.stop_words.STOP_WORDS->set('\nஒரு\nஎன்று\nமற்றும்\nஇந்த\nஇது\nஎன்ற\nகொண்டு\nஎன்பது\nபல\nஆகும்\nஅல்லது\nஅவர்\nநான்\nஉள்ள\nஅந்த\nஇவர்\nஎன\nமுதல்\nஎன்ன\nஇருந்து\nசில\nஎன்\nபோன்ற\nவேண்டும்\nவந்து\nஇதன்\nஅது\nஅவன்\nதான்\nபலரும்\nஎன்னும்\nமேலும்\nபின்னர்\nகொண்ட\nஇருக்கும்\nதனது\nஉள்ளது\nபோது\nஎன்றும்\nஅதன்\nதன்\nபிறகு\nஅவர்கள்\nவரை\nஅவள்\nநீ\nஆகிய\nஇருந்தது\nஉள்ளன\nவந்த\nஇருந்த\nமிகவும்\nஇங்கு\nமீது\nஓர்\nஇவை\nஇந்தக்\nபற்றி\nவரும்\nவேறு\nஇரு\nஇதில்\nபோல்\nஇப்போது\nஅவரது\nமட்டும்\nஇந்தப்\nஎனும்\nமேல்\nபின்\nசேர்ந்த\nஆகியோர்\nஎனக்கு\nஇன்னும்\nஅந்தப்\nஅன்று\nஒரே\nமிக\nஅங்கு\nபல்வேறு\nவிட்டு\nபெரும்\nஅதை\nபற்றிய\nஉன்\nஅதிக\nஅந்தக்\nபேர்\nஇதனால்\nஅவை\nஅதே\nஏன்\nமுறை\nயார்\nஎன்பதை\nஎல்லாம்\nமட்டுமே\nஇங்கே\nஅங்கே\nஇடம்\nஇடத்தில்\nஅதில்\nநாம்\nஅதற்கு\nஎனவே\nபிற\nசிறு\nமற்ற\nவிட\nஎந்த\nஎனவும்\nஎனப்படும்\nஎனினும்\nஅடுத்த\nஇதனை\nஇதை\nகொள்ள\nஇந்தத்\nஇதற்கு\nஅதனால்\nதவிர\nபோல\nவரையில்\nசற்று\nஎனக்\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ta/norm_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/hu/__init__.py----------------------------------------
A:spacy.lang.hu.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.hu.__init__.lex_attr_getters[NORM]->add_lookups(Language.Defaults.lex_attr_getters[NORM], BASE_NORMS)
A:spacy.lang.hu.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS, TOKENIZER_EXCEPTIONS)
spacy.lang.hu.__init__.Hungarian(Language)
spacy.lang.hu.__init__.HungarianDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/hu/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/hu/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.hu.tokenizer_exceptions._suffixes->'-[{al}]+'.format(al=ALPHA_LOWER)
A:spacy.lang.hu.tokenizer_exceptions._numeric_exp->'({n})(({o})({n}))*[%]?'.format(n=_num, o=_ops)
A:spacy.lang.hu.tokenizer_exceptions._nums->'(({ne})|({t})|({on})|({c}))({s})?'.format(ne=_numeric_exp, t=_time_exp, on=_ord_num_or_date, c=CURRENCY, s=_suffixes)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/hu/stop_words.py----------------------------------------
A:spacy.lang.hu.stop_words.STOP_WORDS->set('\na abban ahhoz ahogy ahol aki akik akkor akár alatt amely amelyek amelyekben\namelyeket amelyet amelynek ami amikor amit amolyan amíg annak arra arról az\nazok azon azonban azt aztán azután azzal azért\n\nbe belül benne bár\n\ncikk cikkek cikkeket csak\n\nde\n\ne ebben eddig egy egyes egyetlen egyik egyre egyéb egész ehhez ekkor el ellen\nelo eloször elott elso elég előtt emilyen ennek erre ez ezek ezen ezt ezzel\nezért\n\nfel felé\n\nha hanem hiszen hogy hogyan hát\n\nide igen ill ill. illetve ilyen ilyenkor inkább is ismét ison itt\n\njobban jó jól\n\nkell kellett keressünk keresztül ki kívül között közül\n\nle legalább legyen lehet lehetett lenne lenni lesz lett\n\nma maga magát majd meg mellett mely melyek mert mi miatt mikor milyen minden\nmindenki mindent mindig mint mintha mit mivel miért mondta most már más másik\nmég míg\n\nnagy nagyobb nagyon ne nekem neki nem nincs néha néhány nélkül\n\no oda ok oket olyan ott\n\npedig persze például\n\nrá\n\ns saját sem semmi sok sokat sokkal stb. szemben szerint szinte számára szét\n\ntalán te tehát teljes ti tovább továbbá több túl ugyanis\n\nutolsó után utána\n\nvagy vagyis vagyok valaki valami valamint való van vannak vele vissza viszont\nvolna volt voltak voltam voltunk\n\náltal általában át\n\nén éppen és\n\nígy\n\nön össze\n\núgy új újabb újra\n\nő őket\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/hu/punctuation.py----------------------------------------
A:spacy.lang.hu.punctuation._concat_icons->char_classes.CONCAT_ICONS.replace('°', '')
A:spacy.lang.hu.punctuation._quotes->char_classes.CONCAT_QUOTES.replace("'", '')


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/fr/__init__.py----------------------------------------
A:spacy.lang.fr.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.fr.__init__.lex_attr_getters[NORM]->add_lookups(Language.Defaults.lex_attr_getters[NORM], BASE_NORMS)
A:spacy.lang.fr.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS, TOKENIZER_EXCEPTIONS)
A:spacy.lang.fr.__init__.lookups->Lookups()
spacy.lang.fr.__init__.French(Language)
spacy.lang.fr.__init__.FrenchDefaults(Language.Defaults)
spacy.lang.fr.__init__.FrenchDefaults.create_lemmatizer(cls,nlp=None,lookups=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/fr/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/fr/lemmatizer.py----------------------------------------
A:spacy.lang.fr.lemmatizer.lookup_table->self.lookups.get_table('lemma_lookup', {})
A:spacy.lang.fr.lemmatizer.index_table->self.lookups.get_table('lemma_index', {})
A:spacy.lang.fr.lemmatizer.exc_table->self.lookups.get_table('lemma_exc', {})
A:spacy.lang.fr.lemmatizer.rules_table->self.lookups.get_table('lemma_rules', {})
A:spacy.lang.fr.lemmatizer.lemmas->self.lemmatize(string, index_table.get(univ_pos, {}), exc_table.get(univ_pos, {}), rules_table.get(univ_pos, []))
A:spacy.lang.fr.lemmatizer.string->string.lower().lower()
spacy.lang.fr.FrenchLemmatizer(self,string,univ_pos,morphology=None)
spacy.lang.fr.FrenchLemmatizer.adj(self,string,morphology=None)
spacy.lang.fr.FrenchLemmatizer.is_base_form(self,univ_pos,morphology=None)
spacy.lang.fr.FrenchLemmatizer.lemmatize(self,string,index,exceptions,rules)
spacy.lang.fr.FrenchLemmatizer.lookup(self,string,orth=None)
spacy.lang.fr.FrenchLemmatizer.noun(self,string,morphology=None)
spacy.lang.fr.FrenchLemmatizer.punct(self,string,morphology=None)
spacy.lang.fr.FrenchLemmatizer.verb(self,string,morphology=None)
spacy.lang.fr.lemmatizer.FrenchLemmatizer(self,string,univ_pos,morphology=None)
spacy.lang.fr.lemmatizer.FrenchLemmatizer.__call__(self,string,univ_pos,morphology=None)
spacy.lang.fr.lemmatizer.FrenchLemmatizer.adj(self,string,morphology=None)
spacy.lang.fr.lemmatizer.FrenchLemmatizer.is_base_form(self,univ_pos,morphology=None)
spacy.lang.fr.lemmatizer.FrenchLemmatizer.lemmatize(self,string,index,exceptions,rules)
spacy.lang.fr.lemmatizer.FrenchLemmatizer.lookup(self,string,orth=None)
spacy.lang.fr.lemmatizer.FrenchLemmatizer.noun(self,string,morphology=None)
spacy.lang.fr.lemmatizer.FrenchLemmatizer.punct(self,string,morphology=None)
spacy.lang.fr.lemmatizer.FrenchLemmatizer.verb(self,string,morphology=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/fr/lex_attrs.py----------------------------------------
A:spacy.lang.fr.lex_attrs._num_words->set('\nzero un deux trois quatre cinq six sept huit neuf dix\nonze douze treize quatorze quinze seize dix-sept dix-huit dix-neuf\nvingt trente quarante cinquante soixante soixante-dix septante quatre-vingt huitante quatre-vingt-dix nonante\ncent mille mil million milliard billion quadrillion quintillion\nsextillion septillion octillion nonillion decillion\n'.split())
A:spacy.lang.fr.lex_attrs._ordinal_words->set('\npremier deuxième second troisième quatrième cinquième sixième septième huitième neuvième dixième\nonzième douzième treizième quatorzième quinzième seizième dix-septième dix-huitième dix-neuvième\nvingtième trentième quarantième cinquantième soixantième soixante-dixième septantième quatre-vingtième huitantième quatre-vingt-dixième nonantième\ncentième millième millionnième milliardième billionnième quadrillionnième quintillionnième\nsextillionnième septillionnième octillionnième nonillionnième decillionnième\n'.split())
A:spacy.lang.fr.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.fr.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.fr.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/fr/_tokenizer_exceptions_list.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/fr/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.fr.tokenizer_exceptions.token->'{}-ce'.format(orth)
spacy.lang.fr.tokenizer_exceptions.lower_first_letter(text)
spacy.lang.fr.tokenizer_exceptions.upper_first_letter(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/fr/syntax_iterators.py----------------------------------------
A:spacy.lang.fr.syntax_iterators.conj->doc.vocab.strings.add('conj')
A:spacy.lang.fr.syntax_iterators.np_label->doc.vocab.strings.add('NP')
A:spacy.lang.fr.syntax_iterators.seen->set()
spacy.lang.fr.syntax_iterators.noun_chunks(obj)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/fr/stop_words.py----------------------------------------
A:spacy.lang.fr.stop_words.STOP_WORDS->set("\na à â abord absolument afin ah ai aie ailleurs ainsi ait allaient allo allons\nallô alors anterieur anterieure anterieures apres après as assez attendu au\naucun aucune aujourd aujourd'hui aupres auquel aura auraient aurait auront\naussi autre autrefois autrement autres autrui aux auxquelles auxquels avaient\navais avait avant avec avoir avons ayant\n\nbah bas basee bat beau beaucoup bien bigre boum bravo brrr\n\nc' c’ ça car ce ceci cela celle celle-ci celle-là celles celles-ci celles-là celui\ncelui-ci celui-là cent cependant certain certaine certaines certains certes ces\ncet cette ceux ceux-ci ceux-là chacun chacune chaque cher chers chez chiche\nchut chère chères ci cinq cinquantaine cinquante cinquantième cinquième clac\nclic combien comme comment comparable comparables compris concernant contre\ncouic crac\n\nd' d’ da dans de debout dedans dehors deja delà depuis dernier derniere derriere\nderrière des desormais desquelles desquels dessous dessus deux deuxième\ndeuxièmement devant devers devra different differentes differents différent\ndifférente différentes différents dire directe directement dit dite dits divers\ndiverse diverses dix dix-huit dix-neuf dix-sept dixième doit doivent donc dont\ndouze douzième dring du duquel durant dès désormais\n\neffet egale egalement egales eh elle elle-même elles elles-mêmes en encore\nenfin entre envers environ es ès est et etaient étaient etais étais etait était\netant étant etc été etre être eu euh eux eux-mêmes exactement excepté extenso\nexterieur\n\nfais faisaient faisant fait façon feront fi flac floc font\n\ngens\n\nha hein hem hep hi ho holà hop hormis hors hou houp hue hui huit huitième hum\nhurrah hé hélas i il ils importe\n\nj' j’ je jusqu jusque juste\n\nl' l’ la laisser laquelle las le lequel les lesquelles lesquels leur leurs longtemps\nlors lorsque lui lui-meme lui-même là lès\n\nm' m’ ma maint maintenant mais malgre malgré maximale me meme memes merci mes mien\nmienne miennes miens mille mince minimale moi moi-meme moi-même moindres moins\nmon moyennant même mêmes\n\nn' n’ na naturel naturelle naturelles ne neanmoins necessaire necessairement neuf\nneuvième ni nombreuses nombreux non nos notamment notre nous nous-mêmes nouveau\nnul néanmoins nôtre nôtres\n\no ô oh ohé ollé olé on ont onze onzième ore ou ouf ouias oust ouste outre\nouvert ouverte ouverts où\n\npaf pan par parce parfois parle parlent parler parmi parseme partant\nparticulier particulière particulièrement pas passé pendant pense permet\npersonne peu peut peuvent peux pff pfft pfut pif pire plein plouf plus\nplusieurs plutôt possessif possessifs possible possibles pouah pour pourquoi\npourrais pourrait pouvait prealable precisement premier première premièrement\npres probable probante procedant proche près psitt pu puis puisque pur pure\n\nqu' qu’ quand quant quant-à-soi quanta quarante quatorze quatre quatre-vingt\nquatrième quatrièmement que quel quelconque quelle quelles quelqu'un quelque\nquelques quels qui quiconque quinze quoi quoique\n\nrare rarement rares relative relativement remarquable rend rendre restant reste\nrestent restrictif retour revoici revoilà rien\n\ns' s’ sa sacrebleu sait sans sapristi sauf se sein seize selon semblable semblaient\nsemble semblent sent sept septième sera seraient serait seront ses seul seule\nseulement si sien sienne siennes siens sinon six sixième soi soi-même soit\nsoixante son sont sous souvent specifique specifiques speculatif stop\nstrictement subtiles suffisant suffisante suffit suis suit suivant suivante\nsuivantes suivants suivre superpose sur surtout\n\nt' t’ ta tac tant tardive te tel telle tellement telles tels tenant tend tenir tente\ntes tic tien tienne tiennes tiens toc toi toi-même ton touchant toujours tous\ntout toute toutefois toutes treize trente tres trois troisième troisièmement\ntrop très tsoin tsouin tu té\n\nun une unes uniformement unique uniques uns\n\nva vais vas vers via vif vifs vingt vivat vive vives vlan voici voilà vont vos\nvotre vous vous-mêmes vu vé vôtre vôtres\n\nzut\n".split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/fr/tag_map.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/fr/punctuation.py----------------------------------------
A:spacy.lang.fr.punctuation.ELISION->" ' ’ ".strip().replace(' ', '').replace('\n', '')
A:spacy.lang.fr.punctuation.HYPHENS->'- – — ‐ ‑'.strip().replace(' ', '').replace('\n', '')


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/pt/__init__.py----------------------------------------
A:spacy.lang.pt.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.pt.__init__.lex_attr_getters[NORM]->add_lookups(Language.Defaults.lex_attr_getters[NORM], BASE_NORMS, NORM_EXCEPTIONS)
A:spacy.lang.pt.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS, TOKENIZER_EXCEPTIONS)
spacy.lang.pt.__init__.Portuguese(Language)
spacy.lang.pt.__init__.PortugueseDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/pt/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/pt/lex_attrs.py----------------------------------------
A:spacy.lang.pt.lex_attrs.text->text.replace(',', '').replace('.', '').replace('º', '').replace('ª', '').replace(',', '').replace('.', '').replace('º', '').replace('ª', '')
A:spacy.lang.pt.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace('º', '').replace('ª', '').replace(',', '').replace('.', '').replace('º', '').replace('ª', '').split('/')
spacy.lang.pt.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/pt/tokenizer_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/pt/stop_words.py----------------------------------------
A:spacy.lang.pt.stop_words.STOP_WORDS->set('\nà às área acerca ademais adeus agora ainda algo algumas alguns ali além ambas ambos antes\nao aos apenas apoia apoio apontar após aquela aquelas aquele aqueles aqui aquilo\nas assim através atrás até aí\n\nbaixo bastante bem boa bom breve\n\ncada caminho catorze cedo cento certamente certeza cima cinco coisa com como\ncomprida comprido conhecida conhecido conselho contra contudo corrente cuja\ncujo custa cá\n\nda daquela daquele dar das de debaixo demais dentro depois des desde dessa desse\ndesta deste deve devem deverá dez dezanove dezasseis dezassete dezoito diante\ndireita disso diz dizem dizer do dois dos doze duas dá dão\n\né és ela elas ele eles em embora enquanto entre então era essa essas esse esses esta\nestado estar estará estas estava este estes esteve estive estivemos estiveram\nestiveste estivestes estou está estás estão eu eventual exemplo\n\nfalta fará favor faz fazeis fazem fazemos fazer fazes fazia faço fez fim final\nfoi fomos for fora foram forma foste fostes fui\n\ngeral grande grandes grupo\n\ninclusive iniciar inicio ir irá isso isto\n\njá\n\nlado lhe ligado local logo longe lugar lá\n\nmaior maioria maiorias mais mal mas me meio menor menos meses mesmo meu meus mil\nminha minhas momento muito muitos máximo mês\n\nna nada naquela naquele nas nem nenhuma nessa nesse nesta neste no nos nossa\nnossas nosso nossos nova novas nove novo novos num numa nunca nuns não nível nós\nnúmero números\n\nobrigada obrigado oitava oitavo oito onde ontem onze ora os ou outra outras outros\n\npara parece parte partir pegar pela pelas pelo pelos perto pode podem poder poderá\npodia pois ponto pontos por porquanto porque porquê portanto porém posição\npossivelmente posso possível pouca pouco povo primeira primeiro próprio próxima\npróximo puderam pôde põe põem\n\nquais qual qualquer quando quanto quarta quarto quatro que quem quer querem quero\nquestão quieta quieto quinta quinto quinze quê\n\nrelação\n\nsabe saber se segunda segundo sei seis sem sempre ser seria sete seu seus sexta\nsexto sim sistema sob sobre sois somente somos sou sua suas são sétima sétimo só\n\ntais tal talvez também tanta tanto tarde te tem temos tempo tendes tenho tens\ntentar tentaram tente tentei ter terceira terceiro teu teus teve tipo tive\ntivemos tiveram tiveste tivestes toda todas todo todos treze três tu tua tuas\ntudo tão têm\n\num uma umas uns usa usar último\n\nvai vais valor veja vem vens ver vez vezes vinda vindo vinte você vocês vos vossa\nvossas vosso vossos vários vão vêm vós\n\nzero\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/pt/norm_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/pt/tag_map.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/pt/punctuation.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/data/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/matcher/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/matcher/_schemas.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tokens/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tokens/underscore.py----------------------------------------
A:spacy.tokens.underscore.extensions->list(self._extensions.keys())
A:spacy.tokens.underscore.method_partial->functools.partial(method, self._obj)
A:spacy.tokens.underscore.key->self._get_key(name)
A:spacy.tokens.underscore.new_default->copy.copy(default)
A:spacy.tokens.underscore.default->kwargs.get('default')
A:spacy.tokens.underscore.getter->kwargs.get('getter')
A:spacy.tokens.underscore.setter->kwargs.get('setter')
A:spacy.tokens.underscore.method->kwargs.get('method')
A:spacy.tokens.underscore.nr_defined->sum((t is True for t in valid_opts))
spacy.tokens.underscore.Underscore(self,extensions,obj,start=None,end=None)
spacy.tokens.underscore.Underscore.__dir__(self)
spacy.tokens.underscore.Underscore.__getattr__(self,name)
spacy.tokens.underscore.Underscore.__init__(self,extensions,obj,start=None,end=None)
spacy.tokens.underscore.Underscore.__setattr__(self,name,value)
spacy.tokens.underscore.Underscore._get_key(self,name)
spacy.tokens.underscore.Underscore.get(self,name)
spacy.tokens.underscore.Underscore.has(self,name)
spacy.tokens.underscore.Underscore.set(self,name,value)
spacy.tokens.underscore.get_ext_args(**kwargs)
spacy.tokens.underscore.is_writable_attr(ext)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tokens/_serialize.py----------------------------------------
A:spacy.tokens._serialize.attrs->sorted(intify_attrs(attrs))
A:spacy.tokens._serialize.self.strings->set(msg['strings'])
A:spacy.tokens._serialize.array->array.reshape((array.shape[0], 1)).reshape((array.shape[0], 1))
A:spacy.tokens._serialize.spaces->spaces.reshape((spaces.shape[0], 1)).reshape((spaces.shape[0], 1))
A:spacy.tokens._serialize.orth_col->self.attrs.index(ORTH)
A:spacy.tokens._serialize.doc->doc.from_array(self.attrs, tokens).from_array(self.attrs, tokens)
A:spacy.tokens._serialize.msg->srsly.msgpack_loads(gzip.decompress(bytes_data))
A:spacy.tokens._serialize.lengths->numpy.fromstring(msg['lengths'], dtype='int32')
A:spacy.tokens._serialize.flat_spaces->flat_spaces.reshape((flat_spaces.size, 1)).reshape((flat_spaces.size, 1))
A:spacy.tokens._serialize.flat_tokens->flat_tokens.reshape(shape).reshape(shape)
A:spacy.tokens._serialize.self.tokens->NumpyOps().unflatten(flat_tokens, lengths)
A:spacy.tokens._serialize.self.spaces->NumpyOps().unflatten(flat_spaces, lengths)
A:spacy.tokens._serialize.self.user_data->list(msg['user_data'])
A:spacy.tokens._serialize.doc_bin->DocBin(store_user_data=True).from_bytes(byte_string)
spacy.tokens.DocBin(self,attrs=None,store_user_data=False)
spacy.tokens.DocBin.__len__(self)
spacy.tokens.DocBin.add(self,doc)
spacy.tokens.DocBin.from_bytes(self,bytes_data)
spacy.tokens.DocBin.get_docs(self,vocab)
spacy.tokens.DocBin.merge(self,other)
spacy.tokens.DocBin.to_bytes(self)
spacy.tokens._serialize.DocBin(self,attrs=None,store_user_data=False)
spacy.tokens._serialize.DocBin.__init__(self,attrs=None,store_user_data=False)
spacy.tokens._serialize.DocBin.__len__(self)
spacy.tokens._serialize.DocBin.add(self,doc)
spacy.tokens._serialize.DocBin.from_bytes(self,bytes_data)
spacy.tokens._serialize.DocBin.get_docs(self,vocab)
spacy.tokens._serialize.DocBin.merge(self,other)
spacy.tokens._serialize.DocBin.to_bytes(self)
spacy.tokens._serialize.merge_bins(bins)
spacy.tokens._serialize.pickle_bin(doc_bin)
spacy.tokens._serialize.unpickle_bin(byte_string)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/displacy/__init__.py----------------------------------------
A:spacy.displacy.__init__.renderer->renderer(options=options)
A:spacy.displacy.__init__._html['parsed']->renderer(options=options).render(parsed, page=page, minify=minify).strip()
A:spacy.displacy.__init__.html->RENDER_WRAPPER(html)
A:spacy.displacy.__init__.httpd->wsgiref.simple_server.make_server(host, port, app)
A:spacy.displacy.__init__.res->renderer(options=options).render(parsed, page=page, minify=minify).strip().encode(encoding='utf-8')
A:spacy.displacy.__init__.doc->Doc(orig_doc.vocab).from_bytes(orig_doc.to_bytes(exclude=['user_data']))
A:spacy.displacy.__init__.settings->get_doc_settings(doc)
spacy.displacy.__init__.app(environ,start_response)
spacy.displacy.__init__.get_doc_settings(doc)
spacy.displacy.__init__.parse_deps(orig_doc,options={})
spacy.displacy.__init__.parse_ents(doc,options={})
spacy.displacy.__init__.render(docs,style='dep',page=False,minify=False,jupyter=None,options={},manual=False)
spacy.displacy.__init__.serve(docs,style='dep',page=True,minify=False,options={},manual=False,port=5000,host='0.0.0.0')
spacy.displacy.__init__.set_render_wrapper(func)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/displacy/render.py----------------------------------------
A:spacy.displacy.render.self.compact->options.get('compact', False)
A:spacy.displacy.render.self.word_spacing->options.get('word_spacing', 45)
A:spacy.displacy.render.self.arrow_spacing->options.get('arrow_spacing', 12 if self.compact else 20)
A:spacy.displacy.render.self.arrow_width->options.get('arrow_width', 6 if self.compact else 10)
A:spacy.displacy.render.self.arrow_stroke->options.get('arrow_stroke', 2)
A:spacy.displacy.render.self.distance->options.get('distance', 150 if self.compact else 175)
A:spacy.displacy.render.self.offset_x->options.get('offset_x', 50)
A:spacy.displacy.render.self.color->options.get('color', '#000000')
A:spacy.displacy.render.self.bg->options.get('bg', '#ffffff')
A:spacy.displacy.render.self.font->options.get('font', 'Arial')
A:spacy.displacy.render.settings->p.get('settings', {})
A:spacy.displacy.render.self.direction->p.get('settings', {}).get('direction', DEFAULT_DIR)
A:spacy.displacy.render.self.lang->p.get('settings', {}).get('lang', DEFAULT_LANG)
A:spacy.displacy.render.render_id->'{}-{}'.format(id_prefix, i)
A:spacy.displacy.render.svg->self.render_svg(render_id, p['words'], p['arcs'])
A:spacy.displacy.render.content->''.join([TPL_FIGURE.format(content=svg) for svg in rendered])
A:spacy.displacy.render.markup->templates.TPL_ENTS.format(content=markup, dir=self.direction)
A:spacy.displacy.render.self.levels->self.get_levels(arcs)
A:spacy.displacy.render.self.highest_level->len(self.levels)
A:spacy.displacy.render.html_text->escape_html(text)
A:spacy.displacy.render.error_args->dict(start=start, end=end, label=label, dir=direction)
A:spacy.displacy.render.arrowhead->self.get_arrowhead(direction, x_start, y, x_end)
A:spacy.displacy.render.arc->self.get_arc(x_start, y, y_curve, x_end)
A:spacy.displacy.render.levels->set(map(lambda arc: arc['end'] - arc['start'], arcs))
A:spacy.displacy.render.user_colors->get_entry_points(ENTRY_POINTS.displacy_colors)
A:spacy.displacy.render.self.ents->options.get('ents', None)
A:spacy.displacy.render.template->options.get('template')
A:spacy.displacy.render.docs->''.join([TPL_FIGURE.format(content=doc) for doc in rendered])
A:spacy.displacy.render.additional_params->span.get('params', {})
A:spacy.displacy.render.entity->escape_html(text[start:end])
A:spacy.displacy.render.fragments->text[offset:start].split('\n')
A:spacy.displacy.render.color->self.colors.get(label.upper(), self.default_color)
spacy.displacy.DependencyRenderer(self,options={})
spacy.displacy.DependencyRenderer.get_arc(self,x_start,y,y_curve,x_end)
spacy.displacy.DependencyRenderer.get_arrowhead(self,direction,x,y,end)
spacy.displacy.DependencyRenderer.get_levels(self,arcs)
spacy.displacy.DependencyRenderer.render(self,parsed,page=False,minify=False)
spacy.displacy.DependencyRenderer.render_arrow(self,label,start,end,direction,i)
spacy.displacy.DependencyRenderer.render_svg(self,render_id,words,arcs)
spacy.displacy.DependencyRenderer.render_word(self,text,tag,i)
spacy.displacy.EntityRenderer(self,options={})
spacy.displacy.EntityRenderer.render(self,parsed,page=False,minify=False)
spacy.displacy.EntityRenderer.render_ents(self,text,spans,title)
spacy.displacy.render.DependencyRenderer(self,options={})
spacy.displacy.render.DependencyRenderer.__init__(self,options={})
spacy.displacy.render.DependencyRenderer.get_arc(self,x_start,y,y_curve,x_end)
spacy.displacy.render.DependencyRenderer.get_arrowhead(self,direction,x,y,end)
spacy.displacy.render.DependencyRenderer.get_levels(self,arcs)
spacy.displacy.render.DependencyRenderer.render(self,parsed,page=False,minify=False)
spacy.displacy.render.DependencyRenderer.render_arrow(self,label,start,end,direction,i)
spacy.displacy.render.DependencyRenderer.render_svg(self,render_id,words,arcs)
spacy.displacy.render.DependencyRenderer.render_word(self,text,tag,i)
spacy.displacy.render.EntityRenderer(self,options={})
spacy.displacy.render.EntityRenderer.__init__(self,options={})
spacy.displacy.render.EntityRenderer.render(self,parsed,page=False,minify=False)
spacy.displacy.render.EntityRenderer.render_ents(self,text,spans,title)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/displacy/templates.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/syntax/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/cli/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/cli/link.py----------------------------------------
A:spacy.cli.link.msg->Printer()
A:spacy.cli.link.model_path->util.get_package_path(origin)
A:spacy.cli.link.data_path->util.get_data_path()
spacy.cli.link(origin,link_name,force=False,model_path=None)
spacy.cli.link.link(origin,link_name,force=False,model_path=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/cli/download.py----------------------------------------
A:spacy.cli.download.msg->Printer()
A:spacy.cli.download.components->model.split('-')
A:spacy.cli.download.model_name->get_json(about.__shortcuts__, 'available shortcuts').get(model, model)
A:spacy.cli.download.dl->download_model(dl_tpl.format(m=model_name, v=version), pip_args)
A:spacy.cli.download.shortcuts->get_json(about.__shortcuts__, 'available shortcuts')
A:spacy.cli.download.compatibility->get_compatibility()
A:spacy.cli.download.version->get_version(model_name, compatibility)
A:spacy.cli.download.package_path->get_package_path(model_name)
A:spacy.cli.download.r->requests.get(url)
A:spacy.cli.download.comp_table->get_json(about.__compatibility__, 'compatibility table')
spacy.cli.download(model,direct=False,*pip_args)
spacy.cli.download.download(model,direct=False,*pip_args)
spacy.cli.download.download_model(filename,user_pip_args=None)
spacy.cli.download.get_compatibility()
spacy.cli.download.get_json(url,desc)
spacy.cli.download.get_version(model,comp)
spacy.cli.download.require_package(name)
spacy.cli.download_model(filename,user_pip_args=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/cli/debug_data.py----------------------------------------
A:spacy.cli.debug_data.msg->Printer(pretty=not no_format, ignore_warnings=ignore_warnings)
A:spacy.cli.debug_data.nlp->lang_cls()
A:spacy.cli.debug_data.lang_cls->get_lang_class(lang)
A:spacy.cli.debug_data.corpus->GoldCorpus(train_path, dev_path)
A:spacy.cli.debug_data.train_docs->list(corpus.train_docs(nlp))
A:spacy.cli.debug_data.train_docs_unpreprocessed->list(corpus.train_docs_without_preprocessing(nlp))
A:spacy.cli.debug_data.loading_train_error_message->'Training data cannot be loaded: {}'.format(str(e))
A:spacy.cli.debug_data.dev_docs->list(corpus.dev_docs(nlp))
A:spacy.cli.debug_data.loading_dev_error_message->'Development data cannot be loaded: {}'.format(str(e))
A:spacy.cli.debug_data.gold_train_data->_compile_gold(train_docs, pipeline)
A:spacy.cli.debug_data.gold_train_unpreprocessed_data->_compile_gold(train_docs_unpreprocessed, pipeline)
A:spacy.cli.debug_data.gold_dev_data->_compile_gold(dev_docs, pipeline)
A:spacy.cli.debug_data.overlap->len(train_texts.intersection(dev_texts))
A:spacy.cli.debug_data.text->'Low number of examples to train from a blank model ({})'.format(len(train_docs))
A:spacy.cli.debug_data.most_common_words->gold_train_data['words'].most_common(10)
A:spacy.cli.debug_data.labels->set((label for label in gold_train_data['ner'] if label not in ('O', '-')))
A:spacy.cli.debug_data.model_labels->_get_labels_from_model(nlp, 'textcat')
A:spacy.cli.debug_data.labels_with_counts->_format_labels(gold_train_unpreprocessed_data['deps'].most_common(), counts=True)
A:spacy.cli.debug_data.neg_docs->_get_examples_without_label(train_docs, label)
A:spacy.cli.debug_data.data->srsly.read_jsonl(file_path)
A:spacy.cli.debug_data.pipe->lang_cls().get_pipe(pipe_name)
spacy.cli.debug_data(lang,train_path,dev_path,base_model=None,pipeline='tagger,parser,ner',ignore_warnings=False,verbose=False,no_format=False)
spacy.cli.debug_data._compile_gold(train_docs,pipeline)
spacy.cli.debug_data._format_labels(labels,counts=False)
spacy.cli.debug_data._get_examples_without_label(data,label)
spacy.cli.debug_data._get_labels_from_model(nlp,pipe_name)
spacy.cli.debug_data._load_file(file_path,msg)
spacy.cli.debug_data.debug_data(lang,train_path,dev_path,base_model=None,pipeline='tagger,parser,ner',ignore_warnings=False,verbose=False,no_format=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/cli/info.py----------------------------------------
A:spacy.cli.info.msg->Printer()
A:spacy.cli.info.model_path->util.get_package_path(model)
A:spacy.cli.info.meta->srsly.read_json(meta_path)
A:spacy.cli.info.meta['link']->path2str(model_path)
A:spacy.cli.info.meta['source']->path2str(model_path)
A:spacy.cli.info.title->"Info about model '{}'".format(model)
A:spacy.cli.info.data_path->util.get_data_path()
spacy.cli.info(model=None,markdown=False,silent=False)
spacy.cli.info.info(model=None,markdown=False,silent=False)
spacy.cli.info.list_models()
spacy.cli.info.print_markdown(data,title=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/cli/validate.py----------------------------------------
A:spacy.cli.validate.msg->Printer()
A:spacy.cli.validate.r->requests.get(about.__compatibility__)
A:spacy.cli.validate.current_compat->compat.get(version)
A:spacy.cli.validate.all_models->set()
A:spacy.cli.validate.model_links->get_model_links(current_compat)
A:spacy.cli.validate.model_pkgs->get_model_pkgs(current_compat, all_models)
A:spacy.cli.validate.data_path->get_data_path()
A:spacy.cli.validate.meta->srsly.read_json(meta_path)
A:spacy.cli.validate.package->pkg_name.replace('-', '_')
A:spacy.cli.validate.comp->'--> {}'.format(compat.get(data['name'], ['n/a'])[0])
A:spacy.cli.validate.version->Printer().text(data['version'], color='red', no_print=True)
spacy.cli.validate()
spacy.cli.validate.get_model_links(compat)
spacy.cli.validate.get_model_pkgs(compat,all_models)
spacy.cli.validate.get_model_row(compat,name,data,msg,model_type='package')
spacy.cli.validate.is_compat(compat,name,version)
spacy.cli.validate.is_model_path(model_path)
spacy.cli.validate.reformat_version(version)
spacy.cli.validate.validate()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/cli/profile.py----------------------------------------
A:spacy.cli.profile.msg->Printer()
A:spacy.cli.profile.inputs->_read_inputs(inputs, msg)
A:spacy.cli.profile.(imdb_train, _)->thinc.extra.datasets.imdb()
A:spacy.cli.profile.(inputs, _)->zip(*imdb_train)
A:spacy.cli.profile.nlp->load_model(model)
A:spacy.cli.profile.texts->list(itertools.islice(inputs, n_texts))
A:spacy.cli.profile.s->pstats.Stats('Profile.prof')
A:spacy.cli.profile.input_path->Path(loc)
A:spacy.cli.profile.file_->Path(loc).open()
A:spacy.cli.profile.data->srsly.json_loads(line)
spacy.cli.profile(model,inputs=None,n_texts=10000)
spacy.cli.profile._read_inputs(loc,msg)
spacy.cli.profile.parse_texts(nlp,texts)
spacy.cli.profile.profile(model,inputs=None,n_texts=10000)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/cli/pretrain.py----------------------------------------
A:spacy.cli.pretrain.config->dict(locals())
A:spacy.cli.pretrain.config[key]->str(config[key])
A:spacy.cli.pretrain.msg->Printer()
A:spacy.cli.pretrain.has_gpu->prefer_gpu()
A:spacy.cli.pretrain.output_dir->Path(output_dir)
A:spacy.cli.pretrain.texts_loc->Path(texts_loc)
A:spacy.cli.pretrain.texts->srsly.read_jsonl('-')
A:spacy.cli.pretrain.nlp->util.load_model(vectors_model)
A:spacy.cli.pretrain.model->masked_language_model(nlp.vocab, model)
A:spacy.cli.pretrain.components->_load_pretrained_tok2vec(nlp, init_tok2vec)
A:spacy.cli.pretrain.model_name->re.search('model\\d+\\.bin', str(init_tok2vec))
A:spacy.cli.pretrain.optimizer->create_default_optimizer(model.ops)
A:spacy.cli.pretrain.tracker->ProgressTracker(frequency=10000)
A:spacy.cli.pretrain.(docs, count)->make_docs(nlp, [text for (text, _) in batch], max_length=max_length, min_length=min_length)
A:spacy.cli.pretrain.loss->(d_target ** 2).sum()
A:spacy.cli.pretrain.progress->ProgressTracker(frequency=10000).update(epoch, loss, docs)
A:spacy.cli.pretrain.(predictions, backprop)->masked_language_model(nlp.vocab, model).begin_update(docs, drop=drop)
A:spacy.cli.pretrain.(loss, gradients)->get_vectors_loss(model.ops, docs, predictions, objective)
A:spacy.cli.pretrain.doc->doc.from_array([HEAD], heads).from_array([HEAD], heads)
A:spacy.cli.pretrain.heads->heads.reshape((len(doc), 1)).reshape((len(doc), 1))
A:spacy.cli.pretrain.ids->ops.flatten([doc.to_array(ID).ravel() for doc in docs])
A:spacy.cli.pretrain.(loss, d_target)->get_cossim_loss(prediction, target)
A:spacy.cli.pretrain.output_layer->chain(LN(Maxout(300, pieces=3)), Affine(output_size, drop_factor=0.0))
A:spacy.cli.pretrain.tok2vec->chain(tok2vec, flatten)
A:spacy.cli.pretrain.self.words_per_epoch->Counter()
A:spacy.cli.pretrain.self.last_time->time.time()
A:spacy.cli.pretrain.words_in_batch->sum((len(doc) for doc in docs))
A:spacy.cli.pretrain.self.prev_loss->float(self.loss)
A:spacy.cli.pretrain.n_digits->len(str(int(figure)))
A:spacy.cli.pretrain.n_decimal->min(n_decimal, max_decimal)
spacy.cli.pretrain(texts_loc,vectors_model,output_dir,width=96,depth=4,embed_rows=2000,loss_func='cosine',use_vectors=False,dropout=0.2,n_iter=1000,batch_size=3000,max_length=500,min_length=5,seed=0,n_save_every=None,init_tok2vec=None,epoch_start=None)
spacy.cli.pretrain.ProgressTracker(self,frequency=1000000)
spacy.cli.pretrain.ProgressTracker.__init__(self,frequency=1000000)
spacy.cli.pretrain.ProgressTracker.update(self,epoch,loss,docs)
spacy.cli.pretrain._smart_round(figure,width=10,max_decimal=4)
spacy.cli.pretrain.create_pretraining_model(nlp,tok2vec)
spacy.cli.pretrain.get_vectors_loss(ops,docs,prediction,objective='L2')
spacy.cli.pretrain.make_docs(nlp,batch,min_length,max_length)
spacy.cli.pretrain.make_update(model,docs,optimizer,drop=0.0,objective='L2')
spacy.cli.pretrain.pretrain(texts_loc,vectors_model,output_dir,width=96,depth=4,embed_rows=2000,loss_func='cosine',use_vectors=False,dropout=0.2,n_iter=1000,batch_size=3000,max_length=500,min_length=5,seed=0,n_save_every=None,init_tok2vec=None,epoch_start=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/cli/init_model.py----------------------------------------
A:spacy.cli.init_model.msg->Printer()
A:spacy.cli.init_model.jsonl_loc->ensure_path(jsonl_loc)
A:spacy.cli.init_model.lex_attrs->read_attrs_from_deprecated(freqs_loc, clusters_loc)
A:spacy.cli.init_model.clusters_loc->ensure_path(clusters_loc)
A:spacy.cli.init_model.freqs_loc->ensure_path(freqs_loc)
A:spacy.cli.init_model.nlp->lang_class()
A:spacy.cli.init_model.vec_added->len(nlp.vocab.vectors)
A:spacy.cli.init_model.lex_added->len(nlp.vocab)
A:spacy.cli.init_model.loc->ensure_path(loc)
A:spacy.cli.init_model.zip_file->zipfile.ZipFile(str(loc))
A:spacy.cli.init_model.names->zipfile.ZipFile(str(loc)).namelist()
A:spacy.cli.init_model.file_->zipfile.ZipFile(str(loc)).open(names[0])
A:spacy.cli.init_model.(probs, _)->read_freqs(freqs_loc)
A:spacy.cli.init_model.clusters->read_clusters(clusters_loc)
A:spacy.cli.init_model.sorted_probs->sorted(probs.items(), key=lambda item: item[1], reverse=True)
A:spacy.cli.init_model.attrs['cluster']->int(clusters[word][::-1], 2)
A:spacy.cli.init_model.lang_class->get_lang_class(lang)
A:spacy.cli.init_model.vectors_loc->ensure_path(vectors_loc)
A:spacy.cli.init_model.nlp.vocab.vectors->Vectors(data=vectors_data, keys=vector_keys)
A:spacy.cli.init_model.(vectors_data, vector_keys)->read_vectors(vectors_loc)
A:spacy.cli.init_model.f->open_file(vectors_loc)
A:spacy.cli.init_model.shape->tuple((int(size) for size in next(f).split()))
A:spacy.cli.init_model.vectors_data->numpy.zeros(shape=shape, dtype='f')
A:spacy.cli.init_model.line->line.rstrip().rstrip()
A:spacy.cli.init_model.pieces->line.rstrip().rstrip().rsplit(' ', vectors_data.shape[1])
A:spacy.cli.init_model.word->ftfy.fix_text(word)
A:spacy.cli.init_model.vectors_data[i]->numpy.asarray(pieces, dtype='f')
A:spacy.cli.init_model.counts->PreshCounter()
A:spacy.cli.init_model.(freq, doc_freq, key)->line.rstrip().rstrip().rstrip().split('\t', 2)
A:spacy.cli.init_model.freq->int(freq)
A:spacy.cli.init_model.log_total->math.log(total)
A:spacy.cli.init_model.doc_freq->int(doc_freq)
A:spacy.cli.init_model.smooth_count->PreshCounter().smoother(int(freq))
A:spacy.cli.init_model.(cluster, word, freq)->line.rstrip().rstrip().split()
spacy.cli.init_model(lang,output_dir,freqs_loc=None,clusters_loc=None,jsonl_loc=None,vectors_loc=None,prune_vectors=-1,vectors_name=None,model_name=None)
spacy.cli.init_model.add_vectors(nlp,vectors_loc,prune_vectors,name=None)
spacy.cli.init_model.create_model(lang,lex_attrs,name=None)
spacy.cli.init_model.init_model(lang,output_dir,freqs_loc=None,clusters_loc=None,jsonl_loc=None,vectors_loc=None,prune_vectors=-1,vectors_name=None,model_name=None)
spacy.cli.init_model.open_file(loc)
spacy.cli.init_model.read_attrs_from_deprecated(freqs_loc,clusters_loc)
spacy.cli.init_model.read_clusters(clusters_loc)
spacy.cli.init_model.read_freqs(freqs_loc,max_length=100,min_doc_freq=5,min_freq=50)
spacy.cli.init_model.read_vectors(vectors_loc)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/cli/evaluate.py----------------------------------------
A:spacy.cli.evaluate.msg->Printer()
A:spacy.cli.evaluate.data_path->util.ensure_path(data_path)
A:spacy.cli.evaluate.displacy_path->util.ensure_path(displacy_path)
A:spacy.cli.evaluate.corpus->GoldCorpus(data_path, data_path)
A:spacy.cli.evaluate.nlp->util.load_model(model)
A:spacy.cli.evaluate.dev_docs->list(corpus.dev_docs(nlp, gold_preproc=gold_preproc))
A:spacy.cli.evaluate.begin->timer()
A:spacy.cli.evaluate.scorer->util.load_model(model).evaluate(dev_docs, verbose=False)
A:spacy.cli.evaluate.end->timer()
A:spacy.cli.evaluate.nwords->sum((len(doc_gold[0]) for doc_gold in dev_docs))
A:spacy.cli.evaluate.(docs, golds)->zip(*dev_docs)
A:spacy.cli.evaluate.html->displacy.render(docs[:limit], style='dep', page=True, options={'compact': True})
spacy.cli.evaluate(model,data_path,gpu_id=-1,gold_preproc=False,displacy_path=None,displacy_limit=25,return_scores=False)
spacy.cli.evaluate.evaluate(model,data_path,gpu_id=-1,gold_preproc=False,displacy_path=None,displacy_limit=25,return_scores=False)
spacy.cli.evaluate.render_parses(docs,output_path,model_name='',limit=250,deps=True,ents=True)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/cli/package.py----------------------------------------
A:spacy.cli.package.msg->Printer()
A:spacy.cli.package.input_path->util.ensure_path(input_dir)
A:spacy.cli.package.output_path->util.ensure_path(output_dir)
A:spacy.cli.package.meta_path->util.ensure_path(meta_path)
A:spacy.cli.package.meta->generate_meta(input_dir, meta, msg)
A:spacy.cli.package.nlp->util.load_model_from_path(Path(model_path))
A:spacy.cli.package.response->get_raw_input(desc, default)
A:spacy.cli.package.TEMPLATE_SETUP->"\n#!/usr/bin/env python\n# coding: utf8\nfrom __future__ import unicode_literals\n\nimport io\nimport json\nfrom os import path, walk\nfrom shutil import copy\nfrom setuptools import setup\n\n\ndef load_meta(fp):\n    with io.open(fp, encoding='utf8') as f:\n        return json.load(f)\n\n\ndef list_files(data_dir):\n    output = []\n    for root, _, filenames in walk(data_dir):\n        for filename in filenames:\n            if not filename.startswith('.'):\n                output.append(path.join(root, filename))\n    output = [path.relpath(p, path.dirname(data_dir)) for p in output]\n    output.append('meta.json')\n    return output\n\n\ndef list_requirements(meta):\n    parent_package = meta.get('parent_package', 'spacy')\n    requirements = [parent_package + meta['spacy_version']]\n    if 'setup_requires' in meta:\n        requirements += meta['setup_requires']\n    if 'requirements' in meta:\n        requirements += meta['requirements']\n    return requirements\n\n\ndef setup_package():\n    root = path.abspath(path.dirname(__file__))\n    meta_path = path.join(root, 'meta.json')\n    meta = load_meta(meta_path)\n    model_name = str(meta['lang'] + '_' + meta['name'])\n    model_dir = path.join(model_name, model_name + '-' + meta['version'])\n\n    copy(meta_path, path.join(model_name))\n    copy(meta_path, model_dir)\n\n    setup(\n        name=model_name,\n        description=meta['description'],\n        author=meta['author'],\n        author_email=meta['email'],\n        url=meta['url'],\n        version=meta['version'],\n        license=meta['license'],\n        packages=[model_name],\n        package_data={model_name: list_files(model_dir)},\n        install_requires=list_requirements(meta),\n        zip_safe=False,\n    )\n\n\nif __name__ == '__main__':\n    setup_package()\n".strip()
A:spacy.cli.package.TEMPLATE_MANIFEST->'\ninclude meta.json\n'.strip()
A:spacy.cli.package.TEMPLATE_INIT->"\n# coding: utf8\nfrom __future__ import unicode_literals\n\nfrom pathlib import Path\nfrom spacy.util import load_model_from_init_py, get_model_meta\n\n\n__version__ = get_model_meta(Path(__file__).parent)['version']\n\n\ndef load(**overrides):\n    return load_model_from_init_py(__file__, **overrides)\n".strip()
spacy.cli.package(input_dir,output_dir,meta_path=None,create_meta=False,force=False)
spacy.cli.package.create_file(file_path,contents)
spacy.cli.package.generate_meta(model_path,existing_meta,msg)
spacy.cli.package.package(input_dir,output_dir,meta_path=None,create_meta=False,force=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/cli/train.py----------------------------------------
A:spacy.cli.train.msg->Printer()
A:spacy.cli.train.train_path->util.ensure_path(train_path)
A:spacy.cli.train.dev_path->util.ensure_path(dev_path)
A:spacy.cli.train.meta_path->util.ensure_path(meta_path)
A:spacy.cli.train.output_path->util.ensure_path(output_path)
A:spacy.cli.train.raw_text->list(srsly.read_jsonl(raw_text))
A:spacy.cli.train.dropout_rates->util.decaying(util.env_opt('dropout_from', 0.2), util.env_opt('dropout_to', 0.2), util.env_opt('dropout_decay', 0.0))
A:spacy.cli.train.batch_sizes->util.compounding(util.env_opt('batch_from', 100.0), util.env_opt('batch_to', 1000.0), util.env_opt('batch_compound', 1.001))
A:spacy.cli.train.nlp->lang_cls()
A:spacy.cli.train.lang_cls->util.get_lang_class(lang)
A:spacy.cli.train.pipe->lang_cls().get_pipe(pipe_name)
A:spacy.cli.train.corpus->GoldCorpus(train_path, dev_path, limit=n_examples)
A:spacy.cli.train.n_train_words->GoldCorpus(train_path, dev_path, limit=n_examples).count_train()
A:spacy.cli.train.optimizer->lang_cls().begin_training(lambda : corpus.train_tuples, device=use_gpu)
A:spacy.cli.train.components->_load_pretrained_tok2vec(nlp, init_tok2vec)
A:spacy.cli.train.train_docs->GoldCorpus(train_path, dev_path, limit=n_examples).train_docs(nlp, noise_level=noise_level, orth_variant_level=orth_variant_level, gold_preproc=gold_preproc, max_length=0)
A:spacy.cli.train.train_labels->set()
A:spacy.cli.train.(row_head, output_stats)->_configure_training_output(pipeline, use_gpu, has_beam_widths)
A:spacy.cli.train.raw_batches->util.minibatch((nlp.make_doc(rt['text']) for rt in raw_text), size=8)
A:spacy.cli.train.(docs, golds)->zip(*batch)
A:spacy.cli.train.raw_batch->list(next(raw_batches))
A:spacy.cli.train.nlp_loaded->util.load_model_from_path(epoch_model_path)
A:spacy.cli.train.dev_docs->list(corpus.dev_docs(nlp_loaded, gold_preproc=gold_preproc))
A:spacy.cli.train.nwords->sum((len(doc_gold[0]) for doc_gold in dev_docs))
A:spacy.cli.train.start_time->timer()
A:spacy.cli.train.scorer->util.load_model_from_path(epoch_model_path).evaluate(dev_docs, verbose=verbose)
A:spacy.cli.train.end_time->timer()
A:spacy.cli.train.progress->_get_progress(i, losses, scorer.scores, output_stats, beam_width=beam_width if has_beam_widths else None, cpu_wps=cpu_wps, gpu_wps=gpu_wps)
A:spacy.cli.train.textcats_per_cat->util.load_model_from_path(epoch_model_path).evaluate(dev_docs, verbose=verbose).scores.get('textcats_per_cat', {})
A:spacy.cli.train.current_score->_score_for_model(meta)
A:spacy.cli.train.best_model_path->_collate_best_model(meta, output_path, nlp.pipe_names)
A:spacy.cli.train.mean_acc->list()
A:spacy.cli.train.pbar->tqdm.tqdm(total=total, leave=False)
A:spacy.cli.train.values[lex.vocab.strings[attr]]->func(lex.orth_)
A:spacy.cli.train.weights_data->file_.read()
A:spacy.cli.train.bests[component]->_find_best(output_path, component)
A:spacy.cli.train.accs->srsly.read_json(epoch_model / 'accuracy.json')
A:spacy.cli.train.scores['dep_loss']->losses.get('parser', 0.0)
A:spacy.cli.train.scores['ner_loss']->losses.get('ner', 0.0)
A:spacy.cli.train.scores['tag_loss']->losses.get('tagger', 0.0)
A:spacy.cli.train.scores['textcat_loss']->losses.get('textcat', 0.0)
spacy.cli.train(lang,output_path,train_path,dev_path,raw_text=None,base_model=None,pipeline='tagger,parser,ner',vectors=None,n_iter=30,n_early_stopping=None,n_examples=0,use_gpu=-1,version='0.0.0',meta_path=None,init_tok2vec=None,parser_multitasks='',entity_multitasks='',noise_level=0.0,orth_variant_level=0.0,eval_beam_widths='',gold_preproc=False,learn_tokens=False,textcat_multilabel=False,textcat_arch='bow',textcat_positive_label=None,verbose=False,debug=False)
spacy.cli.train._collate_best_model(meta,output_path,components)
spacy.cli.train._configure_training_output(pipeline,use_gpu,has_beam_widths)
spacy.cli.train._create_progress_bar(total)
spacy.cli.train._find_best(experiment_dir,component)
spacy.cli.train._get_metrics(component)
spacy.cli.train._get_progress(itn,losses,dev_scores,output_stats,beam_width=None,cpu_wps=0.0,gpu_wps=0.0)
spacy.cli.train._load_pretrained_tok2vec(nlp,loc)
spacy.cli.train._load_vectors(nlp,vectors)
spacy.cli.train._score_for_model(meta)
spacy.cli.train.train(lang,output_path,train_path,dev_path,raw_text=None,base_model=None,pipeline='tagger,parser,ner',vectors=None,n_iter=30,n_early_stopping=None,n_examples=0,use_gpu=-1,version='0.0.0',meta_path=None,init_tok2vec=None,parser_multitasks='',entity_multitasks='',noise_level=0.0,orth_variant_level=0.0,eval_beam_widths='',gold_preproc=False,learn_tokens=False,textcat_multilabel=False,textcat_arch='bow',textcat_positive_label=None,verbose=False,debug=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/cli/_schemas.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/cli/convert.py----------------------------------------
A:spacy.cli.convert.msg->Printer()
A:spacy.cli.convert.input_path->Path(input_file)
A:spacy.cli.convert.input_data->Path(input_file).open('r', encoding='utf-8').read()
A:spacy.cli.convert.converter_autodetect->autodetect_ner_format(input_data)
A:spacy.cli.convert.data->func(input_data, n_sents=n_sents, seg_sents=seg_sents, use_morphology=morphology, lang=lang, model=model)
A:spacy.cli.convert.suffix->'.{}'.format(file_type)
A:spacy.cli.convert.iob_re->re.compile('\\S+\\|(O|[IB]-\\S+)')
A:spacy.cli.convert.ner_re->re.compile('\\S+\\s+(O|[IB]-\\S+)$')
A:spacy.cli.convert.line->line.strip().strip()
spacy.cli.convert(input_file,output_dir='-',file_type='json',n_sents=1,seg_sents=False,model=None,morphology=False,converter='auto',lang=None)
spacy.cli.convert.autodetect_ner_format(input_data)
spacy.cli.convert.convert(input_file,output_dir='-',file_type='json',n_sents=1,seg_sents=False,model=None,morphology=False,converter='auto',lang=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/cli/converters/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/cli/converters/conll_ner2json.py----------------------------------------
A:spacy.cli.converters.conll_ner2json.msg->Printer()
A:spacy.cli.converters.conll_ner2json.input_data->segment_sents_and_docs(input_data, n_sents, doc_delimiter, model=model, msg=msg)
A:spacy.cli.converters.conll_ner2json.doc->doc.strip().strip()
A:spacy.cli.converters.conll_ner2json.sent->sent.strip().strip()
A:spacy.cli.converters.conll_ner2json.cols->list(zip(*[line.split() for line in lines]))
A:spacy.cli.converters.conll_ner2json.biluo_ents->iob_to_biluo(iob_ents)
A:spacy.cli.converters.conll_ner2json.nlp->MultiLanguage()
A:spacy.cli.converters.conll_ner2json.sentencizer->MultiLanguage().create_pipe('sentencizer')
A:spacy.cli.converters.conll_ner2json.lines->doc.strip().strip().strip().split('\n')
A:spacy.cli.converters.conll_ner2json.nlpdoc->Doc(nlp.vocab, words=words)
A:spacy.cli.converters.conll_ner2json.sents->segment_sents_and_docs(input_data, n_sents, doc_delimiter, model=model, msg=msg).split(sent_delimiter)
spacy.cli.converters.conll_ner2json(input_data,n_sents=10,seg_sents=False,model=None,**kwargs)
spacy.cli.converters.conll_ner2json.conll_ner2json(input_data,n_sents=10,seg_sents=False,model=None,**kwargs)
spacy.cli.converters.conll_ner2json.n_sents_info(msg,n_sents)
spacy.cli.converters.conll_ner2json.segment_docs(input_data,n_sents,doc_delimiter)
spacy.cli.converters.conll_ner2json.segment_sents_and_docs(doc,n_sents,doc_delimiter,model=None,msg=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/cli/converters/conllu2json.py----------------------------------------
A:spacy.cli.converters.conllu2json.conll_tuples->read_conllx(input_data, use_morphology=use_morphology)
A:spacy.cli.converters.conllu2json.has_ner_tags->is_ner(sentence[5][0])
A:spacy.cli.converters.conllu2json.doc->create_doc(sentences, i)
A:spacy.cli.converters.conllu2json.tag_match->re.match('([A-Z_]+)-([A-Z_]+)', tag)
A:spacy.cli.converters.conllu2json.lines->sent.strip().split('\n')
A:spacy.cli.converters.conllu2json.parts->line.split('\t')
A:spacy.cli.converters.conllu2json.prefix->re.match('([A-Z_]+)-([A-Z_]+)', tag).group(1)
A:spacy.cli.converters.conllu2json.suffix->re.match('([A-Z_]+)-([A-Z_]+)', tag).group(2)
A:spacy.cli.converters.conllu2json.iob->simplify_tags(iob)
A:spacy.cli.converters.conllu2json.biluo->iob_to_biluo(iob)
spacy.cli.converters.conllu2json(input_data,n_sents=10,use_morphology=False,lang=None,**_)
spacy.cli.converters.conllu2json.conllu2json(input_data,n_sents=10,use_morphology=False,lang=None,**_)
spacy.cli.converters.conllu2json.create_doc(sentences,id)
spacy.cli.converters.conllu2json.generate_sentence(sent,has_ner_tags)
spacy.cli.converters.conllu2json.is_ner(tag)
spacy.cli.converters.conllu2json.read_conllx(input_data,use_morphology=False,n=0)
spacy.cli.converters.conllu2json.simplify_tags(iob)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/cli/converters/iob2json.py----------------------------------------
A:spacy.cli.converters.iob2json.msg->Printer()
A:spacy.cli.converters.iob2json.docs->merge_sentences(docs, n_sents)
A:spacy.cli.converters.iob2json.(words, pos, iob)->zip(*tokens)
A:spacy.cli.converters.iob2json.(words, iob)->zip(*tokens)
A:spacy.cli.converters.iob2json.biluo->iob_to_biluo(iob)
A:spacy.cli.converters.iob2json.group->list(group)
A:spacy.cli.converters.iob2json.first->list(group).pop(0)
spacy.cli.converters.iob2json(input_data,n_sents=10,*args,**kwargs)
spacy.cli.converters.iob2json.iob2json(input_data,n_sents=10,*args,**kwargs)
spacy.cli.converters.iob2json.merge_sentences(docs,n_sents)
spacy.cli.converters.iob2json.read_iob(raw_sents)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/cli/converters/jsonl2json.py----------------------------------------
A:spacy.cli.converters.jsonl2json.nlp->get_lang_class(lang)()
A:spacy.cli.converters.jsonl2json.sentencizer->get_lang_class(lang)().create_pipe('sentencizer')
A:spacy.cli.converters.jsonl2json.doc->get_lang_class(lang)().make_doc(raw_text)
A:spacy.cli.converters.jsonl2json.doc.ents->_cleanup_spans(spans)
A:spacy.cli.converters.jsonl2json.seen->set()
spacy.cli.converters.jsonl2json._cleanup_spans(spans)
spacy.cli.converters.jsonl2json.ner_jsonl2json(input_data,lang=None,n_sents=10,use_morphology=False)
spacy.cli.converters.ner_jsonl2json(input_data,lang=None,n_sents=10,use_morphology=False)

