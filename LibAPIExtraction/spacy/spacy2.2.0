
----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/__main__.py----------------------------------------
A:spacy.__main__.msg->Printer()
A:spacy.__main__.command->sys.argv.pop(1)
A:spacy.__main__.available->'Available: {}'.format(', '.join(commands))


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/__init__.py----------------------------------------
A:spacy.__init__.depr_path->overrides.get('path')
A:spacy.__init__.LangClass->util.get_lang_class(name)
spacy.__init__.blank(name,**kwargs)
spacy.__init__.info(model=None,markdown=False,silent=False)
spacy.__init__.load(name,**overrides)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lemmatizer.py----------------------------------------
A:spacy.lemmatizer.lookup_table->self.lookups.get_table('lemma_lookup', {})
A:spacy.lemmatizer.index_table->self.lookups.get_table('lemma_index', {})
A:spacy.lemmatizer.exc_table->self.lookups.get_table('lemma_exc', {})
A:spacy.lemmatizer.rules_table->self.lookups.get_table('lemma_rules', {})
A:spacy.lemmatizer.lemmas->self.lemmatize(string, index_table.get(univ_pos, {}), exc_table.get(univ_pos, {}), rules_table.get(univ_pos, []))
A:spacy.lemmatizer.string->string.lower().lower()
A:spacy.lemmatizer.forms->list(OrderedDict.fromkeys(forms))
spacy.lemmatizer.Lemmatizer(self,lookups,*args,**kwargs)
spacy.lemmatizer.Lemmatizer.__init__(self,lookups,*args,**kwargs)
spacy.lemmatizer.Lemmatizer.adj(self,string,morphology=None)
spacy.lemmatizer.Lemmatizer.adp(self,string,morphology=None)
spacy.lemmatizer.Lemmatizer.det(self,string,morphology=None)
spacy.lemmatizer.Lemmatizer.is_base_form(self,univ_pos,morphology=None)
spacy.lemmatizer.Lemmatizer.lemmatize(self,string,index,exceptions,rules)
spacy.lemmatizer.Lemmatizer.load(cls,*args,**kwargs)
spacy.lemmatizer.Lemmatizer.lookup(self,string,orth=None)
spacy.lemmatizer.Lemmatizer.noun(self,string,morphology=None)
spacy.lemmatizer.Lemmatizer.num(self,string,morphology=None)
spacy.lemmatizer.Lemmatizer.pron(self,string,morphology=None)
spacy.lemmatizer.Lemmatizer.punct(self,string,morphology=None)
spacy.lemmatizer.Lemmatizer.verb(self,string,morphology=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/_ml.py----------------------------------------
A:spacy._ml.xp->get_array_module(yh)
A:spacy._ml.norm1->get_array_module(yh).linalg.norm(vec1)
A:spacy._ml.norm2->get_array_module(yh).linalg.norm(vec2)
A:spacy._ml.learn_rate->util.env_opt('learn_rate', 0.001)
A:spacy._ml.beta1->util.env_opt('optimizer_B1', 0.9)
A:spacy._ml.beta2->util.env_opt('optimizer_B2', 0.999)
A:spacy._ml.eps->util.env_opt('optimizer_eps', 1e-08)
A:spacy._ml.L2->util.env_opt('L2_penalty', 1e-06)
A:spacy._ml.max_grad_norm->util.env_opt('grad_norm_clip', 1.0)
A:spacy._ml.optimizer->Adam(ops, learn_rate, L2=L2, beta1=beta1, beta2=beta2, eps=eps)
A:spacy._ml.lengths->NumpyOps().asarray([len(X) for X in Xs], dtype='i')
A:spacy._ml.X->NumpyOps().flatten(seqs, pad=0)
A:spacy._ml.(cpu_outputs, backprop)->wrap(concatenate_lists_fwd, concat).begin_update(_to_cpu(inputs), drop=drop)
A:spacy._ml.gpu_outputs->_to_device(ops, cpu_outputs)
A:spacy._ml.cpu_d_outputs->_to_cpu(d_outputs)
A:spacy._ml.unigrams->doc.to_array([self.attr])
A:spacy._ml.keys->self.ops.xp.concatenate(ngrams)
A:spacy._ml.(keys, vals)->self.ops.xp.unique(keys, return_counts=True)
A:spacy._ml.batch_keys->self.ops.xp.concatenate(batch_keys)
A:spacy._ml.batch_vals->self.ops.asarray(self.ops.xp.concatenate(batch_vals), dtype='f')
A:spacy._ml.Yf->self._add_padding(Yf)
A:spacy._ml.(dY, ids)->self._backprop_padding(dY, ids)
A:spacy._ml.Xf->Xf.reshape((Xf.shape[0], self.nF * self.nI)).reshape((Xf.shape[0], self.nF * self.nI))
A:spacy._ml.dY->backprop(dY, sgd=sgd)
A:spacy._ml.Wopfi->Wopfi.reshape((self.nO * self.nP, self.nF * self.nI)).reshape((self.nO * self.nP, self.nF * self.nI))
A:spacy._ml.dXf->self.ops.gemm(dY.reshape((dY.shape[0], self.nO * self.nP)), Wopfi)
A:spacy._ml.dWopfi->dWopfi.reshape((self.nO, self.nP, self.nF, self.nI)).reshape((self.nO, self.nP, self.nF, self.nI))
A:spacy._ml.Yf_padded->self.ops.xp.vstack((self.pad, Yf))
A:spacy._ml.mask->numpy.random.uniform(0.0, 1.0, (N,))
A:spacy._ml.ids->NumpyOps().asarray(ids, dtype='i')
A:spacy._ml.tokvecs->NumpyOps().allocate((5000, model.nI), dtype='f')
A:spacy._ml.hiddens->hiddens.reshape((hiddens.shape[0] * model.nF, model.nO * model.nP)).reshape((hiddens.shape[0] * model.nF, model.nO * model.nP))
A:spacy._ml.vectors->concatenate_lists(trained_vectors, static_vectors)
A:spacy._ml.acts1->predict(ids, tokvecs)
A:spacy._ml.var->wrap(concatenate_lists_fwd, concat).ops.xp.var(acts1)
A:spacy._ml.mean->wrap(concatenate_lists_fwd, concat).ops.xp.mean(acts1)
A:spacy._ml.data->NumpyOps().asarray(vectors.data)
A:spacy._ml.model->wrap(concatenate_lists_fwd, concat)
A:spacy._ml.pretrained_vectors->cfg.get('pretrained_vectors', None)
A:spacy._ml.cnn_maxout_pieces->cfg.get('cnn_maxout_pieces', 3)
A:spacy._ml.subword_features->cfg.get('subword_features', True)
A:spacy._ml.char_embed->cfg.get('char_embed', True)
A:spacy._ml.conv_depth->cfg.get('conv_depth', 2)
A:spacy._ml.bilstm_depth->kwargs.get('bilstm_depth', 0)
A:spacy._ml.norm->HashEmbed(width, embed_size, column=cols.index(NORM), name='embed_norm')
A:spacy._ml.prefix->HashEmbed(width // 2, nr_vector, column=2)
A:spacy._ml.suffix->HashEmbed(width // 2, nr_vector, column=3)
A:spacy._ml.shape->HashEmbed(width // 2, nr_vector, column=4)
A:spacy._ml.glove->StaticVectors(pretrained_vectors, width, column=cols.index(ID))
A:spacy._ml.embed->concatenate_lists(CharacterEmbed(nM=64, nC=8), FeatureExtracter(cols) >> with_flatten(norm))
A:spacy._ml.reduce_dimensions->LN(Maxout(width, 64 * 8 + width, pieces=cnn_maxout_pieces))
A:spacy._ml.convolution->Residual(ExtractWindow(nW=1) >> LN(Maxout(width, width * 3, pieces=cnn_maxout_pieces)))
A:spacy._ml.(Y, backprop)->layer.begin_update(X, drop=drop)
A:spacy._ml.ops->NumpyOps()
A:spacy._ml.output->NumpyOps().xp.ascontiguousarray(X[:, idx], dtype=X.dtype)
A:spacy._ml.dX->NumpyOps().allocate(X.shape)
A:spacy._ml.self.nO->sum(out_sizes)
A:spacy._ml.output__BO->self.predict(input__BI)
A:spacy._ml.grad__BI->self.ops.gemm(grad__BO, self.W)
A:spacy._ml.embed_size->util.env_opt('embed_size', 7000)
A:spacy._ml.token_vector_width->util.env_opt('token_vector_width', 128)
A:spacy._ml.tok2vec->Tok2Vec(width=hidden_width, embed_size=embed_width, pretrained_vectors=pretrained_vectors, cnn_maxout_pieces=cnn_maxout_pieces, subword_features=True, conv_depth=conv_depth, bilstm_depth=0)
A:spacy._ml.softmax->with_flatten(MultiSoftmax(class_nums, token_vector_width))
A:spacy._ml.indices->numpy.zeros((len(doc),), dtype='i')
A:spacy._ml.depth->cfg.get('depth', 2)
A:spacy._ml.nr_vector->cfg.get('nr_vector', 5000)
A:spacy._ml.pretrained_dims->cfg.get('pretrained_dims', 0)
A:spacy._ml.lower->HashEmbed(width, nr_vector, column=1)
A:spacy._ml.linear_model->build_bow_text_classifier(nr_class, ngram_size=cfg.get('ngram_size', 1), exclusive_classes=False)
A:spacy._ml.output_layer->Softmax(nr_class, tok2vec.nO)
A:spacy._ml.model.tok2vec->chain(tok2vec, flatten)
A:spacy._ml.context_width->cfg.get('entity_width')
A:spacy._ml.drop_factor->kwargs.get('drop_factor', 1.0)
A:spacy._ml.concat->concatenate(*layers)
A:spacy._ml.(flat_y, bp_flat_y)->concatenate(*layers).begin_update(Xs, drop=drop)
A:spacy._ml.ys->NumpyOps().unflatten(flat_y, lengths)
A:spacy._ml.random_words->_RandomWords(vocab)
A:spacy._ml.(mask, docs)->_apply_mask(docs, random_words, mask_prob=mask_prob)
A:spacy._ml.(output, backprop)->wrap(concatenate_lists_fwd, concat).begin_update(docs, drop=drop)
A:spacy._ml.self.probs->numpy.exp(numpy.array(self.probs, dtype='f'))
A:spacy._ml.index->self._cache.pop()
A:spacy._ml.N->sum((len(doc) for doc in docs))
A:spacy._ml.word->_replace_word(token.text, random_words)
A:spacy._ml.roll->numpy.random.random()
A:spacy._ml.nCv->self.ops.xp.arange(self.nC)
A:spacy._ml.doc_ids->doc.to_utf8_array(nr_char=self.nC)
A:spacy._ml.doc_vectors->self.ops.allocate((len(doc), self.nC, self.nM))
A:spacy._ml.d_doc_vectors->d_doc_vectors.reshape((len(doc_ids), self.nC, self.nM)).reshape((len(doc_ids), self.nC, self.nM))
A:spacy._ml.norm_yh->get_array_module(yh).linalg.norm(yh, axis=1, keepdims=True)
A:spacy._ml.norm_y->get_array_module(yh).linalg.norm(y, axis=1, keepdims=True)
A:spacy._ml.loss->get_array_module(yh).abs(cosine - 1).sum()
spacy._ml.CharacterEmbed(self,nM=None,nC=None,**kwargs)
spacy._ml.CharacterEmbed.__init__(self,nM=None,nC=None,**kwargs)
spacy._ml.CharacterEmbed.begin_update(self,docs,drop=0.0)
spacy._ml.CharacterEmbed.nO(self)
spacy._ml.CharacterEmbed.nV(self)
spacy._ml.MultiSoftmax(self,out_sizes,nI=None,**kwargs)
spacy._ml.MultiSoftmax.__init__(self,out_sizes,nI=None,**kwargs)
spacy._ml.MultiSoftmax.begin_update(self,input__BI,drop=0.0)
spacy._ml.MultiSoftmax.predict(self,input__BI)
spacy._ml.PrecomputableAffine(self,nO=None,nI=None,nF=None,nP=None,**kwargs)
spacy._ml.PrecomputableAffine.__init__(self,nO=None,nI=None,nF=None,nP=None,**kwargs)
spacy._ml.PrecomputableAffine._add_padding(self,Yf)
spacy._ml.PrecomputableAffine._backprop_padding(self,dY,ids)
spacy._ml.PrecomputableAffine.begin_update(self,X,drop=0.0)
spacy._ml.PrecomputableAffine.init_weights(model)
spacy._ml.PyTorchBiLSTM(nO,nI,depth,dropout=0.2)
spacy._ml.SpacyVectors(docs,drop=0.0)
spacy._ml.Tok2Vec(width,embed_size,**kwargs)
spacy._ml._RandomWords(self,vocab)
spacy._ml._RandomWords.__init__(self,vocab)
spacy._ml._RandomWords.next(self)
spacy._ml._apply_mask(docs,random_words,mask_prob=0.15)
spacy._ml._divide_array(X,size)
spacy._ml._flatten_add_lengths(seqs,pad=0,drop=0.0)
spacy._ml._replace_word(word,random_words,mask='[MASK]')
spacy._ml._to_cpu(X)
spacy._ml._to_device(ops,X)
spacy._ml._uniform_init(lo,hi)
spacy._ml._zero_init(model)
spacy._ml.asarray(ops,dtype)
spacy._ml.build_bow_text_classifier(nr_class,ngram_size=1,exclusive_classes=False,no_output_layer=False,**cfg)
spacy._ml.build_morphologizer_model(class_nums,**cfg)
spacy._ml.build_nel_encoder(embed_width,hidden_width,ner_types,**cfg)
spacy._ml.build_simple_cnn_text_classifier(tok2vec,nr_class,exclusive_classes=False,**cfg)
spacy._ml.build_tagger_model(nr_class,**cfg)
spacy._ml.build_text_classifier(nr_class,width=64,**cfg)
spacy._ml.concatenate_lists(*layers,**kwargs)
spacy._ml.cosine(vec1,vec2)
spacy._ml.cpu_softmax(X,drop=0.0)
spacy._ml.create_default_optimizer(ops,**cfg)
spacy._ml.doc2feats(cols=None)
spacy._ml.extract_ngrams(self,ngram_size,attr=LOWER)
spacy._ml.extract_ngrams.__init__(self,ngram_size,attr=LOWER)
spacy._ml.extract_ngrams.begin_update(self,docs,drop=0.0)
spacy._ml.flatten(seqs,drop=0.0)
spacy._ml.get_col(idx)
spacy._ml.get_cossim_loss(yh,y)
spacy._ml.get_token_vectors(tokens_attrs_vectors,drop=0.0)
spacy._ml.getitem(i)
spacy._ml.link_vectors_to_models(vocab)
spacy._ml.logistic(X,drop=0.0)
spacy._ml.masked_language_model(vocab,model,mask_prob=0.15)
spacy._ml.print_shape(prefix)
spacy._ml.reapply(layer,n_times)
spacy._ml.with_cpu(ops,model)
spacy._ml.zero_init(model)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/scorer.py----------------------------------------
A:spacy.scorer.self.saved_score->_roc_auc_score(self.golds, self.cands)
A:spacy.scorer.self.saved_score_at_len->len(self.golds)
A:spacy.scorer.self.tokens->PRFScore()
A:spacy.scorer.self.sbd->PRFScore()
A:spacy.scorer.self.unlabelled->PRFScore()
A:spacy.scorer.self.labelled->PRFScore()
A:spacy.scorer.self.tags->PRFScore()
A:spacy.scorer.self.ner->PRFScore()
A:spacy.scorer.self.ner_per_ents->dict()
A:spacy.scorer.self.textcat_per_cat->dict()
A:spacy.scorer.self.textcat_positive_label->model.cfg.get('positive_label', None)
A:spacy.scorer.self.textcat->PRFScore()
A:spacy.scorer.self.textcat_per_cat[label]->PRFScore()
A:spacy.scorer.gold->gold.GoldParse.from_annot_tuples(doc, zip(*gold.orig_annot))
A:spacy.scorer.gold_deps->set()
A:spacy.scorer.gold_tags->set()
A:spacy.scorer.gold_ents->set(tags_to_entities([annot[-1] for annot in gold.orig_annot]))
A:spacy.scorer.cand_deps->set()
A:spacy.scorer.cand_tags->set()
A:spacy.scorer.ent_labels->set([x[0] for x in gold_ents] + [k.label_ for k in doc.ents])
A:spacy.scorer.self.ner_per_ents[ent_label]->PRFScore()
A:spacy.scorer.cand_ents->set()
A:spacy.scorer.goldcat->max(gold.cats, key=gold.cats.get)
A:spacy.scorer.candcat->max(doc.cats, key=doc.cats.get)
A:spacy.scorer.model_labels->set(self.textcat_per_cat)
A:spacy.scorer.eval_labels->set(gold.cats)
A:spacy.scorer.(fpr, tpr, _)->_roc_curve(y_true, y_score)
A:spacy.scorer.(fps, tps, thresholds)->_binary_clf_curve(y_true, y_score)
A:spacy.scorer.fpr->numpy.repeat(np.nan, fps.shape)
A:spacy.scorer.tpr->numpy.repeat(np.nan, tps.shape)
A:spacy.scorer.y_true->numpy.ravel(y_true)
A:spacy.scorer.y_score->numpy.ravel(y_score)
A:spacy.scorer.out->numpy.cumsum(arr, axis=axis, dtype=np.float64)
A:spacy.scorer.expected->numpy.sum(arr, axis=axis, dtype=np.float64)
A:spacy.scorer.x->numpy.ravel(x)
A:spacy.scorer.y->numpy.ravel(y)
A:spacy.scorer.dx->numpy.diff(x)
A:spacy.scorer.area->area.dtype.type(area).dtype.type(area)
spacy.scorer.PRFScore(self)
spacy.scorer.PRFScore.__init__(self)
spacy.scorer.PRFScore.fscore(self)
spacy.scorer.PRFScore.precision(self)
spacy.scorer.PRFScore.recall(self)
spacy.scorer.PRFScore.score_set(self,cand,gold)
spacy.scorer.ROCAUCScore(self)
spacy.scorer.ROCAUCScore.__init__(self)
spacy.scorer.ROCAUCScore.score(self)
spacy.scorer.ROCAUCScore.score_set(self,cand,gold)
spacy.scorer.Scorer(self,eval_punct=False,pipeline=None)
spacy.scorer.Scorer.__init__(self,eval_punct=False,pipeline=None)
spacy.scorer.Scorer.ents_f(self)
spacy.scorer.Scorer.ents_p(self)
spacy.scorer.Scorer.ents_per_type(self)
spacy.scorer.Scorer.ents_r(self)
spacy.scorer.Scorer.las(self)
spacy.scorer.Scorer.score(self,doc,gold,verbose=False,punct_labels=('p','punct'))
spacy.scorer.Scorer.scores(self)
spacy.scorer.Scorer.tags_acc(self)
spacy.scorer.Scorer.textcat_score(self)
spacy.scorer.Scorer.textcats_per_cat(self)
spacy.scorer.Scorer.token_acc(self)
spacy.scorer.Scorer.uas(self)
spacy.scorer._auc(x,y)
spacy.scorer._binary_clf_curve(y_true,y_score)
spacy.scorer._roc_auc_score(y_true,y_score)
spacy.scorer._roc_curve(y_true,y_score)
spacy.scorer._stable_cumsum(arr,axis=None,rtol=1e-05,atol=1e-08)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/glossary.py----------------------------------------
spacy.explain(term)
spacy.glossary.explain(term)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/about.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/language.py----------------------------------------
A:spacy.language.lookups->cls.create_lookups(nlp)
A:spacy.language.root->util.get_module_path(cls)
A:spacy.language.lang->cls.lex_attr_getters[LANG](None)
A:spacy.language.user_lookups->util.get_entry_point(util.ENTRY_POINTS.lookups, lang, {})
A:spacy.language.data->util.load_language_data(filename)
A:spacy.language.lemmatizer->cls.create_lemmatizer(nlp, lookups=lookups)
A:spacy.language.lex_attr_getters->dict(cls.lex_attr_getters)
A:spacy.language.lex_attr_getters[IS_STOP]->functools.partial(is_stop, stops=cls.stop_words)
A:spacy.language.vocab->factory(self, **meta.get('vocab', {}))
A:spacy.language.prefixes->tuple(TOKENIZER_PREFIXES)
A:spacy.language.suffixes->tuple(TOKENIZER_SUFFIXES)
A:spacy.language.infixes->tuple(TOKENIZER_INFIXES)
A:spacy.language.tag_map->dict(TAG_MAP)
A:spacy.language.stop_words->set()
A:spacy.language.user_factories->util.get_entry_points(util.ENTRY_POINTS.factories)
A:spacy.language.self._meta->dict(meta)
A:spacy.language.vocab.vectors.name->meta.get('vectors', {}).get('name')
A:spacy.language.make_doc->factory(self, **meta.get('tokenizer', {}))
A:spacy.language.labels->OrderedDict()
A:spacy.language.labels[name]->list(pipe.labels)
A:spacy.language.msg->errors.Errors.E003.format(component=repr(component), name=name)
A:spacy.language.name->repr(component)
A:spacy.language.i->self.pipe_names.index(old_name)
A:spacy.language.doc->func(doc, **kwargs)
A:spacy.language.err->errors.Errors.E151.format(unexp=unexpected, exp=expected_keys)
A:spacy.language.gold->GoldParse(doc, **gold)
A:spacy.language.self._optimizer->create_default_optimizer(Model.ops)
A:spacy.language.(docs, golds)->zip(*docs_golds)
A:spacy.language.pipes->list(self.pipeline)
A:spacy.language.kwargs->dict(kwargs)
A:spacy.language.docs->_pipe(proc, docs, kwargs)
A:spacy.language.docs[i]->self.make_doc(doc)
A:spacy.language.docs_golds->proc.preprocess_gold(docs_golds)
A:spacy.language._->annots_brackets.pop()
A:spacy.language.self.vocab.vectors.data->thinc.neural.Model.ops.asarray(self.vocab.vectors.data)
A:spacy.language.sgd->create_default_optimizer(Model.ops)
A:spacy.language.proc._rehearsal_model->deepcopy(proc.model)
A:spacy.language.scorer->Scorer(pipeline=self.pipeline)
A:spacy.language.golds->list(golds)
A:spacy.language.(text_context1, text_context2)->itertools.tee(texts)
A:spacy.language.recent_refs->weakref.WeakSet()
A:spacy.language.old_refs->weakref.WeakSet()
A:spacy.language.original_strings_data->list(self.vocab.strings)
A:spacy.language.(keys, strings)->self.vocab.strings._cleanup_stale_strings(original_strings_data)
A:spacy.language.path->util.ensure_path(path)
A:spacy.language.serializers->OrderedDict()
A:spacy.language.deserializers->OrderedDict()
A:spacy.language.exclude->util.get_serialization_exclude(deserializers, exclude, kwargs)
A:spacy.language.self.original_pipeline->copy(nlp.pipeline)
spacy.language.BaseDefaults(object)
spacy.language.BaseDefaults.create_lemmatizer(cls,nlp=None,lookups=None)
spacy.language.BaseDefaults.create_lookups(cls,nlp=None)
spacy.language.BaseDefaults.create_tokenizer(cls,nlp=None)
spacy.language.BaseDefaults.create_vocab(cls,nlp=None)
spacy.language.DisabledPipes(self,nlp,*names)
spacy.language.DisabledPipes.__enter__(self)
spacy.language.DisabledPipes.__exit__(self,*args)
spacy.language.DisabledPipes.__init__(self,nlp,*names)
spacy.language.DisabledPipes.restore(self)
spacy.language.Language(self,vocab=True,make_doc=True,max_length=10**6,meta={},**kwargs)
spacy.language.Language.__init__(self,vocab=True,make_doc=True,max_length=10**6,meta={},**kwargs)
spacy.language.Language._format_docs_and_golds(self,docs,golds)
spacy.language.Language.add_pipe(self,component,name=None,before=None,after=None,first=None,last=None)
spacy.language.Language.begin_training(self,get_gold_tuples=None,sgd=None,component_cfg=None,**cfg)
spacy.language.Language.create_pipe(self,name,config=dict())
spacy.language.Language.disable_pipes(self,*names)
spacy.language.Language.entity(self)
spacy.language.Language.evaluate(self,docs_golds,verbose=False,batch_size=256,scorer=None,component_cfg=None)
spacy.language.Language.from_bytes(self,bytes_data,exclude=tuple(),disable=None,**kwargs)
spacy.language.Language.from_disk(self,path,exclude=tuple(),disable=None)
spacy.language.Language.get_pipe(self,name)
spacy.language.Language.has_pipe(self,name)
spacy.language.Language.linker(self)
spacy.language.Language.make_doc(self,text)
spacy.language.Language.matcher(self)
spacy.language.Language.meta(self)
spacy.language.Language.meta(self,value)
spacy.language.Language.parser(self)
spacy.language.Language.path(self)
spacy.language.Language.pipe(self,texts,as_tuples=False,n_threads=-1,batch_size=1000,disable=[],cleanup=False,component_cfg=None)
spacy.language.Language.pipe_labels(self)
spacy.language.Language.pipe_names(self)
spacy.language.Language.preprocess_gold(self,docs_golds)
spacy.language.Language.rehearse(self,docs,sgd=None,losses=None,config=None)
spacy.language.Language.remove_pipe(self,name)
spacy.language.Language.rename_pipe(self,old_name,new_name)
spacy.language.Language.replace_pipe(self,name,component)
spacy.language.Language.resume_training(self,sgd=None,**cfg)
spacy.language.Language.tagger(self)
spacy.language.Language.tensorizer(self)
spacy.language.Language.to_bytes(self,exclude=tuple(),disable=None,**kwargs)
spacy.language.Language.to_disk(self,path,exclude=tuple(),disable=None)
spacy.language.Language.update(self,docs,golds,drop=0.0,sgd=None,losses=None,component_cfg=None)
spacy.language.Language.use_params(self,params,**cfg)
spacy.language._fix_pretrained_vectors_name(nlp)
spacy.language._pipe(func,docs,kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/compat.py----------------------------------------
A:spacy.compat.izip->getattr(itertools, 'izip', zip)
A:spacy.compat.is_windows->sys.platform.startswith('win')
A:spacy.compat.is_linux->sys.platform.startswith('linux')
A:spacy.compat.loc->path2str(loc)
A:spacy.compat.spec->importlib.util.spec_from_file_location(name, str(loc))
A:spacy.compat.module->importlib.util.module_from_spec(spec)
A:spacy.compat.string->string.replace('\\\\U', '\\U').replace('\\\\U', '\\U')
spacy.compat.b_to_str(b_str)
spacy.compat.import_file(name,loc)
spacy.compat.is_config(python2=None,python3=None,windows=None,linux=None,osx=None)
spacy.compat.symlink_remove(link)
spacy.compat.symlink_to(orig,dest)
spacy.compat.unescape_unicode(string)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/errors.py----------------------------------------
A:spacy.errors.msg->"Invalid token patterns for matcher rule '{}'\n".format(key)
A:spacy.errors.pattern_errors->'\n'.join(['- {}'.format(e) for e in error_msgs])
A:spacy.errors.SPACY_WARNING_FILTER->os.environ.get('SPACY_WARNING_FILTER')
A:spacy.errors.SPACY_WARNING_TYPES->_get_warn_types(os.environ.get('SPACY_WARNING_TYPES'))
A:spacy.errors.SPACY_WARNING_IGNORE->_get_warn_excl(os.environ.get('SPACY_WARNING_IGNORE'))
spacy.Errors(object)
spacy.Warnings(object)
spacy.deprecation_warning(message)
spacy.errors.Errors(object)
spacy.errors.MatchPatternError(self,key,errors)
spacy.errors.MatchPatternError.__init__(self,key,errors)
spacy.errors.ModelsWarning(UserWarning)
spacy.errors.TempErrors(object)
spacy.errors.Warnings(object)
spacy.errors._get_warn_excl(arg)
spacy.errors._get_warn_types(arg)
spacy.errors._warn(message,warn_type='user')
spacy.errors.add_codes(err_cls)
spacy.errors.deprecation_warning(message)
spacy.errors.models_warning(message)
spacy.errors.user_warning(message)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lookups.py----------------------------------------
A:spacy.lookups.UNSET->object()
A:spacy.lookups.self._tables->OrderedDict()
A:spacy.lookups.table->Table(name=name, data=data)
A:spacy.lookups.self._tables[key]->Table(key)
A:spacy.lookups.path->ensure_path(path)
A:spacy.lookups.data->srsly.msgpack_loads(bytes_data).get('dict', {})
A:spacy.lookups.self->cls(name=name)
A:spacy.lookups.self.bloom->BloomFilter().from_bytes(loaded['bloom'])
A:spacy.lookups.key->get_string_id(key)
A:spacy.lookups.loaded->srsly.msgpack_loads(bytes_data)
spacy.lookups.Lookups(self)
spacy.lookups.Lookups.__contains__(self,name)
spacy.lookups.Lookups.__init__(self)
spacy.lookups.Lookups.__len__(self)
spacy.lookups.Lookups.add_table(self,name,data=SimpleFrozenDict())
spacy.lookups.Lookups.from_bytes(self,bytes_data,**kwargs)
spacy.lookups.Lookups.from_disk(self,path,**kwargs)
spacy.lookups.Lookups.get_table(self,name,default=UNSET)
spacy.lookups.Lookups.has_table(self,name)
spacy.lookups.Lookups.remove_table(self,name)
spacy.lookups.Lookups.tables(self)
spacy.lookups.Lookups.to_bytes(self,**kwargs)
spacy.lookups.Lookups.to_disk(self,path,**kwargs)
spacy.lookups.Table(self,name=None,data=None)
spacy.lookups.Table.__contains__(self,key)
spacy.lookups.Table.__getitem__(self,key)
spacy.lookups.Table.__init__(self,name=None,data=None)
spacy.lookups.Table.__setitem__(self,key,value)
spacy.lookups.Table.from_bytes(self,bytes_data)
spacy.lookups.Table.from_dict(cls,data,name=None)
spacy.lookups.Table.get(self,key,default=None)
spacy.lookups.Table.set(self,key,value)
spacy.lookups.Table.to_bytes(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/util.py----------------------------------------
A:spacy.util.entry_point->get_entry_point(ENTRY_POINTS.languages, lang)
A:spacy.util.module->importlib.import_module('.lang.%s' % lang, 'spacy')
A:spacy.util.LANGUAGES[lang]->getattr(module, module.__all__[0])
A:spacy.util._data_path->ensure_path(path)
A:spacy.util.path->ensure_path(path)
A:spacy.util.data_path->get_data_path()
A:spacy.util.cls->get_lang_class(lang)
A:spacy.util.meta->srsly.read_json(meta_path)
A:spacy.util.lang->srsly.read_json(meta_path).get('lang_factory', meta['lang'])
A:spacy.util.nlp->cls(meta=meta, **overrides)
A:spacy.util.pipeline->srsly.read_json(meta_path).get('pipeline', [])
A:spacy.util.disable->overrides.get('disable', [])
A:spacy.util.config->srsly.read_json(meta_path).get('pipeline_args', {}).get(name, {})
A:spacy.util.component->cls(meta=meta, **overrides).create_pipe(name, config=config)
A:spacy.util.model_path->ensure_path(path)
A:spacy.util.name->name.lower().lower()
A:spacy.util.packages->pkg_resources.working_set.by_key.keys()
A:spacy.util.pkg->importlib.import_module(name)
A:spacy.util.result[entry_point.name]->get_entry_point(ENTRY_POINTS.languages, lang).load()
A:spacy.util.array->compat.cupy.ndarray(numpy_array.shape, order='C', dtype=numpy_array.dtype)
A:spacy.util.value->type_convert(os.environ[name])
A:spacy.util.entries->file_.read().split('\n')
A:spacy.util.expression->'|'.join([piece for piece in entries if piece.strip()])
A:spacy.util.exc->expand_exc(exc, "'", 'â€™')
A:spacy.util.described_orth->''.join((attr[ORTH] for attr in token_attrs))
A:spacy.util.fixed->dict(token)
A:spacy.util.fixed[ORTH]->fixed[ORTH].replace(search, replace).replace(search, replace)
A:spacy.util.new_excs->dict(excs)
A:spacy.util.new_key->token_string.replace(search, replace)
A:spacy.util.start->min(length, max(0, start))
A:spacy.util.stop->min(length, max(start, stop))
A:spacy.util.size_->itertools.repeat(size)
A:spacy.util.items->iter(items)
A:spacy.util.batch_size->next(size_)
A:spacy.util.batch->list(itertools.islice(items, int(batch_size)))
A:spacy.util.curr->float(start)
A:spacy.util.(doc, gold)->next(items)
A:spacy.util.doc->next(items)
A:spacy.util.iterable->iter(iterable)
A:spacy.util.sorted_spans->sorted(spans, key=get_sort_key, reverse=True)
A:spacy.util.seen_tokens->set()
A:spacy.util.result->sorted(result, key=lambda span: span.start)
A:spacy.util.serialized->OrderedDict()
A:spacy.util.serialized[key]->getter()
A:spacy.util.msg->srsly.msgpack_loads(bytes_data)
A:spacy.util.text->text.replace('"', '&quot;').replace('"', '&quot;')
A:spacy.util.device->compat.cupy.cuda.device.Device(gpu_id)
A:spacy.util.Model.ops->CupyOps()
A:spacy.util.validator->get_json_validator(schema)
A:spacy.util.err_path->'[{}]'.format(' -> '.join([str(p) for p in err.path]))
A:spacy.util.exclude->list(exclude)
spacy.util.DummyTokenizer(object)
spacy.util.DummyTokenizer.from_bytes(self,_bytes_data,**kwargs)
spacy.util.DummyTokenizer.from_disk(self,_path,**kwargs)
spacy.util.DummyTokenizer.to_bytes(self,**kwargs)
spacy.util.DummyTokenizer.to_disk(self,_path,**kwargs)
spacy.util.ENTRY_POINTS(object)
spacy.util.SimpleFrozenDict(dict)
spacy.util.SimpleFrozenDict.__setitem__(self,key,value)
spacy.util.SimpleFrozenDict.pop(self,key,default=None)
spacy.util.SimpleFrozenDict.update(self,other)
spacy.util._get_attr_unless_lookup(default_func,lookups,string)
spacy.util.add_lookups(default_func,*lookups)
spacy.util.compile_infix_regex(entries)
spacy.util.compile_prefix_regex(entries)
spacy.util.compile_suffix_regex(entries)
spacy.util.compounding(start,stop,compound)
spacy.util.decaying(start,stop,decay)
spacy.util.ensure_path(path)
spacy.util.env_opt(name,default=None)
spacy.util.escape_html(text)
spacy.util.expand_exc(excs,search,replace)
spacy.util.filter_spans(spans)
spacy.util.fix_random_seed(seed=0)
spacy.util.from_bytes(bytes_data,setters,exclude)
spacy.util.from_disk(path,readers,exclude)
spacy.util.get_async(stream,numpy_array)
spacy.util.get_cuda_stream(require=False)
spacy.util.get_data_path(require_exists=True)
spacy.util.get_entry_point(key,value,default=None)
spacy.util.get_entry_points(key)
spacy.util.get_json_validator(schema)
spacy.util.get_lang_class(lang)
spacy.util.get_model_meta(path)
spacy.util.get_module_path(module)
spacy.util.get_package_path(name)
spacy.util.get_serialization_exclude(serializers,exclude,kwargs)
spacy.util.is_in_jupyter()
spacy.util.is_package(name)
spacy.util.itershuffle(iterable,bufsize=1000)
spacy.util.lang_class_is_loaded(lang)
spacy.util.load_language_data(path)
spacy.util.load_model(name,**overrides)
spacy.util.load_model_from_init_py(init_file,**overrides)
spacy.util.load_model_from_link(name,**overrides)
spacy.util.load_model_from_package(name,**overrides)
spacy.util.load_model_from_path(model_path,meta=False,**overrides)
spacy.util.minibatch(items,size=8)
spacy.util.minibatch_by_words(items,size,tuples=True,count_words=len)
spacy.util.minify_html(html)
spacy.util.normalize_slice(length,start,stop,step=None)
spacy.util.read_regex(path)
spacy.util.set_data_path(path)
spacy.util.set_env_log(value)
spacy.util.set_lang_class(name,cls)
spacy.util.stepping(start,stop,steps)
spacy.util.to_bytes(getters,exclude)
spacy.util.to_disk(path,writers,exclude)
spacy.util.update_exc(base_exceptions,*addition_dicts)
spacy.util.use_gpu(gpu_id)
spacy.util.validate_json(data,validator)
spacy.util.validate_schema(schema)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/pipeline/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/pipeline/entityruler.py----------------------------------------
A:spacy.pipeline.entityruler.self.overwrite->srsly.msgpack_loads(patterns_bytes).get('overwrite', False)
A:spacy.pipeline.entityruler.self.token_patterns->defaultdict(list)
A:spacy.pipeline.entityruler.self.phrase_patterns->defaultdict(list)
A:spacy.pipeline.entityruler.self.matcher->Matcher(nlp.vocab, validate=validate)
A:spacy.pipeline.entityruler.self.phrase_matcher->PhraseMatcher(self.nlp.vocab, attr=self.phrase_matcher_attr)
A:spacy.pipeline.entityruler.self.ent_id_sep->srsly.msgpack_loads(patterns_bytes).get('ent_id_sep', DEFAULT_ENT_ID_SEP)
A:spacy.pipeline.entityruler.patterns->srsly.read_jsonl(depr_patterns_path)
A:spacy.pipeline.entityruler.n_token_patterns->sum((len(p) for p in self.token_patterns.values()))
A:spacy.pipeline.entityruler.n_phrase_patterns->sum((len(p) for p in self.phrase_patterns.values()))
A:spacy.pipeline.entityruler.matches->sorted(matches, key=get_sort_key, reverse=True)
A:spacy.pipeline.entityruler.entities->list(doc.ents)
A:spacy.pipeline.entityruler.seen_tokens->set()
A:spacy.pipeline.entityruler.(ent_label, ent_id)->'{}{}{}'.format(label, self.ent_id_sep, ent_id).rsplit(self.ent_id_sep, 1)
A:spacy.pipeline.entityruler.span->Span(doc, start, end, label=match_id)
A:spacy.pipeline.entityruler.all_labels->set(self.token_patterns.keys())
A:spacy.pipeline.entityruler.all_ent_ids->set()
A:spacy.pipeline.entityruler.(_, ent_id)->self._split_label(l)
A:spacy.pipeline.entityruler.current_index->self.nlp.pipe_names.index(self.name)
A:spacy.pipeline.entityruler.label->'{}{}{}'.format(label, self.ent_id_sep, ent_id)
A:spacy.pipeline.entityruler.cfg->srsly.msgpack_loads(patterns_bytes)
A:spacy.pipeline.entityruler.self.phrase_matcher_attr->srsly.msgpack_loads(patterns_bytes).get('phrase_matcher_attr')
A:spacy.pipeline.entityruler.serial->OrderedDict((('overwrite', self.overwrite), ('ent_id_sep', self.ent_id_sep), ('phrase_matcher_attr', self.phrase_matcher_attr), ('patterns', self.patterns)))
A:spacy.pipeline.entityruler.path->ensure_path(path)
A:spacy.pipeline.entityruler.depr_patterns_path->ensure_path(path).with_suffix('.jsonl')
spacy.pipeline.EntityRuler(self,nlp,phrase_matcher_attr=None,validate=False,**cfg)
spacy.pipeline.EntityRuler.__contains__(self,label)
spacy.pipeline.EntityRuler.__len__(self)
spacy.pipeline.EntityRuler._create_label(self,label,ent_id)
spacy.pipeline.EntityRuler._split_label(self,label)
spacy.pipeline.EntityRuler.add_patterns(self,patterns)
spacy.pipeline.EntityRuler.ent_ids(self)
spacy.pipeline.EntityRuler.from_bytes(self,patterns_bytes,**kwargs)
spacy.pipeline.EntityRuler.from_disk(self,path,**kwargs)
spacy.pipeline.EntityRuler.labels(self)
spacy.pipeline.EntityRuler.patterns(self)
spacy.pipeline.EntityRuler.to_bytes(self,**kwargs)
spacy.pipeline.EntityRuler.to_disk(self,path,**kwargs)
spacy.pipeline.entityruler.EntityRuler(self,nlp,phrase_matcher_attr=None,validate=False,**cfg)
spacy.pipeline.entityruler.EntityRuler.__contains__(self,label)
spacy.pipeline.entityruler.EntityRuler.__init__(self,nlp,phrase_matcher_attr=None,validate=False,**cfg)
spacy.pipeline.entityruler.EntityRuler.__len__(self)
spacy.pipeline.entityruler.EntityRuler._create_label(self,label,ent_id)
spacy.pipeline.entityruler.EntityRuler._split_label(self,label)
spacy.pipeline.entityruler.EntityRuler.add_patterns(self,patterns)
spacy.pipeline.entityruler.EntityRuler.ent_ids(self)
spacy.pipeline.entityruler.EntityRuler.from_bytes(self,patterns_bytes,**kwargs)
spacy.pipeline.entityruler.EntityRuler.from_disk(self,path,**kwargs)
spacy.pipeline.entityruler.EntityRuler.labels(self)
spacy.pipeline.entityruler.EntityRuler.patterns(self)
spacy.pipeline.entityruler.EntityRuler.to_bytes(self,**kwargs)
spacy.pipeline.entityruler.EntityRuler.to_disk(self,path,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/pipeline/functions.py----------------------------------------
A:spacy.pipeline.functions.merger->Matcher(doc.vocab)
A:spacy.pipeline.functions.matches->merger(doc)
spacy.pipeline.functions.merge_entities(doc)
spacy.pipeline.functions.merge_noun_chunks(doc)
spacy.pipeline.functions.merge_subtokens(doc,label='subtok')
spacy.pipeline.merge_entities(doc)
spacy.pipeline.merge_noun_chunks(doc)
spacy.pipeline.merge_subtokens(doc,label='subtok')


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/pipeline/hooks.py----------------------------------------
A:spacy.pipeline.hooks.self.cfg->dict(cfg)
A:spacy.pipeline.hooks.(sims, bp_sims)->self.model.begin_update(doc1_doc2, drop=drop)
A:spacy.pipeline.hooks.self.model->self.Model(pipeline[0].model.nO)
A:spacy.pipeline.hooks.sgd->self.create_optimizer()
spacy.pipeline.SentenceSegmenter(self,vocab,strategy=None)
spacy.pipeline.SentenceSegmenter.split_on_punct(doc)
spacy.pipeline.SimilarityHook(self,vocab,model=True,**cfg)
spacy.pipeline.SimilarityHook.Model(cls,length)
spacy.pipeline.SimilarityHook.begin_training(self,_=tuple(),pipeline=None,sgd=None,**kwargs)
spacy.pipeline.SimilarityHook.pipe(self,docs,**kwargs)
spacy.pipeline.SimilarityHook.predict(self,doc1,doc2)
spacy.pipeline.SimilarityHook.update(self,doc1_doc2,golds,sgd=None,drop=0.0)
spacy.pipeline.hooks.SentenceSegmenter(self,vocab,strategy=None)
spacy.pipeline.hooks.SentenceSegmenter.__init__(self,vocab,strategy=None)
spacy.pipeline.hooks.SentenceSegmenter.split_on_punct(doc)
spacy.pipeline.hooks.SimilarityHook(self,vocab,model=True,**cfg)
spacy.pipeline.hooks.SimilarityHook.Model(cls,length)
spacy.pipeline.hooks.SimilarityHook.__init__(self,vocab,model=True,**cfg)
spacy.pipeline.hooks.SimilarityHook.begin_training(self,_=tuple(),pipeline=None,sgd=None,**kwargs)
spacy.pipeline.hooks.SimilarityHook.pipe(self,docs,**kwargs)
spacy.pipeline.hooks.SimilarityHook.predict(self,doc1,doc2)
spacy.pipeline.hooks.SimilarityHook.update(self,doc1_doc2,golds,sgd=None,drop=0.0)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/test_lemmatizer.py----------------------------------------
A:spacy.tests.test_lemmatizer.nlp->Language()
A:spacy.tests.test_lemmatizer.table->Language().vocab.lookups.add_table('lemma_lookup')
A:spacy.tests.test_lemmatizer.new_nlp->Language()
A:spacy.tests.test_lemmatizer.nlp_bytes->Language().to_bytes()
A:spacy.tests.test_lemmatizer.nlp.vocab.lookups->Lookups()
A:spacy.tests.test_lemmatizer.tagger->Language().create_pipe('tagger')
spacy.tests.test_lemmatizer.test_lemmatizer_reflects_lookups_changes()
spacy.tests.test_lemmatizer.test_tagger_warns_no_lemma_lookups()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/test_scorer.py----------------------------------------
A:spacy.tests.test_scorer.scorer->Scorer()
A:spacy.tests.test_scorer.doc->get_doc(en_vocab, words=input_.split(' '), ents=[[0, 1, 'ORG'], [5, 6, 'GPE'], [6, 7, 'ORG']])
A:spacy.tests.test_scorer.gold->GoldParse(doc, entities=annot['entities'])
A:spacy.tests.test_scorer.(tpr, fpr, _)->_roc_curve(y_true, y_score)
A:spacy.tests.test_scorer.roc_auc->_roc_auc_score(y_true, y_score)
A:spacy.tests.test_scorer.score->ROCAUCScore()
spacy.tests.test_scorer.test_ner_per_type(en_vocab)
spacy.tests.test_scorer.test_roc_auc_score()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/test_misc.py----------------------------------------
A:spacy.tests.test_misc.path->spacy.util.get_package_path(package)
A:spacy.tests.test_misc.model->PrecomputableAffine(nO=nO, nI=nI, nF=nF, nP=nP)
A:spacy.tests.test_misc.tensor->PrecomputableAffine(nO=nO, nI=nI, nF=nF, nP=nP).ops.allocate((10, nI))
A:spacy.tests.test_misc.(Y, get_dX)->PrecomputableAffine(nO=nO, nI=nI, nF=nF, nP=nP).begin_update(tensor)
A:spacy.tests.test_misc.dY->PrecomputableAffine(nO=nO, nI=nI, nF=nF, nP=nP).ops.allocate((15, nO, nP))
A:spacy.tests.test_misc.ids->PrecomputableAffine(nO=nO, nI=nI, nF=nF, nP=nP).ops.allocate((15, nF))
spacy.tests.test_misc.is_admin()
spacy.tests.test_misc.symlink()
spacy.tests.test_misc.symlink_setup_target(request,symlink_target,symlink)
spacy.tests.test_misc.symlink_target()
spacy.tests.test_misc.test_PrecomputableAffine(nO=4,nI=5,nF=3,nP=2)
spacy.tests.test_misc.test_ascii_filenames()
spacy.tests.test_misc.test_create_symlink_windows(symlink_setup_target,symlink_target,symlink,is_admin)
spacy.tests.test_misc.test_prefer_gpu()
spacy.tests.test_misc.test_require_gpu()
spacy.tests.test_misc.test_util_ensure_path_succeeds(text)
spacy.tests.test_misc.test_util_get_package_path(package)
spacy.tests.test_misc.test_util_is_package(package)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/test_language.py----------------------------------------
A:spacy.tests.test_language.nlp->Language(Vocab())
A:spacy.tests.test_language.textcat->Language(Vocab()).create_pipe('textcat')
A:spacy.tests.test_language.doc->Doc(nlp.vocab, words=text.split(' '))
A:spacy.tests.test_language.gold->GoldParse(doc, **annots)
spacy.tests.test_language.nlp()
spacy.tests.test_language.test_language_evaluate(nlp)
spacy.tests.test_language.test_language_update(nlp)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/test_gold.py----------------------------------------
A:spacy.tests.test_gold.doc->nlp(text)
A:spacy.tests.test_gold.tags->biluo_tags_from_offsets(doc, entities)
A:spacy.tests.test_gold.biluo_tags_converted->biluo_tags_from_offsets(doc, offsets)
A:spacy.tests.test_gold.offsets_converted->offsets_from_biluo_tags(doc, biluo_tags)
A:spacy.tests.test_gold.spans->spans_from_biluo_tags(doc, biluo_tags)
A:spacy.tests.test_gold.gold->GoldParse(doc, entities=biluo_tags)
A:spacy.tests.test_gold.nlp->English()
A:spacy.tests.test_gold.goldcorpus->GoldCorpus(str(json_file), str(json_file))
A:spacy.tests.test_gold.(reloaded_doc, goldparse)->next(goldcorpus.train_docs(nlp))
spacy.tests.test_gold.test_biluo_spans(en_tokenizer)
spacy.tests.test_gold.test_gold_biluo_BIL(en_vocab)
spacy.tests.test_gold.test_gold_biluo_BL(en_vocab)
spacy.tests.test_gold.test_gold_biluo_U(en_vocab)
spacy.tests.test_gold.test_gold_biluo_misalign(en_vocab)
spacy.tests.test_gold.test_gold_biluo_overlap(en_vocab)
spacy.tests.test_gold.test_gold_ner_missing_tags(en_tokenizer)
spacy.tests.test_gold.test_roundtrip_docs_to_json()
spacy.tests.test_gold.test_roundtrip_offsets_biluo_conversion(en_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/test_align.py----------------------------------------
A:spacy.tests.test_align.(output_cost, i2j, j2i, matrix)->align(string1, string2)
A:spacy.tests.test_align.(output_cost, output_i2j, j2i, matrix)->align(string1, string2)
A:spacy.tests.test_align.(output_cost, output_i2j, output_j2i, matrix)->align(string1, string2)
A:spacy.tests.test_align.(cost, i2j, j2i, matrix)->align(words1, words2)
A:spacy.tests.test_align.(i2j_multi, j2i_multi)->multi_align(i2j, j2i, lengths1, lengths2)
spacy.tests.test_align.test_align_costs(string1,string2,cost)
spacy.tests.test_align.test_align_i2j(string1,string2,i2j)
spacy.tests.test_align.test_align_i2j_2(string1,string2,j2i)
spacy.tests.test_align.test_align_many_to_one()
spacy.tests.test_align.test_align_strings()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/test_cli.py----------------------------------------
A:spacy.tests.test_cli.input_data->'\n'.join(lines)
A:spacy.tests.test_cli.converted->conll_ner2json(input_data, n_sents=10)
A:spacy.tests.test_cli.nlp->English()
A:spacy.tests.test_cli.(docs, skip_count)->make_docs(nlp, [too_long_jsonl], 1, 5)
spacy.tests.test_cli.test_cli_converters_conll_ner2json()
spacy.tests.test_cli.test_cli_converters_conllu2json()
spacy.tests.test_cli.test_cli_converters_iob2json()
spacy.tests.test_cli.test_pretrain_make_docs()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/test_json_schemas.py----------------------------------------
A:spacy.tests.test_json_schemas.errors->validate_json([data], training_schema_validator)
spacy.tests.test_json_schemas.test_json_schema_training_invalid(data,n_errors,training_schema_validator)
spacy.tests.test_json_schemas.test_json_schema_training_valid(data,training_schema_validator)
spacy.tests.test_json_schemas.test_schemas(schema)
spacy.tests.test_json_schemas.test_validate_schema()
spacy.tests.test_json_schemas.training_schema_validator()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/test_displacy.py----------------------------------------
A:spacy.tests.test_displacy.doc->get_doc(en_vocab, words=['But', 'Google', 'is', 'starting', 'from', 'behind'])
A:spacy.tests.test_displacy.ents->spacy.displacy.parse_ents(doc)
A:spacy.tests.test_displacy.deps->spacy.displacy.parse_deps(doc)
A:spacy.tests.test_displacy.renderer->DependencyRenderer()
A:spacy.tests.test_displacy.html->spacy.displacy.render(doc, style='ent')
A:spacy.tests.test_displacy.nlp->Persian()
spacy.tests.test_displacy.test_displacy_invalid_arcs()
spacy.tests.test_displacy.test_displacy_parse_deps(en_vocab)
spacy.tests.test_displacy.test_displacy_parse_ents(en_vocab)
spacy.tests.test_displacy.test_displacy_raises_for_wrong_type(en_vocab)
spacy.tests.test_displacy.test_displacy_render_wrapper(en_vocab)
spacy.tests.test_displacy.test_displacy_rtl()
spacy.tests.test_displacy.test_displacy_spans(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/util.py----------------------------------------
A:spacy.tests.util.f->tempfile.TemporaryFile(mode=mode)
A:spacy.tests.util.d->Path(tempfile.mkdtemp())
A:spacy.tests.util.doc->Doc(vocab, words=words)
A:spacy.tests.util.attrs->Doc(vocab, words=words).to_array([POS, HEAD, DEP])
A:spacy.tests.util.(move, label)->action_name.split('-')
A:spacy.tests.util.length->len(vectors[0][1])
A:spacy.tests.util.msg1->srsly.msgpack_loads(b1)
A:spacy.tests.util.msg2->srsly.msgpack_loads(b2)
spacy.tests.util.add_vecs_to_vocab(vocab,vectors)
spacy.tests.util.apply_transition_sequence(parser,doc,sequence)
spacy.tests.util.assert_docs_equal(doc1,doc2)
spacy.tests.util.assert_packed_msg_equal(b1,b2)
spacy.tests.util.get_cosine(vec1,vec2)
spacy.tests.util.get_doc(vocab,words=[],pos=None,heads=None,deps=None,tags=None,ents=None)
spacy.tests.util.make_tempdir()
spacy.tests.util.make_tempfile(mode='r')


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/conftest.py----------------------------------------
A:spacy.tests.conftest.nlp->get_lang_class('en')(en_vocab)
spacy.tests.conftest.ar_tokenizer()
spacy.tests.conftest.bn_tokenizer()
spacy.tests.conftest.ca_tokenizer()
spacy.tests.conftest.da_tokenizer()
spacy.tests.conftest.de_tokenizer()
spacy.tests.conftest.el_tokenizer()
spacy.tests.conftest.en_parser(en_vocab)
spacy.tests.conftest.en_tokenizer()
spacy.tests.conftest.en_vocab()
spacy.tests.conftest.es_tokenizer()
spacy.tests.conftest.fi_tokenizer()
spacy.tests.conftest.fr_tokenizer()
spacy.tests.conftest.ga_tokenizer()
spacy.tests.conftest.he_tokenizer()
spacy.tests.conftest.hr_tokenizer()
spacy.tests.conftest.hu_tokenizer()
spacy.tests.conftest.id_tokenizer()
spacy.tests.conftest.it_tokenizer()
spacy.tests.conftest.ja_tokenizer()
spacy.tests.conftest.ko_tokenizer()
spacy.tests.conftest.lt_tokenizer()
spacy.tests.conftest.nb_tokenizer()
spacy.tests.conftest.nl_tokenizer()
spacy.tests.conftest.pl_tokenizer()
spacy.tests.conftest.pt_tokenizer()
spacy.tests.conftest.pytest_addoption(parser)
spacy.tests.conftest.pytest_runtest_setup(item)
spacy.tests.conftest.ro_tokenizer()
spacy.tests.conftest.ru_lemmatizer()
spacy.tests.conftest.ru_tokenizer()
spacy.tests.conftest.sr_tokenizer()
spacy.tests.conftest.sv_tokenizer()
spacy.tests.conftest.th_tokenizer()
spacy.tests.conftest.tokenizer()
spacy.tests.conftest.tr_tokenizer()
spacy.tests.conftest.tt_tokenizer()
spacy.tests.conftest.uk_tokenizer()
spacy.tests.conftest.ur_tokenizer()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/test_pickles.py----------------------------------------
A:spacy.tests.test_pickles.stringstore->StringStore()
A:spacy.tests.test_pickles.data->srsly.pickle_dumps(vocab)
A:spacy.tests.test_pickles.unpickled->srsly.pickle_loads(data)
A:spacy.tests.test_pickles.vocab->Vocab(lex_attr_getters={int(NORM): lambda string: string[:-1]})
spacy.tests.test_pickles.test_pickle_string_store(text1,text2)
spacy.tests.test_pickles.test_pickle_vocab(text1,text2)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/parser/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/parser/test_ner.py----------------------------------------
A:spacy.tests.parser.test_ner.actions->spacy.syntax.ner.BiluoPushDown.get_actions(entity_types=entity_types)
A:spacy.tests.parser.test_ner.gold->GoldParse(doc, words=words, entities=biluo_tags)
A:spacy.tests.parser.test_ner.act_classes->tsys.get_oracle_sequence(doc, gold)
A:spacy.tests.parser.test_ner.doc->nlp('This is Antti L Korhonen speaking in Finland')
A:spacy.tests.parser.test_ner.moves->BiluoPushDown(en_vocab.strings)
A:spacy.tests.parser.test_ner.(action, label)->tag.split('-')
A:spacy.tests.parser.test_ner.seq->BiluoPushDown(en_vocab.strings).get_oracle_sequence(doc, gold)
A:spacy.tests.parser.test_ner.nlp1->English()
A:spacy.tests.parser.test_ner.doc1->nlp1('I live in New York')
A:spacy.tests.parser.test_ner.ner1->English().create_pipe('ner')
A:spacy.tests.parser.test_ner.nlp2->English()
A:spacy.tests.parser.test_ner.doc2->nlp2('I live in New York')
A:spacy.tests.parser.test_ner.ner2->EntityRecognizer(doc.vocab)
A:spacy.tests.parser.test_ner.nlp->English()
A:spacy.tests.parser.test_ner.ruler->EntityRuler(nlp)
A:spacy.tests.parser.test_ner.untrained_ner->English().create_pipe('ner')
spacy.tests.parser.test_ner.BlockerComponent1(self,start,end)
spacy.tests.parser.test_ner.BlockerComponent1.__init__(self,start,end)
spacy.tests.parser.test_ner.doc(vocab)
spacy.tests.parser.test_ner.entity_annots(doc)
spacy.tests.parser.test_ner.entity_types(entity_annots)
spacy.tests.parser.test_ner.test_accept_blocked_token()
spacy.tests.parser.test_ner.test_block_ner()
spacy.tests.parser.test_ner.test_get_oracle_moves(tsys,doc,entity_annots)
spacy.tests.parser.test_ner.test_get_oracle_moves_negative_O(tsys,vocab)
spacy.tests.parser.test_ner.test_get_oracle_moves_negative_entities(tsys,doc,entity_annots)
spacy.tests.parser.test_ner.test_get_oracle_moves_negative_entities2(tsys,vocab)
spacy.tests.parser.test_ner.test_ner_before_ruler()
spacy.tests.parser.test_ner.test_oracle_moves_missing_B(en_vocab)
spacy.tests.parser.test_ner.test_oracle_moves_whitespace(en_vocab)
spacy.tests.parser.test_ner.test_overwrite_token()
spacy.tests.parser.test_ner.test_ruler_before_ner()
spacy.tests.parser.test_ner.tsys(vocab,entity_types)
spacy.tests.parser.test_ner.vocab()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/parser/test_arc_eager_oracle.py----------------------------------------
A:spacy.tests.parser.test_arc_eager_oracle.doc->Doc(Vocab(), words=[t[1] for t in annot_tuples])
A:spacy.tests.parser.test_arc_eager_oracle.gold->GoldParse(doc, words=words, tags=tags, heads=heads, deps=deps)
A:spacy.tests.parser.test_arc_eager_oracle.state->StateClass(doc)
A:spacy.tests.parser.test_arc_eager_oracle.name->M.class_name(i)
A:spacy.tests.parser.test_arc_eager_oracle.state_costs[name]->M.get_cost(state, gold, i)
A:spacy.tests.parser.test_arc_eager_oracle.moves->ArcEager(vocab.strings, ArcEager.get_actions())
A:spacy.tests.parser.test_arc_eager_oracle.vocab->Vocab()
A:spacy.tests.parser.test_arc_eager_oracle.(state, cost_history)->get_sequence_costs(arc_eager, words, heads, deps, actions)
A:spacy.tests.parser.test_arc_eager_oracle.parser->DependencyParser(doc.vocab)
A:spacy.tests.parser.test_arc_eager_oracle.(ids, words, tags, heads, deps, ents)->zip(*annot_tuples)
A:spacy.tests.parser.test_arc_eager_oracle.(heads, deps)->projectivize(heads, deps)
spacy.tests.parser.test_arc_eager_oracle.arc_eager(vocab)
spacy.tests.parser.test_arc_eager_oracle.doc(words,vocab)
spacy.tests.parser.test_arc_eager_oracle.get_sequence_costs(M,words,heads,deps,transitions)
spacy.tests.parser.test_arc_eager_oracle.gold(doc,words)
spacy.tests.parser.test_arc_eager_oracle.test_get_oracle_actions()
spacy.tests.parser.test_arc_eager_oracle.test_oracle_four_words(arc_eager,vocab)
spacy.tests.parser.test_arc_eager_oracle.vocab()
spacy.tests.parser.test_arc_eager_oracle.words()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/parser/test_parse.py----------------------------------------
A:spacy.tests.parser.test_parse.tokens->en_tokenizer(text)
A:spacy.tests.parser.test_parse.doc->get_doc(tokens.vocab, words=[t.text for t in tokens], deps=deps, heads=heads, tags=tags)
spacy.tests.parser.test_parse.test_parser_arc_eager_finalize_state(en_tokenizer,en_parser)
spacy.tests.parser.test_parse.test_parser_initial(en_tokenizer,en_parser)
spacy.tests.parser.test_parse.test_parser_merge_pp(en_tokenizer)
spacy.tests.parser.test_parse.test_parser_parse_one_word_sentence(en_tokenizer,en_parser,text)
spacy.tests.parser.test_parse.test_parser_parse_subtrees(en_tokenizer,en_parser)
spacy.tests.parser.test_parse.test_parser_root(en_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/parser/test_nn_beam.py----------------------------------------
A:spacy.tests.parser.test_nn_beam.aeager->ArcEager(vocab.strings, {})
A:spacy.tests.parser.test_nn_beam.vec->numpy.random.uniform(-0.1, 0.1, (len(doc), vector_size))
A:spacy.tests.parser.test_nn_beam.nlp->Language()
A:spacy.tests.parser.test_nn_beam.doc->Language().make_doc('Australia is a country')
spacy.tests.parser.test_nn_beam.batch_size(docs)
spacy.tests.parser.test_nn_beam.beam(moves,states,golds,beam_width)
spacy.tests.parser.test_nn_beam.beam_width()
spacy.tests.parser.test_nn_beam.docs(vocab)
spacy.tests.parser.test_nn_beam.golds(docs)
spacy.tests.parser.test_nn_beam.moves(vocab)
spacy.tests.parser.test_nn_beam.scores(moves,batch_size,beam_width)
spacy.tests.parser.test_nn_beam.states(docs)
spacy.tests.parser.test_nn_beam.test_beam_advance(beam,scores)
spacy.tests.parser.test_nn_beam.test_beam_advance_too_few_scores(beam,scores)
spacy.tests.parser.test_nn_beam.test_beam_parse()
spacy.tests.parser.test_nn_beam.test_create_beam(beam)
spacy.tests.parser.test_nn_beam.tokvecs(docs,vector_size)
spacy.tests.parser.test_nn_beam.vector_size()
spacy.tests.parser.test_nn_beam.vocab()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/parser/test_neural_parser.py----------------------------------------
A:spacy.tests.parser.test_neural_parser.actions->spacy.syntax.arc_eager.ArcEager.get_actions(left_labels=['L'], right_labels=['R'])
spacy.tests.parser.test_neural_parser.arc_eager(vocab)
spacy.tests.parser.test_neural_parser.doc(vocab)
spacy.tests.parser.test_neural_parser.gold(doc)
spacy.tests.parser.test_neural_parser.model(arc_eager,tok2vec)
spacy.tests.parser.test_neural_parser.parser(vocab,arc_eager)
spacy.tests.parser.test_neural_parser.test_build_model(parser)
spacy.tests.parser.test_neural_parser.test_can_init_nn_parser(parser)
spacy.tests.parser.test_neural_parser.test_predict_doc(parser,tok2vec,model,doc)
spacy.tests.parser.test_neural_parser.test_predict_doc_beam(parser,model,doc)
spacy.tests.parser.test_neural_parser.test_update_doc(parser,model,doc,gold)
spacy.tests.parser.test_neural_parser.test_update_doc_beam(parser,model,doc,gold)
spacy.tests.parser.test_neural_parser.tok2vec()
spacy.tests.parser.test_neural_parser.vocab()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/parser/test_space_attachment.py----------------------------------------
A:spacy.tests.parser.test_space_attachment.tokens->en_tokenizer(text)
A:spacy.tests.parser.test_space_attachment.doc->Doc(en_parser.vocab, words=text)
spacy.tests.parser.test_space_attachment.test_parser_sentence_space(en_tokenizer)
spacy.tests.parser.test_space_attachment.test_parser_space_attachment(en_tokenizer)
spacy.tests.parser.test_space_attachment.test_parser_space_attachment_intermediate_trailing(en_tokenizer,en_parser)
spacy.tests.parser.test_space_attachment.test_parser_space_attachment_leading(en_tokenizer,en_parser)
spacy.tests.parser.test_space_attachment.test_parser_space_attachment_space(en_tokenizer,en_parser,text,length)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/parser/test_preset_sbd.py----------------------------------------
A:spacy.tests.parser.test_preset_sbd.parser->DependencyParser(vocab)
A:spacy.tests.parser.test_preset_sbd.sgd->Adam(NumpyOps(), 0.001)
A:spacy.tests.parser.test_preset_sbd.doc->parser(doc)
A:spacy.tests.parser.test_preset_sbd.gold->GoldParse(doc, heads=[1, 1, 3, 3], deps=['left', 'ROOT', 'left', 'ROOT'])
spacy.tests.parser.test_preset_sbd.parser(vocab)
spacy.tests.parser.test_preset_sbd.test_no_sentences(parser)
spacy.tests.parser.test_preset_sbd.test_sents_1(parser)
spacy.tests.parser.test_preset_sbd.test_sents_1_2(parser)
spacy.tests.parser.test_preset_sbd.test_sents_1_3(parser)
spacy.tests.parser.test_preset_sbd.vocab()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/parser/test_parse_navigate.py----------------------------------------
A:spacy.tests.parser.test_parse_navigate.tokens->en_tokenizer(text)
A:spacy.tests.parser.test_parse_navigate.doc->get_doc(tokens.vocab, words=[t.text for t in tokens], heads=heads)
A:spacy.tests.parser.test_parse_navigate.lefts[head.i]->set()
A:spacy.tests.parser.test_parse_navigate.rights[head.i]->set()
A:spacy.tests.parser.test_parse_navigate.subtree->list(token.subtree)
A:spacy.tests.parser.test_parse_navigate.debug->'\t'.join((token.text, token.right_edge.text, subtree[-1].text, token.right_edge.head.text))
spacy.tests.parser.test_parse_navigate.heads()
spacy.tests.parser.test_parse_navigate.test_parser_parse_navigate_child_consistency(en_tokenizer,text,heads)
spacy.tests.parser.test_parse_navigate.test_parser_parse_navigate_consistency(en_tokenizer,text,heads)
spacy.tests.parser.test_parse_navigate.test_parser_parse_navigate_edges(en_tokenizer,text,heads)
spacy.tests.parser.test_parse_navigate.text()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/parser/test_add_label.py----------------------------------------
A:spacy.tests.parser.test_add_label.parser->_train_parser(parser)
A:spacy.tests.parser.test_add_label.sgd->Adam(NumpyOps(), 0.001)
A:spacy.tests.parser.test_add_label.doc->parser(doc)
A:spacy.tests.parser.test_add_label.gold->GoldParse(doc, heads=[1, 1, 3, 3], deps=['right', 'ROOT', 'left', 'ROOT'])
A:spacy.tests.parser.test_add_label.ner1->EntityRecognizer(Vocab())
A:spacy.tests.parser.test_add_label.ner2->EntityRecognizer(Vocab()).from_bytes(ner1.to_bytes())
A:spacy.tests.parser.test_add_label.pipe->pipe_cls(Vocab())
A:spacy.tests.parser.test_add_label.pipe_labels->sorted(list(pipe.labels))
spacy.tests.parser.test_add_label._train_parser(parser)
spacy.tests.parser.test_add_label.parser(vocab)
spacy.tests.parser.test_add_label.test_add_label(parser)
spacy.tests.parser.test_add_label.test_add_label_deserializes_correctly()
spacy.tests.parser.test_add_label.test_add_label_get_label(pipe_cls,n_moves)
spacy.tests.parser.test_add_label.test_init_parser(parser)
spacy.tests.parser.test_add_label.vocab()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/parser/test_nonproj.py----------------------------------------
A:spacy.tests.parser.test_nonproj.tokens->en_tokenizer('whatever ' * len(proj_heads))
A:spacy.tests.parser.test_nonproj.doc->get_doc(tokens.vocab, words=[t.text for t in tokens], deps=deco_labels, heads=rel_proj_heads)
A:spacy.tests.parser.test_nonproj.(proj_heads, deco_labels)->spacy.syntax.nonproj.projectivize(nonproj_tree2, labels2)
A:spacy.tests.parser.test_nonproj.(deproj_heads, undeco_labels)->deprojectivize(proj_heads, deco_labels)
spacy.tests.parser.test_nonproj.cyclic_tree()
spacy.tests.parser.test_nonproj.multirooted_tree()
spacy.tests.parser.test_nonproj.nonproj_tree()
spacy.tests.parser.test_nonproj.partial_tree()
spacy.tests.parser.test_nonproj.proj_tree()
spacy.tests.parser.test_nonproj.test_parser_ancestors(tree,cyclic_tree,partial_tree,multirooted_tree)
spacy.tests.parser.test_nonproj.test_parser_contains_cycle(tree,cyclic_tree,partial_tree,multirooted_tree)
spacy.tests.parser.test_nonproj.test_parser_is_nonproj_arc(nonproj_tree,partial_tree,multirooted_tree)
spacy.tests.parser.test_nonproj.test_parser_is_nonproj_tree(proj_tree,nonproj_tree,partial_tree,multirooted_tree)
spacy.tests.parser.test_nonproj.test_parser_pseudoprojectivity(en_tokenizer)
spacy.tests.parser.test_nonproj.tree()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/pipeline/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/pipeline/test_entity_ruler.py----------------------------------------
A:spacy.tests.pipeline.test_entity_ruler.ruler->EntityRuler(nlp)
A:spacy.tests.pipeline.test_entity_ruler.doc->nlp('Apple is a technology company')
A:spacy.tests.pipeline.test_entity_ruler.ruler_bytes->EntityRuler(nlp).to_bytes()
A:spacy.tests.pipeline.test_entity_ruler.new_ruler->new_ruler.from_bytes(ruler_bytes).from_bytes(ruler_bytes)
A:spacy.tests.pipeline.test_entity_ruler.validated_ruler->EntityRuler(nlp, validate=True)
spacy.tests.pipeline.test_entity_ruler.add_ent()
spacy.tests.pipeline.test_entity_ruler.nlp()
spacy.tests.pipeline.test_entity_ruler.patterns()
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_cfg_ent_id_sep(nlp,patterns)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_entity_id(nlp,patterns)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_existing(nlp,patterns,add_ent)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_existing_complex(nlp,patterns,add_ent)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_existing_overwrite(nlp,patterns,add_ent)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_init(nlp,patterns)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_serialize_bytes(nlp,patterns)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_serialize_phrase_matcher_attr_bytes(nlp,patterns)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_validate(nlp)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/pipeline/test_pipe_methods.py----------------------------------------
A:spacy.tests.pipeline.test_pipe_methods.(removed_name, removed_component)->nlp.remove_pipe(name)
A:spacy.tests.pipeline.test_pipe_methods.disabled->nlp.disable_pipes(name)
A:spacy.tests.pipeline.test_pipe_methods.pipe->nlp.create_pipe(name)
spacy.tests.pipeline.test_pipe_methods.new_pipe(doc)
spacy.tests.pipeline.test_pipe_methods.nlp()
spacy.tests.pipeline.test_pipe_methods.test_add_lots_of_pipes(nlp,n_pipes)
spacy.tests.pipeline.test_pipe_methods.test_add_pipe_duplicate_name(nlp)
spacy.tests.pipeline.test_pipe_methods.test_add_pipe_first(nlp,name)
spacy.tests.pipeline.test_pipe_methods.test_add_pipe_last(nlp,name1,name2)
spacy.tests.pipeline.test_pipe_methods.test_add_pipe_no_name(nlp)
spacy.tests.pipeline.test_pipe_methods.test_cant_add_pipe_first_and_last(nlp)
spacy.tests.pipeline.test_pipe_methods.test_disable_pipes_context(nlp,name)
spacy.tests.pipeline.test_pipe_methods.test_disable_pipes_method(nlp,name)
spacy.tests.pipeline.test_pipe_methods.test_get_pipe(nlp,name)
spacy.tests.pipeline.test_pipe_methods.test_pipe_base_class_add_label(nlp,component)
spacy.tests.pipeline.test_pipe_methods.test_pipe_labels(nlp)
spacy.tests.pipeline.test_pipe_methods.test_raise_for_invalid_components(nlp,component)
spacy.tests.pipeline.test_pipe_methods.test_remove_pipe(nlp,name)
spacy.tests.pipeline.test_pipe_methods.test_rename_pipe(nlp,old_name,new_name)
spacy.tests.pipeline.test_pipe_methods.test_replace_pipe(nlp,name,replacement,not_callable)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/pipeline/test_sentencizer.py----------------------------------------
A:spacy.tests.pipeline.test_sentencizer.doc->sentencizer(doc)
A:spacy.tests.pipeline.test_sentencizer.sentencizer->Sentencizer(punct_chars=punct_chars)
A:spacy.tests.pipeline.test_sentencizer.bytes_data->Sentencizer(punct_chars=punct_chars).to_bytes()
A:spacy.tests.pipeline.test_sentencizer.new_sentencizer->Sentencizer().from_bytes(bytes_data)
spacy.tests.pipeline.test_sentencizer.test_sentencizer(en_vocab)
spacy.tests.pipeline.test_sentencizer.test_sentencizer_complex(en_vocab,words,sent_starts,n_sents)
spacy.tests.pipeline.test_sentencizer.test_sentencizer_custom_punct(en_vocab,punct_chars,words,sent_starts,n_sents)
spacy.tests.pipeline.test_sentencizer.test_sentencizer_serialize_bytes(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/pipeline/test_entity_linker.py----------------------------------------
A:spacy.tests.pipeline.test_entity_linker.mykb->KnowledgeBase(nlp.vocab, entity_vector_length=1)
A:spacy.tests.pipeline.test_entity_linker.sentencizer->nlp.create_pipe('sentencizer')
A:spacy.tests.pipeline.test_entity_linker.ruler->EntityRuler(nlp)
A:spacy.tests.pipeline.test_entity_linker.el_pipe->nlp.create_pipe(name='entity_linker')
A:spacy.tests.pipeline.test_entity_linker.doc->nlp(text)
A:spacy.tests.pipeline.test_entity_linker.sent_doc->ent.sent.as_doc()
A:spacy.tests.pipeline.test_entity_linker.boston_ent->Span(doc, 3, 4, label='LOC', kb_id='Q1')
A:spacy.tests.pipeline.test_entity_linker.loc->nlp(text).vocab.strings.add('LOC')
A:spacy.tests.pipeline.test_entity_linker.q1->nlp(text).vocab.strings.add('Q1')
spacy.tests.pipeline.test_entity_linker.assert_almost_equal(a,b)
spacy.tests.pipeline.test_entity_linker.nlp()
spacy.tests.pipeline.test_entity_linker.test_candidate_generation(nlp)
spacy.tests.pipeline.test_entity_linker.test_kb_invalid_combination(nlp)
spacy.tests.pipeline.test_entity_linker.test_kb_invalid_entities(nlp)
spacy.tests.pipeline.test_entity_linker.test_kb_invalid_entity_vector(nlp)
spacy.tests.pipeline.test_entity_linker.test_kb_invalid_probabilities(nlp)
spacy.tests.pipeline.test_entity_linker.test_kb_valid_entities(nlp)
spacy.tests.pipeline.test_entity_linker.test_preserving_links_asdoc(nlp)
spacy.tests.pipeline.test_entity_linker.test_preserving_links_ents(nlp)
spacy.tests.pipeline.test_entity_linker.test_preserving_links_ents_2(nlp)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/pipeline/test_textcat.py----------------------------------------
A:spacy.tests.pipeline.test_textcat.nlp->Language()
A:spacy.tests.pipeline.test_textcat.doc->Doc(nlp.vocab, words=['d'] * 3 + [w1, w2] + ['d'] * 3)
A:spacy.tests.pipeline.test_textcat.model->TextCategorizer(nlp.vocab, width=8)
A:spacy.tests.pipeline.test_textcat.optimizer->TextCategorizer(nlp.vocab, width=8).begin_training()
spacy.tests.pipeline.test_textcat.test_simple_train()
spacy.tests.pipeline.test_textcat.test_textcat_learns_multilabel()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/pipeline/test_factories.py----------------------------------------
A:spacy.tests.pipeline.test_factories.tokens->en_tokenizer(text)
A:spacy.tests.pipeline.test_factories.doc->get_doc(tokens.vocab, words=[t.text for t in tokens], heads=heads, tags=tags, pos=pos, deps=deps)
A:spacy.tests.pipeline.test_factories.nlp->Language()
A:spacy.tests.pipeline.test_factories.merge_noun_chunks->Language().create_pipe('merge_noun_chunks')
A:spacy.tests.pipeline.test_factories.merge_entities->Language().create_pipe('merge_entities')
spacy.tests.pipeline.test_factories.doc(en_tokenizer)
spacy.tests.pipeline.test_factories.test_factories_merge_ents(doc)
spacy.tests.pipeline.test_factories.test_factories_merge_noun_chunks(doc)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/morphology/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/morphology/test_morph_features.py----------------------------------------
A:spacy.tests.morphology.test_morph_features.lemmatizer->Lemmatizer(Lookups())
A:spacy.tests.morphology.test_morph_features.tag1->morphology.add({'Case_gen'})
A:spacy.tests.morphology.test_morph_features.tag2->morphology.update(tag1, {'Number_sing'})
A:spacy.tests.morphology.test_morph_features.tag3->morphology.add({'Number_sing', 'Case_gen'})
spacy.tests.morphology.test_morph_features.morphology()
spacy.tests.morphology.test_morph_features.test_add_morphology_with_int_ids(morphology)
spacy.tests.morphology.test_morph_features.test_add_morphology_with_mix_strings_and_ints(morphology)
spacy.tests.morphology.test_morph_features.test_add_morphology_with_string_names(morphology)
spacy.tests.morphology.test_morph_features.test_init(morphology)
spacy.tests.morphology.test_morph_features.test_morphology_tags_hash_distinctly(morphology)
spacy.tests.morphology.test_morph_features.test_morphology_tags_hash_independent_of_order(morphology)
spacy.tests.morphology.test_morph_features.test_update_morphology_tag(morphology)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/test_initialize.py----------------------------------------
A:spacy.tests.lang.test_initialize.nlp->get_lang_class(lang)()
A:spacy.tests.lang.test_initialize.doc->nlp('test')
A:spacy.tests.lang.test_initialize.captured->capfd.readouterr()
spacy.tests.lang.test_initialize.test_lang_initialize(lang,capfd)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/test_attrs.py----------------------------------------
A:spacy.tests.lang.test_attrs.int_attrs->intify_attrs({'F': text, 'is_alpha': True}, strings_map={text: 10}, _do_deprecated=True)
spacy.tests.lang.test_attrs.test_attrs_do_deprecated(text)
spacy.tests.lang.test_attrs.test_attrs_idempotence(text)
spacy.tests.lang.test_attrs.test_attrs_key(text)
spacy.tests.lang.test_attrs.test_lex_attrs_is_ascii(text,match)
spacy.tests.lang.test_attrs.test_lex_attrs_is_currency(text,match)
spacy.tests.lang.test_attrs.test_lex_attrs_is_punct(text,match)
spacy.tests.lang.test_attrs.test_lex_attrs_like_url(text,match)
spacy.tests.lang.test_attrs.test_lex_attrs_word_shape(text,shape)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/ga/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/ga/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.ga.test_tokenizer.tokens->ga_tokenizer(text)
spacy.tests.lang.ga.test_tokenizer.test_ga_tokenizer_handles_exception_cases(ga_tokenizer,text,expected_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/it/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/it/test_prefix_suffix_infix.py----------------------------------------
A:spacy.tests.lang.it.test_prefix_suffix_infix.tokens->it_tokenizer(text)
spacy.tests.lang.it.test_prefix_suffix_infix.test_contractions(it_tokenizer,text,expected_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/en/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/en/test_indices.py----------------------------------------
A:spacy.tests.lang.en.test_indices.tokens->en_tokenizer(text)
spacy.tests.lang.en.test_indices.test_en_complex_punct(en_tokenizer)
spacy.tests.lang.en.test_indices.test_en_simple_punct(en_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/en/test_exceptions.py----------------------------------------
A:spacy.tests.lang.en.test_exceptions.tokens->en_tokenizer(text)
A:spacy.tests.lang.en.test_exceptions.tokens_lower->en_tokenizer(text_lower)
A:spacy.tests.lang.en.test_exceptions.tokens_title->en_tokenizer(text_title)
spacy.tests.lang.en.test_exceptions.test_en_lex_attrs_norm_exceptions(en_tokenizer,text,norm)
spacy.tests.lang.en.test_exceptions.test_en_tokenizer_doesnt_split_apos_exc(en_tokenizer,text)
spacy.tests.lang.en.test_exceptions.test_en_tokenizer_excludes_ambiguous(en_tokenizer,exc)
spacy.tests.lang.en.test_exceptions.test_en_tokenizer_handles_abbr(en_tokenizer,text)
spacy.tests.lang.en.test_exceptions.test_en_tokenizer_handles_basic_contraction(en_tokenizer)
spacy.tests.lang.en.test_exceptions.test_en_tokenizer_handles_basic_contraction_punct(en_tokenizer,text)
spacy.tests.lang.en.test_exceptions.test_en_tokenizer_handles_capitalization(en_tokenizer,text_lower,text_title)
spacy.tests.lang.en.test_exceptions.test_en_tokenizer_handles_exc_in_text(en_tokenizer)
spacy.tests.lang.en.test_exceptions.test_en_tokenizer_handles_ll_contraction(en_tokenizer,text)
spacy.tests.lang.en.test_exceptions.test_en_tokenizer_handles_poss_contraction(en_tokenizer,text_poss,text)
spacy.tests.lang.en.test_exceptions.test_en_tokenizer_handles_times(en_tokenizer,text)
spacy.tests.lang.en.test_exceptions.test_en_tokenizer_keeps_title_case(en_tokenizer,pron,contraction)
spacy.tests.lang.en.test_exceptions.test_en_tokenizer_norm_exceptions(en_tokenizer,text,norms)
spacy.tests.lang.en.test_exceptions.test_en_tokenizer_splits_defined_punct(en_tokenizer,wo_punct,w_punct)
spacy.tests.lang.en.test_exceptions.test_en_tokenizer_splits_trailing_apos(en_tokenizer,text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/en/test_parser.py----------------------------------------
A:spacy.tests.lang.en.test_parser.tokens->en_tokenizer(text)
A:spacy.tests.lang.en.test_parser.doc->get_doc(tokens.vocab, words=[t.text for t in tokens], tags=tags, deps=deps, heads=heads)
A:spacy.tests.lang.en.test_parser.chunks->list(doc.noun_chunks)
spacy.tests.lang.en.test_parser.test_en_parser_noun_chunks_appositional_modifiers(en_tokenizer)
spacy.tests.lang.en.test_parser.test_en_parser_noun_chunks_coordinated(en_tokenizer)
spacy.tests.lang.en.test_parser.test_en_parser_noun_chunks_dative(en_tokenizer)
spacy.tests.lang.en.test_parser.test_en_parser_noun_chunks_pp_chunks(en_tokenizer)
spacy.tests.lang.en.test_parser.test_en_parser_noun_chunks_standard(en_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/en/test_text.py----------------------------------------
A:spacy.tests.lang.en.test_text.tokens->en_tokenizer(text)
spacy.tests.lang.en.test_text.test_en_lex_attrs_capitals(word)
spacy.tests.lang.en.test_text.test_en_tokenizer_handles_cnts(en_tokenizer,text,length)
spacy.tests.lang.en.test_text.test_en_tokenizer_handles_long_text(en_tokenizer)
spacy.tests.lang.en.test_text.test_lex_attrs_like_number(en_tokenizer,text,match)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/en/test_punct.py----------------------------------------
A:spacy.tests.lang.en.test_punct.tokens->en_tokenizer(text)
A:spacy.tests.lang.en.test_punct.tokens_punct->en_tokenizer("''")
A:spacy.tests.lang.en.test_punct.match->en_search_prefixes(text)
spacy.tests.lang.en.test_punct.test_en_tokenizer_handles_only_punct(en_tokenizer,text)
spacy.tests.lang.en.test_punct.test_en_tokenizer_splits_bracket_period(en_tokenizer)
spacy.tests.lang.en.test_punct.test_en_tokenizer_splits_close_punct(en_tokenizer,punct,text)
spacy.tests.lang.en.test_punct.test_en_tokenizer_splits_double_end_quote(en_tokenizer,text)
spacy.tests.lang.en.test_punct.test_en_tokenizer_splits_open_appostrophe(en_tokenizer,text)
spacy.tests.lang.en.test_punct.test_en_tokenizer_splits_open_close_punct(en_tokenizer,punct_open,punct_close,text)
spacy.tests.lang.en.test_punct.test_en_tokenizer_splits_open_punct(en_tokenizer,punct,text)
spacy.tests.lang.en.test_punct.test_en_tokenizer_splits_pre_punct_regex(text,punct)
spacy.tests.lang.en.test_punct.test_en_tokenizer_splits_same_close_punct(en_tokenizer,punct,text)
spacy.tests.lang.en.test_punct.test_en_tokenizer_splits_same_open_punct(en_tokenizer,punct,text)
spacy.tests.lang.en.test_punct.test_en_tokenizer_splits_two_diff_close_punct(en_tokenizer,punct,punct_add,text)
spacy.tests.lang.en.test_punct.test_en_tokenizer_splits_two_diff_open_punct(en_tokenizer,punct,punct_add,text)
spacy.tests.lang.en.test_punct.test_en_tokenizer_two_diff_punct(en_tokenizer,punct_open,punct_close,punct_open2,punct_close2,text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/en/test_customized_tokenizer.py----------------------------------------
A:spacy.tests.lang.en.test_customized_tokenizer.prefix_re->compile_prefix_regex(English.Defaults.prefixes)
A:spacy.tests.lang.en.test_customized_tokenizer.suffix_re->compile_suffix_regex(English.Defaults.suffixes)
A:spacy.tests.lang.en.test_customized_tokenizer.infix_re->compile_infix_regex(custom_infixes)
spacy.tests.lang.en.test_customized_tokenizer.custom_en_tokenizer(en_vocab)
spacy.tests.lang.en.test_customized_tokenizer.test_en_customized_tokenizer_handles_infixes(custom_en_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/en/test_noun_chunks.py----------------------------------------
A:spacy.tests.lang.en.test_noun_chunks.doc->get_doc(en_vocab, words=words, heads=heads, deps=deps)
spacy.tests.lang.en.test_noun_chunks.test_en_noun_chunks_not_nested(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/en/test_prefix_suffix_infix.py----------------------------------------
A:spacy.tests.lang.en.test_prefix_suffix_infix.tokens->en_tokenizer(text)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_comma_infix(en_tokenizer,text)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_double_hyphen_infix(en_tokenizer)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_ellipsis_infix(en_tokenizer,text)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_em_dash_infix(en_tokenizer)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_even_wrap(en_tokenizer,text)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_even_wrap_interact(en_tokenizer,text)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_hyphens(en_tokenizer,text)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_no_punct(en_tokenizer,text)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_no_special(en_tokenizer,text)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_numeric_range(en_tokenizer,text)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_period_abbr(en_tokenizer)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_period_infix(en_tokenizer,text)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_prefix_interact(en_tokenizer,text,length)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_prefix_punct(en_tokenizer,text)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_suffix_interact(en_tokenizer,text)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_suffix_punct(en_tokenizer,text)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_uneven_wrap(en_tokenizer,text)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_uneven_wrap_interact(en_tokenizer,text)
spacy.tests.lang.en.test_prefix_suffix_infix.test_final_period(en_tokenizer,text,length)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/en/test_sbd.py----------------------------------------
A:spacy.tests.lang.en.test_sbd.tokens->en_tokenizer(text)
A:spacy.tests.lang.en.test_sbd.doc->get_doc(tokens.vocab, words=[t.text for t in tokens], heads=heads, deps=deps)
spacy.tests.lang.en.test_sbd.test_en_sbd_single_punct(en_tokenizer,text,punct)
spacy.tests.lang.en.test_sbd.test_en_sentence_breaks(en_tokenizer,en_parser)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/en/test_tagger.py----------------------------------------
A:spacy.tests.lang.en.test_tagger.tokens->en_tokenizer(text)
A:spacy.tests.lang.en.test_tagger.doc->get_doc(tokens.vocab, words=[t.text for t in tokens], tags=tags)
spacy.tests.lang.en.test_tagger.test_en_tagger_load_morph_exc(en_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/fi/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/fi/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.fi.test_tokenizer.tokens->fi_tokenizer(text)
spacy.tests.lang.fi.test_tokenizer.test_fi_tokenizer_handles_testcases(fi_tokenizer,text,expected_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/ca/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/ca/test_text.py----------------------------------------
A:spacy.tests.lang.ca.test_text.tokens->ca_tokenizer(text)
spacy.tests.lang.ca.test_text.test_ca_lex_attrs_like_number(ca_tokenizer,text,match)
spacy.tests.lang.ca.test_text.test_ca_tokenizer_handles_cnts(ca_tokenizer,text,length)
spacy.tests.lang.ca.test_text.test_ca_tokenizer_handles_long_text(ca_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/ca/test_exception.py----------------------------------------
A:spacy.tests.lang.ca.test_exception.tokens->ca_tokenizer(text)
spacy.tests.lang.ca.test_exception.test_ca_tokenizer_handles_abbr(ca_tokenizer,text,lemma)
spacy.tests.lang.ca.test_exception.test_ca_tokenizer_handles_exc_in_text(ca_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/ca/test_prefix_suffix_infix.py----------------------------------------
A:spacy.tests.lang.ca.test_prefix_suffix_infix.tokens->ca_tokenizer(text)
spacy.tests.lang.ca.test_prefix_suffix_infix.test_contractions(ca_tokenizer,text,expected_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/nb/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/nb/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.nb.test_tokenizer.tokens->nb_tokenizer(text)
spacy.tests.lang.nb.test_tokenizer.test_nb_tokenizer_handles_exception_cases(nb_tokenizer,text,expected_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/bn/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/bn/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.bn.test_tokenizer.tokens->bn_tokenizer(text)
spacy.tests.lang.bn.test_tokenizer.test_bn_tokenizer_handles_long_text(bn_tokenizer)
spacy.tests.lang.bn.test_tokenizer.test_bn_tokenizer_handles_testcases(bn_tokenizer,text,expected_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/lt/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/lt/test_text.py----------------------------------------
A:spacy.tests.lang.lt.test_text.tokens->lt_tokenizer(text)
spacy.tests.lang.lt.test_text.test_lt_lex_attrs_like_number(lt_tokenizer,text,match)
spacy.tests.lang.lt.test_text.test_lt_tokenizer_abbrev_exceptions(lt_tokenizer,text)
spacy.tests.lang.lt.test_text.test_lt_tokenizer_handles_long_text(lt_tokenizer)
spacy.tests.lang.lt.test_text.test_lt_tokenizer_handles_punct_abbrev(lt_tokenizer,text,length)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/da/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/da/test_exceptions.py----------------------------------------
A:spacy.tests.lang.da.test_exceptions.tokens->da_tokenizer(text)
spacy.tests.lang.da.test_exceptions.test_da_tokenizer_handles_abbr(da_tokenizer,text)
spacy.tests.lang.da.test_exceptions.test_da_tokenizer_handles_ambiguous_abbr(da_tokenizer,text)
spacy.tests.lang.da.test_exceptions.test_da_tokenizer_handles_custom_base_exc(da_tokenizer)
spacy.tests.lang.da.test_exceptions.test_da_tokenizer_handles_dates(da_tokenizer,text)
spacy.tests.lang.da.test_exceptions.test_da_tokenizer_handles_exc_in_text(da_tokenizer)
spacy.tests.lang.da.test_exceptions.test_da_tokenizer_norm_exceptions(da_tokenizer,text,norm)
spacy.tests.lang.da.test_exceptions.test_da_tokenizer_slash(da_tokenizer,text,n_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/da/test_text.py----------------------------------------
A:spacy.tests.lang.da.test_text.tokens->da_tokenizer(text)
spacy.tests.lang.da.test_text.test_da_lex_attrs_capitals(word)
spacy.tests.lang.da.test_text.test_da_tokenizer_handles_long_text(da_tokenizer)
spacy.tests.lang.da.test_text.test_lex_attrs_like_number(da_tokenizer,text,match)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/da/test_prefix_suffix_infix.py----------------------------------------
A:spacy.tests.lang.da.test_prefix_suffix_infix.tokens->da_tokenizer("'DBA's, Lars' og Liz' bil sku' sgu' ik' ha' en bule, det ka' han ik' li' mere', sagde hun.")
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_handles_no_punct(da_tokenizer,text)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_handles_numeric_range(da_tokenizer,text)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_handles_posessives_and_contractions(da_tokenizer)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_keeps_hyphens(da_tokenizer,text)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_splits_comma_infix(da_tokenizer,text)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_splits_double_hyphen_infix(da_tokenizer)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_splits_ellipsis_infix(da_tokenizer,text)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_splits_even_wrap(da_tokenizer,text,expected)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_splits_even_wrap_interact(da_tokenizer,text)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_splits_no_special(da_tokenizer,text)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_splits_period_infix(da_tokenizer,text)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_splits_prefix_interact(da_tokenizer,text,expected)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_splits_prefix_punct(da_tokenizer,text)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_splits_suffix_interact(da_tokenizer,text)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_splits_suffix_punct(da_tokenizer,text)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_splits_uneven_wrap(da_tokenizer,text)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_splits_uneven_wrap_interact(da_tokenizer,text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/uk/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/uk/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.uk.test_tokenizer.tokens->uk_tokenizer(text)
A:spacy.tests.lang.uk.test_tokenizer.tokens_punct->uk_tokenizer("''")
spacy.tests.lang.uk.test_tokenizer.test_uk_tokenizer_handles_only_punct(uk_tokenizer,text)
spacy.tests.lang.uk.test_tokenizer.test_uk_tokenizer_splits_bracket_period(uk_tokenizer)
spacy.tests.lang.uk.test_tokenizer.test_uk_tokenizer_splits_close_punct(uk_tokenizer,punct,text)
spacy.tests.lang.uk.test_tokenizer.test_uk_tokenizer_splits_double_end_quote(uk_tokenizer,text)
spacy.tests.lang.uk.test_tokenizer.test_uk_tokenizer_splits_open_appostrophe(uk_tokenizer,text)
spacy.tests.lang.uk.test_tokenizer.test_uk_tokenizer_splits_open_close_punct(uk_tokenizer,punct_open,punct_close,text)
spacy.tests.lang.uk.test_tokenizer.test_uk_tokenizer_splits_open_punct(uk_tokenizer,punct,text)
spacy.tests.lang.uk.test_tokenizer.test_uk_tokenizer_splits_same_close_punct(uk_tokenizer,punct,text)
spacy.tests.lang.uk.test_tokenizer.test_uk_tokenizer_splits_same_open_punct(uk_tokenizer,punct,text)
spacy.tests.lang.uk.test_tokenizer.test_uk_tokenizer_splits_trailing_dot(uk_tokenizer,text)
spacy.tests.lang.uk.test_tokenizer.test_uk_tokenizer_splits_two_diff_close_punct(uk_tokenizer,punct,punct_add,text)
spacy.tests.lang.uk.test_tokenizer.test_uk_tokenizer_splits_two_diff_open_punct(uk_tokenizer,punct,punct_add,text)
spacy.tests.lang.uk.test_tokenizer.test_uk_tokenizer_two_diff_punct(uk_tokenizer,punct_open,punct_close,punct_open2,punct_close2,text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/uk/test_tokenizer_exc.py----------------------------------------
A:spacy.tests.lang.uk.test_tokenizer_exc.tokens->uk_tokenizer(text)
spacy.tests.lang.uk.test_tokenizer_exc.test_uk_tokenizer_abbrev_exceptions(uk_tokenizer,text,norms,lemmas)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/ru/test_lemmatizer.py----------------------------------------
A:spacy.tests.lang.ru.test_lemmatizer.doc->get_doc(ru_tokenizer.vocab, words=words, tags=tags)
spacy.tests.lang.ru.test_lemmatizer.test_ru_doc_lemmatization(ru_tokenizer)
spacy.tests.lang.ru.test_lemmatizer.test_ru_lemmatizer_noun_lemmas(ru_lemmatizer,text,lemmas)
spacy.tests.lang.ru.test_lemmatizer.test_ru_lemmatizer_punct(ru_lemmatizer)
spacy.tests.lang.ru.test_lemmatizer.test_ru_lemmatizer_works_with_different_pos_homonyms(ru_lemmatizer,text,pos,morphology,lemma)
spacy.tests.lang.ru.test_lemmatizer.test_ru_lemmatizer_works_with_noun_homonyms(ru_lemmatizer,text,morphology,lemma)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/ru/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/ru/test_exceptions.py----------------------------------------
A:spacy.tests.lang.ru.test_exceptions.tokens->ru_tokenizer(text)
spacy.tests.lang.ru.test_exceptions.test_ru_tokenizer_abbrev_exceptions(ru_tokenizer,text,norms)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/ru/test_text.py----------------------------------------
spacy.tests.lang.ru.test_text.test_ru_lex_attrs_capitals(word)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/ru/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.ru.test_tokenizer.tokens->ru_tokenizer(text)
A:spacy.tests.lang.ru.test_tokenizer.tokens_punct->ru_tokenizer("''")
spacy.tests.lang.ru.test_tokenizer.test_ru_tokenizer_handles_only_punct(ru_tokenizer,text)
spacy.tests.lang.ru.test_tokenizer.test_ru_tokenizer_splits_bracket_period(ru_tokenizer)
spacy.tests.lang.ru.test_tokenizer.test_ru_tokenizer_splits_close_punct(ru_tokenizer,punct,text)
spacy.tests.lang.ru.test_tokenizer.test_ru_tokenizer_splits_double_end_quote(ru_tokenizer,text)
spacy.tests.lang.ru.test_tokenizer.test_ru_tokenizer_splits_open_appostrophe(ru_tokenizer,text)
spacy.tests.lang.ru.test_tokenizer.test_ru_tokenizer_splits_open_close_punct(ru_tokenizer,punct_open,punct_close,text)
spacy.tests.lang.ru.test_tokenizer.test_ru_tokenizer_splits_open_punct(ru_tokenizer,punct,text)
spacy.tests.lang.ru.test_tokenizer.test_ru_tokenizer_splits_same_close_punct(ru_tokenizer,punct,text)
spacy.tests.lang.ru.test_tokenizer.test_ru_tokenizer_splits_same_open_punct(ru_tokenizer,punct,text)
spacy.tests.lang.ru.test_tokenizer.test_ru_tokenizer_splits_trailing_dot(ru_tokenizer,text)
spacy.tests.lang.ru.test_tokenizer.test_ru_tokenizer_splits_two_diff_close_punct(ru_tokenizer,punct,punct_add,text)
spacy.tests.lang.ru.test_tokenizer.test_ru_tokenizer_splits_two_diff_open_punct(ru_tokenizer,punct,punct_add,text)
spacy.tests.lang.ru.test_tokenizer.test_ru_tokenizer_two_diff_punct(ru_tokenizer,punct_open,punct_close,punct_open2,punct_close2,text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/ar/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/ar/test_exceptions.py----------------------------------------
A:spacy.tests.lang.ar.test_exceptions.tokens->ar_tokenizer(text)
spacy.tests.lang.ar.test_exceptions.test_ar_tokenizer_handles_abbr(ar_tokenizer,text)
spacy.tests.lang.ar.test_exceptions.test_ar_tokenizer_handles_exc_in_text(ar_tokenizer)
spacy.tests.lang.ar.test_exceptions.test_ar_tokenizer_handles_exc_in_text_2(ar_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/ar/test_text.py----------------------------------------
A:spacy.tests.lang.ar.test_text.tokens->ar_tokenizer(text)
spacy.tests.lang.ar.test_text.test_ar_tokenizer_handles_long_text(ar_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/ja/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/ja/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.ja.test_tokenizer.tokens->ja_tokenizer('I   like cheese.')
spacy.tests.lang.ja.test_tokenizer.test_extra_spaces(ja_tokenizer)
spacy.tests.lang.ja.test_tokenizer.test_ja_tokenizer(ja_tokenizer,text,expected_tokens)
spacy.tests.lang.ja.test_tokenizer.test_ja_tokenizer_pos(ja_tokenizer,text,expected_pos)
spacy.tests.lang.ja.test_tokenizer.test_ja_tokenizer_tags(ja_tokenizer,text,expected_tags)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/ja/test_lemmatization.py----------------------------------------
spacy.tests.lang.ja.test_lemmatization.test_ja_lemmatizer_assigns(ja_tokenizer,word,lemma)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/el/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/el/test_text.py----------------------------------------
A:spacy.tests.lang.el.test_text.tokens->el_tokenizer(text)
spacy.tests.lang.el.test_text.test_el_tokenizer_handles_cnts(el_tokenizer,text,length)
spacy.tests.lang.el.test_text.test_el_tokenizer_handles_long_text(el_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/el/test_exception.py----------------------------------------
A:spacy.tests.lang.el.test_exception.tokens->el_tokenizer(text)
spacy.tests.lang.el.test_exception.test_el_tokenizer_handles_abbr(el_tokenizer,text)
spacy.tests.lang.el.test_exception.test_el_tokenizer_handles_exc_in_text(el_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/ur/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/ur/test_text.py----------------------------------------
A:spacy.tests.lang.ur.test_text.tokens->ur_tokenizer(text)
spacy.tests.lang.ur.test_text.test_ur_tokenizer_handles_cnts(ur_tokenizer,text,length)
spacy.tests.lang.ur.test_text.test_ur_tokenizer_handles_long_text(ur_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/ur/test_prefix_suffix_infix.py----------------------------------------
A:spacy.tests.lang.ur.test_prefix_suffix_infix.tokens->ur_tokenizer(text)
spacy.tests.lang.ur.test_prefix_suffix_infix.test_contractions(ur_tokenizer,text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/es/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/es/test_text.py----------------------------------------
A:spacy.tests.lang.es.test_text.tokens->es_tokenizer(text)
spacy.tests.lang.es.test_text.test_es_tokenizer_handles_cnts(es_tokenizer,text,length)
spacy.tests.lang.es.test_text.test_es_tokenizer_handles_long_text(es_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/es/test_exception.py----------------------------------------
A:spacy.tests.lang.es.test_exception.tokens->es_tokenizer(text)
spacy.tests.lang.es.test_exception.test_es_tokenizer_handles_abbr(es_tokenizer,text,lemma)
spacy.tests.lang.es.test_exception.test_es_tokenizer_handles_exc_in_text(es_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/tt/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/tt/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.tt.test_tokenizer.tokens->tt_tokenizer(text)
spacy.tests.lang.tt.test_tokenizer.test_tt_tokenizer_handles_norm_exceptions(tt_tokenizer,text,norms)
spacy.tests.lang.tt.test_tokenizer.test_tt_tokenizer_handles_testcases(tt_tokenizer,text,expected_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/sv/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/sv/test_exceptions.py----------------------------------------
A:spacy.tests.lang.sv.test_exceptions.tokens->sv_tokenizer(text)
spacy.tests.lang.sv.test_exceptions.test_sv_tokenizer_handles_abbr(sv_tokenizer,text)
spacy.tests.lang.sv.test_exceptions.test_sv_tokenizer_handles_ambiguous_abbr(sv_tokenizer,text)
spacy.tests.lang.sv.test_exceptions.test_sv_tokenizer_handles_custom_base_exc(sv_tokenizer)
spacy.tests.lang.sv.test_exceptions.test_sv_tokenizer_handles_exc_in_text(sv_tokenizer)
spacy.tests.lang.sv.test_exceptions.test_sv_tokenizer_handles_exception_cases(sv_tokenizer,text,expected_tokens)
spacy.tests.lang.sv.test_exceptions.test_sv_tokenizer_handles_verb_exceptions(sv_tokenizer,text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/sv/test_text.py----------------------------------------
A:spacy.tests.lang.sv.test_text.tokens->sv_tokenizer(text)
spacy.tests.lang.sv.test_text.test_sv_tokenizer_handles_long_text(sv_tokenizer)
spacy.tests.lang.sv.test_text.test_sv_tokenizer_handles_trailing_dot_for_i_in_sentence(sv_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/sv/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.sv.test_tokenizer.tokens->sv_tokenizer(text)
spacy.tests.lang.sv.test_tokenizer.test_sv_tokenizer_handles_exception_cases(sv_tokenizer,text,expected_tokens)
spacy.tests.lang.sv.test_tokenizer.test_sv_tokenizer_handles_verb_exceptions(sv_tokenizer,text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/sv/test_prefix_suffix_infix.py----------------------------------------
A:spacy.tests.lang.sv.test_prefix_suffix_infix.tokens->sv_tokenizer(text)
spacy.tests.lang.sv.test_prefix_suffix_infix.test_tokenizer_handles_no_punct(sv_tokenizer,text)
spacy.tests.lang.sv.test_prefix_suffix_infix.test_tokenizer_splits_comma_infix(sv_tokenizer,text)
spacy.tests.lang.sv.test_prefix_suffix_infix.test_tokenizer_splits_ellipsis_infix(sv_tokenizer,text)
spacy.tests.lang.sv.test_prefix_suffix_infix.test_tokenizer_splits_no_special(sv_tokenizer,text)
spacy.tests.lang.sv.test_prefix_suffix_infix.test_tokenizer_splits_period_infix(sv_tokenizer,text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/id/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/id/test_text.py----------------------------------------
spacy.tests.lang.id.test_text.test_id_lex_attrs_capitals(word)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/id/test_prefix_suffix_infix.py----------------------------------------
A:spacy.tests.lang.id.test_prefix_suffix_infix.tokens->id_tokenizer('Arsene Wenger--manajer Arsenal--melakukan konferensi pers.')
spacy.tests.lang.id.test_prefix_suffix_infix.test_id_tokenizer_splits_comma_infix(id_tokenizer,text)
spacy.tests.lang.id.test_prefix_suffix_infix.test_id_tokenizer_splits_double_hyphen_infix(id_tokenizer)
spacy.tests.lang.id.test_prefix_suffix_infix.test_id_tokenizer_splits_ellipsis_infix(id_tokenizer,text)
spacy.tests.lang.id.test_prefix_suffix_infix.test_id_tokenizer_splits_even_wrap(id_tokenizer,text)
spacy.tests.lang.id.test_prefix_suffix_infix.test_id_tokenizer_splits_even_wrap_interact(id_tokenizer,text)
spacy.tests.lang.id.test_prefix_suffix_infix.test_id_tokenizer_splits_hyphens(id_tokenizer,text,length)
spacy.tests.lang.id.test_prefix_suffix_infix.test_id_tokenizer_splits_no_punct(id_tokenizer,text)
spacy.tests.lang.id.test_prefix_suffix_infix.test_id_tokenizer_splits_no_special(id_tokenizer,text)
spacy.tests.lang.id.test_prefix_suffix_infix.test_id_tokenizer_splits_numeric_range(id_tokenizer,text)
spacy.tests.lang.id.test_prefix_suffix_infix.test_id_tokenizer_splits_period_infix(id_tokenizer,text)
spacy.tests.lang.id.test_prefix_suffix_infix.test_id_tokenizer_splits_prefix_interact(id_tokenizer,text,length)
spacy.tests.lang.id.test_prefix_suffix_infix.test_id_tokenizer_splits_prefix_punct(id_tokenizer,text)
spacy.tests.lang.id.test_prefix_suffix_infix.test_id_tokenizer_splits_suffix_interact(id_tokenizer,text)
spacy.tests.lang.id.test_prefix_suffix_infix.test_id_tokenizer_splits_suffix_punct(id_tokenizer,text)
spacy.tests.lang.id.test_prefix_suffix_infix.test_id_tokenizer_splits_uneven_wrap_interact(id_tokenizer,text)
spacy.tests.lang.id.test_prefix_suffix_infix.test_tokenizer_splits_uneven_wrap(id_tokenizer,text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/sr/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/sr/test_exceptions.py----------------------------------------
A:spacy.tests.lang.sr.test_exceptions.tokens->sr_tokenizer(text)
spacy.tests.lang.sr.test_exceptions.test_sr_tokenizer_abbrev_exceptions(sr_tokenizer,text,norms,lemmas)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/sr/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.sr.test_tokenizer.tokens->sr_tokenizer(text)
A:spacy.tests.lang.sr.test_tokenizer.tokens_punct->sr_tokenizer("''")
spacy.tests.lang.sr.test_tokenizer.test_sr_tokenizer_handles_only_punct(sr_tokenizer,text)
spacy.tests.lang.sr.test_tokenizer.test_sr_tokenizer_splits_bracket_period(sr_tokenizer)
spacy.tests.lang.sr.test_tokenizer.test_sr_tokenizer_splits_close_punct(sr_tokenizer,punct,text)
spacy.tests.lang.sr.test_tokenizer.test_sr_tokenizer_splits_double_end_quote(sr_tokenizer,text)
spacy.tests.lang.sr.test_tokenizer.test_sr_tokenizer_splits_open_appostrophe(sr_tokenizer,text)
spacy.tests.lang.sr.test_tokenizer.test_sr_tokenizer_splits_open_close_punct(sr_tokenizer,punct_open,punct_close,text)
spacy.tests.lang.sr.test_tokenizer.test_sr_tokenizer_splits_open_punct(sr_tokenizer,punct,text)
spacy.tests.lang.sr.test_tokenizer.test_sr_tokenizer_splits_same_close_punct(sr_tokenizer,punct,text)
spacy.tests.lang.sr.test_tokenizer.test_sr_tokenizer_splits_same_open_punct(sr_tokenizer,punct,text)
spacy.tests.lang.sr.test_tokenizer.test_sr_tokenizer_splits_trailing_dot(sr_tokenizer,text)
spacy.tests.lang.sr.test_tokenizer.test_sr_tokenizer_splits_two_diff_close_punct(sr_tokenizer,punct,punct_add,text)
spacy.tests.lang.sr.test_tokenizer.test_sr_tokenizer_splits_two_diff_open_punct(sr_tokenizer,punct,punct_add,text)
spacy.tests.lang.sr.test_tokenizer.test_sr_tokenizer_two_diff_punct(sr_tokenizer,punct_open,punct_close,punct_open2,punct_close2,text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/de/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/de/test_exceptions.py----------------------------------------
A:spacy.tests.lang.de.test_exceptions.tokens->de_tokenizer(text)
spacy.tests.lang.de.test_exceptions.test_de_lex_attrs_norm_exceptions(de_tokenizer,text,norm)
spacy.tests.lang.de.test_exceptions.test_de_tokenizer_handles_abbr(de_tokenizer,text)
spacy.tests.lang.de.test_exceptions.test_de_tokenizer_handles_exc_in_text(de_tokenizer)
spacy.tests.lang.de.test_exceptions.test_de_tokenizer_norm_exceptions(de_tokenizer,text,norms)
spacy.tests.lang.de.test_exceptions.test_de_tokenizer_splits_contractions(de_tokenizer,text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/de/test_parser.py----------------------------------------
A:spacy.tests.lang.de.test_parser.tokens->de_tokenizer(text)
A:spacy.tests.lang.de.test_parser.doc->get_doc(tokens.vocab, words=[t.text for t in tokens], tags=tags, deps=deps, heads=heads)
A:spacy.tests.lang.de.test_parser.chunks->list(doc.noun_chunks)
spacy.tests.lang.de.test_parser.test_de_extended_chunk(de_tokenizer)
spacy.tests.lang.de.test_parser.test_de_parser_noun_chunks_standard_de(de_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/de/test_text.py----------------------------------------
A:spacy.tests.lang.de.test_text.tokens->de_tokenizer(text)
spacy.tests.lang.de.test_text.test_de_tokenizer_handles_examples(de_tokenizer,text,length)
spacy.tests.lang.de.test_text.test_de_tokenizer_handles_long_text(de_tokenizer)
spacy.tests.lang.de.test_text.test_de_tokenizer_handles_long_words(de_tokenizer,text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/de/test_prefix_suffix_infix.py----------------------------------------
A:spacy.tests.lang.de.test_prefix_suffix_infix.tokens->de_tokenizer('Viele Regeln--wie die Bindestrich-Regeln--sind kompliziert.')
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_keeps_hyphens(de_tokenizer,text)
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_splits_comma_infix(de_tokenizer,text)
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_splits_double_hyphen_infix(de_tokenizer)
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_splits_ellipsis_infix(de_tokenizer,text)
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_splits_even_wrap(de_tokenizer,text)
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_splits_even_wrap_interact(de_tokenizer,text)
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_splits_no_punct(de_tokenizer,text)
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_splits_no_special(de_tokenizer,text)
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_splits_numeric_range(de_tokenizer,text)
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_splits_period_infix(de_tokenizer,text)
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_splits_prefix_interact(de_tokenizer,text,length)
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_splits_prefix_punct(de_tokenizer,text)
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_splits_suffix_interact(de_tokenizer,text)
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_splits_suffix_punct(de_tokenizer,text)
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_splits_uneven_wrap(de_tokenizer,text)
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_splits_uneven_wrap_interact(de_tokenizer,text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/he/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/he/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.he.test_tokenizer.tokens->he_tokenizer(text)
spacy.tests.lang.he.test_tokenizer.test_he_tokenizer_handles_abbreviation(he_tokenizer,text,expected_tokens)
spacy.tests.lang.he.test_tokenizer.test_he_tokenizer_handles_punct(he_tokenizer,text,expected_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/ko/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/ko/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.ko.test_tokenizer.tokens->ko_tokenizer('')
spacy.tests.lang.ko.test_tokenizer.test_ko_empty_doc(ko_tokenizer)
spacy.tests.lang.ko.test_tokenizer.test_ko_tokenizer(ko_tokenizer,text,expected_tokens)
spacy.tests.lang.ko.test_tokenizer.test_ko_tokenizer_full_tags(ko_tokenizer,text,expected_tags)
spacy.tests.lang.ko.test_tokenizer.test_ko_tokenizer_pos(ko_tokenizer,text,expected_pos)
spacy.tests.lang.ko.test_tokenizer.test_ko_tokenizer_tags(ko_tokenizer,text,expected_tags)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/ko/test_lemmatization.py----------------------------------------
spacy.tests.lang.ko.test_lemmatization.test_ko_lemmatizer_assigns(ko_tokenizer,word,lemma)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/nl/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/nl/test_text.py----------------------------------------
A:spacy.tests.lang.nl.test_text.tokens->nl_tokenizer(text)
spacy.tests.lang.nl.test_text.test_nl_lex_attrs_capitals(word)
spacy.tests.lang.nl.test_text.test_tokenizer_doesnt_split_hyphens(nl_tokenizer,text,num_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/ro/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/ro/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.ro.test_tokenizer.tokens->ro_tokenizer(text)
spacy.tests.lang.ro.test_tokenizer.test_ro_tokenizer_handles_testcases(ro_tokenizer,text,expected_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/pl/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/pl/test_text.py----------------------------------------
A:spacy.tests.lang.pl.test_text.tokens->pl_tokenizer(text)
spacy.tests.lang.pl.test_text.test_lex_attrs_like_number(pl_tokenizer,text,match)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/pl/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.pl.test_tokenizer.tokens->pl_tokenizer(text)
spacy.tests.lang.pl.test_tokenizer.test_tokenizer_handles_testcases(pl_tokenizer,text,expected_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/th/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/th/test_tokenizer.py----------------------------------------
spacy.tests.lang.th.test_tokenizer.test_th_tokenizer(th_tokenizer,text,expected_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/tr/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/hu/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/hu/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.hu.test_tokenizer.tokens->hu_tokenizer(text)
spacy.tests.lang.hu.test_tokenizer.test_hu_tokenizer_handles_testcases(hu_tokenizer,text,expected_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/fr/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/fr/test_exceptions.py----------------------------------------
A:spacy.tests.lang.fr.test_exceptions.tokens->fr_tokenizer(text)
spacy.tests.lang.fr.test_exceptions.test_fr_tokenizer_handles_abbr(fr_tokenizer,text,lemma)
spacy.tests.lang.fr.test_exceptions.test_fr_tokenizer_handles_exc_in_text(fr_tokenizer)
spacy.tests.lang.fr.test_exceptions.test_fr_tokenizer_handles_exc_in_text_2(fr_tokenizer)
spacy.tests.lang.fr.test_exceptions.test_fr_tokenizer_handles_title(fr_tokenizer)
spacy.tests.lang.fr.test_exceptions.test_fr_tokenizer_handles_title_2(fr_tokenizer)
spacy.tests.lang.fr.test_exceptions.test_fr_tokenizer_handles_title_3(fr_tokenizer)
spacy.tests.lang.fr.test_exceptions.test_fr_tokenizer_infix_exceptions(fr_tokenizer,text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/fr/test_text.py----------------------------------------
A:spacy.tests.lang.fr.test_text.tokens->fr_tokenizer(text)
spacy.tests.lang.fr.test_text.test_fr_lex_attrs_capitals(word)
spacy.tests.lang.fr.test_text.test_tokenizer_handles_long_text(fr_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/fr/test_prefix_suffix_infix.py----------------------------------------
A:spacy.tests.lang.fr.test_prefix_suffix_infix.SPLIT_INFIX->"(?<=[{a}]\\')(?=[{a}])".format(a=ALPHA)
A:spacy.tests.lang.fr.test_prefix_suffix_infix.fr_tokenizer_w_infix->FrenchTest.Defaults.create_tokenizer()
A:spacy.tests.lang.fr.test_prefix_suffix_infix.tokens->fr_tokenizer_w_infix(text)
spacy.tests.lang.fr.test_prefix_suffix_infix.test_issue768(text,expected_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/pt/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/lang/pt/test_text.py----------------------------------------
spacy.tests.lang.pt.test_text.test_pt_lex_attrs_capitals(word)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/serialize/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/serialize/test_serialize_doc.py----------------------------------------
A:spacy.tests.serialize.test_serialize_doc.doc->Doc(en_vocab, words=['hello', 'world'])
A:spacy.tests.serialize.test_serialize_doc.data->Doc(en_vocab, words=['hello', 'world']).to_bytes()
A:spacy.tests.serialize.test_serialize_doc.doc2->Doc(en_vocab)
A:spacy.tests.serialize.test_serialize_doc.doc_b->Doc(en_vocab, words=['hello', 'world']).to_bytes()
A:spacy.tests.serialize.test_serialize_doc.new_doc->Doc(en_vocab).from_bytes(doc.to_bytes(exclude=['user_data']))
A:spacy.tests.serialize.test_serialize_doc.doc_d->Doc(en_vocab).from_disk(file_path)
A:spacy.tests.serialize.test_serialize_doc.file_path->path2str(file_path)
spacy.tests.serialize.test_serialize_doc.test_serialize_doc_exclude(en_vocab)
spacy.tests.serialize.test_serialize_doc.test_serialize_doc_roundtrip_bytes(en_vocab)
spacy.tests.serialize.test_serialize_doc.test_serialize_doc_roundtrip_disk(en_vocab)
spacy.tests.serialize.test_serialize_doc.test_serialize_doc_roundtrip_disk_str_path(en_vocab)
spacy.tests.serialize.test_serialize_doc.test_serialize_empty_doc(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/serialize/test_serialize_kb.py----------------------------------------
A:spacy.tests.serialize.test_serialize_kb.kb1->_get_dummy_kb(en_vocab)
A:spacy.tests.serialize.test_serialize_kb.dir_path->ensure_path(d)
A:spacy.tests.serialize.test_serialize_kb.kb2->KnowledgeBase(vocab=en_vocab, entity_vector_length=3)
A:spacy.tests.serialize.test_serialize_kb.kb->KnowledgeBase(vocab=vocab, entity_vector_length=3)
A:spacy.tests.serialize.test_serialize_kb.candidates->sorted(kb.get_candidates('double07'), key=lambda x: x.entity_)
spacy.tests.serialize.test_serialize_kb._check_kb(kb)
spacy.tests.serialize.test_serialize_kb._get_dummy_kb(vocab)
spacy.tests.serialize.test_serialize_kb.test_serialize_kb_disk(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/serialize/test_serialize_language.py----------------------------------------
A:spacy.tests.serialize.test_serialize_language.language->Language(meta=meta_data)
A:spacy.tests.serialize.test_serialize_language.new_language->Language().from_disk(d)
A:spacy.tests.serialize.test_serialize_language.prefix_re->re.compile('1/|2/|:[0-9][0-9][A-K]:|:[0-9][0-9]:')
A:spacy.tests.serialize.test_serialize_language.suffix_re->re.compile('')
A:spacy.tests.serialize.test_serialize_language.infix_re->re.compile('[~]')
A:spacy.tests.serialize.test_serialize_language.nlp->Language(meta=meta_data)
A:spacy.tests.serialize.test_serialize_language.nlp.tokenizer->custom_tokenizer(nlp)
A:spacy.tests.serialize.test_serialize_language.new_nlp->Language().from_bytes(nlp.to_bytes(exclude=['meta']))
spacy.tests.serialize.test_serialize_language.meta_data()
spacy.tests.serialize.test_serialize_language.test_serialize_language_exclude(meta_data)
spacy.tests.serialize.test_serialize_language.test_serialize_language_meta_disk(meta_data)
spacy.tests.serialize.test_serialize_language.test_serialize_with_custom_tokenizer()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/serialize/test_serialize_extension_attrs.py----------------------------------------
A:spacy.tests.serialize.test_serialize_extension_attrs.doc->Doc(Vocab()).from_bytes(doc_b)
A:spacy.tests.serialize.test_serialize_extension_attrs.doc_b->doc_w_attrs.to_bytes()
spacy.tests.serialize.test_serialize_extension_attrs.doc_w_attrs(en_tokenizer)
spacy.tests.serialize.test_serialize_extension_attrs.test_serialize_ext_attrs_from_bytes(doc_w_attrs)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/serialize/test_serialize_tokenizer.py----------------------------------------
A:spacy.tests.serialize.test_serialize_tokenizer.tok->get_lang_class('en').Defaults.create_tokenizer()
A:spacy.tests.serialize.test_serialize_tokenizer.tokenizer->Tokenizer(en_vocab, suffix_search=en_tokenizer.suffix_search)
A:spacy.tests.serialize.test_serialize_tokenizer.tokenizer_bytes->Tokenizer(en_vocab, suffix_search=en_tokenizer.suffix_search).to_bytes()
A:spacy.tests.serialize.test_serialize_tokenizer.new_tokenizer->load_tokenizer(tokenizer.to_bytes())
A:spacy.tests.serialize.test_serialize_tokenizer.doc1->tokenizer(text)
A:spacy.tests.serialize.test_serialize_tokenizer.doc2->new_tokenizer(text)
A:spacy.tests.serialize.test_serialize_tokenizer.tokenizer_d->en_tokenizer.from_disk(file_path)
spacy.tests.serialize.test_serialize_tokenizer.load_tokenizer(b)
spacy.tests.serialize.test_serialize_tokenizer.test_serialize_custom_tokenizer(en_vocab,en_tokenizer)
spacy.tests.serialize.test_serialize_tokenizer.test_serialize_tokenizer_roundtrip_bytes(en_tokenizer,text)
spacy.tests.serialize.test_serialize_tokenizer.test_serialize_tokenizer_roundtrip_disk(en_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/serialize/test_serialize_pipeline.py----------------------------------------
A:spacy.tests.serialize.test_serialize_pipeline.parser->Parser(en_vocab)
A:spacy.tests.serialize.test_serialize_pipeline.(parser.model, cfg)->Parser(en_vocab).Model(parser.moves.n_moves)
A:spacy.tests.serialize.test_serialize_pipeline.tagger1->tagger1.from_bytes(tagger1_b).from_bytes(tagger1_b)
A:spacy.tests.serialize.test_serialize_pipeline.tagger2->Tagger(en_vocab)
A:spacy.tests.serialize.test_serialize_pipeline.tagger1.model->tagger1.from_bytes(tagger1_b).from_bytes(tagger1_b).Model(8)
A:spacy.tests.serialize.test_serialize_pipeline.(parser.model, _)->Parser(en_vocab).Model(0)
A:spacy.tests.serialize.test_serialize_pipeline.new_parser->get_new_parser().from_bytes(parser.to_bytes(exclude=['cfg']), exclude=['vocab'])
A:spacy.tests.serialize.test_serialize_pipeline.(new_parser.model, _)->get_new_parser().from_bytes(parser.to_bytes(exclude=['cfg']), exclude=['vocab']).Model(0)
A:spacy.tests.serialize.test_serialize_pipeline.parser_d->parser_d.from_disk(file_path).from_disk(file_path)
A:spacy.tests.serialize.test_serialize_pipeline.(parser_d.model, _)->parser_d.from_disk(file_path).from_disk(file_path).Model(0)
A:spacy.tests.serialize.test_serialize_pipeline.parser_bytes->Parser(en_vocab).to_bytes(exclude=['model', 'vocab'])
A:spacy.tests.serialize.test_serialize_pipeline.parser_d_bytes->parser_d.from_disk(file_path).from_disk(file_path).to_bytes(exclude=['model', 'vocab'])
A:spacy.tests.serialize.test_serialize_pipeline.bytes_data->Parser(en_vocab).to_bytes(exclude=['vocab'])
A:spacy.tests.serialize.test_serialize_pipeline.tagger1_b->tagger1.from_bytes(tagger1_b).from_bytes(tagger1_b).to_bytes()
A:spacy.tests.serialize.test_serialize_pipeline.new_tagger1->Tagger(en_vocab).from_bytes(tagger1_b)
A:spacy.tests.serialize.test_serialize_pipeline.tagger1_d->Tagger(en_vocab).from_disk(file_path1)
A:spacy.tests.serialize.test_serialize_pipeline.tagger2_d->Tagger(en_vocab).from_disk(file_path2)
A:spacy.tests.serialize.test_serialize_pipeline.tensorizer->Tensorizer(en_vocab)
A:spacy.tests.serialize.test_serialize_pipeline.tensorizer.model->Tensorizer(en_vocab).Model()
A:spacy.tests.serialize.test_serialize_pipeline.tensorizer_b->Tensorizer(en_vocab).to_bytes(exclude=['vocab'])
A:spacy.tests.serialize.test_serialize_pipeline.new_tensorizer->Tensorizer(en_vocab).from_bytes(tensorizer_b)
A:spacy.tests.serialize.test_serialize_pipeline.tensorizer_d->Tensorizer(en_vocab).from_disk(file_path)
A:spacy.tests.serialize.test_serialize_pipeline.textcat->TextCategorizer(en_vocab, labels=['ENTITY', 'ACTION', 'MODIFIER'])
spacy.tests.serialize.test_serialize_pipeline.blank_parser(en_vocab)
spacy.tests.serialize.test_serialize_pipeline.parser(en_vocab)
spacy.tests.serialize.test_serialize_pipeline.taggers(en_vocab)
spacy.tests.serialize.test_serialize_pipeline.test_serialize_parser_roundtrip_bytes(en_vocab,Parser)
spacy.tests.serialize.test_serialize_pipeline.test_serialize_parser_roundtrip_disk(en_vocab,Parser)
spacy.tests.serialize.test_serialize_pipeline.test_serialize_pipe_exclude(en_vocab,Parser)
spacy.tests.serialize.test_serialize_pipeline.test_serialize_tagger_roundtrip_bytes(en_vocab,taggers)
spacy.tests.serialize.test_serialize_pipeline.test_serialize_tagger_roundtrip_disk(en_vocab,taggers)
spacy.tests.serialize.test_serialize_pipeline.test_serialize_tensorizer_roundtrip_bytes(en_vocab)
spacy.tests.serialize.test_serialize_pipeline.test_serialize_tensorizer_roundtrip_disk(en_vocab)
spacy.tests.serialize.test_serialize_pipeline.test_serialize_textcat_empty(en_vocab)
spacy.tests.serialize.test_serialize_pipeline.test_to_from_bytes(parser,blank_parser)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/serialize/test_serialize_vocab_strings.py----------------------------------------
A:spacy.tests.serialize.test_serialize_vocab_strings.text_hash->en_vocab.strings.add(text)
A:spacy.tests.serialize.test_serialize_vocab_strings.vocab_bytes->en_vocab.to_bytes(exclude=['lookups'])
A:spacy.tests.serialize.test_serialize_vocab_strings.new_vocab->Vocab().from_bytes(vocab_bytes)
A:spacy.tests.serialize.test_serialize_vocab_strings.vocab1->Vocab(strings=strings)
A:spacy.tests.serialize.test_serialize_vocab_strings.vocab2->vocab2.from_disk(file_path).from_disk(file_path)
A:spacy.tests.serialize.test_serialize_vocab_strings.vocab1_b->Vocab(strings=strings).to_bytes()
A:spacy.tests.serialize.test_serialize_vocab_strings.vocab2_b->vocab2.from_disk(file_path).from_disk(file_path).to_bytes()
A:spacy.tests.serialize.test_serialize_vocab_strings.new_vocab1->Vocab().from_bytes(vocab1_b)
A:spacy.tests.serialize.test_serialize_vocab_strings.vocab1_d->Vocab().from_disk(file_path1)
A:spacy.tests.serialize.test_serialize_vocab_strings.vocab2_d->Vocab().from_disk(file_path2)
A:spacy.tests.serialize.test_serialize_vocab_strings.vocab->Vocab(strings=strings)
A:spacy.tests.serialize.test_serialize_vocab_strings.length->len(vocab)
A:spacy.tests.serialize.test_serialize_vocab_strings.sstore1->StringStore(strings=strings1)
A:spacy.tests.serialize.test_serialize_vocab_strings.sstore2->StringStore(strings=strings2)
A:spacy.tests.serialize.test_serialize_vocab_strings.sstore1_b->StringStore(strings=strings1).to_bytes()
A:spacy.tests.serialize.test_serialize_vocab_strings.sstore2_b->StringStore(strings=strings2).to_bytes()
A:spacy.tests.serialize.test_serialize_vocab_strings.new_sstore1->StringStore().from_bytes(sstore1_b)
A:spacy.tests.serialize.test_serialize_vocab_strings.sstore1_d->StringStore().from_disk(file_path1)
A:spacy.tests.serialize.test_serialize_vocab_strings.sstore2_d->StringStore().from_disk(file_path2)
spacy.tests.serialize.test_serialize_vocab_strings.test_deserialize_vocab_seen_entries(strings,lex_attr)
spacy.tests.serialize.test_serialize_vocab_strings.test_serialize_stringstore_roundtrip_bytes(strings1,strings2)
spacy.tests.serialize.test_serialize_vocab_strings.test_serialize_stringstore_roundtrip_disk(strings1,strings2)
spacy.tests.serialize.test_serialize_vocab_strings.test_serialize_vocab(en_vocab,text)
spacy.tests.serialize.test_serialize_vocab_strings.test_serialize_vocab_lex_attrs_bytes(strings,lex_attr)
spacy.tests.serialize.test_serialize_vocab_strings.test_serialize_vocab_lex_attrs_disk(strings,lex_attr)
spacy.tests.serialize.test_serialize_vocab_strings.test_serialize_vocab_roundtrip_bytes(strings1,strings2)
spacy.tests.serialize.test_serialize_vocab_strings.test_serialize_vocab_roundtrip_disk(strings1,strings2)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/vocab_vectors/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/vocab_vectors/test_vectors.py----------------------------------------
A:spacy.tests.vocab_vectors.test_vectors.v->Vectors(data=data)
A:spacy.tests.vocab_vectors.test_vectors.orig->numpy.ndarray((5, 3), dtype='f').copy()
A:spacy.tests.vocab_vectors.test_vectors.doc->Doc(vocab, words=text)
A:spacy.tests.vocab_vectors.test_vectors.truth->list(ngrams_vocab.get_vector(text, 1, 6))
A:spacy.tests.vocab_vectors.test_vectors.test->list([(ngrams_vectors[1][1][i] + ngrams_vectors[2][1][i] + ngrams_vectors[3][1][i]) / 3 for i in range(len(ngrams_vectors[1][1]))])
A:spacy.tests.vocab_vectors.test_vectors.token->tokenizer_v(text1)
A:spacy.tests.vocab_vectors.test_vectors.doc1->Doc(vocab, words=text1)
A:spacy.tests.vocab_vectors.test_vectors.doc2->Doc(vocab, words=text2)
A:spacy.tests.vocab_vectors.test_vectors.vocab->Vocab(vectors_name='test_vocab_prune_vectors')
A:spacy.tests.vocab_vectors.test_vectors.data->numpy.ndarray((5, 3), dtype='f')
A:spacy.tests.vocab_vectors.test_vectors.remap->Vocab(vectors_name='test_vocab_prune_vectors').prune_vectors(2)
spacy.tests.vocab_vectors.test_vectors.data()
spacy.tests.vocab_vectors.test_vectors.ngrams_vectors()
spacy.tests.vocab_vectors.test_vectors.ngrams_vocab(en_vocab,ngrams_vectors)
spacy.tests.vocab_vectors.test_vectors.resize_data()
spacy.tests.vocab_vectors.test_vectors.strings()
spacy.tests.vocab_vectors.test_vectors.test_get_vector(strings,data)
spacy.tests.vocab_vectors.test_vectors.test_get_vector_resize(strings,data,resize_data)
spacy.tests.vocab_vectors.test_vectors.test_init_vectors_with_data(strings,data)
spacy.tests.vocab_vectors.test_vectors.test_init_vectors_with_resize_data(data,resize_data)
spacy.tests.vocab_vectors.test_vectors.test_init_vectors_with_resize_shape(strings,resize_data)
spacy.tests.vocab_vectors.test_vectors.test_init_vectors_with_shape(strings)
spacy.tests.vocab_vectors.test_vectors.test_set_vector(strings,data)
spacy.tests.vocab_vectors.test_vectors.test_vectors__ngrams_subword(ngrams_vocab,ngrams_vectors,text)
spacy.tests.vocab_vectors.test_vectors.test_vectors__ngrams_word(ngrams_vocab,ngrams_vectors,text)
spacy.tests.vocab_vectors.test_vectors.test_vectors_doc_doc_similarity(vocab,text1,text2)
spacy.tests.vocab_vectors.test_vectors.test_vectors_doc_vector(vocab,text)
spacy.tests.vocab_vectors.test_vectors.test_vectors_lexeme_doc_similarity(vocab,text)
spacy.tests.vocab_vectors.test_vectors.test_vectors_lexeme_lexeme_similarity(vocab,text1,text2)
spacy.tests.vocab_vectors.test_vectors.test_vectors_lexeme_span_similarity(vocab,text)
spacy.tests.vocab_vectors.test_vectors.test_vectors_lexeme_vector(vocab,text)
spacy.tests.vocab_vectors.test_vectors.test_vectors_span_doc_similarity(vocab,text)
spacy.tests.vocab_vectors.test_vectors.test_vectors_span_span_similarity(vocab,text)
spacy.tests.vocab_vectors.test_vectors.test_vectors_span_vector(vocab,text)
spacy.tests.vocab_vectors.test_vectors.test_vectors_token_doc_similarity(vocab,text)
spacy.tests.vocab_vectors.test_vectors.test_vectors_token_lexeme_similarity(tokenizer_v,vocab,text1,text2)
spacy.tests.vocab_vectors.test_vectors.test_vectors_token_span_similarity(vocab,text)
spacy.tests.vocab_vectors.test_vectors.test_vectors_token_token_similarity(tokenizer_v,text)
spacy.tests.vocab_vectors.test_vectors.test_vectors_token_vector(tokenizer_v,vectors,text)
spacy.tests.vocab_vectors.test_vectors.test_vocab_add_vector()
spacy.tests.vocab_vectors.test_vectors.test_vocab_prune_vectors()
spacy.tests.vocab_vectors.test_vectors.tokenizer_v(vocab)
spacy.tests.vocab_vectors.test_vectors.vectors()
spacy.tests.vocab_vectors.test_vectors.vocab(en_vocab,vectors)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/vocab_vectors/test_lookups.py----------------------------------------
A:spacy.tests.vocab_vectors.test_lookups.lookups->Lookups()
A:spacy.tests.vocab_vectors.test_lookups.table->Vocab().lookups.get_table(table_name)
A:spacy.tests.vocab_vectors.test_lookups.table_bytes->Vocab().lookups.get_table(table_name).to_bytes()
A:spacy.tests.vocab_vectors.test_lookups.new_table->Table().from_bytes(table_bytes)
A:spacy.tests.vocab_vectors.test_lookups.new_table2->Table(data={'def': 456})
A:spacy.tests.vocab_vectors.test_lookups.lookups_bytes->Lookups().to_bytes()
A:spacy.tests.vocab_vectors.test_lookups.new_lookups->Lookups()
A:spacy.tests.vocab_vectors.test_lookups.table1->Lookups().get_table('table1')
A:spacy.tests.vocab_vectors.test_lookups.table2->Lookups().get_table('table2')
A:spacy.tests.vocab_vectors.test_lookups.vocab->Vocab()
A:spacy.tests.vocab_vectors.test_lookups.vocab_bytes->Vocab().to_bytes()
A:spacy.tests.vocab_vectors.test_lookups.new_vocab->Vocab()
spacy.tests.vocab_vectors.test_lookups.test_lookups_api()
spacy.tests.vocab_vectors.test_lookups.test_lookups_to_from_bytes()
spacy.tests.vocab_vectors.test_lookups.test_lookups_to_from_bytes_via_vocab()
spacy.tests.vocab_vectors.test_lookups.test_lookups_to_from_disk()
spacy.tests.vocab_vectors.test_lookups.test_lookups_to_from_disk_via_vocab()
spacy.tests.vocab_vectors.test_lookups.test_table_api()
spacy.tests.vocab_vectors.test_lookups.test_table_api_to_from_bytes()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/vocab_vectors/test_lexeme.py----------------------------------------
A:spacy.tests.vocab_vectors.test_lexeme.is_len4->en_vocab.add_flag(lambda string: len(string) == 4, flag_id=IS_DIGIT)
spacy.tests.vocab_vectors.test_lexeme.test_lexeme_bytes_roundtrip(en_vocab)
spacy.tests.vocab_vectors.test_lexeme.test_vocab_lexeme_add_flag_auto_id(en_vocab)
spacy.tests.vocab_vectors.test_lexeme.test_vocab_lexeme_add_flag_provided_id(en_vocab)
spacy.tests.vocab_vectors.test_lexeme.test_vocab_lexeme_hash(en_vocab,text1,text2)
spacy.tests.vocab_vectors.test_lexeme.test_vocab_lexeme_is_alpha(en_vocab)
spacy.tests.vocab_vectors.test_lexeme.test_vocab_lexeme_is_digit(en_vocab)
spacy.tests.vocab_vectors.test_lexeme.test_vocab_lexeme_lt(en_vocab,text1,text2,prob1,prob2)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/vocab_vectors/test_stringstore.py----------------------------------------
A:spacy.tests.vocab_vectors.test_stringstore.h->stringstore.add(heart)
A:spacy.tests.vocab_vectors.test_stringstore.apple_hash->stringstore.add('apple')
A:spacy.tests.vocab_vectors.test_stringstore.banana_hash->stringstore.add('banana')
A:spacy.tests.vocab_vectors.test_stringstore.key->stringstore.add(text)
A:spacy.tests.vocab_vectors.test_stringstore.store->stringstore.add(text)
A:spacy.tests.vocab_vectors.test_stringstore.serialized->stringstore.to_bytes()
A:spacy.tests.vocab_vectors.test_stringstore.new_stringstore->StringStore().from_bytes(serialized)
spacy.tests.vocab_vectors.test_stringstore.stringstore()
spacy.tests.vocab_vectors.test_stringstore.test_string_hash(stringstore)
spacy.tests.vocab_vectors.test_stringstore.test_stringstore_freeze_oov(stringstore,text)
spacy.tests.vocab_vectors.test_stringstore.test_stringstore_from_api_docs(stringstore)
spacy.tests.vocab_vectors.test_stringstore.test_stringstore_long_string(stringstore)
spacy.tests.vocab_vectors.test_stringstore.test_stringstore_massive_strings(stringstore)
spacy.tests.vocab_vectors.test_stringstore.test_stringstore_med_string(stringstore,text1,text2)
spacy.tests.vocab_vectors.test_stringstore.test_stringstore_multiply(stringstore,factor)
spacy.tests.vocab_vectors.test_stringstore.test_stringstore_retrieve_id(stringstore,text)
spacy.tests.vocab_vectors.test_stringstore.test_stringstore_save_bytes(stringstore,text1,text2,text3)
spacy.tests.vocab_vectors.test_stringstore.test_stringstore_save_unicode(stringstore,text1,text2,text3)
spacy.tests.vocab_vectors.test_stringstore.test_stringstore_to_bytes(stringstore,text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/vocab_vectors/test_vocab_api.py----------------------------------------
spacy.tests.vocab_vectors.test_vocab_api.test_vocab_api_contains(en_vocab,text)
spacy.tests.vocab_vectors.test_vocab_api.test_vocab_api_eq(en_vocab,text)
spacy.tests.vocab_vectors.test_vocab_api.test_vocab_api_neq(en_vocab,text1,text2)
spacy.tests.vocab_vectors.test_vocab_api.test_vocab_api_shape_attr(en_vocab,text)
spacy.tests.vocab_vectors.test_vocab_api.test_vocab_api_symbols(en_vocab,string,symbol)
spacy.tests.vocab_vectors.test_vocab_api.test_vocab_writing_system(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/vocab_vectors/test_similarity.py----------------------------------------
A:spacy.tests.vocab_vectors.test_similarity.doc->Doc(vocab, words=[word1, word2])
spacy.tests.vocab_vectors.test_similarity.test_vectors_similarity_DS(vocab,vectors)
spacy.tests.vocab_vectors.test_similarity.test_vectors_similarity_LL(vocab,vectors)
spacy.tests.vocab_vectors.test_similarity.test_vectors_similarity_TD(vocab,vectors)
spacy.tests.vocab_vectors.test_similarity.test_vectors_similarity_TS(vocab,vectors)
spacy.tests.vocab_vectors.test_similarity.test_vectors_similarity_TT(vocab,vectors)
spacy.tests.vocab_vectors.test_similarity.vectors()
spacy.tests.vocab_vectors.test_similarity.vocab(en_vocab,vectors)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/doc/test_underscore.py----------------------------------------
A:spacy.tests.doc.test_underscore.doc->Doc(en_vocab, words=['hello', 'world'])
A:spacy.tests.doc.test_underscore.uscore->Underscore(Underscore.span_extensions, span, start=span.start, end=span.end)
A:spacy.tests.doc.test_underscore.doc._->Underscore(Underscore.doc_extensions, doc)
A:spacy.tests.doc.test_underscore.span->Mock(doc=Mock(), start=0, end=2)
A:spacy.tests.doc.test_underscore.span._->Underscore(Underscore.span_extensions, span, start=span.start, end=span.end)
A:spacy.tests.doc.test_underscore.token->Mock(doc=Mock(), idx=7, say_cheese=lambda token: 'cheese')
A:spacy.tests.doc.test_underscore.token._->Underscore(Underscore.token_extensions, token, start=token.idx)
A:spacy.tests.doc.test_underscore.doc1->Doc(en_vocab, words=['one'])
A:spacy.tests.doc.test_underscore.doc2->Doc(en_vocab, words=['two'])
spacy.tests.doc.test_underscore.test_create_doc_underscore()
spacy.tests.doc.test_underscore.test_create_span_underscore()
spacy.tests.doc.test_underscore.test_doc_underscore_getattr_setattr()
spacy.tests.doc.test_underscore.test_doc_underscore_remove_extension(obj)
spacy.tests.doc.test_underscore.test_span_underscore_getter_setter()
spacy.tests.doc.test_underscore.test_token_underscore_method()
spacy.tests.doc.test_underscore.test_underscore_accepts_valid(valid_kwargs)
spacy.tests.doc.test_underscore.test_underscore_dir(en_vocab)
spacy.tests.doc.test_underscore.test_underscore_docstring(en_vocab)
spacy.tests.doc.test_underscore.test_underscore_mutable_defaults_dict(en_vocab)
spacy.tests.doc.test_underscore.test_underscore_mutable_defaults_list(en_vocab)
spacy.tests.doc.test_underscore.test_underscore_raises_for_dup(obj)
spacy.tests.doc.test_underscore.test_underscore_raises_for_invalid(invalid_kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/doc/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/doc/test_retokenize_merge.py----------------------------------------
A:spacy.tests.doc.test_retokenize_merge.doc->Doc(en_vocab, words=['hello', 'world', '!'])
A:spacy.tests.doc.test_retokenize_merge.new_doc->Doc(doc.vocab, words=['beach boys'])
A:spacy.tests.doc.test_retokenize_merge.tokens->en_tokenizer(text)
A:spacy.tests.doc.test_retokenize_merge.ent_type->max((w.ent_type_ for w in ent))
A:spacy.tests.doc.test_retokenize_merge.(sent1, sent2)->list(doc.sents)
A:spacy.tests.doc.test_retokenize_merge.init_len->len(list(sent1.root.subtree))
A:spacy.tests.doc.test_retokenize_merge.init_len2->len(sent2)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_lex_attrs(en_tokenizer)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_merge(en_tokenizer)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_merge_children(en_tokenizer)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_merge_extension_attrs(en_vocab)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_merge_extension_attrs_invalid(en_vocab,underscore_attrs)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_merge_hang(en_tokenizer)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_retokenizer(en_tokenizer)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_retokenizer_attrs(en_tokenizer)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_span_np_merges(en_tokenizer)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_spans_entity_merge(en_tokenizer)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_spans_entity_merge_iob(en_vocab)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_spans_merge_heads(en_tokenizer)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_spans_merge_non_disjoint(en_tokenizer)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_spans_merge_tokens(en_tokenizer)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_spans_merge_tokens_default_attrs(en_tokenizer)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_spans_sentence_update_after_merge(en_tokenizer)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_spans_subtree_size_check(en_tokenizer)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenizer_merge_lex_attrs(en_vocab)
spacy.tests.doc.test_retokenize_merge.test_retokenize_skip_duplicates(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/doc/test_pickle_doc.py----------------------------------------
A:spacy.tests.doc.test_pickle_doc.nlp->Language()
A:spacy.tests.doc.test_pickle_doc.doc->nlp('Hello')
A:spacy.tests.doc.test_pickle_doc.data->spacy.compat.pickle.dumps(doc, 1)
A:spacy.tests.doc.test_pickle_doc.doc2->spacy.compat.pickle.loads(b)
A:spacy.tests.doc.test_pickle_doc.one_pickled->spacy.compat.pickle.dumps(nlp('0'), -1)
A:spacy.tests.doc.test_pickle_doc.docs->list(nlp.pipe((unicode_(i) for i in range(100))))
A:spacy.tests.doc.test_pickle_doc.many_pickled->spacy.compat.pickle.dumps(docs, -1)
A:spacy.tests.doc.test_pickle_doc.many_unpickled->spacy.compat.pickle.loads(many_pickled)
A:spacy.tests.doc.test_pickle_doc.b->spacy.compat.pickle.dumps(doc)
spacy.tests.doc.test_pickle_doc.test_hooks_unpickle()
spacy.tests.doc.test_pickle_doc.test_list_of_docs_pickles_efficiently()
spacy.tests.doc.test_pickle_doc.test_pickle_single_doc()
spacy.tests.doc.test_pickle_doc.test_user_data_from_disk()
spacy.tests.doc.test_pickle_doc.test_user_data_unpickles()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/doc/test_doc_api.py----------------------------------------
A:spacy.tests.doc.test_doc_api.doc->Doc(en_vocab, words=['Hello', 'world'])
A:spacy.tests.doc.test_doc_api.tokens->en_tokenizer(sentence)
A:spacy.tests.doc.test_doc_api.new_tokens->Doc(tokens.vocab).from_bytes(tokens.to_bytes(exclude=['sentiment']), exclude=['sentiment'])
A:spacy.tests.doc.test_doc_api.sents->list(doc.sents)
A:spacy.tests.doc.test_doc_api.vocab->Vocab()
A:spacy.tests.doc.test_doc_api.doc2->Doc(doc.vocab, words=['a', 'b', 'c'])
A:spacy.tests.doc.test_doc_api.lca->Doc(en_vocab, words=['Hello', 'world']).get_lca_matrix()
A:spacy.tests.doc.test_doc_api.arr->numpy.array([[0, 0], [0, 0], [0, 0], [384, 3], [384, 1]], dtype='uint64')
A:spacy.tests.doc.test_doc_api.new_doc->Doc(en_vocab).from_bytes(doc.to_bytes())
spacy.tests.doc.test_doc_api.test_doc_api_compare_by_string_position(en_vocab,text)
spacy.tests.doc.test_doc_api.test_doc_api_getitem(en_tokenizer)
spacy.tests.doc.test_doc_api.test_doc_api_has_vector()
spacy.tests.doc.test_doc_api.test_doc_api_right_edge(en_tokenizer)
spacy.tests.doc.test_doc_api.test_doc_api_runtime_error(en_tokenizer)
spacy.tests.doc.test_doc_api.test_doc_api_sents_empty_string(en_tokenizer)
spacy.tests.doc.test_doc_api.test_doc_api_serialize(en_tokenizer,text)
spacy.tests.doc.test_doc_api.test_doc_api_set_ents(en_tokenizer)
spacy.tests.doc.test_doc_api.test_doc_api_similarity_match()
spacy.tests.doc.test_doc_api.test_doc_is_nered(en_vocab)
spacy.tests.doc.test_doc_api.test_doc_lang(en_vocab)
spacy.tests.doc.test_doc_api.test_lowest_common_ancestor(en_tokenizer,sentence,heads,lca_matrix)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/doc/test_creation.py----------------------------------------
A:spacy.tests.doc.test_creation.lookups->Lookups()
A:spacy.tests.doc.test_creation.doc->Doc(vocab, words=['dogs', 'dogses'])
spacy.tests.doc.test_creation.lemmatizer()
spacy.tests.doc.test_creation.test_empty_doc(vocab)
spacy.tests.doc.test_creation.test_lookup_lemmatization(vocab)
spacy.tests.doc.test_creation.test_single_word(vocab)
spacy.tests.doc.test_creation.vocab(lemmatizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/doc/test_span.py----------------------------------------
A:spacy.tests.doc.test_span.tokens->en_tokenizer(text)
A:spacy.tests.doc.test_span.doc->Doc(tokens.vocab, words=[t.text for t in tokens])
A:spacy.tests.doc.test_span.sents->list(doc.sents)
A:spacy.tests.doc.test_span.lca->doc[2:].get_lca_matrix()
A:spacy.tests.doc.test_span.span2->Doc(tokens.vocab, words=[t.text for t in tokens]).char_span(span1.start_char, span1.end_char, label='GPE')
A:spacy.tests.doc.test_span.arr->Span(doc, 0, 1).to_array([ORTH, LENGTH])
A:spacy.tests.doc.test_span.span_doc->Span(doc, 0, 1).as_doc()
A:spacy.tests.doc.test_span.span_doc_with->Span(doc, 0, 1).as_doc(copy_user_data=True)
A:spacy.tests.doc.test_span.span_doc_without->Span(doc, 0, 1).as_doc()
A:spacy.tests.doc.test_span.span->Span(doc, 0, 1)
A:spacy.tests.doc.test_span.sentences->list(doc.sents)
A:spacy.tests.doc.test_span.filtered->filter_spans(spans)
spacy.tests.doc.test_span.doc(en_tokenizer)
spacy.tests.doc.test_span.doc_not_parsed(en_tokenizer)
spacy.tests.doc.test_span.test_filter_spans(doc)
spacy.tests.doc.test_span.test_span_as_doc(doc)
spacy.tests.doc.test_span.test_span_as_doc_user_data(doc)
spacy.tests.doc.test_span.test_span_ents_property(doc)
spacy.tests.doc.test_span.test_span_kb_id_readonly(doc)
spacy.tests.doc.test_span.test_span_label_readonly(doc)
spacy.tests.doc.test_span.test_span_similarity_match()
spacy.tests.doc.test_span.test_span_string_label_kb_id(doc)
spacy.tests.doc.test_span.test_span_to_array(doc)
spacy.tests.doc.test_span.test_spans_are_hashable(en_tokenizer)
spacy.tests.doc.test_span.test_spans_by_character(doc)
spacy.tests.doc.test_span.test_spans_default_sentiment(en_tokenizer)
spacy.tests.doc.test_span.test_spans_lca_matrix(en_tokenizer)
spacy.tests.doc.test_span.test_spans_override_sentiment(en_tokenizer)
spacy.tests.doc.test_span.test_spans_root(doc)
spacy.tests.doc.test_span.test_spans_root2(en_tokenizer)
spacy.tests.doc.test_span.test_spans_sent_spans(doc)
spacy.tests.doc.test_span.test_spans_span_sent(doc,doc_not_parsed)
spacy.tests.doc.test_span.test_spans_string_fn(doc)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/doc/test_array.py----------------------------------------
A:spacy.tests.doc.test_array.doc->Doc(en_vocab, words=words)
A:spacy.tests.doc.test_array.feats_array->Doc(en_vocab, words=words).to_array((ORTH, DEP))
A:spacy.tests.doc.test_array.feats_array_stringy->Doc(en_vocab, words=words).to_array(('ORTH', 'SHAPE'))
spacy.tests.doc.test_array.test_doc_array_attr_of_token(en_vocab)
spacy.tests.doc.test_array.test_doc_array_dep(en_vocab)
spacy.tests.doc.test_array.test_doc_array_tag(en_vocab)
spacy.tests.doc.test_array.test_doc_array_to_from_string_attrs(en_vocab,attrs)
spacy.tests.doc.test_array.test_doc_scalar_attr_of_token(en_vocab)
spacy.tests.doc.test_array.test_doc_stringy_array_attr_of_token(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/doc/test_to_json.py----------------------------------------
A:spacy.tests.doc.test_to_json.json_doc->doc.to_json()
A:spacy.tests.doc.test_to_json.validator->get_json_validator(TRAINING_SCHEMA)
A:spacy.tests.doc.test_to_json.errors->validate_json([json_doc], validator)
spacy.tests.doc.test_to_json.doc(en_vocab)
spacy.tests.doc.test_to_json.test_doc_to_json(doc)
spacy.tests.doc.test_to_json.test_doc_to_json_underscore(doc)
spacy.tests.doc.test_to_json.test_doc_to_json_underscore_error_attr(doc)
spacy.tests.doc.test_to_json.test_doc_to_json_underscore_error_serialize(doc)
spacy.tests.doc.test_to_json.test_doc_to_json_valid_training(doc)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/doc/test_retokenize_split.py----------------------------------------
A:spacy.tests.doc.test_retokenize_split.doc->Doc(en_vocab, words=['LosAngeles', 'start'])
A:spacy.tests.doc.test_retokenize_split.dep1->Doc(en_vocab, words=['LosAngeles', 'start']).vocab.strings.add('amod')
A:spacy.tests.doc.test_retokenize_split.dep2->Doc(en_vocab, words=['LosAngeles', 'start']).vocab.strings.add('subject')
A:spacy.tests.doc.test_retokenize_split.(sent1, sent2)->list(doc.sents)
A:spacy.tests.doc.test_retokenize_split.init_len->len(sent1)
A:spacy.tests.doc.test_retokenize_split.init_len2->len(sent2)
spacy.tests.doc.test_retokenize_split.test_doc_retokenize_spans_entity_split_iob()
spacy.tests.doc.test_retokenize_split.test_doc_retokenize_spans_sentence_update_after_split(en_vocab)
spacy.tests.doc.test_retokenize_split.test_doc_retokenize_split(en_vocab)
spacy.tests.doc.test_retokenize_split.test_doc_retokenize_split_dependencies(en_vocab)
spacy.tests.doc.test_retokenize_split.test_doc_retokenize_split_extension_attrs(en_vocab)
spacy.tests.doc.test_retokenize_split.test_doc_retokenize_split_extension_attrs_invalid(en_vocab,underscore_attrs)
spacy.tests.doc.test_retokenize_split.test_doc_retokenize_split_heads_error(en_vocab)
spacy.tests.doc.test_retokenize_split.test_doc_retokenize_split_orths_mismatch(en_vocab)
spacy.tests.doc.test_retokenize_split.test_doc_retokenizer_split_lex_attrs(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/doc/test_add_entities.py----------------------------------------
A:spacy.tests.doc.test_add_entities.doc->get_doc(en_vocab, text)
A:spacy.tests.doc.test_add_entities.ner->EntityRecognizer(en_vocab)
A:spacy.tests.doc.test_add_entities.doc.ents->list(doc.ents)
A:spacy.tests.doc.test_add_entities.entity->Span(doc, 0, 4, label=391)
A:spacy.tests.doc.test_add_entities.new_entity->Span(doc, 0, 1, label=392)
spacy.tests.doc.test_add_entities.test_add_overlapping_entities(en_vocab)
spacy.tests.doc.test_add_entities.test_doc_add_entities_set_ents_iob(en_vocab)
spacy.tests.doc.test_add_entities.test_ents_reset(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/doc/test_morphanalysis.py----------------------------------------
A:spacy.tests.doc.test_morphanalysis.doc->en_tokenizer('I has')
spacy.tests.doc.test_morphanalysis.i_has(en_tokenizer)
spacy.tests.doc.test_morphanalysis.test_morph_get(i_has)
spacy.tests.doc.test_morphanalysis.test_morph_iter(i_has)
spacy.tests.doc.test_morphanalysis.test_morph_props(i_has)
spacy.tests.doc.test_morphanalysis.test_token_morph_id(i_has)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/doc/test_token_api.py----------------------------------------
A:spacy.tests.doc.test_token_api.tokens->en_tokenizer(text)
A:spacy.tests.doc.test_token_api.doc->get_doc(en_vocab, words=words, heads=heads, deps=deps)
A:spacy.tests.doc.test_token_api.vocab->Vocab()
A:spacy.tests.doc.test_token_api.words->'They came .'.split()
spacy.tests.doc.test_token_api.doc(en_tokenizer)
spacy.tests.doc.test_token_api.test_doc_token_api_ancestors(en_tokenizer)
spacy.tests.doc.test_token_api.test_doc_token_api_flags(en_tokenizer)
spacy.tests.doc.test_token_api.test_doc_token_api_head_setter(en_tokenizer)
spacy.tests.doc.test_token_api.test_doc_token_api_is_properties(en_vocab)
spacy.tests.doc.test_token_api.test_doc_token_api_prob_inherited_from_vocab(en_tokenizer,text)
spacy.tests.doc.test_token_api.test_doc_token_api_str_builtin(en_tokenizer,text)
spacy.tests.doc.test_token_api.test_doc_token_api_strings(en_tokenizer)
spacy.tests.doc.test_token_api.test_doc_token_api_vectors()
spacy.tests.doc.test_token_api.test_is_sent_start(en_tokenizer)
spacy.tests.doc.test_token_api.test_set_pos()
spacy.tests.doc.test_token_api.test_token0_has_sent_start_true()
spacy.tests.doc.test_token_api.test_token_api_conjuncts_chain(en_vocab)
spacy.tests.doc.test_token_api.test_token_api_conjuncts_simple(en_vocab)
spacy.tests.doc.test_token_api.test_token_api_non_conjuncts(en_vocab)
spacy.tests.doc.test_token_api.test_tokens_sent(doc)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/matcher/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/matcher/test_pattern_validation.py----------------------------------------
A:spacy.tests.matcher.test_pattern_validation.matcher->Matcher(en_vocab)
A:spacy.tests.matcher.test_pattern_validation.errors->validate_json(pattern, validator)
spacy.tests.matcher.test_pattern_validation.test_matcher_pattern_validation(en_vocab,pattern)
spacy.tests.matcher.test_pattern_validation.test_minimal_pattern_validation(en_vocab,pattern,n_errors,n_min_errors)
spacy.tests.matcher.test_pattern_validation.test_pattern_validation(validator,pattern,n_errors,_)
spacy.tests.matcher.test_pattern_validation.test_xfail_pattern_validation(validator,pattern,n_errors,_)
spacy.tests.matcher.test_pattern_validation.validator()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/matcher/test_matcher_api.py----------------------------------------
A:spacy.tests.matcher.test_matcher_api.matcher->Matcher(en_vocab)
A:spacy.tests.matcher.test_matcher_api.(on_match, patterns)->Matcher(en_vocab).get('Rule')
A:spacy.tests.matcher.test_matcher_api.doc->Doc(en_vocab, words=text.split(' '))
A:spacy.tests.matcher.test_matcher_api.matches->matcher(doc)
A:spacy.tests.matcher.test_matcher_api.words1->'He said , " some words " ...'.split()
A:spacy.tests.matcher.test_matcher_api.words2->'He said , " some three words " ...'.split()
A:spacy.tests.matcher.test_matcher_api.words->'He said , " some words " ...'.split()
A:spacy.tests.matcher.test_matcher_api.control->Matcher(matcher.vocab)
A:spacy.tests.matcher.test_matcher_api.m->matcher(doc)
A:spacy.tests.matcher.test_matcher_api.IS_BROWN_YELLOW->en_vocab.add_flag(is_brown_yellow)
A:spacy.tests.matcher.test_matcher_api.doc1->Doc(en_vocab, words=['Test'])
A:spacy.tests.matcher.test_matcher_api.doc2->Doc(en_vocab, words=['Test'])
A:spacy.tests.matcher.test_matcher_api.doc3->Doc(en_vocab, words=['Test'])
spacy.tests.matcher.test_matcher_api.dependency_matcher(en_vocab)
spacy.tests.matcher.test_matcher_api.deps()
spacy.tests.matcher.test_matcher_api.heads()
spacy.tests.matcher.test_matcher_api.matcher(en_vocab)
spacy.tests.matcher.test_matcher_api.test_attr_pipeline_checks(en_vocab)
spacy.tests.matcher.test_matcher_api.test_dependency_matcher_compile(dependency_matcher)
spacy.tests.matcher.test_matcher_api.test_matcher_any_token_operator(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_compare_length(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_empty_dict(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_extension_attribute(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_extension_set_membership(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_from_api_docs(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_from_usage_docs(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_len_contains(matcher)
spacy.tests.matcher.test_matcher_api.test_matcher_match_end(matcher)
spacy.tests.matcher.test_matcher_api.test_matcher_match_middle(matcher)
spacy.tests.matcher.test_matcher_api.test_matcher_match_multi(matcher)
spacy.tests.matcher.test_matcher_api.test_matcher_match_one_plus(matcher)
spacy.tests.matcher.test_matcher_api.test_matcher_match_start(matcher)
spacy.tests.matcher.test_matcher_api.test_matcher_match_zero(matcher)
spacy.tests.matcher.test_matcher_api.test_matcher_match_zero_plus(matcher)
spacy.tests.matcher.test_matcher_api.test_matcher_no_match(matcher)
spacy.tests.matcher.test_matcher_api.test_matcher_operator_shadow(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_regex(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_regex_shape(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_schema_token_attributes(en_vocab,pattern,text)
spacy.tests.matcher.test_matcher_api.test_matcher_set_value(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_set_value_operator(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_valid_callback(en_vocab)
spacy.tests.matcher.test_matcher_api.text()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/matcher/test_matcher_logic.py----------------------------------------
A:spacy.tests.matcher.test_matcher_logic.doc->Doc(en_vocab, words='zero one two three'.split())
A:spacy.tests.matcher.test_matcher_logic.matcher->Matcher(en_vocab)
A:spacy.tests.matcher.test_matcher_logic.matches->matcher(doc)
spacy.tests.matcher.test_matcher_logic.doc(en_tokenizer,text)
spacy.tests.matcher.test_matcher_logic.test_greedy_matching(doc,text,pattern,re_pattern)
spacy.tests.matcher.test_matcher_logic.test_match_consuming(doc,text,pattern,re_pattern)
spacy.tests.matcher.test_matcher_logic.test_matcher_end_zero_plus(en_vocab)
spacy.tests.matcher.test_matcher_logic.test_matcher_sets_return_correct_tokens(en_vocab)
spacy.tests.matcher.test_matcher_logic.test_operator_combos(en_vocab)
spacy.tests.matcher.test_matcher_logic.text()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/matcher/test_phrase_matcher.py----------------------------------------
A:spacy.tests.matcher.test_phrase_matcher.doc->Doc(en_vocab, words=words2)
A:spacy.tests.matcher.test_phrase_matcher.pattern->Doc(en_vocab, words=words1)
A:spacy.tests.matcher.test_phrase_matcher.matcher->PhraseMatcher(en_vocab, attr='TEXT')
A:spacy.tests.matcher.test_phrase_matcher.matches->matcher(doc)
A:spacy.tests.matcher.test_phrase_matcher.doc1->Doc(en_vocab, words=['Test'])
A:spacy.tests.matcher.test_phrase_matcher.doc2->Doc(en_vocab, words=['Test'])
A:spacy.tests.matcher.test_phrase_matcher.doc3->Doc(en_vocab, words=['Test'])
spacy.tests.matcher.test_phrase_matcher.test_attr_pipeline_checks(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_attr_validation(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_matcher_phrase_matcher(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_phrase_matcher_bool_attrs(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_phrase_matcher_contains(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_phrase_matcher_length(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_phrase_matcher_overlapping_with_remove(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_phrase_matcher_remove(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_phrase_matcher_repeated_add(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_phrase_matcher_string_attrs(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_phrase_matcher_string_attrs_negative(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_phrase_matcher_validation(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue4042.py----------------------------------------
A:spacy.tests.regression.test_issue4042.nlp->English()
A:spacy.tests.regression.test_issue4042.ner->English().create_pipe('ner')
A:spacy.tests.regression.test_issue4042.ruler->EntityRuler(nlp)
A:spacy.tests.regression.test_issue4042.doc1->nlp1('What do you think about Apple ?')
A:spacy.tests.regression.test_issue4042.output_dir->ensure_path(d)
A:spacy.tests.regression.test_issue4042.nlp2->English(vocab)
A:spacy.tests.regression.test_issue4042.doc2->nlp2('What do you think about Apple ?')
A:spacy.tests.regression.test_issue4042.nlp1->English()
A:spacy.tests.regression.test_issue4042.ner1->English().create_pipe('ner')
A:spacy.tests.regression.test_issue4042.apple_ent->Span(doc1, 5, 6, label='MY_ORG')
A:spacy.tests.regression.test_issue4042.ner2->EntityRecognizer(vocab)
spacy.tests.regression.test_issue4042.test_issue4042()
spacy.tests.regression.test_issue4042.test_issue4042_bug2()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue3625.py----------------------------------------
A:spacy.tests.regression.test_issue3625.nlp->Hindi()
A:spacy.tests.regression.test_issue3625.doc->nlp('hi. how à¤¹à¥à¤. à¤¹à¥‹à¤Ÿà¤², à¤¹à¥‹à¤Ÿà¤²')
spacy.tests.regression.test_issue3625.test_issue3625()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue3972.py----------------------------------------
A:spacy.tests.regression.test_issue3972.matcher->PhraseMatcher(en_vocab)
A:spacy.tests.regression.test_issue3972.doc->Doc(en_vocab, words=['I', 'live', 'in', 'New', 'York'])
A:spacy.tests.regression.test_issue3972.matches->matcher(doc)
spacy.tests.regression.test_issue3972.test_issue3972(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue1001-1500.py----------------------------------------
A:spacy.tests.regression.test_issue1001-1500.tokenizer->spacy.lang.en.English.Defaults.create_tokenizer()
A:spacy.tests.regression.test_issue1001-1500.doc->nlp('This is a test.')
A:spacy.tests.regression.test_issue1001-1500.nlp->English()
A:spacy.tests.regression.test_issue1001-1500.docs->list(nlp.pipe(['', 'hello']))
A:spacy.tests.regression.test_issue1001-1500.doc1->Doc(Vocab(), words=['a', 'b', 'c'])
A:spacy.tests.regression.test_issue1001-1500.doc2->Doc(Vocab(), words=['a', 'c', 'e'])
A:spacy.tests.regression.test_issue1001-1500.lookups->Lookups()
A:spacy.tests.regression.test_issue1001-1500.lemmatizer->Lemmatizer(lookups)
A:spacy.tests.regression.test_issue1001-1500.vocab->Vocab(lex_attr_getters=LEX_ATTRS)
A:spacy.tests.regression.test_issue1001-1500.hello_world->Doc(vocab, words=['Hello', 'World'])
A:spacy.tests.regression.test_issue1001-1500.hello->Doc(vocab, words=['Hello'])
A:spacy.tests.regression.test_issue1001-1500.matcher->Matcher(Vocab())
A:spacy.tests.regression.test_issue1001-1500.matches->matcher(doc)
A:spacy.tests.regression.test_issue1001-1500.prefix_re->re.compile('[\\[\\("\']')
A:spacy.tests.regression.test_issue1001-1500.suffix_re->re.compile('[\\]\\)"\']')
A:spacy.tests.regression.test_issue1001-1500.infix_re->re.compile('[^a-z]')
A:spacy.tests.regression.test_issue1001-1500.simple_url_re->re.compile('^https?://')
A:spacy.tests.regression.test_issue1001-1500.nlp.tokenizer->new_tokenizer(nlp)
spacy.tests.regression.test_issue1001-1500.test_issue1061()
spacy.tests.regression.test_issue1001-1500.test_issue1235()
spacy.tests.regression.test_issue1001-1500.test_issue1242()
spacy.tests.regression.test_issue1001-1500.test_issue1250()
spacy.tests.regression.test_issue1001-1500.test_issue1257()
spacy.tests.regression.test_issue1001-1500.test_issue1375()
spacy.tests.regression.test_issue1001-1500.test_issue1387()
spacy.tests.regression.test_issue1001-1500.test_issue1434()
spacy.tests.regression.test_issue1001-1500.test_issue1450(string,start,end)
spacy.tests.regression.test_issue1001-1500.test_issue1488()
spacy.tests.regression.test_issue1001-1500.test_issue1494()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue3611.py----------------------------------------
A:spacy.tests.regression.test_issue3611.pos_cats->list()
A:spacy.tests.regression.test_issue3611.train_data->list(zip(x_train, [{'cats': cats} for cats in pos_cats]))
A:spacy.tests.regression.test_issue3611.nlp->spacy.blank('en')
A:spacy.tests.regression.test_issue3611.textcat->spacy.blank('en').create_pipe('textcat', config={'exclusive_classes': True, 'architecture': 'bow', 'ngram_size': 2})
A:spacy.tests.regression.test_issue3611.optimizer->spacy.blank('en').begin_training()
A:spacy.tests.regression.test_issue3611.batches->minibatch(train_data, size=compounding(4.0, 32.0, 1.001))
A:spacy.tests.regression.test_issue3611.(texts, annotations)->zip(*batch)
spacy.tests.regression.test_issue3611.test_issue3611()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue4278.py----------------------------------------
A:spacy.tests.regression.test_issue4278.doc->nlp.make_doc('foo')
A:spacy.tests.regression.test_issue4278.dummy_pipe->DummyPipe()
spacy.tests.regression.test_issue4278.DummyPipe(self)
spacy.tests.regression.test_issue4278.DummyPipe.__init__(self)
spacy.tests.regression.test_issue4278.DummyPipe.predict(self,docs)
spacy.tests.regression.test_issue4278.DummyPipe.set_annotations(self,docs,scores,tensors=None)
spacy.tests.regression.test_issue4278.nlp()
spacy.tests.regression.test_issue4278.test_multiple_predictions(nlp)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue3001-3500.py----------------------------------------
A:spacy.tests.regression.test_issue3001-3500.nlp->English()
A:spacy.tests.regression.test_issue3001-3500.doc->nlp('Hello world')
A:spacy.tests.regression.test_issue3001-3500.matcher->Matcher(nlp.vocab)
A:spacy.tests.regression.test_issue3001-3500.matches->matcher(doc)
A:spacy.tests.regression.test_issue3001-3500.ent_array->nlp('Hello world').to_array(header)
A:spacy.tests.regression.test_issue3001-3500.doc_bytes->nlp('Hello world').to_bytes()
A:spacy.tests.regression.test_issue3001-3500.doc2->Doc(en_vocab).from_bytes(doc_bytes)
A:spacy.tests.regression.test_issue3001-3500.ner->EntityRecognizer(doc.vocab)
A:spacy.tests.regression.test_issue3001-3500.nlp2->English()
A:spacy.tests.regression.test_issue3001-3500.data->spacy.compat.pickle.dumps(matcher)
A:spacy.tests.regression.test_issue3001-3500.new_matcher->spacy.compat.pickle.loads(data)
A:spacy.tests.regression.test_issue3001-3500.doc.tensor->numpy.zeros((len(words), 96), dtype='float32')
A:spacy.tests.regression.test_issue3001-3500.bytes_data->English().to_bytes()
A:spacy.tests.regression.test_issue3001-3500.new_nlp->English()
A:spacy.tests.regression.test_issue3001-3500.ruler->EntityRuler(nlp, patterns=[{'label': 'GPE', 'pattern': 'New York'}])
A:spacy.tests.regression.test_issue3001-3500.pattern->re.compile(unescape_unicode(prefix_search.decode('utf8')))
A:spacy.tests.regression.test_issue3001-3500.phrasematcher->PhraseMatcher(nlp.vocab)
A:spacy.tests.regression.test_issue3001-3500.docs->list(nlp.tokenizer.pipe(texts, n_threads=4))
A:spacy.tests.regression.test_issue3001-3500.sizes->decaying(10.0, 1.0, 0.5)
A:spacy.tests.regression.test_issue3001-3500.size->next(sizes)
A:spacy.tests.regression.test_issue3001-3500.t1->nlp(text1)
A:spacy.tests.regression.test_issue3001-3500.t2->nlp(text2)
A:spacy.tests.regression.test_issue3001-3500.t3->nlp(text3)
A:spacy.tests.regression.test_issue3001-3500.new_doc->Doc(nlp.vocab).from_bytes(doc_bytes)
spacy.tests.regression.test_issue3001-3500.test_issue3002()
spacy.tests.regression.test_issue3001-3500.test_issue3009(en_vocab)
spacy.tests.regression.test_issue3001-3500.test_issue3012(en_vocab)
spacy.tests.regression.test_issue3001-3500.test_issue3199()
spacy.tests.regression.test_issue3001-3500.test_issue3209()
spacy.tests.regression.test_issue3001-3500.test_issue3248_1()
spacy.tests.regression.test_issue3001-3500.test_issue3248_2()
spacy.tests.regression.test_issue3001-3500.test_issue3277(es_tokenizer)
spacy.tests.regression.test_issue3001-3500.test_issue3288(en_vocab)
spacy.tests.regression.test_issue3001-3500.test_issue3289()
spacy.tests.regression.test_issue3001-3500.test_issue3328(en_vocab)
spacy.tests.regression.test_issue3001-3500.test_issue3331(en_vocab)
spacy.tests.regression.test_issue3001-3500.test_issue3345()
spacy.tests.regression.test_issue3001-3500.test_issue3410()
spacy.tests.regression.test_issue3001-3500.test_issue3447()
spacy.tests.regression.test_issue3001-3500.test_issue3449()
spacy.tests.regression.test_issue3001-3500.test_issue3456()
spacy.tests.regression.test_issue3001-3500.test_issue3468()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue2501-3000.py----------------------------------------
A:spacy.tests.regression.test_issue2501-3000.nlp->Japanese()
A:spacy.tests.regression.test_issue2501-3000.tagger->Japanese().create_pipe('tagger')
A:spacy.tests.regression.test_issue2501-3000.doc->fr_tokenizer('Learn html5/css3/javascript/jquery')
A:spacy.tests.regression.test_issue2501-3000.docs->Japanese().pipe(['hello', 'world'])
A:spacy.tests.regression.test_issue2501-3000.piped_doc->next(docs)
A:spacy.tests.regression.test_issue2501-3000.matcher->Matcher(nlp.vocab)
A:spacy.tests.regression.test_issue2501-3000.matched->sorted(matched, key=len, reverse=True)
A:spacy.tests.regression.test_issue2501-3000.doc1->nlp('This is a high-adrenaline situation.')
A:spacy.tests.regression.test_issue2501-3000.doc2->nlp('This is a high adrenaline situation.')
A:spacy.tests.regression.test_issue2501-3000.matches1->matcher(doc1)
A:spacy.tests.regression.test_issue2501-3000.matches2->matcher(doc2)
A:spacy.tests.regression.test_issue2501-3000.html->spacy.displacy.render(doc, style='ent')
A:spacy.tests.regression.test_issue2501-3000.a->en_tokenizer('a')
A:spacy.tests.regression.test_issue2501-3000.am->en_tokenizer('am')
A:spacy.tests.regression.test_issue2501-3000.words->'When we write or communicate virtually , we can hide our true feelings .'.split()
A:spacy.tests.regression.test_issue2501-3000.ner->Japanese().create_pipe('ner')
A:spacy.tests.regression.test_issue2501-3000.optimizer->Japanese().begin_training()
A:spacy.tests.regression.test_issue2501-3000.vocab->Vocab(vectors_name='test_issue2871')
A:spacy.tests.regression.test_issue2501-3000.vector_data->numpy.zeros((3, 10), dtype='f')
spacy.tests.regression.test_issue2501-3000.test_issue2564()
spacy.tests.regression.test_issue2501-3000.test_issue2569(en_tokenizer)
spacy.tests.regression.test_issue2501-3000.test_issue2626_2835(en_tokenizer,text)
spacy.tests.regression.test_issue2501-3000.test_issue2656(en_tokenizer)
spacy.tests.regression.test_issue2501-3000.test_issue2671()
spacy.tests.regression.test_issue2501-3000.test_issue2728(en_vocab)
spacy.tests.regression.test_issue2501-3000.test_issue2754(en_tokenizer)
spacy.tests.regression.test_issue2501-3000.test_issue2772(en_vocab)
spacy.tests.regression.test_issue2501-3000.test_issue2782(text,lang_cls)
spacy.tests.regression.test_issue2501-3000.test_issue2800()
spacy.tests.regression.test_issue2501-3000.test_issue2822(it_tokenizer)
spacy.tests.regression.test_issue2501-3000.test_issue2833(en_vocab)
spacy.tests.regression.test_issue2501-3000.test_issue2871()
spacy.tests.regression.test_issue2501-3000.test_issue2901()
spacy.tests.regression.test_issue2501-3000.test_issue2926(fr_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue3526.py----------------------------------------
A:spacy.tests.regression.test_issue3526.nlp->Language(vocab=en_vocab)
A:spacy.tests.regression.test_issue3526.ruler->Language(vocab=en_vocab).get_pipe('entity_ruler')
A:spacy.tests.regression.test_issue3526.ruler_bytes->Language(vocab=en_vocab).get_pipe('entity_ruler').to_bytes()
A:spacy.tests.regression.test_issue3526.new_ruler->load(tmpdir).get_pipe('entity_ruler')
A:spacy.tests.regression.test_issue3526.bytes_old_style->srsly.msgpack_dumps(ruler.patterns)
A:spacy.tests.regression.test_issue3526.nlp2->load(tmpdir)
spacy.tests.regression.test_issue3526.add_ent()
spacy.tests.regression.test_issue3526.patterns()
spacy.tests.regression.test_issue3526.test_entity_ruler_existing_bytes_old_format_safe(patterns,en_vocab)
spacy.tests.regression.test_issue3526.test_entity_ruler_existing_overwrite_serialize_bytes(patterns,en_vocab)
spacy.tests.regression.test_issue3526.test_entity_ruler_from_disk_old_format_safe(patterns,en_vocab)
spacy.tests.regression.test_issue3526.test_entity_ruler_in_pipeline_from_issue(patterns,en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue3803.py----------------------------------------
A:spacy.tests.regression.test_issue3803.nlp->Spanish()
A:spacy.tests.regression.test_issue3803.doc->nlp(text)
spacy.tests.regression.test_issue3803.test_issue3803()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue1501-2000.py----------------------------------------
A:spacy.tests.regression.test_issue1501-2000.nlp->Language()
A:spacy.tests.regression.test_issue1501-2000.vectors->Vectors(data=data, keys=['I', 'am', 'Matt'])
A:spacy.tests.regression.test_issue1501-2000.doc->Doc(en_vocab, words=['this', 'is', 'text'])
A:spacy.tests.regression.test_issue1501-2000.sents->list(doc.sents)
A:spacy.tests.regression.test_issue1501-2000.sent0->sents[0].as_doc()
A:spacy.tests.regression.test_issue1501-2000.sent1->sents[1].as_doc()
A:spacy.tests.regression.test_issue1501-2000.v->Vectors(shape=(10, 10), keys=[5, 3, 98, 100])
A:spacy.tests.regression.test_issue1501-2000.nlp2->Language(Vocab())
A:spacy.tests.regression.test_issue1501-2000.data->numpy.ones((3, 300), dtype='f')
A:spacy.tests.regression.test_issue1501-2000.tagger->Tagger(Vocab()).from_disk(path)
A:spacy.tests.regression.test_issue1501-2000.tokens->en_tokenizer("would've")
A:spacy.tests.regression.test_issue1501-2000.heads_deps->numpy.asarray([[1, 397], [4, 436], [2, 426], [1, 402], [0, 8206900633647566924], [18446744073709551615, 440], [18446744073709551614, 442]], dtype='uint64')
A:spacy.tests.regression.test_issue1501-2000.vocab->Vocab()
A:spacy.tests.regression.test_issue1501-2000.new_doc->Doc(new_matcher.vocab, words=['hello'])
A:spacy.tests.regression.test_issue1501-2000.int_id->Vocab().strings.add('some string')
A:spacy.tests.regression.test_issue1501-2000.matcher->Matcher(en_vocab)
A:spacy.tests.regression.test_issue1501-2000.new_matcher->copy.deepcopy(matcher)
A:spacy.tests.regression.test_issue1501-2000.matches->matcher(doc)
A:spacy.tests.regression.test_issue1501-2000.doc.tensor->numpy.ones((len(doc), 128), dtype='f')
A:spacy.tests.regression.test_issue1501-2000.ner->EntityRecognizer(Vocab())
spacy.tests.regression.test_issue1501-2000.test_issue1506()
spacy.tests.regression.test_issue1501-2000.test_issue1518()
spacy.tests.regression.test_issue1501-2000.test_issue1537()
spacy.tests.regression.test_issue1501-2000.test_issue1539()
spacy.tests.regression.test_issue1501-2000.test_issue1547()
spacy.tests.regression.test_issue1501-2000.test_issue1612(en_tokenizer)
spacy.tests.regression.test_issue1501-2000.test_issue1654()
spacy.tests.regression.test_issue1501-2000.test_issue1698(en_tokenizer,text)
spacy.tests.regression.test_issue1501-2000.test_issue1727()
spacy.tests.regression.test_issue1501-2000.test_issue1757()
spacy.tests.regression.test_issue1501-2000.test_issue1758(en_tokenizer)
spacy.tests.regression.test_issue1501-2000.test_issue1773(en_tokenizer)
spacy.tests.regression.test_issue1501-2000.test_issue1799()
spacy.tests.regression.test_issue1501-2000.test_issue1807()
spacy.tests.regression.test_issue1501-2000.test_issue1834()
spacy.tests.regression.test_issue1501-2000.test_issue1868()
spacy.tests.regression.test_issue1501-2000.test_issue1883()
spacy.tests.regression.test_issue1501-2000.test_issue1889(word)
spacy.tests.regression.test_issue1501-2000.test_issue1915()
spacy.tests.regression.test_issue1501-2000.test_issue1945()
spacy.tests.regression.test_issue1501-2000.test_issue1963(en_tokenizer)
spacy.tests.regression.test_issue1501-2000.test_issue1967(label)
spacy.tests.regression.test_issue1501-2000.test_issue1971(en_vocab)
spacy.tests.regression.test_issue1501-2000.test_issue_1971_2(en_vocab)
spacy.tests.regression.test_issue1501-2000.test_issue_1971_3(en_vocab)
spacy.tests.regression.test_issue1501-2000.test_issue_1971_4(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue3555.py----------------------------------------
A:spacy.tests.regression.test_issue3555.matcher->Matcher(en_vocab)
A:spacy.tests.regression.test_issue3555.doc->Doc(en_vocab, words=['have', 'apple'])
spacy.tests.regression.test_issue3555.test_issue3555(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue4030.py----------------------------------------
A:spacy.tests.regression.test_issue4030.pos_cats->list()
A:spacy.tests.regression.test_issue4030.train_data->list(zip(x_train, [{'cats': cats} for cats in pos_cats]))
A:spacy.tests.regression.test_issue4030.nlp->spacy.blank('en')
A:spacy.tests.regression.test_issue4030.textcat->spacy.blank('en').create_pipe('textcat', config={'exclusive_classes': True, 'architecture': 'bow', 'ngram_size': 2})
A:spacy.tests.regression.test_issue4030.optimizer->spacy.blank('en').begin_training()
A:spacy.tests.regression.test_issue4030.batches->minibatch(train_data, size=compounding(4.0, 32.0, 1.001))
A:spacy.tests.regression.test_issue4030.(texts, annotations)->zip(*batch)
A:spacy.tests.regression.test_issue4030.doc->nlp('')
spacy.tests.regression.test_issue4030.test_issue4030()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue4313.py----------------------------------------
A:spacy.tests.regression.test_issue4313.nlp->English()
A:spacy.tests.regression.test_issue4313.ner->EntityRecognizer(nlp.vocab)
A:spacy.tests.regression.test_issue4313.doc->nlp('What do you think about Apple ?')
A:spacy.tests.regression.test_issue4313.apple_ent->Span(doc, 5, 6, label='MY_ORG')
A:spacy.tests.regression.test_issue4313.beams->English().entity.beam_parse(docs, beam_width=beam_width, beam_density=beam_density)
A:spacy.tests.regression.test_issue4313.entity_scores->defaultdict(float)
spacy.tests.regression.test_issue4313.test_issue4313()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue3882.py----------------------------------------
A:spacy.tests.regression.test_issue3882.doc->Doc(en_vocab, words=['Hello', 'world'])
A:spacy.tests.regression.test_issue3882.doc.user_data['test']->set()
spacy.tests.regression.test_issue3882.test_issue3882(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue3879.py----------------------------------------
A:spacy.tests.regression.test_issue3879.doc->Doc(en_vocab, words=['This', 'is', 'a', 'test', '.'])
A:spacy.tests.regression.test_issue3879.matcher->Matcher(en_vocab)
spacy.tests.regression.test_issue3879.test_issue3879(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue3830.py----------------------------------------
A:spacy.tests.regression.test_issue3830.parser->DependencyParser(Vocab(), learn_tokens=True)
spacy.tests.regression.test_issue3830.test_issue3830_no_subtok()
spacy.tests.regression.test_issue3830.test_issue3830_with_subtok()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue4272.py----------------------------------------
A:spacy.tests.regression.test_issue4272.nlp->Greek()
A:spacy.tests.regression.test_issue4272.doc->nlp('Î§Î¸ÎµÏ‚')
spacy.tests.regression.test_issue4272.test_issue4272()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue4348.py----------------------------------------
A:spacy.tests.regression.test_issue4348.nlp->English()
A:spacy.tests.regression.test_issue4348.tagger->English().create_pipe('tagger')
A:spacy.tests.regression.test_issue4348.optimizer->English().begin_training()
A:spacy.tests.regression.test_issue4348.batches->minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))
A:spacy.tests.regression.test_issue4348.(texts, annotations)->zip(*batch)
spacy.tests.regression.test_issue4348.test_issue4348()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue3531.py----------------------------------------
A:spacy.tests.regression.test_issue3531.dep_html->spacy.displacy.render(example_dep, style='dep', manual=True)
A:spacy.tests.regression.test_issue3531.ent_html->spacy.displacy.render(example_ent, style='ent', manual=True)
spacy.tests.regression.test_issue3531.test_issue3531()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue4120.py----------------------------------------
A:spacy.tests.regression.test_issue4120.matcher->Matcher(en_vocab)
A:spacy.tests.regression.test_issue4120.doc1->Doc(en_vocab, words=['a'])
A:spacy.tests.regression.test_issue4120.doc2->Doc(en_vocab, words=['a', 'b', 'c'])
A:spacy.tests.regression.test_issue4120.doc3->Doc(en_vocab, words=['a', 'b', 'b', 'c'])
A:spacy.tests.regression.test_issue4120.doc4->Doc(en_vocab, words=['a', 'b', 'b', 'c'])
spacy.tests.regression.test_issue4120.test_issue4120(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue3959.py----------------------------------------
A:spacy.tests.regression.test_issue3959.nlp->English()
A:spacy.tests.regression.test_issue3959.doc->nlp('displaCy uses JavaScript, SVG and CSS to show you how computers understand language')
A:spacy.tests.regression.test_issue3959.doc2->nlp('')
spacy.tests.regression.test_issue3959.test_issue3959()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue3549.py----------------------------------------
A:spacy.tests.regression.test_issue3549.matcher->Matcher(en_vocab, validate=True)
spacy.tests.regression.test_issue3549.test_issue3549(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue3880.py----------------------------------------
A:spacy.tests.regression.test_issue3880.nlp->English()
spacy.tests.regression.test_issue3880.test_issue3880()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue3869.py----------------------------------------
A:spacy.tests.regression.test_issue3869.nlp->English()
A:spacy.tests.regression.test_issue3869.doc->nlp(sentence)
spacy.tests.regression.test_issue3869.test_issue3869(sentence)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue3521.py----------------------------------------
spacy.tests.regression.test_issue3521.test_issue3521(en_tokenizer,word)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue3951.py----------------------------------------
A:spacy.tests.regression.test_issue3951.matcher->Matcher(en_vocab)
A:spacy.tests.regression.test_issue3951.doc->Doc(en_vocab, words=['Hello', 'my', 'new', 'world'])
A:spacy.tests.regression.test_issue3951.matches->matcher(doc)
spacy.tests.regression.test_issue3951.test_issue3951(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue2001-2500.py----------------------------------------
A:spacy.tests.regression.test_issue2001-2500.nlp->Italian()
A:spacy.tests.regression.test_issue2001-2500.doc->Doc(en_vocab, words=['a', 'b'])
A:spacy.tests.regression.test_issue2001-2500.ner->Italian().create_pipe('ner')
A:spacy.tests.regression.test_issue2001-2500.nlp2->Italian()
A:spacy.tests.regression.test_issue2001-2500.doc_array->Doc(en_vocab, words=['a', 'b']).to_array(['TAG', 'LEMMA'])
A:spacy.tests.regression.test_issue2001-2500.new_doc->Doc(doc.vocab, words=words).from_array(['TAG', 'LEMMA'], doc_array)
A:spacy.tests.regression.test_issue2001-2500.html->render(doc)
A:spacy.tests.regression.test_issue2001-2500.matrix->numpy.array([[0, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 2, 3, 3, 3], [1, 1, 3, 3, 3, 3], [1, 1, 3, 3, 4, 4], [1, 1, 3, 3, 4, 5]], dtype=numpy.int32)
A:spacy.tests.regression.test_issue2001-2500.matcher->Matcher(en_vocab)
A:spacy.tests.regression.test_issue2001-2500.matches->matcher(doc)
A:spacy.tests.regression.test_issue2001-2500.b->Italian().to_bytes()
spacy.tests.regression.test_issue2001-2500.test_issue2070()
spacy.tests.regression.test_issue2001-2500.test_issue2179()
spacy.tests.regression.test_issue2001-2500.test_issue2203(en_vocab)
spacy.tests.regression.test_issue2001-2500.test_issue2219(en_vocab)
spacy.tests.regression.test_issue2001-2500.test_issue2361(de_tokenizer)
spacy.tests.regression.test_issue2001-2500.test_issue2385()
spacy.tests.regression.test_issue2001-2500.test_issue2385_biluo(tags)
spacy.tests.regression.test_issue2001-2500.test_issue2396(en_vocab)
spacy.tests.regression.test_issue2001-2500.test_issue2464(en_vocab)
spacy.tests.regression.test_issue2001-2500.test_issue2482()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue4133.py----------------------------------------
A:spacy.tests.regression.test_issue4133.nlp->English()
A:spacy.tests.regression.test_issue4133.vocab_bytes->English().vocab.to_bytes()
A:spacy.tests.regression.test_issue4133.doc->Doc(vocab).from_bytes(doc_bytes)
A:spacy.tests.regression.test_issue4133.doc_bytes->Doc(vocab).from_bytes(doc_bytes).to_bytes()
A:spacy.tests.regression.test_issue4133.vocab->vocab.from_bytes(vocab_bytes).from_bytes(vocab_bytes)
spacy.tests.regression.test_issue4133.test_issue4133(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue3839.py----------------------------------------
A:spacy.tests.regression.test_issue3839.doc->Doc(en_vocab, words=['terrific', 'group', 'of', 'people'])
A:spacy.tests.regression.test_issue3839.matcher->Matcher(en_vocab)
A:spacy.tests.regression.test_issue3839.matches->matcher(doc)
spacy.tests.regression.test_issue3839.test_issue3839(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue4267.py----------------------------------------
A:spacy.tests.regression.test_issue4267.nlp->English()
A:spacy.tests.regression.test_issue4267.ner->English().create_pipe('ner')
A:spacy.tests.regression.test_issue4267.doc1->nlp('hi')
A:spacy.tests.regression.test_issue4267.ruler->EntityRuler(nlp)
A:spacy.tests.regression.test_issue4267.doc2->nlp('hi')
spacy.tests.regression.test_issue4267.test_issue4267()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue3540.py----------------------------------------
A:spacy.tests.regression.test_issue3540.tensor->numpy.asarray([[1.0, 1.1], [2.0, 2.1], [3.0, 3.1], [4.0, 4.1], [5.0, 5.1], [6.0, 6.1]], dtype='f')
A:spacy.tests.regression.test_issue3540.doc->Doc(en_vocab, words=words)
spacy.tests.regression.test_issue3540.test_issue3540(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue4002.py----------------------------------------
A:spacy.tests.regression.test_issue4002.matcher->PhraseMatcher(en_vocab, attr='NORM')
A:spacy.tests.regression.test_issue4002.pattern1->Doc(en_vocab, words=['c', 'd'])
A:spacy.tests.regression.test_issue4002.doc->Doc(en_vocab, words=['a', 'b', 'c', 'd'])
A:spacy.tests.regression.test_issue4002.matches->matcher(doc)
A:spacy.tests.regression.test_issue4002.pattern2->Doc(en_vocab, words=['1', '2'])
spacy.tests.regression.test_issue4002.test_issue4002(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue4054.py----------------------------------------
A:spacy.tests.regression.test_issue4054.nlp1->English()
A:spacy.tests.regression.test_issue4054.vocab_dir->ensure_path(d / 'vocab')
A:spacy.tests.regression.test_issue4054.vocab2->Vocab().from_disk(vocab_dir)
A:spacy.tests.regression.test_issue4054.nlp2->spacy.blank('en', vocab=vocab2)
A:spacy.tests.regression.test_issue4054.nlp_dir->ensure_path(d / 'nlp')
A:spacy.tests.regression.test_issue4054.nlp3->spacy.load(nlp_dir)
spacy.tests.regression.test_issue4054.test_issue4054(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue1-1000.py----------------------------------------
A:spacy.tests.regression.test_issue1-1000.doc->nlp2(raw_text)
A:spacy.tests.regression.test_issue1-1000.matcher->Matcher(vocab)
A:spacy.tests.regression.test_issue1-1000.ents->list(doc.ents)
A:spacy.tests.regression.test_issue1-1000.tokens->en_tokenizer(text)
A:spacy.tests.regression.test_issue1-1000.sents->list(doc.sents)
A:spacy.tests.regression.test_issue1-1000.matches->matcher(doc)
A:spacy.tests.regression.test_issue1-1000.vocab->Vocab(lex_attr_getters={LOWER: lambda string: string.lower()})
A:spacy.tests.regression.test_issue1-1000.lookups->Lookups()
A:spacy.tests.regression.test_issue1-1000.lemmatizer->Lemmatizer(lookups)
A:spacy.tests.regression.test_issue1-1000.doc2->Doc(doc.vocab)
A:spacy.tests.regression.test_issue1-1000.entities->list(doc.ents)
A:spacy.tests.regression.test_issue1-1000.s->set([token])
A:spacy.tests.regression.test_issue1-1000.items->list(s)
A:spacy.tests.regression.test_issue1-1000.match->matcher(doc)
A:spacy.tests.regression.test_issue1-1000.nlp->Language()
A:spacy.tests.regression.test_issue1-1000.ner->Language().create_pipe('ner')
A:spacy.tests.regression.test_issue1-1000.nlp2->Language().from_disk(model_dir)
spacy.tests.regression.test_issue1-1000.test_control_issue792(en_tokenizer,text)
spacy.tests.regression.test_issue1-1000.test_issue118(en_tokenizer,patterns)
spacy.tests.regression.test_issue1-1000.test_issue118_prefix_reorder(en_tokenizer,patterns)
spacy.tests.regression.test_issue1-1000.test_issue242(en_tokenizer)
spacy.tests.regression.test_issue1-1000.test_issue309(en_tokenizer)
spacy.tests.regression.test_issue1-1000.test_issue351(en_tokenizer)
spacy.tests.regression.test_issue1-1000.test_issue360(en_tokenizer)
spacy.tests.regression.test_issue1-1000.test_issue361(en_vocab,text1,text2)
spacy.tests.regression.test_issue1-1000.test_issue587(en_tokenizer)
spacy.tests.regression.test_issue1-1000.test_issue588(en_vocab)
spacy.tests.regression.test_issue1-1000.test_issue589()
spacy.tests.regression.test_issue1-1000.test_issue590(en_vocab)
spacy.tests.regression.test_issue1-1000.test_issue595()
spacy.tests.regression.test_issue1-1000.test_issue599(en_vocab)
spacy.tests.regression.test_issue1-1000.test_issue600()
spacy.tests.regression.test_issue1-1000.test_issue615(en_tokenizer)
spacy.tests.regression.test_issue1-1000.test_issue736(en_tokenizer,text,number)
spacy.tests.regression.test_issue1-1000.test_issue740(en_tokenizer,text)
spacy.tests.regression.test_issue1-1000.test_issue743()
spacy.tests.regression.test_issue1-1000.test_issue744(en_tokenizer,text)
spacy.tests.regression.test_issue1-1000.test_issue759(en_tokenizer,text,is_num)
spacy.tests.regression.test_issue1-1000.test_issue775(en_tokenizer,text)
spacy.tests.regression.test_issue1-1000.test_issue792(en_tokenizer,text)
spacy.tests.regression.test_issue1-1000.test_issue801(en_tokenizer,text,tokens)
spacy.tests.regression.test_issue1-1000.test_issue805(sv_tokenizer,text,expected_tokens)
spacy.tests.regression.test_issue1-1000.test_issue850()
spacy.tests.regression.test_issue1-1000.test_issue850_basic()
spacy.tests.regression.test_issue1-1000.test_issue852(fr_tokenizer,text)
spacy.tests.regression.test_issue1-1000.test_issue859(en_tokenizer,text)
spacy.tests.regression.test_issue1-1000.test_issue886(en_tokenizer,text)
spacy.tests.regression.test_issue1-1000.test_issue891(en_tokenizer,text)
spacy.tests.regression.test_issue1-1000.test_issue912(en_vocab,text,tag,lemma)
spacy.tests.regression.test_issue1-1000.test_issue957(en_tokenizer)
spacy.tests.regression.test_issue1-1000.test_issue999(train_data)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue4190.py----------------------------------------
A:spacy.tests.regression.test_issue4190.nlp_1->English()
A:spacy.tests.regression.test_issue4190.doc_1a->nlp_1(test_string)
A:spacy.tests.regression.test_issue4190.doc_1b->nlp_1(test_string)
A:spacy.tests.regression.test_issue4190.nlp_2->spacy.util.load_model(model_dir)
A:spacy.tests.regression.test_issue4190.doc_2->nlp_2(test_string)
A:spacy.tests.regression.test_issue4190.prefix_re->spacy.util.compile_prefix_regex(nlp.Defaults.prefixes)
A:spacy.tests.regression.test_issue4190.suffix_re->spacy.util.compile_suffix_regex(nlp.Defaults.suffixes)
A:spacy.tests.regression.test_issue4190.infix_re->spacy.util.compile_infix_regex(nlp.Defaults.infixes)
A:spacy.tests.regression.test_issue4190.new_tokenizer->Tokenizer(nlp.vocab, exceptions, prefix_search=prefix_re.search, suffix_search=suffix_re.search, infix_finditer=infix_re.finditer, token_match=nlp.tokenizer.token_match)
spacy.tests.regression.test_issue4190.customize_tokenizer(nlp)
spacy.tests.regression.test_issue4190.test_issue4190()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/regression/test_issue3962.py----------------------------------------
A:spacy.tests.regression.test_issue3962.tokens->en_tokenizer(text)
A:spacy.tests.regression.test_issue3962.doc2->span2.as_doc()
A:spacy.tests.regression.test_issue3962.doc2_json->span2.as_doc().to_json()
A:spacy.tests.regression.test_issue3962.doc3->span3.as_doc()
A:spacy.tests.regression.test_issue3962.doc3_json->span3.as_doc().to_json()
A:spacy.tests.regression.test_issue3962.sents->list(doc2.sents)
spacy.tests.regression.test_issue3962.doc(en_tokenizer)
spacy.tests.regression.test_issue3962.test_issue3962(doc)
spacy.tests.regression.test_issue3962.test_issue3962_long(two_sent_doc)
spacy.tests.regression.test_issue3962.two_sent_doc(en_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/tokenizer/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/tokenizer/test_whitespace.py----------------------------------------
A:spacy.tests.tokenizer.test_whitespace.tokens->tokenizer(text)
spacy.tests.tokenizer.test_whitespace.test_tokenizer_handles_double_trainling_ws(tokenizer,text)
spacy.tests.tokenizer.test_whitespace.test_tokenizer_splits_double_space(tokenizer,text)
spacy.tests.tokenizer.test_whitespace.test_tokenizer_splits_newline(tokenizer,text)
spacy.tests.tokenizer.test_whitespace.test_tokenizer_splits_newline_double_space(tokenizer,text)
spacy.tests.tokenizer.test_whitespace.test_tokenizer_splits_newline_space(tokenizer,text)
spacy.tests.tokenizer.test_whitespace.test_tokenizer_splits_newline_space_wrap(tokenizer,text)
spacy.tests.tokenizer.test_whitespace.test_tokenizer_splits_single_space(tokenizer,text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/tokenizer/test_exceptions.py----------------------------------------
A:spacy.tests.tokenizer.test_exceptions.tokens->tokenizer(text)
spacy.tests.tokenizer.test_exceptions.test_tokenizer_excludes_false_pos_emoticons(tokenizer,text,length)
spacy.tests.tokenizer.test_exceptions.test_tokenizer_handles_emoji(tokenizer,text,length)
spacy.tests.tokenizer.test_exceptions.test_tokenizer_handles_emoticons(tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/tokenizer/test_naughty_strings.py----------------------------------------
A:spacy.tests.tokenizer.test_naughty_strings.tokens->tokenizer(text)
spacy.tests.tokenizer.test_naughty_strings.test_tokenizer_naughty_strings(tokenizer,text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/tokenizer/test_tokenizer.py----------------------------------------
A:spacy.tests.tokenizer.test_tokenizer.tokens->tokenizer(text)
A:spacy.tests.tokenizer.test_tokenizer.text->loc.open('r', encoding='utf8').read()
A:spacy.tests.tokenizer.test_tokenizer.tokens1->tokenizer(text1)
A:spacy.tests.tokenizer.test_tokenizer.tokens2->tokenizer(text2)
A:spacy.tests.tokenizer.test_tokenizer.doc->tokenizer(text)
A:spacy.tests.tokenizer.test_tokenizer.vocab->Vocab(tag_map={'NN': {'pos': 'NOUN'}})
A:spacy.tests.tokenizer.test_tokenizer.tokenizer->Tokenizer(vocab, {}, None, None, None)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_add_special_case(tokenizer,text,tokens)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_add_special_case_tag(text,tokens)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_colons(tokenizer,text)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_handle_text_from_file(tokenizer,file_name)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_handles_digits(tokenizer)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_handles_long_text(tokenizer)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_handles_no_word(tokenizer)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_handles_punct(tokenizer)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_handles_punct_braces(tokenizer)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_handles_single_word(tokenizer,text)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_keep_urls(tokenizer,text)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_keeps_email(tokenizer,text)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_suspected_freeing_strings(tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tests/tokenizer/test_urls.py----------------------------------------
A:spacy.tests.tokenizer.test_urls.tokens->tokenizer(url + suffix1 + suffix2)
spacy.tests.tokenizer.test_urls.test_should_match(en_tokenizer,url)
spacy.tests.tokenizer.test_urls.test_should_not_match(en_tokenizer,url)
spacy.tests.tokenizer.test_urls.test_tokenizer_handles_prefixed_url(tokenizer,prefix,url)
spacy.tests.tokenizer.test_urls.test_tokenizer_handles_simple_surround_url(tokenizer,url)
spacy.tests.tokenizer.test_urls.test_tokenizer_handles_simple_url(tokenizer,url)
spacy.tests.tokenizer.test_urls.test_tokenizer_handles_suffixed_url(tokenizer,url,suffix)
spacy.tests.tokenizer.test_urls.test_tokenizer_handles_surround_url(tokenizer,prefix,suffix,url)
spacy.tests.tokenizer.test_urls.test_tokenizer_handles_two_prefix_url(tokenizer,prefix1,prefix2,url)
spacy.tests.tokenizer.test_urls.test_tokenizer_handles_two_suffix_url(tokenizer,suffix1,suffix2,url)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/char_classes.py----------------------------------------
A:spacy.lang.char_classes.ALPHA->group_chars(LATIN + _russian + _tatar + _greek + _ukrainian + _uncased)
A:spacy.lang.char_classes.ALPHA_LOWER->group_chars(_lower + _uncased)
A:spacy.lang.char_classes.ALPHA_UPPER->group_chars(_upper + _uncased)
A:spacy.lang.char_classes.UNITS->merge_chars(_units)
A:spacy.lang.char_classes.CURRENCY->merge_chars(_currency)
A:spacy.lang.char_classes.PUNCT->merge_chars(_punct)
A:spacy.lang.char_classes.HYPHENS->merge_chars(_hyphens)
A:spacy.lang.char_classes.LIST_UNITS->split_chars(_units)
A:spacy.lang.char_classes.LIST_CURRENCY->split_chars(_currency)
A:spacy.lang.char_classes.LIST_QUOTES->split_chars(_quotes)
A:spacy.lang.char_classes.LIST_PUNCT->split_chars(_punct)
A:spacy.lang.char_classes.LIST_HYPHENS->split_chars(_hyphens)
A:spacy.lang.char_classes.CONCAT_QUOTES->group_chars(_quotes)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/lex_attrs.py----------------------------------------
A:spacy.lang.lex_attrs._tlds->set('com|org|edu|gov|net|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|sk|sl|sm|sn|so|sr|ss|st|su|sv|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|za|zm|zw'.split('|'))
A:spacy.lang.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.lex_attrs.cluster(string)
spacy.lang.lex_attrs.get_prob(string)
spacy.lang.lex_attrs.is_alpha(string)
spacy.lang.lex_attrs.is_ascii(text)
spacy.lang.lex_attrs.is_bracket(text)
spacy.lang.lex_attrs.is_currency(text)
spacy.lang.lex_attrs.is_digit(string)
spacy.lang.lex_attrs.is_left_punct(text)
spacy.lang.lex_attrs.is_lower(string)
spacy.lang.lex_attrs.is_oov(string)
spacy.lang.lex_attrs.is_punct(text)
spacy.lang.lex_attrs.is_quote(text)
spacy.lang.lex_attrs.is_right_punct(text)
spacy.lang.lex_attrs.is_space(string)
spacy.lang.lex_attrs.is_stop(string,stops=set())
spacy.lang.lex_attrs.is_title(string)
spacy.lang.lex_attrs.is_upper(string)
spacy.lang.lex_attrs.like_email(text)
spacy.lang.lex_attrs.like_num(text)
spacy.lang.lex_attrs.like_url(text)
spacy.lang.lex_attrs.lower(string)
spacy.lang.lex_attrs.prefix(string)
spacy.lang.lex_attrs.suffix(string)
spacy.lang.lex_attrs.word_shape(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.tokenizer_exceptions.URL_PATTERN->'^(?=[\\w])(?:(?:https?|ftp|mailto)://)?(?:\\S+(?::\\S*)?@)?(?:(?!(?:10|127)(?:\\.\\d{1,3}){3})(?!(?:169\\.254|192\\.168)(?:\\.\\d{1,3}){2})(?!172\\.(?:1[6-9]|2\\d|3[0-1])(?:\\.\\d{1,3}){2})(?:[1-9]\\d?|1\\d\\d|2[01]\\d|22[0-3])(?:\\.(?:1?\\d{1,2}|2[0-4]\\d|25[0-5])){2}(?:\\.(?:[1-9]\\d?|1\\d\\d|2[0-4]\\d|25[0-4]))|(?:(?:[a-z0-9\\-]*)?[a-z0-9]+)(?:\\.(?:[a-z0-9])(?:[a-z0-9\\-])*[a-z0-9])?(?:\\.(?:[a-z]{2,})))(?::\\d{2,5})?(?:/\\S*)?\\??(:?\\S*)?(?<=[\\w/])$'.strip()
A:spacy.lang.tokenizer_exceptions.emoticons->set('\n:)\n:-)\n:))\n:-))\n:)))\n:-)))\n(:\n(-:\n=)\n(=\n")\n:]\n:-]\n[:\n[-:\n:o)\n(o:\n:}\n:-}\n8)\n8-)\n(-8\n;)\n;-)\n(;\n(-;\n:(\n:-(\n:((\n:-((\n:(((\n:-(((\n):\n)-:\n=(\n>:(\n:\')\n:\'-)\n:\'(\n:\'-(\n:/\n:-/\n=/\n=|\n:|\n:-|\n:1\n:P\n:-P\n:p\n:-p\n:O\n:-O\n:o\n:-o\n:0\n:-0\n:()\n>:o\n:*\n:-*\n:3\n:-3\n=3\n:>\n:->\n:X\n:-X\n:x\n:-x\n:D\n:-D\n;D\n;-D\n=D\nxD\nXD\nxDD\nXDD\n8D\n8-D\n\n^_^\n^__^\n^___^\n>.<\n>.>\n<.<\n._.\n;_;\n-_-\n-__-\nv.v\nV.V\nv_v\nV_V\no_o\no_O\nO_o\nO_O\n0_o\no_0\n0_0\no.O\nO.o\nO.O\no.o\n0.0\no.0\n0.o\n@_@\n<3\n<33\n<333\n</3\n(^_^)\n(-_-)\n(._.)\n(>_<)\n(*_*)\n(Â¬_Â¬)\nà² _à² \nà² ï¸µà² \n(à² _à² )\nÂ¯\\(ãƒ„)/Â¯\n(â•¯Â°â–¡Â°ï¼‰â•¯ï¸µâ”»â”â”»\n><(((*>\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/norm_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/tag_map.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/punctuation.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/vi/__init__.py----------------------------------------
A:spacy.lang.vi.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.vi.__init__.lex_attr_getters[NORM]->add_lookups(Language.Defaults.lex_attr_getters[NORM], BASE_NORMS)
A:spacy.lang.vi.__init__.(words, spaces)->pyvi.ViTokenizer.spacy_tokenize(text)
A:spacy.lang.vi.__init__.spaces[-1]->bool(token.whitespace_)
spacy.lang.vi.__init__.Vietnamese(Language)
spacy.lang.vi.__init__.Vietnamese.make_doc(self,text)
spacy.lang.vi.__init__.VietnameseDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/vi/lex_attrs.py----------------------------------------
A:spacy.lang.vi.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.vi.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.vi.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/vi/stop_words.py----------------------------------------
A:spacy.lang.vi.stop_words.STOP_WORDS->set('\na_lÃ´\na_ha\nai\nai_ai\nai_náº¥y\nai_Ä‘Ã³\nalÃ´\namen\nanh\nanh_áº¥y\nba\nba_bau\nba_báº£n\nba_cÃ¹ng\nba_há»\nba_ngÃ y\nba_ngÃ´i\nba_tÄƒng\nbao_giá»\nbao_lÃ¢u\nbao_nhiÃªu\nbao_náº£\nbay_biáº¿n\nbiáº¿t\nbiáº¿t_bao\nbiáº¿t_bao_nhiÃªu\nbiáº¿t_cháº¯c\nbiáº¿t_chá»«ng_nÃ o\nbiáº¿t_mÃ¬nh\nbiáº¿t_máº¥y\nbiáº¿t_tháº¿\nbiáº¿t_trÆ°á»›c\nbiáº¿t_viá»‡c\nbiáº¿t_Ä‘Ã¢u\nbiáº¿t_Ä‘Ã¢u_chá»«ng\nbiáº¿t_Ä‘Ã¢u_Ä‘áº¥y\nbiáº¿t_Ä‘Æ°á»£c\nbuá»•i\nbuá»•i_lÃ m\nbuá»•i_má»›i\nbuá»•i_ngÃ y\nbuá»•i_sá»›m\nbÃ \nbÃ _áº¥y\nbÃ i\nbÃ i_bÃ¡c\nbÃ i_bá»\nbÃ i_cÃ¡i\nbÃ¡c\nbÃ¡n\nbÃ¡n_cáº¥p\nbÃ¡n_dáº¡\nbÃ¡n_tháº¿\nbÃ¢y_báº©y\nbÃ¢y_chá»«\nbÃ¢y_giá»\nbÃ¢y_nhiÃªu\nbÃ¨n\nbÃ©ng\nbÃªn\nbÃªn_bá»‹\nbÃªn_cÃ³\nbÃªn_cáº¡nh\nbÃ´ng\nbÆ°á»›c\nbÆ°á»›c_khá»i\nbÆ°á»›c_tá»›i\nbÆ°á»›c_Ä‘i\nbáº¡n\nbáº£n\nbáº£n_bá»™\nbáº£n_riÃªng\nbáº£n_thÃ¢n\nbáº£n_Ã½\nbáº¥t_chá»£t\nbáº¥t_cá»©\nbáº¥t_giÃ¡c\nbáº¥t_kÃ¬\nbáº¥t_ká»ƒ\nbáº¥t_ká»³\nbáº¥t_luáº­n\nbáº¥t_ngá»\nbáº¥t_nhÆ°á»£c\nbáº¥t_quÃ¡\nbáº¥t_quÃ¡_chá»‰\nbáº¥t_thÃ¬nh_lÃ¬nh\nbáº¥t_tá»­\nbáº¥t_Ä‘á»“\nbáº¥y\nbáº¥y_cháº§y\nbáº¥y_chá»«\nbáº¥y_giá»\nbáº¥y_lÃ¢u\nbáº¥y_lÃ¢u_nay\nbáº¥y_nay\nbáº¥y_nhiÃªu\nbáº­p_bÃ _báº­p_bÃµm\nbáº­p_bÃµm\nbáº¯t_Ä‘áº§u\nbáº¯t_Ä‘áº§u_tá»«\nbáº±ng\nbáº±ng_cá»©\nbáº±ng_khÃ´ng\nbáº±ng_ngÆ°á»i\nbáº±ng_nhau\nbáº±ng_nhÆ°\nbáº±ng_nÃ o\nbáº±ng_náº¥y\nbáº±ng_vÃ o\nbáº±ng_Ä‘Æ°á»£c\nbáº±ng_áº¥y\nbá»ƒn\nbá»‡t\nbá»‹\nbá»‹_chÃº\nbá»‹_vÃ¬\nbá»\nbá»_bÃ \nbá»_cha\nbá»_cuá»™c\nbá»_khÃ´ng\nbá»_láº¡i\nbá»_mÃ¬nh\nbá»_máº¥t\nbá»_máº¹\nbá»_nhá»\nbá»_quÃ¡\nbá»_ra\nbá»_riÃªng\nbá»_viá»‡c\nbá»_xa\nbá»—ng\nbá»—ng_chá»‘c\nbá»—ng_dÆ°ng\nbá»—ng_khÃ´ng\nbá»—ng_nhiÃªn\nbá»—ng_nhÆ°ng\nbá»—ng_tháº¥y\nbá»—ng_Ä‘Ã¢u\nbá»™\nbá»™_thuá»™c\nbá»™_Ä‘iá»u\nbá»™i_pháº§n\nbá»›\nbá»Ÿi\nbá»Ÿi_ai\nbá»Ÿi_chÆ°ng\nbá»Ÿi_nhÆ°ng\nbá»Ÿi_sao\nbá»Ÿi_tháº¿\nbá»Ÿi_tháº¿_cho_nÃªn\nbá»Ÿi_táº¡i\nbá»Ÿi_vÃ¬\nbá»Ÿi_váº­y\nbá»Ÿi_Ä‘Ã¢u\nbá»©c\ncao\ncao_lÃ¢u\ncao_rÃ¡o\ncao_rÄƒng\ncao_sang\ncao_sá»‘\ncao_tháº¥p\ncao_tháº¿\ncao_xa\ncha\ncha_cháº£\nchao_Ã´i\nchia_sáº»\nchiáº¿c\ncho\ncho_biáº¿t\ncho_cháº¯c\ncho_hay\ncho_nhau\ncho_nÃªn\ncho_ráº±ng\ncho_rá»“i\ncho_tháº¥y\ncho_tin\ncho_tá»›i\ncho_tá»›i_khi\ncho_vá»\ncho_Äƒn\ncho_Ä‘ang\ncho_Ä‘Æ°á»£c\ncho_Ä‘áº¿n\ncho_Ä‘áº¿n_khi\ncho_Ä‘áº¿n_ná»—i\nchoa\nchu_cha\nchui_cha\nchung\nchung_cho\nchung_chung\nchung_cuá»™c\nchung_cá»¥c\nchung_nhau\nchung_qui\nchung_quy\nchung_quy_láº¡i\nchung_Ã¡i\nchuyá»ƒn\nchuyá»ƒn_tá»±\nchuyá»ƒn_Ä‘áº¡t\nchuyá»‡n\nchuáº©n_bá»‹\nchÃ nh_cháº¡nh\nchÃ­_cháº¿t\nchÃ­nh\nchÃ­nh_báº£n\nchÃ­nh_giá»¯a\nchÃ­nh_lÃ \nchÃ­nh_thá»‹\nchÃ­nh_Ä‘iá»ƒm\nchÃ¹n_chÃ¹n\nchÃ¹n_chÅ©n\nchÃº\nchÃº_dáº«n\nchÃº_khÃ¡ch\nchÃº_mÃ y\nchÃº_mÃ¬nh\nchÃºng\nchÃºng_mÃ¬nh\nchÃºng_ta\nchÃºng_tÃ´i\nchÃºng_Ã´ng\nchÄƒn_cháº¯n\nchÄƒng\nchÄƒng_cháº¯c\nchÄƒng_ná»¯a\nchÆ¡i\nchÆ¡i_há»\nchÆ°a\nchÆ°a_bao_giá»\nchÆ°a_cháº¯c\nchÆ°a_cÃ³\nchÆ°a_cáº§n\nchÆ°a_dÃ¹ng\nchÆ°a_dá»…\nchÆ°a_ká»ƒ\nchÆ°a_tÃ­nh\nchÆ°a_tá»«ng\ncháº§m_cháº­p\ncháº­c\ncháº¯c\ncháº¯c_cháº¯n\ncháº¯c_dáº¡\ncháº¯c_háº³n\ncháº¯c_lÃ²ng\ncháº¯c_ngÆ°á»i\ncháº¯c_vÃ o\ncháº¯c_Äƒn\ncháº³ng_láº½\ncháº³ng_nhá»¯ng\ncháº³ng_ná»¯a\ncháº³ng_pháº£i\ncháº¿t_ná»—i\ncháº¿t_tháº­t\ncháº¿t_tiá»‡t\nchá»‰\nchá»‰_chÃ­nh\nchá»‰_cÃ³\nchá»‰_lÃ \nchá»‰_tÃªn\nchá»‰n\nchá»‹\nchá»‹_bá»™\nchá»‹_áº¥y\nchá»‹u\nchá»‹u_chÆ°a\nchá»‹u_lá»i\nchá»‹u_tá»‘t\nchá»‹u_Äƒn\nchá»n\nchá»n_bÃªn\nchá»n_ra\nchá»‘c_chá»‘c\nchá»›\nchá»›_chi\nchá»›_gÃ¬\nchá»›_khÃ´ng\nchá»›_ká»ƒ\nchá»›_nhÆ°\nchá»£t\nchá»£t_nghe\nchá»£t_nhÃ¬n\nchá»§n\nchá»©\nchá»©_ai\nchá»©_cÃ²n\nchá»©_gÃ¬\nchá»©_khÃ´ng\nchá»©_khÃ´ng_pháº£i\nchá»©_láº¡i\nchá»©_lá»‹\nchá»©_nhÆ°\nchá»©_sao\ncoi_bá»™\ncoi_mÃ²i\ncon\ncon_con\ncon_dáº¡\ncon_nhÃ \ncon_tÃ­nh\ncu_cáº­u\ncuá»‘i\ncuá»‘i_cÃ¹ng\ncuá»‘i_Ä‘iá»ƒm\ncuá»‘n\ncuá»™c\ncÃ ng\ncÃ ng_cÃ ng\ncÃ ng_hay\ncÃ¡_nhÃ¢n\ncÃ¡c\ncÃ¡c_cáº­u\ncÃ¡ch\ncÃ¡ch_bá»©c\ncÃ¡ch_khÃ´ng\ncÃ¡ch_nhau\ncÃ¡ch_Ä‘á»u\ncÃ¡i\ncÃ¡i_gÃ¬\ncÃ¡i_há»\ncÃ¡i_Ä‘Ã£\ncÃ¡i_Ä‘Ã³\ncÃ¡i_áº¥y\ncÃ¢u_há»i\ncÃ¢y\ncÃ¢y_nÆ°á»›c\ncÃ²n\ncÃ²n_nhÆ°\ncÃ²n_ná»¯a\ncÃ²n_thá»i_gian\ncÃ²n_vá»\ncÃ³\ncÃ³_ai\ncÃ³_chuyá»‡n\ncÃ³_chÄƒng\ncÃ³_chÄƒng_lÃ \ncÃ³_chá»©\ncÃ³_cÆ¡\ncÃ³_dá»…\ncÃ³_há»\ncÃ³_khi\ncÃ³_ngÃ y\ncÃ³_ngÆ°á»i\ncÃ³_nhiá»u\ncÃ³_nhÃ \ncÃ³_pháº£i\ncÃ³_sá»‘\ncÃ³_thÃ¡ng\ncÃ³_tháº¿\ncÃ³_thá»ƒ\ncÃ³_váº»\ncÃ³_Ã½\ncÃ³_Äƒn\ncÃ³_Ä‘iá»u\ncÃ³_Ä‘iá»u_kiá»‡n\ncÃ³_Ä‘Ã¡ng\ncÃ³_Ä‘Ã¢u\ncÃ³_Ä‘Æ°á»£c\ncÃ³c_khÃ´\ncÃ´\ncÃ´_mÃ¬nh\ncÃ´_quáº£\ncÃ´_tÄƒng\ncÃ´_áº¥y\ncÃ´ng_nhiÃªn\ncÃ¹ng\ncÃ¹ng_chung\ncÃ¹ng_cá»±c\ncÃ¹ng_nhau\ncÃ¹ng_tuá»•i\ncÃ¹ng_tá»™t\ncÃ¹ng_vá»›i\ncÃ¹ng_Äƒn\ncÄƒn\ncÄƒn_cÃ¡i\ncÄƒn_cáº¯t\ncÄƒn_tÃ­nh\ncÅ©ng\ncÅ©ng_nhÆ°\ncÅ©ng_nÃªn\ncÅ©ng_tháº¿\ncÅ©ng_váº­y\ncÅ©ng_váº­y_thÃ´i\ncÅ©ng_Ä‘Æ°á»£c\ncÆ¡\ncÆ¡_chá»‰\ncÆ¡_chá»«ng\ncÆ¡_cÃ¹ng\ncÆ¡_dáº«n\ncÆ¡_há»“\ncÆ¡_há»™i\ncÆ¡_mÃ \ncÆ¡n\ncáº£\ncáº£_nghe\ncáº£_nghÄ©\ncáº£_ngÃ y\ncáº£_ngÆ°á»i\ncáº£_nhÃ \ncáº£_nÄƒm\ncáº£_tháº£y\ncáº£_thá»ƒ\ncáº£_tin\ncáº£_Äƒn\ncáº£_Ä‘áº¿n\ncáº£m_tháº¥y\ncáº£m_Æ¡n\ncáº¥p\ncáº¥p_sá»‘\ncáº¥p_trá»±c_tiáº¿p\ncáº§n\ncáº§n_cáº¥p\ncáº§n_gÃ¬\ncáº§n_sá»‘\ncáº­t_lá»±c\ncáº­t_sá»©c\ncáº­u\ncá»•_lai\ncá»¥_thá»ƒ\ncá»¥_thá»ƒ_lÃ \ncá»¥_thá»ƒ_nhÆ°\ncá»§a\ncá»§a_ngá»t\ncá»§a_tin\ncá»©\ncá»©_nhÆ°\ncá»©_viá»‡c\ncá»©_Ä‘iá»ƒm\ncá»±c_lá»±c\ndo\ndo_vÃ¬\ndo_váº­y\ndo_Ä‘Ã³\nduy\nduy_chá»‰\nduy_cÃ³\ndÃ i\ndÃ i_lá»i\ndÃ i_ra\ndÃ nh\ndÃ nh_dÃ nh\ndÃ o\ndÃ¬\ndÃ¹\ndÃ¹_cho\ndÃ¹_dÃ¬\ndÃ¹_gÃ¬\ndÃ¹_ráº±ng\ndÃ¹_sao\ndÃ¹ng\ndÃ¹ng_cho\ndÃ¹ng_háº¿t\ndÃ¹ng_lÃ m\ndÃ¹ng_Ä‘áº¿n\ndÆ°á»›i\ndÆ°á»›i_nÆ°á»›c\ndáº¡\ndáº¡_bÃ¡n\ndáº¡_con\ndáº¡_dÃ i\ndáº¡_dáº¡\ndáº¡_khÃ¡ch\ndáº§n_dÃ \ndáº§n_dáº§n\ndáº§u_sao\ndáº«n\ndáº«u\ndáº«u_mÃ \ndáº«u_ráº±ng\ndáº«u_sao\ndá»…\ndá»…_dÃ¹ng\ndá»…_gÃ¬\ndá»…_khiáº¿n\ndá»…_nghe\ndá»…_ngÆ°Æ¡i\ndá»…_nhÆ°_chÆ¡i\ndá»…_sá»£\ndá»…_sá»­_dá»¥ng\ndá»…_thÆ°á»ng\ndá»…_tháº¥y\ndá»…_Äƒn\ndá»…_Ä‘Ã¢u\ndá»Ÿ_chá»«ng\ndá»¯\ndá»¯_cÃ¡ch\nem\nem_em\ngiÃ¡_trá»‹\ngiÃ¡_trá»‹_thá»±c_táº¿\ngiáº£m\ngiáº£m_chÃ­nh\ngiáº£m_tháº¥p\ngiáº£m_tháº¿\ngiá»‘ng\ngiá»‘ng_ngÆ°á»i\ngiá»‘ng_nhau\ngiá»‘ng_nhÆ°\ngiá»\ngiá»_lÃ¢u\ngiá»_nÃ y\ngiá»_Ä‘i\ngiá»_Ä‘Ã¢y\ngiá»_Ä‘áº¿n\ngiá»¯\ngiá»¯_láº¥y\ngiá»¯_Ã½\ngiá»¯a\ngiá»¯a_lÃºc\ngÃ¢y\ngÃ¢y_cho\ngÃ¢y_giá»‘ng\ngÃ¢y_ra\ngÃ¢y_thÃªm\ngÃ¬\ngÃ¬_gÃ¬\ngÃ¬_Ä‘Ã³\ngáº§n\ngáº§n_bÃªn\ngáº§n_háº¿t\ngáº§n_ngÃ y\ngáº§n_nhÆ°\ngáº§n_xa\ngáº§n_Ä‘Ã¢y\ngáº§n_Ä‘áº¿n\ngáº·p\ngáº·p_khÃ³_khÄƒn\ngáº·p_pháº£i\ngá»“m\nhay\nhay_biáº¿t\nhay_hay\nhay_khÃ´ng\nhay_lÃ \nhay_lÃ m\nhay_nhá»‰\nhay_nÃ³i\nhay_sao\nhay_tin\nhay_Ä‘Ã¢u\nhiá»ƒu\nhiá»‡n_nay\nhiá»‡n_táº¡i\nhoÃ n_toÃ n\nhoáº·c\nhoáº·c_lÃ \nhÃ£y\nhÃ£y_cÃ²n\nhÆ¡n\nhÆ¡n_cáº£\nhÆ¡n_háº¿t\nhÆ¡n_lÃ \nhÆ¡n_ná»¯a\nhÆ¡n_trÆ°á»›c\nháº§u_háº¿t\nháº¿t\nháº¿t_chuyá»‡n\nháº¿t_cáº£\nháº¿t_cá»§a\nháº¿t_nÃ³i\nháº¿t_rÃ¡o\nháº¿t_rá»“i\nháº¿t_Ã½\nhá»\nhá»_gáº§n\nhá»_xa\nhá»i\nhá»i_láº¡i\nhá»i_xem\nhá»i_xin\nhá»—_trá»£\nkhi\nkhi_khÃ¡c\nkhi_khÃ´ng\nkhi_nÃ o\nkhi_nÃªn\nkhi_trÆ°á»›c\nkhiáº¿n\nkhoáº£ng\nkhoáº£ng_cÃ¡ch\nkhoáº£ng_khÃ´ng\nkhÃ¡\nkhÃ¡_tá»‘t\nkhÃ¡c\nkhÃ¡c_gÃ¬\nkhÃ¡c_khÃ¡c\nkhÃ¡c_nhau\nkhÃ¡c_nÃ o\nkhÃ¡c_thÆ°á»ng\nkhÃ¡c_xa\nkhÃ¡ch\nkhÃ³\nkhÃ³_biáº¿t\nkhÃ³_chÆ¡i\nkhÃ³_khÄƒn\nkhÃ³_lÃ m\nkhÃ³_má»Ÿ\nkhÃ³_nghe\nkhÃ³_nghÄ©\nkhÃ³_nÃ³i\nkhÃ³_tháº¥y\nkhÃ³_trÃ¡nh\nkhÃ´ng\nkhÃ´ng_ai\nkhÃ´ng_bao_giá»\nkhÃ´ng_bao_lÃ¢u\nkhÃ´ng_biáº¿t\nkhÃ´ng_bÃ¡n\nkhÃ´ng_chá»‰\nkhÃ´ng_cÃ²n\nkhÃ´ng_cÃ³\nkhÃ´ng_cÃ³_gÃ¬\nkhÃ´ng_cÃ¹ng\nkhÃ´ng_cáº§n\nkhÃ´ng_cá»©\nkhÃ´ng_dÃ¹ng\nkhÃ´ng_gÃ¬\nkhÃ´ng_hay\nkhÃ´ng_khá»i\nkhÃ´ng_ká»ƒ\nkhÃ´ng_ngoÃ i\nkhÃ´ng_nháº­n\nkhÃ´ng_nhá»¯ng\nkhÃ´ng_pháº£i\nkhÃ´ng_pháº£i_khÃ´ng\nkhÃ´ng_thá»ƒ\nkhÃ´ng_tÃ­nh\nkhÃ´ng_Ä‘iá»u_kiá»‡n\nkhÃ´ng_Ä‘Æ°á»£c\nkhÃ´ng_Ä‘áº§y\nkhÃ´ng_Ä‘á»ƒ\nkháº³ng_Ä‘á»‹nh\nkhá»i\nkhá»i_nÃ³i\nká»ƒ\nká»ƒ_cáº£\nká»ƒ_nhÆ°\nká»ƒ_tá»›i\nká»ƒ_tá»«\nliÃªn_quan\nloáº¡i\nloáº¡i_tá»«\nluÃ´n\nluÃ´n_cáº£\nluÃ´n_luÃ´n\nluÃ´n_tay\nlÃ \nlÃ _cÃ¹ng\nlÃ _lÃ \nlÃ _nhiá»u\nlÃ _pháº£i\nlÃ _tháº¿_nÃ o\nlÃ _vÃ¬\nlÃ _Ã­t\nlÃ m\nlÃ m_báº±ng\nlÃ m_cho\nlÃ m_dáº§n_dáº§n\nlÃ m_gÃ¬\nlÃ m_lÃ²ng\nlÃ m_láº¡i\nlÃ m_láº¥y\nlÃ m_máº¥t\nlÃ m_ngay\nlÃ m_nhÆ°\nlÃ m_nÃªn\nlÃ m_ra\nlÃ m_riÃªng\nlÃ m_sao\nlÃ m_theo\nlÃ m_tháº¿_nÃ o\nlÃ m_tin\nlÃ m_tÃ´i\nlÃ m_tÄƒng\nlÃ m_táº¡i\nlÃ m_táº¯p_lá»±\nlÃ m_vÃ¬\nlÃ m_Ä‘Ãºng\nlÃ m_Ä‘Æ°á»£c\nlÃ¢u\nlÃ¢u_cÃ¡c\nlÃ¢u_lÃ¢u\nlÃ¢u_nay\nlÃ¢u_ngÃ y\nlÃªn\nlÃªn_cao\nlÃªn_cÆ¡n\nlÃªn_máº¡nh\nlÃªn_ngÃ´i\nlÃªn_nÆ°á»›c\nlÃªn_sá»‘\nlÃªn_xuá»‘ng\nlÃªn_Ä‘áº¿n\nlÃ²ng\nlÃ²ng_khÃ´ng\nlÃºc\nlÃºc_khÃ¡c\nlÃºc_lÃ¢u\nlÃºc_nÃ o\nlÃºc_nÃ y\nlÃºc_sÃ¡ng\nlÃºc_trÆ°á»›c\nlÃºc_Ä‘i\nlÃºc_Ä‘Ã³\nlÃºc_Ä‘áº¿n\nlÃºc_áº¥y\nlÃ½_do\nlÆ°á»£ng\nlÆ°á»£ng_cáº£\nlÆ°á»£ng_sá»‘\nlÆ°á»£ng_tá»«\nláº¡i\nláº¡i_bá»™\nláº¡i_cÃ¡i\nláº¡i_cÃ²n\nláº¡i_giá»‘ng\nláº¡i_lÃ m\nláº¡i_ngÆ°á»i\nláº¡i_nÃ³i\nláº¡i_ná»¯a\nláº¡i_quáº£\nláº¡i_thÃ´i\nláº¡i_Äƒn\nláº¡i_Ä‘Ã¢y\nláº¥y\nláº¥y_cÃ³\nláº¥y_cáº£\nláº¥y_giá»‘ng\nláº¥y_lÃ m\nláº¥y_lÃ½_do\nláº¥y_láº¡i\nláº¥y_ra\nláº¥y_rÃ¡o\nláº¥y_sau\nláº¥y_sá»‘\nláº¥y_thÃªm\nláº¥y_tháº¿\nláº¥y_vÃ o\nláº¥y_xuá»‘ng\nláº¥y_Ä‘Æ°á»£c\nláº¥y_Ä‘á»ƒ\nláº§n\nláº§n_khÃ¡c\nláº§n_láº§n\nláº§n_nÃ o\nláº§n_nÃ y\nláº§n_sang\nláº§n_sau\nláº§n_theo\nláº§n_trÆ°á»›c\nláº§n_tÃ¬m\nlá»›n\nlá»›n_lÃªn\nlá»›n_nhá»\nlá»i\nlá»i_chÃº\nlá»i_nÃ³i\nmang\nmang_láº¡i\nmang_mang\nmang_náº·ng\nmang_vá»\nmuá»‘n\nmÃ \nmÃ _cáº£\nmÃ _khÃ´ng\nmÃ _láº¡i\nmÃ _thÃ´i\nmÃ _váº«n\nmÃ¬nh\nmáº¡nh\nmáº¥t\nmáº¥t_cÃ²n\nmá»i\nmá»i_giá»\nmá»i_khi\nmá»i_lÃºc\nmá»i_ngÆ°á»i\nmá»i_nÆ¡i\nmá»i_sá»±\nmá»i_thá»©\nmá»i_viá»‡c\nmá»‘i\nmá»—i\nmá»—i_lÃºc\nmá»—i_láº§n\nmá»—i_má»™t\nmá»—i_ngÃ y\nmá»—i_ngÆ°á»i\nmá»™t\nmá»™t_cÃ¡ch\nmá»™t_cÆ¡n\nmá»™t_khi\nmá»™t_lÃºc\nmá»™t_sá»‘\nmá»™t_vÃ i\nmá»™t_Ã­t\nmá»›i\nmá»›i_hay\nmá»›i_rá»“i\nmá»›i_Ä‘Ã¢y\nmá»Ÿ\nmá»Ÿ_mang\nmá»Ÿ_nÆ°á»›c\nmá»Ÿ_ra\nmá»£\nmá»©c\nnay\nngay\nngay_bÃ¢y_giá»\nngay_cáº£\nngay_khi\nngay_khi_Ä‘áº¿n\nngay_lÃºc\nngay_lÃºc_nÃ y\nngay_láº­p_tá»©c\nngay_tháº­t\nngay_tá»©c_kháº¯c\nngay_tá»©c_thÃ¬\nngay_tá»«\nnghe\nnghe_chá»«ng\nnghe_hiá»ƒu\nnghe_khÃ´ng\nnghe_láº¡i\nnghe_nhÃ¬n\nnghe_nhÆ°\nnghe_nÃ³i\nnghe_ra\nnghe_rÃµ\nnghe_tháº¥y\nnghe_tin\nnghe_trá»±c_tiáº¿p\nnghe_Ä‘Ã¢u\nnghe_Ä‘Ã¢u_nhÆ°\nnghe_Ä‘Æ°á»£c\nnghen\nnghiá»…m_nhiÃªn\nnghÄ©\nnghÄ©_láº¡i\nnghÄ©_ra\nnghÄ©_tá»›i\nnghÄ©_xa\nnghÄ©_Ä‘áº¿n\nnghá»‰m\nngoÃ i\nngoÃ i_nÃ y\nngoÃ i_ra\nngoÃ i_xa\nngoáº£i\nnguá»“n\nngÃ y\nngÃ y_cÃ ng\nngÃ y_cáº¥p\nngÃ y_giá»\nngÃ y_ngÃ y\nngÃ y_nÃ o\nngÃ y_nÃ y\nngÃ y_ná»\nngÃ y_qua\nngÃ y_rÃ y\nngÃ y_thÃ¡ng\nngÃ y_xÆ°a\nngÃ y_xá»­a\nngÃ y_Ä‘áº¿n\nngÃ y_áº¥y\nngÃ´i\nngÃ´i_nhÃ \nngÃ´i_thá»©\nngÃµ_háº§u\nngÄƒn_ngáº¯t\nngÆ°Æ¡i\nngÆ°á»i\nngÆ°á»i_há»i\nngÆ°á»i_khÃ¡c\nngÆ°á»i_khÃ¡ch\nngÆ°á»i_mÃ¬nh\nngÆ°á»i_nghe\nngÆ°á»i_ngÆ°á»i\nngÆ°á»i_nháº­n\nngá»n\nngá»n_nguá»“n\nngá»t\nngá»“i\nngá»“i_bá»‡t\nngá»“i_khÃ´ng\nngá»“i_sau\nngá»“i_trá»‡t\nngá»™_nhá»¡\nnhanh\nnhanh_lÃªn\nnhanh_tay\nnhau\nnhiÃªn_háº­u\nnhiá»u\nnhiá»u_Ã­t\nnhiá»‡t_liá»‡t\nnhung_nhÄƒng\nnhÃ \nnhÃ _chung\nnhÃ _khÃ³\nnhÃ _lÃ m\nnhÃ _ngoÃ i\nnhÃ _ngÆ°Æ¡i\nnhÃ _tÃ´i\nnhÃ _viá»‡c\nnhÃ¢n_dá»‹p\nnhÃ¢n_tiá»‡n\nnhÃ©\nnhÃ¬n\nnhÃ¬n_chung\nnhÃ¬n_láº¡i\nnhÃ¬n_nháº­n\nnhÃ¬n_theo\nnhÃ¬n_tháº¥y\nnhÃ¬n_xuá»‘ng\nnhÃ³m\nnhÃ³n_nhÃ©n\nnhÆ°\nnhÆ°_ai\nnhÆ°_chÆ¡i\nnhÆ°_khÃ´ng\nnhÆ°_lÃ \nnhÆ°_nhau\nnhÆ°_quáº£\nnhÆ°_sau\nnhÆ°_thÆ°á»ng\nnhÆ°_tháº¿\nnhÆ°_tháº¿_nÃ o\nnhÆ°_thá»ƒ\nnhÆ°_trÃªn\nnhÆ°_trÆ°á»›c\nnhÆ°_tuá»“ng\nnhÆ°_váº­y\nnhÆ°_Ã½\nnhÆ°ng\nnhÆ°ng_mÃ \nnhÆ°á»£c_báº±ng\nnháº¥t\nnháº¥t_loáº¡t\nnháº¥t_luáº­t\nnháº¥t_lÃ \nnháº¥t_má»±c\nnháº¥t_nháº¥t\nnháº¥t_quyáº¿t\nnháº¥t_sinh\nnháº¥t_thiáº¿t\nnháº¥t_thÃ¬\nnháº¥t_tÃ¢m\nnháº¥t_tá»\nnháº¥t_Ä‘Ã¡n\nnháº¥t_Ä‘á»‹nh\nnháº­n\nnháº­n_biáº¿t\nnháº­n_há»\nnháº­n_lÃ m\nnháº­n_nhau\nnháº­n_ra\nnháº­n_tháº¥y\nnháº­n_viá»‡c\nnháº­n_Ä‘Æ°á»£c\nnháº±m\nnháº±m_khi\nnháº±m_lÃºc\nnháº±m_vÃ o\nnháº±m_Ä‘á»ƒ\nnhá»‰\nnhá»\nnhá»_ngÆ°á»i\nnhá»›\nnhá»›_báº­p_bÃµm\nnhá»›_láº¡i\nnhá»›_láº¥y\nnhá»›_ra\nnhá»\nnhá»_chuyá»ƒn\nnhá»_cÃ³\nnhá»_nhá»\nnhá»_Ä‘Ã³\nnhá»¡_ra\nnhá»¯ng\nnhá»¯ng_ai\nnhá»¯ng_khi\nnhá»¯ng_lÃ \nnhá»¯ng_lÃºc\nnhá»¯ng_muá»‘n\nnhá»¯ng_nhÆ°\nnÃ o\nnÃ o_cÅ©ng\nnÃ o_hay\nnÃ o_lÃ \nnÃ o_pháº£i\nnÃ o_Ä‘Ã¢u\nnÃ o_Ä‘Ã³\nnÃ y\nnÃ y_ná»\nnÃªn\nnÃªn_chi\nnÃªn_chÄƒng\nnÃªn_lÃ m\nnÃªn_ngÆ°á»i\nnÃªn_trÃ¡nh\nnÃ³\nnÃ³c\nnÃ³i\nnÃ³i_bÃ´ng\nnÃ³i_chung\nnÃ³i_khÃ³\nnÃ³i_lÃ \nnÃ³i_lÃªn\nnÃ³i_láº¡i\nnÃ³i_nhá»\nnÃ³i_pháº£i\nnÃ³i_qua\nnÃ³i_ra\nnÃ³i_riÃªng\nnÃ³i_rÃµ\nnÃ³i_thÃªm\nnÃ³i_tháº­t\nnÃ³i_toáº¹t\nnÃ³i_trÆ°á»›c\nnÃ³i_tá»‘t\nnÃ³i_vá»›i\nnÃ³i_xa\nnÃ³i_Ã½\nnÃ³i_Ä‘áº¿n\nnÃ³i_Ä‘á»§\nnÄƒm\nnÄƒm_thÃ¡ng\nnÆ¡i\nnÆ¡i_nÆ¡i\nnÆ°á»›c\nnÆ°á»›c_bÃ i\nnÆ°á»›c_cÃ¹ng\nnÆ°á»›c_lÃªn\nnÆ°á»›c_náº·ng\nnÆ°á»›c_quáº£\nnÆ°á»›c_xuá»‘ng\nnÆ°á»›c_Äƒn\nnÆ°á»›c_Ä‘áº¿n\nnáº¥y\nnáº·ng\nnáº·ng_cÄƒn\nnáº·ng_mÃ¬nh\nnáº·ng_vá»\nnáº¿u\nnáº¿u_cÃ³\nnáº¿u_cáº§n\nnáº¿u_khÃ´ng\nnáº¿u_mÃ \nnáº¿u_nhÆ°\nnáº¿u_tháº¿\nnáº¿u_váº­y\nnáº¿u_Ä‘Æ°á»£c\nná»n\nná»\nná»›\nná»©c_ná»Ÿ\nná»¯a\nná»¯a_khi\nná»¯a_lÃ \nná»¯a_rá»“i\noai_oÃ¡i\noÃ¡i\npho\nphÃ¨\nphÃ¨_phÃ¨\nphÃ­a\nphÃ­a_bÃªn\nphÃ­a_báº¡n\nphÃ­a_dÆ°á»›i\nphÃ­a_sau\nphÃ­a_trong\nphÃ­a_trÃªn\nphÃ­a_trÆ°á»›c\nphÃ³c\nphÃ³t\nphÃ¹_há»£p\nphÄƒn_pháº¯t\nphÆ°Æ¡ng_chi\npháº£i\npháº£i_biáº¿t\npháº£i_chi\npháº£i_chÄƒng\npháº£i_cÃ¡ch\npháº£i_cÃ¡i\npháº£i_giá»\npháº£i_khi\npháº£i_khÃ´ng\npháº£i_láº¡i\npháº£i_lá»i\npháº£i_ngÆ°á»i\npháº£i_nhÆ°\npháº£i_rá»“i\npháº£i_tay\npháº§n\npháº§n_lá»›n\npháº§n_nhiá»u\npháº§n_nÃ o\npháº§n_sau\npháº§n_viá»‡c\npháº¯t\nphá»‰_phui\nphá»ng\nphá»ng_nhÆ°\nphá»ng_nÆ°á»›c\nphá»ng_theo\nphá»ng_tÃ­nh\nphá»‘c\nphá»¥t\nphá»©t\nqua\nqua_chuyá»‡n\nqua_khá»i\nqua_láº¡i\nqua_láº§n\nqua_ngÃ y\nqua_tay\nqua_thÃ¬\nqua_Ä‘i\nquan_trá»ng\nquan_trá»ng_váº¥n_Ä‘á»\nquan_tÃ¢m\nquay\nquay_bÆ°á»›c\nquay_láº¡i\nquay_sá»‘\nquay_Ä‘i\nquÃ¡\nquÃ¡_bÃ¡n\nquÃ¡_bá»™\nquÃ¡_giá»\nquÃ¡_lá»i\nquÃ¡_má»©c\nquÃ¡_nhiá»u\nquÃ¡_tay\nquÃ¡_thÃ¬\nquÃ¡_tin\nquÃ¡_trÃ¬nh\nquÃ¡_tuá»•i\nquÃ¡_Ä‘Ã¡ng\nquÃ¡_Æ°\nquáº£\nquáº£_lÃ \nquáº£_tháº­t\nquáº£_tháº¿\nquáº£_váº­y\nquáº­n\nra\nra_bÃ i\nra_bá»™\nra_chÆ¡i\nra_gÃ¬\nra_láº¡i\nra_lá»i\nra_ngÃ´i\nra_ngÆ°á»i\nra_sao\nra_tay\nra_vÃ o\nra_Ã½\nra_Ä‘iá»u\nra_Ä‘Ã¢y\nren_rÃ©n\nriu_rÃ­u\nriÃªng\nriÃªng_tá»«ng\nriá»‡t\nrÃ y\nrÃ¡o\nrÃ¡o_cáº£\nrÃ¡o_nÆ°á»›c\nrÃ¡o_trá»i\nrÃ©n\nrÃ©n_bÆ°á»›c\nrÃ­ch\nrÃ³n_rÃ©n\nrÃµ\nrÃµ_lÃ \nrÃµ_tháº­t\nrÃºt_cá»¥c\nrÄƒng\nrÄƒng_rÄƒng\nráº¥t\nráº¥t_lÃ¢u\nráº±ng\nráº±ng_lÃ \nrá»‘t_cuá»™c\nrá»‘t_cá»¥c\nrá»“i\nrá»“i_ná»¯a\nrá»“i_ra\nrá»“i_sao\nrá»“i_sau\nrá»“i_tay\nrá»“i_thÃ¬\nrá»“i_xem\nrá»“i_Ä‘Ã¢y\nrá»©a\nsa_sáº£\nsang\nsang_nÄƒm\nsang_sÃ¡ng\nsang_tay\nsao\nsao_báº£n\nsao_báº±ng\nsao_cho\nsao_váº­y\nsao_Ä‘ang\nsau\nsau_chÃ³t\nsau_cuá»‘i\nsau_cÃ¹ng\nsau_háº¿t\nsau_nÃ y\nsau_ná»¯a\nsau_sau\nsau_Ä‘Ã¢y\nsau_Ä‘Ã³\nso\nso_vá»›i\nsong_le\nsuÃ½t\nsuÃ½t_ná»¯a\nsÃ¡ng\nsÃ¡ng_ngÃ y\nsÃ¡ng_rÃµ\nsÃ¡ng_tháº¿\nsÃ¡ng_Ã½\nsÃ¬\nsÃ¬_sÃ¬\nsáº¥t\nsáº¯p\nsáº¯p_Ä‘áº·t\nsáº½\nsáº½_biáº¿t\nsáº½_hay\nsá»‘\nsá»‘_cho_biáº¿t\nsá»‘_cá»¥_thá»ƒ\nsá»‘_loáº¡i\nsá»‘_lÃ \nsá»‘_ngÆ°á»i\nsá»‘_pháº§n\nsá»‘_thiáº¿u\nsá»‘t_sá»™t\nsá»›m\nsá»›m_ngÃ y\nsá»Ÿ_dÄ©\nsá»­_dá»¥ng\nsá»±\nsá»±_tháº¿\nsá»±_viá»‡c\ntanh\ntanh_tanh\ntay\ntay_quay\ntha_há»“\ntha_há»“_chÆ¡i\ntha_há»“_Äƒn\nthan_Ã´i\nthanh\nthanh_ba\nthanh_chuyá»ƒn\nthanh_khÃ´ng\nthanh_thanh\nthanh_tÃ­nh\nthanh_Ä‘iá»u_kiá»‡n\nthanh_Ä‘iá»ƒm\nthay_Ä‘á»•i\nthay_Ä‘á»•i_tÃ¬nh_tráº¡ng\ntheo\ntheo_bÆ°á»›c\ntheo_nhÆ°\ntheo_tin\nthi_thoáº£ng\nthiáº¿u\nthiáº¿u_gÃ¬\nthiáº¿u_Ä‘iá»ƒm\nthoáº¡t\nthoáº¡t_nghe\nthoáº¡t_nhiÃªn\nthoáº¯t\nthuáº§n\nthuáº§n_Ã¡i\nthuá»™c\nthuá»™c_bÃ i\nthuá»™c_cÃ¡ch\nthuá»™c_láº¡i\nthuá»™c_tá»«\nthÃ \nthÃ _lÃ \nthÃ _ráº±ng\nthÃ nh_ra\nthÃ nh_thá»­\nthÃ¡i_quÃ¡\nthÃ¡ng\nthÃ¡ng_ngÃ y\nthÃ¡ng_nÄƒm\nthÃ¡ng_thÃ¡ng\nthÃªm\nthÃªm_chuyá»‡n\nthÃªm_giá»\nthÃªm_vÃ o\nthÃ¬\nthÃ¬_giá»\nthÃ¬_lÃ \nthÃ¬_pháº£i\nthÃ¬_ra\nthÃ¬_thÃ´i\nthÃ¬nh_lÃ¬nh\nthÃ­ch\nthÃ­ch_cá»©\nthÃ­ch_thuá»™c\nthÃ­ch_tá»±\nthÃ­ch_Ã½\nthÃ­m\nthÃ´i\nthÃ´i_viá»‡c\nthÃºng_tháº¯ng\nthÆ°Æ¡ng_Ã´i\nthÆ°á»ng\nthÆ°á»ng_bá»‹\nthÆ°á»ng_hay\nthÆ°á»ng_khi\nthÆ°á»ng_sá»‘\nthÆ°á»ng_sá»±\nthÆ°á»ng_thÃ´i\nthÆ°á»ng_thÆ°á»ng\nthÆ°á»ng_tÃ­nh\nthÆ°á»ng_táº¡i\nthÆ°á»ng_xuáº¥t_hiá»‡n\nthÆ°á»ng_Ä‘áº¿n\ntháº£o_hÃ¨n\ntháº£o_nÃ o\ntháº¥p\ntháº¥p_cÆ¡\ntháº¥p_thá»m\ntháº¥p_xuá»‘ng\ntháº¥y\ntháº¥y_thÃ¡ng\ntháº©y\ntháº­m\ntháº­m_chÃ­\ntháº­m_cáº¥p\ntháº­m_tá»«\ntháº­t\ntháº­t_cháº¯c\ntháº­t_lÃ \ntháº­t_lá»±c\ntháº­t_quáº£\ntháº­t_ra\ntháº­t_sá»±\ntháº­t_thÃ \ntháº­t_tá»‘t\ntháº­t_váº­y\ntháº¿\ntháº¿_chuáº©n_bá»‹\ntháº¿_lÃ \ntháº¿_láº¡i\ntháº¿_mÃ \ntháº¿_nÃ o\ntháº¿_nÃªn\ntháº¿_ra\ntháº¿_sá»±\ntháº¿_thÃ¬\ntháº¿_thÃ´i\ntháº¿_thÆ°á»ng\ntháº¿_tháº¿\ntháº¿_Ã \ntháº¿_Ä‘Ã³\ntháº¿ch\nthá»‰nh_thoáº£ng\nthá»m\nthá»‘c\nthá»‘c_thÃ¡o\nthá»‘t\nthá»‘t_nhiÃªn\nthá»‘t_nÃ³i\nthá»‘t_thÃ´i\nthá»™c\nthá»i_gian\nthá»i_gian_sá»­_dá»¥ng\nthá»i_gian_tÃ­nh\nthá»i_Ä‘iá»ƒm\nthá»¥c_máº¡ng\nthá»©\nthá»©_báº£n\nthá»©_Ä‘áº¿n\nthá»­a\nthá»±c_hiá»‡n\nthá»±c_hiá»‡n_Ä‘Ãºng\nthá»±c_ra\nthá»±c_sá»±\nthá»±c_táº¿\nthá»±c_váº­y\ntin\ntin_thÃªm\ntin_vÃ o\ntiáº¿p_theo\ntiáº¿p_tá»¥c\ntiáº¿p_Ä‘Ã³\ntiá»‡n_thá»ƒ\ntoÃ \ntoÃ©_khÃ³i\ntoáº¹t\ntrong\ntrong_khi\ntrong_lÃºc\ntrong_mÃ¬nh\ntrong_ngoÃ i\ntrong_nÃ y\ntrong_sá»‘\ntrong_vÃ¹ng\ntrong_Ä‘Ã³\ntrong_áº¥y\ntrÃ¡nh\ntrÃ¡nh_khá»i\ntrÃ¡nh_ra\ntrÃ¡nh_tÃ¬nh_tráº¡ng\ntrÃ¡nh_xa\ntrÃªn\ntrÃªn_bá»™\ntrÃªn_dÆ°á»›i\ntrÆ°á»›c\ntrÆ°á»›c_háº¿t\ntrÆ°á»›c_khi\ntrÆ°á»›c_kia\ntrÆ°á»›c_nay\ntrÆ°á»›c_ngÃ y\ntrÆ°á»›c_nháº¥t\ntrÆ°á»›c_sau\ntrÆ°á»›c_tiÃªn\ntrÆ°á»›c_tuá»•i\ntrÆ°á»›c_Ä‘Ã¢y\ntrÆ°á»›c_Ä‘Ã³\ntráº£\ntráº£_cá»§a\ntráº£_láº¡i\ntráº£_ngay\ntráº£_trÆ°á»›c\ntráº¿u_trÃ¡o\ntrá»ƒn\ntrá»‡t\ntrá»‡u_tráº¡o\ntrá»ng\ntrá»i_Ä‘áº¥t_Æ¡i\ntrá»Ÿ_thÃ nh\ntrá»«_phi\ntrá»±c_tiáº¿p\ntrá»±c_tiáº¿p_lÃ m\ntuy\ntuy_cÃ³\ntuy_lÃ \ntuy_nhiÃªn\ntuy_ráº±ng\ntuy_tháº¿\ntuy_váº­y\ntuy_Ä‘Ã£\ntuyá»‡t_nhiÃªn\ntuáº§n_tá»±\ntuá»‘t_luá»‘t\ntuá»‘t_tuá»“n_tuá»™t\ntuá»‘t_tuá»™t\ntuá»•i\ntuá»•i_cáº£\ntuá»•i_tÃ´i\ntÃ _tÃ \ntÃªn\ntÃªn_chÃ­nh\ntÃªn_cÃ¡i\ntÃªn_há»\ntÃªn_tá»±\ntÃªnh\ntÃªnh_tÃªnh\ntÃ¬m\ntÃ¬m_báº¡n\ntÃ¬m_cÃ¡ch\ntÃ¬m_hiá»ƒu\ntÃ¬m_ra\ntÃ¬m_viá»‡c\ntÃ¬nh_tráº¡ng\ntÃ­nh\ntÃ­nh_cÃ¡ch\ntÃ­nh_cÄƒn\ntÃ­nh_ngÆ°á»i\ntÃ­nh_phá»ng\ntÃ­nh_tá»«\ntÃ­t_mÃ¹\ntÃ²_te\ntÃ´i\ntÃ´i_con\ntÃ´ng_tá»‘c\ntÃ¹_tÃ¬\ntÄƒm_táº¯p\ntÄƒng\ntÄƒng_chÃºng\ntÄƒng_cáº¥p\ntÄƒng_giáº£m\ntÄƒng_thÃªm\ntÄƒng_tháº¿\ntáº¡i\ntáº¡i_lÃ²ng\ntáº¡i_nÆ¡i\ntáº¡i_sao\ntáº¡i_tÃ´i\ntáº¡i_vÃ¬\ntáº¡i_Ä‘Ã¢u\ntáº¡i_Ä‘Ã¢y\ntáº¡i_Ä‘Ã³\ntáº¡o\ntáº¡o_cÆ¡_há»™i\ntáº¡o_nÃªn\ntáº¡o_ra\ntáº¡o_Ã½\ntáº¡o_Ä‘iá»u_kiá»‡n\ntáº¥m\ntáº¥m_báº£n\ntáº¥m_cÃ¡c\ntáº¥n\ntáº¥n_tá»›i\ntáº¥t_cáº£\ntáº¥t_cáº£_bao_nhiÃªu\ntáº¥t_tháº£y\ntáº¥t_táº§n_táº­t\ntáº¥t_táº­t\ntáº­p_trung\ntáº¯p\ntáº¯p_lá»±\ntáº¯p_táº¯p\ntá»t\ntá»_ra\ntá»_váº»\ntá»‘c_táº£\ntá»‘i_Æ°\ntá»‘t\ntá»‘t_báº¡n\ntá»‘t_bá»™\ntá»‘t_hÆ¡n\ntá»‘t_má»‘i\ntá»‘t_ngÃ y\ntá»™t\ntá»™t_cÃ¹ng\ntá»›\ntá»›i\ntá»›i_gáº§n\ntá»›i_má»©c\ntá»›i_nÆ¡i\ntá»›i_thÃ¬\ntá»©c_thÃ¬\ntá»©c_tá»‘c\ntá»«\ntá»«_cÄƒn\ntá»«_giá»\ntá»«_khi\ntá»«_loáº¡i\ntá»«_nay\ntá»«_tháº¿\ntá»«_tÃ­nh\ntá»«_táº¡i\ntá»«_tá»«\ntá»«_Ã¡i\ntá»«_Ä‘iá»u\ntá»«_Ä‘Ã³\ntá»«_áº¥y\ntá»«ng\ntá»«ng_cÃ¡i\ntá»«ng_giá»\ntá»«ng_nhÃ \ntá»«ng_pháº§n\ntá»«ng_thá»i_gian\ntá»«ng_Ä‘Æ¡n_vá»‹\ntá»«ng_áº¥y\ntá»±\ntá»±_cao\ntá»±_khi\ntá»±_lÆ°á»£ng\ntá»±_tÃ­nh\ntá»±_táº¡o\ntá»±_vÃ¬\ntá»±_Ã½\ntá»±_Äƒn\ntá»±u_trung\nveo\nveo_veo\nviá»‡c\nviá»‡c_gÃ¬\nvung_thiÃªn_Ä‘á»‹a\nvung_tÃ n_tÃ¡n\nvung_tÃ¡n_tÃ n\nvÃ \nvÃ i\nvÃ i_ba\nvÃ i_ngÆ°á»i\nvÃ i_nhÃ \nvÃ i_nÆ¡i\nvÃ i_tÃªn\nvÃ i_Ä‘iá»u\nvÃ o\nvÃ o_gáº·p\nvÃ o_khoáº£ng\nvÃ o_lÃºc\nvÃ o_vÃ¹ng\nvÃ o_Ä‘áº¿n\nvÃ¢ng\nvÃ¢ng_chá»‹u\nvÃ¢ng_dáº¡\nvÃ¢ng_vÃ¢ng\nvÃ¢ng_Ã½\nvÃ¨o\nvÃ¨o_vÃ¨o\nvÃ¬\nvÃ¬_chÆ°ng\nvÃ¬_ráº±ng\nvÃ¬_sao\nvÃ¬_tháº¿\nvÃ¬_váº­y\nvÃ­_báº±ng\nvÃ­_dÃ¹\nvÃ­_phá»ng\nvÃ­_thá»­\nvÃ´_hÃ¬nh_trung\nvÃ´_ká»ƒ\nvÃ´_luáº­n\nvÃ´_vÃ n\nvÃ¹ng\nvÃ¹ng_lÃªn\nvÃ¹ng_nÆ°á»›c\nvÄƒng_tÃª\nvÆ°á»£t\nvÆ°á»£t_khá»i\nvÆ°á»£t_quÃ¡\nváº¡n_nháº¥t\nváº£_chÄƒng\nváº£_láº¡i\nváº¥n_Ä‘á»\nváº¥n_Ä‘á»_quan_trá»ng\nváº«n\nváº«n_tháº¿\nváº­y\nváº­y_lÃ \nváº­y_mÃ \nváº­y_nÃªn\nváº­y_ra\nváº­y_thÃ¬\nváº­y_Æ°\nvá»\nvá»_khÃ´ng\nvá»_nÆ°á»›c\nvá»_pháº§n\nvá»_sau\nvá»_tay\nvá»‹_trÃ­\nvá»‹_táº¥t\nvá»‘n_dÄ©\nvá»›i\nvá»›i_láº¡i\nvá»›i_nhau\nvá»Ÿ\nvá»¥t\nvá»«a\nvá»«a_khi\nvá»«a_lÃºc\nvá»«a_má»›i\nvá»«a_qua\nvá»«a_rá»“i\nvá»«a_vá»«a\nxa\nxa_cÃ¡ch\nxa_gáº§n\nxa_nhÃ \nxa_tanh\nxa_táº¯p\nxa_xa\nxa_xáº£\nxem\nxem_láº¡i\nxem_ra\nxem_sá»‘\nxin\nxin_gáº·p\nxin_vÃ¢ng\nxiáº¿t_bao\nxon_xÃ³n\nxoÃ nh_xoáº¡ch\nxoÃ©t\nxoáº³n\nxoáº¹t\nxuáº¥t_hiá»‡n\nxuáº¥t_kÃ¬_báº¥t_Ã½\nxuáº¥t_ká»³_báº¥t_Ã½\nxuá»ƒ\nxuá»‘ng\nxÄƒm_xÃºi\nxÄƒm_xÄƒm\nxÄƒm_xáº¯m\nxáº£y_ra\nxá»nh_xá»‡ch\nxá»‡p\nxá»­_lÃ½\nyÃªu_cáº§u\nÃ \nÃ _nÃ y\nÃ _Æ¡i\nÃ o\nÃ o_vÃ o\nÃ o_Ã o\nÃ¡\nÃ¡_Ã \nÃ¡i\nÃ¡i_chÃ \nÃ¡i_dÃ \nÃ¡ng\nÃ¡ng_nhÆ°\nÃ¢u_lÃ \nÃ­t\nÃ­t_biáº¿t\nÃ­t_cÃ³\nÃ­t_hÆ¡n\nÃ­t_khi\nÃ­t_lÃ¢u\nÃ­t_nhiá»u\nÃ­t_nháº¥t\nÃ­t_ná»¯a\nÃ­t_quÃ¡\nÃ­t_ra\nÃ­t_thÃ´i\nÃ­t_tháº¥y\nÃ´_hay\nÃ´_hÃ´\nÃ´_kÃª\nÃ´_kÃ¬a\nÃ´i_chao\nÃ´i_thÃ´i\nÃ´ng\nÃ´ng_nhá»\nÃ´ng_táº¡o\nÃ´ng_tá»«\nÃ´ng_áº¥y\nÃ´ng_á»•ng\nÃºi\nÃºi_chÃ \nÃºi_dÃ o\nÃ½\nÃ½_chá»«ng\nÃ½_da\nÃ½_hoáº·c\nÄƒn\nÄƒn_chung\nÄƒn_cháº¯c\nÄƒn_chá»‹u\nÄƒn_cuá»™c\nÄƒn_háº¿t\nÄƒn_há»i\nÄƒn_lÃ m\nÄƒn_ngÆ°á»i\nÄƒn_ngá»“i\nÄƒn_quÃ¡\nÄƒn_riÃªng\nÄƒn_sÃ¡ng\nÄƒn_tay\nÄƒn_trÃªn\nÄƒn_vá»\nÄ‘ang\nÄ‘ang_tay\nÄ‘ang_thÃ¬\nÄ‘iá»u\nÄ‘iá»u_gÃ¬\nÄ‘iá»u_kiá»‡n\nÄ‘iá»ƒm\nÄ‘iá»ƒm_chÃ­nh\nÄ‘iá»ƒm_gáº·p\nÄ‘iá»ƒm_Ä‘áº§u_tiÃªn\nÄ‘Ã nh_Ä‘áº¡ch\nÄ‘Ã¡ng\nÄ‘Ã¡ng_ká»ƒ\nÄ‘Ã¡ng_lÃ­\nÄ‘Ã¡ng_lÃ½\nÄ‘Ã¡ng_láº½\nÄ‘Ã¡ng_sá»‘\nÄ‘Ã¡nh_giÃ¡\nÄ‘Ã¡nh_Ä‘Ã¹ng\nÄ‘Ã¡o_Ä‘á»ƒ\nÄ‘Ã¢u\nÄ‘Ã¢u_cÃ³\nÄ‘Ã¢u_cÅ©ng\nÄ‘Ã¢u_nhÆ°\nÄ‘Ã¢u_nÃ o\nÄ‘Ã¢u_pháº£i\nÄ‘Ã¢u_Ä‘Ã¢u\nÄ‘Ã¢u_Ä‘Ã¢y\nÄ‘Ã¢u_Ä‘Ã³\nÄ‘Ã¢y\nÄ‘Ã¢y_nÃ y\nÄ‘Ã¢y_rá»“i\nÄ‘Ã¢y_Ä‘Ã³\nÄ‘Ã£\nÄ‘Ã£_hay\nÄ‘Ã£_khÃ´ng\nÄ‘Ã£_lÃ \nÄ‘Ã£_lÃ¢u\nÄ‘Ã£_tháº¿\nÄ‘Ã£_váº­y\nÄ‘Ã£_Ä‘á»§\nÄ‘Ã³\nÄ‘Ã³_Ä‘Ã¢y\nÄ‘Ãºng\nÄ‘Ãºng_ngÃ y\nÄ‘Ãºng_ra\nÄ‘Ãºng_tuá»•i\nÄ‘Ãºng_vá»›i\nÄ‘Æ¡n_vá»‹\nÄ‘Æ°a\nÄ‘Æ°a_cho\nÄ‘Æ°a_chuyá»‡n\nÄ‘Æ°a_em\nÄ‘Æ°a_ra\nÄ‘Æ°a_tay\nÄ‘Æ°a_tin\nÄ‘Æ°a_tá»›i\nÄ‘Æ°a_vÃ o\nÄ‘Æ°a_vá»\nÄ‘Æ°a_xuá»‘ng\nÄ‘Æ°a_Ä‘áº¿n\nÄ‘Æ°á»£c\nÄ‘Æ°á»£c_cÃ¡i\nÄ‘Æ°á»£c_lá»i\nÄ‘Æ°á»£c_nÆ°á»›c\nÄ‘Æ°á»£c_tin\nÄ‘áº¡i_loáº¡i\nÄ‘áº¡i_nhÃ¢n\nÄ‘áº¡i_phÃ m\nÄ‘áº¡i_Ä‘á»ƒ\nÄ‘áº¡t\nÄ‘áº£m_báº£o\nÄ‘áº§u_tiÃªn\nÄ‘áº§y\nÄ‘áº§y_nÄƒm\nÄ‘áº§y_phÃ¨\nÄ‘áº§y_tuá»•i\nÄ‘áº·c_biá»‡t\nÄ‘áº·t\nÄ‘áº·t_lÃ m\nÄ‘áº·t_mÃ¬nh\nÄ‘áº·t_má»©c\nÄ‘áº·t_ra\nÄ‘áº·t_trÆ°á»›c\nÄ‘áº·t_Ä‘á»ƒ\nÄ‘áº¿n\nÄ‘áº¿n_bao_giá»\nÄ‘áº¿n_cÃ¹ng\nÄ‘áº¿n_cÃ¹ng_cá»±c\nÄ‘áº¿n_cáº£\nÄ‘áº¿n_giá»\nÄ‘áº¿n_gáº§n\nÄ‘áº¿n_hay\nÄ‘áº¿n_khi\nÄ‘áº¿n_lÃºc\nÄ‘áº¿n_lá»i\nÄ‘áº¿n_nay\nÄ‘áº¿n_ngÃ y\nÄ‘áº¿n_nÆ¡i\nÄ‘áº¿n_ná»—i\nÄ‘áº¿n_thÃ¬\nÄ‘áº¿n_tháº¿\nÄ‘áº¿n_tuá»•i\nÄ‘áº¿n_xem\nÄ‘áº¿n_Ä‘iá»u\nÄ‘áº¿n_Ä‘Ã¢u\nÄ‘á»u\nÄ‘á»u_bÆ°á»›c\nÄ‘á»u_nhau\nÄ‘á»u_Ä‘á»u\nÄ‘á»ƒ\nÄ‘á»ƒ_cho\nÄ‘á»ƒ_giá»‘ng\nÄ‘á»ƒ_khÃ´ng\nÄ‘á»ƒ_lÃ²ng\nÄ‘á»ƒ_láº¡i\nÄ‘á»ƒ_mÃ \nÄ‘á»ƒ_pháº§n\nÄ‘á»ƒ_Ä‘Æ°á»£c\nÄ‘á»ƒ_Ä‘áº¿n_ná»—i\nÄ‘á»‘i_vá»›i\nÄ‘á»“ng_thá»i\nÄ‘á»§\nÄ‘á»§_dÃ¹ng\nÄ‘á»§_nÆ¡i\nÄ‘á»§_sá»‘\nÄ‘á»§_Ä‘iá»u\nÄ‘á»§_Ä‘iá»ƒm\nÆ¡\nÆ¡_hay\nÆ¡_kÃ¬a\nÆ¡i\nÆ¡i_lÃ \nÆ°\náº¡\náº¡_Æ¡i\náº¥y\náº¥y_lÃ \náº§u_Æ¡\náº¯t\náº¯t_háº³n\náº¯t_lÃ \náº¯t_pháº£i\náº¯t_tháº­t\ná»‘i_dÃ o\ná»‘i_giá»i\ná»‘i_giá»i_Æ¡i\ná»“\ná»“_á»“\ná»•ng\ná»›\ná»›_nÃ y\ná»\ná»_á»\ná»Ÿ\ná»Ÿ_láº¡i\ná»Ÿ_nhÆ°\ná»Ÿ_nhá»\ná»Ÿ_nÄƒm\ná»Ÿ_trÃªn\ná»Ÿ_vÃ o\ná»Ÿ_Ä‘Ã¢y\ná»Ÿ_Ä‘Ã³\ná»Ÿ_Ä‘Æ°á»£c\ná»§a\ná»©_há»±\ná»©_á»«\ná»«\ná»«_nhÃ©\ná»«_thÃ¬\ná»«_Ã o\ná»«_á»«\ná»­\n'.split('\n'))


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/vi/tag_map.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ga/__init__.py----------------------------------------
A:spacy.lang.ga.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.ga.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS, TOKENIZER_EXCEPTIONS)
A:spacy.lang.ga.__init__.stop_words->set(STOP_WORDS)
spacy.lang.ga.__init__.Irish(Language)
spacy.lang.ga.__init__.IrishDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ga/irish_morphology_helpers.py----------------------------------------
spacy.lang.ga.irish_morphology_helpers.deduplicate(word)
spacy.lang.ga.irish_morphology_helpers.devoice(word)
spacy.lang.ga.irish_morphology_helpers.ends_dentals(word)
spacy.lang.ga.irish_morphology_helpers.ends_with_vowel(word)
spacy.lang.ga.irish_morphology_helpers.starts_with_vowel(word)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ga/tokenizer_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ga/stop_words.py----------------------------------------
A:spacy.lang.ga.stop_words.STOP_WORDS->set('\na ach ag agus an aon ar arna as\n\nba beirt bhÃºr\n\ncaoga ceathair ceathrar chomh chuig chun cois cÃ©ad cÃºig cÃºigear\n\ndaichead dar de deich deichniÃºr den dhÃ¡ do don dtÃ­ dÃ¡ dÃ¡r dÃ³\n\nfaoi faoin faoina faoinÃ¡r fara fiche\n\ngach gan go gur\n\nhaon hocht\n\ni iad idir in ina ins inÃ¡r is\n\nle leis lena lenÃ¡r\n\nmar mo muid mÃ©\n\nna nach naoi naonÃºr nÃ¡ nÃ­ nÃ­or nÃ³ nÃ³cha\n\nocht ochtar ochtÃ³ os\n\nroimh\n\nsa seacht seachtar seachtÃ³ seasca seisear siad sibh sinn sna sÃ© sÃ­\n\ntar thar thÃº triÃºr trÃ­ trÃ­na trÃ­nÃ¡r trÃ­ocha tÃº\n\num\n\nÃ¡r\n\nÃ© Ã©is\n\nÃ­\n\nÃ³ Ã³n Ã³na Ã³nÃ¡r\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ga/tag_map.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/te/__init__.py----------------------------------------
A:spacy.lang.te.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
spacy.lang.te.__init__.Telugu(Language)
spacy.lang.te.__init__.TeluguDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/te/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/te/lex_attrs.py----------------------------------------
A:spacy.lang.te.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.te.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.te.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/te/stop_words.py----------------------------------------
A:spacy.lang.te.stop_words.STOP_WORDS->set('\nà°…à°‚à°¦à°°à±‚\nà°…à°‚à°¦à±à°¬à°¾à°Ÿà±à°²à±‹\nà°…à°¡à°—à°‚à°¡à°¿\nà°…à°¡à°—à°¡à°‚\nà°…à°¡à±à°¡à°‚à°—à°¾\nà°…à°¨à±à°—à±à°£à°‚à°—à°¾\nà°…à°¨à±à°®à°¤à°¿à°‚à°šà±\nà°…à°¨à±à°®à°¤à°¿à°¸à±à°¤à±à°‚à°¦à°¿\nà°…à°¯à°¿à°¤à±‡\nà°‡à°ªà±à°ªà°Ÿà°¿à°•à±‡\nà°‰à°¨à±à°¨à°¾à°°à±\nà°à°•à±à°•à°¡à±ˆà°¨à°¾\nà°à°ªà±à°ªà±à°¡à±\nà°à°µà°°à±ˆà°¨à°¾\nà°à°µà°°à±‹ à°’à°•à°°à±\nà°\nà°à°¦à±ˆà°¨à°¾\nà°à°®à±ˆà°¨à°ªà±à°ªà°Ÿà°¿à°•à°¿\nà°à°®à±ˆà°¨à°ªà±à°ªà°Ÿà°¿à°•à°¿\nà°’à°•\nà°’à°• à°ªà±à°°à°•à±à°•à°¨\nà°•à°¨à°¿à°ªà°¿à°¸à±à°¤à°¾à°¯à°¿\nà°•à°¾à°¦à±\nà°•à°¾à°¦à±\nà°•à±‚à°¡à°¾\nà°—à°¾\nà°—à±à°°à°¿à°‚à°šà°¿\nà°šà±à°Ÿà±à°Ÿà±‚\nà°šà±‡à°¯à°—à°²à°¿à°—à°¿à°‚à°¦à°¿\nà°¤à°—à°¿à°¨\nà°¤à°°à±à°µà°¾à°¤\nà°¤à°°à±à°µà°¾à°¤\nà°¦à°¾à°¦à°¾à°ªà±\nà°¦à±‚à°°à°‚à°—à°¾\nà°¨à°¿à°œà°‚à°—à°¾\nà°ªà±ˆ\nà°ªà±à°°à°•à°¾à°°à°‚\nà°®à°§à±à°¯\nà°®à°§à±à°¯\nà°®à°°à°¿à°¯à±\nà°®à°°à±Šà°•\nà°®à°³à±à°³à±€\nà°®à°¾à°¤à±à°°à°®à±‡\nà°®à±†à°šà±à°šà±à°•à±‹\nà°µà°¦à±à°¦\nà°µà°¦à±à°¦\nà°µà±†à°‚à°Ÿ\nà°µà±‡à°°à±à°—à°¾\nà°µà±à°¯à°¤à°¿à°°à±‡à°•à°‚à°—à°¾\nà°¸à°‚à°¬à°‚à°§à°‚\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/si/__init__.py----------------------------------------
A:spacy.lang.si.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
spacy.lang.si.__init__.Sinhala(Language)
spacy.lang.si.__init__.SinhalaDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/si/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/si/lex_attrs.py----------------------------------------
A:spacy.lang.si.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.si.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.si.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/si/stop_words.py----------------------------------------
A:spacy.lang.si.stop_words.STOP_WORDS->set('\nà¶…à¶­à¶»\nà¶‘à¶ à·Šà¶ à¶»\nà¶‘à¶´à¶¸à¶«\nà¶‘à¶½à·™à·ƒ\nà¶‘à·€à·’à¶§\nà¶’\nà¶šà¶§\nà¶šà¶¯à·“\nà¶šà·’à¶±à·Š\nà¶šà·Š\nà¶§\nà¶­à·”à¶»\nà¶­à·Š\nà¶¯\nà¶±à¶¸à·”à¶­à·Š\nà¶±à·œà·„à·œà¶­à·Š\nà¶´à¶¸à¶«\nà¶´à¶¸à¶«à·’\nà¶¸\nà¶¸à·™à¶ à·Šà¶ à¶»\nà¶¸à·™à¶´à¶¸à¶«\nà¶¸à·™à¶½à·™à·ƒ\nà¶¸à·™à·€à·’à¶§\nà¶¸à·š\nà¶º\nà¶ºà·’\nà¶½à¶¯à·“\nà¶½à·™à·ƒ\nà·€à¶œà·š\nà·€à¶±\nà·€à·’à¶§\nà·€à·’à¶§à·™à¶š\nà·€à·’à¶­à¶»\nà·€à·’à¶º\nà·€à·”à·€\nà·€à·”à·€à¶­à·Š\nà·€à·”à·€à¶¯\nà·€à·–\nà·ƒà¶¸à¶Ÿ\nà·ƒà·„\nà·„à·\nà·„à·™à·€à¶­à·Š\nà·„à·\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/it/__init__.py----------------------------------------
A:spacy.lang.it.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.it.__init__.lex_attr_getters[NORM]->add_lookups(Language.Defaults.lex_attr_getters[NORM], BASE_NORMS)
A:spacy.lang.it.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS, TOKENIZER_EXCEPTIONS)
spacy.lang.it.__init__.Italian(Language)
spacy.lang.it.__init__.ItalianDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/it/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/it/tokenizer_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/it/stop_words.py----------------------------------------
A:spacy.lang.it.stop_words.STOP_WORDS->set('\na abbastanza abbia abbiamo abbiano abbiate accidenti ad adesso affinche agl\nagli ahime ahimÃ¨ ai al alcuna alcuni alcuno all alla alle allo allora altri\naltrimenti altro altrove altrui anche ancora anni anno ansa anticipo assai\nattesa attraverso avanti avemmo avendo avente aver avere averlo avesse\navessero avessi avessimo aveste avesti avete aveva avevamo avevano avevate\navevi avevo avrai avranno avrebbe avrebbero avrei avremmo avremo avreste\navresti avrete avrÃ  avrÃ² avuta avute avuti avuto\n\nbasta bene benissimo brava bravo\n\ncasa caso cento certa certe certi certo che chi chicchessia chiunque ci\nciascuna ciascuno cima cio cioe circa citta cittÃ  co codesta codesti codesto\ncogli coi col colei coll coloro colui come cominci comunque con concernente\nconciliarsi conclusione consiglio contro cortesia cos cosa cosi cosÃ¬ cui\n\nda dagl dagli dai dal dall dalla dalle dallo dappertutto davanti degl degli\ndei del dell della delle dello dentro detto deve di dice dietro dire\ndirimpetto diventa diventare diventato dopo dov dove dovra dovrÃ  dovunque due\ndunque durante\n\nebbe ebbero ebbi ecc ecco ed effettivamente egli ella entrambi eppure era\nerano eravamo eravate eri ero esempio esse essendo esser essere essi ex\n\nfa faccia facciamo facciano facciate faccio facemmo facendo facesse facessero\nfacessi facessimo faceste facesti faceva facevamo facevano facevate facevi\nfacevo fai fanno farai faranno fare farebbe farebbero farei faremmo faremo\nfareste faresti farete farÃ  farÃ² fatto favore fece fecero feci fin finalmente\nfinche fine fino forse forza fosse fossero fossi fossimo foste fosti fra\nfrattempo fu fui fummo fuori furono futuro generale\n\ngia giÃ  giacche giorni giorno gli gliela gliele glieli glielo gliene governo\ngrande grazie gruppo\n\nha haha hai hanno ho\n\nieri il improvviso in inc infatti inoltre insieme intanto intorno invece io\n\nla lÃ  lasciato lato lavoro le lei li lo lontano loro lui lungo luogo\n\nma macche magari maggior mai male malgrado malissimo mancanza marche me\nmedesimo mediante meglio meno mentre mesi mezzo mi mia mie miei mila miliardi\nmilioni minimi ministro mio modo molti moltissimo molto momento mondo mosto\n\nnazionale ne negl negli nei nel nell nella nelle nello nemmeno neppure nessun\nnessuna nessuno niente no noi non nondimeno nonostante nonsia nostra nostre\nnostri nostro novanta nove nulla nuovo\n\nod oggi ogni ognuna ognuno oltre oppure ora ore osi ossia ottanta otto\n\npaese parecchi parecchie parecchio parte partendo peccato peggio per perche\nperchÃ© percio perciÃ² perfino pero persino persone perÃ² piedi pieno piglia piu\npiuttosto piÃ¹ po pochissimo poco poi poiche possa possedere posteriore posto\npotrebbe preferibilmente presa press prima primo principalmente probabilmente\nproprio puo puÃ² pure purtroppo\n\nqualche qualcosa qualcuna qualcuno quale quali qualunque quando quanta quante\nquanti quanto quantunque quasi quattro quel quella quelle quelli quello quest\nquesta queste questi questo qui quindi\n\nrealmente recente recentemente registrazione relativo riecco salvo\n\nsara sarÃ  sarai saranno sarebbe sarebbero sarei saremmo saremo sareste\nsaresti sarete saro sarÃ² scola scopo scorso se secondo seguente seguito sei\nsembra sembrare sembrato sembri sempre senza sette si sia siamo siano siate\nsiete sig solito solo soltanto sono sopra sotto spesso srl sta stai stando\nstanno starai staranno starebbe starebbero starei staremmo staremo stareste\nstaresti starete starÃ  starÃ² stata state stati stato stava stavamo stavano\nstavate stavi stavo stemmo stessa stesse stessero stessi stessimo stesso\nsteste stesti stette stettero stetti stia stiamo stiano stiate sto su sua\nsubito successivamente successivo sue sugl sugli sui sul sull sulla sulle\nsullo suo suoi\n\ntale tali talvolta tanto te tempo ti titolo torino tra tranne tre trenta\ntroppo trovato tu tua tue tuo tuoi tutta tuttavia tutte tutti tutto\n\nuguali ulteriore ultimo un una uno uomo\n\nva vale vari varia varie vario verso vi via vicino visto vita voi volta volte\nvostra vostre vostri vostro\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/it/tag_map.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/it/punctuation.py----------------------------------------
A:spacy.lang.it.punctuation.ELISION->" ' â€™ ".strip().replace(' ', '').replace('\n', '')


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/en/__init__.py----------------------------------------
A:spacy.lang.en.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.en.__init__.lex_attr_getters[NORM]->add_lookups(Language.Defaults.lex_attr_getters[NORM], BASE_NORMS, NORM_EXCEPTIONS)
A:spacy.lang.en.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS, TOKENIZER_EXCEPTIONS)
spacy.lang.en.__init__.English(Language)
spacy.lang.en.__init__.EnglishDefaults(Language.Defaults)
spacy.lang.en.__init__._return_en(_)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/en/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/en/lex_attrs.py----------------------------------------
A:spacy.lang.en.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.en.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.en.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/en/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.en.tokenizer_exceptions.verb_data_tc->dict(verb_data)
A:spacy.lang.en.tokenizer_exceptions.verb_data_tc[ORTH]->verb_data_tc[ORTH].title().title()
A:spacy.lang.en.tokenizer_exceptions.exc_data_tc->dict(exc_data)
A:spacy.lang.en.tokenizer_exceptions.exc_data_tc[ORTH]->exc_data_tc[ORTH].title().title()
A:spacy.lang.en.tokenizer_exceptions.data_apos->dict(data)
A:spacy.lang.en.tokenizer_exceptions.exc_data_apos->dict(exc_data)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/en/syntax_iterators.py----------------------------------------
A:spacy.lang.en.syntax_iterators.conj->doc.vocab.strings.add('conj')
A:spacy.lang.en.syntax_iterators.np_label->doc.vocab.strings.add('NP')
A:spacy.lang.en.syntax_iterators.seen->set()
spacy.lang.en.syntax_iterators.noun_chunks(obj)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/en/morph_rules.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/en/stop_words.py----------------------------------------
A:spacy.lang.en.stop_words.STOP_WORDS->set('\na about above across after afterwards again against all almost alone along\nalready also although always am among amongst amount an and another any anyhow\nanyone anything anyway anywhere are around as at\n\nback be became because become becomes becoming been before beforehand behind\nbeing below beside besides between beyond both bottom but by\n\ncall can cannot ca could\n\ndid do does doing done down due during\n\neach eight either eleven else elsewhere empty enough even ever every\neveryone everything everywhere except\n\nfew fifteen fifty first five for former formerly forty four from front full\nfurther\n\nget give go\n\nhad has have he hence her here hereafter hereby herein hereupon hers herself\nhim himself his how however hundred\n\ni if in indeed into is it its itself\n\nkeep\n\nlast latter latterly least less\n\njust\n\nmade make many may me meanwhile might mine more moreover most mostly move much\nmust my myself\n\nname namely neither never nevertheless next nine no nobody none noone nor not\nnothing now nowhere\n\nof off often on once one only onto or other others otherwise our ours ourselves\nout over own\n\npart per perhaps please put\n\nquite\n\nrather re really regarding\n\nsame say see seem seemed seeming seems serious several she should show side\nsince six sixty so some somehow someone something sometime sometimes somewhere\nstill such\n\ntake ten than that the their them themselves then thence there thereafter\nthereby therefore therein thereupon these they third this those though three\nthrough throughout thru thus to together too top toward towards twelve twenty\ntwo\n\nunder until up unless upon us used using\n\nvarious very very via was we well were what whatever when whence whenever where\nwhereafter whereas whereby wherein whereupon wherever whether which while\nwhither who whoever whole whom whose why will with within without would\n\nyet you your yours yourself yourselves\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/en/norm_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/en/tag_map.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/fi/__init__.py----------------------------------------
A:spacy.lang.fi.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.fi.__init__.lex_attr_getters[NORM]->add_lookups(Language.Defaults.lex_attr_getters[NORM], BASE_NORMS)
A:spacy.lang.fi.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS, TOKENIZER_EXCEPTIONS)
spacy.lang.fi.__init__.Finnish(Language)
spacy.lang.fi.__init__.FinnishDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/fi/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/fi/lex_attrs.py----------------------------------------
A:spacy.lang.fi.lex_attrs.text->text.replace('.', '').replace(',', '').replace('.', '').replace(',', '')
A:spacy.lang.fi.lex_attrs.(num, denom)->text.replace('.', '').replace(',', '').replace('.', '').replace(',', '').split('/')
spacy.lang.fi.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/fi/tokenizer_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/fi/stop_words.py----------------------------------------
A:spacy.lang.fi.stop_words.STOP_WORDS->set('\naiemmin aika aikaa aikaan aikaisemmin aikaisin aikana aikoina aikoo aikovat\naina ainakaan ainakin ainoa ainoat aiomme aion aiotte aivan ajan alas alemmas\nalkuisin alkuun alla alle aloitamme aloitan aloitat aloitatte aloitattivat\naloitettava aloitettavaksi aloitettu aloitimme aloitin aloitit aloititte\naloittaa aloittamatta aloitti aloittivat alta aluksi alussa alusta annettavaksi\nannettava annettu ansiosta antaa antamatta antoi apu asia asiaa asian asiasta\nasiat asioiden asioihin asioita asti avuksi avulla avun avutta\n\nedelle edelleen edellÃ¤ edeltÃ¤ edemmÃ¤s edes edessÃ¤ edestÃ¤ ehkÃ¤ ei eikÃ¤ eilen\neivÃ¤t eli ellei elleivÃ¤t ellemme ellen ellet ellette emme en enemmÃ¤n eniten\nennen ensi ensimmÃ¤inen ensimmÃ¤iseksi ensimmÃ¤isen ensimmÃ¤isenÃ¤ ensimmÃ¤iset\nensimmÃ¤isiksi ensimmÃ¤isinÃ¤ ensimmÃ¤isiÃ¤ ensimmÃ¤istÃ¤ ensin entinen entisen\nentisiÃ¤ entisten entistÃ¤ enÃ¤Ã¤ eri erittÃ¤in erityisesti erÃ¤iden erÃ¤s erÃ¤Ã¤t esi\nesiin esillÃ¤ esimerkiksi et eteen etenkin ette ettei ettÃ¤\n\nhalua haluaa haluamatta haluamme haluan haluat haluatte haluavat halunnut\nhalusi halusimme halusin halusit halusitte halusivat halutessa haluton he hei\nheidÃ¤n heidÃ¤t heihin heille heillÃ¤ heiltÃ¤ heissÃ¤ heistÃ¤ heitÃ¤ helposti heti\nhetkellÃ¤ hieman hitaasti huolimatta huomenna hyvien hyviin hyviksi hyville\nhyviltÃ¤ hyvin hyvinÃ¤ hyvissÃ¤ hyvistÃ¤ hyviÃ¤ hyvÃ¤ hyvÃ¤t hyvÃ¤Ã¤ hÃ¤n hÃ¤neen hÃ¤nelle\nhÃ¤nellÃ¤ hÃ¤neltÃ¤ hÃ¤nen hÃ¤nessÃ¤ hÃ¤nestÃ¤ hÃ¤net hÃ¤ntÃ¤\n\nihan ilman ilmeisesti itse itsensÃ¤ itseÃ¤Ã¤n\n\nja jo johon joiden joihin joiksi joilla joille joilta joina joissa joista joita\njoka jokainen jokin joko joksi joku jolla jolle jolloin jolta jompikumpi jona\njonka jonkin jonne joo jopa jos joskus jossa josta jota jotain joten jotenkin\njotenkuten jotka jotta jouduimme jouduin jouduit jouduitte joudumme joudun\njoudutte joukkoon joukossa joukosta joutua joutui joutuivat joutumaan joutuu\njoutuvat juuri jÃ¤lkeen jÃ¤lleen jÃ¤Ã¤\n\nkahdeksan kahdeksannen kahdella kahdelle kahdelta kahden kahdessa kahdesta\nkahta kahteen kai kaiken kaikille kaikilta kaikkea kaikki kaikkia kaikkiaan\nkaikkialla kaikkialle kaikkialta kaikkien kaikkiin kaksi kannalta kannattaa\nkanssa kanssaan kanssamme kanssani kanssanne kanssasi kauan kauemmas kaukana\nkautta kehen keiden keihin keiksi keille keillÃ¤ keiltÃ¤ keinÃ¤ keissÃ¤ keistÃ¤\nkeitten keittÃ¤ keitÃ¤ keneen keneksi kenelle kenellÃ¤ keneltÃ¤ kenen kenenÃ¤\nkenessÃ¤ kenestÃ¤ kenet kenettÃ¤ kenties kerran kerta kertaa keskellÃ¤ kesken\nkeskimÃ¤Ã¤rin ketkÃ¤ ketÃ¤ kiitos kohti koko kokonaan kolmas kolme kolmen kolmesti\nkoska koskaan kovin kuin kuinka kuinkaan kuitenkaan kuitenkin kuka kukaan kukin\nkumpainen kumpainenkaan kumpi kumpikaan kumpikin kun kuten kuuden kuusi kuutta\nkylliksi kyllÃ¤ kymmenen kyse\n\nliian liki lisÃ¤ksi lisÃ¤Ã¤ lla luo luona lÃ¤hekkÃ¤in lÃ¤helle lÃ¤hellÃ¤ lÃ¤heltÃ¤\nlÃ¤hemmÃ¤s lÃ¤hes lÃ¤hinnÃ¤ lÃ¤htien lÃ¤pi\n\nmahdollisimman mahdollista me meidÃ¤n meidÃ¤t meihin meille meillÃ¤ meiltÃ¤ meissÃ¤\nmeistÃ¤ meitÃ¤ melkein melko menee menemme menen menet menette menevÃ¤t meni\nmenimme menin menit menivÃ¤t mennessÃ¤ mennyt menossa mihin miksi mikÃ¤ mikÃ¤li\nmikÃ¤Ã¤n mille milloin milloinkan millÃ¤ miltÃ¤ minkÃ¤ minne minua minulla minulle\nminulta minun minussa minusta minut minuun minÃ¤ missÃ¤ mistÃ¤ miten mitkÃ¤ mitÃ¤\nmitÃ¤Ã¤n moi molemmat mones monesti monet moni moniaalla moniaalle moniaalta\nmonta muassa muiden muita muka mukaan mukaansa mukana mutta muu muualla muualle\nmuualta muuanne muulloin muun muut muuta muutama muutaman muuten myÃ¶hemmin myÃ¶s\nmyÃ¶skin myÃ¶skÃ¤Ã¤n myÃ¶tÃ¤\n\nne neljÃ¤ neljÃ¤n neljÃ¤Ã¤ niiden niihin niiksi niille niillÃ¤ niiltÃ¤ niin niinÃ¤\nniissÃ¤ niistÃ¤ niitÃ¤ noiden noihin noiksi noilla noille noilta noin noina noissa\nnoista noita nopeammin nopeasti nopeiten nro nuo nyt nÃ¤iden nÃ¤ihin nÃ¤iksi\nnÃ¤ille nÃ¤illÃ¤ nÃ¤iltÃ¤ nÃ¤in nÃ¤inÃ¤ nÃ¤issÃ¤ nÃ¤istÃ¤ nÃ¤itÃ¤ nÃ¤mÃ¤\n\nohi oikea oikealla oikein ole olemme olen olet olette oleva olevan olevat oli\nolimme olin olisi olisimme olisin olisit olisitte olisivat olit olitte olivat\nolla olleet ollut oma omaa omaan omaksi omalle omalta oman omassa omat omia\nomien omiin omiksi omille omilta omissa omista on onkin onko ovat\n\npaikoittain paitsi pakosti paljon paremmin parempi parhaillaan parhaiten\nperusteella perÃ¤ti pian pieneen pieneksi pienelle pienellÃ¤ pieneltÃ¤ pienempi\npienestÃ¤ pieni pienin poikki puolesta puolestaan pÃ¤Ã¤lle\n\nrunsaasti\n\nsaakka sama samaa samaan samalla saman samat samoin satojen se\nseitsemÃ¤n sekÃ¤ sen seuraavat siellÃ¤ sieltÃ¤ siihen siinÃ¤ siis siitÃ¤ sijaan siksi\nsille silloin sillÃ¤ silti siltÃ¤ sinne sinua sinulla sinulle sinulta sinun\nsinussa sinusta sinut sinuun sinÃ¤ sisÃ¤kkÃ¤in sisÃ¤llÃ¤ siten sitten sitÃ¤ ssa sta\nsuoraan suuntaan suuren suuret suuri suuria suurin suurten\n\ntaa taas taemmas tahansa tai takaa takaisin takana takia tallÃ¤ tapauksessa\ntarpeeksi tavalla tavoitteena te teidÃ¤n teidÃ¤t teihin teille teillÃ¤ teiltÃ¤\nteissÃ¤ teistÃ¤ teitÃ¤ tietysti todella toinen toisaalla toisaalle toisaalta\ntoiseen toiseksi toisella toiselle toiselta toisemme toisen toisensa toisessa\ntoisesta toista toistaiseksi toki tosin tule tulee tulemme tulen\ntulet tulette tulevat tulimme tulin tulisi tulisimme tulisin tulisit tulisitte\ntulisivat tulit tulitte tulivat tulla tulleet tullut tuntuu tuo tuohon tuoksi\ntuolla tuolle tuolloin tuolta tuon tuona tuonne tuossa tuosta tuota tuskin tykÃ¶\ntÃ¤hÃ¤n tÃ¤ksi tÃ¤lle tÃ¤llÃ¤ tÃ¤llÃ¶in tÃ¤ltÃ¤ tÃ¤mÃ¤ tÃ¤mÃ¤n tÃ¤nne tÃ¤nÃ¤ tÃ¤nÃ¤Ã¤n tÃ¤ssÃ¤ tÃ¤stÃ¤\ntÃ¤ten tÃ¤tÃ¤ tÃ¤ysin tÃ¤ytyvÃ¤t tÃ¤ytyy tÃ¤Ã¤llÃ¤ tÃ¤Ã¤ltÃ¤\n\nulkopuolella usea useasti useimmiten usein useita uudeksi uudelleen uuden uudet\nuusi uusia uusien uusinta uuteen uutta\n\nvaan vai vaiheessa vaikea vaikean vaikeat vaikeilla vaikeille vaikeilta\nvaikeissa vaikeista vaikka vain varmasti varsin varsinkin varten vasen\nvasemmalla vasta vastaan vastakkain vastan verran vielÃ¤ vierekkÃ¤in vieressÃ¤\nvieri viiden viime viimeinen viimeisen viimeksi viisi voi voidaan voimme voin\nvoisi voit voitte voivat vuoden vuoksi vuosi vuosien vuosina vuotta vÃ¤hemmÃ¤n\nvÃ¤hintÃ¤Ã¤n vÃ¤hiten vÃ¤hÃ¤n vÃ¤lillÃ¤\n\nyhdeksÃ¤n yhden yhdessÃ¤ yhteen yhteensÃ¤ yhteydessÃ¤ yhteyteen yhtÃ¤ yhtÃ¤Ã¤lle\nyhtÃ¤Ã¤llÃ¤ yhtÃ¤Ã¤ltÃ¤ yhtÃ¤Ã¤n yhÃ¤ yksi yksin yksittÃ¤in yleensÃ¤ ylemmÃ¤s yli ylÃ¶s\nympÃ¤ri\n\nÃ¤lkÃ¶Ã¶n Ã¤lÃ¤\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/sq/__init__.py----------------------------------------
A:spacy.lang.sq.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
spacy.lang.sq.__init__.Albanian(Language)
spacy.lang.sq.__init__.AlbanianDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/sq/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/sq/stop_words.py----------------------------------------
A:spacy.lang.sq.stop_words.STOP_WORDS->set('\na\nafert\nai\najo\nandej\nanes\naq\nas\nasaj\nashtu\nata\nate\natij\natje\nato\naty\natyre\nb\nbe\nbehem\nbehet\nbej\nbeje\nbejne\nben\nbene\nbere\nberi\nbie\nc\nca\ncdo\ncfare\ncila\ncilat\ncilave\ncilen\nciles\ncilet\ncili\ncilin\ncilit\nderi\ndhe\ndic\ndicka\ndickaje\ndike\ndikujt\ndikush\ndisa\ndo\ndot\ndrejt\nduke\ndy\ne\nedhe\nende\neshte\netj\nfare\ngjate\ngje\ngjitha\ngjithcka\ngjithe\ngjithnje\nhere\ni\nia\nishin\nishte\niu\nja\njam\njane\njap\nje\njemi\njo\nju\nk\nka\nkam\nkane\nkem\nkemi\nkeq\nkesaj\nkeshtu\nkete\nketej\nketij\nketo\nketu\nketyre\nkishin\nkishte\nkjo\nkrejt\nkryer\nkryesisht\nkryhet\nku\nkudo\nkundrejt\nkur\nkurre\nkush\nky\nla\nle\nlloj\nm\nma\nmadhe\nmarr\nmarre\nmban\nmbi\nme\nmenjehere\nmerr\nmerret\nmes\nmi\nmidis\nmire\nmjaft\nmori\nmos\nmua\nmund\nna\nndaj\nnder\nndermjet\nndersa\nndonje\nndryshe\nne\nnen\nneper\nnepermjet\nnese\nnga\nnje\nnjera\nnuk\nose\npa\npak\npapritur\npara\npas\npasi\npasur\nper\nperbashket\nperpara\npo\npor\nprane\nprapa\nprej\npse\nqe\nqene\nqenet\nrralle\nrreth\nrri\ns\nsa\nsaj\nsapo\nse\nsecila\nsepse\nsh\nshih\nshume\nsi\nsic\nsikur\nsipas\nsiper\nsone\nt\nta\ntani\nte\ntej\ntek\nteper\ntere\nti\ntij\ntilla\ntille\ntjera\ntjeret\ntjeter\ntjetren\nto\ntone\nty\ntyre\nu\nua\nune\nvazhdimisht\nvend\nvet\nveta\nvete\nvetem\nveten\nvetes\nvjen\nyne\nzakonisht\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/is/__init__.py----------------------------------------
A:spacy.lang.is.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
spacy.lang.is.__init__.Icelandic(Language)
spacy.lang.is.__init__.IcelandicDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/is/stop_words.py----------------------------------------
A:spacy.lang.is.stop_words.STOP_WORDS->set('\nafhverju\naftan\naftur\nafÃ¾vÃ­\naldrei\nallir\nallt\nalveg\nannaÃ°\nannars\nbara\ndag\neÃ°a\neftir\neiga\neinhver\neinhverjir\neinhvers\neins\neinu\neitthvaÃ°\nekkert\nekki\nennÃ¾Ã¡\neru\nfara\nfer\nfinna\nfjÃ¶ldi\nfÃ³lk\nframan\nfrÃ¡\nfrekar\nfyrir\ngegnum\ngeta\ngetur\ngmg\ngott\nhann\nhafa\nhef\nhefur\nheyra\nhÃ©r\nhÃ©rna\nhjÃ¡\nhÃºn\nhvaÃ°\nhvar\nhver\nhverjir\nhverjum\nhvernig\nhvor\nhvort\nhÃ¦gt\nimg\ninn\nkannski\nkoma\nlÃ­ka\nlol\nmaÃ°ur\nmÃ¡tt\nmÃ©r\nmeÃ°\nmega\nmeira\nmig\nmikiÃ°\nminna\nminni\nmissa\nmjÃ¶g\nnei\nniÃ°ur\nnÃºna\noft\nokkar\nokkur\npÃ³st\npÃ³stur\nrofl\nsaman\nsem\nsÃ©r\nsig\nsinni\nsÃ­Ã°an\nsjÃ¡\nsmÃ¡\nsmÃ¡tt\nspurja\nspyrja\nstaÃ°ar\nstÃ³rt\nsvo\nsvona\nsÃ¦lir\nsÃ¦ll\ntaka\ntakk\ntil\ntilvitnun\ntitlar\nupp\nvar\nvel\nvelkomin\nvelkominn\nvera\nverÃ°ur\nveriÃ°\nvel\nviÃ°\nvil\nvilja\nvill\nvita\nvÃ¦ri\nyfir\nykkar\nÃ¾aÃ°\nÃ¾akka\nÃ¾akkir\nÃ¾annig\nÃ¾aÃ°\nÃ¾ar\nÃ¾arf\nÃ¾au\nÃ¾eim\nÃ¾eir\nÃ¾eirra\nÃ¾eirra\nÃ¾egar\nÃ¾ess\nÃ¾essa\nÃ¾essi\nÃ¾essu\nÃ¾essum\nÃ¾etta\nÃ¾Ã©r\nÃ¾iÃ°\nÃ¾inn\nÃ¾itt\nÃ¾Ã­n\nÃ¾rÃ¡Ã°\nÃ¾rÃ¡Ã°ur\nÃ¾vÃ­\nÃ¾Ã¦r\nÃ¦tti\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ca/__init__.py----------------------------------------
A:spacy.lang.ca.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.ca.__init__.lex_attr_getters[NORM]->add_lookups(Language.Defaults.lex_attr_getters[NORM], BASE_NORMS)
A:spacy.lang.ca.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS, TOKENIZER_EXCEPTIONS)
spacy.lang.ca.__init__.Catalan(Language)
spacy.lang.ca.__init__.CatalanDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ca/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ca/lex_attrs.py----------------------------------------
A:spacy.lang.ca.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.ca.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.ca.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ca/tokenizer_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ca/stop_words.py----------------------------------------
A:spacy.lang.ca.stop_words.STOP_WORDS->set("\na abans acÃ­ ah aixÃ­ aixÃ² al aleshores algun alguna algunes alguns alhora allÃ  allÃ­ allÃ²\nals altra altre altres amb ambdues ambdÃ³s anar ans apa aquell aquella aquelles aquells\naquest aquesta aquestes aquests aquÃ­\n\nbaix bastant bÃ©\n\ncada cadascuna cadascunes cadascuns cadascÃº com consegueixo conseguim conseguir\nconsigueix consigueixen consigueixes contra\n\nd'un d'una d'unes d'uns dalt de del dels des des de desprÃ©s dins dintre donat doncs durant\n\ne eh el elles ells els em en encara ens entre era erem eren eres es esta estan estat\nestava estaven estem esteu estic estÃ  estÃ vem estÃ veu et etc ets Ã©rem Ã©reu Ã©s Ã©ssent\n\nfa faig fan fas fem fer feu fi fins fora\n\ngairebÃ©\n\nha han has haver havia he hem heu hi ho\n\ni igual iguals inclÃ²s\n\nja jo\n\nl'hi la les li li'n llarg llavors\n\nm'he ma mal malgrat mateix mateixa mateixes mateixos me mentre meu meus meva\nmeves mode molt molta moltes molts mon mons mÃ©s\n\nn'he n'hi ne ni no nogensmenys nomÃ©s nosaltres nostra nostre nostres\n\no oh oi on\n\npas pel pels per per que perquÃ¨ perÃ² poc poca pocs podem poden poder\npodeu poques potser primer propi puc\n\nqual quals quan quant que quelcom qui quin quina quines quins quÃ¨\n\ns'ha s'han sa sabem saben saber sabeu sap saps semblant semblants sense ser ses\nseu seus seva seves si sobre sobretot soc solament sols som son sons sota sou sÃ³c sÃ³n\n\nt'ha t'han t'he ta tal tambÃ© tampoc tan tant tanta tantes te tene tenim tenir teniu\nteu teus teva teves tinc ton tons tot tota totes tots\n\nun una unes uns us Ãºltim Ãºs\n\nva vaig vam van vas veu vosaltres vostra vostre vostres\n\n".split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ca/tag_map.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ca/punctuation.py----------------------------------------
A:spacy.lang.ca.punctuation.ELISION->" ' â€™ ".strip().replace(' ', '').replace('\n', '')


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/nb/__init__.py----------------------------------------
A:spacy.lang.nb.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.nb.__init__.lex_attr_getters[NORM]->add_lookups(Language.Defaults.lex_attr_getters[NORM], BASE_NORMS)
A:spacy.lang.nb.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS, TOKENIZER_EXCEPTIONS)
spacy.lang.nb.__init__.Norwegian(Language)
spacy.lang.nb.__init__.NorwegianDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/nb/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/nb/tokenizer_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/nb/syntax_iterators.py----------------------------------------
A:spacy.lang.nb.syntax_iterators.conj->doc.vocab.strings.add('conj')
A:spacy.lang.nb.syntax_iterators.np_label->doc.vocab.strings.add('NP')
A:spacy.lang.nb.syntax_iterators.seen->set()
spacy.lang.nb.syntax_iterators.noun_chunks(obj)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/nb/morph_rules.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/nb/stop_words.py----------------------------------------
A:spacy.lang.nb.stop_words.STOP_WORDS->set('\nalle allerede alt and andre annen annet at av\n\nbak bare bedre beste blant ble bli blir blitt bris by bÃ¥de\n\nda dag de del dem den denne der dermed det dette disse drept du\n\neller en enn er et ett etter\n\nfem fikk fire fjor flere folk for fortsatt fotball fra fram frankrike fredag\nfunnet fÃ¥ fÃ¥r fÃ¥tt fÃ¸r fÃ¸rst fÃ¸rste\n\ngang gi gikk gjennom gjorde gjort gjÃ¸r gjÃ¸re god godt grunn gÃ¥ gÃ¥r\n\nha hadde ham han hans har hele helt henne hennes her hun hva hvor hvordan\nhvorfor\n\ni ifÃ¸lge igjen ikke ingen inn\n\nja jeg\n\nkamp kampen kan kl klart kom komme kommer kontakt kort kroner kunne kveld\nkvinner\n\nla laget land landet langt leder ligger like litt lÃ¸pet lÃ¸rdag\n\nman mandag mange mannen mars med meg mellom men mener menn mennesker mens mer\nmillioner minutter mot msci mye mÃ¥ mÃ¥l mÃ¥tte\n\nned neste noe noen nok norge norsk norske ntb ny nye nÃ¥ nÃ¥r\n\nog ogsÃ¥ om onsdag opp opplyser oslo oss over\n\npersoner plass poeng politidistrikt politiet president prosent pÃ¥\n\nregjeringen runde rundt russland\n\nsa saken samme sammen samtidig satt se seg seks selv senere september ser sett\nsiden sier sin sine siste sitt skal skriver skulle slik som sted stedet stor\nstore stÃ¥r sverige svÃ¦rt sÃ¥ sÃ¸ndag\n\nta tatt tid tidligere til tilbake tillegg tirsdag to tok torsdag tre tror\ntyskland\n\nunder usa ut uten utenfor\n\nvant var ved veldig vi videre viktig vil ville viser vÃ¥r vÃ¦re vÃ¦rt\n\nÃ¥ Ã¥r\n\nÃ¸nsker\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/nb/tag_map.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/nb/punctuation.py----------------------------------------
A:spacy.lang.nb.punctuation._quotes->char_classes.CONCAT_QUOTES.replace("'", '')


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/xx/__init__.py----------------------------------------
A:spacy.lang.xx.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.xx.__init__.lex_attr_getters[NORM]->add_lookups(Language.Defaults.lex_attr_getters[NORM], BASE_NORMS)
A:spacy.lang.xx.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS)
spacy.lang.xx.__init__.MultiLanguage(Language)
spacy.lang.xx.__init__.MultiLanguageDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/bn/__init__.py----------------------------------------
A:spacy.lang.bn.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.bn.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS, TOKENIZER_EXCEPTIONS)
spacy.lang.bn.__init__.Bengali(Language)
spacy.lang.bn.__init__.BengaliDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/bn/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/bn/tokenizer_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/bn/morph_rules.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/bn/stop_words.py----------------------------------------
A:spacy.lang.bn.stop_words.STOP_WORDS->set('\nà¦…à¦¤à¦à¦¬ à¦…à¦¥à¦š à¦…à¦¥à¦¬à¦¾ à¦…à¦¨à§à¦¯à¦¾à¦¯à¦¼à§€ à¦…à¦¨à§‡à¦• à¦…à¦¨à§‡à¦•à§‡ à¦…à¦¨à§‡à¦•à§‡à¦‡ à¦…à¦¨à§à¦¤à¦¤  à¦…à¦¬à¦§à¦¿ à¦…à¦¬à¦¶à§à¦¯ à¦…à¦°à§à¦¥à¦¾à§ à¦…à¦¨à§à¦¯ à¦…à¦¨à§à¦¯à¦¾à§Ÿà§€ à¦…à¦°à§à¦§à¦­à¦¾à¦—à§‡\nà¦†à¦—à¦¾à¦®à§€ à¦†à¦—à§‡ à¦†à¦—à§‡à¦‡ à¦†à¦›à§‡ à¦†à¦œ à¦†à¦¦à§à¦¯à¦­à¦¾à¦—à§‡ à¦†à¦ªà¦¨à¦¾à¦° à¦†à¦ªà¦¨à¦¿ à¦†à¦¬à¦¾à¦° à¦†à¦®à¦°à¦¾ à¦†à¦®à¦¾à¦•à§‡ à¦†à¦®à¦¾à¦¦à§‡à¦° à¦†à¦®à¦¾à¦°  à¦†à¦®à¦¿ à¦†à¦° à¦†à¦°à¦“\nà¦‡à¦¤à§à¦¯à¦¾à¦¦à¦¿ à¦‡à¦¹à¦¾\nà¦‰à¦šà¦¿à¦¤ à¦‰à¦¨à¦¿ à¦‰à¦ªà¦° à¦‰à¦ªà¦°à§‡ à¦‰à¦¤à§à¦¤à¦°\nà¦ à¦à¦à¦¦à§‡à¦° à¦à¦à¦°à¦¾ à¦à¦‡ à¦à¦• à¦à¦•à¦‡ à¦à¦•à¦œà¦¨ à¦à¦•à¦Ÿà¦¾ à¦à¦•à¦Ÿà¦¿  à¦à¦•à¦¬à¦¾à¦° à¦à¦•à§‡ à¦à¦–à¦¨ à¦à¦–à¦¨à¦“ à¦à¦–à¦¾à¦¨à§‡ à¦à¦–à¦¾à¦¨à§‡à¦‡ à¦à¦Ÿà¦¾ à¦à¦¸à§‹\nà¦à¦Ÿà¦¾à¦‡ à¦à¦Ÿà¦¿ à¦à¦¤ à¦à¦¤à¦Ÿà¦¾à¦‡ à¦à¦¤à§‡ à¦à¦¦à§‡à¦° à¦à¦¬à¦‚ à¦à¦¬à¦¾à¦° à¦à¦®à¦¨ à¦à¦®à¦¨à¦¿ à¦à¦®à¦¨à¦•à¦¿ à¦à¦° à¦à¦°à¦¾ à¦à¦²à§‹ à¦à¦¸ à¦à¦¸à§‡\nà¦\nà¦“ à¦“à¦à¦¦à§‡à¦° à¦“à¦à¦° à¦“à¦à¦°à¦¾ à¦“à¦‡ à¦“à¦•à§‡ à¦“à¦–à¦¾à¦¨à§‡ à¦“à¦¦à§‡à¦° à¦“à¦° à¦“à¦°à¦¾\nà¦•à¦–à¦¨à¦“ à¦•à¦¤ à¦•à¦¥à¦¾ à¦•à¦¬à§‡ à¦•à¦¯à¦¼à§‡à¦•  à¦•à¦¯à¦¼à§‡à¦•à¦Ÿà¦¿ à¦•à¦°à¦›à§‡ à¦•à¦°à¦›à§‡à¦¨ à¦•à¦°à¦¤à§‡  à¦•à¦°à¦¬à§‡ à¦•à¦°à¦¬à§‡à¦¨ à¦•à¦°à¦²à§‡ à¦•à§Ÿà§‡à¦•  à¦•à§Ÿà§‡à¦•à¦Ÿà¦¿ à¦•à¦°à¦¿à§Ÿà§‡ à¦•à¦°à¦¿à§Ÿà¦¾ à¦•à¦°à¦¾à§Ÿ\nà¦•à¦°à¦²à§‡à¦¨ à¦•à¦°à¦¾ à¦•à¦°à¦¾à¦‡ à¦•à¦°à¦¾à¦¯à¦¼ à¦•à¦°à¦¾à¦° à¦•à¦°à¦¿ à¦•à¦°à¦¿à¦¤à§‡ à¦•à¦°à¦¿à¦¯à¦¼à¦¾ à¦•à¦°à¦¿à¦¯à¦¼à§‡ à¦•à¦°à§‡ à¦•à¦°à§‡à¦‡ à¦•à¦°à§‡à¦›à¦¿à¦²à§‡à¦¨ à¦•à¦°à§‡à¦›à§‡ à¦•à¦°à§‡à¦›à§‡à¦¨ à¦•à¦°à§‡à¦¨ à¦•à¦¾à¦‰à¦•à§‡\nà¦•à¦¾à¦› à¦•à¦¾à¦›à§‡ à¦•à¦¾à¦œ à¦•à¦¾à¦œà§‡ à¦•à¦¾à¦°à¦“ à¦•à¦¾à¦°à¦£ à¦•à¦¿ à¦•à¦¿à¦‚à¦¬à¦¾ à¦•à¦¿à¦›à§ à¦•à¦¿à¦›à§à¦‡ à¦•à¦¿à¦¨à§à¦¤à§ à¦•à§€ à¦•à§‡ à¦•à§‡à¦‰ à¦•à§‡à¦‰à¦‡ à¦•à§‡à¦¨ à¦•à§‹à¦¨ à¦•à§‹à¦¨à¦“ à¦•à§‹à¦¨à§‹ à¦•à§‡à¦®à¦¨à§‡ à¦•à§‹à¦Ÿà¦¿\nà¦•à§à¦·à§‡à¦¤à§à¦°à§‡ à¦–à§à¦¬\nà¦—à¦¿à¦¯à¦¼à§‡ à¦—à¦¿à¦¯à¦¼à§‡à¦›à§‡ à¦—à§à¦²à¦¿ à¦—à§‡à¦›à§‡ à¦—à§‡à¦² à¦—à§‡à¦²à§‡ à¦—à§‹à¦Ÿà¦¾ à¦—à¦¿à§Ÿà§‡ à¦—à¦¿à§Ÿà§‡à¦›à§‡\nà¦šà¦²à§‡ à¦šà¦¾à¦¨ à¦šà¦¾à¦¯à¦¼ à¦šà§‡à¦¯à¦¼à§‡ à¦šà¦¾à§Ÿ à¦šà§‡à§Ÿà§‡ à¦šà¦¾à¦° à¦šà¦¾à¦²à§ à¦šà§‡à¦·à§à¦Ÿà¦¾\nà¦›à¦¾à¦¡à¦¼à¦¾ à¦›à¦¾à¦¡à¦¼à¦¾à¦“ à¦›à¦¿à¦² à¦›à¦¿à¦²à§‡à¦¨ à¦›à¦¾à§œà¦¾ à¦›à¦¾à§œà¦¾à¦“\nà¦œà¦¨ à¦œà¦¨à¦•à§‡ à¦œà¦¨à§‡à¦° à¦œà¦¨à§à¦¯ à¦œà¦¨à§à¦¯à§‡ à¦œà¦¾à¦¨à¦¤à§‡ à¦œà¦¾à¦¨à¦¾ à¦œà¦¾à¦¨à¦¾à¦¨à§‹ à¦œà¦¾à¦¨à¦¾à¦¯à¦¼  à¦œà¦¾à¦¨à¦¿à¦¯à¦¼à§‡  à¦œà¦¾à¦¨à¦¿à¦¯à¦¼à§‡à¦›à§‡ à¦œà¦¾à¦¨à¦¾à§Ÿ à¦œà¦¾à¦¾à¦¨à¦¿à§Ÿà§‡ à¦œà¦¾à¦¨à¦¿à§Ÿà§‡à¦›à§‡\nà¦Ÿà¦¿\nà¦ à¦¿à¦•\nà¦¤à¦–à¦¨ à¦¤à¦¤ à¦¤à¦¥à¦¾ à¦¤à¦¬à§ à¦¤à¦¬à§‡ à¦¤à¦¾ à¦¤à¦¾à¦à¦•à§‡ à¦¤à¦¾à¦à¦¦à§‡à¦° à¦¤à¦¾à¦à¦° à¦¤à¦¾à¦à¦°à¦¾ à¦¤à¦¾à¦à¦¹à¦¾à¦°à¦¾ à¦¤à¦¾à¦‡ à¦¤à¦¾à¦“ à¦¤à¦¾à¦•à§‡ à¦¤à¦¾à¦¤à§‡ à¦¤à¦¾à¦¦à§‡à¦° à¦¤à¦¾à¦° à¦¤à¦¾à¦°à¦ªà¦° à¦¤à¦¾à¦°à¦¾ à¦¤à¦¾à¦°à¦‡ à¦¤à¦¾à¦¹à¦²à§‡ à¦¤à¦¾à¦¹à¦¾ à¦¤à¦¾à¦¹à¦¾à¦¤à§‡ à¦¤à¦¾à¦¹à¦¾à¦° à¦¤à¦¿à¦¨à¦‡\nà¦¤à¦¿à¦¨à¦¿ à¦¤à¦¿à¦¨à¦¿à¦“ à¦¤à§à¦®à¦¿ à¦¤à§à¦²à§‡ à¦¤à§‡à¦®à¦¨ à¦¤à§‹ à¦¤à§‹à¦®à¦¾à¦° à¦¤à§à¦‡ à¦¤à§‹à¦°à¦¾ à¦¤à§‹à¦° à¦¤à§‹à¦®à¦¾à¦¦à§‡à¦° à¦¤à§‹à¦¦à§‡à¦°\nà¦¥à¦¾à¦•à¦¬à§‡ à¦¥à¦¾à¦•à¦¬à§‡à¦¨ à¦¥à¦¾à¦•à¦¾ à¦¥à¦¾à¦•à¦¾à¦¯à¦¼ à¦¥à¦¾à¦•à§‡ à¦¥à¦¾à¦•à§‡à¦¨ à¦¥à§‡à¦•à§‡ à¦¥à§‡à¦•à§‡à¦‡  à¦¥à§‡à¦•à§‡à¦“ à¦¥à¦¾à¦•à¦¾à§Ÿ\nà¦¦à¦¿à¦•à§‡ à¦¦à¦¿à¦¤à§‡ à¦¦à¦¿à¦¯à¦¼à§‡ à¦¦à¦¿à¦¯à¦¼à§‡à¦›à§‡ à¦¦à¦¿à¦¯à¦¼à§‡à¦›à§‡à¦¨ à¦¦à¦¿à¦²à§‡à¦¨ à¦¦à¦¿à§Ÿà§‡ à¦¦à§  à¦¦à§à¦Ÿà¦¿  à¦¦à§à¦Ÿà§‹ à¦¦à§‡à¦“à¦¯à¦¼à¦¾ à¦¦à§‡à¦“à¦¯à¦¼à¦¾à¦° à¦¦à§‡à¦–à¦¤à§‡ à¦¦à§‡à¦–à¦¾ à¦¦à§‡à¦–à§‡ à¦¦à§‡à¦¨ à¦¦à§‡à¦¯à¦¼  à¦¦à§‡à¦¶à§‡à¦°\nà¦¦à§à¦¬à¦¾à¦°à¦¾ à¦¦à¦¿à§Ÿà§‡à¦›à§‡ à¦¦à¦¿à§Ÿà§‡à¦›à§‡à¦¨ à¦¦à§‡à§Ÿ à¦¦à§‡à¦“à§Ÿà¦¾ à¦¦à§‡à¦“à§Ÿà¦¾à¦° à¦¦à¦¿à¦¨ à¦¦à§à¦‡\nà¦§à¦°à¦¾ à¦§à¦°à§‡\nà¦¨à¦¯à¦¼ à¦¨à¦¾ à¦¨à¦¾à¦‡ à¦¨à¦¾à¦•à¦¿ à¦¨à¦¾à¦—à¦¾à¦¦ à¦¨à¦¾à¦¨à¦¾ à¦¨à¦¿à¦œà§‡ à¦¨à¦¿à¦œà§‡à¦‡ à¦¨à¦¿à¦œà§‡à¦¦à§‡à¦° à¦¨à¦¿à¦œà§‡à¦° à¦¨à¦¿à¦¤à§‡ à¦¨à¦¿à¦¯à¦¼à§‡ à¦¨à¦¿à§Ÿà§‡ à¦¨à§‡à¦‡ à¦¨à§‡à¦“à§Ÿà¦¾ à¦¨à§‡à¦“à¦¯à¦¼à¦¾à¦° à¦¨à§Ÿ à¦¨à¦¤à§à¦¨\nà¦ªà¦•à§à¦·à§‡ à¦ªà¦° à¦ªà¦°à§‡ à¦ªà¦°à§‡à¦‡ à¦ªà¦°à§‡à¦“ à¦ªà¦°à§à¦¯à¦¨à§à¦¤ à¦ªà¦¾à¦“à¦¯à¦¼à¦¾ à¦ªà¦¾à¦°à¦¿ à¦ªà¦¾à¦°à§‡ à¦ªà¦¾à¦°à§‡à¦¨ à¦ªà§‡à¦¯à¦¼à§‡ à¦ªà§à¦°à¦¤à¦¿ à¦ªà§à¦°à¦­à§ƒà¦¤à¦¿ à¦ªà§à¦°à¦¾à¦¯à¦¼ à¦ªà¦¾à¦“à§Ÿà¦¾ à¦ªà§‡à§Ÿà§‡ à¦ªà§à¦°à¦¾à§Ÿ à¦ªà¦¾à¦à¦š à¦ªà§à¦°à¦¥à¦® à¦ªà§à¦°à¦¾à¦¥à¦®à¦¿à¦•\nà¦«à¦²à§‡ à¦«à¦¿à¦°à§‡ à¦«à§‡à¦°\nà¦¬à¦›à¦° à¦¬à¦¦à¦²à§‡ à¦¬à¦°à¦‚ à¦¬à¦²à¦¤à§‡ à¦¬à¦²à¦² à¦¬à¦²à¦²à§‡à¦¨ à¦¬à¦²à¦¾ à¦¬à¦²à§‡ à¦¬à¦²à§‡à¦›à§‡à¦¨ à¦¬à¦²à§‡à¦¨  à¦¬à¦¸à§‡ à¦¬à¦¹à§ à¦¬à¦¾ à¦¬à¦¾à¦¦à§‡ à¦¬à¦¾à¦° à¦¬à¦¿à¦¨à¦¾ à¦¬à¦¿à¦­à¦¿à¦¨à§à¦¨ à¦¬à¦¿à¦¶à§‡à¦· à¦¬à¦¿à¦·à¦¯à¦¼à¦Ÿà¦¿ à¦¬à§‡à¦¶ à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦¬à§à¦¯à¦¾à¦ªà¦¾à¦°à§‡ à¦¬à¦•à§à¦¤à¦¬à§à¦¯ à¦¬à¦¨ à¦¬à§‡à¦¶à¦¿\nà¦­à¦¾à¦¬à§‡  à¦­à¦¾à¦¬à§‡à¦‡\nà¦®à¦¤ à¦®à¦¤à§‹ à¦®à¦¤à§‹à¦‡ à¦®à¦§à§à¦¯à¦­à¦¾à¦—à§‡ à¦®à¦§à§à¦¯à§‡ à¦®à¦§à§à¦¯à§‡à¦‡  à¦®à¦§à§à¦¯à§‡à¦“ à¦®à¦¨à§‡ à¦®à¦¾à¦¤à§à¦° à¦®à¦¾à¦§à§à¦¯à¦®à§‡ à¦®à¦¾à¦¨à§à¦· à¦®à¦¾à¦¨à§à¦·à§‡à¦° à¦®à§‹à¦Ÿ à¦®à§‹à¦Ÿà§‡à¦‡ à¦®à§‹à¦¦à§‡à¦° à¦®à§‹à¦°\nà¦¯à¦–à¦¨ à¦¯à¦¤ à¦¯à¦¤à¦Ÿà¦¾ à¦¯à¦¥à§‡à¦·à§à¦Ÿ à¦¯à¦¦à¦¿ à¦¯à¦¦à¦¿à¦“ à¦¯à¦¾ à¦¯à¦¾à¦à¦° à¦¯à¦¾à¦à¦°à¦¾ à¦¯à¦¾à¦“à¦¯à¦¼à¦¾  à¦¯à¦¾à¦“à¦¯à¦¼à¦¾à¦° à¦¯à¦¾à¦•à§‡ à¦¯à¦¾à¦šà§à¦›à§‡ à¦¯à¦¾à¦¤à§‡ à¦¯à¦¾à¦¦à§‡à¦° à¦¯à¦¾à¦¨ à¦¯à¦¾à¦¬à§‡ à¦¯à¦¾à¦¯à¦¼ à¦¯à¦¾à¦°  à¦¯à¦¾à¦°à¦¾ à¦¯à¦¾à§Ÿ à¦¯à¦¿à¦¨à¦¿ à¦¯à§‡ à¦¯à§‡à¦–à¦¾à¦¨à§‡ à¦¯à§‡à¦¤à§‡ à¦¯à§‡à¦¨\nà¦¯à§‡à¦®à¦¨\nà¦°à¦•à¦® à¦°à¦¯à¦¼à§‡à¦›à§‡ à¦°à¦¾à¦–à¦¾ à¦°à§‡à¦–à§‡ à¦°à§Ÿà§‡à¦›à§‡\nà¦²à¦•à§à¦·\nà¦¶à§à¦§à§ à¦¶à§à¦°à§\nà¦¸à¦¾à¦§à¦¾à¦°à¦£ à¦¸à¦¾à¦®à¦¨à§‡ à¦¸à¦™à§à¦—à§‡ à¦¸à¦™à§à¦—à§‡à¦“ à¦¸à¦¬ à¦¸à¦¬à¦¾à¦° à¦¸à¦®à¦¸à§à¦¤ à¦¸à¦®à§à¦ªà§à¦°à¦¤à¦¿ à¦¸à¦®à§Ÿ à¦¸à¦¹ à¦¸à¦¹à¦¿à¦¤ à¦¸à¦¾à¦¥à§‡ à¦¸à§à¦¤à¦°à¦¾à¦‚ à¦¸à§‡  à¦¸à§‡à¦‡ à¦¸à§‡à¦–à¦¾à¦¨ à¦¸à§‡à¦–à¦¾à¦¨à§‡  à¦¸à§‡à¦Ÿà¦¾ à¦¸à§‡à¦Ÿà¦¾à¦‡ à¦¸à§‡à¦Ÿà¦¾à¦“ à¦¸à§‡à¦Ÿà¦¿ à¦¸à§à¦ªà¦·à§à¦Ÿ à¦¸à§à¦¬à¦¯à¦¼à¦‚\nà¦¹à¦‡à¦¤à§‡ à¦¹à¦‡à¦¬à§‡ à¦¹à¦‡à¦¯à¦¼à¦¾ à¦¹à¦“à¦¯à¦¼à¦¾ à¦¹à¦“à¦¯à¦¼à¦¾à¦¯à¦¼ à¦¹à¦“à¦¯à¦¼à¦¾à¦° à¦¹à¦šà§à¦›à§‡ à¦¹à¦¤ à¦¹à¦¤à§‡ à¦¹à¦¤à§‡à¦‡ à¦¹à¦¨ à¦¹à¦¬à§‡ à¦¹à¦¬à§‡à¦¨ à¦¹à¦¯à¦¼ à¦¹à¦¯à¦¼à¦¤à§‹ à¦¹à¦¯à¦¼à¦¨à¦¿ à¦¹à¦¯à¦¼à§‡ à¦¹à¦¯à¦¼à§‡à¦‡ à¦¹à¦¯à¦¼à§‡à¦›à¦¿à¦² à¦¹à¦¯à¦¼à§‡à¦›à§‡ à¦¹à¦¾à¦œà¦¾à¦°\nà¦¹à§Ÿà§‡à¦›à§‡à¦¨ à¦¹à¦² à¦¹à¦²à§‡ à¦¹à¦²à§‡à¦‡ à¦¹à¦²à§‡à¦“ à¦¹à¦²à§‹ à¦¹à¦¿à¦¸à¦¾à¦¬à§‡ à¦¹à¦¿à¦¸à§‡à¦¬à§‡ à¦¹à§ˆà¦²à§‡ à¦¹à§‹à¦• à¦¹à§Ÿ à¦¹à§Ÿà§‡ à¦¹à§Ÿà§‡à¦›à§‡ à¦¹à§ˆà¦¤à§‡ à¦¹à¦‡à§Ÿà¦¾  à¦¹à§Ÿà§‡à¦›à¦¿à¦² à¦¹à§Ÿà§‡à¦›à§‡à¦¨ à¦¹à§Ÿà¦¨à¦¿ à¦¹à§Ÿà§‡à¦‡ à¦¹à§Ÿà¦¤à§‹ à¦¹à¦“à§Ÿà¦¾ à¦¹à¦“à§Ÿà¦¾à¦° à¦¹à¦“à§Ÿà¦¾à§Ÿ\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/bn/tag_map.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/bn/punctuation.py----------------------------------------
A:spacy.lang.bn.punctuation._quotes->char_classes.CONCAT_QUOTES.replace("'", '')


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/bg/__init__.py----------------------------------------
A:spacy.lang.bg.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
spacy.lang.bg.__init__.Bulgarian(Language)
spacy.lang.bg.__init__.BulgarianDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/bg/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/bg/stop_words.py----------------------------------------
A:spacy.lang.bg.stop_words.STOP_WORDS->set('\nĞ°\nĞ°Ğ²Ñ‚ĞµĞ½Ñ‚Ğ¸Ñ‡ĞµĞ½\nĞ°Ğ·\nĞ°ĞºĞ¾\nĞ°Ğ»Ğ°\nĞ±Ğµ\nĞ±ĞµĞ·\nĞ±ĞµÑˆĞµ\nĞ±Ğ¸\nĞ±Ğ¸Ğ²Ñˆ\nĞ±Ğ¸Ğ²ÑˆĞ°\nĞ±Ğ¸Ğ²ÑˆĞ¾\nĞ±Ğ¸Ğ»\nĞ±Ğ¸Ğ»Ğ°\nĞ±Ğ¸Ğ»Ğ¸\nĞ±Ğ¸Ğ»Ğ¾\nĞ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ\nĞ±Ğ»Ğ¸Ğ·Ğ¾\nĞ±ÑŠĞ´Ğ°Ñ‚\nĞ±ÑŠĞ´Ğµ\nĞ±ÑÑ…Ğ°\nĞ²\nĞ²Ğ°Ñ\nĞ²Ğ°Ñˆ\nĞ²Ğ°ÑˆĞ°\nĞ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾\nĞ²ĞµÑ‡Ğµ\nĞ²Ğ·ĞµĞ¼Ğ°\nĞ²Ğ¸\nĞ²Ğ¸Ğµ\nĞ²Ğ¸Ğ½Ğ°Ğ³Ğ¸\nĞ²Ğ½Ğ¸Ğ¼Ğ°Ğ²Ğ°\nĞ²Ñ€ĞµĞ¼Ğµ\nĞ²ÑĞµ\nĞ²ÑĞµĞºĞ¸\nĞ²ÑĞ¸Ñ‡ĞºĞ¸\nĞ²ÑĞ¸Ñ‡ĞºĞ¾\nĞ²ÑÑĞºĞ°\nĞ²ÑŠĞ²\nĞ²ÑŠĞ¿Ñ€ĞµĞºĞ¸\nĞ²ÑŠÑ€Ñ…Ñƒ\nĞ³\nĞ³Ğ¸\nĞ³Ğ»Ğ°Ğ²ĞµĞ½\nĞ³Ğ»Ğ°Ğ²Ğ½Ğ°\nĞ³Ğ»Ğ°Ğ²Ğ½Ğ¾\nĞ³Ğ»Ğ°Ñ\nĞ³Ğ¾\nĞ³Ğ¾Ğ´Ğ¸Ğ½Ğ°\nĞ³Ğ¾Ğ´Ğ¸Ğ½Ğ¸\nĞ³Ğ¾Ğ´Ğ¸ÑˆĞµĞ½\nĞ´\nĞ´Ğ°\nĞ´Ğ°Ğ»Ğ¸\nĞ´Ğ²Ğ°\nĞ´Ğ²Ğ°Ğ¼Ğ°\nĞ´Ğ²Ğ°Ğ¼Ğ°Ñ‚Ğ°\nĞ´Ğ²Ğµ\nĞ´Ğ²ĞµÑ‚Ğµ\nĞ´ĞµĞ½\nĞ´Ğ½ĞµÑ\nĞ´Ğ½Ğ¸\nĞ´Ğ¾\nĞ´Ğ¾Ğ±Ñ€Ğ°\nĞ´Ğ¾Ğ±Ñ€Ğµ\nĞ´Ğ¾Ğ±Ñ€Ğ¾\nĞ´Ğ¾Ğ±ÑŠÑ€\nĞ´Ğ¾ĞºĞ°Ñ‚Ğ¾\nĞ´Ğ¾ĞºĞ¾Ğ³Ğ°\nĞ´Ğ¾Ñ€Ğ¸\nĞ´Ğ¾ÑĞµĞ³Ğ°\nĞ´Ğ¾ÑÑ‚Ğ°\nĞ´Ñ€ÑƒĞ³\nĞ´Ñ€ÑƒĞ³Ğ°\nĞ´Ñ€ÑƒĞ³Ğ¸\nĞµ\nĞµĞ²Ñ‚Ğ¸Ğ½\nĞµĞ´Ğ²Ğ°\nĞµĞ´Ğ¸Ğ½\nĞµĞ´Ğ½Ğ°\nĞµĞ´Ğ½Ğ°ĞºĞ²Ğ°\nĞµĞ´Ğ½Ğ°ĞºĞ²Ğ¸\nĞµĞ´Ğ½Ğ°ĞºÑŠĞ²\nĞµĞ´Ğ½Ğ¾\nĞµĞºĞ¸Ğ¿\nĞµÑ‚Ğ¾\nĞ¶Ğ¸Ğ²Ğ¾Ñ‚\nĞ·Ğ°\nĞ·Ğ°Ğ±Ğ°Ğ²ÑĞ¼\nĞ·Ğ°Ğ´\nĞ·Ğ°ĞµĞ´Ğ½Ğ¾\nĞ·Ğ°Ñ€Ğ°Ğ´Ğ¸\nĞ·Ğ°ÑĞµĞ³Ğ°\nĞ·Ğ°ÑĞ¿Ğ°Ğ»\nĞ·Ğ°Ñ‚Ğ¾Ğ²Ğ°\nĞ·Ğ°Ñ‰Ğ¾\nĞ·Ğ°Ñ‰Ğ¾Ñ‚Ğ¾\nĞ¸\nĞ¸Ğ·\nĞ¸Ğ»Ğ¸\nĞ¸Ğ¼\nĞ¸Ğ¼Ğ°\nĞ¸Ğ¼Ğ°Ñ‚\nĞ¸ÑĞºĞ°\nĞ¹\nĞºĞ°Ğ·Ğ°\nĞºĞ°Ğº\nĞºĞ°ĞºĞ²Ğ°\nĞºĞ°ĞºĞ²Ğ¾\nĞºĞ°ĞºÑ‚Ğ¾\nĞºĞ°ĞºÑŠĞ²\nĞºĞ°Ñ‚Ğ¾\nĞºĞ¾Ğ³Ğ°\nĞºĞ¾Ğ³Ğ°Ñ‚Ğ¾\nĞºĞ¾ĞµÑ‚Ğ¾\nĞºĞ¾Ğ¸Ñ‚Ğ¾\nĞºĞ¾Ğ¹\nĞºĞ¾Ğ¹Ñ‚Ğ¾\nĞºĞ¾Ğ»ĞºĞ¾\nĞºĞ¾ÑÑ‚Ğ¾\nĞºÑŠĞ´Ğµ\nĞºÑŠĞ´ĞµÑ‚Ğ¾\nĞºÑŠĞ¼\nĞ»ĞµÑĞµĞ½\nĞ»ĞµÑĞ½Ğ¾\nĞ»Ğ¸\nĞ»Ğ¾Ñˆ\nĞ¼\nĞ¼Ğ°Ğ¹\nĞ¼Ğ°Ğ»ĞºĞ¾\nĞ¼Ğµ\nĞ¼ĞµĞ¶Ğ´Ñƒ\nĞ¼ĞµĞº\nĞ¼ĞµĞ½\nĞ¼ĞµÑĞµÑ†\nĞ¼Ğ¸\nĞ¼Ğ½Ğ¾Ğ³Ğ¾\nĞ¼Ğ½Ğ¾Ğ·Ğ¸Ğ½Ğ°\nĞ¼Ğ¾Ğ³Ğ°\nĞ¼Ğ¾Ğ³Ğ°Ñ‚\nĞ¼Ğ¾Ğ¶Ğµ\nĞ¼Ğ¾ĞºÑŠÑ€\nĞ¼Ğ¾Ğ»Ñ\nĞ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ğ°\nĞ¼Ñƒ\nĞ½\nĞ½Ğ°\nĞ½Ğ°Ğ´\nĞ½Ğ°Ğ·Ğ°Ğ´\nĞ½Ğ°Ğ¹\nĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ¸\nĞ½Ğ°Ğ¿Ñ€ĞµĞ´\nĞ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€\nĞ½Ğ°Ñ\nĞ½Ğµ\nĞ½ĞµĞ³Ğ¾\nĞ½ĞµÑ‰Ğ¾\nĞ½ĞµÑ\nĞ½Ğ¸\nĞ½Ğ¸Ğµ\nĞ½Ğ¸ĞºĞ¾Ğ¹\nĞ½Ğ¸Ñ‚Ğ¾\nĞ½Ğ¸Ñ‰Ğ¾\nĞ½Ğ¾\nĞ½Ğ¾Ğ²\nĞ½Ğ¾Ğ²Ğ°\nĞ½Ğ¾Ğ²Ğ¸\nĞ½Ğ¾Ğ²Ğ¸Ğ½Ğ°\nĞ½ÑĞºĞ¾Ğ¸\nĞ½ÑĞºĞ¾Ğ¹\nĞ½ÑĞºĞ¾Ğ»ĞºĞ¾\nĞ½ÑĞ¼Ğ°\nĞ¾Ğ±Ğ°Ñ‡Ğµ\nĞ¾ĞºĞ¾Ğ»Ğ¾\nĞ¾ÑĞ²ĞµĞ½\nĞ¾ÑĞ¾Ğ±ĞµĞ½Ğ¾\nĞ¾Ñ‚\nĞ¾Ñ‚Ğ³Ğ¾Ñ€Ğµ\nĞ¾Ñ‚Ğ½Ğ¾Ğ²Ğ¾\nĞ¾Ñ‰Ğµ\nĞ¿Ğ°Ğº\nĞ¿Ğ¾\nĞ¿Ğ¾Ğ²ĞµÑ‡Ğµ\nĞ¿Ğ¾Ğ²ĞµÑ‡ĞµÑ‚Ğ¾\nĞ¿Ğ¾Ğ´\nĞ¿Ğ¾Ğ½Ğµ\nĞ¿Ğ¾Ñ€Ğ°Ğ´Ğ¸\nĞ¿Ğ¾ÑĞ»Ğµ\nĞ¿Ğ¾Ñ‡Ñ‚Ğ¸\nĞ¿Ñ€Ğ°Ğ²Ğ¸\nĞ¿Ñ€ĞµĞ´\nĞ¿Ñ€ĞµĞ´Ğ¸\nĞ¿Ñ€ĞµĞ·\nĞ¿Ñ€Ğ¸\nĞ¿ÑŠĞº\nĞ¿ÑŠÑ€Ğ²Ğ°Ñ‚Ğ°\nĞ¿ÑŠÑ€Ğ²Ğ¸\nĞ¿ÑŠÑ€Ğ²Ğ¾\nĞ¿ÑŠÑ‚Ğ¸\nÑ€Ğ°Ğ²ĞµĞ½\nÑ€Ğ°Ğ²Ğ½Ğ°\nÑ\nÑĞ°\nÑĞ°Ğ¼\nÑĞ°Ğ¼Ğ¾\nÑĞµ\nÑĞµĞ³Ğ°\nÑĞ¸\nÑĞ¸Ğ½\nÑĞºĞ¾Ñ€Ğ¾\nÑĞ»ĞµĞ´\nÑĞ»ĞµĞ´Ğ²Ğ°Ñ‰\nÑĞ¼Ğµ\nÑĞ¼ÑÑ…\nÑĞ¿Ğ¾Ñ€ĞµĞ´\nÑÑ€ĞµĞ´\nÑÑ€ĞµÑ‰Ñƒ\nÑÑ‚Ğµ\nÑÑŠĞ¼\nÑÑŠÑ\nÑÑŠÑ‰Ğ¾\nÑ‚\nÑ‚Ğ°Ğ·Ğ¸\nÑ‚Ğ°ĞºĞ°\nÑ‚Ğ°ĞºĞ¸Ğ²Ğ°\nÑ‚Ğ°ĞºÑŠĞ²\nÑ‚Ğ°Ğ¼\nÑ‚Ğ²Ğ¾Ğ¹\nÑ‚Ğµ\nÑ‚ĞµĞ·Ğ¸\nÑ‚Ğ¸\nÑ‚.Ğ½.\nÑ‚Ğ¾\nÑ‚Ğ¾Ğ²Ğ°\nÑ‚Ğ¾Ğ³Ğ°Ğ²Ğ°\nÑ‚Ğ¾Ğ·Ğ¸\nÑ‚Ğ¾Ğ¹\nÑ‚Ğ¾Ğ»ĞºĞ¾Ğ²Ğ°\nÑ‚Ğ¾Ñ‡Ğ½Ğ¾\nÑ‚Ñ€Ğ¸\nÑ‚Ñ€ÑĞ±Ğ²Ğ°\nÑ‚ÑƒĞº\nÑ‚ÑŠĞ¹\nÑ‚Ñ\nÑ‚ÑÑ…\nÑƒ\nÑƒÑ‚Ñ€Ğµ\nÑ…Ğ°Ñ€ĞµÑĞ²Ğ°\nÑ…Ğ¸Ğ»ÑĞ´Ğ¸\nÑ‡\nÑ‡Ğ°ÑĞ°\nÑ‡Ğµ\nÑ‡ĞµÑÑ‚Ğ¾\nÑ‡Ñ€ĞµĞ·\nÑ‰Ğµ\nÑ‰Ğ¾Ğ¼\nÑĞ¼Ñ€ÑƒĞº\nÑ\nÑĞº\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/lt/__init__.py----------------------------------------
A:spacy.lang.lt.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.lt.__init__.lex_attr_getters[NORM]->add_lookups(Language.Defaults.lex_attr_getters[NORM], BASE_NORMS)
A:spacy.lang.lt.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS, TOKENIZER_EXCEPTIONS)
spacy.lang.lt.__init__.Lithuanian(Language)
spacy.lang.lt.__init__.LithuanianDefaults(Language.Defaults)
spacy.lang.lt.__init__._return_lt(_)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/lt/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/lt/lex_attrs.py----------------------------------------
A:spacy.lang.lt.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.lt.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.lt.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/lt/tokenizer_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/lt/morph_rules.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/lt/stop_words.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/lt/tag_map.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/da/__init__.py----------------------------------------
A:spacy.lang.da.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.da.__init__.lex_attr_getters[NORM]->add_lookups(Language.Defaults.lex_attr_getters[NORM], BASE_NORMS, NORM_EXCEPTIONS)
A:spacy.lang.da.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS, TOKENIZER_EXCEPTIONS)
spacy.lang.da.__init__.Danish(Language)
spacy.lang.da.__init__.DanishDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/da/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/da/lex_attrs.py----------------------------------------
A:spacy.lang.da.lex_attrs._num_words->'nul\nen et to tre fire fem seks syv otte ni ti\nelleve tolv tretten fjorten femten seksten sytten atten nitten tyve\nenogtyve toogtyve treogtyve fireogtyve femogtyve seksogtyve syvogtyve otteogtyve niogtyve tredive\nenogtredive toogtredive treogtredive fireogtredive femogtredive seksogtredive syvogtredive otteogtredive niogtredive fyrre\nenogfyrre toogfyrre treogfyrre fireogfyrre femgogfyrre seksogfyrre syvogfyrre otteogfyrre niogfyrre halvtreds\nenoghalvtreds tooghalvtreds treoghalvtreds fireoghalvtreds femoghalvtreds seksoghalvtreds syvoghalvtreds otteoghalvtreds nioghalvtreds tres\nenogtres toogtres treogtres fireogtres femogtres seksogtres syvogtres otteogtres niogtres halvfjerds\nenoghalvfjerds tooghalvfjerds treoghalvfjerds fireoghalvfjerds femoghalvfjerds seksoghalvfjerds syvoghalvfjerds otteoghalvfjerds nioghalvfjerds firs\nenogfirs toogfirs treogfirs fireogfirs femogfirs seksogfirs syvogfirs otteogfirs niogfirs halvfems\nenoghalvfems tooghalvfems treoghalvfems fireoghalvfems femoghalvfems seksoghalvfems syvoghalvfems otteoghalvfems nioghalvfems hundrede\nmillion milliard billion billiard trillion trilliard\n'.split()
A:spacy.lang.da.lex_attrs._ordinal_words->'nulte\nfÃ¸rste anden tredje fjerde femte sjette syvende ottende niende tiende\nelfte tolvte trettende fjortende femtende sekstende syttende attende nittende tyvende\nenogtyvende toogtyvende treogtyvende fireogtyvende femogtyvende seksogtyvende syvogtyvende otteogtyvende niogtyvende tredivte enogtredivte toogtredivte treogtredivte fireogtredivte femogtredivte seksogtredivte syvogtredivte otteogtredivte niogtredivte fyrretyvende\nenogfyrretyvende toogfyrretyvende treogfyrretyvende fireogfyrretyvende femogfyrretyvende seksogfyrretyvende syvogfyrretyvende otteogfyrretyvende niogfyrretyvende halvtredsindstyvende enoghalvtredsindstyvende\ntooghalvtredsindstyvende treoghalvtredsindstyvende fireoghalvtredsindstyvende femoghalvtredsindstyvende seksoghalvtredsindstyvende syvoghalvtredsindstyvende otteoghalvtredsindstyvende nioghalvtredsindstyvende\ntresindstyvende enogtresindstyvende toogtresindstyvende treogtresindstyvende fireogtresindstyvende femogtresindstyvende seksogtresindstyvende syvogtresindstyvende otteogtresindstyvende niogtresindstyvende halvfjerdsindstyvende\nenoghalvfjerdsindstyvende tooghalvfjerdsindstyvende treoghalvfjerdsindstyvende fireoghalvfjerdsindstyvende femoghalvfjerdsindstyvende seksoghalvfjerdsindstyvende syvoghalvfjerdsindstyvende otteoghalvfjerdsindstyvende nioghalvfjerdsindstyvende firsindstyvende\nenogfirsindstyvende toogfirsindstyvende treogfirsindstyvende fireogfirsindstyvende femogfirsindstyvende seksogfirsindstyvende syvogfirsindstyvende otteogfirsindstyvende niogfirsindstyvende halvfemsindstyvende\nenoghalvfemsindstyvende tooghalvfemsindstyvende treoghalvfemsindstyvende fireoghalvfemsindstyvende femoghalvfemsindstyvende seksoghalvfemsindstyvende syvoghalvfemsindstyvende otteoghalvfemsindstyvende nioghalvfemsindstyvende\n'.split()
A:spacy.lang.da.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.da.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.da.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/da/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.da.tokenizer_exceptions.capitalized->orth.capitalize()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/da/morph_rules.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/da/stop_words.py----------------------------------------
A:spacy.lang.da.stop_words.STOP_WORDS->set('\naf aldrig alene alle allerede alligevel alt altid anden andet andre at\n\nbag begge blandt blev blive bliver burde bÃ¸r\n\nda de dem den denne dens der derefter deres derfor derfra deri dermed derpÃ¥ derved det dette dig din dine disse dog du\n\nefter egen eller ellers en end endnu ene eneste enhver ens enten er et\n\nflere flest fleste for foran fordi forrige fra fÃ¥ fÃ¸r fÃ¸rst\n\ngennem gjorde gjort god gÃ¸r gÃ¸re gÃ¸rende\n\nham han hans har havde have hel heller hen hende hendes henover her herefter heri hermed herpÃ¥ hun hvad hvem hver hvilke hvilken hvilkes hvis hvor hvordan hvorefter hvorfor hvorfra hvorhen hvori hvorimod hvornÃ¥r hvorved\n\ni igen igennem ikke imellem imens imod ind indtil ingen intet\n\njeg jer jeres jo\n\nkan kom kommer kun kunne\n\nlad langs lav lave lavet lidt lige ligesom lille lÃ¦ngere\n\nman mange med meget mellem men mens mere mest mig min mindre mindst mine mit mÃ¥ mÃ¥ske\n\nned nemlig nogen nogensinde noget nogle nok nu ny nyt nÃ¦r nÃ¦ste nÃ¦sten\n\nog ogsÃ¥ om omkring op os over overalt\n\npÃ¥\n\nsamme sammen selv selvom senere ses siden sig sige skal skulle som stadig synes syntes sÃ¥ sÃ¥dan sÃ¥ledes\n\ntemmelig tidligere til tilbage tit\n\nud uden udover under undtagen\n\nvar ved vi via vil ville vore vores vÃ¦r vÃ¦re vÃ¦ret\n\nÃ¸vrigt\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/da/norm_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/da/punctuation.py----------------------------------------
A:spacy.lang.da.punctuation._quotes->char_classes.CONCAT_QUOTES.replace("'", '')


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/uk/__init__.py----------------------------------------
A:spacy.lang.uk.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.uk.__init__.lex_attr_getters[NORM]->add_lookups(Language.Defaults.lex_attr_getters[NORM], BASE_NORMS)
A:spacy.lang.uk.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS, TOKENIZER_EXCEPTIONS)
A:spacy.lang.uk.__init__.lookups->Lookups()
spacy.lang.uk.__init__.Ukrainian(Language)
spacy.lang.uk.__init__.UkrainianDefaults(Language.Defaults)
spacy.lang.uk.__init__.UkrainianDefaults.create_lemmatizer(cls,nlp=None,lookups=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/uk/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/uk/lemmatizer.py----------------------------------------
A:spacy.lang.uk.lemmatizer.UkrainianLemmatizer._morph->MorphAnalyzer(lang='uk')
A:spacy.lang.uk.lemmatizer.univ_pos->self.normalize_univ_pos(univ_pos)
A:spacy.lang.uk.lemmatizer.analyses->self._morph.parse(string)
A:spacy.lang.uk.lemmatizer.(analysis_pos, _)->oc2ud(str(analysis.tag))
A:spacy.lang.uk.lemmatizer.(_, analysis_morph)->oc2ud(str(analysis.tag))
A:spacy.lang.uk.lemmatizer.morphology->dict()
A:spacy.lang.uk.lemmatizer.unmatched->set()
A:spacy.lang.uk.lemmatizer.grams->oc_tag.replace(' ', ',').split(',')
A:spacy.lang.uk.lemmatizer.gram->set().pop()
spacy.lang.uk.UkrainianLemmatizer(self,lookups=None)
spacy.lang.uk.UkrainianLemmatizer.lookup(self,string,orth=None)
spacy.lang.uk.UkrainianLemmatizer.normalize_univ_pos(univ_pos)
spacy.lang.uk.lemmatizer.UkrainianLemmatizer(self,lookups=None)
spacy.lang.uk.lemmatizer.UkrainianLemmatizer.__init__(self,lookups=None)
spacy.lang.uk.lemmatizer.UkrainianLemmatizer.lookup(self,string,orth=None)
spacy.lang.uk.lemmatizer.UkrainianLemmatizer.normalize_univ_pos(univ_pos)
spacy.lang.uk.lemmatizer.oc2ud(oc_tag)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/uk/lex_attrs.py----------------------------------------
A:spacy.lang.uk.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.uk.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.uk.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/uk/tokenizer_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/uk/stop_words.py----------------------------------------
A:spacy.lang.uk.stop_words.STOP_WORDS->set("Ğ°\nĞ°Ğ±Ğ¾\nĞ°Ğ´Ğ¶Ğµ\nĞ°Ğ¶\nĞ°Ğ»Ğµ\nĞ°Ğ»Ğ»Ğ¾\nĞ±\nĞ±Ğ°Ğ³Ğ°Ñ‚Ğ¾\nĞ±ĞµĞ·\nĞ±ĞµĞ·Ğ¿ĞµÑ€ĞµÑ€Ğ²Ğ½Ğ¾\nĞ±Ğ¸\nĞ±Ñ–Ğ»ÑŒÑˆ\nĞ±Ñ–Ğ»ÑŒÑˆĞµ\nĞ±Ñ–Ğ»Ñ\nĞ±Ğ»Ğ¸Ğ·ÑŒĞºĞ¾\nĞ±Ğ¾\nĞ±ÑƒĞ²\nĞ±ÑƒĞ²Ğ°Ñ”\nĞ±ÑƒĞ´Ğµ\nĞ±ÑƒĞ´ĞµĞ¼Ğ¾\nĞ±ÑƒĞ´ĞµÑ‚Ğµ\nĞ±ÑƒĞ´ĞµÑˆ\nĞ±ÑƒĞ´Ñƒ\nĞ±ÑƒĞ´ÑƒÑ‚ÑŒ\nĞ±ÑƒĞ´ÑŒ\nĞ±ÑƒĞ»Ğ°\nĞ±ÑƒĞ»Ğ¸\nĞ±ÑƒĞ»Ğ¾\nĞ±ÑƒÑ‚Ğ¸\nĞ²\nĞ²Ğ°Ğ¼\nĞ²Ğ°Ğ¼Ğ¸\nĞ²Ğ°Ñ\nĞ²Ğ°Ñˆ\nĞ²Ğ°ÑˆĞ°\nĞ²Ğ°ÑˆĞµ\nĞ²Ğ°ÑˆĞ¸Ğ¼\nĞ²Ğ°ÑˆĞ¸Ğ¼Ğ¸\nĞ²Ğ°ÑˆĞ¸Ñ…\nĞ²Ğ°ÑˆÑ–\nĞ²Ğ°ÑˆÑ–Ğ¹\nĞ²Ğ°ÑˆĞ¾Ğ³Ğ¾\nĞ²Ğ°ÑˆĞ¾Ñ—\nĞ²Ğ°ÑˆĞ¾Ğ¼Ñƒ\nĞ²Ğ°ÑˆĞ¾Ñ\nĞ²Ğ°ÑˆÑƒ\nĞ²Ğ³Ğ¾Ñ€Ñ–\nĞ²Ğ³Ğ¾Ñ€Ñƒ\nĞ²Ğ´Ğ°Ğ»Ğ¸Ğ½Ñ–\nĞ²ĞµÑÑŒ\nĞ²Ğ¶Ğµ\nĞ²Ğ¸\nĞ²Ñ–Ğ´\nĞ²Ñ–Ğ´ÑĞ¾Ñ‚ĞºÑ–Ğ²\nĞ²Ñ–Ğ½\nĞ²Ñ–ÑÑ–Ğ¼\nĞ²Ñ–ÑÑ–Ğ¼Ğ½Ğ°Ğ´Ñ†ÑÑ‚Ğ¸Ğ¹\nĞ²Ñ–ÑÑ–Ğ¼Ğ½Ğ°Ğ´Ñ†ÑÑ‚ÑŒ\nĞ²Ğ½Ğ¸Ğ·\nĞ²Ğ½Ğ¸Ğ·Ñƒ\nĞ²Ğ¾Ğ½Ğ°\nĞ²Ğ¾Ğ½Ğ¸\nĞ²Ğ¾Ğ½Ğ¾\nĞ²Ğ¾ÑÑŒĞ¼Ğ¸Ğ¹\nĞ²ÑĞµ\nĞ²ÑĞµÑ\nĞ²ÑÑ–\nĞ²ÑÑ–Ğ¼\nĞ²ÑÑ–Ñ…\nĞ²ÑÑŒĞ¾Ğ³Ğ¾\nĞ²ÑÑŒĞ¾Ğ¼Ñƒ\nĞ²ÑÑ\nĞ²ÑÑ\nĞ²Ñ‚Ñ–Ğ¼\nĞ³\nĞ³ĞµÑ‚ÑŒ\nĞ³Ğ¾Ğ²Ğ¾Ñ€Ğ¸Ğ²\nĞ³Ğ¾Ğ²Ğ¾Ñ€Ğ¸Ñ‚ÑŒ\nĞ´Ğ°Ğ²Ğ½Ğ¾\nĞ´Ğ°Ğ»ĞµĞºĞ¾\nĞ´Ğ°Ğ»Ñ–\nĞ´Ğ°Ñ€Ğ¼Ğ°\nĞ´Ğ²Ğ°\nĞ´Ğ²Ğ°Ğ´Ñ†ÑÑ‚Ğ¸Ğ¹\nĞ´Ğ²Ğ°Ğ´Ñ†ÑÑ‚ÑŒ\nĞ´Ğ²Ğ°Ğ½Ğ°Ğ´Ñ†ÑÑ‚Ğ¸Ğ¹\nĞ´Ğ²Ğ°Ğ½Ğ°Ğ´Ñ†ÑÑ‚ÑŒ\nĞ´Ğ²Ñ–\nĞ´Ğ²Ğ¾Ñ…\nĞ´Ğµ\nĞ´ĞµĞ²'ÑÑ‚Ğ¸Ğ¹\nĞ´ĞµĞ²'ÑÑ‚Ğ½Ğ°Ğ´Ñ†ÑÑ‚Ğ¸Ğ¹\nĞ´ĞµĞ²'ÑÑ‚Ğ½Ğ°Ğ´Ñ†ÑÑ‚ÑŒ\nĞ´ĞµĞ²'ÑÑ‚ÑŒ\nĞ´ĞµĞºÑ–Ğ»ÑŒĞºĞ°\nĞ´ĞµĞ½ÑŒ\nĞ´ĞµÑÑÑ‚Ğ¸Ğ¹\nĞ´ĞµÑÑÑ‚ÑŒ\nĞ´Ñ–Ğ¹ÑĞ½Ğ¾\nĞ´Ğ»Ñ\nĞ´Ğ½Ñ\nĞ´Ğ¾\nĞ´Ğ¾Ğ±Ñ€Ğµ\nĞ´Ğ¾Ğ²Ğ³Ğ¾\nĞ´Ğ¾ĞºĞ¸\nĞ´Ğ¾ÑĞ¸Ñ‚ÑŒ\nĞ´Ñ€ÑƒĞ³Ğ¸Ğ¹\nĞ´ÑƒĞ¶Ğµ\nĞ´ÑĞºÑƒÑ\nĞµ\nÑ”\nĞ¶\nĞ¶Ğµ\nĞ·\nĞ·Ğ°\nĞ·Ğ°Ğ²Ğ¶Ğ´Ğ¸\nĞ·Ğ°Ğ·Ğ²Ğ¸Ñ‡Ğ°Ğ¹\nĞ·Ğ°Ğ½Ğ°Ğ´Ñ‚Ğ¾\nĞ·Ğ°Ñ€Ğ°Ğ·\nĞ·Ğ°Ñ‚Ğµ\nĞ·Ğ²Ğ¸Ñ‡Ğ°Ğ¹Ğ½Ğ¾\nĞ·Ğ²Ñ–Ğ´ÑĞ¸\nĞ·Ğ²Ñ–Ğ´ÑƒÑÑ–Ğ»ÑŒ\nĞ·Ğ´Ğ°Ñ”Ñ‚ÑŒÑÑ\nĞ·Ñ–\nĞ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ÑŒ\nĞ·Ğ½Ğ¾Ğ²Ñƒ\nĞ·Ğ¾Ğ²ÑÑ–Ğ¼\nÑ–\nÑ–Ğ·\nÑ—Ñ—\nÑ—Ğ¹\nÑ—Ğ¼\nÑ–Ğ½Ğ¾Ğ´Ñ–\nÑ–Ğ½ÑˆĞ°\nÑ–Ğ½ÑˆĞµ\nÑ–Ğ½ÑˆĞ¸Ğ¹\nÑ–Ğ½ÑˆĞ¸Ñ…\nÑ–Ğ½ÑˆÑ–\nÑ—Ñ…\nĞ¹\nĞ¹Ğ¾Ğ³Ğ¾\nĞ¹Ğ¾Ğ¼Ñƒ\nĞºĞ°Ğ¶Ğµ\nĞºĞ¸Ğ¼\nĞºÑ–Ğ»ÑŒĞºĞ°\nĞºĞ¾Ğ³Ğ¾\nĞºĞ¾Ğ¶ĞµĞ½\nĞºĞ¾Ğ¶Ğ½Ğ°\nĞºĞ¾Ğ¶Ğ½Ğµ\nĞºĞ¾Ğ¶Ğ½Ñ–\nĞºĞ¾Ğ»Ğ¸\nĞºĞ¾Ğ¼Ñƒ\nĞºÑ€Ğ°Ñ‰Ğµ\nĞºÑ€Ñ–Ğ¼\nĞºÑƒĞ´Ğ¸\nĞ»Ğ°ÑĞºĞ°\nĞ»ĞµĞ´Ğ²Ğµ\nĞ»Ğ¸ÑˆĞµ\nĞ¼\nĞ¼Ğ°Ñ”\nĞ¼Ğ°Ğ¹Ğ¶Ğµ\nĞ¼Ğ°Ğ»Ğ¾\nĞ¼Ğ°Ñ‚Ğ¸\nĞ¼ĞµĞ½Ğµ\nĞ¼ĞµĞ½Ñ–\nĞ¼ĞµĞ½Ñˆ\nĞ¼ĞµĞ½ÑˆĞµ\nĞ¼Ğ¸\nĞ¼Ğ¸Ğ¼Ğ¾\nĞ¼Ñ–Ğ³\nĞ¼Ñ–Ğ¶\nĞ¼Ñ–Ğ¹\nĞ¼Ñ–Ğ»ÑŒĞ¹Ğ¾Ğ½Ñ–Ğ²\nĞ¼Ğ½Ğ¾Ñ\nĞ¼Ğ¾Ğ³Ğ¾\nĞ¼Ğ¾Ğ³Ñ‚Ğ¸\nĞ¼Ğ¾Ñ”\nĞ¼Ğ¾Ñ”Ñ—\nĞ¼Ğ¾Ñ”Ğ¼Ñƒ\nĞ¼Ğ¾Ñ”Ñ\nĞ¼Ğ¾Ğ¶Ğµ\nĞ¼Ğ¾Ğ¶Ğ½Ğ°\nĞ¼Ğ¾Ğ¶Ğ½Ğ¾\nĞ¼Ğ¾Ğ¶ÑƒÑ‚ÑŒ\nĞ¼Ğ¾Ñ—\nĞ¼Ğ¾Ñ—Ğ¹\nĞ¼Ğ¾Ñ—Ğ¼\nĞ¼Ğ¾Ñ—Ğ¼Ğ¸\nĞ¼Ğ¾Ñ—Ñ…\nĞ¼Ğ¾Ñ\nĞ¼Ğ¾Ñ\nĞ½Ğ°\nĞ½Ğ°Ğ²Ñ–Ñ‚ÑŒ\nĞ½Ğ°Ğ²Ñ–Ñ‰Ğ¾\nĞ½Ğ°Ğ²ĞºĞ¾Ğ»Ğ¾\nĞ½Ğ°Ğ²ĞºÑ€ÑƒĞ³Ğ¸\nĞ½Ğ°Ğ³Ğ¾Ñ€Ñ–\nĞ½Ğ°Ğ´\nĞ½Ğ°Ğ·Ğ°Ğ´\nĞ½Ğ°Ğ¹Ğ±Ñ–Ğ»ÑŒÑˆ\nĞ½Ğ°Ğ¼\nĞ½Ğ°Ğ¼Ğ¸\nĞ½Ğ°Ñ€ĞµÑˆÑ‚Ñ–\nĞ½Ğ°Ñ\nĞ½Ğ°Ñˆ\nĞ½Ğ°ÑˆĞ°\nĞ½Ğ°ÑˆĞµ\nĞ½Ğ°ÑˆĞ¸Ğ¼\nĞ½Ğ°ÑˆĞ¸Ğ¼Ğ¸\nĞ½Ğ°ÑˆĞ¸Ñ…\nĞ½Ğ°ÑˆÑ–\nĞ½Ğ°ÑˆÑ–Ğ¹\nĞ½Ğ°ÑˆĞ¾Ğ³Ğ¾\nĞ½Ğ°ÑˆĞ¾Ñ—\nĞ½Ğ°ÑˆĞ¾Ğ¼Ñƒ\nĞ½Ğ°ÑˆĞ¾Ñ\nĞ½Ğ°ÑˆÑƒ\nĞ½Ğµ\nĞ½ĞµĞ±Ğ°Ğ³Ğ°Ñ‚Ğ¾\nĞ½ĞµĞ±ÑƒĞ´ÑŒ\nĞ½ĞµĞ´Ğ°Ğ»ĞµĞºĞ¾\nĞ½ĞµÑ—\nĞ½ĞµĞ¼Ğ°Ñ”\nĞ½ĞµÑ€Ñ–Ğ´ĞºĞ¾\nĞ½ĞµÑ‰Ğ¾Ğ´Ğ°Ğ²Ğ½Ğ¾\nĞ½ĞµÑ\nĞ½Ğ¸Ğ±ÑƒĞ´ÑŒ\nĞ½Ğ¸Ğ¶Ñ‡Ğµ\nĞ½Ğ¸Ğ·ÑŒĞºĞ¾\nĞ½Ğ¸Ğ¼\nĞ½Ğ¸Ğ¼Ğ¸\nĞ½Ğ¸Ñ…\nĞ½Ñ–\nĞ½Ñ–Ğ±Ğ¸\nĞ½Ñ–Ğ¶\nĞ½Ñ–Ğ¹\nĞ½Ñ–ĞºĞ¾Ğ»Ğ¸\nĞ½Ñ–ĞºÑƒĞ´Ğ¸\nĞ½Ñ–Ğ¼\nĞ½Ñ–Ñ‡Ğ¾Ğ³Ğ¾\nĞ½Ñƒ\nĞ½ÑŒĞ¾Ğ³Ğ¾\nĞ½ÑŒĞ¾Ğ¼Ñƒ\nĞ¾\nĞ¾Ğ±Ğ¸Ğ´Ğ²Ğ°\nĞ¾Ğ±Ğ¾Ñ”\nĞ¾Ğ´Ğ¸Ğ½\nĞ¾Ğ´Ğ¸Ğ½Ğ°Ğ´Ñ†ÑÑ‚Ğ¸Ğ¹\nĞ¾Ğ´Ğ¸Ğ½Ğ°Ğ´Ñ†ÑÑ‚ÑŒ\nĞ¾Ğ´Ğ½Ğ°Ğº\nĞ¾Ğ´Ğ½Ñ–Ñ”Ñ—\nĞ¾Ğ´Ğ½Ñ–Ğ¹\nĞ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾\nĞ¾Ğ·Ğ½Ğ°Ñ‡Ğ°Ñ”\nĞ¾ĞºÑ€Ñ–Ğ¼\nĞ¾Ğ½\nĞ¾ÑĞ¾Ğ±Ğ»Ğ¸Ğ²Ğ¾\nĞ¾ÑÑŒ\nĞ¿'ÑÑ‚Ğ¸Ğ¹\nĞ¿'ÑÑ‚Ğ½Ğ°Ğ´Ñ†ÑÑ‚Ğ¸Ğ¹\nĞ¿'ÑÑ‚Ğ½Ğ°Ğ´Ñ†ÑÑ‚ÑŒ\nĞ¿'ÑÑ‚ÑŒ\nĞ¿ĞµÑ€ĞµĞ´\nĞ¿ĞµÑ€ÑˆĞ¸Ğ¹\nĞ¿Ñ–Ğ´\nĞ¿Ñ–Ğ·Ğ½Ñ–ÑˆĞµ\nĞ¿Ñ–Ñ€\nĞ¿Ñ–ÑĞ»Ñ\nĞ¿Ğ¾\nĞ¿Ğ¾Ğ²Ğ¸Ğ½Ğ½Ğ¾\nĞ¿Ğ¾Ğ´Ñ–Ğ²\nĞ¿Ğ¾ĞºĞ¸\nĞ¿Ğ¾Ñ€Ğ°\nĞ¿Ğ¾Ñ€ÑƒÑ‡\nĞ¿Ğ¾ÑĞµÑ€ĞµĞ´\nĞ¿Ğ¾Ñ‚Ñ–Ğ¼\nĞ¿Ğ¾Ñ‚Ñ€Ñ–Ğ±Ğ½Ğ¾\nĞ¿Ğ¾Ñ‡Ğ°Ğ»Ğ°\nĞ¿Ğ¾Ñ‡Ğ°Ñ‚ĞºÑƒ\nĞ¿Ñ€Ğ¸\nĞ¿Ñ€Ğ¾\nĞ¿Ñ€Ğ¾ÑÑ‚Ğ¾\nĞ¿Ñ€Ğ¾Ñ‚Ğµ\nĞ¿Ñ€Ğ¾Ñ‚Ğ¸\nÑ€Ğ°Ğ·\nÑ€Ğ°Ğ·Ñƒ\nÑ€Ğ°Ğ½Ñ–ÑˆĞµ\nÑ€Ğ°Ğ½Ğ¾\nÑ€Ğ°Ğ¿Ñ‚Ğ¾Ğ¼\nÑ€Ñ–Ğº\nÑ€Ğ¾ĞºĞ¸\nÑ€Ğ¾ĞºÑ–Ğ²\nÑ€Ğ¾ĞºÑƒ\nÑ€Ğ¾Ñ†Ñ–\nÑĞ°Ğ¼\nÑĞ°Ğ¼Ğ°\nÑĞ°Ğ¼Ğµ\nÑĞ°Ğ¼Ğ¸Ğ¼\nÑĞ°Ğ¼Ğ¸Ğ¼Ğ¸\nÑĞ°Ğ¼Ğ¸Ñ…\nÑĞ°Ğ¼Ñ–\nÑĞ°Ğ¼Ñ–Ğ¹\nÑĞ°Ğ¼Ğ¾\nÑĞ°Ğ¼Ğ¾Ğ³Ğ¾\nÑĞ°Ğ¼Ğ¾Ğ¼Ñƒ\nÑĞ°Ğ¼Ñƒ\nÑĞ²Ğ¾Ğ³Ğ¾\nÑĞ²Ğ¾Ñ”\nÑĞ²Ğ¾Ñ”Ñ—\nÑĞ²Ğ¾Ñ—\nÑĞ²Ğ¾Ñ—Ğ¹\nÑĞ²Ğ¾Ñ—Ñ…\nÑĞ²Ğ¾Ñ\nÑĞµĞ±Ğµ\nÑĞ¸Ñ…\nÑÑ–Ğ¼\nÑÑ–Ğ¼Ğ½Ğ°Ğ´Ñ†ÑÑ‚Ğ¸Ğ¹\nÑÑ–Ğ¼Ğ½Ğ°Ğ´Ñ†ÑÑ‚ÑŒ\nÑĞºĞ°Ğ·Ğ°Ğ²\nÑĞºĞ°Ğ·Ğ°Ğ»Ğ°\nÑĞºĞ°Ğ·Ğ°Ñ‚Ğ¸\nÑĞºÑ–Ğ»ÑŒĞºĞ¸\nÑĞºÑ€Ñ–Ğ·ÑŒ\nÑĞ¾Ğ±Ñ–\nÑĞ¾Ğ±Ğ¾Ñ\nÑĞ¿Ğ°ÑĞ¸Ğ±Ñ–\nÑĞ¿Ğ¾Ñ‡Ğ°Ñ‚ĞºÑƒ\nÑĞ¿Ñ€Ğ°Ğ²\nÑÑ‚Ğ°Ğ²\nÑÑƒÑ‚ÑŒ\nÑÑŒĞ¾Ğ³Ğ¾Ğ´Ğ½Ñ–\nÑÑŒĞ¾Ğ¼Ğ¸Ğ¹\nÑ‚\nÑ‚Ğ°\nÑ‚Ğ°Ğº\nÑ‚Ğ°ĞºĞ°\nÑ‚Ğ°ĞºĞµ\nÑ‚Ğ°ĞºĞ¸Ğ¹\nÑ‚Ğ°ĞºÑ–\nÑ‚Ğ°ĞºĞ¾Ğ¶\nÑ‚Ğ°Ğ¼\nÑ‚Ğ²Ñ–Ğ¹\nÑ‚Ğ²Ğ¾Ğ³Ğ¾\nÑ‚Ğ²Ğ¾Ñ”\nÑ‚Ğ²Ğ¾Ñ”Ñ—\nÑ‚Ğ²Ğ¾Ñ”Ğ¼Ñƒ\nÑ‚Ğ²Ğ¾Ñ”Ñ\nÑ‚Ğ²Ğ¾Ñ—\nÑ‚Ğ²Ğ¾Ñ—Ğ¹\nÑ‚Ğ²Ğ¾Ñ—Ğ¼\nÑ‚Ğ²Ğ¾Ñ—Ğ¼Ğ¸\nÑ‚Ğ²Ğ¾Ñ—Ñ…\nÑ‚Ğ²Ğ¾Ñ\nÑ‚Ğ²Ğ¾Ñ\nÑ‚Ğµ\nÑ‚ĞµĞ±Ğµ\nÑ‚ĞµĞ¶\nÑ‚ĞµĞ¿ĞµÑ€\nÑ‚Ğ¸\nÑ‚Ğ¸Ğ¼\nÑ‚Ğ¸Ğ¼Ğ¸\nÑ‚Ğ¸ÑÑÑ‡\nÑ‚Ğ¸Ñ…\nÑ‚Ñ–\nÑ‚Ñ–Ñ”Ñ—\nÑ‚Ñ–Ñ”Ñ\nÑ‚Ñ–Ğ¹\nÑ‚Ñ–Ğ»ÑŒĞºĞ¸\nÑ‚Ñ–Ğ¼\nÑ‚Ğ¾\nÑ‚Ğ¾Ğ±Ñ–\nÑ‚Ğ¾Ğ±Ğ¾Ñ\nÑ‚Ğ¾Ğ³Ğ¾\nÑ‚Ğ¾Ğ´Ñ–\nÑ‚Ğ¾Ğ¹\nÑ‚Ğ¾Ğ¼Ñƒ\nÑ‚Ğ¾Ñ\nÑ‚Ñ€ĞµĞ±Ğ°\nÑ‚Ñ€ĞµÑ‚Ñ–Ğ¹\nÑ‚Ñ€Ğ¸\nÑ‚Ñ€Ğ¸Ğ½Ğ°Ğ´Ñ†ÑÑ‚Ğ¸Ğ¹\nÑ‚Ñ€Ğ¸Ğ½Ğ°Ğ´Ñ†ÑÑ‚ÑŒ\nÑ‚Ñ€Ğ¾Ñ…Ğ¸\nÑ‚Ñƒ\nÑ‚ÑƒĞ´Ğ¸\nÑ‚ÑƒÑ‚\nÑƒ\nÑƒĞ²ĞµÑÑŒ\nÑƒĞ¼Ñ–Ñ‚Ğ¸\nÑƒÑĞµ\nÑƒÑÑ–\nÑƒÑÑ–Ğ¼\nÑƒÑÑ–Ğ¼Ğ°\nÑƒÑÑ–Ñ…\nÑƒÑÑŒĞ¾Ğ³Ğ¾\nÑƒÑÑŒĞ¾Ğ¼Ñƒ\nÑƒÑÑ\nÑƒÑÑĞ´Ğ¸\nÑƒÑÑ\nÑ…Ñ–Ğ±Ğ°\nÑ…Ğ¾Ñ‚Ñ–Ñ‚Ğ¸\nÑ…Ğ¾Ñ‡\nÑ…Ğ¾Ñ‡Ğ°\nÑ…Ğ¾Ñ‡ĞµÑˆ\nÑ…Ñ‚Ğ¾\nÑ†Ğµ\nÑ†ĞµĞ¹\nÑ†Ğ¸Ğ¼\nÑ†Ğ¸Ğ¼Ğ¸\nÑ†Ğ¸Ñ…\nÑ†Ñ–\nÑ†Ñ–Ñ”Ñ—\nÑ†Ñ–Ğ¹\nÑ†ÑŒĞ¾Ğ³Ğ¾\nÑ†ÑŒĞ¾Ğ¼Ñƒ\nÑ†Ñ\nÑ†Ñ\nÑ‡Ğ°Ñ\nÑ‡Ğ°ÑÑ‚Ñ–ÑˆĞµ\nÑ‡Ğ°ÑÑ‚Ğ¾\nÑ‡Ğ°ÑÑƒ\nÑ‡ĞµÑ€ĞµĞ·\nÑ‡ĞµÑ‚Ğ²ĞµÑ€Ñ‚Ğ¸Ğ¹\nÑ‡Ğ¸\nÑ‡Ğ¸Ñ”\nÑ‡Ğ¸Ñ”Ñ—\nÑ‡Ğ¸Ñ”Ğ¼Ñƒ\nÑ‡Ğ¸Ñ—\nÑ‡Ğ¸Ñ—Ğ¹\nÑ‡Ğ¸Ñ—Ğ¼\nÑ‡Ğ¸Ñ—Ğ¼Ğ¸\nÑ‡Ğ¸Ñ—Ñ…\nÑ‡Ğ¸Ğ¹\nÑ‡Ğ¸Ğ¹Ğ¾Ğ³Ğ¾\nÑ‡Ğ¸Ğ¹Ğ¾Ğ¼Ñƒ\nÑ‡Ğ¸Ğ¼\nÑ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ğ°\nÑ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ğµ\nÑ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ğ¸Ğ¹\nÑ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ñ–\nÑ‡Ğ¸Ñ\nÑ‡Ğ¸Ñ\nÑ‡Ğ¾Ğ³Ğ¾\nÑ‡Ğ¾Ğ¼Ñƒ\nÑ‡Ğ¾Ñ‚Ğ¸Ñ€Ğ¸\nÑ‡Ğ¾Ñ‚Ğ¸Ñ€Ğ½Ğ°Ğ´Ñ†ÑÑ‚Ğ¸Ğ¹\nÑ‡Ğ¾Ñ‚Ğ¸Ñ€Ğ½Ğ°Ğ´Ñ†ÑÑ‚ÑŒ\nÑˆÑ–ÑÑ‚Ğ½Ğ°Ğ´Ñ†ÑÑ‚Ğ¸Ğ¹\nÑˆÑ–ÑÑ‚Ğ½Ğ°Ğ´Ñ†ÑÑ‚ÑŒ\nÑˆÑ–ÑÑ‚ÑŒ\nÑˆĞ¾ÑÑ‚Ğ¸Ğ¹\nÑ‰Ğµ\nÑ‰Ğ¾\nÑ‰Ğ¾Ğ±\nÑ‰Ğ¾Ğ´Ğ¾\nÑ‰Ğ¾ÑÑŒ\nÑ\nÑĞº\nÑĞºĞ°\nÑĞºĞ¸Ğ¹\nÑĞºĞ¸Ñ…\nÑĞºÑ–\nÑĞºÑ–Ğ¹\nÑĞºĞ¾Ğ³Ğ¾\nÑĞºĞ¾Ñ—\nÑĞºÑ‰Ğ¾".split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/uk/tag_map.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/hi/__init__.py----------------------------------------
A:spacy.lang.hi.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
spacy.lang.hi.__init__.Hindi(Language)
spacy.lang.hi.__init__.HindiDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/hi/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/hi/lex_attrs.py----------------------------------------
A:spacy.lang.hi.lex_attrs.length->len(suffix_group[0])
A:spacy.lang.hi.lex_attrs.text->text.replace(', ', '').replace('.', '').replace(', ', '').replace('.', '')
A:spacy.lang.hi.lex_attrs.(num, denom)->text.replace(', ', '').replace('.', '').replace(', ', '').replace('.', '').split('/')
spacy.lang.hi.lex_attrs.like_num(text)
spacy.lang.hi.lex_attrs.norm(string)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/hi/stop_words.py----------------------------------------
A:spacy.lang.hi.stop_words.STOP_WORDS->set('\nà¤…à¤‚à¤¦à¤°\nà¤…à¤¤\nà¤…à¤¦à¤¿\nà¤…à¤ª\nà¤…à¤ªà¤¨à¤¾\nà¤…à¤ªà¤¨à¤¿\nà¤…à¤ªà¤¨à¥€\nà¤…à¤ªà¤¨à¥‡\nà¤…à¤­à¤¿\nà¤…à¤­à¥€\nà¤…à¤‚à¤¦à¤°\nà¤†à¤¦à¤¿\nà¤†à¤ª\nà¤…à¤—à¤°\nà¤‡à¤‚à¤¹à¤¿à¤‚\nà¤‡à¤‚à¤¹à¥‡à¤‚\nà¤‡à¤‚à¤¹à¥‹à¤‚\nà¤‡à¤¤à¤¯à¤¾à¤¦à¤¿\nà¤‡à¤¤à¥à¤¯à¤¾à¤¦à¤¿\nà¤‡à¤¨\nà¤‡à¤¨à¤•à¤¾\nà¤‡à¤¨à¥à¤¹à¥€à¤‚\nà¤‡à¤¨à¥à¤¹à¥‡à¤‚\nà¤‡à¤¨à¥à¤¹à¥‹à¤‚\nà¤‡à¤¸\nà¤‡à¤¸à¤•à¤¾\nà¤‡à¤¸à¤•à¤¿\nà¤‡à¤¸à¤•à¥€\nà¤‡à¤¸à¤•à¥‡\nà¤‡à¤¸à¤®à¥‡à¤‚\nà¤‡à¤¸à¤¿\nà¤‡à¤¸à¥€\nà¤‡à¤¸à¥‡\nà¤‰à¤‚à¤¹à¤¿à¤‚\nà¤‰à¤‚à¤¹à¥‡à¤‚\nà¤‰à¤‚à¤¹à¥‹à¤‚\nà¤‰à¤¨\nà¤‰à¤¨à¤•à¤¾\nà¤‰à¤¨à¤•à¤¿\nà¤‰à¤¨à¤•à¥€\nà¤‰à¤¨à¤•à¥‡\nà¤‰à¤¨à¤•à¥‹\nà¤‰à¤¨à¥à¤¹à¥€à¤‚\nà¤‰à¤¨à¥à¤¹à¥‡à¤‚\nà¤‰à¤¨à¥à¤¹à¥‹à¤‚\nà¤‰à¤¸\nà¤‰à¤¸à¤•à¥‡\nà¤‰à¤¸à¤¿\nà¤‰à¤¸à¥€\nà¤‰à¤¸à¥‡\nà¤à¤•\nà¤à¤µà¤‚\nà¤à¤¸\nà¤à¤¸à¥‡\nà¤à¤¸à¥‡\nà¤“à¤°\nà¤”à¤°\nà¤•à¤‡\nà¤•à¤ˆ\nà¤•à¤°\nà¤•à¤°à¤¤à¤¾\nà¤•à¤°à¤¤à¥‡\nà¤•à¤°à¤¨à¤¾\nà¤•à¤°à¤¨à¥‡\nà¤•à¤°à¥‡à¤‚\nà¤•à¤¹à¤¤à¥‡\nà¤•à¤¹à¤¾\nà¤•à¤¾\nà¤•à¤¾à¤«à¤¿\nà¤•à¤¾à¥à¥€\nà¤•à¤¿\nà¤•à¤¿à¤‚à¤¹à¥‡à¤‚\nà¤•à¤¿à¤‚à¤¹à¥‹à¤‚\nà¤•à¤¿à¤¤à¤¨à¤¾\nà¤•à¤¿à¤¨à¥à¤¹à¥‡à¤‚\nà¤•à¤¿à¤¨à¥à¤¹à¥‹à¤‚\nà¤•à¤¿à¤¯à¤¾\nà¤•à¤¿à¤°\nà¤•à¤¿à¤¸\nà¤•à¤¿à¤¸à¤¿\nà¤•à¤¿à¤¸à¥€\nà¤•à¤¿à¤¸à¥‡\nà¤•à¥€\nà¤•à¥à¤›\nà¤•à¥à¤²\nà¤•à¥‡\nà¤•à¥‹\nà¤•à¥‹à¤‡\nà¤•à¥‹à¤ˆ\nà¤•à¥‹à¤¨\nà¤•à¥‹à¤¨à¤¸à¤¾\nà¤•à¥Œà¤¨\nà¤•à¥Œà¤¨à¤¸à¤¾\nà¤—à¤¯à¤¾\nà¤˜à¤°\nà¤œà¤¬\nà¤œà¤¹à¤¾à¤\nà¤œà¤¹à¤¾à¤‚\nà¤œà¤¾\nà¤œà¤¿à¤‚à¤¹à¥‡à¤‚\nà¤œà¤¿à¤‚à¤¹à¥‹à¤‚\nà¤œà¤¿à¤¤à¤¨à¤¾\nà¤œà¤¿à¤§à¤°\nà¤œà¤¿à¤¨\nà¤œà¤¿à¤¨à¥à¤¹à¥‡à¤‚\nà¤œà¤¿à¤¨à¥à¤¹à¥‹à¤‚\nà¤œà¤¿à¤¸\nà¤œà¤¿à¤¸à¥‡\nà¤œà¥€à¤§à¤°\nà¤œà¥‡à¤¸à¤¾\nà¤œà¥‡à¤¸à¥‡\nà¤œà¥ˆà¤¸à¤¾\nà¤œà¥ˆà¤¸à¥‡\nà¤œà¥‹\nà¤¤à¤•\nà¤¤à¤¬\nà¤¤à¤°à¤¹\nà¤¤à¤¿à¤‚à¤¹à¥‡à¤‚\nà¤¤à¤¿à¤‚à¤¹à¥‹à¤‚\nà¤¤à¤¿à¤¨\nà¤¤à¤¿à¤¨à¥à¤¹à¥‡à¤‚\nà¤¤à¤¿à¤¨à¥à¤¹à¥‹à¤‚\nà¤¤à¤¿à¤¸\nà¤¤à¤¿à¤¸à¥‡\nà¤¤à¥‹\nà¤¥à¤¾\nà¤¥à¤¿\nà¤¥à¥€\nà¤¥à¥‡\nà¤¦à¤¬à¤¾à¤°à¤¾\nà¤¦à¤µà¤¾à¤°à¤¾\nà¤¦à¤¿à¤¯à¤¾\nà¤¦à¥à¤¸à¤°à¤¾\nà¤¦à¥à¤¸à¤°à¥‡\nà¤¦à¥‚à¤¸à¤°à¥‡\nà¤¦à¥‹\nà¤¦à¥à¤µà¤¾à¤°à¤¾\nà¤¨\nà¤¨à¤¹à¤¿à¤‚\nà¤¨à¤¹à¥€à¤‚\nà¤¨à¤¾\nà¤¨à¤¿à¤šà¥‡\nà¤¨à¤¿à¤¹à¤¾à¤¯à¤¤\nà¤¨à¥€à¤šà¥‡\nà¤¨à¥‡\nà¤ªà¤°\nà¤ªà¤¹à¤²à¥‡\nà¤ªà¥à¤°à¤¾\nà¤ªà¥‚à¤°à¤¾\nà¤ªà¥‡\nà¤«à¤¿à¤°\nà¤¬à¤¨à¤¿\nà¤¬à¤¨à¥€\nà¤¬à¤¹à¤¿\nà¤¬à¤¹à¥€\nà¤¬à¤¹à¥à¤¤\nà¤¬à¤¾à¤¦\nà¤¬à¤¾à¤²à¤¾\nà¤¬à¤¿à¤²à¤•à¥à¤²\nà¤­à¤¿\nà¤­à¤¿à¤¤à¤°\nà¤­à¥€\nà¤­à¥€à¤¤à¤°\nà¤®à¤—à¤°\nà¤®à¤¾à¤¨à¥‹\nà¤®à¥‡\nà¤®à¥‡à¤‚\nà¤®à¥ˆà¤‚\nà¤®à¥à¤à¤•à¥‹\nà¤®à¥‡à¤°à¤¾\nà¤¯à¤¦à¤¿\nà¤¯à¤¹\nà¤¯à¤¹à¤¾à¤\nà¤¯à¤¹à¤¾à¤‚\nà¤¯à¤¹à¤¿\nà¤¯à¤¹à¥€\nà¤¯à¤¾\nà¤¯à¤¿à¤¹\nà¤¯à¥‡\nà¤°à¤–à¥‡à¤‚\nà¤°à¤µà¤¾à¤¸à¤¾\nà¤°à¤¹à¤¾\nà¤°à¤¹à¥‡\nà¤±à¥à¤µà¤¾à¤¸à¤¾\nà¤²à¤¿à¤\nà¤²à¤¿à¤¯à¥‡\nà¤²à¥‡à¤•à¤¿à¤¨\nà¤µ\nà¤µà¤—à¥‡à¤°à¤¹\nà¤µà¥šà¥ˆà¤°à¤¹\nà¤µà¤°à¤—\nà¤µà¤°à¥à¤—\nà¤µà¤¹\nà¤µà¤¹à¤¾à¤\nà¤µà¤¹à¤¾à¤‚\nà¤µà¤¹à¤¿à¤‚\nà¤µà¤¹à¥€à¤‚\nà¤µà¤¾à¤²à¥‡\nà¤µà¥à¤¹\nà¤µà¥‡\nà¤µà¥šà¥ˆà¤°à¤¹\nà¤¸à¤‚à¤—\nà¤¸à¤•à¤¤à¤¾\nà¤¸à¤•à¤¤à¥‡\nà¤¸à¤¬à¤¸à¥‡\nà¤¸à¤­à¤¿\nà¤¸à¤­à¥€\nà¤¸à¤¾à¤¥\nà¤¸à¤¾à¤¬à¥à¤¤\nà¤¸à¤¾à¤­\nà¤¸à¤¾à¤°à¤¾\nà¤¸à¥‡\nà¤¸à¥‹\nà¤¸à¤‚à¤—\nà¤¹à¤¿\nà¤¹à¥€\nà¤¹à¥à¤…\nà¤¹à¥à¤†\nà¤¹à¥à¤‡\nà¤¹à¥à¤ˆ\nà¤¹à¥à¤\nà¤¹à¥‡\nà¤¹à¥‡à¤‚\nà¤¹à¥ˆ\nà¤¹à¥ˆà¤‚\nà¤¹à¥‹\nà¤¹à¥‚à¤\nà¤¹à¥‹à¤¤à¤¾\nà¤¹à¥‹à¤¤à¤¿\nà¤¹à¥‹à¤¤à¥€\nà¤¹à¥‹à¤¤à¥‡\nà¤¹à¥‹à¤¨à¤¾\nà¤¹à¥‹à¤¨à¥‡\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ru/__init__.py----------------------------------------
A:spacy.lang.ru.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.ru.__init__.lex_attr_getters[NORM]->add_lookups(Language.Defaults.lex_attr_getters[NORM], BASE_NORMS, NORM_EXCEPTIONS)
A:spacy.lang.ru.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS, TOKENIZER_EXCEPTIONS)
A:spacy.lang.ru.__init__.lookups->Lookups()
spacy.lang.ru.__init__.Russian(Language)
spacy.lang.ru.__init__.RussianDefaults(Language.Defaults)
spacy.lang.ru.__init__.RussianDefaults.create_lemmatizer(cls,nlp=None,lookups=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ru/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ru/lemmatizer.py----------------------------------------
A:spacy.lang.ru.lemmatizer.RussianLemmatizer._morph->MorphAnalyzer()
A:spacy.lang.ru.lemmatizer.univ_pos->self.normalize_univ_pos(univ_pos)
A:spacy.lang.ru.lemmatizer.analyses->self._morph.parse(string)
A:spacy.lang.ru.lemmatizer.(analysis_pos, _)->oc2ud(str(analysis.tag))
A:spacy.lang.ru.lemmatizer.(_, analysis_morph)->oc2ud(str(analysis.tag))
A:spacy.lang.ru.lemmatizer.morphology->dict()
A:spacy.lang.ru.lemmatizer.unmatched->set()
A:spacy.lang.ru.lemmatizer.grams->oc_tag.replace(' ', ',').split(',')
A:spacy.lang.ru.lemmatizer.gram->set().pop()
spacy.lang.ru.RussianLemmatizer(self,lookups=None)
spacy.lang.ru.RussianLemmatizer.lookup(self,string,orth=None)
spacy.lang.ru.RussianLemmatizer.normalize_univ_pos(univ_pos)
spacy.lang.ru.lemmatizer.RussianLemmatizer(self,lookups=None)
spacy.lang.ru.lemmatizer.RussianLemmatizer.__init__(self,lookups=None)
spacy.lang.ru.lemmatizer.RussianLemmatizer.lookup(self,string,orth=None)
spacy.lang.ru.lemmatizer.RussianLemmatizer.normalize_univ_pos(univ_pos)
spacy.lang.ru.lemmatizer.oc2ud(oc_tag)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ru/lex_attrs.py----------------------------------------
A:spacy.lang.ru.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.ru.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.ru.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ru/tokenizer_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ru/stop_words.py----------------------------------------
A:spacy.lang.ru.stop_words.STOP_WORDS->set('\nĞ°\n\nĞ±ÑƒĞ´ĞµĞ¼ Ğ±ÑƒĞ´ĞµÑ‚ Ğ±ÑƒĞ´ĞµÑ‚Ğµ Ğ±ÑƒĞ´ĞµÑˆÑŒ Ğ±ÑƒĞ´Ñƒ Ğ±ÑƒĞ´ÑƒÑ‚ Ğ±ÑƒĞ´ÑƒÑ‡Ğ¸ Ğ±ÑƒĞ´ÑŒ Ğ±ÑƒĞ´ÑŒÑ‚Ğµ Ğ±Ñ‹ Ğ±Ñ‹Ğ» Ğ±Ñ‹Ğ»Ğ° Ğ±Ñ‹Ğ»Ğ¸ Ğ±Ñ‹Ğ»Ğ¾\nĞ±Ñ‹Ñ‚ÑŒ\n\nĞ² Ğ²Ğ°Ğ¼ Ğ²Ğ°Ğ¼Ğ¸ Ğ²Ğ°Ñ Ğ²ĞµÑÑŒ Ğ²Ğ¾ Ğ²Ğ¾Ñ‚ Ğ²ÑĞµ Ğ²ÑÑ‘ Ğ²ÑĞµĞ³Ğ¾ Ğ²ÑĞµĞ¹ Ğ²ÑĞµĞ¼ Ğ²ÑÑ‘Ğ¼ Ğ²ÑĞµĞ¼Ğ¸ Ğ²ÑĞµĞ¼Ñƒ Ğ²ÑĞµÑ… Ğ²ÑĞµÑ\nĞ²ÑĞµÑ Ğ²ÑÑ Ğ²ÑÑ Ğ²Ñ‹\n\nĞ´Ğ° Ğ´Ğ»Ñ Ğ´Ğ¾\n\nĞµĞ³Ğ¾ ĞµĞ´Ğ¸Ğ¼ ĞµĞ´ÑÑ‚ ĞµĞµ ĞµÑ‘ ĞµĞ¹ ĞµĞ» ĞµĞ»Ğ° ĞµĞ¼ ĞµĞ¼Ñƒ ĞµĞ¼ÑŠ ĞµÑĞ»Ğ¸ ĞµÑÑ‚ ĞµÑÑ‚ÑŒ ĞµÑˆÑŒ ĞµÑ‰Ğµ ĞµÑ‰Ñ‘ ĞµÑ\n\nĞ¶Ğµ\n\nĞ·Ğ°\n\nĞ¸ Ğ¸Ğ· Ğ¸Ğ»Ğ¸ Ğ¸Ğ¼ Ğ¸Ğ¼Ğ¸ Ğ¸Ğ¼ÑŠ Ğ¸Ñ…\n\nĞº ĞºĞ°Ğº ĞºĞµĞ¼ ĞºĞ¾ ĞºĞ¾Ğ³Ğ´Ğ° ĞºĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ¼ ĞºĞ¾Ğ¼Ñƒ ĞºĞ¾Ğ¼ÑŒÑ ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ³Ğ¾ ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ¼\nĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ¼Ñƒ ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ñ ĞºĞ¾Ñ‚Ğ¾Ñ€ÑƒÑ ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¼ ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… ĞºÑ‚Ğ¾\n\nĞ¼ĞµĞ½Ñ Ğ¼Ğ½Ğµ Ğ¼Ğ½Ğ¾Ğ¹ Ğ¼Ğ½Ğ¾Ñ Ğ¼Ğ¾Ğ³ Ğ¼Ğ¾Ğ³Ğ¸ Ğ¼Ğ¾Ğ³Ğ¸Ñ‚Ğµ Ğ¼Ğ¾Ğ³Ğ»Ğ° Ğ¼Ğ¾Ğ³Ğ»Ğ¸ Ğ¼Ğ¾Ğ³Ğ»Ğ¾ Ğ¼Ğ¾Ğ³Ñƒ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¼Ğ¾Ğµ Ğ¼Ğ¾Ñ‘ Ğ¼Ğ¾ĞµĞ³Ğ¾\nĞ¼Ğ¾ĞµĞ¹ Ğ¼Ğ¾ĞµĞ¼ Ğ¼Ğ¾Ñ‘Ğ¼ Ğ¼Ğ¾ĞµĞ¼Ñƒ Ğ¼Ğ¾ĞµÑ Ğ¼Ğ¾Ğ¶ĞµĞ¼ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¼Ğ¾Ğ¶ĞµÑ‚Ğµ Ğ¼Ğ¾Ğ¶ĞµÑˆÑŒ Ğ¼Ğ¾Ğ¸ Ğ¼Ğ¾Ğ¹ Ğ¼Ğ¾Ğ¸Ğ¼ Ğ¼Ğ¾Ğ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ¸Ñ…\nĞ¼Ğ¾Ñ‡ÑŒ Ğ¼Ğ¾Ñ Ğ¼Ğ¾Ñ Ğ¼Ñ‹\n\nĞ½Ğ° Ğ½Ğ°Ğ¼ Ğ½Ğ°Ğ¼Ğ¸ Ğ½Ğ°Ñ Ğ½Ğ°ÑĞ° Ğ½Ğ°Ñˆ Ğ½Ğ°ÑˆĞ° Ğ½Ğ°ÑˆĞµ Ğ½Ğ°ÑˆĞµĞ³Ğ¾ Ğ½Ğ°ÑˆĞµĞ¹ Ğ½Ğ°ÑˆĞµĞ¼ Ğ½Ğ°ÑˆĞµĞ¼Ñƒ Ğ½Ğ°ÑˆĞµÑ Ğ½Ğ°ÑˆĞ¸ Ğ½Ğ°ÑˆĞ¸Ğ¼\nĞ½Ğ°ÑˆĞ¸Ğ¼Ğ¸ Ğ½Ğ°ÑˆĞ¸Ñ… Ğ½Ğ°ÑˆÑƒ Ğ½Ğµ Ğ½ĞµĞ³Ğ¾ Ğ½ĞµĞµ Ğ½ĞµÑ‘ Ğ½ĞµĞ¹ Ğ½ĞµĞ¼ Ğ½Ñ‘Ğ¼ Ğ½ĞµĞ¼Ñƒ Ğ½ĞµÑ‚ Ğ½ĞµÑ Ğ½Ğ¸Ğ¼ Ğ½Ğ¸Ğ¼Ğ¸ Ğ½Ğ¸Ñ… Ğ½Ğ¾\n\nĞ¾ Ğ¾Ğ± Ğ¾Ğ´Ğ¸Ğ½ Ğ¾Ğ´Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¸ Ğ¾Ğ´Ğ½Ğ¸Ğ¼ Ğ¾Ğ´Ğ½Ğ¸Ğ¼Ğ¸ Ğ¾Ğ´Ğ½Ğ¸Ñ… Ğ¾Ğ´Ğ½Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ´Ğ½Ğ¾Ñ\nĞ¾Ğ´Ğ½Ñƒ Ğ¾Ğ½ Ğ¾Ğ½Ğ° Ğ¾Ğ½Ğµ Ğ¾Ğ½Ğ¸ Ğ¾Ğ½Ğ¾ Ğ¾Ñ‚\n\nĞ¿Ğ¾ Ğ¿Ñ€Ğ¸\n\nÑ ÑĞ°Ğ¼ ÑĞ°Ğ¼Ğ° ÑĞ°Ğ¼Ğ¸ ÑĞ°Ğ¼Ğ¸Ğ¼ ÑĞ°Ğ¼Ğ¸Ğ¼Ğ¸ ÑĞ°Ğ¼Ğ¸Ñ… ÑĞ°Ğ¼Ğ¾ ÑĞ°Ğ¼Ğ¾Ğ³Ğ¾ ÑĞ°Ğ¼Ğ¾Ğ¼ ÑĞ°Ğ¼Ğ¾Ğ¼Ñƒ ÑĞ°Ğ¼Ñƒ ÑĞ²Ğ¾Ğµ ÑĞ²Ğ¾Ñ‘\nÑĞ²Ğ¾ĞµĞ³Ğ¾ ÑĞ²Ğ¾ĞµĞ¹ ÑĞ²Ğ¾ĞµĞ¼ ÑĞ²Ğ¾Ñ‘Ğ¼ ÑĞ²Ğ¾ĞµĞ¼Ñƒ ÑĞ²Ğ¾ĞµÑ ÑĞ²Ğ¾Ğ¸ ÑĞ²Ğ¾Ğ¹ ÑĞ²Ğ¾Ğ¸Ğ¼ ÑĞ²Ğ¾Ğ¸Ğ¼Ğ¸ ÑĞ²Ğ¾Ğ¸Ñ… ÑĞ²Ğ¾Ñ ÑĞ²Ğ¾Ñ\nÑĞµĞ±Ğµ ÑĞµĞ±Ñ ÑĞ¾Ğ±Ğ¾Ğ¹ ÑĞ¾Ğ±Ğ¾Ñ\n\nÑ‚Ğ° Ñ‚Ğ°Ğº Ñ‚Ğ°ĞºĞ°Ñ Ñ‚Ğ°ĞºĞ¸Ğµ Ñ‚Ğ°ĞºĞ¸Ğ¼ Ñ‚Ğ°ĞºĞ¸Ğ¼Ğ¸ Ñ‚Ğ°ĞºĞ¸Ñ… Ñ‚Ğ°ĞºĞ¾Ğ³Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğµ Ñ‚Ğ°ĞºĞ¾Ğ¹ Ñ‚Ğ°ĞºĞ¾Ğ¼ Ñ‚Ğ°ĞºĞ¾Ğ¼Ñƒ Ñ‚Ğ°ĞºĞ¾Ñ\nÑ‚Ğ°ĞºÑƒÑ Ñ‚Ğµ Ñ‚ĞµĞ±Ğµ Ñ‚ĞµĞ±Ñ Ñ‚ĞµĞ¼ Ñ‚ĞµĞ¼Ğ¸ Ñ‚ĞµÑ… Ñ‚Ğ¾ Ñ‚Ğ¾Ğ±Ğ¾Ğ¹ Ñ‚Ğ¾Ğ±Ğ¾Ñ Ñ‚Ğ¾Ğ³Ğ¾ Ñ‚Ğ¾Ğ¹ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‚Ğ¾Ğ¼ Ñ‚Ğ¾Ğ¼Ğ°Ñ… Ñ‚Ğ¾Ğ¼Ñƒ\nÑ‚Ğ¾Ñ‚ Ñ‚Ğ¾Ñ Ñ‚Ñƒ Ñ‚Ñ‹\n\nÑƒ ÑƒĞ¶Ğµ\n\nÑ‡ĞµĞ³Ğ¾ Ñ‡ĞµĞ¼ Ñ‡Ñ‘Ğ¼ Ñ‡ĞµĞ¼Ñƒ Ñ‡Ñ‚Ğ¾ Ñ‡Ñ‚Ğ¾Ğ±Ñ‹\n\nÑÑ‚Ğ° ÑÑ‚Ğ¸ ÑÑ‚Ğ¸Ğ¼ ÑÑ‚Ğ¸Ğ¼Ğ¸ ÑÑ‚Ğ¸Ñ… ÑÑ‚Ğ¾ ÑÑ‚Ğ¾Ğ³Ğ¾ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ¾Ğ¼ ÑÑ‚Ğ¾Ğ¼Ñƒ ÑÑ‚Ğ¾Ñ‚ ÑÑ‚Ğ¾Ñ ÑÑ‚Ñƒ\n\nÑ\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ru/norm_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ru/tag_map.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/zh/__init__.py----------------------------------------
A:spacy.lang.zh.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.zh.__init__.words->list(jieba.cut(text, cut_all=False))
A:spacy.lang.zh.__init__.spaces[-1]->bool(token.whitespace_)
spacy.lang.zh.__init__.Chinese(Language)
spacy.lang.zh.__init__.Chinese.make_doc(self,text)
spacy.lang.zh.__init__.ChineseDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/zh/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/zh/lex_attrs.py----------------------------------------
A:spacy.lang.zh.lex_attrs.text->text.replace(',', '').replace('.', '').replace('ï¼Œ', '').replace('ã€‚', '').replace(',', '').replace('.', '').replace('ï¼Œ', '').replace('ã€‚', '')
A:spacy.lang.zh.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace('ï¼Œ', '').replace('ã€‚', '').replace(',', '').replace('.', '').replace('ï¼Œ', '').replace('ã€‚', '').split('/')
spacy.lang.zh.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/zh/stop_words.py----------------------------------------
A:spacy.lang.zh.stop_words.STOP_WORDS->set('\n!\n"\n#\n$\n%\n&\n\'\n(\n)\n*\n+\n,\n-\n--\n.\n..\n...\n......\n...................\n./\n.ä¸€\n.æ•°\n.æ—¥\n/\n//\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n:\n://\n::\n;\n<\n=\n>\n>>\n?\n@\nA\nLex\n[\n]\n^\n_\n`\nexp\nsub\nsup\n|\n}\n~\n~~~~\nÂ·\nÃ—\nÃ—Ã—Ã—\nÎ”\nÎ¨\nÎ³\nÎ¼\nÏ†\nÏ†ï¼\nĞ’\nâ€”\nâ€”â€”\nâ€”â€”â€”\nâ€˜\nâ€™\nâ€™â€˜\nâ€œ\nâ€\nâ€ï¼Œ\nâ€¦\nâ€¦â€¦\nâ€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â‘¢\nâ€²âˆˆ\nâ€²ï½œ\nâ„ƒ\nâ…¢\nâ†‘\nâ†’\nâˆˆï¼»\nâˆªÏ†âˆˆ\nâ‰ˆ\nâ‘ \nâ‘¡\nâ‘¡ï½ƒ\nâ‘¢\nâ‘¢ï¼½\nâ‘£\nâ‘¤\nâ‘¥\nâ‘¦\nâ‘§\nâ‘¨\nâ‘©\nâ”€â”€\nâ– \nâ–²\n\u3000\nã€\nã€‚\nã€ˆ\nã€‰\nã€Š\nã€‹\nã€‹ï¼‰ï¼Œ\nã€\nã€\nã€\nã€\nã€‘\nã€”\nã€•\nã€•ã€”\nãˆ§\nä¸€\nä¸€.\nä¸€ä¸€\nä¸€ä¸‹\nä¸€ä¸ª\nä¸€äº›\nä¸€ä½•\nä¸€åˆ‡\nä¸€åˆ™\nä¸€åˆ™é€šè¿‡\nä¸€å¤©\nä¸€å®š\nä¸€æ–¹é¢\nä¸€æ—¦\nä¸€æ—¶\nä¸€æ¥\nä¸€æ ·\nä¸€æ¬¡\nä¸€ç‰‡\nä¸€ç•ª\nä¸€ç›´\nä¸€è‡´\nä¸€èˆ¬\nä¸€èµ·\nä¸€è½¬çœ¼\nä¸€è¾¹\nä¸€é¢\nä¸ƒ\nä¸‡ä¸€\nä¸‰\nä¸‰å¤©ä¸¤å¤´\nä¸‰ç•ªä¸¤æ¬¡\nä¸‰ç•ªäº”æ¬¡\nä¸Š\nä¸Šä¸‹\nä¸Šå‡\nä¸Šå»\nä¸Šæ¥\nä¸Šè¿°\nä¸Šé¢\nä¸‹\nä¸‹åˆ—\nä¸‹å»\nä¸‹æ¥\nä¸‹é¢\nä¸\nä¸ä¸€\nä¸ä¸‹\nä¸ä¹…\nä¸äº†\nä¸äº¦ä¹ä¹\nä¸ä»…\nä¸ä»…...è€Œä¸”\nä¸ä»…ä»…\nä¸ä»…ä»…æ˜¯\nä¸ä¼š\nä¸ä½†\nä¸ä½†...è€Œä¸”\nä¸å…‰\nä¸å…\nä¸å†\nä¸åŠ›\nä¸å•\nä¸å˜\nä¸åª\nä¸å¯\nä¸å¯å¼€äº¤\nä¸å¯æŠ—æ‹’\nä¸åŒ\nä¸å¤–\nä¸å¤–ä¹\nä¸å¤Ÿ\nä¸å¤§\nä¸å¦‚\nä¸å¦¨\nä¸å®š\nä¸å¯¹\nä¸å°‘\nä¸å°½\nä¸å°½ç„¶\nä¸å·§\nä¸å·²\nä¸å¸¸\nä¸å¾—\nä¸å¾—ä¸\nä¸å¾—äº†\nä¸å¾—å·²\nä¸å¿…\nä¸æ€ä¹ˆ\nä¸æ€•\nä¸æƒŸ\nä¸æˆ\nä¸æ‹˜\nä¸æ‹©æ‰‹æ®µ\nä¸æ•¢\nä¸æ–™\nä¸æ–­\nä¸æ—¥\nä¸æ—¶\nä¸æ˜¯\nä¸æ›¾\nä¸æ­¢\nä¸æ­¢ä¸€æ¬¡\nä¸æ¯”\nä¸æ¶ˆ\nä¸æ»¡\nä¸ç„¶\nä¸ç„¶çš„è¯\nä¸ç‰¹\nä¸ç‹¬\nä¸ç”±å¾—\nä¸çŸ¥ä¸è§‰\nä¸ç®¡\nä¸ç®¡æ€æ ·\nä¸ç»æ„\nä¸èƒœ\nä¸èƒ½\nä¸èƒ½ä¸\nä¸è‡³äº\nä¸è‹¥\nä¸è¦\nä¸è®º\nä¸èµ·\nä¸è¶³\nä¸è¿‡\nä¸è¿­\nä¸é—®\nä¸é™\nä¸\nä¸å…¶\nä¸å…¶è¯´\nä¸å¦\nä¸æ­¤åŒæ—¶\nä¸“é—¨\nä¸”\nä¸”ä¸è¯´\nä¸”è¯´\nä¸¤è€…\nä¸¥æ ¼\nä¸¥é‡\nä¸ª\nä¸ªäºº\nä¸ªåˆ«\nä¸­å°\nä¸­é—´\nä¸°å¯Œ\nä¸²è¡Œ\nä¸´\nä¸´åˆ°\nä¸º\nä¸ºä¸»\nä¸ºäº†\nä¸ºä»€ä¹ˆ\nä¸ºä»€éº½\nä¸ºä½•\nä¸ºæ­¢\nä¸ºæ­¤\nä¸ºç€\nä¸»å¼ \nä¸»è¦\nä¸¾å‡¡\nä¸¾è¡Œ\nä¹ƒ\nä¹ƒè‡³\nä¹ƒè‡³äº\nä¹ˆ\nä¹‹\nä¹‹ä¸€\nä¹‹å‰\nä¹‹å\nä¹‹å¾Œ\nä¹‹æ‰€ä»¥\nä¹‹ç±»\nä¹Œä¹\nä¹\nä¹’\nä¹˜\nä¹˜åŠ¿\nä¹˜æœº\nä¹˜èƒœ\nä¹˜è™š\nä¹˜éš™\nä¹\nä¹Ÿ\nä¹Ÿå¥½\nä¹Ÿå°±æ˜¯è¯´\nä¹Ÿæ˜¯\nä¹Ÿç½¢\näº†\näº†è§£\näº‰å–\näºŒ\näºŒæ¥\näºŒè¯ä¸è¯´\näºŒè¯æ²¡è¯´\näº\näºæ˜¯\näºæ˜¯ä¹\näº‘äº‘\näº‘å°”\näº’\näº’ç›¸\näº”\näº›\näº¤å£\näº¦\näº§ç”Ÿ\näº²å£\näº²æ‰‹\näº²çœ¼\näº²è‡ª\näº²èº«\näºº\näººäºº\näººä»¬\näººå®¶\näººæ°‘\nä»€ä¹ˆ\nä»€ä¹ˆæ ·\nä»€éº½\nä»…\nä»…ä»…\nä»Š\nä»Šå\nä»Šå¤©\nä»Šå¹´\nä»Šå¾Œ\nä»‹äº\nä»\nä»æ—§\nä»ç„¶\nä»\nä»ä¸\nä»ä¸¥\nä»ä¸­\nä»äº‹\nä»ä»Šä»¥å\nä»ä¼˜\nä»å¤åˆ°ä»Š\nä»å¤è‡³ä»Š\nä»å¤´\nä»å®½\nä»å°\nä»æ–°\nä»æ— åˆ°æœ‰\nä»æ—©åˆ°æ™š\nä»æœª\nä»æ¥\nä»æ­¤\nä»æ­¤ä»¥å\nä»è€Œ\nä»è½»\nä»é€Ÿ\nä»é‡\nä»–\nä»–äºº\nä»–ä»¬\nä»–æ˜¯\nä»–çš„\nä»£æ›¿\nä»¥\nä»¥ä¸Š\nä»¥ä¸‹\nä»¥ä¸º\nä»¥ä¾¿\nä»¥å…\nä»¥å‰\nä»¥åŠ\nä»¥å\nä»¥å¤–\nä»¥å¾Œ\nä»¥æ•…\nä»¥æœŸ\nä»¥æ¥\nä»¥è‡³\nä»¥è‡³äº\nä»¥è‡´\nä»¬\nä»»\nä»»ä½•\nä»»å‡­\nä»»åŠ¡\nä¼å›¾\nä¼™åŒ\nä¼š\nä¼Ÿå¤§\nä¼ \nä¼ è¯´\nä¼ é—»\nä¼¼ä¹\nä¼¼çš„\nä½†\nä½†å‡¡\nä½†æ„¿\nä½†æ˜¯\nä½•\nä½•ä¹è€Œä¸ä¸º\nä½•ä»¥\nä½•å†µ\nä½•å¤„\nä½•å¦¨\nä½•å°\nä½•å¿…\nä½•æ—¶\nä½•æ­¢\nä½•è‹¦\nä½•é¡»\nä½™å¤–\nä½œä¸º\nä½ \nä½ ä»¬\nä½ æ˜¯\nä½ çš„\nä½¿\nä½¿å¾—\nä½¿ç”¨\nä¾‹å¦‚\nä¾\nä¾æ®\nä¾ç…§\nä¾é \nä¾¿\nä¾¿äº\nä¿ƒè¿›\nä¿æŒ\nä¿ç®¡\nä¿é™©\nä¿º\nä¿ºä»¬\nå€åŠ \nå€æ„Ÿ\nå€’ä¸å¦‚\nå€’ä¸å¦‚è¯´\nå€’æ˜¯\nå€˜\nå€˜ä½¿\nå€˜æˆ–\nå€˜ç„¶\nå€˜è‹¥\nå€Ÿ\nå€Ÿä»¥\nå€Ÿæ­¤\nå‡ä½¿\nå‡å¦‚\nå‡è‹¥\nåå\nåšåˆ°\nå¶å°”\nå¶è€Œ\nå‚¥ç„¶\nåƒ\nå„¿\nå…è®¸\nå…ƒï¼å¨\nå……å…¶æ\nå……å…¶é‡\nå……åˆ†\nå…ˆä¸å…ˆ\nå…ˆå\nå…ˆå¾Œ\nå…ˆç”Ÿ\nå…‰\nå…‰æ˜¯\nå…¨ä½“\nå…¨åŠ›\nå…¨å¹´\nå…¨ç„¶\nå…¨èº«å¿ƒ\nå…¨éƒ¨\nå…¨éƒ½\nå…¨é¢\nå…«\nå…«æˆ\nå…¬ç„¶\nå…­\nå…®\nå…±\nå…±åŒ\nå…±æ€»\nå…³äº\nå…¶\nå…¶ä¸€\nå…¶ä¸­\nå…¶äºŒ\nå…¶ä»–\nå…¶ä½™\nå…¶å\nå…¶å®ƒ\nå…¶å®\nå…¶æ¬¡\nå…·ä½“\nå…·ä½“åœ°è¯´\nå…·ä½“æ¥è¯´\nå…·ä½“è¯´æ¥\nå…·æœ‰\nå…¼ä¹‹\nå†…\nå†\nå†å…¶æ¬¡\nå†åˆ™\nå†æœ‰\nå†æ¬¡\nå†è€…\nå†è€…è¯´\nå†è¯´\nå†’\nå†²\nå†³ä¸\nå†³å®š\nå†³é\nå†µä¸”\nå‡†å¤‡\nå‡‘å·§\nå‡ç¥\nå‡ \nå‡ ä¹\nå‡ åº¦\nå‡ æ—¶\nå‡ ç•ª\nå‡ ç»\nå‡¡\nå‡¡æ˜¯\nå‡­\nå‡­å€Ÿ\nå‡º\nå‡ºäº\nå‡ºå»\nå‡ºæ¥\nå‡ºç°\nåˆ†åˆ«\nåˆ†å¤´\nåˆ†æœŸ\nåˆ†æœŸåˆ†æ‰¹\nåˆ‡\nåˆ‡ä¸å¯\nåˆ‡åˆ‡\nåˆ‡å‹¿\nåˆ‡è«\nåˆ™\nåˆ™ç”š\nåˆš\nåˆšå¥½\nåˆšå·§\nåˆšæ‰\nåˆ\nåˆ«\nåˆ«äºº\nåˆ«å¤„\nåˆ«æ˜¯\nåˆ«çš„\nåˆ«ç®¡\nåˆ«è¯´\nåˆ°\nåˆ°äº†å„¿\nåˆ°å¤„\nåˆ°å¤´\nåˆ°å¤´æ¥\nåˆ°åº•\nåˆ°ç›®å‰ä¸ºæ­¢\nå‰å\nå‰æ­¤\nå‰è€…\nå‰è¿›\nå‰é¢\nåŠ ä¸Š\nåŠ ä¹‹\nåŠ ä»¥\nåŠ å…¥\nåŠ å¼º\nåŠ¨ä¸åŠ¨\nåŠ¨è¾„\nå‹ƒç„¶\nåŒ†åŒ†\nååˆ†\nåƒ\nåƒä¸‡\nåƒä¸‡åƒä¸‡\nåŠ\nå•\nå•å•\nå•çº¯\nå³\nå³ä»¤\nå³ä½¿\nå³ä¾¿\nå³åˆ»\nå³å¦‚\nå³å°†\nå³æˆ–\nå³æ˜¯è¯´\nå³è‹¥\nå´\nå´ä¸\nå†\nåŸæ¥\nå»\nåˆ\nåˆåŠ\nåŠ\nåŠå…¶\nåŠæ—¶\nåŠè‡³\nåŒæ–¹\nåä¹‹\nåä¹‹äº¦ç„¶\nåä¹‹åˆ™\nåå€’\nåå€’æ˜¯\nååº”\nåæ‰‹\nåæ˜ \nåè€Œ\nåè¿‡æ¥\nåè¿‡æ¥è¯´\nå–å¾—\nå–é“\nå—åˆ°\nå˜æˆ\nå¤æ¥\nå¦\nå¦ä¸€ä¸ª\nå¦ä¸€æ–¹é¢\nå¦å¤–\nå¦æ‚‰\nå¦æ–¹é¢\nå¦è¡Œ\nåª\nåªå½“\nåªæ€•\nåªæ˜¯\nåªæœ‰\nåªæ¶ˆ\nåªè¦\nåªé™\nå«\nå«åš\nå¬å¼€\nå®å’š\nå®å½“\nå¯\nå¯ä»¥\nå¯å¥½\nå¯æ˜¯\nå¯èƒ½\nå¯è§\nå„\nå„ä¸ª\nå„äºº\nå„ä½\nå„åœ°\nå„å¼\nå„ç§\nå„çº§\nå„è‡ª\nåˆç†\nåŒ\nåŒä¸€\nåŒæ—¶\nåŒæ ·\nå\nåæ¥\nåè€…\nåé¢\nå‘\nå‘ä½¿\nå‘ç€\nå“\nå—\nå¦åˆ™\nå§\nå§å“’\nå±\nå‘€\nå‘ƒ\nå‘†å‘†åœ°\nå‘\nå‘•\nå‘—\nå‘œ\nå‘œå‘¼\nå‘¢\nå‘¨å›´\nå‘µ\nå‘µå‘µ\nå‘¸\nå‘¼å“§\nå‘¼å•¦\nå’‹\nå’Œ\nå’š\nå’¦\nå’§\nå’±\nå’±ä»¬\nå’³\nå“‡\nå“ˆ\nå“ˆå“ˆ\nå“‰\nå“\nå“å‘€\nå“å“Ÿ\nå“—\nå“—å•¦\nå“Ÿ\nå“¦\nå“©\nå“ª\nå“ªä¸ª\nå“ªäº›\nå“ªå„¿\nå“ªå¤©\nå“ªå¹´\nå“ªæ€•\nå“ªæ ·\nå“ªè¾¹\nå“ªé‡Œ\nå“¼\nå“¼å”·\nå”‰\nå”¯æœ‰\nå•Š\nå•Šå‘€\nå•Šå“ˆ\nå•Šå“Ÿ\nå•\nå•¥\nå•¦\nå•ªè¾¾\nå•·å½“\nå–€\nå–‚\nå–\nå–”å”·\nå–½\nå—¡\nå—¡å—¡\nå—¬\nå—¯\nå—³\nå˜\nå˜å˜\nå˜ç™»\nå˜˜\nå˜›\nå˜»\nå˜¿\nå˜¿å˜¿\nå››\nå› \nå› ä¸º\nå› äº†\nå› æ­¤\nå› ç€\nå› è€Œ\nå›º\nå›ºç„¶\nåœ¨\nåœ¨ä¸‹\nåœ¨äº\nåœ°\nå‡\nåšå†³\nåšæŒ\nåŸºäº\nåŸºæœ¬\nåŸºæœ¬ä¸Š\nå¤„åœ¨\nå¤„å¤„\nå¤„ç†\nå¤æ‚\nå¤š\nå¤šä¹ˆ\nå¤šäº\nå¤šå¤š\nå¤šå¤šå°‘å°‘\nå¤šå¤šç›Šå–„\nå¤šå°‘\nå¤šå¹´å‰\nå¤šå¹´æ¥\nå¤šæ•°\nå¤šæ¬¡\nå¤Ÿç§çš„\nå¤§\nå¤§ä¸äº†\nå¤§ä¸¾\nå¤§äº‹\nå¤§ä½“\nå¤§ä½“ä¸Š\nå¤§å‡¡\nå¤§åŠ›\nå¤§å¤š\nå¤§å¤šæ•°\nå¤§å¤§\nå¤§å®¶\nå¤§å¼ æ——é¼“\nå¤§æ‰¹\nå¤§æŠµ\nå¤§æ¦‚\nå¤§ç•¥\nå¤§çº¦\nå¤§è‡´\nå¤§éƒ½\nå¤§é‡\nå¤§é¢å„¿ä¸Š\nå¤±å»\nå¥‡\nå¥ˆ\nå¥‹å‹‡\nå¥¹\nå¥¹ä»¬\nå¥¹æ˜¯\nå¥¹çš„\nå¥½\nå¥½åœ¨\nå¥½çš„\nå¥½è±¡\nå¦‚\nå¦‚ä¸Š\nå¦‚ä¸Šæ‰€è¿°\nå¦‚ä¸‹\nå¦‚ä»Š\nå¦‚ä½•\nå¦‚å…¶\nå¦‚å‰æ‰€è¿°\nå¦‚åŒ\nå¦‚å¸¸\nå¦‚æ˜¯\nå¦‚æœŸ\nå¦‚æœ\nå¦‚æ¬¡\nå¦‚æ­¤\nå¦‚æ­¤ç­‰ç­‰\nå¦‚è‹¥\nå§‹è€Œ\nå§‘ä¸”\nå­˜åœ¨\nå­˜å¿ƒ\nå­°æ–™\nå­°çŸ¥\nå®\nå®å¯\nå®æ„¿\nå®è‚¯\nå®ƒ\nå®ƒä»¬\nå®ƒä»¬çš„\nå®ƒæ˜¯\nå®ƒçš„\nå®‰å…¨\nå®Œå…¨\nå®Œæˆ\nå®š\nå®ç°\nå®é™…\nå®£å¸ƒ\nå®¹æ˜“\nå¯†åˆ‡\nå¯¹\nå¯¹äº\nå¯¹åº”\nå¯¹å¾…\nå¯¹æ–¹\nå¯¹æ¯”\nå°†\nå°†æ‰\nå°†è¦\nå°†è¿‘\nå°\nå°‘æ•°\nå°”\nå°”å\nå°”å°”\nå°”ç­‰\nå°šä¸”\nå°¤å…¶\nå°±\nå°±åœ°\nå°±æ˜¯\nå°±æ˜¯äº†\nå°±æ˜¯è¯´\nå°±æ­¤\nå°±ç®—\nå°±è¦\nå°½\nå°½å¯èƒ½\nå°½å¦‚äººæ„\nå°½å¿ƒå°½åŠ›\nå°½å¿ƒç«­åŠ›\nå°½å¿«\nå°½æ—©\nå°½ç„¶\nå°½ç®¡\nå°½ç®¡å¦‚æ­¤\nå°½é‡\nå±€å¤–\nå±…ç„¶\nå±Šæ—¶\nå±äº\nå±¡\nå±¡å±¡\nå±¡æ¬¡\nå±¡æ¬¡ä¸‰ç•ª\nå²‚\nå²‚ä½†\nå²‚æ­¢\nå²‚é\nå·æµä¸æ¯\nå·¦å³\nå·¨å¤§\nå·©å›º\nå·®ä¸€ç‚¹\nå·®ä¸å¤š\nå·±\nå·²\nå·²çŸ£\nå·²ç»\nå·´\nå·´å·´\nå¸¦\nå¸®åŠ©\nå¸¸\nå¸¸å¸¸\nå¸¸è¨€è¯´\nå¸¸è¨€è¯´å¾—å¥½\nå¸¸è¨€é“\nå¹³ç´ \nå¹´å¤ä¸€å¹´\nå¹¶\nå¹¶ä¸\nå¹¶ä¸æ˜¯\nå¹¶ä¸”\nå¹¶æ’\nå¹¶æ— \nå¹¶æ²¡\nå¹¶æ²¡æœ‰\nå¹¶è‚©\nå¹¶é\nå¹¿å¤§\nå¹¿æ³›\nåº”å½“\nåº”ç”¨\nåº”è¯¥\nåº¶ä¹\nåº¶å‡ \nå¼€å¤–\nå¼€å§‹\nå¼€å±•\nå¼•èµ·\nå¼—\nå¼¹æŒ‡ä¹‹é—´\nå¼ºçƒˆ\nå¼ºè°ƒ\nå½’\nå½’æ ¹åˆ°åº•\nå½’æ ¹ç»“åº•\nå½’é½\nå½“\nå½“ä¸‹\nå½“ä¸­\nå½“å„¿\nå½“å‰\nå½“å³\nå½“å£å„¿\nå½“åœ°\nå½“åœº\nå½“å¤´\nå½“åº­\nå½“æ—¶\nå½“ç„¶\nå½“çœŸ\nå½“ç€\nå½¢æˆ\nå½»å¤œ\nå½»åº•\nå½¼\nå½¼æ—¶\nå½¼æ­¤\nå¾€\nå¾€å¾€\nå¾…\nå¾…åˆ°\nå¾ˆ\nå¾ˆå¤š\nå¾ˆå°‘\nå¾Œæ¥\nå¾Œé¢\nå¾—\nå¾—äº†\nå¾—å‡º\nå¾—åˆ°\nå¾—å¤©ç‹¬åš\nå¾—èµ·\nå¿ƒé‡Œ\nå¿…\nå¿…å®š\nå¿…å°†\nå¿…ç„¶\nå¿…è¦\nå¿…é¡»\nå¿«\nå¿«è¦\nå¿½åœ°\nå¿½ç„¶\næ€\næ€ä¹ˆ\næ€ä¹ˆåŠ\næ€ä¹ˆæ ·\næ€å¥ˆ\næ€æ ·\næ€éº½\næ€•\næ€¥åŒ†åŒ†\næ€ª\næ€ªä¸å¾—\næ€»ä¹‹\næ€»æ˜¯\næ€»çš„æ¥çœ‹\næ€»çš„æ¥è¯´\næ€»çš„è¯´æ¥\næ€»ç»“\næ€»è€Œè¨€ä¹‹\næç„¶\nææ€•\næ°ä¼¼\næ°å¥½\næ°å¦‚\næ°å·§\næ°æ°\næ°æ°ç›¸å\næ°é€¢\næ‚¨\næ‚¨ä»¬\næ‚¨æ˜¯\næƒŸå…¶\næƒ¯å¸¸\næ„æ€\næ„¤ç„¶\næ„¿æ„\næ…¢è¯´\næˆä¸º\næˆå¹´\næˆå¹´ç´¯æœˆ\næˆå¿ƒ\næˆ‘\næˆ‘ä»¬\næˆ‘æ˜¯\næˆ‘çš„\næˆ–\næˆ–åˆ™\næˆ–å¤šæˆ–å°‘\næˆ–æ˜¯\næˆ–æ›°\næˆ–è€…\næˆ–è®¸\næˆ˜æ–—\næˆªç„¶\næˆªè‡³\næ‰€\næ‰€ä»¥\næ‰€åœ¨\næ‰€å¹¸\næ‰€æœ‰\næ‰€è°“\næ‰\næ‰èƒ½\næ‰‘é€š\næ‰“\næ‰“ä»\næ‰“å¼€å¤©çª—è¯´äº®è¯\næ‰©å¤§\næŠŠ\næŠ‘æˆ–\næŠ½å†·å­\næ‹¦è…°\næ‹¿\næŒ‰\næŒ‰æ—¶\næŒ‰æœŸ\næŒ‰ç…§\næŒ‰ç†\næŒ‰è¯´\næŒ¨ä¸ª\næŒ¨å®¶æŒ¨æˆ·\næŒ¨æ¬¡\næŒ¨ç€\næŒ¨é—¨æŒ¨æˆ·\næŒ¨é—¨é€æˆ·\næ¢å¥è¯è¯´\næ¢è¨€ä¹‹\næ®\næ®å®\næ®æ‚‰\næ®æˆ‘æ‰€çŸ¥\næ®æ­¤\næ®ç§°\næ®è¯´\næŒæ¡\næ¥ä¸‹æ¥\næ¥ç€\næ¥è‘—\næ¥è¿ä¸æ–­\næ”¾é‡\næ•…\næ•…æ„\næ•…æ­¤\næ•…è€Œ\næ•å¼€å„¿\næ•¢\næ•¢äº\næ•¢æƒ…\næ•°/\næ•´ä¸ª\næ–­ç„¶\næ–¹\næ–¹ä¾¿\næ–¹æ‰\næ–¹èƒ½\næ–¹é¢\næ—äºº\næ— \næ— å®\næ— æ³•\næ— è®º\næ—¢\næ—¢...åˆ\næ—¢å¾€\næ—¢æ˜¯\næ—¢ç„¶\næ—¥å¤ä¸€æ—¥\næ—¥æ¸\næ—¥ç›Š\næ—¥è‡»\næ—¥è§\næ—¶å€™\næ˜‚ç„¶\næ˜æ˜¾\næ˜ç¡®\næ˜¯\næ˜¯ä¸æ˜¯\næ˜¯ä»¥\næ˜¯å¦\næ˜¯çš„\næ˜¾ç„¶\næ˜¾è‘—\næ™®é€š\næ™®é\næš—ä¸­\næš—åœ°é‡Œ\næš—è‡ª\næ›´\næ›´ä¸º\næ›´åŠ \næ›´è¿›ä¸€æ­¥\næ›¾\næ›¾ç»\næ›¿\næ›¿ä»£\næœ€\næœ€å\næœ€å¤§\næœ€å¥½\næœ€å¾Œ\næœ€è¿‘\næœ€é«˜\næœ‰\næœ‰äº›\næœ‰å…³\næœ‰åˆ©\næœ‰åŠ›\næœ‰åŠ\næœ‰æ‰€\næœ‰æ•ˆ\næœ‰æ—¶\næœ‰ç‚¹\næœ‰çš„\næœ‰çš„æ˜¯\næœ‰ç€\næœ‰è‘—\næœ›\næœ\næœç€\næœ«##æœ«\næœ¬\næœ¬äºº\næœ¬åœ°\næœ¬ç€\næœ¬èº«\næƒæ—¶\næ¥\næ¥ä¸åŠ\næ¥å¾—åŠ\næ¥çœ‹\næ¥ç€\næ¥è‡ª\næ¥è®²\næ¥è¯´\næ\næä¸º\næäº†\næå…¶\næåŠ›\næå¤§\næåº¦\næç«¯\næ„æˆ\næœç„¶\næœçœŸ\næŸ\næŸä¸ª\næŸäº›\næŸæŸ\næ ¹æ®\næ ¹æœ¬\næ ¼å¤–\næ¢†\næ¦‚\næ¬¡ç¬¬\næ¬¢è¿\næ¬¤\næ­£å€¼\næ­£åœ¨\næ­£å¦‚\næ­£å·§\næ­£å¸¸\næ­£æ˜¯\næ­¤\næ­¤ä¸­\næ­¤å\næ­¤åœ°\næ­¤å¤„\næ­¤å¤–\næ­¤æ—¶\næ­¤æ¬¡\næ­¤é—´\næ®†\næ¯‹å®\næ¯\næ¯ä¸ª\næ¯å¤©\næ¯å¹´\næ¯å½“\næ¯æ—¶æ¯åˆ»\næ¯æ¯\næ¯é€¢\næ¯”\næ¯”åŠ\næ¯”å¦‚\næ¯”å¦‚è¯´\næ¯”æ–¹\næ¯”ç…§\næ¯”èµ·\næ¯”è¾ƒ\næ¯•ç«Ÿ\næ¯«ä¸\næ¯«æ— \næ¯«æ— ä¾‹å¤–\næ¯«æ— ä¿ç•™åœ°\næ±\næ²™æ²™\næ²¡\næ²¡å¥ˆä½•\næ²¡æœ‰\næ²¿\næ²¿ç€\næ³¨æ„\næ´»\næ·±å…¥\næ¸…æ¥š\næ»¡\næ»¡è¶³\næ¼«è¯´\nç„‰\nç„¶\nç„¶åˆ™\nç„¶å\nç„¶å¾Œ\nç„¶è€Œ\nç…§\nç…§ç€\nç‰¢ç‰¢\nç‰¹åˆ«æ˜¯\nç‰¹æ®Š\nç‰¹ç‚¹\nçŠ¹ä¸”\nçŠ¹è‡ª\nç‹¬\nç‹¬è‡ª\nçŒ›ç„¶\nçŒ›ç„¶é—´\nç‡å°”\nç‡ç„¶\nç°ä»£\nç°åœ¨\nç†åº”\nç†å½“\nç†è¯¥\nç‘Ÿç‘Ÿ\nç”šä¸”\nç”šä¹ˆ\nç”šæˆ–\nç”šè€Œ\nç”šè‡³\nç”šè‡³äº\nç”¨\nç”¨æ¥\nç”«\nç”­\nç”±\nç”±äº\nç”±æ˜¯\nç”±æ­¤\nç”±æ­¤å¯è§\nç•¥\nç•¥ä¸º\nç•¥åŠ \nç•¥å¾®\nç™½\nç™½ç™½\nçš„\nçš„ç¡®\nçš„è¯\nçš†å¯\nç›®å‰\nç›´åˆ°\nç›´æ¥\nç›¸ä¼¼\nç›¸ä¿¡\nç›¸å\nç›¸åŒ\nç›¸å¯¹\nç›¸å¯¹è€Œè¨€\nç›¸åº”\nç›¸å½“\nç›¸ç­‰\nçœå¾—\nçœ‹\nçœ‹ä¸Šå»\nçœ‹å‡º\nçœ‹åˆ°\nçœ‹æ¥\nçœ‹æ ·å­\nçœ‹çœ‹\nçœ‹è§\nçœ‹èµ·æ¥\nçœŸæ˜¯\nçœŸæ­£\nçœ¨çœ¼\nç€\nç€å‘¢\nçŸ£\nçŸ£ä¹\nçŸ£å“‰\nçŸ¥é“\nç °\nç¡®å®š\nç¢°å·§\nç¤¾ä¼šä¸»ä¹‰\nç¦»\nç§\nç§¯æ\nç§»åŠ¨\nç©¶ç«Ÿ\nç©·å¹´ç´¯æœˆ\nçªå‡º\nçªç„¶\nçªƒ\nç«‹\nç«‹åˆ»\nç«‹å³\nç«‹åœ°\nç«‹æ—¶\nç«‹é©¬\nç«Ÿ\nç«Ÿç„¶\nç«Ÿè€Œ\nç¬¬\nç¬¬äºŒ\nç­‰\nç­‰åˆ°\nç­‰ç­‰\nç­–ç•¥åœ°\nç®€ç›´\nç®€è€Œè¨€ä¹‹\nç®€è¨€ä¹‹\nç®¡\nç±»å¦‚\nç²—\nç²¾å…‰\nç´§æ¥ç€\nç´¯å¹´\nç´¯æ¬¡\nçº¯\nçº¯ç²¹\nçºµ\nçºµä»¤\nçºµä½¿\nçºµç„¶\nç»ƒä¹ \nç»„æˆ\nç»\nç»å¸¸\nç»è¿‡\nç»“åˆ\nç»“æœ\nç»™\nç»\nç»ä¸\nç»å¯¹\nç»é\nç»é¡¶\nç»§ä¹‹\nç»§å\nç»§ç»­\nç»§è€Œ\nç»´æŒ\nç»¼ä¸Šæ‰€è¿°\nç¼•ç¼•\nç½¢äº†\nè€\nè€å¤§\nè€æ˜¯\nè€è€å®å®\nè€ƒè™‘\nè€…\nè€Œ\nè€Œä¸”\nè€Œå†µ\nè€Œåˆ\nè€Œå\nè€Œå¤–\nè€Œå·²\nè€Œæ˜¯\nè€Œè¨€\nè€Œè®º\nè”ç³»\nè”è¢‚\nèƒŒåœ°é‡Œ\nèƒŒé èƒŒ\nèƒ½\nèƒ½å¦\nèƒ½å¤Ÿ\nè…¾\nè‡ª\nè‡ªä¸ªå„¿\nè‡ªä»\nè‡ªå„å„¿\nè‡ªå\nè‡ªå®¶\nè‡ªå·±\nè‡ªæ‰“\nè‡ªèº«\nè‡­\nè‡³\nè‡³äº\nè‡³ä»Š\nè‡³è‹¥\nè‡´\nèˆ¬çš„\nè‰¯å¥½\nè‹¥\nè‹¥å¤«\nè‹¥æ˜¯\nè‹¥æœ\nè‹¥é\nèŒƒå›´\nè«\nè«ä¸\nè«ä¸ç„¶\nè«å¦‚\nè«è‹¥\nè«é\nè·å¾—\nè—‰ä»¥\nè™½\nè™½åˆ™\nè™½ç„¶\nè™½è¯´\nè›®\nè¡Œä¸º\nè¡ŒåŠ¨\nè¡¨æ˜\nè¡¨ç¤º\nè¢«\nè¦\nè¦ä¸\nè¦ä¸æ˜¯\nè¦ä¸ç„¶\nè¦ä¹ˆ\nè¦æ˜¯\nè¦æ±‚\nè§\nè§„å®š\nè§‰å¾—\nè­¬å–»\nè­¬å¦‚\nè®¤ä¸º\nè®¤çœŸ\nè®¤è¯†\nè®©\nè®¸å¤š\nè®º\nè®ºè¯´\nè®¾ä½¿\nè®¾æˆ–\nè®¾è‹¥\nè¯šå¦‚\nè¯šç„¶\nè¯è¯´\nè¯¥\nè¯¥å½“\nè¯´æ˜\nè¯´æ¥\nè¯´è¯´\nè¯·å‹¿\nè¯¸\nè¯¸ä½\nè¯¸å¦‚\nè°\nè°äºº\nè°æ–™\nè°çŸ¥\nè°¨\nè±ç„¶\nè´¼æ­»\nèµ–ä»¥\nèµ¶\nèµ¶å¿«\nèµ¶æ—©ä¸èµ¶æ™š\nèµ·\nèµ·å…ˆ\nèµ·åˆ\nèµ·å¤´\nèµ·æ¥\nèµ·è§\nèµ·é¦–\nè¶\nè¶ä¾¿\nè¶åŠ¿\nè¶æ—©\nè¶æœº\nè¶çƒ­\nè¶ç€\nè¶Šæ˜¯\nè·\nè·Ÿ\nè·¯ç»\nè½¬åŠ¨\nè½¬å˜\nè½¬è´´\nè½°ç„¶\nè¾ƒ\nè¾ƒä¸º\nè¾ƒä¹‹\nè¾ƒæ¯”\nè¾¹\nè¾¾åˆ°\nè¾¾æ—¦\nè¿„\nè¿…é€Ÿ\nè¿‡\nè¿‡äº\nè¿‡å»\nè¿‡æ¥\nè¿ç”¨\nè¿‘\nè¿‘å‡ å¹´æ¥\nè¿‘å¹´æ¥\nè¿‘æ¥\nè¿˜\nè¿˜æ˜¯\nè¿˜æœ‰\nè¿˜è¦\nè¿™\nè¿™ä¸€æ¥\nè¿™ä¸ª\nè¿™ä¹ˆ\nè¿™ä¹ˆäº›\nè¿™ä¹ˆæ ·\nè¿™ä¹ˆç‚¹å„¿\nè¿™äº›\nè¿™ä¼šå„¿\nè¿™å„¿\nè¿™å°±æ˜¯è¯´\nè¿™æ—¶\nè¿™æ ·\nè¿™æ¬¡\nè¿™ç‚¹\nè¿™ç§\nè¿™èˆ¬\nè¿™è¾¹\nè¿™é‡Œ\nè¿™éº½\nè¿›å…¥\nè¿›å»\nè¿›æ¥\nè¿›æ­¥\nè¿›è€Œ\nè¿›è¡Œ\nè¿\nè¿åŒ\nè¿å£°\nè¿æ—¥\nè¿æ—¥æ¥\nè¿è¢‚\nè¿è¿\nè¿Ÿæ—©\nè¿«äº\né€‚åº”\né€‚å½“\né€‚ç”¨\né€æ­¥\né€æ¸\né€šå¸¸\né€šè¿‡\né€ æˆ\né€¢\né‡åˆ°\né­åˆ°\néµå¾ª\néµç…§\né¿å…\né‚£\né‚£ä¸ª\né‚£ä¹ˆ\né‚£ä¹ˆäº›\né‚£ä¹ˆæ ·\né‚£äº›\né‚£ä¼šå„¿\né‚£å„¿\né‚£æ—¶\né‚£æœ«\né‚£æ ·\né‚£èˆ¬\né‚£è¾¹\né‚£é‡Œ\né‚£éº½\néƒ¨åˆ†\néƒ½\né„™äºº\né‡‡å–\né‡Œé¢\né‡å¤§\né‡æ–°\né‡è¦\né‰´äº\né’ˆå¯¹\né•¿æœŸä»¥æ¥\né•¿æ­¤ä¸‹å»\né•¿çº¿\né•¿è¯çŸ­è¯´\né—®é¢˜\né—´æˆ–\né˜²æ­¢\né˜¿\né™„è¿‘\né™ˆå¹´\né™åˆ¶\né™¡ç„¶\né™¤\né™¤äº†\né™¤å´\né™¤å»\né™¤å¤–\né™¤å¼€\né™¤æ­¤\né™¤æ­¤ä¹‹å¤–\né™¤æ­¤ä»¥å¤–\né™¤æ­¤è€Œå¤–\né™¤é\néš\néšå\néšæ—¶\néšç€\néšè‘—\néš”å¤œ\néš”æ—¥\néš¾å¾—\néš¾æ€ª\néš¾è¯´\néš¾é“\néš¾é“è¯´\né›†ä¸­\né›¶\néœ€è¦\néä½†\néå¸¸\néå¾’\néå¾—\néç‰¹\néç‹¬\né \né¡¶å¤š\né¡·\né¡·åˆ»\né¡·åˆ»ä¹‹é—´\né¡·åˆ»é—´\né¡º\né¡ºç€\né¡¿æ—¶\né¢‡\né£é›¨æ— é˜»\né¥±\né¦–å…ˆ\né©¬ä¸Š\né«˜ä½\né«˜å…´\né»˜ç„¶\né»˜é»˜åœ°\né½\nï¸¿\nï¼\nï¼ƒ\nï¼„\nï¼…\nï¼†\nï¼‡\nï¼ˆ\nï¼‰\nï¼‰Ã·ï¼ˆï¼‘ï¼\nï¼‰ã€\nï¼Š\nï¼‹\nï¼‹Î¾\nï¼‹ï¼‹\nï¼Œ\nï¼Œä¹Ÿ\nï¼\nï¼Î²\nï¼ï¼\nï¼ï¼»ï¼Šï¼½ï¼\nï¼\nï¼\nï¼\nï¼ï¼šï¼’\nï¼‘\nï¼‘ï¼\nï¼‘ï¼’ï¼…\nï¼’\nï¼’ï¼ï¼“ï¼…\nï¼“\nï¼”\nï¼•\nï¼•ï¼šï¼\nï¼–\nï¼—\nï¼˜\nï¼™\nï¼š\nï¼›\nï¼œ\nï¼œÂ±\nï¼œÎ”\nï¼œÎ»\nï¼œÏ†\nï¼œï¼œ\nï¼\nï¼â€³\nï¼â˜†\nï¼ï¼ˆ\nï¼ï¼\nï¼ï¼»\nï¼ï½›\nï¼\nï¼Î»\nï¼Ÿ\nï¼ \nï¼¡\nï¼¬ï¼©\nï¼²ï¼ï¼¬ï¼\nï¼ºï¼¸ï¼¦ï¼©ï¼´ï¼¬\nï¼»\nï¼»â‘ â‘ ï¼½\nï¼»â‘ â‘¡ï¼½\nï¼»â‘ â‘¢ï¼½\nï¼»â‘ â‘£ï¼½\nï¼»â‘ â‘¤ï¼½\nï¼»â‘ â‘¥ï¼½\nï¼»â‘ â‘¦ï¼½\nï¼»â‘ â‘§ï¼½\nï¼»â‘ â‘¨ï¼½\nï¼»â‘ ï¼¡ï¼½\nï¼»â‘ ï¼¢ï¼½\nï¼»â‘ ï¼£ï¼½\nï¼»â‘ ï¼¤ï¼½\nï¼»â‘ ï¼¥ï¼½\nï¼»â‘ ï¼½\nï¼»â‘ ï½ï¼½\nï¼»â‘ ï½ƒï¼½\nï¼»â‘ ï½„ï¼½\nï¼»â‘ ï½…ï¼½\nï¼»â‘ ï½†ï¼½\nï¼»â‘ ï½‡ï¼½\nï¼»â‘ ï½ˆï¼½\nï¼»â‘ ï½‰ï¼½\nï¼»â‘ ï½ï¼½\nï¼»â‘¡\nï¼»â‘¡â‘ ï¼½\nï¼»â‘¡â‘¡ï¼½\nï¼»â‘¡â‘¢ï¼½\nï¼»â‘¡â‘£\nï¼»â‘¡â‘¤ï¼½\nï¼»â‘¡â‘¥ï¼½\nï¼»â‘¡â‘¦ï¼½\nï¼»â‘¡â‘§ï¼½\nï¼»â‘¡â‘©ï¼½\nï¼»â‘¡ï¼¢ï¼½\nï¼»â‘¡ï¼§ï¼½\nï¼»â‘¡ï¼½\nï¼»â‘¡ï½ï¼½\nï¼»â‘¡ï½‚ï¼½\nï¼»â‘¡ï½ƒï¼½\nï¼»â‘¡ï½„ï¼½\nï¼»â‘¡ï½…ï¼½\nï¼»â‘¡ï½†ï¼½\nï¼»â‘¡ï½‡ï¼½\nï¼»â‘¡ï½ˆï¼½\nï¼»â‘¡ï½‰ï¼½\nï¼»â‘¡ï½Šï¼½\nï¼»â‘¢â‘ ï¼½\nï¼»â‘¢â‘©ï¼½\nï¼»â‘¢ï¼¦ï¼½\nï¼»â‘¢ï¼½\nï¼»â‘¢ï½ï¼½\nï¼»â‘¢ï½‚ï¼½\nï¼»â‘¢ï½ƒï¼½\nï¼»â‘¢ï½„ï¼½\nï¼»â‘¢ï½…ï¼½\nï¼»â‘¢ï½‡ï¼½\nï¼»â‘¢ï½ˆï¼½\nï¼»â‘£ï¼½\nï¼»â‘£ï½ï¼½\nï¼»â‘£ï½‚ï¼½\nï¼»â‘£ï½ƒï¼½\nï¼»â‘£ï½„ï¼½\nï¼»â‘£ï½…ï¼½\nï¼»â‘¤ï¼½\nï¼»â‘¤ï¼½ï¼½\nï¼»â‘¤ï½ï¼½\nï¼»â‘¤ï½‚ï¼½\nï¼»â‘¤ï½„ï¼½\nï¼»â‘¤ï½…ï¼½\nï¼»â‘¤ï½†ï¼½\nï¼»â‘¥ï¼½\nï¼»â‘¦ï¼½\nï¼»â‘§ï¼½\nï¼»â‘¨ï¼½\nï¼»â‘©ï¼½\nï¼»ï¼Šï¼½\nï¼»ï¼\nï¼»ï¼½\nï¼½\nï¼½âˆ§â€²ï¼ï¼»\nï¼½ï¼»\nï¼¿\nï½ï¼½\nï½‚ï¼½\nï½ƒï¼½\nï½…ï¼½\nï½†ï¼½\nï½ï½‡æ˜‰\nï½›\nï½›ï¼\nï½œ\nï½\nï½ï¼\nï½\nï½Â±\nï½ï¼‹\nï¿¥\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/zh/tag_map.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ar/__init__.py----------------------------------------
A:spacy.lang.ar.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.ar.__init__.lex_attr_getters[NORM]->add_lookups(Language.Defaults.lex_attr_getters[NORM], BASE_NORMS)
A:spacy.lang.ar.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS, TOKENIZER_EXCEPTIONS)
spacy.lang.ar.__init__.Arabic(Language)
spacy.lang.ar.__init__.ArabicDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ar/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ar/lex_attrs.py----------------------------------------
A:spacy.lang.ar.lex_attrs._num_words->set('\nØµÙØ±\nÙˆØ§Ø­Ø¯\nØ¥Ø«Ù†Ø§Ù†\nØ§Ø«Ù†Ø§Ù†\nØ«Ù„Ø§Ø«Ø©\nØ«Ù„Ø§Ø«Ù‡\nØ£Ø±Ø¨Ø¹Ø©\nØ£Ø±Ø¨Ø¹Ù‡\nØ®Ù…Ø³Ø©\nØ®Ù…Ø³Ù‡\nØ³ØªØ©\nØ³ØªÙ‡\nØ³Ø¨Ø¹Ø©\nØ³Ø¨Ø¹Ù‡\nØ«Ù…Ø§Ù†ÙŠØ©\nØ«Ù…Ø§Ù†ÙŠÙ‡\nØªØ³Ø¹Ø©\nØªØ³Ø¹Ù‡\nï»‹ïº¸ïº®ïº“\nï»‹ïº¸ïº®Ù‡\nØ¹Ø´Ø±ÙˆÙ†\nØ¹Ø´Ø±ÙŠÙ†\nØ«Ù„Ø§Ø«ÙˆÙ†\nØ«Ù„Ø§Ø«ÙŠÙ†\nØ§Ø±Ø¨Ø¹ÙˆÙ†\nØ§Ø±Ø¨Ø¹ÙŠÙ†\nØ£Ø±Ø¨Ø¹ÙˆÙ†\nØ£Ø±Ø¨Ø¹ÙŠÙ†\nØ®Ù…Ø³ÙˆÙ†\nØ®Ù…Ø³ÙŠÙ†\nØ³ØªÙˆÙ†\nØ³ØªÙŠÙ†\nØ³Ø¨Ø¹ÙˆÙ†\nØ³Ø¨Ø¹ÙŠÙ†\nØ«Ù…Ø§Ù†ÙˆÙ†\nØ«Ù…Ø§Ù†ÙŠÙ†\nØªØ³Ø¹ÙˆÙ†\nØªØ³Ø¹ÙŠÙ†\nÙ…Ø§Ø¦ØªÙŠÙ†\nÙ…Ø§Ø¦ØªØ§Ù†\nØ«Ù„Ø§Ø«Ù…Ø§Ø¦Ø©\nØ®Ù…Ø³Ù…Ø§Ø¦Ø©\nØ³Ø¨Ø¹Ù…Ø§Ø¦Ø©\nØ§Ù„Ù\nØ¢Ù„Ø§Ù\nÙ…Ù„Ø§ÙŠÙŠÙ†\nÙ…Ù„ÙŠÙˆÙ†\nÙ…Ù„ÙŠØ§Ø±\nÙ…Ù„ÙŠØ§Ø±Ø§Øª\n'.split())
A:spacy.lang.ar.lex_attrs._ordinal_words->set('\nØ§ÙˆÙ„\nØ£ÙˆÙ„\nØ­Ø§Ø¯\nÙˆØ§Ø­Ø¯\nØ«Ø§Ù†\nØ«Ø§Ù†ÙŠ\nØ«Ø§Ù„Ø«\nØ±Ø§Ø¨Ø¹\nØ®Ø§Ù…Ø³\nØ³Ø§Ø¯Ø³\nØ³Ø§Ø¨Ø¹\nØ«Ø§Ù…Ù†\nØªØ§Ø³Ø¹\nØ¹Ø§Ø´Ø±\n'.split())
A:spacy.lang.ar.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.ar.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.ar.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ar/tokenizer_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ar/stop_words.py----------------------------------------
A:spacy.lang.ar.stop_words.STOP_WORDS->set('\nÙ…Ù†\nÙ†Ø­Ùˆ\nÙ„Ø¹Ù„\nØ¨Ù…Ø§\nØ¨ÙŠÙ†\nÙˆØ¨ÙŠÙ†\nØ§ÙŠØ¶Ø§\nÙˆØ¨ÙŠÙ†Ù…Ø§\nØªØ­Øª\nÙ…Ø«Ù„Ø§\nÙ„Ø¯ÙŠ\nØ¹Ù†Ù‡\nÙ…Ø¹\nÙ‡ÙŠ\nÙˆÙ‡Ø°Ø§\nÙˆØ§Ø°Ø§\nÙ‡Ø°Ø§Ù†\nØ§Ù†Ù‡\nØ¨ÙŠÙ†Ù…Ø§\nØ£Ù…Ø³Ù‰\nÙˆØ³ÙˆÙ\nÙˆÙ„Ù…\nÙ„Ø°Ù„Ùƒ\nØ¥Ù„Ù‰\nÙ…Ù†Ù‡\nÙ…Ù†Ù‡Ø§\nÙƒÙ…Ø§\nØ¸Ù„\nÙ‡Ù†Ø§\nØ¨Ù‡\nÙƒØ°Ù„Ùƒ\nØ§Ù…Ø§\nÙ‡Ù…Ø§\nØ¨Ø¹Ø¯\nØ¨ÙŠÙ†Ù‡Ù…\nØ§Ù„ØªÙŠ\nØ£Ø¨Ùˆ\nØ§Ø°Ø§\nØ¨Ø¯Ù„Ø§\nÙ„Ù‡Ø§\nØ£Ù…Ø§Ù…\nÙŠÙ„ÙŠ\nØ­ÙŠÙ†\nØ¶Ø¯\nØ§Ù„Ø°ÙŠ\nÙ‚Ø¯\nØµØ§Ø±\nØ¥Ø°Ø§\nÙ…Ø§Ø¨Ø±Ø­\nÙ‚Ø¨Ù„\nÙƒÙ„\nÙˆÙ„ÙŠØ³Øª\nØ§Ù„Ø°ÙŠÙ†\nÙ„Ù‡Ø°Ø§\nÙˆØ«ÙŠ\nØ§Ù†Ù‡Ù…\nØ¨Ø§Ù„Ù„ØªÙŠ\nÙ…Ø§ÙØªØ¦\nÙˆÙ„Ø§\nØ¨Ù‡Ø°Ù‡\nØ¨Ø­ÙŠØ«\nÙƒÙŠÙ\nÙˆÙ„Ù‡\nØ¹Ù„ÙŠ\nØ¨Ø§Øª\nÙ„Ø§Ø³ÙŠÙ…Ø§\nØ­ØªÙ‰\nÙˆÙ‚Ø¯\nÙˆ\nØ£Ù…Ø§\nÙÙŠÙ‡Ø§\nØ¨Ù‡Ø°Ø§\nÙ„Ø°Ø§\nØ­ÙŠØ«\nÙ„Ù‚Ø¯\nØ¥Ù†\nÙØ¥Ù†\nØ§ÙˆÙ„\nÙ„ÙŠØª\nÙØ§Ù„Ù„ØªÙŠ\nÙˆÙ„Ù‚Ø¯\nÙ„Ø³ÙˆÙ\nÙ‡Ø°Ù‡\nÙˆÙ„Ù…Ø§Ø°Ø§\nÙ…Ø¹Ù‡\nØ§Ù„Ø­Ø§Ù„ÙŠ\nØ¨Ø¥Ù†\nØ­ÙˆÙ„\nÙÙŠ\nØ¹Ù„ÙŠÙ‡\nÙ…Ø§ÙŠØ²Ø§Ù„\nÙˆÙ„Ø¹Ù„\nØ£Ù†Ù‡\nØ£Ø¶Ø­Ù‰\nØ§ÙŠ\nØ³ØªÙƒÙˆÙ†\nÙ„Ù†\nØ£Ù†\nØ¶Ù…Ù†\nÙˆØ¹Ù„Ù‰\nØ§Ù…Ø³Ù‰\nØ§Ù„ÙŠ\nØ°Ø§Øª\nÙˆÙ„Ø§ÙŠØ²Ø§Ù„\nØ°Ù„Ùƒ\nÙÙ‚Ø¯\nÙ‡Ù…\nØ£ÙŠ\nØ¹Ù†Ø¯\nØ§Ø¨Ù†\nØ£Ùˆ\nÙÙ‡Ùˆ\nÙØ§Ù†Ù‡\nØ³ÙˆÙ\nÙ…Ø§\nØ¢Ù„\nÙƒÙ„Ø§\nØ¹Ù†Ù‡Ø§\nÙˆÙƒØ°Ù„Ùƒ\nÙ„ÙŠØ³Øª\nÙ„Ù…\nÙˆØ£Ù†\nÙ…Ø§Ø°Ø§\nÙ„Ùˆ\nÙˆÙ‡Ù„\nØ§Ù„Ù„ØªÙŠ\nÙˆÙ„Ø°Ø§\nÙŠÙ…ÙƒÙ†\nÙÙŠÙ‡\nØ§Ù„Ø§\nØ¹Ù„ÙŠÙ‡Ø§\nÙˆØ¨ÙŠÙ†Ù‡Ù…\nÙŠÙˆÙ…\nÙˆØ¨Ù…Ø§\nÙ„Ù…Ø§\nÙÙƒØ§Ù†\nØ§Ø¶Ø­Ù‰\nØ§ØµØ¨Ø­\nÙ„Ù‡Ù…\nØ¨Ù‡Ø§\nØ§Ùˆ\nØ§Ù„Ø°Ù‰\nØ§Ù„Ù‰\nØ¥Ù„ÙŠ\nÙ‚Ø§Ù„\nÙˆØ§Ù„ØªÙŠ\nÙ„Ø§Ø²Ø§Ù„\nØ£ØµØ¨Ø­\nÙˆÙ„Ù‡Ø°Ø§\nÙ…Ø«Ù„\nÙˆÙƒØ§Ù†Øª\nÙ„ÙƒÙ†Ù‡\nØ¨Ø°Ù„Ùƒ\nÙ‡Ø°Ø§\nÙ„Ù…Ø§Ø°Ø§\nÙ‚Ø§Ù„Øª\nÙÙ‚Ø·\nÙ„ÙƒÙ†\nÙ…Ù…Ø§\nÙˆÙƒÙ„\nÙˆØ§Ù†\nÙˆØ£Ø¨Ùˆ\nÙˆÙ…Ù†\nÙƒØ§Ù†\nÙ…Ø§Ø²Ø§Ù„\nÙ‡Ù„\nØ¨ÙŠÙ†Ù‡Ù†\nÙ‡Ùˆ\nÙˆÙ…Ø§\nØ¹Ù„Ù‰\nÙˆÙ‡Ùˆ\nÙ„Ø£Ù†\nÙˆØ§Ù„Ù„ØªÙŠ\nÙˆØ§Ù„Ø°ÙŠ\nØ¯ÙˆÙ†\nØ¹Ù†\nÙˆØ§ÙŠØ¶Ø§\nÙ‡Ù†Ø§Ùƒ\nØ¨Ù„Ø§\nØ¬Ø¯Ø§\nØ«Ù…\nÙ…Ù†Ø°\nØ§Ù„Ù„Ø°ÙŠÙ†\nÙ„Ø§ÙŠØ²Ø§Ù„\nØ¨Ø¹Ø¶\nÙ…Ø³Ø§Ø¡\nØªÙƒÙˆÙ†\nÙÙ„Ø§\nØ¨ÙŠÙ†Ù†Ø§\nÙ„Ø§\nÙˆÙ„ÙƒÙ†\nØ¥Ø°\nÙˆØ£Ø«Ù†Ø§Ø¡\nÙ„ÙŠØ³\nÙˆÙ…Ø¹\nÙÙŠÙ‡Ù…\nÙˆÙ„Ø³ÙˆÙ\nØ¨Ù„\nØªÙ„Ùƒ\nØ£Ø­Ø¯\nÙˆÙ‡ÙŠ\nÙˆÙƒØ§Ù†\nÙˆÙ…Ù†Ù‡Ø§\nÙˆÙÙŠ\nÙ…Ø§Ø§Ù†ÙÙƒ\nØ§Ù„ÙŠÙˆÙ…\nÙˆÙ…Ø§Ø°Ø§\nÙ‡Ø¤Ù„Ø§Ø¡\nÙˆÙ„ÙŠØ³\nÙ„Ù‡\nØ£Ø«Ù†Ø§Ø¡\nØ¨Ø¯\nØ§Ù„ÙŠÙ‡\nÙƒØ£Ù†\nØ§Ù„ÙŠÙ‡Ø§\nØ¨ØªÙ„Ùƒ\nÙŠÙƒÙˆÙ†\nÙˆÙ„Ù…Ø§\nÙ‡Ù†\nÙˆØ§Ù„Ù‰\nÙƒØ§Ù†Øª\nÙˆÙ‚Ø¨Ù„\nØ§Ù†\nÙ„Ø¯Ù‰\nØ¥Ø°Ù…Ø§\nØ¥Ø°Ù†\nØ£Ù\nØ£Ù‚Ù„\nØ£ÙƒØ«Ø±\nØ£Ù„Ø§\nØ¥Ù„Ø§\nØ§Ù„Ù„Ø§ØªÙŠ\nØ§Ù„Ù„Ø§Ø¦ÙŠ\nØ§Ù„Ù„ØªØ§Ù†\nØ§Ù„Ù„ØªÙŠØ§\nØ§Ù„Ù„ØªÙŠÙ†\nØ§Ù„Ù„Ø°Ø§Ù†\nØ§Ù„Ù„ÙˆØ§ØªÙŠ\nØ¥Ù„ÙŠÙƒ\nØ¥Ù„ÙŠÙƒÙ…\nØ¥Ù„ÙŠÙƒÙ…Ø§\nØ¥Ù„ÙŠÙƒÙ†\nØ£Ù…\nØ£Ù…Ø§\nØ¥Ù…Ø§\nØ¥Ù†Ø§\nØ£Ù†Ø§\nØ£Ù†Øª\nØ£Ù†ØªÙ…\nØ£Ù†ØªÙ…Ø§\nØ£Ù†ØªÙ†\nØ¥Ù†Ù…Ø§\nØ¥Ù†Ù‡\nØ£Ù†Ù‰\nØ£Ù†Ù‰\nØ¢Ù‡\nØ¢Ù‡Ø§\nØ£ÙˆÙ„Ø§Ø¡\nØ£ÙˆÙ„Ø¦Ùƒ\nØ£ÙˆÙ‡\nØ¢ÙŠ\nØ£ÙŠÙ‡Ø§\nØ¥ÙŠ\nØ£ÙŠÙ†\nØ£ÙŠÙ†\nØ£ÙŠÙ†Ù…Ø§\nØ¥ÙŠÙ‡\nØ¨Ø®\nØ¨Ø³\nØ¨Ùƒ\nØ¨ÙƒÙ…\nØ¨ÙƒÙ…\nØ¨ÙƒÙ…Ø§\nØ¨ÙƒÙ†\nØ¨Ù„Ù‰\nØ¨Ù…Ø§Ø°Ø§\nØ¨Ù…Ù†\nØ¨Ù†Ø§\nØ¨Ù‡Ù…\nØ¨Ù‡Ù…Ø§\nØ¨Ù‡Ù†\nØ¨ÙŠ\nØ¨ÙŠØ¯\nØªÙ„ÙƒÙ…\nØªÙ„ÙƒÙ…Ø§\nØªÙ‡\nØªÙŠ\nØªÙŠÙ†\nØªÙŠÙ†Ùƒ\nØ«Ù…Ø©\nØ­Ø§Ø´Ø§\nØ­Ø¨Ø°Ø§\nØ­ÙŠØ«Ù…Ø§\nØ®Ù„Ø§\nØ°Ø§\nØ°Ø§Ùƒ\nØ°Ø§Ù†\nØ°Ø§Ù†Ùƒ\nØ°Ù„ÙƒÙ…\nØ°Ù„ÙƒÙ…Ø§\nØ°Ù„ÙƒÙ†\nØ°Ù‡\nØ°Ùˆ\nØ°ÙˆØ§\nØ°ÙˆØ§ØªØ§\nØ°ÙˆØ§ØªÙŠ\nØ°ÙŠ\nØ°ÙŠÙ†\nØ°ÙŠÙ†Ùƒ\nØ±ÙŠØ«\nØ³ÙˆÙ‰\nØ´ØªØ§Ù†\nØ¹Ø¯Ø§\nØ¹Ø³Ù‰\nØ¹Ù„\nØ¹Ù„ÙŠÙƒ\nØ¹Ù…Ø§\nØºÙŠØ±\nÙØ¥Ø°Ø§\nÙÙ…Ù†\nÙÙŠÙ…\nÙÙŠÙ…Ø§\nÙƒØ£Ù†Ù…Ø§\nÙƒØ£ÙŠ\nÙƒØ£ÙŠÙ†\nÙƒØ°Ø§\nÙƒÙ„Ø§Ù‡Ù…Ø§\nÙƒÙ„ØªØ§\nÙƒÙ„Ù…Ø§\nÙƒÙ„ÙŠÙƒÙ…Ø§\nÙƒÙ„ÙŠÙ‡Ù…Ø§\nÙƒÙ…\nÙƒÙ…\nÙƒÙŠ\nÙƒÙŠØª\nÙƒÙŠÙÙ…Ø§\nÙ„Ø³Øª\nÙ„Ø³ØªÙ…\nÙ„Ø³ØªÙ…Ø§\nÙ„Ø³ØªÙ†\nÙ„Ø³Ù†\nÙ„Ø³Ù†Ø§\nÙ„Ùƒ\nÙ„ÙƒÙ…\nÙ„ÙƒÙ…Ø§\nÙ„ÙƒÙ†Ù…Ø§\nÙ„ÙƒÙŠ\nÙ„ÙƒÙŠÙ„Ø§\nÙ„Ù†Ø§\nÙ„Ù‡Ù…Ø§\nÙ„Ù‡Ù†\nÙ„ÙˆÙ„Ø§\nÙ„ÙˆÙ…Ø§\nÙ„ÙŠ\nÙ„Ø¦Ù†\nÙ„ÙŠØ³Ø§\nÙ„ÙŠØ³ØªØ§\nÙ„ÙŠØ³ÙˆØ§\nÙ…ØªÙ‰\nÙ…Ø°\nÙ…Ù…Ù†\nÙ…Ù‡\nÙ…Ù‡Ù…Ø§\nÙ†Ø­Ù†\nÙ†Ø¹Ù…\nÙ‡Ø§\nÙ‡Ø§ØªØ§Ù†\nÙ‡Ø§ØªÙ‡\nÙ‡Ø§ØªÙŠ\nÙ‡Ø§ØªÙŠÙ†\nÙ‡Ø§Ùƒ\nÙ‡Ø§Ù‡Ù†Ø§\nÙ‡Ø°ÙŠ\nÙ‡Ø°ÙŠÙ†\nÙ‡ÙƒØ°Ø§\nÙ‡Ù„Ø§\nÙ‡Ù†Ø§Ù„Ùƒ\nÙ‡ÙŠØ§\nÙ‡ÙŠØª\nÙ‡ÙŠÙ‡Ø§Øª\nÙˆØ§Ù„Ø°ÙŠÙ†\nÙˆØ¥Ø°\nÙˆØ¥Ø°Ø§\nÙˆØ¥Ù†\nÙˆÙ„Ùˆ\nÙŠØ§\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ar/punctuation.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ja/__init__.py----------------------------------------
A:spacy.lang.ja.__init__.ShortUnitWord->namedtuple('ShortUnitWord', ['surface', 'lemma', 'pos'])
A:spacy.lang.ja.__init__.node->tokenizer.parseToNode(text)
A:spacy.lang.ja.__init__.parts->tokenizer.parseToNode(text).feature.split(',')
A:spacy.lang.ja.__init__.pos->','.join(parts[0:4])
A:spacy.lang.ja.__init__.self.tokenizer->try_mecab_import().Tagger()
A:spacy.lang.ja.__init__.(dtokens, spaces)->detailed_tokens(self.tokenizer, text)
A:spacy.lang.ja.__init__.doc->Doc(self.vocab, words=words, spaces=spaces)
A:spacy.lang.ja.__init__.token.tag_->resolve_pos(dtoken)
A:spacy.lang.ja.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
spacy.lang.ja.__init__.Japanese(Language)
spacy.lang.ja.__init__.Japanese.make_doc(self,text)
spacy.lang.ja.__init__.JapaneseDefaults(Language.Defaults)
spacy.lang.ja.__init__.JapaneseDefaults.create_tokenizer(cls,nlp=None)
spacy.lang.ja.__init__.JapaneseTokenizer(self,cls,nlp=None)
spacy.lang.ja.__init__.JapaneseTokenizer.__init__(self,cls,nlp=None)
spacy.lang.ja.__init__.detailed_tokens(tokenizer,text)
spacy.lang.ja.__init__.pickle_japanese(instance)
spacy.lang.ja.__init__.resolve_pos(token)
spacy.lang.ja.__init__.try_mecab_import()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ja/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ja/stop_words.py----------------------------------------
A:spacy.lang.ja.stop_words.STOP_WORDS->set('\nã‚ ã‚ã£ ã‚ã¾ã‚Š ã‚ã‚Š ã‚ã‚‹ ã‚ã‚‹ã„ã¯ ã‚ã‚Œ\nã„ ã„ã„ ã„ã† ã„ã ã„ãšã‚Œ ã„ã£ ã„ã¤ ã„ã‚‹ ã„ã‚\nã†ã¡\nãˆ\nãŠ ãŠã„ ãŠã‘ ãŠã‚ˆã³ ãŠã‚‰ ãŠã‚Š\nã‹ ã‹ã‘ ã‹ã¤ ã‹ã¤ã¦ ã‹ãªã‚Š ã‹ã‚‰ ãŒ\nã ãã£ã‹ã‘\nãã‚‹ ãã‚“\nã“ ã“ã† ã“ã“ ã“ã¨ ã“ã® ã“ã‚Œ ã” ã”ã¨\nã• ã•ã‚‰ã« ã•ã‚“\nã— ã—ã‹ ã—ã‹ã— ã—ã¾ã† ã—ã¾ã£ ã—ã‚ˆã†\nã™ ã™ã ã™ã¹ã¦ ã™ã‚‹ ãš\nã› ã›ã„ ã›ã‚‹\nãã† ãã“ ãã—ã¦ ãã® ãã‚Œ ãã‚Œãã‚Œ\nãŸ ãŸã„ ãŸã ã— ãŸã¡ ãŸã‚ ãŸã‚‰ ãŸã‚Š ã  ã ã‘ ã ã£\nã¡ ã¡ã‚ƒã‚“\nã¤ ã¤ã„ ã¤ã‘ ã¤ã¤\nã¦ ã§ ã§ã ã§ãã‚‹ ã§ã™\nã¨ ã¨ã ã¨ã“ã‚ ã¨ã£ ã¨ã‚‚ ã©ã†\nãª ãªã„ ãªãŠ ãªã‹ã£ ãªãŒã‚‰ ãªã ãªã‘ã‚Œ ãªã— ãªã£ ãªã© ãªã‚‰ ãªã‚Š ãªã‚‹\nã« ã«ã¦\nã¬\nã­\nã® ã®ã¡ ã®ã¿\nã¯ ã¯ã˜ã‚ ã°\nã²ã¨\nã¶ã‚Š\nã¸ ã¹ã\nã»ã‹ ã»ã¨ã‚“ã© ã»ã© ã»ã¼\nã¾ ã¾ã™ ã¾ãŸ ã¾ã§ ã¾ã¾\nã¿\nã‚‚ ã‚‚ã† ã‚‚ã£ ã‚‚ã¨ ã‚‚ã®\nã‚„ ã‚„ã£\nã‚ˆ ã‚ˆã† ã‚ˆã ã‚ˆã£ ã‚ˆã‚Š ã‚ˆã‚‹ ã‚ˆã‚Œ\nã‚‰ ã‚‰ã—ã„ ã‚‰ã‚Œ ã‚‰ã‚Œã‚‹\nã‚‹\nã‚Œ ã‚Œã‚‹\nã‚’\nã‚“\nä¸€\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ja/tag_map.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/lv/__init__.py----------------------------------------
A:spacy.lang.lv.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
spacy.lang.lv.__init__.Latvian(Language)
spacy.lang.lv.__init__.LatvianDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/lv/stop_words.py----------------------------------------
A:spacy.lang.lv.stop_words.STOP_WORDS->set('\naiz\nap\napakÅ¡\napakÅ¡pus\nar\narÄ«\naugÅ¡pus\nbet\nbez\nbija\nbiji\nbiju\nbijÄm\nbijÄt\nbÅ«s\nbÅ«si\nbÅ«siet\nbÅ«sim\nbÅ«t\nbÅ«Å¡u\ncaur\ndiemÅ¾Ä“l\ndiezin\ndroÅ¡i\ndÄ“Ä¼\nesam\nesat\nesi\nesmu\ngan\ngar\niekam\niekams\niekÄm\niekÄms\niekÅ¡\niekÅ¡pus\nik\nir\nit\nitin\niz\nja\njau\njeb\njebÅ¡u\njel\njo\njÄ\nka\nkamÄ“r\nkaut\nkolÄ«dz\nkopÅ¡\nkÄ\nkÄ¼uva\nkÄ¼uvi\nkÄ¼uvu\nkÄ¼uvÄm\nkÄ¼uvÄt\nkÄ¼Å«s\nkÄ¼Å«si\nkÄ¼Å«siet\nkÄ¼Å«sim\nkÄ¼Å«st\nkÄ¼Å«stam\nkÄ¼Å«stat\nkÄ¼Å«sti\nkÄ¼Å«stu\nkÄ¼Å«t\nkÄ¼Å«Å¡u\nlabad\nlai\nlejpus\nlÄ«dz\nlÄ«dzko\nne\nnebÅ«t\nnedz\nnekÄ\nnevis\nnezin\nno\nnu\nnÄ“\notrpus\npa\npar\npat\npie\npirms\npret\npriekÅ¡\npÄr\npÄ“c\nstarp\ntad\ntak\ntapi\ntaps\ntapsi\ntapsiet\ntapsim\ntapt\ntapÄt\ntapÅ¡u\ntaÄu\nte\ntiec\ntiek\ntiekam\ntiekat\ntieku\ntik\ntika\ntikai\ntiki\ntikko\ntiklab\ntiklÄ«dz\ntiks\ntiksiet\ntiksim\ntikt\ntiku\ntikvien\ntikÄm\ntikÄt\ntikÅ¡u\ntomÄ“r\ntopat\nturpretim\nturpretÄ«\ntÄ\ntÄdÄ“Ä¼\ntÄlab\ntÄpÄ“c\nun\nuz\nvai\nvar\nvarat\nvarÄ“ja\nvarÄ“ji\nvarÄ“ju\nvarÄ“jÄm\nvarÄ“jÄt\nvarÄ“s\nvarÄ“si\nvarÄ“siet\nvarÄ“sim\nvarÄ“t\nvarÄ“Å¡u\nvien\nvirs\nvirspus\nvis\nviÅ†pus\nzem\nÄrpus\nÅ¡aipus\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/et/__init__.py----------------------------------------
A:spacy.lang.et.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
spacy.lang.et.__init__.Estonian(Language)
spacy.lang.et.__init__.EstonianDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/et/stop_words.py----------------------------------------
A:spacy.lang.et.stop_words.STOP_WORDS->set('\naga\nei\net\nja\njah\nkas\nkui\nkÃµik\nma\nme\nmida\nmidagi\nmind\nminu\nmis\nmu\nmul\nmulle\nnad\nnii\noled\nolen\noli\noma\non\npole\nsa\nseda\nsee\nselle\nsiin\nsiis\nta\nte\nÃ¤ra\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/af/__init__.py----------------------------------------
A:spacy.lang.af.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
spacy.lang.af.__init__.Afrikaans(Language)
spacy.lang.af.__init__.AfrikaansDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/af/stop_words.py----------------------------------------
A:spacy.lang.af.stop_words.STOP_WORDS->set("\n'n\naan\naf\nal\nas\nbaie\nby\ndaar\ndag\ndat\ndie\ndit\neen\nek\nen\ngaan\ngesÃª\nhaar\nhet\nhom\nhulle\nhy\nin\nis\njou\njy\nkan\nkom\nma\nmaar\nmet\nmy\nna\nnie\nom\nons\nop\nsaam\nsal\nse\nsien\nso\nsy\nte\ntoe\nuit\nvan\nvir\nwas\nwat\nÅ‰\n".split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/tl/__init__.py----------------------------------------
A:spacy.lang.tl.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.tl.__init__.lex_attr_getters[NORM]->add_lookups(Language.Defaults.lex_attr_getters[NORM], BASE_NORMS)
A:spacy.lang.tl.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS, TOKENIZER_EXCEPTIONS)
spacy.lang.tl.__init__.Tagalog(Language)
spacy.lang.tl.__init__.TagalogDefaults(Language.Defaults)
spacy.lang.tl.__init__._return_tl(_)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/tl/lex_attrs.py----------------------------------------
A:spacy.lang.tl.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.tl.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.tl.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/tl/tokenizer_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/tl/stop_words.py----------------------------------------
A:spacy.lang.tl.stop_words.STOP_WORDS->set('\nakin\naking\nako\nalin\nam\namin\naming\nang\nano\nanumang\napat\nat\natin\nating\nay\nbababa\nbago\nbakit\nbawat\nbilang\ndahil\ndalawa\ndapat\ndin\ndito\ndoon\ngagawin\ngayunman\nginagawa\nginawa\nginawang\ngumawa\ngusto\nhabang\nhanggang\nhindi\nhuwag\niba\nibaba\nibabaw\nibig\nikaw\nilagay\nilalim\nilan\ninyong\nisa\nisang\nitaas\nito\niyo\niyon\niyong\nka\nkahit\nkailangan\nkailanman\nkami\nkanila\nkanilang\nkanino\nkanya\nkanyang\nkapag\nkapwa\nkaramihan\nkatiyakan\nkatulad\nkaya\nkaysa\nko\nkong\nkulang\nkumuha\nkung\nlaban\nlahat\nlamang\nlikod\nlima\nmaaari\nmaaaring\nmaging\nmahusay\nmakita\nmarami\nmarapat\nmasyado\nmay\nmayroon\nmga\nminsan\nmismo\nmula\nmuli\nna\nnabanggit\nnaging\nnagkaroon\nnais\nnakita\nnamin\nnapaka\nnarito\nnasaan\nng\nngayon\nni\nnila\nnilang\nnito\nniya\nniyang\nnoon\no\npa\npaano\npababa\npaggawa\npagitan\npagkakaroon\npagkatapos\npalabas\npamamagitan\npanahon\npangalawa\npara\nparaan\npareho\npataas\npero\npumunta\npumupunta\nsa\nsaan\nsabi\nsabihin\nsarili\nsila\nsino\nsiya\ntatlo\ntayo\ntulad\ntungkol\nuna\nwalang\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/el/get_pos_from_wiktionary.py----------------------------------------
A:spacy.lang.el.get_pos_from_wiktionary.regex->re.compile('==={{(\\w+)\\|el}}===')
A:spacy.lang.el.get_pos_from_wiktionary.regex2->re.compile('==={{(\\w+ \\w+)\\|el}}===')
A:spacy.lang.el.get_pos_from_wiktionary.title->title.lower().lower()
A:spacy.lang.el.get_pos_from_wiktionary.all_regex->re.compile('==={{(\\w+)\\|el}}===').findall(text)
A:spacy.lang.el.get_pos_from_wiktionary.words->sorted(expected_parts_dict[i])
spacy.lang.el.get_pos_from_wiktionary.get_pos_from_wiktionary()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/el/__init__.py----------------------------------------
A:spacy.lang.el.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.el.__init__.lex_attr_getters[NORM]->add_lookups(Language.Defaults.lex_attr_getters[NORM], BASE_NORMS, NORM_EXCEPTIONS)
A:spacy.lang.el.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS, TOKENIZER_EXCEPTIONS)
A:spacy.lang.el.__init__.lookups->Lookups()
spacy.lang.el.__init__.Greek(Language)
spacy.lang.el.__init__.GreekDefaults(Language.Defaults)
spacy.lang.el.__init__.GreekDefaults.create_lemmatizer(cls,nlp=None,lookups=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/el/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/el/lemmatizer.py----------------------------------------
A:spacy.lang.el.lemmatizer.string->string.lower().lower()
spacy.lang.el.GreekLemmatizer(Lemmatizer)
spacy.lang.el.GreekLemmatizer.lemmatize(self,string,index,exceptions,rules)
spacy.lang.el.lemmatizer.GreekLemmatizer(Lemmatizer)
spacy.lang.el.lemmatizer.GreekLemmatizer.lemmatize(self,string,index,exceptions,rules)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/el/tag_map_general.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/el/lex_attrs.py----------------------------------------
A:spacy.lang.el.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.el.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('^')
spacy.lang.el.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/el/tokenizer_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/el/syntax_iterators.py----------------------------------------
A:spacy.lang.el.syntax_iterators.conj->doc.vocab.strings.add('conj')
A:spacy.lang.el.syntax_iterators.nmod->doc.vocab.strings.add('nmod')
A:spacy.lang.el.syntax_iterators.np_label->doc.vocab.strings.add('NP')
A:spacy.lang.el.syntax_iterators.seen->set()
spacy.lang.el.syntax_iterators.noun_chunks(obj)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/el/stop_words.py----------------------------------------
A:spacy.lang.el.stop_words.STOP_WORDS->set('\nÎ±Î´Î¹Î¬ÎºÎ¿Ï€Î± Î±Î¹ Î±ÎºÏŒÎ¼Î± Î±ÎºÏŒÎ¼Î· Î±ÎºÏÎ¹Î²ÏÏ‚ Î¬Î»Î»Î± Î±Î»Î»Î¬ Î±Î»Î»Î±Ï‡Î¿Ï Î¬Î»Î»ÎµÏ‚ Î¬Î»Î»Î· Î¬Î»Î»Î·Î½\nÎ¬Î»Î»Î·Ï‚ Î±Î»Î»Î¹ÏÏ‚ Î±Î»Î»Î¹ÏÏ„Î¹ÎºÎ± Î¬Î»Î»Î¿ Î¬Î»Î»Î¿Î¹ Î±Î»Î»Î¿Î¹ÏÏ‚ Î±Î»Î»Î¿Î¹ÏÏ„Î¹ÎºÎ± Î¬Î»Î»Î¿Î½ Î¬Î»Î»Î¿Ï‚ Î¬Î»Î»Î¿Ï„Îµ Î±Î»Î»Î¿Ï\nÎ¬Î»Î»Î¿Ï…Ï‚ Î¬Î»Î»Ï‰Î½ Î¬Î¼Î± Î¬Î¼ÎµÏƒÎ± Î±Î¼Î­ÏƒÏ‰Ï‚ Î±Î½ Î±Î½Î¬ Î±Î½Î¬Î¼ÎµÏƒÎ± Î±Î½Î±Î¼ÎµÏ„Î±Î¾Ï Î¬Î½ÎµÏ… Î±Î½Ï„Î¯ Î±Î½Ï„Î¯Ï€ÎµÏÎ± Î±Î½Ï„Î¯Ï‚\nÎ¬Î½Ï‰ Î±Î½Ï‰Ï„Î­ÏÏ‰ Î¬Î¾Î±Ï†Î½Î± Î±Ï€ Î±Ï€Î­Î½Î±Î½Ï„Î¹ Î±Ï€ÏŒ Î±Ï€ÏŒÏˆÎµ Î¬ÏÎ± Î¬ÏÎ±Î³Îµ Î±ÏÎºÎµÏ„Î¬ Î±ÏÎºÎµÏ„Î­Ï‚\nÎ±ÏÏ‡Î¹ÎºÎ¬ Î±Ï‚ Î±ÏÏÎ¹Î¿ Î±Ï…Ï„Î¬ Î±Ï…Ï„Î­Ï‚ Î±Ï…Ï„Î® Î±Ï…Ï„Î®Î½ Î±Ï…Ï„Î®Ï‚ Î±Ï…Ï„ÏŒ Î±Ï…Ï„Î¿Î¯ Î±Ï…Ï„ÏŒÎ½ Î±Ï…Ï„ÏŒÏ‚ Î±Ï…Ï„Î¿Ï Î±Ï…Ï„Î¿ÏÏ‚\nÎ±Ï…Ï„ÏÎ½ Î±Ï†ÏŒÏ„Î¿Ï… Î±Ï†Î¿Ï\n\nÎ²Î­Î²Î±Î¹Î± Î²ÎµÎ²Î±Î¹ÏŒÏ„Î±Ï„Î±\n\nÎ³Î¹ Î³Î¹Î± Î³Î¹Î±Ï„Î¯ Î³ÏÎ®Î³Î¿ÏÎ± Î³ÏÏÏ‰\n\nÎ´Î± Î´Îµ Î´ÎµÎ¯Î½Î± Î´ÎµÎ½ Î´ÎµÎ¾Î¹Î¬ Î´Î®Î¸ÎµÎ½ Î´Î·Î»Î±Î´Î® Î´Î¹ Î´Î¹Î± Î´Î¹Î±ÏÎºÏÏ‚ Î´Î¹ÎºÎ¬ Î´Î¹ÎºÏŒ Î´Î¹ÎºÎ¿Î¯ Î´Î¹ÎºÏŒÏ‚ Î´Î¹ÎºÎ¿Ï\nÎ´Î¹ÎºÎ¿ÏÏ‚ Î´Î¹ÏŒÎ»Î¿Ï… Î´Î¯Ï€Î»Î± Î´Î¯Ï‡Ï‰Ï‚\n\nÎµÎ¬Î½ ÎµÎ±Ï…Ï„ÏŒ ÎµÎ±Ï…Ï„ÏŒÎ½ ÎµÎ±Ï…Ï„Î¿Ï ÎµÎ±Ï…Ï„Î¿ÏÏ‚ ÎµÎ±Ï…Ï„ÏÎ½ Î­Î³ÎºÎ±Î¹ÏÎ± ÎµÎ³ÎºÎ±Î¯ÏÏ‰Ï‚ ÎµÎ³Ï ÎµÎ´Ï ÎµÎ¹Î´ÎµÎ¼Î® ÎµÎ¯Î¸Îµ ÎµÎ¯Î¼Î±Î¹\nÎµÎ¯Î¼Î±ÏƒÏ„Îµ ÎµÎ¯Î½Î±Î¹ ÎµÎ¹Ï‚ ÎµÎ¯ÏƒÎ±Î¹ ÎµÎ¯ÏƒÎ±ÏƒÏ„Îµ ÎµÎ¯ÏƒÏ„Îµ ÎµÎ¯Ï„Îµ ÎµÎ¯Ï‡Î± ÎµÎ¯Ï‡Î±Î¼Îµ ÎµÎ¯Ï‡Î±Î½ ÎµÎ¯Ï‡Î±Ï„Îµ ÎµÎ¯Ï‡Îµ ÎµÎ¯Ï‡ÎµÏ‚ Î­ÎºÎ±ÏƒÏ„Î±\nÎ­ÎºÎ±ÏƒÏ„ÎµÏ‚ Î­ÎºÎ±ÏƒÏ„Î· Î­ÎºÎ±ÏƒÏ„Î·Î½ Î­ÎºÎ±ÏƒÏ„Î·Ï‚ Î­ÎºÎ±ÏƒÏ„Î¿ Î­ÎºÎ±ÏƒÏ„Î¿Î¹ Î­ÎºÎ±ÏƒÏ„Î¿Î½ Î­ÎºÎ±ÏƒÏ„Î¿Ï‚ ÎµÎºÎ¬ÏƒÏ„Î¿Ï… ÎµÎºÎ¬ÏƒÏ„Î¿Ï…Ï‚ ÎµÎºÎ¬ÏƒÏ„Ï‰Î½\nÎµÎºÎµÎ¯ ÎµÎºÎµÎ¯Î½Î± ÎµÎºÎµÎ¯Î½ÎµÏ‚ ÎµÎºÎµÎ¯Î½Î· ÎµÎºÎµÎ¯Î½Î·Î½ ÎµÎºÎµÎ¯Î½Î·Ï‚ ÎµÎºÎµÎ¯Î½Î¿ ÎµÎºÎµÎ¯Î½Î¿Î¹ ÎµÎºÎµÎ¯Î½Î¿Î½ ÎµÎºÎµÎ¯Î½Î¿Ï‚ ÎµÎºÎµÎ¯Î½Î¿Ï…\nÎµÎºÎµÎ¯Î½Î¿Ï…Ï‚ ÎµÎºÎµÎ¯Î½Ï‰Î½ ÎµÎºÏ„ÏŒÏ‚ ÎµÎ¼Î¬Ï‚ ÎµÎ¼ÎµÎ¯Ï‚ ÎµÎ¼Î­Î½Î± ÎµÎ¼Ï€ÏÏŒÏ‚ ÎµÎ½ Î­Î½Î± Î­Î½Î±Î½ Î­Î½Î±Ï‚ ÎµÎ½ÏŒÏ‚ ÎµÎ½Ï„ÎµÎ»ÏÏ‚ ÎµÎ½Ï„ÏŒÏ‚\nÎµÎ½Î±Î½Ï„Î¯Î¿Î½  ÎµÎ¾Î®Ï‚  ÎµÎ¾Î±Î¹Ï„Î¯Î±Ï‚  ÎµÏ€Î¹Ï€Î»Î­Î¿Î½ ÎµÏ€ÏŒÎ¼ÎµÎ½Î· ÎµÎ½Ï„Ï‰Î¼ÎµÏ„Î±Î¾Ï ÎµÎ½Ï ÎµÎ¾ Î­Î¾Î±Ï†Î½Î± ÎµÎ¾Î®Ïƒ ÎµÎ¾Î¯ÏƒÎ¿Ï… Î­Î¾Ï‰ ÎµÏ€Î¬Î½Ï‰\nÎµÏ€ÎµÎ¹Î´Î® Î­Ï€ÎµÎ¹Ï„Î± ÎµÏ€Î¯ ÎµÏ€Î¯ÏƒÎ·Ï‚ ÎµÏ€Î¿Î¼Î­Î½Ï‰Ï‚ ÎµÏƒÎ¬Ï‚ ÎµÏƒÎµÎ¯Ï‚ ÎµÏƒÎ­Î½Î± Î­ÏƒÏ„Ï‰ ÎµÏƒÏ ÎµÏ„Î­ÏÎ± ÎµÏ„Î­ÏÎ±Î¹ ÎµÏ„Î­ÏÎ±Ï‚ Î­Ï„ÎµÏÎµÏ‚\nÎ­Ï„ÎµÏÎ· Î­Ï„ÎµÏÎ·Ï‚ Î­Ï„ÎµÏÎ¿ Î­Ï„ÎµÏÎ¿Î¹ Î­Ï„ÎµÏÎ¿Î½ Î­Ï„ÎµÏÎ¿Ï‚ ÎµÏ„Î­ÏÎ¿Ï… Î­Ï„ÎµÏÎ¿Ï…Ï‚ ÎµÏ„Î­ÏÏ‰Î½ ÎµÏ„Î¿ÏÏ„Î± ÎµÏ„Î¿ÏÏ„ÎµÏ‚ ÎµÏ„Î¿ÏÏ„Î· ÎµÏ„Î¿ÏÏ„Î·Î½\nÎµÏ„Î¿ÏÏ„Î·Ï‚ ÎµÏ„Î¿ÏÏ„Î¿ ÎµÏ„Î¿ÏÏ„Î¿Î¹ ÎµÏ„Î¿ÏÏ„Î¿Î½ ÎµÏ„Î¿ÏÏ„Î¿Ï‚ ÎµÏ„Î¿ÏÏ„Î¿Ï… ÎµÏ„Î¿ÏÏ„Î¿Ï…Ï‚ ÎµÏ„Î¿ÏÏ„Ï‰Î½ Î­Ï„ÏƒÎ¹ ÎµÏÎ³Îµ ÎµÏ…Î¸ÏÏ‚ ÎµÏ…Ï„Ï…Ï‡ÏÏ‚ ÎµÏ†ÎµÎ¾Î®Ï‚\nÎ­Ï‡ÎµÎ¹ Î­Ï‡ÎµÎ¹Ï‚ Î­Ï‡ÎµÏ„Îµ Î­Ï‡Î¿Î¼Îµ Î­Ï‡Î¿Ï…Î¼Îµ Î­Ï‡Î¿Ï…Î½ ÎµÏ‡Ï„Î­Ï‚ Î­Ï‡Ï‰ Î­Ï‰Ï‚ Î­Î³Î¹Î½Î±Î½  Î­Î³Î¹Î½Îµ  Î­ÎºÎ±Î½Îµ  Î­Î¾Î¹  Î­Ï‡Î¿Î½Ï„Î±Ï‚\n\nÎ· Î®Î´Î· Î®Î¼Î±ÏƒÏ„Î±Î½ Î®Î¼Î±ÏƒÏ„Îµ Î®Î¼Î¿Ï…Î½ Î®ÏƒÎ±ÏƒÏ„Î±Î½ Î®ÏƒÎ±ÏƒÏ„Îµ Î®ÏƒÎ¿Ï…Î½ Î®Ï„Î±Î½ Î®Ï„Î±Î½Îµ Î®Ï„Î¿Î¹ Î®Ï„Ï„Î¿Î½\n\nÎ¸Î±\n\nÎ¹ Î¹Î´Î¯Î± Î¯Î´Î¹Î± Î¯Î´Î¹Î±Î½ Î¹Î´Î¯Î±Ï‚ Î¯Î´Î¹ÎµÏ‚ Î¯Î´Î¹Î¿ Î¯Î´Î¹Î¿Î¹ Î¯Î´Î¹Î¿Î½ Î¯Î´Î¹Î¿Ïƒ Î¯Î´Î¹Î¿Ï‚ Î¹Î´Î¯Î¿Ï… Î¯Î´Î¹Î¿Ï…Ï‚ Î¯Î´Î¹Ï‰Î½ Î¹Î´Î¯Ï‰Ï‚ Î¹Î¹ Î¹Î¹Î¹\nÎ¯ÏƒÎ±Î¼Îµ Î¯ÏƒÎ¹Î± Î¯ÏƒÏ‰Ï‚\n\nÎºÎ¬Î¸Îµ ÎºÎ±Î¸ÎµÎ¼Î¯Î± ÎºÎ±Î¸ÎµÎ¼Î¯Î±Ï‚ ÎºÎ±Î¸Î­Î½Î± ÎºÎ±Î¸Î­Î½Î±Ï‚ ÎºÎ±Î¸ÎµÎ½ÏŒÏ‚ ÎºÎ±Î¸ÎµÏ„Î¯ ÎºÎ±Î¸ÏŒÎ»Î¿Ï… ÎºÎ±Î¸ÏÏ‚ ÎºÎ±Î¹ ÎºÎ±ÎºÎ¬ ÎºÎ±ÎºÏÏ‚ ÎºÎ±Î»Î¬\nÎºÎ±Î»ÏÏ‚ ÎºÎ±Î¼Î¯Î± ÎºÎ±Î¼Î¯Î±Î½ ÎºÎ±Î¼Î¯Î±Ï‚ ÎºÎ¬Î¼Ï€Î¿ÏƒÎ± ÎºÎ¬Î¼Ï€Î¿ÏƒÎµÏ‚ ÎºÎ¬Î¼Ï€Î¿ÏƒÎ· ÎºÎ¬Î¼Ï€Î¿ÏƒÎ·Î½ ÎºÎ¬Î¼Ï€Î¿ÏƒÎ·Ï‚ ÎºÎ¬Î¼Ï€Î¿ÏƒÎ¿ ÎºÎ¬Î¼Ï€Î¿ÏƒÎ¿Î¹\nÎºÎ¬Î¼Ï€Î¿ÏƒÎ¿Î½ ÎºÎ¬Î¼Ï€Î¿ÏƒÎ¿Ï‚ ÎºÎ¬Î¼Ï€Î¿ÏƒÎ¿Ï… ÎºÎ¬Î¼Ï€Î¿ÏƒÎ¿Ï…Ï‚ ÎºÎ¬Î¼Ï€Î¿ÏƒÏ‰Î½ ÎºÎ±Î½ÎµÎ¯Ï‚ ÎºÎ¬Î½ÎµÎ½ ÎºÎ±Î½Î­Î½Î± ÎºÎ±Î½Î­Î½Î±Î½ ÎºÎ±Î½Î­Î½Î±Ï‚\nÎºÎ±Î½ÎµÎ½ÏŒÏ‚ ÎºÎ¬Ï€Î¿Î¹Î± ÎºÎ¬Ï€Î¿Î¹Î±Î½ ÎºÎ¬Ï€Î¿Î¹Î±Ï‚ ÎºÎ¬Ï€Î¿Î¹ÎµÏ‚ ÎºÎ¬Ï€Î¿Î¹Î¿ ÎºÎ¬Ï€Î¿Î¹Î¿Î¹ ÎºÎ¬Ï€Î¿Î¹Î¿Î½ ÎºÎ¬Ï€Î¿Î¹Î¿Ï‚ ÎºÎ¬Ï€Î¿Î¹Î¿Ï… ÎºÎ¬Ï€Î¿Î¹Î¿Ï…Ï‚\nÎºÎ¬Ï€Î¿Î¹Ï‰Î½ ÎºÎ¬Ï€Î¿Ï„Îµ ÎºÎ¬Ï€Î¿Ï… ÎºÎ¬Ï€Ï‰Ï‚ ÎºÎ±Ï„ ÎºÎ±Ï„Î¬ ÎºÎ¬Ï„Î¹ ÎºÎ±Ï„Î¹Ï„Î¯ ÎºÎ±Ï„ÏŒÏ€Î¹Î½ ÎºÎ¬Ï„Ï‰ ÎºÎ¹ÏŒÎ»Î±Ï‚ ÎºÎ»Ï€ ÎºÎ¿Î½Ï„Î¬ ÎºÏ„Î» ÎºÏ…ÏÎ¯Ï‰Ï‚\n\nÎ»Î¹Î³Î¬ÎºÎ¹ Î»Î¯Î³Î¿ Î»Î¹Î³ÏŒÏ„ÎµÏÎ¿ Î»ÏŒÎ³Ï‰ Î»Î¿Î¹Ï€Î¬ Î»Î¿Î¹Ï€ÏŒÎ½\n\nÎ¼Î± Î¼Î±Î¶Î¯ Î¼Î±ÎºÎ¬ÏÎ¹ Î¼Î±ÎºÏÏ…Î¬ Î¼Î¬Î»Î¹ÏƒÏ„Î± Î¼Î¬Î»Î»Î¿Î½ Î¼Î±Ï‚ Î¼Îµ Î¼ÎµÎ¸Î±ÏÏÎ¹Î¿ Î¼ÎµÎ¯Î¿Î½ Î¼Î­Î»ÎµÎ¹ Î¼Î­Î»Î»ÎµÏ„Î±Î¹ Î¼ÎµÎ¼Î¹Î¬Ï‚ Î¼ÎµÎ½\nÎ¼ÎµÏÎ¹ÎºÎ¬ Î¼ÎµÏÎ¹ÎºÎ­Ï‚ Î¼ÎµÏÎ¹ÎºÎ¿Î¯ Î¼ÎµÏÎ¹ÎºÎ¿ÏÏ‚ Î¼ÎµÏÎ¹ÎºÏÎ½ Î¼Î­ÏƒÎ± Î¼ÎµÏ„ Î¼ÎµÏ„Î¬ Î¼ÎµÏ„Î±Î¾Ï Î¼Î­Ï‡ÏÎ¹ Î¼Î· Î¼Î®Î´Îµ Î¼Î·Î½ Î¼Î®Ï€Ï‰Ï‚\nÎ¼Î®Ï„Îµ Î¼Î¹Î± Î¼Î¹Î±Î½ Î¼Î¹Î±Ï‚ Î¼ÏŒÎ»Î¹Ï‚ Î¼Î¿Î»Î¿Î½ÏŒÏ„Î¹ Î¼Î¿Î½Î¬Ï‡Î± Î¼ÏŒÎ½ÎµÏ‚ Î¼ÏŒÎ½Î· Î¼ÏŒÎ½Î·Î½ Î¼ÏŒÎ½Î·Ï‚ Î¼ÏŒÎ½Î¿ Î¼ÏŒÎ½Î¿Î¹ Î¼Î¿Î½Î¿Î¼Î¹Î¬Ï‚\nÎ¼ÏŒÎ½Î¿Ï‚ Î¼ÏŒÎ½Î¿Ï… Î¼ÏŒÎ½Î¿Ï…Ï‚ Î¼ÏŒÎ½Ï‰Î½ Î¼Î¿Ï… Î¼Ï€Î¿ÏÎµÎ¯ Î¼Ï€Î¿ÏÎ¿ÏÎ½ Î¼Ï€ÏÎ¿Ï‚ Î¼Î­ÏƒÏ‰  Î¼Î¯Î±  Î¼ÎµÏƒÏ\n\nÎ½Î± Î½Î±Î¹ Î½Ï‰ÏÎ¯Ï‚\n\nÎ¾Î±Î½Î¬ Î¾Î±Ï†Î½Î¹ÎºÎ¬\n\nÎ¿ Î¿Î¹ ÏŒÎ»Î± ÏŒÎ»ÎµÏ‚ ÏŒÎ»Î· ÏŒÎ»Î·Î½ ÏŒÎ»Î·Ï‚ ÏŒÎ»Î¿ Î¿Î»ÏŒÎ³Ï…ÏÎ± ÏŒÎ»Î¿Î¹ ÏŒÎ»Î¿Î½ Î¿Î»Î¿Î½Î­Î½ ÏŒÎ»Î¿Ï‚ Î¿Î»ÏŒÏ„ÎµÎ»Î± ÏŒÎ»Î¿Ï… ÏŒÎ»Î¿Ï…Ï‚ ÏŒÎ»Ï‰Î½\nÏŒÎ»Ï‰Ï‚ Î¿Î»Ï‰ÏƒÎ´Î¹ÏŒÎ»Î¿Ï… ÏŒÎ¼Ï‰Ï‚ ÏŒÏ€Î¿Î¹Î± Î¿Ï€Î¿Î¹Î±Î´Î®Ï€Î¿Ï„Îµ Î¿Ï€Î¿Î¯Î±Î½ Î¿Ï€Î¿Î¹Î±Î½Î´Î®Ï€Î¿Ï„Îµ Î¿Ï€Î¿Î¯Î±Ï‚ Î¿Ï€Î¿Î¯Î¿Ï‚ Î¿Ï€Î¿Î¹Î±ÏƒÎ´Î®Ï€Î¿Ï„Îµ Î¿Ï€Î¿Î¹Î´Î®Ï€Î¿Ï„Îµ\nÏŒÏ€Î¿Î¹ÎµÏ‚ Î¿Ï€Î¿Î¹ÎµÏƒÎ´Î®Ï€Î¿Ï„Îµ ÏŒÏ€Î¿Î¹Î¿ Î¿Ï€Î¿Î¹Î¿Î´Î·Î®Ï€Î¿Ï„Îµ ÏŒÏ€Î¿Î¹Î¿Î¹ ÏŒÏ€Î¿Î¹Î¿Î½ Î¿Ï€Î¿Î¹Î¿Î½Î´Î®Ï€Î¿Ï„Îµ ÏŒÏ€Î¿Î¹Î¿Ï‚ Î¿Ï€Î¿Î¹Î¿ÏƒÎ´Î®Ï€Î¿Ï„Îµ\nÎ¿Ï€Î¿Î¯Î¿Ï… Î¿Ï€Î¿Î¹Î¿Ï…Î´Î®Ï€Î¿Ï„Îµ Î¿Ï€Î¿Î¯Î¿Ï…Ï‚ Î¿Ï€Î¿Î¹Î¿Ï…ÏƒÎ´Î®Ï€Î¿Ï„Îµ Î¿Ï€Î¿Î¯Ï‰Î½ Î¿Ï€Î¿Î¹Ï‰Î½Î´Î®Ï€Î¿Ï„Îµ ÏŒÏ€Î¿Ï„Îµ Î¿Ï€Î¿Ï„ÎµÎ´Î®Ï€Î¿Ï„Îµ ÏŒÏ€Î¿Ï…\nÎ¿Ï€Î¿Ï…Î´Î®Ï€Î¿Ï„Îµ ÏŒÏ€Ï‰Ï‚ Î¿ÏÎ¹ÏƒÎ¼Î­Î½Î± Î¿ÏÎ¹ÏƒÎ¼Î­Î½ÎµÏ‚ Î¿ÏÎ¹ÏƒÎ¼Î­Î½Ï‰Î½ Î¿ÏÎ¹ÏƒÎ¼Î­Î½Ï‰Ï‚ ÏŒÏƒÎ± Î¿ÏƒÎ±Î´Î®Ï€Î¿Ï„Îµ ÏŒÏƒÎµÏ‚ Î¿ÏƒÎµÏƒÎ´Î®Ï€Î¿Ï„Îµ\nÏŒÏƒÎ· Î¿ÏƒÎ·Î´Î®Ï€Î¿Ï„Îµ ÏŒÏƒÎ·Î½ Î¿ÏƒÎ·Î½Î´Î®Ï€Î¿Ï„Îµ ÏŒÏƒÎ·Ï‚ Î¿ÏƒÎ·ÏƒÎ´Î®Ï€Î¿Ï„Îµ ÏŒÏƒÎ¿ Î¿ÏƒÎ¿Î´Î®Ï€Î¿Ï„Îµ ÏŒÏƒÎ¿Î¹ Î¿ÏƒÎ¿Î¹Î´Î®Ï€Î¿Ï„Îµ ÏŒÏƒÎ¿Î½ Î¿ÏƒÎ¿Î½Î´Î®Ï€Î¿Ï„Îµ\nÏŒÏƒÎ¿Ï‚ Î¿ÏƒÎ¿ÏƒÎ´Î®Ï€Î¿Ï„Îµ ÏŒÏƒÎ¿Ï… Î¿ÏƒÎ¿Ï…Î´Î®Ï€Î¿Ï„Îµ ÏŒÏƒÎ¿Ï…Ï‚ Î¿ÏƒÎ¿Ï…ÏƒÎ´Î®Ï€Î¿Ï„Îµ ÏŒÏƒÏ‰Î½ Î¿ÏƒÏ‰Î½Î´Î®Ï€Î¿Ï„Îµ ÏŒÏ„Î±Î½ ÏŒÏ„Î¹ Î¿Ï„Î¹Î´Î®Ï€Î¿Ï„Îµ\nÏŒÏ„Î¿Ï… Î¿Ï… Î¿Ï…Î´Î­ Î¿ÏÏ„Îµ ÏŒÏ‡Î¹ Î¿Ï€Î¿Î¯Î±  Î¿Ï€Î¿Î¯ÎµÏ‚  Î¿Ï€Î¿Î¯Î¿  Î¿Ï€Î¿Î¯Î¿Î¹  Î¿Ï€ÏŒÏ„Îµ  Î¿Ï‚\n\nÏ€Î¬Î½Ï‰  Ï€Î±ÏÎ¬  Ï€ÎµÏÎ¯  Ï€Î¿Î»Î»Î¬  Ï€Î¿Î»Î»Î­Ï‚  Ï€Î¿Î»Î»Î¿Î¯  Ï€Î¿Î»Î»Î¿ÏÏ‚  Ï€Î¿Ï…  Ï€ÏÏÏ„Î±  Ï€ÏÏÏ„ÎµÏ‚  Ï€ÏÏÏ„Î·  Ï€ÏÏÏ„Î¿  Ï€ÏÏÏ„Î¿Ï‚  Ï€Ï‰Ï‚\nÏ€Î¬Î»Î¹ Ï€Î¬Î½Ï„Î± Ï€Î¬Î½Ï„Î¿Ï„Îµ Ï€Î±Î½Ï„Î¿Ï Ï€Î¬Î½Ï„Ï‰Ï‚ Ï€Î¬ÏÎ± Ï€Î­ÏÎ± Ï€Î­ÏÎ¹ Ï€ÎµÏÎ¯Ï€Î¿Ï… Ï€ÎµÏÎ¹ÏƒÏƒÏŒÏ„ÎµÏÎ¿ Ï€Î­ÏÏƒÎ¹ Ï€Î­ÏÏ…ÏƒÎ¹ Ï€Î¹Î± Ï€Î¹Î¸Î±Î½ÏŒÎ½\nÏ€Î¹Î¿ Ï€Î¯ÏƒÏ‰ Ï€Î»Î¬Î¹ Ï€Î»Î­Î¿Î½ Ï€Î»Î·Î½ Ï€Î¿Î¹Î¬ Ï€Î¿Î¹Î¬Î½ Ï€Î¿Î¹Î¬Ï‚ Ï€Î¿Î¹Î­Ï‚ Ï€Î¿Î¹ÏŒ Ï€Î¿Î¹Î¿Î¯ Ï€Î¿Î¹ÏŒÎ½ Ï€Î¿Î¹ÏŒÏ‚ Ï€Î¿Î¹Î¿Ï Ï€Î¿Î¹Î¿ÏÏ‚\nÏ€Î¿Î¹ÏÎ½ Ï€Î¿Î»Ï Ï€ÏŒÏƒÎµÏ‚ Ï€ÏŒÏƒÎ· Ï€ÏŒÏƒÎ·Î½ Ï€ÏŒÏƒÎ·Ï‚ Ï€ÏŒÏƒÎ¿Î¹ Ï€ÏŒÏƒÎ¿Ï‚ Ï€ÏŒÏƒÎ¿Ï…Ï‚ Ï€ÏŒÏ„Îµ Ï€Î¿Ï„Î­ Ï€Î¿Ï Ï€Î¿ÏÎ¸Îµ Ï€Î¿Ï…Î¸ÎµÎ½Î¬ Ï€ÏÎ­Ï€ÎµÎ¹\nÏ€ÏÎ¹Î½ Ï€ÏÎ¿ Ï€ÏÎ¿ÎºÎµÎ¹Î¼Î­Î½Î¿Ï… Ï€ÏÏŒÎºÎµÎ¹Ï„Î±Î¹ Ï€ÏÏŒÏ€ÎµÏÏƒÎ¹ Ï€ÏÎ¿Ï‚ Ï€ÏÎ¿Ï„Î¿Ï Ï€ÏÎ¿Ï‡Î¸Î­Ï‚ Ï€ÏÎ¿Ï‡Ï„Î­Ï‚ Ï€ÏÏ‰Ï„ÏÏ„ÎµÏÎ± Ï€ÏÏ‚\n\nÏƒÎ±Î½ ÏƒÎ±Ï‚ ÏƒÎµ ÏƒÎµÎ¹Ï‚ ÏƒÎ¿Ï… ÏƒÏ„Î± ÏƒÏ„Î· ÏƒÏ„Î·Î½ ÏƒÏ„Î·Ï‚ ÏƒÏ„Î¹Ï‚ ÏƒÏ„Î¿ ÏƒÏ„Î¿Î½ ÏƒÏ„Î¿Ï… ÏƒÏ„Î¿Ï…Ï‚ ÏƒÏ„Ï‰Î½ ÏƒÏ…Î³Ï‡ÏÏŒÎ½Ï‰Ï‚\nÏƒÏ…Î½ ÏƒÏ…Î½Î¬Î¼Î± ÏƒÏ…Î½ÎµÏ€ÏÏ‚ ÏƒÏ…Ï‡Î½Î¬Ï‚ ÏƒÏ…Ï‡Î½Î­Ï‚ ÏƒÏ…Ï‡Î½Î® ÏƒÏ…Ï‡Î½Î®Î½ ÏƒÏ…Ï‡Î½Î®Ï‚ ÏƒÏ…Ï‡Î½ÏŒ ÏƒÏ…Ï‡Î½Î¿Î¯ ÏƒÏ…Ï‡Î½ÏŒÎ½\nÏƒÏ…Ï‡Î½ÏŒÏ‚ ÏƒÏ…Ï‡Î½Î¿Ï ÏƒÏ…Ï‡Î½Î¿ÏÏ‚ ÏƒÏ…Ï‡Î½ÏÎ½ ÏƒÏ…Ï‡Î½ÏÏ‚ ÏƒÏ‡ÎµÎ´ÏŒÎ½\n\nÏ„Î± Ï„Î¬Î´Îµ Ï„Î±ÏÏ„Î± Ï„Î±ÏÏ„ÎµÏ‚ Ï„Î±ÏÏ„Î· Ï„Î±ÏÏ„Î·Î½ Ï„Î±ÏÏ„Î·Ï‚ Ï„Î±ÏÏ„Î¿Ï„Î±ÏÏ„Î¿Î½ Ï„Î±ÏÏ„Î¿Ï‚ Ï„Î±ÏÏ„Î¿Ï… Ï„Î±ÏÏ„Ï‰Î½ Ï„Î¬Ï‡Î± Ï„Î¬Ï‡Î±Ï„Îµ\nÏ„ÎµÎ»ÎµÏ…Ï„Î±Î¯Î±  Ï„ÎµÎ»ÎµÏ…Ï„Î±Î¯Î¿  Ï„ÎµÎ»ÎµÏ…Ï„Î±Î¯Î¿Ï‚  Ï„Î¿Ï  Ï„ÏÎ¯Î±  Ï„ÏÎ¯Ï„Î·  Ï„ÏÎµÎ¹Ï‚ Ï„ÎµÎ»Î¹ÎºÎ¬ Ï„ÎµÎ»Î¹ÎºÏÏ‚ Ï„ÎµÏ‚ Ï„Î­Ï„Î¿Î¹Î± Ï„Î­Ï„Î¿Î¹Î±Î½\nÏ„Î­Ï„Î¿Î¹Î±Ï‚ Ï„Î­Ï„Î¿Î¹ÎµÏ‚ Ï„Î­Ï„Î¿Î¹Î¿ Ï„Î­Ï„Î¿Î¹Î¿Î¹ Ï„Î­Ï„Î¿Î¹Î¿Î½ Ï„Î­Ï„Î¿Î¹Î¿Ï‚ Ï„Î­Ï„Î¿Î¹Î¿Ï…\nÏ„Î­Ï„Î¿Î¹Î¿Ï…Ï‚ Ï„Î­Ï„Î¿Î¹Ï‰Î½ Ï„Î· Ï„Î·Î½ Ï„Î·Ï‚ Ï„Î¹ Ï„Î¯Ï€Î¿Ï„Î± Ï„Î¯Ï€Î¿Ï„Îµ Ï„Î¹Ï‚ Ï„Î¿ Ï„Î¿Î¹ Ï„Î¿Î½ Ï„Î¿Ïƒ Ï„ÏŒÏƒÎ± Ï„ÏŒÏƒÎµÏ‚ Ï„ÏŒÏƒÎ· Ï„ÏŒÏƒÎ·Î½\nÏ„ÏŒÏƒÎ·Ï‚ Ï„ÏŒÏƒÎ¿ Ï„ÏŒÏƒÎ¿Î¹ Ï„ÏŒÏƒÎ¿Î½ Ï„ÏŒÏƒÎ¿Ï‚ Ï„ÏŒÏƒÎ¿Ï… Ï„ÏŒÏƒÎ¿Ï…Ï‚ Ï„ÏŒÏƒÏ‰Î½ Ï„ÏŒÏ„Îµ Ï„Î¿Ï… Ï„Î¿Ï…Î»Î¬Ï‡Î¹ÏƒÏ„Î¿ Ï„Î¿Ï…Î»Î¬Ï‡Î¹ÏƒÏ„Î¿Î½ Ï„Î¿Ï…Ï‚ Ï„Î¿ÏÏ‚ Ï„Î¿ÏÏ„Î±\nÏ„Î¿ÏÏ„ÎµÏ‚ Ï„Î¿ÏÏ„Î· Ï„Î¿ÏÏ„Î·Î½ Ï„Î¿ÏÏ„Î·Ï‚ Ï„Î¿ÏÏ„Î¿ Ï„Î¿ÏÏ„Î¿Î¹ Ï„Î¿ÏÏ„Î¿Î¹Ï‚ Ï„Î¿ÏÏ„Î¿Î½ Ï„Î¿ÏÏ„Î¿Ï‚ Ï„Î¿ÏÏ„Î¿Ï… Ï„Î¿ÏÏ„Î¿Ï…Ï‚ Ï„Î¿ÏÏ„Ï‰Î½ Ï„Ï…Ï‡ÏŒÎ½\nÏ„Ï‰Î½ Ï„ÏÏÎ±\n\nÏ…Ï€ Ï…Ï€Î­Ï Ï…Ï€ÏŒ Ï…Ï€ÏŒÏˆÎ· Ï…Ï€ÏŒÏˆÎ¹Î½ ÏÏƒÏ„ÎµÏÎ±\n\nÏ‡Ï‰ÏÎ¯Ï‚ Ï‡Ï‰ÏÎ¹ÏƒÏ„Î¬\n\nÏ‰ Ï‰Ï‚ Ï‰ÏƒÎ¬Î½ Ï‰ÏƒÏŒÏ„Î¿Ï… ÏÏƒÏ€Î¿Ï… ÏÏƒÏ„Îµ Ï‰ÏƒÏ„ÏŒÏƒÎ¿ Ï‰Ï‡\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/el/norm_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/el/tag_map.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/el/punctuation.py----------------------------------------
A:spacy.lang.el.punctuation.UNITS->merge_chars(_units)
spacy.lang.el.punctuation.merge_chars(char)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/mr/__init__.py----------------------------------------
A:spacy.lang.mr.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
spacy.lang.mr.__init__.Marathi(Language)
spacy.lang.mr.__init__.MarathiDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/mr/stop_words.py----------------------------------------
A:spacy.lang.mr.stop_words.STOP_WORDS->set('\nà¤¨\nà¤…à¤¤à¤°à¥€\nà¤¤à¥‹\nà¤¹à¥‡à¤‚\nà¤¤à¥‡à¤‚\nà¤•à¤¾à¤‚\nà¤†à¤£à¤¿\nà¤œà¥‡à¤‚\nà¤œà¥‡\nà¤®à¤—\nà¤¤à¥‡\nà¤®à¥€\nà¤œà¥‹\nà¤ªà¤°à¥€\nà¤—à¤¾\nà¤¹à¥‡\nà¤à¤¸à¥‡à¤‚\nà¤†à¤¤à¤¾à¤‚\nà¤¨à¤¾à¤¹à¥€à¤‚\nà¤¤à¥‡à¤¥\nà¤¹à¤¾\nà¤¤à¤¯à¤¾\nà¤…à¤¸à¥‡\nà¤®à¥à¤¹à¤£à¥‡\nà¤•à¤¾à¤¯\nà¤•à¥€à¤‚\nà¤œà¥ˆà¤¸à¥‡à¤‚\nà¤¤à¤‚à¤µ\nà¤¤à¥‚à¤‚\nà¤¹à¥‹à¤¯\nà¤œà¥ˆà¤¸à¤¾\nà¤†à¤¹à¥‡\nà¤ªà¥ˆà¤‚\nà¤¤à¥ˆà¤¸à¤¾\nà¤œà¤°à¥€\nà¤®à¥à¤¹à¤£à¥‹à¤¨à¤¿\nà¤à¤•\nà¤à¤¸à¤¾\nà¤œà¥€\nà¤¨à¤¾\nà¤®à¤œ\nà¤à¤¥\nà¤¯à¤¾\nà¤œà¥‡à¤¥\nà¤œà¤¯à¤¾\nà¤¤à¥à¤œ\nà¤¤à¥‡à¤£à¥‡à¤‚\nà¤¤à¥ˆà¤‚\nà¤ªà¤¾à¤‚\nà¤…à¤¸à¥‹\nà¤•à¤°à¥€\nà¤à¤¸à¥€\nà¤¯à¥‡à¤£à¥‡à¤‚\nà¤œà¤¾à¤¹à¤²à¤¾\nà¤¤à¥‡à¤‚à¤šà¤¿\nà¤†à¤˜à¤µà¥‡à¤‚\nà¤¹à¥‹à¤¤à¥€\nà¤•à¤¾à¤‚à¤¹à¥€à¤‚\nà¤¹à¥‹à¤Šà¤¨à¤¿\nà¤à¤•à¥‡à¤‚\nà¤®à¤¾à¤¤à¥‡à¤‚\nà¤ à¤¾à¤¯à¥€à¤‚\nà¤¯à¥‡\nà¤¸à¤•à¤³\nà¤•à¥‡à¤²à¥‡à¤‚\nà¤œà¥‡à¤£à¥‡à¤‚\nà¤œà¤¾à¤£\nà¤œà¥ˆà¤¸à¥€\nà¤¹à¥‹à¤¯à¥‡\nà¤œà¥‡à¤µà¥€à¤‚\nà¤à¤±à¥à¤¹à¤µà¥€à¤‚\nà¤®à¥€à¤šà¤¿\nà¤•à¤¿à¤°à¥€à¤Ÿà¥€\nà¤¦à¤¿à¤¸à¥‡\nà¤¦à¥‡à¤µà¤¾\nà¤¹à¥‹\nà¤¤à¤°à¤¿\nà¤•à¥€à¤œà¥‡\nà¤¤à¥ˆà¤¸à¥‡\nà¤†à¤ªà¤£\nà¤¤à¤¿à¤¯à¥‡\nà¤•à¤°à¥à¤®\nà¤¨à¥‹à¤¹à¥‡\nà¤‡à¤¯à¥‡\nà¤ªà¤¡à¥‡\nà¤®à¤¾à¤à¥‡à¤‚\nà¤¤à¥ˆà¤¸à¥€\nà¤²à¤¾à¤—à¥‡\nà¤¨à¤¾à¤¨à¤¾\nà¤œà¤‚à¤µ\nà¤•à¥€à¤°\nà¤…à¤§à¤¿à¤•\nà¤…à¤¨à¥‡à¤•\nà¤…à¤¶à¥€\nà¤…à¤¸à¤²à¤¯à¤¾à¤šà¥‡\nà¤…à¤¸à¤²à¥‡à¤²à¥à¤¯à¤¾\nà¤…à¤¸à¤¾\nà¤…à¤¸à¥‚à¤¨\nà¤…à¤¸à¥‡\nà¤†à¤œ\nà¤†à¤£à¤¿\nà¤†à¤¤à¤¾\nà¤†à¤ªà¤²à¥à¤¯à¤¾\nà¤†à¤²à¤¾\nà¤†à¤²à¥€\nà¤†à¤²à¥‡\nà¤†à¤¹à¥‡\nà¤†à¤¹à¥‡à¤¤\nà¤à¤•\nà¤à¤•à¤¾\nà¤•à¤®à¥€\nà¤•à¤°à¤£à¤¯à¤¾à¤¤\nà¤•à¤°à¥‚à¤¨\nà¤•à¤¾\nà¤•à¤¾à¤®\nà¤•à¤¾à¤¯\nà¤•à¤¾à¤¹à¥€\nà¤•à¤¿à¤µà¤¾\nà¤•à¥€\nà¤•à¥‡à¤²à¤¾\nà¤•à¥‡à¤²à¥€\nà¤•à¥‡à¤²à¥‡\nà¤•à¥‹à¤Ÿà¥€\nà¤—à¥‡à¤²à¥à¤¯à¤¾\nà¤˜à¥‡à¤Šà¤¨\nà¤œà¤¾à¤¤\nà¤à¤¾à¤²à¤¾\nà¤à¤¾à¤²à¥€\nà¤à¤¾à¤²à¥‡\nà¤à¤¾à¤²à¥‡à¤²à¥à¤¯à¤¾\nà¤Ÿà¤¾\nà¤¤à¤°\nà¤¤à¤°à¥€\nà¤¤à¤¸à¥‡à¤š\nà¤¤à¤¾\nà¤¤à¥€\nà¤¤à¥€à¤¨\nà¤¤à¥‡\nà¤¤à¥‹\nà¤¤à¥à¤¯à¤¾\nà¤¤à¥à¤¯à¤¾à¤šà¤¾\nà¤¤à¥à¤¯à¤¾à¤šà¥€\nà¤¤à¥à¤¯à¤¾à¤šà¥à¤¯à¤¾\nà¤¤à¥à¤¯à¤¾à¤¨à¤¾\nà¤¤à¥à¤¯à¤¾à¤¨à¥€\nà¤¤à¥à¤¯à¤¾à¤®à¥à¤³à¥‡\nà¤¤à¥à¤°à¥€\nà¤¦à¤¿à¤²à¥€\nà¤¦à¥‹à¤¨\nà¤¨\nà¤ªà¤£\nà¤ªà¤®\nà¤ªà¤°à¤¯à¤¤à¤¨\nà¤ªà¤¾à¤Ÿà¥€à¤²\nà¤®\nà¤®à¤¾à¤¤à¥à¤°\nà¤®à¤¾à¤¹à¤¿à¤¤à¥€\nà¤®à¥€\nà¤®à¥à¤¬à¥€\nà¤®à¥à¤¹à¤£à¤œà¥‡\nà¤®à¥à¤¹à¤£à¤¾à¤²à¥‡\nà¤®à¥à¤¹à¤£à¥‚à¤¨\nà¤¯à¤¾\nà¤¯à¤¾à¤šà¤¾\nà¤¯à¤¾à¤šà¥€\nà¤¯à¤¾à¤šà¥à¤¯à¤¾\nà¤¯à¤¾à¤¨à¤¾\nà¤¯à¤¾à¤¨à¥€\nà¤¯à¥‡à¤£à¤¾à¤°\nà¤¯à¥‡à¤¤\nà¤¯à¥‡à¤¥à¥€à¤²\nà¤¯à¥‡à¤¥à¥‡\nà¤²à¤¾à¤–\nà¤µ\nà¤µà¥à¤¯à¤•à¤¤\nà¤¸à¤°à¥à¤µ\nà¤¸à¤¾à¤—à¤¿à¤¤à¥à¤²à¥‡\nà¤¸à¥à¤°à¥‚\nà¤¹à¤œà¤¾à¤°\nà¤¹à¤¾\nà¤¹à¥€\nà¤¹à¥‡\nà¤¹à¥‹à¤£à¤¾à¤°\nà¤¹à¥‹à¤¤\nà¤¹à¥‹à¤¤à¤¾\nà¤¹à¥‹à¤¤à¥€\nà¤¹à¥‹à¤¤à¥‡\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/kn/__init__.py----------------------------------------
A:spacy.lang.kn.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
spacy.lang.kn.__init__.Kannada(Language)
spacy.lang.kn.__init__.KannadaDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/kn/stop_words.py----------------------------------------
A:spacy.lang.kn.stop_words.STOP_WORDS->set('\nà²¹à²²à²µà³\nà²®à³‚à²²à²•\nà²¹à²¾à²—à³‚\nà²…à²¦à³\nà²¨à³€à²¡à²¿à²¦à³à²¦à²¾à²°à³†\nà²¯à²¾à²µ\nà²à²‚à²¦à²°à³\nà²…à²µà²°à³\nà²ˆà²—\nà²à²‚à²¬\nà²¹à²¾à²—à²¾à²—à²¿\nà²…à²·à³à²Ÿà³‡\nà²¨à²¾à²µà³\nà²‡à²¦à³‡\nà²¹à³‡à²³à²¿\nà²¤à²®à³à²®\nà²¹à³€à²—à³†\nà²¨à²®à³à²®\nà²¬à³‡à²°à³†\nà²¨à³€à²¡à²¿à²¦à²°à³\nà²®à²¤à³à²¤à³†\nà²‡à²¦à³\nà²ˆ\nà²¨à³€à²µà³\nà²¨à²¾à²¨à³\nà²‡à²¤à³à²¤à³\nà²à²²à³à²²à²¾\nà²¯à²¾à²µà³à²¦à³‡\nà²¨à²¡à³†à²¦\nà²…à²¦à²¨à³à²¨à³\nà²à²‚à²¦à²°à³†\nà²¨à³€à²¡à²¿à²¦à³†\nà²¹à³€à²—à²¾à²—à²¿\nà²œà³†à³‚à²¤à³†à²—à³†\nà²‡à²¦à²°à²¿à²‚à²¦\nà²¨à²¨à²—à³†\nà²…à²²à³à²²à²¦à³†\nà²à²·à³à²Ÿà³\nà²‡à²¦à²°\nà²‡à²²à³à²²\nà²•à²³à³†à²¦\nà²¤à³à²‚à²¬à²¾\nà²ˆà²—à²¾à²—à²²à³‡\nà²®à²¾à²¡à²¿\nà²…à²¦à²•à³à²•à³†\nà²¬à²—à³à²—à³†\nà²…à²µà²°\nà²‡à²¦à²¨à³à²¨à³\nà²†\nà²‡à²¦à³†\nà²¹à³†à²šà³à²šà³\nà²‡à²¨à³à²¨à³\nà²à²²à³à²²\nà²‡à²°à³à²µ\nà²…à²µà²°à²¿à²—à³†\nà²¨à²¿à²®à³à²®\nà²à²¨à³\nà²•à³‚à²¡\nà²‡à²²à³à²²à²¿\nà²¨à²¨à³à²¨à²¨à³à²¨à³\nà²•à³†à²²à²µà³\nà²®à²¾à²¤à³à²°\nà²¬à²³à²¿à²•\nà²…à²‚à²¤\nà²¤à²¨à³à²¨\nà²†à²—\nà²…à²¥à²µà²¾\nà²…à²²à³à²²\nà²•à³‡à²µà²²\nà²†à²¦à²°à³†\nà²®à²¤à³à²¤à³\nà²‡à²¨à³à²¨à³‚\nà²…à²¦à³‡\nà²†à²—à²¿\nà²…à²µà²°à²¨à³à²¨à³\nà²¹à³‡à²³à²¿à²¦à³à²¦à²¾à²°à³†\nà²¨à²¡à³†à²¦à²¿à²¦à³†\nà²‡à²¦à²•à³à²•à³†\nà²à²‚à²¬à³à²¦à³\nà²à²‚à²¦à³\nà²¨à²¨à³à²¨\nà²®à³‡à²²à³†\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ur/__init__.py----------------------------------------
A:spacy.lang.ur.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
spacy.lang.ur.__init__.Urdu(Language)
spacy.lang.ur.__init__.UrduDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ur/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ur/lex_attrs.py----------------------------------------
A:spacy.lang.ur.lex_attrs._num_words->'Ø§ÛŒÚ© Ø¯Ùˆ ØªÛŒÙ† Ú†Ø§Ø± Ù¾Ø§Ù†Ú† Ú†Ú¾ Ø³Ø§Øª Ø¢Ù¹Ú¾ Ù†Ùˆ Ø¯Ø³ Ú¯ÛŒØ§Ø±Û Ø¨Ø§Ø±Û ØªÛŒØ±Û Ú†ÙˆØ¯Û Ù¾Ù†Ø¯Ø±Û Ø³ÙˆÙ„Û Ø³ØªØ±Û\n Ø§Ù¹Ù‡Ø§Ø±Ø§ Ø§Ù†ÛŒØ³ Ø¨ÛŒØ³ Ø§Ú©ÛŒØ³ Ø¨Ø§Ø¦ÛŒØ³ ØªØ¦ÛŒØ³ Ú†ÙˆØ¨ÛŒØ³ Ù¾Ú†ÛŒØ³ Ú†Ú¾Ø¨Ø¨ÛŒØ³\nØ³ØªØ§ÛŒØ³ Ø§Ù¹Ú¾Ø§Ø¦Ø³ Ø§Ù†ØªÙŠØ³ ØªÛŒØ³ Ø§Ú©ØªÛŒØ³ Ø¨ØªÛŒØ³ ØªÛŒÙ†ØªÛŒØ³ Ú†ÙˆÙ†ØªÛŒØ³ Ù¾ÛŒÙ†ØªÛŒØ³\n Ú†Ú¾ØªÛŒØ³ Ø³ÛŒÙ†ØªÛŒØ³ Ø§Ø±ØªÛŒØ³ Ø§Ù†ØªØ§Ù„ÛŒØ³ Ú†Ø§Ù„ÛŒØ³ Ø§Ú©ØªØ§Ù„ÛŒØ³ Ø¨ÛŒØ§Ù„ÛŒØ³ ØªÛŒØªØ§Ù„ÛŒØ³\nÚ†ÙˆØ§Ù„ÛŒØ³ Ù¾ÛŒØªØ§Ù„ÛŒØ³ Ú†Ú¾ÛŒØ§Ù„ÛŒØ³ Ø³ÛŒÙ†ØªØ§Ù„ÛŒØ³ Ø§Ú‘ØªØ§Ù„ÛŒØ³ Ø§Ù†Ú†Ø§Ù„ÛŒØ³ Ù¾Ú†Ø§Ø³ Ø§Ú©Ø§ÙˆÙ† Ø¨Ø§ÙˆÙ†\n ØªØ±ÛŒÙ¾Ù† Ú†ÙˆÙ† Ù¾Ú†Ù¾Ù† Ú†Ú¾Ù¾Ù† Ø³ØªØ§ÙˆÙ† Ø§Ù¹Ú¾Ø§ÙˆÙ† Ø§Ù†Ø³Ù¹Ú¾ Ø³Ø§Ø«Ú¾\nØ§Ú©Ø³Ù¹Ú¾ Ø¨Ø§Ø³Ù¹Ú¾ ØªØ±ÛŒØ³Ù¹Ú¾ Ú†ÙˆØ³Ù¹Ú¾ Ù¾ÛŒØ³Ù¹Ú¾ Ú†Ú¾ÛŒØ§Ø³Ù¹Ú¾ Ø³Ú‘Ø³Ù¹Ú¾ Ø§Ú‘Ø³Ù¹Ú¾\nØ§Ù†Ú¾ØªØ± Ø³ØªØ± Ø§Ú©Ú¾ØªØ± Ø¨Ú¾ØªØªØ± ØªÛŒÚ¾ØªØ± Ú†ÙˆÚ¾ØªØ± ØªÚ†Ú¾ØªØ± Ú†Ú¾ÛŒØªØ± Ø³ØªØªØ±\nØ§Ù¹Ú¾ØªØ± Ø§Ù†ÛŒØ§Ø³ÛŒ Ø§Ø³ÛŒ Ø§Ú©ÛŒØ§Ø³ÛŒ Ø¨ÛŒØ§Ø³ÛŒ ØªÛŒØ±Ø§Ø³ÛŒ Ú†ÙˆØ±Ø§Ø³ÛŒ Ù¾Ú†ÛŒØ§Ø³ÛŒ Ú†Ú¾ÛŒØ§Ø³ÛŒ\n Ø³Ù¹ÛŒØ§Ø³ÛŒ Ø§Ù¹Ú¾ÛŒØ§Ø³ÛŒ Ù†ÙˆØ§Ø³ÛŒ Ù†ÙˆÛ’ Ø§Ú©Ø§Ù†ÙˆÛ’ Ø¨Ø§Ù†ÙˆÛ’ ØªØ±Ø§Ù†ÙˆÛ’\nÚ†ÙˆØ±Ø§Ù†ÙˆÛ’ Ù¾Ú†Ø§Ù†ÙˆÛ’ Ú†Ú¾ÛŒØ§Ù†ÙˆÛ’ Ø³ØªØ§Ù†ÙˆÛ’ Ø§Ù¹Ú¾Ø§Ù†ÙˆÛ’ Ù†Ù†Ø§Ù†ÙˆÛ’ Ø³Ùˆ\n'.split()
A:spacy.lang.ur.lex_attrs._ordinal_words->'Ù¾ÛÙ„Ø§ Ø¯ÙˆØ³Ø±Ø§ ØªÛŒØ³Ø±Ø§ Ú†ÙˆØªÚ¾Ø§ Ù¾Ø§Ù†Ú†ÙˆØ§Úº Ú†Ú¾Ù¹Ø§ Ø³Ø§ØªÙˆØ§Úº Ø¢Ù¹Ú¾ÙˆØ§Úº Ù†ÙˆØ§Úº Ø¯Ø³ÙˆØ§Úº Ú¯ÛŒØ§Ø±ÛÙˆØ§Úº Ø¨Ø§Ø±ÛÙˆØ§Úº ØªÛŒØ±Ú¾ÙˆØ§Úº Ú†ÙˆØ¯Ú¾ÙˆØ§Úº\n Ù¾Ù†Ø¯Ø±Ú¾ÙˆØ§Úº Ø³ÙˆÙ„ÛÙˆØ§Úº Ø³ØªØ±Ú¾ÙˆØ§Úº Ø§Ù¹Ú¾Ø§Ø±ÙˆØ§Úº Ø§Ù†ÛŒØ³ÙˆØ§Úº Ø¨Ø³ÛŒÙˆØ§Úº\n'.split()
A:spacy.lang.ur.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.ur.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.ur.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ur/stop_words.py----------------------------------------
A:spacy.lang.ur.stop_words.STOP_WORDS->set('\nØ«Ú¾ÛŒ\nØ®Ùˆ\nÚ¯ÛŒ\nØ§Ù¾ÙŒÛ’\nÚ¯Ø¦Û’\nØ«ÛØª\nØ·Ø±Ù\nÛÙˆØ¨Ø±ÛŒ\nÙ¾Ø¨Ø¦Û’\nØ§Ù¾ÙŒØ¨\nØ¯ÙˆØ¶Ø±ÛŒ\nÚ¯ÛŒØ¨\nÚ©Øª\nÚ¯Ø¨\nØ«Ú¾ÛŒ\nØ¶Û’\nÛØ±\nÙ¾Ø±\nØ§Ø´\nØ¯ÛŒ\nÚ¯Û’\nÙ„Ú¯ÛŒÚº\nÛÛ’\nØ«Ø¹Ø°\nØ¶Ú©ØªÛ’\nØªÚ¾ÛŒ\nØ§Ù‰\nØ¯ÛŒØ¨\nÙ„Ø¦Û’\nÙˆØ§Ù„Û’\nÛŒÛ\nØ«Ø¯Ø¨Ø¦Û’\nØ¶Ú©ØªÛŒ\nØªÚ¾Ø¨\nØ§Ù‹Ø°Ø±\nØ±Ø±ÛŒØ¹Û’\nÙ„Ú¯ÛŒ\nÛÙˆØ¨Ø±Ø§\nÛÙˆÙ‹Û’\nØ«Ø¨ÛØ±\nØ¶Ú©ØªØ¨\nÙ‹ÛÛŒÚº\nØªÙˆ\nØ§ÙˆØ±\nØ±ÛØ¨\nÙ„Ú¯Û’\nÛÙˆØ¶Ú©ØªØ¨\nÛÙˆÚº\nÚ©Ø¨\nÛÙˆØ¨Ø±Û’\nØªÙˆØ¨Ù…\nÚ©ÛŒØ¨\nØ§ÛŒØ·Û’\nØ±ÛÛŒ\nÙ‡Ú¯Ø±\nÛÙˆØ¶Ú©ØªÛŒ\nÛÛŒÚº\nÚ©Ø±ÛŒÚº\nÛÙˆ\nØªÚ©\nÚ©ÛŒ\nØ§ÛŒÚ©\nØ±ÛÛ’\nÙ‡ÛŒÚº\nÛÙˆØ¶Ú©ØªÛ’\nÚ©ÛŒØ·Û’\nÛÙˆÙ‹Ø¨\nØªØª\nÚ©Û\nÛÙˆØ§\nØ¢Ø¦Û’\nØ¶Ø¨Øª\nØªÚ¾Û’\nÚ©ÛŒÙˆÚº\nÛÙˆ\nØªØ¨\nÚ©Û’\nÙ¾Ú¾Ø±\nØ«ØºÛŒØ±\nØ®Ø¨Ø±\nÛÛ’\nØ±Ú©Ú¾\nÚ©ÛŒ\nØ·Ø¨\nÚ©ÙˆØ¦ÛŒ\n  Ø±Ø±ÛŒØ¹Û’\nØ«Ø¨Ø±Û’\nØ®Ø¨\nØ§Ø¶Ø·Ø±Ø°\nØ«Ù„Ú©Û\nØ®Ø¬Ú©Û\nØ±Ú©Ú¾\nØªØ¨\nÚ©ÛŒ\nØ·Ø±Ù\nØ«Ø±Ø§Úº\nØ®Ø¨Ø±\nØ±Ø±ÛŒØ¹Û\nØ§Ø¶Ú©Ø¨\nØ«ÙŒØ°\nØ®Øµ\nÚ©ÛŒ\nÙ„Ø¦Û’\nØªÙˆÛÛŒÚº\nØ¯ÙˆØ¶Ø±Û’\nÚ©Ø±Ø±ÛÛŒ\nØ§Ø¶Ú©ÛŒ\nØ«ÛŒÚ†\nØ®ÙˆÚ©Û\nØ±Ú©Ú¾ØªÛŒ\nÚ©ÛŒÙˆÙ‹Ú©Û\nØ¯ÙˆÙ‹ÙˆÚº\nÚ©Ø±\nØ±ÛÛ’\nØ®Ø¨Ø±\nÛÛŒ\nØ«Ø±Ø¢Úº\nØ§Ø¶Ú©Û’\nÙ¾Ú†Ú¾Ù„Ø§\nØ®ÛŒØ·Ø¨\nØ±Ú©Ú¾ØªÛ’\nÚ©Û’\nØ«Ø¹Ø°\nØªÙˆ\nÛÛŒ\n  Ø¯ÙˆØ±Ù‰\nÚ©Ø±\nÛŒÛØ¨Úº\nØ¢Ø´\nØªÚ¾ÙˆÚ‘Ø§\nÚ†Ú©Û’\nØ²Ú©ÙˆÛŒÛ\nØ¯ÙˆØ¶Ø±ÙˆÚº\nØ¶Ú©Ø¨\nØ§ÙˆÙ‹Ú†Ø¨\nØ«ÙŒØ¨\nÙ¾Ù„\nØªÚ¾ÙˆÚ‘ÛŒ\nÚ†Ù„Ø§\nØ®Ø¨Ù‡ÙˆØ¸\nØ¯ÛŒØªØ¨\nØ¶Ú©ÙŒØ¨\nØ§Ø®Ø¨Ø²Øª\nØ§ÙˆÙ‹Ú†Ø¨Ø¦ÛŒ\nØ«ÙŒØ¨Ø±ÛØ¨\nÙ¾ÙˆÚ†Ú¾Ø¨\nØªÚ¾ÙˆÚ‘Û’\nÚ†Ù„Ùˆ\nØ®ØªÙ†\nØ¯ÛŒØªÛŒ\nØ¶Ú©ÛŒ\nØ§Ú†Ú¾Ø¨\nØ§ÙˆÙ‹Ú†ÛŒ\nØ«ÙŒØ¨Ø±ÛÛŒ\nÙ¾ÙˆÚ†Ú¾ØªØ¨\nØªÛŒÙŠ\nÚ†Ù„ÛŒÚº\nØ¯Ø±\nØ¯ÛŒØªÛ’\nØ¶Ú©Û’\nØ§Ú†Ú¾ÛŒ\nØ§ÙˆÙ‹Ú†Û’\nØ«ÙŒØ¨Ø±ÛÛ’\nÙ¾ÙˆÚ†Ú¾ØªÛŒ\nØ®Ø¨Ù‹Ø¨\nÚ†Ù„Û’\nØ¯Ø±Ø®Ø¨Øª\nØ¯ÛŒØ±\nØ¶Ù„Ø·Ù„Û\nØ§Ú†Ú¾Û’\nØ§Ù¹Ú¾Ø¨Ù‹Ø¨\nØ«ÙŒØ¨Ù‹Ø¨\nÙ¾ÙˆÚ†Ú¾ØªÛ’\nØ®Ø¨Ù‹ØªØ¨\nÚ†Ú¾ÙˆÙ¹Ø¨\nØ¯Ø±Ø®Û\nØ¯ÛŒÚ©Ú¾ÙŒØ¨\nØ¶ÙˆÚ†\nØ§Ø®ØªØªØ¨Ù…\nØ§ÛÙ†\nØ«ÙŒØ°\nÙ¾ÙˆÚ†Ú¾ÙŒØ¨\nØ®Ø¨Ù‹ØªÛŒ\nÚ†Ú¾ÙˆÙ¹ÙˆÚº\nØ¯Ø±Ø®Û’\nØ¯ÛŒÚ©Ú¾Ùˆ\nØ¶ÙˆÚ†Ø¨\nØ§Ø¯Ú¾Ø±\nØ¢Ø¦ÛŒ\nØ«ÙŒØ°Ú©Ø±Ù‹Ø¨\nÙ¾ÙˆÚ†Ú¾Ùˆ\nØ®Ø¨Ù‹ØªÛ’\nÚ†Ú¾ÙˆÙ¹ÛŒ\nØ¯Ø±Ø²Ù‚ÛŒÙ‚Øª\nØ¯ÛŒÚ©Ú¾ÛŒ\nØ¶ÙˆÚ†ØªØ¨\nØ§Ø±Ø¯\nØ¢Ø¦Û’\nØ«ÙŒØ°Ú©Ø±Ùˆ\nÙ¾ÙˆÚ†Ú¾ÙˆÚº\nØ®Ø¨Ù‹ÙŒØ¨\nÚ†Ú¾ÙˆÙ¹Û’\nØ¯Ø±Ø¶Øª\nØ¯ÛŒÚ©Ú¾ÛŒÚº\nØ¶ÙˆÚ†ØªÛŒ\nØ§Ø±Ø¯Ú¯Ø±Ø¯\nØ¢Ø¬\nØ«ÙŒØ°ÛŒ\nÙ¾ÙˆÚ†Ú¾ÛŒÚº\nØ®Ø·Ø·Ø±Ø°\nÚ†Ú¾Û\nØ¯Ø´\nØ¯ÛŒÙŒØ¨\nØ¶ÙˆÚ†ØªÛ’\nØ§Ø±Ú©Ø¨Ù‰\nØ¢Ø®Ø±\nØ«Ú‘Ø§\nÙ¾ÙˆØ±Ø§\nØ®Ú¯Û\nÚ†ÛŒØ³ÛŒÚº\nØ¯ÙØ¹Û\nØ¯Û’\nØ¶ÙˆÚ†ÙŒØ¨\nØ§Ø¶ØªØ¹ÙˆØ¨Ù„\nØ¢Ø®Ø±\nÙ¾ÛÙ„Ø§\nØ®Ú¯ÛÙˆÚº\nØ²Ø¨ØµÙ„\nØ¯Ú©Ú¾Ø¨Ø¦ÛŒÚº\nØ±Ø§Ø¶ØªÙˆÚº\nØ¶ÙˆÚ†Ùˆ\nØ§Ø¶ØªØ¹ÙˆØ¨Ù„Ø§Øª\nØ¢Ø¯Ù‡ÛŒ\nØ«Ú‘ÛŒ\nÙ¾ÛÙ„ÛŒ\nØ®Ú¯ÛÛŒÚº\nØ²Ø¨Ø¶Ø±\nØ¯Ú©Ú¾Ø¨ØªØ¨\nØ±Ø§Ø¶ØªÛ\nØ¶ÙˆÚ†ÛŒ\nØ§ØºÛŒØ¨\nØ¢Ù‹Ø¨\nØ«Ú‘Û’\nÙ¾ÛÙ„Û’Ø¶ÛŒ\nØ®Ù„Ø°ÛŒ\nØ²Ø¨Ù„\nØ¯Ú©Ú¾Ø¨ØªÛŒ\nØ±Ø§Ø¶ØªÛ’\nØ¶ÙˆÚ†ÛŒÚº\nØ§Ø·Ø±Ø§Ù\nØ¢Ù¹Ú¾\nØ«Ú¾Ø±\nØ®ÙŒØ¨Ø©\nØ²Ø¨Ù„\nØ¯Ú©Ú¾Ø¨ØªÛ’\nØ±Ú©ÙŠ\nØ¶ÛŒØ°Ú¾Ø¨\nØ§ÙØ±Ø§Ø¯\nØ¢ÛŒØ¨\nØ«Ú¾Ø±Ø§\nÙ¾ÛÙ„Û’\nØ®ÙˆØ§Ù‰\nØ²Ø¨Ù„Ø§Øª\nØ¯Ú©Ú¾Ø¨Ù‹Ø¨\nØ±Ú©Ú¾Ø¨\nØ¶ÛŒØ°Ú¾ÛŒ\nØ§Ú©Ø«Ø±\nØ«Ø¨\nÛÙˆØ§\nÙ¾ÛŒØ¹\nØ®ÙˆÙ‹ÛÛŒ\nØ²Ø¨Ù„ÛŒÛ\nØ¯Ú©Ú¾Ø¨Ùˆ\nØ±Ú©Ú¾ÛŒ\nØ¶ÛŒØ°Ú¾Û’\nØ§Ú©Ù¹Ú¾Ø¨\nØ«Ú¾Ø±Ù¾ÙˆØ±\nØªØ¨Ø²Ù\nØ®ÛŒØ·Ø¨Ú©Û\nØ²ØµÙˆÚº\nØ±Ú©Ú¾Û’\nØ¶ÛŒÚ©ÙŒÚˆ\nØ§Ú©Ù¹Ú¾ÛŒ\nØ«Ø¨Ø±ÛŒ\nØ«ÛØªØ±\nØªØ±\nÚ†Ø¨Ø±\nØ²ØµÛ\nØ¯Ù„Ú†Ø·Ù¾\nØ²ÛŒØ¨Ø¯Ù\nØºØ¨ÛŒØ°\nØ§Ú©Ù¹Ú¾Û’\nØ«Ø¨Ù„Ø§\nØ«ÛØªØ±ÛŒ\nØªØ±ØªÛŒØª\nÚ†Ø¨ÛØ¨\nØ²ØµÛ’\nØ¯Ù„Ú†Ø·Ù¾ÛŒ\nØ¶Ø¨Øª\nØºØ®Øµ\nØ§Ú©ÛŒÙ„Ø§\nØ«Ø¨Ù„ØªØ±ØªÛŒØª\nØ«ÛØªØ±ÛŒÙŠ\nØªØ±ÛŒÙŠ\nÚ†Ø¨ÛÙŒØ¨\nØ²Ù‚Ø¨Ø¦Ù‚\nØ¯Ù„Ú†Ø·Ù¾ÛŒØ¨Úº\nØ¶Ø¨Ø¯Ù\nØºØ°\nØ§Ú©ÛŒÙ„ÛŒ\nØ«Ø±Ø´\nÙ¾Ø¨Ø´\nØªØ¹Ø°Ø§Ø¯\nÚ†Ø¨ÛÛ’\nØ²Ù‚ÛŒØªÛŒÚº\nÙ‡ÙŒØ¨Ø¶Øª\nØ¶Ø¨Ø±Ø§\nØºØ±ÙˆØ¹\nØ§Ú©ÛŒÙ„Û’\nØ«ØºÛŒØ±\nÙ¾Ø¨Ù‹Ø¨\nÚ†Ú©Ø¨\nØ²Ù‚ÛŒÙ‚Øª\nØ¯Ùˆ\nØ¶Ø¨Ø±Û’\nØºØ±ÙˆØ¹Ø¨Øª\nØ§Ú¯Ø±Ú†Û\nØ«Ù„ÙŒØ°\nÙ¾Ø¨Ù‹Ú†\nØªÙ†\nÚ†Ú©ÛŒ\nØ²Ú©Ù†\nØ¯ÙˆØ±\nØ¶Ø¨Ù„\nØºÛ’\nØ§Ù„Ú¯\nÙ¾Ø±Ø§Ù‹Ø¨\nØªÙŒÛØ¨\nÚ†Ú©ÛŒÚº\nØ¯ÙˆØ¶Ø±Ø§\nØ¶Ø¨Ù„ÙˆÚº\nØµØ¨Ù\nØµØ³ÛŒØ±\nÙ‚Ø¬ÛŒÙ„Û\nÚ©ÙˆÙ‹Ø·Û’\nÙ„Ø§Ø²Ù‡ÛŒ\nÙ‡Ø·Ø¦Ù„Û’\nÙ‹ÛŒØ¨\nØ·Ø±ÛŒÙ‚\nÚ©Ø±ØªÛŒ\nÚ©ÛØªÛ’\nØµÙØ±\nÙ‚Ø·Ù†\nÚ©Ú¾ÙˆÙ„Ø§\nÙ„Ú¯ØªØ¨\nÙ‡Ø·Ø¨Ø¦Ù„\nÙˆØ§Ø±\nØ·Ø±ÛŒÙ‚ÙˆÚº\nÚ©Ø±ØªÛ’\nÚ©ÛÙŒØ¨\nØµÙˆØ±Øª\nÚ©Ø¦ÛŒ\nÚ©Ú¾ÙˆÙ„ÙŒØ¨\nÙ„Ú¯ØªÛŒ\nÙ‡Ø·ØªØ¹ÙˆÙ„\nÙˆØ§Ø±\nØ·Ø±ÛŒÙ‚Û\nÚ©Ø±ØªÛ’\nÛÙˆ\nÚ©ÛÙŒØ¨\nØµÙˆØ±ØªØ³Ø¨Ù„\nÚ©Ø¦Û’\nÚ©Ú¾ÙˆÙ„Ùˆ\nÙ„Ú¯ØªÛ’\nÙ‡Ø»ØªÙˆÙ„\nÙ¹Ú¾ÛŒÚ©\nØ·Ø±ÛŒÙ‚Û’\nÚ©Ø±Ù‹Ø¨\nÚ©ÛÙˆ\nØµÙˆØ±ØªÙˆÚº\nÚ©Ø¨ÙÛŒ\nÙ‡Ø·Ù„Ù‚\nÚˆÚ¾ÙˆÙ‹ÚˆØ§\nØ·ÙˆØ±\nÚ©Ø±Ùˆ\nÚ©ÛÙˆÚº\nØµÙˆØ±ØªÛŒÚº\nÚ©Ø¨Ù…\nÚ©Ú¾ÙˆÙ„ÛŒÚº\nÙ„Ú¯ÛŒ\nÙ‡Ø¹Ù„ÙˆÙ…\nÚˆÚ¾ÙˆÙ‹ÚˆÙ„ÛŒØ¨\nØ·ÙˆØ±Ù¾Ø±\nÚ©Ø±ÛŒÚº\nÚ©ÛÛŒ\nØ¶Ø±ÙˆØ±\nÚ©Ø¬Ú¾ÛŒ\nÚ©Ú¾ÙˆÙ„Û’\nÙ„Ú¯Û’\nÙ‡Ú©ÙˆÙ„\nÚˆÚ¾ÙˆÙ‹ÚˆÙ‹Ø¨\nØ¸Ø¨ÛØ±\nÚ©Ø±Û’\nÚ©ÛÛŒÚº\nØ¶Ø±ÙˆØ±Øª\nÚ©Ø±Ø§\nÚ©ÛØ¨\nÙ„ÙˆØ¬Ø¨\nÙ‡Ù„Ø§\nÚˆÚ¾ÙˆÙ‹ÚˆÙˆ\nØ¹Ø°Ø¯\nÚ©Ù„\nÚ©ÛÛŒÚº\nÚ©Ø±ØªØ¨\nÚ©ÛØªØ¨\nÙ„ÙˆØ¬ÛŒ\nÙ‡ÙˆÚ©ÙŠ\nÚˆÚ¾ÙˆÙ‹ÚˆÛŒ\nØ¹Ø¸ÛŒÙ†\nÚ©Ù†\nÚ©ÛÛ’\nØ¶Ø±ÙˆØ±ÛŒ\nÚ©Ø±ØªØ¨ÛÙˆÚº\nÚ©ÛØªÛŒ\nÙ„ÙˆØ¬Û’\nÙ‡ÙˆÚ©ÙŒØ¨Øª\nÚˆÚ¾ÙˆÙ‹ÚˆÛŒÚº\nØ¹Ù„Ø§Ù‚ÙˆÚº\nÚ©ÙˆØªØ±\nÚ©ÛŒÛ’\nÙ„ÙˆØ³Ø¨Øª\nÙ‡ÙˆÚ©ÙŒÛ\nÛÙ†\nÙ„Û’\nÙ‹Ø¨Ù¾Ø·ÙŒØ°\nÛÙˆØ±ÛÛ’\nØ¹Ù„Ø§Ù‚Û\nÚ©ÙˆØ±Ø§\nÚ©Û’\nØ±Ø±ÛŒØ¹Û’\nÙ„ÙˆØ³Û\nÙ‡Ú‘Ø§\nÛÙˆØ¦ÛŒ\nÙ‡ØªØ¹Ù„Ù‚\nÙ‹Ø¨Ú¯Ø³ÛŒØ±\nÛÙˆÚ¯Ø¦ÛŒ\nØ¹Ù„Ø§Ù‚Û’\nÚ©ÙˆØ±ÙˆÚº\nÚ¯Ø¦ÛŒ\nÙ„Ùˆ\nÙ‡Ú‘Ù‹Ø¨\nÛÙˆØ¦Û’\nÙ‡Ø³ØªØ±Ù…\nÙ‹Ø·Ø¬Øª\nÛÙˆ\nÚ¯Ø¦Û’\nØ¹Ù„Ø§ÙˆÙ\nÚ©ÙˆØ±Ù\nÚ¯Ø±Ø¯\nÙ„ÙˆÚ¯\nÙ‡Ú‘Û’\nÛÙˆØªÛŒ\nÙ‡Ø³ØªØ±Ù‡Û\nÙ‹Ù‚Ø·Û\nÛÙˆÚ¯ÛŒØ¨\nÚ©ÙˆØ±Û’\nÚ¯Ø±ÙˆÙ¾\nÙ„ÙˆÚ¯ÙˆÚº\nÙ‡ÛØ±Ø«Ø¨Ù‰\nÛÙˆØªÛ’\nÙ‡Ø³Ø·ÙˆØ´\nÙ‹Ú©Ø¨Ù„ÙŒØ¨\nÛÙˆÙ‹ÛŒ\nØ¹ÙˆÙˆÙ‡ÛŒ\nÚ©ÙˆØ·ÙŠ\nÚ¯Ø±ÙˆÙ\nÙ„Ú‘Ú©Ù¾ÙŠ\nÙ‡ÛŒØ±Ø§\nÛÙˆÚ†Ú©Ø¨\nÙ‡Ø®ØªÙ„Ù\nÙ‹Ú©ØªÛ\nÛÛŒ\nÙØ±Ø¯\nÚ©ÙˆÙ‰\nÚ¯Ø±ÙˆÛÙˆÚº\nÙ„ÛŒ\nÙ‡ÛŒØ±ÛŒ\nÛÙˆÚ†Ú©ÛŒ\nÙ‡Ø³ÛŒØ°\nÙÛŒ\nÚ©ÙˆÙ‹Ø·Ø¨\nÚ¯ÙŒØªÛŒ\nÙ„ÛŒØ¨\nÙ‡ÛŒØ±Û’\nÛÙˆÚ†Ú©Û’\nÙ‡Ø·Ø¦Ù„Û\nÙ‹ÙˆØ®ÙˆØ§Ù‰\nÛŒÙ‚ÛŒÙŒÛŒ\nÙ‚Ø¬Ù„\nÚ©ÙˆÙ‹Ø·ÛŒ\nÙ„ÛŒÙŒØ¨\nÙ‹Ø¦ÛŒ\nÛÙˆØ±ÛØ¨\nÙ„ÛŒÚº\nÙ‹Ø¦Û’\nÛÙˆØ±ÛÛŒ\nØ«Ø¨Ø¹Ø«\nØ¶Øª\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ur/tag_map.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ur/punctuation.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/hr/__init__.py----------------------------------------
A:spacy.lang.hr.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.hr.__init__.lex_attr_getters[NORM]->add_lookups(Language.Defaults.lex_attr_getters[NORM], BASE_NORMS)
A:spacy.lang.hr.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS)
spacy.lang.hr.__init__.Croatian(Language)
spacy.lang.hr.__init__.CroatianDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/hr/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/hr/stop_words.py----------------------------------------
A:spacy.lang.hr.stop_words.STOP_WORDS->set('\na\nah\naha\naj\nako\nal\nali\narh\nau\navaj\nbar\nbaÅ¡\nbez\nbi\nbih\nbijah\nbijahu\nbijaÅ¡e\nbijasmo\nbijaste\nbila\nbili\nbilo\nbio\nbismo\nbiste\nbiti\nbrr\nbuÄ‡\nbudavÅ¡i\nbude\nbudimo\nbudite\nbudu\nbuduÄ‡i\nbum\nbumo\nÄ‡e\nÄ‡emo\nÄ‡eÅ¡\nÄ‡ete\nÄijem\nÄijim\nÄijima\nÄ‡u\nda\ndaj\ndakle\nde\ndeder\ndem\ndjelomice\ndjelomiÄno\ndo\ndoista\ndok\ndokle\ndonekle\ndosad\ndoskoro\ndotad\ndotle\ndoveÄer\ndrugamo\ndrugdje\nduÅ¾\ne\neh\nehe\nej\neno\neto\nevo\nga\ngdjekakav\ngdjekoje\ngic\ngod\nhalo\nhej\nhm\nhoÄ‡e\nhoÄ‡emo\nhoÄ‡eÅ¡\nhoÄ‡ete\nhoÄ‡u\nhop\nhtijahu\nhtijasmo\nhtijaste\nhtio\nhtjedoh\nhtjedoÅ¡e\nhtjedoste\nhtjela\nhtjele\nhtjeli\nhura\ni\niako\nih\niju\nijuju\nikada\nikakav\nikakva\nikakve\nikakvi\nikakvih\nikakvim\nikakvima\nikakvo\nikakvog\nikakvoga\nikakvoj\nikakvom\nikakvome\nili\nim\niz\nja\nje\njedna\njedne\njedni\njedno\njer\njesam\njesi\njesmo\njest\njeste\njesu\njim\njoj\njoÅ¡\nju\nkada\nkako\nkao\nkoja\nkoje\nkoji\nkojima\nkoju\nkroz\nlani\nli\nme\nmene\nmeni\nmi\nmimo\nmoj\nmoja\nmoje\nmoji\nmoju\nmu\nna\nnad\nnakon\nnam\nnama\nnas\nnaÅ¡\nnaÅ¡a\nnaÅ¡e\nnaÅ¡eg\nnaÅ¡i\nne\nneÄ‡e\nneÄ‡emo\nneÄ‡eÅ¡\nneÄ‡ete\nneÄ‡u\nnego\nneka\nneke\nneki\nnekog\nneku\nnema\nneÅ¡to\nnetko\nni\nnije\nnikoga\nnikoje\nnikoji\nnikoju\nnisam\nnisi\nnismo\nniste\nnisu\nnjega\nnjegov\nnjegova\nnjegovo\nnjemu\nnjezin\nnjezina\nnjezino\nnjih\nnjihov\nnjihova\nnjihovo\nnjim\nnjima\nnjoj\nnju\nno\no\nod\nodmah\non\nona\none\noni\nono\nonu\nonoj\nonom\nonim\nonima\nova\novaj\novim\novima\novoj\npa\npak\npljus\npo\npod\npodalje\npoimence\npoizdalje\nponekad\npored\npostrance\npotajice\npotrbuÅ¡ke\npouzdano\nprije\ns\nsa\nsam\nsamo\nsasvim\nsav\nse\nsebe\nsebi\nsi\nÅ¡ic\nsmo\nste\nÅ¡to\nÅ¡ta\nÅ¡togod\nÅ¡tagod\nsu\nsva\nsve\nsvi\nsvi\nsvog\nsvoj\nsvoja\nsvoje\nsvoju\nsvom\nsvu\nta\ntada\ntaj\ntako\nte\ntebe\ntebi\nti\ntim\ntima\nto\ntoj\ntome\ntu\ntvoj\ntvoja\ntvoje\ntvoji\ntvoju\nu\nusprkos\nutaman\nuvijek\nuz\nuza\nuzagrapce\nuzalud\nuzduÅ¾\nvaljda\nvam\nvama\nvas\nvaÅ¡\nvaÅ¡a\nvaÅ¡e\nvaÅ¡im\nvaÅ¡ima\nveÄ‡\nvi\nvjerojatno\nvjerovatno\nvrh\nvrlo\nza\nzaista\nzar\nzatim\nzato\nzbija\nzbog\nÅ¾eleÄ‡i\nÅ¾eljah\nÅ¾eljela\nÅ¾eljele\nÅ¾eljeli\nÅ¾eljelo\nÅ¾eljen\nÅ¾eljena\nÅ¾eljene\nÅ¾eljeni\nÅ¾eljenu\nÅ¾eljeo\nzimus\nzum\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/es/__init__.py----------------------------------------
A:spacy.lang.es.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.es.__init__.lex_attr_getters[NORM]->add_lookups(Language.Defaults.lex_attr_getters[NORM], BASE_NORMS)
A:spacy.lang.es.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS, TOKENIZER_EXCEPTIONS)
spacy.lang.es.__init__.Spanish(Language)
spacy.lang.es.__init__.SpanishDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/es/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/es/lex_attrs.py----------------------------------------
A:spacy.lang.es.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.es.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.es.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/es/tokenizer_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/es/syntax_iterators.py----------------------------------------
A:spacy.lang.es.syntax_iterators.np_label->doc.vocab.strings.add('NP')
A:spacy.lang.es.syntax_iterators.(left, right)->noun_bounds(doc, token, np_left_deps, np_right_deps, stop_deps)
A:spacy.lang.es.syntax_iterators.token->next_token(token)
spacy.lang.es.syntax_iterators.is_verb_token(token)
spacy.lang.es.syntax_iterators.next_token(token)
spacy.lang.es.syntax_iterators.noun_bounds(doc,root,np_left_deps,np_right_deps,stop_deps)
spacy.lang.es.syntax_iterators.noun_chunks(obj)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/es/stop_words.py----------------------------------------
A:spacy.lang.es.stop_words.STOP_WORDS->set('\nactualmente acuerdo adelante ademas ademÃ¡s adrede afirmÃ³ agregÃ³ ahi ahora ahÃ­\nal algo alguna algunas alguno algunos algÃºn alli allÃ­ alrededor ambos ampleamos\nantano antaÃ±o ante anterior antes apenas aproximadamente aquel aquella aquellas\naquello aquellos aqui aquÃ©l aquÃ©lla aquÃ©llas aquÃ©llos aquÃ­ arriba arribaabajo\nasegurÃ³ asi asÃ­ atras aun aunque ayer aÃ±adiÃ³ aÃºn\n\nbajo bastante bien breve buen buena buenas bueno buenos\n\ncada casi cerca cierta ciertas cierto ciertos cinco claro comentÃ³ como con\nconmigo conocer conseguimos conseguir considera considerÃ³ consigo consigue\nconsiguen consigues contigo contra cosas creo cual cuales cualquier cuando\ncuanta cuantas cuanto cuantos cuatro cuenta cuÃ¡l cuÃ¡les cuÃ¡ndo cuÃ¡nta cuÃ¡ntas\ncuÃ¡nto cuÃ¡ntos cÃ³mo\n\nda dado dan dar de debajo debe deben debido decir dejÃ³ del delante demasiado\ndemÃ¡s dentro deprisa desde despacio despues despuÃ©s detras detrÃ¡s dia dias dice\ndicen dicho dieron diferente diferentes dijeron dijo dio donde dos durante dÃ­a\ndÃ­as dÃ³nde\n\nejemplo el ella ellas ello ellos embargo empleais emplean emplear empleas\nempleo en encima encuentra enfrente enseguida entonces entre era eramos eran\neras eres es esa esas ese eso esos esta estaba estaban estado estados estais\nestamos estan estar estarÃ¡ estas este esto estos estoy estuvo estÃ¡ estÃ¡n ex\nexcepto existe existen explicÃ³ expresÃ³ Ã©l Ã©sa Ã©sas Ã©se Ã©sos Ã©sta Ã©stas Ã©ste\nÃ©stos\n\nfin final fue fuera fueron fui fuimos\n\ngeneral gran grandes gueno\n\nha haber habia habla hablan habrÃ¡ habÃ­a habÃ­an hace haceis hacemos hacen hacer\nhacerlo haces hacia haciendo hago han hasta hay haya he hecho hemos hicieron\nhizo horas hoy hubo\n\nigual incluso indicÃ³ informo informÃ³ intenta intentais intentamos intentan\nintentar intentas intento ir\n\njunto\n\nla lado largo las le lejos les llegÃ³ lleva llevar lo los luego lugar\n\nmal manera manifestÃ³ mas mayor me mediante medio mejor mencionÃ³ menos menudo mi\nmia mias mientras mio mios mis misma mismas mismo mismos modo momento mucha\nmuchas mucho muchos muy mÃ¡s mÃ­ mÃ­a mÃ­as mÃ­o mÃ­os\n\nnada nadie ni ninguna ningunas ninguno ningunos ningÃºn no nos nosotras nosotros\nnuestra nuestras nuestro nuestros nueva nuevas nuevo nuevos nunca\n\nocho os otra otras otro otros\n\npais para parece parte partir pasada pasado paÃ¬s peor pero pesar poca pocas\npoco pocos podeis podemos poder podria podriais podriamos podrian podrias podrÃ¡\npodrÃ¡n podrÃ­a podrÃ­an poner por porque posible primer primera primero primeros\nprincipalmente pronto propia propias propio propios proximo prÃ³ximo prÃ³ximos\npudo pueda puede pueden puedo pues\n\nqeu que quedÃ³ queremos quien quienes quiere quiza quizas quizÃ¡ quizÃ¡s quiÃ©n quiÃ©nes quÃ©\n\nraras realizado realizar realizÃ³ repente respecto\n\nsabe sabeis sabemos saben saber sabes salvo se sea sean segun segunda segundo\nsegÃºn seis ser sera serÃ¡ serÃ¡n serÃ­a seÃ±alÃ³ si sido siempre siendo siete sigue\nsiguiente sin sino sobre sois sola solamente solas solo solos somos son soy\nsoyos su supuesto sus suya suyas suyo sÃ© sÃ­ sÃ³lo\n\ntal tambien tambiÃ©n tampoco tan tanto tarde te temprano tendrÃ¡ tendrÃ¡n teneis\ntenemos tener tenga tengo tenido tenÃ­a tercera ti tiempo tiene tienen toda\ntodas todavia todavÃ­a todo todos total trabaja trabajais trabajamos trabajan\ntrabajar trabajas trabajo tras trata travÃ©s tres tu tus tuvo tuya tuyas tuyo\ntuyos tÃº\n\nultimo un una unas uno unos usa usais usamos usan usar usas uso usted ustedes\nÃºltima Ãºltimas Ãºltimo Ãºltimos\n\nva vais valor vamos van varias varios vaya veces ver verdad verdadera verdadero\nvez vosotras vosotros voy vuestra vuestras vuestro vuestros\n\nya yo\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/es/tag_map.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/tt/__init__.py----------------------------------------
A:spacy.lang.tt.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.tt.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS, TOKENIZER_EXCEPTIONS)
A:spacy.lang.tt.__init__.infixes->tuple(TOKENIZER_INFIXES)
spacy.lang.tt.__init__.Tatar(Language)
spacy.lang.tt.__init__.TatarDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/tt/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/tt/lex_attrs.py----------------------------------------
A:spacy.lang.tt.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.tt.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.tt.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/tt/tokenizer_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/tt/stop_words.py----------------------------------------
A:spacy.lang.tt.stop_words.STOP_WORDS->set('Ğ°Ğ»Ğ°Ğ¹ Ğ°Ğ»Ğ°Ğ¹ÑĞ° Ğ°Ğ»Ğ°Ñ€ Ğ°Ğ»Ğ°Ñ€Ğ³Ğ° Ğ°Ğ»Ğ°Ñ€Ğ´Ğ° Ğ°Ğ»Ğ°Ñ€Ğ´Ğ°Ğ½ Ğ°Ğ»Ğ°Ñ€Ğ½Ñ‹ Ğ°Ğ»Ğ°Ñ€Ğ½Ñ‹Ò£ Ğ°Ğ»Ğ°Ñ€Ñ‡Ğ°\nĞ°Ğ»Ğ°Ñ€Ñ‹ Ğ°Ğ»Ğ°Ñ€Ñ‹Ğ½ Ğ°Ğ»Ğ°Ñ€Ñ‹Ğ½Ğ³Ğ° Ğ°Ğ»Ğ°Ñ€Ñ‹Ğ½Ğ´Ğ° Ğ°Ğ»Ğ°Ñ€Ñ‹Ğ½Ğ½Ğ°Ğ½ Ğ°Ğ»Ğ°Ñ€Ñ‹Ğ½Ñ‹Ò£ Ğ°Ğ»Ñ‚Ğ¼Ñ‹Ñˆ Ğ°Ğ»Ñ‚Ğ¼Ñ‹ÑˆÑ‹Ğ½Ñ‡Ñ‹ Ğ°Ğ»Ñ‚Ğ¼Ñ‹ÑˆÑ‹Ğ½Ñ‡Ñ‹Ğ³Ğ°\nĞ°Ğ»Ñ‚Ğ¼Ñ‹ÑˆÑ‹Ğ½Ñ‡Ñ‹Ğ´Ğ° Ğ°Ğ»Ñ‚Ğ¼Ñ‹ÑˆÑ‹Ğ½Ñ‡Ñ‹Ğ´Ğ°Ğ½ Ğ°Ğ»Ñ‚Ğ¼Ñ‹ÑˆÑ‹Ğ½Ñ‡Ñ‹Ğ»Ğ°Ñ€ Ğ°Ğ»Ñ‚Ğ¼Ñ‹ÑˆÑ‹Ğ½Ñ‡Ñ‹Ğ»Ğ°Ñ€Ğ³Ğ° Ğ°Ğ»Ñ‚Ğ¼Ñ‹ÑˆÑ‹Ğ½Ñ‡Ñ‹Ğ»Ğ°Ñ€Ğ´Ğ°\nĞ°Ğ»Ñ‚Ğ¼Ñ‹ÑˆÑ‹Ğ½Ñ‡Ñ‹Ğ»Ğ°Ñ€Ğ´Ğ°Ğ½ Ğ°Ğ»Ñ‚Ğ¼Ñ‹ÑˆÑ‹Ğ½Ñ‡Ñ‹Ğ»Ğ°Ñ€Ğ½Ñ‹ Ğ°Ğ»Ñ‚Ğ¼Ñ‹ÑˆÑ‹Ğ½Ñ‡Ñ‹Ğ»Ğ°Ñ€Ğ½Ñ‹Ò£ Ğ°Ğ»Ñ‚Ğ¼Ñ‹ÑˆÑ‹Ğ½Ñ‡Ñ‹Ğ½Ñ‹ Ğ°Ğ»Ñ‚Ğ¼Ñ‹ÑˆÑ‹Ğ½Ñ‡Ñ‹Ğ½Ñ‹Ò£\nĞ°Ğ»Ñ‚Ñ‹ Ğ°Ğ»Ñ‚Ñ‹Ğ»Ğ°Ğ¿ Ğ°Ğ»Ñ‚Ñ‹Ğ½Ñ‡Ñ‹ Ğ°Ğ»Ñ‚Ñ‹Ğ½Ñ‡Ñ‹Ğ³Ğ° Ğ°Ğ»Ñ‚Ñ‹Ğ½Ñ‡Ñ‹Ğ´Ğ° Ğ°Ğ»Ñ‚Ñ‹Ğ½Ñ‡Ñ‹Ğ´Ğ°Ğ½ Ğ°Ğ»Ñ‚Ñ‹Ğ½Ñ‡Ñ‹Ğ»Ğ°Ñ€ Ğ°Ğ»Ñ‚Ñ‹Ğ½Ñ‡Ñ‹Ğ»Ğ°Ñ€Ğ³Ğ°\nĞ°Ğ»Ñ‚Ñ‹Ğ½Ñ‡Ñ‹Ğ»Ğ°Ñ€Ğ´Ğ° Ğ°Ğ»Ñ‚Ñ‹Ğ½Ñ‡Ñ‹Ğ»Ğ°Ñ€Ğ´Ğ°Ğ½ Ğ°Ğ»Ñ‚Ñ‹Ğ½Ñ‡Ñ‹Ğ»Ğ°Ñ€Ğ½Ñ‹ Ğ°Ğ»Ñ‚Ñ‹Ğ½Ñ‡Ñ‹Ğ»Ğ°Ñ€Ğ½Ñ‹Ò£ Ğ°Ğ»Ñ‚Ñ‹Ğ½Ñ‡Ñ‹Ğ½Ñ‹ Ğ°Ğ»Ñ‚Ñ‹Ğ½Ñ‡Ñ‹Ğ½Ñ‹Ò£\nĞ°Ğ»Ñ‚Ñ‹ÑˆĞ°Ñ€ Ğ°Ğ½Ğ´Ğ° Ğ°Ğ½Ğ´Ğ°Ğ³Ñ‹ Ğ°Ğ½Ğ´Ğ°Ğ¹ Ğ°Ğ½Ğ´Ñ‹Ğ¹ Ğ°Ğ½Ğ´Ñ‹Ğ¹Ğ³Ğ° Ğ°Ğ½Ğ´Ñ‹Ğ¹Ğ´Ğ° Ğ°Ğ½Ğ´Ñ‹Ğ¹Ğ´Ğ°Ğ½ Ğ°Ğ½Ğ´Ñ‹Ğ¹Ğ½Ñ‹ Ğ°Ğ½Ğ´Ñ‹Ğ¹Ğ½Ñ‹Ò£ Ğ°Ğ½Ğ½Ğ°Ğ½\nĞ°Ğ½ÑÑ‹ Ğ°Ğ½Ñ‡Ğ° Ğ°Ğ½Ñ‹ Ğ°Ğ½Ñ‹ĞºÑ‹ Ğ°Ğ½Ñ‹ĞºÑ‹Ğ½ Ğ°Ğ½Ñ‹ĞºÑ‹Ğ½Ğ³Ğ° Ğ°Ğ½Ñ‹ĞºÑ‹Ğ½Ğ´Ğ° Ğ°Ğ½Ñ‹ĞºÑ‹Ğ½Ğ½Ğ°Ğ½ Ğ°Ğ½Ñ‹ĞºÑ‹Ğ½Ñ‹Ò£ Ğ°Ğ½Ñ‹ÑÑ‹ Ğ°Ğ½Ñ‹ÑÑ‹Ğ½\nĞ°Ğ½Ñ‹ÑÑ‹Ğ½Ğ³Ğ° Ğ°Ğ½Ñ‹ÑÑ‹Ğ½Ğ´Ğ° Ğ°Ğ½Ñ‹ÑÑ‹Ğ½Ğ½Ğ°Ğ½ Ğ°Ğ½Ñ‹ÑÑ‹Ğ½Ñ‹Ò£ Ğ°Ğ½Ñ‹Ò£ Ğ°Ğ½Ñ‹Ò£Ñ‡Ğ° Ğ°Ñ€ĞºÑ‹Ğ»Ñ‹ Ğ°Ñ€Ñ‹ Ğ°ÑˆĞ° Ğ°Ò£Ğ° Ğ°Ò£Ğ°Ñ€ Ğ°Ò£Ğ°Ñ€Ğ³Ğ°\nĞ°Ò£Ğ°Ñ€Ğ´Ğ° Ğ°Ò£Ğ°Ñ€Ğ´Ğ°Ğ³Ñ‹ Ğ°Ò£Ğ°Ñ€Ğ´Ğ°Ğ½\n\nĞ±Ğ°Ñ€ Ğ±Ğ°Ñ€Ğ° Ğ±Ğ°Ñ€Ğ»Ñ‹Ğº Ğ±Ğ°Ñ€Ñ‡Ğ° Ğ±Ğ°Ñ€Ñ‡Ğ°ÑÑ‹ Ğ±Ğ°Ñ€Ñ‡Ğ°ÑÑ‹Ğ½ Ğ±Ğ°Ñ€Ñ‡Ğ°ÑÑ‹Ğ½Ğ° Ğ±Ğ°Ñ€Ñ‡Ğ°ÑÑ‹Ğ½Ğ´Ğ° Ğ±Ğ°Ñ€Ñ‡Ğ°ÑÑ‹Ğ½Ğ½Ğ°Ğ½\nĞ±Ğ°Ñ€Ñ‡Ğ°ÑÑ‹Ğ½Ñ‹Ò£ Ğ±Ğ°Ñ€Ñ‹ Ğ±Ğ°ÑˆĞºĞ° Ğ±Ğ°ÑˆĞºĞ°Ñ‡Ğ° Ğ±Ğµ\xadĞ»Ó™Ğ½ Ğ±ĞµĞ· Ğ±ĞµĞ·Ğ³Ó™ Ğ±ĞµĞ·Ğ´Ó™ Ğ±ĞµĞ·Ğ´Ó™Ğ½ Ğ±ĞµĞ·Ğ½Ğµ Ğ±ĞµĞ·Ğ½ĞµÒ£ Ğ±ĞµĞ·Ğ½ĞµÒ£Ñ‡Ó™\nĞ±ĞµĞ»Ğ´ĞµÑ€Ò¯ĞµĞ½Ñ‡Ó™ Ğ±ĞµĞ»Ó™Ğ½ Ğ±ĞµÑ€ Ğ±ĞµÑ€Ğ³Ó™ Ğ±ĞµÑ€ĞµĞ½Ñ‡Ğµ Ğ±ĞµÑ€ĞµĞ½Ñ‡ĞµĞ³Ó™ Ğ±ĞµÑ€ĞµĞ½Ñ‡ĞµĞ´Ó™ Ğ±ĞµÑ€ĞµĞ½Ñ‡ĞµĞ´Ó™Ğ½ Ğ±ĞµÑ€ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€\nĞ±ĞµÑ€ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ³Ó™ Ğ±ĞµÑ€ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ´Ó™ Ğ±ĞµÑ€ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ´Ó™Ğ½ Ğ±ĞµÑ€ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ½Ğµ Ğ±ĞµÑ€ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ½ĞµÒ£ Ğ±ĞµÑ€ĞµĞ½Ñ‡ĞµĞ½Ğµ\nĞ±ĞµÑ€ĞµĞ½Ñ‡ĞµĞ½ĞµÒ£ Ğ±ĞµÑ€ĞºĞ°Ğ¹Ğ´Ğ° Ğ±ĞµÑ€ĞºĞ°Ğ¹ÑÑ‹ Ğ±ĞµÑ€ĞºĞ°Ñ Ğ±ĞµÑ€ĞºĞ°ÑĞ½ Ğ±ĞµÑ€ĞºĞµĞ¼ Ğ±ĞµÑ€ĞºĞµĞ¼Ğ³Ó™ Ğ±ĞµÑ€ĞºĞµĞ¼Ğ´Ó™ Ğ±ĞµÑ€ĞºĞµĞ¼Ğ½Ğµ\nĞ±ĞµÑ€ĞºĞµĞ¼Ğ½ĞµÒ£ Ğ±ĞµÑ€ĞºĞµĞ¼Ğ½Ó™Ğ½ Ğ±ĞµÑ€Ğ»Ó™Ğ½ Ğ±ĞµÑ€Ğ½Ğ¸ Ğ±ĞµÑ€Ğ½Ğ¸Ğ³Ó™ Ğ±ĞµÑ€Ğ½Ğ¸Ğ´Ó™ Ğ±ĞµÑ€Ğ½Ğ¸Ğ´Ó™Ğ½ Ğ±ĞµÑ€Ğ½Ğ¸Ğ½Ğ´Ğ¸ Ğ±ĞµÑ€Ğ½Ğ¸Ğ½Ğµ\nĞ±ĞµÑ€Ğ½Ğ¸Ğ½ĞµÒ£ Ğ±ĞµÑ€Ğ½Ğ¸Ñ‡ĞµĞº Ğ±ĞµÑ€Ğ½Ğ¸Ñ‡Ó™ Ğ±ĞµÑ€Ğ½Ó™Ñ€ÑÓ™ Ğ±ĞµÑ€Ğ½Ó™Ñ€ÑÓ™Ğ³Ó™ Ğ±ĞµÑ€Ğ½Ó™Ñ€ÑÓ™Ğ´Ó™ Ğ±ĞµÑ€Ğ½Ó™Ñ€ÑÓ™Ğ´Ó™Ğ½ Ğ±ĞµÑ€Ğ½Ó™Ñ€ÑÓ™Ğ½Ğµ\nĞ±ĞµÑ€Ğ½Ó™Ñ€ÑÓ™Ğ½ĞµÒ£ Ğ±ĞµÑ€Ñ€Ó™Ñ‚Ñ‚Ó™Ğ½ Ğ±ĞµÑ€ÑĞµ Ğ±ĞµÑ€ÑĞµĞ½ Ğ±ĞµÑ€ÑĞµĞ½Ğ³Ó™ Ğ±ĞµÑ€ÑĞµĞ½Ğ´Ó™ Ğ±ĞµÑ€ÑĞµĞ½ĞµÒ£ Ğ±ĞµÑ€ÑĞµĞ½Ğ½Ó™Ğ½ Ğ±ĞµÑ€Ó™Ñ€\nĞ±ĞµÑ€Ó™Ñ€ÑĞµ Ğ±ĞµÑ€Ó™Ñ€ÑĞµĞ½ Ğ±ĞµÑ€Ó™Ñ€ÑĞµĞ½Ğ´Ó™ Ğ±ĞµÑ€Ó™Ñ€ÑĞµĞ½ĞµÒ£ Ğ±ĞµÑ€Ó™Ñ€ÑĞµĞ½Ğ½Ó™Ğ½ Ğ±ĞµÑ€Ó™Ñ€ÑĞµĞ½Ó™ Ğ±ĞµÑ€Ó™Ò¯ Ğ±Ğ¸Ğ³Ñ€Ó™Ğº Ğ±Ğ¸Ğº\nĞ±Ğ¸Ñ€Ğ»Ğµ Ğ±Ğ¸Ñ‚ Ğ±Ğ¸Ñˆ Ğ±Ğ¸ÑˆĞµĞ½Ñ‡Ğµ Ğ±Ğ¸ÑˆĞµĞ½Ñ‡ĞµĞ³Ó™ Ğ±Ğ¸ÑˆĞµĞ½Ñ‡ĞµĞ´Ó™ Ğ±Ğ¸ÑˆĞµĞ½Ñ‡ĞµĞ´Ó™Ğ½ Ğ±Ğ¸ÑˆĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€ Ğ±Ğ¸ÑˆĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ³Ó™\nĞ±Ğ¸ÑˆĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ´Ó™ Ğ±Ğ¸ÑˆĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ´Ó™Ğ½ Ğ±Ğ¸ÑˆĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ½Ğµ Ğ±Ğ¸ÑˆĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ½ĞµÒ£ Ğ±Ğ¸ÑˆĞµĞ½Ñ‡ĞµĞ½Ğµ Ğ±Ğ¸ÑˆĞµĞ½Ñ‡ĞµĞ½ĞµÒ£\nĞ±Ğ¸ÑˆĞ»Ó™Ğ¿ Ğ±Ğ¾Ğ»Ğ°Ğ¹ Ğ±Ğ¾Ğ»Ğ°Ñ€ Ğ±Ğ¾Ğ»Ğ°Ñ€Ğ³Ğ° Ğ±Ğ¾Ğ»Ğ°Ñ€Ğ´Ğ° Ğ±Ğ¾Ğ»Ğ°Ñ€Ğ´Ğ°Ğ½ Ğ±Ğ¾Ğ»Ğ°Ñ€Ğ½Ñ‹ Ğ±Ğ¾Ğ»Ğ°Ñ€Ğ½Ñ‹Ò£ Ğ±Ğ¾Ğ»Ğ°Ñ€Ñ‹ Ğ±Ğ¾Ğ»Ğ°Ñ€Ñ‹Ğ½\nĞ±Ğ¾Ğ»Ğ°Ñ€Ñ‹Ğ½Ğ³Ğ° Ğ±Ğ¾Ğ»Ğ°Ñ€Ñ‹Ğ½Ğ´Ğ° Ğ±Ğ¾Ğ»Ğ°Ñ€Ñ‹Ğ½Ğ½Ğ°Ğ½ Ğ±Ğ¾Ğ»Ğ°Ñ€Ñ‹Ğ½Ñ‹Ò£ Ğ±Ñƒ Ğ±ÑƒĞµ Ğ±ÑƒĞµĞ½Ğ° Ğ±ÑƒĞµĞ½Ğ´Ğ° Ğ±ÑƒĞµĞ½Ñ‡Ğ° Ğ±ÑƒĞ¹Ğ»Ğ°Ğ¿\nĞ±ÑƒĞ»Ğ°Ñ€Ğ°Ğº Ğ±ÑƒĞ»Ğ°Ñ‡Ğ°Ğº Ğ±ÑƒĞ»Ğ´Ñ‹ Ğ±ÑƒĞ»Ğ¼Ñ‹Ğ¹ Ğ±ÑƒĞ»ÑĞ° Ğ±ÑƒĞ»Ñ‹Ğ¿ Ğ±ÑƒĞ»Ñ‹Ñ€ Ğ±ÑƒĞ»Ñ‹Ñ€Ğ³Ğ° Ğ±ÑƒÑÑ‹ Ğ±Ò¯Ñ‚Ó™Ğ½ Ğ±Ó™Ğ»ĞºĞ¸ Ğ±Ó™Ğ½\nĞ±Ó™Ñ€Ğ°Ğ±Ó™Ñ€ĞµĞ½Ó™ Ğ±Ó©Ñ‚ĞµĞ½ Ğ±Ó©Ñ‚ĞµĞ½ĞµÑĞµ Ğ±Ó©Ñ‚ĞµĞ½ĞµÑĞµĞ½ Ğ±Ó©Ñ‚ĞµĞ½ĞµÑĞµĞ½Ğ´Ó™ Ğ±Ó©Ñ‚ĞµĞ½ĞµÑĞµĞ½ĞµÒ£ Ğ±Ó©Ñ‚ĞµĞ½ĞµÑĞµĞ½Ğ½Ó™Ğ½\nĞ±Ó©Ñ‚ĞµĞ½ĞµÑĞµĞ½Ó™\n\nĞ²Ó™\n\nĞ³ĞµĞ» Ğ³ĞµĞ½Ó™ Ğ³Ñ‹Ğ½Ğ° Ğ³Ò¯Ñ Ğ³Ò¯ÑĞºĞ¸ Ğ³Ó™Ñ€Ñ‡Ó™\n\nĞ´Ğ° Ğ´Ğ¸ Ğ´Ğ¸Ğ³Ó™Ğ½ Ğ´Ğ¸Ğ´Ğµ Ğ´Ğ¸Ğ¿ Ğ´Ğ¸ÑÑ‚Ó™Ğ»Ó™Ğ³Ó™Ğ½ Ğ´Ğ¸ÑÑ‚Ó™Ğ»Ó™Ñ€Ñ‡Ó™ Ğ´Ò¯Ñ€Ñ‚ Ğ´Ò¯Ñ€Ñ‚ĞµĞ½Ñ‡Ğµ Ğ´Ò¯Ñ€Ñ‚ĞµĞ½Ñ‡ĞµĞ³Ó™ Ğ´Ò¯Ñ€Ñ‚ĞµĞ½Ñ‡ĞµĞ´Ó™\nĞ´Ò¯Ñ€Ñ‚ĞµĞ½Ñ‡ĞµĞ´Ó™Ğ½ Ğ´Ò¯Ñ€Ñ‚ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€ Ğ´Ò¯Ñ€Ñ‚ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ³Ó™ Ğ´Ò¯Ñ€Ñ‚ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ´Ó™ Ğ´Ò¯Ñ€Ñ‚ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ´Ó™Ğ½ Ğ´Ò¯Ñ€Ñ‚ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ½Ğµ\nĞ´Ò¯Ñ€Ñ‚ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ½ĞµÒ£ Ğ´Ò¯Ñ€Ñ‚ĞµĞ½Ñ‡ĞµĞ½Ğµ Ğ´Ò¯Ñ€Ñ‚ĞµĞ½Ñ‡ĞµĞ½ĞµÒ£ Ğ´Ò¯Ñ€Ñ‚Ğ»Ó™Ğ¿ Ğ´Ó™\n\nĞµĞ³ĞµÑ€Ğ¼Ğµ ĞµĞ³ĞµÑ€Ğ¼ĞµĞ½Ñ‡Ğµ ĞµĞ³ĞµÑ€Ğ¼ĞµĞ½Ñ‡ĞµĞ³Ó™ ĞµĞ³ĞµÑ€Ğ¼ĞµĞ½Ñ‡ĞµĞ´Ó™ ĞµĞ³ĞµÑ€Ğ¼ĞµĞ½Ñ‡ĞµĞ´Ó™Ğ½ ĞµĞ³ĞµÑ€Ğ¼ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€\nĞµĞ³ĞµÑ€Ğ¼ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ³Ó™ ĞµĞ³ĞµÑ€Ğ¼ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ´Ó™ ĞµĞ³ĞµÑ€Ğ¼ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ´Ó™Ğ½ ĞµĞ³ĞµÑ€Ğ¼ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ½Ğµ ĞµĞ³ĞµÑ€Ğ¼ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ½ĞµÒ£\nĞµĞ³ĞµÑ€Ğ¼ĞµĞ½Ñ‡ĞµĞ½Ğµ ĞµĞ³ĞµÑ€Ğ¼ĞµĞ½Ñ‡ĞµĞ½ĞµÒ£ ĞµĞ» ĞµĞ»Ğ´Ğ°\n\nĞ¸Ğ´Ğµ Ğ¸Ğ´ĞµĞº Ğ¸Ğ´ĞµĞ¼ Ğ¸ĞºĞµ Ğ¸ĞºĞµĞ½Ñ‡Ğµ Ğ¸ĞºĞµĞ½Ñ‡ĞµĞ³Ó™ Ğ¸ĞºĞµĞ½Ñ‡ĞµĞ´Ó™ Ğ¸ĞºĞµĞ½Ñ‡ĞµĞ´Ó™Ğ½ Ğ¸ĞºĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€ Ğ¸ĞºĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ³Ó™\nĞ¸ĞºĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ´Ó™ Ğ¸ĞºĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ´Ó™Ğ½ Ğ¸ĞºĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ½Ğµ Ğ¸ĞºĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ½ĞµÒ£ Ğ¸ĞºĞµĞ½Ñ‡ĞµĞ½Ğµ Ğ¸ĞºĞµĞ½Ñ‡ĞµĞ½ĞµÒ£ Ğ¸ĞºĞµÑˆÓ™Ñ€ Ğ¸ĞºÓ™Ğ½\nĞ¸Ğ»Ğ»Ğµ Ğ¸Ğ»Ğ»ĞµĞ½Ñ‡Ğµ Ğ¸Ğ»Ğ»ĞµĞ½Ñ‡ĞµĞ³Ó™ Ğ¸Ğ»Ğ»ĞµĞ½Ñ‡ĞµĞ´Ó™ Ğ¸Ğ»Ğ»ĞµĞ½Ñ‡ĞµĞ´Ó™Ğ½ Ğ¸Ğ»Ğ»ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€ Ğ¸Ğ»Ğ»ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ³Ó™\nĞ¸Ğ»Ğ»ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ´Ó™ Ğ¸Ğ»Ğ»ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ´Ó™Ğ½ Ğ¸Ğ»Ğ»ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ½Ğµ Ğ¸Ğ»Ğ»ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ½ĞµÒ£ Ğ¸Ğ»Ğ»ĞµĞ½Ñ‡ĞµĞ½Ğµ Ğ¸Ğ»Ğ»ĞµĞ½Ñ‡ĞµĞ½ĞµÒ£ Ğ¸Ğ»Ó™\nĞ¸Ğ»Ó™Ğ½ Ğ¸Ğ½Ğ´Ğµ Ğ¸ÑÓ™ Ğ¸Ñ‚ĞµĞ¿ Ğ¸Ñ‚ĞºÓ™Ğ½ Ğ¸Ñ‚Ñ‚Ğµ Ğ¸Ñ‚Ò¯ Ğ¸Ñ‚Ó™ Ğ¸Ñ‚Ó™Ñ€Ğ³Ó™ Ğ¸Ò£\n\nĞ¹Ó©Ğ· Ğ¹Ó©Ğ·ĞµĞ½Ñ‡Ğµ Ğ¹Ó©Ğ·ĞµĞ½Ñ‡ĞµĞ³Ó™ Ğ¹Ó©Ğ·ĞµĞ½Ñ‡ĞµĞ´Ó™ Ğ¹Ó©Ğ·ĞµĞ½Ñ‡ĞµĞ´Ó™Ğ½ Ğ¹Ó©Ğ·ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€ Ğ¹Ó©Ğ·ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ³Ó™ Ğ¹Ó©Ğ·ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ´Ó™\nĞ¹Ó©Ğ·ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ´Ó™Ğ½ Ğ¹Ó©Ğ·ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ½Ğµ Ğ¹Ó©Ğ·ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ½ĞµÒ£ Ğ¹Ó©Ğ·ĞµĞ½Ñ‡ĞµĞ½Ğµ Ğ¹Ó©Ğ·ĞµĞ½Ñ‡ĞµĞ½ĞµÒ£ Ğ¹Ó©Ğ·Ğ»Ó™Ğ³Ó™Ğ½ Ğ¹Ó©Ğ·Ğ»Ó™Ñ€Ñ‡Ó™\nĞ¹Ó©Ğ·Ó™Ñ€Ğ»Ó™Ğ³Ó™Ğ½\n\nĞºĞ°Ğ´Ó™Ñ€ ĞºĞ°Ğ¹ ĞºĞ°Ğ¹Ğ±ĞµÑ€ ĞºĞ°Ğ¹Ğ±ĞµÑ€Ğ»Ó™Ñ€Ğµ ĞºĞ°Ğ¹Ğ±ĞµÑ€ÑĞµ ĞºĞ°Ğ¹Ğ±ĞµÑ€Ó™Ò¯ ĞºĞ°Ğ¹Ğ±ĞµÑ€Ó™Ò¯Ğ³Ó™ ĞºĞ°Ğ¹Ğ±ĞµÑ€Ó™Ò¯Ğ´Ó™ ĞºĞ°Ğ¹Ğ±ĞµÑ€Ó™Ò¯Ğ´Ó™Ğ½\nĞºĞ°Ğ¹Ğ±ĞµÑ€Ó™Ò¯Ğ½Ğµ ĞºĞ°Ğ¹Ğ±ĞµÑ€Ó™Ò¯Ğ½ĞµÒ£ ĞºĞ°Ğ¹Ğ´Ğ°Ğ³Ñ‹ ĞºĞ°Ğ¹ÑÑ‹ ĞºĞ°Ğ¹ÑÑ‹Ğ±ĞµÑ€ ĞºĞ°Ğ¹ÑÑ‹Ğ½ ĞºĞ°Ğ¹ÑÑ‹Ğ½Ğ° ĞºĞ°Ğ¹ÑÑ‹Ğ½Ğ´Ğ° ĞºĞ°Ğ¹ÑÑ‹Ğ½Ğ½Ğ°Ğ½\nĞºĞ°Ğ¹ÑÑ‹Ğ½Ñ‹Ò£ ĞºĞ°Ğ¹Ñ‡Ğ°Ğ½Ğ³Ñ‹ ĞºĞ°Ğ¹Ñ‡Ğ°Ğ½Ğ´Ğ°Ğ³Ñ‹ ĞºĞ°Ğ¹Ñ‡Ğ°Ğ½Ğ½Ğ°Ğ½ ĞºĞ°Ñ€Ğ°Ğ³Ğ°Ğ½Ğ´Ğ° ĞºĞ°Ñ€Ğ°Ğ¼Ğ°ÑÑ‚Ğ°Ğ½ ĞºĞ°Ñ€Ğ°Ğ¼Ñ‹Ğ¹ ĞºĞ°Ñ€Ğ°Ñ‚Ğ° ĞºĞ°Ñ€ÑˆÑ‹\nĞºĞ°Ñ€ÑˆÑ‹Ğ½Ğ° ĞºĞ°Ñ€ÑˆÑ‹Ğ½Ğ´Ğ° ĞºĞ°Ñ€ÑˆÑ‹Ğ½Ğ´Ğ°Ğ³Ñ‹ ĞºĞµĞ±ĞµĞº ĞºĞµĞ¼ ĞºĞµĞ¼Ğ³Ó™ ĞºĞµĞ¼Ğ´Ó™ ĞºĞµĞ¼Ğ½Ğµ ĞºĞµĞ¼Ğ½ĞµÒ£ ĞºĞµĞ¼Ğ½Ó™Ğ½ ĞºĞµĞ½Ó™ ĞºĞ¸\nĞºĞ¸Ğ»ĞµĞ¿ ĞºĞ¸Ğ»Ó™ ĞºĞ¸Ñ€Ó™Ğº ĞºÑ‹Ğ½Ğ° ĞºÑ‹Ñ€Ñ‹Ğ³Ñ‹Ğ½Ñ‡Ñ‹ ĞºÑ‹Ñ€Ñ‹Ğ³Ñ‹Ğ½Ñ‡Ñ‹Ğ³Ğ° ĞºÑ‹Ñ€Ñ‹Ğ³Ñ‹Ğ½Ñ‡Ñ‹Ğ´Ğ° ĞºÑ‹Ñ€Ñ‹Ğ³Ñ‹Ğ½Ñ‡Ñ‹Ğ´Ğ°Ğ½\nĞºÑ‹Ñ€Ñ‹Ğ³Ñ‹Ğ½Ñ‡Ñ‹Ğ»Ğ°Ñ€ ĞºÑ‹Ñ€Ñ‹Ğ³Ñ‹Ğ½Ñ‡Ñ‹Ğ»Ğ°Ñ€Ğ³Ğ° ĞºÑ‹Ñ€Ñ‹Ğ³Ñ‹Ğ½Ñ‡Ñ‹Ğ»Ğ°Ñ€Ğ´Ğ° ĞºÑ‹Ñ€Ñ‹Ğ³Ñ‹Ğ½Ñ‡Ñ‹Ğ»Ğ°Ñ€Ğ´Ğ°Ğ½ ĞºÑ‹Ñ€Ñ‹Ğ³Ñ‹Ğ½Ñ‡Ñ‹Ğ»Ğ°Ñ€Ğ½Ñ‹\nĞºÑ‹Ñ€Ñ‹Ğ³Ñ‹Ğ½Ñ‡Ñ‹Ğ»Ğ°Ñ€Ğ½Ñ‹Ò£ ĞºÑ‹Ñ€Ñ‹Ğ³Ñ‹Ğ½Ñ‡Ñ‹Ğ½Ñ‹ ĞºÑ‹Ñ€Ñ‹Ğ³Ñ‹Ğ½Ñ‡Ñ‹Ğ½Ñ‹Ò£ ĞºÑ‹Ñ€Ñ‹Ğº ĞºÒ¯Ğº ĞºÒ¯Ğ¿Ğ»Ó™Ğ³Ó™Ğ½ ĞºÒ¯Ğ¿Ğ¼Ğµ ĞºÒ¯Ğ¿Ğ¼ĞµĞ»Ó™Ğ¿\nĞºÒ¯Ğ¿Ğ¼ĞµÑˆÓ™Ñ€ ĞºÒ¯Ğ¿Ğ¼ĞµÑˆÓ™Ñ€Ğ»Ó™Ğ¿ ĞºÒ¯Ğ¿Ñ‚Ó™Ğ½ ĞºÒ¯Ñ€Ó™\n\nĞ»Ó™ĞºĞ¸Ğ½\n\nĞ¼Ğ°ĞºÑĞ°Ñ‚Ñ‹Ğ½Ğ´Ğ° Ğ¼ĞµĞ½Ó™ Ğ¼ĞµÒ£ Ğ¼ĞµÒ£ĞµĞ½Ñ‡Ğµ Ğ¼ĞµÒ£ĞµĞ½Ñ‡ĞµĞ³Ó™ Ğ¼ĞµÒ£ĞµĞ½Ñ‡ĞµĞ´Ó™ Ğ¼ĞµÒ£ĞµĞ½Ñ‡ĞµĞ´Ó™Ğ½ Ğ¼ĞµÒ£ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€\nĞ¼ĞµÒ£ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ³Ó™ Ğ¼ĞµÒ£ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ´Ó™ Ğ¼ĞµÒ£ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ´Ó™Ğ½ Ğ¼ĞµÒ£ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ½Ğµ Ğ¼ĞµÒ£ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ½ĞµÒ£ Ğ¼ĞµÒ£ĞµĞ½Ñ‡ĞµĞ½Ğµ\nĞ¼ĞµÒ£ĞµĞ½Ñ‡ĞµĞ½ĞµÒ£ Ğ¼ĞµÒ£Ğ»Ó™Ğ³Ó™Ğ½ Ğ¼ĞµÒ£Ğ»Ó™Ğ¿ Ğ¼ĞµÒ£Ğ½Ó™Ñ€Ñ‡Ó™ Ğ¼ĞµÒ£Ó™Ñ€Ğ»Ó™Ğ³Ó™Ğ½ Ğ¼ĞµÒ£Ó™Ñ€Ğ»Ó™Ğ¿ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ»Ğ°Ğ³Ğ°Ğ½\nĞ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ»Ğ°Ñ€Ñ‡Ğ° Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ»Ğ°Ğ³Ğ°Ğ½ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ€Ñ‡Ğ° Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ñ‹Ğ½Ñ‡Ñ‹ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ñ‹Ğ½Ñ‡Ñ‹Ğ³Ğ°\nĞ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ñ‹Ğ½Ñ‡Ñ‹Ğ´Ğ° Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ñ‹Ğ½Ñ‡Ñ‹Ğ´Ğ°Ğ½ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ñ‹Ğ½Ñ‡Ñ‹Ğ»Ğ°Ñ€ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ñ‹Ğ½Ñ‡Ñ‹Ğ»Ğ°Ñ€Ğ³Ğ° Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ñ‹Ğ½Ñ‡Ñ‹Ğ»Ğ°Ñ€Ğ´Ğ°\nĞ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ñ‹Ğ½Ñ‡Ñ‹Ğ»Ğ°Ñ€Ğ´Ğ°Ğ½ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ñ‹Ğ½Ñ‡Ñ‹Ğ»Ğ°Ñ€Ğ½Ñ‹ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ñ‹Ğ½Ñ‡Ñ‹Ğ»Ğ°Ñ€Ğ½Ñ‹Ò£ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ñ‹Ğ½Ñ‡Ñ‹Ğ½Ñ‹\nĞ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ñ‹Ğ½Ñ‡Ñ‹Ğ½Ñ‹Ò£ Ğ¼Ğ¸Ğ½ Ğ¼Ğ¸Ğ½Ğ´Ó™ Ğ¼Ğ¸Ğ½Ğµ Ğ¼Ğ¸Ğ½ĞµĞ¼ Ğ¼Ğ¸Ğ½ĞµĞ¼Ñ‡Ó™ Ğ¼Ğ¸Ğ½Ğ½Ó™Ğ½ Ğ¼Ğ¸Ò£Ğ° Ğ¼Ğ¾Ğ½Ğ´Ğ° Ğ¼Ğ¾Ğ½Ğ´Ğ°Ğ³Ñ‹ Ğ¼Ğ¾Ğ½Ğ´Ñ‹Ğµ\nĞ¼Ğ¾Ğ½Ğ´Ñ‹ĞµĞ½ Ğ¼Ğ¾Ğ½Ğ´Ñ‹ĞµĞ½Ğ³Ó™ Ğ¼Ğ¾Ğ½Ğ´Ñ‹ĞµĞ½Ğ´Ó™ Ğ¼Ğ¾Ğ½Ğ´Ñ‹ĞµĞ½Ğ½Ó™Ğ½ Ğ¼Ğ¾Ğ½Ğ´Ñ‹ĞµĞ½Ñ‹Ò£ Ğ¼Ğ¾Ğ½Ğ´Ñ‹Ğ¹ Ğ¼Ğ¾Ğ½Ğ´Ñ‹Ğ¹Ğ³Ğ° Ğ¼Ğ¾Ğ½Ğ´Ñ‹Ğ¹Ğ´Ğ°\nĞ¼Ğ¾Ğ½Ğ´Ñ‹Ğ¹Ğ´Ğ°Ğ½ Ğ¼Ğ¾Ğ½Ğ´Ñ‹Ğ¹Ğ»Ğ°Ñ€ Ğ¼Ğ¾Ğ½Ğ´Ñ‹Ğ¹Ğ»Ğ°Ñ€Ğ³Ğ° Ğ¼Ğ¾Ğ½Ğ´Ñ‹Ğ¹Ğ»Ğ°Ñ€Ğ´Ğ° Ğ¼Ğ¾Ğ½Ğ´Ñ‹Ğ¹Ğ»Ğ°Ñ€Ğ´Ğ°Ğ½ Ğ¼Ğ¾Ğ½Ğ´Ñ‹Ğ¹Ğ»Ğ°Ñ€Ğ½Ñ‹\nĞ¼Ğ¾Ğ½Ğ´Ñ‹Ğ¹Ğ»Ğ°Ñ€Ğ½Ñ‹Ò£ Ğ¼Ğ¾Ğ½Ğ´Ñ‹Ğ¹Ğ»Ğ°Ñ€Ñ‹ Ğ¼Ğ¾Ğ½Ğ´Ñ‹Ğ¹Ğ»Ğ°Ñ€Ñ‹Ğ½ Ğ¼Ğ¾Ğ½Ğ´Ñ‹Ğ¹Ğ»Ğ°Ñ€Ñ‹Ğ½Ğ³Ğ° Ğ¼Ğ¾Ğ½Ğ´Ñ‹Ğ¹Ğ»Ğ°Ñ€Ñ‹Ğ½Ğ´Ğ° Ğ¼Ğ¾Ğ½Ğ´Ñ‹Ğ¹Ğ»Ğ°Ñ€Ñ‹Ğ½Ğ½Ğ°Ğ½\nĞ¼Ğ¾Ğ½Ğ´Ñ‹Ğ¹Ğ»Ğ°Ñ€Ñ‹Ğ½Ñ‹Ò£ Ğ¼Ğ¾Ğ½Ğ´Ñ‹Ğ¹Ğ½Ñ‹ Ğ¼Ğ¾Ğ½Ğ´Ñ‹Ğ¹Ğ½Ñ‹Ò£ Ğ¼Ğ¾Ğ½Ğ½Ğ°Ğ½ Ğ¼Ğ¾Ğ½ÑÑ‹Ğ· Ğ¼Ğ¾Ğ½Ñ‡Ğ° Ğ¼Ğ¾Ğ½Ñ‹ Ğ¼Ğ¾Ğ½Ñ‹ĞºÑ‹ Ğ¼Ğ¾Ğ½Ñ‹ĞºÑ‹Ğ½\nĞ¼Ğ¾Ğ½Ñ‹ĞºÑ‹Ğ½Ğ³Ğ° Ğ¼Ğ¾Ğ½Ñ‹ĞºÑ‹Ğ½Ğ´Ğ° Ğ¼Ğ¾Ğ½Ñ‹ĞºÑ‹Ğ½Ğ½Ğ°Ğ½ Ğ¼Ğ¾Ğ½Ñ‹ĞºÑ‹Ğ½Ñ‹Ò£ Ğ¼Ğ¾Ğ½Ñ‹ÑÑ‹ Ğ¼Ğ¾Ğ½Ñ‹ÑÑ‹Ğ½ Ğ¼Ğ¾Ğ½Ñ‹ÑÑ‹Ğ½Ğ³Ğ° Ğ¼Ğ¾Ğ½Ñ‹ÑÑ‹Ğ½Ğ´Ğ°\nĞ¼Ğ¾Ğ½Ñ‹ÑÑ‹Ğ½Ğ½Ğ°Ğ½ Ğ¼Ğ¾Ğ½Ñ‹ÑÑ‹Ğ½Ñ‹Ò£ Ğ¼Ğ¾Ğ½Ñ‹Ò£ Ğ¼Ğ¾Ò£Ğ° Ğ¼Ğ¾Ò£Ğ°Ñ€ Ğ¼Ğ¾Ò£Ğ°Ñ€Ğ³Ğ° Ğ¼Ó™Ğ³ÑŠĞ»Ò¯Ğ¼Ğ°Ñ‚Ñ‹Ğ½Ñ‡Ğ° Ğ¼Ó™Ğ³Ó™Ñ€ Ğ¼Ó™Ğ½ Ğ¼Ó©Ğ¼ĞºĞ¸Ğ½\n\nĞ½Ğ¸ Ğ½Ğ¸Ğ±Ğ°Ñ€Ñ‹ÑÑ‹ Ğ½Ğ¸ĞºĞ°Ğ´Ó™Ñ€Ğµ Ğ½Ğ¸Ğ½Ğ´Ğ¸ Ğ½Ğ¸Ğ½Ğ´Ğ¸Ğµ Ğ½Ğ¸Ğ½Ğ´Ğ¸ĞµĞ½ Ğ½Ğ¸Ğ½Ğ´Ğ¸ĞµĞ½Ğ³Ó™ Ğ½Ğ¸Ğ½Ğ´Ğ¸ĞµĞ½Ğ´Ó™ Ğ½Ğ¸Ğ½Ğ´Ğ¸ĞµĞ½ĞµÒ£\nĞ½Ğ¸Ğ½Ğ´Ğ¸ĞµĞ½Ğ½Ó™Ğ½ Ğ½Ğ¸Ğ½Ğ´Ğ¸Ğ»Ó™Ñ€ Ğ½Ğ¸Ğ½Ğ´Ğ¸Ğ»Ó™Ñ€Ğ³Ó™ Ğ½Ğ¸Ğ½Ğ´Ğ¸Ğ»Ó™Ñ€Ğ´Ó™ Ğ½Ğ¸Ğ½Ğ´Ğ¸Ğ»Ó™Ñ€Ğ´Ó™Ğ½ Ğ½Ğ¸Ğ½Ğ´Ğ¸Ğ»Ó™Ñ€ĞµĞ½ Ğ½Ğ¸Ğ½Ğ´Ğ¸Ğ»Ó™Ñ€ĞµĞ½Ğ½\nĞ½Ğ¸Ğ½Ğ´Ğ¸Ğ»Ó™Ñ€ĞµĞ½Ğ½Ğ³Ó™ Ğ½Ğ¸Ğ½Ğ´Ğ¸Ğ»Ó™Ñ€ĞµĞ½Ğ½Ğ´Ó™ Ğ½Ğ¸Ğ½Ğ´Ğ¸Ğ»Ó™Ñ€ĞµĞ½Ğ½ĞµÒ£ Ğ½Ğ¸Ğ½Ğ´Ğ¸Ğ»Ó™Ñ€ĞµĞ½Ğ½Ğ½Ó™Ğ½ Ğ½Ğ¸Ğ½Ğ´Ğ¸Ğ»Ó™Ñ€Ğ½Ğµ Ğ½Ğ¸Ğ½Ğ´Ğ¸Ğ»Ó™Ñ€Ğ½ĞµÒ£\nĞ½Ğ¸Ğ½Ğ´Ğ¸Ñ€Ó™Ğº Ğ½Ğ¸Ñ…Ó™Ñ‚Ğ»Ğµ Ğ½Ğ¸Ñ‡Ğ°ĞºĞ»Ñ‹ Ğ½Ğ¸Ñ‡ĞµĞº Ğ½Ğ¸Ñ‡Ó™ÑˆÓ™Ñ€ Ğ½Ğ¸Ñ‡Ó™ÑˆÓ™Ñ€Ğ»Ó™Ğ¿ Ğ½ÑƒĞ»ÑŒ Ğ½Ñ‡Ğµ Ğ½Ñ‡Ñ‹ Ğ½Ó™Ñ€ÑÓ™ Ğ½Ó™Ñ€ÑÓ™Ğ³Ó™\nĞ½Ó™Ñ€ÑÓ™Ğ´Ó™ Ğ½Ó™Ñ€ÑÓ™Ğ´Ó™Ğ½ Ğ½Ó™Ñ€ÑÓ™Ğ½Ğµ Ğ½Ó™Ñ€ÑÓ™Ğ½ĞµÒ£\n\nÑĞ°ĞµĞ½ ÑĞµĞ· ÑĞµĞ·Ğ³Ó™ ÑĞµĞ·Ğ´Ó™ ÑĞµĞ·Ğ´Ó™Ğ½ ÑĞµĞ·Ğ½Ğµ ÑĞµĞ·Ğ½ĞµÒ£ ÑĞµĞ·Ğ½ĞµÒ£Ñ‡Ó™ ÑĞ¸Ğ³ĞµĞ· ÑĞ¸Ğ³ĞµĞ·ĞµĞ½Ñ‡Ğµ ÑĞ¸Ğ³ĞµĞ·ĞµĞ½Ñ‡ĞµĞ³Ó™\nÑĞ¸Ğ³ĞµĞ·ĞµĞ½Ñ‡ĞµĞ´Ó™ ÑĞ¸Ğ³ĞµĞ·ĞµĞ½Ñ‡ĞµĞ´Ó™Ğ½ ÑĞ¸Ğ³ĞµĞ·ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€ ÑĞ¸Ğ³ĞµĞ·ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ³Ó™ ÑĞ¸Ğ³ĞµĞ·ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ´Ó™\nÑĞ¸Ğ³ĞµĞ·ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ´Ó™Ğ½ ÑĞ¸Ğ³ĞµĞ·ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ½Ğµ ÑĞ¸Ğ³ĞµĞ·ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ½ĞµÒ£ ÑĞ¸Ğ³ĞµĞ·ĞµĞ½Ñ‡ĞµĞ½Ğµ ÑĞ¸Ğ³ĞµĞ·ĞµĞ½Ñ‡ĞµĞ½ĞµÒ£\nÑĞ¸ĞºÑÓ™Ğ½ ÑĞ¸Ğ½ ÑĞ¸Ğ½Ğ´Ó™ ÑĞ¸Ğ½Ğµ ÑĞ¸Ğ½ĞµÒ£ ÑĞ¸Ğ½ĞµÒ£Ñ‡Ó™ ÑĞ¸Ğ½Ğ½Ó™Ğ½ ÑĞ¸Ò£Ğ° ÑĞ¾Ò£ ÑÑ‹Ğ¼Ğ°Ğ½ ÑÒ¯Ğ·ĞµĞ½Ñ‡Ó™ ÑÒ¯Ğ·Ğ»Ó™Ñ€ĞµĞ½Ñ‡Ó™\n\nÑ‚Ğ° Ñ‚Ğ°Ğ±Ğ° Ñ‚ĞµĞ³Ğµ Ñ‚ĞµĞ³ĞµĞ»Ó™Ğ¹ Ñ‚ĞµĞ³ĞµĞ»Ó™Ñ€ Ñ‚ĞµĞ³ĞµĞ»Ó™Ñ€Ğ³Ó™ Ñ‚ĞµĞ³ĞµĞ»Ó™Ñ€Ğ´Ó™ Ñ‚ĞµĞ³ĞµĞ»Ó™Ñ€Ğ´Ó™Ğ½ Ñ‚ĞµĞ³ĞµĞ»Ó™Ñ€Ğµ Ñ‚ĞµĞ³ĞµĞ»Ó™Ñ€ĞµĞ½\nÑ‚ĞµĞ³ĞµĞ»Ó™Ñ€ĞµĞ½Ğ³Ó™ Ñ‚ĞµĞ³ĞµĞ»Ó™Ñ€ĞµĞ½Ğ´Ó™ Ñ‚ĞµĞ³ĞµĞ»Ó™Ñ€ĞµĞ½ĞµÒ£ Ñ‚ĞµĞ³ĞµĞ»Ó™Ñ€ĞµĞ½Ğ½Ó™Ğ½ Ñ‚ĞµĞ³ĞµĞ»Ó™Ñ€Ğ½Ğµ Ñ‚ĞµĞ³ĞµĞ»Ó™Ñ€Ğ½ĞµÒ£ Ñ‚ĞµĞ³ĞµĞ½Ğ´Ğ¸\nÑ‚ĞµĞ³ĞµĞ½Ğ´Ğ¸Ğ³Ó™ Ñ‚ĞµĞ³ĞµĞ½Ğ´Ğ¸Ğ´Ó™ Ñ‚ĞµĞ³ĞµĞ½Ğ´Ğ¸Ğ´Ó™Ğ½ Ñ‚ĞµĞ³ĞµĞ½Ğ´Ğ¸Ğ½Ğµ Ñ‚ĞµĞ³ĞµĞ½Ğ´Ğ¸Ğ½ĞµÒ£ Ñ‚ĞµĞ³ĞµĞ½Ğ´Ó™ Ñ‚ĞµĞ³ĞµĞ½Ğ´Ó™Ğ³Ğµ Ñ‚ĞµĞ³ĞµĞ½Ğµ\nÑ‚ĞµĞ³ĞµĞ½ĞµĞºĞµ Ñ‚ĞµĞ³ĞµĞ½ĞµĞºĞµĞ½ Ñ‚ĞµĞ³ĞµĞ½ĞµĞºĞµĞ½Ğ³Ó™ Ñ‚ĞµĞ³ĞµĞ½ĞµĞºĞµĞ½Ğ´Ó™ Ñ‚ĞµĞ³ĞµĞ½ĞµĞºĞµĞ½ĞµÒ£ Ñ‚ĞµĞ³ĞµĞ½ĞµĞºĞµĞ½Ğ½Ó™Ğ½ Ñ‚ĞµĞ³ĞµĞ½ĞµÒ£\nÑ‚ĞµĞ³ĞµĞ½Ğ½Ó™Ğ½ Ñ‚ĞµĞ³ĞµÑĞµ Ñ‚ĞµĞ³ĞµÑĞµĞ½ Ñ‚ĞµĞ³ĞµÑĞµĞ½Ğ³Ó™ Ñ‚ĞµĞ³ĞµÑĞµĞ½Ğ´Ó™ Ñ‚ĞµĞ³ĞµÑĞµĞ½ĞµÒ£ Ñ‚ĞµĞ³ĞµÑĞµĞ½Ğ½Ó™Ğ½ Ñ‚ĞµĞ³ĞµÒ£Ó™ Ñ‚Ğ¸ĞµÑˆ Ñ‚Ğ¸Ğº\nÑ‚Ğ¸ĞºĞ»Ğµ Ñ‚Ğ¾Ñ€Ğ° Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´ Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½ Ñ‚ÑƒĞ³Ñ‹Ğ· Ñ‚ÑƒĞ³Ñ‹Ğ·Ğ»Ğ°Ğ¿ Ñ‚ÑƒĞ³Ñ‹Ğ·Ğ»Ğ°ÑˆÑ‹Ğ¿ Ñ‚ÑƒĞ³Ñ‹Ğ·Ñ‹Ğ½Ñ‡Ñ‹ Ñ‚ÑƒĞ³Ñ‹Ğ·Ñ‹Ğ½Ñ‡Ñ‹Ğ³Ğ°\nÑ‚ÑƒĞ³Ñ‹Ğ·Ñ‹Ğ½Ñ‡Ñ‹Ğ´Ğ° Ñ‚ÑƒĞ³Ñ‹Ğ·Ñ‹Ğ½Ñ‡Ñ‹Ğ´Ğ°Ğ½ Ñ‚ÑƒĞ³Ñ‹Ğ·Ñ‹Ğ½Ñ‡Ñ‹Ğ»Ğ°Ñ€ Ñ‚ÑƒĞ³Ñ‹Ğ·Ñ‹Ğ½Ñ‡Ñ‹Ğ»Ğ°Ñ€Ğ³Ğ° Ñ‚ÑƒĞ³Ñ‹Ğ·Ñ‹Ğ½Ñ‡Ñ‹Ğ»Ğ°Ñ€Ğ´Ğ°\nÑ‚ÑƒĞ³Ñ‹Ğ·Ñ‹Ğ½Ñ‡Ñ‹Ğ»Ğ°Ñ€Ğ´Ğ°Ğ½ Ñ‚ÑƒĞ³Ñ‹Ğ·Ñ‹Ğ½Ñ‡Ñ‹Ğ»Ğ°Ñ€Ğ½Ñ‹ Ñ‚ÑƒĞ³Ñ‹Ğ·Ñ‹Ğ½Ñ‡Ñ‹Ğ»Ğ°Ñ€Ğ½Ñ‹Ò£ Ñ‚ÑƒĞ³Ñ‹Ğ·Ñ‹Ğ½Ñ‡Ñ‹Ğ½Ñ‹ Ñ‚ÑƒĞ³Ñ‹Ğ·Ñ‹Ğ½Ñ‡Ñ‹Ğ½Ñ‹Ò£ Ñ‚ÑƒĞºÑĞ°Ğ½\nÑ‚ÑƒĞºÑĞ°Ğ½Ñ‹Ğ½Ñ‡Ñ‹ Ñ‚ÑƒĞºÑĞ°Ğ½Ñ‹Ğ½Ñ‡Ñ‹Ğ³Ğ° Ñ‚ÑƒĞºÑĞ°Ğ½Ñ‹Ğ½Ñ‡Ñ‹Ğ´Ğ° Ñ‚ÑƒĞºÑĞ°Ğ½Ñ‹Ğ½Ñ‡Ñ‹Ğ´Ğ°Ğ½ Ñ‚ÑƒĞºÑĞ°Ğ½Ñ‹Ğ½Ñ‡Ñ‹Ğ»Ğ°Ñ€ Ñ‚ÑƒĞºÑĞ°Ğ½Ñ‹Ğ½Ñ‡Ñ‹Ğ»Ğ°Ñ€Ğ³Ğ°\nÑ‚ÑƒĞºÑĞ°Ğ½Ñ‹Ğ½Ñ‡Ñ‹Ğ»Ğ°Ñ€Ğ´Ğ° Ñ‚ÑƒĞºÑĞ°Ğ½Ñ‹Ğ½Ñ‡Ñ‹Ğ»Ğ°Ñ€Ğ´Ğ°Ğ½ Ñ‚ÑƒĞºÑĞ°Ğ½Ñ‹Ğ½Ñ‡Ñ‹Ğ»Ğ°Ñ€Ğ½Ñ‹ Ñ‚ÑƒĞºÑĞ°Ğ½Ñ‹Ğ½Ñ‡Ñ‹Ğ»Ğ°Ñ€Ğ½Ñ‹Ò£ Ñ‚ÑƒĞºÑĞ°Ğ½Ñ‹Ğ½Ñ‡Ñ‹Ğ½Ñ‹\nÑ‚ÑƒĞºÑĞ°Ğ½Ñ‹Ğ½Ñ‡Ñ‹Ğ½Ñ‹Ò£ Ñ‚ÑƒÑ€Ñ‹Ğ½Ğ´Ğ° Ñ‚Ñ‹Ñˆ Ñ‚Ò¯Ğ³ĞµĞ» Ñ‚Ó™ Ñ‚Ó™Ğ³Ğ°ĞµĞ½Ğ»Ó™Ğ½Ğ³Ó™Ğ½ Ñ‚Ó©Ğ¼Ó™Ğ½\n\nÑƒĞµĞ½Ñ‡Ğ° ÑƒĞ¹Ğ»Ğ°Ğ²Ñ‹Ğ½Ñ‡Ğ° ÑƒĞº ÑƒĞ» ÑƒĞ½ ÑƒĞ½Ğ°Ğ»Ñ‚Ñ‹ ÑƒĞ½Ğ°Ğ»Ñ‚Ñ‹Ğ½Ñ‡Ñ‹ ÑƒĞ½Ğ°Ğ»Ñ‚Ñ‹Ğ½Ñ‡Ñ‹Ğ³Ğ° ÑƒĞ½Ğ°Ğ»Ñ‚Ñ‹Ğ½Ñ‡Ñ‹Ğ´Ğ° ÑƒĞ½Ğ°Ğ»Ñ‚Ñ‹Ğ½Ñ‡Ñ‹Ğ´Ğ°Ğ½\nÑƒĞ½Ğ°Ğ»Ñ‚Ñ‹Ğ½Ñ‡Ñ‹Ğ»Ğ°Ñ€ ÑƒĞ½Ğ°Ğ»Ñ‚Ñ‹Ğ½Ñ‡Ñ‹Ğ»Ğ°Ñ€Ğ³Ğ° ÑƒĞ½Ğ°Ğ»Ñ‚Ñ‹Ğ½Ñ‡Ñ‹Ğ»Ğ°Ñ€Ğ´Ğ° ÑƒĞ½Ğ°Ğ»Ñ‚Ñ‹Ğ½Ñ‡Ñ‹Ğ»Ğ°Ñ€Ğ´Ğ°Ğ½ ÑƒĞ½Ğ°Ğ»Ñ‚Ñ‹Ğ½Ñ‡Ñ‹Ğ»Ğ°Ñ€Ğ½Ñ‹\nÑƒĞ½Ğ°Ğ»Ñ‚Ñ‹Ğ½Ñ‡Ñ‹Ğ»Ğ°Ñ€Ğ½Ñ‹Ò£ ÑƒĞ½Ğ°Ğ»Ñ‚Ñ‹Ğ½Ñ‡Ñ‹Ğ½Ñ‹ ÑƒĞ½Ğ°Ğ»Ñ‚Ñ‹Ğ½Ñ‡Ñ‹Ğ½Ñ‹Ò£ ÑƒĞ½Ğ°Ñ€Ğ»Ğ°Ğ³Ğ°Ğ½ ÑƒĞ½Ğ°Ñ€Ğ»Ğ°Ğ¿ ÑƒĞ½Ğ°ÑƒĞ»Ğ° ÑƒĞ½Ğ°ÑƒĞ»Ğ°Ğ¿ ÑƒĞ½Ğ±ĞµÑ€\nÑƒĞ½Ğ±ĞµÑ€ĞµĞ½Ñ‡Ğµ ÑƒĞ½Ğ±ĞµÑ€ĞµĞ½Ñ‡ĞµĞ³Ó™ ÑƒĞ½Ğ±ĞµÑ€ĞµĞ½Ñ‡ĞµĞ´Ó™ ÑƒĞ½Ğ±ĞµÑ€ĞµĞ½Ñ‡ĞµĞ´Ó™Ğ½ ÑƒĞ½Ğ±ĞµÑ€ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€ ÑƒĞ½Ğ±ĞµÑ€ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ³Ó™\nÑƒĞ½Ğ±ĞµÑ€ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ´Ó™ ÑƒĞ½Ğ±ĞµÑ€ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ´Ó™Ğ½ ÑƒĞ½Ğ±ĞµÑ€ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ½Ğµ ÑƒĞ½Ğ±ĞµÑ€ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ½ĞµÒ£ ÑƒĞ½Ğ±ĞµÑ€ĞµĞ½Ñ‡ĞµĞ½Ğµ\nÑƒĞ½Ğ±ĞµÑ€ĞµĞ½Ñ‡ĞµĞ½ĞµÒ£ ÑƒĞ½Ğ±Ğ¸Ñˆ ÑƒĞ½Ğ±Ğ¸ÑˆĞµĞ½Ñ‡Ğµ ÑƒĞ½Ğ±Ğ¸ÑˆĞµĞ½Ñ‡ĞµĞ³Ó™ ÑƒĞ½Ğ±Ğ¸ÑˆĞµĞ½Ñ‡ĞµĞ´Ó™ ÑƒĞ½Ğ±Ğ¸ÑˆĞµĞ½Ñ‡ĞµĞ´Ó™Ğ½ ÑƒĞ½Ğ±Ğ¸ÑˆĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€\nÑƒĞ½Ğ±Ğ¸ÑˆĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ³Ó™ ÑƒĞ½Ğ±Ğ¸ÑˆĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ´Ó™ ÑƒĞ½Ğ±Ğ¸ÑˆĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ´Ó™Ğ½ ÑƒĞ½Ğ±Ğ¸ÑˆĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ½Ğµ ÑƒĞ½Ğ±Ğ¸ÑˆĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ½ĞµÒ£\nÑƒĞ½Ğ±Ğ¸ÑˆĞµĞ½Ñ‡ĞµĞ½Ğµ ÑƒĞ½Ğ±Ğ¸ÑˆĞµĞ½Ñ‡ĞµĞ½ĞµÒ£ ÑƒĞ½Ğ´Ò¯Ñ€Ñ‚ ÑƒĞ½Ğ´Ò¯Ñ€Ñ‚ĞµĞ½Ñ‡Ğµ ÑƒĞ½Ğ´Ò¯Ñ€Ñ‚ĞµĞ½Ñ‡ĞµĞ³Ó™ ÑƒĞ½Ğ´Ò¯Ñ€Ñ‚ĞµĞ½Ñ‡ĞµĞ´Ó™\nÑƒĞ½Ğ´Ò¯Ñ€Ñ‚ĞµĞ½Ñ‡ĞµĞ´Ó™Ğ½ ÑƒĞ½Ğ´Ò¯Ñ€Ñ‚ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€ ÑƒĞ½Ğ´Ò¯Ñ€Ñ‚ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ³Ó™ ÑƒĞ½Ğ´Ò¯Ñ€Ñ‚ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ´Ó™ ÑƒĞ½Ğ´Ò¯Ñ€Ñ‚ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ´Ó™Ğ½\nÑƒĞ½Ğ´Ò¯Ñ€Ñ‚ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ½Ğµ ÑƒĞ½Ğ´Ò¯Ñ€Ñ‚ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ½ĞµÒ£ ÑƒĞ½Ğ´Ò¯Ñ€Ñ‚ĞµĞ½Ñ‡ĞµĞ½Ğµ ÑƒĞ½Ğ´Ò¯Ñ€Ñ‚ĞµĞ½Ñ‡ĞµĞ½ĞµÒ£ ÑƒĞ½Ğ¸ĞºĞµ ÑƒĞ½Ğ¸ĞºĞµĞ½Ñ‡Ğµ\nÑƒĞ½Ğ¸ĞºĞµĞ½Ñ‡ĞµĞ³Ó™ ÑƒĞ½Ğ¸ĞºĞµĞ½Ñ‡ĞµĞ´Ó™ ÑƒĞ½Ğ¸ĞºĞµĞ½Ñ‡ĞµĞ´Ó™Ğ½ ÑƒĞ½Ğ¸ĞºĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€ ÑƒĞ½Ğ¸ĞºĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ³Ó™ ÑƒĞ½Ğ¸ĞºĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ´Ó™\nÑƒĞ½Ğ¸ĞºĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ´Ó™Ğ½ ÑƒĞ½Ğ¸ĞºĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ½Ğµ ÑƒĞ½Ğ¸ĞºĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ½ĞµÒ£ ÑƒĞ½Ğ¸ĞºĞµĞ½Ñ‡ĞµĞ½Ğµ ÑƒĞ½Ğ¸ĞºĞµĞ½Ñ‡ĞµĞ½ĞµÒ£ ÑƒĞ½Ğ»Ğ°Ğ³Ğ°Ğ½\nÑƒĞ½Ğ»Ğ°Ğ¿ ÑƒĞ½Ğ½Ğ°Ñ€Ñ‡Ğ° ÑƒĞ½ÑĞ¸Ğ³ĞµĞ· ÑƒĞ½ÑĞ¸Ğ³ĞµĞ·ĞµĞ½Ñ‡Ğµ ÑƒĞ½ÑĞ¸Ğ³ĞµĞ·ĞµĞ½Ñ‡ĞµĞ³Ó™ ÑƒĞ½ÑĞ¸Ğ³ĞµĞ·ĞµĞ½Ñ‡ĞµĞ´Ó™ ÑƒĞ½ÑĞ¸Ğ³ĞµĞ·ĞµĞ½Ñ‡ĞµĞ´Ó™Ğ½\nÑƒĞ½ÑĞ¸Ğ³ĞµĞ·ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€ ÑƒĞ½ÑĞ¸Ğ³ĞµĞ·ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ³Ó™ ÑƒĞ½ÑĞ¸Ğ³ĞµĞ·ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ´Ó™ ÑƒĞ½ÑĞ¸Ğ³ĞµĞ·ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ´Ó™Ğ½\nÑƒĞ½ÑĞ¸Ğ³ĞµĞ·ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ½Ğµ ÑƒĞ½ÑĞ¸Ğ³ĞµĞ·ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ½ĞµÒ£ ÑƒĞ½ÑĞ¸Ğ³ĞµĞ·ĞµĞ½Ñ‡ĞµĞ½Ğµ ÑƒĞ½ÑĞ¸Ğ³ĞµĞ·ĞµĞ½Ñ‡ĞµĞ½ĞµÒ£ ÑƒĞ½Ñ‚ÑƒĞ³Ñ‹Ğ·\nÑƒĞ½Ñ‚ÑƒĞ³Ñ‹Ğ·Ñ‹Ğ½Ñ‡Ñ‹ ÑƒĞ½Ñ‚ÑƒĞ³Ñ‹Ğ·Ñ‹Ğ½Ñ‡Ñ‹Ğ³Ğ° ÑƒĞ½Ñ‚ÑƒĞ³Ñ‹Ğ·Ñ‹Ğ½Ñ‡Ñ‹Ğ´Ğ° ÑƒĞ½Ñ‚ÑƒĞ³Ñ‹Ğ·Ñ‹Ğ½Ñ‡Ñ‹Ğ´Ğ°Ğ½ ÑƒĞ½Ñ‚ÑƒĞ³Ñ‹Ğ·Ñ‹Ğ½Ñ‡Ñ‹Ğ»Ğ°Ñ€\nÑƒĞ½Ñ‚ÑƒĞ³Ñ‹Ğ·Ñ‹Ğ½Ñ‡Ñ‹Ğ»Ğ°Ñ€Ğ³Ğ° ÑƒĞ½Ñ‚ÑƒĞ³Ñ‹Ğ·Ñ‹Ğ½Ñ‡Ñ‹Ğ»Ğ°Ñ€Ğ´Ğ° ÑƒĞ½Ñ‚ÑƒĞ³Ñ‹Ğ·Ñ‹Ğ½Ñ‡Ñ‹Ğ»Ğ°Ñ€Ğ´Ğ°Ğ½ ÑƒĞ½Ñ‚ÑƒĞ³Ñ‹Ğ·Ñ‹Ğ½Ñ‡Ñ‹Ğ»Ğ°Ñ€Ğ½Ñ‹\nÑƒĞ½Ñ‚ÑƒĞ³Ñ‹Ğ·Ñ‹Ğ½Ñ‡Ñ‹Ğ»Ğ°Ñ€Ğ½Ñ‹Ò£ ÑƒĞ½Ñ‚ÑƒĞ³Ñ‹Ğ·Ñ‹Ğ½Ñ‡Ñ‹Ğ½Ñ‹ ÑƒĞ½Ñ‚ÑƒĞ³Ñ‹Ğ·Ñ‹Ğ½Ñ‡Ñ‹Ğ½Ñ‹Ò£ ÑƒĞ½Ñ‹Ğ½Ñ‡Ñ‹ ÑƒĞ½Ñ‹Ğ½Ñ‡Ñ‹Ğ³Ğ° ÑƒĞ½Ñ‹Ğ½Ñ‡Ñ‹Ğ´Ğ°\nÑƒĞ½Ñ‹Ğ½Ñ‡Ñ‹Ğ´Ğ°Ğ½ ÑƒĞ½Ñ‹Ğ½Ñ‡Ñ‹Ğ»Ğ°Ñ€ ÑƒĞ½Ñ‹Ğ½Ñ‡Ñ‹Ğ»Ğ°Ñ€Ğ³Ğ° ÑƒĞ½Ñ‹Ğ½Ñ‡Ñ‹Ğ»Ğ°Ñ€Ğ´Ğ° ÑƒĞ½Ñ‹Ğ½Ñ‡Ñ‹Ğ»Ğ°Ñ€Ğ´Ğ°Ğ½ ÑƒĞ½Ñ‹Ğ½Ñ‡Ñ‹Ğ»Ğ°Ñ€Ğ½Ñ‹\nÑƒĞ½Ñ‹Ğ½Ñ‡Ñ‹Ğ»Ğ°Ñ€Ğ½Ñ‹Ò£ ÑƒĞ½Ñ‹Ğ½Ñ‡Ñ‹Ğ½Ñ‹ ÑƒĞ½Ñ‹Ğ½Ñ‡Ñ‹Ğ½Ñ‹Ò£ ÑƒĞ½Ò—Ğ¸Ğ´Ğµ ÑƒĞ½Ò—Ğ¸Ğ´ĞµĞ½Ñ‡Ğµ ÑƒĞ½Ò—Ğ¸Ğ´ĞµĞ½Ñ‡ĞµĞ³Ó™ ÑƒĞ½Ò—Ğ¸Ğ´ĞµĞ½Ñ‡ĞµĞ´Ó™\nÑƒĞ½Ò—Ğ¸Ğ´ĞµĞ½Ñ‡ĞµĞ´Ó™Ğ½ ÑƒĞ½Ò—Ğ¸Ğ´ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€ ÑƒĞ½Ò—Ğ¸Ğ´ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ³Ó™ ÑƒĞ½Ò—Ğ¸Ğ´ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ´Ó™ ÑƒĞ½Ò—Ğ¸Ğ´ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ´Ó™Ğ½\nÑƒĞ½Ò—Ğ¸Ğ´ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ½Ğµ ÑƒĞ½Ò—Ğ¸Ğ´ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ½ĞµÒ£ ÑƒĞ½Ò—Ğ¸Ğ´ĞµĞ½Ñ‡ĞµĞ½Ğµ ÑƒĞ½Ò—Ğ¸Ğ´ĞµĞ½Ñ‡ĞµĞ½ĞµÒ£ ÑƒĞ½Ó©Ñ‡ ÑƒĞ½Ó©Ñ‡ĞµĞ½Ñ‡Ğµ ÑƒĞ½Ó©Ñ‡ĞµĞ½Ñ‡ĞµĞ³Ó™\nÑƒĞ½Ó©Ñ‡ĞµĞ½Ñ‡ĞµĞ´Ó™ ÑƒĞ½Ó©Ñ‡ĞµĞ½Ñ‡ĞµĞ´Ó™Ğ½ ÑƒĞ½Ó©Ñ‡ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€ ÑƒĞ½Ó©Ñ‡ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ³Ó™ ÑƒĞ½Ó©Ñ‡ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ´Ó™ ÑƒĞ½Ó©Ñ‡ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ´Ó™Ğ½\nÑƒĞ½Ó©Ñ‡ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ½Ğµ ÑƒĞ½Ó©Ñ‡ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ½ĞµÒ£ ÑƒĞ½Ó©Ñ‡ĞµĞ½Ñ‡ĞµĞ½Ğµ ÑƒĞ½Ó©Ñ‡ĞµĞ½Ñ‡ĞµĞ½ĞµÒ£ ÑƒÑ‚Ñ‹Ğ· ÑƒÑ‚Ñ‹Ğ·Ñ‹Ğ½Ñ‡Ñ‹ ÑƒÑ‚Ñ‹Ğ·Ñ‹Ğ½Ñ‡Ñ‹Ğ³Ğ°\nÑƒÑ‚Ñ‹Ğ·Ñ‹Ğ½Ñ‡Ñ‹Ğ´Ğ° ÑƒÑ‚Ñ‹Ğ·Ñ‹Ğ½Ñ‡Ñ‹Ğ´Ğ°Ğ½ ÑƒÑ‚Ñ‹Ğ·Ñ‹Ğ½Ñ‡Ñ‹Ğ»Ğ°Ñ€ ÑƒÑ‚Ñ‹Ğ·Ñ‹Ğ½Ñ‡Ñ‹Ğ»Ğ°Ñ€Ğ³Ğ° ÑƒÑ‚Ñ‹Ğ·Ñ‹Ğ½Ñ‡Ñ‹Ğ»Ğ°Ñ€Ğ´Ğ° ÑƒÑ‚Ñ‹Ğ·Ñ‹Ğ½Ñ‡Ñ‹Ğ»Ğ°Ñ€Ğ´Ğ°Ğ½\nÑƒÑ‚Ñ‹Ğ·Ñ‹Ğ½Ñ‡Ñ‹Ğ»Ğ°Ñ€Ğ½Ñ‹ ÑƒÑ‚Ñ‹Ğ·Ñ‹Ğ½Ñ‡Ñ‹Ğ»Ğ°Ñ€Ğ½Ñ‹Ò£ ÑƒÑ‚Ñ‹Ğ·Ñ‹Ğ½Ñ‡Ñ‹Ğ½Ñ‹ ÑƒÑ‚Ñ‹Ğ·Ñ‹Ğ½Ñ‡Ñ‹Ğ½Ñ‹Ò£\n\nÑ„Ğ¸ĞºĞµÑ€ĞµĞ½Ñ‡Ó™ Ñ„Ó™ĞºĞ°Ñ‚ÑŒ\n\nÑ…Ğ°ĞºÑ‹Ğ½Ğ´Ğ° Ñ…Ó™Ğ±Ó™Ñ€ Ñ…Ó™Ğ»Ğ±ÑƒĞºĞ¸ Ñ…Ó™Ñ‚Ğ»Ğµ Ñ…Ó™Ñ‚Ñ‚Ğ°\n\nÑ‡Ğ°ĞºĞ»Ñ‹ Ñ‡Ğ°ĞºÑ‚Ğ° Ñ‡Ó©Ğ½ĞºĞ¸\n\nÑˆĞ¸ĞºĞµĞ»Ğ»Ğµ ÑˆÑƒĞ» ÑˆÑƒĞ»Ğ°Ğ¹ ÑˆÑƒĞ»Ğ°Ñ€ ÑˆÑƒĞ»Ğ°Ñ€Ğ³Ğ° ÑˆÑƒĞ»Ğ°Ñ€Ğ´Ğ° ÑˆÑƒĞ»Ğ°Ñ€Ğ´Ğ°Ğ½ ÑˆÑƒĞ»Ğ°Ñ€Ğ½Ñ‹ ÑˆÑƒĞ»Ğ°Ñ€Ğ½Ñ‹Ò£ ÑˆÑƒĞ»Ğ°Ñ€Ñ‹ ÑˆÑƒĞ»Ğ°Ñ€Ñ‹Ğ½\nÑˆÑƒĞ»Ğ°Ñ€Ñ‹Ğ½Ğ³Ğ° ÑˆÑƒĞ»Ğ°Ñ€Ñ‹Ğ½Ğ´Ğ° ÑˆÑƒĞ»Ğ°Ñ€Ñ‹Ğ½Ğ½Ğ°Ğ½ ÑˆÑƒĞ»Ğ°Ñ€Ñ‹Ğ½Ñ‹Ò£ ÑˆÑƒĞ»ĞºĞ°Ğ´Ó™Ñ€ ÑˆÑƒĞ»Ñ‚Ğ¸ĞºĞ»Ğµ ÑˆÑƒĞ»Ñ‚Ğ¸ĞºĞ»ĞµĞ¼ ÑˆÑƒĞ»Ñ…Ó™Ñ‚Ğ»Ğµ\nÑˆÑƒĞ»Ñ‡Ğ°ĞºĞ»Ñ‹ ÑˆÑƒĞ½Ğ´Ğ° ÑˆÑƒĞ½Ğ´Ğ°Ğ³Ñ‹ ÑˆÑƒĞ½Ğ´Ñ‹Ğ¹ ÑˆÑƒĞ½Ğ´Ñ‹Ğ¹Ğ³Ğ° ÑˆÑƒĞ½Ğ´Ñ‹Ğ¹Ğ´Ğ° ÑˆÑƒĞ½Ğ´Ñ‹Ğ¹Ğ´Ğ°Ğ½ ÑˆÑƒĞ½Ğ´Ñ‹Ğ¹Ğ½Ñ‹ ÑˆÑƒĞ½Ğ´Ñ‹Ğ¹Ğ½Ñ‹Ò£\nÑˆÑƒĞ½Ğ»Ñ‹ĞºÑ‚Ğ°Ğ½ ÑˆÑƒĞ½Ğ½Ğ°Ğ½ ÑˆÑƒĞ½ÑÑ‹ ÑˆÑƒĞ½Ñ‡Ğ° ÑˆÑƒĞ½Ñ‹ ÑˆÑƒĞ½Ñ‹ĞºÑ‹ ÑˆÑƒĞ½Ñ‹ĞºÑ‹Ğ½ ÑˆÑƒĞ½Ñ‹ĞºÑ‹Ğ½Ğ³Ğ° ÑˆÑƒĞ½Ñ‹ĞºÑ‹Ğ½Ğ´Ğ° ÑˆÑƒĞ½Ñ‹ĞºÑ‹Ğ½Ğ½Ğ°Ğ½\nÑˆÑƒĞ½Ñ‹ĞºÑ‹Ğ½Ñ‹Ò£ ÑˆÑƒĞ½Ñ‹ÑÑ‹ ÑˆÑƒĞ½Ñ‹ÑÑ‹Ğ½ ÑˆÑƒĞ½Ñ‹ÑÑ‹Ğ½Ğ³Ğ° ÑˆÑƒĞ½Ñ‹ÑÑ‹Ğ½Ğ´Ğ° ÑˆÑƒĞ½Ñ‹ÑÑ‹Ğ½Ğ½Ğ°Ğ½ ÑˆÑƒĞ½Ñ‹ÑÑ‹Ğ½Ñ‹Ò£ ÑˆÑƒĞ½Ñ‹Ò£ ÑˆÑƒÑˆÑ‹\nÑˆÑƒÑˆÑ‹Ğ½Ğ´Ğ° ÑˆÑƒÑˆÑ‹Ğ½Ğ½Ğ°Ğ½ ÑˆÑƒÑˆÑ‹Ğ½Ñ‹ ÑˆÑƒÑˆÑ‹Ğ½Ñ‹Ò£ ÑˆÑƒÑˆÑ‹Ò£Ğ° ÑˆÑƒÒ£Ğ° ÑˆÑƒÒ£Ğ°Ñ€ ÑˆÑƒÒ£Ğ°Ñ€Ğ³Ğ°\n\nÑĞ»ĞµĞº\n\nÑĞ³Ñ‹Ğ¹ÑÓ™ ÑĞº ÑĞºÑĞ°\n\nÑ ÑĞ³ÑŠĞ½Ğ¸ ÑĞ·ÑƒÑ‹Ğ½Ñ‡Ğ° ÑĞ¸ÑÓ™ ÑĞºĞ¸ ÑĞºÑ‚Ğ°Ğ½ ÑĞºÑ‹Ğ½ ÑÑ€Ğ°ÑˆĞ»Ñ‹ ÑÑ…ÑƒÑ‚ ÑÑˆÑŒ ÑÑˆÑŒĞ»ĞµĞº\n\nÒ—Ğ¸Ğ´Ğµ Ò—Ğ¸Ğ´ĞµĞ»Ó™Ğ¿ Ò—Ğ¸Ğ´ĞµĞ½Ñ‡Ğµ Ò—Ğ¸Ğ´ĞµĞ½Ñ‡ĞµĞ³Ó™ Ò—Ğ¸Ğ´ĞµĞ½Ñ‡ĞµĞ´Ó™ Ò—Ğ¸Ğ´ĞµĞ½Ñ‡ĞµĞ´Ó™Ğ½ Ò—Ğ¸Ğ´ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€ Ò—Ğ¸Ğ´ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ³Ó™\nÒ—Ğ¸Ğ´ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ´Ó™ Ò—Ğ¸Ğ´ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ´Ó™Ğ½ Ò—Ğ¸Ğ´ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ½Ğµ Ò—Ğ¸Ğ´ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ½ĞµÒ£ Ò—Ğ¸Ğ´ĞµĞ½Ñ‡ĞµĞ½Ğµ Ò—Ğ¸Ğ´ĞµĞ½Ñ‡ĞµĞ½ĞµÒ£\nÒ—Ğ¸Ğ´ĞµÑˆÓ™Ñ€ Ò—Ğ¸Ñ‚Ğ¼ĞµÑˆ Ò—Ğ¸Ñ‚Ğ¼ĞµÑˆĞµĞ½Ñ‡Ğµ Ò—Ğ¸Ñ‚Ğ¼ĞµÑˆĞµĞ½Ñ‡ĞµĞ³Ó™ Ò—Ğ¸Ñ‚Ğ¼ĞµÑˆĞµĞ½Ñ‡ĞµĞ´Ó™ Ò—Ğ¸Ñ‚Ğ¼ĞµÑˆĞµĞ½Ñ‡ĞµĞ´Ó™Ğ½ Ò—Ğ¸Ñ‚Ğ¼ĞµÑˆĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€\nÒ—Ğ¸Ñ‚Ğ¼ĞµÑˆĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ³Ó™ Ò—Ğ¸Ñ‚Ğ¼ĞµÑˆĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ´Ó™ Ò—Ğ¸Ñ‚Ğ¼ĞµÑˆĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ´Ó™Ğ½ Ò—Ğ¸Ñ‚Ğ¼ĞµÑˆĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ½Ğµ\nÒ—Ğ¸Ñ‚Ğ¼ĞµÑˆĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ½ĞµÒ£ Ò—Ğ¸Ñ‚Ğ¼ĞµÑˆĞµĞ½Ñ‡ĞµĞ½Ğµ Ò—Ğ¸Ñ‚Ğ¼ĞµÑˆĞµĞ½Ñ‡ĞµĞ½ĞµÒ£ Ò—Ñ‹ĞµĞ½Ñ‹ÑÑ‹\n\nÒ¯Ğ· Ò¯Ğ·Ğµ Ò¯Ğ·ĞµĞ¼ Ò¯Ğ·ĞµĞ¼Ğ´Ó™ Ò¯Ğ·ĞµĞ¼Ğ½Ğµ Ò¯Ğ·ĞµĞ¼Ğ½ĞµÒ£ Ò¯Ğ·ĞµĞ¼Ğ½Ó™Ğ½ Ò¯Ğ·ĞµĞ¼Ó™ Ò¯Ğ·ĞµĞ½ Ò¯Ğ·ĞµĞ½Ğ´Ó™ Ò¯Ğ·ĞµĞ½ĞµÒ£ Ò¯Ğ·ĞµĞ½Ğ½Ó™Ğ½ Ò¯Ğ·ĞµĞ½Ó™\nÒ¯Ğº\n\nÒ»Ğ¸Ñ‡Ğ±ĞµÑ€ Ò»Ğ¸Ñ‡Ğ±ĞµÑ€Ğµ Ò»Ğ¸Ñ‡Ğ±ĞµÑ€ĞµĞ½ Ò»Ğ¸Ñ‡Ğ±ĞµÑ€ĞµĞ½Ğ´Ó™ Ò»Ğ¸Ñ‡Ğ±ĞµÑ€ĞµĞ½ĞµÒ£ Ò»Ğ¸Ñ‡Ğ±ĞµÑ€ĞµĞ½Ğ½Ó™Ğ½ Ò»Ğ¸Ñ‡Ğ±ĞµÑ€ĞµĞ½Ó™ Ò»Ğ¸Ñ‡Ğ±ĞµÑ€ÑĞµ\nÒ»Ğ¸Ñ‡Ğ±ĞµÑ€ÑĞµĞ½ Ò»Ğ¸Ñ‡Ğ±ĞµÑ€ÑĞµĞ½Ğ´Ó™ Ò»Ğ¸Ñ‡Ğ±ĞµÑ€ÑĞµĞ½ĞµÒ£ Ò»Ğ¸Ñ‡Ğ±ĞµÑ€ÑĞµĞ½Ğ½Ó™Ğ½ Ò»Ğ¸Ñ‡Ğ±ĞµÑ€ÑĞµĞ½Ó™ Ò»Ğ¸Ñ‡Ğ±ĞµÑ€Ó™Ò¯ Ò»Ğ¸Ñ‡Ğ±ĞµÑ€Ó™Ò¯Ğ³Ó™\nÒ»Ğ¸Ñ‡Ğ±ĞµÑ€Ó™Ò¯Ğ´Ó™ Ò»Ğ¸Ñ‡Ğ±ĞµÑ€Ó™Ò¯Ğ´Ó™Ğ½ Ò»Ğ¸Ñ‡Ğ±ĞµÑ€Ó™Ò¯Ğ½Ğµ Ò»Ğ¸Ñ‡Ğ±ĞµÑ€Ó™Ò¯Ğ½ĞµÒ£ Ò»Ğ¸Ñ‡ĞºĞ°Ğ¹ÑÑ‹ Ò»Ğ¸Ñ‡ĞºĞ°Ğ¹ÑÑ‹Ğ³Ğ° Ò»Ğ¸Ñ‡ĞºĞ°Ğ¹ÑÑ‹Ğ´Ğ°\nÒ»Ğ¸Ñ‡ĞºĞ°Ğ¹ÑÑ‹Ğ´Ğ°Ğ½ Ò»Ğ¸Ñ‡ĞºĞ°Ğ¹ÑÑ‹Ğ½Ñ‹ Ò»Ğ¸Ñ‡ĞºĞ°Ğ¹ÑÑ‹Ğ½Ñ‹Ò£ Ò»Ğ¸Ñ‡ĞºĞµĞ¼ Ò»Ğ¸Ñ‡ĞºĞµĞ¼Ğ³Ó™ Ò»Ğ¸Ñ‡ĞºĞµĞ¼Ğ´Ó™ Ò»Ğ¸Ñ‡ĞºĞµĞ¼Ğ½Ğµ Ò»Ğ¸Ñ‡ĞºĞµĞ¼Ğ½ĞµÒ£\nÒ»Ğ¸Ñ‡ĞºĞµĞ¼Ğ½Ó™Ğ½ Ò»Ğ¸Ñ‡Ğ½Ğ¸ Ò»Ğ¸Ñ‡Ğ½Ğ¸Ğ³Ó™ Ò»Ğ¸Ñ‡Ğ½Ğ¸Ğ´Ó™ Ò»Ğ¸Ñ‡Ğ½Ğ¸Ğ´Ó™Ğ½ Ò»Ğ¸Ñ‡Ğ½Ğ¸Ğ½Ğ´Ğ¸ Ò»Ğ¸Ñ‡Ğ½Ğ¸Ğ½Ğµ Ò»Ğ¸Ñ‡Ğ½Ğ¸Ğ½ĞµÒ£ Ò»Ğ¸Ñ‡Ğ½Ó™Ñ€ÑÓ™\nÒ»Ğ¸Ñ‡Ğ½Ó™Ñ€ÑÓ™Ğ³Ó™ Ò»Ğ¸Ñ‡Ğ½Ó™Ñ€ÑÓ™Ğ´Ó™ Ò»Ğ¸Ñ‡Ğ½Ó™Ñ€ÑÓ™Ğ´Ó™Ğ½ Ò»Ğ¸Ñ‡Ğ½Ó™Ñ€ÑÓ™Ğ½Ğµ Ò»Ğ¸Ñ‡Ğ½Ó™Ñ€ÑÓ™Ğ½ĞµÒ£ Ò»Ó™Ğ¼ Ò»Ó™Ğ¼Ğ¼Ó™ Ò»Ó™Ğ¼Ğ¼Ó™ÑĞµ\nÒ»Ó™Ğ¼Ğ¼Ó™ÑĞµĞ½ Ò»Ó™Ğ¼Ğ¼Ó™ÑĞµĞ½Ğ´Ó™ Ò»Ó™Ğ¼Ğ¼Ó™ÑĞµĞ½ĞµÒ£ Ò»Ó™Ğ¼Ğ¼Ó™ÑĞµĞ½Ğ½Ó™Ğ½ Ò»Ó™Ğ¼Ğ¼Ó™ÑĞµĞ½Ó™ Ò»Ó™Ñ€ Ò»Ó™Ñ€Ğ±ĞµÑ€ Ò»Ó™Ñ€Ğ±ĞµÑ€Ğµ Ò»Ó™Ñ€Ğ±ĞµÑ€ÑĞµ\nÒ»Ó™Ñ€ĞºĞ°Ğ¹ÑÑ‹ Ò»Ó™Ñ€ĞºĞ°Ğ¹ÑÑ‹Ğ³Ğ° Ò»Ó™Ñ€ĞºĞ°Ğ¹ÑÑ‹Ğ´Ğ° Ò»Ó™Ñ€ĞºĞ°Ğ¹ÑÑ‹Ğ´Ğ°Ğ½ Ò»Ó™Ñ€ĞºĞ°Ğ¹ÑÑ‹Ğ½Ñ‹ Ò»Ó™Ñ€ĞºĞ°Ğ¹ÑÑ‹Ğ½Ñ‹Ò£ Ò»Ó™Ñ€ĞºĞµĞ¼\nÒ»Ó™Ñ€ĞºĞµĞ¼Ğ³Ó™ Ò»Ó™Ñ€ĞºĞµĞ¼Ğ´Ó™ Ò»Ó™Ñ€ĞºĞµĞ¼Ğ½Ğµ Ò»Ó™Ñ€ĞºĞµĞ¼Ğ½ĞµÒ£ Ò»Ó™Ñ€ĞºĞµĞ¼Ğ½Ó™Ğ½ Ò»Ó™Ñ€Ğ½Ğ¸ Ò»Ó™Ñ€Ğ½Ó™Ñ€ÑÓ™ Ò»Ó™Ñ€Ğ½Ó™Ñ€ÑÓ™Ğ³Ó™\nÒ»Ó™Ñ€Ğ½Ó™Ñ€ÑÓ™Ğ´Ó™ Ò»Ó™Ñ€Ğ½Ó™Ñ€ÑÓ™Ğ´Ó™Ğ½ Ò»Ó™Ñ€Ğ½Ó™Ñ€ÑÓ™Ğ½Ğµ Ò»Ó™Ñ€Ğ½Ó™Ñ€ÑÓ™Ğ½ĞµÒ£ Ò»Ó™Ñ€Ñ‚Ó©Ñ€Ğ»Ğµ\n\nÓ™ Ó™Ğ³Ó™Ñ€ Ó™Ğ¹Ñ‚Ò¯ĞµĞ½Ñ‡Ó™ Ó™Ğ¹Ñ‚Ò¯Ğ»Ó™Ñ€ĞµĞ½Ñ‡Ó™ Ó™Ğ»Ğ±Ó™Ñ‚Ñ‚Ó™ Ó™Ğ»Ğµ Ó™Ğ»ĞµĞ³Ğµ Ó™Ğ»Ğ»Ó™ Ó™Ğ¼Ğ¼Ğ° Ó™Ğ½Ó™\n\nÓ©ÑÑ‚Ó™Ğ¿ Ó©Ñ‡ Ó©Ñ‡ĞµĞ½ Ó©Ñ‡ĞµĞ½Ñ‡Ğµ Ó©Ñ‡ĞµĞ½Ñ‡ĞµĞ³Ó™ Ó©Ñ‡ĞµĞ½Ñ‡ĞµĞ´Ó™ Ó©Ñ‡ĞµĞ½Ñ‡ĞµĞ´Ó™Ğ½ Ó©Ñ‡ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€ Ó©Ñ‡ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ³Ó™\nÓ©Ñ‡ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ´Ó™ Ó©Ñ‡ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ´Ó™Ğ½ Ó©Ñ‡ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ½Ğµ Ó©Ñ‡ĞµĞ½Ñ‡ĞµĞ»Ó™Ñ€Ğ½ĞµÒ£ Ó©Ñ‡ĞµĞ½Ñ‡ĞµĞ½Ğµ Ó©Ñ‡ĞµĞ½Ñ‡ĞµĞ½ĞµÒ£ Ó©Ñ‡Ğ»Ó™Ğ¿\nÓ©Ñ‡Ó™Ñ€Ğ»Ó™Ğ¿'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/tt/punctuation.py----------------------------------------
A:spacy.lang.tt.punctuation._hyphens_no_dash->char_classes.HYPHENS.replace('-', '').strip('|').replace('||', '')


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/sv/__init__.py----------------------------------------
A:spacy.lang.sv.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.sv.__init__.lex_attr_getters[NORM]->add_lookups(Language.Defaults.lex_attr_getters[NORM], BASE_NORMS)
A:spacy.lang.sv.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS, TOKENIZER_EXCEPTIONS)
spacy.lang.sv.__init__.Swedish(Language)
spacy.lang.sv.__init__.SwedishDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/sv/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/sv/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.sv.tokenizer_exceptions.verb_data_tc->dict(verb_data)
A:spacy.lang.sv.tokenizer_exceptions.verb_data_tc[ORTH]->verb_data_tc[ORTH].title().title()
A:spacy.lang.sv.tokenizer_exceptions.capitalized->orth.capitalize()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/sv/morph_rules.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/sv/stop_words.py----------------------------------------
A:spacy.lang.sv.stop_words.STOP_WORDS->set('\naderton adertonde adjÃ¶ aldrig alla allas allt alltid alltsÃ¥ Ã¤n andra andras\nannan annat Ã¤nnu artonde arton Ã¥tminstone att Ã¥tta Ã¥ttio Ã¥ttionde Ã¥ttonde av\nÃ¤ven\n\nbÃ¥da bÃ¥das bakom bara bÃ¤st bÃ¤ttre behÃ¶va behÃ¶vas behÃ¶vde behÃ¶vt beslut beslutat\nbeslutit bland blev bli blir blivit bort borta bra\n\ndÃ¥ dag dagar dagarna dagen dÃ¤r dÃ¤rfÃ¶r de del delen dem den deras dess det detta\ndig din dina dit ditt dock du\n\nefter eftersom elfte eller elva en enkel enkelt enkla enligt er era ert ett\nettusen\n\nfÃ¥ fanns fÃ¥r fÃ¥tt fem femte femtio femtionde femton femtonde fick fin finnas\nfinns fjÃ¤rde fjorton fjortonde fler flera flesta fÃ¶ljande fÃ¶r fÃ¶re fÃ¶rlÃ¥t fÃ¶rra\nfÃ¶rsta fram framfÃ¶r frÃ¥n fyra fyrtio fyrtionde\n\ngÃ¥ gÃ¤lla gÃ¤ller gÃ¤llt gÃ¥r gÃ¤rna gÃ¥tt genast genom gick gjorde gjort god goda\ngodare godast gÃ¶r gÃ¶ra gott\n\nha hade haft han hans har hÃ¤r heller hellre helst helt henne hennes hit hÃ¶g\nhÃ¶ger hÃ¶gre hÃ¶gst hon honom hundra hundraen hundraett hur\n\ni ibland idag igÃ¥r igen imorgon in infÃ¶r inga ingen ingenting inget innan inne\ninom inte inuti\n\nja jag jÃ¤mfÃ¶rt\n\nkan kanske knappast kom komma kommer kommit kr kunde kunna kunnat kvar\n\nlÃ¤nge lÃ¤ngre lÃ¥ngsam lÃ¥ngsammare lÃ¥ngsammast lÃ¥ngsamt lÃ¤ngst lÃ¥ngt lÃ¤tt lÃ¤ttare\nlÃ¤ttast legat ligga ligger lika likstÃ¤lld likstÃ¤llda lilla lite liten litet\n\nman mÃ¥nga mÃ¥ste med mellan men mer mera mest mig min mina mindre minst mitt\nmittemot mÃ¶jlig mÃ¶jligen mÃ¶jligt mÃ¶jligtvis mot mycket\n\nnÃ¥gon nÃ¥gonting nÃ¥got nÃ¥gra nÃ¤r nÃ¤sta ned nederst nedersta nedre nej ner ni nio\nnionde nittio nittionde nitton nittonde nÃ¶dvÃ¤ndig nÃ¶dvÃ¤ndiga nÃ¶dvÃ¤ndigt\nnÃ¶dvÃ¤ndigtvis nog noll nr nu nummer\n\noch ocksÃ¥ ofta oftast olika olikt om oss\n\nÃ¶ver Ã¶vermorgon Ã¶verst Ã¶vre\n\npÃ¥\n\nrakt rÃ¤tt redan\n\nsÃ¥ sade sÃ¤ga sÃ¤ger sagt samma sÃ¤mre sÃ¤mst sedan senare senast sent sex sextio\nsextionde sexton sextonde sig sin sina sist sista siste sitt sjÃ¤tte sju sjunde\nsjuttio sjuttionde sjutton sjuttonde ska skall skulle slutligen smÃ¥ smÃ¥tt snart\nsom stor stora stÃ¶rre stÃ¶rst stort\n\ntack tidig tidigare tidigast tidigt till tills tillsammans tio tionde tjugo\ntjugoen tjugoett tjugonde tjugotre tjugotvÃ¥ tjungo tolfte tolv tre tredje\ntrettio trettionde tretton trettonde tvÃ¥ tvÃ¥hundra\n\nunder upp ur ursÃ¤kt ut utan utanfÃ¶r ute\n\nvad vÃ¤nster vÃ¤nstra var vÃ¥r vara vÃ¥ra varfÃ¶r varifrÃ¥n varit varken vÃ¤rre\nvarsÃ¥god vart vÃ¥rt vem vems verkligen vi vid vidare viktig viktigare viktigast\nviktigt vilka vilken vilket vill\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/sv/tag_map.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/sl/__init__.py----------------------------------------
A:spacy.lang.sl.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
spacy.lang.sl.__init__.Slovenian(Language)
spacy.lang.sl.__init__.SlovenianDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/sl/stop_words.py----------------------------------------
A:spacy.lang.sl.stop_words.STOP_WORDS->set('\na\nali\napril\navgust\nb\nbi\nbil\nbila\nbile\nbili\nbilo\nbiti\nblizu\nbo\nbodo\nbojo\nbolj\nbom\nbomo\nboste\nbova\nboÅ¡\nbrez\nc\ncel\ncela\nceli\ncelo\nd\nda\ndaleÄ\ndan\ndanes\ndatum\ndecember\ndeset\ndeseta\ndeseti\ndeseto\ndevet\ndeveta\ndeveti\ndeveto\ndo\ndober\ndobra\ndobri\ndobro\ndokler\ndol\ndolg\ndolga\ndolgi\ndovolj\ndrug\ndruga\ndrugi\ndrugo\ndva\ndve\ne\neden\nen\nena\nene\neni\nenkrat\neno\netc.\nf\nfebruar\ng\ng.\nga\nga.\ngor\ngospa\ngospod\nh\nhalo\ni\nidr.\nii\niii\nin\niv\nix\niz\nj\njanuar\njaz\nje\nji\njih\njim\njo\njulij\njunij\njutri\nk\nkadarkoli\nkaj\nkajti\nkako\nkakor\nkamor\nkamorkoli\nkar\nkarkoli\nkaterikoli\nkdaj\nkdo\nkdorkoli\nker\nki\nkje\nkjer\nkjerkoli\nko\nkoder\nkoderkoli\nkoga\nkomu\nkot\nkratek\nkratka\nkratke\nkratki\nl\nlahka\nlahke\nlahki\nlahko\nle\nlep\nlepa\nlepe\nlepi\nlepo\nleto\nm\nmaj\nmajhen\nmajhna\nmajhni\nmalce\nmalo\nmanj\nmarec\nme\nmed\nmedtem\nmene\nmesec\nmi\nmidva\nmidve\nmnogo\nmoj\nmoja\nmoje\nmora\nmorajo\nmoram\nmoramo\nmorate\nmoraÅ¡\nmorem\nmu\nn\nna\nnad\nnaj\nnajina\nnajino\nnajmanj\nnaju\nnajveÄ\nnam\nnarobe\nnas\nnato\nnazaj\nnaÅ¡\nnaÅ¡a\nnaÅ¡e\nne\nnedavno\nnedelja\nnek\nneka\nnekaj\nnekatere\nnekateri\nnekatero\nnekdo\nneke\nnekega\nneki\nnekje\nneko\nnekoga\nnekoÄ\nni\nnikamor\nnikdar\nnikjer\nnikoli\nniÄ\nnje\nnjega\nnjegov\nnjegova\nnjegovo\nnjej\nnjemu\nnjen\nnjena\nnjeno\nnji\nnjih\nnjihov\nnjihova\nnjihovo\nnjiju\nnjim\nnjo\nnjun\nnjuna\nnjuno\nno\nnocoj\nnovember\nnpr.\no\nob\noba\nobe\noboje\nod\nodprt\nodprta\nodprti\nokoli\noktober\non\nonadva\none\noni\nonidve\nosem\nosma\nosmi\nosmo\noz.\np\npa\npet\npeta\npetek\npeti\npeto\npo\npod\npogosto\npoleg\npoln\npolna\npolni\npolno\nponavadi\nponedeljek\nponovno\npotem\npovsod\npozdravljen\npozdravljeni\nprav\nprava\nprave\npravi\npravo\nprazen\nprazna\nprazno\nprbl.\nprecej\npred\nprej\npreko\npri\npribl.\npribliÅ¾no\nprimer\npripravljen\npripravljena\npripravljeni\nproti\nprva\nprvi\nprvo\nr\nravno\nredko\nres\nreÄ\ns\nsaj\nsam\nsama\nsame\nsami\nsamo\nse\nsebe\nsebi\nsedaj\nsedem\nsedma\nsedmi\nsedmo\nsem\nseptember\nseveda\nsi\nsicer\nskoraj\nskozi\nslab\nsmo\nso\nsobota\nspet\nsreda\nsrednja\nsrednji\nsta\nste\nstran\nstvar\nsva\nt\nta\ntak\ntaka\ntake\ntaki\ntako\ntakoj\ntam\nte\ntebe\ntebi\ntega\nteÅ¾ak\nteÅ¾ka\nteÅ¾ki\nteÅ¾ko\nti\ntista\ntiste\ntisti\ntisto\ntj.\ntja\nto\ntoda\ntorek\ntretja\ntretje\ntretji\ntri\ntu\ntudi\ntukaj\ntvoj\ntvoja\ntvoje\nu\nv\nvaju\nvam\nvas\nvaÅ¡\nvaÅ¡a\nvaÅ¡e\nve\nvedno\nvelik\nvelika\nveliki\nveliko\nvendar\nves\nveÄ\nvi\nvidva\nvii\nviii\nvisok\nvisoka\nvisoke\nvisoki\nvsa\nvsaj\nvsak\nvsaka\nvsakdo\nvsake\nvsaki\nvsakomur\nvse\nvsega\nvsi\nvso\nvÄasih\nvÄeraj\nx\nz\nza\nzadaj\nzadnji\nzakaj\nzaprta\nzaprti\nzaprto\nzdaj\nzelo\nzunaj\nÄ\nÄe\nÄesto\nÄetrta\nÄetrtek\nÄetrti\nÄetrto\nÄez\nÄigav\nÅ¡\nÅ¡est\nÅ¡esta\nÅ¡esti\nÅ¡esto\nÅ¡tiri\nÅ¾\nÅ¾e\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/cs/__init__.py----------------------------------------
A:spacy.lang.cs.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
spacy.lang.cs.__init__.Czech(Language)
spacy.lang.cs.__init__.CzechDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/cs/stop_words.py----------------------------------------
A:spacy.lang.cs.stop_words.STOP_WORDS->set("\naÄkoli\nahoj\nale\nanebo\nano\nasi\naspoÅˆ\nbÄ›hem\nbez\nbeze\nblÃ­zko\nbohuÅ¾el\nbrzo\nbude\nbudeme\nbudeÅ¡\nbudete\nbudou\nbudu\nbyl\nbyla\nbyli\nbylo\nbyly\nbys\nÄau\nchce\nchceme\nchceÅ¡\nchcete\nchci\nchtÄ›jÃ­\nchtÃ­t\nchut'\nchuti\nco\nÄtrnÃ¡ct\nÄtyÅ™i\ndÃ¡l\ndÃ¡le\ndaleko\ndÄ›kovat\ndÄ›kujeme\ndÄ›kuji\nden\ndeset\ndevatenÃ¡ct\ndevÄ›t\ndo\ndobrÃ½\ndocela\ndva\ndvacet\ndvanÃ¡ct\ndvÄ›\nhodnÄ›\njÃ¡\njak\njde\nje\njeden\njedenÃ¡ct\njedna\njedno\njednou\njedou\njeho\njejÃ­\njejich\njemu\njen\njenom\njeÅ¡tÄ›\njestli\njestliÅ¾e\njÃ­\njich\njÃ­m\njimi\njinak\njsem\njsi\njsme\njsou\njste\nkam\nkde\nkdo\nkdy\nkdyÅ¾\nke\nkolik\nkromÄ›\nkterÃ¡\nkterÃ©\nkteÅ™Ã­\nkterÃ½\nkvÅ¯li\nmÃ¡\nmajÃ­\nmÃ¡lo\nmÃ¡m\nmÃ¡me\nmÃ¡Å¡\nmÃ¡te\nmÃ©\nmÄ›\nmezi\nmÃ­\nmÃ­t\nmnÄ›\nmnou\nmoc\nmohl\nmohou\nmoje\nmoji\nmoÅ¾nÃ¡\nmÅ¯j\nmusÃ­\nmÅ¯Å¾e\nmy\nna\nnad\nnade\nnÃ¡m\nnÃ¡mi\nnaproti\nnÃ¡s\nnÃ¡Å¡\nnaÅ¡e\nnaÅ¡i\nne\nnÄ›\nnebo\nnebyl\nnebyla\nnebyli\nnebyly\nnÄ›co\nnedÄ›lÃ¡\nnedÄ›lajÃ­\nnedÄ›lÃ¡m\nnedÄ›lÃ¡me\nnedÄ›lÃ¡Å¡\nnedÄ›lÃ¡te\nnÄ›jak\nnejsi\nnÄ›kde\nnÄ›kdo\nnemajÃ­\nnemÃ¡me\nnemÃ¡te\nnemÄ›l\nnÄ›mu\nnenÃ­\nnestaÄÃ­\nnevadÃ­\nneÅ¾\nnic\nnich\nnÃ­m\nnimi\nnula\nod\node\non\nona\noni\nono\nony\nosm\nosmnÃ¡ct\npak\npatnÃ¡ct\npÄ›t\npo\npoÅ™Ã¡d\npotom\npozdÄ›\npÅ™ed\npÅ™es\npÅ™ese\npro\nproÄ\nprosÃ­m\nprostÄ›\nproti\nprotoÅ¾e\nrovnÄ›\nse\nsedm\nsedmnÃ¡ct\nÅ¡est\nÅ¡estnÃ¡ct\nskoro\nsmÄ›jÃ­\nsmÃ­\nsnad\nspolu\nsta\nstÃ©\nsto\nta\ntady\ntak\ntakhle\ntaky\ntam\ntamhle\ntamhleto\ntamto\ntÄ›\ntebe\ntebou\nted'\ntedy\nten\nti\ntisÃ­c\ntisÃ­ce\nto\ntobÄ›\ntohle\ntoto\ntÅ™eba\ntÅ™i\ntÅ™inÃ¡ct\ntroÅ¡ku\ntvÃ¡\ntvÃ©\ntvoje\ntvÅ¯j\nty\nurÄitÄ›\nuÅ¾\nvÃ¡m\nvÃ¡mi\nvÃ¡s\nvÃ¡Å¡\nvaÅ¡e\nvaÅ¡i\nve\nveÄer\nvedle\nvlastnÄ›\nvÅ¡echno\nvÅ¡ichni\nvÅ¯bec\nvy\nvÅ¾dy\nza\nzaÄ\nzatÃ­mco\nze\nÅ¾e\n".split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/sk/__init__.py----------------------------------------
A:spacy.lang.sk.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
spacy.lang.sk.__init__.Slovak(Language)
spacy.lang.sk.__init__.SlovakDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/sk/stop_words.py----------------------------------------
A:spacy.lang.sk.stop_words.STOP_WORDS->set('\na\naby\naj\nak\nako\nakÃ½\nale\nalebo\nand\nani\nasi\navÅ¡ak\naÅ¾\nba\nbez\nbol\nbola\nboli\nbolo\nbude\nbudem\nbudeme\nbudete\nbudeÅ¡\nbudÃº\nbuÃ¯\nbuÄ\nby\nbyÅ¥\ncez\ndnes\ndo\neÅ¡te\nfor\nho\nhoci\ni\niba\nich\nim\ninÃ©\ninÃ½\nja\nje\njeho\njej\njemu\nju\nk\nkam\nkaÅ¾dÃ¡\nkaÅ¾dÃ©\nkaÅ¾dÃ­\nkaÅ¾dÃ½\nkde\nkedÅ¾e\nkeÃ¯\nkeÄ\nkto\nktorou\nktorÃ¡\nktorÃ©\nktorÃ­\nktorÃ½\nku\nlebo\nlen\nma\nmaÅ¥\nmedzi\nmenej\nmi\nmna\nmne\nmnou\nmoja\nmoje\nmu\nmusieÅ¥\nmy\nmÃ¡\nmÃ¡te\nmÃ²a\nmÃ´cÅ¥\nmÃ´j\nmÃ´Å¾e\nna\nnad\nnami\nnaÅ¡i\nnech\nneho\nnej\nnemu\nneÅ¾\nnich\nnie\nniektorÃ½\nnielen\nnim\nniÄ\nno\nnovÃ¡\nnovÃ©\nnovÃ­\nnovÃ½\nnÃ¡m\nnÃ¡s\nnÃ¡Å¡\nnÃ­m\no\nod\nodo\nof\non\nona\noni\nono\nony\npo\npod\npodÄ¾a\npokiaÄ¾\npotom\npre\npred\npredo\npreto\npretoÅ¾e\npreÄo\npri\nprvÃ¡\nprvÃ©\nprvÃ­\nprvÃ½\nprÃ¡ve\npÃ½ta\ns\nsa\nseba\nsem\nsi\nsme\nso\nsom\nspÃ¤Å¥\nste\nsvoj\nsvoje\nsvojich\nsvojÃ­m\nsvojÃ­mi\nsÃº\nta\ntak\ntakÃ½\ntakÅ¾e\ntam\nte\nteba\ntebe\ntebou\nteda\ntej\nten\ntento\nthe\nti\ntie\ntieto\ntieÅ¾\nto\ntoho\ntohoto\ntom\ntomto\ntomu\ntomuto\ntoto\ntou\ntu\ntvoj\ntvojÃ­mi\nty\ntÃ¡\ntÃ¡to\ntÃº\ntÃºto\ntÃ½m\ntÃ½mto\ntÄ›\nuÅ¾\nv\nvami\nvaÅ¡e\nveÃ¯\nviac\nvo\nvy\nvÃ¡m\nvÃ¡s\nvÃ¡Å¡\nvÅ¡ak\nvÅ¡etok\nz\nza\nzo\n\x9da\nÃ¡no\nÃ¨i\nÃ¨o\nÃ¨Ã­\nÃ²om\nÃ²ou\nÃ²u\nÄi\nÄo\nÄalÅ¡ia\nÄalÅ¡ie\nÄalÅ¡Ã­\nÅ¾e\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/id/__init__.py----------------------------------------
A:spacy.lang.id.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.id.__init__.lex_attr_getters[NORM]->add_lookups(Language.Defaults.lex_attr_getters[NORM], BASE_NORMS, NORM_EXCEPTIONS)
A:spacy.lang.id.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS, TOKENIZER_EXCEPTIONS)
spacy.lang.id.__init__.Indonesian(Language)
spacy.lang.id.__init__.IndonesianDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/id/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/id/lex_attrs.py----------------------------------------
A:spacy.lang.id.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.id.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
A:spacy.lang.id.lex_attrs.(_, num)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('-')
spacy.lang.id.lex_attrs.is_currency(text)
spacy.lang.id.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/id/_tokenizer_exceptions_list.py----------------------------------------
A:spacy.lang.id._tokenizer_exceptions_list.ID_BASE_EXCEPTIONS->set('\naba-aba\nabah-abah\nabal-abal\nabang-abang\nabar-abar\nabong-abong\nabrit-abrit\nabrit-abritan\nabu-abu\nabuh-abuhan\nabuk-abuk\nabun-abun\nacak-acak\nacak-acakan\nacang-acang\nacap-acap\naci-aci\naci-acian\naci-acinya\naco-acoan\nad-blocker\nad-interim\nada-ada\nada-adanya\nada-adanyakah\nadang-adang\nadap-adapan\nadd-on\nadd-ons\nadik-adik\nadik-beradik\naduk-adukan\nafter-sales\nagak-agak\nagak-agih\nagama-agama\nagar-agar\nage-related\nagut-agut\nair-air\nair-cooled\nair-to-air\najak-ajak\najar-ajar\naji-aji\nakal-akal\nakal-akalan\nakan-akan\nakar-akar\nakar-akaran\nakhir-akhir\nakhir-akhirnya\naki-aki\naksi-aksi\nalah-mengalahi\nalai-belai\nalan-alan\nalang-alang\nalang-alangan\nalap-alap\nalat-alat\nali-ali\nalif-alifan\nalih-alih\naling-aling\naling-alingan\nalip-alipan\nall-electric\nall-in-one\nall-out\nall-time\nalon-alon\nalt-right\nalt-text\nalu-alu\nalu-aluan\nalun-alun\nalur-alur\nalur-aluran\nalways-on\namai-amai\namatir-amatiran\nambah-ambah\nambai-ambai\nambil-mengambil\nambreng-ambrengan\nambring-ambringan\nambu-ambu\nambung-ambung\namin-amin\namit-amit\nampai-ampai\namprung-amprungan\namung-amung\nanai-anai\nanak-anak\nanak-anakan\nanak-beranak\nanak-cucu\nanak-istri\nancak-ancak\nancang-ancang\nancar-ancar\nandang-andang\nandeng-andeng\naneh-aneh\nangan-angan\nanggar-anggar\nanggaran-red\nanggota-anggota\nanggung-anggip\nangin-angin\nangin-anginan\nangkal-angkal\nangkul-angkul\nangkup-angkup\nangkut-angkut\nani-ani\naning-aning\nanjang-anjang\nanjing-anjing\nanjung-anjung\nanjung-anjungan\nantah-berantah\nantar-antar\nantar-mengantar\nante-mortem\nantek-antek\nanter-anter\nantihuru-hara\nanting-anting\nantung-antung\nanyam-menganyam\nanyang-anyang\napa-apa\napa-apaan\napel-apel\napi-api\napit-apit\naplikasi-aplikasi\napotek-apotek\naprit-apritan\napu-apu\napung-apung\narah-arah\narak-arak\narak-arakan\naram-aram\narek-arek\narem-arem\nari-ari\nartis-artis\naru-aru\narung-arungan\nasa-asaan\nasal-asalan\nasal-muasal\nasal-usul\nasam-asaman\nasas-asas\naset-aset\nasmaul-husna\nasosiasi-asosiasi\nasuh-asuh\nasyik-asyiknya\natas-mengatasi\nati-ati\natung-atung\naturan-aturan\naudio-video\naudio-visual\nauto-brightness\nauto-complete\nauto-focus\nauto-play\nauto-update\navant-garde\nawan-awan\nawan-berawan\nawang-awang\nawang-gemawang\nawar-awar\nawat-awat\nawik-awik\nawut-awutan\nayah-anak\nayak-ayak\nayam-ayam\nayam-ayaman\nayang-ayang\nayat-ayat\nayeng-ayengan\nayun-temayun\nayut-ayutan\nba-bi-bu\nback-to-back\nback-up\nbadan-badan\nbade-bade\nbadut-badut\nbagi-bagi\nbahan-bahan\nbahu-membahu\nbaik-baik\nbail-out\nbajang-bajang\nbaji-baji\nbalai-balai\nbalam-balam\nbalas-berbalas\nbalas-membalas\nbale-bale\nbaling-baling\nball-playing\nbalon-balon\nbalut-balut\nband-band\nbandara-bandara\nbangsa-bangsa\nbangun-bangun\nbangunan-bangunan\nbank-bank\nbantah-bantah\nbantahan-bantahan\nbantal-bantal\nbanyak-banyak\nbapak-anak\nbapak-bapak\nbapak-ibu\nbapak-ibunya\nbarang-barang\nbarat-barat\nbarat-daya\nbarat-laut\nbarau-barau\nbare-bare\nbareng-bareng\nbari-bari\nbarik-barik\nbaris-berbaris\nbaru-baru\nbaru-batu\nbarung-barung\nbasa-basi\nbata-bata\nbatalyon-batalyon\nbatang-batang\nbatas-batas\nbatir-batir\nbatu-batu\nbatuk-batuk\nbatung-batung\nbau-bauan\nbawa-bawa\nbayan-bayan\nbayang-bayang\nbayi-bayi\nbea-cukai\nbedeng-bedeng\nbedil-bedal\nbedil-bedilan\nbegana-begini\nbek-bek\nbekal-bekalan\nbekerdom-kerdom\nbekertak-kertak\nbelang-belang\nbelat-belit\nbeliau-beliau\nbelu-belai\nbelum-belum\nbenar-benar\nbenda-benda\nbengang-bengut\nbenggal-benggil\nbengkal-bengkil\nbengkang-bengkok\nbengkang-bengkong\nbengkang-bengkung\nbenteng-benteng\nbentuk-bentuk\nbenua-benua\nber-selfie\nberabad-abad\nberabun-rabun\nberacah-acah\nberada-ada\nberadik-berkakak\nberagah-agah\nberagak-agak\nberagam-ragam\nberaja-raja\nberakit-rakit\nberaku-akuan\nberalu-aluan\nberalun-alun\nberamah-ramah\nberamah-ramahan\nberamah-tamah\nberamai-ramai\nberambai-ambai\nberambal-ambalan\nberambil-ambil\nberamuk-amuk\nberamuk-amukan\nberandai-andai\nberandai-randai\nberaneh-aneh\nberang-berang\nberangan-angan\nberanggap-anggapan\nberangguk-angguk\nberangin-angin\nberangka-angka\nberangka-angkaan\nberangkai-rangkai\nberangkap-rangkapan\nberani-berani\nberanja-anja\nberantai-rantai\nberapi-api\nberapung-apung\nberarak-arakan\nberas-beras\nberasak-asak\nberasak-asakan\nberasap-asap\nberasing-asingan\nberatus-ratus\nberawa-rawa\nberawas-awas\nberayal-ayalan\nberayun-ayun\nberbagai-bagai\nberbahas-bahasan\nberbahasa-bahasa\nberbaik-baikan\nberbait-bait\nberbala-bala\nberbalas-balasan\nberbalik-balik\nberbalun-balun\nberbanjar-banjar\nberbantah-bantah\nberbanyak-banyak\nberbarik-barik\nberbasa-basi\nberbasah-basah\nberbatu-batu\nberbayang-bayang\nberbecak-becak\nberbeda-beda\nberbedil-bedilan\nberbega-bega\nberbeka-beka\nberbelah-belah\nberbelakang-belakangan\nberbelang-belang\nberbelau-belauan\nberbeli-beli\nberbeli-belian\nberbelit-belit\nberbelok-belok\nberbenang-benang\nberbenar-benar\nberbencah-bencah\nberbencol-bencol\nberbenggil-benggil\nberbentol-bentol\nberbentong-bentong\nberberani-berani\nberbesar-besar\nberbidai-bidai\nberbiduk-biduk\nberbiku-biku\nberbilik-bilik\nberbinar-binar\nberbincang-bincang\nberbingkah-bingkah\nberbintang-bintang\nberbintik-bintik\nberbintil-bintil\nberbisik-bisik\nberbolak-balik\nberbolong-bolong\nberbondong-bondong\nberbongkah-bongkah\nberbuai-buai\nberbual-bual\nberbudak-budak\nberbukit-bukit\nberbulan-bulan\nberbunga-bunga\nberbuntut-buntut\nberbunuh-bunuhan\nberburu-buru\nberburuk-buruk\nberbutir-butir\nbercabang-cabang\nbercaci-cacian\nbercakap-cakap\nbercakar-cakaran\nbercamping-camping\nbercantik-cantik\nbercari-cari\nbercari-carian\nbercarik-carik\nbercarut-carut\nbercebar-cebur\nbercepat-cepat\nbercerai-berai\nbercerai-cerai\nbercetai-cetai\nberciap-ciap\nbercikun-cikun\nbercinta-cintaan\nbercita-cita\nberciut-ciut\nbercompang-camping\nberconteng-conteng\nbercoreng-coreng\nbercoreng-moreng\nbercuang-caing\nbercuit-cuit\nbercumbu-cumbu\nbercumbu-cumbuan\nbercura-bura\nbercura-cura\nberdada-dadaan\nberdahulu-dahuluan\nberdalam-dalam\nberdalih-dalih\nberdampung-dampung\nberdebar-debar\nberdecak-decak\nberdecap-decap\nberdecup-decup\nberdecut-decut\nberdedai-dedai\nberdegap-degap\nberdegar-degar\nberdeham-deham\nberdekah-dekah\nberdekak-dekak\nberdekap-dekapan\nberdekat-dekat\nberdelat-delat\nberdembai-dembai\nberdembun-dembun\nberdempang-dempang\nberdempet-dempet\nberdencing-dencing\nberdendam-dendaman\nberdengkang-dengkang\nberdengut-dengut\nberdentang-dentang\nberdentum-dentum\nberdentung-dentung\nberdenyar-denyar\nberdenyut-denyut\nberdepak-depak\nberdepan-depan\nberderai-derai\nberderak-derak\nberderam-deram\nberderau-derau\nberderik-derik\nberdering-dering\nberderung-derung\nberderus-derus\nberdesak-desakan\nberdesik-desik\nberdesing-desing\nberdesus-desus\nberdikit-dikit\nberdingkit-dingkit\nberdua-dua\nberduri-duri\nberduru-duru\nberduyun-duyun\nberebut-rebut\nberebut-rebutan\nberegang-regang\nberek-berek\nberembut-rembut\nberempat-empat\nberenak-enak\nberencel-encel\nbereng-bereng\nberenggan-enggan\nberenteng-renteng\nberesa-esaan\nberesah-resah\nberfoya-foya\nbergagah-gagahan\nbergagap-gagap\nbergagau-gagau\nbergalur-galur\nberganda-ganda\nberganjur-ganjur\nberganti-ganti\nbergarah-garah\nbergaruk-garuk\nbergaya-gaya\nbergegas-gegas\nbergelang-gelang\nbergelap-gelap\nbergelas-gelasan\nbergeleng-geleng\nbergemal-gemal\nbergembar-gembor\nbergembut-gembut\nbergepok-gepok\nbergerek-gerek\nbergesa-gesa\nbergilir-gilir\nbergolak-golak\nbergolek-golek\nbergolong-golong\nbergores-gores\nbergotong-royong\nbergoyang-goyang\nbergugus-gugus\nbergulung-gulung\nbergulut-gulut\nbergumpal-gumpal\nbergunduk-gunduk\nbergunung-gunung\nberhadap-hadapan\nberhamun-hamun\nberhandai-handai\nberhanyut-hanyut\nberhari-hari\nberhati-hati\nberhati-hatilah\nberhektare-hektare\nberhilau-hilau\nberhormat-hormat\nberhujan-hujan\nberhura-hura\nberi-beri\nberi-memberi\nberia-ia\nberia-ria\nberiak-riak\nberiba-iba\nberibu-ribu\nberigi-rigi\nberimpit-impit\nberindap-indap\nbering-bering\nberingat-ingat\nberinggit-ringgit\nberintik-rintik\nberiring-iring\nberiring-iringan\nberita-berita\nberjabir-jabir\nberjaga-jaga\nberjagung-jagung\nberjalan-jalan\nberjalar-jalar\nberjalin-jalin\nberjalur-jalur\nberjam-jam\nberjari-jari\nberjauh-jauhan\nberjegal-jegalan\nberjejal-jejal\nberjela-jela\nberjengkek-jengkek\nberjenis-jenis\nberjenjang-jenjang\nberjilid-jilid\nberjinak-jinak\nberjingkat-jingkat\nberjingkik-jingkik\nberjingkrak-jingkrak\nberjongkok-jongkok\nberjubel-jubel\nberjujut-jujutan\nberjulai-julai\nberjumbai-jumbai\nberjumbul-jumbul\nberjuntai-juntai\nberjurai-jurai\nberjurus-jurus\nberjuta-juta\nberka-li-kali\nberkabu-kabu\nberkaca-kaca\nberkaing-kaing\nberkait-kaitan\nberkala-kala\nberkali-kali\nberkamit-kamit\nberkanjar-kanjar\nberkaok-kaok\nberkarung-karung\nberkasak-kusuk\nberkasih-kasihan\nberkata-kata\nberkatak-katak\nberkecai-kecai\nberkecek-kecek\nberkecil-kecil\nberkecil-kecilan\nberkedip-kedip\nberkejang-kejang\nberkejap-kejap\nberkejar-kejaran\nberkelar-kelar\nberkelepai-kelepai\nberkelip-kelip\nberkelit-kelit\nberkelok-kelok\nberkelompok-kelompok\nberkelun-kelun\nberkembur-kembur\nberkempul-kempul\nberkena-kenaan\nberkenal-kenalan\nberkendur-kendur\nberkeok-keok\nberkepak-kepak\nberkepal-kepal\nberkeping-keping\nberkepul-kepul\nberkeras-kerasan\nberkering-kering\nberkeritik-keritik\nberkeruit-keruit\nberkerut-kerut\nberketai-ketai\nberketak-ketak\nberketak-ketik\nberketap-ketap\nberketap-ketip\nberketar-ketar\nberketi-keti\nberketil-ketil\nberketuk-ketak\nberketul-ketul\nberkial-kial\nberkian-kian\nberkias-kias\nberkias-kiasan\nberkibar-kibar\nberkilah-kilah\nberkilap-kilap\nberkilat-kilat\nberkilau-kilauan\nberkilo-kilo\nberkimbang-kimbang\nberkinja-kinja\nberkipas-kipas\nberkira-kira\nberkirim-kiriman\nberkisar-kisar\nberkoak-koak\nberkoar-koar\nberkobar-kobar\nberkobok-kobok\nberkocak-kocak\nberkodi-kodi\nberkolek-kolek\nberkomat-kamit\nberkopah-kopah\nberkoper-koper\nberkotak-kotak\nberkuat-kuat\nberkuat-kuatan\nberkumur-kumur\nberkunang-kunang\nberkunar-kunar\nberkunjung-kunjungan\nberkurik-kurik\nberkurun-kurun\nberkusau-kusau\nberkusu-kusu\nberkusut-kusut\nberkuting-kuting\nberkutu-kutuan\nberlabun-labun\nberlain-lainan\nberlaju-laju\nberlalai-lalai\nberlama-lama\nberlambai-lambai\nberlambak-lambak\nberlampang-lampang\nberlanggar-langgar\nberlapang-lapang\nberlapis-lapis\nberlapuk-lapuk\nberlarah-larah\nberlarat-larat\nberlari-lari\nberlari-larian\nberlarih-larih\nberlarik-larik\nberlarut-larut\nberlawak-lawak\nberlayap-layapan\nberlebih-lebih\nberlebih-lebihan\nberleha-leha\nberlekas-lekas\nberlekas-lekasan\nberlekat-lekat\nberlekuk-lekuk\nberlempar-lemparan\nberlena-lena\nberlengah-lengah\nberlenggak-lenggok\nberlenggek-lenggek\nberlenggok-lenggok\nberleret-leret\nberletih-letih\nberliang-liuk\nberlibat-libat\nberligar-ligar\nberliku-liku\nberlikur-likur\nberlimbak-limbak\nberlimpah-limpah\nberlimpap-limpap\nberlimpit-limpit\nberlinang-linang\nberlindak-lindak\nberlipat-lipat\nberlomba-lomba\nberlompok-lompok\nberloncat-loncatan\nberlopak-lopak\nberlubang-lubang\nberlusin-lusin\nbermaaf-maafan\nbermabuk-mabukan\nbermacam-macam\nbermain-main\nbermalam-malam\nbermalas-malas\nbermalas-malasan\nbermanik-manik\nbermanis-manis\nbermanja-manja\nbermasak-masak\nbermati-mati\nbermegah-megah\nbermemek-memek\nbermenung-menung\nbermesra-mesraan\nbermewah-mewah\nbermewah-mewahan\nberminggu-minggu\nberminta-minta\nberminyak-minyak\nbermuda-muda\nbermudah-mudah\nbermuka-muka\nbermula-mula\nbermuluk-muluk\nbermulut-mulut\nbernafsi-nafsi\nbernaka-naka\nbernala-nala\nbernanti-nanti\nberniat-niat\nbernyala-nyala\nberogak-ogak\nberoleng-oleng\nberolok-olok\nberomong-omong\nberoncet-roncet\nberonggok-onggok\nberorang-orang\nberoyal-royal\nberpada-pada\nberpadu-padu\nberpahit-pahit\nberpair-pair\nberpal-pal\nberpalu-palu\nberpalu-paluan\nberpalun-palun\nberpanas-panas\nberpandai-pandai\nberpandang-pandangan\nberpangkat-pangkat\nberpanjang-panjang\nberpantun-pantun\nberpasang-pasang\nberpasang-pasangan\nberpasuk-pasuk\nberpayah-payah\nberpeluh-peluh\nberpeluk-pelukan\nberpenat-penat\nberpencar-pencar\nberpendar-pendar\nberpenggal-penggal\nberperai-perai\nberperang-perangan\nberpesai-pesai\nberpesta-pesta\nberpesuk-pesuk\nberpetak-petak\nberpeti-peti\nberpihak-pihak\nberpijar-pijar\nberpikir-pikir\nberpikul-pikul\nberpilih-pilih\nberpilin-pilin\nberpindah-pindah\nberpintal-pintal\nberpirau-pirau\nberpisah-pisah\nberpolah-polah\nberpolok-polok\nberpongah-pongah\nberpontang-panting\nberporah-porah\nberpotong-potong\nberpotong-potongan\nberpuak-puak\nberpual-pual\nberpugak-pugak\nberpuing-puing\nberpukas-pukas\nberpuluh-puluh\nberpulun-pulun\nberpuntal-puntal\nberpura-pura\nberpusar-pusar\nberpusing-pusing\nberpusu-pusu\nberputar-putar\nberrumpun-rumpun\nbersaf-saf\nbersahut-sahutan\nbersakit-sakit\nbersalah-salahan\nbersalam-salaman\nbersalin-salin\nbersalip-salipan\nbersama-sama\nbersambar-sambaran\nbersambut-sambutan\nbersampan-sampan\nbersantai-santai\nbersapa-sapaan\nbersarang-sarang\nbersedan-sedan\nbersedia-sedia\nbersedu-sedu\nbersejuk-sejuk\nbersekat-sekat\nberselang-selang\nberselang-seli\nberselang-seling\nberselang-tenggang\nberselit-selit\nberseluk-beluk\nbersembunyi-sembunyi\nbersembunyi-sembunyian\nbersembur-semburan\nbersempit-sempit\nbersenang-senang\nbersenang-senangkan\nbersenda-senda\nbersendi-sendi\nbersenggang-senggang\nbersenggau-senggau\nbersepah-sepah\nbersepak-sepakan\nbersepi-sepi\nberserak-serak\nberseri-seri\nberseru-seru\nbersesak-sesak\nbersetai-setai\nbersia-sia\nbersiap-siap\nbersiar-siar\nbersih-bersih\nbersikut-sikutan\nbersilir-silir\nbersimbur-simburan\nbersinau-sinau\nbersopan-sopan\nbersorak-sorai\nbersuap-suapan\nbersudah-sudah\nbersuka-suka\nbersuka-sukaan\nbersuku-suku\nbersulang-sulang\nbersumpah-sumpahan\nbersungguh-sungguh\nbersungut-sungut\nbersunyi-sunyi\nbersuruk-surukan\nbersusah-susah\nbersusuk-susuk\nbersusuk-susukan\nbersutan-sutan\nbertabur-tabur\nbertahan-tahan\nbertahu-tahu\nbertahun-tahun\nbertajuk-tajuk\nbertakik-takik\nbertala-tala\nbertalah-talah\nbertali-tali\nbertalu-talu\nbertalun-talun\nbertambah-tambah\nbertanda-tandaan\nbertangis-tangisan\nbertangkil-tangkil\nbertanya-tanya\nbertarik-tarikan\nbertatai-tatai\nbertatap-tatapan\nbertatih-tatih\nbertawan-tawan\nbertawar-tawaran\nbertebu-tebu\nbertebu-tebukan\nberteguh-teguh\nberteguh-teguhan\nberteka-teki\nbertelang-telang\nbertelau-telau\nbertele-tele\nbertembuk-tembuk\nbertempat-tempat\nbertempuh-tempuh\nbertenang-tenang\nbertenggang-tenggangan\nbertentu-tentu\nbertepek-tepek\nberterang-terang\nberterang-terangan\nberteriak-teriak\nbertikam-tikaman\nbertimbal-timbalan\nbertimbun-timbun\nbertimpa-timpa\nbertimpas-timpas\nbertingkah-tingkah\nbertingkat-tingkat\nbertinjau-tinjauan\nbertiras-tiras\nbertitar-titar\nbertitik-titik\nbertoboh-toboh\nbertolak-tolak\nbertolak-tolakan\nbertolong-tolongan\nbertonjol-tonjol\nbertruk-truk\nbertua-tua\nbertua-tuaan\nbertual-tual\nbertubi-tubi\nbertukar-tukar\nbertukar-tukaran\nbertukas-tukas\nbertumpak-tumpak\nbertumpang-tindih\nbertumpuk-tumpuk\nbertunda-tunda\nbertunjuk-tunjukan\nbertura-tura\nberturut-turut\nbertutur-tutur\nberuas-ruas\nberubah-ubah\nberulang-alik\nberulang-ulang\nberumbai-rumbai\nberundak-undak\nberundan-undan\nberundung-undung\nberunggas-runggas\nberunggun-unggun\nberunggut-unggut\nberungkur-ungkuran\nberuntai-untai\nberuntun-runtun\nberuntung-untung\nberunyai-unyai\nberupa-rupa\nberura-ura\nberuris-uris\nberurut-urutan\nberwarna-warna\nberwarna-warni\nberwindu-windu\nberwiru-wiru\nberyang-yang\nbesar-besar\nbesar-besaran\nbetak-betak\nbeti-beti\nbetik-betik\nbetul-betul\nbiang-biang\nbiar-biar\nbiaya-biaya\nbicu-bicu\nbidadari-bidadari\nbidang-bidang\nbijak-bijaklah\nbiji-bijian\nbila-bila\nbilang-bilang\nbincang-bincang\nbincang-bincut\nbingkah-bingkah\nbini-binian\nbintang-bintang\nbintik-bintik\nbio-oil\nbiri-biri\nbiru-biru\nbiru-hitam\nbiru-kuning\nbisik-bisik\nbiti-biti\nblak-blakan\nblok-blok\nbocah-bocah\nbohong-bohong\nbohong-bohongan\nbola-bola\nbolak-balik\nbolang-baling\nboleh-boleh\nbom-bom\nbomber-bomber\nbonek-bonek\nbongkar-bangkir\nbongkar-membongkar\nbongkar-pasang\nboro-boro\nbos-bos\nbottom-up\nbox-to-box\nboyo-boyo\nbuah-buahan\nbuang-buang\nbuat-buatan\nbuaya-buaya\nbubun-bubun\nbugi-bugi\nbuild-up\nbuilt-in\nbuilt-up\nbuka-buka\nbuka-bukaan\nbuka-tutup\nbukan-bukan\nbukti-bukti\nbuku-buku\nbulan-bulan\nbulan-bulanan\nbulang-baling\nbulang-bulang\nbulat-bulat\nbuli-buli\nbulu-bulu\nbuluh-buluh\nbulus-bulus\nbunga-bunga\nbunga-bungaan\nbunuh-membunuh\nbunyi-bunyian\nbupati-bupati\nbupati-wakil\nburu-buru\nburung-burung\nburung-burungan\nbus-bus\nbusiness-to-business\nbusur-busur\nbutir-butir\nby-pass\nbye-bye\ncabang-cabang\ncabik-cabik\ncabik-mencabik\ncabup-cawabup\ncaci-maki\ncagub-cawagub\ncaing-caing\ncakar-mencakar\ncakup-mencakup\ncalak-calak\ncalar-balar\ncaleg-caleg\ncalo-calo\ncalon-calon\ncampang-camping\ncampur-campur\ncapres-cawapres\ncara-cara\ncari-cari\ncari-carian\ncarut-marut\ncatch-up\ncawali-cawawali\ncawe-cawe\ncawi-cawi\ncebar-cebur\ncelah-celah\ncelam-celum\ncelangak-celinguk\ncelas-celus\nceledang-celedok\ncelengkak-celengkok\ncelingak-celinguk\ncelung-celung\ncemas-cemas\ncenal-cenil\ncengar-cengir\ncengir-cengir\ncengis-cengis\ncengking-mengking\ncentang-perenang\ncepat-cepat\nceplas-ceplos\ncerai-berai\ncerita-cerita\nceruk-menceruk\nceruk-meruk\ncetak-biru\ncetak-mencetak\ncetar-ceter\ncheck-in\ncheck-ins\ncheck-up\nchit-chat\nchoki-choki\ncingak-cinguk\ncipika-cipiki\nciri-ciri\nciri-cirinya\ncirit-birit\ncita-cita\ncita-citaku\nclose-up\nclosed-circuit\ncoba-coba\ncobak-cabik\ncobar-cabir\ncola-cala\ncolang-caling\ncomat-comot\ncomot-comot\ncompang-camping\ncomputer-aided\ncomputer-generated\ncondong-mondong\ncongak-cangit\nconggah-canggih\ncongkah-cangkih\ncongkah-mangkih\ncopak-capik\ncopy-paste\ncorak-carik\ncorat-coret\ncoreng-moreng\ncoret-coret\ncrat-crit\ncross-border\ncross-dressing\ncrypto-ransomware\ncuang-caing\ncublak-cublak\ncubung-cubung\nculik-culik\ncuma-cuma\ncumi-cumi\ncungap-cangip\ncupu-cupu\ndabu-dabu\ndaerah-daerah\ndag-dag\ndag-dig-dug\ndaging-dagingan\ndahulu-mendahului\ndalam-dalam\ndali-dali\ndam-dam\ndanau-danau\ndansa-dansi\ndapil-dapil\ndapur-dapur\ndari-dari\ndaru-daru\ndasar-dasar\ndatang-datang\ndatang-mendatangi\ndaun-daun\ndaun-daunan\ndawai-dawai\ndayang-dayang\ndayung-mayung\ndebak-debuk\ndebu-debu\ndeca-core\ndecision-making\ndeep-lying\ndeg-degan\ndegap-degap\ndekak-dekak\ndekat-dekat\ndengar-dengaran\ndengking-mendengking\ndepartemen-departemen\ndepo-depo\ndeputi-deputi\ndesa-desa\ndesa-kota\ndesas-desus\ndetik-detik\ndewa-dewa\ndewa-dewi\ndewan-dewan\ndewi-dewi\ndial-up\ndiam-diam\ndibayang-bayangi\ndibuat-buat\ndiiming-imingi\ndilebih-lebihkan\ndimana-mana\ndimata-matai\ndinas-dinas\ndinul-Islam\ndiobok-obok\ndiolok-olok\ndireksi-direksi\ndirektorat-direktorat\ndirjen-dirjen\ndirut-dirut\nditunggu-tunggu\ndivisi-divisi\ndo-it-yourself\ndoa-doa\ndog-dog\ndoggy-style\ndokok-dokok\ndolak-dalik\ndor-doran\ndorong-mendorong\ndosa-dosa\ndress-up\ndrive-in\ndua-dua\ndua-duaan\ndua-duanya\ndubes-dubes\nduduk-duduk\ndugaan-dugaan\ndulang-dulang\nduri-duri\nduta-duta\ndwi-kewarganegaraan\ne-arena\ne-billing\ne-budgeting\ne-cctv\ne-class\ne-commerce\ne-counting\ne-elektronik\ne-entertainment\ne-evolution\ne-faktur\ne-filing\ne-fin\ne-form\ne-government\ne-govt\ne-hakcipta\ne-id\ne-info\ne-katalog\ne-ktp\ne-leadership\ne-lhkpn\ne-library\ne-loket\ne-m1\ne-money\ne-news\ne-nisn\ne-npwp\ne-paspor\ne-paten\ne-pay\ne-perda\ne-perizinan\ne-planning\ne-polisi\ne-power\ne-punten\ne-retribusi\ne-samsat\ne-sport\ne-store\ne-tax\ne-ticketing\ne-tilang\ne-toll\ne-visa\ne-voting\ne-wallet\ne-warong\necek-ecek\neco-friendly\neco-park\nedan-edanan\neditor-editor\neditor-in-chief\nefek-efek\nekonomi-ekonomi\neksekutif-legislatif\nekspor-impor\nelang-elang\nelemen-elemen\nemak-emak\nembuh-embuhan\nempat-empat\nempek-empek\nempet-empetan\nempok-empok\nempot-empotan\nenak-enak\nencal-encal\nend-to-end\nend-user\nendap-endap\nendut-endut\nendut-endutan\nengah-engah\nengap-engap\nenggan-enggan\nengkah-engkah\nengket-engket\nentah-berentah\nenten-enten\nentry-level\nequity-linked\nerang-erot\nerat-erat\nerek-erek\nereng-ereng\nerong-erong\nesek-esek\nex-officio\nexchange-traded\nexercise-induced\nextra-time\nface-down\nface-to-face\nfair-play\nfakta-fakta\nfaktor-faktor\nfakultas-fakultas\nfase-fase\nfast-food\nfeed-in\nfifty-fifty\nfile-file\nfirst-leg\nfirst-team\nfitur-fitur\nfitur-fiturnya\nfixed-income\nflip-flop\nflip-plop\nfly-in\nfollow-up\nfoto-foto\nfoya-foya\nfraksi-fraksi\nfree-to-play\nfront-end\nfungsi-fungsi\ngaba-gaba\ngabai-gabai\ngada-gada\ngading-gading\ngadis-gadis\ngado-gado\ngail-gail\ngajah-gajah\ngajah-gajahan\ngala-gala\ngaleri-galeri\ngali-gali\ngali-galian\ngaling-galing\ngalu-galu\ngamak-gamak\ngambar-gambar\ngambar-menggambar\ngamit-gamitan\ngampang-gampangan\ngana-gini\nganal-ganal\nganda-berganda\nganjal-mengganjal\nganjil-genap\nganteng-ganteng\ngantung-gantung\ngapah-gopoh\ngara-gara\ngarah-garah\ngaris-garis\ngasak-gasakan\ngatal-gatal\ngaun-gaun\ngawar-gawar\ngaya-gayanya\ngayang-gayang\nge-er\ngebyah-uyah\ngebyar-gebyar\ngedana-gedini\ngedebak-gedebuk\ngedebar-gedebur\ngedung-gedung\ngelang-gelang\ngelap-gelapan\ngelar-gelar\ngelas-gelas\ngelembung-gelembungan\ngeleng-geleng\ngeli-geli\ngeliang-geliut\ngeliat-geliut\ngembar-gembor\ngembrang-gembreng\ngempul-gempul\ngempur-menggempur\ngendang-gendang\ngengsi-gengsian\ngenjang-genjot\ngenjot-genjotan\ngenjrang-genjreng\ngenome-wide\ngeo-politik\ngerabak-gerubuk\ngerak-gerik\ngerak-geriknya\ngerakan-gerakan\ngerbas-gerbus\ngereja-gereja\ngereng-gereng\ngeriak-geriuk\ngerit-gerit\ngerot-gerot\ngeruh-gerah\ngetak-getuk\ngetem-getem\ngeti-geti\ngial-gial\ngial-giul\ngila-gila\ngila-gilaan\ngilang-gemilang\ngilap-gemilap\ngili-gili\ngiling-giling\ngilir-bergilir\nginang-ginang\ngirap-girap\ngirik-girik\ngiring-giring\ngo-auto\ngo-bills\ngo-bluebird\ngo-box\ngo-car\ngo-clean\ngo-food\ngo-glam\ngo-jek\ngo-kart\ngo-mart\ngo-massage\ngo-med\ngo-points\ngo-pulsa\ngo-ride\ngo-send\ngo-shop\ngo-tix\ngo-to-market\ngoak-goak\ngoal-line\ngol-gol\ngolak-galik\ngondas-gandes\ngonjang-ganjing\ngonjlang-ganjling\ngonta-ganti\ngontok-gontokan\ngorap-gorap\ngorong-gorong\ngotong-royong\ngresek-gresek\ngua-gua\ngual-gail\ngubernur-gubernur\ngudu-gudu\ngula-gula\ngulang-gulang\ngulung-menggulung\nguna-ganah\nguna-guna\ngundala-gundala\nguntang-guntang\ngunung-ganang\ngunung-gemunung\ngunung-gunungan\nguru-guru\nhabis-habis\nhabis-habisan\nhak-hak\nhak-hal\nhakim-hakim\nhal-hal\nhalai-balai\nhalf-time\nhama-hama\nhampir-hampir\nhancur-hancuran\nhancur-menghancurkan\nhands-free\nhands-on\nhang-out\nhantu-hantu\nhappy-happy\nharap-harap\nharap-harapan\nhard-disk\nharga-harga\nhari-hari\nharimau-harimau\nharum-haruman\nhasil-hasil\nhasta-wara\nhat-trick\nhati-hati\nhati-hatilah\nhead-mounted\nhead-to-head\nhead-up\nheads-up\nheavy-duty\nhebat-hebatan\nhewan-hewan\nhexa-core\nhidup-hidup\nhidup-mati\nhila-hila\nhilang-hilang\nhina-menghinakan\nhip-hop\nhiru-biru\nhiru-hara\nhiruk-pikuk\nhitam-putih\nhitung-hitung\nhitung-hitungan\nhormat-menghormati\nhot-swappable\nhotel-hotel\nhow-to\nhubar-habir\nhubaya-hubaya\nhukum-red\nhukuman-hukuman\nhula-hoop\nhula-hula\nhulu-hilir\nhumas-humas\nhura-hura\nhuru-hara\nibar-ibar\nibu-anak\nibu-ibu\nicak-icak\nicip-icip\nidam-idam\nide-ide\nigau-igauan\nikan-ikan\nikut-ikut\nikut-ikutan\nilam-ilam\nilat-ilatan\nilmu-ilmu\nimbang-imbangan\niming-iming\nimut-imut\ninang-inang\ninca-binca\nincang-incut\nindustri-industri\ningar-bingar\ningar-ingar\ningat-ingat\ningat-ingatan\ningau-ingauan\ninggang-inggung\ninjak-injak\ninput-output\ninstansi-instansi\ninstant-on\ninstrumen-instrumen\ninter-governmental\nira-ira\nirah-irahan\niras-iras\niring-iringan\niris-irisan\nisak-isak\nisat-bb\niseng-iseng\nistana-istana\nistri-istri\nisu-isu\niya-iya\njabatan-jabatan\njadi-jadian\njagoan-jagoan\njaja-jajaan\njaksa-jaksa\njala-jala\njalan-jalan\njali-jali\njalin-berjalin\njalin-menjalin\njam-jam\njamah-jamahan\njambak-jambakan\njambu-jambu\njampi-jampi\njanda-janda\njangan-jangan\njanji-janji\njarang-jarang\njari-jari\njaring-jaring\njarum-jarum\njasa-jasa\njatuh-bangun\njauh-dekat\njauh-jauh\njawi-jawi\njebar-jebur\njebat-jebatan\njegal-jegalan\njejak-jejak\njelang-menjelang\njelas-jelas\njelur-jelir\njembatan-jembatan\njenazah-jenazah\njendal-jendul\njenderal-jenderal\njenggar-jenggur\njenis-jenis\njenis-jenisnya\njentik-jentik\njerah-jerih\njinak-jinak\njiwa-jiwa\njoli-joli\njolong-jolong\njongkang-jangking\njongkar-jangkir\njongkat-jangkit\njor-joran\njotos-jotosan\njuak-juak\njual-beli\njuang-juang\njulo-julo\njulung-julung\njulur-julur\njumbai-jumbai\njungkang-jungkit\njungkat-jungkit\njurai-jurai\nkabang-kabang\nkabar-kabari\nkabir-kabiran\nkabruk-kabrukan\nkabu-kabu\nkabupaten-kabupaten\nkabupaten-kota\nkaca-kaca\nkacang-kacang\nkacang-kacangan\nkacau-balau\nkadang-kadang\nkader-kader\nkades-kades\nkadis-kadis\nkail-kail\nkain-kain\nkait-kait\nkakak-adik\nkakak-beradik\nkakak-kakak\nkakek-kakek\nkakek-nenek\nkaki-kaki\nkala-kala\nkalau-kalau\nkaleng-kalengan\nkali-kalian\nkalimat-kalimat\nkalung-kalung\nkalut-malut\nkambing-kambing\nkamit-kamit\nkampung-kampung\nkampus-kampus\nkanak-kanak\nkanak-kanan\nkanan-kanak\nkanan-kiri\nkangen-kangenan\nkanwil-kanwil\nkapa-kapa\nkapal-kapal\nkapan-kapan\nkapolda-kapolda\nkapolres-kapolres\nkapolsek-kapolsek\nkapu-kapu\nkarang-karangan\nkarang-mengarang\nkareseh-peseh\nkarut-marut\nkarya-karya\nkasak-kusuk\nkasus-kasus\nkata-kata\nkatang-katang\nkava-kava\nkawa-kawa\nkawan-kawan\nkawin-cerai\nkawin-mawin\nkayu-kayu\nkayu-kayuan\nke-Allah-an\nkeabu-abuan\nkearab-araban\nkeasyik-asyikan\nkebarat-baratan\nkebasah-basahan\nkebat-kebit\nkebata-bataan\nkebayi-bayian\nkebelanda-belandaan\nkeberlarut-larutan\nkebesar-hatian\nkebiasaan-kebiasaan\nkebijakan-kebijakan\nkebiru-biruan\nkebudak-budakan\nkebun-kebun\nkebut-kebutan\nkecamatan-kecamatan\nkecentang-perenangan\nkecil-kecil\nkecil-kecilan\nkecil-mengecil\nkecokelat-cokelatan\nkecomak-kecimik\nkecuh-kecah\nkedek-kedek\nkedekak-kedekik\nkedesa-desaan\nkedubes-kedubes\nkedutaan-kedutaan\nkeempat-empatnya\nkegadis-gadisan\nkegelap-gelapan\nkegiatan-kegiatan\nkegila-gilaan\nkegirang-girangan\nkehati-hatian\nkeheran-heranan\nkehijau-hijauan\nkehitam-hitaman\nkeinggris-inggrisan\nkejaga-jagaan\nkejahatan-kejahatan\nkejang-kejang\nkejar-kejar\nkejar-kejaran\nkejar-mengejar\nkejingga-jinggaan\nkejut-kejut\nkejutan-kejutan\nkekabur-kaburan\nkekanak-kanakan\nkekoboi-koboian\nkekota-kotaan\nkekuasaan-kekuasaan\nkekuning-kuningan\nkelak-kelik\nkelak-keluk\nkelaki-lakian\nkelang-kelok\nkelap-kelip\nkelasah-kelusuh\nkelek-kelek\nkelek-kelekan\nkelemak-kelemek\nkelik-kelik\nkelip-kelip\nkelompok-kelompok\nkelontang-kelantung\nkeluar-masuk\nkelurahan-kelurahan\nkelusuh-kelasah\nkelut-melut\nkemak-kemik\nkemalu-maluan\nkemana-mana\nkemanja-manjaan\nkemarah-marahan\nkemasam-masaman\nkemati-matian\nkembang-kembang\nkemenpan-rb\nkementerian-kementerian\nkemerah-merahan\nkempang-kempis\nkempas-kempis\nkemuda-mudaan\nkena-mengena\nkenal-mengenal\nkenang-kenangan\nkencang-kencung\nkencing-mengencingi\nkencrang-kencring\nkendang-kendang\nkendang-kendangan\nkeningrat-ningratan\nkentung-kentung\nkenyat-kenyit\nkepala-kepala\nkepala-kepalaan\nkepandir-pandiran\nkepang-kepot\nkeperak-perakan\nkepetah-lidahan\nkepilu-piluan\nkeping-keping\nkepucat-pucatan\nkepuh-kepuh\nkepura-puraan\nkeputih-putihan\nkerah-kerahan\nkerancak-rancakan\nkerang-kerangan\nkerang-keroh\nkerang-kerot\nkerang-keruk\nkerang-kerung\nkerap-kerap\nkeras-mengerasi\nkercap-kercip\nkercap-kercup\nkeriang-keriut\nkerja-kerja\nkernyat-kernyut\nkerobak-kerabit\nkerobak-kerobek\nkerobak-kerobik\nkerobat-kerabit\nkerong-kerong\nkeropas-kerapis\nkertak-kertuk\nkertap-kertap\nkeruntang-pungkang\nkesalahan-kesalahan\nkesap-kesip\nkesemena-menaan\nkesenak-senakan\nkesewenang-wenangan\nkesia-siaan\nkesik-kesik\nkesipu-sipuan\nkesu-kesi\nkesuh-kesih\nkesuk-kesik\nketakar-keteker\nketakutan-ketakutan\nketap-ketap\nketap-ketip\nketar-ketir\nketentuan-ketentuan\nketergesa-gesaan\nketi-keti\nketidur-tiduran\nketiga-tiganya\nketir-ketir\nketua-ketua\nketua-tuaan\nketuan-tuanan\nkeungu-unguan\nkewangi-wangian\nki-ka\nkia-kia\nkiai-kiai\nkiak-kiak\nkial-kial\nkiang-kiut\nkiat-kiat\nkibang-kibut\nkicang-kecoh\nkicang-kicu\nkick-off\nkida-kida\nkijang-kijang\nkilau-mengilau\nkili-kili\nkilik-kilik\nkincir-kincir\nkios-kios\nkira-kira\nkira-kiraan\nkiri-kanan\nkirim-berkirim\nkisah-kisah\nkisi-kisi\nkitab-kitab\nkitang-kitang\nkiu-kiu\nklaim-klaim\nklik-klikan\nklip-klip\nklub-klub\nkluntang-klantung\nknock-knock\nknock-on\nknock-out\nko-as\nko-pilot\nkoak-koak\nkoboi-koboian\nkocah-kacih\nkocar-kacir\nkodam-kodam\nkode-kode\nkodim-kodim\nkodok-kodok\nkolang-kaling\nkole-kole\nkoleh-koleh\nkolong-kolong\nkoma-koma\nkomat-kamit\nkomisaris-komisaris\nkomisi-komisi\nkomite-komite\nkomoditas-komoditas\nkongko-kongko\nkonsulat-konsulat\nkonsultan-konsultan\nkontal-kantil\nkontang-kanting\nkontra-terorisme\nkontrak-kontrak\nkonvensi-konvensi\nkopat-kapit\nkoperasi-koperasi\nkopi-kopi\nkoran-koran\nkoreng-koreng\nkos-kosan\nkosak-kasik\nkota-kota\nkota-wakil\nkotak-katik\nkotak-kotak\nkoyak-koyak\nkuas-kuas\nkuat-kuat\nkubu-kubuan\nkucar-kacir\nkucing-kucing\nkucing-kucingan\nkuda-kuda\nkuda-kudaan\nkudap-kudap\nkue-kue\nkulah-kulah\nkulak-kulak\nkulik-kulik\nkulum-kulum\nkumat-kamit\nkumpul-kumpul\nkunang-kunang\nkunar-kunar\nkung-fu\nkuning-hitam\nkupat-kapit\nkupu-kupu\nkura-kura\nkurang-kurang\nkusat-mesat\nkutat-kutet\nkuti-kuti\nkuwung-kuwung\nkyai-kyai\nlaba-laba\nlabi-labi\nlabu-labu\nlaga-laga\nlagi-lagi\nlagu-lagu\nlaguh-lagah\nlain-lain\nlaki-laki\nlalu-lalang\nlalu-lintas\nlama-kelamaan\nlama-lama\nlamat-lamat\nlambat-lambat\nlampion-lampion\nlampu-lampu\nlancang-lancang\nlancar-lancar\nlangak-longok\nlanggar-melanggar\nlangit-langit\nlangkah-langka\nlangkah-langkah\nlanja-lanjaan\nlapas-lapas\nlapat-lapat\nlaporan-laporan\nlaptop-tablet\nlarge-scale\nlari-lari\nlari-larian\nlaskar-laskar\nlauk-pauk\nlaun-laun\nlaut-timur\nlawah-lawah\nlawak-lawak\nlawan-lawan\nlawi-lawi\nlayang-layang\nlayu-layuan\nlebih-lebih\nlecet-lecet\nlegak-legok\nlegum-legum\nlegup-legup\nleha-leha\nlekak-lekuk\nlekap-lekup\nlekas-lekas\nlekat-lekat\nlekuh-lekih\nlekum-lekum\nlekup-lekap\nlembaga-lembaga\nlempar-lemparan\nlenggak-lenggok\nlenggok-lenggok\nlenggut-lenggut\nlengket-lengket\nlentam-lentum\nlentang-lentok\nlentang-lentung\nlepa-lepa\nlerang-lerang\nlereng-lereng\nlese-majeste\nletah-letai\nlete-lete\nletuk-letuk\nletum-letum\nletup-letup\nleyeh-leyeh\nliang-liuk\nliang-liut\nliar-liar\nliat-liut\nlidah-lidah\nlife-toxins\nliga-liga\nlight-emitting\nlika-liku\nlil-alamin\nlilin-lilin\nline-up\nlintas-selat\nlipat-melipat\nliquid-cooled\nlithium-ion\nlithium-polymer\nliuk-liuk\nliung-liung\nlobi-lobi\nlock-up\nlocked-in\nlokasi-lokasi\nlong-term\nlongak-longok\nlontang-lanting\nlontang-lantung\nlopak-lapik\nlopak-lopak\nlow-cost\nlow-density\nlow-end\nlow-light\nlow-multi\nlow-pass\nlucu-lucu\nluka-luka\nlukisan-lukisan\nlumba-lumba\nlumi-lumi\nluntang-lantung\nlupa-lupa\nlupa-lupaan\nlurah-camat\nmaaf-memaafkan\nmabuk-mabukan\nmabul-mabul\nmacam-macam\nmacan-macanan\nmachine-to-machine\nmafia-mafia\nmahasiswa-mahasiswi\nmahasiswa/i\nmahi-mahi\nmain-main\nmain-mainan\nmain-mainlah\nmajelis-majelis\nmaju-mundur\nmakam-makam\nmakan-makan\nmakan-makanan\nmakanan-red\nmake-up\nmaki-maki\nmaki-makian\nmal-mal\nmalai-malai\nmalam-malam\nmalar-malar\nmalas-malasan\nmali-mali\nmalu-malu\nmama-mama\nman-in-the-middle\nmana-mana\nmanajer-manajer\nmanik-manik\nmanis-manis\nmanis-manisan\nmarah-marah\nmark-up\nmas-mas\nmasa-masa\nmasak-masak\nmasalah-masalah\nmash-up\nmasing-masing\nmasjid-masjid\nmasuk-keluar\nmat-matan\nmata-mata\nmatch-fixing\nmati-mati\nmati-matian\nmaya-maya\nmayat-mayat\nmayday-mayday\nmedia-media\nmega-bintang\nmega-tsunami\nmegal-megol\nmegap-megap\nmeger-meger\nmegrek-megrek\nmelak-melak\nmelambai-lambai\nmelambai-lambaikan\nmelambat-lambatkan\nmelaun-laun\nmelawak-lawak\nmelayang-layang\nmelayap-layap\nmelayap-layapkan\nmelebih-lebihi\nmelebih-lebihkan\nmelejang-lejangkan\nmelek-melekan\nmeleleh-leleh\nmelengah-lengah\nmelihat-lihat\nmelimpah-limpah\nmelincah-lincah\nmeliuk-liuk\nmelolong-lolong\nmelompat-lompat\nmeloncat-loncat\nmelonco-lonco\nmelongak-longok\nmelonjak-lonjak\nmemacak-macak\nmemada-madai\nmemadan-madan\nmemaki-maki\nmemaksa-maksa\nmemanas-manasi\nmemancit-mancitkan\nmemandai-mandai\nmemanggil-manggil\nmemanis-manis\nmemanjut-manjut\nmemantas-mantas\nmemasak-masak\nmemata-matai\nmematah-matah\nmematuk-matuk\nmematut-matut\nmemau-mau\nmemayah-mayahkan\nmembaca-baca\nmembacah-bacah\nmembagi-bagikan\nmembalik-balik\nmembangkit-bangkit\nmembarut-barut\nmembawa-bawa\nmembayang-bayangi\nmembayang-bayangkan\nmembeda-bedakan\nmembelai-belai\nmembeli-beli\nmembelit-belitkan\nmembelu-belai\nmembenar-benar\nmembenar-benari\nmemberai-beraikan\nmembesar-besar\nmembesar-besarkan\nmembikin-bikin\nmembilah-bilah\nmembolak-balikkan\nmembongkar-bangkir\nmembongkar-bongkar\nmembuang-buang\nmembuat-buat\nmembulan-bulani\nmembunga-bungai\nmembungkuk-bungkuk\nmemburu-buru\nmemburu-burukan\nmemburuk-burukkan\nmemelintir-melintir\nmemencak-mencak\nmemencar-mencar\nmemercik-mercik\nmemetak-metak\nmemetang-metangkan\nmemetir-metir\nmemijar-mijar\nmemikir-mikir\nmemikir-mikirkan\nmemilih-milih\nmemilin-milin\nmeminang-minang\nmeminta-minta\nmemisah-misahkan\nmemontang-mantingkan\nmemorak-perandakan\nmemorak-porandakan\nmemotong-motong\nmemperamat-amat\nmemperamat-amatkan\nmemperbagai-bagaikan\nmemperganda-gandakan\nmemperganduh-ganduhkan\nmemperimpit-impitkan\nmemperkuda-kudakan\nmemperlengah-lengah\nmemperlengah-lengahkan\nmempermacam-macamkan\nmemperolok-olok\nmemperolok-olokkan\nmempersama-samakan\nmempertubi-tubi\nmempertubi-tubikan\nmemperturut-turutkan\nmemuja-muja\nmemukang-mukang\nmemulun-mulun\nmemundi-mundi\nmemundi-mundikan\nmemutar-mutar\nmemuyu-muyu\nmen-tweet\nmenagak-nagak\nmenakut-nakuti\nmenang-kalah\nmenanjur-nanjur\nmenanti-nanti\nmenari-nari\nmencabik-cabik\nmencabik-cabikkan\nmencacah-cacah\nmencaing-caing\nmencak-mencak\nmencakup-cakup\nmencapak-capak\nmencari-cari\nmencarik-carik\nmencarik-carikkan\nmencarut-carut\nmencengis-cengis\nmencepak-cepak\nmencepuk-cepuk\nmencerai-beraikan\nmencetai-cetai\nmenciak-ciak\nmenciap-ciap\nmenciar-ciar\nmencita-citakan\nmencium-cium\nmenciut-ciut\nmencla-mencle\nmencoang-coang\nmencoba-coba\nmencocok-cocok\nmencolek-colek\nmenconteng-conteng\nmencubit-cubit\nmencucuh-cucuh\nmencucuh-cucuhkan\nmencuri-curi\nmendecap-decap\nmendegam-degam\nmendengar-dengar\nmendengking-dengking\nmendengus-dengus\nmendengut-dengut\nmenderai-deraikan\nmenderak-derakkan\nmenderau-derau\nmenderu-deru\nmendesas-desuskan\nmendesus-desus\nmendetap-detap\nmendewa-dewakan\nmendudu-dudu\nmenduga-duga\nmenebu-nebu\nmenegur-neguri\nmenepak-nepak\nmenepak-nepakkan\nmengabung-ngabung\nmengaci-acikan\nmengacu-acu\nmengada-ada\nmengada-ngada\nmengadang-adangi\nmengaduk-aduk\nmengagak-agak\nmengagak-agihkan\nmengagut-agut\nmengais-ngais\nmengalang-alangi\nmengali-ali\nmengalur-alur\nmengamang-amang\nmengamat-amati\nmengambai-ambaikan\nmengambang-ambang\nmengambung-ambung\nmengambung-ambungkan\nmengamit-ngamitkan\nmengancai-ancaikan\nmengancak-ancak\nmengancar-ancar\nmengangan-angan\nmengangan-angankan\nmengangguk-angguk\nmenganggut-anggut\nmengangin-anginkan\nmengangkat-angkat\nmenganjung-anjung\nmenganjung-anjungkan\nmengap-mengap\nmengapa-apai\nmengapi-apikan\nmengarah-arahi\nmengarang-ngarang\nmengata-ngatai\nmengatup-ngatupkan\nmengaum-aum\nmengaum-aumkan\nmengejan-ejan\nmengejar-ngejar\nmengejut-ngejuti\nmengelai-ngelai\nmengelepik-ngelepik\nmengelip-ngelip\nmengelu-elukan\nmengelus-elus\nmengembut-embut\nmengempas-empaskan\nmengenap-enapkan\nmengendap-endap\nmengenjak-enjak\nmengentak-entak\nmengentak-entakkan\nmengepak-ngepak\nmengepak-ngepakkan\nmengepal-ngepalkan\nmengerjap-ngerjap\nmengerling-ngerling\nmengertak-ngertakkan\nmengesot-esot\nmenggaba-gabai\nmenggali-gali\nmenggalur-galur\nmenggamak-gamak\nmenggamit-gamitkan\nmenggapai-gapai\nmenggapai-gapaikan\nmenggaruk-garuk\nmenggebu-gebu\nmenggebyah-uyah\nmenggeleng-gelengkan\nmenggelepar-gelepar\nmenggelepar-geleparkan\nmenggeliang-geliutkan\nmenggelinding-gelinding\nmenggemak-gemak\nmenggembar-gemborkan\nmenggerak-gerakkan\nmenggerecak-gerecak\nmenggesa-gesakan\nmenggili-gili\nmenggodot-godot\nmenggolak-galikkan\nmenggorek-gorek\nmenggoreng-goreng\nmenggosok-gosok\nmenggoyang-goyangkan\nmengguit-guit\nmenghalai-balaikan\nmenghalang-halangi\nmenghambur-hamburkan\nmenghinap-hinap\nmenghitam-memutihkan\nmenghitung-hitung\nmenghubung-hubungkan\nmenghujan-hujankan\nmengiang-ngiang\nmengibar-ngibarkan\nmengibas-ngibas\nmengibas-ngibaskan\nmengidam-idamkan\nmengilah-ngilahkan\nmengilai-ilai\nmengilat-ngilatkan\nmengilik-ngilik\nmengimak-imak\nmengimbak-imbak\nmengiming-iming\nmengincrit-incrit\nmengingat-ingat\nmenginjak-injak\nmengipas-ngipas\nmengira-ngira\nmengira-ngirakan\nmengiras-iras\nmengiras-irasi\nmengiris-iris\nmengitar-ngitar\nmengitik-ngitik\nmengodol-odol\nmengogok-ogok\nmengolak-alik\nmengolak-alikkan\nmengolang-aling\nmengolang-alingkan\nmengoleng-oleng\nmengolok-olok\nmengombang-ambing\nmengombang-ambingkan\nmengongkang-ongkang\nmengongkok-ongkok\nmengonyah-anyih\nmengopak-apik\nmengorak-arik\nmengorat-oret\nmengorek-ngorek\nmengoret-oret\nmengorok-orok\nmengotak-atik\nmengotak-ngatikkan\nmengotak-ngotakkan\nmengoyak-ngoyak\nmengoyak-ngoyakkan\nmengoyak-oyak\nmenguar-nguarkan\nmenguar-uarkan\nmengubah-ubah\nmengubek-ubek\nmenguber-uber\nmengubit-ubit\nmengubrak-abrik\nmengucar-ngacirkan\nmengucek-ngucek\nmengucek-ucek\nmenguik-uik\nmenguis-uis\nmengulang-ulang\nmengulas-ulas\nmengulit-ulit\nmengulum-ngulum\nmengulur-ulur\nmenguman-uman\nmengumbang-ambingkan\nmengumpak-umpak\nmengungkat-ungkat\nmengungkit-ungkit\nmengupa-upa\nmengurik-urik\nmengusil-usil\nmengusil-usilkan\nmengutak-atik\nmengutak-ngatikkan\nmengutik-ngutik\nmengutik-utik\nmenika-nika\nmenimang-nimang\nmenimbang-nimbang\nmenimbun-nimbun\nmenimpang-nimpangkan\nmeningkat-ningkat\nmeniru-niru\nmenit-menit\nmenitar-nitarkan\nmeniup-niup\nmenjadi-jadi\nmenjadi-jadikan\nmenjedot-jedotkan\nmenjelek-jelekkan\nmenjengek-jengek\nmenjengit-jengit\nmenjerit-jerit\nmenjilat-jilat\nmenjungkat-jungkit\nmenko-menko\nmenlu-menlu\nmenonjol-nonjolkan\nmentah-mentah\nmentang-mentang\nmenteri-menteri\nmentul-mentul\nmenuding-nuding\nmenumpah-numpahkan\nmenunda-nunda\nmenunduk-nunduk\nmenusuk-nusuk\nmenyala-nyala\nmenyama-nyama\nmenyama-nyamai\nmenyambar-nyambar\nmenyangkut-nyangkutkan\nmenyanjung-nyanjung\nmenyanjung-nyanjungkan\nmenyapu-nyapu\nmenyarat-nyarat\nmenyayat-nyayat\nmenyedang-nyedang\nmenyedang-nyedangkan\nmenyelang-nyelangkan\nmenyelang-nyeling\nmenyelang-nyelingkan\nmenyenak-nyenak\nmenyendi-nyendi\nmenyentak-nyentak\nmenyentuh-nyentuh\nmenyepak-nyepakkan\nmenyerak-nyerakkan\nmenyeret-nyeret\nmenyeru-nyerukan\nmenyetel-nyetel\nmenyia-nyiakan\nmenyibak-nyibak\nmenyobek-nyobek\nmenyorong-nyorongkan\nmenyungguh-nyungguhi\nmenyuruk-nyuruk\nmeraba-raba\nmerah-hitam\nmerah-merah\nmerambang-rambang\nmerangkak-rangkak\nmerasa-rasai\nmerata-ratakan\nmeraung-raung\nmeraung-raungkan\nmerayau-rayau\nmerayu-rayu\nmercak-mercik\nmercedes-benz\nmerek-merek\nmereka-mereka\nmereka-reka\nmerelap-relap\nmerem-merem\nmeremah-remah\nmeremas-remas\nmeremeh-temehkan\nmerempah-rempah\nmerempah-rempahi\nmerengek-rengek\nmerengeng-rengeng\nmerenik-renik\nmerenta-renta\nmerenyai-renyai\nmeresek-resek\nmerintang-rintang\nmerintik-rintik\nmerobek-robek\nmeronta-ronta\nmeruap-ruap\nmerubu-rubu\nmerungus-rungus\nmerungut-rungut\nmeta-analysis\nmetode-metode\nmewanti-wanti\nmewarna-warnikan\nmeyakin-yakini\nmid-range\nmid-size\nmiju-miju\nmikro-kecil\nmimpi-mimpi\nminggu-minggu\nminta-minta\nminuman-minuman\nmixed-use\nmobil-mobil\nmobile-first\nmobile-friendly\nmoga-moga\nmola-mola\nmomen-momen\nmondar-mandir\nmonyet-monyet\nmorak-marik\nmorat-marit\nmove-on\nmuda-muda\nmuda-mudi\nmuda/i\nmudah-mudahan\nmuka-muka\nmula-mula\nmultiple-output\nmuluk-muluk\nmulut-mulutan\nmumi-mumi\nmundur-mundur\nmuntah-muntah\nmurid-muridnya\nmusda-musda\nmuseum-museum\nmuslim-muslimah\nmusuh-musuh\nmusuh-musuhnya\nnabi-nabi\nnada-nadanya\nnaga-naga\nnaga-naganya\nnaik-naik\nnaik-turun\nnakal-nakalan\nnama-nama\nnanti-nantian\nnanya-nanya\nnasi-nasi\nnasib-nasiban\nnear-field\nnegara-negara\nnegera-negara\nnegeri-negeri\nnegeri-red\nneka-neka\nnekat-nekat\nneko-neko\nnenek-nenek\nneo-liberalisme\nnext-gen\nnext-generation\nngeang-ngeang\nngeri-ngeri\nnggak-nggak\nngobrol-ngobrol\nngumpul-ngumpul\nnilai-nilai\nnine-dash\nnipa-nipa\nnong-nong\nnorma-norma\nnovel-novel\nnyai-nyai\nnyolong-nyolong\nnyut-nyutan\nob-gyn\nobat-obat\nobat-obatan\nobjek-objek\nobok-obok\nobrak-abrik\nocta-core\nodong-odong\noedipus-kompleks\noff-road\nogah-agih\nogah-ogah\nogah-ogahan\nogak-agik\nogak-ogak\nogoh-ogoh\nolak-alik\nolak-olak\nolang-aling\nolang-alingan\nole-ole\noleh-oleh\nolok-olok\nolok-olokan\nolong-olong\nom-om\nombang-ambing\nomni-channel\non-board\non-demand\non-fire\non-line\non-off\non-premises\non-roll\non-screen\non-the-go\nonde-onde\nondel-ondel\nondos-ondos\none-click\none-to-one\none-touch\none-two\noneng-oneng\nongkang-ongkang\nongol-ongol\nonline-to-offline\nontran-ontran\nonyah-anyih\nonyak-anyik\nopak-apik\nopsi-opsi\nopt-in\norak-arik\norang-aring\norang-orang\norang-orangan\norat-oret\norganisasi-organisasi\normas-ormas\norok-orok\norong-orong\noseng-oseng\notak-atik\notak-otak\notak-otakan\nover-heating\nover-the-air\nover-the-top\npa-pa\npabrik-pabrik\npadi-padian\npagi-pagi\npagi-sore\npajak-pajak\npaket-paket\npalas-palas\npalato-alveolar\npaling-paling\npalu-arit\npalu-memalu\npanas-dingin\npanas-panas\npandai-pandai\npandang-memandang\npanel-panel\npangeran-pangeran\npanggung-panggung\npangkalan-pangkalan\npanja-panja\npanji-panji\npansus-pansus\npantai-pantai\npao-pao\npara-para\nparang-parang\nparpol-parpol\npartai-partai\nparu-paru\npas-pasan\npasal-pasal\npasang-memasang\npasang-surut\npasar-pasar\npasu-pasu\npaus-paus\npaut-memaut\npay-per-click\npaya-paya\npdi-p\npecah-pecah\npecat-pecatan\npeer-to-peer\npejabat-pejabat\npekak-pekak\npekik-pekuk\npelabuhan-pelabuhan\npelacur-pelacur\npelajar-pelajar\npelan-pelan\npelangi-pelangi\npem-bully\npemain-pemain\npemata-mataan\npemda-pemda\npemeluk-pemeluknya\npemerintah-pemerintah\npemerintah-red\npemerintah-swasta\npemetang-metangan\npemilu-pemilu\npemimpin-pemimpin\npeminta-minta\npemuda-pemuda\npemuda-pemudi\npenanggung-jawab\npengali-ali\npengaturan-pengaturan\npenggembar-gemboran\npengorak-arik\npengotak-ngotakan\npengundang-undang\npengusaha-pengusaha\npentung-pentungan\npenyakit-penyakit\nperak-perak\nperang-perangan\nperas-perus\nperaturan-peraturan\nperda-perda\nperempat-final\nperempuan-perempuan\npergi-pergi\npergi-pulang\nperintang-rintang\nperkereta-apian\nperlahan-lahan\nperlip-perlipan\npermen-permen\npernak-pernik\npernik-pernik\npertama-tama\npertandingan-pertandingan\npertimbangan-pertimbangan\nperudang-undangan\nperundang-undangan\nperundangan-undangan\nperusahaan-perusahaan\nperusahaan-perusahan\nperwakilan-perwakilan\npesan-pesan\npesawat-pesawat\npeta-jalan\npetang-petang\npetantang-petenteng\npetatang-peteteng\npete-pete\npiala-piala\npiat-piut\npick-up\npicture-in-picture\npihak-pihak\npijak-pijak\npijar-pijar\npijat-pijat\npikir-pikir\npil-pil\npilah-pilih\npilih-pilih\npilihan-pilihan\npilin-memilin\npilkada-pilkada\npina-pina\npindah-pindah\nping-pong\npinjam-meminjam\npintar-pintarlah\npisang-pisang\npistol-pistolan\npiting-memiting\nplanet-planet\nplay-off\nplin-plan\nplintat-plintut\nplonga-plongo\nplug-in\nplus-minus\nplus-plus\npoco-poco\npohon-pohonan\npoin-poin\npoint-of-sale\npoint-of-sales\npokemon-pokemon\npokja-pokja\npokok-pokok\npokrol-pokrolan\npolang-paling\npolda-polda\npoleng-poleng\npolong-polongan\npolres-polres\npolsek-polsek\npolwan-polwan\npoma-poma\npondok-pondok\nponpes-ponpes\npontang-panting\npop-up\nporak-parik\nporak-peranda\nporak-poranda\npos-pos\nposko-posko\npotong-memotong\npraktek-praktek\npraktik-praktik\nproduk-produk\nprogram-program\npromosi-degradasi\nprovinsi-provinsi\nproyek-proyek\npuing-puing\npuisi-puisi\npuji-pujian\npukang-pukang\npukul-memukul\npulang-pergi\npulau-pulai\npulau-pulau\npull-up\npulut-pulut\npundi-pundi\npungak-pinguk\npunggung-memunggung\npura-pura\npuruk-parak\npusar-pusar\npusat-pusat\npush-to-talk\npush-up\npush-ups\npusing-pusing\npuskesmas-puskesmas\nputar-putar\nputera-puteri\nputih-hitam\nputih-putih\nputra-putra\nputra-putri\nputra/i\nputri-putri\nputus-putus\nputusan-putusan\npuvi-puvi\nquad-core\nraba-rabaan\nraba-rubu\nrada-rada\nradio-frequency\nragu-ragu\nrahasia-rahasiaan\nraja-raja\nrama-rama\nramai-ramai\nramalan-ramalan\nrambeh-rambeh\nrambu-rambu\nrame-rame\nramu-ramuan\nranda-rondo\nrangkul-merangkul\nrango-rango\nrap-rap\nrasa-rasanya\nrata-rata\nraun-raun\nread-only\nreal-life\nreal-time\nrebah-rebah\nrebah-rebahan\nrebas-rebas\nred-eye\nredam-redam\nredep-redup\nrehab-rekon\nreja-reja\nreka-reka\nreka-rekaan\nrekan-rekan\nrekan-rekannya\nrekor-rekor\nrelief-relief\nremah-remah\nremang-remang\nrembah-rembah\nrembah-rembih\nremeh-cemeh\nremeh-temeh\nrempah-rempah\nrencana-rencana\nrenyai-renyai\nrep-repan\nrepot-repot\nrepuh-repuh\nrestoran-restoran\nretak-retak\nriang-riang\nribu-ribu\nribut-ribut\nrica-rica\nride-sharing\nrigi-rigi\nrinai-rinai\nrintik-rintik\nritual-ritual\nrobak-rabik\nrobat-rabit\nrobot-robot\nrole-play\nrole-playing\nroll-on\nrombang-rambing\nromol-romol\nrompang-romping\nrondah-rondih\nropak-rapik\nroyal-royalan\nroyo-royo\nruak-ruak\nruba-ruba\nrudal-rudal\nruji-ruji\nruku-ruku\nrumah-rumah\nrumah-rumahan\nrumbai-rumbai\nrumput-rumputan\nrunding-merunding\nrundu-rundu\nrunggu-rangga\nrunner-up\nruntang-runtung\nrupa-rupa\nrupa-rupanya\nrusun-rusun\nrute-rute\nsaat-saat\nsaban-saban\nsabu-sabu\nsabung-menyabung\nsah-sah\nsahabat-sahabat\nsaham-saham\nsahut-menyahut\nsaing-menyaing\nsaji-sajian\nsakit-sakitan\nsaksi-saksi\nsaku-saku\nsalah-salah\nsama-sama\nsamar-samar\nsambar-menyambar\nsambung-bersambung\nsambung-menyambung\nsambut-menyambut\nsamo-samo\nsampah-sampah\nsampai-sampai\nsamping-menyamping\nsana-sini\nsandar-menyandar\nsandi-sandi\nsangat-sangat\nsangkut-menyangkut\nsapa-menyapa\nsapai-sapai\nsapi-sapi\nsapu-sapu\nsaran-saran\nsarana-prasarana\nsari-sari\nsarit-sarit\nsatu-dua\nsatu-satu\nsatu-satunya\nsatuan-satuan\nsaudara-saudara\nsauk-menyauk\nsauk-sauk\nsayang-sayang\nsayap-sayap\nsayup-menyayup\nsayup-sayup\nsayur-mayur\nsayur-sayuran\nsci-fi\nseagak-agak\nseakal-akal\nseakan-akan\nsealak-alak\nseari-arian\nsebaik-baiknya\nsebelah-menyebelah\nsebentar-sebentar\nseberang-menyeberang\nseberuntung-beruntungnya\nsebesar-besarnya\nseboleh-bolehnya\nsedalam-dalamnya\nsedam-sedam\nsedang-menyedang\nsedang-sedang\nsedap-sedapan\nsedapat-dapatnya\nsedikit-dikitnya\nsedikit-sedikit\nsedikit-sedikitnya\nsedini-dininya\nseelok-eloknya\nsegala-galanya\nsegan-menyegan\nsegan-menyegani\nsegan-segan\nsehabis-habisnya\nsehari-hari\nsehari-harian\nsehari-harinya\nsejadi-jadinya\nsekali-kali\nsekali-sekali\nsekenyang-kenyangnya\nsekira-kira\nsekolah-sekolah\nsekonyong-konyong\nsekosong-kosongnya\nsektor-sektor\nsekuasa-kuasanya\nsekuat-kuatnya\nsekurang-kurangnya\nsel-sel\nsela-menyela\nsela-sela\nselak-seluk\nselama-lamanya\nselambat-lambatnya\nselang-seli\nselang-seling\nselar-belar\nselat-latnya\nselatan-tenggara\nselekas-lekasnya\nselentang-selenting\nselepas-lepas\nself-driving\nself-esteem\nself-healing\nself-help\nselir-menyelir\nseloyong-seloyong\nseluk-beluk\nseluk-semeluk\nsema-sema\nsemah-semah\nsemak-semak\nsemaksimal-maksimalnya\nsemalam-malaman\nsemang-semang\nsemanis-manisnya\nsemasa-masa\nsemata-mata\nsemau-maunya\nsembunyi-sembunyi\nsembunyi-sembunyian\nsembur-sembur\nsemena-mena\nsemenda-menyemenda\nsemengga-mengga\nsemenggah-menggah\nsementang-mentang\nsemerdeka-merdekanya\nsemi-final\nsemi-permanen\nsempat-sempatnya\nsemu-semu\nsemua-muanya\nsemujur-mujurnya\nsemut-semutan\nsen-senan\nsendiri-sendiri\nsengal-sengal\nsengar-sengir\nsengau-sengauan\nsenggak-sengguk\nsenggang-tenggang\nsenggol-menyenggol\nsenior-junior\nsenjata-senjata\nsenyum-senyum\nseolah-olah\nsepala-pala\nsepandai-pandai\nsepetang-petangan\nsepoi-sepoi\nsepraktis-praktisnya\nsepuas-puasnya\nserak-serak\nserak-serik\nserang-menyerang\nserang-serangan\nserangan-serangan\nseraya-menyeraya\nserba-serbi\nserbah-serbih\nserembah-serembih\nserigala-serigala\nsering-sering\nserobot-serobotan\nserong-menyerong\nserta-menyertai\nserta-merta\nserta-serta\nseru-seruan\nservice-oriented\nsesak-menyesak\nsesal-menyesali\nsesayup-sayup\nsesi-sesi\nsesuang-suang\nsesudah-sudah\nsesudah-sudahnya\nsesuka-suka\nsesuka-sukanya\nset-piece\nsetempat-setempat\nsetengah-setengah\nsetidak-tidaknya\nsetinggi-tingginya\nseupaya-upaya\nseupaya-upayanya\nsewa-menyewa\nsewaktu-waktu\nsewenang-wenang\nsewot-sewotan\nshabu-shabu\nshort-term\nshort-throw\nsia-sia\nsiang-siang\nsiap-siap\nsiapa-siapa\nsibar-sibar\nsibur-sibur\nsida-sida\nside-by-side\nsign-in\nsiku-siku\nsikut-sikutan\nsilah-silah\nsilang-menyilang\nsilir-semilir\nsimbol-simbol\nsimpan-pinjam\nsinar-menyinar\nsinar-seminar\nsinar-suminar\nsindir-menyindir\nsinga-singa\nsinggah-menyinggah\nsingle-core\nsipil-militer\nsir-siran\nsirat-sirat\nsisa-sisa\nsisi-sisi\nsiswa-siswa\nsiswa-siswi\nsiswa/i\nsiswi-siswi\nsitu-situ\nsitus-situs\nsix-core\nsix-speed\nslintat-slintut\nslo-mo\nslow-motion\nsnap-on\nsobek-sobekan\nsodok-sodokan\nsok-sokan\nsolek-menyolek\nsolid-state\nsorak-sorai\nsorak-sorak\nsore-sore\nsosio-ekonomi\nsoya-soya\nspill-resistant\nsplit-screen\nsponsor-sponsor\nsponsor-sponsoran\nsrikandi-srikandi\nstaf-staf\nstand-by\nstand-up\nstart-up\nstasiun-stasiun\nstate-owned\nstriker-striker\nstudi-studi\nsuam-suam\nsuami-isteri\nsuami-istri\nsuami-suami\nsuang-suang\nsuara-suara\nsudin-sudin\nsudu-sudu\nsudung-sudung\nsugi-sugi\nsuka-suka\nsuku-suku\nsulang-menyulang\nsulat-sulit\nsulur-suluran\nsum-sum\nsumber-sumber\nsumpah-sumpah\nsumpit-sumpit\nsundut-bersundut\nsungai-sungai\nsungguh-sungguh\nsungut-sungut\nsunting-menyunting\nsuper-damai\nsuper-rahasia\nsuper-sub\nsupply-demand\nsupply-side\nsuram-suram\nsurat-menyurat\nsurat-surat\nsuruh-suruhan\nsuruk-surukan\nsusul-menyusul\nsuwir-suwir\nsyarat-syarat\nsystem-on-chip\nt-shirt\nt-shirts\ntabar-tabar\ntabir-mabir\ntabrak-tubruk\ntabuh-tabuhan\ntabun-menabun\ntahu-menahu\ntahu-tahu\ntahun-tahun\ntakah-takahnya\ntakang-takik\ntake-off\ntakut-takut\ntakut-takutan\ntali-bertali\ntali-tali\ntalun-temalun\ntaman-taman\ntampak-tampak\ntanak-tanakan\ntanam-menanam\ntanam-tanaman\ntanda-tanda\ntangan-menangan\ntangan-tangan\ntangga-tangga\ntanggal-tanggal\ntanggul-tanggul\ntanggung-menanggung\ntanggung-tanggung\ntank-tank\ntante-tante\ntanya-jawab\ntapa-tapa\ntapak-tapak\ntari-menari\ntari-tarian\ntarik-menarik\ntarik-ulur\ntata-tertib\ntatah-tatah\ntau-tau\ntawa-tawa\ntawak-tawak\ntawang-tawang\ntawar-menawar\ntawar-tawar\ntayum-temayum\ntebak-tebakan\ntebu-tebu\ntedong-tedong\ntegak-tegak\ntegerbang-gerbang\nteh-tehan\ntek-tek\nteka-teki\nteknik-teknik\nteman-teman\nteman-temanku\ntemas-temas\ntembak-menembak\ntemeh-temeh\ntempa-menempa\ntempat-tempat\ntempo-tempo\ntemut-temut\ntenang-tenang\ntengah-tengah\ntenggang-menenggang\ntengok-menengok\nteori-teori\nteraba-raba\nteralang-alang\nterambang-ambang\nterambung-ambung\nterang-terang\nterang-terangan\nteranggar-anggar\nterangguk-angguk\nteranggul-anggul\nterangin-angin\nterangkup-angkup\nteranja-anja\nterapung-apung\nterayan-rayan\nterayap-rayap\nterbada-bada\nterbahak-bahak\nterbang-terbang\nterbata-bata\nterbatuk-batuk\nterbayang-bayang\nterbeda-bedakan\nterbengkil-bengkil\nterbengong-bengong\nterbirit-birit\nterbuai-buai\nterbuang-buang\nterbungkuk-bungkuk\nterburu-buru\ntercangak-cangak\ntercengang-cengang\ntercilap-cilap\ntercongget-congget\ntercoreng-moreng\ntercungap-cungap\nterdangka-dangka\nterdengih-dengih\nterduga-duga\nterekeh-ekeh\nterembut-embut\nterembut-rembut\nterempas-empas\nterengah-engah\nteresak-esak\ntergagap-gagap\ntergagau-gagau\ntergaguk-gaguk\ntergapai-gapai\ntergegap-gegap\ntergegas-gegas\ntergelak-gelak\ntergelang-gelang\ntergeleng-geleng\ntergelung-gelung\ntergerai-gerai\ntergerenyeng-gerenyeng\ntergesa-gesa\ntergila-gila\ntergolek-golek\ntergontai-gontai\ntergudik-gudik\ntergugu-gugu\nterguling-guling\ntergulut-gulut\nterhambat-hambat\nterharak-harak\nterharap-harap\nterhengit-hengit\nterheran-heran\nterhinggut-hinggut\nterigau-igau\nterimpi-impi\nterincut-incut\nteringa-inga\nteringat-ingat\nterinjak-injak\nterisak-isak\nterjembak-jembak\nterjerit-jerit\nterkadang-kadang\nterkagum-kagum\nterkaing-kaing\nterkakah-kakah\nterkakak-kakak\nterkampul-kampul\nterkanjar-kanjar\nterkantuk-kantuk\nterkapah-kapah\nterkapai-kapai\nterkapung-kapung\nterkatah-katah\nterkatung-katung\nterkecap-kecap\nterkedek-kedek\nterkedip-kedip\nterkejar-kejar\nterkekau-kekau\nterkekeh-kekeh\nterkekek-kekek\nterkelinjat-kelinjat\nterkelip-kelip\nterkempul-kempul\nterkemut-kemut\nterkencar-kencar\nterkencing-kencing\nterkentut-kentut\nterkepak-kepak\nterkesot-kesot\nterkesut-kesut\nterkial-kial\nterkijai-kijai\nterkikih-kikih\nterkikik-kikik\nterkincak-kincak\nterkindap-kindap\nterkinja-kinja\nterkirai-kirai\nterkitar-kitar\nterkocoh-kocoh\nterkojol-kojol\nterkokol-kokol\nterkosel-kosel\nterkotak-kotak\nterkoteng-koteng\nterkuai-kuai\nterkumpal-kumpal\nterlara-lara\nterlayang-layang\nterlebih-lebih\nterlincah-lincah\nterliuk-liuk\nterlolong-lolong\nterlongong-longong\nterlunta-lunta\ntermangu-mangu\ntermanja-manja\ntermata-mata\ntermengah-mengah\ntermenung-menung\ntermimpi-mimpi\ntermonyong-monyong\nternanti-nanti\nterngiang-ngiang\nteroleng-oleng\nterombang-ambing\nterpalit-palit\nterpandang-pandang\nterpecah-pecah\nterpekik-pekik\nterpencar-pencar\nterpereh-pereh\nterpijak-pijak\nterpikau-pikau\nterpilah-pilah\nterpinga-pinga\nterpingkal-pingkal\nterpingkau-pingkau\nterpontang-panting\nterpusing-pusing\nterputus-putus\ntersanga-sanga\ntersaruk-saruk\ntersedan-sedan\ntersedih-sedih\ntersedu-sedu\nterseduh-seduh\ntersendat-sendat\ntersendeng-sendeng\ntersengal-sengal\ntersengguk-sengguk\ntersengut-sengut\nterseok-seok\ntersera-sera\nterserak-serak\ntersetai-setai\ntersia-sia\ntersipu-sipu\ntersoja-soja\ntersungkuk-sungkuk\ntersuruk-suruk\ntertagak-tagak\ntertahan-tahan\ntertatih-tatih\ntertegun-tegun\ntertekan-tekan\nterteleng-teleng\ntertendang-tendang\ntertimpang-timpang\ntertitar-titar\nterumbang-ambing\nterumbang-umbang\nterungkap-ungkap\nterus-menerus\nterus-terusan\ntete-a-tete\ntext-to-speech\nthink-tank\nthink-thank\nthird-party\nthird-person\nthree-axis\nthree-point\ntiap-tiap\ntiba-tiba\ntidak-tidak\ntidur-tidur\ntidur-tiduran\ntie-dye\ntie-in\ntiga-tiganya\ntikam-menikam\ntiki-taka\ntikus-tikus\ntilik-menilik\ntim-tim\ntimah-timah\ntimang-timangan\ntimbang-menimbang\ntime-lapse\ntimpa-menimpa\ntimu-timu\ntimun-timunan\ntimur-barat\ntimur-laut\ntimur-tenggara\ntindih-bertindih\ntindih-menindih\ntinjau-meninjau\ntinju-meninju\ntip-off\ntipu-tipu\ntiru-tiruan\ntitik-titik\ntitik-titiknya\ntiup-tiup\nto-do\ntokak-takik\ntoko-toko\ntokoh-tokoh\ntokok-menokok\ntolak-menolak\ntolong-menolong\ntong-tong\ntop-level\ntop-up\ntotol-totol\ntouch-screen\ntrade-in\ntraining-camp\ntrans-nasional\ntreble-winner\ntri-band\ntrik-trik\ntriple-core\ntruk-truk\ntua-tua\ntuan-tuan\ntuang-tuang\ntuban-tuban\ntubuh-tubuh\ntujuan-tujuan\ntuk-tuk\ntukang-menukang\ntukar-menukar\ntulang-belulang\ntulang-tulangan\ntuli-tuli\ntulis-menulis\ntumbuh-tumbuhan\ntumpang-tindih\ntune-up\ntunggang-tunggik\ntunggang-tungging\ntunggang-tunggit\ntunggul-tunggul\ntunjuk-menunjuk\ntupai-tupai\ntupai-tupaian\nturi-turian\nturn-based\nturnamen-turnamen\nturun-temurun\nturut-menurut\nturut-turutan\ntuyuk-tuyuk\ntwin-cam\ntwin-turbocharged\ntwo-state\ntwo-step\ntwo-tone\nu-shape\nuang-uangan\nuar-uar\nubek-ubekan\nubel-ubel\nubrak-abrik\nubun-ubun\nubur-ubur\nuci-uci\nudang-undang\nudap-udapan\nugal-ugalan\nuget-uget\nuir-uir\nujar-ujar\nuji-coba\nujung-ujung\nujung-ujungnya\nuka-uka\nukir-mengukir\nukir-ukiran\nula-ula\nulak-ulak\nulam-ulam\nulang-alik\nulang-aling\nulang-ulang\nulap-ulap\nular-ular\nular-ularan\nulek-ulek\nulu-ulu\nulung-ulung\numang-umang\numbang-ambing\numbi-umbian\numbul-umbul\numbut-umbut\nuncang-uncit\nundak-undakan\nundang-undang\nundang-undangnya\nunduk-unduk\nundung-undung\nundur-undur\nunek-unek\nungah-angih\nunggang-anggit\nunggat-unggit\nunggul-mengungguli\nungkit-ungkit\nunit-unit\nuniversitas-universitas\nunsur-unsur\nuntang-anting\nunting-unting\nuntung-untung\nuntung-untungan\nupah-mengupah\nupih-upih\nupside-down\nura-ura\nuran-uran\nurat-urat\nuring-uringan\nurup-urup\nurup-urupan\nurus-urus\nusaha-usaha\nuser-user\nuser-useran\nutak-atik\nutang-piutang\nutang-utang\nutar-utar\nutara-jauh\nutara-selatan\nuter-uter\nutusan-utusan\nv-belt\nv-neck\nvalue-added\nvery-very\nvideo-video\nvisi-misi\nvisi-misinya\nvoa-islam\nvoice-over\nvolt-ampere\nwajah-wajah\nwajar-wajar\nwake-up\nwakil-wakil\nwalk-in\nwalk-out\nwangi-wangian\nwanita-wanita\nwanti-wanti\nwara-wara\nwara-wiri\nwarna-warna\nwarna-warni\nwas-was\nwater-cooled\nweb-based\nwide-angle\nwilayah-wilayah\nwin-win\nwira-wiri\nwora-wari\nwork-life\nworld-class\nyang-yang\nyayasan-yayasan\nyear-on-year\nyel-yel\nyo-yo\nzam-zam\nzig-zag\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/id/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.id.tokenizer_exceptions.orth_title->'-'.join([part.title() for part in orth.split('-')])
A:spacy.lang.id.tokenizer_exceptions.orth_caps->'-'.join([part.upper() for part in orth.split('-')])
A:spacy.lang.id.tokenizer_exceptions.orth_lower->orth.lower()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/id/syntax_iterators.py----------------------------------------
A:spacy.lang.id.syntax_iterators.conj->doc.vocab.strings.add('conj')
A:spacy.lang.id.syntax_iterators.np_label->doc.vocab.strings.add('NP')
A:spacy.lang.id.syntax_iterators.seen->set()
spacy.lang.id.syntax_iterators.noun_chunks(obj)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/id/stop_words.py----------------------------------------
A:spacy.lang.id.stop_words.STOP_WORDS->set('\nada adalah adanya adapun agak agaknya agar akan akankah akhir akhiri akhirnya\naku akulah amat amatlah anda andalah antar antara antaranya apa apaan apabila\napakah apalagi apatah artinya asal asalkan atas atau ataukah ataupun awal\nawalnya\n\nbagai bagaikan bagaimana bagaimanakah bagaimanapun bagi bagian bahkan bahwa\nbahwasanya baik bakal bakalan balik banyak bapak baru bawah beberapa begini\nbeginian beginikah beginilah begitu begitukah begitulah begitupun bekerja\nbelakang belakangan belum belumlah benar benarkah benarlah berada berakhir\nberakhirlah berakhirnya berapa berapakah berapalah berapapun berarti berawal\nberbagai berdatangan beri berikan berikut berikutnya berjumlah berkali-kali\nberkata berkehendak berkeinginan berkenaan berlainan berlalu berlangsung\nberlebihan bermacam bermacam-macam bermaksud bermula bersama bersama-sama\nbersiap bersiap-siap bertanya bertanya-tanya berturut berturut-turut bertutur\nberujar berupa besar betul betulkah biasa biasanya bila bilakah bisa bisakah\nboleh bolehkah bolehlah buat bukan bukankah bukanlah bukannya bulan bung\n\ncara caranya cukup cukupkah cukuplah cuma\n\ndahulu dalam dan dapat dari daripada datang dekat demi demikian demikianlah\ndengan depan di dia diakhiri diakhirinya dialah diantara diantaranya diberi\ndiberikan diberikannya dibuat dibuatnya didapat didatangkan digunakan\ndiibaratkan diibaratkannya diingat diingatkan diinginkan dijawab dijelaskan\ndijelaskannya dikarenakan dikatakan dikatakannya dikerjakan diketahui\ndiketahuinya dikira dilakukan dilalui dilihat dimaksud dimaksudkan\ndimaksudkannya dimaksudnya diminta dimintai dimisalkan dimulai dimulailah\ndimulainya dimungkinkan dini dipastikan diperbuat diperbuatnya dipergunakan\ndiperkirakan diperlihatkan diperlukan diperlukannya dipersoalkan dipertanyakan\ndipunyai diri dirinya disampaikan disebut disebutkan disebutkannya disini\ndisinilah ditambahkan ditandaskan ditanya ditanyai ditanyakan ditegaskan\nditujukan ditunjuk ditunjuki ditunjukkan ditunjukkannya ditunjuknya dituturkan\ndituturkannya diucapkan diucapkannya diungkapkan dong dua dulu\n\nempat enggak enggaknya entah entahlah\n\nguna gunakan\n\nhal hampir hanya hanyalah hari harus haruslah harusnya hendak hendaklah\nhendaknya hingga\n\nia ialah ibarat ibaratkan ibaratnya ibu ikut ingat ingat-ingat ingin inginkah\ninginkan ini inikah inilah itu itukah itulah\n\njadi jadilah jadinya jangan jangankan janganlah jauh jawab jawaban jawabnya\njelas jelaskan jelaslah jelasnya jika jikalau juga jumlah jumlahnya justru\n\nkala kalau kalaulah kalaupun kalian kami kamilah kamu kamulah kan kapan\nkapankah kapanpun karena karenanya kasus kata katakan katakanlah katanya ke\nkeadaan kebetulan kecil kedua keduanya keinginan kelamaan kelihatan\nkelihatannya kelima keluar kembali kemudian kemungkinan kemungkinannya kenapa\nkepada kepadanya kesampaian keseluruhan keseluruhannya keterlaluan ketika\nkhususnya kini kinilah kira kira-kira kiranya kita kitalah kok kurang\n\nlagi lagian lah lain lainnya lalu lama lamanya lanjut lanjutnya lebih lewat\nlima luar\n\nmacam maka makanya makin malah malahan mampu mampukah mana manakala manalagi\nmasa masalah masalahnya masih masihkah masing masing-masing mau maupun\nmelainkan melakukan melalui melihat melihatnya memang memastikan memberi\nmemberikan membuat memerlukan memihak meminta memintakan memisalkan memperbuat\nmempergunakan memperkirakan memperlihatkan mempersiapkan mempersoalkan\nmempertanyakan mempunyai memulai memungkinkan menaiki menambahkan menandaskan\nmenanti menanti-nanti menantikan menanya menanyai menanyakan mendapat\nmendapatkan mendatang mendatangi mendatangkan menegaskan mengakhiri mengapa\nmengatakan mengatakannya mengenai mengerjakan mengetahui menggunakan\nmenghendaki mengibaratkan mengibaratkannya mengingat mengingatkan menginginkan\nmengira mengucapkan mengucapkannya mengungkapkan menjadi menjawab menjelaskan\nmenuju menunjuk menunjuki menunjukkan menunjuknya menurut menuturkan\nmenyampaikan menyangkut menyatakan menyebutkan menyeluruh menyiapkan merasa\nmereka merekalah merupakan meski meskipun meyakini meyakinkan minta mirip\nmisal misalkan misalnya mula mulai mulailah mulanya mungkin mungkinkah\n\nnah naik namun nanti nantinya nyaris nyatanya\n\noleh olehnya\n\npada padahal padanya pak paling panjang pantas para pasti pastilah penting\npentingnya per percuma perlu perlukah perlunya pernah persoalan pertama\npertama-tama pertanyaan pertanyakan pihak pihaknya pukul pula pun punya\n\nrasa rasanya rata rupanya\n\nsaat saatnya saja sajalah saling sama sama-sama sambil sampai sampai-sampai\nsampaikan sana sangat sangatlah satu saya sayalah se sebab sebabnya sebagai\nsebagaimana sebagainya sebagian sebaik sebaik-baiknya sebaiknya sebaliknya\nsebanyak sebegini sebegitu sebelum sebelumnya sebenarnya seberapa sebesar\nsebetulnya sebisanya sebuah sebut sebutlah sebutnya secara secukupnya sedang\nsedangkan sedemikian sedikit sedikitnya seenaknya segala segalanya segera\nseharusnya sehingga seingat sejak sejauh sejenak sejumlah sekadar sekadarnya\nsekali sekali-kali sekalian sekaligus sekalipun sekarang sekarang sekecil\nseketika sekiranya sekitar sekitarnya sekurang-kurangnya sekurangnya sela\nselain selaku selalu selama selama-lamanya selamanya selanjutnya seluruh\nseluruhnya semacam semakin semampu semampunya semasa semasih semata semata-mata\nsemaunya sementara semisal semisalnya sempat semua semuanya semula sendiri\nsendirian sendirinya seolah seolah-olah seorang sepanjang sepantasnya\nsepantasnyalah seperlunya seperti sepertinya sepihak sering seringnya serta\nserupa sesaat sesama sesampai sesegera sesekali seseorang sesuatu sesuatunya\nsesudah sesudahnya setelah setempat setengah seterusnya setiap setiba setibanya\nsetidak-tidaknya setidaknya setinggi seusai sewaktu siap siapa siapakah\nsiapapun sini sinilah soal soalnya suatu sudah sudahkah sudahlah supaya\n\ntadi tadinya tahu tahun tak tambah tambahnya tampak tampaknya tandas tandasnya\ntanpa tanya tanyakan tanyanya tapi tegas tegasnya telah tempat tengah tentang\ntentu tentulah tentunya tepat terakhir terasa terbanyak terdahulu terdapat\nterdiri terhadap terhadapnya teringat teringat-ingat terjadi terjadilah\nterjadinya terkira terlalu terlebih terlihat termasuk ternyata tersampaikan\ntersebut tersebutlah tertentu tertuju terus terutama tetap tetapi tiap tiba\ntiba-tiba tidak tidakkah tidaklah tiga tinggi toh tunjuk turut tutur tuturnya\n\nucap ucapnya ujar ujarnya umum umumnya ungkap ungkapnya untuk usah usai\n\nwaduh wah wahai waktu waktunya walau walaupun wong\n\nyaitu yakin yakni yang\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/id/norm_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/id/tag_map.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/id/punctuation.py----------------------------------------
A:spacy.lang.id.punctuation.UNITS->merge_chars(_units)
A:spacy.lang.id.punctuation.CURRENCY->merge_chars(_currency)
A:spacy.lang.id.punctuation.MONTHS->merge_chars(_months)
A:spacy.lang.id.punctuation.LIST_CURRENCY->split_chars(_currency)
A:spacy.lang.id.punctuation._prefixes->list(TOKENIZER_PREFIXES)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/sr/__init__.py----------------------------------------
A:spacy.lang.sr.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.sr.__init__.lex_attr_getters[NORM]->add_lookups(Language.Defaults.lex_attr_getters[NORM], BASE_NORMS, NORM_EXCEPTIONS)
A:spacy.lang.sr.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS, TOKENIZER_EXCEPTIONS)
spacy.lang.sr.__init__.Serbian(Language)
spacy.lang.sr.__init__.SerbianDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/sr/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/sr/lex_attrs.py----------------------------------------
A:spacy.lang.sr.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.sr.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.sr.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/sr/tokenizer_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/sr/stop_words.py----------------------------------------
A:spacy.lang.sr.stop_words.STOP_WORDS->set('\nĞ°\nĞ°Ğ²Ğ°Ñ˜\nĞ°ĞºĞ¾\nĞ°Ğ»\nĞ°Ğ»Ğ¸\nĞ°Ñ€Ñ…\nĞ°Ñƒ\nĞ°Ñ…\nĞ°Ñ…Ğ°\nĞ°Ñ˜\nĞ±Ğ°Ñ€\nĞ±Ğ¸\nĞ±Ğ¸Ğ»Ğ°\nĞ±Ğ¸Ğ»Ğ¸\nĞ±Ğ¸Ğ»Ğ¾\nĞ±Ğ¸ÑĞ¼Ğ¾\nĞ±Ğ¸ÑÑ‚Ğµ\nĞ±Ğ¸Ñ…\nĞ±Ğ¸Ñ˜Ğ°ÑĞ¼Ğ¾\nĞ±Ğ¸Ñ˜Ğ°ÑÑ‚Ğµ\nĞ±Ğ¸Ñ˜Ğ°Ñ…\nĞ±Ğ¸Ñ˜Ğ°Ñ…Ñƒ\nĞ±Ğ¸Ñ˜Ğ°ÑˆĞµ\nĞ±Ğ¸Ñ›Ğµ\nĞ±Ğ»Ğ¸Ğ·Ñƒ\nĞ±Ñ€Ğ¾Ñ˜\nĞ±Ñ€Ñ€\nĞ±ÑƒĞ´Ğµ\nĞ±ÑƒĞ´Ğ¸Ğ¼Ğ¾\nĞ±ÑƒĞ´Ğ¸Ñ‚Ğµ\nĞ±ÑƒĞ´Ñƒ\nĞ±ÑƒĞ´ÑƒÑ›Ğ¸\nĞ±ÑƒĞ¼\nĞ±ÑƒÑ›\nĞ²Ğ°Ğ¼\nĞ²Ğ°Ğ¼Ğ°\nĞ²Ğ°Ñ\nĞ²Ğ°ÑˆĞ°\nĞ²Ğ°ÑˆĞµ\nĞ²Ğ°ÑˆĞ¸Ğ¼\nĞ²Ğ°ÑˆĞ¸Ğ¼Ğ°\nĞ²Ğ°Ñ™Ğ´Ğ°\nĞ²ĞµĞ¾Ğ¼Ğ°\nĞ²ĞµÑ€Ğ¾Ğ²Ğ°Ñ‚Ğ½Ğ¾\nĞ²ĞµÑ›\nĞ²ĞµÑ›Ğ¸Ğ½Ğ°\nĞ²Ğ¸\nĞ²Ğ¸Ğ´ĞµĞ¾\nĞ²Ğ¸ÑˆĞµ\nĞ²Ñ€Ğ»Ğ¾\nĞ²Ñ€Ñ…\nĞ³Ğ°\nĞ³Ğ´Ğµ\nĞ³Ğ¸Ñ†\nĞ³Ğ¾Ğ´\nĞ³Ğ¾Ñ€Ğµ\nĞ³Ñ’ĞµĞºĞ¾Ñ˜Ğµ\nĞ´Ğ°\nĞ´Ğ°ĞºĞ»Ğµ\nĞ´Ğ°Ğ½Ğ°\nĞ´Ğ°Ğ½Ğ°Ñ\nĞ´Ğ°Ñ˜\nĞ´Ğ²Ğ°\nĞ´Ğµ\nĞ´ĞµĞ´ĞµÑ€\nĞ´ĞµĞ»Ğ¸Ğ¼Ğ¸Ñ†Ğµ\nĞ´ĞµĞ»Ğ¸Ğ¼Ğ¸Ñ‡Ğ½Ğ¾\nĞ´ĞµĞ¼\nĞ´Ğ¾\nĞ´Ğ¾Ğ±Ğ°Ñ€\nĞ´Ğ¾Ğ±Ğ¸Ñ‚Ğ¸\nĞ´Ğ¾Ğ²ĞµÑ‡ĞµÑ€\nĞ´Ğ¾ĞºĞ»Ğµ\nĞ´Ğ¾Ğ»Ğµ\nĞ´Ğ¾Ğ½ĞµĞºĞ»Ğµ\nĞ´Ğ¾ÑĞ°Ğ´\nĞ´Ğ¾ÑĞºĞ¾Ñ€Ğ¾\nĞ´Ğ¾Ñ‚Ğ°Ğ´\nĞ´Ğ¾Ñ‚Ğ»Ğµ\nĞ´Ğ¾ÑˆĞ°Ğ¾\nĞ´Ğ¾Ñ›Ğ¸\nĞ´Ñ€ÑƒĞ³Ğ°Ğ¼Ğ¾\nĞ´Ñ€ÑƒĞ³Ğ´Ğµ\nĞ´Ñ€ÑƒĞ³Ğ¸\nĞµ\nĞµĞ²Ğ¾\nĞµĞ½Ğ¾\nĞµÑ‚Ğ¾\nĞµÑ…\nĞµÑ…Ğµ\nĞµÑ˜\nĞ¶ĞµĞ»ĞµĞ»Ğ°\nĞ¶ĞµĞ»ĞµĞ»Ğµ\nĞ¶ĞµĞ»ĞµĞ»Ğ¸\nĞ¶ĞµĞ»ĞµĞ»Ğ¾\nĞ¶ĞµĞ»ĞµÑ…\nĞ¶ĞµĞ»ĞµÑ›Ğ¸\nĞ¶ĞµĞ»Ğ¸\nĞ·Ğ°\nĞ·Ğ°Ğ¸ÑÑ‚Ğ°\nĞ·Ğ°Ñ€\nĞ·Ğ°Ñ‚Ğ¸Ğ¼\nĞ·Ğ°Ñ‚Ğ¾\nĞ·Ğ°Ñ…Ğ²Ğ°Ğ»Ğ¸Ñ‚Ğ¸\nĞ·Ğ°ÑˆÑ‚Ğ¾\nĞ·Ğ±Ğ¸Ñ™Ğ°\nĞ·Ğ¸Ğ¼ÑƒÑ\nĞ·Ğ½Ğ°Ñ‚Ğ¸\nĞ·ÑƒĞ¼\nĞ¸\nĞ¸Ğ´Ğµ\nĞ¸Ğ·\nĞ¸Ğ·Ğ²Ğ°Ğ½\nĞ¸Ğ·Ğ²Ğ¾Ğ»Ğ¸\nĞ¸Ğ·Ğ¼ĞµÑ’Ñƒ\nĞ¸Ğ·Ğ½Ğ°Ğ´\nĞ¸ĞºĞ°Ğ´Ğ°\nĞ¸ĞºĞ°ĞºĞ°Ğ²\nĞ¸ĞºĞ°ĞºĞ²Ğ°\nĞ¸ĞºĞ°ĞºĞ²Ğµ\nĞ¸ĞºĞ°ĞºĞ²Ğ¸\nĞ¸ĞºĞ°ĞºĞ²Ğ¸Ğ¼\nĞ¸ĞºĞ°ĞºĞ²Ğ¸Ğ¼Ğ°\nĞ¸ĞºĞ°ĞºĞ²Ğ¸Ñ…\nĞ¸ĞºĞ°ĞºĞ²Ğ¾\nĞ¸ĞºĞ°ĞºĞ²Ğ¾Ğ³\nĞ¸ĞºĞ°ĞºĞ²Ğ¾Ğ³Ğ°\nĞ¸ĞºĞ°ĞºĞ²Ğ¾Ğ¼\nĞ¸ĞºĞ°ĞºĞ²Ğ¾Ğ¼Ğµ\nĞ¸ĞºĞ°ĞºĞ²Ğ¾Ñ˜\nĞ¸Ğ»Ğ¸\nĞ¸Ğ¼\nĞ¸Ğ¼Ğ°\nĞ¸Ğ¼Ğ°Ğ¼\nĞ¸Ğ¼Ğ°Ğ¾\nĞ¸ÑĞ¿Ğ¾Ğ´\nĞ¸Ñ…\nĞ¸Ñ˜Ñƒ\nĞ¸Ñ›Ğ¸\nĞºĞ°Ğ´\nĞºĞ°Ğ´Ğ°\nĞºĞ¾Ğ³Ğ°\nĞºĞ¾Ñ˜ĞµĞºĞ°ĞºĞ°Ğ²\nĞºĞ¾Ñ˜Ğ¸Ğ¼Ğ°\nĞºĞ¾Ñ˜Ñƒ\nĞºÑ€Ğ¸ÑˆĞ¾Ğ¼\nĞ»Ğ°Ğ½Ğ¸\nĞ»Ğ¸\nĞ¼Ğ°Ğ»Ğ¸\nĞ¼Ğ°ÑšĞ¸\nĞ¼Ğµ\nĞ¼ĞµĞ½Ğµ\nĞ¼ĞµĞ½Ğ¸\nĞ¼Ğ¸\nĞ¼Ğ¸Ğ¼Ğ¾\nĞ¼Ğ¸ÑĞ»Ğ¸\nĞ¼Ğ½Ğ¾Ğ³Ğ¾\nĞ¼Ğ¾Ğ³Ñƒ\nĞ¼Ğ¾Ñ€Ğ°\nĞ¼Ğ¾Ñ€Ğ°Ğ¾\nĞ¼Ğ¾Ñ˜\nĞ¼Ğ¾Ñ˜Ğ°\nĞ¼Ğ¾Ñ˜Ğµ\nĞ¼Ğ¾Ñ˜Ğ¸\nĞ¼Ğ¾Ñ˜Ñƒ\nĞ¼Ğ¾Ñ›Ğ¸\nĞ¼Ñƒ\nĞ½Ğ°\nĞ½Ğ°Ğ´\nĞ½Ğ°ĞºĞ¾Ğ½\nĞ½Ğ°Ğ¼\nĞ½Ğ°Ğ¼Ğ°\nĞ½Ğ°Ñ\nĞ½Ğ°ÑˆĞ°\nĞ½Ğ°ÑˆĞµ\nĞ½Ğ°ÑˆĞµĞ³\nĞ½Ğ°ÑˆĞ¸\nĞ½Ğ°Ñ›Ğ¸\nĞ½Ğµ\nĞ½ĞµĞ³Ğ´Ğµ\nĞ½ĞµĞºĞ°\nĞ½ĞµĞºĞ°Ğ´\nĞ½ĞµĞºĞµ\nĞ½ĞµĞºĞ¾Ğ³\nĞ½ĞµĞºÑƒ\nĞ½ĞµĞ¼Ğ°\nĞ½ĞµĞ¼Ğ°Ğ¼\nĞ½ĞµĞºĞ¾\nĞ½ĞµÑ›Ğµ\nĞ½ĞµÑ›ĞµĞ¼Ğ¾\nĞ½ĞµÑ›ĞµÑ‚Ğµ\nĞ½ĞµÑ›ĞµÑˆ\nĞ½ĞµÑ›Ñƒ\nĞ½Ğ¸\nĞ½Ğ¸ĞºĞ°Ğ´Ğ°\nĞ½Ğ¸ĞºĞ¾Ğ³Ğ°\nĞ½Ğ¸ĞºĞ¾Ñ˜Ğµ\nĞ½Ğ¸ĞºĞ¾Ñ˜Ğ¸\nĞ½Ğ¸ĞºĞ¾Ñ˜Ñƒ\nĞ½Ğ¸ÑĞ°Ğ¼\nĞ½Ğ¸ÑĞ¸\nĞ½Ğ¸ÑÑ‚Ğµ\nĞ½Ğ¸ÑÑƒ\nĞ½Ğ¸ÑˆÑ‚Ğ°\nĞ½Ğ¸Ñ˜ĞµĞ´Ğ°Ğ½\nĞ½Ğ¾\nĞ¾\nĞ¾Ğ²Ğ°\nĞ¾Ğ²Ğ°ĞºĞ¾\nĞ¾Ğ²Ğ°Ğ¼Ğ¾\nĞ¾Ğ²Ğ°Ñ˜\nĞ¾Ğ²Ğ´Ğµ\nĞ¾Ğ²Ğµ\nĞ¾Ğ²Ğ¸Ğ¼\nĞ¾Ğ²Ğ¸Ğ¼Ğ°\nĞ¾Ğ²Ğ¾\nĞ¾Ğ²Ğ¾Ñ˜\nĞ¾Ğ´\nĞ¾Ğ´Ğ¼Ğ°Ñ…\nĞ¾ĞºĞ¾\nĞ¾ĞºĞ¾Ğ»Ğ¾\nĞ¾Ğ½\nĞ¾Ğ½Ğ°Ñ˜\nĞ¾Ğ½Ğµ\nĞ¾Ğ½Ğ¸Ğ¼\nĞ¾Ğ½Ğ¸Ğ¼Ğ°\nĞ¾Ğ½Ğ¾Ğ¼\nĞ¾Ğ½Ğ¾Ñ˜\nĞ¾Ğ½Ñƒ\nĞ¾ÑĞ¸Ğ¼\nĞ¾ÑÑ‚Ğ°Ğ»Ğ¸\nĞ¾Ñ‚Ğ¸ÑˆĞ°Ğ¾\nĞ¿Ğ°\nĞ¿Ğ°Ğº\nĞ¿Ğ¸Ñ‚Ğ°Ñ‚Ğ¸\nĞ¿Ğ¾\nĞ¿Ğ¾Ğ²Ğ¾Ğ´Ğ¾Ğ¼\nĞ¿Ğ¾Ğ´\nĞ¿Ğ¾Ğ´Ğ°Ñ™Ğµ\nĞ¿Ğ¾Ğ¶ĞµÑ™Ğ°Ğ½\nĞ¿Ğ¾Ğ¶ĞµÑ™Ğ½Ğ°\nĞ¿Ğ¾Ğ¸Ğ·Ğ´Ğ°Ñ™Ğµ\nĞ¿Ğ¾Ğ¸Ğ¼ĞµĞ½Ñ†Ğµ\nĞ¿Ğ¾Ğ½ĞµĞºĞ°Ğ´\nĞ¿Ğ¾Ğ¿Ñ€ĞµĞºĞ¾\nĞ¿Ğ¾Ñ€ĞµĞ´\nĞ¿Ğ¾ÑĞ»Ğµ\nĞ¿Ğ¾Ñ‚Ğ°Ğ¼Ğ°Ğ½\nĞ¿Ğ¾Ñ‚Ñ€Ğ±ÑƒÑˆĞºĞµ\nĞ¿Ğ¾ÑƒĞ·Ğ´Ğ°Ğ½Ğ¾\nĞ¿Ğ¾Ñ‡ĞµÑ‚Ğ°Ğº\nĞ¿Ğ¾Ñ˜ĞµĞ´Ğ¸Ğ½Ğ¸\nĞ¿Ñ€Ğ°Ğ²Ğ¸Ñ‚Ğ¸\nĞ¿Ñ€Ğ²Ğ¸\nĞ¿Ñ€ĞµĞºĞ¾\nĞ¿Ñ€ĞµĞ¼Ğ°\nĞ¿Ñ€Ğ¸Ñ˜Ğµ\nĞ¿ÑƒÑ‚\nĞ¿Ñ™ÑƒÑ\nÑ€Ğ°Ğ´Ğ¸Ñ˜Ğµ\nÑ\nÑĞ°\nÑĞ°Ğ²\nÑĞ°Ğ´Ğ°\nÑĞ°Ğ¼\nÑĞ°Ğ¼Ğ¾\nÑĞ°ÑĞ²Ğ¸Ğ¼\nÑĞ²Ğ°\nÑĞ²Ğ°ĞºĞ¸\nÑĞ²Ğ¸\nÑĞ²Ğ¸Ğ¼\nÑĞ²Ğ¾Ğ³\nÑĞ²Ğ¾Ğ¼\nÑĞ²Ğ¾Ñ˜\nÑĞ²Ğ¾Ñ˜Ğ°\nÑĞ²Ğ¾Ñ˜Ğµ\nÑĞ²Ğ¾Ñ˜Ñƒ\nÑĞ²Ñƒ\nÑĞ²ÑƒĞ³Ğ´Ğµ\nÑĞµ\nÑĞµĞ±Ğµ\nÑĞµĞ±Ğ¸\nÑĞ¸\nÑĞ¼ĞµÑ‚Ğ¸\nÑĞ¼Ğ¾\nÑÑ‚Ğ²Ğ°Ñ€\nÑÑ‚Ğ²Ğ°Ñ€Ğ½Ğ¾\nÑÑ‚Ğµ\nÑÑƒ\nÑÑƒÑ‚Ñ€Ğ°\nÑ‚Ğ°\nÑ‚Ğ°Ã¨Ğ½Ğ¾\nÑ‚Ğ°ĞºĞ¾\nÑ‚Ğ°ĞºĞ¾Ñ’Ğµ\nÑ‚Ğ°Ğ¼Ğ¾\nÑ‚Ğ²Ğ¾Ñ˜\nÑ‚Ğ²Ğ¾Ñ˜Ğ°\nÑ‚Ğ²Ğ¾Ñ˜Ğµ\nÑ‚Ğ²Ğ¾Ñ˜Ğ¸\nÑ‚Ğ²Ğ¾Ñ˜Ñƒ\nÑ‚Ğµ\nÑ‚ĞµĞ±Ğµ\nÑ‚ĞµĞ±Ğ¸\nÑ‚Ğ¸\nÑ‚Ğ¸Ğ¼Ğ°\nÑ‚Ğ¾\nÑ‚Ğ¾Ğ¼Ğµ\nÑ‚Ğ¾Ñ˜\nÑ‚Ñƒ\nÑƒ\nÑƒĞ²ĞµĞº\nÑƒĞ²Ğ¸Ñ˜ĞµĞº\nÑƒĞ·\nÑƒĞ·Ğ°\nÑƒĞ·Ğ°Ğ»ÑƒĞ´\nÑƒĞ·Ğ´ÑƒĞ¶\nÑƒĞ·ĞµÑ‚Ğ¸\nÑƒĞ¼Ğ°Ğ»Ğ¾\nÑƒĞ½ÑƒÑ‚Ñ€Ğ°\nÑƒĞ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ¸Ñ‚Ğ¸\nÑƒĞ¿Ñ€ĞºĞ¾Ñ\nÑƒÑ‡Ğ¸Ğ½Ğ¸Ğ¾\nÑƒÑ‡Ğ¸Ğ½Ğ¸Ñ‚Ğ¸\nÑ…Ğ°Ğ»Ğ¾\nÑ…Ğ²Ğ°Ğ»Ğ°\nÑ…ĞµÑ˜\nÑ…Ğ¼\nÑ…Ğ¾Ğ¿\nÑ…Ğ¾Ñ›Ğµ\nÑ…Ğ¾Ñ›ĞµĞ¼Ğ¾\nÑ…Ğ¾Ñ›ĞµÑ‚Ğµ\nÑ…Ğ¾Ñ›ĞµÑˆ\nÑ…Ğ¾Ñ›Ñƒ\nÑ…Ñ‚ĞµĞ´Ğ¾ÑÑ‚Ğµ\nÑ…Ñ‚ĞµĞ´Ğ¾Ñ…\nÑ…Ñ‚ĞµĞ´Ğ¾ÑˆĞµ\nÑ…Ñ‚ĞµĞ»Ğ°\nÑ…Ñ‚ĞµĞ»Ğµ\nÑ…Ñ‚ĞµĞ»Ğ¸\nÑ…Ñ‚ĞµĞ¾\nÑ…Ñ‚ĞµÑ˜Ğ°ÑĞ¼Ğ¾\nÑ…Ñ‚ĞµÑ˜Ğ°ÑÑ‚Ğµ\nÑ…Ñ‚ĞµÑ˜Ğ°Ñ…Ñƒ\nÑ…ÑƒÑ€Ğ°\nÑ‡ĞµÑÑ‚Ğ¾\nÑ‡Ğ¸Ñ˜ĞµĞ¼\nÑ‡Ğ¸Ñ˜Ğ¸\nÑ‡Ğ¸Ñ˜Ğ¸Ğ¼\nÑ‡Ğ¸Ñ˜Ğ¸Ğ¼Ğ°\nÑˆĞ¸Ñ†\nÑˆÑ‚Ğ°Ğ³Ğ¾Ğ´\nÑˆÑ‚Ğ¾\nÑˆÑ‚Ğ¾Ğ³Ğ¾Ğ´\nÑ˜Ğ°\nÑ˜Ğµ\nÑ˜ĞµĞ´Ğ°Ğ½\nÑ˜ĞµĞ´Ğ¸Ğ½Ğ¸\nÑ˜ĞµĞ´Ğ½Ğ°\nÑ˜ĞµĞ´Ğ½Ğµ\nÑ˜ĞµĞ´Ğ½Ğ¸\nÑ˜ĞµĞ´Ğ½Ğ¾\nÑ˜ĞµĞ´Ğ½Ğ¾Ğ¼\nÑ˜ĞµÑ€\nÑ˜ĞµÑĞ°Ğ¼\nÑ˜ĞµÑĞ¸\nÑ˜ĞµÑĞ¼Ğ¾\nÑ˜ĞµÑÑƒ\nÑ˜Ğ¸Ğ¼\nÑ˜Ğ¾Ñ˜\nÑ˜Ñƒ\nÑ˜ÑƒÑ‡Ğµ\nÑšĞµĞ³Ğ¾Ğ²Ğ°\nÑšĞµĞ³Ğ¾Ğ²Ğ¾\nÑšĞµĞ·Ğ¸Ğ½\nÑšĞµĞ·Ğ¸Ğ½Ğ°\nÑšĞµĞ·Ğ¸Ğ½Ğ¾\nÑšĞµĞ¼Ñƒ\nÑšĞµĞ½\nÑšĞ¸Ğ¼\nÑšĞ¸Ğ¼Ğ°\nÑšĞ¸Ñ…Ğ¾Ğ²Ğ°\nÑšĞ¸Ñ…Ğ¾Ğ²Ğ¾\nÑšĞ¾Ñ˜\nÑšÑƒ\nÑ›Ğµ\nÑ›ĞµĞ¼Ğ¾\nÑ›ĞµÑ‚Ğµ\nÑ›ĞµÑˆ\nÑ›Ñƒ\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/sr/norm_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/de/__init__.py----------------------------------------
A:spacy.lang.de.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.de.__init__.lex_attr_getters[NORM]->add_lookups(Language.Defaults.lex_attr_getters[NORM], NORM_EXCEPTIONS, BASE_NORMS)
A:spacy.lang.de.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS, TOKENIZER_EXCEPTIONS)
spacy.lang.de.__init__.German(Language)
spacy.lang.de.__init__.GermanDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/de/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/de/tokenizer_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/de/syntax_iterators.py----------------------------------------
A:spacy.lang.de.syntax_iterators.np_label->doc.vocab.strings.add('NP')
A:spacy.lang.de.syntax_iterators.np_deps->set((doc.vocab.strings.add(label) for label in labels))
A:spacy.lang.de.syntax_iterators.close_app->doc.vocab.strings.add('nk')
spacy.lang.de.syntax_iterators.noun_chunks(obj)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/de/stop_words.py----------------------------------------
A:spacy.lang.de.stop_words.STOP_WORDS->set('\nÃ¡ a ab aber ach acht achte achten achter achtes ag alle allein allem allen\naller allerdings alles allgemeinen als also am an andere anderen anderem andern\nanders auch auf aus ausser auÃŸer ausserdem auÃŸerdem\n\nbald bei beide beiden beim beispiel bekannt bereits besonders besser besten bin\nbis bisher bist\n\nda dabei dadurch dafÃ¼r dagegen daher dahin dahinter damals damit danach daneben\ndank dann daran darauf daraus darf darfst darin darÃ¼ber darum darunter das\ndasein daselbst dass daÃŸ dasselbe davon davor dazu dazwischen dein deine deinem\ndeiner dem dementsprechend demgegenÃ¼ber demgemÃ¤ss demgemÃ¤ÃŸ demselben demzufolge\nden denen denn denselben der deren derjenige derjenigen dermassen dermaÃŸen\nderselbe derselben des deshalb desselben dessen deswegen dich die diejenige\ndiejenigen dies diese dieselbe dieselben diesem diesen dieser dieses dir doch\ndort drei drin dritte dritten dritter drittes du durch durchaus dÃ¼rfen dÃ¼rft\ndurfte durften\n\neben ebenso ehrlich eigen eigene eigenen eigener eigenes ein einander eine\neinem einen einer eines einigeeinigen einiger einiges einmal einmaleins elf en\nende endlich entweder er erst erste ersten erster erstes es etwa etwas euch\n\nfrÃ¼her fÃ¼nf fÃ¼nfte fÃ¼nften fÃ¼nfter fÃ¼nftes fÃ¼r\n\ngab ganz ganze ganzen ganzer ganzes gar gedurft gegen gegenÃ¼ber gehabt gehen\ngeht gekannt gekonnt gemacht gemocht gemusst genug gerade gern gesagt geschweige\ngewesen gewollt geworden gibt ging gleich gott gross groÃŸ grosse groÃŸe grossen\ngroÃŸen grosser groÃŸer grosses groÃŸes gut gute guter gutes\n\nhabe haben habt hast hat hatte hÃ¤tte hatten hÃ¤tten heisst heiÃŸt her heute hier\nhin hinter hoch\n\nich ihm ihn ihnen ihr ihre ihrem ihren ihrer ihres im immer in indem\ninfolgedessen ins irgend ist\n\nja jahr jahre jahren je jede jedem jeden jeder jedermann jedermanns jedoch\njemand jemandem jemanden jene jenem jenen jener jenes jetzt\n\nkam kann kannst kaum kein keine keinem keinen keiner kleine kleinen kleiner\nkleines kommen kommt kÃ¶nnen kÃ¶nnt konnte kÃ¶nnte konnten kurz\n\nlang lange leicht leider lieber los\n\nmachen macht machte mag magst man manche manchem manchen mancher manches mehr\nmein meine meinem meinen meiner meines mensch menschen mich mir mit mittel\nmochte mÃ¶chte mochten mÃ¶gen mÃ¶glich mÃ¶gt morgen muss muÃŸ mÃ¼ssen musst mÃ¼sst\nmusste mussten\n\nna nach nachdem nahm natÃ¼rlich neben nein neue neuen neun neunte neunten neunter\nneuntes nicht nichts nie niemand niemandem niemanden noch nun nur\n\nob oben oder offen oft ohne\n\nrecht rechte rechten rechter rechtes richtig rund\n\nsagt sagte sah satt schlecht schon sechs sechste sechsten sechster sechstes\nsehr sei seid seien sein seine seinem seinen seiner seines seit seitdem selbst\nselbst sich sie sieben siebente siebenten siebenter siebentes siebte siebten\nsiebter siebtes sind so solang solche solchem solchen solcher solches soll\nsollen sollte sollten sondern sonst sowie spÃ¤ter statt\n\ntag tage tagen tat teil tel trotzdem tun\n\nÃ¼ber Ã¼berhaupt Ã¼brigens uhr um und uns unser unsere unserer unter\n\nvergangene vergangenen viel viele vielem vielen vielleicht vier vierte vierten\nvierter viertes vom von vor\n\nwahr wÃ¤hrend wÃ¤hrenddem wÃ¤hrenddessen wann war wÃ¤re waren wart warum was wegen\nweil weit weiter weitere weiteren weiteres welche welchem welchen welcher\nwelches wem wen wenig wenige weniger weniges wenigstens wenn wer werde werden\nwerdet wessen wie wieder will willst wir wird wirklich wirst wo wohl wollen\nwollt wollte wollten worden wurde wÃ¼rde wurden wÃ¼rden\n\nzehn zehnte zehnten zehnter zehntes zeit zu zuerst zugleich zum zunÃ¤chst zur\nzurÃ¼ck zusammen zwanzig zwar zwei zweite zweiten zweiter zweites zwischen\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/de/norm_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/de/tag_map.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/de/punctuation.py----------------------------------------
A:spacy.lang.de.punctuation._quotes->char_classes.CONCAT_QUOTES.replace("'", '')


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/he/__init__.py----------------------------------------
A:spacy.lang.he.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.he.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS)
spacy.lang.he.__init__.Hebrew(Language)
spacy.lang.he.__init__.HebrewDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/he/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/he/stop_words.py----------------------------------------
A:spacy.lang.he.stop_words.STOP_WORDS->set('\n×× ×™\n××ª\n××ª×”\n×× ×—× ×•\n××ª×Ÿ\n××ª×\n×”×\n×”×Ÿ\n×”×™×\n×”×•×\n×©×œ×™\n×©×œ×•\n×©×œ×š\n×©×œ×”\n×©×œ× ×•\n×©×œ×›×\n×©×œ×›×Ÿ\n×©×œ×”×\n×©×œ×”×Ÿ\n×œ×™\n×œ×•\n×œ×”\n×œ× ×•\n×œ×›×\n×œ×›×Ÿ\n×œ×”×\n×œ×”×Ÿ\n××•×ª×”\n××•×ª×•\n×–×”\n×–××ª\n××œ×”\n××œ×•\n×ª×—×ª\n××ª×—×ª\n××¢×œ\n×‘×™×Ÿ\n×¢×\n×¢×“\n× ×’×¨\n×¢×œ\n××œ\n××•×œ\n×©×œ\n××¦×œ\n×›××•\n××—×¨\n××•×ª×•\n×‘×œ×™\n×œ×¤× ×™\n××—×¨×™\n×××—×•×¨×™\n×¢×œ×™\n×¢×œ×™×•\n×¢×œ×™×”\n×¢×œ×™×š\n×¢×œ×™× ×•\n×¢×œ×™×›×\n×œ×¢×™×›×Ÿ\n×¢×œ×™×”×\n×¢×œ×™×”×Ÿ\n×›×œ\n×›×•×œ×\n×›×•×œ×Ÿ\n×›×š\n×›×›×”\n×›×–×”\n×–×”\n×–×•×ª\n××•×ª×™\n××•×ª×”\n××•×ª×\n××•×ª×š\n××•×ª×•\n××•×ª×Ÿ\n××•×ª× ×•\n×•××ª\n××ª\n××ª×›×\n××ª×›×Ÿ\n××™×ª×™\n××™×ª×•\n××™×ª×š\n××™×ª×”\n××™×ª×\n××™×ª×Ÿ\n××™×ª× ×•\n××™×ª×›×\n××™×ª×›×Ÿ\n×™×”×™×”\n×ª×”×™×”\n×”×™×ª×™\n×”×™×ª×”\n×”×™×”\n×œ×”×™×•×ª\n×¢×¦××™\n×¢×¦××•\n×¢×¦××”\n×¢×¦××\n×¢×¦××Ÿ\n×¢×¦×× ×•\n×¢×¦××”×\n×¢×¦××”×Ÿ\n××™\n××”\n××™×¤×”\n×”×™×›×Ÿ\n×‘××§×•× ×©×‘×•\n××\n×œ××Ÿ\n×œ××§×•× ×©×‘×•\n××§×•× ×‘×•\n××™×–×”\n××”×™×›×Ÿ\n××™×š\n×›×™×¦×“\n×‘××™×–×• ××™×“×”\n××ª×™\n×‘×©×¢×” ×©\n×›××©×¨\n×›×©\n×œ××¨×•×ª\n×œ×¤× ×™\n××—×¨×™\n×××™×–×• ×¡×™×‘×”\n×”×¡×™×‘×” ×©×‘×’×œ×œ×”\n×œ××”\n××“×•×¢\n×œ××™×–×• ×ª×›×œ×™×ª\n×›×™\n×™×©\n××™×Ÿ\n××š\n×× ×™×Ÿ\n×××™×Ÿ\n×××™×¤×”\n×™×›×œ\n×™×›×œ×”\n×™×›×œ×•\n×™×›×•×œ\n×™×›×•×œ×”\n×™×›×•×œ×™×\n×™×›×•×œ×•×ª\n×™×•×›×œ×•\n×™×•×›×œ\n××¡×•×’×œ\n×œ×\n×¨×§\n××•×œ×™\n××™×Ÿ\n×œ××•\n××™\n×›×œ×œ\n× ×’×“\n××\n×¢×\n××œ\n××œ×”\n××œ×•\n××£\n×¢×œ\n××¢×œ\n××ª×—×ª\n××¦×“\n×‘×©×‘×™×œ\n×œ×‘×™×Ÿ\n×‘×××¦×¢\n×‘×ª×•×š\n×“×¨×š\n××‘×¢×“\n×‘×××¦×¢×•×ª\n×œ××¢×œ×”\n×œ××˜×”\n××—×•×¥\n××Ÿ\n×œ×¢×‘×¨\n××›××Ÿ\n×›××Ÿ\n×”× ×”\n×”×¨×™\n×¤×”\n×©×\n××š\n×‘×¨×\n×©×•×‘\n××‘×œ\n××‘×œ×™\n×‘×œ×™\n××œ×‘×“\n×¨×§\n×‘×’×œ×œ\n××›×™×•×•×Ÿ\n×¢×“\n××©×¨\n×•××™×œ×•\n×œ××¨×•×ª\n××¡\n×›××•\n×›×¤×™\n××–\n××—×¨×™\n×›×Ÿ\n×œ×›×Ÿ\n×œ×¤×™×›×š\n×××“\n×¢×–\n××¢×˜\n××¢×˜×™×\n×‘××™×“×”\n×©×•×‘\n×™×•×ª×¨\n××“×™\n×’×\n×›×Ÿ\n× ×•\n××—×¨\n××—×¨×ª\n××—×¨×™×\n××—×¨×•×ª\n××©×¨\n××•\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/fa/__init__.py----------------------------------------
A:spacy.lang.fa.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.fa.__init__.lex_attr_getters[NORM]->add_lookups(Language.Defaults.lex_attr_getters[NORM], BASE_NORMS)
A:spacy.lang.fa.__init__.tokenizer_exceptions->update_exc(TOKENIZER_EXCEPTIONS)
spacy.lang.fa.__init__.Persian(Language)
spacy.lang.fa.__init__.PersianDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/fa/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/fa/lex_attrs.py----------------------------------------
A:spacy.lang.fa.lex_attrs._num_words->set('\nØµÙØ±\nÛŒÚ©\nØ¯Ùˆ\nØ³Ù‡\nÚ†Ù‡Ø§Ø±\nÙ¾Ù†Ø¬\nØ´Ø´\nØ´ÛŒØ´\nÙ‡ÙØª\nÙ‡Ø´Øª\nÙ†Ù‡\nØ¯Ù‡\nÛŒØ§Ø²Ø¯Ù‡\nØ¯ÙˆØ§Ø²Ø¯Ù‡\nØ³ÛŒØ²Ø¯Ù‡\nÚ†Ù‡Ø§Ø±Ø¯Ù‡\nÙ¾Ø§Ù†Ø²Ø¯Ù‡\nÙ¾ÙˆÙ†Ø²Ø¯Ù‡\nØ´Ø§Ù†Ø²Ø¯Ù‡\nØ´ÙˆÙ†Ø²Ø¯Ù‡\nÙ‡ÙØ¯Ù‡\nÙ‡Ø¬Ø¯Ù‡\nÙ‡ÛŒØ¬Ø¯Ù‡\nÙ†ÙˆØ²Ø¯Ù‡\nØ¨ÛŒØ³Øª\nØ³ÛŒ\nÚ†Ù‡Ù„\nÙ¾Ù†Ø¬Ø§Ù‡\nØ´ØµØª\nÙ‡ÙØªØ§Ø¯\nÙ‡Ø´ØªØ§Ø¯\nÙ†ÙˆØ¯\nØµØ¯\nÛŒÚ©ØµØ¯\nÛŒÚ©\u200cØµØ¯\nØ¯ÙˆÛŒØ³Øª\nØ³ÛŒØµØ¯\nÚ†Ù‡Ø§Ø±ØµØ¯\nÙ¾Ø§Ù†ØµØ¯\nÙ¾ÙˆÙ†ØµØ¯\nØ´Ø´ØµØ¯\nØ´ÛŒØ´ØµØ¯\nÙ‡ÙØªØµØ¯\nÙ‡ÙØµØ¯\nÙ‡Ø´ØªØµØ¯\nÙ†Ù‡ØµØ¯\nÙ‡Ø²Ø§Ø±\nÙ…ÛŒÙ„ÛŒÙˆÙ†\nÙ…ÛŒÙ„ÛŒØ§Ø±Ø¯\nØ¨ÛŒÙ„ÛŒÙˆÙ†\nØ¨ÛŒÙ„ÛŒØ§Ø±Ø¯\nØªØ±ÛŒÙ„ÛŒÙˆÙ†\nØªØ±ÛŒÙ„ÛŒØ§Ø±Ø¯\nÚ©ÙˆØ§Ø¯Ø±ÛŒÙ„ÛŒÙˆÙ†\nÚ©Ø§Ø¯Ø±ÛŒÙ„ÛŒØ§Ø±Ø¯\nÚ©ÙˆÛŒÙ†ØªÛŒÙ„ÛŒÙˆÙ†\n'.split())
A:spacy.lang.fa.lex_attrs._ordinal_words->set('\nØ§ÙˆÙ„\nØ³ÙˆÙ…\nØ³ÛŒ\u200cØ§Ù…'.split())
A:spacy.lang.fa.lex_attrs.text->text.replace(',', '').replace('.', '').replace('ØŒ', '').replace('Ù«', '').replace('/', '').replace(',', '').replace('.', '').replace('ØŒ', '').replace('Ù«', '').replace('/', '')
spacy.lang.fa.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/fa/tokenizer_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/fa/syntax_iterators.py----------------------------------------
A:spacy.lang.fa.syntax_iterators.conj->doc.vocab.strings.add('conj')
A:spacy.lang.fa.syntax_iterators.np_label->doc.vocab.strings.add('NP')
A:spacy.lang.fa.syntax_iterators.seen->set()
spacy.lang.fa.syntax_iterators.noun_chunks(obj)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/fa/stop_words.py----------------------------------------
A:spacy.lang.fa.stop_words.STOP_WORDS->set('\nÙˆ\nØ¯Ø±\nØ¨Ù‡\nØ§Ø²\nÚ©Ù‡\nØ§ÛŒÙ†\nØ±Ø§\nØ¨Ø§\nØ§Ø³Øª\nØ¨Ø±Ø§ÛŒ\nØ¢Ù†\nÛŒÚ©\nØ®ÙˆØ¯\nØªØ§\nÚ©Ø±Ø¯\nØ¨Ø±\nÙ‡Ù…\nÙ†ÛŒØ²\nÚ¯ÙØª\nÙ…ÛŒ\u200cØ´ÙˆØ¯\nÙˆÛŒ\nØ´Ø¯\nØ¯Ø§Ø±Ø¯\nÙ…Ø§\nØ§Ù…Ø§\nÛŒØ§\nØ´Ø¯Ù‡\nØ¨Ø§ÛŒØ¯\nÙ‡Ø±\nØ¢Ù†Ù‡Ø§\nØ¨ÙˆØ¯\nØ§Ùˆ\nØ¯ÛŒÚ¯Ø±\nØ¯Ùˆ\nÙ…ÙˆØ±Ø¯\nÙ…ÛŒ\u200cÚ©Ù†Ø¯\nØ´ÙˆØ¯\nÚ©Ù†Ø¯\nÙˆØ¬ÙˆØ¯\nØ¨ÛŒÙ†\nÙ¾ÛŒØ´\nØ´Ø¯Ù‡\u200cØ§Ø³Øª\nÙ¾Ø³\nÙ†Ø¸Ø±\nØ§Ú¯Ø±\nÙ‡Ù…Ù‡\nÛŒÚ©ÛŒ\nØ­Ø§Ù„\nÙ‡Ø³ØªÙ†Ø¯\nÙ…Ù†\nÚ©Ù†Ù†Ø¯\nÙ†ÛŒØ³Øª\nØ¨Ø§Ø´Ø¯\nÚ†Ù‡\nØ¨ÛŒ\nÙ…ÛŒ\nØ¨Ø®Ø´\nÙ…ÛŒ\u200cÚ©Ù†Ù†Ø¯\nÙ‡Ù…ÛŒÙ†\nØ§ÙØ²ÙˆØ¯\nÙ‡Ø§ÛŒÛŒ\nØ¯Ø§Ø±Ù†Ø¯\nØ±Ø§Ù‡\nÙ‡Ù…Ú†Ù†ÛŒÙ†\nØ±ÙˆÛŒ\nØ¯Ø§Ø¯\nØ¨ÛŒØ´ØªØ±\nØ¨Ø³ÛŒØ§Ø±\nØ³Ù‡\nØ¯Ø§Ø´Øª\nÚ†Ù†Ø¯\nØ³ÙˆÛŒ\nØªÙ†Ù‡Ø§\nÙ‡ÛŒÚ†\nÙ…ÛŒØ§Ù†\nØ§ÛŒÙ†Ú©Ù‡\nØ´Ø¯Ù†\nØ¨Ø¹Ø¯\nØ¬Ø¯ÛŒØ¯\nÙˆÙ„ÛŒ\nØ­ØªÛŒ\nÚ©Ø±Ø¯Ù†\nØ¨Ø±Ø®ÛŒ\nÚ©Ø±Ø¯Ù†Ø¯\nÙ…ÛŒ\u200cØ¯Ù‡Ø¯\nØ§ÙˆÙ„\nÙ†Ù‡\nÚ©Ø±Ø¯Ù‡\u200cØ§Ø³Øª\nÙ†Ø³Ø¨Øª\nØ¨ÛŒØ´\nØ´Ù…Ø§\nÚ†Ù†ÛŒÙ†\nØ·ÙˆØ±\nØ§ÙØ±Ø§Ø¯\nØªÙ…Ø§Ù…\nØ¯Ø±Ø¨Ø§Ø±Ù‡\nØ¨Ø§Ø±\nØ¨Ø³ÛŒØ§Ø±ÛŒ\nÙ…ÛŒ\u200cØªÙˆØ§Ù†Ø¯\nÚ©Ø±Ø¯Ù‡\nÚ†ÙˆÙ†\nÙ†Ø¯Ø§Ø±Ø¯\nØ¯ÙˆÙ…\nØ¨Ø²Ø±Ú¯\nØ·ÛŒ\nØ­Ø¯ÙˆØ¯\nÙ‡Ù…Ø§Ù†\nØ¨Ø¯ÙˆÙ†\nØ§Ù„Ø¨ØªÙ‡\nØ¢Ù†Ø§Ù†\nÙ…ÛŒ\u200cÚ¯ÙˆÛŒØ¯\nØ¯ÛŒÚ¯Ø±ÛŒ\nØ®ÙˆØ§Ù‡Ø¯\u200cØ´Ø¯\nÚ©Ù†ÛŒÙ…\nÙ‚Ø§Ø¨Ù„\nÛŒØ¹Ù†ÛŒ\nØ±Ø´Ø¯\nÙ…ÛŒ\u200cØªÙˆØ§Ù†\nÙˆØ§Ø±Ø¯\nÚ©Ù„\nÙˆÛŒÚ˜Ù‡\nÙ‚Ø¨Ù„\nØ¨Ø±Ø§Ø³Ø§Ø³\nÙ†ÛŒØ§Ø²\nÚ¯Ø°Ø§Ø±ÛŒ\nÙ‡Ù†ÙˆØ²\nÙ„Ø§Ø²Ù…\nØ³Ø§Ø²ÛŒ\nØ¨ÙˆØ¯Ù‡\u200cØ§Ø³Øª\nÚ†Ø±Ø§\nÙ…ÛŒ\u200cØ´ÙˆÙ†Ø¯\nÙˆÙ‚ØªÛŒ\nÚ¯Ø±ÙØª\nÚ©Ù…\nØ¬Ø§ÛŒ\nØ­Ø§Ù„ÛŒ\nØªØºÛŒÛŒØ±\nÙ¾ÛŒØ¯Ø§\nØ§Ú©Ù†ÙˆÙ†\nØªØ­Øª\nØ¨Ø§Ø¹Ø«\nÙ…Ø¯Øª\nÙÙ‚Ø·\nØ²ÛŒØ§Ø¯ÛŒ\nØªØ¹Ø¯Ø§Ø¯\nØ¢ÛŒØ§\nØ¨ÛŒØ§Ù†\nØ±Ùˆ\nØ´Ø¯Ù†Ø¯\nØ¹Ø¯Ù…\nÚ©Ø±Ø¯Ù‡\u200cØ§Ù†Ø¯\nØ¨ÙˆØ¯Ù†\nÙ†ÙˆØ¹\nØ¨Ù„Ú©Ù‡\nØ¬Ø§Ø±ÛŒ\nØ¯Ù‡Ø¯\nØ¨Ø±Ø§Ø¨Ø±\nÙ…Ù‡Ù…\nØ¨ÙˆØ¯Ù‡\nØ§Ø®ÛŒØ±\nÙ…Ø±Ø¨ÙˆØ·\nØ§Ù…Ø±\nØ²ÛŒØ±\nÚ¯ÛŒØ±ÛŒ\nØ´Ø§ÛŒØ¯\nØ®ØµÙˆØµ\nØ¢Ù‚Ø§ÛŒ\nØ§Ø«Ø±\nÚ©Ù†Ù†Ø¯Ù‡\nØ¨ÙˆØ¯Ù†Ø¯\nÙÚ©Ø±\nÚ©Ù†Ø§Ø±\nØ§ÙˆÙ„ÛŒÙ†\nØ³ÙˆÙ…\nØ³Ø§ÛŒØ±\nÚ©Ù†ÛŒØ¯\nØ¶Ù…Ù†\nÙ…Ø§Ù†Ù†Ø¯\nØ¨Ø§Ø²\nÙ…ÛŒ\u200cÚ¯ÛŒØ±Ø¯\nÙ…Ù…Ú©Ù†\nØ­Ù„\nØ¯Ø§Ø±Ø§ÛŒ\nÙ¾ÛŒ\nÙ…Ø«Ù„\nÙ…ÛŒ\u200cØ±Ø³Ø¯\nØ§Ø¬Ø±Ø§\nØ¯ÙˆØ±\nÙ…Ù†Ø¸ÙˆØ±\nÚ©Ø³ÛŒ\nÙ…ÙˆØ¬Ø¨\nØ·ÙˆÙ„\nØ§Ù…Ú©Ø§Ù†\nØ¢Ù†Ú†Ù‡\nØªØ¹ÛŒÛŒÙ†\nÚ¯ÙØªÙ‡\nØ´ÙˆÙ†Ø¯\nØ¬Ù…Ø¹\nØ®ÛŒÙ„ÛŒ\nØ¹Ù„Ø§ÙˆÙ‡\nÚ¯ÙˆÙ†Ù‡\nØªØ§Ú©Ù†ÙˆÙ†\nØ±Ø³ÛŒØ¯\nØ³Ø§Ù„Ù‡\nÚ¯Ø±ÙØªÙ‡\nØ´Ø¯Ù‡\u200cØ§Ù†Ø¯\nØ¹Ù„Øª\nÚ†Ù‡Ø§Ø±\nØ¯Ø§Ø´ØªÙ‡\u200cØ¨Ø§Ø´Ø¯\nØ®ÙˆØ§Ù‡Ø¯\u200cØ¨ÙˆØ¯\nØ·Ø±Ù\nØªÙ‡ÛŒÙ‡\nØªØ¨Ø¯ÛŒÙ„\nÙ…Ù†Ø§Ø³Ø¨\nØ²ÛŒØ±Ø§\nÙ…Ø´Ø®Øµ\nÙ…ÛŒ\u200cØªÙˆØ§Ù†Ù†Ø¯\nÙ†Ø²Ø¯ÛŒÚ©\nØ¬Ø±ÛŒØ§Ù†\nØ±ÙˆÙ†Ø¯\nØ¨Ù†Ø§Ø¨Ø±Ø§ÛŒÙ†\nÙ…ÛŒ\u200cØ¯Ù‡Ù†Ø¯\nÛŒØ§ÙØª\nÙ†Ø®Ø³ØªÛŒÙ†\nØ¨Ø§Ù„Ø§\nÙ¾Ù†Ø¬\nØ±ÛŒØ²ÛŒ\nØ¹Ø§Ù„ÛŒ\nÚ†ÛŒØ²ÛŒ\nÙ†Ø®Ø³Øª\nØ¨ÛŒØ´ØªØ±ÛŒ\nØªØ±ØªÛŒØ¨\nØ´Ø¯Ù‡\u200cØ¨ÙˆØ¯\nØ®Ø§Øµ\nØ®ÙˆØ¨ÛŒ\nØ®ÙˆØ¨\nØ´Ø±ÙˆØ¹\nÙØ±Ø¯\nÚ©Ø§Ù…Ù„\nØºÛŒØ±\nÙ…ÛŒ\u200cØ±ÙˆØ¯\nØ¯Ù‡Ù†Ø¯\nØ¢Ø®Ø±ÛŒÙ†\nØ¯Ø§Ø¯Ù†\nØ¬Ø¯ÛŒ\nØ¨Ù‡ØªØ±ÛŒÙ†\nØ´Ø§Ù…Ù„\nÚ¯ÛŒØ±Ø¯\nØ¨Ø®Ø´ÛŒ\nØ¨Ø§Ø´Ù†Ø¯\nØªÙ…Ø§Ù…ÛŒ\nØ¨Ù‡ØªØ±\nØ¯Ø§Ø¯Ù‡\u200cØ§Ø³Øª\nØ­Ø¯\nÙ†Ø¨ÙˆØ¯\nÚ©Ø³Ø§Ù†ÛŒ\nÙ…ÛŒ\u200cÚ©Ø±Ø¯\nØ¯Ø§Ø±ÛŒÙ…\nØ¹Ù„ÛŒÙ‡\nÙ…ÛŒ\u200cØ¨Ø§Ø´Ø¯\nØ¯Ø§Ù†Ø³Øª\nÙ†Ø§Ø´ÛŒ\nØ¯Ø§Ø´ØªÙ†Ø¯\nØ¯Ù‡Ù‡\nÙ…ÛŒ\u200cØ´Ø¯\nØ§ÛŒØ´Ø§Ù†\nØ¢Ù†Ø¬Ø§\nÚ¯Ø±ÙØªÙ‡\u200cØ§Ø³Øª\nØ¯Ú†Ø§Ø±\nÙ…ÛŒ\u200cØ¢ÛŒØ¯\nÙ„Ø­Ø§Ø¸\nØ¢Ù†Ú©Ù‡\nØ¯Ø§Ø¯Ù‡\nØ¨Ø¹Ø¶ÛŒ\nÙ‡Ø³ØªÛŒÙ…\nØ§Ù†Ø¯\nØ¨Ø±Ø¯Ø§Ø±ÛŒ\nÙ†Ø¨Ø§ÛŒØ¯\nÙ…ÛŒ\u200cÚ©Ù†ÛŒÙ…\nÙ†Ø´Ø³Øª\nØ³Ù‡Ù…\nÙ‡Ù…ÛŒØ´Ù‡\nØ¢Ù…Ø¯\nØ§Ø´\nÙˆÚ¯Ùˆ\nÙ…ÛŒ\u200cÚ©Ù†Ù…\nØ­Ø¯Ø§Ù‚Ù„\nØ·Ø¨Ù‚\nØ¬Ø§\nØ®ÙˆØ§Ù‡Ø¯\u200cÚ©Ø±Ø¯\nÙ†ÙˆØ¹ÛŒ\nÚ†Ú¯ÙˆÙ†Ù‡\nØ±ÙØª\nÙ‡Ù†Ú¯Ø§Ù…\nÙÙˆÙ‚\nØ±ÙˆØ´\nÙ†Ø¯Ø§Ø±Ù†Ø¯\nØ³Ø¹ÛŒ\nØ¨Ù†Ø¯ÛŒ\nØ´Ù…Ø§Ø±\nÚ©Ù„ÛŒ\nÚ©Ø§ÙÛŒ\nÙ…ÙˆØ§Ø¬Ù‡\nÙ‡Ù…Ú†Ù†Ø§Ù†\nØ²ÛŒØ§Ø¯\nØ³Ù…Øª\nÚ©ÙˆÚ†Ú©\nØ¯Ø§Ø´ØªÙ‡\u200cØ§Ø³Øª\nÚ†ÛŒØ²\nÙ¾Ø´Øª\nØ¢ÙˆØ±Ø¯\nØ­Ø§Ù„Ø§\nØ±ÙˆØ¨Ù‡\nØ³Ø§Ù„\u200cÙ‡Ø§ÛŒ\nØ¯Ø§Ø¯Ù†Ø¯\nÙ…ÛŒ\u200cÚ©Ø±Ø¯Ù†Ø¯\nØ¹Ù‡Ø¯Ù‡\nÙ†ÛŒÙ…Ù‡\nØ¬Ø§ÛŒÛŒ\nØ¯ÛŒÚ¯Ø±Ø§Ù†\nØ³ÛŒ\nØ¨Ø±ÙˆØ²\nÛŒÚ©Ø¯ÛŒÚ¯Ø±\nØ¢Ù…Ø¯Ù‡\u200cØ§Ø³Øª\nØ¬Ø²\nÚ©Ù†Ù…\nØ³Ù¾Ø³\nÚ©Ù†Ù†Ø¯Ú¯Ø§Ù†\nØ®ÙˆØ¯Ø´\nÙ‡Ù…ÙˆØ§Ø±Ù‡\nÛŒØ§ÙØªÙ‡\nØ´Ø§Ù†\nØµØ±Ù\nÙ†Ù…ÛŒ\u200cØ´ÙˆØ¯\nØ±Ø³ÛŒØ¯Ù†\nÚ†Ù‡Ø§Ø±Ù…\nÛŒØ§Ø¨Ø¯\nÙ…ØªØ±\nØ³Ø§Ø²\nØ¯Ø§Ø´ØªÙ‡\nÚ©Ø±Ø¯Ù‡\u200cØ¨ÙˆØ¯\nØ¨Ø§Ø±Ù‡\nÙ†Ø­ÙˆÙ‡\nÚ©Ø±Ø¯Ù…\nØªÙˆ\nØ´Ø®ØµÛŒ\nØ¯Ø§Ø´ØªÙ‡\u200cØ¨Ø§Ø´Ù†Ø¯\nÙ…Ø­Ø³ÙˆØ¨\nÙ¾Ø®Ø´\nÚ©Ù…ÛŒ\nÙ…ØªÙØ§ÙˆØª\nØ³Ø±Ø§Ø³Ø±\nÚ©Ø§Ù…Ù„Ø§\nØ¯Ø§Ø´ØªÙ†\nÙ†Ø¸ÛŒØ±\nØ¢Ù…Ø¯Ù‡\nÚ¯Ø±ÙˆÙ‡ÛŒ\nÙØ±Ø¯ÛŒ\nØ¹\nÙ‡Ù…Ú†ÙˆÙ†\nØ®Ø·Ø±\nØ®ÙˆÛŒØ´\nÚ©Ø¯Ø§Ù…\nØ¯Ø³ØªÙ‡\nØ³Ø¨Ø¨\nØ¹ÛŒÙ†\nØ¢ÙˆØ±ÛŒ\nÙ…ØªØ§Ø³ÙØ§Ù†Ù‡\nØ¨ÛŒØ±ÙˆÙ†\nØ¯Ø§Ø±\nØ§Ø¨ØªØ¯Ø§\nØ´Ø´\nØ§ÙØ±Ø§Ø¯ÛŒ\nÙ…ÛŒ\u200cÚ¯ÙˆÛŒÙ†Ø¯\nØ³Ø§Ù„Ù‡Ø§ÛŒ\nØ¯Ø±ÙˆÙ†\nÙ†ÛŒØ³ØªÙ†Ø¯\nÛŒØ§ÙØªÙ‡\u200cØ§Ø³Øª\nÙ¾Ø±\nØ®Ø§Ø·Ø±Ù†Ø´Ø§Ù†\nÚ¯Ø§Ù‡\nØ¬Ù…Ø¹ÛŒ\nØ§ØºÙ„Ø¨\nØ¯ÙˆØ¨Ø§Ø±Ù‡\nÙ…ÛŒ\u200cÛŒØ§Ø¨Ø¯\nÙ„Ø°Ø§\nØ²Ø§Ø¯Ù‡\nÚ¯Ø±Ø¯Ø¯\nØ§ÛŒÙ†Ø¬Ø§'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/fa/tag_map.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/fa/punctuation.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/fa/generate_verbs_exc.py----------------------------------------
A:spacy.lang.fa.generate_verbs_exc.verb_roots->'\n#Ù‡Ø³Øª\nØ¢Ø®Øª#Ø¢Ù‡Ù†Ø¬\nØ¢Ø±Ø§Ø³Øª#Ø¢Ø±Ø§\nØ¢Ø±Ø§Ù…Ø§Ù†Ø¯#Ø¢Ø±Ø§Ù…Ø§Ù†\nØ¢Ø±Ø§Ù…ÛŒØ¯#Ø¢Ø±Ø§Ù…\nØ¢Ø±Ù…ÛŒØ¯#Ø¢Ø±Ø§Ù…\nØ¢Ø²Ø±Ø¯#Ø¢Ø²Ø§Ø±\nØ¢Ø²Ù…ÙˆØ¯#Ø¢Ø²Ù…Ø§\nØ¢Ø³ÙˆØ¯#Ø¢Ø³Ø§\nØ¢Ø´Ø§Ù…ÛŒØ¯#Ø¢Ø´Ø§Ù…\nØ¢Ø´ÙØª#Ø¢Ø´ÙˆØ¨\nØ¢Ø´ÙˆØ¨ÛŒØ¯#Ø¢Ø´ÙˆØ¨\nØ¢ØºØ§Ø²ÛŒØ¯#Ø¢ØºØ§Ø²\nØ¢ØºØ´Øª#Ø¢Ù…ÛŒØ²\nØ¢ÙØ±ÛŒØ¯#Ø¢ÙØ±ÛŒÙ†\nØ¢Ù„ÙˆØ¯#Ø¢Ù„Ø§\nØ¢Ù…Ø¯#Ø¢\nØ¢Ù…Ø±Ø²ÛŒØ¯#Ø¢Ù…Ø±Ø²\nØ¢Ù…ÙˆØ®Øª#Ø¢Ù…ÙˆØ²\nØ¢Ù…ÙˆØ²Ø§Ù†Ø¯#Ø¢Ù…ÙˆØ²Ø§Ù†\nØ¢Ù…ÛŒØ®Øª#Ø¢Ù…ÛŒØ²\nØ¢ÙˆØ±Ø¯#Ø¢Ø±\nØ¢ÙˆØ±Ø¯#Ø¢ÙˆØ±\nØ¢ÙˆÛŒØ®Øª#Ø¢ÙˆÛŒØ²\nØ¢Ú©Ù†Ø¯#Ø¢Ú©Ù†\nØ¢Ú¯Ø§Ù‡Ø§Ù†ÛŒØ¯#Ø¢Ú¯Ø§Ù‡Ø§Ù†\nØ§Ø±Ø²ÛŒØ¯#Ø§Ø±Ø²\nØ§ÙØªØ§Ø¯#Ø§ÙØª\nØ§ÙØ±Ø§Ø®Øª#Ø§ÙØ±Ø§Ø²\nØ§ÙØ±Ø§Ø´Øª#Ø§ÙØ±Ø§Ø²\nØ§ÙØ±ÙˆØ®Øª#Ø§ÙØ±ÙˆØ²\nØ§ÙØ±ÙˆØ²ÛŒØ¯#Ø§ÙØ±ÙˆØ²\nØ§ÙØ²ÙˆØ¯#Ø§ÙØ²Ø§\nØ§ÙØ³Ø±Ø¯#Ø§ÙØ³Ø±\nØ§ÙØ´Ø§Ù†Ø¯#Ø§ÙØ´Ø§Ù†\nØ§ÙÚ©Ù†Ø¯#Ø§ÙÚ©Ù†\nØ§ÙÚ¯Ù†Ø¯#Ø§ÙÚ¯Ù†\nØ§Ù†Ø¨Ø§Ø´Øª#Ø§Ù†Ø¨Ø§Ø±\nØ§Ù†Ø¬Ø§Ù…ÛŒØ¯#Ø§Ù†Ø¬Ø§Ù…\nØ§Ù†Ø¯Ø§Ø®Øª#Ø§Ù†Ø¯Ø§Ø²\nØ§Ù†Ø¯ÙˆØ®Øª#Ø§Ù†Ø¯ÙˆØ²\nØ§Ù†Ø¯ÙˆØ¯#Ø§Ù†Ø¯Ø§\nØ§Ù†Ø¯ÛŒØ´ÛŒØ¯#Ø§Ù†Ø¯ÛŒØ´\nØ§Ù†Ú¯Ø§Ø´Øª#Ø§Ù†Ú¯Ø§Ø±\nØ§Ù†Ú¯ÛŒØ®Øª#Ø§Ù†Ú¯ÛŒØ²\nØ§Ù†Ú¯ÛŒØ²Ø§Ù†Ø¯#Ø§Ù†Ú¯ÛŒØ²Ø§Ù†\nØ§ÛŒØ³ØªØ§Ø¯#Ø§ÛŒØ³Øª\nØ§ÛŒØ³ØªØ§Ù†Ø¯#Ø§ÛŒØ³ØªØ§Ù†\nØ¨Ø§Ø®Øª#Ø¨Ø§Ø²\nØ¨Ø§Ø±Ø§Ù†Ø¯#Ø¨Ø§Ø±Ø§Ù†\nØ¨Ø§Ø±Ú¯Ø°Ø§Ø´Øª#Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±\nØ¨Ø§Ø±ÛŒØ¯#Ø¨Ø§Ø±\nØ¨Ø§Ø²#Ø¨Ø§Ø²Ø®ÙˆØ§Ù‡\nØ¨Ø§Ø²Ø¢ÙØ±ÛŒØ¯#Ø¨Ø§Ø²Ø¢ÙØ±ÛŒÙ†\nØ¨Ø§Ø²Ø¢Ù…Ø¯#Ø¨Ø§Ø²Ø¢\nØ¨Ø§Ø²Ø¢Ù…ÙˆØ®Øª#Ø¨Ø§Ø²Ø¢Ù…ÙˆØ²\nØ¨Ø§Ø²Ø¢ÙˆØ±Ø¯#Ø¨Ø§Ø²Ø¢ÙˆØ±\nØ¨Ø§Ø²Ø§ÛŒØ³ØªØ§Ø¯#Ø¨Ø§Ø²Ø§ÛŒØ³Øª\nØ¨Ø§Ø²ØªØ§Ø¨ÛŒØ¯#Ø¨Ø§Ø²ØªØ§Ø¨\nØ¨Ø§Ø²Ø¬Ø³Øª#Ø¨Ø§Ø²Ø¬Ùˆ\nØ¨Ø§Ø²Ø®ÙˆØ§Ù†Ø¯#Ø¨Ø§Ø²Ø®ÙˆØ§Ù†\nØ¨Ø§Ø²Ø®ÙˆØ±Ø§Ù†Ø¯#Ø¨Ø§Ø²Ø®ÙˆØ±Ø§Ù†\nØ¨Ø§Ø²Ø¯Ø§Ø¯#Ø¨Ø§Ø²Ø¯Ù‡\nØ¨Ø§Ø²Ø¯Ø§Ø´Øª#Ø¨Ø§Ø²Ø¯Ø§Ø±\nØ¨Ø§Ø²Ø±Ø³Ø§Ù†Ø¯#Ø¨Ø§Ø²Ø±Ø³Ø§Ù†\nØ¨Ø§Ø²Ø±Ø³Ø§Ù†ÛŒØ¯#Ø¨Ø§Ø²Ø±Ø³Ø§Ù†\nØ¨Ø§Ø²Ø²Ø¯#Ø¨Ø§Ø²Ø²Ù†\nØ¨Ø§Ø²Ø³ØªØ§Ù†Ø¯#Ø¨Ø§Ø²Ø³ØªØ§Ù†\nØ¨Ø§Ø²Ø´Ù…Ø§Ø±Ø¯#Ø¨Ø§Ø²Ø´Ù…Ø§Ø±\nØ¨Ø§Ø²Ø´Ù…Ø±Ø¯#Ø¨Ø§Ø²Ø´Ù…Ø§Ø±\nØ¨Ø§Ø²Ø´Ù…Ø±Ø¯#Ø¨Ø§Ø²Ø´Ù…Ø±\nØ¨Ø§Ø²Ø´Ù†Ø§Ø®Øª#Ø¨Ø§Ø²Ø´Ù†Ø§Ø³\nØ¨Ø§Ø²Ø´Ù†Ø§Ø³Ø§Ù†Ø¯#Ø¨Ø§Ø²Ø´Ù†Ø§Ø³Ø§Ù†\nØ¨Ø§Ø²ÙØ±Ø³ØªØ§Ø¯#Ø¨Ø§Ø²ÙØ±Ø³Øª\nØ¨Ø§Ø²Ù…Ø§Ù†Ø¯#Ø¨Ø§Ø²Ù…Ø§Ù†\nØ¨Ø§Ø²Ù†Ø´Ø³Øª#Ø¨Ø§Ø²Ù†Ø´ÛŒÙ†\nØ¨Ø§Ø²Ù†Ù…Ø§ÛŒØ§Ù†Ø¯#Ø¨Ø§Ø²Ù†Ù…Ø§ÛŒØ§Ù†\nØ¨Ø§Ø²Ù†Ù‡Ø§Ø¯#Ø¨Ø§Ø²Ù†Ù‡\nØ¨Ø§Ø²Ù†Ú¯Ø±ÛŒØ³Øª#Ø¨Ø§Ø²Ù†Ú¯Ø±\nØ¨Ø§Ø²Ù¾Ø±Ø³ÛŒØ¯#Ø¨Ø§Ø²Ù¾Ø±Ø³\nØ¨Ø§Ø²Ú¯Ø°Ø§Ø±Ø¯#Ø¨Ø§Ø²Ú¯Ø°Ø§Ø±\nØ¨Ø§Ø²Ú¯Ø°Ø§Ø´Øª#Ø¨Ø§Ø²Ú¯Ø°Ø§Ø±\nØ¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†Ø¯#Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†\nØ¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†ÛŒØ¯#Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†\nØ¨Ø§Ø²Ú¯Ø±Ø¯ÛŒØ¯#Ø¨Ø§Ø²Ú¯Ø±Ø¯\nØ¨Ø§Ø²Ú¯Ø±ÙØª#Ø¨Ø§Ø²Ú¯ÛŒØ±\nØ¨Ø§Ø²Ú¯Ø´Øª#Ø¨Ø§Ø²Ú¯Ø±Ø¯\nØ¨Ø§Ø²Ú¯Ø´ÙˆØ¯#Ø¨Ø§Ø²Ú¯Ø´Ø§\nØ¨Ø§Ø²Ú¯ÙØª#Ø¨Ø§Ø²Ú¯Ùˆ\nØ¨Ø§Ø²ÛŒØ§ÙØª#Ø¨Ø§Ø²ÛŒØ§Ø¨\nØ¨Ø§ÙØª#Ø¨Ø§Ù\nØ¨Ø§Ù„ÛŒØ¯#Ø¨Ø§Ù„\nØ¨Ø§ÙˆØ±Ø§Ù†Ø¯#Ø¨Ø§ÙˆØ±Ø§Ù†\nØ¨Ø§ÛŒØ³Øª#Ø¨Ø§ÛŒØ¯\nØ¨Ø®Ø´ÙˆØ¯#Ø¨Ø®Ø´\nØ¨Ø®Ø´ÙˆØ¯#Ø¨Ø®Ø´Ø§\nØ¨Ø®Ø´ÛŒØ¯#Ø¨Ø®Ø´\nØ¨Ø±#Ø¨Ø±Ø®ÙˆØ§Ù‡\nØ¨Ø±Ø¢Ø´ÙØª#Ø¨Ø±Ø¢Ø´ÙˆØ¨\nØ¨Ø±Ø¢Ù…Ø¯#Ø¨Ø±Ø¢\nØ¨Ø±Ø¢ÙˆØ±Ø¯#Ø¨Ø±Ø¢ÙˆØ±\nØ¨Ø±Ø§Ø²ÛŒØ¯#Ø¨Ø±Ø§Ø²\nØ¨Ø±Ø§ÙØªØ§Ø¯#Ø¨Ø±Ø§ÙØª\nØ¨Ø±Ø§ÙØ±Ø§Ø®Øª#Ø¨Ø±Ø§ÙØ±Ø§Ø²\nØ¨Ø±Ø§ÙØ±Ø§Ø´Øª#Ø¨Ø±Ø§ÙØ±Ø§Ø²\nØ¨Ø±Ø§ÙØ±ÙˆØ®Øª#Ø¨Ø±Ø§ÙØ±ÙˆØ²\nØ¨Ø±Ø§ÙØ´Ø§Ù†Ø¯#Ø¨Ø±Ø§ÙØ´Ø§Ù†\nØ¨Ø±Ø§ÙÚ©Ù†Ø¯#Ø¨Ø±Ø§ÙÚ©Ù†\nØ¨Ø±Ø§Ù†Ø¯#Ø¨Ø±Ø§Ù†\nØ¨Ø±Ø§Ù†Ø¯Ø§Ø®Øª#Ø¨Ø±Ø§Ù†Ø¯Ø§Ø²\nØ¨Ø±Ø§Ù†Ú¯ÛŒØ®Øª#Ø¨Ø±Ø§Ù†Ú¯ÛŒØ²\nØ¨Ø±Ø¨Ø³Øª#Ø¨Ø±Ø¨Ù†Ø¯\nØ¨Ø±ØªØ§Ø¨Ø§Ù†Ø¯#Ø¨Ø±ØªØ§Ø¨Ø§Ù†\nØ¨Ø±ØªØ§Ø¨ÛŒØ¯#Ø¨Ø±ØªØ§Ø¨\nØ¨Ø±ØªØ§ÙØª#Ø¨Ø±ØªØ§Ø¨\nØ¨Ø±ØªÙ†ÛŒØ¯#Ø¨Ø±ØªÙ†\nØ¨Ø±Ø¬Ù‡ÛŒØ¯#Ø¨Ø±Ø¬Ù‡\nØ¨Ø±Ø®Ø§Ø³Øª#Ø¨Ø±Ø®ÛŒØ²\nØ¨Ø±Ø®ÙˆØ±Ø¯#Ø¨Ø±Ø®ÙˆØ±\nØ¨Ø±Ø¯#Ø¨Ø±\nØ¨Ø±Ø¯Ø§Ø´Øª#Ø¨Ø±Ø¯Ø§Ø±\nØ¨Ø±Ø¯Ù…ÛŒØ¯#Ø¨Ø±Ø¯Ù…\nØ¨Ø±Ø²Ø¯#Ø¨Ø±Ø²Ù†\nØ¨Ø±Ø´Ø¯#Ø¨Ø±Ø´Ùˆ\nØ¨Ø±Ø´Ù…Ø§Ø±Ø¯#Ø¨Ø±Ø´Ù…Ø§Ø±\nØ¨Ø±Ø´Ù…Ø±Ø¯#Ø¨Ø±Ø´Ù…Ø§Ø±\nØ¨Ø±Ø´Ù…Ø±Ø¯#Ø¨Ø±Ø´Ù…Ø±\nØ¨Ø±Ù†Ø´Ø§Ù†Ø¯#Ø¨Ø±Ù†Ø´Ø§Ù†\nØ¨Ø±Ù†Ø´Ø§Ù†ÛŒØ¯#Ø¨Ø±Ù†Ø´Ø§Ù†\nØ¨Ø±Ù†Ø´Ø³Øª#Ø¨Ø±Ù†Ø´ÛŒÙ†\nØ¨Ø±Ù†Ù‡Ø§Ø¯#Ø¨Ø±Ù†Ù‡\nØ¨Ø±Ú†ÛŒØ¯#Ø¨Ø±Ú†ÛŒÙ†\nØ¨Ø±Ú©Ø±Ø¯#Ø¨Ø±Ú©Ù†\nØ¨Ø±Ú©Ø´ÛŒØ¯#Ø¨Ø±Ú©Ø´\nØ¨Ø±Ú©Ù†Ø¯#Ø¨Ø±Ú©Ù†\nØ¨Ø±Ú¯Ø°Ø´Øª#Ø¨Ø±Ú¯Ø°Ø±\nØ¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯#Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†\nØ¨Ø±Ú¯Ø±Ø¯Ø§Ù†ÛŒØ¯#Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†\nØ¨Ø±Ú¯Ø±Ø¯ÛŒØ¯#Ø¨Ø±Ú¯Ø±Ø¯\nØ¨Ø±Ú¯Ø±ÙØª#Ø¨Ø±Ú¯ÛŒØ±\nØ¨Ø±Ú¯Ø²ÛŒØ¯#Ø¨Ø±Ú¯Ø²ÛŒÙ†\nØ¨Ø±Ú¯Ø´Øª#Ø¨Ø±Ú¯Ø±Ø¯\nØ¨Ø±Ú¯Ø´ÙˆØ¯#Ø¨Ø±Ú¯Ø´Ø§\nØ¨Ø±Ú¯Ù…Ø§Ø±Ø¯#Ø¨Ø±Ú¯Ù…Ø§Ø±\nØ¨Ø±Ú¯Ù…Ø§Ø±ÛŒØ¯#Ø¨Ø±Ú¯Ù…Ø§Ø±\nØ¨Ø±Ú¯Ù…Ø§Ø´Øª#Ø¨Ø±Ú¯Ù…Ø§Ø±\nØ¨Ø±ÛŒØ¯#Ø¨Ø±\nØ¨Ø³Øª#Ø¨Ù†Ø¯\nØ¨Ù„Ø¹ÛŒØ¯#Ø¨Ù„Ø¹\nØ¨ÙˆØ¯#Ø¨Ø§Ø´\nØ¨ÙˆØ³ÛŒØ¯#Ø¨ÙˆØ³\nØ¨ÙˆÛŒÛŒØ¯#Ø¨Ùˆ\nØ¨ÛŒØ®Øª#Ø¨ÛŒØ²\nØ¨ÛŒØ®Øª#Ø¨ÙˆØ²\nØªØ§Ø¨Ø§Ù†Ø¯#ØªØ§Ø¨Ø§Ù†\nØªØ§Ø¨ÛŒØ¯#ØªØ§Ø¨\nØªØ§Ø®Øª#ØªØ§Ø²\nØªØ§Ø±Ø§Ù†Ø¯#ØªØ§Ø±Ø§Ù†\nØªØ§Ø²Ø§Ù†Ø¯#ØªØ§Ø²Ø§Ù†\nØªØ§Ø²ÛŒØ¯#ØªØ§Ø²\nØªØ§ÙØª#ØªØ§Ø¨\nØªØ±Ø§Ø¯ÛŒØ³ÛŒØ¯#ØªØ±Ø§Ø¯ÛŒØ³\nØªØ±Ø§Ø´Ø§Ù†Ø¯#ØªØ±Ø§Ø´Ø§Ù†\nØªØ±Ø§Ø´ÛŒØ¯#ØªØ±Ø§Ø´\nØªØ±Ø§ÙˆÛŒØ¯#ØªØ±Ø§Ùˆ\nØªØ±Ø³Ø§Ù†Ø¯#ØªØ±Ø³Ø§Ù†\nØªØ±Ø³ÛŒØ¯#ØªØ±Ø³\nØªØ±Ø´Ø§Ù†Ø¯#ØªØ±Ø´Ø§Ù†\nØªØ±Ø´ÛŒØ¯#ØªØ±Ø´\nØªØ±Ú©Ø§Ù†Ø¯#ØªØ±Ú©Ø§Ù†\nØªØ±Ú©ÛŒØ¯#ØªØ±Ú©\nØªÙØªÛŒØ¯#ØªÙØª\nØªÙ…Ø±Ú¯ÛŒØ¯#ØªÙ…Ø±Ú¯\nØªÙ†ÛŒØ¯#ØªÙ†\nØªÙˆØ§Ù†Ø³Øª#ØªÙˆØ§Ù†\nØªÙˆÙÛŒØ¯#ØªÙˆÙ\nØªÙ¾Ø§Ù†Ø¯#ØªÙ¾Ø§Ù†\nØªÙ¾ÛŒØ¯#ØªÙ¾\nØªÚ©Ø§Ù†Ø¯#ØªÚ©Ø§Ù†\nØªÚ©Ø§Ù†ÛŒØ¯#ØªÚ©Ø§Ù†\nØ¬Ø³Øª#Ø¬Ù‡\nØ¬Ø³Øª#Ø¬Ùˆ\nØ¬Ù†Ø¨Ø§Ù†Ø¯#Ø¬Ù†Ø¨Ø§Ù†\nØ¬Ù†Ø¨ÛŒØ¯#Ø¬Ù†Ø¨\nØ¬Ù†Ú¯ÛŒØ¯#Ø¬Ù†Ú¯\nØ¬Ù‡Ø§Ù†Ø¯#Ø¬Ù‡Ø§Ù†\nØ¬Ù‡ÛŒØ¯#Ø¬Ù‡\nØ¬ÙˆØ´Ø§Ù†Ø¯#Ø¬ÙˆØ´Ø§Ù†\nØ¬ÙˆØ´Ø§Ù†ÛŒØ¯#Ø¬ÙˆØ´Ø§Ù†\nØ¬ÙˆØ´ÛŒØ¯#Ø¬ÙˆØ´\nØ¬ÙˆÙŠØ¯#Ø¬Ùˆ\nØ¬ÙˆÛŒØ¯#Ø¬Ùˆ\nØ®Ø§Ø±Ø§Ù†Ø¯#Ø®Ø§Ø±Ø§Ù†\nØ®Ø§Ø±ÛŒØ¯#Ø®Ø§Ø±\nØ®Ø§Ø³Øª#Ø®ÛŒØ²\nØ®Ø§ÛŒÛŒØ¯#Ø®Ø§\nØ®Ø±Ø§Ø´Ø§Ù†Ø¯#Ø®Ø±Ø§Ø´Ø§Ù†\nØ®Ø±Ø§Ø´ÛŒØ¯#Ø®Ø±Ø§Ø´\nØ®Ø±Ø§Ù…ÛŒØ¯#Ø®Ø±Ø§Ù…\nØ®Ø±ÙˆØ´ÛŒØ¯#Ø®Ø±ÙˆØ´\nØ®Ø±ÛŒØ¯#Ø®Ø±\nØ®Ø²ÛŒØ¯#Ø®Ø²\nØ®Ø³Ø¨ÛŒØ¯#Ø®Ø³Ø¨\nØ®Ø´Ú©Ø§Ù†Ø¯#Ø®Ø´Ú©Ø§Ù†\nØ®Ø´Ú©ÛŒØ¯#Ø®Ø´Ú©\nØ®ÙØª#Ø®ÙˆØ§Ø¨\nØ®Ù„ÛŒØ¯#Ø®Ù„\nØ®Ù…Ø§Ù†Ø¯#Ø®Ù…Ø§Ù†\nØ®Ù…ÛŒØ¯#Ø®Ù…\nØ®Ù†Ø¯Ø§Ù†Ø¯#Ø®Ù†Ø¯Ø§Ù†\nØ®Ù†Ø¯Ø§Ù†ÛŒØ¯#Ø®Ù†Ø¯Ø§Ù†\nØ®Ù†Ø¯ÛŒØ¯#Ø®Ù†Ø¯\nØ®ÙˆØ§Ø¨Ø§Ù†Ø¯#Ø®ÙˆØ§Ø¨Ø§Ù†\nØ®ÙˆØ§Ø¨Ø§Ù†ÛŒØ¯#Ø®ÙˆØ§Ø¨Ø§Ù†\nØ®ÙˆØ§Ø¨ÛŒØ¯#Ø®ÙˆØ§Ø¨\nØ®ÙˆØ§Ø³Øª#Ø®ÙˆØ§Ù‡\nØ®ÙˆØ§Ø³Øª#Ø®ÛŒØ²\nØ®ÙˆØ§Ù†Ø¯#Ø®ÙˆØ§Ù†\nØ®ÙˆØ±Ø§Ù†Ø¯#Ø®ÙˆØ±Ø§Ù†\nØ®ÙˆØ±Ø¯#Ø®ÙˆØ±\nØ®ÛŒØ²Ø§Ù†Ø¯#Ø®ÛŒØ²Ø§Ù†\nØ®ÛŒØ³Ø§Ù†Ø¯#Ø®ÛŒØ³Ø§Ù†\nØ¯Ø§Ø¯#Ø¯Ù‡\nØ¯Ø§Ø´Øª#Ø¯Ø§Ø±\nØ¯Ø§Ù†Ø³Øª#Ø¯Ø§Ù†\nØ¯Ø±#Ø¯Ø±Ø®ÙˆØ§Ù‡\nØ¯Ø±Ø¢Ù…Ø¯#Ø¯Ø±Ø¢\nØ¯Ø±Ø¢Ù…ÛŒØ®Øª#Ø¯Ø±Ø¢Ù…ÛŒØ²\nØ¯Ø±Ø¢ÙˆØ±Ø¯#Ø¯Ø±Ø¢ÙˆØ±\nØ¯Ø±Ø¢ÙˆÛŒØ®Øª#Ø¯Ø±Ø¢ÙˆÛŒØ²\nØ¯Ø±Ø§ÙØªØ§Ø¯#Ø¯Ø±Ø§ÙØª\nØ¯Ø±Ø§ÙÚ©Ù†Ø¯#Ø¯Ø±Ø§ÙÚ©Ù†\nØ¯Ø±Ø§Ù†Ø¯Ø§Ø®Øª#Ø¯Ø±Ø§Ù†Ø¯Ø§Ø²\nØ¯Ø±Ø§Ù†ÛŒØ¯#Ø¯Ø±Ø§Ù†\nØ¯Ø±Ø¨Ø±Ø¯#Ø¯Ø±Ø¨Ø±\nØ¯Ø±Ø¨Ø±Ú¯Ø±ÙØª#Ø¯Ø±Ø¨Ø±Ú¯ÛŒØ±\nØ¯Ø±Ø®Ø´Ø§Ù†Ø¯#Ø¯Ø±Ø®Ø´Ø§Ù†\nØ¯Ø±Ø®Ø´Ø§Ù†ÛŒØ¯#Ø¯Ø±Ø®Ø´Ø§Ù†\nØ¯Ø±Ø®Ø´ÛŒØ¯#Ø¯Ø±Ø®Ø´\nØ¯Ø±Ø¯Ø§Ø¯#Ø¯Ø±Ø¯Ù‡\nØ¯Ø±Ø±ÙØª#Ø¯Ø±Ø±Ùˆ\nØ¯Ø±Ù…Ø§Ù†Ø¯#Ø¯Ø±Ù…Ø§Ù†\nØ¯Ø±Ù†Ù…ÙˆØ¯#Ø¯Ø±Ù†Ù…Ø§\nØ¯Ø±Ù†ÙˆØ±Ø¯ÛŒØ¯#Ø¯Ø±Ù†ÙˆØ±Ø¯\nØ¯Ø±ÙˆØ¯#Ø¯Ø±Ùˆ\nØ¯Ø±ÙˆÛŒØ¯#Ø¯Ø±Ùˆ\nØ¯Ø±Ú©Ø±Ø¯#Ø¯Ø±Ú©Ù†\nØ¯Ø±Ú©Ø´ÛŒØ¯#Ø¯Ø±Ú©Ø´\nØ¯Ø±Ú¯Ø°Ø´Øª#Ø¯Ø±Ú¯Ø°Ø±\nØ¯Ø±Ú¯Ø±ÙØª#Ø¯Ø±Ú¯ÛŒØ±\nØ¯Ø±ÛŒØ§ÙØª#Ø¯Ø±ÛŒØ§Ø¨\nØ¯Ø±ÛŒØ¯#Ø¯Ø±\nØ¯Ø²Ø¯ÛŒØ¯#Ø¯Ø²Ø¯\nØ¯Ù…ÛŒØ¯#Ø¯Ù…\nØ¯ÙˆØ§Ù†Ø¯#Ø¯ÙˆØ§Ù†\nØ¯ÙˆØ®Øª#Ø¯ÙˆØ²\nØ¯ÙˆØ´ÛŒØ¯#Ø¯ÙˆØ´\nØ¯ÙˆÛŒØ¯#Ø¯Ùˆ\nØ¯ÛŒØ¯#Ø¨ÛŒÙ†\nØ±Ø§Ù†Ø¯#Ø±Ø§Ù†\nØ±Ø¨ÙˆØ¯#Ø±Ø¨Ø§\nØ±Ø¨ÙˆØ¯#Ø±ÙˆØ¨\nØ±Ø®Ø´ÛŒØ¯#Ø±Ø®Ø´\nØ±Ø³Ø§Ù†Ø¯#Ø±Ø³Ø§Ù†\nØ±Ø³Ø§Ù†ÛŒØ¯#Ø±Ø³Ø§Ù†\nØ±Ø³Øª#Ø±Ù‡\nØ±Ø³Øª#Ø±Ùˆ\nØ±Ø³ÛŒØ¯#Ø±Ø³\nØ±Ø´Øª#Ø±ÛŒØ³\nØ±ÙØª#Ø±Ùˆ\nØ±ÙØª#Ø±ÙˆØ¨\nØ±Ù‚ØµØ§Ù†Ø¯#Ø±Ù‚ØµØ§Ù†\nØ±Ù‚ØµÛŒØ¯#Ø±Ù‚Øµ\nØ±Ù…Ø§Ù†Ø¯#Ø±Ù…Ø§Ù†\nØ±Ù…Ø§Ù†ÛŒØ¯#Ø±Ù…Ø§Ù†\nØ±Ù…ÛŒØ¯#Ø±Ù…\nØ±Ù†Ø¬Ø§Ù†Ø¯#Ø±Ù†Ø¬Ø§Ù†\nØ±Ù†Ø¬Ø§Ù†ÛŒØ¯#Ø±Ù†Ø¬Ø§Ù†\nØ±Ù†Ø¬ÛŒØ¯#Ø±Ù†Ø¬\nØ±Ù†Ø¯ÛŒØ¯#Ø±Ù†Ø¯\nØ±Ù‡Ø§Ù†Ø¯#Ø±Ù‡Ø§Ù†\nØ±Ù‡Ø§Ù†ÛŒØ¯#Ø±Ù‡Ø§Ù†\nØ±Ù‡ÛŒØ¯#Ø±Ù‡\nØ±ÙˆØ¨ÛŒØ¯#Ø±ÙˆØ¨\nØ±ÙˆÙØª#Ø±ÙˆØ¨\nØ±ÙˆÛŒØ§Ù†Ø¯#Ø±ÙˆÛŒØ§Ù†\nØ±ÙˆÛŒØ§Ù†ÛŒØ¯#Ø±ÙˆÛŒØ§Ù†\nØ±ÙˆÛŒÛŒØ¯#Ø±Ùˆ\nØ±ÙˆÛŒÛŒØ¯#Ø±ÙˆÛŒ\nØ±ÛŒØ®Øª#Ø±ÛŒØ²\nØ±ÛŒØ¯#Ø±ÛŒÙ†\nØ±ÛŒØ¯Ù†#Ø±ÛŒÙ†\nØ±ÛŒØ³ÛŒØ¯#Ø±ÛŒØ³\nØ²Ø§Ø¯#Ø²Ø§\nØ²Ø§Ø±ÛŒØ¯#Ø²Ø§Ø±\nØ²Ø§ÛŒØ§Ù†Ø¯#Ø²Ø§ÛŒØ§Ù†\nØ²Ø§ÛŒÛŒØ¯#Ø²Ø§\nØ²Ø¯#Ø²Ù†\nØ²Ø¯ÙˆØ¯#Ø²Ø¯Ø§\nØ²ÛŒØ³Øª#Ø²ÛŒ\nØ³Ø§Ø¨Ø§Ù†Ø¯#Ø³Ø§Ø¨Ø§Ù†\nØ³Ø§Ø¨ÛŒØ¯#Ø³Ø§Ø¨\nØ³Ø§Ø®Øª#Ø³Ø§Ø²\nØ³Ø§ÛŒÛŒØ¯#Ø³Ø§\nØ³ØªØ§Ø¯#Ø³ØªØ§Ù†\nØ³ØªØ§Ù†Ø¯#Ø³ØªØ§Ù†\nØ³ØªØ±Ø¯#Ø³ØªØ±\nØ³ØªÙˆØ¯#Ø³ØªØ§\nØ³ØªÛŒØ²ÛŒØ¯#Ø³ØªÛŒØ²\nØ³Ø±Ø§Ù†Ø¯#Ø³Ø±Ø§Ù†\nØ³Ø±Ø§ÛŒÛŒØ¯#Ø³Ø±Ø§\nØ³Ø±Ø´Øª#Ø³Ø±Ø´\nØ³Ø±ÙˆØ¯#Ø³Ø±Ø§\nØ³Ø±Ú©Ø´ÛŒØ¯#Ø³Ø±Ú©Ø´\nØ³Ø±Ú¯Ø±ÙØª#Ø³Ø±Ú¯ÛŒØ±\nØ³Ø±ÛŒØ¯#Ø³Ø±\nØ³Ø²ÛŒØ¯#Ø³Ø²\nØ³ÙØª#Ø³Ù†Ø¨\nØ³Ù†Ø¬ÛŒØ¯#Ø³Ù†Ø¬\nØ³ÙˆØ®Øª#Ø³ÙˆØ²\nØ³ÙˆØ¯#Ø³Ø§\nØ³ÙˆØ²Ø§Ù†Ø¯#Ø³ÙˆØ²Ø§Ù†\nØ³Ù¾Ø§Ø±Ø¯#Ø³Ù¾Ø§Ø±\nØ³Ù¾Ø±Ø¯#Ø³Ù¾Ø§Ø±\nØ³Ù¾Ø±Ø¯#Ø³Ù¾Ø±\nØ³Ù¾ÙˆØ®Øª#Ø³Ù¾ÙˆØ²\nØ³Ú¯Ø§Ù„ÛŒØ¯#Ø³Ú¯Ø§Ù„\nØ´Ø§Ø´ÛŒØ¯#Ø´Ø§Ø´\nØ´Ø§ÛŒØ³Øª#\nØ´Ø§ÛŒØ³Øª#Ø´Ø§ÛŒØ¯\nØ´ØªØ§Ø¨Ø§Ù†Ø¯#Ø´ØªØ§Ø¨Ø§Ù†\nØ´ØªØ§Ø¨ÛŒØ¯#Ø´ØªØ§Ø¨\nØ´ØªØ§ÙØª#Ø´ØªØ§Ø¨\nØ´Ø¯#Ø´Ùˆ\nØ´Ø³Øª#Ø´Ùˆ\nØ´Ø³Øª#Ø´ÙˆÛŒ\nØ´Ù„ÛŒØ¯#Ø´Ù„\nØ´Ù…Ø§Ø±#Ø´Ù…Ø±\nØ´Ù…Ø§Ø±Ø¯#Ø´Ù…Ø§Ø±\nØ´Ù…Ø±Ø¯#Ø´Ù…Ø§Ø±\nØ´Ù…Ø±Ø¯#Ø´Ù…Ø±\nØ´Ù†Ø§Ø®Øª#Ø´Ù†Ø§Ø³\nØ´Ù†Ø§Ø³Ø§Ù†Ø¯#Ø´Ù†Ø§Ø³Ø§Ù†\nØ´Ù†ÙØª#Ø´Ù†Ùˆ\nØ´Ù†ÛŒØ¯#Ø´Ù†Ùˆ\nØ´ÙˆØªÛŒØ¯#Ø´ÙˆØª\nØ´ÙˆØ±Ø§Ù†Ø¯#Ø´ÙˆØ±Ø§Ù†\nØ´ÙˆØ±ÛŒØ¯#Ø´ÙˆØ±\nØ´Ú©Ø§ÙØª#Ø´Ú©Ø§Ù\nØ´Ú©Ø§Ù†Ø¯#Ø´Ú©Ø§Ù†\nØ´Ú©Ø§Ù†Ø¯#Ø´Ú©Ù†\nØ´Ú©Ø³Øª#Ø´Ú©Ù†\nØ´Ú©ÙØª#Ø´Ú©Ù\nØ·Ù„Ø¨ÛŒØ¯#Ø·Ù„Ø¨\nØ·Ù¾ÛŒØ¯#Ø·Ù¾\nØºØ±Ø§Ù†Ø¯#ØºØ±Ø§Ù†\nØºØ±ÛŒØ¯#ØºØ±\nØºÙ„ØªØ§Ù†Ø¯#ØºÙ„ØªØ§Ù†\nØºÙ„ØªØ§Ù†ÛŒØ¯#ØºÙ„ØªØ§Ù†\nØºÙ„ØªÛŒØ¯#ØºÙ„Øª\nØºÙ„Ø·Ø§Ù†Ø¯#ØºÙ„Ø·Ø§Ù†\nØºÙ„Ø·Ø§Ù†ÛŒØ¯#ØºÙ„Ø·Ø§Ù†\nØºÙ„Ø·ÛŒØ¯#ØºÙ„Ø·\nÙØ±Ø§#ÙØ±Ø§Ø®ÙˆØ§Ù‡\nÙØ±Ø§Ø®ÙˆØ§Ù†Ø¯#ÙØ±Ø§Ø®ÙˆØ§Ù†\nÙØ±Ø§Ø¯Ø§Ø´Øª#ÙØ±Ø§Ø¯Ø§Ø±\nÙØ±Ø§Ø±Ø³ÛŒØ¯#ÙØ±Ø§Ø±Ø³\nÙØ±Ø§Ù†Ù…ÙˆØ¯#ÙØ±Ø§Ù†Ù…Ø§\nÙØ±Ø§Ú¯Ø±ÙØª#ÙØ±Ø§Ú¯ÛŒØ±\nÙØ±Ø³ØªØ§Ø¯#ÙØ±Ø³Øª\nÙØ±Ø³ÙˆØ¯#ÙØ±Ø³Ø§\nÙØ±Ù…ÙˆØ¯#ÙØ±Ù…Ø§\nÙØ±Ù‡ÛŒØ®Øª#ÙØ±Ù‡ÛŒØ²\nÙØ±Ùˆ#ÙØ±ÙˆØ®ÙˆØ§Ù‡\nÙØ±ÙˆØ¢Ù…Ø¯#ÙØ±ÙˆØ¢\nÙØ±ÙˆØ¢ÙˆØ±Ø¯#ÙØ±ÙˆØ¢ÙˆØ±\nÙØ±ÙˆØ§ÙØªØ§Ø¯#ÙØ±ÙˆØ§ÙØª\nÙØ±ÙˆØ§ÙÚ©Ù†Ø¯#ÙØ±ÙˆØ§ÙÚ©Ù†\nÙØ±ÙˆØ¨Ø±Ø¯#ÙØ±ÙˆØ¨Ø±\nÙØ±ÙˆØ¨Ø³Øª#ÙØ±ÙˆØ¨Ù†Ø¯\nÙØ±ÙˆØ®Øª#ÙØ±ÙˆØ´\nÙØ±ÙˆØ®ÙØª#ÙØ±ÙˆØ®ÙˆØ§Ø¨\nÙØ±ÙˆØ®ÙˆØ±Ø¯#ÙØ±ÙˆØ®ÙˆØ±\nÙØ±ÙˆØ¯Ø§Ø¯#ÙØ±ÙˆØ¯Ù‡\nÙØ±ÙˆØ¯ÙˆØ®Øª#ÙØ±ÙˆØ¯ÙˆØ²\nÙØ±ÙˆØ±ÙØª#ÙØ±ÙˆØ±Ùˆ\nÙØ±ÙˆØ±ÛŒØ®Øª#ÙØ±ÙˆØ±ÛŒØ²\nÙØ±ÙˆØ´Ú©Ø³Øª#ÙØ±ÙˆØ´Ú©Ù†\nÙØ±ÙˆÙØ±Ø³ØªØ§Ø¯#ÙØ±ÙˆÙØ±Ø³Øª\nÙØ±ÙˆÙ…Ø§Ù†Ø¯#ÙØ±ÙˆÙ…Ø§Ù†\nÙØ±ÙˆÙ†Ø´Ø§Ù†Ø¯#ÙØ±ÙˆÙ†Ø´Ø§Ù†\nÙØ±ÙˆÙ†Ø´Ø§Ù†ÛŒØ¯#ÙØ±ÙˆÙ†Ø´Ø§Ù†\nÙØ±ÙˆÙ†Ø´Ø³Øª#ÙØ±ÙˆÙ†Ø´ÛŒÙ†\nÙØ±ÙˆÙ†Ù…ÙˆØ¯#ÙØ±ÙˆÙ†Ù…Ø§\nÙØ±ÙˆÙ†Ù‡Ø§Ø¯#ÙØ±ÙˆÙ†Ù‡\nÙØ±ÙˆÙ¾Ø§Ø´Ø§Ù†Ø¯#ÙØ±ÙˆÙ¾Ø§Ø´Ø§Ù†\nÙØ±ÙˆÙ¾Ø§Ø´ÛŒØ¯#ÙØ±ÙˆÙ¾Ø§Ø´\nÙØ±ÙˆÚ†Ú©ÛŒØ¯#ÙØ±ÙˆÚ†Ú©\nÙØ±ÙˆÚ©Ø±Ø¯#ÙØ±ÙˆÚ©Ù†\nÙØ±ÙˆÚ©Ø´ÛŒØ¯#ÙØ±ÙˆÚ©Ø´\nÙØ±ÙˆÚ©ÙˆØ¨ÛŒØ¯#ÙØ±ÙˆÚ©ÙˆØ¨\nÙØ±ÙˆÚ©ÙˆÙØª#ÙØ±ÙˆÚ©ÙˆØ¨\nÙØ±ÙˆÚ¯Ø°Ø§Ø±Ø¯#ÙØ±ÙˆÚ¯Ø°Ø§Ø±\nÙØ±ÙˆÚ¯Ø°Ø§Ø´Øª#ÙØ±ÙˆÚ¯Ø°Ø§Ø±\nÙØ±ÙˆÚ¯Ø±ÙØª#ÙØ±ÙˆÚ¯ÛŒØ±\nÙØ±ÛŒÙØª#ÙØ±ÛŒØ¨\nÙØ´Ø§Ù†Ø¯#ÙØ´Ø§Ù†\nÙØ´Ø±Ø¯#ÙØ´Ø§Ø±\nÙØ´Ø±Ø¯#ÙØ´Ø±\nÙÙ„Ø³ÙÛŒØ¯#ÙÙ„Ø³Ù\nÙÙ‡Ù…Ø§Ù†Ø¯#ÙÙ‡Ù…Ø§Ù†\nÙÙ‡Ù…ÛŒØ¯#ÙÙ‡Ù…\nÙ‚Ø§Ù¾ÛŒØ¯#Ù‚Ø§Ù¾\nÙ‚Ø¨ÙˆÙ„Ø§Ù†Ø¯#Ù‚Ø¨ÙˆÙ„\nÙ‚Ø¨ÙˆÙ„Ø§Ù†Ø¯#Ù‚Ø¨ÙˆÙ„Ø§Ù†\nÙ„Ø§Ø³ÛŒØ¯#Ù„Ø§Ø³\nÙ„Ø±Ø²Ø§Ù†Ø¯#Ù„Ø±Ø²Ø§Ù†\nÙ„Ø±Ø²ÛŒØ¯#Ù„Ø±Ø²\nÙ„ØºØ²Ø§Ù†Ø¯#Ù„ØºØ²Ø§Ù†\nÙ„ØºØ²ÛŒØ¯#Ù„ØºØ²\nÙ„Ù…Ø¨Ø§Ù†Ø¯#Ù„Ù…Ø¨Ø§Ù†\nÙ„Ù…ÛŒØ¯#Ù„Ù…\nÙ„Ù†Ú¯ÛŒØ¯#Ù„Ù†Ú¯\nÙ„ÙˆÙ„ÛŒØ¯#Ù„ÙˆÙ„\nÙ„ÛŒØ³ÛŒØ¯#Ù„ÛŒØ³\nÙ…Ø§Ø³ÛŒØ¯#Ù…Ø§Ø³\nÙ…Ø§Ù„Ø§Ù†Ø¯#Ù…Ø§Ù„Ø§Ù†\nÙ…Ø§Ù„ÛŒØ¯#Ù…Ø§Ù„\nÙ…Ø§Ù†Ø¯#Ù…Ø§Ù†\nÙ…Ø§Ù†Ø³Øª#Ù…Ø§Ù†\nÙ…Ø±Ø¯#Ù…ÛŒØ±\nÙ…ÙˆÛŒÛŒØ¯#Ù…Ùˆ\nÙ…Ú©ÛŒØ¯#Ù…Ú©\nÙ†Ø§Ø²ÛŒØ¯#Ù†Ø§Ø²\nÙ†Ø§Ù„Ø§Ù†Ø¯#Ù†Ø§Ù„Ø§Ù†\nÙ†Ø§Ù„ÛŒØ¯#Ù†Ø§Ù„\nÙ†Ø§Ù…ÛŒØ¯#Ù†Ø§Ù…\nÙ†Ø´Ø§Ù†Ø¯#Ù†Ø´Ø§Ù†\nÙ†Ø´Ø³Øª#Ù†Ø´ÛŒÙ†\nÙ†Ù…Ø§ÛŒØ§Ù†Ø¯#Ù†Ù…Ø§\nÙ†Ù…Ø§ÛŒØ§Ù†Ø¯#Ù†Ù…Ø§ÛŒØ§Ù†\nÙ†Ù…ÙˆØ¯#Ù†Ù…Ø§\nÙ†Ù‡Ø§Ø¯#Ù†Ù‡\nÙ†Ù‡ÙØª#Ù†Ù‡Ù†Ø¨\nÙ†ÙˆØ§Ø®Øª#Ù†ÙˆØ§Ø²\nÙ†ÙˆØ§Ø²ÛŒØ¯#Ù†ÙˆØ§Ø²\nÙ†ÙˆØ±Ø¯ÛŒØ¯#Ù†ÙˆØ±Ø¯\nÙ†ÙˆØ´Ø§Ù†Ø¯#Ù†ÙˆØ´Ø§Ù†\nÙ†ÙˆØ´Ø§Ù†ÛŒØ¯#Ù†ÙˆØ´Ø§Ù†\nÙ†ÙˆØ´Øª#Ù†ÙˆÛŒØ³\nÙ†ÙˆØ´ÛŒØ¯#Ù†ÙˆØ´\nÙ†Ú©ÙˆÙ‡ÛŒØ¯#Ù†Ú©ÙˆÙ‡\nÙ†Ú¯Ø§Ø´Øª#Ù†Ú¯Ø§Ø±\nÙ†Ú¯Ø±ÛŒØ¯#\nÙ†Ú¯Ø±ÛŒØ³Øª#Ù†Ú¯Ø±\nÙ‡Ø±Ø§Ø³Ø§Ù†Ø¯#Ù‡Ø±Ø§Ø³Ø§Ù†\nÙ‡Ø±Ø§Ø³Ø§Ù†ÛŒØ¯#Ù‡Ø±Ø§Ø³Ø§Ù†\nÙ‡Ø±Ø§Ø³ÛŒØ¯#Ù‡Ø±Ø§Ø³\nÙ‡Ø´Øª#Ù‡Ù„\nÙˆØ§#ÙˆØ§Ø®ÙˆØ§Ù‡\nÙˆØ§Ø¯Ø§Ø´Øª#ÙˆØ§Ø¯Ø§Ø±\nÙˆØ§Ø±ÙØª#ÙˆØ§Ø±Ùˆ\nÙˆØ§Ø±Ù‡Ø§Ù†Ø¯#ÙˆØ§Ø±Ù‡Ø§Ù†\nÙˆØ§Ù…Ø§Ù†Ø¯#ÙˆØ§Ù…Ø§Ù†\nÙˆØ§Ù†Ù‡Ø§Ø¯#ÙˆØ§Ù†Ù‡\nÙˆØ§Ú©Ø±Ø¯#ÙˆØ§Ú©Ù†\nÙˆØ§Ú¯Ø°Ø§Ø±Ø¯#ÙˆØ§Ú¯Ø°Ø§Ø±\nÙˆØ§Ú¯Ø°Ø§Ø´Øª#ÙˆØ§Ú¯Ø°Ø§Ø±\nÙˆØ±#ÙˆØ±Ø®ÙˆØ§Ù‡\nÙˆØ±Ø¢Ù…Ø¯#ÙˆØ±Ø¢\nÙˆØ±Ø§ÙØªØ§Ø¯#ÙˆØ±Ø§ÙØª\nÙˆØ±Ø±ÙØª#ÙˆØ±Ø±Ùˆ\nÙˆØ±Ø²ÛŒØ¯#ÙˆØ±Ø²\nÙˆØ²Ø§Ù†Ø¯#ÙˆØ²Ø§Ù†\nÙˆØ²ÛŒØ¯#ÙˆØ²\nÙˆÛŒØ±Ø§Ø³Øª#ÙˆÛŒØ±Ø§\nÙ¾Ø§Ø´Ø§Ù†Ø¯#Ù¾Ø§Ø´Ø§Ù†\nÙ¾Ø§Ø´ÛŒØ¯#Ù¾Ø§Ø´\nÙ¾Ø§Ù„ÙˆØ¯#Ù¾Ø§Ù„Ø§\nÙ¾Ø§ÛŒÛŒØ¯#Ù¾Ø§\nÙ¾Ø®Øª#Ù¾Ø²\nÙ¾Ø°ÛŒØ±Ø§Ù†Ø¯#Ù¾Ø°ÛŒØ±Ø§Ù†\nÙ¾Ø°ÛŒØ±ÙØª#Ù¾Ø°ÛŒØ±\nÙ¾Ø±Ø§Ù†Ø¯#Ù¾Ø±Ø§Ù†\nÙ¾Ø±Ø§Ú©Ù†Ø¯#Ù¾Ø±Ø§Ú©Ù†\nÙ¾Ø±Ø¯Ø§Ø®Øª#Ù¾Ø±Ø¯Ø§Ø²\nÙ¾Ø±Ø³ØªÛŒØ¯#Ù¾Ø±Ø³Øª\nÙ¾Ø±Ø³ÛŒØ¯#Ù¾Ø±Ø³\nÙ¾Ø±Ù‡ÛŒØ®Øª#Ù¾Ø±Ù‡ÛŒØ²\nÙ¾Ø±Ù‡ÛŒØ²ÛŒØ¯#Ù¾Ø±Ù‡ÛŒØ²\nÙ¾Ø±ÙˆØ±Ø§Ù†Ø¯#Ù¾Ø±ÙˆØ±Ø§Ù†\nÙ¾Ø±ÙˆØ±Ø¯#Ù¾Ø±ÙˆØ±\nÙ¾Ø±ÛŒØ¯#Ù¾Ø±\nÙ¾Ø³Ù†Ø¯ÛŒØ¯#Ù¾Ø³Ù†Ø¯\nÙ¾Ù„Ø§Ø³Ø§Ù†Ø¯#Ù¾Ù„Ø§Ø³Ø§Ù†\nÙ¾Ù„Ø§Ø³ÛŒØ¯#Ù¾Ù„Ø§Ø³\nÙ¾Ù„Ú©ÛŒØ¯#Ù¾Ù„Ú©\nÙ¾Ù†Ø§Ù‡Ø§Ù†Ø¯#Ù¾Ù†Ø§Ù‡Ø§Ù†\nÙ¾Ù†Ø§Ù‡ÛŒØ¯#Ù¾Ù†Ø§Ù‡\nÙ¾Ù†Ø¯Ø§Ø´Øª#Ù¾Ù†Ø¯Ø§Ø±\nÙ¾ÙˆØ³Ø§Ù†Ø¯#Ù¾ÙˆØ³Ø§Ù†\nÙ¾ÙˆØ³ÛŒØ¯#Ù¾ÙˆØ³\nÙ¾ÙˆØ´Ø§Ù†Ø¯#Ù¾ÙˆØ´Ø§Ù†\nÙ¾ÙˆØ´ÛŒØ¯#Ù¾ÙˆØ´\nÙ¾ÙˆÛŒÛŒØ¯#Ù¾Ùˆ\nÙ¾Ú˜Ù…Ø±Ø¯#Ù¾Ú˜Ù…Ø±\nÙ¾Ú˜ÙˆÙ‡ÛŒØ¯#Ù¾Ú˜ÙˆÙ‡\nÙ¾Ú©ÛŒØ¯#Ù¾Ú©\nÙ¾ÛŒØ±Ø§Ø³Øª#Ù¾ÛŒØ±Ø§\nÙ¾ÛŒÙ…ÙˆØ¯#Ù¾ÛŒÙ…Ø§\nÙ¾ÛŒÙˆØ³Øª#Ù¾ÛŒÙˆÙ†Ø¯\nÙ¾ÛŒÚ†Ø§Ù†Ø¯#Ù¾ÛŒÚ†Ø§Ù†\nÙ¾ÛŒÚ†Ø§Ù†ÛŒØ¯#Ù¾ÛŒÚ†Ø§Ù†\nÙ¾ÛŒÚ†ÛŒØ¯#Ù¾ÛŒÚ†\nÚ†Ø§Ù¾ÛŒØ¯#Ú†Ø§Ù¾\nÚ†Ø§ÛŒÛŒØ¯#Ú†Ø§\nÚ†Ø±Ø§Ù†Ø¯#Ú†Ø±Ø§Ù†\nÚ†Ø±Ø§Ù†ÛŒØ¯#Ú†Ø±Ø§Ù†\nÚ†Ø±Ø¨Ø§Ù†Ø¯#Ú†Ø±Ø¨Ø§Ù†\nÚ†Ø±Ø¨ÛŒØ¯#Ú†Ø±Ø¨\nÚ†Ø±Ø®Ø§Ù†Ø¯#Ú†Ø±Ø®Ø§Ù†\nÚ†Ø±Ø®Ø§Ù†ÛŒØ¯#Ú†Ø±Ø®Ø§Ù†\nÚ†Ø±Ø®ÛŒØ¯#Ú†Ø±Ø®\nÚ†Ø±ÙˆÚ©ÛŒØ¯#Ú†Ø±ÙˆÚ©\nÚ†Ø±ÛŒØ¯#Ú†Ø±\nÚ†Ø²Ø§Ù†Ø¯#Ú†Ø²Ø§Ù†\nÚ†Ø³Ø¨Ø§Ù†Ø¯#Ú†Ø³Ø¨Ø§Ù†\nÚ†Ø³Ø¨ÛŒØ¯#Ú†Ø³Ø¨\nÚ†Ø³ÛŒØ¯#Ú†Ø³\nÚ†Ø´Ø§Ù†Ø¯#Ú†Ø´Ø§Ù†\nÚ†Ø´ÛŒØ¯#Ú†Ø´\nÚ†Ù„Ø§Ù†Ø¯#Ú†Ù„Ø§Ù†\nÚ†Ù„Ø§Ù†ÛŒØ¯#Ú†Ù„Ø§Ù†\nÚ†Ù¾Ø§Ù†Ø¯#Ú†Ù¾Ø§Ù†\nÚ†Ù¾ÛŒØ¯#Ú†Ù¾\nÚ†Ú©Ø§Ù†Ø¯#Ú†Ú©Ø§Ù†\nÚ†Ú©ÛŒØ¯#Ú†Ú©\nÚ†ÛŒØ¯#Ú†ÛŒÙ†\nÚ©Ø§Ø³Øª#Ú©Ø§Ù‡\nÚ©Ø§Ø´Øª#Ú©Ø§Ø±\nÚ©Ø§ÙˆÛŒØ¯#Ú©Ø§Ùˆ\nÚ©Ø±Ø¯#Ú©Ù†\nÚ©Ø´Ø§Ù†Ø¯#Ú©Ø´Ø§Ù†\nÚ©Ø´Ø§Ù†ÛŒØ¯#Ú©Ø´Ø§Ù†\nÚ©Ø´Øª#Ú©Ø§Ø±\nÚ©Ø´Øª#Ú©Ø´\nÚ©Ø´ÛŒØ¯#Ú©Ø´\nÚ©Ù†Ø¯#Ú©Ù†\nÚ©ÙˆØ¨Ø§Ù†Ø¯#Ú©ÙˆØ¨Ø§Ù†\nÚ©ÙˆØ¨ÛŒØ¯#Ú©ÙˆØ¨\nÚ©ÙˆØ´ÛŒØ¯#Ú©ÙˆØ´\nÚ©ÙˆÙØª#Ú©ÙˆØ¨\nÚ©ÙˆÚ†Ø§Ù†ÛŒØ¯#Ú©ÙˆÚ†Ø§Ù†\nÚ©ÙˆÚ†ÛŒØ¯#Ú©ÙˆÚ†\nÚ¯Ø§ÛŒÛŒØ¯#Ú¯Ø§\nÚ¯Ø¯Ø§Ø®Øª#Ú¯Ø¯Ø§Ø²\nÚ¯Ø°Ø§Ø±Ø¯#Ú¯Ø°Ø§Ø±\nÚ¯Ø°Ø§Ø´Øª#Ú¯Ø°Ø§Ø±\nÚ¯Ø°Ø±Ø§Ù†Ø¯#Ú¯Ø°Ø±Ø§Ù†\nÚ¯Ø°Ø´Øª#Ú¯Ø°Ø±\nÚ¯Ø±Ø§Ø²ÛŒØ¯#Ú¯Ø±Ø§Ø²\nÚ¯Ø±Ø§Ù†ÛŒØ¯#Ú¯Ø±Ø§Ù†\nÚ¯Ø±Ø§ÛŒÛŒØ¯#Ú¯Ø±Ø§\nÚ¯Ø±Ø¯Ø§Ù†Ø¯#Ú¯Ø±Ø¯Ø§Ù†\nÚ¯Ø±Ø¯Ø§Ù†ÛŒØ¯#Ú¯Ø±Ø¯Ø§Ù†\nÚ¯Ø±Ø¯ÛŒØ¯#Ú¯Ø±Ø¯\nÚ¯Ø±ÙØª#Ú¯ÛŒØ±\nÚ¯Ø±ÙˆÛŒØ¯#Ú¯Ø±Ùˆ\nÚ¯Ø±ÛŒØ§Ù†Ø¯#Ú¯Ø±ÛŒØ§Ù†\nÚ¯Ø±ÛŒØ®Øª#Ú¯Ø±ÛŒØ²\nÚ¯Ø±ÛŒØ²Ø§Ù†Ø¯#Ú¯Ø±ÛŒØ²Ø§Ù†\nÚ¯Ø±ÛŒØ³Øª#Ú¯Ø±\nÚ¯Ø±ÛŒØ³Øª#Ú¯Ø±ÛŒ\nÚ¯Ø²Ø§Ø±Ø¯#Ú¯Ø²Ø§Ø±\nÚ¯Ø²Ø§Ø´Øª#Ú¯Ø²Ø§Ø±\nÚ¯Ø²ÛŒØ¯#Ú¯Ø²ÛŒÙ†\nÚ¯Ø³Ø§Ø±Ø¯#Ú¯Ø³Ø§Ø±\nÚ¯Ø³ØªØ±Ø§Ù†Ø¯#Ú¯Ø³ØªØ±Ø§Ù†\nÚ¯Ø³ØªØ±Ø§Ù†ÛŒØ¯#Ú¯Ø³ØªØ±Ø§Ù†\nÚ¯Ø³ØªØ±Ø¯#Ú¯Ø³ØªØ±\nÚ¯Ø³Ø³Øª#Ú¯Ø³Ù„\nÚ¯Ø³Ù„Ø§Ù†Ø¯#Ú¯Ø³Ù„\nÚ¯Ø³ÛŒØ®Øª#Ú¯Ø³Ù„\nÚ¯Ø´Ø§Ø¯#Ú¯Ø´Ø§\nÚ¯Ø´Øª#Ú¯Ø±Ø¯\nÚ¯Ø´ÙˆØ¯#Ú¯Ø´Ø§\nÚ¯ÙØª#Ú¯Ùˆ\nÚ¯Ù…Ø§Ø±Ø¯#Ú¯Ù…Ø§Ø±\nÚ¯Ù…Ø§Ø´Øª#Ú¯Ù…Ø§Ø±\nÚ¯Ù†Ø¬Ø§Ù†Ø¯#Ú¯Ù†Ø¬Ø§Ù†\nÚ¯Ù†Ø¬Ø§Ù†ÛŒØ¯#Ú¯Ù†Ø¬Ø§Ù†\nÚ¯Ù†Ø¬ÛŒØ¯#Ú¯Ù†Ø¬\nÚ¯Ù†Ø¯Ø§Ù†Ø¯#Ú¯Ù†Ø¯Ø§Ù†\nÚ¯Ù†Ø¯ÛŒØ¯#Ú¯Ù†Ø¯\nÚ¯ÙˆØ§Ø±ÛŒØ¯#Ú¯ÙˆØ§Ø±\nÚ¯ÙˆØ²ÛŒØ¯#Ú¯ÙˆØ²\nÚ¯ÛŒØ±Ø§Ù†Ø¯#Ú¯ÛŒØ±Ø§Ù†\nÛŒØ§Ø²ÛŒØ¯#ÛŒØ§Ø²\nÛŒØ§ÙØª#ÛŒØ§Ø¨\nÛŒÙˆÙ†ÛŒØ¯#ÛŒÙˆÙ†\n'.strip().split()
A:spacy.lang.fa.generate_verbs_exc.(past, present)->verb_root.split('#')
A:spacy.lang.fa.generate_verbs_exc.conjugations->set(map(lambda item: item.replace('Ø¨Ø¢', 'Ø¨ÛŒØ§').replace('Ù†Ø¢', 'Ù†ÛŒØ§'), conjugations))


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ko/__init__.py----------------------------------------
A:spacy.lang.ko.__init__.idx->text.find(token, start)
A:spacy.lang.ko.__init__.self.Tokenizer->try_mecab_import()
A:spacy.lang.ko.__init__.dtokens->list(self.detailed_tokens(text))
A:spacy.lang.ko.__init__.doc->Doc(self.vocab, words=surfaces, spaces=list(check_spaces(text, surfaces)))
A:spacy.lang.ko.__init__.(first_tag, sep, eomi_tags)->dtoken['tag'].partition('+')
A:spacy.lang.ko.__init__.(tag, _, expr)->feature.partition(',')
A:spacy.lang.ko.__init__.(lemma, _, remainder)->expr.partition('/')
A:spacy.lang.ko.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
spacy.lang.ko.__init__.Korean(Language)
spacy.lang.ko.__init__.Korean.make_doc(self,text)
spacy.lang.ko.__init__.KoreanDefaults(Language.Defaults)
spacy.lang.ko.__init__.KoreanDefaults.create_tokenizer(cls,nlp=None)
spacy.lang.ko.__init__.KoreanTokenizer(self,cls,nlp=None)
spacy.lang.ko.__init__.KoreanTokenizer.__init__(self,cls,nlp=None)
spacy.lang.ko.__init__.KoreanTokenizer.detailed_tokens(self,text)
spacy.lang.ko.__init__.check_spaces(text,tokens)
spacy.lang.ko.__init__.pickle_korean(instance)
spacy.lang.ko.__init__.try_mecab_import()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ko/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ko/stop_words.py----------------------------------------
A:spacy.lang.ko.stop_words.STOP_WORDS->set('\nì´\nìˆ\ní•˜\nê²ƒ\në“¤\nê·¸\në˜\nìˆ˜\nì´\në³´\nì•Š\nì—†\në‚˜\nì£¼\nì•„ë‹ˆ\në“±\nê°™\në•Œ\në…„\nê°€\ní•œ\nì§€\nì˜¤\në§\nì¼\nê·¸ë ‡\nìœ„í•˜\në•Œë¬¸\nê·¸ê²ƒ\në‘\në§í•˜\nì•Œ\nê·¸ëŸ¬ë‚˜\në°›\nëª»í•˜\nì¼\nê·¸ëŸ°\në˜\në”\në§\nê·¸ë¦¬ê³ \nì¢‹\ní¬\nì‹œí‚¤\nê·¸ëŸ¬\ní•˜ë‚˜\nì‚´\në°\nì•ˆ\nì–´ë–¤\në²ˆ\në‚˜\në‹¤ë¥¸\nì–´ë–»\në“¤\nì´ë ‡\nì \nì‹¶\në§\nì¢€\nì›\nì˜\në†“\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ko/tag_map.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/nl/__init__.py----------------------------------------
A:spacy.lang.nl.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.nl.__init__.lex_attr_getters[NORM]->add_lookups(Language.Defaults.lex_attr_getters[NORM], BASE_NORMS)
A:spacy.lang.nl.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS, TOKENIZER_EXCEPTIONS)
A:spacy.lang.nl.__init__.lookups->Lookups()
spacy.lang.nl.__init__.Dutch(Language)
spacy.lang.nl.__init__.DutchDefaults(Language.Defaults)
spacy.lang.nl.__init__.DutchDefaults.create_lemmatizer(cls,nlp=None,lookups=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/nl/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/nl/lemmatizer.py----------------------------------------
A:spacy.lang.nl.lemmatizer.string->string.lower().lower()
A:spacy.lang.nl.lemmatizer.index_table->self.lookups.get_table('lemma_index', {})
A:spacy.lang.nl.lemmatizer.lemma_index->self.lookups.get_table('lemma_index', {}).get(univ_pos, {})
A:spacy.lang.nl.lemmatizer.exc_table->self.lookups.get_table('lemma_exc', {})
A:spacy.lang.nl.lemmatizer.exceptions->self.lookups.get_table('lemma_exc', {}).get(univ_pos, {})
A:spacy.lang.nl.lemmatizer.lookup_table->self.lookups.get_table('lemma_lookup', {})
A:spacy.lang.nl.lemmatizer.looked_up_lemma->self.lookups.get_table('lemma_lookup', {}).get(string)
A:spacy.lang.nl.lemmatizer.rules_table->self.lookups.get_table('lemma_rules', {})
A:spacy.lang.nl.lemmatizer.(forms, is_known)->self.lemmatize(string, lemma_index, exceptions, rules_table.get(univ_pos, []))
spacy.lang.nl.DutchLemmatizer(self,string,univ_pos,morphology=None)
spacy.lang.nl.DutchLemmatizer.lemmatize(self,string,index,exceptions,rules)
spacy.lang.nl.DutchLemmatizer.lookup(self,string,orth=None)
spacy.lang.nl.lemmatizer.DutchLemmatizer(self,string,univ_pos,morphology=None)
spacy.lang.nl.lemmatizer.DutchLemmatizer.__call__(self,string,univ_pos,morphology=None)
spacy.lang.nl.lemmatizer.DutchLemmatizer.lemmatize(self,string,index,exceptions,rules)
spacy.lang.nl.lemmatizer.DutchLemmatizer.lookup(self,string,orth=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/nl/lex_attrs.py----------------------------------------
A:spacy.lang.nl.lex_attrs._num_words->set('\nnul een Ã©Ã©n twee drie vier vijf zes zeven acht negen tien elf twaalf dertien\nveertien twintig dertig veertig vijftig zestig zeventig tachtig negentig honderd\nduizend miljoen miljard biljoen biljard triljoen triljard\n'.split())
A:spacy.lang.nl.lex_attrs._ordinal_words->set('\neerste tweede derde vierde vijfde zesde zevende achtste negende tiende elfde\ntwaalfde dertiende veertiende twintigste dertigste veertigste vijftigste\nzestigste zeventigste tachtigste negentigste honderdste duizendste miljoenste\nmiljardste biljoenste biljardste triljoenste triljardste\n'.split())
A:spacy.lang.nl.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.nl.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.nl.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/nl/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.nl.tokenizer_exceptions.uppered->orth.upper()
A:spacy.lang.nl.tokenizer_exceptions.capsed->orth.capitalize()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/nl/stop_words.py----------------------------------------
A:spacy.lang.nl.stop_words.STOP_WORDS->set("\naan af al alle alles allebei alleen allen als altijd ander anders andere anderen aangaangde aangezien achter achterna\nafgelopen aldus alhoewel anderzijds\n\nben bij bijna bijvoorbeeld behalve beide beiden beneden bent bepaald beter betere betreffende binnen binnenin boven\nbovenal bovendien bovenstaand buiten\n\ndaar dan dat de der den deze die dit doch doen door dus daarheen daarin daarna daarnet daarom daarop des dezelfde dezen\ndien dikwijls doet doorgaand doorgaans\n\neen eens en er echter enige eerder eerst eerste eersten effe eigen elk elke enkel enkele enz erdoor etc even eveneens\nevenwel\n\nff\n\nge geen geweest gauw gedurende gegeven gehad geheel gekund geleden gelijk gemogen geven geweest gewoon gewoonweg\ngeworden gij\n\nhaar had heb hebben heeft hem het hier hij hoe hun hadden hare hebt hele hen hierbeneden hierboven hierin hoewel hun\n\niemand iets ik in is idd ieder ikke ikzelf indien inmiddels inz inzake\n\nja je jou jouw jullie jezelf jij jijzelf jouwe juist\n\nkan kon kunnen klaar konden krachtens kunnen kunt\n\nlang later liet liever\n\nmaar me meer men met mij mijn moet mag mede meer meesten mezelf mijzelf min minder misschien mocht mochten moest moesten\nmoet moeten mogelijk mogen\n\nna naar niet niets nog nu nabij nadat net nogal nooit nr nu\n\nof om omdat ons ook op over omhoog omlaag omstreeks omtrent omver onder ondertussen ongeveer onszelf onze ooit opdat\nopnieuw opzij over overigens\n\npas pp precies prof publ\n\nreeds rond rondom\n\nsedert sinds sindsdien slechts sommige spoedig steeds\n\nâ€˜t 't te tegen toch toen tot tamelijk ten tenzij ter terwijl thans tijdens toe totdat tussen\n\nu uit uw uitgezonderd uwe uwen\n\nvan veel voor vaak vanaf vandaan vanuit vanwege veeleer verder verre vervolgens vgl volgens vooraf vooral vooralsnog\nvoorbij voordat voordien voorheen voorop voort voorts vooruit vrij vroeg\n\nwant waren was wat we wel werd wezen wie wij wil worden waar waarom wanneer want weer weg wegens weinig weinige weldra\nwelk welke welken werd werden wiens wier wilde wordt\n\nzal ze zei zelf zich zij zijn zo zonder zou zeer zeker zekere zelfde zelfs zichzelf zijnde zijne zoâ€™n zoals zodra zouden\n zoveel zowat zulk zulke zulks zullen zult\n".split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/nl/tag_map.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/nl/punctuation.py----------------------------------------
A:spacy.lang.nl.punctuation._quotes->char_classes.CONCAT_QUOTES.replace("'", '')


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ro/__init__.py----------------------------------------
A:spacy.lang.ro.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.ro.__init__.lex_attr_getters[NORM]->add_lookups(Language.Defaults.lex_attr_getters[NORM], BASE_NORMS)
A:spacy.lang.ro.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS, TOKENIZER_EXCEPTIONS)
spacy.lang.ro.__init__.Romanian(Language)
spacy.lang.ro.__init__.RomanianDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ro/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ro/lex_attrs.py----------------------------------------
A:spacy.lang.ro.lex_attrs._num_words->set('\nzero unu doi douÄƒ trei patru cinci È™ase È™apte opt nouÄƒ zece\nunsprezece doisprezece douÄƒsprezece treisprezece patrusprezece cincisprezece È™aisprezece È™aptesprezece optsprezece nouÄƒsprezece\ndouÄƒzeci treizeci patruzeci cincizeci È™aizeci È™aptezeci optzeci nouÄƒzeci\nsutÄƒ mie milion miliard bilion trilion cvadrilion catralion cvintilion sextilion septilion enÈ™pemii\n'.split())
A:spacy.lang.ro.lex_attrs._ordinal_words->set('\nprimul doilea treilea patrulea cincilea È™aselea È™aptelea optulea nouÄƒlea zecelea\nprima doua treia patra cincia È™asea È™aptea opta noua zecea\nunsprezecelea doisprezecelea treisprezecelea patrusprezecelea cincisprezecelea È™aisprezecelea È™aptesprezecelea optsprezecelea nouÄƒsprezecelea\nunsprezecea douÄƒsprezecea treisprezecea patrusprezecea cincisprezecea È™aisprezecea È™aptesprezecea optsprezecea nouÄƒsprezecea\ndouÄƒzecilea treizecilea patruzecilea cincizecilea È™aizecilea È™aptezecilea optzecilea nouÄƒzecilea sutÄƒlea\ndouÄƒzecea treizecea patruzecea cincizecea È™aizecea È™aptezecea optzecea nouÄƒzecea suta\nmiilea mielea mia milionulea milioana miliardulea miliardelea miliarda enÈ™pemia\n'.split())
A:spacy.lang.ro.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.ro.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.ro.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ro/tokenizer_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ro/stop_words.py----------------------------------------
A:spacy.lang.ro.stop_words.STOP_WORDS->set('\na\nabia\nacea\naceasta\naceastÄƒ\naceea\naceeasi\nacei\naceia\nacel\nacela\nacelasi\nacele\nacelea\nacest\nacesta\naceste\nacestea\nacestei\nacestia\nacestui\naceÅŸti\naceÅŸtia\naceÈ™ti\naceÈ™tia\nacolo\nacord\nacum\nadica\nai\naia\naibÄƒ\naici\naiurea\nal\nala\nalaturi\nale\nalea\nalt\nalta\naltceva\naltcineva\nalte\naltfel\nalti\naltii\naltul\nalÄƒturi\nam\nanume\napoi\nar\nare\nas\nasa\nasemenea\nasta\nastazi\nastea\nastfel\nastÄƒzi\nasupra\natare\natat\natata\natatea\natatia\nati\natit\natita\natitea\natitia\natunci\nau\navea\navem\naveÅ£i\naveÈ›i\navut\nazi\naÅŸ\naÅŸadar\naÅ£i\naÈ™\naÈ™adar\naÈ›i\nb\nba\nbine\nbucur\nbunÄƒ\nc\nca\ncam\ncand\ncapat\ncare\ncareia\ncarora\ncaruia\ncat\ncatre\ncaut\nce\ncea\nceea\ncei\nceilalti\ncel\ncele\ncelor\nceva\nchiar\nci\ncinci\ncind\ncine\ncineva\ncit\ncita\ncite\nciteva\nciti\ncitiva\nconform\ncontra\ncu\ncui\ncum\ncumva\ncurÃ¢nd\ncurÃ®nd\ncÃ¢nd\ncÃ¢t\ncÃ¢te\ncÃ¢tva\ncÃ¢Å£i\ncÃ¢È›i\ncÃ®nd\ncÃ®t\ncÃ®te\ncÃ®tva\ncÃ®Å£i\ncÃ®È›i\ncÄƒ\ncÄƒci\ncÄƒrei\ncÄƒror\ncÄƒrui\ncÄƒtre\nd\nda\ndaca\ndacÄƒ\ndar\ndat\ndatoritÄƒ\ndatÄƒ\ndau\nde\ndeasupra\ndeci\ndecit\ndegraba\ndeja\ndeoarece\ndeparte\ndesi\ndespre\ndeÅŸi\ndeÈ™i\ndin\ndinaintea\ndintr\ndintr-\ndintre\ndoar\ndoi\ndoilea\ndouÄƒ\ndrept\ndupa\ndupÄƒ\ndÄƒ\ne\nea\nei\nel\nele\nera\neram\neste\neu\nexact\neÅŸti\neÈ™ti\nf\nface\nfara\nfata\nfel\nfi\nfie\nfiecare\nfii\nfim\nfiu\nfiÅ£i\nfiÈ›i\nfoarte\nfost\nfrumos\nfÄƒrÄƒ\ng\ngeaba\ngraÅ£ie\ngraÈ›ie\nh\nhalbÄƒ\ni\nia\niar\nieri\nii\nil\nimi\nin\ninainte\ninapoi\ninca\nincit\ninsa\nintr\nintre\nisi\niti\nj\nk\nl\nla\nle\nli\nlor\nlui\nlÃ¢ngÄƒ\nlÃ®ngÄƒ\nm\nma\nmai\nmare\nmea\nmei\nmele\nmereu\nmeu\nmi\nmie\nmine\nmod\nmult\nmulta\nmulte\nmulti\nmultÄƒ\nmulÅ£i\nmulÅ£umesc\nmulÈ›i\nmulÈ›umesc\nmÃ¢ine\nmÃ®ine\nmÄƒ\nn\nne\nnevoie\nni\nnici\nniciodata\nnicÄƒieri\nnimeni\nnimeri\nnimic\nniste\nniÅŸte\nniÈ™te\nnoastre\nnoastrÄƒ\nnoi\nnoroc\nnostri\nnostru\nnou\nnoua\nnouÄƒ\nnoÅŸtri\nnoÈ™tri\nnu\nnumai\no\nopt\nor\nori\noricare\norice\noricine\noricum\noricÃ¢nd\noricÃ¢t\noricÃ®nd\noricÃ®t\noriunde\np\npai\nparca\npatra\npatru\npatrulea\npe\npentru\npeste\npic\npina\nplus\npoate\npot\nprea\nprima\nprimul\nprin\nprintr-\nputini\npuÅ£in\npuÅ£ina\npuÅ£inÄƒ\npuÈ›in\npuÈ›ina\npuÈ›inÄƒ\npÃ¢nÄƒ\npÃ®nÄƒ\nr\nrog\ns\nsa\nsa-mi\nsa-ti\nsai\nsale\nsau\nse\nsi\nsint\nsintem\nspate\nspre\nsub\nsunt\nsuntem\nsunteÅ£i\nsunteÈ›i\nsus\nsutÄƒ\nsÃ®nt\nsÃ®ntem\nsÃ®nteÅ£i\nsÃ®nteÈ›i\nsÄƒ\nsÄƒi\nsÄƒu\nt\nta\ntale\nte\nti\ntimp\ntine\ntoata\ntoate\ntoatÄƒ\ntocmai\ntot\ntoti\ntotul\ntotusi\ntotuÅŸi\ntotuÈ™i\ntoÅ£i\ntoÈ›i\ntrei\ntreia\ntreilea\ntu\ntuturor\ntÄƒi\ntÄƒu\nu\nul\nului\nun\nuna\nunde\nundeva\nunei\nuneia\nunele\nuneori\nunii\nunor\nunora\nunu\nunui\nunuia\nunul\nv\nva\nvi\nvoastre\nvoastrÄƒ\nvoi\nvom\nvor\nvostru\nvouÄƒ\nvoÅŸtri\nvoÈ™tri\nvreme\nvreo\nvreun\nvÄƒ\nx\nz\nzece\nzero\nzi\nzice\nÃ®i\nÃ®l\nÃ®mi\nÃ®mpotriva\nÃ®n\nÃ®nainte\nÃ®naintea\nÃ®ncotro\nÃ®ncÃ¢t\nÃ®ncÃ®t\nÃ®ntre\nÃ®ntrucÃ¢t\nÃ®ntrucÃ®t\nÃ®Å£i\nÃ®È›i\nÄƒla\nÄƒlea\nÄƒsta\nÄƒstea\nÄƒÅŸtia\nÄƒÈ™tia\nÅŸapte\nÅŸase\nÅŸi\nÅŸtiu\nÅ£i\nÅ£ie\nÈ™apte\nÈ™ase\nÈ™i\nÈ™tiu\nÈ›i\nÈ›ie\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ro/tag_map.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/pl/__init__.py----------------------------------------
A:spacy.lang.pl.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.pl.__init__.lex_attr_getters[NORM]->add_lookups(Language.Defaults.lex_attr_getters[NORM], BASE_NORMS)
A:spacy.lang.pl.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS, TOKENIZER_EXCEPTIONS)
spacy.lang.pl.__init__.Polish(Language)
spacy.lang.pl.__init__.PolishDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/pl/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/pl/lex_attrs.py----------------------------------------
A:spacy.lang.pl.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.pl.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.pl.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/pl/_tokenizer_exceptions_list.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/pl/tokenizer_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/pl/stop_words.py----------------------------------------
A:spacy.lang.pl.stop_words.STOP_WORDS->set('\na aby ach acz aczkolwiek aj albo ale alez\naleÅ¼ ani az aÅ¼\n\nbardziej bardzo beda bede bedzie bez bo bowiem by\nbyc byl byla byli bylo byly bym bynajmniej byÄ‡ byÅ‚\nbyÅ‚a byÅ‚o byÅ‚y bÄ™dzie bÄ™dÄ… bÄ™dÄ™\n\ncala cali caly caÅ‚a caÅ‚y chce choÄ‡ ci cie\nciebie ciÄ™ co cokolwiek coraz cos coÅ› czasami czasem czemu\nczy czyli czÄ™sto\n\ndaleko dla dlaczego dlatego do dobrze dokad dokÄ…d\ndosc doÅ›Ä‡ duzo duÅ¼o dwa dwaj dwie dwoje dzis\ndzisiaj dziÅ›\n\ngdy gdyby gdyz gdyÅ¼ gdzie gdziekolwiek gdzies gdzieÅ› go\ngodz\n\ni ich ile im inna inne inny\ninnych iv ix iz iÅ¼\n\nja jak jakas jakaÅ› jakby jaki jakichs jakichÅ› jakie\njakis jakiz jakiÅ› jakiÅ¼ jakkolwiek jako jakos jakoÅ› je jeden\njedna jednak jednakze jednakÅ¼e jedno jednym jedynie jego jej jemu\njesli jest jestem jeszcze jezeli jeÅ›li jeÅ¼eli juz juÅ¼ jÄ…\n\nkazdy kaÅ¼dy kiedy kierunku kilka kilku kims kimÅ› kto\nktokolwiek ktora ktore ktorego ktorej ktory ktorych ktorym ktorzy ktos\nktoÅ› ktÃ³ra ktÃ³re ktÃ³rego ktÃ³rej ktÃ³ry ktÃ³rych ktÃ³rym ktÃ³rzy ku\n\nlecz lub\n\nma majÄ… mam mamy maÅ‚o mi miaÅ‚ miedzy\nmimo miÄ™dzy mna mnie mnÄ… moga mogÄ… moi moim moj\nmoja moje moze mozliwe mozna moÅ¼e moÅ¼liwe moÅ¼na mu musi\nmy mÃ³j\n\nna nad nam nami nas nasi nasz nasza nasze\nnaszego naszych natomiast natychmiast nawet nia nic nich nie niech\nniego niej niemu nigdy nim nimi niz niÄ… niÅ¼ no\n\no obok od ok okoÅ‚o on ona one\noni ono oraz oto owszem\n\npan pana pani po pod podczas pomimo ponad\nponiewaz poniewaÅ¼ powinien powinna powinni powinno poza prawie przeciez\nprzecieÅ¼ przed przede przedtem przez przy\n\nraz razie roku rowniez rÃ³wnieÅ¼\n\nsam sama sie siÄ™ skad skÄ…d soba sobie sobÄ…\nsposob sposÃ³b swoje sÄ…\n\nta tak taka taki takich takie takze takÅ¼e tam\nte tego tej tel temu ten teraz teÅ¼ to toba\ntobie tobÄ… totez toteÅ¼ totobÄ… trzeba tu tutaj twoi twoim\ntwoj twoja twoje twym twÃ³j ty tych tylko tym tys\ntzw tÄ™\n\nu\n\nvi vii viii\n\nw wam wami was wasi wasz wasza wasze we\nwedÅ‚ug wie wiele wielu wiÄ™c wiÄ™cej wlasnie wszyscy wszystkich wszystkie\nwszystkim wszystko wtedy wy wÅ‚aÅ›nie wÅ›rÃ³d\n\nxi xii xiii xiv xv\n\nz za zaden zadna zadne zadnych zapewne zawsze zaÅ›\nze zeby znow znowu znÃ³w zostal zostaÅ‚\n\nÅ¼aden Å¼adna Å¼adne Å¼adnych Å¼e Å¼eby'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/pl/tag_map.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/pl/punctuation.py----------------------------------------
A:spacy.lang.pl.punctuation._quotes->char_classes.CONCAT_QUOTES.replace("'", '')


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/th/__init__.py----------------------------------------
A:spacy.lang.th.__init__.words->list(self.word_tokenize(text))
A:spacy.lang.th.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.th.__init__.lex_attr_getters[NORM]->add_lookups(Language.Defaults.lex_attr_getters[NORM], BASE_NORMS, NORM_EXCEPTIONS)
A:spacy.lang.th.__init__.tokenizer_exceptions->dict(TOKENIZER_EXCEPTIONS)
spacy.lang.th.__init__.Thai(Language)
spacy.lang.th.__init__.Thai.make_doc(self,text)
spacy.lang.th.__init__.ThaiDefaults(Language.Defaults)
spacy.lang.th.__init__.ThaiDefaults.create_tokenizer(cls,nlp=None)
spacy.lang.th.__init__.ThaiTokenizer(self,cls,nlp=None)
spacy.lang.th.__init__.ThaiTokenizer.__init__(self,cls,nlp=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/th/lex_attrs.py----------------------------------------
A:spacy.lang.th.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.th.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.th.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/th/tokenizer_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/th/stop_words.py----------------------------------------
A:spacy.lang.th.stop_words.STOP_WORDS->set('\nà¸—à¸±à¹‰à¸‡à¸™à¸µà¹‰ à¸”à¸±à¸‡ à¸‚à¸­ à¸£à¸§à¸¡ à¸«à¸¥à¸±à¸‡à¸ˆà¸²à¸ à¹€à¸›à¹‡à¸™ à¸«à¸¥à¸±à¸‡ à¸«à¸£à¸·à¸­ à¹† à¹€à¸à¸µà¹ˆà¸¢à¸§à¸à¸±à¸š à¸‹à¸¶à¹ˆà¸‡à¹„à¸”à¹‰à¹à¸à¹ˆ à¸”à¹‰à¸§à¸¢à¹€à¸à¸£à¸²à¸° à¸”à¹‰à¸§à¸¢à¸§à¹ˆà¸² à¸”à¹‰à¸§à¸¢à¹€à¸«à¸•à¸¸à¹€à¸à¸£à¸²à¸°\nà¸”à¹‰à¸§à¸¢à¹€à¸«à¸•à¸¸à¸§à¹ˆà¸² à¸ªà¸¸à¸”à¹† à¹€à¸ªà¸£à¹‡à¸ˆà¹à¸¥à¹‰à¸§ à¹€à¸Šà¹ˆà¸™ à¹€à¸‚à¹‰à¸² à¸–à¹‰à¸² à¸–à¸¹à¸ à¸–à¸¶à¸‡ à¸•à¹ˆà¸²à¸‡à¹† à¹ƒà¸„à¸£ à¹€à¸›à¸´à¸”à¹€à¸œà¸¢ à¸„à¸£à¸² à¸£à¸·à¸­ à¸•à¸²à¸¡ à¹ƒà¸™ à¹„à¸”à¹‰à¹à¸à¹ˆ à¹„à¸”à¹‰à¹à¸•à¹ˆ\nà¹„à¸”à¹‰à¸—à¸µà¹ˆ à¸•à¸¥à¸­à¸”à¸–à¸¶à¸‡ à¸™à¸­à¸à¸ˆà¸²à¸à¸§à¹ˆà¸² à¸™à¸­à¸à¸™à¸±à¹‰à¸™ à¸ˆà¸£à¸´à¸‡ à¸­à¸¢à¹ˆà¸²à¸‡à¸”à¸µ à¸ªà¹ˆà¸§à¸™ à¹€à¸à¸µà¸¢à¸‡à¹€à¸à¸·à¹ˆà¸­ à¹€à¸”à¸µà¸¢à¸§ à¸ˆà¸±à¸” à¸—à¸±à¹‰à¸‡à¸—à¸µ à¸—à¸±à¹‰à¸‡à¸„à¸™ à¸—à¸±à¹‰à¸‡à¸•à¸±à¸§ à¹„à¸à¸¥à¹†\nà¸–à¸¶à¸‡à¹€à¸¡à¸·à¹ˆà¸­à¹ƒà¸” à¸„à¸‡à¸ˆà¸° à¸–à¸¹à¸à¹† à¹€à¸›à¹‡à¸™à¸—à¸µ à¸™à¸±à¸šà¹à¸•à¹ˆà¸—à¸µà¹ˆ à¸™à¸±à¸šà¹à¸•à¹ˆà¸™à¸±à¹‰à¸™ à¸£à¸±à¸šà¸£à¸­à¸‡ à¸”à¹‰à¸²à¸™ à¹€à¸›à¹‡à¸™à¸•à¹‰à¸™à¸¡à¸² à¸—à¸¸à¸ à¸à¸£à¸°à¸—à¸±à¹ˆà¸‡ à¸à¸£à¸°à¸—à¸³ à¸ˆà¸§à¸š à¸‹à¸¶à¹ˆà¸‡à¸à¹‡ à¸ˆà¸°\nà¸„à¸£à¸šà¸„à¸£à¸±à¸™ à¸™à¸±à¸šà¹à¸•à¹ˆ à¹€à¸¢à¸­à¸°à¹† à¹€à¸à¸µà¸¢à¸‡à¹„à¸«à¸™ à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™à¹à¸›à¸¥à¸‡ à¹„à¸›à¹ˆ à¸œà¹ˆà¸²à¸™à¹† à¹€à¸à¸·à¹ˆà¸­à¸—à¸µà¹ˆ à¸£à¸§à¸¡à¹† à¸à¸§à¹‰à¸²à¸‡à¸‚à¸§à¸²à¸‡ à¹€à¸ªà¸µà¸¢à¸¢à¸´à¹ˆà¸‡ à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™ à¸œà¹ˆà¸²à¸™\nà¸—à¸£à¸‡ à¸—à¸§à¹ˆà¸² à¸à¸±à¸™à¹€à¸–à¸­à¸° à¹€à¸à¸µà¹ˆà¸¢à¸§à¹† à¹ƒà¸”à¹† à¸„à¸£à¸±à¹‰à¸‡à¸—à¸µà¹ˆ à¸„à¸£à¸±à¹‰à¸‡à¸™à¸±à¹‰à¸™ à¸„à¸£à¸±à¹‰à¸‡à¸™à¸µà¹‰ à¸„à¸£à¸±à¹‰à¸‡à¸¥à¸° à¸„à¸£à¸±à¹‰à¸‡à¸«à¸¥à¸±à¸‡ à¸„à¸£à¸±à¹‰à¸‡à¸«à¸¥à¸±à¸‡à¸ªà¸¸à¸” à¸£à¹ˆà¸§à¸¡à¸à¸±à¸™ à¸£à¹ˆà¸§à¸¡à¸”à¹‰à¸§à¸¢ à¸à¹‡à¸•à¸²à¸¡à¸—à¸µ\nà¸—à¸µà¹ˆà¸ªà¸¸à¸” à¸œà¸´à¸”à¹† à¸¢à¸·à¸™à¸¢à¸‡ à¹€à¸¢à¸­à¸° à¸„à¸£à¸±à¹‰à¸‡à¹† à¹ƒà¸„à¸£à¹† à¸™à¸±à¹ˆà¸™à¹€à¸­à¸‡ à¹€à¸ªà¸¡à¸·à¸­à¸™à¸§à¹ˆà¸² à¹€à¸ªà¸£à¹‡à¸ˆ à¸•à¸¥à¸­à¸”à¸¨à¸ à¸—à¸±à¹‰à¸‡à¸—à¸µà¹ˆ à¸¢à¸·à¸™à¸¢à¸±à¸™ à¸”à¹‰à¸§à¸¢à¸—à¸µà¹ˆ à¸šà¸±à¸”à¸™à¸µà¹‰\nà¸”à¹‰à¸§à¸¢à¸›à¸£à¸°à¸à¸²à¸£à¸‰à¸°à¸™à¸µà¹‰ à¸‹à¸¶à¹ˆà¸‡à¸à¸±à¸™ à¸•à¸¥à¸­à¸”à¸—à¸±à¹ˆà¸§à¸–à¸¶à¸‡ à¸•à¸¥à¸­à¸”à¸—à¸±à¹ˆà¸§à¸—à¸±à¹‰à¸‡ à¸•à¸¥à¸­à¸”à¸›à¸µ à¹€à¸›à¹‡à¸™à¸à¸²à¸£ à¸™à¸±à¹ˆà¸™à¹à¸«à¸¥à¸° à¸à¸£à¹‰à¸­à¸¡ à¹€à¸–à¸´à¸” à¸—à¸±à¹‰à¸‡ à¸ªà¸·à¸šà¹€à¸™à¸·à¹ˆà¸­à¸‡ à¸•à¸±à¹‰à¸‡à¹à¸•à¹ˆ\nà¸à¸¥à¸±à¸š à¸à¸¥à¹ˆà¸²à¸§à¸„à¸·à¸­ à¸à¸¥à¸¸à¹ˆà¸¡à¸à¹‰à¸­à¸™ à¸à¸¥à¸¸à¹ˆà¸¡à¹† à¸„à¸£à¸±à¹‰à¸‡à¸„à¸£à¸² à¸ªà¹ˆà¸‡ à¸£à¸§à¸”à¹€à¸£à¹‡à¸§ à¹€à¸ªà¸£à¹‡à¸ˆà¸ªà¸´à¹‰à¸™ à¹€à¸ªà¸µà¸¢ à¹€à¸ªà¸µà¸¢à¸à¹ˆà¸­à¸™ à¹€à¸ªà¸µà¸¢à¸ˆà¸™ à¸­à¸”à¸µà¸• à¸•à¸±à¹‰à¸‡ à¹€à¸à¸´à¸” à¸­à¸²à¸ˆ\nà¸­à¸µà¸ à¸•à¸¥à¸­à¸”à¹€à¸§à¸¥à¸² à¸ à¸²à¸¢à¸«à¸™à¹‰à¸² à¸ à¸²à¸¢à¸«à¸¥à¸±à¸‡ à¸¡à¸­à¸‡ à¸¡à¸±à¸™à¹† à¸¡à¸­à¸‡à¸§à¹ˆà¸² à¸¡à¸±à¸ à¸¡à¸±à¸à¸ˆà¸° à¸¡à¸±à¸™ à¸«à¸²à¸ à¸„à¸‡à¸­à¸¢à¸¹à¹ˆ à¹€à¸›à¹‡à¸™à¸—à¸µà¹ˆ à¹€à¸›à¹‡à¸™à¸—à¸µà¹ˆà¸ªà¸¸à¸”\nà¹€à¸›à¹‡à¸™à¹€à¸à¸£à¸²à¸°à¹€à¸›à¹‡à¸™à¹€à¸à¸£à¸²à¸°à¸§à¹ˆà¸² à¹€à¸à¸µà¹ˆà¸¢à¸§à¸à¸±à¸™ à¹€à¸à¸µà¸¢à¸‡à¹„à¸£ à¹€à¸›à¹‡à¸™à¹à¸•à¹ˆà¹€à¸à¸µà¸¢à¸‡ à¸à¸¥à¹ˆà¸²à¸§ à¸ˆà¸™à¸šà¸±à¸”à¸™à¸µà¹‰ à¹€à¸›à¹‡à¸™à¸­à¸±à¸™ à¸ˆà¸™ à¸ˆà¸™à¹€à¸¡à¸·à¹ˆà¸­ à¸ˆà¸™à¹à¸¡à¹‰ à¹ƒà¸à¸¥à¹‰\nà¹ƒà¸«à¸¡à¹ˆà¹† à¹€à¸›à¹‡à¸™à¹€à¸à¸µà¸¢à¸‡ à¸­à¸¢à¹ˆà¸²à¸‡à¸—à¸µà¹ˆ à¸–à¸¹à¸à¸•à¹‰à¸­à¸‡ à¸—à¸±à¹‰à¸‡à¸™à¸±à¹‰à¸™ à¸—à¸±à¹‰à¸‡à¸™à¸±à¹‰à¸™à¸”à¹‰à¸§à¸¢ à¸à¸±à¸™à¸”à¸µà¸à¸§à¹ˆà¸² à¸à¸±à¸™à¸”à¸µà¹„à¸«à¸¡ à¸™à¸±à¹ˆà¸™à¹„à¸‡ à¸•à¸£à¸‡à¹† à¹à¸¢à¸°à¹† à¹€à¸›à¹‡à¸™à¸•à¹‰à¸™ à¹ƒà¸à¸¥à¹‰à¹†\nà¸‹à¸¶à¹ˆà¸‡à¹† à¸”à¹‰à¸§à¸¢à¸à¸±à¸™ à¸”à¸±à¸‡à¹€à¸„à¸¢ à¹€à¸–à¸­à¸° à¹€à¸ªà¸¡à¸·à¸­à¸™à¸à¸±à¸š à¹„à¸› à¸„à¸·à¸­ à¸‚à¸“à¸°à¸™à¸µà¹‰ à¸™à¸­à¸à¸ˆà¸²à¸ à¹€à¸à¸·à¹ˆà¸­à¸—à¸µà¹ˆà¸ˆà¸° à¸‚à¸“à¸°à¸«à¸™à¸¶à¹ˆà¸‡ à¸‚à¸§à¸²à¸‡ à¸„à¸£à¸±à¸™ à¸­à¸¢à¸²à¸ à¹„à¸§à¹‰\nà¹à¸šà¸š à¸™à¸­à¸à¸ˆà¸²à¸à¸™à¸µà¹‰ à¹€à¸™à¸·à¹ˆà¸­à¸‡à¸ˆà¸²à¸ à¹€à¸”à¸µà¸¢à¸§à¸à¸±à¸™ à¸„à¸‡ à¹ƒà¸«à¹‰à¸¡à¸² à¸­à¸™à¸¶à¹ˆà¸‡ à¸à¹‡à¹à¸¥à¹‰à¸§à¹à¸•à¹ˆ à¸•à¹‰à¸­à¸‡ à¸‚à¹‰à¸²à¸‡ à¹€à¸à¸·à¹ˆà¸­à¸§à¹ˆà¸² à¸ˆà¸™à¹à¸¡à¹‰à¸™ à¸„à¸£à¸±à¹‰à¸‡à¸«à¸™à¸¶à¹ˆà¸‡ à¸­à¸°à¹„à¸£ à¸‹à¸¶à¹ˆà¸‡\nà¹€à¸à¸´à¸™à¹† à¸”à¹‰à¸§à¸¢à¹€à¸«à¸•à¸¸à¸™à¸±à¹‰à¸™ à¸à¸±à¸™à¹à¸¥à¸°à¸à¸±à¸™ à¸£à¸±à¸š à¸£à¸°à¸«à¸§à¹ˆà¸²à¸‡ à¸„à¸£à¸±à¹‰à¸‡à¹„à¸«à¸™ à¹€à¸ªà¸£à¹‡à¸ˆà¸à¸±à¸™ à¸–à¸¶à¸‡à¸­à¸¢à¹ˆà¸²à¸‡à¹„à¸£ à¸‚à¸²à¸” à¸‚à¹‰à¸²à¸¯ à¹€à¸‚à¹‰à¸²à¹ƒà¸ˆ à¸„à¸£à¸š à¸„à¸£à¸±à¹‰à¸‡à¹ƒà¸”\nà¸„à¸£à¸šà¸–à¹‰à¸§à¸™ à¸£à¸°à¸¢à¸° à¹„à¸¡à¹ˆ à¹€à¸à¸·à¸­à¸š à¹€à¸à¸·à¸­à¸šà¸ˆà¸° à¹€à¸à¸·à¸­à¸šà¹† à¹à¸à¹ˆ à¹à¸ à¸­à¸¢à¹ˆà¸²à¸‡à¹‚à¸™à¹‰à¸™ à¸”à¸±à¸‡à¸à¸±à¸šà¸§à¹ˆà¸² à¸ˆà¸£à¸´à¸‡à¸ˆà¸±à¸‡ à¹€à¸¢à¸­à¸°à¹à¸¢à¸° à¸™à¸±à¹ˆà¸™ à¸”à¹‰à¸§à¸¢ à¸–à¸¶à¸‡à¹à¸¡à¹‰à¸§à¹ˆà¸²\nà¸¡à¸²à¸ à¸•à¸¥à¸­à¸”à¸à¸²à¸¥à¸™à¸²à¸™ à¸•à¸¥à¸­à¸”à¸£à¸°à¸¢à¸°à¹€à¸§à¸¥à¸² à¸•à¸¥à¸­à¸”à¸ˆà¸™ à¸•à¸¥à¸­à¸”à¹„à¸› à¹€à¸›à¹‡à¸™à¸­à¸±à¸™à¹† à¹€à¸›à¹‡à¸™à¸­à¸²à¸—à¸´ à¸à¹‡à¸•à¹ˆà¸­à¹€à¸¡à¸·à¹ˆà¸­ à¸ªà¸¹à¹ˆ à¹€à¸¡à¸·à¹ˆà¸­ à¹€à¸à¸·à¹ˆà¸­ à¸à¹‡ à¸à¸±à¸š\nà¸”à¹‰à¸§à¸¢à¹€à¸«à¸¡à¸·à¸­à¸™à¸à¸±à¸™ à¸”à¹‰à¸§à¸¢à¹€à¸«à¸•à¸¸à¸™à¸µà¹‰ à¸„à¸£à¸±à¹‰à¸‡à¸„à¸£à¸²à¸§ à¸£à¸²à¸¢ à¸£à¹ˆà¸§à¸¡ à¹€à¸›à¹‡à¸™à¸­à¸±à¸™à¸¡à¸²à¸ à¸ªà¸¹à¸‡ à¸£à¸§à¸¡à¸à¸±à¸™ à¸£à¸§à¸¡à¸—à¸±à¹‰à¸‡ à¸£à¹ˆà¸§à¸¡à¸¡à¸·à¸­ à¹€à¸›à¹‡à¸™à¹€à¸à¸µà¸¢à¸‡à¸§à¹ˆà¸² à¸£à¸§à¸¡à¸–à¸¶à¸‡\nà¸•à¹ˆà¸­ à¸™à¸° à¸à¸§à¹‰à¸²à¸‡ à¸¡à¸² à¸„à¸£à¸±à¸š à¸•à¸¥à¸­à¸”à¸—à¸±à¹‰à¸‡ à¸à¸²à¸£ à¸™à¸±à¹‰à¸™à¹† à¸™à¹ˆà¸² à¹€à¸›à¹‡à¸™à¸­à¸±à¸™à¸§à¹ˆà¸² à¹€à¸à¸£à¸²à¸° à¸§à¸±à¸™ à¸ˆà¸™à¸‚à¸“à¸°à¸™à¸µà¹‰ à¸ˆà¸™à¸•à¸¥à¸­à¸” à¸ˆà¸™à¸–à¸¶à¸‡ à¸‚à¹‰à¸² à¸­à¸¢à¹ˆà¸²à¸‡à¹ƒà¸”\nà¹„à¸«à¸™à¹† à¸à¹ˆà¸­à¸™à¸«à¸™à¹‰à¸²à¸™à¸µà¹‰ à¸à¹ˆà¸­à¸™à¹† à¸ªà¸¹à¸‡à¸à¸§à¹ˆà¸² à¸ªà¸¹à¸‡à¸ªà¹ˆà¸‡ à¸ªà¸¹à¸‡à¸ªà¸¸à¸” à¸ªà¸¹à¸‡à¹† à¹€à¸ªà¸µà¸¢à¸”à¹‰à¸§à¸¢ à¹€à¸ªà¸µà¸¢à¸™à¸±à¹ˆà¸™ à¹€à¸ªà¸µà¸¢à¸™à¸µà¹ˆ à¹€à¸ªà¸µà¸¢à¸™à¸µà¹ˆà¸à¸£à¸°à¹„à¸£ à¹€à¸ªà¸µà¸¢à¸™à¸±à¹ˆà¸™à¹€à¸­à¸‡ à¸ªà¸¸à¸”\nà¸ªà¹à¸²à¸«à¸£à¸±à¸š à¸§à¹ˆà¸² à¸¥à¸‡ à¸ à¸²à¸¢à¹ƒà¸•à¹‰ à¹€à¸à¸·à¹ˆà¸­à¹ƒà¸«à¹‰ à¸ à¸²à¸¢à¸™à¸­à¸ à¸ à¸²à¸¢à¹ƒà¸™ à¹€à¸‰à¸à¸²à¸° à¸‹à¸¶à¹ˆà¸‡à¸à¸±à¸™à¹à¸¥à¸°à¸à¸±à¸™ à¸‡à¹ˆà¸²à¸¢ à¸‡à¹ˆà¸²à¸¢à¹† à¹„à¸‡ à¸–à¸¶à¸‡à¹à¸¡à¹‰à¸ˆà¸° à¸–à¸¶à¸‡à¹€à¸¡à¸·à¹ˆà¸­à¹„à¸£\nà¹€à¸à¸´à¸™ à¸à¹‡à¹„à¸”à¹‰ à¸„à¸£à¸²à¹ƒà¸” à¸„à¸£à¸²à¸—à¸µà¹ˆ à¸•à¸¥à¸­à¸”à¸§à¸±à¸™ à¸™à¸±à¸š à¸”à¸±à¸‡à¹€à¸à¹ˆà¸² à¸”à¸±à¹ˆà¸‡à¹€à¸à¹ˆà¸² à¸«à¸¥à¸²à¸¢ à¸«à¸™à¸¶à¹ˆà¸‡ à¸–à¸·à¸­à¸§à¹ˆà¸² à¸à¹ˆà¸­à¸™à¸«à¸™à¹‰à¸² à¸™à¸±à¸šà¸•à¸±à¹‰à¸‡à¹à¸•à¹ˆ à¸ˆà¸£à¸” à¸ˆà¸£à¸´à¸‡à¹†\nà¸ˆà¸§à¸™ à¸ˆà¸§à¸™à¹€à¸ˆà¸µà¸¢à¸™ à¸•à¸¥à¸­à¸”à¸¡à¸² à¸à¸¥à¸¸à¹ˆà¸¡ à¸à¸£à¸°à¸™à¸±à¹‰à¸™ à¸‚à¹‰à¸²à¸‡à¹† à¸•à¸£à¸‡ à¸‚à¹‰à¸²à¸à¹€à¸ˆà¹‰à¸² à¸à¸§à¹ˆà¸² à¹€à¸à¸µà¹ˆà¸¢à¸§à¹€à¸™à¸·à¹ˆà¸­à¸‡ à¸‚à¸¶à¹‰à¸™ à¹ƒà¸«à¹‰à¹„à¸› à¸œà¸¥ à¹à¸•à¹ˆ à¹€à¸­à¸‡ à¹€à¸«à¹‡à¸™\nà¸ˆà¸¶à¸‡ à¹„à¸”à¹‰ à¹ƒà¸«à¹‰ à¹‚à¸”à¸¢ à¸ˆà¸£à¸´à¸‡à¹†à¸ˆà¸±à¸‡à¹† à¸”à¸±à¹ˆà¸‡à¸à¸±à¸šà¸§à¹ˆà¸² à¸—à¸±à¹‰à¸‡à¸™à¸±à¹‰à¸™à¹€à¸à¸£à¸²à¸° à¸™à¸­à¸ à¸™à¸­à¸à¹€à¸«à¸™à¸·à¸­ à¸™à¹ˆà¸° à¸à¸±à¸™à¸™à¸° à¸‚à¸“à¸°à¹€à¸”à¸µà¸¢à¸§à¸à¸±à¸™ à¹à¸¢à¸°\nà¸™à¸­à¸à¹€à¸«à¸™à¸·à¸­à¸ˆà¸²à¸ à¸™à¹‰à¸­à¸¢ à¸à¹ˆà¸­à¸™ à¸ˆà¸§à¸™à¸ˆà¸° à¸‚à¹‰à¸²à¸‡à¹€à¸„à¸µà¸¢à¸‡ à¸à¹‡à¸•à¸²à¸¡à¹à¸•à¹ˆ à¸ˆà¸£à¸”à¸à¸±à¸š à¸™à¹‰à¸­à¸¢à¸à¸§à¹ˆà¸² à¸™à¸±à¹ˆà¸™à¹€à¸›à¹‡à¸™ à¸™à¸±à¸à¹† à¸„à¸£à¸±à¹‰à¸‡à¸à¸£à¸°à¸™à¸±à¹‰à¸™ à¹€à¸¥à¸¢ à¹„à¸à¸¥\nà¸ªà¸´à¹‰à¸™à¸à¸²à¸¥à¸™à¸²à¸™ à¸„à¸£à¸±à¹‰à¸‡ à¸£à¸·à¸­à¸§à¹ˆà¸² à¹€à¸à¹‡à¸š à¸­à¸¢à¹ˆà¸²à¸‡à¹€à¸Šà¹ˆà¸™ à¸šà¸²à¸‡ à¸”à¸±à¹ˆà¸‡ à¸”à¸±à¸‡à¸à¸¥à¹ˆà¸²à¸§ à¸”à¸±à¸‡à¸à¸±à¸š à¸£à¸¶ à¸£à¸¶à¸§à¹ˆà¸² à¸­à¸­à¸ à¹à¸£à¸ à¸ˆà¸‡ à¸¢à¸·à¸™à¸™à¸²à¸™ à¹„à¸”à¹‰à¸¡à¸² à¸•à¸™\nà¸•à¸™à¹€à¸­à¸‡ à¹„à¸”à¹‰à¸£à¸±à¸š à¸£à¸°à¸¢à¸°à¹† à¸à¸£à¸°à¸œà¸¡ à¸à¸±à¸™à¹„à¸«à¸¡ à¸à¸±à¸™à¹€à¸­à¸‡ à¸à¸³à¸¥à¸±à¸‡à¸ˆà¸° à¸à¸³à¸«à¸™à¸” à¸à¸¹ à¸à¸³à¸¥à¸±à¸‡ à¸„à¸§à¸²à¸¡ à¹à¸¥à¹‰à¸§ à¹à¸¥à¸° à¸•à¹ˆà¸²à¸‡ à¸­à¸¢à¹ˆà¸²à¸‡à¸™à¹‰à¸­à¸¢\nà¸­à¸¢à¹ˆà¸²à¸‡à¸™à¸±à¹‰à¸™ à¸­à¸¢à¹ˆà¸²à¸‡à¸™à¸µà¹‰ à¸à¹‡à¸„à¸·à¸­ à¸à¹‡à¹à¸„à¹ˆ à¸”à¹‰à¸§à¸¢à¹€à¸«à¸•à¸¸à¸—à¸µà¹ˆ à¹ƒà¸«à¸à¹ˆà¹† à¹ƒà¸«à¹‰à¸”à¸µ à¸¢à¸±à¸‡ à¹€à¸›à¹‡à¸™à¹€à¸à¸·à¹ˆà¸­ à¸à¹‡à¸•à¸²à¸¡ à¸œà¸¹à¹‰ à¸•à¹ˆà¸­à¸à¸±à¸™ à¸–à¸·à¸­ à¸‹à¸¶à¹ˆà¸‡à¸à¹‡à¸„à¸·à¸­ à¸ à¸²à¸¢à¸ à¸²à¸„\nà¸ à¸²à¸¢à¸ à¸²à¸„à¸«à¸™à¹‰à¸² à¸à¹‡à¸”à¸µ à¸à¹‡à¸ˆà¸° à¸­à¸¢à¸¹à¹ˆ à¹€à¸ªà¸µà¸¢à¸¢à¸´à¹ˆà¸‡à¸™à¸±à¸ à¹ƒà¸«à¸¡à¹ˆ à¸‚à¸“à¸° à¹€à¸£à¸´à¹ˆà¸¡ à¹€à¸£à¸² à¸‚à¸§à¸²à¸‡à¹† à¹€à¸ªà¸µà¸¢à¹à¸¥à¹‰à¸§ à¹ƒà¸„à¸£à¹ˆ à¹ƒà¸„à¸£à¹ˆà¸ˆà¸° à¸•à¸™à¸¯ à¸‚à¸­à¸‡ à¹à¸«à¹ˆà¸‡\nà¸£à¸§à¸” à¸”à¸±à¹ˆà¸‡à¸à¸±à¸š à¸–à¸¶à¸‡à¹€à¸¡à¸·à¹ˆà¸­ à¸™à¹‰à¸­à¸¢à¹† à¸™à¸±à¸šà¸ˆà¸²à¸à¸™à¸±à¹‰à¸™ à¸•à¸¥à¸­à¸” à¸•à¸¥à¸­à¸”à¸à¸²à¸¥ à¹€à¸ªà¸£à¹‡à¸ˆà¸ªà¸¡à¸šà¸¹à¸£à¸“à¹Œ à¹€à¸‚à¸µà¸¢à¸™ à¸à¸§à¹‰à¸²à¸‡à¹† à¸¢à¸·à¸™à¸¢à¸²à¸§ à¸–à¸¶à¸‡à¹à¸à¹ˆ à¸‚à¸“à¸°à¹ƒà¸”\nà¸‚à¸“à¸°à¹ƒà¸”à¹† à¸‚à¸“à¸°à¸—à¸µà¹ˆ à¸‚à¸“à¸°à¸™à¸±à¹‰à¸™ à¸ˆà¸™à¸—à¸±à¹ˆà¸§ à¸ à¸²à¸„à¸¯ à¸ à¸²à¸¢ à¹€à¸›à¹‡à¸™à¹à¸•à¹ˆ à¸­à¸¢à¹ˆà¸²à¸‡ à¸à¸š à¸ à¸²à¸„ à¹ƒà¸«à¹‰à¹à¸”à¹ˆ à¹€à¸ªà¸µà¸¢à¸ˆà¸™à¸à¸£à¸°à¸—à¸±à¹ˆà¸‡ à¹€à¸ªà¸µà¸¢à¸ˆà¸™à¸–à¸¶à¸‡\nà¸ˆà¸™à¸à¸£à¸°à¸—à¸±à¹ˆà¸‡ à¸ˆà¸™à¸à¸§à¹ˆà¸² à¸•à¸¥à¸­à¸”à¸—à¸±à¹ˆà¸§ à¹€à¸›à¹‡à¸™à¹† à¸™à¸­à¸à¸ˆà¸²à¸à¸™à¸±à¹‰à¸™ à¸œà¸´à¸” à¸„à¸£à¸±à¹‰à¸‡à¸à¹ˆà¸­à¸™ à¹à¸à¹‰à¹„à¸‚ à¸‚à¸±à¹‰à¸™ à¸à¸±à¸™ à¸Šà¹ˆà¸§à¸‡ à¸ˆà¸²à¸ à¸£à¸§à¸¡à¸”à¹‰à¸§à¸¢ à¹€à¸‚à¸²\nà¸”à¹‰à¸§à¸¢à¹€à¸Šà¹ˆà¸™à¸à¸±à¸™ à¸™à¸­à¸à¸ˆà¸²à¸à¸—à¸µà¹ˆ à¹€à¸›à¹‡à¸™à¸•à¹‰à¸™à¹„à¸› à¸‚à¹‰à¸²à¸‡à¸•à¹‰à¸™ à¸‚à¹‰à¸²à¸‡à¸šà¸™ à¸‚à¹‰à¸²à¸‡à¸¥à¹ˆà¸²à¸‡ à¸–à¸¶à¸‡à¸ˆà¸° à¸–à¸¶à¸‡à¸šà¸±à¸”à¸™à¸±à¹‰à¸™ à¸–à¸¶à¸‡à¹à¸¡à¹‰ à¸¡à¸µ à¸—à¸²à¸‡ à¹€à¸„à¸¢ à¸™à¸±à¸šà¸ˆà¸²à¸à¸™à¸µà¹‰\nà¸­à¸¢à¹ˆà¸²à¸‡à¹€à¸”à¸µà¸¢à¸§ à¹€à¸à¸µà¹ˆà¸¢à¸§à¸‚à¹‰à¸­à¸‡ à¸™à¸µà¹‰ à¸™à¹à¸² à¸™à¸±à¹‰à¸™ à¸—à¸µà¹ˆ à¸—à¹à¸²à¹ƒà¸«à¹‰ à¸—à¹à¸² à¸„à¸£à¸²à¸™à¸±à¹‰à¸™ à¸„à¸£à¸²à¸™à¸µà¹‰ à¸„à¸£à¸²à¸«à¸™à¸¶à¹ˆà¸‡ à¸„à¸£à¸²à¹„à¸«à¸™ à¸„à¸£à¸²à¸§ à¸„à¸£à¸²à¸§à¸à¹ˆà¸­à¸™ à¸„à¸£à¸²à¸§à¹ƒà¸”\nà¸„à¸£à¸²à¸§à¸—à¸µà¹ˆ à¸„à¸£à¸²à¸§à¸™à¸±à¹‰à¸™ à¸„à¸£à¸²à¸§à¸™à¸µà¹‰ à¸„à¸£à¸²à¸§à¹‚à¸™à¹‰à¸™ à¸„à¸£à¸²à¸§à¸¥à¸° à¸„à¸£à¸²à¸§à¸«à¸™à¹‰à¸² à¸„à¸£à¸²à¸§à¸«à¸™à¸¶à¹ˆà¸‡ à¸„à¸£à¸²à¸§à¸«à¸¥à¸±à¸‡ à¸„à¸£à¸²à¸§à¹„à¸«à¸™ à¸„à¸£à¸²à¸§à¹† à¸„à¸¥à¹‰à¸²à¸¢\nà¸„à¸¥à¹‰à¸²à¸¢à¸à¸±à¸™ à¸„à¸¥à¹‰à¸²à¸¢à¸à¸±à¸™à¸à¸±à¸š à¸„à¸¥à¹‰à¸²à¸¢à¸à¸±à¸š à¸„à¸¥à¹‰à¸²à¸¢à¸à¸±à¸šà¸§à¹ˆà¸² à¸„à¸¥à¹‰à¸²à¸¢à¸§à¹ˆà¸² à¸„à¸§à¸£ à¸„à¹ˆà¸­à¸™ à¸„à¹ˆà¸­à¸™à¸‚à¹‰à¸²à¸‡ à¸„à¹ˆà¸­à¸™à¸‚à¹‰à¸²à¸‡à¸ˆà¸° à¸„à¹ˆà¸­à¸¢à¹„à¸›à¸—à¸²à¸‡ à¸„à¹ˆà¸­à¸™à¸¡à¸²à¸—à¸²à¸‡ à¸„\nà¹ˆà¸­à¸¢ à¸„à¹ˆà¸­à¸¢à¹† à¸„à¸° à¸„à¹ˆà¸° à¸„à¸³ à¸„à¸´à¸” à¸„à¸´à¸”à¸§à¹ˆà¸² à¸„à¸¸à¸“ à¸„à¸¸à¸“à¹† à¹€à¸„à¸¢à¹† à¹à¸„à¹ˆ à¹à¸„à¹ˆà¸ˆà¸° à¹à¸„à¹ˆà¸™à¸±à¹‰à¸™ à¹à¸„à¹ˆà¸™à¸µà¹‰ à¹à¸„à¹ˆà¹€à¸à¸µà¸¢à¸‡ à¹à¸„à¹ˆà¸§à¹ˆà¸² à¹à¸„à¹ˆà¹„à¸«à¸™ à¸ˆà¸±à¸‡à¹†\nà¸ˆà¸§à¸šà¸à¸±à¸š à¸ˆà¸§à¸šà¸ˆà¸™ à¸ˆà¹‰à¸° à¸ˆà¹Šà¸° à¸ˆà¸°à¹„à¸”à¹‰ à¸ˆà¸±à¸‡ à¸ˆà¸±à¸”à¸à¸²à¸£ à¸ˆà¸±à¸”à¸‡à¸²à¸™ à¸ˆà¸±à¸”à¹à¸ˆà¸‡ à¸ˆà¸±à¸”à¸•à¸±à¹‰à¸‡ à¸ˆà¸±à¸”à¸—à¸³ à¸ˆà¸±à¸”à¸«à¸² à¸ˆà¸±à¸”à¹ƒà¸«à¹‰ à¸ˆà¸±à¸š à¸ˆà¹‰à¸² à¸ˆà¹‹à¸² à¸ˆà¸²à¸à¸™à¸±à¹‰à¸™\nà¸ˆà¸²à¸à¸™à¸µà¹‰ à¸ˆà¸²à¸à¸™à¸µà¹‰à¹„à¸› à¸ˆà¸³ à¸ˆà¸³à¹€à¸›à¹‡à¸™ à¸ˆà¸³à¸à¸§à¸ à¸ˆà¸¶à¸‡à¸ˆà¸° à¸ˆà¸¶à¸‡à¹€à¸›à¹‡à¸™ à¸ˆà¸¹à¹ˆà¹† à¸‰à¸°à¸™à¸±à¹‰à¸™ à¸‰à¸°à¸™à¸µà¹‰ à¸‰à¸±à¸™ à¹€à¸‰à¸à¹€à¸Šà¹ˆà¸™ à¹€à¸‰à¸¢ à¹€à¸‰à¸¢à¹† à¹„à¸‰à¸™ à¸Šà¹ˆà¸§à¸‡à¸à¹ˆà¸­à¸™ à¸Š\nà¹ˆà¸§à¸‡à¸•à¹ˆà¸­à¹„à¸› à¸Šà¹ˆà¸§à¸‡à¸–à¸±à¸”à¹„à¸› à¸Šà¹ˆà¸§à¸‡à¸—à¹‰à¸²à¸¢ à¸Šà¹ˆà¸§à¸‡à¸—à¸µà¹ˆ à¸Šà¹ˆà¸§à¸‡à¸™à¸±à¹‰à¸™ à¸Šà¹ˆà¸§à¸‡à¸™à¸µà¹‰ à¸Šà¹ˆà¸§à¸‡à¸£à¸°à¸«à¸§à¹ˆà¸²à¸‡ à¸Šà¹ˆà¸§à¸‡à¹à¸£à¸ à¸Šà¹ˆà¸§à¸‡à¸«à¸™à¹‰à¸² à¸Šà¹ˆà¸§à¸‡à¸«à¸¥à¸±à¸‡ à¸Šà¹ˆà¸§à¸‡à¹† à¸Šà¹ˆà¸§à¸¢ à¸Šà¹‰à¸²\nà¸Šà¹‰à¸²à¸™à¸²à¸™ à¸Šà¸²à¸§ à¸Šà¹‰à¸²à¹† à¹€à¸Šà¹ˆà¸™à¸à¹ˆà¸­à¸™ à¹€à¸Šà¹ˆà¸™à¸à¸±à¸™ à¹€à¸Šà¹ˆà¸™à¹€à¸„à¸¢ à¹€à¸Šà¹ˆà¸™à¸”à¸±à¸‡ à¹€à¸Šà¹ˆà¸™à¸”à¸±à¸‡à¸à¹ˆà¸­à¸™ à¹€à¸Šà¹ˆà¸™à¸”à¸±à¸‡à¹€à¸à¹ˆà¸² à¹€à¸Šà¹ˆà¸™à¸”à¸±à¸‡à¸—à¸µà¹ˆ à¹€à¸Šà¹ˆà¸™à¸”à¸±à¸‡à¸§à¹ˆà¸²\nà¹€à¸Šà¹ˆà¸™à¹€à¸”à¸µà¸¢à¸§à¸à¸±à¸™ à¹€à¸Šà¹ˆà¸™à¹€à¸”à¸µà¸¢à¸§à¸à¸±à¸š à¹€à¸Šà¹ˆà¸™à¹ƒà¸” à¹€à¸Šà¹ˆà¸™à¸—à¸µà¹ˆ à¹€à¸Šà¹ˆà¸™à¸—à¸µà¹ˆà¹€à¸„à¸¢ à¹€à¸Šà¹ˆà¸™à¸—à¸µà¹ˆà¸§à¹ˆà¸² à¹€à¸Šà¹ˆà¸™à¸™à¸±à¹‰à¸™ à¹€à¸Šà¹ˆà¸™à¸™à¸±à¹‰à¸™à¹€à¸­à¸‡ à¹€à¸Šà¹ˆà¸™à¸™à¸µà¹‰ à¹€à¸Šà¹ˆà¸™à¹€à¸¡à¸·à¹ˆà¸­ à¹€à¸Šà¹ˆà¸™à¹„à¸£\nà¹€à¸Šà¸·à¹ˆà¸­ à¹€à¸Šà¸·à¹ˆà¸­à¸–à¸·à¸­ à¹€à¸Šà¸·à¹ˆà¸­à¸¡à¸±à¹ˆà¸™ à¹€à¸Šà¸·à¹ˆà¸­à¸§à¹ˆà¸² à¹ƒà¸Šà¹ˆ à¹ƒà¸Šà¹‰ à¸‹à¸° à¸‹à¸°à¸à¹ˆà¸­à¸™ à¸‹à¸°à¸ˆà¸™ à¸‹à¸°à¸ˆà¸™à¸à¸£à¸°à¸—à¸±à¹ˆà¸‡ à¸‹à¸°à¸ˆà¸™à¸–à¸¶à¸‡ à¸”à¸±à¹ˆà¸‡à¹€à¸„à¸¢ à¸•à¹ˆà¸²à¸‡à¸à¹‡ à¸•à¹ˆà¸²à¸‡à¸«à¸²à¸\nà¸•à¸²à¸¡à¸”à¹‰à¸§à¸¢ à¸•à¸²à¸¡à¹à¸•à¹ˆ à¸•à¸²à¸¡à¸—à¸µà¹ˆ à¸•à¸²à¸¡à¹† à¹€à¸•à¹‡à¸¡à¹„à¸›à¸”à¹‰à¸§à¸¢ à¹€à¸•à¹‡à¸¡à¹„à¸›à¸«à¸¡à¸” à¹€à¸•à¹‡à¸¡à¹† à¹à¸•à¹ˆà¸à¹‡ à¹à¸•à¹ˆà¸à¹ˆà¸­à¸™ à¹à¸•à¹ˆà¸ˆà¸° à¹à¸•à¹ˆà¹€à¸”à¸´à¸¡ à¹à¸•à¹ˆà¸•à¹‰à¸­à¸‡ à¹à¸•à¹ˆà¸–à¹‰à¸²\nà¹à¸•à¹ˆà¸—à¸§à¹ˆà¸² à¹à¸•à¹ˆà¸—à¸µà¹ˆ à¹à¸•à¹ˆà¸™à¸±à¹‰à¸™ à¹à¸•à¹ˆà¹€à¸à¸µà¸¢à¸‡ à¹à¸•à¹ˆà¹€à¸¡à¸·à¹ˆà¸­ à¹à¸•à¹ˆà¹„à¸£ à¹à¸•à¹ˆà¸¥à¸° à¹à¸•à¹ˆà¸§à¹ˆà¸² à¹à¸•à¹ˆà¹„à¸«à¸™ à¹à¸•à¹ˆà¸­à¸¢à¹ˆà¸²à¸‡à¹ƒà¸” à¹‚à¸• à¹‚à¸•à¹† à¹ƒà¸•à¹‰ à¸–à¹‰à¸²à¸ˆà¸° à¸–à¹‰à¸²à¸«à¸²à¸\nà¸—à¸±à¹‰à¸‡à¸›à¸§à¸‡ à¸—à¸±à¹‰à¸‡à¹€à¸›à¹‡à¸™ à¸—à¸±à¹‰à¸‡à¸¡à¸§à¸¥ à¸—à¸±à¹‰à¸‡à¸ªà¸´à¹‰à¸™ à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸” à¸—à¸±à¹‰à¸‡à¸«à¸¥à¸²à¸¢ à¸—à¸±à¹‰à¸‡à¹† à¸—à¸±à¸™ à¸—à¸±à¸™à¹ƒà¸”à¸™à¸±à¹‰à¸™ à¸—à¸±à¸™à¸—à¸µ à¸—à¸±à¸™à¸—à¸µà¸—à¸±à¸™à¹ƒà¸” à¸—à¸±à¹ˆà¸§ à¸—à¸³à¹ƒà¸«à¹‰ à¸—à¸³à¹† à¸—à¸µ à¸—à¸µà¹ˆà¸ˆà¸£à¸´à¸‡\nà¸—à¸µà¹ˆà¸‹à¸¶à¹ˆà¸‡ à¸—à¸µà¹€à¸”à¸µà¸¢à¸§ à¸—à¸µà¹ƒà¸” à¸—à¸µà¹ˆà¹ƒà¸” à¸—à¸µà¹ˆà¹„à¸”à¹‰ à¸—à¸µà¹€à¸–à¸­à¸° à¸—à¸µà¹ˆà¹à¸—à¹‰ à¸—à¸µà¹ˆà¹à¸—à¹‰à¸ˆà¸£à¸´à¸‡ à¸—à¸µà¹ˆà¸™à¸±à¹‰à¸™ à¸—à¸µà¹ˆà¸™à¸µà¹‰ à¸—à¸µà¹„à¸£ à¸—à¸µà¸¥à¸° à¸—à¸µà¹ˆà¸¥à¸° à¸—à¸µà¹ˆà¹à¸¥à¹‰à¸§ à¸—à¸µà¹ˆà¸§à¹ˆà¸² à¸—à¸µà¹ˆà¹à¸«à¹ˆà¸‡à¸™à¸±à¹‰à¸™ à¸—à¸µà¹† à¸—à¸µà¹ˆà¹†\nà¸—à¸¸à¸à¸„à¸™ à¸—à¸¸à¸à¸„à¸£à¸±à¹‰à¸‡ à¸—à¸¸à¸à¸„à¸£à¸² à¸—à¸¸à¸à¸„à¸£à¸²à¸§ à¸—à¸¸à¸à¸Šà¸´à¹‰à¸™ à¸—à¸¸à¸à¸•à¸±à¸§ à¸—à¸¸à¸à¸—à¸²à¸‡ à¸—à¸¸à¸à¸—à¸µ à¸—à¸¸à¸à¸—à¸µà¹ˆ à¸—à¸¸à¸à¹€à¸¡à¸·à¹ˆà¸­ à¸—à¸¸à¸à¸§à¸±à¸™ à¸—à¸¸à¸à¸§à¸±à¸™à¸™à¸µà¹‰ à¸—à¸¸à¸à¸ªà¸´à¹ˆà¸‡ à¸—à¸¸à¸à¸«à¸™ à¸—à¸¸à¸à¹à¸«à¹ˆà¸‡\nà¸—à¸¸à¸à¸­à¸¢à¹ˆà¸²à¸‡ à¸—à¸¸à¸à¸­à¸±à¸™ à¸—à¸¸à¸à¹† à¹€à¸—à¹ˆà¸² à¹€à¸—à¹ˆà¸²à¸à¸±à¸™ à¹€à¸—à¹ˆà¸²à¸à¸±à¸š à¹€à¸—à¹ˆà¸²à¹ƒà¸” à¹€à¸—à¹ˆà¸²à¸—à¸µà¹ˆ à¹€à¸—à¹ˆà¸²à¸™à¸±à¹‰à¸™ à¹€à¸—à¹ˆà¸²à¸™à¸µà¹‰ à¹à¸—à¹‰ à¹à¸—à¹‰à¸ˆà¸£à¸´à¸‡ à¹€à¸˜à¸­ à¸™à¸±à¹‰à¸™à¹„à¸§ à¸™à¸±à¸šà¹à¸•à¹ˆà¸™à¸µà¹‰\nà¸™à¸²à¸‡ à¸™à¸²à¸‡à¸ªà¸²à¸§ à¸™à¹ˆà¸²à¸ˆà¸° à¸™à¸²à¸™ à¸™à¸²à¸™à¹† à¸™à¸²à¸¢ à¸™à¸³ à¸™à¸³à¸à¸² à¸™à¸³à¸¡à¸² à¸™à¸´à¸” à¸™à¸´à¸”à¸«à¸™à¹ˆà¸­à¸¢ à¸™à¸´à¸”à¹† à¸™à¸µà¹ˆ à¸™à¸µà¹ˆà¹„à¸‡ à¸™à¸µà¹ˆà¸™à¸² à¸™à¸µà¹ˆà¹à¸™à¹ˆà¸° à¸™à¸µà¹ˆà¹à¸«à¸¥à¸° à¸™à¸µà¹‰à¹à¸«à¸¥à¹ˆ\nà¸™à¸µà¹ˆà¹€à¸­à¸‡ à¸™à¸µà¹‰à¹€à¸­à¸‡ à¸™à¸¹à¹ˆà¸™ à¸™à¸¹à¹‰à¸™ à¹€à¸™à¹‰à¸™ à¹€à¸™à¸µà¹ˆà¸¢ à¹€à¸™à¸µà¹ˆà¸¢à¹€à¸­à¸‡ à¹ƒà¸™à¸Šà¹ˆà¸§à¸‡ à¹ƒà¸™à¸—à¸µà¹ˆ à¹ƒà¸™à¹€à¸¡à¸·à¹ˆà¸­ à¹ƒà¸™à¸£à¸°à¸«à¸§à¹ˆà¸²à¸‡ à¸šà¸™ à¸šà¸­à¸ à¸šà¸­à¸à¹à¸¥à¹‰à¸§ à¸šà¸­à¸à¸§à¹ˆà¸² à¸šà¹ˆà¸­à¸¢\nà¸šà¹ˆà¸­à¸¢à¸à¸§à¹ˆà¸² à¸šà¹ˆà¸­à¸¢à¸„à¸£à¸±à¹‰à¸‡ à¸šà¹ˆà¸­à¸¢à¹† à¸šà¸±à¸”à¸”à¸¥ à¸šà¸±à¸”à¹€à¸”à¸µà¹‹à¸¢à¸§à¸™à¸µà¹‰ à¸šà¸±à¸”à¸™à¸±à¹‰à¸™ à¸šà¹‰à¸²à¸‡ à¸šà¸²à¸‡à¸à¸§à¹ˆà¸² à¸šà¸²à¸‡à¸‚à¸“à¸° à¸šà¸²à¸‡à¸„à¸£à¸±à¹‰à¸‡ à¸šà¸²à¸‡à¸„à¸£à¸² à¸šà¸²à¸‡à¸„à¸£à¸²à¸§\nà¸šà¸²à¸‡à¸—à¸µ à¸šà¸²à¸‡à¸—à¸µà¹ˆ à¸šà¸²à¸‡à¹à¸«à¹ˆà¸‡ à¸šà¸²à¸‡à¹† à¸›à¸à¸´à¸šà¸±à¸•à¸´ à¸›à¸£à¸°à¸à¸­à¸š à¸›à¸£à¸°à¸à¸²à¸£ à¸›à¸£à¸°à¸à¸²à¸£à¸‰à¸°à¸™à¸µà¹‰ à¸›à¸£à¸°à¸à¸²à¸£à¹ƒà¸” à¸›à¸£à¸°à¸à¸²à¸£à¸«à¸™à¸¶à¹ˆà¸‡ à¸›à¸£à¸°à¸¡à¸²à¸“\nà¸›à¸£à¸°à¸ªà¸š à¸›à¸£à¸±à¸š à¸›à¸£à¸²à¸à¸ à¸›à¸£à¸²à¸à¸à¸§à¹ˆà¸² à¸›à¸±à¸ˆà¸ˆà¸¸à¸šà¸±à¸™ à¸›à¸´à¸” à¹€à¸›à¹‡à¸™à¸”à¹‰à¸§à¸¢ à¹€à¸›à¹‡à¸™à¸”à¸±à¸‡ à¸œà¸¹à¹‰à¹ƒà¸” à¹€à¸œà¸·à¹ˆà¸­ à¹€à¸œà¸·à¹ˆà¸­à¸ˆà¸° à¹€à¸œà¸·à¹ˆà¸­à¸—à¸µà¹ˆ à¹€à¸œà¸·à¹ˆà¸­à¸§à¹ˆà¸² à¸à¹ˆà¸²à¸¢ à¸à¹ˆà¸²à¸¢à¹ƒà¸”\nà¸à¸šà¸§à¹ˆà¸² à¸à¸¢à¸²à¸¢à¸²à¸¡ à¸à¸£à¹‰à¸­à¸¡à¸à¸±à¸™ à¸à¸£à¹‰à¸­à¸¡à¸à¸±à¸š à¸à¸£à¹‰à¸­à¸¡à¸”à¹‰à¸§à¸¢ à¸à¸£à¹‰à¸­à¸¡à¸—à¸±à¹‰à¸‡ à¸à¸£à¹‰à¸­à¸¡à¸—à¸µà¹ˆ à¸à¸£à¹‰à¸­à¸¡à¹€à¸à¸µà¸¢à¸‡ à¸à¸§à¸ à¸à¸§à¸à¸à¸±à¸™ à¸à¸§à¸à¸à¸¹ à¸à¸§à¸à¹à¸\nà¸à¸§à¸à¹€à¸‚à¸² à¸à¸§à¸à¸„à¸¸à¸“ à¸à¸§à¸à¸‰à¸±à¸™ à¸à¸§à¸à¸—à¹ˆà¸²à¸™ à¸à¸§à¸à¸—à¸µà¹ˆ à¸à¸§à¸à¹€à¸˜à¸­ à¸à¸§à¸à¸™à¸±à¹‰à¸™ à¸à¸§à¸à¸™à¸µà¹‰ à¸à¸§à¸à¸™à¸¹à¹‰à¸™ à¸à¸§à¸à¹‚à¸™à¹‰à¸™ à¸à¸§à¸à¸¡à¸±à¸™ à¸à¸§à¸à¸¡à¸¶à¸‡ à¸à¸­ à¸à¸­à¸à¸±à¸™\nà¸à¸­à¸„à¸§à¸£ à¸à¸­à¸ˆà¸° à¸à¸­à¸”à¸µ à¸à¸­à¸•à¸±à¸§ à¸à¸­à¸—à¸µ à¸à¸­à¸—à¸µà¹ˆ à¸à¸­à¹€à¸à¸µà¸¢à¸‡ à¸à¸­à¹à¸¥à¹‰à¸§ à¸à¸­à¸ªà¸¡ à¸à¸­à¸ªà¸¡à¸„à¸§à¸£ à¸à¸­à¹€à¸«à¸¡à¸²à¸° à¸à¸­à¹† à¸à¸² à¸à¸¶à¸‡ à¸à¸¶à¹ˆà¸‡ à¸à¸·à¹‰à¸™à¹† à¸à¸¹à¸”\nà¹€à¸à¸£à¸²à¸°à¸‰à¸°à¸™à¸±à¹‰à¸™ à¹€à¸à¸£à¸²à¸°à¸§à¹ˆà¸² à¹€à¸à¸´à¹ˆà¸‡ à¹€à¸à¸´à¹ˆà¸‡à¸ˆà¸° à¹€à¸à¸´à¹ˆà¸¡ à¹€à¸à¸´à¹ˆà¸¡à¹€à¸•à¸´à¸¡ à¹€à¸à¸µà¸¢à¸‡ à¹€à¸à¸µà¸¢à¸‡à¹à¸„à¹ˆ à¹€à¸à¸µà¸¢à¸‡à¹ƒà¸” à¹€à¸à¸µà¸¢à¸‡à¹à¸•à¹ˆ à¹€à¸à¸µà¸¢à¸‡à¸à¸­ à¹€à¸à¸µà¸¢à¸‡à¹€à¸à¸£à¸²à¸°\nà¸¡à¸²à¸à¸à¸§à¹ˆà¸² à¸¡à¸²à¸à¸¡à¸²à¸¢ à¸¡à¸´ à¸¡à¸´à¸‰à¸°à¸™à¸±à¹‰à¸™ à¸¡à¸´à¹ƒà¸Šà¹ˆ à¸¡à¸´à¹„à¸”à¹‰ à¸¡à¸µà¹à¸•à¹ˆ à¸¡à¸¶à¸‡ à¸¡à¸¸à¹ˆà¸‡ à¸¡à¸¸à¹ˆà¸‡à¹€à¸™à¹‰à¸™ à¸¡à¸¸à¹ˆà¸‡à¸«à¸¡à¸²à¸¢ à¹€à¸¡à¸·à¹ˆà¸­à¸à¹ˆà¸­à¸™ à¹€à¸¡à¸·à¹ˆà¸­à¸„à¸£à¸±à¹‰à¸‡ à¹€à¸¡à¸·à¹ˆà¸­à¸„à¸£à¸±à¹‰à¸‡à¸à¹ˆà¸­à¸™\nà¹€à¸¡à¸·à¹ˆà¸­à¸„à¸£à¸²à¸§à¸à¹ˆà¸­à¸™ à¹€à¸¡à¸·à¹ˆà¸­à¸„à¸£à¸²à¸§à¸—à¸µà¹ˆ à¹€à¸¡à¸·à¹ˆà¸­à¸„à¸£à¸²à¸§ à¹€à¸¡à¸·à¹ˆà¸­à¸„à¸·à¸™ à¹€à¸¡à¸·à¹ˆà¸­à¹€à¸Šà¹‰à¸² à¹€à¸¡à¸·à¹ˆà¸­à¹ƒà¸” à¹€à¸¡à¸·à¹ˆà¸­à¸™à¸±à¹‰à¸™ à¹€à¸¡à¸·à¹ˆà¸­à¸™à¸µà¹‰ à¹€à¸¡à¸·à¹ˆà¸­à¹€à¸¢à¹‡à¸™ à¹€à¸¡à¸·à¹ˆà¸­à¸§à¸±à¸™à¸§à¸²à¸™ à¹€à¸¡à¸·à¹ˆà¸­à¸§à¸²à¸™\nà¹à¸¡à¹‰ à¹à¸¡à¹‰à¸à¸£à¸°à¸—à¸±à¹ˆà¸‡ à¹à¸¡à¹‰à¹à¸•à¹ˆ à¹à¸¡à¹‰à¸™à¸§à¹ˆà¸² à¹à¸¡à¹‰à¸§à¹ˆà¸² à¹„à¸¡à¹ˆà¸„à¹ˆà¸­à¸¢ à¹„à¸¡à¹ˆà¸„à¹ˆà¸­à¸¢à¸ˆà¸° à¹„à¸¡à¹ˆà¸„à¹ˆà¸­à¸¢à¹€à¸›à¹‡à¸™ à¹„à¸¡à¹ˆà¹ƒà¸Šà¹ˆ à¹„à¸¡à¹ˆà¹€à¸›à¹‡à¸™à¹„à¸£ à¹„à¸¡à¹ˆà¸§à¹ˆà¸² à¸¢à¸ à¸¢à¸à¹ƒà¸«à¹‰ à¸¢à¸­à¸¡\nà¸¢à¸­à¸¡à¸£à¸±à¸š à¸¢à¹ˆà¸­à¸¡ à¸¢à¹ˆà¸­à¸¢ à¸¢à¸±à¸‡à¸„à¸‡ à¸¢à¸±à¸‡à¸‡à¸±à¹‰à¸™ à¸¢à¸±à¸‡à¸‡à¸µà¹‰ à¸¢à¸±à¸‡à¹‚à¸‡à¹‰à¸™ à¸¢à¸±à¸‡à¹„à¸‡ à¸¢à¸±à¸‡à¸ˆà¸° à¸¢à¸±à¸‡à¹à¸•à¹ˆ à¸¢à¸²à¸ à¸¢à¸²à¸§ à¸¢à¸²à¸§à¸™à¸²à¸™ à¸¢à¸´à¹ˆà¸‡ à¸¢à¸´à¹ˆà¸‡à¸à¸§à¹ˆà¸² à¸¢à¸´à¹ˆà¸‡à¸‚à¸¶à¹‰à¸™\nà¸¢à¸´à¹ˆà¸‡à¸‚à¸¶à¹‰à¸™à¹„à¸› à¸¢à¸´à¹ˆà¸‡à¸ˆà¸™ à¸¢à¸´à¹ˆà¸‡à¸ˆà¸° à¸¢à¸´à¹ˆà¸‡à¸™à¸±à¸ à¸¢à¸´à¹ˆà¸‡à¹€à¸¡à¸·à¹ˆà¸­ à¸¢à¸´à¹ˆà¸‡à¹à¸¥à¹‰à¸§ à¸¢à¸´à¹ˆà¸‡à¹ƒà¸«à¸à¹ˆ à¹€à¸£à¹‡à¸§ à¹€à¸£à¹‡à¸§à¹† à¹€à¸£à¸²à¹† à¹€à¸£à¸µà¸¢à¸ à¹€à¸£à¸µà¸¢à¸š à¹€à¸£à¸·à¹ˆà¸­à¸¢ à¹€à¸£à¸·à¹ˆà¸­à¸¢à¹† à¸¥à¹‰à¸§à¸™\nà¸¥à¹‰à¸§à¸™à¸ˆà¸™ à¸¥à¹‰à¸§à¸™à¹à¸•à¹ˆ à¸¥à¸° à¸¥à¹ˆà¸²à¸ªà¸¸à¸” à¹€à¸¥à¹‡à¸ à¹€à¸¥à¹‡à¸à¸™à¹‰à¸­à¸¢ à¹€à¸¥à¹‡à¸à¹† à¹€à¸¥à¹ˆà¸²à¸§à¹ˆà¸² à¹à¸¥à¹‰à¸§à¸à¸±à¸™ à¹à¸¥à¹‰à¸§à¹à¸•à¹ˆ à¹à¸¥à¹‰à¸§à¹€à¸ªà¸£à¹‡à¸ˆ à¸§à¸±à¸™à¹ƒà¸” à¸§à¸±à¸™à¸™à¸±à¹‰à¸™ à¸§à¸±à¸™à¸™à¸µà¹‰ à¸§à¸±à¸™à¹„à¸«à¸™\nà¸ªà¸šà¸²à¸¢ à¸ªà¸¡à¸±à¸¢ à¸ªà¸¡à¸±à¸¢à¸à¹ˆà¸­à¸™ à¸ªà¸¡à¸±à¸¢à¸™à¸±à¹‰à¸™ à¸ªà¸¡à¸±à¸¢à¸™à¸µà¹‰ à¸ªà¸¡à¸±à¸¢à¹‚à¸™à¹‰à¸™ à¸ªà¹ˆà¸§à¸™à¹€à¸à¸´à¸™ à¸ªà¹ˆà¸§à¸™à¸”à¹‰à¸­à¸¢ à¸ªà¹ˆà¸§à¸™à¸”à¸µ à¸ªà¹ˆà¸§à¸™à¹ƒà¸” à¸ªà¹ˆà¸§à¸™à¸—à¸µà¹ˆ à¸ªà¹ˆà¸§à¸™à¸™à¹‰à¸­à¸¢ à¸ªà¹ˆà¸§à¸™à¸™à¸±à¹‰à¸™ à¸ª\nà¹ˆà¸§à¸™à¸¡à¸²à¸ à¸ªà¹ˆà¸§à¸™à¹ƒà¸«à¸à¹ˆ à¸ªà¸±à¹‰à¸™ à¸ªà¸±à¹‰à¸™à¹† à¸ªà¸²à¸¡à¸²à¸£à¸– à¸ªà¸³à¸„à¸±à¸ à¸ªà¸´à¹ˆà¸‡ à¸ªà¸´à¹ˆà¸‡à¹ƒà¸” à¸ªà¸´à¹ˆà¸‡à¸™à¸±à¹‰à¸™ à¸ªà¸´à¹ˆà¸‡à¸™à¸µà¹‰ à¸ªà¸´à¹ˆà¸‡à¹„à¸«à¸™ à¸ªà¸´à¹‰à¸™ à¹à¸ªà¸”à¸‡ à¹à¸ªà¸”à¸‡à¸§à¹ˆà¸² à¸«à¸™ à¸«à¸™à¸­ à¸«à¸™à¸­à¸¢\nà¸«à¸™à¹ˆà¸­à¸¢ à¸«à¸¡à¸” à¸«à¸¡à¸”à¸à¸±à¸™ à¸«à¸¡à¸”à¸ªà¸´à¹‰à¸™ à¸«à¸²à¸à¹à¸¡à¹‰ à¸«à¸²à¸à¹à¸¡à¹‰à¸™ à¸«à¸²à¸à¹à¸¡à¹‰à¸™à¸§à¹ˆà¸² à¸«à¸²à¸à¸§à¹ˆà¸² à¸«à¸²à¸„à¸§à¸²à¸¡ à¸«à¸²à¹ƒà¸Šà¹ˆ à¸«à¸²à¸£à¸·à¸­ à¹€à¸«à¸•à¸¸ à¹€à¸«à¸•à¸¸à¸œà¸¥ à¹€à¸«à¸•à¸¸à¸™à¸±à¹‰à¸™\nà¹€à¸«à¸•à¸¸à¸™à¸µà¹‰ à¹€à¸«à¸•à¸¸à¹„à¸£ à¹€à¸«à¹‡à¸™à¹à¸à¹ˆ à¹€à¸«à¹‡à¸™à¸„à¸§à¸£ à¹€à¸«à¹‡à¸™à¸ˆà¸° à¹€à¸«à¹‡à¸™à¸§à¹ˆà¸² à¹€à¸«à¸¥à¸·à¸­ à¹€à¸«à¸¥à¸·à¸­à¹€à¸à¸´à¸™ à¹€à¸«à¸¥à¹ˆà¸² à¹€à¸«à¸¥à¹ˆà¸²à¸™à¸±à¹‰à¸™ à¹€à¸«à¸¥à¹ˆà¸²à¸™à¸µà¹‰ à¹à¸«à¹ˆà¸‡à¹ƒà¸” à¹à¸«à¹ˆà¸‡à¸™à¸±à¹‰à¸™\nà¹à¸«à¹ˆà¸‡à¸™à¸µà¹‰ à¹à¸«à¹ˆà¸‡à¹‚à¸™à¹‰à¸™ à¹à¸«à¹ˆà¸‡à¹„à¸«à¸™ à¹à¸«à¸¥à¸° à¹ƒà¸«à¹‰à¹à¸à¹ˆ à¹ƒà¸«à¸à¹ˆ à¹ƒà¸«à¸à¹ˆà¹‚à¸• à¸­à¸¢à¹ˆà¸²à¸‡à¸¡à¸²à¸ à¸­à¸¢à¹ˆà¸²à¸‡à¸¢à¸´à¹ˆà¸‡ à¸­à¸¢à¹ˆà¸²à¸‡à¹„à¸£à¸à¹‡ à¸­à¸¢à¹ˆà¸²à¸‡à¹„à¸£à¸à¹‡à¹„à¸”à¹‰ à¸­à¸¢à¹ˆà¸²à¸‡à¹„à¸£à¹€à¸ªà¸µà¸¢\nà¸­à¸¢à¹ˆà¸²à¸‡à¸¥à¸° à¸­à¸¢à¹ˆà¸²à¸‡à¸«à¸™à¸¶à¹ˆà¸‡ à¸­à¸¢à¹ˆà¸²à¸‡à¹† à¸­à¸±à¸™ à¸­à¸±à¸™à¸ˆà¸° à¸­à¸±à¸™à¹„à¸”à¹‰à¹à¸à¹ˆ à¸­à¸±à¸™à¸—à¸µà¹ˆ à¸­à¸±à¸™à¸—à¸µà¹ˆà¸ˆà¸£à¸´à¸‡ à¸­à¸±à¸™à¸—à¸µà¹ˆà¸ˆà¸° à¸­à¸±à¸™à¹€à¸™à¸·à¹ˆà¸­à¸‡à¸¡à¸²à¸ˆà¸²à¸ à¸­à¸±à¸™à¸¥à¸° à¸­à¸±à¸™à¹† à¸­à¸²à¸ˆà¸ˆà¸°\nà¸­à¸²à¸ˆà¹€à¸›à¹‡à¸™ à¸­à¸²à¸ˆà¹€à¸›à¹‡à¸™à¸”à¹‰à¸§à¸¢ à¸­à¸·à¹ˆà¸™ à¸­à¸·à¹ˆà¸™à¹† à¹€à¸­à¹‡à¸‡ à¹€à¸­à¸² à¸¯ à¸¯à¸¥ à¸¯à¸¥à¸¯ 555 à¸à¸³ à¸‚à¸­à¹‚à¸—à¸© à¹€à¸¢à¸µà¹ˆà¸¢à¸¡ à¸™à¸µà¹ˆà¸„à¸·à¸­\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/th/norm_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/th/tag_map.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/tr/__init__.py----------------------------------------
A:spacy.lang.tr.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.tr.__init__.lex_attr_getters[NORM]->add_lookups(Language.Defaults.lex_attr_getters[NORM], BASE_NORMS)
A:spacy.lang.tr.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS, TOKENIZER_EXCEPTIONS)
spacy.lang.tr.__init__.Turkish(Language)
spacy.lang.tr.__init__.TurkishDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/tr/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/tr/lex_attrs.py----------------------------------------
A:spacy.lang.tr.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.tr.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.tr.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/tr/tokenizer_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/tr/stop_words.py----------------------------------------
A:spacy.lang.tr.stop_words.STOP_WORDS->set('\nacaba\nacep\nadamakÄ±llÄ±\nadeta\nait\nama\namma\nanca\nancak\narada\nartÄ±k\naslÄ±nda\naynen\nayrÄ±ca\naz\naÃ§Ä±kÃ§a\naÃ§Ä±kÃ§asÄ±\nbana\nbari\nbazen\nbazÄ±\nbazÄ±sÄ±\nbazÄ±sÄ±na\nbazÄ±sÄ±nda\nbazÄ±sÄ±ndan\nbazÄ±sÄ±nÄ±\nbazÄ±sÄ±nÄ±n\nbaÅŸkasÄ±\nbaÅŸkasÄ±na\nbaÅŸkasÄ±nda\nbaÅŸkasÄ±ndan\nbaÅŸkasÄ±nÄ±\nbaÅŸkasÄ±nÄ±n\nbaÅŸka\nbelki\nben\nbende\nbenden\nbeni\nbenim\nberi\nberiki\nberikinin\nberikiyi\nberisi\nbilcÃ¼mle\nbile\nbinaen\nbinaenaleyh\nbiraz\nbirazdan\nbirbiri\nbirbirine\nbirbirini\nbirbirinin\nbirbirinde\nbirbirinden\nbirden\nbirdenbire\nbiri\nbirine\nbirini\nbirinin\nbirinde\nbirinden\nbirice\nbirileri\nbirilerinde\nbirilerinden\nbirilerine\nbirilerini\nbirilerinin\nbirisi\nbirisine\nbirisini\nbirisinin\nbirisinde\nbirisinden\nbirkaÃ§\nbirkaÃ§Ä±\nbirkaÃ§Ä±na\nbirkaÃ§Ä±nÄ±\nbirkaÃ§Ä±nÄ±n\nbirkaÃ§Ä±nda\nbirkaÃ§Ä±ndan\nbirkez\nbirlikte\nbirÃ§ok\nbirÃ§oÄŸu\nbirÃ§oÄŸuna\nbirÃ§oÄŸunda\nbirÃ§oÄŸundan\nbirÃ§oÄŸunu\nbirÃ§oÄŸunun\nbirÅŸey\nbirÅŸeyi\nbitevi\nbiteviye\nbittabi\nbiz\nbizatihi\nbizce\nbizcileyin\nbizden\nbize\nbizi\nbizim\nbizimki\nbizzat\nboÅŸuna\nbu\nbuna\nbunda\nbundan\nbunlar\nbunlarÄ±\nbunlarÄ±n\nbunu\nbunun\nburacÄ±kta\nburada\nburadan\nburasÄ±\nburasÄ±na\nburasÄ±nÄ±\nburasÄ±nÄ±n\nburasÄ±nda\nburasÄ±ndan\nbÃ¶yle\nbÃ¶ylece\nbÃ¶ylecene\nbÃ¶ylelikle\nbÃ¶ylemesine\nbÃ¶ylesine\nbÃ¼sbÃ¼tÃ¼n\nbÃ¼tÃ¼n\ncuk\ncÃ¼mlesi\ncÃ¼mlesine\ncÃ¼mlesini\ncÃ¼mlesinin\ncÃ¼mlesinden\ncÃ¼mlemize\ncÃ¼mlemizi\ncÃ¼mlemizden\nÃ§abuk\nÃ§abukÃ§a\nÃ§eÅŸitli\nÃ§ok\nÃ§oklarÄ±\nÃ§oklarÄ±nca\nÃ§okluk\nÃ§oklukla\nÃ§okÃ§a\nÃ§oÄŸu\nÃ§oÄŸun\nÃ§oÄŸunca\nÃ§oÄŸunda\nÃ§oÄŸundan\nÃ§oÄŸunlukla\nÃ§oÄŸunu\nÃ§oÄŸunun\nÃ§Ã¼nkÃ¼\nda\ndaha\ndahasÄ±\ndahi\ndahil\ndahilen\ndaima\ndair\ndayanarak\nde\ndefa\ndek\ndemin\ndemincek\ndeminden\ndenli\nderakap\nderhal\nderken\ndeÄŸil\ndeÄŸin\ndiye\ndiÄŸer\ndiÄŸeri\ndiÄŸerine\ndiÄŸerini\ndiÄŸerinden\ndolayÄ±\ndolayÄ±sÄ±yla\ndoÄŸru\nedecek\neden\nederek\nedilecek\nediliyor\nedilmesi\nediyor\nelbet\nelbette\nemme\nen\nenikonu\nepey\nepeyce\nepeyi\nesasen\nesnasÄ±nda\netmesi\netraflÄ±\netraflÄ±ca\netti\nettiÄŸi\nettiÄŸini\nevleviyetle\nevvel\nevvela\nevvelce\nevvelden\nevvelemirde\nevveli\neÄŸer\nfakat\nfilanca\nfilancanÄ±n\ngah\ngayet\ngayetle\ngayri\ngayrÄ±\ngelgelelim\ngene\ngerek\ngerÃ§i\ngeÃ§ende\ngeÃ§enlerde\ngibi\ngibilerden\ngibisinden\ngine\ngÃ¶re\ngÄ±rla\nhakeza\nhalbuki\nhalen\nhalihazÄ±rda\nhaliyle\nhandiyse\nhangi\nhangisi\nhangisine\nhangisine\nhangisinde\nhangisinden\nhani\nhariÃ§\nhasebiyle\nhasÄ±lÄ±\nhatta\nhele\nhem\nhenÃ¼z\nhep\nhepsi\nhepsini\nhepsinin\nhepsinde\nhepsinden\nher\nherhangi\nherkes\nherkesi\nherkesin\nherkesten\nhiÃ§\nhiÃ§bir\nhiÃ§biri\nhiÃ§birine\nhiÃ§birini\nhiÃ§birinin\nhiÃ§birinde\nhiÃ§birinden\nhoÅŸ\nhulasaten\niken\nila\nile\nilen\nilgili\nilk\nilla\nillaki\nimdi\nindinde\ninen\ninsermi\nise\nister\nitibaren\nitibariyle\nitibarÄ±yla\niyi\niyice\niyicene\niÃ§in\niÅŸ\niÅŸte\nkadar\nkaffesi\nkah\nkala\nkanÄ±mca\nkarÅŸÄ±n\nkaynak\nkaÃ§Ä±\nkaÃ§Ä±na\nkaÃ§Ä±nda\nkaÃ§Ä±ndan\nkaÃ§Ä±nÄ±\nkaÃ§Ä±nÄ±n\nkelli\nkendi\nkendilerinde\nkendilerinden\nkendilerine\nkendilerini\nkendilerinin\nkendini\nkendisi\nkendisinde\nkendisinden\nkendisine\nkendisini\nkendisinin\nkere\nkez\nkeza\nkezalik\nkeÅŸke\nki\nkim\nkimden\nkime\nkimi\nkiminin\nkimisi\nkimisinde\nkimisinden\nkimisine\nkimisinin\nkimse\nkimsecik\nkimsecikler\nkÃ¼lliyen\nkÄ±saca\nkÄ±sacasÄ±\nlakin\nleh\nlÃ¼tfen\nmaada\nmadem\nmademki\nmamafih\nmebni\nmeÄ‘er\nmeÄŸer\nmeÄŸerki\nmeÄŸerse\nmu\nmÃ¼\nmÄ±\nmi\nnasÄ±l\nnasÄ±lsa\nnazaran\nnaÅŸi\nne\nneden\nnedeniyle\nnedenle\nnedenler\nnedenlerden\nnedense\nnerde\nnerden\nnerdeyse\nnere\nnerede\nnereden\nneredeyse\nneresi\nnereye\nnetekim\nneye\nneyi\nneyse\nnice\nnihayet\nnihayetinde\nnitekim\nniye\nniÃ§in\no\nolan\nolarak\noldu\nolduklarÄ±nÄ±\noldukÃ§a\nolduÄŸu\nolduÄŸunu\nolmak\nolmasÄ±\nolsa\nolsun\nolup\nolur\nolursa\noluyor\nona\nonca\nonculayÄ±n\nonda\nondan\nonlar\nonlara\nonlardan\nonlarÄ±\nonlarÄ±n\nonu\nonun\nora\noracÄ±k\noracÄ±kta\norada\noradan\noranca\noranla\noraya\noysa\noysaki\nÃ¶bÃ¼r\nÃ¶bÃ¼rkÃ¼\nÃ¶bÃ¼rÃ¼\nÃ¶bÃ¼rÃ¼nde\nÃ¶bÃ¼rÃ¼nden\nÃ¶bÃ¼rÃ¼ne\nÃ¶bÃ¼rÃ¼nÃ¼\nÃ¶nce\nÃ¶nceden\nÃ¶nceleri\nÃ¶ncelikle\nÃ¶teki\nÃ¶tekisi\nÃ¶yle\nÃ¶ylece\nÃ¶ylelikle\nÃ¶ylemesine\nÃ¶z\npek\npekala\npeki\npekÃ§e\npeyderpey\nraÄŸmen\nsadece\nsahi\nsahiden\nsana\nsanki\nsen\nsenden\nseni\nsenin\nsiz\nsizden\nsizi\nsizin\nsonra\nsonradan\nsonralarÄ±\nsonunda\nÅŸayet\nÅŸey\nÅŸeyden\nÅŸeyi\nÅŸeyler\nÅŸu\nÅŸuna\nÅŸuncacÄ±k\nÅŸunda\nÅŸundan\nÅŸunlar\nÅŸunlarÄ±\nÅŸunlarÄ±n\nÅŸunu\nÅŸunun\nÅŸura\nÅŸuracÄ±k\nÅŸuracÄ±kta\nÅŸurasÄ±\nÅŸÃ¶yle\nÅŸimdi\ntabii\ntam\ntamam\ntamamen\ntamamÄ±yla\ntarafÄ±ndan\ntek\ntÃ¼m\nÃ¼zere\nvar\nvardÄ±\nvasÄ±tasÄ±yla\nve\nvelev\nvelhasÄ±l\nvelhasÄ±lÄ±kelam\nveya\nveyahut\nya\nyahut\nyakinen\nyakÄ±nda\nyakÄ±ndan\nyakÄ±nlarda\nyalnÄ±z\nyalnÄ±zca\nyani\nyapacak\nyapmak\nyaptÄ±\nyaptÄ±klarÄ±\nyaptÄ±ÄŸÄ±\nyaptÄ±ÄŸÄ±nÄ±\nyapÄ±lan\nyapÄ±lmasÄ±\nyapÄ±yor\nyeniden\nyenilerde\nyerine\nyine\nyok\nyoksa\nyoluyla\nyÃ¼zÃ¼nden\nzarfÄ±nda\nzaten\nzati\nzira\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ta/__init__.py----------------------------------------
A:spacy.lang.ta.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
spacy.lang.ta.__init__.Tamil(Language)
spacy.lang.ta.__init__.TamilDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ta/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ta/lex_attrs.py----------------------------------------
A:spacy.lang.ta.lex_attrs.length->len(num_suffix)
A:spacy.lang.ta.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.ta.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.ta.lex_attrs.like_num(text)
spacy.lang.ta.lex_attrs.suffix_filter(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ta/stop_words.py----------------------------------------
A:spacy.lang.ta.stop_words.STOP_WORDS->set('\nà®’à®°à¯\nà®à®©à¯à®±à¯\nà®®à®±à¯à®±à¯à®®à¯\nà®‡à®¨à¯à®¤\nà®‡à®¤à¯\nà®à®©à¯à®±\nà®•à¯Šà®£à¯à®Ÿà¯\nà®à®©à¯à®ªà®¤à¯\nà®ªà®²\nà®†à®•à¯à®®à¯\nà®…à®²à¯à®²à®¤à¯\nà®…à®µà®°à¯\nà®¨à®¾à®©à¯\nà®‰à®³à¯à®³\nà®…à®¨à¯à®¤\nà®‡à®µà®°à¯\nà®à®©\nà®®à¯à®¤à®²à¯\nà®à®©à¯à®©\nà®‡à®°à¯à®¨à¯à®¤à¯\nà®šà®¿à®²\nà®à®©à¯\nà®ªà¯‹à®©à¯à®±\nà®µà¯‡à®£à¯à®Ÿà¯à®®à¯\nà®µà®¨à¯à®¤à¯\nà®‡à®¤à®©à¯\nà®…à®¤à¯\nà®…à®µà®©à¯\nà®¤à®¾à®©à¯\nà®ªà®²à®°à¯à®®à¯\nà®à®©à¯à®©à¯à®®à¯\nà®®à¯‡à®²à¯à®®à¯\nà®ªà®¿à®©à¯à®©à®°à¯\nà®•à¯Šà®£à¯à®Ÿ\nà®‡à®°à¯à®•à¯à®•à¯à®®à¯\nà®¤à®©à®¤à¯\nà®‰à®³à¯à®³à®¤à¯\nà®ªà¯‹à®¤à¯\nà®à®©à¯à®±à¯à®®à¯\nà®…à®¤à®©à¯\nà®¤à®©à¯\nà®ªà®¿à®±à®•à¯\nà®…à®µà®°à¯à®•à®³à¯\nà®µà®°à¯ˆ\nà®…à®µà®³à¯\nà®¨à¯€\nà®†à®•à®¿à®¯\nà®‡à®°à¯à®¨à¯à®¤à®¤à¯\nà®‰à®³à¯à®³à®©\nà®µà®¨à¯à®¤\nà®‡à®°à¯à®¨à¯à®¤\nà®®à®¿à®•à®µà¯à®®à¯\nà®‡à®™à¯à®•à¯\nà®®à¯€à®¤à¯\nà®“à®°à¯\nà®‡à®µà¯ˆ\nà®‡à®¨à¯à®¤à®•à¯\nà®ªà®±à¯à®±à®¿\nà®µà®°à¯à®®à¯\nà®µà¯‡à®±à¯\nà®‡à®°à¯\nà®‡à®¤à®¿à®²à¯\nà®ªà¯‹à®²à¯\nà®‡à®ªà¯à®ªà¯‹à®¤à¯\nà®…à®µà®°à®¤à¯\nà®®à®Ÿà¯à®Ÿà¯à®®à¯\nà®‡à®¨à¯à®¤à®ªà¯\nà®à®©à¯à®®à¯\nà®®à¯‡à®²à¯\nà®ªà®¿à®©à¯\nà®šà¯‡à®°à¯à®¨à¯à®¤\nà®†à®•à®¿à®¯à¯‹à®°à¯\nà®à®©à®•à¯à®•à¯\nà®‡à®©à¯à®©à¯à®®à¯\nà®…à®¨à¯à®¤à®ªà¯\nà®…à®©à¯à®±à¯\nà®’à®°à¯‡\nà®®à®¿à®•\nà®…à®™à¯à®•à¯\nà®ªà®²à¯à®µà¯‡à®±à¯\nà®µà®¿à®Ÿà¯à®Ÿà¯\nà®ªà¯†à®°à¯à®®à¯\nà®…à®¤à¯ˆ\nà®ªà®±à¯à®±à®¿à®¯\nà®‰à®©à¯\nà®…à®¤à®¿à®•\nà®…à®¨à¯à®¤à®•à¯\nà®ªà¯‡à®°à¯\nà®‡à®¤à®©à®¾à®²à¯\nà®…à®µà¯ˆ\nà®…à®¤à¯‡\nà®à®©à¯\nà®®à¯à®±à¯ˆ\nà®¯à®¾à®°à¯\nà®à®©à¯à®ªà®¤à¯ˆ\nà®à®²à¯à®²à®¾à®®à¯\nà®®à®Ÿà¯à®Ÿà¯à®®à¯‡\nà®‡à®™à¯à®•à¯‡\nà®…à®™à¯à®•à¯‡\nà®‡à®Ÿà®®à¯\nà®‡à®Ÿà®¤à¯à®¤à®¿à®²à¯\nà®…à®¤à®¿à®²à¯\nà®¨à®¾à®®à¯\nà®…à®¤à®±à¯à®•à¯\nà®à®©à®µà¯‡\nà®ªà®¿à®±\nà®šà®¿à®±à¯\nà®®à®±à¯à®±\nà®µà®¿à®Ÿ\nà®à®¨à¯à®¤\nà®à®©à®µà¯à®®à¯\nà®à®©à®ªà¯à®ªà®Ÿà¯à®®à¯\nà®à®©à®¿à®©à¯à®®à¯\nà®…à®Ÿà¯à®¤à¯à®¤\nà®‡à®¤à®©à¯ˆ\nà®‡à®¤à¯ˆ\nà®•à¯Šà®³à¯à®³\nà®‡à®¨à¯à®¤à®¤à¯\nà®‡à®¤à®±à¯à®•à¯\nà®…à®¤à®©à®¾à®²à¯\nà®¤à®µà®¿à®°\nà®ªà¯‹à®²\nà®µà®°à¯ˆà®¯à®¿à®²à¯\nà®šà®±à¯à®±à¯\nà®à®©à®•à¯\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/ta/norm_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/hu/__init__.py----------------------------------------
A:spacy.lang.hu.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.hu.__init__.lex_attr_getters[NORM]->add_lookups(Language.Defaults.lex_attr_getters[NORM], BASE_NORMS)
A:spacy.lang.hu.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS, TOKENIZER_EXCEPTIONS)
spacy.lang.hu.__init__.Hungarian(Language)
spacy.lang.hu.__init__.HungarianDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/hu/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/hu/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.hu.tokenizer_exceptions._suffixes->'-[{al}]+'.format(al=ALPHA_LOWER)
A:spacy.lang.hu.tokenizer_exceptions._numeric_exp->'({n})(({o})({n}))*[%]?'.format(n=_num, o=_ops)
A:spacy.lang.hu.tokenizer_exceptions._nums->'(({ne})|({t})|({on})|({c}))({s})?'.format(ne=_numeric_exp, t=_time_exp, on=_ord_num_or_date, c=CURRENCY, s=_suffixes)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/hu/stop_words.py----------------------------------------
A:spacy.lang.hu.stop_words.STOP_WORDS->set('\na abban ahhoz ahogy ahol aki akik akkor akÃ¡r alatt amely amelyek amelyekben\namelyeket amelyet amelynek ami amikor amit amolyan amÃ­g annak arra arrÃ³l az\nazok azon azonban azt aztÃ¡n azutÃ¡n azzal azÃ©rt\n\nbe belÃ¼l benne bÃ¡r\n\ncikk cikkek cikkeket csak\n\nde\n\ne ebben eddig egy egyes egyetlen egyik egyre egyÃ©b egÃ©sz ehhez ekkor el ellen\nelo eloszÃ¶r elott elso elÃ©g elÅ‘tt emilyen ennek erre ez ezek ezen ezt ezzel\nezÃ©rt\n\nfel felÃ©\n\nha hanem hiszen hogy hogyan hÃ¡t\n\nide igen ill ill. illetve ilyen ilyenkor inkÃ¡bb is ismÃ©t ison itt\n\njobban jÃ³ jÃ³l\n\nkell kellett keressÃ¼nk keresztÃ¼l ki kÃ­vÃ¼l kÃ¶zÃ¶tt kÃ¶zÃ¼l\n\nle legalÃ¡bb legyen lehet lehetett lenne lenni lesz lett\n\nma maga magÃ¡t majd meg mellett mely melyek mert mi miatt mikor milyen minden\nmindenki mindent mindig mint mintha mit mivel miÃ©rt mondta most mÃ¡r mÃ¡s mÃ¡sik\nmÃ©g mÃ­g\n\nnagy nagyobb nagyon ne nekem neki nem nincs nÃ©ha nÃ©hÃ¡ny nÃ©lkÃ¼l\n\no oda ok oket olyan ott\n\npedig persze pÃ©ldÃ¡ul\n\nrÃ¡\n\ns sajÃ¡t sem semmi sok sokat sokkal stb. szemben szerint szinte szÃ¡mÃ¡ra szÃ©t\n\ntalÃ¡n te tehÃ¡t teljes ti tovÃ¡bb tovÃ¡bbÃ¡ tÃ¶bb tÃºl ugyanis\n\nutolsÃ³ utÃ¡n utÃ¡na\n\nvagy vagyis vagyok valaki valami valamint valÃ³ van vannak vele vissza viszont\nvolna volt voltak voltam voltunk\n\nÃ¡ltal Ã¡ltalÃ¡ban Ã¡t\n\nÃ©n Ã©ppen Ã©s\n\nÃ­gy\n\nÃ¶n Ã¶ssze\n\nÃºgy Ãºj Ãºjabb Ãºjra\n\nÅ‘ Å‘ket\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/hu/punctuation.py----------------------------------------
A:spacy.lang.hu.punctuation._concat_icons->char_classes.CONCAT_ICONS.replace('Â°', '')
A:spacy.lang.hu.punctuation._quotes->char_classes.CONCAT_QUOTES.replace("'", '')


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/fr/__init__.py----------------------------------------
A:spacy.lang.fr.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.fr.__init__.lex_attr_getters[NORM]->add_lookups(Language.Defaults.lex_attr_getters[NORM], BASE_NORMS)
A:spacy.lang.fr.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS, TOKENIZER_EXCEPTIONS)
A:spacy.lang.fr.__init__.lookups->Lookups()
spacy.lang.fr.__init__.French(Language)
spacy.lang.fr.__init__.FrenchDefaults(Language.Defaults)
spacy.lang.fr.__init__.FrenchDefaults.create_lemmatizer(cls,nlp=None,lookups=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/fr/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/fr/lemmatizer.py----------------------------------------
A:spacy.lang.fr.lemmatizer.lookup_table->self.lookups.get_table('lemma_lookup', {})
A:spacy.lang.fr.lemmatizer.index_table->self.lookups.get_table('lemma_index', {})
A:spacy.lang.fr.lemmatizer.exc_table->self.lookups.get_table('lemma_exc', {})
A:spacy.lang.fr.lemmatizer.rules_table->self.lookups.get_table('lemma_rules', {})
A:spacy.lang.fr.lemmatizer.lemmas->self.lemmatize(string, index_table.get(univ_pos, {}), exc_table.get(univ_pos, {}), rules_table.get(univ_pos, []))
A:spacy.lang.fr.lemmatizer.string->string.lower().lower()
spacy.lang.fr.FrenchLemmatizer(self,string,univ_pos,morphology=None)
spacy.lang.fr.FrenchLemmatizer.adj(self,string,morphology=None)
spacy.lang.fr.FrenchLemmatizer.is_base_form(self,univ_pos,morphology=None)
spacy.lang.fr.FrenchLemmatizer.lemmatize(self,string,index,exceptions,rules)
spacy.lang.fr.FrenchLemmatizer.lookup(self,string,orth=None)
spacy.lang.fr.FrenchLemmatizer.noun(self,string,morphology=None)
spacy.lang.fr.FrenchLemmatizer.punct(self,string,morphology=None)
spacy.lang.fr.FrenchLemmatizer.verb(self,string,morphology=None)
spacy.lang.fr.lemmatizer.FrenchLemmatizer(self,string,univ_pos,morphology=None)
spacy.lang.fr.lemmatizer.FrenchLemmatizer.__call__(self,string,univ_pos,morphology=None)
spacy.lang.fr.lemmatizer.FrenchLemmatizer.adj(self,string,morphology=None)
spacy.lang.fr.lemmatizer.FrenchLemmatizer.is_base_form(self,univ_pos,morphology=None)
spacy.lang.fr.lemmatizer.FrenchLemmatizer.lemmatize(self,string,index,exceptions,rules)
spacy.lang.fr.lemmatizer.FrenchLemmatizer.lookup(self,string,orth=None)
spacy.lang.fr.lemmatizer.FrenchLemmatizer.noun(self,string,morphology=None)
spacy.lang.fr.lemmatizer.FrenchLemmatizer.punct(self,string,morphology=None)
spacy.lang.fr.lemmatizer.FrenchLemmatizer.verb(self,string,morphology=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/fr/lex_attrs.py----------------------------------------
A:spacy.lang.fr.lex_attrs._num_words->set('\nzero un deux trois quatre cinq six sept huit neuf dix\nonze douze treize quatorze quinze seize dix-sept dix-huit dix-neuf\nvingt trente quarante cinquante soixante soixante-dix septante quatre-vingt huitante quatre-vingt-dix nonante\ncent mille mil million milliard billion quadrillion quintillion\nsextillion septillion octillion nonillion decillion\n'.split())
A:spacy.lang.fr.lex_attrs._ordinal_words->set('\npremier deuxiÃ¨me second troisiÃ¨me quatriÃ¨me cinquiÃ¨me sixiÃ¨me septiÃ¨me huitiÃ¨me neuviÃ¨me dixiÃ¨me\nonziÃ¨me douziÃ¨me treiziÃ¨me quatorziÃ¨me quinziÃ¨me seiziÃ¨me dix-septiÃ¨me dix-huitiÃ¨me dix-neuviÃ¨me\nvingtiÃ¨me trentiÃ¨me quarantiÃ¨me cinquantiÃ¨me soixantiÃ¨me soixante-dixiÃ¨me septantiÃ¨me quatre-vingtiÃ¨me huitantiÃ¨me quatre-vingt-dixiÃ¨me nonantiÃ¨me\ncentiÃ¨me milliÃ¨me millionniÃ¨me milliardiÃ¨me billionniÃ¨me quadrillionniÃ¨me quintillionniÃ¨me\nsextillionniÃ¨me septillionniÃ¨me octillionniÃ¨me nonillionniÃ¨me decillionniÃ¨me\n'.split())
A:spacy.lang.fr.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.fr.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.fr.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/fr/_tokenizer_exceptions_list.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/fr/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.fr.tokenizer_exceptions.token->'{}-ce'.format(orth)
spacy.lang.fr.tokenizer_exceptions.lower_first_letter(text)
spacy.lang.fr.tokenizer_exceptions.upper_first_letter(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/fr/syntax_iterators.py----------------------------------------
A:spacy.lang.fr.syntax_iterators.conj->doc.vocab.strings.add('conj')
A:spacy.lang.fr.syntax_iterators.np_label->doc.vocab.strings.add('NP')
A:spacy.lang.fr.syntax_iterators.seen->set()
spacy.lang.fr.syntax_iterators.noun_chunks(obj)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/fr/stop_words.py----------------------------------------
A:spacy.lang.fr.stop_words.STOP_WORDS->set("\na Ã  Ã¢ abord absolument afin ah ai aie ailleurs ainsi ait allaient allo allons\nallÃ´ alors anterieur anterieure anterieures apres aprÃ¨s as assez attendu au\naucun aucune aujourd aujourd'hui aupres auquel aura auraient aurait auront\naussi autre autrefois autrement autres autrui aux auxquelles auxquels avaient\navais avait avant avec avoir avons ayant\n\nbah bas basee bat beau beaucoup bien bigre boum bravo brrr\n\nc' câ€™ Ã§a car ce ceci cela celle celle-ci celle-lÃ  celles celles-ci celles-lÃ  celui\ncelui-ci celui-lÃ  cent cependant certain certaine certaines certains certes ces\ncet cette ceux ceux-ci ceux-lÃ  chacun chacune chaque cher chers chez chiche\nchut chÃ¨re chÃ¨res ci cinq cinquantaine cinquante cinquantiÃ¨me cinquiÃ¨me clac\nclic combien comme comment comparable comparables compris concernant contre\ncouic crac\n\nd' dâ€™ da dans de debout dedans dehors deja delÃ  depuis dernier derniere derriere\nderriÃ¨re des desormais desquelles desquels dessous dessus deux deuxiÃ¨me\ndeuxiÃ¨mement devant devers devra different differentes differents diffÃ©rent\ndiffÃ©rente diffÃ©rentes diffÃ©rents dire directe directement dit dite dits divers\ndiverse diverses dix dix-huit dix-neuf dix-sept dixiÃ¨me doit doivent donc dont\ndouze douziÃ¨me dring du duquel durant dÃ¨s dÃ©sormais\n\neffet egale egalement egales eh elle elle-mÃªme elles elles-mÃªmes en encore\nenfin entre envers environ es Ã¨s est et etaient Ã©taient etais Ã©tais etait Ã©tait\netant Ã©tant etc Ã©tÃ© etre Ãªtre eu euh eux eux-mÃªmes exactement exceptÃ© extenso\nexterieur\n\nfais faisaient faisant fait faÃ§on feront fi flac floc font\n\ngens\n\nha hein hem hep hi ho holÃ  hop hormis hors hou houp hue hui huit huitiÃ¨me hum\nhurrah hÃ© hÃ©las i il ils importe\n\nj' jâ€™ je jusqu jusque juste\n\nl' lâ€™ la laisser laquelle las le lequel les lesquelles lesquels leur leurs longtemps\nlors lorsque lui lui-meme lui-mÃªme lÃ  lÃ¨s\n\nm' mâ€™ ma maint maintenant mais malgre malgrÃ© maximale me meme memes merci mes mien\nmienne miennes miens mille mince minimale moi moi-meme moi-mÃªme moindres moins\nmon moyennant mÃªme mÃªmes\n\nn' nâ€™ na naturel naturelle naturelles ne neanmoins necessaire necessairement neuf\nneuviÃ¨me ni nombreuses nombreux non nos notamment notre nous nous-mÃªmes nouveau\nnul nÃ©anmoins nÃ´tre nÃ´tres\n\no Ã´ oh ohÃ© ollÃ© olÃ© on ont onze onziÃ¨me ore ou ouf ouias oust ouste outre\nouvert ouverte ouverts oÃ¹\n\npaf pan par parce parfois parle parlent parler parmi parseme partant\nparticulier particuliÃ¨re particuliÃ¨rement pas passÃ© pendant pense permet\npersonne peu peut peuvent peux pff pfft pfut pif pire plein plouf plus\nplusieurs plutÃ´t possessif possessifs possible possibles pouah pour pourquoi\npourrais pourrait pouvait prealable precisement premier premiÃ¨re premiÃ¨rement\npres probable probante procedant proche prÃ¨s psitt pu puis puisque pur pure\n\nqu' quâ€™ quand quant quant-Ã -soi quanta quarante quatorze quatre quatre-vingt\nquatriÃ¨me quatriÃ¨mement que quel quelconque quelle quelles quelqu'un quelque\nquelques quels qui quiconque quinze quoi quoique\n\nrare rarement rares relative relativement remarquable rend rendre restant reste\nrestent restrictif retour revoici revoilÃ  rien\n\ns' sâ€™ sa sacrebleu sait sans sapristi sauf se sein seize selon semblable semblaient\nsemble semblent sent sept septiÃ¨me sera seraient serait seront ses seul seule\nseulement si sien sienne siennes siens sinon six sixiÃ¨me soi soi-mÃªme soit\nsoixante son sont sous souvent specifique specifiques speculatif stop\nstrictement subtiles suffisant suffisante suffit suis suit suivant suivante\nsuivantes suivants suivre superpose sur surtout\n\nt' tâ€™ ta tac tant tardive te tel telle tellement telles tels tenant tend tenir tente\ntes tic tien tienne tiennes tiens toc toi toi-mÃªme ton touchant toujours tous\ntout toute toutefois toutes treize trente tres trois troisiÃ¨me troisiÃ¨mement\ntrop trÃ¨s tsoin tsouin tu tÃ©\n\nun une unes uniformement unique uniques uns\n\nva vais vas vers via vif vifs vingt vivat vive vives vlan voici voilÃ  vont vos\nvotre vous vous-mÃªmes vu vÃ© vÃ´tre vÃ´tres\n\nzut\n".split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/fr/tag_map.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/fr/punctuation.py----------------------------------------
A:spacy.lang.fr.punctuation.ELISION->" ' â€™ ".strip().replace(' ', '').replace('\n', '')
A:spacy.lang.fr.punctuation.HYPHENS->'- â€“ â€” â€ â€‘'.strip().replace(' ', '').replace('\n', '')


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/pt/__init__.py----------------------------------------
A:spacy.lang.pt.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.pt.__init__.lex_attr_getters[NORM]->add_lookups(Language.Defaults.lex_attr_getters[NORM], BASE_NORMS, NORM_EXCEPTIONS)
A:spacy.lang.pt.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS, TOKENIZER_EXCEPTIONS)
spacy.lang.pt.__init__.Portuguese(Language)
spacy.lang.pt.__init__.PortugueseDefaults(Language.Defaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/pt/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/pt/lex_attrs.py----------------------------------------
A:spacy.lang.pt.lex_attrs.text->text.replace(',', '').replace('.', '').replace('Âº', '').replace('Âª', '').replace(',', '').replace('.', '').replace('Âº', '').replace('Âª', '')
A:spacy.lang.pt.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace('Âº', '').replace('Âª', '').replace(',', '').replace('.', '').replace('Âº', '').replace('Âª', '').split('/')
spacy.lang.pt.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/pt/tokenizer_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/pt/stop_words.py----------------------------------------
A:spacy.lang.pt.stop_words.STOP_WORDS->set('\nÃ  Ã s Ã¡rea acerca ademais adeus agora ainda algo algumas alguns ali alÃ©m ambas ambos antes\nao aos apenas apoia apoio apontar apÃ³s aquela aquelas aquele aqueles aqui aquilo\nas assim atravÃ©s atrÃ¡s atÃ© aÃ­\n\nbaixo bastante bem boa bom breve\n\ncada caminho catorze cedo cento certamente certeza cima cinco coisa com como\ncomprida comprido conhecida conhecido conselho contra contudo corrente cuja\ncujo custa cÃ¡\n\nda daquela daquele dar das de debaixo demais dentro depois des desde dessa desse\ndesta deste deve devem deverÃ¡ dez dezanove dezasseis dezassete dezoito diante\ndireita disso diz dizem dizer do dois dos doze duas dÃ¡ dÃ£o\n\nÃ© Ã©s ela elas ele eles em embora enquanto entre entÃ£o era essa essas esse esses esta\nestado estar estarÃ¡ estas estava este estes esteve estive estivemos estiveram\nestiveste estivestes estou estÃ¡ estÃ¡s estÃ£o eu eventual exemplo\n\nfalta farÃ¡ favor faz fazeis fazem fazemos fazer fazes fazia faÃ§o fez fim final\nfoi fomos for fora foram forma foste fostes fui\n\ngeral grande grandes grupo\n\ninclusive iniciar inicio ir irÃ¡ isso isto\n\njÃ¡\n\nlado lhe ligado local logo longe lugar lÃ¡\n\nmaior maioria maiorias mais mal mas me meio menor menos meses mesmo meu meus mil\nminha minhas momento muito muitos mÃ¡ximo mÃªs\n\nna nada naquela naquele nas nem nenhuma nessa nesse nesta neste no nos nossa\nnossas nosso nossos nova novas nove novo novos num numa nunca nuns nÃ£o nÃ­vel nÃ³s\nnÃºmero nÃºmeros\n\nobrigada obrigado oitava oitavo oito onde ontem onze ora os ou outra outras outros\n\npara parece parte partir pegar pela pelas pelo pelos perto pode podem poder poderÃ¡\npodia pois ponto pontos por porquanto porque porquÃª portanto porÃ©m posiÃ§Ã£o\npossivelmente posso possÃ­vel pouca pouco povo primeira primeiro prÃ³prio prÃ³xima\nprÃ³ximo puderam pÃ´de pÃµe pÃµem\n\nquais qual qualquer quando quanto quarta quarto quatro que quem quer querem quero\nquestÃ£o quieta quieto quinta quinto quinze quÃª\n\nrelaÃ§Ã£o\n\nsabe saber se segunda segundo sei seis sem sempre ser seria sete seu seus sexta\nsexto sim sistema sob sobre sois somente somos sou sua suas sÃ£o sÃ©tima sÃ©timo sÃ³\n\ntais tal talvez tambÃ©m tanta tanto tarde te tem temos tempo tendes tenho tens\ntentar tentaram tente tentei ter terceira terceiro teu teus teve tipo tive\ntivemos tiveram tiveste tivestes toda todas todo todos treze trÃªs tu tua tuas\ntudo tÃ£o tÃªm\n\num uma umas uns usa usar Ãºltimo\n\nvai vais valor veja vem vens ver vez vezes vinda vindo vinte vocÃª vocÃªs vos vossa\nvossas vosso vossos vÃ¡rios vÃ£o vÃªm vÃ³s\n\nzero\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/pt/norm_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/pt/tag_map.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/lang/pt/punctuation.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/data/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/matcher/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/matcher/_schemas.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tokens/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tokens/underscore.py----------------------------------------
A:spacy.tokens.underscore.extensions->list(self._extensions.keys())
A:spacy.tokens.underscore.method_partial->functools.partial(method, self._obj)
A:spacy.tokens.underscore.key->self._get_key(name)
A:spacy.tokens.underscore.new_default->copy.copy(default)
A:spacy.tokens.underscore.default->kwargs.get('default')
A:spacy.tokens.underscore.getter->kwargs.get('getter')
A:spacy.tokens.underscore.setter->kwargs.get('setter')
A:spacy.tokens.underscore.method->kwargs.get('method')
A:spacy.tokens.underscore.nr_defined->sum((t is True for t in valid_opts))
spacy.tokens.underscore.Underscore(self,extensions,obj,start=None,end=None)
spacy.tokens.underscore.Underscore.__dir__(self)
spacy.tokens.underscore.Underscore.__getattr__(self,name)
spacy.tokens.underscore.Underscore.__init__(self,extensions,obj,start=None,end=None)
spacy.tokens.underscore.Underscore.__setattr__(self,name,value)
spacy.tokens.underscore.Underscore._get_key(self,name)
spacy.tokens.underscore.Underscore.get(self,name)
spacy.tokens.underscore.Underscore.has(self,name)
spacy.tokens.underscore.Underscore.set(self,name,value)
spacy.tokens.underscore.get_ext_args(**kwargs)
spacy.tokens.underscore.is_writable_attr(ext)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/tokens/_serialize.py----------------------------------------
A:spacy.tokens._serialize.attrs->sorted(intify_attrs(attrs))
A:spacy.tokens._serialize.self.strings->set(msg['strings'])
A:spacy.tokens._serialize.array->array.reshape((array.shape[0], 1)).reshape((array.shape[0], 1))
A:spacy.tokens._serialize.spaces->spaces.reshape((spaces.shape[0], 1)).reshape((spaces.shape[0], 1))
A:spacy.tokens._serialize.orth_col->self.attrs.index(ORTH)
A:spacy.tokens._serialize.doc->doc.from_array(self.attrs, tokens).from_array(self.attrs, tokens)
A:spacy.tokens._serialize.msg->srsly.msgpack_loads(gzip.decompress(bytes_data))
A:spacy.tokens._serialize.lengths->numpy.fromstring(msg['lengths'], dtype='int32')
A:spacy.tokens._serialize.flat_spaces->flat_spaces.reshape((flat_spaces.size, 1)).reshape((flat_spaces.size, 1))
A:spacy.tokens._serialize.flat_tokens->flat_tokens.reshape(shape).reshape(shape)
A:spacy.tokens._serialize.self.tokens->NumpyOps().unflatten(flat_tokens, lengths)
A:spacy.tokens._serialize.self.spaces->NumpyOps().unflatten(flat_spaces, lengths)
A:spacy.tokens._serialize.self.user_data->list(msg['user_data'])
A:spacy.tokens._serialize.doc_bin->DocBin(store_user_data=True).from_bytes(byte_string)
spacy.tokens.DocBin(self,attrs=None,store_user_data=False)
spacy.tokens.DocBin.__len__(self)
spacy.tokens.DocBin.add(self,doc)
spacy.tokens.DocBin.from_bytes(self,bytes_data)
spacy.tokens.DocBin.get_docs(self,vocab)
spacy.tokens.DocBin.merge(self,other)
spacy.tokens.DocBin.to_bytes(self)
spacy.tokens._serialize.DocBin(self,attrs=None,store_user_data=False)
spacy.tokens._serialize.DocBin.__init__(self,attrs=None,store_user_data=False)
spacy.tokens._serialize.DocBin.__len__(self)
spacy.tokens._serialize.DocBin.add(self,doc)
spacy.tokens._serialize.DocBin.from_bytes(self,bytes_data)
spacy.tokens._serialize.DocBin.get_docs(self,vocab)
spacy.tokens._serialize.DocBin.merge(self,other)
spacy.tokens._serialize.DocBin.to_bytes(self)
spacy.tokens._serialize.merge_bins(bins)
spacy.tokens._serialize.pickle_bin(doc_bin)
spacy.tokens._serialize.unpickle_bin(byte_string)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/displacy/__init__.py----------------------------------------
A:spacy.displacy.__init__.renderer->renderer(options=options)
A:spacy.displacy.__init__._html['parsed']->renderer(options=options).render(parsed, page=page, minify=minify).strip()
A:spacy.displacy.__init__.html->RENDER_WRAPPER(html)
A:spacy.displacy.__init__.httpd->wsgiref.simple_server.make_server(host, port, app)
A:spacy.displacy.__init__.res->renderer(options=options).render(parsed, page=page, minify=minify).strip().encode(encoding='utf-8')
A:spacy.displacy.__init__.doc->Doc(orig_doc.vocab).from_bytes(orig_doc.to_bytes(exclude=['user_data']))
A:spacy.displacy.__init__.settings->get_doc_settings(doc)
spacy.displacy.__init__.app(environ,start_response)
spacy.displacy.__init__.get_doc_settings(doc)
spacy.displacy.__init__.parse_deps(orig_doc,options={})
spacy.displacy.__init__.parse_ents(doc,options={})
spacy.displacy.__init__.render(docs,style='dep',page=False,minify=False,jupyter=None,options={},manual=False)
spacy.displacy.__init__.serve(docs,style='dep',page=True,minify=False,options={},manual=False,port=5000,host='0.0.0.0')
spacy.displacy.__init__.set_render_wrapper(func)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/displacy/render.py----------------------------------------
A:spacy.displacy.render.self.compact->options.get('compact', False)
A:spacy.displacy.render.self.word_spacing->options.get('word_spacing', 45)
A:spacy.displacy.render.self.arrow_spacing->options.get('arrow_spacing', 12 if self.compact else 20)
A:spacy.displacy.render.self.arrow_width->options.get('arrow_width', 6 if self.compact else 10)
A:spacy.displacy.render.self.arrow_stroke->options.get('arrow_stroke', 2)
A:spacy.displacy.render.self.distance->options.get('distance', 150 if self.compact else 175)
A:spacy.displacy.render.self.offset_x->options.get('offset_x', 50)
A:spacy.displacy.render.self.color->options.get('color', '#000000')
A:spacy.displacy.render.self.bg->options.get('bg', '#ffffff')
A:spacy.displacy.render.self.font->options.get('font', 'Arial')
A:spacy.displacy.render.settings->p.get('settings', {})
A:spacy.displacy.render.self.direction->p.get('settings', {}).get('direction', DEFAULT_DIR)
A:spacy.displacy.render.self.lang->p.get('settings', {}).get('lang', DEFAULT_LANG)
A:spacy.displacy.render.render_id->'{}-{}'.format(id_prefix, i)
A:spacy.displacy.render.svg->self.render_svg(render_id, p['words'], p['arcs'])
A:spacy.displacy.render.content->''.join([TPL_FIGURE.format(content=svg) for svg in rendered])
A:spacy.displacy.render.markup->templates.TPL_ENTS.format(content=markup, dir=self.direction)
A:spacy.displacy.render.self.levels->self.get_levels(arcs)
A:spacy.displacy.render.self.highest_level->len(self.levels)
A:spacy.displacy.render.html_text->escape_html(text)
A:spacy.displacy.render.error_args->dict(start=start, end=end, label=label, dir=direction)
A:spacy.displacy.render.arrowhead->self.get_arrowhead(direction, x_start, y, x_end)
A:spacy.displacy.render.arc->self.get_arc(x_start, y, y_curve, x_end)
A:spacy.displacy.render.levels->set(map(lambda arc: arc['end'] - arc['start'], arcs))
A:spacy.displacy.render.user_colors->get_entry_points(ENTRY_POINTS.displacy_colors)
A:spacy.displacy.render.self.ents->options.get('ents', None)
A:spacy.displacy.render.template->options.get('template')
A:spacy.displacy.render.docs->''.join([TPL_FIGURE.format(content=doc) for doc in rendered])
A:spacy.displacy.render.additional_params->span.get('params', {})
A:spacy.displacy.render.entity->escape_html(text[start:end])
A:spacy.displacy.render.fragments->text[offset:start].split('\n')
A:spacy.displacy.render.color->self.colors.get(label.upper(), self.default_color)
spacy.displacy.DependencyRenderer(self,options={})
spacy.displacy.DependencyRenderer.get_arc(self,x_start,y,y_curve,x_end)
spacy.displacy.DependencyRenderer.get_arrowhead(self,direction,x,y,end)
spacy.displacy.DependencyRenderer.get_levels(self,arcs)
spacy.displacy.DependencyRenderer.render(self,parsed,page=False,minify=False)
spacy.displacy.DependencyRenderer.render_arrow(self,label,start,end,direction,i)
spacy.displacy.DependencyRenderer.render_svg(self,render_id,words,arcs)
spacy.displacy.DependencyRenderer.render_word(self,text,tag,i)
spacy.displacy.EntityRenderer(self,options={})
spacy.displacy.EntityRenderer.render(self,parsed,page=False,minify=False)
spacy.displacy.EntityRenderer.render_ents(self,text,spans,title)
spacy.displacy.render.DependencyRenderer(self,options={})
spacy.displacy.render.DependencyRenderer.__init__(self,options={})
spacy.displacy.render.DependencyRenderer.get_arc(self,x_start,y,y_curve,x_end)
spacy.displacy.render.DependencyRenderer.get_arrowhead(self,direction,x,y,end)
spacy.displacy.render.DependencyRenderer.get_levels(self,arcs)
spacy.displacy.render.DependencyRenderer.render(self,parsed,page=False,minify=False)
spacy.displacy.render.DependencyRenderer.render_arrow(self,label,start,end,direction,i)
spacy.displacy.render.DependencyRenderer.render_svg(self,render_id,words,arcs)
spacy.displacy.render.DependencyRenderer.render_word(self,text,tag,i)
spacy.displacy.render.EntityRenderer(self,options={})
spacy.displacy.render.EntityRenderer.__init__(self,options={})
spacy.displacy.render.EntityRenderer.render(self,parsed,page=False,minify=False)
spacy.displacy.render.EntityRenderer.render_ents(self,text,spans,title)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/displacy/templates.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/syntax/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/cli/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/cli/link.py----------------------------------------
A:spacy.cli.link.msg->Printer()
A:spacy.cli.link.model_path->util.get_package_path(origin)
A:spacy.cli.link.data_path->util.get_data_path()
spacy.cli.link(origin,link_name,force=False,model_path=None)
spacy.cli.link.link(origin,link_name,force=False,model_path=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/cli/download.py----------------------------------------
A:spacy.cli.download.msg->Printer()
A:spacy.cli.download.components->model.split('-')
A:spacy.cli.download.model_name->get_json(about.__shortcuts__, 'available shortcuts').get(model, model)
A:spacy.cli.download.dl->download_model(dl_tpl.format(m=model_name, v=version), pip_args)
A:spacy.cli.download.shortcuts->get_json(about.__shortcuts__, 'available shortcuts')
A:spacy.cli.download.compatibility->get_compatibility()
A:spacy.cli.download.version->get_version(model_name, compatibility)
A:spacy.cli.download.package_path->get_package_path(model_name)
A:spacy.cli.download.r->requests.get(url)
A:spacy.cli.download.comp_table->get_json(about.__compatibility__, 'compatibility table')
spacy.cli.download(model,direct=False,*pip_args)
spacy.cli.download.download(model,direct=False,*pip_args)
spacy.cli.download.download_model(filename,user_pip_args=None)
spacy.cli.download.get_compatibility()
spacy.cli.download.get_json(url,desc)
spacy.cli.download.get_version(model,comp)
spacy.cli.download.require_package(name)
spacy.cli.download_model(filename,user_pip_args=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/cli/debug_data.py----------------------------------------
A:spacy.cli.debug_data.msg->Printer(pretty=not no_format, ignore_warnings=ignore_warnings)
A:spacy.cli.debug_data.nlp->lang_cls()
A:spacy.cli.debug_data.lang_cls->get_lang_class(lang)
A:spacy.cli.debug_data.corpus->GoldCorpus(train_path, dev_path)
A:spacy.cli.debug_data.train_docs->list(corpus.train_docs(nlp))
A:spacy.cli.debug_data.train_docs_unpreprocessed->list(corpus.train_docs_without_preprocessing(nlp))
A:spacy.cli.debug_data.loading_train_error_message->'Training data cannot be loaded: {}'.format(str(e))
A:spacy.cli.debug_data.dev_docs->list(corpus.dev_docs(nlp))
A:spacy.cli.debug_data.loading_dev_error_message->'Development data cannot be loaded: {}'.format(str(e))
A:spacy.cli.debug_data.gold_train_data->_compile_gold(train_docs, pipeline)
A:spacy.cli.debug_data.gold_train_unpreprocessed_data->_compile_gold(train_docs_unpreprocessed, pipeline)
A:spacy.cli.debug_data.gold_dev_data->_compile_gold(dev_docs, pipeline)
A:spacy.cli.debug_data.overlap->len(train_texts.intersection(dev_texts))
A:spacy.cli.debug_data.text->'Low number of examples to train from a blank model ({})'.format(len(train_docs))
A:spacy.cli.debug_data.most_common_words->gold_train_data['words'].most_common(10)
A:spacy.cli.debug_data.labels->set((label for label in gold_train_data['ner'] if label not in ('O', '-')))
A:spacy.cli.debug_data.model_labels->_get_labels_from_model(nlp, 'textcat')
A:spacy.cli.debug_data.labels_with_counts->_format_labels(gold_train_unpreprocessed_data['deps'].most_common(), counts=True)
A:spacy.cli.debug_data.neg_docs->_get_examples_without_label(train_docs, label)
A:spacy.cli.debug_data.data->srsly.read_jsonl(file_path)
A:spacy.cli.debug_data.pipe->lang_cls().get_pipe(pipe_name)
spacy.cli.debug_data(lang,train_path,dev_path,base_model=None,pipeline='tagger,parser,ner',ignore_warnings=False,verbose=False,no_format=False)
spacy.cli.debug_data._compile_gold(train_docs,pipeline)
spacy.cli.debug_data._format_labels(labels,counts=False)
spacy.cli.debug_data._get_examples_without_label(data,label)
spacy.cli.debug_data._get_labels_from_model(nlp,pipe_name)
spacy.cli.debug_data._load_file(file_path,msg)
spacy.cli.debug_data.debug_data(lang,train_path,dev_path,base_model=None,pipeline='tagger,parser,ner',ignore_warnings=False,verbose=False,no_format=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/cli/info.py----------------------------------------
A:spacy.cli.info.msg->Printer()
A:spacy.cli.info.model_path->util.get_package_path(model)
A:spacy.cli.info.meta->srsly.read_json(meta_path)
A:spacy.cli.info.meta['link']->path2str(model_path)
A:spacy.cli.info.meta['source']->path2str(model_path)
A:spacy.cli.info.title->"Info about model '{}'".format(model)
A:spacy.cli.info.data_path->util.get_data_path()
spacy.cli.info(model=None,markdown=False,silent=False)
spacy.cli.info.info(model=None,markdown=False,silent=False)
spacy.cli.info.list_models()
spacy.cli.info.print_markdown(data,title=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/cli/validate.py----------------------------------------
A:spacy.cli.validate.msg->Printer()
A:spacy.cli.validate.r->requests.get(about.__compatibility__)
A:spacy.cli.validate.current_compat->compat.get(version)
A:spacy.cli.validate.all_models->set()
A:spacy.cli.validate.model_links->get_model_links(current_compat)
A:spacy.cli.validate.model_pkgs->get_model_pkgs(current_compat, all_models)
A:spacy.cli.validate.data_path->get_data_path()
A:spacy.cli.validate.meta->srsly.read_json(meta_path)
A:spacy.cli.validate.package->pkg_name.replace('-', '_')
A:spacy.cli.validate.comp->'--> {}'.format(compat.get(data['name'], ['n/a'])[0])
A:spacy.cli.validate.version->Printer().text(data['version'], color='red', no_print=True)
spacy.cli.validate()
spacy.cli.validate.get_model_links(compat)
spacy.cli.validate.get_model_pkgs(compat,all_models)
spacy.cli.validate.get_model_row(compat,name,data,msg,model_type='package')
spacy.cli.validate.is_compat(compat,name,version)
spacy.cli.validate.is_model_path(model_path)
spacy.cli.validate.reformat_version(version)
spacy.cli.validate.validate()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/cli/profile.py----------------------------------------
A:spacy.cli.profile.msg->Printer()
A:spacy.cli.profile.inputs->_read_inputs(inputs, msg)
A:spacy.cli.profile.(imdb_train, _)->thinc.extra.datasets.imdb()
A:spacy.cli.profile.(inputs, _)->zip(*imdb_train)
A:spacy.cli.profile.nlp->load_model(model)
A:spacy.cli.profile.texts->list(itertools.islice(inputs, n_texts))
A:spacy.cli.profile.s->pstats.Stats('Profile.prof')
A:spacy.cli.profile.input_path->Path(loc)
A:spacy.cli.profile.file_->Path(loc).open()
A:spacy.cli.profile.data->srsly.json_loads(line)
spacy.cli.profile(model,inputs=None,n_texts=10000)
spacy.cli.profile._read_inputs(loc,msg)
spacy.cli.profile.parse_texts(nlp,texts)
spacy.cli.profile.profile(model,inputs=None,n_texts=10000)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/cli/pretrain.py----------------------------------------
A:spacy.cli.pretrain.config->dict(locals())
A:spacy.cli.pretrain.config[key]->str(config[key])
A:spacy.cli.pretrain.msg->Printer()
A:spacy.cli.pretrain.has_gpu->prefer_gpu()
A:spacy.cli.pretrain.output_dir->Path(output_dir)
A:spacy.cli.pretrain.texts_loc->Path(texts_loc)
A:spacy.cli.pretrain.texts->srsly.read_jsonl('-')
A:spacy.cli.pretrain.nlp->util.load_model(vectors_model)
A:spacy.cli.pretrain.model->masked_language_model(nlp.vocab, model)
A:spacy.cli.pretrain.components->_load_pretrained_tok2vec(nlp, init_tok2vec)
A:spacy.cli.pretrain.model_name->re.search('model\\d+\\.bin', str(init_tok2vec))
A:spacy.cli.pretrain.optimizer->create_default_optimizer(model.ops)
A:spacy.cli.pretrain.tracker->ProgressTracker(frequency=10000)
A:spacy.cli.pretrain.(docs, count)->make_docs(nlp, [text for (text, _) in batch], max_length=max_length, min_length=min_length)
A:spacy.cli.pretrain.loss->(d_target ** 2).sum()
A:spacy.cli.pretrain.progress->ProgressTracker(frequency=10000).update(epoch, loss, docs)
A:spacy.cli.pretrain.(predictions, backprop)->masked_language_model(nlp.vocab, model).begin_update(docs, drop=drop)
A:spacy.cli.pretrain.(loss, gradients)->get_vectors_loss(model.ops, docs, predictions, objective)
A:spacy.cli.pretrain.doc->doc.from_array([HEAD], heads).from_array([HEAD], heads)
A:spacy.cli.pretrain.heads->heads.reshape((len(doc), 1)).reshape((len(doc), 1))
A:spacy.cli.pretrain.ids->ops.flatten([doc.to_array(ID).ravel() for doc in docs])
A:spacy.cli.pretrain.(loss, d_target)->get_cossim_loss(prediction, target)
A:spacy.cli.pretrain.output_layer->chain(LN(Maxout(300, pieces=3)), Affine(output_size, drop_factor=0.0))
A:spacy.cli.pretrain.tok2vec->chain(tok2vec, flatten)
A:spacy.cli.pretrain.self.words_per_epoch->Counter()
A:spacy.cli.pretrain.self.last_time->time.time()
A:spacy.cli.pretrain.words_in_batch->sum((len(doc) for doc in docs))
A:spacy.cli.pretrain.self.prev_loss->float(self.loss)
A:spacy.cli.pretrain.n_digits->len(str(int(figure)))
A:spacy.cli.pretrain.n_decimal->min(n_decimal, max_decimal)
spacy.cli.pretrain(texts_loc,vectors_model,output_dir,width=96,depth=4,embed_rows=2000,loss_func='cosine',use_vectors=False,dropout=0.2,n_iter=1000,batch_size=3000,max_length=500,min_length=5,seed=0,n_save_every=None,init_tok2vec=None,epoch_start=None)
spacy.cli.pretrain.ProgressTracker(self,frequency=1000000)
spacy.cli.pretrain.ProgressTracker.__init__(self,frequency=1000000)
spacy.cli.pretrain.ProgressTracker.update(self,epoch,loss,docs)
spacy.cli.pretrain._smart_round(figure,width=10,max_decimal=4)
spacy.cli.pretrain.create_pretraining_model(nlp,tok2vec)
spacy.cli.pretrain.get_vectors_loss(ops,docs,prediction,objective='L2')
spacy.cli.pretrain.make_docs(nlp,batch,min_length,max_length)
spacy.cli.pretrain.make_update(model,docs,optimizer,drop=0.0,objective='L2')
spacy.cli.pretrain.pretrain(texts_loc,vectors_model,output_dir,width=96,depth=4,embed_rows=2000,loss_func='cosine',use_vectors=False,dropout=0.2,n_iter=1000,batch_size=3000,max_length=500,min_length=5,seed=0,n_save_every=None,init_tok2vec=None,epoch_start=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/cli/init_model.py----------------------------------------
A:spacy.cli.init_model.msg->Printer()
A:spacy.cli.init_model.jsonl_loc->ensure_path(jsonl_loc)
A:spacy.cli.init_model.lex_attrs->read_attrs_from_deprecated(freqs_loc, clusters_loc)
A:spacy.cli.init_model.clusters_loc->ensure_path(clusters_loc)
A:spacy.cli.init_model.freqs_loc->ensure_path(freqs_loc)
A:spacy.cli.init_model.nlp->lang_class()
A:spacy.cli.init_model.vec_added->len(nlp.vocab.vectors)
A:spacy.cli.init_model.lex_added->len(nlp.vocab)
A:spacy.cli.init_model.loc->ensure_path(loc)
A:spacy.cli.init_model.zip_file->zipfile.ZipFile(str(loc))
A:spacy.cli.init_model.names->zipfile.ZipFile(str(loc)).namelist()
A:spacy.cli.init_model.file_->zipfile.ZipFile(str(loc)).open(names[0])
A:spacy.cli.init_model.(probs, _)->read_freqs(freqs_loc)
A:spacy.cli.init_model.clusters->read_clusters(clusters_loc)
A:spacy.cli.init_model.sorted_probs->sorted(probs.items(), key=lambda item: item[1], reverse=True)
A:spacy.cli.init_model.attrs['cluster']->int(clusters[word][::-1], 2)
A:spacy.cli.init_model.lang_class->get_lang_class(lang)
A:spacy.cli.init_model.vectors_loc->ensure_path(vectors_loc)
A:spacy.cli.init_model.nlp.vocab.vectors->Vectors(data=vectors_data, keys=vector_keys)
A:spacy.cli.init_model.(vectors_data, vector_keys)->read_vectors(vectors_loc)
A:spacy.cli.init_model.f->open_file(vectors_loc)
A:spacy.cli.init_model.shape->tuple((int(size) for size in next(f).split()))
A:spacy.cli.init_model.vectors_data->numpy.zeros(shape=shape, dtype='f')
A:spacy.cli.init_model.line->line.rstrip().rstrip()
A:spacy.cli.init_model.pieces->line.rstrip().rstrip().rsplit(' ', vectors_data.shape[1])
A:spacy.cli.init_model.word->ftfy.fix_text(word)
A:spacy.cli.init_model.vectors_data[i]->numpy.asarray(pieces, dtype='f')
A:spacy.cli.init_model.counts->PreshCounter()
A:spacy.cli.init_model.(freq, doc_freq, key)->line.rstrip().rstrip().rstrip().split('\t', 2)
A:spacy.cli.init_model.freq->int(freq)
A:spacy.cli.init_model.log_total->math.log(total)
A:spacy.cli.init_model.doc_freq->int(doc_freq)
A:spacy.cli.init_model.smooth_count->PreshCounter().smoother(int(freq))
A:spacy.cli.init_model.(cluster, word, freq)->line.rstrip().rstrip().split()
spacy.cli.init_model(lang,output_dir,freqs_loc=None,clusters_loc=None,jsonl_loc=None,vectors_loc=None,prune_vectors=-1,vectors_name=None,model_name=None)
spacy.cli.init_model.add_vectors(nlp,vectors_loc,prune_vectors,name=None)
spacy.cli.init_model.create_model(lang,lex_attrs,name=None)
spacy.cli.init_model.init_model(lang,output_dir,freqs_loc=None,clusters_loc=None,jsonl_loc=None,vectors_loc=None,prune_vectors=-1,vectors_name=None,model_name=None)
spacy.cli.init_model.open_file(loc)
spacy.cli.init_model.read_attrs_from_deprecated(freqs_loc,clusters_loc)
spacy.cli.init_model.read_clusters(clusters_loc)
spacy.cli.init_model.read_freqs(freqs_loc,max_length=100,min_doc_freq=5,min_freq=50)
spacy.cli.init_model.read_vectors(vectors_loc)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/cli/evaluate.py----------------------------------------
A:spacy.cli.evaluate.msg->Printer()
A:spacy.cli.evaluate.data_path->util.ensure_path(data_path)
A:spacy.cli.evaluate.displacy_path->util.ensure_path(displacy_path)
A:spacy.cli.evaluate.corpus->GoldCorpus(data_path, data_path)
A:spacy.cli.evaluate.nlp->util.load_model(model)
A:spacy.cli.evaluate.dev_docs->list(corpus.dev_docs(nlp, gold_preproc=gold_preproc))
A:spacy.cli.evaluate.begin->timer()
A:spacy.cli.evaluate.scorer->util.load_model(model).evaluate(dev_docs, verbose=False)
A:spacy.cli.evaluate.end->timer()
A:spacy.cli.evaluate.nwords->sum((len(doc_gold[0]) for doc_gold in dev_docs))
A:spacy.cli.evaluate.(docs, golds)->zip(*dev_docs)
A:spacy.cli.evaluate.html->displacy.render(docs[:limit], style='dep', page=True, options={'compact': True})
spacy.cli.evaluate(model,data_path,gpu_id=-1,gold_preproc=False,displacy_path=None,displacy_limit=25,return_scores=False)
spacy.cli.evaluate.evaluate(model,data_path,gpu_id=-1,gold_preproc=False,displacy_path=None,displacy_limit=25,return_scores=False)
spacy.cli.evaluate.render_parses(docs,output_path,model_name='',limit=250,deps=True,ents=True)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/cli/package.py----------------------------------------
A:spacy.cli.package.msg->Printer()
A:spacy.cli.package.input_path->util.ensure_path(input_dir)
A:spacy.cli.package.output_path->util.ensure_path(output_dir)
A:spacy.cli.package.meta_path->util.ensure_path(meta_path)
A:spacy.cli.package.meta->generate_meta(input_dir, meta, msg)
A:spacy.cli.package.nlp->util.load_model_from_path(Path(model_path))
A:spacy.cli.package.response->get_raw_input(desc, default)
A:spacy.cli.package.TEMPLATE_SETUP->"\n#!/usr/bin/env python\n# coding: utf8\nfrom __future__ import unicode_literals\n\nimport io\nimport json\nfrom os import path, walk\nfrom shutil import copy\nfrom setuptools import setup\n\n\ndef load_meta(fp):\n    with io.open(fp, encoding='utf8') as f:\n        return json.load(f)\n\n\ndef list_files(data_dir):\n    output = []\n    for root, _, filenames in walk(data_dir):\n        for filename in filenames:\n            if not filename.startswith('.'):\n                output.append(path.join(root, filename))\n    output = [path.relpath(p, path.dirname(data_dir)) for p in output]\n    output.append('meta.json')\n    return output\n\n\ndef list_requirements(meta):\n    parent_package = meta.get('parent_package', 'spacy')\n    requirements = [parent_package + meta['spacy_version']]\n    if 'setup_requires' in meta:\n        requirements += meta['setup_requires']\n    if 'requirements' in meta:\n        requirements += meta['requirements']\n    return requirements\n\n\ndef setup_package():\n    root = path.abspath(path.dirname(__file__))\n    meta_path = path.join(root, 'meta.json')\n    meta = load_meta(meta_path)\n    model_name = str(meta['lang'] + '_' + meta['name'])\n    model_dir = path.join(model_name, model_name + '-' + meta['version'])\n\n    copy(meta_path, path.join(model_name))\n    copy(meta_path, model_dir)\n\n    setup(\n        name=model_name,\n        description=meta['description'],\n        author=meta['author'],\n        author_email=meta['email'],\n        url=meta['url'],\n        version=meta['version'],\n        license=meta['license'],\n        packages=[model_name],\n        package_data={model_name: list_files(model_dir)},\n        install_requires=list_requirements(meta),\n        zip_safe=False,\n    )\n\n\nif __name__ == '__main__':\n    setup_package()\n".strip()
A:spacy.cli.package.TEMPLATE_MANIFEST->'\ninclude meta.json\n'.strip()
A:spacy.cli.package.TEMPLATE_INIT->"\n# coding: utf8\nfrom __future__ import unicode_literals\n\nfrom pathlib import Path\nfrom spacy.util import load_model_from_init_py, get_model_meta\n\n\n__version__ = get_model_meta(Path(__file__).parent)['version']\n\n\ndef load(**overrides):\n    return load_model_from_init_py(__file__, **overrides)\n".strip()
spacy.cli.package(input_dir,output_dir,meta_path=None,create_meta=False,force=False)
spacy.cli.package.create_file(file_path,contents)
spacy.cli.package.generate_meta(model_path,existing_meta,msg)
spacy.cli.package.package(input_dir,output_dir,meta_path=None,create_meta=False,force=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/cli/train.py----------------------------------------
A:spacy.cli.train.msg->Printer()
A:spacy.cli.train.train_path->util.ensure_path(train_path)
A:spacy.cli.train.dev_path->util.ensure_path(dev_path)
A:spacy.cli.train.meta_path->util.ensure_path(meta_path)
A:spacy.cli.train.output_path->util.ensure_path(output_path)
A:spacy.cli.train.raw_text->list(srsly.read_jsonl(raw_text))
A:spacy.cli.train.dropout_rates->util.decaying(util.env_opt('dropout_from', 0.2), util.env_opt('dropout_to', 0.2), util.env_opt('dropout_decay', 0.0))
A:spacy.cli.train.batch_sizes->util.compounding(util.env_opt('batch_from', 100.0), util.env_opt('batch_to', 1000.0), util.env_opt('batch_compound', 1.001))
A:spacy.cli.train.nlp->lang_cls()
A:spacy.cli.train.lang_cls->util.get_lang_class(lang)
A:spacy.cli.train.pipe->lang_cls().get_pipe(pipe_name)
A:spacy.cli.train.corpus->GoldCorpus(train_path, dev_path, limit=n_examples)
A:spacy.cli.train.n_train_words->GoldCorpus(train_path, dev_path, limit=n_examples).count_train()
A:spacy.cli.train.optimizer->lang_cls().begin_training(lambda : corpus.train_tuples, device=use_gpu)
A:spacy.cli.train.components->_load_pretrained_tok2vec(nlp, init_tok2vec)
A:spacy.cli.train.train_docs->GoldCorpus(train_path, dev_path, limit=n_examples).train_docs(nlp, noise_level=noise_level, orth_variant_level=orth_variant_level, gold_preproc=gold_preproc, max_length=0)
A:spacy.cli.train.train_labels->set()
A:spacy.cli.train.(row_head, output_stats)->_configure_training_output(pipeline, use_gpu, has_beam_widths)
A:spacy.cli.train.raw_batches->util.minibatch((nlp.make_doc(rt['text']) for rt in raw_text), size=8)
A:spacy.cli.train.(docs, golds)->zip(*batch)
A:spacy.cli.train.raw_batch->list(next(raw_batches))
A:spacy.cli.train.nlp_loaded->util.load_model_from_path(epoch_model_path)
A:spacy.cli.train.dev_docs->list(corpus.dev_docs(nlp_loaded, gold_preproc=gold_preproc))
A:spacy.cli.train.nwords->sum((len(doc_gold[0]) for doc_gold in dev_docs))
A:spacy.cli.train.start_time->timer()
A:spacy.cli.train.scorer->util.load_model_from_path(epoch_model_path).evaluate(dev_docs, verbose=verbose)
A:spacy.cli.train.end_time->timer()
A:spacy.cli.train.progress->_get_progress(i, losses, scorer.scores, output_stats, beam_width=beam_width if has_beam_widths else None, cpu_wps=cpu_wps, gpu_wps=gpu_wps)
A:spacy.cli.train.textcats_per_cat->util.load_model_from_path(epoch_model_path).evaluate(dev_docs, verbose=verbose).scores.get('textcats_per_cat', {})
A:spacy.cli.train.current_score->_score_for_model(meta)
A:spacy.cli.train.best_model_path->_collate_best_model(meta, output_path, nlp.pipe_names)
A:spacy.cli.train.mean_acc->list()
A:spacy.cli.train.pbar->tqdm.tqdm(total=total, leave=False)
A:spacy.cli.train.values[lex.vocab.strings[attr]]->func(lex.orth_)
A:spacy.cli.train.weights_data->file_.read()
A:spacy.cli.train.bests[component]->_find_best(output_path, component)
A:spacy.cli.train.accs->srsly.read_json(epoch_model / 'accuracy.json')
A:spacy.cli.train.scores['dep_loss']->losses.get('parser', 0.0)
A:spacy.cli.train.scores['ner_loss']->losses.get('ner', 0.0)
A:spacy.cli.train.scores['tag_loss']->losses.get('tagger', 0.0)
A:spacy.cli.train.scores['textcat_loss']->losses.get('textcat', 0.0)
spacy.cli.train(lang,output_path,train_path,dev_path,raw_text=None,base_model=None,pipeline='tagger,parser,ner',vectors=None,n_iter=30,n_early_stopping=None,n_examples=0,use_gpu=-1,version='0.0.0',meta_path=None,init_tok2vec=None,parser_multitasks='',entity_multitasks='',noise_level=0.0,orth_variant_level=0.0,eval_beam_widths='',gold_preproc=False,learn_tokens=False,textcat_multilabel=False,textcat_arch='bow',textcat_positive_label=None,verbose=False,debug=False)
spacy.cli.train._collate_best_model(meta,output_path,components)
spacy.cli.train._configure_training_output(pipeline,use_gpu,has_beam_widths)
spacy.cli.train._create_progress_bar(total)
spacy.cli.train._find_best(experiment_dir,component)
spacy.cli.train._get_metrics(component)
spacy.cli.train._get_progress(itn,losses,dev_scores,output_stats,beam_width=None,cpu_wps=0.0,gpu_wps=0.0)
spacy.cli.train._load_pretrained_tok2vec(nlp,loc)
spacy.cli.train._load_vectors(nlp,vectors)
spacy.cli.train._score_for_model(meta)
spacy.cli.train.train(lang,output_path,train_path,dev_path,raw_text=None,base_model=None,pipeline='tagger,parser,ner',vectors=None,n_iter=30,n_early_stopping=None,n_examples=0,use_gpu=-1,version='0.0.0',meta_path=None,init_tok2vec=None,parser_multitasks='',entity_multitasks='',noise_level=0.0,orth_variant_level=0.0,eval_beam_widths='',gold_preproc=False,learn_tokens=False,textcat_multilabel=False,textcat_arch='bow',textcat_positive_label=None,verbose=False,debug=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/cli/_schemas.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/cli/convert.py----------------------------------------
A:spacy.cli.convert.msg->Printer()
A:spacy.cli.convert.input_path->Path(input_file)
A:spacy.cli.convert.input_data->Path(input_file).open('r', encoding='utf-8').read()
A:spacy.cli.convert.converter_autodetect->autodetect_ner_format(input_data)
A:spacy.cli.convert.data->func(input_data, n_sents=n_sents, seg_sents=seg_sents, use_morphology=morphology, lang=lang, model=model)
A:spacy.cli.convert.suffix->'.{}'.format(file_type)
A:spacy.cli.convert.iob_re->re.compile('\\S+\\|(O|[IB]-\\S+)')
A:spacy.cli.convert.ner_re->re.compile('\\S+\\s+(O|[IB]-\\S+)$')
A:spacy.cli.convert.line->line.strip().strip()
spacy.cli.convert(input_file,output_dir='-',file_type='json',n_sents=1,seg_sents=False,model=None,morphology=False,converter='auto',lang=None)
spacy.cli.convert.autodetect_ner_format(input_data)
spacy.cli.convert.convert(input_file,output_dir='-',file_type='json',n_sents=1,seg_sents=False,model=None,morphology=False,converter='auto',lang=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/cli/converters/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/cli/converters/conll_ner2json.py----------------------------------------
A:spacy.cli.converters.conll_ner2json.msg->Printer()
A:spacy.cli.converters.conll_ner2json.input_data->segment_sents_and_docs(input_data, n_sents, doc_delimiter, model=model, msg=msg)
A:spacy.cli.converters.conll_ner2json.doc->doc.strip().strip()
A:spacy.cli.converters.conll_ner2json.sent->sent.strip().strip()
A:spacy.cli.converters.conll_ner2json.cols->list(zip(*[line.split() for line in lines]))
A:spacy.cli.converters.conll_ner2json.biluo_ents->iob_to_biluo(iob_ents)
A:spacy.cli.converters.conll_ner2json.nlp->MultiLanguage()
A:spacy.cli.converters.conll_ner2json.sentencizer->MultiLanguage().create_pipe('sentencizer')
A:spacy.cli.converters.conll_ner2json.lines->doc.strip().strip().strip().split('\n')
A:spacy.cli.converters.conll_ner2json.nlpdoc->Doc(nlp.vocab, words=words)
A:spacy.cli.converters.conll_ner2json.sents->segment_sents_and_docs(input_data, n_sents, doc_delimiter, model=model, msg=msg).split(sent_delimiter)
spacy.cli.converters.conll_ner2json(input_data,n_sents=10,seg_sents=False,model=None,**kwargs)
spacy.cli.converters.conll_ner2json.conll_ner2json(input_data,n_sents=10,seg_sents=False,model=None,**kwargs)
spacy.cli.converters.conll_ner2json.n_sents_info(msg,n_sents)
spacy.cli.converters.conll_ner2json.segment_docs(input_data,n_sents,doc_delimiter)
spacy.cli.converters.conll_ner2json.segment_sents_and_docs(doc,n_sents,doc_delimiter,model=None,msg=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/cli/converters/conllu2json.py----------------------------------------
A:spacy.cli.converters.conllu2json.conll_tuples->read_conllx(input_data, use_morphology=use_morphology)
A:spacy.cli.converters.conllu2json.has_ner_tags->is_ner(sentence[5][0])
A:spacy.cli.converters.conllu2json.doc->create_doc(sentences, i)
A:spacy.cli.converters.conllu2json.tag_match->re.match('([A-Z_]+)-([A-Z_]+)', tag)
A:spacy.cli.converters.conllu2json.lines->sent.strip().split('\n')
A:spacy.cli.converters.conllu2json.parts->line.split('\t')
A:spacy.cli.converters.conllu2json.prefix->re.match('([A-Z_]+)-([A-Z_]+)', tag).group(1)
A:spacy.cli.converters.conllu2json.suffix->re.match('([A-Z_]+)-([A-Z_]+)', tag).group(2)
A:spacy.cli.converters.conllu2json.iob->simplify_tags(iob)
A:spacy.cli.converters.conllu2json.biluo->iob_to_biluo(iob)
spacy.cli.converters.conllu2json(input_data,n_sents=10,use_morphology=False,lang=None,**_)
spacy.cli.converters.conllu2json.conllu2json(input_data,n_sents=10,use_morphology=False,lang=None,**_)
spacy.cli.converters.conllu2json.create_doc(sentences,id)
spacy.cli.converters.conllu2json.generate_sentence(sent,has_ner_tags)
spacy.cli.converters.conllu2json.is_ner(tag)
spacy.cli.converters.conllu2json.read_conllx(input_data,use_morphology=False,n=0)
spacy.cli.converters.conllu2json.simplify_tags(iob)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/cli/converters/iob2json.py----------------------------------------
A:spacy.cli.converters.iob2json.msg->Printer()
A:spacy.cli.converters.iob2json.docs->merge_sentences(docs, n_sents)
A:spacy.cli.converters.iob2json.(words, pos, iob)->zip(*tokens)
A:spacy.cli.converters.iob2json.(words, iob)->zip(*tokens)
A:spacy.cli.converters.iob2json.biluo->iob_to_biluo(iob)
A:spacy.cli.converters.iob2json.group->list(group)
A:spacy.cli.converters.iob2json.first->list(group).pop(0)
spacy.cli.converters.iob2json(input_data,n_sents=10,*args,**kwargs)
spacy.cli.converters.iob2json.iob2json(input_data,n_sents=10,*args,**kwargs)
spacy.cli.converters.iob2json.merge_sentences(docs,n_sents)
spacy.cli.converters.iob2json.read_iob(raw_sents)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy2.2.0/lib/python3.6/site-packages/spacy/cli/converters/jsonl2json.py----------------------------------------
A:spacy.cli.converters.jsonl2json.nlp->get_lang_class(lang)()
A:spacy.cli.converters.jsonl2json.sentencizer->get_lang_class(lang)().create_pipe('sentencizer')
A:spacy.cli.converters.jsonl2json.doc->get_lang_class(lang)().make_doc(raw_text)
A:spacy.cli.converters.jsonl2json.doc.ents->_cleanup_spans(spans)
A:spacy.cli.converters.jsonl2json.seen->set()
spacy.cli.converters.jsonl2json._cleanup_spans(spans)
spacy.cli.converters.jsonl2json.ner_jsonl2json(input_data,lang=None,n_sents=10,use_morphology=False)
spacy.cli.converters.ner_jsonl2json(input_data,lang=None,n_sents=10,use_morphology=False)

