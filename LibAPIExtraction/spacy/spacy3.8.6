
----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/__main__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/__init__.py----------------------------------------
A:spacy.__init__.LangClass->util.get_lang_class(name)
A:spacy.__init__.config->util.dot_to_dict(config)
spacy.__init__.blank(name:str,*,vocab:Union[Vocab,bool]=True,config:Union[Dict[str,Any],Config]=util.SimpleFrozenDict(),meta:Dict[str,Any]=util.SimpleFrozenDict())->Language
spacy.__init__.load(name:Union[str,Path],*,vocab:Union[Vocab,bool]=True,disable:Union[str,Iterable[str]]=util._DEFAULT_EMPTY_PIPES,enable:Union[str,Iterable[str]]=util._DEFAULT_EMPTY_PIPES,exclude:Union[str,Iterable[str]]=util._DEFAULT_EMPTY_PIPES,config:Union[Dict[str,Any],Config]=util.SimpleFrozenDict())->Language


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/vocab.pyi----------------------------------------
spacy.Vocab(self,lex_attr_getters:Optional[Dict[str,Callable[[str],Any]]]=...,strings:Optional[Union[List[str],StringStore]]=...,lookups:Optional[Lookups]=...,oov_prob:float=...,vectors_name:Optional[str]=...,writing_system:Dict[str,Any]=...,get_noun_chunks:Optional[Callable[[Union[Doc,Span]],Iterator[Span]]]=...)
spacy.Vocab.__contains__(self,key:str)->bool
spacy.Vocab.__getitem__(self,id_or_string:Union[str,int])->Lexeme
spacy.Vocab.__iter__(self)->Iterator[Lexeme]
spacy.Vocab.__len__(self)->int
spacy.Vocab.add_flag(self,flag_getter:Callable[[str],bool],flag_id:int=...)->int
spacy.Vocab.deduplicate_vectors(self)->None
spacy.Vocab.from_bytes(self,bytes_data:bytes,*,exclude:Iterable[str]=...)->Vocab
spacy.Vocab.from_disk(self,path:Union[str,Path],*,exclude:Iterable[str]=...)->Vocab
spacy.Vocab.get_vector(self,orth:Union[int,str],minn:Optional[int]=...,maxn:Optional[int]=...)->FloatsXd
spacy.Vocab.has_vector(self,orth:Union[int,str])->bool
spacy.Vocab.lang(self)->str
spacy.Vocab.memory_zone(self,mem:Optional[Pool]=None)->Iterator[Pool]
spacy.Vocab.prune_vectors(self,nr_row:int,batch_size:int=...)->Dict[str, float]
spacy.Vocab.reset_vectors(self,*,width:Optional[int]=...,shape:Optional[int]=...)->None
spacy.Vocab.set_vector(self,orth:Union[int,str],vector:Floats1d)->None
spacy.Vocab.to_bytes(self,*,exclude:Iterable[str]=...)->bytes
spacy.Vocab.to_disk(self,path:Union[str,Path],*,exclude:Iterable[str]=...)->None
spacy.Vocab.vectors_length(self)->int
spacy.vocab.Vocab(self,lex_attr_getters:Optional[Dict[str,Callable[[str],Any]]]=...,strings:Optional[Union[List[str],StringStore]]=...,lookups:Optional[Lookups]=...,oov_prob:float=...,vectors_name:Optional[str]=...,writing_system:Dict[str,Any]=...,get_noun_chunks:Optional[Callable[[Union[Doc,Span]],Iterator[Span]]]=...)
spacy.vocab.Vocab.__contains__(self,key:str)->bool
spacy.vocab.Vocab.__getitem__(self,id_or_string:Union[str,int])->Lexeme
spacy.vocab.Vocab.__init__(self,lex_attr_getters:Optional[Dict[str,Callable[[str],Any]]]=...,strings:Optional[Union[List[str],StringStore]]=...,lookups:Optional[Lookups]=...,oov_prob:float=...,vectors_name:Optional[str]=...,writing_system:Dict[str,Any]=...,get_noun_chunks:Optional[Callable[[Union[Doc,Span]],Iterator[Span]]]=...)
spacy.vocab.Vocab.__iter__(self)->Iterator[Lexeme]
spacy.vocab.Vocab.__len__(self)->int
spacy.vocab.Vocab.add_flag(self,flag_getter:Callable[[str],bool],flag_id:int=...)->int
spacy.vocab.Vocab.deduplicate_vectors(self)->None
spacy.vocab.Vocab.from_bytes(self,bytes_data:bytes,*,exclude:Iterable[str]=...)->Vocab
spacy.vocab.Vocab.from_disk(self,path:Union[str,Path],*,exclude:Iterable[str]=...)->Vocab
spacy.vocab.Vocab.get_vector(self,orth:Union[int,str],minn:Optional[int]=...,maxn:Optional[int]=...)->FloatsXd
spacy.vocab.Vocab.has_vector(self,orth:Union[int,str])->bool
spacy.vocab.Vocab.lang(self)->str
spacy.vocab.Vocab.memory_zone(self,mem:Optional[Pool]=None)->Iterator[Pool]
spacy.vocab.Vocab.prune_vectors(self,nr_row:int,batch_size:int=...)->Dict[str, float]
spacy.vocab.Vocab.reset_vectors(self,*,width:Optional[int]=...,shape:Optional[int]=...)->None
spacy.vocab.Vocab.set_vector(self,orth:Union[int,str],vector:Floats1d)->None
spacy.vocab.Vocab.to_bytes(self,*,exclude:Iterable[str]=...)->bytes
spacy.vocab.Vocab.to_disk(self,path:Union[str,Path],*,exclude:Iterable[str]=...)->None
spacy.vocab.Vocab.vectors_length(self)->int
spacy.vocab.create_vocab(lang:Optional[str],defaults:Any,vectors_name:Optional[str]=...)->Vocab
spacy.vocab.pickle_vocab(vocab:Vocab)->Any
spacy.vocab.unpickle_vocab(sstore:StringStore,vectors:Any,morphology:Any,_unused_object:Any,lex_attr_getters:Any,lookups:Any,get_noun_chunks:Any)->Vocab


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/scorer.py----------------------------------------
A:spacy.scorer.MISSING_VALUES->frozenset([None, 0, ''])
A:spacy.scorer.self.saved_score->_roc_auc_score(self.golds, self.cands)
A:spacy.scorer.self.saved_score_at_len->len(self.golds)
A:spacy.scorer.nlp->get_lang_class(default_lang)()
A:spacy.scorer.scores['tokenizer']->self.nlp.tokenizer.score(examples, **self.cfg)
A:spacy.scorer.scores[name]->component.score(examples, **self.cfg)
A:spacy.scorer.acc_score->PRFScore()
A:spacy.scorer.prf_score->PRFScore()
A:spacy.scorer.gold_spans->set()
A:spacy.scorer.pred_spans->set()
A:spacy.scorer.tag_score->PRFScore()
A:spacy.scorer.gold_tags->set()
A:spacy.scorer.missing_indices->set()
A:spacy.scorer.value->getter(token, attr)
A:spacy.scorer.pred_tags->set()
A:spacy.scorer.micro_score->PRFScore()
A:spacy.scorer.(field, values)->feat.split(Morphology.FIELD_SEP)
A:spacy.scorer.per_feat[field]->PRFScore()
A:spacy.scorer.gold_per_feat[field]->set()
A:spacy.scorer.pred_per_feat[field]->set()
A:spacy.scorer.score->PRFScore()
A:spacy.scorer.score_per_type->defaultdict(PRFScore)
A:spacy.scorer.labels->set(labels)
A:spacy.scorer.score_per_type[label]->PRFScore()
A:spacy.scorer.pred_cats->getter(example.predicted, attr)
A:spacy.scorer.gold_cats->getter(example.reference, attr)
A:spacy.scorer.pred_score->getter(example.predicted, attr).get(label, 0.0)
A:spacy.scorer.gold_score->getter(example.reference, attr).get(label)
A:spacy.scorer.(pred_label, pred_score)->max(pred_cats.items(), key=lambda it: it[1])
A:spacy.scorer.(gold_label, gold_score)->max(gold_cats, key=lambda it: it[1])
A:spacy.scorer.micro_prf->PRFScore()
A:spacy.scorer.gold_span->gold_ent_by_offset.get((pred_ent.start_char, pred_ent.end_char), None)
A:spacy.scorer.f_per_type[label]->PRFScore()
A:spacy.scorer.unlabelled->PRFScore()
A:spacy.scorer.labelled->PRFScore()
A:spacy.scorer.labelled_per_dep->dict()
A:spacy.scorer.gold_deps->set()
A:spacy.scorer.dep->getter(token, attr)
A:spacy.scorer.head->head_getter(token, head_attr)
A:spacy.scorer.labelled_per_dep[dep]->PRFScore()
A:spacy.scorer.gold_deps_per_dep[dep]->set()
A:spacy.scorer.pred_deps->set()
A:spacy.scorer.pred_deps_per_dep[dep]->set()
A:spacy.scorer.score_per_type[pred_ent.label_]->PRFScore()
A:spacy.scorer.totals->PRFScore()
A:spacy.scorer.y->numpy.ravel(y)
A:spacy.scorer.x->numpy.ravel(x)
A:spacy.scorer.d->numpy.asarray(d)
A:spacy.scorer.slice1[axis]->slice(1, None)
A:spacy.scorer.slice2[axis]->slice(None, -1)
A:spacy.scorer.ret->numpy.add.reduce(d * (y[tuple(slice1)] + y[tuple(slice2)]) / 2.0, axis)
A:spacy.scorer.(fpr, tpr, _)->_roc_curve(y_true, y_score)
A:spacy.scorer.(fps, tps, thresholds)->_binary_clf_curve(y_true, y_score)
A:spacy.scorer.fpr->numpy.repeat(np.nan, fps.shape)
A:spacy.scorer.tpr->numpy.repeat(np.nan, tps.shape)
A:spacy.scorer.y_true->numpy.ravel(y_true)
A:spacy.scorer.y_score->numpy.ravel(y_score)
A:spacy.scorer.out->numpy.cumsum(arr, axis=axis, dtype=np.float64)
A:spacy.scorer.expected->numpy.sum(arr, axis=axis, dtype=np.float64)
A:spacy.scorer.dx->numpy.diff(x)
A:spacy.scorer.area->area.dtype.type(area).dtype.type(area)
spacy.scorer.PRFScore(self,*,tp:int=0,fp:int=0,fn:int=0)
spacy.scorer.PRFScore.__add__(self,other)
spacy.scorer.PRFScore.__iadd__(self,other)
spacy.scorer.PRFScore.__init__(self,*,tp:int=0,fp:int=0,fn:int=0)
spacy.scorer.PRFScore.__len__(self)->int
spacy.scorer.PRFScore.fscore(self)->float
spacy.scorer.PRFScore.precision(self)->float
spacy.scorer.PRFScore.recall(self)->float
spacy.scorer.PRFScore.score_set(self,cand:set,gold:set)->None
spacy.scorer.PRFScore.to_dict(self)->Dict[str, float]
spacy.scorer.ROCAUCScore(self)
spacy.scorer.ROCAUCScore.__init__(self)
spacy.scorer.ROCAUCScore.is_binary(self)
spacy.scorer.ROCAUCScore.score(self)
spacy.scorer.ROCAUCScore.score_set(self,cand,gold)->None
spacy.scorer.Scorer(self,nlp:Optional['Language']=None,default_lang:str='xx',default_pipeline:Iterable[str]=DEFAULT_PIPELINE,**cfg)
spacy.scorer.Scorer.__init__(self,nlp:Optional['Language']=None,default_lang:str='xx',default_pipeline:Iterable[str]=DEFAULT_PIPELINE,**cfg)
spacy.scorer.Scorer.score(self,examples:Iterable[Example],*,per_component:bool=False)->Dict[str, Any]
spacy.scorer.Scorer.score_cats(examples:Iterable[Example],attr:str,*,getter:Callable[[Doc,str],Any]=getattr,labels:Iterable[str]=SimpleFrozenList(),multi_label:bool=True,positive_label:Optional[str]=None,threshold:Optional[float]=None,**cfg)->Dict[str, Any]
spacy.scorer.Scorer.score_deps(examples:Iterable[Example],attr:str,*,getter:Callable[[Token,str],Any]=getattr,head_attr:str='head',head_getter:Callable[[Token,str],Token]=getattr,ignore_labels:Iterable[str]=SimpleFrozenList(),missing_values:Set[Any]=MISSING_VALUES,**cfg)->Dict[str, Any]
spacy.scorer.Scorer.score_links(examples:Iterable[Example],*,negative_labels:Iterable[str],**cfg)->Dict[str, Any]
spacy.scorer.Scorer.score_spans(examples:Iterable[Example],attr:str,*,getter:Callable[[Doc,str],Iterable[Span]]=getattr,has_annotation:Optional[Callable[[Doc],bool]]=None,labeled:bool=True,allow_overlap:bool=False,**cfg)->Dict[str, Any]
spacy.scorer.Scorer.score_token_attr(examples:Iterable[Example],attr:str,*,getter:Callable[[Token,str],Any]=getattr,missing_values:Set[Any]=MISSING_VALUES,**cfg)->Dict[str, Any]
spacy.scorer.Scorer.score_token_attr_per_feat(examples:Iterable[Example],attr:str,*,getter:Callable[[Token,str],Any]=getattr,missing_values:Set[Any]=MISSING_VALUES,**cfg)->Dict[str, Any]
spacy.scorer.Scorer.score_tokenization(examples:Iterable[Example],**cfg)->Dict[str, Any]
spacy.scorer._auc(x,y)
spacy.scorer._binary_clf_curve(y_true,y_score)
spacy.scorer._roc_auc_score(y_true,y_score)
spacy.scorer._roc_curve(y_true,y_score)
spacy.scorer._stable_cumsum(arr,axis=None,rtol=1e-05,atol=1e-08)
spacy.scorer.get_ner_prf(examples:Iterable[Example],**kwargs)->Dict[str, Any]
spacy.scorer.trapezoid(y,x=None,dx=1.0,axis=-1)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/glossary.py----------------------------------------
spacy.explain(term)
spacy.glossary.explain(term)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/strings.pyi----------------------------------------
spacy.strings.StringStore(self,strings:Optional[Iterable[str]]=...,freeze:bool=...)
spacy.strings.StringStore.__contains__(self,string:str)->bool
spacy.strings.StringStore.__getitem__(self,string_or_id:Union[bytes,str])->int
spacy.strings.StringStore.__getitem__(self,string_or_id:int)->str
spacy.strings.StringStore.__init__(self,strings:Optional[Iterable[str]]=...,freeze:bool=...)
spacy.strings.StringStore.__iter__(self)->Iterator[str]
spacy.strings.StringStore.__len__(self)->int
spacy.strings.StringStore.__reduce__(self)->Any
spacy.strings.StringStore._reset_and_load(self,strings:Iterable[str])->None
spacy.strings.StringStore.add(self,string:str)->int
spacy.strings.StringStore.as_int(self,key:Union[bytes,str,int])->int
spacy.strings.StringStore.as_string(self,key:Union[bytes,str,int])->str
spacy.strings.StringStore.from_bytes(self,bytes_data:bytes,**kwargs:Any)->StringStore
spacy.strings.StringStore.from_disk(self,path:Union[str,Path])->StringStore
spacy.strings.StringStore.to_bytes(self,**kwargs:Any)->bytes
spacy.strings.StringStore.to_disk(self,path:Union[str,Path])->None
spacy.strings.get_string_id(key:Union[str,int])->int


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/ty.py----------------------------------------
spacy.ty.InitializableComponent(Protocol)
spacy.ty.InitializableComponent.initialize(self,get_examples:Callable[[],Iterable['Example']],nlp:'Language',**kwargs:Any)
spacy.ty.ListenedToComponent(Protocol)
spacy.ty.ListenedToComponent.add_listener(self,listener:Model,component_name:str)->None
spacy.ty.ListenedToComponent.find_listeners(self,component)->None
spacy.ty.ListenedToComponent.remove_listener(self,listener:Model,component_name:str)->bool
spacy.ty.TrainableComponent(Protocol)
spacy.ty.TrainableComponent.finish_update(self,sgd:Optimizer)->None
spacy.ty.TrainableComponent.update(self,examples:Iterable['Example'],*,drop:float=0.0,sgd:Optional[Optimizer]=None,losses:Optional[Dict[str,float]]=None)->Dict[str, float]


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/about.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/git_info.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/language.py----------------------------------------
A:spacy.language.DEFAULT_CONFIG->util.load_config(DEFAULT_CONFIG_PATH)
A:spacy.language._AnyContext->TypeVar('_AnyContext')
A:spacy.language.lookups->load_lookups(lang=lang, tables=tables)
A:spacy.language.factories->SimpleFrozenDict(error=Errors.E957)
A:spacy.language.self._config->util.load_config(DEFAULT_CONFIG_PATH).merge(self.default_config)
A:spacy.language.self._meta->dict(meta)
A:spacy.language.vectors_name->meta.get('vectors', {}).get('name')
A:spacy.language.vocab->create_vocab(self.lang, self.Defaults, vectors_name=vectors_name)
A:spacy.language.vocab.vectors->create_vectors(vocab)
A:spacy.language.self.tokenizer->create_tokenizer(self)
A:spacy.language.cls.default_config->util.load_config(DEFAULT_CONFIG_PATH).merge(cls.Defaults.config)
A:spacy.language.spacy_version->util.get_minor_version_range(about.__version__)
A:spacy.language.self._meta['labels']->dict(self.pipe_labels)
A:spacy.language.self._meta['pipeline']->list(self.pipe_names)
A:spacy.language.self._meta['components']->list(self.component_names)
A:spacy.language.self._meta['disabled']->list(self.disabled)
A:spacy.language.pipe_meta->self.get_factory_meta(factory_name)
A:spacy.language.pipe_config->util.copy_config(source_config['components'][source_name])
A:spacy.language.self._config['nlp']['pipeline']->list(self.component_names)
A:spacy.language.self._config['nlp']['disabled']->list(self.disabled)
A:spacy.language.prev_weights->self._config['training'].get('score_weights', {})
A:spacy.language.combined_score_weights->combine_score_weights(score_weights, prev_weights)
A:spacy.language.names->list(self.factories.keys())
A:spacy.language.labels[name]->list(pipe.labels)
A:spacy.language.internal_name->self.get_factory_name(factory_name)
A:spacy.language.err->errors.Errors.E886.format(name=pipe_name, tok2vec=tok2vec_name, path=listener_path)
A:spacy.language.existing_func->util.registry.factories.get(internal_name)
A:spacy.language.arg_names->util.get_arg_names(factory_func)
A:spacy.language.factory_meta->FactoryMeta(factory=name, default_config=default_config, assigns=validate_attrs(assigns), requires=validate_attrs(requires), scores=list(default_score_weights.keys()), default_score_weights=default_score_weights, retokenizes=retokenizes)
A:spacy.language.cls.factories->SimpleFrozenDict(registry.factories.get_all(), error=Errors.E957)
A:spacy.language.analysis->analyze_pipes(self, keys=keys)
A:spacy.language.config->util.copy_config(config)
A:spacy.language.resolved->util.registry.resolve(cfg, validate=validate)
A:spacy.language.filled->util.registry.fill(config, validate=validate, schema=ConfigSchema)
A:spacy.language.pipe->self.get_pipe(pipe_name)
A:spacy.language.source_config->source.config.interpolate()
A:spacy.language.bad_val->repr(factory_name)
A:spacy.language.(pipe_component, factory_name)->self.create_pipe_from_source(factory_name, source, name=name)
A:spacy.language.pipe_component->self.create_pipe(factory_name, name=name, config=config, raw_config=raw_config, validate=validate)
A:spacy.language.pipe_index->self.component_names.index(name)
A:spacy.language.self._pipe_meta[name]->self.get_factory_meta(factory_name)
A:spacy.language.i->self.component_names.index(old_name)
A:spacy.language.self._pipe_meta[new_name]->self._pipe_meta.pop(old_name)
A:spacy.language.self._pipe_configs[new_name]->self._pipe_configs.pop(old_name)
A:spacy.language.init_cfg->self._config['initialize']['components'].pop(old_name)
A:spacy.language.removed->self._components.pop(self.component_names.index(name))
A:spacy.language.doc->Doc(self.vocab).from_bytes(byte_doc)
A:spacy.language.error_handler->proc.get_error_handler()
A:spacy.language.examples->_copy_examples(examples)
A:spacy.language.self._optimizer->self.create_optimizer()
A:spacy.language.pipe_kwargs[name]->deepcopy(component_cfg[name])
A:spacy.language.pipes->list(self.pipeline)
A:spacy.language.I->util.registry.resolve(config['initialize'], schema=ConfigSchemaInit)
A:spacy.language.ops->get_current_ops()
A:spacy.language.tok_settings->validate_init_settings(self.tokenizer.initialize, I['tokenizer'], section='tokenizer', name='tokenizer')
A:spacy.language.p_settings->validate_init_settings(proc.initialize, p_settings, section='components', name=name)
A:spacy.language.pretrain_cfg->util.copy_config(config).get('pretraining')
A:spacy.language.P->util.registry.resolve(pretrain_cfg, schema=ConfigSchemaPretrain)
A:spacy.language.proc._rehearsal_model->deepcopy(proc.model)
A:spacy.language.kwargs->component_cfg.get(name, {})
A:spacy.language.scorer->Scorer(**kwargs)
A:spacy.language.start_time->timer()
A:spacy.language.docs->pipe(docs)
A:spacy.language.end_time->timer()
A:spacy.language.results->Scorer(**kwargs).score(examples, per_component=per_component)
A:spacy.language.n_words->sum((len(eg.predicted) for eg in examples))
A:spacy.language.texts->cast(Iterable[Union[str, Doc]], texts)
A:spacy.language.n_process->multiprocessing.cpu_count()
A:spacy.language.f->functools.partial(_pipe, proc=proc, name=name, kwargs=kwargs, default_error_handler=self.default_error_handler)
A:spacy.language.serialized_texts_with_ctx->prepare_input(texts)
A:spacy.language.(texts, raw_texts)->itertools.tee(serialized_texts_with_ctx)
A:spacy.language.(bytedocs_recv_ch, bytedocs_send_ch)->zip(*[mp.Pipe(False) for _ in range(n_process)])
A:spacy.language.batch_texts->util.minibatch(texts, batch_size)
A:spacy.language.sender->_Sender(batch_texts, texts_q, chunk_size=n_process)
A:spacy.language.byte_tuples->itertools.chain.from_iterable((recv.recv() for recv in cycle(bytedocs_recv_ch)))
A:spacy.language.error->srsly.msgpack_loads(byte_error)
A:spacy.language.config_lang->config['nlp'].get('lang')
A:spacy.language.orig_pipeline->util.copy_config(config).pop('components', {})
A:spacy.language.orig_pretraining->util.copy_config(config).pop('pretraining', None)
A:spacy.language.resolved_nlp->util.registry.resolve(filled['nlp'], validate=validate, schema=ConfigSchemaNlp)
A:spacy.language.lang_cls->before_creation(cls)
A:spacy.language.nlp->after_pipeline_creation(nlp)
A:spacy.language.pipeline->interpolated.get('components', {})
A:spacy.language.opts->', '.join(pipeline.keys())
A:spacy.language.pipe_cfg->util.copy_config(pipeline[pipe_name])
A:spacy.language.raw_config->Config(filled['components'][pipe_name])
A:spacy.language.factory->util.copy_config(pipeline[pipe_name]).pop('factory')
A:spacy.language.vocab_b->after_pipeline_creation(nlp).vocab.to_bytes(exclude=['lookups', 'strings'])
A:spacy.language.source_nlps[model]->util.load_model(model, vocab=nlp.vocab, exclude=['lookups'])
A:spacy.language.source_name->util.copy_config(pipeline[pipe_name]).get('component', pipe_name)
A:spacy.language.source_nlp_vectors_hashes[model]->hash(source_nlps[model].vocab.vectors.to_bytes(exclude=['strings']))
A:spacy.language.enabled->config['nlp'].get('enabled', [])
A:spacy.language.disabled_pipes->cls._resolve_component_status(list({*disable, *config['nlp'].get('disabled', [])}), enable, config['nlp']['pipeline'])
A:spacy.language.nlp._disabled->set((p for p in disabled_pipes if p not in exclude))
A:spacy.language.tok2vec->self.get_pipe(tok2vec_name)
A:spacy.language.tok2vec_cfg->self.get_pipe_config(tok2vec_name)
A:spacy.language.pipe_listeners->self.get_pipe(tok2vec_name).listener_map.get(pipe_name, [])
A:spacy.language.new_config->replace_func(tok2vec_cfg['model'], pipe_cfg['model']['tok2vec'])
A:spacy.language.new_model->replace_listener_func(new_model, listener, tok2vec)
A:spacy.language.replace_listener_func->tok2vec_model.attrs.get('replace_listener')
A:spacy.language.num_params->len(inspect.signature(replace_listener_func).parameters)
A:spacy.language.mem->Pool()
A:spacy.language.path->util.ensure_path(path)
A:spacy.language.data->srsly.json_loads(b)
A:spacy.language.self.vocab.vectors.name->srsly.json_loads(b).get('vectors', {}).get('name')
A:spacy.language.texts_with_ctx->receiver.get()
A:spacy.language.self.data->iter(data)
A:spacy.language.self.queues->iter(cycle(queues))
A:spacy.language._WORK_DONE_SENTINEL->_WorkDoneSentinel()
spacy.Language(self,vocab:Union[Vocab,bool]=True,*,max_length:int=10**6,meta:Dict[str,Any]={},create_tokenizer:Optional[Callable[['Language'],Callable[[str],Doc]]]=None,create_vectors:Optional[Callable[['Vocab'],BaseVectors]]=None,batch_size:int=1000,**kwargs)
spacy.Language.__init_subclass__(cls,**kwargs)
spacy.Language._ensure_doc(self,doc_like:Union[str,Doc,bytes])->Doc
spacy.Language._ensure_doc_with_context(self,doc_like:Union[str,Doc,bytes],context:_AnyContext)->Doc
spacy.Language._get_pipe_index(self,before:Optional[Union[str,int]]=None,after:Optional[Union[str,int]]=None,first:Optional[bool]=None,last:Optional[bool]=None)->int
spacy.Language._has_gpu_model(self,disable:Iterable[str])
spacy.Language._link_components(self)->None
spacy.Language._multiprocessing_pipe(self,texts:Iterable[Union[str,Doc]],pipes:Iterable[Callable[...,Iterator[Doc]]],n_process:int,batch_size:int)->Iterator[Doc]
spacy.Language._resolve_component_status(disable:Union[str,Iterable[str]],enable:Union[str,Iterable[str]],pipe_names:Iterable[str])->Tuple[str, ...]
spacy.Language.add_pipe(self,factory_name:str,name:Optional[str]=None,*,before:Optional[Union[str,int]]=None,after:Optional[Union[str,int]]=None,first:Optional[bool]=None,last:Optional[bool]=None,source:Optional['Language']=None,config:Dict[str,Any]=SimpleFrozenDict(),raw_config:Optional[Config]=None,validate:bool=True)->PipeCallable
spacy.Language.analyze_pipes(self,*,keys:List[str]=['assigns','requires','scores','retokenizes'],pretty:bool=False)->Optional[Dict[str, Any]]
spacy.Language.begin_training(self,get_examples:Optional[Callable[[],Iterable[Example]]]=None,*,sgd:Optional[Optimizer]=None)->Optimizer
spacy.Language.component(cls,name:str,*,assigns:Iterable[str]=SimpleFrozenList(),requires:Iterable[str]=SimpleFrozenList(),retokenizes:bool=False,func:Optional[PipeCallable]=None)->Callable[..., Any]
spacy.Language.component_names(self)->List[str]
spacy.Language.components(self)->List[Tuple[str, PipeCallable]]
spacy.Language.config(self)->Config
spacy.Language.config(self,value:Config)->None
spacy.Language.create_optimizer(self)
spacy.Language.create_pipe(self,factory_name:str,name:Optional[str]=None,*,config:Dict[str,Any]=SimpleFrozenDict(),raw_config:Optional[Config]=None,validate:bool=True)->PipeCallable
spacy.Language.create_pipe_from_source(self,source_name:str,source:'Language',*,name:str)->Tuple[PipeCallable, str]
spacy.Language.disable_pipe(self,name:str)->None
spacy.Language.disable_pipes(self,*names)->'DisabledPipes'
spacy.Language.disabled(self)->List[str]
spacy.Language.enable_pipe(self,name:str)->None
spacy.Language.evaluate(self,examples:Iterable[Example],*,batch_size:Optional[int]=None,scorer:Optional[Scorer]=None,component_cfg:Optional[Dict[str,Dict[str,Any]]]=None,scorer_cfg:Optional[Dict[str,Any]]=None,per_component:bool=False)->Dict[str, Any]
spacy.Language.factory(cls,name:str,*,default_config:Dict[str,Any]=SimpleFrozenDict(),assigns:Iterable[str]=SimpleFrozenList(),requires:Iterable[str]=SimpleFrozenList(),retokenizes:bool=False,default_score_weights:Dict[str,Optional[float]]=SimpleFrozenDict(),func:Optional[Callable]=None)->Callable
spacy.Language.factory_names(self)->List[str]
spacy.Language.from_bytes(self,bytes_data:bytes,*,exclude:Iterable[str]=SimpleFrozenList())->'Language'
spacy.Language.from_config(cls,config:Union[Dict[str,Any],Config]={},*,vocab:Union[Vocab,bool]=True,disable:Union[str,Iterable[str]]=_DEFAULT_EMPTY_PIPES,enable:Union[str,Iterable[str]]=_DEFAULT_EMPTY_PIPES,exclude:Union[str,Iterable[str]]=_DEFAULT_EMPTY_PIPES,meta:Dict[str,Any]=SimpleFrozenDict(),auto_fill:bool=True,validate:bool=True)->'Language'
spacy.Language.from_disk(self,path:Union[str,Path],*,exclude:Iterable[str]=SimpleFrozenList(),overrides:Dict[str,Any]=SimpleFrozenDict())->'Language'
spacy.Language.get_factory_meta(cls,name:str)->'FactoryMeta'
spacy.Language.get_factory_name(cls,name:str)->str
spacy.Language.get_pipe(self,name:str)->PipeCallable
spacy.Language.get_pipe_config(self,name:str)->Config
spacy.Language.get_pipe_meta(self,name:str)->'FactoryMeta'
spacy.Language.has_factory(cls,name:str)->bool
spacy.Language.has_pipe(self,name:str)->bool
spacy.Language.initialize(self,get_examples:Optional[Callable[[],Iterable[Example]]]=None,*,sgd:Optional[Optimizer]=None)->Optimizer
spacy.Language.make_doc(self,text:str)->Doc
spacy.Language.memory_zone(self,mem:Optional[Pool]=None)->Iterator[Pool]
spacy.Language.meta(self)->Dict[str, Any]
spacy.Language.meta(self,value:Dict[str,Any])->None
spacy.Language.path(self)
spacy.Language.pipe(self,texts:Union[Iterable[Union[str,Doc]],Iterable[Tuple[Union[str,Doc],_AnyContext]]],*,as_tuples:bool=False,batch_size:Optional[int]=None,disable:Iterable[str]=SimpleFrozenList(),component_cfg:Optional[Dict[str,Dict[str,Any]]]=None,n_process:int=1)->Union[Iterator[Doc], Iterator[Tuple[Doc, _AnyContext]]]
spacy.Language.pipe_factories(self)->Dict[str, str]
spacy.Language.pipe_labels(self)->Dict[str, List[str]]
spacy.Language.pipe_names(self)->List[str]
spacy.Language.pipeline(self)->List[Tuple[str, PipeCallable]]
spacy.Language.rehearse(self,examples:Iterable[Example],*,sgd:Optional[Optimizer]=None,losses:Optional[Dict[str,float]]=None,component_cfg:Optional[Dict[str,Dict[str,Any]]]=None,exclude:Iterable[str]=SimpleFrozenList())->Dict[str, float]
spacy.Language.remove_pipe(self,name:str)->Tuple[str, PipeCallable]
spacy.Language.rename_pipe(self,old_name:str,new_name:str)->None
spacy.Language.replace_listeners(self,tok2vec_name:str,pipe_name:str,listeners:Iterable[str])->None
spacy.Language.replace_pipe(self,name:str,factory_name:str,*,config:Dict[str,Any]=SimpleFrozenDict(),validate:bool=True)->PipeCallable
spacy.Language.resume_training(self,*,sgd:Optional[Optimizer]=None)->Optimizer
spacy.Language.select_pipes(self,*,disable:Optional[Union[str,Iterable[str]]]=None,enable:Optional[Union[str,Iterable[str]]]=None)->'DisabledPipes'
spacy.Language.set_error_handler(self,error_handler:Callable[[str,PipeCallable,List[Doc],Exception],NoReturn])
spacy.Language.set_factory_meta(cls,name:str,value:'FactoryMeta')->None
spacy.Language.to_bytes(self,*,exclude:Iterable[str]=SimpleFrozenList())->bytes
spacy.Language.to_disk(self,path:Union[str,Path],*,exclude:Iterable[str]=SimpleFrozenList())->None
spacy.Language.update(self,examples:Iterable[Example],_:Optional[Any]=None,*,drop:float=0.0,sgd:Optional[Optimizer]=None,losses:Optional[Dict[str,float]]=None,component_cfg:Optional[Dict[str,Dict[str,Any]]]=None,exclude:Iterable[str]=SimpleFrozenList(),annotates:Iterable[str]=SimpleFrozenList())
spacy.Language.use_params(self,params:Optional[dict])
spacy.language.BaseDefaults
spacy.language.DisabledPipes(self,nlp:Language,names:List[str])
spacy.language.DisabledPipes.__enter__(self)
spacy.language.DisabledPipes.__exit__(self,*args)
spacy.language.DisabledPipes.__init__(self,nlp:Language,names:List[str])
spacy.language.DisabledPipes.restore(self)->None
spacy.language.FactoryMeta
spacy.language.Language(self,vocab:Union[Vocab,bool]=True,*,max_length:int=10**6,meta:Dict[str,Any]={},create_tokenizer:Optional[Callable[['Language'],Callable[[str],Doc]]]=None,create_vectors:Optional[Callable[['Vocab'],BaseVectors]]=None,batch_size:int=1000,**kwargs)
spacy.language.Language.__init__(self,vocab:Union[Vocab,bool]=True,*,max_length:int=10**6,meta:Dict[str,Any]={},create_tokenizer:Optional[Callable[['Language'],Callable[[str],Doc]]]=None,create_vectors:Optional[Callable[['Vocab'],BaseVectors]]=None,batch_size:int=1000,**kwargs)
spacy.language.Language.__init_subclass__(cls,**kwargs)
spacy.language.Language._ensure_doc(self,doc_like:Union[str,Doc,bytes])->Doc
spacy.language.Language._ensure_doc_with_context(self,doc_like:Union[str,Doc,bytes],context:_AnyContext)->Doc
spacy.language.Language._get_pipe_index(self,before:Optional[Union[str,int]]=None,after:Optional[Union[str,int]]=None,first:Optional[bool]=None,last:Optional[bool]=None)->int
spacy.language.Language._has_gpu_model(self,disable:Iterable[str])
spacy.language.Language._link_components(self)->None
spacy.language.Language._multiprocessing_pipe(self,texts:Iterable[Union[str,Doc]],pipes:Iterable[Callable[...,Iterator[Doc]]],n_process:int,batch_size:int)->Iterator[Doc]
spacy.language.Language._resolve_component_status(disable:Union[str,Iterable[str]],enable:Union[str,Iterable[str]],pipe_names:Iterable[str])->Tuple[str, ...]
spacy.language.Language.add_pipe(self,factory_name:str,name:Optional[str]=None,*,before:Optional[Union[str,int]]=None,after:Optional[Union[str,int]]=None,first:Optional[bool]=None,last:Optional[bool]=None,source:Optional['Language']=None,config:Dict[str,Any]=SimpleFrozenDict(),raw_config:Optional[Config]=None,validate:bool=True)->PipeCallable
spacy.language.Language.analyze_pipes(self,*,keys:List[str]=['assigns','requires','scores','retokenizes'],pretty:bool=False)->Optional[Dict[str, Any]]
spacy.language.Language.begin_training(self,get_examples:Optional[Callable[[],Iterable[Example]]]=None,*,sgd:Optional[Optimizer]=None)->Optimizer
spacy.language.Language.component(cls,name:str,*,assigns:Iterable[str]=SimpleFrozenList(),requires:Iterable[str]=SimpleFrozenList(),retokenizes:bool=False,func:Optional[PipeCallable]=None)->Callable[..., Any]
spacy.language.Language.component_names(self)->List[str]
spacy.language.Language.components(self)->List[Tuple[str, PipeCallable]]
spacy.language.Language.config(self)->Config
spacy.language.Language.config(self,value:Config)->None
spacy.language.Language.create_optimizer(self)
spacy.language.Language.create_pipe(self,factory_name:str,name:Optional[str]=None,*,config:Dict[str,Any]=SimpleFrozenDict(),raw_config:Optional[Config]=None,validate:bool=True)->PipeCallable
spacy.language.Language.create_pipe_from_source(self,source_name:str,source:'Language',*,name:str)->Tuple[PipeCallable, str]
spacy.language.Language.disable_pipe(self,name:str)->None
spacy.language.Language.disable_pipes(self,*names)->'DisabledPipes'
spacy.language.Language.disabled(self)->List[str]
spacy.language.Language.enable_pipe(self,name:str)->None
spacy.language.Language.evaluate(self,examples:Iterable[Example],*,batch_size:Optional[int]=None,scorer:Optional[Scorer]=None,component_cfg:Optional[Dict[str,Dict[str,Any]]]=None,scorer_cfg:Optional[Dict[str,Any]]=None,per_component:bool=False)->Dict[str, Any]
spacy.language.Language.factory(cls,name:str,*,default_config:Dict[str,Any]=SimpleFrozenDict(),assigns:Iterable[str]=SimpleFrozenList(),requires:Iterable[str]=SimpleFrozenList(),retokenizes:bool=False,default_score_weights:Dict[str,Optional[float]]=SimpleFrozenDict(),func:Optional[Callable]=None)->Callable
spacy.language.Language.factory_names(self)->List[str]
spacy.language.Language.from_bytes(self,bytes_data:bytes,*,exclude:Iterable[str]=SimpleFrozenList())->'Language'
spacy.language.Language.from_config(cls,config:Union[Dict[str,Any],Config]={},*,vocab:Union[Vocab,bool]=True,disable:Union[str,Iterable[str]]=_DEFAULT_EMPTY_PIPES,enable:Union[str,Iterable[str]]=_DEFAULT_EMPTY_PIPES,exclude:Union[str,Iterable[str]]=_DEFAULT_EMPTY_PIPES,meta:Dict[str,Any]=SimpleFrozenDict(),auto_fill:bool=True,validate:bool=True)->'Language'
spacy.language.Language.from_disk(self,path:Union[str,Path],*,exclude:Iterable[str]=SimpleFrozenList(),overrides:Dict[str,Any]=SimpleFrozenDict())->'Language'
spacy.language.Language.get_factory_meta(cls,name:str)->'FactoryMeta'
spacy.language.Language.get_factory_name(cls,name:str)->str
spacy.language.Language.get_pipe(self,name:str)->PipeCallable
spacy.language.Language.get_pipe_config(self,name:str)->Config
spacy.language.Language.get_pipe_meta(self,name:str)->'FactoryMeta'
spacy.language.Language.has_factory(cls,name:str)->bool
spacy.language.Language.has_pipe(self,name:str)->bool
spacy.language.Language.initialize(self,get_examples:Optional[Callable[[],Iterable[Example]]]=None,*,sgd:Optional[Optimizer]=None)->Optimizer
spacy.language.Language.make_doc(self,text:str)->Doc
spacy.language.Language.memory_zone(self,mem:Optional[Pool]=None)->Iterator[Pool]
spacy.language.Language.meta(self)->Dict[str, Any]
spacy.language.Language.meta(self,value:Dict[str,Any])->None
spacy.language.Language.path(self)
spacy.language.Language.pipe(self,texts:Union[Iterable[Union[str,Doc]],Iterable[Tuple[Union[str,Doc],_AnyContext]]],*,as_tuples:bool=False,batch_size:Optional[int]=None,disable:Iterable[str]=SimpleFrozenList(),component_cfg:Optional[Dict[str,Dict[str,Any]]]=None,n_process:int=1)->Union[Iterator[Doc], Iterator[Tuple[Doc, _AnyContext]]]
spacy.language.Language.pipe_factories(self)->Dict[str, str]
spacy.language.Language.pipe_labels(self)->Dict[str, List[str]]
spacy.language.Language.pipe_names(self)->List[str]
spacy.language.Language.pipeline(self)->List[Tuple[str, PipeCallable]]
spacy.language.Language.rehearse(self,examples:Iterable[Example],*,sgd:Optional[Optimizer]=None,losses:Optional[Dict[str,float]]=None,component_cfg:Optional[Dict[str,Dict[str,Any]]]=None,exclude:Iterable[str]=SimpleFrozenList())->Dict[str, float]
spacy.language.Language.remove_pipe(self,name:str)->Tuple[str, PipeCallable]
spacy.language.Language.rename_pipe(self,old_name:str,new_name:str)->None
spacy.language.Language.replace_listeners(self,tok2vec_name:str,pipe_name:str,listeners:Iterable[str])->None
spacy.language.Language.replace_pipe(self,name:str,factory_name:str,*,config:Dict[str,Any]=SimpleFrozenDict(),validate:bool=True)->PipeCallable
spacy.language.Language.resume_training(self,*,sgd:Optional[Optimizer]=None)->Optimizer
spacy.language.Language.select_pipes(self,*,disable:Optional[Union[str,Iterable[str]]]=None,enable:Optional[Union[str,Iterable[str]]]=None)->'DisabledPipes'
spacy.language.Language.set_error_handler(self,error_handler:Callable[[str,PipeCallable,List[Doc],Exception],NoReturn])
spacy.language.Language.set_factory_meta(cls,name:str,value:'FactoryMeta')->None
spacy.language.Language.to_bytes(self,*,exclude:Iterable[str]=SimpleFrozenList())->bytes
spacy.language.Language.to_disk(self,path:Union[str,Path],*,exclude:Iterable[str]=SimpleFrozenList())->None
spacy.language.Language.update(self,examples:Iterable[Example],_:Optional[Any]=None,*,drop:float=0.0,sgd:Optional[Optimizer]=None,losses:Optional[Dict[str,float]]=None,component_cfg:Optional[Dict[str,Dict[str,Any]]]=None,exclude:Iterable[str]=SimpleFrozenList(),annotates:Iterable[str]=SimpleFrozenList())
spacy.language.Language.use_params(self,params:Optional[dict])
spacy.language._Sender(self,data:Iterable[Any],queues:List[mp.Queue],chunk_size:int)
spacy.language._Sender.__init__(self,data:Iterable[Any],queues:List[mp.Queue],chunk_size:int)
spacy.language._Sender.send(self)->None
spacy.language._Sender.step(self)->None
spacy.language._WorkDoneSentinel
spacy.language._apply_pipes(ensure_doc:Callable[[Union[str,Doc,bytes],_AnyContext],Doc],pipes:Iterable[Callable[...,Iterator[Doc]]],receiver,sender,underscore_state:Tuple[dict,dict,dict])->None
spacy.language._copy_examples(examples:Iterable[Example])->List[Example]
spacy.language._replace_numpy_floats(meta_dict:dict)->dict
spacy.language.create_tokenizer()->Callable[['Language'], Tokenizer]
spacy.language.load_lookups_data(lang,tables)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/pipe_analysis.py----------------------------------------
A:spacy.pipe_analysis.data->dot_to_dict({value: True for value in values})
A:spacy.pipe_analysis.invalid_attrs->', '.join((a for a in values if a.startswith(obj_key)))
A:spacy.pipe_analysis.meta->nlp.get_pipe_meta(name)
A:spacy.pipe_analysis.prev_meta->nlp.get_pipe_meta(prev_name)
A:spacy.pipe_analysis.n_problems->sum((len(p) for p in analysis['problems'].values()))
spacy.pipe_analysis.analyze_pipes(nlp:'Language',*,keys:List[str]=DEFAULT_KEYS)->Dict[str, Dict[str, Union[List[str], Dict]]]
spacy.pipe_analysis.get_attr_info(nlp:'Language',attr:str)->Dict[str, List[str]]
spacy.pipe_analysis.print_pipe_analysis(analysis:Dict[str,Dict[str,Union[List[str],Dict]]],*,keys:List[str]=DEFAULT_KEYS)->None
spacy.pipe_analysis.validate_attrs(values:Iterable[str])->Iterable[str]


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/schemas.py----------------------------------------
A:spacy.schemas.ItemT->TypeVar('ItemT')
A:spacy.schemas.errors->e.errors()
A:spacy.schemas.data->defaultdict(list)
A:spacy.schemas.err_loc->' -> '.join([str(p) for p in error.get('loc', [])])
A:spacy.schemas.sig->inspect.signature(func)
A:spacy.schemas.schema->get_arg_model(func, exclude=exclude, name='InitArgModel')
A:spacy.schemas.regex->re.compile('^({\\d+}|{\\d+,\\d*}|{\\d*,\\d+})$')
spacy.schemas.ArgSchemaConfig
spacy.schemas.ArgSchemaConfigExtra
spacy.schemas.ConfigSchema(BaseModel)
spacy.schemas.ConfigSchema.Config
spacy.schemas.ConfigSchemaInit(BaseModel)
spacy.schemas.ConfigSchemaInit.Config
spacy.schemas.ConfigSchemaNlp(BaseModel)
spacy.schemas.ConfigSchemaNlp.Config
spacy.schemas.ConfigSchemaPretrain(BaseModel)
spacy.schemas.ConfigSchemaPretrain.Config
spacy.schemas.ConfigSchemaPretrainEmpty(BaseModel)
spacy.schemas.ConfigSchemaPretrainEmpty.Config
spacy.schemas.ConfigSchemaTraining(BaseModel)
spacy.schemas.ConfigSchemaTraining.Config
spacy.schemas.DocJSONSchema(BaseModel)
spacy.schemas.ModelMetaSchema(BaseModel)
spacy.schemas.RecommendationSchema(BaseModel)
spacy.schemas.RecommendationTrf(BaseModel)
spacy.schemas.RecommendationTrfItem(BaseModel)
spacy.schemas.TokenPattern(BaseModel)
spacy.schemas.TokenPattern.Config
spacy.schemas.TokenPattern.raise_for_none(cls,v)
spacy.schemas.TokenPatternNumber(BaseModel)
spacy.schemas.TokenPatternNumber.Config
spacy.schemas.TokenPatternNumber.raise_for_none(cls,v)
spacy.schemas.TokenPatternOperatorMinMax(ConstrainedStr)
spacy.schemas.TokenPatternOperatorSimple(str,Enum)
spacy.schemas.TokenPatternSchema(BaseModel)
spacy.schemas.TokenPatternSchema.Config
spacy.schemas.TokenPatternString(BaseModel)
spacy.schemas.TokenPatternString.Config
spacy.schemas.TokenPatternString.raise_for_none(cls,v)
spacy.schemas.get_arg_model(func:Callable,*,exclude:Iterable[str]=tuple(),name:str='ArgModel',strict:bool=True)->ModelMetaclass
spacy.schemas.validate(schema:Type[BaseModel],obj:Dict[str,Any])->List[str]
spacy.schemas.validate_init_settings(func:Callable,settings:Dict[str,Any],*,section:Optional[str]=None,name:str='',exclude:Iterable[str]=('get_examples','nlp'))->Dict[str, Any]
spacy.schemas.validate_token_pattern(obj:list)->List[str]


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/compat.py----------------------------------------
A:spacy.compat.is_windows->sys.platform.startswith('win')
A:spacy.compat.is_linux->sys.platform.startswith('linux')


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/errors.py----------------------------------------
A:spacy.errors.msg->super().__getattribute__(code)
A:spacy.errors.pattern_errors->'\n'.join([f'- {e}' for e in error_msgs])
spacy.Errors(metaclass=ErrorsWithCodes)
spacy.ErrorsWithCodes(type)
spacy.ErrorsWithCodes.__getattribute__(self,code)
spacy.errors.Errors(metaclass=ErrorsWithCodes)
spacy.errors.ErrorsWithCodes(type)
spacy.errors.ErrorsWithCodes.__getattribute__(self,code)
spacy.errors.MatchPatternError(self,key,errors)
spacy.errors.MatchPatternError.__init__(self,key,errors)
spacy.errors.Warnings(metaclass=ErrorsWithCodes)
spacy.errors._escape_warning_msg(msg)
spacy.errors.filter_warning(action:Literal['default','error','ignore','always','module','once'],error_msg:str)
spacy.errors.setup_default_warnings()
spacy.setup_default_warnings()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lookups.py----------------------------------------
A:spacy.lookups.UNSET->object()
A:spacy.lookups.lookups->Lookups()
A:spacy.lookups.data->file_.read()
A:spacy.lookups.language_data->load_language_data(data[table])
A:spacy.lookups.self->cls(name=name)
A:spacy.lookups.self.bloom->BloomFilter().from_bytes(loaded['bloom'])
A:spacy.lookups.key->get_string_id(key)
A:spacy.lookups.loaded->srsly.msgpack_loads(bytes_data)
A:spacy.lookups.table->Table(name=name, data=data)
A:spacy.lookups.self._tables[key]->Table(key, value)
A:spacy.lookups.path->ensure_path(path)
spacy.lookups.Lookups(self)
spacy.lookups.Lookups.__contains__(self,name:str)->bool
spacy.lookups.Lookups.__init__(self)
spacy.lookups.Lookups.__len__(self)->int
spacy.lookups.Lookups.add_table(self,name:str,data:dict=SimpleFrozenDict())->Table
spacy.lookups.Lookups.from_bytes(self,bytes_data:bytes,**kwargs)->'Lookups'
spacy.lookups.Lookups.from_disk(self,path:Union[str,Path],filename:str='lookups.bin',**kwargs)->'Lookups'
spacy.lookups.Lookups.get_table(self,name:str,default:Any=UNSET)->Table
spacy.lookups.Lookups.has_table(self,name:str)->bool
spacy.lookups.Lookups.remove_table(self,name:str)->Table
spacy.lookups.Lookups.set_table(self,name:str,table:Table)->None
spacy.lookups.Lookups.tables(self)->List[str]
spacy.lookups.Lookups.to_bytes(self,**kwargs)->bytes
spacy.lookups.Lookups.to_disk(self,path:Union[str,Path],filename:str='lookups.bin',**kwargs)->None
spacy.lookups.Table(self,name:Optional[str]=None,data:Optional[dict]=None)
spacy.lookups.Table.__contains__(self,key:Union[str,int])->bool
spacy.lookups.Table.__getitem__(self,key:Union[str,int])->Any
spacy.lookups.Table.__init__(self,name:Optional[str]=None,data:Optional[dict]=None)
spacy.lookups.Table.__setitem__(self,key:Union[str,int],value:Any)->None
spacy.lookups.Table.from_bytes(self,bytes_data:bytes)->'Table'
spacy.lookups.Table.from_dict(cls,data:dict,name:Optional[str]=None)->'Table'
spacy.lookups.Table.get(self,key:Union[str,int],default:Optional[Any]=None)->Any
spacy.lookups.Table.set(self,key:Union[str,int],value:Any)->None
spacy.lookups.Table.to_bytes(self)->bytes
spacy.lookups.load_lookups(lang:str,tables:List[str],strict:bool=True)->'Lookups'


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/util.py----------------------------------------
A:spacy.util.logger->logging.getLogger('spacy')
A:spacy.util.logger_stream_handler->logging.StreamHandler()
A:spacy.util.languages->catalogue.create('spacy', 'languages', entry_points=True)
A:spacy.util.architectures->catalogue.create('spacy', 'architectures', entry_points=True)
A:spacy.util.tokenizers->catalogue.create('spacy', 'tokenizers', entry_points=True)
A:spacy.util.lemmatizers->catalogue.create('spacy', 'lemmatizers', entry_points=True)
A:spacy.util.lookups->catalogue.create('spacy', 'lookups', entry_points=True)
A:spacy.util.displacy_colors->catalogue.create('spacy', 'displacy_colors', entry_points=True)
A:spacy.util.misc->catalogue.create('spacy', 'misc', entry_points=True)
A:spacy.util.callbacks->catalogue.create('spacy', 'callbacks', entry_points=True)
A:spacy.util.batchers->catalogue.create('spacy', 'batchers', entry_points=True)
A:spacy.util.readers->catalogue.create('spacy', 'readers', entry_points=True)
A:spacy.util.augmenters->catalogue.create('spacy', 'augmenters', entry_points=True)
A:spacy.util.loggers->catalogue.create('spacy', 'loggers', entry_points=True)
A:spacy.util.scorers->catalogue.create('spacy', 'scorers', entry_points=True)
A:spacy.util.vectors->catalogue.create('spacy', 'vectors', entry_points=True)
A:spacy.util._entry_point_factories->catalogue.create('spacy', 'factories', entry_points=True)
A:spacy.util.factories->catalogue.create('spacy', 'internal_factories')
A:spacy.util.models->catalogue.create('spacy', 'models', entry_points=True)
A:spacy.util.cli->catalogue.create('spacy', 'cli', entry_points=True)
A:spacy.util.reg->getattr(cls, registry_name)
A:spacy.util.func->getattr(cls, registry_name).get(func_name)
A:spacy.util.legacy_name->func_name.replace('spacy.', 'spacy-legacy.')
A:spacy.util.func_info->getattr(cls, registry_name).find(func_name)
A:spacy.util.match->find_matching_language(lang)
A:spacy.util.module->importlib.util.module_from_spec(spec)
A:spacy.util.path->path.setdefault(item, value if is_last else {}).setdefault(item, value if is_last else {})
A:spacy.util.file_path->Path(cast(os.PathLike, sys.modules[module.__module__].__file__))
A:spacy.util._DEFAULT_EMPTY_PIPES->SimpleFrozenList()
A:spacy.util.cls->importlib.import_module(name)
A:spacy.util.meta->srsly.read_json(path)
A:spacy.util.overrides->dict_to_dot(config, for_overrides=True)
A:spacy.util.config->Config(section_order=CONFIG_SECTION_ORDER)
A:spacy.util.nlp->get_lang_class(nlp_config['lang']).from_config(config, vocab=vocab, disable=disable, enable=enable, exclude=exclude, auto_fill=auto_fill, validate=validate, meta=meta)
A:spacy.util.lang_cls->get_lang_class(nlp_config['lang'])
A:spacy.util.result->sorted(result, key=lambda span: span.start)
A:spacy.util.config_path->ensure_path(path)
A:spacy.util.spec->importlib.util.spec_from_file_location(name, str(loc))
A:spacy.util.version->Version(version)
A:spacy.util.has_upper->any((sp.operator in ('<', '<=') for sp in specs))
A:spacy.util.has_lower->any((sp.operator in ('>', '>=') for sp in specs))
A:spacy.util.req->Requirement(requirement)
A:spacy.util.specset->SpecifierSet(constraint)
A:spacy.util.v->Version(version)
A:spacy.util.a->get_minor_version(version_a)
A:spacy.util.b->get_minor_version(version_b)
A:spacy.util.lower_version->get_base_version(lower_version)
A:spacy.util.warn_msg->errors.Warnings.W094.format(model=f"{meta['lang']}_{meta['name']}", model_version=meta['version'], version=meta['spacy_version'], example=get_minor_version_range(about.__version__))
A:spacy.util.model_path->ensure_path(path)
A:spacy.util.pkg->importlib.import_module(name)
A:spacy.util.cmd_list->split_command(command)
A:spacy.util.cmd_str->' '.join(command)
A:spacy.util.ret->subprocess.run(cmd_list, env=os.environ.copy(), input=stdin, encoding='utf8', check=False, stdout=subprocess.PIPE if capture else None, stderr=subprocess.STDOUT if capture else None)
A:spacy.util.error->subprocess.SubprocessError(message)
A:spacy.util.prev_cwd->pathlib.Path.cwd()
A:spacy.util.current->Path(path).resolve()
A:spacy.util.d->Path(tempfile.mkdtemp())
A:spacy.util.ops->get_current_ops()
A:spacy.util.array->compat.cupy.ndarray(numpy_array.shape, order='C', dtype=numpy_array.dtype)
A:spacy.util.entries->file_.read().split('\n')
A:spacy.util.expression->'|'.join([piece for piece in entries if piece.strip()])
A:spacy.util.exc->expand_exc(exc, "'", '’')
A:spacy.util.described_orth->''.join((attr[ORTH] for attr in token_attrs))
A:spacy.util.fixed->dict(token)
A:spacy.util.fixed[ORTH]->fixed[ORTH].replace(search, replace).replace(search, replace)
A:spacy.util.new_excs->dict(excs)
A:spacy.util.new_key->token_string.replace(search, replace)
A:spacy.util.start->min(length, max(0, start))
A:spacy.util.stop->min(length, max(start, stop))
A:spacy.util.sorted_spans->sorted(spans, key=get_sort_key, reverse=True)
A:spacy.util.serialized[key]->getter()
A:spacy.util.text->text.replace('"', '&quot;').replace('"', '&quot;')
A:spacy.util.word_start->text[text_pos:].index(word)
A:spacy.util.parts->section.split('.')
A:spacy.util.argspec->inspect.getfullargspec(func)
A:spacy.util.weight_sum->sum([v if v else 0.0 for v in result.values()])
A:spacy.util.result[key]->round(value / weight_sum, 2)
A:spacy.util.size_->itertools.repeat(size)
A:spacy.util.items->iter(items)
A:spacy.util.batch_size->next(size_)
A:spacy.util.batch->list(itertools.islice(items, int(batch_size)))
A:spacy.util.value->os.environ.get(env_var, False)
A:spacy.util.kwargs->dict(kwargs)
A:spacy.util.error_handler->proc.get_error_handler()
A:spacy.util.doc->proc(doc, **kwargs)
A:spacy.util.lexeme_norms->vocab.lookups.get_table('lexeme_norm', {})
A:spacy.util.langs->', '.join(LEXEME_NORM_LANGS)
A:spacy.util.pkg_to_dist->defaultdict(list)
A:spacy.util.g->itertools.groupby(iterable)
A:spacy.util.s->socket.socket(socket.AF_INET, socket.SOCK_STREAM)
spacy.registry(thinc.registry)
spacy.registry.find(cls,registry_name:str,func_name:str)->Dict[str, Optional[Union[str, int]]]
spacy.registry.get(cls,registry_name:str,func_name:str)->Callable
spacy.registry.get_registry_names(cls)->List[str]
spacy.registry.has(cls,registry_name:str,func_name:str)->bool
spacy.util.DummyTokenizer(self,text)
spacy.util.DummyTokenizer.__call__(self,text)
spacy.util.DummyTokenizer.from_bytes(self,data:bytes,**kwargs)->'DummyTokenizer'
spacy.util.DummyTokenizer.from_disk(self,path:Union[str,Path],**kwargs)->'DummyTokenizer'
spacy.util.DummyTokenizer.pipe(self,texts,**kwargs)
spacy.util.DummyTokenizer.to_bytes(self,**kwargs)
spacy.util.DummyTokenizer.to_disk(self,path:Union[str,Path],**kwargs)->None
spacy.util.ENV_VARS
spacy.util.SimpleFrozenDict(self,*args,error:str=Errors.E095,**kwargs)
spacy.util.SimpleFrozenDict.__init__(self,*args,error:str=Errors.E095,**kwargs)
spacy.util.SimpleFrozenDict.__setitem__(self,key,value)
spacy.util.SimpleFrozenDict.pop(self,key,default=None)
spacy.util.SimpleFrozenDict.update(self,other)
spacy.util.SimpleFrozenList(self,*args,error:str=Errors.E927)
spacy.util.SimpleFrozenList.__init__(self,*args,error:str=Errors.E927)
spacy.util.SimpleFrozenList.append(self,*args,**kwargs)
spacy.util.SimpleFrozenList.clear(self,*args,**kwargs)
spacy.util.SimpleFrozenList.extend(self,*args,**kwargs)
spacy.util.SimpleFrozenList.insert(self,*args,**kwargs)
spacy.util.SimpleFrozenList.pop(self,*args,**kwargs)
spacy.util.SimpleFrozenList.remove(self,*args,**kwargs)
spacy.util.SimpleFrozenList.reverse(self,*args,**kwargs)
spacy.util.SimpleFrozenList.sort(self,*args,**kwargs)
spacy.util._get_attr_unless_lookup(default_func:Callable[[str],Any],lookups:Dict[str,Any],string:str)->Any
spacy.util._is_port_in_use(port:int,host:str='localhost')->bool
spacy.util._pipe(docs:Iterable['Doc'],proc:'PipeCallable',name:str,default_error_handler:Callable[[str,'PipeCallable',List['Doc'],Exception],NoReturn],kwargs:Mapping[str,Any])->Iterator['Doc']
spacy.util.add_lookups(default_func:Callable[[str],Any],*lookups)->Callable[[str], Any]
spacy.util.all_equal(iterable)
spacy.util.check_bool_env_var(env_var:str)->bool
spacy.util.check_lexeme_norms(vocab,component_name)
spacy.util.combine_score_weights(weights:List[Dict[str,Optional[float]]],overrides:Dict[str,Optional[float]]=SimpleFrozenDict())->Dict[str, Optional[float]]
spacy.util.compile_infix_regex(entries:Iterable[Union[str,Pattern]])->Pattern
spacy.util.compile_prefix_regex(entries:Iterable[Union[str,Pattern]])->Pattern
spacy.util.compile_suffix_regex(entries:Iterable[Union[str,Pattern]])->Pattern
spacy.util.copy_config(config:Union[Dict[str,Any],Config])->Config
spacy.util.create_default_optimizer()->Optimizer
spacy.util.dict_to_dot(obj:Dict[str,dict],*,for_overrides:bool=False)->Dict[str, Any]
spacy.util.dot_to_dict(values:Dict[str,Any])->Dict[str, dict]
spacy.util.dot_to_object(config:Config,section:str)
spacy.util.ensure_path(path:Any)->Any
spacy.util.escape_html(text:str)->str
spacy.util.expand_exc(excs:Dict[str,List[dict]],search:str,replace:str)->Dict[str, List[dict]]
spacy.util.filter_chain_spans(*spans:Iterable['Span'])->List['Span']
spacy.util.filter_spans(spans:Iterable['Span'])->List['Span']
spacy.util.find_available_port(start:int,host:str,auto_select:bool=False)->int
spacy.util.find_matching_language(lang:str)->Optional[str]
spacy.util.from_bytes(bytes_data:bytes,setters:Dict[str,Callable[[bytes],Any]],exclude:Iterable[str])->None
spacy.util.from_dict(msg:Dict[str,Any],setters:Dict[str,Callable[[Any],Any]],exclude:Iterable[str])->Dict[str, Any]
spacy.util.from_disk(path:Union[str,Path],readers:Dict[str,Callable[[Path],None]],exclude:Iterable[str])->Path
spacy.util.get_arg_names(func:Callable)->List[str]
spacy.util.get_async(stream,numpy_array)
spacy.util.get_base_version(version:str)->str
spacy.util.get_cuda_stream(require:bool=False,non_blocking:bool=True)->Optional[CudaStream]
spacy.util.get_installed_models()->List[str]
spacy.util.get_lang_class(lang:str)->Type['Language']
spacy.util.get_minor_version(version:str)->Optional[str]
spacy.util.get_minor_version_range(version:str)->str
spacy.util.get_model_lower_version(constraint:str)->Optional[str]
spacy.util.get_model_meta(path:Union[str,Path])->Dict[str, Any]
spacy.util.get_module_path(module:ModuleType)->Path
spacy.util.get_object_name(obj:Any)->str
spacy.util.get_package_path(name:str)->Path
spacy.util.get_package_version(name:str)->Optional[str]
spacy.util.get_sourced_components(config:Union[Dict[str,Any],Config])->Dict[str, Dict[str, Any]]
spacy.util.get_words_and_spaces(words:Iterable[str],text:str)->Tuple[List[str], List[bool]]
spacy.util.ignore_error(proc_name,proc,docs,e)
spacy.util.import_file(name:str,loc:Union[str,Path])->ModuleType
spacy.util.is_compatible_version(version:str,constraint:str,prereleases:bool=True)->Optional[bool]
spacy.util.is_cython_func(func:Callable)->bool
spacy.util.is_in_interactive()->bool
spacy.util.is_in_jupyter()->bool
spacy.util.is_minor_version_match(version_a:str,version_b:str)->bool
spacy.util.is_package(name:str)->bool
spacy.util.is_prerelease_version(version:str)->bool
spacy.util.is_same_func(func1:Callable,func2:Callable)->bool
spacy.util.is_unconstrained_version(constraint:str,prereleases:bool=True)->Optional[bool]
spacy.util.lang_class_is_loaded(lang:str)->bool
spacy.util.load_config(path:Union[str,Path],overrides:Dict[str,Any]=SimpleFrozenDict(),interpolate:bool=False)->Config
spacy.util.load_config_from_str(text:str,overrides:Dict[str,Any]=SimpleFrozenDict(),interpolate:bool=False)
spacy.util.load_language_data(path:Union[str,Path])->Union[dict, list]
spacy.util.load_meta(path:Union[str,Path])->Dict[str, Any]
spacy.util.load_model(name:Union[str,Path],*,vocab:Union['Vocab',bool]=True,disable:Union[str,Iterable[str]]=_DEFAULT_EMPTY_PIPES,enable:Union[str,Iterable[str]]=_DEFAULT_EMPTY_PIPES,exclude:Union[str,Iterable[str]]=_DEFAULT_EMPTY_PIPES,config:Union[Dict[str,Any],Config]=SimpleFrozenDict())->'Language'
spacy.util.load_model_from_config(config:Union[Dict[str,Any],Config],*,meta:Dict[str,Any]=SimpleFrozenDict(),vocab:Union['Vocab',bool]=True,disable:Union[str,Iterable[str]]=_DEFAULT_EMPTY_PIPES,enable:Union[str,Iterable[str]]=_DEFAULT_EMPTY_PIPES,exclude:Union[str,Iterable[str]]=_DEFAULT_EMPTY_PIPES,auto_fill:bool=False,validate:bool=True)->'Language'
spacy.util.load_model_from_init_py(init_file:Union[Path,str],*,vocab:Union['Vocab',bool]=True,disable:Union[str,Iterable[str]]=_DEFAULT_EMPTY_PIPES,enable:Union[str,Iterable[str]]=_DEFAULT_EMPTY_PIPES,exclude:Union[str,Iterable[str]]=_DEFAULT_EMPTY_PIPES,config:Union[Dict[str,Any],Config]=SimpleFrozenDict())->'Language'
spacy.util.load_model_from_package(name:str,*,vocab:Union['Vocab',bool]=True,disable:Union[str,Iterable[str]]=_DEFAULT_EMPTY_PIPES,enable:Union[str,Iterable[str]]=_DEFAULT_EMPTY_PIPES,exclude:Union[str,Iterable[str]]=_DEFAULT_EMPTY_PIPES,config:Union[Dict[str,Any],Config]=SimpleFrozenDict())->'Language'
spacy.util.load_model_from_path(model_path:Path,*,meta:Optional[Dict[str,Any]]=None,vocab:Union['Vocab',bool]=True,disable:Union[str,Iterable[str]]=_DEFAULT_EMPTY_PIPES,enable:Union[str,Iterable[str]]=_DEFAULT_EMPTY_PIPES,exclude:Union[str,Iterable[str]]=_DEFAULT_EMPTY_PIPES,config:Union[Dict[str,Any],Config]=SimpleFrozenDict())->'Language'
spacy.util.make_first_longest_spans_filter()
spacy.util.make_tempdir()->Generator[Path, None, None]
spacy.util.minibatch(items,size)
spacy.util.minify_html(html:str)->str
spacy.util.normalize_slice(length:int,start:int,stop:int,step:Optional[int]=None)->Tuple[int, int]
spacy.util.packages_distributions()->Dict[str, List[str]]
spacy.util.raise_error(proc_name,proc,docs,e)
spacy.util.read_regex(path:Union[str,Path])->Pattern
spacy.util.registry(thinc.registry)
spacy.util.registry.find(cls,registry_name:str,func_name:str)->Dict[str, Optional[Union[str, int]]]
spacy.util.registry.get(cls,registry_name:str,func_name:str)->Callable
spacy.util.registry.get_registry_names(cls)->List[str]
spacy.util.registry.has(cls,registry_name:str,func_name:str)->bool
spacy.util.replace_model_node(model:Model,target:Model,replacement:Model)->None
spacy.util.resolve_dot_names(config:Config,dot_names:List[Optional[str]])->Tuple[Any, ...]
spacy.util.run_command(command:Union[str,List[str]],*,stdin:Optional[Any]=None,capture:bool=False)->subprocess.CompletedProcess
spacy.util.set_dot_to_object(config:Config,section:str,value:Any)->None
spacy.util.set_lang_class(name:str,cls:Type['Language'])->None
spacy.util.split_command(command:str)->List[str]
spacy.util.split_requirement(requirement:str)->Tuple[str, str]
spacy.util.to_bytes(getters:Dict[str,Callable[[],bytes]],exclude:Iterable[str])->bytes
spacy.util.to_dict(getters:Dict[str,Callable[[],Any]],exclude:Iterable[str])->Dict[str, Any]
spacy.util.to_disk(path:Union[str,Path],writers:Dict[str,Callable[[Path],None]],exclude:Iterable[str])->Path
spacy.util.to_ternary_int(val)->int
spacy.util.update_exc(base_exceptions:Dict[str,List[dict]],*addition_dicts)->Dict[str, List[dict]]
spacy.util.walk_dict(node:Dict[str,Any],parent:List[str]=[],*,for_overrides:bool=False)->Iterator[Tuple[List[str], Any]]
spacy.util.warn_if_jupyter_cupy()
spacy.util.working_dir(path:Union[str,Path])->Iterator[Path]


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lexeme.pyi----------------------------------------
spacy.lexeme.Lexeme(self,vocab:Vocab,orth:int)
spacy.lexeme.Lexeme.__hash__(self)->int
spacy.lexeme.Lexeme.__init__(self,vocab:Vocab,orth:int)
spacy.lexeme.Lexeme.__richcmp__(self,other:Lexeme,op:int)->bool
spacy.lexeme.Lexeme.check_flag(self,flag_id:int)->bool
spacy.lexeme.Lexeme.has_vector(self)->bool
spacy.lexeme.Lexeme.is_oov(self)->bool
spacy.lexeme.Lexeme.orth_(self)->str
spacy.lexeme.Lexeme.set_attrs(self,**attrs:Any)->None
spacy.lexeme.Lexeme.set_flag(self,flag_id:int,value:bool)->None
spacy.lexeme.Lexeme.similarity(self,other:Union[Doc,Span,Token,Lexeme])->float
spacy.lexeme.Lexeme.text(self)->str
spacy.lexeme.Lexeme.vector_norm(self)->float


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/pipeline/attributeruler.py----------------------------------------
A:spacy.pipeline.attributeruler.self.matcher->Matcher(self.vocab, validate=self.validate)
A:spacy.pipeline.attributeruler.error_handler->self.get_error_handler()
A:spacy.pipeline.attributeruler.matches->self.matcher(doc, allow_missing=True, as_spans=False)
A:spacy.pipeline.attributeruler.span->Span(doc, start, end, label=match_id)
A:spacy.pipeline.attributeruler.(attrs, morph_attrs)->_split_morph_attrs(attrs)
A:spacy.pipeline.attributeruler.morph->self.vocab.morphology.add(morph_attrs)
A:spacy.pipeline.attributeruler.key->str(len(self.attrs))
A:spacy.pipeline.attributeruler.attrs->normalize_token_attrs(self.vocab, attrs)
spacy.pipeline.AttributeRuler(self,vocab:Vocab,name:str='attribute_ruler',*,validate:bool=False,scorer:Optional[Callable]=attribute_ruler_score)
spacy.pipeline.AttributeRuler.add(self,patterns:Iterable[MatcherPatternType],attrs:Dict,index:int=0)->None
spacy.pipeline.AttributeRuler.add_patterns(self,patterns:Iterable[AttributeRulerPatternType])->None
spacy.pipeline.AttributeRuler.clear(self)->None
spacy.pipeline.AttributeRuler.from_bytes(self,bytes_data:bytes,exclude:Iterable[str]=SimpleFrozenList())->'AttributeRuler'
spacy.pipeline.AttributeRuler.from_disk(self,path:Union[Path,str],exclude:Iterable[str]=SimpleFrozenList())->'AttributeRuler'
spacy.pipeline.AttributeRuler.initialize(self,get_examples:Optional[Callable[[],Iterable[Example]]],*,nlp:Optional[Language]=None,patterns:Optional[Iterable[AttributeRulerPatternType]]=None,tag_map:Optional[TagMapType]=None,morph_rules:Optional[MorphRulesType]=None)->None
spacy.pipeline.AttributeRuler.load_from_morph_rules(self,morph_rules:Dict[str,Dict[str,Dict[Union[int,str],Union[int,str]]]])->None
spacy.pipeline.AttributeRuler.load_from_tag_map(self,tag_map:Dict[str,Dict[Union[int,str],Union[int,str]]])->None
spacy.pipeline.AttributeRuler.match(self,doc:Doc)
spacy.pipeline.AttributeRuler.patterns(self)->List[AttributeRulerPatternType]
spacy.pipeline.AttributeRuler.set_annotations(self,doc,matches)
spacy.pipeline.AttributeRuler.to_bytes(self,exclude:Iterable[str]=SimpleFrozenList())->bytes
spacy.pipeline.AttributeRuler.to_disk(self,path:Union[Path,str],exclude:Iterable[str]=SimpleFrozenList())->None
spacy.pipeline.attributeruler.AttributeRuler(self,vocab:Vocab,name:str='attribute_ruler',*,validate:bool=False,scorer:Optional[Callable]=attribute_ruler_score)
spacy.pipeline.attributeruler.AttributeRuler.__init__(self,vocab:Vocab,name:str='attribute_ruler',*,validate:bool=False,scorer:Optional[Callable]=attribute_ruler_score)
spacy.pipeline.attributeruler.AttributeRuler.add(self,patterns:Iterable[MatcherPatternType],attrs:Dict,index:int=0)->None
spacy.pipeline.attributeruler.AttributeRuler.add_patterns(self,patterns:Iterable[AttributeRulerPatternType])->None
spacy.pipeline.attributeruler.AttributeRuler.clear(self)->None
spacy.pipeline.attributeruler.AttributeRuler.from_bytes(self,bytes_data:bytes,exclude:Iterable[str]=SimpleFrozenList())->'AttributeRuler'
spacy.pipeline.attributeruler.AttributeRuler.from_disk(self,path:Union[Path,str],exclude:Iterable[str]=SimpleFrozenList())->'AttributeRuler'
spacy.pipeline.attributeruler.AttributeRuler.initialize(self,get_examples:Optional[Callable[[],Iterable[Example]]],*,nlp:Optional[Language]=None,patterns:Optional[Iterable[AttributeRulerPatternType]]=None,tag_map:Optional[TagMapType]=None,morph_rules:Optional[MorphRulesType]=None)->None
spacy.pipeline.attributeruler.AttributeRuler.load_from_morph_rules(self,morph_rules:Dict[str,Dict[str,Dict[Union[int,str],Union[int,str]]]])->None
spacy.pipeline.attributeruler.AttributeRuler.load_from_tag_map(self,tag_map:Dict[str,Dict[Union[int,str],Union[int,str]]])->None
spacy.pipeline.attributeruler.AttributeRuler.match(self,doc:Doc)
spacy.pipeline.attributeruler.AttributeRuler.patterns(self)->List[AttributeRulerPatternType]
spacy.pipeline.attributeruler.AttributeRuler.set_annotations(self,doc,matches)
spacy.pipeline.attributeruler.AttributeRuler.to_bytes(self,exclude:Iterable[str]=SimpleFrozenList())->bytes
spacy.pipeline.attributeruler.AttributeRuler.to_disk(self,path:Union[Path,str],exclude:Iterable[str]=SimpleFrozenList())->None
spacy.pipeline.attributeruler._split_morph_attrs(attrs:dict)->Tuple[dict, dict]
spacy.pipeline.attributeruler.attribute_ruler_score(examples:Iterable[Example],**kwargs)->Dict[str, Any]
spacy.pipeline.attributeruler.make_attribute_ruler(nlp:Language,name:str,validate:bool,scorer:Optional[Callable])
spacy.pipeline.attributeruler.make_attribute_ruler_scorer()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/pipeline/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/pipeline/lemmatizer.py----------------------------------------
A:spacy.pipeline.lemmatizer.self.lookups->Lookups()
A:spacy.pipeline.lemmatizer.self.lemmatize->getattr(self, mode_attr)
A:spacy.pipeline.lemmatizer.error_handler->self.get_error_handler()
A:spacy.pipeline.lemmatizer.(required_tables, optional_tables)->self.get_lookups_config(self.mode)
A:spacy.pipeline.lemmatizer.lookups->load_lookups(lang=self.vocab.lang, tables=required_tables)
A:spacy.pipeline.lemmatizer.optional_lookups->load_lookups(lang=self.vocab.lang, tables=optional_tables, strict=False)
A:spacy.pipeline.lemmatizer.lookup_table->self.lookups.get_table('lemma_lookup', {})
A:spacy.pipeline.lemmatizer.result->self.lookups.get_table('lemma_lookup', {}).get(token.text, token.text)
A:spacy.pipeline.lemmatizer.univ_pos->token.pos_.lower()
A:spacy.pipeline.lemmatizer.index_table->self.lookups.get_table('lemma_index', {})
A:spacy.pipeline.lemmatizer.exc_table->self.lookups.get_table('lemma_exc', {})
A:spacy.pipeline.lemmatizer.rules_table->self.lookups.get_table('lemma_rules', {})
A:spacy.pipeline.lemmatizer.index->self.lookups.get_table('lemma_index', {}).get(univ_pos, {})
A:spacy.pipeline.lemmatizer.exceptions->self.lookups.get_table('lemma_exc', {}).get(univ_pos, {})
A:spacy.pipeline.lemmatizer.rules->self.lookups.get_table('lemma_rules', {}).get(univ_pos, {})
A:spacy.pipeline.lemmatizer.string->string.lower().lower()
A:spacy.pipeline.lemmatizer.forms->list(dict.fromkeys(forms))
spacy.pipeline.Lemmatizer(self,vocab:Vocab,model:Optional[Model],name:str='lemmatizer',*,mode:str='lookup',overwrite:bool=False,scorer:Optional[Callable]=lemmatizer_score)
spacy.pipeline.Lemmatizer._validate_tables(self,error_message:str=Errors.E912)->None
spacy.pipeline.Lemmatizer.from_bytes(self,bytes_data:bytes,*,exclude:Iterable[str]=SimpleFrozenList())->'Lemmatizer'
spacy.pipeline.Lemmatizer.from_disk(self,path:Union[str,Path],*,exclude:Iterable[str]=SimpleFrozenList())->'Lemmatizer'
spacy.pipeline.Lemmatizer.get_lookups_config(cls,mode:str)->Tuple[List[str], List[str]]
spacy.pipeline.Lemmatizer.initialize(self,get_examples:Optional[Callable[[],Iterable[Example]]]=None,*,nlp:Optional[Language]=None,lookups:Optional[Lookups]=None)
spacy.pipeline.Lemmatizer.is_base_form(self,token:Token)->bool
spacy.pipeline.Lemmatizer.lookup_lemmatize(self,token:Token)->List[str]
spacy.pipeline.Lemmatizer.mode(self)
spacy.pipeline.Lemmatizer.rule_lemmatize(self,token:Token)->List[str]
spacy.pipeline.Lemmatizer.to_bytes(self,*,exclude:Iterable[str]=SimpleFrozenList())->bytes
spacy.pipeline.Lemmatizer.to_disk(self,path:Union[str,Path],*,exclude:Iterable[str]=SimpleFrozenList())
spacy.pipeline.lemmatizer.Lemmatizer(self,vocab:Vocab,model:Optional[Model],name:str='lemmatizer',*,mode:str='lookup',overwrite:bool=False,scorer:Optional[Callable]=lemmatizer_score)
spacy.pipeline.lemmatizer.Lemmatizer.__init__(self,vocab:Vocab,model:Optional[Model],name:str='lemmatizer',*,mode:str='lookup',overwrite:bool=False,scorer:Optional[Callable]=lemmatizer_score)
spacy.pipeline.lemmatizer.Lemmatizer._validate_tables(self,error_message:str=Errors.E912)->None
spacy.pipeline.lemmatizer.Lemmatizer.from_bytes(self,bytes_data:bytes,*,exclude:Iterable[str]=SimpleFrozenList())->'Lemmatizer'
spacy.pipeline.lemmatizer.Lemmatizer.from_disk(self,path:Union[str,Path],*,exclude:Iterable[str]=SimpleFrozenList())->'Lemmatizer'
spacy.pipeline.lemmatizer.Lemmatizer.get_lookups_config(cls,mode:str)->Tuple[List[str], List[str]]
spacy.pipeline.lemmatizer.Lemmatizer.initialize(self,get_examples:Optional[Callable[[],Iterable[Example]]]=None,*,nlp:Optional[Language]=None,lookups:Optional[Lookups]=None)
spacy.pipeline.lemmatizer.Lemmatizer.is_base_form(self,token:Token)->bool
spacy.pipeline.lemmatizer.Lemmatizer.lookup_lemmatize(self,token:Token)->List[str]
spacy.pipeline.lemmatizer.Lemmatizer.mode(self)
spacy.pipeline.lemmatizer.Lemmatizer.rule_lemmatize(self,token:Token)->List[str]
spacy.pipeline.lemmatizer.Lemmatizer.to_bytes(self,*,exclude:Iterable[str]=SimpleFrozenList())->bytes
spacy.pipeline.lemmatizer.Lemmatizer.to_disk(self,path:Union[str,Path],*,exclude:Iterable[str]=SimpleFrozenList())
spacy.pipeline.lemmatizer.lemmatizer_score(examples:Iterable[Example],**kwargs)->Dict[str, Any]
spacy.pipeline.lemmatizer.make_lemmatizer(nlp:Language,model:Optional[Model],name:str,mode:str,overwrite:bool,scorer:Optional[Callable])
spacy.pipeline.lemmatizer.make_lemmatizer_scorer()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/pipeline/edit_tree_lemmatizer.py----------------------------------------
A:spacy.pipeline.edit_tree_lemmatizer.self.trees->EditTrees(self.vocab.strings)
A:spacy.pipeline.edit_tree_lemmatizer.self.numpy_ops->NumpyOps()
A:spacy.pipeline.edit_tree_lemmatizer.loss_func->SequenceCategoricalCrossentropy(normalize=False, missing_value=-1)
A:spacy.pipeline.edit_tree_lemmatizer.tree_id->self.trees.add(form, lemma)
A:spacy.pipeline.edit_tree_lemmatizer.label->self.tree2label.get(tree_id, 0)
A:spacy.pipeline.edit_tree_lemmatizer.(d_scores, loss)->loss_func(scores, truths)
A:spacy.pipeline.edit_tree_lemmatizer.n_docs->len(list(docs))
A:spacy.pipeline.edit_tree_lemmatizer.n_labels->len(self.cfg['labels'])
A:spacy.pipeline.edit_tree_lemmatizer.scores->self.model.predict(docs)
A:spacy.pipeline.edit_tree_lemmatizer.guesses->scores2guesses(docs, scores)
A:spacy.pipeline.edit_tree_lemmatizer.doc_guesses->self.numpy_ops.asarray(doc_guesses)
A:spacy.pipeline.edit_tree_lemmatizer.top_k->min(self.top_k, len(self.labels))
A:spacy.pipeline.edit_tree_lemmatizer.doc_scores->self.numpy_ops.asarray(doc_scores)
A:spacy.pipeline.edit_tree_lemmatizer.candidate->int(doc_scores[i].argmax())
A:spacy.pipeline.edit_tree_lemmatizer.doc_tree_ids->doc_tree_ids.get().get()
A:spacy.pipeline.edit_tree_lemmatizer.doc[j].lemma->getattr(doc[j], self.backoff)
A:spacy.pipeline.edit_tree_lemmatizer.lemma->self.trees.apply(tree_id, doc[j].text)
A:spacy.pipeline.edit_tree_lemmatizer.gold_label->self._pair2label(token.text, token.lemma_)
A:spacy.pipeline.edit_tree_lemmatizer.gold_labels->cast(Floats2d, gold_labels)
A:spacy.pipeline.edit_tree_lemmatizer.path->util.ensure_path(path)
A:spacy.pipeline.edit_tree_lemmatizer.self.cfg['labels']->list(labels['labels'])
A:spacy.pipeline.edit_tree_lemmatizer.errors->validate_edit_tree(tree)
A:spacy.pipeline.edit_tree_lemmatizer.tree->dict(tree)
A:spacy.pipeline.edit_tree_lemmatizer.tree['orig']->self.vocab.strings.add(tree['orig'])
A:spacy.pipeline.edit_tree_lemmatizer.tree['subst']->self.vocab.strings.add(tree['subst'])
A:spacy.pipeline.edit_tree_lemmatizer.vocab->Vocab()
A:spacy.pipeline.edit_tree_lemmatizer.trees->EditTrees(vocab.strings)
A:spacy.pipeline.edit_tree_lemmatizer.self.tree2label[tree_id]->len(self.cfg['labels'])
spacy.pipeline.EditTreeLemmatizer(self,vocab:Vocab,model:Model,name:str='trainable_lemmatizer',*,backoff:Optional[str]='orth',min_tree_freq:int=3,overwrite:bool=False,top_k:int=1,scorer:Optional[Callable]=lemmatizer_score)
spacy.pipeline.EditTreeLemmatizer._add_labels(self,labels:Dict)
spacy.pipeline.EditTreeLemmatizer._labels_from_data(self,get_examples:Callable[[],Iterable[Example]])
spacy.pipeline.EditTreeLemmatizer._pair2label(self,form,lemma,add_label=False)
spacy.pipeline.EditTreeLemmatizer._scores2guesses_top_k_equals_1(self,docs,scores)
spacy.pipeline.EditTreeLemmatizer._scores2guesses_top_k_greater_1(self,docs,scores)
spacy.pipeline.EditTreeLemmatizer._scores2guesses_top_k_guardrail(self,docs,scores)
spacy.pipeline.EditTreeLemmatizer.from_bytes(self,bytes_data,*,exclude=tuple())
spacy.pipeline.EditTreeLemmatizer.from_disk(self,path,exclude=tuple())
spacy.pipeline.EditTreeLemmatizer.get_loss(self,examples:Iterable[Example],scores:List[Floats2d])->Tuple[float, List[Floats2d]]
spacy.pipeline.EditTreeLemmatizer.hide_labels(self)->bool
spacy.pipeline.EditTreeLemmatizer.initialize(self,get_examples:Callable[[],Iterable[Example]],*,nlp:Optional[Language]=None,labels:Optional[Dict]=None)
spacy.pipeline.EditTreeLemmatizer.label_data(self)->Dict
spacy.pipeline.EditTreeLemmatizer.labels(self)->Tuple[int, ...]
spacy.pipeline.EditTreeLemmatizer.predict(self,docs:Iterable[Doc])->List[Ints2d]
spacy.pipeline.EditTreeLemmatizer.set_annotations(self,docs:Iterable[Doc],batch_tree_ids)
spacy.pipeline.EditTreeLemmatizer.to_bytes(self,*,exclude=tuple())
spacy.pipeline.EditTreeLemmatizer.to_disk(self,path,exclude=tuple())
spacy.pipeline.edit_tree_lemmatizer.EditTreeLemmatizer(self,vocab:Vocab,model:Model,name:str='trainable_lemmatizer',*,backoff:Optional[str]='orth',min_tree_freq:int=3,overwrite:bool=False,top_k:int=1,scorer:Optional[Callable]=lemmatizer_score)
spacy.pipeline.edit_tree_lemmatizer.EditTreeLemmatizer.__init__(self,vocab:Vocab,model:Model,name:str='trainable_lemmatizer',*,backoff:Optional[str]='orth',min_tree_freq:int=3,overwrite:bool=False,top_k:int=1,scorer:Optional[Callable]=lemmatizer_score)
spacy.pipeline.edit_tree_lemmatizer.EditTreeLemmatizer._add_labels(self,labels:Dict)
spacy.pipeline.edit_tree_lemmatizer.EditTreeLemmatizer._labels_from_data(self,get_examples:Callable[[],Iterable[Example]])
spacy.pipeline.edit_tree_lemmatizer.EditTreeLemmatizer._pair2label(self,form,lemma,add_label=False)
spacy.pipeline.edit_tree_lemmatizer.EditTreeLemmatizer._scores2guesses_top_k_equals_1(self,docs,scores)
spacy.pipeline.edit_tree_lemmatizer.EditTreeLemmatizer._scores2guesses_top_k_greater_1(self,docs,scores)
spacy.pipeline.edit_tree_lemmatizer.EditTreeLemmatizer._scores2guesses_top_k_guardrail(self,docs,scores)
spacy.pipeline.edit_tree_lemmatizer.EditTreeLemmatizer.from_bytes(self,bytes_data,*,exclude=tuple())
spacy.pipeline.edit_tree_lemmatizer.EditTreeLemmatizer.from_disk(self,path,exclude=tuple())
spacy.pipeline.edit_tree_lemmatizer.EditTreeLemmatizer.get_loss(self,examples:Iterable[Example],scores:List[Floats2d])->Tuple[float, List[Floats2d]]
spacy.pipeline.edit_tree_lemmatizer.EditTreeLemmatizer.hide_labels(self)->bool
spacy.pipeline.edit_tree_lemmatizer.EditTreeLemmatizer.initialize(self,get_examples:Callable[[],Iterable[Example]],*,nlp:Optional[Language]=None,labels:Optional[Dict]=None)
spacy.pipeline.edit_tree_lemmatizer.EditTreeLemmatizer.label_data(self)->Dict
spacy.pipeline.edit_tree_lemmatizer.EditTreeLemmatizer.labels(self)->Tuple[int, ...]
spacy.pipeline.edit_tree_lemmatizer.EditTreeLemmatizer.predict(self,docs:Iterable[Doc])->List[Ints2d]
spacy.pipeline.edit_tree_lemmatizer.EditTreeLemmatizer.set_annotations(self,docs:Iterable[Doc],batch_tree_ids)
spacy.pipeline.edit_tree_lemmatizer.EditTreeLemmatizer.to_bytes(self,*,exclude=tuple())
spacy.pipeline.edit_tree_lemmatizer.EditTreeLemmatizer.to_disk(self,path,exclude=tuple())
spacy.pipeline.edit_tree_lemmatizer.make_edit_tree_lemmatizer(nlp:Language,name:str,model:Model,backoff:Optional[str],min_tree_freq:int,overwrite:bool,top_k:int,scorer:Optional[Callable])


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/pipeline/entityruler.py----------------------------------------
A:spacy.pipeline.entityruler.self.token_patterns->defaultdict(list, {label: val for (label, val) in self.token_patterns.items() if label not in created_labels})
A:spacy.pipeline.entityruler.self.phrase_patterns->defaultdict(list, {label: val for (label, val) in self.phrase_patterns.items() if label not in created_labels})
A:spacy.pipeline.entityruler.self.matcher->Matcher(self.nlp.vocab, validate=self._validate, fuzzy_compare=self.matcher_fuzzy_compare)
A:spacy.pipeline.entityruler.self.phrase_matcher->PhraseMatcher(self.nlp.vocab, attr=self.phrase_matcher_attr)
A:spacy.pipeline.entityruler.self._ent_ids->defaultdict(tuple)
A:spacy.pipeline.entityruler.n_token_patterns->sum((len(p) for p in self.token_patterns.values()))
A:spacy.pipeline.entityruler.n_phrase_patterns->sum((len(p) for p in self.phrase_patterns.values()))
A:spacy.pipeline.entityruler.error_handler->self.get_error_handler()
A:spacy.pipeline.entityruler.matches->self.match(doc)
A:spacy.pipeline.entityruler.final_matches->sorted(final_matches, key=get_sort_key, reverse=True)
A:spacy.pipeline.entityruler.entities->list(doc.ents)
A:spacy.pipeline.entityruler.seen_tokens->set()
A:spacy.pipeline.entityruler.span->Span(doc, start, end, label=match_id)
A:spacy.pipeline.entityruler.keys->set(self.token_patterns.keys())
A:spacy.pipeline.entityruler.all_labels->set()
A:spacy.pipeline.entityruler.(label, _)->self._split_label(l)
A:spacy.pipeline.entityruler.all_ent_ids->set()
A:spacy.pipeline.entityruler.(_, ent_id)->self._split_label(l)
A:spacy.pipeline.entityruler.(ent_label, ent_id)->self._create_label(label, entry['id']).rsplit(self.ent_id_sep, 1)
A:spacy.pipeline.entityruler.label->self._create_label(label, entry['id'])
A:spacy.pipeline.entityruler.key->self.matcher._normalize_key(label)
A:spacy.pipeline.entityruler.cfg->srsly.msgpack_loads(patterns_bytes)
A:spacy.pipeline.entityruler.self.overwrite->srsly.msgpack_loads(patterns_bytes).get('overwrite', False)
A:spacy.pipeline.entityruler.self.phrase_matcher_attr->srsly.msgpack_loads(patterns_bytes).get('phrase_matcher_attr')
A:spacy.pipeline.entityruler.self.ent_id_sep->srsly.msgpack_loads(patterns_bytes).get('ent_id_sep', DEFAULT_ENT_ID_SEP)
A:spacy.pipeline.entityruler.path->ensure_path(path)
A:spacy.pipeline.entityruler.depr_patterns_path->ensure_path(path).with_suffix('.jsonl')
A:spacy.pipeline.entityruler.patterns->srsly.read_jsonl(depr_patterns_path)
spacy.pipeline.EntityRuler(self,nlp:Language,name:str='entity_ruler',*,phrase_matcher_attr:Optional[Union[int,str]]=None,matcher_fuzzy_compare:Callable=levenshtein_compare,validate:bool=False,overwrite_ents:bool=False,ent_id_sep:str=DEFAULT_ENT_ID_SEP,patterns:Optional[List[PatternType]]=None,scorer:Optional[Callable]=entity_ruler_score)
spacy.pipeline.EntityRuler.__contains__(self,label:str)->bool
spacy.pipeline.EntityRuler.__len__(self)->int
spacy.pipeline.EntityRuler._create_label(self,label:Any,ent_id:Any)->str
spacy.pipeline.EntityRuler._require_patterns(self)->None
spacy.pipeline.EntityRuler._split_label(self,label:str)->Tuple[str, Optional[str]]
spacy.pipeline.EntityRuler.add_patterns(self,patterns:List[PatternType])->None
spacy.pipeline.EntityRuler.clear(self)->None
spacy.pipeline.EntityRuler.ent_ids(self)->Tuple[Optional[str], ...]
spacy.pipeline.EntityRuler.from_bytes(self,patterns_bytes:bytes,*,exclude:Iterable[str]=SimpleFrozenList())->'EntityRuler'
spacy.pipeline.EntityRuler.from_disk(self,path:Union[str,Path],*,exclude:Iterable[str]=SimpleFrozenList())->'EntityRuler'
spacy.pipeline.EntityRuler.initialize(self,get_examples:Callable[[],Iterable[Example]],*,nlp:Optional[Language]=None,patterns:Optional[Sequence[PatternType]]=None)
spacy.pipeline.EntityRuler.labels(self)->Tuple[str, ...]
spacy.pipeline.EntityRuler.match(self,doc:Doc)
spacy.pipeline.EntityRuler.patterns(self)->List[PatternType]
spacy.pipeline.EntityRuler.remove(self,ent_id:str)->None
spacy.pipeline.EntityRuler.set_annotations(self,doc,matches)
spacy.pipeline.EntityRuler.to_bytes(self,*,exclude:Iterable[str]=SimpleFrozenList())->bytes
spacy.pipeline.EntityRuler.to_disk(self,path:Union[str,Path],*,exclude:Iterable[str]=SimpleFrozenList())->None
spacy.pipeline.entityruler.EntityRuler(self,nlp:Language,name:str='entity_ruler',*,phrase_matcher_attr:Optional[Union[int,str]]=None,matcher_fuzzy_compare:Callable=levenshtein_compare,validate:bool=False,overwrite_ents:bool=False,ent_id_sep:str=DEFAULT_ENT_ID_SEP,patterns:Optional[List[PatternType]]=None,scorer:Optional[Callable]=entity_ruler_score)
spacy.pipeline.entityruler.EntityRuler.__contains__(self,label:str)->bool
spacy.pipeline.entityruler.EntityRuler.__init__(self,nlp:Language,name:str='entity_ruler',*,phrase_matcher_attr:Optional[Union[int,str]]=None,matcher_fuzzy_compare:Callable=levenshtein_compare,validate:bool=False,overwrite_ents:bool=False,ent_id_sep:str=DEFAULT_ENT_ID_SEP,patterns:Optional[List[PatternType]]=None,scorer:Optional[Callable]=entity_ruler_score)
spacy.pipeline.entityruler.EntityRuler.__len__(self)->int
spacy.pipeline.entityruler.EntityRuler._create_label(self,label:Any,ent_id:Any)->str
spacy.pipeline.entityruler.EntityRuler._require_patterns(self)->None
spacy.pipeline.entityruler.EntityRuler._split_label(self,label:str)->Tuple[str, Optional[str]]
spacy.pipeline.entityruler.EntityRuler.add_patterns(self,patterns:List[PatternType])->None
spacy.pipeline.entityruler.EntityRuler.clear(self)->None
spacy.pipeline.entityruler.EntityRuler.ent_ids(self)->Tuple[Optional[str], ...]
spacy.pipeline.entityruler.EntityRuler.from_bytes(self,patterns_bytes:bytes,*,exclude:Iterable[str]=SimpleFrozenList())->'EntityRuler'
spacy.pipeline.entityruler.EntityRuler.from_disk(self,path:Union[str,Path],*,exclude:Iterable[str]=SimpleFrozenList())->'EntityRuler'
spacy.pipeline.entityruler.EntityRuler.initialize(self,get_examples:Callable[[],Iterable[Example]],*,nlp:Optional[Language]=None,patterns:Optional[Sequence[PatternType]]=None)
spacy.pipeline.entityruler.EntityRuler.labels(self)->Tuple[str, ...]
spacy.pipeline.entityruler.EntityRuler.match(self,doc:Doc)
spacy.pipeline.entityruler.EntityRuler.patterns(self)->List[PatternType]
spacy.pipeline.entityruler.EntityRuler.remove(self,ent_id:str)->None
spacy.pipeline.entityruler.EntityRuler.set_annotations(self,doc,matches)
spacy.pipeline.entityruler.EntityRuler.to_bytes(self,*,exclude:Iterable[str]=SimpleFrozenList())->bytes
spacy.pipeline.entityruler.EntityRuler.to_disk(self,path:Union[str,Path],*,exclude:Iterable[str]=SimpleFrozenList())->None
spacy.pipeline.entityruler.entity_ruler_score(examples,**kwargs)
spacy.pipeline.entityruler.make_entity_ruler(nlp:Language,name:str,phrase_matcher_attr:Optional[Union[int,str]],matcher_fuzzy_compare:Callable,validate:bool,overwrite_ents:bool,ent_id_sep:str,scorer:Optional[Callable])
spacy.pipeline.entityruler.make_entity_ruler_scorer()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/pipeline/functions.py----------------------------------------
A:spacy.pipeline.functions.merger->Matcher(doc.vocab)
A:spacy.pipeline.functions.matches->merger(doc)
A:spacy.pipeline.functions.spans->util.filter_spans([doc[start:end + 1] for (_, start, end) in matches])
A:spacy.pipeline.functions.self.min_length->config.get('min_length', 0)
A:spacy.pipeline.functions.self.split_length->config.get('split_length', 0)
A:spacy.pipeline.functions.path->util.ensure_path(path)
A:spacy.pipeline.functions.parts->attr.split('.')
A:spacy.pipeline.functions.obj->getattr(obj, part)
spacy.pipeline.functions.DocCleaner(self,attrs:Dict[str,Any],*,silent:bool=True)
spacy.pipeline.functions.DocCleaner.__init__(self,attrs:Dict[str,Any],*,silent:bool=True)
spacy.pipeline.functions.DocCleaner.from_bytes(self,data,**kwargs)
spacy.pipeline.functions.DocCleaner.from_disk(self,path,**kwargs)
spacy.pipeline.functions.DocCleaner.to_bytes(self,**kwargs)
spacy.pipeline.functions.DocCleaner.to_disk(self,path,**kwargs)
spacy.pipeline.functions.TokenSplitter(self,min_length:int=0,split_length:int=0)
spacy.pipeline.functions.TokenSplitter.__init__(self,min_length:int=0,split_length:int=0)
spacy.pipeline.functions.TokenSplitter._get_config(self)->Dict[str, Any]
spacy.pipeline.functions.TokenSplitter._set_config(self,config:Dict[str,Any]={})->None
spacy.pipeline.functions.TokenSplitter.from_bytes(self,data,**kwargs)
spacy.pipeline.functions.TokenSplitter.from_disk(self,path,**kwargs)
spacy.pipeline.functions.TokenSplitter.to_bytes(self,**kwargs)
spacy.pipeline.functions.TokenSplitter.to_disk(self,path,**kwargs)
spacy.pipeline.functions.make_doc_cleaner(nlp:Language,name:str,*,attrs:Dict[str,Any],silent:bool)
spacy.pipeline.functions.make_token_splitter(nlp:Language,name:str,*,min_length:int=0,split_length:int=0)
spacy.pipeline.functions.merge_entities(doc:Doc)
spacy.pipeline.functions.merge_noun_chunks(doc:Doc)->Doc
spacy.pipeline.functions.merge_subtokens(doc:Doc,label:str='subtok')->Doc
spacy.pipeline.merge_entities(doc:Doc)
spacy.pipeline.merge_noun_chunks(doc:Doc)->Doc
spacy.pipeline.merge_subtokens(doc:Doc,label:str='subtok')->Doc


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/pipeline/spancat.py----------------------------------------
A:spacy.pipeline.spancat.ops->get_current_ops()
A:spacy.pipeline.spancat.starts->starts.reshape((-1, 1)).reshape((-1, 1))
A:spacy.pipeline.spancat.lengths_array->cast(Ints1d, ops.asarray(lengths, dtype='i'))
A:spacy.pipeline.spancat.output->Ragged(ops.xp.zeros((0, 0), dtype='i'), lengths_array)
A:spacy.pipeline.spancat.sizes->list(range(min_size, max_size + 1))
A:spacy.pipeline.spancat.kwargs->dict(kwargs)
A:spacy.pipeline.spancat.self.ranges->set()
A:spacy.pipeline.spancat.nO->self.model.get_ref('output_layer').get_dim('nO')
A:spacy.pipeline.spancat.indices->self.model.ops.to_numpy(indices)
A:spacy.pipeline.spancat.scores->self.model.ops.to_numpy(scores)
A:spacy.pipeline.spancat.suggester_output->self.suggester(docs, ops=self.model.ops)
A:spacy.pipeline.spancat.allow_overlap->cast(bool, self.cfg['allow_overlap'])
A:spacy.pipeline.spancat.doc.spans[self.key]->self._make_span_group_multilabel(doc, indices_i, scores[offset:offset + indices.lengths[i]])
A:spacy.pipeline.spancat.spans->SpanGroup(doc, name=self.key)
A:spacy.pipeline.spancat.(scores, backprop_scores)->self.model.begin_update((docs, spans))
A:spacy.pipeline.spancat.(loss, d_scores)->self.get_loss(examples, (spans, scores))
A:spacy.pipeline.spancat.target->self.model.ops.asarray(target, dtype='f')
A:spacy.pipeline.spancat.negative_spans->numpy.ones(scores.shape[0])
A:spacy.pipeline.spancat.start->int(spans_i[j, 0])
A:spacy.pipeline.spancat.end->int(spans_i[j, 1])
A:spacy.pipeline.spancat.neg_weight->cast(float, self.cfg['negative_weight'])
A:spacy.pipeline.spancat.loss->float((d_scores ** 2).sum())
A:spacy.pipeline.spancat.Y->self.model.ops.alloc2f(spans.dataXd.shape[0], self._n_labels)
A:spacy.pipeline.spancat.negative_scores->numpy.copy(scores[:, self._negative_label_i])
A:spacy.pipeline.spancat.ranked->(scores * -1).argsort()
A:spacy.pipeline.spancat.spans.attrs['scores']->numpy.array(attrs_scores)
A:spacy.pipeline.spancat.predicted->self.model.ops.to_numpy(scores).argmax(axis=1)
A:spacy.pipeline.spancat.argmax_scores->numpy.take_along_axis(scores, numpy.expand_dims(predicted, 1), axis=1)
A:spacy.pipeline.spancat.keeps->numpy.logical_and(keeps, (argmax_scores >= threshold).squeeze())
A:spacy.pipeline.spancat.sort_idx->(argmax_scores.squeeze() * -1).argsort()
A:spacy.pipeline.spancat.seen->_Intervals()
spacy.pipeline.SpanCategorizer(self,vocab:Vocab,model:Model[Tuple[List[Doc],Ragged],Floats2d],suggester:Suggester,name:str='spancat',*,add_negative_label:bool=False,spans_key:str='spans',negative_weight:Optional[float]=1.0,allow_overlap:Optional[bool]=True,max_positive:Optional[int]=None,threshold:Optional[float]=0.5,scorer:Optional[Callable]=spancat_score)
spacy.pipeline.SpanCategorizer._allow_extra_label(self)->None
spacy.pipeline.SpanCategorizer._get_aligned_spans(self,eg:Example)
spacy.pipeline.SpanCategorizer._label_map(self)->Dict[str, int]
spacy.pipeline.SpanCategorizer._make_span_group_multilabel(self,doc:Doc,indices:Ints2d,scores:Floats2d)->SpanGroup
spacy.pipeline.SpanCategorizer._make_span_group_singlelabel(self,doc:Doc,indices:Ints2d,scores:Floats2d,allow_overlap:bool=True)->SpanGroup
spacy.pipeline.SpanCategorizer._n_labels(self)->int
spacy.pipeline.SpanCategorizer._negative_label_i(self)->Union[int, None]
spacy.pipeline.SpanCategorizer._validate_categories(self,examples:Iterable[Example])
spacy.pipeline.SpanCategorizer.add_label(self,label:str)->int
spacy.pipeline.SpanCategorizer.get_loss(self,examples:Iterable[Example],spans_scores:Tuple[Ragged,Floats2d])->Tuple[float, float]
spacy.pipeline.SpanCategorizer.initialize(self,get_examples:Callable[[],Iterable[Example]],*,nlp:Optional[Language]=None,labels:Optional[List[str]]=None)->None
spacy.pipeline.SpanCategorizer.key(self)->str
spacy.pipeline.SpanCategorizer.label_data(self)->List[str]
spacy.pipeline.SpanCategorizer.labels(self)->Tuple[str]
spacy.pipeline.SpanCategorizer.predict(self,docs:Iterable[Doc])
spacy.pipeline.SpanCategorizer.set_annotations(self,docs:Iterable[Doc],indices_scores)->None
spacy.pipeline.SpanCategorizer.set_candidates(self,docs:Iterable[Doc],*,candidates_key:str='candidates')->None
spacy.pipeline.SpanCategorizer.update(self,examples:Iterable[Example],*,drop:float=0.0,sgd:Optional[Optimizer]=None,losses:Optional[Dict[str,float]]=None)->Dict[str, float]
spacy.pipeline.spancat.SpanCategorizer(self,vocab:Vocab,model:Model[Tuple[List[Doc],Ragged],Floats2d],suggester:Suggester,name:str='spancat',*,add_negative_label:bool=False,spans_key:str='spans',negative_weight:Optional[float]=1.0,allow_overlap:Optional[bool]=True,max_positive:Optional[int]=None,threshold:Optional[float]=0.5,scorer:Optional[Callable]=spancat_score)
spacy.pipeline.spancat.SpanCategorizer.__init__(self,vocab:Vocab,model:Model[Tuple[List[Doc],Ragged],Floats2d],suggester:Suggester,name:str='spancat',*,add_negative_label:bool=False,spans_key:str='spans',negative_weight:Optional[float]=1.0,allow_overlap:Optional[bool]=True,max_positive:Optional[int]=None,threshold:Optional[float]=0.5,scorer:Optional[Callable]=spancat_score)
spacy.pipeline.spancat.SpanCategorizer._allow_extra_label(self)->None
spacy.pipeline.spancat.SpanCategorizer._get_aligned_spans(self,eg:Example)
spacy.pipeline.spancat.SpanCategorizer._label_map(self)->Dict[str, int]
spacy.pipeline.spancat.SpanCategorizer._make_span_group_multilabel(self,doc:Doc,indices:Ints2d,scores:Floats2d)->SpanGroup
spacy.pipeline.spancat.SpanCategorizer._make_span_group_singlelabel(self,doc:Doc,indices:Ints2d,scores:Floats2d,allow_overlap:bool=True)->SpanGroup
spacy.pipeline.spancat.SpanCategorizer._n_labels(self)->int
spacy.pipeline.spancat.SpanCategorizer._negative_label_i(self)->Union[int, None]
spacy.pipeline.spancat.SpanCategorizer._validate_categories(self,examples:Iterable[Example])
spacy.pipeline.spancat.SpanCategorizer.add_label(self,label:str)->int
spacy.pipeline.spancat.SpanCategorizer.get_loss(self,examples:Iterable[Example],spans_scores:Tuple[Ragged,Floats2d])->Tuple[float, float]
spacy.pipeline.spancat.SpanCategorizer.initialize(self,get_examples:Callable[[],Iterable[Example]],*,nlp:Optional[Language]=None,labels:Optional[List[str]]=None)->None
spacy.pipeline.spancat.SpanCategorizer.key(self)->str
spacy.pipeline.spancat.SpanCategorizer.label_data(self)->List[str]
spacy.pipeline.spancat.SpanCategorizer.labels(self)->Tuple[str]
spacy.pipeline.spancat.SpanCategorizer.predict(self,docs:Iterable[Doc])
spacy.pipeline.spancat.SpanCategorizer.set_annotations(self,docs:Iterable[Doc],indices_scores)->None
spacy.pipeline.spancat.SpanCategorizer.set_candidates(self,docs:Iterable[Doc],*,candidates_key:str='candidates')->None
spacy.pipeline.spancat.SpanCategorizer.update(self,examples:Iterable[Example],*,drop:float=0.0,sgd:Optional[Optimizer]=None,losses:Optional[Dict[str,float]]=None)->Dict[str, float]
spacy.pipeline.spancat.Suggester(self,docs:Iterable[Doc],*,ops:Optional[Ops]=None)
spacy.pipeline.spancat.Suggester.__call__(self,docs:Iterable[Doc],*,ops:Optional[Ops]=None)
spacy.pipeline.spancat._Intervals(self)
spacy.pipeline.spancat._Intervals.__contains__(self,rang)
spacy.pipeline.spancat._Intervals.__init__(self)
spacy.pipeline.spancat._Intervals.add(self,i,j)
spacy.pipeline.spancat.build_ngram_range_suggester(min_size:int,max_size:int)->Suggester
spacy.pipeline.spancat.build_ngram_suggester(sizes:List[int])->Suggester
spacy.pipeline.spancat.build_preset_spans_suggester(spans_key:str)->Suggester
spacy.pipeline.spancat.make_spancat(nlp:Language,name:str,suggester:Suggester,model:Model[Tuple[List[Doc],Ragged],Floats2d],spans_key:str,scorer:Optional[Callable],threshold:float,max_positive:Optional[int])->'SpanCategorizer'
spacy.pipeline.spancat.make_spancat_scorer()
spacy.pipeline.spancat.make_spancat_singlelabel(nlp:Language,name:str,suggester:Suggester,model:Model[Tuple[List[Doc],Ragged],Floats2d],spans_key:str,negative_weight:float,allow_overlap:bool,scorer:Optional[Callable])->'SpanCategorizer'
spacy.pipeline.spancat.ngram_suggester(docs:Iterable[Doc],sizes:List[int],*,ops:Optional[Ops]=None)->Ragged
spacy.pipeline.spancat.preset_spans_suggester(docs:Iterable[Doc],spans_key:str,*,ops:Optional[Ops]=None)->Ragged
spacy.pipeline.spancat.spancat_score(examples:Iterable[Example],**kwargs)->Dict[str, Any]


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/pipeline/textcat_multilabel.py----------------------------------------
A:spacy.pipeline.textcat_multilabel.self.cfg->dict(cfg)
A:spacy.pipeline.textcat_multilabel.subbatch->list(islice(get_examples(), 10))
A:spacy.pipeline.textcat_multilabel.(label_sample, _)->self._examples_to_truth(subbatch)
spacy.pipeline.MultiLabel_TextCategorizer(self,vocab:Vocab,model:Model,name:str='textcat_multilabel',*,threshold:float,scorer:Optional[Callable]=textcat_multilabel_score)
spacy.pipeline.MultiLabel_TextCategorizer._validate_categories(self,examples:Iterable[Example])
spacy.pipeline.MultiLabel_TextCategorizer.initialize(self,get_examples:Callable[[],Iterable[Example]],*,nlp:Optional[Language]=None,labels:Optional[Iterable[str]]=None)
spacy.pipeline.MultiLabel_TextCategorizer.support_missing_values(self)
spacy.pipeline.textcat_multilabel.MultiLabel_TextCategorizer(self,vocab:Vocab,model:Model,name:str='textcat_multilabel',*,threshold:float,scorer:Optional[Callable]=textcat_multilabel_score)
spacy.pipeline.textcat_multilabel.MultiLabel_TextCategorizer.__init__(self,vocab:Vocab,model:Model,name:str='textcat_multilabel',*,threshold:float,scorer:Optional[Callable]=textcat_multilabel_score)
spacy.pipeline.textcat_multilabel.MultiLabel_TextCategorizer._validate_categories(self,examples:Iterable[Example])
spacy.pipeline.textcat_multilabel.MultiLabel_TextCategorizer.initialize(self,get_examples:Callable[[],Iterable[Example]],*,nlp:Optional[Language]=None,labels:Optional[Iterable[str]]=None)
spacy.pipeline.textcat_multilabel.MultiLabel_TextCategorizer.support_missing_values(self)
spacy.pipeline.textcat_multilabel.make_multilabel_textcat(nlp:Language,name:str,model:Model[List[Doc],List[Floats2d]],threshold:float,scorer:Optional[Callable])->'MultiLabel_TextCategorizer'
spacy.pipeline.textcat_multilabel.make_textcat_multilabel_scorer()
spacy.pipeline.textcat_multilabel.textcat_multilabel_score(examples:Iterable[Example],**kwargs)->Dict[str, Any]


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/pipeline/span_finder.py----------------------------------------
A:spacy.pipeline.span_finder.kwargs->dict(kwargs)
A:spacy.pipeline.span_finder.scores->self.model.predict(docs)
A:spacy.pipeline.span_finder.(scores, backprop_scores)->self.model.begin_update(predicted)
A:spacy.pipeline.span_finder.(loss, d_scores)->self.get_loss(examples, scores)
A:spacy.pipeline.span_finder.(truths, masks)->self._get_aligned_truth_scores(examples, self.model.ops)
A:spacy.pipeline.span_finder.loss->float((d_scores ** 2).sum())
A:spacy.pipeline.span_finder.n_tokens->len(eg.predicted)
A:spacy.pipeline.span_finder.truth->ops.xp.zeros((n_tokens, 2), dtype='float32')
A:spacy.pipeline.span_finder.mask->ops.xp.ones((n_tokens, 2), dtype='float32')
A:spacy.pipeline.span_finder.(ref_start_char, ref_end_char)->_char_indices(span)
A:spacy.pipeline.span_finder.pred_span->eg.predicted.char_span(ref_start_char, ref_end_char, alignment_mode='expand')
A:spacy.pipeline.span_finder.(pred_start_char, pred_end_char)->_char_indices(pred_span)
A:spacy.pipeline.span_finder.truths->ops.xp.concatenate(truths, axis=0)
A:spacy.pipeline.span_finder.masks->ops.xp.concatenate(masks, axis=0)
A:spacy.pipeline.span_finder.(Y, _)->self._get_aligned_truth_scores(subbatch, self.model.ops)
spacy.pipeline.SpanFinder(self,nlp:Language,model:Model[Iterable[Doc],Floats2d],name:str='span_finder',*,spans_key:str=DEFAULT_SPANS_KEY,threshold:float=0.5,max_length:Optional[int]=None,min_length:Optional[int]=None,scorer:Optional[Callable]=span_finder_score)
spacy.pipeline.SpanFinder._get_aligned_truth_scores(self,examples,ops)->Tuple[Floats2d, Floats2d]
spacy.pipeline.SpanFinder.get_loss(self,examples,scores)->Tuple[float, Floats2d]
spacy.pipeline.SpanFinder.initialize(self,get_examples:Callable[[],Iterable[Example]],*,nlp:Optional[Language]=None)->None
spacy.pipeline.SpanFinder.predict(self,docs:Iterable[Doc])
spacy.pipeline.SpanFinder.set_annotations(self,docs:Iterable[Doc],scores:Floats2d)->None
spacy.pipeline.SpanFinder.update(self,examples:Iterable[Example],*,drop:float=0.0,sgd:Optional[Optimizer]=None,losses:Optional[Dict[str,float]]=None)->Dict[str, float]
spacy.pipeline.span_finder.SpanFinder(self,nlp:Language,model:Model[Iterable[Doc],Floats2d],name:str='span_finder',*,spans_key:str=DEFAULT_SPANS_KEY,threshold:float=0.5,max_length:Optional[int]=None,min_length:Optional[int]=None,scorer:Optional[Callable]=span_finder_score)
spacy.pipeline.span_finder.SpanFinder.__init__(self,nlp:Language,model:Model[Iterable[Doc],Floats2d],name:str='span_finder',*,spans_key:str=DEFAULT_SPANS_KEY,threshold:float=0.5,max_length:Optional[int]=None,min_length:Optional[int]=None,scorer:Optional[Callable]=span_finder_score)
spacy.pipeline.span_finder.SpanFinder._get_aligned_truth_scores(self,examples,ops)->Tuple[Floats2d, Floats2d]
spacy.pipeline.span_finder.SpanFinder.get_loss(self,examples,scores)->Tuple[float, Floats2d]
spacy.pipeline.span_finder.SpanFinder.initialize(self,get_examples:Callable[[],Iterable[Example]],*,nlp:Optional[Language]=None)->None
spacy.pipeline.span_finder.SpanFinder.predict(self,docs:Iterable[Doc])
spacy.pipeline.span_finder.SpanFinder.set_annotations(self,docs:Iterable[Doc],scores:Floats2d)->None
spacy.pipeline.span_finder.SpanFinder.update(self,examples:Iterable[Example],*,drop:float=0.0,sgd:Optional[Optimizer]=None,losses:Optional[Dict[str,float]]=None)->Dict[str, float]
spacy.pipeline.span_finder._char_indices(span:Span)->Tuple[int, int]
spacy.pipeline.span_finder.make_span_finder(nlp:Language,name:str,model:Model[Iterable[Doc],Floats2d],spans_key:str,threshold:float,max_length:Optional[int],min_length:Optional[int],scorer:Optional[Callable])->'SpanFinder'
spacy.pipeline.span_finder.make_span_finder_scorer()
spacy.pipeline.span_finder.span_finder_score(examples:Iterable[Example],**kwargs)->Dict[str, Any]


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/pipeline/tok2vec.py----------------------------------------
A:spacy.pipeline.tok2vec.width->model.get_dim('nO')
A:spacy.pipeline.tok2vec.tokvecs->self.model.predict(docs)
A:spacy.pipeline.tok2vec.(tokvecs, bp_tokvecs)->self.model.begin_update(docs)
A:spacy.pipeline.tok2vec.d_docs->bp_tokvecs(d_tokvecs)
A:spacy.pipeline.tok2vec.batch_id->self.get_batch_id(inputs)
spacy.pipeline.Tok2Vec(self,vocab:Vocab,model:Model,name:str='tok2vec')
spacy.pipeline.Tok2Vec.add_label(self,label)
spacy.pipeline.Tok2Vec.add_listener(self,listener:'Tok2VecListener',component_name:str)->None
spacy.pipeline.Tok2Vec.find_listeners(self,component)->None
spacy.pipeline.Tok2Vec.get_loss(self,examples,scores)->None
spacy.pipeline.Tok2Vec.initialize(self,get_examples:Callable[[],Iterable[Example]],*,nlp:Optional[Language]=None)
spacy.pipeline.Tok2Vec.listeners(self)->List['Tok2VecListener']
spacy.pipeline.Tok2Vec.listening_components(self)->List[str]
spacy.pipeline.Tok2Vec.predict(self,docs:Iterable[Doc])
spacy.pipeline.Tok2Vec.remove_listener(self,listener:'Tok2VecListener',component_name:str)->bool
spacy.pipeline.Tok2Vec.set_annotations(self,docs:Sequence[Doc],tokvecses)->None
spacy.pipeline.Tok2Vec.update(self,examples:Iterable[Example],*,drop:float=0.0,sgd:Optional[Optimizer]=None,losses:Optional[Dict[str,float]]=None)
spacy.pipeline.Tok2VecListener(self,upstream_name:str,width:int)
spacy.pipeline.Tok2VecListener.get_batch_id(cls,inputs:Iterable[Doc])->int
spacy.pipeline.Tok2VecListener.receive(self,batch_id:int,outputs,backprop)->None
spacy.pipeline.Tok2VecListener.verify_inputs(self,inputs)->bool
spacy.pipeline.tok2vec.Tok2Vec(self,vocab:Vocab,model:Model,name:str='tok2vec')
spacy.pipeline.tok2vec.Tok2Vec.__init__(self,vocab:Vocab,model:Model,name:str='tok2vec')
spacy.pipeline.tok2vec.Tok2Vec.add_label(self,label)
spacy.pipeline.tok2vec.Tok2Vec.add_listener(self,listener:'Tok2VecListener',component_name:str)->None
spacy.pipeline.tok2vec.Tok2Vec.find_listeners(self,component)->None
spacy.pipeline.tok2vec.Tok2Vec.get_loss(self,examples,scores)->None
spacy.pipeline.tok2vec.Tok2Vec.initialize(self,get_examples:Callable[[],Iterable[Example]],*,nlp:Optional[Language]=None)
spacy.pipeline.tok2vec.Tok2Vec.listeners(self)->List['Tok2VecListener']
spacy.pipeline.tok2vec.Tok2Vec.listening_components(self)->List[str]
spacy.pipeline.tok2vec.Tok2Vec.predict(self,docs:Iterable[Doc])
spacy.pipeline.tok2vec.Tok2Vec.remove_listener(self,listener:'Tok2VecListener',component_name:str)->bool
spacy.pipeline.tok2vec.Tok2Vec.set_annotations(self,docs:Sequence[Doc],tokvecses)->None
spacy.pipeline.tok2vec.Tok2Vec.update(self,examples:Iterable[Example],*,drop:float=0.0,sgd:Optional[Optimizer]=None,losses:Optional[Dict[str,float]]=None)
spacy.pipeline.tok2vec.Tok2VecListener(self,upstream_name:str,width:int)
spacy.pipeline.tok2vec.Tok2VecListener.__init__(self,upstream_name:str,width:int)
spacy.pipeline.tok2vec.Tok2VecListener.get_batch_id(cls,inputs:Iterable[Doc])->int
spacy.pipeline.tok2vec.Tok2VecListener.receive(self,batch_id:int,outputs,backprop)->None
spacy.pipeline.tok2vec.Tok2VecListener.verify_inputs(self,inputs)->bool
spacy.pipeline.tok2vec._empty_backprop(dX)
spacy.pipeline.tok2vec.forward(model:Tok2VecListener,inputs,is_train:bool)
spacy.pipeline.tok2vec.make_tok2vec(nlp:Language,name:str,model:Model)->'Tok2Vec'


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/pipeline/textcat.py----------------------------------------
A:spacy.pipeline.textcat.self.cfg->dict(cfg)
A:spacy.pipeline.textcat.scores->self.model.ops.asarray(scores)
A:spacy.pipeline.textcat.doc.cats[label]->float(scores[i, j])
A:spacy.pipeline.textcat.(scores, bp_scores)->self.model.begin_update(docs)
A:spacy.pipeline.textcat.(loss, d_scores)->self.get_loss(examples, scores)
A:spacy.pipeline.textcat.(target, _)->self._rehearsal_model.begin_update(docs)
A:spacy.pipeline.textcat.nr_examples->len(list(examples))
A:spacy.pipeline.textcat.truths->self.model.ops.asarray(truths)
A:spacy.pipeline.textcat.not_missing->self.model.ops.asarray(not_missing)
A:spacy.pipeline.textcat.(truths, not_missing)->self._examples_to_truth(examples)
A:spacy.pipeline.textcat.mean_square_error->(d_scores ** 2).mean()
A:spacy.pipeline.textcat.self.model->self.model.attrs['resize_output'](self.model, len(self.labels))
A:spacy.pipeline.textcat.err->errors.Errors.E919.format(pos_label=positive_label, labels=self.labels)
A:spacy.pipeline.textcat.subbatch->list(islice(get_examples(), 10))
A:spacy.pipeline.textcat.(label_sample, _)->self._examples_to_truth(subbatch)
A:spacy.pipeline.textcat.vals->list(ex.reference.cats.values())
spacy.pipeline.TextCategorizer(self,vocab:Vocab,model:Model,name:str='textcat',*,threshold:float,scorer:Optional[Callable]=textcat_score)
spacy.pipeline.TextCategorizer._examples_to_truth(self,examples:Iterable[Example])->Tuple[numpy.ndarray, numpy.ndarray]
spacy.pipeline.TextCategorizer._validate_categories(self,examples:Iterable[Example])
spacy.pipeline.TextCategorizer.add_label(self,label:str)->int
spacy.pipeline.TextCategorizer.get_loss(self,examples:Iterable[Example],scores)->Tuple[float, float]
spacy.pipeline.TextCategorizer.initialize(self,get_examples:Callable[[],Iterable[Example]],*,nlp:Optional[Language]=None,labels:Optional[Iterable[str]]=None,positive_label:Optional[str]=None)->None
spacy.pipeline.TextCategorizer.label_data(self)->List[str]
spacy.pipeline.TextCategorizer.labels(self)->Tuple[str]
spacy.pipeline.TextCategorizer.predict(self,docs:Iterable[Doc])
spacy.pipeline.TextCategorizer.rehearse(self,examples:Iterable[Example],*,drop:float=0.0,sgd:Optional[Optimizer]=None,losses:Optional[Dict[str,float]]=None)->Dict[str, float]
spacy.pipeline.TextCategorizer.set_annotations(self,docs:Iterable[Doc],scores)->None
spacy.pipeline.TextCategorizer.support_missing_values(self)
spacy.pipeline.TextCategorizer.update(self,examples:Iterable[Example],*,drop:float=0.0,sgd:Optional[Optimizer]=None,losses:Optional[Dict[str,float]]=None)->Dict[str, float]
spacy.pipeline.textcat.TextCategorizer(self,vocab:Vocab,model:Model,name:str='textcat',*,threshold:float,scorer:Optional[Callable]=textcat_score)
spacy.pipeline.textcat.TextCategorizer.__init__(self,vocab:Vocab,model:Model,name:str='textcat',*,threshold:float,scorer:Optional[Callable]=textcat_score)
spacy.pipeline.textcat.TextCategorizer._examples_to_truth(self,examples:Iterable[Example])->Tuple[numpy.ndarray, numpy.ndarray]
spacy.pipeline.textcat.TextCategorizer._validate_categories(self,examples:Iterable[Example])
spacy.pipeline.textcat.TextCategorizer.add_label(self,label:str)->int
spacy.pipeline.textcat.TextCategorizer.get_loss(self,examples:Iterable[Example],scores)->Tuple[float, float]
spacy.pipeline.textcat.TextCategorizer.initialize(self,get_examples:Callable[[],Iterable[Example]],*,nlp:Optional[Language]=None,labels:Optional[Iterable[str]]=None,positive_label:Optional[str]=None)->None
spacy.pipeline.textcat.TextCategorizer.label_data(self)->List[str]
spacy.pipeline.textcat.TextCategorizer.labels(self)->Tuple[str]
spacy.pipeline.textcat.TextCategorizer.predict(self,docs:Iterable[Doc])
spacy.pipeline.textcat.TextCategorizer.rehearse(self,examples:Iterable[Example],*,drop:float=0.0,sgd:Optional[Optimizer]=None,losses:Optional[Dict[str,float]]=None)->Dict[str, float]
spacy.pipeline.textcat.TextCategorizer.set_annotations(self,docs:Iterable[Doc],scores)->None
spacy.pipeline.textcat.TextCategorizer.support_missing_values(self)
spacy.pipeline.textcat.TextCategorizer.update(self,examples:Iterable[Example],*,drop:float=0.0,sgd:Optional[Optimizer]=None,losses:Optional[Dict[str,float]]=None)->Dict[str, float]
spacy.pipeline.textcat.make_textcat(nlp:Language,name:str,model:Model[List[Doc],List[Floats2d]],threshold:float,scorer:Optional[Callable])->'TextCategorizer'
spacy.pipeline.textcat.make_textcat_scorer()
spacy.pipeline.textcat.textcat_score(examples:Iterable[Example],**kwargs)->Dict[str, Any]


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/pipeline/span_ruler.py----------------------------------------
A:spacy.pipeline.span_ruler.spans->self.ents_filter(spans, matches)
A:spacy.pipeline.span_ruler.entities->list(entities)
A:spacy.pipeline.span_ruler.kwargs->dict(kwargs)
A:spacy.pipeline.span_ruler.error_handler->self.get_error_handler()
A:spacy.pipeline.span_ruler.matches->cast(List[Tuple[int, int, int]], list(self.matcher(doc)) + list(self.phrase_matcher(doc)))
A:spacy.pipeline.span_ruler.deduplicated_matches->set((Span(doc, start, end, label=self._match_label_id_map[m_id]['label'], span_id=self._match_label_id_map[m_id]['id']) for (m_id, start, end) in matches if start != end))
A:spacy.pipeline.span_ruler.doc.ents->sorted(spans)
A:spacy.pipeline.span_ruler.p_label->cast(str, entry['label'])
A:spacy.pipeline.span_ruler.p_id->cast(str, entry.get('id', ''))
A:spacy.pipeline.span_ruler.label->repr((p_label, p_id))
A:spacy.pipeline.span_ruler.m_label_str->self.nlp.vocab.strings.as_string(m_label)
A:spacy.pipeline.span_ruler.orig_len->len(self)
A:spacy.pipeline.span_ruler.path->ensure_path(path)
spacy.pipeline.SpanRuler(self,nlp:Language,name:str='span_ruler',*,spans_key:Optional[str]=DEFAULT_SPANS_KEY,spans_filter:Optional[Callable[[Iterable[Span],Iterable[Span]],Iterable[Span]]]=None,annotate_ents:bool=False,ents_filter:Callable[[Iterable[Span],Iterable[Span]],Iterable[Span]]=util.filter_chain_spans,phrase_matcher_attr:Optional[Union[int,str]]=None,matcher_fuzzy_compare:Callable=levenshtein_compare,validate:bool=False,overwrite:bool=False,scorer:Optional[Callable]=partial(overlapping_labeled_spans_score,spans_key=DEFAULT_SPANS_KEY))
spacy.pipeline.SpanRuler.__contains__(self,label:str)->bool
spacy.pipeline.SpanRuler.__len__(self)->int
spacy.pipeline.SpanRuler._require_patterns(self)->None
spacy.pipeline.SpanRuler.add_patterns(self,patterns:List[PatternType])->None
spacy.pipeline.SpanRuler.clear(self)->None
spacy.pipeline.SpanRuler.from_bytes(self,bytes_data:bytes,*,exclude:Iterable[str]=SimpleFrozenList())->'SpanRuler'
spacy.pipeline.SpanRuler.from_disk(self,path:Union[str,Path],*,exclude:Iterable[str]=SimpleFrozenList())->'SpanRuler'
spacy.pipeline.SpanRuler.ids(self)->Tuple[str, ...]
spacy.pipeline.SpanRuler.initialize(self,get_examples:Callable[[],Iterable[Example]],*,nlp:Optional[Language]=None,patterns:Optional[Sequence[PatternType]]=None)
spacy.pipeline.SpanRuler.key(self)->Optional[str]
spacy.pipeline.SpanRuler.labels(self)->Tuple[str, ...]
spacy.pipeline.SpanRuler.match(self,doc:Doc)
spacy.pipeline.SpanRuler.patterns(self)->List[PatternType]
spacy.pipeline.SpanRuler.remove(self,label:str)->None
spacy.pipeline.SpanRuler.remove_by_id(self,pattern_id:str)->None
spacy.pipeline.SpanRuler.set_annotations(self,doc,matches)
spacy.pipeline.SpanRuler.to_bytes(self,*,exclude:Iterable[str]=SimpleFrozenList())->bytes
spacy.pipeline.SpanRuler.to_disk(self,path:Union[str,Path],*,exclude:Iterable[str]=SimpleFrozenList())->None
spacy.pipeline.span_ruler.SpanRuler(self,nlp:Language,name:str='span_ruler',*,spans_key:Optional[str]=DEFAULT_SPANS_KEY,spans_filter:Optional[Callable[[Iterable[Span],Iterable[Span]],Iterable[Span]]]=None,annotate_ents:bool=False,ents_filter:Callable[[Iterable[Span],Iterable[Span]],Iterable[Span]]=util.filter_chain_spans,phrase_matcher_attr:Optional[Union[int,str]]=None,matcher_fuzzy_compare:Callable=levenshtein_compare,validate:bool=False,overwrite:bool=False,scorer:Optional[Callable]=partial(overlapping_labeled_spans_score,spans_key=DEFAULT_SPANS_KEY))
spacy.pipeline.span_ruler.SpanRuler.__contains__(self,label:str)->bool
spacy.pipeline.span_ruler.SpanRuler.__init__(self,nlp:Language,name:str='span_ruler',*,spans_key:Optional[str]=DEFAULT_SPANS_KEY,spans_filter:Optional[Callable[[Iterable[Span],Iterable[Span]],Iterable[Span]]]=None,annotate_ents:bool=False,ents_filter:Callable[[Iterable[Span],Iterable[Span]],Iterable[Span]]=util.filter_chain_spans,phrase_matcher_attr:Optional[Union[int,str]]=None,matcher_fuzzy_compare:Callable=levenshtein_compare,validate:bool=False,overwrite:bool=False,scorer:Optional[Callable]=partial(overlapping_labeled_spans_score,spans_key=DEFAULT_SPANS_KEY))
spacy.pipeline.span_ruler.SpanRuler.__len__(self)->int
spacy.pipeline.span_ruler.SpanRuler._require_patterns(self)->None
spacy.pipeline.span_ruler.SpanRuler.add_patterns(self,patterns:List[PatternType])->None
spacy.pipeline.span_ruler.SpanRuler.clear(self)->None
spacy.pipeline.span_ruler.SpanRuler.from_bytes(self,bytes_data:bytes,*,exclude:Iterable[str]=SimpleFrozenList())->'SpanRuler'
spacy.pipeline.span_ruler.SpanRuler.from_disk(self,path:Union[str,Path],*,exclude:Iterable[str]=SimpleFrozenList())->'SpanRuler'
spacy.pipeline.span_ruler.SpanRuler.ids(self)->Tuple[str, ...]
spacy.pipeline.span_ruler.SpanRuler.initialize(self,get_examples:Callable[[],Iterable[Example]],*,nlp:Optional[Language]=None,patterns:Optional[Sequence[PatternType]]=None)
spacy.pipeline.span_ruler.SpanRuler.key(self)->Optional[str]
spacy.pipeline.span_ruler.SpanRuler.labels(self)->Tuple[str, ...]
spacy.pipeline.span_ruler.SpanRuler.match(self,doc:Doc)
spacy.pipeline.span_ruler.SpanRuler.patterns(self)->List[PatternType]
spacy.pipeline.span_ruler.SpanRuler.remove(self,label:str)->None
spacy.pipeline.span_ruler.SpanRuler.remove_by_id(self,pattern_id:str)->None
spacy.pipeline.span_ruler.SpanRuler.set_annotations(self,doc,matches)
spacy.pipeline.span_ruler.SpanRuler.to_bytes(self,*,exclude:Iterable[str]=SimpleFrozenList())->bytes
spacy.pipeline.span_ruler.SpanRuler.to_disk(self,path:Union[str,Path],*,exclude:Iterable[str]=SimpleFrozenList())->None
spacy.pipeline.span_ruler.make_entity_ruler(nlp:Language,name:str,phrase_matcher_attr:Optional[Union[int,str]],matcher_fuzzy_compare:Callable,validate:bool,overwrite_ents:bool,scorer:Optional[Callable],ent_id_sep:str)
spacy.pipeline.span_ruler.make_overlapping_labeled_spans_scorer(spans_key:str=DEFAULT_SPANS_KEY)
spacy.pipeline.span_ruler.make_preserve_existing_ents_filter()
spacy.pipeline.span_ruler.make_prioritize_new_ents_filter()
spacy.pipeline.span_ruler.make_span_ruler(nlp:Language,name:str,spans_key:Optional[str],spans_filter:Optional[Callable[[Iterable[Span],Iterable[Span]],Iterable[Span]]],annotate_ents:bool,ents_filter:Callable[[Iterable[Span],Iterable[Span]],Iterable[Span]],phrase_matcher_attr:Optional[Union[int,str]],matcher_fuzzy_compare:Callable,validate:bool,overwrite:bool,scorer:Optional[Callable])
spacy.pipeline.span_ruler.overlapping_labeled_spans_score(examples:Iterable[Example],*,spans_key=DEFAULT_SPANS_KEY,**kwargs)->Dict[str, Any]
spacy.pipeline.span_ruler.prioritize_existing_ents_filter(entities:Iterable[Span],spans:Iterable[Span])->List[Span]
spacy.pipeline.span_ruler.prioritize_new_ents_filter(entities:Iterable[Span],spans:Iterable[Span])->List[Span]


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/pipeline/entity_linker.py----------------------------------------
A:spacy.pipeline.entity_linker.self.labels_discard->list(labels_discard)
A:spacy.pipeline.entity_linker.self.distance->CosineDistance(normalize=False)
A:spacy.pipeline.entity_linker.self.kb->kb_loader(self.vocab)
A:spacy.pipeline.entity_linker.examples->self._ensure_ents(examples)
A:spacy.pipeline.entity_linker.docs->self.pipe((eg.predicted for eg in examples))
A:spacy.pipeline.entity_linker.(ents, _)->eg.get_aligned_ents_and_ner()
A:spacy.pipeline.entity_linker.new_eg->eg.copy()
A:spacy.pipeline.entity_linker.has_annotations->any([doc.ents for doc in doc_sample])
A:spacy.pipeline.entity_linker.candidates->list(batch_candidates[j])
A:spacy.pipeline.entity_linker.(sentence_encodings, bp_context)->self.model.begin_update(docs)
A:spacy.pipeline.entity_linker.(loss, d_scores)->self.get_loss(sentence_encodings=sentence_encodings, examples=examples)
A:spacy.pipeline.entity_linker.kb_ids->eg.get_aligned('ENT_KB_ID', as_string=True)
A:spacy.pipeline.entity_linker.entity_encoding->self.kb.get_vector(kb_id)
A:spacy.pipeline.entity_linker.entity_encodings->xp.asarray([c.entity_vector for c in candidates])
A:spacy.pipeline.entity_linker.out->self.model.ops.alloc2f(*sentence_encodings.shape)
A:spacy.pipeline.entity_linker.err->errors.Errors.E147.format(method='predict', msg='result variables not of equal length')
A:spacy.pipeline.entity_linker.gradients->self.distance.get_grad(selected_encodings, entity_encodings)
A:spacy.pipeline.entity_linker.loss->self.distance.get_loss(selected_encodings, entity_encodings)
A:spacy.pipeline.entity_linker.batch_candidates->list(self.get_candidates_batch(self.kb, [ent_batch[idx] for idx in valid_ent_idx]) if self.candidates_batch_size > 1 else [self.get_candidates(self.kb, ent_batch[idx]) for idx in valid_ent_idx])
A:spacy.pipeline.entity_linker.sents->list(ent.sents)
A:spacy.pipeline.entity_linker.start_sentence->max(0, sent_indices[0] - self.n_sents)
A:spacy.pipeline.entity_linker.end_sentence->min(len(sentences) - 1, sent_indices[1] + self.n_sents)
A:spacy.pipeline.entity_linker.sent_doc->doc[start_token:end_token].as_doc()
A:spacy.pipeline.entity_linker.sentence_norm->xp.linalg.norm(sentence_encoding_t)
A:spacy.pipeline.entity_linker.prior_probs->xp.asarray([0.0 for _ in candidates])
A:spacy.pipeline.entity_linker.entity_norm->xp.linalg.norm(entity_encodings, axis=1)
A:spacy.pipeline.entity_linker.count_ents->len([ent for doc in docs for ent in doc.ents])
spacy.pipeline.EntityLinker(self,vocab:Vocab,model:Model,name:str='entity_linker',*,labels_discard:Iterable[str],n_sents:int,incl_prior:bool,incl_context:bool,entity_vector_length:int,get_candidates:Callable[[KnowledgeBase,Span],Iterable[Candidate]],get_candidates_batch:Callable[[KnowledgeBase,Iterable[Span]],Iterable[Iterable[Candidate]]],generate_empty_kb:Callable[[Vocab,int],KnowledgeBase],overwrite:bool=BACKWARD_OVERWRITE,scorer:Optional[Callable]=entity_linker_score,use_gold_ents:bool,candidates_batch_size:int,threshold:Optional[float]=None)
spacy.pipeline.EntityLinker._ensure_ents(self,examples:Iterable[Example])->Iterable[Example]
spacy.pipeline.EntityLinker.add_label(self,label)
spacy.pipeline.EntityLinker.batch_has_learnable_example(self,examples)
spacy.pipeline.EntityLinker.from_bytes(self,bytes_data,*,exclude=tuple())
spacy.pipeline.EntityLinker.from_disk(self,path:Union[str,Path],*,exclude:Iterable[str]=SimpleFrozenList())->'EntityLinker'
spacy.pipeline.EntityLinker.get_loss(self,examples:Iterable[Example],sentence_encodings:Floats2d)
spacy.pipeline.EntityLinker.initialize(self,get_examples:Callable[[],Iterable[Example]],*,nlp:Optional[Language]=None,kb_loader:Optional[Callable[[Vocab],KnowledgeBase]]=None)
spacy.pipeline.EntityLinker.predict(self,docs:Iterable[Doc])->List[str]
spacy.pipeline.EntityLinker.rehearse(self,examples,*,sgd=None,losses=None,**config)
spacy.pipeline.EntityLinker.set_annotations(self,docs:Iterable[Doc],kb_ids:List[str])->None
spacy.pipeline.EntityLinker.set_kb(self,kb_loader:Callable[[Vocab],KnowledgeBase])
spacy.pipeline.EntityLinker.to_bytes(self,*,exclude=tuple())
spacy.pipeline.EntityLinker.to_disk(self,path:Union[str,Path],*,exclude:Iterable[str]=SimpleFrozenList())->None
spacy.pipeline.EntityLinker.update(self,examples:Iterable[Example],*,drop:float=0.0,sgd:Optional[Optimizer]=None,losses:Optional[Dict[str,float]]=None)->Dict[str, float]
spacy.pipeline.EntityLinker.validate_kb(self)->None
spacy.pipeline.entity_linker.EntityLinker(self,vocab:Vocab,model:Model,name:str='entity_linker',*,labels_discard:Iterable[str],n_sents:int,incl_prior:bool,incl_context:bool,entity_vector_length:int,get_candidates:Callable[[KnowledgeBase,Span],Iterable[Candidate]],get_candidates_batch:Callable[[KnowledgeBase,Iterable[Span]],Iterable[Iterable[Candidate]]],generate_empty_kb:Callable[[Vocab,int],KnowledgeBase],overwrite:bool=BACKWARD_OVERWRITE,scorer:Optional[Callable]=entity_linker_score,use_gold_ents:bool,candidates_batch_size:int,threshold:Optional[float]=None)
spacy.pipeline.entity_linker.EntityLinker.__init__(self,vocab:Vocab,model:Model,name:str='entity_linker',*,labels_discard:Iterable[str],n_sents:int,incl_prior:bool,incl_context:bool,entity_vector_length:int,get_candidates:Callable[[KnowledgeBase,Span],Iterable[Candidate]],get_candidates_batch:Callable[[KnowledgeBase,Iterable[Span]],Iterable[Iterable[Candidate]]],generate_empty_kb:Callable[[Vocab,int],KnowledgeBase],overwrite:bool=BACKWARD_OVERWRITE,scorer:Optional[Callable]=entity_linker_score,use_gold_ents:bool,candidates_batch_size:int,threshold:Optional[float]=None)
spacy.pipeline.entity_linker.EntityLinker._ensure_ents(self,examples:Iterable[Example])->Iterable[Example]
spacy.pipeline.entity_linker.EntityLinker.add_label(self,label)
spacy.pipeline.entity_linker.EntityLinker.batch_has_learnable_example(self,examples)
spacy.pipeline.entity_linker.EntityLinker.from_bytes(self,bytes_data,*,exclude=tuple())
spacy.pipeline.entity_linker.EntityLinker.from_disk(self,path:Union[str,Path],*,exclude:Iterable[str]=SimpleFrozenList())->'EntityLinker'
spacy.pipeline.entity_linker.EntityLinker.get_loss(self,examples:Iterable[Example],sentence_encodings:Floats2d)
spacy.pipeline.entity_linker.EntityLinker.initialize(self,get_examples:Callable[[],Iterable[Example]],*,nlp:Optional[Language]=None,kb_loader:Optional[Callable[[Vocab],KnowledgeBase]]=None)
spacy.pipeline.entity_linker.EntityLinker.predict(self,docs:Iterable[Doc])->List[str]
spacy.pipeline.entity_linker.EntityLinker.rehearse(self,examples,*,sgd=None,losses=None,**config)
spacy.pipeline.entity_linker.EntityLinker.set_annotations(self,docs:Iterable[Doc],kb_ids:List[str])->None
spacy.pipeline.entity_linker.EntityLinker.set_kb(self,kb_loader:Callable[[Vocab],KnowledgeBase])
spacy.pipeline.entity_linker.EntityLinker.to_bytes(self,*,exclude=tuple())
spacy.pipeline.entity_linker.EntityLinker.to_disk(self,path:Union[str,Path],*,exclude:Iterable[str]=SimpleFrozenList())->None
spacy.pipeline.entity_linker.EntityLinker.update(self,examples:Iterable[Example],*,drop:float=0.0,sgd:Optional[Optimizer]=None,losses:Optional[Dict[str,float]]=None)->Dict[str, float]
spacy.pipeline.entity_linker.EntityLinker.validate_kb(self)->None
spacy.pipeline.entity_linker.entity_linker_score(examples,**kwargs)
spacy.pipeline.entity_linker.make_entity_linker(nlp:Language,name:str,model:Model,*,labels_discard:Iterable[str],n_sents:int,incl_prior:bool,incl_context:bool,entity_vector_length:int,get_candidates:Callable[[KnowledgeBase,Span],Iterable[Candidate]],get_candidates_batch:Callable[[KnowledgeBase,Iterable[Span]],Iterable[Iterable[Candidate]]],generate_empty_kb:Callable[[Vocab,int],KnowledgeBase],overwrite:bool,scorer:Optional[Callable],use_gold_ents:bool,candidates_batch_size:int,threshold:Optional[float]=None)
spacy.pipeline.entity_linker.make_entity_linker_scorer()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/pipeline/pipe.pyi----------------------------------------
spacy.pipeline.Pipe(self,doc:Doc)
spacy.pipeline.Pipe._require_labels(self)->None
spacy.pipeline.Pipe.get_error_handler(self)->Callable[[str, 'Pipe', List[Doc], Exception], NoReturn]
spacy.pipeline.Pipe.hide_labels(self)->bool
spacy.pipeline.Pipe.initialize(self,get_examples:Callable[[],Iterable[Example]],*,nlp:Language=...)->None
spacy.pipeline.Pipe.is_trainable(self)->bool
spacy.pipeline.Pipe.label_data(self)->Any
spacy.pipeline.Pipe.labels(self)->Tuple[str, ...]
spacy.pipeline.Pipe.pipe(self,stream:Iterable[Doc],*,batch_size:int=...)->Iterator[Doc]
spacy.pipeline.Pipe.score(self,examples:Iterable[Example],**kwargs:Any)->Dict[str, Union[float, Dict[str, float]]]
spacy.pipeline.Pipe.set_error_handler(self,error_handler:Callable[[str,'Pipe',List[Doc],Exception],NoReturn])->None
spacy.pipeline.pipe.Pipe(self,doc:Doc)
spacy.pipeline.pipe.Pipe.__call__(self,doc:Doc)
spacy.pipeline.pipe.Pipe._require_labels(self)->None
spacy.pipeline.pipe.Pipe.get_error_handler(self)->Callable[[str, 'Pipe', List[Doc], Exception], NoReturn]
spacy.pipeline.pipe.Pipe.hide_labels(self)->bool
spacy.pipeline.pipe.Pipe.initialize(self,get_examples:Callable[[],Iterable[Example]],*,nlp:Language=...)->None
spacy.pipeline.pipe.Pipe.is_trainable(self)->bool
spacy.pipeline.pipe.Pipe.label_data(self)->Any
spacy.pipeline.pipe.Pipe.labels(self)->Tuple[str, ...]
spacy.pipeline.pipe.Pipe.pipe(self,stream:Iterable[Doc],*,batch_size:int=...)->Iterator[Doc]
spacy.pipeline.pipe.Pipe.score(self,examples:Iterable[Example],**kwargs:Any)->Dict[str, Union[float, Dict[str, float]]]
spacy.pipeline.pipe.Pipe.set_error_handler(self,error_handler:Callable[[str,'Pipe',List[Doc],Exception],NoReturn])->None
spacy.pipeline.pipe.deserialize_config(path:Path)->Any


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/pipeline/_edit_tree_internals/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/pipeline/_edit_tree_internals/schemas.py----------------------------------------
A:spacy.pipeline._edit_tree_internals.schemas.errors->e.errors()
A:spacy.pipeline._edit_tree_internals.schemas.data->defaultdict(list)
A:spacy.pipeline._edit_tree_internals.schemas.err_loc->' -> '.join([str(p) for p in error.get('loc', [])])
spacy.pipeline._edit_tree_internals.schemas.EditTreeSchema(BaseModel)
spacy.pipeline._edit_tree_internals.schemas.MatchNodeSchema(BaseModel)
spacy.pipeline._edit_tree_internals.schemas.MatchNodeSchema.Config
spacy.pipeline._edit_tree_internals.schemas.SubstNodeSchema(BaseModel)
spacy.pipeline._edit_tree_internals.schemas.SubstNodeSchema.Config
spacy.pipeline._edit_tree_internals.schemas.validate_edit_tree(obj:Dict[str,Any])->List[str]


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/pipeline/_parser_internals/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/pipeline/legacy/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/pipeline/legacy/entity_linker.py----------------------------------------
A:spacy.pipeline.legacy.entity_linker.self.labels_discard->list(labels_discard)
A:spacy.pipeline.legacy.entity_linker.self.distance->CosineDistance(normalize=False)
A:spacy.pipeline.legacy.entity_linker.self.kb->kb_loader(self.vocab)
A:spacy.pipeline.legacy.entity_linker.kb_ids->eg.get_aligned('ENT_KB_ID', as_string=True)
A:spacy.pipeline.legacy.entity_linker.sent_index->sentences.index(sent)
A:spacy.pipeline.legacy.entity_linker.start_sentence->max(0, sent_index - self.n_sents)
A:spacy.pipeline.legacy.entity_linker.end_sentence->min(len(sentences) - 1, sent_index + self.n_sents)
A:spacy.pipeline.legacy.entity_linker.sent_doc->doc[start_token:end_token].as_doc()
A:spacy.pipeline.legacy.entity_linker.(sentence_encodings, bp_context)->self.model.begin_update(sentence_docs)
A:spacy.pipeline.legacy.entity_linker.(loss, d_scores)->self.get_loss(sentence_encodings=sentence_encodings, examples=examples)
A:spacy.pipeline.legacy.entity_linker.entity_encoding->self.kb.get_vector(kb_id)
A:spacy.pipeline.legacy.entity_linker.entity_encodings->xp.asarray([c.entity_vector for c in candidates])
A:spacy.pipeline.legacy.entity_linker.err->errors.Errors.E147.format(method='predict', msg='result variables not of equal length')
A:spacy.pipeline.legacy.entity_linker.gradients->self.distance.get_grad(sentence_encodings, entity_encodings)
A:spacy.pipeline.legacy.entity_linker.loss->self.distance.get_loss(sentence_encodings, entity_encodings)
A:spacy.pipeline.legacy.entity_linker.sentence_norm->xp.linalg.norm(sentence_encoding_t)
A:spacy.pipeline.legacy.entity_linker.candidates->list(self.get_candidates(self.kb, ent))
A:spacy.pipeline.legacy.entity_linker.prior_probs->xp.asarray([0.0 for _ in candidates])
A:spacy.pipeline.legacy.entity_linker.entity_norm->xp.linalg.norm(entity_encodings, axis=1)
A:spacy.pipeline.legacy.entity_linker.best_index->scores.argmax().item()
A:spacy.pipeline.legacy.entity_linker.count_ents->len([ent for doc in docs for ent in doc.ents])
spacy.pipeline.legacy.EntityLinker_v1(self,vocab:Vocab,model:Model,name:str='entity_linker',*,labels_discard:Iterable[str],n_sents:int,incl_prior:bool,incl_context:bool,entity_vector_length:int,get_candidates:Callable[[KnowledgeBase,Span],Iterable[Candidate]],overwrite:bool=BACKWARD_OVERWRITE,scorer:Optional[Callable]=entity_linker_score)
spacy.pipeline.legacy.EntityLinker_v1.add_label(self,label)
spacy.pipeline.legacy.EntityLinker_v1.from_bytes(self,bytes_data,*,exclude=tuple())
spacy.pipeline.legacy.EntityLinker_v1.from_disk(self,path:Union[str,Path],*,exclude:Iterable[str]=SimpleFrozenList())->'EntityLinker_v1'
spacy.pipeline.legacy.EntityLinker_v1.get_loss(self,examples:Iterable[Example],sentence_encodings:Floats2d)
spacy.pipeline.legacy.EntityLinker_v1.initialize(self,get_examples:Callable[[],Iterable[Example]],*,nlp:Optional[Language]=None,kb_loader:Optional[Callable[[Vocab],KnowledgeBase]]=None)
spacy.pipeline.legacy.EntityLinker_v1.predict(self,docs:Iterable[Doc])->List[str]
spacy.pipeline.legacy.EntityLinker_v1.rehearse(self,examples,*,sgd=None,losses=None,**config)
spacy.pipeline.legacy.EntityLinker_v1.set_annotations(self,docs:Iterable[Doc],kb_ids:List[str])->None
spacy.pipeline.legacy.EntityLinker_v1.set_kb(self,kb_loader:Callable[[Vocab],KnowledgeBase])
spacy.pipeline.legacy.EntityLinker_v1.to_bytes(self,*,exclude=tuple())
spacy.pipeline.legacy.EntityLinker_v1.to_disk(self,path:Union[str,Path],*,exclude:Iterable[str]=SimpleFrozenList())->None
spacy.pipeline.legacy.EntityLinker_v1.update(self,examples:Iterable[Example],*,drop:float=0.0,sgd:Optional[Optimizer]=None,losses:Optional[Dict[str,float]]=None)->Dict[str, float]
spacy.pipeline.legacy.EntityLinker_v1.validate_kb(self)->None
spacy.pipeline.legacy.entity_linker.EntityLinker_v1(self,vocab:Vocab,model:Model,name:str='entity_linker',*,labels_discard:Iterable[str],n_sents:int,incl_prior:bool,incl_context:bool,entity_vector_length:int,get_candidates:Callable[[KnowledgeBase,Span],Iterable[Candidate]],overwrite:bool=BACKWARD_OVERWRITE,scorer:Optional[Callable]=entity_linker_score)
spacy.pipeline.legacy.entity_linker.EntityLinker_v1.__init__(self,vocab:Vocab,model:Model,name:str='entity_linker',*,labels_discard:Iterable[str],n_sents:int,incl_prior:bool,incl_context:bool,entity_vector_length:int,get_candidates:Callable[[KnowledgeBase,Span],Iterable[Candidate]],overwrite:bool=BACKWARD_OVERWRITE,scorer:Optional[Callable]=entity_linker_score)
spacy.pipeline.legacy.entity_linker.EntityLinker_v1.add_label(self,label)
spacy.pipeline.legacy.entity_linker.EntityLinker_v1.from_bytes(self,bytes_data,*,exclude=tuple())
spacy.pipeline.legacy.entity_linker.EntityLinker_v1.from_disk(self,path:Union[str,Path],*,exclude:Iterable[str]=SimpleFrozenList())->'EntityLinker_v1'
spacy.pipeline.legacy.entity_linker.EntityLinker_v1.get_loss(self,examples:Iterable[Example],sentence_encodings:Floats2d)
spacy.pipeline.legacy.entity_linker.EntityLinker_v1.initialize(self,get_examples:Callable[[],Iterable[Example]],*,nlp:Optional[Language]=None,kb_loader:Optional[Callable[[Vocab],KnowledgeBase]]=None)
spacy.pipeline.legacy.entity_linker.EntityLinker_v1.predict(self,docs:Iterable[Doc])->List[str]
spacy.pipeline.legacy.entity_linker.EntityLinker_v1.rehearse(self,examples,*,sgd=None,losses=None,**config)
spacy.pipeline.legacy.entity_linker.EntityLinker_v1.set_annotations(self,docs:Iterable[Doc],kb_ids:List[str])->None
spacy.pipeline.legacy.entity_linker.EntityLinker_v1.set_kb(self,kb_loader:Callable[[Vocab],KnowledgeBase])
spacy.pipeline.legacy.entity_linker.EntityLinker_v1.to_bytes(self,*,exclude=tuple())
spacy.pipeline.legacy.entity_linker.EntityLinker_v1.to_disk(self,path:Union[str,Path],*,exclude:Iterable[str]=SimpleFrozenList())->None
spacy.pipeline.legacy.entity_linker.EntityLinker_v1.update(self,examples:Iterable[Example],*,drop:float=0.0,sgd:Optional[Optimizer]=None,losses:Optional[Dict[str,float]]=None)->Dict[str, float]
spacy.pipeline.legacy.entity_linker.EntityLinker_v1.validate_kb(self)->None
spacy.pipeline.legacy.entity_linker.entity_linker_score(examples,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/test_scorer.py----------------------------------------
A:spacy.tests.test_scorer.nlp->English()
A:spacy.tests.test_scorer.doc->Doc(en_vocab, words=input_.split(' '), ents=['B-ORG', 'O', 'O', 'O', 'O', 'B-GPE', 'B-ORG', 'O', 'O', 'O'])
A:spacy.tests.test_scorer.scorer->Scorer()
A:spacy.tests.test_scorer.example->Example(pred_doc, gold_doc)
A:spacy.tests.test_scorer.scores->spacy.scorer.Scorer.score_cats([example], 'cats', labels=list(gold_doc.cats.keys()), multi_label=True, threshold=0.1)
A:spacy.tests.test_scorer.example.predicted->Doc(nlp.vocab, words=['One', 'sentence.', 'Two', 'sentences.', 'Three', 'sentences.'], spaces=[True, True, True, True, True, False])
A:spacy.tests.test_scorer.results->Scorer().score([example], per_component=True)
A:spacy.tests.test_scorer.entities->offsets_to_biluo_tags(doc, annot['entities'])
A:spacy.tests.test_scorer.pred_doc->en_tokenizer(text)
A:spacy.tests.test_scorer.ref_doc->en_tokenizer('a b c d e')
A:spacy.tests.test_scorer.(tpr, fpr, _)->_roc_curve(y_true, y_score)
A:spacy.tests.test_scorer.roc_auc->_roc_auc_score(y_true, y_score)
A:spacy.tests.test_scorer.score->ROCAUCScore()
A:spacy.tests.test_scorer.gold->English().make_doc(text)
A:spacy.tests.test_scorer.pred->English().make_doc(text)
A:spacy.tests.test_scorer.pred.spans[key]->English().make_doc(text).spans[key].copy(doc=pred)
A:spacy.tests.test_scorer.eg->Example(pred, gold)
A:spacy.tests.test_scorer.gold2->set()
A:spacy.tests.test_scorer.a->PRFScore()
A:spacy.tests.test_scorer.b->PRFScore()
A:spacy.tests.test_scorer.gold_doc->en_tokenizer(text)
A:spacy.tests.test_scorer.scores1->spacy.scorer.Scorer.score_cats([example], 'cats', labels=list(gold_doc.cats.keys()), multi_label=False, positive_label='POSITIVE', threshold=0.1)
A:spacy.tests.test_scorer.scores2->spacy.scorer.Scorer.score_cats([example], 'cats', labels=list(gold_doc.cats.keys()), multi_label=False, positive_label='POSITIVE', threshold=0.9)
spacy.tests.test_scorer.sented_doc()
spacy.tests.test_scorer.tagged_doc()
spacy.tests.test_scorer.test_las_per_type(en_vocab)
spacy.tests.test_scorer.test_ner_per_type(en_vocab)
spacy.tests.test_scorer.test_partial_annotation(en_tokenizer)
spacy.tests.test_scorer.test_prf_score()
spacy.tests.test_scorer.test_roc_auc_score()
spacy.tests.test_scorer.test_score_cats(en_tokenizer)
spacy.tests.test_scorer.test_score_spans()
spacy.tests.test_scorer.test_sents(sented_doc)
spacy.tests.test_scorer.test_tag_score(tagged_doc)
spacy.tests.test_scorer.test_tokenization(sented_doc)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/test_misc.py----------------------------------------
A:spacy.tests.test_misc.doc->en_tokenizer('zero one two three four five six')
A:spacy.tests.test_misc.result->spacy.util.resolve_dot_names(config, ['training.optimizer'])
A:spacy.tests.test_misc.path->spacy.util.get_package_path(package)
A:spacy.tests.test_misc.model->PrecomputableAffine(nO=nO, nI=nI, nF=nF, nP=nP).initialize()
A:spacy.tests.test_misc.tensor->PrecomputableAffine(nO=nO, nI=nI, nF=nF, nP=nP).initialize().ops.alloc((10, nI))
A:spacy.tests.test_misc.(Y, get_dX)->PrecomputableAffine(nO=nO, nI=nI, nF=nF, nP=nP).initialize().begin_update(tensor)
A:spacy.tests.test_misc.dY->PrecomputableAffine(nO=nO, nI=nI, nF=nF, nP=nP).initialize().ops.alloc((15, nO, nP))
A:spacy.tests.test_misc.ids->PrecomputableAffine(nO=nO, nI=nI, nF=nF, nP=nP).initialize().ops.alloc((15, nF))
A:spacy.tests.test_misc.d_pad->_backprop_precomputable_affine_padding(model, dY, ids)
A:spacy.tests.test_misc.current_ops->get_current_ops()
A:spacy.tests.test_misc.nlp->spacy.lang.en.English.from_config(config)
A:spacy.tests.test_misc.batches->list(minibatch_by_words(docs, size=batch_size, tolerance=tol, discard_oversize=False))
A:spacy.tests.test_misc.nlp_config->Config().from_str(cfg_string)
A:spacy.tests.test_misc.en_nlp->spacy.util.load_model_from_config(nlp_config, auto_fill=True)
A:spacy.tests.test_misc.default_config->Config().from_disk(DEFAULT_CONFIG_PATH)
A:spacy.tests.test_misc.nl_nlp->spacy.util.load_model_from_config(default_config, auto_fill=True)
A:spacy.tests.test_misc.T->spacy.util.registry.resolve(nl_nlp.config['training'], schema=ConfigSchemaTraining)
A:spacy.tests.test_misc.t->SimpleFrozenList(['foo', 'bar'], error='Error!')
A:spacy.tests.test_misc.code_path->os.path.join(temp_dir, 'code.py')
A:spacy.tests.test_misc.found_port->find_available_port(port, host, auto_select=True)
spacy.tests.test_misc.is_admin()
spacy.tests.test_misc.test_PrecomputableAffine(nO=4,nI=5,nF=3,nP=2)
spacy.tests.test_misc.test_ascii_filenames()
spacy.tests.test_misc.test_dot_to_dict(dot_notation,expected)
spacy.tests.test_misc.test_dot_to_dict_overrides(dot_notation,expected)
spacy.tests.test_misc.test_find_available_port()
spacy.tests.test_misc.test_import_code()
spacy.tests.test_misc.test_is_compatible_version(version,constraint,compatible)
spacy.tests.test_misc.test_is_unconstrained_version(constraint,expected)
spacy.tests.test_misc.test_issue6207(en_tokenizer)
spacy.tests.test_misc.test_issue6258()
spacy.tests.test_misc.test_load_model_blank_shortcut()
spacy.tests.test_misc.test_minor_version(a1,a2,b1,b2,is_match)
spacy.tests.test_misc.test_prefer_gpu()
spacy.tests.test_misc.test_require_cpu()
spacy.tests.test_misc.test_require_gpu()
spacy.tests.test_misc.test_resolve_dot_names()
spacy.tests.test_misc.test_set_dot_to_object()
spacy.tests.test_misc.test_simple_frozen_list()
spacy.tests.test_misc.test_to_ternary_int()
spacy.tests.test_misc.test_util_dot_section()
spacy.tests.test_misc.test_util_ensure_path_succeeds(text)
spacy.tests.test_misc.test_util_get_package_path(package)
spacy.tests.test_misc.test_util_is_package(package,result)
spacy.tests.test_misc.test_util_minibatch(doc_sizes,expected_batches)
spacy.tests.test_misc.test_util_minibatch_oversize(doc_sizes,expected_batches)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/test_language.py----------------------------------------
A:spacy.tests.test_language.logger->logging.getLogger('spacy')
A:spacy.tests.test_language.nlp->English()
A:spacy.tests.test_language.textcat->English().add_pipe('textcat')
A:spacy.tests.test_language.doc->nlp(docs[0])
A:spacy.tests.test_language.example->spacy.training.Example.from_dict(doc, annots)
A:spacy.tests.test_language.scores->English().evaluate([example])
A:spacy.tests.test_language.textcat_multilabel->English().add_pipe('textcat_multilabel')
A:spacy.tests.test_language.span->Span(doc, 0, 1, label='FIRST')
A:spacy.tests.test_language.ops->get_current_ops()
A:spacy.tests.test_language.docs->nlp2.pipe(texts, n_process=2, batch_size=2)
A:spacy.tests.test_language.stream_texts->itertools.cycle(texts)
A:spacy.tests.test_language.(texts0, texts1)->itertools.tee(stream_texts)
A:spacy.tests.test_language.tuples->list(nlp.pipe(texts, as_tuples=True, n_process=n_process))
A:spacy.tests.test_language.words->text.split(' ')
A:spacy.tests.test_language.nlp.tokenizer->WhitespaceTokenizer(nlp.vocab)
A:spacy.tests.test_language.vectors_bytes->English().vocab.vectors.to_bytes()
spacy.tests.test_language.assert_sents_error(doc)
spacy.tests.test_language.evil_component(doc)
spacy.tests.test_language.ner_pipe(doc)
spacy.tests.test_language.nlp()
spacy.tests.test_language.nlp2(nlp,sample_vectors)
spacy.tests.test_language.perhaps_set_sentences(doc)
spacy.tests.test_language.sample_vectors()
spacy.tests.test_language.test_blank_languages(lang,target)
spacy.tests.test_language.test_component_return()
spacy.tests.test_language.test_dot_in_factory_names(nlp)
spacy.tests.test_language.test_evaluate_multiple_textcat_final(en_vocab)
spacy.tests.test_language.test_evaluate_multiple_textcat_separate(en_vocab)
spacy.tests.test_language.test_evaluate_no_pipe(nlp)
spacy.tests.test_language.test_evaluate_textcat_multilabel(en_vocab)
spacy.tests.test_language.test_invalid_arg_to_pipeline(nlp)
spacy.tests.test_language.test_language_custom_tokenizer()
spacy.tests.test_language.test_language_evaluate(nlp)
spacy.tests.test_language.test_language_from_config_before_after_init()
spacy.tests.test_language.test_language_from_config_before_after_init_invalid()
spacy.tests.test_language.test_language_from_config_invalid_lang()
spacy.tests.test_language.test_language_init_invalid_vocab(value)
spacy.tests.test_language.test_language_matching(lang,target)
spacy.tests.test_language.test_language_pipe(nlp2,n_process,texts)
spacy.tests.test_language.test_language_pipe_error_handler(n_process)
spacy.tests.test_language.test_language_pipe_error_handler_custom(en_vocab,n_process)
spacy.tests.test_language.test_language_pipe_error_handler_input_as_tuples(en_vocab,n_process)
spacy.tests.test_language.test_language_pipe_error_handler_make_doc_actual(n_process)
spacy.tests.test_language.test_language_pipe_error_handler_make_doc_preferred(n_process)
spacy.tests.test_language.test_language_pipe_error_handler_pipe(en_vocab,n_process)
spacy.tests.test_language.test_language_pipe_stream(nlp2,n_process,texts)
spacy.tests.test_language.test_language_source_and_vectors(nlp2)
spacy.tests.test_language.test_language_update(nlp)
spacy.tests.test_language.test_language_whitespace_tokenizer()
spacy.tests.test_language.test_multiprocessing_gpu_warning(nlp2,texts)
spacy.tests.test_language.test_pass_doc_to_pipeline(nlp,n_process)
spacy.tests.test_language.test_spacy_blank()
spacy.tests.test_language.texts()
spacy.tests.test_language.userdata_pipe(doc)
spacy.tests.test_language.vector_modification_pipe(doc)
spacy.tests.test_language.warn_error(proc_name,proc,docs,e)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/enable_gpu.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/test_cli.py----------------------------------------
A:spacy.tests.test_cli.docs->list(conllu_to_docs(input_data))
A:spacy.tests.test_cli.nlp->Language()
A:spacy.tests.test_cli.example->spacy.training.Example.from_dict(nlp.make_doc(''), {})
A:spacy.tests.test_cli.source_nlp->spacy.lang.en.English.from_config(source_cfg)
A:spacy.tests.test_cli.base_cfg->Config(base_cfg)
A:spacy.tests.test_cli.filled_cfg->load_config(output_path)
A:spacy.tests.test_cli.doc->Language().make_doc(t[0])
A:spacy.tests.test_cli.raw_data->info(tmp_dir, exclude=[''])
A:spacy.tests.test_cli.input_data->'\n'.join(lines)
A:spacy.tests.test_cli.converted_docs->list(conll_ner_to_docs(input_data, n_sents=10))
A:spacy.tests.test_cli.biluo_tags->offsets_to_biluo_tags(converted_docs[0], ent_offsets, missing='O')
A:spacy.tests.test_cli.converted->docs_to_json(converted_docs)
A:spacy.tests.test_cli.result->list(DocBin().from_disk(output).get_docs(nlp.vocab))
A:spacy.tests.test_cli.config->init_config(lang='nl', pipeline=[component_name], optimize='efficiency', gpu=False)
A:spacy.tests.test_cli.spec->SpecifierSet('==' + about.__version__)
A:spacy.tests.test_cli.compatibility->get_compatibility()
A:spacy.tests.test_cli.version->get_version(model_name, compatibility)
A:spacy.tests.test_cli.(model_pkgs, compat)->get_model_pkgs()
A:spacy.tests.test_cli.spacy_version->get_minor_version(about.__version__)
A:spacy.tests.test_cli.current_compat->compat.get(spacy_version, {})
A:spacy.tests.test_cli.component->Language().add_pipe(component_name)
A:spacy.tests.test_cli.nlp2->load_model_from_config(config, auto_fill=True)
A:spacy.tests.test_cli.pipe->Language().add_pipe(factory_name, name=pipe_name)
A:spacy.tests.test_cli.pred->Doc(nlp.vocab, words=['Welcome', 'to', 'the', 'Bank', 'of', 'China', '.'])
A:spacy.tests.test_cli.ref->Doc(nlp.vocab, words=['Welcome', 'to', 'the', 'Bank', 'of', 'China', '.'])
A:spacy.tests.test_cli.eg->Example(pred, ref)
A:spacy.tests.test_cli.data->_compile_gold(train_examples, ['trainable_lemmatizer'], nlp, True)
A:spacy.tests.test_cli.expected->Counter({'china': 0.5, 'bank': 0.25, 'of': 0.25})
A:spacy.tests.test_cli.freq_distribution->_get_distribution(docs, normalize=True)
A:spacy.tests.test_cli.p->Counter({'a': 0.5, 'b': 0.25})
A:spacy.tests.test_cli.q->Counter({'a': 0.25, 'b': 0.5, 'c': 0.15, 'd': 0.1})
A:spacy.tests.test_cli.span_characteristics->_get_span_characteristics(examples=examples, compiled_gold=data, spans_key=spans_key)
A:spacy.tests.test_cli.span_freqs->_get_spans_length_freq_dist(sample_span_lengths, threshold)
A:spacy.tests.test_cli.docbin->DocBin(store_user_data=True)
A:spacy.tests.test_cli.new_nlp->English()
A:spacy.tests.test_cli.new_examples->make_examples(new_nlp)
A:spacy.tests.test_cli.(nlp, examples)->init_nlp()
A:spacy.tests.test_cli.(best_threshold, best_score, res)->find_threshold(model=nlp_dir, data_path=docs_dir / 'docs.spacy', pipe_name='spancat', threshold_key='threshold', scores_key='spans_sc_f', silent=True)
A:spacy.tests.test_cli.(nlp, _)->init_nlp()
spacy.tests.test_cli.test_applycli_docbin()
spacy.tests.test_cli.test_applycli_empty_dir()
spacy.tests.test_cli.test_applycli_jsonl()
spacy.tests.test_cli.test_applycli_mixed()
spacy.tests.test_cli.test_applycli_txt()
spacy.tests.test_cli.test_applycli_user_data()
spacy.tests.test_cli.test_cli_converters_conll_ner_to_docs()
spacy.tests.test_cli.test_cli_converters_conllu_empty_heads_ner()
spacy.tests.test_cli.test_cli_converters_conllu_to_docs()
spacy.tests.test_cli.test_cli_converters_conllu_to_docs_name_ner_map(lines)
spacy.tests.test_cli.test_cli_converters_conllu_to_docs_subtokens()
spacy.tests.test_cli.test_cli_converters_iob_to_docs()
spacy.tests.test_cli.test_cli_find_threshold(capsys)
spacy.tests.test_cli.test_cli_info()
spacy.tests.test_cli.test_debug_data_compile_gold()
spacy.tests.test_cli.test_debug_data_compile_gold_for_spans(component_name)
spacy.tests.test_cli.test_debug_data_trainable_lemmatizer_basic()
spacy.tests.test_cli.test_debug_data_trainable_lemmatizer_low_cardinality()
spacy.tests.test_cli.test_debug_data_trainable_lemmatizer_not_annotated()
spacy.tests.test_cli.test_debug_data_trainable_lemmatizer_partial()
spacy.tests.test_cli.test_download_compatibility()
spacy.tests.test_cli.test_download_rejects_relative_urls(monkeypatch)
spacy.tests.test_cli.test_ensure_print_span_characteristics_wont_fail()
spacy.tests.test_cli.test_frequency_distribution_is_correct()
spacy.tests.test_cli.test_get_labels_from_model(factory_name,pipe_name)
spacy.tests.test_cli.test_get_span_characteristics_return_value()
spacy.tests.test_cli.test_get_third_party_dependencies()
spacy.tests.test_cli.test_init_config(lang,pipeline,optimize,pretraining)
spacy.tests.test_cli.test_init_labels(component_name)
spacy.tests.test_cli.test_issue12566(factory:str,output_file:str)
spacy.tests.test_cli.test_issue4924()
spacy.tests.test_cli.test_issue7055()
spacy.tests.test_cli.test_kl_divergence_computation_is_correct()
spacy.tests.test_cli.test_model_recommendations()
spacy.tests.test_cli.test_parse_cli_overrides()
spacy.tests.test_cli.test_parse_config_overrides(args,expected)
spacy.tests.test_cli.test_parse_config_overrides_invalid(args)
spacy.tests.test_cli.test_parse_config_overrides_invalid_2(args)
spacy.tests.test_cli.test_permitted_package_names()
spacy.tests.test_cli.test_project_api_imports()
spacy.tests.test_cli.test_span_length_freq_dist_output_must_be_correct()
spacy.tests.test_cli.test_span_length_freq_dist_threshold_must_be_correct(threshold)
spacy.tests.test_cli.test_string_to_list(value)
spacy.tests.test_cli.test_string_to_list_intify(value)
spacy.tests.test_cli.test_validate_compatibility_table()
spacy.tests.test_cli.test_walk_directory()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/tok2vec.py----------------------------------------
A:spacy.tests.tok2vec.width->model.get_dim('nO')
spacy.tests.tok2vec.build_lazy_init_tok2vec(*,width:int)->Model[List[Doc], List[Floats2d]]
spacy.tests.tok2vec.lazy_init_tok2vec_forward(model:Model,X:List[Doc],is_train:bool)
spacy.tests.tok2vec.lazy_init_tok2vec_init(model:Model,X=None,Y=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/test_errors.py----------------------------------------
spacy.tests.test_Errors(metaclass=ErrorsWithCodes)
spacy.tests.test_errors.Errors(metaclass=ErrorsWithCodes)
spacy.tests.test_errors.test_add_codes()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/test_ty.py----------------------------------------
A:spacy.tests.test_ty.nlp->spacy.blank('en')
A:spacy.tests.test_ty.tok2vec->spacy.blank('en').create_pipe('tok2vec')
A:spacy.tests.test_ty.tagger->spacy.blank('en').create_pipe('tagger')
A:spacy.tests.test_ty.entity_ruler->spacy.blank('en').create_pipe('entity_ruler')
spacy.tests.test_ty.test_component_types()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/test_displacy.py----------------------------------------
A:spacy.tests.test_displacy.doc->Doc(en_vocab, words=['test', '<TEST>'])
A:spacy.tests.test_displacy.html->spacy.displacy.render(doc, style='span')
A:spacy.tests.test_displacy.doc.tensor->numpy.zeros((len(words), 96), dtype='float32')
A:spacy.tests.test_displacy.dep_html->spacy.displacy.render(example_dep, style='dep', manual=True)
A:spacy.tests.test_displacy.ent_html->spacy.displacy.render(example_ent, style='ent', manual=True)
A:spacy.tests.test_displacy.doc.user_data['test']->set()
A:spacy.tests.test_displacy.renderer->EntityRenderer({'ents': ents, 'colors': colors})
A:spacy.tests.test_displacy.nlp->Persian()
A:spacy.tests.test_displacy.found->spacy.displacy.render(doc, style='span').count('<br>')
A:spacy.tests.test_displacy.spans->spacy.displacy.parse_spans(doc)
A:spacy.tests.test_displacy.ents->spacy.displacy.parse_ents(doc, {'kb_url_template': 'https://www.wikidata.org/wiki/{}'})
A:spacy.tests.test_displacy.deps->spacy.displacy.parse_deps(doc[:])
A:spacy.tests.test_displacy.result->EntityRenderer({'ents': ents, 'colors': colors}).render_ents('abcde', spans, None).split('\n\n')
A:spacy.tests.test_displacy.per_token_info->spacy.displacy.render.SpanRenderer._assemble_per_token_info(spans=spans, tokens=tokens)
spacy.tests.test_displacy.test_displacy_invalid_arcs()
spacy.tests.test_displacy.test_displacy_manual_sorted_entities()
spacy.tests.test_displacy.test_displacy_options_case()
spacy.tests.test_displacy.test_displacy_parse_deps(en_vocab)
spacy.tests.test_displacy.test_displacy_parse_empty_spans_key(en_vocab)
spacy.tests.test_displacy.test_displacy_parse_ents(en_vocab)
spacy.tests.test_displacy.test_displacy_parse_ents_with_kb_id_options(en_vocab)
spacy.tests.test_displacy.test_displacy_parse_spans(en_vocab)
spacy.tests.test_displacy.test_displacy_parse_spans_different_spans_key(en_vocab)
spacy.tests.test_displacy.test_displacy_parse_spans_with_kb_id_options(en_vocab)
spacy.tests.test_displacy.test_displacy_raises_for_wrong_type(en_vocab)
spacy.tests.test_displacy.test_displacy_render_manual_dep()
spacy.tests.test_displacy.test_displacy_render_manual_ent()
spacy.tests.test_displacy.test_displacy_render_manual_span()
spacy.tests.test_displacy.test_displacy_render_wrapper(en_vocab)
spacy.tests.test_displacy.test_displacy_rtl()
spacy.tests.test_displacy.test_displacy_span_stacking()
spacy.tests.test_displacy.test_displacy_spans(en_vocab)
spacy.tests.test_displacy.test_issue12816(en_vocab)->None
spacy.tests.test_displacy.test_issue2361(de_vocab)
spacy.tests.test_displacy.test_issue2728(en_vocab)
spacy.tests.test_displacy.test_issue3288(en_vocab)
spacy.tests.test_displacy.test_issue3531()
spacy.tests.test_displacy.test_issue3882(en_vocab)
spacy.tests.test_displacy.test_issue5447()
spacy.tests.test_displacy.test_issue5838()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/test_cli_app.py----------------------------------------
A:spacy.tests.test_cli_app.result->CliRunner().invoke(app, ['find-function', function])
A:spacy.tests.test_cli_app.out_files->os.listdir(d_out)
A:spacy.tests.test_cli_app.result_benchmark->CliRunner().invoke(app, ['benchmark', 'accuracy', '--help'])
A:spacy.tests.test_cli_app.result_evaluate->CliRunner().invoke(app, ['evaluate', '--help'])
A:spacy.tests.test_cli_app.train_bin->DocBin(docs=train_docs)
A:spacy.tests.test_cli_app.dev_bin->DocBin(docs=train_docs)
A:spacy.tests.test_cli_app.result_debug_data->CliRunner().invoke(app, ['debug', 'data', f'{d_in}/config.cfg', '--paths.train', f'{d_in}/train.spacy', '--paths.dev', f'{d_in}/dev.spacy'])
A:spacy.tests.test_cli_app.SAMPLE_PROJECT_TEXT->srsly.yaml_dumps(SAMPLE_PROJECT)
A:spacy.tests.test_cli_app.text->readme_path.read_text('utf-8')
A:spacy.tests.test_cli_app.options->options.split().split()
A:spacy.tests.test_cli_app.proj->dict(SAMPLE_PROJECT)
A:spacy.tests.test_cli_app.proj_text->srsly.yaml_dumps(proj)
A:spacy.tests.test_cli_app.TRAIN_EXAMPLE_1->dict(words=example_words_1, lemmas=example_lemmas_1, tags=example_tags, morphs=example_morphs, deps=example_deps, heads=[1, 1, 1], pos=example_pos, ents=example_ents, spans=example_spans, cats={'CAT': 1.0, 'DOG': 0.0})
A:spacy.tests.test_cli_app.TRAIN_EXAMPLE_2->dict(words=example_words_2, lemmas=example_lemmas_2, tags=example_tags, morphs=example_morphs, deps=example_deps, heads=[1, 1, 1], pos=example_pos, ents=example_ents, spans=example_spans, cats={'CAT': 0.0, 'DOG': 1.0})
A:spacy.tests.test_cli_app.doc->Doc(en_vocab, **example)
A:spacy.tests.test_cli_app.init_config_result->CliRunner().invoke(app, ['init', 'config', f'{d_in}/config.cfg', '--lang', 'en', '--pipeline', component])
A:spacy.tests.test_cli_app.train_result->CliRunner().invoke(app, ['train', f'{d_in}/config.cfg', '--paths.train', f'{d_in}/train.spacy', '--paths.dev', f'{d_in}/dev.spacy', '--output', f'{d_in}/model'])
spacy.tests.test_cli_app.has_git()
spacy.tests.test_cli_app.project_dir()
spacy.tests.test_cli_app.test_benchmark_accuracy_alias()
spacy.tests.test_cli_app.test_convert_auto()
spacy.tests.test_cli_app.test_convert_auto_conflict()
spacy.tests.test_cli_app.test_debug_data_trainable_lemmatizer_cli(en_vocab)
spacy.tests.test_cli_app.test_find_function_invalid()
spacy.tests.test_cli_app.test_find_function_valid()
spacy.tests.test_cli_app.test_init_config_trainable(component,examples,en_vocab)
spacy.tests.test_cli_app.test_init_config_trainable_multiple(component,examples,en_vocab)
spacy.tests.test_cli_app.test_project_assets(project_dir)
spacy.tests.test_cli_app.test_project_clone(options)
spacy.tests.test_cli_app.test_project_document(project_dir)
spacy.tests.test_cli_app.test_project_push_pull(project_dir)
spacy.tests.test_cli_app.test_project_run(project_dir)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/util.py----------------------------------------
A:spacy.tests.util.f->tempfile.TemporaryFile(mode=mode)
A:spacy.tests.util.vocab->Vocab()
A:spacy.tests.util.(move, label)->split_bilu_label(action_name)
A:spacy.tests.util.length->len(vectors[0][1])
A:spacy.tests.util.OPS->get_current_ops()
A:spacy.tests.util.v1->get_current_ops().to_numpy(OPS.asarray(vec1))
A:spacy.tests.util.v2->get_current_ops().to_numpy(OPS.asarray(vec2))
A:spacy.tests.util.msg1->srsly.msgpack_loads(b1)
A:spacy.tests.util.msg2->srsly.msgpack_loads(b2)
spacy.tests.util.add_vecs_to_vocab(vocab,vectors)
spacy.tests.util.apply_transition_sequence(parser,doc,sequence)
spacy.tests.util.assert_docs_equal(doc1,doc2)
spacy.tests.util.assert_packed_msg_equal(b1,b2)
spacy.tests.util.get_batch(batch_size)
spacy.tests.util.get_cosine(vec1,vec2)
spacy.tests.util.get_random_doc(n_words)
spacy.tests.util.make_tempfile(mode='r')
spacy.tests.util.normalize_whitespace(s)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/conftest.py----------------------------------------
A:spacy.tests.conftest.issues->getopt('issue')
A:spacy.tests.conftest.nlp->get_lang_class('zh').from_config(config)
spacy.tests.conftest.af_tokenizer()
spacy.tests.conftest.am_tokenizer()
spacy.tests.conftest.ar_tokenizer()
spacy.tests.conftest.bg_tokenizer()
spacy.tests.conftest.bn_tokenizer()
spacy.tests.conftest.bo_tokenizer()
spacy.tests.conftest.ca_tokenizer()
spacy.tests.conftest.cs_tokenizer()
spacy.tests.conftest.da_tokenizer()
spacy.tests.conftest.de_tokenizer()
spacy.tests.conftest.de_vocab()
spacy.tests.conftest.dsb_tokenizer()
spacy.tests.conftest.el_tokenizer()
spacy.tests.conftest.en_parser(en_vocab)
spacy.tests.conftest.en_tokenizer()
spacy.tests.conftest.en_vocab()
spacy.tests.conftest.es_tokenizer()
spacy.tests.conftest.es_vocab()
spacy.tests.conftest.et_tokenizer()
spacy.tests.conftest.eu_tokenizer()
spacy.tests.conftest.fa_tokenizer()
spacy.tests.conftest.fi_tokenizer()
spacy.tests.conftest.fo_tokenizer()
spacy.tests.conftest.fr_tokenizer()
spacy.tests.conftest.fr_vocab()
spacy.tests.conftest.ga_tokenizer()
spacy.tests.conftest.grc_tokenizer()
spacy.tests.conftest.gu_tokenizer()
spacy.tests.conftest.he_tokenizer()
spacy.tests.conftest.hi_tokenizer()
spacy.tests.conftest.hr_tokenizer()
spacy.tests.conftest.hsb_tokenizer()
spacy.tests.conftest.hu_tokenizer()
spacy.tests.conftest.hy_tokenizer()
spacy.tests.conftest.id_tokenizer()
spacy.tests.conftest.is_tokenizer()
spacy.tests.conftest.it_tokenizer()
spacy.tests.conftest.it_vocab()
spacy.tests.conftest.ja_tokenizer()
spacy.tests.conftest.ko_tokenizer()
spacy.tests.conftest.ko_tokenizer_tokenizer()
spacy.tests.conftest.ky_tokenizer()
spacy.tests.conftest.la_tokenizer()
spacy.tests.conftest.lb_tokenizer()
spacy.tests.conftest.lg_tokenizer()
spacy.tests.conftest.lt_tokenizer()
spacy.tests.conftest.lv_tokenizer()
spacy.tests.conftest.mk_tokenizer()
spacy.tests.conftest.ml_tokenizer()
spacy.tests.conftest.ms_tokenizer()
spacy.tests.conftest.nb_tokenizer()
spacy.tests.conftest.ne_tokenizer()
spacy.tests.conftest.nl_tokenizer()
spacy.tests.conftest.nl_vocab()
spacy.tests.conftest.nn_tokenizer()
spacy.tests.conftest.pl_tokenizer()
spacy.tests.conftest.pt_tokenizer()
spacy.tests.conftest.pt_vocab()
spacy.tests.conftest.pytest_addoption(parser)
spacy.tests.conftest.pytest_runtest_setup(item)
spacy.tests.conftest.ro_tokenizer()
spacy.tests.conftest.ru_lemmatizer()
spacy.tests.conftest.ru_lookup_lemmatizer()
spacy.tests.conftest.ru_tokenizer()
spacy.tests.conftest.sa_tokenizer()
spacy.tests.conftest.sk_tokenizer()
spacy.tests.conftest.sl_tokenizer()
spacy.tests.conftest.sq_tokenizer()
spacy.tests.conftest.sr_tokenizer()
spacy.tests.conftest.sv_tokenizer()
spacy.tests.conftest.ta_tokenizer()
spacy.tests.conftest.th_tokenizer()
spacy.tests.conftest.ti_tokenizer()
spacy.tests.conftest.tl_tokenizer()
spacy.tests.conftest.tokenizer()
spacy.tests.conftest.tr_tokenizer()
spacy.tests.conftest.tt_tokenizer()
spacy.tests.conftest.uk_lemmatizer()
spacy.tests.conftest.uk_lookup_lemmatizer()
spacy.tests.conftest.uk_tokenizer()
spacy.tests.conftest.ur_tokenizer()
spacy.tests.conftest.vi_tokenizer()
spacy.tests.conftest.xx_tokenizer()
spacy.tests.conftest.yo_tokenizer()
spacy.tests.conftest.zh_tokenizer_char()
spacy.tests.conftest.zh_tokenizer_jieba()
spacy.tests.conftest.zh_tokenizer_pkuseg()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/test_models.py----------------------------------------
A:spacy.tests.test_models.nlp->English()
A:spacy.tests.test_models.dY->get_gradient(model, Y)
A:spacy.tests.test_models.embed->MultiHashEmbed(width=32, rows=[1000, 50, 250], attrs=['NORM', 'PREFIX', 'SHAPE'], include_static_vectors=False)
A:spacy.tests.test_models.model1->get_updated_model()
A:spacy.tests.test_models.model2->get_updated_model()
A:spacy.tests.test_models.params1->get_all_params(model1)
A:spacy.tests.test_models.params2->get_all_params(model2)
A:spacy.tests.test_models.Y1->get_updated_model().ops.to_numpy(Y1)
A:spacy.tests.test_models.Y2->get_updated_model().ops.to_numpy(Y2)
A:spacy.tests.test_models.tok2vec1->get_updated_model().get_ref('tok2vec').predict(get_X())
A:spacy.tests.test_models.tok2vec2->get_updated_model().get_ref('tok2vec').predict(get_X())
A:spacy.tests.test_models.y1->get_updated_model().ops.to_numpy(y1)
A:spacy.tests.test_models.y2->get_updated_model().ops.to_numpy(y2)
A:spacy.tests.test_models.optimizer->Adam(0.001)
A:spacy.tests.test_models.model->build_spancat_model(tok2vec, reduce_mean(), chain(Relu(nO=nO), Logistic())).initialize(X=(docs, spans))
A:spacy.tests.test_models.initial_params->get_all_params(model)
A:spacy.tests.test_models.(Y, get_dX)->build_spancat_model(tok2vec, reduce_mean(), chain(Relu(nO=nO), Logistic())).initialize(X=(docs, spans)).begin_update(get_X())
A:spacy.tests.test_models.updated_params->get_all_params(model)
A:spacy.tests.test_models.(output, backprop)->build_spancat_model(tok2vec, reduce_mean(), chain(Relu(nO=nO), Logistic())).initialize(X=(docs, spans)).begin_update(docs)
A:spacy.tests.test_models.spans->Ragged(tok2vec.ops.asarray([[s.start, s.end] for s in spans_list], dtype='i'), tok2vec.ops.asarray(lengths, dtype='i'))
A:spacy.tests.test_models.x_lengths->build_spancat_model(tok2vec, reduce_mean(), chain(Relu(nO=nO), Logistic())).initialize(X=(docs, spans)).ops.asarray([5, 10], dtype='i')
A:spacy.tests.test_models.indices->_get_span_indices(model.ops, spans, x_lengths)
A:spacy.tests.test_models.X->Ragged(model.ops.alloc2f(15, 4), model.ops.asarray([5, 10], dtype='i'))
A:spacy.tests.test_models.(Y, backprop)->model((docs, spans), is_train=True)
A:spacy.tests.test_models.(dX, spans2)->backprop(Y)
A:spacy.tests.test_models.tok2vec->make_test_tok2vec()
A:spacy.tests.test_models.docs->get_docs()
A:spacy.tests.test_models.textcat_reduce->spacy.util.registry.architectures.get('spacy.TextCatReduce.v1')
spacy.tests.test_models.get_all_params(model)
spacy.tests.test_models.get_docs()
spacy.tests.test_models.get_gradient(model,Y)
spacy.tests.test_models.get_textcat_bow_kwargs()
spacy.tests.test_models.get_textcat_cnn_kwargs()
spacy.tests.test_models.get_tok2vec_kwargs()
spacy.tests.test_models.make_test_tok2vec()
spacy.tests.test_models.test_empty_docs(model_func,kwargs)
spacy.tests.test_models.test_extract_spans_forward_backward()
spacy.tests.test_models.test_extract_spans_span_indices()
spacy.tests.test_models.test_init_extract_spans()
spacy.tests.test_models.test_models_initialize_consistently(seed,model_func,kwargs)
spacy.tests.test_models.test_models_predict_consistently(seed,model_func,kwargs,get_X)
spacy.tests.test_models.test_models_update_consistently(seed,dropout,model_func,kwargs,get_X)
spacy.tests.test_models.test_multi_hash_embed()
spacy.tests.test_models.test_spancat_model_forward_backward(nO=5)
spacy.tests.test_models.test_spancat_model_init()
spacy.tests.test_models.test_textcat_reduce_invalid_args()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/test_architectures.py----------------------------------------
A:spacy.tests.test_architectures.arch->spacy.registry.architectures.get('my_test_function')
spacy.tests.test_architectures.test_get_architecture()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/test_pickles.py----------------------------------------
A:spacy.tests.test_pickles.stringstore->StringStore()
A:spacy.tests.test_pickles.data->srsly.pickle_dumps(doc)
A:spacy.tests.test_pickles.unpickled->srsly.pickle_loads(data)
A:spacy.tests.test_pickles.vocab->Vocab(lex_attr_getters={int(NORM): lambda string: string[:-1]}, get_noun_chunks=English.Defaults.syntax_iterators.get('noun_chunks'))
A:spacy.tests.test_pickles.doc->Doc(en_vocab, words=words, deps=deps, heads=heads)
spacy.tests.test_pickles.test_pickle_doc(en_vocab)
spacy.tests.test_pickles.test_pickle_string_store(text1,text2)
spacy.tests.test_pickles.test_pickle_vocab(text1,text2)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/parser/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/parser/test_ner.py----------------------------------------
A:spacy.tests.parser.test_ner.actions->spacy.pipeline._parser_internals.ner.BiluoPushDown.get_actions(entity_types=entity_types)
A:spacy.tests.parser.test_ner.nlp->English()
A:spacy.tests.parser.test_ner.ner->English().add_pipe('beam_ner', config=config)
A:spacy.tests.parser.test_ner.example->spacy.training.Example.from_dict(doc, {'ner': iob})
A:spacy.tests.parser.test_ner.nlp2->spacy.util.load_model_from_path(tmp_dir)
A:spacy.tests.parser.test_ner.optimizer->English().initialize()
A:spacy.tests.parser.test_ner.ner2->spacy.util.load_model_from_path(tmp_dir).get_pipe('beam_ner')
A:spacy.tests.parser.test_ner.doc1->nlp1('I live in New York')
A:spacy.tests.parser.test_ner.ruler->English().add_pipe('entity_ruler')
A:spacy.tests.parser.test_ner.doc2->nlp2(test_text)
A:spacy.tests.parser.test_ner.doc->Doc(nlp.vocab, words=tokens)
A:spacy.tests.parser.test_ner.apple_ent->Span(doc, 5, 6, label='MY_ORG')
A:spacy.tests.parser.test_ner.act_classes->BiluoPushDown(vocab.strings, actions, incorrect_spans_key='non_entities').get_oracle_sequence(example)
A:spacy.tests.parser.test_ner.tsys->BiluoPushDown(vocab.strings, actions, incorrect_spans_key='non_entities')
A:spacy.tests.parser.test_ner.moves->BiluoPushDown(en_vocab.strings)
A:spacy.tests.parser.test_ner.(action, label)->split_bilu_label(tag)
A:spacy.tests.parser.test_ner.nlp1->English()
A:spacy.tests.parser.test_ner.ner1->English().create_pipe('ner', config=config)
A:spacy.tests.parser.test_ner.batches->spacy.util.minibatch(train_examples, size=8)
A:spacy.tests.parser.test_ner.untrained_ner->English().add_pipe('ner')
A:spacy.tests.parser.test_ner.doc3->nlp2(test_text)
A:spacy.tests.parser.test_ner.beams->English().add_pipe('beam_ner', config=config).predict(docs)
A:spacy.tests.parser.test_ner.beams2->spacy.util.load_model_from_path(tmp_dir).get_pipe('beam_ner').predict(docs2)
A:spacy.tests.parser.test_ner.neg_doc->English().make_doc(train_text)
A:spacy.tests.parser.test_ner.neg_ex->Example(neg_doc, neg_doc)
A:spacy.tests.parser.test_ner.neg_span->Span(example.reference, 50, 53, 'ORG')
A:spacy.tests.parser.test_ner.nlp.vocab.lookups->Lookups()
spacy.tests.parser.test_ner.BlockerComponent1(self,nlp,start,end,name='my_blocker')
spacy.tests.parser.test_ner.BlockerComponent1.__init__(self,nlp,start,end,name='my_blocker')
spacy.tests.parser.test_ner.doc(vocab)
spacy.tests.parser.test_ner.entity_annots(doc)
spacy.tests.parser.test_ner.entity_types(entity_annots)
spacy.tests.parser.test_ner.neg_key()
spacy.tests.parser.test_ner.test_accept_blocked_token()
spacy.tests.parser.test_ner.test_beam_ner_scores()
spacy.tests.parser.test_ner.test_beam_overfitting_IO(neg_key)
spacy.tests.parser.test_ner.test_beam_valid_parse(neg_key)
spacy.tests.parser.test_ner.test_block_ner()
spacy.tests.parser.test_ner.test_empty_ner()
spacy.tests.parser.test_ner.test_get_oracle_moves(tsys,doc,entity_annots)
spacy.tests.parser.test_ner.test_issue1967(label)
spacy.tests.parser.test_ner.test_issue2179()
spacy.tests.parser.test_ner.test_issue2385()
spacy.tests.parser.test_ner.test_issue2800()
spacy.tests.parser.test_ner.test_issue3209()
spacy.tests.parser.test_ner.test_issue4267()
spacy.tests.parser.test_ner.test_issue4313()
spacy.tests.parser.test_ner.test_labels_from_BILUO()
spacy.tests.parser.test_ner.test_neg_annotation(neg_key)
spacy.tests.parser.test_ner.test_neg_annotation_conflict(neg_key)
spacy.tests.parser.test_ner.test_negative_sample_key_is_in_config(vocab,entity_types)
spacy.tests.parser.test_ner.test_negative_samples_U_entity(tsys,vocab,neg_key)
spacy.tests.parser.test_ner.test_negative_samples_three_word_input(tsys,vocab,neg_key)
spacy.tests.parser.test_ner.test_negative_samples_two_word_input(tsys,vocab,neg_key)
spacy.tests.parser.test_ner.test_ner_before_ruler()
spacy.tests.parser.test_ner.test_ner_constructor(en_vocab)
spacy.tests.parser.test_ner.test_ner_warns_no_lookups(caplog)
spacy.tests.parser.test_ner.test_oracle_moves_missing_B(en_vocab)
spacy.tests.parser.test_ner.test_oracle_moves_whitespace(en_vocab)
spacy.tests.parser.test_ner.test_overfitting_IO(use_upper)
spacy.tests.parser.test_ner.test_overwrite_token()
spacy.tests.parser.test_ner.test_ruler_before_ner()
spacy.tests.parser.test_ner.test_train_empty()
spacy.tests.parser.test_ner.test_train_negative_deprecated()
spacy.tests.parser.test_ner.tsys(vocab,entity_types)
spacy.tests.parser.test_ner.vocab()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/parser/test_arc_eager_oracle.py----------------------------------------
A:spacy.tests.parser.test_arc_eager_oracle.doc->Doc(Vocab(), words=gold_words)
A:spacy.tests.parser.test_arc_eager_oracle.example->Example(predicted=predicted, reference=reference)
A:spacy.tests.parser.test_arc_eager_oracle.(states, golds, _)->M.init_gold_batch([example])
A:spacy.tests.parser.test_arc_eager_oracle.name->M.class_name(i)
A:spacy.tests.parser.test_arc_eager_oracle.state_costs[name]->M.get_cost(state, gold, i)
A:spacy.tests.parser.test_arc_eager_oracle.moves->ArcEager(vocab.strings, ArcEager.get_actions())
A:spacy.tests.parser.test_arc_eager_oracle.vocab->Vocab()
A:spacy.tests.parser.test_arc_eager_oracle.ae->ArcEager(vocab.strings, ArcEager.get_actions(left_labels=['amod'], right_labels=['pobj']))
A:spacy.tests.parser.test_arc_eager_oracle.(state, cost_history)->get_sequence_costs(arc_eager, words, heads, deps, actions)
A:spacy.tests.parser.test_arc_eager_oracle.parser->DependencyParser(doc.vocab, model)
A:spacy.tests.parser.test_arc_eager_oracle.(heads, deps)->projectivize(heads, deps)
A:spacy.tests.parser.test_arc_eager_oracle.line->line.strip().strip()
A:spacy.tests.parser.test_arc_eager_oracle.(word, dep, head)->line.strip().strip().split()
A:spacy.tests.parser.test_arc_eager_oracle.ae_oracle_actions->arc_eager.get_oracle_sequence(example, _debug=False)
A:spacy.tests.parser.test_arc_eager_oracle.reference->Doc(Vocab(), words=gold_words, deps=gold_deps, heads=gold_heads)
A:spacy.tests.parser.test_arc_eager_oracle.predicted->Doc(reference.vocab, words=['[', 'catalase', ']', ':', 'that', 'is', 'bad'])
spacy.tests.parser.test_arc_eager_oracle.arc_eager(vocab)
spacy.tests.parser.test_arc_eager_oracle.get_sequence_costs(M,words,heads,deps,transitions)
spacy.tests.parser.test_arc_eager_oracle.test_get_oracle_actions()
spacy.tests.parser.test_arc_eager_oracle.test_issue7056()
spacy.tests.parser.test_arc_eager_oracle.test_oracle_bad_tokenization(vocab,arc_eager)
spacy.tests.parser.test_arc_eager_oracle.test_oracle_dev_sentence(vocab,arc_eager)
spacy.tests.parser.test_arc_eager_oracle.test_oracle_four_words(arc_eager,vocab)
spacy.tests.parser.test_arc_eager_oracle.vocab()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/parser/test_state.py----------------------------------------
A:spacy.tests.parser.test_state.state->StateClass(doc)
spacy.tests.parser.test_state.doc(vocab)
spacy.tests.parser.test_state.test_H(doc)
spacy.tests.parser.test_state.test_L(doc)
spacy.tests.parser.test_state.test_R(doc)
spacy.tests.parser.test_state.test_init_state(doc)
spacy.tests.parser.test_state.test_push_pop(doc)
spacy.tests.parser.test_state.test_stack_depth(doc)
spacy.tests.parser.test_state.vocab()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/parser/test_parse.py----------------------------------------
A:spacy.tests.parser.test_parse.parser->English().add_pipe('beam_parser', config=config)
A:spacy.tests.parser.test_parse.sgd->Adam(0.001)
A:spacy.tests.parser.test_parse.doc->English().make_doc(test_text)
A:spacy.tests.parser.test_parse.example->spacy.training.Example.from_dict(doc, {'heads': [1, 1, 3, 3], 'deps': ['left', 'ROOT', 'left', 'ROOT']})
A:spacy.tests.parser.test_parse.tokens->Doc(en_vocab, words=words)
A:spacy.tests.parser.test_parse.nlp->English()
A:spacy.tests.parser.test_parse.optimizer->English().initialize()
A:spacy.tests.parser.test_parse.nlp2->spacy.util.load_model_from_path(tmp_dir)
A:spacy.tests.parser.test_parse.doc2->nlp2(test_text)
A:spacy.tests.parser.test_parse.beams->English().add_pipe('beam_parser', config=config).predict(docs)
A:spacy.tests.parser.test_parse.(head_scores, label_scores)->English().add_pipe('beam_parser', config=config).scored_parses(beams)
A:spacy.tests.parser.test_parse.parser2->spacy.util.load_model_from_path(tmp_dir).get_pipe('beam_parser')
A:spacy.tests.parser.test_parse.beams2->spacy.util.load_model_from_path(tmp_dir).get_pipe('beam_parser').predict(docs2)
A:spacy.tests.parser.test_parse.(head_scores2, label_scores2)->spacy.util.load_model_from_path(tmp_dir).get_pipe('beam_parser').scored_parses(beams2)
spacy.tests.parser.test_parse._parser_example(parser)
spacy.tests.parser.test_parse.parser(vocab)
spacy.tests.parser.test_parse.test_beam_overfitting_IO()
spacy.tests.parser.test_parse.test_beam_parser_scores()
spacy.tests.parser.test_parse.test_incomplete_data(pipe_name)
spacy.tests.parser.test_parse.test_issue2772(en_vocab)
spacy.tests.parser.test_parse.test_issue3830_no_subtok()
spacy.tests.parser.test_parse.test_issue3830_with_subtok()
spacy.tests.parser.test_parse.test_overfitting_IO(pipe_name)
spacy.tests.parser.test_parse.test_parser_arc_eager_finalize_state(en_vocab,en_parser)
spacy.tests.parser.test_parse.test_parser_configs(pipe_name,parser_config)
spacy.tests.parser.test_parse.test_parser_constructor(en_vocab)
spacy.tests.parser.test_parse.test_parser_initial(en_vocab,en_parser)
spacy.tests.parser.test_parse.test_parser_merge_pp(en_vocab)
spacy.tests.parser.test_parse.test_parser_parse_one_word_sentence(en_vocab,en_parser,words)
spacy.tests.parser.test_parse.test_parser_parse_subtrees(en_vocab,en_parser)
spacy.tests.parser.test_parse.test_parser_root(en_vocab)
spacy.tests.parser.test_parse.test_parser_set_sent_starts(en_vocab)
spacy.tests.parser.test_parse.test_partial_annotation(parser)
spacy.tests.parser.test_parse.vocab()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/parser/test_nn_beam.py----------------------------------------
A:spacy.tests.parser.test_nn_beam.aeager->ArcEager(vocab.strings, {})
A:spacy.tests.parser.test_nn_beam.vec->numpy.random.uniform(-0.1, 0.1, (len(doc), vector_size))
A:spacy.tests.parser.test_nn_beam.(states, golds, _)->moves.init_gold_batch(examples)
A:spacy.tests.parser.test_nn_beam.n_state->sum((len(beam) for beam in beam))
A:spacy.tests.parser.test_nn_beam.nlp->Language()
A:spacy.tests.parser.test_nn_beam.parser->Language().add_pipe('beam_parser')
A:spacy.tests.parser.test_nn_beam.doc->Language().make_doc('Australia is a country')
A:spacy.tests.parser.test_nn_beam.beam_density->float(hyp.draw(hypothesis.strategies.floats(0.0, 1.0, width=32)))
A:spacy.tests.parser.test_nn_beam.beam->BeamBatch(moves, states, golds, width=beam_width, density=beam_density)
A:spacy.tests.parser.test_nn_beam.scores->hyp.draw(ndarrays_of_shape((n_state, moves.n_moves)))
spacy.tests.parser.test_nn_beam.batch_size(docs)
spacy.tests.parser.test_nn_beam.beam(moves,examples,beam_width)
spacy.tests.parser.test_nn_beam.beam_density(request)
spacy.tests.parser.test_nn_beam.beam_width()
spacy.tests.parser.test_nn_beam.docs(vocab)
spacy.tests.parser.test_nn_beam.examples(docs)
spacy.tests.parser.test_nn_beam.moves(vocab)
spacy.tests.parser.test_nn_beam.scores(moves,batch_size,beam_width)
spacy.tests.parser.test_nn_beam.states(docs)
spacy.tests.parser.test_nn_beam.test_beam_advance(beam,scores)
spacy.tests.parser.test_nn_beam.test_beam_advance_too_few_scores(beam,scores)
spacy.tests.parser.test_nn_beam.test_beam_density(moves,examples,beam_width,hyp)
spacy.tests.parser.test_nn_beam.test_beam_parse(examples,beam_width)
spacy.tests.parser.test_nn_beam.test_create_beam(beam)
spacy.tests.parser.test_nn_beam.tokvecs(docs,vector_size)
spacy.tests.parser.test_nn_beam.vector_size()
spacy.tests.parser.test_nn_beam.vocab()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/parser/test_neural_parser.py----------------------------------------
A:spacy.tests.parser.test_neural_parser.actions->spacy.pipeline._parser_internals.arc_eager.ArcEager.get_actions(left_labels=['L'], right_labels=['R'])
A:spacy.tests.parser.test_neural_parser.example->spacy.training.Example.from_dict(doc, gold)
spacy.tests.parser.test_neural_parser.arc_eager(vocab)
spacy.tests.parser.test_neural_parser.doc(vocab)
spacy.tests.parser.test_neural_parser.gold(doc)
spacy.tests.parser.test_neural_parser.model(arc_eager,tok2vec,vocab)
spacy.tests.parser.test_neural_parser.parser(vocab,arc_eager)
spacy.tests.parser.test_neural_parser.test_build_model(parser,vocab)
spacy.tests.parser.test_neural_parser.test_can_init_nn_parser(parser)
spacy.tests.parser.test_neural_parser.test_predict_doc(parser,tok2vec,model,doc)
spacy.tests.parser.test_neural_parser.test_predict_doc_beam(parser,model,doc)
spacy.tests.parser.test_neural_parser.test_update_doc(parser,model,doc,gold)
spacy.tests.parser.test_neural_parser.test_update_doc_beam(parser,model,doc,gold)
spacy.tests.parser.test_neural_parser.tok2vec()
spacy.tests.parser.test_neural_parser.vocab()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/parser/test_space_attachment.py----------------------------------------
A:spacy.tests.parser.test_space_attachment.doc->Doc(en_parser.vocab, words=text)
spacy.tests.parser.test_space_attachment.test_parser_sentence_space(en_vocab)
spacy.tests.parser.test_space_attachment.test_parser_space_attachment(en_vocab)
spacy.tests.parser.test_space_attachment.test_parser_space_attachment_intermediate_trailing(en_vocab,en_parser)
spacy.tests.parser.test_space_attachment.test_parser_space_attachment_leading(en_vocab,en_parser)
spacy.tests.parser.test_space_attachment.test_parser_space_attachment_space(en_parser,text,length)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/parser/test_preset_sbd.py----------------------------------------
A:spacy.tests.parser.test_preset_sbd.doc->parser(doc)
A:spacy.tests.parser.test_preset_sbd.parser->DependencyParser(vocab, model)
A:spacy.tests.parser.test_preset_sbd.sgd->Adam(0.001)
A:spacy.tests.parser.test_preset_sbd.example->spacy.training.Example.from_dict(doc, {'heads': [1, 1, 3, 3], 'deps': ['left', 'ROOT', 'left', 'ROOT']})
spacy.tests.parser.test_preset_sbd._parser_example(parser)
spacy.tests.parser.test_preset_sbd.parser(vocab)
spacy.tests.parser.test_preset_sbd.test_no_sentences(parser)
spacy.tests.parser.test_preset_sbd.test_sents_1(parser)
spacy.tests.parser.test_preset_sbd.test_sents_1_2(parser)
spacy.tests.parser.test_preset_sbd.test_sents_1_3(parser)
spacy.tests.parser.test_preset_sbd.vocab()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/parser/test_parse_navigate.py----------------------------------------
A:spacy.tests.parser.test_parse_navigate.doc->Doc(en_vocab, words=words, heads=heads, deps=['dep'] * len(heads))
A:spacy.tests.parser.test_parse_navigate.lefts[head.i]->set()
A:spacy.tests.parser.test_parse_navigate.rights[head.i]->set()
A:spacy.tests.parser.test_parse_navigate.subtree->list(token.subtree)
A:spacy.tests.parser.test_parse_navigate.debug->'\t'.join((token.text, token.right_edge.text, subtree[-1].text, token.right_edge.head.text))
spacy.tests.parser.test_parse_navigate.heads()
spacy.tests.parser.test_parse_navigate.test_parser_parse_navigate_child_consistency(en_vocab,words,heads)
spacy.tests.parser.test_parse_navigate.test_parser_parse_navigate_consistency(en_vocab,words,heads)
spacy.tests.parser.test_parse_navigate.test_parser_parse_navigate_edges(en_vocab,words,heads)
spacy.tests.parser.test_parse_navigate.words()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/parser/test_add_label.py----------------------------------------
A:spacy.tests.parser.test_add_label.parser->_train_parser(parser)
A:spacy.tests.parser.test_add_label.sgd->Adam(0.001)
A:spacy.tests.parser.test_add_label.doc->Doc(nlp.vocab, words=['hello', 'world'], ents=['B-D', 'O'])
A:spacy.tests.parser.test_add_label.example->Example(nlp.make_doc(doc.text), doc)
A:spacy.tests.parser.test_add_label.ner1->EntityRecognizer(Vocab(), model)
A:spacy.tests.parser.test_add_label.ner2->EntityRecognizer(Vocab(), model)
A:spacy.tests.parser.test_add_label.pipe->pipe_cls(Vocab(), model)
A:spacy.tests.parser.test_add_label.pipe_labels->sorted(list(pipe.labels))
A:spacy.tests.parser.test_add_label.nlp->Language()
A:spacy.tests.parser.test_add_label.ner->Language().add_pipe('ner')
spacy.tests.parser.test_add_label._ner_example(ner)
spacy.tests.parser.test_add_label._parser_example(parser)
spacy.tests.parser.test_add_label._train_parser(parser)
spacy.tests.parser.test_add_label.parser(vocab)
spacy.tests.parser.test_add_label.test_add_label(parser)
spacy.tests.parser.test_add_label.test_add_label_deserializes_correctly()
spacy.tests.parser.test_add_label.test_add_label_get_label(pipe_cls,n_moves,model_config)
spacy.tests.parser.test_add_label.test_init_parser(parser)
spacy.tests.parser.test_add_label.test_ner_labels_added_implicitly_on_beam_parse()
spacy.tests.parser.test_add_label.test_ner_labels_added_implicitly_on_greedy_parse()
spacy.tests.parser.test_add_label.test_ner_labels_added_implicitly_on_predict()
spacy.tests.parser.test_add_label.test_ner_labels_added_implicitly_on_update()
spacy.tests.parser.test_add_label.vocab()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/parser/test_nonproj.py----------------------------------------
A:spacy.tests.parser.test_nonproj.doc->Doc(en_vocab, words=words, deps=deco_labels, heads=proj_heads)
A:spacy.tests.parser.test_nonproj.(proj_heads, deco_labels)->spacy.pipeline._parser_internals.nonproj.projectivize(nonproj_tree2, labels2)
A:spacy.tests.parser.test_nonproj.(deproj_heads, undeco_labels)->deprojectivize(proj_heads, deco_labels)
spacy.tests.parser.test_nonproj.cyclic_tree()
spacy.tests.parser.test_nonproj.multirooted_tree()
spacy.tests.parser.test_nonproj.nonproj_tree()
spacy.tests.parser.test_nonproj.partial_tree()
spacy.tests.parser.test_nonproj.proj_tree()
spacy.tests.parser.test_nonproj.test_parser_ancestors(tree,cyclic_tree,partial_tree,multirooted_tree)
spacy.tests.parser.test_nonproj.test_parser_contains_cycle(tree,cyclic_tree,partial_tree,multirooted_tree)
spacy.tests.parser.test_nonproj.test_parser_is_nonproj_arc(cyclic_tree,nonproj_tree,partial_tree,multirooted_tree)
spacy.tests.parser.test_nonproj.test_parser_is_nonproj_tree(proj_tree,cyclic_tree,nonproj_tree,partial_tree,multirooted_tree)
spacy.tests.parser.test_nonproj.test_parser_pseudoprojectivity(en_vocab)
spacy.tests.parser.test_nonproj.tree()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/pipeline/test_lemmatizer.py----------------------------------------
A:spacy.tests.pipeline.test_lemmatizer.lookups->Lookups()
A:spacy.tests.pipeline.test_lemmatizer.nlp->English()
A:spacy.tests.pipeline.test_lemmatizer.lemmatizer->English().add_pipe('lemmatizer', config={'mode': 'rule'})
A:spacy.tests.pipeline.test_lemmatizer.lemmatizer.lookups->Lookups()
A:spacy.tests.pipeline.test_lemmatizer.doc->lemmatizer(doc)
A:spacy.tests.pipeline.test_lemmatizer.nlp2->spacy.util.load_model_from_path(tmp_dir)
A:spacy.tests.pipeline.test_lemmatizer.lemmatizer2->spacy.util.load_model_from_path(tmp_dir).add_pipe('lemmatizer', config={'mode': 'rule'})
A:spacy.tests.pipeline.test_lemmatizer.doc2->lemmatizer2(doc2)
spacy.tests.pipeline.test_lemmatizer.nlp()
spacy.tests.pipeline.test_lemmatizer.test_lemmatizer_config(nlp)
spacy.tests.pipeline.test_lemmatizer.test_lemmatizer_init(nlp)
spacy.tests.pipeline.test_lemmatizer.test_lemmatizer_serialize(nlp)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/pipeline/test_initialize.py----------------------------------------
A:spacy.tests.pipeline.test_initialize.nlp->English()
A:spacy.tests.pipeline.test_initialize.nlp.tokenizer->CustomTokenizer(nlp.tokenizer)
A:spacy.tests.pipeline.test_initialize.example->spacy.training.Example.from_dict(nlp('x'), {})
A:spacy.tests.pipeline.test_initialize.pipe->English().get_pipe(name)
spacy.tests.pipeline.test_initialize.test_initialize_arguments()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/pipeline/test_attributeruler.py----------------------------------------
A:spacy.tests.pipeline.test_attributeruler.doc->nlp(text)
A:spacy.tests.pipeline.test_attributeruler.a->nlp.add_pipe('attribute_ruler')
A:spacy.tests.pipeline.test_attributeruler.ruler->nlp.add_pipe('attribute_ruler')
A:spacy.tests.pipeline.test_attributeruler.scores->nlp.evaluate(dev_examples, scorer_cfg={'weird_score': 0.23456})
A:spacy.tests.pipeline.test_attributeruler.a_reloaded->AttributeRuler(nlp.vocab).from_bytes(a.to_bytes())
A:spacy.tests.pipeline.test_attributeruler.doc1->a_reloaded(nlp.make_doc(text))
A:spacy.tests.pipeline.test_attributeruler.nlp2->spacy.util.load_model_from_path(tmp_dir)
A:spacy.tests.pipeline.test_attributeruler.doc2->nlp2(text)
spacy.tests.pipeline.test_attributeruler.check_morph_rules(ruler)
spacy.tests.pipeline.test_attributeruler.check_tag_map(ruler)
spacy.tests.pipeline.test_attributeruler.morph_rules()
spacy.tests.pipeline.test_attributeruler.nlp()
spacy.tests.pipeline.test_attributeruler.pattern_dicts()
spacy.tests.pipeline.test_attributeruler.tag_map()
spacy.tests.pipeline.test_attributeruler.test_attributeruler_indices(nlp)
spacy.tests.pipeline.test_attributeruler.test_attributeruler_init(nlp,pattern_dicts)
spacy.tests.pipeline.test_attributeruler.test_attributeruler_init_clear(nlp,pattern_dicts)
spacy.tests.pipeline.test_attributeruler.test_attributeruler_init_patterns(nlp,pattern_dicts)
spacy.tests.pipeline.test_attributeruler.test_attributeruler_morph_rules(nlp,morph_rules)
spacy.tests.pipeline.test_attributeruler.test_attributeruler_morph_rules_initialize(nlp,morph_rules)
spacy.tests.pipeline.test_attributeruler.test_attributeruler_patterns_prop(nlp,pattern_dicts)
spacy.tests.pipeline.test_attributeruler.test_attributeruler_rule_order(nlp)
spacy.tests.pipeline.test_attributeruler.test_attributeruler_score(nlp,pattern_dicts)
spacy.tests.pipeline.test_attributeruler.test_attributeruler_serialize(nlp,pattern_dicts)
spacy.tests.pipeline.test_attributeruler.test_attributeruler_tag_map(nlp,tag_map)
spacy.tests.pipeline.test_attributeruler.test_attributeruler_tag_map_initialize(nlp,tag_map)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/pipeline/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/pipeline/test_pipe_factories.py----------------------------------------
A:spacy.tests.pipeline.test_pipe_factories.nlp->spacy.lang.en.English.from_config(config)
A:spacy.tests.pipeline.test_pipe_factories.my_component->spacy.lang.en.English.from_config(config).add_pipe(factory_name, name=pipe_name)
A:spacy.tests.pipeline.test_pipe_factories.nlp2->spacy.load(tmpdir, config=overrides)
A:spacy.tests.pipeline.test_pipe_factories.pipe->spacy.lang.en.English.from_config(config).get_pipe(name)
A:spacy.tests.pipeline.test_pipe_factories.nlp_en->English()
A:spacy.tests.pipeline.test_pipe_factories.nlp_de->German()
A:spacy.tests.pipeline.test_pipe_factories.result->combine_score_weights(weights, override)
A:spacy.tests.pipeline.test_pipe_factories.meta1->spacy.language.Language.get_factory_meta(f'{name}1')
A:spacy.tests.pipeline.test_pipe_factories.meta2->spacy.language.Language.get_factory_meta(f'{name}2')
A:spacy.tests.pipeline.test_pipe_factories.config->spacy.lang.en.English.from_config(config).config.copy()
A:spacy.tests.pipeline.test_pipe_factories.source_nlp->English()
A:spacy.tests.pipeline.test_pipe_factories.stop_words->set(['custom', 'stop'])
A:spacy.tests.pipeline.test_pipe_factories.meta->spacy.lang.en.English.from_config(config).get_pipe_meta('custom')
A:spacy.tests.pipeline.test_pipe_factories.pipe_cfg->spacy.lang.en.English.from_config(config).get_pipe_config(name)
spacy.tests.pipeline.test_pipe_factories.PipeFactoriesIdempotent(self,nlp,name)
spacy.tests.pipeline.test_pipe_factories.PipeFactoriesIdempotent.__init__(self,nlp,name)
spacy.tests.pipeline.test_pipe_factories.test_issue5137()
spacy.tests.pipeline.test_pipe_factories.test_language_factories_combine_score_weights(weights,override,expected)
spacy.tests.pipeline.test_pipe_factories.test_language_factories_invalid()
spacy.tests.pipeline.test_pipe_factories.test_language_factories_scores()
spacy.tests.pipeline.test_pipe_factories.test_pipe_class_component_config()
spacy.tests.pipeline.test_pipe_factories.test_pipe_class_component_defaults()
spacy.tests.pipeline.test_pipe_factories.test_pipe_class_component_init()
spacy.tests.pipeline.test_pipe_factories.test_pipe_class_component_model()
spacy.tests.pipeline.test_pipe_factories.test_pipe_class_component_model_custom()
spacy.tests.pipeline.test_pipe_factories.test_pipe_factories_config_excludes_nlp()
spacy.tests.pipeline.test_pipe_factories.test_pipe_factories_decorator_idempotent(i,func,func2)
spacy.tests.pipeline.test_pipe_factories.test_pipe_factories_empty_dict_default()
spacy.tests.pipeline.test_pipe_factories.test_pipe_factories_from_source()
spacy.tests.pipeline.test_pipe_factories.test_pipe_factories_from_source_config()
spacy.tests.pipeline.test_pipe_factories.test_pipe_factories_from_source_custom()
spacy.tests.pipeline.test_pipe_factories.test_pipe_factories_from_source_language_subclass()
spacy.tests.pipeline.test_pipe_factories.test_pipe_factories_language_specific()
spacy.tests.pipeline.test_pipe_factories.test_pipe_factories_wrong_formats()
spacy.tests.pipeline.test_pipe_factories.test_pipe_factory_meta_config_cleanup()
spacy.tests.pipeline.test_pipe_factories.test_pipe_function_component()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/pipeline/test_morphologizer.py----------------------------------------
A:spacy.tests.pipeline.test_morphologizer.nlp->English()
A:spacy.tests.pipeline.test_morphologizer.morphologizer->English().get_pipe('morphologizer')
A:spacy.tests.pipeline.test_morphologizer.morph_no_ls->English().add_pipe('morphologizer', 'no_label_smoothing')
A:spacy.tests.pipeline.test_morphologizer.morph_ls->English().add_pipe('morphologizer', 'label_smoothing', config=dict(label_smoothing=0.05))
A:spacy.tests.pipeline.test_morphologizer.(tag_scores, bp_tag_scores)->English().add_pipe('morphologizer', 'label_smoothing', config=dict(label_smoothing=0.05)).model.begin_update([eg.predicted for eg in train_examples])
A:spacy.tests.pipeline.test_morphologizer.ops->get_current_ops()
A:spacy.tests.pipeline.test_morphologizer.no_ls_grads->get_current_ops().to_numpy(morph_no_ls.get_loss(train_examples, tag_scores)[1][0])
A:spacy.tests.pipeline.test_morphologizer.ls_grads->get_current_ops().to_numpy(morph_ls.get_loss(train_examples, tag_scores)[1][0])
A:spacy.tests.pipeline.test_morphologizer.optimizer->English().initialize(get_examples=lambda : train_examples)
A:spacy.tests.pipeline.test_morphologizer.doc->nlp(test_text)
A:spacy.tests.pipeline.test_morphologizer.nlp2->spacy.util.load_model_from_path(tmp_dir)
A:spacy.tests.pipeline.test_morphologizer.doc2->nlp2(test_text)
spacy.tests.pipeline.test_morphologizer.test_implicit_label()
spacy.tests.pipeline.test_morphologizer.test_initialize_examples()
spacy.tests.pipeline.test_morphologizer.test_label_smoothing()
spacy.tests.pipeline.test_morphologizer.test_label_types()
spacy.tests.pipeline.test_morphologizer.test_no_label()
spacy.tests.pipeline.test_morphologizer.test_no_resize()
spacy.tests.pipeline.test_morphologizer.test_overfitting_IO()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/pipeline/test_span_ruler.py----------------------------------------
A:spacy.tests.pipeline.test_span_ruler.nlp->spacy.blank('xx')
A:spacy.tests.pipeline.test_span_ruler.ruler->spacy.blank('xx').add_pipe('span_ruler', config={'annotate_ents': True, 'ents_filter': {'@misc': 'test_pass_through_filter'}})
A:spacy.tests.pipeline.test_span_ruler.pattern_count->sum((len(mm) for mm in ruler.matcher._patterns.values()))
A:spacy.tests.pipeline.test_span_ruler.after_count->sum((len(mm) for mm in ruler.matcher._patterns.values()))
A:spacy.tests.pipeline.test_span_ruler.doc->ruler(doc)
A:spacy.tests.pipeline.test_span_ruler.ruler_bytes->spacy.blank('xx').add_pipe('span_ruler', config={'annotate_ents': True, 'ents_filter': {'@misc': 'test_pass_through_filter'}}).to_bytes()
A:spacy.tests.pipeline.test_span_ruler.new_nlp->spacy.blank('xx')
A:spacy.tests.pipeline.test_span_ruler.new_ruler->new_ruler.from_bytes(ruler_bytes).from_bytes(ruler_bytes)
A:spacy.tests.pipeline.test_span_ruler.validated_ruler->spacy.blank('xx').add_pipe('span_ruler', name='validated_span_ruler', config={'validate': True})
A:spacy.tests.pipeline.test_span_ruler.pred_doc->ruler(nlp.make_doc(text))
A:spacy.tests.pipeline.test_span_ruler.ref_doc->spacy.blank('xx').make_doc(text)
A:spacy.tests.pipeline.test_span_ruler.scores->spacy.blank('xx').evaluate([Example(pred_doc, ref_doc)])
spacy.tests.pipeline.test_span_ruler.overlapping_patterns()
spacy.tests.pipeline.test_span_ruler.patterns()
spacy.tests.pipeline.test_span_ruler.person_org_date_patterns(person_org_patterns)
spacy.tests.pipeline.test_span_ruler.person_org_patterns()
spacy.tests.pipeline.test_span_ruler.test_span_ruler_add_empty(patterns)
spacy.tests.pipeline.test_span_ruler.test_span_ruler_clear(patterns)
spacy.tests.pipeline.test_span_ruler.test_span_ruler_ents_bad_filter(overlapping_patterns)
spacy.tests.pipeline.test_span_ruler.test_span_ruler_ents_default_filter(overlapping_patterns)
spacy.tests.pipeline.test_span_ruler.test_span_ruler_ents_overwrite_filter(overlapping_patterns)
spacy.tests.pipeline.test_span_ruler.test_span_ruler_existing(patterns)
spacy.tests.pipeline.test_span_ruler.test_span_ruler_existing_overwrite(patterns)
spacy.tests.pipeline.test_span_ruler.test_span_ruler_init(patterns)
spacy.tests.pipeline.test_span_ruler.test_span_ruler_init_clear(patterns)
spacy.tests.pipeline.test_span_ruler.test_span_ruler_init_patterns(patterns)
spacy.tests.pipeline.test_span_ruler.test_span_ruler_multiprocessing(n_process)
spacy.tests.pipeline.test_span_ruler.test_span_ruler_no_patterns_warns()
spacy.tests.pipeline.test_span_ruler.test_span_ruler_overlapping_spans(overlapping_patterns)
spacy.tests.pipeline.test_span_ruler.test_span_ruler_properties(patterns)
spacy.tests.pipeline.test_span_ruler.test_span_ruler_remove_all_patterns(person_org_date_patterns)
spacy.tests.pipeline.test_span_ruler.test_span_ruler_remove_and_add()
spacy.tests.pipeline.test_span_ruler.test_span_ruler_remove_basic(person_org_patterns)
spacy.tests.pipeline.test_span_ruler.test_span_ruler_remove_nonexisting_pattern(person_org_patterns)
spacy.tests.pipeline.test_span_ruler.test_span_ruler_remove_patterns_in_a_row(person_org_date_patterns)
spacy.tests.pipeline.test_span_ruler.test_span_ruler_remove_several_patterns(person_org_patterns)
spacy.tests.pipeline.test_span_ruler.test_span_ruler_scorer(overlapping_patterns)
spacy.tests.pipeline.test_span_ruler.test_span_ruler_serialize_bytes(patterns)
spacy.tests.pipeline.test_span_ruler.test_span_ruler_serialize_dir(patterns)
spacy.tests.pipeline.test_span_ruler.test_span_ruler_spans_filter(overlapping_patterns)
spacy.tests.pipeline.test_span_ruler.test_span_ruler_validate()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/pipeline/test_spancat.py----------------------------------------
A:spacy.tests.pipeline.test_spancat.OPS->get_current_ops()
A:spacy.tests.pipeline.test_spancat.eg->spacy.training.Example.from_dict(nlp.make_doc(t[0]), t[1])
A:spacy.tests.pipeline.test_spancat.nlp->Language()
A:spacy.tests.pipeline.test_spancat.spancat->Language().add_pipe(name, config={'spans_key': SPAN_KEY})
A:spacy.tests.pipeline.test_spancat.train_examples->make_examples(nlp)
A:spacy.tests.pipeline.test_spancat.doc->nlp(test_text)
A:spacy.tests.pipeline.test_spancat.ngram_suggester->spacy.util.registry.misc.get('spacy.ngram_suggester.v1')(sizes=[1])
A:spacy.tests.pipeline.test_spancat.scores->Language().evaluate(train_examples)
A:spacy.tests.pipeline.test_spancat.spangroup->Language().add_pipe(name, config={'spans_key': SPAN_KEY})._make_span_group_singlelabel(doc, indices, scores, allow_overlap)
A:spacy.tests.pipeline.test_spancat.nlp_single->Language()
A:spacy.tests.pipeline.test_spancat.nlp_multi->Language()
A:spacy.tests.pipeline.test_spancat.spancat_single->Language().add_pipe('spancat', config={'spans_key': SPAN_KEY, 'threshold': 0.1, 'max_positive': 1})
A:spacy.tests.pipeline.test_spancat.spancat_multi->Language().add_pipe('spancat', config={'spans_key': SPAN_KEY, 'threshold': 0.1, 'max_positive': 2})
A:spacy.tests.pipeline.test_spancat.spangroup_multi->Language().add_pipe('spancat', config={'spans_key': SPAN_KEY, 'threshold': 0.1, 'max_positive': 2})._make_span_group_multilabel(doc, indices, scores)
A:spacy.tests.pipeline.test_spancat.spangroup_single->Language().add_pipe('spancat', config={'spans_key': SPAN_KEY, 'threshold': 0.1, 'max_positive': 1})._make_span_group_singlelabel(doc, indices, scores)
A:spacy.tests.pipeline.test_spancat.ngrams->ngram_suggester(docs)
A:spacy.tests.pipeline.test_spancat.spans_set->set()
A:spacy.tests.pipeline.test_spancat.size_suggester->spacy.util.registry.misc.get('spacy.ngram_suggester.v1')(sizes=[1, 2, 3])
A:spacy.tests.pipeline.test_spancat.suggester_factory->spacy.util.registry.misc.get('spacy.ngram_range_suggester.v1')
A:spacy.tests.pipeline.test_spancat.range_suggester->suggester_factory(min_size=2, max_size=4)
A:spacy.tests.pipeline.test_spancat.ngrams_1->size_suggester(docs)
A:spacy.tests.pipeline.test_spancat.ngrams_2->range_suggester(docs)
A:spacy.tests.pipeline.test_spancat.ngrams_3->range_suggester(docs)
A:spacy.tests.pipeline.test_spancat.suggester->spacy.util.registry.misc.get('spacy.preset_spans_suggester.v1')(spans_key=SPAN_KEY)
A:spacy.tests.pipeline.test_spancat.candidates->suggester(docs)
A:spacy.tests.pipeline.test_spancat.optimizer->Language().initialize(get_examples=lambda : train_examples)
A:spacy.tests.pipeline.test_spancat.nlp2->spacy.util.load_model_from_path(tmp_dir)
A:spacy.tests.pipeline.test_spancat.doc2->nlp2(test_text)
A:spacy.tests.pipeline.test_spancat.ops->get_current_ops()
A:spacy.tests.pipeline.test_spancat.spans->get_current_ops().asarray2i(spans)
A:spacy.tests.pipeline.test_spancat.lengths_array->get_current_ops().asarray1i(lengths)
A:spacy.tests.pipeline.test_spancat.output->Ragged(ops.xp.zeros((0, 0), dtype='i'), lengths_array)
A:spacy.tests.pipeline.test_spancat.docs->list(nlp.pipe(texts, n_process=n_process))
spacy.tests.pipeline.test_spancat.make_examples(nlp,data=TRAIN_DATA)
spacy.tests.pipeline.test_spancat.test_doc_gc()
spacy.tests.pipeline.test_spancat.test_explicit_labels(name)
spacy.tests.pipeline.test_spancat.test_implicit_labels(name)
spacy.tests.pipeline.test_spancat.test_make_spangroup_multilabel(max_positive,nr_results)
spacy.tests.pipeline.test_spancat.test_make_spangroup_negative_label()
spacy.tests.pipeline.test_spancat.test_make_spangroup_singlelabel(threshold,allow_overlap,nr_results)
spacy.tests.pipeline.test_spancat.test_ngram_sizes(en_tokenizer)
spacy.tests.pipeline.test_spancat.test_ngram_suggester(en_tokenizer)
spacy.tests.pipeline.test_spancat.test_no_label(name)
spacy.tests.pipeline.test_spancat.test_no_resize(name)
spacy.tests.pipeline.test_spancat.test_overfitting_IO()
spacy.tests.pipeline.test_spancat.test_overfitting_IO_overlapping()
spacy.tests.pipeline.test_spancat.test_preset_spans_suggester()
spacy.tests.pipeline.test_spancat.test_set_candidates(name)
spacy.tests.pipeline.test_spancat.test_spancat_multiprocessing(name,n_process)
spacy.tests.pipeline.test_spancat.test_zero_suggestions(name)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/pipeline/test_functions.py----------------------------------------
A:spacy.tests.pipeline.test_functions.doc->nlp(doc)
A:spacy.tests.pipeline.test_functions.nlp->Language()
A:spacy.tests.pipeline.test_functions.merge_noun_chunks->Language().create_pipe('merge_noun_chunks')
A:spacy.tests.pipeline.test_functions.merge_entities->Language().create_pipe('merge_entities')
A:spacy.tests.pipeline.test_functions.token_splitter->Language().add_pipe('token_splitter', config=config)
spacy.tests.pipeline.test_functions.doc(en_vocab)
spacy.tests.pipeline.test_functions.doc2(en_vocab)
spacy.tests.pipeline.test_functions.test_factories_doc_cleaner()
spacy.tests.pipeline.test_functions.test_factories_merge_ents(doc2)
spacy.tests.pipeline.test_functions.test_factories_merge_noun_chunks(doc2)
spacy.tests.pipeline.test_functions.test_merge_subtokens(doc)
spacy.tests.pipeline.test_functions.test_token_splitter()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/pipeline/test_entity_ruler.py----------------------------------------
A:spacy.tests.pipeline.test_entity_ruler.nlp->English()
A:spacy.tests.pipeline.test_entity_ruler.doc->ruler(nlp.make_doc('I saw him last time we met, this time he brought some flowers, another time some chocolate.'))
A:spacy.tests.pipeline.test_entity_ruler.ruler->English().add_pipe(entity_ruler_factory, name='entity_ruler')
A:spacy.tests.pipeline.test_entity_ruler.ner->EntityRecognizer(doc.vocab, model)
A:spacy.tests.pipeline.test_entity_ruler.pattern_count->sum((len(mm) for mm in ruler.matcher._patterns.values()))
A:spacy.tests.pipeline.test_entity_ruler.after_count->sum((len(mm) for mm in ruler.matcher._patterns.values()))
A:spacy.tests.pipeline.test_entity_ruler.ruler_bytes->English().add_pipe(entity_ruler_factory, name='entity_ruler').to_bytes()
A:spacy.tests.pipeline.test_entity_ruler.new_ruler->new_ruler.from_bytes(ruler_bytes).from_bytes(ruler_bytes)
A:spacy.tests.pipeline.test_entity_ruler.validated_ruler->EntityRuler(nlp, validate=True)
spacy.tests.pipeline.test_entity_ruler.add_ent_component(doc)
spacy.tests.pipeline.test_entity_ruler.nlp()
spacy.tests.pipeline.test_entity_ruler.patterns()
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_cfg_ent_id_sep(nlp,patterns,entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_clear(nlp,patterns,entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_entity_id(nlp,patterns,entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_existing(nlp,patterns,entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_existing_complex(nlp,patterns,entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_existing_overwrite(nlp,patterns,entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_fix8216(nlp,patterns,entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_fuzzy(nlp,entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_fuzzy_disabled(nlp,entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_fuzzy_pipe(nlp,entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_init(nlp,patterns,entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_init_clear(nlp,patterns,entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_init_patterns(nlp,patterns,entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_multiprocessing(nlp,n_process,entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_no_patterns_warns(nlp,entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_overlapping_spans(nlp,entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_properties(nlp,patterns,entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_remove_all_patterns(nlp,entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_remove_and_add(nlp,entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_remove_basic(nlp,entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_remove_nonexisting_pattern(nlp,entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_remove_patterns_in_a_row(nlp,entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_remove_same_id_multiple_patterns(nlp,entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_remove_several_patterns(nlp,entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_serialize_bytes(nlp,patterns,entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_serialize_dir(nlp,patterns,entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_serialize_jsonl(nlp,patterns,entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_serialize_phrase_matcher_attr_bytes(nlp,patterns,entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_entity_ruler_validate(nlp,entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_issue3345(entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_issue4849(entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_issue5918(entity_ruler_factory)
spacy.tests.pipeline.test_entity_ruler.test_issue8168(entity_ruler_factory)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/pipeline/test_edit_tree_lemmatizer.py----------------------------------------
A:spacy.tests.pipeline.test_edit_tree_lemmatizer.nlp->English()
A:spacy.tests.pipeline.test_edit_tree_lemmatizer.lemmatizer->English().add_pipe('trainable_lemmatizer')
A:spacy.tests.pipeline.test_edit_tree_lemmatizer.nlp2->English()
A:spacy.tests.pipeline.test_edit_tree_lemmatizer.lemmatizer2->English().add_pipe('trainable_lemmatizer')
A:spacy.tests.pipeline.test_edit_tree_lemmatizer.optimizer->English().initialize(get_examples=lambda : train_examples)
A:spacy.tests.pipeline.test_edit_tree_lemmatizer.doc->nlp(test_text)
A:spacy.tests.pipeline.test_edit_tree_lemmatizer.(scores, _)->English().add_pipe('trainable_lemmatizer').model([eg.predicted for eg in train_examples], is_train=True)
A:spacy.tests.pipeline.test_edit_tree_lemmatizer.(_, dX)->English().add_pipe('trainable_lemmatizer').get_loss(train_examples, scores)
A:spacy.tests.pipeline.test_edit_tree_lemmatizer.doc2->nlp2(test_text)
A:spacy.tests.pipeline.test_edit_tree_lemmatizer.nlp_bytes->pickle.dumps(nlp)
A:spacy.tests.pipeline.test_edit_tree_lemmatizer.nlp3->English()
A:spacy.tests.pipeline.test_edit_tree_lemmatizer.doc3->nlp3(test_text)
A:spacy.tests.pipeline.test_edit_tree_lemmatizer.nlp4->pickle.loads(nlp_bytes)
A:spacy.tests.pipeline.test_edit_tree_lemmatizer.doc4->nlp4(test_text)
A:spacy.tests.pipeline.test_edit_tree_lemmatizer.strings->StringStore()
A:spacy.tests.pipeline.test_edit_tree_lemmatizer.trees->EditTrees(strings)
A:spacy.tests.pipeline.test_edit_tree_lemmatizer.tree->EditTrees(strings).add(form, lemma)
A:spacy.tests.pipeline.test_edit_tree_lemmatizer.b->EditTrees(strings).to_bytes()
A:spacy.tests.pipeline.test_edit_tree_lemmatizer.trees2->trees2.from_disk(trees_file).from_disk(trees_file)
A:spacy.tests.pipeline.test_edit_tree_lemmatizer.tree3->EditTrees(strings).add('deelt', 'delen')
A:spacy.tests.pipeline.test_edit_tree_lemmatizer.no_change->EditTrees(strings).add('xyz', 'xyz')
A:spacy.tests.pipeline.test_edit_tree_lemmatizer.empty->EditTrees(strings).add('', '')
spacy.tests.pipeline.test_edit_tree_lemmatizer.test_dutch()
spacy.tests.pipeline.test_edit_tree_lemmatizer.test_empty_strings()
spacy.tests.pipeline.test_edit_tree_lemmatizer.test_from_to_bytes()
spacy.tests.pipeline.test_edit_tree_lemmatizer.test_from_to_disk()
spacy.tests.pipeline.test_edit_tree_lemmatizer.test_incomplete_data(top_k)
spacy.tests.pipeline.test_edit_tree_lemmatizer.test_initialize_examples()
spacy.tests.pipeline.test_edit_tree_lemmatizer.test_initialize_from_labels()
spacy.tests.pipeline.test_edit_tree_lemmatizer.test_lemmatizer_label_data()
spacy.tests.pipeline.test_edit_tree_lemmatizer.test_lemmatizer_requires_labels()
spacy.tests.pipeline.test_edit_tree_lemmatizer.test_no_data(top_k)
spacy.tests.pipeline.test_edit_tree_lemmatizer.test_overfitting_IO(top_k)
spacy.tests.pipeline.test_edit_tree_lemmatizer.test_roundtrip(form,lemma)
spacy.tests.pipeline.test_edit_tree_lemmatizer.test_roundtrip_small_alphabet(form,lemma)
spacy.tests.pipeline.test_edit_tree_lemmatizer.test_unapplicable_trees()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/pipeline/test_tok2vec.py----------------------------------------
A:spacy.tests.pipeline.test_tok2vec.vocab->Vocab()
A:spacy.tests.pipeline.test_tok2vec.doc->nlp(test_text)
A:spacy.tests.pipeline.test_tok2vec.tok2vec->spacy.util.load_model_from_config(orig_config, auto_fill=True, validate=True).get_pipe('tok2vec')
A:spacy.tests.pipeline.test_tok2vec.(vectors, backprop)->spacy.util.load_model_from_config(orig_config, auto_fill=True, validate=True).get_pipe('tok2vec').begin_update(docs)
A:spacy.tests.pipeline.test_tok2vec.batch->get_batch(batch_size)
A:spacy.tests.pipeline.test_tok2vec.embed->spacy.util.registry.get('architectures', embed_arch)
A:spacy.tests.pipeline.test_tok2vec.encode->spacy.util.registry.get('architectures', encode_arch)
A:spacy.tests.pipeline.test_tok2vec.tok2vec_model->spacy.util.registry.get('architectures', tok2vec_arch)
A:spacy.tests.pipeline.test_tok2vec.docs->list(nlp.pipe(['Eat blue ham', 'I like green eggs']))
A:spacy.tests.pipeline.test_tok2vec.nlp->spacy.util.load_model_from_config(orig_config, auto_fill=True, validate=True)
A:spacy.tests.pipeline.test_tok2vec.orig_config->Config().from_str(cfg_string_multi)
A:spacy.tests.pipeline.test_tok2vec.ops->get_current_ops()
A:spacy.tests.pipeline.test_tok2vec.tagger->spacy.util.load_model_from_config(orig_config, auto_fill=True, validate=True).get_pipe('tagger')
A:spacy.tests.pipeline.test_tok2vec.tagger_tok2vec->spacy.util.load_model_from_config(orig_config, auto_fill=True, validate=True).get_pipe('tagger').model.get_ref('tok2vec')
A:spacy.tests.pipeline.test_tok2vec.optimizer->spacy.util.load_model_from_config(orig_config, auto_fill=True, validate=True).initialize(lambda : train_examples)
A:spacy.tests.pipeline.test_tok2vec.(Y, get_dX)->spacy.util.load_model_from_config(orig_config, auto_fill=True, validate=True).get_pipe('tagger').model.begin_update(docs)
A:spacy.tests.pipeline.test_tok2vec.nlp2->English()
A:spacy.tests.pipeline.test_tok2vec.doc2->nlp2(test_text)
A:spacy.tests.pipeline.test_tok2vec.ner->spacy.util.load_model_from_config(new_config, auto_fill=True).get_pipe('ner3')
A:spacy.tests.pipeline.test_tok2vec.base_model->str(dir_path)
A:spacy.tests.pipeline.test_tok2vec.new_nlp->spacy.util.load_model_from_config(new_config, auto_fill=True)
A:spacy.tests.pipeline.test_tok2vec.textcat->spacy.util.load_model_from_config(orig_config, auto_fill=True, validate=True).get_pipe('textcat_multilabel')
A:spacy.tests.pipeline.test_tok2vec.textcat_tok2vec->spacy.util.load_model_from_config(orig_config, auto_fill=True, validate=True).get_pipe('textcat_multilabel').model.get_ref('tok2vec')
A:spacy.tests.pipeline.test_tok2vec.nlp1->spacy.util.load_model_from_config(orig_config, auto_fill=True, validate=True)
spacy.tests.pipeline.test_tok2vec.test_empty_doc()
spacy.tests.pipeline.test_tok2vec.test_init_tok2vec()
spacy.tests.pipeline.test_tok2vec.test_replace_listeners()
spacy.tests.pipeline.test_tok2vec.test_replace_listeners_from_config()
spacy.tests.pipeline.test_tok2vec.test_tok2vec_batch_sizes(batch_size,width,embed_size)
spacy.tests.pipeline.test_tok2vec.test_tok2vec_configs(width,tok2vec_arch,embed_arch,embed_config,encode_arch,encode_config)
spacy.tests.pipeline.test_tok2vec.test_tok2vec_frozen_not_annotating()
spacy.tests.pipeline.test_tok2vec.test_tok2vec_frozen_overfitting()
spacy.tests.pipeline.test_tok2vec.test_tok2vec_listener(with_vectors)
spacy.tests.pipeline.test_tok2vec.test_tok2vec_listener_callback()
spacy.tests.pipeline.test_tok2vec.test_tok2vec_listener_overfitting()
spacy.tests.pipeline.test_tok2vec.test_tok2vec_listener_source_link_name()
spacy.tests.pipeline.test_tok2vec.test_tok2vec_listener_source_replace_listeners()
spacy.tests.pipeline.test_tok2vec.test_tok2vec_listeners_textcat()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/pipeline/test_pipe_methods.py----------------------------------------
A:spacy.tests.pipeline.test_pipe_methods.nlp->spacy.load(tmp_dir, enable=to_enable, disable=[comp_name for comp_name in nlp.component_names if comp_name not in to_enable])
A:spacy.tests.pipeline.test_pipe_methods.nlp2->Language(Vocab())
A:spacy.tests.pipeline.test_pipe_methods.array1->numpy.asarray([0.1, 0.5, 0.8], dtype=numpy.float32)
A:spacy.tests.pipeline.test_pipe_methods.array2->numpy.asarray([-0.2, -0.6, -0.9], dtype=numpy.float32)
A:spacy.tests.pipeline.test_pipe_methods.array3->numpy.asarray([0.3, -0.1, 0.7], dtype=numpy.float32)
A:spacy.tests.pipeline.test_pipe_methods.array4->numpy.asarray([0.5, 0, 0.3], dtype=numpy.float32)
A:spacy.tests.pipeline.test_pipe_methods.array34->numpy.asarray([0.4, -0.05, 0.5], dtype=numpy.float32)
A:spacy.tests.pipeline.test_pipe_methods.ruler->spacy.load(tmp_dir, enable=to_enable, disable=[comp_name for comp_name in nlp.component_names if comp_name not in to_enable]).add_pipe('entity_ruler')
A:spacy.tests.pipeline.test_pipe_methods.ops->get_current_ops()
A:spacy.tests.pipeline.test_pipe_methods.vocab->Vocab(strings=words)
A:spacy.tests.pipeline.test_pipe_methods.en_doc->Doc(vocab, words=words, pos=pos, heads=heads, deps=deps)
A:spacy.tests.pipeline.test_pipe_methods.merge_nps->spacy.load(tmp_dir, enable=to_enable, disable=[comp_name for comp_name in nlp.component_names if comp_name not in to_enable]).create_pipe('merge_noun_chunks')
A:spacy.tests.pipeline.test_pipe_methods.doc->spacy.load(tmp_dir, enable=to_enable, disable=[comp_name for comp_name in nlp.component_names if comp_name not in to_enable]).make_doc('foo')
A:spacy.tests.pipeline.test_pipe_methods.dummy_pipe->DummyPipe()
A:spacy.tests.pipeline.test_pipe_methods.(removed_name, removed_component)->spacy.load(tmp_dir, enable=to_enable, disable=[comp_name for comp_name in nlp.component_names if comp_name not in to_enable]).remove_pipe(name)
A:spacy.tests.pipeline.test_pipe_methods.disabled->spacy.load(tmp_dir, enable=to_enable, disable=[comp_name for comp_name in nlp.component_names if comp_name not in to_enable]).select_pipes(disable=['c2'])
A:spacy.tests.pipeline.test_pipe_methods.pipe->spacy.load(tmp_dir, enable=to_enable, disable=[comp_name for comp_name in nlp.component_names if comp_name not in to_enable]).add_pipe(pipe)
A:spacy.tests.pipeline.test_pipe_methods.c1->spacy.language.Language.component(f'{name}1', func=make_component(f'{name}1'))
A:spacy.tests.pipeline.test_pipe_methods.c2->spacy.language.Language.component(f'{name}2', func=make_component(f'{name}2'))
A:spacy.tests.pipeline.test_pipe_methods.ner->spacy.load(tmp_dir, enable=to_enable, disable=[comp_name for comp_name in nlp.component_names if comp_name not in to_enable]).add_pipe('ner')
A:spacy.tests.pipeline.test_pipe_methods.initialize->getattr(pipe, 'initialize', None)
A:spacy.tests.pipeline.test_pipe_methods.components->set([f'{name}1', f'{name}2'])
A:spacy.tests.pipeline.test_pipe_methods.base_nlp->English()
spacy.tests.pipeline.test_pipe_methods.new_pipe(doc)
spacy.tests.pipeline.test_pipe_methods.nlp()
spacy.tests.pipeline.test_pipe_methods.other_pipe(doc)
spacy.tests.pipeline.test_pipe_methods.test_add_lots_of_pipes(nlp,n_pipes)
spacy.tests.pipeline.test_pipe_methods.test_add_pipe_before_after()
spacy.tests.pipeline.test_pipe_methods.test_add_pipe_duplicate_name(nlp)
spacy.tests.pipeline.test_pipe_methods.test_add_pipe_first(nlp,name)
spacy.tests.pipeline.test_pipe_methods.test_add_pipe_last(nlp,name1,name2)
spacy.tests.pipeline.test_pipe_methods.test_add_pipe_no_name(nlp)
spacy.tests.pipeline.test_pipe_methods.test_cant_add_pipe_first_and_last(nlp)
spacy.tests.pipeline.test_pipe_methods.test_disable_enable_pipes()
spacy.tests.pipeline.test_pipe_methods.test_disable_pipes_context(nlp,name)
spacy.tests.pipeline.test_pipe_methods.test_disable_pipes_context_restore(nlp,name)
spacy.tests.pipeline.test_pipe_methods.test_disable_pipes_method(nlp,name)
spacy.tests.pipeline.test_pipe_methods.test_enable_disable_conflict_with_config()
spacy.tests.pipeline.test_pipe_methods.test_enable_pipes_method(nlp,name)
spacy.tests.pipeline.test_pipe_methods.test_get_pipe(nlp,name)
spacy.tests.pipeline.test_pipe_methods.test_issue1506()
spacy.tests.pipeline.test_pipe_methods.test_issue1654()
spacy.tests.pipeline.test_pipe_methods.test_issue3880()
spacy.tests.pipeline.test_pipe_methods.test_issue5082()
spacy.tests.pipeline.test_pipe_methods.test_issue5458()
spacy.tests.pipeline.test_pipe_methods.test_load_disable_enable()
spacy.tests.pipeline.test_pipe_methods.test_multiple_predictions()
spacy.tests.pipeline.test_pipe_methods.test_pipe_base_class_add_label(nlp,component)
spacy.tests.pipeline.test_pipe_methods.test_pipe_label_data_exports_labels(pipe)
spacy.tests.pipeline.test_pipe_methods.test_pipe_label_data_no_labels(pipe)
spacy.tests.pipeline.test_pipe_methods.test_pipe_labels(nlp)
spacy.tests.pipeline.test_pipe_methods.test_pipe_methods_frozen()
spacy.tests.pipeline.test_pipe_methods.test_pipe_methods_initialize()
spacy.tests.pipeline.test_pipe_methods.test_raise_for_invalid_components(nlp,component)
spacy.tests.pipeline.test_pipe_methods.test_remove_pipe(nlp,name)
spacy.tests.pipeline.test_pipe_methods.test_rename_pipe(nlp,old_name,new_name)
spacy.tests.pipeline.test_pipe_methods.test_replace_last_pipe(nlp)
spacy.tests.pipeline.test_pipe_methods.test_replace_pipe(nlp,name,replacement,invalid_replacement)
spacy.tests.pipeline.test_pipe_methods.test_replace_pipe_config(nlp)
spacy.tests.pipeline.test_pipe_methods.test_select_pipes_errors(nlp)
spacy.tests.pipeline.test_pipe_methods.test_select_pipes_list_arg(nlp)
spacy.tests.pipeline.test_pipe_methods.test_update_with_annotates()
spacy.tests.pipeline.test_pipe_methods.test_warning_pipe_begin_training()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/pipeline/test_analysis.py----------------------------------------
A:spacy.tests.pipeline.test_analysis.nlp->Language()
A:spacy.tests.pipeline.test_analysis.test_component4_meta->Language().get_pipe_meta('c1')
A:spacy.tests.pipeline.test_analysis.mock->Mock()
A:spacy.tests.pipeline.test_analysis.mock.return_value->TestComponent5()
spacy.tests.pipeline.test_analysis.test_analysis_validate_attrs_invalid(attr)
spacy.tests.pipeline.test_analysis.test_analysis_validate_attrs_remove_pipe()
spacy.tests.pipeline.test_analysis.test_analysis_validate_attrs_valid()
spacy.tests.pipeline.test_analysis.test_component_decorator_assigns()
spacy.tests.pipeline.test_analysis.test_component_factories_class_func()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/pipeline/test_sentencizer.py----------------------------------------
A:spacy.tests.pipeline.test_sentencizer.doc->nlp(text)
A:spacy.tests.pipeline.test_sentencizer.sentencizer->Sentencizer(punct_chars=punct_chars)
A:spacy.tests.pipeline.test_sentencizer.nlp->spacy.blank(lang)
A:spacy.tests.pipeline.test_sentencizer.bytes_data->Sentencizer(punct_chars=punct_chars).to_bytes()
A:spacy.tests.pipeline.test_sentencizer.new_sentencizer->Sentencizer(punct_chars=None).from_bytes(bytes_data)
spacy.tests.pipeline.test_sentencizer.test_sentencizer(en_vocab)
spacy.tests.pipeline.test_sentencizer.test_sentencizer_across_scripts(lang,text)
spacy.tests.pipeline.test_sentencizer.test_sentencizer_complex(en_vocab,words,sent_starts,sent_ends,n_sents)
spacy.tests.pipeline.test_sentencizer.test_sentencizer_custom_punct(en_vocab,punct_chars,words,sent_starts,sent_ends,n_sents)
spacy.tests.pipeline.test_sentencizer.test_sentencizer_empty_docs()
spacy.tests.pipeline.test_sentencizer.test_sentencizer_pipe()
spacy.tests.pipeline.test_sentencizer.test_sentencizer_serialize_bytes(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/pipeline/test_annotates_on_update.py----------------------------------------
A:spacy.tests.pipeline.test_annotates_on_update.nlp->load_model_from_config(orig_config, auto_fill=True, validate=True)
A:spacy.tests.pipeline.test_annotates_on_update.doc->load_model_from_config(orig_config, auto_fill=True, validate=True).make_doc(text)
A:spacy.tests.pipeline.test_annotates_on_update.orig_config->Config().from_str(config_str)
spacy.tests.pipeline.test_annotates_on_update.config_str()
spacy.tests.pipeline.test_annotates_on_update.test_annotates_on_update()
spacy.tests.pipeline.test_annotates_on_update.test_annotating_components_from_config(config_str)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/pipeline/test_senter.py----------------------------------------
A:spacy.tests.pipeline.test_senter.nlp->English()
A:spacy.tests.pipeline.test_senter.senter->English().add_pipe('senter')
A:spacy.tests.pipeline.test_senter.optimizer->English().initialize()
A:spacy.tests.pipeline.test_senter.doc->nlp(test_text)
A:spacy.tests.pipeline.test_senter.nlp2->spacy.util.load_model_from_path(tmp_dir)
A:spacy.tests.pipeline.test_senter.doc2->nlp2(test_text)
spacy.tests.pipeline.test_senter.test_initialize_examples()
spacy.tests.pipeline.test_senter.test_label_types()
spacy.tests.pipeline.test_senter.test_overfitting_IO()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/pipeline/test_entity_linker.py----------------------------------------
A:spacy.tests.pipeline.test_entity_linker.nlp->English()
A:spacy.tests.pipeline.test_entity_linker.kb->InMemoryLookupKB(vocab, entity_vector_length=3)
A:spacy.tests.pipeline.test_entity_linker.dir_path->ensure_path(d)
A:spacy.tests.pipeline.test_entity_linker.kb2->InMemoryLookupKB(vocab=nlp.vocab, entity_vector_length=4)
A:spacy.tests.pipeline.test_entity_linker.ruler->English().add_pipe('entity_ruler', before='entity_linker')
A:spacy.tests.pipeline.test_entity_linker.doc->nlp(text)
A:spacy.tests.pipeline.test_entity_linker.example->spacy.training.Example.from_dict(doc, {'entities': entities, 'links': links, 'sent_starts': sent_starts})
A:spacy.tests.pipeline.test_entity_linker.mykb->InMemoryLookupKB(vocab, entity_vector_length=3)
A:spacy.tests.pipeline.test_entity_linker.entity_linker->English().add_pipe('entity_linker', last=True, config={'threshold': 0.99, 'model': config})
A:spacy.tests.pipeline.test_entity_linker.optimizer->English().initialize(get_examples=lambda : train_examples)
A:spacy.tests.pipeline.test_entity_linker.results->English().evaluate(train_examples)
A:spacy.tests.pipeline.test_entity_linker.kb1->InMemoryLookupKB(vocab=nlp.vocab, entity_vector_length=4)
A:spacy.tests.pipeline.test_entity_linker.mykb_new->InMemoryLookupKB(Vocab(), entity_vector_length=1)
A:spacy.tests.pipeline.test_entity_linker.q2_hash->InMemoryLookupKB(vocab, entity_vector_length=3).add_entity(entity='Q2', freq=12, entity_vector=[2])
A:spacy.tests.pipeline.test_entity_linker.adam_hash->InMemoryLookupKB(vocab, entity_vector_length=3).add_alias(alias='adam', entities=['Q2'], probabilities=[0.9])
A:spacy.tests.pipeline.test_entity_linker.candidates->InMemoryLookupKB(Vocab(), entity_vector_length=1).get_alias_candidates('adam')
A:spacy.tests.pipeline.test_entity_linker.kb_new_vocab->InMemoryLookupKB(Vocab(), entity_vector_length=1)
A:spacy.tests.pipeline.test_entity_linker.sent_doc->ent.sent.as_doc()
A:spacy.tests.pipeline.test_entity_linker.boston_ent->Span(doc, 3, 4, label='LOC', kb_id='Q1')
A:spacy.tests.pipeline.test_entity_linker.loc->nlp(text).vocab.strings.add('LOC')
A:spacy.tests.pipeline.test_entity_linker.q1->nlp(text).vocab.strings.add('Q1')
A:spacy.tests.pipeline.test_entity_linker.nlp2->English()
A:spacy.tests.pipeline.test_entity_linker.entity_linker2->English().get_pipe('entity_linker')
A:spacy.tests.pipeline.test_entity_linker.doc2->nlp('x y z')
A:spacy.tests.pipeline.test_entity_linker.eval->English().evaluate(train_examples)
A:spacy.tests.pipeline.test_entity_linker.ner->English().add_pipe('ner', first=True)
A:spacy.tests.pipeline.test_entity_linker.nlp1->English()
A:spacy.tests.pipeline.test_entity_linker.kb_1->InMemoryLookupKB(nlp.vocab, entity_vector_length=3)
A:spacy.tests.pipeline.test_entity_linker.data->spacy.compat.pickle.dumps(nlp_1)
A:spacy.tests.pipeline.test_entity_linker.kb_2->kb_2.from_bytes(kb_bytes).from_bytes(kb_bytes)
A:spacy.tests.pipeline.test_entity_linker.nlp_1->English()
A:spacy.tests.pipeline.test_entity_linker.entity_linker_1->English().add_pipe('entity_linker', last=True)
A:spacy.tests.pipeline.test_entity_linker.nlp_2->nlp_2.from_bytes(nlp_bytes).from_bytes(nlp_bytes)
A:spacy.tests.pipeline.test_entity_linker.entity_linker_2->nlp_2.from_bytes(nlp_bytes).from_bytes(nlp_bytes).get_pipe('entity_linker')
A:spacy.tests.pipeline.test_entity_linker.kb_bytes->InMemoryLookupKB(nlp.vocab, entity_vector_length=3).to_bytes()
A:spacy.tests.pipeline.test_entity_linker.nlp_bytes->English().to_bytes()
A:spacy.tests.pipeline.test_entity_linker.ref1->nlp('Julia lives in London happily.')
A:spacy.tests.pipeline.test_entity_linker.pred1->nlp('Julia lives in London happily.')
A:spacy.tests.pipeline.test_entity_linker.ref2->nlp('She loves London.')
A:spacy.tests.pipeline.test_entity_linker.pred2->nlp('She loves London.')
A:spacy.tests.pipeline.test_entity_linker.ref3->nlp('London is great.')
A:spacy.tests.pipeline.test_entity_linker.pred3->nlp('London is great.')
A:spacy.tests.pipeline.test_entity_linker.scores->Scorer().score_links(train_examples, negative_labels=['NIL'])
A:spacy.tests.pipeline.test_entity_linker.eg.predicted->ruler(eg.predicted)
A:spacy.tests.pipeline.test_entity_linker.doc1->nlp('a b c')
A:spacy.tests.pipeline.test_entity_linker.eg->Example(doc1, doc2)
A:spacy.tests.pipeline.test_entity_linker.span_maker->build_span_maker()
spacy.tests.pipeline.test_entity_linker.assert_almost_equal(a,b)
spacy.tests.pipeline.test_entity_linker.nlp()
spacy.tests.pipeline.test_entity_linker.test_abstract_kb_instantiation()
spacy.tests.pipeline.test_entity_linker.test_append_alias(nlp)
spacy.tests.pipeline.test_entity_linker.test_append_invalid_alias(nlp)
spacy.tests.pipeline.test_entity_linker.test_candidate_generation(nlp)
spacy.tests.pipeline.test_entity_linker.test_el_pipe_configuration(nlp)
spacy.tests.pipeline.test_entity_linker.test_issue4674()
spacy.tests.pipeline.test_entity_linker.test_issue6730(en_vocab)
spacy.tests.pipeline.test_entity_linker.test_issue7065()
spacy.tests.pipeline.test_entity_linker.test_kb_custom_length(nlp)
spacy.tests.pipeline.test_entity_linker.test_kb_default(nlp)
spacy.tests.pipeline.test_entity_linker.test_kb_initialize_empty(nlp)
spacy.tests.pipeline.test_entity_linker.test_kb_invalid_combination(nlp)
spacy.tests.pipeline.test_entity_linker.test_kb_invalid_entities(nlp)
spacy.tests.pipeline.test_entity_linker.test_kb_invalid_entity_vector(nlp)
spacy.tests.pipeline.test_entity_linker.test_kb_invalid_probabilities(nlp)
spacy.tests.pipeline.test_entity_linker.test_kb_pickle()
spacy.tests.pipeline.test_entity_linker.test_kb_serialization()
spacy.tests.pipeline.test_entity_linker.test_kb_serialize(nlp)
spacy.tests.pipeline.test_entity_linker.test_kb_serialize_2(nlp)
spacy.tests.pipeline.test_entity_linker.test_kb_serialize_vocab(nlp)
spacy.tests.pipeline.test_entity_linker.test_kb_set_entities(nlp)
spacy.tests.pipeline.test_entity_linker.test_kb_to_bytes()
spacy.tests.pipeline.test_entity_linker.test_kb_valid_entities(nlp)
spacy.tests.pipeline.test_entity_linker.test_legacy_architectures(name,config)
spacy.tests.pipeline.test_entity_linker.test_nel_nsents(nlp)
spacy.tests.pipeline.test_entity_linker.test_nel_pickle()
spacy.tests.pipeline.test_entity_linker.test_nel_to_bytes()
spacy.tests.pipeline.test_entity_linker.test_no_entities()
spacy.tests.pipeline.test_entity_linker.test_no_gold_ents(patterns)
spacy.tests.pipeline.test_entity_linker.test_overfitting_IO_gold_entities()
spacy.tests.pipeline.test_entity_linker.test_overfitting_IO_with_ner()
spacy.tests.pipeline.test_entity_linker.test_partial_links()
spacy.tests.pipeline.test_entity_linker.test_preserving_links_asdoc(nlp)
spacy.tests.pipeline.test_entity_linker.test_preserving_links_ents(nlp)
spacy.tests.pipeline.test_entity_linker.test_preserving_links_ents_2(nlp)
spacy.tests.pipeline.test_entity_linker.test_scorer_links()
spacy.tests.pipeline.test_entity_linker.test_sentence_crossing_ents(entity_in_first_sentence:bool)
spacy.tests.pipeline.test_entity_linker.test_span_maker_forward_with_empty()
spacy.tests.pipeline.test_entity_linker.test_threshold(meet_threshold:bool,config:Dict[str,Any])
spacy.tests.pipeline.test_entity_linker.test_tokenization_mismatch()
spacy.tests.pipeline.test_entity_linker.test_vocab_serialization(nlp)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/pipeline/test_textcat.py----------------------------------------
A:spacy.tests.pipeline.test_textcat.nlp->English()
A:spacy.tests.pipeline.test_textcat.textcat->English().add_pipe('textcat')
A:spacy.tests.pipeline.test_textcat.optimizer->English().initialize()
A:spacy.tests.pipeline.test_textcat.batches->spacy.util.minibatch(train_data, size=compounding(4.0, 32.0, 1.001))
A:spacy.tests.pipeline.test_textcat.doc->nlp(test_text)
A:spacy.tests.pipeline.test_textcat.pipe_cfg->Config().from_str(textcat_config)
A:spacy.tests.pipeline.test_textcat.pipe->English().add_pipe(component, config=pipe_cfg, last=True)
A:spacy.tests.pipeline.test_textcat.result->English().add_pipe(component, config=pipe_cfg, last=True).model.predict([doc])
A:spacy.tests.pipeline.test_textcat.ops->get_current_ops()
A:spacy.tests.pipeline.test_textcat.out_data->DocBin(docs=[doc]).to_bytes()
A:spacy.tests.pipeline.test_textcat.config_str->config_str.replace('TRAIN_PLACEHOLDER', train_path.as_posix()).replace('TRAIN_PLACEHOLDER', train_path.as_posix())
A:spacy.tests.pipeline.test_textcat.config->spacy.util.load_config_from_str(config_str)
A:spacy.tests.pipeline.test_textcat.get_examples->make_get_examples_multi_label(nlp)
A:spacy.tests.pipeline.test_textcat.examples->example_getter()
A:spacy.tests.pipeline.test_textcat.scores->English().evaluate(train_examples)
A:spacy.tests.pipeline.test_textcat.example_getter->get_examples(nlp)
A:spacy.tests.pipeline.test_textcat.nlp2->spacy.util.load_model_from_path(tmp_dir)
A:spacy.tests.pipeline.test_textcat.doc2->nlp('two')
A:spacy.tests.pipeline.test_textcat.textcat_multilabel->English().add_pipe('textcat_multilabel')
A:spacy.tests.pipeline.test_textcat.ref1->nlp('one')
A:spacy.tests.pipeline.test_textcat.pred1->nlp('one')
A:spacy.tests.pipeline.test_textcat.ref2->nlp('two')
A:spacy.tests.pipeline.test_textcat.pred2->nlp('two')
A:spacy.tests.pipeline.test_textcat.doc1->nlp('one')
A:spacy.tests.pipeline.test_textcat.(loss, d_scores)->English().add_pipe('textcat').get_loss(train_examples, scores)
spacy.tests.pipeline.test_textcat.make_get_examples_multi_label(nlp)
spacy.tests.pipeline.test_textcat.make_get_examples_single_label(nlp)
spacy.tests.pipeline.test_textcat.test_error_with_multi_labels()
spacy.tests.pipeline.test_textcat.test_implicit_label(name,get_examples)
spacy.tests.pipeline.test_textcat.test_initialize_examples(name,get_examples,train_data)
spacy.tests.pipeline.test_textcat.test_invalid_label_value(name,get_examples)
spacy.tests.pipeline.test_textcat.test_issue3611()
spacy.tests.pipeline.test_textcat.test_issue4030()
spacy.tests.pipeline.test_textcat.test_issue5551(textcat_config)
spacy.tests.pipeline.test_textcat.test_issue6908(component_name)
spacy.tests.pipeline.test_textcat.test_issue7019()
spacy.tests.pipeline.test_textcat.test_issue9904()
spacy.tests.pipeline.test_textcat.test_label_types(name)
spacy.tests.pipeline.test_textcat.test_no_label(name)
spacy.tests.pipeline.test_textcat.test_no_resize(name,textcat_config)
spacy.tests.pipeline.test_textcat.test_overfitting_IO()
spacy.tests.pipeline.test_textcat.test_overfitting_IO_multi()
spacy.tests.pipeline.test_textcat.test_positive_class()
spacy.tests.pipeline.test_textcat.test_positive_class_not_binary()
spacy.tests.pipeline.test_textcat.test_positive_class_not_present()
spacy.tests.pipeline.test_textcat.test_resize(name,textcat_config)
spacy.tests.pipeline.test_textcat.test_resize_same_results(name,textcat_config)
spacy.tests.pipeline.test_textcat.test_simple_train()
spacy.tests.pipeline.test_textcat.test_textcat_configs(name,train_data,textcat_config)
spacy.tests.pipeline.test_textcat.test_textcat_eval_missing(multi_label:bool,spring_p:float)
spacy.tests.pipeline.test_textcat.test_textcat_evaluation()
spacy.tests.pipeline.test_textcat.test_textcat_learns_multilabel()
spacy.tests.pipeline.test_textcat.test_textcat_legacy_scorers(component_name,scorer)
spacy.tests.pipeline.test_textcat.test_textcat_loss(multi_label:bool,expected_loss:float)
spacy.tests.pipeline.test_textcat.test_textcat_multi_threshold()
spacy.tests.pipeline.test_textcat.test_textcat_multilabel_threshold()
spacy.tests.pipeline.test_textcat.test_tok2vec_lazy_init(name,textcat_config)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/pipeline/test_models.py----------------------------------------
A:spacy.tests.pipeline.test_models.OPS->get_current_ops()
A:spacy.tests.pipeline.test_models.array->get_current_ops().xp.asarray(l1, dtype='f')
A:spacy.tests.pipeline.test_models.ragged->Ragged(array, OPS.xp.asarray([2, 1], dtype='i'))
A:spacy.tests.pipeline.test_models.vocab->Vocab()
A:spacy.tests.pipeline.test_models.hash_id->Vocab().strings.add(word)
A:spacy.tests.pipeline.test_models.vector->numpy.random.uniform(-1, 1, (7,))
A:spacy.tests.pipeline.test_models.nlp->English()
A:spacy.tests.pipeline.test_models.proc->English().create_pipe(name)
A:spacy.tests.pipeline.test_models.Y_batched->model.predict(in_data).data.tolist()
spacy.tests.pipeline.test_models.get_docs()
spacy.tests.pipeline.test_models.test_components_batching_array(name)
spacy.tests.pipeline.test_models.test_components_batching_list(name)
spacy.tests.pipeline.test_models.test_layers_batching_all(model,in_data,out_data)
spacy.tests.pipeline.test_models.util_batch_unbatch_docs_array(model:Model[List[Doc],Array2d],in_data:List[Doc],out_data:Array2d)
spacy.tests.pipeline.test_models.util_batch_unbatch_docs_list(model:Model[List[Doc],List[Array2d]],in_data:List[Doc],out_data:List[Array2d])
spacy.tests.pipeline.test_models.util_batch_unbatch_docs_ragged(model:Model[List[Doc],Ragged],in_data:List[Doc],out_data:Ragged)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/pipeline/test_span_finder.py----------------------------------------
A:spacy.tests.pipeline.test_span_finder.eg->spacy.training.Example.from_dict(nlp.make_doc(t[0]), t[1])
A:spacy.tests.pipeline.test_span_finder.nlp->English()
A:spacy.tests.pipeline.test_span_finder.predicted->Doc(nlp.vocab, words=tokens_predicted, spaces=[False] * len(tokens_predicted))
A:spacy.tests.pipeline.test_span_finder.reference->Doc(nlp.vocab, words=tokens_reference, spaces=[False] * len(tokens_reference))
A:spacy.tests.pipeline.test_span_finder.example->Example(predicted, reference)
A:spacy.tests.pipeline.test_span_finder.span_finder->English().add_pipe('span_finder', config={'spans_key': SPANS_KEY})
A:spacy.tests.pipeline.test_span_finder.(truth_scores, masks)->English().add_pipe('span_finder', config={'spans_key': SPANS_KEY})._get_aligned_truth_scores([example], ops)
A:spacy.tests.pipeline.test_span_finder.config->Config().from_str(span_finder_default_config).interpolate()
A:spacy.tests.pipeline.test_span_finder.predictions->model.predict(docs)
A:spacy.tests.pipeline.test_span_finder.docs->list(span_finder.pipe(docs))
A:spacy.tests.pipeline.test_span_finder.doc->nlp('London')
A:spacy.tests.pipeline.test_span_finder.max_length->float('inf')
A:spacy.tests.pipeline.test_span_finder.train_examples->make_examples(nlp)
A:spacy.tests.pipeline.test_span_finder.optimizer->English().initialize(get_examples=lambda : train_examples)
A:spacy.tests.pipeline.test_span_finder.nlp2->spacy.util.load_model_from_path(tmp_dir)
A:spacy.tests.pipeline.test_span_finder.doc2->nlp2(test_text)
A:spacy.tests.pipeline.test_span_finder.scores->English().evaluate(train_examples)
spacy.tests.pipeline.test_span_finder.make_examples(nlp,data=TRAIN_DATA)
spacy.tests.pipeline.test_span_finder.test_loss_alignment_example(tokens_predicted,tokens_reference,reference_truths)
spacy.tests.pipeline.test_span_finder.test_overfitting_IO()
spacy.tests.pipeline.test_span_finder.test_set_annotations_span_lengths(min_length,max_length,span_count)
spacy.tests.pipeline.test_span_finder.test_span_finder_component()
spacy.tests.pipeline.test_span_finder.test_span_finder_model()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/pipeline/test_tagger.py----------------------------------------
A:spacy.tests.pipeline.test_tagger.nlp->English()
A:spacy.tests.pipeline.test_tagger.example->spacy.training.Example.from_dict(nlp.make_doc(''), {'tags': []})
A:spacy.tests.pipeline.test_tagger.tagger->English().add_pipe('tagger')
A:spacy.tests.pipeline.test_tagger.optimizer->English().initialize(get_examples=lambda : train_examples)
A:spacy.tests.pipeline.test_tagger.batches->spacy.util.minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))
A:spacy.tests.pipeline.test_tagger.orig_tag_count->len(tagger.labels)
A:spacy.tests.pipeline.test_tagger.tagger_no_ls->English().add_pipe('tagger', 'no_label_smoothing')
A:spacy.tests.pipeline.test_tagger.tagger_ls->English().add_pipe('tagger', 'label_smoothing', config=dict(label_smoothing=0.05))
A:spacy.tests.pipeline.test_tagger.(tag_scores, bp_tag_scores)->English().add_pipe('tagger', 'label_smoothing', config=dict(label_smoothing=0.05)).model.begin_update([eg.predicted for eg in train_examples])
A:spacy.tests.pipeline.test_tagger.ops->get_current_ops()
A:spacy.tests.pipeline.test_tagger.no_ls_grads->get_current_ops().to_numpy(tagger_no_ls.get_loss(train_examples, tag_scores)[1][0])
A:spacy.tests.pipeline.test_tagger.ls_grads->get_current_ops().to_numpy(tagger_ls.get_loss(train_examples, tag_scores)[1][0])
A:spacy.tests.pipeline.test_tagger.doc->nlp(test_text)
A:spacy.tests.pipeline.test_tagger.nlp2->spacy.util.load_model_from_path(tmp_dir)
A:spacy.tests.pipeline.test_tagger.doc2->nlp2(test_text)
A:spacy.tests.pipeline.test_tagger.neg_ex->spacy.training.Example.from_dict(nlp.make_doc(test_text), {'tags': ['!N', 'V', 'J', 'N']})
A:spacy.tests.pipeline.test_tagger.doc3->nlp(test_text)
spacy.tests.pipeline.test_tagger.test_implicit_label()
spacy.tests.pipeline.test_tagger.test_incomplete_data()
spacy.tests.pipeline.test_tagger.test_initialize_examples()
spacy.tests.pipeline.test_tagger.test_issue4348()
spacy.tests.pipeline.test_tagger.test_label_smoothing()
spacy.tests.pipeline.test_tagger.test_label_types()
spacy.tests.pipeline.test_tagger.test_no_data()
spacy.tests.pipeline.test_tagger.test_no_label()
spacy.tests.pipeline.test_tagger.test_no_resize()
spacy.tests.pipeline.test_tagger.test_overfitting_IO()
spacy.tests.pipeline.test_tagger.test_tagger_initialize_tag_map()
spacy.tests.pipeline.test_tagger.test_tagger_requires_labels()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/morphology/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/morphology/test_morph_features.py----------------------------------------
A:spacy.tests.morphology.test_morph_features.tag1->morphology.add({'Case': 'gen', 'Number': 'sing'})
A:spacy.tests.morphology.test_morph_features.tag2->morphology.add({'Number': 'sing', 'Case': 'gen'})
spacy.tests.morphology.test_morph_features.morphology()
spacy.tests.morphology.test_morph_features.test_add_morphology_with_int_ids(morphology)
spacy.tests.morphology.test_morph_features.test_add_morphology_with_mix_strings_and_ints(morphology)
spacy.tests.morphology.test_morph_features.test_add_morphology_with_string_names(morphology)
spacy.tests.morphology.test_morph_features.test_init(morphology)
spacy.tests.morphology.test_morph_features.test_morphology_tags_hash_distinctly(morphology)
spacy.tests.morphology.test_morph_features.test_morphology_tags_hash_independent_of_order(morphology)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/morphology/test_morph_converters.py----------------------------------------
spacy.tests.morphology.test_morph_converters.test_feats_converters()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/morphology/test_morph_pickle.py----------------------------------------
A:spacy.tests.morphology.test_morph_pickle.morphology->Morphology(StringStore())
A:spacy.tests.morphology.test_morph_pickle.b->pickle.dumps(morphology)
A:spacy.tests.morphology.test_morph_pickle.reloaded_morphology->pickle.loads(b)
A:spacy.tests.morphology.test_morph_pickle.feat->pickle.loads(b).get(morphology.strings['Feat3=Val3|Feat4=Val4'])
spacy.tests.morphology.test_morph_pickle.morphology()
spacy.tests.morphology.test_morph_pickle.test_morphology_pickle_roundtrip(morphology)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/test_initialize.py----------------------------------------
A:spacy.tests.lang.test_initialize.nlp->get_lang_class(lang)()
A:spacy.tests.lang.test_initialize.doc->nlp('test')
A:spacy.tests.lang.test_initialize.captured->capfd.readouterr()
spacy.tests.lang.test_initialize.test_lang_initialize(lang,capfd)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/test_attrs.py----------------------------------------
A:spacy.tests.lang.test_attrs.int_attrs->intify_attrs({ENT_IOB: 'XX'})
spacy.tests.lang.test_attrs.test_attrs_do_deprecated(text)
spacy.tests.lang.test_attrs.test_attrs_ent_iob_intify()
spacy.tests.lang.test_attrs.test_attrs_idempotence(text)
spacy.tests.lang.test_attrs.test_attrs_key(text)
spacy.tests.lang.test_attrs.test_issue1889(word)
spacy.tests.lang.test_attrs.test_lex_attrs_is_ascii(text,match)
spacy.tests.lang.test_attrs.test_lex_attrs_is_currency(text,match)
spacy.tests.lang.test_attrs.test_lex_attrs_is_punct(text,match)
spacy.tests.lang.test_attrs.test_lex_attrs_like_url(text,match)
spacy.tests.lang.test_attrs.test_lex_attrs_word_shape(text,shape)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/test_lemmatizers.py----------------------------------------
A:spacy.tests.lang.test_lemmatizers.lookups->Lookups()
A:spacy.tests.lang.test_lemmatizers.lang_cls->get_lang_class(lang)
A:spacy.tests.lang.test_lemmatizers.nlp->lang_cls()
A:spacy.tests.lang.test_lemmatizers.lemmatizer->lang_cls().add_pipe('lemmatizer', config={'mode': 'lookup'})
A:spacy.tests.lang.test_lemmatizers.doc->nlp('x')
A:spacy.tests.lang.test_lemmatizers.captured->capfd.readouterr()
A:spacy.tests.lang.test_lemmatizers.(required, optional)->lang_cls().add_pipe('lemmatizer', config={'mode': 'lookup'}).get_lookups_config(mode)
spacy.tests.lang.test_lemmatizers.test_lemmatizer_initialize(lang,capfd)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/vi/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/vi/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.vi.test_tokenizer.tokens->vi_tokenizer(text)
A:spacy.tests.lang.vi.test_tokenizer.doc->nlp(text)
A:spacy.tests.lang.vi.test_tokenizer.nlp->spacy.lang.vi.Vietnamese.from_config({'nlp': {'tokenizer': {'use_pyvi': False}}})
spacy.tests.lang.vi.test_tokenizer.test_vi_tokenizer(vi_tokenizer,text,expected_tokens)
spacy.tests.lang.vi.test_tokenizer.test_vi_tokenizer_emptyish_texts(vi_tokenizer)
spacy.tests.lang.vi.test_tokenizer.test_vi_tokenizer_extra_spaces(vi_tokenizer)
spacy.tests.lang.vi.test_tokenizer.test_vi_tokenizer_naughty_strings(vi_tokenizer,text)
spacy.tests.lang.vi.test_tokenizer.test_vi_tokenizer_no_pyvi()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/vi/test_serialize.py----------------------------------------
A:spacy.tests.lang.vi.test_serialize.tokenizer_bytes->vi_tokenizer.to_bytes()
A:spacy.tests.lang.vi.test_serialize.nlp->spacy.lang.vi.Vietnamese.from_config({'nlp': {'tokenizer': {'use_pyvi': False}}})
A:spacy.tests.lang.vi.test_serialize.nlp_bytes->spacy.lang.vi.Vietnamese.from_config({'nlp': {'tokenizer': {'use_pyvi': False}}}).to_bytes()
A:spacy.tests.lang.vi.test_serialize.nlp_r->Vietnamese()
A:spacy.tests.lang.vi.test_serialize.b->pickle.dumps(vi_tokenizer)
A:spacy.tests.lang.vi.test_serialize.vi_tokenizer_re->pickle.loads(b)
spacy.tests.lang.vi.test_serialize.test_vi_tokenizer_pickle(vi_tokenizer)
spacy.tests.lang.vi.test_serialize.test_vi_tokenizer_serialize(vi_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/grc/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/grc/test_text.py----------------------------------------
A:spacy.tests.lang.grc.test_text.tokens->grc_tokenizer(text)
spacy.tests.lang.grc.test_text.test_lex_attrs_like_number(grc_tokenizer,text,match)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/grc/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.grc.test_tokenizer.tokens->grc_tokenizer(text)
spacy.tests.lang.grc.test_tokenizer.test_grc_tokenizer(grc_tokenizer,text,expected_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/eu/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/eu/test_text.py----------------------------------------
A:spacy.tests.lang.eu.test_text.tokens->eu_tokenizer(text)
spacy.tests.lang.eu.test_text.test_eu_tokenizer_handles_cnts(eu_tokenizer,text,length)
spacy.tests.lang.eu.test_text.test_eu_tokenizer_handles_long_text(eu_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/ga/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/ga/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.ga.test_tokenizer.tokens->ga_tokenizer(text)
spacy.tests.lang.ga.test_tokenizer.test_ga_tokenizer_handles_exception_cases(ga_tokenizer,text,expected_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/bo/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/bo/test_text.py----------------------------------------
A:spacy.tests.lang.bo.test_text.tokens->bo_tokenizer(text)
spacy.tests.lang.bo.test_text.test_lex_attrs_like_number(bo_tokenizer,text,match)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/lb/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/lb/test_exceptions.py----------------------------------------
A:spacy.tests.lang.lb.test_exceptions.tokens->lb_tokenizer(text)
spacy.tests.lang.lb.test_exceptions.test_lb_tokenizer_handles_abbr(lb_tokenizer,text)
spacy.tests.lang.lb.test_exceptions.test_lb_tokenizer_handles_exc_in_text(lb_tokenizer)
spacy.tests.lang.lb.test_exceptions.test_lb_tokenizer_splits_contractions(lb_tokenizer,text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/lb/test_text.py----------------------------------------
A:spacy.tests.lang.lb.test_text.tokens->lb_tokenizer(text)
spacy.tests.lang.lb.test_text.test_lb_tokenizer_handles_examples(lb_tokenizer,text,length)
spacy.tests.lang.lb.test_text.test_lb_tokenizer_handles_long_text(lb_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/lb/test_prefix_suffix_infix.py----------------------------------------
A:spacy.tests.lang.lb.test_prefix_suffix_infix.tokens->lb_tokenizer(text)
spacy.tests.lang.lb.test_prefix_suffix_infix.test_lb_tokenizer_splits_even_wrap_interact(lb_tokenizer,text)
spacy.tests.lang.lb.test_prefix_suffix_infix.test_lb_tokenizer_splits_prefix_interact(lb_tokenizer,text,length)
spacy.tests.lang.lb.test_prefix_suffix_infix.test_lb_tokenizer_splits_suffix_interact(lb_tokenizer,text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/sa/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/sa/test_text.py----------------------------------------
A:spacy.tests.lang.sa.test_text.tokens->sa_tokenizer(text)
spacy.tests.lang.sa.test_text.test_lex_attrs_like_number(sa_tokenizer,text,match)
spacy.tests.lang.sa.test_text.test_sa_tokenizer_handles_cnts(sa_tokenizer,text,length)
spacy.tests.lang.sa.test_text.test_sa_tokenizer_handles_long_text(sa_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/it/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/it/test_text.py----------------------------------------
A:spacy.tests.lang.it.test_text.doc->it_tokenizer("Vuoi un po' di zucchero?")
spacy.tests.lang.it.test_text.test_issue2822(it_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/it/test_noun_chunks.py----------------------------------------
A:spacy.tests.lang.it.test_noun_chunks.doc->it_tokenizer('Sei andato a Oxford')
spacy.tests.lang.it.test_noun_chunks.test_it_noun_chunks(it_vocab,words,heads,deps,pos,chunk_offsets)
spacy.tests.lang.it.test_noun_chunks.test_noun_chunks_is_parsed_it(it_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/it/test_prefix_suffix_infix.py----------------------------------------
A:spacy.tests.lang.it.test_prefix_suffix_infix.tokens->it_tokenizer(text)
spacy.tests.lang.it.test_prefix_suffix_infix.test_contractions(it_tokenizer,text,expected_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/it/test_stopwords.py----------------------------------------
spacy.tests.lang.it.test_stopwords.test_stopwords_basic(it_tokenizer,word)
spacy.tests.lang.it.test_stopwords.test_stopwords_elided(it_tokenizer,word)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/en/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/en/test_indices.py----------------------------------------
A:spacy.tests.lang.en.test_indices.tokens->en_tokenizer(text)
spacy.tests.lang.en.test_indices.test_en_complex_punct(en_tokenizer)
spacy.tests.lang.en.test_indices.test_en_simple_punct(en_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/en/test_exceptions.py----------------------------------------
A:spacy.tests.lang.en.test_exceptions.tokens->en_tokenizer(text)
A:spacy.tests.lang.en.test_exceptions.tokens_lower->en_tokenizer(text_lower)
A:spacy.tests.lang.en.test_exceptions.tokens_title->en_tokenizer(text_title)
spacy.tests.lang.en.test_exceptions.test_en_lex_attrs_norm_exceptions(en_tokenizer,text,norm)
spacy.tests.lang.en.test_exceptions.test_en_tokenizer_doesnt_split_apos_exc(en_tokenizer,text)
spacy.tests.lang.en.test_exceptions.test_en_tokenizer_excludes_ambiguous(en_tokenizer,exc)
spacy.tests.lang.en.test_exceptions.test_en_tokenizer_handles_abbr(en_tokenizer,text)
spacy.tests.lang.en.test_exceptions.test_en_tokenizer_handles_basic_contraction(en_tokenizer)
spacy.tests.lang.en.test_exceptions.test_en_tokenizer_handles_basic_contraction_punct(en_tokenizer,text)
spacy.tests.lang.en.test_exceptions.test_en_tokenizer_handles_capitalization(en_tokenizer,text_lower,text_title)
spacy.tests.lang.en.test_exceptions.test_en_tokenizer_handles_exc_in_text(en_tokenizer)
spacy.tests.lang.en.test_exceptions.test_en_tokenizer_handles_ll_contraction(en_tokenizer,text)
spacy.tests.lang.en.test_exceptions.test_en_tokenizer_handles_poss_contraction(en_tokenizer,text_poss,text)
spacy.tests.lang.en.test_exceptions.test_en_tokenizer_handles_times(en_tokenizer,text)
spacy.tests.lang.en.test_exceptions.test_en_tokenizer_keeps_title_case(en_tokenizer,pron,contraction)
spacy.tests.lang.en.test_exceptions.test_en_tokenizer_norm_exceptions(en_tokenizer,text,norms)
spacy.tests.lang.en.test_exceptions.test_en_tokenizer_splits_defined_punct(en_tokenizer,wo_punct,w_punct)
spacy.tests.lang.en.test_exceptions.test_en_tokenizer_splits_trailing_apos(en_tokenizer,text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/en/test_parser.py----------------------------------------
A:spacy.tests.lang.en.test_parser.doc->Doc(en_vocab, words=words, pos=pos, deps=deps, heads=heads)
A:spacy.tests.lang.en.test_parser.chunks->list(doc.noun_chunks)
spacy.tests.lang.en.test_parser.test_en_parser_noun_chunks_appositional_modifiers(en_vocab)
spacy.tests.lang.en.test_parser.test_en_parser_noun_chunks_coordinated(en_vocab)
spacy.tests.lang.en.test_parser.test_en_parser_noun_chunks_dative(en_vocab)
spacy.tests.lang.en.test_parser.test_en_parser_noun_chunks_pp_chunks(en_vocab)
spacy.tests.lang.en.test_parser.test_en_parser_noun_chunks_standard(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/en/test_text.py----------------------------------------
A:spacy.tests.lang.en.test_text.tokens->en_tokenizer(text)
spacy.tests.lang.en.test_text.test_en_lex_attrs_capitals(word)
spacy.tests.lang.en.test_text.test_en_lex_attrs_like_number_for_ordinal(word)
spacy.tests.lang.en.test_text.test_en_tokenizer_handles_cnts(en_tokenizer,text,length)
spacy.tests.lang.en.test_text.test_en_tokenizer_handles_long_text(en_tokenizer)
spacy.tests.lang.en.test_text.test_lex_attrs_like_number(en_tokenizer,text,match)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/en/test_punct.py----------------------------------------
A:spacy.tests.lang.en.test_punct.tokens->en_tokenizer(text)
A:spacy.tests.lang.en.test_punct.tokens_punct->en_tokenizer("''")
A:spacy.tests.lang.en.test_punct.match->en_search_prefixes(text)
spacy.tests.lang.en.test_punct.test_en_tokenizer_handles_only_punct(en_tokenizer,text)
spacy.tests.lang.en.test_punct.test_en_tokenizer_splits_bracket_period(en_tokenizer)
spacy.tests.lang.en.test_punct.test_en_tokenizer_splits_close_punct(en_tokenizer,punct,text)
spacy.tests.lang.en.test_punct.test_en_tokenizer_splits_double_end_quote(en_tokenizer,text)
spacy.tests.lang.en.test_punct.test_en_tokenizer_splits_open_appostrophe(en_tokenizer,text)
spacy.tests.lang.en.test_punct.test_en_tokenizer_splits_open_close_punct(en_tokenizer,punct_open,punct_close,text)
spacy.tests.lang.en.test_punct.test_en_tokenizer_splits_open_punct(en_tokenizer,punct,text)
spacy.tests.lang.en.test_punct.test_en_tokenizer_splits_pre_punct_regex(text,punct)
spacy.tests.lang.en.test_punct.test_en_tokenizer_splits_same_close_punct(en_tokenizer,punct,text)
spacy.tests.lang.en.test_punct.test_en_tokenizer_splits_same_open_punct(en_tokenizer,punct,text)
spacy.tests.lang.en.test_punct.test_en_tokenizer_splits_two_diff_close_punct(en_tokenizer,punct,punct_add,text)
spacy.tests.lang.en.test_punct.test_en_tokenizer_splits_two_diff_open_punct(en_tokenizer,punct,punct_add,text)
spacy.tests.lang.en.test_punct.test_en_tokenizer_two_diff_punct(en_tokenizer,punct_open,punct_close,punct_open2,punct_close2,text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/en/test_customized_tokenizer.py----------------------------------------
A:spacy.tests.lang.en.test_customized_tokenizer.prefix_re->compile_prefix_regex(English.Defaults.prefixes)
A:spacy.tests.lang.en.test_customized_tokenizer.suffix_re->compile_suffix_regex(English.Defaults.suffixes)
A:spacy.tests.lang.en.test_customized_tokenizer.infix_re->compile_infix_regex(custom_infixes)
A:spacy.tests.lang.en.test_customized_tokenizer.token_match_re->re.compile('a-b')
spacy.tests.lang.en.test_customized_tokenizer.custom_en_tokenizer(en_vocab)
spacy.tests.lang.en.test_customized_tokenizer.test_en_customized_tokenizer_handles_infixes(custom_en_tokenizer)
spacy.tests.lang.en.test_customized_tokenizer.test_en_customized_tokenizer_handles_rules(custom_en_tokenizer)
spacy.tests.lang.en.test_customized_tokenizer.test_en_customized_tokenizer_handles_rules_property(custom_en_tokenizer)
spacy.tests.lang.en.test_customized_tokenizer.test_en_customized_tokenizer_handles_token_match(custom_en_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/en/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.en.test_tokenizer.doc->es_tokenizer('—Yo me llamo... –murmuró el niño– Emilio Sánchez Pérez.')
A:spacy.tests.lang.en.test_tokenizer.tokens->en_tokenizer(text)
spacy.tests.lang.en.test_tokenizer.test_control_issue792(en_tokenizer,text)
spacy.tests.lang.en.test_tokenizer.test_issue10699(en_tokenizer,text)
spacy.tests.lang.en.test_tokenizer.test_issue1698(en_tokenizer,text)
spacy.tests.lang.en.test_tokenizer.test_issue1758(en_tokenizer)
spacy.tests.lang.en.test_tokenizer.test_issue1773(en_tokenizer)
spacy.tests.lang.en.test_tokenizer.test_issue3277(es_tokenizer)
spacy.tests.lang.en.test_tokenizer.test_issue351(en_tokenizer)
spacy.tests.lang.en.test_tokenizer.test_issue3521(en_tokenizer,word)
spacy.tests.lang.en.test_tokenizer.test_issue360(en_tokenizer)
spacy.tests.lang.en.test_tokenizer.test_issue736(en_tokenizer,text,number)
spacy.tests.lang.en.test_tokenizer.test_issue740(en_tokenizer,text)
spacy.tests.lang.en.test_tokenizer.test_issue744(en_tokenizer,text)
spacy.tests.lang.en.test_tokenizer.test_issue759(en_tokenizer,text,is_num)
spacy.tests.lang.en.test_tokenizer.test_issue775(en_tokenizer,text)
spacy.tests.lang.en.test_tokenizer.test_issue792(en_tokenizer,text)
spacy.tests.lang.en.test_tokenizer.test_issue859(en_tokenizer,text)
spacy.tests.lang.en.test_tokenizer.test_issue886(en_tokenizer,text)
spacy.tests.lang.en.test_tokenizer.test_issue891(en_tokenizer,text)
spacy.tests.lang.en.test_tokenizer.test_issue957(en_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/en/test_noun_chunks.py----------------------------------------
A:spacy.tests.lang.en.test_noun_chunks.doc->en_tokenizer('This is a sentence')
A:spacy.tests.lang.en.test_noun_chunks.chunks->list(doc.noun_chunks)
A:spacy.tests.lang.en.test_noun_chunks.doc_chunks->list(doc.noun_chunks)
A:spacy.tests.lang.en.test_noun_chunks.span_chunks->list(span.noun_chunks)
spacy.tests.lang.en.test_noun_chunks.doc(en_vocab)
spacy.tests.lang.en.test_noun_chunks.test_en_noun_chunks_not_nested(doc,en_vocab)
spacy.tests.lang.en.test_noun_chunks.test_noun_chunks_is_parsed(en_tokenizer)
spacy.tests.lang.en.test_noun_chunks.test_noun_chunks_span(doc,en_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/en/test_prefix_suffix_infix.py----------------------------------------
A:spacy.tests.lang.en.test_prefix_suffix_infix.tokens->en_tokenizer(text)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_comma_infix(en_tokenizer,text)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_double_hyphen_infix(en_tokenizer)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_ellipsis_infix(en_tokenizer,text)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_em_dash_infix(en_tokenizer)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_even_wrap(en_tokenizer,text)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_even_wrap_interact(en_tokenizer,text)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_hyphens(en_tokenizer,text)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_no_punct(en_tokenizer,text)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_no_special(en_tokenizer,text)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_numeric_range(en_tokenizer,text)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_period_abbr(en_tokenizer)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_period_infix(en_tokenizer,text)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_prefix_interact(en_tokenizer,text,length)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_prefix_punct(en_tokenizer,text)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_suffix_interact(en_tokenizer,text)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_suffix_punct(en_tokenizer,text)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_uneven_wrap(en_tokenizer,text)
spacy.tests.lang.en.test_prefix_suffix_infix.test_en_tokenizer_splits_uneven_wrap_interact(en_tokenizer,text)
spacy.tests.lang.en.test_prefix_suffix_infix.test_final_period(en_tokenizer,text,length)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/en/test_sbd.py----------------------------------------
A:spacy.tests.lang.en.test_sbd.doc->Doc(en_vocab, words=words, heads=heads, deps=deps)
A:spacy.tests.lang.en.test_sbd.sents->list(doc.sents)
spacy.tests.lang.en.test_sbd.test_en_sbd_single_punct(en_vocab,words,punct)
spacy.tests.lang.en.test_sbd.test_en_sentence_breaks(en_vocab,en_parser)
spacy.tests.lang.en.test_sbd.test_issue309(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/fi/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/fi/test_text.py----------------------------------------
A:spacy.tests.lang.fi.test_text.tokens->fi_tokenizer(text)
spacy.tests.lang.fi.test_text.test_fi_lex_attrs_like_number(fi_tokenizer,text,match)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/fi/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.fi.test_tokenizer.tokens->fi_tokenizer(text)
spacy.tests.lang.fi.test_tokenizer.test_fi_tokenizer_abbreviation_inflections(fi_tokenizer,text,expected_tokens)
spacy.tests.lang.fi.test_tokenizer.test_fi_tokenizer_abbreviations(fi_tokenizer,text,expected_tokens)
spacy.tests.lang.fi.test_tokenizer.test_fi_tokenizer_contractions(fi_tokenizer,text,expected_tokens,expected_norms)
spacy.tests.lang.fi.test_tokenizer.test_fi_tokenizer_hyphenated_words(fi_tokenizer,text,expected_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/fi/test_noun_chunks.py----------------------------------------
A:spacy.tests.lang.fi.test_noun_chunks.doc->Doc(tokens.vocab, words=[t.text for t in tokens], heads=[head + i for (i, head) in enumerate(heads)], deps=deps, pos=pos)
A:spacy.tests.lang.fi.test_noun_chunks.tokens->fi_tokenizer(text)
A:spacy.tests.lang.fi.test_noun_chunks.noun_chunks->list(doc.noun_chunks)
spacy.tests.lang.fi.test_noun_chunks.test_fi_noun_chunks(fi_tokenizer,text,pos,deps,heads,expected_noun_chunks)
spacy.tests.lang.fi.test_noun_chunks.test_noun_chunks_is_parsed(fi_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/hsb/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/hsb/test_text.py----------------------------------------
A:spacy.tests.lang.hsb.test_text.tokens->hsb_tokenizer(text)
spacy.tests.lang.hsb.test_text.test_lex_attrs_like_number(hsb_tokenizer,text,match)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/hsb/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.hsb.test_tokenizer.tokens->hsb_tokenizer(text)
spacy.tests.lang.hsb.test_tokenizer.test_hsb_tokenizer_basic(hsb_tokenizer,text,expected_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/sq/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/sq/test_text.py----------------------------------------
A:spacy.tests.lang.sq.test_text.tokens->sq_tokenizer(text)
spacy.tests.lang.sq.test_text.test_long_text(sq_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/sq/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.sq.test_tokenizer.tokens->sq_tokenizer(text)
spacy.tests.lang.sq.test_tokenizer.test_sq_tokenizer_basic(sq_tokenizer,text,expected_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/is/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/is/test_text.py----------------------------------------
A:spacy.tests.lang.is.test_text.tokens->is_tokenizer(text)
spacy.tests.lang.is.test_text.test_long_text(is_tokenizer)
spacy.tests.lang.is.test_text.test_ordinal_number(is_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/is/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.is.test_tokenizer.tokens->is_tokenizer(text)
spacy.tests.lang.is.test_tokenizer.test_is_tokenizer_basic(is_tokenizer,text,expected_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/ca/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/ca/test_text.py----------------------------------------
A:spacy.tests.lang.ca.test_text.tokens->ca_tokenizer(text)
spacy.tests.lang.ca.test_text.test_ca_lex_attrs_like_number(ca_tokenizer,text,match)
spacy.tests.lang.ca.test_text.test_ca_tokenizer_handles_cnts(ca_tokenizer,text,length)
spacy.tests.lang.ca.test_text.test_ca_tokenizer_handles_long_text(ca_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/ca/test_exception.py----------------------------------------
A:spacy.tests.lang.ca.test_exception.tokens->ca_tokenizer(text)
A:spacy.tests.lang.ca.test_exception.doc->ca_tokenizer(text)
spacy.tests.lang.ca.test_exception.test_ca_tokenizer_handles_abbr(ca_tokenizer,text,lemma)
spacy.tests.lang.ca.test_exception.test_ca_tokenizer_handles_exc_in_text(ca_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/ca/test_prefix_suffix_infix.py----------------------------------------
A:spacy.tests.lang.ca.test_prefix_suffix_infix.tokens->ca_tokenizer(text)
spacy.tests.lang.ca.test_prefix_suffix_infix.test_contractions(ca_tokenizer,text,expected_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/nb/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/nb/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.nb.test_tokenizer.tokens->nb_tokenizer(text)
spacy.tests.lang.nb.test_tokenizer.test_nb_tokenizer_handles_exception_cases(nb_tokenizer,text,expected_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/nb/test_noun_chunks.py----------------------------------------
A:spacy.tests.lang.nb.test_noun_chunks.doc->nb_tokenizer('Smørsausen brukes bl.a. til')
spacy.tests.lang.nb.test_noun_chunks.test_noun_chunks_is_parsed_nb(nb_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/ky/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/ky/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.ky.test_tokenizer.tokens->ky_tokenizer(text)
spacy.tests.lang.ky.test_tokenizer.test_ky_tokenizer_handles_norm_exceptions(ky_tokenizer,text,norms)
spacy.tests.lang.ky.test_tokenizer.test_ky_tokenizer_handles_testcases(ky_tokenizer,text,expected_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/xx/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/xx/test_text.py----------------------------------------
A:spacy.tests.lang.xx.test_text.tokens->xx_tokenizer(text)
spacy.tests.lang.xx.test_text.test_long_text(xx_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/xx/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.xx.test_tokenizer.tokens->xx_tokenizer(text)
spacy.tests.lang.xx.test_tokenizer.test_xx_tokenizer_basic(xx_tokenizer,text,expected_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/bn/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/bn/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.bn.test_tokenizer.tokens->bn_tokenizer(text)
spacy.tests.lang.bn.test_tokenizer.test_bn_tokenizer_handles_long_text(bn_tokenizer)
spacy.tests.lang.bn.test_tokenizer.test_bn_tokenizer_handles_testcases(bn_tokenizer,text,expected_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/lt/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/lt/test_text.py----------------------------------------
A:spacy.tests.lang.lt.test_text.tokens->lt_tokenizer(text)
spacy.tests.lang.lt.test_text.test_lt_lex_attrs_like_number(lt_tokenizer,text,match)
spacy.tests.lang.lt.test_text.test_lt_tokenizer_abbrev_exceptions(lt_tokenizer,text)
spacy.tests.lang.lt.test_text.test_lt_tokenizer_handles_long_text(lt_tokenizer)
spacy.tests.lang.lt.test_text.test_lt_tokenizer_handles_punct_abbrev(lt_tokenizer,text,length)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/da/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/da/test_exceptions.py----------------------------------------
A:spacy.tests.lang.da.test_exceptions.tokens->da_tokenizer(text)
spacy.tests.lang.da.test_exceptions.test_da_tokenizer_handles_abbr(da_tokenizer,text)
spacy.tests.lang.da.test_exceptions.test_da_tokenizer_handles_ambiguous_abbr(da_tokenizer,text)
spacy.tests.lang.da.test_exceptions.test_da_tokenizer_handles_custom_base_exc(da_tokenizer)
spacy.tests.lang.da.test_exceptions.test_da_tokenizer_handles_dates(da_tokenizer,text)
spacy.tests.lang.da.test_exceptions.test_da_tokenizer_handles_exc_in_text(da_tokenizer)
spacy.tests.lang.da.test_exceptions.test_da_tokenizer_slash(da_tokenizer,text,n_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/da/test_text.py----------------------------------------
A:spacy.tests.lang.da.test_text.tokens->da_tokenizer(text)
spacy.tests.lang.da.test_text.test_da_lex_attrs_capitals(word)
spacy.tests.lang.da.test_text.test_da_tokenizer_handles_long_text(da_tokenizer)
spacy.tests.lang.da.test_text.test_lex_attrs_like_number(da_tokenizer,text,match)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/da/test_noun_chunks.py----------------------------------------
A:spacy.tests.lang.da.test_noun_chunks.doc->Doc(tokens.vocab, words=[t.text for t in tokens], heads=[head + i for (i, head) in enumerate(heads)], deps=deps, pos=pos)
A:spacy.tests.lang.da.test_noun_chunks.tokens->da_tokenizer(text)
A:spacy.tests.lang.da.test_noun_chunks.noun_chunks->list(doc.noun_chunks)
spacy.tests.lang.da.test_noun_chunks.test_da_noun_chunks(da_tokenizer,text,pos,deps,heads,expected_noun_chunks)
spacy.tests.lang.da.test_noun_chunks.test_noun_chunks_is_parsed(da_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/da/test_prefix_suffix_infix.py----------------------------------------
A:spacy.tests.lang.da.test_prefix_suffix_infix.tokens->da_tokenizer("'DBA's, Lars' og Liz' bil sku' sgu' ik' ha' en bule, det ka' han ik' li' mere', sagde hun.")
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_handles_no_punct(da_tokenizer,text)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_handles_numeric_range(da_tokenizer,text)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_handles_posessives_and_contractions(da_tokenizer)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_keeps_hyphens(da_tokenizer,text)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_splits_comma_infix(da_tokenizer,text)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_splits_double_hyphen_infix(da_tokenizer)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_splits_ellipsis_infix(da_tokenizer,text)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_splits_even_wrap(da_tokenizer,text,expected)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_splits_even_wrap_interact(da_tokenizer,text)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_splits_no_special(da_tokenizer,text)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_splits_period_infix(da_tokenizer,text)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_splits_prefix_interact(da_tokenizer,text,expected)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_splits_prefix_punct(da_tokenizer,text)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_splits_suffix_interact(da_tokenizer,text)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_splits_suffix_punct(da_tokenizer,text)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_splits_uneven_wrap(da_tokenizer,text)
spacy.tests.lang.da.test_prefix_suffix_infix.test_da_tokenizer_splits_uneven_wrap_interact(da_tokenizer,text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/ti/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/ti/test_text.py----------------------------------------
A:spacy.tests.lang.ti.test_text.tokens->ti_tokenizer(text)
spacy.tests.lang.ti.test_text.test_lex_attrs_like_number(ti_tokenizer,text,match)
spacy.tests.lang.ti.test_text.test_ti_tokenizer_handles_cnts(ti_tokenizer,text,length)
spacy.tests.lang.ti.test_text.test_ti_tokenizer_handles_long_text(ti_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/ti/test_exception.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/uk/test_lemmatizer.py----------------------------------------
A:spacy.tests.lang.uk.test_lemmatizer.pytestmark->pytest.mark.filterwarnings('ignore::DeprecationWarning')
A:spacy.tests.lang.uk.test_lemmatizer.doc->Doc(uk_lookup_lemmatizer.vocab, words=[word])
spacy.tests.lang.uk.test_lemmatizer.test_uk_lemmatizer(uk_lemmatizer)
spacy.tests.lang.uk.test_lemmatizer.test_uk_lookup_lemmatizer(uk_lookup_lemmatizer,word,lemma)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/uk/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/uk/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.uk.test_tokenizer.tokens->uk_tokenizer(text)
A:spacy.tests.lang.uk.test_tokenizer.tokens_punct->uk_tokenizer("''")
spacy.tests.lang.uk.test_tokenizer.test_uk_tokenizer_handles_final_diacritics(uk_tokenizer)
spacy.tests.lang.uk.test_tokenizer.test_uk_tokenizer_handles_only_punct(uk_tokenizer,text)
spacy.tests.lang.uk.test_tokenizer.test_uk_tokenizer_splits_bracket_period(uk_tokenizer)
spacy.tests.lang.uk.test_tokenizer.test_uk_tokenizer_splits_close_punct(uk_tokenizer,punct,text)
spacy.tests.lang.uk.test_tokenizer.test_uk_tokenizer_splits_double_end_quote(uk_tokenizer,text)
spacy.tests.lang.uk.test_tokenizer.test_uk_tokenizer_splits_open_appostrophe(uk_tokenizer,text)
spacy.tests.lang.uk.test_tokenizer.test_uk_tokenizer_splits_open_close_punct(uk_tokenizer,punct_open,punct_close,text)
spacy.tests.lang.uk.test_tokenizer.test_uk_tokenizer_splits_open_punct(uk_tokenizer,punct,text)
spacy.tests.lang.uk.test_tokenizer.test_uk_tokenizer_splits_same_close_punct(uk_tokenizer,punct,text)
spacy.tests.lang.uk.test_tokenizer.test_uk_tokenizer_splits_same_open_punct(uk_tokenizer,punct,text)
spacy.tests.lang.uk.test_tokenizer.test_uk_tokenizer_splits_trailing_dot(uk_tokenizer,text)
spacy.tests.lang.uk.test_tokenizer.test_uk_tokenizer_splits_two_diff_close_punct(uk_tokenizer,punct,punct_add,text)
spacy.tests.lang.uk.test_tokenizer.test_uk_tokenizer_splits_two_diff_open_punct(uk_tokenizer,punct,punct_add,text)
spacy.tests.lang.uk.test_tokenizer.test_uk_tokenizer_two_diff_punct(uk_tokenizer,punct_open,punct_close,punct_open2,punct_close2,text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/uk/test_tokenizer_exc.py----------------------------------------
A:spacy.tests.lang.uk.test_tokenizer_exc.tokens->uk_tokenizer(text)
spacy.tests.lang.uk.test_tokenizer_exc.test_uk_tokenizer_abbrev_exceptions(uk_tokenizer,text,norms,lemmas)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/hi/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/hi/test_text.py----------------------------------------
A:spacy.tests.lang.hi.test_text.nlp->Hindi()
A:spacy.tests.lang.hi.test_text.doc->nlp('hi. how हुए. होटल, होटल')
spacy.tests.lang.hi.test_text.test_issue3625()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/hi/test_lex_attrs.py----------------------------------------
A:spacy.tests.lang.hi.test_lex_attrs.tokens->hi_tokenizer(text)
spacy.tests.lang.hi.test_lex_attrs.test_hi_like_num(word)
spacy.tests.lang.hi.test_lex_attrs.test_hi_like_num_ordinal_words(word)
spacy.tests.lang.hi.test_lex_attrs.test_hi_norm(word,word_norm)
spacy.tests.lang.hi.test_lex_attrs.test_hi_tokenizer_handles_long_text(hi_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/ru/test_lemmatizer.py----------------------------------------
A:spacy.tests.lang.ru.test_lemmatizer.pytestmark->pytest.mark.filterwarnings('ignore::DeprecationWarning')
A:spacy.tests.lang.ru.test_lemmatizer.doc->Doc(ru_lookup_lemmatizer.vocab, words=[word])
A:spacy.tests.lang.ru.test_lemmatizer.result_lemmas->ru_lemmatizer.pymorphy2_lemmatize(doc[0])
spacy.tests.lang.ru.test_lemmatizer.test_ru_doc_lemmatization(ru_lemmatizer)
spacy.tests.lang.ru.test_lemmatizer.test_ru_doc_lookup_lemmatization(ru_lookup_lemmatizer)
spacy.tests.lang.ru.test_lemmatizer.test_ru_lemmatizer_noun_lemmas(ru_lemmatizer,text,lemmas)
spacy.tests.lang.ru.test_lemmatizer.test_ru_lemmatizer_punct(ru_lemmatizer)
spacy.tests.lang.ru.test_lemmatizer.test_ru_lemmatizer_works_with_different_pos_homonyms(ru_lemmatizer,text,pos,morph,lemma)
spacy.tests.lang.ru.test_lemmatizer.test_ru_lemmatizer_works_with_noun_homonyms(ru_lemmatizer,text,morph,lemma)
spacy.tests.lang.ru.test_lemmatizer.test_ru_lookup_lemmatizer(ru_lookup_lemmatizer,word,lemma)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/ru/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/ru/test_exceptions.py----------------------------------------
A:spacy.tests.lang.ru.test_exceptions.tokens->ru_tokenizer(text)
spacy.tests.lang.ru.test_exceptions.test_ru_tokenizer_abbrev_exceptions(ru_tokenizer,text,norms)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/ru/test_text.py----------------------------------------
spacy.tests.lang.ru.test_text.test_ru_lex_attrs_capitals(word)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/ru/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.ru.test_tokenizer.tokens->ru_tokenizer(text)
A:spacy.tests.lang.ru.test_tokenizer.tokens_punct->ru_tokenizer("''")
spacy.tests.lang.ru.test_tokenizer.test_ru_tokenizer_handles_final_diacritic_and_period(ru_tokenizer,text)
spacy.tests.lang.ru.test_tokenizer.test_ru_tokenizer_handles_final_diacritics(ru_tokenizer,text)
spacy.tests.lang.ru.test_tokenizer.test_ru_tokenizer_handles_only_punct(ru_tokenizer,text)
spacy.tests.lang.ru.test_tokenizer.test_ru_tokenizer_splits_bracket_period(ru_tokenizer)
spacy.tests.lang.ru.test_tokenizer.test_ru_tokenizer_splits_close_punct(ru_tokenizer,punct,text)
spacy.tests.lang.ru.test_tokenizer.test_ru_tokenizer_splits_double_end_quote(ru_tokenizer,text)
spacy.tests.lang.ru.test_tokenizer.test_ru_tokenizer_splits_open_appostrophe(ru_tokenizer,text)
spacy.tests.lang.ru.test_tokenizer.test_ru_tokenizer_splits_open_close_punct(ru_tokenizer,punct_open,punct_close,text)
spacy.tests.lang.ru.test_tokenizer.test_ru_tokenizer_splits_open_punct(ru_tokenizer,punct,text)
spacy.tests.lang.ru.test_tokenizer.test_ru_tokenizer_splits_same_close_punct(ru_tokenizer,punct,text)
spacy.tests.lang.ru.test_tokenizer.test_ru_tokenizer_splits_same_open_punct(ru_tokenizer,punct,text)
spacy.tests.lang.ru.test_tokenizer.test_ru_tokenizer_splits_trailing_dot(ru_tokenizer,text)
spacy.tests.lang.ru.test_tokenizer.test_ru_tokenizer_splits_two_diff_close_punct(ru_tokenizer,punct,punct_add,text)
spacy.tests.lang.ru.test_tokenizer.test_ru_tokenizer_splits_two_diff_open_punct(ru_tokenizer,punct,punct_add,text)
spacy.tests.lang.ru.test_tokenizer.test_ru_tokenizer_two_diff_punct(ru_tokenizer,punct_open,punct_close,punct_open2,punct_close2,text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/zh/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/zh/test_text.py----------------------------------------
A:spacy.tests.lang.zh.test_text.tokens->zh_tokenizer_jieba(text)
spacy.tests.lang.zh.test_text.test_lex_attrs_like_number(zh_tokenizer_jieba,text,match)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/zh/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.zh.test_tokenizer.user_dict->_get_pkuseg_trie_data(zh_tokenizer_pkuseg.pkuseg_seg.preprocesser.trie)
A:spacy.tests.lang.zh.test_tokenizer.updated_user_dict->_get_pkuseg_trie_data(zh_tokenizer_pkuseg.pkuseg_seg.preprocesser.trie)
A:spacy.tests.lang.zh.test_tokenizer.reset_user_dict->_get_pkuseg_trie_data(zh_tokenizer_pkuseg.pkuseg_seg.preprocesser.trie)
A:spacy.tests.lang.zh.test_tokenizer.tokens->zh_tokenizer_char('I   like cheese.')
A:spacy.tests.lang.zh.test_tokenizer.nlp->spacy.lang.zh.Chinese.from_config(config)
spacy.tests.lang.zh.test_tokenizer.test_zh_extra_spaces(zh_tokenizer_char)
spacy.tests.lang.zh.test_tokenizer.test_zh_tokenizer_char(zh_tokenizer_char,text)
spacy.tests.lang.zh.test_tokenizer.test_zh_tokenizer_jieba(zh_tokenizer_jieba,text,expected_tokens)
spacy.tests.lang.zh.test_tokenizer.test_zh_tokenizer_pkuseg(zh_tokenizer_pkuseg,text,expected_tokens)
spacy.tests.lang.zh.test_tokenizer.test_zh_tokenizer_pkuseg_user_dict(zh_tokenizer_pkuseg,zh_tokenizer_char)
spacy.tests.lang.zh.test_tokenizer.test_zh_uninitialized_pkuseg()
spacy.tests.lang.zh.test_tokenizer.test_zh_unsupported_segmenter()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/zh/test_serialize.py----------------------------------------
A:spacy.tests.lang.zh.test_serialize.tokenizer_bytes->zh_tokenizer.to_bytes()
A:spacy.tests.lang.zh.test_serialize.nlp->spacy.lang.zh.Chinese.from_config(config)
spacy.tests.lang.zh.test_serialize.test_zh_tokenizer_serialize_char(zh_tokenizer_char)
spacy.tests.lang.zh.test_serialize.test_zh_tokenizer_serialize_jieba(zh_tokenizer_jieba)
spacy.tests.lang.zh.test_serialize.test_zh_tokenizer_serialize_pkuseg_with_processors(zh_tokenizer_pkuseg)
spacy.tests.lang.zh.test_serialize.zh_tokenizer_serialize(zh_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/ar/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/ar/test_exceptions.py----------------------------------------
A:spacy.tests.lang.ar.test_exceptions.tokens->ar_tokenizer(text)
spacy.tests.lang.ar.test_exceptions.test_ar_tokenizer_handles_abbr(ar_tokenizer,text)
spacy.tests.lang.ar.test_exceptions.test_ar_tokenizer_handles_exc_in_text(ar_tokenizer)
spacy.tests.lang.ar.test_exceptions.test_ar_tokenizer_handles_exc_in_text_2(ar_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/ar/test_text.py----------------------------------------
A:spacy.tests.lang.ar.test_text.tokens->ar_tokenizer(text)
spacy.tests.lang.ar.test_text.test_ar_tokenizer_handles_long_text(ar_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/ja/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/ja/test_morphologizer_factory.py----------------------------------------
A:spacy.tests.lang.ja.test_morphologizer_factory.nlp->Japanese()
A:spacy.tests.lang.ja.test_morphologizer_factory.morphologizer->Japanese().add_pipe('morphologizer')
spacy.tests.lang.ja.test_morphologizer_factory.test_ja_morphologizer_factory()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/ja/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.ja.test_tokenizer.nlp->Japanese()
A:spacy.tests.lang.ja.test_tokenizer.doc->ja_tokenizer('\n\n\n \t\t \n\n\n')
A:spacy.tests.lang.ja.test_tokenizer.tokens->ja_tokenizer(text)
A:spacy.tests.lang.ja.test_tokenizer.nlp_a->spacy.lang.ja.Japanese.from_config({'nlp': {'tokenizer': {'split_mode': 'A'}}})
A:spacy.tests.lang.ja.test_tokenizer.nlp_b->spacy.lang.ja.Japanese.from_config({'nlp': {'tokenizer': {'split_mode': 'B'}}})
A:spacy.tests.lang.ja.test_tokenizer.nlp_c->spacy.lang.ja.Japanese.from_config({'nlp': {'tokenizer': {'split_mode': 'C'}}})
spacy.tests.lang.ja.test_tokenizer.test_issue2901()
spacy.tests.lang.ja.test_tokenizer.test_ja_tokenizer(ja_tokenizer,text,expected_tokens)
spacy.tests.lang.ja.test_tokenizer.test_ja_tokenizer_emptyish_texts(ja_tokenizer)
spacy.tests.lang.ja.test_tokenizer.test_ja_tokenizer_extra_spaces(ja_tokenizer)
spacy.tests.lang.ja.test_tokenizer.test_ja_tokenizer_inflections_reading_forms(ja_tokenizer,text,inflections,reading_forms)
spacy.tests.lang.ja.test_tokenizer.test_ja_tokenizer_naughty_strings(ja_tokenizer,text)
spacy.tests.lang.ja.test_tokenizer.test_ja_tokenizer_pos(ja_tokenizer,text,expected_pos)
spacy.tests.lang.ja.test_tokenizer.test_ja_tokenizer_sents(ja_tokenizer,text,expected_sents)
spacy.tests.lang.ja.test_tokenizer.test_ja_tokenizer_split_modes(ja_tokenizer,text,len_a,len_b,len_c)
spacy.tests.lang.ja.test_tokenizer.test_ja_tokenizer_sub_tokens(ja_tokenizer,text,sub_tokens_list_b,sub_tokens_list_c)
spacy.tests.lang.ja.test_tokenizer.test_ja_tokenizer_tags(ja_tokenizer,text,expected_tags)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/ja/test_serialize.py----------------------------------------
A:spacy.tests.lang.ja.test_serialize.tokenizer_bytes->ja_tokenizer.to_bytes()
A:spacy.tests.lang.ja.test_serialize.nlp->spacy.lang.ja.Japanese.from_config({'nlp': {'tokenizer': {'split_mode': 'B'}}})
A:spacy.tests.lang.ja.test_serialize.nlp_r->Japanese()
A:spacy.tests.lang.ja.test_serialize.nlp_bytes->spacy.lang.ja.Japanese.from_config({'nlp': {'tokenizer': {'split_mode': 'B'}}}).to_bytes()
A:spacy.tests.lang.ja.test_serialize.b->pickle.dumps(ja_tokenizer)
A:spacy.tests.lang.ja.test_serialize.ja_tokenizer_re->pickle.loads(b)
spacy.tests.lang.ja.test_serialize.test_ja_tokenizer_pickle(ja_tokenizer)
spacy.tests.lang.ja.test_serialize.test_ja_tokenizer_serialize(ja_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/ja/test_lemmatization.py----------------------------------------
spacy.tests.lang.ja.test_lemmatization.test_ja_lemmatizer_assigns(ja_tokenizer,word,lemma)
spacy.tests.lang.ja.test_lemmatization.test_ja_lemmatizer_norm(ja_tokenizer,word,norm)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/am/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/am/test_text.py----------------------------------------
A:spacy.tests.lang.am.test_text.tokens->am_tokenizer(text)
spacy.tests.lang.am.test_text.test_am_tokenizer_handles_cnts(am_tokenizer,text,length)
spacy.tests.lang.am.test_text.test_am_tokenizer_handles_long_text(am_tokenizer)
spacy.tests.lang.am.test_text.test_lex_attrs_like_number(am_tokenizer,text,match)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/am/test_exception.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/lv/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/lv/test_text.py----------------------------------------
A:spacy.tests.lang.lv.test_text.tokens->lv_tokenizer(text)
spacy.tests.lang.lv.test_text.test_long_text(lv_tokenizer)
spacy.tests.lang.lv.test_text.test_ordinal_number(lv_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/lv/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.lv.test_tokenizer.tokens->lv_tokenizer(text)
spacy.tests.lang.lv.test_tokenizer.test_lv_tokenizer_basic(lv_tokenizer,text,expected_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/et/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/et/test_text.py----------------------------------------
A:spacy.tests.lang.et.test_text.tokens->et_tokenizer(text)
spacy.tests.lang.et.test_text.test_long_text(et_tokenizer)
spacy.tests.lang.et.test_text.test_ordinal_number(et_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/et/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.et.test_tokenizer.tokens->et_tokenizer(text)
spacy.tests.lang.et.test_tokenizer.test_et_tokenizer_basic(et_tokenizer,text,expected_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/ne/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/ne/test_text.py----------------------------------------
A:spacy.tests.lang.ne.test_text.tokens->ne_tokenizer(text)
spacy.tests.lang.ne.test_text.test_ne_tokenizer_handlers_long_text(ne_tokenizer)
spacy.tests.lang.ne.test_text.test_ne_tokenizer_handles_cnts(ne_tokenizer,text,length)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/af/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/af/test_text.py----------------------------------------
A:spacy.tests.lang.af.test_text.tokens->af_tokenizer(text)
spacy.tests.lang.af.test_text.test_indefinite_article(af_tokenizer)
spacy.tests.lang.af.test_text.test_long_text(af_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/af/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.af.test_tokenizer.tokens->af_tokenizer(text)
spacy.tests.lang.af.test_tokenizer.test_af_tokenizer_basic(af_tokenizer,text,expected_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/tl/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/tl/test_indices.py----------------------------------------
A:spacy.tests.lang.tl.test_indices.tokens->tl_tokenizer(text)
spacy.tests.lang.tl.test_indices.test_tl_simple_punct(tl_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/tl/test_text.py----------------------------------------
A:spacy.tests.lang.tl.test_text.tokens->tl_tokenizer(text)
spacy.tests.lang.tl.test_text.test_lex_attrs_like_number(tl_tokenizer,text,match)
spacy.tests.lang.tl.test_text.test_tl_lex_attrs_capitals(word)
spacy.tests.lang.tl.test_text.test_tl_tokenizer_handles_cnts(tl_tokenizer,text,length)
spacy.tests.lang.tl.test_text.test_tl_tokenizer_handles_long_text(tl_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/tl/test_punct.py----------------------------------------
A:spacy.tests.lang.tl.test_punct.tokens->tl_tokenizer(text)
A:spacy.tests.lang.tl.test_punct.tokens_punct->tl_tokenizer("''")
A:spacy.tests.lang.tl.test_punct.match->tl_search_prefixes(text)
spacy.tests.lang.tl.test_punct.test_tl_tokenizer_handles_only_punct(tl_tokenizer,text)
spacy.tests.lang.tl.test_punct.test_tl_tokenizer_split_open_punct(tl_tokenizer,punct,text)
spacy.tests.lang.tl.test_punct.test_tl_tokenizer_splits_bracket_period(tl_tokenizer)
spacy.tests.lang.tl.test_punct.test_tl_tokenizer_splits_close_punct(tl_tokenizer,punct,text)
spacy.tests.lang.tl.test_punct.test_tl_tokenizer_splits_double_end_quote(tl_tokenizer,text)
spacy.tests.lang.tl.test_punct.test_tl_tokenizer_splits_open_apostrophe(tl_tokenizer,text)
spacy.tests.lang.tl.test_punct.test_tl_tokenizer_splits_open_close_punct(tl_tokenizer,punct_open,punct_close,text)
spacy.tests.lang.tl.test_punct.test_tl_tokenizer_splits_pre_punct_regex(text,punct)
spacy.tests.lang.tl.test_punct.test_tl_tokenizer_splits_same_close_punct(tl_tokenizer,punct,text)
spacy.tests.lang.tl.test_punct.test_tl_tokenizer_splits_same_open_punct(tl_tokenizer,punct,text)
spacy.tests.lang.tl.test_punct.test_tl_tokenizer_splits_two_diff_close_punct(tl_tokenizer,punct,punct_add,text)
spacy.tests.lang.tl.test_punct.test_tl_tokenizer_splits_two_diff_open_punct(tl_tokenizer,punct,punct_add,text)
spacy.tests.lang.tl.test_punct.test_tl_tokenizer_two_diff_punct(tl_tokenizer,punct_open,punct_close,punct_open2,punct_close2,text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/el/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/el/test_text.py----------------------------------------
A:spacy.tests.lang.el.test_text.tokens->el_tokenizer(text)
spacy.tests.lang.el.test_text.test_el_tokenizer_handles_cnts(el_tokenizer,text,length)
spacy.tests.lang.el.test_text.test_el_tokenizer_handles_long_text(el_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/el/test_exception.py----------------------------------------
A:spacy.tests.lang.el.test_exception.tokens->el_tokenizer(text)
spacy.tests.lang.el.test_exception.test_el_tokenizer_handles_abbr(el_tokenizer,text)
spacy.tests.lang.el.test_exception.test_el_tokenizer_handles_exc_in_text(el_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/el/test_noun_chunks.py----------------------------------------
A:spacy.tests.lang.el.test_noun_chunks.doc->el_tokenizer('είναι χώρα της νοτιοανατολικής')
spacy.tests.lang.el.test_noun_chunks.test_noun_chunks_is_parsed_el(el_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/mk/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/mk/test_text.py----------------------------------------
A:spacy.tests.lang.mk.test_text.tokens->mk_tokenizer(word)
spacy.tests.lang.mk.test_text.test_mk_lex_attrs_capitals(word)
spacy.tests.lang.mk.test_text.test_mk_lex_attrs_like_number(mk_tokenizer,word,match)
spacy.tests.lang.mk.test_text.test_mk_lex_attrs_like_number_for_ordinal(word)
spacy.tests.lang.mk.test_text.test_tokenizer_handles_long_text(mk_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/ur/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/ur/test_text.py----------------------------------------
A:spacy.tests.lang.ur.test_text.tokens->ur_tokenizer(text)
spacy.tests.lang.ur.test_text.test_ur_tokenizer_handles_cnts(ur_tokenizer,text,length)
spacy.tests.lang.ur.test_text.test_ur_tokenizer_handles_long_text(ur_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/ur/test_prefix_suffix_infix.py----------------------------------------
A:spacy.tests.lang.ur.test_prefix_suffix_infix.tokens->ur_tokenizer(text)
spacy.tests.lang.ur.test_prefix_suffix_infix.test_contractions(ur_tokenizer,text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/hr/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/hr/test_text.py----------------------------------------
A:spacy.tests.lang.hr.test_text.tokens->hr_tokenizer(text)
spacy.tests.lang.hr.test_text.test_long_text(hr_tokenizer)
spacy.tests.lang.hr.test_text.test_ordinal_number(hr_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/hr/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.hr.test_tokenizer.tokens->hr_tokenizer(text)
spacy.tests.lang.hr.test_tokenizer.test_hr_tokenizer_basic(hr_tokenizer,text,expected_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/es/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/es/test_text.py----------------------------------------
A:spacy.tests.lang.es.test_text.nlp->Spanish()
A:spacy.tests.lang.es.test_text.doc->nlp(text)
A:spacy.tests.lang.es.test_text.tokens->es_tokenizer(text)
spacy.tests.lang.es.test_text.test_es_lex_attrs_capitals(word)
spacy.tests.lang.es.test_text.test_es_tokenizer_handles_cnts(es_tokenizer,text,length)
spacy.tests.lang.es.test_text.test_es_tokenizer_handles_long_text(es_tokenizer)
spacy.tests.lang.es.test_text.test_issue3803()
spacy.tests.lang.es.test_text.test_lex_attrs_like_number(es_tokenizer,text,match)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/es/test_exception.py----------------------------------------
A:spacy.tests.lang.es.test_exception.tokens->es_tokenizer(text)
spacy.tests.lang.es.test_exception.test_es_tokenizer_handles_abbr(es_tokenizer,text,lemma)
spacy.tests.lang.es.test_exception.test_es_tokenizer_handles_exc_in_text(es_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/es/test_noun_chunks.py----------------------------------------
A:spacy.tests.lang.es.test_noun_chunks.doc->es_tokenizer('en Oxford este verano')
spacy.tests.lang.es.test_noun_chunks.test_es_noun_chunks(es_vocab,words,heads,deps,pos,chunk_offsets)
spacy.tests.lang.es.test_noun_chunks.test_noun_chunks_is_parsed_es(es_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/tt/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/tt/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.tt.test_tokenizer.tokens->tt_tokenizer(text)
spacy.tests.lang.tt.test_tokenizer.test_tt_tokenizer_handles_norm_exceptions(tt_tokenizer,text,norms)
spacy.tests.lang.tt.test_tokenizer.test_tt_tokenizer_handles_testcases(tt_tokenizer,text,expected_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/sv/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/sv/test_exceptions.py----------------------------------------
A:spacy.tests.lang.sv.test_exceptions.tokens->sv_tokenizer(text)
spacy.tests.lang.sv.test_exceptions.test_issue805(sv_tokenizer,text,expected_tokens)
spacy.tests.lang.sv.test_exceptions.test_sv_tokenizer_handles_abbr(sv_tokenizer,text)
spacy.tests.lang.sv.test_exceptions.test_sv_tokenizer_handles_ambiguous_abbr(sv_tokenizer,text)
spacy.tests.lang.sv.test_exceptions.test_sv_tokenizer_handles_custom_base_exc(sv_tokenizer)
spacy.tests.lang.sv.test_exceptions.test_sv_tokenizer_handles_exc_in_text(sv_tokenizer)
spacy.tests.lang.sv.test_exceptions.test_sv_tokenizer_handles_exception_cases(sv_tokenizer,text,expected_tokens)
spacy.tests.lang.sv.test_exceptions.test_sv_tokenizer_handles_verb_exceptions(sv_tokenizer,text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/sv/test_text.py----------------------------------------
A:spacy.tests.lang.sv.test_text.tokens->sv_tokenizer(text)
spacy.tests.lang.sv.test_text.test_sv_tokenizer_handles_long_text(sv_tokenizer)
spacy.tests.lang.sv.test_text.test_sv_tokenizer_handles_trailing_dot_for_i_in_sentence(sv_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/sv/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.sv.test_tokenizer.tokens->sv_tokenizer(text)
spacy.tests.lang.sv.test_tokenizer.test_sv_tokenizer_handles_exception_cases(sv_tokenizer,text,expected_tokens)
spacy.tests.lang.sv.test_tokenizer.test_sv_tokenizer_handles_verb_exceptions(sv_tokenizer,text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/sv/test_noun_chunks.py----------------------------------------
A:spacy.tests.lang.sv.test_noun_chunks.doc->Doc(tokens.vocab, words=words, heads=heads, deps=deps, pos=pos)
A:spacy.tests.lang.sv.test_noun_chunks.tokens->sv_tokenizer(text)
A:spacy.tests.lang.sv.test_noun_chunks.noun_chunks->list(doc.noun_chunks)
spacy.tests.lang.sv.test_noun_chunks.test_noun_chunks_is_parsed_sv(sv_tokenizer)
spacy.tests.lang.sv.test_noun_chunks.test_sv_noun_chunks(sv_tokenizer,text,pos,deps,heads,expected_noun_chunks)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/sv/test_prefix_suffix_infix.py----------------------------------------
A:spacy.tests.lang.sv.test_prefix_suffix_infix.tokens->sv_tokenizer(text)
spacy.tests.lang.sv.test_prefix_suffix_infix.test_sv_tokenizer_handles_colon(sv_tokenizer,text)
spacy.tests.lang.sv.test_prefix_suffix_infix.test_tokenizer_handles_no_punct(sv_tokenizer,text)
spacy.tests.lang.sv.test_prefix_suffix_infix.test_tokenizer_splits_comma_infix(sv_tokenizer,text)
spacy.tests.lang.sv.test_prefix_suffix_infix.test_tokenizer_splits_ellipsis_infix(sv_tokenizer,text)
spacy.tests.lang.sv.test_prefix_suffix_infix.test_tokenizer_splits_no_special(sv_tokenizer,text)
spacy.tests.lang.sv.test_prefix_suffix_infix.test_tokenizer_splits_period_infix(sv_tokenizer,text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/sv/test_lex_attrs.py----------------------------------------
A:spacy.tests.lang.sv.test_lex_attrs.tokens->sv_tokenizer(text)
spacy.tests.lang.sv.test_lex_attrs.test_lex_attrs_like_number(sv_tokenizer,text,match)
spacy.tests.lang.sv.test_lex_attrs.test_sv_lex_attrs_capitals(word)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/sl/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/sl/test_text.py----------------------------------------
A:spacy.tests.lang.sl.test_text.tokens->sl_tokenizer(text)
spacy.tests.lang.sl.test_text.test_long_text(sl_tokenizer)
spacy.tests.lang.sl.test_text.test_ordinal_number(sl_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/sl/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.sl.test_tokenizer.tokens->sl_tokenizer(text)
spacy.tests.lang.sl.test_tokenizer.test_sl_tokenizer_basic(sl_tokenizer,text,expected_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/dsb/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/dsb/test_text.py----------------------------------------
A:spacy.tests.lang.dsb.test_text.tokens->dsb_tokenizer(text)
spacy.tests.lang.dsb.test_text.test_lex_attrs_like_number(dsb_tokenizer,text,match)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/dsb/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.dsb.test_tokenizer.tokens->dsb_tokenizer(text)
spacy.tests.lang.dsb.test_tokenizer.test_dsb_tokenizer_basic(dsb_tokenizer,text,expected_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/cs/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/cs/test_text.py----------------------------------------
A:spacy.tests.lang.cs.test_text.tokens->cs_tokenizer(text)
spacy.tests.lang.cs.test_text.test_lex_attrs_like_number(cs_tokenizer,text,match)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/sk/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/sk/test_text.py----------------------------------------
A:spacy.tests.lang.sk.test_text.tokens->sk_tokenizer(text)
spacy.tests.lang.sk.test_text.test_lex_attrs_like_number(sk_tokenizer,text,match)
spacy.tests.lang.sk.test_text.test_long_text(sk_tokenizer)
spacy.tests.lang.sk.test_text.test_ordinal_number(sk_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/sk/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.sk.test_tokenizer.tokens->sk_tokenizer(text)
spacy.tests.lang.sk.test_tokenizer.test_sk_tokenizer_basic(sk_tokenizer,text,expected_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/lg/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/lg/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.lg.test_tokenizer.tokens->lg_tokenizer(text)
spacy.tests.lang.lg.test_tokenizer.test_lg_tokenizer_basic(lg_tokenizer,text,expected_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/id/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/id/test_text.py----------------------------------------
spacy.tests.lang.id.test_text.test_id_lex_attrs_capitals(word)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/id/test_noun_chunks.py----------------------------------------
A:spacy.tests.lang.id.test_noun_chunks.doc->id_tokenizer('sebelas')
spacy.tests.lang.id.test_noun_chunks.test_noun_chunks_is_parsed_id(id_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/id/test_prefix_suffix_infix.py----------------------------------------
A:spacy.tests.lang.id.test_prefix_suffix_infix.tokens->id_tokenizer('Arsene Wenger--manajer Arsenal--melakukan konferensi pers.')
spacy.tests.lang.id.test_prefix_suffix_infix.test_id_tokenizer_splits_comma_infix(id_tokenizer,text)
spacy.tests.lang.id.test_prefix_suffix_infix.test_id_tokenizer_splits_double_hyphen_infix(id_tokenizer)
spacy.tests.lang.id.test_prefix_suffix_infix.test_id_tokenizer_splits_ellipsis_infix(id_tokenizer,text)
spacy.tests.lang.id.test_prefix_suffix_infix.test_id_tokenizer_splits_even_wrap(id_tokenizer,text)
spacy.tests.lang.id.test_prefix_suffix_infix.test_id_tokenizer_splits_even_wrap_interact(id_tokenizer,text)
spacy.tests.lang.id.test_prefix_suffix_infix.test_id_tokenizer_splits_hyphens(id_tokenizer,text,length)
spacy.tests.lang.id.test_prefix_suffix_infix.test_id_tokenizer_splits_no_punct(id_tokenizer,text)
spacy.tests.lang.id.test_prefix_suffix_infix.test_id_tokenizer_splits_no_special(id_tokenizer,text)
spacy.tests.lang.id.test_prefix_suffix_infix.test_id_tokenizer_splits_numeric_range(id_tokenizer,text)
spacy.tests.lang.id.test_prefix_suffix_infix.test_id_tokenizer_splits_period_infix(id_tokenizer,text)
spacy.tests.lang.id.test_prefix_suffix_infix.test_id_tokenizer_splits_prefix_interact(id_tokenizer,text,length)
spacy.tests.lang.id.test_prefix_suffix_infix.test_id_tokenizer_splits_prefix_punct(id_tokenizer,text)
spacy.tests.lang.id.test_prefix_suffix_infix.test_id_tokenizer_splits_suffix_interact(id_tokenizer,text)
spacy.tests.lang.id.test_prefix_suffix_infix.test_id_tokenizer_splits_suffix_punct(id_tokenizer,text)
spacy.tests.lang.id.test_prefix_suffix_infix.test_id_tokenizer_splits_uneven_wrap_interact(id_tokenizer,text)
spacy.tests.lang.id.test_prefix_suffix_infix.test_tokenizer_splits_uneven_wrap(id_tokenizer,text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/sr/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/sr/test_exceptions.py----------------------------------------
A:spacy.tests.lang.sr.test_exceptions.tokens->sr_tokenizer(text)
spacy.tests.lang.sr.test_exceptions.test_sr_tokenizer_abbrev_exceptions(sr_tokenizer,text,norms,lemmas)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/sr/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.sr.test_tokenizer.tokens->sr_tokenizer(text)
A:spacy.tests.lang.sr.test_tokenizer.tokens_punct->sr_tokenizer("''")
spacy.tests.lang.sr.test_tokenizer.test_sr_tokenizer_handles_only_punct(sr_tokenizer,text)
spacy.tests.lang.sr.test_tokenizer.test_sr_tokenizer_splits_bracket_period(sr_tokenizer)
spacy.tests.lang.sr.test_tokenizer.test_sr_tokenizer_splits_close_punct(sr_tokenizer,punct,text)
spacy.tests.lang.sr.test_tokenizer.test_sr_tokenizer_splits_double_end_quote(sr_tokenizer,text)
spacy.tests.lang.sr.test_tokenizer.test_sr_tokenizer_splits_open_appostrophe(sr_tokenizer,text)
spacy.tests.lang.sr.test_tokenizer.test_sr_tokenizer_splits_open_close_punct(sr_tokenizer,punct_open,punct_close,text)
spacy.tests.lang.sr.test_tokenizer.test_sr_tokenizer_splits_open_punct(sr_tokenizer,punct,text)
spacy.tests.lang.sr.test_tokenizer.test_sr_tokenizer_splits_same_close_punct(sr_tokenizer,punct,text)
spacy.tests.lang.sr.test_tokenizer.test_sr_tokenizer_splits_same_open_punct(sr_tokenizer,punct,text)
spacy.tests.lang.sr.test_tokenizer.test_sr_tokenizer_splits_trailing_dot(sr_tokenizer,text)
spacy.tests.lang.sr.test_tokenizer.test_sr_tokenizer_splits_two_diff_close_punct(sr_tokenizer,punct,punct_add,text)
spacy.tests.lang.sr.test_tokenizer.test_sr_tokenizer_splits_two_diff_open_punct(sr_tokenizer,punct,punct_add,text)
spacy.tests.lang.sr.test_tokenizer.test_sr_tokenizer_two_diff_punct(sr_tokenizer,punct_open,punct_close,punct_open2,punct_close2,text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/hy/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/hy/test_text.py----------------------------------------
spacy.tests.lang.hy.test_text.test_hy_lex_attrs_capitals(word)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/hy/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.hy.test_tokenizer.tokens->hy_tokenizer(text)
spacy.tests.lang.hy.test_tokenizer.test_ga_tokenizer_handles_exception_cases(hy_tokenizer,text,expected_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/fo/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/fo/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.fo.test_tokenizer.tokens->fo_tokenizer(text)
spacy.tests.lang.fo.test_tokenizer.test_fo_tokenizer_handles_exception_cases(fo_tokenizer,text,expected_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/la/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/la/test_text.py----------------------------------------
A:spacy.tests.lang.la.test_text.tokens->la_tokenizer(text)
spacy.tests.lang.la.test_text.test_la_lex_attrs_capitals(word)
spacy.tests.lang.la.test_text.test_lex_attrs_like_number(la_tokenizer,text,match)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/la/test_exception.py----------------------------------------
A:spacy.tests.lang.la.test_exception.tokens->la_tokenizer(text)
spacy.tests.lang.la.test_exception.test_la_tokenizer_handles_exc_in_text(la_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/la/test_noun_chunks.py----------------------------------------
A:spacy.tests.lang.la.test_noun_chunks.doc->Doc(tokens.vocab, words=[t.text for t in tokens], heads=[head + i for (i, head) in enumerate(heads)], deps=deps, pos=pos)
A:spacy.tests.lang.la.test_noun_chunks.tokens->la_tokenizer(text)
A:spacy.tests.lang.la.test_noun_chunks.noun_chunks->list(doc.noun_chunks)
spacy.tests.lang.la.test_noun_chunks.test_la_noun_chunks(la_tokenizer,text,pos,deps,heads,expected_noun_chunks)
spacy.tests.lang.la.test_noun_chunks.test_noun_chunks_is_parsed(la_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/de/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/de/test_exceptions.py----------------------------------------
A:spacy.tests.lang.de.test_exceptions.tokens->de_tokenizer(text)
spacy.tests.lang.de.test_exceptions.test_de_tokenizer_handles_abbr(de_tokenizer,text)
spacy.tests.lang.de.test_exceptions.test_de_tokenizer_handles_exc_in_text(de_tokenizer)
spacy.tests.lang.de.test_exceptions.test_de_tokenizer_splits_contractions(de_tokenizer,text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/de/test_parser.py----------------------------------------
A:spacy.tests.lang.de.test_parser.doc->Doc(de_vocab, words=words, pos=pos, deps=deps, heads=heads)
A:spacy.tests.lang.de.test_parser.chunks->list(doc.noun_chunks)
spacy.tests.lang.de.test_parser.test_de_extended_chunk(de_vocab)
spacy.tests.lang.de.test_parser.test_de_parser_noun_chunks_standard_de(de_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/de/test_text.py----------------------------------------
A:spacy.tests.lang.de.test_text.tokens->de_tokenizer(text)
spacy.tests.lang.de.test_text.test_de_tokenizer_handles_examples(de_tokenizer,text,length)
spacy.tests.lang.de.test_text.test_de_tokenizer_handles_long_text(de_tokenizer)
spacy.tests.lang.de.test_text.test_de_tokenizer_handles_long_words(de_tokenizer,text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/de/test_noun_chunks.py----------------------------------------
A:spacy.tests.lang.de.test_noun_chunks.doc->de_tokenizer('Er lag auf seinem')
spacy.tests.lang.de.test_noun_chunks.test_noun_chunks_is_parsed_de(de_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/de/test_prefix_suffix_infix.py----------------------------------------
A:spacy.tests.lang.de.test_prefix_suffix_infix.tokens->de_tokenizer('Viele Regeln--wie die Bindestrich-Regeln--sind kompliziert.')
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_keeps_hyphens(de_tokenizer,text)
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_splits_comma_infix(de_tokenizer,text)
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_splits_double_hyphen_infix(de_tokenizer)
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_splits_ellipsis_infix(de_tokenizer,text)
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_splits_even_wrap(de_tokenizer,text)
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_splits_even_wrap_interact(de_tokenizer,text)
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_splits_no_punct(de_tokenizer,text)
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_splits_no_special(de_tokenizer,text)
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_splits_numeric_range(de_tokenizer,text)
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_splits_period_infix(de_tokenizer,text)
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_splits_prefix_interact(de_tokenizer,text,length)
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_splits_prefix_punct(de_tokenizer,text)
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_splits_suffix_interact(de_tokenizer,text)
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_splits_suffix_punct(de_tokenizer,text)
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_splits_uneven_wrap(de_tokenizer,text)
spacy.tests.lang.de.test_prefix_suffix_infix.test_de_tokenizer_splits_uneven_wrap_interact(de_tokenizer,text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/nn/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/nn/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.nn.test_tokenizer.tokens->nn_tokenizer(text)
spacy.tests.lang.nn.test_tokenizer.test_nn_tokenizer_handles_exception_cases(nn_tokenizer,text,expected_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/he/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/he/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.he.test_tokenizer.tokens->he_tokenizer(text)
spacy.tests.lang.he.test_tokenizer.test_he_lex_attrs_like_number_for_ordinal(word)
spacy.tests.lang.he.test_tokenizer.test_he_tokenizer_handles_abbreviation(he_tokenizer,text,expected_tokens)
spacy.tests.lang.he.test_tokenizer.test_he_tokenizer_handles_punct(he_tokenizer,text,expected_tokens)
spacy.tests.lang.he.test_tokenizer.test_lex_attrs_like_number(he_tokenizer,text,match)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/fa/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/fa/test_noun_chunks.py----------------------------------------
A:spacy.tests.lang.fa.test_noun_chunks.doc->fa_tokenizer('این یک جمله نمونه می باشد.')
spacy.tests.lang.fa.test_noun_chunks.test_noun_chunks_is_parsed_fa(fa_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/ko/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/ko/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.ko.test_tokenizer.tokens->ko_tokenizer('미닛 리피터')
spacy.tests.lang.ko.test_tokenizer.test_ko_empty_doc(ko_tokenizer)
spacy.tests.lang.ko.test_tokenizer.test_ko_spacy_tokenizer(ko_tokenizer_tokenizer,text,expected_tokens)
spacy.tests.lang.ko.test_tokenizer.test_ko_tokenizer(ko_tokenizer,text,expected_tokens)
spacy.tests.lang.ko.test_tokenizer.test_ko_tokenizer_full_tags(ko_tokenizer,text,expected_tags)
spacy.tests.lang.ko.test_tokenizer.test_ko_tokenizer_pos(ko_tokenizer,text,expected_pos)
spacy.tests.lang.ko.test_tokenizer.test_ko_tokenizer_tags(ko_tokenizer,text,expected_tags)
spacy.tests.lang.ko.test_tokenizer.test_ko_tokenizer_unknown_tag(ko_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/ko/test_serialize.py----------------------------------------
A:spacy.tests.lang.ko.test_serialize.tokenizer_bytes->ko_tokenizer.to_bytes()
A:spacy.tests.lang.ko.test_serialize.nlp->Korean()
A:spacy.tests.lang.ko.test_serialize.b->pickle.dumps(ko_tokenizer)
A:spacy.tests.lang.ko.test_serialize.ko_tokenizer_re->pickle.loads(b)
spacy.tests.lang.ko.test_serialize.test_ko_tokenizer_pickle(ko_tokenizer)
spacy.tests.lang.ko.test_serialize.test_ko_tokenizer_serialize(ko_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/ko/test_lemmatization.py----------------------------------------
spacy.tests.lang.ko.test_lemmatization.test_ko_lemmatizer_assigns(ko_tokenizer,word,lemma)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/ms/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/ms/test_text.py----------------------------------------
spacy.tests.lang.ms.test_text.test_ms_lex_attrs_capitals(word)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/ms/test_noun_chunks.py----------------------------------------
A:spacy.tests.lang.ms.test_noun_chunks.doc->ms_tokenizer('sebelas')
spacy.tests.lang.ms.test_noun_chunks.test_noun_chunks_is_parsed_ms(ms_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/ms/test_prefix_suffix_infix.py----------------------------------------
A:spacy.tests.lang.ms.test_prefix_suffix_infix.tokens->id_tokenizer('Arsene Wenger--pengurus Arsenal--mengadakan sidang media.')
spacy.tests.lang.ms.test_prefix_suffix_infix.test_ms_tokenizer_splits_comma_infix(id_tokenizer,text)
spacy.tests.lang.ms.test_prefix_suffix_infix.test_ms_tokenizer_splits_double_hyphen_infix(id_tokenizer)
spacy.tests.lang.ms.test_prefix_suffix_infix.test_ms_tokenizer_splits_ellipsis_infix(id_tokenizer,text)
spacy.tests.lang.ms.test_prefix_suffix_infix.test_ms_tokenizer_splits_even_wrap(id_tokenizer,text)
spacy.tests.lang.ms.test_prefix_suffix_infix.test_ms_tokenizer_splits_even_wrap_interact(id_tokenizer,text)
spacy.tests.lang.ms.test_prefix_suffix_infix.test_ms_tokenizer_splits_no_punct(id_tokenizer,text)
spacy.tests.lang.ms.test_prefix_suffix_infix.test_ms_tokenizer_splits_no_special(id_tokenizer,text)
spacy.tests.lang.ms.test_prefix_suffix_infix.test_ms_tokenizer_splits_numeric_range(id_tokenizer,text)
spacy.tests.lang.ms.test_prefix_suffix_infix.test_ms_tokenizer_splits_period_infix(id_tokenizer,text)
spacy.tests.lang.ms.test_prefix_suffix_infix.test_ms_tokenizer_splits_prefix_interact(id_tokenizer,text,length)
spacy.tests.lang.ms.test_prefix_suffix_infix.test_ms_tokenizer_splits_prefix_punct(id_tokenizer,text)
spacy.tests.lang.ms.test_prefix_suffix_infix.test_ms_tokenizer_splits_suffix_interact(id_tokenizer,text)
spacy.tests.lang.ms.test_prefix_suffix_infix.test_ms_tokenizer_splits_suffix_punct(id_tokenizer,text)
spacy.tests.lang.ms.test_prefix_suffix_infix.test_ms_tokenizer_splits_uneven_wrap_interact(id_tokenizer,text)
spacy.tests.lang.ms.test_prefix_suffix_infix.test_my_tokenizer_splits_hyphens(ms_tokenizer,text,length)
spacy.tests.lang.ms.test_prefix_suffix_infix.test_tokenizer_splits_uneven_wrap(id_tokenizer,text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/nl/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/nl/test_text.py----------------------------------------
A:spacy.tests.lang.nl.test_text.tokens->nl_tokenizer(text)
spacy.tests.lang.nl.test_text.test_nl_lex_attrs_capitals(word)
spacy.tests.lang.nl.test_text.test_tokenizer_doesnt_split_hyphens(nl_tokenizer,text,num_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/nl/test_noun_chunks.py----------------------------------------
A:spacy.tests.lang.nl.test_noun_chunks.doc->Doc(nl_vocab, words=['Dit', 'programma', 'wordt', 'beschouwd', 'als', "'s", 'werelds', 'eerste', 'computerprogramma'], deps=['det', 'nsubj:pass', 'aux:pass', 'ROOT', 'mark', 'det', 'fixed', 'amod', 'xcomp'], heads=[1, 3, 3, 3, 8, 8, 5, 8, 3], pos=['DET', 'NOUN', 'AUX', 'VERB', 'SCONJ', 'DET', 'NOUN', 'ADJ', 'NOUN'])
A:spacy.tests.lang.nl.test_noun_chunks.chunks->list(doc.noun_chunks)
spacy.tests.lang.nl.test_noun_chunks.nl_reference_chunking()
spacy.tests.lang.nl.test_noun_chunks.nl_sample(nl_vocab)
spacy.tests.lang.nl.test_noun_chunks.test_chunking(nl_sample,nl_reference_chunking)
spacy.tests.lang.nl.test_noun_chunks.test_need_dep(nl_tokenizer)
spacy.tests.lang.nl.test_noun_chunks.test_no_overlapping_chunks(nl_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/ro/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/ro/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.ro.test_tokenizer.tokens->ro_tokenizer(text)
spacy.tests.lang.ro.test_tokenizer.test_ro_tokenizer_handles_testcases(ro_tokenizer,text,expected_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/gu/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/gu/test_text.py----------------------------------------
A:spacy.tests.lang.gu.test_text.tokens->gu_tokenizer(text)
spacy.tests.lang.gu.test_text.test_gu_tokenizer_handlers_long_text(gu_tokenizer)
spacy.tests.lang.gu.test_text.test_gu_tokenizer_handles_cnts(gu_tokenizer,text,length)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/pl/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/pl/test_text.py----------------------------------------
A:spacy.tests.lang.pl.test_text.tokens->pl_tokenizer(text)
spacy.tests.lang.pl.test_text.test_lex_attrs_like_number(pl_tokenizer,text,match)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/pl/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.pl.test_tokenizer.tokens->pl_tokenizer(text)
spacy.tests.lang.pl.test_tokenizer.test_tokenizer_handles_testcases(pl_tokenizer,text,expected_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/th/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/th/test_tokenizer.py----------------------------------------
spacy.tests.lang.th.test_tokenizer.test_th_tokenizer(th_tokenizer,text,expected_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/th/test_serialize.py----------------------------------------
A:spacy.tests.lang.th.test_serialize.tokenizer_bytes->th_tokenizer.to_bytes()
A:spacy.tests.lang.th.test_serialize.nlp->Thai()
A:spacy.tests.lang.th.test_serialize.b->pickle.dumps(th_tokenizer)
A:spacy.tests.lang.th.test_serialize.th_tokenizer_re->pickle.loads(b)
spacy.tests.lang.th.test_serialize.test_th_tokenizer_pickle(th_tokenizer)
spacy.tests.lang.th.test_serialize.test_th_tokenizer_serialize(th_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/tr/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/tr/test_parser.py----------------------------------------
A:spacy.tests.lang.tr.test_parser.tokens->tr_tokenizer(text)
A:spacy.tests.lang.tr.test_parser.doc->Doc(tokens.vocab, words=[t.text for t in tokens], pos=pos, heads=heads, deps=deps)
A:spacy.tests.lang.tr.test_parser.chunks->list(doc.noun_chunks)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_acl_nmod(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_acl_nmod2(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_acl_simple(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_acl_verb(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_amod_simple(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_chain_nmod_head_with_amod_acl(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_chain_nmod_with_acl(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_chain_nmod_with_adj(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_conj_and_adj_phrase(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_conj_fixed_adj_phrase(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_conj_noun_head_verb(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_conj_simple(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_conj_subject(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_conj_three(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_conj_three2(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_det_amod_nmod(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_determiner_simple(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_flat_and_chain_nmod(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_flat_in_nmod(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_flat_name_lastname_and_title(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_flat_names_and_title(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_flat_names_and_title2(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_flat_simple(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_nmod_amod(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_nmod_simple(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_nmod_three(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_nmod_two(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_np_recursive_four_nouns(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_np_recursive_long_two_acls(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_np_recursive_no_nmod(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_np_recursive_nsubj_attached_to_pron_root(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_np_recursive_nsubj_in_subnp(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_np_recursive_nsubj_to_root(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_np_recursive_two_nmods(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_one_det_one_adj_simple(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_one_det_two_adjs_simple(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_two_adjs_simple(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_two_flats_conjed(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_two_nouns_in_nmod(tr_tokenizer)
spacy.tests.lang.tr.test_parser.test_tr_noun_chunks_two_nouns_in_nmod2(tr_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/tr/test_text.py----------------------------------------
A:spacy.tests.lang.tr.test_text.tokens->tr_tokenizer(text)
spacy.tests.lang.tr.test_text.test_tr_lex_attrs_capitals(word)
spacy.tests.lang.tr.test_text.test_tr_lex_attrs_like_number_cardinal_ordinal(word)
spacy.tests.lang.tr.test_text.test_tr_tokenizer_handles_long_text(tr_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/tr/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.tr.test_tokenizer.tokens->tr_tokenizer(text)
spacy.tests.lang.tr.test_tokenizer.test_tr_tokenizer_handles_allcases(tr_tokenizer,text,expected_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/tr/test_noun_chunks.py----------------------------------------
A:spacy.tests.lang.tr.test_noun_chunks.doc->tr_tokenizer('Dün seni gördüm.')
spacy.tests.lang.tr.test_noun_chunks.test_noun_chunks_is_parsed(tr_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/ta/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/ta/test_text.py----------------------------------------
A:spacy.tests.lang.ta.test_text.tokens->ta_tokenizer(text)
A:spacy.tests.lang.ta.test_text.nlp->Tamil()
A:spacy.tests.lang.ta.test_text.doc->nlp(text)
spacy.tests.lang.ta.test_text.test_long_text(ta_tokenizer,text,num_tokens)
spacy.tests.lang.ta.test_text.test_ta_sentencizer(text,num_sents)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/ta/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.ta.test_tokenizer.tokens->nlp(text)
A:spacy.tests.lang.ta.test_tokenizer.nlp->Tamil()
spacy.tests.lang.ta.test_tokenizer.test_ta_tokenizer_basic(ta_tokenizer,text,expected_tokens)
spacy.tests.lang.ta.test_tokenizer.test_ta_tokenizer_special_case(text,expected_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/yo/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/yo/test_text.py----------------------------------------
A:spacy.tests.lang.yo.test_text.tokens->yo_tokenizer(text)
spacy.tests.lang.yo.test_text.test_lex_attrs_like_number(yo_tokenizer,text,match)
spacy.tests.lang.yo.test_text.test_yo_lex_attrs_capitals(word)
spacy.tests.lang.yo.test_text.test_yo_tokenizer_handles_long_text(yo_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/ml/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/ml/test_text.py----------------------------------------
A:spacy.tests.lang.ml.test_text.tokens->ml_tokenizer(text)
spacy.tests.lang.ml.test_text.test_ml_tokenizer_handles_cnts(ml_tokenizer,text,length)
spacy.tests.lang.ml.test_text.test_ml_tokenizer_handles_long_text(ml_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/hu/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/hu/test_tokenizer.py----------------------------------------
A:spacy.tests.lang.hu.test_tokenizer.tokens->hu_tokenizer(text)
spacy.tests.lang.hu.test_tokenizer.test_hu_tokenizer_handles_testcases(hu_tokenizer,text,expected_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/fr/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/fr/test_exceptions.py----------------------------------------
A:spacy.tests.lang.fr.test_exceptions.tokens->fr_tokenizer(text)
spacy.tests.lang.fr.test_exceptions.test_fr_tokenizer_handles_abbr(fr_tokenizer,text)
spacy.tests.lang.fr.test_exceptions.test_fr_tokenizer_handles_exc_in_text(fr_tokenizer)
spacy.tests.lang.fr.test_exceptions.test_fr_tokenizer_handles_exc_in_text_2(fr_tokenizer)
spacy.tests.lang.fr.test_exceptions.test_fr_tokenizer_handles_title(fr_tokenizer)
spacy.tests.lang.fr.test_exceptions.test_fr_tokenizer_handles_title_2(fr_tokenizer)
spacy.tests.lang.fr.test_exceptions.test_fr_tokenizer_handles_title_3(fr_tokenizer)
spacy.tests.lang.fr.test_exceptions.test_fr_tokenizer_infix_exceptions(fr_tokenizer,text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/fr/test_text.py----------------------------------------
A:spacy.tests.lang.fr.test_text.tokens->fr_tokenizer(text)
spacy.tests.lang.fr.test_text.test_fr_lex_attrs_capitals(word)
spacy.tests.lang.fr.test_text.test_tokenizer_handles_long_text(fr_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/fr/test_noun_chunks.py----------------------------------------
A:spacy.tests.lang.fr.test_noun_chunks.doc->fr_tokenizer("Je suis allé à l'école")
spacy.tests.lang.fr.test_noun_chunks.test_fr_noun_chunks(fr_vocab,words,heads,deps,pos,chunk_offsets)
spacy.tests.lang.fr.test_noun_chunks.test_noun_chunks_is_parsed_fr(fr_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/fr/test_prefix_suffix_infix.py----------------------------------------
A:spacy.tests.lang.fr.test_prefix_suffix_infix.SPLIT_INFIX->"(?<=[{a}]\\')(?=[{a}])".format(a=ALPHA)
A:spacy.tests.lang.fr.test_prefix_suffix_infix.tokens->fr_tokenizer_w_infix(text)
spacy.tests.lang.fr.test_prefix_suffix_infix.test_issue768(text,expected_tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/kmr/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/kmr/test_text.py----------------------------------------
spacy.tests.lang.kmr.test_text.test_kmr_lex_attrs_capitals(word)
spacy.tests.lang.kmr.test_text.test_kmr_lex_attrs_like_number_for_ordinal(word)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/pt/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/pt/test_text.py----------------------------------------
spacy.tests.lang.pt.test_text.test_pt_lex_attrs_capitals(word)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/lang/pt/test_noun_chunks.py----------------------------------------
A:spacy.tests.lang.pt.test_noun_chunks.doc->pt_tokenizer('en Oxford este verano')
spacy.tests.lang.pt.test_noun_chunks.test_noun_chunks_is_parsed_pt(pt_tokenizer)
spacy.tests.lang.pt.test_noun_chunks.test_pt_noun_chunks(pt_vocab,words,heads,deps,pos,chunk_offsets)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/serialize/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/serialize/test_serialize_config.py----------------------------------------
A:spacy.tests.serialize.test_serialize_config.tok2vec->build_Tok2Vec_model(MultiHashEmbed(width=321, attrs=['LOWER', 'SHAPE'], rows=[5432, 5432], include_static_vectors=False), MaxoutWindowEncoder(width=321, window_size=3, maxout_pieces=4, depth=2))
A:spacy.tests.serialize.test_serialize_config.parser->spacy.lang.en.English.from_config(load_config_from_str(hyphen_config_str)).add_pipe('parser', config=model_config)
A:spacy.tests.serialize.test_serialize_config.source_nlp->spacy.lang.en.English.from_config(source_cfg)
A:spacy.tests.serialize.test_serialize_config.nlp->spacy.lang.en.English.from_config(load_config_from_str(hyphen_config_str))
A:spacy.tests.serialize.test_serialize_config.config->Config().from_str(parser_config_string)
A:spacy.tests.serialize.test_serialize_config.pretrain_config->load_config(DEFAULT_CONFIG_PRETRAIN_PATH)
A:spacy.tests.serialize.test_serialize_config.filled->spacy.util.registry.fill(config, schema=ConfigSchema, validate=False)
A:spacy.tests.serialize.test_serialize_config.config['nlp']['pipeline']->list(config['components'].keys())
A:spacy.tests.serialize.test_serialize_config.nlp_config->Config().from_str(nlp_config_string)
A:spacy.tests.serialize.test_serialize_config.nlp2->spacy.lang.en.English.from_config(interpolated)
A:spacy.tests.serialize.test_serialize_config.parser_cfg->dict()
A:spacy.tests.serialize.test_serialize_config.model_config->Config().from_str(parser_config_string)
A:spacy.tests.serialize.test_serialize_config.new_nlp->spacy.load(d)
A:spacy.tests.serialize.test_serialize_config.nlp_bytes->spacy.lang.en.English.from_config(load_config_from_str(hyphen_config_str)).to_bytes()
A:spacy.tests.serialize.test_serialize_config.base_config->Config().from_str(nlp_config_string)
A:spacy.tests.serialize.test_serialize_config.base_nlp->load_model_from_config(base_config, auto_fill=True)
A:spacy.tests.serialize.test_serialize_config.nlp_re1->spacy.load(d, config={'components': {'attribute_ruler': {'scorer': {'@scorers': 'spacy.tagger_scorer.v1'}}}})
A:spacy.tests.serialize.test_serialize_config.nlp_re2->spacy.load(d, config={'components': {'attribute_ruler': {'scorer': {'@scorers': 'spacy.overlapping_labeled_spans_scorer.v1', 'spans_key': {'@misc': 'test_some_other_key'}}}}})
A:spacy.tests.serialize.test_serialize_config.example->spacy.training.Example.from_dict(nlp_re2.make_doc('a b c'), {})
A:spacy.tests.serialize.test_serialize_config.scores->spacy.load(d, config={'components': {'attribute_ruler': {'scorer': {'@scorers': 'spacy.overlapping_labeled_spans_scorer.v1', 'spans_key': {'@misc': 'test_some_other_key'}}}}}).evaluate([example])
A:spacy.tests.serialize.test_serialize_config.interpolated->Config().from_str(parser_config_string).interpolate()
A:spacy.tests.serialize.test_serialize_config.interpolated2->spacy.lang.en.English.from_config(load_config_from_str(hyphen_config_str)).config.interpolate()
A:spacy.tests.serialize.test_serialize_config.new_config->Config().from_str(filled.to_str())
spacy.tests.serialize.test_serialize_config.my_parser()
spacy.tests.serialize.test_serialize_config.test_config_auto_fill_extra_fields()
spacy.tests.serialize.test_serialize_config.test_config_interpolation()
spacy.tests.serialize.test_serialize_config.test_config_nlp_roundtrip()
spacy.tests.serialize.test_serialize_config.test_config_nlp_roundtrip_bytes_disk()
spacy.tests.serialize.test_serialize_config.test_config_only_resolve_relevant_blocks()
spacy.tests.serialize.test_serialize_config.test_config_optional_sections()
spacy.tests.serialize.test_serialize_config.test_config_overrides()
spacy.tests.serialize.test_serialize_config.test_config_overrides_registered_functions()
spacy.tests.serialize.test_serialize_config.test_config_validate_literal(parser_config_string)
spacy.tests.serialize.test_serialize_config.test_create_nlp_from_config()
spacy.tests.serialize.test_serialize_config.test_create_nlp_from_config_multiple_instances()
spacy.tests.serialize.test_serialize_config.test_create_nlp_from_pretraining_config()
spacy.tests.serialize.test_serialize_config.test_hyphen_in_config()
spacy.tests.serialize.test_serialize_config.test_issue8190()
spacy.tests.serialize.test_serialize_config.test_serialize_config_language_specific()
spacy.tests.serialize.test_serialize_config.test_serialize_config_missing_pipes()
spacy.tests.serialize.test_serialize_config.test_serialize_custom_nlp()
spacy.tests.serialize.test_serialize_config.test_serialize_nlp()
spacy.tests.serialize.test_serialize_config.test_serialize_parser(parser_config_string)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/serialize/test_serialize_doc.py----------------------------------------
A:spacy.tests.serialize.test_serialize_doc.nlp->English()
A:spacy.tests.serialize.test_serialize_doc.data->Doc(en_vocab, words=['hello', 'world', '!']).to_bytes()
A:spacy.tests.serialize.test_serialize_doc.vectors->Vectors(data=data, keys=['I', 'am', 'Matt'])
A:spacy.tests.serialize.test_serialize_doc.tagger->English().add_pipe('tagger')
A:spacy.tests.serialize.test_serialize_doc.heads_deps->numpy.asarray([[1, 397], [4, 436], [2, 426], [1, 402], [0, 8206900633647566924], [18446744073709551615, 440], [18446744073709551614, 442]], dtype='uint64')
A:spacy.tests.serialize.test_serialize_doc.doc->Doc(en_vocab, words=['hello', 'world', '!'])
A:spacy.tests.serialize.test_serialize_doc.new_doc->Doc(en_vocab).from_bytes(doc.to_bytes())
A:spacy.tests.serialize.test_serialize_doc.matcher->PhraseMatcher(nlp.vocab)
A:spacy.tests.serialize.test_serialize_doc.new_matcher->pickle.loads(data)
A:spacy.tests.serialize.test_serialize_doc.docs->English().pipe(['hello', 'world'])
A:spacy.tests.serialize.test_serialize_doc.piped_doc->next(docs)
A:spacy.tests.serialize.test_serialize_doc.bytes_data->English().to_bytes()
A:spacy.tests.serialize.test_serialize_doc.new_nlp->English()
A:spacy.tests.serialize.test_serialize_doc.doc_bytes->Doc(en_vocab, words=['hello', 'world', '!']).to_bytes()
A:spacy.tests.serialize.test_serialize_doc.doc2->Doc(en_vocab)
A:spacy.tests.serialize.test_serialize_doc.doc_b->Doc(en_vocab, words=['hello', 'world', '!']).to_bytes()
A:spacy.tests.serialize.test_serialize_doc.doc_d->Doc(en_vocab).from_disk(file_path)
A:spacy.tests.serialize.test_serialize_doc.file_path->str(file_path)
spacy.tests.serialize.test_serialize_doc.test_issue1727()
spacy.tests.serialize.test_serialize_doc.test_issue1799()
spacy.tests.serialize.test_serialize_doc.test_issue1834()
spacy.tests.serialize.test_serialize_doc.test_issue1883()
spacy.tests.serialize.test_serialize_doc.test_issue2564()
spacy.tests.serialize.test_serialize_doc.test_issue3248_2()
spacy.tests.serialize.test_serialize_doc.test_issue3289()
spacy.tests.serialize.test_serialize_doc.test_issue3468()
spacy.tests.serialize.test_serialize_doc.test_issue3959()
spacy.tests.serialize.test_serialize_doc.test_serialize_doc_exclude(en_vocab)
spacy.tests.serialize.test_serialize_doc.test_serialize_doc_roundtrip_bytes(en_vocab)
spacy.tests.serialize.test_serialize_doc.test_serialize_doc_roundtrip_disk(en_vocab)
spacy.tests.serialize.test_serialize_doc.test_serialize_doc_roundtrip_disk_str_path(en_vocab)
spacy.tests.serialize.test_serialize_doc.test_serialize_doc_span_groups(en_vocab)
spacy.tests.serialize.test_serialize_doc.test_serialize_empty_doc(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/serialize/test_serialize_kb.py----------------------------------------
A:spacy.tests.serialize.test_serialize_kb.kb1->_get_dummy_kb(en_vocab)
A:spacy.tests.serialize.test_serialize_kb.dir_path->ensure_path(d)
A:spacy.tests.serialize.test_serialize_kb.kb2->InMemoryLookupKB(vocab=en_vocab, entity_vector_length=3)
A:spacy.tests.serialize.test_serialize_kb.kb->SubInMemoryLookupKB(vocab=vocab, entity_vector_length=entity_vector_length, custom_field=custom_field)
A:spacy.tests.serialize.test_serialize_kb.candidates->sorted(kb.get_alias_candidates('double07'), key=lambda x: x.entity_)
A:spacy.tests.serialize.test_serialize_kb.path->ensure_path(path)
A:spacy.tests.serialize.test_serialize_kb.config->Config().from_str(config_string)
A:spacy.tests.serialize.test_serialize_kb.nlp->load_model_from_config(config, auto_fill=True)
A:spacy.tests.serialize.test_serialize_kb.entity_linker->load_model_from_config(config, auto_fill=True).get_pipe('entity_linker')
A:spacy.tests.serialize.test_serialize_kb.nlp2->spacy.util.load_model_from_path(tmp_dir)
A:spacy.tests.serialize.test_serialize_kb.entity_linker2->spacy.util.load_model_from_path(tmp_dir).get_pipe('entity_linker')
spacy.tests.serialize.test_serialize_kb._check_kb(kb)
spacy.tests.serialize.test_serialize_kb._get_dummy_kb(vocab)
spacy.tests.serialize.test_serialize_kb.test_serialize_kb_disk(en_vocab)
spacy.tests.serialize.test_serialize_kb.test_serialize_subclassed_kb()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/serialize/test_serialize_docbin.py----------------------------------------
A:spacy.tests.serialize.test_serialize_docbin.doc->Doc(en_vocab, words=['hello', 'world'])
A:spacy.tests.serialize.test_serialize_docbin.doc_bin->DocBin().from_bytes(DocBin(docs=[doc1, doc2]).to_bytes())
A:spacy.tests.serialize.test_serialize_docbin.doc_bin_bytes->DocBin(store_user_data=writer_flag).to_bytes()
A:spacy.tests.serialize.test_serialize_docbin.new_doc_bin->DocBin(store_user_data=True).from_bytes(doc_bin_bytes)
A:spacy.tests.serialize.test_serialize_docbin.doc_bin_2->DocBin(store_user_data=reader_flag).from_bytes(doc_bin_bytes)
A:spacy.tests.serialize.test_serialize_docbin.nlp->spacy.blank('en')
A:spacy.tests.serialize.test_serialize_docbin.bytes_data->DocBin().from_bytes(DocBin(docs=[doc1, doc2]).to_bytes()).to_bytes()
A:spacy.tests.serialize.test_serialize_docbin.reloaded_docs->list(doc_bin.get_docs(nlp.vocab))
A:spacy.tests.serialize.test_serialize_docbin.doc1->Doc(en_vocab, words=['that', "'s"])
A:spacy.tests.serialize.test_serialize_docbin.doc2->Doc(en_vocab, words=['that', "'s"], spaces=[False, False])
A:spacy.tests.serialize.test_serialize_docbin.(re_doc1, re_doc2)->DocBin().from_bytes(DocBin(docs=[doc1, doc2]).to_bytes()).get_docs(en_vocab)
A:spacy.tests.serialize.test_serialize_docbin.doc_bin_1->DocBin(store_user_data=writer_flag)
spacy.tests.serialize.test_serialize_docbin.test_issue4367()
spacy.tests.serialize.test_serialize_docbin.test_issue4528(en_vocab)
spacy.tests.serialize.test_serialize_docbin.test_issue5141(en_vocab)
spacy.tests.serialize.test_serialize_docbin.test_serialize_custom_extension(en_vocab,writer_flag,reader_flag,reader_value)
spacy.tests.serialize.test_serialize_docbin.test_serialize_doc_bin()
spacy.tests.serialize.test_serialize_docbin.test_serialize_doc_bin_unknown_spaces(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/serialize/test_serialize_language.py----------------------------------------
A:spacy.tests.serialize.test_serialize_language.nlp->Language(meta=meta_data)
A:spacy.tests.serialize.test_serialize_language.b->Language(meta=meta_data).to_bytes()
A:spacy.tests.serialize.test_serialize_language.language->Language(meta=meta_data)
A:spacy.tests.serialize.test_serialize_language.new_language->Language().from_disk(d)
A:spacy.tests.serialize.test_serialize_language.prefix_re->re.compile('1/|2/|:[0-9][0-9][A-K]:|:[0-9][0-9]:')
A:spacy.tests.serialize.test_serialize_language.suffix_re->re.compile('')
A:spacy.tests.serialize.test_serialize_language.infix_re->re.compile('[~]')
A:spacy.tests.serialize.test_serialize_language.nlp.tokenizer->custom_tokenizer(nlp)
A:spacy.tests.serialize.test_serialize_language.new_nlp->Language().from_bytes(nlp.to_bytes(exclude=['meta']))
spacy.tests.serialize.test_serialize_language.meta_data()
spacy.tests.serialize.test_serialize_language.test_issue2482()
spacy.tests.serialize.test_serialize_language.test_issue6950()
spacy.tests.serialize.test_serialize_language.test_serialize_language_exclude(meta_data)
spacy.tests.serialize.test_serialize_language.test_serialize_language_meta_disk(meta_data)
spacy.tests.serialize.test_serialize_language.test_serialize_with_custom_tokenizer()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/serialize/test_resource_warning.py----------------------------------------
A:spacy.tests.serialize.test_resource_warning.data->zeros((3, 1), dtype='f')
A:spacy.tests.serialize.test_resource_warning.self.model->SerializableDummy()
A:spacy.tests.serialize.test_resource_warning.nlp->Language()
A:spacy.tests.serialize.test_resource_warning.tagger->Language().add_pipe('tagger')
A:spacy.tests.serialize.test_resource_warning.kb->InMemoryLookupKB(nlp.vocab, entity_vector_length=1)
A:spacy.tests.serialize.test_resource_warning.entity_linker->Language().add_pipe('entity_linker')
A:spacy.tests.serialize.test_resource_warning.warnings_list->write_obj_and_catch_warnings(scenario[0])
A:spacy.tests.serialize.test_resource_warning.writer->Writer(path)
A:spacy.tests.serialize.test_resource_warning.kb_loaded->InMemoryLookupKB(nlp.vocab, entity_vector_length=1)
A:spacy.tests.serialize.test_resource_warning.scenarios->zip(*objects_to_test)
spacy.tests.serialize.test_resource_warning.TestToDiskResourceWarningUnittest(TestCase)
spacy.tests.serialize.test_resource_warning.TestToDiskResourceWarningUnittest.test_resource_warning(self)
spacy.tests.serialize.test_resource_warning.custom_pipe()
spacy.tests.serialize.test_resource_warning.entity_linker()
spacy.tests.serialize.test_resource_warning.nlp()
spacy.tests.serialize.test_resource_warning.tagger()
spacy.tests.serialize.test_resource_warning.test_save_and_load_knowledge_base()
spacy.tests.serialize.test_resource_warning.test_to_disk_resource_warning(obj)
spacy.tests.serialize.test_resource_warning.test_writer_with_path_py35()
spacy.tests.serialize.test_resource_warning.vectors()
spacy.tests.serialize.test_resource_warning.write_obj_and_catch_warnings(obj)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/serialize/test_serialize_extension_attrs.py----------------------------------------
A:spacy.tests.serialize.test_serialize_extension_attrs.doc->Doc(Vocab()).from_bytes(doc_b)
A:spacy.tests.serialize.test_serialize_extension_attrs.doc_b->doc_w_attrs.to_bytes()
spacy.tests.serialize.test_serialize_extension_attrs.doc_w_attrs(en_tokenizer)
spacy.tests.serialize.test_serialize_extension_attrs.test_serialize_ext_attrs_from_bytes(doc_w_attrs)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/serialize/test_serialize_span_groups.py----------------------------------------
A:spacy.tests.serialize.test_serialize_span_groups.doc->en_tokenizer('0 1 2 3 4 5 6')
A:spacy.tests.serialize.test_serialize_span_groups.doc.spans['test']->SpanGroup(doc, name='test', spans=[doc[0:1]])
A:spacy.tests.serialize.test_serialize_span_groups.doc.spans['test2']->SpanGroup(doc, name='test', spans=[doc[1:2]])
A:spacy.tests.serialize.test_serialize_span_groups.groups['key1']->SpanGroup(doc, name='key1', spans=[doc[0:1], doc[1:2]])
A:spacy.tests.serialize.test_serialize_span_groups.groups['key2']->SpanGroup(doc, name='too', spans=[doc[3:4], doc[4:5]])
A:spacy.tests.serialize.test_serialize_span_groups.groups['key3']->SpanGroup(doc, name='too', spans=[doc[1:2], doc[0:1]])
A:spacy.tests.serialize.test_serialize_span_groups.groups['key4']->SpanGroup(doc, name='key4', spans=[doc[0:1]])
A:spacy.tests.serialize.test_serialize_span_groups.groups['key5']->SpanGroup(doc, name='key4', spans=[doc[0:1]])
A:spacy.tests.serialize.test_serialize_span_groups.sg6->SpanGroup(doc, name='key6', spans=[doc[0:1]])
A:spacy.tests.serialize.test_serialize_span_groups.sg8->SpanGroup(doc, name='also', spans=[doc[1:2]])
A:spacy.tests.serialize.test_serialize_span_groups.regroups->SpanGroups(doc).from_bytes(groups.to_bytes())
A:spacy.tests.serialize.test_serialize_span_groups.span_groups->SpanGroups(doc)
A:spacy.tests.serialize.test_serialize_span_groups.sg1->SpanGroup(doc, spans=spans)
A:spacy.tests.serialize.test_serialize_span_groups.reloaded_span_groups->SpanGroups(doc).from_bytes(span_groups.to_bytes())
spacy.tests.serialize.test_serialize_span_groups.test_deserialize_span_groups_compat(en_tokenizer,spans_bytes,doc_text,expected_spangroups,expected_warning)
spacy.tests.serialize.test_serialize_span_groups.test_issue10685(en_tokenizer)
spacy.tests.serialize.test_serialize_span_groups.test_span_groups_serialization(en_tokenizer)
spacy.tests.serialize.test_serialize_span_groups.test_span_groups_serialization_mismatches(en_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/serialize/test_serialize_tokenizer.py----------------------------------------
A:spacy.tests.serialize.test_serialize_tokenizer.doc->Doc(en_vocab, words=words, tags=tags, pos=pos, ents=ents)
A:spacy.tests.serialize.test_serialize_tokenizer.ent_array->Doc(en_vocab, words=words, tags=tags, pos=pos, ents=ents).to_array(header)
A:spacy.tests.serialize.test_serialize_tokenizer.doc_bytes->Doc(en_vocab, words=words, tags=tags, pos=pos, ents=ents).to_bytes()
A:spacy.tests.serialize.test_serialize_tokenizer.doc2->new_tokenizer(text)
A:spacy.tests.serialize.test_serialize_tokenizer.prefix_re->compile_prefix_regex(nlp.Defaults.prefixes)
A:spacy.tests.serialize.test_serialize_tokenizer.suffix_re->compile_suffix_regex(nlp.Defaults.suffixes)
A:spacy.tests.serialize.test_serialize_tokenizer.infix_re->compile_infix_regex(nlp.Defaults.infixes)
A:spacy.tests.serialize.test_serialize_tokenizer.new_tokenizer->load_tokenizer(tokenizer.to_bytes())
A:spacy.tests.serialize.test_serialize_tokenizer.nlp_1->English()
A:spacy.tests.serialize.test_serialize_tokenizer.doc_1a->nlp_1(test_string)
A:spacy.tests.serialize.test_serialize_tokenizer.doc_1b->nlp_1(test_string)
A:spacy.tests.serialize.test_serialize_tokenizer.nlp_2->load_model(model_dir)
A:spacy.tests.serialize.test_serialize_tokenizer.doc_2->nlp_2(test_string)
A:spacy.tests.serialize.test_serialize_tokenizer.tokenizer->Tokenizer(en_vocab, rules={'ABC.': [{'ORTH': 'ABC'}, {'ORTH': '.'}]})
A:spacy.tests.serialize.test_serialize_tokenizer.tokenizer_bytes->Tokenizer(en_vocab, rules={'ABC.': [{'ORTH': 'ABC'}, {'ORTH': '.'}]}).to_bytes()
A:spacy.tests.serialize.test_serialize_tokenizer.tokenizer_reloaded->Tokenizer(en_vocab).from_bytes(tokenizer_bytes)
A:spacy.tests.serialize.test_serialize_tokenizer.doc1->tokenizer(text)
A:spacy.tests.serialize.test_serialize_tokenizer.tokenizer_d->en_tokenizer.from_disk(file_path)
spacy.tests.serialize.test_serialize_tokenizer.load_tokenizer(b)
spacy.tests.serialize.test_serialize_tokenizer.test_issue2833(en_vocab)
spacy.tests.serialize.test_serialize_tokenizer.test_issue3012(en_vocab)
spacy.tests.serialize.test_serialize_tokenizer.test_issue4190()
spacy.tests.serialize.test_serialize_tokenizer.test_serialize_custom_tokenizer(en_vocab,en_tokenizer)
spacy.tests.serialize.test_serialize_tokenizer.test_serialize_tokenizer_roundtrip_bytes(en_tokenizer,text)
spacy.tests.serialize.test_serialize_tokenizer.test_serialize_tokenizer_roundtrip_disk(en_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/serialize/test_serialize_pipeline.py----------------------------------------
A:spacy.tests.serialize.test_serialize_pipeline.parser->Parser(en_vocab, model)
A:spacy.tests.serialize.test_serialize_pipeline.tagger1->tagger1.from_bytes(tagger1_b).from_bytes(tagger1_b)
A:spacy.tests.serialize.test_serialize_pipeline.tagger2->Tagger(de_vocab, model).from_disk(file_path)
A:spacy.tests.serialize.test_serialize_pipeline.nlp->spacy.blank('en')
A:spacy.tests.serialize.test_serialize_pipeline.tagger->spacy.blank('en').add_pipe('tagger')
A:spacy.tests.serialize.test_serialize_pipeline.ruler->spacy.blank('en').add_pipe('entity_ruler', before='ner')
A:spacy.tests.serialize.test_serialize_pipeline.ruler_bytes->spacy.blank('en').add_pipe('entity_ruler', before='ner').to_bytes()
A:spacy.tests.serialize.test_serialize_pipeline.new_ruler->spacy.lang.en.English.from_config(config).get_pipe('entity_ruler')
A:spacy.tests.serialize.test_serialize_pipeline.bytes_old_style->srsly.msgpack_dumps(ruler.patterns)
A:spacy.tests.serialize.test_serialize_pipeline.nlp2->spacy.lang.en.English.from_config(config)
A:spacy.tests.serialize.test_serialize_pipeline.ner->spacy.blank('en').create_pipe('ner', config=config)
A:spacy.tests.serialize.test_serialize_pipeline.doc1->nlp1('What do you think about Apple ?')
A:spacy.tests.serialize.test_serialize_pipeline.output_dir->ensure_path(d)
A:spacy.tests.serialize.test_serialize_pipeline.doc2->nlp2('What do you think about Apple ?')
A:spacy.tests.serialize.test_serialize_pipeline.nlp1->English()
A:spacy.tests.serialize.test_serialize_pipeline.ner1->English().add_pipe('ner')
A:spacy.tests.serialize.test_serialize_pipeline.apple_ent->Span(doc1, 5, 6, label='MY_ORG')
A:spacy.tests.serialize.test_serialize_pipeline.ner2->pickle.load(file_)
A:spacy.tests.serialize.test_serialize_pipeline.vocab->Vocab(vectors_name='test_vocab_add_vector')
A:spacy.tests.serialize.test_serialize_pipeline.new_parser->get_new_parser().from_bytes(parser.to_bytes(exclude=['cfg']), exclude=['vocab'])
A:spacy.tests.serialize.test_serialize_pipeline.bytes_2->get_new_parser().from_bytes(parser.to_bytes(exclude=['cfg']), exclude=['vocab']).to_bytes(exclude=['vocab'])
A:spacy.tests.serialize.test_serialize_pipeline.bytes_3->Parser(en_vocab, model).to_bytes(exclude=['vocab'])
A:spacy.tests.serialize.test_serialize_pipeline.vocab1->Vocab()
A:spacy.tests.serialize.test_serialize_pipeline.parser1->Parser(vocab1, model)
A:spacy.tests.serialize.test_serialize_pipeline.vocab2->Vocab()
A:spacy.tests.serialize.test_serialize_pipeline.parser2->parser2.from_bytes(parser1.to_bytes(exclude=['vocab'])).from_bytes(parser1.to_bytes(exclude=['vocab']))
A:spacy.tests.serialize.test_serialize_pipeline.parser_d->parser_d.from_disk(file_path).from_disk(file_path)
A:spacy.tests.serialize.test_serialize_pipeline.parser_bytes->Parser(en_vocab, model).to_bytes(exclude=['model', 'vocab'])
A:spacy.tests.serialize.test_serialize_pipeline.parser_d_bytes->parser_d.from_disk(file_path).from_disk(file_path).to_bytes(exclude=['model', 'vocab'])
A:spacy.tests.serialize.test_serialize_pipeline.bytes_data->Parser(en_vocab, model).to_bytes(exclude=['vocab'])
A:spacy.tests.serialize.test_serialize_pipeline.tagger1_b->tagger1.from_bytes(tagger1_b).from_bytes(tagger1_b).to_bytes()
A:spacy.tests.serialize.test_serialize_pipeline.new_tagger1->Tagger(en_vocab, model).from_bytes(tagger1_b)
A:spacy.tests.serialize.test_serialize_pipeline.new_tagger1_b->Tagger(en_vocab, model).from_bytes(tagger1_b).to_bytes()
A:spacy.tests.serialize.test_serialize_pipeline.tagger1_d->Tagger(en_vocab, model).from_disk(file_path1)
A:spacy.tests.serialize.test_serialize_pipeline.tagger2_d->Tagger(en_vocab, model).from_disk(file_path2)
A:spacy.tests.serialize.test_serialize_pipeline.textcat->TextCategorizer(en_vocab, model, threshold=0.5)
A:spacy.tests.serialize.test_serialize_pipeline.sr->SentenceRecognizer(en_vocab, model)
A:spacy.tests.serialize.test_serialize_pipeline.sr_b->SentenceRecognizer(en_vocab, model).to_bytes()
A:spacy.tests.serialize.test_serialize_pipeline.sr_d->SentenceRecognizer(en_vocab, model).from_bytes(sr_b)
A:spacy.tests.serialize.test_serialize_pipeline.config->spacy.blank('en').config.copy()
A:spacy.tests.serialize.test_serialize_pipeline.nlp3->spacy.load(d)
A:spacy.tests.serialize.test_serialize_pipeline.nlp4->spacy.load(d, disable=['ner'])
A:spacy.tests.serialize.test_serialize_pipeline.nlp5->spacy.load(d, exclude=['tagger'])
A:spacy.tests.serialize.test_serialize_pipeline.pipe->CustomPipe(Vocab(), Linear())
A:spacy.tests.serialize.test_serialize_pipeline.pipe_bytes->CustomPipe(Vocab(), Linear()).to_bytes()
A:spacy.tests.serialize.test_serialize_pipeline.new_pipe->CustomPipe(Vocab(), Linear()).from_disk(d)
A:spacy.tests.serialize.test_serialize_pipeline.orig_strings_length->len(nlp.vocab.strings)
A:spacy.tests.serialize.test_serialize_pipeline.reloaded_nlp->load(d, exclude=['strings'])
spacy.tests.serialize.test_serialize_pipeline.blank_parser(en_vocab)
spacy.tests.serialize.test_serialize_pipeline.parser(en_vocab)
spacy.tests.serialize.test_serialize_pipeline.taggers(en_vocab)
spacy.tests.serialize.test_serialize_pipeline.test_issue3456()
spacy.tests.serialize.test_serialize_pipeline.test_issue4042()
spacy.tests.serialize.test_serialize_pipeline.test_issue4042_bug2()
spacy.tests.serialize.test_serialize_pipeline.test_issue4725_1()
spacy.tests.serialize.test_serialize_pipeline.test_issue_3526_1(en_vocab)
spacy.tests.serialize.test_serialize_pipeline.test_issue_3526_2(en_vocab)
spacy.tests.serialize.test_serialize_pipeline.test_issue_3526_3(en_vocab)
spacy.tests.serialize.test_serialize_pipeline.test_issue_3526_4(en_vocab)
spacy.tests.serialize.test_serialize_pipeline.test_load_without_strings()
spacy.tests.serialize.test_serialize_pipeline.test_serialize_custom_trainable_pipe()
spacy.tests.serialize.test_serialize_pipeline.test_serialize_parser_roundtrip_bytes(en_vocab,Parser)
spacy.tests.serialize.test_serialize_pipeline.test_serialize_parser_roundtrip_disk(en_vocab,Parser)
spacy.tests.serialize.test_serialize_pipeline.test_serialize_parser_strings(Parser)
spacy.tests.serialize.test_serialize_pipeline.test_serialize_pipe_exclude(en_vocab,Parser)
spacy.tests.serialize.test_serialize_pipeline.test_serialize_pipeline_disable_enable()
spacy.tests.serialize.test_serialize_pipeline.test_serialize_sentencerecognizer(en_vocab)
spacy.tests.serialize.test_serialize_pipeline.test_serialize_tagger_roundtrip_bytes(en_vocab,taggers)
spacy.tests.serialize.test_serialize_pipeline.test_serialize_tagger_roundtrip_disk(en_vocab,taggers)
spacy.tests.serialize.test_serialize_pipeline.test_serialize_tagger_strings(en_vocab,de_vocab,taggers)
spacy.tests.serialize.test_serialize_pipeline.test_serialize_textcat_empty(en_vocab)
spacy.tests.serialize.test_serialize_pipeline.test_to_from_bytes(parser,blank_parser)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/serialize/test_serialize_vocab_strings.py----------------------------------------
A:spacy.tests.serialize.test_serialize_vocab_strings.doc->Doc(vocab).from_bytes(doc_bytes)
A:spacy.tests.serialize.test_serialize_vocab_strings.doc2->Doc(doc.vocab)
A:spacy.tests.serialize.test_serialize_vocab_strings.nlp1->English()
A:spacy.tests.serialize.test_serialize_vocab_strings.vocab_dir->ensure_path(d / 'vocab')
A:spacy.tests.serialize.test_serialize_vocab_strings.vocab2->vocab2.from_disk(file_path).from_disk(file_path)
A:spacy.tests.serialize.test_serialize_vocab_strings.nlp2->spacy.blank('en', vocab=vocab2)
A:spacy.tests.serialize.test_serialize_vocab_strings.nlp_dir->ensure_path(d / 'nlp')
A:spacy.tests.serialize.test_serialize_vocab_strings.nlp3->load_model(nlp_dir)
A:spacy.tests.serialize.test_serialize_vocab_strings.nlp->English()
A:spacy.tests.serialize.test_serialize_vocab_strings.vocab_bytes->en_vocab.to_bytes(exclude=['lookups'])
A:spacy.tests.serialize.test_serialize_vocab_strings.doc_bytes->Doc(vocab).from_bytes(doc_bytes).to_bytes()
A:spacy.tests.serialize.test_serialize_vocab_strings.vocab->Vocab(strings=strings)
A:spacy.tests.serialize.test_serialize_vocab_strings.text_hash->en_vocab.strings.add(text)
A:spacy.tests.serialize.test_serialize_vocab_strings.new_vocab->Vocab().from_bytes(vocab_bytes)
A:spacy.tests.serialize.test_serialize_vocab_strings.vocab1->Vocab(strings=strings)
A:spacy.tests.serialize.test_serialize_vocab_strings.vocab1_b->Vocab(strings=strings).to_bytes()
A:spacy.tests.serialize.test_serialize_vocab_strings.vocab2_b->vocab2.from_disk(file_path).from_disk(file_path).to_bytes()
A:spacy.tests.serialize.test_serialize_vocab_strings.new_vocab1->Vocab().from_bytes(vocab1_b)
A:spacy.tests.serialize.test_serialize_vocab_strings.vocab1_d->Vocab().from_disk(file_path1)
A:spacy.tests.serialize.test_serialize_vocab_strings.vocab2_d->Vocab().from_disk(file_path2)
A:spacy.tests.serialize.test_serialize_vocab_strings.sstore1->StringStore(strings=strings1)
A:spacy.tests.serialize.test_serialize_vocab_strings.sstore2->StringStore(strings=strings2)
A:spacy.tests.serialize.test_serialize_vocab_strings.sstore1_b->StringStore(strings=strings1).to_bytes()
A:spacy.tests.serialize.test_serialize_vocab_strings.sstore2_b->StringStore(strings=strings2).to_bytes()
A:spacy.tests.serialize.test_serialize_vocab_strings.new_sstore1->StringStore().from_bytes(sstore1_b)
A:spacy.tests.serialize.test_serialize_vocab_strings.sstore1_d->StringStore().from_disk(file_path1)
A:spacy.tests.serialize.test_serialize_vocab_strings.sstore2_d->StringStore().from_disk(file_path2)
A:spacy.tests.serialize.test_serialize_vocab_strings.ops->get_current_ops()
A:spacy.tests.serialize.test_serialize_vocab_strings.vectors->Vectors(data=ops.xp.zeros((10, 10)), mode='floret', hash_count=1)
A:spacy.tests.serialize.test_serialize_vocab_strings.vocab_pickled->pickle.dumps(vocab)
A:spacy.tests.serialize.test_serialize_vocab_strings.vocab_unpickled->pickle.loads(vocab_pickled)
spacy.tests.serialize.test_serialize_vocab_strings.test_deserialize_vocab_seen_entries(strings,lex_attr)
spacy.tests.serialize.test_serialize_vocab_strings.test_issue4054(en_vocab)
spacy.tests.serialize.test_serialize_vocab_strings.test_issue4133(en_vocab)
spacy.tests.serialize.test_serialize_vocab_strings.test_issue599(en_vocab)
spacy.tests.serialize.test_serialize_vocab_strings.test_pickle_vocab(strings,lex_attr)
spacy.tests.serialize.test_serialize_vocab_strings.test_serialize_stringstore_roundtrip_bytes(strings1,strings2)
spacy.tests.serialize.test_serialize_vocab_strings.test_serialize_stringstore_roundtrip_disk(strings1,strings2)
spacy.tests.serialize.test_serialize_vocab_strings.test_serialize_vocab(en_vocab,text)
spacy.tests.serialize.test_serialize_vocab_strings.test_serialize_vocab_lex_attrs_bytes(strings,lex_attr)
spacy.tests.serialize.test_serialize_vocab_strings.test_serialize_vocab_lex_attrs_disk(strings,lex_attr)
spacy.tests.serialize.test_serialize_vocab_strings.test_serialize_vocab_roundtrip_bytes(strings1,strings2)
spacy.tests.serialize.test_serialize_vocab_strings.test_serialize_vocab_roundtrip_disk(strings1,strings2)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/vocab_vectors/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/vocab_vectors/test_vectors.py----------------------------------------
A:spacy.tests.vocab_vectors.test_vectors.OPS->get_current_ops()
A:spacy.tests.vocab_vectors.test_vectors.vectors->Vectors(data=data, keys=['A', 'B', 'C'])
A:spacy.tests.vocab_vectors.test_vectors.v->Vectors(data=data, keys=['a1', 'b1', 'c1', 'a2', 'c2'])
A:spacy.tests.vocab_vectors.test_vectors.vocab->Vocab()
A:spacy.tests.vocab_vectors.test_vectors.vector_data->numpy.zeros((3, 10), dtype='f')
A:spacy.tests.vocab_vectors.test_vectors.data->numpy.asarray([[0, 0, 0], [1, 2, 3], [9, 8, 7]], dtype='f')
A:spacy.tests.vocab_vectors.test_vectors.(keys, best_rows, scores)->Vectors(data=data, keys=['A', 'B', 'C']).most_similar(numpy.asarray([[9, 8, 7], [0, 0, 0]], dtype='f'))
A:spacy.tests.vocab_vectors.test_vectors.nlp->English()
A:spacy.tests.vocab_vectors.test_vectors.orig->numpy.asarray([[0, 0, 0], [1, 2, 3], [9, 8, 7]], dtype='f').copy()
A:spacy.tests.vocab_vectors.test_vectors.(_, best_rows, _)->Vectors(data=data, keys=['a1', 'b1', 'c1', 'a2', 'c2']).most_similar(v.data, batch_size=2, n=2, sort=True)
A:spacy.tests.vocab_vectors.test_vectors.(keys, _, scores)->Vectors(data=data, keys=['a1', 'b1', 'c1', 'a2', 'c2']).most_similar(numpy.asarray([[1, 2, 3]], dtype='f'))
A:spacy.tests.vocab_vectors.test_vectors.doc->Doc(vocab, words=text)
A:spacy.tests.vocab_vectors.test_vectors.token->tokenizer_v(text1)
A:spacy.tests.vocab_vectors.test_vectors.doc1->Doc(vocab, words=text1)
A:spacy.tests.vocab_vectors.test_vectors.doc2->Doc(vocab, words=text2)
A:spacy.tests.vocab_vectors.test_vectors.data[0]->get_current_ops().asarray([1.0, 1.2, 1.1])
A:spacy.tests.vocab_vectors.test_vectors.data[1]->get_current_ops().asarray([0.3, 1.3, 1.0])
A:spacy.tests.vocab_vectors.test_vectors.data[2]->get_current_ops().asarray([0.9, 1.22, 1.05])
A:spacy.tests.vocab_vectors.test_vectors.remap->Vocab().prune_vectors(2, batch_size=2)
A:spacy.tests.vocab_vectors.test_vectors.cosine->get_cosine(data[0], data[2])
A:spacy.tests.vocab_vectors.test_vectors.b->Vectors(data=data, keys=['a1', 'b1', 'c1', 'a2', 'c2']).to_bytes()
A:spacy.tests.vocab_vectors.test_vectors.v_r->Vectors()
A:spacy.tests.vocab_vectors.test_vectors.row->Vectors(data=data, keys=['a1', 'b1', 'c1', 'a2', 'c2']).add('D', vector=OPS.asarray([10, 20, 30, 40], dtype='f'))
A:spacy.tests.vocab_vectors.test_vectors.row_r->Vectors().add('D', vector=OPS.asarray([10, 20, 30, 40], dtype='f'))
A:spacy.tests.vocab_vectors.test_vectors.rows->get_current_ops().xp.asarray([h % nlp.vocab.vectors.shape[0] for ngram in ngrams for h in nlp.vocab.vectors._get_ngram_hashes(ngram)], dtype='uint32')
A:spacy.tests.vocab_vectors.test_vectors.vecs->get_current_ops().as_contig(v.data[rows])
A:spacy.tests.vocab_vectors.test_vectors.vocab_b->Vocab().to_bytes()
A:spacy.tests.vocab_vectors.test_vectors.nlp_plain->English()
A:spacy.tests.vocab_vectors.test_vectors.ngrams->English().vocab.vectors._get_ngrams(word)
A:spacy.tests.vocab_vectors.test_vectors.word->English().vocab.strings.as_string(word)
A:spacy.tests.vocab_vectors.test_vectors.single_vecs->get_current_ops().to_numpy(OPS.asarray([nlp.vocab[word].vector for word in words]))
A:spacy.tests.vocab_vectors.test_vectors.batch_vecs->get_current_ops().to_numpy(nlp.vocab.vectors.get_batch(words))
A:spacy.tests.vocab_vectors.test_vectors.vector->list(range(nlp.vocab.vectors.shape[1]))
A:spacy.tests.vocab_vectors.test_vectors.orig_bytes->English().vocab.vectors.to_bytes(exclude=['strings'])
A:spacy.tests.vocab_vectors.test_vectors.vocab_r->Vocab()
A:spacy.tests.vocab_vectors.test_vectors.vectors1->Vectors(shape=(10, 10))
A:spacy.tests.vocab_vectors.test_vectors.vectors2->Vectors(shape=(10, 10))
A:spacy.tests.vocab_vectors.test_vectors.nlp.vocab.vectors->Vectors(data=data, keys=['a', 'b', 'c'], attr='LOWER')
spacy.tests.vocab_vectors.test_vectors.data()
spacy.tests.vocab_vectors.test_vectors.floret_vectors_hashvec_str()
spacy.tests.vocab_vectors.test_vectors.floret_vectors_vec_str()
spacy.tests.vocab_vectors.test_vectors.most_similar_vectors_data()
spacy.tests.vocab_vectors.test_vectors.most_similar_vectors_keys()
spacy.tests.vocab_vectors.test_vectors.resize_data()
spacy.tests.vocab_vectors.test_vectors.strings()
spacy.tests.vocab_vectors.test_vectors.test_equality()
spacy.tests.vocab_vectors.test_vectors.test_floret_vectors(floret_vectors_vec_str,floret_vectors_hashvec_str)
spacy.tests.vocab_vectors.test_vectors.test_get_vector(strings,data)
spacy.tests.vocab_vectors.test_vectors.test_get_vector_resize(strings,data)
spacy.tests.vocab_vectors.test_vectors.test_init_vectors_unset()
spacy.tests.vocab_vectors.test_vectors.test_init_vectors_with_data(strings,data)
spacy.tests.vocab_vectors.test_vectors.test_init_vectors_with_resize_data(data,resize_data)
spacy.tests.vocab_vectors.test_vectors.test_init_vectors_with_resize_shape(strings,resize_data)
spacy.tests.vocab_vectors.test_vectors.test_init_vectors_with_shape(strings)
spacy.tests.vocab_vectors.test_vectors.test_issue1518()
spacy.tests.vocab_vectors.test_vectors.test_issue1539()
spacy.tests.vocab_vectors.test_vectors.test_issue1807()
spacy.tests.vocab_vectors.test_vectors.test_issue2871()
spacy.tests.vocab_vectors.test_vectors.test_issue3412()
spacy.tests.vocab_vectors.test_vectors.test_issue4725_2()
spacy.tests.vocab_vectors.test_vectors.test_set_vector(strings,data)
spacy.tests.vocab_vectors.test_vectors.test_vector_is_oov()
spacy.tests.vocab_vectors.test_vectors.test_vectors_attr()
spacy.tests.vocab_vectors.test_vectors.test_vectors_clear()
spacy.tests.vocab_vectors.test_vectors.test_vectors_deduplicate()
spacy.tests.vocab_vectors.test_vectors.test_vectors_doc_doc_similarity(vocab,text1,text2)
spacy.tests.vocab_vectors.test_vectors.test_vectors_doc_vector(vocab,text)
spacy.tests.vocab_vectors.test_vectors.test_vectors_get_batch()
spacy.tests.vocab_vectors.test_vectors.test_vectors_lexeme_doc_similarity(vocab,text)
spacy.tests.vocab_vectors.test_vectors.test_vectors_lexeme_lexeme_similarity(vocab,text1,text2)
spacy.tests.vocab_vectors.test_vectors.test_vectors_lexeme_span_similarity(vocab,text)
spacy.tests.vocab_vectors.test_vectors.test_vectors_lexeme_vector(vocab,text)
spacy.tests.vocab_vectors.test_vectors.test_vectors_most_similar(most_similar_vectors_data,most_similar_vectors_keys)
spacy.tests.vocab_vectors.test_vectors.test_vectors_most_similar_identical()
spacy.tests.vocab_vectors.test_vectors.test_vectors_serialize()
spacy.tests.vocab_vectors.test_vectors.test_vectors_span_doc_similarity(vocab,text)
spacy.tests.vocab_vectors.test_vectors.test_vectors_span_span_similarity(vocab,text)
spacy.tests.vocab_vectors.test_vectors.test_vectors_span_vector(vocab,text)
spacy.tests.vocab_vectors.test_vectors.test_vectors_token_doc_similarity(vocab,text)
spacy.tests.vocab_vectors.test_vectors.test_vectors_token_lexeme_similarity(tokenizer_v,vocab,text1,text2)
spacy.tests.vocab_vectors.test_vectors.test_vectors_token_span_similarity(vocab,text)
spacy.tests.vocab_vectors.test_vectors.test_vectors_token_token_similarity(tokenizer_v,text)
spacy.tests.vocab_vectors.test_vectors.test_vectors_token_vector(tokenizer_v,vectors,text)
spacy.tests.vocab_vectors.test_vectors.test_vocab_add_vector()
spacy.tests.vocab_vectors.test_vectors.test_vocab_prune_vectors()
spacy.tests.vocab_vectors.test_vectors.tokenizer_v(vocab)
spacy.tests.vocab_vectors.test_vectors.vectors()
spacy.tests.vocab_vectors.test_vectors.vocab(en_vocab,vectors)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/vocab_vectors/test_lookups.py----------------------------------------
A:spacy.tests.vocab_vectors.test_lookups.lookups->Lookups()
A:spacy.tests.vocab_vectors.test_lookups.table->Vocab().lookups.get_table(table_name)
A:spacy.tests.vocab_vectors.test_lookups.table_bytes->Vocab().lookups.get_table(table_name).to_bytes()
A:spacy.tests.vocab_vectors.test_lookups.new_table->Table().from_bytes(table_bytes)
A:spacy.tests.vocab_vectors.test_lookups.new_table2->Table(data={'def': 456})
A:spacy.tests.vocab_vectors.test_lookups.lookups_bytes->Lookups().to_bytes()
A:spacy.tests.vocab_vectors.test_lookups.new_lookups->Lookups()
A:spacy.tests.vocab_vectors.test_lookups.table1->Lookups().get_table('table1')
A:spacy.tests.vocab_vectors.test_lookups.table2->Lookups().get_table('table2')
A:spacy.tests.vocab_vectors.test_lookups.vocab->Vocab()
A:spacy.tests.vocab_vectors.test_lookups.vocab_bytes->Vocab().to_bytes()
A:spacy.tests.vocab_vectors.test_lookups.new_vocab->Vocab()
spacy.tests.vocab_vectors.test_lookups.test_lookups_api()
spacy.tests.vocab_vectors.test_lookups.test_lookups_to_from_bytes()
spacy.tests.vocab_vectors.test_lookups.test_lookups_to_from_bytes_via_vocab()
spacy.tests.vocab_vectors.test_lookups.test_lookups_to_from_disk()
spacy.tests.vocab_vectors.test_lookups.test_lookups_to_from_disk_via_vocab()
spacy.tests.vocab_vectors.test_lookups.test_table_api()
spacy.tests.vocab_vectors.test_lookups.test_table_api_to_from_bytes()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/vocab_vectors/test_lexeme.py----------------------------------------
A:spacy.tests.vocab_vectors.test_lexeme.vocab->Vocab(tag_map={'NN': {'pos': 'NOUN'}})
A:spacy.tests.vocab_vectors.test_lexeme.doc->Doc(vocab, words=['hello'])
A:spacy.tests.vocab_vectors.test_lexeme.is_len4->en_vocab.add_flag(lambda string: len(string) == 4, flag_id=IS_DIGIT)
spacy.tests.vocab_vectors.test_lexeme.test_issue361(en_vocab,text1,text2)
spacy.tests.vocab_vectors.test_lexeme.test_issue600()
spacy.tests.vocab_vectors.test_lexeme.test_vocab_lexeme_add_flag_auto_id(en_vocab)
spacy.tests.vocab_vectors.test_lexeme.test_vocab_lexeme_add_flag_provided_id(en_vocab)
spacy.tests.vocab_vectors.test_lexeme.test_vocab_lexeme_hash(en_vocab,text1,text2)
spacy.tests.vocab_vectors.test_lexeme.test_vocab_lexeme_is_alpha(en_vocab)
spacy.tests.vocab_vectors.test_lexeme.test_vocab_lexeme_is_digit(en_vocab)
spacy.tests.vocab_vectors.test_lexeme.test_vocab_lexeme_lt(en_vocab,text1,text2,prob1,prob2)
spacy.tests.vocab_vectors.test_lexeme.test_vocab_lexeme_oov_rank(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/vocab_vectors/test_stringstore.py----------------------------------------
A:spacy.tests.vocab_vectors.test_stringstore.h->stringstore.add(heart)
A:spacy.tests.vocab_vectors.test_stringstore.apple_hash->stringstore.add('apple')
A:spacy.tests.vocab_vectors.test_stringstore.banana_hash->stringstore.add('banana')
A:spacy.tests.vocab_vectors.test_stringstore.key->stringstore.add(text)
A:spacy.tests.vocab_vectors.test_stringstore.store->stringstore.add(text)
A:spacy.tests.vocab_vectors.test_stringstore.serialized->stringstore.to_bytes()
A:spacy.tests.vocab_vectors.test_stringstore.new_stringstore->StringStore().from_bytes(serialized)
spacy.tests.vocab_vectors.test_stringstore.stringstore()
spacy.tests.vocab_vectors.test_stringstore.test_string_hash(stringstore)
spacy.tests.vocab_vectors.test_stringstore.test_stringstore_from_api_docs(stringstore)
spacy.tests.vocab_vectors.test_stringstore.test_stringstore_long_string(stringstore)
spacy.tests.vocab_vectors.test_stringstore.test_stringstore_massive_strings(stringstore)
spacy.tests.vocab_vectors.test_stringstore.test_stringstore_med_string(stringstore,text1,text2)
spacy.tests.vocab_vectors.test_stringstore.test_stringstore_multiply(stringstore,factor)
spacy.tests.vocab_vectors.test_stringstore.test_stringstore_retrieve_id(stringstore,text)
spacy.tests.vocab_vectors.test_stringstore.test_stringstore_save_bytes(stringstore,text1,text2,text3)
spacy.tests.vocab_vectors.test_stringstore.test_stringstore_save_unicode(stringstore,text1,text2,text3)
spacy.tests.vocab_vectors.test_stringstore.test_stringstore_to_bytes(stringstore,text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/vocab_vectors/test_vocab_api.py----------------------------------------
A:spacy.tests.vocab_vectors.test_vocab_api.vocab->Vocab()
A:spacy.tests.vocab_vectors.test_vocab_api.int_id->Vocab().strings.add('some string')
A:spacy.tests.vocab_vectors.test_vocab_api.nlp->English()
spacy.tests.vocab_vectors.test_vocab_api.test_issue1868()
spacy.tests.vocab_vectors.test_vocab_api.test_to_disk()
spacy.tests.vocab_vectors.test_vocab_api.test_to_disk_exclude()
spacy.tests.vocab_vectors.test_vocab_api.test_vocab_api_contains(en_vocab,text)
spacy.tests.vocab_vectors.test_vocab_api.test_vocab_api_eq(en_vocab,text)
spacy.tests.vocab_vectors.test_vocab_api.test_vocab_api_neq(en_vocab,text1,text2)
spacy.tests.vocab_vectors.test_vocab_api.test_vocab_api_shape_attr(en_vocab,text)
spacy.tests.vocab_vectors.test_vocab_api.test_vocab_api_symbols(en_vocab,string,symbol)
spacy.tests.vocab_vectors.test_vocab_api.test_vocab_writing_system(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/vocab_vectors/test_memory_zone.py----------------------------------------
A:spacy.tests.vocab_vectors.test_memory_zone.vocab->Vocab()
spacy.tests.vocab_vectors.test_memory_zone.test_memory_zone_insertion()
spacy.tests.vocab_vectors.test_memory_zone.test_memory_zone_no_insertion()
spacy.tests.vocab_vectors.test_memory_zone.test_memory_zone_redundant_insertion()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/vocab_vectors/test_similarity.py----------------------------------------
A:spacy.tests.vocab_vectors.test_similarity.doc->Doc(vocab, words=[word1, word2])
A:spacy.tests.vocab_vectors.test_similarity.doc1->Doc(vocab, words=['a', 'b'])
A:spacy.tests.vocab_vectors.test_similarity.doc2->Doc(vocab, words=['c', 'd', 'e'])
A:spacy.tests.vocab_vectors.test_similarity.vocab->Vocab()
spacy.tests.vocab_vectors.test_similarity.test_issue2219(en_vocab)
spacy.tests.vocab_vectors.test_similarity.test_vectors_similarity_DD(vocab,vectors)
spacy.tests.vocab_vectors.test_similarity.test_vectors_similarity_DS(vocab,vectors)
spacy.tests.vocab_vectors.test_similarity.test_vectors_similarity_LL(vocab,vectors)
spacy.tests.vocab_vectors.test_similarity.test_vectors_similarity_SS(vocab,vectors)
spacy.tests.vocab_vectors.test_similarity.test_vectors_similarity_TD(vocab,vectors)
spacy.tests.vocab_vectors.test_similarity.test_vectors_similarity_TS(vocab,vectors)
spacy.tests.vocab_vectors.test_similarity.test_vectors_similarity_TT(vocab,vectors)
spacy.tests.vocab_vectors.test_similarity.test_vectors_similarity_no_vectors()
spacy.tests.vocab_vectors.test_similarity.vectors()
spacy.tests.vocab_vectors.test_similarity.vocab(en_vocab,vectors)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/doc/test_underscore.py----------------------------------------
A:spacy.tests.doc.test_underscore.doc->Doc(en_vocab, words=['hello', 'world'])
A:spacy.tests.doc.test_underscore.uscore->Underscore(Underscore.span_extensions, span, start=span.start, end=span.end)
A:spacy.tests.doc.test_underscore.doc._->Underscore(Underscore.doc_extensions, doc)
A:spacy.tests.doc.test_underscore.span->Mock(doc=Mock(), start=0, end=2)
A:spacy.tests.doc.test_underscore.span._->Underscore(Underscore.span_extensions, span, start=span.start, end=span.end)
A:spacy.tests.doc.test_underscore.token->Mock(doc=Mock(), idx=7, say_cheese=lambda token: 'cheese')
A:spacy.tests.doc.test_underscore.token._->Underscore(Underscore.token_extensions, token, start=token.idx)
A:spacy.tests.doc.test_underscore.doc1->Doc(en_vocab, words=['one'])
A:spacy.tests.doc.test_underscore.doc2->Doc(en_vocab, words=['two'])
spacy.tests.doc.test_underscore.clean_underscore()
spacy.tests.doc.test_underscore.test_create_doc_underscore()
spacy.tests.doc.test_underscore.test_create_span_underscore()
spacy.tests.doc.test_underscore.test_doc_underscore_getattr_setattr()
spacy.tests.doc.test_underscore.test_doc_underscore_remove_extension(obj)
spacy.tests.doc.test_underscore.test_span_underscore_getter_setter()
spacy.tests.doc.test_underscore.test_token_underscore_method()
spacy.tests.doc.test_underscore.test_underscore_accepts_valid(valid_kwargs)
spacy.tests.doc.test_underscore.test_underscore_dir(en_vocab)
spacy.tests.doc.test_underscore.test_underscore_docstring(en_vocab)
spacy.tests.doc.test_underscore.test_underscore_mutable_defaults_dict(en_vocab)
spacy.tests.doc.test_underscore.test_underscore_mutable_defaults_list(en_vocab)
spacy.tests.doc.test_underscore.test_underscore_raises_for_dup(obj)
spacy.tests.doc.test_underscore.test_underscore_raises_for_invalid(invalid_kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/doc/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/doc/test_retokenize_merge.py----------------------------------------
A:spacy.tests.doc.test_retokenize_merge.doc->Doc(tokens.vocab, words=[t.text for t in tokens], sent_starts=sent_starts)
A:spacy.tests.doc.test_retokenize_merge.new_doc->Doc(doc.vocab, words=['beach boys'])
A:spacy.tests.doc.test_retokenize_merge.tokens->en_tokenizer(text)
A:spacy.tests.doc.test_retokenize_merge.ent_type->max((w.ent_type_ for w in ent))
A:spacy.tests.doc.test_retokenize_merge.(sent1, sent2)->list(doc.sents)
A:spacy.tests.doc.test_retokenize_merge.init_len->len(list(sent1.root.subtree))
A:spacy.tests.doc.test_retokenize_merge.init_len2->len(sent2)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_lex_attrs(en_tokenizer)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_merge(en_tokenizer)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_merge_children(en_tokenizer)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_merge_extension_attrs(en_vocab)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_merge_extension_attrs_invalid(en_vocab,underscore_attrs)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_merge_hang(en_tokenizer)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_merge_without_parse_keeps_sents(en_tokenizer)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_retokenizer(en_tokenizer)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_retokenizer_attrs(en_tokenizer)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_span_np_merges(en_tokenizer)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_spans_entity_merge(en_tokenizer)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_spans_entity_merge_iob(en_vocab)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_spans_merge_heads(en_vocab)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_spans_merge_non_disjoint(en_tokenizer)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_spans_merge_tokens(en_tokenizer)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_spans_merge_tokens_default_attrs(en_vocab)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_spans_sentence_update_after_merge(en_tokenizer)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenize_spans_subtree_size_check(en_tokenizer)
spacy.tests.doc.test_retokenize_merge.test_doc_retokenizer_merge_lex_attrs(en_vocab)
spacy.tests.doc.test_retokenize_merge.test_retokenize_disallow_zero_length(en_vocab)
spacy.tests.doc.test_retokenize_merge.test_retokenize_skip_duplicates(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/doc/test_pickle_doc.py----------------------------------------
A:spacy.tests.doc.test_pickle_doc.nlp->Language()
A:spacy.tests.doc.test_pickle_doc.doc->nlp('Hello')
A:spacy.tests.doc.test_pickle_doc.data->spacy.compat.pickle.dumps(doc, 1)
A:spacy.tests.doc.test_pickle_doc.doc2->spacy.compat.pickle.loads(b)
A:spacy.tests.doc.test_pickle_doc.one_pickled->spacy.compat.pickle.dumps(nlp('0'), -1)
A:spacy.tests.doc.test_pickle_doc.docs->list(nlp.pipe((str(i) for i in range(100))))
A:spacy.tests.doc.test_pickle_doc.many_pickled->spacy.compat.pickle.dumps(docs, -1)
A:spacy.tests.doc.test_pickle_doc.many_unpickled->spacy.compat.pickle.loads(many_pickled)
A:spacy.tests.doc.test_pickle_doc.b->spacy.compat.pickle.dumps(doc)
spacy.tests.doc.test_pickle_doc.test_hooks_unpickle()
spacy.tests.doc.test_pickle_doc.test_list_of_docs_pickles_efficiently()
spacy.tests.doc.test_pickle_doc.test_pickle_single_doc()
spacy.tests.doc.test_pickle_doc.test_user_data_from_disk()
spacy.tests.doc.test_pickle_doc.test_user_data_unpickles()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/doc/test_doc_api.py----------------------------------------
A:spacy.tests.doc.test_doc_api.doc->en_tokenizer('Some text about Colombia and the Czech Republic')
A:spacy.tests.doc.test_doc_api.matrix->numpy.array([[0, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 2, 3, 3, 3], [1, 1, 3, 3, 3, 3], [1, 1, 3, 3, 4, 4], [1, 1, 3, 3, 4, 5]], dtype=numpy.int32)
A:spacy.tests.doc.test_doc_api.nlp->MultiLanguage()
A:spacy.tests.doc.test_doc_api.doc2->en_tokenizer('Some text about Colombia and the Czech Republic').copy()
A:spacy.tests.doc.test_doc_api.doc2_json->en_tokenizer('Some text about Colombia and the Czech Republic').copy().to_json()
A:spacy.tests.doc.test_doc_api.doc3->span3.as_doc()
A:spacy.tests.doc.test_doc_api.doc3_json->span3.as_doc().to_json()
A:spacy.tests.doc.test_doc_api.two_sent_doc->Doc(en_vocab, words=words, heads=heads, deps=deps)
A:spacy.tests.doc.test_doc_api.sents->list(doc.sents)
A:spacy.tests.doc.test_doc_api.sent_ext->self._get_my_ext(sent)
A:spacy.tests.doc.test_doc_api.docs->list(nlp.pipe(text, n_process=2))
A:spacy.tests.doc.test_doc_api.array->numpy.array(list(zip(pos, deps, tags)), dtype='uint64')
A:spacy.tests.doc.test_doc_api.tokens->en_tokenizer(text)
A:spacy.tests.doc.test_doc_api.new_tokens->Doc(tokens.vocab).from_bytes(tokens.to_bytes(exclude=['sentiment']), exclude=['sentiment'])
A:spacy.tests.doc.test_doc_api._->list(doc.noun_chunks)
A:spacy.tests.doc.test_doc_api.vocab->Vocab()
A:spacy.tests.doc.test_doc_api.lca->en_tokenizer('Some text about Colombia and the Czech Republic').get_lca_matrix()
A:spacy.tests.doc.test_doc_api.arr->en_tokenizer('Some text about Colombia and the Czech Republic').to_array(attrs)
A:spacy.tests.doc.test_doc_api.new_doc->Doc(en_vocab, words=words)
A:spacy.tests.doc.test_doc_api.attrs->en_tokenizer('Some text about Colombia and the Czech Republic')._get_array_attrs()
A:spacy.tests.doc.test_doc_api.span_group_texts->sorted([en_docs[0][1:4].text, en_docs[2][1:4].text, en_docs[4][0:1].text])
A:spacy.tests.doc.test_doc_api.de_doc->de_tokenizer(de_text)
A:spacy.tests.doc.test_doc_api.m_doc->spacy.tokens.Doc.from_docs(en_docs, exclude=['tensor'])
A:spacy.tests.doc.test_doc_api.ops->get_current_ops()
A:spacy.tests.doc.test_doc_api.doc.tensor->get_current_ops().asarray([[len(t.text), 0.0] for t in doc])
A:spacy.tests.doc.test_doc_api.doc1->en_tokenizer('Some text about Colombia and the Czech Republic')
A:spacy.tests.doc.test_doc_api.doc1b->en_tokenizer('c d')
spacy.tests.doc.test_doc_api.CustomPipe(self,nlp,name='my_pipe')
spacy.tests.doc.test_doc_api.CustomPipe.__init__(self,nlp,name='my_pipe')
spacy.tests.doc.test_doc_api.CustomPipe._get_my_ext(span)
spacy.tests.doc.test_doc_api.test_doc_api_compare_by_string_position(en_vocab,text)
spacy.tests.doc.test_doc_api.test_doc_api_from_docs(en_tokenizer,de_tokenizer)
spacy.tests.doc.test_doc_api.test_doc_api_from_docs_ents(en_tokenizer)
spacy.tests.doc.test_doc_api.test_doc_api_getitem(en_tokenizer)
spacy.tests.doc.test_doc_api.test_doc_api_has_vector()
spacy.tests.doc.test_doc_api.test_doc_api_init(en_vocab)
spacy.tests.doc.test_doc_api.test_doc_api_right_edge(en_vocab)
spacy.tests.doc.test_doc_api.test_doc_api_runtime_error(en_tokenizer)
spacy.tests.doc.test_doc_api.test_doc_api_sents_empty_string(en_tokenizer)
spacy.tests.doc.test_doc_api.test_doc_api_serialize(en_tokenizer,text)
spacy.tests.doc.test_doc_api.test_doc_api_set_ents(en_tokenizer)
spacy.tests.doc.test_doc_api.test_doc_api_similarity_match()
spacy.tests.doc.test_doc_api.test_doc_ents_setter()
spacy.tests.doc.test_doc_api.test_doc_from_array_morph(en_vocab)
spacy.tests.doc.test_doc_api.test_doc_from_array_sent_starts(en_vocab)
spacy.tests.doc.test_doc_api.test_doc_init_iob()
spacy.tests.doc.test_doc_api.test_doc_is_nered(en_vocab)
spacy.tests.doc.test_doc_api.test_doc_lang(en_vocab)
spacy.tests.doc.test_doc_api.test_doc_morph_setter(en_tokenizer,de_tokenizer)
spacy.tests.doc.test_doc_api.test_doc_noun_chunks_not_implemented()
spacy.tests.doc.test_doc_api.test_doc_set_ents(en_tokenizer)
spacy.tests.doc.test_doc_api.test_doc_set_ents_invalid_spans(en_tokenizer)
spacy.tests.doc.test_doc_api.test_doc_spans_copy(en_tokenizer)
spacy.tests.doc.test_doc_api.test_doc_spans_setdefault(en_tokenizer)
spacy.tests.doc.test_doc_api.test_has_annotation(en_vocab)
spacy.tests.doc.test_doc_api.test_has_annotation_sents(en_vocab)
spacy.tests.doc.test_doc_api.test_init_args_unmodified(en_vocab)
spacy.tests.doc.test_doc_api.test_is_flags_deprecated(en_tokenizer)
spacy.tests.doc.test_doc_api.test_issue1547()
spacy.tests.doc.test_doc_api.test_issue1757()
spacy.tests.doc.test_doc_api.test_issue2396(en_vocab)
spacy.tests.doc.test_doc_api.test_issue2782(text,lang_cls)
spacy.tests.doc.test_doc_api.test_issue3869(sentence)
spacy.tests.doc.test_doc_api.test_issue3962(en_vocab)
spacy.tests.doc.test_doc_api.test_issue3962_long(en_vocab)
spacy.tests.doc.test_doc_api.test_issue4903()
spacy.tests.doc.test_doc_api.test_issue5048(en_vocab)
spacy.tests.doc.test_doc_api.test_lowest_common_ancestor(en_vocab,words,heads,lca_matrix)
spacy.tests.doc.test_doc_api.test_span_groups(en_tokenizer)
spacy.tests.doc.test_doc_api.test_token_lexeme(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/doc/test_graph.py----------------------------------------
A:spacy.tests.doc.test_graph.doc->Doc(Vocab(), words=['a', 'b', 'c', 'd'])
A:spacy.tests.doc.test_graph.graph->Graph(doc, name='hello', nodes=[(0,), (1,), (2,), (3,)], edges=[(0, 1), (0, 2), (0, 3), (3, 0)], labels=None, weights=None)
A:spacy.tests.doc.test_graph.node1->Graph(doc, name='hello', nodes=[(0,), (1,), (2,), (3,)], edges=[(0, 1), (0, 2), (0, 3), (3, 0)], labels=None, weights=None).add_node((0,))
A:spacy.tests.doc.test_graph.node2->Graph(doc, name='hello', nodes=[(0,), (1,), (2,), (3,)], edges=[(0, 1), (0, 2), (0, 3), (3, 0)], labels=None, weights=None).add_node((1, 3))
A:spacy.tests.doc.test_graph.(node0, node1, node2, node3)->list(graph.nodes)
spacy.tests.doc.test_graph.test_graph_edges_and_nodes()
spacy.tests.doc.test_graph.test_graph_init()
spacy.tests.doc.test_graph.test_graph_walk()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/doc/test_creation.py----------------------------------------
A:spacy.tests.doc.test_creation.doc->Doc(vocab, words=words, spaces=spaces)
A:spacy.tests.doc.test_creation.(words, spaces)->spacy.util.get_words_and_spaces(words + ['away'], text)
A:spacy.tests.doc.test_creation.words->'I like ginger'.split()
A:spacy.tests.doc.test_creation.heads->list(range(len(words)))
A:spacy.tests.doc.test_creation.pos->'QQ ZZ XX'.split()
spacy.tests.doc.test_creation.test_create_from_words_and_text(vocab)
spacy.tests.doc.test_creation.test_create_invalid_pos(vocab)
spacy.tests.doc.test_creation.test_create_with_heads_and_no_deps(vocab)
spacy.tests.doc.test_creation.test_empty_doc(vocab)
spacy.tests.doc.test_creation.test_single_word(vocab)
spacy.tests.doc.test_creation.vocab()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/doc/test_span.py----------------------------------------
A:spacy.tests.doc.test_span.tokens->en_tokenizer(text)
A:spacy.tests.doc.test_span.doc->Doc(English().vocab, words=['This', 'is', 'a', 'test.', 'ENTITY'], sent_starts=[1, 0, 0, 0, 1])
A:spacy.tests.doc.test_span.sents->list(doc.ents[0].sents)
A:spacy.tests.doc.test_span.sent0->sents[0].as_doc()
A:spacy.tests.doc.test_span.sent1->sents[1].as_doc()
A:spacy.tests.doc.test_span.nlp->English()
A:spacy.tests.doc.test_span.text->nlp('Talk about being boring!')
A:spacy.tests.doc.test_span.text_var->nlp('Talk of being boring!')
A:spacy.tests.doc.test_span.y->nlp('Let')
A:spacy.tests.doc.test_span.span->Span(doc, 0, 1)
A:spacy.tests.doc.test_span.span1->Doc(English().vocab, words=['This', 'is', 'a', 'test.', 'ENTITY'], sent_starts=[1, 0, 0, 0, 1]).char_span(20, 45, label=label, kb_id=kb_id, span_id=span_id)
A:spacy.tests.doc.test_span.span2->doc[0:2].char_span(span1.start_char - 3, span1.end_char, label='GPE', alignment_mode='contract')
A:spacy.tests.doc.test_span.lca->doc[1:4].get_lca_matrix()
A:spacy.tests.doc.test_span.arr->Span(doc, 0, 1).to_array([ORTH, LENGTH])
A:spacy.tests.doc.test_span.span_doc->doc[0:5].as_doc()
A:spacy.tests.doc.test_span.span_doc_with->Span(doc, 0, 1).as_doc(copy_user_data=True)
A:spacy.tests.doc.test_span.span_doc_without->Span(doc, 0, 1).as_doc()
A:spacy.tests.doc.test_span.sentences->list(doc.sents)
A:spacy.tests.doc.test_span.filtered->filter_spans(spans)
A:spacy.tests.doc.test_span.ops->get_current_ops()
A:spacy.tests.doc.test_span.doc_copy->Doc(English().vocab, words=['This', 'is', 'a', 'test.', 'ENTITY'], sent_starts=[1, 0, 0, 0, 1]).copy()
A:spacy.tests.doc.test_span.doc1->en_tokenizer('a b')
A:spacy.tests.doc.test_span.doc2->en_tokenizer('b c')
spacy.tests.doc.test_span.doc(en_tokenizer)
spacy.tests.doc.test_span.doc_not_parsed(en_tokenizer)
spacy.tests.doc.test_span.test_char_span(doc,i_sent,i,j,text)
spacy.tests.doc.test_span.test_char_span_attributes(doc)
spacy.tests.doc.test_span.test_filter_spans(doc)
spacy.tests.doc.test_span.test_for_no_ent_sents()
spacy.tests.doc.test_span.test_for_partial_ent_sents()
spacy.tests.doc.test_span.test_issue1537()
spacy.tests.doc.test_span.test_issue1612(en_tokenizer)
spacy.tests.doc.test_span.test_issue3199()
spacy.tests.doc.test_span.test_issue5152()
spacy.tests.doc.test_span.test_issue6755(en_tokenizer)
spacy.tests.doc.test_span.test_issue6815_1(sentence,start_idx,end_idx,label)
spacy.tests.doc.test_span.test_issue6815_2(sentence,start_idx,end_idx,kb_id)
spacy.tests.doc.test_span.test_issue6815_3(sentence,start_idx,end_idx,vector)
spacy.tests.doc.test_span.test_sent(en_tokenizer)
spacy.tests.doc.test_span.test_span_api_richcmp_other(en_tokenizer)
spacy.tests.doc.test_span.test_span_as_doc(doc)
spacy.tests.doc.test_span.test_span_as_doc_user_data(doc)
spacy.tests.doc.test_span.test_span_attrs_writable(doc)
spacy.tests.doc.test_span.test_span_boundaries(doc)
spacy.tests.doc.test_span.test_span_comparison(doc)
spacy.tests.doc.test_span.test_span_ents_property(doc)
spacy.tests.doc.test_span.test_span_eq_hash(doc,doc_not_parsed)
spacy.tests.doc.test_span.test_span_group_copy(doc)
spacy.tests.doc.test_span.test_span_lemma(doc)
spacy.tests.doc.test_span.test_span_sents(doc,start,end,expected_sentences,expected_sentences_with_hook)
spacy.tests.doc.test_span.test_span_sents_not_parsed(doc_not_parsed)
spacy.tests.doc.test_span.test_span_similarity_match()
spacy.tests.doc.test_span.test_span_string_label_id(doc)
spacy.tests.doc.test_span.test_span_string_label_kb_id(doc)
spacy.tests.doc.test_span.test_span_to_array(doc)
spacy.tests.doc.test_span.test_span_with_vectors(doc)
spacy.tests.doc.test_span.test_spans_are_hashable(en_tokenizer)
spacy.tests.doc.test_span.test_spans_by_character(doc)
spacy.tests.doc.test_span.test_spans_default_sentiment(en_tokenizer)
spacy.tests.doc.test_span.test_spans_lca_matrix(en_tokenizer)
spacy.tests.doc.test_span.test_spans_override_sentiment(en_tokenizer)
spacy.tests.doc.test_span.test_spans_root(doc)
spacy.tests.doc.test_span.test_spans_root2(en_tokenizer)
spacy.tests.doc.test_span.test_spans_sent_spans(doc)
spacy.tests.doc.test_span.test_spans_span_sent(doc,doc_not_parsed)
spacy.tests.doc.test_span.test_spans_span_sent_user_hooks(doc,start,end,expected_sentence)
spacy.tests.doc.test_span.test_spans_string_fn(doc)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/doc/test_array.py----------------------------------------
A:spacy.tests.doc.test_array.doc->Doc(en_vocab, words=words)
A:spacy.tests.doc.test_array.doc_array->Doc(en_vocab, words=words).to_array(['TAG', 'LEMMA'])
A:spacy.tests.doc.test_array.new_doc->Doc(doc.vocab, words=words).from_array(['TAG', 'LEMMA'], doc_array)
A:spacy.tests.doc.test_array.feats_array->Doc(en_vocab, words=words).to_array((ORTH, DEP))
A:spacy.tests.doc.test_array.feats_array_stringy->Doc(en_vocab, words=words).to_array(('ORTH', 'SHAPE'))
A:spacy.tests.doc.test_array.offsets->Doc(en_vocab, words=words).to_array('IDX')
A:spacy.tests.doc.test_array.arr->Doc(en_vocab, words=words).to_array(['HEAD'])
A:spacy.tests.doc.test_array.doc_from_array->Doc(en_vocab, words=words)
A:spacy.tests.doc.test_array.arr[0]->numpy.int32(5).astype(numpy.uint64)
spacy.tests.doc.test_array.test_doc_array_attr_of_token(en_vocab)
spacy.tests.doc.test_array.test_doc_array_dep(en_vocab)
spacy.tests.doc.test_array.test_doc_array_idx(en_vocab)
spacy.tests.doc.test_array.test_doc_array_morph(en_vocab)
spacy.tests.doc.test_array.test_doc_array_tag(en_vocab)
spacy.tests.doc.test_array.test_doc_array_to_from_string_attrs(en_vocab,attrs)
spacy.tests.doc.test_array.test_doc_from_array_heads_in_bounds(en_vocab)
spacy.tests.doc.test_array.test_doc_scalar_attr_of_token(en_vocab)
spacy.tests.doc.test_array.test_doc_stringy_array_attr_of_token(en_vocab)
spacy.tests.doc.test_array.test_issue2203(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/doc/test_span_group.py----------------------------------------
A:spacy.tests.doc.test_span_group.doc->en_tokenizer('0 1 2 3 4 5 6')
A:spacy.tests.doc.test_span_group.matcher->Matcher(en_tokenizer.vocab, validate=True)
A:spacy.tests.doc.test_span_group.matches->matcher(doc)
A:spacy.tests.doc.test_span_group.doc.spans['SPANS']->SpanGroup(doc, name='SPANS', attrs={'key': 'value'}, spans=spans)
A:spacy.tests.doc.test_span_group.clone->SpanGroup(doc1, spans=[doc1[0:1], doc2[1:2]]).copy()
A:spacy.tests.doc.test_span_group.doc2->en_tokenizer('a b c')
A:spacy.tests.doc.test_span_group.doc3->en_tokenizer('0 1 2 3 4 5 6').copy()
A:spacy.tests.doc.test_span_group.span_group->SpanGroup(doc1, spans=[doc1[0:1], doc2[1:2]])
A:spacy.tests.doc.test_span_group.span->Span(other_doc, 0, 2)
A:spacy.tests.doc.test_span_group.span_group_2->SpanGroup(doc, name='MORE_SPANS', attrs={'key': 'new_value', 'new_key': 'new_value'}, spans=spans)
A:spacy.tests.doc.test_span_group.span_group_3->doc.spans['SPANS'].copy()._concat(span_group_2, inplace=True)
A:spacy.tests.doc.test_span_group.length->len(span_group)
A:spacy.tests.doc.test_span_group.span_group_3_expected->doc.spans['SPANS'].copy()._concat(span_group_2)
A:spacy.tests.doc.test_span_group.span_group_1->en_tokenizer('0 1 2 3 4 5 6').spans['SPANS'].copy()
A:spacy.tests.doc.test_span_group.span_group_1_expected->en_tokenizer('0 1 2 3 4 5 6').spans['SPANS'].copy()._concat(span_group_2)
A:spacy.tests.doc.test_span_group.doc1->en_tokenizer('a b c')
spacy.tests.doc.test_span_group.doc(en_tokenizer)
spacy.tests.doc.test_span_group.other_doc(en_tokenizer)
spacy.tests.doc.test_span_group.span_group(en_tokenizer)
spacy.tests.doc.test_span_group.test_span_doc_delitem(doc)
spacy.tests.doc.test_span_group.test_span_group_add(doc)
spacy.tests.doc.test_span_group.test_span_group_concat(doc,other_doc)
spacy.tests.doc.test_span_group.test_span_group_copy(doc)
spacy.tests.doc.test_span_group.test_span_group_dealloc(span_group)
spacy.tests.doc.test_span_group.test_span_group_extend(doc)
spacy.tests.doc.test_span_group.test_span_group_has_overlap(doc)
spacy.tests.doc.test_span_group.test_span_group_iadd(doc)
spacy.tests.doc.test_span_group.test_span_group_init_doc(en_tokenizer)
spacy.tests.doc.test_span_group.test_span_group_set_item(doc,other_doc)
spacy.tests.doc.test_span_group.test_span_group_typing(doc:Doc)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/doc/test_json_doc_conversion.py----------------------------------------
A:spacy.tests.doc.test_json_doc_conversion.json_doc->spacy.blank('en')('This is just brilliant.').to_json()
A:spacy.tests.doc.test_json_doc_conversion.new_doc->Doc(doc.vocab).from_json(json_doc, validate=True)
A:spacy.tests.doc.test_json_doc_conversion.doc->spacy.blank('en')('This is just brilliant.')
A:spacy.tests.doc.test_json_doc_conversion.doc_json->spacy.blank('en')('This is just brilliant.').to_json(underscore=['text_length'])
spacy.tests.doc.test_json_doc_conversion.doc(en_vocab)
spacy.tests.doc.test_json_doc_conversion.doc_json()
spacy.tests.doc.test_json_doc_conversion.doc_without_deps(en_vocab)
spacy.tests.doc.test_json_doc_conversion.test_doc_to_json(doc)
spacy.tests.doc.test_json_doc_conversion.test_doc_to_json_span(doc)
spacy.tests.doc.test_json_doc_conversion.test_doc_to_json_underscore(doc)
spacy.tests.doc.test_json_doc_conversion.test_doc_to_json_underscore_error_attr(doc)
spacy.tests.doc.test_json_doc_conversion.test_doc_to_json_underscore_error_serialize(doc)
spacy.tests.doc.test_json_doc_conversion.test_doc_to_json_with_custom_user_data(doc)
spacy.tests.doc.test_json_doc_conversion.test_doc_to_json_with_token_attributes_missing(doc)
spacy.tests.doc.test_json_doc_conversion.test_doc_to_json_with_token_span_attributes(doc)
spacy.tests.doc.test_json_doc_conversion.test_doc_to_json_with_token_span_same_identifier(doc)
spacy.tests.doc.test_json_doc_conversion.test_json_to_doc(doc)
spacy.tests.doc.test_json_doc_conversion.test_json_to_doc_attribute_consistency(doc)
spacy.tests.doc.test_json_doc_conversion.test_json_to_doc_cats(doc)
spacy.tests.doc.test_json_doc_conversion.test_json_to_doc_compat(doc,doc_json)
spacy.tests.doc.test_json_doc_conversion.test_json_to_doc_sents(doc,doc_without_deps)
spacy.tests.doc.test_json_doc_conversion.test_json_to_doc_spaces()
spacy.tests.doc.test_json_doc_conversion.test_json_to_doc_spans(doc)
spacy.tests.doc.test_json_doc_conversion.test_json_to_doc_underscore(doc)
spacy.tests.doc.test_json_doc_conversion.test_json_to_doc_validation_error(doc)
spacy.tests.doc.test_json_doc_conversion.test_json_to_doc_with_token_span_attributes(doc)
spacy.tests.doc.test_json_doc_conversion.test_to_json_underscore_doc_getters(doc)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/doc/test_retokenize_split.py----------------------------------------
A:spacy.tests.doc.test_retokenize_split.tensor->numpy.asarray([[1.0, 1.1], [2.0, 2.1], [3.0, 3.1], [4.0, 4.1], [5.0, 5.1], [6.0, 6.1]], dtype='f')
A:spacy.tests.doc.test_retokenize_split.doc->Doc(en_vocab, words=text.split())
A:spacy.tests.doc.test_retokenize_split.dep1->Doc(en_vocab, words=text.split()).vocab.strings.add('amod')
A:spacy.tests.doc.test_retokenize_split.dep2->Doc(en_vocab, words=text.split()).vocab.strings.add('subject')
A:spacy.tests.doc.test_retokenize_split.(sent1, sent2)->list(doc.sents)
A:spacy.tests.doc.test_retokenize_split.init_len->len(sent1)
A:spacy.tests.doc.test_retokenize_split.init_len2->len(sent2)
spacy.tests.doc.test_retokenize_split.test_doc_retokenize_spans_entity_split_iob()
spacy.tests.doc.test_retokenize_split.test_doc_retokenize_spans_sentence_update_after_split(en_vocab)
spacy.tests.doc.test_retokenize_split.test_doc_retokenize_split(en_vocab)
spacy.tests.doc.test_retokenize_split.test_doc_retokenize_split_dependencies(en_vocab)
spacy.tests.doc.test_retokenize_split.test_doc_retokenize_split_extension_attrs(en_vocab)
spacy.tests.doc.test_retokenize_split.test_doc_retokenize_split_extension_attrs_invalid(en_vocab,underscore_attrs)
spacy.tests.doc.test_retokenize_split.test_doc_retokenize_split_heads_error(en_vocab)
spacy.tests.doc.test_retokenize_split.test_doc_retokenize_split_lemmas(en_vocab)
spacy.tests.doc.test_retokenize_split.test_doc_retokenize_split_orths_mismatch(en_vocab)
spacy.tests.doc.test_retokenize_split.test_doc_retokenizer_realloc(en_vocab)
spacy.tests.doc.test_retokenize_split.test_doc_retokenizer_split_lex_attrs(en_vocab)
spacy.tests.doc.test_retokenize_split.test_doc_retokenizer_split_norm(en_vocab)
spacy.tests.doc.test_retokenize_split.test_issue3540(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/doc/test_add_entities.py----------------------------------------
A:spacy.tests.doc.test_add_entities.doc->Doc(en_vocab, words=text)
A:spacy.tests.doc.test_add_entities.ner->EntityRecognizer(en_vocab, model)
A:spacy.tests.doc.test_add_entities.doc.ents->list(doc.ents)
A:spacy.tests.doc.test_add_entities.entity->Span(doc, 0, 4, label=391)
A:spacy.tests.doc.test_add_entities.new_entity->Span(doc, 0, 1, label=392)
spacy.tests.doc.test_add_entities._ner_example(ner)
spacy.tests.doc.test_add_entities.test_add_overlapping_entities(en_vocab)
spacy.tests.doc.test_add_entities.test_doc_add_entities_set_ents_iob(en_vocab)
spacy.tests.doc.test_add_entities.test_ents_reset(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/doc/test_morphanalysis.py----------------------------------------
A:spacy.tests.doc.test_morphanalysis.doc->tokenizer('a dog')
spacy.tests.doc.test_morphanalysis.i_has(en_tokenizer)
spacy.tests.doc.test_morphanalysis.test_morph_get(i_has)
spacy.tests.doc.test_morphanalysis.test_morph_iter(i_has)
spacy.tests.doc.test_morphanalysis.test_morph_property(tokenizer)
spacy.tests.doc.test_morphanalysis.test_morph_props(i_has)
spacy.tests.doc.test_morphanalysis.test_morph_set(i_has)
spacy.tests.doc.test_morphanalysis.test_morph_str(i_has)
spacy.tests.doc.test_morphanalysis.test_token_morph_eq(i_has)
spacy.tests.doc.test_morphanalysis.test_token_morph_key(i_has)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/doc/test_token_api.py----------------------------------------
A:spacy.tests.doc.test_token_api.doc->Doc(en_vocab, words=words, heads=heads, deps=deps)
A:spacy.tests.doc.test_token_api.tokens->en_tokenizer(text)
A:spacy.tests.doc.test_token_api.vocab->Vocab()
A:spacy.tests.doc.test_token_api.doc2->en_tokenizer('b c')
A:spacy.tests.doc.test_token_api.example->spacy.training.Example.from_dict(doc, {'heads': heads, 'deps': deps})
A:spacy.tests.doc.test_token_api.(aligned_heads, aligned_deps)->spacy.training.Example.from_dict(doc, {'heads': heads, 'deps': deps}).get_aligned_parse(projectivize=True)
A:spacy.tests.doc.test_token_api.doc1->en_tokenizer('a b')
spacy.tests.doc.test_token_api.doc(en_vocab)
spacy.tests.doc.test_token_api.test_doc_token_api_ancestors(en_vocab)
spacy.tests.doc.test_token_api.test_doc_token_api_flags(en_tokenizer)
spacy.tests.doc.test_token_api.test_doc_token_api_head_setter(en_vocab)
spacy.tests.doc.test_token_api.test_doc_token_api_is_properties(en_vocab)
spacy.tests.doc.test_token_api.test_doc_token_api_prob_inherited_from_vocab(en_tokenizer,text)
spacy.tests.doc.test_token_api.test_doc_token_api_str_builtin(en_tokenizer,text)
spacy.tests.doc.test_token_api.test_doc_token_api_strings(en_vocab)
spacy.tests.doc.test_token_api.test_doc_token_api_vectors()
spacy.tests.doc.test_token_api.test_is_sent_end(en_tokenizer)
spacy.tests.doc.test_token_api.test_is_sent_start(en_tokenizer)
spacy.tests.doc.test_token_api.test_missing_head_dep(en_vocab)
spacy.tests.doc.test_token_api.test_set_invalid_pos()
spacy.tests.doc.test_token_api.test_set_pos()
spacy.tests.doc.test_token_api.test_token0_has_sent_start_true()
spacy.tests.doc.test_token_api.test_token_api_conjuncts_chain(en_vocab)
spacy.tests.doc.test_token_api.test_token_api_conjuncts_simple(en_vocab)
spacy.tests.doc.test_token_api.test_token_api_non_conjuncts(en_vocab)
spacy.tests.doc.test_token_api.test_token_api_richcmp_other(en_tokenizer)
spacy.tests.doc.test_token_api.test_tokenlast_has_sent_end_true()
spacy.tests.doc.test_token_api.test_tokens_sent(doc)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/matcher/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/matcher/test_pattern_validation.py----------------------------------------
A:spacy.tests.matcher.test_pattern_validation.matcher->Matcher(en_vocab)
A:spacy.tests.matcher.test_pattern_validation.errors->validate_token_pattern(pattern)
spacy.tests.matcher.test_pattern_validation.test_matcher_pattern_validation(en_vocab,pattern)
spacy.tests.matcher.test_pattern_validation.test_minimal_pattern_validation(en_vocab,pattern,n_errors,n_min_errors)
spacy.tests.matcher.test_pattern_validation.test_pattern_errors(en_vocab)
spacy.tests.matcher.test_pattern_validation.test_pattern_validation(pattern,n_errors,_)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/matcher/test_levenshtein.py----------------------------------------
spacy.tests.matcher.test_levenshtein.test_levenshtein(dist,a,b)
spacy.tests.matcher.test_levenshtein.test_levenshtein_compare(a,b,fuzzy,expected)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/matcher/test_matcher_api.py----------------------------------------
A:spacy.tests.matcher.test_matcher_api.matcher->Matcher(en_vocab)
A:spacy.tests.matcher.test_matcher_api.(on_match, patterns)->Matcher(en_vocab).get('Rule')
A:spacy.tests.matcher.test_matcher_api.doc->Doc(en_vocab, words=['foo', 'bar', 'foo', 'foo', 'bar', 'foo', 'foo', 'foo', 'bar', 'bar'])
A:spacy.tests.matcher.test_matcher_api.on_match->Mock()
A:spacy.tests.matcher.test_matcher_api.spans->matcher(doc, as_spans=True)
A:spacy.tests.matcher.test_matcher_api.matches->matcher(doc)
A:spacy.tests.matcher.test_matcher_api.words1->'He said , " some words " ...'.split()
A:spacy.tests.matcher.test_matcher_api.words2->'He said , " some three words " ...'.split()
A:spacy.tests.matcher.test_matcher_api.words->'He said , " some words " ...'.split()
A:spacy.tests.matcher.test_matcher_api.control->Matcher(matcher.vocab)
A:spacy.tests.matcher.test_matcher_api.m->matcher(doc)
A:spacy.tests.matcher.test_matcher_api.doc1->Doc(en_vocab, words=['I', 'visited', 'New', 'York', 'and', 'California'])
A:spacy.tests.matcher.test_matcher_api.doc2->Doc(en_vocab, words=['I', 'visited', 'my', 'friend', 'Alicia'])
A:spacy.tests.matcher.test_matcher_api.doc3->Doc(en_vocab, words=['Test'])
A:spacy.tests.matcher.test_matcher_api.mock->Mock()
spacy.tests.matcher.test_matcher_api.matcher(en_vocab)
spacy.tests.matcher.test_matcher_api.test_attr_pipeline_checks(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_add_new_api(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_any_token_operator(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_as_spans(matcher)
spacy.tests.matcher.test_matcher_api.test_matcher_basic_check(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_callback(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_callback_with_alignments(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_compare_length(en_vocab,cmp,bad)
spacy.tests.matcher.test_matcher_api.test_matcher_deprecated(matcher)
spacy.tests.matcher.test_matcher_api.test_matcher_empty_dict(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_empty_patterns_warns(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_ent_iob_key(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_extension_attribute(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_extension_in_set_predicate(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_extension_set_membership(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_from_api_docs(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_from_usage_docs(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_intersect_value_operator(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_len_contains(matcher)
spacy.tests.matcher.test_matcher_api.test_matcher_match_end(matcher)
spacy.tests.matcher.test_matcher_api.test_matcher_match_fuzzy(en_vocab,rules,match_locs)
spacy.tests.matcher.test_matcher_api.test_matcher_match_fuzzy_set_multiple(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_match_fuzzy_set_op_longest(en_vocab,set_op)
spacy.tests.matcher.test_matcher_api.test_matcher_match_fuzzyn_all_insertions(en_vocab,fuzzyn)
spacy.tests.matcher.test_matcher_api.test_matcher_match_fuzzyn_set_multiple(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_match_fuzzyn_set_op_longest(en_vocab,greedy,set_op)
spacy.tests.matcher.test_matcher_api.test_matcher_match_fuzzyn_various_edits(en_vocab,fuzzyn)
spacy.tests.matcher.test_matcher_api.test_matcher_match_middle(matcher)
spacy.tests.matcher.test_matcher_api.test_matcher_match_multi(matcher)
spacy.tests.matcher.test_matcher_api.test_matcher_match_one_plus(matcher)
spacy.tests.matcher.test_matcher_api.test_matcher_match_start(matcher)
spacy.tests.matcher.test_matcher_api.test_matcher_match_zero(matcher)
spacy.tests.matcher.test_matcher_api.test_matcher_match_zero_plus(matcher)
spacy.tests.matcher.test_matcher_api.test_matcher_min_max_operator(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_morph_handling(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_no_match(matcher)
spacy.tests.matcher.test_matcher_api.test_matcher_no_zero_length(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_operator_shadow(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_regex(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_regex_set_in(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_regex_set_not_in(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_regex_shape(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_remove_zero_operator(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_schema_token_attributes(en_vocab,pattern,text)
spacy.tests.matcher.test_matcher_api.test_matcher_set_value(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_set_value_operator(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_span(matcher)
spacy.tests.matcher.test_matcher_api.test_matcher_subset_value_operator(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_superset_value_operator(en_vocab)
spacy.tests.matcher.test_matcher_api.test_matcher_valid_callback(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/matcher/test_matcher_logic.py----------------------------------------
A:spacy.tests.matcher.test_matcher_logic.doc->Doc(matcher.vocab, words=list(string))
A:spacy.tests.matcher.test_matcher_logic.matcher->Matcher(en_vocab)
A:spacy.tests.matcher.test_matcher_logic.ents->list(doc.ents)
A:spacy.tests.matcher.test_matcher_logic.matches->matcher(doc, with_alignments=True)
A:spacy.tests.matcher.test_matcher_logic.entities->list(doc.ents)
A:spacy.tests.matcher.test_matcher_logic.vocab->Vocab(lex_attr_getters=LEX_ATTRS)
A:spacy.tests.matcher.test_matcher_logic.match->matcher(doc)
A:spacy.tests.matcher.test_matcher_logic.hello_world->Doc(vocab, words=['Hello', 'World'])
A:spacy.tests.matcher.test_matcher_logic.hello->Doc(vocab, words=['Hello'])
A:spacy.tests.matcher.test_matcher_logic.matched->sorted(matched, key=len, reverse=True)
A:spacy.tests.matcher.test_matcher_logic.nlp->English()
A:spacy.tests.matcher.test_matcher_logic.doc1->Doc(en_vocab, words=['a'])
A:spacy.tests.matcher.test_matcher_logic.doc2->Doc(en_vocab, words=['a', 'b', 'c'])
A:spacy.tests.matcher.test_matcher_logic.matches1->matcher(doc1)
A:spacy.tests.matcher.test_matcher_logic.matches2->matcher(doc2)
A:spacy.tests.matcher.test_matcher_logic.doc3->Doc(en_vocab, words=['a', 'b', 'b', 'c'])
A:spacy.tests.matcher.test_matcher_logic.doc4->Doc(en_vocab, words=['a', 'b', 'b', 'c'])
A:spacy.tests.matcher.test_matcher_logic.results1->matcher(nlp(text))
A:spacy.tests.matcher.test_matcher_logic.results2->matcher(nlp(text))
A:spacy.tests.matcher.test_matcher_logic.n_matches->len(matches)
spacy.tests.matcher.test_matcher_logic.doc(en_tokenizer,text)
spacy.tests.matcher.test_matcher_logic.test_greedy_matching_first(doc,text,pattern,re_pattern)
spacy.tests.matcher.test_matcher_logic.test_greedy_matching_longest(doc,text,pattern,longest)
spacy.tests.matcher.test_matcher_logic.test_greedy_matching_longest_first(en_tokenizer)
spacy.tests.matcher.test_matcher_logic.test_invalid_greediness(doc,text)
spacy.tests.matcher.test_matcher_logic.test_issue118(en_tokenizer,patterns)
spacy.tests.matcher.test_matcher_logic.test_issue118_prefix_reorder(en_tokenizer,patterns)
spacy.tests.matcher.test_matcher_logic.test_issue1434()
spacy.tests.matcher.test_matcher_logic.test_issue1450(string,start,end)
spacy.tests.matcher.test_matcher_logic.test_issue1945()
spacy.tests.matcher.test_matcher_logic.test_issue1971(en_vocab)
spacy.tests.matcher.test_matcher_logic.test_issue242(en_tokenizer)
spacy.tests.matcher.test_matcher_logic.test_issue2464(en_vocab)
spacy.tests.matcher.test_matcher_logic.test_issue2569(en_tokenizer)
spacy.tests.matcher.test_matcher_logic.test_issue2671()
spacy.tests.matcher.test_matcher_logic.test_issue3009(en_vocab)
spacy.tests.matcher.test_matcher_logic.test_issue3328(en_vocab)
spacy.tests.matcher.test_matcher_logic.test_issue3549(en_vocab)
spacy.tests.matcher.test_matcher_logic.test_issue3555(en_vocab)
spacy.tests.matcher.test_matcher_logic.test_issue3839(en_vocab)
spacy.tests.matcher.test_matcher_logic.test_issue3879(en_vocab)
spacy.tests.matcher.test_matcher_logic.test_issue3951(en_vocab)
spacy.tests.matcher.test_matcher_logic.test_issue4120(en_vocab)
spacy.tests.matcher.test_matcher_logic.test_issue587(en_tokenizer)
spacy.tests.matcher.test_matcher_logic.test_issue588(en_vocab)
spacy.tests.matcher.test_matcher_logic.test_issue590(en_vocab)
spacy.tests.matcher.test_matcher_logic.test_issue615(en_tokenizer)
spacy.tests.matcher.test_matcher_logic.test_issue850()
spacy.tests.matcher.test_matcher_logic.test_issue850_basic()
spacy.tests.matcher.test_matcher_logic.test_issue_1971_2(en_vocab)
spacy.tests.matcher.test_matcher_logic.test_issue_1971_3(en_vocab)
spacy.tests.matcher.test_matcher_logic.test_issue_1971_4(en_vocab)
spacy.tests.matcher.test_matcher_logic.test_match_consuming(doc,text,pattern,re_pattern)
spacy.tests.matcher.test_matcher_logic.test_matcher_end_zero_plus(en_vocab)
spacy.tests.matcher.test_matcher_logic.test_matcher_remove()
spacy.tests.matcher.test_matcher_logic.test_matcher_sets_return_correct_tokens(en_vocab)
spacy.tests.matcher.test_matcher_logic.test_matcher_with_alignments_greedy_longest(en_vocab)
spacy.tests.matcher.test_matcher_logic.test_matcher_with_alignments_non_greedy(en_vocab)
spacy.tests.matcher.test_matcher_logic.test_operator_combos(en_vocab)
spacy.tests.matcher.test_matcher_logic.text()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/matcher/test_phrase_matcher.py----------------------------------------
A:spacy.tests.matcher.test_phrase_matcher.nlp->English()
A:spacy.tests.matcher.test_phrase_matcher.matcher->PhraseMatcher(en_vocab)
A:spacy.tests.matcher.test_phrase_matcher.doc->Doc(en_vocab, words=words)
A:spacy.tests.matcher.test_phrase_matcher.matches->matcher(doc, as_spans=True)
A:spacy.tests.matcher.test_phrase_matcher.pattern1->Doc(en_vocab, words=['this'])
A:spacy.tests.matcher.test_phrase_matcher.pattern2->Doc(en_vocab, words=['this', 'is'])
A:spacy.tests.matcher.test_phrase_matcher.ruler->English().add_pipe('entity_ruler', config={'phrase_matcher_attr': 'LOWER'})
A:spacy.tests.matcher.test_phrase_matcher.nlp_reloaded->English()
A:spacy.tests.matcher.test_phrase_matcher.doc_reloaded->nlp_reloaded(text)
A:spacy.tests.matcher.test_phrase_matcher.pattern->Doc(en_vocab, words=['Spans', 'and', 'Docs'])
A:spacy.tests.matcher.test_phrase_matcher.new_matches->matcher(doc)
A:spacy.tests.matcher.test_phrase_matcher.no_matches->matcher(doc)
A:spacy.tests.matcher.test_phrase_matcher.on_match->Mock()
A:spacy.tests.matcher.test_phrase_matcher.doc1->Doc(en_vocab, words=['Test'])
A:spacy.tests.matcher.test_phrase_matcher.doc2->Doc(en_vocab, words=['Test'])
A:spacy.tests.matcher.test_phrase_matcher.doc3->Doc(en_vocab, words=['Test'])
A:spacy.tests.matcher.test_phrase_matcher.mock->Mock()
A:spacy.tests.matcher.test_phrase_matcher.pattern3->Doc(en_vocab, words=['this', 'is', 'a'])
A:spacy.tests.matcher.test_phrase_matcher.pattern4->Doc(en_vocab, words=['this', 'is', 'a', 'word'])
A:spacy.tests.matcher.test_phrase_matcher.b->srsly.pickle_dumps(matcher)
A:spacy.tests.matcher.test_phrase_matcher.matcher_unpickled->srsly.pickle_loads(b)
A:spacy.tests.matcher.test_phrase_matcher.matches_unpickled->matcher_unpickled(doc)
A:spacy.tests.matcher.test_phrase_matcher._->PhraseMatcher(en_vocab, attr=attr)
A:spacy.tests.matcher.test_phrase_matcher.matches_doc->matcher(doc)
A:spacy.tests.matcher.test_phrase_matcher.matches_span->matcher(span)
spacy.tests.matcher.test_phrase_matcher.test_attr_pipeline_checks(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_attr_validation(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_issue10643(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_issue3248_1()
spacy.tests.matcher.test_phrase_matcher.test_issue3331(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_issue3972(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_issue4002(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_issue4373()
spacy.tests.matcher.test_phrase_matcher.test_issue4651_with_phrase_matcher_attr()
spacy.tests.matcher.test_phrase_matcher.test_issue6839(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_matcher_phrase_matcher(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_phrase_matcher_add_new_api(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_phrase_matcher_as_spans(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_phrase_matcher_basic_check(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_phrase_matcher_bool_attrs(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_phrase_matcher_callback(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_phrase_matcher_contains(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_phrase_matcher_deprecated(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_phrase_matcher_length(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_phrase_matcher_overlapping_with_remove(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_phrase_matcher_pickle(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_phrase_matcher_remove(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_phrase_matcher_remove_overlapping_patterns(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_phrase_matcher_repeated_add(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_phrase_matcher_sent_start(en_vocab,attr)
spacy.tests.matcher.test_phrase_matcher.test_phrase_matcher_string_attrs(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_phrase_matcher_string_attrs_negative(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_phrase_matcher_validation(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_span_in_phrasematcher(en_vocab)
spacy.tests.matcher.test_phrase_matcher.test_span_v_doc_in_phrasematcher(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/matcher/test_dependency_matcher.py----------------------------------------
A:spacy.tests.matcher.test_dependency_matcher.IS_BROWN_YELLOW->en_vocab.add_flag(is_brown_yellow)
A:spacy.tests.matcher.test_dependency_matcher.matcher->DependencyMatcher(en_tokenizer.vocab)
A:spacy.tests.matcher.test_dependency_matcher.mock->Mock()
A:spacy.tests.matcher.test_dependency_matcher.matches->matcher(doc)
A:spacy.tests.matcher.test_dependency_matcher.b->pickle.dumps(matcher)
A:spacy.tests.matcher.test_dependency_matcher.matcher_r->pickle.loads(b)
A:spacy.tests.matcher.test_dependency_matcher.pattern2->copy.deepcopy(pattern)
A:spacy.tests.matcher.test_dependency_matcher.matcher2->DependencyMatcher(en_vocab)
A:spacy.tests.matcher.test_dependency_matcher.matches2->matcher2(doc)
A:spacy.tests.matcher.test_dependency_matcher.doc->en_tokenizer('The red book')
A:spacy.tests.matcher.test_dependency_matcher.doc_matches->matcher(doc)
A:spacy.tests.matcher.test_dependency_matcher.span_matches->matcher(doc[offset:])
spacy.tests.matcher.test_dependency_matcher.dependency_matcher(en_vocab,patterns,doc)
spacy.tests.matcher.test_dependency_matcher.doc(en_vocab)
spacy.tests.matcher.test_dependency_matcher.patterns(en_vocab)
spacy.tests.matcher.test_dependency_matcher.test_dependency_matcher(dependency_matcher,doc,patterns)
spacy.tests.matcher.test_dependency_matcher.test_dependency_matcher_callback(en_vocab,doc)
spacy.tests.matcher.test_dependency_matcher.test_dependency_matcher_long_matches(en_vocab,doc)
spacy.tests.matcher.test_dependency_matcher.test_dependency_matcher_ops(en_vocab,doc,left,right,op,num_matches)
spacy.tests.matcher.test_dependency_matcher.test_dependency_matcher_order_issue(en_tokenizer)
spacy.tests.matcher.test_dependency_matcher.test_dependency_matcher_pattern_validation(en_vocab)
spacy.tests.matcher.test_dependency_matcher.test_dependency_matcher_pickle(en_vocab,patterns,doc)
spacy.tests.matcher.test_dependency_matcher.test_dependency_matcher_precedence_ops(en_vocab,op,num_matches)
spacy.tests.matcher.test_dependency_matcher.test_dependency_matcher_remove(en_tokenizer)
spacy.tests.matcher.test_dependency_matcher.test_dependency_matcher_span_user_data(en_tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/tokenizer/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/tokenizer/test_whitespace.py----------------------------------------
A:spacy.tests.tokenizer.test_whitespace.tokens->tokenizer(text)
spacy.tests.tokenizer.test_whitespace.test_tokenizer_handles_double_trailing_ws(tokenizer,text)
spacy.tests.tokenizer.test_whitespace.test_tokenizer_splits_double_space(tokenizer,text)
spacy.tests.tokenizer.test_whitespace.test_tokenizer_splits_newline(tokenizer,text)
spacy.tests.tokenizer.test_whitespace.test_tokenizer_splits_newline_double_space(tokenizer,text)
spacy.tests.tokenizer.test_whitespace.test_tokenizer_splits_newline_space(tokenizer,text)
spacy.tests.tokenizer.test_whitespace.test_tokenizer_splits_newline_space_wrap(tokenizer,text)
spacy.tests.tokenizer.test_whitespace.test_tokenizer_splits_single_space(tokenizer,text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/tokenizer/test_exceptions.py----------------------------------------
A:spacy.tests.tokenizer.test_exceptions.tokens->tokenizer(text)
spacy.tests.tokenizer.test_exceptions.test_tokenizer_degree(tokenizer)
spacy.tests.tokenizer.test_exceptions.test_tokenizer_excludes_false_pos_emoticons(tokenizer,text,length)
spacy.tests.tokenizer.test_exceptions.test_tokenizer_handles_emoji(tokenizer,text,length)
spacy.tests.tokenizer.test_exceptions.test_tokenizer_handles_emoticons(tokenizer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/tokenizer/test_naughty_strings.py----------------------------------------
A:spacy.tests.tokenizer.test_naughty_strings.tokens->tokenizer(text)
spacy.tests.tokenizer.test_naughty_strings.test_tokenizer_naughty_strings(tokenizer,text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/tokenizer/test_tokenizer.py----------------------------------------
A:spacy.tests.tokenizer.test_tokenizer.doc->en_tokenizer(text)
A:spacy.tests.tokenizer.test_tokenizer.s->set([token])
A:spacy.tests.tokenizer.test_tokenizer.items->list(s)
A:spacy.tests.tokenizer.test_tokenizer.doc.tensor->numpy.ones((len(doc), 128), dtype='f')
A:spacy.tests.tokenizer.test_tokenizer.nlp->English()
A:spacy.tests.tokenizer.test_tokenizer.docs->list(nlp.pipe(['', 'hello']))
A:spacy.tests.tokenizer.test_tokenizer.doc1->Doc(Vocab(), words=['a', 'b', 'c'])
A:spacy.tests.tokenizer.test_tokenizer.doc2->Doc(Vocab(), words=['a', 'c', 'e'])
A:spacy.tests.tokenizer.test_tokenizer.prefix_re->compile_prefix_regex(prefixes)
A:spacy.tests.tokenizer.test_tokenizer.suffix_re->compile_suffix_regex(suffixes)
A:spacy.tests.tokenizer.test_tokenizer.infix_re->compile_infix_regex(infixes)
A:spacy.tests.tokenizer.test_tokenizer.simple_url_re->re.compile('^https?://')
A:spacy.tests.tokenizer.test_tokenizer.nlp.tokenizer->new_tokenizer(nlp)
A:spacy.tests.tokenizer.test_tokenizer.a->en_tokenizer('a')
A:spacy.tests.tokenizer.test_tokenizer.am->en_tokenizer('am')
A:spacy.tests.tokenizer.test_tokenizer.t1->nlp(text1)
A:spacy.tests.tokenizer.test_tokenizer.t2->nlp(text2)
A:spacy.tests.tokenizer.test_tokenizer.t3->nlp(text3)
A:spacy.tests.tokenizer.test_tokenizer.tokens->tokenizer(text)
A:spacy.tests.tokenizer.test_tokenizer.text->infile.read()
A:spacy.tests.tokenizer.test_tokenizer.tokens1->tokenizer(text1)
A:spacy.tests.tokenizer.test_tokenizer.tokens2->tokenizer(text2)
A:spacy.tests.tokenizer.test_tokenizer.vocab->Vocab()
A:spacy.tests.tokenizer.test_tokenizer.tokenizer->Tokenizer(en_vocab, token_match=re.compile('^id$').match, rules={'id': [{'ORTH': 'i'}, {'ORTH': 'd'}]})
A:spacy.tests.tokenizer.test_tokenizer.tokenizer1->Tokenizer(en_vocab, suffix_search=suffix_re.search, rules=rules)
spacy.tests.tokenizer.test_tokenizer.test_gold_misaligned(en_tokenizer,text,words)
spacy.tests.tokenizer.test_tokenizer.test_issue10086(en_tokenizer)
spacy.tests.tokenizer.test_tokenizer.test_issue1061()
spacy.tests.tokenizer.test_tokenizer.test_issue1235()
spacy.tests.tokenizer.test_tokenizer.test_issue1242()
spacy.tests.tokenizer.test_tokenizer.test_issue1257()
spacy.tests.tokenizer.test_tokenizer.test_issue1375()
spacy.tests.tokenizer.test_tokenizer.test_issue1488()
spacy.tests.tokenizer.test_tokenizer.test_issue1494()
spacy.tests.tokenizer.test_tokenizer.test_issue1963(en_tokenizer)
spacy.tests.tokenizer.test_tokenizer.test_issue2070()
spacy.tests.tokenizer.test_tokenizer.test_issue2626_2835(en_tokenizer,text)
spacy.tests.tokenizer.test_tokenizer.test_issue2656(en_tokenizer)
spacy.tests.tokenizer.test_tokenizer.test_issue2754(en_tokenizer)
spacy.tests.tokenizer.test_tokenizer.test_issue2926(fr_tokenizer)
spacy.tests.tokenizer.test_tokenizer.test_issue3002()
spacy.tests.tokenizer.test_tokenizer.test_issue3449()
spacy.tests.tokenizer.test_tokenizer.test_issue743()
spacy.tests.tokenizer.test_tokenizer.test_issue801(en_tokenizer,text,tokens)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_add_special_case(tokenizer,text,tokens)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_add_special_case_tag(text,tokens)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_colons(tokenizer,text)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_flush_cache(en_vocab)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_flush_specials(en_vocab)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_handle_text_from_file(tokenizer,file_name)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_handles_digits(tokenizer)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_handles_long_text(tokenizer)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_handles_no_word(tokenizer)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_handles_punct(tokenizer)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_handles_punct_braces(tokenizer)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_handles_single_word(tokenizer,text)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_infix_prefix(en_vocab)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_initial_special_case_explain(en_vocab)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_keep_urls(tokenizer,text)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_keeps_email(tokenizer,text)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_prefix_suffix_overlap_lookbehind(en_vocab)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_special_cases_idx(tokenizer)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_special_cases_spaces(tokenizer)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_special_cases_with_affixes(tokenizer)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_special_cases_with_affixes_preserve_spacy()
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_special_cases_with_period(tokenizer)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_suspected_freeing_strings(tokenizer)
spacy.tests.tokenizer.test_tokenizer.test_tokenizer_validate_special_case(tokenizer,text,tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/tokenizer/test_explain.py----------------------------------------
A:spacy.tests.tokenizer.test_explain.examples->pytest.importorskip(f'spacy.lang.{lang}.examples')
A:spacy.tests.tokenizer.test_explain.suffix_re->re.compile('[\\.]$')
A:spacy.tests.tokenizer.test_explain.infix_re->re.compile('[/]')
A:spacy.tests.tokenizer.test_explain.tokenizer->Tokenizer(en_vocab, rules=rules)
A:spacy.tests.tokenizer.test_explain.punctuation_and_space_regex->'|'.join([*[re.escape(p) for p in string.punctuation], '\\s'])
A:spacy.tests.tokenizer.test_explain.sentence->re.sub('\\s+', ' ', sentence).strip()
spacy.tests.tokenizer.test_explain.sentence_strategy(draw:hypothesis.strategies.DrawFn,max_n_words:int=4)->str
spacy.tests.tokenizer.test_explain.test_tokenizer_explain(lang)
spacy.tests.tokenizer.test_explain.test_tokenizer_explain_fuzzy(lang:str,sentence:str)->None
spacy.tests.tokenizer.test_explain.test_tokenizer_explain_special_matcher(en_vocab)
spacy.tests.tokenizer.test_explain.test_tokenizer_explain_special_matcher_whitespace(en_vocab)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/tokenizer/test_urls.py----------------------------------------
A:spacy.tests.tokenizer.test_urls.tokens->tokenizer(url + suffix1 + suffix2)
spacy.tests.tokenizer.test_urls.test_should_match(en_tokenizer,url)
spacy.tests.tokenizer.test_urls.test_should_not_match(en_tokenizer,url)
spacy.tests.tokenizer.test_urls.test_tokenizer_handles_prefixed_url(tokenizer,prefix,url)
spacy.tests.tokenizer.test_urls.test_tokenizer_handles_simple_surround_url(tokenizer,url)
spacy.tests.tokenizer.test_urls.test_tokenizer_handles_simple_url(tokenizer,url)
spacy.tests.tokenizer.test_urls.test_tokenizer_handles_suffixed_url(tokenizer,url,suffix)
spacy.tests.tokenizer.test_urls.test_tokenizer_handles_surround_url(tokenizer,prefix,suffix,url)
spacy.tests.tokenizer.test_urls.test_tokenizer_handles_two_prefix_url(tokenizer,prefix1,prefix2,url)
spacy.tests.tokenizer.test_urls.test_tokenizer_handles_two_suffix_url(tokenizer,suffix1,suffix2,url)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/package/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/package/test_requirements.py----------------------------------------
A:spacy.tests.package.test_requirements.lines->f.readlines()
A:spacy.tests.package.test_requirements.line->line.strip().strip(',').strip('"').strip().strip(',').strip('"')
A:spacy.tests.package.test_requirements.(lib, v)->_parse_req(line)
A:spacy.tests.package.test_requirements.setup_keys->set()
A:spacy.tests.package.test_requirements.req_v->req_dict.get(lib, None)
A:spacy.tests.package.test_requirements.lib->re.match('^[a-z0-9\\-]*', line).group(0)
A:spacy.tests.package.test_requirements.v->line.strip().strip(',').strip('"').strip().strip(',').strip('"').replace(lib, '').strip()
spacy.tests.package.test_requirements._parse_req(line)
spacy.tests.package.test_requirements.test_build_dependencies()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/training/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/training/test_readers.py----------------------------------------
A:spacy.tests.training.test_readers.doc->nlp('Quick test')
A:spacy.tests.training.test_readers.config->Config().from_str(nlp_config_string)
A:spacy.tests.training.test_readers.nlp->load_model_from_config(config, auto_fill=True)
A:spacy.tests.training.test_readers.T->spacy.util.registry.resolve(nlp.config['training'], schema=ConfigSchemaTraining)
A:spacy.tests.training.test_readers.(train_corpus, dev_corpus)->resolve_dot_names(nlp.config, dot_names)
A:spacy.tests.training.test_readers.scores->load_model_from_config(config, auto_fill=True).evaluate(dev_examples)
A:spacy.tests.training.test_readers.dev_examples->list(dev_corpus(nlp))
spacy.tests.training.test_readers.test_cat_readers(reader,additional_config)
spacy.tests.training.test_readers.test_readers()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/training/test_rehearse.py----------------------------------------
A:spacy.tests.training.test_rehearse.pipe->_optimize(nlp, component, TRAIN_DATA, False).get_pipe(component)
A:spacy.tests.training.test_rehearse.optimizer->_optimize(nlp, component, TRAIN_DATA, False).initialize()
A:spacy.tests.training.test_rehearse.doc->_optimize(nlp, component, TRAIN_DATA, False).make_doc(text)
A:spacy.tests.training.test_rehearse.example->spacy.training.Example.from_dict(doc, annotation)
A:spacy.tests.training.test_rehearse.nlp->_optimize(nlp, component, TRAIN_DATA, False)
spacy.tests.training.test_rehearse._add_ner_label(ner,data)
spacy.tests.training.test_rehearse._add_parser_label(parser,data)
spacy.tests.training.test_rehearse._add_tagger_label(tagger,data)
spacy.tests.training.test_rehearse._add_textcat_label(textcat,data)
spacy.tests.training.test_rehearse._optimize(nlp,component:str,data:List,rehearse:bool)
spacy.tests.training.test_rehearse.test_rehearse(component)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/training/test_new_example.py----------------------------------------
A:spacy.tests.training.test_new_example.vocab->Vocab()
A:spacy.tests.training.test_new_example.example->spacy.training.example.Example.from_dict(predicted, annots)
A:spacy.tests.training.test_new_example.predicted->Doc(vocab, words=annots['words'])
A:spacy.tests.training.test_new_example.aligned_tags->spacy.training.example.Example.from_dict(predicted, annots).get_aligned('TAG', as_string=True)
A:spacy.tests.training.test_new_example.example1->spacy.training.example.Example.from_dict(predicted, annots)
A:spacy.tests.training.test_new_example.aligned_tags1->spacy.training.example.Example.from_dict(predicted, annots).get_aligned('TAG', as_string=True)
A:spacy.tests.training.test_new_example.example2->spacy.training.example.Example.from_dict(predicted, example1.to_dict())
A:spacy.tests.training.test_new_example.aligned_tags2->spacy.training.example.Example.from_dict(predicted, example1.to_dict()).get_aligned('TAG', as_string=True)
A:spacy.tests.training.test_new_example.ex->spacy.training.example.Example.from_dict(predicted, annots)
A:spacy.tests.training.test_new_example.example_1->spacy.training.example.Example.from_dict(predicted, annots_head_only)
A:spacy.tests.training.test_new_example.example_2->spacy.training.example.Example.from_dict(predicted, annots_head_dep)
A:spacy.tests.training.test_new_example.reference->Doc(en_vocab, words=words, tags=tags)
A:spacy.tests.training.test_new_example.output_dict->spacy.training.example.Example.from_dict(predicted, annots).to_dict()
A:spacy.tests.training.test_new_example.output_example->spacy.training.example.Example.from_dict(predicted, output_dict)
spacy.tests.training.test_new_example.test_Example_aligned_whitespace(en_vocab)
spacy.tests.training.test_new_example.test_Example_from_dict_basic()
spacy.tests.training.test_new_example.test_Example_from_dict_invalid(annots)
spacy.tests.training.test_new_example.test_Example_from_dict_sentences()
spacy.tests.training.test_new_example.test_Example_from_dict_with_cats(annots)
spacy.tests.training.test_new_example.test_Example_from_dict_with_empty_entities()
spacy.tests.training.test_new_example.test_Example_from_dict_with_entities(annots)
spacy.tests.training.test_new_example.test_Example_from_dict_with_entities_invalid(annots)
spacy.tests.training.test_new_example.test_Example_from_dict_with_entities_overlapping(annots)
spacy.tests.training.test_new_example.test_Example_from_dict_with_links(annots)
spacy.tests.training.test_new_example.test_Example_from_dict_with_links_invalid(annots)
spacy.tests.training.test_new_example.test_Example_from_dict_with_morphology(annots)
spacy.tests.training.test_new_example.test_Example_from_dict_with_parse(annots)
spacy.tests.training.test_new_example.test_Example_from_dict_with_sent_start(annots)
spacy.tests.training.test_new_example.test_Example_from_dict_with_spans(annots)
spacy.tests.training.test_new_example.test_Example_from_dict_with_spans_invalid(annots)
spacy.tests.training.test_new_example.test_Example_from_dict_with_spans_overlapping(annots)
spacy.tests.training.test_new_example.test_Example_from_dict_with_tags(pred_words,annots)
spacy.tests.training.test_new_example.test_Example_init_requires_doc_objects()
spacy.tests.training.test_new_example.test_Example_missing_deps()
spacy.tests.training.test_new_example.test_Example_missing_heads()
spacy.tests.training.test_new_example.test_aligned_tags()
spacy.tests.training.test_new_example.test_aligned_tags_multi()
spacy.tests.training.test_new_example.test_issue11260()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/training/test_augmenters.py----------------------------------------
A:spacy.tests.training.test_augmenters.doc->Doc(nlp.vocab, words=words, spaces=spaces, tags=tags, lemmas=lemmas, heads=heads, deps=deps, ents=ents)
A:spacy.tests.training.test_augmenters.augmenter->create_lower_casing_augmenter(level=1.0)
A:spacy.tests.training.test_augmenters.reader->Corpus(output_file, augmenter=create_spongebob_augmenter())
A:spacy.tests.training.test_augmenters.corpus->list(reader(nlp))
A:spacy.tests.training.test_augmenters.example_dict->Example(nlp.make_doc(text), doc).to_dict()
A:spacy.tests.training.test_augmenters.example->Example(nlp.make_doc(text), doc)
A:spacy.tests.training.test_augmenters.mod_ex->make_whitespace_variant(nlp, example, ' ', 5)
A:spacy.tests.training.test_augmenters.mod_ex2->make_whitespace_variant(nlp, mod_ex, '\t\t\n', j)
spacy.tests.training.test_augmenters.doc(nlp)
spacy.tests.training.test_augmenters.make_docbin(docs,name='roundtrip.spacy')
spacy.tests.training.test_augmenters.nlp()
spacy.tests.training.test_augmenters.test_custom_data_augmentation(nlp,doc)
spacy.tests.training.test_augmenters.test_lowercase_augmenter(nlp,doc)
spacy.tests.training.test_augmenters.test_make_orth_variants(nlp)
spacy.tests.training.test_augmenters.test_make_whitespace_variant(nlp)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/training/test_pretraining.py----------------------------------------
A:spacy.tests.training.test_pretraining.config->Config().from_str(pretrain_string_internal)
A:spacy.tests.training.test_pretraining.nlp->English()
A:spacy.tests.training.test_pretraining.pretrain_config->spacy.util.load_config(DEFAULT_CONFIG_PRETRAIN_PATH)
A:spacy.tests.training.test_pretraining.filled->filled.interpolate().interpolate()
A:spacy.tests.training.test_pretraining.file_path->write_sample_jsonl(pretrain_dir)
A:spacy.tests.training.test_pretraining.nlp_path->write_vectors_model(tmp_dir)
A:spacy.tests.training.test_pretraining.train_config->spacy.util.load_config(DEFAULT_CONFIG_PATH)
A:spacy.tests.training.test_pretraining.(train_path, dev_path)->write_sample_training(train_dir)
A:spacy.tests.training.test_pretraining.nlp_base->init_nlp(filled)
A:spacy.tests.training.test_pretraining.model_base->init_nlp(filled).get_pipe(P['component']).model.get_ref(P['layer']).get_ref('embed')
A:spacy.tests.training.test_pretraining.pretrained_model->Path(pretrain_dir / 'model3.bin')
A:spacy.tests.training.test_pretraining.filled['initialize']['init_tok2vec']->str(pretrained_model)
A:spacy.tests.training.test_pretraining.model->English().get_pipe(P['component']).model.get_ref(P['layer']).get_ref('embed')
A:spacy.tests.training.test_pretraining.doc->Doc(English().vocab, words=words, tags=tags)
A:spacy.tests.training.test_pretraining.doc_bin->DocBin()
A:spacy.tests.training.test_pretraining.vocab->Vocab()
A:spacy.tests.training.test_pretraining.nlp.vocab.vectors->Vectors()
spacy.tests.training.test_pretraining.test_pretrain_default_vectors()
spacy.tests.training.test_pretraining.test_pretraining_default()
spacy.tests.training.test_pretraining.test_pretraining_tagger()
spacy.tests.training.test_pretraining.test_pretraining_tagger_tok2vec(config)
spacy.tests.training.test_pretraining.test_pretraining_tok2vec_characters(objective,skip_last)
spacy.tests.training.test_pretraining.test_pretraining_tok2vec_vectors(objective)
spacy.tests.training.test_pretraining.test_pretraining_tok2vec_vectors_fail(objective)
spacy.tests.training.test_pretraining.test_pretraining_training()
spacy.tests.training.test_pretraining.write_sample_jsonl(tmp_dir)
spacy.tests.training.test_pretraining.write_sample_training(tmp_dir)
spacy.tests.training.test_pretraining.write_vectors_model(tmp_dir)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/training/test_corpus.py----------------------------------------
A:spacy.tests.training.test_corpus.nlp->English()
A:spacy.tests.training.test_corpus.corpus->PlainTextCorpus(file_path, min_length=min_length, max_length=max_length)
A:spacy.tests.training.test_corpus.(reference, predicted)->_examples_to_tokens(corpus(nlp))
spacy.tests.training.test_corpus._examples_to_tokens(examples:Iterable[Example])->Tuple[List[List[str]], List[List[str]]]
spacy.tests.training.test_corpus._string_to_tmp_file(s:str)->Generator[Path, None, None]
spacy.tests.training.test_corpus.test_plain_text_reader(min_length,max_length)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/training/test_training.py----------------------------------------
A:spacy.tests.training.test_training.nlp->spacy.blank('en')
A:spacy.tests.training.test_training.doc->nlp('This is a sentence')
A:spacy.tests.training.test_training.ner->spacy.blank('en').add_pipe('ner')
A:spacy.tests.training.test_training.example->Example(doc1, doc2)
A:spacy.tests.training.test_training.nlp2->load_model_from_path(model_dir)
A:spacy.tests.training.test_training.docs->list(json_to_docs(data))
A:spacy.tests.training.test_training.data->DocBin(docs=docs, attrs=attrs).to_bytes()
A:spacy.tests.training.test_training.reader->Corpus(output_file)
A:spacy.tests.training.test_training.train_data->list(reader(nlp))
A:spacy.tests.training.test_training.optimizer->Adam()
A:spacy.tests.training.test_training.docs1->list(nlp.pipe(texts, batch_size=1))
A:spacy.tests.training.test_training.docs2->list(nlp.pipe(texts, batch_size=4))
A:spacy.tests.training.test_training.tags->Example(doc1, doc2).get_aligned('TAG', as_string=True)
A:spacy.tests.training.test_training.predicted->Doc(en_vocab, words=other_tokens, spaces=[True, False, False, True, False, False])
A:spacy.tests.training.test_training.reference->Doc(en_vocab, words=spacy_tokens, spaces=[True, True, True, False, True, False])
A:spacy.tests.training.test_training.ner_tags->Example(doc1, doc2).get_aligned_ner()
A:spacy.tests.training.test_training.eg->Example(Doc(doc.vocab, words=[w.text for w in doc], spaces=[bool(w.whitespace_) for w in doc]), doc)
A:spacy.tests.training.test_training.split_examples->Example(doc1, doc2).split_sents()
A:spacy.tests.training.test_training.(words, spaces)->get_words_and_spaces(['I', 'flew', 'to', 'San Francisco', 'Valley', '.'], 'I flew  to San Francisco Valley.')
A:spacy.tests.training.test_training.biluo_tags_converted->offsets_to_biluo_tags(doc, offsets)
A:spacy.tests.training.test_training.offsets_converted->biluo_tags_to_offsets(doc, biluo_tags)
A:spacy.tests.training.test_training.spans->biluo_tags_to_spans(doc, biluo_tags)
A:spacy.tests.training.test_training.ents_y2x->Example(doc1, doc2).get_aligned_spans_y2x(ents_ref)
A:spacy.tests.training.test_training.ruler->spacy.blank('en').add_pipe('entity_ruler')
A:spacy.tests.training.test_training.ents_x2y->Example(doc1, doc2).get_aligned_spans_x2y(ents_pred)
A:spacy.tests.training.test_training.gold_doc->spacy.blank('en').make_doc(text)
A:spacy.tests.training.test_training.spans_y2x_no_overlap->Example(doc1, doc2).get_aligned_spans_y2x(spans_gold, allow_overlap=False)
A:spacy.tests.training.test_training.spans_y2x_overlap->Example(doc1, doc2).get_aligned_spans_y2x(spans_gold, allow_overlap=True)
A:spacy.tests.training.test_training.(proj_heads, proj_labels)->Example(doc1, doc2).get_aligned_parse(projectivize=True)
A:spacy.tests.training.test_training.(nonproj_heads, nonproj_labels)->Example(doc1, doc2).get_aligned_parse(projectivize=False)
A:spacy.tests.training.test_training.doc_a->Doc(doc.vocab, words=['Double-Jointed'], spaces=[False], deps=['ROOT'], heads=[0])
A:spacy.tests.training.test_training.doc_b->Doc(doc.vocab, words=['Double', '-', 'Jointed'], spaces=[True, True, True], deps=['amod', 'punct', 'ROOT'], heads=[2, 2, 2])
A:spacy.tests.training.test_training.(proj_heads, proj_deps)->Example(doc1, doc2).get_aligned_parse(projectivize=True)
A:spacy.tests.training.test_training.converted_biluo->iob_to_biluo(good_iob)
A:spacy.tests.training.test_training.reloaded_nlp->English()
A:spacy.tests.training.test_training.reloaded_examples->list(reader(reloaded_nlp))
A:spacy.tests.training.test_training.reloaded_docs->DocBin().from_disk(output_file).get_docs(nlp.vocab)
A:spacy.tests.training.test_training.doc.user_data['check']->set()
A:spacy.tests.training.test_training.(a2b, b2a)->get_alignments(tokens_b, tokens_a)
A:spacy.tests.training.test_training.batches->minibatch(train_examples, size=compounding(4.0, 32.0, 1.001))
A:spacy.tests.training.test_training.align->spacy.training.Alignment.from_strings(other_tokens, spacy_tokens)
A:spacy.tests.training.test_training.a->nlp('This is a sentence').to_array(['TAG'])
A:spacy.tests.training.test_training.doc1->Doc(doc.vocab, words=[t.text for t in doc]).from_array(['TAG'], a)
A:spacy.tests.training.test_training.doc2->Doc(doc.vocab, words=[t.text for t in doc]).from_array(['TAG'], a)
A:spacy.tests.training.test_training.generator->train_while_improving(nlp, optimizer, generate_batch(), lambda : None, dropout=0.1, eval_frequency=100, accumulate_gradient=10, patience=10, max_steps=100, exclude=[], annotating_components=[], before_update=before_update)
spacy.tests.training.test_training._train_tuples(train_data)
spacy.tests.training.test_training.doc()
spacy.tests.training.test_training.merged_dict()
spacy.tests.training.test_training.test_align(tokens_a,tokens_b,expected)
spacy.tests.training.test_training.test_aligned_spans_x2y(en_vocab,en_tokenizer)
spacy.tests.training.test_training.test_aligned_spans_y2x(en_vocab,en_tokenizer)
spacy.tests.training.test_training.test_aligned_spans_y2x_overlap(en_vocab,en_tokenizer)
spacy.tests.training.test_training.test_alignment()
spacy.tests.training.test_training.test_alignment_array()
spacy.tests.training.test_training.test_alignment_case_insensitive()
spacy.tests.training.test_training.test_alignment_complex()
spacy.tests.training.test_training.test_alignment_complex_example(en_vocab)
spacy.tests.training.test_training.test_alignment_different_texts()
spacy.tests.training.test_training.test_alignment_spaces(en_vocab)
spacy.tests.training.test_training.test_biluo_spans(en_tokenizer)
spacy.tests.training.test_training.test_docbin_user_data_not_serialized(doc)
spacy.tests.training.test_training.test_docbin_user_data_serialized(doc)
spacy.tests.training.test_training.test_example_constructor(en_vocab)
spacy.tests.training.test_training.test_example_from_dict_no_ner(en_vocab)
spacy.tests.training.test_training.test_example_from_dict_some_ner(en_vocab)
spacy.tests.training.test_training.test_example_from_dict_tags(en_vocab)
spacy.tests.training.test_training.test_gold_biluo_4791(en_vocab,en_tokenizer)
spacy.tests.training.test_training.test_gold_biluo_BIL(en_vocab)
spacy.tests.training.test_training.test_gold_biluo_BL(en_vocab)
spacy.tests.training.test_training.test_gold_biluo_U(en_vocab)
spacy.tests.training.test_training.test_gold_biluo_additional_whitespace(en_vocab,en_tokenizer)
spacy.tests.training.test_training.test_gold_biluo_many_to_one(en_vocab,en_tokenizer)
spacy.tests.training.test_training.test_gold_biluo_misalign(en_vocab)
spacy.tests.training.test_training.test_gold_biluo_misaligned(en_vocab,en_tokenizer)
spacy.tests.training.test_training.test_gold_biluo_one_to_many(en_vocab,en_tokenizer)
spacy.tests.training.test_training.test_gold_biluo_overlap(en_vocab)
spacy.tests.training.test_training.test_gold_constructor()
spacy.tests.training.test_training.test_gold_ner_missing_tags(en_tokenizer)
spacy.tests.training.test_training.test_goldparse_endswith_space(en_tokenizer)
spacy.tests.training.test_training.test_goldparse_startswith_space(en_tokenizer)
spacy.tests.training.test_training.test_iob_to_biluo()
spacy.tests.training.test_training.test_issue4402()
spacy.tests.training.test_training.test_issue7029()
spacy.tests.training.test_training.test_issue999()
spacy.tests.training.test_training.test_json_to_docs_no_ner(en_vocab)
spacy.tests.training.test_training.test_projectivize(en_tokenizer)
spacy.tests.training.test_training.test_retokenized_docs(doc)
spacy.tests.training.test_training.test_roundtrip_docs_to_docbin(doc)
spacy.tests.training.test_training.test_roundtrip_offsets_biluo_conversion(en_tokenizer)
spacy.tests.training.test_training.test_split_sentences(en_vocab)
spacy.tests.training.test_training.test_split_sents(merged_dict)
spacy.tests.training.test_training.test_training_before_update(doc)
spacy.tests.training.test_training.test_tuple_format_implicit()
spacy.tests.training.test_training.test_tuple_format_implicit_invalid()
spacy.tests.training.test_training.vocab()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tests/training/test_logger.py----------------------------------------
A:spacy.tests.training.test_logger.nlp->spacy.blank('en')
A:spacy.tests.training.test_logger.console_logger->spacy.training.loggers.console_logger(progress_bar=True, console_output=True, output_file=None)
A:spacy.tests.training.test_logger.(log_step, finalize)->console_logger(nlp)
spacy.tests.training.test_logger.info()
spacy.tests.training.test_logger.nlp()
spacy.tests.training.test_logger.test_console_logger(nlp,info)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/char_classes.py----------------------------------------
A:spacy.lang.char_classes.ALPHA->group_chars(LATIN + _russian + _tatar + _greek + _ukrainian + _macedonian + _uncased)
A:spacy.lang.char_classes.ALPHA_LOWER->group_chars(_lower + _uncased)
A:spacy.lang.char_classes.ALPHA_UPPER->group_chars(_upper + _uncased)
A:spacy.lang.char_classes.UNITS->merge_chars(_units)
A:spacy.lang.char_classes.CURRENCY->merge_chars(_currency)
A:spacy.lang.char_classes.PUNCT->merge_chars(_punct)
A:spacy.lang.char_classes.HYPHENS->merge_chars(_hyphens)
A:spacy.lang.char_classes.LIST_UNITS->split_chars(_units)
A:spacy.lang.char_classes.LIST_CURRENCY->split_chars(_currency)
A:spacy.lang.char_classes.LIST_QUOTES->split_chars(_quotes)
A:spacy.lang.char_classes.LIST_PUNCT->split_chars(_punct)
A:spacy.lang.char_classes.LIST_HYPHENS->split_chars(_hyphens)
A:spacy.lang.char_classes.CONCAT_QUOTES->group_chars(_quotes)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/lex_attrs.py----------------------------------------
A:spacy.lang.lex_attrs._tlds->set('com|org|edu|gov|net|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|pro|tel|travel|xyz|icu|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|sk|sl|sm|sn|so|sr|ss|st|su|sv|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|za|zm|zw'.split('|'))
A:spacy.lang.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.lex_attrs.get_lang(text:str,lang:str='')->str
spacy.lang.lex_attrs.is_alpha(string:str)->bool
spacy.lang.lex_attrs.is_ascii(text:str)->bool
spacy.lang.lex_attrs.is_bracket(text:str)->bool
spacy.lang.lex_attrs.is_currency(text:str)->bool
spacy.lang.lex_attrs.is_digit(string:str)->bool
spacy.lang.lex_attrs.is_left_punct(text:str)->bool
spacy.lang.lex_attrs.is_lower(string:str)->bool
spacy.lang.lex_attrs.is_punct(text:str)->bool
spacy.lang.lex_attrs.is_quote(text:str)->bool
spacy.lang.lex_attrs.is_right_punct(text:str)->bool
spacy.lang.lex_attrs.is_space(string:str)->bool
spacy.lang.lex_attrs.is_stop(string:str,stops:Set[str]=set())->bool
spacy.lang.lex_attrs.is_title(string:str)->bool
spacy.lang.lex_attrs.is_upper(string:str)->bool
spacy.lang.lex_attrs.like_email(text:str)->bool
spacy.lang.lex_attrs.like_num(text:str)->bool
spacy.lang.lex_attrs.like_url(text:str)->bool
spacy.lang.lex_attrs.lower(string:str)->str
spacy.lang.lex_attrs.prefix(string:str)->str
spacy.lang.lex_attrs.suffix(string:str)->str
spacy.lang.lex_attrs.word_shape(text:str)->str


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.tokenizer_exceptions.URL_PATTERN->('^(?:(?:[\\w\\+\\-\\.]{2,})://)?(?:\\S+(?::\\S*)?@)?(?:(?!(?:10|127)(?:\\.\\d{1,3}){3})(?!(?:169\\.254|192\\.168)(?:\\.\\d{1,3}){2})(?!172\\.(?:1[6-9]|2\\d|3[0-1])(?:\\.\\d{1,3}){2})(?:[1-9]\\d?|1\\d\\d|2[01]\\d|22[0-3])(?:\\.(?:1?\\d{1,2}|2[0-4]\\d|25[0-5])){2}(?:\\.(?:[1-9]\\d?|1\\d\\d|2[0-4]\\d|25[0-4]))|(?:(?:[A-Za-z0-9\\u00a1-\\uffff][A-Za-z0-9\\u00a1-\\uffff_-]{0,62})?[A-Za-z0-9\\u00a1-\\uffff]\\.)+(?:[' + ALPHA_LOWER + ']{2,63}))(?::\\d{2,5})?(?:[/?#]\\S*)?$').strip()
A:spacy.lang.tokenizer_exceptions.emoticons->set("\n:)\n:-)\n:))\n:-))\n:)))\n:-)))\n(:\n(-:\n=)\n(=\n:]\n:-]\n[:\n[-:\n[=\n=]\n:o)\n(o:\n:}\n:-}\n8)\n8-)\n(-8\n;)\n;-)\n(;\n(-;\n:(\n:-(\n:((\n:-((\n:(((\n:-(((\n):\n)-:\n=(\n>:(\n:')\n:'-)\n:'(\n:'-(\n:/\n:-/\n=/\n=|\n:|\n:-|\n]=\n=[\n:1\n:P\n:-P\n:p\n:-p\n:O\n:-O\n:o\n:-o\n:0\n:-0\n:()\n>:o\n:*\n:-*\n:3\n:-3\n=3\n:>\n:->\n:X\n:-X\n:x\n:-x\n:D\n:-D\n;D\n;-D\n=D\nxD\nXD\nxDD\nXDD\n8D\n8-D\n\n^_^\n^__^\n^___^\n>.<\n>.>\n<.<\n._.\n;_;\n-_-\n-__-\nv.v\nV.V\nv_v\nV_V\no_o\no_O\nO_o\nO_O\n0_o\no_0\n0_0\no.O\nO.o\nO.O\no.o\n0.0\no.0\n0.o\n@_@\n<3\n<33\n<333\n</3\n(^_^)\n(-_-)\n(._.)\n(>_<)\n(*_*)\n(¬_¬)\nಠ_ಠ\nಠ︵ಠ\n(ಠ_ಠ)\n¯\\(ツ)/¯\n(╯°□°）╯︵┻━┻\n><(((*>\n".split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/norm_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/punctuation.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/lij/__init__.py----------------------------------------
spacy.lang.lij.__init__.Ligurian(Language)
spacy.lang.lij.__init__.LigurianDefaults(BaseDefaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/lij/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/lij/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.lij.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/lij/stop_words.py----------------------------------------
A:spacy.lang.lij.stop_words.STOP_WORDS->set("\na à â a-a a-e a-i a-o aiva aloa an ancheu ancon apreuvo ascì atra atre atri atro avanti avei\n\nbella belle belli bello ben\n\nch' che chì chi ciù co-a co-e co-i co-o comm' comme con cösa coscì cöse\n\nd' da da-a da-e da-i da-o dapeu de delongo derê di do doe doî donde dòppo\n\né e ê ea ean emmo en ëse\n\nfin fiña\n\ngh' ghe guæei\n\ni î in insemme int' inta inte inti into\n\nl' lê lì lô\n\nm' ma manco me megio meno mezo mi\n\nna n' ne ni ninte nisciun nisciuña no\n\no ò ô oua\n\nparte pe pe-a pe-i pe-e pe-o perché pittin pö primma pròpio\n\nquæ quand' quande quarche quella quelle quelli quello\n\ns' sce scê sci sciâ sciô sciù se segge seu sò solo son sott' sta stæta stæte stæti stæto ste sti sto\n\ntanta tante tanti tanto te ti torna tra tròppo tutta tutte tutti tutto\n\nun uña unn' unna\n\nza zu\n".split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/lij/punctuation.py----------------------------------------
A:spacy.lang.lij.punctuation.ELISION->" ' ’ ".strip().replace(' ', '').replace('\n', '')


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/vi/__init__.py----------------------------------------
A:spacy.lang.vi.__init__.words->self.pyvi_tokenize(text)
A:spacy.lang.vi.__init__.(words, spaces)->util.get_words_and_spaces(text.split(), text)
A:spacy.lang.vi.__init__.tokens->re.findall(patterns, text, re.UNICODE)
A:spacy.lang.vi.__init__.segs->self.pyvi_sylabelize_with_ws(text)
A:spacy.lang.vi.__init__.labels->self.ViTokenizer.ViTokenizer.model.predict([self.ViTokenizer.ViTokenizer.sent2features(words, False)])
A:spacy.lang.vi.__init__.self.use_pyvi->load_config_from_str(DEFAULT_CONFIG).get('use_pyvi', False)
A:spacy.lang.vi.__init__.path->util.ensure_path(path)
A:spacy.lang.vi.__init__.config->load_config_from_str(DEFAULT_CONFIG)
spacy.lang.vi.__init__.Vietnamese(Language)
spacy.lang.vi.__init__.VietnameseDefaults(BaseDefaults)
spacy.lang.vi.__init__.VietnameseTokenizer(self,vocab:Vocab,use_pyvi:bool=False)
spacy.lang.vi.__init__.VietnameseTokenizer.__init__(self,vocab:Vocab,use_pyvi:bool=False)
spacy.lang.vi.__init__.VietnameseTokenizer.__reduce__(self)
spacy.lang.vi.__init__.VietnameseTokenizer._get_config(self)->Dict[str, Any]
spacy.lang.vi.__init__.VietnameseTokenizer._set_config(self,config:Dict[str,Any]={})->None
spacy.lang.vi.__init__.VietnameseTokenizer.from_bytes(self,data:bytes,**kwargs)->'VietnameseTokenizer'
spacy.lang.vi.__init__.VietnameseTokenizer.from_disk(self,path:Union[str,Path],**kwargs)->'VietnameseTokenizer'
spacy.lang.vi.__init__.VietnameseTokenizer.pyvi_sylabelize_with_ws(self,text)
spacy.lang.vi.__init__.VietnameseTokenizer.pyvi_tokenize(self,text)
spacy.lang.vi.__init__.VietnameseTokenizer.to_bytes(self,**kwargs)->bytes
spacy.lang.vi.__init__.VietnameseTokenizer.to_disk(self,path:Union[str,Path],**kwargs)->None
spacy.lang.vi.__init__.create_vietnamese_tokenizer(use_pyvi:bool=True)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/vi/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/vi/lex_attrs.py----------------------------------------
A:spacy.lang.vi.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.vi.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.vi.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/vi/stop_words.py----------------------------------------
A:spacy.lang.vi.stop_words.STOP_WORDS->set('\na_lô\na_ha\nai\nai_ai\nai_nấy\nai_đó\nalô\namen\nanh\nanh_ấy\nba\nba_bau\nba_bản\nba_cùng\nba_họ\nba_ngày\nba_ngôi\nba_tăng\nbao_giờ\nbao_lâu\nbao_nhiêu\nbao_nả\nbay_biến\nbiết\nbiết_bao\nbiết_bao_nhiêu\nbiết_chắc\nbiết_chừng_nào\nbiết_mình\nbiết_mấy\nbiết_thế\nbiết_trước\nbiết_việc\nbiết_đâu\nbiết_đâu_chừng\nbiết_đâu_đấy\nbiết_được\nbuổi\nbuổi_làm\nbuổi_mới\nbuổi_ngày\nbuổi_sớm\nbà\nbà_ấy\nbài\nbài_bác\nbài_bỏ\nbài_cái\nbác\nbán\nbán_cấp\nbán_dạ\nbán_thế\nbây_bẩy\nbây_chừ\nbây_giờ\nbây_nhiêu\nbèn\nbéng\nbên\nbên_bị\nbên_có\nbên_cạnh\nbông\nbước\nbước_khỏi\nbước_tới\nbước_đi\nbạn\nbản\nbản_bộ\nbản_riêng\nbản_thân\nbản_ý\nbất_chợt\nbất_cứ\nbất_giác\nbất_kì\nbất_kể\nbất_kỳ\nbất_luận\nbất_ngờ\nbất_nhược\nbất_quá\nbất_quá_chỉ\nbất_thình_lình\nbất_tử\nbất_đồ\nbấy\nbấy_chầy\nbấy_chừ\nbấy_giờ\nbấy_lâu\nbấy_lâu_nay\nbấy_nay\nbấy_nhiêu\nbập_bà_bập_bõm\nbập_bõm\nbắt_đầu\nbắt_đầu_từ\nbằng\nbằng_cứ\nbằng_không\nbằng_người\nbằng_nhau\nbằng_như\nbằng_nào\nbằng_nấy\nbằng_vào\nbằng_được\nbằng_ấy\nbển\nbệt\nbị\nbị_chú\nbị_vì\nbỏ\nbỏ_bà\nbỏ_cha\nbỏ_cuộc\nbỏ_không\nbỏ_lại\nbỏ_mình\nbỏ_mất\nbỏ_mẹ\nbỏ_nhỏ\nbỏ_quá\nbỏ_ra\nbỏ_riêng\nbỏ_việc\nbỏ_xa\nbỗng\nbỗng_chốc\nbỗng_dưng\nbỗng_không\nbỗng_nhiên\nbỗng_nhưng\nbỗng_thấy\nbỗng_đâu\nbộ\nbộ_thuộc\nbộ_điều\nbội_phần\nbớ\nbởi\nbởi_ai\nbởi_chưng\nbởi_nhưng\nbởi_sao\nbởi_thế\nbởi_thế_cho_nên\nbởi_tại\nbởi_vì\nbởi_vậy\nbởi_đâu\nbức\ncao\ncao_lâu\ncao_ráo\ncao_răng\ncao_sang\ncao_số\ncao_thấp\ncao_thế\ncao_xa\ncha\ncha_chả\nchao_ôi\nchia_sẻ\nchiếc\ncho\ncho_biết\ncho_chắc\ncho_hay\ncho_nhau\ncho_nên\ncho_rằng\ncho_rồi\ncho_thấy\ncho_tin\ncho_tới\ncho_tới_khi\ncho_về\ncho_ăn\ncho_đang\ncho_được\ncho_đến\ncho_đến_khi\ncho_đến_nỗi\nchoa\nchu_cha\nchui_cha\nchung\nchung_cho\nchung_chung\nchung_cuộc\nchung_cục\nchung_nhau\nchung_qui\nchung_quy\nchung_quy_lại\nchung_ái\nchuyển\nchuyển_tự\nchuyển_đạt\nchuyện\nchuẩn_bị\nchành_chạnh\nchí_chết\nchính\nchính_bản\nchính_giữa\nchính_là\nchính_thị\nchính_điểm\nchùn_chùn\nchùn_chũn\nchú\nchú_dẫn\nchú_khách\nchú_mày\nchú_mình\nchúng\nchúng_mình\nchúng_ta\nchúng_tôi\nchúng_ông\nchăn_chắn\nchăng\nchăng_chắc\nchăng_nữa\nchơi\nchơi_họ\nchưa\nchưa_bao_giờ\nchưa_chắc\nchưa_có\nchưa_cần\nchưa_dùng\nchưa_dễ\nchưa_kể\nchưa_tính\nchưa_từng\nchầm_chập\nchậc\nchắc\nchắc_chắn\nchắc_dạ\nchắc_hẳn\nchắc_lòng\nchắc_người\nchắc_vào\nchắc_ăn\nchẳng_lẽ\nchẳng_những\nchẳng_nữa\nchẳng_phải\nchết_nỗi\nchết_thật\nchết_tiệt\nchỉ\nchỉ_chính\nchỉ_có\nchỉ_là\nchỉ_tên\nchỉn\nchị\nchị_bộ\nchị_ấy\nchịu\nchịu_chưa\nchịu_lời\nchịu_tốt\nchịu_ăn\nchọn\nchọn_bên\nchọn_ra\nchốc_chốc\nchớ\nchớ_chi\nchớ_gì\nchớ_không\nchớ_kể\nchớ_như\nchợt\nchợt_nghe\nchợt_nhìn\nchủn\nchứ\nchứ_ai\nchứ_còn\nchứ_gì\nchứ_không\nchứ_không_phải\nchứ_lại\nchứ_lị\nchứ_như\nchứ_sao\ncoi_bộ\ncoi_mòi\ncon\ncon_con\ncon_dạ\ncon_nhà\ncon_tính\ncu_cậu\ncuối\ncuối_cùng\ncuối_điểm\ncuốn\ncuộc\ncàng\ncàng_càng\ncàng_hay\ncá_nhân\ncác\ncác_cậu\ncách\ncách_bức\ncách_không\ncách_nhau\ncách_đều\ncái\ncái_gì\ncái_họ\ncái_đã\ncái_đó\ncái_ấy\ncâu_hỏi\ncây\ncây_nước\ncòn\ncòn_như\ncòn_nữa\ncòn_thời_gian\ncòn_về\ncó\ncó_ai\ncó_chuyện\ncó_chăng\ncó_chăng_là\ncó_chứ\ncó_cơ\ncó_dễ\ncó_họ\ncó_khi\ncó_ngày\ncó_người\ncó_nhiều\ncó_nhà\ncó_phải\ncó_số\ncó_tháng\ncó_thế\ncó_thể\ncó_vẻ\ncó_ý\ncó_ăn\ncó_điều\ncó_điều_kiện\ncó_đáng\ncó_đâu\ncó_được\ncóc_khô\ncô\ncô_mình\ncô_quả\ncô_tăng\ncô_ấy\ncông_nhiên\ncùng\ncùng_chung\ncùng_cực\ncùng_nhau\ncùng_tuổi\ncùng_tột\ncùng_với\ncùng_ăn\ncăn\ncăn_cái\ncăn_cắt\ncăn_tính\ncũng\ncũng_như\ncũng_nên\ncũng_thế\ncũng_vậy\ncũng_vậy_thôi\ncũng_được\ncơ\ncơ_chỉ\ncơ_chừng\ncơ_cùng\ncơ_dẫn\ncơ_hồ\ncơ_hội\ncơ_mà\ncơn\ncả\ncả_nghe\ncả_nghĩ\ncả_ngày\ncả_người\ncả_nhà\ncả_năm\ncả_thảy\ncả_thể\ncả_tin\ncả_ăn\ncả_đến\ncảm_thấy\ncảm_ơn\ncấp\ncấp_số\ncấp_trực_tiếp\ncần\ncần_cấp\ncần_gì\ncần_số\ncật_lực\ncật_sức\ncậu\ncổ_lai\ncụ_thể\ncụ_thể_là\ncụ_thể_như\ncủa\ncủa_ngọt\ncủa_tin\ncứ\ncứ_như\ncứ_việc\ncứ_điểm\ncực_lực\ndo\ndo_vì\ndo_vậy\ndo_đó\nduy\nduy_chỉ\nduy_có\ndài\ndài_lời\ndài_ra\ndành\ndành_dành\ndào\ndì\ndù\ndù_cho\ndù_dì\ndù_gì\ndù_rằng\ndù_sao\ndùng\ndùng_cho\ndùng_hết\ndùng_làm\ndùng_đến\ndưới\ndưới_nước\ndạ\ndạ_bán\ndạ_con\ndạ_dài\ndạ_dạ\ndạ_khách\ndần_dà\ndần_dần\ndầu_sao\ndẫn\ndẫu\ndẫu_mà\ndẫu_rằng\ndẫu_sao\ndễ\ndễ_dùng\ndễ_gì\ndễ_khiến\ndễ_nghe\ndễ_ngươi\ndễ_như_chơi\ndễ_sợ\ndễ_sử_dụng\ndễ_thường\ndễ_thấy\ndễ_ăn\ndễ_đâu\ndở_chừng\ndữ\ndữ_cách\nem\nem_em\ngiá_trị\ngiá_trị_thực_tế\ngiảm\ngiảm_chính\ngiảm_thấp\ngiảm_thế\ngiống\ngiống_người\ngiống_nhau\ngiống_như\ngiờ\ngiờ_lâu\ngiờ_này\ngiờ_đi\ngiờ_đây\ngiờ_đến\ngiữ\ngiữ_lấy\ngiữ_ý\ngiữa\ngiữa_lúc\ngây\ngây_cho\ngây_giống\ngây_ra\ngây_thêm\ngì\ngì_gì\ngì_đó\ngần\ngần_bên\ngần_hết\ngần_ngày\ngần_như\ngần_xa\ngần_đây\ngần_đến\ngặp\ngặp_khó_khăn\ngặp_phải\ngồm\nhay\nhay_biết\nhay_hay\nhay_không\nhay_là\nhay_làm\nhay_nhỉ\nhay_nói\nhay_sao\nhay_tin\nhay_đâu\nhiểu\nhiện_nay\nhiện_tại\nhoàn_toàn\nhoặc\nhoặc_là\nhãy\nhãy_còn\nhơn\nhơn_cả\nhơn_hết\nhơn_là\nhơn_nữa\nhơn_trước\nhầu_hết\nhết\nhết_chuyện\nhết_cả\nhết_của\nhết_nói\nhết_ráo\nhết_rồi\nhết_ý\nhọ\nhọ_gần\nhọ_xa\nhỏi\nhỏi_lại\nhỏi_xem\nhỏi_xin\nhỗ_trợ\nkhi\nkhi_khác\nkhi_không\nkhi_nào\nkhi_nên\nkhi_trước\nkhiến\nkhoảng\nkhoảng_cách\nkhoảng_không\nkhá\nkhá_tốt\nkhác\nkhác_gì\nkhác_khác\nkhác_nhau\nkhác_nào\nkhác_thường\nkhác_xa\nkhách\nkhó\nkhó_biết\nkhó_chơi\nkhó_khăn\nkhó_làm\nkhó_mở\nkhó_nghe\nkhó_nghĩ\nkhó_nói\nkhó_thấy\nkhó_tránh\nkhông\nkhông_ai\nkhông_bao_giờ\nkhông_bao_lâu\nkhông_biết\nkhông_bán\nkhông_chỉ\nkhông_còn\nkhông_có\nkhông_có_gì\nkhông_cùng\nkhông_cần\nkhông_cứ\nkhông_dùng\nkhông_gì\nkhông_hay\nkhông_khỏi\nkhông_kể\nkhông_ngoài\nkhông_nhận\nkhông_những\nkhông_phải\nkhông_phải_không\nkhông_thể\nkhông_tính\nkhông_điều_kiện\nkhông_được\nkhông_đầy\nkhông_để\nkhẳng_định\nkhỏi\nkhỏi_nói\nkể\nkể_cả\nkể_như\nkể_tới\nkể_từ\nliên_quan\nloại\nloại_từ\nluôn\nluôn_cả\nluôn_luôn\nluôn_tay\nlà\nlà_cùng\nlà_là\nlà_nhiều\nlà_phải\nlà_thế_nào\nlà_vì\nlà_ít\nlàm\nlàm_bằng\nlàm_cho\nlàm_dần_dần\nlàm_gì\nlàm_lòng\nlàm_lại\nlàm_lấy\nlàm_mất\nlàm_ngay\nlàm_như\nlàm_nên\nlàm_ra\nlàm_riêng\nlàm_sao\nlàm_theo\nlàm_thế_nào\nlàm_tin\nlàm_tôi\nlàm_tăng\nlàm_tại\nlàm_tắp_lự\nlàm_vì\nlàm_đúng\nlàm_được\nlâu\nlâu_các\nlâu_lâu\nlâu_nay\nlâu_ngày\nlên\nlên_cao\nlên_cơn\nlên_mạnh\nlên_ngôi\nlên_nước\nlên_số\nlên_xuống\nlên_đến\nlòng\nlòng_không\nlúc\nlúc_khác\nlúc_lâu\nlúc_nào\nlúc_này\nlúc_sáng\nlúc_trước\nlúc_đi\nlúc_đó\nlúc_đến\nlúc_ấy\nlý_do\nlượng\nlượng_cả\nlượng_số\nlượng_từ\nlại\nlại_bộ\nlại_cái\nlại_còn\nlại_giống\nlại_làm\nlại_người\nlại_nói\nlại_nữa\nlại_quả\nlại_thôi\nlại_ăn\nlại_đây\nlấy\nlấy_có\nlấy_cả\nlấy_giống\nlấy_làm\nlấy_lý_do\nlấy_lại\nlấy_ra\nlấy_ráo\nlấy_sau\nlấy_số\nlấy_thêm\nlấy_thế\nlấy_vào\nlấy_xuống\nlấy_được\nlấy_để\nlần\nlần_khác\nlần_lần\nlần_nào\nlần_này\nlần_sang\nlần_sau\nlần_theo\nlần_trước\nlần_tìm\nlớn\nlớn_lên\nlớn_nhỏ\nlời\nlời_chú\nlời_nói\nmang\nmang_lại\nmang_mang\nmang_nặng\nmang_về\nmuốn\nmà\nmà_cả\nmà_không\nmà_lại\nmà_thôi\nmà_vẫn\nmình\nmạnh\nmất\nmất_còn\nmọi\nmọi_giờ\nmọi_khi\nmọi_lúc\nmọi_người\nmọi_nơi\nmọi_sự\nmọi_thứ\nmọi_việc\nmối\nmỗi\nmỗi_lúc\nmỗi_lần\nmỗi_một\nmỗi_ngày\nmỗi_người\nmột\nmột_cách\nmột_cơn\nmột_khi\nmột_lúc\nmột_số\nmột_vài\nmột_ít\nmới\nmới_hay\nmới_rồi\nmới_đây\nmở\nmở_mang\nmở_nước\nmở_ra\nmợ\nmức\nnay\nngay\nngay_bây_giờ\nngay_cả\nngay_khi\nngay_khi_đến\nngay_lúc\nngay_lúc_này\nngay_lập_tức\nngay_thật\nngay_tức_khắc\nngay_tức_thì\nngay_từ\nnghe\nnghe_chừng\nnghe_hiểu\nnghe_không\nnghe_lại\nnghe_nhìn\nnghe_như\nnghe_nói\nnghe_ra\nnghe_rõ\nnghe_thấy\nnghe_tin\nnghe_trực_tiếp\nnghe_đâu\nnghe_đâu_như\nnghe_được\nnghen\nnghiễm_nhiên\nnghĩ\nnghĩ_lại\nnghĩ_ra\nnghĩ_tới\nnghĩ_xa\nnghĩ_đến\nnghỉm\nngoài\nngoài_này\nngoài_ra\nngoài_xa\nngoải\nnguồn\nngày\nngày_càng\nngày_cấp\nngày_giờ\nngày_ngày\nngày_nào\nngày_này\nngày_nọ\nngày_qua\nngày_rày\nngày_tháng\nngày_xưa\nngày_xửa\nngày_đến\nngày_ấy\nngôi\nngôi_nhà\nngôi_thứ\nngõ_hầu\nngăn_ngắt\nngươi\nngười\nngười_hỏi\nngười_khác\nngười_khách\nngười_mình\nngười_nghe\nngười_người\nngười_nhận\nngọn\nngọn_nguồn\nngọt\nngồi\nngồi_bệt\nngồi_không\nngồi_sau\nngồi_trệt\nngộ_nhỡ\nnhanh\nnhanh_lên\nnhanh_tay\nnhau\nnhiên_hậu\nnhiều\nnhiều_ít\nnhiệt_liệt\nnhung_nhăng\nnhà\nnhà_chung\nnhà_khó\nnhà_làm\nnhà_ngoài\nnhà_ngươi\nnhà_tôi\nnhà_việc\nnhân_dịp\nnhân_tiện\nnhé\nnhìn\nnhìn_chung\nnhìn_lại\nnhìn_nhận\nnhìn_theo\nnhìn_thấy\nnhìn_xuống\nnhóm\nnhón_nhén\nnhư\nnhư_ai\nnhư_chơi\nnhư_không\nnhư_là\nnhư_nhau\nnhư_quả\nnhư_sau\nnhư_thường\nnhư_thế\nnhư_thế_nào\nnhư_thể\nnhư_trên\nnhư_trước\nnhư_tuồng\nnhư_vậy\nnhư_ý\nnhưng\nnhưng_mà\nnhược_bằng\nnhất\nnhất_loạt\nnhất_luật\nnhất_là\nnhất_mực\nnhất_nhất\nnhất_quyết\nnhất_sinh\nnhất_thiết\nnhất_thì\nnhất_tâm\nnhất_tề\nnhất_đán\nnhất_định\nnhận\nnhận_biết\nnhận_họ\nnhận_làm\nnhận_nhau\nnhận_ra\nnhận_thấy\nnhận_việc\nnhận_được\nnhằm\nnhằm_khi\nnhằm_lúc\nnhằm_vào\nnhằm_để\nnhỉ\nnhỏ\nnhỏ_người\nnhớ\nnhớ_bập_bõm\nnhớ_lại\nnhớ_lấy\nnhớ_ra\nnhờ\nnhờ_chuyển\nnhờ_có\nnhờ_nhờ\nnhờ_đó\nnhỡ_ra\nnhững\nnhững_ai\nnhững_khi\nnhững_là\nnhững_lúc\nnhững_muốn\nnhững_như\nnào\nnào_cũng\nnào_hay\nnào_là\nnào_phải\nnào_đâu\nnào_đó\nnày\nnày_nọ\nnên\nnên_chi\nnên_chăng\nnên_làm\nnên_người\nnên_tránh\nnó\nnóc\nnói\nnói_bông\nnói_chung\nnói_khó\nnói_là\nnói_lên\nnói_lại\nnói_nhỏ\nnói_phải\nnói_qua\nnói_ra\nnói_riêng\nnói_rõ\nnói_thêm\nnói_thật\nnói_toẹt\nnói_trước\nnói_tốt\nnói_với\nnói_xa\nnói_ý\nnói_đến\nnói_đủ\nnăm\nnăm_tháng\nnơi\nnơi_nơi\nnước\nnước_bài\nnước_cùng\nnước_lên\nnước_nặng\nnước_quả\nnước_xuống\nnước_ăn\nnước_đến\nnấy\nnặng\nnặng_căn\nnặng_mình\nnặng_về\nnếu\nnếu_có\nnếu_cần\nnếu_không\nnếu_mà\nnếu_như\nnếu_thế\nnếu_vậy\nnếu_được\nnền\nnọ\nnớ\nnức_nở\nnữa\nnữa_khi\nnữa_là\nnữa_rồi\noai_oái\noái\npho\nphè\nphè_phè\nphía\nphía_bên\nphía_bạn\nphía_dưới\nphía_sau\nphía_trong\nphía_trên\nphía_trước\nphóc\nphót\nphù_hợp\nphăn_phắt\nphương_chi\nphải\nphải_biết\nphải_chi\nphải_chăng\nphải_cách\nphải_cái\nphải_giờ\nphải_khi\nphải_không\nphải_lại\nphải_lời\nphải_người\nphải_như\nphải_rồi\nphải_tay\nphần\nphần_lớn\nphần_nhiều\nphần_nào\nphần_sau\nphần_việc\nphắt\nphỉ_phui\nphỏng\nphỏng_như\nphỏng_nước\nphỏng_theo\nphỏng_tính\nphốc\nphụt\nphứt\nqua\nqua_chuyện\nqua_khỏi\nqua_lại\nqua_lần\nqua_ngày\nqua_tay\nqua_thì\nqua_đi\nquan_trọng\nquan_trọng_vấn_đề\nquan_tâm\nquay\nquay_bước\nquay_lại\nquay_số\nquay_đi\nquá\nquá_bán\nquá_bộ\nquá_giờ\nquá_lời\nquá_mức\nquá_nhiều\nquá_tay\nquá_thì\nquá_tin\nquá_trình\nquá_tuổi\nquá_đáng\nquá_ư\nquả\nquả_là\nquả_thật\nquả_thế\nquả_vậy\nquận\nra\nra_bài\nra_bộ\nra_chơi\nra_gì\nra_lại\nra_lời\nra_ngôi\nra_người\nra_sao\nra_tay\nra_vào\nra_ý\nra_điều\nra_đây\nren_rén\nriu_ríu\nriêng\nriêng_từng\nriệt\nrày\nráo\nráo_cả\nráo_nước\nráo_trọi\nrén\nrén_bước\nrích\nrón_rén\nrõ\nrõ_là\nrõ_thật\nrút_cục\nrăng\nrăng_răng\nrất\nrất_lâu\nrằng\nrằng_là\nrốt_cuộc\nrốt_cục\nrồi\nrồi_nữa\nrồi_ra\nrồi_sao\nrồi_sau\nrồi_tay\nrồi_thì\nrồi_xem\nrồi_đây\nrứa\nsa_sả\nsang\nsang_năm\nsang_sáng\nsang_tay\nsao\nsao_bản\nsao_bằng\nsao_cho\nsao_vậy\nsao_đang\nsau\nsau_chót\nsau_cuối\nsau_cùng\nsau_hết\nsau_này\nsau_nữa\nsau_sau\nsau_đây\nsau_đó\nso\nso_với\nsong_le\nsuýt\nsuýt_nữa\nsáng\nsáng_ngày\nsáng_rõ\nsáng_thế\nsáng_ý\nsì\nsì_sì\nsất\nsắp\nsắp_đặt\nsẽ\nsẽ_biết\nsẽ_hay\nsố\nsố_cho_biết\nsố_cụ_thể\nsố_loại\nsố_là\nsố_người\nsố_phần\nsố_thiếu\nsốt_sột\nsớm\nsớm_ngày\nsở_dĩ\nsử_dụng\nsự\nsự_thế\nsự_việc\ntanh\ntanh_tanh\ntay\ntay_quay\ntha_hồ\ntha_hồ_chơi\ntha_hồ_ăn\nthan_ôi\nthanh\nthanh_ba\nthanh_chuyển\nthanh_không\nthanh_thanh\nthanh_tính\nthanh_điều_kiện\nthanh_điểm\nthay_đổi\nthay_đổi_tình_trạng\ntheo\ntheo_bước\ntheo_như\ntheo_tin\nthi_thoảng\nthiếu\nthiếu_gì\nthiếu_điểm\nthoạt\nthoạt_nghe\nthoạt_nhiên\nthoắt\nthuần\nthuần_ái\nthuộc\nthuộc_bài\nthuộc_cách\nthuộc_lại\nthuộc_từ\nthà\nthà_là\nthà_rằng\nthành_ra\nthành_thử\nthái_quá\ntháng\ntháng_ngày\ntháng_năm\ntháng_tháng\nthêm\nthêm_chuyện\nthêm_giờ\nthêm_vào\nthì\nthì_giờ\nthì_là\nthì_phải\nthì_ra\nthì_thôi\nthình_lình\nthích\nthích_cứ\nthích_thuộc\nthích_tự\nthích_ý\nthím\nthôi\nthôi_việc\nthúng_thắng\nthương_ôi\nthường\nthường_bị\nthường_hay\nthường_khi\nthường_số\nthường_sự\nthường_thôi\nthường_thường\nthường_tính\nthường_tại\nthường_xuất_hiện\nthường_đến\nthảo_hèn\nthảo_nào\nthấp\nthấp_cơ\nthấp_thỏm\nthấp_xuống\nthấy\nthấy_tháng\nthẩy\nthậm\nthậm_chí\nthậm_cấp\nthậm_từ\nthật\nthật_chắc\nthật_là\nthật_lực\nthật_quả\nthật_ra\nthật_sự\nthật_thà\nthật_tốt\nthật_vậy\nthế\nthế_chuẩn_bị\nthế_là\nthế_lại\nthế_mà\nthế_nào\nthế_nên\nthế_ra\nthế_sự\nthế_thì\nthế_thôi\nthế_thường\nthế_thế\nthế_à\nthế_đó\nthếch\nthỉnh_thoảng\nthỏm\nthốc\nthốc_tháo\nthốt\nthốt_nhiên\nthốt_nói\nthốt_thôi\nthộc\nthời_gian\nthời_gian_sử_dụng\nthời_gian_tính\nthời_điểm\nthục_mạng\nthứ\nthứ_bản\nthứ_đến\nthửa\nthực_hiện\nthực_hiện_đúng\nthực_ra\nthực_sự\nthực_tế\nthực_vậy\ntin\ntin_thêm\ntin_vào\ntiếp_theo\ntiếp_tục\ntiếp_đó\ntiện_thể\ntoà\ntoé_khói\ntoẹt\ntrong\ntrong_khi\ntrong_lúc\ntrong_mình\ntrong_ngoài\ntrong_này\ntrong_số\ntrong_vùng\ntrong_đó\ntrong_ấy\ntránh\ntránh_khỏi\ntránh_ra\ntránh_tình_trạng\ntránh_xa\ntrên\ntrên_bộ\ntrên_dưới\ntrước\ntrước_hết\ntrước_khi\ntrước_kia\ntrước_nay\ntrước_ngày\ntrước_nhất\ntrước_sau\ntrước_tiên\ntrước_tuổi\ntrước_đây\ntrước_đó\ntrả\ntrả_của\ntrả_lại\ntrả_ngay\ntrả_trước\ntrếu_tráo\ntrển\ntrệt\ntrệu_trạo\ntrỏng\ntrời_đất_ơi\ntrở_thành\ntrừ_phi\ntrực_tiếp\ntrực_tiếp_làm\ntuy\ntuy_có\ntuy_là\ntuy_nhiên\ntuy_rằng\ntuy_thế\ntuy_vậy\ntuy_đã\ntuyệt_nhiên\ntuần_tự\ntuốt_luốt\ntuốt_tuồn_tuột\ntuốt_tuột\ntuổi\ntuổi_cả\ntuổi_tôi\ntà_tà\ntên\ntên_chính\ntên_cái\ntên_họ\ntên_tự\ntênh\ntênh_tênh\ntìm\ntìm_bạn\ntìm_cách\ntìm_hiểu\ntìm_ra\ntìm_việc\ntình_trạng\ntính\ntính_cách\ntính_căn\ntính_người\ntính_phỏng\ntính_từ\ntít_mù\ntò_te\ntôi\ntôi_con\ntông_tốc\ntù_tì\ntăm_tắp\ntăng\ntăng_chúng\ntăng_cấp\ntăng_giảm\ntăng_thêm\ntăng_thế\ntại\ntại_lòng\ntại_nơi\ntại_sao\ntại_tôi\ntại_vì\ntại_đâu\ntại_đây\ntại_đó\ntạo\ntạo_cơ_hội\ntạo_nên\ntạo_ra\ntạo_ý\ntạo_điều_kiện\ntấm\ntấm_bản\ntấm_các\ntấn\ntấn_tới\ntất_cả\ntất_cả_bao_nhiêu\ntất_thảy\ntất_tần_tật\ntất_tật\ntập_trung\ntắp\ntắp_lự\ntắp_tắp\ntọt\ntỏ_ra\ntỏ_vẻ\ntốc_tả\ntối_ư\ntốt\ntốt_bạn\ntốt_bộ\ntốt_hơn\ntốt_mối\ntốt_ngày\ntột\ntột_cùng\ntớ\ntới\ntới_gần\ntới_mức\ntới_nơi\ntới_thì\ntức_thì\ntức_tốc\ntừ\ntừ_căn\ntừ_giờ\ntừ_khi\ntừ_loại\ntừ_nay\ntừ_thế\ntừ_tính\ntừ_tại\ntừ_từ\ntừ_ái\ntừ_điều\ntừ_đó\ntừ_ấy\ntừng\ntừng_cái\ntừng_giờ\ntừng_nhà\ntừng_phần\ntừng_thời_gian\ntừng_đơn_vị\ntừng_ấy\ntự\ntự_cao\ntự_khi\ntự_lượng\ntự_tính\ntự_tạo\ntự_vì\ntự_ý\ntự_ăn\ntựu_trung\nveo\nveo_veo\nviệc\nviệc_gì\nvung_thiên_địa\nvung_tàn_tán\nvung_tán_tàn\nvà\nvài\nvài_ba\nvài_người\nvài_nhà\nvài_nơi\nvài_tên\nvài_điều\nvào\nvào_gặp\nvào_khoảng\nvào_lúc\nvào_vùng\nvào_đến\nvâng\nvâng_chịu\nvâng_dạ\nvâng_vâng\nvâng_ý\nvèo\nvèo_vèo\nvì\nvì_chưng\nvì_rằng\nvì_sao\nvì_thế\nvì_vậy\nví_bằng\nví_dù\nví_phỏng\nví_thử\nvô_hình_trung\nvô_kể\nvô_luận\nvô_vàn\nvùng\nvùng_lên\nvùng_nước\nvăng_tê\nvượt\nvượt_khỏi\nvượt_quá\nvạn_nhất\nvả_chăng\nvả_lại\nvấn_đề\nvấn_đề_quan_trọng\nvẫn\nvẫn_thế\nvậy\nvậy_là\nvậy_mà\nvậy_nên\nvậy_ra\nvậy_thì\nvậy_ư\nvề\nvề_không\nvề_nước\nvề_phần\nvề_sau\nvề_tay\nvị_trí\nvị_tất\nvốn_dĩ\nvới\nvới_lại\nvới_nhau\nvở\nvụt\nvừa\nvừa_khi\nvừa_lúc\nvừa_mới\nvừa_qua\nvừa_rồi\nvừa_vừa\nxa\nxa_cách\nxa_gần\nxa_nhà\nxa_tanh\nxa_tắp\nxa_xa\nxa_xả\nxem\nxem_lại\nxem_ra\nxem_số\nxin\nxin_gặp\nxin_vâng\nxiết_bao\nxon_xón\nxoành_xoạch\nxoét\nxoẳn\nxoẹt\nxuất_hiện\nxuất_kì_bất_ý\nxuất_kỳ_bất_ý\nxuể\nxuống\nxăm_xúi\nxăm_xăm\nxăm_xắm\nxảy_ra\nxềnh_xệch\nxệp\nxử_lý\nyêu_cầu\nà\nà_này\nà_ơi\nào\nào_vào\nào_ào\ná\ná_à\nái\nái_chà\nái_dà\náng\náng_như\nâu_là\nít\nít_biết\nít_có\nít_hơn\nít_khi\nít_lâu\nít_nhiều\nít_nhất\nít_nữa\nít_quá\nít_ra\nít_thôi\nít_thấy\nô_hay\nô_hô\nô_kê\nô_kìa\nôi_chao\nôi_thôi\nông\nông_nhỏ\nông_tạo\nông_từ\nông_ấy\nông_ổng\núi\núi_chà\núi_dào\ný\ný_chừng\ný_da\ný_hoặc\năn\năn_chung\năn_chắc\năn_chịu\năn_cuộc\năn_hết\năn_hỏi\năn_làm\năn_người\năn_ngồi\năn_quá\năn_riêng\năn_sáng\năn_tay\năn_trên\năn_về\nđang\nđang_tay\nđang_thì\nđiều\nđiều_gì\nđiều_kiện\nđiểm\nđiểm_chính\nđiểm_gặp\nđiểm_đầu_tiên\nđành_đạch\nđáng\nđáng_kể\nđáng_lí\nđáng_lý\nđáng_lẽ\nđáng_số\nđánh_giá\nđánh_đùng\nđáo_để\nđâu\nđâu_có\nđâu_cũng\nđâu_như\nđâu_nào\nđâu_phải\nđâu_đâu\nđâu_đây\nđâu_đó\nđây\nđây_này\nđây_rồi\nđây_đó\nđã\nđã_hay\nđã_không\nđã_là\nđã_lâu\nđã_thế\nđã_vậy\nđã_đủ\nđó\nđó_đây\nđúng\nđúng_ngày\nđúng_ra\nđúng_tuổi\nđúng_với\nđơn_vị\nđưa\nđưa_cho\nđưa_chuyện\nđưa_em\nđưa_ra\nđưa_tay\nđưa_tin\nđưa_tới\nđưa_vào\nđưa_về\nđưa_xuống\nđưa_đến\nđược\nđược_cái\nđược_lời\nđược_nước\nđược_tin\nđại_loại\nđại_nhân\nđại_phàm\nđại_để\nđạt\nđảm_bảo\nđầu_tiên\nđầy\nđầy_năm\nđầy_phè\nđầy_tuổi\nđặc_biệt\nđặt\nđặt_làm\nđặt_mình\nđặt_mức\nđặt_ra\nđặt_trước\nđặt_để\nđến\nđến_bao_giờ\nđến_cùng\nđến_cùng_cực\nđến_cả\nđến_giờ\nđến_gần\nđến_hay\nđến_khi\nđến_lúc\nđến_lời\nđến_nay\nđến_ngày\nđến_nơi\nđến_nỗi\nđến_thì\nđến_thế\nđến_tuổi\nđến_xem\nđến_điều\nđến_đâu\nđều\nđều_bước\nđều_nhau\nđều_đều\nđể\nđể_cho\nđể_giống\nđể_không\nđể_lòng\nđể_lại\nđể_mà\nđể_phần\nđể_được\nđể_đến_nỗi\nđối_với\nđồng_thời\nđủ\nđủ_dùng\nđủ_nơi\nđủ_số\nđủ_điều\nđủ_điểm\nơ\nơ_hay\nơ_kìa\nơi\nơi_là\nư\nạ\nạ_ơi\nấy\nấy_là\nầu_ơ\nắt\nắt_hẳn\nắt_là\nắt_phải\nắt_thật\nối_dào\nối_giời\nối_giời_ơi\nồ\nồ_ồ\nổng\nớ\nớ_này\nờ\nờ_ờ\nở\nở_lại\nở_như\nở_nhờ\nở_năm\nở_trên\nở_vào\nở_đây\nở_đó\nở_được\nủa\nứ_hự\nứ_ừ\nừ\nừ_nhé\nừ_thì\nừ_ào\nừ_ừ\nử\n'.split('\n'))


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/grc/__init__.py----------------------------------------
spacy.lang.grc.__init__.AncientGreek(Language)
spacy.lang.grc.__init__.AncientGreekDefaults(BaseDefaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/grc/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/grc/lex_attrs.py----------------------------------------
spacy.lang.grc.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/grc/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.grc.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/grc/stop_words.py----------------------------------------
A:spacy.lang.grc.stop_words.STOP_WORDS->set("\nαὐτῷ αὐτοῦ αὐτῆς αὐτόν αὐτὸν αὐτῶν αὐτὸς αὐτὸ αὐτό αὐτός αὐτὴν αὐτοῖς αὐτοὺς αὔτ' αὐτὰ αὐτῇ αὐτὴ\nαὐτὼ αὑταὶ καὐτὸς αὐτά αὑτός αὐτοῖσι αὐτοῖσιν αὑτὸς αὐτήν αὐτοῖσί αὐτοί αὐτοὶ αὐτοῖο αὐτάων αὐτὰς\nαὐτέων αὐτώ αὐτάς αὐτούς αὐτή αὐταί αὐταὶ αὐτῇσιν τὠυτῷ τὠυτὸ ταὐτὰ ταύτῃ αὐτῇσι αὐτῇς αὐταῖς αὐτᾶς αὐτὰν ταὐτὸν\n\nγε γ' γέ γὰρ γάρ δαῖτα δαιτὸς δαιτὶ δαὶ δαιτί δαῖτ' δαΐδας δαΐδων δἰ διὰ διά δὲ δ' δέ δὴ δή εἰ εἴ κεἰ κεἴ αἴ αἲ εἲ αἰ\n\nἐστί ἐστιν ὢν ἦν ἐστὶν ὦσιν εἶναι ὄντι εἰσιν ἐστι ὄντα οὖσαν ἦσαν ἔστι ὄντας ἐστὲ εἰσὶ εἶ ὤν ἦ οὖσαι ἔσται ἐσμὲν ἐστ' ἐστίν ἔστ' ὦ ἔσει ἦμεν εἰμι εἰσὶν ἦσθ' \nἐστὶ ᾖ οὖσ' ἔστιν εἰμὶ εἴμ' ἐσθ' ᾖς στί εἴην εἶναί οὖσα κἄστ' εἴη ἦσθα εἰμ' ἔστω ὄντ' ἔσθ' ἔμμεναι ἔω ἐὼν ἐσσι ἔσσεται ἐστὸν ἔσαν ἔστων ἐόντα ἦεν ἐοῦσαν ἔην \nἔσσομαι εἰσί ἐστόν ἔσκεν ἐόντ' ἐών ἔσσεσθ' εἰσ' ἐόντες ἐόντε ἐσσεῖται εἰμεν ἔασιν ἔσκε ἔμεναι ἔσεσθαι ἔῃ εἰμὲν εἰσι ἐόντας ἔστε εἰς ἦτε εἰμί ἔσσεαι ἔμμεν \nἐοῦσα ἔμεν ᾖσιν ἐστε ἐόντι εἶεν ἔσσονται ἔησθα ἔσεσθε ἐσσί ἐοῦσ' ἔασι ἔα ἦα ἐόν ἔσσεσθαι ἔσομαι ἔσκον εἴης ἔωσιν εἴησαν ἐὸν ἐουσέων ἔσσῃ ἐούσης ἔσονται \nἐούσας ἐόντων ἐόντος ἐσομένην ἔστωσαν ἔωσι ἔας ἐοῦσαι ἣν εἰσίν ἤστην ὄντες ὄντων οὔσας οὔσαις ὄντος οὖσι οὔσης ἔσῃ ὂν ἐσμεν ἐσμέν οὖσιν ἐσομένους ἐσσόμεσθα\n\nἒς ἐς ἔς ἐν κεἰς εἲς κἀν ἔν κατὰ κατ' καθ' κατά κάτα κὰπ κὰκ κὰδ κὰρ κάρ κὰγ κὰμ καὶ καί μετὰ μεθ' μετ' μέτα μετά μέθ' μέτ' μὲν μέν μὴ\n\nμή μη οὐκ οὒ οὐ οὐχ οὐχὶ κοὐ κοὐχ οὔ κοὐκ οὐχί οὐκὶ οὐδὲν οὐδεὶς οὐδέν κοὐδεὶς κοὐδὲν οὐδένα οὐδενὸς οὐδέν' οὐδενός οὐδενὶ\nοὐδεμία οὐδείς οὐδεμίαν οὐδὲ οὐδ' κοὐδ' οὐδέ οὔτε οὔθ' οὔτέ τε οὔτ' οὕτως οὕτω οὕτῶ χοὔτως οὖν ὦν ὧν τοῦτο τοῦθ' τοῦτον τούτῳ\nτούτοις ταύτας αὕτη ταῦτα οὗτος ταύτης ταύτην τούτων ταῦτ' τοῦτ' τούτου αὗται τούτους τοῦτό ταῦτά τούτοισι χαὔτη ταῦθ' χοὖτοι\nτούτοισιν οὗτός οὗτοι τούτω τουτέων τοῦτὸν οὗτοί τοῦτου οὗτοὶ ταύτῃσι ταύταις ταυτὶ παρὰ παρ' πάρα παρά πὰρ παραὶ πάρ' περὶ\nπέρι περί πρὸς πρός ποτ' ποτὶ προτὶ προτί πότι\n\nσὸς σήν σὴν σὸν σόν σὰ σῶν σοῖσιν σός σῆς σῷ σαῖς σῇ σοῖς σοῦ σ' σὰν σά σὴ σὰς\nσᾷ σοὺς σούς σοῖσι σῇς σῇσι σή σῇσιν σοὶ σου ὑμεῖς σὲ σύ σοι ὑμᾶς ὑμῶν ὑμῖν σε\nσέ σὺ σέθεν σοί ὑμὶν σφῷν ὑμίν τοι τοὶ σφὼ ὔμμ' σφῶϊ σεῖο τ' σφῶϊν ὔμμιν σέο σευ σεῦ\nὔμμι ὑμέων τύνη ὑμείων τοί ὔμμες σεο τέ τεοῖο ὑμέας σὺν ξὺν σύν \n\nθ' τί τι τις τινες τινα τινος τινὸς τινὶ τινῶν τίς τίνες τινὰς τιν' τῳ του τίνα τοῦ τῷ τινί τινά τίνος τινι τινας τινὰ τινων\nτίν' τευ τέο τινές τεο τινὲς τεῷ τέῳ τινός τεῳ τισὶ \n\nτοιαῦτα τοιοῦτον τοιοῦθ' τοιοῦτος τοιαύτην τοιαῦτ' τοιούτου τοιαῦθ' τοιαύτῃ τοιούτοις τοιαῦται τοιαῦτά τοιαύτη τοιοῦτοι τοιούτων τοιούτοισι\nτοιοῦτο τοιούτους τοιούτῳ τοιαύτης τοιαύταις τοιαύτας τοιοῦτός τίνι τοῖσι τίνων τέων τέοισί τὰ τῇ τώ τὼ\n\nἀλλὰ ἀλλ' ἀλλά ἀπ' ἀπὸ κἀπ' ἀφ' τἀπὸ κἀφ' ἄπο ἀπό τὠπὸ τἀπ' ἄλλων ἄλλῳ ἄλλη ἄλλης ἄλλους ἄλλοις ἄλλον ἄλλο ἄλλου τἄλλα ἄλλα \nἄλλᾳ ἄλλοισιν τἄλλ' ἄλλ' ἄλλος ἄλλοισι κἄλλ' ἄλλοι ἄλλῃσι ἄλλόν ἄλλην ἄλλά ἄλλαι ἄλλοισίν ὧλλοι ἄλλῃ ἄλλας ἀλλέων τἆλλα ἄλλως\nἀλλάων ἄλλαις τἆλλ'\n\nἂν ἄν κἂν τἂν ἃν κεν κ' κέν κέ κε χ' ἄρα τἄρα ἄρ' τἄρ' ἄρ ῥα ῥά ῥ τὰρ ἄρά ἂρ\n\nἡμᾶς με ἐγὼ ἐμὲ μοι κἀγὼ ἡμῶν ἡμεῖς ἐμοὶ ἔγωγ' ἁμοὶ ἡμῖν μ' ἔγωγέ ἐγώ ἐμοί ἐμοῦ κἀμοῦ ἔμ' κἀμὲ ἡμὶν μου ἐμέ ἔγωγε νῷν νὼ χἠμεῖς ἁμὲ κἀγώ κἀμοὶ χἠμᾶς\nἁγὼ ἡμίν κἄμ' ἔμοιγ' μοί τοὐμὲ ἄμμε ἐγὼν ἐμεῦ ἐμεῖο μευ ἔμοιγε ἄμμι μέ ἡμέας νῶϊ ἄμμιν ἧμιν ἐγών νῶΐ ἐμέθεν ἥμιν ἄμμες νῶι ἡμείων ἄμμ' ἡμέων ἐμέο\nἐκ ἔκ ἐξ κἀκ κ ἃκ κἀξ ἔξ εξ Ἐκ τἀμὰ ἐμοῖς τοὐμόν ἐμᾶς τοὐμὸν ἐμῶν ἐμὸς ἐμῆς ἐμῷ τὠμῷ ἐμὸν τἄμ' ἐμὴ ἐμὰς ἐμαῖς ἐμὴν ἐμόν ἐμὰ ἐμός ἐμοὺς ἐμῇ ἐμᾷ\nοὑμὸς ἐμοῖν οὑμός κἀμὸν ἐμαὶ ἐμή ἐμάς ἐμοῖσι ἐμοῖσιν ἐμῇσιν ἐμῇσι ἐμῇς ἐμήν \n\nἔνι ἐνὶ εἰνὶ εἰν ἐμ ἐπὶ ἐπ' ἔπι ἐφ' κἀπὶ τἀπὶ ἐπί ἔφ' ἔπ' ἐὰν ἢν ἐάν ἤν ἄνπερ\n\nαὑτοῖς αὑτὸν αὑτῷ ἑαυτοῦ αὑτόν αὑτῆς αὑτῶν αὑτοῦ αὑτὴν αὑτοῖν χαὐτοῦ αὑταῖς ἑωυτοῦ ἑωυτῇ ἑωυτὸν ἐωυτῷ ἑωυτῆς ἑωυτόν ἑωυτῷ\nἑωυτάς ἑωυτῶν ἑωυτοὺς ἑωυτοῖσι ἑαυτῇ ἑαυτούς αὑτοὺς ἑαυτῶν ἑαυτοὺς ἑαυτὸν ἑαυτῷ ἑαυτοῖς ἑαυτὴν ἑαυτῆς \n\nἔτι ἔτ' ἔθ' κἄτι ἢ ἤ ἠέ ἠὲ ἦε ἦέ ἡ τοὺς τὴν τὸ τῶν τὸν ὁ ἁ οἱ τοῖς ταῖς τῆς τὰς αἱ τό τὰν τᾶς τοῖσιν αἳ χὠ τήν τά τοῖν τάς ὅ\nχοἰ ἣ ἥ χἠ τάν τᾶν ὃ οἳ οἵ τοῖο τόν τοῖιν τούς τάων ταὶ τῇς τῇσι τῇσιν αἵ τοῖό τοῖσίν ὅττί ταί Τὴν τῆ τῶ τάδε ὅδε τοῦδε τόδε τόνδ'\nτάδ' τῆσδε τῷδε ὅδ' τῶνδ' τῇδ' τοῦδέ τῶνδε τόνδε τόδ' τοῦδ' τάσδε τήνδε τάσδ' τήνδ' ταῖσδέ τῇδε τῆσδ' τάνδ' τῷδ' τάνδε ἅδε τοῖσδ' ἥδ'\nτᾷδέ τοῖσδε τούσδ' ἥδε τούσδε τώδ' ἅδ' οἵδ' τῶνδέ οἵδε τᾷδε τοῖσδεσσι τώδε τῇδέ τοῖσιδε αἵδε τοῦδὲ τῆδ' αἵδ' τοῖσδεσι ὃν ἃ ὃς ᾧ οὗ ἅπερ\nοὓς ἧς οἷς ἅσπερ ᾗ ἅ χὦνπερ ὣ αἷς ᾇ ὅς ἥπερ ἃς ὅσπερ ὅνπερ ὧνπερ ᾧπερ ὅν αἷν οἷσι ἇς ἅς ὥ οὕς ἥν οἷσιν ἕης ὅου ᾗς οἷσί οἷσίν τοῖσί ᾗσιν οἵπερ αἷσπερ\nὅστις ἥτις ὅτου ὅτοισι ἥντιν' ὅτῳ ὅντιν' ὅττι ἅσσά ὅτεῳ ὅτις ὅτιν' ὅτευ ἥντινα αἵτινές ὅντινα ἅσσα ᾧτινι οἵτινες ὅτι ἅτις ὅτ' ὑμὴ\nὑμήν ὑμὸν ὑπὲρ ὕπερ ὑπέρτερον ὑπεὶρ ὑπέρτατος ὑπὸ ὑπ' ὑφ' ὕπο ὑπαὶ ὑπό ὕπ' ὕφ'\n\n ὣς ὡς ὥς ὧς ὥστ' ὥστε ὥσθ' ὤ ὢ \n \n ".split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/grc/punctuation.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/eu/__init__.py----------------------------------------
spacy.lang.eu.__init__.Basque(Language)
spacy.lang.eu.__init__.BasqueDefaults(BaseDefaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/eu/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/eu/lex_attrs.py----------------------------------------
A:spacy.lang.eu.lex_attrs._num_words->'\nbat\nbi\nhiru\nlau\nbost\nsei\nzazpi\nzortzi\nbederatzi\nhamar\nhamaika\nhamabi\nhamahiru\nhamalau\nhamabost\nhamasei\nhamazazpi\nHemezortzi\nhemeretzi\nhogei\nehun\nmila\nmilioi\n'.split()
A:spacy.lang.eu.lex_attrs._ordinal_words->'\nlehen\nbigarren\nhirugarren\nlaugarren\nbosgarren\nseigarren\nzazpigarren\nzortzigarren\nbederatzigarren\nhamargarren\nhamaikagarren\nhamabigarren\nhamahirugarren\nhamalaugarren\nhamabosgarren\nhamaseigarren\nhamazazpigarren\nhamazortzigarren\nhemeretzigarren\nhogeigarren\nbehin\n'.split()
A:spacy.lang.eu.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.eu.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.eu.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/eu/stop_words.py----------------------------------------
A:spacy.lang.eu.stop_words.STOP_WORDS->set('\nal\nanitz\narabera\nasko\nbaina\nbat\nbatean\nbatek\nbati\nbatzuei\nbatzuek\nbatzuetan\nbatzuk\nbera\nberaiek\nberau\nberauek\nbere\nberori\nberoriek\nbeste\nbezala\nda\ndago\ndira\nditu\ndu\ndute\nedo\negin\nere\neta\neurak\nez\ngainera\ngu\ngutxi\nguzti\nhaiei\nhaiek\nhaietan\nhainbeste\nhala\nhan\nhandik\nhango\nhara\nhari\nhark\nhartan\nhau\nhauei\nhauek\nhauetan\nhemen\nhemendik\nhemengo\nhi\nhona\nhonek\nhonela\nhonetan\nhoni\nhor\nhori\nhoriei\nhoriek\nhorietan\nhorko\nhorra\nhorrek\nhorrela\nhorretan\nhorri\nhortik\nhura\nizan\nni\nnoiz\nnola\nnon\nnondik\nnongo\nnor\nnora\nze\nzein\nzen\nzenbait\nzenbat\nzer\nzergatik\nziren\nzituen\nzu\nzuek\nzuen\nzuten\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/eu/punctuation.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ga/__init__.py----------------------------------------
spacy.lang.ga.__init__.Irish(Language)
spacy.lang.ga.__init__.IrishDefaults(BaseDefaults)
spacy.lang.ga.__init__.make_lemmatizer(nlp:Language,model:Optional[Model],name:str,mode:str,overwrite:bool)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ga/lemmatizer.py----------------------------------------
A:spacy.lang.ga.lemmatizer.string->unponc(token.text)
A:spacy.lang.ga.lemmatizer.demutated->demutate(string)
A:spacy.lang.ga.lemmatizer.lookup_pos->univ_pos.lower()
A:spacy.lang.ga.lemmatizer.lookup_table->self.lookups.get_table('lemma_lookup_' + lookup_pos, {})
A:spacy.lang.ga.lemmatizer.lc->word.lower()
spacy.lang.ga.IrishLemmatizer(Lemmatizer)
spacy.lang.ga.IrishLemmatizer.get_lookups_config(cls,mode:str)->Tuple[List[str], List[str]]
spacy.lang.ga.IrishLemmatizer.pos_lookup_lemmatize(self,token:Token)->List[str]
spacy.lang.ga.lemmatizer.IrishLemmatizer(Lemmatizer)
spacy.lang.ga.lemmatizer.IrishLemmatizer.get_lookups_config(cls,mode:str)->Tuple[List[str], List[str]]
spacy.lang.ga.lemmatizer.IrishLemmatizer.pos_lookup_lemmatize(self,token:Token)->List[str]
spacy.lang.ga.lemmatizer.demutate(word:str,is_hpref:bool=False)->str
spacy.lang.ga.lemmatizer.unponc(word:str)->str


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ga/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.ga.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ga/stop_words.py----------------------------------------
A:spacy.lang.ga.stop_words.STOP_WORDS->set('\na ach ag agus an aon ar arna as\n\nba beirt bhúr\n\ncaoga ceathair ceathrar chomh chuig chun cois céad cúig cúigear\n\ndaichead dar de deich deichniúr den dhá do don dtí dá dár dó\n\nfaoi faoin faoina faoinár fara fiche\n\ngach gan go gur\n\nhaon hocht\n\ni iad idir in ina ins inár is\n\nle leis lena lenár\n\nmar mo muid mé\n\nna nach naoi naonúr ná ní níor nó nócha\n\nocht ochtar ochtó os\n\nroimh\n\nsa seacht seachtar seachtó seasca seisear siad sibh sinn sna sé sí\n\ntar thar thú triúr trí trína trínár tríocha tú\n\num\n\nár\n\né éis\n\ní\n\nó ón óna ónár\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/bo/__init__.py----------------------------------------
spacy.lang.bo.__init__.Tibetan(Language)
spacy.lang.bo.__init__.TibetanDefaults(BaseDefaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/bo/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/bo/lex_attrs.py----------------------------------------
A:spacy.lang.bo.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.bo.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.bo.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/bo/stop_words.py----------------------------------------
A:spacy.lang.bo.stop_words.STOP_WORDS->set('\nའི་\n།\nདུ་\nགིས་\nསོགས་\nཏེ\nགི་\nརྣམས་\nནི\nཀུན་\nཡི་\nའདི\nཀྱི་\nསྙེད་\nཔས་\nགཞན་\nཀྱིས་\nཡི\nལ\nནི་\nདང་\nསོགས\nཅིང་\nར\nདུ\nམི་\nསུ་\nབཅས་\nཡོངས་\nལས\nཙམ་\nགྱིས་\nདེ་\nཡང་\nམཐའ་དག་\nཏུ་\nཉིད་\nས\nཏེ་\nགྱི་\nསྤྱི\nདེ\nཀ་\nཡིན་\nཞིང་\nའདི་\nརུང་\nརང་\nཞིག་\nསྟེ\nསྟེ་\nན་རེ\nངམ\nཤིང་\nདག་\nཏོ\nརེ་\nའང་\nཀྱང་\nལགས་པ\nཚུ\nདོ\nཡིན་པ\nརེ\nན་རེ་\nཨེ་\nཚང་མ\nཐམས་ཅད་\nདམ་\nའོ་\nཅིག་\nགྱིན་\nཡིན\nན\nཁོ་ན་\nའམ་\nཀྱིན་\nལོ\nཀྱིས\nབས་\nལགས་\nཤིག\nགིས\nཀི་\nསྣ་ཚོགས་\nརྣམས\nསྙེད་པ\nཡིས་\nགྱི\nགི\nབམ་\nཤིག་\nརེ་རེ་\nནམ\nམིན་\nནམ་\nངམ་\nརུ་\nའགའ་\nཀུན\nཤས་\nཏུ\nཡིས\nགིན་\nགམ་\nའོ\nཡིན་པ་\nམིན\nལགས\nགྱིས\nཅང་\nའགའ\nསམ་\nཞིག\nའང\nལས་ཆེ་\nའཕྲལ་\nབར་\nརུ\nདང\nཡ\nའག\nསམ\nཀ\nཅུང་ཟད་\nཅིག\nཉིད\nདུ་མ\nམ\nཡིན་བ\nའམ\nམམ\nདམ\nདག\nཁོ་ན\nཀྱི\nལམ\nཕྱི་\nནང་\nཙམ\nནོ་\nསོ་\nརམ་\nབོ་\nཨང་\nཕྱི\nཏོ་\nཚོ\nལ་ལ་\nཚོ་\nཅིང\nམ་གི་\nགེ\nགོ\nཡིན་ལུགས་\nརོ་\nབོ\nལགས་པ་\nཔས\nརབ་\nའི\nརམ\nབས\nགཞན\nསྙེད་པ་\nའབའ་\nམཾ་\nཔོ\nག་\nག\nགམ\nསྤྱི་\nབམ\nམོ་\nཙམ་པ་\nཤ་སྟག་\nམམ་\nརེ་རེ\nསྙེད\nཏམ་\nངོ\nགྲང་\nཏ་རེ\nཏམ\nཁ་\nངེ་\nཅོག་\nརིལ་\nཉུང་ཤས་\nགིང་\nཚ་\nཀྱང\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/te/__init__.py----------------------------------------
spacy.lang.te.__init__.Telugu(Language)
spacy.lang.te.__init__.TeluguDefaults(BaseDefaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/te/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/te/lex_attrs.py----------------------------------------
A:spacy.lang.te.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.te.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.te.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/te/stop_words.py----------------------------------------
A:spacy.lang.te.stop_words.STOP_WORDS->set('\nఅందరూ\nఅందుబాటులో\nఅడగండి\nఅడగడం\nఅడ్డంగా\nఅనుగుణంగా\nఅనుమతించు\nఅనుమతిస్తుంది\nఅయితే\nఇప్పటికే\nఉన్నారు\nఎక్కడైనా\nఎప్పుడు\nఎవరైనా\nఎవరో ఒకరు\nఏ\nఏదైనా\nఏమైనప్పటికి\nఏమైనప్పటికి\nఒక\nఒక ప్రక్కన\nకనిపిస్తాయి\nకాదు\nకాదు\nకూడా\nగా\nగురించి\nచుట్టూ\nచేయగలిగింది\nతగిన\nతర్వాత\nతర్వాత\nదాదాపు\nదూరంగా\nనిజంగా\nపై\nప్రకారం\nమధ్య\nమధ్య\nమరియు\nమరొక\nమళ్ళీ\nమాత్రమే\nమెచ్చుకో\nవద్ద\nవద్ద\nవెంట\nవేరుగా\nవ్యతిరేకంగా\nసంబంధం\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/si/__init__.py----------------------------------------
spacy.lang.si.__init__.Sinhala(Language)
spacy.lang.si.__init__.SinhalaDefaults(BaseDefaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/si/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/si/lex_attrs.py----------------------------------------
A:spacy.lang.si.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.si.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.si.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/si/stop_words.py----------------------------------------
A:spacy.lang.si.stop_words.STOP_WORDS->set('\nසහ\nසමග\nසමඟ\nඅහා\nආහ්\nආ\nඕහෝ\nඅනේ\nඅඳෝ\nඅපොයි\nඅපෝ\nඅයියෝ\nආයි\nඌයි\nචී\nචිහ්\nචික්\nහෝ\u200d\nදෝ\nදෝහෝ\nමෙන්\nසේ\nවැනි\nබඳු\nවන්\nඅයුරු\nඅයුරින්\nලෙස\nවැඩි\nශ්\u200dරී\nහා\nය\nනිසා\nනිසාවෙන්\nබවට\nබව\nබවෙන්\nනම්\nවැඩි\nසිට\nදී\nමහා\nමහ\nපමණ\nපමණින්\nපමන\nවන\nවිට\nවිටින්\nමේ\nමෙලෙස\nමෙයින්\nඇති\nලෙස\nසිදු\nවශයෙන්\nයන\nසඳහා\nමගින්\nහෝ\u200d\nඉතා\nඒ\nඑම\nද\nඅතර\nවිසින්\nසමග\nපිළිබඳව\nපිළිබඳ\nතුළ\nබව\nවැනි\nමහ\nමෙම\nමෙහි\nමේ\nවෙත\nවෙතින්\nවෙතට\nවෙනුවෙන්\nවෙනුවට\nවෙන\nගැන\nනෑ\nඅනුව\nනව\nපිළිබඳ\nවිශේෂ\nදැනට\nඑහෙන්\nමෙහෙන්\nඑහේ\nමෙහේ\nම\nතවත්\nතව \nසහ\nදක්වා\nට\nගේ\nඑ\nක\nක්\nබවත්\nබවද\nමත\nඇතුලු\nඇතුළු\nමෙසේ\nවඩා\nවඩාත්ම\nනිති\nනිතිත්\nනිතොර\nනිතර\nඉක්බිති\nදැන්\nයලි\nපුන\nඉතින්\nසිට\nසිටන්\nපටන්\nතෙක්\nදක්වා\nසා\nතාක්\nතුවක්\nපවා\nද\nහෝ\u200d\nවත්\nවිනා\nහැර\nමිස\nමුත්\nකිම\nකිම්\nඇයි\nමන්ද\nහෙවත්\nනොහොත්\nපතා\nපාසා\nගානෙ\nතව\nඉතා\nබොහෝ\nවහා\nසෙද\nසැනින්\nහනික\nඑම්බා\nඑම්බල\nබොල\nනම්\nවනාහි\nකලී\nඉඳුරා\nඅන්න\nඔන්න\nමෙන්න\nඋදෙසා\nපිණිස\nසඳහා\nඅරබයා\nනිසා\nඑනිසා\nඑබැවින්\nබැවින්\nහෙයින්\nසේක්\nසේක\nගැන\nඅනුව\nපරිදි\nවිට\nතෙක්\nමෙතෙක්\nමේතාක්\nතුරු\nතුරා\nතුරාවට\nතුලින්\nනමුත්\nඑනමුත්\nවස්\nමෙන්\nලෙස\nපරිදි\nඑහෙත්\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/lb/__init__.py----------------------------------------
spacy.lang.lb.__init__.Luxembourgish(Language)
spacy.lang.lb.__init__.LuxembourgishDefaults(BaseDefaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/lb/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/lb/lex_attrs.py----------------------------------------
A:spacy.lang.lb.lex_attrs._num_words->set('\nnull eent zwee dräi véier fënnef sechs ziwen aacht néng zéng eelef zwielef dräizéng\nvéierzéng foffzéng siechzéng siwwenzéng uechtzeng uechzeng nonnzéng nongzéng zwanzeg drësseg véierzeg foffzeg sechzeg siechzeg siwenzeg achtzeg achzeg uechtzeg uechzeg nonnzeg\nhonnert dausend millioun milliard billioun billiard trillioun triliard\n'.split())
A:spacy.lang.lb.lex_attrs._ordinal_words->set('\néischten zweeten drëtten véierten fënneften sechsten siwenten aachten néngten zéngten eeleften\nzwieleften dräizéngten véierzéngten foffzéngten siechzéngten uechtzéngen uechzéngten nonnzéngten nongzéngten zwanzegsten\ndrëssegsten véierzegsten foffzegsten siechzegsten siwenzegsten uechzegsten nonnzegsten\nhonnertsten dausendsten milliounsten\nmilliardsten billiounsten billiardsten trilliounsten trilliardsten\n'.split())
A:spacy.lang.lb.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.lb.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.lb.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/lb/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.lb.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/lb/stop_words.py----------------------------------------
A:spacy.lang.lb.stop_words.STOP_WORDS->set("\na\nà\näis\när\närt\näert\nären\nall\nallem\nalles\nalleguer\nals\nalso\nam\nan\nanerefalls\nass\naus\nawer\nbei\nbeim\nbis\nbis\nd'\ndach\ndatt\ndäin\ndär\ndat\nde\ndee\nden\ndeel\ndeem\ndeen\ndeene\ndéi\nden\ndeng\ndenger\ndem\nder\ndësem\ndi\ndir\ndo\nda\ndann\ndomat\ndozou\ndrop\ndu\nduerch\nduerno\ne\nee\nem\neen\neent\në\nen\nënner\nëm\nech\neis\neise\neisen\neiser\neises\neisereen\nesou\neen\neng\nenger\nengem\nentweder\net\neréischt\nfalls\nfir\ngéint\ngéif\ngëtt\ngët\ngeet\ngi\nginn\ngouf\ngouff\ngoung\nhat\nhaten\nhatt\nhätt\nhei\nhu\nhuet\nhun\nhunn\nhiren\nhien\nhin\nhier\nhir\njidderen\njiddereen\njiddwereen\njiddereng\njiddwerengen\njo\nins\niech\niwwer\nkann\nkee\nkeen\nkënne\nkënnt\nkéng\nkéngen\nkéngem\nkoum\nkuckt\nmam\nmat\nma\nmä\nmech\nméi\nmécht\nmeng\nmenger\nmer\nmir\nmuss\nnach\nnämmlech\nnämmelech\nnäischt\nnawell\nnëmme\nnëmmen\nnet\nnees\nnee\nno\nnu\nnom\noch\noder\nons\nonsen\nonser\nonsereen\nonst\nom\nop\nouni\nsäi\nsäin\nschonn\nschonns\nsi\nsid\nsie\nse\nsech\nseng\nsenge\nsengem\nsenger\nselwecht\nselwer\nsinn\nsollten\nsouguer\nsou\nsoss\nsot\n't\ntëscht\nu\nun\num\nvirdrun\nvu\nvum\nvun\nwann\nwar\nwaren\nwas\nwat\nwëllt\nweider\nwéi\nwéini\nwéinst\nwi\nwollt\nwou\nwouhin\nzanter\nze\nzu\nzum\nzwar\n".split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/lb/punctuation.py----------------------------------------
A:spacy.lang.lb.punctuation.ELISION->" ' ’ ".strip().replace(' ', '')


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/sa/__init__.py----------------------------------------
spacy.lang.sa.__init__.Sanskrit(Language)
spacy.lang.sa.__init__.SanskritDefaults(BaseDefaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/sa/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/sa/lex_attrs.py----------------------------------------
A:spacy.lang.sa.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.sa.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.sa.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/sa/stop_words.py----------------------------------------
A:spacy.lang.sa.stop_words.STOP_WORDS->set('\nअहम्\nआवाम्\nवयम्\nमाम्  मा\nआवाम्\nअस्मान्  नः\nमया\nआवाभ्याम्\nअस्माभिस्\nमह्यम्  मे\nआवाभ्याम्  नौ\nअस्मभ्यम्  नः\nमत्\nआवाभ्याम्\nअस्मत्\nमम  मे\nआवयोः\nअस्माकम्  नः\nमयि\nआवयोः\nअस्मासु\nत्वम्\nयुवाम्\nयूयम्\nत्वाम्  त्वा\nयुवाम्  वाम्\nयुष्मान्  वः\nत्वया\nयुवाभ्याम्\nयुष्माभिः\nतुभ्यम्  ते\nयुवाभ्याम्  वाम्\nयुष्मभ्यम्  वः\nत्वत्\nयुवाभ्याम्\nयुष्मत्\nतव  ते\nयुवयोः  वाम्\nयुष्माकम्  वः\nत्वयि\nयुवयोः\nयुष्मासु\nसः\nतौ\nते\nतम्\nतौ\nतान्\nतेन\nताभ्याम्\nतैः\nतस्मै\nताभ्याम्\nतेभ्यः\nतस्मात्\nताभ्याम्\nतेभ्यः\nतस्य\nतयोः\nतेषाम्\nतस्मिन्\nतयोः\nतेषु\nसा\nते\nताः\nताम्\nते\nताः\nतया\nताभ्याम्\nताभिः\nतस्यै\nताभ्याम्\nताभ्यः\nतस्याः\nताभ्याम्\nताभ्यः\nतस्य\nतयोः\nतासाम्\nतस्याम्\nतयोः\nतासु\nतत्\nते\nतानि\nतत्\nते\nतानि\nतया\nताभ्याम्\nताभिः\nतस्यै\nताभ्याम्\nताभ्यः\nतस्याः\nताभ्याम्\nताभ्यः\nतस्य\nतयोः\nतासाम्\nतस्याम्\nतयोः\nतासु\nअयम्\nइमौ\nइमे\nइमम्\nइमौ\nइमान्\nअनेन\nआभ्याम्\nएभिः\nअस्मै\nआभ्याम्\nएभ्यः\nअस्मात्\nआभ्याम्\nएभ्यः\nअस्य\nअनयोः\nएषाम्\nअस्मिन्\nअनयोः\nएषु\nइयम्\nइमे\nइमाः\nइमाम्\nइमे\nइमाः\nअनया\nआभ्याम्\nआभिः\nअस्यै\nआभ्याम्\nआभ्यः\nअस्याः\nआभ्याम्\nआभ्यः\nअस्याः\nअनयोः\nआसाम्\nअस्याम्\nअनयोः\nआसु\nइदम्\nइमे\nइमानि\nइदम्\nइमे\nइमानि\nअनेन\nआभ्याम्\nएभिः\nअस्मै\nआभ्याम्\nएभ्यः\nअस्मात्\nआभ्याम्\nएभ्यः\nअस्य\nअनयोः\nएषाम्\nअस्मिन्\nअनयोः\nएषु\nएषः\nएतौ\nएते\nएतम्  एनम्\nएतौ  एनौ\nएतान्  एनान्\nएतेन\nएताभ्याम्\nएतैः\nएतस्मै\nएताभ्याम्\nएतेभ्यः\nएतस्मात्\nएताभ्याम्\nएतेभ्यः\nएतस्य\nएतस्मिन्\nएतेषाम्\nएतस्मिन्\nएतस्मिन्\nएतेषु\nएषा\nएते\nएताः\nएताम्  एनाम्\nएते  एने\nएताः  एनाः\nएतया  एनया\nएताभ्याम्\nएताभिः\nएतस्यै\nएताभ्याम्\nएताभ्यः\nएतस्याः\nएताभ्याम्\nएताभ्यः\nएतस्याः\nएतयोः  एनयोः\nएतासाम्\nएतस्याम्\nएतयोः  एनयोः\nएतासु\nएतत्  एतद्\nएते\nएतानि\nएतत्  एतद्  एनत्  एनद्\nएते  एने\nएतानि  एनानि\nएतेन  एनेन\nएताभ्याम्\nएतैः\nएतस्मै\nएताभ्याम्\nएतेभ्यः\nएतस्मात्\nएताभ्याम्\nएतेभ्यः\nएतस्य\nएतयोः  एनयोः\nएतेषाम्\nएतस्मिन्\nएतयोः  एनयोः\nएतेषु\nअसौ\nअमू\nअमी\nअमूम्\nअमू\nअमून्\nअमुना\nअमूभ्याम्\nअमीभिः\nअमुष्मै\nअमूभ्याम्\nअमीभ्यः\nअमुष्मात्\nअमूभ्याम्\nअमीभ्यः\nअमुष्य\nअमुयोः\nअमीषाम्\nअमुष्मिन्\nअमुयोः\nअमीषु\nअसौ\nअमू\nअमूः\nअमूम्\nअमू\nअमूः\nअमुया\nअमूभ्याम्\nअमूभिः\nअमुष्यै\nअमूभ्याम्\nअमूभ्यः\nअमुष्याः\nअमूभ्याम्\nअमूभ्यः\nअमुष्याः\nअमुयोः\nअमूषाम्\nअमुष्याम्\nअमुयोः\nअमूषु\nअमु\nअमुनी\nअमूनि\nअमु\nअमुनी\nअमूनि\nअमुना\nअमूभ्याम्\nअमीभिः\nअमुष्मै\nअमूभ्याम्\nअमीभ्यः\nअमुष्मात्\nअमूभ्याम्\nअमीभ्यः\nअमुष्य\nअमुयोः\nअमीषाम्\nअमुष्मिन्\nअमुयोः\nअमीषु\nकः\nकौ\nके\nकम्\nकौ\nकान्\nकेन\nकाभ्याम्\nकैः\nकस्मै\nकाभ्याम्\nकेभ्य\nकस्मात्\nकाभ्याम्\nकेभ्य\nकस्य\nकयोः\nकेषाम्\nकस्मिन्\nकयोः\nकेषु\nका\nके\nकाः\nकाम्\nके\nकाः\nकया\nकाभ्याम्\nकाभिः\nकस्यै\nकाभ्याम्\nकाभ्यः\nकस्याः\nकाभ्याम्\nकाभ्यः\nकस्याः\nकयोः\nकासाम्\nकस्याम्\nकयोः\nकासु\nकिम्\nके\nकानि\nकिम्\nके\nकानि\nकेन\nकाभ्याम्\nकैः\nकस्मै\nकाभ्याम्\nकेभ्य\nकस्मात्\nकाभ्याम्\nकेभ्य\nकस्य\nकयोः\nकेषाम्\nकस्मिन्\nकयोः\nकेषु\nभवान्\nभवन्तौ\nभवन्तः\nभवन्तम्\nभवन्तौ\nभवतः\nभवता\nभवद्भ्याम्\nभवद्भिः\nभवते\nभवद्भ्याम्\nभवद्भ्यः\nभवतः\nभवद्भ्याम्\nभवद्भ्यः\nभवतः\nभवतोः\nभवताम्\nभवति\nभवतोः\nभवत्सु\nभवती\nभवत्यौ\nभवत्यः\nभवतीम्\nभवत्यौ\nभवतीः\nभवत्या\nभवतीभ्याम्\nभवतीभिः\nभवत्यै\nभवतीभ्याम्\nभवतीभिः\nभवत्याः\nभवतीभ्याम्\nभवतीभिः\nभवत्याः\nभवत्योः\nभवतीनाम्\nभवत्याम्\nभवत्योः\nभवतीषु\nभवत्\nभवती\nभवन्ति\nभवत्\nभवती\nभवन्ति\nभवता\nभवद्भ्याम्\nभवद्भिः\nभवते\nभवद्भ्याम्\nभवद्भ्यः\nभवतः\nभवद्भ्याम्\nभवद्भ्यः\nभवतः\nभवतोः\nभवताम्\nभवति\nभवतोः\nभवत्सु\nअये\nअरे\nअरेरे\nअविधा\nअसाधुना\nअस्तोभ\nअहह\nअहावस्\nआम्\nआर्यहलम्\nआह\nआहो\nइस्\nउम्\nउवे\nकाम्\nकुम्\nचमत्\nटसत्\nदृन्\nधिक्\nपाट्\nफत्\nफाट्\nफुडुत्\nबत\nबाल्\nवट्\nव्यवस्तोभति व्यवस्तुभ्\nषाट्\nस्तोभ\nहुम्मा\nहूम्\nअति\nअधि\nअनु\nअप\nअपि\nअभि\nअव\nआ\nउद्\nउप\nनि\nनिर्\nपरा\nपरि\nप्र\nप्रति\nवि\nसम्\nअथवा उत\nअन्यथा\nइव\nच\nचेत् यदि\nतु परन्तु\nयतः करणेन हि यतस् यदर्थम् यदर्थे यर्हि यथा यत्कारणम् येन ही हिन\nयथा यतस्\nयद्यपि\nयात् अवधेस् यावति\nयेन प्रकारेण\nस्थाने\nअह\nएव\nएवम्\nकच्चित्\nकु\nकुवित्\nकूपत्\nच\nचण्\nचेत्\nतत्र\nनकिम्\nनह\nनुनम्\nनेत्\nभूयस्\nमकिम्\nमकिर्\nयत्र\nयुगपत्\nवा\nशश्वत्\nसूपत्\nह\nहन्त\nहि\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/it/__init__.py----------------------------------------
spacy.lang.it.__init__.Italian(Language)
spacy.lang.it.__init__.ItalianDefaults(BaseDefaults)
spacy.lang.it.__init__.make_lemmatizer(nlp:Language,model:Optional[Model],name:str,mode:str,overwrite:bool,scorer:Optional[Callable])


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/it/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/it/lemmatizer.py----------------------------------------
A:spacy.lang.it.lemmatizer.morphology->token.morph.to_dict()
A:spacy.lang.it.lemmatizer.lookup_pos->univ_pos.lower()
A:spacy.lang.it.lemmatizer.lookup_table->self.lookups.get_table('lemma_lookup')
A:spacy.lang.it.lemmatizer.string->string.lower().lower()
A:spacy.lang.it.lemmatizer.lemma->self.lookups.get_table('lemma_lookup').get(string, string)
spacy.lang.it.ItalianLemmatizer(Lemmatizer)
spacy.lang.it.ItalianLemmatizer.get_lookups_config(cls,mode:str)->Tuple[List[str], List[str]]
spacy.lang.it.ItalianLemmatizer.lemmatize_adj(self,string:str,morphology:dict,lookup_table:Dict[str,str])->List[str]
spacy.lang.it.ItalianLemmatizer.lemmatize_adp(self,string:str,morphology:dict,lookup_table:Dict[str,str])->List[str]
spacy.lang.it.ItalianLemmatizer.lemmatize_det(self,string:str,morphology:dict,lookup_table:Dict[str,str])->List[str]
spacy.lang.it.ItalianLemmatizer.lemmatize_noun(self,string:str,morphology:dict,lookup_table:Dict[str,str])->List[str]
spacy.lang.it.ItalianLemmatizer.lemmatize_pron(self,string:str,morphology:dict,lookup_table:Dict[str,str])->List[str]
spacy.lang.it.ItalianLemmatizer.pos_lookup_lemmatize(self,token:Token)->List[str]
spacy.lang.it.lemmatizer.ItalianLemmatizer(Lemmatizer)
spacy.lang.it.lemmatizer.ItalianLemmatizer.get_lookups_config(cls,mode:str)->Tuple[List[str], List[str]]
spacy.lang.it.lemmatizer.ItalianLemmatizer.lemmatize_adj(self,string:str,morphology:dict,lookup_table:Dict[str,str])->List[str]
spacy.lang.it.lemmatizer.ItalianLemmatizer.lemmatize_adp(self,string:str,morphology:dict,lookup_table:Dict[str,str])->List[str]
spacy.lang.it.lemmatizer.ItalianLemmatizer.lemmatize_det(self,string:str,morphology:dict,lookup_table:Dict[str,str])->List[str]
spacy.lang.it.lemmatizer.ItalianLemmatizer.lemmatize_noun(self,string:str,morphology:dict,lookup_table:Dict[str,str])->List[str]
spacy.lang.it.lemmatizer.ItalianLemmatizer.lemmatize_pron(self,string:str,morphology:dict,lookup_table:Dict[str,str])->List[str]
spacy.lang.it.lemmatizer.ItalianLemmatizer.pos_lookup_lemmatize(self,token:Token)->List[str]


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/it/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.it.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/it/syntax_iterators.py----------------------------------------
A:spacy.lang.it.syntax_iterators.np_label->doc.vocab.strings.add('NP')
A:spacy.lang.it.syntax_iterators.adj_label->doc.vocab.strings.add('amod')
A:spacy.lang.it.syntax_iterators.det_pos->doc.vocab.strings.add('DET')
A:spacy.lang.it.syntax_iterators.adp_label->doc.vocab.strings.add('ADP')
A:spacy.lang.it.syntax_iterators.conj->doc.vocab.strings.add('conj')
A:spacy.lang.it.syntax_iterators.conj_pos->doc.vocab.strings.add('CCONJ')
A:spacy.lang.it.syntax_iterators.right_childs->list(word.rights)
spacy.lang.it.syntax_iterators.noun_chunks(doclike:Union[Doc,Span])->Iterator[Tuple[int, int, int]]


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/it/stop_words.py----------------------------------------
A:spacy.lang.it.stop_words.STOP_WORDS->set("\na abbastanza abbia abbiamo abbiano abbiate accidenti ad adesso affinche agl\nagli ahime ahimè ai al alcuna alcuni alcuno all alla alle allo allora altri\naltrimenti altro altrove altrui anche ancora anni anno ansa anticipo assai\nattesa attraverso avanti avemmo avendo avente aver avere averlo avesse\navessero avessi avessimo aveste avesti avete aveva avevamo avevano avevate\navevi avevo avrai avranno avrebbe avrebbero avrei avremmo avremo avreste\navresti avrete avrà avrò avuta avute avuti avuto\n\nbasta bene benissimo brava bravo\n\ncasa caso cento certa certe certi certo che chi chicchessia chiunque ci c'\nciascuna ciascuno cima cio cioe circa citta città co codesta codesti codesto\ncogli coi col colei coll coloro colui come cominci comunque con concernente\nconciliarsi conclusione consiglio contro cortesia cos cosa cosi così cui\n\nd' da dagl dagli dai dal dall dall' dalla dalle dallo dappertutto davanti degl degli\ndei del dell dell' della delle dello dentro detto deve di dice dietro dire\ndirimpetto diventa diventare diventato dopo dov dove dovra dovrà dovunque due\ndunque durante\n\ne ebbe ebbero ebbi ecc ecco ed effettivamente egli ella entrambi eppure era\nerano eravamo eravate eri ero esempio esse essendo esser essere essi ex è\n\nfa faccia facciamo facciano facciate faccio facemmo facendo facesse facessero\nfacessi facessimo faceste facesti faceva facevamo facevano facevate facevi\nfacevo fai fanno farai faranno fare farebbe farebbero farei faremmo faremo\nfareste faresti farete farà farò fatto favore fece fecero feci fin finalmente\nfinche fine fino forse forza fosse fossero fossi fossimo foste fosti fra\nfrattempo fu fui fummo fuori furono futuro generale\n\ngia già giacche giorni giorno gli gl' gliela gliele glieli glielo gliene governo\ngrande grazie gruppo\n\nha haha hai hanno ho\n\nieri il improvviso in inc infatti inoltre insieme intanto intorno invece io\n\nl' la là lasciato lato lavoro le lei li lo lontano loro lui lungo luogo\n\nm' ma macche magari maggior mai male malgrado malissimo mancanza marche me\nmedesimo mediante meglio meno mentre mesi mezzo mi mia mie miei mila miliardi\nmilioni minimi ministro mio modo molti moltissimo molto momento mondo mosto\n\nnazionale ne negl negli nei nel nell nella nelle nello nemmeno neppure nessun nessun'\nnessuna nessuno nient' niente no noi non nondimeno nonostante nonsia nostra nostre\nnostri nostro novanta nove nulla nuovo\n\nod oggi ogni ognuna ognuno oltre oppure ora ore osi ossia ottanta otto\n\npaese parecchi parecchie parecchio parte partendo peccato peggio per perche\nperché percio perciò perfino pero persino persone però piedi pieno piglia piu\npiuttosto più po pochissimo poco poi poiche possa possedere posteriore posto\npotrebbe preferibilmente presa press prima primo principalmente probabilmente\nproprio puo può pure purtroppo\n\nqualche qualcosa qualcuna qualcuno quale quali qualunque quando quanta quante\nquanti quanto quantunque quasi quattro quel quel' quella quelle quelli quello quest quest'\nquesta queste questi questo qui quindi\n\nrealmente recente recentemente registrazione relativo riecco salvo\n\ns' sara sarà sarai saranno sarebbe sarebbero sarei saremmo saremo sareste\nsaresti sarete saro sarò scola scopo scorso se secondo seguente seguito sei\nsembra sembrare sembrato sembri sempre senza sette si sia siamo siano siate\nsiete sig solito solo soltanto sono sopra sotto spesso srl sta stai stando\nstanno starai staranno starebbe starebbero starei staremmo staremo stareste\nstaresti starete starà starò stata state stati stato stava stavamo stavano\nstavate stavi stavo stemmo stessa stesse stessero stessi stessimo stesso\nsteste stesti stette stettero stetti stia stiamo stiano stiate sto su sua\nsubito successivamente successivo sue sugl sugli sui sul sull sulla sulle\nsullo suo suoi\n\nt' tale tali talvolta tanto te tempo ti titolo tra tranne tre trenta\ntroppo trovato tu tua tue tuo tuoi tutta tuttavia tutte tutti tutto\n\nuguali ulteriore ultimo un un' una uno uomo\n\nv' va vale vari varia varie vario verso vi via vicino visto vita voi volta volte\nvostra vostre vostri vostro\n".split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/it/punctuation.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/en/__init__.py----------------------------------------
spacy.lang.en.__init__.English(Language)
spacy.lang.en.__init__.EnglishDefaults(BaseDefaults)
spacy.lang.en.__init__.make_lemmatizer(nlp:Language,model:Optional[Model],name:str,mode:str,overwrite:bool,scorer:Optional[Callable])


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/en/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/en/lemmatizer.py----------------------------------------
A:spacy.lang.en.lemmatizer.univ_pos->token.pos_.lower()
A:spacy.lang.en.lemmatizer.morphology->token.morph.to_dict()
spacy.lang.en.EnglishLemmatizer(Lemmatizer)
spacy.lang.en.EnglishLemmatizer.is_base_form(self,token:Token)->bool
spacy.lang.en.lemmatizer.EnglishLemmatizer(Lemmatizer)
spacy.lang.en.lemmatizer.EnglishLemmatizer.is_base_form(self,token:Token)->bool


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/en/lex_attrs.py----------------------------------------
A:spacy.lang.en.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.en.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
A:spacy.lang.en.lex_attrs.text_lower->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').lower()
spacy.lang.en.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/en/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.en.tokenizer_exceptions.verb_data_tc->dict(verb_data)
A:spacy.lang.en.tokenizer_exceptions.verb_data_tc[ORTH]->verb_data_tc[ORTH].title().title()
A:spacy.lang.en.tokenizer_exceptions.exc_data_tc->dict(exc_data)
A:spacy.lang.en.tokenizer_exceptions.exc_data_tc[ORTH]->exc_data_tc[ORTH].title().title()
A:spacy.lang.en.tokenizer_exceptions.data_apos->dict(data)
A:spacy.lang.en.tokenizer_exceptions.exc_data_apos->dict(exc_data)
A:spacy.lang.en.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/en/syntax_iterators.py----------------------------------------
A:spacy.lang.en.syntax_iterators.conj->doc.vocab.strings.add('conj')
A:spacy.lang.en.syntax_iterators.np_label->doc.vocab.strings.add('NP')
spacy.lang.en.syntax_iterators.noun_chunks(doclike:Union[Doc,Span])->Iterator[Tuple[int, int, int]]


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/en/stop_words.py----------------------------------------
A:spacy.lang.en.stop_words.STOP_WORDS->set('\na about above across after afterwards again against all almost alone along\nalready also although always am among amongst amount an and another any anyhow\nanyone anything anyway anywhere are around as at\n\nback be became because become becomes becoming been before beforehand behind\nbeing below beside besides between beyond both bottom but by\n\ncall can cannot ca could\n\ndid do does doing done down due during\n\neach eight either eleven else elsewhere empty enough even ever every\neveryone everything everywhere except\n\nfew fifteen fifty first five for former formerly forty four from front full\nfurther\n\nget give go\n\nhad has have he hence her here hereafter hereby herein hereupon hers herself\nhim himself his how however hundred\n\ni if in indeed into is it its itself\n\nkeep\n\nlast latter latterly least less\n\njust\n\nmade make many may me meanwhile might mine more moreover most mostly move much\nmust my myself\n\nname namely neither never nevertheless next nine no nobody none noone nor not\nnothing now nowhere\n\nof off often on once one only onto or other others otherwise our ours ourselves\nout over own\n\npart per perhaps please put\n\nquite\n\nrather re really regarding\n\nsame say see seem seemed seeming seems serious several she should show side\nsince six sixty so some somehow someone something sometime sometimes somewhere\nstill such\n\ntake ten than that the their them themselves then thence there thereafter\nthereby therefore therein thereupon these they third this those though three\nthrough throughout thru thus to together too top toward towards twelve twenty\ntwo\n\nunder until up unless upon us used using\n\nvarious very very via was we well were what whatever when whence whenever where\nwhereafter whereas whereby wherein whereupon wherever whether which while\nwhither who whoever whole whom whose why will with within without would\n\nyet you your yours yourself yourselves\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/en/punctuation.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/fi/__init__.py----------------------------------------
spacy.lang.fi.__init__.Finnish(Language)
spacy.lang.fi.__init__.FinnishDefaults(BaseDefaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/fi/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/fi/lex_attrs.py----------------------------------------
A:spacy.lang.fi.lex_attrs.text->text.replace('.', '').replace(',', '').replace('.', '').replace(',', '')
A:spacy.lang.fi.lex_attrs.(num, denom)->text.replace('.', '').replace(',', '').replace('.', '').replace(',', '').split('/')
spacy.lang.fi.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/fi/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.fi.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/fi/syntax_iterators.py----------------------------------------
A:spacy.lang.fi.syntax_iterators.np_label->doc.vocab.strings.add('NP')
A:spacy.lang.fi.syntax_iterators.conj_label->doc.vocab.strings.add('conj')
spacy.lang.fi.syntax_iterators.noun_chunks(doclike:Union[Doc,Span])->Iterator[Tuple[int, int, int]]


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/fi/stop_words.py----------------------------------------
A:spacy.lang.fi.stop_words.STOP_WORDS->set('\naiemmin aika aikaa aikaan aikaisemmin aikaisin aikana aikoina aikoo aikovat\naina ainakaan ainakin ainoa ainoat aiomme aion aiotte aivan ajan alas alemmas\nalkuisin alkuun alla alle aloitamme aloitan aloitat aloitatte aloitattivat\naloitettava aloitettavaksi aloitettu aloitimme aloitin aloitit aloititte\naloittaa aloittamatta aloitti aloittivat alta aluksi alussa alusta annettavaksi\nannettava annettu ansiosta antaa antamatta antoi apu asia asiaa asian asiasta\nasiat asioiden asioihin asioita asti avuksi avulla avun avutta\n\nedelle edelleen edellä edeltä edemmäs edes edessä edestä ehkä ei eikä eilen\neivät eli ellei elleivät ellemme ellen ellet ellette emme en enemmän eniten\nennen ensi ensimmäinen ensimmäiseksi ensimmäisen ensimmäisenä ensimmäiset\nensimmäisiksi ensimmäisinä ensimmäisiä ensimmäistä ensin entinen entisen\nentisiä entisten entistä enää eri erittäin erityisesti eräiden eräs eräät esi\nesiin esillä esimerkiksi et eteen etenkin ette ettei että\n\nhalua haluaa haluamatta haluamme haluan haluat haluatte haluavat halunnut\nhalusi halusimme halusin halusit halusitte halusivat halutessa haluton he hei\nheidän heidät heihin heille heillä heiltä heissä heistä heitä helposti heti\nhetkellä hieman hitaasti huolimatta huomenna hyvien hyviin hyviksi hyville\nhyviltä hyvin hyvinä hyvissä hyvistä hyviä hyvä hyvät hyvää hän häneen hänelle\nhänellä häneltä hänen hänessä hänestä hänet häntä\n\nihan ilman ilmeisesti itse itsensä itseään\n\nja jo johon joiden joihin joiksi joilla joille joilta joina joissa joista joita\njoka jokainen jokin joko joksi joku jolla jolle jolloin jolta jompikumpi jona\njonka jonkin jonne joo jopa jos joskus jossa josta jota jotain joten jotenkin\njotenkuten jotka jotta jouduimme jouduin jouduit jouduitte joudumme joudun\njoudutte joukkoon joukossa joukosta joutua joutui joutuivat joutumaan joutuu\njoutuvat juuri jälkeen jälleen jää\n\nkahdeksan kahdeksannen kahdella kahdelle kahdelta kahden kahdessa kahdesta\nkahta kahteen kai kaiken kaikille kaikilta kaikkea kaikki kaikkia kaikkiaan\nkaikkialla kaikkialle kaikkialta kaikkien kaikkiin kaksi kannalta kannattaa\nkanssa kanssaan kanssamme kanssani kanssanne kanssasi kauan kauemmas kaukana\nkautta kehen keiden keihin keiksi keille keillä keiltä keinä keissä keistä\nkeitten keittä keitä keneen keneksi kenelle kenellä keneltä kenen kenenä\nkenessä kenestä kenet kenettä kenties kerran kerta kertaa keskellä kesken\nkeskimäärin ketkä ketä kiitos kohti koko kokonaan kolmas kolme kolmen kolmesti\nkoska koskaan kovin kuin kuinka kuinkaan kuitenkaan kuitenkin kuka kukaan kukin\nkumpainen kumpainenkaan kumpi kumpikaan kumpikin kun kuten kuuden kuusi kuutta\nkylliksi kyllä kymmenen kyse\n\nliian liki lisäksi lisää lla luo luona lähekkäin lähelle lähellä läheltä\nlähemmäs lähes lähinnä lähtien läpi\n\nmahdollisimman mahdollista me meidän meidät meihin meille meillä meiltä meissä\nmeistä meitä melkein melko menee menemme menen menet menette menevät meni\nmenimme menin menit menivät mennessä mennyt menossa mihin miksi mikä mikäli\nmikään mille milloin milloinkan millä miltä minkä minne minua minulla minulle\nminulta minun minussa minusta minut minuun minä missä mistä miten mitkä mitä\nmitään moi molemmat mones monesti monet moni moniaalla moniaalle moniaalta\nmonta muassa muiden muita muka mukaan mukaansa mukana mutta muu muualla muualle\nmuualta muuanne muulloin muun muut muuta muutama muutaman muuten myöhemmin myös\nmyöskin myöskään myötä\n\nne neljä neljän neljää niiden niihin niiksi niille niillä niiltä niin niinä\nniissä niistä niitä noiden noihin noiksi noilla noille noilta noin noina noissa\nnoista noita nopeammin nopeasti nopeiten nro nuo nyt näiden näihin näiksi\nnäille näillä näiltä näin näinä näissä näistä näitä nämä\n\nohi oikea oikealla oikein ole olemme olen olet olette oleva olevan olevat oli\nolimme olin olisi olisimme olisin olisit olisitte olisivat olit olitte olivat\nolla olleet ollut oma omaa omaan omaksi omalle omalta oman omassa omat omia\nomien omiin omiksi omille omilta omissa omista on onkin onko ovat\n\npaikoittain paitsi pakosti paljon paremmin parempi parhaillaan parhaiten\nperusteella peräti pian pieneen pieneksi pienelle pienellä pieneltä pienempi\npienestä pieni pienin poikki puolesta puolestaan päälle\n\nrunsaasti\n\nsaakka sama samaa samaan samalla saman samat samoin satojen se\nseitsemän sekä sen seuraavat siellä sieltä siihen siinä siis siitä sijaan siksi\nsille silloin sillä silti siltä sinne sinua sinulla sinulle sinulta sinun\nsinussa sinusta sinut sinuun sinä sisäkkäin sisällä siten sitten sitä ssa sta\nsuoraan suuntaan suuren suuret suuri suuria suurin suurten\n\ntaa taas taemmas tahansa tai takaa takaisin takana takia tallä tapauksessa\ntarpeeksi tavalla tavoitteena te teidän teidät teihin teille teillä teiltä\nteissä teistä teitä tietysti todella toinen toisaalla toisaalle toisaalta\ntoiseen toiseksi toisella toiselle toiselta toisemme toisen toisensa toisessa\ntoisesta toista toistaiseksi toki tosin tule tulee tulemme tulen\ntulet tulette tulevat tulimme tulin tulisi tulisimme tulisin tulisit tulisitte\ntulisivat tulit tulitte tulivat tulla tulleet tullut tuntuu tuo tuohon tuoksi\ntuolla tuolle tuolloin tuolta tuon tuona tuonne tuossa tuosta tuota tuskin tykö\ntähän täksi tälle tällä tällöin tältä tämä tämän tänne tänä tänään tässä tästä\ntäten tätä täysin täytyvät täytyy täällä täältä\n\nulkopuolella usea useasti useimmiten usein useita uudeksi uudelleen uuden uudet\nuusi uusia uusien uusinta uuteen uutta\n\nvaan vai vaiheessa vaikea vaikean vaikeat vaikeilla vaikeille vaikeilta\nvaikeissa vaikeista vaikka vain varmasti varsin varsinkin varten vasen\nvasemmalla vasta vastaan vastakkain vastan verran vielä vierekkäin vieressä\nvieri viiden viime viimeinen viimeisen viimeksi viisi voi voidaan voimme voin\nvoisi voit voitte voivat vuoden vuoksi vuosi vuosien vuosina vuotta vähemmän\nvähintään vähiten vähän välillä\n\nyhdeksän yhden yhdessä yhteen yhteensä yhteydessä yhteyteen yhtä yhtäälle\nyhtäällä yhtäältä yhtään yhä yksi yksin yksittäin yleensä ylemmäs yli ylös\nympäri\n\nälköön älä\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/fi/punctuation.py----------------------------------------
A:spacy.lang.fi.punctuation._quotes->char_classes.CONCAT_QUOTES.replace("'", '')
A:spacy.lang.fi.punctuation.DASHES->'|'.join((x for x in LIST_HYPHENS if x != '-'))


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/hsb/__init__.py----------------------------------------
spacy.lang.hsb.__init__.UpperSorbian(Language)
spacy.lang.hsb.__init__.UpperSorbianDefaults(BaseDefaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/hsb/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/hsb/lex_attrs.py----------------------------------------
A:spacy.lang.hsb.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.hsb.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
A:spacy.lang.hsb.lex_attrs.text_lower->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').lower()
spacy.lang.hsb.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/hsb/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.hsb.tokenizer_exceptions._exc->dict()
A:spacy.lang.hsb.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/hsb/stop_words.py----------------------------------------
A:spacy.lang.hsb.stop_words.STOP_WORDS->set('\na abo ale ani\n\ndokelž\n\nhdyž\n\njeli jelizo\n\nkaž\n\npak potom\n\ntež tohodla\n\nzo zoby\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/sq/__init__.py----------------------------------------
spacy.lang.sq.__init__.Albanian(Language)
spacy.lang.sq.__init__.AlbanianDefaults(BaseDefaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/sq/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/sq/stop_words.py----------------------------------------
A:spacy.lang.sq.stop_words.STOP_WORDS->set('\na\nafert\nai\najo\nandej\nanes\naq\nas\nasaj\nashtu\nata\nate\natij\natje\nato\naty\natyre\nb\nbe\nbehem\nbehet\nbej\nbeje\nbejne\nben\nbene\nbere\nberi\nbie\nc\nca\ncdo\ncfare\ncila\ncilat\ncilave\ncilen\nciles\ncilet\ncili\ncilin\ncilit\nderi\ndhe\ndic\ndicka\ndickaje\ndike\ndikujt\ndikush\ndisa\ndo\ndot\ndrejt\nduke\ndy\ne\nedhe\nende\neshte\netj\nfare\ngjate\ngje\ngjitha\ngjithcka\ngjithe\ngjithnje\nhere\ni\nia\nishin\nishte\niu\nja\njam\njane\njap\nje\njemi\njo\nju\nk\nka\nkam\nkane\nkem\nkemi\nkeq\nkesaj\nkeshtu\nkete\nketej\nketij\nketo\nketu\nketyre\nkishin\nkishte\nkjo\nkrejt\nkryer\nkryesisht\nkryhet\nku\nkudo\nkundrejt\nkur\nkurre\nkush\nky\nla\nle\nlloj\nm\nma\nmadhe\nmarr\nmarre\nmban\nmbi\nme\nmenjehere\nmerr\nmerret\nmes\nmi\nmidis\nmire\nmjaft\nmori\nmos\nmua\nmund\nna\nndaj\nnder\nndermjet\nndersa\nndonje\nndryshe\nne\nnen\nneper\nnepermjet\nnese\nnga\nnje\nnjera\nnuk\nose\npa\npak\npapritur\npara\npas\npasi\npasur\nper\nperbashket\nperpara\npo\npor\nprane\nprapa\nprej\npse\nqe\nqene\nqenet\nrralle\nrreth\nrri\ns\nsa\nsaj\nsapo\nse\nsecila\nsepse\nsh\nshih\nshume\nsi\nsic\nsikur\nsipas\nsiper\nsone\nt\nta\ntani\nte\ntej\ntek\nteper\ntere\nti\ntij\ntilla\ntille\ntjera\ntjeret\ntjeter\ntjetren\nto\ntone\nty\ntyre\nu\nua\nune\nvazhdimisht\nvend\nvet\nveta\nvete\nvetem\nveten\nvetes\nvjen\nyne\nzakonisht\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/is/__init__.py----------------------------------------
spacy.lang.is.__init__.Icelandic(Language)
spacy.lang.is.__init__.IcelandicDefaults(BaseDefaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/is/stop_words.py----------------------------------------
A:spacy.lang.is.stop_words.STOP_WORDS->set('\nafhverju\naftan\naftur\nafþví\naldrei\nallir\nallt\nalveg\nannað\nannars\nbara\ndag\neða\neftir\neiga\neinhver\neinhverjir\neinhvers\neins\neinu\neitthvað\nekkert\nekki\nennþá\neru\nfara\nfer\nfinna\nfjöldi\nfólk\nframan\nfrá\nfrekar\nfyrir\ngegnum\ngeta\ngetur\ngmg\ngott\nhann\nhafa\nhef\nhefur\nheyra\nhér\nhérna\nhjá\nhún\nhvað\nhvar\nhver\nhverjir\nhverjum\nhvernig\nhvor\nhvort\nhægt\nimg\ninn\nkannski\nkoma\nlíka\nlol\nmaður\nmátt\nmér\nmeð\nmega\nmeira\nmig\nmikið\nminna\nminni\nmissa\nmjög\nnei\nniður\nnúna\noft\nokkar\nokkur\npóst\npóstur\nrofl\nsaman\nsem\nsér\nsig\nsinni\nsíðan\nsjá\nsmá\nsmátt\nspurja\nspyrja\nstaðar\nstórt\nsvo\nsvona\nsælir\nsæll\ntaka\ntakk\ntil\ntilvitnun\ntitlar\nupp\nvar\nvel\nvelkomin\nvelkominn\nvera\nverður\nverið\nvel\nvið\nvil\nvilja\nvill\nvita\nværi\nyfir\nykkar\nþað\nþakka\nþakkir\nþannig\nþað\nþar\nþarf\nþau\nþeim\nþeir\nþeirra\nþeirra\nþegar\nþess\nþessa\nþessi\nþessu\nþessum\nþetta\nþér\nþið\nþinn\nþitt\nþín\nþráð\nþráður\nþví\nþær\nætti\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ca/__init__.py----------------------------------------
spacy.lang.ca.__init__.Catalan(Language)
spacy.lang.ca.__init__.CatalanDefaults(BaseDefaults)
spacy.lang.ca.__init__.make_lemmatizer(nlp:Language,model:Optional[Model],name:str,mode:str,overwrite:bool,scorer:Optional[Callable])


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ca/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ca/lemmatizer.py----------------------------------------
A:spacy.lang.ca.lemmatizer.univ_pos->token.pos_.lower()
A:spacy.lang.ca.lemmatizer.index_table->self.lookups.get_table('lemma_index', {})
A:spacy.lang.ca.lemmatizer.exc_table->self.lookups.get_table('lemma_exc', {})
A:spacy.lang.ca.lemmatizer.rules_table->self.lookups.get_table('lemma_rules', {})
A:spacy.lang.ca.lemmatizer.lookup_table->self.lookups.get_table('lemma_lookup', {})
A:spacy.lang.ca.lemmatizer.index->self.lookups.get_table('lemma_index', {}).get(univ_pos, {})
A:spacy.lang.ca.lemmatizer.exceptions->self.lookups.get_table('lemma_exc', {}).get(univ_pos, {})
A:spacy.lang.ca.lemmatizer.rules->self.lookups.get_table('lemma_rules', {}).get(univ_pos, [])
A:spacy.lang.ca.lemmatizer.string->string.lower().lower()
A:spacy.lang.ca.lemmatizer.forms->list(dict.fromkeys(forms))
spacy.lang.ca.CatalanLemmatizer(Lemmatizer)
spacy.lang.ca.CatalanLemmatizer.get_lookups_config(cls,mode:str)->Tuple[List[str], List[str]]
spacy.lang.ca.CatalanLemmatizer.rule_lemmatize(self,token:Token)->List[str]
spacy.lang.ca.lemmatizer.CatalanLemmatizer(Lemmatizer)
spacy.lang.ca.lemmatizer.CatalanLemmatizer.get_lookups_config(cls,mode:str)->Tuple[List[str], List[str]]
spacy.lang.ca.lemmatizer.CatalanLemmatizer.rule_lemmatize(self,token:Token)->List[str]


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ca/lex_attrs.py----------------------------------------
A:spacy.lang.ca.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.ca.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.ca.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ca/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.ca.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ca/syntax_iterators.py----------------------------------------
A:spacy.lang.ca.syntax_iterators.np_label->doc.vocab.strings.add('NP')
spacy.lang.ca.syntax_iterators.noun_chunks(doclike:Union[Doc,Span])->Iterator[Tuple[int, int, int]]


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ca/stop_words.py----------------------------------------
A:spacy.lang.ca.stop_words.STOP_WORDS->set("\na abans ací ah així això al aleshores algun alguna algunes alguns alhora allà allí allò\nals altra altre altres amb ambdues ambdós anar ans apa aquell aquella aquelles aquells\naquest aquesta aquestes aquests aquí\n\nbaix bastant bé\n\ncada cadascuna cadascunes cadascuns cadascú com consegueixo conseguim conseguir\nconsigueix consigueixen consigueixes contra\n\nd'un d'una d'unes d'uns dalt de del dels des des de després dins dintre donat doncs durant\n\ne eh el elles ells els em en encara ens entre era erem eren eres es esta estan estat\nestava estaven estem esteu estic està estàvem estàveu et etc ets érem éreu és éssent\n\nfa faig fan fas fem fer feu fi fins fora\n\ngairebé\n\nha han has haver havia he hem heu hi ho\n\ni igual iguals inclòs\n\nja jo\n\nl'hi la les li li'n llarg llavors\n\nm'he ma mal malgrat mateix mateixa mateixes mateixos me mentre meu meus meva\nmeves mode molt molta moltes molts mon mons més\n\nn'he n'hi ne ni no nogensmenys només nosaltres nostra nostre nostres\n\no oh oi on\n\npas pel pels per per que perquè però poc poca pocs podem poden poder\npodeu poques potser primer propi puc\n\nqual quals quan quant que quelcom qui quin quina quines quins què\n\ns'ha s'han sa sabem saben saber sabeu sap saps semblant semblants sense ser ses\nseu seus seva seves si sobre sobretot soc solament sols som son sons sota sou sóc són\n\nt'ha t'han t'he ta tal també tampoc tan tant tanta tantes te tene tenim tenir teniu\nteu teus teva teves tinc ton tons tot tota totes tots\n\nun una unes uns us últim ús\n\nva vaig vam van vas veu vosaltres vostra vostre vostres\n\n".split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ca/punctuation.py----------------------------------------
A:spacy.lang.ca.punctuation.ELISION->" ' ’ ".strip().replace(' ', '').replace('\n', '')
A:spacy.lang.ca.punctuation._units->char_classes._units.replace('% ', '').replace('% ', '')
A:spacy.lang.ca.punctuation.UNITS->merge_chars(_units)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/nb/__init__.py----------------------------------------
spacy.lang.nb.__init__.Norwegian(Language)
spacy.lang.nb.__init__.NorwegianDefaults(BaseDefaults)
spacy.lang.nb.__init__.make_lemmatizer(nlp:Language,model:Optional[Model],name:str,mode:str,overwrite:bool,scorer:Optional[Callable])


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/nb/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/nb/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.nb.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/nb/syntax_iterators.py----------------------------------------
A:spacy.lang.nb.syntax_iterators.conj->doc.vocab.strings.add('conj')
A:spacy.lang.nb.syntax_iterators.np_label->doc.vocab.strings.add('NP')
spacy.lang.nb.syntax_iterators.noun_chunks(doclike:Union[Doc,Span])->Iterator[Tuple[int, int, int]]


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/nb/stop_words.py----------------------------------------
A:spacy.lang.nb.stop_words.STOP_WORDS->set('\nalle allerede alt and andre annen annet at av\n\nbak bare bedre beste blant ble bli blir blitt bris by både\n\nda dag de del dem den denne der dermed det dette disse du\n\neller en enn er et ett etter\n\nfem fikk fire fjor flere folk for fortsatt fra fram\nfunnet få får fått før først første\n\ngang gi gikk gjennom gjorde gjort gjør gjøre god godt grunn gå går\n\nha hadde ham han hans har hele helt henne hennes her hun\n\ni ifølge igjen ikke ingen inn\n\nja jeg\n\nkamp kampen kan kl klart kom komme kommer kontakt kort kroner kunne kveld\n\nla laget land landet langt leder ligger like litt løpet\n\nman mange med meg mellom men mener mennesker mens mer mot mye må mål måtte\n\nned neste noe noen nok ny nye nå når\n\nog også om opp opplyser oss over\n\npersoner plass poeng på\n\nrunde rundt\n\nsa saken samme sammen samtidig satt se seg seks selv senere ser sett\nsiden sier sin sine siste sitt skal skriver skulle slik som sted stedet stor\nstore står svært så\n\nta tatt tid tidligere til tilbake tillegg tok tror\n\nunder ut uten utenfor\n\nvant var ved veldig vi videre viktig vil ville viser vår være vært\n\nå år\n\nønsker\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/nb/punctuation.py----------------------------------------
A:spacy.lang.nb.punctuation._quotes->char_classes.CONCAT_QUOTES.replace("'", '')


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ky/__init__.py----------------------------------------
spacy.lang.ky.__init__.Kyrgyz(Language)
spacy.lang.ky.__init__.KyrgyzDefaults(BaseDefaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ky/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ky/lex_attrs.py----------------------------------------
A:spacy.lang.ky.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.ky.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.ky.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ky/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.ky.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ky/stop_words.py----------------------------------------
A:spacy.lang.ky.stop_words.STOP_WORDS->set('\nага адам айтты айтымында айтып ал алар\nалардын алган алуу алып анда андан аны\nанын ар\n\nбар басма баш башка башкы башчысы берген\nбиз билдирген билдирди бир биринчи бирок\nбишкек болгон болот болсо болуп боюнча\nбуга бул\n\nгана\n\nда дагы деген деди деп\n\nжана жатат жаткан жаңы же жогорку жок жол\nжолу\n\nкабыл калган кандай карата каршы катары\nкелген керек кийин кол кылмыш кыргыз\nкүнү көп\n\nмаалымат мамлекеттик мен менен миң\nмурдагы мыйзам мындай мүмкүн\n\nошол ошондой\n\nсүрөт сөз\n\nтарабынан турган тууралуу\n\nукук учурда\n\nчейин чек\n\nэкенин эки эл эле эмес эми эч\n\nүч үчүн\n\nөз\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ky/punctuation.py----------------------------------------
A:spacy.lang.ky.punctuation._hyphens_no_dash->char_classes.HYPHENS.replace('-', '').strip('|').replace('||', '')


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/xx/__init__.py----------------------------------------
spacy.lang.xx.__init__.MultiLanguage(Language)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/xx/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/bn/__init__.py----------------------------------------
spacy.lang.bn.__init__.Bengali(Language)
spacy.lang.bn.__init__.BengaliDefaults(BaseDefaults)
spacy.lang.bn.__init__.make_lemmatizer(nlp:Language,model:Optional[Model],name:str,mode:str,overwrite:bool,scorer:Optional[Callable])


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/bn/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/bn/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.bn.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/bn/stop_words.py----------------------------------------
A:spacy.lang.bn.stop_words.STOP_WORDS->set('\nঅতএব অথচ অথবা অনুযায়ী অনেক অনেকে অনেকেই অন্তত  অবধি অবশ্য অর্থাৎ অন্য অনুযায়ী অর্ধভাগে\nআগামী আগে আগেই আছে আজ আদ্যভাগে আপনার আপনি আবার আমরা আমাকে আমাদের আমার  আমি আর আরও\nইত্যাদি ইহা\nউচিত উনি উপর উপরে উত্তর\nএ এঁদের এঁরা এই এক একই একজন একটা একটি  একবার একে এখন এখনও এখানে এখানেই এটা এসো\nএটাই এটি এত এতটাই এতে এদের এবং এবার এমন এমনি এমনকি এর এরা এলো এস এসে\nঐ\nও ওঁদের ওঁর ওঁরা ওই ওকে ওখানে ওদের ওর ওরা\nকখনও কত কথা কবে কয়েক  কয়েকটি করছে করছেন করতে  করবে করবেন করলে কয়েক  কয়েকটি করিয়ে করিয়া করায়\nকরলেন করা করাই করায় করার করি করিতে করিয়া করিয়ে করে করেই করেছিলেন করেছে করেছেন করেন কাউকে\nকাছ কাছে কাজ কাজে কারও কারণ কি কিংবা কিছু কিছুই কিন্তু কী কে কেউ কেউই কেন কোন কোনও কোনো কেমনে কোটি\nক্ষেত্রে খুব\nগিয়ে গিয়েছে গুলি গেছে গেল গেলে গোটা গিয়ে গিয়েছে\nচলে চান চায় চেয়ে চায় চেয়ে চার চালু চেষ্টা\nছাড়া ছাড়াও ছিল ছিলেন ছাড়া ছাড়াও\nজন জনকে জনের জন্য জন্যে জানতে জানা জানানো জানায়  জানিয়ে  জানিয়েছে জানায় জাানিয়ে জানিয়েছে\nটি\nঠিক\nতখন তত তথা তবু তবে তা তাঁকে তাঁদের তাঁর তাঁরা তাঁহারা তাই তাও তাকে তাতে তাদের তার তারপর তারা তারই তাহলে তাহা তাহাতে তাহার তিনই\nতিনি তিনিও তুমি তুলে তেমন তো তোমার তুই তোরা তোর তোমাদের তোদের\nথাকবে থাকবেন থাকা থাকায় থাকে থাকেন থেকে থেকেই  থেকেও থাকায়\nদিকে দিতে দিয়ে দিয়েছে দিয়েছেন দিলেন দিয়ে দু  দুটি  দুটো দেওয়া দেওয়ার দেখতে দেখা দেখে দেন দেয়  দেশের\nদ্বারা দিয়েছে দিয়েছেন দেয় দেওয়া দেওয়ার দিন দুই\nধরা ধরে\nনয় না নাই নাকি নাগাদ নানা নিজে নিজেই নিজেদের নিজের নিতে নিয়ে নিয়ে নেই নেওয়া নেওয়ার নয় নতুন\nপক্ষে পর পরে পরেই পরেও পর্যন্ত পাওয়া পারি পারে পারেন পেয়ে প্রতি প্রভৃতি প্রায় পাওয়া পেয়ে প্রায় পাঁচ প্রথম প্রাথমিক\nফলে ফিরে ফের\nবছর বদলে বরং বলতে বলল বললেন বলা বলে বলেছেন বলেন  বসে বহু বা বাদে বার বিনা বিভিন্ন বিশেষ বিষয়টি বেশ ব্যবহার ব্যাপারে বক্তব্য বন বেশি\nভাবে  ভাবেই\nমত মতো মতোই মধ্যভাগে মধ্যে মধ্যেই  মধ্যেও মনে মাত্র মাধ্যমে মানুষ মানুষের মোট মোটেই মোদের মোর\nযখন যত যতটা যথেষ্ট যদি যদিও যা যাঁর যাঁরা যাওয়া  যাওয়ার যাকে যাচ্ছে যাতে যাদের যান যাবে যায় যার  যারা যায় যিনি যে যেখানে যেতে যেন\nযেমন\nরকম রয়েছে রাখা রেখে রয়েছে\nলক্ষ\nশুধু শুরু\nসাধারণ সামনে সঙ্গে সঙ্গেও সব সবার সমস্ত সম্প্রতি সময় সহ সহিত সাথে সুতরাং সে  সেই সেখান সেখানে  সেটা সেটাই সেটাও সেটি স্পষ্ট স্বয়ং\nহইতে হইবে হইয়া হওয়া হওয়ায় হওয়ার হচ্ছে হত হতে হতেই হন হবে হবেন হয় হয়তো হয়নি হয়ে হয়েই হয়েছিল হয়েছে হাজার\nহয়েছেন হল হলে হলেই হলেও হলো হিসাবে হিসেবে হৈলে হোক হয় হয়ে হয়েছে হৈতে হইয়া  হয়েছিল হয়েছেন হয়নি হয়েই হয়তো হওয়া হওয়ার হওয়ায়\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/bn/punctuation.py----------------------------------------
A:spacy.lang.bn.punctuation._quotes->char_classes.CONCAT_QUOTES.replace("'", '')


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/bg/__init__.py----------------------------------------
A:spacy.lang.bg.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.bg.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS, TOKENIZER_EXCEPTIONS)
spacy.lang.bg.__init__.Bulgarian(Language)
spacy.lang.bg.__init__.BulgarianDefaults(BaseDefaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/bg/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/bg/lex_attrs.py----------------------------------------
A:spacy.lang.bg.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.bg.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.bg.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/bg/tokenizer_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/bg/stop_words.py----------------------------------------
A:spacy.lang.bg.stop_words.STOP_WORDS->set('\nа автентичен аз ако ала\n\nбе без беше би бивш бивша бившо бивши бил била били било благодаря близо бъдат\nбъде бъда бяха\n\nв вас ваш ваша вашата вашият вероятно вече взема ви вие винаги внимава време все \nвсеки всички вместо всичко вследствие всъщност всяка втори във въпреки върху\nвътре веднъж \n\nг ги главен главна главно глас го годно година години годишен\n\nд да дали далеч далече два двама двамата две двете ден днес дни до добра добре \nдобро добър достатъчно докато докога дори досега доста друг друга другаде други\n\nе евтин едва един една еднаква еднакви еднакъв едно екип ето\n\nживот жив\n\nза здравей здрасти знае зная забавям зад зададени заедно заради засега заспал \nзатова запазва започвам защо защото завинаги\n\nи из или им има имат иска искам използвайки изглежда изглеждаше изглеждайки \nизвън имайки\n\nй йо \n\nказа казва казвайки казвам как каква какво както какъв като кога кауза каузи \nкогато когото което които кой който колко която къде където към край кратък \nкръгъл\n\nлесен лесно ли летя летиш летим лош\n\nм май малко макар малцина междувременно минус ме между мек мен месец ми мис \nмисля много мнозина мога могат може мой можем мокър моля момента му\n\nн на над назад най наш навсякъде навътре нагоре направи напред надолу наистина \nнапример наопаки наполовина напоследък нека независимо нас насам наскоро \nнастрана необходимо него негов нещо нея ни ние никой нито нищо но нов някак нова \nнови новина някои някой някога някъде няколко няма\n\nо обаче около описан опитах опитва опитвайки опитвам определен определено освен \nобикновено осигурява обратно означава особен особено от ох отвъд отгоре отдолу \nотново отива отивам отидох отсега отделно отколкото откъдето очевидно оттам \nотносно още\n\nп пак по повече повечето под поне просто пряко поради после последен последно \nпосочен почти прави прав прави правя пред преди през при пък първата първи първо \nпът пъти плюс\n\nравен равна различен различни разумен разумно\n\nс са сам само себе сериозно сигурен сигурно се сега си син скоро скорошен след \nследващ следващия следва следното следователно случва сме смях собствен \nсравнително смея според сред става срещу съвсем съдържа съдържащ съжалявам \nсъответен съответно сте съм със също\n\nт така техен техни такива такъв твърде там трета твой те тези ти то това \nтогава този той търси толкова точно три трябва тук тъй тя тях\n\nу утре ужасно употреба успоредно уточнен уточняване\n\nхаресва харесали хиляди\n\nч часа ценя цяло цялостен че често чрез чудя\n\nще щеше щом щяха\n\nюмрук\n\nя як\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/lt/__init__.py----------------------------------------
spacy.lang.lt.__init__.Lithuanian(Language)
spacy.lang.lt.__init__.LithuanianDefaults(BaseDefaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/lt/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/lt/lex_attrs.py----------------------------------------
A:spacy.lang.lt.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.lt.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.lt.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/lt/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.lt.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(mod_base_exceptions, _exc)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/lt/stop_words.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/lt/punctuation.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/da/__init__.py----------------------------------------
spacy.lang.da.__init__.Danish(Language)
spacy.lang.da.__init__.DanishDefaults(BaseDefaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/da/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/da/lex_attrs.py----------------------------------------
A:spacy.lang.da.lex_attrs._num_words->'nul\nen et to tre fire fem seks syv otte ni ti\nelleve tolv tretten fjorten femten seksten sytten atten nitten tyve\nenogtyve toogtyve treogtyve fireogtyve femogtyve seksogtyve syvogtyve otteogtyve niogtyve tredive\nenogtredive toogtredive treogtredive fireogtredive femogtredive seksogtredive syvogtredive otteogtredive niogtredive fyrre\nenogfyrre toogfyrre treogfyrre fireogfyrre femgogfyrre seksogfyrre syvogfyrre otteogfyrre niogfyrre halvtreds\nenoghalvtreds tooghalvtreds treoghalvtreds fireoghalvtreds femoghalvtreds seksoghalvtreds syvoghalvtreds otteoghalvtreds nioghalvtreds tres\nenogtres toogtres treogtres fireogtres femogtres seksogtres syvogtres otteogtres niogtres halvfjerds\nenoghalvfjerds tooghalvfjerds treoghalvfjerds fireoghalvfjerds femoghalvfjerds seksoghalvfjerds syvoghalvfjerds otteoghalvfjerds nioghalvfjerds firs\nenogfirs toogfirs treogfirs fireogfirs femogfirs seksogfirs syvogfirs otteogfirs niogfirs halvfems\nenoghalvfems tooghalvfems treoghalvfems fireoghalvfems femoghalvfems seksoghalvfems syvoghalvfems otteoghalvfems nioghalvfems hundrede\nmillion milliard billion billiard trillion trilliard\n'.split()
A:spacy.lang.da.lex_attrs._ordinal_words->'nulte\nførste anden tredje fjerde femte sjette syvende ottende niende tiende\nelfte tolvte trettende fjortende femtende sekstende syttende attende nittende tyvende\nenogtyvende toogtyvende treogtyvende fireogtyvende femogtyvende seksogtyvende syvogtyvende otteogtyvende niogtyvende tredivte enogtredivte toogtredivte treogtredivte fireogtredivte femogtredivte seksogtredivte syvogtredivte otteogtredivte niogtredivte fyrretyvende\nenogfyrretyvende toogfyrretyvende treogfyrretyvende fireogfyrretyvende femogfyrretyvende seksogfyrretyvende syvogfyrretyvende otteogfyrretyvende niogfyrretyvende halvtredsindstyvende enoghalvtredsindstyvende\ntooghalvtredsindstyvende treoghalvtredsindstyvende fireoghalvtredsindstyvende femoghalvtredsindstyvende seksoghalvtredsindstyvende syvoghalvtredsindstyvende otteoghalvtredsindstyvende nioghalvtredsindstyvende\ntresindstyvende enogtresindstyvende toogtresindstyvende treogtresindstyvende fireogtresindstyvende femogtresindstyvende seksogtresindstyvende syvogtresindstyvende otteogtresindstyvende niogtresindstyvende halvfjerdsindstyvende\nenoghalvfjerdsindstyvende tooghalvfjerdsindstyvende treoghalvfjerdsindstyvende fireoghalvfjerdsindstyvende femoghalvfjerdsindstyvende seksoghalvfjerdsindstyvende syvoghalvfjerdsindstyvende otteoghalvfjerdsindstyvende nioghalvfjerdsindstyvende firsindstyvende\nenogfirsindstyvende toogfirsindstyvende treogfirsindstyvende fireogfirsindstyvende femogfirsindstyvende seksogfirsindstyvende syvogfirsindstyvende otteogfirsindstyvende niogfirsindstyvende halvfemsindstyvende\nenoghalvfemsindstyvende tooghalvfemsindstyvende treoghalvfemsindstyvende fireoghalvfemsindstyvende femoghalvfemsindstyvende seksoghalvfemsindstyvende syvoghalvfemsindstyvende otteoghalvfemsindstyvende nioghalvfemsindstyvende\n'.split()
A:spacy.lang.da.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.da.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.da.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/da/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.da.tokenizer_exceptions.capitalized->orth.capitalize()
A:spacy.lang.da.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/da/syntax_iterators.py----------------------------------------
A:spacy.lang.da.syntax_iterators.right->get_right_bound(doc, tok)
A:spacy.lang.da.syntax_iterators.np_label->doc.vocab.strings.add('NP')
A:spacy.lang.da.syntax_iterators.(left, right)->get_bounds(doc, token)
spacy.lang.da.syntax_iterators.noun_chunks(doclike:Union[Doc,Span])->Iterator[Tuple[int, int, int]]


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/da/stop_words.py----------------------------------------
A:spacy.lang.da.stop_words.STOP_WORDS->set('\naf aldrig alene alle allerede alligevel alt altid anden andet andre at\n\nbag begge blandt blev blive bliver burde bør\n\nda de dem den denne dens der derefter deres derfor derfra deri dermed derpå derved det dette dig din dine disse dog du\n\nefter egen eller ellers en end endnu ene eneste enhver ens enten er et\n\nflere flest fleste for foran fordi forrige fra få før først\n\ngennem gjorde gjort god gør gøre gørende\n\nham han hans har havde have hel heller hen hende hendes henover her herefter heri hermed herpå hun hvad hvem hver hvilke hvilken hvilkes hvis hvor hvordan hvorefter hvorfor hvorfra hvorhen hvori hvorimod hvornår hvorved\n\ni igen igennem ikke imellem imens imod ind indtil ingen intet\n\njeg jer jeres jo\n\nkan kom kommer kun kunne\n\nlad langs lav lave lavet lidt lige ligesom lille længere\n\nman mange med meget mellem men mens mere mest mig min mindre mindst mine mit må måske\n\nned nemlig nogen nogensinde noget nogle nok nu ny nyt nær næste næsten\n\nog også om omkring op os over overalt\n\npå\n\nsamme sammen selv selvom senere ses siden sig sige skal skulle som stadig synes syntes så sådan således\n\ntemmelig tidligere til tilbage tit\n\nud uden udover under undtagen\n\nvar ved vi via vil ville vore vores vær være været\n\nøvrigt\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/da/punctuation.py----------------------------------------
A:spacy.lang.da.punctuation._quotes->char_classes.CONCAT_QUOTES.replace("'", '')


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ti/__init__.py----------------------------------------
A:spacy.lang.ti.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.ti.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS, TOKENIZER_EXCEPTIONS)
spacy.lang.ti.__init__.Tigrinya(Language)
spacy.lang.ti.__init__.TigrinyaDefaults(BaseDefaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ti/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ti/lex_attrs.py----------------------------------------
A:spacy.lang.ti.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.ti.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
A:spacy.lang.ti.lex_attrs.text_lower->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').lower()
spacy.lang.ti.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ti/tokenizer_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ti/stop_words.py----------------------------------------
A:spacy.lang.ti.stop_words.STOP_WORDS->set("\n'ምበር 'ሞ 'ቲ 'ታ 'ኳ 'ውን 'ዚ 'የ 'ዩ 'ያ 'ዮም 'ዮን\nልዕሊ ሒዙ ሒዛ ሕጂ መበል መን መንጎ መጠን ማለት ምስ ምባል\nምእንቲ ምኽንያቱ ምኽንያት ምዃኑ ምዃንና ምዃኖም\nስለ ስለዚ ስለዝበላ ሽዑ ቅድሚ በለ በቲ በዚ ብምባል ብተወሳኺ ብኸመይ\nብዘይ ብዘይካ ብዙሕ ብዛዕባ ብፍላይ ተባሂሉ ነበረ ነቲ ነታ ነቶም\nነዚ ነይሩ ነገራት ነገር ናብ ናብቲ ናትኩም ናትኪ ናትካ ናትክን\nናይ ናይቲ ንሕና ንሱ ንሳ ንሳቶም ንስኺ ንስኻ ንስኻትኩም ንስኻትክን ንዓይ\nኢለ ኢሉ ኢላ ኢልካ ኢሎም ኢና ኢኻ ኢዩ ኣለኹ\nኣለዉ ኣለዎ ኣሎ ኣብ ኣብቲ ኣብታ ኣብኡ ኣብዚ ኣነ ኣዝዩ ኣይኮነን ኣይኰነን\nእምበር እሞ እተን እቲ እታ እቶም እንተ እንተሎ\nኣላ እንተኾነ እንታይ እንከሎ እኳ እዋን እውን እዚ እዛ እዞም\nእየ እየን እዩ እያ እዮም\nከሎ ከመይ ከም ከምቲ ከምኡ ከምዘሎ\nከምዚ ከኣ ኩሉ ካልእ ካልኦት ካብ ካብቲ ካብቶም ክሳብ ክሳዕ ክብል\nክንደይ ክንዲ ክኸውን ኮይኑ ኰይኑ ኵሉ ኸም ኸኣ ወይ\nዋላ ዘለና ዘለዉ ዘለዋ ዘለዎ ዘለዎም ዘላ ዘሎ ዘይብሉ  \nዝርከብ ዝበሃል ዝበለ ዝብል ዝተባህለ ዝተኻየደ ዝተፈላለየ ዝተፈላለዩ\nዝነበረ ዝነበረት ዝነበሩ ዝካየድ ዝኸውን ዝኽእል ዝኾነ ዝዀነ\nየለን ይቕረብ ይብል ይኸውን ይኹን ይኽእል ደኣ ድሕሪ ድማ\nገለ ገሊጹ ገና ገይሩ ግና ግን ጥራይ\n".split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ti/punctuation.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/uk/__init__.py----------------------------------------
spacy.lang.uk.__init__.Ukrainian(Language)
spacy.lang.uk.__init__.UkrainianDefaults(BaseDefaults)
spacy.lang.uk.__init__.make_lemmatizer(nlp:Language,model:Optional[Model],name:str,mode:str,overwrite:bool,scorer:Optional[Callable])


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/uk/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/uk/lemmatizer.py----------------------------------------
A:spacy.lang.uk.lemmatizer.self._morph->MorphAnalyzer(lang='uk')
spacy.lang.uk.UkrainianLemmatizer(self,vocab:Vocab,model:Optional[Model],name:str='lemmatizer',*,mode:str='pymorphy3',overwrite:bool=False,scorer:Optional[Callable]=lemmatizer_score)
spacy.lang.uk.lemmatizer.UkrainianLemmatizer(self,vocab:Vocab,model:Optional[Model],name:str='lemmatizer',*,mode:str='pymorphy3',overwrite:bool=False,scorer:Optional[Callable]=lemmatizer_score)
spacy.lang.uk.lemmatizer.UkrainianLemmatizer.__init__(self,vocab:Vocab,model:Optional[Model],name:str='lemmatizer',*,mode:str='pymorphy3',overwrite:bool=False,scorer:Optional[Callable]=lemmatizer_score)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/uk/lex_attrs.py----------------------------------------
A:spacy.lang.uk.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.uk.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.uk.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/uk/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.uk.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/uk/stop_words.py----------------------------------------
A:spacy.lang.uk.stop_words.STOP_WORDS->set("а\nабо\nадже\nаж\nале\nалло\nб\nбагато\nбез\nбезперервно\nби\nбільш\nбільше\nбіля\nблизько\nбо\nбув\nбуває\nбуде\nбудемо\nбудете\nбудеш\nбуду\nбудуть\nбудь\nбула\nбули\nбуло\nбути\nв\nвам\nвами\nвас\nваш\nваша\nваше\nвашим\nвашими\nваших\nваші\nвашій\nвашого\nвашої\nвашому\nвашою\nвашу\nвгорі\nвгору\nвдалині\nвесь\nвже\nви\nвід\nвідсотків\nвін\nвісім\nвісімнадцятий\nвісімнадцять\nвниз\nвнизу\nвона\nвони\nвоно\nвосьмий\nвсе\nвсею\nвсі\nвсім\nвсіх\nвсього\nвсьому\nвсю\nвся\nвтім\nг\nгеть\nговорив\nговорить\nдавно\nдалеко\nдалі\nдарма\nдва\nдвадцятий\nдвадцять\nдванадцятий\nдванадцять\nдві\nдвох\nде\nдев'ятий\nдев'ятнадцятий\nдев'ятнадцять\nдев'ять\nдекілька\nдень\nдесятий\nдесять\nдійсно\nдля\nдня\nдо\nдобре\nдовго\nдоки\nдосить\nдругий\nдуже\nдякую\nе\nє\nж\nже\nз\nза\nзавжди\nзазвичай\nзанадто\nзараз\nзате\nзвичайно\nзвідси\nзвідусіль\nздається\nзі\nзначить\nзнову\nзовсім\nі\nіз\nїї\nїй\nїм\nіноді\nінша\nінше\nінший\nінших\nінші\nїх\nй\nйого\nйому\nкаже\nким\nкілька\nкого\nкожен\nкожна\nкожне\nкожні\nколи\nкому\nкраще\nкрім\nкуди\nласка\nледве\nлише\nм\nмає\nмайже\nмало\nмати\nмене\nмені\nменш\nменше\nми\nмимо\nміг\nміж\nмій\nмільйонів\nмною\nмого\nмогти\nмоє\nмоєї\nмоєму\nмоєю\nможе\nможна\nможно\nможуть\nмої\nмоїй\nмоїм\nмоїми\nмоїх\nмою\nмоя\nна\nнавіть\nнавіщо\nнавколо\nнавкруги\nнагорі\nнад\nназад\nнайбільш\nнам\nнами\nнарешті\nнас\nнаш\nнаша\nнаше\nнашим\nнашими\nнаших\nнаші\nнашій\nнашого\nнашої\nнашому\nнашою\nнашу\nне\nнебагато\nнебудь\nнедалеко\nнеї\nнемає\nнерідко\nнещодавно\nнею\nнибудь\nнижче\nнизько\nним\nними\nних\nні\nніби\nніж\nній\nніколи\nнікуди\nнім\nнічого\nну\nнього\nньому\nо\nобидва\nобоє\nодин\nодинадцятий\nодинадцять\nоднак\nоднієї\nодній\nодного\nозначає\nокрім\nон\nособливо\nось\nп'ятий\nп'ятнадцятий\nп'ятнадцять\nп'ять\nперед\nперший\nпід\nпізніше\nпір\nпісля\nпо\nповинно\nподів\nпоки\nпора\nпоруч\nпосеред\nпотім\nпотрібно\nпочала\nпочатку\nпри\nпро\nпросто\nпроте\nпроти\nраз\nразу\nраніше\nрано\nраптом\nрік\nроки\nроків\nроку\nроці\nсам\nсама\nсаме\nсамим\nсамими\nсамих\nсамі\nсамій\nсамо\nсамого\nсамому\nсаму\nсвого\nсвоє\nсвоєї\nсвої\nсвоїй\nсвоїх\nсвою\nсебе\nсих\nсім\nсімнадцятий\nсімнадцять\nсказав\nсказала\nсказати\nскільки\nскрізь\nсобі\nсобою\nспасибі\nспочатку\nсправ\nстав\nсуть\nсьогодні\nсьомий\nт\nта\nтак\nтака\nтаке\nтакий\nтакі\nтакож\nтам\nтвій\nтвого\nтвоє\nтвоєї\nтвоєму\nтвоєю\nтвої\nтвоїй\nтвоїм\nтвоїми\nтвоїх\nтвою\nтвоя\nте\nтебе\nтеж\nтепер\nти\nтим\nтими\nтисяч\nтих\nті\nтієї\nтією\nтій\nтільки\nтім\nто\nтобі\nтобою\nтого\nтоді\nтой\nтому\nтою\nтреба\nтретій\nтри\nтринадцятий\nтринадцять\nтрохи\nту\nтуди\nтут\nу\nувесь\nуміти\nусе\nусі\nусім\nусіма\nусіх\nусього\nусьому\nусю\nусюди\nуся\nхіба\nхотіти\nхоч\nхоча\nхочеш\nхто\nце\nцей\nцим\nцими\nцих\nці\nцієї\nцій\nцього\nцьому\nцю\nця\nчас\nчастіше\nчасто\nчасу\nчерез\nчетвертий\nчи\nчиє\nчиєї\nчиєму\nчиї\nчиїй\nчиїм\nчиїми\nчиїх\nчий\nчийого\nчийому\nчим\nчисленна\nчисленне\nчисленний\nчисленні\nчию\nчия\nчого\nчому\nчотири\nчотирнадцятий\nчотирнадцять\nшістнадцятий\nшістнадцять\nшість\nшостий\nще\nщо\nщоб\nщодо\nщось\nя\nяк\nяка\nякий\nяких\nякі\nякій\nякого\nякої\nякщо".split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/hi/__init__.py----------------------------------------
spacy.lang.hi.__init__.Hindi(Language)
spacy.lang.hi.__init__.HindiDefaults(BaseDefaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/hi/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/hi/lex_attrs.py----------------------------------------
A:spacy.lang.hi.lex_attrs.length->len(suffix_group[0])
A:spacy.lang.hi.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.hi.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.hi.lex_attrs.like_num(text)
spacy.lang.hi.lex_attrs.norm(string)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/hi/stop_words.py----------------------------------------
A:spacy.lang.hi.stop_words.STOP_WORDS->set('\nअंदर\nअत\nअदि\nअप\nअपना\nअपनि\nअपनी\nअपने\nअभि\nअभी\nअंदर\nआदि\nआप\nअगर\nइंहिं\nइंहें\nइंहों\nइतयादि\nइत्यादि\nइन\nइनका\nइन्हीं\nइन्हें\nइन्हों\nइस\nइसका\nइसकि\nइसकी\nइसके\nइसमें\nइसि\nइसी\nइसे\nउंहिं\nउंहें\nउंहों\nउन\nउनका\nउनकि\nउनकी\nउनके\nउनको\nउन्हीं\nउन्हें\nउन्हों\nउस\nउसके\nउसि\nउसी\nउसे\nएक\nएवं\nएस\nएसे\nऐसे\nओर\nऔर\nकइ\nकई\nकर\nकरता\nकरते\nकरना\nकरने\nकरें\nकहते\nकहा\nका\nकाफि\nकाफ़ी\nकि\nकिंहें\nकिंहों\nकितना\nकिन्हें\nकिन्हों\nकिया\nकिर\nकिस\nकिसि\nकिसी\nकिसे\nकी\nकुछ\nकुल\nके\nको\nकोइ\nकोई\nकोन\nकोनसा\nकौन\nकौनसा\nगया\nघर\nजब\nजहाँ\nजहां\nजा\nजिंहें\nजिंहों\nजितना\nजिधर\nजिन\nजिन्हें\nजिन्हों\nजिस\nजिसे\nजीधर\nजेसा\nजेसे\nजैसा\nजैसे\nजो\nतक\nतब\nतरह\nतिंहें\nतिंहों\nतिन\nतिन्हें\nतिन्हों\nतिस\nतिसे\nतो\nथा\nथि\nथी\nथे\nदबारा\nदवारा\nदिया\nदुसरा\nदुसरे\nदूसरे\nदो\nद्वारा\nन\nनहिं\nनहीं\nना\nनिचे\nनिहायत\nनीचे\nने\nपर\nपहले\nपुरा\nपूरा\nपे\nफिर\nबनि\nबनी\nबहि\nबही\nबहुत\nबाद\nबाला\nबिलकुल\nभि\nभितर\nभी\nभीतर\nमगर\nमानो\nमे\nमें\nमैं\nमुझको\nमेरा\nयदि\nयह\nयहाँ\nयहां\nयहि\nयही\nया\nयिह\nये\nरखें\nरवासा\nरहा\nरहे\nऱ्वासा\nलिए\nलिये\nलेकिन\nव\nवगेरह\nवग़ैरह\nवरग\nवर्ग\nवह\nवहाँ\nवहां\nवहिं\nवहीं\nवाले\nवुह\nवे\nवग़ैरह\nसंग\nसकता\nसकते\nसबसे\nसभि\nसभी\nसाथ\nसाबुत\nसाभ\nसारा\nसे\nसो\nसंग\nहि\nही\nहुअ\nहुआ\nहुइ\nहुई\nहुए\nहे\nहें\nहै\nहैं\nहो\nहूँ\nहोता\nहोति\nहोती\nहोते\nहोना\nहोने\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ru/__init__.py----------------------------------------
spacy.lang.ru.__init__.Russian(Language)
spacy.lang.ru.__init__.RussianDefaults(BaseDefaults)
spacy.lang.ru.__init__.make_lemmatizer(nlp:Language,model:Optional[Model],name:str,mode:str,overwrite:bool,scorer:Optional[Callable])


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ru/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ru/lemmatizer.py----------------------------------------
A:spacy.lang.ru.lemmatizer.self._morph->MorphAnalyzer(lang='ru')
A:spacy.lang.ru.lemmatizer.morphology->dict()
A:spacy.lang.ru.lemmatizer.analyses->self._morph.parse(string)
A:spacy.lang.ru.lemmatizer.(analysis_pos, _)->oc2ud(str(analysis.tag))
A:spacy.lang.ru.lemmatizer.(_, analysis_morph)->oc2ud(str(analysis.tag))
A:spacy.lang.ru.lemmatizer.normal_forms->set([an.normal_form for an in analyses])
A:spacy.lang.ru.lemmatizer.unmatched->set()
A:spacy.lang.ru.lemmatizer.grams->oc_tag.replace(' ', ',').split(',')
A:spacy.lang.ru.lemmatizer.gram->set().pop()
spacy.lang.ru.RussianLemmatizer(self,vocab:Vocab,model:Optional[Model],name:str='lemmatizer',*,mode:str='pymorphy3',overwrite:bool=False,scorer:Optional[Callable]=lemmatizer_score)
spacy.lang.ru.RussianLemmatizer._pymorphy_lemmatize(self,token:Token)->List[str]
spacy.lang.ru.RussianLemmatizer._pymorphy_lookup_lemmatize(self,token:Token)->List[str]
spacy.lang.ru.RussianLemmatizer.pymorphy2_lemmatize(self,token:Token)->List[str]
spacy.lang.ru.RussianLemmatizer.pymorphy2_lookup_lemmatize(self,token:Token)->List[str]
spacy.lang.ru.RussianLemmatizer.pymorphy3_lemmatize(self,token:Token)->List[str]
spacy.lang.ru.RussianLemmatizer.pymorphy3_lookup_lemmatize(self,token:Token)->List[str]
spacy.lang.ru.lemmatizer.RussianLemmatizer(self,vocab:Vocab,model:Optional[Model],name:str='lemmatizer',*,mode:str='pymorphy3',overwrite:bool=False,scorer:Optional[Callable]=lemmatizer_score)
spacy.lang.ru.lemmatizer.RussianLemmatizer.__init__(self,vocab:Vocab,model:Optional[Model],name:str='lemmatizer',*,mode:str='pymorphy3',overwrite:bool=False,scorer:Optional[Callable]=lemmatizer_score)
spacy.lang.ru.lemmatizer.RussianLemmatizer._pymorphy_lemmatize(self,token:Token)->List[str]
spacy.lang.ru.lemmatizer.RussianLemmatizer._pymorphy_lookup_lemmatize(self,token:Token)->List[str]
spacy.lang.ru.lemmatizer.RussianLemmatizer.pymorphy2_lemmatize(self,token:Token)->List[str]
spacy.lang.ru.lemmatizer.RussianLemmatizer.pymorphy2_lookup_lemmatize(self,token:Token)->List[str]
spacy.lang.ru.lemmatizer.RussianLemmatizer.pymorphy3_lemmatize(self,token:Token)->List[str]
spacy.lang.ru.lemmatizer.RussianLemmatizer.pymorphy3_lookup_lemmatize(self,token:Token)->List[str]
spacy.lang.ru.lemmatizer.oc2ud(oc_tag:str)->Tuple[str, Dict[str, str]]


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ru/lex_attrs.py----------------------------------------
A:spacy.lang.ru.lex_attrs._num_words->list(set('\nноль ноля нолю нолём ноле нулевой нулевого нулевому нулевым нулевом нулевая нулевую нулевое нулевые нулевых нулевыми \n\nчетверть четверти четвертью четвертей четвертям четвертями четвертях \n\nтреть трети третью третей третям третями третях \n\nполовина половины половине половину половиной половин половинам половинами половинах половиною \n\nодин одного одному одним одном \nпервой первого первому первом первый первым первых \nво-первых \nединица единицы единице единицу единицей единиц единицам единицами единицах единицею \n\nдва двумя двум двух двоих двое две \nвторого второму второй втором вторым вторых \nдвойка двойки двойке двойку двойкой двоек двойкам двойками двойках двойкою  \nво-вторых \nоба обе обеим обеими обеих обоим обоими обоих \n\nполтора полторы полутора \n\nтри третьего третьему третьем третьим третий тремя трем трех трое троих трёх \nтройка тройки тройке тройку тройкою троек тройкам тройками тройках тройкой \nтроечка троечки троечке троечку троечкой троечек троечкам троечками троечках троечкой \nтрешка трешки трешке трешку трешкой трешек трешкам трешками трешках трешкою \nтрёшка трёшки трёшке трёшку трёшкой трёшек трёшкам трёшками трёшках трёшкою \nтрояк трояка трояку трояком трояке трояки трояков троякам трояками трояках  \nтреха треху трехой \nтрёха трёху трёхой \nвтроем втроём \n\nчетыре четвертого четвертому четвертом четвертый четвертым четверка четырьмя четырем четырех четверо четырёх четверым \nчетверых \nвчетвером \n\nпять пятого пятому пятом пятый пятым пятью пяти пятеро пятерых пятерыми \nвпятером \nпятерочка пятерочки пятерочке пятерочками пятерочкой пятерочку пятерочкой пятерочками \nпятёрочка пятёрочки пятёрочке пятёрочками пятёрочкой пятёрочку пятёрочкой пятёрочками \nпятерка пятерки пятерке пятерками пятеркой пятерку пятерками \nпятёрка пятёрки пятёрке пятёрками пятёркой пятёрку пятёрками \nпятёра пятёры пятёре пятёрами пятёрой пятёру пятёрами \nпятера пятеры пятере пятерами пятерой пятеру пятерами \nпятак пятаки пятаке пятаками пятаком пятаку пятаками \n\nшесть шестерка шестого шестому шестой шестом шестым шестью шести шестеро шестерых \nвшестером \n\nсемь семерка седьмого седьмому седьмой седьмом седьмым семью семи семеро седьмых \nвсемером \n\nвосемь восьмерка восьмого восьмому восемью восьмой восьмом восьмым восеми восьмером восьми восьмью \nвосьмерых \nввосьмером \n\nдевять девятого девятому девятка девятом девятый девятым девятью девяти девятером вдевятером девятерых \nвдевятером \n\nдесять десятого десятому десятка десятом десятый десятым десятью десяти десятером десятых \nвдесятером \n\nодиннадцать одиннадцатого одиннадцатому одиннадцатом одиннадцатый одиннадцатым одиннадцатью одиннадцати \nодиннадцатых \n\nдвенадцать двенадцатого двенадцатому двенадцатом двенадцатый двенадцатым двенадцатью двенадцати \nдвенадцатых \n\nтринадцать тринадцатого тринадцатому тринадцатом тринадцатый тринадцатым тринадцатью тринадцати \nтринадцатых \n\nчетырнадцать четырнадцатого четырнадцатому четырнадцатом четырнадцатый четырнадцатым четырнадцатью четырнадцати \nчетырнадцатых \n\nпятнадцать пятнадцатого пятнадцатому пятнадцатом пятнадцатый пятнадцатым пятнадцатью пятнадцати \nпятнадцатых \nпятнарик пятнарику пятнариком пятнарики \n\nшестнадцать шестнадцатого шестнадцатому шестнадцатом шестнадцатый шестнадцатым шестнадцатью шестнадцати \nшестнадцатых \n\nсемнадцать семнадцатого семнадцатому семнадцатом семнадцатый семнадцатым семнадцатью семнадцати семнадцатых \n\nвосемнадцать восемнадцатого восемнадцатому восемнадцатом восемнадцатый восемнадцатым восемнадцатью восемнадцати \nвосемнадцатых \n\nдевятнадцать девятнадцатого девятнадцатому девятнадцатом девятнадцатый девятнадцатым девятнадцатью девятнадцати \nдевятнадцатых \n\nдвадцать двадцатого двадцатому двадцатом двадцатый двадцатым двадцатью двадцати двадцатых \n\nчетвертак четвертака четвертаке четвертаку четвертаки четвертаком четвертаками \n\nтридцать тридцатого тридцатому тридцатом тридцатый тридцатым тридцатью тридцати тридцатых \nтридцадка тридцадку тридцадке тридцадки тридцадкой тридцадкою тридцадками \n\nтридевять тридевяти тридевятью \n\nсорок сорокового сороковому сороковом сороковым сороковой сороковых \nсорокет сорокета сорокету сорокете сорокеты сорокетом сорокетами сорокетам \n\nпятьдесят пятьдесятого пятьдесятому пятьюдесятью пятьдесятом пятьдесятый пятьдесятым пятидесяти пятьдесятых \nполтинник полтинника полтиннике полтиннику полтинники полтинником полтинниками полтинникам полтинниках \nпятидесятка пятидесятке пятидесятку пятидесятки пятидесяткой пятидесятками пятидесяткам пятидесятках \nполтос полтоса полтосе полтосу полтосы полтосом полтосами полтосам полтосах \n\nшестьдесят шестьдесятого шестьдесятому шестьюдесятью шестьдесятом шестьдесятый шестьдесятым шестидесятые шестидесяти \nшестьдесятых \n\nсемьдесят семьдесятого семьдесятому семьюдесятью семьдесятом семьдесятый семьдесятым семидесяти семьдесятых \n\nвосемьдесят восемьдесятого восемьдесятому восемьюдесятью восемьдесятом восемьдесятый восемьдесятым восемидесяти \nвосьмидесяти восьмидесятых \n\nдевяносто девяностого девяностому девяностом девяностый девяностым девяноста девяностых \n\nсто сотого сотому сотом сотен сотый сотым ста \nстольник стольника стольнику стольнике стольники стольником стольниками \nсотка сотки сотке соткой сотками соткам сотках \nсотня сотни сотне сотней сотнями сотням сотнях \n\nдвести двумястами двухсотого двухсотому двухсотом двухсотый двухсотым двумстам двухстах двухсот \n\nтриста тремястами трехсотого трехсотому трехсотом трехсотый трехсотым тремстам трехстах трехсот \n\nчетыреста четырехсотого четырехсотому четырьмястами четырехсотом четырехсотый четырехсотым четыремстам четырехстах \nчетырехсот \n\nпятьсот пятисотого пятисотому пятьюстами пятисотом пятисотый пятисотым пятистам пятистах пятисот \nпятисотка пятисотки пятисотке пятисоткой пятисотками пятисоткам пятисоткою пятисотках \nпятихатка пятихатки пятихатке пятихаткой пятихатками пятихаткам пятихаткою пятихатках \nпятифан пятифаны пятифане пятифаном пятифанами пятифанах \n\nшестьсот шестисотого шестисотому шестьюстами шестисотом шестисотый шестисотым шестистам шестистах шестисот \n\nсемьсот семисотого семисотому семьюстами семисотом семисотый семисотым семистам семистах семисот \n\nвосемьсот восемисотого восемисотому восемисотом восемисотый восемисотым восьмистами восьмистам восьмистах восьмисот \n\nдевятьсот девятисотого девятисотому девятьюстами девятисотом девятисотый девятисотым девятистам девятистах девятисот \n\nтысяча тысячного тысячному тысячном тысячный тысячным тысячам тысячах тысячей тысяч тысячи тыс \nкосарь косаря косару косарем косарями косарях косарям косарей \n\nдесятитысячный десятитысячного десятитысячному десятитысячным десятитысячном десятитысячная десятитысячной \nдесятитысячную десятитысячною десятитысячное десятитысячные десятитысячных десятитысячными \n\nдвадцатитысячный двадцатитысячного двадцатитысячному двадцатитысячным двадцатитысячном двадцатитысячная \nдвадцатитысячной двадцатитысячную двадцатитысячною двадцатитысячное двадцатитысячные двадцатитысячных \nдвадцатитысячными \n\nтридцатитысячный тридцатитысячного тридцатитысячному тридцатитысячным тридцатитысячном тридцатитысячная \nтридцатитысячной тридцатитысячную тридцатитысячною тридцатитысячное тридцатитысячные тридцатитысячных \nтридцатитысячными \n\nсорокатысячный сорокатысячного сорокатысячному сорокатысячным сорокатысячном сорокатысячная \nсорокатысячной сорокатысячную сорокатысячною сорокатысячное сорокатысячные сорокатысячных \nсорокатысячными \n\nпятидесятитысячный пятидесятитысячного пятидесятитысячному пятидесятитысячным пятидесятитысячном пятидесятитысячная \nпятидесятитысячной пятидесятитысячную пятидесятитысячною пятидесятитысячное пятидесятитысячные пятидесятитысячных \nпятидесятитысячными \n\nшестидесятитысячный шестидесятитысячного шестидесятитысячному шестидесятитысячным шестидесятитысячном шестидесятитысячная \nшестидесятитысячной шестидесятитысячную шестидесятитысячною шестидесятитысячное шестидесятитысячные шестидесятитысячных \nшестидесятитысячными \n\nсемидесятитысячный семидесятитысячного семидесятитысячному семидесятитысячным семидесятитысячном семидесятитысячная \nсемидесятитысячной семидесятитысячную семидесятитысячною семидесятитысячное семидесятитысячные семидесятитысячных \nсемидесятитысячными \n\nвосьмидесятитысячный восьмидесятитысячного восьмидесятитысячному восьмидесятитысячным восьмидесятитысячном восьмидесятитысячная \nвосьмидесятитысячной восьмидесятитысячную восьмидесятитысячною восьмидесятитысячное восьмидесятитысячные восьмидесятитысячных \nвосьмидесятитысячными \n\nстотысячный стотысячного стотысячному стотысячным стотысячном стотысячная стотысячной стотысячную стотысячное \nстотысячные стотысячных стотысячными стотысячною \n\nмиллион миллионного миллионов миллионному миллионном миллионный миллионным миллионом миллиона миллионе миллиону \nмиллионов \nлям ляма лямы лямом лямами лямах лямов \nмлн \n\nдесятимиллионная десятимиллионной десятимиллионными десятимиллионный десятимиллионным десятимиллионному \nдесятимиллионными десятимиллионную десятимиллионное  десятимиллионные десятимиллионных десятимиллионною \n\nмиллиард миллиардного миллиардному миллиардном миллиардный миллиардным миллиардом миллиарда миллиарде миллиарду \nмиллиардов \nлярд лярда лярды лярдом лярдами лярдах лярдов \nмлрд \n\nтриллион триллионного триллионному триллионном триллионный триллионным триллионом триллиона триллионе триллиону \nтриллионов трлн \n\nквадриллион квадриллионного квадриллионному квадриллионный квадриллионным квадриллионом квадриллиона квадриллионе \nквадриллиону квадриллионов квадрлн \n\nквинтиллион квинтиллионного квинтиллионному квинтиллионный квинтиллионным квинтиллионом квинтиллиона квинтиллионе \nквинтиллиону квинтиллионов квинтлн \n\ni ii iii iv v vi vii viii ix x xi xii xiii xiv xv xvi xvii xviii xix xx xxi xxii xxiii xxiv xxv xxvi xxvii xxvii xxix\n'.split()))
A:spacy.lang.ru.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.ru.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.ru.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ru/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.ru.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ru/stop_words.py----------------------------------------
A:spacy.lang.ru.stop_words.STOP_WORDS->set('\nа авось ага агу аж ай али алло ау ах ая\n\nб будем будет будете будешь буду будут будучи будь будьте бы был была были было\nбыть бац без безусловно бишь благо благодаря ближайшие близко более больше\nбудто бывает бывала бывали бываю бывают бытует\n\nв вам вами вас весь во вот все всё всего всей всем всём всеми всему всех всею\nвсея всю вся вы ваш ваша ваше ваши вдали вдобавок вдруг ведь везде вернее\nвзаимно взаправду видно вишь включая вместо внакладе вначале вне вниз внизу\nвновь вовсе возможно воистину вокруг вон вообще вопреки вперекор вплоть\nвполне вправду вправе впрочем впрямь вресноту вроде вряд всегда всюду\nвсякий всякого всякой всячески вчеред\n\nг го где гораздо гав\n\nд да для до дабы давайте давно давным даже далее далеко дальше данная\nданного данное данной данном данному данные данный данных дану данунах\nдаром де действительно довольно доколе доколь долго должен должна\nдолжно должны должный дополнительно другая другие другим другими\nдругих другое другой\n\nе его едим едят ее её ей ел ела ем ему емъ если ест есть ешь еще ещё ею едва\nежели еле\n\nж же\n\nз за затем зато зачем здесь значит зря\n\nи из или им ими имъ их ибо иль имеет имел имела имело именно иметь иначе\nиногда иным иными итак ишь\n\nй\n\nк как кем ко когда кого ком кому комья которая которого которое которой котором\nкоторому которою которую которые который которым которыми которых кто ка кабы\nкаждая каждое каждые каждый кажется казалась казались казалось казался казаться\nкакая какие каким какими каков какого какой какому какою касательно кой коли\nколь конечно короче кроме кстати ку куда\n\nл ли либо лишь любая любого любое любой любом любую любыми любых\n\nм меня мне мной мною мог моги могите могла могли могло могу могут мое моё моего\nмоей моем моём моему моею можем может можете можешь мои мой моим моими моих\nмочь мою моя мы мало меж между менее меньше мимо многие много многого многое\nмногом многому можно мол му\n\nн на нам нами нас наса наш наша наше нашего нашей нашем нашему нашею наши нашим\nнашими наших нашу не него нее неё ней нем нём нему нет нею ним ними них но\nнаверняка наверху навряд навыворот над надо назад наиболее наизворот\nнаизнанку наипаче накануне наконец наоборот наперед наперекор наподобие\nнапример напротив напрямую насилу настоящая настоящее настоящие настоящий\nнасчет нате находиться начала начале неважно негде недавно недалеко незачем\nнекем некогда некому некоторая некоторые некоторый некоторых некто некуда\nнельзя немногие немногим немного необходимо необходимости необходимые\nнеобходимым неоткуда непрерывно нередко несколько нету неужели нечего\nнечем нечему нечто нешто нибудь нигде ниже низко никак никакой никем\nникогда никого никому никто никуда ниоткуда нипочем ничего ничем ничему\nничто ну нужная нужно нужного нужные нужный нужных ныне нынешнее нынешней\nнынешних нынче\n\nо об один одна одни одним одними одних одно одного одной одном одному одною\nодну он она оне они оно от оба общую обычно ого однажды однако ой около оный\nоп опять особенно особо особую особые откуда отнелижа отнелиже отовсюду\nотсюда оттого оттот оттуда отчего отчему ох очевидно очень ом\n\nп по при паче перед под подавно поди подобная подобно подобного подобные\nподобный подобным подобных поелику пожалуй пожалуйста позже поистине\nпока покамест поколе поколь покуда покудова помимо понеже поприще пор\nпора посему поскольку после посреди посредством потом потому потомушта\nпохожем почему почти поэтому прежде притом причем про просто прочего\nпрочее прочему прочими проще прям пусть\n\nр ради разве ранее рано раньше рядом\n\nс сам сама сами самим самими самих само самого самом самому саму свое своё\nсвоего своей своем своём своему своею свои свой своим своими своих свою своя\nсебе себя собой собою самая самое самой самый самых сверх свыше се сего сей\nсейчас сие сих сквозь сколько скорее скоро следует слишком смогут сможет\nсначала снова со собственно совсем сперва спокону спустя сразу среди сродни\nстал стала стали стало стать суть сызнова\n\nта то ту ты ти так такая такие таким такими таких такого такое такой таком такому такою\nтакую те тебе тебя тем теми тех тобой тобою того той только том томах тому\nтот тою также таки таков такова там твои твоим твоих твой твоя твоё\nтеперь тогда тоже тотчас точно туда тут тьфу тая\n\nу уже увы уж ура ух ую\n\nф фу\n\nх ха хе хорошо хотел хотела хотелось хотеть хоть хотя хочешь хочу хуже\n\nч чего чем чём чему что чтобы часто чаще чей через чтоб чуть чхать чьим\nчьих чьё чё\n\nш ша\n\nщ ща щас\n\nы ых ые ый\n\nэ эта эти этим этими этих это этого этой этом этому этот этою эту эдак эдакий\nэй эка экий этак этакий эх\n\nю\n\nя явно явных яко якобы якоже\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/zh/__init__.py----------------------------------------
A:spacy.lang.zh.__init__.warn_msg->errors.Warnings.W104.format(target='pkuseg', current=self.segmenter)
A:spacy.lang.zh.__init__.self.jieba_seg->try_jieba_import()
A:spacy.lang.zh.__init__.self.pkuseg_seg->spacy_pkuseg.pkuseg(path)
A:spacy.lang.zh.__init__.words->list(text)
A:spacy.lang.zh.__init__.(words, spaces)->util.get_words_and_spaces(words, text)
A:spacy.lang.zh.__init__.self.pkuseg_seg.preprocesser->spacy_pkuseg.Preprocesser(user_dict)
A:spacy.lang.zh.__init__.self.segmenter->load_config_from_str(DEFAULT_CONFIG).get('segmenter', Segmenter.char)
A:spacy.lang.zh.__init__.tempdir->Path(tempdir)
A:spacy.lang.zh.__init__.pkuseg_features_b->fileh.read()
A:spacy.lang.zh.__init__.pkuseg_weights_b->fileh.read()
A:spacy.lang.zh.__init__.pkuseg_data['processors_data']->srsly.msgpack_loads(b)
A:spacy.lang.zh.__init__.self.pkuseg_seg.postprocesser.common_words->set(common_words)
A:spacy.lang.zh.__init__.self.pkuseg_seg.postprocesser.other_words->set(other_words)
A:spacy.lang.zh.__init__.path->util.ensure_path(path)
A:spacy.lang.zh.__init__.data->srsly.read_msgpack(path)
A:spacy.lang.zh.__init__.config->load_config_from_str(DEFAULT_CONFIG)
spacy.lang.zh.__init__.Chinese(Language)
spacy.lang.zh.__init__.ChineseDefaults(BaseDefaults)
spacy.lang.zh.__init__.ChineseTokenizer(self,vocab:Vocab,segmenter:Segmenter=Segmenter.char)
spacy.lang.zh.__init__.ChineseTokenizer.__init__(self,vocab:Vocab,segmenter:Segmenter=Segmenter.char)
spacy.lang.zh.__init__.ChineseTokenizer._get_config(self)->Dict[str, Any]
spacy.lang.zh.__init__.ChineseTokenizer._set_config(self,config:Dict[str,Any]={})->None
spacy.lang.zh.__init__.ChineseTokenizer.from_bytes(self,data,**kwargs)
spacy.lang.zh.__init__.ChineseTokenizer.from_disk(self,path,**kwargs)
spacy.lang.zh.__init__.ChineseTokenizer.initialize(self,get_examples:Optional[Callable[[],Iterable[Example]]]=None,*,nlp:Optional[Language]=None,pkuseg_model:Optional[str]=None,pkuseg_user_dict:Optional[str]='default')
spacy.lang.zh.__init__.ChineseTokenizer.pkuseg_update_user_dict(self,words:List[str],reset:bool=False)
spacy.lang.zh.__init__.ChineseTokenizer.score(self,examples)
spacy.lang.zh.__init__.ChineseTokenizer.to_bytes(self,**kwargs)
spacy.lang.zh.__init__.ChineseTokenizer.to_disk(self,path,**kwargs)
spacy.lang.zh.__init__.Segmenter(str,Enum)
spacy.lang.zh.__init__.Segmenter.values(cls)
spacy.lang.zh.__init__._get_pkuseg_trie_data(node,path='')
spacy.lang.zh.__init__.create_chinese_tokenizer(segmenter:Segmenter=Segmenter.char)
spacy.lang.zh.__init__.try_jieba_import()
spacy.lang.zh.__init__.try_pkuseg_import(pkuseg_model:Optional[str],pkuseg_user_dict:Optional[str])


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/zh/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/zh/lex_attrs.py----------------------------------------
A:spacy.lang.zh.lex_attrs.text->text.replace(',', '').replace('.', '').replace('，', '').replace('。', '').replace(',', '').replace('.', '').replace('，', '').replace('。', '')
A:spacy.lang.zh.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace('，', '').replace('。', '').replace(',', '').replace('.', '').replace('，', '').replace('。', '').split('/')
spacy.lang.zh.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/zh/stop_words.py----------------------------------------
A:spacy.lang.zh.stop_words.STOP_WORDS->set('\n!\n"\n#\n$\n%\n&\n\'\n(\n)\n*\n+\n,\n-\n--\n.\n..\n...\n......\n...................\n./\n.一\n.数\n.日\n/\n//\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n:\n://\n::\n;\n<\n=\n>\n>>\n?\n@\nA\nLex\n[\n]\n^\n_\n`\nexp\nsub\nsup\n|\n}\n~\n~~~~\n·\n×\n×××\nΔ\nΨ\nγ\nμ\nφ\nφ．\nВ\n—\n——\n———\n‘\n’\n’‘\n“\n”\n”，\n…\n……\n…………………………………………………③\n′∈\n′｜\n℃\nⅢ\n↑\n→\n∈［\n∪φ∈\n≈\n①\n②\n②ｃ\n③\n③］\n④\n⑤\n⑥\n⑦\n⑧\n⑨\n⑩\n──\n■\n▲\n\u3000\n、\n。\n〈\n〉\n《\n》\n》），\n」\n『\n』\n【\n】\n〔\n〕\n〕〔\n㈧\n一\n一.\n一一\n一下\n一个\n一些\n一何\n一切\n一则\n一则通过\n一天\n一定\n一方面\n一旦\n一时\n一来\n一样\n一次\n一片\n一番\n一直\n一致\n一般\n一起\n一转眼\n一边\n一面\n七\n万一\n三\n三天两头\n三番两次\n三番五次\n上\n上下\n上升\n上去\n上来\n上述\n上面\n下\n下列\n下去\n下来\n下面\n不\n不一\n不下\n不久\n不了\n不亦乐乎\n不仅\n不仅...而且\n不仅仅\n不仅仅是\n不会\n不但\n不但...而且\n不光\n不免\n不再\n不力\n不单\n不变\n不只\n不可\n不可开交\n不可抗拒\n不同\n不外\n不外乎\n不够\n不大\n不如\n不妨\n不定\n不对\n不少\n不尽\n不尽然\n不巧\n不已\n不常\n不得\n不得不\n不得了\n不得已\n不必\n不怎么\n不怕\n不惟\n不成\n不拘\n不择手段\n不敢\n不料\n不断\n不日\n不时\n不是\n不曾\n不止\n不止一次\n不比\n不消\n不满\n不然\n不然的话\n不特\n不独\n不由得\n不知不觉\n不管\n不管怎样\n不经意\n不胜\n不能\n不能不\n不至于\n不若\n不要\n不论\n不起\n不足\n不过\n不迭\n不问\n不限\n与\n与其\n与其说\n与否\n与此同时\n专门\n且\n且不说\n且说\n两者\n严格\n严重\n个\n个人\n个别\n中小\n中间\n丰富\n串行\n临\n临到\n为\n为主\n为了\n为什么\n为什麽\n为何\n为止\n为此\n为着\n主张\n主要\n举凡\n举行\n乃\n乃至\n乃至于\n么\n之\n之一\n之前\n之后\n之後\n之所以\n之类\n乌乎\n乎\n乒\n乘\n乘势\n乘机\n乘胜\n乘虚\n乘隙\n九\n也\n也好\n也就是说\n也是\n也罢\n了\n了解\n争取\n二\n二来\n二话不说\n二话没说\n于\n于是\n于是乎\n云云\n云尔\n互\n互相\n五\n些\n交口\n亦\n产生\n亲口\n亲手\n亲眼\n亲自\n亲身\n人\n人人\n人们\n人家\n人民\n什么\n什么样\n什麽\n仅\n仅仅\n今\n今后\n今天\n今年\n今後\n介于\n仍\n仍旧\n仍然\n从\n从不\n从严\n从中\n从事\n从今以后\n从优\n从古到今\n从古至今\n从头\n从宽\n从小\n从新\n从无到有\n从早到晚\n从未\n从来\n从此\n从此以后\n从而\n从轻\n从速\n从重\n他\n他人\n他们\n他是\n他的\n代替\n以\n以上\n以下\n以为\n以便\n以免\n以前\n以及\n以后\n以外\n以後\n以故\n以期\n以来\n以至\n以至于\n以致\n们\n任\n任何\n任凭\n任务\n企图\n伙同\n会\n伟大\n传\n传说\n传闻\n似乎\n似的\n但\n但凡\n但愿\n但是\n何\n何乐而不为\n何以\n何况\n何处\n何妨\n何尝\n何必\n何时\n何止\n何苦\n何须\n余外\n作为\n你\n你们\n你是\n你的\n使\n使得\n使用\n例如\n依\n依据\n依照\n依靠\n便\n便于\n促进\n保持\n保管\n保险\n俺\n俺们\n倍加\n倍感\n倒不如\n倒不如说\n倒是\n倘\n倘使\n倘或\n倘然\n倘若\n借\n借以\n借此\n假使\n假如\n假若\n偏偏\n做到\n偶尔\n偶而\n傥然\n像\n儿\n允许\n元／吨\n充其极\n充其量\n充分\n先不先\n先后\n先後\n先生\n光\n光是\n全体\n全力\n全年\n全然\n全身心\n全部\n全都\n全面\n八\n八成\n公然\n六\n兮\n共\n共同\n共总\n关于\n其\n其一\n其中\n其二\n其他\n其余\n其后\n其它\n其实\n其次\n具体\n具体地说\n具体来说\n具体说来\n具有\n兼之\n内\n再\n再其次\n再则\n再有\n再次\n再者\n再者说\n再说\n冒\n冲\n决不\n决定\n决非\n况且\n准备\n凑巧\n凝神\n几\n几乎\n几度\n几时\n几番\n几经\n凡\n凡是\n凭\n凭借\n出\n出于\n出去\n出来\n出现\n分别\n分头\n分期\n分期分批\n切\n切不可\n切切\n切勿\n切莫\n则\n则甚\n刚\n刚好\n刚巧\n刚才\n初\n别\n别人\n别处\n别是\n别的\n别管\n别说\n到\n到了儿\n到处\n到头\n到头来\n到底\n到目前为止\n前后\n前此\n前者\n前进\n前面\n加上\n加之\n加以\n加入\n加强\n动不动\n动辄\n勃然\n匆匆\n十分\n千\n千万\n千万千万\n半\n单\n单单\n单纯\n即\n即令\n即使\n即便\n即刻\n即如\n即将\n即或\n即是说\n即若\n却\n却不\n历\n原来\n去\n又\n又及\n及\n及其\n及时\n及至\n双方\n反之\n反之亦然\n反之则\n反倒\n反倒是\n反应\n反手\n反映\n反而\n反过来\n反过来说\n取得\n取道\n受到\n变成\n古来\n另\n另一个\n另一方面\n另外\n另悉\n另方面\n另行\n只\n只当\n只怕\n只是\n只有\n只消\n只要\n只限\n叫\n叫做\n召开\n叮咚\n叮当\n可\n可以\n可好\n可是\n可能\n可见\n各\n各个\n各人\n各位\n各地\n各式\n各种\n各级\n各自\n合理\n同\n同一\n同时\n同样\n后\n后来\n后者\n后面\n向\n向使\n向着\n吓\n吗\n否则\n吧\n吧哒\n吱\n呀\n呃\n呆呆地\n呐\n呕\n呗\n呜\n呜呼\n呢\n周围\n呵\n呵呵\n呸\n呼哧\n呼啦\n咋\n和\n咚\n咦\n咧\n咱\n咱们\n咳\n哇\n哈\n哈哈\n哉\n哎\n哎呀\n哎哟\n哗\n哗啦\n哟\n哦\n哩\n哪\n哪个\n哪些\n哪儿\n哪天\n哪年\n哪怕\n哪样\n哪边\n哪里\n哼\n哼唷\n唉\n唯有\n啊\n啊呀\n啊哈\n啊哟\n啐\n啥\n啦\n啪达\n啷当\n喀\n喂\n喏\n喔唷\n喽\n嗡\n嗡嗡\n嗬\n嗯\n嗳\n嘎\n嘎嘎\n嘎登\n嘘\n嘛\n嘻\n嘿\n嘿嘿\n四\n因\n因为\n因了\n因此\n因着\n因而\n固\n固然\n在\n在下\n在于\n地\n均\n坚决\n坚持\n基于\n基本\n基本上\n处在\n处处\n处理\n复杂\n多\n多么\n多亏\n多多\n多多少少\n多多益善\n多少\n多年前\n多年来\n多数\n多次\n够瞧的\n大\n大不了\n大举\n大事\n大体\n大体上\n大凡\n大力\n大多\n大多数\n大大\n大家\n大张旗鼓\n大批\n大抵\n大概\n大略\n大约\n大致\n大都\n大量\n大面儿上\n失去\n奇\n奈\n奋勇\n她\n她们\n她是\n她的\n好\n好在\n好的\n好象\n如\n如上\n如上所述\n如下\n如今\n如何\n如其\n如前所述\n如同\n如常\n如是\n如期\n如果\n如次\n如此\n如此等等\n如若\n始而\n姑且\n存在\n存心\n孰料\n孰知\n宁\n宁可\n宁愿\n宁肯\n它\n它们\n它们的\n它是\n它的\n安全\n完全\n完成\n定\n实现\n实际\n宣布\n容易\n密切\n对\n对于\n对应\n对待\n对方\n对比\n将\n将才\n将要\n将近\n小\n少数\n尔\n尔后\n尔尔\n尔等\n尚且\n尤其\n就\n就地\n就是\n就是了\n就是说\n就此\n就算\n就要\n尽\n尽可能\n尽如人意\n尽心尽力\n尽心竭力\n尽快\n尽早\n尽然\n尽管\n尽管如此\n尽量\n局外\n居然\n届时\n属于\n屡\n屡屡\n屡次\n屡次三番\n岂\n岂但\n岂止\n岂非\n川流不息\n左右\n巨大\n巩固\n差一点\n差不多\n己\n已\n已矣\n已经\n巴\n巴巴\n带\n帮助\n常\n常常\n常言说\n常言说得好\n常言道\n平素\n年复一年\n并\n并不\n并不是\n并且\n并排\n并无\n并没\n并没有\n并肩\n并非\n广大\n广泛\n应当\n应用\n应该\n庶乎\n庶几\n开外\n开始\n开展\n引起\n弗\n弹指之间\n强烈\n强调\n归\n归根到底\n归根结底\n归齐\n当\n当下\n当中\n当儿\n当前\n当即\n当口儿\n当地\n当场\n当头\n当庭\n当时\n当然\n当真\n当着\n形成\n彻夜\n彻底\n彼\n彼时\n彼此\n往\n往往\n待\n待到\n很\n很多\n很少\n後来\n後面\n得\n得了\n得出\n得到\n得天独厚\n得起\n心里\n必\n必定\n必将\n必然\n必要\n必须\n快\n快要\n忽地\n忽然\n怎\n怎么\n怎么办\n怎么样\n怎奈\n怎样\n怎麽\n怕\n急匆匆\n怪\n怪不得\n总之\n总是\n总的来看\n总的来说\n总的说来\n总结\n总而言之\n恍然\n恐怕\n恰似\n恰好\n恰如\n恰巧\n恰恰\n恰恰相反\n恰逢\n您\n您们\n您是\n惟其\n惯常\n意思\n愤然\n愿意\n慢说\n成为\n成年\n成年累月\n成心\n我\n我们\n我是\n我的\n或\n或则\n或多或少\n或是\n或曰\n或者\n或许\n战斗\n截然\n截至\n所\n所以\n所在\n所幸\n所有\n所谓\n才\n才能\n扑通\n打\n打从\n打开天窗说亮话\n扩大\n把\n抑或\n抽冷子\n拦腰\n拿\n按\n按时\n按期\n按照\n按理\n按说\n挨个\n挨家挨户\n挨次\n挨着\n挨门挨户\n挨门逐户\n换句话说\n换言之\n据\n据实\n据悉\n据我所知\n据此\n据称\n据说\n掌握\n接下来\n接着\n接著\n接连不断\n放量\n故\n故意\n故此\n故而\n敞开儿\n敢\n敢于\n敢情\n数/\n整个\n断然\n方\n方便\n方才\n方能\n方面\n旁人\n无\n无宁\n无法\n无论\n既\n既...又\n既往\n既是\n既然\n日复一日\n日渐\n日益\n日臻\n日见\n时候\n昂然\n明显\n明确\n是\n是不是\n是以\n是否\n是的\n显然\n显著\n普通\n普遍\n暗中\n暗地里\n暗自\n更\n更为\n更加\n更进一步\n曾\n曾经\n替\n替代\n最\n最后\n最大\n最好\n最後\n最近\n最高\n有\n有些\n有关\n有利\n有力\n有及\n有所\n有效\n有时\n有点\n有的\n有的是\n有着\n有著\n望\n朝\n朝着\n末##末\n本\n本人\n本地\n本着\n本身\n权时\n来\n来不及\n来得及\n来看\n来着\n来自\n来讲\n来说\n极\n极为\n极了\n极其\n极力\n极大\n极度\n极端\n构成\n果然\n果真\n某\n某个\n某些\n某某\n根据\n根本\n格外\n梆\n概\n次第\n欢迎\n欤\n正值\n正在\n正如\n正巧\n正常\n正是\n此\n此中\n此后\n此地\n此处\n此外\n此时\n此次\n此间\n殆\n毋宁\n每\n每个\n每天\n每年\n每当\n每时每刻\n每每\n每逢\n比\n比及\n比如\n比如说\n比方\n比照\n比起\n比较\n毕竟\n毫不\n毫无\n毫无例外\n毫无保留地\n汝\n沙沙\n没\n没奈何\n没有\n沿\n沿着\n注意\n活\n深入\n清楚\n满\n满足\n漫说\n焉\n然\n然则\n然后\n然後\n然而\n照\n照着\n牢牢\n特别是\n特殊\n特点\n犹且\n犹自\n独\n独自\n猛然\n猛然间\n率尔\n率然\n现代\n现在\n理应\n理当\n理该\n瑟瑟\n甚且\n甚么\n甚或\n甚而\n甚至\n甚至于\n用\n用来\n甫\n甭\n由\n由于\n由是\n由此\n由此可见\n略\n略为\n略加\n略微\n白\n白白\n的\n的确\n的话\n皆可\n目前\n直到\n直接\n相似\n相信\n相反\n相同\n相对\n相对而言\n相应\n相当\n相等\n省得\n看\n看上去\n看出\n看到\n看来\n看样子\n看看\n看见\n看起来\n真是\n真正\n眨眼\n着\n着呢\n矣\n矣乎\n矣哉\n知道\n砰\n确定\n碰巧\n社会主义\n离\n种\n积极\n移动\n究竟\n穷年累月\n突出\n突然\n窃\n立\n立刻\n立即\n立地\n立时\n立马\n竟\n竟然\n竟而\n第\n第二\n等\n等到\n等等\n策略地\n简直\n简而言之\n简言之\n管\n类如\n粗\n精光\n紧接着\n累年\n累次\n纯\n纯粹\n纵\n纵令\n纵使\n纵然\n练习\n组成\n经\n经常\n经过\n结合\n结果\n给\n绝\n绝不\n绝对\n绝非\n绝顶\n继之\n继后\n继续\n继而\n维持\n综上所述\n缕缕\n罢了\n老\n老大\n老是\n老老实实\n考虑\n者\n而\n而且\n而况\n而又\n而后\n而外\n而已\n而是\n而言\n而论\n联系\n联袂\n背地里\n背靠背\n能\n能否\n能够\n腾\n自\n自个儿\n自从\n自各儿\n自后\n自家\n自己\n自打\n自身\n臭\n至\n至于\n至今\n至若\n致\n般的\n良好\n若\n若夫\n若是\n若果\n若非\n范围\n莫\n莫不\n莫不然\n莫如\n莫若\n莫非\n获得\n藉以\n虽\n虽则\n虽然\n虽说\n蛮\n行为\n行动\n表明\n表示\n被\n要\n要不\n要不是\n要不然\n要么\n要是\n要求\n见\n规定\n觉得\n譬喻\n譬如\n认为\n认真\n认识\n让\n许多\n论\n论说\n设使\n设或\n设若\n诚如\n诚然\n话说\n该\n该当\n说明\n说来\n说说\n请勿\n诸\n诸位\n诸如\n谁\n谁人\n谁料\n谁知\n谨\n豁然\n贼死\n赖以\n赶\n赶快\n赶早不赶晚\n起\n起先\n起初\n起头\n起来\n起见\n起首\n趁\n趁便\n趁势\n趁早\n趁机\n趁热\n趁着\n越是\n距\n跟\n路经\n转动\n转变\n转贴\n轰然\n较\n较为\n较之\n较比\n边\n达到\n达旦\n迄\n迅速\n过\n过于\n过去\n过来\n运用\n近\n近几年来\n近年来\n近来\n还\n还是\n还有\n还要\n这\n这一来\n这个\n这么\n这么些\n这么样\n这么点儿\n这些\n这会儿\n这儿\n这就是说\n这时\n这样\n这次\n这点\n这种\n这般\n这边\n这里\n这麽\n进入\n进去\n进来\n进步\n进而\n进行\n连\n连同\n连声\n连日\n连日来\n连袂\n连连\n迟早\n迫于\n适应\n适当\n适用\n逐步\n逐渐\n通常\n通过\n造成\n逢\n遇到\n遭到\n遵循\n遵照\n避免\n那\n那个\n那么\n那么些\n那么样\n那些\n那会儿\n那儿\n那时\n那末\n那样\n那般\n那边\n那里\n那麽\n部分\n都\n鄙人\n采取\n里面\n重大\n重新\n重要\n鉴于\n针对\n长期以来\n长此下去\n长线\n长话短说\n问题\n间或\n防止\n阿\n附近\n陈年\n限制\n陡然\n除\n除了\n除却\n除去\n除外\n除开\n除此\n除此之外\n除此以外\n除此而外\n除非\n随\n随后\n随时\n随着\n随著\n隔夜\n隔日\n难得\n难怪\n难说\n难道\n难道说\n集中\n零\n需要\n非但\n非常\n非徒\n非得\n非特\n非独\n靠\n顶多\n顷\n顷刻\n顷刻之间\n顷刻间\n顺\n顺着\n顿时\n颇\n风雨无阻\n饱\n首先\n马上\n高低\n高兴\n默然\n默默地\n齐\n︿\n！\n＃\n＄\n％\n＆\n＇\n（\n）\n）÷（１－\n）、\n＊\n＋\n＋ξ\n＋＋\n，\n，也\n－\n－β\n－－\n－［＊］－\n．\n／\n０\n０：２\n１\n１．\n１２％\n２\n２．３％\n３\n４\n５\n５：０\n６\n７\n８\n９\n：\n；\n＜\n＜±\n＜Δ\n＜λ\n＜φ\n＜＜\n＝\n＝″\n＝☆\n＝（\n＝－\n＝［\n＝｛\n＞\n＞λ\n？\n＠\nＡ\nＬＩ\nＲ．Ｌ．\nＺＸＦＩＴＬ\n［\n［①①］\n［①②］\n［①③］\n［①④］\n［①⑤］\n［①⑥］\n［①⑦］\n［①⑧］\n［①⑨］\n［①Ａ］\n［①Ｂ］\n［①Ｃ］\n［①Ｄ］\n［①Ｅ］\n［①］\n［①ａ］\n［①ｃ］\n［①ｄ］\n［①ｅ］\n［①ｆ］\n［①ｇ］\n［①ｈ］\n［①ｉ］\n［①ｏ］\n［②\n［②①］\n［②②］\n［②③］\n［②④\n［②⑤］\n［②⑥］\n［②⑦］\n［②⑧］\n［②⑩］\n［②Ｂ］\n［②Ｇ］\n［②］\n［②ａ］\n［②ｂ］\n［②ｃ］\n［②ｄ］\n［②ｅ］\n［②ｆ］\n［②ｇ］\n［②ｈ］\n［②ｉ］\n［②ｊ］\n［③①］\n［③⑩］\n［③Ｆ］\n［③］\n［③ａ］\n［③ｂ］\n［③ｃ］\n［③ｄ］\n［③ｅ］\n［③ｇ］\n［③ｈ］\n［④］\n［④ａ］\n［④ｂ］\n［④ｃ］\n［④ｄ］\n［④ｅ］\n［⑤］\n［⑤］］\n［⑤ａ］\n［⑤ｂ］\n［⑤ｄ］\n［⑤ｅ］\n［⑤ｆ］\n［⑥］\n［⑦］\n［⑧］\n［⑨］\n［⑩］\n［＊］\n［－\n［］\n］\n］∧′＝［\n］［\n＿\nａ］\nｂ］\nｃ］\nｅ］\nｆ］\nｎｇ昉\n｛\n｛－\n｜\n｝\n｝＞\n～\n～±\n～＋\n￥\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ar/__init__.py----------------------------------------
spacy.lang.ar.__init__.Arabic(Language)
spacy.lang.ar.__init__.ArabicDefaults(BaseDefaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ar/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ar/lex_attrs.py----------------------------------------
A:spacy.lang.ar.lex_attrs._num_words->set('\nصفر\nواحد\nإثنان\nاثنان\nثلاثة\nثلاثه\nأربعة\nأربعه\nخمسة\nخمسه\nستة\nسته\nسبعة\nسبعه\nثمانية\nثمانيه\nتسعة\nتسعه\nﻋﺸﺮﺓ\nﻋﺸﺮه\nعشرون\nعشرين\nثلاثون\nثلاثين\nاربعون\nاربعين\nأربعون\nأربعين\nخمسون\nخمسين\nستون\nستين\nسبعون\nسبعين\nثمانون\nثمانين\nتسعون\nتسعين\nمائتين\nمائتان\nثلاثمائة\nخمسمائة\nسبعمائة\nالف\nآلاف\nملايين\nمليون\nمليار\nمليارات\n'.split())
A:spacy.lang.ar.lex_attrs._ordinal_words->set('\nاول\nأول\nحاد\nواحد\nثان\nثاني\nثالث\nرابع\nخامس\nسادس\nسابع\nثامن\nتاسع\nعاشر\n'.split())
A:spacy.lang.ar.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.ar.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.ar.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ar/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.ar.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ar/stop_words.py----------------------------------------
A:spacy.lang.ar.stop_words.STOP_WORDS->set('\nمن\nنحو\nلعل\nبما\nبين\nوبين\nايضا\nوبينما\nتحت\nمثلا\nلدي\nعنه\nمع\nهي\nوهذا\nواذا\nهذان\nانه\nبينما\nأمسى\nوسوف\nولم\nلذلك\nإلى\nمنه\nمنها\nكما\nظل\nهنا\nبه\nكذلك\nاما\nهما\nبعد\nبينهم\nالتي\nأبو\nاذا\nبدلا\nلها\nأمام\nيلي\nحين\nضد\nالذي\nقد\nصار\nإذا\nمابرح\nقبل\nكل\nوليست\nالذين\nلهذا\nوثي\nانهم\nباللتي\nمافتئ\nولا\nبهذه\nبحيث\nكيف\nوله\nعلي\nبات\nلاسيما\nحتى\nوقد\nو\nأما\nفيها\nبهذا\nلذا\nحيث\nلقد\nإن\nفإن\nاول\nليت\nفاللتي\nولقد\nلسوف\nهذه\nولماذا\nمعه\nالحالي\nبإن\nحول\nفي\nعليه\nمايزال\nولعل\nأنه\nأضحى\nاي\nستكون\nلن\nأن\nضمن\nوعلى\nامسى\nالي\nذات\nولايزال\nذلك\nفقد\nهم\nأي\nعند\nابن\nأو\nفهو\nفانه\nسوف\nما\nآل\nكلا\nعنها\nوكذلك\nليست\nلم\nوأن\nماذا\nلو\nوهل\nاللتي\nولذا\nيمكن\nفيه\nالا\nعليها\nوبينهم\nيوم\nوبما\nلما\nفكان\nاضحى\nاصبح\nلهم\nبها\nاو\nالذى\nالى\nإلي\nقال\nوالتي\nلازال\nأصبح\nولهذا\nمثل\nوكانت\nلكنه\nبذلك\nهذا\nلماذا\nقالت\nفقط\nلكن\nمما\nوكل\nوان\nوأبو\nومن\nكان\nمازال\nهل\nبينهن\nهو\nوما\nعلى\nوهو\nلأن\nواللتي\nوالذي\nدون\nعن\nوايضا\nهناك\nبلا\nجدا\nثم\nمنذ\nاللذين\nلايزال\nبعض\nمساء\nتكون\nفلا\nبيننا\nلا\nولكن\nإذ\nوأثناء\nليس\nومع\nفيهم\nولسوف\nبل\nتلك\nأحد\nوهي\nوكان\nومنها\nوفي\nماانفك\nاليوم\nوماذا\nهؤلاء\nوليس\nله\nأثناء\nبد\nاليه\nكأن\nاليها\nبتلك\nيكون\nولما\nهن\nوالى\nكانت\nوقبل\nان\nلدى\nإذما\nإذن\nأف\nأقل\nأكثر\nألا\nإلا\nاللاتي\nاللائي\nاللتان\nاللتيا\nاللتين\nاللذان\nاللواتي\nإليك\nإليكم\nإليكما\nإليكن\nأم\nأما\nإما\nإنا\nأنا\nأنت\nأنتم\nأنتما\nأنتن\nإنما\nإنه\nأنى\nأنى\nآه\nآها\nأولاء\nأولئك\nأوه\nآي\nأيها\nإي\nأين\nأين\nأينما\nإيه\nبخ\nبس\nبك\nبكم\nبكم\nبكما\nبكن\nبلى\nبماذا\nبمن\nبنا\nبهم\nبهما\nبهن\nبي\nبيد\nتلكم\nتلكما\nته\nتي\nتين\nتينك\nثمة\nحاشا\nحبذا\nحيثما\nخلا\nذا\nذاك\nذان\nذانك\nذلكم\nذلكما\nذلكن\nذه\nذو\nذوا\nذواتا\nذواتي\nذي\nذين\nذينك\nريث\nسوى\nشتان\nعدا\nعسى\nعل\nعليك\nعما\nغير\nفإذا\nفمن\nفيم\nفيما\nكأنما\nكأي\nكأين\nكذا\nكلاهما\nكلتا\nكلما\nكليكما\nكليهما\nكم\nكم\nكي\nكيت\nكيفما\nلست\nلستم\nلستما\nلستن\nلسن\nلسنا\nلك\nلكم\nلكما\nلكنما\nلكي\nلكيلا\nلنا\nلهما\nلهن\nلولا\nلوما\nلي\nلئن\nليسا\nليستا\nليسوا\nمتى\nمذ\nممن\nمه\nمهما\nنحن\nنعم\nها\nهاتان\nهاته\nهاتي\nهاتين\nهاك\nهاهنا\nهذي\nهذين\nهكذا\nهلا\nهنالك\nهيا\nهيت\nهيهات\nوالذين\nوإذ\nوإذا\nوإن\nولو\nيا\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ar/punctuation.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ja/__init__.py----------------------------------------
A:spacy.lang.ja.__init__.self.tokenizer->try_sudachi_import(self.split_mode)
A:spacy.lang.ja.__init__.sudachipy_tokens->self.tokenizer.tokenize(text)
A:spacy.lang.ja.__init__.dtokens->self._get_dtokens(sub_a, False)
A:spacy.lang.ja.__init__.(dtokens, spaces)->get_dtokens_and_spaces(dtokens, text)
A:spacy.lang.ja.__init__.sub_tokens_list->list(sub_tokens_list)
A:spacy.lang.ja.__init__.doc->Doc(self.vocab, words=words, spaces=spaces)
A:spacy.lang.ja.__init__.(token.pos, next_pos)->resolve_pos(token.orth_, dtoken.tag, tags[idx + 1] if idx + 1 < len(tags) else None)
A:spacy.lang.ja.__init__.morph['Reading']->re.sub('[=|]', '_', dtoken.reading)
A:spacy.lang.ja.__init__.token.morph->MorphAnalysis(self.vocab, morph)
A:spacy.lang.ja.__init__.sub_a->token.split(self.tokenizer.SplitMode.A)
A:spacy.lang.ja.__init__.sub_b->token.split(self.tokenizer.SplitMode.B)
A:spacy.lang.ja.__init__.self.split_mode->load_config_from_str(DEFAULT_CONFIG).get('split_mode', None)
A:spacy.lang.ja.__init__.path->util.ensure_path(path)
A:spacy.lang.ja.__init__.config->load_config_from_str(DEFAULT_CONFIG)
A:spacy.lang.ja.__init__.DetailedToken->namedtuple('DetailedToken', ['surface', 'tag', 'inf', 'lemma', 'norm', 'reading', 'sub_tokens'])
A:spacy.lang.ja.__init__.tok->sudachipy.dictionary.Dictionary().create(mode=split_mode)
A:spacy.lang.ja.__init__.word_start->text[text_pos:].index(word)
spacy.lang.ja.__init__.Japanese(Language)
spacy.lang.ja.__init__.JapaneseDefaults(BaseDefaults)
spacy.lang.ja.__init__.JapaneseTokenizer(self,vocab:Vocab,split_mode:Optional[str]=None)
spacy.lang.ja.__init__.JapaneseTokenizer.__init__(self,vocab:Vocab,split_mode:Optional[str]=None)
spacy.lang.ja.__init__.JapaneseTokenizer.__reduce__(self)
spacy.lang.ja.__init__.JapaneseTokenizer._get_config(self)->Dict[str, Any]
spacy.lang.ja.__init__.JapaneseTokenizer._get_dtokens(self,sudachipy_tokens,need_sub_tokens:bool=True)
spacy.lang.ja.__init__.JapaneseTokenizer._get_sub_tokens(self,sudachipy_tokens)
spacy.lang.ja.__init__.JapaneseTokenizer._set_config(self,config:Dict[str,Any]={})->None
spacy.lang.ja.__init__.JapaneseTokenizer.from_bytes(self,data:bytes,**kwargs)->'JapaneseTokenizer'
spacy.lang.ja.__init__.JapaneseTokenizer.from_disk(self,path:Union[str,Path],**kwargs)->'JapaneseTokenizer'
spacy.lang.ja.__init__.JapaneseTokenizer.score(self,examples)
spacy.lang.ja.__init__.JapaneseTokenizer.to_bytes(self,**kwargs)->bytes
spacy.lang.ja.__init__.JapaneseTokenizer.to_disk(self,path:Union[str,Path],**kwargs)->None
spacy.lang.ja.__init__.create_tokenizer(split_mode:Optional[str]=None)
spacy.lang.ja.__init__.get_dtokens_and_spaces(dtokens,text,gap_tag='空白')
spacy.lang.ja.__init__.make_morphologizer(nlp:Language,model:Model,name:str,overwrite:bool,extend:bool,scorer:Optional[Callable])
spacy.lang.ja.__init__.resolve_pos(orth,tag,next_tag)
spacy.lang.ja.__init__.try_sudachi_import(split_mode='A')


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ja/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ja/tag_orth_map.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ja/syntax_iterators.py----------------------------------------
A:spacy.lang.ja.syntax_iterators.np_label->doc.vocab.strings.add('NP')
spacy.lang.ja.syntax_iterators.noun_chunks(doclike:Union[Doc,Span])->Iterator[Tuple[int, int, int]]


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ja/tag_bigram_map.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ja/stop_words.py----------------------------------------
A:spacy.lang.ja.stop_words.STOP_WORDS->set('\nあ あっ あまり あり ある あるいは あれ\nい いい いう いく いずれ いっ いつ いる いわ\nうち\nえ\nお おい おけ および おら おり\nか かけ かつ かつて かなり から が\nき きっかけ\nくる くん\nこ こう ここ こと この これ ご ごと\nさ さらに さん\nし しか しかし しまう しまっ しよう\nす すぐ すべて する ず\nせ せい せる\nそう そこ そして その それ それぞれ\nた たい ただし たち ため たら たり だ だけ だっ\nち ちゃん\nつ つい つけ つつ\nて で でき できる です\nと とき ところ とっ とも どう\nな ない なお なかっ ながら なく なけれ なし なっ など なら なり なる\nに にて\nぬ\nね\nの のち のみ\nは はじめ ば\nひと\nぶり\nへ べき\nほか ほとんど ほど ほぼ\nま ます また まで まま\nみ\nも もう もっ もと もの\nや やっ\nよ よう よく よっ より よる よれ\nら らしい られ られる\nる\nれ れる\nを\nん\n一\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ja/tag_map.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/am/__init__.py----------------------------------------
A:spacy.lang.am.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.am.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS, TOKENIZER_EXCEPTIONS)
spacy.lang.am.__init__.Amharic(Language)
spacy.lang.am.__init__.AmharicDefaults(BaseDefaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/am/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/am/lex_attrs.py----------------------------------------
A:spacy.lang.am.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.am.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
A:spacy.lang.am.lex_attrs.text_lower->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').lower()
spacy.lang.am.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/am/tokenizer_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/am/stop_words.py----------------------------------------
A:spacy.lang.am.stop_words.STOP_WORDS->set('\nግን አንቺ አንተ እናንተ ያንተ ያንቺ የናንተ ራስህን ራስሽን ራሳችሁን\nሁሉ ኋላ በሰሞኑ አሉ በኋላ ሁኔታ በኩል አስታውቀዋል ሆነ በውስጥ\nአስታውሰዋል ሆኑ ባጣም እስካሁን ሆኖም በተለይ አሳሰበ ሁል በተመለከተ\nአሳስበዋል ላይ በተመሳሳይ አስፈላጊ ሌላ የተለያየ አስገነዘቡ ሌሎች የተለያዩ\nአስገንዝበዋል ልዩ ተባለ አብራርተዋል መሆኑ ተገለጸ አስረድተዋል  ተገልጿል\nማለቱ ተጨማሪ እባክህ የሚገኝ ተከናወነ እባክሽ ማድረግ ችግር አንጻር ማን\nትናንት እስኪደርስ ነበረች እንኳ ሰሞኑን ነበሩ እንኳን ሲሆን ነበር እዚሁ ሲል\nነው እንደገለጹት አለ ና እንደተናገሩት ቢሆን ነገር እንዳስረዱት ብለዋል ነገሮች\nእንደገና ብዙ ናት ወቅት ቦታ ናቸው እንዲሁም በርካታ አሁን እንጂ እስከ\nማለት የሚሆኑት ስለማናቸውም ውስጥ ይሆናሉ ሲባል ከሆነው ስለዚሁ ከአንድ\nያልሆነ ሳለ የነበረውን ከአንዳንድ በማናቸውም በሙሉ የሆነው ያሉ በእነዚሁ\nወር መሆናቸው ከሌሎች በዋና አንዲት ወይም\nበላይ እንደ በማቀድ ለሌሎች በሆኑ ቢሆንም ጊዜና  ይሆኑበታል በሆነ አንዱ\nለዚህ ለሆነው ለነዚህ ከዚህ የሌላውን ሶስተኛ አንዳንድ ለማንኛውም የሆነ ከሁለት\nየነገሩ ሰኣት አንደኛ እንዲሆን እንደነዚህ ማንኛውም ካልሆነ የሆኑት  ጋር ቢያንስ\nይህንንም እነደሆነ እነዚህን ይኸው  የማናቸውም\nበሙሉም ይህችው በተለይም አንዱን የሚችለውን በነዚህ ከእነዚህ በሌላ\nየዚሁ ከእነዚሁ ለዚሁ በሚገባ ለእያንዳንዱ የአንቀጹ ወደ ይህም ስለሆነ ወይ\nማናቸውንም ተብሎ እነዚህ መሆናቸውን የሆነችን ከአስር ሳይሆን ከዚያ የለውም\nየማይበልጥ እንደሆነና እንዲሆኑ  በሚችሉ ብቻ ብሎ ከሌላ የሌላቸውን\nለሆነ በሌሎች ሁለቱንም በቀር ይህ በታች አንደሆነ በነሱ\nይህን የሌላ እንዲህ ከሆነ ያላቸው በነዚሁ በሚል የዚህ ይህንኑ\nበእንደዚህ ቁጥር ማናቸውም ሆነው ባሉ በዚህ በስተቀር ሲሆንና\nበዚህም መሆን ምንጊዜም እነዚህም በዚህና ያለ ስም\nሲኖር ከዚህም መሆኑን በሁኔታው የማያንስ እነዚህኑ ማንም ከነዚሁ\nያላቸውን እጅግ ሲሆኑ ለሆኑ ሊሆን  ለማናቸውም\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/am/punctuation.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/lv/__init__.py----------------------------------------
spacy.lang.lv.__init__.Latvian(Language)
spacy.lang.lv.__init__.LatvianDefaults(BaseDefaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/lv/stop_words.py----------------------------------------
A:spacy.lang.lv.stop_words.STOP_WORDS->set('\naiz\nap\napakš\napakšpus\nar\narī\naugšpus\nbet\nbez\nbija\nbiji\nbiju\nbijām\nbijāt\nbūs\nbūsi\nbūsiet\nbūsim\nbūt\nbūšu\ncaur\ndiemžēl\ndiezin\ndroši\ndēļ\nesam\nesat\nesi\nesmu\ngan\ngar\niekam\niekams\niekām\niekāms\niekš\niekšpus\nik\nir\nit\nitin\niz\nja\njau\njeb\njebšu\njel\njo\njā\nka\nkamēr\nkaut\nkolīdz\nkopš\nkā\nkļuva\nkļuvi\nkļuvu\nkļuvām\nkļuvāt\nkļūs\nkļūsi\nkļūsiet\nkļūsim\nkļūst\nkļūstam\nkļūstat\nkļūsti\nkļūstu\nkļūt\nkļūšu\nlabad\nlai\nlejpus\nlīdz\nlīdzko\nne\nnebūt\nnedz\nnekā\nnevis\nnezin\nno\nnu\nnē\notrpus\npa\npar\npat\npie\npirms\npret\npriekš\npār\npēc\nstarp\ntad\ntak\ntapi\ntaps\ntapsi\ntapsiet\ntapsim\ntapt\ntapāt\ntapšu\ntaču\nte\ntiec\ntiek\ntiekam\ntiekat\ntieku\ntik\ntika\ntikai\ntiki\ntikko\ntiklab\ntiklīdz\ntiks\ntiksiet\ntiksim\ntikt\ntiku\ntikvien\ntikām\ntikāt\ntikšu\ntomēr\ntopat\nturpretim\nturpretī\ntā\ntādēļ\ntālab\ntāpēc\nun\nuz\nvai\nvar\nvarat\nvarēja\nvarēji\nvarēju\nvarējām\nvarējāt\nvarēs\nvarēsi\nvarēsiet\nvarēsim\nvarēt\nvarēšu\nvien\nvirs\nvirspus\nvis\nviņpus\nzem\nārpus\nšaipus\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/et/__init__.py----------------------------------------
spacy.lang.et.__init__.Estonian(Language)
spacy.lang.et.__init__.EstonianDefaults(BaseDefaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/et/stop_words.py----------------------------------------
A:spacy.lang.et.stop_words.STOP_WORDS->set('\naga\nei\net\nja\njah\nkas\nkui\nkõik\nma\nme\nmida\nmidagi\nmind\nminu\nmis\nmu\nmul\nmulle\nnad\nnii\noled\nolen\noli\noma\non\npole\nsa\nseda\nsee\nselle\nsiin\nsiis\nta\nte\nära\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ne/__init__.py----------------------------------------
spacy.lang.ne.__init__.Nepali(Language)
spacy.lang.ne.__init__.NepaliDefaults(BaseDefaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ne/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ne/lex_attrs.py----------------------------------------
A:spacy.lang.ne.lex_attrs.length->len(suffix_group[0])
A:spacy.lang.ne.lex_attrs.text->text.replace(', ', '').replace('.', '').replace(', ', '').replace('.', '')
A:spacy.lang.ne.lex_attrs.(num, denom)->text.replace(', ', '').replace('.', '').replace(', ', '').replace('.', '').split('/')
spacy.lang.ne.lex_attrs.like_num(text)
spacy.lang.ne.lex_attrs.norm(string)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ne/stop_words.py----------------------------------------
A:spacy.lang.ne.stop_words.STOP_WORDS->set('\nअक्सर\nअगाडि\nअगाडी\nअघि\nअझै\nअठार\nअथवा\nअनि\nअनुसार\nअन्तर्गत\nअन्य\nअन्यत्र\nअन्यथा\nअब\nअरु\nअरुलाई\nअरू\nअर्को\nअर्थात\nअर्थात्\nअलग\nअलि\nअवस्था\nअहिले\nआए\nआएका\nआएको\nआज\nआजको\nआठ\nआत्म\nआदि\nआदिलाई\nआफनो\nआफू\nआफूलाई\nआफै\nआफैँ\nआफ्नै\nआफ्नो\nआयो\nउ\nउक्त\nउदाहरण\nउनको\nउनलाई\nउनले\nउनि\nउनी\nउनीहरुको\nउन्नाइस\nउप\nउसको\nउसलाई\nउसले\nउहालाई\nऊ\nएउटा\nएउटै\nएक\nएकदम\nएघार\nओठ\nऔ\nऔं\nकता\nकति\nकतै\nकम\nकमसेकम\nकसरि\nकसरी\nकसै\nकसैको\nकसैलाई\nकसैले\nकसैसँग\nकस्तो\nकहाँबाट\nकहिलेकाहीं\nका\nकाम\nकारण\nकि\nकिन\nकिनभने\nकुन\nकुनै\nकुन्नी\nकुरा\nकृपया\nके\nकेहि\nकेही\nको\nकोहि\nकोहिपनि\nकोही\nकोहीपनि\nक्रमशः\nगए\nगएको\nगएर\nगयौ\nगरि\nगरी\nगरे\nगरेका\nगरेको\nगरेर\nगरौं\nगर्छ\nगर्छन्\nगर्छु\nगर्दा\nगर्दै\nगर्न\nगर्नु\nगर्नुपर्छ\nगर्ने\nगैर\nघर\nचार\nचाले\nचाहनुहुन्छ\nचाहन्छु\nचाहिं\nचाहिए\nचाहिंले\nचाहीं\nचाहेको\nचाहेर\nचोटी\nचौथो\nचौध\nछ\nछन\nछन्\nछु\nछू\nछैन\nछैनन्\nछौ\nछौं\nजता\nजताततै\nजना\nजनाको\nजनालाई\nजनाले\nजब\nजबकि\nजबकी\nजसको\nजसबाट\nजसमा\nजसरी\nजसलाई\nजसले\nजस्ता\nजस्तै\nजस्तो\nजस्तोसुकै\nजहाँ\nजान\nजाने\nजाहिर\nजुन\nजुनै\nजे\nजो\nजोपनि\nजोपनी\nझैं\nठाउँमा\nठीक\nठूलो\nत\nतता\nतत्काल\nतथा\nतथापि\nतथापी\nतदनुसार\nतपाइ\nतपाई\nतपाईको\nतब\nतर\nतर्फ\nतल\nतसरी\nतापनि\nतापनी\nतिन\nतिनि\nतिनिहरुलाई\nतिनी\nतिनीहरु\nतिनीहरुको\nतिनीहरू\nतिनीहरूको\nतिनै\nतिमी\nतिर\nतिरको\nती\nतीन\nतुरन्त\nतुरुन्त\nतुरुन्तै\nतेश्रो\nतेस्कारण\nतेस्रो\nतेह्र\nतैपनि\nतैपनी\nत्यत्तिकै\nत्यत्तिकैमा\nत्यस\nत्यसकारण\nत्यसको\nत्यसले\nत्यसैले\nत्यसो\nत्यस्तै\nत्यस्तो\nत्यहाँ\nत्यहिँ\nत्यही\nत्यहीँ\nत्यहीं\nत्यो\nत्सपछि\nत्सैले\nथप\nथरि\nथरी\nथाहा\nथिए\nथिएँ\nथिएन\nथियो\nदर्ता\nदश\nदिए\nदिएको\nदिन\nदिनुभएको\nदिनुहुन्छ\nदुइ\nदुइवटा\nदुई\nदेखि\nदेखिन्छ\nदेखियो\nदेखे\nदेखेको\nदेखेर\nदोश्री\nदोश्रो\nदोस्रो\nद्वारा\nधन्न\nधेरै\nधौ\nन\nनगर्नु\nनगर्नू\nनजिकै\nनत्र\nनत्रभने\nनभई\nनभएको\nनभनेर\nनयाँ\nनि\nनिकै\nनिम्ति\nनिम्न\nनिम्नानुसार\nनिर्दिष्ट\nनै\nनौ\nपक्का\nपक्कै\nपछाडि\nपछाडी\nपछि\nपछिल्लो\nपछी\nपटक\nपनि\nपन्ध्र\nपर्छ\nपर्थ्यो\nपर्दैन\nपर्ने\nपर्नेमा\nपर्याप्त\nपहिले\nपहिलो\nपहिल्यै\nपाँच\nपांच\nपाचौँ\nपाँचौं\nपिच्छे\nपूर्व\nपो\nप्रति\nप्रतेक\nप्रत्यक\nप्राय\nप्लस\nफरक\nफेरि\nफेरी\nबढी\nबताए\nबने\nबरु\nबाट\nबारे\nबाहिर\nबाहेक\nबाह्र\nबिच\nबिचमा\nबिरुद्ध\nबिशेष\nबिस\nबीच\nबीचमा\nबीस\nभए\nभएँ\nभएका\nभएकालाई\nभएको\nभएन\nभएर\nभन\nभने\nभनेको\nभनेर\nभन्\nभन्छन्\nभन्छु\nभन्दा\nभन्दै\nभन्नुभयो\nभन्ने\nभन्या\nभयेन\nभयो\nभर\nभरि\nभरी\nभा\nभित्र\nभित्री\nभीत्र\nम\nमध्य\nमध्ये\nमलाई\nमा\nमात्र\nमात्रै\nमाथि\nमाथी\nमुख्य\nमुनि\nमुन्तिर\nमेरो\nमैले\nयति\nयथोचित\nयदि\nयद्ध्यपि\nयद्यपि\nयस\nयसका\nयसको\nयसपछि\nयसबाहेक\nयसमा\nयसरी\nयसले\nयसो\nयस्तै\nयस्तो\nयहाँ\nयहाँसम्म\nयही\nया\nयी\nयो\nर\nरही\nरहेका\nरहेको\nरहेछ\nराखे\nराख्छ\nराम्रो\nरुपमा\nरूप\nरे\nलगभग\nलगायत\nलाई\nलाख\nलागि\nलागेको\nले\nवटा\nवरीपरी\nवा\nवाट\nवापत\nवास्तवमा\nशायद\nसक्छ\nसक्ने\nसँग\nसंग\nसँगको\nसँगसँगै\nसँगै\nसंगै\nसङ्ग\nसङ्गको\nसट्टा\nसत्र\nसधै\nसबै\nसबैको\nसबैलाई\nसमय\nसमेत\nसम्भव\nसम्म\nसय\nसरह\nसहित\nसहितै\nसही\nसाँच्चै\nसात\nसाथ\nसाथै\nसायद\nसारा\nसुनेको\nसुनेर\nसुरु\nसुरुको\nसुरुमै\nसो\nसोचेको\nसोचेर\nसोही\nसोह्र\nस्थित\nस्पष्ट\nहजार\nहरे\nहरेक\nहामी\nहामीले\nहाम्रा\nहाम्रो\nहुँदैन\nहुन\nहुनत\nहुनु\nहुने\nहुनेछ\nहुन्\nहुन्छ\nहुन्थ्यो\nहैन\nहो\nहोइन\nहोकि\nहोला\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/af/__init__.py----------------------------------------
spacy.lang.af.__init__.Afrikaans(Language)
spacy.lang.af.__init__.AfrikaansDefaults(BaseDefaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/af/stop_words.py----------------------------------------
A:spacy.lang.af.stop_words.STOP_WORDS->set("\n'n\naan\naf\nal\nas\nbaie\nby\ndaar\ndag\ndat\ndie\ndit\neen\nek\nen\ngaan\ngesê\nhaar\nhet\nhom\nhulle\nhy\nin\nis\njou\njy\nkan\nkom\nma\nmaar\nmet\nmy\nna\nnie\nom\nons\nop\nsaam\nsal\nse\nsien\nso\nsy\nte\ntoe\nuit\nvan\nvir\nwas\nwat\nŉ\n".split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/tl/__init__.py----------------------------------------
spacy.lang.tl.__init__.Tagalog(Language)
spacy.lang.tl.__init__.TagalogDefaults(BaseDefaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/tl/lex_attrs.py----------------------------------------
A:spacy.lang.tl.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.tl.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.tl.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/tl/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.tl.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/tl/stop_words.py----------------------------------------
A:spacy.lang.tl.stop_words.STOP_WORDS->set('\nakin\naking\nako\nalin\nam\namin\naming\nang\nano\nanumang\napat\nat\natin\nating\nay\nbababa\nbago\nbakit\nbawat\nbilang\ndahil\ndalawa\ndapat\ndin\ndito\ndoon\ngagawin\ngayunman\nginagawa\nginawa\nginawang\ngumawa\ngusto\nhabang\nhanggang\nhindi\nhuwag\niba\nibaba\nibabaw\nibig\nikaw\nilagay\nilalim\nilan\ninyong\nisa\nisang\nitaas\nito\niyo\niyon\niyong\nka\nkahit\nkailangan\nkailanman\nkami\nkanila\nkanilang\nkanino\nkanya\nkanyang\nkapag\nkapwa\nkaramihan\nkatiyakan\nkatulad\nkaya\nkaysa\nko\nkong\nkulang\nkumuha\nkung\nlaban\nlahat\nlamang\nlikod\nlima\nmaaari\nmaaaring\nmaging\nmahusay\nmakita\nmarami\nmarapat\nmasyado\nmay\nmayroon\nmga\nminsan\nmismo\nmula\nmuli\nna\nnabanggit\nnaging\nnagkaroon\nnais\nnakita\nnamin\nnapaka\nnarito\nnasaan\nng\nngayon\nni\nnila\nnilang\nnito\nniya\nniyang\nnoon\no\npa\npaano\npababa\npaggawa\npagitan\npagkakaroon\npagkatapos\npalabas\npamamagitan\npanahon\npangalawa\npara\nparaan\npareho\npataas\npero\npumunta\npumupunta\nsa\nsaan\nsabi\nsabihin\nsarili\nsila\nsino\nsiya\ntatlo\ntayo\ntulad\ntungkol\nuna\nwalang\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/el/get_pos_from_wiktionary.py----------------------------------------
A:spacy.lang.el.get_pos_from_wiktionary.regex->re.compile('==={{(\\w+)\\|el}}===')
A:spacy.lang.el.get_pos_from_wiktionary.regex2->re.compile('==={{(\\w+ \\w+)\\|el}}===')
A:spacy.lang.el.get_pos_from_wiktionary.title->title.lower().lower()
A:spacy.lang.el.get_pos_from_wiktionary.all_regex->re.compile('==={{(\\w+)\\|el}}===').findall(text)
A:spacy.lang.el.get_pos_from_wiktionary.words->sorted(expected_parts_dict[i])
spacy.lang.el.get_pos_from_wiktionary.get_pos_from_wiktionary()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/el/__init__.py----------------------------------------
spacy.lang.el.__init__.Greek(Language)
spacy.lang.el.__init__.GreekDefaults(BaseDefaults)
spacy.lang.el.__init__.make_lemmatizer(nlp:Language,model:Optional[Model],name:str,mode:str,overwrite:bool,scorer:Optional[Callable])


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/el/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/el/lemmatizer.py----------------------------------------
A:spacy.lang.el.lemmatizer.univ_pos->token.pos_.lower()
A:spacy.lang.el.lemmatizer.index_table->self.lookups.get_table('lemma_index', {})
A:spacy.lang.el.lemmatizer.exc_table->self.lookups.get_table('lemma_exc', {})
A:spacy.lang.el.lemmatizer.rules_table->self.lookups.get_table('lemma_rules', {})
A:spacy.lang.el.lemmatizer.index->self.lookups.get_table('lemma_index', {}).get(univ_pos, {})
A:spacy.lang.el.lemmatizer.exceptions->self.lookups.get_table('lemma_exc', {}).get(univ_pos, {})
A:spacy.lang.el.lemmatizer.rules->self.lookups.get_table('lemma_rules', {}).get(univ_pos, {})
A:spacy.lang.el.lemmatizer.string->string.lower().lower()
A:spacy.lang.el.lemmatizer.forms->list(dict.fromkeys(forms))
spacy.lang.el.GreekLemmatizer(Lemmatizer)
spacy.lang.el.GreekLemmatizer.rule_lemmatize(self,token:Token)->List[str]
spacy.lang.el.lemmatizer.GreekLemmatizer(Lemmatizer)
spacy.lang.el.lemmatizer.GreekLemmatizer.rule_lemmatize(self,token:Token)->List[str]


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/el/lex_attrs.py----------------------------------------
A:spacy.lang.el.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.el.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('^')
spacy.lang.el.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/el/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.el.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/el/syntax_iterators.py----------------------------------------
A:spacy.lang.el.syntax_iterators.conj->doc.vocab.strings.add('conj')
A:spacy.lang.el.syntax_iterators.nmod->doc.vocab.strings.add('nmod')
A:spacy.lang.el.syntax_iterators.np_label->doc.vocab.strings.add('NP')
spacy.lang.el.syntax_iterators.noun_chunks(doclike:Union[Doc,Span])->Iterator[Tuple[int, int, int]]


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/el/stop_words.py----------------------------------------
A:spacy.lang.el.stop_words.STOP_WORDS->set('\nαδιάκοπα αι ακόμα ακόμη ακριβώς άλλα αλλά αλλαχού άλλες άλλη άλλην\nάλλης αλλιώς αλλιώτικα άλλο άλλοι αλλοιώς αλλοιώτικα άλλον άλλος άλλοτε αλλού\nάλλους άλλων άμα άμεσα αμέσως αν ανά ανάμεσα αναμεταξύ άνευ αντί αντίπερα αντίς\nάνω ανωτέρω άξαφνα απ απέναντι από απόψε άρα άραγε αρκετά αρκετές\nαρχικά ας αύριο αυτά αυτές αυτή αυτήν αυτής αυτό αυτοί αυτόν αυτός αυτού αυτούς\nαυτών αφότου αφού\n\nβέβαια βεβαιότατα\n\nγι για γιατί γρήγορα γύρω\n\nδα δε δείνα δεν δεξιά δήθεν δηλαδή δι δια διαρκώς δικά δικό δικοί δικός δικού\nδικούς διόλου δίπλα δίχως\n\nεάν εαυτό εαυτόν εαυτού εαυτούς εαυτών έγκαιρα εγκαίρως εγώ εδώ ειδεμή είθε είμαι\nείμαστε είναι εις είσαι είσαστε είστε είτε είχα είχαμε είχαν είχατε είχε είχες έκαστα\nέκαστες έκαστη έκαστην έκαστης έκαστο έκαστοι έκαστον έκαστος εκάστου εκάστους εκάστων\nεκεί εκείνα εκείνες εκείνη εκείνην εκείνης εκείνο εκείνοι εκείνον εκείνος εκείνου\nεκείνους εκείνων εκτός εμάς εμείς εμένα εμπρός εν ένα έναν ένας ενός εντελώς εντός\nεναντίον  εξής  εξαιτίας  επιπλέον επόμενη εντωμεταξύ ενώ εξ έξαφνα εξήσ εξίσου έξω επάνω\nεπειδή έπειτα επί επίσης επομένως εσάς εσείς εσένα έστω εσύ ετέρα ετέραι ετέρας έτερες\nέτερη έτερης έτερο έτεροι έτερον έτερος ετέρου έτερους ετέρων ετούτα ετούτες ετούτη ετούτην\nετούτης ετούτο ετούτοι ετούτον ετούτος ετούτου ετούτους ετούτων έτσι εύγε ευθύς ευτυχώς εφεξής\nέχει έχεις έχετε έχομε έχουμε έχουν εχτές έχω έως έγιναν  έγινε  έκανε  έξι  έχοντας\n\nη ήδη ήμασταν ήμαστε ήμουν ήσασταν ήσαστε ήσουν ήταν ήτανε ήτοι ήττον\n\nθα\n\nι ιδία ίδια ίδιαν ιδίας ίδιες ίδιο ίδιοι ίδιον ίδιοσ ίδιος ιδίου ίδιους ίδιων ιδίως ιι ιιι\nίσαμε ίσια ίσως\n\nκάθε καθεμία καθεμίας καθένα καθένας καθενός καθετί καθόλου καθώς και κακά κακώς καλά\nκαλώς καμία καμίαν καμίας κάμποσα κάμποσες κάμποση κάμποσην κάμποσης κάμποσο κάμποσοι\nκάμποσον κάμποσος κάμποσου κάμποσους κάμποσων κανείς κάνεν κανένα κανέναν κανένας\nκανενός κάποια κάποιαν κάποιας κάποιες κάποιο κάποιοι κάποιον κάποιος κάποιου κάποιους\nκάποιων κάποτε κάπου κάπως κατ κατά κάτι κατιτί κατόπιν κάτω κιόλας κλπ κοντά κτλ κυρίως\n\nλιγάκι λίγο λιγότερο λόγω λοιπά λοιπόν\n\nμα μαζί μακάρι μακρυά μάλιστα μάλλον μας με μεθαύριο μείον μέλει μέλλεται μεμιάς μεν\nμερικά μερικές μερικοί μερικούς μερικών μέσα μετ μετά μεταξύ μέχρι μη μήδε μην μήπως\nμήτε μια μιαν μιας μόλις μολονότι μονάχα μόνες μόνη μόνην μόνης μόνο μόνοι μονομιάς\nμόνος μόνου μόνους μόνων μου μπορεί μπορούν μπρος μέσω  μία  μεσώ\n\nνα ναι νωρίς\n\nξανά ξαφνικά\n\nο οι όλα όλες όλη όλην όλης όλο ολόγυρα όλοι όλον ολονέν όλος ολότελα όλου όλους όλων\nόλως ολωσδιόλου όμως όποια οποιαδήποτε οποίαν οποιανδήποτε οποίας οποίος οποιασδήποτε οποιδήποτε\nόποιες οποιεσδήποτε όποιο οποιοδηήποτε όποιοι όποιον οποιονδήποτε όποιος οποιοσδήποτε\nοποίου οποιουδήποτε οποίους οποιουσδήποτε οποίων οποιωνδήποτε όποτε οποτεδήποτε όπου\nοπουδήποτε όπως ορισμένα ορισμένες ορισμένων ορισμένως όσα οσαδήποτε όσες οσεσδήποτε\nόση οσηδήποτε όσην οσηνδήποτε όσης οσησδήποτε όσο οσοδήποτε όσοι οσοιδήποτε όσον οσονδήποτε\nόσος οσοσδήποτε όσου οσουδήποτε όσους οσουσδήποτε όσων οσωνδήποτε όταν ότι οτιδήποτε\nότου ου ουδέ ούτε όχι οποία  οποίες  οποίο  οποίοι  οπότε  ος\n\nπάνω  παρά  περί  πολλά  πολλές  πολλοί  πολλούς  που  πρώτα  πρώτες  πρώτη  πρώτο  πρώτος  πως\nπάλι πάντα πάντοτε παντού πάντως πάρα πέρα πέρι περίπου περισσότερο πέρσι πέρυσι πια πιθανόν\nπιο πίσω πλάι πλέον πλην ποιά ποιάν ποιάς ποιές ποιό ποιοί ποιόν ποιός ποιού ποιούς\nποιών πολύ πόσες πόση πόσην πόσης πόσοι πόσος πόσους πότε ποτέ πού πούθε πουθενά πρέπει\nπριν προ προκειμένου πρόκειται πρόπερσι προς προτού προχθές προχτές πρωτύτερα πώς\n\nσαν σας σε σεις σου στα στη στην στης στις στο στον στου στους στων συγχρόνως\nσυν συνάμα συνεπώς συχνάς συχνές συχνή συχνήν συχνής συχνό συχνοί συχνόν\nσυχνός συχνού συχνούς συχνών συχνώς σχεδόν\n\nτα τάδε ταύτα ταύτες ταύτη ταύτην ταύτης ταύτοταύτον ταύτος ταύτου ταύτων τάχα τάχατε\nτελευταία  τελευταίο  τελευταίος  τού  τρία  τρίτη  τρεις τελικά τελικώς τες τέτοια τέτοιαν\nτέτοιας τέτοιες τέτοιο τέτοιοι τέτοιον τέτοιος τέτοιου\nτέτοιους τέτοιων τη την της τι τίποτα τίποτε τις το τοι τον τοσ τόσα τόσες τόση τόσην\nτόσης τόσο τόσοι τόσον τόσος τόσου τόσους τόσων τότε του τουλάχιστο τουλάχιστον τους τούς τούτα\nτούτες τούτη τούτην τούτης τούτο τούτοι τούτοις τούτον τούτος τούτου τούτους τούτων τυχόν\nτων τώρα\n\nυπ υπέρ υπό υπόψη υπόψιν ύστερα\n\nχωρίς χωριστά\n\nω ως ωσάν ωσότου ώσπου ώστε ωστόσο ωχ\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/el/punctuation.py----------------------------------------
A:spacy.lang.el.punctuation.UNITS->merge_chars(_units)
spacy.lang.el.punctuation.merge_chars(char)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/mr/__init__.py----------------------------------------
spacy.lang.mr.__init__.Marathi(Language)
spacy.lang.mr.__init__.MarathiDefaults(BaseDefaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/mr/stop_words.py----------------------------------------
A:spacy.lang.mr.stop_words.STOP_WORDS->set('\nन\nअतरी\nतो\nहें\nतें\nकां\nआणि\nजें\nजे\nमग\nते\nमी\nजो\nपरी\nगा\nहे\nऐसें\nआतां\nनाहीं\nतेथ\nहा\nतया\nअसे\nम्हणे\nकाय\nकीं\nजैसें\nतंव\nतूं\nहोय\nजैसा\nआहे\nपैं\nतैसा\nजरी\nम्हणोनि\nएक\nऐसा\nजी\nना\nमज\nएथ\nया\nजेथ\nजया\nतुज\nतेणें\nतैं\nपां\nअसो\nकरी\nऐसी\nयेणें\nजाहला\nतेंचि\nआघवें\nहोती\nकांहीं\nहोऊनि\nएकें\nमातें\nठायीं\nये\nसकळ\nकेलें\nजेणें\nजाण\nजैसी\nहोये\nजेवीं\nएऱ्हवीं\nमीचि\nकिरीटी\nदिसे\nदेवा\nहो\nतरि\nकीजे\nतैसे\nआपण\nतिये\nकर्म\nनोहे\nइये\nपडे\nमाझें\nतैसी\nलागे\nनाना\nजंव\nकीर\nअधिक\nअनेक\nअशी\nअसलयाचे\nअसलेल्या\nअसा\nअसून\nअसे\nआज\nआणि\nआता\nआपल्या\nआला\nआली\nआले\nआहे\nआहेत\nएक\nएका\nकमी\nकरणयात\nकरून\nका\nकाम\nकाय\nकाही\nकिवा\nकी\nकेला\nकेली\nकेले\nकोटी\nगेल्या\nघेऊन\nजात\nझाला\nझाली\nझाले\nझालेल्या\nटा\nतर\nतरी\nतसेच\nता\nती\nतीन\nते\nतो\nत्या\nत्याचा\nत्याची\nत्याच्या\nत्याना\nत्यानी\nत्यामुळे\nत्री\nदिली\nदोन\nन\nपण\nपम\nपरयतन\nपाटील\nम\nमात्र\nमाहिती\nमी\nमुबी\nम्हणजे\nम्हणाले\nम्हणून\nया\nयाचा\nयाची\nयाच्या\nयाना\nयानी\nयेणार\nयेत\nयेथील\nयेथे\nलाख\nव\nव्यकत\nसर्व\nसागित्ले\nसुरू\nहजार\nहा\nही\nहे\nहोणार\nहोत\nहोता\nहोती\nहोते\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/tn/__init__.py----------------------------------------
spacy.lang.tn.__init__.Setswana(Language)
spacy.lang.tn.__init__.SetswanaDefaults(BaseDefaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/tn/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/tn/lex_attrs.py----------------------------------------
A:spacy.lang.tn.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.tn.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
A:spacy.lang.tn.lex_attrs.text_lower->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').lower()
spacy.lang.tn.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/tn/stop_words.py----------------------------------------
A:spacy.lang.tn.stop_words.STOP_WORDS->set('\nke gareng ga selekanyo tlhwatlhwa yo mongwe se\nsengwe fa go le jalo gongwe ba na mo tikologong\njaaka kwa morago nna gonne ka sa pele nako teng\ntlase fela ntle magareng tsona feta bobedi kgabaganya\nmoo gape kgatlhanong botlhe tsotlhe bokana e esi\nsetseng mororo dinako golo kgolo nnye wena gago\no ntse ntle tla goreng gangwe mang yotlhe gore\neo yona tseraganyo eng ne sentle re rona thata\ngodimo fitlha pedi masomamabedi lesomepedi mmogo\ntharo tseo boraro tseno yone jaanong bobona bona\nlesome tsaya tsamaiso nngwe masomethataro thataro\ntsa mmatota tota sale thoko supa dira tshwanetse di mmalwa masisi\nbonala e tshwanang bogolo tsenya tsweetswee karolo\nsepe tlhalosa dirwa robedi robongwe lesomenngwe gaisa\ntlhano lesometlhano botlalo lekgolo\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/tn/punctuation.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/kn/__init__.py----------------------------------------
spacy.lang.kn.__init__.Kannada(Language)
spacy.lang.kn.__init__.KannadaDefaults(BaseDefaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/kn/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/kn/stop_words.py----------------------------------------
A:spacy.lang.kn.stop_words.STOP_WORDS->set('\nಹಲವು\nಮೂಲಕ\nಹಾಗೂ\nಅದು\nನೀಡಿದ್ದಾರೆ\nಯಾವ\nಎಂದರು\nಅವರು\nಈಗ\nಎಂಬ\nಹಾಗಾಗಿ\nಅಷ್ಟೇ\nನಾವು\nಇದೇ\nಹೇಳಿ\nತಮ್ಮ\nಹೀಗೆ\nನಮ್ಮ\nಬೇರೆ\nನೀಡಿದರು\nಮತ್ತೆ\nಇದು\nಈ\nನೀವು\nನಾನು\nಇತ್ತು\nಎಲ್ಲಾ\nಯಾವುದೇ\nನಡೆದ\nಅದನ್ನು\nಎಂದರೆ\nನೀಡಿದೆ\nಹೀಗಾಗಿ\nಜೊತೆಗೆ\nಇದರಿಂದ\nನನಗೆ\nಅಲ್ಲದೆ\nಎಷ್ಟು\nಇದರ\nಇಲ್ಲ\nಕಳೆದ\nತುಂಬಾ\nಈಗಾಗಲೇ\nಮಾಡಿ\nಅದಕ್ಕೆ\nಬಗ್ಗೆ\nಅವರ\nಇದನ್ನು\nಆ\nಇದೆ\nಹೆಚ್ಚು\nಇನ್ನು\nಎಲ್ಲ\nಇರುವ\nಅವರಿಗೆ\nನಿಮ್ಮ\nಏನು\nಕೂಡ\nಇಲ್ಲಿ\nನನ್ನನ್ನು\nಕೆಲವು\nಮಾತ್ರ\nಬಳಿಕ\nಅಂತ\nತನ್ನ\nಆಗ\nಅಥವಾ\nಅಲ್ಲ\nಕೇವಲ\nಆದರೆ\nಮತ್ತು\nಇನ್ನೂ\nಅದೇ\nಆಗಿ\nಅವರನ್ನು\nಹೇಳಿದ್ದಾರೆ\nನಡೆದಿದೆ\nಇದಕ್ಕೆ\nಎಂಬುದು\nಎಂದು\nನನ್ನ\nಮೇಲೆ\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/mk/__init__.py----------------------------------------
A:spacy.lang.mk.__init__.lex_attr_getters->dict(Language.Defaults.lex_attr_getters)
A:spacy.lang.mk.__init__.tokenizer_exceptions->update_exc(BASE_EXCEPTIONS, TOKENIZER_EXCEPTIONS)
spacy.lang.mk.__init__.Macedonian(Language)
spacy.lang.mk.__init__.MacedonianDefaults(BaseDefaults)
spacy.lang.mk.__init__.make_lemmatizer(nlp:Language,model:Optional[Model],name:str,mode:str,overwrite:bool,scorer:Optional[Callable])


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/mk/lemmatizer.py----------------------------------------
A:spacy.lang.mk.lemmatizer.univ_pos->token.pos_.lower()
A:spacy.lang.mk.lemmatizer.index_table->self.lookups.get_table('lemma_index', {})
A:spacy.lang.mk.lemmatizer.exc_table->self.lookups.get_table('lemma_exc', {})
A:spacy.lang.mk.lemmatizer.rules_table->self.lookups.get_table('lemma_rules', {})
A:spacy.lang.mk.lemmatizer.index->self.lookups.get_table('lemma_index', {}).get(univ_pos, {})
A:spacy.lang.mk.lemmatizer.exceptions->self.lookups.get_table('lemma_exc', {}).get(univ_pos, {})
A:spacy.lang.mk.lemmatizer.rules->self.lookups.get_table('lemma_rules', {}).get(univ_pos, [])
A:spacy.lang.mk.lemmatizer.string->string.lower().lower()
A:spacy.lang.mk.lemmatizer.forms->list(OrderedDict.fromkeys(forms))
spacy.lang.mk.MacedonianLemmatizer(Lemmatizer)
spacy.lang.mk.MacedonianLemmatizer.rule_lemmatize(self,token:Token)->List[str]
spacy.lang.mk.lemmatizer.MacedonianLemmatizer(Lemmatizer)
spacy.lang.mk.lemmatizer.MacedonianLemmatizer.rule_lemmatize(self,token:Token)->List[str]


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/mk/lex_attrs.py----------------------------------------
A:spacy.lang.mk.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.mk.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
A:spacy.lang.mk.lex_attrs.text_lower->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').lower()
spacy.lang.mk.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/mk/tokenizer_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/mk/stop_words.py----------------------------------------
A:spacy.lang.mk.stop_words.STOP_WORDS->set('\nа\nабре\naв\nаи\nако\nалало\nам\nама\nаман\nами\nамин\nаприли-ли-ли\nау\nаух\nауч\nах\nаха\nаха-ха\nаш\nашколсум\nашколсун\nај\nајде\nајс\nаџаба\nбавно\nбам\nбам-бум\nбап\nбар\nбаре\nбарем\nбау\nбау-бау\nбаш\nбај\nбе\nбеа\nбев\nбевме\nбевте\nбез\nбезбели\nбездруго\nбелки\nбеше\nби\nбидејќи\nбим\nбис\nбла\nблазе\nбогами\nбожем\nбоц\nбраво\nбравос\nбре\nбреј\nбрзо\nбришка\nбррр\nбу\nбум\nбуф\nбуц\nбујрум\nваа\nвам\nварај\nварда\nвас\nвај\nве\nвелат\nвели\nверсус\nвеќе\nви\nвиа\nвиди\nвие\nвистина\nвитос\nвнатре\nво\nвоз\nвон\nвпрочем\nврв\nвред\nвреме\nврз\nвсушност\nвтор\nгалиба\nги\nгитла\nго\nгоде\nгодишник\nгоре\nгра\nгуц\nгљу\nда\nдаан\nдава\nдал\nдали\nдан\nдва\nдваесет\nдванаесет\nдвајца\nдве\nдвесте\nдвижам\nдвижат\nдвижи\nдвижиме\nдвижите\nдвижиш\nде\nдеведесет\nдевет\nдеветнаесет\nдеветстотини\nдеветти\nдека\nдел\nделми\nдемек\nдесет\nдесетина\nдесетти\nдеситици\nдејгиди\nдејди\nди\nдилми\nдин\nдип\nдно\nдо\nдоволно\nдодека\nдодуша\nдокај\nдоколку\nдоправено\nдоправи\nдосамоти\nдоста\nдржи\nдрн\nдруг\nдруга\nдругата\nдруги\nдругиот\nдругите\nдруго\nдругото\nдум\nдур\nдури\nе\nевала\nеве\nевет\nега\nегиди\nеден\nедикојси\nединаесет\nединствено\nеднаш\nедно\nексик\nела\nелбете\nелем\nели\nем\nеми\nене\nете\nеурека\nех\nеј\nжими\nжити\nза\nзавал\nзаврши\nзад\nзадека\nзадоволна\nзадржи\nзаедно\nзар\nзарад\nзаради\nзаре\nзарем\nзатоа\nзашто\nзгора\nзема\nземе\nземува\nзер\nзначи\nзошто\nзуј\nи\nиако\nиз\nизвезен\nизгледа\nизмеѓу\nизнос\nили\nили-или\nилјада\nилјади\nим\nима\nимаа\nимаат\nимавме\nимавте\nимам\nимаме\nимате\nимаш\nимаше\nиме\nимено\nименува\nимплицира\nимплицираат\nимплицирам\nимплицираме\nимплицирате\nимплицираш\nинаку\nиндицира\nисечок\nисклучен\nисклучена\nисклучени\nисклучено\nискористен\nискористена\nискористени\nискористено\nискористи\nискрај\nисти\nисто\nитака\nитн\nих\nиха\nихуу\nиш\nишала\nиј\nка\nкаде\nкажува\nкако\nкаков\nкамоли\nкај\nква\nки\nкит\nкло\nклум\nкога\nкого\nкого-годе\nкое\nкои\nколичество\nколичина\nколку\nкому\nкон\nкористена\nкористени\nкористено\nкористи\nкот\nкотрр\nкош-кош\nкој\nкоја\nкојзнае\nкојшто\nкр-кр-кр\nкрај\nкрек\nкрз\nкрк\nкрц\nкуку\nкукуригу\nкуш\nле\nлебами\nлеле\nлели\nли\nлиду\nлуп\nма\nмакар\nмалку\nмарш\nмат\nмац\nмашала\nме\nмене\nместо\nмеѓу\nмеѓувреме\nмеѓутоа\nми\nмое\nможе\nможеби\nмолам\nмоли\nмор\nмора\nморе\nмори\nмразец\nму\nмуклец\nмутлак\nмуц\nмјау\nна\nнавидум\nнавистина\nнад\nнадвор\nназад\nнакај\nнакрај\nнали\nнам\nнаместо\nнаоколу\nнаправено\nнаправи\nнапред\nнас\nнаспоред\nнаспрема\nнаспроти\nнасред\nнатаму\nнатема\nначин\nнаш\nнаша\nнаше\nнаши\nнај\nнајдоцна\nнајмалку\nнајмногу\nне\nнеа\nнего\nнегов\nнегова\nнегови\nнегово\nнезе\nнека\nнекаде\nнекако\nнекаков\nнекого\nнекое\nнекои\nнеколку\nнекому\nнекој\nнекојси\nнели\nнемој\nнему\nнеоти\nнечиј\nнешто\nнејзе\nнејзин\nнејзини\nнејзино\nнејсе\nни\nнив\nнивен\nнивна\nнивни\nнивно\nние\nниз\nникаде\nникако\nникогаш\nникого\nникому\nникој\nним\nнити\nнито\nниту\nничиј\nништо\nно\nнѐ\nо\nобр\nова\nова-она\nоваа\nовај\nовде\nовега\nовие\nовој\nод\nодавде\nоди\nоднесува\nодносно\nодошто\nоколу\nолеле\nолкацок\nон\nона\nонаа\nонака\nонаков\nонде\nони\nоние\nоно\nоној\nоп\nосвем\nосвен\nосем\nосми\nосум\nосумдесет\nосумнаесет\nосумстотитни\nотаде\nоти\nоткако\nоткај\nоткога\nотколку\nоттаму\nоттука\nоф\nох\nој\nпа\nпак\nпапа\nпардон\nпате-ќуте\nпати\nпау\nпаче\nпеесет\nпеки\nпет\nпетнаесет\nпетстотини\nпетти\nпи\nпи-пи\nпис\nплас\nплус\nпо\nпобавно\nпоблиску\nпобрзо\nпобуни\nповеќе\nповторно\nпод\nподалеку\nподолу\nподоцна\nподруго\nпозади\nпоинаква\nпоинакви\nпоинакво\nпоинаков\nпоинаку\nпокаже\nпокажува\nпокрај\nполно\nпомалку\nпомеѓу\nпонатаму\nпонекогаш\nпонекој\nпоради\nпоразличен\nпоразлична\nпоразлични\nпоразлично\nпоседува\nпосле\nпоследен\nпоследна\nпоследни\nпоследно\nпоспоро\nпотег\nпотоа\nпошироко\nправи\nпразно\nпрв\nпред\nпрез\nпреку\nпретежно\nпретходен\nпретходна\nпретходни\nпретходник\nпретходно\nпри\nприсвои\nпритоа\nпричинува\nпријатно\nпросто\nпротив\nпрр\nпст\nпук\nпусто\nпуф\nпуј\nпфуј\nпшт\nради\nразличен\nразлична\nразлични\nразлично\nразни\nразоружен\nразредлив\nрамките\nрамнообразно\nрастревожено\nрастреперено\nрасчувствувано\nратоборно\nрече\nроден\nс\nсакан\nсам\nсама\nсами\nсамите\nсамо\nсамоти\nсвое\nсвои\nсвој\nсвоја\nсе\nсебе\nсебеси\nсега\nседми\nседум\nседумдесет\nседумнаесет\nседумстотини\nсекаде\nсекаков\nсеки\nсекогаш\nсекого\nсекому\nсекој\nсекојдневно\nсем\nсенешто\nсепак\nсериозен\nсериозна\nсериозни\nсериозно\nсет\nсечиј\nсешто\nси\nсиктер\nсиот\nсип\nсиреч\nсите\nсичко\nскок\nскоро\nскрц\nследбеник\nследбеничка\nследен\nследователно\nследствено\nсме\nсо\nсоне\nсопствен\nсопствена\nсопствени\nсопствено\nсосе\nсосем\nсполај\nспоред\nспоро\nспрема\nспроти\nспротив\nсред\nсреде\nсреќно\nсрочен\nсст\nстава\nставаат\nставам\nставаме\nставате\nставаш\nстави\nсте\nсто\nстоп\nстрана\nсум\nсума\nсупер\nсус\nсѐ\nта\nтаа\nтака\nтаква\nтакви\nтаков\nтамам\nтаму\nтангар-мангар\nтандар-мандар\nтап\nтвое\nте\nтебе\nтебека\nтек\nтекот\nти\nтие\nтизе\nтик-так\nтики\nтоа\nтогаш\nтој\nтрак\nтрака-трука\nтрас\nтреба\nтрет\nтри\nтриесет\nтринаест\nтриста\nтруп\nтрупа\nтрус\nту\nтука\nтуку\nтукушто\nтуф\nу\nуа\nубаво\nуви\nужасно\nуз\nура\nуу\nуф\nуха\nуш\nуште\nфазен\nфала\nфил\nфилан\nфис\nфиу\nфиљан\nфоб\nфон\nха\nха-ха\nхе\nхеј\nхеј\nхи\nхм\nхо\nцак\nцап\nцелина\nцело\nцигу-лигу\nциц\nчекај\nчесто\nчетврт\nчетири\nчетириесет\nчетиринаесет\nчетирстотини\nчие\nчии\nчик\nчик-чирик\nчини\nчиш\nчиј\nчија\nчијшто\nчкрап\nчому\nчук\nчукш\nчуму\nчунки\nшеесет\nшеснаесет\nшест\nшести\nшестотини\nширум\nшлак\nшлап\nшлапа-шлупа\nшлуп\nшмрк\nшто\nштогоде\nштом\nштотуку\nштрак\nштрап\nштрап-штруп\nшуќур\nѓиди\nѓоа\nѓоамити\nѕан\nѕе\nѕин\nја\nјадец\nјазе\nјали\nјас\nјаска\nјок\nќе\nќешки\nѝ\nџагара-магара\nџанам\nџив-џив\n    '.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ur/__init__.py----------------------------------------
spacy.lang.ur.__init__.Urdu(Language)
spacy.lang.ur.__init__.UrduDefaults(BaseDefaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ur/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ur/lex_attrs.py----------------------------------------
A:spacy.lang.ur.lex_attrs._num_words->'ایک دو تین چار پانچ چھ سات آٹھ نو دس گیارہ بارہ تیرہ چودہ پندرہ سولہ سترہ\n اٹهارا انیس بیس اکیس بائیس تئیس چوبیس پچیس چھببیس\nستایس اٹھائس انتيس تیس اکتیس بتیس تینتیس چونتیس پینتیس\n چھتیس سینتیس ارتیس انتالیس چالیس اکتالیس بیالیس تیتالیس\nچوالیس پیتالیس چھیالیس سینتالیس اڑتالیس انچالیس پچاس اکاون باون\n تریپن چون پچپن چھپن ستاون اٹھاون انسٹھ ساثھ\nاکسٹھ باسٹھ تریسٹھ چوسٹھ پیسٹھ چھیاسٹھ سڑسٹھ اڑسٹھ\nانھتر ستر اکھتر بھتتر تیھتر چوھتر تچھتر چھیتر ستتر\nاٹھتر انیاسی اسی اکیاسی بیاسی تیراسی چوراسی پچیاسی چھیاسی\n سٹیاسی اٹھیاسی نواسی نوے اکانوے بانوے ترانوے\nچورانوے پچانوے چھیانوے ستانوے اٹھانوے ننانوے سو\n'.split()
A:spacy.lang.ur.lex_attrs._ordinal_words->'پہلا دوسرا تیسرا چوتھا پانچواں چھٹا ساتواں آٹھواں نواں دسواں گیارہواں بارہواں تیرھواں چودھواں\n پندرھواں سولہواں سترھواں اٹھارواں انیسواں بسیواں\n'.split()
A:spacy.lang.ur.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.ur.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.ur.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ur/stop_words.py----------------------------------------
A:spacy.lang.ur.stop_words.STOP_WORDS->set('\nثھی\nخو\nگی\nاپٌے\nگئے\nثہت\nطرف\nہوبری\nپبئے\nاپٌب\nدوضری\nگیب\nکت\nگب\nثھی\nضے\nہر\nپر\nاش\nدی\nگے\nلگیں\nہے\nثعذ\nضکتے\nتھی\nاى\nدیب\nلئے\nوالے\nیہ\nثدبئے\nضکتی\nتھب\nاًذر\nرریعے\nلگی\nہوبرا\nہوًے\nثبہر\nضکتب\nًہیں\nتو\nاور\nرہب\nلگے\nہوضکتب\nہوں\nکب\nہوبرے\nتوبم\nکیب\nایطے\nرہی\nهگر\nہوضکتی\nہیں\nکریں\nہو\nتک\nکی\nایک\nرہے\nهیں\nہوضکتے\nکیطے\nہوًب\nتت\nکہ\nہوا\nآئے\nضبت\nتھے\nکیوں\nہو\nتب\nکے\nپھر\nثغیر\nخبر\nہے\nرکھ\nکی\nطب\nکوئی\n  رریعے\nثبرے\nخب\nاضطرذ\nثلکہ\nخجکہ\nرکھ\nتب\nکی\nطرف\nثراں\nخبر\nرریعہ\nاضکب\nثٌذ\nخص\nکی\nلئے\nتوہیں\nدوضرے\nکررہی\nاضکی\nثیچ\nخوکہ\nرکھتی\nکیوًکہ\nدوًوں\nکر\nرہے\nخبر\nہی\nثرآں\nاضکے\nپچھلا\nخیطب\nرکھتے\nکے\nثعذ\nتو\nہی\n  دورى\nکر\nیہبں\nآش\nتھوڑا\nچکے\nزکویہ\nدوضروں\nضکب\nاوًچب\nثٌب\nپل\nتھوڑی\nچلا\nخبهوظ\nدیتب\nضکٌب\nاخبزت\nاوًچبئی\nثٌبرہب\nپوچھب\nتھوڑے\nچلو\nختن\nدیتی\nضکی\nاچھب\nاوًچی\nثٌبرہی\nپوچھتب\nتیي\nچلیں\nدر\nدیتے\nضکے\nاچھی\nاوًچے\nثٌبرہے\nپوچھتی\nخبًب\nچلے\nدرخبت\nدیر\nضلطلہ\nاچھے\nاٹھبًب\nثٌبًب\nپوچھتے\nخبًتب\nچھوٹب\nدرخہ\nدیکھٌب\nضوچ\nاختتبم\nاہن\nثٌذ\nپوچھٌب\nخبًتی\nچھوٹوں\nدرخے\nدیکھو\nضوچب\nادھر\nآئی\nثٌذکرًب\nپوچھو\nخبًتے\nچھوٹی\nدرزقیقت\nدیکھی\nضوچتب\nارد\nآئے\nثٌذکرو\nپوچھوں\nخبًٌب\nچھوٹے\nدرضت\nدیکھیں\nضوچتی\nاردگرد\nآج\nثٌذی\nپوچھیں\nخططرذ\nچھہ\nدش\nدیٌب\nضوچتے\nارکبى\nآخر\nثڑا\nپورا\nخگہ\nچیسیں\nدفعہ\nدے\nضوچٌب\nاضتعوبل\nآخر\nپہلا\nخگہوں\nزبصل\nدکھبئیں\nراضتوں\nضوچو\nاضتعوبلات\nآدهی\nثڑی\nپہلی\nخگہیں\nزبضر\nدکھبتب\nراضتہ\nضوچی\nاغیب\nآًب\nثڑے\nپہلےضی\nخلذی\nزبل\nدکھبتی\nراضتے\nضوچیں\nاطراف\nآٹھ\nثھر\nخٌبة\nزبل\nدکھبتے\nرکي\nضیذھب\nافراد\nآیب\nثھرا\nپہلے\nخواى\nزبلات\nدکھبًب\nرکھب\nضیذھی\nاکثر\nثب\nہوا\nپیع\nخوًہی\nزبلیہ\nدکھبو\nرکھی\nضیذھے\nاکٹھب\nثھرپور\nتبزٍ\nخیطبکہ\nزصوں\nرکھے\nضیکٌڈ\nاکٹھی\nثبری\nثہتر\nتر\nچبر\nزصہ\nدلچطپ\nزیبدٍ\nغبیذ\nاکٹھے\nثبلا\nثہتری\nترتیت\nچبہب\nزصے\nدلچطپی\nضبت\nغخص\nاکیلا\nثبلترتیت\nثہتریي\nتریي\nچبہٌب\nزقبئق\nدلچطپیبں\nضبدٍ\nغذ\nاکیلی\nثرش\nپبش\nتعذاد\nچبہے\nزقیتیں\nهٌبضت\nضبرا\nغروع\nاکیلے\nثغیر\nپبًب\nچکب\nزقیقت\nدو\nضبرے\nغروعبت\nاگرچہ\nثلٌذ\nپبًچ\nتن\nچکی\nزکن\nدور\nضبل\nغے\nالگ\nپراًب\nتٌہب\nچکیں\nدوضرا\nضبلوں\nصبف\nصسیر\nقجیلہ\nکوًطے\nلازهی\nهطئلے\nًیب\nطریق\nکرتی\nکہتے\nصفر\nقطن\nکھولا\nلگتب\nهطبئل\nوار\nطریقوں\nکرتے\nکہٌب\nصورت\nکئی\nکھولٌب\nلگتی\nهطتعول\nوار\nطریقہ\nکرتے\nہو\nکہٌب\nصورتسبل\nکئے\nکھولو\nلگتے\nهػتول\nٹھیک\nطریقے\nکرًب\nکہو\nصورتوں\nکبفی\nهطلق\nڈھوًڈا\nطور\nکرو\nکہوں\nصورتیں\nکبم\nکھولیں\nلگی\nهعلوم\nڈھوًڈلیب\nطورپر\nکریں\nکہی\nضرور\nکجھی\nکھولے\nلگے\nهکول\nڈھوًڈًب\nظبہر\nکرے\nکہیں\nضرورت\nکرا\nکہب\nلوجب\nهلا\nڈھوًڈو\nعذد\nکل\nکہیں\nکرتب\nکہتب\nلوجی\nهوکي\nڈھوًڈی\nعظین\nکن\nکہے\nضروری\nکرتبہوں\nکہتی\nلوجے\nهوکٌبت\nڈھوًڈیں\nعلاقوں\nکوتر\nکیے\nلوسبت\nهوکٌہ\nہن\nلے\nًبپطٌذ\nہورہے\nعلاقہ\nکورا\nکے\nرریعے\nلوسہ\nهڑا\nہوئی\nهتعلق\nًبگسیر\nہوگئی\nعلاقے\nکوروں\nگئی\nلو\nهڑًب\nہوئے\nهسترم\nًطجت\nہو\nگئے\nعلاوٍ\nکورٍ\nگرد\nلوگ\nهڑے\nہوتی\nهسترهہ\nًقطہ\nہوگیب\nکورے\nگروپ\nلوگوں\nهہرثبى\nہوتے\nهسطوش\nًکبلٌب\nہوًی\nعووهی\nکوطي\nگروٍ\nلڑکپي\nهیرا\nہوچکب\nهختلف\nًکتہ\nہی\nفرد\nکوى\nگروہوں\nلی\nهیری\nہوچکی\nهسیذ\nفی\nکوًطب\nگٌتی\nلیب\nهیرے\nہوچکے\nهطئلہ\nًوخواى\nیقیٌی\nقجل\nکوًطی\nلیٌب\nًئی\nہورہب\nلیں\nًئے\nہورہی\nثبعث\nضت\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ur/punctuation.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/hr/__init__.py----------------------------------------
spacy.lang.hr.__init__.Croatian(Language)
spacy.lang.hr.__init__.CroatianDefaults(BaseDefaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/hr/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/hr/stop_words.py----------------------------------------
A:spacy.lang.hr.stop_words.STOP_WORDS->set('\na\nah\naha\naj\nako\nal\nali\narh\nau\navaj\nbar\nbaš\nbez\nbi\nbih\nbijah\nbijahu\nbijaše\nbijasmo\nbijaste\nbila\nbili\nbilo\nbio\nbismo\nbiste\nbiti\nbrr\nbuć\nbudavši\nbude\nbudimo\nbudite\nbudu\nbudući\nbum\nbumo\nće\nćemo\nćeš\nćete\nčijem\nčijim\nčijima\nću\nda\ndaj\ndakle\nde\ndeder\ndem\ndjelomice\ndjelomično\ndo\ndoista\ndok\ndokle\ndonekle\ndosad\ndoskoro\ndotad\ndotle\ndovečer\ndrugamo\ndrugdje\nduž\ne\neh\nehe\nej\neno\neto\nevo\nga\ngdjekakav\ngdjekoje\ngic\ngod\nhalo\nhej\nhm\nhoće\nhoćemo\nhoćeš\nhoćete\nhoću\nhop\nhtijahu\nhtijasmo\nhtijaste\nhtio\nhtjedoh\nhtjedoše\nhtjedoste\nhtjela\nhtjele\nhtjeli\nhura\ni\niako\nih\niju\nijuju\nikada\nikakav\nikakva\nikakve\nikakvi\nikakvih\nikakvim\nikakvima\nikakvo\nikakvog\nikakvoga\nikakvoj\nikakvom\nikakvome\nili\nim\niz\nja\nje\njedna\njedne\njedni\njedno\njer\njesam\njesi\njesmo\njest\njeste\njesu\njim\njoj\njoš\nju\nkada\nkako\nkao\nkoja\nkoje\nkoji\nkojima\nkoju\nkroz\nlani\nli\nme\nmene\nmeni\nmi\nmimo\nmoj\nmoja\nmoje\nmoji\nmoju\nmu\nna\nnad\nnakon\nnam\nnama\nnas\nnaš\nnaša\nnaše\nnašeg\nnaši\nne\nneće\nnećemo\nnećeš\nnećete\nneću\nnego\nneka\nneke\nneki\nnekog\nneku\nnema\nnešto\nnetko\nni\nnije\nnikoga\nnikoje\nnikoji\nnikoju\nnisam\nnisi\nnismo\nniste\nnisu\nnjega\nnjegov\nnjegova\nnjegovo\nnjemu\nnjezin\nnjezina\nnjezino\nnjih\nnjihov\nnjihova\nnjihovo\nnjim\nnjima\nnjoj\nnju\nno\no\nod\nodmah\non\nona\none\noni\nono\nonu\nonoj\nonom\nonim\nonima\nova\novaj\novim\novima\novoj\npa\npak\npljus\npo\npod\npodalje\npoimence\npoizdalje\nponekad\npored\npostrance\npotajice\npotrbuške\npouzdano\nprije\ns\nsa\nsam\nsamo\nsasvim\nsav\nse\nsebe\nsebi\nsi\nšic\nsmo\nste\nšto\nšta\nštogod\nštagod\nsu\nsva\nsve\nsvi\nsvi\nsvog\nsvoj\nsvoja\nsvoje\nsvoju\nsvom\nsvu\nta\ntada\ntaj\ntako\nte\ntebe\ntebi\nti\ntim\ntima\nto\ntoj\ntome\ntu\ntvoj\ntvoja\ntvoje\ntvoji\ntvoju\nu\nusprkos\nutaman\nuvijek\nuz\nuza\nuzagrapce\nuzalud\nuzduž\nvaljda\nvam\nvama\nvas\nvaš\nvaša\nvaše\nvašim\nvašima\nveć\nvi\nvjerojatno\nvjerovatno\nvrh\nvrlo\nza\nzaista\nzar\nzatim\nzato\nzbija\nzbog\nželeći\nželjah\nželjela\nželjele\nželjeli\nželjelo\nželjen\nželjena\nželjene\nželjeni\nželjenu\nželjeo\nzimus\nzum\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/es/__init__.py----------------------------------------
spacy.lang.es.__init__.Spanish(Language)
spacy.lang.es.__init__.SpanishDefaults(BaseDefaults)
spacy.lang.es.__init__.make_lemmatizer(nlp:Language,model:Optional[Model],name:str,mode:str,overwrite:bool,scorer:Optional[Callable])


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/es/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/es/lemmatizer.py----------------------------------------
A:spacy.lang.es.lemmatizer.pos->token.pos_.lower()
A:spacy.lang.es.lemmatizer.features->set(token.morph)
A:spacy.lang.es.lemmatizer.string->string.lower().lower()
A:spacy.lang.es.lemmatizer.exc->self.lookups.get_table('lemma_exc').get('pron', {}).get(pron)
A:spacy.lang.es.lemmatizer.lemmas->list(dict.fromkeys(lemmas))
A:spacy.lang.es.lemmatizer.rule->self.select_rule('pron', features)
A:spacy.lang.es.lemmatizer.index->self.lookups.get_table('lemma_index').get(rule_pos, [])
A:spacy.lang.es.lemmatizer.groups->self.lookups.get_table('lemma_rules_groups')
A:spacy.lang.es.lemmatizer.possible_lemma->re.sub(old + '$', new, word)
A:spacy.lang.es.lemmatizer.splitted_word->re.sub(',', '.', word).split(',')
A:spacy.lang.es.lemmatizer.word->re.sub(',', '.', word)
A:spacy.lang.es.lemmatizer.voc_alt_lemma->lemma.replace(old, new, 1)
A:spacy.lang.es.lemmatizer.m->re.search(pron_patt, verb)
A:spacy.lang.es.lemmatizer.verb->re.sub(old, new, verb)
spacy.lang.es.SpanishLemmatizer(Lemmatizer)
spacy.lang.es.SpanishLemmatizer.get_lookups_config(cls,mode:str)->Tuple[List[str], List[str]]
spacy.lang.es.SpanishLemmatizer.lemmatize_adj(self,word:str,features:List[str],rule:str,index:List[str])->List[str]
spacy.lang.es.SpanishLemmatizer.lemmatize_adv(self,word:str,features:List[str],rule:str,index:List[str])->List[str]
spacy.lang.es.SpanishLemmatizer.lemmatize_det(self,word:str,features:List[str],rule:str,index:List[str])->List[str]
spacy.lang.es.SpanishLemmatizer.lemmatize_noun(self,word:str,features:List[str],rule:str,index:List[str])->List[str]
spacy.lang.es.SpanishLemmatizer.lemmatize_num(self,word:str,features:List[str],rule:str,index:List[str])->List[str]
spacy.lang.es.SpanishLemmatizer.lemmatize_pron(self,word:str,features:List[str],rule:Optional[str],index:List[str])->List[str]
spacy.lang.es.SpanishLemmatizer.lemmatize_verb(self,word:str,features:List[str],rule:Optional[str],index:List[str])->List[str]
spacy.lang.es.SpanishLemmatizer.lemmatize_verb_pron(self,word:str,features:List[str],rule:Optional[str],index:List[str])->List[str]
spacy.lang.es.SpanishLemmatizer.rule_lemmatize(self,token:Token)->List[str]
spacy.lang.es.SpanishLemmatizer.select_rule(self,pos:str,features:List[str])->Optional[str]
spacy.lang.es.lemmatizer.SpanishLemmatizer(Lemmatizer)
spacy.lang.es.lemmatizer.SpanishLemmatizer.get_lookups_config(cls,mode:str)->Tuple[List[str], List[str]]
spacy.lang.es.lemmatizer.SpanishLemmatizer.lemmatize_adj(self,word:str,features:List[str],rule:str,index:List[str])->List[str]
spacy.lang.es.lemmatizer.SpanishLemmatizer.lemmatize_adv(self,word:str,features:List[str],rule:str,index:List[str])->List[str]
spacy.lang.es.lemmatizer.SpanishLemmatizer.lemmatize_det(self,word:str,features:List[str],rule:str,index:List[str])->List[str]
spacy.lang.es.lemmatizer.SpanishLemmatizer.lemmatize_noun(self,word:str,features:List[str],rule:str,index:List[str])->List[str]
spacy.lang.es.lemmatizer.SpanishLemmatizer.lemmatize_num(self,word:str,features:List[str],rule:str,index:List[str])->List[str]
spacy.lang.es.lemmatizer.SpanishLemmatizer.lemmatize_pron(self,word:str,features:List[str],rule:Optional[str],index:List[str])->List[str]
spacy.lang.es.lemmatizer.SpanishLemmatizer.lemmatize_verb(self,word:str,features:List[str],rule:Optional[str],index:List[str])->List[str]
spacy.lang.es.lemmatizer.SpanishLemmatizer.lemmatize_verb_pron(self,word:str,features:List[str],rule:Optional[str],index:List[str])->List[str]
spacy.lang.es.lemmatizer.SpanishLemmatizer.rule_lemmatize(self,token:Token)->List[str]
spacy.lang.es.lemmatizer.SpanishLemmatizer.select_rule(self,pos:str,features:List[str])->Optional[str]


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/es/lex_attrs.py----------------------------------------
A:spacy.lang.es.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.es.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
A:spacy.lang.es.lex_attrs.text_lower->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').lower()
spacy.lang.es.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/es/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.es.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/es/syntax_iterators.py----------------------------------------
A:spacy.lang.es.syntax_iterators.np_label->doc.vocab.strings.add('NP')
A:spacy.lang.es.syntax_iterators.adj_label->doc.vocab.strings.add('amod')
A:spacy.lang.es.syntax_iterators.adp_label->doc.vocab.strings.add('ADP')
A:spacy.lang.es.syntax_iterators.conj->doc.vocab.strings.add('conj')
A:spacy.lang.es.syntax_iterators.conj_pos->doc.vocab.strings.add('CCONJ')
A:spacy.lang.es.syntax_iterators.right_childs->list(word.rights)
spacy.lang.es.syntax_iterators.noun_chunks(doclike:Union[Doc,Span])->Iterator[Tuple[int, int, int]]


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/es/stop_words.py----------------------------------------
A:spacy.lang.es.stop_words.STOP_WORDS->set('\na acuerdo adelante ademas además afirmó agregó ahi ahora ahí al algo alguna\nalgunas alguno algunos algún alli allí alrededor ambos ante anterior antes\napenas aproximadamente aquel aquella aquellas aquello aquellos aqui aquél\naquélla aquéllas aquéllos aquí arriba aseguró asi así atras aun aunque añadió\naún\n\nbajo bastante bien breve buen buena buenas bueno buenos\n\ncada casi cierta ciertas cierto ciertos cinco claro comentó como con conmigo\nconocer conseguimos conseguir considera consideró consigo consigue consiguen\nconsigues contigo contra creo cual cuales cualquier cuando cuanta cuantas\ncuanto cuantos cuatro cuenta cuál cuáles cuándo cuánta cuántas cuánto cuántos\ncómo\n\nda dado dan dar de debajo debe deben debido decir dejó del delante demasiado\ndemás dentro deprisa desde despacio despues después detras detrás dia dias dice\ndicen dicho dieron diez diferente diferentes dijeron dijo dio doce donde dos\ndurante día días dónde\n\ne el ella ellas ello ellos embargo en encima encuentra enfrente enseguida\nentonces entre era eramos eran eras eres es esa esas ese eso esos esta estaba\nestaban estado estados estais estamos estan estar estará estas este esto estos\nestoy estuvo está están excepto existe existen explicó expresó él ésa ésas ése\nésos ésta éstas éste éstos\n\nfin final fue fuera fueron fui fuimos\n\ngran grande grandes\n\nha haber habia habla hablan habrá había habían hace haceis hacemos hacen hacer\nhacerlo haces hacia haciendo hago han hasta hay haya he hecho hemos hicieron\nhizo hoy hubo\n\nigual incluso indicó informo informó ir\n\njunto\n\nla lado largo las le les llegó lleva llevar lo los luego\n\nmal manera manifestó mas mayor me mediante medio mejor mencionó menos menudo mi\nmia mias mientras mio mios mis misma mismas mismo mismos modo mucha muchas\nmucho muchos muy más mí mía mías mío míos\n\nnada nadie ni ninguna ningunas ninguno ningunos ningún no nos nosotras nosotros\nnuestra nuestras nuestro nuestros nueva nuevas nueve nuevo nuevos nunca\n\no ocho once os otra otras otro otros\n\npara parece parte partir pasada pasado paìs peor pero pesar poca pocas poco\npocos podeis podemos poder podria podriais podriamos podrian podrias podrá\npodrán podría podrían poner por porque posible primer primera primero primeros\npronto propia propias propio propios proximo próximo próximos pudo pueda puede\npueden puedo pues\n\nqeu que quedó queremos quien quienes quiere quiza quizas quizá quizás quién\nquiénes qué\n\nrealizado realizar realizó repente respecto\n\nsabe sabeis sabemos saben saber sabes salvo se sea sean segun segunda segundo\nsegún seis ser sera será serán sería señaló si sido siempre siendo siete sigue\nsiguiente sin sino sobre sois sola solamente solas solo solos somos son soy su\nsupuesto sus suya suyas suyo suyos sé sí sólo\n\ntal tambien también tampoco tan tanto tarde te temprano tendrá tendrán teneis\ntenemos tener tenga tengo tenido tenía tercera tercero ti tiene tienen toda\ntodas todavia todavía todo todos total tras trata través tres tu tus tuvo tuya\ntuyas tuyo tuyos tú\n\nu ultimo un una unas uno unos usa usais usamos usan usar usas uso usted ustedes\núltima últimas último últimos\n\nva vais vamos van varias varios vaya veces ver verdad verdadera verdadero vez\nvosotras vosotros voy vuestra vuestras vuestro vuestros\n\ny ya yo\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/es/punctuation.py----------------------------------------
A:spacy.lang.es.punctuation._units->merge_chars(' '.join(_list_units))


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/tt/__init__.py----------------------------------------
spacy.lang.tt.__init__.Tatar(Language)
spacy.lang.tt.__init__.TatarDefaults(BaseDefaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/tt/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/tt/lex_attrs.py----------------------------------------
A:spacy.lang.tt.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.tt.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.tt.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/tt/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.tt.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/tt/stop_words.py----------------------------------------
A:spacy.lang.tt.stop_words.STOP_WORDS->set('алай алайса алар аларга аларда алардан аларны аларның аларча\nалары аларын аларынга аларында аларыннан аларының алтмыш алтмышынчы алтмышынчыга\nалтмышынчыда алтмышынчыдан алтмышынчылар алтмышынчыларга алтмышынчыларда\nалтмышынчылардан алтмышынчыларны алтмышынчыларның алтмышынчыны алтмышынчының\nалты алтылап алтынчы алтынчыга алтынчыда алтынчыдан алтынчылар алтынчыларга\nалтынчыларда алтынчылардан алтынчыларны алтынчыларның алтынчыны алтынчының\nалтышар анда андагы андай андый андыйга андыйда андыйдан андыйны андыйның аннан\nансы анча аны аныкы аныкын аныкынга аныкында аныкыннан аныкының анысы анысын\nанысынга анысында анысыннан анысының аның аныңча аркылы ары аша аңа аңар аңарга\nаңарда аңардагы аңардан\n\nбар бара барлык барча барчасы барчасын барчасына барчасында барчасыннан\nбарчасының бары башка башкача бе\xadлән без безгә бездә бездән безне безнең безнеңчә\nбелдерүенчә белән бер бергә беренче беренчегә беренчедә беренчедән беренчеләр\nберенчеләргә беренчеләрдә беренчеләрдән беренчеләрне беренчеләрнең беренчене\nберенченең беркайда беркайсы беркая беркаян беркем беркемгә беркемдә беркемне\nберкемнең беркемнән берлән берни бернигә бернидә бернидән бернинди бернине\nбернинең берничек берничә бернәрсә бернәрсәгә бернәрсәдә бернәрсәдән бернәрсәне\nбернәрсәнең беррәттән берсе берсен берсенгә берсендә берсенең берсеннән берәр\nберәрсе берәрсен берәрсендә берәрсенең берәрсеннән берәрсенә берәү бигрәк бик\nбирле бит биш бишенче бишенчегә бишенчедә бишенчедән бишенчеләр бишенчеләргә\nбишенчеләрдә бишенчеләрдән бишенчеләрне бишенчеләрнең бишенчене бишенченең\nбишләп болай болар боларга боларда болардан боларны боларның болары боларын\nболарынга боларында боларыннан боларының бу буе буена буенда буенча буйлап\nбуларак булачак булды булмый булса булып булыр булырга бусы бүтән бәлки бән\nбәрабәренә бөтен бөтенесе бөтенесен бөтенесендә бөтенесенең бөтенесеннән\nбөтенесенә\n\nвә\n\nгел генә гына гүя гүяки гәрчә\n\nда ди дигән диде дип дистәләгән дистәләрчә дүрт дүртенче дүртенчегә дүртенчедә\nдүртенчедән дүртенчеләр дүртенчеләргә дүртенчеләрдә дүртенчеләрдән дүртенчеләрне\nдүртенчеләрнең дүртенчене дүртенченең дүртләп дә\n\nегерме егерменче егерменчегә егерменчедә егерменчедән егерменчеләр\nегерменчеләргә егерменчеләрдә егерменчеләрдән егерменчеләрне егерменчеләрнең\nегерменчене егерменченең ел елда\n\nиде идек идем ике икенче икенчегә икенчедә икенчедән икенчеләр икенчеләргә\nикенчеләрдә икенчеләрдән икенчеләрне икенчеләрнең икенчене икенченең икешәр икән\nилле илленче илленчегә илленчедә илленчедән илленчеләр илленчеләргә\nилленчеләрдә илленчеләрдән илленчеләрне илленчеләрнең илленчене илленченең илә\nилән инде исә итеп иткән итте итү итә итәргә иң\n\nйөз йөзенче йөзенчегә йөзенчедә йөзенчедән йөзенчеләр йөзенчеләргә йөзенчеләрдә\nйөзенчеләрдән йөзенчеләрне йөзенчеләрнең йөзенчене йөзенченең йөзләгән йөзләрчә\nйөзәрләгән\n\nкадәр кай кайбер кайберләре кайберсе кайберәү кайберәүгә кайберәүдә кайберәүдән\nкайберәүне кайберәүнең кайдагы кайсы кайсыбер кайсын кайсына кайсында кайсыннан\nкайсының кайчангы кайчандагы кайчаннан караганда карамастан карамый карата каршы\nкаршына каршында каршындагы кебек кем кемгә кемдә кемне кемнең кемнән кенә ки\nкилеп килә кирәк кына кырыгынчы кырыгынчыга кырыгынчыда кырыгынчыдан\nкырыгынчылар кырыгынчыларга кырыгынчыларда кырыгынчылардан кырыгынчыларны\nкырыгынчыларның кырыгынчыны кырыгынчының кырык күк күпләгән күпме күпмеләп\nкүпмешәр күпмешәрләп күптән күрә\n\nләкин\n\nмаксатында менә мең меңенче меңенчегә меңенчедә меңенчедән меңенчеләр\nмеңенчеләргә меңенчеләрдә меңенчеләрдән меңенчеләрне меңенчеләрнең меңенчене\nмеңенченең меңләгән меңләп меңнәрчә меңәрләгән меңәрләп миллиард миллиардлаган\nмиллиардларча миллион миллионлаган миллионнарча миллионынчы миллионынчыга\nмиллионынчыда миллионынчыдан миллионынчылар миллионынчыларга миллионынчыларда\nмиллионынчылардан миллионынчыларны миллионынчыларның миллионынчыны\nмиллионынчының мин миндә мине минем минемчә миннән миңа монда мондагы мондые\nмондыен мондыенгә мондыендә мондыеннән мондыеның мондый мондыйга мондыйда\nмондыйдан мондыйлар мондыйларга мондыйларда мондыйлардан мондыйларны\nмондыйларның мондыйлары мондыйларын мондыйларынга мондыйларында мондыйларыннан\nмондыйларының мондыйны мондыйның моннан монсыз монча моны моныкы моныкын\nмоныкынга моныкында моныкыннан моныкының монысы монысын монысынга монысында\nмонысыннан монысының моның моңа моңар моңарга мәгълүматынча мәгәр мән мөмкин\n\nни нибарысы никадәре нинди ниндие ниндиен ниндиенгә ниндиендә ниндиенең\nниндиеннән ниндиләр ниндиләргә ниндиләрдә ниндиләрдән ниндиләрен ниндиләренн\nниндиләреннгә ниндиләренндә ниндиләреннең ниндиләренннән ниндиләрне ниндиләрнең\nниндирәк нихәтле ничаклы ничек ничәшәр ничәшәрләп нуль нче нчы нәрсә нәрсәгә\nнәрсәдә нәрсәдән нәрсәне нәрсәнең\n\nсаен сез сезгә сездә сездән сезне сезнең сезнеңчә сигез сигезенче сигезенчегә\nсигезенчедә сигезенчедән сигезенчеләр сигезенчеләргә сигезенчеләрдә\nсигезенчеләрдән сигезенчеләрне сигезенчеләрнең сигезенчене сигезенченең\nсиксән син синдә сине синең синеңчә синнән сиңа соң сыман сүзенчә сүзләренчә\n\nта таба теге тегеләй тегеләр тегеләргә тегеләрдә тегеләрдән тегеләре тегеләрен\nтегеләренгә тегеләрендә тегеләренең тегеләреннән тегеләрне тегеләрнең тегенди\nтегендигә тегендидә тегендидән тегендине тегендинең тегендә тегендәге тегене\nтегенеке тегенекен тегенекенгә тегенекендә тегенекенең тегенекеннән тегенең\nтегеннән тегесе тегесен тегесенгә тегесендә тегесенең тегесеннән тегеңә тиеш тик\nтикле тора триллиард триллион тугыз тугызлап тугызлашып тугызынчы тугызынчыга\nтугызынчыда тугызынчыдан тугызынчылар тугызынчыларга тугызынчыларда\nтугызынчылардан тугызынчыларны тугызынчыларның тугызынчыны тугызынчының туксан\nтуксанынчы туксанынчыга туксанынчыда туксанынчыдан туксанынчылар туксанынчыларга\nтуксанынчыларда туксанынчылардан туксанынчыларны туксанынчыларның туксанынчыны\nтуксанынчының турында тыш түгел тә тәгаенләнгән төмән\n\nуенча уйлавынча ук ул ун уналты уналтынчы уналтынчыга уналтынчыда уналтынчыдан\nуналтынчылар уналтынчыларга уналтынчыларда уналтынчылардан уналтынчыларны\nуналтынчыларның уналтынчыны уналтынчының унарлаган унарлап унаула унаулап унбер\nунберенче унберенчегә унберенчедә унберенчедән унберенчеләр унберенчеләргә\nунберенчеләрдә унберенчеләрдән унберенчеләрне унберенчеләрнең унберенчене\nунберенченең унбиш унбишенче унбишенчегә унбишенчедә унбишенчедән унбишенчеләр\nунбишенчеләргә унбишенчеләрдә унбишенчеләрдән унбишенчеләрне унбишенчеләрнең\nунбишенчене унбишенченең ундүрт ундүртенче ундүртенчегә ундүртенчедә\nундүртенчедән ундүртенчеләр ундүртенчеләргә ундүртенчеләрдә ундүртенчеләрдән\nундүртенчеләрне ундүртенчеләрнең ундүртенчене ундүртенченең унике уникенче\nуникенчегә уникенчедә уникенчедән уникенчеләр уникенчеләргә уникенчеләрдә\nуникенчеләрдән уникенчеләрне уникенчеләрнең уникенчене уникенченең унлаган\nунлап уннарча унсигез унсигезенче унсигезенчегә унсигезенчедә унсигезенчедән\nунсигезенчеләр унсигезенчеләргә унсигезенчеләрдә унсигезенчеләрдән\nунсигезенчеләрне унсигезенчеләрнең унсигезенчене унсигезенченең унтугыз\nунтугызынчы унтугызынчыга унтугызынчыда унтугызынчыдан унтугызынчылар\nунтугызынчыларга унтугызынчыларда унтугызынчылардан унтугызынчыларны\nунтугызынчыларның унтугызынчыны унтугызынчының унынчы унынчыга унынчыда\nунынчыдан унынчылар унынчыларга унынчыларда унынчылардан унынчыларны\nунынчыларның унынчыны унынчының унҗиде унҗиденче унҗиденчегә унҗиденчедә\nунҗиденчедән унҗиденчеләр унҗиденчеләргә унҗиденчеләрдә унҗиденчеләрдән\nунҗиденчеләрне унҗиденчеләрнең унҗиденчене унҗиденченең унөч унөченче унөченчегә\nунөченчедә унөченчедән унөченчеләр унөченчеләргә унөченчеләрдә унөченчеләрдән\nунөченчеләрне унөченчеләрнең унөченчене унөченченең утыз утызынчы утызынчыга\nутызынчыда утызынчыдан утызынчылар утызынчыларга утызынчыларда утызынчылардан\nутызынчыларны утызынчыларның утызынчыны утызынчының\n\nфикеренчә фәкать\n\nхакында хәбәр хәлбуки хәтле хәтта\n\nчаклы чакта чөнки\n\nшикелле шул шулай шулар шуларга шуларда шулардан шуларны шуларның шулары шуларын\nшуларынга шуларында шуларыннан шуларының шулкадәр шултикле шултиклем шулхәтле\nшулчаклы шунда шундагы шундый шундыйга шундыйда шундыйдан шундыйны шундыйның\nшунлыктан шуннан шунсы шунча шуны шуныкы шуныкын шуныкынга шуныкында шуныкыннан\nшуныкының шунысы шунысын шунысынга шунысында шунысыннан шунысының шуның шушы\nшушында шушыннан шушыны шушының шушыңа шуңа шуңар шуңарга\n\nэлек\n\nюгыйсә юк юкса\n\nя ягъни язуынча яисә яки яктан якын ярашлы яхут яшь яшьлек\n\nҗиде җиделәп җиденче җиденчегә җиденчедә җиденчедән җиденчеләр җиденчеләргә\nҗиденчеләрдә җиденчеләрдән җиденчеләрне җиденчеләрнең җиденчене җиденченең\nҗидешәр җитмеш җитмешенче җитмешенчегә җитмешенчедә җитмешенчедән җитмешенчеләр\nҗитмешенчеләргә җитмешенчеләрдә җитмешенчеләрдән җитмешенчеләрне\nҗитмешенчеләрнең җитмешенчене җитмешенченең җыенысы\n\nүз үзе үзем үземдә үземне үземнең үземнән үземә үзен үзендә үзенең үзеннән үзенә\nүк\n\nһичбер һичбере һичберен һичберендә һичберенең һичбереннән һичберенә һичберсе\nһичберсен һичберсендә һичберсенең һичберсеннән һичберсенә һичберәү һичберәүгә\nһичберәүдә һичберәүдән һичберәүне һичберәүнең һичкайсы һичкайсыга һичкайсыда\nһичкайсыдан һичкайсыны һичкайсының һичкем һичкемгә һичкемдә һичкемне һичкемнең\nһичкемнән һични һичнигә һичнидә һичнидән һичнинди һичнине һичнинең һичнәрсә\nһичнәрсәгә һичнәрсәдә һичнәрсәдән һичнәрсәне һичнәрсәнең һәм һәммә һәммәсе\nһәммәсен һәммәсендә һәммәсенең һәммәсеннән һәммәсенә һәр һәрбер һәрбере һәрберсе\nһәркайсы һәркайсыга һәркайсыда һәркайсыдан һәркайсыны һәркайсының һәркем\nһәркемгә һәркемдә һәркемне һәркемнең һәркемнән һәрни һәрнәрсә һәрнәрсәгә\nһәрнәрсәдә һәрнәрсәдән һәрнәрсәне һәрнәрсәнең һәртөрле\n\nә әгәр әйтүенчә әйтүләренчә әлбәттә әле әлеге әллә әмма әнә\n\nөстәп өч өчен өченче өченчегә өченчедә өченчедән өченчеләр өченчеләргә\nөченчеләрдә өченчеләрдән өченчеләрне өченчеләрнең өченчене өченченең өчләп\nөчәрләп'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/tt/punctuation.py----------------------------------------
A:spacy.lang.tt.punctuation._hyphens_no_dash->char_classes.HYPHENS.replace('-', '').strip('|').replace('||', '')


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/sv/__init__.py----------------------------------------
spacy.lang.sv.__init__.Swedish(Language)
spacy.lang.sv.__init__.SwedishDefaults(BaseDefaults)
spacy.lang.sv.__init__.make_lemmatizer(nlp:Language,model:Optional[Model],name:str,mode:str,overwrite:bool,scorer:Optional[Callable])


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/sv/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/sv/lex_attrs.py----------------------------------------
A:spacy.lang.sv.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.sv.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.sv.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/sv/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.sv.tokenizer_exceptions.verb_data_tc->dict(verb_data)
A:spacy.lang.sv.tokenizer_exceptions.verb_data_tc[ORTH]->verb_data_tc[ORTH].title().title()
A:spacy.lang.sv.tokenizer_exceptions.capitalized->orth.capitalize()
A:spacy.lang.sv.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/sv/syntax_iterators.py----------------------------------------
A:spacy.lang.sv.syntax_iterators.conj->doc.vocab.strings.add('conj')
A:spacy.lang.sv.syntax_iterators.np_label->doc.vocab.strings.add('NP')
spacy.lang.sv.syntax_iterators.noun_chunks(doclike:Union[Doc,Span])->Iterator[Tuple[int, int, int]]


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/sv/stop_words.py----------------------------------------
A:spacy.lang.sv.stop_words.STOP_WORDS->set('\naderton adertonde adjö aldrig alla allas allt alltid alltså än andra andras\nannan annat ännu artonde arton åtminstone att åtta åttio åttionde åttonde av\näven\n\nbåda bådas bakom bara bäst bättre behöva behövas behövde behövt beslut beslutat\nbeslutit bland blev bli blir blivit bort borta bra\n\ndå dag dagar dagarna dagen där därför de del delen dem den deras dess det detta\ndig din dina dit ditt dock du\n\nefter eftersom elfte eller elva en enkel enkelt enkla enligt er era ert ett\nettusen\n\nfå fanns får fått fem femte femtio femtionde femton femtonde fick fin finnas\nfinns fjärde fjorton fjortonde fler flera flesta följande för före förlåt förra\nförsta fram framför från fyra fyrtio fyrtionde\n\ngå gälla gäller gällt går gärna gått genast genom gick gjorde gjort god goda\ngodare godast gör göra gott\n\nha hade haft han hans har här heller hellre helst helt henne hennes hit hög\nhöger högre högst hon honom hundra hundraen hundraett hur\n\ni ibland idag igår igen imorgon in inför inga ingen ingenting inget innan inne\ninom inte inuti\n\nja jag jämfört\n\nkan kanske knappast kom komma kommer kommit kr kunde kunna kunnat kvar\n\nlänge längre långsam långsammare långsammast långsamt längst långt lätt lättare\nlättast legat ligga ligger lika likställd likställda lilla lite liten litet\n\nman många måste med mellan men mer mera mest mig min mina mindre minst mitt\nmittemot möjlig möjligen möjligt möjligtvis mot mycket\n\nnågon någonting något några när nästa ned nederst nedersta nedre nej ner ni nio\nnionde nittio nittionde nitton nittonde nödvändig nödvändiga nödvändigt\nnödvändigtvis nog noll nr nu nummer\n\noch också ofta oftast olika olikt om oss\n\növer övermorgon överst övre\n\npå\n\nrakt rätt redan\n\nså sade säga säger sagt samma sämre sämst sedan senare senast sent sex sextio\nsextionde sexton sextonde sig sin sina sist sista siste sitt sjätte sju sjunde\nsjuttio sjuttionde sjutton sjuttonde ska skall skulle slutligen små smått snart\nsom stor stora större störst stort\n\ntack tidig tidigare tidigast tidigt till tills tillsammans tio tionde tjugo\ntjugoen tjugoett tjugonde tjugotre tjugotvå tjungo tolfte tolv tre tredje\ntrettio trettionde tretton trettonde två tvåhundra\n\nunder upp ur ursäkt ut utan utanför ute\n\nvad vänster vänstra var vår vara våra varför varifrån varit varken värre\nvarsågod vart vårt vem vems verkligen vi vid vidare viktig viktigare viktigast\nviktigt vilka vilken vilket vill\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/sv/punctuation.py----------------------------------------
A:spacy.lang.sv.punctuation._quotes->char_classes.CONCAT_QUOTES.replace("'", '')


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/sl/__init__.py----------------------------------------
spacy.lang.sl.__init__.Slovenian(Language)
spacy.lang.sl.__init__.SlovenianDefaults(BaseDefaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/sl/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/sl/lex_attrs.py----------------------------------------
A:spacy.lang.sl.lex_attrs._num_words->set('\n\tnula ničla nič ena dva tri štiri pet šest sedem osem\n\tdevet deset enajst dvanajst trinajst štirinajst petnajst\n\tšestnajst sedemnajst osemnajst devetnajst dvajset trideset štirideset\n\tpetdeset šestdest sedemdeset osemdeset devedeset sto tisoč\n\tmilijon bilijon trilijon kvadrilijon nešteto\n\t\n\ten eden enega enemu ennem enim enih enima enimi ene eni eno\n\tdveh dvema dvem dvoje trije treh trem tremi troje štirje štirih štirim štirimi\n\tpetih petim petimi šestih šestim šestimi sedmih sedmim sedmimi osmih osmim osmimi\n\tdevetih devetim devetimi desetih desetim desetimi enajstih enajstim enajstimi\n\tdvanajstih dvanajstim dvanajstimi trinajstih trinajstim trinajstimi\n\tšestnajstih šestnajstim šestnajstimi petnajstih petnajstim petnajstimi\n\tsedemnajstih sedemnajstim sedemnajstimi osemnajstih osemnajstim osemnajstimi\n\tdevetnajstih devetnajstim devetnajstimi dvajsetih dvajsetim dvajsetimi  \n\t'.split())
A:spacy.lang.sl.lex_attrs._ordinal_words->set('\n\tprvi drugi tretji četrti peti šesti sedmi osmi\n\tdeveti deseti enajsti dvanajsti trinajsti štirinajsti\n\tpetnajsti šestnajsti sedemnajsti osemnajsti devetnajsti\n\tdvajseti trideseti štirideseti petdeseti šestdeseti sedemdeseti\n\tosemdeseti devetdeseti stoti tisoči milijonti bilijonti\n\ttrilijonti kvadrilijonti nešteti\n\t\n\tprva druga tretja četrta peta šesta sedma osma\n\tdeveta deseta enajsta dvanajsta trinajsta štirnajsta\n\tpetnajsta šestnajsta sedemnajsta osemnajsta devetnajsta\n\tdvajseta trideseta štirideseta petdeseta šestdeseta sedemdeseta\n\tosemdeseta devetdeseta stota tisoča milijonta bilijonta\n\ttrilijonta kvadrilijonta nešteta\n\t\n\tprvo drugo tretje četrto peto šestro sedmo osmo\n\tdeveto deseto enajsto dvanajsto trinajsto štirnajsto\n\tpetnajsto šestnajsto sedemnajsto osemnajsto devetnajsto\n\tdvajseto trideseto štirideseto petdeseto šestdeseto sedemdeseto\n\tosemdeseto devetdeseto stoto tisočo milijonto bilijonto\n\ttrilijonto kvadrilijonto nešteto\n\t\n\tprvega drugega tretjega četrtega petega šestega sedmega osmega \n\tdevega desetega enajstega dvanajstega trinajstega štirnajstega\n\tpetnajstega šestnajstega sedemnajstega osemnajstega devetnajstega\n\tdvajsetega tridesetega štiridesetega petdesetega šestdesetega sedemdesetega\n\tosemdesetega devetdesetega stotega tisočega milijontega bilijontega\n\ttrilijontega kvadrilijontega neštetega\n\t\n\tprvemu drugemu tretjemu četrtemu petemu šestemu sedmemu osmemu devetemu desetemu \n\tenajstemu dvanajstemu trinajstemu štirnajstemu petnajstemu šestnajstemu sedemnajstemu\n\tosemnajstemu devetnajstemu dvajsetemu tridesetemu štiridesetemu petdesetemu šestdesetemu\n\tsedemdesetemu osemdesetemu devetdesetemu stotemu tisočemu milijontemu bilijontemu\n\ttrilijontemu kvadrilijontemu neštetemu\n\t\n\tprvem drugem tretjem četrtem petem šestem sedmem osmem devetem desetem\n\tenajstem dvanajstem trinajstem štirnajstem petnajstem šestnajstem sedemnajstem\n\tosemnajstem devetnajstem dvajsetem tridesetem štiridesetem petdesetem šestdesetem\n\tsedemdesetem osemdesetem devetdesetem stotem tisočem milijontem bilijontem\n\ttrilijontem kvadrilijontem neštetem\n\t\n\tprvim drugim tretjim četrtim petim šestim sedtim osmim devetim desetim\n\tenajstim dvanajstim trinajstim štirnajstim petnajstim šestnajstim sedemnajstim\n\tosemnajstim devetnajstim dvajsetim tridesetim štiridesetim petdesetim šestdesetim\n\tsedemdesetim osemdesetim devetdesetim stotim tisočim milijontim bilijontim\n\ttrilijontim kvadrilijontim neštetim\n\t    \n\tprvih drugih tretjih četrthih petih šestih sedmih osmih deveth desetih\n\tenajstih dvanajstih trinajstih štirnajstih petnajstih šestnajstih sedemnajstih\n\tosemnajstih devetnajstih dvajsetih tridesetih štiridesetih petdesetih šestdesetih\n\tsedemdesetih osemdesetih devetdesetih stotih tisočih milijontih bilijontih\n\ttrilijontih kvadrilijontih nešteth\n\t\n\tprvima drugima tretjima četrtima petima šestima sedmima osmima devetima desetima\n\tenajstima dvanajstima trinajstima štirnajstima petnajstima šestnajstima sedemnajstima\n\tosemnajstima devetnajstima dvajsetima tridesetima štiridesetima petdesetima šestdesetima\n\tsedemdesetima osemdesetima devetdesetima stotima tisočima milijontima bilijontima\n\ttrilijontima kvadrilijontima neštetima\n\t\n\tprve druge četrte pete šeste sedme osme devete desete\n\tenajste dvanajste trinajste štirnajste petnajste šestnajste sedemnajste\n\tosemnajste devetnajste dvajsete tridesete štiridesete petdesete šestdesete\n\tsedemdesete osemdesete devetdesete stote tisoče milijonte bilijonte \n\ttrilijonte kvadrilijonte neštete\n\t\n\tprvimi drugimi tretjimi četrtimi petimi šestimi sedtimi osmimi devetimi desetimi\n\tenajstimi dvanajstimi trinajstimi štirnajstimi petnajstimi šestnajstimi sedemnajstimi\n\tosemnajstimi devetnajstimi dvajsetimi tridesetimi štiridesetimi petdesetimi šestdesetimi\n\tsedemdesetimi osemdesetimi devetdesetimi stotimi tisočimi milijontimi bilijontimi\n\ttrilijontimi kvadrilijontimi neštetimi\n\t'.split())
A:spacy.lang.sl.lex_attrs._currency_words->set('\n\tevro evra evru evrom evrov evroma evrih evrom evre evri evr eur\n\tcent centa centu cenom centov centoma centih centom cente centi\n\tdolar dolarja dolarji dolarju dolarjem dolarjev dolarjema dolarjih dolarje usd\n\ttolar tolarja tolarji tolarju tolarjem tolarjev tolarjema tolarjih tolarje tol\n\tdinar dinarja dinarji dinarju dinarjem dinarjev dinarjema dinarjih dinarje din\n\tfunt funta funti funtu funtom funtov funtoma funtih funte gpb\n\tforint forinta forinti forintu forintom forintov forintoma forintih forinte\n\tzlot zlota zloti zlotu zlotom zlotov zlotoma zlotih zlote \n\trupij rupija rupiji rupiju rupijem rupijev rupijema rupijih rupije\n\tjen jena jeni jenu jenom jenov jenoma jenih jene\n\tkuna kuni kune kuno kun kunama kunah kunam kunami\n\tmarka marki marke markama markah markami \n\t'.split())
A:spacy.lang.sl.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.sl.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
A:spacy.lang.sl.lex_attrs.text_lower->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').lower()
spacy.lang.sl.lex_attrs.is_currency(text)
spacy.lang.sl.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/sl/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.sl.tokenizer_exceptions.abbrv->'\nCo. Ch. DIPL. DR. Dr. Ev. Inc. Jr. Kr. Mag. M. MR. Mr. Mt. Murr. Npr. OZ. \nOpr. Osn. Prim. Roj. ST. Sim. Sp. Sred. St. Sv. Škofl. Tel. UR. Zb. \na. aa. ab. abc. abit. abl. abs. abt. acc. accel. add. adj. adv. aet. afr. akad. al. alban. all. alleg. \nalp. alt. alter. alžir. am. an. andr. ang. anh. anon. ans. antrop. apoc. app. approx. apt. ar. arc. arch. \narh. arr. as. asist. assist. assoc. asst. astr. attn. aug. avstral. az. b. bab. bal. bbl. bd. belg. bioinf. \nbiomed. bk. bl. bn. borg. bp. br. braz. brit. bros. broš. bt. bu. c. ca. cal. can. cand. cantab. cap. capt.\ncat. cath. cc. cca. cd. cdr. cdre. cent. cerkv. cert. cf. cfr. ch. chap. chem. chr. chs. cic. circ. civ. cl.\ncm. cmd. cnr. co. cod. col. coll. colo. com. comp. con. conc. cond. conn. cons. cont. coop. corr. cost. cp.\ncpl. cr. crd. cres. cresc. ct. cu. d. dan. dat. davč. ddr. dec. ded. def. dem. dent. dept. dia. dip. dipl. \ndir. disp. diss. div. do. doc. dok. dol. doo. dop. dott. dr. dram. druž. družb. drž. dt. duh. dur. dvr. dwt. e.\nea. ecc. eccl. eccles. econ. edn. egipt. egr. ekon. eksp. el. em. enc. eng. eo. ep. err. esp. esq. est.\net. etc. etnogr. etnol. ev. evfem. evr. ex. exc. excl. exp. expl. ext. exx. f. fa. facs. fak. faks. fas.\nfasc. fco. fcp. feb. febr. fec. fed. fem. ff. fff. fid. fig. fil. film. fiziol. fiziot. flam. fm. fo. fol. folk.\nfrag. fran. franc. fsc. g. ga. gal. gdč. ge. gen. geod. geog. geotehnol. gg. gimn. glas. glav. gnr. go. gor.\ngosp. gp. graf. gram. gren. grš. gs. h. hab. hf. hist. ho. hort. i. ia. ib. ibid. id. idr. idridr. ill. imen.\nimp. impf. impr. in. inc. incl. ind. indus. inf. inform. ing. init. ins. int. inv. inšp. inštr. inž. is. islam.\nist. ital. iur. iz. izbr. izd. izg. izgr. izr. izv. j. jak. jam. jan. jav. je. jez. jr. jsl. jud. jug.\njugoslovan. jur. juž. jv. jz. k. kal. kan. kand. kat. kdo. kem. kip. kmet. kol. kom. komp. konf. kont. kost. kov. \nkp. kpfw. kr. kraj. krat. kub. kult. kv. kval. l. la. lab. lb. ld. let. lib. lik. litt. lj. ljud. ll. loc. log. \nloč. lt. ma. madž. mag. manag. manjš. masc. mass. mater. max. maxmax. mb. md. mech. medic. medij. medn. \nmehč. mem. menedž. mes. mess. metal. meteor. meteorol. mex. mi. mikr. mil. minn. mio. misc. miss. mit. mk. \nmkt. ml. mlad. mlle. mlr. mm. mme. množ. mo. moj. moš. možn. mr. mrd. mrs. ms. msc. msgr. mt. murr. mus. mut. \nn. na. nad. nadalj. nadom. nagl. nakl. namer. nan. naniz. nasl. nat. navt. nač. ned. nem. nik. nizoz. nm. nn. \nno. nom. norv. notr. nov. novogr. ns. o. ob. obd. obj. oblač. obl. oblik. obr. obraz. obs. obst. obt. obč. oc. \noct. od. odd. odg. odn. odst. odv. oec. off. ok. okla. okr. ont. oo. op. opis. opp. opr. orch. ord. ore. oreg. \norg. orient. orig. ork. ort. oseb. osn. ot. ozir. ošk. p. pag. par. para. parc. parl. part. past. pat. pdk. \npen. perf. pert. perz. pesn. pet. pev. pf. pfc. ph. pharm. phil. pis. pl. po. pod. podr. podaljš. pogl. pogoj. pojm. \npok. pokr. pol. poljed. poljub. polu. pom. pomen. pon. ponov. pop. por. port. pos. posl. posn. pov. pp. ppl. pr. \npraet. prav. pravopis. pravosl. preb. pred. predl. predm. predp. preds. pref. pregib. prel. prem. premen. prep. \npres. pret. prev. pribl. prih. pril. primerj. primor. prip. pripor. prir. prist. priv. proc. prof. prog. proiz. \nprom. pron. prop. prot. protest. prov. ps. pss. pt. publ. pz. q. qld. qu. quad. que. r. racc. rastl. razgl. \nrazl. razv. rd. red. ref. reg. rel. relig. rep. repr. rer. resp. rest. ret. rev. revol. rež. rim. rist. rkp. rm. \nroj. rom. romun. rp. rr. rt. rud. ruš. ry. sal. samogl. san. sc. scen. sci. scr. sdv. seg. sek. sen. sept. ser. \nsev. sg. sgt. sh. sig. sigg. sign. sim. sin. sing. sinh. skand. skl. sklad. sklanj. sklep. skr. sl. slik. slov. \nslovak. slovn. sn. so. sob. soc. sociol. sod. sopomen. sopr. sor. sov. sovj. sp. spec. spl. spr. spreg. sq. sr. \nsre. sred. sredoz. srh. ss. ssp. st. sta. stan. stanstar. stcsl. ste. stim. stol. stom. str. stroj. strok. stsl. \nstud. sup. supl. suppl. svet. sz. t. tab. tech. ted. tehn. tehnol. tek. teks. tekst. tel. temp. ten. teol. ter. \nterm. test. th. theol. tim. tip. tisočl. tit. tl. tol. tolmač. tom. tor. tov. tr. trad. traj. trans. tren. \ntrib. tril. trop. trp. trž. ts. tt. tu. tur. turiz. tvor. tvorb. tč. u. ul. umet. un. univ. up. upr. ur. urad. \nus. ust. utr. v. va. val. var. varn. ven. ver. verb. vest. vezal. vic. vis. viv. viz. viš. vod. vok. vol. vpr. \nvrst. vrstil. vs. vv. vzd. vzg. vzh. vzor. w. wed. wg. wk. x. y. z. zah. zaim. zak. zap. zasl. zavar. zač. zb. \nzdruž. zg. zn. znan. znanstv. zoot. zun. zv. zvd. á. é. ć. č. čas. čet. čl. člen. čustv. đ. ľ. ł. ş. ŠT. š. šir. \nškofl. škot. šol. št. števil. štud. ů. ű. žen. žival. \n'.split()
A:spacy.lang.sl.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/sl/stop_words.py----------------------------------------
A:spacy.lang.sl.stop_words.STOP_WORDS->set('\na ali \n\nb bi bil bila bile bili bilo biti blizu bo bodo bojo bolj bom bomo \nboste bova boš brez\n\nc cel cela celi celo\n\nč če često četrta četrtek četrti četrto čez čigav\n\nd da daleč dan danes datum deset deseta deseti deseto devet\ndeveta deveti deveto do dober dobra dobri dobro dokler dol dolg\ndolga dolgi dovolj drug druga drugi drugo dva dve\n\ne eden en ena ene eni enkrat eno etc.\n\nf\n\ng g. ga ga. gor gospa gospod \n\nh halo \n\ni idr. ii iii in iv ix iz\n\nj jaz je ji jih jim jo jutri\n\nk kadarkoli kaj kajti kako kakor kamor kamorkoli kar karkoli\nkaterikoli kdaj kdo kdorkoli ker ki kje kjer kjerkoli\nko koder koderkoli koga komu kot kratek kratka kratke kratki\n\nl lahka lahke lahki lahko le lep lepa lepe lepi lepo leto\n\nm majhen majhna majhni malce malo manj me med medtem mene\nmesec mi midva midve mnogo moj moja moje mora morajo moram\nmoramo morate moraš morem mu\n\nn na nad naj najina najino najmanj naju največ nam narobe\nnas nato nazaj naš naša naše ne nedavno nedelja nek neka\nnekaj nekatere nekateri nekatero nekdo neke nekega neki\nnekje neko nekoga nekoč ni nikamor nikdar nikjer nikoli\nnič nje njega njegov njegova njegovo njej njemu njen\nnjena njeno nji njih njihov njihova njihovo njiju njim\nnjo njun njuna njuno no nocoj npr.\n\no ob oba obe oboje od odprt odprta odprti okoli on\nonadva one oni onidve osem osma osmi osmo oz.\n\np pa pet peta petek peti peto po pod pogosto poleg poln\npolna polni polno ponavadi ponedeljek ponovno potem\npovsod pozdravljen pozdravljeni prav prava prave pravi\npravo prazen prazna prazno prbl. precej pred prej preko\npri pribl. približno primer pripravljen pripravljena\npripravljeni proti prva prvi prvo\n\nr ravno redko res reč\n\ns saj sam sama same sami samo se sebe sebi sedaj sedem\nsedma sedmi sedmo sem seveda si sicer skoraj skozi slab sm\nso sobota spet sreda srednja srednji sta ste stran stvar sva\n\nš šest šesta šesti šesto štiri \n\nt ta tak taka take taki tako takoj tam te tebe tebi tega\ntežak težka težki težko ti tista tiste tisti tisto tj.\ntja to toda torek tretja tretje tretji tri tu tudi tukaj\ntvoj tvoja tvoje\n\nu\n\nv vaju vam vas vaš vaša vaše ve vedno velik velika veliki\nveliko vendar ves več vi vidva vii viii visok visoka visoke\nvisoki vsa vsaj vsak vsaka vsakdo vsake vsaki vsakomur vse\nvsega vsi vso včasih včeraj \n\nx \n\nz za zadaj zadnji zakaj zaprta zaprti zaprto zdaj zelo zunaj\n\nž že\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/sl/punctuation.py----------------------------------------
A:spacy.lang.sl.punctuation.CONCAT_QUOTES->char_classes.CONCAT_QUOTES.replace("'", '').replace("'", '')


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/dsb/__init__.py----------------------------------------
spacy.lang.dsb.__init__.LowerSorbian(Language)
spacy.lang.dsb.__init__.LowerSorbianDefaults(BaseDefaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/dsb/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/dsb/lex_attrs.py----------------------------------------
A:spacy.lang.dsb.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.dsb.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
A:spacy.lang.dsb.lex_attrs.text_lower->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').lower()
spacy.lang.dsb.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/dsb/stop_words.py----------------------------------------
A:spacy.lang.dsb.stop_words.STOP_WORDS->set('\na abo aby ako ale až\n\ndaniž dokulaž\n\ngaž\n\njolic\n\npak pótom\n\nteke togodla\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/cs/__init__.py----------------------------------------
spacy.lang.cs.__init__.Czech(Language)
spacy.lang.cs.__init__.CzechDefaults(BaseDefaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/cs/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/cs/lex_attrs.py----------------------------------------
A:spacy.lang.cs.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.cs.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.cs.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/cs/stop_words.py----------------------------------------
A:spacy.lang.cs.stop_words.STOP_WORDS->set('\na\naby\nahoj\načkoli\nale\nalespoň\nanebo\nani\naniž\nano\natd.\natp.\nasi\naspoň\naž\nběhem\nbez\nbeze\nblízko\nbohužel\nbrzo\nbude\nbudeme\nbudeš\nbudete\nbudou\nbudu\nby\nbyl\nbyla\nbyli\nbylo\nbyly\nbys\nbýt\nčau\nchce\nchceme\nchceš\nchcete\nchci\nchtějí\nchtít\nchuť\nchuti\nco\ncož\ncz\nči\nčlánek\nčlánku\nčlánky\nčtrnáct\nčtyři\ndál\ndále\ndaleko\ndalší\nděkovat\nděkujeme\nděkuji\nden\ndeset\ndevatenáct\ndevět\ndnes\ndo\ndobrý\ndocela\ndva\ndvacet\ndvanáct\ndvě\nemail\nho\nhodně\ni\njá\njak\njakmile\njako\njakož\njde\nje\njeden\njedenáct\njedna\njedno\njednou\njedou\njeho\njehož\njej\njejí\njejich\njejichž\njehož\njelikož\njemu\njen\njenom\njenž\njež\nještě\njestli\njestliže\nještě\nji\njí\njich\njím\njim\njimi\njinak\njiné\njiž\njsi\njsme\njsem\njsou\njste\nk\nkam\nkaždý\nkde\nkdo\nkdy\nkdyž\nke\nkolik\nkromě\nkterá\nkterak\nkterou\nkteré\nkteří\nkterý\nkvůli\nku\nmá\nmají\nmálo\nmám\nmáme\nmáš\nmáte\nmé\nmě\nmezi\nmi\nmí\nmít\nmne\nmně\nmnou\nmoc\nmohl\nmohou\nmoje\nmoji\nmožná\nmůj\nmusí\nmůže\nmy\nna\nnad\nnade\nnám\nnámi\nnaproti\nnás\nnáš\nnaše\nnaši\nnačež\nne\nně\nnebo\nnebyl\nnebyla\nnebyli\nnebyly\nnechť\nněco\nnedělá\nnedělají\nnedělám\nneděláme\nneděláš\nneděláte\nnějak\nnejsi\nnejsou\nněkde\nněkdo\nnemají\nnemáme\nnemáte\nneměl\nněmu\nněmuž\nnení\nnestačí\nně\nnevadí\nnové\nnový\nnoví\nnež\nnic\nnich\nní\nním\nnimi\nnula\no\nod\node\non\nona\noni\nono\nony\nosm\nosmnáct\npak\npatnáct\npět\npo\npod\npokud\npořád\npouze\npotom\npozdě\npravé\npřed\npřede\npřes\npřece\npro\nproč\nprosím\nprostě\nproto\nproti\nprvní\nprávě\nprotože\npři\npřičemž\nrovně\ns\nse\nsedm\nsedmnáct\nsi\nsice\nskoro\nsic\nšest\nšestnáct\nskoro\nsmějí\nsmí\nsnad\nspolu\nsta\nsvůj\nsvé\nsvá\nsvých\nsvým\nsvými\nsvůj\nsté\nsto\nstrana\nta\ntady\ntak\ntakhle\ntaky\ntaké\ntakže\ntam\ntámhle\ntámhleto\ntamto\ntě\ntebe\ntebou\nteď\ntedy\nten\ntento\ntéto\nti\ntím\ntímto\ntisíc\ntisíce\nto\ntobě\ntohle\ntohoto\ntom\ntomto\ntomu\ntomuto\ntoto\ntřeba\ntři\ntřináct\ntrošku\ntrochu\ntu\ntuto\ntvá\ntvé\ntvoje\ntvůj\nty\ntyto\ntěm\ntěma\ntěmi\nu\nurčitě\nuž\nv\nvám\nvámi\nvás\nváš\nvaše\nvaši\nve\nvečer\nvedle\nvíce\nvlastně\nvšak\nvšechen\nvšechno\nvšichni\nvůbec\nvy\nvždy\nz\nzda\nza\nzde\nzač\nzatímco\nze\nže\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/sk/__init__.py----------------------------------------
spacy.lang.sk.__init__.Slovak(Language)
spacy.lang.sk.__init__.SlovakDefaults(BaseDefaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/sk/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/sk/lex_attrs.py----------------------------------------
A:spacy.lang.sk.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.sk.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.sk.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/sk/stop_words.py----------------------------------------
A:spacy.lang.sk.stop_words.STOP_WORDS->set('\na\naby\naj\nak\nakej\nakejže\nako\nakom\nakomže\nakou\nakouže\nakože\naká\nakáže\naké\nakého\nakéhože\nakému\nakémuže\nakéže\nakú\nakúže\naký\nakých\nakýchže\nakým\nakými\nakýmiže\nakýmže\nakýže\nale\nalebo\nani\nasi\navšak\naž\nba\nbez\nbezo\nbol\nbola\nboli\nbolo\nbude\nbudem\nbudeme\nbudete\nbudeš\nbudú\nbuď\nby\nbyť\ncez\ncezo\ndnes\ndo\nešte\nho\nhoci\ni\niba\nich\nim\ninej\ninom\niná\niné\niného\ninému\niní\ninú\niný\niných\niným\ninými\nja\nje\njeho\njej\njemu\nju\nk\nkam\nkamže\nkaždou\nkaždá\nkaždé\nkaždého\nkaždému\nkaždí\nkaždú\nkaždý\nkaždých\nkaždým\nkaždými\nkde\nkej\nkejže\nkeď\nkeďže\nkie\nkieho\nkiehože\nkiemu\nkiemuže\nkieže\nkoho\nkom\nkomu\nkou\nkouže\nkto\nktorej\nktorou\nktorá\nktoré\nktorí\nktorú\nktorý\nktorých\nktorým\nktorými\nku\nká\nkáže\nké\nkéže\nkú\nkúže\nký\nkýho\nkýhože\nkým\nkýmu\nkýmuže\nkýže\nlebo\nleda\nledaže\nlen\nma\nmajú\nmal\nmala\nmali\nmať\nmedzi\nmi\nmne\nmnou\nmoja\nmoje\nmojej\nmojich\nmojim\nmojimi\nmojou\nmoju\nmožno\nmu\nmusia\nmusieť\nmusí\nmusím\nmusíme\nmusíte\nmusíš\nmy\nmá\nmám\nmáme\nmáte\nmáš\nmôcť\nmôj\nmôjho\nmôže\nmôžem\nmôžeme\nmôžete\nmôžeš\nmôžu\nmňa\nna\nnad\nnado\nnajmä\nnami\nnaša\nnaše\nnašej\nnaši\nnašich\nnašim\nnašimi\nnašou\nne\nnech\nneho\nnej\nnejakej\nnejakom\nnejakou\nnejaká\nnejaké\nnejakého\nnejakému\nnejakú\nnejaký\nnejakých\nnejakým\nnejakými\nnemu\nnež\nnich\nnie\nniektorej\nniektorom\nniektorou\nniektorá\nniektoré\nniektorého\nniektorému\nniektorú\nniektorý\nniektorých\nniektorým\nniektorými\nnielen\nniečo\nnim\nnimi\nnič\nničoho\nničom\nničomu\nničím\nno\nnám\nnás\nnáš\nnášho\nním\no\nod\nodo\non\nona\noni\nono\nony\noň\noňho\npo\npod\npodo\npodľa\npokiaľ\npopod\npopri\npotom\npoza\npre\npred\npredo\npreto\npretože\nprečo\npri\npráve\ns\nsa\nseba\nsebe\nsebou\nsem\nsi\nsme\nso\nsom\nste\nsvoj\nsvoja\nsvoje\nsvojho\nsvojich\nsvojim\nsvojimi\nsvojou\nsvoju\nsvojím\nsú\nta\ntak\ntakej\ntakejto\ntaká\ntakáto\ntaké\ntakého\ntakéhoto\ntakému\ntakémuto\ntakéto\ntakí\ntakú\ntakúto\ntaký\ntakýto\ntakže\ntam\nteba\ntebe\ntebou\nteda\ntej\ntejto\nten\ntento\nti\ntie\ntieto\ntiež\nto\ntoho\ntohoto\ntohto\ntom\ntomto\ntomu\ntomuto\ntoto\ntou\ntouto\ntu\ntvoj\ntvoja\ntvoje\ntvojej\ntvojho\ntvoji\ntvojich\ntvojim\ntvojimi\ntvojím\nty\ntá\ntáto\ntí\ntíto\ntú\ntúto\ntých\ntým\ntými\ntýmto\nu\nuž\nv\nvami\nvaša\nvaše\nvašej\nvaši\nvašich\nvašim\nvaším\nveď\nviac\nvo\nvy\nvám\nvás\nváš\nvášho\nvšak\nvšetci\nvšetka\nvšetko\nvšetky\nvšetok\nz\nza\nzačo\nzačože\nzo\náno\nčej\nči\nčia\nčie\nčieho\nčiemu\nčiu\nčo\nčoho\nčom\nčomu\nčou\nčože\nčí\nčím\nčími\nďalšia\nďalšie\nďalšieho\nďalšiemu\nďalšiu\nďalšom\nďalšou\nďalší\nďalších\nďalším\nďalšími\nňom\nňou\nňu\nže\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/lg/__init__.py----------------------------------------
spacy.lang.lg.__init__.Luganda(Language)
spacy.lang.lg.__init__.LugandaDefaults(BaseDefaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/lg/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/lg/lex_attrs.py----------------------------------------
A:spacy.lang.lg.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.lg.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
A:spacy.lang.lg.lex_attrs.text_lower->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').lower()
spacy.lang.lg.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/lg/stop_words.py----------------------------------------
A:spacy.lang.lg.stop_words.STOP_WORDS->set('\nabadde abalala abamu abangi abava ajja ali alina ani anti ateekeddwa atewamu\natya awamu aweebwa ayinza ba baali babadde babalina bajja\nbajjanewankubade bali balina bandi bangi bano bateekeddwa baweebwa bayina bebombi beera bibye\nbimu bingi bino bo bokka bonna buli bulijjo bulungi bwabwe bwaffe bwayo bwe bwonna bya byabwe\nbyaffe byebimu byonna ddaa ddala ddi e ebimu ebiri ebweruobulungi ebyo edda ejja ekirala ekyo\nendala engeri ennyo era erimu erina ffe ffenna ga gujja gumu gunno guno gwa gwe kaseera kati\nkennyini ki kiki kikino kikye kikyo kino kirungi kki ku kubangabyombi kubangaolwokuba kudda\nkuva kuwa kwegamba kyaffe kye kyekimuoyo kyekyo kyonna leero liryo lwa lwaki lyabwezaabwe\nlyaffe lyange mbadde mingi mpozzi mu mulinaoyina munda mwegyabwe nolwekyo nabadde nabo nandiyagadde\nnandiye nanti naye ne nedda neera nga nnyingi nnyini nnyinza nnyo nti nyinza nze oba ojja okudda\nokugenda okuggyako okutuusa okuva okuwa oli olina oluvannyuma olwekyobuva omuli ono osobola otya\noyina oyo seetaaga si sinakindi singa talina tayina tebaali tebaalina tebayina terina tetulina\ntetuteekeddwa tewali teyalina teyayina tolina tu tuyina tulina tuyina twafuna twetaaga wa wabula\nwabweru wadde waggulunnina wakati waliwobangi waliyo wandi wange wano wansi weebwa yabadde yaffe\nye yenna yennyini yina yonna ziba zijja zonna\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/lg/punctuation.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/id/__init__.py----------------------------------------
spacy.lang.id.__init__.Indonesian(Language)
spacy.lang.id.__init__.IndonesianDefaults(BaseDefaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/id/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/id/lex_attrs.py----------------------------------------
A:spacy.lang.id.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.id.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
A:spacy.lang.id.lex_attrs.(_, num)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('-')
spacy.lang.id.lex_attrs.is_currency(text)
spacy.lang.id.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/id/_tokenizer_exceptions_list.py----------------------------------------
A:spacy.lang.id._tokenizer_exceptions_list.ID_BASE_EXCEPTIONS->set('\naba-aba\nabah-abah\nabal-abal\nabang-abang\nabar-abar\nabong-abong\nabrit-abrit\nabrit-abritan\nabu-abu\nabuh-abuhan\nabuk-abuk\nabun-abun\nacak-acak\nacak-acakan\nacang-acang\nacap-acap\naci-aci\naci-acian\naci-acinya\naco-acoan\nad-blocker\nad-interim\nada-ada\nada-adanya\nada-adanyakah\nadang-adang\nadap-adapan\nadd-on\nadd-ons\nadik-adik\nadik-beradik\naduk-adukan\nafter-sales\nagak-agak\nagak-agih\nagama-agama\nagar-agar\nage-related\nagut-agut\nair-air\nair-cooled\nair-to-air\najak-ajak\najar-ajar\naji-aji\nakal-akal\nakal-akalan\nakan-akan\nakar-akar\nakar-akaran\nakhir-akhir\nakhir-akhirnya\naki-aki\naksi-aksi\nalah-mengalahi\nalai-belai\nalan-alan\nalang-alang\nalang-alangan\nalap-alap\nalat-alat\nali-ali\nalif-alifan\nalih-alih\naling-aling\naling-alingan\nalip-alipan\nall-electric\nall-in-one\nall-out\nall-time\nalon-alon\nalt-right\nalt-text\nalu-alu\nalu-aluan\nalun-alun\nalur-alur\nalur-aluran\nalways-on\namai-amai\namatir-amatiran\nambah-ambah\nambai-ambai\nambil-mengambil\nambreng-ambrengan\nambring-ambringan\nambu-ambu\nambung-ambung\namin-amin\namit-amit\nampai-ampai\namprung-amprungan\namung-amung\nanai-anai\nanak-anak\nanak-anakan\nanak-beranak\nanak-cucu\nanak-istri\nancak-ancak\nancang-ancang\nancar-ancar\nandang-andang\nandeng-andeng\naneh-aneh\nangan-angan\nanggar-anggar\nanggaran-red\nanggota-anggota\nanggung-anggip\nangin-angin\nangin-anginan\nangkal-angkal\nangkul-angkul\nangkup-angkup\nangkut-angkut\nani-ani\naning-aning\nanjang-anjang\nanjing-anjing\nanjung-anjung\nanjung-anjungan\nantah-berantah\nantar-antar\nantar-mengantar\nante-mortem\nantek-antek\nanter-anter\nantihuru-hara\nanting-anting\nantung-antung\nanyam-menganyam\nanyang-anyang\napa-apa\napa-apaan\napel-apel\napi-api\napit-apit\naplikasi-aplikasi\napotek-apotek\naprit-apritan\napu-apu\napung-apung\narah-arah\narak-arak\narak-arakan\naram-aram\narek-arek\narem-arem\nari-ari\nartis-artis\naru-aru\narung-arungan\nasa-asaan\nasal-asalan\nasal-muasal\nasal-usul\nasam-asaman\nasas-asas\naset-aset\nasmaul-husna\nasosiasi-asosiasi\nasuh-asuh\nasyik-asyiknya\natas-mengatasi\nati-ati\natung-atung\naturan-aturan\naudio-video\naudio-visual\nauto-brightness\nauto-complete\nauto-focus\nauto-play\nauto-update\navant-garde\nawan-awan\nawan-berawan\nawang-awang\nawang-gemawang\nawar-awar\nawat-awat\nawik-awik\nawut-awutan\nayah-anak\nayak-ayak\nayam-ayam\nayam-ayaman\nayang-ayang\nayat-ayat\nayeng-ayengan\nayun-temayun\nayut-ayutan\nba-bi-bu\nback-to-back\nback-up\nbadan-badan\nbade-bade\nbadut-badut\nbagi-bagi\nbahan-bahan\nbahu-membahu\nbaik-baik\nbail-out\nbajang-bajang\nbaji-baji\nbalai-balai\nbalam-balam\nbalas-berbalas\nbalas-membalas\nbale-bale\nbaling-baling\nball-playing\nbalon-balon\nbalut-balut\nband-band\nbandara-bandara\nbangsa-bangsa\nbangun-bangun\nbangunan-bangunan\nbank-bank\nbantah-bantah\nbantahan-bantahan\nbantal-bantal\nbanyak-banyak\nbapak-anak\nbapak-bapak\nbapak-ibu\nbapak-ibunya\nbarang-barang\nbarat-barat\nbarat-daya\nbarat-laut\nbarau-barau\nbare-bare\nbareng-bareng\nbari-bari\nbarik-barik\nbaris-berbaris\nbaru-baru\nbaru-batu\nbarung-barung\nbasa-basi\nbata-bata\nbatalyon-batalyon\nbatang-batang\nbatas-batas\nbatir-batir\nbatu-batu\nbatuk-batuk\nbatung-batung\nbau-bauan\nbawa-bawa\nbayan-bayan\nbayang-bayang\nbayi-bayi\nbea-cukai\nbedeng-bedeng\nbedil-bedal\nbedil-bedilan\nbegana-begini\nbek-bek\nbekal-bekalan\nbekerdom-kerdom\nbekertak-kertak\nbelang-belang\nbelat-belit\nbeliau-beliau\nbelu-belai\nbelum-belum\nbenar-benar\nbenda-benda\nbengang-bengut\nbenggal-benggil\nbengkal-bengkil\nbengkang-bengkok\nbengkang-bengkong\nbengkang-bengkung\nbenteng-benteng\nbentuk-bentuk\nbenua-benua\nber-selfie\nberabad-abad\nberabun-rabun\nberacah-acah\nberada-ada\nberadik-berkakak\nberagah-agah\nberagak-agak\nberagam-ragam\nberaja-raja\nberakit-rakit\nberaku-akuan\nberalu-aluan\nberalun-alun\nberamah-ramah\nberamah-ramahan\nberamah-tamah\nberamai-ramai\nberambai-ambai\nberambal-ambalan\nberambil-ambil\nberamuk-amuk\nberamuk-amukan\nberandai-andai\nberandai-randai\nberaneh-aneh\nberang-berang\nberangan-angan\nberanggap-anggapan\nberangguk-angguk\nberangin-angin\nberangka-angka\nberangka-angkaan\nberangkai-rangkai\nberangkap-rangkapan\nberani-berani\nberanja-anja\nberantai-rantai\nberapi-api\nberapung-apung\nberarak-arakan\nberas-beras\nberasak-asak\nberasak-asakan\nberasap-asap\nberasing-asingan\nberatus-ratus\nberawa-rawa\nberawas-awas\nberayal-ayalan\nberayun-ayun\nberbagai-bagai\nberbahas-bahasan\nberbahasa-bahasa\nberbaik-baikan\nberbait-bait\nberbala-bala\nberbalas-balasan\nberbalik-balik\nberbalun-balun\nberbanjar-banjar\nberbantah-bantah\nberbanyak-banyak\nberbarik-barik\nberbasa-basi\nberbasah-basah\nberbatu-batu\nberbayang-bayang\nberbecak-becak\nberbeda-beda\nberbedil-bedilan\nberbega-bega\nberbeka-beka\nberbelah-belah\nberbelakang-belakangan\nberbelang-belang\nberbelau-belauan\nberbeli-beli\nberbeli-belian\nberbelit-belit\nberbelok-belok\nberbenang-benang\nberbenar-benar\nberbencah-bencah\nberbencol-bencol\nberbenggil-benggil\nberbentol-bentol\nberbentong-bentong\nberberani-berani\nberbesar-besar\nberbidai-bidai\nberbiduk-biduk\nberbiku-biku\nberbilik-bilik\nberbinar-binar\nberbincang-bincang\nberbingkah-bingkah\nberbintang-bintang\nberbintik-bintik\nberbintil-bintil\nberbisik-bisik\nberbolak-balik\nberbolong-bolong\nberbondong-bondong\nberbongkah-bongkah\nberbuai-buai\nberbual-bual\nberbudak-budak\nberbukit-bukit\nberbulan-bulan\nberbunga-bunga\nberbuntut-buntut\nberbunuh-bunuhan\nberburu-buru\nberburuk-buruk\nberbutir-butir\nbercabang-cabang\nbercaci-cacian\nbercakap-cakap\nbercakar-cakaran\nbercamping-camping\nbercantik-cantik\nbercari-cari\nbercari-carian\nbercarik-carik\nbercarut-carut\nbercebar-cebur\nbercepat-cepat\nbercerai-berai\nbercerai-cerai\nbercetai-cetai\nberciap-ciap\nbercikun-cikun\nbercinta-cintaan\nbercita-cita\nberciut-ciut\nbercompang-camping\nberconteng-conteng\nbercoreng-coreng\nbercoreng-moreng\nbercuang-caing\nbercuit-cuit\nbercumbu-cumbu\nbercumbu-cumbuan\nbercura-bura\nbercura-cura\nberdada-dadaan\nberdahulu-dahuluan\nberdalam-dalam\nberdalih-dalih\nberdampung-dampung\nberdebar-debar\nberdecak-decak\nberdecap-decap\nberdecup-decup\nberdecut-decut\nberdedai-dedai\nberdegap-degap\nberdegar-degar\nberdeham-deham\nberdekah-dekah\nberdekak-dekak\nberdekap-dekapan\nberdekat-dekat\nberdelat-delat\nberdembai-dembai\nberdembun-dembun\nberdempang-dempang\nberdempet-dempet\nberdencing-dencing\nberdendam-dendaman\nberdengkang-dengkang\nberdengut-dengut\nberdentang-dentang\nberdentum-dentum\nberdentung-dentung\nberdenyar-denyar\nberdenyut-denyut\nberdepak-depak\nberdepan-depan\nberderai-derai\nberderak-derak\nberderam-deram\nberderau-derau\nberderik-derik\nberdering-dering\nberderung-derung\nberderus-derus\nberdesak-desakan\nberdesik-desik\nberdesing-desing\nberdesus-desus\nberdikit-dikit\nberdingkit-dingkit\nberdua-dua\nberduri-duri\nberduru-duru\nberduyun-duyun\nberebut-rebut\nberebut-rebutan\nberegang-regang\nberek-berek\nberembut-rembut\nberempat-empat\nberenak-enak\nberencel-encel\nbereng-bereng\nberenggan-enggan\nberenteng-renteng\nberesa-esaan\nberesah-resah\nberfoya-foya\nbergagah-gagahan\nbergagap-gagap\nbergagau-gagau\nbergalur-galur\nberganda-ganda\nberganjur-ganjur\nberganti-ganti\nbergarah-garah\nbergaruk-garuk\nbergaya-gaya\nbergegas-gegas\nbergelang-gelang\nbergelap-gelap\nbergelas-gelasan\nbergeleng-geleng\nbergemal-gemal\nbergembar-gembor\nbergembut-gembut\nbergepok-gepok\nbergerek-gerek\nbergesa-gesa\nbergilir-gilir\nbergolak-golak\nbergolek-golek\nbergolong-golong\nbergores-gores\nbergotong-royong\nbergoyang-goyang\nbergugus-gugus\nbergulung-gulung\nbergulut-gulut\nbergumpal-gumpal\nbergunduk-gunduk\nbergunung-gunung\nberhadap-hadapan\nberhamun-hamun\nberhandai-handai\nberhanyut-hanyut\nberhari-hari\nberhati-hati\nberhati-hatilah\nberhektare-hektare\nberhilau-hilau\nberhormat-hormat\nberhujan-hujan\nberhura-hura\nberi-beri\nberi-memberi\nberia-ia\nberia-ria\nberiak-riak\nberiba-iba\nberibu-ribu\nberigi-rigi\nberimpit-impit\nberindap-indap\nbering-bering\nberingat-ingat\nberinggit-ringgit\nberintik-rintik\nberiring-iring\nberiring-iringan\nberita-berita\nberjabir-jabir\nberjaga-jaga\nberjagung-jagung\nberjalan-jalan\nberjalar-jalar\nberjalin-jalin\nberjalur-jalur\nberjam-jam\nberjari-jari\nberjauh-jauhan\nberjegal-jegalan\nberjejal-jejal\nberjela-jela\nberjengkek-jengkek\nberjenis-jenis\nberjenjang-jenjang\nberjilid-jilid\nberjinak-jinak\nberjingkat-jingkat\nberjingkik-jingkik\nberjingkrak-jingkrak\nberjongkok-jongkok\nberjubel-jubel\nberjujut-jujutan\nberjulai-julai\nberjumbai-jumbai\nberjumbul-jumbul\nberjuntai-juntai\nberjurai-jurai\nberjurus-jurus\nberjuta-juta\nberka-li-kali\nberkabu-kabu\nberkaca-kaca\nberkaing-kaing\nberkait-kaitan\nberkala-kala\nberkali-kali\nberkamit-kamit\nberkanjar-kanjar\nberkaok-kaok\nberkarung-karung\nberkasak-kusuk\nberkasih-kasihan\nberkata-kata\nberkatak-katak\nberkecai-kecai\nberkecek-kecek\nberkecil-kecil\nberkecil-kecilan\nberkedip-kedip\nberkejang-kejang\nberkejap-kejap\nberkejar-kejaran\nberkelar-kelar\nberkelepai-kelepai\nberkelip-kelip\nberkelit-kelit\nberkelok-kelok\nberkelompok-kelompok\nberkelun-kelun\nberkembur-kembur\nberkempul-kempul\nberkena-kenaan\nberkenal-kenalan\nberkendur-kendur\nberkeok-keok\nberkepak-kepak\nberkepal-kepal\nberkeping-keping\nberkepul-kepul\nberkeras-kerasan\nberkering-kering\nberkeritik-keritik\nberkeruit-keruit\nberkerut-kerut\nberketai-ketai\nberketak-ketak\nberketak-ketik\nberketap-ketap\nberketap-ketip\nberketar-ketar\nberketi-keti\nberketil-ketil\nberketuk-ketak\nberketul-ketul\nberkial-kial\nberkian-kian\nberkias-kias\nberkias-kiasan\nberkibar-kibar\nberkilah-kilah\nberkilap-kilap\nberkilat-kilat\nberkilau-kilauan\nberkilo-kilo\nberkimbang-kimbang\nberkinja-kinja\nberkipas-kipas\nberkira-kira\nberkirim-kiriman\nberkisar-kisar\nberkoak-koak\nberkoar-koar\nberkobar-kobar\nberkobok-kobok\nberkocak-kocak\nberkodi-kodi\nberkolek-kolek\nberkomat-kamit\nberkopah-kopah\nberkoper-koper\nberkotak-kotak\nberkuat-kuat\nberkuat-kuatan\nberkumur-kumur\nberkunang-kunang\nberkunar-kunar\nberkunjung-kunjungan\nberkurik-kurik\nberkurun-kurun\nberkusau-kusau\nberkusu-kusu\nberkusut-kusut\nberkuting-kuting\nberkutu-kutuan\nberlabun-labun\nberlain-lainan\nberlaju-laju\nberlalai-lalai\nberlama-lama\nberlambai-lambai\nberlambak-lambak\nberlampang-lampang\nberlanggar-langgar\nberlapang-lapang\nberlapis-lapis\nberlapuk-lapuk\nberlarah-larah\nberlarat-larat\nberlari-lari\nberlari-larian\nberlarih-larih\nberlarik-larik\nberlarut-larut\nberlawak-lawak\nberlayap-layapan\nberlebih-lebih\nberlebih-lebihan\nberleha-leha\nberlekas-lekas\nberlekas-lekasan\nberlekat-lekat\nberlekuk-lekuk\nberlempar-lemparan\nberlena-lena\nberlengah-lengah\nberlenggak-lenggok\nberlenggek-lenggek\nberlenggok-lenggok\nberleret-leret\nberletih-letih\nberliang-liuk\nberlibat-libat\nberligar-ligar\nberliku-liku\nberlikur-likur\nberlimbak-limbak\nberlimpah-limpah\nberlimpap-limpap\nberlimpit-limpit\nberlinang-linang\nberlindak-lindak\nberlipat-lipat\nberlomba-lomba\nberlompok-lompok\nberloncat-loncatan\nberlopak-lopak\nberlubang-lubang\nberlusin-lusin\nbermaaf-maafan\nbermabuk-mabukan\nbermacam-macam\nbermain-main\nbermalam-malam\nbermalas-malas\nbermalas-malasan\nbermanik-manik\nbermanis-manis\nbermanja-manja\nbermasak-masak\nbermati-mati\nbermegah-megah\nbermemek-memek\nbermenung-menung\nbermesra-mesraan\nbermewah-mewah\nbermewah-mewahan\nberminggu-minggu\nberminta-minta\nberminyak-minyak\nbermuda-muda\nbermudah-mudah\nbermuka-muka\nbermula-mula\nbermuluk-muluk\nbermulut-mulut\nbernafsi-nafsi\nbernaka-naka\nbernala-nala\nbernanti-nanti\nberniat-niat\nbernyala-nyala\nberogak-ogak\nberoleng-oleng\nberolok-olok\nberomong-omong\nberoncet-roncet\nberonggok-onggok\nberorang-orang\nberoyal-royal\nberpada-pada\nberpadu-padu\nberpahit-pahit\nberpair-pair\nberpal-pal\nberpalu-palu\nberpalu-paluan\nberpalun-palun\nberpanas-panas\nberpandai-pandai\nberpandang-pandangan\nberpangkat-pangkat\nberpanjang-panjang\nberpantun-pantun\nberpasang-pasang\nberpasang-pasangan\nberpasuk-pasuk\nberpayah-payah\nberpeluh-peluh\nberpeluk-pelukan\nberpenat-penat\nberpencar-pencar\nberpendar-pendar\nberpenggal-penggal\nberperai-perai\nberperang-perangan\nberpesai-pesai\nberpesta-pesta\nberpesuk-pesuk\nberpetak-petak\nberpeti-peti\nberpihak-pihak\nberpijar-pijar\nberpikir-pikir\nberpikul-pikul\nberpilih-pilih\nberpilin-pilin\nberpindah-pindah\nberpintal-pintal\nberpirau-pirau\nberpisah-pisah\nberpolah-polah\nberpolok-polok\nberpongah-pongah\nberpontang-panting\nberporah-porah\nberpotong-potong\nberpotong-potongan\nberpuak-puak\nberpual-pual\nberpugak-pugak\nberpuing-puing\nberpukas-pukas\nberpuluh-puluh\nberpulun-pulun\nberpuntal-puntal\nberpura-pura\nberpusar-pusar\nberpusing-pusing\nberpusu-pusu\nberputar-putar\nberrumpun-rumpun\nbersaf-saf\nbersahut-sahutan\nbersakit-sakit\nbersalah-salahan\nbersalam-salaman\nbersalin-salin\nbersalip-salipan\nbersama-sama\nbersambar-sambaran\nbersambut-sambutan\nbersampan-sampan\nbersantai-santai\nbersapa-sapaan\nbersarang-sarang\nbersedan-sedan\nbersedia-sedia\nbersedu-sedu\nbersejuk-sejuk\nbersekat-sekat\nberselang-selang\nberselang-seli\nberselang-seling\nberselang-tenggang\nberselit-selit\nberseluk-beluk\nbersembunyi-sembunyi\nbersembunyi-sembunyian\nbersembur-semburan\nbersempit-sempit\nbersenang-senang\nbersenang-senangkan\nbersenda-senda\nbersendi-sendi\nbersenggang-senggang\nbersenggau-senggau\nbersepah-sepah\nbersepak-sepakan\nbersepi-sepi\nberserak-serak\nberseri-seri\nberseru-seru\nbersesak-sesak\nbersetai-setai\nbersia-sia\nbersiap-siap\nbersiar-siar\nbersih-bersih\nbersikut-sikutan\nbersilir-silir\nbersimbur-simburan\nbersinau-sinau\nbersopan-sopan\nbersorak-sorai\nbersuap-suapan\nbersudah-sudah\nbersuka-suka\nbersuka-sukaan\nbersuku-suku\nbersulang-sulang\nbersumpah-sumpahan\nbersungguh-sungguh\nbersungut-sungut\nbersunyi-sunyi\nbersuruk-surukan\nbersusah-susah\nbersusuk-susuk\nbersusuk-susukan\nbersutan-sutan\nbertabur-tabur\nbertahan-tahan\nbertahu-tahu\nbertahun-tahun\nbertajuk-tajuk\nbertakik-takik\nbertala-tala\nbertalah-talah\nbertali-tali\nbertalu-talu\nbertalun-talun\nbertambah-tambah\nbertanda-tandaan\nbertangis-tangisan\nbertangkil-tangkil\nbertanya-tanya\nbertarik-tarikan\nbertatai-tatai\nbertatap-tatapan\nbertatih-tatih\nbertawan-tawan\nbertawar-tawaran\nbertebu-tebu\nbertebu-tebukan\nberteguh-teguh\nberteguh-teguhan\nberteka-teki\nbertelang-telang\nbertelau-telau\nbertele-tele\nbertembuk-tembuk\nbertempat-tempat\nbertempuh-tempuh\nbertenang-tenang\nbertenggang-tenggangan\nbertentu-tentu\nbertepek-tepek\nberterang-terang\nberterang-terangan\nberteriak-teriak\nbertikam-tikaman\nbertimbal-timbalan\nbertimbun-timbun\nbertimpa-timpa\nbertimpas-timpas\nbertingkah-tingkah\nbertingkat-tingkat\nbertinjau-tinjauan\nbertiras-tiras\nbertitar-titar\nbertitik-titik\nbertoboh-toboh\nbertolak-tolak\nbertolak-tolakan\nbertolong-tolongan\nbertonjol-tonjol\nbertruk-truk\nbertua-tua\nbertua-tuaan\nbertual-tual\nbertubi-tubi\nbertukar-tukar\nbertukar-tukaran\nbertukas-tukas\nbertumpak-tumpak\nbertumpang-tindih\nbertumpuk-tumpuk\nbertunda-tunda\nbertunjuk-tunjukan\nbertura-tura\nberturut-turut\nbertutur-tutur\nberuas-ruas\nberubah-ubah\nberulang-alik\nberulang-ulang\nberumbai-rumbai\nberundak-undak\nberundan-undan\nberundung-undung\nberunggas-runggas\nberunggun-unggun\nberunggut-unggut\nberungkur-ungkuran\nberuntai-untai\nberuntun-runtun\nberuntung-untung\nberunyai-unyai\nberupa-rupa\nberura-ura\nberuris-uris\nberurut-urutan\nberwarna-warna\nberwarna-warni\nberwindu-windu\nberwiru-wiru\nberyang-yang\nbesar-besar\nbesar-besaran\nbetak-betak\nbeti-beti\nbetik-betik\nbetul-betul\nbiang-biang\nbiar-biar\nbiaya-biaya\nbicu-bicu\nbidadari-bidadari\nbidang-bidang\nbijak-bijaklah\nbiji-bijian\nbila-bila\nbilang-bilang\nbincang-bincang\nbincang-bincut\nbingkah-bingkah\nbini-binian\nbintang-bintang\nbintik-bintik\nbio-oil\nbiri-biri\nbiru-biru\nbiru-hitam\nbiru-kuning\nbisik-bisik\nbiti-biti\nblak-blakan\nblok-blok\nbocah-bocah\nbohong-bohong\nbohong-bohongan\nbola-bola\nbolak-balik\nbolang-baling\nboleh-boleh\nbom-bom\nbomber-bomber\nbonek-bonek\nbongkar-bangkir\nbongkar-membongkar\nbongkar-pasang\nboro-boro\nbos-bos\nbottom-up\nbox-to-box\nboyo-boyo\nbuah-buahan\nbuang-buang\nbuat-buatan\nbuaya-buaya\nbubun-bubun\nbugi-bugi\nbuild-up\nbuilt-in\nbuilt-up\nbuka-buka\nbuka-bukaan\nbuka-tutup\nbukan-bukan\nbukti-bukti\nbuku-buku\nbulan-bulan\nbulan-bulanan\nbulang-baling\nbulang-bulang\nbulat-bulat\nbuli-buli\nbulu-bulu\nbuluh-buluh\nbulus-bulus\nbunga-bunga\nbunga-bungaan\nbunuh-membunuh\nbunyi-bunyian\nbupati-bupati\nbupati-wakil\nburu-buru\nburung-burung\nburung-burungan\nbus-bus\nbusiness-to-business\nbusur-busur\nbutir-butir\nby-pass\nbye-bye\ncabang-cabang\ncabik-cabik\ncabik-mencabik\ncabup-cawabup\ncaci-maki\ncagub-cawagub\ncaing-caing\ncakar-mencakar\ncakup-mencakup\ncalak-calak\ncalar-balar\ncaleg-caleg\ncalo-calo\ncalon-calon\ncampang-camping\ncampur-campur\ncapres-cawapres\ncara-cara\ncari-cari\ncari-carian\ncarut-marut\ncatch-up\ncawali-cawawali\ncawe-cawe\ncawi-cawi\ncebar-cebur\ncelah-celah\ncelam-celum\ncelangak-celinguk\ncelas-celus\nceledang-celedok\ncelengkak-celengkok\ncelingak-celinguk\ncelung-celung\ncemas-cemas\ncenal-cenil\ncengar-cengir\ncengir-cengir\ncengis-cengis\ncengking-mengking\ncentang-perenang\ncepat-cepat\nceplas-ceplos\ncerai-berai\ncerita-cerita\nceruk-menceruk\nceruk-meruk\ncetak-biru\ncetak-mencetak\ncetar-ceter\ncheck-in\ncheck-ins\ncheck-up\nchit-chat\nchoki-choki\ncingak-cinguk\ncipika-cipiki\nciri-ciri\nciri-cirinya\ncirit-birit\ncita-cita\ncita-citaku\nclose-up\nclosed-circuit\ncoba-coba\ncobak-cabik\ncobar-cabir\ncola-cala\ncolang-caling\ncomat-comot\ncomot-comot\ncompang-camping\ncomputer-aided\ncomputer-generated\ncondong-mondong\ncongak-cangit\nconggah-canggih\ncongkah-cangkih\ncongkah-mangkih\ncopak-capik\ncopy-paste\ncorak-carik\ncorat-coret\ncoreng-moreng\ncoret-coret\ncrat-crit\ncross-border\ncross-dressing\ncrypto-ransomware\ncuang-caing\ncublak-cublak\ncubung-cubung\nculik-culik\ncuma-cuma\ncumi-cumi\ncungap-cangip\ncupu-cupu\ndabu-dabu\ndaerah-daerah\ndag-dag\ndag-dig-dug\ndaging-dagingan\ndahulu-mendahului\ndalam-dalam\ndali-dali\ndam-dam\ndanau-danau\ndansa-dansi\ndapil-dapil\ndapur-dapur\ndari-dari\ndaru-daru\ndasar-dasar\ndatang-datang\ndatang-mendatangi\ndaun-daun\ndaun-daunan\ndawai-dawai\ndayang-dayang\ndayung-mayung\ndebak-debuk\ndebu-debu\ndeca-core\ndecision-making\ndeep-lying\ndeg-degan\ndegap-degap\ndekak-dekak\ndekat-dekat\ndengar-dengaran\ndengking-mendengking\ndepartemen-departemen\ndepo-depo\ndeputi-deputi\ndesa-desa\ndesa-kota\ndesas-desus\ndetik-detik\ndewa-dewa\ndewa-dewi\ndewan-dewan\ndewi-dewi\ndial-up\ndiam-diam\ndibayang-bayangi\ndibuat-buat\ndiiming-imingi\ndilebih-lebihkan\ndimana-mana\ndimata-matai\ndinas-dinas\ndinul-Islam\ndiobok-obok\ndiolok-olok\ndireksi-direksi\ndirektorat-direktorat\ndirjen-dirjen\ndirut-dirut\nditunggu-tunggu\ndivisi-divisi\ndo-it-yourself\ndoa-doa\ndog-dog\ndoggy-style\ndokok-dokok\ndolak-dalik\ndor-doran\ndorong-mendorong\ndosa-dosa\ndress-up\ndrive-in\ndua-dua\ndua-duaan\ndua-duanya\ndubes-dubes\nduduk-duduk\ndugaan-dugaan\ndulang-dulang\nduri-duri\nduta-duta\ndwi-kewarganegaraan\ne-arena\ne-billing\ne-budgeting\ne-cctv\ne-class\ne-commerce\ne-counting\ne-elektronik\ne-entertainment\ne-evolution\ne-faktur\ne-filing\ne-fin\ne-form\ne-government\ne-govt\ne-hakcipta\ne-id\ne-info\ne-katalog\ne-ktp\ne-leadership\ne-lhkpn\ne-library\ne-loket\ne-m1\ne-money\ne-news\ne-nisn\ne-npwp\ne-paspor\ne-paten\ne-pay\ne-perda\ne-perizinan\ne-planning\ne-polisi\ne-power\ne-punten\ne-retribusi\ne-samsat\ne-sport\ne-store\ne-tax\ne-ticketing\ne-tilang\ne-toll\ne-visa\ne-voting\ne-wallet\ne-warong\necek-ecek\neco-friendly\neco-park\nedan-edanan\neditor-editor\neditor-in-chief\nefek-efek\nekonomi-ekonomi\neksekutif-legislatif\nekspor-impor\nelang-elang\nelemen-elemen\nemak-emak\nembuh-embuhan\nempat-empat\nempek-empek\nempet-empetan\nempok-empok\nempot-empotan\nenak-enak\nencal-encal\nend-to-end\nend-user\nendap-endap\nendut-endut\nendut-endutan\nengah-engah\nengap-engap\nenggan-enggan\nengkah-engkah\nengket-engket\nentah-berentah\nenten-enten\nentry-level\nequity-linked\nerang-erot\nerat-erat\nerek-erek\nereng-ereng\nerong-erong\nesek-esek\nex-officio\nexchange-traded\nexercise-induced\nextra-time\nface-down\nface-to-face\nfair-play\nfakta-fakta\nfaktor-faktor\nfakultas-fakultas\nfase-fase\nfast-food\nfeed-in\nfifty-fifty\nfile-file\nfirst-leg\nfirst-team\nfitur-fitur\nfitur-fiturnya\nfixed-income\nflip-flop\nflip-plop\nfly-in\nfollow-up\nfoto-foto\nfoya-foya\nfraksi-fraksi\nfree-to-play\nfront-end\nfungsi-fungsi\ngaba-gaba\ngabai-gabai\ngada-gada\ngading-gading\ngadis-gadis\ngado-gado\ngail-gail\ngajah-gajah\ngajah-gajahan\ngala-gala\ngaleri-galeri\ngali-gali\ngali-galian\ngaling-galing\ngalu-galu\ngamak-gamak\ngambar-gambar\ngambar-menggambar\ngamit-gamitan\ngampang-gampangan\ngana-gini\nganal-ganal\nganda-berganda\nganjal-mengganjal\nganjil-genap\nganteng-ganteng\ngantung-gantung\ngapah-gopoh\ngara-gara\ngarah-garah\ngaris-garis\ngasak-gasakan\ngatal-gatal\ngaun-gaun\ngawar-gawar\ngaya-gayanya\ngayang-gayang\nge-er\ngebyah-uyah\ngebyar-gebyar\ngedana-gedini\ngedebak-gedebuk\ngedebar-gedebur\ngedung-gedung\ngelang-gelang\ngelap-gelapan\ngelar-gelar\ngelas-gelas\ngelembung-gelembungan\ngeleng-geleng\ngeli-geli\ngeliang-geliut\ngeliat-geliut\ngembar-gembor\ngembrang-gembreng\ngempul-gempul\ngempur-menggempur\ngendang-gendang\ngengsi-gengsian\ngenjang-genjot\ngenjot-genjotan\ngenjrang-genjreng\ngenome-wide\ngeo-politik\ngerabak-gerubuk\ngerak-gerik\ngerak-geriknya\ngerakan-gerakan\ngerbas-gerbus\ngereja-gereja\ngereng-gereng\ngeriak-geriuk\ngerit-gerit\ngerot-gerot\ngeruh-gerah\ngetak-getuk\ngetem-getem\ngeti-geti\ngial-gial\ngial-giul\ngila-gila\ngila-gilaan\ngilang-gemilang\ngilap-gemilap\ngili-gili\ngiling-giling\ngilir-bergilir\nginang-ginang\ngirap-girap\ngirik-girik\ngiring-giring\ngo-auto\ngo-bills\ngo-bluebird\ngo-box\ngo-car\ngo-clean\ngo-food\ngo-glam\ngo-jek\ngo-kart\ngo-mart\ngo-massage\ngo-med\ngo-points\ngo-pulsa\ngo-ride\ngo-send\ngo-shop\ngo-tix\ngo-to-market\ngoak-goak\ngoal-line\ngol-gol\ngolak-galik\ngondas-gandes\ngonjang-ganjing\ngonjlang-ganjling\ngonta-ganti\ngontok-gontokan\ngorap-gorap\ngorong-gorong\ngotong-royong\ngresek-gresek\ngua-gua\ngual-gail\ngubernur-gubernur\ngudu-gudu\ngula-gula\ngulang-gulang\ngulung-menggulung\nguna-ganah\nguna-guna\ngundala-gundala\nguntang-guntang\ngunung-ganang\ngunung-gemunung\ngunung-gunungan\nguru-guru\nhabis-habis\nhabis-habisan\nhak-hak\nhak-hal\nhakim-hakim\nhal-hal\nhalai-balai\nhalf-time\nhama-hama\nhampir-hampir\nhancur-hancuran\nhancur-menghancurkan\nhands-free\nhands-on\nhang-out\nhantu-hantu\nhappy-happy\nharap-harap\nharap-harapan\nhard-disk\nharga-harga\nhari-hari\nharimau-harimau\nharum-haruman\nhasil-hasil\nhasta-wara\nhat-trick\nhati-hati\nhati-hatilah\nhead-mounted\nhead-to-head\nhead-up\nheads-up\nheavy-duty\nhebat-hebatan\nhewan-hewan\nhexa-core\nhidup-hidup\nhidup-mati\nhila-hila\nhilang-hilang\nhina-menghinakan\nhip-hop\nhiru-biru\nhiru-hara\nhiruk-pikuk\nhitam-putih\nhitung-hitung\nhitung-hitungan\nhormat-menghormati\nhot-swappable\nhotel-hotel\nhow-to\nhubar-habir\nhubaya-hubaya\nhukum-red\nhukuman-hukuman\nhula-hoop\nhula-hula\nhulu-hilir\nhumas-humas\nhura-hura\nhuru-hara\nibar-ibar\nibu-anak\nibu-ibu\nicak-icak\nicip-icip\nidam-idam\nide-ide\nigau-igauan\nikan-ikan\nikut-ikut\nikut-ikutan\nilam-ilam\nilat-ilatan\nilmu-ilmu\nimbang-imbangan\niming-iming\nimut-imut\ninang-inang\ninca-binca\nincang-incut\nindustri-industri\ningar-bingar\ningar-ingar\ningat-ingat\ningat-ingatan\ningau-ingauan\ninggang-inggung\ninjak-injak\ninput-output\ninstansi-instansi\ninstant-on\ninstrumen-instrumen\ninter-governmental\nira-ira\nirah-irahan\niras-iras\niring-iringan\niris-irisan\nisak-isak\nisat-bb\niseng-iseng\nistana-istana\nistri-istri\nisu-isu\niya-iya\njabatan-jabatan\njadi-jadian\njagoan-jagoan\njaja-jajaan\njaksa-jaksa\njala-jala\njalan-jalan\njali-jali\njalin-berjalin\njalin-menjalin\njam-jam\njamah-jamahan\njambak-jambakan\njambu-jambu\njampi-jampi\njanda-janda\njangan-jangan\njanji-janji\njarang-jarang\njari-jari\njaring-jaring\njarum-jarum\njasa-jasa\njatuh-bangun\njauh-dekat\njauh-jauh\njawi-jawi\njebar-jebur\njebat-jebatan\njegal-jegalan\njejak-jejak\njelang-menjelang\njelas-jelas\njelur-jelir\njembatan-jembatan\njenazah-jenazah\njendal-jendul\njenderal-jenderal\njenggar-jenggur\njenis-jenis\njenis-jenisnya\njentik-jentik\njerah-jerih\njinak-jinak\njiwa-jiwa\njoli-joli\njolong-jolong\njongkang-jangking\njongkar-jangkir\njongkat-jangkit\njor-joran\njotos-jotosan\njuak-juak\njual-beli\njuang-juang\njulo-julo\njulung-julung\njulur-julur\njumbai-jumbai\njungkang-jungkit\njungkat-jungkit\njurai-jurai\nkabang-kabang\nkabar-kabari\nkabir-kabiran\nkabruk-kabrukan\nkabu-kabu\nkabupaten-kabupaten\nkabupaten-kota\nkaca-kaca\nkacang-kacang\nkacang-kacangan\nkacau-balau\nkadang-kadang\nkader-kader\nkades-kades\nkadis-kadis\nkail-kail\nkain-kain\nkait-kait\nkakak-adik\nkakak-beradik\nkakak-kakak\nkakek-kakek\nkakek-nenek\nkaki-kaki\nkala-kala\nkalau-kalau\nkaleng-kalengan\nkali-kalian\nkalimat-kalimat\nkalung-kalung\nkalut-malut\nkambing-kambing\nkamit-kamit\nkampung-kampung\nkampus-kampus\nkanak-kanak\nkanak-kanan\nkanan-kanak\nkanan-kiri\nkangen-kangenan\nkanwil-kanwil\nkapa-kapa\nkapal-kapal\nkapan-kapan\nkapolda-kapolda\nkapolres-kapolres\nkapolsek-kapolsek\nkapu-kapu\nkarang-karangan\nkarang-mengarang\nkareseh-peseh\nkarut-marut\nkarya-karya\nkasak-kusuk\nkasus-kasus\nkata-kata\nkatang-katang\nkava-kava\nkawa-kawa\nkawan-kawan\nkawin-cerai\nkawin-mawin\nkayu-kayu\nkayu-kayuan\nke-Allah-an\nkeabu-abuan\nkearab-araban\nkeasyik-asyikan\nkebarat-baratan\nkebasah-basahan\nkebat-kebit\nkebata-bataan\nkebayi-bayian\nkebelanda-belandaan\nkeberlarut-larutan\nkebesar-hatian\nkebiasaan-kebiasaan\nkebijakan-kebijakan\nkebiru-biruan\nkebudak-budakan\nkebun-kebun\nkebut-kebutan\nkecamatan-kecamatan\nkecentang-perenangan\nkecil-kecil\nkecil-kecilan\nkecil-mengecil\nkecokelat-cokelatan\nkecomak-kecimik\nkecuh-kecah\nkedek-kedek\nkedekak-kedekik\nkedesa-desaan\nkedubes-kedubes\nkedutaan-kedutaan\nkeempat-empatnya\nkegadis-gadisan\nkegelap-gelapan\nkegiatan-kegiatan\nkegila-gilaan\nkegirang-girangan\nkehati-hatian\nkeheran-heranan\nkehijau-hijauan\nkehitam-hitaman\nkeinggris-inggrisan\nkejaga-jagaan\nkejahatan-kejahatan\nkejang-kejang\nkejar-kejar\nkejar-kejaran\nkejar-mengejar\nkejingga-jinggaan\nkejut-kejut\nkejutan-kejutan\nkekabur-kaburan\nkekanak-kanakan\nkekoboi-koboian\nkekota-kotaan\nkekuasaan-kekuasaan\nkekuning-kuningan\nkelak-kelik\nkelak-keluk\nkelaki-lakian\nkelang-kelok\nkelap-kelip\nkelasah-kelusuh\nkelek-kelek\nkelek-kelekan\nkelemak-kelemek\nkelik-kelik\nkelip-kelip\nkelompok-kelompok\nkelontang-kelantung\nkeluar-masuk\nkelurahan-kelurahan\nkelusuh-kelasah\nkelut-melut\nkemak-kemik\nkemalu-maluan\nkemana-mana\nkemanja-manjaan\nkemarah-marahan\nkemasam-masaman\nkemati-matian\nkembang-kembang\nkemenpan-rb\nkementerian-kementerian\nkemerah-merahan\nkempang-kempis\nkempas-kempis\nkemuda-mudaan\nkena-mengena\nkenal-mengenal\nkenang-kenangan\nkencang-kencung\nkencing-mengencingi\nkencrang-kencring\nkendang-kendang\nkendang-kendangan\nkeningrat-ningratan\nkentung-kentung\nkenyat-kenyit\nkepala-kepala\nkepala-kepalaan\nkepandir-pandiran\nkepang-kepot\nkeperak-perakan\nkepetah-lidahan\nkepilu-piluan\nkeping-keping\nkepucat-pucatan\nkepuh-kepuh\nkepura-puraan\nkeputih-putihan\nkerah-kerahan\nkerancak-rancakan\nkerang-kerangan\nkerang-keroh\nkerang-kerot\nkerang-keruk\nkerang-kerung\nkerap-kerap\nkeras-mengerasi\nkercap-kercip\nkercap-kercup\nkeriang-keriut\nkerja-kerja\nkernyat-kernyut\nkerobak-kerabit\nkerobak-kerobek\nkerobak-kerobik\nkerobat-kerabit\nkerong-kerong\nkeropas-kerapis\nkertak-kertuk\nkertap-kertap\nkeruntang-pungkang\nkesalahan-kesalahan\nkesap-kesip\nkesemena-menaan\nkesenak-senakan\nkesewenang-wenangan\nkesia-siaan\nkesik-kesik\nkesipu-sipuan\nkesu-kesi\nkesuh-kesih\nkesuk-kesik\nketakar-keteker\nketakutan-ketakutan\nketap-ketap\nketap-ketip\nketar-ketir\nketentuan-ketentuan\nketergesa-gesaan\nketi-keti\nketidur-tiduran\nketiga-tiganya\nketir-ketir\nketua-ketua\nketua-tuaan\nketuan-tuanan\nkeungu-unguan\nkewangi-wangian\nki-ka\nkia-kia\nkiai-kiai\nkiak-kiak\nkial-kial\nkiang-kiut\nkiat-kiat\nkibang-kibut\nkicang-kecoh\nkicang-kicu\nkick-off\nkida-kida\nkijang-kijang\nkilau-mengilau\nkili-kili\nkilik-kilik\nkincir-kincir\nkios-kios\nkira-kira\nkira-kiraan\nkiri-kanan\nkirim-berkirim\nkisah-kisah\nkisi-kisi\nkitab-kitab\nkitang-kitang\nkiu-kiu\nklaim-klaim\nklik-klikan\nklip-klip\nklub-klub\nkluntang-klantung\nknock-knock\nknock-on\nknock-out\nko-as\nko-pilot\nkoak-koak\nkoboi-koboian\nkocah-kacih\nkocar-kacir\nkodam-kodam\nkode-kode\nkodim-kodim\nkodok-kodok\nkolang-kaling\nkole-kole\nkoleh-koleh\nkolong-kolong\nkoma-koma\nkomat-kamit\nkomisaris-komisaris\nkomisi-komisi\nkomite-komite\nkomoditas-komoditas\nkongko-kongko\nkonsulat-konsulat\nkonsultan-konsultan\nkontal-kantil\nkontang-kanting\nkontra-terorisme\nkontrak-kontrak\nkonvensi-konvensi\nkopat-kapit\nkoperasi-koperasi\nkopi-kopi\nkoran-koran\nkoreng-koreng\nkos-kosan\nkosak-kasik\nkota-kota\nkota-wakil\nkotak-katik\nkotak-kotak\nkoyak-koyak\nkuas-kuas\nkuat-kuat\nkubu-kubuan\nkucar-kacir\nkucing-kucing\nkucing-kucingan\nkuda-kuda\nkuda-kudaan\nkudap-kudap\nkue-kue\nkulah-kulah\nkulak-kulak\nkulik-kulik\nkulum-kulum\nkumat-kamit\nkumpul-kumpul\nkunang-kunang\nkunar-kunar\nkung-fu\nkuning-hitam\nkupat-kapit\nkupu-kupu\nkura-kura\nkurang-kurang\nkusat-mesat\nkutat-kutet\nkuti-kuti\nkuwung-kuwung\nkyai-kyai\nlaba-laba\nlabi-labi\nlabu-labu\nlaga-laga\nlagi-lagi\nlagu-lagu\nlaguh-lagah\nlain-lain\nlaki-laki\nlalu-lalang\nlalu-lintas\nlama-kelamaan\nlama-lama\nlamat-lamat\nlambat-lambat\nlampion-lampion\nlampu-lampu\nlancang-lancang\nlancar-lancar\nlangak-longok\nlanggar-melanggar\nlangit-langit\nlangkah-langka\nlangkah-langkah\nlanja-lanjaan\nlapas-lapas\nlapat-lapat\nlaporan-laporan\nlaptop-tablet\nlarge-scale\nlari-lari\nlari-larian\nlaskar-laskar\nlauk-pauk\nlaun-laun\nlaut-timur\nlawah-lawah\nlawak-lawak\nlawan-lawan\nlawi-lawi\nlayang-layang\nlayu-layuan\nlebih-lebih\nlecet-lecet\nlegak-legok\nlegum-legum\nlegup-legup\nleha-leha\nlekak-lekuk\nlekap-lekup\nlekas-lekas\nlekat-lekat\nlekuh-lekih\nlekum-lekum\nlekup-lekap\nlembaga-lembaga\nlempar-lemparan\nlenggak-lenggok\nlenggok-lenggok\nlenggut-lenggut\nlengket-lengket\nlentam-lentum\nlentang-lentok\nlentang-lentung\nlepa-lepa\nlerang-lerang\nlereng-lereng\nlese-majeste\nletah-letai\nlete-lete\nletuk-letuk\nletum-letum\nletup-letup\nleyeh-leyeh\nliang-liuk\nliang-liut\nliar-liar\nliat-liut\nlidah-lidah\nlife-toxins\nliga-liga\nlight-emitting\nlika-liku\nlil-alamin\nlilin-lilin\nline-up\nlintas-selat\nlipat-melipat\nliquid-cooled\nlithium-ion\nlithium-polymer\nliuk-liuk\nliung-liung\nlobi-lobi\nlock-up\nlocked-in\nlokasi-lokasi\nlong-term\nlongak-longok\nlontang-lanting\nlontang-lantung\nlopak-lapik\nlopak-lopak\nlow-cost\nlow-density\nlow-end\nlow-light\nlow-multi\nlow-pass\nlucu-lucu\nluka-luka\nlukisan-lukisan\nlumba-lumba\nlumi-lumi\nluntang-lantung\nlupa-lupa\nlupa-lupaan\nlurah-camat\nmaaf-memaafkan\nmabuk-mabukan\nmabul-mabul\nmacam-macam\nmacan-macanan\nmachine-to-machine\nmafia-mafia\nmahasiswa-mahasiswi\nmahasiswa/i\nmahi-mahi\nmain-main\nmain-mainan\nmain-mainlah\nmajelis-majelis\nmaju-mundur\nmakam-makam\nmakan-makan\nmakan-makanan\nmakanan-red\nmake-up\nmaki-maki\nmaki-makian\nmal-mal\nmalai-malai\nmalam-malam\nmalar-malar\nmalas-malasan\nmali-mali\nmalu-malu\nmama-mama\nman-in-the-middle\nmana-mana\nmanajer-manajer\nmanik-manik\nmanis-manis\nmanis-manisan\nmarah-marah\nmark-up\nmas-mas\nmasa-masa\nmasak-masak\nmasalah-masalah\nmash-up\nmasing-masing\nmasjid-masjid\nmasuk-keluar\nmat-matan\nmata-mata\nmatch-fixing\nmati-mati\nmati-matian\nmaya-maya\nmayat-mayat\nmayday-mayday\nmedia-media\nmega-bintang\nmega-tsunami\nmegal-megol\nmegap-megap\nmeger-meger\nmegrek-megrek\nmelak-melak\nmelambai-lambai\nmelambai-lambaikan\nmelambat-lambatkan\nmelaun-laun\nmelawak-lawak\nmelayang-layang\nmelayap-layap\nmelayap-layapkan\nmelebih-lebihi\nmelebih-lebihkan\nmelejang-lejangkan\nmelek-melekan\nmeleleh-leleh\nmelengah-lengah\nmelihat-lihat\nmelimpah-limpah\nmelincah-lincah\nmeliuk-liuk\nmelolong-lolong\nmelompat-lompat\nmeloncat-loncat\nmelonco-lonco\nmelongak-longok\nmelonjak-lonjak\nmemacak-macak\nmemada-madai\nmemadan-madan\nmemaki-maki\nmemaksa-maksa\nmemanas-manasi\nmemancit-mancitkan\nmemandai-mandai\nmemanggil-manggil\nmemanis-manis\nmemanjut-manjut\nmemantas-mantas\nmemasak-masak\nmemata-matai\nmematah-matah\nmematuk-matuk\nmematut-matut\nmemau-mau\nmemayah-mayahkan\nmembaca-baca\nmembacah-bacah\nmembagi-bagikan\nmembalik-balik\nmembangkit-bangkit\nmembarut-barut\nmembawa-bawa\nmembayang-bayangi\nmembayang-bayangkan\nmembeda-bedakan\nmembelai-belai\nmembeli-beli\nmembelit-belitkan\nmembelu-belai\nmembenar-benar\nmembenar-benari\nmemberai-beraikan\nmembesar-besar\nmembesar-besarkan\nmembikin-bikin\nmembilah-bilah\nmembolak-balikkan\nmembongkar-bangkir\nmembongkar-bongkar\nmembuang-buang\nmembuat-buat\nmembulan-bulani\nmembunga-bungai\nmembungkuk-bungkuk\nmemburu-buru\nmemburu-burukan\nmemburuk-burukkan\nmemelintir-melintir\nmemencak-mencak\nmemencar-mencar\nmemercik-mercik\nmemetak-metak\nmemetang-metangkan\nmemetir-metir\nmemijar-mijar\nmemikir-mikir\nmemikir-mikirkan\nmemilih-milih\nmemilin-milin\nmeminang-minang\nmeminta-minta\nmemisah-misahkan\nmemontang-mantingkan\nmemorak-perandakan\nmemorak-porandakan\nmemotong-motong\nmemperamat-amat\nmemperamat-amatkan\nmemperbagai-bagaikan\nmemperganda-gandakan\nmemperganduh-ganduhkan\nmemperimpit-impitkan\nmemperkuda-kudakan\nmemperlengah-lengah\nmemperlengah-lengahkan\nmempermacam-macamkan\nmemperolok-olok\nmemperolok-olokkan\nmempersama-samakan\nmempertubi-tubi\nmempertubi-tubikan\nmemperturut-turutkan\nmemuja-muja\nmemukang-mukang\nmemulun-mulun\nmemundi-mundi\nmemundi-mundikan\nmemutar-mutar\nmemuyu-muyu\nmen-tweet\nmenagak-nagak\nmenakut-nakuti\nmenang-kalah\nmenanjur-nanjur\nmenanti-nanti\nmenari-nari\nmencabik-cabik\nmencabik-cabikkan\nmencacah-cacah\nmencaing-caing\nmencak-mencak\nmencakup-cakup\nmencapak-capak\nmencari-cari\nmencarik-carik\nmencarik-carikkan\nmencarut-carut\nmencengis-cengis\nmencepak-cepak\nmencepuk-cepuk\nmencerai-beraikan\nmencetai-cetai\nmenciak-ciak\nmenciap-ciap\nmenciar-ciar\nmencita-citakan\nmencium-cium\nmenciut-ciut\nmencla-mencle\nmencoang-coang\nmencoba-coba\nmencocok-cocok\nmencolek-colek\nmenconteng-conteng\nmencubit-cubit\nmencucuh-cucuh\nmencucuh-cucuhkan\nmencuri-curi\nmendecap-decap\nmendegam-degam\nmendengar-dengar\nmendengking-dengking\nmendengus-dengus\nmendengut-dengut\nmenderai-deraikan\nmenderak-derakkan\nmenderau-derau\nmenderu-deru\nmendesas-desuskan\nmendesus-desus\nmendetap-detap\nmendewa-dewakan\nmendudu-dudu\nmenduga-duga\nmenebu-nebu\nmenegur-neguri\nmenepak-nepak\nmenepak-nepakkan\nmengabung-ngabung\nmengaci-acikan\nmengacu-acu\nmengada-ada\nmengada-ngada\nmengadang-adangi\nmengaduk-aduk\nmengagak-agak\nmengagak-agihkan\nmengagut-agut\nmengais-ngais\nmengalang-alangi\nmengali-ali\nmengalur-alur\nmengamang-amang\nmengamat-amati\nmengambai-ambaikan\nmengambang-ambang\nmengambung-ambung\nmengambung-ambungkan\nmengamit-ngamitkan\nmengancai-ancaikan\nmengancak-ancak\nmengancar-ancar\nmengangan-angan\nmengangan-angankan\nmengangguk-angguk\nmenganggut-anggut\nmengangin-anginkan\nmengangkat-angkat\nmenganjung-anjung\nmenganjung-anjungkan\nmengap-mengap\nmengapa-apai\nmengapi-apikan\nmengarah-arahi\nmengarang-ngarang\nmengata-ngatai\nmengatup-ngatupkan\nmengaum-aum\nmengaum-aumkan\nmengejan-ejan\nmengejar-ngejar\nmengejut-ngejuti\nmengelai-ngelai\nmengelepik-ngelepik\nmengelip-ngelip\nmengelu-elukan\nmengelus-elus\nmengembut-embut\nmengempas-empaskan\nmengenap-enapkan\nmengendap-endap\nmengenjak-enjak\nmengentak-entak\nmengentak-entakkan\nmengepak-ngepak\nmengepak-ngepakkan\nmengepal-ngepalkan\nmengerjap-ngerjap\nmengerling-ngerling\nmengertak-ngertakkan\nmengesot-esot\nmenggaba-gabai\nmenggali-gali\nmenggalur-galur\nmenggamak-gamak\nmenggamit-gamitkan\nmenggapai-gapai\nmenggapai-gapaikan\nmenggaruk-garuk\nmenggebu-gebu\nmenggebyah-uyah\nmenggeleng-gelengkan\nmenggelepar-gelepar\nmenggelepar-geleparkan\nmenggeliang-geliutkan\nmenggelinding-gelinding\nmenggemak-gemak\nmenggembar-gemborkan\nmenggerak-gerakkan\nmenggerecak-gerecak\nmenggesa-gesakan\nmenggili-gili\nmenggodot-godot\nmenggolak-galikkan\nmenggorek-gorek\nmenggoreng-goreng\nmenggosok-gosok\nmenggoyang-goyangkan\nmengguit-guit\nmenghalai-balaikan\nmenghalang-halangi\nmenghambur-hamburkan\nmenghinap-hinap\nmenghitam-memutihkan\nmenghitung-hitung\nmenghubung-hubungkan\nmenghujan-hujankan\nmengiang-ngiang\nmengibar-ngibarkan\nmengibas-ngibas\nmengibas-ngibaskan\nmengidam-idamkan\nmengilah-ngilahkan\nmengilai-ilai\nmengilat-ngilatkan\nmengilik-ngilik\nmengimak-imak\nmengimbak-imbak\nmengiming-iming\nmengincrit-incrit\nmengingat-ingat\nmenginjak-injak\nmengipas-ngipas\nmengira-ngira\nmengira-ngirakan\nmengiras-iras\nmengiras-irasi\nmengiris-iris\nmengitar-ngitar\nmengitik-ngitik\nmengodol-odol\nmengogok-ogok\nmengolak-alik\nmengolak-alikkan\nmengolang-aling\nmengolang-alingkan\nmengoleng-oleng\nmengolok-olok\nmengombang-ambing\nmengombang-ambingkan\nmengongkang-ongkang\nmengongkok-ongkok\nmengonyah-anyih\nmengopak-apik\nmengorak-arik\nmengorat-oret\nmengorek-ngorek\nmengoret-oret\nmengorok-orok\nmengotak-atik\nmengotak-ngatikkan\nmengotak-ngotakkan\nmengoyak-ngoyak\nmengoyak-ngoyakkan\nmengoyak-oyak\nmenguar-nguarkan\nmenguar-uarkan\nmengubah-ubah\nmengubek-ubek\nmenguber-uber\nmengubit-ubit\nmengubrak-abrik\nmengucar-ngacirkan\nmengucek-ngucek\nmengucek-ucek\nmenguik-uik\nmenguis-uis\nmengulang-ulang\nmengulas-ulas\nmengulit-ulit\nmengulum-ngulum\nmengulur-ulur\nmenguman-uman\nmengumbang-ambingkan\nmengumpak-umpak\nmengungkat-ungkat\nmengungkit-ungkit\nmengupa-upa\nmengurik-urik\nmengusil-usil\nmengusil-usilkan\nmengutak-atik\nmengutak-ngatikkan\nmengutik-ngutik\nmengutik-utik\nmenika-nika\nmenimang-nimang\nmenimbang-nimbang\nmenimbun-nimbun\nmenimpang-nimpangkan\nmeningkat-ningkat\nmeniru-niru\nmenit-menit\nmenitar-nitarkan\nmeniup-niup\nmenjadi-jadi\nmenjadi-jadikan\nmenjedot-jedotkan\nmenjelek-jelekkan\nmenjengek-jengek\nmenjengit-jengit\nmenjerit-jerit\nmenjilat-jilat\nmenjungkat-jungkit\nmenko-menko\nmenlu-menlu\nmenonjol-nonjolkan\nmentah-mentah\nmentang-mentang\nmenteri-menteri\nmentul-mentul\nmenuding-nuding\nmenumpah-numpahkan\nmenunda-nunda\nmenunduk-nunduk\nmenusuk-nusuk\nmenyala-nyala\nmenyama-nyama\nmenyama-nyamai\nmenyambar-nyambar\nmenyangkut-nyangkutkan\nmenyanjung-nyanjung\nmenyanjung-nyanjungkan\nmenyapu-nyapu\nmenyarat-nyarat\nmenyayat-nyayat\nmenyedang-nyedang\nmenyedang-nyedangkan\nmenyelang-nyelangkan\nmenyelang-nyeling\nmenyelang-nyelingkan\nmenyenak-nyenak\nmenyendi-nyendi\nmenyentak-nyentak\nmenyentuh-nyentuh\nmenyepak-nyepakkan\nmenyerak-nyerakkan\nmenyeret-nyeret\nmenyeru-nyerukan\nmenyetel-nyetel\nmenyia-nyiakan\nmenyibak-nyibak\nmenyobek-nyobek\nmenyorong-nyorongkan\nmenyungguh-nyungguhi\nmenyuruk-nyuruk\nmeraba-raba\nmerah-hitam\nmerah-merah\nmerambang-rambang\nmerangkak-rangkak\nmerasa-rasai\nmerata-ratakan\nmeraung-raung\nmeraung-raungkan\nmerayau-rayau\nmerayu-rayu\nmercak-mercik\nmercedes-benz\nmerek-merek\nmereka-mereka\nmereka-reka\nmerelap-relap\nmerem-merem\nmeremah-remah\nmeremas-remas\nmeremeh-temehkan\nmerempah-rempah\nmerempah-rempahi\nmerengek-rengek\nmerengeng-rengeng\nmerenik-renik\nmerenta-renta\nmerenyai-renyai\nmeresek-resek\nmerintang-rintang\nmerintik-rintik\nmerobek-robek\nmeronta-ronta\nmeruap-ruap\nmerubu-rubu\nmerungus-rungus\nmerungut-rungut\nmeta-analysis\nmetode-metode\nmewanti-wanti\nmewarna-warnikan\nmeyakin-yakini\nmid-range\nmid-size\nmiju-miju\nmikro-kecil\nmimpi-mimpi\nminggu-minggu\nminta-minta\nminuman-minuman\nmixed-use\nmobil-mobil\nmobile-first\nmobile-friendly\nmoga-moga\nmola-mola\nmomen-momen\nmondar-mandir\nmonyet-monyet\nmorak-marik\nmorat-marit\nmove-on\nmuda-muda\nmuda-mudi\nmuda/i\nmudah-mudahan\nmuka-muka\nmula-mula\nmultiple-output\nmuluk-muluk\nmulut-mulutan\nmumi-mumi\nmundur-mundur\nmuntah-muntah\nmurid-muridnya\nmusda-musda\nmuseum-museum\nmuslim-muslimah\nmusuh-musuh\nmusuh-musuhnya\nnabi-nabi\nnada-nadanya\nnaga-naga\nnaga-naganya\nnaik-naik\nnaik-turun\nnakal-nakalan\nnama-nama\nnanti-nantian\nnanya-nanya\nnasi-nasi\nnasib-nasiban\nnear-field\nnegara-negara\nnegera-negara\nnegeri-negeri\nnegeri-red\nneka-neka\nnekat-nekat\nneko-neko\nnenek-nenek\nneo-liberalisme\nnext-gen\nnext-generation\nngeang-ngeang\nngeri-ngeri\nnggak-nggak\nngobrol-ngobrol\nngumpul-ngumpul\nnilai-nilai\nnine-dash\nnipa-nipa\nnong-nong\nnorma-norma\nnovel-novel\nnyai-nyai\nnyolong-nyolong\nnyut-nyutan\nob-gyn\nobat-obat\nobat-obatan\nobjek-objek\nobok-obok\nobrak-abrik\nocta-core\nodong-odong\noedipus-kompleks\noff-road\nogah-agih\nogah-ogah\nogah-ogahan\nogak-agik\nogak-ogak\nogoh-ogoh\nolak-alik\nolak-olak\nolang-aling\nolang-alingan\nole-ole\noleh-oleh\nolok-olok\nolok-olokan\nolong-olong\nom-om\nombang-ambing\nomni-channel\non-board\non-demand\non-fire\non-line\non-off\non-premises\non-roll\non-screen\non-the-go\nonde-onde\nondel-ondel\nondos-ondos\none-click\none-to-one\none-touch\none-two\noneng-oneng\nongkang-ongkang\nongol-ongol\nonline-to-offline\nontran-ontran\nonyah-anyih\nonyak-anyik\nopak-apik\nopsi-opsi\nopt-in\norak-arik\norang-aring\norang-orang\norang-orangan\norat-oret\norganisasi-organisasi\normas-ormas\norok-orok\norong-orong\noseng-oseng\notak-atik\notak-otak\notak-otakan\nover-heating\nover-the-air\nover-the-top\npa-pa\npabrik-pabrik\npadi-padian\npagi-pagi\npagi-sore\npajak-pajak\npaket-paket\npalas-palas\npalato-alveolar\npaling-paling\npalu-arit\npalu-memalu\npanas-dingin\npanas-panas\npandai-pandai\npandang-memandang\npanel-panel\npangeran-pangeran\npanggung-panggung\npangkalan-pangkalan\npanja-panja\npanji-panji\npansus-pansus\npantai-pantai\npao-pao\npara-para\nparang-parang\nparpol-parpol\npartai-partai\nparu-paru\npas-pasan\npasal-pasal\npasang-memasang\npasang-surut\npasar-pasar\npasu-pasu\npaus-paus\npaut-memaut\npay-per-click\npaya-paya\npdi-p\npecah-pecah\npecat-pecatan\npeer-to-peer\npejabat-pejabat\npekak-pekak\npekik-pekuk\npelabuhan-pelabuhan\npelacur-pelacur\npelajar-pelajar\npelan-pelan\npelangi-pelangi\npem-bully\npemain-pemain\npemata-mataan\npemda-pemda\npemeluk-pemeluknya\npemerintah-pemerintah\npemerintah-red\npemerintah-swasta\npemetang-metangan\npemilu-pemilu\npemimpin-pemimpin\npeminta-minta\npemuda-pemuda\npemuda-pemudi\npenanggung-jawab\npengali-ali\npengaturan-pengaturan\npenggembar-gemboran\npengorak-arik\npengotak-ngotakan\npengundang-undang\npengusaha-pengusaha\npentung-pentungan\npenyakit-penyakit\nperak-perak\nperang-perangan\nperas-perus\nperaturan-peraturan\nperda-perda\nperempat-final\nperempuan-perempuan\npergi-pergi\npergi-pulang\nperintang-rintang\nperkereta-apian\nperlahan-lahan\nperlip-perlipan\npermen-permen\npernak-pernik\npernik-pernik\npertama-tama\npertandingan-pertandingan\npertimbangan-pertimbangan\nperudang-undangan\nperundang-undangan\nperundangan-undangan\nperusahaan-perusahaan\nperusahaan-perusahan\nperwakilan-perwakilan\npesan-pesan\npesawat-pesawat\npeta-jalan\npetang-petang\npetantang-petenteng\npetatang-peteteng\npete-pete\npiala-piala\npiat-piut\npick-up\npicture-in-picture\npihak-pihak\npijak-pijak\npijar-pijar\npijat-pijat\npikir-pikir\npil-pil\npilah-pilih\npilih-pilih\npilihan-pilihan\npilin-memilin\npilkada-pilkada\npina-pina\npindah-pindah\nping-pong\npinjam-meminjam\npintar-pintarlah\npisang-pisang\npistol-pistolan\npiting-memiting\nplanet-planet\nplay-off\nplin-plan\nplintat-plintut\nplonga-plongo\nplug-in\nplus-minus\nplus-plus\npoco-poco\npohon-pohonan\npoin-poin\npoint-of-sale\npoint-of-sales\npokemon-pokemon\npokja-pokja\npokok-pokok\npokrol-pokrolan\npolang-paling\npolda-polda\npoleng-poleng\npolong-polongan\npolres-polres\npolsek-polsek\npolwan-polwan\npoma-poma\npondok-pondok\nponpes-ponpes\npontang-panting\npop-up\nporak-parik\nporak-peranda\nporak-poranda\npos-pos\nposko-posko\npotong-memotong\npraktek-praktek\npraktik-praktik\nproduk-produk\nprogram-program\npromosi-degradasi\nprovinsi-provinsi\nproyek-proyek\npuing-puing\npuisi-puisi\npuji-pujian\npukang-pukang\npukul-memukul\npulang-pergi\npulau-pulai\npulau-pulau\npull-up\npulut-pulut\npundi-pundi\npungak-pinguk\npunggung-memunggung\npura-pura\npuruk-parak\npusar-pusar\npusat-pusat\npush-to-talk\npush-up\npush-ups\npusing-pusing\npuskesmas-puskesmas\nputar-putar\nputera-puteri\nputih-hitam\nputih-putih\nputra-putra\nputra-putri\nputra/i\nputri-putri\nputus-putus\nputusan-putusan\npuvi-puvi\nquad-core\nraba-rabaan\nraba-rubu\nrada-rada\nradio-frequency\nragu-ragu\nrahasia-rahasiaan\nraja-raja\nrama-rama\nramai-ramai\nramalan-ramalan\nrambeh-rambeh\nrambu-rambu\nrame-rame\nramu-ramuan\nranda-rondo\nrangkul-merangkul\nrango-rango\nrap-rap\nrasa-rasanya\nrata-rata\nraun-raun\nread-only\nreal-life\nreal-time\nrebah-rebah\nrebah-rebahan\nrebas-rebas\nred-eye\nredam-redam\nredep-redup\nrehab-rekon\nreja-reja\nreka-reka\nreka-rekaan\nrekan-rekan\nrekan-rekannya\nrekor-rekor\nrelief-relief\nremah-remah\nremang-remang\nrembah-rembah\nrembah-rembih\nremeh-cemeh\nremeh-temeh\nrempah-rempah\nrencana-rencana\nrenyai-renyai\nrep-repan\nrepot-repot\nrepuh-repuh\nrestoran-restoran\nretak-retak\nriang-riang\nribu-ribu\nribut-ribut\nrica-rica\nride-sharing\nrigi-rigi\nrinai-rinai\nrintik-rintik\nritual-ritual\nrobak-rabik\nrobat-rabit\nrobot-robot\nrole-play\nrole-playing\nroll-on\nrombang-rambing\nromol-romol\nrompang-romping\nrondah-rondih\nropak-rapik\nroyal-royalan\nroyo-royo\nruak-ruak\nruba-ruba\nrudal-rudal\nruji-ruji\nruku-ruku\nrumah-rumah\nrumah-rumahan\nrumbai-rumbai\nrumput-rumputan\nrunding-merunding\nrundu-rundu\nrunggu-rangga\nrunner-up\nruntang-runtung\nrupa-rupa\nrupa-rupanya\nrusun-rusun\nrute-rute\nsaat-saat\nsaban-saban\nsabu-sabu\nsabung-menyabung\nsah-sah\nsahabat-sahabat\nsaham-saham\nsahut-menyahut\nsaing-menyaing\nsaji-sajian\nsakit-sakitan\nsaksi-saksi\nsaku-saku\nsalah-salah\nsama-sama\nsamar-samar\nsambar-menyambar\nsambung-bersambung\nsambung-menyambung\nsambut-menyambut\nsamo-samo\nsampah-sampah\nsampai-sampai\nsamping-menyamping\nsana-sini\nsandar-menyandar\nsandi-sandi\nsangat-sangat\nsangkut-menyangkut\nsapa-menyapa\nsapai-sapai\nsapi-sapi\nsapu-sapu\nsaran-saran\nsarana-prasarana\nsari-sari\nsarit-sarit\nsatu-dua\nsatu-satu\nsatu-satunya\nsatuan-satuan\nsaudara-saudara\nsauk-menyauk\nsauk-sauk\nsayang-sayang\nsayap-sayap\nsayup-menyayup\nsayup-sayup\nsayur-mayur\nsayur-sayuran\nsci-fi\nseagak-agak\nseakal-akal\nseakan-akan\nsealak-alak\nseari-arian\nsebaik-baiknya\nsebelah-menyebelah\nsebentar-sebentar\nseberang-menyeberang\nseberuntung-beruntungnya\nsebesar-besarnya\nseboleh-bolehnya\nsedalam-dalamnya\nsedam-sedam\nsedang-menyedang\nsedang-sedang\nsedap-sedapan\nsedapat-dapatnya\nsedikit-dikitnya\nsedikit-sedikit\nsedikit-sedikitnya\nsedini-dininya\nseelok-eloknya\nsegala-galanya\nsegan-menyegan\nsegan-menyegani\nsegan-segan\nsehabis-habisnya\nsehari-hari\nsehari-harian\nsehari-harinya\nsejadi-jadinya\nsekali-kali\nsekali-sekali\nsekenyang-kenyangnya\nsekira-kira\nsekolah-sekolah\nsekonyong-konyong\nsekosong-kosongnya\nsektor-sektor\nsekuasa-kuasanya\nsekuat-kuatnya\nsekurang-kurangnya\nsel-sel\nsela-menyela\nsela-sela\nselak-seluk\nselama-lamanya\nselambat-lambatnya\nselang-seli\nselang-seling\nselar-belar\nselat-latnya\nselatan-tenggara\nselekas-lekasnya\nselentang-selenting\nselepas-lepas\nself-driving\nself-esteem\nself-healing\nself-help\nselir-menyelir\nseloyong-seloyong\nseluk-beluk\nseluk-semeluk\nsema-sema\nsemah-semah\nsemak-semak\nsemaksimal-maksimalnya\nsemalam-malaman\nsemang-semang\nsemanis-manisnya\nsemasa-masa\nsemata-mata\nsemau-maunya\nsembunyi-sembunyi\nsembunyi-sembunyian\nsembur-sembur\nsemena-mena\nsemenda-menyemenda\nsemengga-mengga\nsemenggah-menggah\nsementang-mentang\nsemerdeka-merdekanya\nsemi-final\nsemi-permanen\nsempat-sempatnya\nsemu-semu\nsemua-muanya\nsemujur-mujurnya\nsemut-semutan\nsen-senan\nsendiri-sendiri\nsengal-sengal\nsengar-sengir\nsengau-sengauan\nsenggak-sengguk\nsenggang-tenggang\nsenggol-menyenggol\nsenior-junior\nsenjata-senjata\nsenyum-senyum\nseolah-olah\nsepala-pala\nsepandai-pandai\nsepetang-petangan\nsepoi-sepoi\nsepraktis-praktisnya\nsepuas-puasnya\nserak-serak\nserak-serik\nserang-menyerang\nserang-serangan\nserangan-serangan\nseraya-menyeraya\nserba-serbi\nserbah-serbih\nserembah-serembih\nserigala-serigala\nsering-sering\nserobot-serobotan\nserong-menyerong\nserta-menyertai\nserta-merta\nserta-serta\nseru-seruan\nservice-oriented\nsesak-menyesak\nsesal-menyesali\nsesayup-sayup\nsesi-sesi\nsesuang-suang\nsesudah-sudah\nsesudah-sudahnya\nsesuka-suka\nsesuka-sukanya\nset-piece\nsetempat-setempat\nsetengah-setengah\nsetidak-tidaknya\nsetinggi-tingginya\nseupaya-upaya\nseupaya-upayanya\nsewa-menyewa\nsewaktu-waktu\nsewenang-wenang\nsewot-sewotan\nshabu-shabu\nshort-term\nshort-throw\nsia-sia\nsiang-siang\nsiap-siap\nsiapa-siapa\nsibar-sibar\nsibur-sibur\nsida-sida\nside-by-side\nsign-in\nsiku-siku\nsikut-sikutan\nsilah-silah\nsilang-menyilang\nsilir-semilir\nsimbol-simbol\nsimpan-pinjam\nsinar-menyinar\nsinar-seminar\nsinar-suminar\nsindir-menyindir\nsinga-singa\nsinggah-menyinggah\nsingle-core\nsipil-militer\nsir-siran\nsirat-sirat\nsisa-sisa\nsisi-sisi\nsiswa-siswa\nsiswa-siswi\nsiswa/i\nsiswi-siswi\nsitu-situ\nsitus-situs\nsix-core\nsix-speed\nslintat-slintut\nslo-mo\nslow-motion\nsnap-on\nsobek-sobekan\nsodok-sodokan\nsok-sokan\nsolek-menyolek\nsolid-state\nsorak-sorai\nsorak-sorak\nsore-sore\nsosio-ekonomi\nsoya-soya\nspill-resistant\nsplit-screen\nsponsor-sponsor\nsponsor-sponsoran\nsrikandi-srikandi\nstaf-staf\nstand-by\nstand-up\nstart-up\nstasiun-stasiun\nstate-owned\nstriker-striker\nstudi-studi\nsuam-suam\nsuami-isteri\nsuami-istri\nsuami-suami\nsuang-suang\nsuara-suara\nsudin-sudin\nsudu-sudu\nsudung-sudung\nsugi-sugi\nsuka-suka\nsuku-suku\nsulang-menyulang\nsulat-sulit\nsulur-suluran\nsum-sum\nsumber-sumber\nsumpah-sumpah\nsumpit-sumpit\nsundut-bersundut\nsungai-sungai\nsungguh-sungguh\nsungut-sungut\nsunting-menyunting\nsuper-damai\nsuper-rahasia\nsuper-sub\nsupply-demand\nsupply-side\nsuram-suram\nsurat-menyurat\nsurat-surat\nsuruh-suruhan\nsuruk-surukan\nsusul-menyusul\nsuwir-suwir\nsyarat-syarat\nsystem-on-chip\nt-shirt\nt-shirts\ntabar-tabar\ntabir-mabir\ntabrak-tubruk\ntabuh-tabuhan\ntabun-menabun\ntahu-menahu\ntahu-tahu\ntahun-tahun\ntakah-takahnya\ntakang-takik\ntake-off\ntakut-takut\ntakut-takutan\ntali-bertali\ntali-tali\ntalun-temalun\ntaman-taman\ntampak-tampak\ntanak-tanakan\ntanam-menanam\ntanam-tanaman\ntanda-tanda\ntangan-menangan\ntangan-tangan\ntangga-tangga\ntanggal-tanggal\ntanggul-tanggul\ntanggung-menanggung\ntanggung-tanggung\ntank-tank\ntante-tante\ntanya-jawab\ntapa-tapa\ntapak-tapak\ntari-menari\ntari-tarian\ntarik-menarik\ntarik-ulur\ntata-tertib\ntatah-tatah\ntau-tau\ntawa-tawa\ntawak-tawak\ntawang-tawang\ntawar-menawar\ntawar-tawar\ntayum-temayum\ntebak-tebakan\ntebu-tebu\ntedong-tedong\ntegak-tegak\ntegerbang-gerbang\nteh-tehan\ntek-tek\nteka-teki\nteknik-teknik\nteman-teman\nteman-temanku\ntemas-temas\ntembak-menembak\ntemeh-temeh\ntempa-menempa\ntempat-tempat\ntempo-tempo\ntemut-temut\ntenang-tenang\ntengah-tengah\ntenggang-menenggang\ntengok-menengok\nteori-teori\nteraba-raba\nteralang-alang\nterambang-ambang\nterambung-ambung\nterang-terang\nterang-terangan\nteranggar-anggar\nterangguk-angguk\nteranggul-anggul\nterangin-angin\nterangkup-angkup\nteranja-anja\nterapung-apung\nterayan-rayan\nterayap-rayap\nterbada-bada\nterbahak-bahak\nterbang-terbang\nterbata-bata\nterbatuk-batuk\nterbayang-bayang\nterbeda-bedakan\nterbengkil-bengkil\nterbengong-bengong\nterbirit-birit\nterbuai-buai\nterbuang-buang\nterbungkuk-bungkuk\nterburu-buru\ntercangak-cangak\ntercengang-cengang\ntercilap-cilap\ntercongget-congget\ntercoreng-moreng\ntercungap-cungap\nterdangka-dangka\nterdengih-dengih\nterduga-duga\nterekeh-ekeh\nterembut-embut\nterembut-rembut\nterempas-empas\nterengah-engah\nteresak-esak\ntergagap-gagap\ntergagau-gagau\ntergaguk-gaguk\ntergapai-gapai\ntergegap-gegap\ntergegas-gegas\ntergelak-gelak\ntergelang-gelang\ntergeleng-geleng\ntergelung-gelung\ntergerai-gerai\ntergerenyeng-gerenyeng\ntergesa-gesa\ntergila-gila\ntergolek-golek\ntergontai-gontai\ntergudik-gudik\ntergugu-gugu\nterguling-guling\ntergulut-gulut\nterhambat-hambat\nterharak-harak\nterharap-harap\nterhengit-hengit\nterheran-heran\nterhinggut-hinggut\nterigau-igau\nterimpi-impi\nterincut-incut\nteringa-inga\nteringat-ingat\nterinjak-injak\nterisak-isak\nterjembak-jembak\nterjerit-jerit\nterkadang-kadang\nterkagum-kagum\nterkaing-kaing\nterkakah-kakah\nterkakak-kakak\nterkampul-kampul\nterkanjar-kanjar\nterkantuk-kantuk\nterkapah-kapah\nterkapai-kapai\nterkapung-kapung\nterkatah-katah\nterkatung-katung\nterkecap-kecap\nterkedek-kedek\nterkedip-kedip\nterkejar-kejar\nterkekau-kekau\nterkekeh-kekeh\nterkekek-kekek\nterkelinjat-kelinjat\nterkelip-kelip\nterkempul-kempul\nterkemut-kemut\nterkencar-kencar\nterkencing-kencing\nterkentut-kentut\nterkepak-kepak\nterkesot-kesot\nterkesut-kesut\nterkial-kial\nterkijai-kijai\nterkikih-kikih\nterkikik-kikik\nterkincak-kincak\nterkindap-kindap\nterkinja-kinja\nterkirai-kirai\nterkitar-kitar\nterkocoh-kocoh\nterkojol-kojol\nterkokol-kokol\nterkosel-kosel\nterkotak-kotak\nterkoteng-koteng\nterkuai-kuai\nterkumpal-kumpal\nterlara-lara\nterlayang-layang\nterlebih-lebih\nterlincah-lincah\nterliuk-liuk\nterlolong-lolong\nterlongong-longong\nterlunta-lunta\ntermangu-mangu\ntermanja-manja\ntermata-mata\ntermengah-mengah\ntermenung-menung\ntermimpi-mimpi\ntermonyong-monyong\nternanti-nanti\nterngiang-ngiang\nteroleng-oleng\nterombang-ambing\nterpalit-palit\nterpandang-pandang\nterpecah-pecah\nterpekik-pekik\nterpencar-pencar\nterpereh-pereh\nterpijak-pijak\nterpikau-pikau\nterpilah-pilah\nterpinga-pinga\nterpingkal-pingkal\nterpingkau-pingkau\nterpontang-panting\nterpusing-pusing\nterputus-putus\ntersanga-sanga\ntersaruk-saruk\ntersedan-sedan\ntersedih-sedih\ntersedu-sedu\nterseduh-seduh\ntersendat-sendat\ntersendeng-sendeng\ntersengal-sengal\ntersengguk-sengguk\ntersengut-sengut\nterseok-seok\ntersera-sera\nterserak-serak\ntersetai-setai\ntersia-sia\ntersipu-sipu\ntersoja-soja\ntersungkuk-sungkuk\ntersuruk-suruk\ntertagak-tagak\ntertahan-tahan\ntertatih-tatih\ntertegun-tegun\ntertekan-tekan\nterteleng-teleng\ntertendang-tendang\ntertimpang-timpang\ntertitar-titar\nterumbang-ambing\nterumbang-umbang\nterungkap-ungkap\nterus-menerus\nterus-terusan\ntete-a-tete\ntext-to-speech\nthink-tank\nthink-thank\nthird-party\nthird-person\nthree-axis\nthree-point\ntiap-tiap\ntiba-tiba\ntidak-tidak\ntidur-tidur\ntidur-tiduran\ntie-dye\ntie-in\ntiga-tiganya\ntikam-menikam\ntiki-taka\ntikus-tikus\ntilik-menilik\ntim-tim\ntimah-timah\ntimang-timangan\ntimbang-menimbang\ntime-lapse\ntimpa-menimpa\ntimu-timu\ntimun-timunan\ntimur-barat\ntimur-laut\ntimur-tenggara\ntindih-bertindih\ntindih-menindih\ntinjau-meninjau\ntinju-meninju\ntip-off\ntipu-tipu\ntiru-tiruan\ntitik-titik\ntitik-titiknya\ntiup-tiup\nto-do\ntokak-takik\ntoko-toko\ntokoh-tokoh\ntokok-menokok\ntolak-menolak\ntolong-menolong\ntong-tong\ntop-level\ntop-up\ntotol-totol\ntouch-screen\ntrade-in\ntraining-camp\ntrans-nasional\ntreble-winner\ntri-band\ntrik-trik\ntriple-core\ntruk-truk\ntua-tua\ntuan-tuan\ntuang-tuang\ntuban-tuban\ntubuh-tubuh\ntujuan-tujuan\ntuk-tuk\ntukang-menukang\ntukar-menukar\ntulang-belulang\ntulang-tulangan\ntuli-tuli\ntulis-menulis\ntumbuh-tumbuhan\ntumpang-tindih\ntune-up\ntunggang-tunggik\ntunggang-tungging\ntunggang-tunggit\ntunggul-tunggul\ntunjuk-menunjuk\ntupai-tupai\ntupai-tupaian\nturi-turian\nturn-based\nturnamen-turnamen\nturun-temurun\nturut-menurut\nturut-turutan\ntuyuk-tuyuk\ntwin-cam\ntwin-turbocharged\ntwo-state\ntwo-step\ntwo-tone\nu-shape\nuang-uangan\nuar-uar\nubek-ubekan\nubel-ubel\nubrak-abrik\nubun-ubun\nubur-ubur\nuci-uci\nudang-undang\nudap-udapan\nugal-ugalan\nuget-uget\nuir-uir\nujar-ujar\nuji-coba\nujung-ujung\nujung-ujungnya\nuka-uka\nukir-mengukir\nukir-ukiran\nula-ula\nulak-ulak\nulam-ulam\nulang-alik\nulang-aling\nulang-ulang\nulap-ulap\nular-ular\nular-ularan\nulek-ulek\nulu-ulu\nulung-ulung\numang-umang\numbang-ambing\numbi-umbian\numbul-umbul\numbut-umbut\nuncang-uncit\nundak-undakan\nundang-undang\nundang-undangnya\nunduk-unduk\nundung-undung\nundur-undur\nunek-unek\nungah-angih\nunggang-anggit\nunggat-unggit\nunggul-mengungguli\nungkit-ungkit\nunit-unit\nuniversitas-universitas\nunsur-unsur\nuntang-anting\nunting-unting\nuntung-untung\nuntung-untungan\nupah-mengupah\nupih-upih\nupside-down\nura-ura\nuran-uran\nurat-urat\nuring-uringan\nurup-urup\nurup-urupan\nurus-urus\nusaha-usaha\nuser-user\nuser-useran\nutak-atik\nutang-piutang\nutang-utang\nutar-utar\nutara-jauh\nutara-selatan\nuter-uter\nutusan-utusan\nv-belt\nv-neck\nvalue-added\nvery-very\nvideo-video\nvisi-misi\nvisi-misinya\nvoa-islam\nvoice-over\nvolt-ampere\nwajah-wajah\nwajar-wajar\nwake-up\nwakil-wakil\nwalk-in\nwalk-out\nwangi-wangian\nwanita-wanita\nwanti-wanti\nwara-wara\nwara-wiri\nwarna-warna\nwarna-warni\nwas-was\nwater-cooled\nweb-based\nwide-angle\nwilayah-wilayah\nwin-win\nwira-wiri\nwora-wari\nwork-life\nworld-class\nyang-yang\nyayasan-yayasan\nyear-on-year\nyel-yel\nyo-yo\nzam-zam\nzig-zag\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/id/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.id.tokenizer_exceptions.orth_title->'-'.join([part.title() for part in orth.split('-')])
A:spacy.lang.id.tokenizer_exceptions.orth_caps->'-'.join([part.upper() for part in orth.split('-')])
A:spacy.lang.id.tokenizer_exceptions.orth_lower->orth.lower()
A:spacy.lang.id.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/id/syntax_iterators.py----------------------------------------
A:spacy.lang.id.syntax_iterators.conj->doc.vocab.strings.add('conj')
A:spacy.lang.id.syntax_iterators.np_label->doc.vocab.strings.add('NP')
spacy.lang.id.syntax_iterators.noun_chunks(doclike:Union[Doc,Span])->Iterator[Tuple[int, int, int]]


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/id/stop_words.py----------------------------------------
A:spacy.lang.id.stop_words.STOP_WORDS->set('\nada adalah adanya adapun agak agaknya agar akan akankah akhir akhiri akhirnya\naku akulah amat amatlah anda andalah antar antara antaranya apa apaan apabila\napakah apalagi apatah artinya asal asalkan atas atau ataukah ataupun awal\nawalnya\n\nbagai bagaikan bagaimana bagaimanakah bagaimanapun bagi bagian bahkan bahwa\nbahwasanya baik bakal bakalan balik banyak bapak baru bawah beberapa begini\nbeginian beginikah beginilah begitu begitukah begitulah begitupun bekerja\nbelakang belakangan belum belumlah benar benarkah benarlah berada berakhir\nberakhirlah berakhirnya berapa berapakah berapalah berapapun berarti berawal\nberbagai berdatangan beri berikan berikut berikutnya berjumlah berkali-kali\nberkata berkehendak berkeinginan berkenaan berlainan berlalu berlangsung\nberlebihan bermacam bermacam-macam bermaksud bermula bersama bersama-sama\nbersiap bersiap-siap bertanya bertanya-tanya berturut berturut-turut bertutur\nberujar berupa besar betul betulkah biasa biasanya bila bilakah bisa bisakah\nboleh bolehkah bolehlah buat bukan bukankah bukanlah bukannya bulan bung\n\ncara caranya cukup cukupkah cukuplah cuma\n\ndahulu dalam dan dapat dari daripada datang dekat demi demikian demikianlah\ndengan depan di dia diakhiri diakhirinya dialah diantara diantaranya diberi\ndiberikan diberikannya dibuat dibuatnya didapat didatangkan digunakan\ndiibaratkan diibaratkannya diingat diingatkan diinginkan dijawab dijelaskan\ndijelaskannya dikarenakan dikatakan dikatakannya dikerjakan diketahui\ndiketahuinya dikira dilakukan dilalui dilihat dimaksud dimaksudkan\ndimaksudkannya dimaksudnya diminta dimintai dimisalkan dimulai dimulailah\ndimulainya dimungkinkan dini dipastikan diperbuat diperbuatnya dipergunakan\ndiperkirakan diperlihatkan diperlukan diperlukannya dipersoalkan dipertanyakan\ndipunyai diri dirinya disampaikan disebut disebutkan disebutkannya disini\ndisinilah ditambahkan ditandaskan ditanya ditanyai ditanyakan ditegaskan\nditujukan ditunjuk ditunjuki ditunjukkan ditunjukkannya ditunjuknya dituturkan\ndituturkannya diucapkan diucapkannya diungkapkan dong dua dulu\n\nempat enggak enggaknya entah entahlah\n\nguna gunakan\n\nhal hampir hanya hanyalah hari harus haruslah harusnya hendak hendaklah\nhendaknya hingga\n\nia ialah ibarat ibaratkan ibaratnya ibu ikut ingat ingat-ingat ingin inginkah\ninginkan ini inikah inilah itu itukah itulah\n\njadi jadilah jadinya jangan jangankan janganlah jauh jawab jawaban jawabnya\njelas jelaskan jelaslah jelasnya jika jikalau juga jumlah jumlahnya justru\n\nkala kalau kalaulah kalaupun kalian kami kamilah kamu kamulah kan kapan\nkapankah kapanpun karena karenanya kasus kata katakan katakanlah katanya ke\nkeadaan kebetulan kecil kedua keduanya keinginan kelamaan kelihatan\nkelihatannya kelima keluar kembali kemudian kemungkinan kemungkinannya kenapa\nkepada kepadanya kesampaian keseluruhan keseluruhannya keterlaluan ketika\nkhususnya kini kinilah kira kira-kira kiranya kita kitalah kok kurang\n\nlagi lagian lah lain lainnya lalu lama lamanya lanjut lanjutnya lebih lewat\nlima luar\n\nmacam maka makanya makin malah malahan mampu mampukah mana manakala manalagi\nmasa masalah masalahnya masih masihkah masing masing-masing mau maupun\nmelainkan melakukan melalui melihat melihatnya memang memastikan memberi\nmemberikan membuat memerlukan memihak meminta memintakan memisalkan memperbuat\nmempergunakan memperkirakan memperlihatkan mempersiapkan mempersoalkan\nmempertanyakan mempunyai memulai memungkinkan menaiki menambahkan menandaskan\nmenanti menanti-nanti menantikan menanya menanyai menanyakan mendapat\nmendapatkan mendatang mendatangi mendatangkan menegaskan mengakhiri mengapa\nmengatakan mengatakannya mengenai mengerjakan mengetahui menggunakan\nmenghendaki mengibaratkan mengibaratkannya mengingat mengingatkan menginginkan\nmengira mengucapkan mengucapkannya mengungkapkan menjadi menjawab menjelaskan\nmenuju menunjuk menunjuki menunjukkan menunjuknya menurut menuturkan\nmenyampaikan menyangkut menyatakan menyebutkan menyeluruh menyiapkan merasa\nmereka merekalah merupakan meski meskipun meyakini meyakinkan minta mirip\nmisal misalkan misalnya mula mulai mulailah mulanya mungkin mungkinkah\n\nnah naik namun nanti nantinya nyaris nyatanya\n\noleh olehnya\n\npada padahal padanya pak paling panjang pantas para pasti pastilah penting\npentingnya per percuma perlu perlukah perlunya pernah persoalan pertama\npertama-tama pertanyaan pertanyakan pihak pihaknya pukul pula pun punya\n\nrasa rasanya rata rupanya\n\nsaat saatnya saja sajalah saling sama sama-sama sambil sampai sampai-sampai\nsampaikan sana sangat sangatlah satu saya sayalah se sebab sebabnya sebagai\nsebagaimana sebagainya sebagian sebaik sebaik-baiknya sebaiknya sebaliknya\nsebanyak sebegini sebegitu sebelum sebelumnya sebenarnya seberapa sebesar\nsebetulnya sebisanya sebuah sebut sebutlah sebutnya secara secukupnya sedang\nsedangkan sedemikian sedikit sedikitnya seenaknya segala segalanya segera\nseharusnya sehingga seingat sejak sejauh sejenak sejumlah sekadar sekadarnya\nsekali sekali-kali sekalian sekaligus sekalipun sekarang sekarang sekecil\nseketika sekiranya sekitar sekitarnya sekurang-kurangnya sekurangnya sela\nselain selaku selalu selama selama-lamanya selamanya selanjutnya seluruh\nseluruhnya semacam semakin semampu semampunya semasa semasih semata semata-mata\nsemaunya sementara semisal semisalnya sempat semua semuanya semula sendiri\nsendirian sendirinya seolah seolah-olah seorang sepanjang sepantasnya\nsepantasnyalah seperlunya seperti sepertinya sepihak sering seringnya serta\nserupa sesaat sesama sesampai sesegera sesekali seseorang sesuatu sesuatunya\nsesudah sesudahnya setelah setempat setengah seterusnya setiap setiba setibanya\nsetidak-tidaknya setidaknya setinggi seusai sewaktu siap siapa siapakah\nsiapapun sini sinilah soal soalnya suatu sudah sudahkah sudahlah supaya\n\ntadi tadinya tahu tahun tak tambah tambahnya tampak tampaknya tandas tandasnya\ntanpa tanya tanyakan tanyanya tapi tegas tegasnya telah tempat tengah tentang\ntentu tentulah tentunya tepat terakhir terasa terbanyak terdahulu terdapat\nterdiri terhadap terhadapnya teringat teringat-ingat terjadi terjadilah\nterjadinya terkira terlalu terlebih terlihat termasuk ternyata tersampaikan\ntersebut tersebutlah tertentu tertuju terus terutama tetap tetapi tiap tiba\ntiba-tiba tidak tidakkah tidaklah tiga tinggi toh tunjuk turut tutur tuturnya\n\nucap ucapnya ujar ujarnya umum umumnya ungkap ungkapnya untuk usah usai\n\nwaduh wah wahai waktu waktunya walau walaupun wong\n\nyaitu yakin yakni yang\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/id/punctuation.py----------------------------------------
A:spacy.lang.id.punctuation.UNITS->merge_chars(_units)
A:spacy.lang.id.punctuation.CURRENCY->merge_chars(_currency)
A:spacy.lang.id.punctuation.MONTHS->merge_chars(_months)
A:spacy.lang.id.punctuation.LIST_CURRENCY->split_chars(_currency)
A:spacy.lang.id.punctuation._prefixes->list(TOKENIZER_PREFIXES)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/sr/__init__.py----------------------------------------
spacy.lang.sr.__init__.Serbian(Language)
spacy.lang.sr.__init__.SerbianDefaults(BaseDefaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/sr/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/sr/lex_attrs.py----------------------------------------
A:spacy.lang.sr.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.sr.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.sr.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/sr/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.sr.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/sr/stop_words.py----------------------------------------
A:spacy.lang.sr.stop_words.STOP_WORDS->set('\nа\nавај\nако\nал\nали\nарх\nау\nах\nаха\nај\nбар\nби\nбила\nбили\nбило\nбисмо\nбисте\nбих\nбијасмо\nбијасте\nбијах\nбијаху\nбијаше\nбиће\nблизу\nброј\nбрр\nбуде\nбудимо\nбудите\nбуду\nбудући\nбум\nбућ\nвам\nвама\nвас\nваша\nваше\nвашим\nвашима\nваљда\nвеома\nвероватно\nвећ\nвећина\nви\nвидео\nвише\nврло\nврх\nга\nгде\nгиц\nгод\nгоре\nгђекоје\nда\nдакле\nдана\nданас\nдај\nдва\nде\nдедер\nделимице\nделимично\nдем\nдо\nдобар\nдобити\nдовечер\nдокле\nдоле\nдонекле\nдосад\nдоскоро\nдотад\nдотле\nдошао\nдоћи\nдругамо\nдругде\nдруги\nе\nево\nено\nето\nех\nехе\nеј\nжелела\nжелеле\nжелели\nжелело\nжелех\nжелећи\nжели\nза\nзаиста\nзар\nзатим\nзато\nзахвалити\nзашто\nзбиља\nзимус\nзнати\nзум\nи\nиде\nиз\nизван\nизволи\nизмеђу\nизнад\nикада\nикакав\nикаква\nикакве\nикакви\nикаквим\nикаквима\nикаквих\nикакво\nикаквог\nикаквога\nикаквом\nикаквоме\nикаквој\nили\nим\nима\nимам\nимао\nиспод\nих\nију\nићи\nкад\nкада\nкога\nкојекакав\nкојима\nкоју\nкришом\nлани\nли\nмали\nмањи\nме\nмене\nмени\nми\nмимо\nмисли\nмного\nмогу\nмора\nморао\nмој\nмоја\nмоје\nмоји\nмоју\nмоћи\nму\nна\nнад\nнакон\nнам\nнама\nнас\nнаша\nнаше\nнашег\nнаши\nнаћи\nне\nнегде\nнека\nнекад\nнеке\nнеког\nнеку\nнема\nнемам\nнеко\nнеће\nнећемо\nнећете\nнећеш\nнећу\nни\nникада\nникога\nникоје\nникоји\nникоју\nнисам\nниси\nнисте\nнису\nништа\nниједан\nно\nо\nова\nовако\nовамо\nовај\nовде\nове\nовим\nовима\nово\nовој\nод\nодмах\nоко\nоколо\nон\nонај\nоне\nоним\nонима\nоном\nоној\nону\nосим\nостали\nотишао\nпа\nпак\nпитати\nпо\nповодом\nпод\nподаље\nпожељан\nпожељна\nпоиздаље\nпоименце\nпонекад\nпопреко\nпоред\nпосле\nпотаман\nпотрбушке\nпоуздано\nпочетак\nпоједини\nправити\nпрви\nпреко\nпрема\nприје\nпут\nпљус\nрадије\nс\nса\nсав\nсада\nсам\nсамо\nсасвим\nсва\nсваки\nсви\nсвим\nсвог\nсвом\nсвој\nсвоја\nсвоје\nсвоју\nсву\nсвугде\nсе\nсебе\nсеби\nси\nсмети\nсмо\nствар\nстварно\nсте\nсу\nсутра\nта\nтаèно\nтако\nтакође\nтамо\nтвој\nтвоја\nтвоје\nтвоји\nтвоју\nте\nтебе\nтеби\nти\nтима\nто\nтоме\nтој\nту\nу\nувек\nувијек\nуз\nуза\nузалуд\nуздуж\nузети\nумало\nунутра\nупотребити\nупркос\nучинио\nучинити\nхало\nхвала\nхеј\nхм\nхоп\nхоће\nхоћемо\nхоћете\nхоћеш\nхоћу\nхтедосте\nхтедох\nхтедоше\nхтела\nхтеле\nхтели\nхтео\nхтејасмо\nхтејасте\nхтејаху\nхура\nчесто\nчијем\nчији\nчијим\nчијима\nшиц\nштагод\nшто\nштогод\nја\nје\nједан\nједини\nједна\nједне\nједни\nједно\nједном\nјер\nјесам\nјеси\nјесмо\nјесу\nјим\nјој\nју\nјуче\nњегова\nњегово\nњезин\nњезина\nњезино\nњему\nњен\nњим\nњима\nњихова\nњихово\nњој\nњу\nће\nћемо\nћете\nћеш\nћу\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/sr/punctuation.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/hy/__init__.py----------------------------------------
spacy.lang.hy.__init__.Armenian(Language)
spacy.lang.hy.__init__.ArmenianDefaults(BaseDefaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/hy/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/hy/lex_attrs.py----------------------------------------
A:spacy.lang.hy.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.hy.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.hy.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/hy/stop_words.py----------------------------------------
A:spacy.lang.hy.stop_words.STOP_WORDS->set('\nնա\nողջը\nայստեղ\nենք\nնա\nէիր\nորպես\nուրիշ\nբոլորը\nայն\nայլ\nնույնչափ\nէի\nմի\nև\nողջ\nես\nոմն\nհետ\nնրանք\nամենքը\nըստ\nինչ-ինչ\nայսպես\nհամայն\nմի\nնաև\nնույնքան\nդա\nովևէ\nհամար\nայնտեղ\nէին\nորոնք\nսույն\nինչ-որ\nամենը\nնույնպիսի\nու\nիր\nորոշ\nմիևնույն\nի\nայնպիսի\nմենք\nամեն ոք\nնույն\nերբևէ\nայն\nորևէ\nին\nայդպես\nնրա\nորը\nվրա\nդու\nէինք\nայդպիսի\nէիք\nյուրաքանչյուրը\nեմ\nպիտի\nայդ\nամբողջը\nհետո\nեք\nամեն\nայլ\nկամ\nայսքան\nոր\nայնպես\nայսինչ\nբոլոր\nէ\nմեկնումեկը\nայդչափ\nայնքան\nամբողջ\nերբևիցե\nայնչափ\nամենայն\nմյուս\nայնինչ\nիսկ\nայդտեղ\nայս\nսա\nեն\nամեն ինչ\nորևիցե\nում\nմեկը\nայդ\nդուք\nայսչափ\nայդքան\nայսպիսի\nէր\nյուրաքանչյուր\nայս\nմեջ\nթ\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/fo/__init__.py----------------------------------------
spacy.lang.fo.__init__.Faroese(Language)
spacy.lang.fo.__init__.FaroeseDefaults(BaseDefaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/fo/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.fo.tokenizer_exceptions.capitalized->orth.capitalize()
A:spacy.lang.fo.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/la/__init__.py----------------------------------------
spacy.lang.la.__init__.Latin(Language)
spacy.lang.la.__init__.LatinDefaults(BaseDefaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/la/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/la/lex_attrs.py----------------------------------------
A:spacy.lang.la.lex_attrs.roman_numerals_compile->re.compile('(?i)^(?=[MDCLXVI])M*(C[MD]|D?C{0,4})(X[CL]|L?X{0,4})(I[XV]|V?I{0,4})$')
A:spacy.lang.la.lex_attrs._num_words->set(_num_words)
A:spacy.lang.la.lex_attrs._ordinal_words->set(_ordinal_words)
spacy.lang.la.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/la/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.la.tokenizer_exceptions._abbrev_exc->"A. A.D. Aa. Aaa. Acc. Agr. Ap. Apr. April. A.U.C. Aug. C. Caes. Caess. Cc. Cn. Coll. Cons. Conss. Cos. Coss. D. D.N. Dat. Dd. Dec. Decemb. Decembr. F. Feb. Febr. Februar. Ian. Id. Imp. Impp. Imppp. Iul. Iun. K. Kal. L. M'. M. Mai. Mam. Mar. Mart. Med. N. Nn. Nob. Non. Nov. Novemb. Oct. Octob. Opet. Ord. P. Paul. Pf. Pl. Plur. Post. Pp. Prid. Pro. Procos. Q. Quint. S. S.C. Scr. Sept. Septemb. Ser. Sert. Sex. Sext. St. Sta. Suff. T. Ti. Trib. V. Vol. Vop. Vv.".split()
A:spacy.lang.la.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/la/syntax_iterators.py----------------------------------------
A:spacy.lang.la.syntax_iterators.right->get_right_bound(doc, tok)
A:spacy.lang.la.syntax_iterators.np_label->doc.vocab.strings.add('NP')
A:spacy.lang.la.syntax_iterators.(left, right)->get_bounds(doc, token)
spacy.lang.la.syntax_iterators.noun_chunks(doclike:Union[Doc,Span])->Iterator[Tuple[int, int, int]]


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/la/stop_words.py----------------------------------------
A:spacy.lang.la.stop_words.STOP_WORDS->set('\nab ac ad adhuc aliqui aliquis an ante apud at atque aut autem \n\ncum cur \n\nde deinde dum \n\nego enim ergo es est et etiam etsi ex \n\nfio \n\nhaud hic \n\niam idem igitur ille in infra inter interim ipse is ita \n\nmagis modo mox \n\nnam ne nec necque neque nisi non nos \n\no ob \n\nper possum post pro \n\nquae quam quare qui quia quicumque quidem quilibet quis quisnam quisquam quisque quisquis quo quoniam \n\nsed si sic sive sub sui sum super suus \n\ntam tamen trans tu tum \n\nubi uel uero\n\nvel vero\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/de/__init__.py----------------------------------------
spacy.lang.de.__init__.German(Language)
spacy.lang.de.__init__.GermanDefaults(BaseDefaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/de/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/de/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.de.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/de/syntax_iterators.py----------------------------------------
A:spacy.lang.de.syntax_iterators.np_label->doc.vocab.strings.add('NP')
A:spacy.lang.de.syntax_iterators.np_deps->set((doc.vocab.strings.add(label) for label in labels))
A:spacy.lang.de.syntax_iterators.close_app->doc.vocab.strings.add('nk')
spacy.lang.de.syntax_iterators.noun_chunks(doclike:Union[Doc,Span])->Iterator[Tuple[int, int, int]]


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/de/stop_words.py----------------------------------------
A:spacy.lang.de.stop_words.STOP_WORDS->set('\ná a ab aber ach acht achte achten achter achtes ag alle allein allem allen\naller allerdings alles allgemeinen als also am an andere anderen anderem andern\nanders auch auf aus ausser außer ausserdem außerdem\n\nbald bei beide beiden beim beispiel bekannt bereits besonders besser besten bin\nbis bisher bist\n\nda dabei dadurch dafür dagegen daher dahin dahinter damals damit danach daneben\ndank dann daran darauf daraus darf darfst darin darüber darum darunter das\ndasein daselbst dass daß dasselbe davon davor dazu dazwischen dein deine deinem\ndeiner dem dementsprechend demgegenüber demgemäss demgemäß demselben demzufolge\nden denen denn denselben der deren derjenige derjenigen dermassen dermaßen\nderselbe derselben des deshalb desselben dessen deswegen dich die diejenige\ndiejenigen dies diese dieselbe dieselben diesem diesen dieser dieses dir doch\ndort drei drin dritte dritten dritter drittes du durch durchaus dürfen dürft\ndurfte durften\n\neben ebenso ehrlich eigen eigene eigenen eigener eigenes ein einander eine\neinem einen einer eines einige einigen einiger einiges einmal einmaleins elf en\nende endlich entweder er erst erste ersten erster erstes es etwa etwas euch\n\nfrüher fünf fünfte fünften fünfter fünftes für\n\ngab ganz ganze ganzen ganzer ganzes gar gedurft gegen gegenüber gehabt gehen\ngeht gekannt gekonnt gemacht gemocht gemusst genug gerade gern gesagt geschweige\ngewesen gewollt geworden gibt ging gleich gross groß grosse große grossen\ngroßen grosser großer grosses großes gut gute guter gutes\n\nhabe haben habt hast hat hatte hätte hatten hätten heisst heißt her heute hier\nhin hinter hoch\n\nich ihm ihn ihnen ihr ihre ihrem ihren ihrer ihres im immer in indem\ninfolgedessen ins irgend ist\n\nja jahr jahre jahren je jede jedem jeden jeder jedermann jedermanns jedoch\njemand jemandem jemanden jene jenem jenen jener jenes jetzt\n\nkam kann kannst kaum kein keine keinem keinen keiner kleine kleinen kleiner\nkleines kommen kommt können könnt konnte könnte konnten kurz\n\nlang lange leicht leider lieber los\n\nmachen macht machte mag magst man manche manchem manchen mancher manches mehr\nmein meine meinem meinen meiner meines mich mir mit mittel mochte möchte mochten\nmögen möglich mögt morgen muss muß müssen musst müsst musste mussten\n\nna nach nachdem nahm natürlich neben nein neue neuen neun neunte neunten neunter\nneuntes nicht nichts nie niemand niemandem niemanden noch nun nur\n\nob oben oder offen oft ohne\n\nrecht rechte rechten rechter rechtes richtig rund\n\nsagt sagte sah satt schlecht schon sechs sechste sechsten sechster sechstes\nsehr sei seid seien sein seine seinem seinen seiner seines seit seitdem selbst\nselbst sich sie sieben siebente siebenten siebenter siebentes siebte siebten\nsiebter siebtes sind so solang solche solchem solchen solcher solches soll\nsollen sollte sollten sondern sonst sowie später statt\n\ntag tage tagen tat teil tel trotzdem tun\n\nüber überhaupt übrigens uhr um und uns unser unsere unserer unter\n\nvergangene vergangenen viel viele vielem vielen vielleicht vier vierte vierten\nvierter viertes vom von vor\n\nwahr während währenddem währenddessen wann war wäre waren wart warum was wegen\nweil weit weiter weitere weiteren weiteres welche welchem welchen welcher\nwelches wem wen wenig wenige weniger weniges wenigstens wenn wer werde werden\nwerdet wessen wie wieder will willst wir wird wirklich wirst wo wohl wollen\nwollt wollte wollten worden wurde würde wurden würden\n\nzehn zehnte zehnten zehnter zehntes zeit zu zuerst zugleich zum zunächst zur\nzurück zusammen zwanzig zwar zwei zweite zweiten zweiter zweites zwischen\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/de/punctuation.py----------------------------------------
A:spacy.lang.de.punctuation._quotes->char_classes.CONCAT_QUOTES.replace("'", '')


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/nn/__init__.py----------------------------------------
spacy.lang.nn.__init__.NorwegianNynorsk(Language)
spacy.lang.nn.__init__.NorwegianNynorskDefaults(BaseDefaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/nn/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/nn/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.nn.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/nn/punctuation.py----------------------------------------
A:spacy.lang.nn.punctuation._quotes->char_classes.CONCAT_QUOTES.replace("'", '')


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/he/__init__.py----------------------------------------
spacy.lang.he.__init__.Hebrew(Language)
spacy.lang.he.__init__.HebrewDefaults(BaseDefaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/he/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/he/lex_attrs.py----------------------------------------
A:spacy.lang.he.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.he.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.he.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/he/stop_words.py----------------------------------------
A:spacy.lang.he.stop_words.STOP_WORDS->set('\nאני\nאת\nאתה\nאנחנו\nאתן\nאתם\nהם\nהן\nהיא\nהוא\nשלי\nשלו\nשלך\nשלה\nשלנו\nשלכם\nשלכן\nשלהם\nשלהן\nלי\nלו\nלה\nלנו\nלכם\nלכן\nלהם\nלהן\nאותה\nאותו\nזה\nזאת\nאלה\nאלו\nתחת\nמתחת\nמעל\nבין\nעם\nעד\nעל\nאל\nמול\nשל\nאצל\nכמו\nאחר\nאותו\nבלי\nלפני\nאחרי\nמאחורי\nעלי\nעליו\nעליה\nעליך\nעלינו\nעליכם\nעליכן\nעליהם\nעליהן\nכל\nכולם\nכולן\nכך\nככה\nכזה\nכזאת\nזה\nאותי\nאותה\nאותם\nאותך\nאותו\nאותן\nאותנו\nואת\nאת\nאתכם\nאתכן\nאיתי\nאיתו\nאיתך\nאיתה\nאיתם\nאיתן\nאיתנו\nאיתכם\nאיתכן\nיהיה\nתהיה\nהייתי\nהיתה\nהיה\nלהיות\nעצמי\nעצמו\nעצמה\nעצמם\nעצמן\nעצמנו\nמי\nמה\nאיפה\nהיכן\nבמקום שבו\nאם\nלאן\nלמקום שבו\nמקום בו\nאיזה\nמהיכן\nאיך\nכיצד\nבאיזו מידה\nמתי\nבשעה ש\nכאשר\nכש\nלמרות\nלפני\nאחרי\nמאיזו סיבה\nהסיבה שבגללה\nלמה\nמדוע\nלאיזו תכלית\nכי\nיש\nאין\nאך\nמנין\nמאין\nמאיפה\nיכל\nיכלה\nיכלו\nיכול\nיכולה\nיכולים\nיכולות\nיוכלו\nיוכל\nמסוגל\nלא\nרק\nאולי\nאין\nלאו\nאי\nכלל\nבעד\nנגד\nאם\nעם\nאל\nאלה\nאלו\nאף\nעל\nמעל\nמתחת\nמצד\nבשביל\nלבין\nבאמצע\nבתוך\nדרך\nמבעד\nבאמצעות\nלמעלה\nלמטה\nמחוץ\nמן\nלעבר\nמכאן\nכאן\nהנה\nהרי\nפה\nשם\nאך\nברם\nשוב\nאבל\nמבלי\nבלי\nמלבד\nרק\nבגלל\nמכיוון\nעד\nאשר\nואילו\nלמרות\nכמו\nכפי\nאז\nאחרי\nכן\nלכן\nלפיכך\nעז\nמאוד\nמעט\nמעטים\nבמידה\nשוב\nיותר\nמדי\nגם\nכן\nנו\nאחר\nאחרת\nאחרים\nאחרות\nאשר\nאו\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/fa/__init__.py----------------------------------------
spacy.lang.fa.__init__.Persian(Language)
spacy.lang.fa.__init__.PersianDefaults(BaseDefaults)
spacy.lang.fa.__init__.make_lemmatizer(nlp:Language,model:Optional[Model],name:str,mode:str,overwrite:bool,scorer:Optional[Callable])


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/fa/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/fa/lex_attrs.py----------------------------------------
A:spacy.lang.fa.lex_attrs._num_words->set('\nصفر\nیک\nدو\nسه\nچهار\nپنج\nشش\nشیش\nهفت\nهشت\nنه\nده\nیازده\nدوازده\nسیزده\nچهارده\nپانزده\nپونزده\nشانزده\nشونزده\nهفده\nهجده\nهیجده\nنوزده\nبیست\nسی\nچهل\nپنجاه\nشصت\nهفتاد\nهشتاد\nنود\nصد\nیکصد\nیک\u200cصد\nدویست\nسیصد\nچهارصد\nپانصد\nپونصد\nششصد\nشیشصد\nهفتصد\nهفصد\nهشتصد\nنهصد\nهزار\nمیلیون\nمیلیارد\nبیلیون\nبیلیارد\nتریلیون\nتریلیارد\nکوادریلیون\nکادریلیارد\nکوینتیلیون\n'.split())
A:spacy.lang.fa.lex_attrs._ordinal_words->set('\nاول\nسوم\nسی\u200cام'.split())
A:spacy.lang.fa.lex_attrs.text->text.replace(',', '').replace('.', '').replace('،', '').replace('٫', '').replace('/', '').replace(',', '').replace('.', '').replace('،', '').replace('٫', '').replace('/', '')
spacy.lang.fa.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/fa/tokenizer_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/fa/syntax_iterators.py----------------------------------------
A:spacy.lang.fa.syntax_iterators.conj->doc.vocab.strings.add('conj')
A:spacy.lang.fa.syntax_iterators.np_label->doc.vocab.strings.add('NP')
spacy.lang.fa.syntax_iterators.noun_chunks(doclike:Union[Doc,Span])->Iterator[Tuple[int, int, int]]


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/fa/stop_words.py----------------------------------------
A:spacy.lang.fa.stop_words.STOP_WORDS->set('\nو\nدر\nبه\nاز\nکه\nاین\nرا\nبا\nاست\nبرای\nآن\nیک\nخود\nتا\nکرد\nبر\nهم\nنیز\nگفت\nمی\u200cشود\nوی\nشد\nدارد\nما\nاما\nیا\nشده\nباید\nهر\nآنها\nبود\nاو\nدیگر\nدو\nمورد\nمی\u200cکند\nشود\nکند\nوجود\nبین\nپیش\nشده\u200cاست\nپس\nنظر\nاگر\nهمه\nیکی\nحال\nهستند\nمن\nکنند\nنیست\nباشد\nچه\nبی\nمی\nبخش\nمی\u200cکنند\nهمین\nافزود\nهایی\nدارند\nراه\nهمچنین\nروی\nداد\nبیشتر\nبسیار\nسه\nداشت\nچند\nسوی\nتنها\nهیچ\nمیان\nاینکه\nشدن\nبعد\nجدید\nولی\nحتی\nکردن\nبرخی\nکردند\nمی\u200cدهد\nاول\nنه\nکرده\u200cاست\nنسبت\nبیش\nشما\nچنین\nطور\nافراد\nتمام\nدرباره\nبار\nبسیاری\nمی\u200cتواند\nکرده\nچون\nندارد\nدوم\nبزرگ\nطی\nحدود\nهمان\nبدون\nالبته\nآنان\nمی\u200cگوید\nدیگری\nخواهد\u200cشد\nکنیم\nقابل\nیعنی\nرشد\nمی\u200cتوان\nوارد\nکل\nویژه\nقبل\nبراساس\nنیاز\nگذاری\nهنوز\nلازم\nسازی\nبوده\u200cاست\nچرا\nمی\u200cشوند\nوقتی\nگرفت\nکم\nجای\nحالی\nتغییر\nپیدا\nاکنون\nتحت\nباعث\nمدت\nفقط\nزیادی\nتعداد\nآیا\nبیان\nرو\nشدند\nعدم\nکرده\u200cاند\nبودن\nنوع\nبلکه\nجاری\nدهد\nبرابر\nمهم\nبوده\nاخیر\nمربوط\nامر\nزیر\nگیری\nشاید\nخصوص\nآقای\nاثر\nکننده\nبودند\nفکر\nکنار\nاولین\nسوم\nسایر\nکنید\nضمن\nمانند\nباز\nمی\u200cگیرد\nممکن\nحل\nدارای\nپی\nمثل\nمی\u200cرسد\nاجرا\nدور\nمنظور\nکسی\nموجب\nطول\nامکان\nآنچه\nتعیین\nگفته\nشوند\nجمع\nخیلی\nعلاوه\nگونه\nتاکنون\nرسید\nساله\nگرفته\nشده\u200cاند\nعلت\nچهار\nداشته\u200cباشد\nخواهد\u200cبود\nطرف\nتهیه\nتبدیل\nمناسب\nزیرا\nمشخص\nمی\u200cتوانند\nنزدیک\nجریان\nروند\nبنابراین\nمی\u200cدهند\nیافت\nنخستین\nبالا\nپنج\nریزی\nعالی\nچیزی\nنخست\nبیشتری\nترتیب\nشده\u200cبود\nخاص\nخوبی\nخوب\nشروع\nفرد\nکامل\nغیر\nمی\u200cرود\nدهند\nآخرین\nدادن\nجدی\nبهترین\nشامل\nگیرد\nبخشی\nباشند\nتمامی\nبهتر\nداده\u200cاست\nحد\nنبود\nکسانی\nمی\u200cکرد\nداریم\nعلیه\nمی\u200cباشد\nدانست\nناشی\nداشتند\nدهه\nمی\u200cشد\nایشان\nآنجا\nگرفته\u200cاست\nدچار\nمی\u200cآید\nلحاظ\nآنکه\nداده\nبعضی\nهستیم\nاند\nبرداری\nنباید\nمی\u200cکنیم\nنشست\nسهم\nهمیشه\nآمد\nاش\nوگو\nمی\u200cکنم\nحداقل\nطبق\nجا\nخواهد\u200cکرد\nنوعی\nچگونه\nرفت\nهنگام\nفوق\nروش\nندارند\nسعی\nبندی\nشمار\nکلی\nکافی\nمواجه\nهمچنان\nزیاد\nسمت\nکوچک\nداشته\u200cاست\nچیز\nپشت\nآورد\nحالا\nروبه\nسال\u200cهای\nدادند\nمی\u200cکردند\nعهده\nنیمه\nجایی\nدیگران\nسی\nبروز\nیکدیگر\nآمده\u200cاست\nجز\nکنم\nسپس\nکنندگان\nخودش\nهمواره\nیافته\nشان\nصرف\nنمی\u200cشود\nرسیدن\nچهارم\nیابد\nمتر\nساز\nداشته\nکرده\u200cبود\nباره\nنحوه\nکردم\nتو\nشخصی\nداشته\u200cباشند\nمحسوب\nپخش\nکمی\nمتفاوت\nسراسر\nکاملا\nداشتن\nنظیر\nآمده\nگروهی\nفردی\nع\nهمچون\nخطر\nخویش\nکدام\nدسته\nسبب\nعین\nآوری\nمتاسفانه\nبیرون\nدار\nابتدا\nشش\nافرادی\nمی\u200cگویند\nسالهای\nدرون\nنیستند\nیافته\u200cاست\nپر\nخاطرنشان\nگاه\nجمعی\nاغلب\nدوباره\nمی\u200cیابد\nلذا\nزاده\nگردد\nاینجا'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/fa/punctuation.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/fa/generate_verbs_exc.py----------------------------------------
A:spacy.lang.fa.generate_verbs_exc.verb_roots->'\n#هست\nآخت#آهنج\nآراست#آرا\nآراماند#آرامان\nآرامید#آرام\nآرمید#آرام\nآزرد#آزار\nآزمود#آزما\nآسود#آسا\nآشامید#آشام\nآشفت#آشوب\nآشوبید#آشوب\nآغازید#آغاز\nآغشت#آمیز\nآفرید#آفرین\nآلود#آلا\nآمد#آ\nآمرزید#آمرز\nآموخت#آموز\nآموزاند#آموزان\nآمیخت#آمیز\nآورد#آر\nآورد#آور\nآویخت#آویز\nآکند#آکن\nآگاهانید#آگاهان\nارزید#ارز\nافتاد#افت\nافراخت#افراز\nافراشت#افراز\nافروخت#افروز\nافروزید#افروز\nافزود#افزا\nافسرد#افسر\nافشاند#افشان\nافکند#افکن\nافگند#افگن\nانباشت#انبار\nانجامید#انجام\nانداخت#انداز\nاندوخت#اندوز\nاندود#اندا\nاندیشید#اندیش\nانگاشت#انگار\nانگیخت#انگیز\nانگیزاند#انگیزان\nایستاد#ایست\nایستاند#ایستان\nباخت#باز\nباراند#باران\nبارگذاشت#بارگذار\nبارید#بار\nباز#بازخواه\nبازآفرید#بازآفرین\nبازآمد#بازآ\nبازآموخت#بازآموز\nبازآورد#بازآور\nبازایستاد#بازایست\nبازتابید#بازتاب\nبازجست#بازجو\nبازخواند#بازخوان\nبازخوراند#بازخوران\nبازداد#بازده\nبازداشت#بازدار\nبازرساند#بازرسان\nبازرسانید#بازرسان\nباززد#باززن\nبازستاند#بازستان\nبازشمارد#بازشمار\nبازشمرد#بازشمار\nبازشمرد#بازشمر\nبازشناخت#بازشناس\nبازشناساند#بازشناسان\nبازفرستاد#بازفرست\nبازماند#بازمان\nبازنشست#بازنشین\nبازنمایاند#بازنمایان\nبازنهاد#بازنه\nبازنگریست#بازنگر\nبازپرسید#بازپرس\nبازگذارد#بازگذار\nبازگذاشت#بازگذار\nبازگرداند#بازگردان\nبازگردانید#بازگردان\nبازگردید#بازگرد\nبازگرفت#بازگیر\nبازگشت#بازگرد\nبازگشود#بازگشا\nبازگفت#بازگو\nبازیافت#بازیاب\nبافت#باف\nبالید#بال\nباوراند#باوران\nبایست#باید\nبخشود#بخش\nبخشود#بخشا\nبخشید#بخش\nبر#برخواه\nبرآشفت#برآشوب\nبرآمد#برآ\nبرآورد#برآور\nبرازید#براز\nبرافتاد#برافت\nبرافراخت#برافراز\nبرافراشت#برافراز\nبرافروخت#برافروز\nبرافشاند#برافشان\nبرافکند#برافکن\nبراند#بران\nبرانداخت#برانداز\nبرانگیخت#برانگیز\nبربست#بربند\nبرتاباند#برتابان\nبرتابید#برتاب\nبرتافت#برتاب\nبرتنید#برتن\nبرجهید#برجه\nبرخاست#برخیز\nبرخورد#برخور\nبرد#بر\nبرداشت#بردار\nبردمید#بردم\nبرزد#برزن\nبرشد#برشو\nبرشمارد#برشمار\nبرشمرد#برشمار\nبرشمرد#برشمر\nبرنشاند#برنشان\nبرنشانید#برنشان\nبرنشست#برنشین\nبرنهاد#برنه\nبرچید#برچین\nبرکرد#برکن\nبرکشید#برکش\nبرکند#برکن\nبرگذشت#برگذر\nبرگرداند#برگردان\nبرگردانید#برگردان\nبرگردید#برگرد\nبرگرفت#برگیر\nبرگزید#برگزین\nبرگشت#برگرد\nبرگشود#برگشا\nبرگمارد#برگمار\nبرگمارید#برگمار\nبرگماشت#برگمار\nبرید#بر\nبست#بند\nبلعید#بلع\nبود#باش\nبوسید#بوس\nبویید#بو\nبیخت#بیز\nبیخت#بوز\nتاباند#تابان\nتابید#تاب\nتاخت#تاز\nتاراند#تاران\nتازاند#تازان\nتازید#تاز\nتافت#تاب\nترادیسید#ترادیس\nتراشاند#تراشان\nتراشید#تراش\nتراوید#تراو\nترساند#ترسان\nترسید#ترس\nترشاند#ترشان\nترشید#ترش\nترکاند#ترکان\nترکید#ترک\nتفتید#تفت\nتمرگید#تمرگ\nتنید#تن\nتوانست#توان\nتوفید#توف\nتپاند#تپان\nتپید#تپ\nتکاند#تکان\nتکانید#تکان\nجست#جه\nجست#جو\nجنباند#جنبان\nجنبید#جنب\nجنگید#جنگ\nجهاند#جهان\nجهید#جه\nجوشاند#جوشان\nجوشانید#جوشان\nجوشید#جوش\nجويد#جو\nجوید#جو\nخاراند#خاران\nخارید#خار\nخاست#خیز\nخایید#خا\nخراشاند#خراشان\nخراشید#خراش\nخرامید#خرام\nخروشید#خروش\nخرید#خر\nخزید#خز\nخسبید#خسب\nخشکاند#خشکان\nخشکید#خشک\nخفت#خواب\nخلید#خل\nخماند#خمان\nخمید#خم\nخنداند#خندان\nخندانید#خندان\nخندید#خند\nخواباند#خوابان\nخوابانید#خوابان\nخوابید#خواب\nخواست#خواه\nخواست#خیز\nخواند#خوان\nخوراند#خوران\nخورد#خور\nخیزاند#خیزان\nخیساند#خیسان\nداد#ده\nداشت#دار\nدانست#دان\nدر#درخواه\nدرآمد#درآ\nدرآمیخت#درآمیز\nدرآورد#درآور\nدرآویخت#درآویز\nدرافتاد#درافت\nدرافکند#درافکن\nدرانداخت#درانداز\nدرانید#دران\nدربرد#دربر\nدربرگرفت#دربرگیر\nدرخشاند#درخشان\nدرخشانید#درخشان\nدرخشید#درخش\nدرداد#درده\nدررفت#دررو\nدرماند#درمان\nدرنمود#درنما\nدرنوردید#درنورد\nدرود#درو\nدروید#درو\nدرکرد#درکن\nدرکشید#درکش\nدرگذشت#درگذر\nدرگرفت#درگیر\nدریافت#دریاب\nدرید#در\nدزدید#دزد\nدمید#دم\nدواند#دوان\nدوخت#دوز\nدوشید#دوش\nدوید#دو\nدید#بین\nراند#ران\nربود#ربا\nربود#روب\nرخشید#رخش\nرساند#رسان\nرسانید#رسان\nرست#ره\nرست#رو\nرسید#رس\nرشت#ریس\nرفت#رو\nرفت#روب\nرقصاند#رقصان\nرقصید#رقص\nرماند#رمان\nرمانید#رمان\nرمید#رم\nرنجاند#رنجان\nرنجانید#رنجان\nرنجید#رنج\nرندید#رند\nرهاند#رهان\nرهانید#رهان\nرهید#ره\nروبید#روب\nروفت#روب\nرویاند#رویان\nرویانید#رویان\nرویید#رو\nرویید#روی\nریخت#ریز\nرید#رین\nریدن#رین\nریسید#ریس\nزاد#زا\nزارید#زار\nزایاند#زایان\nزایید#زا\nزد#زن\nزدود#زدا\nزیست#زی\nساباند#سابان\nسابید#ساب\nساخت#ساز\nسایید#سا\nستاد#ستان\nستاند#ستان\nسترد#ستر\nستود#ستا\nستیزید#ستیز\nسراند#سران\nسرایید#سرا\nسرشت#سرش\nسرود#سرا\nسرکشید#سرکش\nسرگرفت#سرگیر\nسرید#سر\nسزید#سز\nسفت#سنب\nسنجید#سنج\nسوخت#سوز\nسود#سا\nسوزاند#سوزان\nسپارد#سپار\nسپرد#سپار\nسپرد#سپر\nسپوخت#سپوز\nسگالید#سگال\nشاشید#شاش\nشایست#\nشایست#شاید\nشتاباند#شتابان\nشتابید#شتاب\nشتافت#شتاب\nشد#شو\nشست#شو\nشست#شوی\nشلید#شل\nشمار#شمر\nشمارد#شمار\nشمرد#شمار\nشمرد#شمر\nشناخت#شناس\nشناساند#شناسان\nشنفت#شنو\nشنید#شنو\nشوتید#شوت\nشوراند#شوران\nشورید#شور\nشکافت#شکاف\nشکاند#شکان\nشکاند#شکن\nشکست#شکن\nشکفت#شکف\nطلبید#طلب\nطپید#طپ\nغراند#غران\nغرید#غر\nغلتاند#غلتان\nغلتانید#غلتان\nغلتید#غلت\nغلطاند#غلطان\nغلطانید#غلطان\nغلطید#غلط\nفرا#فراخواه\nفراخواند#فراخوان\nفراداشت#فرادار\nفرارسید#فرارس\nفرانمود#فرانما\nفراگرفت#فراگیر\nفرستاد#فرست\nفرسود#فرسا\nفرمود#فرما\nفرهیخت#فرهیز\nفرو#فروخواه\nفروآمد#فروآ\nفروآورد#فروآور\nفروافتاد#فروافت\nفروافکند#فروافکن\nفروبرد#فروبر\nفروبست#فروبند\nفروخت#فروش\nفروخفت#فروخواب\nفروخورد#فروخور\nفروداد#فروده\nفرودوخت#فرودوز\nفرورفت#فرورو\nفروریخت#فروریز\nفروشکست#فروشکن\nفروفرستاد#فروفرست\nفروماند#فرومان\nفرونشاند#فرونشان\nفرونشانید#فرونشان\nفرونشست#فرونشین\nفرونمود#فرونما\nفرونهاد#فرونه\nفروپاشاند#فروپاشان\nفروپاشید#فروپاش\nفروچکید#فروچک\nفروکرد#فروکن\nفروکشید#فروکش\nفروکوبید#فروکوب\nفروکوفت#فروکوب\nفروگذارد#فروگذار\nفروگذاشت#فروگذار\nفروگرفت#فروگیر\nفریفت#فریب\nفشاند#فشان\nفشرد#فشار\nفشرد#فشر\nفلسفید#فلسف\nفهماند#فهمان\nفهمید#فهم\nقاپید#قاپ\nقبولاند#قبول\nقبولاند#قبولان\nلاسید#لاس\nلرزاند#لرزان\nلرزید#لرز\nلغزاند#لغزان\nلغزید#لغز\nلمباند#لمبان\nلمید#لم\nلنگید#لنگ\nلولید#لول\nلیسید#لیس\nماسید#ماس\nمالاند#مالان\nمالید#مال\nماند#مان\nمانست#مان\nمرد#میر\nمویید#مو\nمکید#مک\nنازید#ناز\nنالاند#نالان\nنالید#نال\nنامید#نام\nنشاند#نشان\nنشست#نشین\nنمایاند#نما\nنمایاند#نمایان\nنمود#نما\nنهاد#نه\nنهفت#نهنب\nنواخت#نواز\nنوازید#نواز\nنوردید#نورد\nنوشاند#نوشان\nنوشانید#نوشان\nنوشت#نویس\nنوشید#نوش\nنکوهید#نکوه\nنگاشت#نگار\nنگرید#\nنگریست#نگر\nهراساند#هراسان\nهراسانید#هراسان\nهراسید#هراس\nهشت#هل\nوا#واخواه\nواداشت#وادار\nوارفت#وارو\nوارهاند#وارهان\nواماند#وامان\nوانهاد#وانه\nواکرد#واکن\nواگذارد#واگذار\nواگذاشت#واگذار\nور#ورخواه\nورآمد#ورآ\nورافتاد#ورافت\nوررفت#وررو\nورزید#ورز\nوزاند#وزان\nوزید#وز\nویراست#ویرا\nپاشاند#پاشان\nپاشید#پاش\nپالود#پالا\nپایید#پا\nپخت#پز\nپذیراند#پذیران\nپذیرفت#پذیر\nپراند#پران\nپراکند#پراکن\nپرداخت#پرداز\nپرستید#پرست\nپرسید#پرس\nپرهیخت#پرهیز\nپرهیزید#پرهیز\nپروراند#پروران\nپرورد#پرور\nپرید#پر\nپسندید#پسند\nپلاساند#پلاسان\nپلاسید#پلاس\nپلکید#پلک\nپناهاند#پناهان\nپناهید#پناه\nپنداشت#پندار\nپوساند#پوسان\nپوسید#پوس\nپوشاند#پوشان\nپوشید#پوش\nپویید#پو\nپژمرد#پژمر\nپژوهید#پژوه\nپکید#پک\nپیراست#پیرا\nپیمود#پیما\nپیوست#پیوند\nپیچاند#پیچان\nپیچانید#پیچان\nپیچید#پیچ\nچاپید#چاپ\nچایید#چا\nچراند#چران\nچرانید#چران\nچرباند#چربان\nچربید#چرب\nچرخاند#چرخان\nچرخانید#چرخان\nچرخید#چرخ\nچروکید#چروک\nچرید#چر\nچزاند#چزان\nچسباند#چسبان\nچسبید#چسب\nچسید#چس\nچشاند#چشان\nچشید#چش\nچلاند#چلان\nچلانید#چلان\nچپاند#چپان\nچپید#چپ\nچکاند#چکان\nچکید#چک\nچید#چین\nکاست#کاه\nکاشت#کار\nکاوید#کاو\nکرد#کن\nکشاند#کشان\nکشانید#کشان\nکشت#کار\nکشت#کش\nکشید#کش\nکند#کن\nکوباند#کوبان\nکوبید#کوب\nکوشید#کوش\nکوفت#کوب\nکوچانید#کوچان\nکوچید#کوچ\nگایید#گا\nگداخت#گداز\nگذارد#گذار\nگذاشت#گذار\nگذراند#گذران\nگذشت#گذر\nگرازید#گراز\nگرانید#گران\nگرایید#گرا\nگرداند#گردان\nگردانید#گردان\nگردید#گرد\nگرفت#گیر\nگروید#گرو\nگریاند#گریان\nگریخت#گریز\nگریزاند#گریزان\nگریست#گر\nگریست#گری\nگزارد#گزار\nگزاشت#گزار\nگزید#گزین\nگسارد#گسار\nگستراند#گستران\nگسترانید#گستران\nگسترد#گستر\nگسست#گسل\nگسلاند#گسل\nگسیخت#گسل\nگشاد#گشا\nگشت#گرد\nگشود#گشا\nگفت#گو\nگمارد#گمار\nگماشت#گمار\nگنجاند#گنجان\nگنجانید#گنجان\nگنجید#گنج\nگنداند#گندان\nگندید#گند\nگوارید#گوار\nگوزید#گوز\nگیراند#گیران\nیازید#یاز\nیافت#یاب\nیونید#یون\n'.strip().split()
A:spacy.lang.fa.generate_verbs_exc.(past, present)->verb_root.split('#')
A:spacy.lang.fa.generate_verbs_exc.conjugations->list(set(map(lambda item: item.replace('بآ', 'بیا').replace('نآ', 'نیا'), conjugations)))


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ko/__init__.py----------------------------------------
A:spacy.lang.ko.__init__.self._mecab->try_mecab_import()
A:spacy.lang.ko.__init__.self._mecab_tokenizer->self._mecab('-F%f[0],%f[7]')
A:spacy.lang.ko.__init__.dtokens->list(self.detailed_tokens(text))
A:spacy.lang.ko.__init__.doc->Doc(self.vocab, words=surfaces, spaces=list(check_spaces(text, surfaces)))
A:spacy.lang.ko.__init__.(first_tag, sep, eomi_tags)->dtoken['tag'].partition('+')
A:spacy.lang.ko.__init__.(tag, _, expr)->feature.partition(',')
A:spacy.lang.ko.__init__.(lemma, _, remainder)->expr.partition('/')
A:spacy.lang.ko.__init__.config->load_config_from_str(DEFAULT_CONFIG)
A:spacy.lang.ko.__init__.idx->text.find(token, start)
spacy.lang.ko.__init__.Korean(Language)
spacy.lang.ko.__init__.KoreanDefaults(BaseDefaults)
spacy.lang.ko.__init__.KoreanTokenizer(self,vocab:Vocab)
spacy.lang.ko.__init__.KoreanTokenizer.__init__(self,vocab:Vocab)
spacy.lang.ko.__init__.KoreanTokenizer.__reduce__(self)
spacy.lang.ko.__init__.KoreanTokenizer.detailed_tokens(self,text:str)->Iterator[Dict[str, Any]]
spacy.lang.ko.__init__.KoreanTokenizer.mecab_tokenizer(self)
spacy.lang.ko.__init__.KoreanTokenizer.score(self,examples)
spacy.lang.ko.__init__.check_spaces(text,tokens)
spacy.lang.ko.__init__.create_tokenizer()
spacy.lang.ko.__init__.try_mecab_import()->None


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ko/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ko/lex_attrs.py----------------------------------------
A:spacy.lang.ko.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.ko.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.ko.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ko/stop_words.py----------------------------------------
A:spacy.lang.ko.stop_words.STOP_WORDS->set('\n이\n있\n하\n것\n들\n그\n되\n수\n이\n보\n않\n없\n나\n주\n아니\n등\n같\n때\n년\n가\n한\n지\n오\n말\n일\n그렇\n위하\n때문\n그것\n두\n말하\n알\n그러나\n받\n못하\n일\n그런\n또\n더\n많\n그리고\n좋\n크\n시키\n그러\n하나\n살\n데\n안\n어떤\n번\n나\n다른\n어떻\n들\n이렇\n점\n싶\n말\n좀\n원\n잘\n놓\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ko/tag_map.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ko/punctuation.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ms/__init__.py----------------------------------------
spacy.lang.ms.__init__.Malay(Language)
spacy.lang.ms.__init__.MalayDefaults(BaseDefaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ms/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ms/lex_attrs.py----------------------------------------
A:spacy.lang.ms.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.ms.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
A:spacy.lang.ms.lex_attrs.(_, num)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('-')
spacy.lang.ms.lex_attrs.is_currency(text)
spacy.lang.ms.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ms/_tokenizer_exceptions_list.py----------------------------------------
A:spacy.lang.ms._tokenizer_exceptions_list.MS_BASE_EXCEPTIONS->set('\naba-aba\nabah-abah\nabar-abar\nabrit-abritan\nabu-abu\nabuk-abuk\nabun-abun\nacak-acak\nacak-acakan\nacang-acang\naci-aci\naci-acian\naci-acinya\nadang-adang\nadap-adapan\nadik-beradik\naduk-adukan\nagak-agak\nagar-agar\nagut-agut\nair-cooled\najar-ajar\naji-aji\nakal-akal\nakhir-akhir\naki-aki\nalah-mengalahi\nalan-alan\nalang-alang\nalang-alangan\nalap-alap\nali-ali\nalih-alih\naling-aling\naling-alingan\nalip-alipan\nalon-alon\nalu-alu\nalu-aluan\nalun-alun\nalur-alur\nambah-ambah\nambai-ambai\nambil-mengambil\nambring-ambringan\nambu-ambu\nambung-ambung\namin-amin\nampai-ampai\namung-amung\nanai-anai\nanak-anak\nanak-anakan\nanak-beranak\nancak-ancak\nancang-ancang\nandang-andang\nangan-angan\nanggar-anggar\nangin-angin\nangin-anginan\nangkul-angkul\nangkup-angkup\nangkut-angkut\nani-ani\naning-aning\nanjang-anjang\nanjing-anjing\nanjung-anjung\nanjung-anjungan\nantar-antar\nante-mortem\nanting-anting\nantung-antung\nanyam-menganyam\napa-apa\napi-api\napit-apit\naprit-apritan\narah-arah\narak-arakan\naram-aram\nari-ari\naru-aru\nasa-asaan\nasam-asaman\nasuh-asuh\natas-mengatasi\nati-ati\naudio-visual\navant-garde\nawang-awang\nawang-gemawang\nayak-ayak\nayam-ayam\nayam-ayaman\nayang-ayang\nayeng-ayengan\nayun-temayun\nback-up\nbahu-membahu\nbaik-baik\nbajang-bajang\nbaji-baji\nbalai-balai\nbalam-balam\nbalas-membalas\nbaling-baling\nbalut-balut\nbangun-bangun\nbantal-bantal\nbarat-barat\nbarau-barau\nbari-bari\nbarung-barung\nbasa-basi\nbata-bata\nbatir-batir\nbau-bauan\nbayang-bayang\nbedil-bedal\nbegana-begini\nbekal-bekalan\nbelat-belit\nbelu-belai\nbenggal-benggil\nbengkal-bengkil\nbengkang-bengkok\nbengkang-bengkong\nberabad-abad\nberabun-rabun\nberada-ada\nberagah-agah\nberagak-agak\nberagam-ragam\nberaja-raja\nberakit-rakit\nberaku-akuan\nberalun-alun\nberamah-ramahan\nberamah-tamah\nberamai-ramai\nberambai-ambai\nberambal-ambalan\nberamuk-amukan\nberandai-andai\nberandai-randai\nberang-berang\nberangan-angan\nberanggap-anggapan\nberangguk-angguk\nberangin-angin\nberangka-angka\nberangka-angkaan\nberangkai-rangkai\nberanja-anja\nberantai-rantai\nberapi-api\nberapung-apung\nberarak-arakan\nberas-beras\nberasing-asingan\nberatus-ratus\nberawas-awas\nberayal-ayalan\nberayun-ayun\nberbagai-bagai\nberbahas-bahasan\nberbalas-balasan\nberbalik-balik\nberbanjar-banjar\nberbantah-bantah\nberbanyak-banyak\nberbarik-barik\nberbasah-basah\nberbatu-batu\nberbayang-bayang\nberbecak-becak\nberbedil-bedilan\nberbeka-beka\nberbelakang-belakangan\nberbelang-belang\nberbeli-belian\nberbelit-belit\nberbelok-belok\nberbenar-benar\nberbencah-bencah\nberbesar-besar\nberbidai-bidai\nberbiku-biku\nberbilik-bilik\nberbinar-binar\nberbincang-bincang\nberbingkah-bingkah\nberbintang-bintang\nberbintik-bintik\nberbintil-bintil\nberbisik-bisik\nberbolak-balik\nberbolong-bolong\nberbondong-bondong\nberbongkah-bongkah\nberbuai-buai\nberbual-bual\nberbukit-bukit\nberbulan-bulan\nberbunga-bunga\nberbuntut-buntut\nberbunuh-bunuhan\nberburu-buru\nberburuk-buruk\nberbutir-butir\nbercabang-cabang\nbercaci-cacian\nbercakap-cakap\nbercakar-cakaran\nbercantik-cantik\nbercari-cari\nbercari-carian\nbercarik-carik\nbercepat-cepat\nbercerai-berai\nbercerai-cerai\nbercetai-cetai\nbercikun-cikun\nbercinta-cintaan\nbercita-cita\nberciut-ciut\nberconteng-conteng\nbercoreng-coreng\nbercoreng-moreng\nbercuit-cuit\nbercumbu-cumbu\nbercumbu-cumbuan\nbercura-bura\nbercura-cura\nberdada-dadaan\nberdahulu-dahuluan\nberdalam-dalam\nberdebar-debar\nberdecap-decap\nberdedai-dedai\nberdegap-degap\nberdegar-degar\nberdeham-deham\nberdekah-dekah\nberdekat-dekat\nberdelat-delat\nberdembun-dembun\nberdempang-dempang\nberdendam-dendaman\nberdengkang-dengkang\nberdentang-dentang\nberdentum-dentum\nberdentung-dentung\nberdepak-depak\nberdepan-depan\nberderai-derai\nberderak-derak\nberderau-derau\nberdering-dering\nberderung-derung\nberdesak-desakan\nberdesing-desing\nberdesus-desus\nberdikit-dikit\nberdingkit-dingkit\nberdua-dua\nberduri-duri\nberduru-duru\nberduyun-duyun\nberebut-rebut\nberebut-rebutan\nberegang-regang\nberek-berek\nberembut-rembut\nberempat-empat\nberenak-enak\nberenteng-renteng\nberesah-resah\nberfoya-foya\nbergagah-gagahan\nbergagap-gagap\nbergalur-galur\nberganda-ganda\nberganti-ganti\nbergarah-garah\nbergaruk-garuk\nbergegas-gegas\nbergelang-gelang\nbergelap-gelap\nbergelas-gelasan\nbergeleng-geleng\nbergemal-gemal\nbergembut-gembut\nbergerek-gerek\nbergesa-gesa\nbergilir-gilir\nbergolek-golek\nbergores-gores\nbergotong-royong\nbergugus-gugus\nbergulung-gulung\nbergulut-gulut\nbergumpal-gumpal\nbergunung-gunung\nberhadap-hadapan\nberhamun-hamun\nberhandai-handai\nberhanyut-hanyut\nberhari-hari\nberhati-hati\nberhilau-hilau\nberhujan-hujan\nberia-ia\nberia-ria\nberiak-riak\nberibu-ribu\nberigi-rigi\nbering-bering\nberingat-ingat\nberinggit-ringgit\nberintik-rintik\nberiring-iring\nberiring-iringan\nberjabir-jabir\nberjaga-jaga\nberjagung-jagung\nberjalan-jalan\nberjalar-jalar\nberjalin-jalin\nberjalur-jalur\nberjam-jam\nberjauh-jauhan\nberjejal-jejal\nberjela-jela\nberjenis-jenis\nberjenjang-jenjang\nberjilid-jilid\nberjinak-jinak\nberjingkat-jingkat\nberjingkrak-jingkrak\nberjongkok-jongkok\nberjubel-jubel\nberjujut-jujutan\nberjulai-julai\nberjumbai-jumbai\nberjurai-jurai\nberjurus-jurus\nberjuta-juta\nberkaca-kaca\nberkait-kaitan\nberkala-kala\nberkali-kali\nberkanjar-kanjar\nberkaok-kaok\nberkarung-karung\nberkasih-kasihan\nberkata-kata\nberkatak-katak\nberkecai-kecai\nberkecek-kecek\nberkecil-kecil\nberkecil-kecilan\nberkedip-kedip\nberkejang-kejang\nberkejap-kejap\nberkejar-kejaran\nberkelar-kelar\nberkelip-kelip\nberkelit-kelit\nberkelok-kelok\nberkelompok-kelompok\nberkelun-kelun\nberkembur-kembur\nberkempul-kempul\nberkena-kenaan\nberkenal-kenalan\nberkendur-kendur\nberkeok-keok\nberkepak-kepak\nberkepal-kepal\nberkeping-keping\nberkepul-kepul\nberkeras-kerasan\nberkeritik-keritik\nberkeruit-keruit\nberkerut-kerut\nberketak-ketak\nberketak-ketik\nberketi-keti\nberketil-ketil\nberketuk-ketak\nberketul-ketul\nberkial-kial\nberkian-kian\nberkias-kiasan\nberkibar-kibar\nberkilah-kilah\nberkilat-kilat\nberkilau-kilauan\nberkilo-kilo\nberkinja-kinja\nberkipas-kipas\nberkira-kira\nberkirim-kiriman\nberkobar-kobar\nberkobok-kobok\nberkocak-kocak\nberkodi-kodi\nberkolek-kolek\nberkopah-kopah\nberkotak-kotak\nberkuat-kuatan\nberkunang-kunang\nberkurun-kurun\nberkusau-kusau\nberkusu-kusu\nberkusut-kusut\nberkuting-kuting\nberkutu-kutuan\nberlabun-labun\nberlain-lainan\nberlalai-lalai\nberlama-lama\nberlambai-lambai\nberlambak-lambak\nberlampang-lampang\nberlapang-lapang\nberlapis-lapis\nberlapuk-lapuk\nberlarah-larah\nberlarat-larat\nberlari-larian\nberlarik-larik\nberlarut-larut\nberlawak-lawak\nberlayap-layapan\nberlebih-lebih\nberlebih-lebihan\nberlekas-lekas\nberlena-lena\nberlengah-lengah\nberlenggek-lenggek\nberlenggok-lenggok\nberleret-leret\nberliang-liuk\nberliku-liku\nberlimpah-limpah\nberlimpap-limpap\nberlimpit-limpit\nberlinang-linang\nberlindak-lindak\nberlipat-lipat\nberlompok-lompok\nberloncat-loncatan\nberlopak-lopak\nberlubang-lubang\nbermaaf-maafan\nbermacam-macam\nbermain-main\nbermalas-malas\nbermanik-manik\nbermanis-manis\nbermanja-manja\nbermasak-masak\nbermati-mati\nbermegah-megah\nbermemek-memek\nbermesra-mesraan\nbermewah-mewah\nberminggu-minggu\nberminta-minta\nbermuda-muda\nbermudah-mudah\nbermuka-muka\nbermula-mula\nbermulut-mulut\nbernafsi-nafsi\nbernaka-naka\nberniat-niat\nberogak-ogak\nberoleng-oleng\nberolok-olok\nberomong-omong\nberonggok-onggok\nberorang-orang\nberoyal-royal\nberpada-pada\nberpahit-pahit\nberpair-pair\nberpal-pal\nberpalu-palu\nberpalu-paluan\nberpalun-palun\nberpandai-pandai\nberpandang-pandangan\nberpangkat-pangkat\nberpanjang-panjang\nberpasang-pasang\nberpasang-pasangan\nberpayah-payah\nberpeluh-peluh\nberpeluk-pelukan\nberpenat-penat\nberpencar-pencar\nberpendar-pendar\nberpenggal-penggal\nberperai-perai\nberpesai-pesai\nberpesta-pesta\nberpesuk-pesuk\nberpetak-petak\nberpeti-peti\nberpihak-pihak\nberpijar-pijar\nberpikul-pikul\nberpilih-pilih\nberpilin-pilin\nberpindah-pindah\nberpintal-pintal\nberpirau-pirau\nberpisah-pisah\nberpolah-polah\nberpongah-pongah\nberpontang-panting\nberporah-porah\nberpotong-potong\nberpuak-puak\nberpual-pual\nberpugak-pugak\nberpuluh-puluh\nberpulun-pulun\nberpuntal-puntal\nberpura-pura\nberpusar-pusar\nberpusing-pusing\nberpusu-pusu\nberputar-putar\nbersaf-saf\nbersahut-sahutan\nbersakit-sakit\nbersalah-salahan\nbersalam-salaman\nbersalin-salin\nbersama-sama\nbersambut-sambutan\nbersampan-sampan\nbersantai-santai\nbersapa-sapaan\nbersarang-sarang\nbersedan-sedan\nbersedia-sedia\nbersedu-sedu\nbersekat-sekat\nberselang-selang\nberselang-seli\nbersembur-semburan\nbersempit-sempit\nbersenang-senang\nbersenang-senangkan\nbersenda-senda\nbersendi-sendi\nbersepah-sepah\nbersepi-sepi\nberserak-serak\nberseri-seri\nbersesak-sesak\nbersetai-setai\nbersia-sia\nbersiap-siap\nbersiar-siar\nbersilir-silir\nbersimbur-simburan\nbersinau-sinau\nbersorak-sorai\nbersuap-suapan\nbersudah-sudah\nbersuka-suka\nbersuka-sukaan\nbersuku-suku\nbersumpah-sumpahan\nbersungguh-sungguh\nbersungut-sungut\nbersunyi-sunyi\nbersusah-susah\nbersusuk-susuk\nbersusuk-susukan\nbersutan-sutan\nbertabur-tabur\nbertahu-tahu\nbertahun-tahun\nbertajuk-tajuk\nbertakik-takik\nbertala-tala\nbertali-tali\nbertalu-talu\nbertambah-tambah\nbertanda-tandaan\nbertangis-tangisan\nbertangkil-tangkil\nbertanya-tanya\nbertarik-tarikan\nbertatai-tatai\nbertatih-tatih\nbertawan-tawan\nbertawar-tawaran\nbertebu-tebu\nbertebu-tebukan\nberteguh-teguh\nberteguh-teguhan\nberteka-teki\nbertelau-telau\nbertele-tele\nbertempat-tempat\nbertempuh-tempuh\nbertenang-tenang\nbertenggang-tenggangan\nbertentu-tentu\nbertepek-tepek\nberterang-terang\nberterang-terangan\nbertikam-tikaman\nbertimbal-timbalan\nbertimbun-timbun\nbertimpa-timpa\nbertimpas-timpas\nbertingkah-tingkah\nbertingkat-tingkat\nbertinjau-tinjauan\nbertiras-tiras\nbertitar-titar\nbertoboh-toboh\nbertolak-tolak\nbertolak-tolakan\nbertolong-tolongan\nbertonjol-tonjol\nbertua-tua\nbertua-tuaan\nbertual-tual\nbertubi-tubi\nbertukar-tukar\nbertukar-tukaran\nbertukas-tukas\nbertumpak-tumpak\nbertunda-tunda\nbertunjuk-tunjukan\nbertura-tura\nberturut-turut\nbertutur-tutur\nberuas-ruas\nberubah-ubah\nberulang-alik\nberulang-ulang\nberumbai-rumbai\nberundung-undung\nberunggas-runggas\nberungkur-ungkuran\nberuntai-untai\nberuntun-runtun\nberunyai-unyai\nberupa-rupa\nberura-ura\nberuris-uris\nberurut-urutan\nberwarna-warna\nberwarna-warni\nberwindu-windu\nberwiru-wiru\nberyang-yang\nbesar-besaran\nbetak-betak\nbeti-beti\nbetul-betul\nbiang-biang\nbiar-biar\nbiji-bijian\nbila-bila\nbilang-bilang\nbincang-bincut\nbini-binian\nbiri-biri\nbiru-biru\nbisik-bisik\nbiti-biti\nbolak-balik\nbolang-baling\nbongkar-bangkir\nbuah-buahan\nbuat-buatan\nbuaya-buaya\nbubun-bubun\nbugi-bugi\nbuilt-in\nbukan-bukan\nbulan-bulan\nbulan-bulanan\nbulang-bulang\nbulat-bulat\nbuli-buli\nbulu-bulu\nbuluh-buluh\nbulus-bulus\nbunga-bungaan\nbunuh-membunuh\nbunyi-bunyian\nburu-buru\nburung-burungan\nbye-bye\ncabik-cabik\ncaing-caing\ncalar-balar\ncara-cara\ncarut-marut\ncawi-cawi\ncebar-cebur\ncelam-celum\ncelangak-celinguk\ncelas-celus\nceledang-celedok\ncelengkak-celengkok\ncemas-cemas\ncentang-perenang\ncepat-cepat\ncerai-berai\nceruk-menceruk\nceruk-meruk\ncheck-up\nchit-chat\ncirit-birit\ncita-cita\nclose-up\nclosed-circuit\ncobak-cabik\ncobar-cabir\ncola-cala\ncompang-camping\ncongak-cangit\ncongkah-cangkih\ncongkah-mangkih\ncopak-capik\ncorak-carik\ncorat-coret\ncoreng-moreng\ncuang-caing\ncubung-cubung\nculik-culik\ncuma-cuma\ncumi-cumi\ncungap-cangip\ncupu-cupu\ndahulu-mendahului\ndali-dali\ndapur-dapur\ndari-dari\ndaru-daru\ndatang-datang\ndatang-mendatangi\ndaun-daunan\ndawai-dawai\ndayang-dayang\ndegap-degap\ndekak-dekak\ndekat-dekat\ndengar-dengaran\ndesas-desus\ndiam-diam\ndo-it-yourself\ndokok-dokok\ndolak-dalik\ndorong-mendorong\ndrive-in\ndua-dua\ndua-duanya\nduduk-duduk\ndulang-dulang\necek-ecek\nembuh-embuhan\nempek-empek\nempok-empok\nencal-encal\nendap-endap\nendut-endutan\nengah-engah\nenggan-enggan\nengkah-engkah\nentah-berentah\nerang-erot\nerong-erong\nfast-food\nfifty-fifty\nflip-flop\nfollow-up\nfoya-foya\ngaba-gaba\ngabai-gabai\ngada-gada\ngading-gading\ngado-gado\ngajah-gajahan\ngala-gala\ngali-galian\ngaling-galing\ngalu-galu\ngamit-gamitan\ngampang-gampangan\nganal-ganal\nganda-berganda\ngapah-gopoh\ngara-gara\ngarah-garah\ngatal-gatal\ngawar-gawar\ngaya-gayanya\ngedebak-gedebuk\ngelang-gelang\ngelembung-gelembungan\ngeli-geli\ngeliang-geliut\ngeliat-geliut\ngempul-gempul\ngendang-gendang\ngenjang-genjot\ngerabak-gerubuk\ngerak-gerik\ngerbas-gerbus\ngerit-gerit\ngeruh-gerah\ngetak-getuk\ngeti-geti\ngila-gila\ngila-gilaan\ngilang-gemilang\ngilap-gemilap\ngili-gili\ngiling-giling\nginang-ginang\ngirik-girik\ngiring-giring\ngo-kart\ngolak-galik\ngonta-ganti\ngotong-royong\ngual-gail\ngudu-gudu\ngula-gula\ngulang-gulang\nguna-guna\nguntang-guntang\ngunung-ganang\ngunung-gemunung\ngunung-gunungan\nhabis-habis\nhabis-habisan\nhalai-balai\nhalf-time\nhampir-hampir\nharap-harapan\nharum-haruman\nhati-hati\nheavy-duty\nhebat-hebatan\nhidup-hidup\nhiru-biru\nhiruk-pikuk\nhubaya-hubaya\nhula-hula\nhuru-hara\nibar-ibar\nicak-icak\nigau-igauan\nikut-ikut\nikut-ikutan\nilam-ilam\nimbang-imbangan\ninang-inang\ninca-binca\nincang-incut\ningat-ingat\ningat-ingatan\ningau-ingauan\ninggang-inggung\ninjak-injak\niras-iras\niring-iringan\niseng-iseng\njadi-jadian\njala-jala\njamah-jamahan\njambu-jambu\njangan-jangan\njarang-jarang\njari-jari\njaring-jaring\njarum-jarum\njauh-jauh\njawi-jawi\njebat-jebatan\njelur-jelir\njendal-jendul\njenggar-jenggur\njentik-jentik\njerah-jerih\njolong-jolong\njongkar-jangkir\njuak-juak\njuang-juang\njulung-julung\njurai-jurai\nkabu-kabu\nkacang-kacang\nkacang-kacangan\nkacau-balau\nkadang-kadang\nkail-kail\nkait-kait\nkakek-kakek\nkalau-kalau\nkaleng-kalengan\nkalut-malut\nkambing-kambing\nkanak-kanak\nkapa-kapa\nkapan-kapan\nkapu-kapu\nkarang-karangan\nkarang-mengarang\nkareseh-peseh\nkarut-marut\nkatang-katang\nkawa-kawa\nkayu-kayuan\nkeabu-abuan\nkeasyik-asyikan\nkebarat-baratan\nkebasah-basahan\nkebat-kebit\nkebata-bataan\nkebelanda-belandaan\nkebiru-biruan\nkebudak-budakan\nkecil-kecilan\nkecil-mengecil\nkecuh-kecah\nkedek-kedek\nkegadis-gadisan\nkegelap-gelapan\nkegila-gilaan\nkegirang-girangan\nkehijau-hijauan\nkehitam-hitaman\nkejaga-jagaan\nkejingga-jinggaan\nkekabur-kaburan\nkekanak-kanakan\nkekoboi-koboian\nkekuning-kuningan\nkelak-kelik\nkelak-keluk\nkelaki-lakian\nkelang-kelok\nkelap-kelip\nkelek-kelek\nkelek-kelekan\nkelik-kelik\nkelip-kelip\nkelusuh-kelasah\nkelut-melut\nkemak-kemik\nkemalu-maluan\nkemanja-manjaan\nkemarah-marahan\nkemasam-masaman\nkemati-matian\nkemerah-merahan\nkempang-kempis\nkempas-kempis\nkemuda-mudaan\nkena-mengena\nkenal-mengenal\nkenang-kenangan\nkencang-kencung\nkendang-kendang\nkendang-kendangan\nkentung-kentung\nkenyat-kenyit\nkepandir-pandiran\nkepang-kepot\nkeperak-perakan\nkepilu-piluan\nkepura-puraan\nkeputih-putihan\nkerah-kerahan\nkerancak-rancakan\nkerang-kerangan\nkerang-keroh\nkerang-kerung\nkerap-kerap\nkeras-mengerasi\nkercap-kercip\nkercap-kercup\nkeriang-keriut\nkernyat-kernyut\nkerong-kerong\nkeropas-kerapis\nkertak-kertuk\nkeruntang-pungkang\nkesap-kesip\nkesenak-senakan\nkesewenang-wenangan\nkesia-siaan\nkesik-kesik\nkesipu-sipuan\nkesu-kesi\nkesuh-kesih\nkesuk-kesik\nketergesa-gesaan\nketi-keti\nketidur-tiduran\nketiga-tiganya\nketua-tuaan\nketuan-tuanan\nkeungu-unguan\nkia-kia\nkiak-kiak\nkial-kial\nkiang-kiut\nkibang-kibut\nkicang-kecoh\nkicang-kicu\nkida-kida\nkilau-mengilau\nkili-kili\nkira-kira\nkira-kiraan\nkisi-kisi\nkocah-kacih\nkodok-kodok\nkolang-kaling\nkoleh-koleh\nkolong-kolong\nkoma-koma\nkomat-kamit\nkontal-kantil\nkontang-kanting\nkosak-kasik\nkotak-katik\nkotak-kotak\nkuat-kuat\nkucar-kacir\nkucing-kucing\nkucing-kucingan\nkuda-kuda\nkuda-kudaan\nkudap-kudap\nkulah-kulah\nkulak-kulak\nkulik-kulik\nkulum-kulum\nkumat-kamit\nkunang-kunang\nkupat-kapit\nkupu-kupu\nkura-kura\nkurang-kurang\nkusat-mesat\nkutat-kutet\nkuti-kuti\nlabi-labi\nlabu-labu\nlagi-lagi\nlaguh-lagah\nlaki-laki\nlalu-lalang\nlama-kelamaan\nlama-lama\nlamat-lamat\nlambat-lambat\nlancar-lancar\nlangak-longok\nlangit-langit\nlanja-lanjaan\nlapat-lapat\nlarge-scale\nlari-lari\nlauk-pauk\nlawah-lawah\nlawak-lawak\nlawi-lawi\nlayang-layang\nlayu-layuan\nlebih-lebih\nlegak-legok\nlekak-lekuk\nlekap-lekup\nlekas-lekas\nlekuh-lekih\nlekup-lekap\nlenggak-lenggok\nlenggok-lenggok\nlengket-lengket\nlentam-lentum\nlentang-lentok\nlentang-lentung\nlepa-lepa\nlerang-lerang\nlereng-lereng\nletah-letai\nletup-letup\nliang-liuk\nlidah-lidah\nline-up\nliuk-liuk\nliung-liung\nlobi-lobi\nlock-up\nlopak-lapik\nlopak-lopak\nlumba-lumba\nlumi-lumi\nluntang-lantung\nlupa-lupa\nlupa-lupaan\nmain-mainan\nmakan-makanan\nmake-up\nmalai-malai\nmalam-malam\nmalar-malar\nmali-mali\nmalu-malu\nmana-mana\nmanik-manik\nmanis-manisan\nmark-up\nmasing-masing\nmata-mata\nmati-matian\nmaya-maya\nmegap-megap\nmegrek-megrek\nmelak-melak\nmelambai-lambai\nmelambai-lambaikan\nmelambat-lambatkan\nmelaun-laun\nmelawak-lawak\nmelayap-layap\nmelayap-layapkan\nmelebih-lebihi\nmelebih-lebihkan\nmelejang-lejangkan\nmelengah-lengah\nmelihat-lihat\nmelimpah-limpah\nmelincah-lincah\nmeloncat-loncat\nmelonco-lonco\nmelonjak-lonjak\nmemacak-macak\nmemaki-maki\nmemaksa-maksa\nmemandai-mandai\nmemanggil-manggil\nmemanis-manis\nmemanjut-manjut\nmemasak-masak\nmemata-matai\nmematah-matah\nmematut-matut\nmemayah-mayahkan\nmembagi-bagikan\nmembalik-balik\nmembangkit-bangkit\nmembayang-bayangi\nmembayang-bayangkan\nmembelai-belai\nmembenar-benar\nmembenar-benari\nmemberai-beraikan\nmembesar-besarkan\nmembolak-balikkan\nmembuang-buang\nmembuat-buat\nmembunga-bungai\nmemburu-buru\nmemburu-burukan\nmemburuk-burukkan\nmemencak-mencak\nmemencar-mencar\nmemetak-metak\nmemetang-metangkan\nmemetir-metir\nmemikir-mikirkan\nmemilih-milih\nmeminang-minang\nmeminta-minta\nmemisah-misahkan\nmemontang-mantingkan\nmemperamat-amat\nmemperamat-amatkan\nmemperbagai-bagaikan\nmemperganda-gandakan\nmemperganduh-ganduhkan\nmempermacam-macamkan\nmemperolok-olokkan\nmempersama-samakan\nmempertubi-tubi\nmempertubi-tubikan\nmemperturut-turutkan\nmemuja-muja\nmemukang-mukang\nmemulun-mulun\nmemundi-mundi\nmemundi-mundikan\nmemuyu-muyu\nmenagak-nagak\nmenakut-nakuti\nmenanjur-nanjur\nmenanti-nanti\nmenari-nari\nmencabik-cabik\nmencabik-cabikkan\nmencaing-caing\nmencak-mencak\nmencakup-cakup\nmencapak-capak\nmencari-cari\nmencarik-carik\nmencarut-carut\nmencengis-cengis\nmencepak-cepak\nmencepuk-cepuk\nmencerai-beraikan\nmencetai-cetai\nmenciap-ciap\nmenciar-ciar\nmencita-citakan\nmenciut-ciut\nmencoang-coang\nmencubit-cubit\nmencuri-curi\nmendecap-decap\nmendengking-dengking\nmenderak-derakkan\nmenderau-derau\nmenderu-deru\nmendesas-desuskan\nmendesus-desus\nmendewa-dewakan\nmendudu-dudu\nmenebu-nebu\nmenegur-neguri\nmengabung-ngabung\nmengaci-acikan\nmengada-ada\nmengaduk-aduk\nmengagak-agak\nmengagak-agihkan\nmengagut-agut\nmengais-ngais\nmengali-ali\nmengalur-alur\nmengamang-amang\nmengamat-amati\nmengambai-ambaikan\nmengambang-ambang\nmengancak-ancak\nmengangan-angankan\nmengangguk-angguk\nmengangin-anginkan\nmengangkat-angkat\nmengap-mengap\nmengapa-apai\nmengapi-apikan\nmengarah-arahi\nmengata-ngatai\nmengaum-aumkan\nmengejan-ejan\nmengelai-ngelai\nmengelepik-ngelepik\nmengelus-elus\nmengembut-embut\nmengenap-enapkan\nmengenjak-enjak\nmengepak-ngepak\nmengepak-ngepakkan\nmenggaba-gabai\nmenggalur-galur\nmenggamak-gamak\nmenggapai-gapai\nmenggapai-gapaikan\nmenggelepar-gelepar\nmenggelepar-geleparkan\nmenggemak-gemak\nmenggerecak-gerecak\nmenggesa-gesakan\nmenggili-gili\nmenggorek-gorek\nmenggosok-gosok\nmengguit-guit\nmenghalai-balaikan\nmenghinap-hinap\nmengiang-ngiang\nmengibas-ngibas\nmengidam-idamkan\nmengilah-ngilahkan\nmengilai-ilai\nmengilat-ngilatkan\nmengilik-ngilik\nmengimak-imak\nmengiming-iming\nmenginjak-injak\nmengipas-ngipas\nmengira-ngira\nmengira-ngirakan\nmengiras-iras\nmengiras-irasi\nmengitar-ngitar\nmengitik-ngitik\nmengogok-ogok\nmengolak-alikkan\nmengoleng-oleng\nmengongkang-ongkang\nmengongkok-ongkok\nmengonyah-anyih\nmengotak-ngatikkan\nmengoyak-ngoyakkan\nmengoyak-oyak\nmenguar-nguarkan\nmenguar-uarkan\nmenguber-uber\nmengubit-ubit\nmengubrak-abrik\nmengucar-ngacirkan\nmengucek-ngucek\nmenguik-uik\nmenguis-uis\nmengulit-ulit\nmenguman-uman\nmengumbang-ambingkan\nmengumpak-umpak\nmengungkat-ungkat\nmengungkit-ungkit\nmengurik-urik\nmengutak-ngatikkan\nmengutik-ngutik\nmenimang-nimang\nmeningkat-ningkat\nmeniru-niru\nmeniup-niup\nmenjadi-jadi\nmenjengek-jengek\nmenjengit-jengit\nmenjilat-jilat\nmentah-mentah\nmentang-mentang\nmenunda-nunda\nmenusuk-nusuk\nmenyama-nyama\nmenyambar-nyambar\nmenyanjung-nyanjung\nmenyapu-nyapu\nmenyarat-nyarat\nmenyendi-nyendi\nmenyeret-nyeret\nmenyeru-nyerukan\nmenyia-nyiakan\nmenyungguh-nyungguhi\nmeraba-raba\nmerangkak-rangkak\nmerasa-rasai\nmeraung-raung\nmeraung-raungkan\nmerayau-rayau\nmerayu-rayu\nmereka-reka\nmerelap-relap\nmeremah-remah\nmeremeh-temehkan\nmerempah-rempahi\nmerengek-rengek\nmerenik-renik\nmerenta-renta\nmerenyai-renyai\nmerintang-rintang\nmerintik-rintik\nmerobek-robek\nmeronta-ronta\nmerungus-rungus\nmerungut-rungut\nmewarna-warnikan\nmeyakin-yakini\nmiju-miju\nminta-minta\nmoga-moga\nmorat-marit\nmuda-mudi\nmudah-mudahan\nmuka-muka\nmula-mula\nmuluk-muluk\nnaga-naga\nnanti-nantian\nnasi-nasi\nnasib-nasiban\nnenek-nenek\nnyolong-nyolong\nogah-ogahan\nogak-ogak\nolak-alik\nolak-olak\nolang-aling\nolang-alingan\noleh-oleh\nolok-olok\nolok-olokan\nolong-olong\non-screen\nonde-onde\none-to-one\noneng-oneng\nongkang-ongkang\nongol-ongol\nonyah-anyih\norak-arik\norang-aring\norang-orangan\norok-orok\norong-orong\notak-otak\notak-otakan\npadi-padian\npagi-pagi\npalas-palas\npaling-paling\npalu-memalu\npanas-panas\npandang-memandang\npanji-panji\npara-para\nparu-paru\npasang-memasang\npasu-pasu\npaya-paya\npecah-pecah\npelan-pelan\npengundang-undang\nperang-perangan\nperintang-rintang\nperlahan-lahan\nperlip-perlipan\npertama-tama\nperundang-undangan\npesan-pesan\npiat-piut\npick-up\npijak-pijak\npijar-pijar\npijat-pijat\npina-pina\npisang-pisang\nplay-off\npohon-pohonan\npokrol-pokrolan\npolang-paling\npoma-poma\npontang-panting\nporak-parik\nporak-peranda\npotong-memotong\npuji-pujian\npukang-pukang\npukul-memukul\npulang-pergi\npulut-pulut\npundi-pundi\npunggung-memunggung\npura-pura\npusar-pusar\npush-up\npusing-pusing\nputus-putus\nrada-rada\nradio-frequency\nragu-ragu\nrama-rama\nrambu-rambu\nrango-rango\nrasa-rasanya\nrata-rata\nreal-time\nrebah-rebah\nrebah-rebahan\nredam-redam\nreka-reka\nreka-rekaan\nremah-remah\nremang-remang\nrembah-rembih\nremeh-temeh\nrempah-rempah\nrepuh-repuh\nriang-riang\nribu-ribu\nrigi-rigi\nrobak-rabik\nrobat-rabit\nrole-play\nroll-on\nrombang-rambing\nruak-ruak\nruku-ruku\nrumah-rumah\nrumah-rumahan\nrumput-rumputan\nrunding-merunding\nrunggu-rangga\nrunner-up\nrupa-rupa\nrupa-rupanya\nsaban-saban\nsabung-menyabung\nsaing-menyaing\nsalah-salah\nsama-sama\nsamar-samar\nsambar-menyambar\nsambung-bersambung\nsambung-menyambung\nsambut-menyambut\nsampai-sampai\nsandar-menyandar\nsangat-sangat\nsangkut-menyangkut\nsapa-menyapa\nsapu-sapu\nsarit-sarit\nsatu-satu\nsatu-satunya\nsayup-menyayup\nsayup-sayup\nsayur-mayur\nsayur-sayuran\nsci-fi\nseakal-akal\nseakan-akan\nsealak-alak\nsebaik-baiknya\nsebelah-menyebelah\nsebentar-sebentar\nseberang-menyeberang\nseboleh-bolehnya\nsedalam-dalamnya\nsedang-menyedang\nsedap-sedapan\nsedapat-dapatnya\nsedikit-dikitnya\nsedikit-sedikit\nsedikit-sedikitnya\nseelok-eloknya\nsegala-galanya\nsegan-menyegan\nsegan-menyegani\nsegan-segan\nsehari-hari\nsehari-harian\nsejadi-jadinya\nsekali-kali\nsekali-sekali\nsekira-kira\nsekonyong-konyong\nsekuasa-kuasanya\nsekurang-kurangnya\nsela-menyela\nsela-sela\nselama-lamanya\nselambat-lambatnya\nselang-seli\nselang-seling\nselar-belar\nselat-latnya\nselekas-lekasnya\nselepas-lepas\nself-esteem\nself-help\nsema-sema\nsemah-semah\nsemak-semak\nsemalam-malaman\nsemasa-masa\nsemata-mata\nsembunyi-sembunyi\nsembunyi-sembunyian\nsemena-mena\nsemenda-menyemenda\nsemengga-mengga\nsementang-mentang\nsemu-semu\nsemut-semutan\nsengal-sengal\nsengau-sengauan\nseolah-olah\nsepala-pala\nsepandai-pandai\nsepetang-petangan\nsepoi-sepoi\nsepuas-puasnya\nserang-menyerang\nseraya-menyeraya\nserba-serbi\nserbah-serbih\nserembah-serembih\nsering-sering\nserta-menyertai\nserta-serta\nsesal-menyesali\nsesudah-sudah\nsesudah-sudahnya\nsesuka-suka\nsetempat-setempat\nsetengah-setengah\nsetidak-tidaknya\nseupaya-upaya\nseupaya-upayanya\nsewaktu-waktu\nsewenang-wenang\nshort-term\nsia-sia\nsiang-siang\nsiapa-siapa\nsibar-sibar\nsibur-sibur\nsida-sida\nsiku-siku\nsilah-silah\nsilang-menyilang\nsilir-semilir\nsinar-seminar\nsindir-menyindir\nsinggah-menyinggah\nsorak-sorai\nstand-by\nstand-up\nsudu-sudu\nsudung-sudung\nsuka-suka\nsulang-menyulang\nsulur-suluran\nsumpah-sumpah\nsumpit-sumpit\nsungguh-sungguh\nsungut-sungut\nsuram-suram\nsurat-menyurat\nsuruh-suruhan\ntabar-tabar\ntabir-mabir\ntabrak-tubruk\ntabuh-tabuhan\ntahu-menahu\ntahu-tahu\ntakang-takik\ntake-off\ntakut-takut\ntakut-takutan\ntali-bertali\ntali-tali\ntampak-tampak\ntanam-menanam\ntanam-tanaman\ntanda-tanda\ntangan-menangan\ntangan-tangan\ntanggung-menanggung\ntapa-tapa\ntapak-tapak\ntari-menari\ntari-tarian\ntarik-menarik\ntatah-tatah\ntawak-tawak\ntawang-tawang\ntawar-menawar\ntawar-tawar\ntayum-temayum\ntebu-tebu\ntegak-tegak\nteka-teki\ntemas-temas\ntembak-menembak\ntemut-temut\ntenggang-menenggang\nteraba-raba\nterambang-ambang\nterang-terang\nterang-terangan\nteranggar-anggar\nterangguk-angguk\nteranggul-anggul\nterangin-angin\nterangkup-angkup\nteranja-anja\nterapung-apung\nterayan-rayan\nterayap-rayap\nterbada-bada\nterbahak-bahak\nterbata-bata\nterbatuk-batuk\nterbayang-bayang\nterbengkil-bengkil\nterbirit-birit\nterbuai-buai\nterbuang-buang\nterburu-buru\ntercangak-cangak\ntercengang-cengang\ntercilap-cilap\ntercongget-congget\ntercungap-cungap\nterdangka-dangka\nterdengih-dengih\nterekeh-ekeh\nterembut-embut\nterembut-rembut\nterengah-engah\nteresak-esak\ntergagap-gagap\ntergagau-gagau\ntergaguk-gaguk\ntergapai-gapai\ntergegap-gegap\ntergegas-gegas\ntergelung-gelung\ntergerenyeng-gerenyeng\ntergesa-gesa\ntergila-gila\ntergontai-gontai\ntergudik-gudik\nterguling-guling\ntergulut-gulut\nterharak-harak\nterharap-harap\nterhengit-hengit\nterhinggut-hinggut\nterigau-igau\nterincut-incut\nteringa-inga\nteringat-ingat\nterinjak-injak\nterjembak-jembak\nterjerit-jerit\nterkadang-kadang\nterkakah-kakah\nterkakak-kakak\nterkanjar-kanjar\nterkapah-kapah\nterkapai-kapai\nterkapung-kapung\nterkatah-katah\nterkatung-katung\nterkecap-kecap\nterkedek-kedek\nterkedip-kedip\nterkejar-kejar\nterkekau-kekau\nterkekeh-kekeh\nterkekek-kekek\nterkelinjat-kelinjat\nterkelip-kelip\nterkempul-kempul\nterkemut-kemut\nterkencar-kencar\nterkepak-kepak\nterkesot-kesot\nterkesut-kesut\nterkial-kial\nterkincak-kincak\nterkindap-kindap\nterkinja-kinja\nterkirai-kirai\nterkitar-kitar\nterkocoh-kocoh\nterkokol-kokol\nterkosel-kosel\nterkoteng-koteng\nterkumpal-kumpal\nterlara-lara\nterlayang-layang\nterlebih-lebih\nterlincah-lincah\nterliuk-liuk\nterlolong-lolong\nterlongong-longong\ntermangu-mangu\ntermanja-manja\ntermata-mata\ntermengah-mengah\ntermimpi-mimpi\nternanti-nanti\nterngiang-ngiang\nteroleng-oleng\nterpandang-pandang\nterpecah-pecah\nterpekik-pekik\nterpereh-pereh\nterpikau-pikau\nterpinga-pinga\nterpingkal-pingkal\nterpontang-panting\nterpusing-pusing\nterputus-putus\ntersanga-sanga\ntersaruk-saruk\ntersedan-sedan\ntersedih-sedih\ntersedu-sedu\ntersendat-sendat\ntersendeng-sendeng\ntersengal-sengal\ntersengguk-sengguk\ntersengut-sengut\ntersera-sera\nterserak-serak\ntersetai-setai\ntersia-sia\ntersipu-sipu\ntersoja-soja\ntersungkuk-sungkuk\ntertagak-tagak\ntertahan-tahan\ntertatih-tatih\ntertegun-tegun\ntertekan-tekan\nterteleng-teleng\nterumbang-ambing\nterumbang-umbang\nterungkap-ungkap\nterus-menerus\nterus-terusan\nthink-tank\ntiap-tiap\ntiba-tiba\ntidak-tidak\ntidur-tidur\ntie-dye\ntiga-tiganya\ntikam-menikam\ntilik-menilik\ntimah-timah\ntimang-timangan\ntimbang-menimbang\ntimu-timu\ntindih-bertindih\ntinjau-meninjau\ntip-off\ntiru-tiruan\ntiup-tiup\ntokak-takik\ntokok-menokok\ntolak-menolak\ntolong-menolong\ntop-level\ntrade-in\ntua-tua\ntuan-tuan\ntuang-tuang\ntuban-tuban\ntukang-menukang\ntukar-menukar\ntulang-tulangan\ntuli-tuli\ntulis-menulis\ntumbuh-tumbuhan\ntune-up\ntunggang-tunggit\ntupai-tupai\nturun-temurun\nturut-menurut\nturut-turutan\ntwo-tone\nuar-uar\nubel-ubel\nubun-ubun\nubur-ubur\nuci-uci\nudap-udapan\nugal-ugalan\nuir-uir\nujar-ujar\nukir-mengukir\nula-ula\nulak-ulak\nulang-alik\nulang-aling\nulang-ulang\nulap-ulap\nular-ular\nular-ularan\nulung-ulung\numang-umang\numbang-ambing\numbi-umbian\numbul-umbul\numbut-umbut\nuncang-uncit\nundak-undakan\nundang-undang\nunduk-unduk\nundung-undung\nundur-undur\nunggat-unggit\nungkit-ungkit\nunting-unting\nuntung-untung\nuntung-untungan\nupside-down\nura-ura\nuran-uran\nurat-urat\nuring-uringan\nurup-urup\nurup-urupan\nurus-urus\nuser-user\nuser-useran\nutar-utar\nvoice-over\nwalk-out\nwangi-wangian\nwanti-wanti\nwara-wara\nwarna-warni\nwater-cooled\nworld-class\nyang-yang\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ms/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.ms.tokenizer_exceptions.orth_title->'-'.join([part.title() for part in orth.split('-')])
A:spacy.lang.ms.tokenizer_exceptions.orth_caps->'-'.join([part.upper() for part in orth.split('-')])
A:spacy.lang.ms.tokenizer_exceptions.orth_lower->orth.lower()
A:spacy.lang.ms.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ms/syntax_iterators.py----------------------------------------
A:spacy.lang.ms.syntax_iterators.conj->doc.vocab.strings.add('conj')
A:spacy.lang.ms.syntax_iterators.np_label->doc.vocab.strings.add('NP')
spacy.lang.ms.syntax_iterators.noun_chunks(doclike:Union[Doc,Span])->Iterator[Tuple[int, int, int]]


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ms/stop_words.py----------------------------------------
A:spacy.lang.ms.stop_words.STOP_WORDS->set('\nada adalah adanya adapun agak agaknya agar akan akankah akhir akhiri akhirnya\naku akulah amat amatlah anda andalah antar antara antaranya apa apaan apabila\napakah apalagi apatah artinya asal asalkan atas atau ataukah ataupun awal\nawalnya\n\nbagai bagaikan bagaimana bagaimanakah bagaimanapun bagi bagian bahkan bahwa\nbahwasanya baik bakal bakalan balik banyak bapak baru bawah beberapa begini\nbeginian beginikah beginilah begitu begitukah begitulah begitupun bekerja\nbelakang belakangan belum belumlah benar benarkah benarlah berada berakhir\nberakhirlah berakhirnya berapa berapakah berapalah berapapun berarti berawal\nberbagai berdatangan beri berikan berikut berikutnya berjumlah berkali-kali\nberkata berkehendak berkeinginan berkenaan berlainan berlalu berlangsung\nberlebihan bermacam bermacam-macam bermaksud bermula bersama bersama-sama\nbersiap bersiap-siap bertanya bertanya-tanya berturut berturut-turut bertutur\nberujar berupa besar betul betulkah biasa biasanya bila bilakah bisa bisakah\nboleh bolehkah bolehlah buat bukan bukankah bukanlah bukannya bulan bung\n\ncara caranya cukup cukupkah cukuplah cuma\n\ndahulu dalam dan dapat dari daripada datang dekat demi demikian demikianlah\ndengan depan di dia diakhiri diakhirinya dialah diantara diantaranya diberi\ndiberikan diberikannya dibuat dibuatnya didapat didatangkan digunakan\ndiibaratkan diibaratkannya diingat diingatkan diinginkan dijawab dijelaskan\ndijelaskannya dikarenakan dikatakan dikatakannya dikerjakan diketahui\ndiketahuinya dikira dilakukan dilalui dilihat dimaksud dimaksudkan\ndimaksudkannya dimaksudnya diminta dimintai dimisalkan dimulai dimulailah\ndimulainya dimungkinkan dini dipastikan diperbuat diperbuatnya dipergunakan\ndiperkirakan diperlihatkan diperlukan diperlukannya dipersoalkan dipertanyakan\ndipunyai diri dirinya disampaikan disebut disebutkan disebutkannya disini\ndisinilah ditambahkan ditandaskan ditanya ditanyai ditanyakan ditegaskan\nditujukan ditunjuk ditunjuki ditunjukkan ditunjukkannya ditunjuknya dituturkan\ndituturkannya diucapkan diucapkannya diungkapkan dong dua dulu\n\nempat enggak enggaknya entah entahlah\n\nguna gunakan\n\nhal hampir hanya hanyalah hari harus haruslah harusnya hendak hendaklah\nhendaknya hingga\n\nia ialah ibarat ibaratkan ibaratnya ibu ikut ingat ingat-ingat ingin inginkah\ninginkan ini inikah inilah itu itukah itulah\n\njadi jadilah jadinya jangan jangankan janganlah jauh jawab jawaban jawabnya\njelas jelaskan jelaslah jelasnya jika jikalau juga jumlah jumlahnya justru\n\nkala kalau kalaulah kalaupun kalian kami kamilah kamu kamulah kan kapan\nkapankah kapanpun karena karenanya kasus kata katakan katakanlah katanya ke\nkeadaan kebetulan kecil kedua keduanya keinginan kelamaan kelihatan\nkelihatannya kelima keluar kembali kemudian kemungkinan kemungkinannya kenapa\nkepada kepadanya kesampaian keseluruhan keseluruhannya keterlaluan ketika\nkhususnya kini kinilah kira kira-kira kiranya kita kitalah kok kurang\n\nlagi lagian lah lain lainnya lalu lama lamanya lanjut lanjutnya lebih lewat\nlima luar\n\nmacam maka makanya makin malah malahan mampu mampukah mana manakala manalagi\nmasa masalah masalahnya masih masihkah masing masing-masing mau maupun\nmelainkan melakukan melalui melihat melihatnya memang memastikan memberi\nmemberikan membuat memerlukan memihak meminta memintakan memisalkan memperbuat\nmempergunakan memperkirakan memperlihatkan mempersiapkan mempersoalkan\nmempertanyakan mempunyai memulai memungkinkan menaiki menambahkan menandaskan\nmenanti menanti-nanti menantikan menanya menanyai menanyakan mendapat\nmendapatkan mendatang mendatangi mendatangkan menegaskan mengakhiri mengapa\nmengatakan mengatakannya mengenai mengerjakan mengetahui menggunakan\nmenghendaki mengibaratkan mengibaratkannya mengingat mengingatkan menginginkan\nmengira mengucapkan mengucapkannya mengungkapkan menjadi menjawab menjelaskan\nmenuju menunjuk menunjuki menunjukkan menunjuknya menurut menuturkan\nmenyampaikan menyangkut menyatakan menyebutkan menyeluruh menyiapkan merasa\nmereka merekalah merupakan meski meskipun meyakini meyakinkan minta mirip\nmisal misalkan misalnya mula mulai mulailah mulanya mungkin mungkinkah\n\nnah naik namun nanti nantinya nyaris nyatanya\n\noleh olehnya\n\npada padahal padanya pak paling panjang pantas para pasti pastilah penting\npentingnya per percuma perlu perlukah perlunya pernah persoalan pertama\npertama-tama pertanyaan pertanyakan pihak pihaknya pukul pula pun punya\n\nrasa rasanya rata rupanya\n\nsaat saatnya saja sajalah saling sama sama-sama sambil sampai sampai-sampai\nsampaikan sana sangat sangatlah satu saya sayalah se sebab sebabnya sebagai\nsebagaimana sebagainya sebagian sebaik sebaik-baiknya sebaiknya sebaliknya\nsebanyak sebegini sebegitu sebelum sebelumnya sebenarnya seberapa sebesar\nsebetulnya sebisanya sebuah sebut sebutlah sebutnya secara secukupnya sedang\nsedangkan sedemikian sedikit sedikitnya seenaknya segala segalanya segera\nseharusnya sehingga seingat sejak sejauh sejenak sejumlah sekadar sekadarnya\nsekali sekali-kali sekalian sekaligus sekalipun sekarang sekarang sekecil\nseketika sekiranya sekitar sekitarnya sekurang-kurangnya sekurangnya sela\nselain selaku selalu selama selama-lamanya selamanya selanjutnya seluruh\nseluruhnya semacam semakin semampu semampunya semasa semasih semata semata-mata\nsemaunya sementara semisal semisalnya sempat semua semuanya semula sendiri\nsendirian sendirinya seolah seolah-olah seorang sepanjang sepantasnya\nsepantasnyalah seperlunya seperti sepertinya sepihak sering seringnya serta\nserupa sesaat sesama sesampai sesegera sesekali seseorang sesuatu sesuatunya\nsesudah sesudahnya setelah setempat setengah seterusnya setiap setiba setibanya\nsetidak-tidaknya setidaknya setinggi seusai sewaktu siap siapa siapakah\nsiapapun sini sinilah soal soalnya suatu sudah sudahkah sudahlah supaya\n\ntadi tadinya tahu tahun tak tambah tambahnya tampak tampaknya tandas tandasnya\ntanpa tanya tanyakan tanyanya tapi tegas tegasnya telah tempat tengah tentang\ntentu tentulah tentunya tepat terakhir terasa terbanyak terdahulu terdapat\nterdiri terhadap terhadapnya teringat teringat-ingat terjadi terjadilah\nterjadinya terkira terlalu terlebih terlihat termasuk ternyata tersampaikan\ntersebut tersebutlah tertentu tertuju terus terutama tetap tetapi tiap tiba\ntiba-tiba tidak tidakkah tidaklah tiga tinggi toh tunjuk turut tutur tuturnya\n\nucap ucapnya ujar ujarnya umum umumnya ungkap ungkapnya untuk usah usai\n\nwaduh wah wahai waktu waktunya walau walaupun wong\n\nyaitu yakin yakni yang\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ms/punctuation.py----------------------------------------
A:spacy.lang.ms.punctuation.UNITS->merge_chars(_units)
A:spacy.lang.ms.punctuation.CURRENCY->merge_chars(_currency)
A:spacy.lang.ms.punctuation.MONTHS->merge_chars(_months)
A:spacy.lang.ms.punctuation.LIST_CURRENCY->split_chars(_currency)
A:spacy.lang.ms.punctuation._prefixes->list(TOKENIZER_PREFIXES)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/nl/__init__.py----------------------------------------
spacy.lang.nl.__init__.Dutch(Language)
spacy.lang.nl.__init__.DutchDefaults(BaseDefaults)
spacy.lang.nl.__init__.make_lemmatizer(nlp:Language,model:Optional[Model],name:str,mode:str,overwrite:bool,scorer:Optional[Callable])


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/nl/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/nl/lemmatizer.py----------------------------------------
A:spacy.lang.nl.lemmatizer.lookup_table->self.lookups.get_table('lemma_lookup', {})
A:spacy.lang.nl.lemmatizer.string->string.lower().lower()
A:spacy.lang.nl.lemmatizer.univ_pos->token.pos_.lower()
A:spacy.lang.nl.lemmatizer.index_table->self.lookups.get_table('lemma_index', {})
A:spacy.lang.nl.lemmatizer.exc_table->self.lookups.get_table('lemma_exc', {})
A:spacy.lang.nl.lemmatizer.rules_table->self.lookups.get_table('lemma_rules', {})
A:spacy.lang.nl.lemmatizer.index->self.lookups.get_table('lemma_index', {}).get(univ_pos, {})
A:spacy.lang.nl.lemmatizer.exceptions->self.lookups.get_table('lemma_exc', {}).get(univ_pos, {})
A:spacy.lang.nl.lemmatizer.rules->self.lookups.get_table('lemma_rules', {}).get(univ_pos, {})
A:spacy.lang.nl.lemmatizer.lemma_index->self.lookups.get_table('lemma_index', {}).get(univ_pos, {})
A:spacy.lang.nl.lemmatizer.looked_up_lemma->self.lookups.get_table('lemma_lookup', {}).get(string)
A:spacy.lang.nl.lemmatizer.forms->list(dict.fromkeys(oov_forms))
spacy.lang.nl.DutchLemmatizer(Lemmatizer)
spacy.lang.nl.DutchLemmatizer.get_lookups_config(cls,mode:str)->Tuple[List[str], List[str]]
spacy.lang.nl.DutchLemmatizer.lookup_lemmatize(self,token:Token)->List[str]
spacy.lang.nl.DutchLemmatizer.rule_lemmatize(self,token:Token)->List[str]
spacy.lang.nl.lemmatizer.DutchLemmatizer(Lemmatizer)
spacy.lang.nl.lemmatizer.DutchLemmatizer.get_lookups_config(cls,mode:str)->Tuple[List[str], List[str]]
spacy.lang.nl.lemmatizer.DutchLemmatizer.lookup_lemmatize(self,token:Token)->List[str]
spacy.lang.nl.lemmatizer.DutchLemmatizer.rule_lemmatize(self,token:Token)->List[str]


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/nl/lex_attrs.py----------------------------------------
A:spacy.lang.nl.lex_attrs._num_words->set('\nnul een één twee drie vier vijf zes zeven acht negen tien elf twaalf dertien\nveertien twintig dertig veertig vijftig zestig zeventig tachtig negentig honderd\nduizend miljoen miljard biljoen biljard triljoen triljard\n'.split())
A:spacy.lang.nl.lex_attrs._ordinal_words->set('\neerste tweede derde vierde vijfde zesde zevende achtste negende tiende elfde\ntwaalfde dertiende veertiende twintigste dertigste veertigste vijftigste\nzestigste zeventigste tachtigste negentigste honderdste duizendste miljoenste\nmiljardste biljoenste biljardste triljoenste triljardste\n'.split())
A:spacy.lang.nl.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.nl.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.nl.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/nl/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.nl.tokenizer_exceptions.uppered->orth.upper()
A:spacy.lang.nl.tokenizer_exceptions.capsed->orth.capitalize()
A:spacy.lang.nl.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/nl/syntax_iterators.py----------------------------------------
A:spacy.lang.nl.syntax_iterators.span_label->doc.vocab.strings.add('NP')
A:spacy.lang.nl.syntax_iterators.nsubjs->filter(lambda x: x.dep == doc.vocab.strings['nsubj'], word.children)
A:spacy.lang.nl.syntax_iterators.next_word->next(nsubjs, None)
A:spacy.lang.nl.syntax_iterators.children->filter(lambda x: x.dep in noun_deps, word.children)
A:spacy.lang.nl.syntax_iterators.start_span->min(children_i)
spacy.lang.nl.syntax_iterators.noun_chunks(doclike:Union[Doc,Span])->Iterator[Tuple[int, int, int]]


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/nl/stop_words.py----------------------------------------
A:spacy.lang.nl.stop_words.STOP_WORDS->set("\naan af al alle alles allebei alleen allen als altijd ander anders andere anderen aangaande aangezien achter achterna\nafgelopen aldus alhoewel anderzijds\n\nben bij bijna bijvoorbeeld behalve beide beiden beneden bent bepaald beter betere betreffende binnen binnenin boven\nbovenal bovendien bovenstaand buiten\n\ndaar dan dat de der den deze die dit doch doen door dus daarheen daarin daarna daarnet daarom daarop des dezelfde dezen\ndien dikwijls doet doorgaand doorgaans\n\neen eens en er echter enige eerder eerst eerste eersten effe eigen elk elke enkel enkele enz erdoor etc even eveneens\nevenwel\n\nff\n\nge geen geweest gauw gedurende gegeven gehad geheel gekund geleden gelijk gemogen geven geweest gewoon gewoonweg\ngeworden gij\n\nhaar had heb hebben heeft hem het hier hij hoe hun hadden hare hebt hele hen hierbeneden hierboven hierin hoewel hun\n\niemand iets ik in is idd ieder ikke ikzelf indien inmiddels inz inzake\n\nja je jou jouw jullie jezelf jij jijzelf jouwe juist\n\nkan kon kunnen klaar konden krachtens kunnen kunt\n\nlang later liet liever\n\nmaar me meer men met mij mijn moet mag mede meer meesten mezelf mijzelf min minder misschien mocht mochten moest moesten\nmoet moeten mogelijk mogen\n\nna naar niet niets nog nu nabij nadat net nogal nooit nr nu\n\nof om omdat ons ook op over omhoog omlaag omstreeks omtrent omver onder ondertussen ongeveer onszelf onze ooit opdat\nopnieuw opzij over overigens\n\npas pp precies prof publ\n\nreeds rond rondom\n\nsedert sinds sindsdien slechts sommige spoedig steeds\n\n‘t 't te tegen toch toen tot tamelijk ten tenzij ter terwijl thans tijdens toe totdat tussen\n\nu uit uw uitgezonderd uwe uwen\n\nvan veel voor vaak vanaf vandaan vanuit vanwege veeleer verder verre vervolgens vgl volgens vooraf vooral vooralsnog\nvoorbij voordat voordien voorheen voorop voort voorts vooruit vrij vroeg\n\nwant waren was wat we wel werd wezen wie wij wil worden waar waarom wanneer want weer weg wegens weinig weinige weldra\nwelk welke welken werd werden wiens wier wilde wordt\n\nzal ze zei zelf zich zij zijn zo zonder zou zeer zeker zekere zelfde zelfs zichzelf zijnde zijne zo’n zoals zodra zouden\n zoveel zowat zulk zulke zulks zullen zult\n".split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/nl/punctuation.py----------------------------------------
A:spacy.lang.nl.punctuation._quotes->char_classes.CONCAT_QUOTES.replace("'", '')
A:spacy.lang.nl.punctuation._units->merge_chars(' '.join(_list_units))


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ro/__init__.py----------------------------------------
spacy.lang.ro.__init__.Romanian(Language)
spacy.lang.ro.__init__.RomanianDefaults(BaseDefaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ro/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ro/lex_attrs.py----------------------------------------
A:spacy.lang.ro.lex_attrs._num_words->set('\nzero unu doi două trei patru cinci șase șapte opt nouă zece\nunsprezece doisprezece douăsprezece treisprezece patrusprezece cincisprezece șaisprezece șaptesprezece optsprezece nouăsprezece\ndouăzeci treizeci patruzeci cincizeci șaizeci șaptezeci optzeci nouăzeci\nsută mie milion miliard bilion trilion cvadrilion catralion cvintilion sextilion septilion enșpemii\n'.split())
A:spacy.lang.ro.lex_attrs._ordinal_words->set('\nprimul doilea treilea patrulea cincilea șaselea șaptelea optulea nouălea zecelea\nprima doua treia patra cincia șasea șaptea opta noua zecea\nunsprezecelea doisprezecelea treisprezecelea patrusprezecelea cincisprezecelea șaisprezecelea șaptesprezecelea optsprezecelea nouăsprezecelea\nunsprezecea douăsprezecea treisprezecea patrusprezecea cincisprezecea șaisprezecea șaptesprezecea optsprezecea nouăsprezecea\ndouăzecilea treizecilea patruzecilea cincizecilea șaizecilea șaptezecilea optzecilea nouăzecilea sutălea\ndouăzecea treizecea patruzecea cincizecea șaizecea șaptezecea optzecea nouăzecea suta\nmiilea mielea mia milionulea milioana miliardulea miliardelea miliarda enșpemia\n'.split())
A:spacy.lang.ro.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.ro.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.ro.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ro/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.ro.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ro/stop_words.py----------------------------------------
A:spacy.lang.ro.stop_words.STOP_WORDS->set('\na\nabia\nacea\naceasta\naceastă\naceea\naceeasi\naceeași\nacei\naceia\nacel\nacela\nacelasi\nacelași\nacele\nacelea\nacest\nacesta\naceste\nacestea\nacestei\nacestia\nacestui\naceşti\naceştia\nacolo\nacord\nacum\nadica\nadică\nai\naia\naibă\naici\naiurea\nal\nala\nalaturi\nale\nalea\nalt\nalta\naltceva\naltcineva\nalte\naltfel\nalti\naltii\naltul\nalături\nam\nanume\napoi\napai\napăi\nar\nare\nas\nasa\nasemenea\nasta\nastazi\nastea\nastfel\nastăzi\nasupra\natare\natat\natata\natatea\natatia\nati\natit\natita\natitea\natitia\natunci\nau\navea\navem\naveţi\naveți\navut\nazi\naş\naşadar\naţi\naș\nașadar\nați\nb\nba\nbine\nbucur\nbună\nc\nca\ncam\ncand\ncapat\ncare\ncareia\ncarora\ncaruia\ncat\ncatre\ncaut\nce\ncea\nceea\ncei\nceilalti\ncel\ncele\ncelor\nceva\nchiar\nci\ncinci\ncind\ncine\ncineva\ncit\ncita\ncite\nciteva\nciti\ncitiva\nconform\ncontra\ncu\ncui\ncum\ncumva\ncurând\ncurînd\ncând\ncât\ncâte\ncâtva\ncâţi\ncâți\ncînd\ncît\ncîte\ncîtva\ncîţi\ncîți\ncă\ncăci\ncărei\ncăror\ncărora\ncărui\ncăruia\ncătre\nd\nda\ndaca\ndacă\ndar\ndat\ndatorită\ndată\ndau\nde\ndeasupra\ndeci\ndecit\ndegraba\ndeja\ndeoarece\ndeparte\ndesi\ndespre\ndeşi\ndeși\ndin\ndinaintea\ndincolo\ndincoace\ndintr\ndintr-\ndintre\ndoar\ndoi\ndoilea\ndouă\ndrept\ndupa\ndupă\ndă\ndeunaseara\ndeunăseară\ndeunazi\ndeunăzi\ne\nea\nei\nel\nele\nera\neram\neste\neu\nexact\neşti\nești\nf\nface\nfara\nfata\nfel\nfi\nfie\nfiecare\nfii\nfim\nfiu\nfiţi\nfiți\nfoarte\nfost\nfrumos\nfără\ng\ngeaba\ngraţie\ngrație\nh\ni\nia\niar\nieri\nii\nil\nimi\nin\ninainte\ninapoi\ninca\nincotro\nincit\ninsa\nintr\nintre\nisi\niti\nj\nk\nl\nla\nle\nli\nlor\nlui\nlângă\nlîngă\nm\nma\nmai\nmare\nmacar\nmăcar\nmata\nmatale\nmea\nmei\nmele\nmereu\nmeu\nmi\nmie\nmine\nmod\nmult\nmulta\nmulte\nmulti\nmultă\nmulţi\nmulţumesc\nmulți\nmulțumesc\nmâine\nmîine\nmă\nn\nna\nne\nneincetat\nneîncetat\nnevoie\nni\nnici\nnicidecum\nnicidecat\nnicidecât\nniciodata\nniciodată\nnicăieri\nnimeni\nnimeri\nnimic\nniste\nnişte\nniște\nnoastre\nnoastră\nnoi\nnoroc\nnostri\nnostru\nnou\nnoua\nnouă\nnoştri\nnoștri\nnu\nnumai\no\nodata\nodată\nodinioara\nodinioară\nopt\nor\nori\noricare\norice\noricine\noricum\noricând\noricât\noricînd\noricît\noriunde\np\npai\npăi\nparca\nparcă\npatra\npatru\npatrulea\npe\npentru\npeste\npic\npina\nplus\npoate\npot\nprea\nprima\nprimul\nprin\nprintr-\nprintre\nputini\npuţin\npuţina\npuţină\npână\npînă\nr\nrog\ns\nsa\nsa-mi\nsa-ti\nsai\nsale\nsau\nse\nsi\nsint\nsintem\nspate\nspre\nsub\nsunt\nsuntem\nsunteţi\nsunteți\nsus\nsută\nsînt\nsîntem\nsînteţi\nsînteți\nsă\nsăi\nsău\nt\nta\ntale\nte\nti\ntimp\ntine\ntoata\ntoate\ntoată\ntocmai\ntot\ntoti\ntotul\ntotusi\ntotuşi\ntotuși\ntoţi\ntoți\ntrei\ntreia\ntreilea\ntu\ntuturor\ntăi\ntău\nu\nul\nului\nun\nuna\nunde\nundeva\nunei\nuneia\nunele\nuneori\nunii\nunor\nunora\nunu\nunui\nunuia\nunul\nv\nva\nvai\nvi\nvoastre\nvoastră\nvoi\nvom\nvor\nvostru\nvouă\nvoştri\nvoștri\nvreme\nvreo\nvreun\nvă\nx\nz\nzece\nzero\nzi\nzice\nîi\nîl\nîmi\nîmpotriva\nîn\nînainte\nînaintea\nîncotro\nîncât\nîncît\nîntre\nîntrucât\nîntrucît\nîţi\nîți\năla\nălea\năsta\năstea\năştia\năștia\nşapte\nşase\nşi\nştiu\nţi\nţie\nșapte\nșase\nși\nștiu\nți\nție\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ro/punctuation.py----------------------------------------
A:spacy.lang.ro.punctuation.upper_token->token.upper()
A:spacy.lang.ro.punctuation._ud_rrt_prefix_variants->_make_ro_variants(_ud_rrt_prefixes)
A:spacy.lang.ro.punctuation._ud_rrt_suffix_variants->_make_ro_variants(_ud_rrt_suffixes)
spacy.lang.ro.punctuation._make_ro_variants(tokens)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/gu/__init__.py----------------------------------------
spacy.lang.gu.__init__.Gujarati(Language)
spacy.lang.gu.__init__.GujaratiDefaults(BaseDefaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/gu/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/gu/stop_words.py----------------------------------------
A:spacy.lang.gu.stop_words.STOP_WORDS->set('\nએમ\nઆ\nએ\nરહી\nછે\nછો\nહતા\nહતું\nહતી\nહોય\nહતો\nશકે\nતે\nતેના\nતેનું\nતેને\nતેની\nતેઓ\nતેમને\nતેમના\nતેમણે\nતેમનું\nતેમાં\nઅને\nઅહીં\nથી\nથઈ\nથાય\nજે\n ને\nકે\nના\nની\nનો\nને\nનું\nશું\nમાં\nપણ\nપર\nજેવા\nજેવું\nજાય\nજેમ\nજેથી\nમાત્ર\nમાટે\nપરથી\nઆવ્યું\nએવી\nઆવી\nરીતે\nસુધી\nથાય\nથઈ\nસાથે\nલાગે\nહોવા\nછતાં\nરહેલા\nકરી\nકરે\nકેટલા\nકોઈ\nકેમ\nકર્યો\nકર્યુ\nકરે\nસૌથી\nત્યારબાદ\nતથા\nદ્વારા\nજુઓ\nજાઓ\nજ્યારે\nત્યારે\nશકો\nનથી\nહવે\nઅથવા\nથતો\nદર\nએટલો\nપરંતુ\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/pl/__init__.py----------------------------------------
spacy.lang.pl.__init__.Polish(Language)
spacy.lang.pl.__init__.PolishDefaults(BaseDefaults)
spacy.lang.pl.__init__.make_lemmatizer(nlp:Language,model:Optional[Model],name:str,mode:str,overwrite:bool,scorer:Optional[Callable])


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/pl/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/pl/lemmatizer.py----------------------------------------
A:spacy.lang.pl.lemmatizer.morphology->token.morph.to_dict()
A:spacy.lang.pl.lemmatizer.lookup_pos->univ_pos.lower()
A:spacy.lang.pl.lemmatizer.lookup_table->self.lookups.get_table('lemma_lookup_' + lookup_pos, {})
A:spacy.lang.pl.lemmatizer.string->string.lower().lower()
spacy.lang.pl.PolishLemmatizer(Lemmatizer)
spacy.lang.pl.PolishLemmatizer.get_lookups_config(cls,mode:str)->Tuple[List[str], List[str]]
spacy.lang.pl.PolishLemmatizer.lemmatize_adj(self,string:str,morphology:dict,lookup_table:Dict[str,str])->List[str]
spacy.lang.pl.PolishLemmatizer.lemmatize_noun(self,string:str,morphology:dict,lookup_table:Dict[str,str])->List[str]
spacy.lang.pl.PolishLemmatizer.lemmatize_verb(self,string:str,morphology:dict,lookup_table:Dict[str,str])->List[str]
spacy.lang.pl.PolishLemmatizer.pos_lookup_lemmatize(self,token:Token)->List[str]
spacy.lang.pl.lemmatizer.PolishLemmatizer(Lemmatizer)
spacy.lang.pl.lemmatizer.PolishLemmatizer.get_lookups_config(cls,mode:str)->Tuple[List[str], List[str]]
spacy.lang.pl.lemmatizer.PolishLemmatizer.lemmatize_adj(self,string:str,morphology:dict,lookup_table:Dict[str,str])->List[str]
spacy.lang.pl.lemmatizer.PolishLemmatizer.lemmatize_noun(self,string:str,morphology:dict,lookup_table:Dict[str,str])->List[str]
spacy.lang.pl.lemmatizer.PolishLemmatizer.lemmatize_verb(self,string:str,morphology:dict,lookup_table:Dict[str,str])->List[str]
spacy.lang.pl.lemmatizer.PolishLemmatizer.pos_lookup_lemmatize(self,token:Token)->List[str]


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/pl/lex_attrs.py----------------------------------------
A:spacy.lang.pl.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.pl.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.pl.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/pl/stop_words.py----------------------------------------
A:spacy.lang.pl.stop_words.STOP_WORDS->set('\na aby ach acz aczkolwiek aj albo ale alez\należ ani az aż\n\nbardziej bardzo beda bede bedzie bez bo bowiem by\nbyc byl byla byli bylo byly bym bynajmniej być był\nbyła było były będzie będą będę\n\ncala cali caly cała cały chce choć ci cie\nciebie cię co cokolwiek coraz cos coś czasami czasem czemu\nczy czyli często\n\ndaleko dla dlaczego dlatego do dobrze dokad dokąd\ndosc dość duzo dużo dwa dwaj dwie dwoje dzis\ndzisiaj dziś\n\ngdy gdyby gdyz gdyż gdzie gdziekolwiek gdzies gdzieś go\ngodz\n\ni ich ile im inna inne inny\ninnych iv ix iz iż\n\nja jak jakas jakaś jakby jaki jakichs jakichś jakie\njakis jakiz jakiś jakiż jakkolwiek jako jakos jakoś je jeden\njedna jednak jednakze jednakże jedno jednym jedynie jego jej jemu\njesli jest jestem jeszcze jezeli jeśli jeżeli juz już ją\n\nkazdy każdy kiedy kierunku kilka kilku kims kimś kto\nktokolwiek ktora ktore ktorego ktorej ktory ktorych ktorym ktorzy ktos\nktoś która które którego której który których którym którzy ku\n\nlecz lub\n\nma mają mam mamy mało mi miał miedzy\nmimo między mna mnie mną moga mogą moi moim moj\nmoja moje moze mozliwe mozna może możliwe można mu musi\nmy mój\n\nna nad nam nami nas nasi nasz nasza nasze\nnaszego naszych natomiast natychmiast nawet nia nic nich nie niech\nniego niej niemu nigdy nim nimi niz nią niż no\n\no obok od ok około on ona one\noni ono oraz oto owszem\n\npan pana pani po pod podczas pomimo ponad\nponiewaz ponieważ powinien powinna powinni powinno poza prawie przeciez\nprzecież przed przede przedtem przez przy\n\nraz razie roku rowniez również\n\nsam sama sie się skad skąd soba sobie sobą\nsposob sposób swoje są\n\nta tak taka taki takich takie takze także tam\nte tego tej tel temu ten teraz też to toba\ntobie tobą totez toteż totobą trzeba tu tutaj twoi twoim\ntwoj twoja twoje twym twój ty tych tylko tym tys\ntzw tę\n\nu\n\nvi vii viii\n\nw wam wami was wasi wasz wasza wasze we\nwedług wie wiele wielu więc więcej wlasnie wszyscy wszystkich wszystkie\nwszystkim wszystko wtedy wy właśnie wśród\n\nxi xii xiii xiv xv\n\nz za zaden zadna zadne zadnych zapewne zawsze zaś\nze zeby znow znowu znów zostal został\n\nżaden żadna żadne żadnych że żeby'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/pl/punctuation.py----------------------------------------
A:spacy.lang.pl.punctuation._quotes->char_classes.CONCAT_QUOTES.replace("'", '')


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/az/__init__.py----------------------------------------
spacy.lang.az.__init__.Azerbaijani(Language)
spacy.lang.az.__init__.AzerbaijaniDefaults(BaseDefaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/az/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/az/lex_attrs.py----------------------------------------
A:spacy.lang.az.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.az.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
A:spacy.lang.az.lex_attrs.text_lower->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').lower()
spacy.lang.az.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/az/stop_words.py----------------------------------------
A:spacy.lang.az.stop_words.STOP_WORDS->set('\namma\narasında\nartıq\nay\naz\nbax\nbelə\nbeş\nbilər\nbir\nbiraz\nbiri\nbirşey\nbiz\nbizim\nbizlər\nbu\nbuna\nbundan\nbunların\nbunu\nbunun\nburadan\nbütün\nbəli\nbəlkə\nbəy\nbəzi\nbəzən\ndaha\ndedi\ndeyil\ndir\ndüz\ndə\ndək\ndən\ndəqiqə\nedir\nedən\nelə\net\netdi\netmə\netmək\nfaiz\ngilə\ngörə\nha\nhaqqında\nharada\nheç\nhə\nhəm\nhəmin\nhəmişə\nhər\nidi\nil\nildə\nilk\nilə\nin\nindi\nistifadə\nisə\nki\nkim\nkimi\nkimə\nlakin\nlap\nmirşey\nməhz\nmən\nmənə\nniyə\nnə\nnəhayət\no\nobirisi\nof\nolan\nolar\nolaraq\noldu\nolduğu\nolmadı\nolmaz\nolmuşdur\nolsun\nolur\non\nona\nondan\nonlar\nonlardan\nonların\nonsuzda\nonu\nonun\noradan\nqarşı\nqədər\nsaat\nsadəcə\nsaniyə\nsiz\nsizin\nsizlər\nsonra\nsəhv\nsən\nsənin\nsənə\ntəəssüf\nvar\nvə\nxan\nxanım\nxeyr\nya\nyalnız\nyaxşı\nyeddi\nyenə\nyox\nyoxdur\nyoxsa\nyəni\nzaman\nçox\nçünki\nöz\nözü\nüçün\nəgər\nəlbəttə\nən\nəslində\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/th/__init__.py----------------------------------------
A:spacy.lang.th.__init__.words->list(self.word_tokenize(text))
A:spacy.lang.th.__init__.config->load_config_from_str(DEFAULT_CONFIG)
spacy.lang.th.__init__.Thai(Language)
spacy.lang.th.__init__.ThaiDefaults(BaseDefaults)
spacy.lang.th.__init__.ThaiTokenizer(self,vocab:Vocab)
spacy.lang.th.__init__.ThaiTokenizer.__init__(self,vocab:Vocab)
spacy.lang.th.__init__.create_thai_tokenizer()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/th/lex_attrs.py----------------------------------------
A:spacy.lang.th.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.th.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.th.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/th/tokenizer_exceptions.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/th/stop_words.py----------------------------------------
A:spacy.lang.th.stop_words.STOP_WORDS->set('\nทั้งนี้ ดัง ขอ รวม หลังจาก เป็น หลัง หรือ ๆ เกี่ยวกับ ซึ่งได้แก่ ด้วยเพราะ ด้วยว่า ด้วยเหตุเพราะ\nด้วยเหตุว่า สุดๆ เสร็จแล้ว เช่น เข้า ถ้า ถูก ถึง ต่างๆ ใคร เปิดเผย ครา รือ ตาม ใน ได้แก่ ได้แต่\nได้ที่ ตลอดถึง นอกจากว่า นอกนั้น จริง อย่างดี ส่วน เพียงเพื่อ เดียว จัด ทั้งที ทั้งคน ทั้งตัว ไกลๆ\nถึงเมื่อใด คงจะ ถูกๆ เป็นที นับแต่ที่ นับแต่นั้น รับรอง ด้าน เป็นต้นมา ทุก กระทั่ง กระทำ จวบ ซึ่งก็ จะ\nครบครัน นับแต่ เยอะๆ เพียงไหน เปลี่ยนแปลง ไป่ ผ่านๆ เพื่อที่ รวมๆ กว้างขวาง เสียยิ่ง เปลี่ยน ผ่าน\nทรง ทว่า กันเถอะ เกี่ยวๆ ใดๆ ครั้งที่ ครั้งนั้น ครั้งนี้ ครั้งละ ครั้งหลัง ครั้งหลังสุด ร่วมกัน ร่วมด้วย ก็ตามที\nที่สุด ผิดๆ ยืนยง เยอะ ครั้งๆ ใครๆ นั่นเอง เสมือนว่า เสร็จ ตลอดศก ทั้งที่ ยืนยัน ด้วยที่ บัดนี้\nด้วยประการฉะนี้ ซึ่งกัน ตลอดทั่วถึง ตลอดทั่วทั้ง ตลอดปี เป็นการ นั่นแหละ พร้อม เถิด ทั้ง สืบเนื่อง ตั้งแต่\nกลับ กล่าวคือ กลุ่มก้อน กลุ่มๆ ครั้งครา ส่ง รวดเร็ว เสร็จสิ้น เสีย เสียก่อน เสียจน อดีต ตั้ง เกิด อาจ\nอีก ตลอดเวลา ภายหน้า ภายหลัง มอง มันๆ มองว่า มัก มักจะ มัน หาก คงอยู่ เป็นที่ เป็นที่สุด\nเป็นเพราะเป็นเพราะว่า เกี่ยวกัน เพียงไร เป็นแต่เพียง กล่าว จนบัดนี้ เป็นอัน จน จนเมื่อ จนแม้ ใกล้\nใหม่ๆ เป็นเพียง อย่างที่ ถูกต้อง ทั้งนั้น ทั้งนั้นด้วย กันดีกว่า กันดีไหม นั่นไง ตรงๆ แยะๆ เป็นต้น ใกล้ๆ\nซึ่งๆ ด้วยกัน ดังเคย เถอะ เสมือนกับ ไป คือ ขณะนี้ นอกจาก เพื่อที่จะ ขณะหนึ่ง ขวาง ครัน อยาก ไว้\nแบบ นอกจากนี้ เนื่องจาก เดียวกัน คง ให้มา อนึ่ง ก็แล้วแต่ ต้อง ข้าง เพื่อว่า จนแม้น ครั้งหนึ่ง อะไร ซึ่ง\nเกินๆ ด้วยเหตุนั้น กันและกัน รับ ระหว่าง ครั้งไหน เสร็จกัน ถึงอย่างไร ขาด ข้าฯ เข้าใจ ครบ ครั้งใด\nครบถ้วน ระยะ ไม่ เกือบ เกือบจะ เกือบๆ แก่ แก อย่างโน้น ดังกับว่า จริงจัง เยอะแยะ นั่น ด้วย ถึงแม้ว่า\nมาก ตลอดกาลนาน ตลอดระยะเวลา ตลอดจน ตลอดไป เป็นอันๆ เป็นอาทิ ก็ต่อเมื่อ สู่ เมื่อ เพื่อ ก็ กับ\nด้วยเหมือนกัน ด้วยเหตุนี้ ครั้งคราว ราย ร่วม เป็นอันมาก สูง รวมกัน รวมทั้ง ร่วมมือ เป็นเพียงว่า รวมถึง\nต่อ นะ กว้าง มา ครับ ตลอดทั้ง การ นั้นๆ น่า เป็นอันว่า เพราะ วัน จนขณะนี้ จนตลอด จนถึง ข้า อย่างใด\nไหนๆ ก่อนหน้านี้ ก่อนๆ สูงกว่า สูงส่ง สูงสุด สูงๆ เสียด้วย เสียนั่น เสียนี่ เสียนี่กระไร เสียนั่นเอง สุด\nสําหรับ ว่า ลง ภายใต้ เพื่อให้ ภายนอก ภายใน เฉพาะ ซึ่งกันและกัน ง่าย ง่ายๆ ไง ถึงแม้จะ ถึงเมื่อไร\nเกิน ก็ได้ คราใด คราที่ ตลอดวัน นับ ดังเก่า ดั่งเก่า หลาย หนึ่ง ถือว่า ก่อนหน้า นับตั้งแต่ จรด จริงๆ\nจวน จวนเจียน ตลอดมา กลุ่ม กระนั้น ข้างๆ ตรง ข้าพเจ้า กว่า เกี่ยวเนื่อง ขึ้น ให้ไป ผล แต่ เอง เห็น\nจึง ได้ ให้ โดย จริงๆจังๆ ดั่งกับว่า ทั้งนั้นเพราะ นอก นอกเหนือ น่ะ กันนะ ขณะเดียวกัน แยะ\nนอกเหนือจาก น้อย ก่อน จวนจะ ข้างเคียง ก็ตามแต่ จรดกับ น้อยกว่า นั่นเป็น นักๆ ครั้งกระนั้น เลย ไกล\nสิ้นกาลนาน ครั้ง รือว่า เก็บ อย่างเช่น บาง ดั่ง ดังกล่าว ดังกับ รึ รึว่า ออก แรก จง ยืนนาน ได้มา ตน\nตนเอง ได้รับ ระยะๆ กระผม กันไหม กันเอง กำลังจะ กำหนด กู กำลัง ความ แล้ว และ ต่าง อย่างน้อย\nอย่างนั้น อย่างนี้ ก็คือ ก็แค่ ด้วยเหตุที่ ใหญ่ๆ ให้ดี ยัง เป็นเพื่อ ก็ตาม ผู้ ต่อกัน ถือ ซึ่งก็คือ ภายภาค\nภายภาคหน้า ก็ดี ก็จะ อยู่ เสียยิ่งนัก ใหม่ ขณะ เริ่ม เรา ขวางๆ เสียแล้ว ใคร่ ใคร่จะ ตนฯ ของ แห่ง\nรวด ดั่งกับ ถึงเมื่อ น้อยๆ นับจากนั้น ตลอด ตลอดกาล เสร็จสมบูรณ์ เขียน กว้างๆ ยืนยาว ถึงแก่ ขณะใด\nขณะใดๆ ขณะที่ ขณะนั้น จนทั่ว ภาคฯ ภาย เป็นแต่ อย่าง พบ ภาค ให้แด่ เสียจนกระทั่ง เสียจนถึง\nจนกระทั่ง จนกว่า ตลอดทั่ว เป็นๆ นอกจากนั้น ผิด ครั้งก่อน แก้ไข ขั้น กัน ช่วง จาก รวมด้วย เขา\nด้วยเช่นกัน นอกจากที่ เป็นต้นไป ข้างต้น ข้างบน ข้างล่าง ถึงจะ ถึงบัดนั้น ถึงแม้ มี ทาง เคย นับจากนี้\nอย่างเดียว เกี่ยวข้อง นี้ นํา นั้น ที่ ทําให้ ทํา ครานั้น ครานี้ คราหนึ่ง คราไหน คราว คราวก่อน คราวใด\nคราวที่ คราวนั้น คราวนี้ คราวโน้น คราวละ คราวหน้า คราวหนึ่ง คราวหลัง คราวไหน คราวๆ คล้าย\nคล้ายกัน คล้ายกันกับ คล้ายกับ คล้ายกับว่า คล้ายว่า ควร ค่อน ค่อนข้าง ค่อนข้างจะ ค่อยไปทาง ค่อนมาทาง ค\n่อย ค่อยๆ คะ ค่ะ คำ คิด คิดว่า คุณ คุณๆ เคยๆ แค่ แค่จะ แค่นั้น แค่นี้ แค่เพียง แค่ว่า แค่ไหน จังๆ\nจวบกับ จวบจน จ้ะ จ๊ะ จะได้ จัง จัดการ จัดงาน จัดแจง จัดตั้ง จัดทำ จัดหา จัดให้ จับ จ้า จ๋า จากนั้น\nจากนี้ จากนี้ไป จำ จำเป็น จำพวก จึงจะ จึงเป็น จู่ๆ ฉะนั้น ฉะนี้ ฉัน เฉกเช่น เฉย เฉยๆ ไฉน ช่วงก่อน ช\n่วงต่อไป ช่วงถัดไป ช่วงท้าย ช่วงที่ ช่วงนั้น ช่วงนี้ ช่วงระหว่าง ช่วงแรก ช่วงหน้า ช่วงหลัง ช่วงๆ ช่วย ช้า\nช้านาน ชาว ช้าๆ เช่นก่อน เช่นกัน เช่นเคย เช่นดัง เช่นดังก่อน เช่นดังเก่า เช่นดังที่ เช่นดังว่า\nเช่นเดียวกัน เช่นเดียวกับ เช่นใด เช่นที่ เช่นที่เคย เช่นที่ว่า เช่นนั้น เช่นนั้นเอง เช่นนี้ เช่นเมื่อ เช่นไร\nเชื่อ เชื่อถือ เชื่อมั่น เชื่อว่า ใช่ ใช้ ซะ ซะก่อน ซะจน ซะจนกระทั่ง ซะจนถึง ดั่งเคย ต่างก็ ต่างหาก\nตามด้วย ตามแต่ ตามที่ ตามๆ เต็มไปด้วย เต็มไปหมด เต็มๆ แต่ก็ แต่ก่อน แต่จะ แต่เดิม แต่ต้อง แต่ถ้า\nแต่ทว่า แต่ที่ แต่นั้น แต่เพียง แต่เมื่อ แต่ไร แต่ละ แต่ว่า แต่ไหน แต่อย่างใด โต โตๆ ใต้ ถ้าจะ ถ้าหาก\nทั้งปวง ทั้งเป็น ทั้งมวล ทั้งสิ้น ทั้งหมด ทั้งหลาย ทั้งๆ ทัน ทันใดนั้น ทันที ทันทีทันใด ทั่ว ทำให้ ทำๆ ที ที่จริง\nที่ซึ่ง ทีเดียว ทีใด ที่ใด ที่ได้ ทีเถอะ ที่แท้ ที่แท้จริง ที่นั้น ที่นี้ ทีไร ทีละ ที่ละ ที่แล้ว ที่ว่า ที่แห่งนั้น ทีๆ ที่ๆ\nทุกคน ทุกครั้ง ทุกครา ทุกคราว ทุกชิ้น ทุกตัว ทุกทาง ทุกที ทุกที่ ทุกเมื่อ ทุกวัน ทุกวันนี้ ทุกสิ่ง ทุกหน ทุกแห่ง\nทุกอย่าง ทุกอัน ทุกๆ เท่า เท่ากัน เท่ากับ เท่าใด เท่าที่ เท่านั้น เท่านี้ แท้ แท้จริง เธอ นั้นไว นับแต่นี้\nนาง นางสาว น่าจะ นาน นานๆ นาย นำ นำพา นำมา นิด นิดหน่อย นิดๆ นี่ นี่ไง นี่นา นี่แน่ะ นี่แหละ นี้แหล่\nนี่เอง นี้เอง นู่น นู้น เน้น เนี่ย เนี่ยเอง ในช่วง ในที่ ในเมื่อ ในระหว่าง บน บอก บอกแล้ว บอกว่า บ่อย\nบ่อยกว่า บ่อยครั้ง บ่อยๆ บัดดล บัดเดี๋ยวนี้ บัดนั้น บ้าง บางกว่า บางขณะ บางครั้ง บางครา บางคราว\nบางที บางที่ บางแห่ง บางๆ ปฏิบัติ ประกอบ ประการ ประการฉะนี้ ประการใด ประการหนึ่ง ประมาณ\nประสบ ปรับ ปรากฏ ปรากฏว่า ปัจจุบัน ปิด เป็นด้วย เป็นดัง ผู้ใด เผื่อ เผื่อจะ เผื่อที่ เผื่อว่า ฝ่าย ฝ่ายใด\nพบว่า พยายาม พร้อมกัน พร้อมกับ พร้อมด้วย พร้อมทั้ง พร้อมที่ พร้อมเพียง พวก พวกกัน พวกกู พวกแก\nพวกเขา พวกคุณ พวกฉัน พวกท่าน พวกที่ พวกเธอ พวกนั้น พวกนี้ พวกนู้น พวกโน้น พวกมัน พวกมึง พอ พอกัน\nพอควร พอจะ พอดี พอตัว พอที พอที่ พอเพียง พอแล้ว พอสม พอสมควร พอเหมาะ พอๆ พา พึง พึ่ง พื้นๆ พูด\nเพราะฉะนั้น เพราะว่า เพิ่ง เพิ่งจะ เพิ่ม เพิ่มเติม เพียง เพียงแค่ เพียงใด เพียงแต่ เพียงพอ เพียงเพราะ\nมากกว่า มากมาย มิ มิฉะนั้น มิใช่ มิได้ มีแต่ มึง มุ่ง มุ่งเน้น มุ่งหมาย เมื่อก่อน เมื่อครั้ง เมื่อครั้งก่อน\nเมื่อคราวก่อน เมื่อคราวที่ เมื่อคราว เมื่อคืน เมื่อเช้า เมื่อใด เมื่อนั้น เมื่อนี้ เมื่อเย็น เมื่อวันวาน เมื่อวาน\nแม้ แม้กระทั่ง แม้แต่ แม้นว่า แม้ว่า ไม่ค่อย ไม่ค่อยจะ ไม่ค่อยเป็น ไม่ใช่ ไม่เป็นไร ไม่ว่า ยก ยกให้ ยอม\nยอมรับ ย่อม ย่อย ยังคง ยังงั้น ยังงี้ ยังโง้น ยังไง ยังจะ ยังแต่ ยาก ยาว ยาวนาน ยิ่ง ยิ่งกว่า ยิ่งขึ้น\nยิ่งขึ้นไป ยิ่งจน ยิ่งจะ ยิ่งนัก ยิ่งเมื่อ ยิ่งแล้ว ยิ่งใหญ่ เร็ว เร็วๆ เราๆ เรียก เรียบ เรื่อย เรื่อยๆ ล้วน\nล้วนจน ล้วนแต่ ละ ล่าสุด เล็ก เล็กน้อย เล็กๆ เล่าว่า แล้วกัน แล้วแต่ แล้วเสร็จ วันใด วันนั้น วันนี้ วันไหน\nสบาย สมัย สมัยก่อน สมัยนั้น สมัยนี้ สมัยโน้น ส่วนเกิน ส่วนด้อย ส่วนดี ส่วนใด ส่วนที่ ส่วนน้อย ส่วนนั้น ส\n่วนมาก ส่วนใหญ่ สั้น สั้นๆ สามารถ สำคัญ สิ่ง สิ่งใด สิ่งนั้น สิ่งนี้ สิ่งไหน สิ้น แสดง แสดงว่า หน หนอ หนอย\nหน่อย หมด หมดกัน หมดสิ้น หากแม้ หากแม้น หากแม้นว่า หากว่า หาความ หาใช่ หารือ เหตุ เหตุผล เหตุนั้น\nเหตุนี้ เหตุไร เห็นแก่ เห็นควร เห็นจะ เห็นว่า เหลือ เหลือเกิน เหล่า เหล่านั้น เหล่านี้ แห่งใด แห่งนั้น\nแห่งนี้ แห่งโน้น แห่งไหน แหละ ให้แก่ ใหญ่ ใหญ่โต อย่างมาก อย่างยิ่ง อย่างไรก็ อย่างไรก็ได้ อย่างไรเสีย\nอย่างละ อย่างหนึ่ง อย่างๆ อัน อันจะ อันได้แก่ อันที่ อันที่จริง อันที่จะ อันเนื่องมาจาก อันละ อันๆ อาจจะ\nอาจเป็น อาจเป็นด้วย อื่น อื่นๆ เอ็ง เอา ฯ ฯล ฯลฯ 555 กำ ขอโทษ เยี่ยม นี่คือ\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/tr/__init__.py----------------------------------------
spacy.lang.tr.__init__.Turkish(Language)
spacy.lang.tr.__init__.TurkishDefaults(BaseDefaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/tr/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/tr/lex_attrs.py----------------------------------------
A:spacy.lang.tr.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.tr.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
A:spacy.lang.tr.lex_attrs.text_lower->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').lower()
spacy.lang.tr.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/tr/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.tr.tokenizer_exceptions._dash_num->'(([{al}\\d]+/\\d+)|(\\d+/[{al}]))'.format(al=ALPHA)
A:spacy.lang.tr.tokenizer_exceptions._roman_ord->'({rn})\\.'.format(rn=_roman_num)
A:spacy.lang.tr.tokenizer_exceptions._inflections->"'[{al}]+".format(al=ALPHA_LOWER)
A:spacy.lang.tr.tokenizer_exceptions._abbrev_inflected->"[{a}]+\\.'[{al}]+".format(a=ALPHA, al=ALPHA_LOWER)
A:spacy.lang.tr.tokenizer_exceptions._nums->'(({d})|({dn})|({te})|({on})|({n})|({ro})|({rn}))({inf})?'.format(d=_date, dn=_dash_num, te=_time_exp, on=_ord_num, n=_num, ro=_roman_ord, rn=_roman_num, inf=_inflections)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/tr/syntax_iterators.py----------------------------------------
A:spacy.lang.tr.syntax_iterators.conj->doc.vocab.strings.add('conj')
A:spacy.lang.tr.syntax_iterators.flat->doc.vocab.strings.add('flat')
A:spacy.lang.tr.syntax_iterators.np_label->doc.vocab.strings.add('NP')
spacy.lang.tr.syntax_iterators.noun_chunks(doclike:Union[Doc,Span])->Iterator[Tuple[int, int, int]]


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/tr/stop_words.py----------------------------------------
A:spacy.lang.tr.stop_words.STOP_WORDS->set('\nacaba\nacep\nadamakıllı\nadeta\nait\nama\namma\nanca\nancak\narada\nartık\naslında\naynen\nayrıca\naz\naçıkça\naçıkçası\nbana\nbari\nbazen\nbazı\nbazısı\nbazısına\nbazısında\nbazısından\nbazısını\nbazısının\nbaşkası\nbaşkasına\nbaşkasında\nbaşkasından\nbaşkasını\nbaşkasının\nbaşka\nbelki\nben\nbende\nbenden\nbeni\nbenim\nberi\nberiki\nberikinin\nberikiyi\nberisi\nbilcümle\nbile\nbinaen\nbinaenaleyh\nbiraz\nbirazdan\nbirbiri\nbirbirine\nbirbirini\nbirbirinin\nbirbirinde\nbirbirinden\nbirden\nbirdenbire\nbiri\nbirine\nbirini\nbirinin\nbirinde\nbirinden\nbirice\nbirileri\nbirilerinde\nbirilerinden\nbirilerine\nbirilerini\nbirilerinin\nbirisi\nbirisine\nbirisini\nbirisinin\nbirisinde\nbirisinden\nbirkaç\nbirkaçı\nbirkaçına\nbirkaçını\nbirkaçının\nbirkaçında\nbirkaçından\nbirkez\nbirlikte\nbirçok\nbirçoğu\nbirçoğuna\nbirçoğunda\nbirçoğundan\nbirçoğunu\nbirçoğunun\nbirşey\nbirşeyi\nbitevi\nbiteviye\nbittabi\nbiz\nbizatihi\nbizce\nbizcileyin\nbizden\nbize\nbizi\nbizim\nbizimki\nbizzat\nboşuna\nbu\nbuna\nbunda\nbundan\nbunlar\nbunları\nbunların\nbunu\nbunun\nburacıkta\nburada\nburadan\nburası\nburasına\nburasını\nburasının\nburasında\nburasından\nböyle\nböylece\nböylecene\nböylelikle\nböylemesine\nböylesine\nbüsbütün\nbütün\ncuk\ncümlesi\ncümlesine\ncümlesini\ncümlesinin\ncümlesinden\ncümlemize\ncümlemizi\ncümlemizden\nçabuk\nçabukça\nçeşitli\nçok\nçokları\nçoklarınca\nçokluk\nçoklukla\nçokça\nçoğu\nçoğun\nçoğunca\nçoğunda\nçoğundan\nçoğunlukla\nçoğunu\nçoğunun\nçünkü\nda\ndaha\ndahası\ndahi\ndahil\ndahilen\ndaima\ndair\ndayanarak\nde\ndefa\ndek\ndemin\ndemincek\ndeminden\ndenli\nderakap\nderhal\nderken\ndeğil\ndeğin\ndiye\ndiğer\ndiğeri\ndiğerine\ndiğerini\ndiğerinden\ndolayı\ndolayısıyla\ndoğru\nedecek\neden\nederek\nedilecek\nediliyor\nedilmesi\nediyor\nelbet\nelbette\nemme\nen\nenikonu\nepey\nepeyce\nepeyi\nesasen\nesnasında\netmesi\netraflı\netraflıca\netti\nettiği\nettiğini\nevleviyetle\nevvel\nevvela\nevvelce\nevvelden\nevvelemirde\nevveli\neğer\nfakat\nfilanca\nfilancanın\ngah\ngayet\ngayetle\ngayri\ngayrı\ngelgelelim\ngene\ngerek\ngerçi\ngeçende\ngeçenlerde\ngibi\ngibilerden\ngibisinden\ngine\ngöre\ngırla\nhakeza\nhalbuki\nhalen\nhalihazırda\nhaliyle\nhandiyse\nhangi\nhangisi\nhangisine\nhangisine\nhangisinde\nhangisinden\nhani\nhariç\nhasebiyle\nhasılı\nhatta\nhele\nhem\nhenüz\nhep\nhepsi\nhepsini\nhepsinin\nhepsinde\nhepsinden\nher\nherhangi\nherkes\nherkesi\nherkesin\nherkesten\nhiç\nhiçbir\nhiçbiri\nhiçbirine\nhiçbirini\nhiçbirinin\nhiçbirinde\nhiçbirinden\nhoş\nhulasaten\niken\nila\nile\nilen\nilgili\nilk\nilla\nillaki\nimdi\nindinde\ninen\ninsermi\nise\nister\nitibaren\nitibariyle\nitibarıyla\niyi\niyice\niyicene\niçin\niş\nişte\nkadar\nkaffesi\nkah\nkala\nkanımca\nkarşın\nkaynak\nkaçı\nkaçına\nkaçında\nkaçından\nkaçını\nkaçının\nkelli\nkendi\nkendilerinde\nkendilerinden\nkendilerine\nkendilerini\nkendilerinin\nkendini\nkendisi\nkendisinde\nkendisinden\nkendisine\nkendisini\nkendisinin\nkere\nkez\nkeza\nkezalik\nkeşke\nki\nkim\nkimden\nkime\nkimi\nkiminin\nkimisi\nkimisinde\nkimisinden\nkimisine\nkimisinin\nkimse\nkimsecik\nkimsecikler\nkülliyen\nkısaca\nkısacası\nlakin\nleh\nlütfen\nmaada\nmadem\nmademki\nmamafih\nmebni\nmeđer\nmeğer\nmeğerki\nmeğerse\nmu\nmü\nmı\nmi\nnasıl\nnasılsa\nnazaran\nnaşi\nne\nneden\nnedeniyle\nnedenle\nnedenler\nnedenlerden\nnedense\nnerde\nnerden\nnerdeyse\nnere\nnerede\nnereden\nneredeyse\nneresi\nnereye\nnetekim\nneye\nneyi\nneyse\nnice\nnihayet\nnihayetinde\nnitekim\nniye\nniçin\no\nolan\nolarak\noldu\nolduklarını\noldukça\nolduğu\nolduğunu\nolmak\nolması\nolsa\nolsun\nolup\nolur\nolursa\noluyor\nona\nonca\nonculayın\nonda\nondan\nonlar\nonlara\nonlardan\nonları\nonların\nonu\nonun\nora\noracık\noracıkta\norada\noradan\noranca\noranla\noraya\noysa\noysaki\nöbür\nöbürkü\nöbürü\nöbüründe\nöbüründen\nöbürüne\nöbürünü\nönce\nönceden\nönceleri\nöncelikle\nöteki\nötekisi\nöyle\nöylece\nöylelikle\nöylemesine\nöz\npek\npekala\npeki\npekçe\npeyderpey\nrağmen\nsadece\nsahi\nsahiden\nsana\nsanki\nsen\nsenden\nseni\nsenin\nsiz\nsizden\nsizi\nsizin\nsonra\nsonradan\nsonraları\nsonunda\nşayet\nşey\nşeyden\nşeyi\nşeyler\nşu\nşuna\nşuncacık\nşunda\nşundan\nşunlar\nşunları\nşunların\nşunu\nşunun\nşura\nşuracık\nşuracıkta\nşurası\nşöyle\nşimdi\ntabii\ntam\ntamam\ntamamen\ntamamıyla\ntarafından\ntek\ntüm\nüzere\nvar\nvardı\nvasıtasıyla\nve\nvelev\nvelhasıl\nvelhasılıkelam\nveya\nveyahut\nya\nyahut\nyakinen\nyakında\nyakından\nyakınlarda\nyalnız\nyalnızca\nyani\nyapacak\nyapmak\nyaptı\nyaptıkları\nyaptığı\nyaptığını\nyapılan\nyapılması\nyapıyor\nyeniden\nyenilerde\nyerine\nyine\nyok\nyoksa\nyoluyla\nyüzünden\nzarfında\nzaten\nzati\nzira\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ta/__init__.py----------------------------------------
spacy.lang.ta.__init__.Tamil(Language)
spacy.lang.ta.__init__.TamilDefaults(BaseDefaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ta/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ta/lex_attrs.py----------------------------------------
A:spacy.lang.ta.lex_attrs.length->len(num_suffix)
A:spacy.lang.ta.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.ta.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.ta.lex_attrs.like_num(text)
spacy.lang.ta.lex_attrs.suffix_filter(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ta/stop_words.py----------------------------------------
A:spacy.lang.ta.stop_words.STOP_WORDS->set('\nஒரு\nஎன்று\nமற்றும்\nஇந்த\nஇது\nஎன்ற\nகொண்டு\nஎன்பது\nபல\nஆகும்\nஅல்லது\nஅவர்\nநான்\nஉள்ள\nஅந்த\nஇவர்\nஎன\nமுதல்\nஎன்ன\nஇருந்து\nசில\nஎன்\nபோன்ற\nவேண்டும்\nவந்து\nஇதன்\nஅது\nஅவன்\nதான்\nபலரும்\nஎன்னும்\nமேலும்\nபின்னர்\nகொண்ட\nஇருக்கும்\nதனது\nஉள்ளது\nபோது\nஎன்றும்\nஅதன்\nதன்\nபிறகு\nஅவர்கள்\nவரை\nஅவள்\nநீ\nஆகிய\nஇருந்தது\nஉள்ளன\nவந்த\nஇருந்த\nமிகவும்\nஇங்கு\nமீது\nஓர்\nஇவை\nஇந்தக்\nபற்றி\nவரும்\nவேறு\nஇரு\nஇதில்\nபோல்\nஇப்போது\nஅவரது\nமட்டும்\nஇந்தப்\nஎனும்\nமேல்\nபின்\nசேர்ந்த\nஆகியோர்\nஎனக்கு\nஇன்னும்\nஅந்தப்\nஅன்று\nஒரே\nமிக\nஅங்கு\nபல்வேறு\nவிட்டு\nபெரும்\nஅதை\nபற்றிய\nஉன்\nஅதிக\nஅந்தக்\nபேர்\nஇதனால்\nஅவை\nஅதே\nஏன்\nமுறை\nயார்\nஎன்பதை\nஎல்லாம்\nமட்டுமே\nஇங்கே\nஅங்கே\nஇடம்\nஇடத்தில்\nஅதில்\nநாம்\nஅதற்கு\nஎனவே\nபிற\nசிறு\nமற்ற\nவிட\nஎந்த\nஎனவும்\nஎனப்படும்\nஎனினும்\nஅடுத்த\nஇதனை\nஇதை\nகொள்ள\nஇந்தத்\nஇதற்கு\nஅதனால்\nதவிர\nபோல\nவரையில்\nசற்று\nஎனக்\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/yo/__init__.py----------------------------------------
spacy.lang.yo.__init__.Yoruba(Language)
spacy.lang.yo.__init__.YorubaDefaults(BaseDefaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/yo/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/yo/lex_attrs.py----------------------------------------
A:spacy.lang.yo.lex_attrs.text->strip_accents_text(text)
spacy.lang.yo.lex_attrs.like_num(text)
spacy.lang.yo.lex_attrs.strip_accents_text(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/yo/stop_words.py----------------------------------------
A:spacy.lang.yo.stop_words.STOP_WORDS->set('a an b bá bí bẹ̀rẹ̀ d e f fún fẹ́ g gbogbo i inú j jù jẹ jẹ́ k kan kì kí kò l láti lè lọ m mi mo máa mọ̀ n ni náà ní nígbà nítorí nǹkan o p padà pé púpọ̀ pẹ̀lú r rẹ̀ s sì sí sínú t ti tí u w wà wá wọn wọ́n y yìí à àti àwọn á è é ì í ò òun ó ù ú ń ńlá ǹ ̀ ́ ̣ ṣ ṣe ṣé ṣùgbọ́n ẹ ẹmọ́ ọ ọjọ́ ọ̀pọ̀lọpọ̀'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ml/__init__.py----------------------------------------
spacy.lang.ml.__init__.Malayalam(Language)
spacy.lang.ml.__init__.MalayalamDefaults(BaseDefaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ml/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ml/lex_attrs.py----------------------------------------
A:spacy.lang.ml.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.ml.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.ml.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/ml/stop_words.py----------------------------------------
A:spacy.lang.ml.stop_words.STOP_WORDS->set('\nഅത്\nഇത്\nആയിരുന്നു\nആകുന്നു\nവരെ\nഅന്നേരം\nഅന്ന്\nഇന്ന്\nആണ്\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/hu/__init__.py----------------------------------------
spacy.lang.hu.__init__.Hungarian(Language)
spacy.lang.hu.__init__.HungarianDefaults(BaseDefaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/hu/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/hu/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.hu.tokenizer_exceptions._suffixes->'-[{al}]+'.format(al=ALPHA_LOWER)
A:spacy.lang.hu.tokenizer_exceptions._numeric_exp->'({n})(({o})({n}))*[%]?'.format(n=_num, o=_ops)
A:spacy.lang.hu.tokenizer_exceptions._nums->'(({ne})|({t})|({on})|({c}))({s})?'.format(ne=_numeric_exp, t=_time_exp, on=_ord_num_or_date, c=CURRENCY, s=_suffixes)
A:spacy.lang.hu.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/hu/stop_words.py----------------------------------------
A:spacy.lang.hu.stop_words.STOP_WORDS->set('\na abban ahhoz ahogy ahol aki akik akkor akár alatt amely amelyek amelyekben\namelyeket amelyet amelynek ami amikor amit amolyan amíg annak arra arról az\nazok azon azonban azt aztán azután azzal azért\n\nbe belül benne bár\n\ncikk cikkek cikkeket csak\n\nde\n\ne ebben eddig egy egyes egyetlen egyik egyre egyéb egész ehhez ekkor el ellen\nelo eloször elott elso elég előtt emilyen ennek erre ez ezek ezen ezt ezzel\nezért\n\nfel felé\n\nha hanem hiszen hogy hogyan hát\n\nide igen ill ill. illetve ilyen ilyenkor inkább is ismét ison itt\n\njobban jó jól\n\nkell kellett keressünk keresztül ki kívül között közül\n\nle legalább legyen lehet lehetett lenne lenni lesz lett\n\nma maga magát majd meg mellett mely melyek mert mi miatt mikor milyen minden\nmindenki mindent mindig mint mintha mit mivel miért mondta most már más másik\nmég míg\n\nnagy nagyobb nagyon ne nekem neki nem nincs néha néhány nélkül\n\no oda ok oket olyan ott\n\npedig persze például\n\nrá\n\ns saját sem semmi sok sokat sokkal stb. szemben szerint szinte számára szét\n\ntalán te tehát teljes ti tovább továbbá több túl ugyanis\n\nutolsó után utána\n\nvagy vagyis vagyok valaki valami valamint való van vannak vele vissza viszont\nvolna volt voltak voltam voltunk\n\náltal általában át\n\nén éppen és\n\nígy\n\nön össze\n\núgy új újabb újra\n\nő őket\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/hu/punctuation.py----------------------------------------
A:spacy.lang.hu.punctuation._concat_icons->char_classes.CONCAT_ICONS.replace('°', '')
A:spacy.lang.hu.punctuation._quotes->char_classes.CONCAT_QUOTES.replace("'", '')
A:spacy.lang.hu.punctuation._units->char_classes.UNITS.replace('%', '')


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/fr/__init__.py----------------------------------------
spacy.lang.fr.__init__.French(Language)
spacy.lang.fr.__init__.FrenchDefaults(BaseDefaults)
spacy.lang.fr.__init__.make_lemmatizer(nlp:Language,model:Optional[Model],name:str,mode:str,overwrite:bool,scorer:Optional[Callable])


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/fr/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/fr/lemmatizer.py----------------------------------------
A:spacy.lang.fr.lemmatizer.univ_pos->token.pos_.lower()
A:spacy.lang.fr.lemmatizer.index_table->self.lookups.get_table('lemma_index', {})
A:spacy.lang.fr.lemmatizer.exc_table->self.lookups.get_table('lemma_exc', {})
A:spacy.lang.fr.lemmatizer.rules_table->self.lookups.get_table('lemma_rules', {})
A:spacy.lang.fr.lemmatizer.lookup_table->self.lookups.get_table('lemma_lookup', {})
A:spacy.lang.fr.lemmatizer.index->self.lookups.get_table('lemma_index', {}).get(univ_pos, {})
A:spacy.lang.fr.lemmatizer.exceptions->self.lookups.get_table('lemma_exc', {}).get(univ_pos, {})
A:spacy.lang.fr.lemmatizer.rules->self.lookups.get_table('lemma_rules', {}).get(univ_pos, [])
A:spacy.lang.fr.lemmatizer.string->string.lower().lower()
A:spacy.lang.fr.lemmatizer.forms->list(dict.fromkeys(forms))
spacy.lang.fr.FrenchLemmatizer(Lemmatizer)
spacy.lang.fr.FrenchLemmatizer.get_lookups_config(cls,mode:str)->Tuple[List[str], List[str]]
spacy.lang.fr.FrenchLemmatizer.rule_lemmatize(self,token:Token)->List[str]
spacy.lang.fr.lemmatizer.FrenchLemmatizer(Lemmatizer)
spacy.lang.fr.lemmatizer.FrenchLemmatizer.get_lookups_config(cls,mode:str)->Tuple[List[str], List[str]]
spacy.lang.fr.lemmatizer.FrenchLemmatizer.rule_lemmatize(self,token:Token)->List[str]


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/fr/lex_attrs.py----------------------------------------
A:spacy.lang.fr.lex_attrs._num_words->set('\nzero un une deux trois quatre cinq six sept huit neuf dix\nonze douze treize quatorze quinze seize dix-sept dix-huit dix-neuf\nvingt trente quarante cinquante soixante soixante-dix septante quatre-vingt huitante quatre-vingt-dix nonante\ncent mille mil million milliard billion quadrillion quintillion\nsextillion septillion octillion nonillion decillion\n'.split())
A:spacy.lang.fr.lex_attrs._ordinal_words->set('\npremier première deuxième second seconde troisième quatrième cinquième sixième septième huitième neuvième dixième\nonzième douzième treizième quatorzième quinzième seizième dix-septième dix-huitième dix-neuvième\nvingtième trentième quarantième cinquantième soixantième soixante-dixième septantième quatre-vingtième huitantième quatre-vingt-dixième nonantième\ncentième millième millionnième milliardième billionnième quadrillionnième quintillionnième\nsextillionnième septillionnième octillionnième nonillionnième decillionnième\n'.split())
A:spacy.lang.fr.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.fr.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
spacy.lang.fr.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/fr/_tokenizer_exceptions_list.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/fr/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.fr.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)
spacy.lang.fr.tokenizer_exceptions.lower_first_letter(text)
spacy.lang.fr.tokenizer_exceptions.upper_first_letter(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/fr/syntax_iterators.py----------------------------------------
A:spacy.lang.fr.syntax_iterators.np_label->doc.vocab.strings.add('NP')
A:spacy.lang.fr.syntax_iterators.adj_label->doc.vocab.strings.add('amod')
A:spacy.lang.fr.syntax_iterators.det_label->doc.vocab.strings.add('det')
A:spacy.lang.fr.syntax_iterators.det_pos->doc.vocab.strings.add('DET')
A:spacy.lang.fr.syntax_iterators.adp_pos->doc.vocab.strings.add('ADP')
A:spacy.lang.fr.syntax_iterators.conj_label->doc.vocab.strings.add('conj')
A:spacy.lang.fr.syntax_iterators.conj_pos->doc.vocab.strings.add('CCONJ')
A:spacy.lang.fr.syntax_iterators.right_childs->list(word.rights)
spacy.lang.fr.syntax_iterators.noun_chunks(doclike:Union[Doc,Span])->Iterator[Tuple[int, int, int]]


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/fr/stop_words.py----------------------------------------
A:spacy.lang.fr.stop_words.STOP_WORDS->set("\na à â abord afin ah ai aie ainsi ait allaient allons\nalors anterieur anterieure anterieures antérieur antérieure antérieures\napres après as assez attendu au\naupres auquel aura auraient aurait auront\naussi autre autrement autres autrui aux auxquelles auxquels avaient\navais avait avant avec avoir avons ayant\n\nbas basee bat\n\nc' c’ ça car ce ceci cela celle celle-ci celle-la celle-là celles celles-ci celles-la celles-là\ncelui celui-ci celui-la celui-là cent cependant certain certaine certaines certains certes ces\ncet cette ceux ceux-ci ceux-là chacun chacune chaque chez ci cinq cinquantaine cinquante\ncinquantième cinquième combien comme comment compris concernant\n\nd' d’ da dans de debout dedans dehors deja dejà delà depuis derriere\nderrière des desormais desquelles desquels dessous dessus deux deuxième\ndeuxièmement devant devers devra different differente differentes differents différent\ndifférente différentes différents dire directe directement dit dite dits divers\ndiverse diverses dix dix-huit dix-neuf dix-sept dixième doit doivent donc dont\ndouze douzième du duquel durant dès déja déjà désormais\n\neffet egalement eh elle elle-meme elle-même elles elles-memes elles-mêmes en encore\nenfin entre envers environ es ès est et etaient étaient etais étais etait était\netant étant etc etre être eu eux eux-mêmes exactement excepté également\n\nfais faisaient faisant fait facon façon feront font\n\ngens\n\nha hem hep hi ho hormis hors hou houp hue hui huit huitième\nhé i il ils importe\n\nj' j’ je jusqu jusque juste\n\nl' l’ la laisser laquelle le lequel les lesquelles lesquels leur leurs longtemps\nlors lorsque lui lui-meme lui-même là lès\n\nm' m’ ma maint maintenant mais malgre malgré me meme memes merci mes mien\nmienne miennes miens mille moi moi-meme moi-même moindres moins\nmon même mêmes\n\nn' n’ na ne neanmoins neuvième ni nombreuses nombreux nos notamment\nnotre nous nous-mêmes nouveau nul néanmoins nôtre nôtres\n\no ô on ont onze onzième or ou ouias ouste outre\nouvert ouverte ouverts où\n\npar parce parfois parle parlent parler parmi partant\npas pendant pense permet personne peu peut peuvent peux plus\nplusieurs plutot plutôt possible possibles pour pourquoi\npourrais pourrait pouvait prealable precisement\npremier première premièrement\npres procedant proche près préalable précisement pu puis puisque\n\nqu' qu’ quand quant quant-à-soi quarante quatorze quatre quatre-vingt\nquatrième quatrièmement que quel quelconque quelle quelles quelqu'un quelque\nquelques quels qui quiconque quinze quoi quoique\n\nrelative relativement rend rendre restant reste\nrestent retour revoici revoila revoilà\n\ns' s’ sa sait sans sauf se seize selon semblable semblaient\nsemble semblent sent sept septième sera seraient serait seront ses seul seule\nseulement seuls seules si sien sienne siennes siens sinon six sixième soi soi-meme soi-même soit\nsoixante son sont sous souvent specifique specifiques spécifique spécifiques stop\nsuffisant suffisante suffit suis suit suivant suivante\nsuivantes suivants suivre sur surtout\n\nt' t’ ta tant te tel telle tellement telles tels tenant tend tenir tente\ntes tien tienne tiennes tiens toi toi-meme toi-même ton touchant toujours tous\ntout toute toutes treize trente tres trois troisième troisièmement très\ntu té\n\nun une unes uns\n\nva vais vas vers via vingt voici voila voilà vont vos\nvotre votres vous vous-mêmes vu vé vôtre vôtres\n\ny\n\n".split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/fr/punctuation.py----------------------------------------
A:spacy.lang.fr.punctuation.ELISION->"' ’".replace(' ', '')
A:spacy.lang.fr.punctuation.HYPHENS->'- – — ‐ ‑'.replace(' ', '')


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/gd/__init__.py----------------------------------------
spacy.lang.gd.__init__.Scottish(Language)
spacy.lang.gd.__init__.ScottishDefaults(BaseDefaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/gd/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.gd.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/gd/stop_words.py----------------------------------------
A:spacy.lang.gd.stop_words.STOP_WORDS->set("\n'ad\n'ar\n'd # iad\n'g # ag\n'ga\n'gam\n'gan\n'gar\n'gur\n'm # am\n'n # an\n'n seo\n'na\n'nad\n'nam\n'nan\n'nar\n'nuair\n'nur\n's\n'sa\n'san\n'sann\n'se\n'sna\na\na'\na'd # agad\na'm # agam\na-chèile\na-seo\na-sin\na-siud\na chionn\na chionn 's\na chèile\na chéile\na dh'\na h-uile\na seo\nac' # aca\naca\naca-san\nacasan\nach\nag\nagad\nagad-sa\nagads'\nagadsa\nagaibh\nagaibhse\nagainn\nagainne\nagam\nagam-sa\nagams'\nagamsa\nagus\naice\naice-se\naicese\naig\naig' # aige\naige\naige-san\naigesan\nair\nair-san\nair neo\nairsan\nam\nan\nan seo\nan sin\nan siud\nan uair\nann\nann a\nann a'\nann a shin\nann am\nann an\nannad\nannam\nannam-s'\nannamsa\nanns\nanns an\nannta\naon\nar\nas\nasad\nasda\nasta\nb'\nbho\nbhon\nbhuaidhe # bhuaithe\nbhuainn\nbhuaipe\nbhuaithe\nbhuapa\nbhur\nbrì\nbu\nc'à\ncar son\ncarson\ncha\nchan\nchionn\nchoir\nchon\nchun\nchèile\nchéile\nchòir\ncia mheud\nciamar\nco-dhiubh\ncuide\ncuin\ncuin'\ncuine\ncà\ncà'\ncàil\ncàit\ncàit'\ncàite\ncò\ncò mheud\ncó\nd'\nda\nde\ndh'\ndha\ndhaibh\ndhaibh-san\ndhaibhsan\ndhan\ndhasan\ndhe\ndhen\ndheth\ndhi\ndhiom\ndhiot\ndhith\ndhiubh\ndhomh\ndhomh-s'\ndhomhsa\ndhu'sa # dhut-sa\ndhuibh\ndhuibhse\ndhuinn\ndhuinne\ndhuit\ndhut\ndhutsa\ndhut-sa\ndhà\ndhà-san\ndhàsan\ndhòmhsa\ndiubh\ndo\ndocha\ndon\ndà\ndè\ndè mar\ndé\ndé mar\ndòch'\ndòcha\ne\neadar\neatarra\neatorra\neile\nesan\nfa\nfar\nfeud\nfhad\nfheudar\nfhearr\nfhein\nfheudar\nfheàrr\nfhèin\nfhéin\nfhìn\nfo\nfodha\nfodhainn\nfoipe\nfon\nfèin\nga\ngach\ngam\ngan\nge brith\nged\ngu\ngu dè\ngu ruige\ngun\ngur\ngus\ni\niad\niadsan\ninnte\nis\nise\nle\nleam\nleam-sa\nleamsa\nleat\nleat-sa\nleatha\nleatsa\nleibh\nleis\nleis-san\nleoth'\nleotha\nleotha-san\nlinn\nm'\nm'a\nma\nmac\nman\nmar\nmas\nmathaid\nmi\nmis'\nmise\nmo\nmu\nmu 'n\nmun\nmur\nmura\nmus\nna\nna b'\nna bu\nna iad\nnach\nnad\nnam\nnan\nnar\nnas\nneo\nno\nnuair\no\no'n\noir\noirbh\noirbh-se\noirnn\noirnne\noirre\non\norm\norm-sa\normsa\norra\norra-san\norrasan\nort\nos\nr'\nri\nribh\nrinn\nris\nrithe\nrithe-se\nrium\nrium-sa\nriums'\nriumsa\nriut\nriuth'\nriutha\nriuthasan\nro\nro'n\nroimh\nroimhe\nromhainn\nromham\nromhpa\nron\nruibh\nruinn\nruinne\nsa\nsan\nsann\nse\nseach\nseo\nseothach\nshin\nsibh\nsibh-se\nsibhse\nsin\nsineach\nsinn\nsinne\nsiod\nsiodach\nsiud\nsiudach\nsna # ann an\nsè\nt'\ntarsaing\ntarsainn\ntarsuinn\nthar\nthoigh\nthro\nthu\nthuc'\nthuca\nthugad\nthugaibh\nthugainn\nthugam\nthugamsa\nthuice\nthuige\nthus'\nthusa\ntimcheall\ntoigh\ntoil\ntro\ntro' # troimh\ntroimh\ntroimhe\ntron\ntu\ntusa\nuair\nud\nugaibh\nugam-s'\nugam-sa\nuice\nuige\nuige-san\numad\nunnta # ann an\nur\nurrainn\nà\nàs\nàsan\ná\nás\nè\nì\nò\nó\n".split('\n'))


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/kmr/__init__.py----------------------------------------
spacy.lang.kmr.__init__.Kurmanji(Language)
spacy.lang.kmr.__init__.KurmanjiDefaults(BaseDefaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/kmr/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/kmr/lex_attrs.py----------------------------------------
A:spacy.lang.kmr.lex_attrs.text->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '')
A:spacy.lang.kmr.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').split('/')
A:spacy.lang.kmr.lex_attrs.text_lower->text.replace(',', '').replace('.', '').replace(',', '').replace('.', '').lower()
A:spacy.lang.kmr.lex_attrs.to->len(ending)
spacy.lang.kmr.lex_attrs.is_digit(text)
spacy.lang.kmr.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/kmr/stop_words.py----------------------------------------
A:spacy.lang.kmr.stop_words.STOP_WORDS->set('\nû\nli\nbi\ndi\nda\nde\nji\nku\new\nez\ntu\nem\nhûn\new\nev\nmin\nte\nwî\nwê\nme\nwe\nwan\nvê\nvî\nva\nçi\nkî\nkê\nçawa\nçima\nkengî\nli ku\nçend\nçiqas\nher\nhin\ngelek\nhemû\nkes\ntişt\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/pt/__init__.py----------------------------------------
spacy.lang.pt.__init__.Portuguese(Language)
spacy.lang.pt.__init__.PortugueseDefaults(BaseDefaults)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/pt/examples.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/pt/lex_attrs.py----------------------------------------
A:spacy.lang.pt.lex_attrs.text->text.replace(',', '').replace('.', '').replace('º', '').replace('ª', '').replace(',', '').replace('.', '').replace('º', '').replace('ª', '')
A:spacy.lang.pt.lex_attrs.(num, denom)->text.replace(',', '').replace('.', '').replace('º', '').replace('ª', '').replace(',', '').replace('.', '').replace('º', '').replace('ª', '').split('/')
spacy.lang.pt.lex_attrs.like_num(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/pt/tokenizer_exceptions.py----------------------------------------
A:spacy.lang.pt.tokenizer_exceptions.TOKENIZER_EXCEPTIONS->update_exc(BASE_EXCEPTIONS, _exc)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/pt/syntax_iterators.py----------------------------------------
A:spacy.lang.pt.syntax_iterators.np_label->doc.vocab.strings.add('NP')
A:spacy.lang.pt.syntax_iterators.adj_label->doc.vocab.strings.add('amod')
A:spacy.lang.pt.syntax_iterators.det_label->doc.vocab.strings.add('det')
A:spacy.lang.pt.syntax_iterators.det_pos->doc.vocab.strings.add('DET')
A:spacy.lang.pt.syntax_iterators.adp_label->doc.vocab.strings.add('ADP')
A:spacy.lang.pt.syntax_iterators.conj->doc.vocab.strings.add('conj')
A:spacy.lang.pt.syntax_iterators.conj_pos->doc.vocab.strings.add('CCONJ')
A:spacy.lang.pt.syntax_iterators.right_childs->list(word.rights)
spacy.lang.pt.syntax_iterators.noun_chunks(doclike:Union[Doc,Span])->Iterator[Tuple[int, int, int]]


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/pt/stop_words.py----------------------------------------
A:spacy.lang.pt.stop_words.STOP_WORDS->set('\na à às área acerca ademais adeus agora ainda algo algumas alguns ali além ambas ambos antes\nao aos apenas apoia apoio apontar após aquela aquelas aquele aqueles aqui aquilo\nas assim através atrás até aí\n\nbaixo bastante bem boa bom breve\n\ncada caminho catorze cedo cento certamente certeza cima cinco coisa com como\ncomprida comprido conhecida conhecido conselho contra contudo corrente cuja\ncujo custa cá\n\nda daquela daquele dar das de debaixo demais dentro depois des desde dessa desse\ndesta deste deve devem deverá dez dezanove dezasseis dezassete dezoito diante\ndireita disso diz dizem dizer do dois dos doze duas dá dão\n\ne é és ela elas ele eles em embora enquanto entre então era essa essas esse esses esta\nestado estar estará estas estava este estes esteve estive estivemos estiveram\nestiveste estivestes estou está estás estão eu eventual exemplo\n\nfalta fará favor faz fazeis fazem fazemos fazer fazes fazia faço fez fim final\nfoi fomos for fora foram forma foste fostes fui\n\ngeral grande grandes grupo\n\ninclusive iniciar inicio ir irá isso isto\n\njá\n\nlado lhe ligado local logo longe lugar lá\n\nmaior maioria maiorias mais mal mas me meio menor menos meses mesmo meu meus mil\nminha minhas momento muito muitos máximo mês\n\nna nada naquela naquele nas nem nenhuma nessa nesse nesta neste no nos nossa\nnossas nosso nossos nova novas nove novo novos num numa nunca nuns não nível nós\nnúmero números\n\no obrigada obrigado oitava oitavo oito onde ontem onze ora os ou outra outras outros\n\npara parece parte partir pegar pela pelas pelo pelos perto pode podem poder poderá\npodia pois ponto pontos por porquanto porque porquê portanto porém posição\npossivelmente posso possível pouca pouco povo primeira primeiro próprio próxima\npróximo puderam pôde põe põem\n\nquais qual qualquer quando quanto quarta quarto quatro que quem quer querem quero\nquestão quieta quieto quinta quinto quinze quê\n\nrelação\n\nsabe saber se segunda segundo sei seis sem sempre ser seria sete seu seus sexta\nsexto sim sistema sob sobre sois somente somos sou sua suas são sétima sétimo só\n\ntais tal talvez também tanta tanto tarde te tem temos tempo tendes tenho tens\ntentar tentaram tente tentei ter terceira terceiro teu teus teve tipo tive\ntivemos tiveram tiveste tivestes toda todas todo todos treze três tu tua tuas\ntudo tão têm\n\num uma umas uns usa usar último\n\nvai vais valor veja vem vens ver vez vezes vinda vindo vinte você vocês vos vossa\nvossas vosso vossos vários vão vêm vós\n\nzero\n'.split())


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/lang/pt/punctuation.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/matcher/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/matcher/matcher.pyi----------------------------------------
spacy.matcher.Matcher(self,vocab:Vocab,validate:bool=...,fuzzy_compare:Callable[[str,str,int],bool]=...)
spacy.matcher.Matcher.__contains__(self,key:str)->bool
spacy.matcher.Matcher.__len__(self)->int
spacy.matcher.Matcher.__reduce__(self)->Any
spacy.matcher.Matcher._normalize_key(self,key:Any)->Any
spacy.matcher.Matcher.add(self,key:Union[str,int],patterns:List[List[Dict[str,Any]]],*,on_match:Optional[Callable[[Matcher,Doc,int,List[Tuple[Any,...]]],Any]]=...,greedy:Optional[str]=...)->None
spacy.matcher.Matcher.get(self,key:Union[str,int],default:Optional[Any]=...)->Tuple[Optional[Callable[[Any], Any]], List[List[Dict[Any, Any]]]]
spacy.matcher.Matcher.has_key(self,key:Union[str,int])->bool
spacy.matcher.Matcher.pipe(self,docs:Iterable[Tuple[Doc,Any]],batch_size:int=...,return_matches:bool=...,as_tuples:bool=...)->Union[Iterator[Tuple[Tuple[Doc, Any], Any]], Iterator[Tuple[Doc, Any]], Iterator[Doc]]
spacy.matcher.Matcher.remove(self,key:str)->None
spacy.matcher.matcher.Matcher(self,vocab:Vocab,validate:bool=...,fuzzy_compare:Callable[[str,str,int],bool]=...)
spacy.matcher.matcher.Matcher.__contains__(self,key:str)->bool
spacy.matcher.matcher.Matcher.__init__(self,vocab:Vocab,validate:bool=...,fuzzy_compare:Callable[[str,str,int],bool]=...)
spacy.matcher.matcher.Matcher.__len__(self)->int
spacy.matcher.matcher.Matcher.__reduce__(self)->Any
spacy.matcher.matcher.Matcher._normalize_key(self,key:Any)->Any
spacy.matcher.matcher.Matcher.add(self,key:Union[str,int],patterns:List[List[Dict[str,Any]]],*,on_match:Optional[Callable[[Matcher,Doc,int,List[Tuple[Any,...]]],Any]]=...,greedy:Optional[str]=...)->None
spacy.matcher.matcher.Matcher.get(self,key:Union[str,int],default:Optional[Any]=...)->Tuple[Optional[Callable[[Any], Any]], List[List[Dict[Any, Any]]]]
spacy.matcher.matcher.Matcher.has_key(self,key:Union[str,int])->bool
spacy.matcher.matcher.Matcher.pipe(self,docs:Iterable[Tuple[Doc,Any]],batch_size:int=...,return_matches:bool=...,as_tuples:bool=...)->Union[Iterator[Tuple[Tuple[Doc, Any], Any]], Iterator[Tuple[Doc, Any]], Iterator[Doc]]
spacy.matcher.matcher.Matcher.remove(self,key:str)->None


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/matcher/dependencymatcher.pyi----------------------------------------
spacy.matcher.DependencyMatcher(self,vocab:Vocab,*,validate:bool=...)
spacy.matcher.DependencyMatcher.__contains__(self,key:Union[str,int])->bool
spacy.matcher.DependencyMatcher.__len__(self)->int
spacy.matcher.DependencyMatcher.__reduce__(self)->Tuple[Callable[[Vocab, Dict[str, Any], Dict[str, Callable[..., Any]]], DependencyMatcher], Tuple[Vocab, Dict[str, List[Any]], Dict[str, Callable[[DependencyMatcher, Doc, int, List[Tuple[int, List[int]]]], Any]]], None, None]
spacy.matcher.DependencyMatcher.add(self,key:Union[str,int],patterns:List[List[Dict[str,Any]]],*,on_match:Optional[Callable[[DependencyMatcher,Doc,int,List[Tuple[int,List[int]]]],Any]]=...)->None
spacy.matcher.DependencyMatcher.get(self,key:Union[str,int],default:Optional[Any]=...)->Tuple[Optional[Callable[[DependencyMatcher, Doc, int, List[Tuple[int, List[int]]]], Any]], List[List[Dict[str, Any]]]]
spacy.matcher.DependencyMatcher.has_key(self,key:Union[str,int])->bool
spacy.matcher.DependencyMatcher.remove(self,key:Union[str,int])->None
spacy.matcher.dependencymatcher.DependencyMatcher(self,vocab:Vocab,*,validate:bool=...)
spacy.matcher.dependencymatcher.DependencyMatcher.__contains__(self,key:Union[str,int])->bool
spacy.matcher.dependencymatcher.DependencyMatcher.__init__(self,vocab:Vocab,*,validate:bool=...)
spacy.matcher.dependencymatcher.DependencyMatcher.__len__(self)->int
spacy.matcher.dependencymatcher.DependencyMatcher.__reduce__(self)->Tuple[Callable[[Vocab, Dict[str, Any], Dict[str, Callable[..., Any]]], DependencyMatcher], Tuple[Vocab, Dict[str, List[Any]], Dict[str, Callable[[DependencyMatcher, Doc, int, List[Tuple[int, List[int]]]], Any]]], None, None]
spacy.matcher.dependencymatcher.DependencyMatcher.add(self,key:Union[str,int],patterns:List[List[Dict[str,Any]]],*,on_match:Optional[Callable[[DependencyMatcher,Doc,int,List[Tuple[int,List[int]]]],Any]]=...)->None
spacy.matcher.dependencymatcher.DependencyMatcher.get(self,key:Union[str,int],default:Optional[Any]=...)->Tuple[Optional[Callable[[DependencyMatcher, Doc, int, List[Tuple[int, List[int]]]], Any]], List[List[Dict[str, Any]]]]
spacy.matcher.dependencymatcher.DependencyMatcher.has_key(self,key:Union[str,int])->bool
spacy.matcher.dependencymatcher.DependencyMatcher.remove(self,key:Union[str,int])->None
spacy.matcher.dependencymatcher.unpickle_matcher(vocab:Vocab,patterns:Dict[str,Any],callbacks:Dict[str,Callable[...,Any]])->DependencyMatcher


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/matcher/phrasematcher.pyi----------------------------------------
spacy.matcher.PhraseMatcher(self,vocab:Vocab,attr:Optional[Union[int,str]]=...,validate:bool=...)
spacy.matcher.PhraseMatcher.__contains__(self,key:str)->bool
spacy.matcher.PhraseMatcher.__len__(self)->int
spacy.matcher.PhraseMatcher.__reduce__(self)->Any
spacy.matcher.PhraseMatcher.add(self,key:str,docs:List[Doc],*,on_match:Optional[Callable[[Matcher,Doc,int,List[Tuple[Any,...]]],Any]]=...)->None
spacy.matcher.PhraseMatcher.remove(self,key:str)->None
spacy.matcher.phrasematcher.PhraseMatcher(self,vocab:Vocab,attr:Optional[Union[int,str]]=...,validate:bool=...)
spacy.matcher.phrasematcher.PhraseMatcher.__contains__(self,key:str)->bool
spacy.matcher.phrasematcher.PhraseMatcher.__init__(self,vocab:Vocab,attr:Optional[Union[int,str]]=...,validate:bool=...)
spacy.matcher.phrasematcher.PhraseMatcher.__len__(self)->int
spacy.matcher.phrasematcher.PhraseMatcher.__reduce__(self)->Any
spacy.matcher.phrasematcher.PhraseMatcher.add(self,key:str,docs:List[Doc],*,on_match:Optional[Callable[[Matcher,Doc,int,List[Tuple[Any,...]]],Any]]=...)->None
spacy.matcher.phrasematcher.PhraseMatcher.remove(self,key:str)->None


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/kb/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tokens/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tokens/_retokenize.pyi----------------------------------------
spacy.tokens._retokenize.Retokenizer(self,doc:Doc)
spacy.tokens._retokenize.Retokenizer.__enter__(self)->Retokenizer
spacy.tokens._retokenize.Retokenizer.__exit__(self,*args:Any)->None
spacy.tokens._retokenize.Retokenizer.__init__(self,doc:Doc)
spacy.tokens._retokenize.Retokenizer.merge(self,span:Span,attrs:Dict[Union[str,int],Any]=...)->None
spacy.tokens._retokenize.Retokenizer.split(self,token:Token,orths:List[str],heads:List[Union[Token,Tuple[Token,int]]],attrs:Dict[Union[str,int],List[Any]]=...)->None
spacy.tokens._retokenize.normalize_token_attrs(vocab:Vocab,attrs:Dict)
spacy.tokens._retokenize.set_token_attrs(py_token:Token,attrs:Dict)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tokens/morphanalysis.pyi----------------------------------------
spacy.tokens.MorphAnalysis(self,vocab:Vocab,features:Union[Dict[str,str],str]=...)
spacy.tokens.MorphAnalysis.__contains__(self,feature:str)->bool
spacy.tokens.MorphAnalysis.__eq__(self,other:MorphAnalysis)->bool
spacy.tokens.MorphAnalysis.__hash__(self)->int
spacy.tokens.MorphAnalysis.__iter__(self)->Iterator[str]
spacy.tokens.MorphAnalysis.__len__(self)->int
spacy.tokens.MorphAnalysis.__ne__(self,other:MorphAnalysis)->bool
spacy.tokens.MorphAnalysis.__repr__(self)->str
spacy.tokens.MorphAnalysis.__str__(self)->str
spacy.tokens.MorphAnalysis.from_id(cls,vocab:Vocab,key:Any)->MorphAnalysis
spacy.tokens.MorphAnalysis.get(self,field:Any,default:Optional[List[str]])->List[str]
spacy.tokens.MorphAnalysis.to_dict(self)->Dict[str, str]
spacy.tokens.MorphAnalysis.to_json(self)->str
spacy.tokens.morphanalysis.MorphAnalysis(self,vocab:Vocab,features:Union[Dict[str,str],str]=...)
spacy.tokens.morphanalysis.MorphAnalysis.__contains__(self,feature:str)->bool
spacy.tokens.morphanalysis.MorphAnalysis.__eq__(self,other:MorphAnalysis)->bool
spacy.tokens.morphanalysis.MorphAnalysis.__hash__(self)->int
spacy.tokens.morphanalysis.MorphAnalysis.__init__(self,vocab:Vocab,features:Union[Dict[str,str],str]=...)
spacy.tokens.morphanalysis.MorphAnalysis.__iter__(self)->Iterator[str]
spacy.tokens.morphanalysis.MorphAnalysis.__len__(self)->int
spacy.tokens.morphanalysis.MorphAnalysis.__ne__(self,other:MorphAnalysis)->bool
spacy.tokens.morphanalysis.MorphAnalysis.__repr__(self)->str
spacy.tokens.morphanalysis.MorphAnalysis.__str__(self)->str
spacy.tokens.morphanalysis.MorphAnalysis.from_id(cls,vocab:Vocab,key:Any)->MorphAnalysis
spacy.tokens.morphanalysis.MorphAnalysis.get(self,field:Any,default:Optional[List[str]])->List[str]
spacy.tokens.morphanalysis.MorphAnalysis.to_dict(self)->Dict[str, str]
spacy.tokens.morphanalysis.MorphAnalysis.to_json(self)->str


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tokens/span.pyi----------------------------------------
spacy.tokens.Span(self,doc:Doc,start:int,end:int,label:Union[str,int]=...,vector:Optional[Floats1d]=...,vector_norm:Optional[float]=...,kb_id:Union[str,int]=...,span_id:Union[str,int]=...)
spacy.tokens.Span._(self)->Underscore
spacy.tokens.Span.__eq__(self,other:Any)->bool
spacy.tokens.Span.__ge__(self,other:Any)->bool
spacy.tokens.Span.__getitem__(self,i:int)->Token
spacy.tokens.Span.__getitem__(self,i:slice)->Span
spacy.tokens.Span.__gt__(self,other:Any)->bool
spacy.tokens.Span.__hash__(self)->int
spacy.tokens.Span.__iter__(self)->Iterator[Token]
spacy.tokens.Span.__le__(self,other:Any)->bool
spacy.tokens.Span.__len__(self)->int
spacy.tokens.Span.__lt__(self,other:Any)->bool
spacy.tokens.Span.__ne__(self,other:Any)->bool
spacy.tokens.Span.__repr__(self)->str
spacy.tokens.Span.as_doc(self,*,copy_user_data:bool=...)->Doc
spacy.tokens.Span.char_span(self,start_idx:int,end_idx:int,label:Union[int,str]=...,kb_id:Union[int,str]=...,vector:Optional[Floats1d]=...,id:Union[int,str]=...,alignment_mode:str=...,span_id:Union[int,str]=...)->Span
spacy.tokens.Span.conjuncts(self)->Tuple[Token]
spacy.tokens.Span.doc(self)->Doc
spacy.tokens.Span.ents(self)->Tuple[Span]
spacy.tokens.Span.get_extension(cls,name:str)->Tuple[Optional[Any], Optional[SpanMethod], Optional[Callable[[Span], Any]], Optional[Callable[[Span, Any], None]]]
spacy.tokens.Span.get_lca_matrix(self)->Ints2d
spacy.tokens.Span.has_extension(cls,name:str)->bool
spacy.tokens.Span.has_vector(self)->bool
spacy.tokens.Span.lefts(self)->Iterator[Token]
spacy.tokens.Span.lemma_(self)->str
spacy.tokens.Span.n_lefts(self)->int
spacy.tokens.Span.n_rights(self)->int
spacy.tokens.Span.noun_chunks(self)->Iterator[Span]
spacy.tokens.Span.orth_(self)->str
spacy.tokens.Span.remove_extension(cls,name:str)->Tuple[Optional[Any], Optional[SpanMethod], Optional[Callable[[Span], Any]], Optional[Callable[[Span, Any], None]]]
spacy.tokens.Span.rights(self)->Iterator[Token]
spacy.tokens.Span.root(self)->Token
spacy.tokens.Span.sent(self)->Span
spacy.tokens.Span.sentiment(self)->float
spacy.tokens.Span.set_extension(cls,name:str,default:Optional[Any]=...,getter:Optional[Callable[[Span],Any]]=...,setter:Optional[Callable[[Span,Any],None]]=...,method:Optional[SpanMethod]=...,force:bool=...)->None
spacy.tokens.Span.similarity(self,other:Union[Doc,Span,Token,Lexeme])->float
spacy.tokens.Span.subtree(self)->Iterator[Token]
spacy.tokens.Span.tensor(self)->FloatsXd
spacy.tokens.Span.text(self)->str
spacy.tokens.Span.text_with_ws(self)->str
spacy.tokens.Span.vector(self)->Floats1d
spacy.tokens.Span.vector_norm(self)->float
spacy.tokens.Span.vocab(self)->Vocab
spacy.tokens.SpanMethod(self:Span,*args:Any,**kwargs:Any)
spacy.tokens.span.Span(self,doc:Doc,start:int,end:int,label:Union[str,int]=...,vector:Optional[Floats1d]=...,vector_norm:Optional[float]=...,kb_id:Union[str,int]=...,span_id:Union[str,int]=...)
spacy.tokens.span.Span._(self)->Underscore
spacy.tokens.span.Span.__eq__(self,other:Any)->bool
spacy.tokens.span.Span.__ge__(self,other:Any)->bool
spacy.tokens.span.Span.__getitem__(self,i:int)->Token
spacy.tokens.span.Span.__getitem__(self,i:slice)->Span
spacy.tokens.span.Span.__gt__(self,other:Any)->bool
spacy.tokens.span.Span.__hash__(self)->int
spacy.tokens.span.Span.__init__(self,doc:Doc,start:int,end:int,label:Union[str,int]=...,vector:Optional[Floats1d]=...,vector_norm:Optional[float]=...,kb_id:Union[str,int]=...,span_id:Union[str,int]=...)
spacy.tokens.span.Span.__iter__(self)->Iterator[Token]
spacy.tokens.span.Span.__le__(self,other:Any)->bool
spacy.tokens.span.Span.__len__(self)->int
spacy.tokens.span.Span.__lt__(self,other:Any)->bool
spacy.tokens.span.Span.__ne__(self,other:Any)->bool
spacy.tokens.span.Span.__repr__(self)->str
spacy.tokens.span.Span.as_doc(self,*,copy_user_data:bool=...)->Doc
spacy.tokens.span.Span.char_span(self,start_idx:int,end_idx:int,label:Union[int,str]=...,kb_id:Union[int,str]=...,vector:Optional[Floats1d]=...,id:Union[int,str]=...,alignment_mode:str=...,span_id:Union[int,str]=...)->Span
spacy.tokens.span.Span.conjuncts(self)->Tuple[Token]
spacy.tokens.span.Span.doc(self)->Doc
spacy.tokens.span.Span.ents(self)->Tuple[Span]
spacy.tokens.span.Span.get_extension(cls,name:str)->Tuple[Optional[Any], Optional[SpanMethod], Optional[Callable[[Span], Any]], Optional[Callable[[Span, Any], None]]]
spacy.tokens.span.Span.get_lca_matrix(self)->Ints2d
spacy.tokens.span.Span.has_extension(cls,name:str)->bool
spacy.tokens.span.Span.has_vector(self)->bool
spacy.tokens.span.Span.lefts(self)->Iterator[Token]
spacy.tokens.span.Span.lemma_(self)->str
spacy.tokens.span.Span.n_lefts(self)->int
spacy.tokens.span.Span.n_rights(self)->int
spacy.tokens.span.Span.noun_chunks(self)->Iterator[Span]
spacy.tokens.span.Span.orth_(self)->str
spacy.tokens.span.Span.remove_extension(cls,name:str)->Tuple[Optional[Any], Optional[SpanMethod], Optional[Callable[[Span], Any]], Optional[Callable[[Span, Any], None]]]
spacy.tokens.span.Span.rights(self)->Iterator[Token]
spacy.tokens.span.Span.root(self)->Token
spacy.tokens.span.Span.sent(self)->Span
spacy.tokens.span.Span.sentiment(self)->float
spacy.tokens.span.Span.set_extension(cls,name:str,default:Optional[Any]=...,getter:Optional[Callable[[Span],Any]]=...,setter:Optional[Callable[[Span,Any],None]]=...,method:Optional[SpanMethod]=...,force:bool=...)->None
spacy.tokens.span.Span.similarity(self,other:Union[Doc,Span,Token,Lexeme])->float
spacy.tokens.span.Span.subtree(self)->Iterator[Token]
spacy.tokens.span.Span.tensor(self)->FloatsXd
spacy.tokens.span.Span.text(self)->str
spacy.tokens.span.Span.text_with_ws(self)->str
spacy.tokens.span.Span.vector(self)->Floats1d
spacy.tokens.span.Span.vector_norm(self)->float
spacy.tokens.span.Span.vocab(self)->Vocab
spacy.tokens.span.SpanMethod(self:Span,*args:Any,**kwargs:Any)
spacy.tokens.span.SpanMethod.__call__(self:Span,*args:Any,**kwargs:Any)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tokens/span_group.pyi----------------------------------------
spacy.tokens.SpanGroup(self,doc:Doc,*,name:str=...,attrs:Dict[str,Any]=...,spans:Iterable[Span]=...)
spacy.tokens.SpanGroup.__getitem__(self,i:int)->Span
spacy.tokens.SpanGroup.__iter__(self)->Iterator[Span]
spacy.tokens.SpanGroup.__len__(self)->int
spacy.tokens.SpanGroup.__repr__(self)->str
spacy.tokens.SpanGroup.append(self,span:Span)->None
spacy.tokens.SpanGroup.copy(self,doc:Optional[Doc]=...)->SpanGroup
spacy.tokens.SpanGroup.doc(self)->Doc
spacy.tokens.SpanGroup.extend(self,spans:Iterable[Span])->None
spacy.tokens.SpanGroup.from_bytes(self,bytes_data:bytes)->SpanGroup
spacy.tokens.SpanGroup.has_overlap(self)->bool
spacy.tokens.SpanGroup.to_bytes(self)->bytes
spacy.tokens.span_group.SpanGroup(self,doc:Doc,*,name:str=...,attrs:Dict[str,Any]=...,spans:Iterable[Span]=...)
spacy.tokens.span_group.SpanGroup.__getitem__(self,i:int)->Span
spacy.tokens.span_group.SpanGroup.__init__(self,doc:Doc,*,name:str=...,attrs:Dict[str,Any]=...,spans:Iterable[Span]=...)
spacy.tokens.span_group.SpanGroup.__iter__(self)->Iterator[Span]
spacy.tokens.span_group.SpanGroup.__len__(self)->int
spacy.tokens.span_group.SpanGroup.__repr__(self)->str
spacy.tokens.span_group.SpanGroup.append(self,span:Span)->None
spacy.tokens.span_group.SpanGroup.copy(self,doc:Optional[Doc]=...)->SpanGroup
spacy.tokens.span_group.SpanGroup.doc(self)->Doc
spacy.tokens.span_group.SpanGroup.extend(self,spans:Iterable[Span])->None
spacy.tokens.span_group.SpanGroup.from_bytes(self,bytes_data:bytes)->SpanGroup
spacy.tokens.span_group.SpanGroup.has_overlap(self)->bool
spacy.tokens.span_group.SpanGroup.to_bytes(self)->bytes


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tokens/doc.pyi----------------------------------------
spacy.tokens.Doc(self,vocab:Vocab,words:Optional[List[str]]=...,spaces:Optional[List[bool]]=...,user_data:Optional[Dict[Any,Any]]=...,tags:Optional[List[str]]=...,pos:Optional[List[str]]=...,morphs:Optional[List[str]]=...,lemmas:Optional[List[str]]=...,heads:Optional[List[int]]=...,deps:Optional[List[str]]=...,sent_starts:Optional[List[Union[bool,int,None]]]=...,ents:Optional[List[str]]=...)
spacy.tokens.Doc._(self)->Underscore
spacy.tokens.Doc.__bytes__(self)->bytes
spacy.tokens.Doc.__getitem__(self,i:int)->Token
spacy.tokens.Doc.__getitem__(self,i:slice)->Span
spacy.tokens.Doc.__iter__(self)->Iterator[Token]
spacy.tokens.Doc.__len__(self)->int
spacy.tokens.Doc.__repr__(self)->str
spacy.tokens.Doc.__str__(self)->str
spacy.tokens.Doc.__unicode__(self)->str
spacy.tokens.Doc._get_array_attrs()->Tuple[Any]
spacy.tokens.Doc.char_span(self,start_idx:int,end_idx:int,label:Union[int,str]=...,kb_id:Union[int,str]=...,vector:Optional[Floats1d]=...,alignment_mode:str=...,span_id:Union[int,str]=...)->Optional[Span]
spacy.tokens.Doc.copy(self)->Doc
spacy.tokens.Doc.count_by(self,attr_id:int,exclude:Optional[Any]=...,counts:Optional[Any]=...)->Dict[Any, int]
spacy.tokens.Doc.doc(self)->Doc
spacy.tokens.Doc.ents(self)->Sequence[Span]
spacy.tokens.Doc.ents(self,value:Sequence[Span])->None
spacy.tokens.Doc.extend_tensor(self,tensor:Floats2d)->None
spacy.tokens.Doc.from_array(self,attrs:Union[int,str,List[Union[int,str]]],array:Ints2d)->Doc
spacy.tokens.Doc.from_bytes(self,bytes_data:bytes,*,exclude:Iterable[str]=...)->Doc
spacy.tokens.Doc.from_dict(self,msg:Dict[str,Any],*,exclude:Iterable[str]=...)->Doc
spacy.tokens.Doc.from_disk(self,path:Union[str,Path],*,exclude:Iterable[str]=...)->Doc
spacy.tokens.Doc.from_docs(docs:List[Doc],ensure_whitespace:bool=...,attrs:Optional[Union[Tuple[Union[str,int]],List[Union[int,str]]]]=...)->Doc
spacy.tokens.Doc.from_json(self,doc_json:Dict[str,Any]=...,validate:bool=False)->Doc
spacy.tokens.Doc.get_extension(cls,name:str)->Tuple[Optional[Any], Optional[DocMethod], Optional[Callable[[Doc], Any]], Optional[Callable[[Doc, Any], None]]]
spacy.tokens.Doc.get_lca_matrix(self)->Ints2d
spacy.tokens.Doc.has_annotation(self,attr:Union[int,str],*,require_complete:bool=...)->bool
spacy.tokens.Doc.has_extension(cls,name:str)->bool
spacy.tokens.Doc.has_vector(self)->bool
spacy.tokens.Doc.is_nered(self)->bool
spacy.tokens.Doc.is_parsed(self)->bool
spacy.tokens.Doc.is_sentenced(self)->bool
spacy.tokens.Doc.is_tagged(self)->bool
spacy.tokens.Doc.lang(self)->int
spacy.tokens.Doc.lang_(self)->str
spacy.tokens.Doc.noun_chunks(self)->Iterator[Span]
spacy.tokens.Doc.remove_extension(cls,name:str)->Tuple[Optional[Any], Optional[DocMethod], Optional[Callable[[Doc], Any]], Optional[Callable[[Doc, Any], None]]]
spacy.tokens.Doc.retokenize(self)->Retokenizer
spacy.tokens.Doc.sents(self)->Iterator[Span]
spacy.tokens.Doc.set_ents(self,entities:List[Span],*,blocked:Optional[List[Span]]=...,missing:Optional[List[Span]]=...,outside:Optional[List[Span]]=...,default:str=...)->None
spacy.tokens.Doc.set_extension(cls,name:str,default:Optional[Any]=...,getter:Optional[Callable[[Doc],Any]]=...,setter:Optional[Callable[[Doc,Any],None]]=...,method:Optional[DocMethod]=...,force:bool=...)->None
spacy.tokens.Doc.similarity(self,other:Union[Doc,Span,Token,Lexeme])->float
spacy.tokens.Doc.text(self)->str
spacy.tokens.Doc.text_with_ws(self)->str
spacy.tokens.Doc.to_array(self,py_attr_ids:Union[int,str,List[Union[int,str]]])->np.ndarray[Any, np.dtype[np.float64]]
spacy.tokens.Doc.to_bytes(self,*,exclude:Iterable[str]=...)->bytes
spacy.tokens.Doc.to_dict(self,*,exclude:Iterable[str]=...)->Dict[str, Any]
spacy.tokens.Doc.to_disk(self,path:Union[str,Path],*,exclude:Iterable[str]=...)->None
spacy.tokens.Doc.to_json(self,underscore:Optional[List[str]]=...)->Dict[str, Any]
spacy.tokens.Doc.to_utf8_array(self,nr_char:int=...)->Ints2d
spacy.tokens.DocMethod(self:Doc,*args:Any,**kwargs:Any)
spacy.tokens.doc.Doc(self,vocab:Vocab,words:Optional[List[str]]=...,spaces:Optional[List[bool]]=...,user_data:Optional[Dict[Any,Any]]=...,tags:Optional[List[str]]=...,pos:Optional[List[str]]=...,morphs:Optional[List[str]]=...,lemmas:Optional[List[str]]=...,heads:Optional[List[int]]=...,deps:Optional[List[str]]=...,sent_starts:Optional[List[Union[bool,int,None]]]=...,ents:Optional[List[str]]=...)
spacy.tokens.doc.Doc._(self)->Underscore
spacy.tokens.doc.Doc.__bytes__(self)->bytes
spacy.tokens.doc.Doc.__getitem__(self,i:int)->Token
spacy.tokens.doc.Doc.__getitem__(self,i:slice)->Span
spacy.tokens.doc.Doc.__init__(self,vocab:Vocab,words:Optional[List[str]]=...,spaces:Optional[List[bool]]=...,user_data:Optional[Dict[Any,Any]]=...,tags:Optional[List[str]]=...,pos:Optional[List[str]]=...,morphs:Optional[List[str]]=...,lemmas:Optional[List[str]]=...,heads:Optional[List[int]]=...,deps:Optional[List[str]]=...,sent_starts:Optional[List[Union[bool,int,None]]]=...,ents:Optional[List[str]]=...)
spacy.tokens.doc.Doc.__iter__(self)->Iterator[Token]
spacy.tokens.doc.Doc.__len__(self)->int
spacy.tokens.doc.Doc.__repr__(self)->str
spacy.tokens.doc.Doc.__str__(self)->str
spacy.tokens.doc.Doc.__unicode__(self)->str
spacy.tokens.doc.Doc._get_array_attrs()->Tuple[Any]
spacy.tokens.doc.Doc.char_span(self,start_idx:int,end_idx:int,label:Union[int,str]=...,kb_id:Union[int,str]=...,vector:Optional[Floats1d]=...,alignment_mode:str=...,span_id:Union[int,str]=...)->Optional[Span]
spacy.tokens.doc.Doc.copy(self)->Doc
spacy.tokens.doc.Doc.count_by(self,attr_id:int,exclude:Optional[Any]=...,counts:Optional[Any]=...)->Dict[Any, int]
spacy.tokens.doc.Doc.doc(self)->Doc
spacy.tokens.doc.Doc.ents(self)->Sequence[Span]
spacy.tokens.doc.Doc.ents(self,value:Sequence[Span])->None
spacy.tokens.doc.Doc.extend_tensor(self,tensor:Floats2d)->None
spacy.tokens.doc.Doc.from_array(self,attrs:Union[int,str,List[Union[int,str]]],array:Ints2d)->Doc
spacy.tokens.doc.Doc.from_bytes(self,bytes_data:bytes,*,exclude:Iterable[str]=...)->Doc
spacy.tokens.doc.Doc.from_dict(self,msg:Dict[str,Any],*,exclude:Iterable[str]=...)->Doc
spacy.tokens.doc.Doc.from_disk(self,path:Union[str,Path],*,exclude:Iterable[str]=...)->Doc
spacy.tokens.doc.Doc.from_docs(docs:List[Doc],ensure_whitespace:bool=...,attrs:Optional[Union[Tuple[Union[str,int]],List[Union[int,str]]]]=...)->Doc
spacy.tokens.doc.Doc.from_json(self,doc_json:Dict[str,Any]=...,validate:bool=False)->Doc
spacy.tokens.doc.Doc.get_extension(cls,name:str)->Tuple[Optional[Any], Optional[DocMethod], Optional[Callable[[Doc], Any]], Optional[Callable[[Doc, Any], None]]]
spacy.tokens.doc.Doc.get_lca_matrix(self)->Ints2d
spacy.tokens.doc.Doc.has_annotation(self,attr:Union[int,str],*,require_complete:bool=...)->bool
spacy.tokens.doc.Doc.has_extension(cls,name:str)->bool
spacy.tokens.doc.Doc.has_vector(self)->bool
spacy.tokens.doc.Doc.is_nered(self)->bool
spacy.tokens.doc.Doc.is_parsed(self)->bool
spacy.tokens.doc.Doc.is_sentenced(self)->bool
spacy.tokens.doc.Doc.is_tagged(self)->bool
spacy.tokens.doc.Doc.lang(self)->int
spacy.tokens.doc.Doc.lang_(self)->str
spacy.tokens.doc.Doc.noun_chunks(self)->Iterator[Span]
spacy.tokens.doc.Doc.remove_extension(cls,name:str)->Tuple[Optional[Any], Optional[DocMethod], Optional[Callable[[Doc], Any]], Optional[Callable[[Doc, Any], None]]]
spacy.tokens.doc.Doc.retokenize(self)->Retokenizer
spacy.tokens.doc.Doc.sents(self)->Iterator[Span]
spacy.tokens.doc.Doc.set_ents(self,entities:List[Span],*,blocked:Optional[List[Span]]=...,missing:Optional[List[Span]]=...,outside:Optional[List[Span]]=...,default:str=...)->None
spacy.tokens.doc.Doc.set_extension(cls,name:str,default:Optional[Any]=...,getter:Optional[Callable[[Doc],Any]]=...,setter:Optional[Callable[[Doc,Any],None]]=...,method:Optional[DocMethod]=...,force:bool=...)->None
spacy.tokens.doc.Doc.similarity(self,other:Union[Doc,Span,Token,Lexeme])->float
spacy.tokens.doc.Doc.text(self)->str
spacy.tokens.doc.Doc.text_with_ws(self)->str
spacy.tokens.doc.Doc.to_array(self,py_attr_ids:Union[int,str,List[Union[int,str]]])->np.ndarray[Any, np.dtype[np.float64]]
spacy.tokens.doc.Doc.to_bytes(self,*,exclude:Iterable[str]=...)->bytes
spacy.tokens.doc.Doc.to_dict(self,*,exclude:Iterable[str]=...)->Dict[str, Any]
spacy.tokens.doc.Doc.to_disk(self,path:Union[str,Path],*,exclude:Iterable[str]=...)->None
spacy.tokens.doc.Doc.to_json(self,underscore:Optional[List[str]]=...)->Dict[str, Any]
spacy.tokens.doc.Doc.to_utf8_array(self,nr_char:int=...)->Ints2d
spacy.tokens.doc.DocMethod(self:Doc,*args:Any,**kwargs:Any)
spacy.tokens.doc.DocMethod.__call__(self:Doc,*args:Any,**kwargs:Any)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tokens/underscore.py----------------------------------------
A:spacy.tokens.underscore.extensions->list(self._extensions.keys())
A:spacy.tokens.underscore.method_partial->functools.partial(method, self._obj)
A:spacy.tokens.underscore.key->self._get_key(name)
A:spacy.tokens.underscore.new_default->copy.copy(default)
A:spacy.tokens.underscore.default->kwargs.get('default')
A:spacy.tokens.underscore.getter->kwargs.get('getter')
A:spacy.tokens.underscore.setter->kwargs.get('setter')
A:spacy.tokens.underscore.method->kwargs.get('method')
A:spacy.tokens.underscore.nr_defined->sum((t is True for t in valid_opts))
spacy.tokens.underscore.Underscore(self,extensions:Dict[str,Any],obj:Union['Doc','Span','Token'],start:Optional[int]=None,end:Optional[int]=None)
spacy.tokens.underscore.Underscore.__dir__(self)->List[str]
spacy.tokens.underscore.Underscore.__getattr__(self,name:str)->Any
spacy.tokens.underscore.Underscore.__init__(self,extensions:Dict[str,Any],obj:Union['Doc','Span','Token'],start:Optional[int]=None,end:Optional[int]=None)
spacy.tokens.underscore.Underscore.__setattr__(self,name:str,value:Any)
spacy.tokens.underscore.Underscore._get_key(self,name:str)->Tuple[str, str, Optional[int], Optional[int]]
spacy.tokens.underscore.Underscore.get(self,name:str)->Any
spacy.tokens.underscore.Underscore.get_state(cls)->Tuple[Dict[Any, Any], Dict[Any, Any], Dict[Any, Any]]
spacy.tokens.underscore.Underscore.has(self,name:str)->bool
spacy.tokens.underscore.Underscore.load_state(cls,state:Tuple[Dict[Any,Any],Dict[Any,Any],Dict[Any,Any]])->None
spacy.tokens.underscore.Underscore.set(self,name:str,value:Any)
spacy.tokens.underscore.get_ext_args(**kwargs:Any)
spacy.tokens.underscore.is_writable_attr(ext)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tokens/_dict_proxies.py----------------------------------------
A:spacy.tokens._dict_proxies._EMPTY_BYTES->srsly.msgpack_dumps([])
A:spacy.tokens._dict_proxies.self.doc_ref->weakref.ref(doc)
A:spacy.tokens._dict_proxies.value->self._make_span_group(key, value)
A:spacy.tokens._dict_proxies.doc->self.doc_ref()
A:spacy.tokens._dict_proxies.default->self._make_span_group(key, spans)
A:spacy.tokens._dict_proxies.group->SpanGroup(doc).from_bytes(value_bytes)
A:spacy.tokens._dict_proxies.self[key]->SpanGroup(doc).from_bytes(value_bytes).copy()
spacy.tokens._dict_proxies.SpanGroups(self,doc:'Doc',items:Iterable[Tuple[str,SpanGroup]]=tuple())
spacy.tokens._dict_proxies.SpanGroups.__init__(self,doc:'Doc',items:Iterable[Tuple[str,SpanGroup]]=tuple())
spacy.tokens._dict_proxies.SpanGroups.__setitem__(self,key:str,value:Union[SpanGroup,Iterable['Span']])->None
spacy.tokens._dict_proxies.SpanGroups._ensure_doc(self)->'Doc'
spacy.tokens._dict_proxies.SpanGroups._make_span_group(self,name:str,spans:Iterable['Span'])->SpanGroup
spacy.tokens._dict_proxies.SpanGroups.copy(self,doc:Optional['Doc']=None)->'SpanGroups'
spacy.tokens._dict_proxies.SpanGroups.from_bytes(self,bytes_data:bytes)->'SpanGroups'
spacy.tokens._dict_proxies.SpanGroups.setdefault(self,key,default=None)
spacy.tokens._dict_proxies.SpanGroups.to_bytes(self)->bytes


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tokens/_serialize.py----------------------------------------
A:spacy.tokens._serialize.attrs->sorted(int_attrs)
A:spacy.tokens._serialize.array->array.reshape((array.shape[0], 1)).reshape((array.shape[0], 1))
A:spacy.tokens._serialize.spaces->spaces.reshape((spaces.shape[0], 1)).reshape((spaces.shape[0], 1))
A:spacy.tokens._serialize.orth_col->self.attrs.index(ORTH)
A:spacy.tokens._serialize.doc->doc.from_array(self.attrs, tokens).from_array(self.attrs, tokens)
A:spacy.tokens._serialize.user_data->srsly.msgpack_loads(self.user_data[i], use_list=False)
A:spacy.tokens._serialize.msg->srsly.msgpack_loads(zlib.decompress(bytes_data))
A:spacy.tokens._serialize.self.strings->set(msg['strings'])
A:spacy.tokens._serialize.lengths->numpy.frombuffer(msg['lengths'], dtype='int32')
A:spacy.tokens._serialize.flat_spaces->flat_spaces.reshape((flat_spaces.size, 1)).reshape((flat_spaces.size, 1))
A:spacy.tokens._serialize.flat_tokens->flat_tokens.reshape(shape).reshape(shape)
A:spacy.tokens._serialize.self.tokens->NumpyOps().unflatten(flat_tokens, lengths)
A:spacy.tokens._serialize.self.spaces->NumpyOps().unflatten(flat_spaces, lengths)
A:spacy.tokens._serialize.self.span_groups->srsly.msgpack_loads(zlib.decompress(bytes_data)).get('span_groups', [b'' for _ in lengths])
A:spacy.tokens._serialize.self.flags->srsly.msgpack_loads(zlib.decompress(bytes_data)).get('flags', [{} for _ in lengths])
A:spacy.tokens._serialize.self.user_data->list(msg['user_data'])
A:spacy.tokens._serialize.path->ensure_path(path)
A:spacy.tokens._serialize.doc_bin->DocBin(store_user_data=True).from_bytes(byte_string)
spacy.tokens.DocBin(self,attrs:Iterable[str]=ALL_ATTRS,store_user_data:bool=False,docs:Iterable[Doc]=SimpleFrozenList())
spacy.tokens.DocBin.__len__(self)->int
spacy.tokens.DocBin.add(self,doc:Doc)->None
spacy.tokens.DocBin.from_bytes(self,bytes_data:bytes)->'DocBin'
spacy.tokens.DocBin.from_disk(self,path:Union[str,Path])->'DocBin'
spacy.tokens.DocBin.get_docs(self,vocab:Vocab)->Iterator[Doc]
spacy.tokens.DocBin.merge(self,other:'DocBin')->None
spacy.tokens.DocBin.to_bytes(self)->bytes
spacy.tokens.DocBin.to_disk(self,path:Union[str,Path])->None
spacy.tokens._serialize.DocBin(self,attrs:Iterable[str]=ALL_ATTRS,store_user_data:bool=False,docs:Iterable[Doc]=SimpleFrozenList())
spacy.tokens._serialize.DocBin.__init__(self,attrs:Iterable[str]=ALL_ATTRS,store_user_data:bool=False,docs:Iterable[Doc]=SimpleFrozenList())
spacy.tokens._serialize.DocBin.__len__(self)->int
spacy.tokens._serialize.DocBin.add(self,doc:Doc)->None
spacy.tokens._serialize.DocBin.from_bytes(self,bytes_data:bytes)->'DocBin'
spacy.tokens._serialize.DocBin.from_disk(self,path:Union[str,Path])->'DocBin'
spacy.tokens._serialize.DocBin.get_docs(self,vocab:Vocab)->Iterator[Doc]
spacy.tokens._serialize.DocBin.merge(self,other:'DocBin')->None
spacy.tokens._serialize.DocBin.to_bytes(self)->bytes
spacy.tokens._serialize.DocBin.to_disk(self,path:Union[str,Path])->None
spacy.tokens._serialize.merge_bins(bins)
spacy.tokens._serialize.pickle_bin(doc_bin)
spacy.tokens._serialize.unpickle_bin(byte_string)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/tokens/token.pyi----------------------------------------
spacy.tokens.Token(self,vocab:Vocab,doc:Doc,offset:int)
spacy.tokens.Token._(self)->Underscore
spacy.tokens.Token.__bytes__(self)->bytes
spacy.tokens.Token.__eq__(self,other:Any)->bool
spacy.tokens.Token.__ge__(self,other:Any)->bool
spacy.tokens.Token.__gt__(self,other:Any)->bool
spacy.tokens.Token.__hash__(self)->int
spacy.tokens.Token.__le__(self,other:Any)->bool
spacy.tokens.Token.__len__(self)->int
spacy.tokens.Token.__lt__(self,other:Any)->bool
spacy.tokens.Token.__ne__(self,other:Any)->bool
spacy.tokens.Token.__repr__(self)->str
spacy.tokens.Token.__str__(self)->str
spacy.tokens.Token.__unicode__(self)->str
spacy.tokens.Token.ancestors(self)->Iterator[Token]
spacy.tokens.Token.children(self)->Iterator[Token]
spacy.tokens.Token.cluster(self)->int
spacy.tokens.Token.conjuncts(self)->Tuple[Token]
spacy.tokens.Token.ent_iob(self)->int
spacy.tokens.Token.ent_iob_(self)->str
spacy.tokens.Token.get_extension(cls,name:str)->Tuple[Optional[Any], Optional[TokenMethod], Optional[Callable[[Token], Any]], Optional[Callable[[Token, Any], None]]]
spacy.tokens.Token.has_dep(self)->bool
spacy.tokens.Token.has_extension(cls,name:str)->bool
spacy.tokens.Token.has_head(self)->bool
spacy.tokens.Token.has_morph(self)->bool
spacy.tokens.Token.has_vector(self)->bool
spacy.tokens.Token.idx(self)->int
spacy.tokens.Token.iob_strings(cls)->Tuple[str]
spacy.tokens.Token.is_alpha(self)->bool
spacy.tokens.Token.is_ancestor(self,descendant:Token)->bool
spacy.tokens.Token.is_ascii(self)->bool
spacy.tokens.Token.is_bracket(self)->bool
spacy.tokens.Token.is_currency(self)->bool
spacy.tokens.Token.is_digit(self)->bool
spacy.tokens.Token.is_left_punct(self)->bool
spacy.tokens.Token.is_lower(self)->bool
spacy.tokens.Token.is_oov(self)->bool
spacy.tokens.Token.is_punct(self)->bool
spacy.tokens.Token.is_quote(self)->bool
spacy.tokens.Token.is_right_punct(self)->bool
spacy.tokens.Token.is_space(self)->bool
spacy.tokens.Token.is_stop(self)->bool
spacy.tokens.Token.is_title(self)->bool
spacy.tokens.Token.is_upper(self)->bool
spacy.tokens.Token.lang(self)->int
spacy.tokens.Token.lang_(self)->str
spacy.tokens.Token.left_edge(self)->Token
spacy.tokens.Token.lefts(self)->Iterator[Token]
spacy.tokens.Token.lex(self)->Lexeme
spacy.tokens.Token.lex_id(self)->int
spacy.tokens.Token.like_email(self)->bool
spacy.tokens.Token.like_num(self)->bool
spacy.tokens.Token.like_url(self)->bool
spacy.tokens.Token.lower(self)->int
spacy.tokens.Token.lower_(self)->str
spacy.tokens.Token.n_lefts(self)->int
spacy.tokens.Token.n_rights(self)->int
spacy.tokens.Token.nbor(self,i:int=...)->Token
spacy.tokens.Token.norm(self)->int
spacy.tokens.Token.orth(self)->int
spacy.tokens.Token.orth_(self)->str
spacy.tokens.Token.prefix(self)->int
spacy.tokens.Token.prefix_(self)->str
spacy.tokens.Token.prob(self)->float
spacy.tokens.Token.rank(self)->int
spacy.tokens.Token.remove_extension(cls,name:str)->Tuple[Optional[Any], Optional[TokenMethod], Optional[Callable[[Token], Any]], Optional[Callable[[Token, Any], None]]]
spacy.tokens.Token.right_edge(self)->Token
spacy.tokens.Token.rights(self)->Iterator[Token]
spacy.tokens.Token.sent(self)->Span
spacy.tokens.Token.sentiment(self)->float
spacy.tokens.Token.set_extension(cls,name:str,default:Optional[Any]=...,getter:Optional[Callable[[Token],Any]]=...,setter:Optional[Callable[[Token,Any],None]]=...,method:Optional[TokenMethod]=...,force:bool=...)->None
spacy.tokens.Token.shape(self)->int
spacy.tokens.Token.shape_(self)->str
spacy.tokens.Token.similarity(self,other:Union[Doc,Span,Token,Lexeme])->float
spacy.tokens.Token.subtree(self)->Iterator[Token]
spacy.tokens.Token.suffix(self)->int
spacy.tokens.Token.suffix_(self)->str
spacy.tokens.Token.tensor(self)->Optional[FloatsXd]
spacy.tokens.Token.text(self)->str
spacy.tokens.Token.text_with_ws(self)->str
spacy.tokens.Token.vector(self)->Floats1d
spacy.tokens.Token.vector_norm(self)->float
spacy.tokens.Token.whitespace_(self)->str
spacy.tokens.TokenMethod(self:Token,*args:Any,**kwargs:Any)
spacy.tokens.token.Token(self,vocab:Vocab,doc:Doc,offset:int)
spacy.tokens.token.Token._(self)->Underscore
spacy.tokens.token.Token.__bytes__(self)->bytes
spacy.tokens.token.Token.__eq__(self,other:Any)->bool
spacy.tokens.token.Token.__ge__(self,other:Any)->bool
spacy.tokens.token.Token.__gt__(self,other:Any)->bool
spacy.tokens.token.Token.__hash__(self)->int
spacy.tokens.token.Token.__init__(self,vocab:Vocab,doc:Doc,offset:int)
spacy.tokens.token.Token.__le__(self,other:Any)->bool
spacy.tokens.token.Token.__len__(self)->int
spacy.tokens.token.Token.__lt__(self,other:Any)->bool
spacy.tokens.token.Token.__ne__(self,other:Any)->bool
spacy.tokens.token.Token.__repr__(self)->str
spacy.tokens.token.Token.__str__(self)->str
spacy.tokens.token.Token.__unicode__(self)->str
spacy.tokens.token.Token.ancestors(self)->Iterator[Token]
spacy.tokens.token.Token.children(self)->Iterator[Token]
spacy.tokens.token.Token.cluster(self)->int
spacy.tokens.token.Token.conjuncts(self)->Tuple[Token]
spacy.tokens.token.Token.ent_iob(self)->int
spacy.tokens.token.Token.ent_iob_(self)->str
spacy.tokens.token.Token.get_extension(cls,name:str)->Tuple[Optional[Any], Optional[TokenMethod], Optional[Callable[[Token], Any]], Optional[Callable[[Token, Any], None]]]
spacy.tokens.token.Token.has_dep(self)->bool
spacy.tokens.token.Token.has_extension(cls,name:str)->bool
spacy.tokens.token.Token.has_head(self)->bool
spacy.tokens.token.Token.has_morph(self)->bool
spacy.tokens.token.Token.has_vector(self)->bool
spacy.tokens.token.Token.idx(self)->int
spacy.tokens.token.Token.iob_strings(cls)->Tuple[str]
spacy.tokens.token.Token.is_alpha(self)->bool
spacy.tokens.token.Token.is_ancestor(self,descendant:Token)->bool
spacy.tokens.token.Token.is_ascii(self)->bool
spacy.tokens.token.Token.is_bracket(self)->bool
spacy.tokens.token.Token.is_currency(self)->bool
spacy.tokens.token.Token.is_digit(self)->bool
spacy.tokens.token.Token.is_left_punct(self)->bool
spacy.tokens.token.Token.is_lower(self)->bool
spacy.tokens.token.Token.is_oov(self)->bool
spacy.tokens.token.Token.is_punct(self)->bool
spacy.tokens.token.Token.is_quote(self)->bool
spacy.tokens.token.Token.is_right_punct(self)->bool
spacy.tokens.token.Token.is_space(self)->bool
spacy.tokens.token.Token.is_stop(self)->bool
spacy.tokens.token.Token.is_title(self)->bool
spacy.tokens.token.Token.is_upper(self)->bool
spacy.tokens.token.Token.lang(self)->int
spacy.tokens.token.Token.lang_(self)->str
spacy.tokens.token.Token.left_edge(self)->Token
spacy.tokens.token.Token.lefts(self)->Iterator[Token]
spacy.tokens.token.Token.lex(self)->Lexeme
spacy.tokens.token.Token.lex_id(self)->int
spacy.tokens.token.Token.like_email(self)->bool
spacy.tokens.token.Token.like_num(self)->bool
spacy.tokens.token.Token.like_url(self)->bool
spacy.tokens.token.Token.lower(self)->int
spacy.tokens.token.Token.lower_(self)->str
spacy.tokens.token.Token.n_lefts(self)->int
spacy.tokens.token.Token.n_rights(self)->int
spacy.tokens.token.Token.nbor(self,i:int=...)->Token
spacy.tokens.token.Token.norm(self)->int
spacy.tokens.token.Token.orth(self)->int
spacy.tokens.token.Token.orth_(self)->str
spacy.tokens.token.Token.prefix(self)->int
spacy.tokens.token.Token.prefix_(self)->str
spacy.tokens.token.Token.prob(self)->float
spacy.tokens.token.Token.rank(self)->int
spacy.tokens.token.Token.remove_extension(cls,name:str)->Tuple[Optional[Any], Optional[TokenMethod], Optional[Callable[[Token], Any]], Optional[Callable[[Token, Any], None]]]
spacy.tokens.token.Token.right_edge(self)->Token
spacy.tokens.token.Token.rights(self)->Iterator[Token]
spacy.tokens.token.Token.sent(self)->Span
spacy.tokens.token.Token.sentiment(self)->float
spacy.tokens.token.Token.set_extension(cls,name:str,default:Optional[Any]=...,getter:Optional[Callable[[Token],Any]]=...,setter:Optional[Callable[[Token,Any],None]]=...,method:Optional[TokenMethod]=...,force:bool=...)->None
spacy.tokens.token.Token.shape(self)->int
spacy.tokens.token.Token.shape_(self)->str
spacy.tokens.token.Token.similarity(self,other:Union[Doc,Span,Token,Lexeme])->float
spacy.tokens.token.Token.subtree(self)->Iterator[Token]
spacy.tokens.token.Token.suffix(self)->int
spacy.tokens.token.Token.suffix_(self)->str
spacy.tokens.token.Token.tensor(self)->Optional[FloatsXd]
spacy.tokens.token.Token.text(self)->str
spacy.tokens.token.Token.text_with_ws(self)->str
spacy.tokens.token.Token.vector(self)->Floats1d
spacy.tokens.token.Token.vector_norm(self)->float
spacy.tokens.token.Token.whitespace_(self)->str
spacy.tokens.token.TokenMethod(self:Token,*args:Any,**kwargs:Any)
spacy.tokens.token.TokenMethod.__call__(self:Token,*args:Any,**kwargs:Any)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/displacy/__init__.py----------------------------------------
A:spacy.displacy.__init__.renderer->renderer_func(options=options)
A:spacy.displacy.__init__.doc['ents']->sorted(doc['ents'], key=lambda x: (x['start'], x['end']))
A:spacy.displacy.__init__._html['parsed']->renderer_func(options=options).render(parsed, page=page, minify=minify).strip()
A:spacy.displacy.__init__.html->RENDER_WRAPPER(html)
A:spacy.displacy.__init__.port->find_available_port(port, host, auto_select_port)
A:spacy.displacy.__init__.httpd->wsgiref.simple_server.make_server(host, port, app)
A:spacy.displacy.__init__.res->renderer_func(options=options).render(parsed, page=page, minify=minify).strip().encode(encoding='utf-8')
A:spacy.displacy.__init__.orig_doc->orig_doc.as_doc().as_doc()
A:spacy.displacy.__init__.doc->Doc(orig_doc.vocab).from_bytes(orig_doc.to_bytes(exclude=['user_data', 'user_hooks']))
A:spacy.displacy.__init__.fine_grained->options.get('fine_grained')
A:spacy.displacy.__init__.add_lemma->options.get('add_lemma')
A:spacy.displacy.__init__.kb_url_template->options.get('kb_url_template', None)
A:spacy.displacy.__init__.settings->get_doc_settings(doc)
A:spacy.displacy.__init__.spans_key->options.get('spans_key', 'sc')
A:spacy.displacy.__init__.keys->list(doc.spans.keys())
spacy.displacy.__init__.app(environ,start_response)
spacy.displacy.__init__.get_doc_settings(doc:Doc)->Dict[str, Any]
spacy.displacy.__init__.parse_deps(orig_doc:Union[Doc,Span],options:Dict[str,Any]={})->Dict[str, Any]
spacy.displacy.__init__.parse_ents(doc:Doc,options:Dict[str,Any]={})->Dict[str, Any]
spacy.displacy.__init__.parse_spans(doc:Doc,options:Dict[str,Any]={})->Dict[str, Any]
spacy.displacy.__init__.render(docs:Union[Iterable[Union[Doc,Span,dict]],Doc,Span,dict],style:str='dep',page:bool=False,minify:bool=False,jupyter:Optional[bool]=None,options:Dict[str,Any]={},manual:bool=False)->str
spacy.displacy.__init__.serve(docs:Union[Iterable[Doc],Doc],style:str='dep',page:bool=True,minify:bool=False,options:Dict[str,Any]={},manual:bool=False,port:int=5000,host:str='0.0.0.0',auto_select_port:bool=False)->None
spacy.displacy.__init__.set_render_wrapper(func:Callable[[str],str])->None


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/displacy/render.py----------------------------------------
A:spacy.displacy.render.colors->dict(DEFAULT_LABEL_COLORS)
A:spacy.displacy.render.user_colors->util.registry.displacy_colors.get_all()
A:spacy.displacy.render.user_color->user_color()
A:spacy.displacy.render.self.top_offset->options.get('top_offset', 40)
A:spacy.displacy.render.self.span_label_offset->options.get('span_label_offset', 20)
A:spacy.displacy.render.self.offset_step->options.get('top_offset_step', 17)
A:spacy.displacy.render.template->options.get('template')
A:spacy.displacy.render.settings->p.get('settings', {})
A:spacy.displacy.render.self.direction->p.get('settings', {}).get('direction', DEFAULT_DIR)
A:spacy.displacy.render.self.lang->p.get('settings', {}).get('lang', DEFAULT_LANG)
A:spacy.displacy.render.docs->''.join([TPL_FIGURE.format(content=doc) for doc in rendered])
A:spacy.displacy.render.markup->templates.TPL_ENTS.format(content=markup, dir=self.direction)
A:spacy.displacy.render.per_token_info->self._assemble_per_token_info(tokens, spans)
A:spacy.displacy.render.spans->sorted(spans, key=lambda s: (s['start_token'], -(s['end_token'] - s['start_token']), s['label']))
A:spacy.displacy.render.kb_id->span.get('kb_id', '')
A:spacy.displacy.render.kb_url->span.get('kb_url', '#')
A:spacy.displacy.render.entities->sorted(token['entities'], key=lambda d: d['render_slot'])
A:spacy.displacy.render.slices->self._get_span_slices(token['entities'])
A:spacy.displacy.render.starts->self._get_span_starts(token['entities'])
A:spacy.displacy.render.color->self.colors.get(label.upper(), self.default_color)
A:spacy.displacy.render.span_slice->self.span_slice_template.format(bg=color, top_offset=top_offset)
A:spacy.displacy.render.self.compact->options.get('compact', False)
A:spacy.displacy.render.self.word_spacing->options.get('word_spacing', 45)
A:spacy.displacy.render.self.arrow_spacing->options.get('arrow_spacing', 12 if self.compact else 20)
A:spacy.displacy.render.self.arrow_width->options.get('arrow_width', 6 if self.compact else 10)
A:spacy.displacy.render.self.arrow_stroke->options.get('arrow_stroke', 2)
A:spacy.displacy.render.self.distance->options.get('distance', 150 if self.compact else 175)
A:spacy.displacy.render.self.offset_x->options.get('offset_x', 50)
A:spacy.displacy.render.self.color->options.get('color', '#000000')
A:spacy.displacy.render.self.bg->options.get('bg', '#ffffff')
A:spacy.displacy.render.self.font->options.get('font', 'Arial')
A:spacy.displacy.render.svg->self.render_svg(render_id, p['words'], p['arcs'])
A:spacy.displacy.render.content->''.join([TPL_FIGURE.format(content=svg) for svg in rendered])
A:spacy.displacy.render.self.levels->self.get_levels(arcs)
A:spacy.displacy.render.self.highest_level->max(self.levels.values(), default=0)
A:spacy.displacy.render.html_text->escape_html(text)
A:spacy.displacy.render.error_args->dict(start=start, end=end, label=label, dir=direction)
A:spacy.displacy.render.arrowhead->self.get_arrowhead(direction, x_start, y, x_end)
A:spacy.displacy.render.arc->self.get_arc(x_start, y, y_curve, x_end)
A:spacy.displacy.render.length->max([arc['end'] for arc in arcs], default=0)
A:spacy.displacy.render.self.ents->options.get('ents', None)
A:spacy.displacy.render.additional_params->span.get('params', {})
A:spacy.displacy.render.entity->escape_html(text[start:end])
A:spacy.displacy.render.fragments->text[offset:].split('\n')
spacy.displacy.DependencyRenderer(self,options:Dict[str,Any]={})
spacy.displacy.DependencyRenderer.get_arc(self,x_start:int,y:int,y_curve:int,x_end:int)->str
spacy.displacy.DependencyRenderer.get_arrowhead(self,direction:str,x:int,y:int,end:int)->str
spacy.displacy.DependencyRenderer.get_levels(self,arcs:List[Dict[str,Any]])->Dict[Tuple[int, int, str], int]
spacy.displacy.DependencyRenderer.render(self,parsed:List[Dict[str,Any]],page:bool=False,minify:bool=False)->str
spacy.displacy.DependencyRenderer.render_arrow(self,label:str,start:int,end:int,direction:str,i:int)->str
spacy.displacy.DependencyRenderer.render_svg(self,render_id:Union[int,str],words:List[Dict[str,Any]],arcs:List[Dict[str,Any]])->str
spacy.displacy.DependencyRenderer.render_word(self,text:str,tag:str,lemma:str,i:int)->str
spacy.displacy.EntityRenderer(self,options:Dict[str,Any]={})
spacy.displacy.EntityRenderer.render(self,parsed:List[Dict[str,Any]],page:bool=False,minify:bool=False)->str
spacy.displacy.EntityRenderer.render_ents(self,text:str,spans:List[Dict[str,Any]],title:Optional[str])->str
spacy.displacy.SpanRenderer(self,options:Dict[str,Any]={})
spacy.displacy.SpanRenderer._assemble_per_token_info(tokens:List[str],spans:List[Dict[str,Any]])->List[Dict[str, List[Dict[str, Any]]]]
spacy.displacy.SpanRenderer._get_span_slices(self,entities:List[Dict])->str
spacy.displacy.SpanRenderer._get_span_starts(self,entities:List[Dict])->str
spacy.displacy.SpanRenderer._render_markup(self,per_token_info:List[Dict[str,Any]])->str
spacy.displacy.SpanRenderer.render(self,parsed:List[Dict[str,Any]],page:bool=False,minify:bool=False)->str
spacy.displacy.SpanRenderer.render_spans(self,tokens:List[str],spans:List[Dict[str,Any]],title:Optional[str])->str
spacy.displacy.render.DependencyRenderer(self,options:Dict[str,Any]={})
spacy.displacy.render.DependencyRenderer.__init__(self,options:Dict[str,Any]={})
spacy.displacy.render.DependencyRenderer.get_arc(self,x_start:int,y:int,y_curve:int,x_end:int)->str
spacy.displacy.render.DependencyRenderer.get_arrowhead(self,direction:str,x:int,y:int,end:int)->str
spacy.displacy.render.DependencyRenderer.get_levels(self,arcs:List[Dict[str,Any]])->Dict[Tuple[int, int, str], int]
spacy.displacy.render.DependencyRenderer.render(self,parsed:List[Dict[str,Any]],page:bool=False,minify:bool=False)->str
spacy.displacy.render.DependencyRenderer.render_arrow(self,label:str,start:int,end:int,direction:str,i:int)->str
spacy.displacy.render.DependencyRenderer.render_svg(self,render_id:Union[int,str],words:List[Dict[str,Any]],arcs:List[Dict[str,Any]])->str
spacy.displacy.render.DependencyRenderer.render_word(self,text:str,tag:str,lemma:str,i:int)->str
spacy.displacy.render.EntityRenderer(self,options:Dict[str,Any]={})
spacy.displacy.render.EntityRenderer.__init__(self,options:Dict[str,Any]={})
spacy.displacy.render.EntityRenderer.render(self,parsed:List[Dict[str,Any]],page:bool=False,minify:bool=False)->str
spacy.displacy.render.EntityRenderer.render_ents(self,text:str,spans:List[Dict[str,Any]],title:Optional[str])->str
spacy.displacy.render.SpanRenderer(self,options:Dict[str,Any]={})
spacy.displacy.render.SpanRenderer.__init__(self,options:Dict[str,Any]={})
spacy.displacy.render.SpanRenderer._assemble_per_token_info(tokens:List[str],spans:List[Dict[str,Any]])->List[Dict[str, List[Dict[str, Any]]]]
spacy.displacy.render.SpanRenderer._get_span_slices(self,entities:List[Dict])->str
spacy.displacy.render.SpanRenderer._get_span_starts(self,entities:List[Dict])->str
spacy.displacy.render.SpanRenderer._render_markup(self,per_token_info:List[Dict[str,Any]])->str
spacy.displacy.render.SpanRenderer.render(self,parsed:List[Dict[str,Any]],page:bool=False,minify:bool=False)->str
spacy.displacy.render.SpanRenderer.render_spans(self,tokens:List[str],spans:List[Dict[str,Any]],title:Optional[str])->str


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/displacy/templates.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/cli/__init__.py----------------------------------------
spacy.cli.__init__.link(*args,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/cli/_util.py----------------------------------------
A:spacy.cli._util.app->typer.Typer(name=NAME, help=HELP)
A:spacy.cli._util.benchmark_cli->typer.Typer(name='benchmark', help=BENCHMARK_HELP, no_args_is_help=True)
A:spacy.cli._util.debug_cli->typer.Typer(name='debug', help=DEBUG_HELP, no_args_is_help=True)
A:spacy.cli._util.init_cli->typer.Typer(name='init', help=INIT_HELP, no_args_is_help=True)
A:spacy.cli._util.command->get_command(app)
A:spacy.cli._util.env_overrides->_parse_overrides(split_arg_string(env_string))
A:spacy.cli._util.cli_overrides->_parse_overrides(args, is_cli=True)
A:spacy.cli._util.opt->opt.replace('-', '_').replace('-', '_')
A:spacy.cli._util.(opt, value)->opt.replace('-', '_').replace('-', '_').split('=', 1)
A:spacy.cli._util.value->args.pop(0)
A:spacy.cli._util.result[opt]->_parse_override(value)
A:spacy.cli._util.err->e.from_error(e, title='', desc=desc, show_config=show_config)
A:spacy.cli._util.ret->run_command('git --version', capture=True)
A:spacy.cli._util.stdout->run_command('git --version', capture=True).stdout.strip()
A:spacy.cli._util.version->stdout[11:].strip().split('.')
A:spacy.cli._util.p->int(p)
A:spacy.cli._util.local_msg->Printer(no_print=silent, pretty=not silent)
A:spacy.cli._util.seen->set()
spacy.cli._util._format_number(number:Union[int,float],ndigits:int=2)->str
spacy.cli._util._parse_override(value:Any)->Any
spacy.cli._util._parse_overrides(args:List[str],is_cli:bool=False)->Dict[str, Any]
spacy.cli._util.get_git_version(error:str="Couldnotrun'git'.Makesureit'sinstalledandtheexecutableisavailable.")->Tuple[int, int]
spacy.cli._util.import_code(code_path:Optional[Union[Path,str]])->None
spacy.cli._util.parse_config_overrides(args:List[str],env_var:Optional[str]=ENV_VARS.CONFIG_OVERRIDES)->Dict[str, Any]
spacy.cli._util.setup_cli()->None
spacy.cli._util.setup_gpu(use_gpu:int,silent=None)->None
spacy.cli._util.show_validation_error(file_path:Optional[Union[str,Path]]=None,*,title:Optional[str]=None,desc:str='',show_config:Optional[bool]=None,hint_fill:bool=True)
spacy.cli._util.string_to_list(value:str,intify:bool=False)->Union[List[str], List[int]]
spacy.cli._util.walk_directory(path:Path,suffix:Optional[str]=None)->List[Path]
spacy.cli.setup_cli()->None


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/cli/find_threshold.py----------------------------------------
A:spacy.cli.find_threshold.data_path->util.ensure_path(data_path)
A:spacy.cli.find_threshold.nlp->util.load_model(model, config=set_nested_item(filter_config(nlp.config, config_keys_full, '.'.join(config_keys_full)).copy(), config_keys_full, threshold))
A:spacy.cli.find_threshold.pipe->util.load_model(model, config=set_nested_item(filter_config(nlp.config, config_keys_full, '.'.join(config_keys_full)).copy(), config_keys_full, threshold)).get_pipe(pipe_name)
A:spacy.cli.find_threshold.corpus->Corpus(data_path, gold_preproc=gold_preproc)
A:spacy.cli.find_threshold.dev_dataset->list(corpus(nlp))
A:spacy.cli.find_threshold.config_keys->threshold_key.split('.')
A:spacy.cli.find_threshold.thresholds->numpy.linspace(0, 1, n_trials)
A:spacy.cli.find_threshold.eval_scores->util.load_model(model, config=set_nested_item(filter_config(nlp.config, config_keys_full, '.'.join(config_keys_full)).copy(), config_keys_full, threshold)).evaluate(dev_dataset)
A:spacy.cli.find_threshold.best_threshold->max(scores.keys(), key=lambda key: scores[key])
spacy.cli.find_threshold(model:str,data_path:Path,pipe_name:str,threshold_key:str,scores_key:str,*,n_trials:int=_DEFAULTS['n_trials'],use_gpu:int=_DEFAULTS['use_gpu'],gold_preproc:bool=_DEFAULTS['gold_preproc'],silent:bool=True)->Tuple[float, float, Dict[float, float]]
spacy.cli.find_threshold.find_threshold(model:str,data_path:Path,pipe_name:str,threshold_key:str,scores_key:str,*,n_trials:int=_DEFAULTS['n_trials'],use_gpu:int=_DEFAULTS['use_gpu'],gold_preproc:bool=_DEFAULTS['gold_preproc'],silent:bool=True)->Tuple[float, float, Dict[float, float]]
spacy.cli.find_threshold.find_threshold_cli(model:str=Arg(...,help='Modelnameorpath'),data_path:Path=Arg(...,help='Locationofbinaryevaluationdatain.spacyformat',exists=True),pipe_name:str=Arg(...,help='Nameofpipetoexaminethresholdsfor'),threshold_key:str=Arg(...,help="Keyofthresholdattributeincomponent'sconfiguration"),scores_key:str=Arg(...,help='Metrictooptimize'),n_trials:int=Opt(_DEFAULTS['n_trials'],'--n_trials','-n',help='Numberoftrialstodetermineoptimalthresholds'),code_path:Optional[Path]=Opt(None,'--code','-c',help='PathtoPythonfilewithadditionalcode(registeredfunctions)tobeimported'),use_gpu:int=Opt(_DEFAULTS['use_gpu'],'--gpu-id','-g',help='GPUIDor-1forCPU'),gold_preproc:bool=Opt(_DEFAULTS['gold_preproc'],'--gold-preproc','-G',help='Usegoldpreprocessing'),verbose:bool=Opt(False,'--verbose','-V','-VV',help='Displaymoreinformationfordebuggingpurposes'))
spacy.cli.find_threshold_cli(model:str=Arg(...,help='Modelnameorpath'),data_path:Path=Arg(...,help='Locationofbinaryevaluationdatain.spacyformat',exists=True),pipe_name:str=Arg(...,help='Nameofpipetoexaminethresholdsfor'),threshold_key:str=Arg(...,help="Keyofthresholdattributeincomponent'sconfiguration"),scores_key:str=Arg(...,help='Metrictooptimize'),n_trials:int=Opt(_DEFAULTS['n_trials'],'--n_trials','-n',help='Numberoftrialstodetermineoptimalthresholds'),code_path:Optional[Path]=Opt(None,'--code','-c',help='PathtoPythonfilewithadditionalcode(registeredfunctions)tobeimported'),use_gpu:int=Opt(_DEFAULTS['use_gpu'],'--gpu-id','-g',help='GPUIDor-1forCPU'),gold_preproc:bool=Opt(_DEFAULTS['gold_preproc'],'--gold-preproc','-G',help='Usegoldpreprocessing'),verbose:bool=Opt(False,'--verbose','-V','-VV',help='Displaymoreinformationfordebuggingpurposes'))


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/cli/init_pipeline.py----------------------------------------
A:spacy.cli.init_pipeline.nlp->init_nlp(config, use_gpu=use_gpu)
A:spacy.cli.init_pipeline.lex_attrs->srsly.read_jsonl(jsonl_loc)
A:spacy.cli.init_pipeline.overrides->parse_config_overrides(ctx.args)
A:spacy.cli.init_pipeline.config->util.load_config(config_path, overrides=overrides)
spacy.cli.init_pipeline._init_labels(nlp,output_path)
spacy.cli.init_pipeline.init_labels_cli(ctx:typer.Context,config_path:Path=Arg(...,help='Pathtoconfigfile',exists=True,allow_dash=True),output_path:Path=Arg(...,help='Outputdirectoryforthelabels'),code_path:Optional[Path]=Opt(None,'--code','-c',help='PathtoPythonfilewithadditionalcode(registeredfunctions)tobeimported'),verbose:bool=Opt(False,'--verbose','-V','-VV',help='Displaymoreinformationfordebuggingpurposes'),use_gpu:int=Opt(-1,'--gpu-id','-g',help='GPUIDor-1forCPU'))
spacy.cli.init_pipeline.init_pipeline_cli(ctx:typer.Context,config_path:Path=Arg(...,help='Pathtoconfigfile',exists=True,allow_dash=True),output_path:Path=Arg(...,help='Outputdirectoryfortheprepareddata'),code_path:Optional[Path]=Opt(None,'--code','-c',help='PathtoPythonfilewithadditionalcode(registeredfunctions)tobeimported'),verbose:bool=Opt(False,'--verbose','-V','-VV',help='Displaymoreinformationfordebuggingpurposes'),use_gpu:int=Opt(-1,'--gpu-id','-g',help='GPUIDor-1forCPU'))
spacy.cli.init_pipeline.init_vectors_cli(lang:str=Arg(...,help='Thelanguageofthenlpobjecttocreate'),vectors_loc:Path=Arg(...,help='VectorsfileinWord2Vecformat',exists=True),output_dir:Path=Arg(...,help='Pipelineoutputdirectory'),prune:int=Opt(-1,'--prune','-p',help='Optionalnumberofvectorstopruneto'),truncate:int=Opt(0,'--truncate','-t',help='Optionalnumberofvectorstotruncatetowhenreadinginvectorsfile'),mode:str=Opt('default','--mode','-m',help='Vectorsmode:defaultorfloret'),name:Optional[str]=Opt(None,'--name','-n',help='Optionalnameforthewordvectors,e.g.en_core_web_lg.vectors'),verbose:bool=Opt(False,'--verbose','-V','-VV',help='Displaymoreinformationfordebuggingpurposes'),jsonl_loc:Optional[Path]=Opt(None,'--lexemes-jsonl','-j',help='LocationofJSONL-formattedattributesfile',hidden=True),attr:str=Opt('ORTH','--attr','-a',help='Optionaltokenattributetouseforvectors,e.g.LOWERorNORM'))
spacy.cli.init_pipeline.update_lexemes(nlp:Language,jsonl_loc:Path)->None
spacy.cli.init_pipeline_cli(ctx:typer.Context,config_path:Path=Arg(...,help='Pathtoconfigfile',exists=True,allow_dash=True),output_path:Path=Arg(...,help='Outputdirectoryfortheprepareddata'),code_path:Optional[Path]=Opt(None,'--code','-c',help='PathtoPythonfilewithadditionalcode(registeredfunctions)tobeimported'),verbose:bool=Opt(False,'--verbose','-V','-VV',help='Displaymoreinformationfordebuggingpurposes'),use_gpu:int=Opt(-1,'--gpu-id','-g',help='GPUIDor-1forCPU'))


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/cli/debug_config.py----------------------------------------
A:spacy.cli.debug_config.overrides->parse_config_overrides(ctx.args)
A:spacy.cli.debug_config.config->util.load_model_from_config(config).config.interpolate()
A:spacy.cli.debug_config.nlp->util.load_model_from_config(config)
A:spacy.cli.debug_config.T->getattr(util.registry, reg_name).resolve(config['training'], schema=ConfigSchemaTraining)
A:spacy.cli.debug_config.variables->get_variables(config)
A:spacy.cli.debug_config.funcs->get_registered_funcs(config)
A:spacy.cli.debug_config.registry->getattr(util.registry, reg_name)
A:spacy.cli.debug_config.path->variable[2:-1].replace(':', '.')
A:spacy.cli.debug_config.info->getattr(util.registry, reg_name).find(value)
A:spacy.cli.debug_config.value->util.dot_to_object(config, path)
A:spacy.cli.debug_config.result[variable]->repr(value)
spacy.cli.debug_config(config_path:Path,*,overrides:Dict[str,Any]={},show_funcs:bool=False,show_vars:bool=False)
spacy.cli.debug_config.debug_config(config_path:Path,*,overrides:Dict[str,Any]={},show_funcs:bool=False,show_vars:bool=False)
spacy.cli.debug_config.debug_config_cli(ctx:typer.Context,config_path:Path=Arg(...,help='Pathtoconfigfile',exists=True,allow_dash=True),code_path:Optional[Path]=Opt(None,'--code-path','--code','-c',help='PathtoPythonfilewithadditionalcode(registeredfunctions)tobeimported'),show_funcs:bool=Opt(False,'--show-functions','-F',help='Showanoverviewofallregisteredfunctionsusedintheconfigandwheretheycomefrom(modules,filesetc.)'),show_vars:bool=Opt(False,'--show-variables','-V',help='Showanoverviewofallvariablesreferencedintheconfigandtheirvalues.ThiswillalsoreflectvariablesoverwrittenontheCLI.'))
spacy.cli.debug_config.get_registered_funcs(config:Config)->List[Dict[str, Optional[Union[str, int]]]]
spacy.cli.debug_config.get_variables(config:Config)->Dict[str, Any]
spacy.cli.debug_config_cli(ctx:typer.Context,config_path:Path=Arg(...,help='Pathtoconfigfile',exists=True,allow_dash=True),code_path:Optional[Path]=Opt(None,'--code-path','--code','-c',help='PathtoPythonfilewithadditionalcode(registeredfunctions)tobeimported'),show_funcs:bool=Opt(False,'--show-functions','-F',help='Showanoverviewofallregisteredfunctionsusedintheconfigandwheretheycomefrom(modules,filesetc.)'),show_vars:bool=Opt(False,'--show-variables','-V',help='Showanoverviewofallvariablesreferencedintheconfigandtheirvalues.ThiswillalsoreflectvariablesoverwrittenontheCLI.'))


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/cli/debug_model.py----------------------------------------
A:spacy.cli.debug_model.layers->print_settings.get('layers', '')
A:spacy.cli.debug_model.config_overrides->parse_config_overrides(ctx.args)
A:spacy.cli.debug_model.raw_config->util.load_config(config_path, overrides=config_overrides, interpolate=False)
A:spacy.cli.debug_model.config->util.get_lang_class(lang)().config.interpolate()
A:spacy.cli.debug_model.nlp->util.get_lang_class(lang)()
A:spacy.cli.debug_model.T->util.registry.resolve(config['training'], schema=ConfigSchemaTraining)
A:spacy.cli.debug_model.pipe->util.get_lang_class(lang)().get_pipe(component)
A:spacy.cli.debug_model.(train_corpus,)->resolve_dot_names(config, dot_names)
A:spacy.cli.debug_model.examples->list(itertools.islice(train_corpus(nlp), 5))
A:spacy.cli.debug_model.upstream_component->util.get_lang_class(lang)().get_pipe('transformer')
A:spacy.cli.debug_model.prediction->model.predict([ex.predicted for ex in examples])
A:spacy.cli.debug_model.parameters->print_settings.get('parameters', False)
A:spacy.cli.debug_model.dimensions->print_settings.get('dimensions', False)
A:spacy.cli.debug_model.gradients->print_settings.get('gradients', False)
A:spacy.cli.debug_model.attributes->print_settings.get('attributes', False)
A:spacy.cli.debug_model.print_value->_print_matrix(node.get_grad(name))
spacy.cli.debug_model(config,resolved_train_config,nlp,pipe,*,print_settings:Optional[Dict[str,Any]]=None)
spacy.cli.debug_model._get_docs(lang:str='en')
spacy.cli.debug_model._print_matrix(value)
spacy.cli.debug_model._print_model(model,print_settings)
spacy.cli.debug_model._sentences()
spacy.cli.debug_model._set_output_dim(model,nO)
spacy.cli.debug_model.debug_model(config,resolved_train_config,nlp,pipe,*,print_settings:Optional[Dict[str,Any]]=None)
spacy.cli.debug_model.debug_model_cli(ctx:typer.Context,config_path:Path=Arg(...,help='Pathtoconfigfile',exists=True,allow_dash=True),component:str=Arg(...,help='Nameofthepipelinecomponentofwhichthemodelshouldbeanalysed'),layers:str=Opt('','--layers','-l',help='Comma-separatednamesoflayerIDstoprint'),dimensions:bool=Opt(False,'--dimensions','-DIM',help='Showdimensions'),parameters:bool=Opt(False,'--parameters','-PAR',help='Showparameters'),gradients:bool=Opt(False,'--gradients','-GRAD',help='Showgradients'),attributes:bool=Opt(False,'--attributes','-ATTR',help='Showattributes'),P0:bool=Opt(False,'--print-step0','-P0',help='Printmodelbeforetraining'),P1:bool=Opt(False,'--print-step1','-P1',help='Printmodelafterinitialization'),P2:bool=Opt(False,'--print-step2','-P2',help='Printmodelaftertraining'),P3:bool=Opt(False,'--print-step3','-P3',help='Printfinalpredictions'),use_gpu:int=Opt(-1,'--gpu-id','-g',help='GPUIDor-1forCPU'))
spacy.cli.debug_model_cli(ctx:typer.Context,config_path:Path=Arg(...,help='Pathtoconfigfile',exists=True,allow_dash=True),component:str=Arg(...,help='Nameofthepipelinecomponentofwhichthemodelshouldbeanalysed'),layers:str=Opt('','--layers','-l',help='Comma-separatednamesoflayerIDstoprint'),dimensions:bool=Opt(False,'--dimensions','-DIM',help='Showdimensions'),parameters:bool=Opt(False,'--parameters','-PAR',help='Showparameters'),gradients:bool=Opt(False,'--gradients','-GRAD',help='Showgradients'),attributes:bool=Opt(False,'--attributes','-ATTR',help='Showattributes'),P0:bool=Opt(False,'--print-step0','-P0',help='Printmodelbeforetraining'),P1:bool=Opt(False,'--print-step1','-P1',help='Printmodelafterinitialization'),P2:bool=Opt(False,'--print-step2','-P2',help='Printmodelaftertraining'),P3:bool=Opt(False,'--print-step3','-P3',help='Printfinalpredictions'),use_gpu:int=Opt(-1,'--gpu-id','-g',help='GPUIDor-1forCPU'))


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/cli/benchmark_speed.py----------------------------------------
A:spacy.cli.benchmark_speed.nlp->util.load_model(model)
A:spacy.cli.benchmark_speed.corpus->Corpus(data_path)
A:spacy.cli.benchmark_speed.wps->benchmark(nlp, docs, n_batches, batch_size, not no_shuffle)
A:spacy.cli.benchmark_speed.self.start->time.perf_counter()
A:spacy.cli.benchmark_speed.self.q1->numpy.quantile(sample, 0.25)
A:spacy.cli.benchmark_speed.self.q2->numpy.quantile(sample, 0.5)
A:spacy.cli.benchmark_speed.self.q3->numpy.quantile(sample, 0.75)
A:spacy.cli.benchmark_speed.docs->util.load_model(model).pipe(tqdm(docs, unit='doc', disable=None), batch_size=batch_size)
A:spacy.cli.benchmark_speed.batch_docs->list(islice(docs, batch_size if batch_size else nlp.batch_size))
A:spacy.cli.benchmark_speed.n_tokens->count_tokens(batch_docs)
A:spacy.cli.benchmark_speed.mean->numpy.mean(sample)
A:spacy.cli.benchmark_speed.bootstrap_means->bootstrap(sample)
A:spacy.cli.benchmark_speed.quartiles->Quartiles(sample)
A:spacy.cli.benchmark_speed.n_outliers->numpy.sum((sample < quartiles.q1 - 1.5 * quartiles.iqr) | (sample > quartiles.q3 + 1.5 * quartiles.iqr))
A:spacy.cli.benchmark_speed.n_extreme_outliers->numpy.sum((sample < quartiles.q1 - 3.0 * quartiles.iqr) | (sample > quartiles.q3 + 3.0 * quartiles.iqr))
spacy.cli.benchmark_speed.Quartiles(self,sample:numpy.ndarray)
spacy.cli.benchmark_speed.Quartiles.__init__(self,sample:numpy.ndarray)
spacy.cli.benchmark_speed.annotate(nlp:Language,docs:List[Doc],batch_size:Optional[int])->numpy.ndarray
spacy.cli.benchmark_speed.benchmark(nlp:Language,docs:List[Doc],n_batches:int,batch_size:int,shuffle:bool)->numpy.ndarray
spacy.cli.benchmark_speed.benchmark_speed_cli(ctx:typer.Context,model:str=Arg(...,help='Modelnameorpath'),data_path:Path=Arg(...,help='Locationofbinaryevaluationdatain.spacyformat',exists=True),batch_size:Optional[int]=Opt(None,'--batch-size','-b',min=1,help='Overridethepipelinebatchsize'),no_shuffle:bool=Opt(False,'--no-shuffle',help='Donotshufflebenchmarkdata'),use_gpu:int=Opt(-1,'--gpu-id','-g',help='GPUIDor-1forCPU'),n_batches:int=Opt(50,'--batches',help='Minimumnumberofbatchestobenchmark',min=30),warmup_epochs:int=Opt(3,'--warmup','-w',min=0,help='Numberofiterationsoverthedataforwarmup'),code_path:Optional[Path]=Opt(None,'--code','-c',help='PathtoPythonfilewithadditionalcode(registeredfunctions)tobeimported'))
spacy.cli.benchmark_speed.bootstrap(x,statistic=numpy.mean,iterations=10000)->numpy.ndarray
spacy.cli.benchmark_speed.count_tokens(docs:Iterable[Doc])->int
spacy.cli.benchmark_speed.print_mean_with_ci(sample:numpy.ndarray)
spacy.cli.benchmark_speed.print_outliers(sample:numpy.ndarray)
spacy.cli.benchmark_speed.time_context
spacy.cli.benchmark_speed.time_context.__enter__(self)
spacy.cli.benchmark_speed.time_context.__exit__(self,type,value,traceback)
spacy.cli.benchmark_speed.warmup(nlp:Language,docs:List[Doc],warmup_epochs:int,batch_size:Optional[int])->numpy.ndarray
spacy.cli.benchmark_speed_cli(ctx:typer.Context,model:str=Arg(...,help='Modelnameorpath'),data_path:Path=Arg(...,help='Locationofbinaryevaluationdatain.spacyformat',exists=True),batch_size:Optional[int]=Opt(None,'--batch-size','-b',min=1,help='Overridethepipelinebatchsize'),no_shuffle:bool=Opt(False,'--no-shuffle',help='Donotshufflebenchmarkdata'),use_gpu:int=Opt(-1,'--gpu-id','-g',help='GPUIDor-1forCPU'),n_batches:int=Opt(50,'--batches',help='Minimumnumberofbatchestobenchmark',min=30),warmup_epochs:int=Opt(3,'--warmup','-w',min=0,help='Numberofiterationsoverthedataforwarmup'),code_path:Optional[Path]=Opt(None,'--code','-c',help='PathtoPythonfilewithadditionalcode(registeredfunctions)tobeimported'))


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/cli/apply.py----------------------------------------
A:spacy.cli.apply.docbin->DocBin(store_user_data=True)
A:spacy.cli.apply.text->fin.read()
A:spacy.cli.apply.data_path->ensure_path(data_path)
A:spacy.cli.apply.output_file->output_file.with_suffix('.spacy').with_suffix('.spacy')
A:spacy.cli.apply.code_path->ensure_path(code_path)
A:spacy.cli.apply.paths->walk_directory(data_path)
A:spacy.cli.apply.nlp->load_model(model)
A:spacy.cli.apply.datagen->cast(DocOrStrStream, chain(*streams))
spacy.cli.apply(data_path:Path,output_file:Path,model:str,json_field:str,batch_size:int,n_process:int)
spacy.cli.apply._stream_docbin(path:Path,vocab:Vocab)->Iterable[Doc]
spacy.cli.apply._stream_jsonl(path:Path,field:str)->Iterable[str]
spacy.cli.apply._stream_texts(paths:Iterable[Path])->Iterable[str]
spacy.cli.apply.apply(data_path:Path,output_file:Path,model:str,json_field:str,batch_size:int,n_process:int)
spacy.cli.apply.apply_cli(model:str=Arg(...,help='Modelnameorpath'),data_path:Path=Arg(...,help=path_help,exists=True),output_file:Path=Arg(...,help=out_help,dir_okay=False),code_path:Optional[Path]=Opt(None,'--code','-c',help=code_help),text_key:str=Opt('text','--text-key','-tk',help='KeycontainingtextstringforJSONL'),force_overwrite:bool=Opt(False,'--force','-F',help='Forceoverwritingtheoutputfile'),use_gpu:int=Opt(-1,'--gpu-id','-g',help='GPUIDor-1forCPU.'),batch_size:int=Opt(1,'--batch-size','-b',help='Batchsize.'),n_process:int=Opt(1,'--n-process','-n',help='numberofprocessorstouse.'))
spacy.cli.apply_cli(model:str=Arg(...,help='Modelnameorpath'),data_path:Path=Arg(...,help=path_help,exists=True),output_file:Path=Arg(...,help=out_help,dir_okay=False),code_path:Optional[Path]=Opt(None,'--code','-c',help=code_help),text_key:str=Opt('text','--text-key','-tk',help='KeycontainingtextstringforJSONL'),force_overwrite:bool=Opt(False,'--force','-F',help='Forceoverwritingtheoutputfile'),use_gpu:int=Opt(-1,'--gpu-id','-g',help='GPUIDor-1forCPU.'),batch_size:int=Opt(1,'--batch-size','-b',help='Batchsize.'),n_process:int=Opt(1,'--n-process','-n',help='numberofprocessorstouse.'))


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/cli/download.py----------------------------------------
A:spacy.cli.download.components->model.split('-')
A:spacy.cli.download.model_name->''.join(components[:-1])
A:spacy.cli.download.compatibility->get_compatibility()
A:spacy.cli.download.version->get_minor_version(about.__version__)
A:spacy.cli.download.filename->dl_tpl.format(m=model_name, v=version, s=suffix)
A:spacy.cli.download.r->requests.get(about.__compatibility__)
A:spacy.cli.download.comp_table->requests.get(about.__compatibility__).json()
A:spacy.cli.download.comp->get_compatibility()
A:spacy.cli.download.download_url->urljoin(base_url, filename)
spacy.cli.download(model:str,direct:bool=False,sdist:bool=False,*pip_args)->None
spacy.cli.download.download(model:str,direct:bool=False,sdist:bool=False,*pip_args)->None
spacy.cli.download.download_cli(ctx:typer.Context,model:str=Arg(...,help='Nameofpipelinepackagetodownload'),direct:bool=Opt(False,'--direct','-d','-D',help='Forcedirectdownloadofname+version'),sdist:bool=Opt(False,'--sdist','-S',help='Downloadsdist(.tar.gz)archiveinsteadofpre-builtbinarywheel'))
spacy.cli.download.download_model(filename:str,user_pip_args:Optional[Sequence[str]]=None)->None
spacy.cli.download.get_compatibility()->dict
spacy.cli.download.get_latest_version(model:str)->str
spacy.cli.download.get_model_filename(model_name:str,version:str,sdist:bool=False)->str
spacy.cli.download.get_version(model:str,comp:dict)->str
spacy.cli.download_cli(ctx:typer.Context,model:str=Arg(...,help='Nameofpipelinepackagetodownload'),direct:bool=Opt(False,'--direct','-d','-D',help='Forcedirectdownloadofname+version'),sdist:bool=Opt(False,'--sdist','-S',help='Downloadsdist(.tar.gz)archiveinsteadofpre-builtbinarywheel'))
spacy.cli.download_model(filename:str,user_pip_args:Optional[Sequence[str]]=None)->None


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/cli/debug_data.py----------------------------------------
A:spacy.cli.debug_data.overrides->parse_config_overrides(ctx.args)
A:spacy.cli.debug_data.msg->Printer(no_print=silent, pretty=not no_format, ignore_warnings=ignore_warnings)
A:spacy.cli.debug_data.cfg->util.load_config(config_path, overrides=config_overrides)
A:spacy.cli.debug_data.nlp->util.load_model_from_config(cfg)
A:spacy.cli.debug_data.config->util.load_model_from_config(cfg).config.interpolate()
A:spacy.cli.debug_data.T->util.registry.resolve(config['training'], schema=ConfigSchemaTraining)
A:spacy.cli.debug_data.sourced_components->get_sourced_components(cfg)
A:spacy.cli.debug_data.(train_corpus, dev_corpus)->resolve_dot_names(config, dot_names)
A:spacy.cli.debug_data.train_dataset->list(train_corpus(nlp))
A:spacy.cli.debug_data.dev_dataset->list(dev_corpus(nlp))
A:spacy.cli.debug_data.gold_train_data->_compile_gold(train_dataset, factory_names, nlp, make_proj=True)
A:spacy.cli.debug_data.gold_train_unpreprocessed_data->_compile_gold(train_dataset, factory_names, nlp, make_proj=False)
A:spacy.cli.debug_data.gold_dev_data->_compile_gold(dev_dataset, factory_names, nlp, make_proj=True)
A:spacy.cli.debug_data.overlap->len(train_texts.intersection(dev_texts))
A:spacy.cli.debug_data.most_common_words->gold_train_data['words'].most_common(10)
A:spacy.cli.debug_data.n_missing_vectors->sum(gold_train_data['words_missing_vectors'].values())
A:spacy.cli.debug_data.model_labels_spancat->_get_labels_from_spancat(nlp)
A:spacy.cli.debug_data.neg_docs->_get_examples_without_label(train_dataset, label, 'ner')
A:spacy.cli.debug_data.span_characteristics->_get_span_characteristics(train_dataset, gold_train_data, spans_key)
A:spacy.cli.debug_data._span_freqs->_get_spans_length_freq_dist(gold_train_data['spans_length'][spans_key])
A:spacy.cli.debug_data._filtered_span_freqs->_filter_spans_length_freq_dist(_span_freqs, threshold=SPAN_LENGTH_THRESHOLD_PERCENTAGE)
A:spacy.cli.debug_data.p_spans->span_characteristics['p_spans'].values()
A:spacy.cli.debug_data.p_bounds->span_characteristics['p_bounds'].values()
A:spacy.cli.debug_data.labels->set(label_list)
A:spacy.cli.debug_data.model_labels->_get_labels_from_model(nlp, 'morphologizer')
A:spacy.cli.debug_data.labels_with_counts->_format_labels(gold_train_unpreprocessed_data['deps'].most_common(), counts=True)
A:spacy.cli.debug_data.(label_list, counts)->zip(*gold_train_data['tags'].items())
A:spacy.cli.debug_data.p->numpy.array(counts)
A:spacy.cli.debug_data.data->srsly.read_jsonl(file_path)
A:spacy.cli.debug_data.trees->EditTrees(nlp.vocab.strings)
A:spacy.cli.debug_data.sent_starts->eg.get_aligned_sent_starts()
A:spacy.cli.debug_data.combined_label->remove_bilu_prefix(label)
A:spacy.cli.debug_data.data['spancat'][spans_key]->Counter()
A:spacy.cli.debug_data.data['spans_length'][spans_key]->dict()
A:spacy.cli.debug_data.data['spans_per_type'][spans_key]->dict()
A:spacy.cli.debug_data.data['sb_per_type'][spans_key]->dict()
A:spacy.cli.debug_data.tags->eg.get_aligned('TAG', as_string=True)
A:spacy.cli.debug_data.pos_tags->eg.get_aligned('POS', as_string=True)
A:spacy.cli.debug_data.morphs->eg.get_aligned('MORPH', as_string=True)
A:spacy.cli.debug_data.label_dict->morphology.Morphology.feats_to_dict(morph)
A:spacy.cli.debug_data.(aligned_heads, aligned_deps)->eg.get_aligned_parse(projectivize=make_proj)
A:spacy.cli.debug_data.lemma_set->set()
A:spacy.cli.debug_data.tree_id->EditTrees(nlp.vocab.strings).add(token.text, token.lemma_)
A:spacy.cli.debug_data.tree_str->EditTrees(nlp.vocab.strings).tree_to_str(tree_id)
A:spacy.cli.debug_data.freqs->dict(sorted(freqs.items()))
A:spacy.cli.debug_data.pipe->util.load_model_from_config(cfg).get_pipe(pipe_name)
A:spacy.cli.debug_data.labels[pipe.key]->set()
A:spacy.cli.debug_data.total->sum(word_counts.values(), 0.0)
A:spacy.cli.debug_data.t->token.text.lower().replace('``', '"').replace("''", '"')
A:spacy.cli.debug_data.word_counts->Counter({k: v / total for (k, v) in word_counts.items()})
A:spacy.cli.debug_data.p_corpus->_get_distribution([eg.reference for eg in examples], normalize=True)
A:spacy.cli.debug_data.max_col->max(30, max((len(label) for label in span_characteristics['labels'])))
A:spacy.cli.debug_data.table->_format_span_row(span_data=table_data, labels=span_characteristics['labels'])
A:spacy.cli.debug_data.percentage->round(percentage, 2)
spacy.cli.debug_data(config_path:Path,*,config_overrides:Dict[str,Any]={},ignore_warnings:bool=False,verbose:bool=False,no_format:bool=True,silent:bool=True)
spacy.cli.debug_data._compile_gold(examples:Sequence[Example],factory_names:List[str],nlp:Language,make_proj:bool)->Dict[str, Any]
spacy.cli.debug_data._filter_spans_length_freq_dist(freq_dist:Dict[int,float],threshold:int)->Dict[int, float]
spacy.cli.debug_data._format_freqs(freqs:Dict[int,float],sort:bool=True)->str
spacy.cli.debug_data._format_labels(labels:Union[Iterable[str],Iterable[Tuple[str,int]]],counts:bool=False)->str
spacy.cli.debug_data._format_span_row(span_data:List[Dict],labels:List[str])->List[Any]
spacy.cli.debug_data._get_distribution(docs,normalize:bool=True)->Counter
spacy.cli.debug_data._get_examples_without_label(data:Sequence[Example],label:str,component:Literal['ner','spancat']='ner',spans_key:Optional[str]='sc')->int
spacy.cli.debug_data._get_kl_divergence(p:Counter,q:Counter)->float
spacy.cli.debug_data._get_labels_from_model(nlp:Language,factory_name:str)->Set[str]
spacy.cli.debug_data._get_labels_from_spancat(nlp:Language)->Dict[str, Set[str]]
spacy.cli.debug_data._get_span_characteristics(examples:List[Example],compiled_gold:Dict[str,Any],spans_key:str)->Dict[str, Any]
spacy.cli.debug_data._get_spans_length_freq_dist(length_dict:Dict,threshold=SPAN_LENGTH_THRESHOLD_PERCENTAGE)->Dict[int, float]
spacy.cli.debug_data._gmean(l:List)->float
spacy.cli.debug_data._load_file(file_path:Path,msg:Printer)->None
spacy.cli.debug_data._print_span_characteristics(span_characteristics:Dict[str,Any])
spacy.cli.debug_data._wgt_average(metric:Dict[str,float],frequencies:Counter)->float
spacy.cli.debug_data.debug_data(config_path:Path,*,config_overrides:Dict[str,Any]={},ignore_warnings:bool=False,verbose:bool=False,no_format:bool=True,silent:bool=True)
spacy.cli.debug_data.debug_data_cli(ctx:typer.Context,config_path:Path=Arg(...,help='Pathtoconfigfile',exists=True,allow_dash=True),code_path:Optional[Path]=Opt(None,'--code-path','--code','-c',help='PathtoPythonfilewithadditionalcode(registeredfunctions)tobeimported'),ignore_warnings:bool=Opt(False,'--ignore-warnings','-IW',help='Ignorewarnings,onlyshowstatsanderrors'),verbose:bool=Opt(False,'--verbose','-V',help='Printadditionalinformationandexplanations'),no_format:bool=Opt(False,'--no-format','-NF',help="Don'tpretty-printtheresults"))
spacy.cli.debug_data_cli(ctx:typer.Context,config_path:Path=Arg(...,help='Pathtoconfigfile',exists=True,allow_dash=True),code_path:Optional[Path]=Opt(None,'--code-path','--code','-c',help='PathtoPythonfilewithadditionalcode(registeredfunctions)tobeimported'),ignore_warnings:bool=Opt(False,'--ignore-warnings','-IW',help='Ignorewarnings,onlyshowstatsanderrors'),verbose:bool=Opt(False,'--verbose','-V',help='Printadditionalinformationandexplanations'),no_format:bool=Opt(False,'--no-format','-NF',help="Don'tpretty-printtheresults"))


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/cli/info.py----------------------------------------
A:spacy.cli.info.exclude->string_to_list(exclude)
A:spacy.cli.info.msg->Printer(no_print=silent, pretty=not silent)
A:spacy.cli.info.data->json.loads(text)
A:spacy.cli.info.data['Pipelines']->', '.join((f'{n} ({v})' for (n, v) in data['Pipelines'].items()))
A:spacy.cli.info.markdown_data->get_markdown(data, title=title, exclude=exclude)
A:spacy.cli.info.package->pkg_name.replace('-', '_')
A:spacy.cli.info.all_models[package]->util.get_package_version(pkg_name)
A:spacy.cli.info.model_path->Path(model)
A:spacy.cli.info.meta->srsly.read_json(meta_path)
A:spacy.cli.info.meta['source']->str(model_path)
A:spacy.cli.info.download_url->info_installed_model_url(model)
A:spacy.cli.info.dist->compat.importlib_metadata.distribution(model)
A:spacy.cli.info.text->compat.importlib_metadata.distribution(model).read_text('direct_url.json')
A:spacy.cli.info.version->get_latest_version(model)
A:spacy.cli.info.filename->get_model_filename(model, version)
A:spacy.cli.info.release_url->release_tpl.format(m=model, v=version)
A:spacy.cli.info.md->MarkdownRenderer()
A:spacy.cli.info.existing_path->Path(value).exists()
spacy.cli.info(model:Optional[str]=None,*,markdown:bool=False,silent:bool=True,exclude:Optional[List[str]]=None,url:bool=False)->Union[str, dict]
spacy.cli.info.get_markdown(data:Dict[str,Any],title:Optional[str]=None,exclude:Optional[List[str]]=None)->str
spacy.cli.info.info(model:Optional[str]=None,*,markdown:bool=False,silent:bool=True,exclude:Optional[List[str]]=None,url:bool=False)->Union[str, dict]
spacy.cli.info.info_cli(model:Optional[str]=Arg(None,help='OptionalloadablespaCypipeline'),markdown:bool=Opt(False,'--markdown','-md',help='GenerateMarkdownforGitHubissues'),silent:bool=Opt(False,'--silent','-s','-S',help="Don'tprintanything(justreturn)"),exclude:str=Opt('labels','--exclude','-e',help='Comma-separatedkeystoexcludefromtheprint-out'),url:bool=Opt(False,'--url','-u',help='PrinttheURLtodownloadthemostrecentcompatibleversionofthepipeline'))
spacy.cli.info.info_installed_model_url(model:str)->Optional[str]
spacy.cli.info.info_model(model:str,*,silent:bool=True)->Dict[str, Any]
spacy.cli.info.info_model_url(model:str)->Dict[str, Any]
spacy.cli.info.info_spacy()->Dict[str, Any]
spacy.cli.info_cli(model:Optional[str]=Arg(None,help='OptionalloadablespaCypipeline'),markdown:bool=Opt(False,'--markdown','-md',help='GenerateMarkdownforGitHubissues'),silent:bool=Opt(False,'--silent','-s','-S',help="Don'tprintanything(justreturn)"),exclude:str=Opt('labels','--exclude','-e',help='Comma-separatedkeystoexcludefromtheprint-out'),url:bool=Opt(False,'--url','-u',help='PrinttheURLtodownloadthemostrecentcompatibleversionofthepipeline'))
spacy.cli.info_installed_model_url(model:str)->Optional[str]
spacy.cli.info_model(model:str,*,silent:bool=True)->Dict[str, Any]
spacy.cli.info_model_url(model:str)->Dict[str, Any]
spacy.cli.info_spacy()->Dict[str, Any]


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/cli/validate.py----------------------------------------
A:spacy.cli.validate.(model_pkgs, compat)->get_model_pkgs()
A:spacy.cli.validate.spacy_version->get_model_meta(model_path).get('spacy_version', 'n/a')
A:spacy.cli.validate.current_compat->compat.get(spacy_version, {})
A:spacy.cli.validate.comp->Printer(no_print=silent, pretty=not silent).text('', color='green', icon='good', no_print=True)
A:spacy.cli.validate.version->get_package_version(pkg_name)
A:spacy.cli.validate.msg->Printer(no_print=silent, pretty=not silent)
A:spacy.cli.validate.r->requests.get(about.__compatibility__)
A:spacy.cli.validate.all_models->set()
A:spacy.cli.validate.installed_models->get_installed_models()
A:spacy.cli.validate.package->pkg_name.replace('-', '_')
A:spacy.cli.validate.model_path->get_package_path(package)
A:spacy.cli.validate.model_meta->get_model_meta(model_path)
A:spacy.cli.validate.is_compat->is_compatible_version(about.__version__, spacy_version)
spacy.cli.validate()->None
spacy.cli.validate.get_model_pkgs(silent:bool=False)->Tuple[dict, dict]
spacy.cli.validate.reformat_version(version:str)->str
spacy.cli.validate.validate()->None
spacy.cli.validate.validate_cli()
spacy.cli.validate_cli()


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/cli/profile.py----------------------------------------
A:spacy.cli.profile.texts->list(itertools.islice(texts, n_texts))
A:spacy.cli.profile.(imdb_train, _)->ml_datasets.imdb(train_limit=n_texts, dev_limit=0)
A:spacy.cli.profile.(texts, _)->zip(*imdb_train)
A:spacy.cli.profile.nlp->load_model(model)
A:spacy.cli.profile.s->pstats.Stats('Profile.prof')
A:spacy.cli.profile.input_path->Path(loc)
A:spacy.cli.profile.file_->Path(loc).open()
A:spacy.cli.profile.data->srsly.json_loads(line)
spacy.cli.profile(model:str,inputs:Optional[Path]=None,n_texts:int=10000)->None
spacy.cli.profile._read_inputs(loc:Union[Path,str],msg:Printer)->Iterator[str]
spacy.cli.profile.parse_texts(nlp:Language,texts:Sequence[str])->None
spacy.cli.profile.profile(model:str,inputs:Optional[Path]=None,n_texts:int=10000)->None
spacy.cli.profile.profile_cli(ctx:typer.Context,model:str=Arg(...,help='Trainedpipelinetoload'),inputs:Optional[Path]=Arg(None,help="Locationofinputfile.'-'forstdin.",exists=True,allow_dash=True),n_texts:int=Opt(10000,'--n-texts','-n',help='Maximumnumberoftextstouseifavailable'))
spacy.cli.profile_cli(ctx:typer.Context,model:str=Arg(...,help='Trainedpipelinetoload'),inputs:Optional[Path]=Arg(None,help="Locationofinputfile.'-'forstdin.",exists=True,allow_dash=True),n_texts:int=Opt(10000,'--n-texts','-n',help='Maximumnumberoftextstouseifavailable'))


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/cli/debug_diff.py----------------------------------------
A:spacy.cli.debug_diff.msg->Printer()
A:spacy.cli.debug_diff.user_config->load_config(config_path)
A:spacy.cli.debug_diff.other_config->init_config(lang=lang, pipeline=pipeline, optimize=optimize.value, gpu=gpu, pretraining=pretraining, silent=True)
A:spacy.cli.debug_diff.pipeline->list(user_config['nlp']['pipeline'])
A:spacy.cli.debug_diff.user->load_config(config_path).to_str()
A:spacy.cli.debug_diff.other->init_config(lang=lang, pipeline=pipeline, optimize=optimize.value, gpu=gpu, pretraining=pretraining, silent=True).to_str()
A:spacy.cli.debug_diff.diff_text->diff_strings(other, user, add_symbols=markdown)
A:spacy.cli.debug_diff.md->MarkdownRenderer()
spacy.cli.debug_diff(config_path:Path,compare_to:Optional[Path],gpu:bool,optimize:Optimizations,pretraining:bool,markdown:bool)
spacy.cli.debug_diff.debug_diff(config_path:Path,compare_to:Optional[Path],gpu:bool,optimize:Optimizations,pretraining:bool,markdown:bool)
spacy.cli.debug_diff.debug_diff_cli(ctx:typer.Context,config_path:Path=Arg(...,help='Pathtoconfigfile',exists=True,allow_dash=True),compare_to:Optional[Path]=Opt(None,help='Pathtoaconfigfiletodiffagainst,or`None`tocompareagainstdefaultsettings',exists=True,allow_dash=True),optimize:Optimizations=Opt(Optimizations.efficiency.value,'--optimize','-o',help='Whethertheuserconfigwasoptimizedforefficiencyoraccuracy.Onlyrelevantwhencomparingagainstthedefaultconfig.'),gpu:bool=Opt(False,'--gpu','-G',help='WhethertheoriginalconfigcanrunonaGPU.Onlyrelevantwhencomparingagainstthedefaultconfig.'),pretraining:bool=Opt(False,'--pretraining','--pt',help='Whethertocompareonaconfigwithpretraininginvolved.Onlyrelevantwhencomparingagainstthedefaultconfig.'),markdown:bool=Opt(False,'--markdown','-md',help='GenerateMarkdownforGitHubissues'))
spacy.cli.debug_diff_cli(ctx:typer.Context,config_path:Path=Arg(...,help='Pathtoconfigfile',exists=True,allow_dash=True),compare_to:Optional[Path]=Opt(None,help='Pathtoaconfigfiletodiffagainst,or`None`tocompareagainstdefaultsettings',exists=True,allow_dash=True),optimize:Optimizations=Opt(Optimizations.efficiency.value,'--optimize','-o',help='Whethertheuserconfigwasoptimizedforefficiencyoraccuracy.Onlyrelevantwhencomparingagainstthedefaultconfig.'),gpu:bool=Opt(False,'--gpu','-G',help='WhethertheoriginalconfigcanrunonaGPU.Onlyrelevantwhencomparingagainstthedefaultconfig.'),pretraining:bool=Opt(False,'--pretraining','--pt',help='Whethertocompareonaconfigwithpretraininginvolved.Onlyrelevantwhencomparingagainstthedefaultconfig.'),markdown:bool=Opt(False,'--markdown','-md',help='GenerateMarkdownforGitHubissues'))


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/cli/pretrain.py----------------------------------------
A:spacy.cli.pretrain.config_overrides->parse_config_overrides(ctx.args)
A:spacy.cli.pretrain.raw_config->load_config(config_path, overrides=config_overrides, interpolate=False)
A:spacy.cli.pretrain.config->load_config(config_path, overrides=config_overrides, interpolate=False).interpolate()
A:spacy.cli.pretrain.model_name->re.search('model\\d+\\.bin', str(resume_path))
spacy.cli.pretrain.pretrain_cli(ctx:typer.Context,config_path:Path=Arg(...,help='Pathtoconfigfile',exists=True,dir_okay=False,allow_dash=True),output_dir:Path=Arg(...,help='Directorytowriteweightstooneachepoch'),code_path:Optional[Path]=Opt(None,'--code','-c',help='PathtoPythonfilewithadditionalcode(registeredfunctions)tobeimported'),resume_path:Optional[Path]=Opt(None,'--resume-path','-r',help='Pathtopretrainedweightsfromwhichtoresumepretraining'),epoch_resume:Optional[int]=Opt(None,'--epoch-resume','-er',help='Theepochtoresumecountingfromwhenusing--resume-path.Preventsunintendedoverwritingofexistingweightfiles.'),use_gpu:int=Opt(-1,'--gpu-id','-g',help='GPUIDor-1forCPU'),skip_last:bool=Opt(False,'--skip-last','-L',help='Skipsavingmodel-last.bin'))
spacy.cli.pretrain.verify_cli_args(config_path,output_dir,resume_path,epoch_resume)
spacy.cli.pretrain_cli(ctx:typer.Context,config_path:Path=Arg(...,help='Pathtoconfigfile',exists=True,dir_okay=False,allow_dash=True),output_dir:Path=Arg(...,help='Directorytowriteweightstooneachepoch'),code_path:Optional[Path]=Opt(None,'--code','-c',help='PathtoPythonfilewithadditionalcode(registeredfunctions)tobeimported'),resume_path:Optional[Path]=Opt(None,'--resume-path','-r',help='Pathtopretrainedweightsfromwhichtoresumepretraining'),epoch_resume:Optional[int]=Opt(None,'--epoch-resume','-er',help='Theepochtoresumecountingfromwhenusing--resume-path.Preventsunintendedoverwritingofexistingweightfiles.'),use_gpu:int=Opt(-1,'--gpu-id','-g',help='GPUIDor-1forCPU'),skip_last:bool=Opt(False,'--skip-last','-L',help='Skipsavingmodel-last.bin'))


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/cli/find_function.py----------------------------------------
A:spacy.cli.find_function.registry_names->util.registry.get_registry_names()
A:spacy.cli.find_function.registry_desc->util.registry.find(registry_name, func_name)
spacy.cli.find_function(func_name:str,registry_name:str)->Tuple[str, int]
spacy.cli.find_function.find_function(func_name:str,registry_name:str)->Tuple[str, int]
spacy.cli.find_function.find_function_cli(func_name:str=Arg(...,help='Nameoftheregisteredfunction.'),registry_name:Optional[str]=Opt(None,'--registry','-r',help='Nameofthecatalogueregistry.'))
spacy.cli.find_function_cli(func_name:str=Arg(...,help='Nameoftheregisteredfunction.'),registry_name:Optional[str]=Opt(None,'--registry','-r',help='Nameofthecatalogueregistry.'))


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/cli/init_config.py----------------------------------------
A:spacy.cli.init_config.RECOMMENDATIONS->srsly.read_yaml(ROOT / 'quickstart_training_recommendations.yml')
A:spacy.cli.init_config.pipeline->string_to_list(pipeline)
A:spacy.cli.init_config.msg->Printer(no_print=no_print)
A:spacy.cli.init_config.config->util.load_config(DEFAULT_CONFIG_PRETRAIN_PATH).merge(config)
A:spacy.cli.init_config.nlp->util.load_model_from_config(config, auto_fill=True)
A:spacy.cli.init_config.sourced->util.get_sourced_components(config)
A:spacy.cli.init_config.pretrain_config->util.load_config(DEFAULT_CONFIG_PRETRAIN_PATH)
A:spacy.cli.init_config.filled->util.load_config(DEFAULT_CONFIG_PRETRAIN_PATH).merge(filled)
A:spacy.cli.init_config.before->util.load_config(DEFAULT_CONFIG_PRETRAIN_PATH).merge(config).to_str()
A:spacy.cli.init_config.after->util.load_config(DEFAULT_CONFIG_PRETRAIN_PATH).merge(filled).to_str()
A:spacy.cli.init_config.template->Template(f.read())
A:spacy.cli.init_config.reco->RecommendationSchema(**RECOMMENDATIONS.get(lang, defaults)).dict()
A:spacy.cli.init_config.base_template->re.sub('\\n\\n\\n+', '\n\n', base_template)
A:spacy.cli.init_config.template_vars->Template(f.read()).make_module(variables)
spacy.cli.fill_config(output_file:Path,base_path:Path,*,pretraining:bool=False,diff:bool=False,silent:bool=False)->Tuple[Config, Config]
spacy.cli.init_config(*,lang:str=InitValues.lang,pipeline:List[str]=InitValues.pipeline,optimize:str=InitValues.optimize,gpu:bool=InitValues.gpu,pretraining:bool=InitValues.pretraining,silent:bool=True)->Config
spacy.cli.init_config.InitValues
spacy.cli.init_config.Optimizations(str,Enum)
spacy.cli.init_config.fill_config(output_file:Path,base_path:Path,*,pretraining:bool=False,diff:bool=False,silent:bool=False)->Tuple[Config, Config]
spacy.cli.init_config.has_spacy_transformers()->bool
spacy.cli.init_config.init_config(*,lang:str=InitValues.lang,pipeline:List[str]=InitValues.pipeline,optimize:str=InitValues.optimize,gpu:bool=InitValues.gpu,pretraining:bool=InitValues.pretraining,silent:bool=True)->Config
spacy.cli.init_config.init_config_cli(output_file:Path=Arg(...,help='Filetosavetheconfigtoor-forstdout(willonlyoutputconfigandnoadditionallogginginfo)',allow_dash=True),lang:str=Opt(InitValues.lang,'--lang','-l',help='Two-lettercodeofthelanguagetouse'),pipeline:str=Opt(','.join(InitValues.pipeline),'--pipeline','-p',help="Comma-separatednamesoftrainablepipelinecomponentstoinclude(without'tok2vec'or'transformer')"),optimize:Optimizations=Opt(InitValues.optimize,'--optimize','-o',help='Whethertooptimizeforefficiency(fasterinference,smallermodel,lowermemoryconsumption)orhigheraccuracy(potentiallylargerandslowermodel).Thiswillimpactthechoiceofarchitecture,pretrainedweightsandrelatedhyperparameters.'),gpu:bool=Opt(InitValues.gpu,'--gpu','-G',help='WhetherthemodelcanrunonGPU.Thiswillimpactthechoiceofarchitecture,pretrainedweightsandrelatedhyperparameters.'),pretraining:bool=Opt(InitValues.pretraining,'--pretraining','-pt',help="Includeconfigforpretraining(with'spacypretrain')"),force_overwrite:bool=Opt(InitValues.force_overwrite,'--force','-F',help='Forceoverwritingtheoutputfile'))
spacy.cli.init_config.init_fill_config_cli(base_path:Path=Arg(...,help='Pathtobaseconfigtofill',exists=True,dir_okay=False),output_file:Path=Arg('-',help='Pathtooutput.cfgfile(or-forstdout)',allow_dash=True),pretraining:bool=Opt(False,'--pretraining','-pt',help="Includeconfigforpretraining(with'spacypretrain')"),diff:bool=Opt(False,'--diff','-D',help='Printavisualdiffhighlightingthechanges'),code_path:Optional[Path]=Opt(None,'--code-path','--code','-c',help='PathtoPythonfilewithadditionalcode(registeredfunctions)tobeimported'))
spacy.cli.init_config.save_config(config:Config,output_file:Path,is_stdout:bool=False,silent:bool=False)->None
spacy.cli.init_config.validate_config_for_pretrain(config:Config,msg:Printer)->None
spacy.cli.init_config_cli(output_file:Path=Arg(...,help='Filetosavetheconfigtoor-forstdout(willonlyoutputconfigandnoadditionallogginginfo)',allow_dash=True),lang:str=Opt(InitValues.lang,'--lang','-l',help='Two-lettercodeofthelanguagetouse'),pipeline:str=Opt(','.join(InitValues.pipeline),'--pipeline','-p',help="Comma-separatednamesoftrainablepipelinecomponentstoinclude(without'tok2vec'or'transformer')"),optimize:Optimizations=Opt(InitValues.optimize,'--optimize','-o',help='Whethertooptimizeforefficiency(fasterinference,smallermodel,lowermemoryconsumption)orhigheraccuracy(potentiallylargerandslowermodel).Thiswillimpactthechoiceofarchitecture,pretrainedweightsandrelatedhyperparameters.'),gpu:bool=Opt(InitValues.gpu,'--gpu','-G',help='WhetherthemodelcanrunonGPU.Thiswillimpactthechoiceofarchitecture,pretrainedweightsandrelatedhyperparameters.'),pretraining:bool=Opt(InitValues.pretraining,'--pretraining','-pt',help="Includeconfigforpretraining(with'spacypretrain')"),force_overwrite:bool=Opt(InitValues.force_overwrite,'--force','-F',help='Forceoverwritingtheoutputfile'))


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/cli/evaluate.py----------------------------------------
A:spacy.cli.evaluate.msg->Printer(no_print=silent, pretty=not silent)
A:spacy.cli.evaluate.data_path->util.ensure_path(data_path)
A:spacy.cli.evaluate.output_path->util.ensure_path(output)
A:spacy.cli.evaluate.displacy_path->util.ensure_path(displacy_path)
A:spacy.cli.evaluate.corpus->Corpus(data_path, gold_preproc=gold_preproc)
A:spacy.cli.evaluate.nlp->util.load_model(model)
A:spacy.cli.evaluate.dev_dataset->list(corpus(nlp))
A:spacy.cli.evaluate.scores->util.load_model(model).evaluate(dev_dataset, per_component=per_component)
A:spacy.cli.evaluate.data->handle_scores_per_type(scores, data, spans_key=spans_key, silent=silent)
A:spacy.cli.evaluate.docs->list(nlp.pipe((ex.reference.text for ex in dev_dataset[:displacy_limit])))
A:spacy.cli.evaluate.html->displacy.render(docs[:limit], style='span', page=True)
spacy.cli.evaluate(model:str,data_path:Path,output:Optional[Path]=None,use_gpu:int=-1,gold_preproc:bool=False,displacy_path:Optional[Path]=None,displacy_limit:int=25,silent:bool=True,spans_key:str='sc',per_component:bool=False)->Dict[str, Any]
spacy.cli.evaluate.evaluate(model:str,data_path:Path,output:Optional[Path]=None,use_gpu:int=-1,gold_preproc:bool=False,displacy_path:Optional[Path]=None,displacy_limit:int=25,silent:bool=True,spans_key:str='sc',per_component:bool=False)->Dict[str, Any]
spacy.cli.evaluate.evaluate_cli(model:str=Arg(...,help='Modelnameorpath'),data_path:Path=Arg(...,help='Locationofbinaryevaluationdatain.spacyformat',exists=True),output:Optional[Path]=Opt(None,'--output','-o',help='OutputJSONfileformetrics',dir_okay=False),code_path:Optional[Path]=Opt(None,'--code','-c',help='PathtoPythonfilewithadditionalcode(registeredfunctions)tobeimported'),use_gpu:int=Opt(-1,'--gpu-id','-g',help='GPUIDor-1forCPU'),gold_preproc:bool=Opt(False,'--gold-preproc','-G',help='Usegoldpreprocessing'),displacy_path:Optional[Path]=Opt(None,'--displacy-path','-dp',help='DirectorytooutputrenderedparsesasHTML',exists=True,file_okay=False),displacy_limit:int=Opt(25,'--displacy-limit','-dl',help='LimitofparsestorenderasHTML'),per_component:bool=Opt(False,'--per-component','-P',help='Returnscorespercomponent,onlyapplicablewhenanoutputJSONfileisspecified.'),spans_key:str=Opt('sc','--spans-key','-sk',help='SpanskeytousewhenevaluatingDoc.spans'))
spacy.cli.evaluate.handle_scores_per_type(scores:Dict[str,Any],data:Dict[str,Any]={},*,spans_key:str='sc',silent:bool=False)->Dict[str, Any]
spacy.cli.evaluate.print_prf_per_type(msg:Printer,scores:Dict[str,Dict[str,float]],name:str,type:str)->None
spacy.cli.evaluate.print_textcats_auc_per_cat(msg:Printer,scores:Dict[str,Dict[str,float]])->None
spacy.cli.evaluate.render_parses(docs:List[Doc],output_path:Path,model_name:str='',limit:int=250,deps:bool=True,ents:bool=True,spans:bool=True)
spacy.cli.evaluate_cli(model:str=Arg(...,help='Modelnameorpath'),data_path:Path=Arg(...,help='Locationofbinaryevaluationdatain.spacyformat',exists=True),output:Optional[Path]=Opt(None,'--output','-o',help='OutputJSONfileformetrics',dir_okay=False),code_path:Optional[Path]=Opt(None,'--code','-c',help='PathtoPythonfilewithadditionalcode(registeredfunctions)tobeimported'),use_gpu:int=Opt(-1,'--gpu-id','-g',help='GPUIDor-1forCPU'),gold_preproc:bool=Opt(False,'--gold-preproc','-G',help='Usegoldpreprocessing'),displacy_path:Optional[Path]=Opt(None,'--displacy-path','-dp',help='DirectorytooutputrenderedparsesasHTML',exists=True,file_okay=False),displacy_limit:int=Opt(25,'--displacy-limit','-dl',help='LimitofparsestorenderasHTML'),per_component:bool=Opt(False,'--per-component','-P',help='Returnscorespercomponent,onlyapplicablewhenanoutputJSONfileisspecified.'),spans_key:str=Opt('sc','--spans-key','-sk',help='SpanskeytousewhenevaluatingDoc.spans'))


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/cli/assemble.py----------------------------------------
A:spacy.cli.assemble.overrides->parse_config_overrides(ctx.args)
A:spacy.cli.assemble.config->config.interpolate().interpolate()
A:spacy.cli.assemble.nlp->load_model_from_config(config, auto_fill=True)
A:spacy.cli.assemble.sourced->get_sourced_components(config)
spacy.cli.assemble.assemble_cli(ctx:typer.Context,config_path:Path=Arg(...,help='Pathtoconfigfile',exists=True,allow_dash=True),output_path:Path=Arg(...,help='Outputdirectorytostoreassembledpipelinein'),code_path:Optional[Path]=Opt(None,'--code','-c',help='PathtoPythonfilewithadditionalcode(registeredfunctions)tobeimported'),verbose:bool=Opt(False,'--verbose','-V','-VV',help='Displaymoreinformationfordebuggingpurposes'))
spacy.cli.assemble_cli(ctx:typer.Context,config_path:Path=Arg(...,help='Pathtoconfigfile',exists=True,allow_dash=True),output_path:Path=Arg(...,help='Outputdirectorytostoreassembledpipelinein'),code_path:Optional[Path]=Opt(None,'--code','-c',help='PathtoPythonfilewithadditionalcode(registeredfunctions)tobeimported'),verbose:bool=Opt(False,'--verbose','-V','-VV',help='Displaymoreinformationfordebuggingpurposes'))


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/cli/package.py----------------------------------------
A:spacy.cli.package.(create_sdist, create_wheel)->get_build_formats(string_to_list(build))
A:spacy.cli.package.msg->Printer()
A:spacy.cli.package.input_path->util.ensure_path(input_dir)
A:spacy.cli.package.output_path->util.ensure_path(output_dir)
A:spacy.cli.package.meta_path->util.ensure_path(meta_path)
A:spacy.cli.package.meta->generate_meta(meta, msg)
A:spacy.cli.package.errors->validate(ModelMetaSchema, meta)
A:spacy.cli.package.readme->generate_readme(meta)
A:spacy.cli.package.init_py->"\nfrom pathlib import Path\nfrom spacy.util import load_model_from_init_py, get_model_meta\n\n{imports}\n\n__version__ = get_model_meta(Path(__file__).parent)['version']\n\n\ndef load(**overrides):\n    return load_model_from_init_py(__file__, **overrides)\n".lstrip().format(imports='\n'.join((f'from . import {m}' for m in imports)))
A:spacy.cli.package.ret->subprocess.run([sys.executable, '-m', 'build', '.', '--wheel'], env=os.environ.copy())
A:spacy.cli.package.wheel_name_squashed->re.sub('_+', '_', model_name_v)
A:spacy.cli.package.distributions->util.packages_distributions()
A:spacy.cli.package.funcs->defaultdict(set)
A:spacy.cli.package.modules->set()
A:spacy.cli.package.func_info->util.registry.find(reg_name, func_name)
A:spacy.cli.package.module_name->util.registry.find(reg_name, func_name).get('module')
A:spacy.cli.package.dist->util.packages_distributions().get(module_name)
A:spacy.cli.package.version->util.get_package_version(pkg)
A:spacy.cli.package.version_range->util.get_minor_version_range(version)
A:spacy.cli.package.nlp->util.load_model_from_path(Path(model_path))
A:spacy.cli.package.meta['spacy_version']->util.get_minor_version_range(about.__version__)
A:spacy.cli.package.reqs->get_third_party_dependencies(nlp.config, exclude=existing_reqs)
A:spacy.cli.package.response->get_raw_input(desc, default)
A:spacy.cli.package.md->MarkdownRenderer()
A:spacy.cli.package.pipeline->', '.join([md.code(p) for p in meta.get('pipeline', [])])
A:spacy.cli.package.components->', '.join([md.code(p) for p in meta.get('components', [])])
A:spacy.cli.package.vecs->generate_meta(meta, msg).get('vectors', {})
A:spacy.cli.package.notes->generate_meta(meta, msg).get('notes', '')
A:spacy.cli.package.license_name->generate_meta(meta, msg).get('license')
A:spacy.cli.package.sources->_format_sources(meta.get('sources'))
A:spacy.cli.package.description->generate_meta(meta, msg).get('description')
A:spacy.cli.package.label_scheme->_format_label_scheme(cast(Dict[str, Any], meta.get('labels')))
A:spacy.cli.package.accuracy->_format_accuracy(cast(Dict[str, Any], meta.get('performance')))
A:spacy.cli.package.name->source.get('name')
A:spacy.cli.package.url->source.get('url')
A:spacy.cli.package.author->source.get('author')
A:spacy.cli.package.col1->MarkdownRenderer().bold(md.code(pipe))
A:spacy.cli.package.col2->', '.join([md.code(str(label).replace('|', '\\|')) for label in labels])
A:spacy.cli.package.permitted_match->re.search('^([A-Z0-9]|[A-Z0-9][A-Z0-9._-]*[A-Z0-9])$', package_name, re.IGNORECASE)
A:spacy.cli.package.TEMPLATE_SETUP->'\n#!/usr/bin/env python\nimport io\nimport json\nfrom os import path, walk\nfrom shutil import copy\nfrom setuptools import setup\n\n\ndef load_meta(fp):\n    with io.open(fp, encoding=\'utf8\') as f:\n        return json.load(f)\n\n\ndef load_readme(fp):\n    if path.exists(fp):\n        with io.open(fp, encoding=\'utf8\') as f:\n            return f.read()\n    return ""\n\n\ndef list_files(data_dir):\n    output = []\n    for root, _, filenames in walk(data_dir):\n        for filename in filenames:\n            if not filename.startswith(\'.\'):\n                output.append(path.join(root, filename))\n    output = [path.relpath(p, path.dirname(data_dir)) for p in output]\n    output.append(\'meta.json\')\n    return output\n\n\ndef list_requirements(meta):\n    # Up to version 3.7, we included the parent package\n    # in requirements by default. This behaviour is removed\n    # in 3.8, with a setting to include the parent package in\n    # the requirements list in the meta if desired.\n    requirements = []\n    if \'setup_requires\' in meta:\n        requirements += meta[\'setup_requires\']\n    if \'requirements\' in meta:\n        requirements += meta[\'requirements\']\n    return requirements\n\n\ndef setup_package():\n    root = path.abspath(path.dirname(__file__))\n    meta_path = path.join(root, \'meta.json\')\n    meta = load_meta(meta_path)\n    readme_path = path.join(root, \'README.md\')\n    readme = load_readme(readme_path)\n    model_name = str(meta[\'lang\'] + \'_\' + meta[\'name\'])\n    model_dir = path.join(model_name, model_name + \'-\' + meta[\'version\'])\n\n    copy(meta_path, path.join(model_name))\n    copy(meta_path, model_dir)\n\n    setup(\n        name=model_name,\n        description=meta.get(\'description\'),\n        long_description=readme,\n        author=meta.get(\'author\'),\n        author_email=meta.get(\'email\'),\n        url=meta.get(\'url\'),\n        version=meta[\'version\'],\n        license=meta.get(\'license\'),\n        packages=[model_name],\n        package_data={model_name: list_files(model_dir)},\n        install_requires=list_requirements(meta),\n        zip_safe=False,\n        entry_points={\'spacy_models\': [\'{m} = {m}\'.format(m=model_name)]}\n    )\n\n\nif __name__ == \'__main__\':\n    setup_package()\n'.lstrip()
A:spacy.cli.package.TEMPLATE_MANIFEST->'\ninclude meta.json\ninclude LICENSE\ninclude LICENSES_SOURCES\ninclude README.md\n'.strip()
A:spacy.cli.package.TEMPLATE_INIT->"\nfrom pathlib import Path\nfrom spacy.util import load_model_from_init_py, get_model_meta\n\n{imports}\n\n__version__ = get_model_meta(Path(__file__).parent)['version']\n\n\ndef load(**overrides):\n    return load_model_from_init_py(__file__, **overrides)\n".lstrip()
spacy.cli.package(input_dir:Path,output_dir:Path,meta_path:Optional[Path]=None,code_paths:List[Path]=[],name:Optional[str]=None,version:Optional[str]=None,create_meta:bool=False,create_sdist:bool=True,create_wheel:bool=False,require_parent:bool=False,force:bool=False,silent:bool=True)->None
spacy.cli.package._format_accuracy(data:Dict[str,Any],exclude:List[str]=['speed'])->str
spacy.cli.package._format_label_scheme(data:Dict[str,Any])->str
spacy.cli.package._format_sources(data:Any)->str
spacy.cli.package._is_permitted_package_name(package_name:str)->bool
spacy.cli.package.create_file(file_path:Path,contents:str)->None
spacy.cli.package.generate_meta(existing_meta:Dict[str,Any],msg:Printer)->Dict[str, Any]
spacy.cli.package.generate_readme(meta:Dict[str,Any])->str
spacy.cli.package.get_build_formats(formats:List[str])->Tuple[bool, bool]
spacy.cli.package.get_meta(model_path:Union[str,Path],existing_meta:Dict[str,Any],require_parent:bool=False)->Dict[str, Any]
spacy.cli.package.get_third_party_dependencies(config:Config,exclude:List[str]=util.SimpleFrozenList())->List[str]
spacy.cli.package.has_build()->bool
spacy.cli.package.has_wheel()->bool
spacy.cli.package.package(input_dir:Path,output_dir:Path,meta_path:Optional[Path]=None,code_paths:List[Path]=[],name:Optional[str]=None,version:Optional[str]=None,create_meta:bool=False,create_sdist:bool=True,create_wheel:bool=False,require_parent:bool=False,force:bool=False,silent:bool=True)->None
spacy.cli.package.package_cli(input_dir:Path=Arg(...,help='Directorywithpipelinedata',exists=True,file_okay=False),output_dir:Path=Arg(...,help='Outputparentdirectory',exists=True,file_okay=False),code_paths:str=Opt('','--code','-c',help='Comma-separatedpathstoPythonfilewithadditionalcode(registeredfunctions)tobeincludedinthepackage'),meta_path:Optional[Path]=Opt(None,'--meta-path','--meta','-m',help='Pathtometa.json',exists=True,dir_okay=False),create_meta:bool=Opt(False,'--create-meta','-C',help='Createmeta.json,evenifoneexists'),name:Optional[str]=Opt(None,'--name','-n',help='Packagenametooverridemeta'),version:Optional[str]=Opt(None,'--version','-v',help='Packageversiontooverridemeta'),build:str=Opt('sdist','--build','-b',help='Comma-separatedformatstobuild:sdistand/orwheel,ornone.'),force:bool=Opt(False,'--force','-f','-F',help='Forceoverwritingexistingdatainoutputdirectory'),require_parent:bool=Opt(True,'--require-parent/--no-require-parent','-R','-R',help='Includetheparentpackage(e.g.spacy)intherequirements'))
spacy.cli.package_cli(input_dir:Path=Arg(...,help='Directorywithpipelinedata',exists=True,file_okay=False),output_dir:Path=Arg(...,help='Outputparentdirectory',exists=True,file_okay=False),code_paths:str=Opt('','--code','-c',help='Comma-separatedpathstoPythonfilewithadditionalcode(registeredfunctions)tobeincludedinthepackage'),meta_path:Optional[Path]=Opt(None,'--meta-path','--meta','-m',help='Pathtometa.json',exists=True,dir_okay=False),create_meta:bool=Opt(False,'--create-meta','-C',help='Createmeta.json,evenifoneexists'),name:Optional[str]=Opt(None,'--name','-n',help='Packagenametooverridemeta'),version:Optional[str]=Opt(None,'--version','-v',help='Packageversiontooverridemeta'),build:str=Opt('sdist','--build','-b',help='Comma-separatedformatstobuild:sdistand/orwheel,ornone.'),force:bool=Opt(False,'--force','-f','-F',help='Forceoverwritingexistingdatainoutputdirectory'),require_parent:bool=Opt(True,'--require-parent/--no-require-parent','-R','-R',help='Includetheparentpackage(e.g.spacy)intherequirements'))


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/cli/train.py----------------------------------------
A:spacy.cli.train.overrides->parse_config_overrides(ctx.args)
A:spacy.cli.train.config_path->util.ensure_path(config_path)
A:spacy.cli.train.output_path->util.ensure_path(output_path)
A:spacy.cli.train.config->util.load_config(config_path, overrides=overrides, interpolate=False)
A:spacy.cli.train.nlp->init_nlp(config, use_gpu=use_gpu)
spacy.cli.train.train(config_path:Union[str,Path],output_path:Optional[Union[str,Path]]=None,*,use_gpu:int=-1,overrides:Dict[str,Any]=util.SimpleFrozenDict())
spacy.cli.train.train_cli(ctx:typer.Context,config_path:Path=Arg(...,help='Pathtoconfigfile',exists=True,allow_dash=True),output_path:Optional[Path]=Opt(None,'--output','--output-path','-o',help='Outputdirectorytostoretrainedpipelinein'),code_path:Optional[Path]=Opt(None,'--code','-c',help='PathtoPythonfilewithadditionalcode(registeredfunctions)tobeimported'),verbose:bool=Opt(False,'--verbose','-V','-VV',help='Displaymoreinformationfordebuggingpurposes'),use_gpu:int=Opt(-1,'--gpu-id','-g',help='GPUIDor-1forCPU'))
spacy.cli.train_cli(ctx:typer.Context,config_path:Path=Arg(...,help='Pathtoconfigfile',exists=True,allow_dash=True),output_path:Optional[Path]=Opt(None,'--output','--output-path','-o',help='Outputdirectorytostoretrainedpipelinein'),code_path:Optional[Path]=Opt(None,'--code','-c',help='PathtoPythonfilewithadditionalcode(registeredfunctions)tobeimported'),verbose:bool=Opt(False,'--verbose','-V','-VV',help='Displaymoreinformationfordebuggingpurposes'),use_gpu:int=Opt(-1,'--gpu-id','-g',help='GPUIDor-1forCPU'))


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/cli/convert.py----------------------------------------
A:spacy.cli.convert.input_path->Path(input_path)
A:spacy.cli.convert.msg->Printer(no_print=silent)
A:spacy.cli.convert.converter->_get_converter(msg, converter, input_path)
A:spacy.cli.convert.input_data->file_.read()
A:spacy.cli.convert.docs->func(input_data, n_sents=n_sents, seg_sents=seg_sents, append_morphology=morphology, merge_subtokens=merge_subtokens, lang=lang, model=model, no_print=silent, ner_map=ner_map)
A:spacy.cli.convert.all_docs->itertools.chain.from_iterable([docs for (_, docs) in doc_files])
A:spacy.cli.convert.len_docs->len(db)
A:spacy.cli.convert.db->DocBin(docs=docs, store_user_data=True)
A:spacy.cli.convert.data->DocBin(docs=docs, store_user_data=True).to_bytes()
A:spacy.cli.convert.subpath->input_loc.relative_to(input_path)
A:spacy.cli.convert.output_file->output_file.with_suffix(f'.{file_type}').with_suffix(f'.{file_type}')
A:spacy.cli.convert.iob_re->re.compile('\\S+\\|(O|[IB]-\\S+)')
A:spacy.cli.convert.ner_re->re.compile('\\S+\\s+(O|[IB]-\\S+)$')
A:spacy.cli.convert.line->line.strip().strip()
A:spacy.cli.convert.input_locs->walk_directory(input_path, suffix=None)
A:spacy.cli.convert.file_types->list(set([loc.suffix[1:] for loc in input_locs]))
A:spacy.cli.convert.file_types_str->','.join(file_types)
A:spacy.cli.convert.converter_autodetect->autodetect_ner_format(input_data)
spacy.cli.convert(input_path:Path,output_dir:Union[str,Path],*,file_type:str='json',n_sents:int=1,seg_sents:bool=False,model:Optional[str]=None,morphology:bool=False,merge_subtokens:bool=False,converter:str,ner_map:Optional[Path]=None,lang:Optional[str]=None,concatenate:bool=False,silent:bool=True,msg:Optional[Printer]=None)->None
spacy.cli.convert.FileTypes(str,Enum)
spacy.cli.convert._get_converter(msg,converter,input_path:Path)
spacy.cli.convert._print_docs_to_stdout(data:Any,output_type:str)->None
spacy.cli.convert._write_docs_to_file(data:Any,output_file:Path,output_type:str)->None
spacy.cli.convert.autodetect_ner_format(input_data:str)->Optional[str]
spacy.cli.convert.convert(input_path:Path,output_dir:Union[str,Path],*,file_type:str='json',n_sents:int=1,seg_sents:bool=False,model:Optional[str]=None,morphology:bool=False,merge_subtokens:bool=False,converter:str,ner_map:Optional[Path]=None,lang:Optional[str]=None,concatenate:bool=False,silent:bool=True,msg:Optional[Printer]=None)->None
spacy.cli.convert.convert_cli(input_path:str=Arg(...,help='Inputfileordirectory',exists=True),output_dir:Path=Arg('-',help="Outputdirectory.'-'forstdout.",allow_dash=True,exists=True),file_type:FileTypes=Opt('spacy','--file-type','-t',help='Typeofdatatoproduce'),n_sents:int=Opt(1,'--n-sents','-n',help='Numberofsentencesperdoc(0todisable)'),seg_sents:bool=Opt(False,'--seg-sents','-s',help='Segmentsentences(for-cner)'),model:Optional[str]=Opt(None,'--model','--base','-b',help='TrainedspaCypipelineforsentencesegmentationtouseasbase(for--seg-sents)'),morphology:bool=Opt(False,'--morphology','-m',help='Enableappendingmorphologytotags'),merge_subtokens:bool=Opt(False,'--merge-subtokens','-T',help='MergeCoNLL-Usubtokens'),converter:str=Opt(AUTO,'--converter','-c',help=f'Converter:{tuple(CONVERTERS.keys())}'),ner_map:Optional[Path]=Opt(None,'--ner-map','-nm',help='NERtagmapping(asJSON-encodeddictofentitytypes)',exists=True),lang:Optional[str]=Opt(None,'--lang','-l',help='Language(iftokenizerrequired)'),concatenate:bool=Opt(None,'--concatenate','-C',help='Concatenateoutputtoasinglefile'))
spacy.cli.convert.verify_cli_args(msg:Printer,input_path:Path,output_dir:Union[str,Path],file_type:str,converter:str,ner_map:Optional[Path])
spacy.cli.convert_cli(input_path:str=Arg(...,help='Inputfileordirectory',exists=True),output_dir:Path=Arg('-',help="Outputdirectory.'-'forstdout.",allow_dash=True,exists=True),file_type:FileTypes=Opt('spacy','--file-type','-t',help='Typeofdatatoproduce'),n_sents:int=Opt(1,'--n-sents','-n',help='Numberofsentencesperdoc(0todisable)'),seg_sents:bool=Opt(False,'--seg-sents','-s',help='Segmentsentences(for-cner)'),model:Optional[str]=Opt(None,'--model','--base','-b',help='TrainedspaCypipelineforsentencesegmentationtouseasbase(for--seg-sents)'),morphology:bool=Opt(False,'--morphology','-m',help='Enableappendingmorphologytotags'),merge_subtokens:bool=Opt(False,'--merge-subtokens','-T',help='MergeCoNLL-Usubtokens'),converter:str=Opt(AUTO,'--converter','-c',help=f'Converter:{tuple(CONVERTERS.keys())}'),ner_map:Optional[Path]=Opt(None,'--ner-map','-nm',help='NERtagmapping(asJSON-encodeddictofentitytypes)',exists=True),lang:Optional[str]=Opt(None,'--lang','-l',help='Language(iftokenizerrequired)'),concatenate:bool=Opt(None,'--concatenate','-C',help='Concatenateoutputtoasinglefile'))


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/cli/project/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/cli/project/pull.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/cli/project/clone.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/cli/project/push.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/cli/project/remote_storage.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/cli/project/run.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/cli/project/assets.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/cli/project/document.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/cli/project/dvc.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/ml/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/ml/extract_spans.py----------------------------------------
A:spacy.ml.extract_spans.indices->_get_span_indices(ops, spans, X.lengths)
A:spacy.ml.extract_spans.Y->Ragged(ops.xp.zeros(X.dataXd.shape, dtype=X.dataXd.dtype), ops.xp.zeros((len(X.lengths),), dtype='i'))
A:spacy.ml.extract_spans.dX->Ragged(ops.alloc2f(*x_shape), x_lengths)
A:spacy.ml.extract_spans.(spans, lengths)->_ensure_cpu(spans, lengths)
spacy.ml.extract_spans._ensure_cpu(spans:Ragged,lengths:Ints1d)->Tuple[Ragged, Ints1d]
spacy.ml.extract_spans._get_span_indices(ops,spans:Ragged,lengths:Ints1d)->Ints1d
spacy.ml.extract_spans.extract_spans()->Model[Tuple[Ragged, Ragged], Ragged]
spacy.ml.extract_spans.forward(model:Model,source_spans:Tuple[Ragged,Ragged],is_train:bool)->Tuple[Ragged, Callable]
spacy.ml.extract_spans.init(model,X=None,Y=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/ml/extract_ngrams.py----------------------------------------
A:spacy.ml.extract_ngrams.unigrams->model.ops.asarray(doc.to_array([model.attrs['attr']]))
A:spacy.ml.extract_ngrams.keys->model.ops.xp.concatenate(ngrams)
A:spacy.ml.extract_ngrams.(keys, vals)->model.ops.xp.unique(keys, return_counts=True)
A:spacy.ml.extract_ngrams.lengths->model.ops.asarray([arr.shape[0] for arr in batch_keys], dtype='int32')
A:spacy.ml.extract_ngrams.batch_keys->model.ops.xp.concatenate(batch_keys)
A:spacy.ml.extract_ngrams.batch_vals->model.ops.asarray(model.ops.xp.concatenate(batch_vals), dtype='f')
spacy.ml.extract_ngrams.extract_ngrams(ngram_size:int,attr:int=LOWER)->Model
spacy.ml.extract_ngrams.forward(model:Model,docs,is_train:bool)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/ml/callbacks.py----------------------------------------
A:spacy.ml.callbacks.extra_funcs->additional_pipe_functions.get(pipe.name, [])
A:spacy.ml.callbacks.func->getattr(pipe, name, None)
A:spacy.ml.callbacks.wrapped_func->functools.partial(types.MethodType(nvtx_range_wrapper_for_pipe_method, pipe), func)
A:spacy.ml.callbacks.wrapped_func.__signature__->inspect.signature(func)
A:spacy.ml.callbacks.nlp->pipes_with_nvtx_range(nlp, additional_pipe_functions)
spacy.ml.callbacks.create_models_and_pipes_with_nvtx_range(forward_color:int=-1,backprop_color:int=-1,additional_pipe_functions:Optional[Dict[str,List[str]]]=None)->Callable[['Language'], 'Language']
spacy.ml.callbacks.create_models_with_nvtx_range(forward_color:int=-1,backprop_color:int=-1)->Callable[['Language'], 'Language']
spacy.ml.callbacks.models_with_nvtx_range(nlp,forward_color:int,backprop_color:int)
spacy.ml.callbacks.nvtx_range_wrapper_for_pipe_method(self,func,*args,**kwargs)
spacy.ml.callbacks.pipes_with_nvtx_range(nlp,additional_pipe_functions:Optional[Dict[str,List[str]]])
spacy.ml.create_models_with_nvtx_range(forward_color:int=-1,backprop_color:int=-1)->Callable[['Language'], 'Language']


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/ml/_character_embed.py----------------------------------------
A:spacy.ml._character_embed.vectors_table->model.ops.alloc3f(model.get_dim('nC'), model.get_dim('nV'), model.get_dim('nM'))
A:spacy.ml._character_embed.E->model.get_param('E')
A:spacy.ml._character_embed.nC->model.get_dim('nC')
A:spacy.ml._character_embed.nM->model.get_dim('nM')
A:spacy.ml._character_embed.nO->model.get_dim('nO')
A:spacy.ml._character_embed.nCv->model.ops.xp.arange(nC)
A:spacy.ml._character_embed.doc_ids->model.ops.asarray(doc.to_utf8_array(nr_char=nC))
A:spacy.ml._character_embed.doc_vectors->model.ops.alloc3f(len(doc), nC, nM)
A:spacy.ml._character_embed.dE->model.ops.alloc(E.shape, dtype=E.dtype)
A:spacy.ml._character_embed.d_doc_vectors->d_doc_vectors.reshape((len(doc_ids), nC, nM)).reshape((len(doc_ids), nC, nM))
spacy.ml._character_embed.CharacterEmbed(nM:int,nC:int)->Model[List[Doc], List[Floats2d]]
spacy.ml._character_embed.forward(model:Model,docs:List[Doc],is_train:bool)
spacy.ml._character_embed.init(model:Model,X=None,Y=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/ml/featureextractor.py----------------------------------------
A:spacy.ml.featureextractor.attrs->attrs.reshape((attrs.shape[0], 1)).reshape((attrs.shape[0], 1))
spacy.ml.featureextractor.FeatureExtractor(columns:List[Union[int,str]])->Model[List[Doc], List[Ints2d]]
spacy.ml.featureextractor.forward(model:Model[List[Doc],List[Ints2d]],docs,is_train:bool)->Tuple[List[Ints2d], Callable]


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/ml/_precomputable_affine.py----------------------------------------
A:spacy.ml._precomputable_affine.model->Model('precomputable_affine', forward, init=init, dims={'nO': nO, 'nI': nI, 'nF': nF, 'nP': nP}, params={'W': None, 'b': None, 'pad': None}, attrs={'dropout_rate': dropout})
A:spacy.ml._precomputable_affine.nF->Model('precomputable_affine', forward, init=init, dims={'nO': nO, 'nI': nI, 'nF': nF, 'nP': nP}, params={'W': None, 'b': None, 'pad': None}, attrs={'dropout_rate': dropout}).get_dim('nF')
A:spacy.ml._precomputable_affine.nO->Model('precomputable_affine', forward, init=init, dims={'nO': nO, 'nI': nI, 'nF': nF, 'nP': nP}, params={'W': None, 'b': None, 'pad': None}, attrs={'dropout_rate': dropout}).get_dim('nO')
A:spacy.ml._precomputable_affine.nP->Model('precomputable_affine', forward, init=init, dims={'nO': nO, 'nI': nI, 'nF': nF, 'nP': nP}, params={'W': None, 'b': None, 'pad': None}, attrs={'dropout_rate': dropout}).get_dim('nP')
A:spacy.ml._precomputable_affine.nI->Model('precomputable_affine', forward, init=init, dims={'nO': nO, 'nI': nI, 'nF': nF, 'nP': nP}, params={'W': None, 'b': None, 'pad': None}, attrs={'dropout_rate': dropout}).get_dim('nI')
A:spacy.ml._precomputable_affine.W->Model('precomputable_affine', forward, init=init, dims={'nO': nO, 'nI': nI, 'nF': nF, 'nP': nP}, params={'W': None, 'b': None, 'pad': None}, attrs={'dropout_rate': dropout}).get_param('W').copy()
A:spacy.ml._precomputable_affine.Yf->Yf.reshape((Yf.shape[0], nF, nO, nP)).reshape((Yf.shape[0], nF, nO, nP))
A:spacy.ml._precomputable_affine.Yf[0]->Model('precomputable_affine', forward, init=init, dims={'nO': nO, 'nI': nI, 'nF': nF, 'nP': nP}, params={'W': None, 'b': None, 'pad': None}, attrs={'dropout_rate': dropout}).ops.xp.squeeze(model.get_param('pad'), 0)
A:spacy.ml._precomputable_affine.Xf->Xf.reshape((Xf.shape[0], nF * nI)).reshape((Xf.shape[0], nF * nI))
A:spacy.ml._precomputable_affine.dY->dY.reshape((dY.shape[0], nO * nP)).reshape((dY.shape[0], nO * nP))
A:spacy.ml._precomputable_affine.Wopfi->Wopfi.reshape((nO * nP, nF * nI)).reshape((nO * nP, nF * nI))
A:spacy.ml._precomputable_affine.dXf->Model('precomputable_affine', forward, init=init, dims={'nO': nO, 'nI': nI, 'nF': nF, 'nP': nP}, params={'W': None, 'b': None, 'pad': None}, attrs={'dropout_rate': dropout}).ops.gemm(dY.reshape((dY.shape[0], nO * nP)), Wopfi)
A:spacy.ml._precomputable_affine.dWopfi->dWopfi.transpose((2, 0, 1, 3)).transpose((2, 0, 1, 3))
A:spacy.ml._precomputable_affine.mask->Model('precomputable_affine', forward, init=init, dims={'nO': nO, 'nI': nI, 'nF': nF, 'nP': nP}, params={'W': None, 'b': None, 'pad': None}, attrs={'dropout_rate': dropout}).ops.asarray(ids < 0, dtype='f')
A:spacy.ml._precomputable_affine.d_pad->Model('precomputable_affine', forward, init=init, dims={'nO': nO, 'nI': nI, 'nF': nF, 'nP': nP}, params={'W': None, 'b': None, 'pad': None}, attrs={'dropout_rate': dropout}).ops.gemm(mask, dY.reshape(nB, nO * nP), trans1=True)
A:spacy.ml._precomputable_affine.b->Model('precomputable_affine', forward, init=init, dims={'nO': nO, 'nI': nI, 'nF': nF, 'nP': nP}, params={'W': None, 'b': None, 'pad': None}, attrs={'dropout_rate': dropout}).get_param('b').copy()
A:spacy.ml._precomputable_affine.pad->normal_init(ops, pad.shape, mean=1.0)
A:spacy.ml._precomputable_affine.ids->ops.asarray(ids, dtype='i')
A:spacy.ml._precomputable_affine.tokvecs->ops.alloc((5000, nI), dtype='f')
A:spacy.ml._precomputable_affine.hiddens->hiddens.reshape((hiddens.shape[0] * nF, nO * nP)).reshape((hiddens.shape[0] * nF, nO * nP))
A:spacy.ml._precomputable_affine.vectors->Model('precomputable_affine', forward, init=init, dims={'nO': nO, 'nI': nI, 'nF': nF, 'nP': nP}, params={'W': None, 'b': None, 'pad': None}, attrs={'dropout_rate': dropout}).ops.asarray(vectors)
A:spacy.ml._precomputable_affine.acts1->predict(ids, tokvecs)
A:spacy.ml._precomputable_affine.var->Model('precomputable_affine', forward, init=init, dims={'nO': nO, 'nI': nI, 'nF': nF, 'nP': nP}, params={'W': None, 'b': None, 'pad': None}, attrs={'dropout_rate': dropout}).ops.xp.var(acts1)
A:spacy.ml._precomputable_affine.mean->Model('precomputable_affine', forward, init=init, dims={'nO': nO, 'nI': nI, 'nF': nF, 'nP': nP}, params={'W': None, 'b': None, 'pad': None}, attrs={'dropout_rate': dropout}).ops.xp.mean(acts1)
spacy.ml._precomputable_affine.PrecomputableAffine(nO,nI,nF,nP,dropout=0.1)
spacy.ml._precomputable_affine._backprop_precomputable_affine_padding(model,dY,ids)
spacy.ml._precomputable_affine.forward(model,X,is_train)
spacy.ml._precomputable_affine.init(model,X=None,Y=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/ml/staticvectors.py----------------------------------------
A:spacy.ml.staticvectors.token_count->sum((len(doc) for doc in docs))
A:spacy.ml.staticvectors.keys->model.ops.flatten([cast(Ints1d, doc.to_array(key_attr)) for doc in docs])
A:spacy.ml.staticvectors.W->cast(Floats2d, model.ops.as_contig(model.get_param('W')))
A:spacy.ml.staticvectors.V->model.ops.as_contig(V)
A:spacy.ml.staticvectors.rows->vocab.vectors.find(keys=keys)
A:spacy.ml.staticvectors.vectors_data->model.ops.gemm(V, W, trans2=True)
A:spacy.ml.staticvectors.output->Ragged(vectors_data, model.ops.asarray1i([len(doc) for doc in docs]))
A:spacy.ml.staticvectors.mask->ops.get_dropout_mask((nO,), rate)
spacy.ml.staticvectors.StaticVectors(nO:Optional[int]=None,nM:Optional[int]=None,*,dropout:Optional[float]=None,init_W:Callable=glorot_uniform_init,key_attr:str='ORTH')->Model[List[Doc], Ragged]
spacy.ml.staticvectors._get_drop_mask(ops:Ops,nO:int,rate:Optional[float])->Optional[Floats1d]
spacy.ml.staticvectors._handle_empty(ops:Ops,nO:int)
spacy.ml.staticvectors.forward(model:Model[List[Doc],Ragged],docs:List[Doc],is_train:bool)->Tuple[Ragged, Callable]
spacy.ml.staticvectors.init(init_W:Callable,model:Model[List[Doc],Ragged],X:Optional[List[Doc]]=None,Y:Optional[Ragged]=None)->Model[List[Doc], Ragged]


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/ml/tb_framework.py----------------------------------------
A:spacy.ml.tb_framework.upper->noop()
A:spacy.ml.tb_framework.step_model->ParserStepModel(X, model.layers, unseen_classes=model.attrs['unseen_classes'], train=is_train, has_upper=model.attrs['has_upper'])
A:spacy.ml.tb_framework.lower->model.get_ref('lower')
A:spacy.ml.tb_framework.statevecs->model.ops.alloc2f(2, lower.get_dim('nO'))
spacy.ml.tb_framework.TransitionModel(tok2vec,lower,upper,resize_output,dropout=0.2,unseen_classes=set())
spacy.ml.tb_framework.forward(model,X,is_train)
spacy.ml.tb_framework.init(model,X=None,Y=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/ml/models/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/ml/models/tagger.py----------------------------------------
A:spacy.ml.models.tagger.output_layer->Softmax_v2(nO, t2v_width, init_W=zero_init, normalize_outputs=normalize)
A:spacy.ml.models.tagger.softmax->with_array(output_layer)
A:spacy.ml.models.tagger.model->chain(tok2vec, softmax)
spacy.ml.build_tagger_model(tok2vec:Model[List[Doc],List[Floats2d]],nO:Optional[int]=None,normalize=False)->Model[List[Doc], List[Floats2d]]
spacy.ml.models.tagger.build_tagger_model(tok2vec:Model[List[Doc],List[Floats2d]],nO:Optional[int]=None,normalize=False)->Model[List[Doc], List[Floats2d]]


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/ml/models/spancat.py----------------------------------------
A:spacy.ml.models.spancat.model->chain(cast(Model[Tuple[List[Doc], Ragged], Tuple[Ragged, Ragged]], with_getitem(0, chain(tok2vec, cast(Model[List[Floats2d], Ragged], list2ragged())))), extract_spans(), reducer, scorer)
spacy.ml.build_linear_logistic(nO=None,nI=None)->Model[Floats2d, Floats2d]
spacy.ml.build_mean_max_reducer(hidden_size:int)->Model[Ragged, Floats2d]
spacy.ml.build_spancat_model(tok2vec:Model[List[Doc],List[Floats2d]],reducer:Model[Ragged,Floats2d],scorer:Model[Floats2d,Floats2d])->Model[Tuple[List[Doc], Ragged], Floats2d]
spacy.ml.models.spancat.build_linear_logistic(nO=None,nI=None)->Model[Floats2d, Floats2d]
spacy.ml.models.spancat.build_mean_max_reducer(hidden_size:int)->Model[Ragged, Floats2d]
spacy.ml.models.spancat.build_spancat_model(tok2vec:Model[List[Doc],List[Floats2d]],reducer:Model[Ragged,Floats2d],scorer:Model[Floats2d,Floats2d])->Model[Tuple[List[Doc], Ragged], Floats2d]


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/ml/models/span_finder.py----------------------------------------
A:spacy.ml.models.span_finder.lens->model.ops.asarray1i([len(doc) for doc in X])
A:spacy.ml.models.span_finder.Y->model.ops.flatten(X)
spacy.ml.build_finder_model(tok2vec:Model[InT,List[Floats2d]],scorer:Model[OutT,OutT])->Model[InT, OutT]
spacy.ml.flattener()->Model[List[Floats2d], Floats2d]
spacy.ml.models.span_finder.build_finder_model(tok2vec:Model[InT,List[Floats2d]],scorer:Model[OutT,OutT])->Model[InT, OutT]
spacy.ml.models.span_finder.flattener()->Model[List[Floats2d], Floats2d]


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/ml/models/tok2vec.py----------------------------------------
A:spacy.ml.models.tok2vec.tok2vec->chain(embed, encode)
A:spacy.ml.models.tok2vec.nO->chain(embed, encode).get_ref('listener').get_dim('nO')
A:spacy.ml.models.tok2vec.model->clone(residual(cnn), depth)
A:spacy.ml.models.tok2vec.feature->intify_attr(feature)
A:spacy.ml.models.tok2vec.char_embed->chain(_character_embed.CharacterEmbed(nM=nM, nC=nC), cast(Model[List[Floats2d], Ragged], list2ragged()))
A:spacy.ml.models.tok2vec.max_out->with_array(Maxout(width, nM * nC + width, nP=3, normalize=True, dropout=0.0))
A:spacy.ml.models.tok2vec.cnn->chain(expand_window(window_size=window_size), Mish(nO=width, nI=width * (window_size * 2 + 1), dropout=0.0, normalize=True))
spacy.ml.BiLSTMEncoder(width:int,depth:int,dropout:float)->Model[List[Floats2d], List[Floats2d]]
spacy.ml.CharacterEmbed(width:int,rows:int,nM:int,nC:int,include_static_vectors:bool,feature:Union[int,str]='LOWER')->Model[List[Doc], List[Floats2d]]
spacy.ml.MaxoutWindowEncoder(width:int,window_size:int,maxout_pieces:int,depth:int)->Model[List[Floats2d], List[Floats2d]]
spacy.ml.MishWindowEncoder(width:int,window_size:int,depth:int)->Model[List[Floats2d], List[Floats2d]]
spacy.ml.MultiHashEmbed(width:int,attrs:List[Union[str,int]],rows:List[int],include_static_vectors:bool)->Model[List[Doc], List[Floats2d]]
spacy.ml.build_Tok2Vec_model(embed:Model[List[Doc],List[Floats2d]],encode:Model[List[Floats2d],List[Floats2d]])->Model[List[Doc], List[Floats2d]]
spacy.ml.build_hash_embed_cnn_tok2vec(*,width:int,depth:int,embed_size:int,window_size:int,maxout_pieces:int,subword_features:bool,pretrained_vectors:Optional[bool])->Model[List[Doc], List[Floats2d]]
spacy.ml.get_tok2vec_width(model:Model)
spacy.ml.models.tok2vec.BiLSTMEncoder(width:int,depth:int,dropout:float)->Model[List[Floats2d], List[Floats2d]]
spacy.ml.models.tok2vec.CharacterEmbed(width:int,rows:int,nM:int,nC:int,include_static_vectors:bool,feature:Union[int,str]='LOWER')->Model[List[Doc], List[Floats2d]]
spacy.ml.models.tok2vec.MaxoutWindowEncoder(width:int,window_size:int,maxout_pieces:int,depth:int)->Model[List[Floats2d], List[Floats2d]]
spacy.ml.models.tok2vec.MishWindowEncoder(width:int,window_size:int,depth:int)->Model[List[Floats2d], List[Floats2d]]
spacy.ml.models.tok2vec.MultiHashEmbed(width:int,attrs:List[Union[str,int]],rows:List[int],include_static_vectors:bool)->Model[List[Doc], List[Floats2d]]
spacy.ml.models.tok2vec.build_Tok2Vec_model(embed:Model[List[Doc],List[Floats2d]],encode:Model[List[Floats2d],List[Floats2d]])->Model[List[Doc], List[Floats2d]]
spacy.ml.models.tok2vec.build_hash_embed_cnn_tok2vec(*,width:int,depth:int,embed_size:int,window_size:int,maxout_pieces:int,subword_features:bool,pretrained_vectors:Optional[bool])->Model[List[Doc], List[Floats2d]]
spacy.ml.models.tok2vec.get_tok2vec_width(model:Model)
spacy.ml.models.tok2vec.tok2vec_listener_v1(width:int,upstream:str='*')
spacy.ml.tok2vec_listener_v1(width:int,upstream:str='*')


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/ml/models/multi_task.py----------------------------------------
A:spacy.ml.models.multi_task.model->build_masked_language_model(vocab, chain(tok2vec, output_layer))
A:spacy.ml.models.multi_task.model.attrs['loss']->partial(get_characters_loss, nr_char=n_characters)
A:spacy.ml.models.multi_task.distance->L2Distance(normalize=True)
A:spacy.ml.models.multi_task.ids->ops.flatten([doc.to_array(ID).ravel() for doc in docs])
A:spacy.ml.models.multi_task.(d_target, loss)->distance(prediction, target)
A:spacy.ml.models.multi_task.keys->ops.flatten([cast(Ints1d, doc.to_array(ORTH)) for doc in docs])
A:spacy.ml.models.multi_task.target->target.reshape((-1, 256 * nr_char)).reshape((-1, 256 * nr_char))
A:spacy.ml.models.multi_task.target_ids->target_ids.reshape((-1,)).reshape((-1,))
A:spacy.ml.models.multi_task.loss->(diff ** 2).sum()
A:spacy.ml.models.multi_task.softmax->Softmax(nO=nO, nI=token_vector_width * 2)
A:spacy.ml.models.multi_task.output_layer->chain(cast(Model[List['Floats2d'], Floats2d], list2array()), Maxout(nO=hidden_size, nP=maxout_pieces), LayerNorm(nI=hidden_size), MultiSoftmax([256] * nr_char, nI=hidden_size))
A:spacy.ml.models.multi_task.random_words->_RandomWords(vocab)
A:spacy.ml.models.multi_task.(mask, docs)->_apply_mask(docs, random_words, mask_prob=mask_prob)
A:spacy.ml.models.multi_task.mask->numpy.random.uniform(0.0, 1.0, (N,))
A:spacy.ml.models.multi_task.(output, backprop)->build_masked_language_model(vocab, chain(tok2vec, output_layer)).layers[0](docs, is_train)
A:spacy.ml.models.multi_task.index->self._cache.pop()
A:spacy.ml.models.multi_task.N->sum((len(doc) for doc in docs))
A:spacy.ml.models.multi_task.word->_replace_word(token.text, random_words)
A:spacy.ml.models.multi_task.roll->numpy.random.random()
spacy.ml._RandomWords(self,vocab:'Vocab')
spacy.ml._RandomWords.next(self)->str
spacy.ml._apply_mask(docs:Iterable['Doc'],random_words:_RandomWords,mask_prob:float=0.15)->Tuple[numpy.ndarray, List['Doc']]
spacy.ml._replace_word(word:str,random_words:_RandomWords,mask:str='[MASK]')->str
spacy.ml.build_cloze_characters_multi_task_model(vocab:'Vocab',tok2vec:Model,maxout_pieces:int,hidden_size:int,nr_char:int)->Model
spacy.ml.build_cloze_multi_task_model(vocab:'Vocab',tok2vec:Model,maxout_pieces:int,hidden_size:int)->Model
spacy.ml.build_masked_language_model(vocab:'Vocab',wrapped_model:Model,mask_prob:float=0.15)->Model
spacy.ml.build_multi_task_model(tok2vec:Model,maxout_pieces:int,token_vector_width:int,nO:Optional[int]=None)->Model
spacy.ml.create_pretrain_characters(maxout_pieces:int,hidden_size:int,n_characters:int)->Callable[['Vocab', Model], Model]
spacy.ml.create_pretrain_vectors(maxout_pieces:int,hidden_size:int,loss:str)->Callable[['Vocab', Model], Model]
spacy.ml.get_characters_loss(ops,docs,prediction,nr_char)
spacy.ml.get_vectors_loss(ops,docs,prediction,distance)
spacy.ml.models.multi_task._RandomWords(self,vocab:'Vocab')
spacy.ml.models.multi_task._RandomWords.__init__(self,vocab:'Vocab')
spacy.ml.models.multi_task._RandomWords.next(self)->str
spacy.ml.models.multi_task._apply_mask(docs:Iterable['Doc'],random_words:_RandomWords,mask_prob:float=0.15)->Tuple[numpy.ndarray, List['Doc']]
spacy.ml.models.multi_task._replace_word(word:str,random_words:_RandomWords,mask:str='[MASK]')->str
spacy.ml.models.multi_task.build_cloze_characters_multi_task_model(vocab:'Vocab',tok2vec:Model,maxout_pieces:int,hidden_size:int,nr_char:int)->Model
spacy.ml.models.multi_task.build_cloze_multi_task_model(vocab:'Vocab',tok2vec:Model,maxout_pieces:int,hidden_size:int)->Model
spacy.ml.models.multi_task.build_masked_language_model(vocab:'Vocab',wrapped_model:Model,mask_prob:float=0.15)->Model
spacy.ml.models.multi_task.build_multi_task_model(tok2vec:Model,maxout_pieces:int,token_vector_width:int,nO:Optional[int]=None)->Model
spacy.ml.models.multi_task.create_pretrain_characters(maxout_pieces:int,hidden_size:int,n_characters:int)->Callable[['Vocab', Model], Model]
spacy.ml.models.multi_task.create_pretrain_vectors(maxout_pieces:int,hidden_size:int,loss:str)->Callable[['Vocab', Model], Model]
spacy.ml.models.multi_task.get_characters_loss(ops,docs,prediction,nr_char)
spacy.ml.models.multi_task.get_vectors_loss(ops,docs,prediction,distance)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/ml/models/textcat.py----------------------------------------
A:spacy.ml.models.textcat.resizable_layer->resizable(output_layer, resize_layer=partial(resize_linear_weighted, fill_defaults=fill_defaults))
A:spacy.ml.models.textcat.model->with_cpu(model, model.ops)
A:spacy.ml.models.textcat.model.attrs['resize_output']->partial(resize_and_set_ref, resizable_layer=resizable_layer)
A:spacy.ml.models.textcat.width->model.get_ref('tok2vec').maybe_get_dim('nO')
A:spacy.ml.models.textcat.attention_layer->ParametricAttention_v2(nO=width, key_transform=key_transform)
A:spacy.ml.models.textcat.maxout_layer->Maxout(nO=width, nI=width)
A:spacy.ml.models.textcat.norm_layer->LayerNorm(nI=width)
A:spacy.ml.models.textcat.output_layer->Linear(nO=nO, nI=nI)
A:spacy.ml.models.textcat.tok2vec->with_cpu(model, model.ops).get_ref('tok2vec')
A:spacy.ml.models.textcat.tok2vec_width->get_tok2vec_width(model)
A:spacy.ml.models.textcat.parametric_attention->_build_parametric_attention_with_residual_nonlinear(tok2vec=tok2vec, nonlinear_layer=Maxout(nI=width, nO=width), key_transform=Gelu(nI=width, nO=width))
A:spacy.ml.models.textcat.nO_tok2vec->with_cpu(model, model.ops).get_ref('tok2vec').maybe_get_dim('nO')
spacy.ml._build_bow_text_classifier(exclusive_classes:bool,ngram_size:int,no_output_layer:bool,sparse_linear:Model[Tuple[ArrayXd,ArrayXd,ArrayXd],ArrayXd],nO:Optional[int]=None)->Model[List[Doc], Floats2d]
spacy.ml._build_parametric_attention_with_residual_nonlinear(*,tok2vec:Model[List[Doc],List[Floats2d]],nonlinear_layer:Model[Floats2d,Floats2d],key_transform:Optional[Model[Floats2d,Floats2d]]=None)->Model[List[Doc], Floats2d]
spacy.ml._init_parametric_attention_with_residual_nonlinear(model,X,Y)->Model
spacy.ml.build_bow_text_classifier(exclusive_classes:bool,ngram_size:int,no_output_layer:bool,nO:Optional[int]=None)->Model[List[Doc], Floats2d]
spacy.ml.build_bow_text_classifier_v3(exclusive_classes:bool,ngram_size:int,no_output_layer:bool,length:int=262144,nO:Optional[int]=None)->Model[List[Doc], Floats2d]
spacy.ml.build_reduce_text_classifier(tok2vec:Model,exclusive_classes:bool,use_reduce_first:bool,use_reduce_last:bool,use_reduce_max:bool,use_reduce_mean:bool,nO:Optional[int]=None)->Model[List[Doc], Floats2d]
spacy.ml.build_simple_cnn_text_classifier(tok2vec:Model,exclusive_classes:bool,nO:Optional[int]=None)->Model[List[Doc], Floats2d]
spacy.ml.build_text_classifier_lowdata(width:int,dropout:Optional[float],nO:Optional[int]=None)->Model[List[Doc], Floats2d]
spacy.ml.build_text_classifier_v2(tok2vec:Model[List[Doc],List[Floats2d]],linear_model:Model[List[Doc],Floats2d],nO:Optional[int]=None)->Model[List[Doc], Floats2d]
spacy.ml.build_textcat_parametric_attention_v1(tok2vec:Model[List[Doc],List[Floats2d]],exclusive_classes:bool,nO:Optional[int]=None)->Model[List[Doc], Floats2d]
spacy.ml.init_ensemble_textcat(model,X,Y)->Model
spacy.ml.models.textcat._build_bow_text_classifier(exclusive_classes:bool,ngram_size:int,no_output_layer:bool,sparse_linear:Model[Tuple[ArrayXd,ArrayXd,ArrayXd],ArrayXd],nO:Optional[int]=None)->Model[List[Doc], Floats2d]
spacy.ml.models.textcat._build_parametric_attention_with_residual_nonlinear(*,tok2vec:Model[List[Doc],List[Floats2d]],nonlinear_layer:Model[Floats2d,Floats2d],key_transform:Optional[Model[Floats2d,Floats2d]]=None)->Model[List[Doc], Floats2d]
spacy.ml.models.textcat._init_parametric_attention_with_residual_nonlinear(model,X,Y)->Model
spacy.ml.models.textcat.build_bow_text_classifier(exclusive_classes:bool,ngram_size:int,no_output_layer:bool,nO:Optional[int]=None)->Model[List[Doc], Floats2d]
spacy.ml.models.textcat.build_bow_text_classifier_v3(exclusive_classes:bool,ngram_size:int,no_output_layer:bool,length:int=262144,nO:Optional[int]=None)->Model[List[Doc], Floats2d]
spacy.ml.models.textcat.build_reduce_text_classifier(tok2vec:Model,exclusive_classes:bool,use_reduce_first:bool,use_reduce_last:bool,use_reduce_max:bool,use_reduce_mean:bool,nO:Optional[int]=None)->Model[List[Doc], Floats2d]
spacy.ml.models.textcat.build_simple_cnn_text_classifier(tok2vec:Model,exclusive_classes:bool,nO:Optional[int]=None)->Model[List[Doc], Floats2d]
spacy.ml.models.textcat.build_text_classifier_lowdata(width:int,dropout:Optional[float],nO:Optional[int]=None)->Model[List[Doc], Floats2d]
spacy.ml.models.textcat.build_text_classifier_v2(tok2vec:Model[List[Doc],List[Floats2d]],linear_model:Model[List[Doc],Floats2d],nO:Optional[int]=None)->Model[List[Doc], Floats2d]
spacy.ml.models.textcat.build_textcat_parametric_attention_v1(tok2vec:Model[List[Doc],List[Floats2d]],exclusive_classes:bool,nO:Optional[int]=None)->Model[List[Doc], Floats2d]
spacy.ml.models.textcat.init_ensemble_textcat(model,X,Y)->Model
spacy.ml.models.textcat.resize_and_set_ref(model,new_nO,resizable_layer)
spacy.ml.resize_and_set_ref(model,new_nO,resizable_layer)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/ml/models/parser.py----------------------------------------
A:spacy.ml.models.parser.tok2vec->chain(tok2vec, list2array(), Linear(hidden_width, t2v_width))
A:spacy.ml.models.parser.lower->model.get_ref('lower')
A:spacy.ml.models.parser.upper->model.get_ref('upper')
A:spacy.ml.models.parser.nI->smaller.maybe_get_dim('nI')
A:spacy.ml.models.parser.larger->_define_lower(nO=new_nO, nI=nI, nF=nF, nP=nP)
A:spacy.ml.models.parser.larger_W->_define_lower(nO=new_nO, nI=nI, nF=nF, nP=nP).ops.alloc4f(nF, new_nO, nP, nI)
A:spacy.ml.models.parser.larger_b->_define_lower(nO=new_nO, nI=nI, nF=nF, nP=nP).ops.alloc2f(new_nO, nP)
A:spacy.ml.models.parser.smaller_W->smaller.get_param('W')
A:spacy.ml.models.parser.smaller_b->smaller.get_param('b')
A:spacy.ml.models.parser.old_nO->smaller.get_dim('nO')
A:spacy.ml.models.parser.nF->smaller.maybe_get_dim('nF')
A:spacy.ml.models.parser.nP->smaller.maybe_get_dim('nP')
A:spacy.ml.models.parser.larger_pad->_define_lower(nO=new_nO, nI=nI, nF=nF, nP=nP).ops.alloc4f(1, nF, new_nO, nP)
A:spacy.ml.models.parser.smaller_pad->smaller.get_param('pad')
spacy.ml._define_lower(nO,nF,nI,nP)
spacy.ml._define_upper(nO,nI)
spacy.ml._resize_lower(model,new_nO)
spacy.ml._resize_upper(model,new_nO)
spacy.ml.build_tb_parser_model(tok2vec:Model[List[Doc],List[Floats2d]],state_type:Literal['parser','ner'],extra_state_tokens:bool,hidden_width:int,maxout_pieces:int,use_upper:bool,nO:Optional[int]=None)->Model
spacy.ml.models.parser._define_lower(nO,nF,nI,nP)
spacy.ml.models.parser._define_upper(nO,nI)
spacy.ml.models.parser._resize_lower(model,new_nO)
spacy.ml.models.parser._resize_upper(model,new_nO)
spacy.ml.models.parser.build_tb_parser_model(tok2vec:Model[List[Doc],List[Floats2d]],state_type:Literal['parser','ner'],extra_state_tokens:bool,hidden_width:int,maxout_pieces:int,use_upper:bool,nO:Optional[int]=None)->Model
spacy.ml.models.parser.resize_output(model,new_nO)
spacy.ml.resize_output(model,new_nO)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/ml/models/entity_linker.py----------------------------------------
A:spacy.ml.models.entity_linker.token_width->tok2vec.maybe_get_dim('nO')
A:spacy.ml.models.entity_linker.output_layer->Linear(nO=nO, nI=token_width)
A:spacy.ml.models.entity_linker.sent_index->sentences.index(ent.sent)
A:spacy.ml.models.entity_linker.start_sentence->max(0, sent_index - n_sents)
A:spacy.ml.models.entity_linker.end_sentence->min(len(sentences) - 1, sent_index + n_sents)
A:spacy.ml.models.entity_linker.lengths->model.ops.asarray1i([len(cands) for cands in candidates])
A:spacy.ml.models.entity_linker.out->Ragged(model.ops.flatten(candidates), lengths)
A:spacy.ml.models.entity_linker.kb->InMemoryLookupKB(vocab, entity_vector_length=1)
spacy.ml.build_nel_encoder(tok2vec:Model,nO:Optional[int]=None)->Model[List[Doc], Floats2d]
spacy.ml.build_span_maker(n_sents:int=0)->Model
spacy.ml.create_candidates()->Callable[[KnowledgeBase, Span], Iterable[Candidate]]
spacy.ml.create_candidates_batch()->Callable[[KnowledgeBase, Iterable[Span]], Iterable[Iterable[Candidate]]]
spacy.ml.empty_kb(entity_vector_length:int)->Callable[[Vocab], KnowledgeBase]
spacy.ml.empty_kb_for_config()->Callable[[Vocab, int], KnowledgeBase]
spacy.ml.load_kb(kb_path:Path)->Callable[[Vocab], KnowledgeBase]
spacy.ml.models.entity_linker.build_nel_encoder(tok2vec:Model,nO:Optional[int]=None)->Model[List[Doc], Floats2d]
spacy.ml.models.entity_linker.build_span_maker(n_sents:int=0)->Model
spacy.ml.models.entity_linker.create_candidates()->Callable[[KnowledgeBase, Span], Iterable[Candidate]]
spacy.ml.models.entity_linker.create_candidates_batch()->Callable[[KnowledgeBase, Iterable[Span]], Iterable[Iterable[Candidate]]]
spacy.ml.models.entity_linker.empty_kb(entity_vector_length:int)->Callable[[Vocab], KnowledgeBase]
spacy.ml.models.entity_linker.empty_kb_for_config()->Callable[[Vocab, int], KnowledgeBase]
spacy.ml.models.entity_linker.load_kb(kb_path:Path)->Callable[[Vocab], KnowledgeBase]
spacy.ml.models.entity_linker.span_maker_forward(model,docs:List[Doc],is_train)->Tuple[Ragged, Callable]
spacy.ml.span_maker_forward(model,docs:List[Doc],is_train)->Tuple[Ragged, Callable]


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/training/example.pyi----------------------------------------
spacy.training.Example(self,predicted:Doc,reference:Doc,*,alignment:Optional[Alignment]=None)
spacy.training.Example.__len__(self)->int
spacy.training.Example.__repr__(self)->str
spacy.training.Example.__str__(self)->str
spacy.training.Example.alignment(self)->Alignment
spacy.training.Example.copy(self)->Example
spacy.training.Example.from_dict(cls,predicted:Doc,example_dict:Dict[str,Any])->Example
spacy.training.Example.get_aligned(self,field:str,as_string=False)
spacy.training.Example.get_aligned_ents_and_ner(self)->Tuple[List[Span], List[str]]
spacy.training.Example.get_aligned_ner(self)->List[str]
spacy.training.Example.get_aligned_parse(self,projectivize=True)
spacy.training.Example.get_aligned_sent_starts(self)
spacy.training.Example.get_aligned_spans_x2y(self,x_spans:Iterable[Span],allow_overlap=False)->List[Span]
spacy.training.Example.get_aligned_spans_y2x(self,y_spans:Iterable[Span],allow_overlap=False)->List[Span]
spacy.training.Example.get_matching_ents(self,check_label:bool=True)->List[Span]
spacy.training.Example.predicted(self)->Doc
spacy.training.Example.predicted(self,doc:Doc)->None
spacy.training.Example.reference(self)->Doc
spacy.training.Example.reference(self,doc:Doc)->None
spacy.training.Example.split_sents(self)->List[Example]
spacy.training.Example.text(self)->str
spacy.training.Example.to_dict(self)->Dict[str, Any]
spacy.training.example.Example(self,predicted:Doc,reference:Doc,*,alignment:Optional[Alignment]=None)
spacy.training.example.Example.__init__(self,predicted:Doc,reference:Doc,*,alignment:Optional[Alignment]=None)
spacy.training.example.Example.__len__(self)->int
spacy.training.example.Example.__repr__(self)->str
spacy.training.example.Example.__str__(self)->str
spacy.training.example.Example.alignment(self)->Alignment
spacy.training.example.Example.copy(self)->Example
spacy.training.example.Example.from_dict(cls,predicted:Doc,example_dict:Dict[str,Any])->Example
spacy.training.example.Example.get_aligned(self,field:str,as_string=False)
spacy.training.example.Example.get_aligned_ents_and_ner(self)->Tuple[List[Span], List[str]]
spacy.training.example.Example.get_aligned_ner(self)->List[str]
spacy.training.example.Example.get_aligned_parse(self,projectivize=True)
spacy.training.example.Example.get_aligned_sent_starts(self)
spacy.training.example.Example.get_aligned_spans_x2y(self,x_spans:Iterable[Span],allow_overlap=False)->List[Span]
spacy.training.example.Example.get_aligned_spans_y2x(self,y_spans:Iterable[Span],allow_overlap=False)->List[Span]
spacy.training.example.Example.get_matching_ents(self,check_label:bool=True)->List[Span]
spacy.training.example.Example.predicted(self)->Doc
spacy.training.example.Example.predicted(self,doc:Doc)->None
spacy.training.example.Example.reference(self)->Doc
spacy.training.example.Example.reference(self,doc:Doc)->None
spacy.training.example.Example.split_sents(self)->List[Example]
spacy.training.example.Example.text(self)->str
spacy.training.example.Example.to_dict(self)->Dict[str, Any]
spacy.training.example._fix_legacy_dict_data(example_dict)
spacy.training.example._parse_example_dict_data(example_dict)
spacy.training.example.annotations_to_doc(vocab:Vocab,tok_annot:Dict[str,Any],doc_annot:Dict[str,Any])->Doc
spacy.training.example.validate_examples(examples:Iterable[Example],method:str)->None
spacy.training.example.validate_get_examples(get_examples:Callable[[],Iterable[Example]],method:str)
spacy.training.validate_examples(examples:Iterable[Example],method:str)->None
spacy.training.validate_get_examples(get_examples:Callable[[],Iterable[Example]],method:str)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/training/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/training/initialize.py----------------------------------------
A:spacy.training.initialize.config->load_model_from_config(raw_config, auto_fill=True).config.interpolate()
A:spacy.training.initialize.sourced->get_sourced_components(config)
A:spacy.training.initialize.nlp->load_model_from_config(raw_config, auto_fill=True)
A:spacy.training.initialize.T->util.registry.resolve(config['training'], schema=ConfigSchemaTraining)
A:spacy.training.initialize.(train_corpus, dev_corpus)->resolve_dot_names(config, dot_names)
A:spacy.training.initialize.data_path->ensure_path(data)
A:spacy.training.initialize.lex_attrs->srsly.read_jsonl(data_path)
A:spacy.training.initialize.sourced_vectors_hashes->load_model_from_config(raw_config, auto_fill=True).meta.pop('_sourced_vectors_hashes', {})
A:spacy.training.initialize.vectors_hash->hash(nlp.vocab.vectors.to_bytes(exclude=['strings']))
A:spacy.training.initialize.vectors_nlp->load_model(name, vocab=nlp.vocab, exclude=exclude)
A:spacy.training.initialize.err->thinc.api.ConfigValidationError.from_error(e, title=title, desc=desc)
A:spacy.training.initialize.lex.rank->load_model_from_config(raw_config, auto_fill=True).vocab.vectors.key2row.get(lex.orth, OOV_RANK)
A:spacy.training.initialize.init_tok2vec->ensure_path(I['init_tok2vec'])
A:spacy.training.initialize.weights_data->zip_file.open(names[0]).read()
A:spacy.training.initialize.layer->get_tok2vec_ref(nlp, P)
A:spacy.training.initialize.vectors_loc->ensure_path(vectors_loc)
A:spacy.training.initialize.nlp.vocab.vectors->Vectors(strings=nlp.vocab.strings, data=vectors_data, keys=vector_keys, attr=attr)
A:spacy.training.initialize.(vectors_data, vector_keys, floret_settings)->read_vectors(vectors_loc, truncate, mode=mode)
A:spacy.training.initialize.f->ensure_shape(vectors_loc)
A:spacy.training.initialize.header_parts->next(f).split()
A:spacy.training.initialize.shape->tuple((int(size) for size in first_line.split()[:2]))
A:spacy.training.initialize.vectors_data->numpy.zeros(shape=shape, dtype='f')
A:spacy.training.initialize.line->line.rstrip().rstrip()
A:spacy.training.initialize.pieces->line.rstrip().rstrip().rsplit(' ', vectors_data.shape[1])
A:spacy.training.initialize.word->line.rstrip().rstrip().rsplit(' ', vectors_data.shape[1]).pop(0)
A:spacy.training.initialize.vectors_data[i]->numpy.asarray(pieces, dtype='f')
A:spacy.training.initialize.loc->ensure_path(loc)
A:spacy.training.initialize.zip_file->zipfile.ZipFile(str(loc))
A:spacy.training.initialize.names->zipfile.ZipFile(str(loc)).namelist()
A:spacy.training.initialize.file_->zipfile.ZipFile(str(loc)).open(names[0])
A:spacy.training.initialize.lines->open_file(vectors_loc)
A:spacy.training.initialize.first_line->next(lines)
A:spacy.training.initialize.lines2->open_file(vectors_loc)
spacy.training.initialize.convert_vectors(nlp:'Language',vectors_loc:Optional[Path],*,truncate:int,prune:int,name:Optional[str]=None,mode:str=VectorsMode.default,attr:str='ORTH')->None
spacy.training.initialize.ensure_shape(vectors_loc)
spacy.training.initialize.init_nlp(config:Config,*,use_gpu:int=-1)->'Language'
spacy.training.initialize.init_tok2vec(nlp:'Language',pretrain_config:Dict[str,Any],init_config:Dict[str,Any])->bool
spacy.training.initialize.init_vocab(nlp:'Language',*,data:Optional[Path]=None,lookups:Optional[Lookups]=None,vectors:Optional[str]=None)->None
spacy.training.initialize.load_vectors_into_model(nlp:'Language',name:Union[str,Path],*,add_strings:bool=True)->None
spacy.training.initialize.open_file(loc:Union[str,Path])->IO
spacy.training.initialize.read_vectors(vectors_loc:Path,truncate_vectors:int,*,mode:str=VectorsMode.default)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/training/alignment.py----------------------------------------
A:spacy.training.alignment.x2y->AlignmentArray(x2y)
A:spacy.training.alignment.y2x->AlignmentArray(y2x)
A:spacy.training.alignment.(x2y, y2x)->get_alignments(A, B)
spacy.training.Alignment
spacy.training.Alignment.from_indices(cls,x2y:List[List[int]],y2x:List[List[int]])->'Alignment'
spacy.training.Alignment.from_strings(cls,A:List[str],B:List[str])->'Alignment'
spacy.training.alignment.Alignment
spacy.training.alignment.Alignment.from_indices(cls,x2y:List[List[int]],y2x:List[List[int]])->'Alignment'
spacy.training.alignment.Alignment.from_strings(cls,A:List[str],B:List[str])->'Alignment'


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/training/callbacks.py----------------------------------------
A:spacy.training.callbacks.base_nlp->load_model(vocab)
spacy.training.callbacks.create_copy_from_base_model(tokenizer:Optional[str]=None,vocab:Optional[str]=None)->Callable[['Language'], 'Language']
spacy.training.create_copy_from_base_model(tokenizer:Optional[str]=None,vocab:Optional[str]=None)->Callable[['Language'], 'Language']


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/training/pretrain.py----------------------------------------
A:spacy.training.pretrain.msg->Printer(no_print=silent)
A:spacy.training.pretrain.nlp->load_model_from_config(config)
A:spacy.training.pretrain._config->load_model_from_config(config).config.interpolate()
A:spacy.training.pretrain.P->util.registry.resolve(_config['pretraining'], schema=ConfigSchemaPretrain)
A:spacy.training.pretrain.corpus->dot_to_object(_config, P['corpus'])
A:spacy.training.pretrain.model->create_function(nlp.vocab, tok2vec)
A:spacy.training.pretrain.epoch_resume->_resume_model(model, resume_path, epoch_resume, silent=silent)
A:spacy.training.pretrain.tracker->ProgressTracker(frequency=10000)
A:spacy.training.pretrain.docs->ensure_docs(batch)
A:spacy.training.pretrain.loss->make_update(model, docs, optimizer, objective)
A:spacy.training.pretrain.progress->ProgressTracker(frequency=10000).update(epoch, loss, docs)
A:spacy.training.pretrain.weights_data->file_.read()
A:spacy.training.pretrain.model_name->re.search('model\\d+\\.bin', str(resume_path))
A:spacy.training.pretrain.(predictions, backprop)->create_function(nlp.vocab, tok2vec).begin_update(docs)
A:spacy.training.pretrain.(loss, gradients)->objective_func(model.ops, docs, predictions)
A:spacy.training.pretrain.tok2vec->get_tok2vec_ref(nlp, pretrain_config)
A:spacy.training.pretrain.layer->layer.get_ref(pretrain_config['layer']).get_ref(pretrain_config['layer'])
A:spacy.training.pretrain.self.words_per_epoch->Counter()
A:spacy.training.pretrain.self.last_time->time.time()
A:spacy.training.pretrain.words_in_batch->sum((len(doc) for doc in docs))
A:spacy.training.pretrain.self.prev_loss->float(self.loss)
A:spacy.training.pretrain.n_digits->len(str(int(figure)))
A:spacy.training.pretrain.n_decimal->min(n_decimal, max_decimal)
spacy.training.pretrain.ProgressTracker(self,frequency=1000000)
spacy.training.pretrain.ProgressTracker.__init__(self,frequency=1000000)
spacy.training.pretrain.ProgressTracker.update(self,epoch,loss,docs)
spacy.training.pretrain._resume_model(model:Model,resume_path:Path,epoch_resume:Optional[int],silent:bool=True)->int
spacy.training.pretrain._smart_round(figure:Union[float,int],width:int=10,max_decimal:int=4)->str
spacy.training.pretrain.create_pretraining_model(nlp,pretrain_config)
spacy.training.pretrain.ensure_docs(examples_or_docs:Iterable[Union[Doc,Example]])->List[Doc]
spacy.training.pretrain.get_tok2vec_ref(nlp,pretrain_config)
spacy.training.pretrain.make_update(model:Model,docs:Iterable[Doc],optimizer:Optimizer,objective_func:Callable)->float
spacy.training.pretrain.pretrain(config:Config,output_dir:Path,resume_path:Optional[Path]=None,epoch_resume:Optional[int]=None,use_gpu:int=-1,silent:bool=True,skip_last:bool=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/training/iob_utils.py----------------------------------------
A:spacy.training.iob_utils.tags->list(tags)
A:spacy.training.iob_utils.tag->list(tags).pop(0)
A:spacy.training.iob_utils.ents->doc_to_biluo_tags(doc, missing='-')
A:spacy.training.iob_utils.start_token->starts.get(start_char)
A:spacy.training.iob_utils.end_token->ends.get(end_char)
A:spacy.training.iob_utils.entity_chars->set()
A:spacy.training.iob_utils.ent_str->str(entities)
A:spacy.training.iob_utils.token_offsets->tags_to_entities(tags)
A:spacy.training.iob_utils.span->Span(doc, start_idx, end_idx + 1, label=label)
A:spacy.training.iob_utils.spans->biluo_tags_to_spans(doc, tags)
spacy.training.biluo_tags_to_offsets(doc:Doc,tags:Iterable[str])->List[Tuple[int, int, Union[str, int]]]
spacy.training.biluo_tags_to_spans(doc:Doc,tags:Iterable[str])->List[Span]
spacy.training.biluo_to_iob(tags:Iterable[str])->List[str]
spacy.training.iob_to_biluo(tags:Iterable[str])->List[str]
spacy.training.iob_utils._consume_ent(tags:List[str])->List[str]
spacy.training.iob_utils._consume_os(tags:List[str])->Iterator[str]
spacy.training.iob_utils._doc_to_biluo_tags_with_partial(doc:Doc)->List[str]
spacy.training.iob_utils.biluo_tags_to_offsets(doc:Doc,tags:Iterable[str])->List[Tuple[int, int, Union[str, int]]]
spacy.training.iob_utils.biluo_tags_to_spans(doc:Doc,tags:Iterable[str])->List[Span]
spacy.training.iob_utils.biluo_to_iob(tags:Iterable[str])->List[str]
spacy.training.iob_utils.doc_to_biluo_tags(doc:Doc,missing:str='O')
spacy.training.iob_utils.iob_to_biluo(tags:Iterable[str])->List[str]
spacy.training.iob_utils.offsets_to_biluo_tags(doc:Doc,entities:Iterable[Tuple[int,int,Union[str,int]]],missing:str='O')->List[str]
spacy.training.iob_utils.remove_bilu_prefix(label:str)->str
spacy.training.iob_utils.split_bilu_label(label:str)->Tuple[str, str]
spacy.training.iob_utils.tags_to_entities(tags:Iterable[str])->List[Tuple[str, int, int]]
spacy.training.offsets_to_biluo_tags(doc:Doc,entities:Iterable[Tuple[int,int,Union[str,int]]],missing:str='O')->List[str]
spacy.training.remove_bilu_prefix(label:str)->str
spacy.training.split_bilu_label(label:str)->Tuple[str, str]
spacy.training.tags_to_entities(tags:Iterable[str])->List[Tuple[str, int, int]]


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/training/augment.py----------------------------------------
A:spacy.training.augment.example->make_whitespace_variant(nlp, example, random.choice(whitespace_variants), random.randrange(0, len(example.reference)))
A:spacy.training.augment.orig_dict->make_whitespace_variant(nlp, example, random.choice(whitespace_variants), random.randrange(0, len(example.reference))).to_dict()
A:spacy.training.augment.orig_dict['doc_annotation']['entities']->_doc_to_biluo_tags_with_partial(example.reference)
A:spacy.training.augment.(variant_text, variant_token_annot)->make_orth_variants(nlp, raw_text, orig_dict['token_annotation'], orth_variants, lower=raw_text is not None and random.random() < lower)
A:spacy.training.augment.example_dict->make_whitespace_variant(nlp, example, random.choice(whitespace_variants), random.randrange(0, len(example.reference))).to_dict()
A:spacy.training.augment.example_dict['doc_annotation']['entities']->_doc_to_biluo_tags_with_partial(example.reference)
A:spacy.training.augment.doc->nlp.make_doc(example.text.lower())
A:spacy.training.augment.words->example_dict.get('token_annotation', {}).get('ORTH', [])
A:spacy.training.augment.tags->example_dict.get('token_annotation', {}).get('TAG', [])
A:spacy.training.augment.raw->construct_modified_raw_text(token_dict)
A:spacy.training.augment.ndsv->orth_variants.get('single', [])
A:spacy.training.augment.ndpv->orth_variants.get('paired', [])
A:spacy.training.augment.pair_idx->pair.index(words[word_idx])
A:spacy.training.augment.doc_dict->make_whitespace_variant(nlp, example, random.choice(whitespace_variants), random.randrange(0, len(example.reference))).to_dict().get('doc_annotation', {})
A:spacy.training.augment.token_dict->make_whitespace_variant(nlp, example, random.choice(whitespace_variants), random.randrange(0, len(example.reference))).to_dict().get('token_annotation', {})
A:spacy.training.augment.length->len(words)
A:spacy.training.augment.(ent_iob_prev, ent_type_prev)->split_bilu_label(ent_prev)
A:spacy.training.augment.(ent_iob_next, ent_type_next)->split_bilu_label(ent_next)
spacy.training.augment.combined_augmenter(nlp:'Language',example:Example,*,lower_level:float=0.0,orth_level:float=0.0,orth_variants:Optional[Dict[str,List[Dict]]]=None,whitespace_level:float=0.0,whitespace_per_token:float=0.0,whitespace_variants:Optional[List[str]]=None)->Iterator[Example]
spacy.training.augment.construct_modified_raw_text(token_dict)
spacy.training.augment.create_combined_augmenter(lower_level:float,orth_level:float,orth_variants:Optional[Dict[str,List[Dict]]],whitespace_level:float,whitespace_per_token:float,whitespace_variants:Optional[List[str]])->Callable[['Language', Example], Iterator[Example]]
spacy.training.augment.create_lower_casing_augmenter(level:float)->Callable[['Language', Example], Iterator[Example]]
spacy.training.augment.create_orth_variants_augmenter(level:float,lower:float,orth_variants:Dict[str,List[Dict]])->Callable[['Language', Example], Iterator[Example]]
spacy.training.augment.dont_augment(nlp:'Language',example:Example)->Iterator[Example]
spacy.training.augment.lower_casing_augmenter(nlp:'Language',example:Example,*,level:float)->Iterator[Example]
spacy.training.augment.make_lowercase_variant(nlp:'Language',example:Example)
spacy.training.augment.make_orth_variants(nlp:'Language',raw:str,token_dict:Dict[str,List[str]],orth_variants:Dict[str,List[Dict[str,List[str]]]],*,lower:bool=False)->Tuple[str, Dict[str, List[str]]]
spacy.training.augment.make_whitespace_variant(nlp:'Language',example:Example,whitespace:str,position:int)->Example
spacy.training.augment.orth_variants_augmenter(nlp:'Language',example:Example,orth_variants:Dict[str,List[Dict]],*,level:float=0.0,lower:float=0.0)->Iterator[Example]
spacy.training.dont_augment(nlp:'Language',example:Example)->Iterator[Example]
spacy.training.orth_variants_augmenter(nlp:'Language',example:Example,orth_variants:Dict[str,List[Dict]],*,level:float=0.0,lower:float=0.0)->Iterator[Example]


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/training/corpus.py----------------------------------------
A:spacy.training.corpus.path->util.ensure_path(path)
A:spacy.training.corpus.seen->set()
A:spacy.training.corpus.self.path->util.ensure_path(path)
A:spacy.training.corpus.ref_docs->list(ref_docs)
A:spacy.training.corpus.examples->self.make_examples(nlp, ref_docs)
A:spacy.training.corpus.eg->self._make_example(nlp, ref_sent, True)
A:spacy.training.corpus.loc->util.ensure_path(loc)
A:spacy.training.corpus.doc_bin->DocBin().from_disk(loc)
A:spacy.training.corpus.docs->DocBin().from_disk(loc).get_docs(vocab)
A:spacy.training.corpus.records->srsly.read_jsonl(loc)
A:spacy.training.corpus.doc->nlp.make_doc(text)
A:spacy.training.corpus.text->text.rstrip('\r\n').rstrip('\r\n')
spacy.training.Corpus(self,path:Union[str,Path],*,limit:int=0,gold_preproc:bool=False,max_length:int=0,augmenter:Optional[Callable]=None,shuffle:bool=False)
spacy.training.Corpus._make_example(self,nlp:'Language',reference:Doc,gold_preproc:bool)->Example
spacy.training.Corpus.make_examples(self,nlp:'Language',reference_docs:Iterable[Doc])->Iterator[Example]
spacy.training.Corpus.make_examples_gold_preproc(self,nlp:'Language',reference_docs:Iterable[Doc])->Iterator[Example]
spacy.training.Corpus.read_docbin(self,vocab:Vocab,locs:Iterable[Union[str,Path]])->Iterator[Doc]
spacy.training.JsonlCorpus(self,path:Optional[Union[str,Path]],*,limit:int=0,min_length:int=0,max_length:int=0)
spacy.training.PlainTextCorpus(self,path:Optional[Union[str,Path]],*,min_length:int=0,max_length:int=0)
spacy.training.corpus.Corpus(self,path:Union[str,Path],*,limit:int=0,gold_preproc:bool=False,max_length:int=0,augmenter:Optional[Callable]=None,shuffle:bool=False)
spacy.training.corpus.Corpus.__init__(self,path:Union[str,Path],*,limit:int=0,gold_preproc:bool=False,max_length:int=0,augmenter:Optional[Callable]=None,shuffle:bool=False)
spacy.training.corpus.Corpus._make_example(self,nlp:'Language',reference:Doc,gold_preproc:bool)->Example
spacy.training.corpus.Corpus.make_examples(self,nlp:'Language',reference_docs:Iterable[Doc])->Iterator[Example]
spacy.training.corpus.Corpus.make_examples_gold_preproc(self,nlp:'Language',reference_docs:Iterable[Doc])->Iterator[Example]
spacy.training.corpus.Corpus.read_docbin(self,vocab:Vocab,locs:Iterable[Union[str,Path]])->Iterator[Doc]
spacy.training.corpus.JsonlCorpus(self,path:Optional[Union[str,Path]],*,limit:int=0,min_length:int=0,max_length:int=0)
spacy.training.corpus.JsonlCorpus.__init__(self,path:Optional[Union[str,Path]],*,limit:int=0,min_length:int=0,max_length:int=0)
spacy.training.corpus.PlainTextCorpus(self,path:Optional[Union[str,Path]],*,min_length:int=0,max_length:int=0)
spacy.training.corpus.PlainTextCorpus.__init__(self,path:Optional[Union[str,Path]],*,min_length:int=0,max_length:int=0)
spacy.training.corpus.create_docbin_reader(path:Optional[Path],gold_preproc:bool,max_length:int=0,limit:int=0,augmenter:Optional[Callable]=None)->Callable[['Language'], Iterable[Example]]
spacy.training.corpus.create_jsonl_reader(path:Optional[Union[str,Path]],min_length:int=0,max_length:int=0,limit:int=0)->Callable[['Language'], Iterable[Example]]
spacy.training.corpus.create_plain_text_reader(path:Optional[Path],min_length:int=0,max_length:int=0)->Callable[['Language'], Iterable[Example]]
spacy.training.corpus.read_labels(path:Path,*,require:bool=False)
spacy.training.corpus.walk_corpus(path:Union[str,Path],file_type)->List[Path]


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/training/batchers.py----------------------------------------
A:spacy.training.batchers.ItemT->TypeVar('ItemT')
A:spacy.training.batchers.size_->iter(size)
A:spacy.training.batchers.outer_batch->list(outer_batch)
A:spacy.training.batchers.target_size->next(size_)
A:spacy.training.batchers.n_words->get_length(seq)
spacy.training.batchers._batch_by_length(seqs:Sequence[Any],max_words:int,get_length=len)->List[List[Any]]
spacy.training.batchers.configure_minibatch(size:Sizing,get_length:Optional[Callable[[ItemT],int]]=None)->BatcherT
spacy.training.batchers.configure_minibatch_by_padded_size(*,size:Sizing,buffer:int,discard_oversize:bool,get_length:Optional[Callable[[ItemT],int]]=None)->BatcherT
spacy.training.batchers.configure_minibatch_by_words(*,size:Sizing,tolerance:float,discard_oversize:bool,get_length:Optional[Callable[[ItemT],int]]=None)->BatcherT
spacy.training.batchers.minibatch_by_padded_size(seqs:Iterable[ItemT],size:Sizing,buffer:int=256,discard_oversize:bool=False,get_length:Callable=len)->Iterable[List[ItemT]]
spacy.training.batchers.minibatch_by_words(seqs:Iterable[ItemT],size:Sizing,tolerance=0.2,discard_oversize=False,get_length=len)->Iterable[List[ItemT]]
spacy.training.minibatch_by_padded_size(seqs:Iterable[ItemT],size:Sizing,buffer:int=256,discard_oversize:bool=False,get_length:Callable=len)->Iterable[List[ItemT]]
spacy.training.minibatch_by_words(seqs:Iterable[ItemT],size:Sizing,tolerance=0.2,discard_oversize=False,get_length=len)->Iterable[List[ItemT]]


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/training/loggers.py----------------------------------------
A:spacy.training.loggers.output_file->util.ensure_path(output_file)
A:spacy.training.loggers.msg->Printer(no_print=True)
A:spacy.training.loggers.output_stream->open(output_file, 'w', encoding='utf-8')
A:spacy.training.loggers.(table_header, table_widths, table_aligns)->setup_table(cols=['E', '#'] + loss_cols + score_cols + ['Score'], widths=[3, 6] + [8 for _ in loss_cols] + [6 for _ in score_cols] + [6])
A:spacy.training.loggers.log_losses[pipe_name]->float(info['losses'][pipe_name])
A:spacy.training.loggers.score->float(score)
A:spacy.training.loggers.err->errors.Errors.E916.format(name=col, score_type=type(score))
A:spacy.training.loggers.progress->tqdm.tqdm(total=total, disable=None, leave=False, file=stderr, initial=initial)
spacy.training.console_logger(progress_bar:bool=False,console_output:bool=True,output_file:Optional[Union[str,Path]]=None)
spacy.training.console_logger_v3(progress_bar:Optional[str]=None,console_output:bool=True,output_file:Optional[Union[str,Path]]=None)
spacy.training.loggers.console_logger(progress_bar:bool=False,console_output:bool=True,output_file:Optional[Union[str,Path]]=None)
spacy.training.loggers.console_logger_v3(progress_bar:Optional[str]=None,console_output:bool=True,output_file:Optional[Union[str,Path]]=None)
spacy.training.loggers.setup_table(*,cols:List[str],widths:List[int],max_width:int=13)->Tuple[List[str], List[int], List[str]]


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/training/loop.py----------------------------------------
A:spacy.training.loop.msg->Printer(no_print=True)
A:spacy.training.loop.config->nlp.config.interpolate()
A:spacy.training.loop.T->util.registry.resolve(config['training'], schema=ConfigSchemaTraining)
A:spacy.training.loop.(train_corpus, dev_corpus)->resolve_dot_names(config, dot_names)
A:spacy.training.loop.before_to_disk->create_before_to_disk_callback(T['before_to_disk'])
A:spacy.training.loop.training_step_iterator->train_while_improving(nlp, optimizer, create_train_batches(nlp, train_corpus, batcher, T['max_epochs']), create_evaluation_callback(nlp, dev_corpus, score_weights), dropout=T['dropout'], accumulate_gradient=T['accumulate_gradient'], patience=T['patience'], max_steps=T['max_steps'], eval_frequency=T['eval_frequency'], exclude=frozen_components, annotating_components=annotating_components, before_update=before_update)
A:spacy.training.loop.(log_step, finalize_logger)->train_logger(nlp, stdout, stderr)
A:spacy.training.loop.info['output_path']->str(output_path / DIR_MODEL_LAST)
A:spacy.training.loop.dropouts->constant(dropout)
A:spacy.training.loop.start_time->timer()
A:spacy.training.loop.dropout->next(dropouts)
A:spacy.training.loop.(score, other_scores)->evaluate()
A:spacy.training.loop.best_result->max(((r_score, -r_step) for (r_score, r_step) in results))
A:spacy.training.loop.batch->list(batch)
A:spacy.training.loop.scores->nlp.evaluate(dev_corpus(nlp))
A:spacy.training.loop.weighted_score->sum((scores.get(s, 0.0) * weights.get(s, 0.0) for s in weights))
A:spacy.training.loop.keys->list(scores.keys())
A:spacy.training.loop.err->errors.Errors.E914.format(name='before_to_disk', value=type(modified_nlp))
A:spacy.training.loop.examples->corpus(nlp)
A:spacy.training.loop.nlp.meta['performance'][metric]->info['other_scores'].get(metric, 0.0)
A:spacy.training.loop.modified_nlp->callback(nlp)
spacy.training.loop.clean_output_dir(path:Optional[Path])->None
spacy.training.loop.create_before_to_disk_callback(callback:Optional[Callable[['Language'],'Language']])->Callable[['Language'], 'Language']
spacy.training.loop.create_evaluation_callback(nlp:'Language',dev_corpus:Callable,weights:Dict[str,float])->Callable[[], Tuple[float, Dict[str, float]]]
spacy.training.loop.create_train_batches(nlp:'Language',corpus:Callable[['Language'],Iterable[Example]],batcher:Callable[[Iterable[Example]],Iterable[Example]],max_epochs:int)
spacy.training.loop.subdivide_batch(batch,accumulate_gradient)
spacy.training.loop.train(nlp:'Language',output_path:Optional[Path]=None,*,use_gpu:int=-1,stdout:IO=sys.stdout,stderr:IO=sys.stderr)->Tuple['Language', Optional[Path]]
spacy.training.loop.train_while_improving(nlp:'Language',optimizer:Optimizer,train_data,evaluate,*,dropout:float,eval_frequency:int,accumulate_gradient:int,patience:int,max_steps:int,exclude:List[str],annotating_components:List[str],before_update:Optional[Callable[['Language',Dict[str,Any]],None]])
spacy.training.loop.update_meta(training:Union[Dict[str,Any],Config],nlp:'Language',info:Dict[str,Any])->None


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/training/converters/iob_to_docs.py----------------------------------------
A:spacy.training.converters.iob_to_docs.vocab->Vocab()
A:spacy.training.converters.iob_to_docs.msg->Printer(no_print=no_print)
A:spacy.training.converters.iob_to_docs.(sent_words, sent_tags, sent_iob)->zip(*sent_tokens)
A:spacy.training.converters.iob_to_docs.(sent_words, sent_iob)->zip(*sent_tokens)
A:spacy.training.converters.iob_to_docs.doc->Doc(vocab, words=words)
A:spacy.training.converters.iob_to_docs.biluo->iob_to_biluo(iob)
A:spacy.training.converters.iob_to_docs.entities->tags_to_entities(biluo)
spacy.training.converters.iob_to_docs(input_data,n_sents=10,no_print=False,*args,**kwargs)
spacy.training.converters.iob_to_docs.iob_to_docs(input_data,n_sents=10,no_print=False,*args,**kwargs)
spacy.training.converters.iob_to_docs.read_iob(raw_sents,vocab,n_sents)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/training/converters/conll_ner_to_docs.py----------------------------------------
A:spacy.training.converters.conll_ner_to_docs.msg->Printer(no_print=no_print)
A:spacy.training.converters.conll_ner_to_docs.input_data->segment_sents_and_docs(input_data, n_sents, doc_delimiter, model=model, msg=msg)
A:spacy.training.converters.conll_ner_to_docs.nlp->get_lang_class('xx')()
A:spacy.training.converters.conll_ner_to_docs.conll_doc->conll_doc.strip().strip()
A:spacy.training.converters.conll_ner_to_docs.conll_sent->conll_sent.strip().strip()
A:spacy.training.converters.conll_ner_to_docs.cols->list(zip(*[line.split() for line in lines]))
A:spacy.training.converters.conll_ner_to_docs.length->len(cols[0])
A:spacy.training.converters.conll_ner_to_docs.doc->Doc(nlp.vocab, words=words)
A:spacy.training.converters.conll_ner_to_docs.entities->tags_to_entities(biluo_tags)
A:spacy.training.converters.conll_ner_to_docs.sentencizer->get_lang_class('xx')().create_pipe('sentencizer')
A:spacy.training.converters.conll_ner_to_docs.lines->Doc(nlp.vocab, words=words).strip().split('\n')
A:spacy.training.converters.conll_ner_to_docs.nlpdoc->Doc(nlp.vocab, words=words)
A:spacy.training.converters.conll_ner_to_docs.sents->segment_sents_and_docs(input_data, n_sents, doc_delimiter, model=model, msg=msg).split(sent_delimiter)
spacy.training.converters.conll_ner_to_docs(input_data,n_sents=10,seg_sents=False,model=None,no_print=False,**kwargs)
spacy.training.converters.conll_ner_to_docs.conll_ner_to_docs(input_data,n_sents=10,seg_sents=False,model=None,no_print=False,**kwargs)
spacy.training.converters.conll_ner_to_docs.n_sents_info(msg,n_sents)
spacy.training.converters.conll_ner_to_docs.segment_docs(input_data,n_sents,doc_delimiter)
spacy.training.converters.conll_ner_to_docs.segment_sents_and_docs(doc,n_sents,doc_delimiter,model=None,msg=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/training/converters/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/training/converters/json_to_docs.py----------------------------------------
A:spacy.training.converters.json_to_docs.input_data->input_data.encode('utf8').encode('utf8')
A:spacy.training.converters.json_to_docs.example_dict->_fix_legacy_dict_data(json_para)
A:spacy.training.converters.json_to_docs.(tok_dict, doc_dict)->_parse_example_dict_data(example_dict)
A:spacy.training.converters.json_to_docs.doc->annotations_to_doc(nlp.vocab, tok_dict, doc_dict)
spacy.training.converters.json_to_docs(input_data,model=None,**kwargs)
spacy.training.converters.json_to_docs.json_to_docs(input_data,model=None,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/spacy3.8.6/lib/python3.9/site-packages/spacy/training/converters/conllu_to_docs.py----------------------------------------
A:spacy.training.converters.conllu_to_docs.msg->Printer(no_print=no_print)
A:spacy.training.converters.conllu_to_docs.sent_docs->read_conllx(input_data, append_morphology=append_morphology, ner_tag_pattern=MISC_NER_PATTERN, ner_map=ner_map, merge_subtokens=merge_subtokens)
A:spacy.training.converters.conllu_to_docs.lines->sent.strip().split('\n')
A:spacy.training.converters.conllu_to_docs.parts->line.split('\t')
A:spacy.training.converters.conllu_to_docs.vocab->Vocab()
A:spacy.training.converters.conllu_to_docs.set_ents->has_ner(input_data, ner_tag_pattern)
A:spacy.training.converters.conllu_to_docs.doc->merge_conllu_subtokens(lines, doc)
A:spacy.training.converters.conllu_to_docs.tag_match->re.match(tag_pattern, misc_part)
A:spacy.training.converters.conllu_to_docs.prefix->re.match(tag_pattern, misc_part).group(2)
A:spacy.training.converters.conllu_to_docs.suffix->ner_map.get(suffix, suffix)
A:spacy.training.converters.conllu_to_docs.(subtok_start, subtok_end)->id_.split('-')
A:spacy.training.converters.conllu_to_docs.ents->get_entities(lines, ner_tag_pattern, ner_map)
A:spacy.training.converters.conllu_to_docs.doc.ents->biluo_tags_to_spans(doc, ents)
A:spacy.training.converters.conllu_to_docs.doc_x->Doc(vocab, words=words, spaces=spaces, tags=tags, morphs=morphs, lemmas=lemmas, pos=poses, deps=deps, heads=heads)
A:spacy.training.converters.conllu_to_docs.(field, values)->feature.split('=', 1)
A:spacy.training.converters.conllu_to_docs.morphs[field]->set()
A:spacy.training.converters.conllu_to_docs.token._.merged_lemma->' '.join(lemmas)
A:spacy.training.converters.conllu_to_docs.token.tag_->'_'.join(tags)
A:spacy.training.converters.conllu_to_docs.token._.merged_morph->'|'.join(sorted(morphs.values()))
spacy.training.converters.conllu_to_docs(input_data,n_sents=10,append_morphology=False,ner_map=None,merge_subtokens=False,no_print=False,**_)
spacy.training.converters.conllu_to_docs.conllu_sentence_to_doc(vocab,lines,ner_tag_pattern,merge_subtokens=False,append_morphology=False,ner_map=None,set_ents=False)
spacy.training.converters.conllu_to_docs.conllu_to_docs(input_data,n_sents=10,append_morphology=False,ner_map=None,merge_subtokens=False,no_print=False,**_)
spacy.training.converters.conllu_to_docs.get_entities(lines,tag_pattern,ner_map=None)
spacy.training.converters.conllu_to_docs.has_ner(input_data,ner_tag_pattern)
spacy.training.converters.conllu_to_docs.merge_conllu_subtokens(lines,doc)
spacy.training.converters.conllu_to_docs.read_conllx(input_data,append_morphology=False,merge_subtokens=False,ner_tag_pattern='',ner_map=None)

