
----------------------------------------/home/zhang/Packages/dask/dask0.15.3/threaded.py----------------------------------------
A:dask.threaded.main_thread->current_thread()
A:dask.threaded.pools->defaultdict(dict)
A:dask.threaded.pools_lock->Lock()
A:dask.threaded.thread->current_thread()
A:dask.threaded.default_pool->ThreadPool()
A:dask.threaded.pool->ThreadPool(num_workers)
A:dask.threaded.results->get_async(pool.apply_async, len(pool._pool), dsk, result, cache=cache, get_id=_thread_get_id, pack_exception=pack_exception, **kwargs)
A:dask.threaded.active_threads->set(threading.enumerate())
dask.threaded._thread_get_id()
dask.threaded.get(dsk,result,cache=None,num_workers=None,**kwargs)
dask.threaded.pack_exception(e,dumps)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/local.py----------------------------------------
A:dask.local.cache->dict()
A:dask.local.data_keys->set()
A:dask.local.dsk2->dict(dsk).copy()
A:dask.local.dependents->reverse_dict(dependencies)
A:dask.local.waiting_data->dict(((k, v.copy()) for (k, v) in dependents.items() if v))
A:dask.local.ready_set->set([k for (k, v) in waiting.items() if not v])
A:dask.local.ready->sorted(ready_set, key=sortkey, reverse=True)
A:dask.local.waiting->dict(((k, v) for (k, v) in waiting.items() if v))
A:dask.local.(task, data)->loads(task_info)
A:dask.local.result->pack_exception(e, dumps)
A:dask.local.id->get_id()
A:dask.local.queue->Queue()
A:dask.local.result_flat->set([result])
A:dask.local.results->set(result_flat)
A:dask.local.dsk->dict(dsk)
A:dask.local.(_, _, pretask_cbs, posttask_cbs, _)->unpack_callbacks(callbacks)
A:dask.local.(dsk, dependencies)->cull(dsk, list(results))
A:dask.local.keyorder->order(dsk)
A:dask.local.state->start_state_from_dask(dsk, cache=cache, sortkey=keyorder.get)
A:dask.local.rerun_exceptions_locally->context._globals.get('rerun_exceptions_locally', False)
A:dask.local.key->state['ready'].pop()
A:dask.local.data->dict(((dep, state['cache'][dep]) for dep in get_dependencies(dsk, key)))
A:dask.local.(key, res_info, failed)->queue_get(queue)
A:dask.local.(exc, tb)->loads(res_info)
A:dask.local.(res, worker_id)->loads(res_info)
A:dask.local.res->func(*args, **kwds)
dask.local._execute_task(arg,cache,dsk=None)
dask.local.apply_sync(func,args=(),kwds={},callback=None)
dask.local.default_get_id()
dask.local.default_pack_exception(e,dumps)
dask.local.execute_task(key,task_info,dumps,loads,get_id,pack_exception)
dask.local.finish_task(dsk,key,state,results,sortkey,delete=True,release_data=release_data)
dask.local.get_async(apply_async,num_workers,dsk,result,cache=None,get_id=default_get_id,rerun_exceptions_locally=None,pack_exception=default_pack_exception,raise_exception=reraise,callbacks=None,dumps=identity,loads=identity,**kwargs)
dask.local.get_sync(dsk,keys,**kwargs)
dask.local.identity(x)
dask.local.nested_get(ind,coll)
dask.local.release_data(key,state,delete=True)
dask.local.sortkey(item)
dask.local.start_state_from_dask(dsk,cache=None,sortkey=None)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/order.py----------------------------------------
A:dask.order.dependents->reverse_dict(dependencies)
A:dask.order.ndeps->ndependents(dependencies, dependents)
A:dask.order.maxes->child_max(dependencies, dependents, ndeps)
A:dask.order.result->dict()
A:dask.order.num_needed->dict(((k, len(v)) for (k, v) in dependencies.items()))
A:dask.order.current->set((k for (k, v) in num_needed.items() if v == 0))
A:dask.order.key->set((k for (k, v) in num_needed.items() if v == 0)).pop()
A:dask.order.stack->sorted(roots, key=key, reverse=True)
A:dask.order.seen->set()
A:dask.order.item->sorted(roots, key=key, reverse=True).pop()
A:dask.order.deps->sorted(deps, key=key, reverse=True)
dask.order.child_max(dependencies,dependents,scores)
dask.order.dfs(dependencies,dependents,key=lambdax:x)
dask.order.ndependents(dependencies,dependents)
dask.order.order(dsk,dependencies=None)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/cache.py----------------------------------------
A:dask.cache.cache->cachey.Cache(cache, *args, **kwargs)
A:dask.cache.self.starttimes->dict()
A:dask.cache.self.durations->dict()
A:dask.cache.self.starttimes[key]->default_timer()
dask.cache.Cache(self,cache,*args,**kwargs)
dask.cache.Cache.__init__(self,cache,*args,**kwargs)
dask.cache.Cache._finish(self,dsk,state,errored)
dask.cache.Cache._posttask(self,key,value,dsk,state,id)
dask.cache.Cache._pretask(self,key,dsk,state)
dask.cache.Cache._start(self,dsk)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/compatibility.py----------------------------------------
A:dask.compatibility.reraise->_make_reraise()
A:dask.compatibility.f->gzip.GzipFile(fileobj=bio, mode='w')
A:dask.compatibility.result->BytesIO().read()
A:dask.compatibility.bio->BytesIO()
A:dask.compatibility.self.__obj->gzip.GzipFile(*args, **kwargs)
dask.compatibility.LZMAFile(self,*args,**kwargs)
dask.compatibility.LZMAFile.__init__(self,*args,**kwargs)
dask.compatibility.bind_method(cls,name,func)
dask.compatibility.getargspec(func)
dask.compatibility.skip(func)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/utils.py----------------------------------------
A:dask.utils.system_encoding->sys.getdefaultencoding()
A:dask.utils.(handle, filename)->tempfile.mkstemp(extension, dir=dir)
A:dask.utils.dirname->tempfile.mkdtemp(dir=dir)
A:dask.utils.f->open(filename, 'w' + mode)
A:dask.utils.old_cwd->os.getcwd()
A:dask.utils.seq->list(map(concrete, seq))
A:dask.utils.p->list(p)
A:dask.utils.cp->numpy.cumsum([0] + p)
A:dask.utils.random_state->numpy.random.RandomState(random_state)
A:dask.utils.x->numpy.random.RandomState(random_state).random_sample(n)
A:dask.utils.out->numpy.empty(n, dtype='i1')
A:dask.utils.random_data->numpy.random.RandomState(random_state).bytes(624 * n * 4)
A:dask.utils.l->list(np.frombuffer(random_data, dtype=np.uint32).reshape((n, -1)))
A:dask.utils.ONE_ARITY_BUILTINS->set([abs, all, any, bool, bytearray, bytes, callable, chr, classmethod, complex, dict, dir, enumerate, eval, float, format, frozenset, hash, hex, id, int, iter, len, list, max, min, next, oct, open, ord, range, repr, reversed, round, set, slice, sorted, staticmethod, str, sum, tuple, type, vars, zip, memoryview])
A:dask.utils.MULTI_ARITY_BUILTINS->set([compile, delattr, divmod, filter, getattr, hasattr, isinstance, issubclass, map, pow, setattr])
A:dask.utils.spec->getargspec(func)
A:dask.utils.(toplevel, _, _)->cls.__module__.partition('.')
A:dask.utils.register->object.__new__(cls)._lazy.pop(toplevel)
A:dask.utils.meth->object.__new__(cls).dispatch(type(arg))
A:dask.utils.stripped->line.strip()
A:dask.utils.lines->extra_titles(doc).split('\n')
A:dask.utils.seen->set()
A:dask.utils.lines[i]->lines[i].replace(title, new_title).replace(title, new_title)
A:dask.utils.lines[i + 1]->lines[i + 1].replace('-' * len(title), '-' * len(new_title)).replace('-' * len(title), '-' * len(new_title))
A:dask.utils.original_method->getattr(original_klass, method_name)
A:dask.utils.args->''.join(['        * {0}\n'.format(a) for a in not_supported])
A:dask.utils.doc->extra_titles(doc)
A:dask.utils.msg->"Base package doesn't support '{0}'.".format(method_name)
A:dask.utils.L->list(tup)
A:dask.utils.(deps, _)->get_deps(dsk)
A:dask.utils.columns->tuple((str(i) for i in columns))
A:dask.utils.widths->tuple((max(max(map(len, x)), len(c)) for (x, c) in zip(zip(*rows), columns)))
A:dask.utils.data->'\n'.join((row_template % r for r in rows))
A:dask.utils.func->property(lambda self: self.method)
A:dask.utils.self->object.__new__(cls)
A:dask.utils.__getattr__->staticmethod(methodcaller)
A:dask.utils.M->MethodCache()
A:dask.utils._locks->WeakValueDictionary()
A:dask.utils.self.lock->Lock()
A:dask.utils.actual_get->effective_get(get, collection)
A:dask.utils.parsed_path->urlsplit(urlpath)
A:dask.utils.windows_path->re.match('^/([a-zA-Z])[:|]([\\\\/].*)$', path)
A:dask.utils.function->kwargs.pop('function')
A:dask.utils.other->kwargs.pop('other')
A:dask.utils.args2->list(args)
dask.utils.Dispatch(self,name=None)
dask.utils.Dispatch.__init__(self,name=None)
dask.utils.Dispatch.dispatch(self,cls)
dask.utils.Dispatch.register(self,type,func=None)
dask.utils.Dispatch.register_lazy(self,toplevel,func=None)
dask.utils.IndexCallable(self,fn)
dask.utils.IndexCallable.__getitem__(self,key)
dask.utils.IndexCallable.__init__(self,fn)
dask.utils.MethodCache(object)
dask.utils.SerializableLock(self,token=None)
dask.utils.SerializableLock.__enter__(self)
dask.utils.SerializableLock.__exit__(self,*args)
dask.utils.SerializableLock.__getstate__(self)
dask.utils.SerializableLock.__init__(self,token=None)
dask.utils.SerializableLock.__setstate__(self,token)
dask.utils.SerializableLock.__str__(self)
dask.utils.SerializableLock.acquire(self,*args)
dask.utils.SerializableLock.locked(self)
dask.utils.SerializableLock.release(self,*args)
dask.utils._skip_doctest(line)
dask.utils.asciitable(columns,rows)
dask.utils.changed_cwd(new_cwd)
dask.utils.concrete(seq)
dask.utils.deepmap(func,*seqs)
dask.utils.dependency_depth(dsk)
dask.utils.derived_from(original_klass,version=None,ua_args=[])
dask.utils.digit(n,k,base)
dask.utils.effective_get(get=None,collection=None)
dask.utils.ensure_bytes(s)
dask.utils.ensure_dict(d)
dask.utils.ensure_not_exists(filename)
dask.utils.ensure_unicode(s)
dask.utils.extra_titles(doc)
dask.utils.filetext(text,extension='',open=open,mode='w')
dask.utils.filetexts(d,open=open,mode='t',use_tmpdir=True)
dask.utils.funcname(func)
dask.utils.get_scheduler_lock(get=None,collection=None)
dask.utils.homogeneous_deepmap(func,seq)
dask.utils.ignoring(*exceptions)
dask.utils.import_required(mod_name,error_msg)
dask.utils.infer_storage_options(urlpath,inherit_storage_options=None)
dask.utils.insert(tup,loc,val)
dask.utils.is_integer(i)
dask.utils.itemgetter(self,index)
dask.utils.itemgetter.__init__(self,index)
dask.utils.itemgetter.__reduce__(self)
dask.utils.memory_repr(num)
dask.utils.methodcaller(cls,method)
dask.utils.methodcaller.__new__(cls,method)
dask.utils.methodcaller.__reduce__(self)
dask.utils.methodcaller.__str__(self)
dask.utils.ndeepmap(n,func,seq)
dask.utils.noop_context()
dask.utils.partial_by_order(*args,**kwargs)
dask.utils.pseudorandom(n,p,random_state=None)
dask.utils.put_lines(buf,lines)
dask.utils.random_state_data(n,random_state=None)
dask.utils.skip_doctest(doc)
dask.utils.takes_multiple_arguments(func,varargs=True)
dask.utils.tmp_cwd(dir=None)
dask.utils.tmpdir(dir=None)
dask.utils.tmpfile(extension='',dir=None)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/sizeof.py----------------------------------------
A:dask.sizeof.sizeof->Dispatch(name='sizeof')
A:dask.sizeof.p->int(i.memory_usage())
A:dask.sizeof.obj->int((df.dtypes == object).sum() * len(df) * 100)
dask.sizeof.register_numpy()
dask.sizeof.register_pandas()
dask.sizeof.register_spmatrix()
dask.sizeof.sizeof_default(o)
dask.sizeof.sizeof_python_collection(seq)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/multiprocessing.py----------------------------------------
A:dask.multiprocessing.exceptions->dict()
A:dask.multiprocessing.typ->type(exc.__class__.__name__, (RemoteException, type(exc)), {'exception_type': type(exc)})
A:dask.multiprocessing.exc->remote_exception(exc, tb)
A:dask.multiprocessing.(exc_type, exc_value, exc_traceback)->sys.exc_info()
A:dask.multiprocessing.tb->_pack_traceback(exc_traceback)
A:dask.multiprocessing.result->get_async(pool.apply_async, len(pool._pool), dsk3, keys, get_id=_process_get_id, dumps=dumps, loads=loads, pack_exception=pack_exception, raise_exception=reraise, **kwargs)
A:dask.multiprocessing.pool->multiprocessing.Pool(num_workers, initializer=initialize_worker_process)
A:dask.multiprocessing.(dsk2, dependencies)->cull(dsk, keys)
A:dask.multiprocessing.(dsk3, dependencies)->fuse(dsk2, keys, dependencies)
A:dask.multiprocessing.np->sys.modules.get('numpy')
dask.multiprocessing.RemoteException(self,exception,traceback)
dask.multiprocessing.RemoteException.__dir__(self)
dask.multiprocessing.RemoteException.__getattr__(self,key)
dask.multiprocessing.RemoteException.__init__(self,exception,traceback)
dask.multiprocessing.RemoteException.__str__(self)
dask.multiprocessing._dumps(x)
dask.multiprocessing._process_get_id()
dask.multiprocessing._reduce_method_descriptor(m)
dask.multiprocessing.get(dsk,keys,num_workers=None,func_loads=None,func_dumps=None,optimize_graph=True,**kwargs)
dask.multiprocessing.initialize_worker_process()
dask.multiprocessing.pack_exception(e,dumps)
dask.multiprocessing.remote_exception(exc,tb)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/utils_test.py----------------------------------------
A:dask.utils_test.result->self.get(d, [['badkey'], 'y'])
A:dask.utils_test.d->dict((('x%d' % (i + 1), (inc, 'x%d' % i)) for i in range(10000)))
A:dask.utils_test.dsk->ShareDict()
dask.utils_test.GetFunctionTestMixin(object)
dask.utils_test.GetFunctionTestMixin.test_badkey(self)
dask.utils_test.GetFunctionTestMixin.test_data_not_in_dict_is_ok(self)
dask.utils_test.GetFunctionTestMixin.test_get(self)
dask.utils_test.GetFunctionTestMixin.test_get_stack_limit(self)
dask.utils_test.GetFunctionTestMixin.test_get_with_list(self)
dask.utils_test.GetFunctionTestMixin.test_get_with_list_top_level(self)
dask.utils_test.GetFunctionTestMixin.test_get_with_nested_list(self)
dask.utils_test.GetFunctionTestMixin.test_get_works_with_unhashables_in_values(self)
dask.utils_test.GetFunctionTestMixin.test_nested_badkey(self)
dask.utils_test.GetFunctionTestMixin.test_nested_tasks(self)
dask.utils_test.GetFunctionTestMixin.test_with_sharedict(self)
dask.utils_test.add(x,y)
dask.utils_test.dec(x)
dask.utils_test.inc(x)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/_version.py----------------------------------------
dask._version.get_versions()


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/core.py----------------------------------------
A:dask.core.val->func(*results)
A:dask.core.cycle->'->'.join(cycle)
A:dask.core.key->args.pop()
A:dask.core.typ->type(w)
A:dask.core.dependents->reverse_dict(dependencies)
A:dask.core.type_task->type(task)
A:dask.core.type_arg->type(arg)
A:dask.core.arg->subs(arg, key, val)
A:dask.core.completed->set()
A:dask.core.seen->set()
A:dask.core.dependencies->dict(((k, get_dependencies(dsk, k)) for k in dsk))
dask.core._get_nonrecursive(d,x,maxdepth=1000)
dask.core._get_recursive(d,x)
dask.core._toposort(dsk,keys=None,returncycle=False,dependencies=None)
dask.core.flatten(seq,container=list)
dask.core.get(d,x,recursive=False)
dask.core.get_dependencies(dsk,key=None,task=None,as_list=False)
dask.core.get_deps(dsk)
dask.core.getcycle(d,keys)
dask.core.has_tasks(dsk,x)
dask.core.isdag(d,keys)
dask.core.ishashable(x)
dask.core.istask(x)
dask.core.literal(self,data)
dask.core.literal.__init__(self,data)
dask.core.literal.__reduce__(self)
dask.core.literal.__repr__(self)
dask.core.preorder_traversal(task)
dask.core.quote(x)
dask.core.reverse_dict(d)
dask.core.subs(task,key,val)
dask.core.toposort(dsk,dependencies=None)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/hashing.py----------------------------------------
A:dask.hashing.h->hash_buffer(buf, hasher)
A:dask.hashing.s->binascii.b2a_hex(h)
dask.hashing._hash_sha1(buf)
dask.hashing.hash_buffer(buf,hasher=None)
dask.hashing.hash_buffer_hex(buf,hasher=None)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/sharedict.py----------------------------------------
A:dask.sharedict.self.dicts->dict()
A:dask.sharedict.key->id(arg)
A:dask.sharedict.seen->set()
A:dask.sharedict.result->ShareDict()
dask.sharedict.ShareDict(self)
dask.sharedict.ShareDict.__getitem__(self,key)
dask.sharedict.ShareDict.__init__(self)
dask.sharedict.ShareDict.__iter__(self)
dask.sharedict.ShareDict.__len__(self)
dask.sharedict.ShareDict.items(self)
dask.sharedict.ShareDict.update(self,arg)
dask.sharedict.ShareDict.update_with_key(self,arg,key=None)
dask.sharedict.merge(*dicts)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/base.py----------------------------------------
A:dask.base.(result,)->compute(self, traverse=False, **kwargs)
A:dask.base.dsk2->optimization_function(cls)(ensure_dict(dsk), keys, **kwargs)
A:dask.base.meth->'__{0}__'.format(name)
A:dask.base.rmeth->'__r{0}__'.format(name)
A:dask.base.traverse->kwargs.pop('traverse', True)
A:dask.base.args->tuple((delayed(a) if isinstance(a, (list, set, tuple, dict, Iterator)) else a for a in args))
A:dask.base.optimize_graph->kwargs.pop('optimize_graph', True)
A:dask.base.dsk->collections_to_dsk(collections, optimize_graph, **kwargs)
A:dask.base.results->get(dsk, keys, **kwargs)
A:dask.base.results_iter->iter(result)
A:dask.base.filename->kwargs.pop('filename', 'mydask')
A:dask.base.function_cache_lock->threading.Lock()
A:dask.base.result->pickle.dumps(func, protocol=0)
A:dask.base.first->getattr(func, 'first', None)
A:dask.base.normalize_token->Dispatch()
A:dask.base.data->hash_buffer_hex(x.copy().ravel(order='K').view('i1'))
A:dask.base.cls->type(obj)
A:dask.base.groups->groupby(optimization_function, collections)
A:dask.base.cc->copy.copy(c)
A:dask.base.client->default_client()
A:dask.base.collections->tuple(collections)
A:dask.base.keys->list(flatten([var._keys() for var in collections]))
A:dask.base.d->dict(zip(keys, results))
dask.base.Base(object)
dask.base.Base._bind_operator(cls,op)
dask.base.Base._get(cls,dsk,keys,get=None,**kwargs)
dask.base.Base._get_binary_operator(cls,op,inv=False)
dask.base.Base._get_unary_operator(cls,op)
dask.base.Base.compute(self,**kwargs)
dask.base.Base.persist(self,**kwargs)
dask.base.Base.visualize(self,filename='mydask',format=None,optimize_graph=False,**kwargs)
dask.base._extract_graph_and_keys(vals)
dask.base._normalize_function(func)
dask.base.collections_to_dsk(collections,optimize_graph=True,**kwargs)
dask.base.compute(*args,**kwargs)
dask.base.dont_optimize(dsk,keys)
dask.base.normalize_base(b)
dask.base.normalize_dict(d)
dask.base.normalize_function(func)
dask.base.normalize_object(o)
dask.base.normalize_ordered_dict(d)
dask.base.normalize_seq(seq)
dask.base.normalize_set(s)
dask.base.optimization_function(obj)
dask.base.persist(*args,**kwargs)
dask.base.redict_collection(c,dsk)
dask.base.register_numpy()
dask.base.register_pandas()
dask.base.tokenize(*args,**kwargs)
dask.base.visualize(*args,**kwargs)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/distributed.py----------------------------------------


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/optimize.py----------------------------------------
A:dask.optimize.seen->set()
A:dask.optimize.dependencies->dict()
A:dask.optimize.work->list(set(flatten(keys)))
A:dask.optimize.typ->type(first_key)
A:dask.optimize.keys->set(flatten(keys))
A:dask.optimize.unfusible->set()
A:dask.optimize.parent2child->dict(map(reversed, child2parent.items()))
A:dask.optimize.(child, parent)->child2parent.popitem()
A:dask.optimize.parent->reducible_pop()
A:dask.optimize.child->chain.pop()
A:dask.optimize.fused->set()
A:dask.optimize.aliases->set()
A:dask.optimize.new_key->key_renamer(chain)
A:dask.optimize.val->subs(val, cur_child, child_info[1])
A:dask.optimize.rv[key]->subs(rv[key], old_key, new_key)
A:dask.optimize.replaceorder->toposort(dict(((k, dsk[k]) for k in keys if k in dsk)), dependencies=dependencies)
A:dask.optimize.dsk2->dict()
A:dask.optimize.output->set(output)
A:dask.optimize.fast_functions->set(fast_functions)
A:dask.optimize.dependents->reverse_dict(dependencies)
A:dask.optimize.dsk->inline(dsk, keys, inline_constants=inline_constants, dependencies=dependencies)
A:dask.optimize.funcs->set()
A:dask.optimize.dsk2[k]->merge(v, dsk[v[1]])
A:dask.optimize.it->reversed(keys)
A:dask.optimize.first_key->next(it)
A:dask.optimize.first_name->key_split(first_key)
A:dask.optimize.names->sorted(names)
A:dask.optimize.max_height->len(dsk)
A:dask.optimize.rename_keys->context._globals.get('fuse_rename_keys', True)
A:dask.optimize.deps->dict(dependencies)
A:dask.optimize.deps[k]->set(vals)
A:dask.optimize.rv->inline(dsk, keys, inline_constants=inline_constants, dependencies=dependencies).copy()
A:dask.optimize.num_children->len(children)
A:dask.optimize.(child_key, child_task, child_keys, height, width, num_nodes, fudge, children_edges)->info_stack_pop()
A:dask.optimize.num_children_edges->len(children_edges)
A:dask.optimize.fudge->int(ave_width - 1)
A:dask.optimize.children_edges->set()
A:dask.optimize.max_num_edges->len(cur_edges)
A:dask.optimize.children_deps->set()
A:dask.optimize.alias->key_renamer(fused_keys)
A:dask.optimize.hex_pattern->re.compile('[a-f]+')
A:dask.optimize.s->s.decode().decode()
A:dask.optimize.words->s.decode().decode().split('-')
A:dask.optimize.result->words[0].lstrip('\'("')
dask.optimize._flat_set(x)
dask.optimize.cull(dsk,keys)
dask.optimize.default_fused_keys_renamer(keys)
dask.optimize.default_fused_linear_keys_renamer(keys)
dask.optimize.dont_optimize(dsk,keys,**kwargs)
dask.optimize.functions_of(task)
dask.optimize.fuse(dsk,keys=None,dependencies=None,ave_width=None,max_width=None,max_height=None,max_depth_new_edges=None,rename_keys=None)
dask.optimize.fuse_getitem(dsk,func,place)
dask.optimize.fuse_linear(dsk,keys=None,dependencies=None,rename_keys=True)
dask.optimize.fuse_selections(dsk,head1,head2,merge)
dask.optimize.inline(dsk,keys=None,inline_constants=True,dependencies=None)
dask.optimize.inline_functions(dsk,output,fast_functions=None,inline_constants=False,dependencies=None)
dask.optimize.key_split(s)
dask.optimize.unwrap_partial(func)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/delayed.py----------------------------------------
A:dask.delayed.out->list(zip(*ls))
A:dask.delayed.keys->list(expr)._keys()
A:dask.delayed.dsk->sharedict.ShareDict()
A:dask.delayed.expr->list(expr)
A:dask.delayed.typ->type(expr)
A:dask.delayed.(args, dasks)->unzip((to_task_dask(e) for e in expr), 2)
A:dask.delayed.args->tuple(pluck(0, args_dasks))
A:dask.delayed.(args, dsk)->to_task_dask([expr.start, expr.stop, expr.step])
A:dask.delayed.pure->kwargs.pop('pure', pure)
A:dask.delayed.(task, dsk)->to_task_dask(obj)
A:dask.delayed.task->quote(obj)
A:dask.delayed.token->tokenize(obj, nout, pure=pure)
A:dask.delayed.optimize->defer_to_globals('delayed_optimize', falsey=dont_optimize)(dont_optimize)
A:dask.delayed._finalize->staticmethod(first)
A:dask.delayed._default_get->staticmethod(threaded.get)
A:dask.delayed._optimize->staticmethod(optimize)
A:dask.delayed.name->kwargs.pop('dask_key_name', None)
A:dask.delayed.func->delayed(apply, pure=pure)
A:dask.delayed.method->delayed(right(op) if inv else op, pure=True)
A:dask.delayed.dask_key_name->kwargs.pop('dask_key_name', None)
A:dask.delayed.args_dasks->list(map(to_task_dask, args))
A:dask.delayed.(dask_kwargs, dsk2)->to_task_dask(kwargs)
dask.delayed.Delayed(self,key,dsk,length=None)
dask.delayed.Delayed.__bool__(self)
dask.delayed.Delayed.__dir__(self)
dask.delayed.Delayed.__getattr__(self,attr)
dask.delayed.Delayed.__getstate__(self)
dask.delayed.Delayed.__hash__(self)
dask.delayed.Delayed.__init__(self,key,dsk,length=None)
dask.delayed.Delayed.__iter__(self)
dask.delayed.Delayed.__len__(self)
dask.delayed.Delayed.__repr__(self)
dask.delayed.Delayed.__setattr__(self,attr,val)
dask.delayed.Delayed.__setitem__(self,index,val)
dask.delayed.Delayed.__setstate__(self,state)
dask.delayed.Delayed._get_binary_operator(cls,op,inv=False)
dask.delayed.Delayed._keys(self)
dask.delayed.Delayed.key(self)
dask.delayed.DelayedAttr(self,obj,attr)
dask.delayed.DelayedAttr.__init__(self,obj,attr)
dask.delayed.DelayedAttr.dask(self)
dask.delayed.DelayedLeaf(self,obj,key,pure=None,nout=None)
dask.delayed.DelayedLeaf.__init__(self,obj,key,pure=None,nout=None)
dask.delayed.DelayedLeaf.dask(self)
dask.delayed.call_function(func,func_token,args,kwargs,pure=None,nout=None)
dask.delayed.delayed(obj,name=None,pure=None,nout=None,traverse=True)
dask.delayed.right(method)
dask.delayed.to_task_dask(expr)
dask.delayed.tokenize(*args,**kwargs)
dask.delayed.unzip(ls,nout)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/rewrite.py----------------------------------------
A:dask.rewrite.self._stack->deque([END])
A:dask.rewrite.subterms->args(self.term)
A:dask.rewrite.self.term->self._stack.pop()
A:dask.rewrite.VAR->Token('?')
A:dask.rewrite.END->Token('end')
A:dask.rewrite.self.vars->tuple(sorted(set(self._varlist)))
A:dask.rewrite.term->rule.subs(sd)
A:dask.rewrite.self._net->Node()
A:dask.rewrite.ind->len(self.rules)
A:dask.rewrite.curr_node.edges[t]->Node()
A:dask.rewrite.S->Traverser(term)
A:dask.rewrite.subs->_process_match(rule, syms)
A:dask.rewrite.stack->deque()
A:dask.rewrite.n->N.edges.get(VAR, None)
A:dask.rewrite.(S, N, matches)->deque().pop()
dask.rewrite.Node(cls,edges=None,patterns=None)
dask.rewrite.Node.__new__(cls,edges=None,patterns=None)
dask.rewrite.Node.edges(self)
dask.rewrite.Node.patterns(self)
dask.rewrite.RewriteRule(self,lhs,rhs,vars=())
dask.rewrite.RewriteRule.__init__(self,lhs,rhs,vars=())
dask.rewrite.RewriteRule.__repr__(self)
dask.rewrite.RewriteRule.__str__(self)
dask.rewrite.RewriteRule._apply(self,sub_dict)
dask.rewrite.RuleSet(self,*rules)
dask.rewrite.RuleSet.__init__(self,*rules)
dask.rewrite.RuleSet._rewrite(self,term)
dask.rewrite.RuleSet.add(self,rule)
dask.rewrite.RuleSet.iter_matches(self,term)
dask.rewrite.RuleSet.rewrite(self,task,strategy='bottom_up')
dask.rewrite.Token(self,name)
dask.rewrite.Token.__init__(self,name)
dask.rewrite.Token.__repr__(self)
dask.rewrite.Traverser(self,term,stack=None)
dask.rewrite.Traverser.__init__(self,term,stack=None)
dask.rewrite.Traverser.__iter__(self)
dask.rewrite.Traverser.copy(self)
dask.rewrite.Traverser.current(self)
dask.rewrite.Traverser.next(self)
dask.rewrite.Traverser.skip(self)
dask.rewrite._bottom_up(net,term)
dask.rewrite._match(S,N)
dask.rewrite._process_match(rule,syms)
dask.rewrite._top_level(net,term)
dask.rewrite.args(task)
dask.rewrite.head(task)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/callbacks.py----------------------------------------
A:dask.callbacks.self._cm->add_callbacks(self)
A:dask.callbacks.self.old->_globals['callbacks'].copy()
dask.callbacks.Callback(self,start=None,start_state=None,pretask=None,posttask=None,finish=None)
dask.callbacks.Callback.__enter__(self)
dask.callbacks.Callback.__exit__(self,*args)
dask.callbacks.Callback.__init__(self,start=None,start_state=None,pretask=None,posttask=None,finish=None)
dask.callbacks.Callback._callback(self)
dask.callbacks.Callback.register(self)
dask.callbacks.Callback.unregister(self)
dask.callbacks.add_callbacks(self,*args)
dask.callbacks.add_callbacks.__enter__(self)
dask.callbacks.add_callbacks.__exit__(self,type,value,traceback)
dask.callbacks.add_callbacks.__init__(self,*args)
dask.callbacks.local_callbacks(callbacks=None)
dask.callbacks.normalize_callback(cb)
dask.callbacks.unpack_callbacks(cbs)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/context.py----------------------------------------
A:dask.context._globals->defaultdict(lambda : None)
A:dask.context._globals['callbacks']->set()
A:dask.context.self.old->defaultdict(lambda : None).copy()
dask.context.defer_to_globals(name,falsey=None)
dask.context.set_options(self,**kwargs)
dask.context.set_options.__enter__(self)
dask.context.set_options.__exit__(self,type,value,traceback)
dask.context.set_options.__init__(self,**kwargs)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/async.py----------------------------------------
dask.async.get_async(*args,**kwargs)
dask.async.get_sync(*args,**kwargs)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/dot.py----------------------------------------
A:dask.dot.graphviz->import_required('graphviz', 'Drawing dask graphs requires the `graphviz` python library and the `graphviz` system library to be installed.')
A:dask.dot.head->funcname(func)
A:dask.dot._HASHPAT->re.compile('([0-9a-z]{32})')
A:dask.dot._UUIDPAT->re.compile('([0-9a-z]{8}-[0-9a-z]{4}-[0-9a-z]{4}-[0-9a-z]{4}-[0-9a-z]{12})')
A:dask.dot.s->s.replace(h, label).replace(h, label)
A:dask.dot.m->re.search(pattern, s)
A:dask.dot.n->cache.get(h, len(cache))
A:dask.dot.label->'#{0}'.format(n)
A:dask.dot.g->to_graphviz(dsk, **kwargs)
A:dask.dot.seen->set()
A:dask.dot.k_name->name(k)
A:dask.dot.func_name->name((k, 'function'))
A:dask.dot.dep_name->name(dep)
A:dask.dot.IPYTHON_IMAGE_FORMATS->frozenset(['jpeg', 'png'])
A:dask.dot.IPYTHON_NO_DISPLAY_FORMATS->frozenset(['dot', 'pdf'])
A:dask.dot.(filename, format)->os.path.splitext(filename)
A:dask.dot.format->format[1:].lower()
A:dask.dot.data->to_graphviz(dsk, **kwargs).pipe(format=format)
A:dask.dot.display_cls->_get_display_cls(format)
A:dask.dot.full_filename->'.'.join([filename, format])
dask.dot._get_display_cls(format)
dask.dot.dot_graph(dsk,filename='mydask',format=None,**kwargs)
dask.dot.has_sub_tasks(task)
dask.dot.label(x,cache=None)
dask.dot.name(x)
dask.dot.task_label(task)
dask.dot.to_graphviz(dsk,data_attributes=None,function_attributes=None,rankdir='BT',graph_attr={},node_attr=None,edge_attr=None,**kwargs)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/bag/text.py----------------------------------------
A:dask.bag.text.delayed->delayed(pure=True)
A:dask.bag.text.blocks->list(concat(blocks))
A:dask.bag.text.files->open_text_files(urlpath, encoding=encoding, errors=errors, compression=compression, **storage_options or {})
A:dask.bag.text.(_, blocks)->read_bytes(urlpath, delimiter=linedelimiter.encode(), blocksize=blocksize, sample=False, compression=compression, **storage_options or {})
A:dask.bag.text.text->block.decode(encoding, errors)
A:dask.bag.text.lines->io.StringIO(text)
dask.bag.read_text(urlpath,blocksize=None,compression='infer',encoding=system_encoding,errors='strict',linedelimiter=os.linesep,collection=True,storage_options=None)
dask.bag.text.decode(block,encoding,errors)
dask.bag.text.file_to_blocks(lazy_file)
dask.bag.text.read_text(urlpath,blocksize=None,compression='infer',encoding=system_encoding,errors='strict',linedelimiter=os.linesep,collection=True,storage_options=None)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/bag/core.py----------------------------------------
A:dask.bag.core.no_result->type('no_result', (object,), {'__slots__': (), '__reduce__': lambda self: 'no_result'})
A:dask.bag.core.dependents->reverse_dict(dependencies)
A:dask.bag.core.dsk->merge(b2.dask, start, end, *groups + splits + joins)
A:dask.bag.core.(dsk2, dependencies)->cull(dsk, keys)
A:dask.bag.core.(dsk3, dependencies)->fuse(dsk2, keys + (fuse_keys or []), dependencies, rename_keys=rename_fused_keys)
A:dask.bag.core.dsk4->dict((((name, i), (collect, grouper, i, p, barrier_token)) for i in range(npartitions)))
A:dask.bag.core.dsk5->lazify(dsk4)
A:dask.bag.core.(writes, names)->write_bytes(b.to_delayed(), path, name_function, compression, encoding=encoding, **storage_options or {})
A:dask.bag.core.dsk2->dict((((name, i), (partition, grouper, (b.name, i), npartitions, p, blocksize)) for i in range(b.npartitions)))
A:dask.bag.core.results->list(results)
A:dask.bag.core.func->partial(topk, k)
A:dask.bag.core._optimize->staticmethod(optimize)
A:dask.bag.core._default_get->staticmethod(mpget)
A:dask.bag.core._finalize->staticmethod(finalize)
A:dask.bag.core.value->delayed(value)
A:dask.bag.core.name->'groupby-part-{0}-{1}'.format(funcname(grouper), token)
A:dask.bag.core.str->property(fget=StringAccessor)
A:dask.bag.core.(kw_dsk, kwargs)->unpack_scalar_dask_kwargs(kwargs)
A:dask.bag.core.random_state->Random(random_state)
A:dask.bag.core.state_data->random_state_data_python(self.npartitions, random_state)
A:dask.bag.core.key->partial(apply, key)
A:dask.bag.core.token->tokenize(b, grouper, npartitions, blocksize)
A:dask.bag.core.k->int(math.ceil(n ** (1 / stages)))
A:dask.bag.core.dsk[fmt]->merge(b2.dask, start, end, *groups + splits + joins).pop((fmt, 0))
A:dask.bag.core.(totals, counts)->list(zip(*x))
A:dask.bag.core.(squares, totals, counts)->list(zip(*x))
A:dask.bag.core.b->Bag(merge(self.dask, dsk), name, 1)
A:dask.bag.core.get->context._globals.get('get')
A:dask.bag.core.meta->dask.dataframe.utils.make_meta(meta)
A:dask.bag.core.cols->list(meta.columns)
A:dask.bag.core.dtypes->dask.dataframe.utils.make_meta(meta).dtypes.to_dict()
A:dask.bag.core.binop_name->funcname(binop)
A:dask.bag.core.res->pandas.DataFrame(seq, columns=list(columns))
A:dask.bag.core.d->dict((((name, i), list(part)) for (i, part) in enumerate(parts)))
A:dask.bag.core.d2->defaultdict(list)
A:dask.bag.core.seq->list(seq)
A:dask.bag.core.partition_size->int(len(seq) / 100)
A:dask.bag.core.parts->list(partition_all(partition_size, seq))
A:dask.bag.core.counter->itertools.count(0)
A:dask.bag.core.seqs->list(seqs)
A:dask.bag.core.out->defaultdict(int)
A:dask.bag.core.ijs->list(enumerate(take(npartitions, range(0, n, size))))
A:dask.bag.core.bags_dsk->merge(*(bag.dask for bag in bags))
A:dask.bag.core.f->partial(f, **kwargs)
A:dask.bag.core.iters->list(args)
A:dask.bag.core.keys->list(bag_kwargs)
A:dask.bag.core.(kw_dsk, other_kwargs)->unpack_scalar_dask_kwargs(other_kwargs)
A:dask.bag.core.npartitions->npartitions.pop().pop()
A:dask.bag.core.return_type->set(map(type, bags))
A:dask.bag.core.stages->int(math.ceil(math.log(n) / math.log(max_branch)))
A:dask.bag.core.b2->Bag(merge(self.dask, dsk), name, 1).map(lambda x: (hash(grouper(x)), x))
A:dask.bag.core.start->dict(((('shuffle-join-' + token, 0, inp), (b2.name, i) if i < b.npartitions else []) for (i, inp) in enumerate(inputs)))
A:dask.bag.core.group->dict(((('shuffle-group-' + token, stage, inp), (groupby, (make_group, k, stage - 1), ('shuffle-join-' + token, stage - 1, inp))) for inp in inputs))
A:dask.bag.core.split->dict(((('shuffle-split-' + token, stage, i, inp), (dict.get, ('shuffle-group-' + token, stage, inp), i, {})) for i in range(k) for inp in inputs))
A:dask.bag.core.join->dict(((('shuffle-join-' + token, stage, inp), (list, (toolz.concat, [('shuffle-split-' + token, stage, inp[stage - 1], insert(inp, stage - 1, j)) for j in range(k)]))) for inp in inputs))
A:dask.bag.core.end->dict(((('shuffle-' + token, i), (list, (dict.items, (groupby, grouper, (pluck, 1, j))))) for (i, j) in enumerate(join)))
A:dask.bag.core.dirname->context._globals.get('temporary_directory', None)
A:dask.bag.core.(_, part)->peek(part)
A:dask.bag.core.r->list(take(n, b))
dask.bag.Bag(self,dsk,name,npartitions)
dask.bag.Bag.__getstate__(self)
dask.bag.Bag.__iter__(self)
dask.bag.Bag.__setstate__(self,state)
dask.bag.Bag.__str__(self)
dask.bag.Bag._args(self)
dask.bag.Bag._keys(self)
dask.bag.Bag.accumulate(self,binop,initial=no_default)
dask.bag.Bag.all(self,split_every=None)
dask.bag.Bag.any(self,split_every=None)
dask.bag.Bag.count(self,split_every=None)
dask.bag.Bag.distinct(self)
dask.bag.Bag.filter(self,predicate)
dask.bag.Bag.flatten(self)
dask.bag.Bag.fold(self,binop,combine=None,initial=no_default,split_every=None)
dask.bag.Bag.foldby(self,key,binop,initial=no_default,combine=None,combine_initial=no_default)
dask.bag.Bag.frequencies(self,split_every=None)
dask.bag.Bag.groupby(self,grouper,method=None,npartitions=None,blocksize=2**20,max_branch=None)
dask.bag.Bag.join(self,other,on_self,on_other=None)
dask.bag.Bag.map(self,func,*args,**kwargs)
dask.bag.Bag.map_partitions(self,func,*args,**kwargs)
dask.bag.Bag.max(self,split_every=None)
dask.bag.Bag.mean(self)
dask.bag.Bag.min(self,split_every=None)
dask.bag.Bag.pluck(self,key,default=no_default)
dask.bag.Bag.product(self,other)
dask.bag.Bag.random_sample(self,prob,random_state=None)
dask.bag.Bag.reduction(self,perpartition,aggregate,split_every=None,out_type=Item,name=None)
dask.bag.Bag.remove(self,predicate)
dask.bag.Bag.repartition(self,npartitions)
dask.bag.Bag.starmap(self,func,**kwargs)
dask.bag.Bag.std(self,ddof=0)
dask.bag.Bag.sum(self,split_every=None)
dask.bag.Bag.take(self,k,npartitions=1,compute=True)
dask.bag.Bag.to_dataframe(self,meta=None,columns=None)
dask.bag.Bag.to_delayed(self)
dask.bag.Bag.to_textfiles(self,path,name_function=None,compression='infer',encoding=system_encoding,compute=True,get=None,storage_options=None)
dask.bag.Bag.topk(self,k,key=None,split_every=None)
dask.bag.Bag.unzip(self,n)
dask.bag.Bag.var(self,ddof=0)
dask.bag.Item(self,dsk,key)
dask.bag.Item.__getstate__(self)
dask.bag.Item.__setstate__(self,state)
dask.bag.Item._args(self)
dask.bag.Item._keys(self)
dask.bag.Item.apply(self,func)
dask.bag.Item.from_delayed(value)
dask.bag.Item.to_delayed(self)
dask.bag.concat(bags)
dask.bag.core.Bag(self,dsk,name,npartitions)
dask.bag.core.Bag.__getstate__(self)
dask.bag.core.Bag.__init__(self,dsk,name,npartitions)
dask.bag.core.Bag.__iter__(self)
dask.bag.core.Bag.__setstate__(self,state)
dask.bag.core.Bag.__str__(self)
dask.bag.core.Bag._args(self)
dask.bag.core.Bag._keys(self)
dask.bag.core.Bag.accumulate(self,binop,initial=no_default)
dask.bag.core.Bag.all(self,split_every=None)
dask.bag.core.Bag.any(self,split_every=None)
dask.bag.core.Bag.count(self,split_every=None)
dask.bag.core.Bag.distinct(self)
dask.bag.core.Bag.filter(self,predicate)
dask.bag.core.Bag.flatten(self)
dask.bag.core.Bag.fold(self,binop,combine=None,initial=no_default,split_every=None)
dask.bag.core.Bag.foldby(self,key,binop,initial=no_default,combine=None,combine_initial=no_default)
dask.bag.core.Bag.frequencies(self,split_every=None)
dask.bag.core.Bag.groupby(self,grouper,method=None,npartitions=None,blocksize=2**20,max_branch=None)
dask.bag.core.Bag.join(self,other,on_self,on_other=None)
dask.bag.core.Bag.map(self,func,*args,**kwargs)
dask.bag.core.Bag.map_partitions(self,func,*args,**kwargs)
dask.bag.core.Bag.max(self,split_every=None)
dask.bag.core.Bag.mean(self)
dask.bag.core.Bag.min(self,split_every=None)
dask.bag.core.Bag.pluck(self,key,default=no_default)
dask.bag.core.Bag.product(self,other)
dask.bag.core.Bag.random_sample(self,prob,random_state=None)
dask.bag.core.Bag.reduction(self,perpartition,aggregate,split_every=None,out_type=Item,name=None)
dask.bag.core.Bag.remove(self,predicate)
dask.bag.core.Bag.repartition(self,npartitions)
dask.bag.core.Bag.starmap(self,func,**kwargs)
dask.bag.core.Bag.std(self,ddof=0)
dask.bag.core.Bag.sum(self,split_every=None)
dask.bag.core.Bag.take(self,k,npartitions=1,compute=True)
dask.bag.core.Bag.to_dataframe(self,meta=None,columns=None)
dask.bag.core.Bag.to_delayed(self)
dask.bag.core.Bag.to_textfiles(self,path,name_function=None,compression='infer',encoding=system_encoding,compute=True,get=None,storage_options=None)
dask.bag.core.Bag.topk(self,k,key=None,split_every=None)
dask.bag.core.Bag.unzip(self,n)
dask.bag.core.Bag.var(self,ddof=0)
dask.bag.core.Item(self,dsk,key)
dask.bag.core.Item.__getstate__(self)
dask.bag.core.Item.__init__(self,dsk,key)
dask.bag.core.Item.__setstate__(self,state)
dask.bag.core.Item._args(self)
dask.bag.core.Item._keys(self)
dask.bag.core.Item.apply(self,func)
dask.bag.core.Item.from_delayed(value)
dask.bag.core.Item.to_delayed(self)
dask.bag.core.StringAccessor(self,bag)
dask.bag.core.StringAccessor.__dir__(self)
dask.bag.core.StringAccessor.__getattr__(self,key)
dask.bag.core.StringAccessor.__init__(self,bag)
dask.bag.core.StringAccessor._strmap(self,key,*args,**kwargs)
dask.bag.core.StringAccessor.match(self,pattern)
dask.bag.core._reduce(binop,sequence,initial=no_default)
dask.bag.core.accumulate_part(binop,seq,initial,is_first=False)
dask.bag.core.bag_map(func,*args,**kwargs)
dask.bag.core.bag_range(n,npartitions)
dask.bag.core.bag_zip(*bags)
dask.bag.core.collect(grouper,group,p,barrier_token)
dask.bag.core.concat(bags)
dask.bag.core.dictitems(d)
dask.bag.core.empty_safe_aggregate(func,parts,is_last)
dask.bag.core.empty_safe_apply(func,part,is_last)
dask.bag.core.finalize(results)
dask.bag.core.finalize_item(results)
dask.bag.core.from_delayed(values)
dask.bag.core.from_sequence(seq,partition_size=None,npartitions=None)
dask.bag.core.from_url(urls)
dask.bag.core.groupby_disk(b,grouper,npartitions=None,blocksize=2**20)
dask.bag.core.groupby_tasks(b,grouper,hash=hash,max_branch=32)
dask.bag.core.inline_singleton_lists(dsk,dependencies=None)
dask.bag.core.lazify(dsk)
dask.bag.core.lazify_task(task,start=True)
dask.bag.core.make_group(k,stage)
dask.bag.core.map_chunk(f,args,bag_kwargs,kwargs)
dask.bag.core.map_partitions(func,*args,**kwargs)
dask.bag.core.merge_distinct(seqs)
dask.bag.core.merge_frequencies(seqs)
dask.bag.core.optimize(dsk,keys,fuse_keys=None,rename_fused_keys=True,**kwargs)
dask.bag.core.partition(grouper,sequence,npartitions,p,nelements=2**20)
dask.bag.core.random_sample(x,state_data,prob)
dask.bag.core.random_state_data_python(n,random_state=None)
dask.bag.core.reify(seq)
dask.bag.core.robust_wraps(wrapper)
dask.bag.core.safe_take(n,b)
dask.bag.core.split(seq,n)
dask.bag.core.starmap_chunk(f,x,kwargs)
dask.bag.core.to_dataframe(seq,columns,dtypes)
dask.bag.core.to_textfiles(b,path,name_function=None,compression='infer',encoding=system_encoding,compute=True,get=None,storage_options=None)
dask.bag.core.unpack_scalar_dask_kwargs(kwargs)
dask.bag.from_delayed(values)
dask.bag.from_sequence(seq,partition_size=None,npartitions=None)
dask.bag.from_url(urls)
dask.bag.map(func,*args,**kwargs)
dask.bag.map_partitions(func,*args,**kwargs)
dask.bag.range(n,npartitions)
dask.bag.to_textfiles(b,path,name_function=None,compression='infer',encoding=system_encoding,compute=True,get=None,storage_options=None)
dask.bag.zip(*bags)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/bag/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/bag/tests/test_bag.py----------------------------------------
A:dask.bag.tests.test_bag.b->dask.bag.from_sequence(['abc', '123', 'xyz'], npartitions=2)
A:dask.bag.tests.test_bag.b2->dask.bag.from_sequence(['abc', '123', 'xyz'], npartitions=2).groupby(lambda x: x % 2)
A:dask.bag.tests.test_bag.x->dask.bag.range(10, npartitions=2)
A:dask.bag.tests.test_bag.x2->dask.bag.from_sequence(['abc', '123', 'xyz'], npartitions=2).groupby(lambda x: x % 2).compute()
A:dask.bag.tests.test_bag.x_sum->sum(x)
A:dask.bag.tests.test_bag.a->dask.bag.read_text(fn)
A:dask.bag.tests.test_bag.fewer_parts->dask.bag.from_sequence(range(100), npartitions=5)
A:dask.bag.tests.test_bag.unequal->dask.bag.from_sequence(range(110), npartitions=10)
A:dask.bag.tests.test_bag.max_second->dask.bag.from_sequence(['abc', '123', 'xyz'], npartitions=2).pluck(1).max()
A:dask.bag.tests.test_bag.c->dask.bag.from_sequence(['abc', '123', 'xyz'], npartitions=2).groupby(lambda x: x % 3)
A:dask.bag.tests.test_bag.expected->merge(dsk, dict((((c.name, i), (reify, (filter, iseven, (b.name, i)))) for i in range(b.npartitions))))
A:dask.bag.tests.test_bag.(one, two, three)->dask.bag.from_sequence(['abc', '123', 'xyz'], npartitions=2).unzip(3)
A:dask.bag.tests.test_bag.c2->dask.bag.from_sequence(['abc', '123', 'xyz'], npartitions=2).topk(4, key=lambda x: -x, split_every=2)
A:dask.bag.tests.test_bag.acc->acc.copy().copy()
A:dask.bag.tests.test_bag.d->dask.bag.from_sequence(['abc', '123', 'xyz'], npartitions=2).repartition(20)
A:dask.bag.tests.test_bag.e->dask.bag.read_text('s3://tip-data/t*.gz', storage_options=dict(anon=True))
A:dask.bag.tests.test_bag.bag->dask.bag.from_delayed([delayed_records(5) for _ in range(5)])
A:dask.bag.tests.test_bag.bag2->dask.bag.from_delayed([delayed_records(5) for _ in range(5)]).filter(None).frequencies(split_every=2)
A:dask.bag.tests.test_bag.L->list(range(1001))
A:dask.bag.tests.test_bag.np->pytest.importorskip('numpy')
A:dask.bag.tests.test_bag.data->list(range(100))
A:dask.bag.tests.test_bag.dx->dask.bag.from_sequence(x, npartitions=10)
A:dask.bag.tests.test_bag.dy->dask.bag.from_sequence(y, npartitions=10)
A:dask.bag.tests.test_bag.sol->pytest.importorskip('pandas').DataFrame({'a': range(100)})
A:dask.bag.tests.test_bag.dy_mean->dask.delayed(dy_mean)
A:dask.bag.tests.test_bag.f->myopen(os.path.join(dir, '1.' + ext), 'rb')
A:dask.bag.tests.test_bag.y->dask.bag.range(10, npartitions=2).map(inc)
A:dask.bag.tests.test_bag.z->dask.bag.range(10, npartitions=2).map(inc).map(inc)
A:dask.bag.tests.test_bag.result->dask.bag.from_sequence(['abc', '123', 'xyz'], npartitions=2).compute(get=dask.get)
A:dask.bag.tests.test_bag.result2->dict(result)
A:dask.bag.tests.test_bag.dd->pytest.importorskip('dask.dataframe')
A:dask.bag.tests.test_bag.pd->pytest.importorskip('pandas')
A:dask.bag.tests.test_bag.df->dask.bag.from_sequence(['abc', '123', 'xyz'], npartitions=2).map_partitions(f).to_dataframe(meta=sol)
A:dask.bag.tests.test_bag.text->str(dict(d.dask))
A:dask.bag.tests.test_bag.out->dask.bag.from_sequence(['abc', '123', 'xyz'], npartitions=2).groupby(lambda x: x % 2834, max_branch=24, method='tasks')
A:dask.bag.tests.test_bag.B->dask.bag.from_sequence(['abc', '123', 'xyz'], npartitions=2)
A:dask.bag.tests.test_bag.dictbag->BagOfDicts(*db.from_sequence([{'a': {'b': 'c'}}])._args)
A:dask.bag.tests.test_bag.bin_data->u'€'.encode('utf-8')
A:dask.bag.tests.test_bag.(a, b, c)->dask.bag.from_sequence(['abc', '123', 'xyz'], npartitions=2).map(inc).to_delayed()
A:dask.bag.tests.test_bag.t->dask.bag.from_sequence(['abc', '123', 'xyz'], npartitions=2).sum().to_delayed()
A:dask.bag.tests.test_bag.[d]->dask.bag.from_sequence(['abc', '123', 'xyz'], npartitions=2).groupby(lambda x: x % 2).to_textfiles('foo.txt', compute=False)
A:dask.bag.tests.test_bag.bb->from_delayed([a, b, c])
A:dask.bag.tests.test_bag.asum_value->delayed(lambda X: sum(X))(a)
A:dask.bag.tests.test_bag.asum_item->dask.bag.Item.from_delayed(asum_value)
A:dask.bag.tests.test_bag.delayed_records->delayed(lazy_records, pure=False)
A:dask.bag.tests.test_bag.evens->dask.bag.from_sequence(range(0, hi, 2), npartitions=npartitions)
A:dask.bag.tests.test_bag.odds->dask.bag.from_sequence(range(1, hi, 2), npartitions=npartitions)
A:dask.bag.tests.test_bag.pairs->dask.bag.zip(evens, odds)
A:dask.bag.tests.test_bag.results->dask.get(c.dask, c._keys())
A:dask.bag.tests.test_bag.dsk->dask.bag.range(10, npartitions=2).map(inc).map(inc)._optimize(z.dask, z._keys(), fuse_keys=y._keys())
A:dask.bag.tests.test_bag.r->dask.bag.from_sequence(['abc', '123', 'xyz'], npartitions=2).accumulate(add)
A:dask.bag.tests.test_bag.partitions->dask.get(out.dask, out._keys())
A:dask.bag.tests.test_bag.vals->dask.bag.compute(b.min(split_every=2), b.max(split_every=2), get=dask.get)
A:dask.bag.tests.test_bag.sp->pytest.importorskip('scipy.sparse')
A:dask.bag.tests.test_bag.s->dask.bag.from_sequence(['abc', '123', 'xyz'], npartitions=2).sum()
A:dask.bag.tests.test_bag.s2->loads(dumps(s))
A:dask.bag.tests.test_bag.res->dask.bag.from_sequence(['abc', '123', 'xyz'], npartitions=2).reduction(func, sum)
dask.bag.tests.test_bag.BagOfDicts(db.Bag)
dask.bag.tests.test_bag.BagOfDicts.get(self,key,default=None)
dask.bag.tests.test_bag.BagOfDicts.set(self,key,value)
dask.bag.tests.test_bag.StrictReal(int)
dask.bag.tests.test_bag.StrictReal.__eq__(self,other)
dask.bag.tests.test_bag.StrictReal.__ne__(self,other)
dask.bag.tests.test_bag.iseven(x)
dask.bag.tests.test_bag.isodd(x)
dask.bag.tests.test_bag.test_Bag()
dask.bag.tests.test_bag.test_accumulate()
dask.bag.tests.test_bag.test_aggregation(npartitions)
dask.bag.tests.test_bag.test_args()
dask.bag.tests.test_bag.test_bag_class_extend()
dask.bag.tests.test_bag.test_bag_compute_forward_kwargs()
dask.bag.tests.test_bag.test_bag_map()
dask.bag.tests.test_bag.test_bag_paths()
dask.bag.tests.test_bag.test_bag_picklable()
dask.bag.tests.test_bag.test_bag_with_single_callable()
dask.bag.tests.test_bag.test_can_use_dict_to_make_concrete()
dask.bag.tests.test_bag.test_concat()
dask.bag.tests.test_bag.test_concat_after_map()
dask.bag.tests.test_bag.test_distinct()
dask.bag.tests.test_bag.test_empty()
dask.bag.tests.test_bag.test_empty_bag()
dask.bag.tests.test_bag.test_ensure_compute_output_is_concrete()
dask.bag.tests.test_bag.test_filter()
dask.bag.tests.test_bag.test_flatten()
dask.bag.tests.test_bag.test_fold()
dask.bag.tests.test_bag.test_foldby()
dask.bag.tests.test_bag.test_frequencies()
dask.bag.tests.test_bag.test_from_delayed()
dask.bag.tests.test_bag.test_from_delayed_iterator()
dask.bag.tests.test_bag.test_from_long_sequence()
dask.bag.tests.test_bag.test_from_s3()
dask.bag.tests.test_bag.test_from_sequence()
dask.bag.tests.test_bag.test_from_url()
dask.bag.tests.test_bag.test_gh715()
dask.bag.tests.test_bag.test_groupby()
dask.bag.tests.test_bag.test_groupby_tasks()
dask.bag.tests.test_bag.test_groupby_tasks_2(size,npartitions,groups)
dask.bag.tests.test_bag.test_groupby_tasks_3()
dask.bag.tests.test_bag.test_groupby_tasks_names()
dask.bag.tests.test_bag.test_groupby_with_indexer()
dask.bag.tests.test_bag.test_groupby_with_npartitions_changed()
dask.bag.tests.test_bag.test_inline_singleton_lists()
dask.bag.tests.test_bag.test_iter()
dask.bag.tests.test_bag.test_join()
dask.bag.tests.test_bag.test_keys()
dask.bag.tests.test_bag.test_lambdas()
dask.bag.tests.test_bag.test_lazify()
dask.bag.tests.test_bag.test_lazify_task()
dask.bag.tests.test_bag.test_map_is_lazy()
dask.bag.tests.test_bag.test_map_method()
dask.bag.tests.test_bag.test_map_partitions()
dask.bag.tests.test_bag.test_map_partitions_args_kwargs()
dask.bag.tests.test_bag.test_map_with_iterator_function()
dask.bag.tests.test_bag.test_msgpack_unicode()
dask.bag.tests.test_bag.test_non_splittable_reductions(npartitions)
dask.bag.tests.test_bag.test_optimize_fuse_keys()
dask.bag.tests.test_bag.test_partition_collect()
dask.bag.tests.test_bag.test_pluck()
dask.bag.tests.test_bag.test_pluck_with_default()
dask.bag.tests.test_bag.test_product()
dask.bag.tests.test_bag.test_random_sample_different_definitions()
dask.bag.tests.test_bag.test_random_sample_prob_range()
dask.bag.tests.test_bag.test_random_sample_random_state()
dask.bag.tests.test_bag.test_random_sample_repeated_computation()
dask.bag.tests.test_bag.test_random_sample_size()
dask.bag.tests.test_bag.test_range()
dask.bag.tests.test_bag.test_read_text()
dask.bag.tests.test_bag.test_read_text_encoding()
dask.bag.tests.test_bag.test_read_text_large()
dask.bag.tests.test_bag.test_read_text_large_gzip()
dask.bag.tests.test_bag.test_reduction_empty()
dask.bag.tests.test_bag.test_reduction_empty_aggregate(npartitions)
dask.bag.tests.test_bag.test_reduction_names()
dask.bag.tests.test_bag.test_reduction_with_non_comparable_objects()
dask.bag.tests.test_bag.test_reduction_with_sparse_matrices()
dask.bag.tests.test_bag.test_reductions()
dask.bag.tests.test_bag.test_reductions_are_lazy()
dask.bag.tests.test_bag.test_remove()
dask.bag.tests.test_bag.test_repartition(nin,nout)
dask.bag.tests.test_bag.test_repartition_names()
dask.bag.tests.test_bag.test_repeated_groupby()
dask.bag.tests.test_bag.test_repr(func)
dask.bag.tests.test_bag.test_starmap()
dask.bag.tests.test_bag.test_std()
dask.bag.tests.test_bag.test_str_empty_split()
dask.bag.tests.test_bag.test_string_namespace()
dask.bag.tests.test_bag.test_string_namespace_with_unicode()
dask.bag.tests.test_bag.test_take()
dask.bag.tests.test_bag.test_take_npartitions()
dask.bag.tests.test_bag.test_take_npartitions_warn()
dask.bag.tests.test_bag.test_temporary_directory(tmpdir)
dask.bag.tests.test_bag.test_to_dataframe()
dask.bag.tests.test_bag.test_to_delayed()
dask.bag.tests.test_bag.test_to_delayed_optimizes()
dask.bag.tests.test_bag.test_to_textfiles(ext,myopen)
dask.bag.tests.test_bag.test_to_textfiles_empty_partitions()
dask.bag.tests.test_bag.test_to_textfiles_encoding()
dask.bag.tests.test_bag.test_to_textfiles_endlines()
dask.bag.tests.test_bag.test_to_textfiles_inputs()
dask.bag.tests.test_bag.test_to_textfiles_name_function_preserves_order()
dask.bag.tests.test_bag.test_to_textfiles_name_function_warn()
dask.bag.tests.test_bag.test_topk()
dask.bag.tests.test_bag.test_topk_with_multiarg_lambda()
dask.bag.tests.test_bag.test_topk_with_non_callable_key(npartitions)
dask.bag.tests.test_bag.test_tree_reductions()
dask.bag.tests.test_bag.test_unzip()
dask.bag.tests.test_bag.test_var()
dask.bag.tests.test_bag.test_zip(npartitions,hi=1000)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/bag/tests/test_text.py----------------------------------------
A:dask.bag.tests.test_text.compute->partial(compute, get=get)
A:dask.bag.tests.test_text.expected->''.join([files[v] for v in sorted(files)])
A:dask.bag.tests.test_text.files2->dict(((k, compress(v.encode(encoding))) for (k, v) in files.items()))
A:dask.bag.tests.test_text.b->read_text('.test.accounts.*.json', compression=fmt, blocksize=bs, encoding=encoding)
A:dask.bag.tests.test_text.(L,)->compute(b)
A:dask.bag.tests.test_text.blocks->read_text('.test.accounts.*.json', compression=fmt, blocksize=bs, encoding=encoding, collection=False)
A:dask.bag.tests.test_text.L->compute(*blocks)
A:dask.bag.tests.test_text.result->result.compute(get=get).compute(get=get)
dask.bag.tests.test_text.test_errors()
dask.bag.tests.test_text.test_read_text(fmt,bs,encoding)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/bag/tests/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/tests/test_base.py----------------------------------------
A:dask.tests.test_base.tz->pytest.importorskip('toolz')
A:dask.tests.test_base.da->pytest.importorskip('dask.array')
A:dask.tests.test_base.db->pytest.importorskip('dask.bag')
A:dask.tests.test_base.dd->import_or_none('dask.dataframe')
A:dask.tests.test_base.np->import_or_none('numpy')
A:dask.tests.test_base.pd->import_or_none('pandas')
A:dask.tests.test_base.x->pytest.importorskip('dask.array').ones((100, 3), chunks=10)
A:dask.tests.test_base.y->pytest.importorskip('dask.array').array(x)
A:dask.tests.test_base.z->tokenize(np.load(fn, mmap_mode='r'))
A:dask.tests.test_base.mm->import_or_none('numpy').load(fn, mmap_mode='r')
A:dask.tests.test_base.mm2->import_or_none('numpy').load(fn, mmap_mode='r')
A:dask.tests.test_base.a->import_or_none('dask.dataframe').from_pandas(pd.DataFrame({'A': range(12)}), npartitions=3)
A:dask.tests.test_base.b->import_or_none('dask.dataframe').from_pandas(pd.Series(range(12), name='B'), npartitions=4)
A:dask.tests.test_base.c->OrderedDict([('b', 2), ('a', 1)])
A:dask.tests.test_base.d->tokenize(mm[:, 0])
A:dask.tests.test_base.inc->import_or_none('numpy').frompyfunc(lambda x: x + 1, 1, 1)
A:dask.tests.test_base.a['z']->import_or_none('dask.dataframe').from_pandas(pd.DataFrame({'A': range(12)}), npartitions=3).y.astype('category')
A:dask.tests.test_base.b['z']->import_or_none('dask.dataframe').from_pandas(pd.DataFrame({'A': range(12)}), npartitions=3).y.astype('category')
A:dask.tests.test_base.add1->pytest.importorskip('toolz').partial(add, 1)
A:dask.tests.test_base.mul2->pytest.importorskip('toolz').partial(mul, 2)
A:dask.tests.test_base.o->import_or_none('dask.dataframe').from_pandas(pd.Series(range(12), name='B'), npartitions=4).map(add1).map(mul2)
A:dask.tests.test_base.arr->import_or_none('numpy').arange(100).reshape((10, 10))
A:dask.tests.test_base.darr->pytest.importorskip('dask.array').from_array(arr, chunks=(5, 5))
A:dask.tests.test_base.(out1, out2)->compute(ddf1, ddf2)
A:dask.tests.test_base.df->import_or_none('pandas').DataFrame({'a': [1, 2, 3, 4], 'b': [5, 5, 3, 3]})
A:dask.tests.test_base.ddf->import_or_none('dask.dataframe').from_pandas(df, npartitions=2)
A:dask.tests.test_base.(arr_out, df_out)->compute(darr, ddf)
A:dask.tests.test_base.(xx, bb)->dask.compute(x + 1, b.map(inc), get=dask.get)
A:dask.tests.test_base.(xx, yy)->compute(x, y)
A:dask.tests.test_base.res->compute([a, b], c, traverse=False)
A:dask.tests.test_base.defn->dedent('\n    def inc():\n        return x\n    ')
A:dask.tests.test_base.t->normalize_token(f)
A:dask.tests.test_base.out->subprocess.check_output([sys.executable, '-c', code])
A:dask.tests.test_base.modules->set(eval(out.decode()))
A:dask.tests.test_base.x1->delayed(1)
A:dask.tests.test_base.x2->delayed(inc)(x1)
A:dask.tests.test_base.x3->delayed(inc)(x2)
A:dask.tests.test_base.(xx,)->persist(x3)
dask.tests.test_base.import_or_none(path)
dask.tests.test_base.test_array_nondim()
dask.tests.test_base.test_compute_array()
dask.tests.test_base.test_compute_array_bag()
dask.tests.test_base.test_compute_array_dataframe()
dask.tests.test_base.test_compute_dataframe()
dask.tests.test_base.test_compute_nested()
dask.tests.test_base.test_compute_no_opt()
dask.tests.test_base.test_compute_with_literal()
dask.tests.test_base.test_default_imports()
dask.tests.test_base.test_normalize_base()
dask.tests.test_base.test_normalize_function()
dask.tests.test_base.test_normalize_function_limited_size()
dask.tests.test_base.test_optimizations_keyword()
dask.tests.test_base.test_optimize_None()
dask.tests.test_base.test_optimize_globals()
dask.tests.test_base.test_persist_array()
dask.tests.test_base.test_persist_array_bag()
dask.tests.test_base.test_persist_delayed()
dask.tests.test_base.test_persist_literals()
dask.tests.test_base.test_setitem_triggering_realign()
dask.tests.test_base.test_tokenize()
dask.tests.test_base.test_tokenize_base_types(x)
dask.tests.test_base.test_tokenize_dict()
dask.tests.test_base.test_tokenize_discontiguous_numpy_array()
dask.tests.test_base.test_tokenize_kwargs()
dask.tests.test_base.test_tokenize_numpy_array_consistent_on_values()
dask.tests.test_base.test_tokenize_numpy_array_on_object_dtype()
dask.tests.test_base.test_tokenize_numpy_array_supports_uneven_sizes()
dask.tests.test_base.test_tokenize_numpy_datetime()
dask.tests.test_base.test_tokenize_numpy_memmap()
dask.tests.test_base.test_tokenize_numpy_memmap_no_filename()
dask.tests.test_base.test_tokenize_numpy_scalar()
dask.tests.test_base.test_tokenize_numpy_ufunc_consistent()
dask.tests.test_base.test_tokenize_object_array_with_nans()
dask.tests.test_base.test_tokenize_ordered_dict()
dask.tests.test_base.test_tokenize_pandas()
dask.tests.test_base.test_tokenize_same_repr()
dask.tests.test_base.test_tokenize_sequences()
dask.tests.test_base.test_tokenize_set()
dask.tests.test_base.test_use_cloudpickle_to_tokenize_functions_in__main__()
dask.tests.test_base.test_visualize()


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/tests/test_optimize.py----------------------------------------
A:dask.tests.test_optimize.(culled, dependencies)->cull(d, 'out')
A:dask.tests.test_optimize.rv1->fuse_linear(*args, **kwargs)
A:dask.tests.test_optimize.rv2->fuse(*args, **kwargs)
A:dask.tests.test_optimize.result->inline_functions(dsk, [], fast_functions=set([inc]))
A:dask.tests.test_optimize.dsk2->fuse_selections(dsk, getitem, load, merge)
A:dask.tests.test_optimize.(dsk2, dependencies)->cull(dsk2, 'y')
A:dask.tests.test_optimize.(d2, dependencies)->cull(d, ['d', 'e'])
A:dask.tests.test_optimize.expected->with_deps(d)
A:dask.tests.test_optimize.rv->fuse(d, keys=keys, ave_width=2, rename_keys=True)
dask.tests.test_optimize.double(x)
dask.tests.test_optimize.fuse2(*args,**kwargs)
dask.tests.test_optimize.test_cull()
dask.tests.test_optimize.test_functions_of()
dask.tests.test_optimize.test_fuse()
dask.tests.test_optimize.test_fuse_getitem()
dask.tests.test_optimize.test_fuse_keys()
dask.tests.test_optimize.test_fuse_reductions_multiple_input()
dask.tests.test_optimize.test_fuse_reductions_single_input()
dask.tests.test_optimize.test_fuse_selections()
dask.tests.test_optimize.test_fuse_stressed()
dask.tests.test_optimize.test_inline()
dask.tests.test_optimize.test_inline_cull_dependencies()
dask.tests.test_optimize.test_inline_doesnt_shrink_fast_functions_at_top()
dask.tests.test_optimize.test_inline_functions()
dask.tests.test_optimize.test_inline_functions_protects_output_keys()
dask.tests.test_optimize.test_inline_ignores_curries_and_partials()
dask.tests.test_optimize.test_inline_traverses_lists()
dask.tests.test_optimize.with_deps(dsk)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/tests/test_threaded.py----------------------------------------
A:dask.tests.test_threaded.pool->ThreadPool()
A:dask.tests.test_threaded.result->get({'x': (lambda : i,)}, 'x', num_workers=2)
A:dask.tests.test_threaded.before->threading.active_count()
A:dask.tests.test_threaded.t->threading.Thread(target=test_f)
A:dask.tests.test_threaded.start->time()
A:dask.tests.test_threaded.after->threading.active_count()
A:dask.tests.test_threaded.main_thread->threading.get_ident()
A:dask.tests.test_threaded.interrupter->threading.Timer(0.5, interrupt_main)
A:dask.tests.test_threaded.stop->time()
dask.tests.test_threaded.bad(x)
dask.tests.test_threaded.test_dont_spawn_too_many_threads()
dask.tests.test_threaded.test_exceptions_rise_to_top()
dask.tests.test_threaded.test_get()
dask.tests.test_threaded.test_get_without_computation()
dask.tests.test_threaded.test_interrupt()
dask.tests.test_threaded.test_nested_get()
dask.tests.test_threaded.test_reuse_pool()
dask.tests.test_threaded.test_thread_safety()
dask.tests.test_threaded.test_threaded_within_thread()


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/tests/test_utils.py----------------------------------------
A:dask.tests.test_utils.foo->Dispatch()
A:dask.tests.test_utils.b->SerializableLock('b')
A:dask.tests.test_utils.state->numpy.random.RandomState(seed)
A:dask.tests.test_utils.states->random_state_data(10, 1234)
A:dask.tests.test_utils.states2->random_state_data(n, state)
A:dask.tests.test_utils.f->methodcaller('count')
A:dask.tests.test_utils.res->asciitable(['fruit', 'color'], [('apple', 'red'), ('banana', 'yellow'), ('tomato', 'red'), ('pear', 'green')])
A:dask.tests.test_utils.a->SerializableLock('a')
A:dask.tests.test_utils.a2->pickle.loads(pickle.dumps(a))
A:dask.tests.test_utils.a3->pickle.loads(pickle.dumps(a))
A:dask.tests.test_utils.a4->pickle.loads(pickle.dumps(a2))
A:dask.tests.test_utils.b2->pickle.loads(pickle.dumps(b))
A:dask.tests.test_utils.b3->pickle.loads(pickle.dumps(b2))
A:dask.tests.test_utils.c->SerializableLock('a')
A:dask.tests.test_utils.d->SerializableLock()
A:dask.tests.test_utils.toolz->pytest.importorskip('toolz')
A:dask.tests.test_utils.md->mydict()
A:dask.tests.test_utils.sd->ShareDict()
A:dask.tests.test_utils.g->itemgetter(1)
A:dask.tests.test_utils.g2->pickle.loads(pickle.dumps(g))
dask.tests.test_utils.test_SerializableLock()
dask.tests.test_utils.test_SerializableLock_name_collision()
dask.tests.test_utils.test_asciitable()
dask.tests.test_utils.test_dispatch()
dask.tests.test_utils.test_dispatch_lazy()
dask.tests.test_utils.test_ensure_dict()
dask.tests.test_utils.test_extra_titles()
dask.tests.test_utils.test_funcname()
dask.tests.test_utils.test_funcname_multipledispatch()
dask.tests.test_utils.test_funcname_toolz()
dask.tests.test_utils.test_itemgetter()
dask.tests.test_utils.test_memory_repr()
dask.tests.test_utils.test_method_caller()
dask.tests.test_utils.test_ndeepmap()
dask.tests.test_utils.test_partial_by_order()
dask.tests.test_utils.test_random_state_data()
dask.tests.test_utils.test_skip_doctest()
dask.tests.test_utils.test_takes_multiple_arguments()


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/tests/test_cache.py----------------------------------------
A:dask.tests.test_cache.cachey->pytest.importorskip('cachey')
A:dask.tests.test_cache.c->Cache(10000)
A:dask.tests.test_cache.cc->Cache(c)
dask.tests.test_cache.f(duration,size,*args)
dask.tests.test_cache.inc(x)
dask.tests.test_cache.test_cache()
dask.tests.test_cache.test_cache_with_number()
dask.tests.test_cache.test_prefer_cheap_dependent()


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/tests/test_distributed.py----------------------------------------
A:dask.tests.test_distributed.distributed->pytest.importorskip('distributed')
A:dask.tests.test_distributed.gen_cluster->partial(gen_cluster, should_check_state=False)
A:dask.tests.test_distributed.cluster->partial(cluster, should_check_state=False)
A:dask.tests.test_distributed.x->delayed(inc)(1).persist()
A:dask.tests.test_distributed.(x2,)->persist(x)
A:dask.tests.test_distributed.y->delayed(inc)(2).persist(get=dask.get)
A:dask.tests.test_distributed.(y2, one)->persist(y, 1)
A:dask.tests.test_distributed.pd->pytest.importorskip('pandas')
A:dask.tests.test_distributed.dd->pytest.importorskip('dask.dataframe')
A:dask.tests.test_distributed.df->pytest.importorskip('pandas').DataFrame({'x': [1, 2, 3, 4], 'y': [1, 0, 1, 0]})
A:dask.tests.test_distributed.futures->c.scatter([x, x])
A:dask.tests.test_distributed.ddf->pytest.importorskip('dask.dataframe').from_pandas(df, npartitions=2)
A:dask.tests.test_distributed.db->pytest.importorskip('dask.bag')
A:dask.tests.test_distributed.b->pytest.importorskip('dask.bag').from_delayed(futures)
A:dask.tests.test_distributed.da->pytest.importorskip('dask.array')
A:dask.tests.test_distributed.np->pytest.importorskip('numpy')
A:dask.tests.test_distributed.A->pytest.importorskip('dask.array').concatenate([da.from_delayed(f, shape=x.shape, dtype=x.dtype) for f in futures], axis=0)
A:dask.tests.test_distributed.result->pytest.importorskip('dask.dataframe').from_pandas(df, npartitions=2).groupby('y').agg('count')
dask.tests.test_distributed.test_can_import_client()
dask.tests.test_distributed.test_futures_to_delayed_array(loop)
dask.tests.test_distributed.test_futures_to_delayed_bag(loop)
dask.tests.test_distributed.test_futures_to_delayed_dataframe(loop)
dask.tests.test_distributed.test_local_get_with_distributed_active(c,s,a,b)
dask.tests.test_distributed.test_persist(c,s,a,b)
dask.tests.test_distributed.test_serializable_groupby_agg(c,s,a,b)
dask.tests.test_distributed.test_to_hdf_distributed(loop)
dask.tests.test_distributed.test_to_hdf_scheduler_distributed(npartitions,loop)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/tests/test_multiprocessing.py----------------------------------------
A:dask.tests.test_multiprocessing.e->TypeError('hello')
A:dask.tests.test_multiprocessing.a->NotUnpickleable()
A:dask.tests.test_multiprocessing.b->remote_exception(e, 'traceback-body')
A:dask.tests.test_multiprocessing.pool->multiprocessing.Pool()
A:dask.tests.test_multiprocessing.(results,)->compute([delayed(f, pure=False)() for i in range(N)])
dask.tests.test_multiprocessing.NotUnpickleable(object)
dask.tests.test_multiprocessing.NotUnpickleable.__getstate__(self)
dask.tests.test_multiprocessing.NotUnpickleable.__setstate__(self,state)
dask.tests.test_multiprocessing.bad()
dask.tests.test_multiprocessing.make_bad_result()
dask.tests.test_multiprocessing.test_dumps_loads()
dask.tests.test_multiprocessing.test_errors_propagate()
dask.tests.test_multiprocessing.test_fuse_doesnt_clobber_intermediates()
dask.tests.test_multiprocessing.test_optimize_graph_false()
dask.tests.test_multiprocessing.test_pickle_globals()
dask.tests.test_multiprocessing.test_random_seeds(random)
dask.tests.test_multiprocessing.test_remote_exception()
dask.tests.test_multiprocessing.test_reuse_pool()
dask.tests.test_multiprocessing.test_unpicklable_args_generate_errors()
dask.tests.test_multiprocessing.test_unpicklable_results_generate_errors()


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/tests/test_callbacks.py----------------------------------------
A:dask.tests.test_callbacks.inner_callback->MyCallback()
A:dask.tests.test_callbacks.outer_callback->MyCallback()
dask.tests.test_callbacks.test_finish_always_called()
dask.tests.test_callbacks.test_nested_schedulers()
dask.tests.test_callbacks.test_start_callback()
dask.tests.test_callbacks.test_start_state_callback()


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/tests/test_local.py----------------------------------------
A:dask.tests.test_local.result->start_state_from_dask(dsk)
A:dask.tests.test_local.state->start_state_from_dask(dsk)
A:dask.tests.test_local.deps->dict(((k, set()) for k in 'abc'))
A:dask.tests.test_local.state['running']->set(['z', 'other-task'])
A:dask.tests.test_local.get->staticmethod(get_sync)
A:dask.tests.test_local.cache->Chest()
A:dask.tests.test_local.x_keys->sorted(dsk)
dask.tests.test_local.TestGetAsync(GetFunctionTestMixin)
dask.tests.test_local.TestGetAsync.test_get_sync_num_workers(self)
dask.tests.test_local.test_cache_options()
dask.tests.test_local.test_callback()
dask.tests.test_local.test_exceptions_propagate()
dask.tests.test_local.test_finish_task()
dask.tests.test_local.test_order_of_startstate()
dask.tests.test_local.test_ordering()
dask.tests.test_local.test_sort_key()
dask.tests.test_local.test_start_state()
dask.tests.test_local.test_start_state_looks_at_cache()
dask.tests.test_local.test_start_state_with_independent_but_runnable_tasks()
dask.tests.test_local.test_start_state_with_redirects()
dask.tests.test_local.test_start_state_with_tasks_no_deps()


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/tests/test_sizeof.py----------------------------------------
A:dask.tests.test_sizeof.np->pytest.importorskip('numpy')
A:dask.tests.test_sizeof.dt->pytest.importorskip('numpy').dtype('f8')
A:dask.tests.test_sizeof.pd->pytest.importorskip('pandas')
A:dask.tests.test_sizeof.df->pytest.importorskip('pandas').DataFrame({'x': [1, 2, 3], 'y': ['a' * 100, 'b' * 100, 'c' * 100]}, index=[10, 20, 30])
A:dask.tests.test_sizeof.sparse->pytest.importorskip('scipy.sparse')
A:dask.tests.test_sizeof.sp->pytest.importorskip('scipy.sparse').eye(10)
dask.tests.test_sizeof.test_base()
dask.tests.test_sizeof.test_containers()
dask.tests.test_sizeof.test_name()
dask.tests.test_sizeof.test_numpy()
dask.tests.test_sizeof.test_pandas()
dask.tests.test_sizeof.test_sparse_matrix()


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/tests/test_hashing.py----------------------------------------
A:dask.tests.test_hashing.np->pytest.importorskip('numpy')
A:dask.tests.test_hashing.h->hasher(x)
dask.tests.test_hashing.test_hash_buffer(x)
dask.tests.test_hashing.test_hash_buffer_hex(x)
dask.tests.test_hashing.test_hashers(hasher)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/tests/test_core.py----------------------------------------
A:dask.tests.test_core.f->namedtuple('f', ['x', 'y'])
A:dask.tests.test_core.get->staticmethod(core.get)
A:dask.tests.test_core.custom_testget->TestCustomGetPass()
A:dask.tests.test_core.s->get_dependencies(dsk, task=[], as_list=True)
A:dask.tests.test_core.(dependencies, dependents)->get_deps(dsk)
A:dask.tests.test_core.a->MutateOnEq()
A:dask.tests.test_core.task->F()
A:dask.tests.test_core.df->pandas.DataFrame()
A:dask.tests.test_core.l->literal((add, 1, 2))
dask.tests.test_core.MutateOnEq(object)
dask.tests.test_core.MutateOnEq.__eq__(self,other)
dask.tests.test_core.TestGet(GetFunctionTestMixin)
dask.tests.test_core.TestRecursiveGet(GetFunctionTestMixin)
dask.tests.test_core.TestRecursiveGet.test_get_stack_limit(self)
dask.tests.test_core.contains(a,b)
dask.tests.test_core.test_GetFunctionTestMixin_class()
dask.tests.test_core.test_flatten()
dask.tests.test_core.test_get_dependencies_empty()
dask.tests.test_core.test_get_dependencies_list()
dask.tests.test_core.test_get_dependencies_many()
dask.tests.test_core.test_get_dependencies_nested()
dask.tests.test_core.test_get_dependencies_nothing()
dask.tests.test_core.test_get_dependencies_task()
dask.tests.test_core.test_get_deps()
dask.tests.test_core.test_has_tasks()
dask.tests.test_core.test_istask()
dask.tests.test_core.test_literal_serializable()
dask.tests.test_core.test_preorder_traversal()
dask.tests.test_core.test_quote()
dask.tests.test_core.test_subs()
dask.tests.test_core.test_subs_no_key_data_eq()
dask.tests.test_core.test_subs_with_surprisingly_friendly_eq()
dask.tests.test_core.test_subs_with_unfriendly_eq()


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/tests/test_order.py----------------------------------------
A:dask.tests.test_order.d->dict((((a, i), (f,)) for i in range(4)))
A:dask.tests.test_order.o->order(dsk)
A:dask.tests.test_order.dsk->dict(chain((((a, i), i * 2) for i in range(5)), (((b, i), (add, i, (a, i))) for i in range(5)), (((c, i), (add, i, (b, i))) for i in range(5))))
A:dask.tests.test_order.(dependencies, dependents)->get_deps(dsk)
A:dask.tests.test_order.scores->dict.fromkeys(dsk, 1)
A:dask.tests.test_order.result->ndependents(*get_deps(dsk))
A:dask.tests.test_order.expected->dict(chain((((a, i), 3) for i in range(5)), (((b, i), 2) for i in range(5)), (((c, i), 1) for i in range(5))))
A:dask.tests.test_order.deps->get_deps(dsk)
A:dask.tests.test_order.x_keys->sorted(dsk)
A:dask.tests.test_order.dsk['y']->list(x_keys)
dask.tests.test_order.f(*args)
dask.tests.test_order.issorted(L,reverse=False)
dask.tests.test_order.test_base_of_reduce_preferred()
dask.tests.test_order.test_break_ties_by_str()
dask.tests.test_order.test_deep_bases_win_over_dependents()
dask.tests.test_order.test_ndependents()
dask.tests.test_order.test_order_doesnt_fail_on_mixed_type_keys()
dask.tests.test_order.test_ordering_keeps_groups_together()
dask.tests.test_order.test_prefer_broker_nodes()
dask.tests.test_order.test_prefer_deep()
dask.tests.test_order.test_stacklimit()


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/tests/test_delayed.py----------------------------------------
A:dask.tests.test_delayed.a->func(1)
A:dask.tests.test_delayed.b->pytest.importorskip('dask.bag').from_sequence([1, 2, 3])
A:dask.tests.test_delayed.(task, dask)->to_task_dask(MyClass())
A:dask.tests.test_delayed.f->delayed(foo)
A:dask.tests.test_delayed.x->pytest.importorskip('dask.array').ones(10, chunks=5)
A:dask.tests.test_delayed.add2->delayed(add)
A:dask.tests.test_delayed.o->func(1).index(1)
A:dask.tests.test_delayed.dsk->func(1).index(1)._optimize(o.dask, o._keys())
A:dask.tests.test_delayed.c->delayed(f)(iter([a, b]))
A:dask.tests.test_delayed.lit->set((a, b, 3))
A:dask.tests.test_delayed.res->delayed(x, traverse=False).compute()
A:dask.tests.test_delayed.v1->delayed(add, pure=True)(1, 2)
A:dask.tests.test_delayed.v2->delayed(add, pure=True)(1, 2)
A:dask.tests.test_delayed.myrand->delayed(random)
A:dask.tests.test_delayed.func->delayed(identity, pure=True)
A:dask.tests.test_delayed.data->delayed([1, 2, 3])
A:dask.tests.test_delayed.dmysum->delayed(mysum, pure=True)
A:dask.tests.test_delayed.ten->dmysum(1, 2, c=c, four=dmysum(2, 2))
A:dask.tests.test_delayed.np->pytest.importorskip('numpy')
A:dask.tests.test_delayed.da->pytest.importorskip('dask.array')
A:dask.tests.test_delayed.arr->pytest.importorskip('numpy').arange(100).reshape((10, 10))
A:dask.tests.test_delayed.darr->pytest.importorskip('dask.array').from_array(arr, chunks=(5, 5))
A:dask.tests.test_delayed.val->delayed(sum)([arr, darr, 1])
A:dask.tests.test_delayed.(task, dsk)->to_task_dask(darr)
A:dask.tests.test_delayed.orig->set(darr.dask)
A:dask.tests.test_delayed.final->set(dsk)
A:dask.tests.test_delayed.diff->set(dsk).difference(orig)
A:dask.tests.test_delayed.delayed_arr->delayed(darr)
A:dask.tests.test_delayed.db->pytest.importorskip('dask.bag')
A:dask.tests.test_delayed.arr1->pytest.importorskip('numpy').arange(100).reshape((10, 10))
A:dask.tests.test_delayed.arr2->pytest.importorskip('numpy').arange(100).reshape((10, 10)).dot(arr1.T)
A:dask.tests.test_delayed.darr1->pytest.importorskip('dask.array').from_array(arr1, chunks=(5, 5))
A:dask.tests.test_delayed.darr2->pytest.importorskip('dask.array').from_array(arr2, chunks=(5, 5))
A:dask.tests.test_delayed.out->delayed(sum)([i.sum() for i in seq])
A:dask.tests.test_delayed.y->pickle.loads(pickle.dumps(x))
A:dask.tests.test_delayed.v->delayed([x])
A:dask.tests.test_delayed.foo->Foo(1)
dask.tests.test_delayed.test_array_bag_delayed()
dask.tests.test_delayed.test_array_delayed()
dask.tests.test_delayed.test_attributes()
dask.tests.test_delayed.test_callable_obj()
dask.tests.test_delayed.test_common_subexpressions()
dask.tests.test_delayed.test_delayed()
dask.tests.test_delayed.test_delayed_callable()
dask.tests.test_delayed.test_delayed_compute_forward_kwargs()
dask.tests.test_delayed.test_delayed_errors()
dask.tests.test_delayed.test_delayed_method_descriptor()
dask.tests.test_delayed.test_delayed_name()
dask.tests.test_delayed.test_delayed_name_on_call()
dask.tests.test_delayed.test_delayed_picklable()
dask.tests.test_delayed.test_finalize_name()
dask.tests.test_delayed.test_iterators()
dask.tests.test_delayed.test_kwargs()
dask.tests.test_delayed.test_lists()
dask.tests.test_delayed.test_lists_are_concrete()
dask.tests.test_delayed.test_literates()
dask.tests.test_delayed.test_literates_keys()
dask.tests.test_delayed.test_method_getattr_optimize()
dask.tests.test_delayed.test_methods()
dask.tests.test_delayed.test_name_consistent_across_instances()
dask.tests.test_delayed.test_nout()
dask.tests.test_delayed.test_operators()
dask.tests.test_delayed.test_pure()
dask.tests.test_delayed.test_pure_global_setting()
dask.tests.test_delayed.test_sensitive_to_partials()
dask.tests.test_delayed.test_to_task_dask()
dask.tests.test_delayed.test_traverse_false()


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/tests/test_dot.py----------------------------------------
A:dask.tests.test_dot.pytestmark->pytest.mark.skipif(True, reason='graphviz exception with Python -OO flag')
A:dask.tests.test_dot.label_re->re.compile('.*\\[label=(.*?) shape=.*\\]')
A:dask.tests.test_dot.m->re.compile('.*\\[label=(.*?) shape=.*\\]').match(line)
A:dask.tests.test_dot.result->dot_graph(dsk, filename=filename, format=format)
A:dask.tests.test_dot.g->to_graphviz({'x': 1, 'y': 'x'})
A:dask.tests.test_dot.labels->list(filter(None, map(get_label, g.body)))
A:dask.tests.test_dot.funcs->set(('add', 'sum', 'neg'))
A:dask.tests.test_dot.filename->str(tmpdir.join('$(touch should_not_get_created.txt)'))
A:dask.tests.test_dot.target->'.'.join([default_name, default_format])
A:dask.tests.test_dot.before->tmpdir.listdir()
A:dask.tests.test_dot.after->tmpdir.listdir()
A:dask.tests.test_dot.x->delayed(f)(1, y=2)
A:dask.tests.test_dot.label->task_label(x.dask[x.key])
dask.tests.test_dot.get_label(line)
dask.tests.test_dot.test_aliases()
dask.tests.test_dot.test_delayed_kwargs_apply()
dask.tests.test_dot.test_dot_graph(tmpdir)
dask.tests.test_dot.test_dot_graph_defaults()
dask.tests.test_dot.test_dot_graph_no_filename(tmpdir)
dask.tests.test_dot.test_filenames_and_formats()
dask.tests.test_dot.test_label()
dask.tests.test_dot.test_task_label()
dask.tests.test_dot.test_to_graphviz()
dask.tests.test_dot.test_to_graphviz_attributes()


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/tests/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/tests/test_rewrite.py----------------------------------------
A:dask.tests.test_rewrite.t->Traverser(term)
A:dask.tests.test_rewrite.t2->Traverser(term).copy()
A:dask.tests.test_rewrite.rule1->RewriteRule((add, 'a', 1), (inc, 'a'), vars)
A:dask.tests.test_rewrite.rule2->RewriteRule((add, 'a', 'a'), (double, 'a'), vars)
A:dask.tests.test_rewrite.rule3->RewriteRule((add, (inc, 'a'), (inc, 'a')), (add, (double, 'a'), 2), vars)
A:dask.tests.test_rewrite.rule4->RewriteRule((add, (inc, 'b'), (inc, 'a')), (add, (add, 'a', 'b'), 2), vars)
A:dask.tests.test_rewrite.rule5->RewriteRule((sum, ['c', 'b', 'a']), (add, (add, 'a', 'b'), 'c'), vars)
A:dask.tests.test_rewrite.rule6->RewriteRule((list, 'x'), repl_list, ('x',))
A:dask.tests.test_rewrite.rs->RuleSet(*rules)
A:dask.tests.test_rewrite.matches->list(rs.iter_matches(term))
A:dask.tests.test_rewrite.new_term->RuleSet(*rules).rewrite(new_term)
dask.tests.test_rewrite.double(x)
dask.tests.test_rewrite.repl_list(sd)
dask.tests.test_rewrite.test_RewriteRule()
dask.tests.test_rewrite.test_RewriteRuleSubs()
dask.tests.test_rewrite.test_RuleSet()
dask.tests.test_rewrite.test_args()
dask.tests.test_rewrite.test_head()
dask.tests.test_rewrite.test_matches()
dask.tests.test_rewrite.test_rewrite()
dask.tests.test_rewrite.test_traverser()


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/tests/test_sharedict.py----------------------------------------
A:dask.tests.test_sharedict.s->ShareDict()
A:dask.tests.test_sharedict.d->merge(a, b, c)
A:dask.tests.test_sharedict.s2->ShareDict()
dask.tests.test_sharedict.test_core()
dask.tests.test_sharedict.test_keys_items()
dask.tests.test_sharedict.test_structure()
dask.tests.test_sharedict.test_structure_2()
dask.tests.test_sharedict.test_update_with_sharedict()


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/tests/test_context.py----------------------------------------
A:dask.tests.test_context.x->dask.array.ones(10, chunks=(5,))
dask.tests.test_context.test_defer_to_globals()
dask.tests.test_context.test_set_options_context_manger()
dask.tests.test_context.test_with_get()


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/dataframe/indexing.py----------------------------------------
A:dask.dataframe.indexing.iindexer->_maybe_partial_time_string(self.obj._meta_nonempty.index, iindexer, kind='loc')
A:dask.dataframe.indexing.meta->self._make_meta(iindexer, cindexer)
A:dask.dataframe.indexing.parts->self._get_partitions(iindexer)
A:dask.dataframe.indexing.items->sorted(parts.items())
A:dask.dataframe.indexing.part->self._get_partitions(iindexer)
A:dask.dataframe.indexing.start->index._maybe_cast_slice_bound(indexer, 'left', 'loc')
A:dask.dataframe.indexing.stop->index._maybe_cast_slice_bound(indexer, 'right', 'loc')
A:dask.dataframe.indexing.istart->self._coerce_loc_index(iindexer.start)
A:dask.dataframe.indexing.istop->self._coerce_loc_index(iindexer.stop)
A:dask.dataframe.indexing.div_start->max(istart, self.obj.divisions[start])
A:dask.dataframe.indexing.div_stop->min(istop, self.obj.divisions[stop + 1])
A:dask.dataframe.indexing.val->_coerce_loc_index(divisions, val)
A:dask.dataframe.indexing.i->bisect.bisect_right(divisions, val)
A:dask.dataframe.indexing.results->defaultdict(list)
A:dask.dataframe.indexing.values->pandas.Index(values, dtype=object)
A:dask.dataframe.indexing.div->min(len(divisions) - 2, max(0, i - 1))
dask.dataframe.indexing._LocIndexer(self,obj)
dask.dataframe.indexing._LocIndexer.__getitem__(self,key)
dask.dataframe.indexing._LocIndexer.__init__(self,obj)
dask.dataframe.indexing._LocIndexer._coerce_loc_index(self,key)
dask.dataframe.indexing._LocIndexer._get_partitions(self,keys)
dask.dataframe.indexing._LocIndexer._loc(self,iindexer,cindexer)
dask.dataframe.indexing._LocIndexer._loc_element(self,iindexer,cindexer)
dask.dataframe.indexing._LocIndexer._loc_list(self,iindexer,cindexer)
dask.dataframe.indexing._LocIndexer._loc_series(self,iindexer,cindexer)
dask.dataframe.indexing._LocIndexer._loc_slice(self,iindexer,cindexer)
dask.dataframe.indexing._LocIndexer._make_meta(self,iindexer,cindexer)
dask.dataframe.indexing._LocIndexer._maybe_partial_time_string(self,iindexer)
dask.dataframe.indexing._LocIndexer._name(self)
dask.dataframe.indexing._coerce_loc_index(divisions,o)
dask.dataframe.indexing._maybe_partial_time_string(index,indexer,kind)
dask.dataframe.indexing._partition_of_index_value(divisions,val)
dask.dataframe.indexing._partitions_of_index_values(divisions,values)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/dataframe/utils.py----------------------------------------
A:dask.dataframe.utils.PANDAS_VERSION->LooseVersion(pd.__version__)
A:dask.dataframe.utils.divisions->numpy.array(divisions)
A:dask.dataframe.utils.df->df.sort_index().sort_index()
A:dask.dataframe.utils.index->index.as_ordered().as_ordered()
A:dask.dataframe.utils.indices->index.as_ordered().as_ordered().searchsorted(divisions)
A:dask.dataframe.utils.body->textwrap.wrap(_META_DESCRIPTION, initial_indent=indent, subsequent_indent=indent, width=78)
A:dask.dataframe.utils.descr->'{0}\n{1}'.format(_META_TYPES, '\n'.join(body))
A:dask.dataframe.utils.f.__doc__->'{0}{1}{2}\n{3}{4}\n\n{5}'.format(first, parameter_header, parameters, indent[4:], descr, rest)
A:dask.dataframe.utils.(first, last)->re.split('Parameters\\n[ ]*----------', f.__doc__)
A:dask.dataframe.utils.(parameters, rest)->last.split('\n\n', 1)
A:dask.dataframe.utils.(exc_type, exc_value, exc_traceback)->sys.exc_info()
A:dask.dataframe.utils.tb->''.join(traceback.format_tb(exc_traceback))
A:dask.dataframe.utils.msg->'Unsupported dask instance {0} found'.format(type(dsk))
A:dask.dataframe.utils.x->x.set_categories([UNKNOWN_CATEGORIES]).set_categories([UNKNOWN_CATEGORIES])
A:dask.dataframe.utils.x.index->x.set_categories([UNKNOWN_CATEGORIES]).set_categories([UNKNOWN_CATEGORIES]).index.set_categories([UNKNOWN_CATEGORIES])
A:dask.dataframe.utils.dtype->numpy.dtype(x)
A:dask.dataframe.utils.typ->type(idx)
A:dask.dataframe.utils.start->numpy.timedelta64(1, 'D')
A:dask.dataframe.utils.data->numpy.array([entry, entry], dtype=dtype)
A:dask.dataframe.utils.entry->_scalar_from_dtype(dtype)
A:dask.dataframe.utils.idx->_nonempty_index(x.index)
A:dask.dataframe.utils.res->pandas.DataFrame(data, index=idx, columns=np.arange(len(x.columns)))
A:dask.dataframe.utils.dtypes->pandas.concat([x.dtypes, meta.dtypes], axis=1)
A:dask.dataframe.utils.result->dsk.compute(get=get_sync)
A:dask.dataframe.utils.a->_maybe_sort(a)
A:dask.dataframe.utils.at->type(np.asarray(a.divisions).tolist()[0])
A:dask.dataframe.utils.bt->type(np.asarray(b.divisions).tolist()[0])
A:dask.dataframe.utils.b->numpy.dtype(type(res))
A:dask.dataframe.utils.results->get_sync(ddf.dask, ddf._keys())
A:dask.dataframe.utils.(dependencies, dependents)->get_deps(x.dask)
dask.dataframe.utils._check_dask(dsk,check_names=True,check_dtypes=True,result=None)
dask.dataframe.utils._empty_series(name,dtype,index=None)
dask.dataframe.utils._maybe_sort(a)
dask.dataframe.utils._nonempty_index(idx)
dask.dataframe.utils._nonempty_scalar(x)
dask.dataframe.utils._nonempty_series(s,idx)
dask.dataframe.utils._scalar_from_dtype(dtype)
dask.dataframe.utils.assert_dask_dtypes(ddf,res,numeric_equal=True)
dask.dataframe.utils.assert_dask_graph(dask,label)
dask.dataframe.utils.assert_divisions(ddf)
dask.dataframe.utils.assert_eq(a,b,check_names=True,check_dtypes=True,check_divisions=True,check_index=True,**kwargs)
dask.dataframe.utils.assert_max_deps(x,n,eq=True)
dask.dataframe.utils.assert_sane_keynames(ddf)
dask.dataframe.utils.check_meta(x,meta,funcname=None,numeric_equal=True)
dask.dataframe.utils.clear_known_categories(x,cols=None,index=True)
dask.dataframe.utils.has_known_categories(x)
dask.dataframe.utils.insert_meta_param_description(*args,**kwargs)
dask.dataframe.utils.make_meta(x,index=None)
dask.dataframe.utils.meta_nonempty(x)
dask.dataframe.utils.raise_on_meta_error(funcname=None)
dask.dataframe.utils.shard_df_on_index(df,divisions)
dask.dataframe.utils.strip_unknown_categories(x)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/dataframe/groupby.py----------------------------------------
A:dask.dataframe.groupby.columns->sorted((col for col in columns if col.startswith(prefix)))
A:dask.dataframe.groupby.by->dict(name=df.columns[0], levels=_determine_levels(index)).get('by', None)
A:dask.dataframe.groupby.g->SeriesGroupBy(self.obj, by=self.index, slice=key)
A:dask.dataframe.groupby.grouped->sample.groupby(index_meta)
A:dask.dataframe.groupby.func->funcname(known_np_funcs.get(func, func))
A:dask.dataframe.groupby.df->df.to_frame().to_frame()
A:dask.dataframe.groupby.x->SeriesGroupBy(self.obj, by=self.index, slice=key).sum()
A:dask.dataframe.groupby.x2->g[g.columns[nc // 3:2 * nc // 3]].rename(columns=lambda c: c[:-3])
A:dask.dataframe.groupby.n->g[g.columns[-nc // 3:]].rename(columns=lambda c: c[:-6])
A:dask.dataframe.groupby.nc->len(g.columns)
A:dask.dataframe.groupby.levels->_determine_levels(self.index)
A:dask.dataframe.groupby.name->dict(name=df.columns[0], levels=_determine_levels(index)).pop('name')
A:dask.dataframe.groupby.grouped.index->sample.groupby(index_meta).index.get_level_values(level=levels)
A:dask.dataframe.groupby.result->super(SeriesGroupBy, self).aggregate(arg, split_every=split_every, split_out=split_out)
A:dask.dataframe.groupby.result.index->super(SeriesGroupBy, self).aggregate(arg, split_every=split_every, split_out=split_out).index.get_level_values(level=levels)
A:dask.dataframe.groupby.kwargs->dict(name=df.columns[0], levels=_determine_levels(index))
A:dask.dataframe.groupby.spec->_normalize_spec({None: arg}, [])
A:dask.dataframe.groupby.impls->_build_agg_args_var(result_column, func, input_column)
A:dask.dataframe.groupby.chunks->sorted(chunks.values())
A:dask.dataframe.groupby.aggs->sorted(aggs.values())
A:dask.dataframe.groupby.intermediate->_make_agg_id(func, input_column)
A:dask.dataframe.groupby.int_sum->_make_agg_id('sum', input_column)
A:dask.dataframe.groupby.int_sum2->_make_agg_id('sum2', input_column)
A:dask.dataframe.groupby.int_count->_make_agg_id('count', input_column)
A:dask.dataframe.groupby.col->_make_agg_id(funcname(func), input_column)
A:dask.dataframe.groupby.funcs->dict(name=df.columns[0], levels=_determine_levels(index)).pop('funcs')
A:dask.dataframe.groupby.r->func(grouped, **func_kwargs)
A:dask.dataframe.groupby.result[result_column]->func(df, **kwargs)
A:dask.dataframe.groupby.align->cum_last.reindex(part.set_index(index).index, fill_value=initial)
A:dask.dataframe.groupby.union->a.index.union(b.index)
A:dask.dataframe.groupby.self.index->_normalize_index(df, by)
A:dask.dataframe.groupby.do_index_partition_align->all((item.divisions == df.divisions if isinstance(item, Series) else True for item in self.index))
A:dask.dataframe.groupby.self._meta->self.obj._meta.groupby(index_meta)
A:dask.dataframe.groupby.meta->make_meta(meta)
A:dask.dataframe.groupby.cumpart_raw->map_partitions(_apply_chunk, self.obj, *index, chunk=chunk, columns=columns, token=name_part, meta=meta)
A:dask.dataframe.groupby.cumpart_ext->cumpart_raw_frame.assign(**{i: self.obj[i] if np.isscalar(i) and i in self.obj.columns else self.obj.index for i in index})
A:dask.dataframe.groupby.cumlast->map_partitions(_apply_chunk, cumpart_ext, *index_groupers, columns=0 if columns is None else columns, chunk=M.last, meta=meta, token=name_last)
A:dask.dataframe.groupby.v->self.var(ddof, split_every=split_every, split_out=split_out)
A:dask.dataframe.groupby.group_columns->set()
A:dask.dataframe.groupby.(chunk_funcs, aggregate_funcs, finalizers)->_build_agg_args(spec)
A:dask.dataframe.groupby.obj->aca(chunk_args, chunk=_groupby_apply_funcs, chunk_kwargs=dict(funcs=chunk_funcs), aggregate=_groupby_apply_funcs, aggregate_kwargs=dict(funcs=aggregate_funcs, level=levels), combine=_groupby_apply_funcs, combine_kwargs=dict(funcs=aggregate_funcs, level=levels), token='aggregate', split_every=split_every, split_out=split_out, split_out_setup=split_out_on_index)
A:dask.dataframe.groupby.df2->df.to_frame().to_frame().assign(_index=self.index)
A:dask.dataframe.groupby.index->df.to_frame().to_frame()._select_columns_or_index(self.index)
A:dask.dataframe.groupby.df3->shuffle(df2, index)
A:dask.dataframe.groupby.df4->shuffle(df2, index).drop('_index', axis=1)
A:dask.dataframe.groupby.df5->map_partitions(_groupby_slice_apply, df4, index2, self._slice, func, meta=meta)
dask.dataframe.Aggregation(self,name,chunk,agg,finalize=None)
dask.dataframe.groupby.Aggregation(self,name,chunk,agg,finalize=None)
dask.dataframe.groupby.Aggregation.__init__(self,name,chunk,agg,finalize=None)
dask.dataframe.groupby.DataFrameGroupBy(_GroupBy)
dask.dataframe.groupby.DataFrameGroupBy.__dir__(self)
dask.dataframe.groupby.DataFrameGroupBy.__getattr__(self,key)
dask.dataframe.groupby.DataFrameGroupBy.__getitem__(self,key)
dask.dataframe.groupby.DataFrameGroupBy.agg(self,arg,split_every=None,split_out=1)
dask.dataframe.groupby.DataFrameGroupBy.aggregate(self,arg,split_every=None,split_out=1)
dask.dataframe.groupby.SeriesGroupBy(self,df,by=None,slice=None)
dask.dataframe.groupby.SeriesGroupBy.__init__(self,df,by=None,slice=None)
dask.dataframe.groupby.SeriesGroupBy.agg(self,arg,split_every=None,split_out=1)
dask.dataframe.groupby.SeriesGroupBy.aggregate(self,arg,split_every=None,split_out=1)
dask.dataframe.groupby.SeriesGroupBy.nunique(self,split_every=None,split_out=1)
dask.dataframe.groupby._GroupBy(self,df,by=None,slice=None)
dask.dataframe.groupby._GroupBy.__init__(self,df,by=None,slice=None)
dask.dataframe.groupby._GroupBy._aca_agg(self,token,func,aggfunc=None,split_every=None,split_out=1)
dask.dataframe.groupby._GroupBy._cum_agg(self,token,chunk,aggregate,initial)
dask.dataframe.groupby._GroupBy._meta_nonempty(self)
dask.dataframe.groupby._GroupBy.aggregate(self,arg,split_every,split_out=1)
dask.dataframe.groupby._GroupBy.apply(self,func,meta=no_default)
dask.dataframe.groupby._GroupBy.count(self,split_every=None,split_out=1)
dask.dataframe.groupby._GroupBy.cumcount(self,axis=None)
dask.dataframe.groupby._GroupBy.cumprod(self,axis=0)
dask.dataframe.groupby._GroupBy.cumsum(self,axis=0)
dask.dataframe.groupby._GroupBy.get_group(self,key)
dask.dataframe.groupby._GroupBy.max(self,split_every=None,split_out=1)
dask.dataframe.groupby._GroupBy.mean(self,split_every=None,split_out=1)
dask.dataframe.groupby._GroupBy.min(self,split_every=None,split_out=1)
dask.dataframe.groupby._GroupBy.size(self,split_every=None,split_out=1)
dask.dataframe.groupby._GroupBy.std(self,ddof=1,split_every=None,split_out=1)
dask.dataframe.groupby._GroupBy.sum(self,split_every=None,split_out=1)
dask.dataframe.groupby._GroupBy.var(self,ddof=1,split_every=None,split_out=1)
dask.dataframe.groupby._agg_finalize(df,funcs)
dask.dataframe.groupby._apply_chunk(df,*index,**kwargs)
dask.dataframe.groupby._apply_func_to_column(df_like,column,func)
dask.dataframe.groupby._apply_func_to_columns(df_like,prefix,func)
dask.dataframe.groupby._build_agg_args(spec)
dask.dataframe.groupby._build_agg_args_custom(result_column,func,input_column)
dask.dataframe.groupby._build_agg_args_mean(result_column,func,input_column)
dask.dataframe.groupby._build_agg_args_simple(result_column,func,input_column,impl_pair)
dask.dataframe.groupby._build_agg_args_single(result_column,func,input_column)
dask.dataframe.groupby._build_agg_args_std(result_column,func,input_column)
dask.dataframe.groupby._build_agg_args_var(result_column,func,input_column)
dask.dataframe.groupby._compute_sum_of_squares(grouped,column)
dask.dataframe.groupby._cum_agg_aligned(part,cum_last,index,columns,func,initial)
dask.dataframe.groupby._cum_agg_filled(a,b,func,initial)
dask.dataframe.groupby._cumcount_aggregate(a,b,fill_value=None)
dask.dataframe.groupby._determine_levels(index)
dask.dataframe.groupby._finalize_mean(df,sum_column,count_column)
dask.dataframe.groupby._finalize_std(df,count_column,sum_column,sum2_column,ddof=1)
dask.dataframe.groupby._finalize_var(df,count_column,sum_column,sum2_column,ddof=1)
dask.dataframe.groupby._groupby_aggregate(df,aggfunc=None,levels=None)
dask.dataframe.groupby._groupby_apply_funcs(df,*index,**kwargs)
dask.dataframe.groupby._groupby_get_group(df,by_key,get_key,columns)
dask.dataframe.groupby._groupby_raise_unaligned(df,**kwargs)
dask.dataframe.groupby._groupby_slice_apply(df,grouper,key,func)
dask.dataframe.groupby._is_aligned(df,by)
dask.dataframe.groupby._make_agg_id(func,column)
dask.dataframe.groupby._maybe_slice(grouped,columns)
dask.dataframe.groupby._normalize_index(df,index)
dask.dataframe.groupby._normalize_spec(spec,non_group_columns)
dask.dataframe.groupby._nunique_df_aggregate(df,levels,name)
dask.dataframe.groupby._nunique_df_chunk(df,*index,**kwargs)
dask.dataframe.groupby._nunique_df_combine(df,levels)
dask.dataframe.groupby._nunique_series_chunk(df,*index,**_ignored_)
dask.dataframe.groupby._var_agg(g,levels,ddof)
dask.dataframe.groupby._var_chunk(df,*index)
dask.dataframe.groupby._var_combine(g,levels)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/dataframe/core.py----------------------------------------
A:dask.dataframe.core._optimize->staticmethod(optimize)
A:dask.dataframe.core._default_get->staticmethod(threaded.get)
A:dask.dataframe.core._finalize->staticmethod(finalize)
A:dask.dataframe.core.meta->pandas.Series([pd.Timedelta(1, unit=unit)])
A:dask.dataframe.core.o->aca_combine(x, **kwargs)
A:dask.dataframe.core.name->'{0}-{1}'.format(funcname, token)
A:dask.dataframe.core.return_type->_get_return_type(other)
A:dask.dataframe.core.dsk->df._get_numeric_data()._optimize(df.dask, df._keys())
A:dask.dataframe.core.other_meta->make_meta(other)
A:dask.dataframe.core.other_meta_nonempty->meta_nonempty(other_meta)
A:dask.dataframe.core.self.divisions->tuple(divisions)
A:dask.dataframe.core.self._computed->self.compute()
A:dask.dataframe.core.x->x.set_index('idx').set_index('idx')
A:dask.dataframe.core.divisions->list(divisions)
A:dask.dataframe.core.data->numpy.concatenate(data).reshape((len(data),) + data[0].shape)
A:dask.dataframe.core.msg->'Unable to {0} dd.Series with axis=1'.format(name)
A:dask.dataframe.core.state_data->random_state_data(self.npartitions, random_state)
A:dask.dataframe.core.token->tokenize(df, divisions)
A:dask.dataframe.core.result->result.groupby(result.index).sum().groupby(result.index).sum()
A:dask.dataframe.core.axis->self._validate_axis(axis)
A:dask.dataframe.core.parts->new_dd_object(merge(dsk, self.dask), name, meta, self.divisions)
A:dask.dataframe.core.random_state->numpy.random.RandomState()
A:dask.dataframe.core.out->cov_corr_combine(data, corr)
A:dask.dataframe.core.method->getattr(M, name)
A:dask.dataframe.core.num->self._get_numeric_data()
A:dask.dataframe.core.s->self.get_partition(i).compute()
A:dask.dataframe.core.n->len(divisions)
A:dask.dataframe.core.v->self._get_numeric_data().var(skipna=skipna, ddof=ddof, split_every=split_every)
A:dask.dataframe.core.quantiles->tuple((quantile(self[c], q) for c in num.columns))
A:dask.dataframe.core.dask->merge(dask, *[_q.dask for _q in quantiles])
A:dask.dataframe.core.name1->'{0}{1}-map'.format(self._token_prefix, token)
A:dask.dataframe.core.cumpart->map_partitions(chunk, self, token=name1, meta=self, **chunk_kwargs)
A:dask.dataframe.core.name2->'{0}{1}-take-last'.format(self._token_prefix, token)
A:dask.dataframe.core.cumlast->map_partitions(_take_last, cumpart, skipna, meta=pd.Series([]), token=name2)
A:dask.dataframe.core.cname->'{0}{1}-cum-last'.format(self._token_prefix, token)
A:dask.dataframe.core.(meta1, meta2)->_emulate(M.align, self, other, join, axis=axis, fill_value=fill_value)
A:dask.dataframe.core.aligned->self.map_partitions(M.align, other, join=join, axis=axis, fill_value=fill_value)
A:dask.dataframe.core.dsk1->dict((((name1, i), (getitem, key, 0)) for (i, key) in enumerate(aligned._keys())))
A:dask.dataframe.core.result1->new_dd_object(dsk1, name1, meta1, aligned.divisions)
A:dask.dataframe.core.dsk2->dict((((name2, i), (getitem, key, 1)) for (i, key) in enumerate(aligned._keys())))
A:dask.dataframe.core.result2->new_dd_object(dsk2, name2, meta2, aligned.divisions)
A:dask.dataframe.core.offset->pandas.tseries.frequencies.to_offset(offset)
A:dask.dataframe.core.end->self.loc._get_partitions(date)
A:dask.dataframe.core.start->self.loc._get_partitions(date)
A:dask.dataframe.core.renamed->_rename_dask(self, columns)
A:dask.dataframe.core.footer->'dtype: {dtype}'.format(dtype=self.dtype)
A:dask.dataframe.core.df->df._get_numeric_data()._get_numeric_data()
A:dask.dataframe.core.df.divisions->tuple(pd.Index(self.divisions).to_timestamp())
A:dask.dataframe.core.(self, key)->_maybe_align_partitions([self, key])
A:dask.dataframe.core.pairs->list(sum(kwargs.items(), ()))
A:dask.dataframe.core.df2->self._meta.assign(**_extract_meta(kwargs))
A:dask.dataframe.core.numerics->self._meta._get_numeric_data()
A:dask.dataframe.core.computations->dict(zip(computations.keys(), da.compute(*computations.values())))
A:dask.dataframe.core.memory_int->computations['memory_usage'].sum()
A:dask.dataframe.core.selected_df->selected_df.assign(_index=self.index).assign(_index=self.index)
A:dask.dataframe.core.meth->getattr(pd.Series, name)
A:dask.dataframe.core.args->_maybe_align_partitions(args)
A:dask.dataframe.core._is_broadcastable->partial(is_broadcastable, dfs)
A:dask.dataframe.core.dfs->shard_df_on_index(df, divisions[1:-1])
A:dask.dataframe.core.h->pandas.DataFrame([], index=h).reset_index()
A:dask.dataframe.core.chunk_kwargs->dict()
A:dask.dataframe.core.aggregate_kwargs->dict()
A:dask.dataframe.core.combine_kwargs->dict()
A:dask.dataframe.core.npartitions->npartitions.pop().pop()
A:dask.dataframe.core.token_key->tokenize(token or (chunk, aggregate), meta, args, chunk_kwargs, aggregate_kwargs, combine_kwargs, split_every, split_out, split_out_setup, split_out_setup_kwargs)
A:dask.dataframe.core.a->'{0}-chunk-{1}'.format(funcname, df._name)
A:dask.dataframe.core.b->'{0}-agg-{1}'.format(token or funcname(aggregate), token_key)
A:dask.dataframe.core.meta_chunk->_emulate(apply, chunk, args, chunk_kwargs)
A:dask.dataframe.core.res[k]->_extract_meta(x[k], nonempty)
A:dask.dataframe.core.columns->pandas.Index(columns)
A:dask.dataframe.core.metadata->_rename(names, df._meta)
A:dask.dataframe.core.empty_index->pandas.Index([], dtype=float)
A:dask.dataframe.core.val_dsk->dict((((name, i), (_percentile, (getattr, key, 'values'), qs)) for (i, key) in enumerate(df._keys())))
A:dask.dataframe.core.len_dsk->dict((((name2, i), (len, key)) for (i, key) in enumerate(df._keys())))
A:dask.dataframe.core.prefix->'{0}-combine-{1}-'.format(funcname, df._name)
A:dask.dataframe.core.mask->numpy.isfinite(mat)
A:dask.dataframe.core.keep->numpy.bitwise_and(mask[:, None, :], mask[:, :, None])
A:dask.dataframe.core.sums->numpy.nan_to_num(data['sum'])
A:dask.dataframe.core.counts->numpy.bitwise_and(mask[:, None, :], mask[:, :, None]).astype('int').sum(0)
A:dask.dataframe.core.m->numpy.nansum(data['m'] + counts * (sums / counts_na - mu) ** 2, axis=0)
A:dask.dataframe.core.cum_sums->numpy.cumsum(sums, 0)
A:dask.dataframe.core.cum_counts->numpy.cumsum(counts, 0)
A:dask.dataframe.core.nobs->numpy.where(cum_counts[-1], cum_counts[-1], np.nan)
A:dask.dataframe.core.counts_na->numpy.where(counts, counts, np.nan)
A:dask.dataframe.core.den->numpy.sqrt(m2 * m2.T)
A:dask.dataframe.core.p->list(p)
A:dask.dataframe.core.index->pseudorandom(len(df), p, random_state)
A:dask.dataframe.core.group_dummy->numpy.ones(len(a.index))
A:dask.dataframe.core.last_row->'{0}-chunk-{1}'.format(funcname, df._name).groupby(group_dummy).last()
A:dask.dataframe.core.d->dict()
A:dask.dataframe.core.last_elem->_is_single_last_div(c)
A:dask.dataframe.core.freq->pandas.tseries.frequencies.to_offset(freq)
A:dask.dataframe.core.original_divisionsdivisions->pandas.Series(df.divisions)
A:dask.dataframe.core.k->int(new - last)
A:dask.dataframe.core.idx->getattr(x, fn)(skipna=skipna)
A:dask.dataframe.core.value->getattr(x, minmax)(skipna=skipna)
A:dask.dataframe.core.idxvalue->pandas.Series([], dtype='i8')
A:dask.dataframe.core.r->df._get_numeric_data()._get_numeric_data().head(n=n)
A:dask.dataframe.core.divs->pandas.Series(range(len(df.divisions)), index=df.divisions)
A:dask.dataframe.core.dtype->str(s.dtype)
dask.dataframe.DataFrame(_Frame)
dask.dataframe.DataFrame.__array_wrap__(self,array,context=None)
dask.dataframe.DataFrame.__delitem__(self,key)
dask.dataframe.DataFrame.__dir__(self)
dask.dataframe.DataFrame.__getattr__(self,key)
dask.dataframe.DataFrame.__getitem__(self,key)
dask.dataframe.DataFrame.__setattr__(self,key,value)
dask.dataframe.DataFrame.__setitem__(self,key,value)
dask.dataframe.DataFrame._bind_comparison_method(cls,name,comparison)
dask.dataframe.DataFrame._bind_operator_method(cls,name,op)
dask.dataframe.DataFrame._contains_index_name(self,columns_or_index)
dask.dataframe.DataFrame._get_numeric_data(self,how='any',subset=None)
dask.dataframe.DataFrame._is_column_label(self,c)
dask.dataframe.DataFrame._is_index_label(self,i)
dask.dataframe.DataFrame._repr_data(self)
dask.dataframe.DataFrame._repr_html_(self)
dask.dataframe.DataFrame._select_columns_or_index(self,columns_or_index)
dask.dataframe.DataFrame._validate_axis(cls,axis=0)
dask.dataframe.DataFrame.append(self,other)
dask.dataframe.DataFrame.apply(self,func,axis=0,args=(),meta=no_default,**kwds)
dask.dataframe.DataFrame.applymap(self,func,meta='__no_default__')
dask.dataframe.DataFrame.assign(self,**kwargs)
dask.dataframe.DataFrame.categorize(self,columns=None,index=None,split_every=None,**kwargs)
dask.dataframe.DataFrame.clip(self,lower=None,upper=None,out=None)
dask.dataframe.DataFrame.clip_lower(self,threshold)
dask.dataframe.DataFrame.clip_upper(self,threshold)
dask.dataframe.DataFrame.columns(self)
dask.dataframe.DataFrame.columns(self,columns)
dask.dataframe.DataFrame.corr(self,method='pearson',min_periods=None,split_every=False)
dask.dataframe.DataFrame.cov(self,min_periods=None,split_every=False)
dask.dataframe.DataFrame.drop(self,labels,axis=0,errors='raise')
dask.dataframe.DataFrame.dropna(self,how='any',subset=None)
dask.dataframe.DataFrame.dtypes(self)
dask.dataframe.DataFrame.eval(self,expr,inplace=None,**kwargs)
dask.dataframe.DataFrame.get_dtype_counts(self)
dask.dataframe.DataFrame.get_ftype_counts(self)
dask.dataframe.DataFrame.groupby(self,by=None,**kwargs)
dask.dataframe.DataFrame.info(self,buf=None,verbose=False,memory_usage=False)
dask.dataframe.DataFrame.iterrows(self)
dask.dataframe.DataFrame.itertuples(self)
dask.dataframe.DataFrame.join(self,other,on=None,how='left',lsuffix='',rsuffix='',npartitions=None,shuffle=None)
dask.dataframe.DataFrame.memory_usage(self,index=True,deep=False)
dask.dataframe.DataFrame.merge(self,right,how='inner',on=None,left_on=None,right_on=None,left_index=False,right_index=False,suffixes=('_x','_y'),indicator=False,npartitions=None,shuffle=None)
dask.dataframe.DataFrame.ndim(self)
dask.dataframe.DataFrame.nlargest(self,n=5,columns=None,split_every=None)
dask.dataframe.DataFrame.nsmallest(self,n=5,columns=None,split_every=None)
dask.dataframe.DataFrame.pivot_table(self,index=None,columns=None,values=None,aggfunc='mean')
dask.dataframe.DataFrame.query(self,expr,**kwargs)
dask.dataframe.DataFrame.rename(self,index=None,columns=None)
dask.dataframe.DataFrame.round(self,decimals=0)
dask.dataframe.DataFrame.select_dtypes(self,include=None,exclude=None)
dask.dataframe.DataFrame.set_index(self,other,drop=True,sorted=False,npartitions=None,divisions=None,**kwargs)
dask.dataframe.DataFrame.to_bag(self,index=False)
dask.dataframe.DataFrame.to_html(self,max_rows=5)
dask.dataframe.DataFrame.to_records(self,index=False)
dask.dataframe.DataFrame.to_string(self,max_rows=5)
dask.dataframe.DataFrame.to_timestamp(self,freq=None,how='start',axis=0)
dask.dataframe.Index(Series)
dask.dataframe.Index.__array_wrap__(self,array,context=None)
dask.dataframe.Index.__dir__(self)
dask.dataframe.Index.__getattr__(self,key)
dask.dataframe.Index.count(self,split_every=False)
dask.dataframe.Index.head(self,n=5,compute=True)
dask.dataframe.Index.index(self)
dask.dataframe.Index.max(self,split_every=False)
dask.dataframe.Index.min(self,split_every=False)
dask.dataframe.Index.shift(self,periods=1,freq=None)
dask.dataframe.Series(_Frame)
dask.dataframe.Series.__array_wrap__(self,array,context=None)
dask.dataframe.Series.__dir__(self)
dask.dataframe.Series.__getitem__(self,key)
dask.dataframe.Series.__repr__(self)
dask.dataframe.Series._bind_comparison_method(cls,name,comparison)
dask.dataframe.Series._bind_operator_method(cls,name,op)
dask.dataframe.Series._get_numeric_data(self,how='any',subset=None)
dask.dataframe.Series._repartition_quantiles(self,npartitions,upsample=1.0)
dask.dataframe.Series._repr_data(self)
dask.dataframe.Series._validate_axis(cls,axis=0)
dask.dataframe.Series.align(self,other,join='outer',axis=None,fill_value=None)
dask.dataframe.Series.apply(self,func,convert_dtype=True,meta=no_default,args=(),**kwds)
dask.dataframe.Series.autocorr(self,lag=1,split_every=False)
dask.dataframe.Series.between(self,left,right,inclusive=True)
dask.dataframe.Series.cat(self)
dask.dataframe.Series.clip(self,lower=None,upper=None,out=None)
dask.dataframe.Series.clip_lower(self,threshold)
dask.dataframe.Series.clip_upper(self,threshold)
dask.dataframe.Series.combine(self,other,func,fill_value=None)
dask.dataframe.Series.combine_first(self,other)
dask.dataframe.Series.corr(self,other,method='pearson',min_periods=None,split_every=False)
dask.dataframe.Series.count(self,split_every=False)
dask.dataframe.Series.cov(self,other,min_periods=None,split_every=False)
dask.dataframe.Series.dropna(self)
dask.dataframe.Series.dt(self)
dask.dataframe.Series.dtype(self)
dask.dataframe.Series.groupby(self,by=None,**kwargs)
dask.dataframe.Series.isin(self,values)
dask.dataframe.Series.iteritems(self)
dask.dataframe.Series.map(self,arg,na_action=None,meta=no_default)
dask.dataframe.Series.memory_usage(self,index=True,deep=False)
dask.dataframe.Series.name(self)
dask.dataframe.Series.name(self,name)
dask.dataframe.Series.nbytes(self)
dask.dataframe.Series.ndim(self)
dask.dataframe.Series.nlargest(self,n=5,split_every=None)
dask.dataframe.Series.nsmallest(self,n=5,split_every=None)
dask.dataframe.Series.nunique(self,split_every=None)
dask.dataframe.Series.quantile(self,q=0.5)
dask.dataframe.Series.round(self,decimals=0)
dask.dataframe.Series.str(self)
dask.dataframe.Series.to_bag(self,index=False)
dask.dataframe.Series.to_frame(self,name=None)
dask.dataframe.Series.to_string(self,max_rows=5)
dask.dataframe.Series.to_timestamp(self,freq=None,how='start',axis=0)
dask.dataframe.Series.unique(self,split_every=None,split_out=1)
dask.dataframe.Series.value_counts(self,split_every=None,split_out=1)
dask.dataframe._Frame(self,dsk,name,meta,divisions)
dask.dataframe._Frame.__array__(self,dtype=None,**kwargs)
dask.dataframe._Frame.__array_wrap__(self,array,context=None)
dask.dataframe._Frame.__getstate__(self)
dask.dataframe._Frame.__len__(self)
dask.dataframe._Frame.__repr__(self)
dask.dataframe._Frame.__setstate__(self,state)
dask.dataframe._Frame._args(self)
dask.dataframe._Frame._bind_operator_method(cls,name,op)
dask.dataframe._Frame._constructor(self)
dask.dataframe._Frame._cum_agg(self,token,chunk,aggregate,axis,skipna=True,chunk_kwargs=None)
dask.dataframe._Frame._elemwise(self)
dask.dataframe._Frame._get_binary_operator(cls,op,inv=False)
dask.dataframe._Frame._get_unary_operator(cls,op)
dask.dataframe._Frame._keys(self)
dask.dataframe._Frame._meta_nonempty(self)
dask.dataframe._Frame._reduction_agg(self,name,axis=None,skipna=True,split_every=False)
dask.dataframe._Frame._repr_data(self)
dask.dataframe._Frame._repr_divisions(self)
dask.dataframe._Frame.abs(self)
dask.dataframe._Frame.align(self,other,join='outer',axis=None,fill_value=None)
dask.dataframe._Frame.all(self,axis=None,skipna=True,split_every=False)
dask.dataframe._Frame.any(self,axis=None,skipna=True,split_every=False)
dask.dataframe._Frame.append(self,other)
dask.dataframe._Frame.astype(self,dtype)
dask.dataframe._Frame.bfill(self,axis=None,limit=None)
dask.dataframe._Frame.clear_divisions(self)
dask.dataframe._Frame.combine(self,other,func,fill_value=None,overwrite=True)
dask.dataframe._Frame.combine_first(self,other)
dask.dataframe._Frame.copy(self)
dask.dataframe._Frame.count(self,axis=None,split_every=False)
dask.dataframe._Frame.cummax(self,axis=None,skipna=True)
dask.dataframe._Frame.cummin(self,axis=None,skipna=True)
dask.dataframe._Frame.cumprod(self,axis=None,skipna=True)
dask.dataframe._Frame.cumsum(self,axis=None,skipna=True)
dask.dataframe._Frame.describe(self,split_every=False)
dask.dataframe._Frame.diff(self,periods=1,axis=0)
dask.dataframe._Frame.drop_duplicates(self,split_every=None,split_out=1,**kwargs)
dask.dataframe._Frame.ffill(self,axis=None,limit=None)
dask.dataframe._Frame.fillna(self,value=None,method=None,limit=None,axis=None)
dask.dataframe._Frame.first(self,offset)
dask.dataframe._Frame.get_partition(self,n)
dask.dataframe._Frame.head(self,n=5,npartitions=1,compute=True)
dask.dataframe._Frame.idxmax(self,axis=None,skipna=True,split_every=False)
dask.dataframe._Frame.idxmin(self,axis=None,skipna=True,split_every=False)
dask.dataframe._Frame.index(self)
dask.dataframe._Frame.isin(self,values)
dask.dataframe._Frame.isnull(self)
dask.dataframe._Frame.known_divisions(self)
dask.dataframe._Frame.last(self,offset)
dask.dataframe._Frame.loc(self)
dask.dataframe._Frame.map_overlap(self,func,before,after,*args,**kwargs)
dask.dataframe._Frame.map_partitions(self,func,*args,**kwargs)
dask.dataframe._Frame.mask(self,cond,other=np.nan)
dask.dataframe._Frame.max(self,axis=None,skipna=True,split_every=False)
dask.dataframe._Frame.mean(self,axis=None,skipna=True,split_every=False)
dask.dataframe._Frame.min(self,axis=None,skipna=True,split_every=False)
dask.dataframe._Frame.notnull(self)
dask.dataframe._Frame.npartitions(self)
dask.dataframe._Frame.nunique_approx(self,split_every=None)
dask.dataframe._Frame.pipe(self,func,*args,**kwargs)
dask.dataframe._Frame.prod(self,axis=None,skipna=True,split_every=False)
dask.dataframe._Frame.quantile(self,q=0.5,axis=0)
dask.dataframe._Frame.random_split(self,frac,random_state=None)
dask.dataframe._Frame.reduction(self,chunk,aggregate=None,combine=None,meta=no_default,token=None,split_every=None,chunk_kwargs=None,aggregate_kwargs=None,combine_kwargs=None,**kwargs)
dask.dataframe._Frame.repartition(self,divisions=None,npartitions=None,freq=None,force=False)
dask.dataframe._Frame.resample(self,rule,how=None,closed=None,label=None)
dask.dataframe._Frame.reset_index(self,drop=False)
dask.dataframe._Frame.rolling(self,window,min_periods=None,freq=None,center=False,win_type=None,axis=0)
dask.dataframe._Frame.sample(self,frac,replace=False,random_state=None)
dask.dataframe._Frame.sem(self,axis=None,skipna=None,ddof=1,split_every=False)
dask.dataframe._Frame.shift(self,periods=1,freq=None,axis=0)
dask.dataframe._Frame.size(self)
dask.dataframe._Frame.std(self,axis=None,skipna=True,ddof=1,split_every=False)
dask.dataframe._Frame.sum(self,axis=None,skipna=True,split_every=False)
dask.dataframe._Frame.tail(self,n=5,compute=True)
dask.dataframe._Frame.to_csv(self,filename,**kwargs)
dask.dataframe._Frame.to_delayed(self)
dask.dataframe._Frame.to_hdf(self,path_or_buf,key,mode='a',append=False,get=None,**kwargs)
dask.dataframe._Frame.to_parquet(self,path,*args,**kwargs)
dask.dataframe._Frame.values(self)
dask.dataframe._Frame.var(self,axis=None,skipna=True,ddof=1,split_every=False)
dask.dataframe._Frame.where(self,cond,other=np.nan)
dask.dataframe.core.DataFrame(_Frame)
dask.dataframe.core.DataFrame.__array_wrap__(self,array,context=None)
dask.dataframe.core.DataFrame.__delitem__(self,key)
dask.dataframe.core.DataFrame.__dir__(self)
dask.dataframe.core.DataFrame.__getattr__(self,key)
dask.dataframe.core.DataFrame.__getitem__(self,key)
dask.dataframe.core.DataFrame.__setattr__(self,key,value)
dask.dataframe.core.DataFrame.__setitem__(self,key,value)
dask.dataframe.core.DataFrame._bind_comparison_method(cls,name,comparison)
dask.dataframe.core.DataFrame._bind_operator_method(cls,name,op)
dask.dataframe.core.DataFrame._contains_index_name(self,columns_or_index)
dask.dataframe.core.DataFrame._get_numeric_data(self,how='any',subset=None)
dask.dataframe.core.DataFrame._is_column_label(self,c)
dask.dataframe.core.DataFrame._is_index_label(self,i)
dask.dataframe.core.DataFrame._repr_data(self)
dask.dataframe.core.DataFrame._repr_html_(self)
dask.dataframe.core.DataFrame._select_columns_or_index(self,columns_or_index)
dask.dataframe.core.DataFrame._validate_axis(cls,axis=0)
dask.dataframe.core.DataFrame.append(self,other)
dask.dataframe.core.DataFrame.apply(self,func,axis=0,args=(),meta=no_default,**kwds)
dask.dataframe.core.DataFrame.applymap(self,func,meta='__no_default__')
dask.dataframe.core.DataFrame.assign(self,**kwargs)
dask.dataframe.core.DataFrame.categorize(self,columns=None,index=None,split_every=None,**kwargs)
dask.dataframe.core.DataFrame.clip(self,lower=None,upper=None,out=None)
dask.dataframe.core.DataFrame.clip_lower(self,threshold)
dask.dataframe.core.DataFrame.clip_upper(self,threshold)
dask.dataframe.core.DataFrame.columns(self)
dask.dataframe.core.DataFrame.columns(self,columns)
dask.dataframe.core.DataFrame.corr(self,method='pearson',min_periods=None,split_every=False)
dask.dataframe.core.DataFrame.cov(self,min_periods=None,split_every=False)
dask.dataframe.core.DataFrame.drop(self,labels,axis=0,errors='raise')
dask.dataframe.core.DataFrame.dropna(self,how='any',subset=None)
dask.dataframe.core.DataFrame.dtypes(self)
dask.dataframe.core.DataFrame.eval(self,expr,inplace=None,**kwargs)
dask.dataframe.core.DataFrame.get_dtype_counts(self)
dask.dataframe.core.DataFrame.get_ftype_counts(self)
dask.dataframe.core.DataFrame.groupby(self,by=None,**kwargs)
dask.dataframe.core.DataFrame.info(self,buf=None,verbose=False,memory_usage=False)
dask.dataframe.core.DataFrame.iterrows(self)
dask.dataframe.core.DataFrame.itertuples(self)
dask.dataframe.core.DataFrame.join(self,other,on=None,how='left',lsuffix='',rsuffix='',npartitions=None,shuffle=None)
dask.dataframe.core.DataFrame.memory_usage(self,index=True,deep=False)
dask.dataframe.core.DataFrame.merge(self,right,how='inner',on=None,left_on=None,right_on=None,left_index=False,right_index=False,suffixes=('_x','_y'),indicator=False,npartitions=None,shuffle=None)
dask.dataframe.core.DataFrame.ndim(self)
dask.dataframe.core.DataFrame.nlargest(self,n=5,columns=None,split_every=None)
dask.dataframe.core.DataFrame.nsmallest(self,n=5,columns=None,split_every=None)
dask.dataframe.core.DataFrame.pivot_table(self,index=None,columns=None,values=None,aggfunc='mean')
dask.dataframe.core.DataFrame.query(self,expr,**kwargs)
dask.dataframe.core.DataFrame.rename(self,index=None,columns=None)
dask.dataframe.core.DataFrame.round(self,decimals=0)
dask.dataframe.core.DataFrame.select_dtypes(self,include=None,exclude=None)
dask.dataframe.core.DataFrame.set_index(self,other,drop=True,sorted=False,npartitions=None,divisions=None,**kwargs)
dask.dataframe.core.DataFrame.to_bag(self,index=False)
dask.dataframe.core.DataFrame.to_html(self,max_rows=5)
dask.dataframe.core.DataFrame.to_records(self,index=False)
dask.dataframe.core.DataFrame.to_string(self,max_rows=5)
dask.dataframe.core.DataFrame.to_timestamp(self,freq=None,how='start',axis=0)
dask.dataframe.core.Index(Series)
dask.dataframe.core.Index.__array_wrap__(self,array,context=None)
dask.dataframe.core.Index.__dir__(self)
dask.dataframe.core.Index.__getattr__(self,key)
dask.dataframe.core.Index.count(self,split_every=False)
dask.dataframe.core.Index.head(self,n=5,compute=True)
dask.dataframe.core.Index.index(self)
dask.dataframe.core.Index.max(self,split_every=False)
dask.dataframe.core.Index.min(self,split_every=False)
dask.dataframe.core.Index.shift(self,periods=1,freq=None)
dask.dataframe.core.Scalar(self,dsk,name,meta,divisions=None)
dask.dataframe.core.Scalar.__array__(self)
dask.dataframe.core.Scalar.__dir__(self)
dask.dataframe.core.Scalar.__getstate__(self)
dask.dataframe.core.Scalar.__init__(self,dsk,name,meta,divisions=None)
dask.dataframe.core.Scalar.__repr__(self)
dask.dataframe.core.Scalar.__setstate__(self,state)
dask.dataframe.core.Scalar._args(self)
dask.dataframe.core.Scalar._get_binary_operator(cls,op,inv=False)
dask.dataframe.core.Scalar._get_unary_operator(cls,op)
dask.dataframe.core.Scalar._keys(self)
dask.dataframe.core.Scalar._meta_nonempty(self)
dask.dataframe.core.Scalar.divisions(self)
dask.dataframe.core.Scalar.dtype(self)
dask.dataframe.core.Scalar.key(self)
dask.dataframe.core.Series(_Frame)
dask.dataframe.core.Series.__array_wrap__(self,array,context=None)
dask.dataframe.core.Series.__dir__(self)
dask.dataframe.core.Series.__getitem__(self,key)
dask.dataframe.core.Series.__repr__(self)
dask.dataframe.core.Series._bind_comparison_method(cls,name,comparison)
dask.dataframe.core.Series._bind_operator_method(cls,name,op)
dask.dataframe.core.Series._get_numeric_data(self,how='any',subset=None)
dask.dataframe.core.Series._repartition_quantiles(self,npartitions,upsample=1.0)
dask.dataframe.core.Series._repr_data(self)
dask.dataframe.core.Series._validate_axis(cls,axis=0)
dask.dataframe.core.Series.align(self,other,join='outer',axis=None,fill_value=None)
dask.dataframe.core.Series.apply(self,func,convert_dtype=True,meta=no_default,args=(),**kwds)
dask.dataframe.core.Series.autocorr(self,lag=1,split_every=False)
dask.dataframe.core.Series.between(self,left,right,inclusive=True)
dask.dataframe.core.Series.cat(self)
dask.dataframe.core.Series.clip(self,lower=None,upper=None,out=None)
dask.dataframe.core.Series.clip_lower(self,threshold)
dask.dataframe.core.Series.clip_upper(self,threshold)
dask.dataframe.core.Series.combine(self,other,func,fill_value=None)
dask.dataframe.core.Series.combine_first(self,other)
dask.dataframe.core.Series.corr(self,other,method='pearson',min_periods=None,split_every=False)
dask.dataframe.core.Series.count(self,split_every=False)
dask.dataframe.core.Series.cov(self,other,min_periods=None,split_every=False)
dask.dataframe.core.Series.dropna(self)
dask.dataframe.core.Series.dt(self)
dask.dataframe.core.Series.dtype(self)
dask.dataframe.core.Series.groupby(self,by=None,**kwargs)
dask.dataframe.core.Series.isin(self,values)
dask.dataframe.core.Series.iteritems(self)
dask.dataframe.core.Series.map(self,arg,na_action=None,meta=no_default)
dask.dataframe.core.Series.memory_usage(self,index=True,deep=False)
dask.dataframe.core.Series.name(self)
dask.dataframe.core.Series.name(self,name)
dask.dataframe.core.Series.nbytes(self)
dask.dataframe.core.Series.ndim(self)
dask.dataframe.core.Series.nlargest(self,n=5,split_every=None)
dask.dataframe.core.Series.nsmallest(self,n=5,split_every=None)
dask.dataframe.core.Series.nunique(self,split_every=None)
dask.dataframe.core.Series.quantile(self,q=0.5)
dask.dataframe.core.Series.round(self,decimals=0)
dask.dataframe.core.Series.str(self)
dask.dataframe.core.Series.to_bag(self,index=False)
dask.dataframe.core.Series.to_frame(self,name=None)
dask.dataframe.core.Series.to_string(self,max_rows=5)
dask.dataframe.core.Series.to_timestamp(self,freq=None,how='start',axis=0)
dask.dataframe.core.Series.unique(self,split_every=None,split_out=1)
dask.dataframe.core.Series.value_counts(self,split_every=None,split_out=1)
dask.dataframe.core._Frame(self,dsk,name,meta,divisions)
dask.dataframe.core._Frame.__array__(self,dtype=None,**kwargs)
dask.dataframe.core._Frame.__array_wrap__(self,array,context=None)
dask.dataframe.core._Frame.__getstate__(self)
dask.dataframe.core._Frame.__init__(self,dsk,name,meta,divisions)
dask.dataframe.core._Frame.__len__(self)
dask.dataframe.core._Frame.__repr__(self)
dask.dataframe.core._Frame.__setstate__(self,state)
dask.dataframe.core._Frame._args(self)
dask.dataframe.core._Frame._bind_operator_method(cls,name,op)
dask.dataframe.core._Frame._constructor(self)
dask.dataframe.core._Frame._cum_agg(self,token,chunk,aggregate,axis,skipna=True,chunk_kwargs=None)
dask.dataframe.core._Frame._elemwise(self)
dask.dataframe.core._Frame._get_binary_operator(cls,op,inv=False)
dask.dataframe.core._Frame._get_unary_operator(cls,op)
dask.dataframe.core._Frame._keys(self)
dask.dataframe.core._Frame._meta_nonempty(self)
dask.dataframe.core._Frame._reduction_agg(self,name,axis=None,skipna=True,split_every=False)
dask.dataframe.core._Frame._repr_data(self)
dask.dataframe.core._Frame._repr_divisions(self)
dask.dataframe.core._Frame.abs(self)
dask.dataframe.core._Frame.align(self,other,join='outer',axis=None,fill_value=None)
dask.dataframe.core._Frame.all(self,axis=None,skipna=True,split_every=False)
dask.dataframe.core._Frame.any(self,axis=None,skipna=True,split_every=False)
dask.dataframe.core._Frame.append(self,other)
dask.dataframe.core._Frame.astype(self,dtype)
dask.dataframe.core._Frame.bfill(self,axis=None,limit=None)
dask.dataframe.core._Frame.clear_divisions(self)
dask.dataframe.core._Frame.combine(self,other,func,fill_value=None,overwrite=True)
dask.dataframe.core._Frame.combine_first(self,other)
dask.dataframe.core._Frame.copy(self)
dask.dataframe.core._Frame.count(self,axis=None,split_every=False)
dask.dataframe.core._Frame.cummax(self,axis=None,skipna=True)
dask.dataframe.core._Frame.cummin(self,axis=None,skipna=True)
dask.dataframe.core._Frame.cumprod(self,axis=None,skipna=True)
dask.dataframe.core._Frame.cumsum(self,axis=None,skipna=True)
dask.dataframe.core._Frame.describe(self,split_every=False)
dask.dataframe.core._Frame.diff(self,periods=1,axis=0)
dask.dataframe.core._Frame.drop_duplicates(self,split_every=None,split_out=1,**kwargs)
dask.dataframe.core._Frame.ffill(self,axis=None,limit=None)
dask.dataframe.core._Frame.fillna(self,value=None,method=None,limit=None,axis=None)
dask.dataframe.core._Frame.first(self,offset)
dask.dataframe.core._Frame.get_partition(self,n)
dask.dataframe.core._Frame.head(self,n=5,npartitions=1,compute=True)
dask.dataframe.core._Frame.idxmax(self,axis=None,skipna=True,split_every=False)
dask.dataframe.core._Frame.idxmin(self,axis=None,skipna=True,split_every=False)
dask.dataframe.core._Frame.index(self)
dask.dataframe.core._Frame.isin(self,values)
dask.dataframe.core._Frame.isnull(self)
dask.dataframe.core._Frame.known_divisions(self)
dask.dataframe.core._Frame.last(self,offset)
dask.dataframe.core._Frame.loc(self)
dask.dataframe.core._Frame.map_overlap(self,func,before,after,*args,**kwargs)
dask.dataframe.core._Frame.map_partitions(self,func,*args,**kwargs)
dask.dataframe.core._Frame.mask(self,cond,other=np.nan)
dask.dataframe.core._Frame.max(self,axis=None,skipna=True,split_every=False)
dask.dataframe.core._Frame.mean(self,axis=None,skipna=True,split_every=False)
dask.dataframe.core._Frame.min(self,axis=None,skipna=True,split_every=False)
dask.dataframe.core._Frame.notnull(self)
dask.dataframe.core._Frame.npartitions(self)
dask.dataframe.core._Frame.nunique_approx(self,split_every=None)
dask.dataframe.core._Frame.pipe(self,func,*args,**kwargs)
dask.dataframe.core._Frame.prod(self,axis=None,skipna=True,split_every=False)
dask.dataframe.core._Frame.quantile(self,q=0.5,axis=0)
dask.dataframe.core._Frame.random_split(self,frac,random_state=None)
dask.dataframe.core._Frame.reduction(self,chunk,aggregate=None,combine=None,meta=no_default,token=None,split_every=None,chunk_kwargs=None,aggregate_kwargs=None,combine_kwargs=None,**kwargs)
dask.dataframe.core._Frame.repartition(self,divisions=None,npartitions=None,freq=None,force=False)
dask.dataframe.core._Frame.resample(self,rule,how=None,closed=None,label=None)
dask.dataframe.core._Frame.reset_index(self,drop=False)
dask.dataframe.core._Frame.rolling(self,window,min_periods=None,freq=None,center=False,win_type=None,axis=0)
dask.dataframe.core._Frame.sample(self,frac,replace=False,random_state=None)
dask.dataframe.core._Frame.sem(self,axis=None,skipna=None,ddof=1,split_every=False)
dask.dataframe.core._Frame.shift(self,periods=1,freq=None,axis=0)
dask.dataframe.core._Frame.size(self)
dask.dataframe.core._Frame.std(self,axis=None,skipna=True,ddof=1,split_every=False)
dask.dataframe.core._Frame.sum(self,axis=None,skipna=True,split_every=False)
dask.dataframe.core._Frame.tail(self,n=5,compute=True)
dask.dataframe.core._Frame.to_csv(self,filename,**kwargs)
dask.dataframe.core._Frame.to_delayed(self)
dask.dataframe.core._Frame.to_hdf(self,path_or_buf,key,mode='a',append=False,get=None,**kwargs)
dask.dataframe.core._Frame.to_parquet(self,path,*args,**kwargs)
dask.dataframe.core._Frame.values(self)
dask.dataframe.core._Frame.var(self,axis=None,skipna=True,ddof=1,split_every=False)
dask.dataframe.core._Frame.where(self,cond,other=np.nan)
dask.dataframe.core._concat(args)
dask.dataframe.core._emulate(func,*args,**kwargs)
dask.dataframe.core._extract_meta(x,nonempty=False)
dask.dataframe.core._get_return_type(meta)
dask.dataframe.core._maybe_from_pandas(dfs)
dask.dataframe.core._raise_if_object_series(x,funcname)
dask.dataframe.core._reduction_aggregate(x,aca_aggregate=None,**kwargs)
dask.dataframe.core._reduction_chunk(x,aca_chunk=None,**kwargs)
dask.dataframe.core._reduction_combine(x,aca_combine=None,**kwargs)
dask.dataframe.core._rename(columns,df)
dask.dataframe.core._rename_dask(df,names)
dask.dataframe.core._repr_data_series(s,index)
dask.dataframe.core._scalar_binary(op,self,other,inv=False)
dask.dataframe.core._take_last(a,skipna=True)
dask.dataframe.core.apply_and_enforce(func,args,kwargs,meta)
dask.dataframe.core.apply_concat_apply(args,chunk=None,aggregate=None,combine=None,meta=no_default,token=None,chunk_kwargs=None,aggregate_kwargs=None,combine_kwargs=None,split_every=None,split_out=None,split_out_setup=None,split_out_setup_kwargs=None,**kwargs)
dask.dataframe.core.check_divisions(divisions)
dask.dataframe.core.cov_corr(df,min_periods=None,corr=False,scalar=False,split_every=False)
dask.dataframe.core.cov_corr_agg(data,cols,min_periods=2,corr=False,scalar=False)
dask.dataframe.core.cov_corr_chunk(df,corr=False)
dask.dataframe.core.cov_corr_combine(data,corr=False)
dask.dataframe.core.elemwise(op,*args,**kwargs)
dask.dataframe.core.finalize(results)
dask.dataframe.core.hash_shard(df,nparts,split_out_setup=None,split_out_setup_kwargs=None)
dask.dataframe.core.idxmaxmin_agg(x,fn=None,skipna=True,scalar=False)
dask.dataframe.core.idxmaxmin_chunk(x,fn=None,skipna=True)
dask.dataframe.core.idxmaxmin_combine(x,fn=None,skipna=True)
dask.dataframe.core.idxmaxmin_row(x,fn=None,skipna=True)
dask.dataframe.core.is_broadcastable(dfs,s)
dask.dataframe.core.map_partitions(func,*args,**kwargs)
dask.dataframe.core.maybe_shift_divisions(df,periods,freq)
dask.dataframe.core.new_dd_object(dsk,_name,meta,divisions)
dask.dataframe.core.optimize(dsk,keys,**kwargs)
dask.dataframe.core.pd_split(df,p,random_state=None)
dask.dataframe.core.quantile(df,q)
dask.dataframe.core.repartition(df,divisions=None,force=False)
dask.dataframe.core.repartition_divisions(a,b,name,out1,out2,force=False)
dask.dataframe.core.repartition_freq(df,freq=None)
dask.dataframe.core.repartition_npartitions(df,npartitions)
dask.dataframe.core.safe_head(df,n)
dask.dataframe.core.split_evenly(df,k)
dask.dataframe.core.split_out_on_cols(df,cols=None)
dask.dataframe.core.split_out_on_index(df)
dask.dataframe.core.to_datetime(arg,**kwargs)
dask.dataframe.core.to_delayed(df)
dask.dataframe.core.to_timedelta(arg,unit='ns',errors='raise')
dask.dataframe.map_partitions(func,*args,**kwargs)
dask.dataframe.repartition(df,divisions=None,force=False)
dask.dataframe.repartition_divisions(a,b,name,out1,out2,force=False)
dask.dataframe.repartition_freq(df,freq=None)
dask.dataframe.repartition_npartitions(df,npartitions)
dask.dataframe.to_datetime(arg,**kwargs)
dask.dataframe.to_delayed(df)
dask.dataframe.to_timedelta(arg,unit='ns',errors='raise')


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/dataframe/hashing.py----------------------------------------
A:dask.dataframe.hashing.h->pandas.Series(h, index=obj.index, dtype='uint64')
A:dask.dataframe.hashing.cols->obj.iteritems()
A:dask.dataframe.hashing.hashed->hash_array(c.categories.values, encoding, hash_key, categorize=False)
A:dask.dataframe.hashing.mask->c.isnull()
A:dask.dataframe.hashing.result->numpy.zeros(len(mask), dtype='uint64')
A:dask.dataframe.hashing.vals->hash_object_array(vals, hash_key, encoding)
A:dask.dataframe.hashing.(codes, categories)->pandas.factorize(vals, sort=False)
A:dask.dataframe.hashing.cat->pandas.Categorical(codes, pd.Index(categories), ordered=False, fastpath=True)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/dataframe/rolling.py----------------------------------------
A:dask.dataframe.rolling.combined->pandas.concat(parts)
A:dask.dataframe.rolling.out->func(combined, *args, **kwargs)
A:dask.dataframe.rolling.before->pandas.Timedelta(self.window)
A:dask.dataframe.rolling.after->len(next_part)
A:dask.dataframe.rolling.func_name->kwargs.pop('token')
A:dask.dataframe.rolling.token->tokenize(func, df, before, after, *args, **kwargs)
A:dask.dataframe.rolling.meta->pandas_rolling_method(self.obj._meta_nonempty, rolling_kwargs, method_name, *args, **kwargs)
A:dask.dataframe.rolling.name->'{0}-{1}'.format(func_name, token)
A:dask.dataframe.rolling.dsk->df.dask.copy()
A:dask.dataframe.rolling.rolling->df.rolling(**rolling_kwargs)
A:dask.dataframe.rolling.rolling_count->wrap_rolling(pd.rolling_count, 'count')
A:dask.dataframe.rolling.rolling_sum->wrap_rolling(pd.rolling_sum, 'sum')
A:dask.dataframe.rolling.rolling_mean->wrap_rolling(pd.rolling_mean, 'mean')
A:dask.dataframe.rolling.rolling_median->wrap_rolling(pd.rolling_median, 'median')
A:dask.dataframe.rolling.rolling_min->wrap_rolling(pd.rolling_min, 'min')
A:dask.dataframe.rolling.rolling_max->wrap_rolling(pd.rolling_max, 'max')
A:dask.dataframe.rolling.rolling_std->wrap_rolling(pd.rolling_std, 'std')
A:dask.dataframe.rolling.rolling_var->wrap_rolling(pd.rolling_var, 'var')
A:dask.dataframe.rolling.rolling_skew->wrap_rolling(pd.rolling_skew, 'skew')
A:dask.dataframe.rolling.rolling_kurt->wrap_rolling(pd.rolling_kurt, 'kurt')
A:dask.dataframe.rolling.rolling_quantile->wrap_rolling(pd.rolling_quantile, 'quantile')
A:dask.dataframe.rolling.rolling_apply->wrap_rolling(pd.rolling_apply, 'apply')
A:dask.dataframe.rolling.pd_roll->obj._meta.rolling(**self._rolling_kwargs())
A:dask.dataframe.rolling.rolling_kwargs->self._rolling_kwargs()
dask.dataframe.rolling.Rolling(self,obj,window=None,min_periods=None,freq=None,center=False,win_type=None,axis=0)
dask.dataframe.rolling.Rolling.__init__(self,obj,window=None,min_periods=None,freq=None,center=False,win_type=None,axis=0)
dask.dataframe.rolling.Rolling.__repr__(self)
dask.dataframe.rolling.Rolling._call_method(self,method_name,*args,**kwargs)
dask.dataframe.rolling.Rolling._has_single_partition(self)
dask.dataframe.rolling.Rolling._rolling_kwargs(self)
dask.dataframe.rolling.Rolling.apply(self,func,args=(),kwargs={})
dask.dataframe.rolling.Rolling.count(self)
dask.dataframe.rolling.Rolling.kurt(self)
dask.dataframe.rolling.Rolling.max(self)
dask.dataframe.rolling.Rolling.mean(self)
dask.dataframe.rolling.Rolling.median(self)
dask.dataframe.rolling.Rolling.min(self)
dask.dataframe.rolling.Rolling.quantile(self,quantile)
dask.dataframe.rolling.Rolling.skew(self)
dask.dataframe.rolling.Rolling.std(self,ddof=1)
dask.dataframe.rolling.Rolling.sum(self)
dask.dataframe.rolling.Rolling.var(self,ddof=1)
dask.dataframe.rolling._head_timedelta(current,next_,after)
dask.dataframe.rolling._tail_timedelta(prev,current,before)
dask.dataframe.rolling.map_overlap(func,df,before,after,*args,**kwargs)
dask.dataframe.rolling.overlap_chunk(func,prev_part,current_part,next_part,before,after,args,kwargs)
dask.dataframe.rolling.pandas_rolling_method(df,rolling_kwargs,name,*args,**kwargs)
dask.dataframe.rolling.rolling_window(arg,window,**kwargs)
dask.dataframe.rolling.wrap_rolling(func,method_name)
dask.dataframe.rolling_window(arg,window,**kwargs)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/dataframe/partitionquantiles.py----------------------------------------
A:dask.dataframe.partitionquantiles.random_state->numpy.random.RandomState(state)
A:dask.dataframe.partitionquantiles.q_fixed->numpy.linspace(0, 100, num_fixed)
A:dask.dataframe.partitionquantiles.qs->numpy.linspace(0, 1, npartitions + 1)
A:dask.dataframe.partitionquantiles.group_size->int(math.log(N))
A:dask.dataframe.partitionquantiles.prev_width->len(keys)
A:dask.dataframe.partitionquantiles.prev_keys->iter(keys)
A:dask.dataframe.partitionquantiles.width->tree_width(prev_width)
A:dask.dataframe.partitionquantiles.groups->tree_groups(prev_width, width)
A:dask.dataframe.partitionquantiles.diff->numpy.ediff1d(qs, 0.0, 0.0)
A:dask.dataframe.partitionquantiles.it->merge_sorted(*[zip(x, y) for (x, y) in vals_and_weights])
A:dask.dataframe.partitionquantiles.(val, weight)(prev_val, prev_weight)->next(it)
A:dask.dataframe.partitionquantiles.vals->numpy.round(vals).astype(data.dtype)
A:dask.dataframe.partitionquantiles.weights->numpy.array(weights)
A:dask.dataframe.partitionquantiles.q_weights->numpy.cumsum(trimmed_weights)
A:dask.dataframe.partitionquantiles.q_target->numpy.linspace(0, q_weights[-1], trimmed_npartitions + 1)
A:dask.dataframe.partitionquantiles.rv->rv.astype(dtype).astype(dtype)
A:dask.dataframe.partitionquantiles.duplicated_index->numpy.linspace(0, len(vals) - 1, npartitions - len(vals) + 1, dtype=int)
A:dask.dataframe.partitionquantiles.left->numpy.searchsorted(q_weights, q_target, side='left')
A:dask.dataframe.partitionquantiles.lower->numpy.minimum(left, right)
A:dask.dataframe.partitionquantiles.length->len(df)
A:dask.dataframe.partitionquantiles.vals_and_weights->percentiles_to_weights(qs, vals, length)
A:dask.dataframe.partitionquantiles.token->tokenize(df, qs, upsample)
A:dask.dataframe.partitionquantiles.state_data->random_state_data(df.npartitions, random_state)
A:dask.dataframe.partitionquantiles.df_keys->df._keys()
A:dask.dataframe.partitionquantiles.merge_dsk->create_merge_tree(merge_and_compress_summaries, sorted(val_dsk), name2)
A:dask.dataframe.partitionquantiles.merged_key->max(merge_dsk)
A:dask.dataframe.partitionquantiles.dsk->merge(df.dask, dtype_dsk, val_dsk, merge_dsk, last_dsk)
dask.dataframe.partitionquantiles.create_merge_tree(func,keys,token)
dask.dataframe.partitionquantiles.dtype_info(df)
dask.dataframe.partitionquantiles.merge_and_compress_summaries(vals_and_weights)
dask.dataframe.partitionquantiles.partition_quantiles(df,npartitions,upsample=1.0,random_state=None)
dask.dataframe.partitionquantiles.percentiles_summary(df,num_old,num_new,upsample,state)
dask.dataframe.partitionquantiles.percentiles_to_weights(qs,vals,length)
dask.dataframe.partitionquantiles.process_val_weights(vals_and_weights,npartitions,dtype_info)
dask.dataframe.partitionquantiles.sample_percentiles(num_old,num_new,chunk_length,upsample=1.0,random_state=None)
dask.dataframe.partitionquantiles.tree_groups(N,num_groups)
dask.dataframe.partitionquantiles.tree_width(N,to_binary=False)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/dataframe/multi.py----------------------------------------
A:dask.dataframe.multi._is_broadcastable->partial(is_broadcastable, args)
A:dask.dataframe.multi.divisions->tuple(divisions[min(present):max(present) + 2])
A:dask.dataframe.multi.result->list()
A:dask.dataframe.multi.L->list()
A:dask.dataframe.multi.dfs2->iter(align_partitions(*dfs)[0])
A:dask.dataframe.multi.parts->tuple(parts[min(present):max(present) + 1])
A:dask.dataframe.multi.((lhs, rhs), divisions, parts)->align_partitions(lhs, rhs)
A:dask.dataframe.multi.(divisions, parts)->require(divisions, parts, required[how])
A:dask.dataframe.multi.dsk->dict((((name, i), (methods.concat, part, axis, join)) for (i, part) in enumerate(parts2)))
A:dask.dataframe.multi.meta->methods.concat([df._meta for df in dfs], join=join)
A:dask.dataframe.multi.npartitions->max(lhs.npartitions, rhs.npartitions)
A:dask.dataframe.multi.lhs2->shuffle_func(lhs, left_on, npartitions=npartitions, shuffle=shuffle)
A:dask.dataframe.multi.rhs2->shuffle_func(rhs, right_on, npartitions=npartitions, shuffle=shuffle)
A:dask.dataframe.multi.token->tokenize(lhs2, left_on, rhs2, right_on, left_index, right_index, how, npartitions, suffixes, shuffle, indicator)
A:dask.dataframe.multi.left_key->first(left._keys())
A:dask.dataframe.multi.right_key->first(right._keys())
A:dask.dataframe.multi.left->rearrange_by_divisions(left, left_on, right.divisions, max_branch, shuffle=shuffle)
A:dask.dataframe.multi.right->right.clear_divisions().clear_divisions()
A:dask.dataframe.multi.(dfs2, divisions, parts)->align_partitions(*dfs)
A:dask.dataframe.multi.empty->strip_unknown_categories(meta)
A:dask.dataframe.multi.name->'concat-{0}'.format(tokenize(*dfs))
A:dask.dataframe.multi.axis->core.DataFrame._validate_axis(axis)
A:dask.dataframe.multi.dfs->_maybe_from_pandas(dfs)
dask.dataframe.concat(dfs,axis=0,join='outer',interleave_partitions=False)
dask.dataframe.concat_and_check(dfs)
dask.dataframe.concat_indexed_dataframes(dfs,axis=0,join='outer')
dask.dataframe.concat_unindexed_dataframes(dfs)
dask.dataframe.merge(left,right,how='inner',on=None,left_on=None,right_on=None,left_index=False,right_index=False,suffixes=('_x','_y'),indicator=False,npartitions=None,shuffle=None,max_branch=None)
dask.dataframe.merge_indexed_dataframes(lhs,rhs,how='left',lsuffix='',rsuffix='',indicator=False)
dask.dataframe.multi._maybe_align_partitions(args)
dask.dataframe.multi.align_partitions(*dfs)
dask.dataframe.multi.concat(dfs,axis=0,join='outer',interleave_partitions=False)
dask.dataframe.multi.concat_and_check(dfs)
dask.dataframe.multi.concat_indexed_dataframes(dfs,axis=0,join='outer')
dask.dataframe.multi.concat_unindexed_dataframes(dfs)
dask.dataframe.multi.hash_join(lhs,left_on,rhs,right_on,how='inner',npartitions=None,suffixes=('_x','_y'),shuffle=None,indicator=False)
dask.dataframe.multi.merge(left,right,how='inner',on=None,left_on=None,right_on=None,left_index=False,right_index=False,suffixes=('_x','_y'),indicator=False,npartitions=None,shuffle=None,max_branch=None)
dask.dataframe.multi.merge_indexed_dataframes(lhs,rhs,how='left',lsuffix='',rsuffix='',indicator=False)
dask.dataframe.multi.require(divisions,parts,required=None)
dask.dataframe.multi.single_partition_join(left,right,**kwargs)
dask.dataframe.multi.stack_partitions(dfs,divisions,join='outer')


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/dataframe/optimize.py----------------------------------------
A:dask.dataframe.optimize.(dsk, dependencies)->fuse(dsk, keys, dependencies=dependencies, ave_width=_globals.get('fuse_ave_width', 0))
A:dask.dataframe.optimize.dsk->fuse_getitem(dsk, _read_parquet_row_group, 4)
A:dask.dataframe.optimize.(dsk, _)->cull(dsk, keys)
dask.dataframe.optimize(dsk,keys,**kwargs)
dask.dataframe.optimize.optimize(dsk,keys,**kwargs)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/dataframe/methods.py----------------------------------------
A:dask.dataframe.methods.right_index->result.index.get_slice_bound(stop, 'left', kind)
A:dask.dataframe.methods.left_index->result.index.get_slice_bound(start, 'right', kind)
A:dask.dataframe.methods.part1->typ([count, mean, std, min], index=['count', 'mean', 'std', 'min'])
A:dask.dataframe.methods.part3->typ([max], index=['max'])
A:dask.dataframe.methods.kwargs->dict(partition(2, pairs))
A:dask.dataframe.methods.rs->numpy.random.RandomState(state)
A:dask.dataframe.methods.df->df.drop(columns, axis=1).drop(columns, axis=1)
A:dask.dataframe.methods.df.columns->df.drop(columns, axis=1).drop(columns, axis=1).columns.astype(dtype)
A:dask.dataframe.methods.out->pandas.concat(dfs2, join=join)
A:dask.dataframe.methods.new_tuples->numpy.concatenate(to_concat)
A:dask.dataframe.methods.ind->concat([df.index for df in dfs2])
A:dask.dataframe.methods.cat_mask->pandas.concat([(df.dtypes == 'category').to_frame().T for df in dfs3], join=join).any()
A:dask.dataframe.methods.sample->df.drop(columns, axis=1).drop(columns, axis=1).get(col)
A:dask.dataframe.methods.codes->numpy.full(len(df), -1, dtype='i8')
A:dask.dataframe.methods.data->pandas.Categorical.from_codes(codes, sample.cat.categories, sample.cat.ordered)
A:dask.dataframe.methods.out[col]->union_categoricals(parts)
dask.dataframe.methods.assign(df,*pairs)
dask.dataframe.methods.boundary_slice(df,start,stop,right_boundary=True,left_boundary=True,kind='loc')
dask.dataframe.methods.concat(dfs,axis=0,join='outer',uniform=False)
dask.dataframe.methods.cummax_aggregate(x,y)
dask.dataframe.methods.cummin_aggregate(x,y)
dask.dataframe.methods.describe_aggregate(values)
dask.dataframe.methods.drop_columns(df,columns,dtype)
dask.dataframe.methods.fillna_check(df,method,check=True)
dask.dataframe.methods.index_count(x)
dask.dataframe.methods.loc(df,iindexer,cindexer=None)
dask.dataframe.methods.mean_aggregate(s,n)
dask.dataframe.methods.merge(left,right,how,left_on,right_on,left_index,right_index,indicator,suffixes,default_left,default_right)
dask.dataframe.methods.nbytes(x)
dask.dataframe.methods.pivot_agg(df)
dask.dataframe.methods.pivot_count(df,index,columns,values)
dask.dataframe.methods.pivot_sum(df,index,columns,values)
dask.dataframe.methods.sample(df,state,frac,replace)
dask.dataframe.methods.size(x)
dask.dataframe.methods.try_loc(df,iindexer,cindexer=None)
dask.dataframe.methods.unique(x,series_name=None)
dask.dataframe.methods.value_counts_aggregate(x)
dask.dataframe.methods.value_counts_combine(x)
dask.dataframe.methods.var_aggregate(x2,x,n,ddof)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/dataframe/shuffle.py----------------------------------------
A:dask.dataframe.shuffle.npartitions->min(npartitions, df.npartitions)
A:dask.dataframe.shuffle.divisions->compute_divisions(result, **kwargs)
A:dask.dataframe.shuffle.parts->df.to_delayed()
A:dask.dataframe.shuffle.iparts->index2.to_delayed()
A:dask.dataframe.shuffle.(divisions, sizes, mins, maxes)->base.compute(divisions, sizes, mins, maxes)
A:dask.dataframe.shuffle.total->sum(sizes)
A:dask.dataframe.shuffle.n->len(divisions)
A:dask.dataframe.shuffle.indexes->numpy.linspace(0, n - 1, npartitions + 1).astype(int)
A:dask.dataframe.shuffle.mins->df.index.map_partitions(M.min, meta=df.index)
A:dask.dataframe.shuffle.maxes->df.index.map_partitions(M.max, meta=df.index)
A:dask.dataframe.shuffle.result->map_partitions(M.set_index, df, index, drop=drop, meta=meta)
A:dask.dataframe.shuffle.partitions->df[column].map_partitions(set_partitions_pre, divisions=divisions, meta=pd.Series([0]))
A:dask.dataframe.shuffle.df2->df.drop('_partitions', axis=1).set_index('_index', drop=True)
A:dask.dataframe.shuffle.df3->DataFrame(merge(df2.dask, dsk), 'repartition-get-' + token, df2, [None] * (npartitions + 1))
A:dask.dataframe.shuffle.df4->DataFrame(merge(df2.dask, dsk), 'repartition-get-' + token, df2, [None] * (npartitions + 1)).map_partitions(drop_columns, '_partitions', df.columns.dtype)
A:dask.dataframe.shuffle.file->partd.File()
A:dask.dataframe.shuffle.token->tokenize(df2, npartitions)
A:dask.dataframe.shuffle.dsk->merge(df.dask, start, end, *groups + splits + joins)
A:dask.dataframe.shuffle.(pp, values)->(_globals.get('get') or DataFrame._get)(dsk, keys)
A:dask.dataframe.shuffle.stages->int(math.ceil(math.log(n) / math.log(max_branch)))
A:dask.dataframe.shuffle.k->int(math.ceil(n ** (1 / stages)))
A:dask.dataframe.shuffle.start->dict(((('shuffle-join-' + token, 0, inp), (df._name, i) if i < df.npartitions else df._meta) for (i, inp) in enumerate(inputs)))
A:dask.dataframe.shuffle.group->dict(((('shuffle-group-' + token, stage, inp), (shuffle_group, ('shuffle-join-' + token, stage - 1, inp), column, stage - 1, k, n)) for inp in inputs))
A:dask.dataframe.shuffle.split->dict(((('shuffle-split-' + token, stage, i, inp), (getitem, ('shuffle-group-' + token, stage, inp), i)) for i in range(k) for inp in inputs))
A:dask.dataframe.shuffle.join->dict(((('shuffle-join-' + token, stage, inp), (_concat, [('shuffle-split-' + token, stage, inp[stage - 1], insert(inp, stage - 1, j)) for j in range(k)])) for inp in inputs))
A:dask.dataframe.shuffle.end->dict(((('shuffle-' + token, i), ('shuffle-join-' + token, stages, inp)) for (i, inp) in enumerate(inputs)))
A:dask.dataframe.shuffle.res->p.get(part)
A:dask.dataframe.shuffle.ind->hash_pandas_object(df[col], index=False)
A:dask.dataframe.shuffle.(indexer, locations)->groupsort_indexer(c.astype(np.int64), k)
A:dask.dataframe.shuffle.locations->locations.cumsum().cumsum()
A:dask.dataframe.shuffle.result2->dict(zip(range(n), parts))
A:dask.dataframe.shuffle.typ->numpy.min_scalar_type(npartitions * 2)
A:dask.dataframe.shuffle.c->numpy.mod(c, k, out=c)
A:dask.dataframe.shuffle.g->df.groupby(col)
A:dask.dataframe.shuffle.df2.columns->df.drop('_partitions', axis=1).set_index('_index', drop=True).columns.astype(column_dtype)
A:dask.dataframe.shuffle.meta->df._meta.set_index(index._meta, drop=drop)
A:dask.dataframe.shuffle.result.divisions->tuple(divisions)
A:dask.dataframe.shuffle.(mins, maxes)->compute(mins, maxes, **kwargs)
dask.dataframe.shuffle.barrier(args)
dask.dataframe.shuffle.collect(p,part,meta,barrier_token)
dask.dataframe.shuffle.compute_divisions(df,**kwargs)
dask.dataframe.shuffle.maybe_buffered_partd(self,buffer=True,tempdir=None)
dask.dataframe.shuffle.maybe_buffered_partd.__init__(self,buffer=True,tempdir=None)
dask.dataframe.shuffle.maybe_buffered_partd.__reduce__(self)
dask.dataframe.shuffle.partitioning_index(df,npartitions)
dask.dataframe.shuffle.rearrange_by_column(df,col,npartitions=None,max_branch=None,shuffle=None,compute=None)
dask.dataframe.shuffle.rearrange_by_column_disk(df,column,npartitions=None,compute=False)
dask.dataframe.shuffle.rearrange_by_column_tasks(df,column,max_branch=32,npartitions=None)
dask.dataframe.shuffle.rearrange_by_divisions(df,column,divisions,max_branch=None,shuffle=None)
dask.dataframe.shuffle.remove_nans(divisions)
dask.dataframe.shuffle.set_index(df,index,npartitions=None,shuffle=None,compute=False,drop=True,upsample=1.0,divisions=None,**kwargs)
dask.dataframe.shuffle.set_index_post_scalar(df,index_name,drop,column_dtype)
dask.dataframe.shuffle.set_index_post_series(df,index_name,drop,column_dtype)
dask.dataframe.shuffle.set_partition(df,index,divisions,max_branch=32,drop=True,shuffle=None,compute=None)
dask.dataframe.shuffle.set_partitions_pre(s,divisions)
dask.dataframe.shuffle.set_sorted_index(df,index,drop=True,divisions=None,**kwargs)
dask.dataframe.shuffle.shuffle(df,index,shuffle=None,npartitions=None,max_branch=32,compute=None)
dask.dataframe.shuffle.shuffle_group(df,col,stage,k,npartitions)
dask.dataframe.shuffle.shuffle_group_2(df,col)
dask.dataframe.shuffle.shuffle_group_3(df,col,npartitions,p)
dask.dataframe.shuffle.shuffle_group_get(g_head,i)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/dataframe/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/dataframe/categorical.py----------------------------------------
A:dask.dataframe.categorical.df->df.copy().copy()
A:dask.dataframe.categorical.df[col]->pandas.Categorical(df[col], categories=vals, ordered=False)
A:dask.dataframe.categorical.ind->pandas.Categorical(df.index, categories=index, ordered=False)
A:dask.dataframe.categorical.res[col]->x.dropna().drop_duplicates()
A:dask.dataframe.categorical.res->defaultdict(list)
A:dask.dataframe.categorical.columns->list(meta.select_dtypes(['object', 'category']).columns)
A:dask.dataframe.categorical.token->tokenize(df, columns, index, split_every)
A:dask.dataframe.categorical.(categories, index)->df.copy().copy()._get(dsk, (prefix, 0), **kwargs)
A:dask.dataframe.categorical.categories->self._property_map('categories').unique().compute(**kwargs)
A:dask.dataframe.categorical.out->self._series.copy()
A:dask.dataframe.categorical.out._meta->clear_known_categories(out._meta)
A:dask.dataframe.categorical.present->pandas.Index(present.compute())
A:dask.dataframe.categorical.(ordered, mask)->pandas.Index(present.compute()).reindex(meta_cat.categories)
A:dask.dataframe.categorical.meta->meta_cat.set_categories(new_categories, ordered=meta_cat.ordered)
dask.dataframe.categorical.CategoricalAccessor(Accessor)
dask.dataframe.categorical.CategoricalAccessor._validate(self,series)
dask.dataframe.categorical.CategoricalAccessor.as_known(self,**kwargs)
dask.dataframe.categorical.CategoricalAccessor.as_unknown(self)
dask.dataframe.categorical.CategoricalAccessor.categories(self)
dask.dataframe.categorical.CategoricalAccessor.codes(self)
dask.dataframe.categorical.CategoricalAccessor.known(self)
dask.dataframe.categorical.CategoricalAccessor.ordered(self)
dask.dataframe.categorical.CategoricalAccessor.remove_unused_categories(self)
dask.dataframe.categorical._categorize_block(df,categories,index)
dask.dataframe.categorical._get_categories(df,columns,index)
dask.dataframe.categorical._get_categories_agg(parts)
dask.dataframe.categorical.categorize(df,columns=None,index=None,split_every=None,**kwargs)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/dataframe/hyperloglog.py----------------------------------------
A:dask.dataframe.hyperloglog.bits->bits.cumsum(axis=1).astype(np.bool).cumsum(axis=1).astype(np.bool)
A:dask.dataframe.hyperloglog.hashes->hashes.astype(np.uint32).astype(np.uint32)
A:dask.dataframe.hyperloglog.first_bit->compute_first_bit(hashes)
A:dask.dataframe.hyperloglog.df->pandas.DataFrame({'j': j, 'first_bit': first_bit})
A:dask.dataframe.hyperloglog.Ms->Ms.reshape(len(Ms) // m, m).reshape(len(Ms) // m, m)
A:dask.dataframe.hyperloglog.M->reduce_state(Ms, b)
A:dask.dataframe.hyperloglog.V->(M == 0).sum()
dask.dataframe.hyperloglog.compute_first_bit(a)
dask.dataframe.hyperloglog.compute_hll_array(obj,b)
dask.dataframe.hyperloglog.estimate_count(Ms,b)
dask.dataframe.hyperloglog.reduce_state(Ms,b)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/dataframe/accessor.py----------------------------------------
A:dask.dataframe.accessor._not_implemented->set()
A:dask.dataframe.accessor.out->getattr(getattr(obj, accessor, obj), attr)(*args, **kwargs)
A:dask.dataframe.accessor.meta->self._delegate_method(self._series._meta_nonempty, self._accessor_name, attr, args, kwargs)
dask.dataframe.accessor.Accessor(self,series)
dask.dataframe.accessor.Accessor.__dir__(self)
dask.dataframe.accessor.Accessor.__getattr__(self,key)
dask.dataframe.accessor.Accessor.__init__(self,series)
dask.dataframe.accessor.Accessor._delegate_method(obj,accessor,attr,args,kwargs)
dask.dataframe.accessor.Accessor._delegate_property(obj,accessor,attr)
dask.dataframe.accessor.Accessor._delegates(self)
dask.dataframe.accessor.Accessor._function_map(self,attr,*args,**kwargs)
dask.dataframe.accessor.Accessor._property_map(self,attr)
dask.dataframe.accessor.Accessor._validate(self,series)
dask.dataframe.accessor.DatetimeAccessor(Accessor)
dask.dataframe.accessor.StringAccessor(Accessor)
dask.dataframe.accessor.StringAccessor.__getitem__(self,index)
dask.dataframe.accessor.StringAccessor._validate(self,series)
dask.dataframe.accessor.StringAccessor.split(self,pat=None,n=-1)
dask.dataframe.accessor.maybe_wrap_pandas(obj,x)
dask.dataframe.accessor.str_get(series,index)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/dataframe/reshape.py----------------------------------------
A:dask.dataframe.reshape.new_columns->pandas.CategoricalIndex(df[columns].cat.categories, name=columns)
A:dask.dataframe.reshape.meta->pandas.DataFrame(columns=new_columns, dtype=np.float64)
A:dask.dataframe.reshape.pv_sum->apply_concat_apply([df], chunk=methods.pivot_sum, aggregate=methods.pivot_agg, meta=meta, token='pivot_table_sum', chunk_kwargs=kwargs)
A:dask.dataframe.reshape.pv_count->apply_concat_apply([df], chunk=methods.pivot_count, aggregate=methods.pivot_agg, meta=meta, token='pivot_table_count', chunk_kwargs=kwargs)
dask.dataframe.get_dummies(data,prefix=None,prefix_sep='_',dummy_na=False,columns=None,sparse=False,drop_first=False)
dask.dataframe.melt(frame,id_vars=None,value_vars=None,var_name=None,value_name='value',col_level=None)
dask.dataframe.pivot_table(df,index=None,columns=None,values=None,aggfunc='mean')
dask.dataframe.reshape.get_dummies(data,prefix=None,prefix_sep='_',dummy_na=False,columns=None,sparse=False,drop_first=False)
dask.dataframe.reshape.melt(frame,id_vars=None,value_vars=None,var_name=None,value_name='value',col_level=None)
dask.dataframe.reshape.pivot_table(df,index=None,columns=None,values=None,aggfunc='mean')


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/dataframe/tseries/resample.py----------------------------------------
A:dask.dataframe.tseries.resample.resampler->Resampler(obj, rule, **kwargs)
A:dask.dataframe.tseries.resample.w->FutureWarning('how in .resample() is deprecated the new syntax is .resample(...).{0}()'.format(how))
A:dask.dataframe.tseries.resample.out->getattr(series.resample(rule, **resample_kwargs), how)()
A:dask.dataframe.tseries.resample.rule->pandas.tseries.frequencies.to_offset(rule)
A:dask.dataframe.tseries.resample.g->pandas.TimeGrouper(rule, how='count', closed=closed, label=label)
A:dask.dataframe.tseries.resample.divs->pandas.Series(range(len(divisions)), index=divisions)
A:dask.dataframe.tseries.resample.temp->pandas.Series(range(len(divisions)), index=divisions).resample(rule, closed=closed, label='left').count()
A:dask.dataframe.tseries.resample.newdivs->newdivs.tolist().tolist()
A:dask.dataframe.tseries.resample.outdivs->outdivs.tolist().tolist()
A:dask.dataframe.tseries.resample.(newdivs, outdivs)->_resample_bin_and_out_divs(self.obj.divisions, rule, **kwargs)
A:dask.dataframe.tseries.resample.partitioned->self.obj.repartition(newdivs, force=True)
A:dask.dataframe.tseries.resample.keys->self.obj.repartition(newdivs, force=True)._keys()
A:dask.dataframe.tseries.resample.args->zip(keys, outdivs, outdivs[1:], ['left'] * (len(keys) - 1) + [None])
A:dask.dataframe.tseries.resample.meta_r->self.obj._meta_nonempty.resample(self._rule, **self._kwargs)
A:dask.dataframe.tseries.resample.meta->getattr(meta_r, how)()
dask.dataframe.tseries.resample.Resampler(self,obj,rule,**kwargs)
dask.dataframe.tseries.resample.Resampler.__init__(self,obj,rule,**kwargs)
dask.dataframe.tseries.resample.Resampler._agg(self,how,meta=None,fill_value=np.nan)
dask.dataframe.tseries.resample.Resampler.count(self)
dask.dataframe.tseries.resample.Resampler.first(self)
dask.dataframe.tseries.resample.Resampler.last(self)
dask.dataframe.tseries.resample.Resampler.max(self)
dask.dataframe.tseries.resample.Resampler.mean(self)
dask.dataframe.tseries.resample.Resampler.median(self)
dask.dataframe.tseries.resample.Resampler.min(self)
dask.dataframe.tseries.resample.Resampler.ohlc(self)
dask.dataframe.tseries.resample.Resampler.prod(self)
dask.dataframe.tseries.resample.Resampler.sem(self)
dask.dataframe.tseries.resample.Resampler.std(self)
dask.dataframe.tseries.resample.Resampler.sum(self)
dask.dataframe.tseries.resample.Resampler.var(self)
dask.dataframe.tseries.resample._resample(obj,rule,how,**kwargs)
dask.dataframe.tseries.resample._resample_bin_and_out_divs(divisions,rule,closed='left',label='left')
dask.dataframe.tseries.resample._resample_series(series,start,end,reindex_closed,rule,resample_kwargs,how,fill_value)
dask.dataframe.tseries.resample.getnanos(rule)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/dataframe/tseries/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/dataframe/tseries/tests/test_resample.py----------------------------------------
A:dask.dataframe.tseries.tests.test_resample.index->pandas.date_range(start='20120102', periods=100, freq='T')
A:dask.dataframe.tseries.tests.test_resample.ps->pandas.DataFrame({'a': range(len(index))}, index=index)
A:dask.dataframe.tseries.tests.test_resample.ds->dask.dataframe.from_pandas(s, npartitions=5)
A:dask.dataframe.tseries.tests.test_resample.result->resample(ds, freq, how=method, closed=closed, label=label)
A:dask.dataframe.tseries.tests.test_resample.expected->resample(ps, freq, how=method, closed=closed, label=label)
A:dask.dataframe.tseries.tests.test_resample.s->pandas.Series(range(len(index)), index=index)
A:dask.dataframe.tseries.tests.test_resample.df->pandas.DataFrame({'x': [1, 2, 3]})
A:dask.dataframe.tseries.tests.test_resample.ddf->dask.dataframe.from_pandas(df, npartitions=2, sort=False)
dask.dataframe.tseries.tests.test_resample.resample(df,freq,how='mean',**kwargs)
dask.dataframe.tseries.tests.test_resample.test_series_resample(obj,method,npartitions,freq,closed,label)
dask.dataframe.tseries.tests.test_resample.test_series_resample_not_implemented()
dask.dataframe.tseries.tests.test_resample.test_unknown_divisions_error()


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/dataframe/tseries/tests/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/dataframe/tests/test_categorical.py----------------------------------------
A:dask.dataframe.tests.test_categorical.s->pandas.Series(range(6), index=pd.Categorical(list('bacbac')))
A:dask.dataframe.tests.test_categorical.ds->dask.dataframe.from_pandas(s, npartitions=2)
A:dask.dataframe.tests.test_categorical.a->dask.dataframe.from_delayed([dask.delayed(make_empty)(), dask.delayed(make_full)()])
A:dask.dataframe.tests.test_categorical.b->dask.dataframe.from_delayed([dask.delayed(make_empty)(), dask.delayed(make_full)()]).set_index('y', divisions=['a', 'b', 'c'], npartitions=a.npartitions)
A:dask.dataframe.tests.test_categorical.c->pandas.DataFrame({'v': list('klmno'), 'w': list('zzzzz'), 'x': np.arange(10, 15), 'y': list('bcbcc'), 'z': np.arange(10, 15, dtype='f8')})
A:dask.dataframe.tests.test_categorical.df.w->dask.dataframe.from_pandas(pd.DataFrame({'A': ['a', 'b', 'a', float('nan')]}), npartitions=2).w.astype('category')
A:dask.dataframe.tests.test_categorical.df.y->dask.dataframe.from_pandas(pd.DataFrame({'A': ['a', 'b', 'a', float('nan')]}), npartitions=2).y.astype('category')
A:dask.dataframe.tests.test_categorical.ddf->dask.dataframe.from_pandas(df, npartitions=2)
A:dask.dataframe.tests.test_categorical.df->dask.dataframe.from_pandas(pd.DataFrame({'A': ['a', 'b', 'a', float('nan')]}), npartitions=2)
A:dask.dataframe.tests.test_categorical.meta->clear_known_categories(frames4[0])
A:dask.dataframe.tests.test_categorical.ddf2->dask.dataframe.from_pandas(df2, npartitions=2, sort=False)
A:dask.dataframe.tests.test_categorical.ddf_known_index->dask.dataframe.from_pandas(df, npartitions=2).categorize(columns=[], index=True)
A:dask.dataframe.tests.test_categorical.df['y']->df['y'].astype('category').astype('category')
A:dask.dataframe.tests.test_categorical.ddf['y']->ddf['y'].astype('category').astype('category')
A:dask.dataframe.tests.test_categorical.df.x->dask.dataframe.from_pandas(pd.DataFrame({'A': ['a', 'b', 'a', float('nan')]}), npartitions=2).x.astype('category')
A:dask.dataframe.tests.test_categorical.df2->dask.dataframe.from_pandas(pd.DataFrame({'A': ['a', 'b', 'a', float('nan')]}), npartitions=2).set_index(df.x)
A:dask.dataframe.tests.test_categorical.expected->op(get_cat(s))
A:dask.dataframe.tests.test_categorical.result->op(get_cat(ds))
A:dask.dataframe.tests.test_categorical.op->operator.methodcaller(method, **kwargs)
A:dask.dataframe.tests.test_categorical.da->da.cat.as_unknown().cat.as_unknown()
A:dask.dataframe.tests.test_categorical.db->da.cat.as_unknown().cat.as_unknown().cat.as_known()
A:dask.dataframe.tests.test_categorical.res->da.cat.as_unknown().cat.as_unknown().cat.as_known().compute()
dask.dataframe.tests.test_categorical.TestCategoricalAccessor
dask.dataframe.tests.test_categorical.TestCategoricalAccessor.test_callable(self,series,method,kwargs)
dask.dataframe.tests.test_categorical.TestCategoricalAccessor.test_categorical_empty(self)
dask.dataframe.tests.test_categorical.TestCategoricalAccessor.test_properties(self,series,prop,compare)
dask.dataframe.tests.test_categorical.TestCategoricalAccessor.test_unknown_categories(self,series)
dask.dataframe.tests.test_categorical.assert_array_index_eq(left,right)
dask.dataframe.tests.test_categorical.get_cat(x)
dask.dataframe.tests.test_categorical.test_categorical_accessor_presence()
dask.dataframe.tests.test_categorical.test_categorical_set_index(shuffle)
dask.dataframe.tests.test_categorical.test_categorize()
dask.dataframe.tests.test_categorical.test_categorize_index()
dask.dataframe.tests.test_categorical.test_categorize_nan()
dask.dataframe.tests.test_categorical.test_concat_unions_categoricals()
dask.dataframe.tests.test_categorical.test_is_categorical_dtype()
dask.dataframe.tests.test_categorical.test_repartition_on_categoricals(npartitions)
dask.dataframe.tests.test_categorical.test_unknown_categoricals()


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/dataframe/tests/test_ufunc.py----------------------------------------
A:dask.dataframe.tests.test_ufunc.pd->pytest.importorskip('pandas')
A:dask.dataframe.tests.test_ufunc.dafunc->getattr(da, ufunc)
A:dask.dataframe.tests.test_ufunc.npfunc->getattr(np, ufunc)
A:dask.dataframe.tests.test_ufunc.s->pytest.importorskip('pandas').Series(np.random.randint(1, 100, size=20))
A:dask.dataframe.tests.test_ufunc.ds->dask.dataframe.from_pandas(s, 3)
A:dask.dataframe.tests.test_ufunc.df->pytest.importorskip('pandas').DataFrame(np.random.randint(1, 100, size=(20, 2)), columns=['A', 'B'])
A:dask.dataframe.tests.test_ufunc.ddf->dask.dataframe.from_pandas(df, 3)
A:dask.dataframe.tests.test_ufunc.exp->pytest.importorskip('pandas').DataFrame(npfunc(df).copy(), columns=df.columns, index=df.index)
A:dask.dataframe.tests.test_ufunc.s1->pytest.importorskip('pandas').Series(np.random.randint(1, 100, size=20))
A:dask.dataframe.tests.test_ufunc.ds1->dask.dataframe.from_pandas(s1, 3)
A:dask.dataframe.tests.test_ufunc.s2->pytest.importorskip('pandas').Series(np.random.randint(1, 100, size=20))
A:dask.dataframe.tests.test_ufunc.ds2->dask.dataframe.from_pandas(s2, 4)
A:dask.dataframe.tests.test_ufunc.df1->pytest.importorskip('pandas').DataFrame(np.random.randint(1, 100, size=(20, 2)), columns=['A', 'B'])
A:dask.dataframe.tests.test_ufunc.ddf1->dask.dataframe.from_pandas(df1, 3)
A:dask.dataframe.tests.test_ufunc.df2->pytest.importorskip('pandas').DataFrame(np.random.randint(1, 100, size=(20, 2)), columns=['A', 'B'])
A:dask.dataframe.tests.test_ufunc.ddf2->dask.dataframe.from_pandas(df2, 4)
dask.dataframe.tests.test_ufunc.test_clip()
dask.dataframe.tests.test_ufunc.test_ufunc(ufunc)
dask.dataframe.tests.test_ufunc.test_ufunc_array_wrap(ufunc)
dask.dataframe.tests.test_ufunc.test_ufunc_with_2args(ufunc)
dask.dataframe.tests.test_ufunc.test_ufunc_with_index(ufunc)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/dataframe/tests/test_utils_dataframe.py----------------------------------------
A:dask.dataframe.tests.test_utils_dataframe.df->pandas.DataFrame({'a': ['x', 'y', 'z'], 'b': [True, False, True], 'c': [1, 2.5, 3.5], 'd': [1, 2, 3], 'e': pd.Categorical(['x', 'y', 'z'])})
A:dask.dataframe.tests.test_utils_dataframe.result->list(shard_df_on_index(df, [20, 50]))
A:dask.dataframe.tests.test_utils_dataframe.meta->meta_nonempty(x)
A:dask.dataframe.tests.test_utils_dataframe.ddf->dask.dataframe.from_pandas(df, npartitions=2)
A:dask.dataframe.tests.test_utils_dataframe.x->pandas.Timestamp(2000, 1, 1)
A:dask.dataframe.tests.test_utils_dataframe.df1->pandas.DataFrame({'A': pd.Categorical(['Alice', 'Bob', 'Carol']), 'B': list('abc'), 'C': 'bar', 'D': np.float32(1), 'E': np.int32(1), 'F': pd.Timestamp('2016-01-01'), 'G': pd.date_range('2016-01-01', periods=3, tz='America/New_York'), 'H': pd.Timedelta('1 hours', 'ms'), 'I': np.void(b' '), 'J': pd.Categorical([UNKNOWN_CATEGORIES] * 3)}, columns=list('DCBAHGFEIJ'))
A:dask.dataframe.tests.test_utils_dataframe.df3->meta_nonempty(df2)
A:dask.dataframe.tests.test_utils_dataframe.s->pandas.UInt64Index([1], name='foo').to_series()
A:dask.dataframe.tests.test_utils_dataframe.res->meta_nonempty(idx)
A:dask.dataframe.tests.test_utils_dataframe.exp->pandas.DataFrame([['foo', 'foo', 'foo'], ['foo', 'foo', 'foo']], index=['a', 'b'], columns=['A', 'A', 'B'])
A:dask.dataframe.tests.test_utils_dataframe.idx->pandas.UInt64Index([1], name='foo')
dask.dataframe.tests.test_utils_dataframe.test_check_meta()
dask.dataframe.tests.test_utils_dataframe.test_make_meta()
dask.dataframe.tests.test_utils_dataframe.test_meta_duplicated()
dask.dataframe.tests.test_utils_dataframe.test_meta_nonempty()
dask.dataframe.tests.test_utils_dataframe.test_meta_nonempty_empty_categories()
dask.dataframe.tests.test_utils_dataframe.test_meta_nonempty_index()
dask.dataframe.tests.test_utils_dataframe.test_meta_nonempty_scalar()
dask.dataframe.tests.test_utils_dataframe.test_meta_nonempty_uint64index()
dask.dataframe.tests.test_utils_dataframe.test_raise_on_meta_error()
dask.dataframe.tests.test_utils_dataframe.test_shard_df_on_index()


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/dataframe/tests/test_rolling.py----------------------------------------
A:dask.dataframe.tests.test_rolling.df->pandas.DataFrame(np.random.randn(50, 2))
A:dask.dataframe.tests.test_rolling.ddf->dask.dataframe.from_pandas(pd.DataFrame([10] * 30), npartitions=3)
A:dask.dataframe.tests.test_rolling.ts->pandas.Series(np.random.randn(25).cumsum())
A:dask.dataframe.tests.test_rolling.dts->dask.dataframe.from_pandas(ts, 3)
A:dask.dataframe.tests.test_rolling.a->dask.dataframe.from_pandas(df, npartitions=2)
A:dask.dataframe.tests.test_rolling.b->pandas.DataFrame(np.random.randn(50, 2)).shift(-after.seconds)
A:dask.dataframe.tests.test_rolling.res->dask.dataframe.from_pandas(pd.DataFrame([10] * 30), npartitions=3).map_overlap(shifted_sum, 0, 3, 0, 3, c=2)
A:dask.dataframe.tests.test_rolling.sol->shifted_sum(df.b, before, after, c=2)
A:dask.dataframe.tests.test_rolling.res2->dask.dataframe.from_pandas(pd.DataFrame([10] * 30), npartitions=3).map_overlap(shifted_sum, 0, 3, 0, 3, c=2)
A:dask.dataframe.tests.test_rolling.res3->dask.dataframe.from_pandas(pd.DataFrame([10] * 30), npartitions=3).map_overlap(shifted_sum, 0, 3, 0, 3, c=3)
A:dask.dataframe.tests.test_rolling.diff->set(res3.dask).difference(res.dask)
A:dask.dataframe.tests.test_rolling.res4->dask.dataframe.from_pandas(pd.DataFrame([10] * 30), npartitions=3).map_overlap(shifted_sum, 3, 0, 0, 3, c=2)
A:dask.dataframe.tests.test_rolling.prolling->pandas.Series(np.random.randn(25).cumsum()).a.rolling(window)
A:dask.dataframe.tests.test_rolling.drolling->dask.dataframe.from_pandas(ts, 3).a.rolling(window)
A:dask.dataframe.tests.test_rolling.result->dask.dataframe.from_pandas(ts, 3).map_overlap(lambda x: x.rolling(window).count(), before, after)
A:dask.dataframe.tests.test_rolling.before->pandas.Timedelta(before)
A:dask.dataframe.tests.test_rolling.after->pandas.Timedelta(after)
A:dask.dataframe.tests.test_rolling.expected->dask.dataframe.from_pandas(ts, 3).compute().rolling(window).count()
dask.dataframe.tests.test_rolling.mad(x)
dask.dataframe.tests.test_rolling.rolling_functions_tests(p,d)
dask.dataframe.tests.test_rolling.shifted_sum(df,before,after,c=0)
dask.dataframe.tests.test_rolling.test_map_overlap(npartitions)
dask.dataframe.tests.test_rolling.test_map_partitions_errors()
dask.dataframe.tests.test_rolling.test_map_partitions_names()
dask.dataframe.tests.test_rolling.test_rolling_axis()
dask.dataframe.tests.test_rolling.test_rolling_functions_dataframe()
dask.dataframe.tests.test_rolling.test_rolling_functions_series()
dask.dataframe.tests.test_rolling.test_rolling_methods(method,args,window,center,check_less_precise)
dask.dataframe.tests.test_rolling.test_rolling_names()
dask.dataframe.tests.test_rolling.test_rolling_partition_size()
dask.dataframe.tests.test_rolling.test_rolling_raises()
dask.dataframe.tests.test_rolling.test_rolling_repr()
dask.dataframe.tests.test_rolling.test_time_rolling(before,after)
dask.dataframe.tests.test_rolling.test_time_rolling_constructor()
dask.dataframe.tests.test_rolling.test_time_rolling_methods(method,args,window,check_less_precise)
dask.dataframe.tests.test_rolling.test_time_rolling_repr()
dask.dataframe.tests.test_rolling.test_time_rolling_window_too_large(window)
dask.dataframe.tests.test_rolling.ts_shifted_sum(df,before,after,c=0)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/dataframe/tests/test_reshape.py----------------------------------------
A:dask.dataframe.tests.test_reshape.exp->pandas.pivot_table(df, index='A', columns='B', values='C', aggfunc='count').astype(np.float64)
A:dask.dataframe.tests.test_reshape.ddata->dask.dataframe.from_pandas(data, 2)
A:dask.dataframe.tests.test_reshape.res->dask.dataframe.pivot_table(ddf, index='A', columns='B', values='C', aggfunc='count')
A:dask.dataframe.tests.test_reshape.df->pandas.DataFrame({'A': np.random.choice(list('abc'), size=10), 'B': np.random.randn(10), 'C': np.random.choice(list('abc'), size=10)})
A:dask.dataframe.tests.test_reshape.ddf->dask.dataframe.from_pandas(df, 2)
A:dask.dataframe.tests.test_reshape.s->pandas.Series([1, 1, 1, 2, 2, 1, 3, 4])
A:dask.dataframe.tests.test_reshape.ds->dask.dataframe.from_pandas(s, 2)
A:dask.dataframe.tests.test_reshape.ddf._meta->make_meta({'A': object, 'B': float, 'C': 'category'})
A:dask.dataframe.tests.test_reshape.exp_index->pandas.CategoricalIndex(['A', 'B'], name='B')
dask.dataframe.tests.test_reshape.test_get_dummies(data)
dask.dataframe.tests.test_reshape.test_get_dummies_errors()
dask.dataframe.tests.test_reshape.test_get_dummies_kwargs()
dask.dataframe.tests.test_reshape.test_get_dummies_object()
dask.dataframe.tests.test_reshape.test_pivot_table(aggfunc)
dask.dataframe.tests.test_reshape.test_pivot_table_dtype()
dask.dataframe.tests.test_reshape.test_pivot_table_errors()


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/dataframe/tests/test_indexing.py----------------------------------------
A:dask.dataframe.tests.test_indexing.meta->make_meta({'a': 'i8', 'b': 'i8'}, index=pd.Index([], 'i8'))
A:dask.dataframe.tests.test_indexing.d->dask.dataframe.DataFrame(dsk, 'x', meta, [0, 5, 9, 9])
A:dask.dataframe.tests.test_indexing.full->dask.dataframe.DataFrame(dsk, 'x', meta, [0, 5, 9, 9]).compute()
A:dask.dataframe.tests.test_indexing.df->pandas.DataFrame({'A': np.random.randn(100), 'B': np.random.randn(100)}, index=pd.period_range('2011-01-01', freq='D', periods=100))
A:dask.dataframe.tests.test_indexing.ddf->dask.dataframe.from_pandas(df, 50)
A:dask.dataframe.tests.test_indexing.s->dask.dataframe.Series({('df', 0): A, ('df', 1): B}, 'df', A, [A.index.min(), B.index.min(), B.index.max()])
A:dask.dataframe.tests.test_indexing.a->dask.dataframe.from_pandas(df, 2)
A:dask.dataframe.tests.test_indexing.a.divisions->list(map(pd.Timestamp, a.divisions))
A:dask.dataframe.tests.test_indexing.datetime_index->pandas.date_range('2016-01-01', '2016-01-31', freq='12h')
A:dask.dataframe.tests.test_indexing.slice_->slice('2016-01-03', '2016-01-05')
dask.dataframe.tests.test_indexing.test_coerce_loc_index()
dask.dataframe.tests.test_indexing.test_getitem()
dask.dataframe.tests.test_indexing.test_getitem_period_str()
dask.dataframe.tests.test_indexing.test_getitem_slice()
dask.dataframe.tests.test_indexing.test_getitem_timestamp_str()
dask.dataframe.tests.test_indexing.test_loc()
dask.dataframe.tests.test_indexing.test_loc2d()
dask.dataframe.tests.test_indexing.test_loc2d_duplicated_columns()
dask.dataframe.tests.test_indexing.test_loc2d_with_known_divisions()
dask.dataframe.tests.test_indexing.test_loc2d_with_unknown_divisions()
dask.dataframe.tests.test_indexing.test_loc_datetime_no_freq()
dask.dataframe.tests.test_indexing.test_loc_non_informative_index()
dask.dataframe.tests.test_indexing.test_loc_on_numpy_datetimes()
dask.dataframe.tests.test_indexing.test_loc_on_pandas_datetimes()
dask.dataframe.tests.test_indexing.test_loc_period_str()
dask.dataframe.tests.test_indexing.test_loc_timestamp_str()
dask.dataframe.tests.test_indexing.test_loc_with_series()
dask.dataframe.tests.test_indexing.test_loc_with_series_different_partition()
dask.dataframe.tests.test_indexing.test_loc_with_text_dates()


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/dataframe/tests/test_dataframe.py----------------------------------------
A:dask.dataframe.tests.test_dataframe.meta->make_meta({'x': 'i8'})
A:dask.dataframe.tests.test_dataframe.d->dask.dataframe.from_pandas(p, npartitions=2)
A:dask.dataframe.tests.test_dataframe.full->dask.dataframe.from_pandas(p, npartitions=2).compute(get=dask.get)
A:dask.dataframe.tests.test_dataframe.expected->pandas.DataFrame({'A': range(len(index))}, index=index).drop(drop)
A:dask.dataframe.tests.test_dataframe.ddf->dask.dataframe.from_pandas(df, 1).set_index('A').query('B != {}'.format(drop))
A:dask.dataframe.tests.test_dataframe.val->pandas.Timestamp('2001-01-01')
A:dask.dataframe.tests.test_dataframe.s->pandas.Series(['a', 'b', 'c', 'd'])
A:dask.dataframe.tests.test_dataframe.df->pandas.DataFrame({'A': range(len(index))}, index=index)
A:dask.dataframe.tests.test_dataframe.idx->pandas.Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], name='x')
A:dask.dataframe.tests.test_dataframe.s_naive->pandas.Series(pd.date_range('20130101', periods=10))
A:dask.dataframe.tests.test_dataframe.s_aware->pandas.Series(pd.date_range('20130101', periods=10, tz='US/Eastern'))
A:dask.dataframe.tests.test_dataframe.pdf->pandas.DataFrame({'A': range(len(index))}, index=index).set_index('A').query('B != {}'.format(drop))
A:dask.dataframe.tests.test_dataframe.ds->dask.dataframe.from_pandas(s, npartitions=2)
A:dask.dataframe.tests.test_dataframe.pdf1->pandas.DataFrame({'a': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'b': [3, 5, 2, 5, 7, 2, 4, 2, 4]})
A:dask.dataframe.tests.test_dataframe.ddf1->dask.dataframe.from_pandas(df1, 4)
A:dask.dataframe.tests.test_dataframe.pdf2->pandas.DataFrame({'a': [True, False, True] * 3, 'b': [False, False, True] * 3})
A:dask.dataframe.tests.test_dataframe.ddf2->dask.dataframe.from_pandas(df, 1).set_index('A').query('B != {}'.format(drop)).assign(y=ddf.x + 1, z=ddf.x - 1)
A:dask.dataframe.tests.test_dataframe.pdf3->pandas.DataFrame({'a': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'b': [3, 5, 2, 5, 7, 2, 4, 2, 4]}, index=[0, 1, 2, 3, 4, 5, 6, 7, 8])
A:dask.dataframe.tests.test_dataframe.ddf3->dask.dataframe.from_pandas(pdf3, 2)
A:dask.dataframe.tests.test_dataframe.pdf4->pandas.DataFrame({'a': [True, False, True] * 3, 'b': [False, False, True] * 3}, index=[5, 6, 7, 8, 9, 10, 11, 12, 13])
A:dask.dataframe.tests.test_dataframe.ddf4->dask.dataframe.from_pandas(pdf4, 2)
A:dask.dataframe.tests.test_dataframe.pdf5->pandas.DataFrame({'a': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'b': [9, 4, 2, 6, 2, 3, 1, 6, 2], 'c': [5, 6, 7, 8, 9, 10, 11, 12, 13]}, index=[0, 1, 2, 3, 4, 5, 6, 7, 8])
A:dask.dataframe.tests.test_dataframe.ddf5->dask.dataframe.from_pandas(pdf5, 2)
A:dask.dataframe.tests.test_dataframe.pdf6->pandas.DataFrame({'a': [True, False, True] * 3, 'b': [False, False, True] * 3, 'c': [False] * 9, 'd': [True] * 9}, index=[5, 6, 7, 8, 9, 10, 11, 12, 13])
A:dask.dataframe.tests.test_dataframe.ddf6->dask.dataframe.from_pandas(pdf6, 2)
A:dask.dataframe.tests.test_dataframe.result->dask.dataframe.methods.boundary_slice(df, left, right)
A:dask.dataframe.tests.test_dataframe.x->numpy.array([-1, -2, 2, 4, 3])
A:dask.dataframe.tests.test_dataframe.a->dask.dataframe.from_pandas(df, 2)
A:dask.dataframe.tests.test_dataframe.b->dask.dataframe.from_pandas(df, 2).copy()
A:dask.dataframe.tests.test_dataframe.res->dask.dataframe.from_pandas(df, 1).set_index('A').query('B != {}'.format(drop)).drop_duplicates(subset=subset, keep=keep, split_every=split_every, split_out=10)
A:dask.dataframe.tests.test_dataframe.res2->f(ddf.a, 5, split_every=2)
A:dask.dataframe.tests.test_dataframe.sol->pandas.DataFrame({'A': range(len(index))}, index=index).drop_duplicates(subset=subset, keep=keep)
A:dask.dataframe.tests.test_dataframe.div1->dask.dataframe.from_pandas(df, 1).set_index('A').query('B != {}'.format(drop)).a.get_partition(0)
A:dask.dataframe.tests.test_dataframe.div2->dask.dataframe.from_pandas(df, 1).set_index('A').query('B != {}'.format(drop)).a.get_partition(1)
A:dask.dataframe.tests.test_dataframe.div3->dask.dataframe.from_pandas(df, 1).set_index('A').query('B != {}'.format(drop)).a.get_partition(2)
A:dask.dataframe.tests.test_dataframe.result2->dask.dataframe.from_pandas(df, 1).set_index('A').query('B != {}'.format(drop)).x.value_counts(split_every=2)
A:dask.dataframe.tests.test_dataframe.exp->dask.dataframe.from_pandas(p, npartitions=2).compute(get=dask.get).b.quantile([])
A:dask.dataframe.tests.test_dataframe.minexp->pandas.DataFrame([[1, 21, 11], [17, 37, 27]], index=[0.25, 0.75], columns=['A', 'X', 'B'])
A:dask.dataframe.tests.test_dataframe.maxexp->pandas.DataFrame([[2, 22, 12], [18, 38, 28]], index=[0.25, 0.75], columns=['A', 'X', 'B'])
A:dask.dataframe.tests.test_dataframe.d_unknown->dask.dataframe.from_pandas(full, npartitions=3, sort=False)
A:dask.dataframe.tests.test_dataframe.res_unknown->dask.dataframe.from_pandas(full, npartitions=3, sort=False).assign(c=1, d='string', e=d_unknown.a.sum(), f=d_unknown.a + d_unknown.b, g=lambda x: x.a + x.b)
A:dask.dataframe.tests.test_dataframe.lk->pandas.Series(lk)
A:dask.dataframe.tests.test_dataframe.e->dask.dataframe.from_pandas(p, npartitions=2).assign(c=d.a + 1)
A:dask.dataframe.tests.test_dataframe.f->type(e)(*e._args)
A:dask.dataframe.tests.test_dataframe.df1a->pandas.DataFrame({'A': np.random.randn(10), 'B': np.random.randn(10), 'C': np.random.randn(10)}, index=[1, 12, 5, 6, 3, 9, 10, 4, 13, 11])
A:dask.dataframe.tests.test_dataframe.df1b->pandas.DataFrame({'B': np.random.randn(10), 'C': np.random.randn(10), 'D': np.random.randn(10)}, index=[0, 3, 2, 10, 5, 6, 7, 8, 12, 13])
A:dask.dataframe.tests.test_dataframe.ddf1a->dask.dataframe.from_pandas(df1a, 3)
A:dask.dataframe.tests.test_dataframe.ddf1b->dask.dataframe.from_pandas(df1b, 3)
A:dask.dataframe.tests.test_dataframe.(res1, res2)->dask.dataframe.from_pandas(df1a, 3).align(ddf1b, join=join, axis='columns')
A:dask.dataframe.tests.test_dataframe.(exp1, exp2)->pandas.DataFrame({'A': np.random.randn(10), 'B': np.random.randn(10), 'C': np.random.randn(10)}, index=[1, 12, 5, 6, 3, 9, 10, 4, 13, 11]).align(df1b, join=join, axis='columns')
A:dask.dataframe.tests.test_dataframe.df1->pandas.DataFrame({'A': np.random.choice([1, 2, np.nan], 100), 'B': np.random.choice(['a', 'b', np.nan], 100)})
A:dask.dataframe.tests.test_dataframe.df2->pandas.DataFrame({'A': range(len(index))}, index=index).assign(y=df.x + 1, z=df.x - 1)
A:dask.dataframe.tests.test_dataframe.cloudpickle->pytest.importorskip('cloudpickle')
A:dask.dataframe.tests.test_dataframe.a2->loads(cp_dumps(df.A))
A:dask.dataframe.tests.test_dataframe.i2->loads(cp_dumps(df.index))
A:dask.dataframe.tests.test_dataframe.s2->loads(cp_dumps(s))
A:dask.dataframe.tests.test_dataframe.(a, b)->dask.dataframe.from_pandas(p, npartitions=2).random_split([0.5, 0.5], 42)
A:dask.dataframe.tests.test_dataframe.(a2, b2)->dask.dataframe.from_pandas(p, npartitions=2).random_split([0.5, 0.5], 42)
A:dask.dataframe.tests.test_dataframe.parts->dask.get(b.dask, b._keys())
A:dask.dataframe.tests.test_dataframe.names->set([p._name for p in parts])
A:dask.dataframe.tests.test_dataframe.ps->pandas.Series([1.123, 2.123, 3.123, 1.234, 2.234, 3.234], name='a')
A:dask.dataframe.tests.test_dataframe.keys->sorted(keys)
A:dask.dataframe.tests.test_dataframe.sp->pandas.concat([d._get(d.dask, k) for k in keys])
A:dask.dataframe.tests.test_dataframe.rddf->dask.dataframe.from_pandas(df, 1).set_index('A').query('B != {}'.format(drop)).repartition(divisions=div, force=True)
A:dask.dataframe.tests.test_dataframe.rds->dask.dataframe.from_pandas(df, 1).set_index('A').query('B != {}'.format(drop)).x.repartition(divisions=div, force=True)
A:dask.dataframe.tests.test_dataframe.start->pandas.Timestamp(start)
A:dask.dataframe.tests.test_dataframe.end->pandas.Timestamp(end)
A:dask.dataframe.tests.test_dataframe.ind->pandas.DatetimeIndex(start=start, end=end, freq='60s')
A:dask.dataframe.tests.test_dataframe.c->dask.dataframe.from_pandas(df, 2).sample(0.5, random_state=1234)
A:dask.dataframe.tests.test_dataframe.bb->dask.dataframe.from_pandas(df, 2).copy().index.compute()
A:dask.dataframe.tests.test_dataframe.df['x']->pandas.DataFrame({'A': range(len(index))}, index=index).x.astype('M8[us]')
A:dask.dataframe.tests.test_dataframe.q->dask.dataframe.from_pandas(df, 2).query('x**2 > y')
A:dask.dataframe.tests.test_dataframe.p->pandas.DataFrame({'x': [1, 2, 3, 4], 'y': [5, 6, 7, 8]})
A:dask.dataframe.tests.test_dataframe.r3->f(3)
A:dask.dataframe.tests.test_dataframe.r4->f(4)
A:dask.dataframe.tests.test_dataframe.arr->numpy.random.randn(100, 2)
A:dask.dataframe.tests.test_dataframe.index->pandas.PeriodIndex(freq='A', start='1/1/2001', end='12/1/2004')
A:dask.dataframe.tests.test_dataframe.res3->dask.dataframe.from_pandas(a, npartitions=6).corr(db, min_periods=10)
A:dask.dataframe.tests.test_dataframe.res4->dask.dataframe.from_pandas(a, npartitions=6).corr(db, min_periods=10, split_every=2)
A:dask.dataframe.tests.test_dataframe.sol2->dask.dataframe.from_pandas(a, npartitions=6).corr(db, min_periods=10)
A:dask.dataframe.tests.test_dataframe.da->dask.dataframe.from_pandas(a, npartitions=6)
A:dask.dataframe.tests.test_dataframe.db->dask.dataframe.from_pandas(b, npartitions=7)
A:dask.dataframe.tests.test_dataframe.dx->dask.dataframe.from_pandas(df, 1).set_index('A').query('B != {}'.format(drop)).x.astype('category')
A:dask.dataframe.tests.test_dataframe.i->pandas.util.testing.makeTimeSeries()
A:dask.dataframe.tests.test_dataframe.stdout_pd->buf_pd.getvalue()
A:dask.dataframe.tests.test_dataframe.stdout_da->stdout_da.replace(str(type(ddf)), str(type(df))).replace(str(type(ddf)), str(type(df)))
A:dask.dataframe.tests.test_dataframe.buf->StringIO()
A:dask.dataframe.tests.test_dataframe.g->dask.dataframe.from_pandas(df, 1).set_index('A').query('B != {}'.format(drop)).groupby(['A', 'B']).agg(['count', 'sum'])
A:dask.dataframe.tests.test_dataframe.orig->dask.dataframe.from_pandas(df, 1).set_index('A').query('B != {}'.format(drop)).copy()
A:dask.dataframe.tests.test_dataframe.dropped->dask.dataframe.from_pandas(s, npartitions=2).unique(split_every=split_every, split_out=split_out)
A:dask.dataframe.tests.test_dataframe.dsk->dask.dataframe.from_pandas(s, npartitions=2).unique(split_every=split_every, split_out=split_out)._optimize(dropped.dask, dropped._keys())
A:dask.dataframe.tests.test_dataframe.(dependencies, dependents)->get_deps(dsk)
A:dask.dataframe.tests.test_dataframe.y->numpy.array([-1, -2, 2, 4, 3]).copy()
A:dask.dataframe.tests.test_dataframe.rs->numpy.random.RandomState(1)
A:dask.dataframe.tests.test_dataframe.dtRange->pandas.date_range('01.01.2015', '05.05.2015')
dask.dataframe.tests.test_dataframe._assert_info(df,ddf,memory_usage=True)
dask.dataframe.tests.test_dataframe.test_Dataframe()
dask.dataframe.tests.test_dataframe.test_Index()
dask.dataframe.tests.test_dataframe.test_Scalar()
dask.dataframe.tests.test_dataframe.test_Series()
dask.dataframe.tests.test_dataframe.test_abs()
dask.dataframe.tests.test_dataframe.test_aca_meta_infer()
dask.dataframe.tests.test_dataframe.test_aca_split_every()
dask.dataframe.tests.test_dataframe.test_align(join)
dask.dataframe.tests.test_dataframe.test_align_axis(join)
dask.dataframe.tests.test_dataframe.test_apply()
dask.dataframe.tests.test_dataframe.test_apply_infer_columns()
dask.dataframe.tests.test_dataframe.test_apply_warns()
dask.dataframe.tests.test_dataframe.test_applymap()
dask.dataframe.tests.test_dataframe.test_args()
dask.dataframe.tests.test_dataframe.test_assign()
dask.dataframe.tests.test_dataframe.test_astype()
dask.dataframe.tests.test_dataframe.test_astype_categoricals()
dask.dataframe.tests.test_dataframe.test_attribute_assignment()
dask.dataframe.tests.test_dataframe.test_attributes()
dask.dataframe.tests.test_dataframe.test_autocorr()
dask.dataframe.tests.test_dataframe.test_better_errors_object_reductions()
dask.dataframe.tests.test_dataframe.test_boundary_slice_nonmonotonic()
dask.dataframe.tests.test_dataframe.test_boundary_slice_same(index,left,right)
dask.dataframe.tests.test_dataframe.test_categorize_info()
dask.dataframe.tests.test_dataframe.test_clip(lower,upper)
dask.dataframe.tests.test_dataframe.test_column_assignment()
dask.dataframe.tests.test_dataframe.test_column_names()
dask.dataframe.tests.test_dataframe.test_columns_assignment()
dask.dataframe.tests.test_dataframe.test_combine()
dask.dataframe.tests.test_dataframe.test_combine_first()
dask.dataframe.tests.test_dataframe.test_concat()
dask.dataframe.tests.test_dataframe.test_copy()
dask.dataframe.tests.test_dataframe.test_corr()
dask.dataframe.tests.test_dataframe.test_cov()
dask.dataframe.tests.test_dataframe.test_cov_corr_meta()
dask.dataframe.tests.test_dataframe.test_cov_corr_stable()
dask.dataframe.tests.test_dataframe.test_cumulative()
dask.dataframe.tests.test_dataframe.test_dataframe_compute_forward_kwargs()
dask.dataframe.tests.test_dataframe.test_dataframe_iterrows()
dask.dataframe.tests.test_dataframe.test_dataframe_itertuples()
dask.dataframe.tests.test_dataframe.test_dataframe_picklable()
dask.dataframe.tests.test_dataframe.test_dataframe_quantile()
dask.dataframe.tests.test_dataframe.test_dataframe_reductions_arithmetic(reduction)
dask.dataframe.tests.test_dataframe.test_datetime_accessor()
dask.dataframe.tests.test_dataframe.test_datetime_loc_open_slicing()
dask.dataframe.tests.test_dataframe.test_del()
dask.dataframe.tests.test_dataframe.test_describe()
dask.dataframe.tests.test_dataframe.test_describe_empty()
dask.dataframe.tests.test_dataframe.test_deterministic_apply_concat_apply_names()
dask.dataframe.tests.test_dataframe.test_diff()
dask.dataframe.tests.test_dataframe.test_drop_axis_1()
dask.dataframe.tests.test_dataframe.test_drop_duplicates()
dask.dataframe.tests.test_dataframe.test_drop_duplicates_subset()
dask.dataframe.tests.test_dataframe.test_dropna()
dask.dataframe.tests.test_dataframe.test_dtype()
dask.dataframe.tests.test_dataframe.test_embarrassingly_parallel_operations()
dask.dataframe.tests.test_dataframe.test_empty_max()
dask.dataframe.tests.test_dataframe.test_empty_quantile()
dask.dataframe.tests.test_dataframe.test_eval()
dask.dataframe.tests.test_dataframe.test_ffill_bfill()
dask.dataframe.tests.test_dataframe.test_fillna()
dask.dataframe.tests.test_dataframe.test_fillna_multi_dataframe()
dask.dataframe.tests.test_dataframe.test_first_and_last(method)
dask.dataframe.tests.test_dataframe.test_get_partition()
dask.dataframe.tests.test_dataframe.test_getitem_meta()
dask.dataframe.tests.test_dataframe.test_getitem_multilevel()
dask.dataframe.tests.test_dataframe.test_gh580()
dask.dataframe.tests.test_dataframe.test_gh_1301()
dask.dataframe.tests.test_dataframe.test_gh_517()
dask.dataframe.tests.test_dataframe.test_groupby_callable()
dask.dataframe.tests.test_dataframe.test_groupby_multilevel_info()
dask.dataframe.tests.test_dataframe.test_hash_split_unique(npartitions,split_every,split_out)
dask.dataframe.tests.test_dataframe.test_head_npartitions()
dask.dataframe.tests.test_dataframe.test_head_npartitions_warn()
dask.dataframe.tests.test_dataframe.test_head_tail()
dask.dataframe.tests.test_dataframe.test_idxmaxmin(idx,skipna)
dask.dataframe.tests.test_dataframe.test_idxmaxmin_empty_partitions()
dask.dataframe.tests.test_dataframe.test_index()
dask.dataframe.tests.test_dataframe.test_index_head()
dask.dataframe.tests.test_dataframe.test_index_names()
dask.dataframe.tests.test_dataframe.test_index_time_properties()
dask.dataframe.tests.test_dataframe.test_info()
dask.dataframe.tests.test_dataframe.test_inplace_operators()
dask.dataframe.tests.test_dataframe.test_isin()
dask.dataframe.tests.test_dataframe.test_known_divisions()
dask.dataframe.tests.test_dataframe.test_len()
dask.dataframe.tests.test_dataframe.test_map()
dask.dataframe.tests.test_dataframe.test_map_partitions()
dask.dataframe.tests.test_dataframe.test_map_partitions_column_info()
dask.dataframe.tests.test_dataframe.test_map_partitions_keeps_kwargs_in_dict()
dask.dataframe.tests.test_dataframe.test_map_partitions_method_names()
dask.dataframe.tests.test_dataframe.test_map_partitions_multi_argument()
dask.dataframe.tests.test_dataframe.test_map_partitions_names()
dask.dataframe.tests.test_dataframe.test_memory_usage(index,deep)
dask.dataframe.tests.test_dataframe.test_methods_tokenize_differently()
dask.dataframe.tests.test_dataframe.test_nbytes()
dask.dataframe.tests.test_dataframe.test_ndim()
dask.dataframe.tests.test_dataframe.test_nlargest_nsmallest()
dask.dataframe.tests.test_dataframe.test_pipe()
dask.dataframe.tests.test_dataframe.test_quantile()
dask.dataframe.tests.test_dataframe.test_query()
dask.dataframe.tests.test_dataframe.test_random_partitions()
dask.dataframe.tests.test_dataframe.test_reduction_method()
dask.dataframe.tests.test_dataframe.test_reduction_method_split_every()
dask.dataframe.tests.test_dataframe.test_rename_columns()
dask.dataframe.tests.test_dataframe.test_rename_dict()
dask.dataframe.tests.test_dataframe.test_rename_function()
dask.dataframe.tests.test_dataframe.test_rename_index()
dask.dataframe.tests.test_dataframe.test_rename_series()
dask.dataframe.tests.test_dataframe.test_repartition()
dask.dataframe.tests.test_dataframe.test_repartition_divisions()
dask.dataframe.tests.test_dataframe.test_repartition_freq(npartitions,freq,start,end)
dask.dataframe.tests.test_dataframe.test_repartition_freq_divisions()
dask.dataframe.tests.test_dataframe.test_repartition_freq_errors()
dask.dataframe.tests.test_dataframe.test_repartition_npartitions(use_index,n,k,dtype,transform)
dask.dataframe.tests.test_dataframe.test_repartition_npartitions_same_limits()
dask.dataframe.tests.test_dataframe.test_repartition_object_index()
dask.dataframe.tests.test_dataframe.test_repartition_on_pandas_dataframe()
dask.dataframe.tests.test_dataframe.test_reset_index()
dask.dataframe.tests.test_dataframe.test_round()
dask.dataframe.tests.test_dataframe.test_sample()
dask.dataframe.tests.test_dataframe.test_sample_without_replacement()
dask.dataframe.tests.test_dataframe.test_select_dtypes(include,exclude)
dask.dataframe.tests.test_dataframe.test_series_iteritems()
dask.dataframe.tests.test_dataframe.test_series_round()
dask.dataframe.tests.test_dataframe.test_shift()
dask.dataframe.tests.test_dataframe.test_shift_with_freq()
dask.dataframe.tests.test_dataframe.test_size()
dask.dataframe.tests.test_dataframe.test_slice_on_filtered_boundary(drop)
dask.dataframe.tests.test_dataframe.test_split_out_drop_duplicates(split_every)
dask.dataframe.tests.test_dataframe.test_split_out_value_counts(split_every)
dask.dataframe.tests.test_dataframe.test_str_accessor()
dask.dataframe.tests.test_dataframe.test_timeseries_sorted()
dask.dataframe.tests.test_dataframe.test_timezone_freq(npartitions)
dask.dataframe.tests.test_dataframe.test_to_datetime()
dask.dataframe.tests.test_dataframe.test_to_frame()
dask.dataframe.tests.test_dataframe.test_to_timedelta()
dask.dataframe.tests.test_dataframe.test_to_timestamp()
dask.dataframe.tests.test_dataframe.test_unique()
dask.dataframe.tests.test_dataframe.test_unknown_divisions()
dask.dataframe.tests.test_dataframe.test_value_counts()
dask.dataframe.tests.test_dataframe.test_values()
dask.dataframe.tests.test_dataframe.test_where_mask()
dask.dataframe.tests.test_dataframe.test_with_boundary(start,stop,right_boundary,left_boundary,drop)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/dataframe/tests/test_format.py----------------------------------------
A:dask.dataframe.tests.test_format.df->pandas.DataFrame({'A': [1, 2, 3, 4, 5, 6, 7, 8] * 10, 'B': list('ABCDEFGH') * 10, 'C': pd.Categorical(list('AAABBBCC') * 10)})
A:dask.dataframe.tests.test_format.ddf->dask.dataframe.from_pandas(df, 10)
A:dask.dataframe.tests.test_format.s1->repr(ddf)
A:dask.dataframe.tests.test_format.ddf.b->dask.dataframe.from_pandas(df, 10).b.astype('category')
A:dask.dataframe.tests.test_format.exp->u'<div><strong>Dask DataFrame Structure:</strong></div>\n<div>\n{style}{exp_table}\n</div>\n<div>Dask Name: from_pandas, 10 tasks</div>'.format(style=style, exp_table=exp_table)
A:dask.dataframe.tests.test_format.s->pandas.Series(['a', 'b', 'c']).astype('category')
A:dask.dataframe.tests.test_format.ds->dask.dataframe.from_pandas(s, 3)
A:dask.dataframe.tests.test_format.known->dask.dataframe.from_pandas(s, npartitions=1)
A:dask.dataframe.tests.test_format.unknown->dask.dataframe.from_pandas(s, npartitions=1).cat.as_unknown()
dask.dataframe.tests.test_format.test_categorical_format()
dask.dataframe.tests.test_format.test_dataframe_format()
dask.dataframe.tests.test_format.test_dataframe_format_long()
dask.dataframe.tests.test_format.test_dataframe_format_unknown_divisions()
dask.dataframe.tests.test_format.test_dataframe_format_with_index()
dask.dataframe.tests.test_format.test_index_format()
dask.dataframe.tests.test_format.test_repr()
dask.dataframe.tests.test_format.test_repr_meta_mutation()
dask.dataframe.tests.test_format.test_series_format()
dask.dataframe.tests.test_format.test_series_format_long()


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/dataframe/tests/test_hashing.py----------------------------------------
A:dask.dataframe.tests.test_hashing.a->hash_pandas_object(obj)
A:dask.dataframe.tests.test_hashing.b->hash_pandas_object(obj)
A:dask.dataframe.tests.test_hashing.s2->s1.astype('category').cat.set_categories(s1)
A:dask.dataframe.tests.test_hashing.s3->s1.astype('category').cat.set_categories(s1).cat.set_categories(list(reversed(s1)))
A:dask.dataframe.tests.test_hashing.h1->hash_pandas_object(s1, categorize=categorize)
A:dask.dataframe.tests.test_hashing.h2->hash_pandas_object(s.iloc[:3])
A:dask.dataframe.tests.test_hashing.h3->hash_pandas_object(s3, categorize=categorize)
A:dask.dataframe.tests.test_hashing.s->pandas.Series(['a', 'b', 'c', None])
dask.dataframe.tests.test_hashing.test_categorical_consistency()
dask.dataframe.tests.test_hashing.test_hash_pandas_object(obj)
dask.dataframe.tests.test_hashing.test_object_missing_values()


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/dataframe/tests/test_arithmetics_reduction.py----------------------------------------
A:dask.dataframe.tests.test_arithmetics_reduction.meta->make_meta({'a': 'i8', 'b': 'i8'}, index=pd.Index([], 'i8'))
A:dask.dataframe.tests.test_arithmetics_reduction.ddf1->dask.dataframe.DataFrame(dsk, 'x', meta, [0, 4, 9, 9])
A:dask.dataframe.tests.test_arithmetics_reduction.pdf1->dask.dataframe.DataFrame(dsk, 'x', meta, [0, 4, 9, 9]).compute()
A:dask.dataframe.tests.test_arithmetics_reduction.pdf2->pandas.DataFrame(np.random.randn(10, 4), index=list('abcdefghjk'), columns=list('ABCX'))
A:dask.dataframe.tests.test_arithmetics_reduction.pdf3->pandas.DataFrame({'a': [1, 2, 3, 4, 5], 'b': [3, 5, 2, 5, 7]}, index=[1, 2, 3, 4, 5])
A:dask.dataframe.tests.test_arithmetics_reduction.ddf2->dask.dataframe.from_pandas(pdf2, 2)
A:dask.dataframe.tests.test_arithmetics_reduction.ddf3->dask.dataframe.from_pandas(pdf3, 2)
A:dask.dataframe.tests.test_arithmetics_reduction.ddf4->dask.dataframe.from_pandas(pdf4, 2)
A:dask.dataframe.tests.test_arithmetics_reduction.pdf4->pandas.DataFrame({'a': [3, 2, 6, 7, 8], 'b': [9, 4, 2, 6, 2]}, index=[10, 11, 12, 13, 14])
A:dask.dataframe.tests.test_arithmetics_reduction.pdf5->pandas.DataFrame({'a': [1, 2, 3, 4, 5], 'b': [3, 5, 2, 5, 7]}, index=[1, 3, 5, 7, 9])
A:dask.dataframe.tests.test_arithmetics_reduction.ddf5->dask.dataframe.from_pandas(pdf5, 2)
A:dask.dataframe.tests.test_arithmetics_reduction.pdf6->pandas.DataFrame({'a': [3, 2, 6, 7, 8], 'b': [9, 4, 2, 6, 2]}, index=[2, 3, 4, 5, 6])
A:dask.dataframe.tests.test_arithmetics_reduction.ddf6->dask.dataframe.from_pandas(pdf6, 2)
A:dask.dataframe.tests.test_arithmetics_reduction.pdf7->pandas.DataFrame({'a': [1, 2, 3, 4, 5, 6, 7, 8], 'b': [5, 6, 7, 8, 1, 2, 3, 4]}, index=[0, 2, 4, 8, 9, 10, 11, 13])
A:dask.dataframe.tests.test_arithmetics_reduction.pdf8->pandas.DataFrame({'a': [5, 6, 7, 8, 4, 3, 2, 1], 'b': [2, 4, 5, 3, 4, 2, 1, 0]}, index=[1, 3, 4, 8, 9, 11, 12, 13])
A:dask.dataframe.tests.test_arithmetics_reduction.ddf7->dask.dataframe.from_pandas(pdf7, 3)
A:dask.dataframe.tests.test_arithmetics_reduction.ddf8->dask.dataframe.from_pandas(pdf8, 2)
A:dask.dataframe.tests.test_arithmetics_reduction.pdf9->pandas.DataFrame({'a': [1, 2, 3, 4, 5, 6, 7, 8], 'b': [5, 6, 7, 8, 1, 2, 3, 4]}, index=[0, 2, 4, 8, 9, 10, 11, 13])
A:dask.dataframe.tests.test_arithmetics_reduction.pdf10->pandas.DataFrame({'a': [5, 6, 7, 8, 4, 3, 2, 1], 'b': [2, 4, 5, 3, 4, 2, 1, 0]}, index=[0, 3, 4, 8, 9, 11, 12, 13])
A:dask.dataframe.tests.test_arithmetics_reduction.ddf9->dask.dataframe.from_pandas(pdf9, 3)
A:dask.dataframe.tests.test_arithmetics_reduction.ddf10->dask.dataframe.from_pandas(pdf10, 2)
A:dask.dataframe.tests.test_arithmetics_reduction.df->pandas.DataFrame({'a': [1, 2, np.nan, 4, 5, 6, 7, 8], 'b': [1, 2, np.nan, np.nan, np.nan, 5, np.nan, np.nan], 'c': [np.nan] * 8})
A:dask.dataframe.tests.test_arithmetics_reduction.a->dask.dataframe.from_pandas(df, npartitions=2)
A:dask.dataframe.tests.test_arithmetics_reduction.el->numpy.int64(10)
A:dask.dataframe.tests.test_arithmetics_reduction.er->numpy.int64(4)
A:dask.dataframe.tests.test_arithmetics_reduction.l->dask.dataframe.core.Scalar({('l', 0): el}, 'l', 'i8')
A:dask.dataframe.tests.test_arithmetics_reduction.r->dask.dataframe.core.Scalar({('r', 0): er}, 'r', 'i8')
A:dask.dataframe.tests.test_arithmetics_reduction.s->dask.dataframe.core.Scalar({('s', 0): 4}, 's', 'i8')
A:dask.dataframe.tests.test_arithmetics_reduction.pds->pandas.Series(pd.timedelta_range('1 days', freq='D', periods=5))
A:dask.dataframe.tests.test_arithmetics_reduction.dds->dask.dataframe.from_pandas(pds, 2)
A:dask.dataframe.tests.test_arithmetics_reduction.pdf->pandas.DataFrame({'a': [1, 2, 3, 4, 5, 6, 7], 'b': [7, 6, 5, 4, 3, 2, 1]})
A:dask.dataframe.tests.test_arithmetics_reduction.ddf->dask.dataframe.from_pandas(df, 3)
A:dask.dataframe.tests.test_arithmetics_reduction.ps3->pandas.Series(np.random.randn(10), index=list('ABCDXabcde'))
A:dask.dataframe.tests.test_arithmetics_reduction.nans1->pandas.Series([1] + [np.nan] * 4 + [2] + [np.nan] * 3)
A:dask.dataframe.tests.test_arithmetics_reduction.nands1->dask.dataframe.from_pandas(nans1, 2)
A:dask.dataframe.tests.test_arithmetics_reduction.nans2->pandas.Series([1] + [np.nan] * 8)
A:dask.dataframe.tests.test_arithmetics_reduction.nands2->dask.dataframe.from_pandas(nans2, 2)
A:dask.dataframe.tests.test_arithmetics_reduction.nans3->pandas.Series([np.nan] * 9)
A:dask.dataframe.tests.test_arithmetics_reduction.nands3->dask.dataframe.from_pandas(nans3, 2)
A:dask.dataframe.tests.test_arithmetics_reduction.bools->pandas.Series([True, False, True, False, True], dtype=bool)
A:dask.dataframe.tests.test_arithmetics_reduction.boolds->dask.dataframe.from_pandas(bools, 2)
dask.dataframe.tests.test_arithmetics_reduction.check_frame_arithmetics(l,r,el,er,allow_comparison_ops=True)
dask.dataframe.tests.test_arithmetics_reduction.check_series_arithmetics(l,r,el,er,allow_comparison_ops=True)
dask.dataframe.tests.test_arithmetics_reduction.test_allany(split_every)
dask.dataframe.tests.test_arithmetics_reduction.test_arithmetics()
dask.dataframe.tests.test_arithmetics_reduction.test_arithmetics_different_index()
dask.dataframe.tests.test_arithmetics_reduction.test_deterministic_arithmetic_names()
dask.dataframe.tests.test_arithmetics_reduction.test_deterministic_reduction_names(split_every)
dask.dataframe.tests.test_arithmetics_reduction.test_frame_series_arithmetic_methods()
dask.dataframe.tests.test_arithmetics_reduction.test_reduction_series_invalid_axis()
dask.dataframe.tests.test_arithmetics_reduction.test_reductions(split_every)
dask.dataframe.tests.test_arithmetics_reduction.test_reductions_frame(split_every)
dask.dataframe.tests.test_arithmetics_reduction.test_reductions_frame_dtypes()
dask.dataframe.tests.test_arithmetics_reduction.test_reductions_frame_nan(split_every)
dask.dataframe.tests.test_arithmetics_reduction.test_reductions_non_numeric_dtypes()
dask.dataframe.tests.test_arithmetics_reduction.test_scalar_arithmetics()
dask.dataframe.tests.test_arithmetics_reduction.test_scalar_arithmetics_with_dask_instances()


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/dataframe/tests/test_shuffle.py----------------------------------------
A:dask.dataframe.tests.test_shuffle.meta->make_meta({'a': 'i8', 'b': 'i8'}, index=pd.Index([], 'i8'))
A:dask.dataframe.tests.test_shuffle.d->dask.dataframe.from_pandas(df, 2)
A:dask.dataframe.tests.test_shuffle.full->dask.dataframe.from_pandas(df, 2).compute()
A:dask.dataframe.tests.test_shuffle.s->shuffle(ddf, ddf.x, npartitions=6, shuffle=method)
A:dask.dataframe.tests.test_shuffle.x->dask.get(s.dask, (s._name, 0))
A:dask.dataframe.tests.test_shuffle.y->dask.get(s.dask, (s._name, 1))
A:dask.dataframe.tests.test_shuffle.df->pandas.DataFrame({'a': list(range(10))})
A:dask.dataframe.tests.test_shuffle.ddf->ddf.set_index('c').set_index('c')
A:dask.dataframe.tests.test_shuffle.sc->shuffle(ddf, ddf.x, npartitions=6, shuffle=method).compute(get=dask.get)
A:dask.dataframe.tests.test_shuffle.res1->shuffle(d, d[['b']], shuffle=method).compute()
A:dask.dataframe.tests.test_shuffle.res2->partitioning_index(df2, 5)
A:dask.dataframe.tests.test_shuffle.res3->shuffle(d, 'b', shuffle=method).compute()
A:dask.dataframe.tests.test_shuffle.a->dask.dataframe.from_pandas(df, 2, sort=False)
A:dask.dataframe.tests.test_shuffle.b->copy(a)
A:dask.dataframe.tests.test_shuffle.parts->get(result.dask, result._keys())
A:dask.dataframe.tests.test_shuffle.df2->pandas.DataFrame({'a': list(range(10))}).set_index('y', sorted=True)
A:dask.dataframe.tests.test_shuffle.res->ddf.set_index('c').set_index('c').set_index('y', divisions=['a', 'b', 'd', 'e'], sorted=True)
A:dask.dataframe.tests.test_shuffle.df.a->pandas.DataFrame({'a': list(range(10))}).a.astype('category')
A:dask.dataframe.tests.test_shuffle.df2.a->pandas.DataFrame({'a': list(range(10))}).set_index('y', sorted=True).a.cat.set_categories(list(reversed(df2.a.cat.categories)))
A:dask.dataframe.tests.test_shuffle.ddf2->ddf.set_index('c').set_index('c').set_index('x', shuffle='disk')
A:dask.dataframe.tests.test_shuffle.result->ddf.set_index('c').set_index('c').set_index('y', divisions=['a', 'c', 'd'])
A:dask.dataframe.tests.test_shuffle.A->pandas.DataFrame({'x': [1, 2, 3, 4, 5, 6], 'y': [1, 1, 2, 2, 3, 4]})
A:dask.dataframe.tests.test_shuffle.f->maybe_buffered_partd()
A:dask.dataframe.tests.test_shuffle.p1->pandas.DataFrame({'x': [10, 11, 12], 'y': ['a', 'a', 'a']})
A:dask.dataframe.tests.test_shuffle.f2->pickle.loads(pickle.dumps(f))
A:dask.dataframe.tests.test_shuffle.p2->pandas.DataFrame({'x': [13, 14, 15], 'y': ['b', 'b', 'c']})
A:dask.dataframe.tests.test_shuffle.d2->dask.dataframe.from_pandas(df, 2).set_index('tz', npartitions=2)
A:dask.dataframe.tests.test_shuffle.d3->dask.dataframe.from_pandas(df, 2).set_index(d.b, npartitions=3)
A:dask.dataframe.tests.test_shuffle.d4->dask.dataframe.from_pandas(df, 2).set_index('b')
A:dask.dataframe.tests.test_shuffle.d5->dask.dataframe.from_pandas(df, 2).set_index(d.b, divisions=[0, 2, 9], compute=True)
A:dask.dataframe.tests.test_shuffle.exp->dask.dataframe.from_pandas(df, 2).compute().copy()
A:dask.dataframe.tests.test_shuffle.p3->pandas.DataFrame({'x': [16, 17, 18], 'y': ['d', 'e', 'e']})
A:dask.dataframe.tests.test_shuffle.vals->pandas.to_datetime(vals, unit='ns')
A:dask.dataframe.tests.test_shuffle.lo->sum(breaks[:i])
A:dask.dataframe.tests.test_shuffle.hi->sum(breaks[i:i + 1])
A:dask.dataframe.tests.test_shuffle.d1->dask.dataframe.from_pandas(df, 2).set_index('notz', npartitions=2)
A:dask.dataframe.tests.test_shuffle.L->sorted(list(range(0, 200, 10)) * 2)
A:dask.dataframe.tests.test_shuffle.s_naive->pandas.Series(pd.date_range('20130101', periods=3))
A:dask.dataframe.tests.test_shuffle.s_aware->pandas.Series(pd.date_range('20130101', periods=3, tz='US/Eastern'))
A:dask.dataframe.tests.test_shuffle.s1->pandas.DatetimeIndex(s_naive.values, dtype=s_naive.dtype)
A:dask.dataframe.tests.test_shuffle.s2->pandas.DatetimeIndex(s_aware, dtype=s_aware.dtype)
A:dask.dataframe.tests.test_shuffle.s2badtype->pandas.DatetimeIndex(s_aware.values, dtype=s_naive.dtype)
A:dask.dataframe.tests.test_shuffle.pdf->pandas.DataFrame({0: list('ABAABBABAA'), 1: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 2: [1, 2, 3, 2, 1, 3, 2, 4, 2, 3]})
A:dask.dataframe.tests.test_shuffle.aa->delayed(a)
A:dask.dataframe.tests.test_shuffle.bb->delayed(b)
A:dask.dataframe.tests.test_shuffle.divisions->compute_divisions(a)
A:dask.dataframe.tests.test_shuffle.df['c']->df['b'].astype(str)
dask.dataframe.tests.test_shuffle.test_compute_divisions()
dask.dataframe.tests.test_shuffle.test_default_partitions()
dask.dataframe.tests.test_shuffle.test_empty_partitions()
dask.dataframe.tests.test_shuffle.test_index_with_dataframe(method)
dask.dataframe.tests.test_shuffle.test_index_with_non_series(method)
dask.dataframe.tests.test_shuffle.test_maybe_buffered_partd()
dask.dataframe.tests.test_shuffle.test_partitioning_index()
dask.dataframe.tests.test_shuffle.test_partitioning_index_categorical_on_values()
dask.dataframe.tests.test_shuffle.test_rearrange(shuffle,get)
dask.dataframe.tests.test_shuffle.test_rearrange_by_column_with_narrow_divisions()
dask.dataframe.tests.test_shuffle.test_set_index()
dask.dataframe.tests.test_shuffle.test_set_index_detects_sorted_data(shuffle)
dask.dataframe.tests.test_shuffle.test_set_index_divisions_2()
dask.dataframe.tests.test_shuffle.test_set_index_divisions_compute()
dask.dataframe.tests.test_shuffle.test_set_index_divisions_sorted()
dask.dataframe.tests.test_shuffle.test_set_index_doesnt_increase_partitions(shuffle)
dask.dataframe.tests.test_shuffle.test_set_index_drop(drop)
dask.dataframe.tests.test_shuffle.test_set_index_interpolate()
dask.dataframe.tests.test_shuffle.test_set_index_interpolate_int()
dask.dataframe.tests.test_shuffle.test_set_index_names(shuffle)
dask.dataframe.tests.test_shuffle.test_set_index_raises_error_on_bad_input()
dask.dataframe.tests.test_shuffle.test_set_index_reduces_partitions_large(shuffle)
dask.dataframe.tests.test_shuffle.test_set_index_reduces_partitions_small(shuffle)
dask.dataframe.tests.test_shuffle.test_set_index_self_index(shuffle)
dask.dataframe.tests.test_shuffle.test_set_index_sorted_min_max_same()
dask.dataframe.tests.test_shuffle.test_set_index_sorted_single_partition()
dask.dataframe.tests.test_shuffle.test_set_index_sorted_true()
dask.dataframe.tests.test_shuffle.test_set_index_sorts()
dask.dataframe.tests.test_shuffle.test_set_index_tasks(npartitions)
dask.dataframe.tests.test_shuffle.test_set_index_tasks_2(shuffle)
dask.dataframe.tests.test_shuffle.test_set_index_tasks_3(shuffle)
dask.dataframe.tests.test_shuffle.test_set_index_timezone()
dask.dataframe.tests.test_shuffle.test_set_index_with_explicit_divisions()
dask.dataframe.tests.test_shuffle.test_shuffle(shuffle)
dask.dataframe.tests.test_shuffle.test_shuffle_empty_partitions(method)
dask.dataframe.tests.test_shuffle.test_shuffle_from_one_partition_to_one_other(method)
dask.dataframe.tests.test_shuffle.test_shuffle_npartitions_task()
dask.dataframe.tests.test_shuffle.test_shuffle_sort(shuffle)
dask.dataframe.tests.test_shuffle.test_temporary_directory(tmpdir)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/dataframe/tests/test_optimize_dataframe.py----------------------------------------
A:dask.dataframe.tests.test_optimize_dataframe.dfs->list(dsk.values())
A:dask.dataframe.tests.test_optimize_dataframe.bcolz->pytest.importorskip('bcolz')
A:dask.dataframe.tests.test_optimize_dataframe.bc->pytest.importorskip('bcolz').ctable([[1, 2, 3], [10, 20, 30]], names=['a', 'b'])
A:dask.dataframe.tests.test_optimize_dataframe.dsk2->merge(dict(((('x', i), (dataframe_from_ctable, bc, slice(0, 2), cols, {})) for i in [1, 2, 3])), dict(((('y', i), (getitem, ('x', i), ['a', 'b'])) for i in [1, 2, 3])))
A:dask.dataframe.tests.test_optimize_dataframe.expected->dict(((('y', i), (dataframe_from_ctable, bc, slice(0, 2), ['a', 'b'], {})) for i in [1, 2, 3]))
A:dask.dataframe.tests.test_optimize_dataframe.result->dask.dataframe.optimize(dsk2, [('y', i) for i in [1, 2, 3]])
A:dask.dataframe.tests.test_optimize_dataframe.df->dask.dataframe.from_pandas(df, npartitions=5)
A:dask.dataframe.tests.test_optimize_dataframe.a->s._optimize(s.dask, s._keys())
A:dask.dataframe.tests.test_optimize_dataframe.b->s._optimize(s.dask, s._keys())
dask.dataframe.tests.test_optimize_dataframe.test_column_optimizations_with_bcolz_and_rewrite()
dask.dataframe.tests.test_optimize_dataframe.test_fuse_ave_width()


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/dataframe/tests/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/dataframe/tests/test_multi.py----------------------------------------
A:dask.dataframe.tests.test_multi.A->pandas.DataFrame({'x': [1, 2, 3, 4, 5, 6], 'y': [1, 1, 2, 2, 3, 4]})
A:dask.dataframe.tests.test_multi.a->dask.dataframe.DataFrame(dsk, 'x', meta, [None, None])
A:dask.dataframe.tests.test_multi.B->pandas.DataFrame({'y': [1, 3, 4, 4, 5, 6], 'z': [6, 5, 4, 3, 2, 1]})
A:dask.dataframe.tests.test_multi.b->dask.dataframe.DataFrame(dsk, 'y', meta, [None, None])
A:dask.dataframe.tests.test_multi.s->pandas.Series([7, 8], name=6, index=['a', 'b'])
A:dask.dataframe.tests.test_multi.((aa, bb), divisions, L)->align_partitions(a, b)
A:dask.dataframe.tests.test_multi.((aa, ss, bb), divisions, L)->align_partitions(a, s, b)
A:dask.dataframe.tests.test_multi.ldf->pandas.DataFrame({'a': [1, 2, 3, 4, 5, 6, 7], 'b': [7, 6, 5, 4, 3, 2, 1]}, index=list('abcdefg'))
A:dask.dataframe.tests.test_multi.rdf->pandas.DataFrame({'c': [1, 2, 3, 4, 5, 6, 7], 'd': [7, 6, 5, 4, 3, 2, 1]}, index=list('fghijkl'))
A:dask.dataframe.tests.test_multi.((lresult, rresult), div, parts)->align_partitions(lhs, rhs)
A:dask.dataframe.tests.test_multi.df->pandas.DataFrame({'x': [1, 1, 1]}, index=[1, 2, 3])
A:dask.dataframe.tests.test_multi.ddf->dask.dataframe.from_pandas(df, npartitions=2)
A:dask.dataframe.tests.test_multi.ddf2->dask.dataframe.from_pandas(df, npartitions=2).set_index('x')
A:dask.dataframe.tests.test_multi.(a, b)->_maybe_align_partitions([ddf, ddf2])
A:dask.dataframe.tests.test_multi.c->dask.dataframe.DataFrame(dsk, 'y', meta, [None, None])
A:dask.dataframe.tests.test_multi.result->dask.dataframe.concat([ddf1, ddf2, ddf3])
A:dask.dataframe.tests.test_multi.expected->dask.dataframe.DataFrame(dsk, 'x', meta, [None, None]).join(b, how='inner')
A:dask.dataframe.tests.test_multi.pdf1->pandas.DataFrame(np.random.randn(7, 5), columns=list('ABCDE'), index=list('abcdefg'))
A:dask.dataframe.tests.test_multi.ddf1->dask.dataframe.DataFrame(dsk, 'x', meta, [None, None])
A:dask.dataframe.tests.test_multi.pdf2->pandas.DataFrame(np.random.randn(7, 6), columns=list('FGHIJK'), index=list('abcdefg'))
A:dask.dataframe.tests.test_multi.pdf3->pandas.DataFrame(np.random.randn(7, 6), columns=list('FGHIJK'), index=list('cdefghi'))
A:dask.dataframe.tests.test_multi.ddf3->dask.dataframe.DataFrame(dsk, 'y', meta, [None, None])
A:dask.dataframe.tests.test_multi.aa->dask.dataframe.from_pandas(a, npartitions=2, sort=False)
A:dask.dataframe.tests.test_multi.bb->dask.dataframe.from_pandas(b, npartitions=2, sort=False)
A:dask.dataframe.tests.test_multi.cc->dask.dataframe.from_pandas(b, npartitions=1, sort=False)
A:dask.dataframe.tests.test_multi.pdf1l->pandas.DataFrame({'a': list('abcdefghij'), 'b': list('abcdefghij'), 'c': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}, index=list('abcdefghij'))
A:dask.dataframe.tests.test_multi.pdf1r->pandas.DataFrame({'d': list('abcdefghij'), 'e': list('abcdefghij'), 'f': [10, 9, 8, 7, 6, 5, 4, 3, 2, 1]}, index=list('abcdefghij'))
A:dask.dataframe.tests.test_multi.pdf2l->pandas.DataFrame({'a': list('abcdeabcde'), 'b': list('abcabcabca'), 'c': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}, index=list('abcdefghij'))
A:dask.dataframe.tests.test_multi.pdf2r->pandas.DataFrame({'d': list('edcbaedcba'), 'e': list('aaabbbcccd'), 'f': [10, 9, 8, 7, 6, 5, 4, 3, 2, 1]}, index=list('fghijklmno'))
A:dask.dataframe.tests.test_multi.pdf3r->pandas.DataFrame({'d': list('aaabbbccaa'), 'e': list('abbbbbbbbb'), 'f': [10, 9, 8, 7, 6, 5, 4, 3, 2, 1]}, index=list('ABCDEFGHIJ'))
A:dask.dataframe.tests.test_multi.pdf4r->pandas.DataFrame({'c': list('abda'), 'd': [5, 4, 3, 2]}, index=list('abdg'))
A:dask.dataframe.tests.test_multi.pdf5l->pandas.DataFrame({'a': list('lmnopqr'), 'b': [7, 6, 5, 4, 3, 2, 1]}, index=list('lmnopqr'))
A:dask.dataframe.tests.test_multi.pdf5r->pandas.DataFrame({'c': list('abcd'), 'd': [5, 4, 3, 2]}, index=list('abcd'))
A:dask.dataframe.tests.test_multi.pdf6l->pandas.DataFrame({'a': list('cdefghi'), 'b': [7, 6, 5, 4, 3, 2, 1]}, index=list('cdefghi'))
A:dask.dataframe.tests.test_multi.pdf6r->pandas.DataFrame({'c': list('abab'), 'd': [5, 4, 3, 2]}, index=list('abcd'))
A:dask.dataframe.tests.test_multi.pdf7l->pandas.DataFrame({'a': list('aabbccd'), 'b': [7, 6, 5, 4, 3, 2, 1]}, index=list('abcdefg'))
A:dask.dataframe.tests.test_multi.pdf7r->pandas.DataFrame({'c': list('aabb'), 'd': [5, 4, 3, 2]}, index=list('fghi'))
A:dask.dataframe.tests.test_multi.ddl->dask.dataframe.from_pandas(pdl, lpart)
A:dask.dataframe.tests.test_multi.ddr->dask.dataframe.from_pandas(pdr, rpart)
A:dask.dataframe.tests.test_multi.pdf4l->pandas.DataFrame({'a': list('abcabce'), 'b': [7, 6, 5, 4, 3, 2, 1]}, index=list('abcdefg'))
A:dask.dataframe.tests.test_multi.pdf3l->pandas.DataFrame({'a': list('aaaaaaaaaa'), 'b': list('aaaaaaaaaa'), 'c': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}, index=list('abcdefghij'))
A:dask.dataframe.tests.test_multi.pdf->pandas.DataFrame({'A': list('abcd') * 5, 'B': list('XY') * 10, 'C': np.random.randn(20)})
A:dask.dataframe.tests.test_multi.da->dask.dataframe.from_pandas(a, npartitions=3)
A:dask.dataframe.tests.test_multi.dc->dask.dataframe.from_pandas(a, npartitions=3).merge(b, on='x', how='inner')
A:dask.dataframe.tests.test_multi.actual->dask.dataframe.from_pandas(b, npartitions=2, sort=False).merge(aa, right_index=True, left_on='x', how='inner')
A:dask.dataframe.tests.test_multi.lhs->pandas.DataFrame({'A': [1, 2, 3], 'B': list('abc'), 'C': 'foo', 'D': 1.0}, columns=list('DCBA'))
A:dask.dataframe.tests.test_multi.rhs->pandas.DataFrame({'G': [4, 5], 'H': 6.0, 'I': 'bar', 'B': list('ab')}, columns=list('GHIB'))
A:dask.dataframe.tests.test_multi.merged->dask.dataframe.merge(ddf, rhs, on='B').compute()
A:dask.dataframe.tests.test_multi.meta->make_meta({'b': 'i8', 'c': 'i8'})
A:dask.dataframe.tests.test_multi.d->dask.dataframe.DataFrame(dsk, 'y', meta, [0, 3, 5])
A:dask.dataframe.tests.test_multi.pdf4->pandas.DataFrame(np.random.randn(7, 5), columns=list('FGHAB'), index=list('cdefghi'))
A:dask.dataframe.tests.test_multi.pdf5->pandas.DataFrame(np.random.randn(7, 5), columns=list('FGHAB'), index=list('fklmnop'))
A:dask.dataframe.tests.test_multi.ddf4->dask.dataframe.from_pandas(df4, 2)
A:dask.dataframe.tests.test_multi.ddf5->dask.dataframe.from_pandas(pdf5, 3)
A:dask.dataframe.tests.test_multi.df.w->pandas.DataFrame({'x': [1, 1, 1]}, index=[1, 2, 3]).w.astype('category')
A:dask.dataframe.tests.test_multi.df.y->pandas.DataFrame({'x': [1, 1, 1]}, index=[1, 2, 3]).y.astype('category')
A:dask.dataframe.tests.test_multi.dframes[0]._meta->clear_known_categories(dframes[0]._meta, ['y'], index=True)
A:dask.dataframe.tests.test_multi.sol->concat(dfs, join=join)
A:dask.dataframe.tests.test_multi.res->dask.dataframe.DataFrame(dsk, 'x', meta, [None, None]).index.append(ddf2.index)
A:dask.dataframe.tests.test_multi.df2->pandas.DataFrame({'x': [1, 1, 1]}, index=[1, 2, 3]).copy()
A:dask.dataframe.tests.test_multi.df3->pandas.DataFrame({'b': [1, 2, 3, 4, 5, 6], 'c': [1, 2, 3, 4, 5, 6]}, index=[6, 7, 8, 9, 10, 11])
A:dask.dataframe.tests.test_multi.df4->pandas.DataFrame({'a': [1, 2, 3, 4, 5, 6], 'b': [1, 2, 3, 4, 5, 6]}, index=[4, 5, 6, 7, 8, 9])
A:dask.dataframe.tests.test_multi.df2.y->pandas.DataFrame({'x': [1, 1, 1]}, index=[1, 2, 3]).copy().y.cat.set_categories(list('abc'))
A:dask.dataframe.tests.test_multi.joined->dask.dataframe.from_pandas(df, npartitions=2).set_index('x').join(ddf2, rsuffix='r')
dask.dataframe.tests.test_multi.list_eq(aa,bb)
dask.dataframe.tests.test_multi.test__maybe_align_partitions()
dask.dataframe.tests.test_multi.test_align_partitions()
dask.dataframe.tests.test_multi.test_align_partitions_unknown_divisions()
dask.dataframe.tests.test_multi.test_append()
dask.dataframe.tests.test_multi.test_append2()
dask.dataframe.tests.test_multi.test_append_categorical()
dask.dataframe.tests.test_multi.test_cheap_inner_merge_with_pandas_object()
dask.dataframe.tests.test_multi.test_cheap_single_partition_merge()
dask.dataframe.tests.test_multi.test_cheap_single_partition_merge_divisions()
dask.dataframe.tests.test_multi.test_cheap_single_partition_merge_on_index()
dask.dataframe.tests.test_multi.test_concat(join)
dask.dataframe.tests.test_multi.test_concat2()
dask.dataframe.tests.test_multi.test_concat3()
dask.dataframe.tests.test_multi.test_concat4_interleave_partitions()
dask.dataframe.tests.test_multi.test_concat5()
dask.dataframe.tests.test_multi.test_concat_categorical(known,cat_index,divisions)
dask.dataframe.tests.test_multi.test_concat_unknown_divisions()
dask.dataframe.tests.test_multi.test_concat_unknown_divisions_errors()
dask.dataframe.tests.test_multi.test_errors_for_merge_on_frame_columns()
dask.dataframe.tests.test_multi.test_half_indexed_dataframe_avoids_shuffle()
dask.dataframe.tests.test_multi.test_hash_join(how,shuffle)
dask.dataframe.tests.test_multi.test_indexed_concat(join)
dask.dataframe.tests.test_multi.test_join_by_index_patterns(how,shuffle)
dask.dataframe.tests.test_multi.test_melt()
dask.dataframe.tests.test_multi.test_merge(how,shuffle)
dask.dataframe.tests.test_multi.test_merge_by_index_patterns(how,shuffle)
dask.dataframe.tests.test_multi.test_merge_by_multiple_columns(how,shuffle)
dask.dataframe.tests.test_multi.test_merge_index_without_divisions(shuffle)
dask.dataframe.tests.test_multi.test_merge_indexed_dataframe_to_indexed_dataframe()
dask.dataframe.tests.test_multi.test_merge_maintains_columns()
dask.dataframe.tests.test_multi.test_merge_tasks_passes_through()
dask.dataframe.tests.test_multi.test_singleton_divisions()


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/dataframe/tests/test_groupby.py----------------------------------------
A:dask.dataframe.tests.test_groupby.pdf->pandas.DataFrame({'a': [1, 2, 6, 4, 4, 6, 4, 3, 7] * 10, 'b': [4, 2, 7, 3, 3, 1, 1, 1, 2] * 10, 'c': [0, 1, 2, 3, 4, 5, 6, 7, 8] * 10}, columns=['c', 'b', 'a'])
A:dask.dataframe.tests.test_groupby.ddf->dask.dataframe.from_pandas(df, npartitions=2)
A:dask.dataframe.tests.test_groupby.gp->pandas.DataFrame({'a': [1, 2, 6, 4, 4, 6, 4, 3, 7] * 10, 'b': [4, 2, 7, 3, 3, 1, 1, 1, 2] * 10, 'c': [0, 1, 2, 3, 4, 5, 6, 7, 8] * 10}, columns=['c', 'b', 'a']).groupby('y')
A:dask.dataframe.tests.test_groupby.dp->dask.dataframe.from_pandas(df, npartitions=2).groupby('y')
A:dask.dataframe.tests.test_groupby.df->pandas.DataFrame({'g': [0, 0, 1] * 3, 'b': [1, 2, 3] * 3})
A:dask.dataframe.tests.test_groupby.g->dask.dataframe.from_pandas(df, npartitions=2).groupby('a')
A:dask.dataframe.tests.test_groupby.ddf2->dask.dataframe.from_pandas(df, npartitions=2).set_index('a')
A:dask.dataframe.tests.test_groupby.pdf2->pandas.DataFrame({'a': [1, 2, 6, 4, 4, 6, 4, 3, 7] * 10, 'b': [4, 2, 7, 3, 3, 1, 1, 1, 2] * 10, 'c': [0, 1, 2, 3, 4, 5, 6, 7, 8] * 10}, columns=['c', 'b', 'a']).set_index('a')
A:dask.dataframe.tests.test_groupby.sol->call(pdf.a.groupby(pdf.b), 'var', ddof=2)
A:dask.dataframe.tests.test_groupby.res->call(ddf.a.groupby(ddf.b), 'var', ddof=2, **{keyword: 2})
A:dask.dataframe.tests.test_groupby.d->pandas.DataFrame({'g': [0, 0, 1] * 3, 'b': [1, 2, 3] * 3, 'c': [4, 5, 6] * 3})
A:dask.dataframe.tests.test_groupby.full->pandas.DataFrame({'a': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'b': [4, 5, 6, 3, 2, 1, 0, 0, 0]}, index=[0, 1, 3, 5, 6, 8, 9, 9, 9])
A:dask.dataframe.tests.test_groupby.ddgrouped->pandas.DataFrame({'g': [0, 0, 1] * 3, 'b': [1, 2, 3] * 3, 'c': [4, 5, 6] * 3}).groupby(ddkey)
A:dask.dataframe.tests.test_groupby.pdgrouped->pandas.DataFrame({'a': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'b': [4, 5, 6, 3, 2, 1, 0, 0, 0]}, index=[0, 1, 3, 5, 6, 8, 9, 9, 9]).groupby(pdkey)
A:dask.dataframe.tests.test_groupby.strings->list('aaabbccccdddeee')
A:dask.dataframe.tests.test_groupby.data->list(map(int, '123111223323412'))
A:dask.dataframe.tests.test_groupby.ps->pandas.DataFrame(dict(strings=strings, data=data))
A:dask.dataframe.tests.test_groupby.s->pandas.Series([1, 2, 2, 1, 1])
A:dask.dataframe.tests.test_groupby.expected->pandas.DataFrame({'g': [0, 0, 1] * 3, 'b': [1, 2, 3] * 3, 'c': [4, 5, 6] * 3}).groupby('g').aggregate({'b': 'mean', 'c': 'sum'})
A:dask.dataframe.tests.test_groupby.result->dask.dataframe.from_pandas(d, npartitions=2).groupby('g').aggregate({'b': agg_func, 'c': 'sum'})
A:dask.dataframe.tests.test_groupby.pd_group->pandas.Series([1, 2, 2, 1, 1]).groupby(s)
A:dask.dataframe.tests.test_groupby.ss->dask.dataframe.from_pandas(s, npartitions=2)
A:dask.dataframe.tests.test_groupby.dask_group->dask.dataframe.from_pandas(s, npartitions=2).groupby(ss)
A:dask.dataframe.tests.test_groupby.pd_group2->pandas.Series([1, 2, 2, 1, 1]).groupby(s + 1)
A:dask.dataframe.tests.test_groupby.dask_group2->dask.dataframe.from_pandas(s, npartitions=2).groupby(ss + 1)
A:dask.dataframe.tests.test_groupby.sss->dask.dataframe.from_pandas(s, npartitions=3)
A:dask.dataframe.tests.test_groupby.a->dask.dataframe.from_pandas(d, npartitions=2)
A:dask.dataframe.tests.test_groupby.b->dask.dataframe.from_pandas(df, npartitions=2).groupby(ind(ddf)).B.apply(len)
A:dask.dataframe.tests.test_groupby.(no_mean_chunks, no_mean_aggs, no_mean_finalizers)->_build_agg_args(no_mean_spec)
A:dask.dataframe.tests.test_groupby.(with_mean_chunks, with_mean_aggs, with_mean_finalizers)->_build_agg_args(with_mean_spec)
A:dask.dataframe.tests.test_groupby.dask_holder->collections.namedtuple('dask_holder', ['dask'])
A:dask.dataframe.tests.test_groupby.result1->dask.dataframe.from_pandas(df, npartitions=2).groupby(['a', 'b']).agg(spec, split_every=2)
A:dask.dataframe.tests.test_groupby.result2->dask.dataframe.from_pandas(df, npartitions=2).groupby(['a', 'b']).agg(spec, split_every=2)
A:dask.dataframe.tests.test_groupby.agg_dask1->get_agg_dask(result1)
A:dask.dataframe.tests.test_groupby.agg_dask2->get_agg_dask(result2)
A:dask.dataframe.tests.test_groupby.other->dask.dataframe.from_pandas(df, npartitions=2).groupby(['a', 'b']).agg(other_spec, split_every=2)
A:dask.dataframe.tests.test_groupby.meta->group_and_slice(ddf, grouper)._meta.first()
A:dask.dataframe.tests.test_groupby.meta_nonempty->group_and_slice(ddf, grouper)._meta_nonempty.first().head(0)
A:dask.dataframe.tests.test_groupby.ddf3->dask.dataframe.from_pandas(pdf, npartitions=3)
A:dask.dataframe.tests.test_groupby.ddf7->dask.dataframe.from_pandas(pdf, npartitions=7)
A:dask.dataframe.tests.test_groupby.dsk->dask.dataframe.from_pandas(d, npartitions=2).groupby('g').aggregate({'b': agg_func, 'c': 'sum'})._optimize(result.dask, result._keys())
A:dask.dataframe.tests.test_groupby.(dependencies, dependents)->get_deps(dsk)
A:dask.dataframe.tests.test_groupby.ddf_group->dfiltered.groupby(ddf.a)
A:dask.dataframe.tests.test_groupby.ds_group->dfiltered.b.groupby(ddf.a)
A:dask.dataframe.tests.test_groupby.df_group->filtered.groupby(df.a)
A:dask.dataframe.tests.test_groupby.ddf_no_divs->dask.dataframe.from_pandas(df, npartitions=df.index.nunique(), sort=False)
A:dask.dataframe.tests.test_groupby.custom_mean->dask.dataframe.Aggregation('mean', lambda s: (s.count(), s.sum()), lambda s0, s1: (s0.sum(), s1.sum()), lambda s0, s1: s1 / s0)
A:dask.dataframe.tests.test_groupby.custom_sum->dask.dataframe.Aggregation('sum', lambda s: s.sum(), lambda s0: s0.sum())
A:dask.dataframe.tests.test_groupby.agg_func->dask.dataframe.Aggregation('sum', lambda s: (s.count(), s.sum()), lambda s0, s1: (s0.sum(), s1.sum()), lambda s0, s1: s1 / s0)
dask.dataframe.tests.test_groupby.groupby_error()
dask.dataframe.tests.test_groupby.groupby_internal_head()
dask.dataframe.tests.test_groupby.groupby_internal_repr()
dask.dataframe.tests.test_groupby.test_aggregate__dask()
dask.dataframe.tests.test_groupby.test_aggregate__examples(spec,split_every,grouper)
dask.dataframe.tests.test_groupby.test_aggregate__single_element_groups(spec)
dask.dataframe.tests.test_groupby.test_aggregate_build_agg_args__reuse_of_intermediates()
dask.dataframe.tests.test_groupby.test_apply_shuffle()
dask.dataframe.tests.test_groupby.test_apply_shuffle_multilevel(grouper)
dask.dataframe.tests.test_groupby.test_cumulative(func,key,sel)
dask.dataframe.tests.test_groupby.test_cumulative_axis1(func)
dask.dataframe.tests.test_groupby.test_dataframe_aggregations_multilevel(grouper,agg_func)
dask.dataframe.tests.test_groupby.test_dataframe_groupby_agg_custom_sum(pandas_spec,dask_spec,check_dtype)
dask.dataframe.tests.test_groupby.test_dataframe_groupby_nunique()
dask.dataframe.tests.test_groupby.test_dataframe_groupby_nunique_across_group_same_value()
dask.dataframe.tests.test_groupby.test_full_groupby()
dask.dataframe.tests.test_groupby.test_full_groupby_multilevel(grouper)
dask.dataframe.tests.test_groupby.test_groupby_agg_custom__name_clash_with_internal_different_column()
dask.dataframe.tests.test_groupby.test_groupby_agg_custom__name_clash_with_internal_same_column()
dask.dataframe.tests.test_groupby.test_groupby_agg_grouper_multiple(slice_)
dask.dataframe.tests.test_groupby.test_groupby_agg_grouper_single()
dask.dataframe.tests.test_groupby.test_groupby_apply_tasks()
dask.dataframe.tests.test_groupby.test_groupby_column_and_index_agg_funcs(agg_func)
dask.dataframe.tests.test_groupby.test_groupby_column_and_index_apply(group_args,apply_func)
dask.dataframe.tests.test_groupby.test_groupby_dir()
dask.dataframe.tests.test_groupby.test_groupby_get_group()
dask.dataframe.tests.test_groupby.test_groupby_index_array()
dask.dataframe.tests.test_groupby.test_groupby_meta_content(group_and_slice,grouper)
dask.dataframe.tests.test_groupby.test_groupby_multilevel_agg()
dask.dataframe.tests.test_groupby.test_groupby_multilevel_getitem()
dask.dataframe.tests.test_groupby.test_groupby_multiprocessing()
dask.dataframe.tests.test_groupby.test_groupby_normalize_index()
dask.dataframe.tests.test_groupby.test_groupby_not_supported()
dask.dataframe.tests.test_groupby.test_groupby_numeric_column()
dask.dataframe.tests.test_groupby.test_groupby_on_index(get)
dask.dataframe.tests.test_groupby.test_groupby_reduction_split(keyword)
dask.dataframe.tests.test_groupby.test_groupby_set_index()
dask.dataframe.tests.test_groupby.test_groupby_slice_agg_reduces()
dask.dataframe.tests.test_groupby.test_groupby_split_out_num()
dask.dataframe.tests.test_groupby.test_groupby_unaligned_index()
dask.dataframe.tests.test_groupby.test_groupy_non_aligned_index()
dask.dataframe.tests.test_groupby.test_groupy_series_wrong_grouper()
dask.dataframe.tests.test_groupby.test_hash_groupby_aggregate(npartitions,split_every,split_out)
dask.dataframe.tests.test_groupby.test_numeric_column_names()
dask.dataframe.tests.test_groupby.test_series_aggregate__examples(spec,split_every,grouper)
dask.dataframe.tests.test_groupby.test_series_aggregations_multilevel(grouper,agg_func)
dask.dataframe.tests.test_groupby.test_series_groupby()
dask.dataframe.tests.test_groupby.test_series_groupby_agg_custom_mean(pandas_spec,dask_spec)
dask.dataframe.tests.test_groupby.test_series_groupby_errors()
dask.dataframe.tests.test_groupby.test_series_groupby_propagates_names()
dask.dataframe.tests.test_groupby.test_split_apply_combine_on_series()
dask.dataframe.tests.test_groupby.test_split_out_multi_column_groupby()


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/dataframe/tests/test_hyperloglog.py----------------------------------------
A:dask.dataframe.tests.test_hyperloglog.rs->numpy.random.RandomState(96)
A:dask.dataframe.tests.test_hyperloglog.ddf->dask.dataframe.from_pandas(df, npartitions=npartitions)
A:dask.dataframe.tests.test_hyperloglog.approx->dask.dataframe.from_pandas(df, npartitions=npartitions).nunique_approx(split_every=split_every).compute(get=dask.local.get_sync)
A:dask.dataframe.tests.test_hyperloglog.exact->len(df.drop_duplicates())
A:dask.dataframe.tests.test_hyperloglog.df->dask.dataframe.demo.make_timeseries('2000-01-01', '2000-04-01', {'value': float, 'id': int}, freq='10s', partition_freq='1D', seed=1)
dask.dataframe.tests.test_hyperloglog.test_basic(df,npartitions)
dask.dataframe.tests.test_hyperloglog.test_larger_data()
dask.dataframe.tests.test_hyperloglog.test_split_every(split_every,npartitions)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/dataframe/io/csv.py----------------------------------------
A:dask.dataframe.io.csv.delayed->delayed(pure=True)
A:dask.dataframe.io.csv.bio->BytesIO()
A:dask.dataframe.io.csv.df->delayed_pandas_read_text(reader, blocks[0], header, kwargs, dtypes, columns, write_header=False, enforce=enforce)
A:dask.dataframe.io.csv.df[c]->df[c].astype(dtypes[c]).astype(dtypes[c])
A:dask.dataframe.io.csv.ex->'\n'.join(('- %s\n  %r' % (c, e) for (c, e) in sorted(errors, key=lambda x: str(x[0]))))
A:dask.dataframe.io.csv.bad->sorted(bad, key=lambda x: str(x[0]))
A:dask.dataframe.io.csv.table->asciitable(['Column', 'Found', 'Expected'], bad)
A:dask.dataframe.io.csv.msg->"Mismatched dtypes found in `pd.read_csv`/`pd.read_table`.\n\n{table}\n\n{exceptions}Usually this is due to dask's dtype inference failing, and\n*may* be fixed by specifying dtypes manually by adding:\n\n{dtype_kw}\n\nto the call to `read_csv`/`read_table`.{extra}".format(table=table, exceptions=exceptions, dtype_kw=dtype_kw, extra=extra)
A:dask.dataframe.io.csv.dtypes->reader(BytesIO(b_sample), **kwargs).dtypes.to_dict()
A:dask.dataframe.io.csv.columns->list(head.columns)
A:dask.dataframe.io.csv.delayed_pandas_read_text->delayed(pandas_read_text)
A:dask.dataframe.io.csv.rest_kwargs->kwargs.copy()
A:dask.dataframe.io.csv.head->reader(BytesIO(b_sample), **kwargs)
A:dask.dataframe.io.csv.blocksize->int(total_memory // cpu_count / memory_factor)
A:dask.dataframe.io.csv.CPU_COUNT->psutil.cpu_count()
A:dask.dataframe.io.csv.AUTO_BLOCKSIZE->auto_blocksize(TOTAL_MEM, CPU_COUNT)
A:dask.dataframe.io.csv.b_lineterminator->lineterminator.encode()
A:dask.dataframe.io.csv.(b_sample, values)->read_bytes(urlpath, delimiter=b_lineterminator, blocksize=blocksize, sample=sample, compression=compression, **storage_options or {})
A:dask.dataframe.io.csv.skiprows->kwargs.get('skiprows', 0)
A:dask.dataframe.io.csv.header->kwargs.get('header', 'infer')
A:dask.dataframe.io.csv.parts->b_sample.split(b_lineterminator, skiprows + need)
A:dask.dataframe.io.csv.specified_dtypes->kwargs.get('dtype', {})
A:dask.dataframe.io.csv.head[c]->head[c].astype(float).astype(float)
A:dask.dataframe.io.csv.read.__doc__->READ_DOC_TEMPLATE.format(reader=reader_name, file_type=file_type)
A:dask.dataframe.io.csv.read_csv->make_reader(pd.read_csv, 'read_csv', 'CSV')
A:dask.dataframe.io.csv.read_table->make_reader(pd.read_table, 'read_table', 'delimited')
A:dask.dataframe.io.csv.out->io.StringIO()
A:dask.dataframe.io.csv.encoding->kwargs.get('encoding', sys.getdefaultencoding())
A:dask.dataframe.io.csv.(values, names)->write_bytes(values, filename, name_function, compression, encoding=None, **storage_options or {})
dask.dataframe.io.csv._to_csv_chunk(df,**kwargs)
dask.dataframe.io.csv.auto_blocksize(total_memory,cpu_count)
dask.dataframe.io.csv.coerce_dtypes(df,dtypes)
dask.dataframe.io.csv.make_reader(reader,reader_name,file_type)
dask.dataframe.io.csv.pandas_read_text(reader,b,header,kwargs,dtypes=None,columns=None,write_header=True,enforce=False)
dask.dataframe.io.csv.read_pandas(reader,urlpath,blocksize=AUTO_BLOCKSIZE,collection=True,lineterminator=None,compression=None,sample=256000,enforce=False,assume_missing=False,storage_options=None,**kwargs)
dask.dataframe.io.csv.text_blocks_to_pandas(reader,block_lists,header,head,kwargs,collection=True,enforce=False)
dask.dataframe.io.csv.to_csv(df,filename,name_function=None,compression=None,compute=True,get=None,storage_options=None,**kwargs)
dask.dataframe.to_csv(df,filename,name_function=None,compression=None,compute=True,get=None,storage_options=None,**kwargs)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/dataframe/io/parquet.py----------------------------------------
A:dask.dataframe.io.parquet.meta->_meta_from_dtypes(all_columns, schema.names, dtypes)
A:dask.dataframe.io.parquet.columns->list(columns)
A:dask.dataframe.io.parquet.pf->fastparquet.api.ParquetFile(path, open_with=myopen, sep=sep)
A:dask.dataframe.io.parquet.index_col->fastparquet.api.ParquetFile(path, open_with=myopen, sep=sep)._get_index()
A:dask.dataframe.io.parquet.all_columns->tuple(pf.columns + list(pf.cats))
A:dask.dataframe.io.parquet.dtypes->_get_pyarrow_dtypes(schema)
A:dask.dataframe.io.parquet.meta[cat]->pandas.Series(pd.Categorical([], categories=[UNKNOWN_CATEGORIES]))
A:dask.dataframe.io.parquet.minmax->fastparquet.api.sorted_partitioned_columns(pf)
A:dask.dataframe.io.parquet.(df, views)->_pre_allocate(rg.num_rows, columns, categories, index, cs, dt)
A:dask.dataframe.io.parquet.dataset->api.ParquetDataset(paths)
A:dask.dataframe.io.parquet.schema->api.ParquetDataset(paths).schema.to_arrow_schema()
A:dask.dataframe.io.parquet.numpy_dtype->field.type.to_pandas_dtype()
A:dask.dataframe.io.parquet.table->piece.read(columns=columns, partitions=partitions, file=f)
A:dask.dataframe.io.parquet.df->df.reset_index().reset_index()
A:dask.dataframe.io.parquet.(fs, paths, file_opener)->get_fs_paths_myopen(path, None, 'rb', **storage_options or {})
A:dask.dataframe.io.parquet.(fs, paths, myopen)->get_fs_paths_myopen(path, None, 'wb', **storage_options or {})
A:dask.dataframe.io.parquet.metadata_fn->sep.join([path, '_metadata'])
A:dask.dataframe.io.parquet.fmd->fastparquet.writer.make_metadata(df._meta, has_nulls=has_nulls, fixed_text=fixed_text, object_encoding=object_encoding, index_cols=[index_col], ignore_columns=partition_on)
A:dask.dataframe.io.parquet.i_offset->fastparquet.writer.find_max_part(fmd.row_groups)
A:dask.dataframe.io.parquet.partitions->df.reset_index().reset_index().to_delayed()
A:dask.dataframe.io.parquet.writes->delayed(writes).compute()
A:dask.dataframe.io.parquet.out->delayed(_write_metadata)(writes, filenames, fmd, path, metadata_fn, myopen, sep)
A:dask.dataframe.io.parquet.fn->sep.join([path, '_common_metadata'])
dask.dataframe.io.parquet._get_pyarrow_dtypes(schema)
dask.dataframe.io.parquet._meta_from_dtypes(to_read_columns,file_columns,file_dtypes)
dask.dataframe.io.parquet._read_arrow_parquet_piece(open_file_func,piece,columns,is_series,partitions)
dask.dataframe.io.parquet._read_fastparquet(fs,paths,myopen,columns=None,filters=None,categories=None,index=None,storage_options=None)
dask.dataframe.io.parquet._read_parquet_row_group(open,fn,index,columns,rg,series,categories,schema,cs,dt,*args)
dask.dataframe.io.parquet._read_pyarrow(fs,paths,file_opener,columns=None,filters=None,categories=None,index=None)
dask.dataframe.io.parquet._write_metadata(writes,filenames,fmd,path,metadata_fn,myopen,sep)
dask.dataframe.io.parquet.read_parquet(path,columns=None,filters=None,categories=None,index=None,storage_options=None,engine='auto')
dask.dataframe.io.parquet.to_parquet(path,df,compression=None,write_index=None,has_nulls=True,fixed_text=None,object_encoding=None,storage_options=None,append=False,ignore_divisions=False,partition_on=None,compute=True)
dask.dataframe.read_parquet(path,columns=None,filters=None,categories=None,index=None,storage_options=None,engine='auto')
dask.dataframe.to_parquet(path,df,compression=None,write_index=None,has_nulls=True,fixed_text=None,object_encoding=None,storage_options=None,append=False,ignore_divisions=False,partition_on=None,compute=True)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/dataframe/io/demo.py----------------------------------------
A:dask.dataframe.io.demo.index->pandas.DatetimeIndex(start=start, end=end, freq=freq)
A:dask.dataframe.io.demo.state->numpy.random.RandomState(state_data)
A:dask.dataframe.io.demo.columns->dict(((k, make[dt](len(index), state)) for (k, dt) in dtypes.items()))
A:dask.dataframe.io.demo.df->pandas_datareader.data.DataReader(symbol, data_source, start, stop)
A:dask.dataframe.io.demo.divisions->list(pd.DatetimeIndex(start=start, end=end, freq=partition_freq))
A:dask.dataframe.io.demo.state_data->random_state_data(len(divisions) - 1, seed)
A:dask.dataframe.io.demo.head->make_timeseries_part('2000', '2000', dtypes, '1H', state_data[0])
A:dask.dataframe.io.demo.random_state->numpy.random.RandomState(random_state)
A:dask.dataframe.io.demo.date->pandas.Timestamp(date)
A:dask.dataframe.io.demo.freq->pandas.Timedelta(freq)
A:dask.dataframe.io.demo.time->pandas.date_range(date + pd.Timedelta(hours=9), date + pd.Timedelta(hours=12 + 4), freq=freq / 5, name='timestamp')
A:dask.dataframe.io.demo.n->len(time)
A:dask.dataframe.io.demo.values->(random_state.random_sample(n) - 0.5).cumsum()
A:dask.dataframe.io.demo.mx->max(close, open)
A:dask.dataframe.io.demo.mn->min(close, open)
A:dask.dataframe.io.demo.s->pandas.Series(values.round(3), index=time)
A:dask.dataframe.io.demo.rs->pandas.Series(values.round(3), index=time).resample(freq)
A:dask.dataframe.io.demo.seeds->random_state_data(len(df), random_state=random_state)
A:dask.dataframe.io.demo.part->delayed(generate_day)(s.name, s.loc['Open'], s.loc['High'], s.loc['Low'], s.loc['Close'], s.loc['Volume'], freq=freq, random_state=seed)
A:dask.dataframe.io.demo.meta->generate_day('2000-01-01', 1, 2, 0, 1, 100)
dask.dataframe.demo.daily_stock(symbol,start,stop,freq=pd.Timedelta(seconds=1),data_source='yahoo',random_state=None)
dask.dataframe.demo.generate_day(date,open,high,low,close,volume,freq=pd.Timedelta(seconds=60),random_state=None)
dask.dataframe.demo.make_categorical(n,rstate)
dask.dataframe.demo.make_float(n,rstate)
dask.dataframe.demo.make_int(n,rstate)
dask.dataframe.demo.make_string(n,rstate)
dask.dataframe.demo.make_timeseries(start,end,dtypes,freq,partition_freq,seed=None)
dask.dataframe.demo.make_timeseries_part(start,end,dtypes,freq,state_data)
dask.dataframe.io.demo.daily_stock(symbol,start,stop,freq=pd.Timedelta(seconds=1),data_source='yahoo',random_state=None)
dask.dataframe.io.demo.generate_day(date,open,high,low,close,volume,freq=pd.Timedelta(seconds=60),random_state=None)
dask.dataframe.io.demo.make_categorical(n,rstate)
dask.dataframe.io.demo.make_float(n,rstate)
dask.dataframe.io.demo.make_int(n,rstate)
dask.dataframe.io.demo.make_string(n,rstate)
dask.dataframe.io.demo.make_timeseries(start,end,dtypes,freq,partition_freq,seed=None)
dask.dataframe.io.demo.make_timeseries_part(start,end,dtypes,freq,state_data)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/dataframe/io/sql.py----------------------------------------
A:dask.dataframe.io.sql.engine->sqlalchemy.create_engine(uri)
A:dask.dataframe.io.sql.meta->sqlalchemy.MetaData()
A:dask.dataframe.io.sql.schema->kwargs.pop('schema', None)
A:dask.dataframe.io.sql.table->sqlalchemy.Table(table, meta, autoload=True, autoload_with=engine, schema=schema)
A:dask.dataframe.io.sql.q->sqlalchemy.sql.select(columns).where(sql.and_(index >= lower, cond)).select_from(table)
A:dask.dataframe.io.sql.head->pandas.read_sql(q, engine, **kwargs)
A:dask.dataframe.io.sql.minmax->pandas.read_sql(q, engine)
A:dask.dataframe.io.sql.divisions->numpy.linspace(mini, maxi, npartitions + 1).tolist()
dask.dataframe.io.sql.read_sql_table(table,uri,index_col,divisions=None,npartitions=None,limits=None,columns=None,bytes_per_chunk=256*2**20,**kwargs)
dask.dataframe.read_sql_table(table,uri,index_col,divisions=None,npartitions=None,limits=None,columns=None,bytes_per_chunk=256*2**20,**kwargs)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/dataframe/io/hdf.py----------------------------------------
A:dask.dataframe.io.hdf.pd_to_hdf->getattr(df._partition_type, 'to_hdf')
A:dask.dataframe.io.hdf.name_function->build_name_function(df.npartitions - 1)
A:dask.dataframe.io.hdf._actual_get->effective_get(get, df)
A:dask.dataframe.io.hdf.lock->get_scheduler_lock()
A:dask.dataframe.io.hdf.dsk->dict((((name, i), (_pd_read_hdf, path, key, lock, update(s))) for (i, s) in enumerate(range(start, stop, chunksize))))
A:dask.dataframe.io.hdf.i_name->name_function(i)
A:dask.dataframe.io.hdf.kwargs2->kwargs.copy()
A:dask.dataframe.io.hdf.storer->hdf.get_storer(k)
A:dask.dataframe.io.hdf.empty->pandas.read_hdf(path, key, mode=mode, stop=0)
A:dask.dataframe.io.hdf.token->tokenize((path, os.path.getmtime(path), key, start, stop, empty, chunksize, division))
A:dask.dataframe.io.hdf.new->base.copy()
A:dask.dataframe.io.hdf.(keys, stops, divisions)->get_keys_stops_divisions(path, key, stop, sorted_index)
A:dask.dataframe.io.hdf.result->pandas.read_hdf(path, key, **kwargs)
A:dask.dataframe.io.hdf.paths->sorted(glob(pattern))
dask.dataframe.io.hdf._pd_read_hdf(path,key,lock,kwargs)
dask.dataframe.io.hdf._pd_to_hdf(pd_to_hdf,lock,args,kwargs=None)
dask.dataframe.io.hdf._read_single_hdf(path,key,start=0,stop=None,columns=None,chunksize=int(1000000.0),sorted_index=False,lock=None,mode='a')
dask.dataframe.io.hdf.read_hdf(pattern,key,start=0,stop=None,columns=None,chunksize=1000000,sorted_index=False,lock=True,mode='a')
dask.dataframe.io.hdf.to_hdf(df,path,key,mode='a',append=False,get=None,name_function=None,compute=True,lock=None,dask_kwargs={},**kwargs)
dask.dataframe.read_hdf(pattern,key,start=0,stop=None,columns=None,chunksize=1000000,sorted_index=False,lock=True,mode='a')
dask.dataframe.to_hdf(df,path,key,mode='a',append=False,get=None,name_function=None,compute=True,lock=None,dask_kwargs={},**kwargs)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/dataframe/io/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/dataframe/io/io.py----------------------------------------
A:dask.dataframe.io.io.lock->Lock()
A:dask.dataframe.io.io.columns->list(columns)
A:dask.dataframe.io.io.extra->sorted(set(columns).difference(x.dtype.names))
A:dask.dataframe.io.io.meta->make_meta(meta)
A:dask.dataframe.io.io.divisions->compute_divisions(df)
A:dask.dataframe.io.io.token->tokenize((id(x), x.shape, x.dtype), chunksize, categorize, index, kwargs)
A:dask.dataframe.io.io.nrows->len(data)
A:dask.dataframe.io.io.chunksize->ceil(len(seq) / npartitions)
A:dask.dataframe.io.io.npartitions->int(ceil(nrows / chunksize))
A:dask.dataframe.io.io.data->data.sort_index(ascending=True).sort_index(ascending=True)
A:dask.dataframe.io.io.(divisions, locations)->sorted_division_locations(data.index, chunksize=chunksize)
A:dask.dataframe.io.io.dsk->merge((df.dask for df in dfs))
A:dask.dataframe.io.io.x->new_dd_object(dsk, name, meta, divs)._meta.to_records()
A:dask.dataframe.io.io.bc_chunklen->max((x[name].chunklen for name in x.names))
A:dask.dataframe.io.io.categories->dict()
A:dask.dataframe.io.io.a->dask.array.from_array(x[index], chunks=(chunksize * len(x.names),))
A:dask.dataframe.io.io.categories[name]->dask.array.unique(a)
A:dask.dataframe.io.io.result->pandas.Series(chunk, name=columns, index=idx)
A:dask.dataframe.io.io.q->numpy.linspace(0, 100, len(x) // chunksize + 2)
A:dask.dataframe.io.io.idx->pandas.Index(range(start, stop))
A:dask.dataframe.io.io.chunk->pandas.Categorical.from_codes(np.searchsorted(categories[columns], chunk), categories[columns], True)
A:dask.dataframe.io.io.divs->tuple(divisions)
A:dask.dataframe.io.io.df->new_dd_object(dsk, name, meta, divs)
dask.dataframe.from_array(x,chunksize=50000,columns=None)
dask.dataframe.from_bcolz(x,chunksize=None,categorize=True,index=None,lock=lock,**kwargs)
dask.dataframe.from_dask_array(x,columns=None)
dask.dataframe.from_delayed(dfs,meta=None,divisions=None,prefix='from-delayed')
dask.dataframe.from_pandas(data,npartitions=None,chunksize=None,sort=True,name=None)
dask.dataframe.io.dataframe_from_ctable(x,slc,columns=None,categories=None,lock=lock)
dask.dataframe.io.io._df_to_bag(df,index=False)
dask.dataframe.io.io._link(token,result)
dask.dataframe.io.io._meta_from_array(x,columns=None)
dask.dataframe.io.io.dataframe_from_ctable(x,slc,columns=None,categories=None,lock=lock)
dask.dataframe.io.io.from_array(x,chunksize=50000,columns=None)
dask.dataframe.io.io.from_bcolz(x,chunksize=None,categorize=True,index=None,lock=lock,**kwargs)
dask.dataframe.io.io.from_dask_array(x,columns=None)
dask.dataframe.io.io.from_delayed(dfs,meta=None,divisions=None,prefix='from-delayed')
dask.dataframe.io.io.from_pandas(data,npartitions=None,chunksize=None,sort=True,name=None)
dask.dataframe.io.io.sorted_division_locations(seq,npartitions=None,chunksize=None)
dask.dataframe.io.io.to_bag(df,index=False)
dask.dataframe.io.io.to_records(df)
dask.dataframe.to_bag(df,index=False)
dask.dataframe.to_records(df)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/dataframe/io/tests/test_demo.py----------------------------------------
A:dask.dataframe.io.tests.test_demo.df->dask.dataframe.demo.daily_stock('GOOG', start='2010-01-01', stop='2010-01-30', freq='1h')
A:dask.dataframe.io.tests.test_demo.a->dask.dataframe.demo.make_timeseries('2000', '2015', {'A': float, 'B': int, 'C': str}, freq='2D', partition_freq='6M', seed=123)
A:dask.dataframe.io.tests.test_demo.b->dask.dataframe.demo.make_timeseries('2000', '2015', {'A': float, 'B': int, 'C': str}, freq='2D', partition_freq='6M', seed=123)
A:dask.dataframe.io.tests.test_demo.c->dask.dataframe.demo.make_timeseries('2000', '2015', {'A': float, 'B': int, 'C': str}, freq='2D', partition_freq='6M', seed=456)
A:dask.dataframe.io.tests.test_demo.d->dask.dataframe.demo.make_timeseries('2000', '2015', {'A': float, 'B': int, 'C': str}, freq='2D', partition_freq='3M', seed=123)
A:dask.dataframe.io.tests.test_demo.e->dask.dataframe.demo.make_timeseries('2000', '2015', {'A': float, 'B': int, 'C': str}, freq='1D', partition_freq='6M', seed=123)
dask.dataframe.io.tests.test_demo.test_daily_stock()
dask.dataframe.io.tests.test_demo.test_make_timeseries()
dask.dataframe.io.tests.test_demo.test_no_overlaps()


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/dataframe/io/tests/test_csv.py----------------------------------------
A:dask.dataframe.io.tests.test_csv.pd->pytest.importorskip('pandas')
A:dask.dataframe.io.tests.test_csv.dd->pytest.importorskip('dask.dataframe')
A:dask.dataframe.io.tests.test_csv.csv_text->'\nname,amount\nAlice,100\nBob,-200\nCharlie,300\nDennis,400\nEdith,-500\nFrank,600\nAlice,200\nFrank,-200\nBob,600\nAlice,400\nFrank,200\nAlice,300\nEdith,600\n'.strip()
A:dask.dataframe.io.tests.test_csv.tsv_text->'\nname,amount\nAlice,100\nBob,-200\nCharlie,300\nDennis,400\nEdith,-500\nFrank,600\nAlice,200\nFrank,-200\nBob,600\nAlice,400\nFrank,200\nAlice,300\nEdith,600\n'.strip().replace(',', '\t')
A:dask.dataframe.io.tests.test_csv.tsv_text2->'\nname   amount\nAlice    100\nBob     -200\nCharlie  300\nDennis   400\nEdith   -500\nFrank    600\nAlice    200\nFrank   -200\nBob      600\nAlice    400\nFrank    200\nAlice    300\nEdith    600\n'.strip()
A:dask.dataframe.io.tests.test_csv.timeseries->'\nDate,Open,High,Low,Close,Volume,Adj Close\n2015-08-28,198.50,199.839996,197.919998,199.240005,143298900,199.240005\n2015-08-27,197.020004,199.419998,195.210007,199.160004,266244700,199.160004\n2015-08-26,192.080002,194.789993,188.369995,194.679993,328058100,194.679993\n2015-08-25,195.429993,195.449997,186.919998,187.229996,353966700,187.229996\n2015-08-24,197.630005,197.630005,182.399994,189.550003,478672400,189.550003\n2015-08-21,201.729996,203.940002,197.520004,197.630005,328271500,197.630005\n2015-08-20,206.509995,208.289993,203.899994,204.009995,185865600,204.009995\n2015-08-19,209.089996,210.009995,207.350006,208.279999,167316300,208.279999\n2015-08-18,210.259995,210.679993,209.699997,209.929993,70043800,209.929993\n'.strip()
A:dask.dataframe.io.tests.test_csv.expected->pytest.importorskip('pandas').DataFrame({0: [1, 3], 1: [2, 4]})
A:dask.dataframe.io.tests.test_csv.csv_and_table->pytest.mark.parametrize('reader,files', [(pd.read_csv, csv_files), (pd.read_table, tsv_files)])
A:dask.dataframe.io.tests.test_csv.df->pytest.importorskip('pandas').DataFrame({'A': range(10)})
A:dask.dataframe.io.tests.test_csv.(header, b)->dask.dataframe.read_csv(fn).split(b'\n', 1)
A:dask.dataframe.io.tests.test_csv.head->reader(BytesIO(blocks[0][0]), header=0)
A:dask.dataframe.io.tests.test_csv.values->text_blocks_to_pandas(reader, blocks, header, head, kwargs, collection=False)
A:dask.dataframe.io.tests.test_csv.result->pytest.importorskip('dask.dataframe').read_csv(os.path.join(dn, '*')).compute().reset_index(drop=True)
A:dask.dataframe.io.tests.test_csv.lines->dask.dataframe.read_csv(fn).split(b'\n')
A:dask.dataframe.io.tests.test_csv.skip->len(comment_header.splitlines())
A:dask.dataframe.io.tests.test_csv.expected_df->pytest.importorskip('pandas').concat([pd_read(n, skiprows=skip) for n in sorted(files)])
A:dask.dataframe.io.tests.test_csv.dfs->text_blocks_to_pandas(reader, blocks, header, head, {}, collection=False, enforce=True)
A:dask.dataframe.io.tests.test_csv.f->gzip.open(os.path.join(tdir, 'b.csv.gz'), 'wb')
A:dask.dataframe.io.tests.test_csv.expected2->pd_read(BytesIO(files[fn]))
A:dask.dataframe.io.tests.test_csv.blocks->pytest.importorskip('dask.dataframe').DataFrame._get(f.dask, f._keys(), get=dask.get)
A:dask.dataframe.io.tests.test_csv.text->'a b c-d\n1 2 3\n4 5 6'.replace(' ', '\t')
A:dask.dataframe.io.tests.test_csv.text1->normalize_text('\n    fruit,count\n    apple,10\n    apple,25\n    pear,100\n    orange,15\n    ')
A:dask.dataframe.io.tests.test_csv.text2->normalize_text('\n    fruit,count\n    apple,200\n    banana,300\n    orange,400\n    banana,10\n    ')
A:dask.dataframe.io.tests.test_csv.res->pytest.importorskip('dask.dataframe').read_csv(fn, sample=50, assume_missing=True, dtype=None)
A:dask.dataframe.io.tests.test_csv.a->pytest.importorskip('dask.dataframe').from_pandas(df, npartitions)
A:dask.dataframe.io.tests.test_csv.b->pytest.importorskip('dask.dataframe').read_csv(fn)
A:dask.dataframe.io.tests.test_csv.files2->valmap(compress['gzip'], csv_files)
A:dask.dataframe.io.tests.test_csv.msg->str(w[0].message)
A:dask.dataframe.io.tests.test_csv.blocksize->auto_blocksize(1000000000000, 3)
A:dask.dataframe.io.tests.test_csv.psutil->pytest.importorskip('psutil')
A:dask.dataframe.io.tests.test_csv.mock->pytest.importorskip('mock')
A:dask.dataframe.io.tests.test_csv.cpu_count->pytest.importorskip('psutil').cpu_count()
A:dask.dataframe.io.tests.test_csv.mock_read_bytes->pytest.importorskip('mock').Mock(wraps=read_bytes)
A:dask.dataframe.io.tests.test_csv.expected_block_size->auto_blocksize(total_memory, cpu_count)
A:dask.dataframe.io.tests.test_csv.c->pytest.importorskip('dask.dataframe').read_csv(fn, skiprows=1, na_values=[0])
A:dask.dataframe.io.tests.test_csv.sol->pytest.importorskip('pandas').read_csv(fn)
A:dask.dataframe.io.tests.test_csv.ddf->pytest.importorskip('dask.dataframe').from_pandas(df, npartitions=2)
A:dask.dataframe.io.tests.test_csv.ar->pytest.importorskip('pandas').Series(range(0, 100))
A:dask.dataframe.io.tests.test_csv.test_df->pytest.importorskip('pandas').DataFrame({'a': ar, 'b': br, 'c': cr, 'd': dr})
A:dask.dataframe.io.tests.test_csv.d->d.compute().compute()
A:dask.dataframe.io.tests.test_csv.d.index->range(len(d.index))
A:dask.dataframe.io.tests.test_csv.pdmc_text->normalize_text('\n    ID,date,time\n    10,2003-11-04,180036\n    11,2003-11-05,125640\n    12,2003-11-01,2519\n    13,2003-10-22,142559\n    14,2003-10-24,163113\n    15,2003-10-20,170133\n    16,2003-11-11,160448\n    17,2003-11-03,171759\n    18,2003-11-07,190928\n    19,2003-10-21,84623\n    20,2003-10-25,192207\n    21,2003-11-13,180156\n    22,2003-11-15,131037\n    ')
A:dask.dataframe.io.tests.test_csv.sep_text->normalize_text('\n    name###amount\n    alice###100\n    bob###200\n    charlie###300')
A:dask.dataframe.io.tests.test_csv.files->csv_files.copy()
A:dask.dataframe.io.tests.test_csv.files[k]->files[k].replace(b'name', b'Name').replace(b'name', b'Name')
A:dask.dataframe.io.tests.test_csv.r->pytest.importorskip('dask.dataframe').from_pandas(df, npartitions).to_csv(dn, index=False, compute=False)
A:dask.dataframe.io.tests.test_csv.fn->os.path.join(dn, 'data_*.csv')
A:dask.dataframe.io.tests.test_csv.df16->pytest.importorskip('pandas').DataFrame({'x': ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p'], 'y': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]})
A:dask.dataframe.io.tests.test_csv.df0->pytest.importorskip('pandas').Series(['a', 'b', 'c', 'd'], index=[1.0, 2.0, 3.0, 4.0])
A:dask.dataframe.io.tests.test_csv.dir->str(dir)
dask.dataframe.io.tests.test_csv.normalize_text(s)
dask.dataframe.io.tests.test_csv.test_assume_missing()
dask.dataframe.io.tests.test_csv.test_auto_blocksize()
dask.dataframe.io.tests.test_csv.test_auto_blocksize_csv(monkeypatch)
dask.dataframe.io.tests.test_csv.test_auto_blocksize_max64mb()
dask.dataframe.io.tests.test_csv.test_categorical_dtypes()
dask.dataframe.io.tests.test_csv.test_compression_multiple_files()
dask.dataframe.io.tests.test_csv.test_consistent_dtypes()
dask.dataframe.io.tests.test_csv.test_consistent_dtypes_2()
dask.dataframe.io.tests.test_csv.test_csv_with_integer_names()
dask.dataframe.io.tests.test_csv.test_empty_csv_file()
dask.dataframe.io.tests.test_csv.test_encoding_gh601(encoding)
dask.dataframe.io.tests.test_csv.test_enforce_columns(reader,blocks)
dask.dataframe.io.tests.test_csv.test_enforce_dtypes(reader,blocks)
dask.dataframe.io.tests.test_csv.test_error_if_sample_is_too_small()
dask.dataframe.io.tests.test_csv.test_head_partial_line_fix()
dask.dataframe.io.tests.test_csv.test_header_None()
dask.dataframe.io.tests.test_csv.test_index_col()
dask.dataframe.io.tests.test_csv.test_late_dtypes()
dask.dataframe.io.tests.test_csv.test_multiple_read_csv_has_deterministic_name()
dask.dataframe.io.tests.test_csv.test_none_usecols()
dask.dataframe.io.tests.test_csv.test_pandas_read_text(reader,files)
dask.dataframe.io.tests.test_csv.test_pandas_read_text_dtype_coercion(reader,files)
dask.dataframe.io.tests.test_csv.test_pandas_read_text_kwargs(reader,files)
dask.dataframe.io.tests.test_csv.test_pandas_read_text_with_header(reader,files)
dask.dataframe.io.tests.test_csv.test_parse_dates_multi_column()
dask.dataframe.io.tests.test_csv.test_read_csv(dd_read,pd_read,text,sep)
dask.dataframe.io.tests.test_csv.test_read_csv_compression(fmt,blocksize)
dask.dataframe.io.tests.test_csv.test_read_csv_files(dd_read,pd_read,files)
dask.dataframe.io.tests.test_csv.test_read_csv_has_deterministic_name()
dask.dataframe.io.tests.test_csv.test_read_csv_header_issue_823()
dask.dataframe.io.tests.test_csv.test_read_csv_index()
dask.dataframe.io.tests.test_csv.test_read_csv_of_modified_file_has_different_name()
dask.dataframe.io.tests.test_csv.test_read_csv_raises_on_no_files()
dask.dataframe.io.tests.test_csv.test_read_csv_sensitive_to_enforce()
dask.dataframe.io.tests.test_csv.test_read_csv_sep()
dask.dataframe.io.tests.test_csv.test_read_csv_singleton_dtype()
dask.dataframe.io.tests.test_csv.test_read_csv_slash_r()
dask.dataframe.io.tests.test_csv.test_read_csv_with_datetime_index_partitions_n()
dask.dataframe.io.tests.test_csv.test_read_csv_with_datetime_index_partitions_one()
dask.dataframe.io.tests.test_csv.test_robust_column_mismatch()
dask.dataframe.io.tests.test_csv.test_skipinitialspace()
dask.dataframe.io.tests.test_csv.test_skiprows(dd_read,pd_read,files)
dask.dataframe.io.tests.test_csv.test_text_blocks_to_pandas_blocked(reader,files)
dask.dataframe.io.tests.test_csv.test_text_blocks_to_pandas_kwargs(reader,files)
dask.dataframe.io.tests.test_csv.test_text_blocks_to_pandas_simple(reader,files)
dask.dataframe.io.tests.test_csv.test_to_csv()
dask.dataframe.io.tests.test_csv.test_to_csv_gzip()
dask.dataframe.io.tests.test_csv.test_to_csv_multiple_files_cornercases()
dask.dataframe.io.tests.test_csv.test_to_csv_paths()
dask.dataframe.io.tests.test_csv.test_to_csv_series()
dask.dataframe.io.tests.test_csv.test_to_csv_simple()
dask.dataframe.io.tests.test_csv.test_to_csv_with_get()
dask.dataframe.io.tests.test_csv.test_usecols()
dask.dataframe.io.tests.test_csv.test_warn_non_seekable_files()
dask.dataframe.io.tests.test_csv.test_windows_line_terminator()


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/dataframe/io/tests/test_io.py----------------------------------------
A:dask.dataframe.io.tests.test_io.x->numpy.array([(1, 'a'), (2, 'b')], dtype=[('a', 'i4'), ('b', 'object')])
A:dask.dataframe.io.tests.test_io.res->_meta_from_array(x, columns=['b', 'a'])
A:dask.dataframe.io.tests.test_io.d->dask.dataframe.from_bcolz(fn, chunksize=2)
A:dask.dataframe.io.tests.test_io.bcolz->pytest.importorskip('bcolz')
A:dask.dataframe.io.tests.test_io.t->pytest.importorskip('bcolz').ctable([[1, 2, 3], [1.0, 2.0, 3.0], ['a', 'b', 'a']], names=['x', 'y', 'a'])
A:dask.dataframe.io.tests.test_io.L->list(d.index.compute(get=get_sync))
A:dask.dataframe.io.tests.test_io.thread->threading.Thread(target=check)
A:dask.dataframe.io.tests.test_io.locktype->type(Lock())
A:dask.dataframe.io.tests.test_io.a->pandas.DataFrame({'x': [1, 2]}, index=[1, 10])
A:dask.dataframe.io.tests.test_io.b->pandas.DataFrame({'x': [4, 1]}, index=[100, 200])
A:dask.dataframe.io.tests.test_io.c->dask.dataframe.from_bcolz(t, chunksize=2, lock=False)
A:dask.dataframe.io.tests.test_io.df->pandas.DataFrame({'x': list(range(20))})
A:dask.dataframe.io.tests.test_io.ddf->dask.dataframe.from_pandas(df, npartitions=20)
A:dask.dataframe.io.tests.test_io.s->dask.dataframe.from_delayed([d.a for d in dfs], meta=meta.a, divisions=divisions)
A:dask.dataframe.io.tests.test_io.ds->dask.dataframe.from_pandas(s, chunksize=8)
A:dask.dataframe.io.tests.test_io.df.Date->pandas.DataFrame({'x': list(range(20))}).Date.astype('datetime64[ns]')
A:dask.dataframe.io.tests.test_io.df2->dask.dataframe.from_array(x, columns=['a', 'b', 'c'])
A:dask.dataframe.io.tests.test_io.ser->dask.dataframe.from_dask_array(x)
A:dask.dataframe.io.tests.test_io.ser2->dask.dataframe.from_array(x)
A:dask.dataframe.io.tests.test_io.d1->dask.dataframe.from_dask_array(x, columns=['name'])
A:dask.dataframe.io.tests.test_io.d2->dask.dataframe.from_array(x.compute(), columns=['name'])
A:dask.dataframe.io.tests.test_io.y->dask.array.from_array(x, chunks=(1,))
A:dask.dataframe.io.tests.test_io.dx->dask.array.Array(dsk, 'x', ((np.nan, np.nan), (np.nan,)), np.float64)
A:dask.dataframe.io.tests.test_io.meta->dfs[0].compute()
A:dask.dataframe.io.tests.test_io.A->dask.dataframe.from_delayed([delayed(a), delayed(b)], divisions='sorted')
A:dask.dataframe.io.tests.test_io.(a, b)->dask.dataframe.from_pandas(df, npartitions=20).to_delayed()
dask.dataframe.io.tests.test_io.test_DataFrame_from_dask_array()
dask.dataframe.io.tests.test_io.test_Series_from_dask_array()
dask.dataframe.io.tests.test_io.test_from_array()
dask.dataframe.io.tests.test_io.test_from_array_with_record_dtype()
dask.dataframe.io.tests.test_io.test_from_bcolz()
dask.dataframe.io.tests.test_io.test_from_bcolz_column_order()
dask.dataframe.io.tests.test_io.test_from_bcolz_filename()
dask.dataframe.io.tests.test_io.test_from_bcolz_multiple_threads()
dask.dataframe.io.tests.test_io.test_from_bcolz_no_lock()
dask.dataframe.io.tests.test_io.test_from_dask_array_compat_numpy_array()
dask.dataframe.io.tests.test_io.test_from_dask_array_compat_numpy_array_1d()
dask.dataframe.io.tests.test_io.test_from_dask_array_struct_dtype()
dask.dataframe.io.tests.test_io.test_from_dask_array_unknown_chunks()
dask.dataframe.io.tests.test_io.test_from_delayed()
dask.dataframe.io.tests.test_io.test_from_delayed_sorted()
dask.dataframe.io.tests.test_io.test_from_pandas_dataframe()
dask.dataframe.io.tests.test_io.test_from_pandas_non_sorted()
dask.dataframe.io.tests.test_io.test_from_pandas_npartitions_is_accurate()
dask.dataframe.io.tests.test_io.test_from_pandas_series()
dask.dataframe.io.tests.test_io.test_from_pandas_single_row()
dask.dataframe.io.tests.test_io.test_from_pandas_small()
dask.dataframe.io.tests.test_io.test_from_pandas_with_datetime_index()
dask.dataframe.io.tests.test_io.test_meta_from_1darray()
dask.dataframe.io.tests.test_io.test_meta_from_array()
dask.dataframe.io.tests.test_io.test_meta_from_recarray()
dask.dataframe.io.tests.test_io.test_to_bag()
dask.dataframe.io.tests.test_io.test_to_delayed()
dask.dataframe.io.tests.test_io.test_to_delayed_optimizes()
dask.dataframe.io.tests.test_io.test_to_records()


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/dataframe/io/tests/test_hdf.py----------------------------------------
A:dask.dataframe.io.tests.test_hdf.df->pandas.DataFrame({'x': ['a', 'b', 'c', 'd'], 'y': [1, 2, 3, 4]}, index=[1.0, 2.0, 3.0, 4.0])
A:dask.dataframe.io.tests.test_hdf.a->dask.dataframe.from_pandas(df, 16)
A:dask.dataframe.io.tests.test_hdf.out->dask.dataframe.read_hdf(fn, '/data*')
A:dask.dataframe.io.tests.test_hdf.r->dask.dataframe.read_hdf(fn, '/data*', sorted_index=True)
A:dask.dataframe.io.tests.test_hdf.df16->pandas.DataFrame({'x': ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p'], 'y': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]}, index=[1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0])
A:dask.dataframe.io.tests.test_hdf.b->dask.dataframe.from_pandas(df16, 16)
A:dask.dataframe.io.tests.test_hdf.fn->os.path.join(dn, 'data_*.csv')
A:dask.dataframe.io.tests.test_hdf.d->dask.dataframe.from_pandas(df, 16).to_hdf(fn, '/data', compute=False)
A:dask.dataframe.io.tests.test_hdf.ddf->dask.dataframe.from_pandas(df, npartitions=2)
A:dask.dataframe.io.tests.test_hdf.df2->pandas.read_hdf(fn, 'foo4')
A:dask.dataframe.io.tests.test_hdf.res->dask.dataframe.read_hdf(input_files, 'dataframe')
A:dask.dataframe.io.tests.test_hdf.expected->pandas.read_hdf(os.path.join(tdir, 'one.h5'), '/foo/data', start=1, stop=3)
dask.dataframe.io.tests.test_hdf.test_hdf_file_list()
dask.dataframe.io.tests.test_hdf.test_hdf_filenames()
dask.dataframe.io.tests.test_hdf.test_hdf_globbing()
dask.dataframe.io.tests.test_hdf.test_read_hdf(data,compare)
dask.dataframe.io.tests.test_hdf.test_read_hdf_doesnt_segfault()
dask.dataframe.io.tests.test_hdf.test_read_hdf_multiple()
dask.dataframe.io.tests.test_hdf.test_read_hdf_multiply_open()
dask.dataframe.io.tests.test_hdf.test_read_hdf_start_stop_values()
dask.dataframe.io.tests.test_hdf.test_to_fmt_warns()
dask.dataframe.io.tests.test_hdf.test_to_hdf()
dask.dataframe.io.tests.test_hdf.test_to_hdf_exceptions()
dask.dataframe.io.tests.test_hdf.test_to_hdf_kwargs()
dask.dataframe.io.tests.test_hdf.test_to_hdf_link_optimizations()
dask.dataframe.io.tests.test_hdf.test_to_hdf_lock_delays()
dask.dataframe.io.tests.test_hdf.test_to_hdf_modes_multiple_files()
dask.dataframe.io.tests.test_hdf.test_to_hdf_modes_multiple_nodes()
dask.dataframe.io.tests.test_hdf.test_to_hdf_multiple_files()
dask.dataframe.io.tests.test_hdf.test_to_hdf_multiple_nodes()
dask.dataframe.io.tests.test_hdf.test_to_hdf_schedulers(get,npartitions)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/dataframe/io/tests/test_parquet.py----------------------------------------
A:dask.dataframe.io.tests.test_parquet.fastparquet->pytest.importorskip('fastparquet')
A:dask.dataframe.io.tests.test_parquet.df->pandas.DataFrame({'a': [1, 2, 3, 4], 'b': [1.0, 2.0, 3.0, 4.0]})
A:dask.dataframe.io.tests.test_parquet.ddf->dask.dataframe.from_pandas(df, npartitions=2)
A:dask.dataframe.io.tests.test_parquet.tmp->str(tmp)
A:dask.dataframe.io.tests.test_parquet.data->pandas.DataFrame({'i32': np.arange(1000, dtype=np.int32), 'f': np.arange(1000, dtype=np.float64)})
A:dask.dataframe.io.tests.test_parquet.files->os.listdir(fn)
A:dask.dataframe.io.tests.test_parquet.df2->pytest.importorskip('fastparquet').ParquetFile(fn).to_pandas(filters=[('at', '==', 'aa')])
A:dask.dataframe.io.tests.test_parquet.out->dask.dataframe.read_parquet(fn).compute()
A:dask.dataframe.io.tests.test_parquet.read_df->dask.dataframe.read_parquet(tmp)
A:dask.dataframe.io.tests.test_parquet.dsk->x._optimize(x.dask, x._keys())
A:dask.dataframe.io.tests.test_parquet.ddf2->dask.dataframe.read_parquet(tmpdir)
A:dask.dataframe.io.tests.test_parquet.ddf1->dask.dataframe.from_pandas(df1, chunksize=2)
A:dask.dataframe.io.tests.test_parquet.ddf3->dask.dataframe.read_parquet(fn)
A:dask.dataframe.io.tests.test_parquet.df0->pandas.DataFrame({'lat': np.arange(0, 10), 'lon': np.arange(10, 20), 'value': np.arange(100, 110)})
A:dask.dataframe.io.tests.test_parquet.df1->pandas.DataFrame({'i32': np.arange(100, dtype=np.int32)})
A:dask.dataframe.io.tests.test_parquet.dd_df0->dask.dataframe.from_pandas(df0, npartitions=1)
A:dask.dataframe.io.tests.test_parquet.dd_df1->dask.dataframe.from_pandas(df1, npartitions=1)
A:dask.dataframe.io.tests.test_parquet.out['lon']->dask.dataframe.read_parquet(fn).compute().lon.astype('int64')
A:dask.dataframe.io.tests.test_parquet.df3->read_parquet(tmp, columns=['f', 'i32'], engine=engine)
A:dask.dataframe.io.tests.test_parquet.pf->pytest.importorskip('fastparquet').ParquetFile(tmp)
A:dask.dataframe.io.tests.test_parquet.ddf['y']->dask.dataframe.from_pandas(df, npartitions=2).y.astype('category')
A:dask.dataframe.io.tests.test_parquet.cats_set->dask.dataframe.read_parquet(tmpdir).map_partitions(lambda x: x.y.cat.categories).compute()
A:dask.dataframe.io.tests.test_parquet.table->pyarrow.parquet.read_table(fn)
A:dask.dataframe.io.tests.test_parquet.tmpdir->str(tmpdir)
A:dask.dataframe.io.tests.test_parquet.d->dask.dataframe.from_pandas(df, npartitions=2)
A:dask.dataframe.io.tests.test_parquet.value->dask.dataframe.from_pandas(df, npartitions=2).to_parquet(tmpdir, compute=False)
dask.dataframe.io.tests.test_parquet.engine(request)
dask.dataframe.io.tests.test_parquet.fn(tmpdir)
dask.dataframe.io.tests.test_parquet.test_append()
dask.dataframe.io.tests.test_parquet.test_append_different_columns()
dask.dataframe.io.tests.test_parquet.test_append_overlapping_divisions()
dask.dataframe.io.tests.test_parquet.test_append_with_partition()
dask.dataframe.io.tests.test_parquet.test_append_wo_index()
dask.dataframe.io.tests.test_parquet.test_auto_add_index(fn)
dask.dataframe.io.tests.test_parquet.test_categorical()
dask.dataframe.io.tests.test_parquet.test_categories(fn)
dask.dataframe.io.tests.test_parquet.test_empty(fn)
dask.dataframe.io.tests.test_parquet.test_empty_index()
dask.dataframe.io.tests.test_parquet.test_empty_partition(fn)
dask.dataframe.io.tests.test_parquet.test_filters(fn)
dask.dataframe.io.tests.test_parquet.test_glob(fn)
dask.dataframe.io.tests.test_parquet.test_index(fn)
dask.dataframe.io.tests.test_parquet.test_index_column(fn,engine)
dask.dataframe.io.tests.test_parquet.test_index_column_false_index(fn)
dask.dataframe.io.tests.test_parquet.test_index_column_no_index(fn,engine)
dask.dataframe.io.tests.test_parquet.test_local(engine)
dask.dataframe.io.tests.test_parquet.test_names(fn,engine)
dask.dataframe.io.tests.test_parquet.test_no_columns_no_index(fn)
dask.dataframe.io.tests.test_parquet.test_no_columns_yes_index(fn)
dask.dataframe.io.tests.test_parquet.test_no_index(fn)
dask.dataframe.io.tests.test_parquet.test_optimize(fn,c)
dask.dataframe.io.tests.test_parquet.test_ordering()
dask.dataframe.io.tests.test_parquet.test_partition_on(tmpdir)
dask.dataframe.io.tests.test_parquet.test_read_parquet_custom_columns(engine)
dask.dataframe.io.tests.test_parquet.test_roundtrip(df,write_kwargs,read_kwargs)
dask.dataframe.io.tests.test_parquet.test_roundtrip_from_pandas(engine)
dask.dataframe.io.tests.test_parquet.test_series(fn)
dask.dataframe.io.tests.test_parquet.test_timestamp_index()
dask.dataframe.io.tests.test_parquet.test_to_parquet_default_writes_nulls()
dask.dataframe.io.tests.test_parquet.test_to_parquet_lazy(tmpdir,get)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/dataframe/io/tests/test_sql.py----------------------------------------
A:dask.dataframe.io.tests.test_sql.pd->pytest.importorskip('pandas')
A:dask.dataframe.io.tests.test_sql.dd->pytest.importorskip('dask.dataframe')
A:dask.dataframe.io.tests.test_sql.df->pytest.importorskip('pandas').DataFrame({'a': list('ghjkl'), 'b': [now + i * d for i in range(2, -3, -1)]})
A:dask.dataframe.io.tests.test_sql.data->read_sql_table('test', db, npartitions=2, index_col=index, columns=['negish', 'age'])
A:dask.dataframe.io.tests.test_sql.out->read_sql_table(s1, db, npartitions=2, index_col='number')
A:dask.dataframe.io.tests.test_sql.m->read_sql_table(s1, db, npartitions=2, index_col='number').map_partitions(lambda d: d.memory_usage(deep=True, index=True).sum()).compute()
A:dask.dataframe.io.tests.test_sql.now->datetime.datetime.now()
A:dask.dataframe.io.tests.test_sql.d->read_sql_table('test', db, npartitions=2, index_col=index, columns=['negish', 'age']).compute()
A:dask.dataframe.io.tests.test_sql.df2->pytest.importorskip('pandas').DataFrame({'a': list('ghjkl'), 'b': [now + i * d for i in range(2, -3, -1)]}).set_index('b')
A:dask.dataframe.io.tests.test_sql.index->sqlalchemy.sql.func.abs(sql.column('negish'))
A:dask.dataframe.io.tests.test_sql.part->read_sql_table('test', db, npartitions=2, index_col=index, columns=['negish', 'age']).get_partition(0).compute()
A:dask.dataframe.io.tests.test_sql.s1->sqlalchemy.sql.select([sql.column('number'), sql.column('name')]).select_from(sql.table('test'))
dask.dataframe.io.tests.test_sql.db()
dask.dataframe.io.tests.test_sql.test_datetimes()
dask.dataframe.io.tests.test_sql.test_division_or_partition(db)
dask.dataframe.io.tests.test_sql.test_divisions(db)
dask.dataframe.io.tests.test_sql.test_no_nameless_index(db)
dask.dataframe.io.tests.test_sql.test_npartitions(db)
dask.dataframe.io.tests.test_sql.test_range(db)
dask.dataframe.io.tests.test_sql.test_select_from_select(db)
dask.dataframe.io.tests.test_sql.test_simple(db)
dask.dataframe.io.tests.test_sql.test_with_func(db)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/dataframe/io/tests/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/store/core.py----------------------------------------
A:dask.store.core.self.dsk->dict()
A:dask.store.core.cache->dict()
A:dask.store.core.self.data->set()
A:dask.store.core.self.compute_time->dict()
A:dask.store.core.self.access_times->defaultdict(list)
A:dask.store.core.start->time()
A:dask.store.core.result->func(*args)
A:dask.store.core.end->time()
dask.store.Store(self,cache=None)
dask.store.Store.__delitem__(self,key)
dask.store.Store.__getitem__(self,key)
dask.store.Store.__iter__(self)
dask.store.Store.__len__(self)
dask.store.Store.__setitem__(self,key,value)
dask.store.core.Store(self,cache=None)
dask.store.core.Store.__delitem__(self,key)
dask.store.core.Store.__getitem__(self,key)
dask.store.core.Store.__init__(self,cache=None)
dask.store.core.Store.__iter__(self)
dask.store.core.Store.__len__(self)
dask.store.core.Store.__setitem__(self,key,value)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/store/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/store/tests/test_store.py----------------------------------------
A:dask.store.tests.test_store.s->Store()
A:dask.store.tests.test_store.cache->Store().cache.copy()
dask.store.tests.test_store.test_basic()
dask.store.tests.test_store.test_update()


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/store/tests/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/bytes/local.py----------------------------------------
A:dask.bytes.local.self.cwd->os.getcwd()
A:dask.bytes.local.path->self._trim_filename(path)
dask.bytes.local.LocalFileSystem(self,**storage_options)
dask.bytes.local.LocalFileSystem.__init__(self,**storage_options)
dask.bytes.local.LocalFileSystem._trim_filename(self,fn)
dask.bytes.local.LocalFileSystem.glob(self,path)
dask.bytes.local.LocalFileSystem.mkdirs(self,path)
dask.bytes.local.LocalFileSystem.open(self,path,mode='rb',**kwargs)
dask.bytes.local.LocalFileSystem.size(self,path)
dask.bytes.local.LocalFileSystem.ukey(self,path)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/bytes/utils.py----------------------------------------
A:dask.bytes.utils.extension->os.path.splitext(filename)[-1].strip('.')
A:dask.bytes.utils.current->file.read(blocksize)
A:dask.bytes.utils.i->full.index(delimiter)
A:dask.bytes.utils.start->f.tell()
A:dask.bytes.utils.end->f.tell()
A:dask.bytes.utils.pad_length->int(math.ceil(math.log10(max_int)))
dask.bytes.utils.build_name_function(max_int)
dask.bytes.utils.infer_compression(filename)
dask.bytes.utils.read_block(f,offset,length,delimiter=None)
dask.bytes.utils.seek_delimiter(file,delimiter,blocksize)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/bytes/core.py----------------------------------------
A:dask.bytes.core.out->data.read(64 * 2 ** 10)
A:dask.bytes.core.(fs, names, myopen)->get_fs_paths_myopen(urlpath, compression, mode, name_function=name_function, num=len(data), encoding=encoding, **kwargs)
A:dask.bytes.core.(fs, paths, myopen)->get_fs_paths_myopen(urlpath, compression, mode, encoding=encoding, num=num, name_function=name_function, errors=errors, **kwargs)
A:dask.bytes.core.blocksize->int(blocksize)
A:dask.bytes.core.(blocks, lengths, machines)->fs.get_block_locations(paths)
A:dask.bytes.core.size->fs.logical_size(path, compression)
A:dask.bytes.core.off->list(range(0, size, blocksize))
A:dask.bytes.core.ukey->fs.ukey(path)
A:dask.bytes.core.client->default_client()
A:dask.bytes.core.sample->read_block(f, 0, nbytes, delimiter)
A:dask.bytes.core.compression->infer_compression(path)
A:dask.bytes.core.self.storage_options->infer_storage_options(urlpath, inherit_storage_options=kwargs)
A:dask.bytes.core.self.protocol->self.storage_options.pop('protocol')
A:dask.bytes.core.self.fs->_filesystems[self.protocol](**self.storage_options)
A:dask.bytes.core.ff2->self.myopen(self.path, mode=mode)
A:dask.bytes.core.f2->SeekableFile(f)
A:dask.bytes.core.f3->CompressFile(f2, mode=mode)
A:dask.bytes.core.f4->io.TextIOWrapper(f3, encoding=self.encoding, errors=self.errors)
A:dask.bytes.core.urlpath->str(urlpath)
A:dask.bytes.core.myopen->OpenFileCreator(urlpath[0], compression, text='b' not in mode, encoding=encoding, **kwargs)
A:dask.bytes.core.paths->OpenFileCreator(urlpath[0], compression, text='b' not in mode, encoding=encoding, **kwargs).fs.glob(urlpath)
A:dask.bytes.core.name_function->build_name_function(num - 1)
A:dask.bytes.core.path->os.path.join(path, '*.part')
A:dask.bytes.core._filesystems->dict()
A:dask.bytes.core.f->SeekableFile(f)
A:dask.bytes.core.g->seekable_files[compression](f)
A:dask.bytes.core.result->seekable_files[compression](f).tell()
dask.bytes.core.FileSystem(object)
dask.bytes.core.FileSystem.get_block_locations(self,path)
dask.bytes.core.FileSystem.logical_size(self,path,compression)
dask.bytes.core.OpenFile(self,myopen,path,compression,mode,text,encoding,errors=None)
dask.bytes.core.OpenFile.__enter__(self)
dask.bytes.core.OpenFile.__exit__(self,*args)
dask.bytes.core.OpenFile.__init__(self,myopen,path,compression,mode,text,encoding,errors=None)
dask.bytes.core.OpenFile.close(self)
dask.bytes.core.OpenFileCreator(self,urlpath,compression=None,text=False,encoding='utf8',errors=None,**kwargs)
dask.bytes.core.OpenFileCreator.__init__(self,urlpath,compression=None,text=False,encoding='utf8',errors=None,**kwargs)
dask.bytes.core._expand_paths(path,name_function,num)
dask.bytes.core.ensure_protocol(protocol)
dask.bytes.core.get_fs_paths_myopen(urlpath,compression,mode,encoding='utf8',errors='strict',num=1,name_function=None,**kwargs)
dask.bytes.core.normalize_OpenFileCreator(ofc)
dask.bytes.core.open_files(urlpath,compression=None,mode='rb',encoding='utf8',errors=None,name_function=None,num=1,**kwargs)
dask.bytes.core.open_text_files(urlpath,compression=None,mode='rt',encoding='utf8',errors='strict',**kwargs)
dask.bytes.core.read_block_from_file(lazy_file,off,bs,delimiter)
dask.bytes.core.read_bytes(urlpath,delimiter=None,not_zero=False,blocksize=2**27,sample=True,compression=None,**kwargs)
dask.bytes.core.write_block_to_file(data,lazy_file)
dask.bytes.core.write_bytes(data,urlpath,name_function=None,compression=None,encoding=None,**kwargs)
dask.bytes.open_files(urlpath,compression=None,mode='rb',encoding='utf8',errors=None,name_function=None,num=1,**kwargs)
dask.bytes.open_text_files(urlpath,compression=None,mode='rt',encoding='utf8',errors='strict',**kwargs)
dask.bytes.read_bytes(urlpath,delimiter=None,not_zero=False,blocksize=2**27,sample=True,compression=None,**kwargs)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/bytes/compression.py----------------------------------------
A:dask.bytes.compression.data->f.read(size)
A:dask.bytes.compression.stream_flags->decode_stream_footer(_peek(fp, STREAM_HEADER_SIZE))
A:dask.bytes.compression.index->decode_index(_peek(fp, stream_flags.backward_size), padding)
A:dask.bytes.compression.hsize->decode_block_header_size(data[:1])
A:dask.bytes.compression.dc->LZMADecompressor(format=FORMAT_BLOCK, header=header, unpadded_size=len(data), check=check)
dask.bytes.compression.get_xz_blocks(fp)
dask.bytes.compression.noop_file(file,**kwargs)
dask.bytes.compression.xz_decompress(data,check)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/bytes/s3.py----------------------------------------
A:dask.bytes.s3.so->infer_storage_options(fn)
A:dask.bytes.s3.s3_path->self._trim_filename(path)
A:dask.bytes.s3.f->s3fs.S3FileSystem.open(self, s3_path, mode=mode)
dask.bytes.s3.DaskS3FileSystem(self,key=None,username=None,secret=None,password=None,path=None,host=None,s3=None,**kwargs)
dask.bytes.s3.DaskS3FileSystem.__init__(self,key=None,username=None,secret=None,password=None,path=None,host=None,s3=None,**kwargs)
dask.bytes.s3.DaskS3FileSystem._trim_filename(self,fn)
dask.bytes.s3.DaskS3FileSystem.glob(self,path)
dask.bytes.s3.DaskS3FileSystem.mkdirs(self,path)
dask.bytes.s3.DaskS3FileSystem.open(self,path,mode='rb')
dask.bytes.s3.DaskS3FileSystem.size(self,path)
dask.bytes.s3.DaskS3FileSystem.ukey(self,path)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/bytes/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/bytes/tests/test_bytes_utils.py----------------------------------------
A:dask.bytes.tests.test_bytes_utils.data->delimiter.join([b'123', b'456', b'789'])
A:dask.bytes.tests.test_bytes_utils.f->io.BytesIO(b'123\n456')
A:dask.bytes.tests.test_bytes_utils.dd->pytest.importorskip('dask.dataframe')
A:dask.bytes.tests.test_bytes_utils.so->infer_storage_options(urlpath)
dask.bytes.tests.test_bytes_utils.test_ensure_protocol()
dask.bytes.tests.test_bytes_utils.test_infer_storage_options()
dask.bytes.tests.test_bytes_utils.test_infer_storage_options_c(urlpath,expected_path)
dask.bytes.tests.test_bytes_utils.test_read_block()
dask.bytes.tests.test_bytes_utils.test_seek_delimiter_endline()


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/bytes/tests/test_compression.py----------------------------------------
A:dask.bytes.tests.test_compression.b->BytesIO(compressed)
A:dask.bytes.tests.test_compression.c->decomp(b)
A:dask.bytes.tests.test_compression.out->BytesIO()
A:dask.bytes.tests.test_compression.f->File(out, mode='wb')
A:dask.bytes.tests.test_compression.compressed->BytesIO().read()
A:dask.bytes.tests.test_compression.g->File(b, mode='rb')
A:dask.bytes.tests.test_compression.data2->File(b, mode='rb').read()
dask.bytes.tests.test_compression.test_compression()
dask.bytes.tests.test_compression.test_files(fmt,File)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/bytes/tests/test_local.py----------------------------------------
A:dask.bytes.tests.test_local.compute->partial(compute, get=get)
A:dask.bytes.tests.test_local.(sample, values)->func('.test.accounts.*', compression='not-found')
A:dask.bytes.tests.test_local.results->compute(*concat(values))
A:dask.bytes.tests.test_local.(sample, vals)->read_bytes('.test.account*', blocksize=bs)
A:dask.bytes.tests.test_local.ourlines->b''.join(res).split(b'\n')
A:dask.bytes.tests.test_local.testlines->b''.join((files[k] for k in sorted(files))).split(b'\n')
A:dask.bytes.tests.test_local.pathlib->pytest.importorskip('pathlib')
A:dask.bytes.tests.test_local.url->pytest.importorskip('pathlib').Path('s3://bucket/test.accounts.*')
A:dask.bytes.tests.test_local.(_, values)->read_bytes('.test.accounts*', blocksize=bs, delimiter=d)
A:dask.bytes.tests.test_local.(_, values2)->read_bytes('.test.accounts*', blocksize=bs, delimiter=b'foo')
A:dask.bytes.tests.test_local.ours->b''.join(res)
A:dask.bytes.tests.test_local.test->b''.join((files[v] for v in sorted(files)))
A:dask.bytes.tests.test_local.files2->valmap(compression.compress[fmt], files)
A:dask.bytes.tests.test_local.myfiles->open_text_files('.test.accounts.*', compression=fmt)
A:dask.bytes.tests.test_local.x->f.read()
A:dask.bytes.tests.test_local.fs->LocalFileSystem()
A:dask.bytes.tests.test_local.(_, a)->read_bytes('.test.accounts.*')
A:dask.bytes.tests.test_local.(_, b)->read_bytes('.test.accounts.*')
A:dask.bytes.tests.test_local.a->list(concat(a))
A:dask.bytes.tests.test_local.b->list(concat(b))
A:dask.bytes.tests.test_local.(_, c)->read_bytes('.test.accounts.*')
A:dask.bytes.tests.test_local.c->list(concat(c))
A:dask.bytes.tests.test_local.tmpdir->str(tmpdir)
A:dask.bytes.tests.test_local.some_bytes->delayed(make_bytes)()
A:dask.bytes.tests.test_local.out->LocalFileSystem().glob('*')
A:dask.bytes.tests.test_local.files->open_files([os.path.join(tmpdir, 'test1'), os.path.join(tmpdir, 'test2')], mode='wb')
A:dask.bytes.tests.test_local.d->gzip.GzipFile(os.path.join(tmpdir, files[0])).read()
A:dask.bytes.tests.test_local.cloudpickle->pytest.importorskip('cloudpickle')
A:dask.bytes.tests.test_local.fn->str(tmpdir / 'myfile.txt.gz')
A:dask.bytes.tests.test_local.opener->OpenFileCreator('file://foo.py', open=open)
A:dask.bytes.tests.test_local.opener2->pytest.importorskip('cloudpickle').loads(cloudpickle.dumps(opener))
A:dask.bytes.tests.test_local.lazy_file->ofc(fn)
A:dask.bytes.tests.test_local.lazy_file2->pytest.importorskip('cloudpickle').loads(cloudpickle.dumps(lazy_file))
A:dask.bytes.tests.test_local.lazy_file3->pytest.importorskip('cloudpickle').loads(cloudpickle.dumps(lazy_file))
A:dask.bytes.tests.test_local.ofc->OpenFileCreator(fn, text=True, open=open, mode='rt', compression='gzip', encoding='utf-8')
A:dask.bytes.tests.test_local.here->os.getcwd()
dask.bytes.tests.test_local.test_abs_paths(tmpdir)
dask.bytes.tests.test_local.test_bad_compression()
dask.bytes.tests.test_local.test_compressed_write(tmpdir)
dask.bytes.tests.test_local.test_compression(fmt,blocksize)
dask.bytes.tests.test_local.test_compression_binary(fmt)
dask.bytes.tests.test_local.test_compression_text(fmt)
dask.bytes.tests.test_local.test_getsize(fmt)
dask.bytes.tests.test_local.test_names()
dask.bytes.tests.test_local.test_not_found()
dask.bytes.tests.test_local.test_open_files()
dask.bytes.tests.test_local.test_open_files_write(tmpdir)
dask.bytes.tests.test_local.test_pickability_of_lazy_files(tmpdir)
dask.bytes.tests.test_local.test_py2_local_bytes(tmpdir)
dask.bytes.tests.test_local.test_read_bytes()
dask.bytes.tests.test_local.test_read_bytes_block()
dask.bytes.tests.test_local.test_read_bytes_blocksize_float()
dask.bytes.tests.test_local.test_read_bytes_blocksize_none()
dask.bytes.tests.test_local.test_read_bytes_delimited()
dask.bytes.tests.test_local.test_read_bytes_sample_delimiter()
dask.bytes.tests.test_local.test_registered_open_files()
dask.bytes.tests.test_local.test_registered_open_text_files(encoding)
dask.bytes.tests.test_local.test_registered_read_bytes()
dask.bytes.tests.test_local.test_simple_write(tmpdir)
dask.bytes.tests.test_local.test_with_paths()
dask.bytes.tests.test_local.test_with_urls()


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/bytes/tests/test_s3.py----------------------------------------
A:dask.bytes.tests.test_s3.compute->partial(compute, get=get)
A:dask.bytes.tests.test_s3.client->boto3.client('s3')
A:dask.bytes.tests.test_s3.m->moto.mock_s3()
A:dask.bytes.tests.test_s3.s3->DaskS3FileSystem(username='key', password='secret')
A:dask.bytes.tests.test_s3.out->dask.bytes.core.write_bytes(values, paths)
A:dask.bytes.tests.test_s3.(sample, values)->read_bytes('s3://compress/test/accounts.*', compression=fmt, blocksize=blocksize)
A:dask.bytes.tests.test_s3.results->compute(*concat(values))
A:dask.bytes.tests.test_s3.(_, values)->read_bytes('s3://' + test_bucket_name + '/test/accounts*', blocksize=blocksize, delimiter=d)
A:dask.bytes.tests.test_s3.(_, L)->read_bytes('s3://dask-data/nyc-taxi/2014/*.csv', blocksize=None, anon=True)
A:dask.bytes.tests.test_s3.(_, vals)->read_bytes('s3://' + test_bucket_name + '/test/account*', blocksize=blocksize)
A:dask.bytes.tests.test_s3.ourlines->b''.join(res).split(b'\n')
A:dask.bytes.tests.test_s3.testlines->b''.join((files[k] for k in sorted(files))).split(b'\n')
A:dask.bytes.tests.test_s3.(_, values2)->read_bytes('s3://' + test_bucket_name + '/test/accounts*', blocksize=blocksize, delimiter=b'foo')
A:dask.bytes.tests.test_s3.ours->b''.join(res)
A:dask.bytes.tests.test_s3.test->b''.join((files[v] for v in sorted(files)))
A:dask.bytes.tests.test_s3.myfiles->open_files('s3://' + test_bucket_name + '/test/accounts.*')
A:dask.bytes.tests.test_s3.data->pandas.DataFrame({'i32': np.array([0, 5, 2, 5])})
A:dask.bytes.tests.test_s3.(_, a)->read_bytes('s3://compress/test/accounts.*')
A:dask.bytes.tests.test_s3.(_, b)->read_bytes('s3://compress/test/accounts.*')
A:dask.bytes.tests.test_s3.(_, c)->read_bytes('s3://compress/test/accounts.*')
A:dask.bytes.tests.test_s3.a->open_files('s3://compress/test/accounts.*')
A:dask.bytes.tests.test_s3.b->open_files('s3://compress/test/accounts.*')
A:dask.bytes.tests.test_s3.c->open_files('s3://compress/test/accounts.*')
A:dask.bytes.tests.test_s3.dd->pytest.importorskip('dask.dataframe')
A:dask.bytes.tests.test_s3.df->pytest.importorskip('dask.dataframe').from_pandas(data, chunksize=500)
A:dask.bytes.tests.test_s3.db->pytest.importorskip('dask.bag')
A:dask.bytes.tests.test_s3.df2->read_parquet(url, index='foo')
dask.bytes.tests.test_s3.s3()
dask.bytes.tests.test_s3.s3_context(bucket,files)
dask.bytes.tests.test_s3.test_compression(s3,fmt,blocksize)
dask.bytes.tests.test_s3.test_files(s3)
dask.bytes.tests.test_s3.test_get_s3()
dask.bytes.tests.test_s3.test_getsize(fmt)
dask.bytes.tests.test_s3.test_modification_time_open_files()
dask.bytes.tests.test_s3.test_modification_time_read_bytes()
dask.bytes.tests.test_s3.test_parquet(s3)
dask.bytes.tests.test_s3.test_parquet_wstoragepars(s3)
dask.bytes.tests.test_s3.test_read_bytes(s3)
dask.bytes.tests.test_s3.test_read_bytes_block(s3,blocksize)
dask.bytes.tests.test_s3.test_read_bytes_blocksize_none(s3)
dask.bytes.tests.test_s3.test_read_bytes_blocksize_on_large_data()
dask.bytes.tests.test_s3.test_read_bytes_delimited(s3,blocksize)
dask.bytes.tests.test_s3.test_read_bytes_non_existing_glob(s3)
dask.bytes.tests.test_s3.test_read_bytes_sample_delimiter(s3)
dask.bytes.tests.test_s3.test_read_csv_passes_through_options()
dask.bytes.tests.test_s3.test_read_text_passes_through_options()
dask.bytes.tests.test_s3.test_registered(s3)
dask.bytes.tests.test_s3.test_registered_open_files(s3)
dask.bytes.tests.test_s3.test_registered_open_text_files(s3)
dask.bytes.tests.test_s3.test_write_bytes(s3)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/bytes/tests/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/array/ufunc.py----------------------------------------
A:dask.array.ufunc.target.__doc__->skip_doctest(source.__doc__)
A:dask.array.ufunc.wrapped.__doc__->skip_doctest(numpy_ufunc.__doc__)
A:dask.array.ufunc.A_is_dask->isinstance(A, Base)
A:dask.array.ufunc.B_is_dask->isinstance(B, Base)
A:dask.array.ufunc.A->asarray(A)
A:dask.array.ufunc.B->asarray(B)
A:dask.array.ufunc.out_inds->tuple(range(ndim))
A:dask.array.ufunc.dtype->apply_infer_dtype(self._ufunc.outer, [A, B], kwargs, 'ufunc.outer', suggest_dtype=False)
A:dask.array.ufunc.func->partial(self._ufunc.outer, dtype=kwargs.pop('dtype'))
A:dask.array.ufunc.add->ufunc(np.add)
A:dask.array.ufunc.subtract->ufunc(np.subtract)
A:dask.array.ufunc.multiply->ufunc(np.multiply)
A:dask.array.ufunc.divide->ufunc(np.divide)
A:dask.array.ufunc.logaddexp->ufunc(np.logaddexp)
A:dask.array.ufunc.logaddexp2->ufunc(np.logaddexp2)
A:dask.array.ufunc.true_divide->ufunc(np.true_divide)
A:dask.array.ufunc.floor_divide->ufunc(np.floor_divide)
A:dask.array.ufunc.negative->ufunc(np.negative)
A:dask.array.ufunc.power->ufunc(np.power)
A:dask.array.ufunc.remainder->ufunc(np.remainder)
A:dask.array.ufunc.mod->ufunc(np.mod)
A:dask.array.ufunc.conjconjugate->ufunc(np.conjugate)
A:dask.array.ufunc.exp->ufunc(np.exp)
A:dask.array.ufunc.exp2->ufunc(np.exp2)
A:dask.array.ufunc.log->ufunc(np.log)
A:dask.array.ufunc.log2->ufunc(np.log2)
A:dask.array.ufunc.log10->ufunc(np.log10)
A:dask.array.ufunc.log1p->ufunc(np.log1p)
A:dask.array.ufunc.expm1->ufunc(np.expm1)
A:dask.array.ufunc.sqrt->ufunc(np.sqrt)
A:dask.array.ufunc.square->ufunc(np.square)
A:dask.array.ufunc.cbrt->ufunc(np.cbrt)
A:dask.array.ufunc.reciprocal->ufunc(np.reciprocal)
A:dask.array.ufunc.sin->ufunc(np.sin)
A:dask.array.ufunc.cos->ufunc(np.cos)
A:dask.array.ufunc.tan->ufunc(np.tan)
A:dask.array.ufunc.arcsin->ufunc(np.arcsin)
A:dask.array.ufunc.arccos->ufunc(np.arccos)
A:dask.array.ufunc.arctan->ufunc(np.arctan)
A:dask.array.ufunc.arctan2->ufunc(np.arctan2)
A:dask.array.ufunc.hypot->ufunc(np.hypot)
A:dask.array.ufunc.sinh->ufunc(np.sinh)
A:dask.array.ufunc.cosh->ufunc(np.cosh)
A:dask.array.ufunc.tanh->ufunc(np.tanh)
A:dask.array.ufunc.arcsinh->ufunc(np.arcsinh)
A:dask.array.ufunc.arccosh->ufunc(np.arccosh)
A:dask.array.ufunc.arctanh->ufunc(np.arctanh)
A:dask.array.ufunc.deg2rad->ufunc(np.deg2rad)
A:dask.array.ufunc.rad2deg->ufunc(np.rad2deg)
A:dask.array.ufunc.greater->ufunc(np.greater)
A:dask.array.ufunc.greater_equal->ufunc(np.greater_equal)
A:dask.array.ufunc.less->ufunc(np.less)
A:dask.array.ufunc.less_equal->ufunc(np.less_equal)
A:dask.array.ufunc.not_equal->ufunc(np.not_equal)
A:dask.array.ufunc.equal->ufunc(np.equal)
A:dask.array.ufunc.logical_and->ufunc(np.logical_and)
A:dask.array.ufunc.logical_or->ufunc(np.logical_or)
A:dask.array.ufunc.logical_xor->ufunc(np.logical_xor)
A:dask.array.ufunc.logical_not->ufunc(np.logical_not)
A:dask.array.ufunc.maximum->ufunc(np.maximum)
A:dask.array.ufunc.minimum->ufunc(np.minimum)
A:dask.array.ufunc.fmax->ufunc(np.fmax)
A:dask.array.ufunc.fmin->ufunc(np.fmin)
A:dask.array.ufunc.isfinite->ufunc(np.isfinite)
A:dask.array.ufunc.isinf->ufunc(np.isinf)
A:dask.array.ufunc.isnan->ufunc(np.isnan)
A:dask.array.ufunc.signbit->ufunc(np.signbit)
A:dask.array.ufunc.copysign->ufunc(np.copysign)
A:dask.array.ufunc.nextafter->ufunc(np.nextafter)
A:dask.array.ufunc.spacing->ufunc(np.spacing)
A:dask.array.ufunc.ldexp->ufunc(np.ldexp)
A:dask.array.ufunc.fmod->ufunc(np.fmod)
A:dask.array.ufunc.floor->ufunc(np.floor)
A:dask.array.ufunc.ceil->ufunc(np.ceil)
A:dask.array.ufunc.trunc->ufunc(np.trunc)
A:dask.array.ufunc.degrees->ufunc(np.degrees)
A:dask.array.ufunc.radians->ufunc(np.radians)
A:dask.array.ufunc.rint->ufunc(np.rint)
A:dask.array.ufunc.fabs->ufunc(np.fabs)
A:dask.array.ufunc.sign->ufunc(np.sign)
A:dask.array.ufunc.absolute->ufunc(np.absolute)
A:dask.array.ufunc.clip->wrap_elemwise(np.clip)
A:dask.array.ufunc.isreal->wrap_elemwise(np.isreal, array_wrap=True)
A:dask.array.ufunc.iscomplex->wrap_elemwise(np.iscomplex, array_wrap=True)
A:dask.array.ufunc.real->wrap_elemwise(np.real, array_wrap=True)
A:dask.array.ufunc.imag->wrap_elemwise(np.imag, array_wrap=True)
A:dask.array.ufunc.fix->wrap_elemwise(np.fix, array_wrap=True)
A:dask.array.ufunc.i0->wrap_elemwise(np.i0, array_wrap=True)
A:dask.array.ufunc.sinc->wrap_elemwise(np.sinc, array_wrap=True)
A:dask.array.ufunc.nan_to_num->wrap_elemwise(np.nan_to_num, array_wrap=True)
A:dask.array.ufunc.deg->bool(deg)
A:dask.array.ufunc.tmp->elemwise(np.modf, x, dtype=object)
A:dask.array.ufunc.ldsk->dict((((left,) + key[1:], (getitem, key, 0)) for key in core.flatten(tmp._keys())))
A:dask.array.ufunc.rdsk->dict((((right,) + key[1:], (getitem, key, 1)) for key in core.flatten(tmp._keys())))
A:dask.array.ufunc.a->numpy.empty((1,), dtype=x.dtype)
A:dask.array.ufunc.(l, r)->numpy.modf(a)
A:dask.array.ufunc.L->Array(sharedict.merge(tmp.dask, (left, ldsk)), left, chunks=tmp.chunks, dtype=ldt)
A:dask.array.ufunc.R->Array(sharedict.merge(tmp.dask, (right, rdsk)), right, chunks=tmp.chunks, dtype=rdt)
dask.array.angle(x,deg=0)
dask.array.frexp(x)
dask.array.modf(x)
dask.array.ufunc.__array_wrap__(numpy_ufunc,x,*args,**kwargs)
dask.array.ufunc.angle(x,deg=0)
dask.array.ufunc.copy_docstring(target,source=None)
dask.array.ufunc.frexp(x)
dask.array.ufunc.modf(x)
dask.array.ufunc.ufunc(self,ufunc)
dask.array.ufunc.ufunc.__dir__(self)
dask.array.ufunc.ufunc.__getattr__(self,key)
dask.array.ufunc.ufunc.__init__(self,ufunc)
dask.array.ufunc.ufunc.__repr__(self)
dask.array.ufunc.ufunc.outer(self,A,B,**kwargs)
dask.array.ufunc.wrap_elemwise(numpy_ufunc,array_wrap=False)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/array/stats.py----------------------------------------
A:dask.array.stats.v1->dask.array.var(a, axis, ddof=1)
A:dask.array.stats.v2->dask.array.var(b, axis, ddof=1)
A:dask.array.stats.(df, denom)->_unequal_var_ttest_denom(v1, n1, v2, n2)
A:dask.array.stats.res->_ttest_ind_from_stats(da.mean(a, axis), da.mean(b, axis), denom, df)
A:dask.array.stats.v->dask.array.var(d, axis, ddof=1)
A:dask.array.stats.denom->dask.array.sqrt(vn1 + vn2)
A:dask.array.stats.t->dask.array.divide(d, denom)
A:dask.array.stats.(t, prob)->_ttest_finish(df, t)
A:dask.array.stats.df->dask.array.where(da.isnan(df), 1, df)
A:dask.array.stats.d->(a - b).astype(np.float64)
A:dask.array.stats.dm->dask.array.mean(d, axis)
A:dask.array.stats.f_exp->f_obs.mean(axis=axis, keepdims=True)
A:dask.array.stats.stat->terms.sum(axis=axis)
A:dask.array.stats.num_obs->_count(terms, axis=axis)
A:dask.array.stats.p->delayed(distributions.chi2.sf)(stat, num_obs - 1 - ddof)
A:dask.array.stats.m2->moment(a, 2, axis)
A:dask.array.stats.m3->moment(a, 3, axis)
A:dask.array.stats.vals->dask.array.where(zero, 0, m4 / m2 ** 2.0)
A:dask.array.stats.b2->kurtosis(a, axis, fisher=False)
A:dask.array.stats.n->float(a.shape[axis])
A:dask.array.stats.alpha->math.sqrt(2.0 / (W2 - 1))
A:dask.array.stats.y->numpy.where(y == 0, 1, y)
A:dask.array.stats.m4->moment(a, 4, axis)
A:dask.array.stats.olderr->numpy.seterr(all='ignore')
A:dask.array.stats.term2->numpy.where(denom < 0, term1, np.power((1 - 2.0 / A) / denom, 1 / 3.0))
A:dask.array.stats.Z->numpy.where(denom == 99, 0, Z)
A:dask.array.stats.(s, _)->skewtest(a, axis)
A:dask.array.stats.(k, _)->kurtosistest(a, axis)
A:dask.array.stats.num_groups->len(args)
A:dask.array.stats.alldata->dask.array.concatenate(args)
A:dask.array.stats.bign->len(alldata)
A:dask.array.stats.offset->dask.array.concatenate(args).mean()
A:dask.array.stats.prob->_fdtrc(dfbn, dfwn, f)
A:dask.array.stats._xlogy->wrap_elemwise(special.xlogy)
A:dask.array.stats._fdtrc->wrap_elemwise(special.fdtrc)
A:dask.array.stats.s->dask.array.sum(a, axis)
dask.array.stats._count(x,axis=None)
dask.array.stats._equal_var_ttest_denom(v1,n1,v2,n2)
dask.array.stats._square_of_sums(a,axis=0)
dask.array.stats._sum_of_squares(a,axis=0)
dask.array.stats._ttest_finish(df,t)
dask.array.stats._ttest_ind_from_stats(mean1,mean2,denom,df)
dask.array.stats._unequal_var_ttest_denom(v1,n1,v2,n2)
dask.array.stats.chisquare(f_obs,f_exp=None,ddof=0,axis=0)
dask.array.stats.f_oneway(*args)
dask.array.stats.kurtosis(a,axis=0,fisher=True,bias=True,nan_policy='propagate')
dask.array.stats.kurtosistest(a,axis=0,nan_policy='propagate')
dask.array.stats.moment(a,moment=1,axis=0,nan_policy='propagate')
dask.array.stats.normaltest(a,axis=0,nan_policy='propagate')
dask.array.stats.power_divergence(f_obs,f_exp=None,ddof=0,axis=0,lambda_=None)
dask.array.stats.skew(a,axis=0,bias=True,nan_policy='propagate')
dask.array.stats.skewtest(a,axis=0,nan_policy='propagate')
dask.array.stats.ttest_1samp(a,popmean,axis=0,nan_policy='propagate')
dask.array.stats.ttest_ind(a,b,axis=0,equal_var=True)
dask.array.stats.ttest_rel(a,b,axis=0,nan_policy='propagate')


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/array/routines.py----------------------------------------
A:dask.array.routines.x->asarray(x)
A:dask.array.routines.tup->tuple((atleast_3d(x) for x in tup))
A:dask.array.routines.ind->argwhere(a)
A:dask.array.routines.out->atop(np.compress, inds, condition, (inds[axis],), a, inds, axis=axis, dtype=a.dtype)
A:dask.array.routines.axes->tuple(axes)
A:dask.array.routines.ALPHABET->alphabet.upper()
A:dask.array.routines.tensordot->core.tensordot_lookup.dispatch(type(x))
A:dask.array.routines.left_axes->tuple(left_axes)
A:dask.array.routines.right_axes->tuple(right_axes)
A:dask.array.routines.dt->numpy.promote_types(lhs.dtype, rhs.dtype)
A:dask.array.routines.left_index->list(alphabet[:lhs.ndim])
A:dask.array.routines.right_index->list(ALPHABET[:rhs.ndim])
A:dask.array.routines.intermediate->atop(_tensordot, out_index, lhs, left_index, rhs, right_index, dtype=dt, axes=(left_axes, right_axes))
A:dask.array.routines.result->result.reshape(array.shape).reshape(array.shape)
A:dask.array.routines.arr->arr.rechunk(arr.chunks[:axis] + (arr.shape[axis:axis + 1],) + arr.chunks[axis + 1:]).rechunk(arr.chunks[:axis] + (arr.shape[axis:axis + 1],) + arr.chunks[axis + 1:])
A:dask.array.routines.axis->tuple((i for (i, d) in enumerate(a.shape) if d == 1))
A:dask.array.routines.test_data->numpy.ones((1,), dtype=arr.dtype)
A:dask.array.routines.test_result->numpy.array(func1d(test_data, *args, **kwargs))
A:dask.array.routines.a->asarray(a)
A:dask.array.routines.n->Array(all_dsk, name, chunks, dtype=dtype).sum(axis=0)
A:dask.array.routines.sl_1[axis]->slice(1, None)
A:dask.array.routines.sl_2[axis]->slice(None, -1)
A:dask.array.routines.sl_1->tuple(sl_1)
A:dask.array.routines.sl_2->tuple(sl_2)
A:dask.array.routines.ary->asarray(ary)
A:dask.array.routines.aryf->asarray(ary).flatten()
A:dask.array.routines.r->concatenate(r)
A:dask.array.routines.token->tokenize(k, x)
A:dask.array.routines.dsk->dict((((name,) + key[1:], (chunk.coarsen, reduction, key, axes, trim_excess)) for key in flatten(x._keys())))
A:dask.array.routines.bins->numpy.linspace(mn, mx, bins + 1, endpoint=True)
A:dask.array.routines.nchunks->len(list(flatten(a._keys())))
A:dask.array.routines.a_keys->flatten(a._keys())
A:dask.array.routines.w_keys->flatten(weights._keys())
A:dask.array.routines.all_dsk->sharedict.merge(a.dask, (name, dsk))
A:dask.array.routines.mapped->Array(all_dsk, name, chunks, dtype=dtype)
A:dask.array.routines.db->from_array(np.diff(bins).astype(float), chunks=n.chunks)
A:dask.array.routines.m->asarray(m)
A:dask.array.routines.dtype->getattr(values, 'dtype', type(values))
A:dask.array.routines.y->asarray(y)
A:dask.array.routines.X->concatenate((X, y), axis)
A:dask.array.routines.fact->float(N - ddof)
A:dask.array.routines.c->cov(x, y, rowvar)
A:dask.array.routines.d->d.reshape((d.shape[0], 1)).reshape((d.shape[0], 1))
A:dask.array.routines.sqr_d->sqrt(d)
A:dask.array.routines.parts->core.Array._get(sharedict.merge((name, dsk), x.dask), list(dsk.keys()))
A:dask.array.routines.sl1[i]->slice(s, None)
A:dask.array.routines.sl2[i]->slice(None, s)
A:dask.array.routines.sl1->tuple(sl1)
A:dask.array.routines.sl2->tuple(sl2)
A:dask.array.routines.b->asarray(a).map_blocks(partial(np.squeeze, axis=axis), dtype=a.dtype)
A:dask.array.routines.chunks->tuple((tuple((int(bd // axes.get(i, 1)) for bd in bds)) for (i, bds) in enumerate(x.chunks)))
A:dask.array.routines.old_keys->list(product([b.name], *[range(len(bd)) for bd in b.chunks]))
A:dask.array.routines.new_keys->list(product([name], *[range(len(bd)) for bd in chunks]))
A:dask.array.routines.condition->numpy.array(condition, dtype=bool)
A:dask.array.routines.inds->tuple(range(a.ndim))
A:dask.array.routines.out._chunks->tuple(((np.NaN,) * len(c) if i == axis else c for (i, c) in enumerate(out.chunks)))
A:dask.array.routines.func->partial(numpy_compat.isclose, rtol=rtol, atol=atol, equal_nan=equal_nan)
A:dask.array.routines._isnonzero_vec->numpy.vectorize(_isnonzero_vec, otypes=[bool])
A:dask.array.routines.nz->isnonzero(a).flatten()
A:dask.array.routines.shape->broadcast_shapes(x.shape, y.shape)
A:dask.array.routines.reduction->getattr(np, reduction.__name__)
A:dask.array.routines.padded_breaks->concat([[None], breaks, [None]])
A:dask.array.routines.obj->numpy.where(obj < 0, obj + arr.shape[axis], obj)
A:dask.array.routines.split_arr->split_at_breaks(arr, np.unique(obj), axis)
A:dask.array.routines.values->values.rechunk(values_chunks).rechunk(values_chunks)
A:dask.array.routines.values_shape->tuple((len(obj) if axis == n else s for (n, s) in enumerate(arr.shape)))
A:dask.array.routines.values_chunks->tuple((values_bd if axis == n else arr_bd for (n, (arr_bd, values_bd)) in enumerate(zip(arr.chunks, values.chunks))))
A:dask.array.routines.values_breaks->numpy.cumsum(counts[counts > 0])
A:dask.array.routines.split_values->split_at_breaks(values, values_breaks, axis)
A:dask.array.routines.interleaved->list(interleave([split_arr, split_values]))
dask.array.apply_along_axis(func1d,axis,arr,*args,**kwargs)
dask.array.apply_over_axes(func,a,axes)
dask.array.argwhere(a)
dask.array.around(x,decimals=0)
dask.array.array(x,dtype=None,ndmin=None)
dask.array.bincount(x,weights=None,minlength=None)
dask.array.choose(a,choices)
dask.array.coarsen(reduction,x,axes,trim_excess=False)
dask.array.compress(condition,a,axis=None)
dask.array.corrcoef(x,y=None,rowvar=1)
dask.array.count_nonzero(a,axis=None)
dask.array.cov(m,y=None,rowvar=1,bias=0,ddof=None)
dask.array.diff(a,n=1,axis=-1)
dask.array.digitize(a,bins,right=False)
dask.array.dot(a,b)
dask.array.dstack(tup)
dask.array.ediff1d(ary,to_end=None,to_begin=None)
dask.array.extract(condition,arr)
dask.array.flatnonzero(a)
dask.array.histogram(a,bins=None,range=None,normed=False,weights=None,density=None)
dask.array.hstack(tup)
dask.array.insert(arr,obj,values,axis)
dask.array.isclose(arr1,arr2,rtol=1e-05,atol=1e-08,equal_nan=False)
dask.array.isnull(values)
dask.array.nonzero(a)
dask.array.notnull(values)
dask.array.ptp(a,axis=None)
dask.array.ravel(array)
dask.array.result_type(*args)
dask.array.roll(array,shift,axis=None)
dask.array.round(a,decimals=0)
dask.array.routines._inner_apply_along_axis(arr,func1d,func1d_axis,func1d_args,func1d_kwargs)
dask.array.routines._isnonzero_vec(v)
dask.array.routines._take_dask_array_from_numpy(a,indices,axis)
dask.array.routines._tensordot(a,b,axes)
dask.array.routines.apply_along_axis(func1d,axis,arr,*args,**kwargs)
dask.array.routines.apply_over_axes(func,a,axes)
dask.array.routines.argwhere(a)
dask.array.routines.around(x,decimals=0)
dask.array.routines.array(x,dtype=None,ndmin=None)
dask.array.routines.atleast_2d(x)
dask.array.routines.atleast_3d(x)
dask.array.routines.bincount(x,weights=None,minlength=None)
dask.array.routines.choose(a,choices)
dask.array.routines.coarsen(reduction,x,axes,trim_excess=False)
dask.array.routines.compress(condition,a,axis=None)
dask.array.routines.corrcoef(x,y=None,rowvar=1)
dask.array.routines.count_nonzero(a,axis=None)
dask.array.routines.cov(m,y=None,rowvar=1,bias=0,ddof=None)
dask.array.routines.diff(a,n=1,axis=-1)
dask.array.routines.digitize(a,bins,right=False)
dask.array.routines.dot(a,b)
dask.array.routines.dstack(tup)
dask.array.routines.ediff1d(ary,to_end=None,to_begin=None)
dask.array.routines.extract(condition,arr)
dask.array.routines.flatnonzero(a)
dask.array.routines.histogram(a,bins=None,range=None,normed=False,weights=None,density=None)
dask.array.routines.hstack(tup)
dask.array.routines.insert(arr,obj,values,axis)
dask.array.routines.isclose(arr1,arr2,rtol=1e-05,atol=1e-08,equal_nan=False)
dask.array.routines.isnonzero(a)
dask.array.routines.isnull(values)
dask.array.routines.nonzero(a)
dask.array.routines.notnull(values)
dask.array.routines.ptp(a,axis=None)
dask.array.routines.ravel(array)
dask.array.routines.result_type(*args)
dask.array.routines.roll(array,shift,axis=None)
dask.array.routines.round(a,decimals=0)
dask.array.routines.split_at_breaks(array,breaks,axis=0)
dask.array.routines.squeeze(a,axis=None)
dask.array.routines.swapaxes(a,axis1,axis2)
dask.array.routines.take(a,indices,axis=0)
dask.array.routines.tensordot(lhs,rhs,axes=2)
dask.array.routines.topk(k,x)
dask.array.routines.transpose(a,axes=None)
dask.array.routines.unique(x)
dask.array.routines.variadic_choose(a,*choices)
dask.array.routines.vstack(tup)
dask.array.routines.where(condition,x=None,y=None)
dask.array.squeeze(a,axis=None)
dask.array.swapaxes(a,axis1,axis2)
dask.array.take(a,indices,axis=0)
dask.array.tensordot(lhs,rhs,axes=2)
dask.array.topk(k,x)
dask.array.transpose(a,axes=None)
dask.array.unique(x)
dask.array.vstack(tup)
dask.array.where(condition,x=None,y=None)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/array/chunk.py----------------------------------------
A:dask.array.chunk.r->a_callable(x, *args, axis=axis, **kwargs)
A:dask.array.chunk.axes->range(x.ndim)
A:dask.array.chunk.r_slice->tuple()
A:dask.array.chunk.sum->keepdims_wrapper(np.sum)
A:dask.array.chunk.prod->keepdims_wrapper(np.prod)
A:dask.array.chunk.min->keepdims_wrapper(np.min)
A:dask.array.chunk.max->keepdims_wrapper(np.max)
A:dask.array.chunk.argmin->keepdims_wrapper(np.argmin)
A:dask.array.chunk.nanargmin->keepdims_wrapper(np.nanargmin)
A:dask.array.chunk.argmax->keepdims_wrapper(np.argmax)
A:dask.array.chunk.nanargmax->keepdims_wrapper(np.nanargmax)
A:dask.array.chunk.any->keepdims_wrapper(np.any)
A:dask.array.chunk.all->keepdims_wrapper(np.all)
A:dask.array.chunk.nansum->keepdims_wrapper(np.nansum)
A:dask.array.chunk.nanprod->keepdims_wrapper(nanprod)
A:dask.array.chunk.nancumprod->keepdims_wrapper(nancumprod)
A:dask.array.chunk.nancumsum->keepdims_wrapper(nancumsum)
A:dask.array.chunk.nanmin->keepdims_wrapper(np.nanmin)
A:dask.array.chunk.nanmax->keepdims_wrapper(np.nanmax)
A:dask.array.chunk.mean->keepdims_wrapper(np.mean)
A:dask.array.chunk.nanmean->keepdims_wrapper(np.nanmean)
A:dask.array.chunk.var->keepdims_wrapper(np.var)
A:dask.array.chunk.nanvar->keepdims_wrapper(np.nanvar)
A:dask.array.chunk.std->keepdims_wrapper(np.std)
A:dask.array.chunk.nanstd->keepdims_wrapper(np.nanstd)
A:dask.array.chunk.ind->tuple((slice(0, -(d % axes[i])) if d % axes[i] else slice(None, None) for (i, d) in enumerate(x.shape)))
A:dask.array.chunk.newshape->tuple(concat([(x.shape[i] // axes[i], axes[i]) for i in range(x.ndim)]))
A:dask.array.chunk.k->numpy.minimum(k, len(x))
A:dask.array.chunk.res->numpy.arange(start, stop, step, dtype)
dask.array.chunk.arange(start,stop,step,length,dtype)
dask.array.chunk.astype(x,astype_dtype=None,**kwargs)
dask.array.chunk.coarsen(reduction,x,axes,trim_excess=False)
dask.array.chunk.keepdims_wrapper(a_callable)
dask.array.chunk.topk(k,x)
dask.array.chunk.trim(x,axes=None)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/array/linalg.py----------------------------------------
A:dask.array.linalg.dsk_qr_st1->top(np.linalg.qr, name_qr_st1, 'ij', data.name, 'ij', numblocks={data.name: numblocks})
A:dask.array.linalg.dsk_q_st1->dict((((name_q_st1, i, 0), (operator.getitem, (name_qr_st1, i, 0), 0)) for i in range(numblocks[0])))
A:dask.array.linalg.dsk_r_st1->dict((((name_r_st1, i, 0), (operator.getitem, (name_qr_st1, i, 0), 1)) for i in range(numblocks[0])))
A:dask.array.linalg.dsk_qr_st2->top(np.linalg.qr, name_qr_st2, 'ij', name_r_st1_stacked, 'ij', numblocks={name_r_st1_stacked: (1, 1)})
A:dask.array.linalg.dsk_q_blockslices->toolz.merge(dsk_n, dsk_q2_shapes, dsk_q2_cumsum, dsk_block_slices)
A:dask.array.linalg.dsk_q_st2->dict((((name_q_st2, i, 0), (operator.getitem, (name_q_st2_aux, 0, 0), b)) for (i, b) in enumerate(block_slices)))
A:dask.array.linalg.dsk_q_st3->top(np.dot, name_q_st3, 'ij', name_q_st1, 'ij', name_q_st2, 'ij', numblocks={name_q_st1: numblocks, name_q_st2: numblocks})
A:dask.array.linalg.dsk->sharedict.merge(a.dask, (name, dsk))
A:dask.array.linalg.(qq, rr)->numpy.linalg.qr(np.ones(shape=(1, 1), dtype=data.dtype))
A:dask.array.linalg.q->Array(dsk, name_q_st3, shape=data.shape, chunks=data.chunks, dtype=qq.dtype)
A:dask.array.linalg.r->svd(x)[1][None].min(keepdims=keepdims)
A:dask.array.linalg.dsk_svd_st2->top(np.linalg.svd, name_svd_st2, 'ij', name_r_st2, 'ij', numblocks={name_r_st2: (1, 1)})
A:dask.array.linalg.dsk_u_st4->top(dotmany, name_u_st4, 'ij', name_q_st3, 'ik', name_u_st2, 'kj', numblocks={name_q_st3: numblocks, name_u_st2: (1, 1)})
A:dask.array.linalg.(uu, ss, vv)->numpy.linalg.svd(np.ones(shape=(1, 1), dtype=data.dtype))
A:dask.array.linalg.u->Array(dsk, name_u, shape=a.shape, chunks=a.chunks, dtype=uu.dtype)
A:dask.array.linalg.s->Array(sdsk, sname, shape=(r.shape[0],), chunks=r.shape[0], dtype=ss.dtype)
A:dask.array.linalg.v->Array(dsk, name_v_st2, shape=(n, n), chunks=((n,), (n,)), dtype=vv.dtype)
A:dask.array.linalg.comp_level->compression_level(n, q)
A:dask.array.linalg.state->RandomState(seed)
A:dask.array.linalg.omega->RandomState(seed).standard_normal(size=(n, comp_level), chunks=(data.chunks[1], (comp_level,)))
A:dask.array.linalg.mat_h->data.dot(data.T.dot(mat_h))
A:dask.array.linalg.(q, _)->tsqr(mat_h)
A:dask.array.linalg.comp->compression_matrix(a, k, n_power_iter=n_power_iter, seed=seed)
A:dask.array.linalg.a_compressed->compression_matrix(a, k, n_power_iter=n_power_iter, seed=seed).dot(a)
A:dask.array.linalg.(v, s, u)->tsqr(a_compressed.T, name, compute_svd=True)
A:dask.array.linalg.vdim->len(a.chunks[0])
A:dask.array.linalg.hdim->len(a.chunks[1])
A:dask.array.linalg.token->tokenize(a, b)
A:dask.array.linalg.(pp, ll, uu)->scipy.linalg.lu(np.ones(shape=(1, 1), dtype=a.dtype))
A:dask.array.linalg.p->Array(dsk, name_p, shape=a.shape, chunks=a.chunks, dtype=pp.dtype)
A:dask.array.linalg.l->Array(dsk, name_l, shape=a.shape, chunks=a.chunks, dtype=ll.dtype)
A:dask.array.linalg.vchunks->len(a.chunks[1])
A:dask.array.linalg.target->_b_init(i, j)
A:dask.array.linalg.res->_solve_triangular_lower(np.array([[1, 0], [1, 2]], dtype=a.dtype), np.array([0, 1], dtype=b.dtype))
A:dask.array.linalg.(l, u)->_cholesky(a)
A:dask.array.linalg.(p, l, u)->lu(a)
A:dask.array.linalg.b->Array(dsk, name_p, shape=a.shape, chunks=a.chunks, dtype=pp.dtype).T.dot(b)
A:dask.array.linalg.uy->solve_triangular(l, b, lower=True)
A:dask.array.linalg.cho->scipy.linalg.cholesky(np.array([[1, 2], [2, 5]], dtype=a.dtype))
A:dask.array.linalg.lower->Array(dsk, name, shape=a.shape, chunks=a.chunks, dtype=cho.dtype)
A:dask.array.linalg.upper->Array(dsk, name_upper, shape=a.shape, chunks=a.chunks, dtype=cho.dtype)
A:dask.array.linalg.(q, r)->qr(a)
A:dask.array.linalg.x->solve_triangular(r, q.T.dot(b))
A:dask.array.linalg.residuals->(residuals ** 2).sum(keepdims=True)
A:dask.array.linalg.rdsk->sharedict.merge(r.dask, (rname, rdsk))
A:dask.array.linalg.rank->Array(rdsk, rname, shape=(), chunks=(), dtype=int)
A:dask.array.linalg.sdsk->sharedict.merge(rt.dask, (sname, sdsk))
A:dask.array.linalg.(_, _, _, ss)->numpy.linalg.lstsq(np.array([[1, 0], [1, 2]], dtype=a.dtype), np.array([0, 1], dtype=b.dtype))
A:dask.array.linalg.axis->tuple(axis)
dask.array.linalg._cholesky(a)
dask.array.linalg._cholesky_lower(a)
dask.array.linalg._cumsum_blocks(it)
dask.array.linalg._cumsum_part(last,new)
dask.array.linalg._solve_triangular_lower(a,b)
dask.array.linalg._sort_decreasing(x)
dask.array.linalg.cholesky(a,lower=False)
dask.array.linalg.compression_level(n,q,oversampling=10,min_subspace_size=20)
dask.array.linalg.compression_matrix(data,q,n_power_iter=0,seed=None)
dask.array.linalg.inv(a)
dask.array.linalg.lstsq(a,b)
dask.array.linalg.lu(a)
dask.array.linalg.norm(x,ord=None,axis=None,keepdims=False)
dask.array.linalg.qr(a,name=None)
dask.array.linalg.solve(a,b,sym_pos=False)
dask.array.linalg.solve_triangular(a,b,lower=False)
dask.array.linalg.svd(a,name=None)
dask.array.linalg.svd_compressed(a,k,n_power_iter=0,seed=None,name=None)
dask.array.linalg.tsqr(data,name=None,compute_svd=False)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/array/utils.py----------------------------------------
A:dask.array.utils.a_nans->numpy.isnan(a)
A:dask.array.utils.b_nans->numpy.isnan(b)
A:dask.array.utils.freqs->frequencies(concat(dsk.dicts.values()))
A:dask.array.utils.a->a.todense().todense()
A:dask.array.utils.adt->getattr(a, 'dtype', None)
A:dask.array.utils.b->b.todense().todense()
A:dask.array.utils.bdt->getattr(b, 'dtype', None)
A:dask.array.utils.diff->difflib.ndiff(str(adt).splitlines(), str(bdt).splitlines())
dask.array.utils._check_dsk(dsk)
dask.array.utils._not_empty(x)
dask.array.utils.assert_eq(a,b,check_shape=True,**kwargs)
dask.array.utils.assert_eq_shape(a,b,check_nan=True)
dask.array.utils.same_keys(a,b)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/array/learn.py----------------------------------------
A:dask.array.learn.x->x.reblock(chunks=(x.chunks[0], sum(x.chunks[1]))).reblock(chunks=(x.chunks[0], sum(x.chunks[1])))
A:dask.array.learn.nblocks->len(x.chunks[0])
A:dask.array.learn.func->partial(_predict, model)
A:dask.array.learn.xx->numpy.zeros((1, x.shape[1]), dtype=x.dtype)
dask.array.learn._partial_fit(model,x,y,kwargs=None)
dask.array.learn._predict(model,x)
dask.array.learn.fit(model,x,y,get=threaded.get,**kwargs)
dask.array.learn.predict(model,x)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/array/core.py----------------------------------------
A:dask.array.core.concatenate_lookup->Dispatch('concatenate')
A:dask.array.core.tensordot_lookup->Dispatch('tensordot')
A:dask.array.core.b2->tuple((x for x in b if x is not None))
A:dask.array.core.b3->tuple((None if x is None else slice(None, None) for x in b if not isinstance(x, (int, long))))
A:dask.array.core.c->super(Array, cls).__new__(cls).copy()
A:dask.array.core.shapes->product(*chunks)
A:dask.array.core.starts->product(*cumdims)
A:dask.array.core.chunks->tuple((c if i == axis else (sum(c),) for (i, c) in enumerate(x.chunks)))
A:dask.array.core.keys->list(product([name], *[range(len(c)) for c in chunks]))
A:dask.array.core.slices->slices_from_chunks(arr.chunks)
A:dask.array.core.A->map(leftfunc, A)
A:dask.array.core.B->map(rightfunc, B)
A:dask.array.core.L->ndeepmap(self.ndim, lambda k: Delayed(k, dsk), self._keys())
A:dask.array.core.g->dict(((k, set([d for (i, d) in v])) for (k, v) in g.items()))
A:dask.array.core.g2->dict(((k, v - set(sentinels) if len(v) > 1 else v) for (k, v) in g.items()))
A:dask.array.core.numblocks->list(arrs[0].numblocks)
A:dask.array.core.concatenate->Dispatch('concatenate').dispatch(type(max(arrays, key=lambda x: x.__array_priority__)))
A:dask.array.core.new_axes->kwargs.get('new_axes', {})
A:dask.array.core.argpairs->list(partition(2, arrind_pairs))
A:dask.array.core.all_indices->pipe(argpairs, pluck(1), filter(None), concat, set)
A:dask.array.core.dims->broadcast_dimensions(argpairs, numblocks)
A:dask.array.core.keytups->list(product(*[range(dims[i]) for i in out_indices]))
A:dask.array.core.dummies->dict(((i, list(range(dims[i]))) for i in dummy_indices))
A:dask.array.core.(arg2, dsk2)->to_task_dask(arg)
A:dask.array.core.tups->lol_tuples((arg,), ind, kd, dummies)
A:dask.array.core.tups2->zero_broadcast_dimensions(tups, numblocks[arg])
A:dask.array.core.(task, dsk2)->to_task_dask(kwargs)
A:dask.array.core.arrays->concrete(arrays)
A:dask.array.core.o->func(*args, **kwargs)
A:dask.array.core.(exc_type, exc_value, exc_traceback)->sys.exc_info()
A:dask.array.core.tb->''.join(traceback.format_tb(exc_traceback))
A:dask.array.core.msg->'`dtype` inference failed in `{0}`.\n\n{1}Original error is below:\n------------------------\n{2}\n\nTraceback:\n---------\n{3}'.format(funcname, suggest, repr(e), tb)
A:dask.array.core.name->kwargs.pop('name', None)
A:dask.array.core.token->tokenize(x, indexes)
A:dask.array.core.dtype->kwargs.pop('enforce_dtype')
A:dask.array.core.drop_axis->kwargs.pop('drop_axis', [])
A:dask.array.core.new_axis->list(range(self.ndim, self.ndim + len(dt.shape)))
A:dask.array.core.arginds->list(zip(arrays, args[1::2]))
A:dask.array.core.spec->getargspec(func)
A:dask.array.core.dsk->dict(zip(keys, values))
A:dask.array.core.kwargs2->assoc(kwargs, 'block_id', first(dsk.keys())[1:])
A:dask.array.core.ndim->len(indexes)
A:dask.array.core.new_key->list(key)
A:dask.array.core.chunks2->list(broadcast_chunks(*[a.chunks for a in arrs]))
A:dask.array.core.n->sum(map(len, locations))
A:dask.array.core.update->insert_to_ooc(tgt, src, lock=lock, region=reg)
A:dask.array.core.shape->tuple(shape)
A:dask.array.core.CHUNKS_NONE_ERROR_MESSAGE->'\nYou must specify a chunks= keyword argument.\nThis specifies the chunksize of your array blocks.\n\nSee the following documentation page for details:\n  http://dask.pydata.org/en/latest/array-creation.html#chunks\n'.strip()
A:dask.array.core._optimize->staticmethod(optimize)
A:dask.array.core._default_get->staticmethod(threaded.get)
A:dask.array.core._finalize->staticmethod(finalize)
A:dask.array.core.self->super(Array, cls).__new__(cls)
A:dask.array.core.s->ShareDict()
A:dask.array.core.self._chunks->normalize_chunks(chunks, shape)
A:dask.array.core.self.dtype->numpy.dtype(dtype)
A:dask.array.core.result->numpy.empty(shape=shape, dtype=dtype(deepfirst(arrays)))
A:dask.array.core.out->kwargs.pop('out', None)
A:dask.array.core.da_ufunc->getattr(ufunc, numpy_ufunc.__name__)
A:dask.array.core.chunksize->str(tuple((c[0] for c in self.chunks)))
A:dask.array.core.ind->list(range(ndim))
A:dask.array.core.x->numpy.empty(shape, dtype=dtype)
A:dask.array.core.y->where(key, value, self)
A:dask.array.core.dt->reduce(np.promote_types, [a.dtype for a in seq])
A:dask.array.core.index2->normalize_index(index, self.shape)
A:dask.array.core.(self, index2)->slice_with_dask_array(self, index2)
A:dask.array.core.(dsk, chunks)->slice_array(out, self.name, self.chunks, index2)
A:dask.array.core.dsk2->sharedict.merge((name, dsk), *[a.dask for a in seq])
A:dask.array.core.casting->kwargs.get('casting', 'unsafe')
A:dask.array.core.copy->kwargs.get('copy', True)
A:dask.array.core.other->numpy.asarray(other)
A:dask.array.core.i->int(f)
A:dask.array.core.lock->Lock()
A:dask.array.core.value->delayed(value)
A:dask.array.core.func->partial(func, *args, **kwargs)
A:dask.array.core.non_trivial_dims->set([d for d in blockdims if len(d) > 1])
A:dask.array.core.total->sum(first(non_trivial_dims))
A:dask.array.core.m->min((c[-1] for c in rchunks))
A:dask.array.core.args->list(concat(arginds))
A:dask.array.core.warn->kwargs.get('warn', True)
A:dask.array.core.(arrays, inds)->zip(*arginds)
A:dask.array.core.chunkss->broadcast_dimensions(nameinds, blockdim_dict, consolidate=common_blockdim)
A:dask.array.core.max_parts->max((arg.npartitions for (arg, ind) in arginds if ind is not None))
A:dask.array.core.nparts->numpy.prod(list(map(len, chunkss.values())))
A:dask.array.core.adjust_chunks->kwargs.pop('adjust_chunks', None)
A:dask.array.core.(chunkss, arrays)->unify_chunks(*args)
A:dask.array.core.argindsstr->list(concat([(a if ind is None else a.name, ind) for (a, ind) in arginds]))
A:dask.array.core.chunks[i]->tuple(adjust_chunks[ind])
A:dask.array.core.uc_args->list(concat(((x, ind) for x in seq)))
A:dask.array.core.(_, seq)->unify_chunks(*uc_args)
A:dask.array.core.out[index]->numpy.asanyarray(x)
A:dask.array.core.out[fuse_slice(region, index)]->numpy.asanyarray(x)
A:dask.array.core.array->numpy.asanyarray(array)
A:dask.array.core.dim->max(sizes)
A:dask.array.core.out_ndim->len(broadcast_shapes(*shapes))
A:dask.array.core.need_enforce_dtype->any((not is_scalar_for_elemwise(a) and a.ndim == 0 for a in args))
A:dask.array.core.atop_kwargs->dict(dtype=dt, name=name, token=funcname(op).strip('_'))
A:dask.array.core.function->kwargs.pop('enforce_dtype_function')
A:dask.array.core.args2->list(map(add, args, offset))
A:dask.array.core.advanced->max(core.flatten(arrays, container=(list, tuple)), key=lambda x: getattr(x, '__array_priority__', 0))
A:dask.array.core.extradims->max(0, deepfirst(arrays).ndim - (max(axes) + 1))
A:dask.array.core.indexes->replace_ellipsis(x.ndim, indexes)
A:dask.array.core.key->tuple((partial_slices.get(i, slice(None)) for i in range(len(indexes))))
A:dask.array.core.broadcast_indexes->numpy.broadcast_arrays(*array_indexes.values())
A:dask.array.core.shapes_str->' '.join((str(a.shape) for a in array_indexes.values()))
A:dask.array.core.lookup->dict(zip(array_indexes, broadcast_indexes))
A:dask.array.core.result_1d->_vindex_1d(x, *flat_indexes)
A:dask.array.core.axis->_get_axis(indexes)
A:dask.array.core.points->list()
A:dask.array.core.per_block->dict(((k, v) for (k, v) in per_block.items() if v))
A:dask.array.core.other_blocks->list(product(*[list(range(len(c))) if i is None else [None] for (i, c) in zip(indexes, x.chunks)]))
A:dask.array.core.locations->list(map(list, locations))
A:dask.array.core.values->list(values)
A:dask.array.core.xx->numpy.empty(shape, dtype=dtype).rechunk(chunks)
A:dask.array.core.info->pickle.load(f)
dask.array.Array(cls,dask,name,chunks,dtype,shape=None)
dask.array.Array.A(self)
dask.array.Array.T(self)
dask.array.Array.__abs__(self)
dask.array.Array.__add__(self,other)
dask.array.Array.__and__(self,other)
dask.array.Array.__array__(self,dtype=None,**kwargs)
dask.array.Array.__array_ufunc__(self,numpy_ufunc,method,*inputs,**kwargs)
dask.array.Array.__bool__(self)
dask.array.Array.__complex__(self)
dask.array.Array.__deepcopy__(self,memo)
dask.array.Array.__div__(self,other)
dask.array.Array.__eq__(self,other)
dask.array.Array.__float__(self)
dask.array.Array.__floordiv__(self,other)
dask.array.Array.__ge__(self,other)
dask.array.Array.__getitem__(self,index)
dask.array.Array.__gt__(self,other)
dask.array.Array.__int__(self)
dask.array.Array.__invert__(self)
dask.array.Array.__le__(self,other)
dask.array.Array.__len__(self)
dask.array.Array.__lshift__(self,other)
dask.array.Array.__lt__(self,other)
dask.array.Array.__matmul__(self,other)
dask.array.Array.__mod__(self,other)
dask.array.Array.__mul__(self,other)
dask.array.Array.__ne__(self,other)
dask.array.Array.__neg__(self)
dask.array.Array.__or__(self,other)
dask.array.Array.__pos__(self)
dask.array.Array.__pow__(self,other)
dask.array.Array.__radd__(self,other)
dask.array.Array.__rand__(self,other)
dask.array.Array.__rdiv__(self,other)
dask.array.Array.__reduce__(self)
dask.array.Array.__repr__(self)
dask.array.Array.__rfloordiv__(self,other)
dask.array.Array.__rlshift__(self,other)
dask.array.Array.__rmatmul__(self,other)
dask.array.Array.__rmod__(self,other)
dask.array.Array.__rmul__(self,other)
dask.array.Array.__ror__(self,other)
dask.array.Array.__rpow__(self,other)
dask.array.Array.__rrshift__(self,other)
dask.array.Array.__rshift__(self,other)
dask.array.Array.__rsub__(self,other)
dask.array.Array.__rtruediv__(self,other)
dask.array.Array.__rxor__(self,other)
dask.array.Array.__setitem__(self,key,value)
dask.array.Array.__sub__(self,other)
dask.array.Array.__truediv__(self,other)
dask.array.Array.__xor__(self,other)
dask.array.Array._elemwise(self)
dask.array.Array._get_chunks(self)
dask.array.Array._keys(self,*args)
dask.array.Array._set_chunks(self,chunks)
dask.array.Array._vindex(self,key)
dask.array.Array.all(self,axis=None,keepdims=False,split_every=None,out=None)
dask.array.Array.any(self,axis=None,keepdims=False,split_every=None,out=None)
dask.array.Array.argmax(self,axis=None,split_every=None,out=None)
dask.array.Array.argmin(self,axis=None,split_every=None,out=None)
dask.array.Array.astype(self,dtype,**kwargs)
dask.array.Array.choose(self,choices)
dask.array.Array.clip(self,min=None,max=None)
dask.array.Array.conj(self)
dask.array.Array.copy(self)
dask.array.Array.cumprod(self,axis,dtype=None,out=None)
dask.array.Array.cumsum(self,axis,dtype=None,out=None)
dask.array.Array.dot(self,other)
dask.array.Array.imag(self)
dask.array.Array.itemsize(self)
dask.array.Array.map_blocks(self,func,*args,**kwargs)
dask.array.Array.map_overlap(self,func,depth,boundary=None,trim=True,**kwargs)
dask.array.Array.max(self,axis=None,keepdims=False,split_every=None,out=None)
dask.array.Array.mean(self,axis=None,dtype=None,keepdims=False,split_every=None,out=None)
dask.array.Array.min(self,axis=None,keepdims=False,split_every=None,out=None)
dask.array.Array.moment(self,order,axis=None,dtype=None,keepdims=False,ddof=0,split_every=None,out=None)
dask.array.Array.name(self)
dask.array.Array.name(self,val)
dask.array.Array.nbytes(self)
dask.array.Array.ndim(self)
dask.array.Array.nonzero(self)
dask.array.Array.npartitions(self)
dask.array.Array.numblocks(self)
dask.array.Array.prod(self,axis=None,dtype=None,keepdims=False,split_every=None,out=None)
dask.array.Array.ravel(self)
dask.array.Array.real(self)
dask.array.Array.rechunk(self,chunks,threshold=None,block_size_limit=None)
dask.array.Array.repeat(self,repeats,axis=None)
dask.array.Array.reshape(self,*shape)
dask.array.Array.round(self,decimals=0)
dask.array.Array.shape(self)
dask.array.Array.size(self)
dask.array.Array.squeeze(self)
dask.array.Array.std(self,axis=None,dtype=None,keepdims=False,ddof=0,split_every=None,out=None)
dask.array.Array.store(self,target,**kwargs)
dask.array.Array.sum(self,axis=None,dtype=None,keepdims=False,split_every=None,out=None)
dask.array.Array.swapaxes(self,axis1,axis2)
dask.array.Array.to_dask_dataframe(self,columns=None)
dask.array.Array.to_delayed(self)
dask.array.Array.to_hdf5(self,filename,datapath,**kwargs)
dask.array.Array.topk(self,k)
dask.array.Array.transpose(self,*axes)
dask.array.Array.var(self,axis=None,dtype=None,keepdims=False,ddof=0,split_every=None,out=None)
dask.array.Array.view(self,dtype,order='C')
dask.array.Array.vindex(self)
dask.array.Array.vnorm(self,ord=None,axis=None,keepdims=False,split_every=None,out=None)
dask.array.asanyarray(array)
dask.array.asarray(array)
dask.array.atop(func,out_ind,*args,**kwargs)
dask.array.broadcast_to(x,shape)
dask.array.concatenate(seq,axis=0,allow_unknown_chunksizes=False)
dask.array.concatenate3(arrays)
dask.array.concatenate_axes(arrays,axes)
dask.array.core.Array(cls,dask,name,chunks,dtype,shape=None)
dask.array.core.Array.A(self)
dask.array.core.Array.T(self)
dask.array.core.Array.__abs__(self)
dask.array.core.Array.__add__(self,other)
dask.array.core.Array.__and__(self,other)
dask.array.core.Array.__array__(self,dtype=None,**kwargs)
dask.array.core.Array.__array_ufunc__(self,numpy_ufunc,method,*inputs,**kwargs)
dask.array.core.Array.__bool__(self)
dask.array.core.Array.__complex__(self)
dask.array.core.Array.__deepcopy__(self,memo)
dask.array.core.Array.__div__(self,other)
dask.array.core.Array.__eq__(self,other)
dask.array.core.Array.__float__(self)
dask.array.core.Array.__floordiv__(self,other)
dask.array.core.Array.__ge__(self,other)
dask.array.core.Array.__getitem__(self,index)
dask.array.core.Array.__gt__(self,other)
dask.array.core.Array.__int__(self)
dask.array.core.Array.__invert__(self)
dask.array.core.Array.__le__(self,other)
dask.array.core.Array.__len__(self)
dask.array.core.Array.__lshift__(self,other)
dask.array.core.Array.__lt__(self,other)
dask.array.core.Array.__matmul__(self,other)
dask.array.core.Array.__mod__(self,other)
dask.array.core.Array.__mul__(self,other)
dask.array.core.Array.__ne__(self,other)
dask.array.core.Array.__neg__(self)
dask.array.core.Array.__new__(cls,dask,name,chunks,dtype,shape=None)
dask.array.core.Array.__or__(self,other)
dask.array.core.Array.__pos__(self)
dask.array.core.Array.__pow__(self,other)
dask.array.core.Array.__radd__(self,other)
dask.array.core.Array.__rand__(self,other)
dask.array.core.Array.__rdiv__(self,other)
dask.array.core.Array.__reduce__(self)
dask.array.core.Array.__repr__(self)
dask.array.core.Array.__rfloordiv__(self,other)
dask.array.core.Array.__rlshift__(self,other)
dask.array.core.Array.__rmatmul__(self,other)
dask.array.core.Array.__rmod__(self,other)
dask.array.core.Array.__rmul__(self,other)
dask.array.core.Array.__ror__(self,other)
dask.array.core.Array.__rpow__(self,other)
dask.array.core.Array.__rrshift__(self,other)
dask.array.core.Array.__rshift__(self,other)
dask.array.core.Array.__rsub__(self,other)
dask.array.core.Array.__rtruediv__(self,other)
dask.array.core.Array.__rxor__(self,other)
dask.array.core.Array.__setitem__(self,key,value)
dask.array.core.Array.__sub__(self,other)
dask.array.core.Array.__truediv__(self,other)
dask.array.core.Array.__xor__(self,other)
dask.array.core.Array._elemwise(self)
dask.array.core.Array._get_chunks(self)
dask.array.core.Array._keys(self,*args)
dask.array.core.Array._set_chunks(self,chunks)
dask.array.core.Array._vindex(self,key)
dask.array.core.Array.all(self,axis=None,keepdims=False,split_every=None,out=None)
dask.array.core.Array.any(self,axis=None,keepdims=False,split_every=None,out=None)
dask.array.core.Array.argmax(self,axis=None,split_every=None,out=None)
dask.array.core.Array.argmin(self,axis=None,split_every=None,out=None)
dask.array.core.Array.astype(self,dtype,**kwargs)
dask.array.core.Array.choose(self,choices)
dask.array.core.Array.clip(self,min=None,max=None)
dask.array.core.Array.conj(self)
dask.array.core.Array.copy(self)
dask.array.core.Array.cumprod(self,axis,dtype=None,out=None)
dask.array.core.Array.cumsum(self,axis,dtype=None,out=None)
dask.array.core.Array.dot(self,other)
dask.array.core.Array.imag(self)
dask.array.core.Array.itemsize(self)
dask.array.core.Array.map_blocks(self,func,*args,**kwargs)
dask.array.core.Array.map_overlap(self,func,depth,boundary=None,trim=True,**kwargs)
dask.array.core.Array.max(self,axis=None,keepdims=False,split_every=None,out=None)
dask.array.core.Array.mean(self,axis=None,dtype=None,keepdims=False,split_every=None,out=None)
dask.array.core.Array.min(self,axis=None,keepdims=False,split_every=None,out=None)
dask.array.core.Array.moment(self,order,axis=None,dtype=None,keepdims=False,ddof=0,split_every=None,out=None)
dask.array.core.Array.name(self)
dask.array.core.Array.name(self,val)
dask.array.core.Array.nbytes(self)
dask.array.core.Array.ndim(self)
dask.array.core.Array.nonzero(self)
dask.array.core.Array.npartitions(self)
dask.array.core.Array.numblocks(self)
dask.array.core.Array.prod(self,axis=None,dtype=None,keepdims=False,split_every=None,out=None)
dask.array.core.Array.ravel(self)
dask.array.core.Array.real(self)
dask.array.core.Array.rechunk(self,chunks,threshold=None,block_size_limit=None)
dask.array.core.Array.repeat(self,repeats,axis=None)
dask.array.core.Array.reshape(self,*shape)
dask.array.core.Array.round(self,decimals=0)
dask.array.core.Array.shape(self)
dask.array.core.Array.size(self)
dask.array.core.Array.squeeze(self)
dask.array.core.Array.std(self,axis=None,dtype=None,keepdims=False,ddof=0,split_every=None,out=None)
dask.array.core.Array.store(self,target,**kwargs)
dask.array.core.Array.sum(self,axis=None,dtype=None,keepdims=False,split_every=None,out=None)
dask.array.core.Array.swapaxes(self,axis1,axis2)
dask.array.core.Array.to_dask_dataframe(self,columns=None)
dask.array.core.Array.to_delayed(self)
dask.array.core.Array.to_hdf5(self,filename,datapath,**kwargs)
dask.array.core.Array.topk(self,k)
dask.array.core.Array.transpose(self,*axes)
dask.array.core.Array.var(self,axis=None,dtype=None,keepdims=False,ddof=0,split_every=None,out=None)
dask.array.core.Array.view(self,dtype,order='C')
dask.array.core.Array.vindex(self)
dask.array.core.Array.vnorm(self,ord=None,axis=None,keepdims=False,split_every=None,out=None)
dask.array.core._concatenate2(arrays,axes=[])
dask.array.core._enforce_dtype(*args,**kwargs)
dask.array.core._get_axis(indexes)
dask.array.core._vindex(x,*indexes)
dask.array.core._vindex_1d(x,*indexes)
dask.array.core._vindex_merge(locations,values)
dask.array.core._vindex_slice(block,points)
dask.array.core._vindex_transpose(block,axis)
dask.array.core.apply_infer_dtype(func,args,kwargs,funcname,suggest_dtype=True)
dask.array.core.asanyarray(array)
dask.array.core.asarray(array)
dask.array.core.atop(func,out_ind,*args,**kwargs)
dask.array.core.blockdims_from_blockshape(shape,chunks)
dask.array.core.broadcast_chunks(*chunkss)
dask.array.core.broadcast_dimensions(argpairs,numblocks,sentinels=(1,(1,)),consolidate=None)
dask.array.core.broadcast_shapes(*shapes)
dask.array.core.broadcast_to(x,shape)
dask.array.core.chunks_from_arrays(arrays)
dask.array.core.common_blockdim(blockdims)
dask.array.core.concatenate(seq,axis=0,allow_unknown_chunksizes=False)
dask.array.core.concatenate3(arrays)
dask.array.core.concatenate_axes(arrays,axes)
dask.array.core.deepfirst(seq)
dask.array.core.dotmany(A,B,leftfunc=None,rightfunc=None,**kwargs)
dask.array.core.elemwise(op,*args,**kwargs)
dask.array.core.ensure_int(f)
dask.array.core.finalize(results)
dask.array.core.from_array(x,chunks,name=None,lock=False,asarray=True,fancy=True,getitem=None)
dask.array.core.from_delayed(value,shape,dtype,name=None)
dask.array.core.from_func(func,shape,dtype=None,name=None,args=(),kwargs={})
dask.array.core.from_npy_stack(dirname,mmap_mode='r')
dask.array.core.getem(arr,chunks,getitem=getter,shape=None,out_name=None,lock=False,asarray=True)
dask.array.core.getter(a,b,asarray=True,lock=None)
dask.array.core.getter_inline(a,b,asarray=True,lock=None)
dask.array.core.getter_nofancy(a,b,asarray=True,lock=None)
dask.array.core.handle_out(out,result)
dask.array.core.insert_to_ooc(out,arr,lock=True,region=None)
dask.array.core.interleave_none(a,b)
dask.array.core.is_scalar_for_elemwise(arg)
dask.array.core.keyname(name,i,okey)
dask.array.core.lol_tuples(head,ind,values,dummies)
dask.array.core.map_blocks(func,*args,**kwargs)
dask.array.core.ndimlist(seq)
dask.array.core.normalize_chunks(chunks,shape=None)
dask.array.core.offset_func(func,offset,*args)
dask.array.core.register_sparse()
dask.array.core.reshapelist(shape,seq)
dask.array.core.shapelist(a)
dask.array.core.slices_from_chunks(chunks)
dask.array.core.stack(seq,axis=0)
dask.array.core.store(sources,targets,lock=True,regions=None,compute=True,**kwargs)
dask.array.core.to_hdf5(filename,*args,**kwargs)
dask.array.core.to_npy_stack(dirname,x,axis=0)
dask.array.core.top(func,output,out_indices,*arrind_pairs,**kwargs)
dask.array.core.transposelist(arrays,axes,extradims=0)
dask.array.core.unify_chunks(*args,**kwargs)
dask.array.core.unpack_singleton(x)
dask.array.core.zero_broadcast_dimensions(lol,nblocks)
dask.array.from_array(x,chunks,name=None,lock=False,asarray=True,fancy=True,getitem=None)
dask.array.from_delayed(value,shape,dtype,name=None)
dask.array.from_npy_stack(dirname,mmap_mode='r')
dask.array.map_blocks(func,*args,**kwargs)
dask.array.stack(seq,axis=0)
dask.array.store(sources,targets,lock=True,regions=None,compute=True,**kwargs)
dask.array.to_hdf5(filename,*args,**kwargs)
dask.array.to_npy_stack(dirname,x,axis=0)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/array/ghost.py----------------------------------------
A:dask.array.ghost.depth->dict(zip(range(ndim), depth))
A:dask.array.ghost.index->tuple(index)
A:dask.array.ghost.seq->list(product([k[0]], *[inds(i, ind) for (i, ind) in enumerate(k[1:])]))
A:dask.array.ghost.dims->list(map(len, x.chunks))
A:dask.array.ghost.expand_key2->partial(expand_key, dims=dims)
A:dask.array.ghost.interior_keys->pipe(x._keys(), flatten, map(expand_key2), map(flatten), concat, list)
A:dask.array.ghost.token->tokenize(x, axes)
A:dask.array.ghost.frac_slice->fractional_slice(k, axes)
A:dask.array.ghost.dsk->sharedict.merge(x.dask, (name, dsk))
A:dask.array.ghost.chunks->list(x.chunks)
A:dask.array.ghost.(l, r)->_remove_ghost_boundaries(l, r, axis, depth)
A:dask.array.ghost.l->l.rechunk(tuple(lchunks)).rechunk(tuple(lchunks))
A:dask.array.ghost.r->r.rechunk(tuple(rchunks)).rechunk(tuple(rchunks))
A:dask.array.ghost.c->wrap.full(tuple(map(sum, chunks)), value, chunks=tuple(chunks), dtype=x.dtype)
A:dask.array.ghost.lchunks->list(l.chunks)
A:dask.array.ghost.rchunks->list(r.chunks)
A:dask.array.ghost.kind->dict(((i, kind) for i in range(x.ndim)))
A:dask.array.ghost.d->dict(zip(range(ndim), depth)).get(i, 0)
A:dask.array.ghost.this_kind->dict(((i, kind) for i in range(x.ndim))).get(i, 'none')
A:dask.array.ghost.x->x.rechunk(out_chunks).rechunk(out_chunks)
A:dask.array.ghost.depth2->coerce_depth(x.ndim, depth)
A:dask.array.ghost.boundary2->coerce_boundary(x.ndim, boundary)
A:dask.array.ghost.x2->boundaries(x, depth2, boundary2)
A:dask.array.ghost.x3->ghost_internal(x2, depth2)
A:dask.array.ghost.trim->dict(((k, v * 2 if boundary2.get(k, 'none') != 'none' else 0) for (k, v) in depth2.items()))
A:dask.array.ghost.x4->chunk.trim(x3, trim)
A:dask.array.ghost.empty_shape->list(x.shape)
A:dask.array.ghost.empty_chunks->list(x.chunks)
A:dask.array.ghost.empty->wrap.empty(empty_shape, chunks=empty_chunks, dtype=x.dtype)
A:dask.array.ghost.out_chunks->list(x.chunks)
A:dask.array.ghost.ax_chunks->list(out_chunks[k])
A:dask.array.ghost.g->ghost(x, depth=depth2, boundary=boundary2)
A:dask.array.ghost.g2->ghost(x, depth=depth2, boundary=boundary2).map_blocks(func, **kwargs)
A:dask.array.ghost.g3->add_dummy_padding(g2, depth2, boundary2)
A:dask.array.ghost.boundary->dict(zip(range(ndim), boundary))
dask.array.ghost._remove_ghost_boundaries(l,r,axis,depth)
dask.array.ghost.add_dummy_padding(x,depth,boundary)
dask.array.ghost.boundaries(x,depth=None,kind=None)
dask.array.ghost.coerce_boundary(ndim,boundary)
dask.array.ghost.coerce_depth(ndim,depth)
dask.array.ghost.constant(x,axis,depth,value)
dask.array.ghost.expand_key(k,dims)
dask.array.ghost.fractional_slice(task,axes)
dask.array.ghost.ghost(x,depth,boundary)
dask.array.ghost.ghost_internal(x,axes)
dask.array.ghost.map_overlap(x,func,depth,boundary=None,trim=True,**kwargs)
dask.array.ghost.nearest(x,axis,depth)
dask.array.ghost.periodic(x,axis,depth)
dask.array.ghost.reflect(x,axis,depth)
dask.array.ghost.trim_internal(x,axes)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/array/ma.py----------------------------------------
A:dask.array.ma.data->asanyarray(data)
A:dask.array.ma.mask->mask.reshape((1,) * data.ndim).reshape((1,) * data.ndim)
A:dask.array.ma.fill_value->numpy.ma.core._check_fill_value(fill_value, a.dtype)
A:dask.array.ma.out->numpy.ma.concatenate(arrays, axis=axis)
A:dask.array.ma.fill_values->numpy.unique(fill_values)
A:dask.array.ma.axes_a->list(axes_a)
A:dask.array.ma.axes_b->list(axes_b)
A:dask.array.ma.na->len(axes_a)
A:dask.array.ma.nb->len(axes_b)
A:dask.array.ma.at->asanyarray(a).transpose(newaxes_a).reshape(newshape_a)
A:dask.array.ma.bt->b.transpose(newaxes_b).reshape(newshape_b)
A:dask.array.ma.res->asanyarray(a).map_blocks(_set_fill_value, fill_value)
A:dask.array.ma.a->asanyarray(a)
A:dask.array.ma.value->asanyarray(value)
A:dask.array.ma.oinds->max(ainds, vinds, key=len)
A:dask.array.ma.masked_greater->_wrap_masked(np.ma.masked_greater)
A:dask.array.ma.masked_greater_equal->_wrap_masked(np.ma.masked_greater_equal)
A:dask.array.ma.masked_less->_wrap_masked(np.ma.masked_less)
A:dask.array.ma.masked_less_equal->_wrap_masked(np.ma.masked_less_equal)
A:dask.array.ma.masked_not_equal->_wrap_masked(np.ma.masked_not_equal)
A:dask.array.ma.inds->tuple(range(data.ndim))
A:dask.array.ma.x->x.copy().copy()
A:dask.array.ma.cshape->getattr(condition, 'shape', ())
A:dask.array.ma.condition->asanyarray(condition)
A:dask.array.ma.ainds->tuple(range(a.ndim))
A:dask.array.ma.cinds->tuple(range(condition.ndim))
A:dask.array.ma.dtype->kwargs.pop('masked_dtype', None)
dask.array.ma._concatenate(arrays,axis=0)
dask.array.ma._masked_array(data,mask=np.ma.nomask,**kwargs)
dask.array.ma._set_fill_value(x,fill_value)
dask.array.ma._tensordot(a,b,axes=2)
dask.array.ma._wrap_masked(f)
dask.array.ma.filled(a,fill_value=None)
dask.array.ma.fix_invalid(a,fill_value=None)
dask.array.ma.getdata(a)
dask.array.ma.getmaskarray(a)
dask.array.ma.masked_array(data,mask=np.ma.nomask,fill_value=None,**kwargs)
dask.array.ma.masked_equal(a,value)
dask.array.ma.masked_inside(x,v1,v2)
dask.array.ma.masked_invalid(a)
dask.array.ma.masked_outside(x,v1,v2)
dask.array.ma.masked_values(x,value,rtol=1e-05,atol=1e-08,shrink=True)
dask.array.ma.masked_where(condition,a)
dask.array.ma.normalize_masked_array(x)
dask.array.ma.set_fill_value(a,fill_value)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/array/slicing.py----------------------------------------
A:dask.array.slicing.colon->slice(None, None, None)
A:dask.array.slicing.ind2->int(ind)
A:dask.array.slicing.index_array->numpy.asanyarray(ind)
A:dask.array.slicing.nonzero->numpy.nonzero(index_array)
A:dask.array.slicing.int_index->numpy.asanyarray(ind).astype(np.intp)
A:dask.array.slicing.check_int->numpy.isclose(index_array, int_index)
A:dask.array.slicing.blockdims->tuple(map(tuple, blockdims))
A:dask.array.slicing.suffixes->product(*[range(len(bd)) for bd in blockdims])
A:dask.array.slicing.dsk->dict((((out_name,) + s, (in_name,) + s) for s in suffixes))
A:dask.array.slicing.not_none_count->sum((i is not None for i in index))
A:dask.array.slicing.(dsk_out, bd_out)->slice_with_newaxes(out_name, in_name, blockdims, index)
A:dask.array.slicing.bd_out->tuple(map(tuple, bd_out))
A:dask.array.slicing.index2->tuple([ind for ind in index if ind is not None])
A:dask.array.slicing.where_none_orig->list(where_none)
A:dask.array.slicing.n->len(blockdims)
A:dask.array.slicing.(dsk, blockdims2)->slice_slices_and_integers(tmp, in_name, blockdims, index_without_list)
A:dask.array.slicing.expand->expander(where_none)
A:dask.array.slicing.expand_orig->expander(where_none_orig)
A:dask.array.slicing.dsk3->merge(dsk, dsk2)
A:dask.array.slicing.blockdims3->expand(blockdims2, (1,))
A:dask.array.slicing.index->numpy.asanyarray(index)
A:dask.array.slicing.index[where_list.pop()]->slice(0, 0, 1)
A:dask.array.slicing.index_without_list->tuple((slice(None, None, None) if isinstance(i, np.ndarray) else i for i in index))
A:dask.array.slicing.(blockdims2, dsk3)->take(out_name, in_name, blockdims, index[where_list[0]], axis=axis)
A:dask.array.slicing.(blockdims2, dsk2)->take(out_name, tmp, blockdims2, index[axis], axis=axis2)
A:dask.array.slicing.shape->tuple(map(sum, blockdims))
A:dask.array.slicing.block_slices->list(map(_slice_1d, shape, blockdims, index))
A:dask.array.slicing.in_names->list(product([in_name], *[pluck(0, s) for s in sorted_block_slices]))
A:dask.array.slicing.out_names->list(product([out_name], *[range(len(d))[::-1] if i.step and i.step < 0 else range(len(d)) for (d, i) in zip(block_slices, index) if not isinstance(i, Integral)]))
A:dask.array.slicing.all_slices->list(product(*[pluck(1, s) for s in sorted_block_slices]))
A:dask.array.slicing.lens->list(lengths)
A:dask.array.slicing.d->dict()
A:dask.array.slicing.d[i]->slice(rstart - chunk_stop, max(chunk_start - chunk_stop - 1, stop - chunk_stop), step)
A:dask.array.slicing.chunk_boundaries->list(accumulate(add, lengths))
A:dask.array.slicing.d[k]->slice(None, None, None)
A:dask.array.slicing.d[0]->slice(0, 0, 1)
A:dask.array.slicing.seq->numpy.asanyarray(seq)
A:dask.array.slicing.left->''.join(left)
A:dask.array.slicing.right->numpy.cumsum(sizes, out=left[1:])
A:dask.array.slicing.locations->numpy.empty(len(sizes) + 1, dtype=int)
A:dask.array.slicing.locations[1:]->numpy.searchsorted(seq, right)
A:dask.array.slicing.index_lists->partition_by_size(sizes, sorted_idx)
A:dask.array.slicing.indims->list(dims)
A:dask.array.slicing.indims[axis]->list(range(len(where_index)))
A:dask.array.slicing.keys->list(product([outname], *dims))
A:dask.array.slicing.outdims->list(dims)
A:dask.array.slicing.slices->list(product(*slices))
A:dask.array.slicing.inkeys->list(product([inname], *outdims))
A:dask.array.slicing.blockdims2->list(blockdims)
A:dask.array.slicing.blockdims2[axis]->tuple(map(len, index_lists))
A:dask.array.slicing.sorted_idx->numpy.sort(index)
A:dask.array.slicing.rev_index->numpy.searchsorted(sorted_idx, index)
A:dask.array.slicing.ind->numpy.asanyarray(ind)
A:dask.array.slicing.decl->decl.format(**locals()).format(**locals())
A:dask.array.slicing.pairs->sorted(_slice_1d(dim_shape, lengths, index).items(), key=itemgetter(0))
A:dask.array.slicing.start->max(0, start + dim)
A:dask.array.slicing.stop->max(0, stop + dim)
A:dask.array.slicing.idx->posify_index(none_shape, idx)
A:dask.array.slicing.x->numpy.asanyarray(ind)
A:dask.array.slicing.y->elemwise(getitem, x, *index, dtype=x.dtype)
A:dask.array.slicing.arginds->list(concat(arginds))
A:dask.array.slicing.out->atop(getitem_variadic, tuple(range(x.ndim)), x, tuple(range(x.ndim)), *arginds, dtype=x.dtype)
A:dask.array.slicing.out._chunks->tuple(chunks)
dask.array.slicing._expander(where)
dask.array.slicing._sanitize_index_element(ind)
dask.array.slicing._slice_1d(dim_shape,lengths,index)
dask.array.slicing.check_index(ind,dimension)
dask.array.slicing.expander(where)
dask.array.slicing.getitem_variadic(x,*index)
dask.array.slicing.issorted(seq)
dask.array.slicing.new_blockdim(dim_shape,lengths,index)
dask.array.slicing.normalize_index(idx,shape)
dask.array.slicing.normalize_slice(idx,dim)
dask.array.slicing.partition_by_size(sizes,seq)
dask.array.slicing.posify_index(shape,ind)
dask.array.slicing.replace_ellipsis(n,index)
dask.array.slicing.sanitize_index(ind)
dask.array.slicing.slice_array(out_name,in_name,blockdims,index)
dask.array.slicing.slice_slices_and_integers(out_name,in_name,blockdims,index)
dask.array.slicing.slice_with_dask_array(x,index)
dask.array.slicing.slice_with_newaxes(out_name,in_name,blockdims,index)
dask.array.slicing.slice_wrap_lists(out_name,in_name,blockdims,index)
dask.array.slicing.take(outname,inname,blockdims,index,axis=0)
dask.array.slicing.take_sorted(outname,inname,blockdims,index,axis=0)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/array/reductions.py----------------------------------------
A:dask.array.reductions.empty_lookup->Dispatch('empty')
A:dask.array.reductions.divide_lookup->Dispatch('divide')
A:dask.array.reductions.f->Dispatch('divide').dispatch(type(builtins.max(a, b, key=key)))
A:dask.array.reductions.axis->validate_axis(x.ndim, axis)
A:dask.array.reductions.chunk->partial(arg_chunk, func, argfunc)
A:dask.array.reductions.aggregate->partial(aggregate, dtype=dtype)
A:dask.array.reductions.inds->tuple(range(x.ndim))
A:dask.array.reductions.tmp->Array(sharedict.merge(x.dask, (name, dsk)), name, chunks, dtype=x.dtype)
A:dask.array.reductions.tmp._chunks->tuple(((1,) * len(c) if i in axis else c for (i, c) in enumerate(tmp.chunks)))
A:dask.array.reductions.result->Array(sharedict.merge(m.dask, (name, dsk)), name, x.chunks, m.dtype)
A:dask.array.reductions.split_every->dict(((k, v) for (k, v) in enumerate(x.numblocks) if k in axis))
A:dask.array.reductions.n->sum(ns, **keepdim_kw)
A:dask.array.reductions.depth->int(builtins.max(depth, ceil(log(n, split_every[i]))))
A:dask.array.reductions.func->compose(partial(aggregate, axis=axis, keepdims=keepdims), partial(_concatenate2, axes=axis))
A:dask.array.reductions.x->x.flatten().flatten()
A:dask.array.reductions.keys->list(product(*map(range, x.numblocks)))
A:dask.array.reductions.out_chunks->list(getter(out_chunks))
A:dask.array.reductions.decided->dict(((i, j[0]) for (i, j) in enumerate(p) if len(j) == 1))
A:dask.array.reductions.dummy->dict((i for i in enumerate(p) if i[0] not in decided))
A:dask.array.reductions.g->lol_tuples((x.name,), range(x.ndim), decided, dummy)
A:dask.array.reductions.total->totals.sum(**kwargs)
A:dask.array.reductions.empty->Dispatch('empty').dispatch(type(n))
A:dask.array.reductions.nanmean->wraps(chunk.nanmean)(nanmean)
A:dask.array.reductions.M->_moment_helper(Ms, ns, inner_term, order, sum, kwargs)
A:dask.array.reductions.M[..., i - 2]->sum((A - u) ** i, dtype=dtype, **kwargs)
A:dask.array.reductions.mu->divide(totals.sum(**keepdim_kw), n, dtype=dtype)
A:dask.array.reductions.M[..., o - 2]->_moment_helper(Ms, ns, inner_term, o, sum, kwargs)
A:dask.array.reductions.keepdim_kw->kwargs.copy()
A:dask.array.reductions.reduced->a.sum(axis=axis)
A:dask.array.reductions.nanvar->wraps(chunk.nanvar)(nanvar)
A:dask.array.reductions.nanstd->wraps(chunk.nanstd)(nanstd)
A:dask.array.reductions.local_args->argfunc(vals, axis=axis)
A:dask.array.reductions.vals->numpy.ma.filled(vals, fill_value)
A:dask.array.reductions.arg->argfunc(x, axis=arg_axis, keepdims=True)
A:dask.array.reductions.ind->numpy.unravel_index(arg.ravel()[0], x.shape)
A:dask.array.reductions.total_ind->tuple((o + i for (o, i) in zip(offset, ind)))
A:dask.array.reductions.arg[:]->numpy.ravel_multi_index(total_ind, total_shape)
A:dask.array.reductions.fill_value->numpy.ma.maximum_fill_value(vals)
A:dask.array.reductions.(arg, vals)->_arg_combine(data, axis, argfunc, keepdims=False)
A:dask.array.reductions.name->'arg-reduce-chunk-{0}'.format(tokenize(chunk, axis))
A:dask.array.reductions.offsets->list(product(*(accumulate(operator.add, bd[:-1], 0) for bd in x.chunks)))
A:dask.array.reductions.offset_info->pluck(axis[0], offsets)
A:dask.array.reductions.chunks->tuple(((1,) * len(c) if i in axis else c for (i, c) in enumerate(x.chunks)))
A:dask.array.reductions.dsk->dict()
A:dask.array.reductions.combine->partial(arg_combine, func, argfunc)
A:dask.array.reductions.agg->partial(arg_agg, func, argfunc)
A:dask.array.reductions.argmin->make_arg_reduction(chunk.min, chunk.argmin)
A:dask.array.reductions.argmax->make_arg_reduction(chunk.max, chunk.argmax)
A:dask.array.reductions.nanargmin->make_arg_reduction(chunk.nanmin, _nanargmin, True)
A:dask.array.reductions.nanargmax->make_arg_reduction(chunk.nanmax, _nanargmax, True)
A:dask.array.reductions.m->x.flatten().flatten().map_blocks(func, axis=axis, dtype=dtype)
A:dask.array.reductions.full->slice(None, None, None)
A:dask.array.reductions.indices->list(product(*[range(nb) if ii != axis else [i] for (ii, nb) in enumerate(x.numblocks)]))
A:dask.array.reductions.shape->tuple((x.chunks[i][ii] if i != axis else 1 for (i, ii) in enumerate(ind)))
dask.array.all(a,axis=None,keepdims=False,split_every=None,out=None)
dask.array.any(a,axis=None,keepdims=False,split_every=None,out=None)
dask.array.cumprod(x,axis=None,dtype=None,out=None)
dask.array.cumsum(x,axis=None,dtype=None,out=None)
dask.array.max(a,axis=None,keepdims=False,split_every=None,out=None)
dask.array.mean(a,axis=None,dtype=None,keepdims=False,split_every=None,out=None)
dask.array.mean_agg(pair,dtype='f8',**kwargs)
dask.array.mean_chunk(x,sum=chunk.sum,numel=numel,dtype='f8',**kwargs)
dask.array.mean_combine(pair,sum=chunk.sum,numel=numel,dtype='f8',**kwargs)
dask.array.min(a,axis=None,keepdims=False,split_every=None,out=None)
dask.array.moment(a,order,axis=None,dtype=None,keepdims=False,ddof=0,split_every=None,out=None)
dask.array.moment_agg(data,order=2,ddof=0,dtype='f8',sum=np.sum,**kwargs)
dask.array.moment_chunk(A,order=2,sum=chunk.sum,numel=numel,dtype='f8',**kwargs)
dask.array.moment_combine(data,order=2,ddof=0,dtype='f8',sum=np.sum,**kwargs)
dask.array.nanmax(a,axis=None,keepdims=False,split_every=None,out=None)
dask.array.nanmean(a,axis=None,dtype=None,keepdims=False,split_every=None,out=None)
dask.array.nanmin(a,axis=None,keepdims=False,split_every=None,out=None)
dask.array.nanstd(a,axis=None,dtype=None,keepdims=False,ddof=0,split_every=None,out=None)
dask.array.nansum(a,axis=None,dtype=None,keepdims=False,split_every=None,out=None)
dask.array.nanvar(a,axis=None,dtype=None,keepdims=False,ddof=0,split_every=None,out=None)
dask.array.prod(a,axis=None,dtype=None,keepdims=False,split_every=None,out=None)
dask.array.reductions._arg_combine(data,axis,argfunc,keepdims=False)
dask.array.reductions._cumprod_merge(a,b)
dask.array.reductions._cumsum_merge(a,b)
dask.array.reductions._moment_helper(Ms,ns,inner_term,order,sum,kwargs)
dask.array.reductions._nanargmax(x,axis,**kwargs)
dask.array.reductions._nanargmin(x,axis,**kwargs)
dask.array.reductions._tree_reduce(x,aggregate,axis,keepdims,dtype,split_every=None,combine=None,name=None)
dask.array.reductions.all(a,axis=None,keepdims=False,split_every=None,out=None)
dask.array.reductions.any(a,axis=None,keepdims=False,split_every=None,out=None)
dask.array.reductions.arg_agg(func,argfunc,data,axis=None,**kwargs)
dask.array.reductions.arg_chunk(func,argfunc,x,axis,offset_info)
dask.array.reductions.arg_combine(func,argfunc,data,axis=None,**kwargs)
dask.array.reductions.arg_reduction(x,chunk,combine,agg,axis=None,split_every=None,out=None)
dask.array.reductions.cumprod(x,axis=None,dtype=None,out=None)
dask.array.reductions.cumreduction(func,binop,ident,x,axis=None,dtype=None,out=None)
dask.array.reductions.cumsum(x,axis=None,dtype=None,out=None)
dask.array.reductions.divide(a,b,dtype=None)
dask.array.reductions.make_arg_reduction(func,argfunc,is_nan_func=False)
dask.array.reductions.max(a,axis=None,keepdims=False,split_every=None,out=None)
dask.array.reductions.mean(a,axis=None,dtype=None,keepdims=False,split_every=None,out=None)
dask.array.reductions.mean_agg(pair,dtype='f8',**kwargs)
dask.array.reductions.mean_chunk(x,sum=chunk.sum,numel=numel,dtype='f8',**kwargs)
dask.array.reductions.mean_combine(pair,sum=chunk.sum,numel=numel,dtype='f8',**kwargs)
dask.array.reductions.min(a,axis=None,keepdims=False,split_every=None,out=None)
dask.array.reductions.moment(a,order,axis=None,dtype=None,keepdims=False,ddof=0,split_every=None,out=None)
dask.array.reductions.moment_agg(data,order=2,ddof=0,dtype='f8',sum=np.sum,**kwargs)
dask.array.reductions.moment_chunk(A,order=2,sum=chunk.sum,numel=numel,dtype='f8',**kwargs)
dask.array.reductions.moment_combine(data,order=2,ddof=0,dtype='f8',sum=np.sum,**kwargs)
dask.array.reductions.nanarg_agg(func,argfunc,data,axis=None,**kwargs)
dask.array.reductions.nanmax(a,axis=None,keepdims=False,split_every=None,out=None)
dask.array.reductions.nanmean(a,axis=None,dtype=None,keepdims=False,split_every=None,out=None)
dask.array.reductions.nanmin(a,axis=None,keepdims=False,split_every=None,out=None)
dask.array.reductions.nannumel(x,**kwargs)
dask.array.reductions.nanstd(a,axis=None,dtype=None,keepdims=False,ddof=0,split_every=None,out=None)
dask.array.reductions.nansum(a,axis=None,dtype=None,keepdims=False,split_every=None,out=None)
dask.array.reductions.nanvar(a,axis=None,dtype=None,keepdims=False,ddof=0,split_every=None,out=None)
dask.array.reductions.numel(x,**kwargs)
dask.array.reductions.partial_reduce(func,x,split_every,keepdims=False,dtype=None,name=None)
dask.array.reductions.prod(a,axis=None,dtype=None,keepdims=False,split_every=None,out=None)
dask.array.reductions.reduction(x,chunk,aggregate,axis=None,keepdims=None,dtype=None,split_every=None,combine=None,name=None,out=None)
dask.array.reductions.std(a,axis=None,dtype=None,keepdims=False,ddof=0,split_every=None,out=None)
dask.array.reductions.sum(a,axis=None,dtype=None,keepdims=False,split_every=None,out=None)
dask.array.reductions.validate_axis(ndim,axis)
dask.array.reductions.var(a,axis=None,dtype=None,keepdims=False,ddof=0,split_every=None,out=None)
dask.array.reductions.vnorm(a,ord=None,axis=None,dtype=None,keepdims=False,split_every=None,out=None)
dask.array.std(a,axis=None,dtype=None,keepdims=False,ddof=0,split_every=None,out=None)
dask.array.sum(a,axis=None,dtype=None,keepdims=False,split_every=None,out=None)
dask.array.var(a,axis=None,dtype=None,keepdims=False,ddof=0,split_every=None,out=None)
dask.array.vnorm(a,ord=None,axis=None,dtype=None,keepdims=False,split_every=None,out=None)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/array/optimization.py----------------------------------------
A:dask.array.optimization.dsk->dsk.copy().copy()
A:dask.array.optimization.keys->list(flatten(keys))
A:dask.array.optimization.(dsk2, dependencies)->cull(dsk, keys)
A:dask.array.optimization.hold->hold_keys(dsk2, dependencies)
A:dask.array.optimization.(dsk3, dependencies)->fuse(dsk2, hold + keys + (fuse_keys or []), dependencies, rename_keys=rename_fused_keys)
A:dask.array.optimization.dsk4->inline_functions(dsk3, keys, dependencies=dependencies, fast_functions=inline_functions_fast_functions)
A:dask.array.optimization.dsk5->optimize_slices(dsk4)
A:dask.array.optimization.dependents->reverse_dict(dependencies)
A:dask.array.optimization.hold_keys->list(data)
A:dask.array.optimization.new_dep->next(iter(dependents[dep]))
A:dask.array.optimization.c_index->fuse_slice(b_index, a_index)
A:dask.array.optimization.a->normalize_slice(a)
A:dask.array.optimization.b->normalize_slice(b)
A:dask.array.optimization.stop->min(a.stop, stop)
A:dask.array.optimization.a_has_lists->any((isinstance(item, list) for item in a))
A:dask.array.optimization.b_has_lists->any((isinstance(item, list) for item in b))
A:dask.array.optimization.result->list()
dask.array.optimization.check_for_nonfusible_fancy_indexing(fancy,normal)
dask.array.optimization.fuse_slice(a,b)
dask.array.optimization.hold_keys(dsk,dependencies)
dask.array.optimization.normalize_slice(s)
dask.array.optimization.optimize(dsk,keys,fuse_keys=None,fast_functions=None,inline_functions_fast_functions=(getter_inline,),rename_fused_keys=True,**kwargs)
dask.array.optimization.optimize_slices(dsk)
dask.array.optimize(dsk,keys,fuse_keys=None,fast_functions=None,inline_functions_fast_functions=(getter_inline,),rename_fused_keys=True,**kwargs)
dask.array.optimize_slices(dsk)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/array/creation.py----------------------------------------
A:dask.array.creation.a->asarray(a)
A:dask.array.creation.num->max(np.ceil((stop - start) / step), 0)
A:dask.array.creation.chunks->tuple(chunks)
A:dask.array.creation.dtype->numpy.dtype(dtype)
A:dask.array.creation.dimensions->tuple(dimensions)
A:dask.array.creation.s[i]->slice(None)
A:dask.array.creation.s->tuple(s)
A:dask.array.creation.r->r.repeat(dimensions[j], axis=j).repeat(dimensions[j], axis=j)
A:dask.array.creation.grid->empty((len(dimensions),) + dimensions, dtype=dtype, chunks=(1,) + chunks)
A:dask.array.creation.token->tokenize(m, k)
A:dask.array.creation.dsk->dict(zip(keys, values))
A:dask.array.creation.blocks->v._keys()
A:dask.array.creation.rdim->len(m.chunks[0])
A:dask.array.creation.hdim->len(m.chunks[1])
A:dask.array.creation.args2->list(map(add, args, offset))
A:dask.array.creation.keys->list(product([name], *[range(len(bd)) for bd in chunks]))
A:dask.array.creation.offsets->list(product(*aggdims))
A:dask.array.creation.shapes->list(product(*chunks))
A:dask.array.creation.cchunks->numpy.cumsum((0,) + a.chunks[axis])
A:dask.array.creation.ls->numpy.linspace(c_start, c_stop, repeats).round(0)
A:dask.array.creation.all_slice->slice(None, None, None)
A:dask.array.creation.result->slab.map_blocks(np.repeat, repeats, axis=axis, chunks=chunks, dtype=slab.dtype)
dask.array.arange(*args,**kwargs)
dask.array.creation.arange(*args,**kwargs)
dask.array.creation.diag(v)
dask.array.creation.empty_like(a,dtype=None,chunks=None)
dask.array.creation.eye(N,chunks,M=None,k=0,dtype=float)
dask.array.creation.fromfunction(func,chunks=None,shape=None,dtype=None)
dask.array.creation.full_like(a,fill_value,dtype=None,chunks=None)
dask.array.creation.indices(dimensions,dtype=int,chunks=None)
dask.array.creation.linspace(start,stop,num=50,chunks=None,dtype=None)
dask.array.creation.offset_func(func,offset,*args)
dask.array.creation.ones_like(a,dtype=None,chunks=None)
dask.array.creation.repeat(a,repeats,axis=None)
dask.array.creation.tile(A,reps)
dask.array.creation.tril(m,k=0)
dask.array.creation.triu(m,k=0)
dask.array.creation.zeros_like(a,dtype=None,chunks=None)
dask.array.diag(v)
dask.array.empty_like(a,dtype=None,chunks=None)
dask.array.eye(N,chunks,M=None,k=0,dtype=float)
dask.array.fromfunction(func,chunks=None,shape=None,dtype=None)
dask.array.full_like(a,fill_value,dtype=None,chunks=None)
dask.array.indices(dimensions,dtype=int,chunks=None)
dask.array.linspace(start,stop,num=50,chunks=None,dtype=None)
dask.array.ones_like(a,dtype=None,chunks=None)
dask.array.repeat(a,repeats,axis=None)
dask.array.tile(A,reps)
dask.array.tril(m,k=0)
dask.array.triu(m,k=0)
dask.array.zeros_like(a,dtype=None,chunks=None)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/array/wrap.py----------------------------------------
A:dask.array.wrap.shape->kwargs.pop('shape')
A:dask.array.wrap.chunks->normalize_chunks(chunks, shape)
A:dask.array.wrap.name->kwargs.pop('name', None)
A:dask.array.wrap.dtype->kwargs.pop('dtype', None)
A:dask.array.wrap.keys->product([name], *[range(len(bd)) for bd in chunks])
A:dask.array.wrap.shapes->product(*chunks)
A:dask.array.wrap.func->partial(func, dtype=dtype, **kwargs)
A:dask.array.wrap.dsk->dict(zip(keys, vals))
A:dask.array.wrap.f->partial(wrap_func, func, **kwargs)
A:dask.array.wrap.w->wrap(wrap_func_shape_as_first_arg)
A:dask.array.wrap.ones->w(np.ones, dtype='f8')
A:dask.array.wrap.zeros->w(np.zeros, dtype='f8')
A:dask.array.wrap.empty->w(np.empty, dtype='f8')
A:dask.array.wrap.full->w(full)
dask.array.wrap.wrap(wrap_func,func,**kwargs)
dask.array.wrap.wrap_func_shape_as_first_arg(func,*args,**kwargs)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/array/numpy_compat.py----------------------------------------
A:dask.array.numpy_compat.x->x.astype(dtype).astype(dtype)
A:dask.array.numpy_compat.ma_divide->numpy.ma.core._DomainedBinaryOperation(divide, np.ma.core._DomainSafeDivide(), 0, 1)
A:dask.array.numpy_compat.new_array->new_array.view(type=type(original_array)).view(type=type(original_array))
A:dask.array.numpy_compat.array->numpy.array(array, copy=False, subok=subok)
A:dask.array.numpy_compat.result->_maybe_view_as_subclass(array, broadcast)
A:dask.array.numpy_compat.a->numpy.array(a, subok=True)
A:dask.array.numpy_compat.mask->numpy.isnan(a)
A:dask.array.numpy_compat.(a, mask)->_replace_nan(a, 1)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/array/random.py----------------------------------------
A:dask.array.random.func2.__doc__->getattr(state, func).__doc__.replace('>>>', '>>').replace('...', '..')
A:dask.array.random.self._numpy_state->numpy.random.RandomState(seed)
A:dask.array.random.size->broadcast_shapes(*shapes)
A:dask.array.random.chunks->normalize_chunks(chunks, size)
A:dask.array.random.extra_chunks->kwargs.pop('extra_chunks', ())
A:dask.array.random.shapes->list(args_shapes)
A:dask.array.random.slices->slices_from_chunks(chunks)
A:dask.array.random.res->_broadcast_any(ar, size, chunks)
A:dask.array.random.name->'da.random.{0}-{1}'.format(func.__name__, token)
A:dask.array.random.sizes->list(product(*chunks))
A:dask.array.random.state_data->random_state_data(len(sizes), self._numpy_state)
A:dask.array.random.token->tokenize(state_data, size, chunks, args, kwargs)
A:dask.array.random.keys->product([name], *[range(len(bd)) for bd in chunks] + [[0]] * len(extra_chunks))
A:dask.array.random.blocks->product(*[range(len(bd)) for bd in chunks])
A:dask.array.random.dsk->sharedict.merge((name, dsk), *dsks)
A:dask.array.random.state->numpy.random.RandomState(state_data)
A:dask.array.random.func->getattr(state, func)
A:dask.array.random._state->RandomState()
dask.array.random.RandomState(self,seed=None)
dask.array.random.RandomState.__init__(self,seed=None)
dask.array.random.RandomState._wrap(self,func,*args,**kwargs)
dask.array.random.RandomState.beta(self,a,b,size=None,chunks=None)
dask.array.random.RandomState.binomial(self,n,p,size=None,chunks=None)
dask.array.random.RandomState.chisquare(self,df,size=None,chunks=None)
dask.array.random.RandomState.exponential(self,scale=1.0,size=None,chunks=None)
dask.array.random.RandomState.f(self,dfnum,dfden,size=None,chunks=None)
dask.array.random.RandomState.gamma(self,shape,scale=1.0,size=None,chunks=None)
dask.array.random.RandomState.geometric(self,p,size=None,chunks=None)
dask.array.random.RandomState.gumbel(self,loc=0.0,scale=1.0,size=None,chunks=None)
dask.array.random.RandomState.hypergeometric(self,ngood,nbad,nsample,size=None,chunks=None)
dask.array.random.RandomState.laplace(self,loc=0.0,scale=1.0,size=None,chunks=None)
dask.array.random.RandomState.logistic(self,loc=0.0,scale=1.0,size=None,chunks=None)
dask.array.random.RandomState.lognormal(self,mean=0.0,sigma=1.0,size=None,chunks=None)
dask.array.random.RandomState.logseries(self,p,size=None,chunks=None)
dask.array.random.RandomState.multinomial(self,n,pvals,size=None,chunks=None)
dask.array.random.RandomState.negative_binomial(self,n,p,size=None,chunks=None)
dask.array.random.RandomState.noncentral_chisquare(self,df,nonc,size=None,chunks=None)
dask.array.random.RandomState.noncentral_f(self,dfnum,dfden,nonc,size=None,chunks=None)
dask.array.random.RandomState.normal(self,loc=0.0,scale=1.0,size=None,chunks=None)
dask.array.random.RandomState.pareto(self,a,size=None,chunks=None)
dask.array.random.RandomState.poisson(self,lam=1.0,size=None,chunks=None)
dask.array.random.RandomState.power(self,a,size=None,chunks=None)
dask.array.random.RandomState.randint(self,low,high=None,size=None,chunks=None)
dask.array.random.RandomState.random_integers(self,low,high=None,size=None,chunks=None)
dask.array.random.RandomState.random_sample(self,size=None,chunks=None)
dask.array.random.RandomState.rayleigh(self,scale=1.0,size=None,chunks=None)
dask.array.random.RandomState.seed(self,seed=None)
dask.array.random.RandomState.standard_cauchy(self,size=None,chunks=None)
dask.array.random.RandomState.standard_exponential(self,size=None,chunks=None)
dask.array.random.RandomState.standard_gamma(self,shape,size=None,chunks=None)
dask.array.random.RandomState.standard_normal(self,size=None,chunks=None)
dask.array.random.RandomState.standard_t(self,df,size=None,chunks=None)
dask.array.random.RandomState.tomaxint(self,size=None,chunks=None)
dask.array.random.RandomState.triangular(self,left,mode,right,size=None,chunks=None)
dask.array.random.RandomState.uniform(self,low=0.0,high=1.0,size=None,chunks=None)
dask.array.random.RandomState.vonmises(self,mu,kappa,size=None,chunks=None)
dask.array.random.RandomState.wald(self,mean,scale,size=None,chunks=None)
dask.array.random.RandomState.weibull(self,a,size=None,chunks=None)
dask.array.random.RandomState.zipf(self,a,size=None,chunks=None)
dask.array.random._apply_random(func,state_data,size,args,kwargs)
dask.array.random.doc_wraps(func)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/array/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/array/conftest.py----------------------------------------
dask.array.conftest.pytest_ignore_collect(path,config)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/array/rechunk.py----------------------------------------
A:dask.array.rechunk.cmo->cumdims_label(old_known, 'o')
A:dask.array.rechunk.cmn->cumdims_label(new_known, 'n')
A:dask.array.rechunk.old_to_new->_old_to_new(old_chunks, new_chunks)
A:dask.array.rechunk.cross1->product(*old_to_new)
A:dask.array.rechunk.cross->chain((tuple(product(*cr)) for cr in cross1))
A:dask.array.rechunk.newlist->list(old)
A:dask.array.rechunk.shape->tuple(map(sum, old_chunks))
A:dask.array.rechunk.new_chunks->list(old_chunks)
A:dask.array.rechunk.chunks->find_split_rechunk(current_chunks, new_chunks, graph_size * threshold)
A:dask.array.rechunk.new_shapes->tuple(map(sum, chunks))
A:dask.array.rechunk.steps->plan_rechunk(x.chunks, chunks, x.dtype.itemsize, threshold, block_size_limit)
A:dask.array.rechunk.x->_compute_rechunk(x, c)
A:dask.array.rechunk.crossed_size->reduce(mul, (len(oc) + len(nc) for (oc, nc) in zip(old_chunks, new_chunks)))
A:dask.array.rechunk.nb_divides->int(np.ceil(c / max_width))
A:dask.array.rechunk.distinct->set(desired_chunks)
A:dask.array.rechunk.w->set(desired_chunks).pop()
A:dask.array.rechunk.n->len(desired_chunks)
A:dask.array.rechunk.(width, i, j)->heapq.heappop(heap)
A:dask.array.rechunk.ndim->len(new_chunks)
A:dask.array.rechunk.sorted_candidates->sorted(merge_candidates, key=key)
A:dask.array.rechunk.largest_block_size->reduce(mul, old_largest_width)
A:dask.array.rechunk.chunk_limit->int(block_size_limit * largest_width / largest_block_size)
A:dask.array.rechunk.c->merge_to_number(new_chunks[dim], max_number)
A:dask.array.rechunk.graph_size->estimate_graph_size(current_chunks, new_chunks)
A:dask.array.rechunk.max_number->int(len(old_chunks[dim]) * graph_size_limit / graph_size)
A:dask.array.rechunk.largest_old_block->_largest_block_size(old_chunks)
A:dask.array.rechunk.largest_new_block->_largest_block_size(new_chunks)
A:dask.array.rechunk.block_size_limit->max([block_size_limit, largest_old_block, largest_new_block])
A:dask.array.rechunk.(chunks, memory_limit_hit)->find_merge_rechunk(chunks, new_chunks, block_size_limit)
A:dask.array.rechunk.crossed->intersect_chunks(x.chunks, chunks)
A:dask.array.rechunk.x2->sharedict.merge(x.dask, (merge_temp_name, toolz.merge(x2, intermediates)))
A:dask.array.rechunk.intermediates->dict()
A:dask.array.rechunk.token->tokenize(x, chunks)
A:dask.array.rechunk.split_name_suffixes->count()
A:dask.array.rechunk.old_blocks->numpy.empty([len(c) for c in x.chunks], dtype='O')
A:dask.array.rechunk.new_index->product(*(range(len(c)) for c in chunks))
A:dask.array.rechunk.rec_cat_arg->numpy.empty(subdims1, dtype='O')
A:dask.array.rechunk.(old_block_index, slices)->zip(*ind_slices)
dask.array.rechunk(x,chunks,threshold=DEFAULT_THRESHOLD,block_size_limit=DEFAULT_BLOCK_SIZE_LIMIT)
dask.array.rechunk._PrettyBlocks(self,blocks)
dask.array.rechunk._PrettyBlocks.__init__(self,blocks)
dask.array.rechunk._PrettyBlocks.__str__(self)
dask.array.rechunk._breakpoints(cumold,cumnew)
dask.array.rechunk._compute_rechunk(x,chunks)
dask.array.rechunk._intersect_1d(breaks)
dask.array.rechunk._largest_block_size(chunks)
dask.array.rechunk._number_of_blocks(chunks)
dask.array.rechunk._old_to_new(old_chunks,new_chunks)
dask.array.rechunk.blockdims_dict_to_tuple(old,new)
dask.array.rechunk.blockshape_dict_to_tuple(old_chunks,d)
dask.array.rechunk.cumdims_label(chunks,const)
dask.array.rechunk.divide_to_width(desired_chunks,max_width)
dask.array.rechunk.estimate_graph_size(old_chunks,new_chunks)
dask.array.rechunk.find_merge_rechunk(old_chunks,new_chunks,block_size_limit)
dask.array.rechunk.find_split_rechunk(old_chunks,new_chunks,graph_size_limit)
dask.array.rechunk.format_blocks(blocks)
dask.array.rechunk.format_chunks(chunks)
dask.array.rechunk.format_plan(plan)
dask.array.rechunk.intersect_chunks(old_chunks,new_chunks)
dask.array.rechunk.merge_to_number(desired_chunks,max_number)
dask.array.rechunk.plan_rechunk(old_chunks,new_chunks,itemsize,threshold=DEFAULT_THRESHOLD,block_size_limit=DEFAULT_BLOCK_SIZE_LIMIT)
dask.array.rechunk.rechunk(x,chunks,threshold=DEFAULT_THRESHOLD,block_size_limit=DEFAULT_BLOCK_SIZE_LIMIT)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/array/percentile.py----------------------------------------
A:dask.array.percentile.q->numpy.array(q)
A:dask.array.percentile.result->merge_percentiles(finalq, qs, [v.codes for v in vals], Ns, interpolation)
A:dask.array.percentile.a2->a.astype('i8')
A:dask.array.percentile.token->tokenize(a, list(q), interpolation)
A:dask.array.percentile.dsk->sharedict.merge(a.dask, (name2, dsk))
A:dask.array.percentile.finalq->numpy.array(finalq)
A:dask.array.percentile.qs->list(map(list, qs))
A:dask.array.percentile.vals->list(vals)
A:dask.array.percentile.Ns->list(Ns)
A:dask.array.percentile.L->list(zip(*[(q, val, N) for (q, val, N) in zip(qs, vals, Ns) if N]))
A:dask.array.percentile.count->numpy.empty(len(q))
A:dask.array.percentile.count[1:]->numpy.diff(q)
A:dask.array.percentile.combined_vals_counts->merge_sorted(*map(zip, vals, counts))
A:dask.array.percentile.(combined_vals, combined_counts)->zip(*combined_vals_counts)
A:dask.array.percentile.combined_vals->numpy.array(combined_vals)
A:dask.array.percentile.combined_counts->numpy.array(combined_counts)
A:dask.array.percentile.combined_q->numpy.cumsum(combined_counts)
A:dask.array.percentile.rv->numpy.interp(desired_q, combined_q, combined_vals)
A:dask.array.percentile.left->numpy.searchsorted(combined_q, desired_q, side='left')
A:dask.array.percentile.lower->numpy.minimum(left, right)
A:dask.array.percentile.upper->numpy.maximum(left, right)
A:dask.array.percentile.lower_residual->numpy.abs(combined_q[lower] - desired_q)
A:dask.array.percentile.upper_residual->numpy.abs(combined_q[upper] - desired_q)
dask.array.percentile(a,q,interpolation='linear')
dask.array.percentile._percentile(a,q,interpolation='linear')
dask.array.percentile.merge_percentiles(finalq,qs,vals,Ns,interpolation='lower')
dask.array.percentile.percentile(a,q,interpolation='linear')


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/array/image.py----------------------------------------
A:dask.array.image.filenames->sorted(glob(filename))
A:dask.array.image.sample->preprocess(sample)
A:dask.array.image.dsk->dict(zip(keys, values))
dask.array.image.add_leading_dimension(x)
dask.array.image.imread(filename,imread=None,preprocess=None)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/array/reshape.py----------------------------------------
A:dask.array.reshape.chunk_reduction->reduce(mul, map(len, inchunks[ileft + 1:ii + 1]))
A:dask.array.reshape.result_inchunks[ileft]->expand_tuple(inchunks[ileft], chunk_reduction)
A:dask.array.reshape.prod->reduce(mul, inshape[ileft + 1:ii + 1])
A:dask.array.reshape.result_outchunks[oi]->tuple((prod * c for c in result_inchunks[ileft]))
A:dask.array.reshape.cs->reduce(mul, outshape[oleft + 1:oi + 1])
A:dask.array.reshape.result_inchunks[ii]->contract_tuple(inchunks[ii], cs)
A:dask.array.reshape.result_outchunks[oleft]->tuple((c // cs for c in result_inchunks[ii]))
A:dask.array.reshape.part->max(x / factor, 1)
A:dask.array.reshape.shape->tuple((missing_size if s == -1 else s for s in shape))
A:dask.array.reshape.missing_size->sanitize_index(x.size / reduce(mul, known_sizes, 1))
A:dask.array.reshape.key->next(flatten(x._keys()))
A:dask.array.reshape.chunks->tuple(((d,) for d in shape))
A:dask.array.reshape.(inchunks, outchunks)->reshape_rechunk(x.shape, shape, x.chunks)
A:dask.array.reshape.x2->x.rechunk(inchunks)
A:dask.array.reshape.in_keys->list(product([x2.name], *[range(len(c)) for c in inchunks]))
A:dask.array.reshape.out_keys->list(product([name], *[range(len(c)) for c in outchunks]))
A:dask.array.reshape.shapes->list(product(*outchunks))
dask.array.reshape(x,shape)
dask.array.reshape.contract_tuple(chunks,factor)
dask.array.reshape.expand_tuple(chunks,factor)
dask.array.reshape.reshape(x,shape)
dask.array.reshape.reshape_rechunk(inshape,outshape,inchunks)
dask.array.reshape_rechunk(inshape,outshape,inchunks)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/array/fft.py----------------------------------------
A:dask.array.fft.chunks->out_chunk_fn(a, s, axes)
A:dask.array.fft.s->list(s)
A:dask.array.fft.axes->list(range(x.ndim))
A:dask.array.fft.func_mod->inspect.getmodule(fft_func)
A:dask.array.fft.fft->fft_wrap(np.fft.fft, dtype=np.complex_)
A:dask.array.fft.fft2->fft_wrap(np.fft.fft2, dtype=np.complex_)
A:dask.array.fft.fftn->fft_wrap(np.fft.fftn, dtype=np.complex_)
A:dask.array.fft.ifft->fft_wrap(np.fft.ifft, dtype=np.complex_)
A:dask.array.fft.ifft2->fft_wrap(np.fft.ifft2, dtype=np.complex_)
A:dask.array.fft.ifftn->fft_wrap(np.fft.ifftn, dtype=np.complex_)
A:dask.array.fft.rfft->fft_wrap(np.fft.rfft, dtype=np.complex_)
A:dask.array.fft.rfft2->fft_wrap(np.fft.rfft2, dtype=np.complex_)
A:dask.array.fft.rfftn->fft_wrap(np.fft.rfftn, dtype=np.complex_)
A:dask.array.fft.irfft->fft_wrap(np.fft.irfft, dtype=np.float_)
A:dask.array.fft.irfft2->fft_wrap(np.fft.irfft2, dtype=np.float_)
A:dask.array.fft.irfftn->fft_wrap(np.fft.irfftn, dtype=np.float_)
A:dask.array.fft.hfft->fft_wrap(np.fft.hfft, dtype=np.float_)
A:dask.array.fft.ihfft->fft_wrap(np.fft.ihfft, dtype=np.complex_)
A:dask.array.fft.r->tuple(r)
A:dask.array.fft.n->int(n)
A:dask.array.fft.d->float(d)
A:dask.array.fft.l[i]->slice(None, n_2)
A:dask.array.fft.l->tuple(l)
A:dask.array.fft.r[i]->slice(n_2, None)
A:dask.array.fft.y->_concatenate([y[r], y[l]], axis=i)
dask.array.fft._fft_out_chunks(a,s,axes)
dask.array.fft._fftfreq_block(i,n,d)
dask.array.fft._fftshift_helper(x,axes=None,inverse=False)
dask.array.fft._hfft_out_chunks(a,s,axes)
dask.array.fft._ihfft_out_chunks(a,s,axes)
dask.array.fft._irfft_out_chunks(a,s,axes)
dask.array.fft._rfft_out_chunks(a,s,axes)
dask.array.fft.fft_wrap(fft_func,kind=None,dtype=None)
dask.array.fft.fftfreq(n,d=1.0,chunks=None)
dask.array.fft.fftshift(x,axes=None)
dask.array.fft.ifftshift(x,axes=None)
dask.array.fft.rfftfreq(n,d=1.0,chunks=None)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/array/tests/test_sparse.py----------------------------------------
A:dask.array.tests.test_sparse.sparse->pytest.importorskip('sparse')
A:dask.array.tests.test_sparse.x->dask.array.zeros((10, 1), chunks=(5, 1))
A:dask.array.tests.test_sparse.y->y.map_blocks(sparse.COO.from_numpy).map_blocks(sparse.COO.from_numpy)
A:dask.array.tests.test_sparse.xx->dask.array.zeros((10, 1), chunks=(5, 1)).map_blocks(sparse.COO.from_numpy)
A:dask.array.tests.test_sparse.yy->y.map_blocks(sparse.COO.from_numpy).map_blocks(sparse.COO.from_numpy).map_blocks(sparse.COO.from_numpy)
A:dask.array.tests.test_sparse.zz->dask.array.concatenate([x, y], axis=1).compute()
A:dask.array.tests.test_sparse.d->dask.array.random.random((4, 3, 4), chunks=(1, 2, 2))
A:dask.array.tests.test_sparse.s->dask.array.random.random((4, 3, 4), chunks=(1, 2, 2)).map_blocks(fn)
A:dask.array.tests.test_sparse.dd->func(d)
A:dask.array.tests.test_sparse.ss->func(s)
A:dask.array.tests.test_sparse.z->dask.array.concatenate([x, y], axis=1)
dask.array.tests.test_sparse.test_basic(func)
dask.array.tests.test_sparse.test_mixed_concatenate(func)
dask.array.tests.test_sparse.test_mixed_output_type()
dask.array.tests.test_sparse.test_mixed_random(func)
dask.array.tests.test_sparse.test_tensordot()


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/array/tests/test_creation.py----------------------------------------
A:dask.array.tests.test_creation.np_func->getattr(np, funcname)
A:dask.array.tests.test_creation.da_func->getattr(da, funcname)
A:dask.array.tests.test_creation.dtype->numpy.dtype(dtype)
A:dask.array.tests.test_creation.a->numpy.random.randint(0, 10, shape).astype(dtype)
A:dask.array.tests.test_creation.np_r->np_func(a)
A:dask.array.tests.test_creation.da_r->da_func(a, chunks=chunks)
A:dask.array.tests.test_creation.darr->dask.array.diag(v)
A:dask.array.tests.test_creation.nparr->numpy.diag(v)
A:dask.array.tests.test_creation.A->numpy.random.randint(0, 11, (30, 35))
A:dask.array.tests.test_creation.dA->dask.array.from_array(A, chunks=(5, 5))
A:dask.array.tests.test_creation.v->dask.array.arange(11, chunks=11)
A:dask.array.tests.test_creation.x->numpy.random.random(shape)
A:dask.array.tests.test_creation.d->dask.array.from_array(x, chunks=chunks)
dask.array.tests.test_creation.test_arange()
dask.array.tests.test_creation.test_arange_cast_float_int_step()
dask.array.tests.test_creation.test_arange_float_step()
dask.array.tests.test_creation.test_arange_has_dtype()
dask.array.tests.test_creation.test_arr_like(funcname,shape,dtype,chunks)
dask.array.tests.test_creation.test_diag()
dask.array.tests.test_creation.test_empty_indicies()
dask.array.tests.test_creation.test_eye()
dask.array.tests.test_creation.test_fromfunction()
dask.array.tests.test_creation.test_indices_no_chunks()
dask.array.tests.test_creation.test_indices_wrong_chunks()
dask.array.tests.test_creation.test_indicies()
dask.array.tests.test_creation.test_linspace()
dask.array.tests.test_creation.test_repeat()
dask.array.tests.test_creation.test_tile(shape,chunks,reps)
dask.array.tests.test_creation.test_tile_array_reps(shape,chunks,reps)
dask.array.tests.test_creation.test_tile_neg_reps(shape,chunks,reps)
dask.array.tests.test_creation.test_tril_triu()
dask.array.tests.test_creation.test_tril_triu_errors()


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/array/tests/test_ufunc.py----------------------------------------
A:dask.array.tests.test_ufunc.np->pytest.importorskip('numpy')
A:dask.array.tests.test_ufunc.dafunc->getattr(da, ufunc)
A:dask.array.tests.test_ufunc.npfunc->getattr(np, ufunc)
A:dask.array.tests.test_ufunc.arr->pytest.importorskip('numpy').random.randint(1, 100, size=(20, 20))
A:dask.array.tests.test_ufunc.darr->dask.array.from_array(arr, 3)
A:dask.array.tests.test_ufunc.arr1->pytest.importorskip('numpy').random.randint(1, 100, size=20)
A:dask.array.tests.test_ufunc.darr1->dask.array.from_array(arr1, 3)
A:dask.array.tests.test_ufunc.arr2->pytest.importorskip('numpy').random.randint(1, 100, size=(10, 3))
A:dask.array.tests.test_ufunc.darr2->dask.array.from_array(arr2, 3)
A:dask.array.tests.test_ufunc.real->pytest.importorskip('numpy').random.randint(1, 100, size=(20, 20))
A:dask.array.tests.test_ufunc.dareal->dask.array.from_array(real, 3)
A:dask.array.tests.test_ufunc.daimag->dask.array.from_array(imag, 3)
A:dask.array.tests.test_ufunc.dacomp->dask.array.from_array(comp, 3)
A:dask.array.tests.test_ufunc.(res1, res2)->dafunc(arr)
A:dask.array.tests.test_ufunc.(exp1, exp2)->npfunc(arr)
A:dask.array.tests.test_ufunc.x->dask.array.arange(10, chunks=(5,))
A:dask.array.tests.test_ufunc.d->dask.array.from_array(x, chunks=(2, 2))
A:dask.array.tests.test_ufunc.empty->pytest.importorskip('numpy').empty(10, dtype=x.dtype)
dask.array.tests.test_ufunc.test_angle()
dask.array.tests.test_ufunc.test_array_ufunc()
dask.array.tests.test_ufunc.test_array_ufunc_binop()
dask.array.tests.test_ufunc.test_array_ufunc_out()
dask.array.tests.test_ufunc.test_binary_ufunc(ufunc)
dask.array.tests.test_ufunc.test_clip()
dask.array.tests.test_ufunc.test_complex(ufunc)
dask.array.tests.test_ufunc.test_out_numpy()
dask.array.tests.test_ufunc.test_ufunc()
dask.array.tests.test_ufunc.test_ufunc_2results(ufunc)
dask.array.tests.test_ufunc.test_ufunc_meta()
dask.array.tests.test_ufunc.test_ufunc_outer()
dask.array.tests.test_ufunc.test_unary_ufunc(ufunc)
dask.array.tests.test_ufunc.test_unsupported_ufunc_methods()


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/array/tests/test_percentiles.py----------------------------------------
A:dask.array.tests.test_percentiles.d->dask.array.from_array(x, chunks=(3,))
A:dask.array.tests.test_percentiles.x->dask.array.ones(10, chunks=((5, 0, 5),))
A:dask.array.tests.test_percentiles.result->dask.array.percentile(d, [0, 50, 100])
A:dask.array.tests.test_percentiles.x0->pandas.Categorical(['Alice', 'Bob', 'Charlie', 'Dennis', 'Alice', 'Alice'])
A:dask.array.tests.test_percentiles.x1->pandas.Categorical(['Alice', 'Bob', 'Charlie', 'Dennis', 'Alice', 'Alice'])
A:dask.array.tests.test_percentiles.p->dask.array.percentile(x, [50])
dask.array.tests.test_percentiles.test_percentile()
dask.array.tests.test_percentiles.test_percentile_with_categoricals()
dask.array.tests.test_percentiles.test_percentiles_with_empty_arrays()


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/array/tests/test_ghost.py----------------------------------------
A:dask.array.tests.test_ghost.fs->fractional_slice(('x', 4.9), {0: 2})
A:dask.array.tests.test_ghost.x->dask.array.ones((10, 10), chunks=(5, 5))
A:dask.array.tests.test_ghost.d->dask.array.from_array(x, chunks=(2, 2))
A:dask.array.tests.test_ghost.g->ghost(darr, depth=0, boundary=0)
A:dask.array.tests.test_ghost.result->trim_internal(constant, depth)
A:dask.array.tests.test_ghost.expected->numpy.arange(144).reshape(12, 12)
A:dask.array.tests.test_ghost.e->boundaries(d, {0: 2, 1: 1}, {0: 0, 1: 'periodic'})
A:dask.array.tests.test_ghost.y->dask.array.ones((10, 10), chunks=(5, 5)).map_overlap(lambda x: x, depth=1, boundary='none')
A:dask.array.tests.test_ghost.exp1->dask.array.from_array(x, chunks=(2, 2)).map_overlap(lambda x: x + x.size, depth=1, dtype=d.dtype)
A:dask.array.tests.test_ghost.exp2->dask.array.from_array(x, chunks=(2, 2)).map_overlap(lambda x: x + x.size, depth={0: 1, 1: 1}, boundary={0: 'reflect', 1: 'none'}, dtype=d.dtype)
A:dask.array.tests.test_ghost.a->numpy.arange(1 * 9).reshape(1, 9)
A:dask.array.tests.test_ghost.darr->dask.array.from_array(expected, chunks=(5, 5))
A:dask.array.tests.test_ghost.garr->ghost(darr, depth={0: 5, 1: 5}, boundary={0: 'nearest', 1: 'nearest'})
A:dask.array.tests.test_ghost.tarr->trim_internal(garr, {0: 5, 1: 5})
A:dask.array.tests.test_ghost.reflected->ghost(darr, depth=depth, boundary='reflect')
A:dask.array.tests.test_ghost.nearest->ghost(darr, depth=depth, boundary='nearest')
A:dask.array.tests.test_ghost.periodic->ghost(darr, depth=depth, boundary='periodic')
A:dask.array.tests.test_ghost.constant->ghost(darr, depth=depth, boundary=42)
A:dask.array.tests.test_ghost.b->boundaries(darr, {0: 0, 1: 0}, {0: 0, 1: 0})
A:dask.array.tests.test_ghost.exp->boundaries(x, 2, {0: 'none', 1: 33})
A:dask.array.tests.test_ghost.res->numpy.array([[33, 33, 0, 1, 2, 3, 33, 33], [33, 33, 4, 5, 6, 7, 33, 33], [33, 33, 8, 9, 10, 11, 33, 33], [33, 33, 12, 13, 14, 15, 33, 33]])
dask.array.tests.test_ghost.test_0_depth()
dask.array.tests.test_ghost.test_bad_depth_raises()
dask.array.tests.test_ghost.test_boundaries()
dask.array.tests.test_ghost.test_constant()
dask.array.tests.test_ghost.test_constant_boundaries()
dask.array.tests.test_ghost.test_depth_equals_boundary_length()
dask.array.tests.test_ghost.test_depth_greater_than_boundary_length()
dask.array.tests.test_ghost.test_fractional_slice()
dask.array.tests.test_ghost.test_ghost()
dask.array.tests.test_ghost.test_ghost_internal()
dask.array.tests.test_ghost.test_ghost_small()
dask.array.tests.test_ghost.test_map_overlap()
dask.array.tests.test_ghost.test_map_overlap_no_depth(boundary)
dask.array.tests.test_ghost.test_nearest()
dask.array.tests.test_ghost.test_nearest_ghost()
dask.array.tests.test_ghost.test_none_boundaries()
dask.array.tests.test_ghost.test_one_chunk_along_axis()
dask.array.tests.test_ghost.test_periodic()
dask.array.tests.test_ghost.test_reflect()
dask.array.tests.test_ghost.test_some_0_depth()
dask.array.tests.test_ghost.test_trim_internal()


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/array/tests/test_reshape.py----------------------------------------
A:dask.array.tests.test_reshape.(result_in, result_out)->reshape_rechunk(inshape, outshape, prechunks)
dask.array.tests.test_reshape.test_contract_tuple()
dask.array.tests.test_reshape.test_expand_tuple()
dask.array.tests.test_reshape.test_reshape_rechunk(inshape,outshape,prechunks,inchunks,outchunks)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/array/tests/test_rechunk.py----------------------------------------
A:dask.array.tests.test_rechunk.np->pytest.importorskip('numpy')
A:dask.array.tests.test_rechunk.new->cumdims_label(((1, 1, 2), (1, 5, 1)), 'n')
A:dask.array.tests.test_rechunk.old->cumdims_label(((4,), (1,) * 5), 'o')
A:dask.array.tests.test_rechunk.breaks->tuple((_breakpoints(o, n) for (o, n) in zip(old, new)))
A:dask.array.tests.test_rechunk.cross->list(intersect_chunks(old_chunks=old, new_chunks=new))
A:dask.array.tests.test_rechunk.a->pytest.importorskip('numpy').array(42)
A:dask.array.tests.test_rechunk.x->dask.array.ones(shape=(10, 10), chunks=(5, 2))
A:dask.array.tests.test_rechunk.x2->dask.array.ones(shape=(10, 10), chunks=(5, 2)).rechunk(chunks=new)
A:dask.array.tests.test_rechunk.y->dask.array.ones(shape=(10, 10), chunks=(5, 2)).rechunk(chunks)
A:dask.array.tests.test_rechunk.orig->pytest.importorskip('numpy').random.uniform(0, 1, a ** b).reshape((a,) * b)
A:dask.array.tests.test_rechunk.new_blockdims->normalize_chunks(new_chunks, new_shape)
A:dask.array.tests.test_rechunk.check1->rechunk(x, chunks=new_chunks)
A:dask.array.tests.test_rechunk.chunks->merge_to_number((1, 1, 1, 1, 3, 1, 1), 1)
A:dask.array.tests.test_rechunk.steps->plan_rechunk(a, b, itemsize=8)
A:dask.array.tests.test_rechunk.steps2->_plan((f, c), (c, f), block_size_limit=3999, itemsize=10)
A:dask.array.tests.test_rechunk.dsk->dict(y.dask)
A:dask.array.tests.test_rechunk.result->_old_to_new(old, new)
A:dask.array.tests.test_rechunk.dd->pytest.importorskip('dask.dataframe')
A:dask.array.tests.test_rechunk.pd->pytest.importorskip('pandas')
A:dask.array.tests.test_rechunk.arr->pytest.importorskip('numpy').random.randn(50, 10)
A:dask.array.tests.test_rechunk.expected->dask.array.ones(shape=(10, 10), chunks=(5, 2)).rechunk((None, (5, 5)))
A:dask.array.tests.test_rechunk.nan->float('nan')
dask.array.tests.test_rechunk._assert_steps(steps,expected)
dask.array.tests.test_rechunk._plan(old_chunks,new_chunks,itemsize=1,block_size_limit=10000000.0)
dask.array.tests.test_rechunk.assert_chunks_match(left,right)
dask.array.tests.test_rechunk.test_changing_raises()
dask.array.tests.test_rechunk.test_divide_to_width()
dask.array.tests.test_rechunk.test_dont_concatenate_single_chunks(shape,chunks)
dask.array.tests.test_rechunk.test_dtype()
dask.array.tests.test_rechunk.test_intersect_1()
dask.array.tests.test_rechunk.test_intersect_2()
dask.array.tests.test_rechunk.test_intersect_nan()
dask.array.tests.test_rechunk.test_intersect_nan_long()
dask.array.tests.test_rechunk.test_intersect_nan_single()
dask.array.tests.test_rechunk.test_merge_to_number()
dask.array.tests.test_rechunk.test_old_to_new()
dask.array.tests.test_rechunk.test_old_to_new_known()
dask.array.tests.test_rechunk.test_old_to_new_large()
dask.array.tests.test_rechunk.test_old_to_new_single()
dask.array.tests.test_rechunk.test_plan_rechunk()
dask.array.tests.test_rechunk.test_plan_rechunk_5d()
dask.array.tests.test_rechunk.test_plan_rechunk_asymmetric()
dask.array.tests.test_rechunk.test_plan_rechunk_heterogenous()
dask.array.tests.test_rechunk.test_rechunk_0d()
dask.array.tests.test_rechunk.test_rechunk_1d()
dask.array.tests.test_rechunk.test_rechunk_2d()
dask.array.tests.test_rechunk.test_rechunk_4d()
dask.array.tests.test_rechunk.test_rechunk_blockshape()
dask.array.tests.test_rechunk.test_rechunk_empty()
dask.array.tests.test_rechunk.test_rechunk_expand()
dask.array.tests.test_rechunk.test_rechunk_expand2()
dask.array.tests.test_rechunk.test_rechunk_intermediates()
dask.array.tests.test_rechunk.test_rechunk_internals_1()
dask.array.tests.test_rechunk.test_rechunk_method()
dask.array.tests.test_rechunk.test_rechunk_same()
dask.array.tests.test_rechunk.test_rechunk_unknown(x,chunks)
dask.array.tests.test_rechunk.test_rechunk_unknown_explicit()
dask.array.tests.test_rechunk.test_rechunk_unknown_from_array()
dask.array.tests.test_rechunk.test_rechunk_unknown_from_pandas()
dask.array.tests.test_rechunk.test_rechunk_unknown_raises()
dask.array.tests.test_rechunk.test_rechunk_warning()
dask.array.tests.test_rechunk.test_rechunk_with_dict()
dask.array.tests.test_rechunk.test_rechunk_with_empty_input()
dask.array.tests.test_rechunk.test_rechunk_with_integer()
dask.array.tests.test_rechunk.test_rechunk_with_null_dimensions()


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/array/tests/test_stats.py----------------------------------------
A:dask.array.tests.test_stats.scipy->pytest.importorskip('scipy')
A:dask.array.tests.test_stats.x->numpy.random.random(size=(30, 2))
A:dask.array.tests.test_stats.y->dask.array.from_array(x, 3)
A:dask.array.tests.test_stats.dfunc->getattr(dask.array.stats, kind)
A:dask.array.tests.test_stats.sfunc->getattr(scipy.stats, kind)
A:dask.array.tests.test_stats.expected->pytest.importorskip('scipy').stats.f_oneway(*np_args)
A:dask.array.tests.test_stats.result->dask.array.stats.f_oneway(*da_args)
A:dask.array.tests.test_stats.a->dask.array.ones((7,), chunks=(7,))
A:dask.array.tests.test_stats.a_->dask.array.from_array(a, 3)
A:dask.array.tests.test_stats.dask_test->getattr(dask.array.stats, kind)
A:dask.array.tests.test_stats.scipy_test->getattr(scipy.stats, kind)
A:dask.array.tests.test_stats.b->numpy.random.random(size=30)
A:dask.array.tests.test_stats.b_->dask.array.from_array(b, 3)
dask.array.tests.test_stats.test_anova()
dask.array.tests.test_stats.test_bias_raises()
dask.array.tests.test_stats.test_measures(kind,kwargs)
dask.array.tests.test_stats.test_moments(k)
dask.array.tests.test_stats.test_nan_raises(func,nargs,nan_policy)
dask.array.tests.test_stats.test_one(kind)
dask.array.tests.test_stats.test_power_divergence_invalid()
dask.array.tests.test_stats.test_skew_raises()
dask.array.tests.test_stats.test_two(kind,kwargs)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/array/tests/test_array_core.py----------------------------------------
A:dask.array.tests.test_array_core.np->pytest.importorskip('numpy')
A:dask.array.tests.test_array_core.x->dask.array.ones(5, chunks=3)
A:dask.array.tests.test_array_core.z->dask.array.ones(5, chunks=3).map_blocks(add, y, dtype=x.dtype)
A:dask.array.tests.test_array_core.o->pytest.importorskip('numpy').ones((20, 20))
A:dask.array.tests.test_array_core.getx->getem('x', (5, 5), shape=(20, 20))
A:dask.array.tests.test_array_core.geto->getem('o', (5, 5), shape=(20, 20))
A:dask.array.tests.test_array_core.result->optimize(expr.dask, expr._keys())
A:dask.array.tests.test_array_core.dsk->dask.array.ones(5, chunks=3).map_blocks(add, y, dtype=x.dtype)._optimize(z.dask, z._keys(), fuse_keys=y._keys())
A:dask.array.tests.test_array_core.out->delayed(identity)(a).map_blocks(func, dtype='i8', c=1)
A:dask.array.tests.test_array_core.comp->top(f, 'out', 'ij', 'x', 'ji', numblocks={'x': (4, 4)})
A:dask.array.tests.test_array_core.a->dask.array.zeros((1,), chunks=(1,))
A:dask.array.tests.test_array_core.dx->dask.array.from_array(x, chunks=(2, 2))
A:dask.array.tests.test_array_core.d->delayed(identity)(a)
A:dask.array.tests.test_array_core.s->dask.array.sum(x)
A:dask.array.tests.test_array_core.colon->slice(None, None, None)
A:dask.array.tests.test_array_core.s2->stack([a, b, c], axis=2)
A:dask.array.tests.test_array_core.i->pytest.importorskip('numpy').arange(10, dtype='i4')
A:dask.array.tests.test_array_core.f->pytest.importorskip('h5py').File(fn1)
A:dask.array.tests.test_array_core.di->dask.array.from_array(i, chunks=5)
A:dask.array.tests.test_array_core.df->dask.array.from_array(f, chunks=5)
A:dask.array.tests.test_array_core.res->dask.array.core.map_blocks(func, d, f, dtype=d.dtype)
A:dask.array.tests.test_array_core.y->dask.array.ones(10, chunks=(5,))
A:dask.array.tests.test_array_core.dd->pytest.importorskip('dask.dataframe')
A:dask.array.tests.test_array_core.pd->pytest.importorskip('pandas')
A:dask.array.tests.test_array_core.a_df->pytest.importorskip('pandas').DataFrame({'x': np.arange(12)})
A:dask.array.tests.test_array_core.b_df->pytest.importorskip('pandas').DataFrame({'y': np.arange(12) * 10})
A:dask.array.tests.test_array_core.a_ddf->pytest.importorskip('dask.dataframe').from_pandas(a_df, sort=False, npartitions=3)
A:dask.array.tests.test_array_core.b_ddf->pytest.importorskip('dask.dataframe').from_pandas(b_df, sort=False, npartitions=3)
A:dask.array.tests.test_array_core.c_x->dask.array.concatenate([a_x, b_x], axis=1, allow_unknown_chunksizes=True)
A:dask.array.tests.test_array_core.b->dask.array.from_delayed(d, shape=a.shape, dtype=a.dtype)
A:dask.array.tests.test_array_core.c->dask.array.from_array(z, chunks=(2,))
A:dask.array.tests.test_array_core.data->pytest.importorskip('numpy').ones((100, 50), dtype=dtype)
A:dask.array.tests.test_array_core.list_vec->list(range(1, 6))
A:dask.array.tests.test_array_core.xb->dask.array.chunk.broadcast_to(x, shape)
A:dask.array.tests.test_array_core.ab->broadcast_to(a, shape)
A:dask.array.tests.test_array_core.xr->dask.array.ones(5, chunks=3).reshape(new_shape)
A:dask.array.tests.test_array_core.ar->dask.array.zeros((1,), chunks=(1,)).reshape(new_shape)
A:dask.array.tests.test_array_core.e->delayed(identity)(a).map_blocks(lambda b: b.sum(axis=1)[:, None, None], drop_axis=1, new_axis=(1, 2), dtype=d.dtype)
A:dask.array.tests.test_array_core.expected->_vindex_transpose(x[tuple(slc)], axis)
A:dask.array.tests.test_array_core.dy->dask.array.ones(11, chunks=(3,))
A:dask.array.tests.test_array_core.dz->dask.array.map_blocks(np.add, dx, dy, chunks=dx.chunks)
A:dask.array.tests.test_array_core.cast->kwargs.pop('cast', 'i8')
A:dask.array.tests.test_array_core.atd->delayed(make_target)('at')
A:dask.array.tests.test_array_core.btd->delayed(make_target)('bt')
A:dask.array.tests.test_array_core.at->pytest.importorskip('numpy').zeros(shape=(10, 10))
A:dask.array.tests.test_array_core.bt->pytest.importorskip('numpy').zeros(shape=(10, 10))
A:dask.array.tests.test_array_core.v->delayed(np.ones)((5, 3))
A:dask.array.tests.test_array_core.self.max_concurrent_uses->max(self.concurrent_uses, self.max_concurrent_uses)
A:dask.array.tests.test_array_core._Lock->type(Lock())
A:dask.array.tests.test_array_core.lock->Lock()
A:dask.array.tests.test_array_core.locks->set((vv for v in dsk.values() for vv in v if isinstance(vv, _Lock)))
A:dask.array.tests.test_array_core.h5py->pytest.importorskip('h5py')
A:dask.array.tests.test_array_core.(l1, l2)->dask.array.modf(a)
A:dask.array.tests.test_array_core.(r1, r2)->pytest.importorskip('numpy').modf(x)
A:dask.array.tests.test_array_core.chunks->dask.array.ones(5, chunks=3)._get(x.dask, x._keys())
A:dask.array.tests.test_array_core.axis->_get_axis(ind)
A:dask.array.tests.test_array_core.k->len(next((i for i in ind if isinstance(i, (np.ndarray, list)))))
A:dask.array.tests.test_array_core.inds->pytest.importorskip('numpy').array([0, 3, 6], dtype='u8')
A:dask.array.tests.test_array_core.target->pytest.importorskip('numpy').memmap(fn_1, shape=x.shape, mode='w+', dtype=x.dtype)
A:dask.array.tests.test_array_core.g->pytest.importorskip('h5py').File(fn2)
A:dask.array.tests.test_array_core.f['x']->pytest.importorskip('numpy').arange(10).astype(float)
A:dask.array.tests.test_array_core.g['x']->pytest.importorskip('numpy').ones(10).astype(float)
A:dask.array.tests.test_array_core.[[a, b], [c, d]]->dask.array.ones(10, chunks=(5,)).to_delayed()
A:dask.array.tests.test_array_core.rs->pytest.importorskip('numpy').random.RandomState(0)
A:dask.array.tests.test_array_core.y2->dask.array.from_array(x, chunks=x.shape[0] / 10, name=False)
A:dask.array.tests.test_array_core.xx->pytest.importorskip('numpy').arange(5)
A:dask.array.tests.test_array_core.names->countby(key_split, d.dask)
A:dask.array.tests.test_array_core.a2->loads(dumps(a))
A:dask.array.tests.test_array_core.yy->delayed(y)
A:dask.array.tests.test_array_core.zz->dask.array.ones(5, chunks=3).map_blocks(add, yy, dtype=x.dtype)
A:dask.array.tests.test_array_core.X->pytest.importorskip('numpy').arange(24).reshape((4, 6))
A:dask.array.tests.test_array_core.dind->dask.array.from_array(ind, (2, 2))
A:dask.array.tests.test_array_core.start->time.time()
A:dask.array.tests.test_array_core.end->time.time()
dask.array.tests.test_array_core.NonthreadSafeStore(self)
dask.array.tests.test_array_core.NonthreadSafeStore.__init__(self)
dask.array.tests.test_array_core.NonthreadSafeStore.__setitem__(self,key,value)
dask.array.tests.test_array_core.ThreadSafeStore(self)
dask.array.tests.test_array_core.ThreadSafeStore.__init__(self)
dask.array.tests.test_array_core.ThreadSafeStore.__setitem__(self,key,value)
dask.array.tests.test_array_core.ThreadSafetyError(Exception)
dask.array.tests.test_array_core.test_A_property()
dask.array.tests.test_array_core.test_Array()
dask.array.tests.test_array_core.test_Array_computation()
dask.array.tests.test_array_core.test_Array_normalizes_dtype()
dask.array.tests.test_array_core.test_T()
dask.array.tests.test_array_core.test_arithmetic()
dask.array.tests.test_array_core.test_array_compute_forward_kwargs()
dask.array.tests.test_array_core.test_array_picklable()
dask.array.tests.test_array_core.test_asanyarray()
dask.array.tests.test_array_core.test_asarray()
dask.array.tests.test_array_core.test_asarray_h5py()
dask.array.tests.test_array_core.test_astype()
dask.array.tests.test_array_core.test_astype_gh1151()
dask.array.tests.test_array_core.test_atop_chunks()
dask.array.tests.test_array_core.test_atop_concatenate()
dask.array.tests.test_array_core.test_atop_kwargs()
dask.array.tests.test_array_core.test_atop_literals()
dask.array.tests.test_array_core.test_atop_names()
dask.array.tests.test_array_core.test_atop_new_axes()
dask.array.tests.test_array_core.test_atop_with_numpy_arrays()
dask.array.tests.test_array_core.test_atop_zero_shape()
dask.array.tests.test_array_core.test_atop_zero_shape_new_axes()
dask.array.tests.test_array_core.test_binops()
dask.array.tests.test_array_core.test_blockdims_from_blockshape()
dask.array.tests.test_array_core.test_broadcast_against_zero_shape()
dask.array.tests.test_array_core.test_broadcast_chunks()
dask.array.tests.test_array_core.test_broadcast_dimensions()
dask.array.tests.test_array_core.test_broadcast_dimensions_works_with_singleton_dimensions()
dask.array.tests.test_array_core.test_broadcast_shapes()
dask.array.tests.test_array_core.test_broadcast_to()
dask.array.tests.test_array_core.test_broadcast_to_array()
dask.array.tests.test_array_core.test_broadcast_to_scalar()
dask.array.tests.test_array_core.test_chunked_dot_product()
dask.array.tests.test_array_core.test_chunked_transpose_plus_one()
dask.array.tests.test_array_core.test_chunks_error()
dask.array.tests.test_array_core.test_chunks_is_immutable()
dask.array.tests.test_array_core.test_coerce()
dask.array.tests.test_array_core.test_common_blockdim()
dask.array.tests.test_array_core.test_concatenate()
dask.array.tests.test_array_core.test_concatenate3_2()
dask.array.tests.test_array_core.test_concatenate3_on_scalars()
dask.array.tests.test_array_core.test_concatenate_axes()
dask.array.tests.test_array_core.test_concatenate_errs()
dask.array.tests.test_array_core.test_concatenate_fixlen_strings()
dask.array.tests.test_array_core.test_concatenate_rechunk()
dask.array.tests.test_array_core.test_concatenate_stack_dont_warn()
dask.array.tests.test_array_core.test_concatenate_unknown_axes()
dask.array.tests.test_array_core.test_constructor_plugin()
dask.array.tests.test_array_core.test_copy_mutate()
dask.array.tests.test_array_core.test_cumulative()
dask.array.tests.test_array_core.test_delayed_array_key_hygeine()
dask.array.tests.test_array_core.test_dont_dealias_outputs()
dask.array.tests.test_array_core.test_dont_fuse_outputs()
dask.array.tests.test_array_core.test_dtype()
dask.array.tests.test_array_core.test_dtype_complex()
dask.array.tests.test_array_core.test_elemwise_consistent_names()
dask.array.tests.test_array_core.test_elemwise_differently_chunked()
dask.array.tests.test_array_core.test_elemwise_dtype()
dask.array.tests.test_array_core.test_elemwise_name()
dask.array.tests.test_array_core.test_elemwise_on_scalars()
dask.array.tests.test_array_core.test_elemwise_uneven_chunks()
dask.array.tests.test_array_core.test_elemwise_with_lists(chunks,other)
dask.array.tests.test_array_core.test_elemwise_with_ndarrays()
dask.array.tests.test_array_core.test_ellipsis_slicing()
dask.array.tests.test_array_core.test_empty_array()
dask.array.tests.test_array_core.test_fast_from_array()
dask.array.tests.test_array_core.test_field_access()
dask.array.tests.test_array_core.test_field_access_with_shape()
dask.array.tests.test_array_core.test_from_array_getitem()
dask.array.tests.test_array_core.test_from_array_names()
dask.array.tests.test_array_core.test_from_array_no_asarray()
dask.array.tests.test_array_core.test_from_array_raises_on_bad_chunks()
dask.array.tests.test_array_core.test_from_array_with_lock()
dask.array.tests.test_array_core.test_from_array_with_missing_chunks()
dask.array.tests.test_array_core.test_from_delayed()
dask.array.tests.test_array_core.test_from_func()
dask.array.tests.test_array_core.test_from_function_requires_block_args()
dask.array.tests.test_array_core.test_full()
dask.array.tests.test_array_core.test_getem()
dask.array.tests.test_array_core.test_getter()
dask.array.tests.test_array_core.test_h5py_newaxis()
dask.array.tests.test_array_core.test_h5py_tokenize()
dask.array.tests.test_array_core.test_index_array_with_array_1d()
dask.array.tests.test_array_core.test_index_array_with_array_2d()
dask.array.tests.test_array_core.test_index_array_with_array_3d_2d()
dask.array.tests.test_array_core.test_itemsize()
dask.array.tests.test_array_core.test_keys()
dask.array.tests.test_array_core.test_long_slice()
dask.array.tests.test_array_core.test_map_blocks()
dask.array.tests.test_array_core.test_map_blocks2()
dask.array.tests.test_array_core.test_map_blocks3()
dask.array.tests.test_array_core.test_map_blocks_delayed()
dask.array.tests.test_array_core.test_map_blocks_dtype_inference()
dask.array.tests.test_array_core.test_map_blocks_name()
dask.array.tests.test_array_core.test_map_blocks_with_changed_dimension()
dask.array.tests.test_array_core.test_map_blocks_with_chunks()
dask.array.tests.test_array_core.test_map_blocks_with_constants()
dask.array.tests.test_array_core.test_map_blocks_with_kwargs()
dask.array.tests.test_array_core.test_matmul()
dask.array.tests.test_array_core.test_memmap()
dask.array.tests.test_array_core.test_nbytes()
dask.array.tests.test_array_core.test_no_chunks()
dask.array.tests.test_array_core.test_no_chunks_2d()
dask.array.tests.test_array_core.test_no_chunks_slicing_2d()
dask.array.tests.test_array_core.test_no_chunks_yes_chunks()
dask.array.tests.test_array_core.test_no_warnings_on_metadata()
dask.array.tests.test_array_core.test_norm()
dask.array.tests.test_array_core.test_normalize_chunks()
dask.array.tests.test_array_core.test_np_array_with_zero_dimensions()
dask.array.tests.test_array_core.test_npartitions()
dask.array.tests.test_array_core.test_numblocks_suppoorts_singleton_block_dims()
dask.array.tests.test_array_core.test_operator_dtype_promotion()
dask.array.tests.test_array_core.test_operators()
dask.array.tests.test_array_core.test_optimize()
dask.array.tests.test_array_core.test_optimize_fuse_keys()
dask.array.tests.test_array_core.test_point_slicing()
dask.array.tests.test_array_core.test_point_slicing_with_full_slice()
dask.array.tests.test_array_core.test_raise_informative_errors_no_chunks()
dask.array.tests.test_array_core.test_raise_on_bad_kwargs()
dask.array.tests.test_array_core.test_raise_on_no_chunks()
dask.array.tests.test_array_core.test_random_from_array()
dask.array.tests.test_array_core.test_repr()
dask.array.tests.test_array_core.test_reshape(original_shape,new_shape,chunks)
dask.array.tests.test_array_core.test_reshape_exceptions()
dask.array.tests.test_array_core.test_reshape_fails_for_dask_only()
dask.array.tests.test_array_core.test_reshape_splat()
dask.array.tests.test_array_core.test_reshape_unknown_dimensions()
dask.array.tests.test_array_core.test_setitem_1d()
dask.array.tests.test_array_core.test_setitem_2d()
dask.array.tests.test_array_core.test_setitem_errs()
dask.array.tests.test_array_core.test_setitem_mixed_d()
dask.array.tests.test_array_core.test_short_stack()
dask.array.tests.test_array_core.test_size()
dask.array.tests.test_array_core.test_slice_with_floats()
dask.array.tests.test_array_core.test_slice_with_uint()
dask.array.tests.test_array_core.test_slicing_with_ellipsis()
dask.array.tests.test_array_core.test_slicing_with_ndarray()
dask.array.tests.test_array_core.test_slicing_with_non_ndarrays()
dask.array.tests.test_array_core.test_stack()
dask.array.tests.test_array_core.test_stack_errs()
dask.array.tests.test_array_core.test_stack_promote_type()
dask.array.tests.test_array_core.test_stack_rechunk()
dask.array.tests.test_array_core.test_stack_scalars()
dask.array.tests.test_array_core.test_store()
dask.array.tests.test_array_core.test_store_compute_false()
dask.array.tests.test_array_core.test_store_delayed_target()
dask.array.tests.test_array_core.test_store_locks()
dask.array.tests.test_array_core.test_store_multiprocessing_lock()
dask.array.tests.test_array_core.test_store_regions()
dask.array.tests.test_array_core.test_timedelta_op()
dask.array.tests.test_array_core.test_to_dask_dataframe()
dask.array.tests.test_array_core.test_to_delayed()
dask.array.tests.test_array_core.test_to_delayed_optimizes()
dask.array.tests.test_array_core.test_to_hdf5()
dask.array.tests.test_array_core.test_to_npy_stack()
dask.array.tests.test_array_core.test_top()
dask.array.tests.test_array_core.test_top_literals()
dask.array.tests.test_array_core.test_top_supports_broadcasting_rules()
dask.array.tests.test_array_core.test_uneven_chunks()
dask.array.tests.test_array_core.test_uneven_chunks_atop()
dask.array.tests.test_array_core.test_uneven_chunks_that_fit_neatly()
dask.array.tests.test_array_core.test_view()
dask.array.tests.test_array_core.test_view_fortran()
dask.array.tests.test_array_core.test_vindex_basic()
dask.array.tests.test_array_core.test_vindex_errors()
dask.array.tests.test_array_core.test_vindex_merge()
dask.array.tests.test_array_core.test_vindex_nd()
dask.array.tests.test_array_core.test_warn_bad_rechunking()
dask.array.tests.test_array_core.test_zero_sized_array_rechunk()
dask.array.tests.test_array_core.test_zero_slice_dtypes()


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/array/tests/test_fft.py----------------------------------------
A:dask.array.tests.test_fft.nparr->numpy.arange(100).reshape(10, 10)
A:dask.array.tests.test_fft.darr->dask.array.from_array(nparr, chunks=(1, 10))
A:dask.array.tests.test_fft.darr2->dask.array.from_array(nparr, chunks=(10, 1))
A:dask.array.tests.test_fft.darr3->dask.array.from_array(nparr, chunks=(10, 10))
A:dask.array.tests.test_fft.da_fft->getattr(da.fft, funcname)
A:dask.array.tests.test_fft.bad_darr->dask.array.from_array(nparr, chunks=(5, 5))
A:dask.array.tests.test_fft.np_fft->getattr(np.fft, funcname)
A:dask.array.tests.test_fft.a->numpy.arange(np.prod(s)).reshape(s)
A:dask.array.tests.test_fft.d->dask.array.from_array(a, chunks=(2, 3, 4))
A:dask.array.tests.test_fft.cs->list(chunk_size)
A:dask.array.tests.test_fft.d2->dask.array.from_array(a, chunks=(2, 3, 4)).rechunk(cs)
A:dask.array.tests.test_fft.r->da_fft(d2, axes=axes)
A:dask.array.tests.test_fft.er->np_fft(a, axes=axes)
A:dask.array.tests.test_fft.fft_mod->pytest.importorskip(modname)
A:dask.array.tests.test_fft.func->getattr(fft_mod, funcname)
A:dask.array.tests.test_fft.darrc->dask.array.from_array(nparr, chunks=(1, 10)).astype(dtype).rechunk(darr.shape)
A:dask.array.tests.test_fft.darr2c->dask.array.from_array(nparr, chunks=(10, 1)).astype(dtype).rechunk(darr2.shape)
A:dask.array.tests.test_fft.nparrc->numpy.arange(100).reshape(10, 10).astype(dtype)
A:dask.array.tests.test_fft.wfunc->fft_wrap(func)
A:dask.array.tests.test_fft.c->c(n)
A:dask.array.tests.test_fft.r1->numpy.fft.rfftfreq(n, d)
A:dask.array.tests.test_fft.r2->dask.array.fft.rfftfreq(n, d, chunks=c)
A:dask.array.tests.test_fft.np_func->getattr(np.fft, funcname)
A:dask.array.tests.test_fft.da_func->getattr(da.fft, funcname)
A:dask.array.tests.test_fft.da_func1->getattr(da.fft, funcname1)
A:dask.array.tests.test_fft.da_func2->getattr(da.fft, funcname2)
dask.array.tests.test_fft.test_cant_fft_chunked_axis(funcname)
dask.array.tests.test_fft.test_fft(funcname)
dask.array.tests.test_fft.test_fft2n_shapes(funcname)
dask.array.tests.test_fft.test_fft_consistent_names(funcname)
dask.array.tests.test_fft.test_fft_n_kwarg(funcname)
dask.array.tests.test_fft.test_fftfreq(n,d,c)
dask.array.tests.test_fft.test_fftshift(funcname,axes)
dask.array.tests.test_fft.test_fftshift_identity(funcname1,funcname2,axes)
dask.array.tests.test_fft.test_nd_ffts_axes(funcname,dtype)
dask.array.tests.test_fft.test_rfftfreq(n,d,c)
dask.array.tests.test_fft.test_wrap_bad_kind()
dask.array.tests.test_fft.test_wrap_fftns(modname,funcname,dtype)
dask.array.tests.test_fft.test_wrap_ffts(modname,funcname,dtype)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/array/tests/test_linalg.py----------------------------------------
A:dask.array.tests.test_linalg.mat->numpy.random.randn(m, r).dot(mat2)
A:dask.array.tests.test_linalg.data->dask.array.from_array(mat, chunks=(500, 50))
A:dask.array.tests.test_linalg.(q, r)->tsqr(data)
A:dask.array.tests.test_linalg.q->numpy.array(q)
A:dask.array.tests.test_linalg.r->numpy.array(r)
A:dask.array.tests.test_linalg.(u, s, vt)->svd_compressed(x, 3, seed=1234)
A:dask.array.tests.test_linalg.u->numpy.array(u)
A:dask.array.tests.test_linalg.s->numpy.array(s)
A:dask.array.tests.test_linalg.vt->numpy.array(vt)
A:dask.array.tests.test_linalg.usvt->numpy.dot(u, np.dot(np.diag(s), vt))
A:dask.array.tests.test_linalg.(q1, r1)->qr(data)
A:dask.array.tests.test_linalg.(q2, r2)->qr(data)
A:dask.array.tests.test_linalg.(u1, s1, v1)->svd(data)
A:dask.array.tests.test_linalg.(u2, s2, v2)->svd(data)
A:dask.array.tests.test_linalg.mat1->numpy.random.randn(m, r)
A:dask.array.tests.test_linalg.mat2->numpy.random.randn(r, n)
A:dask.array.tests.test_linalg.x->numpy.random.random((100, 10))
A:dask.array.tests.test_linalg.(u2, s2, vt2)->svd_compressed(x, 3, seed=1234)
A:dask.array.tests.test_linalg.A1->numpy.array([[7, 3, -1, 2], [3, 8, 1, -4], [-1, 1, 4, -1], [2, -4, -1, 6]])
A:dask.array.tests.test_linalg.A2->numpy.array([[7, 0, 0, 0, 0, 0], [0, 8, 0, 0, 0, 0], [0, 0, 4, 0, 0, 0], [0, 0, 0, 6, 0, 0], [0, 0, 0, 0, 3, 0], [0, 0, 0, 0, 0, 5]])
A:dask.array.tests.test_linalg.dA->dask.array.from_array(A, (chunk, ncol))
A:dask.array.tests.test_linalg.(p, l, u)->scipy.linalg.lu(A)
A:dask.array.tests.test_linalg.(dp, dl, du)->dask.array.linalg.lu(dA)
A:dask.array.tests.test_linalg.A3->numpy.array([[7, 3, 2, 1, 4, 1], [7, 11, 5, 2, 5, 2], [21, 25, 16, 10, 16, 5], [21, 41, 18, 13, 16, 11], [14, 46, 23, 24, 21, 22], [0, 56, 29, 17, 14, 8]])
A:dask.array.tests.test_linalg.A->numpy.random.randint(1, 20, (nrow, ncol))
A:dask.array.tests.test_linalg.b->numpy.random.randint(1, 20, nrow)
A:dask.array.tests.test_linalg.Au->numpy.triu(A)
A:dask.array.tests.test_linalg.dAu->dask.array.from_array(Au, (chunk, chunk))
A:dask.array.tests.test_linalg.db->dask.array.from_array(b, chunk)
A:dask.array.tests.test_linalg.res->dask.array.linalg.solve(dA, db, sym_pos=True)
A:dask.array.tests.test_linalg.Al->numpy.tril(A)
A:dask.array.tests.test_linalg.dAl->dask.array.from_array(Al, (chunk, chunk))
A:dask.array.tests.test_linalg.lA->numpy.tril(A)
A:dask.array.tests.test_linalg.(x, r, rank, s)->numpy.linalg.lstsq(A, b, rcond=np.finfo(np.double).eps * max(nrow, ncol))
A:dask.array.tests.test_linalg.(dx, dr, drank, ds)->dask.array.linalg.lstsq(dA, db)
A:dask.array.tests.test_linalg.(u, s, v)->numpy.linalg.svd(x, full_matrices=0)
A:dask.array.tests.test_linalg.dx->dask.array.from_array(x, chunks=(10, 10))
A:dask.array.tests.test_linalg.(du, ds, dv)->dask.array.linalg.svd(dx)
A:dask.array.tests.test_linalg.a->numpy.random.random(shape)
A:dask.array.tests.test_linalg.d->d.rechunk((d.chunks[0], d.shape[1])).rechunk((d.chunks[0], d.shape[1]))
A:dask.array.tests.test_linalg.a_r->numpy.linalg.norm(a, ord=norm, axis=axis, keepdims=keepdims)
A:dask.array.tests.test_linalg.d_r->dask.array.linalg.norm(d, ord=norm, axis=axis, keepdims=keepdims)
dask.array.tests.test_linalg._check_lu_result(p,l,u,A)
dask.array.tests.test_linalg._get_symmat(size)
dask.array.tests.test_linalg.test_cholesky(shape,chunk)
dask.array.tests.test_linalg.test_inv(shape,chunk)
dask.array.tests.test_linalg.test_linalg_consistent_names()
dask.array.tests.test_linalg.test_lstsq(nrow,ncol,chunk)
dask.array.tests.test_linalg.test_lu_1()
dask.array.tests.test_linalg.test_lu_2(size)
dask.array.tests.test_linalg.test_lu_3(size)
dask.array.tests.test_linalg.test_lu_errors()
dask.array.tests.test_linalg.test_no_chunks_svd()
dask.array.tests.test_linalg.test_norm_1dim(shape,chunks,axis,norm,keepdims)
dask.array.tests.test_linalg.test_norm_2dim(shape,chunks,axis,norm,keepdims)
dask.array.tests.test_linalg.test_norm_any_ndim(shape,chunks,axis,norm,keepdims)
dask.array.tests.test_linalg.test_solve(shape,chunk)
dask.array.tests.test_linalg.test_solve_sym_pos(shape,chunk)
dask.array.tests.test_linalg.test_solve_triangular_errors()
dask.array.tests.test_linalg.test_solve_triangular_matrix(shape,chunk)
dask.array.tests.test_linalg.test_solve_triangular_matrix2(shape,chunk)
dask.array.tests.test_linalg.test_solve_triangular_vector(shape,chunk)
dask.array.tests.test_linalg.test_svd_compressed()
dask.array.tests.test_linalg.test_svd_compressed_deterministic()
dask.array.tests.test_linalg.test_tsqr_irregular_blocks()
dask.array.tests.test_linalg.test_tsqr_regular_blocks()
dask.array.tests.test_linalg.test_tsqr_svd_irregular_blocks()
dask.array.tests.test_linalg.test_tsqr_svd_regular_blocks()


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/array/tests/test_random.py----------------------------------------
A:dask.array.tests.test_random.state->dask.array.random.RandomState(5)
A:dask.array.tests.test_random.x->dask.array.random.multinomial(20, [1 / 6.0] * 6, size=size, chunks=chunks)
A:dask.array.tests.test_random.y->numpy.random.multinomial(20, [1 / 6.0] * 6, size=size)
A:dask.array.tests.test_random.samples_1->dask.array.random.RandomState(42).normal(size=1000, chunks=10)
A:dask.array.tests.test_random.samples_2->dask.array.random.RandomState(42).normal(size=1000, chunks=10)
A:dask.array.tests.test_random.state1->dask.array.random.RandomState(42)
A:dask.array.tests.test_random.state2->dask.array.random.RandomState(42)
A:dask.array.tests.test_random.a->dask.array.random.normal(1000 * o, 0.01, chunks=(50,))
A:dask.array.tests.test_random.b->dask.array.random.normal(size=10, chunks=5)
A:dask.array.tests.test_random.x1->dask.array.random.RandomState(123).random(20, chunks=20)
A:dask.array.tests.test_random.arr->numpy.arange(6).reshape((2, 3))
A:dask.array.tests.test_random.daones->dask.array.ones((2, 3, 4), chunks=3)
A:dask.array.tests.test_random.z->dask.array.random.normal(y, 0.01, chunks=(10,))
dask.array.tests.test_random.test_RandomState()
dask.array.tests.test_random.test_array_broadcasting()
dask.array.tests.test_random.test_can_make_really_big_random_array()
dask.array.tests.test_random.test_concurrency()
dask.array.tests.test_random.test_consistent_across_sizes()
dask.array.tests.test_random.test_determinisim_through_dask_values()
dask.array.tests.test_random.test_doc_randomstate()
dask.array.tests.test_random.test_docs()
dask.array.tests.test_random.test_kwargs()
dask.array.tests.test_random.test_multinomial()
dask.array.tests.test_random.test_parametrized_random_function()
dask.array.tests.test_random.test_random()
dask.array.tests.test_random.test_random_all()
dask.array.tests.test_random.test_random_seed()
dask.array.tests.test_random.test_randomstate_consistent_names()
dask.array.tests.test_random.test_serializability()
dask.array.tests.test_random.test_unique_names()


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/array/tests/test_chunk.py----------------------------------------
A:dask.array.tests.test_chunk.summer_wrapped->keepdims_wrapper(summer)
A:dask.array.tests.test_chunk.a->numpy.arange(24).reshape(1, 2, 3, 4)
A:dask.array.tests.test_chunk.r->summer(a, axis=(1, 3))
A:dask.array.tests.test_chunk.rw->summer_wrapped(a, axis=(1, 3), keepdims=True)
A:dask.array.tests.test_chunk.rwf->summer_wrapped(a, axis=(1, 3), keepdims=False)
A:dask.array.tests.test_chunk.x->numpy.random.randint(10, size=(24, 24))
A:dask.array.tests.test_chunk.y->coarsen(np.sum, x, {0: 2, 1: 4})
dask.array.tests.test_chunk.test_coarsen()
dask.array.tests.test_chunk.test_integer_input()
dask.array.tests.test_chunk.test_keepdims_wrapper_no_axis()
dask.array.tests.test_chunk.test_keepdims_wrapper_one_axis()
dask.array.tests.test_chunk.test_keepdims_wrapper_two_axes()


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/array/tests/test_learn.py----------------------------------------
A:dask.array.tests.test_learn.x->numpy.array([[1, 0], [2, 0], [3, 0], [4, 0], [0, 1], [0, 2], [3, 3], [4, 4]])
A:dask.array.tests.test_learn.y->numpy.array([1, 1, 1, 1, -1, -1, 0, 0])
A:dask.array.tests.test_learn.z->numpy.array([[1, -1], [-1, 1], [10, -10], [-10, 10]])
A:dask.array.tests.test_learn.X->dask.array.from_array(x, chunks=(3, 2))
A:dask.array.tests.test_learn.Y->dask.array.from_array(y, chunks=(3,))
A:dask.array.tests.test_learn.Z->dask.array.from_array(z, chunks=(2, 2))
A:dask.array.tests.test_learn.sgd->dask.array.learn.fit(sgd, X, Y, get=dask.get, classes=np.array([-1, 0, 1]))
A:dask.array.tests.test_learn.sol->dask.array.learn.fit(sgd, X, Y, get=dask.get, classes=np.array([-1, 0, 1])).predict(z)
A:dask.array.tests.test_learn.result->dask.array.learn.predict(sgd, Z)
dask.array.tests.test_learn.test_fit()


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/array/tests/test_routines.py----------------------------------------
A:dask.array.tests.test_routines.np->pytest.importorskip('numpy')
A:dask.array.tests.test_routines.x->pytest.importorskip('numpy').ones(5, np.float32)
A:dask.array.tests.test_routines.d->dask.array.from_array(x, chunks=(4, 8))
A:dask.array.tests.test_routines.y->pytest.importorskip('numpy').random.randint(10, size=(5, 10))
A:dask.array.tests.test_routines.a->dask.array.from_array(np.ones(5, np.float32), chunks=(3,))
A:dask.array.tests.test_routines.b->dask.array.from_array(np.ones(5, np.int16), chunks=(3,))
A:dask.array.tests.test_routines.e->dask.array.from_array(y, chunks=(4, 5))
A:dask.array.tests.test_routines.weights->pytest.importorskip('numpy').array([1, 2, 1, 0.5, 1])
A:dask.array.tests.test_routines.dweights->dask.array.from_array(weights, chunks=2)
A:dask.array.tests.test_routines.bins->pytest.importorskip('numpy').arange(0, 1.01, 0.01)
A:dask.array.tests.test_routines.v->dask.array.random.random(100, chunks=10)
A:dask.array.tests.test_routines.(a1, b1)->dask.array.histogram(v, bins=10, range=(0, 1))
A:dask.array.tests.test_routines.(a2, b2)->pytest.importorskip('numpy').histogram(v, bins=10, range=(0, 1))
A:dask.array.tests.test_routines.a_flat->dask.array.from_array(np.ones(5, np.float32), chunks=(3,)).ravel()
A:dask.array.tests.test_routines.z->pytest.importorskip('numpy').random.randint(10, size=(1, 2))
A:dask.array.tests.test_routines.c1->pytest.importorskip('numpy').array([1, 0, 1])
A:dask.array.tests.test_routines.c2->dask.array.from_array(c1, chunks=2)
A:dask.array.tests.test_routines.dc1->dask.array.from_array(c1, chunks=3)
A:dask.array.tests.test_routines.dc2->dask.array.from_array(c2, chunks=(2, 1))
A:dask.array.tests.test_routines.res->dask.array.extract(dc, a)
A:dask.array.tests.test_routines.c3->pytest.importorskip('numpy').array([True, False])
A:dask.array.tests.test_routines.dc3->dask.array.from_array(c3, chunks=2)
A:dask.array.tests.test_routines.x_nz->pytest.importorskip('numpy').ones(5, np.float32).nonzero()
A:dask.array.tests.test_routines.d_nz->dask.array.from_array(x, chunks=(4, 8)).nonzero()
A:dask.array.tests.test_routines.w1->dask.array.where(c, d, e)
A:dask.array.tests.test_routines.w2->pytest.importorskip('numpy').where(c, x, y)
A:dask.array.tests.test_routines.y1->pytest.importorskip('numpy').array([4, 5, 6], dtype=np.int16)
A:dask.array.tests.test_routines.y2->dask.array.from_array(y1, chunks=2)
A:dask.array.tests.test_routines.w3->pytest.importorskip('numpy').where(True, x, y1)
A:dask.array.tests.test_routines.w4->dask.array.where(True, x, y1)
A:dask.array.tests.test_routines.x_w->pytest.importorskip('numpy').where(x)
A:dask.array.tests.test_routines.d_w->dask.array.where(d)
A:dask.array.tests.test_routines.x_c->pytest.importorskip('numpy').count_nonzero(x)
A:dask.array.tests.test_routines.d_c->dask.array.count_nonzero(d)
A:dask.array.tests.test_routines.x_fnz->pytest.importorskip('numpy').flatnonzero(x)
A:dask.array.tests.test_routines.d_fnz->dask.array.flatnonzero(d)
A:dask.array.tests.test_routines.c->dask.array.from_array(np.ones((), np.float64), chunks=())
dask.array.tests.test_routines._maybe_len(l)
dask.array.tests.test_routines.test_apply_along_axis(func1d_name,func1d,shape,axis)
dask.array.tests.test_routines.test_apply_over_axes(func_name,func,shape,axes)
dask.array.tests.test_routines.test_argwhere()
dask.array.tests.test_routines.test_argwhere_obj()
dask.array.tests.test_routines.test_argwhere_str()
dask.array.tests.test_routines.test_array()
dask.array.tests.test_routines.test_bincount()
dask.array.tests.test_routines.test_bincount_raises_informative_error_on_missing_minlength_kwarg()
dask.array.tests.test_routines.test_bincount_with_weights()
dask.array.tests.test_routines.test_choose()
dask.array.tests.test_routines.test_coarsen()
dask.array.tests.test_routines.test_coarsen_with_excess()
dask.array.tests.test_routines.test_compress()
dask.array.tests.test_routines.test_corrcoef()
dask.array.tests.test_routines.test_count_nonzero()
dask.array.tests.test_routines.test_count_nonzero_axis(axis)
dask.array.tests.test_routines.test_count_nonzero_obj()
dask.array.tests.test_routines.test_count_nonzero_obj_axis(axis)
dask.array.tests.test_routines.test_count_nonzero_str()
dask.array.tests.test_routines.test_cov()
dask.array.tests.test_routines.test_diff(shape,n,axis)
dask.array.tests.test_routines.test_digitize()
dask.array.tests.test_routines.test_dot_method()
dask.array.tests.test_routines.test_dstack()
dask.array.tests.test_routines.test_ediff1d(shape,to_end,to_begin)
dask.array.tests.test_routines.test_extract()
dask.array.tests.test_routines.test_flatnonzero()
dask.array.tests.test_routines.test_histogram()
dask.array.tests.test_routines.test_histogram_alternative_bins_range()
dask.array.tests.test_routines.test_histogram_extra_args_and_shapes()
dask.array.tests.test_routines.test_histogram_return_type()
dask.array.tests.test_routines.test_hstack()
dask.array.tests.test_routines.test_insert()
dask.array.tests.test_routines.test_isclose()
dask.array.tests.test_routines.test_isnull()
dask.array.tests.test_routines.test_multi_insert()
dask.array.tests.test_routines.test_nonzero()
dask.array.tests.test_routines.test_nonzero_method()
dask.array.tests.test_routines.test_ptp(shape,axis)
dask.array.tests.test_routines.test_ravel()
dask.array.tests.test_routines.test_result_type()
dask.array.tests.test_routines.test_roll(chunks,shift,axis)
dask.array.tests.test_routines.test_round()
dask.array.tests.test_routines.test_squeeze()
dask.array.tests.test_routines.test_swapaxes()
dask.array.tests.test_routines.test_take()
dask.array.tests.test_routines.test_take_dask_from_numpy()
dask.array.tests.test_routines.test_tensordot()
dask.array.tests.test_routines.test_tensordot_2(axes)
dask.array.tests.test_routines.test_topk()
dask.array.tests.test_routines.test_topk_k_bigger_than_chunk()
dask.array.tests.test_routines.test_transpose()
dask.array.tests.test_routines.test_transpose_negative_axes()
dask.array.tests.test_routines.test_unique()
dask.array.tests.test_routines.test_vstack()
dask.array.tests.test_routines.test_where()
dask.array.tests.test_routines.test_where_bool_optimization()
dask.array.tests.test_routines.test_where_incorrect_args()
dask.array.tests.test_routines.test_where_nonzero()
dask.array.tests.test_routines.test_where_scalar_dtype()


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/array/tests/test_wrap.py----------------------------------------
A:dask.array.tests.test_wrap.a->dask.array.full((3, 3), 100, chunks=(2, 2), dtype='i8')
A:dask.array.tests.test_wrap.x->numpy.array(a)
dask.array.tests.test_wrap.test_can_make_really_big_array_of_ones()
dask.array.tests.test_wrap.test_full()
dask.array.tests.test_wrap.test_kwargs()
dask.array.tests.test_wrap.test_ones()
dask.array.tests.test_wrap.test_singleton_size()
dask.array.tests.test_wrap.test_size_as_list()
dask.array.tests.test_wrap.test_wrap_consistent_names()


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/array/tests/test_optimization.py----------------------------------------
A:dask.array.tests.test_optimization.result->optimize_slices(fuse(dsk, ['c', 'd', 'e'], rename_keys=False)[0])
A:dask.array.tests.test_optimization.lock1->SerializableLock()
A:dask.array.tests.test_optimization.lock2->SerializableLock()
A:dask.array.tests.test_optimization.nil->slice(None)
A:dask.array.tests.test_optimization.x->dask.array.ones(10, chunks=(5,))
A:dask.array.tests.test_optimization.y->dask.array.sum(x + 1 + 2 + 3)
A:dask.array.tests.test_optimization.dsk->z._optimize(z.dask, z._keys())
A:dask.array.tests.test_optimization.keys->list(dsk)
A:dask.array.tests.test_optimization.results->dask.get(dsk, keys)
A:dask.array.tests.test_optimization.(dependencies, dependents)->dask.core.get_deps(dsk)
A:dask.array.tests.test_optimization.dsk2->optimize(dsk, keys)
A:dask.array.tests.test_optimization.fused_key->set(dsk2).difference(['x', ('dx2', 0)]).pop()
A:dask.array.tests.test_optimization.s->str(v)
A:dask.array.tests.test_optimization.n_getters->len([v for v in dsk.values() if v[0] in (getitem, getter)])
A:dask.array.tests.test_optimization.a->dask.array.sum(x + 1 + 2 + 3)._optimize(y.dask, y._keys())
A:dask.array.tests.test_optimization.b->dask.array.sum(x + 1 + 2 + 3)._optimize(y.dask, y._keys())
dask.array.tests.test_optimization.test_dont_fuse_fancy_indexing_in_getter_nofancy()
dask.array.tests.test_optimization.test_dont_fuse_numpy_arrays()
dask.array.tests.test_optimization.test_fuse_getitem()
dask.array.tests.test_optimization.test_fuse_getitem_lock()
dask.array.tests.test_optimization.test_fuse_getter_with_asarray(chunks)
dask.array.tests.test_optimization.test_fuse_slice()
dask.array.tests.test_optimization.test_fuse_slice_with_lists()
dask.array.tests.test_optimization.test_fuse_slices_with_alias()
dask.array.tests.test_optimization.test_hard_fuse_slice_cases()
dask.array.tests.test_optimization.test_minimize_data_transfer()
dask.array.tests.test_optimization.test_nonfusible_fancy_indexing()
dask.array.tests.test_optimization.test_optimize_slicing()
dask.array.tests.test_optimization.test_optimize_with_getitem_fusion()
dask.array.tests.test_optimization.test_turn_off_fusion()


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/array/tests/test_testing.py----------------------------------------
dask.array.tests.test_testing.test_assert_eq_checks_scalars()


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/array/tests/test_linearoperator.py----------------------------------------
A:dask.array.tests.test_linearoperator.X->numpy.random.random(size=(3, 2))
A:dask.array.tests.test_linearoperator.y->numpy.random.random(size=(2, 1))
A:dask.array.tests.test_linearoperator.w->numpy.random.random(size=(3, 1))
A:dask.array.tests.test_linearoperator.square->numpy.random.random(size=(2, 2))
A:dask.array.tests.test_linearoperator.dX->dask.array.from_array(X, chunks=(2, 1))
A:dask.array.tests.test_linearoperator.npLO->scipy.sparse.linalg.aslinearoperator(X)
A:dask.array.tests.test_linearoperator.daLO->scipy.sparse.linalg.interface.MatrixLinearOperator(dX)
dask.array.tests.test_linearoperator.test_LinearOperator()


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/array/tests/test_image.py----------------------------------------
A:dask.array.tests.test_image.fn->os.path.join(dirname, 'image.%d.png' % i)
A:dask.array.tests.test_image.x->numpy.random.randint(0, 255, size=shape).astype('i1')
A:dask.array.tests.test_image.im->da_imread(globstring, preprocess=preprocess)
dask.array.tests.test_image.random_images(n,shape)
dask.array.tests.test_image.test_imread()
dask.array.tests.test_image.test_imread_with_custom_function()
dask.array.tests.test_image.test_preprocess()


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/array/tests/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/array/tests/test_masked.py----------------------------------------
A:dask.array.tests.test_masked.m->numpy.ma.masked_array([1, 2, 3], mask=[True, True, False], fill_value=10)
A:dask.array.tests.test_masked.m2->numpy.ma.masked_array([1, 2, 3], mask=[True, True, False], fill_value=0)
A:dask.array.tests.test_masked.m3->numpy.ma.masked_array([1, 2, 3], mask=False, fill_value=10)
A:dask.array.tests.test_masked.dm->dask.array.from_array(m, chunks=(2,), asarray=False)
A:dask.array.tests.test_masked.x->numpy.random.randint(0, 10, (10, 10))
A:dask.array.tests.test_masked.y->numpy.random.randint(0, 10, (10, 10)).astype('f8')
A:dask.array.tests.test_masked.xx->dask.array.ma.masked_equal(x, 0)
A:dask.array.tests.test_masked.yy->dask.array.ma.masked_equal(y, 0)
A:dask.array.tests.test_masked.zz->dask.array.concatenate([x, y], axis=1).compute()
A:dask.array.tests.test_masked.d->dask.array.random.random((4, 3, 4), chunks=(1, 2, 2))
A:dask.array.tests.test_masked.s->dask.array.random.random((4, 3, 4), chunks=(1, 2, 2)).map_blocks(fn)
A:dask.array.tests.test_masked.dd->func(d)
A:dask.array.tests.test_masked.ss->func(s)
A:dask.array.tests.test_masked.z->dask.array.concatenate([x, y], axis=1)
A:dask.array.tests.test_masked.dx->dask.array.from_array(x, chunks=(3, 4))
A:dask.array.tests.test_masked.dy->dask.array.from_array(y, chunks=5)
A:dask.array.tests.test_masked.sol->numpy.ma.masked_greater(x, y)
A:dask.array.tests.test_masked.my->numpy.ma.masked_greater(y, 0)
A:dask.array.tests.test_masked.dmy->dask.array.ma.masked_greater(dy, 0)
A:dask.array.tests.test_masked.mx->numpy.ma.masked_greater(x, 3)
A:dask.array.tests.test_masked.mdx->dask.array.ma.masked_greater(dx, 5)
A:dask.array.tests.test_masked.res->dask.array.ma.filled(a).compute()
A:dask.array.tests.test_masked.a->dask.array.ma.filled(a)
A:dask.array.tests.test_masked.b->numpy.ma.filled(b)
A:dask.array.tests.test_masked.dfunc->getattr(da, reduction)
A:dask.array.tests.test_masked.func->getattr(np, reduction)
A:dask.array.tests.test_masked.dmx->dask.array.ma.masked_greater(dx, 3)
A:dask.array.tests.test_masked.f1->dask.array.from_array(np.array(1), chunks=())
dask.array.tests.test_masked.assert_eq_ma(a,b)
dask.array.tests.test_masked.test_accessors()
dask.array.tests.test_masked.test_arg_reductions(reduction)
dask.array.tests.test_masked.test_basic(func)
dask.array.tests.test_masked.test_creation_functions()
dask.array.tests.test_masked.test_cumulative()
dask.array.tests.test_masked.test_filled()
dask.array.tests.test_masked.test_from_array_masked_array()
dask.array.tests.test_masked.test_masked_array()
dask.array.tests.test_masked.test_mixed_concatenate(func)
dask.array.tests.test_masked.test_mixed_output_type()
dask.array.tests.test_masked.test_mixed_random(func)
dask.array.tests.test_masked.test_reductions(dtype,reduction)
dask.array.tests.test_masked.test_set_fill_value()
dask.array.tests.test_masked.test_tensordot()
dask.array.tests.test_masked.test_tokenize_masked_array()


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/array/tests/test_reductions.py----------------------------------------
A:dask.array.tests.test_reductions.a1->da_func(darr, split_every=4)
A:dask.array.tests.test_reductions.a2->dask.array.from_array(x2, chunks=3)
A:dask.array.tests.test_reductions.x->dask.array.ones((10, 10), chunks=(4, 4))
A:dask.array.tests.test_reductions.a->numpy.arange(np.prod(s)).reshape(s)
A:dask.array.tests.test_reductions.b->numpy.arange(np.prod(s)).reshape(s).sum(keepdims=True)
A:dask.array.tests.test_reductions.x2->dask.array.ones((0, 0, 0), chunks=4).compute()
A:dask.array.tests.test_reductions.x[:2, :2]->numpy.array([[1, 2], [3, 4]])
A:dask.array.tests.test_reductions.d->dask.array.from_array(a, chunks=(4, 5, 6))
A:dask.array.tests.test_reductions.y->dask.array.ones((10, 10), chunks=(4, 4))
A:dask.array.tests.test_reductions.dx1->dask.array.ones((10, 0, 5), chunks=4)
A:dask.array.tests.test_reductions.x1->dask.array.ones((10, 0, 5), chunks=4).compute()
A:dask.array.tests.test_reductions.dx2->dask.array.ones((0, 0, 0), chunks=4)
A:dask.array.tests.test_reductions.(dependencies, dependents)->get_deps(x.dask)
A:dask.array.tests.test_reductions.np_func->getattr(np, func)
A:dask.array.tests.test_reductions.da_func->getattr(da, func)
A:dask.array.tests.test_reductions.a_r->np_func(a, axis=axis)
A:dask.array.tests.test_reductions.d_r->da_func(d, axis=axis)
dask.array.tests.test_reductions.assert_eq(a,b)
dask.array.tests.test_reductions.assert_max_deps(x,n,eq=True)
dask.array.tests.test_reductions.reduction_1d_test(da_func,darr,np_func,narr,use_dtype=True,split_every=True)
dask.array.tests.test_reductions.reduction_2d_test(da_func,darr,np_func,narr,use_dtype=True,split_every=True)
dask.array.tests.test_reductions.test_0d_array()
dask.array.tests.test_reductions.test_arg_reductions(dfunc,func)
dask.array.tests.test_reductions.test_array_cumreduction_axis(func,axis)
dask.array.tests.test_reductions.test_array_cumreduction_out(func)
dask.array.tests.test_reductions.test_array_reduction_out(func)
dask.array.tests.test_reductions.test_moment()
dask.array.tests.test_reductions.test_nan()
dask.array.tests.test_reductions.test_nanarg_reductions(dfunc,func)
dask.array.tests.test_reductions.test_reduction_errors()
dask.array.tests.test_reductions.test_reduction_names()
dask.array.tests.test_reductions.test_reduction_on_scalar()
dask.array.tests.test_reductions.test_reductions_1D(dtype)
dask.array.tests.test_reductions.test_reductions_2D(dtype)
dask.array.tests.test_reductions.test_reductions_2D_nans()
dask.array.tests.test_reductions.test_reductions_with_empty_array()
dask.array.tests.test_reductions.test_reductions_with_negative_axes()
dask.array.tests.test_reductions.test_tree_reduce_depth()
dask.array.tests.test_reductions.test_tree_reduce_set_options()


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/array/tests/test_slicing.py----------------------------------------
A:dask.array.tests.test_slicing.np->pytest.importorskip('numpy')
A:dask.array.tests.test_slicing.result->_slice_1d(104, [20, 23, 27, 13, 21], slice(100, 27, -3))
A:dask.array.tests.test_slicing.(result, chunks)->slice_array('y', 'x', ([5, 5], [5, 5]), (slice(0, 7), 1))
A:dask.array.tests.test_slicing.(chunks, dsk)->take('y', 'x', [(20, 20, 20, 20), (20, 20)], [1, 3, 5, 37], axis=1)
A:dask.array.tests.test_slicing.expected->merge(dict(((('y', i, 0), (getitem, ('x', i, 0), (slice(None, None, None), [1, 3, 5]))) for i in range(4))), dict(((('y', i, 1), (getitem, ('x', i, 1), (slice(None, None, None), [17]))) for i in range(4))))
A:dask.array.tests.test_slicing.(y, chunks)->slice_array('y', 'x', ((3, 3, 3, 1), (3, 3, 3, 1)), (np.array([2, 1, 9]), slice(None, None, None)))
A:dask.array.tests.test_slicing.(a, bd1)->slice_array('y', 'x', ((3, 3, 3, 1), (3, 3, 3, 1)), (np.array([1, 2, 9]), slice(None, None, None)))
A:dask.array.tests.test_slicing.(b, bd2)->slice_array('y', 'x', ((3, 3, 3, 1), (3, 3, 3, 1)), (np.array([1, 2, 9]), slice(None, None, None)))
A:dask.array.tests.test_slicing.index->normalize_index(index, (10, 10))
A:dask.array.tests.test_slicing.(c, bd3)->slice_array('y', 'x', ((3, 3, 3, 1), (3, 3, 3, 1)), index)
A:dask.array.tests.test_slicing.o->dask.array.ones((24, 16), chunks=((4, 8, 8, 4), (2, 6, 6, 2)))
A:dask.array.tests.test_slicing.a->dask.array.from_array(x, chunks=(5, 5))
A:dask.array.tests.test_slicing.x->pytest.importorskip('numpy').arange(5)
A:dask.array.tests.test_slicing.I->ReturnItem()
A:dask.array.tests.test_slicing.dx->dask.array.from_array(x, chunks=2)
A:dask.array.tests.test_slicing.pd->pytest.importorskip('pandas')
A:dask.array.tests.test_slicing.(dsk_out, bd_out)->slice_array('in', 'out', blockdims, index)
A:dask.array.tests.test_slicing.d->dask.array.from_array(x, chunks=shape)
A:dask.array.tests.test_slicing.ind->dask.array.from_array(ind, chunks=2)
A:dask.array.tests.test_slicing.dind->dask.array.from_array(ind, chunks=4)
A:dask.array.tests.test_slicing.X->dask.array.random.random((100, 2), (2, 2))
A:dask.array.tests.test_slicing.idx->pytest.importorskip('numpy').array([0, 0, 1, 1])
A:dask.array.tests.test_slicing.y->dask.array.core.asarray(x)
A:dask.array.tests.test_slicing.(result,)->normalize_index([-5, -2, 1], (np.nan,))
dask.array.tests.test_slicing.ReturnItem(object)
dask.array.tests.test_slicing.ReturnItem.__getitem__(self,key)
dask.array.tests.test_slicing.test_None_overlap_int()
dask.array.tests.test_slicing.test_cull()
dask.array.tests.test_slicing.test_empty_list()
dask.array.tests.test_slicing.test_empty_slice()
dask.array.tests.test_slicing.test_index_with_dask_array()
dask.array.tests.test_slicing.test_index_with_dask_array_2()
dask.array.tests.test_slicing.test_multiple_list_slicing()
dask.array.tests.test_slicing.test_negative_list_slicing()
dask.array.tests.test_slicing.test_negative_n_slicing()
dask.array.tests.test_slicing.test_new_blockdim()
dask.array.tests.test_slicing.test_normalize_index()
dask.array.tests.test_slicing.test_oob_check()
dask.array.tests.test_slicing.test_permit_oob_slices()
dask.array.tests.test_slicing.test_sanitize_index()
dask.array.tests.test_slicing.test_sanitize_index_element()
dask.array.tests.test_slicing.test_slice_1d()
dask.array.tests.test_slicing.test_slice_array_1d()
dask.array.tests.test_slicing.test_slice_array_2d()
dask.array.tests.test_slicing.test_slice_list_then_None()
dask.array.tests.test_slicing.test_slice_lists()
dask.array.tests.test_slicing.test_slice_optimizations()
dask.array.tests.test_slicing.test_slice_singleton_value_on_boundary()
dask.array.tests.test_slicing.test_slice_stop_0()
dask.array.tests.test_slicing.test_slicing_and_chunks()
dask.array.tests.test_slicing.test_slicing_chunks()
dask.array.tests.test_slicing.test_slicing_consistent_names()
dask.array.tests.test_slicing.test_slicing_consistent_names_after_normalization()
dask.array.tests.test_slicing.test_slicing_exhaustively()
dask.array.tests.test_slicing.test_slicing_integer_no_warnings()
dask.array.tests.test_slicing.test_slicing_none_int_ellipes()
dask.array.tests.test_slicing.test_slicing_with_Nones(shape,index)
dask.array.tests.test_slicing.test_slicing_with_negative_step_flops_keys()
dask.array.tests.test_slicing.test_slicing_with_newaxis()
dask.array.tests.test_slicing.test_slicing_with_numpy_arrays()
dask.array.tests.test_slicing.test_slicing_with_singleton_indices()
dask.array.tests.test_slicing.test_take()
dask.array.tests.test_slicing.test_take_sorted()
dask.array.tests.test_slicing.test_uneven_blockdims()
dask.array.tests.test_slicing.test_uneven_chunks()


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/diagnostics/progress.py----------------------------------------
A:dask.diagnostics.progress.(m, s)->divmod(t, 60)
A:dask.diagnostics.progress.(h, m)->divmod(m, 60)
A:dask.diagnostics.progress.self._start_time->default_timer()
A:dask.diagnostics.progress.self._timer->threading.Thread(target=self._timer_func)
A:dask.diagnostics.progress.ndone->len(s['finished'])
A:dask.diagnostics.progress.percent->int(100 * frac)
A:dask.diagnostics.progress.elapsed->format_time(elapsed)
A:dask.diagnostics.progress.msg->'\r[{0:<{1}}] | {2}% Completed | {3}'.format(bar, self._width, percent, elapsed)
dask.diagnostics.ProgressBar(self,minimum=0,width=40,dt=0.1)
dask.diagnostics.ProgressBar._draw_bar(self,frac,elapsed)
dask.diagnostics.ProgressBar._finish(self,dsk,state,errored)
dask.diagnostics.ProgressBar._pretask(self,key,dsk,state)
dask.diagnostics.ProgressBar._start(self,dsk)
dask.diagnostics.ProgressBar._timer_func(self)
dask.diagnostics.ProgressBar._update_bar(self,elapsed)
dask.diagnostics.progress.ProgressBar(self,minimum=0,width=40,dt=0.1)
dask.diagnostics.progress.ProgressBar.__init__(self,minimum=0,width=40,dt=0.1)
dask.diagnostics.progress.ProgressBar._draw_bar(self,frac,elapsed)
dask.diagnostics.progress.ProgressBar._finish(self,dsk,state,errored)
dask.diagnostics.progress.ProgressBar._pretask(self,key,dsk,state)
dask.diagnostics.progress.ProgressBar._start(self,dsk)
dask.diagnostics.progress.ProgressBar._timer_func(self)
dask.diagnostics.progress.ProgressBar._update_bar(self,elapsed)
dask.diagnostics.progress.format_time(t)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/diagnostics/profile.py----------------------------------------
A:dask.diagnostics.profile.TaskData->namedtuple('TaskData', ('key', 'task', 'start_time', 'end_time', 'worker_id'))
A:dask.diagnostics.profile.start->default_timer()
A:dask.diagnostics.profile.end->default_timer()
A:dask.diagnostics.profile.results->dict(((k, v) for (k, v) in self._results.items() if len(v) == 5))
A:dask.diagnostics.profile.ResourceData->namedtuple('ResourceData', ('time', 'mem', 'cpu'))
A:dask.diagnostics.profile.self._tracker->_Tracker(dt)
A:dask.diagnostics.profile.psutil->import_required('psutil', 'Tracking resource usage requires `psutil` to be installed')
A:dask.diagnostics.profile.self.parent->import_required('psutil', 'Tracking resource usage requires `psutil` to be installed').Process(current_process().pid)
A:dask.diagnostics.profile.(self.parent_conn, self.child_conn)->Pipe()
A:dask.diagnostics.profile.pid->current_process()
A:dask.diagnostics.profile.msg->self.child_conn.recv()
A:dask.diagnostics.profile.ps->self._update_pids(pid)
A:dask.diagnostics.profile.tic->default_timer()
A:dask.diagnostics.profile.cpu2->p.cpu_percent()
A:dask.diagnostics.profile.CacheData->namedtuple('CacheData', ('key', 'task', 'metric', 'cache_time', 'free_time'))
A:dask.diagnostics.profile.self._start_time->default_timer()
A:dask.diagnostics.profile.t->default_timer()
A:dask.diagnostics.profile.(metric, start)->self._cache.pop(k)
dask.diagnostics.CacheProfiler(self,metric=None,metric_name=None)
dask.diagnostics.CacheProfiler.__enter__(self)
dask.diagnostics.CacheProfiler._finish(self,dsk,state,failed)
dask.diagnostics.CacheProfiler._plot(self,**kwargs)
dask.diagnostics.CacheProfiler._posttask(self,key,value,dsk,state,id)
dask.diagnostics.CacheProfiler._start(self,dsk)
dask.diagnostics.CacheProfiler.clear(self)
dask.diagnostics.CacheProfiler.visualize(self,**kwargs)
dask.diagnostics.Profiler(self)
dask.diagnostics.Profiler.__enter__(self)
dask.diagnostics.Profiler._finish(self,dsk,state,failed)
dask.diagnostics.Profiler._plot(self,**kwargs)
dask.diagnostics.Profiler._posttask(self,key,value,dsk,state,id)
dask.diagnostics.Profiler._pretask(self,key,dsk,state)
dask.diagnostics.Profiler._start(self,dsk)
dask.diagnostics.Profiler.clear(self)
dask.diagnostics.Profiler.visualize(self,**kwargs)
dask.diagnostics.ResourceProfiler(self,dt=1)
dask.diagnostics.ResourceProfiler.__enter__(self)
dask.diagnostics.ResourceProfiler.__exit__(self,*args)
dask.diagnostics.ResourceProfiler._finish(self,dsk,state,failed)
dask.diagnostics.ResourceProfiler._plot(self,**kwargs)
dask.diagnostics.ResourceProfiler._start(self,dsk)
dask.diagnostics.ResourceProfiler._start_collect(self)
dask.diagnostics.ResourceProfiler._stop_collect(self)
dask.diagnostics.ResourceProfiler.clear(self)
dask.diagnostics.ResourceProfiler.close(self)
dask.diagnostics.ResourceProfiler.visualize(self,**kwargs)
dask.diagnostics.profile.CacheProfiler(self,metric=None,metric_name=None)
dask.diagnostics.profile.CacheProfiler.__enter__(self)
dask.diagnostics.profile.CacheProfiler.__init__(self,metric=None,metric_name=None)
dask.diagnostics.profile.CacheProfiler._finish(self,dsk,state,failed)
dask.diagnostics.profile.CacheProfiler._plot(self,**kwargs)
dask.diagnostics.profile.CacheProfiler._posttask(self,key,value,dsk,state,id)
dask.diagnostics.profile.CacheProfiler._start(self,dsk)
dask.diagnostics.profile.CacheProfiler.clear(self)
dask.diagnostics.profile.CacheProfiler.visualize(self,**kwargs)
dask.diagnostics.profile.Profiler(self)
dask.diagnostics.profile.Profiler.__enter__(self)
dask.diagnostics.profile.Profiler.__init__(self)
dask.diagnostics.profile.Profiler._finish(self,dsk,state,failed)
dask.diagnostics.profile.Profiler._plot(self,**kwargs)
dask.diagnostics.profile.Profiler._posttask(self,key,value,dsk,state,id)
dask.diagnostics.profile.Profiler._pretask(self,key,dsk,state)
dask.diagnostics.profile.Profiler._start(self,dsk)
dask.diagnostics.profile.Profiler.clear(self)
dask.diagnostics.profile.Profiler.visualize(self,**kwargs)
dask.diagnostics.profile.ResourceProfiler(self,dt=1)
dask.diagnostics.profile.ResourceProfiler.__enter__(self)
dask.diagnostics.profile.ResourceProfiler.__exit__(self,*args)
dask.diagnostics.profile.ResourceProfiler.__init__(self,dt=1)
dask.diagnostics.profile.ResourceProfiler._finish(self,dsk,state,failed)
dask.diagnostics.profile.ResourceProfiler._plot(self,**kwargs)
dask.diagnostics.profile.ResourceProfiler._start(self,dsk)
dask.diagnostics.profile.ResourceProfiler._start_collect(self)
dask.diagnostics.profile.ResourceProfiler._stop_collect(self)
dask.diagnostics.profile.ResourceProfiler.clear(self)
dask.diagnostics.profile.ResourceProfiler.close(self)
dask.diagnostics.profile.ResourceProfiler.visualize(self,**kwargs)
dask.diagnostics.profile._Tracker(self,dt=1)
dask.diagnostics.profile._Tracker.__init__(self,dt=1)
dask.diagnostics.profile._Tracker._update_pids(self,pid)
dask.diagnostics.profile._Tracker.run(self)
dask.diagnostics.profile._Tracker.shutdown(self)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/diagnostics/profile_visualize.py----------------------------------------
A:dask.diagnostics.profile_visualize.head->funcname(task[0])
A:dask.diagnostics.profile_visualize.label_size2->int((label_size - 2 - 2 * len(task)) // len(task))
A:dask.diagnostics.profile_visualize.args->', '.join((pprint_task(t, keys, label_size2) for t in task))
A:dask.diagnostics.profile_visualize.result->pprint_task(task[:3], keys, label_size)
A:dask.diagnostics.profile_visualize.palettes->import_required('bokeh.palettes', _BOKEH_MISSING_MSG)
A:dask.diagnostics.profile_visualize.tz->import_required('toolz', _TOOLZ_MISSING_MSG)
A:dask.diagnostics.profile_visualize.unique_funcs->list(sorted(tz.unique(funcs)))
A:dask.diagnostics.profile_visualize.n_funcs->len(unique_funcs)
A:dask.diagnostics.profile_visualize.keys->list(sorted(palette_lookup.keys()))
A:dask.diagnostics.profile_visualize.palette->list(tz.unique(palette))
A:dask.diagnostics.profile_visualize.color_lookup->dict(zip(unique_funcs, cycle(palette)))
A:dask.diagnostics.profile_visualize.bp->import_required('bokeh.plotting', _BOKEH_MISSING_MSG)
A:dask.diagnostics.profile_visualize.p->import_required('bokeh.plotting', _BOKEH_MISSING_MSG).figure(y_range=[0, 10], x_range=[0, 10], **defaults)
A:dask.diagnostics.profile_visualize.o->import_required('bokeh.plotting', _BOKEH_MISSING_MSG).Figure.properties()
A:dask.diagnostics.profile_visualize.defaults->dict(title='Profile Results', tools='hover,save,reset,wheel_zoom,xpan', plot_width=800, plot_height=300)
A:dask.diagnostics.profile_visualize.(keys, tasks, starts, ends, ids)->zip(*results)
A:dask.diagnostics.profile_visualize.id_group->import_required('toolz', _TOOLZ_MISSING_MSG).groupby(itemgetter(4), results)
A:dask.diagnostics.profile_visualize.timings->dict(((k, [i.end_time - i.start_time for i in v]) for (k, v) in id_group.items()))
A:dask.diagnostics.profile_visualize.id_lk->dict(((t[0], n) for (n, t) in enumerate(sorted(timings.items(), key=itemgetter(1), reverse=True))))
A:dask.diagnostics.profile_visualize.left->min(starts)
A:dask.diagnostics.profile_visualize.right->max(ends)
A:dask.diagnostics.profile_visualize.data['color']->get_colors(palette, funcs)
A:dask.diagnostics.profile_visualize.source->import_required('bokeh.plotting', _BOKEH_MISSING_MSG).ColumnDataSource(data=data)
A:dask.diagnostics.profile_visualize.hover->import_required('bokeh.plotting', _BOKEH_MISSING_MSG).figure(y_range=[0, 10], x_range=[0, 10], **defaults).select(HoverTool)
A:dask.diagnostics.profile_visualize.(t, mem, cpu)->zip(*results)
A:dask.diagnostics.profile_visualize.tics->list(sorted(tz.unique(starts + ends)))
A:dask.diagnostics.profile_visualize.groups->import_required('toolz', _TOOLZ_MISSING_MSG).groupby(lambda d: pprint_task(d[1], dsk, label_size), results)
A:dask.diagnostics.profile_visualize.cnts->dict.fromkeys(tics, 0)
A:dask.diagnostics.profile_visualize.p.yaxis.axis_label->'Cache Size ({0})'.format(metric_name)
dask.diagnostics.profile_visualize._get_figure_keywords()
dask.diagnostics.profile_visualize.get_colors(palette,funcs)
dask.diagnostics.profile_visualize.plot_cache(results,dsk,start_time,metric_name,palette='Viridis',label_size=60,**kwargs)
dask.diagnostics.profile_visualize.plot_resources(results,palette='Viridis',**kwargs)
dask.diagnostics.profile_visualize.plot_tasks(results,dsk,palette='Viridis',label_size=60,**kwargs)
dask.diagnostics.profile_visualize.pprint_task(task,keys,label_size=60)
dask.diagnostics.profile_visualize.unquote(expr)
dask.diagnostics.profile_visualize.visualize(profilers,file_path=None,show=True,save=True,**kwargs)
dask.diagnostics.visualize(profilers,file_path=None,show=True,save=True,**kwargs)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/diagnostics/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/diagnostics/tests/test_progress.py----------------------------------------
A:dask.diagnostics.tests.test_progress.(out, err)->capsys.readouterr()
A:dask.diagnostics.tests.test_progress.out->get_threaded(dsk, 'e')
A:dask.diagnostics.tests.test_progress.p->ProgressBar()
A:dask.diagnostics.tests.test_progress.cachey->pytest.importorskip('cachey')
A:dask.diagnostics.tests.test_progress.c->pytest.importorskip('cachey').Cache(10000)
A:dask.diagnostics.tests.test_progress.cc->Cache(c)
dask.diagnostics.tests.test_progress.check_bar_completed(capsys,width=40)
dask.diagnostics.tests.test_progress.test_clean_exit(get)
dask.diagnostics.tests.test_progress.test_format_time()
dask.diagnostics.tests.test_progress.test_minimum_time(capsys)
dask.diagnostics.tests.test_progress.test_no_tasks(capsys)
dask.diagnostics.tests.test_progress.test_progressbar(capsys)
dask.diagnostics.tests.test_progress.test_register(capsys)
dask.diagnostics.tests.test_progress.test_store_time()
dask.diagnostics.tests.test_progress.test_with_alias(capsys)
dask.diagnostics.tests.test_progress.test_with_cache(capsys)


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/diagnostics/tests/test_profiler.py----------------------------------------
A:dask.diagnostics.tests.test_profiler.prof->profiler()
A:dask.diagnostics.tests.test_profiler.out->get(dsk, 'e')
A:dask.diagnostics.tests.test_profiler.prof_data->sorted(prof.results, key=lambda d: d.key)
A:dask.diagnostics.tests.test_profiler.n->len(prof.results)
A:dask.diagnostics.tests.test_profiler.m->len(prof.results)
A:dask.diagnostics.tests.test_profiler.keys->set(['a', 'b', 'c', 'd', 'e'])
A:dask.diagnostics.tests.test_profiler.p->visualize([prof, rprof], label_size=50, title='Not the default', show=False, save=False)
A:dask.diagnostics.tests.test_profiler.funcs->list(range(300))
A:dask.diagnostics.tests.test_profiler.cmap->get_colors('Viridis', funcs)
A:dask.diagnostics.tests.test_profiler.lk->dict(zip(funcs, Blues5))
dask.diagnostics.tests.test_profiler.check_title(p,title)
dask.diagnostics.tests.test_profiler.test_cache_profiler()
dask.diagnostics.tests.test_profiler.test_cache_profiler_plot()
dask.diagnostics.tests.test_profiler.test_get_colors()
dask.diagnostics.tests.test_profiler.test_plot_multiple()
dask.diagnostics.tests.test_profiler.test_pprint_task()
dask.diagnostics.tests.test_profiler.test_profiler()
dask.diagnostics.tests.test_profiler.test_profiler_plot()
dask.diagnostics.tests.test_profiler.test_profiler_works_under_error()
dask.diagnostics.tests.test_profiler.test_register(profiler)
dask.diagnostics.tests.test_profiler.test_resource_profiler()
dask.diagnostics.tests.test_profiler.test_resource_profiler_multiple_gets()
dask.diagnostics.tests.test_profiler.test_resource_profiler_plot()
dask.diagnostics.tests.test_profiler.test_saves_file()
dask.diagnostics.tests.test_profiler.test_two_gets()
dask.diagnostics.tests.test_profiler.test_unquote()


----------------------------------------/home/zhang/Packages/dask/dask0.15.3/diagnostics/tests/__init__.py----------------------------------------

