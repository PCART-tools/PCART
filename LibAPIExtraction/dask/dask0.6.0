
----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.6.0/lib/python3.6/site-packages/dask/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.6.0/lib/python3.6/site-packages/dask/compatibility.py----------------------------------------
dask.compatibility.skip(func)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.6.0/lib/python3.6/site-packages/dask/optimize.py----------------------------------------
A:dask.optimize.nxt->set()
A:dask.optimize.keys->set(flatten(keys))
A:dask.optimize.unfusible->set()
A:dask.optimize.deps->tuple(get_dependencies(dsk2, key2, True))
A:dask.optimize.parent2child->dict(map(reversed, child2parent.items()))
A:dask.optimize.(child, parent)->child2parent.popitem()
A:dask.optimize.parent->chain.pop()
A:dask.optimize.child->chain.pop()
A:dask.optimize.fused->set()
A:dask.optimize.val->subs(val, item, keysubs[item])
A:dask.optimize.replaceorder->toposort(dict(((k, dsk[k]) for k in keys if k in dsk)))
A:dask.optimize.fast_functions->set(fast_functions)
A:dask.optimize.dependencies->dict(((k, get_dependencies(dsk2, k)) for k in dsk2))
A:dask.optimize.dependents->reverse_dict(dependencies)
A:dask.optimize.result->set()
A:dask.optimize.aliases->set((k for (k, task) in dsk.items() if ishashable(task) and task in dsk))
A:dask.optimize.roots->set((k for (k, v) in dependents.items() if not v))
A:dask.optimize.dsk2->inline(dsk, aliases - roots, inline_constants=False)
A:dask.optimize.dsk3->inline(dsk, aliases - roots, inline_constants=False).copy()
A:dask.optimize.head_type->type(term1)
A:dask.optimize.pot1->preorder_traversal(term1)
A:dask.optimize.pot2->preorder_traversal(term2)
A:dask.optimize.v->core.subs.get(d, None)
A:dask.optimize.deps2->tuple(deps2)
A:dask.optimize.dep_dict1->dependency_dict(dsk1)
A:dask.optimize.possible_matches->_possible_matches(dep_dict1, deps, subs)
A:dask.optimize.dsk2_topo->toposort(dsk2)
A:dask.optimize.sd->_sync_keys(dsk1, dsk2, dsk2_topo)
A:dask.optimize.new_dsk->dsk1.copy()
A:dask.optimize.new_key->next(merge_sync.names)
A:dask.optimize.task->subs(task, a, b)
dask.optimize._possible_matches(dep_dict,deps,subs)
dask.optimize._sync_keys(dsk1,dsk2,dsk2_topo)
dask.optimize.cull(dsk,keys)
dask.optimize.dealias(dsk)
dask.optimize.dependency_dict(dsk)
dask.optimize.equivalent(term1,term2,subs=None)
dask.optimize.functions_of(task)
dask.optimize.fuse(dsk,keys=None)
dask.optimize.inline(dsk,keys=None,inline_constants=True)
dask.optimize.inline_functions(dsk,fast_functions=None,inline_constants=False)
dask.optimize.merge_sync(dsk1,dsk2)
dask.optimize.sync_keys(dsk1,dsk2)
dask.optimize.unwrap_partial(func)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.6.0/lib/python3.6/site-packages/dask/core.py----------------------------------------
A:dask.core.val->func(*results)
A:dask.core.cycle->'->'.join(cycle)
A:dask.core.key->args.pop()
A:dask.core.v->list(v)
A:dask.core.arg->subs(arg, key, val)
A:dask.core.result->dict(((t, set()) for t in terms))
A:dask.core.completed->set()
A:dask.core.seen->set()
dask.core._deps(dsk,arg)
dask.core._get_task(d,task,maxdepth=1000)
dask.core._toposort(dsk,keys=None,returncycle=False)
dask.core.flatten(seq)
dask.core.get(d,key,get=None,concrete=True,**kwargs)
dask.core.get_dependencies(dsk,task,as_list=False)
dask.core.getcycle(d,keys)
dask.core.inc(x)
dask.core.isdag(d,keys)
dask.core.ishashable(x)
dask.core.istask(x)
dask.core.preorder_traversal(task)
dask.core.reverse_dict(d)
dask.core.subs(task,key,val)
dask.core.toposort(dsk)
dask.get(d,key,get=None,concrete=True,**kwargs)
dask.get_dependencies(dsk,task,as_list=False)
dask.getcycle(d,keys)
dask.istask(x)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.6.0/lib/python3.6/site-packages/dask/utils.py----------------------------------------
A:dask.utils.(handle, filename)->tempfile.mkstemp(extension)
A:dask.utils.f->myopen(file, 'rb')
A:dask.utils.myopen->opens.get(compression, open)
A:dask.utils.result->textblock(f, start, stop)
A:dask.utils.line->file.readline()
A:dask.utils.start->file.tell()
A:dask.utils.stop->file.tell()
A:dask.utils.seq->list(map(concrete, seq))
A:dask.utils.p->list(p)
A:dask.utils.cp->numpy.cumsum([0] + p)
A:dask.utils.x->numpy.random.RandomState(key).random_sample(n)
A:dask.utils.out->numpy.empty(n, dtype='i1')
dask.utils.IndexCallable(self,fn)
dask.utils.IndexCallable.__getitem__(self,key)
dask.utils.IndexCallable.__init__(self,fn)
dask.utils.concrete(seq)
dask.utils.deepmap(func,*seqs)
dask.utils.filetext(text,extension='',open=open,mode='w')
dask.utils.filetexts(d,open=open)
dask.utils.getargspec(func)
dask.utils.ignoring(*exceptions)
dask.utils.is_integer(i)
dask.utils.pseudorandom(n,p,key)
dask.utils.raises(err,lamda)
dask.utils.repr_long_list(seq)
dask.utils.skip(func)
dask.utils.textblock(file,start,stop,compression=None)
dask.utils.tmpfile(extension='')


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.6.0/lib/python3.6/site-packages/dask/context.py----------------------------------------
A:dask.context._globals->defaultdict(lambda : None)
A:dask.context.self.old->defaultdict(lambda : None).copy()
dask.context.set_options(self,**kwargs)
dask.context.set_options.__enter__(self)
dask.context.set_options.__exit__(self,type,value,traceback)
dask.context.set_options.__init__(self,**kwargs)
dask.set_options(self,**kwargs)
dask.set_options.__enter__(self)
dask.set_options.__exit__(self,type,value,traceback)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.6.0/lib/python3.6/site-packages/dask/dot.py----------------------------------------
A:dask.dot.data_attributes->dict()
A:dask.dot.function_attributes->dict()
A:dask.dot.g->networkx.DiGraph()
A:dask.dot.func_node->make_hashable((v, 'function'))
A:dask.dot.arg2->make_hashable(dep)
A:dask.dot.v_hash->make_hashable(v)
A:dask.dot.p->networkx.to_pydot(dg)
A:dask.dot.dg->to_networkx(d, **kwargs)
dask.dot.dot_graph(d,filename='mydask',**kwargs)
dask.dot.lower(func)
dask.dot.make_hashable(x)
dask.dot.name(func)
dask.dot.to_networkx(d,data_attributes=None,function_attributes=None)
dask.dot.write_networkx_to_dot(dg,filename='mydask')


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.6.0/lib/python3.6/site-packages/dask/async.py----------------------------------------
A:dask.async.cache->dict()
A:dask.async.data_keys->set()
A:dask.async.dependencies->dict(((k, get_dependencies(dsk, k)) for k in dsk))
A:dask.async.waiting->dict(((k, v) for (k, v) in waiting.items() if v))
A:dask.async.dependents->reverse_dict(dependencies)
A:dask.async.waiting_data->dict(((k, v.copy()) for (k, v) in dependents.items() if v))
A:dask.async.ready_set->set([k for (k, v) in waiting.items() if not v])
A:dask.async.ready->sorted(ready_set, key=sortkey)
A:dask.async.result->_execute_task(task, data)
A:dask.async.id->get_id()
A:dask.async.(exc_type, exc_value, exc_traceback)->sys.exc_info()
A:dask.async.tb->''.join(traceback.format_tb(exc_traceback))
A:dask.async.result_flat->set([result])
A:dask.async.results->set(result_flat)
A:dask.async.state->start_state_from_dask(dsk, cache=cache)
A:dask.async.key->state['ready'].pop()
A:dask.async.data->dict(((dep, state['cache'][dep]) for dep in get_dependencies(dsk, key)))
A:dask.async.(key, res, tb, worker_id)->Queue().get()
A:dask.async.queue->Queue()
A:dask.async.g->state_to_networkx(dsk, state)
A:dask.async.(data, func)->color_nodes(dsk, state)
dask.async._execute_task(arg,cache,dsk=None)
dask.async.apply_sync(func,args=(),kwds={})
dask.async.color_nodes(dsk,state)
dask.async.default_get_id()
dask.async.double(x)
dask.async.execute_task(key,task,data,queue,get_id,raise_on_exception=False)
dask.async.finish_task(dsk,key,state,results,delete=True,release_data=release_data)
dask.async.get_async(apply_async,num_workers,dsk,result,cache=None,queue=None,get_id=default_get_id,raise_on_exception=False,start_callback=None,end_callback=None,**kwargs)
dask.async.get_sync(dsk,keys,**kwargs)
dask.async.inc(x)
dask.async.nested_get(ind,coll,lazy=False)
dask.async.release_data(key,state,delete=True)
dask.async.sortkey(item)
dask.async.start_state_from_dask(dsk,cache=None)
dask.async.state_to_networkx(dsk,state)
dask.async.visualize(dsk,state,filename='dask')


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.6.0/lib/python3.6/site-packages/dask/rewrite.py----------------------------------------
A:dask.rewrite.self._stack->deque([END])
A:dask.rewrite.subterms->args(self.term)
A:dask.rewrite.self.term->self._stack.pop()
A:dask.rewrite.VAR->Token('?')
A:dask.rewrite.END->Token('end')
A:dask.rewrite.self.vars->tuple(sorted(set(self._varlist)))
A:dask.rewrite.term->rule.subs(sd)
A:dask.rewrite.self._net->Node()
A:dask.rewrite.ind->len(self.rules)
A:dask.rewrite.curr_node.edges[t]->Node()
A:dask.rewrite.S->Traverser(term)
A:dask.rewrite.subs->_process_match(rule, syms)
A:dask.rewrite.stack->deque()
A:dask.rewrite.n->N.edges.get(VAR, None)
A:dask.rewrite.(S, N, matches)->deque().pop()
dask.rewrite.Node(cls,edges=None,patterns=None)
dask.rewrite.Node.__new__(cls,edges=None,patterns=None)
dask.rewrite.Node.edges(self)
dask.rewrite.Node.patterns(self)
dask.rewrite.RewriteRule(self,lhs,rhs,vars=())
dask.rewrite.RewriteRule.__init__(self,lhs,rhs,vars=())
dask.rewrite.RewriteRule.__repr__(self)
dask.rewrite.RewriteRule.__str__(self)
dask.rewrite.RewriteRule._apply(self,sub_dict)
dask.rewrite.RuleSet(self,*rules)
dask.rewrite.RuleSet.__init__(self,*rules)
dask.rewrite.RuleSet._rewrite(self,term)
dask.rewrite.RuleSet.add(self,rule)
dask.rewrite.RuleSet.iter_matches(self,term)
dask.rewrite.RuleSet.rewrite(self,task,strategy='bottom_up')
dask.rewrite.Token(self,name)
dask.rewrite.Token.__init__(self,name)
dask.rewrite.Token.__repr__(self)
dask.rewrite.Traverser(self,term,stack=None)
dask.rewrite.Traverser.__init__(self,term,stack=None)
dask.rewrite.Traverser.__iter__(self)
dask.rewrite.Traverser.copy(self)
dask.rewrite.Traverser.current(self)
dask.rewrite.Traverser.next(self)
dask.rewrite.Traverser.skip(self)
dask.rewrite._bottom_up(net,term)
dask.rewrite._match(S,N)
dask.rewrite._process_match(rule,syms)
dask.rewrite._top_level(net,term)
dask.rewrite.args(task)
dask.rewrite.head(task)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.6.0/lib/python3.6/site-packages/dask/hdfs_utils.py----------------------------------------
A:dask.hdfs_utils.path->path.strip('/').strip('/')
dask.hdfs_utils.filenames(hdfs,path)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.6.0/lib/python3.6/site-packages/dask/array/chunk.py----------------------------------------
A:dask.array.chunk.r->a_callable(x, *args, axis=axis, **kwargs)
A:dask.array.chunk.axes->range(x.ndim)
A:dask.array.chunk.r_slice->tuple()
A:dask.array.chunk.sum->keepdims_wrapper(np.sum)
A:dask.array.chunk.prod->keepdims_wrapper(np.prod)
A:dask.array.chunk.min->keepdims_wrapper(np.min)
A:dask.array.chunk.max->keepdims_wrapper(np.max)
A:dask.array.chunk.argmin->keepdims_wrapper(np.argmin)
A:dask.array.chunk.nanargmin->keepdims_wrapper(np.nanargmin)
A:dask.array.chunk.argmax->keepdims_wrapper(np.argmax)
A:dask.array.chunk.nanargmax->keepdims_wrapper(np.nanargmax)
A:dask.array.chunk.any->keepdims_wrapper(np.any)
A:dask.array.chunk.all->keepdims_wrapper(np.all)
A:dask.array.chunk.nansum->keepdims_wrapper(np.nansum)
A:dask.array.chunk.nanprod->keepdims_wrapper(np.nanprod)
A:dask.array.chunk.nanmin->keepdims_wrapper(np.nanmin)
A:dask.array.chunk.nanmax->keepdims_wrapper(np.nanmax)
A:dask.array.chunk.mean->keepdims_wrapper(np.mean)
A:dask.array.chunk.nanmean->keepdims_wrapper(np.nanmean)
A:dask.array.chunk.var->keepdims_wrapper(np.var)
A:dask.array.chunk.nanvar->keepdims_wrapper(np.nanvar)
A:dask.array.chunk.std->keepdims_wrapper(np.std)
A:dask.array.chunk.nanstd->keepdims_wrapper(np.nanstd)
A:dask.array.chunk.newshape->tuple(concat([(x.shape[i] / axes[i], axes[i]) for i in range(x.ndim)]))
A:dask.array.chunk.new_array->new_array.view(type=type(original_array)).view(type=type(original_array))
A:dask.array.chunk.array->numpy.array(array, copy=False, subok=subok)
A:dask.array.chunk.result->_maybe_view_as_subclass(array, broadcast)
dask.array.chunk.coarsen(reduction,x,axes)
dask.array.chunk.keepdims_wrapper(a_callable)
dask.array.chunk.topk(k,x)
dask.array.chunk.trim(x,axes=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.6.0/lib/python3.6/site-packages/dask/array/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.6.0/lib/python3.6/site-packages/dask/array/rechunk.py----------------------------------------
A:dask.array.rechunk.old_chunks->normalize_chunks(old_chunks, shape)
A:dask.array.rechunk.new_chunks->list(old_chunks)
A:dask.array.rechunk.cmo->cumdims_label(old_chunks, 'o')
A:dask.array.rechunk.cmn->cumdims_label(new_chunks, 'n')
A:dask.array.rechunk.old_to_new->tuple((_intersect_1d(_breakpoints(cm[0], cm[1])) for cm in zip(cmo, cmn)))
A:dask.array.rechunk.cross1->tuple(product(*old_to_new))
A:dask.array.rechunk.cross->tuple(chain((tuple(product(*cr)) for cr in cross1)))
A:dask.array.rechunk.newlist->list(old)
A:dask.array.rechunk.shape->tuple(map(sum, old_chunks))
A:dask.array.rechunk.chunks->normalize_chunks(chunks, x.shape)
A:dask.array.rechunk.crossed->intersect_chunks(x.chunks, chunks)
A:dask.array.rechunk.x2->merge(x.dask, x2)
A:dask.array.rechunk.temp_name->next(rechunk_names)
A:dask.array.rechunk.new_index->tuple(product(*(tuple(range(len(n))) for n in chunks)))
A:dask.array.rechunk.cr2->iter(cross1)
A:dask.array.rechunk.old_blocks->tuple((tuple((ind for (ind, _) in cr)) for cr in cross1))
A:dask.array.rechunk.subdims->tuple((len(set((ss[i] for ss in old_blocks))) for i in range(x.ndim)))
A:dask.array.rechunk.rec_cat_arg->numpy.empty(subdims).tolist()
A:dask.array.rechunk.inds_in_block->product(*(range(s) for s in subdims))
A:dask.array.rechunk.ind_slics->next(cr2)
A:dask.array.rechunk.old_inds->tuple((tuple((s[0] for s in ind_slics)) for i in range(x.ndim)))
A:dask.array.rechunk.slic->tuple((tuple((s[1] for s in ind_slics)) for i in range(x.ndim)))
A:dask.array.rechunk.ind_in_blk->next(inds_in_block)
A:dask.array.rechunk.temp->getitem(temp, ind_in_blk[i])
dask.array.rechunk(x,chunks)
dask.array.rechunk._breakpoints(cumold,cumnew)
dask.array.rechunk._intersect_1d(breaks)
dask.array.rechunk.blockdims_dict_to_tuple(old,new)
dask.array.rechunk.blockshape_dict_to_tuple(old_chunks,d)
dask.array.rechunk.cumdims_label(chunks,const)
dask.array.rechunk.intersect_chunks(old_chunks=None,new_chunks=None,shape=None)
dask.array.rechunk.rechunk(x,chunks)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.6.0/lib/python3.6/site-packages/dask/array/ghost.py----------------------------------------
A:dask.array.ghost.depth->dict(zip(range(x.ndim), depth))
A:dask.array.ghost.index->tuple(index)
A:dask.array.ghost.seq->list(product([k[0]], *[inds(i, ind) for (i, ind) in enumerate(k[1:])]))
A:dask.array.ghost.n->int(len(seq) / shape[0])
A:dask.array.ghost.dims->list(map(len, x.chunks))
A:dask.array.ghost.expand_key2->partial(expand_key, dims=dims)
A:dask.array.ghost.interior_keys->pipe(x._keys(), flatten, map(expand_key2), map(flatten), concat, list)
A:dask.array.ghost.name->next(ghost_names)
A:dask.array.ghost.frac_slice->fractional_slice(k, axes)
A:dask.array.ghost.chunks->list(x.chunks)
A:dask.array.ghost.(l, r)->_remove_ghost_boundaries(l, r, axis, depth)
A:dask.array.ghost.l->l.rechunk(tuple(lchunks)).rechunk(tuple(lchunks))
A:dask.array.ghost.r->r.rechunk(tuple(rchunks)).rechunk(tuple(rchunks))
A:dask.array.ghost.c->wrap.full(tuple(map(sum, chunks)), value, chunks=tuple(chunks), dtype=x._dtype)
A:dask.array.ghost.lchunks->list(l.chunks)
A:dask.array.ghost.rchunks->list(r.chunks)
A:dask.array.ghost.kind->dict(((i, kind) for i in range(x.ndim)))
A:dask.array.ghost.d->dict(zip(range(x.ndim), depth)).get(i, 0)
A:dask.array.ghost.x->constant(x, i, d, kind[i])
A:dask.array.ghost.boundary->dict(zip(range(x.ndim), boundary))
A:dask.array.ghost.x2->boundaries(x, depth, boundary)
A:dask.array.ghost.x3->ghost_internal(x2, depth)
A:dask.array.ghost.trim->dict(((k, v * 2 if boundary.get(k, None) is not None else 0) for (k, v) in depth.items()))
A:dask.array.ghost.x4->chunk.trim(x3, trim)
A:dask.array.ghost.g->ghost(x, depth=depth, boundary=boundary)
A:dask.array.ghost.g2->ghost(x, depth=depth, boundary=boundary).map_blocks(func, **kwargs)
dask.array.ghost._remove_ghost_boundaries(l,r,axis,depth)
dask.array.ghost.boundaries(x,depth=None,kind=None)
dask.array.ghost.constant(x,axis,depth,value)
dask.array.ghost.expand_key(k,dims)
dask.array.ghost.fractional_slice(task,axes)
dask.array.ghost.ghost(x,depth,boundary)
dask.array.ghost.ghost_internal(x,axes)
dask.array.ghost.map_overlap(x,func,depth,boundary=None,trim=True,**kwargs)
dask.array.ghost.nearest(x,axis,depth)
dask.array.ghost.periodic(x,axis,depth)
dask.array.ghost.reflect(x,axis,depth)
dask.array.ghost.reshape(shape,seq)
dask.array.ghost.trim_internal(x,axes)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.6.0/lib/python3.6/site-packages/dask/array/creation.py----------------------------------------
A:dask.array.creation.remainder->tuple()
A:dask.array.creation.num->int(abs(range_ // step))
A:dask.array.creation.chunks->normalize_chunks(chunks, (num,))
A:dask.array.creation.name->next(diag_names)
A:dask.array.creation.dtype->kwargs.get('dtype', None)
A:dask.array.creation.blocks->v._keys()
A:dask.array.creation.dsk->v.dask.copy()
dask.array.arange(*args,**kwargs)
dask.array.creation._get_blocksizes(num,blocksize)
dask.array.creation.arange(*args,**kwargs)
dask.array.creation.diag(v)
dask.array.creation.linspace(start,stop,num=50,chunks=None,dtype=None)
dask.array.diag(v)
dask.array.linspace(start,stop,num=50,chunks=None,dtype=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.6.0/lib/python3.6/site-packages/dask/array/core.py----------------------------------------
A:dask.array.core.c->numpy.asarray(c)
A:dask.array.core.shapes->list(product(*chunks))
A:dask.array.core.starts->product(*cumdims)
A:dask.array.core.chunks->chunks_from_arrays(arrays)
A:dask.array.core.keys->list(product([name], *[range(len(bd)) for bd in chunks]))
A:dask.array.core.A->map(leftfunc, A)
A:dask.array.core.B->map(rightfunc, B)
A:dask.array.core.L->Array(merge(tmp.dask, ldsk), left, chunks=tmp.chunks, dtype=ldt)
A:dask.array.core.g->dict(((k, set([d for (i, d) in v])) for (k, v) in g.items()))
A:dask.array.core.g2->dict(((k, v - set(sentinels) if len(v) > 1 else v) for (k, v) in g.items()))
A:dask.array.core.argpairs->list(partition(2, arrind_pairs))
A:dask.array.core.all_indices->pipe(argpairs, pluck(1), concat, set)
A:dask.array.core.dims->broadcast_dimensions(nameinds, shapes)
A:dask.array.core.keytups->list(product(*[range(dims[i]) for i in out_indices]))
A:dask.array.core.dummies->dict(((i, list(range(dims[i]))) for i in dummy_indices))
A:dask.array.core.tups->lol_tuples((arg,), ind, kd, dummies)
A:dask.array.core.tups2->zero_broadcast_dimensions(tups, numblocks[arg])
A:dask.array.core.arrays->concrete(arrays)
A:dask.array.core.dtype->getattr(values, 'dtype', type(values))
A:dask.array.core.args->list(concat(zip(arrs, inds)))
A:dask.array.core.result->numpy.empty(shape=shape, dtype=deepfirst(arrays).dtype)
A:dask.array.core.spec->inspect.getargspec(func)
A:dask.array.core.axis->tuple((i for (i, d) in enumerate(a.shape) if d == 1))
A:dask.array.core.b->numpy.empty((1,), dtype=x._dtype).map_blocks(partial(np.squeeze, axis=axis), dtype=a.dtype)
A:dask.array.core.old_keys->list(product([b.name], *[range(len(bd)) for bd in b.chunks]))
A:dask.array.core.new_keys->list(product([b.name], *[range(len(bd)) for bd in chunks]))
A:dask.array.core.dsk->dict((((name, i), (np.bincount, (x.name, i), None, minlength)) for (i, _) in enumerate(x._keys())))
A:dask.array.core.name->next(names)
A:dask.array.core.name2->next(names)
A:dask.array.core.results->get(dsk, keys, **kwargs)
A:dask.array.core.results2->tuple((concatenate3(x) if arg.shape else unpack_singleton(x) for (x, arg) in zip(results, args)))
A:dask.array.core.shape->tuple(map(sum, chunks))
A:dask.array.core.self.chunks->normalize_chunks(chunks, shape)
A:dask.array.core.ind->len(args)
A:dask.array.core.x->numpy.array(x)
A:dask.array.core.kwargs['chunks']->tuple([c[0] for c in self.chunks])
A:dask.array.core.d->partial(elemwise, func, **kwargs).require_dataset(datapath, shape=self.shape, dtype=self.dtype, **kwargs)
A:dask.array.core.slices->slices_from_chunks(arr.chunks)
A:dask.array.core.myget->kwargs.get('get', get)
A:dask.array.core.(result,)->compute(self, **kwargs)
A:dask.array.core.store->dict()
A:dask.array.core.dsk2->merge(dsk, *[a.dask for a in seq])
A:dask.array.core.dt->numpy.promote_types(lhs._dtype, rhs._dtype)
A:dask.array.core.out->next(names)
A:dask.array.core.(dsk, chunks)->slice_array(out, self.name, self.chunks, index)
A:dask.array.core.lock->Lock()
A:dask.array.core.arginds->list(partition(2, args))
A:dask.array.core.numblocks->dict([(a.name, a.numblocks) for (a, ind) in arginds])
A:dask.array.core.argindsstr->list(concat([(a.name, ind) for (a, ind) in arginds]))
A:dask.array.core.blockdim_dict->dict(((a.name, a.chunks) for (a, _) in arginds))
A:dask.array.core.chunkss->broadcast_dimensions(nameinds, blockdim_dict)
A:dask.array.core.n->len(seq)
A:dask.array.core.ndim->ndimlist(arrays)
A:dask.array.core.ALPHABET->alphabet.upper()
A:dask.array.core.left_axes->tuple(left_axes)
A:dask.array.core.right_axes->tuple(right_axes)
A:dask.array.core.left_index->list(alphabet[:lhs.ndim])
A:dask.array.core.right_index->list(ALPHABET[:rhs.ndim])
A:dask.array.core.func->partial(numpy_compat.isclose, rtol=rtol, atol=atol, equal_nan=equal_nan)
A:dask.array.core.out[index]->numpy.asanyarray(x)
A:dask.array.core.args2->list(map(add, args, offset))
A:dask.array.core.out_ndim->max((len(arg.shape) if isinstance(arg, Array) else 0 for arg in args))
A:dask.array.core.op2->partial_by_order(op, other)
A:dask.array.core.f->partial(elemwise, func, **kwargs)
A:dask.array.core.logaddexp->wrap_elemwise(np.logaddexp)
A:dask.array.core.logaddexp2->wrap_elemwise(np.logaddexp2)
A:dask.array.core.conj->wrap_elemwise(np.conj)
A:dask.array.core.exp->wrap_elemwise(np.exp)
A:dask.array.core.log->wrap_elemwise(np.log)
A:dask.array.core.log2->wrap_elemwise(np.log2)
A:dask.array.core.log10->wrap_elemwise(np.log10)
A:dask.array.core.log1p->wrap_elemwise(np.log1p)
A:dask.array.core.expm1->wrap_elemwise(np.expm1)
A:dask.array.core.sqrt->wrap_elemwise(np.sqrt)
A:dask.array.core.square->wrap_elemwise(np.square)
A:dask.array.core.sin->wrap_elemwise(np.sin)
A:dask.array.core.cos->wrap_elemwise(np.cos)
A:dask.array.core.tan->wrap_elemwise(np.tan)
A:dask.array.core.arcsin->wrap_elemwise(np.arcsin)
A:dask.array.core.arccos->wrap_elemwise(np.arccos)
A:dask.array.core.arctan->wrap_elemwise(np.arctan)
A:dask.array.core.arctan2->wrap_elemwise(np.arctan2)
A:dask.array.core.hypot->wrap_elemwise(np.hypot)
A:dask.array.core.sinh->wrap_elemwise(np.sinh)
A:dask.array.core.cosh->wrap_elemwise(np.cosh)
A:dask.array.core.tanh->wrap_elemwise(np.tanh)
A:dask.array.core.arcsinh->wrap_elemwise(np.arcsinh)
A:dask.array.core.arccosh->wrap_elemwise(np.arccosh)
A:dask.array.core.arctanh->wrap_elemwise(np.arctanh)
A:dask.array.core.deg2rad->wrap_elemwise(np.deg2rad)
A:dask.array.core.rad2deg->wrap_elemwise(np.rad2deg)
A:dask.array.core.logical_and->wrap_elemwise(np.logical_and, dtype='bool')
A:dask.array.core.logical_or->wrap_elemwise(np.logical_or, dtype='bool')
A:dask.array.core.logical_xor->wrap_elemwise(np.logical_xor, dtype='bool')
A:dask.array.core.logical_not->wrap_elemwise(np.logical_not, dtype='bool')
A:dask.array.core.maximum->wrap_elemwise(np.maximum)
A:dask.array.core.minimum->wrap_elemwise(np.minimum)
A:dask.array.core.fmax->wrap_elemwise(np.fmax)
A:dask.array.core.fmin->wrap_elemwise(np.fmin)
A:dask.array.core.isreal->wrap_elemwise(np.isreal, dtype='bool')
A:dask.array.core.iscomplex->wrap_elemwise(np.iscomplex, dtype='bool')
A:dask.array.core.isfinite->wrap_elemwise(np.isfinite, dtype='bool')
A:dask.array.core.isinf->wrap_elemwise(np.isinf, dtype='bool')
A:dask.array.core.isnan->wrap_elemwise(np.isnan, dtype='bool')
A:dask.array.core.signbit->wrap_elemwise(np.signbit, dtype='bool')
A:dask.array.core.copysign->wrap_elemwise(np.copysign)
A:dask.array.core.nextafter->wrap_elemwise(np.nextafter)
A:dask.array.core.ldexp->wrap_elemwise(np.ldexp)
A:dask.array.core.fmod->wrap_elemwise(np.fmod)
A:dask.array.core.floor->wrap_elemwise(np.floor)
A:dask.array.core.ceil->wrap_elemwise(np.ceil)
A:dask.array.core.trunc->wrap_elemwise(np.trunc)
A:dask.array.core.degrees->wrap_elemwise(np.degrees)
A:dask.array.core.radians->wrap_elemwise(np.radians)
A:dask.array.core.rint->wrap_elemwise(np.rint)
A:dask.array.core.fix->wrap_elemwise(np.fix)
A:dask.array.core.angle->wrap_elemwise(np.angle)
A:dask.array.core.real->wrap_elemwise(np.real)
A:dask.array.core.imag->wrap_elemwise(np.imag)
A:dask.array.core.clip->wrap_elemwise(np.clip)
A:dask.array.core.fabs->wrap_elemwise(np.fabs)
A:dask.array.core.sign->wrap_elemwise(np.fabs)
A:dask.array.core.tmp->elemwise(np.modf, x)
A:dask.array.core.left->next(names)
A:dask.array.core.right->next(names)
A:dask.array.core.ldsk->dict((((left,) + key[1:], (getitem, key, 0)) for key in core.flatten(tmp._keys())))
A:dask.array.core.rdsk->dict((((right,) + key[1:], (getitem, key, 1)) for key in core.flatten(tmp._keys())))
A:dask.array.core.a->numpy.empty((1,), dtype=x._dtype)
A:dask.array.core.(l, r)->numpy.modf(a)
A:dask.array.core.R->Array(merge(tmp.dask, rdsk), right, chunks=tmp.chunks, dtype=rdt)
A:dask.array.core.where_error_message->'\nThe dask.array version of where only handles the three argument case.\n\n    da.where(x > 0, x, 0)\n\nand not the single argument case\n\n    da.where(x > 0)\n\nThis is because dask.array operations must be able to infer the shape of their\noutputs prior to execution.  The number of positive elements of x requires\nexecution.  See the ``np.where`` docstring for examples and the following link\nfor a more thorough explanation:\n\n    http://dask.pydata.org/en/latest/array-overview.html#construct\n'.strip()
A:dask.array.core.reduction->getattr(np, reduction.__name__)
A:dask.array.core.padded_breaks->concat([[None], breaks, [None]])
A:dask.array.core.obj->numpy.where(obj < 0, obj + arr.shape[axis], obj)
A:dask.array.core.split_arr->split_at_breaks(arr, np.unique(obj), axis)
A:dask.array.core.values->values.rechunk(values_chunks).rechunk(values_chunks)
A:dask.array.core.values_shape->tuple((len(obj) if axis == n else s for (n, s) in enumerate(arr.shape)))
A:dask.array.core.values_chunks->tuple((values_bd if axis == n else arr_bd for (n, (arr_bd, values_bd)) in enumerate(zip(arr.chunks, values.chunks))))
A:dask.array.core.values_breaks->numpy.cumsum(counts[counts > 0])
A:dask.array.core.split_values->split_at_breaks(values, values_breaks, axis)
A:dask.array.core.interleaved->list(interleave([split_arr, split_values]))
A:dask.array.core.offsets->list(product(*aggdims))
A:dask.array.core.parts->get(merge(dsk, x.dask), list(dsk.keys()))
dask.array.Array(self,dask,name,chunks,dtype=None,shape=None)
dask.array.Array.T(self)
dask.array.Array.__abs__(self)
dask.array.Array.__add__(self,other)
dask.array.Array.__and__(self,other)
dask.array.Array.__array__(self,dtype=None,**kwargs)
dask.array.Array.__bool__(self)
dask.array.Array.__complex__(self)
dask.array.Array.__div__(self,other)
dask.array.Array.__eq__(self,other)
dask.array.Array.__float__(self)
dask.array.Array.__floordiv__(self,other)
dask.array.Array.__ge__(self,other)
dask.array.Array.__getitem__(self,index)
dask.array.Array.__gt__(self,other)
dask.array.Array.__int__(self)
dask.array.Array.__invert__(self)
dask.array.Array.__le__(self,other)
dask.array.Array.__len__(self)
dask.array.Array.__lshift__(self,other)
dask.array.Array.__lt__(self,other)
dask.array.Array.__mod__(self,other)
dask.array.Array.__mul__(self,other)
dask.array.Array.__ne__(self,other)
dask.array.Array.__neg__(self)
dask.array.Array.__or__(self,other)
dask.array.Array.__pos__(self)
dask.array.Array.__pow__(self,other)
dask.array.Array.__radd__(self,other)
dask.array.Array.__rand__(self,other)
dask.array.Array.__rdiv__(self,other)
dask.array.Array.__repr__(self)
dask.array.Array.__rfloordiv__(self,other)
dask.array.Array.__rlshift__(self,other)
dask.array.Array.__rmod__(self,other)
dask.array.Array.__rmul__(self,other)
dask.array.Array.__ror__(self,other)
dask.array.Array.__rpow__(self,other)
dask.array.Array.__rrshift__(self,other)
dask.array.Array.__rshift__(self,other)
dask.array.Array.__rsub__(self,other)
dask.array.Array.__rtruediv__(self,other)
dask.array.Array.__rxor__(self,other)
dask.array.Array.__sub__(self,other)
dask.array.Array.__truediv__(self,other)
dask.array.Array.__xor__(self,other)
dask.array.Array._args(self)
dask.array.Array._keys(self,*args)
dask.array.Array._visualize(self,optimize_graph=False)
dask.array.Array.all(self,axis=None,keepdims=False)
dask.array.Array.any(self,axis=None,keepdims=False)
dask.array.Array.argmax(self,axis=None)
dask.array.Array.argmin(self,axis=None)
dask.array.Array.astype(self,dtype,**kwargs)
dask.array.Array.cache(self,store=None,**kwargs)
dask.array.Array.compute(self,**kwargs)
dask.array.Array.dot(self,other)
dask.array.Array.dtype(self)
dask.array.Array.map_blocks(self,func,chunks=None,dtype=None)
dask.array.Array.map_overlap(self,func,depth,boundary=None,trim=True,**kwargs)
dask.array.Array.max(self,axis=None,keepdims=False)
dask.array.Array.mean(self,axis=None,dtype=None,keepdims=False)
dask.array.Array.min(self,axis=None,keepdims=False)
dask.array.Array.moment(self,order,axis=None,dtype=None,keepdims=False,ddof=0)
dask.array.Array.nbytes(self)
dask.array.Array.ndim(self)
dask.array.Array.numblocks(self)
dask.array.Array.prod(self,axis=None,dtype=None,keepdims=False)
dask.array.Array.rechunk(self,chunks)
dask.array.Array.shape(self)
dask.array.Array.size(self)
dask.array.Array.squeeze(self)
dask.array.Array.std(self,axis=None,dtype=None,keepdims=False,ddof=0)
dask.array.Array.store(self,target,**kwargs)
dask.array.Array.sum(self,axis=None,dtype=None,keepdims=False)
dask.array.Array.to_hdf5(self,filename,datapath,**kwargs)
dask.array.Array.topk(self,k)
dask.array.Array.transpose(self,axes=None)
dask.array.Array.var(self,axis=None,dtype=None,keepdims=False,ddof=0)
dask.array.Array.vnorm(self,ord=None,axis=None,keepdims=False)
dask.array.around(x,decimals=0)
dask.array.atop(func,out_ind,*args,**kwargs)
dask.array.bincount(x,weights=None,minlength=None)
dask.array.broadcast_to(x,shape)
dask.array.choose(a,choices)
dask.array.coarsen(reduction,x,axes)
dask.array.compute(*args,**kwargs)
dask.array.concatenate(seq,axis=0)
dask.array.concatenate3(arrays)
dask.array.core.Array(self,dask,name,chunks,dtype=None,shape=None)
dask.array.core.Array.T(self)
dask.array.core.Array.__abs__(self)
dask.array.core.Array.__add__(self,other)
dask.array.core.Array.__and__(self,other)
dask.array.core.Array.__array__(self,dtype=None,**kwargs)
dask.array.core.Array.__bool__(self)
dask.array.core.Array.__complex__(self)
dask.array.core.Array.__div__(self,other)
dask.array.core.Array.__eq__(self,other)
dask.array.core.Array.__float__(self)
dask.array.core.Array.__floordiv__(self,other)
dask.array.core.Array.__ge__(self,other)
dask.array.core.Array.__getitem__(self,index)
dask.array.core.Array.__gt__(self,other)
dask.array.core.Array.__init__(self,dask,name,chunks,dtype=None,shape=None)
dask.array.core.Array.__int__(self)
dask.array.core.Array.__invert__(self)
dask.array.core.Array.__le__(self,other)
dask.array.core.Array.__len__(self)
dask.array.core.Array.__lshift__(self,other)
dask.array.core.Array.__lt__(self,other)
dask.array.core.Array.__mod__(self,other)
dask.array.core.Array.__mul__(self,other)
dask.array.core.Array.__ne__(self,other)
dask.array.core.Array.__neg__(self)
dask.array.core.Array.__or__(self,other)
dask.array.core.Array.__pos__(self)
dask.array.core.Array.__pow__(self,other)
dask.array.core.Array.__radd__(self,other)
dask.array.core.Array.__rand__(self,other)
dask.array.core.Array.__rdiv__(self,other)
dask.array.core.Array.__repr__(self)
dask.array.core.Array.__rfloordiv__(self,other)
dask.array.core.Array.__rlshift__(self,other)
dask.array.core.Array.__rmod__(self,other)
dask.array.core.Array.__rmul__(self,other)
dask.array.core.Array.__ror__(self,other)
dask.array.core.Array.__rpow__(self,other)
dask.array.core.Array.__rrshift__(self,other)
dask.array.core.Array.__rshift__(self,other)
dask.array.core.Array.__rsub__(self,other)
dask.array.core.Array.__rtruediv__(self,other)
dask.array.core.Array.__rxor__(self,other)
dask.array.core.Array.__sub__(self,other)
dask.array.core.Array.__truediv__(self,other)
dask.array.core.Array.__xor__(self,other)
dask.array.core.Array._args(self)
dask.array.core.Array._keys(self,*args)
dask.array.core.Array._visualize(self,optimize_graph=False)
dask.array.core.Array.all(self,axis=None,keepdims=False)
dask.array.core.Array.any(self,axis=None,keepdims=False)
dask.array.core.Array.argmax(self,axis=None)
dask.array.core.Array.argmin(self,axis=None)
dask.array.core.Array.astype(self,dtype,**kwargs)
dask.array.core.Array.cache(self,store=None,**kwargs)
dask.array.core.Array.compute(self,**kwargs)
dask.array.core.Array.dot(self,other)
dask.array.core.Array.dtype(self)
dask.array.core.Array.map_blocks(self,func,chunks=None,dtype=None)
dask.array.core.Array.map_overlap(self,func,depth,boundary=None,trim=True,**kwargs)
dask.array.core.Array.max(self,axis=None,keepdims=False)
dask.array.core.Array.mean(self,axis=None,dtype=None,keepdims=False)
dask.array.core.Array.min(self,axis=None,keepdims=False)
dask.array.core.Array.moment(self,order,axis=None,dtype=None,keepdims=False,ddof=0)
dask.array.core.Array.nbytes(self)
dask.array.core.Array.ndim(self)
dask.array.core.Array.numblocks(self)
dask.array.core.Array.prod(self,axis=None,dtype=None,keepdims=False)
dask.array.core.Array.rechunk(self,chunks)
dask.array.core.Array.shape(self)
dask.array.core.Array.size(self)
dask.array.core.Array.squeeze(self)
dask.array.core.Array.std(self,axis=None,dtype=None,keepdims=False,ddof=0)
dask.array.core.Array.store(self,target,**kwargs)
dask.array.core.Array.sum(self,axis=None,dtype=None,keepdims=False)
dask.array.core.Array.to_hdf5(self,filename,datapath,**kwargs)
dask.array.core.Array.topk(self,k)
dask.array.core.Array.transpose(self,axes=None)
dask.array.core.Array.var(self,axis=None,dtype=None,keepdims=False,ddof=0)
dask.array.core.Array.vnorm(self,ord=None,axis=None,keepdims=False)
dask.array.core._concatenate2(arrays,axes=[])
dask.array.core._take_dask_array_from_numpy(a,indices,axis)
dask.array.core.around(x,decimals=0)
dask.array.core.atop(func,out_ind,*args,**kwargs)
dask.array.core.bincount(x,weights=None,minlength=None)
dask.array.core.blockdims_from_blockshape(shape,chunks)
dask.array.core.broadcast_dimensions(argpairs,numblocks,sentinels=(1,(1,)))
dask.array.core.broadcast_to(x,shape)
dask.array.core.choose(a,choices)
dask.array.core.chunks_from_arrays(arrays)
dask.array.core.coarsen(reduction,x,axes)
dask.array.core.compute(*args,**kwargs)
dask.array.core.concatenate(seq,axis=0)
dask.array.core.concatenate3(arrays)
dask.array.core.deepfirst(seq)
dask.array.core.dotmany(A,B,leftfunc=None,rightfunc=None,**kwargs)
dask.array.core.elemwise(op,*args,**kwargs)
dask.array.core.frexp(x)
dask.array.core.from_array(x,chunks,name=None,lock=False,**kwargs)
dask.array.core.fromfunction(func,chunks=None,shape=None,dtype=None)
dask.array.core.get(dsk,keys,get=None,**kwargs)
dask.array.core.getarray(a,b,lock=None)
dask.array.core.getem(arr,chunks,shape=None)
dask.array.core.insert(arr,obj,values,axis)
dask.array.core.insert_to_ooc(out,arr)
dask.array.core.isclose(arr1,arr2,rtol=1e-05,atol=1e-08,equal_nan=False)
dask.array.core.isnull(values)
dask.array.core.lol_tuples(head,ind,values,dummies)
dask.array.core.many(a,b,binop=None,reduction=None,**kwargs)
dask.array.core.map_blocks(func,*arrs,**kwargs)
dask.array.core.modf(x)
dask.array.core.ndimlist(seq)
dask.array.core.normalize_chunks(chunks,shape=None)
dask.array.core.notnull(values)
dask.array.core.offset_func(func,offset,*args)
dask.array.core.partial_by_order(op,other)
dask.array.core.slices_from_chunks(chunks)
dask.array.core.split_at_breaks(array,breaks,axis=0)
dask.array.core.squeeze(a,axis=None)
dask.array.core.stack(seq,axis=0)
dask.array.core.store(sources,targets,**kwargs)
dask.array.core.take(a,indices,axis=0)
dask.array.core.tensordot(lhs,rhs,axes=2)
dask.array.core.top(func,output,out_indices,*arrind_pairs,**kwargs)
dask.array.core.topk(k,x)
dask.array.core.transpose(a,axes=None)
dask.array.core.unique(x)
dask.array.core.unpack_singleton(x)
dask.array.core.variadic_choose(a,*choices)
dask.array.core.where(condition,x=None,y=None)
dask.array.core.wrap_elemwise(func,**kwargs)
dask.array.core.write_hdf5_chunk(fn,datapath,index,data)
dask.array.core.zero_broadcast_dimensions(lol,nblocks)
dask.array.frexp(x)
dask.array.from_array(x,chunks,name=None,lock=False,**kwargs)
dask.array.fromfunction(func,chunks=None,shape=None,dtype=None)
dask.array.get(dsk,keys,get=None,**kwargs)
dask.array.getarray(a,b,lock=None)
dask.array.getem(arr,chunks,shape=None)
dask.array.insert(arr,obj,values,axis)
dask.array.insert_to_ooc(out,arr)
dask.array.isclose(arr1,arr2,rtol=1e-05,atol=1e-08,equal_nan=False)
dask.array.isnull(values)
dask.array.map_blocks(func,*arrs,**kwargs)
dask.array.modf(x)
dask.array.notnull(values)
dask.array.squeeze(a,axis=None)
dask.array.stack(seq,axis=0)
dask.array.store(sources,targets,**kwargs)
dask.array.take(a,indices,axis=0)
dask.array.tensordot(lhs,rhs,axes=2)
dask.array.topk(k,x)
dask.array.transpose(a,axes=None)
dask.array.unique(x)
dask.array.where(condition,x=None,y=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.6.0/lib/python3.6/site-packages/dask/array/optimization.py----------------------------------------
A:dask.array.optimization.fast_functions->kwargs.get('fast_functions', set([getarray, getitem, np.transpose]))
A:dask.array.optimization.dsk2->dict(((k, task[1] if k in full_slice_keys else task) for (k, task) in dsk.items()))
A:dask.array.optimization.dsk3->dealias(dsk2)
A:dask.array.optimization.dsk4->fuse(dsk3)
A:dask.array.optimization.dsk5->valmap(rewrite_rules.rewrite, dsk4)
A:dask.array.optimization.dsk6->inline_functions(dsk5, fast_functions=fast_functions)
A:dask.array.optimization.full_slice_keys->set((k for (k, task) in dsk.items() if is_full_slice(task)))
A:dask.array.optimization.c->fuse_slice(match[a], match[b])
A:dask.array.optimization.rewrite_rules->RuleSet(RewriteRule((getitem, (getitem, x, a), b), partial(fuse_slice_dict, getter=getitem), (a, b, x)), RewriteRule((getarray, (getitem, x, a), b), fuse_slice_dict, (a, b, x)), RewriteRule((getarray, (getarray, x, a), b), fuse_slice_dict, (a, b, x)), RewriteRule((getitem, (getarray, x, a), b), fuse_slice_dict, (a, b, x)))
A:dask.array.optimization.a->normalize_slice(a)
A:dask.array.optimization.b->normalize_slice(b)
A:dask.array.optimization.stop->min(a.stop, stop)
A:dask.array.optimization.result->list()
dask.array.optimization.fuse_slice(a,b)
dask.array.optimization.fuse_slice_dict(match,getter=getarray)
dask.array.optimization.is_full_slice(task)
dask.array.optimization.normalize_slice(s)
dask.array.optimization.optimize(dsk,keys,**kwargs)
dask.array.optimization.remove_full_slices(dsk)
dask.array.optimize(dsk,keys,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.6.0/lib/python3.6/site-packages/dask/array/numpy_compat.py----------------------------------------
A:dask.array.numpy_compat.out->numpy.asscalar(out)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.6.0/lib/python3.6/site-packages/dask/array/wrap.py----------------------------------------
A:dask.array.wrap.kwargs['size']->kwargs.pop('shape')
A:dask.array.wrap.size->kwargs.pop('size')
A:dask.array.wrap.chunks->normalize_chunks(chunks, shape)
A:dask.array.wrap.name->kwargs.pop('name', None)
A:dask.array.wrap.dtype->kwargs.pop('dtype', None)
A:dask.array.wrap.kw->kwargs.copy()
A:dask.array.wrap.keys->product([name], *[range(len(bd)) for bd in chunks])
A:dask.array.wrap.sizes->product(*chunks)
A:dask.array.wrap.dsk->dict(zip(keys, vals))
A:dask.array.wrap.shape->kwargs.pop('shape')
A:dask.array.wrap.shapes->product(*chunks)
A:dask.array.wrap.func->partial(func, dtype=dtype, **kwargs)
A:dask.array.wrap.f->partial(wrap_func, func, **kwargs)
A:dask.array.wrap.w->wrap(wrap_func_shape_as_first_arg)
A:dask.array.wrap.ones->w(np.ones, dtype='f8')
A:dask.array.wrap.zeros->w(np.zeros, dtype='f8')
A:dask.array.wrap.empty->w(np.empty, dtype='f8')
A:dask.array.wrap.full->w(full)
dask.array.wrap.dims_from_size(size,blocksize)
dask.array.wrap.wrap(wrap_func,func,**kwargs)
dask.array.wrap.wrap_func_shape_as_first_arg(func,*args,**kwargs)
dask.array.wrap.wrap_func_size_as_kwarg(func,*args,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.6.0/lib/python3.6/site-packages/dask/array/linalg.py----------------------------------------
A:dask.array.linalg.dsk_qr_st1->top(np.linalg.qr, name_qr_st1, 'ij', data.name, 'ij', numblocks={data.name: numblocks})
A:dask.array.linalg.dsk_q_st1->dict((((name_q_st1, i, 0), (operator.getitem, (name_qr_st1, i, 0), 0)) for i in range(numblocks[0])))
A:dask.array.linalg.dsk_r_st1->dict((((name_r_st1, i, 0), (operator.getitem, (name_qr_st1, i, 0), 1)) for i in range(numblocks[0])))
A:dask.array.linalg.dsk_qr_st2->top(np.linalg.qr, name_qr_st2, 'ij', name_r_st1_stacked, 'ij', numblocks={name_r_st1_stacked: (1, 1)})
A:dask.array.linalg.dsk_q_st2->dict((((name_q_st2, i, 0), (operator.getitem, (name_q_st2_aux, 0, 0), b)) for (i, b) in enumerate(block_slices)))
A:dask.array.linalg.dsk_q_st3->top(np.dot, name_q_st3, 'ij', name_q_st1, 'ij', name_q_st2, 'ij', numblocks={name_q_st1: numblocks, name_q_st2: numblocks})
A:dask.array.linalg.q->Array(dsk_q, name_q_st3, shape=data.shape, chunks=data.chunks)
A:dask.array.linalg.r->Array(dsk_r, name_r_st2, shape=(n, n), chunks=(n, n))
A:dask.array.linalg.dsk_svd_st2->top(np.linalg.svd, name_svd_st2, 'ij', name_r_st2, 'ij', numblocks={name_r_st2: (1, 1)})
A:dask.array.linalg.dsk_u_st4->top(dotmany, name_u_st4, 'ij', name_q_st3, 'ik', name_u_st2, 'kj', numblocks={name_q_st3: numblocks, name_u_st2: (1, 1)})
A:dask.array.linalg.u->compression_matrix(a, k, n_power_iter=n_power_iter).T.dot(u)
A:dask.array.linalg.s->Array(dsk_s, name_s_st2, shape=(n,), chunks=(n, n))
A:dask.array.linalg.v->Array(dsk_v, name_v_st2, shape=(n, n), chunks=(n, n))
A:dask.array.linalg.comp_level->compression_level(n, q)
A:dask.array.linalg.omega->standard_normal(size=(n, comp_level), chunks=(data.chunks[1], (comp_level,)))
A:dask.array.linalg.mat_h->data.dot(data.T.dot(mat_h))
A:dask.array.linalg.(q, _)->tsqr(mat_h)
A:dask.array.linalg.comp->compression_matrix(a, k, n_power_iter=n_power_iter)
A:dask.array.linalg.a_compressed->compression_matrix(a, k, n_power_iter=n_power_iter).dot(a)
A:dask.array.linalg.(v, s, u)->tsqr(a_compressed.T, name, compute_svd=True)
dask.array.linalg._cumsum_blocks(it)
dask.array.linalg.compression_level(n,q,oversampling=10,min_subspace_size=20)
dask.array.linalg.compression_matrix(data,q,n_power_iter=0)
dask.array.linalg.qr(a,name=None)
dask.array.linalg.svd(a,name=None)
dask.array.linalg.svd_compressed(a,k,n_power_iter=0,name=None)
dask.array.linalg.tsqr(data,name=None,compute_svd=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.6.0/lib/python3.6/site-packages/dask/array/slicing.py----------------------------------------
A:dask.array.slicing.ind->ind.tolist().tolist()
A:dask.array.slicing.index->tuple(map(sanitize_index_elements, index))
A:dask.array.slicing.blockdims->tuple(map(tuple, blockdims))
A:dask.array.slicing.(dsk_out, bd_out)->slice_with_newaxes(out_name, in_name, blockdims, index)
A:dask.array.slicing.bd_out->tuple(map(tuple, bd_out))
A:dask.array.slicing.index2->posify_index(shape, index)
A:dask.array.slicing.(dsk, blockdims2)->slice_slices_and_integers(tmp, in_name, blockdims, index_without_list)
A:dask.array.slicing.dsk2->dict((((out_name,) + insert_many(k[1:], where_none, 0), v[:2] + (insert_many(v[2], where_none, None),)) for (k, v) in dsk.items() if k[0] == out_name))
A:dask.array.slicing.dsk3->merge(dsk, dsk2)
A:dask.array.slicing.blockdims3->insert_many(blockdims2, where_none, (1,))
A:dask.array.slicing.shape->tuple(map(sum, blockdims))
A:dask.array.slicing.index_without_list->tuple((slice(None, None, None) if isinstance(i, list) else i for i in index2))
A:dask.array.slicing.(blockdims2, dsk3)->take(out_name, in_name, blockdims, index2[where_list[0]], axis=axis)
A:dask.array.slicing.tmp->next(slice_names)
A:dask.array.slicing.(blockdims2, dsk2)->take(out_name, tmp, blockdims2, index2[axis], axis=axis2)
A:dask.array.slicing.block_slices->list(map(_slice_1d, shape, blockdims, index))
A:dask.array.slicing.in_names->list(product([in_name], *[i.keys() for i in block_slices]))
A:dask.array.slicing.out_names->list(product([out_name], *[range(len(d))[::-1] if i.step and i.step < 0 else range(len(d)) for (d, i) in zip(block_slices, index) if not isinstance(i, (int, long))]))
A:dask.array.slicing.all_slices->list(product(*[i.values() for i in block_slices]))
A:dask.array.slicing.dsk_out->dict(((out_name, (getitem, in_name, slices)) for (out_name, in_name, slices) in zip(out_names, in_names, all_slices)))
A:dask.array.slicing.lens->list(lengths)
A:dask.array.slicing.d->dict()
A:dask.array.slicing.d[i]->slice(rstart - chunk_stop, max(chunk_start - chunk_stop - 1, stop - chunk_stop), step)
A:dask.array.slicing.chunk_boundaries->list(accumulate(add, lengths))
A:dask.array.slicing.d[k]->slice(None, None, None)
A:dask.array.slicing.d[0]->slice(0, 0, 1)
A:dask.array.slicing.seq->list(seq)
A:dask.array.slicing.result->list()
A:dask.array.slicing.L->list()
A:dask.array.slicing.colon->slice(None, None, None)
A:dask.array.slicing.n->len(blockdims)
A:dask.array.slicing.index_lists->partition_by_size(sizes, sorted(index))
A:dask.array.slicing.indims->list(dims)
A:dask.array.slicing.indims[axis]->list(range(len(where_index)))
A:dask.array.slicing.keys->list(product([outname], *dims))
A:dask.array.slicing.outdims->list(dims)
A:dask.array.slicing.slices->list(product(*slices))
A:dask.array.slicing.inkeys->list(product([inname], *outdims))
A:dask.array.slicing.blockdims2->list(blockdims)
A:dask.array.slicing.blockdims2[axis]->tuple(map(len, index_lists))
A:dask.array.slicing.rev_index->list(map(sorted(index).index, index))
A:dask.array.slicing.pairs->sorted(_slice_1d(dim_shape, lengths, index).items(), key=first)
dask.array.slicing._slice_1d(dim_shape,lengths,index)
dask.array.slicing.insert_many(seq,where,val)
dask.array.slicing.issorted(seq)
dask.array.slicing.new_blockdim(dim_shape,lengths,index)
dask.array.slicing.partition_by_size(sizes,seq)
dask.array.slicing.posify_index(shape,ind)
dask.array.slicing.replace_ellipsis(n,index)
dask.array.slicing.sanitize_index_elements(ind)
dask.array.slicing.slice_array(out_name,in_name,blockdims,index)
dask.array.slicing.slice_slices_and_integers(out_name,in_name,blockdims,index)
dask.array.slicing.slice_with_newaxes(out_name,in_name,blockdims,index)
dask.array.slicing.slice_wrap_lists(out_name,in_name,blockdims,index)
dask.array.slicing.take(outname,inname,blockdims,index,axis=0)
dask.array.slicing.take_sorted(outname,inname,blockdims,index,axis=0)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.6.0/lib/python3.6/site-packages/dask/array/random.py----------------------------------------
A:dask.array.random.func2.__doc__->getattr(state, func).__doc__.replace('>>>', '>>').replace('...', '..')
A:dask.array.random.self._numpy_state->numpy.random.RandomState(seed)
A:dask.array.random.size->kwargs.pop('size')
A:dask.array.random.chunks->normalize_chunks(chunks, size)
A:dask.array.random.name->next(names)
A:dask.array.random.kw->kwargs.copy()
A:dask.array.random.keys->product([name], *[range(len(bd)) for bd in chunks])
A:dask.array.random.sizes->product(*chunks)
A:dask.array.random.dsk->dict(zip(keys, vals))
A:dask.array.random.state->numpy.random.RandomState(seed)
A:dask.array.random.func->getattr(state, func)
A:dask.array.random._state->RandomState()
dask.array.random.RandomState(self,seed=None)
dask.array.random.RandomState.__init__(self,seed=None)
dask.array.random.RandomState._wrap(self,func,*args,**kwargs)
dask.array.random.RandomState.beta(self,a,b,size=None,chunks=None)
dask.array.random.RandomState.binomial(self,n,p,size=None,chunks=None)
dask.array.random.RandomState.chisquare(self,df,size=None,chunks=None)
dask.array.random.RandomState.choice(self,a,size=None,replace=True,p=None,chunks=None)
dask.array.random.RandomState.exponential(self,scale=1.0,size=None,chunks=None)
dask.array.random.RandomState.f(self,dfnum,dfden,size=None,chunks=None)
dask.array.random.RandomState.gamma(self,shape,scale=1.0,chunks=None)
dask.array.random.RandomState.geometric(self,p,size=None,chunks=None)
dask.array.random.RandomState.gumbel(self,loc=0.0,scale=1.0,size=None,chunks=None)
dask.array.random.RandomState.hypergeometric(self,ngood,nbad,nsample,size=None,chunks=None)
dask.array.random.RandomState.laplace(self,loc=0.0,scale=1.0,size=None,chunks=None)
dask.array.random.RandomState.logistic(self,loc=0.0,scale=1.0,size=None,chunks=None)
dask.array.random.RandomState.lognormal(self,mean=0.0,sigma=1.0,size=None,chunks=None)
dask.array.random.RandomState.logseries(self,p,size=None,chunks=None)
dask.array.random.RandomState.negative_binomial(self,n,p,size=None,chunks=None)
dask.array.random.RandomState.noncentral_chisquare(self,df,nonc,size=None,chunks=None)
dask.array.random.RandomState.noncentral_f(self,dfnum,dfden,nonc,size=None,chunks=None)
dask.array.random.RandomState.normal(self,loc=0.0,scale=1.0,size=None,chunks=None)
dask.array.random.RandomState.pareto(self,a,size=None,chunks=None)
dask.array.random.RandomState.poisson(self,lam=1.0,size=None,chunks=None)
dask.array.random.RandomState.power(self,a,size=None,chunks=None)
dask.array.random.RandomState.randint(self,low,high=None,size=None,chunks=None)
dask.array.random.RandomState.random_integers(self,low,high=None,size=None,chunks=None)
dask.array.random.RandomState.random_sample(self,size=None,chunks=None)
dask.array.random.RandomState.rayleigh(self,scale=1.0,size=None,chunks=None)
dask.array.random.RandomState.standard_cauchy(self,size=None,chunks=None)
dask.array.random.RandomState.standard_exponential(self,size=None,chunks=None)
dask.array.random.RandomState.standard_gamma(self,shape,size=None,chunks=None)
dask.array.random.RandomState.standard_normal(self,size=None,chunks=None)
dask.array.random.RandomState.standard_t(self,df,size=None,chunks=None)
dask.array.random.RandomState.tomaxint(self,size=None,chunks=None)
dask.array.random.RandomState.triangular(self,left,mode,right,size=None,chunks=None)
dask.array.random.RandomState.uniform(self,low=0.0,high=1.0,size=None,chunks=None)
dask.array.random.RandomState.vonmises(self,mu,kappa,size=None,chunks=None)
dask.array.random.RandomState.wald(self,mean,scale,size=None,chunks=None)
dask.array.random.RandomState.weibull(self,a,size=None,chunks=None)
dask.array.random.RandomState.zipf(self,a,size=None,chunks=None)
dask.array.random._apply_random(func,seed,size,args,kwargs)
dask.array.random.doc_wraps(func)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.6.0/lib/python3.6/site-packages/dask/array/percentile.py----------------------------------------
A:dask.array.percentile.q->numpy.array(q)
A:dask.array.percentile.result->merge_percentiles(finalq, qs, [v.codes for v in vals], Ns, interpolation)
A:dask.array.percentile.a2->a.astype('i8')
A:dask.array.percentile.name->next(names)
A:dask.array.percentile.dsk->dict((((name, i), (_percentile, key, q, interpolation)) for (i, key) in enumerate(a._keys())))
A:dask.array.percentile.name2->next(names)
A:dask.array.percentile.finalq->numpy.array(finalq)
A:dask.array.percentile.qs->list(map(list, qs))
A:dask.array.percentile.vals->list(vals)
A:dask.array.percentile.Ns->list(Ns)
A:dask.array.percentile.L->list(zip(*[(q, val, N) for (q, val, N) in zip(qs, vals, Ns) if N]))
A:dask.array.percentile.count->numpy.empty(len(q))
A:dask.array.percentile.count[1:]->numpy.diff(q)
A:dask.array.percentile.combined_vals_counts->merge_sorted(*map(zip, vals, counts))
A:dask.array.percentile.(combined_vals, combined_counts)->zip(*combined_vals_counts)
A:dask.array.percentile.combined_vals->numpy.array(combined_vals)
A:dask.array.percentile.combined_counts->numpy.array(combined_counts)
A:dask.array.percentile.combined_q->numpy.cumsum(combined_counts)
A:dask.array.percentile.rv->numpy.interp(desired_q, combined_q, combined_vals)
A:dask.array.percentile.left->numpy.searchsorted(combined_q, desired_q, side='left')
A:dask.array.percentile.lower->numpy.minimum(left, right)
A:dask.array.percentile.upper->numpy.maximum(left, right)
A:dask.array.percentile.lower_residual->numpy.abs(combined_q[lower] - desired_q)
A:dask.array.percentile.upper_residual->numpy.abs(combined_q[upper] - desired_q)
dask.array.percentile(a,q,interpolation='linear')
dask.array.percentile._percentile(a,q,interpolation='linear')
dask.array.percentile.merge_percentiles(finalq,qs,vals,Ns,interpolation='lower')
dask.array.percentile.percentile(a,q,interpolation='linear')


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.6.0/lib/python3.6/site-packages/dask/array/reductions.py----------------------------------------
A:dask.array.reductions.axis->tuple((i if i >= 0 else x.ndim + i for i in axis))
A:dask.array.reductions.chunk->partial(chunk, dtype=dtype)
A:dask.array.reductions.aggregate->partial(aggregate, dtype=dtype)
A:dask.array.reductions.chunk2->partial(chunk, axis=axis, keepdims=True)
A:dask.array.reductions.aggregate2->partial(aggregate, axis=axis, keepdims=keepdims)
A:dask.array.reductions.inds->tuple(range(x.ndim))
A:dask.array.reductions.tmp->atop(chunk2, inds, x, inds)
A:dask.array.reductions.inds2->tuple((i for i in inds if i not in axis))
A:dask.array.reductions.result->result.astype(dtype).astype(dtype)
A:dask.array.reductions.dsk->result.astype(dtype).astype(dtype).dask.copy()
A:dask.array.reductions.dsk[k2]->result.astype(dtype).astype(dtype).dask.copy().pop(k)
A:dask.array.reductions.chunks->insert_many(result.chunks, axis, [1])
A:dask.array.reductions.n->ns.sum(**keepdim_kw)
A:dask.array.reductions.total->sum(A, dtype=dtype, **kwargs)
A:dask.array.reductions.nanmean->wraps(chunk.nanmean)(nanmean)
A:dask.array.reductions.M->numpy.empty(shape=n.shape + (order - 1,), dtype=dtype)
A:dask.array.reductions.M[..., i - 2]->sum((A - u) ** i, dtype=dtype, **kwargs)
A:dask.array.reductions.keepdim_kw->kwargs.copy()
A:dask.array.reductions.mu->divide(totals.sum(**keepdim_kw), n, dtype=dtype)
A:dask.array.reductions.nanvar->wraps(chunk.nanvar)(nanvar)
A:dask.array.reductions.nanstd->wraps(chunk.nanstd)(nanstd)
A:dask.array.reductions.pairs->list(pairs)
A:dask.array.reductions.(mins, argmins)->zip(*pairs)
A:dask.array.reductions.mins->numpy.array(mins)
A:dask.array.reductions.argmins->numpy.array(argmins)
A:dask.array.reductions.args->argfunc(mins, axis=0)
A:dask.array.reductions.offsets->offsets.reshape((len(offsets),) + (1,) * (argmins.ndim - 1)).reshape((len(offsets),) + (1,) * (argmins.ndim - 1))
A:dask.array.reductions.a2->elemwise(argreduce, a)
dask.array.all(a,axis=None,keepdims=False)
dask.array.any(a,axis=None,keepdims=False)
dask.array.argmax(a,axis=None)
dask.array.argmin(a,axis=None)
dask.array.max(a,axis=None,keepdims=False)
dask.array.mean(a,axis=None,dtype=None,keepdims=False)
dask.array.mean_agg(pair,dtype='f8',**kwargs)
dask.array.mean_chunk(x,sum=chunk.sum,numel=numel,dtype='f8',**kwargs)
dask.array.min(a,axis=None,keepdims=False)
dask.array.moment(a,order,axis=None,dtype=None,keepdims=False,ddof=0)
dask.array.moment_agg(data,order=2,ddof=0,dtype='f8',**kwargs)
dask.array.moment_chunk(A,order=2,sum=chunk.sum,numel=numel,dtype='f8',**kwargs)
dask.array.nanargmax(a,axis=None)
dask.array.nanargmin(a,axis=None)
dask.array.nanmax(a,axis=None,keepdims=False)
dask.array.nanmean(a,axis=None,dtype=None,keepdims=False)
dask.array.nanmin(a,axis=None,keepdims=False)
dask.array.nanstd(a,axis=None,dtype=None,keepdims=False,ddof=0)
dask.array.nansum(a,axis=None,dtype=None,keepdims=False)
dask.array.nanvar(a,axis=None,dtype=None,keepdims=False,ddof=0)
dask.array.prod(a,axis=None,dtype=None,keepdims=False)
dask.array.reductions.all(a,axis=None,keepdims=False)
dask.array.reductions.any(a,axis=None,keepdims=False)
dask.array.reductions.arg_aggregate(func,argfunc,dims,pairs)
dask.array.reductions.arg_reduction(a,func,argfunc,axis=0,dtype=None)
dask.array.reductions.argmax(a,axis=None)
dask.array.reductions.argmin(a,axis=None)
dask.array.reductions.max(a,axis=None,keepdims=False)
dask.array.reductions.mean(a,axis=None,dtype=None,keepdims=False)
dask.array.reductions.mean_agg(pair,dtype='f8',**kwargs)
dask.array.reductions.mean_chunk(x,sum=chunk.sum,numel=numel,dtype='f8',**kwargs)
dask.array.reductions.min(a,axis=None,keepdims=False)
dask.array.reductions.moment(a,order,axis=None,dtype=None,keepdims=False,ddof=0)
dask.array.reductions.moment_agg(data,order=2,ddof=0,dtype='f8',**kwargs)
dask.array.reductions.moment_chunk(A,order=2,sum=chunk.sum,numel=numel,dtype='f8',**kwargs)
dask.array.reductions.nanargmax(a,axis=None)
dask.array.reductions.nanargmin(a,axis=None)
dask.array.reductions.nanmax(a,axis=None,keepdims=False)
dask.array.reductions.nanmean(a,axis=None,dtype=None,keepdims=False)
dask.array.reductions.nanmin(a,axis=None,keepdims=False)
dask.array.reductions.nannumel(x,**kwargs)
dask.array.reductions.nanstd(a,axis=None,dtype=None,keepdims=False,ddof=0)
dask.array.reductions.nansum(a,axis=None,dtype=None,keepdims=False)
dask.array.reductions.nanvar(a,axis=None,dtype=None,keepdims=False,ddof=0)
dask.array.reductions.numel(x,**kwargs)
dask.array.reductions.prod(a,axis=None,dtype=None,keepdims=False)
dask.array.reductions.reduction(x,chunk,aggregate,axis=None,keepdims=None,dtype=None)
dask.array.reductions.std(a,axis=None,dtype=None,keepdims=False,ddof=0)
dask.array.reductions.sum(a,axis=None,dtype=None,keepdims=False)
dask.array.reductions.var(a,axis=None,dtype=None,keepdims=False,ddof=0)
dask.array.reductions.vnorm(a,ord=None,axis=None,dtype=None,keepdims=False)
dask.array.std(a,axis=None,dtype=None,keepdims=False,ddof=0)
dask.array.sum(a,axis=None,dtype=None,keepdims=False)
dask.array.var(a,axis=None,dtype=None,keepdims=False,ddof=0)
dask.array.vnorm(a,ord=None,axis=None,dtype=None,keepdims=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.6.0/lib/python3.6/site-packages/dask/array/learn.py----------------------------------------
A:dask.array.learn.x->x.reblock(chunks=(x.chunks[0], sum(x.chunks[1]))).reblock(chunks=(x.chunks[0], sum(x.chunks[1])))
A:dask.array.learn.nblocks->len(x.chunks[0])
A:dask.array.learn.name->next(names)
A:dask.array.learn.func->partial(_predict, model)
dask.array.learn._partial_fit(model,x,y,kwargs=None)
dask.array.learn._predict(model,x)
dask.array.learn.fit(model,x,y,get=threaded.get,**kwargs)
dask.array.learn.predict(model,x)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.6.0/lib/python3.6/site-packages/dask/dataframe/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.6.0/lib/python3.6/site-packages/dask/dataframe/optimize.py----------------------------------------
A:dask.dataframe.optimize.rewrite_rules->RuleSet(*rules)
A:dask.dataframe.optimize.dsk2->cull(dsk, [keys])
A:dask.dataframe.optimize.dsk3->inline_functions(dsk2, fast_functions)
A:dask.dataframe.optimize.dsk4->fuse(dsk3)
A:dask.dataframe.optimize.dsk5->valmap(rewrite_rules.rewrite, dsk4)
dask.dataframe.optimize(dsk,keys,**kwargs)
dask.dataframe.optimize.optimize(dsk,keys,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.6.0/lib/python3.6/site-packages/dask/dataframe/shuffle.py----------------------------------------
A:dask.dataframe.shuffle.divisions->list(divisions)
A:dask.dataframe.shuffle.dsk2->dict((((name, i), (partition, part, index, npartitions, p)) for (i, part) in enumerate(df._keys())))
A:dask.dataframe.shuffle.dsk->merge(df.dask, dsk1, dsk2, dsk3, dsk4)
A:dask.dataframe.shuffle.(p, barrier_token)->get(dsk, [p, barrier_token], **kwargs)
A:dask.dataframe.shuffle.dsk4->dict((((name, i), (collect, i, p, barrier_token)) for i in range(npartitions)))
A:dask.dataframe.shuffle.df->df.set_index(index).set_index(index)
A:dask.dataframe.shuffle.shards->shard_df_on_index(df, divisions[1:-1])
A:dask.dataframe.shuffle.rng->pandas.Series(np.arange(len(df)))
A:dask.dataframe.shuffle.index->list(index)
A:dask.dataframe.shuffle.groups->pandas.Series(np.arange(len(df))).groupby(index.apply(lambda row: abs(hash(tuple(row))) % npartitions, axis=1).values)
A:dask.dataframe.shuffle.d->dict(((i, df.iloc[groups.groups[i]]) for i in range(npartitions) if i in groups.groups))
dask.dataframe.shuffle._set_collect(group,p,barrier_token)
dask.dataframe.shuffle._set_partition(df,index,divisions,p)
dask.dataframe.shuffle.barrier(args)
dask.dataframe.shuffle.collect(group,p,barrier_token)
dask.dataframe.shuffle.partition(df,index,npartitions,p)
dask.dataframe.shuffle.set_index(df,index,npartitions=None,compute=True,**kwargs)
dask.dataframe.shuffle.set_partition(df,index,divisions,compute=False,**kwargs)
dask.dataframe.shuffle.shuffle(df,index,npartitions=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.6.0/lib/python3.6/site-packages/dask/dataframe/pdutils.py----------------------------------------
A:dask.dataframe.pdutils.df->pandas.DataFrame(create_block_manager_from_blocks(blocks, axes))
dask.dataframe.pdutils.from_blocks(blocks,index,index_name,columns,placement)
dask.dataframe.pdutils.to_blocks(df)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.6.0/lib/python3.6/site-packages/dask/dataframe/utils.py----------------------------------------
A:dask.dataframe.utils.divisions->numpy.array(divisions)
A:dask.dataframe.utils.df->df.sort_index().sort_index()
A:dask.dataframe.utils.indices->df.sort_index().sort_index().index.searchsorted(divisions)
A:dask.dataframe.utils.cat->pandas.Categorical.from_codes(df.values, categories[df.name])
dask.dataframe.utils._categorize(categories,df)
dask.dataframe.utils.get_categories(df)
dask.dataframe.utils.iscategorical(dt)
dask.dataframe.utils.shard_df_on_index(df,divisions)
dask.dataframe.utils.strip_categories(df)
dask.dataframe.utils.unique(divisions)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.6.0/lib/python3.6/site-packages/dask/dataframe/multi.py----------------------------------------
A:dask.dataframe.multi.divisions->tuple(divisions[min(present):max(present) + 2])
A:dask.dataframe.multi.dfs2->list(map(repartition, dfs, divisionss))
A:dask.dataframe.multi.result->list()
A:dask.dataframe.multi.L->list()
A:dask.dataframe.multi.parts->tuple(parts[min(present):max(present) + 1])
A:dask.dataframe.multi.((lhs, rhs), divisions, parts)->align_partitions(lhs, rhs)
A:dask.dataframe.multi.(divisions, parts)->require(divisions, parts, required[how])
A:dask.dataframe.multi.left_empty->pandas.DataFrame([], columns=lhs.columns)
A:dask.dataframe.multi.right_empty->pandas.DataFrame([], columns=rhs.columns)
A:dask.dataframe.multi.dsk->dict((((name, i), (pd.concat, part, 0, join)) for (i, part) in enumerate(parts2)))
A:dask.dataframe.multi.j->pandas.merge(left_empty, right_empty, how, None, left_on=left_on, right_on=right_on, left_index=left_index, right_index=right_index)
A:dask.dataframe.multi.default_left_columns->kwargs.pop('left_columns')
A:dask.dataframe.multi.default_right_columns->kwargs.pop('right_columns')
A:dask.dataframe.multi.left->from_pandas(left, npartitions=1)
A:dask.dataframe.multi.right->from_pandas(right, npartitions=1)
A:dask.dataframe.multi.npartitions->max(lhs.npartitions, rhs.npartitions)
A:dask.dataframe.multi.lhs2->shuffle(lhs, on_left, npartitions)
A:dask.dataframe.multi.rhs2->shuffle(rhs, on_right, npartitions)
A:dask.dataframe.multi.left_index->isinstance(on_left, Index)
A:dask.dataframe.multi.right_index->isinstance(on_right, Index)
A:dask.dataframe.multi.pdmerge2->partial(pdmerge, suffixes=suffixes, left_columns=list(lhs.columns), right_columns=list(rhs.columns))
A:dask.dataframe.multi.(dfs2, divisions, parts)->align_partitions(*dfs)
dask.dataframe.merge(left,right,how='inner',on=None,left_on=None,right_on=None,left_index=False,right_index=False,suffixes=('_x','_y'),npartitions=None)
dask.dataframe.multi.align_partitions(*dfs)
dask.dataframe.multi.bound(seq,left,right)
dask.dataframe.multi.concat_indexed_dataframes(dfs,join='outer')
dask.dataframe.multi.hash_join(lhs,on_left,rhs,on_right,how='inner',npartitions=None,suffixes=('_x','_y'))
dask.dataframe.multi.join_indexed_dataframes(lhs,rhs,how='left',lsuffix='',rsuffix='')
dask.dataframe.multi.merge(left,right,how='inner',on=None,left_on=None,right_on=None,left_index=False,right_index=False,suffixes=('_x','_y'),npartitions=None)
dask.dataframe.multi.pdmerge(left,right,*args,**kwargs)
dask.dataframe.multi.require(divisions,parts,required=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.6.0/lib/python3.6/site-packages/dask/dataframe/io.py----------------------------------------
A:dask.dataframe.io.data->data.sort_index(ascending=True).sort_index(ascending=True)
A:dask.dataframe.io.kwargs->kwargs.copy().copy()
A:dask.dataframe.io.sample_nrows->kwargs.copy().copy().pop('sample_nrows', 1000)
A:dask.dataframe.io.kwargs['names']->csv_names(fn, **kwargs)
A:dask.dataframe.io.kwargs['header']->infer_header(fn, **kwargs)
A:dask.dataframe.io.head->pandas.read_csv(fn, *args, **kwargs)
A:dask.dataframe.io.kwargs['dtype']->dict(head.dtypes)
A:dask.dataframe.io.kwargs['columns']->list(head.columns)
A:dask.dataframe.io.chunkbytes->kwargs.copy().copy().pop('chunkbytes', 2 ** 25)
A:dask.dataframe.io.categorize->kwargs.copy().copy().pop('categorize', None)
A:dask.dataframe.io.index->kwargs.copy().copy().pop('index', None)
A:dask.dataframe.io.columns->list(columns)
A:dask.dataframe.io.total_bytes->file_size(fn, compression)
A:dask.dataframe.io.nchunks->int(ceil(total_bytes / chunkbytes))
A:dask.dataframe.io.header->kwargs.copy().copy().pop('header')
A:dask.dataframe.io.first_read_csv->partial(pd.read_csv, *args, header=header, **dissoc(kwargs, 'compression'))
A:dask.dataframe.io.rest_read_csv->partial(pd.read_csv, *args, header=None, **dissoc(kwargs, 'compression'))
A:dask.dataframe.io.dsk->dict((((new_name, i), (dataframe_from_ctable, x, (slice(i * chunksize, (i + 1) * chunksize),), None, categories)) for i in range(0, int(ceil(len(x) / chunksize)))))
A:dask.dataframe.io.result->DataFrame(dsk, new_name, columns, divisions)
A:dask.dataframe.io.(categories, quantiles)->categories_and_quantiles(fn, args, kwargs, index, categorize, chunkbytes=chunkbytes)
A:dask.dataframe.io.func->partial(categorize_block, categories=categories)
A:dask.dataframe.io.df->pandas.read_csv(fn, encoding=encoding, compression=compression, names=names, parse_dates=parse_dates, **kwargs)
A:dask.dataframe.io.compression->kwargs.copy().copy().get('compression', None)
A:dask.dataframe.io.one_chunk->pandas.read_csv(fn, *args, nrows=100, **kwargs)
A:dask.dataframe.io.dtypes->dict(((c, one_chunk.dtypes[c]) for c in cols))
A:dask.dataframe.io.d->read_csv(fn, *args, **merge(kwargs, dict(usecols=cols, parse_dates=None, dtype=dtypes)))
A:dask.dataframe.io.quantiles->d[index].quantiles(np.linspace(0, 100, nchunks + 1))
A:dask.dataframe.io.categories->dict()
A:dask.dataframe.io.divisions->dask.array.percentile(a, q).compute()
A:dask.dataframe.io.nrows->len(data)
A:dask.dataframe.io.chunksize->int(ceil(nrows / npartitions))
A:dask.dataframe.io.x->bcolz.ctable(rootdir=x)
A:dask.dataframe.io.bc_chunklen->max((x[name].chunklen for name in x.names))
A:dask.dataframe.io.a->dask.array.from_array(x[index], chunks=(chunksize * len(x.names),))
A:dask.dataframe.io.categories[name]->dask.array.unique(a)
A:dask.dataframe.io.q->numpy.linspace(0, 100, len(x) // chunksize + 2)
A:dask.dataframe.io.chunk->pandas.Categorical.from_codes(np.searchsorted(categories[columns], chunk), categories[columns], True)
dask.dataframe.from_array(x,chunksize=50000)
dask.dataframe.from_bcolz(x,chunksize=None,categorize=True,index=None,**kwargs)
dask.dataframe.from_pandas(data,npartitions)
dask.dataframe.io._StringIO(data)
dask.dataframe.io.categories_and_quantiles(fn,args,kwargs,index=None,categorize=None,chunkbytes=2**26)
dask.dataframe.io.csv_names(fn,encoding='utf-8',compression=None,names=None,parse_dates=None,usecols=None,dtype=None,**kwargs)
dask.dataframe.io.dataframe_from_ctable(x,slc,columns=None,categories=None)
dask.dataframe.io.file_size(fn,compression=None)
dask.dataframe.io.fill_kwargs(fn,args,kwargs)
dask.dataframe.io.from_array(x,chunksize=50000)
dask.dataframe.io.from_bcolz(x,chunksize=None,categorize=True,index=None,**kwargs)
dask.dataframe.io.from_pandas(data,npartitions)
dask.dataframe.io.infer_header(fn,encoding='utf-8',compression=None,**kwargs)
dask.dataframe.io.read_csv(fn,*args,**kwargs)
dask.dataframe.read_csv(fn,*args,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.6.0/lib/python3.6/site-packages/dask/store/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.6.0/lib/python3.6/site-packages/dask/store/core.py----------------------------------------
A:dask.store.core.self.dsk->dict()
A:dask.store.core.cache->dict()
A:dask.store.core.self.data->set()
A:dask.store.core.self.compute_time->dict()
A:dask.store.core.self.access_times->defaultdict(list)
A:dask.store.core.start->time()
A:dask.store.core.result->func(*args)
A:dask.store.core.end->time()
dask.store.Store(self,cache=None)
dask.store.Store.__delitem__(self,key)
dask.store.Store.__getitem__(self,key)
dask.store.Store.__iter__(self)
dask.store.Store.__len__(self)
dask.store.Store.__setitem__(self,key,value)
dask.store.core.Store(self,cache=None)
dask.store.core.Store.__delitem__(self,key)
dask.store.core.Store.__getitem__(self,key)
dask.store.core.Store.__init__(self,cache=None)
dask.store.core.Store.__iter__(self)
dask.store.core.Store.__len__(self)
dask.store.core.Store.__setitem__(self,key,value)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.6.0/lib/python3.6/site-packages/dask/diagnostics/__init__.py----------------------------------------
A:dask.diagnostics.__init__.thread_prof->Profiler(threaded.get)
A:dask.diagnostics.__init__.process_prof->Profiler(multiprocessing.get)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.6.0/lib/python3.6/site-packages/dask/diagnostics/profile_visualize.py----------------------------------------
A:dask.diagnostics.profile_visualize.unique_funcs->list(sorted(unique(funcs)))
A:dask.diagnostics.profile_visualize.n_funcs->len(unique_funcs)
A:dask.diagnostics.profile_visualize.keys->list(palette_lookup.keys())
A:dask.diagnostics.profile_visualize.colors->cycle(palette_lookup[high])
A:dask.diagnostics.profile_visualize.color_lookup->dict(zip(unique_funcs, colors))
A:dask.diagnostics.profile_visualize.(keys, tasks, starts, ends, ids)->zip(*results)
A:dask.diagnostics.profile_visualize.id_group->groupby(itemgetter(4), results)
A:dask.diagnostics.profile_visualize.timings->dict(((k, [i.end_time - i.start_time for i in v]) for (k, v) in id_group.items()))
A:dask.diagnostics.profile_visualize.id_lk->dict(((t[0], n) for (n, t) in enumerate(sorted(timings.items(), key=itemgetter(1), reverse=True))))
A:dask.diagnostics.profile_visualize.left->min(starts)
A:dask.diagnostics.profile_visualize.right->max(ends)
A:dask.diagnostics.profile_visualize.defaults->dict(title='Profile Results', tools='hover,save,reset,resize,xwheel_zoom,xpan', plot_width=800, plot_height=400)
A:dask.diagnostics.profile_visualize.p->bokeh.plotting.figure(y_range=[str(i) for i in range(len(id_lk))], x_range=[0, right - left], **defaults)
A:dask.diagnostics.profile_visualize.data['color']->get_colors(palette, funcs)
A:dask.diagnostics.profile_visualize.source->bokeh.plotting.ColumnDataSource(data=data)
A:dask.diagnostics.profile_visualize.hover->bokeh.plotting.figure(y_range=[str(i) for i in range(len(id_lk))], x_range=[0, right - left], **defaults).select(type=HoverTool)
dask.diagnostics.profile_visualize.get_colors(palette,funcs)
dask.diagnostics.profile_visualize.visualize(results,palette='GnBu',file_path='profile.html',show=True,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.6.0/lib/python3.6/site-packages/dask/diagnostics/profile.py----------------------------------------
A:dask.diagnostics.profile.TaskData->namedtuple('TaskData', ('key', 'task', 'start_time', 'end_time', 'worker_id'))
A:dask.diagnostics.profile.start->default_timer()
A:dask.diagnostics.profile.end->default_timer()
dask.diagnostics.Profiler(self,get)
dask.diagnostics.Profiler._end_callback(self,key,dask,state,id)
dask.diagnostics.Profiler._start_callback(self,key,dask,state)
dask.diagnostics.Profiler.clear(self)
dask.diagnostics.Profiler.get(self,dsk,result,**kwargs)
dask.diagnostics.Profiler.results(self)
dask.diagnostics.Profiler.visualize(self,**kwargs)
dask.diagnostics.profile.Profiler(self,get)
dask.diagnostics.profile.Profiler.__init__(self,get)
dask.diagnostics.profile.Profiler._end_callback(self,key,dask,state,id)
dask.diagnostics.profile.Profiler._start_callback(self,key,dask,state)
dask.diagnostics.profile.Profiler.clear(self)
dask.diagnostics.profile.Profiler.get(self,dsk,result,**kwargs)
dask.diagnostics.profile.Profiler.results(self)
dask.diagnostics.profile.Profiler.visualize(self,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.6.0/lib/python3.6/site-packages/dask/bag/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.6.0/lib/python3.6/site-packages/dask/bag/core.py----------------------------------------
A:dask.bag.core.dependencies->dict(((k, get_dependencies(dsk, k)) for k in dsk))
A:dask.bag.core.dependents->reverse_dict(dependencies)
A:dask.bag.core.dsk2->dict((((name, i), (partition, grouper, (self.name, i), npartitions, p, blocksize)) for i in range(self.npartitions)))
A:dask.bag.core.dsk3->fuse(dsk2)
A:dask.bag.core.dsk4->dict((((name, i), (collect, grouper, i, p, barrier_token)) for i in range(npartitions)))
A:dask.bag.core.dsk5->lazify(dsk4)
A:dask.bag.core.name->next(names)
A:dask.bag.core.dsk->dict((((name, next(counter)), key) for bag in bags for key in sorted(bag.dask)))
A:dask.bag.core.self.str->StringAccessor(self)
A:dask.bag.core.func->getattr(str, key)
A:dask.bag.core.a->next(names)
A:dask.bag.core.b->Bag(merge(self.dask, dsk), name, 1)
A:dask.bag.core.key->_get_s3_bucket(bucket_name, *conn_args).get_key(key_name)
A:dask.bag.core.(totals, counts)->list(zip(*x))
A:dask.bag.core.(squares, totals, counts)->list(zip(*x))
A:dask.bag.core.results->list(results)
A:dask.bag.core.columns->list(range(len(head)))
A:dask.bag.core.DataFrame->partial(pd.DataFrame, columns=columns)
A:dask.bag.core.d->dict((((name, i), part) for (i, part) in enumerate(parts)))
A:dask.bag.core.d2->defaultdict(list)
A:dask.bag.core.filenames->hdfs_utils.filenames(hdfs, path)
A:dask.bag.core.extension->os.path.splitext(filenames[0])[1].strip('.')
A:dask.bag.core.myopen->opens.get(extension, open)
A:dask.bag.core.dirname->os.path.dirname(filename)
A:dask.bag.core.f->open(filename, 'w')
A:dask.bag.core.connection->boto.connect_s3(aws_access_key, aws_secret_key)
A:dask.bag.core._memoized_get_bucket->toolz.memoize(_get_s3_bucket)
A:dask.bag.core.bucket->_get_s3_bucket(bucket_name, *conn_args)
A:dask.bag.core.o->urlparse('s3://' + quote(bucket_name[len('s3://'):]))
A:dask.bag.core.paths->unquote(o.path[1:])
A:dask.bag.core.bucket_name->unquote(o.hostname)
A:dask.bag.core.(bucket_name, paths)->_parse_s3_URI(bucket_name, paths)
A:dask.bag.core.get_key->partial(_get_key, bucket_name, conn_args)
A:dask.bag.core.keys->_get_s3_bucket(bucket_name, *conn_args).list()
A:dask.bag.core.decompressor->bz2.BZ2Decompressor()
A:dask.bag.core.decompressed->bz2.BZ2Decompressor().decompress(chunk).decode()
A:dask.bag.core.seq->list(map(list, seq))
A:dask.bag.core.partition_size->int(len(seq) / 100)
A:dask.bag.core.parts->list(partition_all(partition_size, seq))
A:dask.bag.core.spec->inspect.getargspec(func)
A:dask.bag.core.counter->itertools.count(0)
dask.bag.Bag(self,dsk,name,npartitions)
dask.bag.Bag.__iter__(self)
dask.bag.Bag._args(self)
dask.bag.Bag._keys(self)
dask.bag.Bag._visualize(self,optimize_graph=False)
dask.bag.Bag.all(self)
dask.bag.Bag.any(self)
dask.bag.Bag.compute(self,**kwargs)
dask.bag.Bag.concat(self)
dask.bag.Bag.count(self)
dask.bag.Bag.distinct(self)
dask.bag.Bag.filter(self,predicate)
dask.bag.Bag.fold(self,binop,combine=None,initial=None)
dask.bag.Bag.foldby(self,key,binop,initial=no_default,combine=None,combine_initial=no_default)
dask.bag.Bag.frequencies(self)
dask.bag.Bag.from_filenames(cls,*args,**kwargs)
dask.bag.Bag.from_sequence(cls,*args,**kwargs)
dask.bag.Bag.groupby(self,grouper,npartitions=None,blocksize=2**20)
dask.bag.Bag.join(self,other,on_self,on_other=None)
dask.bag.Bag.map(self,func)
dask.bag.Bag.map_partitions(self,func)
dask.bag.Bag.max(self)
dask.bag.Bag.mean(self)
dask.bag.Bag.min(self)
dask.bag.Bag.pluck(self,key,default=no_default)
dask.bag.Bag.product(self,other)
dask.bag.Bag.reduction(self,perpartition,aggregate)
dask.bag.Bag.remove(self,predicate)
dask.bag.Bag.std(self,ddof=0)
dask.bag.Bag.sum(self)
dask.bag.Bag.take(self,k,compute=True)
dask.bag.Bag.to_dataframe(self,columns=None)
dask.bag.Bag.to_textfiles(self,path,name_function=str)
dask.bag.Bag.topk(self,k,key=None)
dask.bag.Bag.var(self,ddof=0)
dask.bag.Item(self,dsk,key)
dask.bag.Item.apply(self,func)
dask.bag.Item.compute(self,**kwargs)
dask.bag.concat(bags)
dask.bag.core.Bag(self,dsk,name,npartitions)
dask.bag.core.Bag.__init__(self,dsk,name,npartitions)
dask.bag.core.Bag.__iter__(self)
dask.bag.core.Bag._args(self)
dask.bag.core.Bag._keys(self)
dask.bag.core.Bag._visualize(self,optimize_graph=False)
dask.bag.core.Bag.all(self)
dask.bag.core.Bag.any(self)
dask.bag.core.Bag.compute(self,**kwargs)
dask.bag.core.Bag.concat(self)
dask.bag.core.Bag.count(self)
dask.bag.core.Bag.distinct(self)
dask.bag.core.Bag.filter(self,predicate)
dask.bag.core.Bag.fold(self,binop,combine=None,initial=None)
dask.bag.core.Bag.foldby(self,key,binop,initial=no_default,combine=None,combine_initial=no_default)
dask.bag.core.Bag.frequencies(self)
dask.bag.core.Bag.from_filenames(cls,*args,**kwargs)
dask.bag.core.Bag.from_sequence(cls,*args,**kwargs)
dask.bag.core.Bag.groupby(self,grouper,npartitions=None,blocksize=2**20)
dask.bag.core.Bag.join(self,other,on_self,on_other=None)
dask.bag.core.Bag.map(self,func)
dask.bag.core.Bag.map_partitions(self,func)
dask.bag.core.Bag.max(self)
dask.bag.core.Bag.mean(self)
dask.bag.core.Bag.min(self)
dask.bag.core.Bag.pluck(self,key,default=no_default)
dask.bag.core.Bag.product(self,other)
dask.bag.core.Bag.reduction(self,perpartition,aggregate)
dask.bag.core.Bag.remove(self,predicate)
dask.bag.core.Bag.std(self,ddof=0)
dask.bag.core.Bag.sum(self)
dask.bag.core.Bag.take(self,k,compute=True)
dask.bag.core.Bag.to_dataframe(self,columns=None)
dask.bag.core.Bag.to_textfiles(self,path,name_function=str)
dask.bag.core.Bag.topk(self,k,key=None)
dask.bag.core.Bag.var(self,ddof=0)
dask.bag.core.Item(self,dsk,key)
dask.bag.core.Item.__init__(self,dsk,key)
dask.bag.core.Item.apply(self,func)
dask.bag.core.Item.compute(self,**kwargs)
dask.bag.core.StringAccessor(self,bag)
dask.bag.core.StringAccessor.__dir__(self)
dask.bag.core.StringAccessor.__getattr__(self,key)
dask.bag.core.StringAccessor.__init__(self,bag)
dask.bag.core.StringAccessor._strmap(self,key,*args,**kwargs)
dask.bag.core.StringAccessor.match(self,pattern)
dask.bag.core._get_key(bucket_name,conn_args,key_name)
dask.bag.core._get_s3_bucket(bucket_name,aws_access_key,aws_secret_key,connection,anon)
dask.bag.core._parse_s3_URI(bucket_name,paths)
dask.bag.core.bz2_stream(compressed,chunksize=100000)
dask.bag.core.collect(grouper,group,p,barrier_token)
dask.bag.core.concat(bags)
dask.bag.core.dictitems(d)
dask.bag.core.from_filenames(filenames)
dask.bag.core.from_hdfs(path,hdfs=None,host='localhost',port='50070',user_name=None)
dask.bag.core.from_s3(bucket_name,paths='*',aws_access_key=None,aws_secret_key=None,connection=None,anon=False)
dask.bag.core.from_sequence(seq,partition_size=None,npartitions=None)
dask.bag.core.from_url(urls)
dask.bag.core.get(dsk,keys,get=None,**kwargs)
dask.bag.core.inline_singleton_lists(dsk)
dask.bag.core.lazify(dsk)
dask.bag.core.lazify_task(task,start=True)
dask.bag.core.list2(seq)
dask.bag.core.normalize_s3_names(bucket_name,paths,conn_args)
dask.bag.core.optimize(dsk,keys)
dask.bag.core.partition(grouper,sequence,npartitions,p,nelements=2**20)
dask.bag.core.reify(seq)
dask.bag.core.robust_wraps(wrapper)
dask.bag.core.stream_decompress(fmt,data)
dask.bag.core.takes_multiple_arguments(func)
dask.bag.core.to_textfiles(b,path,name_function=str)
dask.bag.core.write(data,filename)
dask.bag.from_filenames(filenames)
dask.bag.from_hdfs(path,hdfs=None,host='localhost',port='50070',user_name=None)
dask.bag.from_s3(bucket_name,paths='*',aws_access_key=None,aws_secret_key=None,connection=None,anon=False)
dask.bag.from_sequence(seq,partition_size=None,npartitions=None)
dask.bag.from_url(urls)
dask.bag.get(dsk,keys,get=None,**kwargs)
dask.bag.to_textfiles(b,path,name_function=str)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.6.0/lib/python3.6/site-packages/dask/distributed/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.6.0/lib/python3.6/site-packages/dask/distributed/client.py----------------------------------------
A:dask.distributed.client.context->zmq.Context()
A:dask.distributed.client.address->address.encode().encode()
A:dask.distributed.client.self.socket->zmq.Context().socket(zmq.DEALER)
A:dask.distributed.client.(header2, payload2)->self.recv_from_scheduler()
A:dask.distributed.client.header['timestamp']->datetime.datetime.utcnow()
A:dask.distributed.client.(header, payload)->self.recv_from_scheduler()
A:dask.distributed.client.header->scheduler.pickle.loads(header)
A:dask.distributed.client.loads->scheduler.pickle.loads(header).get('loads', pickle.loads)
A:dask.distributed.client.payload->loads(payload)
dask.distributed.Client(self,scheduler,address=None)
dask.distributed.Client.close(self,close_scheduler=False)
dask.distributed.Client.close_scheduler(self)
dask.distributed.Client.get(self,dsk,keys)
dask.distributed.Client.get_collection(self,name)
dask.distributed.Client.get_registered_workers(self)
dask.distributed.Client.recv_from_scheduler(self)
dask.distributed.Client.register_client(self)
dask.distributed.Client.scheduler_status(self)
dask.distributed.Client.send_to_scheduler(self,header,payload)
dask.distributed.Client.set_collection(self,name,collection)
dask.distributed.client.Client(self,scheduler,address=None)
dask.distributed.client.Client.__init__(self,scheduler,address=None)
dask.distributed.client.Client.close(self,close_scheduler=False)
dask.distributed.client.Client.close_scheduler(self)
dask.distributed.client.Client.get(self,dsk,keys)
dask.distributed.client.Client.get_collection(self,name)
dask.distributed.client.Client.get_registered_workers(self)
dask.distributed.client.Client.recv_from_scheduler(self)
dask.distributed.client.Client.register_client(self)
dask.distributed.client.Client.scheduler_status(self)
dask.distributed.client.Client.send_to_scheduler(self,header,payload)
dask.distributed.client.Client.set_collection(self,name,collection)
dask.distributed.client.log(*args)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.6.0/lib/python3.6/site-packages/dask/distributed/worker.py----------------------------------------
A:dask.distributed.worker.(exc_type, exc_value, exc_traceback)->sys.exc_info()
A:dask.distributed.worker.tb->''.join(traceback.format_tb(exc_traceback))
A:dask.distributed.worker.scheduler->scheduler.encode().encode()
A:dask.distributed.worker.self.pool->ThreadPool(nthreads)
A:dask.distributed.worker.self.context->zmq.Context()
A:dask.distributed.worker.self.to_workers->self.context.socket(zmq.ROUTER)
A:dask.distributed.worker.port_to_workers->self.to_workers.bind_to_random_port('tcp://' + bind_to_workers)
A:dask.distributed.worker.self.address->('tcp://%s:%s' % (self.hostname, port_to_workers)).encode()
A:dask.distributed.worker.self.dealers->dict()
A:dask.distributed.worker.self.lock->Lock()
A:dask.distributed.worker.self.queues->dict()
A:dask.distributed.worker.self.to_scheduler->self.context.socket(zmq.DEALER)
A:dask.distributed.worker.self._listen_scheduler_thread->Thread(target=self.listen_to_scheduler)
A:dask.distributed.worker.self._listen_workers_thread->Thread(target=self.listen_to_workers)
A:dask.distributed.worker.self._heartbeat_thread->Thread(target=self.heart, kwargs={'pulse': heartbeat})
A:dask.distributed.worker.self._heartbeat_thread.event->Event()
A:dask.distributed.worker.loads->pickle.loads(header).get('loads', pickle.loads)
A:dask.distributed.worker.payload->loads(payload)
A:dask.distributed.worker.queue->Queue()
A:dask.distributed.worker.header['timestamp']->datetime.datetime.utcnow()
A:dask.distributed.worker.dumps->pickle.loads(header).get('dumps', pickle_dumps)
A:dask.distributed.worker.sock->self.context.socket(zmq.DEALER)
A:dask.distributed.worker.(header, payload)->self.to_scheduler.recv_multipart()
A:dask.distributed.worker.header->pickle.loads(header)
A:dask.distributed.worker.future->self.pool.apply_async(function, args=(header, payload))
A:dask.distributed.worker.(address, header, payload)->self.to_workers.recv_multipart()
A:dask.distributed.worker.qkey->str(uuid.uuid1())
A:dask.distributed.worker.start->time()
A:dask.distributed.worker.worker->random.choice(tuple(locs))
A:dask.distributed.worker.result->core.get(self.data, task)
A:dask.distributed.worker.end->time()
dask.distributed.Worker(self,scheduler,data=None,nthreads=100,hostname=None,port_to_workers=None,bind_to_workers='*',block=False,heartbeat=5)
dask.distributed.Worker.__del__(self)
dask.distributed.Worker.block(self)
dask.distributed.Worker.close(self)
dask.distributed.Worker.close_from_scheduler(self,header,payload)
dask.distributed.Worker.collect(self,locations)
dask.distributed.Worker.compute(self,header,payload)
dask.distributed.Worker.delitem(self,header,payload)
dask.distributed.Worker.getitem_ack(self,header,payload)
dask.distributed.Worker.getitem_scheduler(self,header,payload)
dask.distributed.Worker.getitem_worker(self,header,payload)
dask.distributed.Worker.heart(self,pulse=5)
dask.distributed.Worker.listen_to_scheduler(self)
dask.distributed.Worker.listen_to_workers(self)
dask.distributed.Worker.send_to_scheduler(self,header,payload)
dask.distributed.Worker.send_to_worker(self,address,header,payload)
dask.distributed.Worker.setitem(self,header,payload)
dask.distributed.Worker.status_to_scheduler(self,header,payload)
dask.distributed.Worker.status_to_worker(self,header,payload)
dask.distributed.worker.Worker(self,scheduler,data=None,nthreads=100,hostname=None,port_to_workers=None,bind_to_workers='*',block=False,heartbeat=5)
dask.distributed.worker.Worker.__del__(self)
dask.distributed.worker.Worker.__init__(self,scheduler,data=None,nthreads=100,hostname=None,port_to_workers=None,bind_to_workers='*',block=False,heartbeat=5)
dask.distributed.worker.Worker.block(self)
dask.distributed.worker.Worker.close(self)
dask.distributed.worker.Worker.close_from_scheduler(self,header,payload)
dask.distributed.worker.Worker.collect(self,locations)
dask.distributed.worker.Worker.compute(self,header,payload)
dask.distributed.worker.Worker.delitem(self,header,payload)
dask.distributed.worker.Worker.getitem_ack(self,header,payload)
dask.distributed.worker.Worker.getitem_scheduler(self,header,payload)
dask.distributed.worker.Worker.getitem_worker(self,header,payload)
dask.distributed.worker.Worker.heart(self,pulse=5)
dask.distributed.worker.Worker.listen_to_scheduler(self)
dask.distributed.worker.Worker.listen_to_workers(self)
dask.distributed.worker.Worker.send_to_scheduler(self,header,payload)
dask.distributed.worker.Worker.send_to_worker(self,address,header,payload)
dask.distributed.worker.Worker.setitem(self,header,payload)
dask.distributed.worker.Worker.status_to_scheduler(self,header,payload)
dask.distributed.worker.Worker.status_to_worker(self,header,payload)
dask.distributed.worker.log(*args)
dask.distributed.worker.logerrors()
dask.distributed.worker.pickle_dumps(obj)
dask.distributed.worker.status()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.6.0/lib/python3.6/site-packages/dask/distributed/ipython_utils.py----------------------------------------
A:dask.distributed.ipython_utils.scheduler->Scheduler()
A:dask.distributed.ipython_utils.worker->Worker(scheduler_address)
A:dask.distributed.ipython_utils.(to_clients, to_workers)->scheduler_target.apply_sync(start_scheduler)
A:dask.distributed.ipython_utils.workers_targets->workers_targets.apply_sync(start_worker, to_workers).apply_sync(start_worker, to_workers)
A:dask.distributed.ipython_utils.dask_client->Client(to_clients)
dask.distributed.dask_client_from_ipclient(ipclient)
dask.distributed.ipython_utils.dask_client_from_ipclient(ipclient)

