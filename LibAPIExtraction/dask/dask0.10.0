
----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/compatibility.py----------------------------------------
A:dask.compatibility.f->gzip.GzipFile(fileobj=bio, mode='w')
A:dask.compatibility.result->BytesIO().read()
A:dask.compatibility.bio->BytesIO()
A:dask.compatibility.self.__obj->gzip.GzipFile(*args, **kwargs)
dask.compatibility.bind_method(cls,name,func)
dask.compatibility.getargspec(func)
dask.compatibility.skip(func)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/order.py----------------------------------------
A:dask.order.dependencies->dict(((k, get_dependencies(dsk, k)) for k in dsk))
A:dask.order.dependents->reverse_dict(dependencies)
A:dask.order.ndeps->ndependents(dependencies, dependents)
A:dask.order.maxes->child_max(dependencies, dependents, ndeps)
A:dask.order.result->dict()
A:dask.order.num_needed->dict(((k, len(v)) for (k, v) in dependencies.items()))
A:dask.order.current->set((k for (k, v) in num_needed.items() if v == 0))
A:dask.order.key->set((k for (k, v) in num_needed.items() if v == 0)).pop()
A:dask.order.stack->sorted(roots, key=key)
A:dask.order.seen->set()
A:dask.order.item->sorted(roots, key=key).pop()
A:dask.order.deps->sorted(deps, key=key)
dask.order.child_max(dependencies,dependents,scores)
dask.order.dfs(dependencies,dependents,key=lambdax:x)
dask.order.inc(x)
dask.order.ndependents(dependencies,dependents)
dask.order.order(dsk,dependencies=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/optimize.py----------------------------------------
A:dask.optimize.out->dict()
A:dask.optimize.seen->set()
A:dask.optimize.dependencies->dict(((k, get_dependencies(dsk2, k)) for k in dsk2))
A:dask.optimize.stack->list(set(flatten(keys)))
A:dask.optimize.key->list(set(flatten(keys))).pop()
A:dask.optimize.deps->tuple(get_dependencies(dsk2, key2, True))
A:dask.optimize.keys->set(keys)
A:dask.optimize.unfusible->set()
A:dask.optimize.parent2child->dict(map(reversed, child2parent.items()))
A:dask.optimize.(child, parent)->child2parent.popitem()
A:dask.optimize.parent->chain.pop()
A:dask.optimize.child->chain.pop()
A:dask.optimize.fused->set()
A:dask.optimize.val->subs(val, item, keysubs[item])
A:dask.optimize.replaceorder->toposort(dict(((k, dsk[k]) for k in keys if k in dsk)), dependencies=dependencies)
A:dask.optimize.output->set(output)
A:dask.optimize.fast_functions->set(fast_functions)
A:dask.optimize.dependents->reverse_dict(dependencies)
A:dask.optimize.aliases->set((k for (k, task) in dsk.items() if ishashable(task) and task in dsk))
A:dask.optimize.roots->set((k for (k, v) in dependents.items() if not v))
A:dask.optimize.dsk2->dict()
A:dask.optimize.dsk3->dict().copy()
A:dask.optimize.head_type->type(term1)
A:dask.optimize.pot1->preorder_traversal(term1)
A:dask.optimize.pot2->preorder_traversal(term2)
A:dask.optimize.v->core.subs.get(d, None)
A:dask.optimize.deps2->tuple(deps2)
A:dask.optimize.dep_dict1->dependency_dict(dsk1)
A:dask.optimize.possible_matches->_possible_matches(dep_dict1, deps, subs)
A:dask.optimize.dsk2_topo->toposort(dsk2)
A:dask.optimize.sd->_sync_keys(dsk1, dsk2, dsk2_topo)
A:dask.optimize.new_dsk->dsk1.copy()
A:dask.optimize.new_key->next(merge_sync.names)
A:dask.optimize.task->subs(task, a, b)
A:dask.optimize.dsk2[k]->merge(v, dsk[v[1]])
dask.optimize._possible_matches(dep_dict,deps,subs)
dask.optimize._sync_keys(dsk1,dsk2,dsk2_topo)
dask.optimize.cull(dsk,keys)
dask.optimize.dealias(dsk,keys=None)
dask.optimize.dependency_dict(dsk)
dask.optimize.equivalent(term1,term2,subs=None)
dask.optimize.functions_of(task)
dask.optimize.fuse(dsk,keys=None,dependencies=None)
dask.optimize.fuse_getitem(dsk,func,place)
dask.optimize.fuse_selections(dsk,head1,head2,merge)
dask.optimize.identity(x)
dask.optimize.inline(dsk,keys=None,inline_constants=True,dependencies=None)
dask.optimize.inline_functions(dsk,output,fast_functions=None,inline_constants=False,dependencies=None)
dask.optimize.merge_sync(dsk1,dsk2)
dask.optimize.sync_keys(dsk1,dsk2)
dask.optimize.unwrap_partial(func)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/core.py----------------------------------------
A:dask.core.val->func(*results)
A:dask.core.cycle->'->'.join(cycle)
A:dask.core.key->args.pop()
A:dask.core.arg->subs(arg, key, val)
A:dask.core.dependencies->dict(((k, get_dependencies(dsk, k)) for k in dsk))
A:dask.core.dependents->reverse_dict(dependencies)
A:dask.core.result->dict(((t, set()) for t in terms))
A:dask.core.completed->set()
A:dask.core.seen->set()
dask.core._deps(dsk,arg)
dask.core._get_nonrecursive(d,x,maxdepth=1000)
dask.core._get_recursive(d,x)
dask.core._toposort(dsk,keys=None,returncycle=False,dependencies=None)
dask.core.flatten(seq)
dask.core.get(d,x,recursive=False)
dask.core.get_dependencies(dsk,task,as_list=False)
dask.core.get_deps(dsk)
dask.core.getcycle(d,keys)
dask.core.has_tasks(dsk,x)
dask.core.inc(x)
dask.core.isdag(d,keys)
dask.core.ishashable(x)
dask.core.istask(x)
dask.core.list2(L)
dask.core.preorder_traversal(task)
dask.core.quote(x)
dask.core.reverse_dict(d)
dask.core.subs(task,key,val)
dask.core.toposort(dsk,dependencies=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/utils.py----------------------------------------
A:dask.utils.system_encoding->getdefaultencoding()
A:dask.utils.(handle, filename)->tempfile.mkstemp(extension, dir=dir)
A:dask.utils.dirname->tempfile.mkdtemp(dir=dir)
A:dask.utils.f->open(filename, 'w' + mode)
A:dask.utils.extension->os.path.splitext(filename)[-1].strip('.')
A:dask.utils.compression->infer_compression(filename)
A:dask.utils.boms->set((codecs.BOM_UTF16, codecs.BOM_UTF16_BE, codecs.BOM_UTF16_LE))
A:dask.utils.bom->open(filename, 'w' + mode).read(2)
A:dask.utils.linesep->str(linesep)
A:dask.utils.bin_linesep->get_bin_linesep(encoding, linesep)
A:dask.utils.bin_linesep_len->len(bin_linesep)
A:dask.utils.start->max(0, start - bin_linesep_len)
A:dask.utils.buf->open(filename, 'w' + mode).read(buffersize)
A:dask.utils.line->next(fbw)
A:dask.utils.bin_line_len->len(line.encode(encoding))
A:dask.utils.seq->list(map(concrete, seq))
A:dask.utils.p->list(p)
A:dask.utils.cp->numpy.cumsum([0] + p)
A:dask.utils.random_state->numpy.random.RandomState(random_state)
A:dask.utils.x->numpy.random.RandomState(random_state).random_sample(n)
A:dask.utils.out->numpy.empty(n, dtype='i1')
A:dask.utils.seeds->set(random_state.randint(big_n, size=n))
A:dask.utils.result->open(filename, 'w' + mode).seek(0, 2)
A:dask.utils.ONE_ARITY_BUILTINS->set([abs, all, any, bool, bytearray, bytes, callable, chr, classmethod, complex, dict, dir, enumerate, eval, float, format, frozenset, hash, hex, id, int, iter, len, list, max, min, next, oct, open, ord, range, repr, reversed, round, set, slice, sorted, staticmethod, str, sum, tuple, type, vars, zip])
A:dask.utils.MULTI_ARITY_BUILTINS->set([compile, delattr, divmod, filter, getattr, hasattr, isinstance, issubclass, map, pow, setattr])
A:dask.utils.spec->getargspec(func)
A:dask.utils.typ->type(arg)
A:dask.utils.original_method->getattr(original_klass, method_name)
A:dask.utils.args->''.join(['        * {0}\n'.format(a) for a in not_supported])
A:dask.utils.doc->'\n'.join([_skip_doctest(line) for line in doc.split('\n')])
A:dask.utils.msg->"Base package doesn't support '{0}'.".format(method_name)
A:dask.utils.L->list(tup)
dask.utils.Dispatch(self)
dask.utils.Dispatch.__init__(self)
dask.utils.Dispatch.register(self,type,func)
dask.utils.IndexCallable(self,fn)
dask.utils.IndexCallable.__getitem__(self,key)
dask.utils.IndexCallable.__init__(self,fn)
dask.utils._skip_doctest(line)
dask.utils.concrete(seq)
dask.utils.deepmap(func,*seqs)
dask.utils.derived_from(original_klass,version=None,ua_args=[])
dask.utils.different_seeds(n,random_state=None)
dask.utils.digit(n,k,base)
dask.utils.ensure_bytes(s)
dask.utils.ensure_not_exists(filename)
dask.utils.file_size(fn,compression=None)
dask.utils.filetext(text,extension='',open=open,mode='w')
dask.utils.filetexts(d,open=open,mode='t')
dask.utils.funcname(func,full=False)
dask.utils.get_bin_linesep(encoding,linesep)
dask.utils.get_bom(fn,compression=None)
dask.utils.ignoring(*exceptions)
dask.utils.infer_compression(filename)
dask.utils.insert(tup,loc,val)
dask.utils.is_integer(i)
dask.utils.open(filename,mode='rb',compression=None,**kwargs)
dask.utils.pseudorandom(n,p,random_state=None)
dask.utils.raises(err,lamda)
dask.utils.repr_long_list(seq)
dask.utils.skip(func)
dask.utils.takes_multiple_arguments(func)
dask.utils.textblock(filename,start,end,compression=None,encoding=system_encoding,linesep=os.linesep,buffersize=4096)
dask.utils.tmpdir(dir=None)
dask.utils.tmpfile(extension='',dir=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/context.py----------------------------------------
A:dask.context._globals->defaultdict(lambda : None)
A:dask.context._globals['callbacks']->set()
A:dask.context.self.old->defaultdict(lambda : None).copy()
dask.context.set_options(self,**kwargs)
dask.context.set_options.__enter__(self)
dask.context.set_options.__exit__(self,type,value,traceback)
dask.context.set_options.__init__(self,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/callbacks.py----------------------------------------
A:dask.callbacks.self._cm->add_callbacks(self)
A:dask.callbacks.self.old->_globals['callbacks'].copy()
dask.callbacks.Callback(self,start=None,start_state=None,pretask=None,posttask=None,finish=None)
dask.callbacks.Callback.__enter__(self)
dask.callbacks.Callback.__exit__(self,*args)
dask.callbacks.Callback.__init__(self,start=None,start_state=None,pretask=None,posttask=None,finish=None)
dask.callbacks.Callback._callback(self)
dask.callbacks.Callback.register(self)
dask.callbacks.Callback.unregister(self)
dask.callbacks.add_callbacks(self,*args)
dask.callbacks.add_callbacks.__enter__(self)
dask.callbacks.add_callbacks.__exit__(self,type,value,traceback)
dask.callbacks.add_callbacks.__init__(self,*args)
dask.callbacks.normalize_callback(cb)
dask.callbacks.unpack_callbacks(cbs)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/dot.py----------------------------------------
A:dask.dot.head->funcname(task[0])
A:dask.dot._HASHPAT->re.compile('([0-9a-z]{32})')
A:dask.dot._UUIDPAT->re.compile('([0-9a-z]{8}-[0-9a-z]{4}-[0-9a-z]{4}-[0-9a-z]{4}-[0-9a-z]{12})')
A:dask.dot.s->s.replace(h, label).replace(h, label)
A:dask.dot.m->re.search(pattern, s)
A:dask.dot.n->cache.get(h, len(cache))
A:dask.dot.label->'#{0}'.format(n)
A:dask.dot.g->to_graphviz(dsk, **kwargs)
A:dask.dot.seen->set()
A:dask.dot.k_name->name(k)
A:dask.dot.func_name->name((k, 'function'))
A:dask.dot.dep_name->name(dep)
A:dask.dot.IPYTHON_IMAGE_FORMATS->frozenset(['jpeg', 'png'])
A:dask.dot.IPYTHON_NO_DISPLAY_FORMATS->frozenset(['dot', 'pdf'])
A:dask.dot.(filename, format)->os.path.splitext(filename)
A:dask.dot.format->format[1:].lower()
A:dask.dot.data->to_graphviz(dsk, **kwargs).pipe(format=format)
A:dask.dot.display_cls->_get_display_cls(format)
A:dask.dot.full_filename->'.'.join([filename, format])
dask.dot._get_display_cls(format)
dask.dot.dot_graph(dsk,filename='mydask',format=None,**kwargs)
dask.dot.has_sub_tasks(task)
dask.dot.label(x,cache=None)
dask.dot.name(x)
dask.dot.task_label(task)
dask.dot.to_graphviz(dsk,data_attributes=None,function_attributes=None,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/distributed.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/async.py----------------------------------------
A:dask.async.cache->dict()
A:dask.async.data_keys->set()
A:dask.async.dsk2->dsk.copy().copy()
A:dask.async.dependencies->dict(((k, get_dependencies(dsk2, k)) for k in dsk))
A:dask.async.waiting->dict(((k, v) for (k, v) in waiting.items() if v))
A:dask.async.dependents->reverse_dict(dependencies)
A:dask.async.waiting_data->dict(((k, v.copy()) for (k, v) in dependents.items() if v))
A:dask.async.ready_set->set([k for (k, v) in waiting.items() if not v])
A:dask.async.ready->sorted(ready_set, key=sortkey, reverse=True)
A:dask.async.result->_execute_task(task, data)
A:dask.async.id->get_id()
A:dask.async.(exc_type, exc_value, exc_traceback)->sys.exc_info()
A:dask.async.tb->''.join(traceback.format_tb(exc_traceback))
A:dask.async.(start_cbs, start_state_cbs, pretask_cbs, posttask_cbs, finish_cbs)->unpack_callbacks(callbacks)
A:dask.async.result_flat->set([result])
A:dask.async.results->set(result_flat)
A:dask.async.dsk->dsk.copy().copy()
A:dask.async.(dsk, dependencies)->cull(dsk, list(results))
A:dask.async.keyorder->order(dsk)
A:dask.async.state->start_state_from_dask(dsk, cache=cache, sortkey=keyorder.get)
A:dask.async.rerun_exceptions_locally->context._globals.get('rerun_exceptions_locally', False)
A:dask.async.key->state['ready'].pop()
A:dask.async.data->dict(((dep, state['cache'][dep]) for dep in get_dependencies(dsk, key)))
A:dask.async.(key, res, tb, worker_id)->Queue().get()
A:dask.async.queue->Queue()
A:dask.async.exceptions->dict()
A:dask.async.typ->type(exc.__class__.__name__, (RemoteException, type(exc)), {'exception_type': type(exc)})
dask.async.RemoteException(self,exception,traceback)
dask.async.RemoteException.__dir__(self)
dask.async.RemoteException.__getattr__(self,key)
dask.async.RemoteException.__init__(self,exception,traceback)
dask.async.RemoteException.__str__(self)
dask.async._execute_task(arg,cache,dsk=None)
dask.async.apply_sync(func,args=(),kwds={})
dask.async.default_get_id()
dask.async.execute_task(key,task,data,queue,get_id,raise_on_exception=False)
dask.async.finish_task(dsk,key,state,results,sortkey,delete=True,release_data=release_data)
dask.async.get_async(apply_async,num_workers,dsk,result,cache=None,queue=None,get_id=default_get_id,raise_on_exception=False,rerun_exceptions_locally=None,callbacks=None,**kwargs)
dask.async.get_sync(dsk,keys,**kwargs)
dask.async.inc(x)
dask.async.nested_get(ind,coll)
dask.async.release_data(key,state,delete=True)
dask.async.remote_exception(exc,tb)
dask.async.sortkey(item)
dask.async.start_state_from_dask(dsk,cache=None,sortkey=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/base.py----------------------------------------
A:dask.base.warn->DeprecationWarning('``_visualize`` is deprecated, use ``visualize`` instead.')
A:dask.base.dsk2->cls._optimize(dsk, keys, **kwargs)
A:dask.base.meth->'__{0}__'.format(name)
A:dask.base.rmeth->'__r{0}__'.format(name)
A:dask.base.groups->groupby(attrgetter('_optimize'), variables)
A:dask.base.dsk->merge(dsks)
A:dask.base.results->get(dsk, keys, **kwargs)
A:dask.base.results_iter->iter(results)
A:dask.base.filename->kwargs.pop('filename', 'mydask')
A:dask.base.optimize_graph->kwargs.pop('optimize_graph', False)
A:dask.base.first->getattr(func, 'first', None)
A:dask.base.normalize_token->Dispatch()
A:dask.base.data->md5(x.copy().ravel().view('i1').data).hexdigest()
dask.base.Base(object)
dask.base.Base._bind_operator(cls,op)
dask.base.Base._get(cls,dsk,keys,get=None,**kwargs)
dask.base.Base._get_binary_operator(cls,op,inv=False)
dask.base.Base._get_unary_operator(cls,op)
dask.base.Base._visualize(self,filename='mydask',format=None,optimize_graph=False)
dask.base.Base.compute(self,**kwargs)
dask.base.Base.visualize(self,filename='mydask',format=None,optimize_graph=False,**kwargs)
dask.base.compute(*args,**kwargs)
dask.base.normalize_base(b)
dask.base.normalize_dict(d)
dask.base.normalize_function(func)
dask.base.normalize_object(o)
dask.base.normalize_seq(seq)
dask.base.tokenize(*args,**kwargs)
dask.base.visualize(*args,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/rewrite.py----------------------------------------
A:dask.rewrite.self._stack->deque([END])
A:dask.rewrite.subterms->args(self.term)
A:dask.rewrite.self.term->self._stack.pop()
A:dask.rewrite.VAR->Token('?')
A:dask.rewrite.END->Token('end')
A:dask.rewrite.self.vars->tuple(sorted(set(self._varlist)))
A:dask.rewrite.term->rule.subs(sd)
A:dask.rewrite.self._net->Node()
A:dask.rewrite.ind->len(self.rules)
A:dask.rewrite.curr_node.edges[t]->Node()
A:dask.rewrite.S->Traverser(term)
A:dask.rewrite.subs->_process_match(rule, syms)
A:dask.rewrite.stack->deque()
A:dask.rewrite.n->N.edges.get(VAR, None)
A:dask.rewrite.(S, N, matches)->deque().pop()
dask.rewrite.Node(cls,edges=None,patterns=None)
dask.rewrite.Node.__new__(cls,edges=None,patterns=None)
dask.rewrite.Node.edges(self)
dask.rewrite.Node.patterns(self)
dask.rewrite.RewriteRule(self,lhs,rhs,vars=())
dask.rewrite.RewriteRule.__init__(self,lhs,rhs,vars=())
dask.rewrite.RewriteRule.__repr__(self)
dask.rewrite.RewriteRule.__str__(self)
dask.rewrite.RewriteRule._apply(self,sub_dict)
dask.rewrite.RuleSet(self,*rules)
dask.rewrite.RuleSet.__init__(self,*rules)
dask.rewrite.RuleSet._rewrite(self,term)
dask.rewrite.RuleSet.add(self,rule)
dask.rewrite.RuleSet.iter_matches(self,term)
dask.rewrite.RuleSet.rewrite(self,task,strategy='bottom_up')
dask.rewrite.Token(self,name)
dask.rewrite.Token.__init__(self,name)
dask.rewrite.Token.__repr__(self)
dask.rewrite.Traverser(self,term,stack=None)
dask.rewrite.Traverser.__init__(self,term,stack=None)
dask.rewrite.Traverser.__iter__(self)
dask.rewrite.Traverser.copy(self)
dask.rewrite.Traverser.current(self)
dask.rewrite.Traverser.next(self)
dask.rewrite.Traverser.skip(self)
dask.rewrite._bottom_up(net,term)
dask.rewrite._match(S,N)
dask.rewrite._process_match(rule,syms)
dask.rewrite._top_level(net,term)
dask.rewrite.args(task)
dask.rewrite.head(task)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/imperative.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/cache.py----------------------------------------
A:dask.cache.cache->cachey.Cache(cache, *args, **kwargs)
A:dask.cache.self.starttimes->dict()
A:dask.cache.self.durations->dict()
A:dask.cache.self.starttimes[key]->default_timer()
dask.cache.Cache(self,cache,*args,**kwargs)
dask.cache.Cache.__init__(self,cache,*args,**kwargs)
dask.cache.Cache._finish(self,dsk,state,errored)
dask.cache.Cache._posttask(self,key,value,dsk,state,id)
dask.cache.Cache._pretask(self,key,dsk,state)
dask.cache.Cache._start(self,dsk)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/delayed.py----------------------------------------
A:dask.delayed.out->list(zip(*ls))
A:dask.delayed.is_top->kwargs.get('is_top', True)
A:dask.delayed.name->kwargs.pop('dask_key_name', None)
A:dask.delayed.keys->expr._keys()
A:dask.delayed.dsk->expr._optimize(expr.dask, keys)
A:dask.delayed.(args, dasks)->unzip(map(to_task_dasks, args), 2)
A:dask.delayed.args->list(args)
A:dask.delayed.dasks->flat_unique(dasks)
A:dask.delayed.(task, dasks)->to_task_dasks(obj)
A:dask.delayed._finalize->staticmethod(first)
A:dask.delayed._default_get->staticmethod(threaded.get)
A:dask.delayed.pure->kwargs.pop('pure', self.pure)
A:dask.delayed.func->delayed(apply, pure=pure)
A:dask.delayed.method->delayed(right(op) if inv else op, pure=True)
A:dask.delayed.dask_key_name->kwargs.pop('dask_key_name', None)
A:dask.delayed.(dask_kwargs, dasks2)->to_task_dasks(kwargs)
dask.delayed.Delayed(self,name,dasks)
dask.delayed.Delayed.__bool__(self)
dask.delayed.Delayed.__dir__(self)
dask.delayed.Delayed.__getattr__(self,attr)
dask.delayed.Delayed.__getstate__(self)
dask.delayed.Delayed.__hash__(self)
dask.delayed.Delayed.__init__(self,name,dasks)
dask.delayed.Delayed.__iter__(self)
dask.delayed.Delayed.__repr__(self)
dask.delayed.Delayed.__setattr__(self,attr,val)
dask.delayed.Delayed.__setitem__(self,index,val)
dask.delayed.Delayed.__setstate__(self,state)
dask.delayed.Delayed._get_binary_operator(cls,op,inv=False)
dask.delayed.Delayed._keys(self)
dask.delayed.Delayed._optimize(dsk,keys,**kwargs)
dask.delayed.Delayed.dask(self)
dask.delayed.Delayed.key(self)
dask.delayed.DelayedLeaf(self,obj,name=None,pure=False)
dask.delayed.DelayedLeaf.__getstate__(self)
dask.delayed.DelayedLeaf.__init__(self,obj,name=None,pure=False)
dask.delayed.DelayedLeaf.__setstate__(self,state)
dask.delayed.DelayedLeaf._data(self)
dask.delayed.DelayedLeaf._key(self)
dask.delayed.compute(*args,**kwargs)
dask.delayed.delayed(obj,name=None,pure=False)
dask.delayed.flat_unique(ls)
dask.delayed.right(method)
dask.delayed.to_task_dasks(expr,**kwargs)
dask.delayed.tokenize(*args,**kwargs)
dask.delayed.unzip(ls,nout)
dask.delayed.value(val,name=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/utils_test.py----------------------------------------
A:dask.utils_test.get->staticmethod(get)
A:dask.utils_test.result->self.get(d, [['badkey'], 'y'])
A:dask.utils_test.d->dict((('x%s' % (i + 1), (inc, 'x%s' % i)) for i in range(10000)))
dask.utils_test.GetFunctionTestMixin(object)
dask.utils_test.GetFunctionTestMixin.test_badkey(self)
dask.utils_test.GetFunctionTestMixin.test_data_not_in_dict_is_ok(self)
dask.utils_test.GetFunctionTestMixin.test_get(self)
dask.utils_test.GetFunctionTestMixin.test_get_stack_limit(self)
dask.utils_test.GetFunctionTestMixin.test_get_with_list(self)
dask.utils_test.GetFunctionTestMixin.test_get_with_list_top_level(self)
dask.utils_test.GetFunctionTestMixin.test_get_with_nested_list(self)
dask.utils_test.GetFunctionTestMixin.test_get_works_with_unhashables_in_values(self)
dask.utils_test.GetFunctionTestMixin.test_nested_badkey(self)
dask.utils_test.GetFunctionTestMixin.test_nested_tasks(self)
dask.utils_test.add(x,y)
dask.utils_test.inc(x)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/array/chunk.py----------------------------------------
A:dask.array.chunk.r->a_callable(x, *args, axis=axis, **kwargs)
A:dask.array.chunk.axes->range(x.ndim)
A:dask.array.chunk.r_slice->tuple()
A:dask.array.chunk.sum->keepdims_wrapper(np.sum)
A:dask.array.chunk.prod->keepdims_wrapper(np.prod)
A:dask.array.chunk.min->keepdims_wrapper(np.min)
A:dask.array.chunk.max->keepdims_wrapper(np.max)
A:dask.array.chunk.argmin->keepdims_wrapper(np.argmin)
A:dask.array.chunk.nanargmin->keepdims_wrapper(np.nanargmin)
A:dask.array.chunk.argmax->keepdims_wrapper(np.argmax)
A:dask.array.chunk.nanargmax->keepdims_wrapper(np.nanargmax)
A:dask.array.chunk.any->keepdims_wrapper(np.any)
A:dask.array.chunk.all->keepdims_wrapper(np.all)
A:dask.array.chunk.nansum->keepdims_wrapper(np.nansum)
A:dask.array.chunk.nanprod->keepdims_wrapper(nanprod)
A:dask.array.chunk.nancumprod->keepdims_wrapper(nancumprod)
A:dask.array.chunk.nancumsum->keepdims_wrapper(nancumsum)
A:dask.array.chunk.nanmin->keepdims_wrapper(np.nanmin)
A:dask.array.chunk.nanmax->keepdims_wrapper(np.nanmax)
A:dask.array.chunk.mean->keepdims_wrapper(np.mean)
A:dask.array.chunk.nanmean->keepdims_wrapper(np.nanmean)
A:dask.array.chunk.var->keepdims_wrapper(np.var)
A:dask.array.chunk.nanvar->keepdims_wrapper(np.nanvar)
A:dask.array.chunk.std->keepdims_wrapper(np.std)
A:dask.array.chunk.nanstd->keepdims_wrapper(np.nanstd)
A:dask.array.chunk.ind->tuple((slice(0, -(d % axes[i])) if d % axes[i] else slice(None, None) for (i, d) in enumerate(x.shape)))
A:dask.array.chunk.newshape->tuple(concat([(x.shape[i] / axes[i], axes[i]) for i in range(x.ndim)]))
A:dask.array.chunk.k->numpy.minimum(k, len(x))
dask.array.chunk.coarsen(reduction,x,axes,trim_excess=False)
dask.array.chunk.keepdims_wrapper(a_callable)
dask.array.chunk.topk(k,x)
dask.array.chunk.trim(x,axes=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/array/image.py----------------------------------------
A:dask.array.image.filenames->sorted(glob(filename))
A:dask.array.image.sample->preprocess(sample)
A:dask.array.image.dsk->dict(zip(keys, values))
dask.array.image.add_leading_dimension(x)
dask.array.image.imread(filename,imread=None,preprocess=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/array/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/array/rechunk.py----------------------------------------
A:dask.array.rechunk.old_chunks->normalize_chunks(old_chunks, shape)
A:dask.array.rechunk.new_chunks->list(old_chunks)
A:dask.array.rechunk.cmo->cumdims_label(old_chunks, 'o')
A:dask.array.rechunk.cmn->cumdims_label(new_chunks, 'n')
A:dask.array.rechunk.old_to_new->tuple((_intersect_1d(_breakpoints(cm[0], cm[1])) for cm in zip(cmo, cmn)))
A:dask.array.rechunk.cross1->tuple(product(*old_to_new))
A:dask.array.rechunk.cross->tuple(chain((tuple(product(*cr)) for cr in cross1)))
A:dask.array.rechunk.newlist->list(old)
A:dask.array.rechunk.shape->tuple(map(sum, old_chunks))
A:dask.array.rechunk.chunks->normalize_chunks(chunks, x.shape)
A:dask.array.rechunk.crossed->intersect_chunks(x.chunks, chunks)
A:dask.array.rechunk.x2->merge(x.dask, x2, intermediates)
A:dask.array.rechunk.intermediates->dict()
A:dask.array.rechunk.token->tokenize(x, chunks)
A:dask.array.rechunk.new_index->tuple(product(*(tuple(range(len(n))) for n in chunks)))
A:dask.array.rechunk.cr2->iter(cross1)
A:dask.array.rechunk.rec_cat_arg->numpy.empty(subdims).tolist()
A:dask.array.rechunk.inds_in_block->product(*[range(s) for s in subdims])
A:dask.array.rechunk.ind_slics->next(cr2)
A:dask.array.rechunk.ind_in_blk->next(inds_in_block)
A:dask.array.rechunk.temp->getitem(temp, ind_in_blk[i])
dask.array.rechunk(x,chunks)
dask.array.rechunk._breakpoints(cumold,cumnew)
dask.array.rechunk._intersect_1d(breaks)
dask.array.rechunk.blockdims_dict_to_tuple(old,new)
dask.array.rechunk.blockshape_dict_to_tuple(old_chunks,d)
dask.array.rechunk.cumdims_label(chunks,const)
dask.array.rechunk.intersect_chunks(old_chunks=None,new_chunks=None,shape=None)
dask.array.rechunk.rechunk(x,chunks)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/array/ghost.py----------------------------------------
A:dask.array.ghost.depth->dict(zip(range(ndim), depth))
A:dask.array.ghost.index->tuple(index)
A:dask.array.ghost.seq->list(product([k[0]], *[inds(i, ind) for (i, ind) in enumerate(k[1:])]))
A:dask.array.ghost.n->int(len(seq) / shape[0])
A:dask.array.ghost.dims->list(map(len, x.chunks))
A:dask.array.ghost.expand_key2->partial(expand_key, dims=dims)
A:dask.array.ghost.interior_keys->pipe(x._keys(), flatten, map(expand_key2), map(flatten), concat, list)
A:dask.array.ghost.token->tokenize(x, axes)
A:dask.array.ghost.frac_slice->fractional_slice(k, axes)
A:dask.array.ghost.chunks->list(x.chunks)
A:dask.array.ghost.(l, r)->_remove_ghost_boundaries(l, r, axis, depth)
A:dask.array.ghost.l->l.rechunk(tuple(lchunks)).rechunk(tuple(lchunks))
A:dask.array.ghost.r->r.rechunk(tuple(rchunks)).rechunk(tuple(rchunks))
A:dask.array.ghost.c->wrap.full(tuple(map(sum, chunks)), value, chunks=tuple(chunks), dtype=x._dtype)
A:dask.array.ghost.lchunks->list(l.chunks)
A:dask.array.ghost.rchunks->list(r.chunks)
A:dask.array.ghost.kind->dict(((i, kind) for i in range(x.ndim)))
A:dask.array.ghost.d->dict(zip(range(ndim), depth)).get(i, 0)
A:dask.array.ghost.this_kind->dict(((i, kind) for i in range(x.ndim))).get(i, 'none')
A:dask.array.ghost.x->x.rechunk(out_chunks).rechunk(out_chunks)
A:dask.array.ghost.depth2->coerce_depth(x.ndim, depth)
A:dask.array.ghost.boundary2->coerce_boundary(x.ndim, boundary)
A:dask.array.ghost.x2->boundaries(x, depth2, boundary2)
A:dask.array.ghost.x3->ghost_internal(x2, depth2)
A:dask.array.ghost.trim->dict(((k, v * 2 if boundary2.get(k, 'none') != 'none' else 0) for (k, v) in depth2.items()))
A:dask.array.ghost.x4->chunk.trim(x3, trim)
A:dask.array.ghost.empty_shape->list(x.shape)
A:dask.array.ghost.empty_chunks->list(x.chunks)
A:dask.array.ghost.empty->wrap.empty(empty_shape, chunks=empty_chunks, dtype=x.dtype)
A:dask.array.ghost.out_chunks->list(x.chunks)
A:dask.array.ghost.ax_chunks->list(out_chunks[k])
A:dask.array.ghost.g->ghost(x, depth=depth2, boundary=boundary2)
A:dask.array.ghost.g2->ghost(x, depth=depth2, boundary=boundary2).map_blocks(func, **kwargs)
A:dask.array.ghost.g3->add_dummy_padding(g2, depth2, boundary2)
A:dask.array.ghost.boundary->dict(zip(range(ndim), boundary))
dask.array.ghost._remove_ghost_boundaries(l,r,axis,depth)
dask.array.ghost.add_dummy_padding(x,depth,boundary)
dask.array.ghost.boundaries(x,depth=None,kind=None)
dask.array.ghost.coerce_boundary(ndim,boundary)
dask.array.ghost.coerce_depth(ndim,depth)
dask.array.ghost.constant(x,axis,depth,value)
dask.array.ghost.expand_key(k,dims)
dask.array.ghost.fractional_slice(task,axes)
dask.array.ghost.ghost(x,depth,boundary)
dask.array.ghost.ghost_internal(x,axes)
dask.array.ghost.map_overlap(x,func,depth,boundary=None,trim=True,**kwargs)
dask.array.ghost.nearest(x,axis,depth)
dask.array.ghost.periodic(x,axis,depth)
dask.array.ghost.reflect(x,axis,depth)
dask.array.ghost.reshape(shape,seq)
dask.array.ghost.trim_internal(x,axes)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/array/creation.py----------------------------------------
A:dask.array.creation.num->int(abs(range_ // step))
A:dask.array.creation.chunks->normalize_chunks(chunks, (num,))
A:dask.array.creation.dtype->kwargs.get('dtype', None)
dask.array.arange(*args,**kwargs)
dask.array.creation.arange(*args,**kwargs)
dask.array.creation.linspace(start,stop,num=50,chunks=None,dtype=None)
dask.array.linspace(start,stop,num=50,chunks=None,dtype=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/array/fft.py----------------------------------------
A:dask.array.fft.chunks->list(a.chunks)
A:dask.array.fft.fft->_fft_wrap(npfft.fft, np.complex_, _fft_out_chunks)
A:dask.array.fft.ifft->_fft_wrap(npfft.ifft, np.complex_, _fft_out_chunks)
A:dask.array.fft.rfft->_fft_wrap(npfft.rfft, np.complex_, _rfft_out_chunks)
A:dask.array.fft.irfft->_fft_wrap(npfft.irfft, np.float_, _irfft_out_chunks)
A:dask.array.fft.hfft->_fft_wrap(npfft.hfft, np.float_, _hfft_out_chunks)
A:dask.array.fft.ihfft->_fft_wrap(npfft.ihfft, np.complex_, _ihfft_out_chunks)
dask.array.fft._fft_out_chunks(a,n,axis)
dask.array.fft._fft_wrap(fft_func,dtype,out_chunk_fn)
dask.array.fft._hfft_out_chunks(a,n,axis)
dask.array.fft._ihfft_out_chunks(a,n,axis)
dask.array.fft._irfft_out_chunks(a,n,axis)
dask.array.fft._rfft_out_chunks(a,n,axis)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/array/core.py----------------------------------------
A:dask.array.core.b2->tuple((x for x in b if x is not None))
A:dask.array.core.b3->tuple((None if x is None else slice(None, None) for x in b if not isinstance(x, (int, long))))
A:dask.array.core.c->cov(x, y, rowvar)
A:dask.array.core.shapes->list(product(*chunks))
A:dask.array.core.starts->product(*cumdims)
A:dask.array.core.chunks->tuple((c if i == axis else (sum(c),) for (i, c) in enumerate(x.chunks)))
A:dask.array.core.keys->list(product([name], *[range(len(c)) for c in chunks]))
A:dask.array.core.A->map(leftfunc, A)
A:dask.array.core.B->map(rightfunc, B)
A:dask.array.core.L->Array(merge(tmp.dask, ldsk), left, chunks=tmp.chunks, dtype=ldt)
A:dask.array.core.g->dict(((k, set([d for (i, d) in v])) for (k, v) in g.items()))
A:dask.array.core.g2->valmap(consolidate, g2)
A:dask.array.core.numblocks->dict([(a.name, a.numblocks) for (a, _) in arginds])
A:dask.array.core.argpairs->list(partition(2, arrind_pairs))
A:dask.array.core.all_indices->pipe(argpairs, pluck(1), concat, set)
A:dask.array.core.dims->broadcast_dimensions(argpairs, numblocks)
A:dask.array.core.keytups->list(product(*[range(dims[i]) for i in out_indices]))
A:dask.array.core.dummies->dict(((i, list(range(dims[i]))) for i in dummy_indices))
A:dask.array.core.tups->lol_tuples((arg,), ind, kd, dummies)
A:dask.array.core.tups2->zero_broadcast_dimensions(tups, numblocks[arg])
A:dask.array.core.arrays->concrete(arrays)
A:dask.array.core.name->kwargs.pop('name', None)
A:dask.array.core.dtype->numpy.result_type(m, y, np.float64)
A:dask.array.core.drop_axis->kwargs.pop('drop_axis', [])
A:dask.array.core.new_axis->kwargs.pop('new_axis', [])
A:dask.array.core.argindsstr->list(concat([(a.name, ind) for (a, ind) in arginds]))
A:dask.array.core.dsk->dict(zip(keys, values))
A:dask.array.core.spec->getargspec(func)
A:dask.array.core.new_key->list(key)
A:dask.array.core.n->sum(map(len, locations))
A:dask.array.core.axis->_get_axis(indexes)
A:dask.array.core.b->numpy.empty((1,), dtype=x._dtype).map_blocks(partial(np.squeeze, axis=axis), dtype=a.dtype)
A:dask.array.core.old_keys->list(product([b.name], *[range(len(bd)) for bd in b.chunks]))
A:dask.array.core.new_keys->list(product([b.name], *[range(len(bd)) for bd in chunks]))
A:dask.array.core.token->tokenize(x, indexes)
A:dask.array.core.shape->tuple(shape)
A:dask.array.core._optimize->staticmethod(optimize)
A:dask.array.core._default_get->staticmethod(threaded.get)
A:dask.array.core._finalize->staticmethod(finalize)
A:dask.array.core.self._chunks->normalize_chunks(chunks, shape)
A:dask.array.core.chunksize->str(tuple((c[0] for c in self.chunks)))
A:dask.array.core.ind->len(args)
A:dask.array.core.x->x.astype(dtype).astype(dtype)
A:dask.array.core.store->dict()
A:dask.array.core.dsk2->dict()
A:dask.array.core.dt->numpy.promote_types(lhs._dtype, rhs._dtype)
A:dask.array.core.(dsk, chunks)->slice_array(out, self.name, self.chunks, index)
A:dask.array.core.out->kwargs.pop('name', None)
A:dask.array.core.i->int(f)
A:dask.array.core.lock->Lock()
A:dask.array.core.func->partial(numpy_compat.isclose, rtol=rtol, atol=atol, equal_nan=equal_nan)
A:dask.array.core.non_trivial_dims->set([d for d in blockdims if len(d) > 1])
A:dask.array.core.arginds->list(zip(arrays, args[1::2]))
A:dask.array.core.blockdim_dict->dict(((a.name, a.chunks) for (a, _) in arginds))
A:dask.array.core.chunkss->broadcast_dimensions(nameinds, blockdim_dict, consolidate=common_blockdim)
A:dask.array.core.(chunkss, arrays)->unify_chunks(*args)
A:dask.array.core.ndim->len(indexes)
A:dask.array.core.tup->tuple((atleast_3d(x) for x in tup))
A:dask.array.core.condition->condition.copy().copy()
A:dask.array.core.ALPHABET->alphabet.upper()
A:dask.array.core.left_axes->tuple(left_axes)
A:dask.array.core.right_axes->tuple(right_axes)
A:dask.array.core.lhs->from_array(lhs, chunks=chunks)
A:dask.array.core.rhs->from_array(rhs, chunks=chunks)
A:dask.array.core.left_index->list(alphabet[:lhs.ndim])
A:dask.array.core.right_index->list(ALPHABET[:rhs.ndim])
A:dask.array.core.intermediate->atop(func, out_index, lhs, left_index, rhs, right_index, dtype=dt)
A:dask.array.core.int_index->list(out_index)
A:dask.array.core.out[index]->numpy.asanyarray(x)
A:dask.array.core.slices->slices_from_chunks(arr.chunks)
A:dask.array.core.array->from_array(array, chunks=array.shape, name=name)
A:dask.array.core.function->kwargs.pop('function')
A:dask.array.core.other->kwargs.pop('other')
A:dask.array.core.args2->list(map(add, args, offset))
A:dask.array.core.dim->max(sizes)
A:dask.array.core.out_ndim->len(broadcast_shapes(*shapes))
A:dask.array.core.f->partial(elemwise, func, **kwargs)
A:dask.array.core.logaddexp->wrap_elemwise(np.logaddexp)
A:dask.array.core.logaddexp2->wrap_elemwise(np.logaddexp2)
A:dask.array.core.conj->wrap_elemwise(np.conj)
A:dask.array.core.exp->wrap_elemwise(np.exp)
A:dask.array.core.log->wrap_elemwise(np.log)
A:dask.array.core.log2->wrap_elemwise(np.log2)
A:dask.array.core.log10->wrap_elemwise(np.log10)
A:dask.array.core.log1p->wrap_elemwise(np.log1p)
A:dask.array.core.expm1->wrap_elemwise(np.expm1)
A:dask.array.core.sqrt->wrap_elemwise(np.sqrt)
A:dask.array.core.square->wrap_elemwise(np.square)
A:dask.array.core.sin->wrap_elemwise(np.sin)
A:dask.array.core.cos->wrap_elemwise(np.cos)
A:dask.array.core.tan->wrap_elemwise(np.tan)
A:dask.array.core.arcsin->wrap_elemwise(np.arcsin)
A:dask.array.core.arccos->wrap_elemwise(np.arccos)
A:dask.array.core.arctan->wrap_elemwise(np.arctan)
A:dask.array.core.arctan2->wrap_elemwise(np.arctan2)
A:dask.array.core.hypot->wrap_elemwise(np.hypot)
A:dask.array.core.sinh->wrap_elemwise(np.sinh)
A:dask.array.core.cosh->wrap_elemwise(np.cosh)
A:dask.array.core.tanh->wrap_elemwise(np.tanh)
A:dask.array.core.arcsinh->wrap_elemwise(np.arcsinh)
A:dask.array.core.arccosh->wrap_elemwise(np.arccosh)
A:dask.array.core.arctanh->wrap_elemwise(np.arctanh)
A:dask.array.core.deg2rad->wrap_elemwise(np.deg2rad)
A:dask.array.core.rad2deg->wrap_elemwise(np.rad2deg)
A:dask.array.core.logical_and->wrap_elemwise(np.logical_and, dtype='bool')
A:dask.array.core.logical_or->wrap_elemwise(np.logical_or, dtype='bool')
A:dask.array.core.logical_xor->wrap_elemwise(np.logical_xor, dtype='bool')
A:dask.array.core.logical_not->wrap_elemwise(np.logical_not, dtype='bool')
A:dask.array.core.maximum->wrap_elemwise(np.maximum)
A:dask.array.core.minimum->wrap_elemwise(np.minimum)
A:dask.array.core.fmax->wrap_elemwise(np.fmax)
A:dask.array.core.fmin->wrap_elemwise(np.fmin)
A:dask.array.core.isreal->wrap_elemwise(np.isreal, dtype='bool')
A:dask.array.core.iscomplex->wrap_elemwise(np.iscomplex, dtype='bool')
A:dask.array.core.isfinite->wrap_elemwise(np.isfinite, dtype='bool')
A:dask.array.core.isinf->wrap_elemwise(np.isinf, dtype='bool')
A:dask.array.core.isnan->wrap_elemwise(np.isnan, dtype='bool')
A:dask.array.core.signbit->wrap_elemwise(np.signbit, dtype='bool')
A:dask.array.core.copysign->wrap_elemwise(np.copysign)
A:dask.array.core.nextafter->wrap_elemwise(np.nextafter)
A:dask.array.core.ldexp->wrap_elemwise(np.ldexp)
A:dask.array.core.fmod->wrap_elemwise(np.fmod)
A:dask.array.core.floor->wrap_elemwise(np.floor)
A:dask.array.core.ceil->wrap_elemwise(np.ceil)
A:dask.array.core.trunc->wrap_elemwise(np.trunc)
A:dask.array.core.degrees->wrap_elemwise(np.degrees)
A:dask.array.core.radians->wrap_elemwise(np.radians)
A:dask.array.core.rint->wrap_elemwise(np.rint)
A:dask.array.core.fix->wrap_elemwise(np.fix)
A:dask.array.core.angle->wrap_elemwise(np.angle)
A:dask.array.core.real->wrap_elemwise(np.real)
A:dask.array.core.imag->wrap_elemwise(np.imag)
A:dask.array.core.clip->wrap_elemwise(np.clip)
A:dask.array.core.fabs->wrap_elemwise(np.fabs)
A:dask.array.core.sign->wrap_elemwise(np.sign)
A:dask.array.core.tmp->elemwise(np.modf, x)
A:dask.array.core.ldsk->dict((((left,) + key[1:], (getitem, key, 0)) for key in core.flatten(tmp._keys())))
A:dask.array.core.rdsk->dict((((right,) + key[1:], (getitem, key, 1)) for key in core.flatten(tmp._keys())))
A:dask.array.core.a->numpy.empty((1,), dtype=x._dtype)
A:dask.array.core.(l, r)->numpy.modf(a)
A:dask.array.core.R->Array(merge(tmp.dask, rdsk), right, chunks=tmp.chunks, dtype=rdt)
A:dask.array.core.where_error_message->'\nThe dask.array version of where only handles the three argument case.\n\n    da.where(x > 0, x, 0)\n\nand not the single argument case\n\n    da.where(x > 0)\n\nThis is because dask.array operations must be able to infer the shape of their\noutputs prior to execution.  The number of positive elements of x requires\nexecution.  See the ``np.where`` docstring for examples and the following link\nfor a more thorough explanation:\n\n    http://dask.pydata.org/en/latest/array-overview.html#construct\n'.strip()
A:dask.array.core.chunks_none_error_message->'\nYou must specify a chunks= keyword argument.\nThis specifies the chunksize of your array blocks.\n\nSee the following documentation page for details:\n  http://dask.pydata.org/en/latest/array-creation.html#chunks\n'.strip()
A:dask.array.core.reduction->getattr(np, reduction.__name__)
A:dask.array.core.padded_breaks->concat([[None], breaks, [None]])
A:dask.array.core.obj->numpy.where(obj < 0, obj + arr.shape[axis], obj)
A:dask.array.core.split_arr->split_at_breaks(arr, np.unique(obj), axis)
A:dask.array.core.values->list(values)
A:dask.array.core.values_shape->tuple((len(obj) if axis == n else s for (n, s) in enumerate(arr.shape)))
A:dask.array.core.values_chunks->tuple((values_bd if axis == n else arr_bd for (n, (arr_bd, values_bd)) in enumerate(zip(arr.chunks, values.chunks))))
A:dask.array.core.values_breaks->numpy.cumsum(counts[counts > 0])
A:dask.array.core.split_values->split_at_breaks(values, values_breaks, axis)
A:dask.array.core.interleaved->list(interleave([split_arr, split_values]))
A:dask.array.core.trailing_size->int(np.prod(shape[1:]))
A:dask.array.core.missing_size->sanitize_index(array.size / np.prod(known_sizes))
A:dask.array.core.offsets->list(product(*aggdims))
A:dask.array.core.parts->Array._get(merge(dsk, x.dask), list(dsk.keys()))
A:dask.array.core.bins->numpy.linspace(mn, mx, bins + 1, endpoint=True)
A:dask.array.core.nchunks->len(list(core.flatten(a._keys())))
A:dask.array.core.a_keys->core.flatten(a._keys())
A:dask.array.core.w_keys->core.flatten(weights._keys())
A:dask.array.core.mapped->Array(dsk, name, chunks, dtype=dtype)
A:dask.array.core.db->from_array(np.diff(bins).astype(float), chunks=n.chunks)
A:dask.array.core.blocks->v._keys()
A:dask.array.core.rdim->len(m.chunks[0])
A:dask.array.core.hdim->len(m.chunks[1])
A:dask.array.core.result->numpy.empty(shape=shape, dtype=dtype(deepfirst(arrays)))
A:dask.array.core.points->list()
A:dask.array.core.per_block->dict(((k, v) for (k, v) in per_block.items() if v))
A:dask.array.core.other_blocks->list(product(*[list(range(len(c))) if i is None else [None] for (i, c) in zip(indexes, x.chunks)]))
A:dask.array.core.locations->list(map(list, locations))
A:dask.array.core.m->asarray(m)
A:dask.array.core.y->array(y, ndmin=2, dtype=dtype)
A:dask.array.core.X->concatenate((X, y), axis)
A:dask.array.core.fact->float(N - ddof)
A:dask.array.core.d->d.reshape((d.shape[0], 1)).reshape((d.shape[0], 1))
A:dask.array.core.sqr_d->sqrt(d)
A:dask.array.core.xx->x.astype(dtype).astype(dtype).rechunk(chunks)
A:dask.array.core.info->pickle.load(f)
dask.array.Array(self,dask,name,chunks,dtype=None,shape=None)
dask.array.Array.A(self)
dask.array.Array.T(self)
dask.array.Array.__abs__(self)
dask.array.Array.__add__(self,other)
dask.array.Array.__and__(self,other)
dask.array.Array.__array__(self,dtype=None,**kwargs)
dask.array.Array.__bool__(self)
dask.array.Array.__complex__(self)
dask.array.Array.__div__(self,other)
dask.array.Array.__eq__(self,other)
dask.array.Array.__float__(self)
dask.array.Array.__floordiv__(self,other)
dask.array.Array.__ge__(self,other)
dask.array.Array.__getitem__(self,index)
dask.array.Array.__gt__(self,other)
dask.array.Array.__int__(self)
dask.array.Array.__invert__(self)
dask.array.Array.__le__(self,other)
dask.array.Array.__len__(self)
dask.array.Array.__lshift__(self,other)
dask.array.Array.__lt__(self,other)
dask.array.Array.__mod__(self,other)
dask.array.Array.__mul__(self,other)
dask.array.Array.__ne__(self,other)
dask.array.Array.__neg__(self)
dask.array.Array.__or__(self,other)
dask.array.Array.__pos__(self)
dask.array.Array.__pow__(self,other)
dask.array.Array.__radd__(self,other)
dask.array.Array.__rand__(self,other)
dask.array.Array.__rdiv__(self,other)
dask.array.Array.__repr__(self)
dask.array.Array.__rfloordiv__(self,other)
dask.array.Array.__rlshift__(self,other)
dask.array.Array.__rmod__(self,other)
dask.array.Array.__rmul__(self,other)
dask.array.Array.__ror__(self,other)
dask.array.Array.__rpow__(self,other)
dask.array.Array.__rrshift__(self,other)
dask.array.Array.__rshift__(self,other)
dask.array.Array.__rsub__(self,other)
dask.array.Array.__rtruediv__(self,other)
dask.array.Array.__rxor__(self,other)
dask.array.Array.__sub__(self,other)
dask.array.Array.__truediv__(self,other)
dask.array.Array.__xor__(self,other)
dask.array.Array._args(self)
dask.array.Array._get_chunks(self)
dask.array.Array._keys(self,*args)
dask.array.Array._set_chunks(self,chunks)
dask.array.Array._vindex(self,key)
dask.array.Array.all(self,axis=None,keepdims=False,split_every=None)
dask.array.Array.any(self,axis=None,keepdims=False,split_every=None)
dask.array.Array.argmax(self,axis=None,split_every=None)
dask.array.Array.argmin(self,axis=None,split_every=None)
dask.array.Array.astype(self,dtype,**kwargs)
dask.array.Array.cache(self,store=None,**kwargs)
dask.array.Array.conj(self)
dask.array.Array.copy(self)
dask.array.Array.cumprod(self,axis,dtype=None)
dask.array.Array.cumsum(self,axis,dtype=None)
dask.array.Array.dot(self,other)
dask.array.Array.dtype(self)
dask.array.Array.imag(self)
dask.array.Array.map_blocks(self,func,*args,**kwargs)
dask.array.Array.map_overlap(self,func,depth,boundary=None,trim=True,**kwargs)
dask.array.Array.max(self,axis=None,keepdims=False,split_every=None)
dask.array.Array.mean(self,axis=None,dtype=None,keepdims=False,split_every=None)
dask.array.Array.min(self,axis=None,keepdims=False,split_every=None)
dask.array.Array.moment(self,order,axis=None,dtype=None,keepdims=False,ddof=0,split_every=None)
dask.array.Array.nbytes(self)
dask.array.Array.ndim(self)
dask.array.Array.npartitions(self)
dask.array.Array.numblocks(self)
dask.array.Array.prod(self,axis=None,dtype=None,keepdims=False,split_every=None)
dask.array.Array.ravel(self)
dask.array.Array.real(self)
dask.array.Array.rechunk(self,chunks)
dask.array.Array.reshape(self,*shape)
dask.array.Array.shape(self)
dask.array.Array.size(self)
dask.array.Array.squeeze(self)
dask.array.Array.std(self,axis=None,dtype=None,keepdims=False,ddof=0,split_every=None)
dask.array.Array.store(self,target,**kwargs)
dask.array.Array.sum(self,axis=None,dtype=None,keepdims=False,split_every=None)
dask.array.Array.to_delayed(self)
dask.array.Array.to_hdf5(self,filename,datapath,**kwargs)
dask.array.Array.to_imperative(self)
dask.array.Array.topk(self,k)
dask.array.Array.transpose(self,axes=None)
dask.array.Array.var(self,axis=None,dtype=None,keepdims=False,ddof=0,split_every=None)
dask.array.Array.view(self,dtype,order='C')
dask.array.Array.vindex(self)
dask.array.Array.vnorm(self,ord=None,axis=None,keepdims=False,split_every=None)
dask.array.around(x,decimals=0)
dask.array.array(x,dtype=None,ndmin=None)
dask.array.atop(func,out_ind,*args,**kwargs)
dask.array.bincount(x,weights=None,minlength=None)
dask.array.broadcast_to(x,shape)
dask.array.choose(a,choices)
dask.array.coarsen(reduction,x,axes,trim_excess=False)
dask.array.compress(condition,a,axis=None)
dask.array.concatenate(seq,axis=0)
dask.array.concatenate3(arrays)
dask.array.core.Array(self,dask,name,chunks,dtype=None,shape=None)
dask.array.core.Array.A(self)
dask.array.core.Array.T(self)
dask.array.core.Array.__abs__(self)
dask.array.core.Array.__add__(self,other)
dask.array.core.Array.__and__(self,other)
dask.array.core.Array.__array__(self,dtype=None,**kwargs)
dask.array.core.Array.__bool__(self)
dask.array.core.Array.__complex__(self)
dask.array.core.Array.__div__(self,other)
dask.array.core.Array.__eq__(self,other)
dask.array.core.Array.__float__(self)
dask.array.core.Array.__floordiv__(self,other)
dask.array.core.Array.__ge__(self,other)
dask.array.core.Array.__getitem__(self,index)
dask.array.core.Array.__gt__(self,other)
dask.array.core.Array.__init__(self,dask,name,chunks,dtype=None,shape=None)
dask.array.core.Array.__int__(self)
dask.array.core.Array.__invert__(self)
dask.array.core.Array.__le__(self,other)
dask.array.core.Array.__len__(self)
dask.array.core.Array.__lshift__(self,other)
dask.array.core.Array.__lt__(self,other)
dask.array.core.Array.__mod__(self,other)
dask.array.core.Array.__mul__(self,other)
dask.array.core.Array.__ne__(self,other)
dask.array.core.Array.__neg__(self)
dask.array.core.Array.__or__(self,other)
dask.array.core.Array.__pos__(self)
dask.array.core.Array.__pow__(self,other)
dask.array.core.Array.__radd__(self,other)
dask.array.core.Array.__rand__(self,other)
dask.array.core.Array.__rdiv__(self,other)
dask.array.core.Array.__repr__(self)
dask.array.core.Array.__rfloordiv__(self,other)
dask.array.core.Array.__rlshift__(self,other)
dask.array.core.Array.__rmod__(self,other)
dask.array.core.Array.__rmul__(self,other)
dask.array.core.Array.__ror__(self,other)
dask.array.core.Array.__rpow__(self,other)
dask.array.core.Array.__rrshift__(self,other)
dask.array.core.Array.__rshift__(self,other)
dask.array.core.Array.__rsub__(self,other)
dask.array.core.Array.__rtruediv__(self,other)
dask.array.core.Array.__rxor__(self,other)
dask.array.core.Array.__sub__(self,other)
dask.array.core.Array.__truediv__(self,other)
dask.array.core.Array.__xor__(self,other)
dask.array.core.Array._args(self)
dask.array.core.Array._get_chunks(self)
dask.array.core.Array._keys(self,*args)
dask.array.core.Array._set_chunks(self,chunks)
dask.array.core.Array._vindex(self,key)
dask.array.core.Array.all(self,axis=None,keepdims=False,split_every=None)
dask.array.core.Array.any(self,axis=None,keepdims=False,split_every=None)
dask.array.core.Array.argmax(self,axis=None,split_every=None)
dask.array.core.Array.argmin(self,axis=None,split_every=None)
dask.array.core.Array.astype(self,dtype,**kwargs)
dask.array.core.Array.cache(self,store=None,**kwargs)
dask.array.core.Array.conj(self)
dask.array.core.Array.copy(self)
dask.array.core.Array.cumprod(self,axis,dtype=None)
dask.array.core.Array.cumsum(self,axis,dtype=None)
dask.array.core.Array.dot(self,other)
dask.array.core.Array.dtype(self)
dask.array.core.Array.imag(self)
dask.array.core.Array.map_blocks(self,func,*args,**kwargs)
dask.array.core.Array.map_overlap(self,func,depth,boundary=None,trim=True,**kwargs)
dask.array.core.Array.max(self,axis=None,keepdims=False,split_every=None)
dask.array.core.Array.mean(self,axis=None,dtype=None,keepdims=False,split_every=None)
dask.array.core.Array.min(self,axis=None,keepdims=False,split_every=None)
dask.array.core.Array.moment(self,order,axis=None,dtype=None,keepdims=False,ddof=0,split_every=None)
dask.array.core.Array.nbytes(self)
dask.array.core.Array.ndim(self)
dask.array.core.Array.npartitions(self)
dask.array.core.Array.numblocks(self)
dask.array.core.Array.prod(self,axis=None,dtype=None,keepdims=False,split_every=None)
dask.array.core.Array.ravel(self)
dask.array.core.Array.real(self)
dask.array.core.Array.rechunk(self,chunks)
dask.array.core.Array.reshape(self,*shape)
dask.array.core.Array.shape(self)
dask.array.core.Array.size(self)
dask.array.core.Array.squeeze(self)
dask.array.core.Array.std(self,axis=None,dtype=None,keepdims=False,ddof=0,split_every=None)
dask.array.core.Array.store(self,target,**kwargs)
dask.array.core.Array.sum(self,axis=None,dtype=None,keepdims=False,split_every=None)
dask.array.core.Array.to_delayed(self)
dask.array.core.Array.to_hdf5(self,filename,datapath,**kwargs)
dask.array.core.Array.to_imperative(self)
dask.array.core.Array.topk(self,k)
dask.array.core.Array.transpose(self,axes=None)
dask.array.core.Array.var(self,axis=None,dtype=None,keepdims=False,ddof=0,split_every=None)
dask.array.core.Array.view(self,dtype,order='C')
dask.array.core.Array.vindex(self)
dask.array.core.Array.vnorm(self,ord=None,axis=None,keepdims=False,split_every=None)
dask.array.core._astype(x,dtype,**kwargs)
dask.array.core._concatenate2(arrays,axes=[])
dask.array.core._get_axis(indexes)
dask.array.core._take_dask_array_from_numpy(a,indices,axis)
dask.array.core._vindex(x,*indexes)
dask.array.core._vindex_merge(locations,values)
dask.array.core._vindex_slice(block,points)
dask.array.core._vindex_transpose(block,axis)
dask.array.core.around(x,decimals=0)
dask.array.core.array(x,dtype=None,ndmin=None)
dask.array.core.asarray(array)
dask.array.core.atleast_2d(x)
dask.array.core.atleast_3d(x)
dask.array.core.atop(func,out_ind,*args,**kwargs)
dask.array.core.bincount(x,weights=None,minlength=None)
dask.array.core.blockdims_from_blockshape(shape,chunks)
dask.array.core.broadcast_chunks(*chunkss)
dask.array.core.broadcast_dimensions(argpairs,numblocks,sentinels=(1,(1,)),consolidate=None)
dask.array.core.broadcast_shapes(*shapes)
dask.array.core.broadcast_to(x,shape)
dask.array.core.choose(a,choices)
dask.array.core.chunks_from_arrays(arrays)
dask.array.core.coarsen(reduction,x,axes,trim_excess=False)
dask.array.core.common_blockdim(blockdims)
dask.array.core.compress(condition,a,axis=None)
dask.array.core.concatenate(seq,axis=0)
dask.array.core.concatenate3(arrays)
dask.array.core.corrcoef(x,y=None,rowvar=1)
dask.array.core.cov(m,y=None,rowvar=1,bias=0,ddof=None)
dask.array.core.deepfirst(seq)
dask.array.core.diag(v)
dask.array.core.dot(a,b)
dask.array.core.dotmany(A,B,leftfunc=None,rightfunc=None,**kwargs)
dask.array.core.dstack(tup)
dask.array.core.elemwise(op,*args,**kwargs)
dask.array.core.ensure_int(f)
dask.array.core.eye(N,chunks,M=None,k=0,dtype=float)
dask.array.core.finalize(results)
dask.array.core.frexp(x)
dask.array.core.from_array(x,chunks,name=None,lock=False)
dask.array.core.from_delayed(value,shape,dtype=None,name=None)
dask.array.core.from_func(func,shape,dtype=None,name=None,args=(),kwargs={})
dask.array.core.from_imperative(*args,**kwargs)
dask.array.core.from_npy_stack(dirname,mmap_mode='r')
dask.array.core.fromfunction(func,chunks=None,shape=None,dtype=None)
dask.array.core.getarray(a,b,lock=None)
dask.array.core.getem(arr,chunks,shape=None,out_name=None)
dask.array.core.histogram(a,bins=None,range=None,normed=False,weights=None,density=None)
dask.array.core.hstack(tup)
dask.array.core.insert(arr,obj,values,axis)
dask.array.core.insert_to_ooc(out,arr,lock=True)
dask.array.core.interleave_none(a,b)
dask.array.core.is_scalar_for_elemwise(arg)
dask.array.core.isclose(arr1,arr2,rtol=1e-05,atol=1e-08,equal_nan=False)
dask.array.core.isnull(values)
dask.array.core.keyname(name,i,okey)
dask.array.core.lol_tuples(head,ind,values,dummies)
dask.array.core.map_blocks(func,*args,**kwargs)
dask.array.core.modf(x)
dask.array.core.ndimlist(seq)
dask.array.core.normalize_chunks(chunks,shape=None)
dask.array.core.notnull(values)
dask.array.core.offset_func(func,offset,*args)
dask.array.core.partial_by_order(*args,**kwargs)
dask.array.core.ravel(array)
dask.array.core.reshape(array,shape)
dask.array.core.slices_from_chunks(chunks)
dask.array.core.split_at_breaks(array,breaks,axis=0)
dask.array.core.squeeze(a,axis=None)
dask.array.core.stack(seq,axis=0)
dask.array.core.store(sources,targets,lock=True,compute=True,**kwargs)
dask.array.core.take(a,indices,axis=0)
dask.array.core.tensordot(lhs,rhs,axes=2)
dask.array.core.to_hdf5(filename,*args,**kwargs)
dask.array.core.to_npy_stack(dirname,x,axis=0)
dask.array.core.top(func,output,out_indices,*arrind_pairs,**kwargs)
dask.array.core.topk(k,x)
dask.array.core.transpose(a,axes=None)
dask.array.core.tril(m,k=0)
dask.array.core.triu(m,k=0)
dask.array.core.unify_chunks(*args)
dask.array.core.unique(x)
dask.array.core.unpack_singleton(x)
dask.array.core.unravel(array,shape)
dask.array.core.variadic_choose(a,*choices)
dask.array.core.vstack(tup)
dask.array.core.where(condition,x=None,y=None)
dask.array.core.wrap_elemwise(func,**kwargs)
dask.array.core.zero_broadcast_dimensions(lol,nblocks)
dask.array.corrcoef(x,y=None,rowvar=1)
dask.array.cov(m,y=None,rowvar=1,bias=0,ddof=None)
dask.array.diag(v)
dask.array.dot(a,b)
dask.array.dotmany(A,B,leftfunc=None,rightfunc=None,**kwargs)
dask.array.dstack(tup)
dask.array.eye(N,chunks,M=None,k=0,dtype=float)
dask.array.frexp(x)
dask.array.from_array(x,chunks,name=None,lock=False)
dask.array.from_delayed(value,shape,dtype=None,name=None)
dask.array.from_imperative(*args,**kwargs)
dask.array.from_npy_stack(dirname,mmap_mode='r')
dask.array.fromfunction(func,chunks=None,shape=None,dtype=None)
dask.array.histogram(a,bins=None,range=None,normed=False,weights=None,density=None)
dask.array.hstack(tup)
dask.array.insert(arr,obj,values,axis)
dask.array.insert_to_ooc(out,arr,lock=True)
dask.array.isclose(arr1,arr2,rtol=1e-05,atol=1e-08,equal_nan=False)
dask.array.isnull(values)
dask.array.map_blocks(func,*args,**kwargs)
dask.array.modf(x)
dask.array.notnull(values)
dask.array.ravel(array)
dask.array.reshape(array,shape)
dask.array.squeeze(a,axis=None)
dask.array.stack(seq,axis=0)
dask.array.store(sources,targets,lock=True,compute=True,**kwargs)
dask.array.take(a,indices,axis=0)
dask.array.tensordot(lhs,rhs,axes=2)
dask.array.to_hdf5(filename,*args,**kwargs)
dask.array.to_npy_stack(dirname,x,axis=0)
dask.array.topk(k,x)
dask.array.transpose(a,axes=None)
dask.array.tril(m,k=0)
dask.array.triu(m,k=0)
dask.array.unique(x)
dask.array.vstack(tup)
dask.array.where(condition,x=None,y=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/array/optimization.py----------------------------------------
A:dask.array.optimization.keys->list(flatten(keys))
A:dask.array.optimization.fast_functions->kwargs.get('fast_functions', set([getarray, np.transpose]))
A:dask.array.optimization.(dsk2, dependencies)->cull(dsk, keys)
A:dask.array.optimization.(dsk4, dependencies)->fuse(dsk2, keys, dependencies)
A:dask.array.optimization.dsk5->optimize_slices(dsk4)
A:dask.array.optimization.dsk6->inline_functions(dsk5, keys, fast_functions=fast_functions, dependencies=dependencies)
A:dask.array.optimization.dsk->dsk.copy().copy()
A:dask.array.optimization.c_index->fuse_slice(b_index, a_index)
A:dask.array.optimization.a->normalize_slice(a)
A:dask.array.optimization.b->normalize_slice(b)
A:dask.array.optimization.stop->min(a.stop, stop)
A:dask.array.optimization.result->list()
dask.array.optimization.fuse_slice(a,b)
dask.array.optimization.normalize_slice(s)
dask.array.optimization.optimize(dsk,keys,**kwargs)
dask.array.optimization.optimize_slices(dsk)
dask.array.optimize(dsk,keys,**kwargs)
dask.array.optimize_slices(dsk)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/array/numpy_compat.py----------------------------------------
A:dask.array.numpy_compat.x->x.astype(dtype).astype(dtype)
A:dask.array.numpy_compat.new_array->new_array.view(type=type(original_array)).view(type=type(original_array))
A:dask.array.numpy_compat.array->numpy.array(array, copy=False, subok=subok)
A:dask.array.numpy_compat.result->_maybe_view_as_subclass(array, broadcast)
A:dask.array.numpy_compat.a->numpy.array(a, subok=True)
A:dask.array.numpy_compat.mask->numpy.isnan(a)
A:dask.array.numpy_compat.(a, mask)->_replace_nan(a, 1)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/array/wrap.py----------------------------------------
A:dask.array.wrap.shape->kwargs.pop('shape')
A:dask.array.wrap.chunks->normalize_chunks(chunks, shape)
A:dask.array.wrap.name->kwargs.pop('name', None)
A:dask.array.wrap.dtype->kwargs.pop('dtype', None)
A:dask.array.wrap.keys->product([name], *[range(len(bd)) for bd in chunks])
A:dask.array.wrap.shapes->product(*chunks)
A:dask.array.wrap.func->partial(func, dtype=dtype, **kwargs)
A:dask.array.wrap.dsk->dict(zip(keys, vals))
A:dask.array.wrap.f->partial(wrap_func, func, **kwargs)
A:dask.array.wrap.w->wrap(wrap_func_shape_as_first_arg)
A:dask.array.wrap.ones->w(np.ones, dtype='f8')
A:dask.array.wrap.zeros->w(np.zeros, dtype='f8')
A:dask.array.wrap.empty->w(np.empty, dtype='f8')
A:dask.array.wrap.full->w(full)
dask.array.wrap.dims_from_size(size,blocksize)
dask.array.wrap.wrap(wrap_func,func,**kwargs)
dask.array.wrap.wrap_func_shape_as_first_arg(func,*args,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/array/linalg.py----------------------------------------
A:dask.array.linalg.dsk_qr_st1->top(np.linalg.qr, name_qr_st1, 'ij', data.name, 'ij', numblocks={data.name: numblocks})
A:dask.array.linalg.dsk_q_st1->dict((((name_q_st1, i, 0), (operator.getitem, (name_qr_st1, i, 0), 0)) for i in range(numblocks[0])))
A:dask.array.linalg.dsk_r_st1->dict((((name_r_st1, i, 0), (operator.getitem, (name_qr_st1, i, 0), 1)) for i in range(numblocks[0])))
A:dask.array.linalg.dsk_qr_st2->top(np.linalg.qr, name_qr_st2, 'ij', name_r_st1_stacked, 'ij', numblocks={name_r_st1_stacked: (1, 1)})
A:dask.array.linalg.dsk_q_st2->dict((((name_q_st2, i, 0), (operator.getitem, (name_q_st2_aux, 0, 0), b)) for (i, b) in enumerate(block_slices)))
A:dask.array.linalg.dsk_q_st3->top(np.dot, name_q_st3, 'ij', name_q_st1, 'ij', name_q_st2, 'ij', numblocks={name_q_st1: numblocks, name_q_st2: numblocks})
A:dask.array.linalg.(qq, rr)->numpy.linalg.qr(np.ones(shape=(1, 1), dtype=data.dtype))
A:dask.array.linalg.q->Array(dsk_q, name_q_st3, shape=data.shape, chunks=data.chunks, dtype=qq.dtype)
A:dask.array.linalg.r->Array(dsk_r, name_r_st2, shape=(n, n), chunks=(n, n), dtype=rr.dtype)
A:dask.array.linalg.dsk_svd_st2->top(np.linalg.svd, name_svd_st2, 'ij', name_r_st2, 'ij', numblocks={name_r_st2: (1, 1)})
A:dask.array.linalg.dsk_u_st4->top(dotmany, name_u_st4, 'ij', name_q_st3, 'ik', name_u_st2, 'kj', numblocks={name_q_st3: numblocks, name_u_st2: (1, 1)})
A:dask.array.linalg.(uu, ss, vv)->numpy.linalg.svd(np.ones(shape=(1, 1), dtype=data.dtype))
A:dask.array.linalg.u->Array(dsk, name_u, shape=a.shape, chunks=a.chunks, dtype=uu.dtype)
A:dask.array.linalg.s->Array(sdsk, sname, shape=(r.shape[0],), chunks=r.shape[0], dtype=ss.dtype)
A:dask.array.linalg.v->Array(dsk_v, name_v_st2, shape=(n, n), chunks=(n, n), dtype=vv.dtype)
A:dask.array.linalg.comp_level->compression_level(n, q)
A:dask.array.linalg.state->RandomState(seed)
A:dask.array.linalg.omega->RandomState(seed).standard_normal(size=(n, comp_level), chunks=(data.chunks[1], (comp_level,)))
A:dask.array.linalg.mat_h->data.dot(data.T.dot(mat_h))
A:dask.array.linalg.(q, _)->tsqr(mat_h)
A:dask.array.linalg.comp->compression_matrix(a, k, n_power_iter=n_power_iter, seed=seed)
A:dask.array.linalg.a_compressed->compression_matrix(a, k, n_power_iter=n_power_iter, seed=seed).dot(a)
A:dask.array.linalg.(v, s, u)->tsqr(a_compressed.T, name, compute_svd=True)
A:dask.array.linalg.vdim->len(a.chunks[0])
A:dask.array.linalg.hdim->len(a.chunks[1])
A:dask.array.linalg.token->tokenize(a, b)
A:dask.array.linalg.(pp, ll, uu)->scipy.linalg.lu(np.ones(shape=(1, 1), dtype=a.dtype))
A:dask.array.linalg.p->Array(dsk, name_p, shape=a.shape, chunks=a.chunks, dtype=pp.dtype)
A:dask.array.linalg.l->Array(dsk, name_l, shape=a.shape, chunks=a.chunks, dtype=ll.dtype)
A:dask.array.linalg.vchunks->len(a.chunks[1])
A:dask.array.linalg.target->_b_init(i, j)
A:dask.array.linalg.res->_solve_triangular_lower(np.array([[1, 0], [1, 2]], dtype=a.dtype), np.array([0, 1], dtype=b.dtype))
A:dask.array.linalg.(l, u)->_cholesky(a)
A:dask.array.linalg.(p, l, u)->lu(a)
A:dask.array.linalg.b->Array(dsk, name_p, shape=a.shape, chunks=a.chunks, dtype=pp.dtype).T.dot(b)
A:dask.array.linalg.uy->solve_triangular(l, b, lower=True)
A:dask.array.linalg.cho->scipy.linalg.cholesky(np.array([[1, 2], [2, 5]], dtype=a.dtype))
A:dask.array.linalg.lower->Array(dsk, name, shape=a.shape, chunks=a.chunks, dtype=cho.dtype)
A:dask.array.linalg.upper->Array(dsk, name_upper, shape=a.shape, chunks=a.chunks, dtype=cho.dtype)
A:dask.array.linalg.(q, r)->qr(a)
A:dask.array.linalg.x->solve_triangular(r, q.T.dot(b))
A:dask.array.linalg.residuals->(residuals ** 2).sum()
A:dask.array.linalg.rank->Array(rdsk, rname, shape=(), chunks=(), dtype=int)
A:dask.array.linalg.(_, _, _, ss)->numpy.linalg.lstsq(np.array([[1, 0], [1, 2]], dtype=a.dtype), np.array([0, 1], dtype=b.dtype))
dask.array.linalg._cholesky(a)
dask.array.linalg._cholesky_lower(a)
dask.array.linalg._cumsum_blocks(it)
dask.array.linalg._solve_triangular_lower(a,b)
dask.array.linalg._sort_decreasing(x)
dask.array.linalg.cholesky(a,lower=False)
dask.array.linalg.compression_level(n,q,oversampling=10,min_subspace_size=20)
dask.array.linalg.compression_matrix(data,q,n_power_iter=0,seed=None)
dask.array.linalg.inv(a)
dask.array.linalg.lstsq(a,b)
dask.array.linalg.lu(a)
dask.array.linalg.qr(a,name=None)
dask.array.linalg.solve(a,b,sym_pos=False)
dask.array.linalg.solve_triangular(a,b,lower=False)
dask.array.linalg.svd(a,name=None)
dask.array.linalg.svd_compressed(a,k,n_power_iter=0,seed=None,name=None)
dask.array.linalg.tsqr(data,name=None,compute_svd=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/array/slicing.py----------------------------------------
A:dask.array.slicing.ind2->int(ind)
A:dask.array.slicing.ind->ind.tolist().tolist()
A:dask.array.slicing.index->tuple(map(sanitize_index, index))
A:dask.array.slicing.blockdims->tuple(map(tuple, blockdims))
A:dask.array.slicing.suffixes->product(*[range(len(bd)) for bd in blockdims])
A:dask.array.slicing.dsk->dict((((out_name,) + s, (in_name,) + s) for s in suffixes))
A:dask.array.slicing.(dsk_out, bd_out)->slice_with_newaxes(out_name, in_name, blockdims, index)
A:dask.array.slicing.bd_out->tuple(map(tuple, bd_out))
A:dask.array.slicing.index2->posify_index(shape, index)
A:dask.array.slicing.(dsk, blockdims2)->slice_slices_and_integers(tmp, in_name, blockdims, index_without_list)
A:dask.array.slicing.dsk2->dict((((out_name,) + insert_many(k[1:], where_none, 0), v[:2] + (insert_many(v[2], where_none, None),)) for (k, v) in dsk.items() if k[0] == out_name))
A:dask.array.slicing.dsk3->merge(dsk, dsk2)
A:dask.array.slicing.blockdims3->insert_many(blockdims2, where_none, (1,))
A:dask.array.slicing.shape->tuple(map(sum, blockdims))
A:dask.array.slicing.index_without_list->tuple((slice(None, None, None) if isinstance(i, list) else i for i in index2))
A:dask.array.slicing.(blockdims2, dsk3)->take(out_name, in_name, blockdims, index2[where_list[0]], axis=axis)
A:dask.array.slicing.(blockdims2, dsk2)->take(out_name, tmp, blockdims2, index2[axis], axis=axis2)
A:dask.array.slicing.block_slices->list(map(_slice_1d, shape, blockdims, index))
A:dask.array.slicing.in_names->list(product([in_name], *[pluck(0, s) for s in sorted_block_slices]))
A:dask.array.slicing.out_names->list(product([out_name], *[range(len(d))[::-1] if i.step and i.step < 0 else range(len(d)) for (d, i) in zip(block_slices, index) if not isinstance(i, (int, long))]))
A:dask.array.slicing.all_slices->list(product(*[pluck(1, s) for s in sorted_block_slices]))
A:dask.array.slicing.dsk_out->dict(((out_name, (getitem, in_name, slices)) for (out_name, in_name, slices) in zip(out_names, in_names, all_slices)))
A:dask.array.slicing.lens->list(lengths)
A:dask.array.slicing.d->dict()
A:dask.array.slicing.d[i]->slice(rstart - chunk_stop, max(chunk_start - chunk_stop - 1, stop - chunk_stop), step)
A:dask.array.slicing.chunk_boundaries->list(accumulate(add, lengths))
A:dask.array.slicing.d[k]->slice(None, None, None)
A:dask.array.slicing.d[0]->slice(0, 0, 1)
A:dask.array.slicing.seq->list(seq)
A:dask.array.slicing.result->list()
A:dask.array.slicing.L->list()
A:dask.array.slicing.colon->slice(None, None, None)
A:dask.array.slicing.index_lists->partition_by_size(sizes, sorted(index))
A:dask.array.slicing.indims->list(dims)
A:dask.array.slicing.indims[axis]->list(range(len(where_index)))
A:dask.array.slicing.keys->list(product([outname], *dims))
A:dask.array.slicing.outdims->list(dims)
A:dask.array.slicing.slices->list(product(*slices))
A:dask.array.slicing.inkeys->list(product([inname], *outdims))
A:dask.array.slicing.blockdims2->list(blockdims)
A:dask.array.slicing.blockdims2[axis]->tuple(map(len, index_lists))
A:dask.array.slicing.n->len(blockdims)
A:dask.array.slicing.rev_index->list(map(sorted(index).index, index))
A:dask.array.slicing.pairs->sorted(_slice_1d(dim_shape, lengths, index).items(), key=first)
dask.array.slicing._slice_1d(dim_shape,lengths,index)
dask.array.slicing.check_index(ind,dimension)
dask.array.slicing.insert_many(seq,where,val)
dask.array.slicing.issorted(seq)
dask.array.slicing.new_blockdim(dim_shape,lengths,index)
dask.array.slicing.partition_by_size(sizes,seq)
dask.array.slicing.posify_index(shape,ind)
dask.array.slicing.replace_ellipsis(n,index)
dask.array.slicing.sanitize_index(ind)
dask.array.slicing.slice_array(out_name,in_name,blockdims,index)
dask.array.slicing.slice_slices_and_integers(out_name,in_name,blockdims,index)
dask.array.slicing.slice_with_newaxes(out_name,in_name,blockdims,index)
dask.array.slicing.slice_wrap_lists(out_name,in_name,blockdims,index)
dask.array.slicing.take(outname,inname,blockdims,index,axis=0)
dask.array.slicing.take_sorted(outname,inname,blockdims,index,axis=0)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/array/random.py----------------------------------------
A:dask.array.random.func2.__doc__->getattr(state, func).__doc__.replace('>>>', '>>').replace('...', '..')
A:dask.array.random.self._numpy_state->numpy.random.RandomState(seed)
A:dask.array.random.size->kwargs.pop('size')
A:dask.array.random.chunks->normalize_chunks(chunks, size)
A:dask.array.random.extra_chunks->kwargs.pop('extra_chunks', ())
A:dask.array.random.kw->kwargs.copy()
A:dask.array.random.sizes->list(product(*chunks))
A:dask.array.random.seeds->different_seeds(len(sizes), self._numpy_state)
A:dask.array.random.token->tokenize(seeds, size, chunks, args, kwargs)
A:dask.array.random.name->'da.random.{0}-{1}'.format(func.__name__, token)
A:dask.array.random.keys->product([name], *[range(len(bd)) for bd in chunks] + [[0]] * len(extra_chunks))
A:dask.array.random.dsk->dict(zip(keys, vals))
A:dask.array.random.state->numpy.random.RandomState(seed)
A:dask.array.random.func->getattr(state, func)
A:dask.array.random._state->RandomState()
dask.array.random.RandomState(self,seed=None)
dask.array.random.RandomState.__init__(self,seed=None)
dask.array.random.RandomState._wrap(self,func,*args,**kwargs)
dask.array.random.RandomState.beta(self,a,b,size=None,chunks=None)
dask.array.random.RandomState.binomial(self,n,p,size=None,chunks=None)
dask.array.random.RandomState.chisquare(self,df,size=None,chunks=None)
dask.array.random.RandomState.exponential(self,scale=1.0,size=None,chunks=None)
dask.array.random.RandomState.f(self,dfnum,dfden,size=None,chunks=None)
dask.array.random.RandomState.gamma(self,shape,scale=1.0,chunks=None)
dask.array.random.RandomState.geometric(self,p,size=None,chunks=None)
dask.array.random.RandomState.gumbel(self,loc=0.0,scale=1.0,size=None,chunks=None)
dask.array.random.RandomState.hypergeometric(self,ngood,nbad,nsample,size=None,chunks=None)
dask.array.random.RandomState.laplace(self,loc=0.0,scale=1.0,size=None,chunks=None)
dask.array.random.RandomState.logistic(self,loc=0.0,scale=1.0,size=None,chunks=None)
dask.array.random.RandomState.lognormal(self,mean=0.0,sigma=1.0,size=None,chunks=None)
dask.array.random.RandomState.logseries(self,p,size=None,chunks=None)
dask.array.random.RandomState.multinomial(self,n,pvals,size=None,chunks=None)
dask.array.random.RandomState.negative_binomial(self,n,p,size=None,chunks=None)
dask.array.random.RandomState.noncentral_chisquare(self,df,nonc,size=None,chunks=None)
dask.array.random.RandomState.noncentral_f(self,dfnum,dfden,nonc,size=None,chunks=None)
dask.array.random.RandomState.normal(self,loc=0.0,scale=1.0,size=None,chunks=None)
dask.array.random.RandomState.pareto(self,a,size=None,chunks=None)
dask.array.random.RandomState.poisson(self,lam=1.0,size=None,chunks=None)
dask.array.random.RandomState.power(self,a,size=None,chunks=None)
dask.array.random.RandomState.randint(self,low,high=None,size=None,chunks=None)
dask.array.random.RandomState.random_integers(self,low,high=None,size=None,chunks=None)
dask.array.random.RandomState.random_sample(self,size=None,chunks=None)
dask.array.random.RandomState.rayleigh(self,scale=1.0,size=None,chunks=None)
dask.array.random.RandomState.seed(self,seed=None)
dask.array.random.RandomState.standard_cauchy(self,size=None,chunks=None)
dask.array.random.RandomState.standard_exponential(self,size=None,chunks=None)
dask.array.random.RandomState.standard_gamma(self,shape,size=None,chunks=None)
dask.array.random.RandomState.standard_normal(self,size=None,chunks=None)
dask.array.random.RandomState.standard_t(self,df,size=None,chunks=None)
dask.array.random.RandomState.tomaxint(self,size=None,chunks=None)
dask.array.random.RandomState.triangular(self,left,mode,right,size=None,chunks=None)
dask.array.random.RandomState.uniform(self,low=0.0,high=1.0,size=None,chunks=None)
dask.array.random.RandomState.vonmises(self,mu,kappa,size=None,chunks=None)
dask.array.random.RandomState.wald(self,mean,scale,size=None,chunks=None)
dask.array.random.RandomState.weibull(self,a,size=None,chunks=None)
dask.array.random.RandomState.zipf(self,a,size=None,chunks=None)
dask.array.random._apply_random(func,seed,size,args,kwargs)
dask.array.random.doc_wraps(func)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/array/conftest.py----------------------------------------
dask.array.conftest.pytest_ignore_collect(path,config)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/array/percentile.py----------------------------------------
A:dask.array.percentile.q->numpy.array(q)
A:dask.array.percentile.result->merge_percentiles(finalq, qs, [v.codes for v in vals], Ns, interpolation)
A:dask.array.percentile.a2->a.astype('i8')
A:dask.array.percentile.token->tokenize(a, list(q), interpolation)
A:dask.array.percentile.dsk->dict((((name, i), (_percentile, key, q, interpolation)) for (i, key) in enumerate(a._keys())))
A:dask.array.percentile.finalq->numpy.array(finalq)
A:dask.array.percentile.qs->list(map(list, qs))
A:dask.array.percentile.vals->list(vals)
A:dask.array.percentile.Ns->list(Ns)
A:dask.array.percentile.L->list(zip(*[(q, val, N) for (q, val, N) in zip(qs, vals, Ns) if N]))
A:dask.array.percentile.count->numpy.empty(len(q))
A:dask.array.percentile.count[1:]->numpy.diff(q)
A:dask.array.percentile.combined_vals_counts->merge_sorted(*map(zip, vals, counts))
A:dask.array.percentile.(combined_vals, combined_counts)->zip(*combined_vals_counts)
A:dask.array.percentile.combined_vals->numpy.array(combined_vals)
A:dask.array.percentile.combined_counts->numpy.array(combined_counts)
A:dask.array.percentile.combined_q->numpy.cumsum(combined_counts)
A:dask.array.percentile.rv->numpy.interp(desired_q, combined_q, combined_vals)
A:dask.array.percentile.left->numpy.searchsorted(combined_q, desired_q, side='left')
A:dask.array.percentile.lower->numpy.minimum(left, right)
A:dask.array.percentile.upper->numpy.maximum(left, right)
A:dask.array.percentile.lower_residual->numpy.abs(combined_q[lower] - desired_q)
A:dask.array.percentile.upper_residual->numpy.abs(combined_q[upper] - desired_q)
dask.array.percentile(a,q,interpolation='linear')
dask.array.percentile._percentile(a,q,interpolation='linear')
dask.array.percentile.merge_percentiles(finalq,qs,vals,Ns,interpolation='lower')
dask.array.percentile.percentile(a,q,interpolation='linear')


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/array/reductions.py----------------------------------------
A:dask.array.reductions.axis->tuple(range(x.ndim))
A:dask.array.reductions.chunk->partial(arg_chunk, func, argfunc)
A:dask.array.reductions.aggregate->partial(aggregate, dtype=dtype)
A:dask.array.reductions.inds->tuple(range(x.ndim))
A:dask.array.reductions.tmp->Array(merge(dsk, x.dask), name, chunks)
A:dask.array.reductions.tmp._chunks->tuple(((1,) * len(c) if i in axis else c for (i, c) in enumerate(tmp.chunks)))
A:dask.array.reductions.split_every->dict(((k, v) for (k, v) in enumerate(x.numblocks) if k in axis))
A:dask.array.reductions.n->sum(ns, **keepdim_kw)
A:dask.array.reductions.depth->int(builtins.max(depth, ceil(log(n, split_every[i]))))
A:dask.array.reductions.func->compose(partial(aggregate, axis=axis, keepdims=keepdims), partial(_concatenate2, axes=axis))
A:dask.array.reductions.x->partial_reduce(func, x, split_every, True, None, name=(name or funcname(combine or aggregate)) + '-partial')
A:dask.array.reductions.keys->list(product(*map(range, x.numblocks)))
A:dask.array.reductions.out_chunks->list(getter(out_chunks))
A:dask.array.reductions.decided->dict(((i, j[0]) for (i, j) in enumerate(p) if len(j) == 1))
A:dask.array.reductions.dummy->dict((i for i in enumerate(p) if i[0] not in decided))
A:dask.array.reductions.g->lol_tuples((x.name,), range(x.ndim), decided, dummy)
A:dask.array.reductions.total->totals.sum(**kwargs)
A:dask.array.reductions.result->numpy.empty(shape=vals.shape, dtype=[('vals', vals.dtype), ('arg', arg.dtype)])
A:dask.array.reductions.nanmean->wraps(chunk.nanmean)(nanmean)
A:dask.array.reductions.M->_moment_helper(Ms, ns, inner_term, order, sum, kwargs)
A:dask.array.reductions.M[..., i - 2]->sum((A - u) ** i, dtype=dtype, **kwargs)
A:dask.array.reductions.mu->divide(totals.sum(**keepdim_kw), n, dtype=dtype)
A:dask.array.reductions.M[..., o - 2]->_moment_helper(Ms, ns, inner_term, o, sum, kwargs)
A:dask.array.reductions.keepdim_kw->kwargs.copy()
A:dask.array.reductions.nanvar->wraps(chunk.nanvar)(nanvar)
A:dask.array.reductions.nanstd->wraps(chunk.nanstd)(nanstd)
A:dask.array.reductions.local_args->argfunc(vals, axis=axis)
A:dask.array.reductions.vals->func(x, axis=arg_axis, keepdims=True)
A:dask.array.reductions.arg->argfunc(x, axis=arg_axis, keepdims=True)
A:dask.array.reductions.ind->numpy.unravel_index(arg.ravel()[0], x.shape)
A:dask.array.reductions.total_ind->tuple((o + i for (o, i) in zip(offset, ind)))
A:dask.array.reductions.arg[:]->numpy.ravel_multi_index(total_ind, total_shape)
A:dask.array.reductions.(arg, vals)->_arg_combine(data, axis, argfunc, keepdims=False)
A:dask.array.reductions.name->'arg-reduce-chunk-{0}'.format(tokenize(chunk, axis))
A:dask.array.reductions.offsets->list(product(*(accumulate(operator.add, bd[:-1], 0) for bd in x.chunks)))
A:dask.array.reductions.offset_info->pluck(axis[0], offsets)
A:dask.array.reductions.chunks->tuple(((1,) * len(c) if i in axis else c for (i, c) in enumerate(x.chunks)))
A:dask.array.reductions.dsk->dict()
A:dask.array.reductions.combine->partial(arg_combine, func, argfunc)
A:dask.array.reductions.agg->partial(arg_agg, func, argfunc)
A:dask.array.reductions.argmin->make_arg_reduction(chunk.min, chunk.argmin)
A:dask.array.reductions.argmax->make_arg_reduction(chunk.max, chunk.argmax)
A:dask.array.reductions.nanargmin->make_arg_reduction(chunk.nanmin, _nanargmin, True)
A:dask.array.reductions.nanargmax->make_arg_reduction(chunk.nanmax, _nanargmax, True)
A:dask.array.reductions.m->partial_reduce(func, x, split_every, True, None, name=(name or funcname(combine or aggregate)) + '-partial').map_blocks(func, axis=axis, dtype=dtype)
A:dask.array.reductions.full->slice(None, None, None)
A:dask.array.reductions.indices->list(product(*[range(nb) if ii != axis else [i] for (ii, nb) in enumerate(x.numblocks)]))
A:dask.array.reductions.shape->tuple((x.chunks[i][ii] if i != axis else 1 for (i, ii) in enumerate(ind)))
dask.array.all(a,axis=None,keepdims=False,split_every=None)
dask.array.any(a,axis=None,keepdims=False,split_every=None)
dask.array.cumprod(x,axis,dtype=None)
dask.array.cumsum(x,axis,dtype=None)
dask.array.max(a,axis=None,keepdims=False,split_every=None)
dask.array.mean(a,axis=None,dtype=None,keepdims=False,split_every=None)
dask.array.mean_agg(pair,dtype='f8',**kwargs)
dask.array.mean_chunk(x,sum=chunk.sum,numel=numel,dtype='f8',**kwargs)
dask.array.mean_combine(pair,sum=chunk.sum,numel=numel,dtype='f8',**kwargs)
dask.array.min(a,axis=None,keepdims=False,split_every=None)
dask.array.moment(a,order,axis=None,dtype=None,keepdims=False,ddof=0,split_every=None)
dask.array.moment_agg(data,order=2,ddof=0,dtype='f8',sum=np.sum,**kwargs)
dask.array.moment_chunk(A,order=2,sum=chunk.sum,numel=numel,dtype='f8',**kwargs)
dask.array.moment_combine(data,order=2,ddof=0,dtype='f8',sum=np.sum,**kwargs)
dask.array.nanmax(a,axis=None,keepdims=False,split_every=None)
dask.array.nanmean(a,axis=None,dtype=None,keepdims=False,split_every=None)
dask.array.nanmin(a,axis=None,keepdims=False,split_every=None)
dask.array.nanstd(a,axis=None,dtype=None,keepdims=False,ddof=0,split_every=None)
dask.array.nansum(a,axis=None,dtype=None,keepdims=False,split_every=None)
dask.array.nanvar(a,axis=None,dtype=None,keepdims=False,ddof=0,split_every=None)
dask.array.prod(a,axis=None,dtype=None,keepdims=False,split_every=None)
dask.array.reductions._arg_combine(data,axis,argfunc,keepdims=False)
dask.array.reductions._moment_helper(Ms,ns,inner_term,order,sum,kwargs)
dask.array.reductions._nanargmax(x,axis,**kwargs)
dask.array.reductions._nanargmin(x,axis,**kwargs)
dask.array.reductions._tree_reduce(x,aggregate,axis,keepdims,dtype,split_every=None,combine=None,name=None)
dask.array.reductions.all(a,axis=None,keepdims=False,split_every=None)
dask.array.reductions.any(a,axis=None,keepdims=False,split_every=None)
dask.array.reductions.arg_agg(func,argfunc,data,axis=None,**kwargs)
dask.array.reductions.arg_chunk(func,argfunc,x,axis,offset_info)
dask.array.reductions.arg_combine(func,argfunc,data,axis=None,**kwargs)
dask.array.reductions.arg_reduction(x,chunk,combine,agg,axis=None,split_every=None)
dask.array.reductions.cumprod(x,axis,dtype=None)
dask.array.reductions.cumreduction(func,binop,ident,x,axis,dtype=None)
dask.array.reductions.cumsum(x,axis,dtype=None)
dask.array.reductions.make_arg_reduction(func,argfunc,is_nan_func=False)
dask.array.reductions.max(a,axis=None,keepdims=False,split_every=None)
dask.array.reductions.mean(a,axis=None,dtype=None,keepdims=False,split_every=None)
dask.array.reductions.mean_agg(pair,dtype='f8',**kwargs)
dask.array.reductions.mean_chunk(x,sum=chunk.sum,numel=numel,dtype='f8',**kwargs)
dask.array.reductions.mean_combine(pair,sum=chunk.sum,numel=numel,dtype='f8',**kwargs)
dask.array.reductions.min(a,axis=None,keepdims=False,split_every=None)
dask.array.reductions.moment(a,order,axis=None,dtype=None,keepdims=False,ddof=0,split_every=None)
dask.array.reductions.moment_agg(data,order=2,ddof=0,dtype='f8',sum=np.sum,**kwargs)
dask.array.reductions.moment_chunk(A,order=2,sum=chunk.sum,numel=numel,dtype='f8',**kwargs)
dask.array.reductions.moment_combine(data,order=2,ddof=0,dtype='f8',sum=np.sum,**kwargs)
dask.array.reductions.nanarg_agg(func,argfunc,data,axis=None,**kwargs)
dask.array.reductions.nanmax(a,axis=None,keepdims=False,split_every=None)
dask.array.reductions.nanmean(a,axis=None,dtype=None,keepdims=False,split_every=None)
dask.array.reductions.nanmin(a,axis=None,keepdims=False,split_every=None)
dask.array.reductions.nannumel(x,**kwargs)
dask.array.reductions.nanstd(a,axis=None,dtype=None,keepdims=False,ddof=0,split_every=None)
dask.array.reductions.nansum(a,axis=None,dtype=None,keepdims=False,split_every=None)
dask.array.reductions.nanvar(a,axis=None,dtype=None,keepdims=False,ddof=0,split_every=None)
dask.array.reductions.numel(x,**kwargs)
dask.array.reductions.partial_reduce(func,x,split_every,keepdims=False,dtype=None,name=None)
dask.array.reductions.prod(a,axis=None,dtype=None,keepdims=False,split_every=None)
dask.array.reductions.reduction(x,chunk,aggregate,axis=None,keepdims=None,dtype=None,split_every=None,combine=None,name=None)
dask.array.reductions.std(a,axis=None,dtype=None,keepdims=False,ddof=0,split_every=None)
dask.array.reductions.sum(a,axis=None,dtype=None,keepdims=False,split_every=None)
dask.array.reductions.var(a,axis=None,dtype=None,keepdims=False,ddof=0,split_every=None)
dask.array.reductions.vnorm(a,ord=None,axis=None,dtype=None,keepdims=False,split_every=None)
dask.array.std(a,axis=None,dtype=None,keepdims=False,ddof=0,split_every=None)
dask.array.sum(a,axis=None,dtype=None,keepdims=False,split_every=None)
dask.array.var(a,axis=None,dtype=None,keepdims=False,ddof=0,split_every=None)
dask.array.vnorm(a,ord=None,axis=None,dtype=None,keepdims=False,split_every=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/array/learn.py----------------------------------------
A:dask.array.learn.x->x.reblock(chunks=(x.chunks[0], sum(x.chunks[1]))).reblock(chunks=(x.chunks[0], sum(x.chunks[1])))
A:dask.array.learn.nblocks->len(x.chunks[0])
A:dask.array.learn.func->partial(_predict, model)
dask.array.learn._partial_fit(model,x,y,kwargs=None)
dask.array.learn._predict(model,x)
dask.array.learn.fit(model,x,y,get=threaded.get,**kwargs)
dask.array.learn.predict(model,x)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/array/tests/test_linalg.py----------------------------------------
A:dask.array.tests.test_linalg.mat->numpy.random.randn(m, r).dot(mat2)
A:dask.array.tests.test_linalg.data->dask.array.from_array(mat, chunks=(500, 50))
A:dask.array.tests.test_linalg.(q, r)->tsqr(data)
A:dask.array.tests.test_linalg.q->numpy.array(q)
A:dask.array.tests.test_linalg.r->numpy.array(r)
A:dask.array.tests.test_linalg.(u, s, vt)->svd_compressed(x, 3, seed=1234)
A:dask.array.tests.test_linalg.u->numpy.array(u)
A:dask.array.tests.test_linalg.s->numpy.array(s)
A:dask.array.tests.test_linalg.vt->numpy.array(vt)
A:dask.array.tests.test_linalg.usvt->numpy.dot(u, np.dot(np.diag(s), vt))
A:dask.array.tests.test_linalg.(q1, r1)->qr(data)
A:dask.array.tests.test_linalg.(q2, r2)->qr(data)
A:dask.array.tests.test_linalg.(u1, s1, v1)->svd(data)
A:dask.array.tests.test_linalg.(u2, s2, v2)->svd(data)
A:dask.array.tests.test_linalg.mat1->numpy.random.randn(m, r)
A:dask.array.tests.test_linalg.mat2->numpy.random.randn(r, n)
A:dask.array.tests.test_linalg.x->dask.array.random.RandomState(1234).random_sample(size=(m, n), chunks=(5, 5))
A:dask.array.tests.test_linalg.(u2, s2, vt2)->svd_compressed(x, 3, seed=1234)
A:dask.array.tests.test_linalg.A1->numpy.array([[7, 3, -1, 2], [3, 8, 1, -4], [-1, 1, 4, -1], [2, -4, -1, 6]])
A:dask.array.tests.test_linalg.A2->numpy.array([[7, 0, 0, 0, 0, 0], [0, 8, 0, 0, 0, 0], [0, 0, 4, 0, 0, 0], [0, 0, 0, 6, 0, 0], [0, 0, 0, 0, 3, 0], [0, 0, 0, 0, 0, 5]])
A:dask.array.tests.test_linalg.dA->dask.array.from_array(A, (chunk, ncol))
A:dask.array.tests.test_linalg.(p, l, u)->scipy.linalg.lu(A)
A:dask.array.tests.test_linalg.(dp, dl, du)->dask.array.linalg.lu(dA)
A:dask.array.tests.test_linalg.A3->numpy.array([[7, 3, 2, 1, 4, 1], [7, 11, 5, 2, 5, 2], [21, 25, 16, 10, 16, 5], [21, 41, 18, 13, 16, 11], [14, 46, 23, 24, 21, 22], [0, 56, 29, 17, 14, 8]])
A:dask.array.tests.test_linalg.A->numpy.random.random_integers(1, 20, (nrow, ncol))
A:dask.array.tests.test_linalg.b->numpy.random.random_integers(1, 20, nrow)
A:dask.array.tests.test_linalg.Au->numpy.triu(A)
A:dask.array.tests.test_linalg.dAu->dask.array.from_array(Au, (chunk, chunk))
A:dask.array.tests.test_linalg.db->dask.array.from_array(b, chunk)
A:dask.array.tests.test_linalg.res->dask.array.linalg.solve(dA, db, sym_pos=True)
A:dask.array.tests.test_linalg.Al->numpy.tril(A)
A:dask.array.tests.test_linalg.dAl->dask.array.from_array(Al, (chunk, chunk))
A:dask.array.tests.test_linalg.lA->numpy.tril(A)
A:dask.array.tests.test_linalg.(x, r, rank, s)->numpy.linalg.lstsq(A, b)
A:dask.array.tests.test_linalg.(dx, dr, drank, ds)->dask.array.linalg.lstsq(dA, db)
dask.array.tests.test_linalg._check_lu_result(p,l,u,A)
dask.array.tests.test_linalg._get_symmat(size)
dask.array.tests.test_linalg.same_keys(a,b)
dask.array.tests.test_linalg.test_cholesky(shape,chunk)
dask.array.tests.test_linalg.test_inv(shape,chunk)
dask.array.tests.test_linalg.test_linalg_consistent_names()
dask.array.tests.test_linalg.test_lstsq(nrow,ncol,chunk)
dask.array.tests.test_linalg.test_lu_1()
dask.array.tests.test_linalg.test_lu_2(size)
dask.array.tests.test_linalg.test_lu_3(size)
dask.array.tests.test_linalg.test_lu_errors()
dask.array.tests.test_linalg.test_solve(shape,chunk)
dask.array.tests.test_linalg.test_solve_sym_pos(shape,chunk)
dask.array.tests.test_linalg.test_solve_triangular_errors()
dask.array.tests.test_linalg.test_solve_triangular_matrix(shape,chunk)
dask.array.tests.test_linalg.test_solve_triangular_matrix2(shape,chunk)
dask.array.tests.test_linalg.test_solve_triangular_vector(shape,chunk)
dask.array.tests.test_linalg.test_svd_compressed()
dask.array.tests.test_linalg.test_svd_compressed_deterministic()
dask.array.tests.test_linalg.test_tsqr_irregular_blocks()
dask.array.tests.test_linalg.test_tsqr_regular_blocks()
dask.array.tests.test_linalg.test_tsqr_svd_irregular_blocks()
dask.array.tests.test_linalg.test_tsqr_svd_regular_blocks()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/array/tests/test_ghost.py----------------------------------------
A:dask.array.tests.test_ghost.a->numpy.arange(1 * 9).reshape(1, 9)
A:dask.array.tests.test_ghost.b->boundaries(darr, {0: 0, 1: 0}, {0: 0, 1: 0})
A:dask.array.tests.test_ghost.c->c.all().all()
A:dask.array.tests.test_ghost.x->dask.array.from_array(np.arange(16).reshape(4, 4), chunks=(2, 2))
A:dask.array.tests.test_ghost.d->dask.array.from_array(x, chunks=(2, 2))
A:dask.array.tests.test_ghost.g->ghost(darr, depth=0, boundary=0)
A:dask.array.tests.test_ghost.result->trim_internal(constant, depth)
A:dask.array.tests.test_ghost.expected->numpy.arange(144).reshape(12, 12)
A:dask.array.tests.test_ghost.e->boundaries(d, {0: 2, 1: 1}, {0: 0, 1: 'periodic'})
A:dask.array.tests.test_ghost.y->dask.array.from_array(np.arange(16).reshape(4, 4), chunks=(2, 2)).map_overlap(lambda x: x + len(x), depth=2)
A:dask.array.tests.test_ghost.exp1->dask.array.from_array(x, chunks=(2, 2)).map_overlap(lambda x: x + x.size, depth=1).compute()
A:dask.array.tests.test_ghost.exp2->dask.array.from_array(x, chunks=(2, 2)).map_overlap(lambda x: x + x.size, depth={0: 1, 1: 1}, boundary={0: 'reflect', 1: 'none'}).compute()
A:dask.array.tests.test_ghost.darr->dask.array.from_array(expected, chunks=(5, 5))
A:dask.array.tests.test_ghost.garr->ghost(darr, depth={0: 5, 1: 5}, boundary={0: 'nearest', 1: 'nearest'})
A:dask.array.tests.test_ghost.tarr->trim_internal(garr, {0: 5, 1: 5})
A:dask.array.tests.test_ghost.reflected->ghost(darr, depth=depth, boundary='reflect')
A:dask.array.tests.test_ghost.nearest->ghost(darr, depth=depth, boundary='nearest')
A:dask.array.tests.test_ghost.periodic->ghost(darr, depth=depth, boundary='periodic')
A:dask.array.tests.test_ghost.constant->ghost(darr, depth=depth, boundary=42)
A:dask.array.tests.test_ghost.exp->boundaries(x, 2, {0: 'none', 1: 33})
A:dask.array.tests.test_ghost.res->numpy.array([[33, 33, 0, 1, 2, 3, 33, 33], [33, 33, 4, 5, 6, 7, 33, 33], [33, 33, 8, 9, 10, 11, 33, 33], [33, 33, 12, 13, 14, 15, 33, 33]])
dask.array.tests.test_ghost.eq(a,b)
dask.array.tests.test_ghost.same_keys(a,b)
dask.array.tests.test_ghost.test_0_depth()
dask.array.tests.test_ghost.test_bad_depth_raises()
dask.array.tests.test_ghost.test_boundaries()
dask.array.tests.test_ghost.test_constant()
dask.array.tests.test_ghost.test_constant_boundaries()
dask.array.tests.test_ghost.test_depth_equals_boundary_length()
dask.array.tests.test_ghost.test_depth_greater_than_boundary_length()
dask.array.tests.test_ghost.test_fractional_slice()
dask.array.tests.test_ghost.test_ghost()
dask.array.tests.test_ghost.test_ghost_internal()
dask.array.tests.test_ghost.test_map_overlap()
dask.array.tests.test_ghost.test_nearest()
dask.array.tests.test_ghost.test_nearest_ghost()
dask.array.tests.test_ghost.test_none_boundaries()
dask.array.tests.test_ghost.test_one_chunk_along_axis()
dask.array.tests.test_ghost.test_periodic()
dask.array.tests.test_ghost.test_reflect()
dask.array.tests.test_ghost.test_some_0_depth()
dask.array.tests.test_ghost.test_trim_internal()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/array/tests/test_optimization.py----------------------------------------
A:dask.array.tests.test_optimization.result->optimize_slices(fuse(dsk, ['c', 'd', 'e'])[0])
A:dask.array.tests.test_optimization.x->dask.array.random.random(size=(10, 10), chunks=(10, 1))
A:dask.array.tests.test_optimization.y->dask.array.random.random(size=(10, 10), chunks=(10, 1)).rechunk((1, 10))
A:dask.array.tests.test_optimization.dsk->optimize(y.dask, y._keys())
dask.array.tests.test_optimization.test_dont_fuse_different_slices()
dask.array.tests.test_optimization.test_fuse_getitem()
dask.array.tests.test_optimization.test_fuse_slice()
dask.array.tests.test_optimization.test_fuse_slice_with_lists()
dask.array.tests.test_optimization.test_hard_fuse_slice_cases()
dask.array.tests.test_optimization.test_optimize_slicing()
dask.array.tests.test_optimization.test_optimize_with_getitem_fusion()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/array/tests/test_rechunk.py----------------------------------------
A:dask.array.tests.test_rechunk.new->cumdims_label(((1, 1, 2), (1, 5, 1)), 'n')
A:dask.array.tests.test_rechunk.old->cumdims_label(((4,), (1,) * 5), 'o')
A:dask.array.tests.test_rechunk.breaks->tuple((_breakpoints(o, n) for (o, n) in zip(old, new)))
A:dask.array.tests.test_rechunk.cross->intersect_chunks(old_chunks=old, new_chunks=new)
A:dask.array.tests.test_rechunk.a->numpy.array(42)
A:dask.array.tests.test_rechunk.x->dask.array.random.normal(10, 0.1, (10, 10), chunks=(10, 1))
A:dask.array.tests.test_rechunk.x2->dask.array.random.normal(10, 0.1, (10, 10), chunks=(10, 1)).rechunk(chunks=new)
A:dask.array.tests.test_rechunk.y->dask.array.random.normal(10, 0.1, (10, 10), chunks=(10, 1)).rechunk((1, 10))
A:dask.array.tests.test_rechunk.orig->numpy.random.uniform(0, 1, a ** b).reshape((a,) * b)
A:dask.array.tests.test_rechunk.new_blockdims->normalize_chunks(new_chunks, new_shape)
A:dask.array.tests.test_rechunk.check1->rechunk(x, chunks=new_chunks)
dask.array.tests.test_rechunk.test_dtype()
dask.array.tests.test_rechunk.test_intersect_1()
dask.array.tests.test_rechunk.test_intersect_2()
dask.array.tests.test_rechunk.test_rechunk_0d()
dask.array.tests.test_rechunk.test_rechunk_1d()
dask.array.tests.test_rechunk.test_rechunk_2d()
dask.array.tests.test_rechunk.test_rechunk_4d()
dask.array.tests.test_rechunk.test_rechunk_blockshape()
dask.array.tests.test_rechunk.test_rechunk_expand()
dask.array.tests.test_rechunk.test_rechunk_expand2()
dask.array.tests.test_rechunk.test_rechunk_intermediates()
dask.array.tests.test_rechunk.test_rechunk_internals_1()
dask.array.tests.test_rechunk.test_rechunk_method()
dask.array.tests.test_rechunk.test_rechunk_same()
dask.array.tests.test_rechunk.test_rechunk_with_dict()
dask.array.tests.test_rechunk.test_rechunk_with_empty_input()
dask.array.tests.test_rechunk.test_rechunk_with_integer()
dask.array.tests.test_rechunk.test_rechunk_with_null_dimensions()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/array/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/array/tests/test_percentiles.py----------------------------------------
A:dask.array.tests.test_percentiles.a->a.compute(get=dask.get).compute(get=dask.get)
A:dask.array.tests.test_percentiles.b->b.compute(get=dask.get).compute(get=dask.get)
A:dask.array.tests.test_percentiles.c->c.all().all()
A:dask.array.tests.test_percentiles.d->dask.array.from_array(x, chunks=(3,))
A:dask.array.tests.test_percentiles.x->dask.array.ones(10, chunks=((5, 0, 5),))
A:dask.array.tests.test_percentiles.x0->pandas.Categorical(['Alice', 'Bob', 'Charlie', 'Dennis', 'Alice', 'Alice'])
A:dask.array.tests.test_percentiles.x1->pandas.Categorical(['Alice', 'Bob', 'Charlie', 'Dennis', 'Alice', 'Alice'])
A:dask.array.tests.test_percentiles.p->dask.array.percentile(x, [50])
dask.array.tests.test_percentiles.eq(a,b)
dask.array.tests.test_percentiles.same_keys(a,b)
dask.array.tests.test_percentiles.test_percentile()
dask.array.tests.test_percentiles.test_percentile_with_categoricals()
dask.array.tests.test_percentiles.test_percentiles_with_empty_arrays()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/array/tests/test_slicing.py----------------------------------------
A:dask.array.tests.test_slicing.a->dask.array.from_array(x, chunks=(5, 5))
A:dask.array.tests.test_slicing.b->b.compute(get=dask.get).compute(get=dask.get)
A:dask.array.tests.test_slicing.c->c.all().all()
A:dask.array.tests.test_slicing.result->_slice_1d(104, [20, 23, 27, 13, 21], slice(100, 27, -3))
A:dask.array.tests.test_slicing.(result, chunks)->slice_array('y', 'x', ([5, 5], [5, 5]), (slice(0, 7), 1))
A:dask.array.tests.test_slicing.(chunks, dsk)->take('y', 'x', [(20, 20, 20, 20), (20, 20)], [1, 3, 5, 37], axis=1)
A:dask.array.tests.test_slicing.expected->merge(dict(((('y', i, 0), (getitem, ('x', i, 0), (slice(None, None, None), [1, 3, 5]))) for i in range(4))), dict(((('y', i, 1), (getitem, ('x', i, 1), (slice(None, None, None), [17]))) for i in range(4))))
A:dask.array.tests.test_slicing.(y, chunks)->slice_array('y', 'x', ((3, 3, 3, 1), (3, 3, 3, 1)), ([2, 1, 9], slice(None, None, None)))
A:dask.array.tests.test_slicing.(a, bd1)->slice_array('y', 'x', ((3, 3, 3, 1), (3, 3, 3, 1)), ([1, 2, 9], slice(None, None, None)))
A:dask.array.tests.test_slicing.(b, bd2)->slice_array('y', 'x', ((3, 3, 3, 1), (3, 3, 3, 1)), (np.array([1, 2, 9]), slice(None, None, None)))
A:dask.array.tests.test_slicing.(c, bd3)->slice_array('y', 'x', ((3, 3, 3, 1), (3, 3, 3, 1)), (i, slice(None, None, None)))
A:dask.array.tests.test_slicing.o->dask.array.ones((24, 16), chunks=((4, 8, 8, 4), (2, 6, 6, 2)))
A:dask.array.tests.test_slicing.x->dask.array.ones(5, chunks=(2,))
A:dask.array.tests.test_slicing.I->ReturnItem()
A:dask.array.tests.test_slicing.pd->pytest.importorskip('pandas')
A:dask.array.tests.test_slicing.(dsk_out, bd_out)->slice_array('in', 'out', blockdims, index)
dask.array.tests.test_slicing.ReturnItem(object)
dask.array.tests.test_slicing.ReturnItem.__getitem__(self,key)
dask.array.tests.test_slicing.eq(a,b)
dask.array.tests.test_slicing.same_keys(a,b)
dask.array.tests.test_slicing.test_empty_slice()
dask.array.tests.test_slicing.test_multiple_list_slicing()
dask.array.tests.test_slicing.test_new_blockdim()
dask.array.tests.test_slicing.test_oob_check()
dask.array.tests.test_slicing.test_sanitize_index()
dask.array.tests.test_slicing.test_slice_1d()
dask.array.tests.test_slicing.test_slice_array_1d()
dask.array.tests.test_slicing.test_slice_array_2d()
dask.array.tests.test_slicing.test_slice_list_then_None()
dask.array.tests.test_slicing.test_slice_lists()
dask.array.tests.test_slicing.test_slice_optimizations()
dask.array.tests.test_slicing.test_slice_singleton_value_on_boundary()
dask.array.tests.test_slicing.test_slice_stop_0()
dask.array.tests.test_slicing.test_slicing_and_chunks()
dask.array.tests.test_slicing.test_slicing_chunks()
dask.array.tests.test_slicing.test_slicing_consistent_names()
dask.array.tests.test_slicing.test_slicing_exhaustively()
dask.array.tests.test_slicing.test_slicing_with_negative_step_flops_keys()
dask.array.tests.test_slicing.test_slicing_with_newaxis()
dask.array.tests.test_slicing.test_slicing_with_numpy_arrays()
dask.array.tests.test_slicing.test_slicing_with_singleton_indices()
dask.array.tests.test_slicing.test_take()
dask.array.tests.test_slicing.test_take_sorted()
dask.array.tests.test_slicing.test_uneven_blockdims()
dask.array.tests.test_slicing.test_uneven_chunks()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/array/tests/test_creation.py----------------------------------------
A:dask.array.tests.test_creation.a->numpy.array(a)
A:dask.array.tests.test_creation.darr->dask.array.arange(7.7, 1.5, -0.8, chunks=3)
A:dask.array.tests.test_creation.nparr->numpy.arange(7.7, 1.5, -0.8)
dask.array.tests.test_creation.eq(a,b)
dask.array.tests.test_creation.test_arange()
dask.array.tests.test_creation.test_arange_cast_float_int_step()
dask.array.tests.test_creation.test_arange_float_step()
dask.array.tests.test_creation.test_arange_has_dtype()
dask.array.tests.test_creation.test_arange_working_float_step()
dask.array.tests.test_creation.test_linspace()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/array/tests/test_fft.py----------------------------------------
A:dask.array.tests.test_fft.a->a.compute(get=dask.get).compute(get=dask.get)
A:dask.array.tests.test_fft.adt->getattr(a, 'dtype', None)
A:dask.array.tests.test_fft.b->b.compute(get=dask.get).compute(get=dask.get)
A:dask.array.tests.test_fft.bdt->getattr(b, 'dtype', None)
A:dask.array.tests.test_fft.nparr->numpy.arange(100).reshape(10, 10)
A:dask.array.tests.test_fft.darr->dask.array.from_array(nparr, chunks=(1, 10))
A:dask.array.tests.test_fft.darr2->dask.array.from_array(nparr, chunks=(10, 1))
A:dask.array.tests.test_fft.bad_darr->dask.array.from_array(nparr, chunks=(5, 5))
dask.array.tests.test_fft.eq(a,b)
dask.array.tests.test_fft.mismatch_err(mismatch_type,got,expected)
dask.array.tests.test_fft.same_keys(a,b)
dask.array.tests.test_fft.test_cant_fft_chunked_axis()
dask.array.tests.test_fft.test_fft()
dask.array.tests.test_fft.test_fft_consistent_names()
dask.array.tests.test_fft.test_fft_n_kwarg()
dask.array.tests.test_fft.test_hfft()
dask.array.tests.test_fft.test_hfft_nkwarg()
dask.array.tests.test_fft.test_ifft()
dask.array.tests.test_fft.test_ifft_n_kwarg()
dask.array.tests.test_fft.test_ihfft()
dask.array.tests.test_fft.test_ihfft_n_kwarg()
dask.array.tests.test_fft.test_irfft()
dask.array.tests.test_fft.test_irfft_n_kwarg()
dask.array.tests.test_fft.test_rfft()
dask.array.tests.test_fft.test_rfft_n_kwarg()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/array/tests/test_image.py----------------------------------------
A:dask.array.tests.test_image.fn->os.path.join(dirname, 'image.%d.png' % i)
A:dask.array.tests.test_image.x->numpy.random.randint(0, 255, size=shape).astype('i1')
A:dask.array.tests.test_image.im->da_imread(globstring, preprocess=preprocess)
dask.array.tests.test_image.random_images(n,shape)
dask.array.tests.test_image.test_imread()
dask.array.tests.test_image.test_imread_with_custom_function()
dask.array.tests.test_image.test_preprocess()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/array/tests/test_linearoperator.py----------------------------------------
A:dask.array.tests.test_linearoperator.X->numpy.random.random(size=(3, 2))
A:dask.array.tests.test_linearoperator.y->numpy.random.random(size=(2, 1))
A:dask.array.tests.test_linearoperator.w->numpy.random.random(size=(3, 1))
A:dask.array.tests.test_linearoperator.square->numpy.random.random(size=(2, 2))
A:dask.array.tests.test_linearoperator.dX->dask.array.from_array(X, chunks=(2, 1))
A:dask.array.tests.test_linearoperator.npLO->scipy.sparse.linalg.aslinearoperator(X)
A:dask.array.tests.test_linearoperator.daLO->scipy.sparse.linalg.interface.MatrixLinearOperator(dX)
dask.array.tests.test_linearoperator.test_LinearOperator()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/array/tests/test_chunk.py----------------------------------------
A:dask.array.tests.test_chunk.summer_wrapped->keepdims_wrapper(summer)
A:dask.array.tests.test_chunk.a->numpy.arange(24).reshape(1, 2, 3, 4)
A:dask.array.tests.test_chunk.r->summer(a, axis=(1, 3))
A:dask.array.tests.test_chunk.rw->summer_wrapped(a, axis=(1, 3), keepdims=True)
A:dask.array.tests.test_chunk.rwf->summer_wrapped(a, axis=(1, 3), keepdims=False)
A:dask.array.tests.test_chunk.c->c.all().all()
A:dask.array.tests.test_chunk.x->numpy.random.randint(10, size=(24, 24))
A:dask.array.tests.test_chunk.y->coarsen(np.sum, x, {0: 2, 1: 4})
dask.array.tests.test_chunk.eq(a,b)
dask.array.tests.test_chunk.test_coarsen()
dask.array.tests.test_chunk.test_integer_input()
dask.array.tests.test_chunk.test_keepdims_wrapper_no_axis()
dask.array.tests.test_chunk.test_keepdims_wrapper_one_axis()
dask.array.tests.test_chunk.test_keepdims_wrapper_two_axes()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/array/tests/test_wrap.py----------------------------------------
A:dask.array.tests.test_wrap.a->ones(shape=(1000000, 1000000), chunks=(100000, 100000))
A:dask.array.tests.test_wrap.x->numpy.array(a)
dask.array.tests.test_wrap.test_can_make_really_big_array_of_ones()
dask.array.tests.test_wrap.test_full()
dask.array.tests.test_wrap.test_kwargs()
dask.array.tests.test_wrap.test_ones()
dask.array.tests.test_wrap.test_singleton_size()
dask.array.tests.test_wrap.test_size_as_list()
dask.array.tests.test_wrap.test_wrap_consistent_names()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/array/tests/test_reductions.py----------------------------------------
A:dask.array.tests.test_reductions.a->dask.array.from_array(x, chunks=2)
A:dask.array.tests.test_reductions.b->dask.array.from_array(x, chunks=2).sum(keepdims=True)
A:dask.array.tests.test_reductions.a1->da_func(darr, split_every=4)
A:dask.array.tests.test_reductions.a2->dask.array.from_array(x2, chunks=3)
A:dask.array.tests.test_reductions.x->dask.array.ones(5, chunks=(2,))
A:dask.array.tests.test_reductions.x2->numpy.arange(10)
A:dask.array.tests.test_reductions.x[:2, :2]->numpy.array([[1, 2], [3, 4]])
A:dask.array.tests.test_reductions.d->dask.array.from_array(x, chunks=(2, 2))
A:dask.array.tests.test_reductions.y->numpy.sum(np.zeros(4))
A:dask.array.tests.test_reductions.(dependencies, dependents)->get_deps(x.dask)
dask.array.tests.test_reductions.assert_max_deps(x,n,eq=True)
dask.array.tests.test_reductions.eq(a,b)
dask.array.tests.test_reductions.reduction_1d_test(da_func,darr,np_func,narr,use_dtype=True,split_every=True)
dask.array.tests.test_reductions.reduction_2d_test(da_func,darr,np_func,narr,use_dtype=True,split_every=True)
dask.array.tests.test_reductions.same_keys(a,b)
dask.array.tests.test_reductions.test_0d_array()
dask.array.tests.test_reductions.test_arg_reductions(dfunc,func)
dask.array.tests.test_reductions.test_moment()
dask.array.tests.test_reductions.test_nan()
dask.array.tests.test_reductions.test_nanarg_reductions(dfunc,func)
dask.array.tests.test_reductions.test_reduction_names()
dask.array.tests.test_reductions.test_reduction_on_scalar()
dask.array.tests.test_reductions.test_reductions_1D(dtype)
dask.array.tests.test_reductions.test_reductions_2D(dtype)
dask.array.tests.test_reductions.test_reductions_2D_nans()
dask.array.tests.test_reductions.test_reductions_with_negative_axes()
dask.array.tests.test_reductions.test_tree_reduce_depth()
dask.array.tests.test_reductions.test_tree_reduce_set_options()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/array/tests/test_learn.py----------------------------------------
A:dask.array.tests.test_learn.x->numpy.array([[1, 0], [2, 0], [3, 0], [4, 0], [0, 1], [0, 2], [3, 3], [4, 4]])
A:dask.array.tests.test_learn.y->numpy.array([1, 1, 1, 1, -1, -1, 0, 0])
A:dask.array.tests.test_learn.z->numpy.array([[1, -1], [-1, 1], [10, -10], [-10, 10]])
A:dask.array.tests.test_learn.X->dask.array.from_array(x, chunks=(3, 2))
A:dask.array.tests.test_learn.Y->dask.array.from_array(y, chunks=(3,))
A:dask.array.tests.test_learn.Z->dask.array.from_array(z, chunks=(2, 2))
A:dask.array.tests.test_learn.sgd->dask.array.learn.fit(sgd, X, Y, get=dask.get, classes=np.array([-1, 1]))
A:dask.array.tests.test_learn.result->dask.array.learn.predict(sgd, Z)
dask.array.tests.test_learn.test_fit()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/array/tests/test_random.py----------------------------------------
A:dask.array.tests.test_random.state->dask.array.random.RandomState(5)
A:dask.array.tests.test_random.x->dask.array.random.multinomial(20, [1 / 6.0] * 6, size=size, chunks=chunks)
A:dask.array.tests.test_random.y->numpy.random.multinomial(20, [1 / 6.0] * 6, size=size)
A:dask.array.tests.test_random.samples_1->dask.array.random.RandomState(42).normal(size=1000, chunks=10)
A:dask.array.tests.test_random.samples_2->dask.array.random.RandomState(42).normal(size=1000, chunks=10)
A:dask.array.tests.test_random.state1->dask.array.random.RandomState(42)
A:dask.array.tests.test_random.state2->dask.array.random.RandomState(42)
A:dask.array.tests.test_random.a->dask.array.random.normal(size=10, chunks=5)
A:dask.array.tests.test_random.b->dask.array.random.normal(size=10, chunks=5)
dask.array.tests.test_random.test_RandomState()
dask.array.tests.test_random.test_can_make_really_big_random_array()
dask.array.tests.test_random.test_concurrency()
dask.array.tests.test_random.test_determinisim_through_dask_values()
dask.array.tests.test_random.test_doc_randomstate()
dask.array.tests.test_random.test_docs()
dask.array.tests.test_random.test_kwargs()
dask.array.tests.test_random.test_multinomial()
dask.array.tests.test_random.test_parametrized_random_function()
dask.array.tests.test_random.test_random()
dask.array.tests.test_random.test_random_all()
dask.array.tests.test_random.test_random_seed()
dask.array.tests.test_random.test_randomstate_consistent_names()
dask.array.tests.test_random.test_serializability()
dask.array.tests.test_random.test_unique_names()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/dataframe/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/dataframe/categorical.py----------------------------------------
A:dask.dataframe.categorical.df->df.copy().copy()
A:dask.dataframe.categorical.df[col]->pandas.Categorical(df[col], categories=vals, ordered=False)
A:dask.dataframe.categorical.values->compute(*distincts, **kwargs)
A:dask.dataframe.categorical.func->partial(_categorize_block, categories=dict(zip(columns, values)))
A:dask.dataframe.categorical.index->pandas.CategoricalIndex(pd.Categorical.from_codes(df.index.values, categories['.index']))
A:dask.dataframe.categorical.cat->pandas.Categorical.from_codes(df.values, categories[df.name])
A:dask.dataframe.categorical.result->dict(((col, df[col].cat.categories) for col in df.columns if iscategorical(df.dtypes[col])))
dask.dataframe.categorical._categorize(categories,df)
dask.dataframe.categorical._categorize_block(df,categories)
dask.dataframe.categorical.categorize(df,columns=None,**kwargs)
dask.dataframe.categorical.get_categories(df)
dask.dataframe.categorical.iscategorical(dt)
dask.dataframe.categorical.strip_categories(df)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/dataframe/optimize.py----------------------------------------
A:dask.dataframe.optimize.(dsk2, dependencies)->cull(dsk, [keys])
A:dask.dataframe.optimize.dsk3->fuse_getitem(dsk2, Castra.load_partition, 3)
A:dask.dataframe.optimize.dsk4->fuse_castra_index(dsk3)
A:dask.dataframe.optimize.dsk5->fuse_getitem(dsk4, dataframe_from_ctable, 3)
A:dask.dataframe.optimize.(dsk6, _)->cull(dsk5, keys)
dask.dataframe.optimize(dsk,keys,**kwargs)
dask.dataframe.optimize.fuse_castra_index(dsk)
dask.dataframe.optimize.optimize(dsk,keys,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/dataframe/shuffle.py----------------------------------------
A:dask.dataframe.shuffle.divisions->list(divisions)
A:dask.dataframe.shuffle.categories->categories.copy().copy()
A:dask.dataframe.shuffle.categories['.index']->categories.copy().copy().pop(index)
A:dask.dataframe.shuffle.get->context._globals.get('get')
A:dask.dataframe.shuffle.df->df.set_index('new_index', drop=True).set_index('new_index', drop=True)
A:dask.dataframe.shuffle.shards->list(map(strip_categories, shards))
A:dask.dataframe.shuffle.token->tokenize(df, index, divisions)
A:dask.dataframe.shuffle.dsk2->dict((((name, i), (_set_partition, part, index, divisions, p, drop)) for (i, part) in enumerate(df._keys())))
A:dask.dataframe.shuffle.dsk4->dict((((name, i), (collect, i, p, meta, barrier_token)) for i in range(npartitions)))
A:dask.dataframe.shuffle.dsk->merge(df.dask, dsk1, dsk2, dsk3, dsk4)
A:dask.dataframe.shuffle.h->numpy.add(h, hash_series(col), h)
A:dask.dataframe.shuffle.cols->df.set_index('new_index', drop=True).set_index('new_index', drop=True).iteritems()
A:dask.dataframe.shuffle.rng->pandas.Series(np.arange(len(df)))
A:dask.dataframe.shuffle.index->pandas.Index(values)
A:dask.dataframe.shuffle.groups->pandas.Series(np.arange(len(df))).groupby(partitioning_index(index, npartitions))
A:dask.dataframe.shuffle.d->dict(((i, df.iloc[groups.groups[i]]) for i in range(npartitions) if i in groups.groups))
A:dask.dataframe.shuffle.res->p.get(group)
A:dask.dataframe.shuffle.result->DataFrame(dsk, name, metadata, divisions)
A:dask.dataframe.shuffle.inds->set(index.drop_duplicates())
A:dask.dataframe.shuffle.part->result[k].set_index('.old-index')
A:dask.dataframe.shuffle.stages->int(math.ceil(math.log(n) / math.log(max_branch)))
A:dask.dataframe.shuffle.k->int(math.ceil(n ** (1 / stages)))
A:dask.dataframe.shuffle.meta->df.set_index('new_index', drop=True).set_index('new_index', drop=True)._pd.set_index(index if np.isscalar(index) else index._pd)
A:dask.dataframe.shuffle.df2->map_partitions(shuffle_pre_partition_series, meta, df, index, divisions, drop)
A:dask.dataframe.shuffle.start->dict(((('shuffle-join-' + token, 0, inp), (df2._name, i) if i < df2.npartitions else df._pd) for (i, inp) in enumerate(inputs)))
A:dask.dataframe.shuffle.group->dict(((('shuffle-group-' + token, stage, inp), (shuffle_group, ('shuffle-join-' + token, stage - 1, inp), stage - 1, k)) for inp in inputs))
A:dask.dataframe.shuffle.split->dict(((('shuffle-split-' + token, stage, i, inp), (dict.get, ('shuffle-group-' + token, stage, inp), i, {})) for i in range(k) for inp in inputs))
A:dask.dataframe.shuffle.join->dict(((('shuffle-join-' + token, stage, inp), (_concat, [('shuffle-split-' + token, stage, inp[stage - 1], insert(inp, stage - 1, j)) for j in range(k)])) for inp in inputs))
A:dask.dataframe.shuffle.end->dict(((('shuffle-' + token, i), (post, ('shuffle-join-' + token, stages, inp), index if np.isscalar(index) else index.name)) for (i, inp) in enumerate(inputs)))
A:dask.dataframe.shuffle.metadata->df.set_index('new_index', drop=True).set_index('new_index', drop=True)._pd.set_index(index, drop=drop)
A:dask.dataframe.shuffle.(p, barrier_token, categories)->df.set_index('new_index', drop=True).set_index('new_index', drop=True)._get(dsk, [p, barrier_token, catname2], **kwargs)
A:dask.dataframe.shuffle.(dsk, _)->cull(dsk, list(dsk4.keys()))
dask.dataframe.shuffle._set_collect(group,p,barrier_token,columns)
dask.dataframe.shuffle._set_partition(df,index,divisions,p,drop=True)
dask.dataframe.shuffle.barrier(args)
dask.dataframe.shuffle.collect(group,p,meta,barrier_token)
dask.dataframe.shuffle.hash_series(s)
dask.dataframe.shuffle.new_categories(categories,index)
dask.dataframe.shuffle.partition(df,index,npartitions,p)
dask.dataframe.shuffle.partitioning_index(df,npartitions)
dask.dataframe.shuffle.set_index(df,index,npartitions=None,method=None,compute=True,drop=True,**kwargs)
dask.dataframe.shuffle.set_partition(df,index,divisions,method=None,compute=False,drop=True,max_branch=32,**kwargs)
dask.dataframe.shuffle.set_partition_disk(df,index,divisions,compute=False,drop=True,**kwargs)
dask.dataframe.shuffle.set_partition_tasks(df,index,divisions,max_branch=32,drop=True)
dask.dataframe.shuffle.shuffle(df,index,npartitions=None)
dask.dataframe.shuffle.shuffle_group(df,stage,k)
dask.dataframe.shuffle.shuffle_post_scalar(df,index_name)
dask.dataframe.shuffle.shuffle_post_series(df,index_name)
dask.dataframe.shuffle.shuffle_pre_partition_scalar(df,index,divisions,drop)
dask.dataframe.shuffle.shuffle_pre_partition_series(df,index,divisions,drop)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/dataframe/groupby.py----------------------------------------
A:dask.dataframe.groupby.grouped->_maybe_slice(grouped, self._slice)
A:dask.dataframe.groupby.df->shuffle(self.obj, self.index, **self.kwargs)
A:dask.dataframe.groupby.x->shuffle(self.obj, self.index, **self.kwargs).groupby(index).sum()
A:dask.dataframe.groupby.x2->g[g.columns[nc // 3:2 * nc // 3]].rename(columns=lambda c: c[:-3])
A:dask.dataframe.groupby.n->g[g.columns[-nc // 3:]].rename(columns=lambda c: c[:-6])
A:dask.dataframe.groupby.result->map_partitions(np.sqrt, v, v)
A:dask.dataframe.groupby.g->SeriesGroupBy(self.obj, index=self.index, slice=key, **self.kwargs)
A:dask.dataframe.groupby.nc->len(g.columns)
A:dask.dataframe.groupby.grouped.index->_maybe_slice(grouped, self._slice).index.get_level_values(level=0)
A:dask.dataframe.groupby.index->list(index.columns)
A:dask.dataframe.groupby.self._pd->self.obj._pd.groupby(self.index)
A:dask.dataframe.groupby.head->self.obj.head()
A:dask.dataframe.groupby.dummy->self._head().apply(func)
A:dask.dataframe.groupby.levels->list(range(len(self.index)))
A:dask.dataframe.groupby.meta->meta.groupby(self.index).var(ddof=1).groupby(self.index).var(ddof=1)
A:dask.dataframe.groupby.v->self.var(ddof)
dask.dataframe.groupby.DataFrameGroupBy(self,df,index=None,slice=None,**kwargs)
dask.dataframe.groupby.DataFrameGroupBy.__dir__(self)
dask.dataframe.groupby.DataFrameGroupBy.__getattr__(self,key)
dask.dataframe.groupby.DataFrameGroupBy.__getitem__(self,key)
dask.dataframe.groupby.DataFrameGroupBy.__init__(self,df,index=None,slice=None,**kwargs)
dask.dataframe.groupby.DataFrameGroupBy.column_info(self)
dask.dataframe.groupby.SeriesGroupBy(self,df,index,slice=None,**kwargs)
dask.dataframe.groupby.SeriesGroupBy.__init__(self,df,index,slice=None,**kwargs)
dask.dataframe.groupby.SeriesGroupBy.column_info(self)
dask.dataframe.groupby.SeriesGroupBy.nunique(self)
dask.dataframe.groupby._GroupBy(self,df,index=None,slice=None,**kwargs)
dask.dataframe.groupby._GroupBy.__init__(self,df,index=None,slice=None,**kwargs)
dask.dataframe.groupby._GroupBy._aca_agg(self,token,func,aggfunc=None)
dask.dataframe.groupby._GroupBy._head(self)
dask.dataframe.groupby._GroupBy._is_grouped_by_sliced_column(self,df,index)
dask.dataframe.groupby._GroupBy.apply(self,func,columns=no_default)
dask.dataframe.groupby._GroupBy.count(self)
dask.dataframe.groupby._GroupBy.get_group(self,key)
dask.dataframe.groupby._GroupBy.max(self)
dask.dataframe.groupby._GroupBy.mean(self)
dask.dataframe.groupby._GroupBy.min(self)
dask.dataframe.groupby._GroupBy.std(self,ddof=1)
dask.dataframe.groupby._GroupBy.sum(self)
dask.dataframe.groupby._GroupBy.var(self,ddof=1)
dask.dataframe.groupby._apply_chunk(df,index,func,columns)
dask.dataframe.groupby._count(g)
dask.dataframe.groupby._groupby_apply_index(df,ind,key,func)
dask.dataframe.groupby._groupby_apply_level0(df,key,func)
dask.dataframe.groupby._groupby_get_group(df,by_key,get_key,columns)
dask.dataframe.groupby._max(g)
dask.dataframe.groupby._maybe_slice(grouped,columns)
dask.dataframe.groupby._min(g)
dask.dataframe.groupby._nunique_df_chunk(df,index)
dask.dataframe.groupby._nunique_series_chunk(df,index)
dask.dataframe.groupby._sum(g)
dask.dataframe.groupby._var_agg(g,ddof)
dask.dataframe.groupby._var_chunk(df,index)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/dataframe/csv.py----------------------------------------
A:dask.dataframe.csv.delayed->delayed(pure=True)
A:dask.dataframe.csv.bio->BytesIO()
A:dask.dataframe.csv.df->read_csv_from_bytes(values, header, head, kwargs, collection=collection, enforce=enforce)
A:dask.dataframe.csv.df[c]->df[c].astype(dtypes[c]).astype(dtypes[c])
A:dask.dataframe.csv.dtypes->pandas.read_csv(BytesIO(sample), **kwargs).dtypes.to_dict()
A:dask.dataframe.csv.columns->list(head.columns)
A:dask.dataframe.csv.func->delayed(bytes_read_csv)
A:dask.dataframe.csv.b_lineterminator->lineterminator.encode()
A:dask.dataframe.csv.(sample, values)->read_bytes(filename, delimiter=b_lineterminator, blocksize=blocksize, sample=sample, compression=compression, **storage_options or {})
A:dask.dataframe.csv.head->pandas.read_csv(BytesIO(sample), **kwargs)
dask.dataframe.csv.bytes_read_csv(b,header,kwargs,dtypes=None,columns=None,write_header=True,enforce=False)
dask.dataframe.csv.coerce_dtypes(df,dtypes)
dask.dataframe.csv.read_csv(filename,blocksize=2**25,chunkbytes=None,collection=True,lineterminator=None,compression=None,sample=256000,enforce=False,storage_options=None,**kwargs)
dask.dataframe.csv.read_csv_from_bytes(block_lists,header,head,kwargs,collection=True,enforce=False)
dask.dataframe.read_csv(filename,blocksize=2**25,chunkbytes=None,collection=True,lineterminator=None,compression=None,sample=256000,enforce=False,storage_options=None,**kwargs)
dask.dataframe.read_csv_from_bytes(block_lists,header,head,kwargs,collection=True,enforce=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/dataframe/multi.py----------------------------------------
A:dask.dataframe.multi.divisions->tuple(divisions[min(present):max(present) + 2])
A:dask.dataframe.multi.result->pandas.merge(left, right, how=how, left_on=left_on, right_on=right_on, left_index=left_index, right_index=right_index, suffixes=suffixes)
A:dask.dataframe.multi.L->list()
A:dask.dataframe.multi.(indexer, dasks)->zip(*[x for x in enumerate(args) if isinstance(x[1], (_Frame, Scalar))])
A:dask.dataframe.multi.(dasks, _, _)->align_partitions(*dasks)
A:dask.dataframe.multi.parts->tuple(parts[min(present):max(present) + 1])
A:dask.dataframe.multi.((lhs, rhs), divisions, parts)->align_partitions(lhs, rhs)
A:dask.dataframe.multi.(divisions, parts)->require(divisions, parts, required[how])
A:dask.dataframe.multi.dsk->toolz.merge(dsk, df.dask, other.dask)
A:dask.dataframe.multi.dummy->df._pd.append(other._pd)
A:dask.dataframe.multi.left->from_pandas(left, npartitions=1)
A:dask.dataframe.multi.right->from_pandas(right, npartitions=1)
A:dask.dataframe.multi.npartitions->max(lhs.npartitions, rhs.npartitions)
A:dask.dataframe.multi.lhs2->shuffle(lhs, left_on, npartitions)
A:dask.dataframe.multi.rhs2->shuffle(rhs, right_on, npartitions)
A:dask.dataframe.multi.merger->partial(_pdmerge, suffixes=suffixes, default_left_columns=list(lhs.columns), default_right_columns=list(rhs.columns))
A:dask.dataframe.multi.token->tokenize(df, other)
A:dask.dataframe.multi.meta->pandas.merge(left._pd_nonempty, right._pd_nonempty, **kwargs)
A:dask.dataframe.multi.left_key->first(left._keys())
A:dask.dataframe.multi.right_key->first(right._keys())
A:dask.dataframe.multi.dfs->_maybe_from_pandas(dfs)
A:dask.dataframe.multi.(dfs2, divisions, parts)->align_partitions(*dfs)
A:dask.dataframe.multi.columns->pandas.Index([])
A:dask.dataframe.multi.axis->core.DataFrame._validate_axis(axis)
A:dask.dataframe.multi.name->'{0}-append--{1}'.format(df._token_prefix, token)
A:dask.dataframe.multi.(dsk, dummy)->_concat_dfs(dfs, name, join=join)
dask.dataframe.concat(dfs,axis=0,join='outer',interleave_partitions=False)
dask.dataframe.concat_indexed_dataframes(dfs,axis=0,join='outer')
dask.dataframe.melt(frame,id_vars=None,value_vars=None,var_name=None,value_name='value',col_level=None)
dask.dataframe.merge(left,right,how='inner',on=None,left_on=None,right_on=None,left_index=False,right_index=False,suffixes=('_x','_y'),npartitions=None)
dask.dataframe.multi._append(df,other,divisions)
dask.dataframe.multi._concat_dfs(dfs,name,join='outer')
dask.dataframe.multi._maybe_align_partitions(args)
dask.dataframe.multi._pdconcat(dfs,axis=0,join='outer')
dask.dataframe.multi._pdmerge(left,right,how,left_on,right_on,left_index,right_index,suffixes,default_left_columns,default_right_columns)
dask.dataframe.multi.align_partitions(*dfs)
dask.dataframe.multi.bound(seq,left,right)
dask.dataframe.multi.concat(dfs,axis=0,join='outer',interleave_partitions=False)
dask.dataframe.multi.concat_indexed_dataframes(dfs,axis=0,join='outer')
dask.dataframe.multi.hash_join(lhs,left_on,rhs,right_on,how='inner',npartitions=None,suffixes=('_x','_y'))
dask.dataframe.multi.join_indexed_dataframes(lhs,rhs,how='left',lsuffix='',rsuffix='')
dask.dataframe.multi.melt(frame,id_vars=None,value_vars=None,var_name=None,value_name='value',col_level=None)
dask.dataframe.multi.merge(left,right,how='inner',on=None,left_on=None,right_on=None,left_index=False,right_index=False,suffixes=('_x','_y'),npartitions=None)
dask.dataframe.multi.require(divisions,parts,required=None)
dask.dataframe.multi.single_partition_join(left,right,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/dataframe/rolling.py----------------------------------------
A:dask.dataframe.rolling.combined->pandas.concat([prev_partition.iloc[-extra:], this_partition])
A:dask.dataframe.rolling.applied->method(*method_args, **method_kwargs)
A:dask.dataframe.rolling.token->tokenize(func, arg, window, args, kwargs)
A:dask.dataframe.rolling.f->partial(func, **kwargs)
A:dask.dataframe.rolling.rolling_count->wrap_rolling(pd.rolling_count)
A:dask.dataframe.rolling.rolling_sum->wrap_rolling(pd.rolling_sum)
A:dask.dataframe.rolling.rolling_mean->wrap_rolling(pd.rolling_mean)
A:dask.dataframe.rolling.rolling_median->wrap_rolling(pd.rolling_median)
A:dask.dataframe.rolling.rolling_min->wrap_rolling(pd.rolling_min)
A:dask.dataframe.rolling.rolling_max->wrap_rolling(pd.rolling_max)
A:dask.dataframe.rolling.rolling_std->wrap_rolling(pd.rolling_std)
A:dask.dataframe.rolling.rolling_var->wrap_rolling(pd.rolling_var)
A:dask.dataframe.rolling.rolling_skew->wrap_rolling(pd.rolling_skew)
A:dask.dataframe.rolling.rolling_kurt->wrap_rolling(pd.rolling_kurt)
A:dask.dataframe.rolling.rolling_quantile->wrap_rolling(pd.rolling_quantile)
A:dask.dataframe.rolling.rolling_apply->wrap_rolling(pd.rolling_apply)
A:dask.dataframe.rolling.rolling_window->wrap_rolling(pd.rolling_window)
A:dask.dataframe.rolling.method->getattr(this_partition.rolling(window), method_name)
A:dask.dataframe.rolling.args->list(args)
A:dask.dataframe.rolling.tail_name->'tail-{}-{}'.format(self.window - 1, old_name)
A:dask.dataframe.rolling.pd_rolling->self.obj._pd.rolling(**self._rolling_kwargs())
A:dask.dataframe.rolling.metadata->getattr(pd_rolling, method_name)(*args, **kwargs)
dask.dataframe.rolling.Rolling(self,obj,window=None,min_periods=None,win_type=None,axis=0)
dask.dataframe.rolling.Rolling.__init__(self,obj,window=None,min_periods=None,win_type=None,axis=0)
dask.dataframe.rolling.Rolling.__repr__(self)
dask.dataframe.rolling.Rolling._call_method(self,method_name,*args,**kwargs)
dask.dataframe.rolling.Rolling._rolling_kwargs(self)
dask.dataframe.rolling.Rolling.apply(self,*args,**kwargs)
dask.dataframe.rolling.Rolling.count(self,*args,**kwargs)
dask.dataframe.rolling.Rolling.kurt(self,*args,**kwargs)
dask.dataframe.rolling.Rolling.max(self,*args,**kwargs)
dask.dataframe.rolling.Rolling.mean(self,*args,**kwargs)
dask.dataframe.rolling.Rolling.median(self,*args,**kwargs)
dask.dataframe.rolling.Rolling.min(self,*args,**kwargs)
dask.dataframe.rolling.Rolling.quantile(self,*args,**kwargs)
dask.dataframe.rolling.Rolling.skew(self,*args,**kwargs)
dask.dataframe.rolling.Rolling.std(self,*args,**kwargs)
dask.dataframe.rolling.Rolling.sum(self,*args,**kwargs)
dask.dataframe.rolling.Rolling.var(self,*args,**kwargs)
dask.dataframe.rolling.call_pandas_rolling_method_single(this_partition,rolling_kwargs,method_name,method_args,method_kwargs)
dask.dataframe.rolling.call_pandas_rolling_method_with_neighbor(prev_partition,this_partition,rolling_kwargs,method_name,method_args,method_kwargs)
dask.dataframe.rolling.rolling_chunk(func,part1,part2,window,*args)
dask.dataframe.rolling.tail(obj,n)
dask.dataframe.rolling.wrap_rolling(func)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/dataframe/demo.py----------------------------------------
A:dask.dataframe.demo.index->pandas.DatetimeIndex(start=start, end=end, freq=freq)
A:dask.dataframe.demo.state->numpy.random.RandomState(seed)
A:dask.dataframe.demo.columns->dict(((k, make[dt](len(index), state)) for (k, dt) in dtypes.items()))
A:dask.dataframe.demo.df->pandas.DataFrame(columns, index=index, columns=sorted(columns))
A:dask.dataframe.demo.divisions->list(pd.DatetimeIndex(start=start, end=end, freq=partition_freq))
A:dask.dataframe.demo.seeds->different_seeds(len(divisions), state)
A:dask.dataframe.demo.dsk->dict((((name, i), (make_timeseries_part, divisions[i], divisions[i + 1], dtypes, freq, seeds[i])) for i in range(len(divisions) - 1)))
A:dask.dataframe.demo.head->make_timeseries_part('2000', '2000', dtypes, '1H', 1)
dask.dataframe.demo.make_categorical(n,rstate)
dask.dataframe.demo.make_float(n,rstate)
dask.dataframe.demo.make_int(n,rstate)
dask.dataframe.demo.make_string(n,rstate)
dask.dataframe.demo.make_timeseries(start,end,dtypes,freq,partition_freq,seed=None)
dask.dataframe.demo.make_timeseries_part(start,end,dtypes,freq,seed)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/dataframe/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/dataframe/tests/test_optimize_dataframe.py----------------------------------------
A:dask.dataframe.tests.test_optimize_dataframe.dfs->list(dsk.values())
A:dask.dataframe.tests.test_optimize_dataframe.bc->bcolz.ctable([[1, 2, 3], [10, 20, 30]], names=['a', 'b'])
A:dask.dataframe.tests.test_optimize_dataframe.dsk2->merge(dict(((('x', i), (dataframe_from_ctable, bc, slice(0, 2), cols, {})) for i in [1, 2, 3])), dict(((('y', i), (getitem, ('x', i), (list, ['a', 'b']))) for i in [1, 2, 3])))
A:dask.dataframe.tests.test_optimize_dataframe.expected->dict(((('y', i), (dataframe_from_ctable, bc, slice(0, 2), (list, ['a', 'b']), {})) for i in [1, 2, 3]))
A:dask.dataframe.tests.test_optimize_dataframe.result->dask.dataframe.optimize(dsk2, [('y', i) for i in [1, 2, 3]])
A:dask.dataframe.tests.test_optimize_dataframe.df->c.to_dask()
A:dask.dataframe.tests.test_optimize_dataframe.dsk->dask.dataframe.optimize(df3.dask, df3._keys())
dask.dataframe.tests.test_optimize_dataframe.test_castra_column_store()
dask.dataframe.tests.test_optimize_dataframe.test_column_optimizations_with_bcolz_and_rewrite()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/dataframe/tests/test_groupby.py----------------------------------------
A:dask.dataframe.tests.test_groupby.pdf->pandas.DataFrame({'A': [1, 2, 3, 4] * 5, 'B': np.random.randn(20), 'C': np.random.randn(20), 'D': np.random.randn(20)})
A:dask.dataframe.tests.test_groupby.ddf->dask.dataframe.from_pandas(pdf, 3)
A:dask.dataframe.tests.test_groupby.gp->pandas.DataFrame({'A': [1, 2, 3, 4] * 5, 'B': np.random.randn(20), 'C': np.random.randn(20), 'D': np.random.randn(20)}).groupby('y')
A:dask.dataframe.tests.test_groupby.dp->dask.dataframe.from_pandas(pdf, 3).groupby('y')
A:dask.dataframe.tests.test_groupby.d->dask.dataframe.DataFrame(dsk, 'x', ['a', 'b'], [0, 4, 9, 9])
A:dask.dataframe.tests.test_groupby.full->dask.dataframe.DataFrame(dsk, 'x', ['a', 'b'], [0, 4, 9, 9]).compute()
A:dask.dataframe.tests.test_groupby.df->pandas.util.testing.makeTimeDataFrame()
A:dask.dataframe.tests.test_groupby.g->dask.dataframe.from_pandas(pdf, 3).groupby('a')
A:dask.dataframe.tests.test_groupby.e->dask.dataframe.DataFrame(dsk, 'x', ['a', 'b'], [0, 4, 9, 9]).set_index('a')
A:dask.dataframe.tests.test_groupby.efull->dask.dataframe.DataFrame(dsk, 'x', ['a', 'b'], [0, 4, 9, 9]).compute().set_index('a')
A:dask.dataframe.tests.test_groupby.sol->pandas.util.testing.makeTimeDataFrame().groupby(['a', 'c']).mean()
A:dask.dataframe.tests.test_groupby.res->dask.dataframe.from_pandas(pdf, 3).groupby(['a', 'c']).mean()
A:dask.dataframe.tests.test_groupby.ddgrouped->dask.dataframe.DataFrame(dsk, 'x', ['a', 'b'], [0, 4, 9, 9]).groupby(ddkey)
A:dask.dataframe.tests.test_groupby.pdgrouped->dask.dataframe.DataFrame(dsk, 'x', ['a', 'b'], [0, 4, 9, 9]).compute().groupby(pdkey)
A:dask.dataframe.tests.test_groupby.strings->list('aaabbccccdddeee')
A:dask.dataframe.tests.test_groupby.data->list(map(int, '123111223323412'))
A:dask.dataframe.tests.test_groupby.ps->pandas.DataFrame(dict(strings=strings, data=data))
A:dask.dataframe.tests.test_groupby.s->pandas.Series([1, 2, 2, 1, 1])
A:dask.dataframe.tests.test_groupby.expected->pandas.util.testing.makeTimeDataFrame().groupby('x').apply(func)
A:dask.dataframe.tests.test_groupby.result->dask.dataframe.from_pandas(pdf, 3).groupby('x').apply(func, columns='y')
A:dask.dataframe.tests.test_groupby.pd_group->pandas.Series([1, 2, 2, 1, 1]).groupby(s)
A:dask.dataframe.tests.test_groupby.ss->dask.dataframe.from_pandas(s, npartitions=2)
A:dask.dataframe.tests.test_groupby.dask_group->dask.dataframe.from_pandas(s, npartitions=2).groupby(ss)
A:dask.dataframe.tests.test_groupby.pd_group2->pandas.Series([1, 2, 2, 1, 1]).groupby(s + 1)
A:dask.dataframe.tests.test_groupby.dask_group2->dask.dataframe.from_pandas(s, npartitions=2).groupby(ss + 1)
A:dask.dataframe.tests.test_groupby.sss->dask.dataframe.from_pandas(s, npartitions=3)
A:dask.dataframe.tests.test_groupby.pdf1->pandas.DataFrame({'a': [1, 2, 6, 4, 4, 6, 4, 3, 7], 'b': [4, 2, 7, 3, 3, 1, 1, 1, 2]}, index=[0, 1, 3, 5, 6, 8, 9, 9, 9])
dask.dataframe.tests.test_groupby.groupby_error()
dask.dataframe.tests.test_groupby.groupby_internal_head()
dask.dataframe.tests.test_groupby.groupby_internal_repr()
dask.dataframe.tests.test_groupby.test_apply_shuffle()
dask.dataframe.tests.test_groupby.test_dataframe_groupby_nunique()
dask.dataframe.tests.test_groupby.test_dataframe_groupby_nunique_across_group_same_value()
dask.dataframe.tests.test_groupby.test_full_groupby()
dask.dataframe.tests.test_groupby.test_groupby_dir()
dask.dataframe.tests.test_groupby.test_groupby_get_group()
dask.dataframe.tests.test_groupby.test_groupby_index_array()
dask.dataframe.tests.test_groupby.test_groupby_multilevel_agg()
dask.dataframe.tests.test_groupby.test_groupby_multilevel_getitem()
dask.dataframe.tests.test_groupby.test_groupby_on_index()
dask.dataframe.tests.test_groupby.test_groupby_set_index()
dask.dataframe.tests.test_groupby.test_series_groupby()
dask.dataframe.tests.test_groupby.test_series_groupby_errors()
dask.dataframe.tests.test_groupby.test_series_groupby_propagates_names()
dask.dataframe.tests.test_groupby.test_split_apply_combine_on_series()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/dataframe/tests/test_demo.py----------------------------------------
A:dask.dataframe.tests.test_demo.df->dask.dataframe.demo.make_timeseries('2000', '2001', {'A': float}, freq='3H', partition_freq='3M')
A:dask.dataframe.tests.test_demo.a->dask.dataframe.demo.make_timeseries('2000', '2015', {'A': float, 'B': int, 'C': str}, freq='2D', partition_freq='6M', seed=123)
A:dask.dataframe.tests.test_demo.b->dask.dataframe.demo.make_timeseries('2000', '2015', {'A': float, 'B': int, 'C': str}, freq='2D', partition_freq='6M', seed=123)
dask.dataframe.tests.test_demo.test_make_timeseries()
dask.dataframe.tests.test_demo.test_no_overlaps()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/dataframe/tests/test_arithmetics_reduction.py----------------------------------------
A:dask.dataframe.tests.test_arithmetics_reduction.ddf1->dask.dataframe.DataFrame(dsk, 'x', ['a', 'b'], [0, 4, 9, 9])
A:dask.dataframe.tests.test_arithmetics_reduction.pdf1->dask.dataframe.DataFrame(dsk, 'x', ['a', 'b'], [0, 4, 9, 9]).compute()
A:dask.dataframe.tests.test_arithmetics_reduction.pdf2->pandas.DataFrame(np.random.randn(10, 4), index=list('abcdefghjk'), columns=list('ABCX'))
A:dask.dataframe.tests.test_arithmetics_reduction.pdf3->pandas.DataFrame({'a': [1, 2, 3, 4, 5], 'b': [3, 5, 2, 5, 7]}, index=[1, 2, 3, 4, 5])
A:dask.dataframe.tests.test_arithmetics_reduction.ddf2->dask.dataframe.from_pandas(pdf2, 2)
A:dask.dataframe.tests.test_arithmetics_reduction.ddf3->dask.dataframe.from_pandas(pdf3, 2)
A:dask.dataframe.tests.test_arithmetics_reduction.ddf4->dask.dataframe.from_pandas(pdf4, 2)
A:dask.dataframe.tests.test_arithmetics_reduction.pdf4->pandas.DataFrame({'a': [3, 2, 6, 7, 8], 'b': [9, 4, 2, 6, 2]}, index=[10, 11, 12, 13, 14])
A:dask.dataframe.tests.test_arithmetics_reduction.pdf5->pandas.DataFrame({'a': [1, 2, 3, 4, 5], 'b': [3, 5, 2, 5, 7]}, index=[1, 3, 5, 7, 9])
A:dask.dataframe.tests.test_arithmetics_reduction.ddf5->dask.dataframe.from_pandas(pdf5, 2)
A:dask.dataframe.tests.test_arithmetics_reduction.pdf6->pandas.DataFrame({'a': [3, 2, 6, 7, 8], 'b': [9, 4, 2, 6, 2]}, index=[2, 3, 4, 5, 6])
A:dask.dataframe.tests.test_arithmetics_reduction.ddf6->dask.dataframe.from_pandas(pdf6, 2)
A:dask.dataframe.tests.test_arithmetics_reduction.pdf7->pandas.DataFrame({'a': [1, 2, 3, 4, 5, 6, 7, 8], 'b': [5, 6, 7, 8, 1, 2, 3, 4]}, index=[0, 2, 4, 8, 9, 10, 11, 13])
A:dask.dataframe.tests.test_arithmetics_reduction.pdf8->pandas.DataFrame({'a': [5, 6, 7, 8, 4, 3, 2, 1], 'b': [2, 4, 5, 3, 4, 2, 1, 0]}, index=[1, 3, 4, 8, 9, 11, 12, 13])
A:dask.dataframe.tests.test_arithmetics_reduction.ddf7->dask.dataframe.from_pandas(pdf7, 3)
A:dask.dataframe.tests.test_arithmetics_reduction.ddf8->dask.dataframe.from_pandas(pdf8, 2)
A:dask.dataframe.tests.test_arithmetics_reduction.pdf9->pandas.DataFrame({'a': [1, 2, 3, 4, 5, 6, 7, 8], 'b': [5, 6, 7, 8, 1, 2, 3, 4]}, index=[0, 2, 4, 8, 9, 10, 11, 13])
A:dask.dataframe.tests.test_arithmetics_reduction.pdf10->pandas.DataFrame({'a': [5, 6, 7, 8, 4, 3, 2, 1], 'b': [2, 4, 5, 3, 4, 2, 1, 0]}, index=[0, 3, 4, 8, 9, 11, 12, 13])
A:dask.dataframe.tests.test_arithmetics_reduction.ddf9->dask.dataframe.from_pandas(pdf9, 3)
A:dask.dataframe.tests.test_arithmetics_reduction.ddf10->dask.dataframe.from_pandas(pdf10, 2)
A:dask.dataframe.tests.test_arithmetics_reduction.l->dask.dataframe.core.Scalar({('l', 0): 10}, 'l')
A:dask.dataframe.tests.test_arithmetics_reduction.r->dask.dataframe.core.Scalar({('r', 0): 4}, 'r')
A:dask.dataframe.tests.test_arithmetics_reduction.s->dask.dataframe.core.Scalar({('s', 0): 4}, 's')
A:dask.dataframe.tests.test_arithmetics_reduction.pds->pandas.Series(pd.timedelta_range('1 days', freq='D', periods=5))
A:dask.dataframe.tests.test_arithmetics_reduction.dds->dask.dataframe.from_pandas(pds, 2)
A:dask.dataframe.tests.test_arithmetics_reduction.pdf->pandas.DataFrame({'a': [1, 2, 3, 4, 5, 6, 7], 'b': [7, 6, 5, 4, 3, 2, 1]})
A:dask.dataframe.tests.test_arithmetics_reduction.ddf->dask.dataframe.from_pandas(df, 3)
A:dask.dataframe.tests.test_arithmetics_reduction.nans1->pandas.Series([1] + [np.nan] * 4 + [2] + [np.nan] * 3)
A:dask.dataframe.tests.test_arithmetics_reduction.nands1->dask.dataframe.from_pandas(nans1, 2)
A:dask.dataframe.tests.test_arithmetics_reduction.nans2->pandas.Series([1] + [np.nan] * 8)
A:dask.dataframe.tests.test_arithmetics_reduction.nands2->dask.dataframe.from_pandas(nans2, 2)
A:dask.dataframe.tests.test_arithmetics_reduction.nans3->pandas.Series([np.nan] * 9)
A:dask.dataframe.tests.test_arithmetics_reduction.nands3->dask.dataframe.from_pandas(nans3, 2)
A:dask.dataframe.tests.test_arithmetics_reduction.bools->pandas.Series([True, False, True, False, True], dtype=bool)
A:dask.dataframe.tests.test_arithmetics_reduction.boolds->dask.dataframe.from_pandas(bools, 2)
A:dask.dataframe.tests.test_arithmetics_reduction.df->pandas.DataFrame({'a': [1, 2, np.nan, 4, 5, 6, 7, 8], 'b': [1, 2, np.nan, np.nan, np.nan, 5, np.nan, np.nan], 'c': [np.nan] * 8})
dask.dataframe.tests.test_arithmetics_reduction.check_frame_arithmetics(l,r,el,er,allow_comparison_ops=True)
dask.dataframe.tests.test_arithmetics_reduction.check_series_arithmetics(l,r,el,er,allow_comparison_ops=True)
dask.dataframe.tests.test_arithmetics_reduction.test_arithmetics()
dask.dataframe.tests.test_arithmetics_reduction.test_arithmetics_different_index()
dask.dataframe.tests.test_arithmetics_reduction.test_frame_series_arithmetic_methods()
dask.dataframe.tests.test_arithmetics_reduction.test_get_numeric_data_unknown_part()
dask.dataframe.tests.test_arithmetics_reduction.test_reduction_series_invalid_axis()
dask.dataframe.tests.test_arithmetics_reduction.test_reductions()
dask.dataframe.tests.test_arithmetics_reduction.test_reductions_frame()
dask.dataframe.tests.test_arithmetics_reduction.test_reductions_frame_dtypes()
dask.dataframe.tests.test_arithmetics_reduction.test_reductions_frame_nan()
dask.dataframe.tests.test_arithmetics_reduction.test_reductions_non_numeric_dtypes()
dask.dataframe.tests.test_arithmetics_reduction.test_scalar_arithmetics()
dask.dataframe.tests.test_arithmetics_reduction.test_scalar_arithmetics_with_dask_instances()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/dataframe/tests/test_utils_dataframe.py----------------------------------------
A:dask.dataframe.tests.test_utils_dataframe.df->pandas.DataFrame({'x': [1, 2, 3, 4, 5, 6], 'y': list('abdabd')}, index=[10, 20, 30, 40, 50, 60])
A:dask.dataframe.tests.test_utils_dataframe.result->list(shard_df_on_index(df, [20, 50]))
A:dask.dataframe.tests.test_utils_dataframe.df1->pandas.DataFrame({'A': pd.Categorical(['Alice', 'Bob', 'Carol']), 'B': list('abc'), 'C': 'bar', 'D': 3.0, 'E': pd.Timestamp('2016-01-01'), 'F': pd.date_range('2016-01-01', periods=3, tz='America/New_York'), 'G': pd.Timedelta('1 hours'), 'H': np.void(b' ')}, columns=list('DCBAHGFE'))
A:dask.dataframe.tests.test_utils_dataframe.df3->nonempty_sample_df(df2)
dask.dataframe.tests.test_utils_dataframe.test_nonempty_sample_df()
dask.dataframe.tests.test_utils_dataframe.test_shard_df_on_index()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/dataframe/tseries/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/dataframe/tseries/resample.py----------------------------------------
A:dask.dataframe.tseries.resample.resampler->Resampler(obj, rule, **kwargs)
A:dask.dataframe.tseries.resample.out->_resample_apply(series, rule, how, resample_kwargs)
A:dask.dataframe.tseries.resample.rule->pandas.datetools.to_offset(rule)
A:dask.dataframe.tseries.resample.g->pandas.TimeGrouper(rule, how='count', closed=closed, label=label)
A:dask.dataframe.tseries.resample.divs->pandas.Series(range(len(divisions)), index=divisions)
A:dask.dataframe.tseries.resample.temp->pandas.Series(range(len(divisions)), index=divisions).resample(rule, how='count', closed=closed, label='left')
A:dask.dataframe.tseries.resample.newdivs->newdivs.tolist().tolist()
A:dask.dataframe.tseries.resample.outdivs->outdivs.tolist().tolist()
A:dask.dataframe.tseries.resample.(newdivs, outdivs)->_resample_bin_and_out_divs(self.obj.divisions, rule, **kwargs)
A:dask.dataframe.tseries.resample.partitioned->self.obj.repartition(newdivs, force=True)
A:dask.dataframe.tseries.resample.keys->self.obj.repartition(newdivs, force=True)._keys()
A:dask.dataframe.tseries.resample.args->zip(keys, outdivs, outdivs[1:], ['left'] * (len(keys) - 1) + [None])
dask.dataframe.tseries.resample.Resampler(self,obj,rule,**kwargs)
dask.dataframe.tseries.resample.Resampler.__init__(self,obj,rule,**kwargs)
dask.dataframe.tseries.resample.Resampler._agg(self,how,columns=None,fill_value=np.nan)
dask.dataframe.tseries.resample.Resampler.count(self)
dask.dataframe.tseries.resample.Resampler.first(self)
dask.dataframe.tseries.resample.Resampler.last(self)
dask.dataframe.tseries.resample.Resampler.max(self)
dask.dataframe.tseries.resample.Resampler.mean(self)
dask.dataframe.tseries.resample.Resampler.median(self)
dask.dataframe.tseries.resample.Resampler.min(self)
dask.dataframe.tseries.resample.Resampler.ohlc(self)
dask.dataframe.tseries.resample.Resampler.prod(self)
dask.dataframe.tseries.resample.Resampler.sem(self)
dask.dataframe.tseries.resample.Resampler.std(self)
dask.dataframe.tseries.resample.Resampler.sum(self)
dask.dataframe.tseries.resample.Resampler.var(self)
dask.dataframe.tseries.resample._resample_bin_and_out_divs(divisions,rule,closed='left',label='left')
dask.dataframe.tseries.resample._resample_series(series,start,end,reindex_closed,rule,resample_kwargs,how,fill_value)
dask.dataframe.tseries.resample.getnanos(rule)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/dataframe/tseries/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/dataframe/tseries/tests/test_resample.py----------------------------------------
A:dask.dataframe.tseries.tests.test_resample.index->pandas.date_range(start='20120102', periods=100, freq='T')
A:dask.dataframe.tseries.tests.test_resample.ps->pandas.Series(range(len(index)), index=index)
A:dask.dataframe.tseries.tests.test_resample.ds->dask.dataframe.from_pandas(s, npartitions=5)
A:dask.dataframe.tseries.tests.test_resample.result->resample(ds, freq, how=method, closed=closed, label=label)
A:dask.dataframe.tseries.tests.test_resample.expected->resample(ps, freq, how=method, closed=closed, label=label)
A:dask.dataframe.tseries.tests.test_resample.s->pandas.Series(range(len(index)), index=index)
dask.dataframe.tseries.tests.test_resample.test_series_resample(method,npartitions,freq,closed,label)
dask.dataframe.tseries.tests.test_resample.test_series_resample_not_implemented()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/tests/test_rewrite.py----------------------------------------
A:dask.tests.test_rewrite.t->Traverser(term)
A:dask.tests.test_rewrite.t2->Traverser(term).copy()
A:dask.tests.test_rewrite.rule1->RewriteRule((add, 'a', 1), (inc, 'a'), vars)
A:dask.tests.test_rewrite.rule2->RewriteRule((add, 'a', 'a'), (double, 'a'), vars)
A:dask.tests.test_rewrite.rule3->RewriteRule((add, (inc, 'a'), (inc, 'a')), (add, (double, 'a'), 2), vars)
A:dask.tests.test_rewrite.rule4->RewriteRule((add, (inc, 'b'), (inc, 'a')), (add, (add, 'a', 'b'), 2), vars)
A:dask.tests.test_rewrite.rule5->RewriteRule((sum, ['c', 'b', 'a']), (add, (add, 'a', 'b'), 'c'), vars)
A:dask.tests.test_rewrite.rule6->RewriteRule((list, 'x'), repl_list, ('x',))
A:dask.tests.test_rewrite.rs->RuleSet(*rules)
A:dask.tests.test_rewrite.matches->list(rs.iter_matches(term))
A:dask.tests.test_rewrite.new_term->RuleSet(*rules).rewrite(new_term)
dask.tests.test_rewrite.add(x,y)
dask.tests.test_rewrite.double(x)
dask.tests.test_rewrite.inc(x)
dask.tests.test_rewrite.repl_list(sd)
dask.tests.test_rewrite.test_RewriteRule()
dask.tests.test_rewrite.test_RewriteRuleSubs()
dask.tests.test_rewrite.test_RuleSet()
dask.tests.test_rewrite.test_args()
dask.tests.test_rewrite.test_head()
dask.tests.test_rewrite.test_matches()
dask.tests.test_rewrite.test_rewrite()
dask.tests.test_rewrite.test_traverser()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/tests/test_context.py----------------------------------------
A:dask.tests.test_context.x->dask.array.ones(10, chunks=(5,))
dask.tests.test_context.test_set_options_context_manger()
dask.tests.test_context.test_with_get()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/tests/test_optimize.py----------------------------------------
A:dask.tests.test_optimize.(culled, dependencies)->cull(d, 'out')
A:dask.tests.test_optimize.(dsk, dependencies)->fuse({'w': (inc, 'x'), 'x': (inc, 'y'), 'y': (inc, 'z'), 'z': (add, 'a', 'b'), 'a': 1, 'b': 2}, keys=['x', 'z'])
A:dask.tests.test_optimize.result->inline_functions(dsk, [], fast_functions=set([inc]))
A:dask.tests.test_optimize.t1->Uncomparable()
A:dask.tests.test_optimize.t2->Uncomparable()
A:dask.tests.test_optimize.(new_dsk, key_map)->merge_sync(dsk1, dsk2)
A:dask.tests.test_optimize.dsk2->fuse_selections(dsk, getitem, load, merge)
A:dask.tests.test_optimize.(dsk2, dependencies)->cull(dsk2, 'y')
dask.tests.test_optimize.Uncomparable(object)
dask.tests.test_optimize.Uncomparable.__eq__(self,other)
dask.tests.test_optimize.double(x)
dask.tests.test_optimize.inc(x)
dask.tests.test_optimize.test_cull()
dask.tests.test_optimize.test_dealias()
dask.tests.test_optimize.test_dealias_keys()
dask.tests.test_optimize.test_equivalence_uncomparable()
dask.tests.test_optimize.test_equivalent()
dask.tests.test_optimize.test_functions_of()
dask.tests.test_optimize.test_fuse()
dask.tests.test_optimize.test_fuse_getitem()
dask.tests.test_optimize.test_fuse_keys()
dask.tests.test_optimize.test_fuse_selections()
dask.tests.test_optimize.test_inline()
dask.tests.test_optimize.test_inline_doesnt_shrink_fast_functions_at_top()
dask.tests.test_optimize.test_inline_functions()
dask.tests.test_optimize.test_inline_ignores_curries_and_partials()
dask.tests.test_optimize.test_inline_protects_output_keys()
dask.tests.test_optimize.test_inline_traverses_lists()
dask.tests.test_optimize.test_merge_sync()
dask.tests.test_optimize.test_sync_keys()
dask.tests.test_optimize.test_sync_uncomparable()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/tests/test_delayed.py----------------------------------------
A:dask.tests.test_delayed.v->f(1, 2)
A:dask.tests.test_delayed.a->delayed(1)
A:dask.tests.test_delayed.b->pytest.importorskip('dask.bag').from_sequence([1, 2, 3])
A:dask.tests.test_delayed.(task, dasks)->to_task_dasks(darr)
A:dask.tests.test_delayed.f->delayed(foo)
A:dask.tests.test_delayed.x->delayed(1)
A:dask.tests.test_delayed.o->delayed(1).index(1)
A:dask.tests.test_delayed.dsk->delayed(1).index(1)._optimize(o.dask, o._keys())
A:dask.tests.test_delayed.add2->delayed(add)
A:dask.tests.test_delayed.c->delayed(f)(iter([a, b]))
A:dask.tests.test_delayed.lit->set((a, b, 3))
A:dask.tests.test_delayed.v1->delayed(add, pure=True)(1, 2)
A:dask.tests.test_delayed.v2->delayed(add, pure=True)(1, 2)
A:dask.tests.test_delayed.myrand->delayed(random)
A:dask.tests.test_delayed.dmysum->delayed(mysum, pure=True)
A:dask.tests.test_delayed.ten->dmysum(1, 2, c=c, four=dmysum(2, 2))
A:dask.tests.test_delayed.np->pytest.importorskip('numpy')
A:dask.tests.test_delayed.da->pytest.importorskip('dask.array')
A:dask.tests.test_delayed.arr->pytest.importorskip('numpy').arange(100).reshape((10, 10))
A:dask.tests.test_delayed.darr->pytest.importorskip('dask.array').from_array(arr, chunks=(5, 5))
A:dask.tests.test_delayed.val->delayed(sum)([arr, darr, 1])
A:dask.tests.test_delayed.orig->set(darr.dask)
A:dask.tests.test_delayed.final->set(dasks[0])
A:dask.tests.test_delayed.diff->set(dasks[0]).difference(orig)
A:dask.tests.test_delayed.db->pytest.importorskip('dask.bag')
A:dask.tests.test_delayed.arr1->pytest.importorskip('numpy').arange(100).reshape((10, 10))
A:dask.tests.test_delayed.arr2->pytest.importorskip('numpy').arange(100).reshape((10, 10)).dot(arr1.T)
A:dask.tests.test_delayed.darr1->pytest.importorskip('dask.array').from_array(arr1, chunks=(5, 5))
A:dask.tests.test_delayed.darr2->pytest.importorskip('dask.array').from_array(arr2, chunks=(5, 5))
A:dask.tests.test_delayed.out->delayed(sum)([i.sum() for i in seq])
A:dask.tests.test_delayed.y->pickle.loads(pickle.dumps(x))
A:dask.tests.test_delayed.foo->Foo(1)
A:dask.tests.test_delayed.func->delayed(identity, pure=True)
dask.tests.test_delayed.test_array_bag_delayed()
dask.tests.test_delayed.test_array_delayed()
dask.tests.test_delayed.test_attr_optimize()
dask.tests.test_delayed.test_attributes()
dask.tests.test_delayed.test_callable_obj()
dask.tests.test_delayed.test_common_subexpressions()
dask.tests.test_delayed.test_compute()
dask.tests.test_delayed.test_delayed()
dask.tests.test_delayed.test_delayed_callable()
dask.tests.test_delayed.test_delayed_compute_forward_kwargs()
dask.tests.test_delayed.test_delayed_name_on_call()
dask.tests.test_delayed.test_do_method_descriptor()
dask.tests.test_delayed.test_iterators()
dask.tests.test_delayed.test_key_names_include_function_names()
dask.tests.test_delayed.test_key_names_include_type_names()
dask.tests.test_delayed.test_kwargs()
dask.tests.test_delayed.test_lists()
dask.tests.test_delayed.test_lists_are_concrete()
dask.tests.test_delayed.test_literates()
dask.tests.test_delayed.test_literates_keys()
dask.tests.test_delayed.test_methods()
dask.tests.test_delayed.test_name_consitent_across_instances()
dask.tests.test_delayed.test_named_value()
dask.tests.test_delayed.test_operators()
dask.tests.test_delayed.test_pure()
dask.tests.test_delayed.test_sensitive_to_partials()
dask.tests.test_delayed.test_to_task_dasks()
dask.tests.test_delayed.test_value()
dask.tests.test_delayed.test_value_errors()
dask.tests.test_delayed.test_value_name()
dask.tests.test_delayed.test_value_picklable()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/tests/test_distributed.py----------------------------------------
dask.tests.test_distributed.test_can_import_executor()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/tests/test_core.py----------------------------------------
A:dask.tests.test_core.f->namedtuple('f', ['x', 'y'])
A:dask.tests.test_core.get->staticmethod(core.get)
A:dask.tests.test_core.custom_testget->TestCustomGetPass()
A:dask.tests.test_core.task->F()
A:dask.tests.test_core.df->pandas.DataFrame()
dask.tests.test_core.TestGet(GetFunctionTestMixin)
dask.tests.test_core.TestRecursiveGet(GetFunctionTestMixin)
dask.tests.test_core.TestRecursiveGet.test_get_stack_limit(self)
dask.tests.test_core.add(x,y)
dask.tests.test_core.contains(a,b)
dask.tests.test_core.inc(x)
dask.tests.test_core.test_GetFunctionTestMixin_class()
dask.tests.test_core.test__deps()
dask.tests.test_core.test_flatten()
dask.tests.test_core.test_get_dependencies_empty()
dask.tests.test_core.test_get_dependencies_list()
dask.tests.test_core.test_get_dependencies_nested()
dask.tests.test_core.test_has_tasks()
dask.tests.test_core.test_istask()
dask.tests.test_core.test_preorder_traversal()
dask.tests.test_core.test_quote()
dask.tests.test_core.test_subs()
dask.tests.test_core.test_subs_with_surprisingly_friendly_eq()
dask.tests.test_core.test_subs_with_unfriendly_eq()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/tests/test_order.py----------------------------------------
A:dask.tests.test_order.d->dict((((a, i), (f,)) for i in range(4)))
A:dask.tests.test_order.o->order(dsk)
A:dask.tests.test_order.(dependencies, dependents)->get_deps(dsk)
A:dask.tests.test_order.nd->ndependents(dependencies, dependents)
A:dask.tests.test_order.cm->child_max(dependencies, dependents, nd)
A:dask.tests.test_order.dsk->dict(chain((((a, i), i * 2) for i in range(5)), (((b, i), (add, i, (a, i))) for i in range(5)), (((c, i), (add, i, (b, i))) for i in range(5))))
A:dask.tests.test_order.scores->dict.fromkeys(dsk, 1)
A:dask.tests.test_order.result->ndependents(*get_deps(dsk))
A:dask.tests.test_order.expected->dict(chain((((a, i), 3) for i in range(5)), (((b, i), 2) for i in range(5)), (((c, i), 1) for i in range(5))))
A:dask.tests.test_order.deps->get_deps(dsk)
dask.tests.test_order.f(*args)
dask.tests.test_order.issorted(L,reverse=False)
dask.tests.test_order.test_base_of_reduce_preferred()
dask.tests.test_order.test_deep_bases_win_over_dependents()
dask.tests.test_order.test_ndependents()
dask.tests.test_order.test_ordering_keeps_groups_together()
dask.tests.test_order.test_prefer_broker_nodes()
dask.tests.test_order.test_prefer_deep()
dask.tests.test_order.test_stacklimit()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/tests/test_utils.py----------------------------------------
A:dask.tests.test_utils.SKIP_XZ->pytest.mark.skipif(not LZMA_AVAILABLE, reason='no lzma library')
A:dask.tests.test_utils.text->b'12 34 56 78'.replace(b' ', b'\r\n')
A:dask.tests.test_utils.foo->Dispatch()
A:dask.tests.test_utils.b->Bar()
A:dask.tests.test_utils.bin_euro->u''.encode(encoding)
A:dask.tests.test_utils.bin_yen->u''.encode(encoding)
A:dask.tests.test_utils.bin_linesep->linesep.encode(encoding)
A:dask.tests.test_utils.bin_data->data.encode(encoding)
A:dask.tests.test_utils.res->''.join(textblock(fn, 0, stop, encoding=encoding)).encode(encoding)
A:dask.tests.test_utils.state->numpy.random.RandomState(seed)
A:dask.tests.test_utils.seeds->set(different_seeds(n, seed))
A:dask.tests.test_utils.seeds2->set(different_seeds(n, state))
A:dask.tests.test_utils.smallseeds->different_seeds(10, 1234)
dask.tests.test_utils.test_different_seeds()
dask.tests.test_utils.test_dispatch()
dask.tests.test_utils.test_filesize(myopen,compression)
dask.tests.test_utils.test_gh606()
dask.tests.test_utils.test_takes_multiple_arguments()
dask.tests.test_utils.test_textblock(myopen,compression)
dask.tests.test_utils.test_textblock_multibyte_linesep()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/tests/test_dot.py----------------------------------------
A:dask.tests.test_dot.label_re->re.compile('.*\\[label=(.*?) shape=.*\\]')
A:dask.tests.test_dot.m->re.compile('.*\\[label=(.*?) shape=.*\\]').match(line)
A:dask.tests.test_dot.result->dot_graph(dsk, filename=filename, format=format)
A:dask.tests.test_dot.g->to_graphviz({'x': 1, 'y': 'x'})
A:dask.tests.test_dot.labels->list(filter(None, map(get_label, g.body)))
A:dask.tests.test_dot.funcs->set(('add', 'sum', 'neg'))
A:dask.tests.test_dot.filename->str(tmpdir.join('$(touch should_not_get_created.txt)'))
A:dask.tests.test_dot.target->'.'.join([default_name, default_format])
A:dask.tests.test_dot.before->tmpdir.listdir()
A:dask.tests.test_dot.after->tmpdir.listdir()
dask.tests.test_dot.get_label(line)
dask.tests.test_dot.test_aliases()
dask.tests.test_dot.test_dot_graph(tmpdir)
dask.tests.test_dot.test_dot_graph_defaults()
dask.tests.test_dot.test_dot_graph_no_filename(tmpdir)
dask.tests.test_dot.test_filenames_and_formats()
dask.tests.test_dot.test_label()
dask.tests.test_dot.test_task_label()
dask.tests.test_dot.test_to_graphviz()
dask.tests.test_dot.test_to_graphviz_attributes()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/tests/test_multiprocessing.py----------------------------------------
A:dask.tests.test_multiprocessing.p->multiprocessing.Pool()
A:dask.tests.test_multiprocessing.result->get(dsk, 'x')
A:dask.tests.test_multiprocessing.pool->multiprocessing.Pool()
dask.tests.test_multiprocessing.bad()
dask.tests.test_multiprocessing.make_bad_result()
dask.tests.test_multiprocessing.test_apply_lambda()
dask.tests.test_multiprocessing.test_dumps_loads()
dask.tests.test_multiprocessing.test_errors_propagate()
dask.tests.test_multiprocessing.test_fuse_doesnt_clobber_intermediates()
dask.tests.test_multiprocessing.test_optimize_graph_false()
dask.tests.test_multiprocessing.test_pickle_globals()
dask.tests.test_multiprocessing.test_reuse_pool()
dask.tests.test_multiprocessing.test_unpicklable_results_genreate_errors()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/store/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/store/core.py----------------------------------------
A:dask.store.core.self.dsk->dict()
A:dask.store.core.cache->dict()
A:dask.store.core.self.data->set()
A:dask.store.core.self.compute_time->dict()
A:dask.store.core.self.access_times->defaultdict(list)
A:dask.store.core.start->time()
A:dask.store.core.result->func(*args)
A:dask.store.core.end->time()
dask.store.Store(self,cache=None)
dask.store.Store.__delitem__(self,key)
dask.store.Store.__getitem__(self,key)
dask.store.Store.__iter__(self)
dask.store.Store.__len__(self)
dask.store.Store.__setitem__(self,key,value)
dask.store.core.Store(self,cache=None)
dask.store.core.Store.__delitem__(self,key)
dask.store.core.Store.__getitem__(self,key)
dask.store.core.Store.__init__(self,cache=None)
dask.store.core.Store.__iter__(self)
dask.store.core.Store.__len__(self)
dask.store.core.Store.__setitem__(self,key,value)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/store/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/store/tests/test_store.py----------------------------------------
A:dask.store.tests.test_store.s->Store()
A:dask.store.tests.test_store.cache->Store().cache.copy()
dask.store.tests.test_store.inc(x)
dask.store.tests.test_store.test_basic()
dask.store.tests.test_store.test_update()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/diagnostics/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/diagnostics/profile_visualize.py----------------------------------------
A:dask.diagnostics.profile_visualize.head->funcname(task[0])
A:dask.diagnostics.profile_visualize.label_size2->int((label_size - 2 - 2 * len(task)) // len(task))
A:dask.diagnostics.profile_visualize.args->', '.join((pprint_task(t, keys, label_size2) for t in task))
A:dask.diagnostics.profile_visualize.result->pprint_task(task[:3], keys, label_size)
A:dask.diagnostics.profile_visualize.unique_funcs->list(sorted(unique(funcs)))
A:dask.diagnostics.profile_visualize.n_funcs->len(unique_funcs)
A:dask.diagnostics.profile_visualize.keys->list(palette_lookup.keys())
A:dask.diagnostics.profile_visualize.colors->cycle(palette_lookup[high])
A:dask.diagnostics.profile_visualize.color_lookup->dict(zip(unique_funcs, colors))
A:dask.diagnostics.profile_visualize.p->bokeh.plotting.figure(y_range=[0, 10], x_range=[0, 10], **defaults)
A:dask.diagnostics.profile_visualize.defaults->dict(title='Profile Results', tools='hover,save,reset,resize,wheel_zoom,xpan', plot_width=800, plot_height=300)
A:dask.diagnostics.profile_visualize.(keys, tasks, starts, ends, ids)->zip(*results)
A:dask.diagnostics.profile_visualize.id_group->groupby(itemgetter(4), results)
A:dask.diagnostics.profile_visualize.timings->dict(((k, [i.end_time - i.start_time for i in v]) for (k, v) in id_group.items()))
A:dask.diagnostics.profile_visualize.id_lk->dict(((t[0], n) for (n, t) in enumerate(sorted(timings.items(), key=itemgetter(1), reverse=True))))
A:dask.diagnostics.profile_visualize.left->min(starts)
A:dask.diagnostics.profile_visualize.right->max(ends)
A:dask.diagnostics.profile_visualize.data['color']->get_colors(palette, funcs)
A:dask.diagnostics.profile_visualize.source->bokeh.plotting.ColumnDataSource(data=data)
A:dask.diagnostics.profile_visualize.hover->bokeh.plotting.figure(y_range=[0, 10], x_range=[0, 10], **defaults).select(HoverTool)
A:dask.diagnostics.profile_visualize.(t, mem, cpu)->zip(*results)
A:dask.diagnostics.profile_visualize.tics->list(sorted(unique(starts + ends)))
A:dask.diagnostics.profile_visualize.groups->groupby(lambda d: pprint_task(d[1], dsk, label_size), results)
A:dask.diagnostics.profile_visualize.cnts->dict.fromkeys(tics, 0)
A:dask.diagnostics.profile_visualize.p.yaxis.axis_label->'Cache Size ({0})'.format(metric_name)
dask.diagnostics.profile_visualize.get_colors(palette,funcs)
dask.diagnostics.profile_visualize.plot_cache(results,dsk,start_time,metric_name,palette='YlGnBu',label_size=60,**kwargs)
dask.diagnostics.profile_visualize.plot_resources(results,palette='YlGnBu',**kwargs)
dask.diagnostics.profile_visualize.plot_tasks(results,dsk,palette='YlGnBu',label_size=60,**kwargs)
dask.diagnostics.profile_visualize.pprint_task(task,keys,label_size=60)
dask.diagnostics.profile_visualize.unquote(expr)
dask.diagnostics.profile_visualize.visualize(profilers,file_path=None,show=True,save=True,**kwargs)
dask.diagnostics.visualize(profilers,file_path=None,show=True,save=True,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/diagnostics/profile.py----------------------------------------
A:dask.diagnostics.profile.TaskData->namedtuple('TaskData', ('key', 'task', 'start_time', 'end_time', 'worker_id'))
A:dask.diagnostics.profile.start->default_timer()
A:dask.diagnostics.profile.end->default_timer()
A:dask.diagnostics.profile.results->dict(((k, v) for (k, v) in self._results.items() if len(v) == 5))
A:dask.diagnostics.profile.ResourceData->namedtuple('ResourceData', ('time', 'mem', 'cpu'))
A:dask.diagnostics.profile.self._tracker->_Tracker(dt)
A:dask.diagnostics.profile.self.parent->psutil.Process(current_process().pid)
A:dask.diagnostics.profile.(self.parent_conn, self.child_conn)->Pipe()
A:dask.diagnostics.profile.pid->current_process()
A:dask.diagnostics.profile.ps->self._update_pids(pid)
A:dask.diagnostics.profile.msg->self.child_conn.recv()
A:dask.diagnostics.profile.tic->default_timer()
A:dask.diagnostics.profile.cpu->sum((p.cpu_percent() for p in ps))
A:dask.diagnostics.profile.CacheData->namedtuple('CacheData', ('key', 'task', 'metric', 'cache_time', 'free_time'))
A:dask.diagnostics.profile.self._start_time->default_timer()
A:dask.diagnostics.profile.t->default_timer()
A:dask.diagnostics.profile.(metric, start)->self._cache.pop(k)
dask.diagnostics.CacheProfiler(self,metric=None,metric_name=None)
dask.diagnostics.CacheProfiler.__enter__(self)
dask.diagnostics.CacheProfiler._finish(self,dsk,state,failed)
dask.diagnostics.CacheProfiler._plot(self,**kwargs)
dask.diagnostics.CacheProfiler._posttask(self,key,value,dsk,state,id)
dask.diagnostics.CacheProfiler._start(self,dsk)
dask.diagnostics.CacheProfiler.clear(self)
dask.diagnostics.CacheProfiler.visualize(self,**kwargs)
dask.diagnostics.Profiler(self)
dask.diagnostics.Profiler.__enter__(self)
dask.diagnostics.Profiler._finish(self,dsk,state,failed)
dask.diagnostics.Profiler._plot(self,**kwargs)
dask.diagnostics.Profiler._posttask(self,key,value,dsk,state,id)
dask.diagnostics.Profiler._pretask(self,key,dsk,state)
dask.diagnostics.Profiler._start(self,dsk)
dask.diagnostics.Profiler.clear(self)
dask.diagnostics.Profiler.visualize(self,**kwargs)
dask.diagnostics.ResourceProfiler(self,dt=1)
dask.diagnostics.ResourceProfiler.__enter__(self)
dask.diagnostics.ResourceProfiler.__exit__(self,*args)
dask.diagnostics.ResourceProfiler._finish(self,dsk,state,failed)
dask.diagnostics.ResourceProfiler._plot(self,**kwargs)
dask.diagnostics.ResourceProfiler._start(self,dsk)
dask.diagnostics.ResourceProfiler._start_collect(self)
dask.diagnostics.ResourceProfiler._stop_collect(self)
dask.diagnostics.ResourceProfiler.clear(self)
dask.diagnostics.ResourceProfiler.close(self)
dask.diagnostics.ResourceProfiler.visualize(self,**kwargs)
dask.diagnostics.profile.CacheProfiler(self,metric=None,metric_name=None)
dask.diagnostics.profile.CacheProfiler.__enter__(self)
dask.diagnostics.profile.CacheProfiler.__init__(self,metric=None,metric_name=None)
dask.diagnostics.profile.CacheProfiler._finish(self,dsk,state,failed)
dask.diagnostics.profile.CacheProfiler._plot(self,**kwargs)
dask.diagnostics.profile.CacheProfiler._posttask(self,key,value,dsk,state,id)
dask.diagnostics.profile.CacheProfiler._start(self,dsk)
dask.diagnostics.profile.CacheProfiler.clear(self)
dask.diagnostics.profile.CacheProfiler.visualize(self,**kwargs)
dask.diagnostics.profile.Profiler(self)
dask.diagnostics.profile.Profiler.__enter__(self)
dask.diagnostics.profile.Profiler.__init__(self)
dask.diagnostics.profile.Profiler._finish(self,dsk,state,failed)
dask.diagnostics.profile.Profiler._plot(self,**kwargs)
dask.diagnostics.profile.Profiler._posttask(self,key,value,dsk,state,id)
dask.diagnostics.profile.Profiler._pretask(self,key,dsk,state)
dask.diagnostics.profile.Profiler._start(self,dsk)
dask.diagnostics.profile.Profiler.clear(self)
dask.diagnostics.profile.Profiler.visualize(self,**kwargs)
dask.diagnostics.profile.ResourceProfiler(self,dt=1)
dask.diagnostics.profile.ResourceProfiler.__enter__(self)
dask.diagnostics.profile.ResourceProfiler.__exit__(self,*args)
dask.diagnostics.profile.ResourceProfiler.__init__(self,dt=1)
dask.diagnostics.profile.ResourceProfiler._finish(self,dsk,state,failed)
dask.diagnostics.profile.ResourceProfiler._plot(self,**kwargs)
dask.diagnostics.profile.ResourceProfiler._start(self,dsk)
dask.diagnostics.profile.ResourceProfiler._start_collect(self)
dask.diagnostics.profile.ResourceProfiler._stop_collect(self)
dask.diagnostics.profile.ResourceProfiler.clear(self)
dask.diagnostics.profile.ResourceProfiler.close(self)
dask.diagnostics.profile.ResourceProfiler.visualize(self,**kwargs)
dask.diagnostics.profile._Tracker(self,dt=1)
dask.diagnostics.profile._Tracker.__init__(self,dt=1)
dask.diagnostics.profile._Tracker._update_pids(self,pid)
dask.diagnostics.profile._Tracker.run(self)
dask.diagnostics.profile._Tracker.shutdown(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/diagnostics/progress.py----------------------------------------
A:dask.diagnostics.progress.(m, s)->divmod(t, 60)
A:dask.diagnostics.progress.(h, m)->divmod(m, 60)
A:dask.diagnostics.progress.self._start_time->default_timer()
A:dask.diagnostics.progress.self._timer->threading.Thread(target=self._timer_func)
A:dask.diagnostics.progress.ndone->len(s['finished'])
A:dask.diagnostics.progress.percent->int(100 * frac)
A:dask.diagnostics.progress.elapsed->format_time(elapsed)
A:dask.diagnostics.progress.msg->'\r[{0:<{1}}] | {2}% Completed | {3}'.format(bar, self._width, percent, elapsed)
dask.diagnostics.ProgressBar(self,minimum=0,width=40,dt=0.1)
dask.diagnostics.ProgressBar._draw_bar(self,frac,elapsed)
dask.diagnostics.ProgressBar._finish(self,dsk,state,errored)
dask.diagnostics.ProgressBar._pretask(self,key,dsk,state)
dask.diagnostics.ProgressBar._start(self,dsk)
dask.diagnostics.ProgressBar._timer_func(self)
dask.diagnostics.ProgressBar._update_bar(self,elapsed)
dask.diagnostics.progress.ProgressBar(self,minimum=0,width=40,dt=0.1)
dask.diagnostics.progress.ProgressBar.__init__(self,minimum=0,width=40,dt=0.1)
dask.diagnostics.progress.ProgressBar._draw_bar(self,frac,elapsed)
dask.diagnostics.progress.ProgressBar._finish(self,dsk,state,errored)
dask.diagnostics.progress.ProgressBar._pretask(self,key,dsk,state)
dask.diagnostics.progress.ProgressBar._start(self,dsk)
dask.diagnostics.progress.ProgressBar._timer_func(self)
dask.diagnostics.progress.ProgressBar._update_bar(self,elapsed)
dask.diagnostics.progress.format_time(t)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/diagnostics/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/diagnostics/tests/test_profiler.py----------------------------------------
A:dask.diagnostics.tests.test_profiler.prof->Profiler()
A:dask.diagnostics.tests.test_profiler.out->get(dsk2, 'c')
A:dask.diagnostics.tests.test_profiler.prof_data->sorted(prof.results, key=lambda d: d.key)
A:dask.diagnostics.tests.test_profiler.n->len(prof.results)
A:dask.diagnostics.tests.test_profiler.m->len(prof.results)
A:dask.diagnostics.tests.test_profiler.keys->set(['a', 'b', 'c', 'd', 'e'])
A:dask.diagnostics.tests.test_profiler.p->visualize([prof, rprof], label_size=50, title='Not the default', show=False, save=False)
A:dask.diagnostics.tests.test_profiler.funcs->list(range(5))
A:dask.diagnostics.tests.test_profiler.cmap->get_colors('BrBG', funcs)
A:dask.diagnostics.tests.test_profiler.lk->dict(zip([0, 1], BrBG3))
dask.diagnostics.tests.test_profiler.test_cache_profiler()
dask.diagnostics.tests.test_profiler.test_cache_profiler_plot()
dask.diagnostics.tests.test_profiler.test_get_colors()
dask.diagnostics.tests.test_profiler.test_plot_multiple()
dask.diagnostics.tests.test_profiler.test_pprint_task()
dask.diagnostics.tests.test_profiler.test_profiler()
dask.diagnostics.tests.test_profiler.test_profiler_plot()
dask.diagnostics.tests.test_profiler.test_profiler_works_under_error()
dask.diagnostics.tests.test_profiler.test_resource_profiler()
dask.diagnostics.tests.test_profiler.test_resource_profiler_multiple_gets()
dask.diagnostics.tests.test_profiler.test_resource_profiler_plot()
dask.diagnostics.tests.test_profiler.test_saves_file()
dask.diagnostics.tests.test_profiler.test_two_gets()
dask.diagnostics.tests.test_profiler.test_unquote()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/diagnostics/tests/test_progress.py----------------------------------------
A:dask.diagnostics.tests.test_progress.(out, err)->capsys.readouterr()
A:dask.diagnostics.tests.test_progress.out->get(dsk, 'e')
A:dask.diagnostics.tests.test_progress.p->ProgressBar()
A:dask.diagnostics.tests.test_progress.cachey->pytest.importorskip('cachey')
A:dask.diagnostics.tests.test_progress.c->pytest.importorskip('cachey').Cache(10000)
A:dask.diagnostics.tests.test_progress.cc->Cache(c)
dask.diagnostics.tests.test_progress.check_bar_completed(capsys,width=40)
dask.diagnostics.tests.test_progress.test_clean_exit()
dask.diagnostics.tests.test_progress.test_format_time()
dask.diagnostics.tests.test_progress.test_minimum_time(capsys)
dask.diagnostics.tests.test_progress.test_no_tasks(capsys)
dask.diagnostics.tests.test_progress.test_progressbar(capsys)
dask.diagnostics.tests.test_progress.test_register(capsys)
dask.diagnostics.tests.test_progress.test_store_time()
dask.diagnostics.tests.test_progress.test_with_alias(capsys)
dask.diagnostics.tests.test_progress.test_with_cache(capsys)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/bag/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/bag/text.py----------------------------------------
A:dask.bag.text.delayed->delayed(pure=True)
A:dask.bag.text.blocks->list(concat(blocks))
A:dask.bag.text.compression->infer_compression(path)
A:dask.bag.text.files->open_text_files(path, encoding=encoding, errors=errors, compression=compression, **kwargs)
A:dask.bag.text.(_, blocks)->read_bytes(path, delimiter=linedelimiter.encode(), blocksize=blocksize, sample=False, compression=compression, **kwargs)
A:dask.bag.text.text->block.decode(encoding, errors)
A:dask.bag.text.lines->io.StringIO(text)
dask.bag.read_text(path,blocksize=None,compression='infer',encoding=system_encoding,errors='strict',linedelimiter=os.linesep,collection=True,**kwargs)
dask.bag.text.decode(block,encoding,errors)
dask.bag.text.read_text(path,blocksize=None,compression='infer',encoding=system_encoding,errors='strict',linedelimiter=os.linesep,collection=True,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/bag/core.py----------------------------------------
A:dask.bag.core.dependencies->dict(((k, get_dependencies(dsk, k)) for k in dsk))
A:dask.bag.core.dependents->reverse_dict(dependencies)
A:dask.bag.core.(dsk2, dependencies)->cull(dsk, keys)
A:dask.bag.core.(dsk3, dependencies)->fuse(dsk2, keys, dependencies)
A:dask.bag.core.dsk4->dict((((name, i), (collect, grouper, i, p, barrier_token)) for i in range(npartitions)))
A:dask.bag.core.dsk5->lazify(dsk4)
A:dask.bag.core.compression->infer_compression(path)
A:dask.bag.core.dsk->merge(b2.dask, start, end, *groups + splits + joins)
A:dask.bag.core.result->Bag(merge(b.dask, dsk), name, b.npartitions)
A:dask.bag.core.results->list(results)
A:dask.bag.core._optimize->staticmethod(optimize)
A:dask.bag.core._default_get->staticmethod(mpget)
A:dask.bag.core._finalize->staticmethod(finalize)
A:dask.bag.core.name->'groupby-part-{0}-{1}'.format(funcname(grouper), token)
A:dask.bag.core.self.str->StringAccessor(self)
A:dask.bag.core.func->getattr(str, key)
A:dask.bag.core.(kw_dsk, kw_pairs)->unpack_kwargs(kwargs)
A:dask.bag.core.key->partial(apply, key)
A:dask.bag.core.initial->quote(initial)
A:dask.bag.core.token->tokenize(b, grouper, npartitions, blocksize)
A:dask.bag.core.dsk2->dict((((name, i), (partition, grouper, (b.name, i), npartitions, p, blocksize)) for i in range(b.npartitions)))
A:dask.bag.core.k->int(math.ceil(n ** (1 / stages)))
A:dask.bag.core.dsk[b]->merge(b2.dask, start, end, *groups + splits + joins).pop((b, 0))
A:dask.bag.core.(totals, counts)->list(zip(*x))
A:dask.bag.core.(squares, totals, counts)->list(zip(*x))
A:dask.bag.core.b->Bag(merge(self.dask, dsk), name, 1)
A:dask.bag.core.get->context._globals.get('get')
A:dask.bag.core.columns->list(range(len(head)))
A:dask.bag.core.DataFrame->partial(pd.DataFrame, columns=columns)
A:dask.bag.core.binop_name->funcname(binop)
A:dask.bag.core.res->list(accumulate(binop, seq, initial=initial))
A:dask.bag.core.d->dict((((name, i), part) for (i, part) in enumerate(parts)))
A:dask.bag.core.d2->defaultdict(list)
A:dask.bag.core.dirname->os.path.dirname(filename)
A:dask.bag.core.f->open(filename, mode='wb', compression=compression)
A:dask.bag.core.data->iter(data)
A:dask.bag.core.firstline->next(data)
A:dask.bag.core.seq->list(map(list, seq))
A:dask.bag.core.partition_size->int(len(seq) / 100)
A:dask.bag.core.parts->list(partition_all(partition_size, seq))
A:dask.bag.core.x->Castra(x.path, readonly=True)
A:dask.bag.core.df->castra.load_partition(part, columns)
A:dask.bag.core.items->list(items)
A:dask.bag.core.counter->itertools.count(0)
A:dask.bag.core.out->defaultdict(int)
A:dask.bag.core.ijs->list(enumerate(take(npartitions, range(0, n, size))))
A:dask.bag.core.bags_dsk->merge(*(bag.dask for bag in bags))
A:dask.bag.core.stages->int(math.ceil(math.log(n) / math.log(max_branch)))
A:dask.bag.core.sinputs->set(inputs)
A:dask.bag.core.b2->Bag(merge(self.dask, dsk), name, 1).map(lambda x: (hash(grouper(x)), x))
A:dask.bag.core.start->dict(((('shuffle-join-' + token, 0, inp), (b2.name, i) if i < b.npartitions else []) for (i, inp) in enumerate(inputs)))
A:dask.bag.core.group->dict(((('shuffle-group-' + token, stage, inp), (groupby, (make_group, k, stage - 1), ('shuffle-join-' + token, stage - 1, inp))) for inp in inputs))
A:dask.bag.core.split->dict(((('shuffle-split-' + token, stage, i, inp), (dict.get, ('shuffle-group-' + token, stage, inp), i, {})) for i in range(k) for inp in inputs))
A:dask.bag.core.join->dict(((('shuffle-join-' + token, stage, inp), (list, (toolz.concat, [('shuffle-split-' + token, stage, inp[stage - 1], insert(inp, stage - 1, j)) for j in range(k)]))) for inp in inputs))
A:dask.bag.core.end->dict(((('shuffle-' + token, i), (list, (dict.items, (groupby, grouper, (pluck, 1, j))))) for (i, j) in enumerate(join)))
dask.bag.Bag(self,dsk,name,npartitions)
dask.bag.Bag.__iter__(self)
dask.bag.Bag.__str__(self)
dask.bag.Bag._args(self)
dask.bag.Bag._keys(self)
dask.bag.Bag.accumulate(self,binop,initial=no_default)
dask.bag.Bag.all(self,split_every=None)
dask.bag.Bag.any(self,split_every=None)
dask.bag.Bag.concat(self)
dask.bag.Bag.count(self,split_every=None)
dask.bag.Bag.distinct(self)
dask.bag.Bag.filter(self,predicate)
dask.bag.Bag.fold(self,binop,combine=None,initial=no_default,split_every=None)
dask.bag.Bag.foldby(self,key,binop,initial=no_default,combine=None,combine_initial=no_default)
dask.bag.Bag.frequencies(self,split_every=None)
dask.bag.Bag.groupby(self,grouper,method=None,npartitions=None,blocksize=2**20,max_branch=None)
dask.bag.Bag.join(self,other,on_self,on_other=None)
dask.bag.Bag.map(self,func,**kwargs)
dask.bag.Bag.map_partitions(self,func,**kwargs)
dask.bag.Bag.max(self,split_every=None)
dask.bag.Bag.mean(self)
dask.bag.Bag.min(self,split_every=None)
dask.bag.Bag.pluck(self,key,default=no_default)
dask.bag.Bag.product(self,other)
dask.bag.Bag.reduction(self,perpartition,aggregate,split_every=None,out_type=Item,name=None)
dask.bag.Bag.remove(self,predicate)
dask.bag.Bag.repartition(self,npartitions)
dask.bag.Bag.std(self,ddof=0)
dask.bag.Bag.sum(self,split_every=None)
dask.bag.Bag.take(self,k,compute=True)
dask.bag.Bag.to_dataframe(self,columns=None)
dask.bag.Bag.to_delayed(self)
dask.bag.Bag.to_imperative(self)
dask.bag.Bag.to_textfiles(self,path,name_function=str,compression='infer',encoding=system_encoding,compute=True)
dask.bag.Bag.topk(self,k,key=None,split_every=None)
dask.bag.Bag.unzip(self,n)
dask.bag.Bag.var(self,ddof=0)
dask.bag.Item(self,dsk,key)
dask.bag.Item._keys(self)
dask.bag.Item.apply(self,func)
dask.bag.Item.from_delayed(value)
dask.bag.Item.from_imperative(value)
dask.bag.Item.to_delayed(self)
dask.bag.Item.to_imperative(self)
dask.bag.concat(bags)
dask.bag.core.Bag(self,dsk,name,npartitions)
dask.bag.core.Bag.__init__(self,dsk,name,npartitions)
dask.bag.core.Bag.__iter__(self)
dask.bag.core.Bag.__str__(self)
dask.bag.core.Bag._args(self)
dask.bag.core.Bag._keys(self)
dask.bag.core.Bag.accumulate(self,binop,initial=no_default)
dask.bag.core.Bag.all(self,split_every=None)
dask.bag.core.Bag.any(self,split_every=None)
dask.bag.core.Bag.concat(self)
dask.bag.core.Bag.count(self,split_every=None)
dask.bag.core.Bag.distinct(self)
dask.bag.core.Bag.filter(self,predicate)
dask.bag.core.Bag.fold(self,binop,combine=None,initial=no_default,split_every=None)
dask.bag.core.Bag.foldby(self,key,binop,initial=no_default,combine=None,combine_initial=no_default)
dask.bag.core.Bag.frequencies(self,split_every=None)
dask.bag.core.Bag.groupby(self,grouper,method=None,npartitions=None,blocksize=2**20,max_branch=None)
dask.bag.core.Bag.join(self,other,on_self,on_other=None)
dask.bag.core.Bag.map(self,func,**kwargs)
dask.bag.core.Bag.map_partitions(self,func,**kwargs)
dask.bag.core.Bag.max(self,split_every=None)
dask.bag.core.Bag.mean(self)
dask.bag.core.Bag.min(self,split_every=None)
dask.bag.core.Bag.pluck(self,key,default=no_default)
dask.bag.core.Bag.product(self,other)
dask.bag.core.Bag.reduction(self,perpartition,aggregate,split_every=None,out_type=Item,name=None)
dask.bag.core.Bag.remove(self,predicate)
dask.bag.core.Bag.repartition(self,npartitions)
dask.bag.core.Bag.std(self,ddof=0)
dask.bag.core.Bag.sum(self,split_every=None)
dask.bag.core.Bag.take(self,k,compute=True)
dask.bag.core.Bag.to_dataframe(self,columns=None)
dask.bag.core.Bag.to_delayed(self)
dask.bag.core.Bag.to_imperative(self)
dask.bag.core.Bag.to_textfiles(self,path,name_function=str,compression='infer',encoding=system_encoding,compute=True)
dask.bag.core.Bag.topk(self,k,key=None,split_every=None)
dask.bag.core.Bag.unzip(self,n)
dask.bag.core.Bag.var(self,ddof=0)
dask.bag.core.Item(self,dsk,key)
dask.bag.core.Item.__init__(self,dsk,key)
dask.bag.core.Item._keys(self)
dask.bag.core.Item.apply(self,func)
dask.bag.core.Item.from_delayed(value)
dask.bag.core.Item.from_imperative(value)
dask.bag.core.Item.to_delayed(self)
dask.bag.core.Item.to_imperative(self)
dask.bag.core.StringAccessor(self,bag)
dask.bag.core.StringAccessor.__dir__(self)
dask.bag.core.StringAccessor.__getattr__(self,key)
dask.bag.core.StringAccessor.__init__(self,bag)
dask.bag.core.StringAccessor._strmap(self,key,*args,**kwargs)
dask.bag.core.StringAccessor.match(self,pattern)
dask.bag.core._reduce(binop,sequence,initial=no_default)
dask.bag.core.accumulate_part(binop,seq,initial,is_first=False)
dask.bag.core.bag_range(n,npartitions)
dask.bag.core.bag_zip(*bags)
dask.bag.core.collect(grouper,group,p,barrier_token)
dask.bag.core.concat(bags)
dask.bag.core.dictitems(d)
dask.bag.core.finalize(results)
dask.bag.core.finalize_item(results)
dask.bag.core.from_castra(x,columns=None,index=False)
dask.bag.core.from_delayed(values)
dask.bag.core.from_filenames(filenames,chunkbytes=None,compression='infer',encoding=system_encoding,linesep=os.linesep)
dask.bag.core.from_imperative(values)
dask.bag.core.from_sequence(seq,partition_size=None,npartitions=None)
dask.bag.core.from_url(urls)
dask.bag.core.groupby_disk(b,grouper,npartitions=None,blocksize=2**20)
dask.bag.core.groupby_tasks(b,grouper,hash=hash,max_branch=32)
dask.bag.core.inline_singleton_lists(dsk,dependencies=None)
dask.bag.core.lazify(dsk)
dask.bag.core.lazify_task(task,start=True)
dask.bag.core.load_castra_partition(castra,part,columns,index)
dask.bag.core.make_group(k,stage)
dask.bag.core.merge_frequencies(seqs)
dask.bag.core.optimize(dsk,keys,**kwargs)
dask.bag.core.partition(grouper,sequence,npartitions,p,nelements=2**20)
dask.bag.core.reify(seq)
dask.bag.core.robust_wraps(wrapper)
dask.bag.core.to_textfiles(b,path,name_function=str,compression='infer',encoding=system_encoding,compute=True)
dask.bag.core.unpack_kwargs(kwargs)
dask.bag.core.write(data,filename,compression,encoding)
dask.bag.from_castra(x,columns=None,index=False)
dask.bag.from_delayed(values)
dask.bag.from_filenames(filenames,chunkbytes=None,compression='infer',encoding=system_encoding,linesep=os.linesep)
dask.bag.from_imperative(values)
dask.bag.from_sequence(seq,partition_size=None,npartitions=None)
dask.bag.from_url(urls)
dask.bag.range(n,npartitions)
dask.bag.to_textfiles(b,path,name_function=str,compression='infer',encoding=system_encoding,compute=True)
dask.bag.zip(*bags)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/bag/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/bag/tests/test_text.py----------------------------------------
A:dask.bag.tests.test_text.compute->partial(compute, get=get)
A:dask.bag.tests.test_text.expected->''.join([files[v] for v in sorted(files)])
A:dask.bag.tests.test_text.files2->dict(((k, compress(v.encode(encoding))) for (k, v) in files.items()))
A:dask.bag.tests.test_text.b->read_text('.test.accounts.*.json', compression=fmt, blocksize=bs, encoding=encoding)
A:dask.bag.tests.test_text.(L,)->compute(b)
A:dask.bag.tests.test_text.blocks->read_text('.test.accounts.*.json', compression=fmt, blocksize=bs, encoding=encoding, collection=False)
A:dask.bag.tests.test_text.L->compute(*blocks)
dask.bag.tests.test_text.test_read_text(fmt,bs,encoding)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/bytes/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/bytes/local.py----------------------------------------
A:dask.bytes.local.logger->logging.getLogger(__name__)
A:dask.bytes.local.filenames->sorted(glob(path))
A:dask.bytes.local.(sample, first)->read_bytes(filenames[0], delimiter, not_zero, blocksize, sample=True, compression=compression)
A:dask.bytes.local.size->getsize(fn, compression)
A:dask.bytes.local.offsets->list(range(0, size, blocksize))
A:dask.bytes.local.token->tokenize(fn, delimiter, blocksize, not_zero, compression, os.path.getmtime(fn))
A:dask.bytes.local.f->SeekableFile(f)
A:dask.bytes.local.sample->read_block_from_file(fn, 0, nbytes, None, compression)
A:dask.bytes.local.result->seekable_files[compression](f).tell()
A:dask.bytes.local.myopen->delayed(open)
A:dask.bytes.local.g->seekable_files[compression](f)
dask.bytes.local.getsize(fn,compression=None)
dask.bytes.local.open_files(path)
dask.bytes.local.read_block_from_file(fn,offset,length,delimiter,compression)
dask.bytes.local.read_bytes(fn,delimiter=None,not_zero=False,blocksize=2**27,sample=True,compression=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/bytes/core.py----------------------------------------
A:dask.bytes.core.delayed->delayed(pure=True)
A:dask.bytes.core._read_bytes->dict()
A:dask.bytes.core._open_files->dict()
A:dask.bytes.core._open_text_files->dict()
A:dask.bytes.core.(protocol, path)->path.split('://', 1)
A:dask.bytes.core.files->open_files(original_path, compression=compression, **kwargs)
dask.bytes.core.open_files(path,compression=None,**kwargs)
dask.bytes.core.open_text_files(path,encoding=system_encoding,errors='strict',compression=None,**kwargs)
dask.bytes.core.read_bytes(path,delimiter=None,not_zero=False,blocksize=2**27,sample=True,compression=None,**kwargs)
dask.bytes.open_files(path,compression=None,**kwargs)
dask.bytes.open_text_files(path,encoding=system_encoding,errors='strict',compression=None,**kwargs)
dask.bytes.read_bytes(path,delimiter=None,not_zero=False,blocksize=2**27,sample=True,compression=None,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/bytes/utils.py----------------------------------------
A:dask.bytes.utils.current->file.read(blocksize)
A:dask.bytes.utils.i->full.index(delimiter)
A:dask.bytes.utils.start->f.tell()
A:dask.bytes.utils.end->f.tell()
dask.bytes.utils.read_block(f,offset,length,delimiter=None)
dask.bytes.utils.seek_delimiter(file,delimiter,blocksize)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/bytes/s3.py----------------------------------------
A:dask.bytes.s3.logger->logging.getLogger(__name__)
A:dask.bytes.s3.s3->S3FileSystem(**s3_params)
A:dask.bytes.s3.filenames->sorted(s3.glob(path))
A:dask.bytes.s3.(sample, first)->read_bytes(filenames[0], s3, delimiter, not_zero, blocksize, sample=True, compression=compression, **s3_params)
A:dask.bytes.s3.size->getsize(path, compression, s3)
A:dask.bytes.s3.offsets->list(range(0, size, blocksize))
A:dask.bytes.s3.token->tokenize(info['ETag'], delimiter, blocksize, not_zero, compression)
A:dask.bytes.s3.s3safe_pars->s3_params.copy().copy()
A:dask.bytes.s3.sample->read_block_from_s3(path, 0, nbytes, s3safe_pars, delimiter, compression)
A:dask.bytes.s3.f->compress_files[compression](f)
A:dask.bytes.s3.result->seekable_files[compression](f).tell()
A:dask.bytes.s3.myopen->delayed(s3_open_file)
A:dask.bytes.s3.s3_params->s3_params.copy().copy()
A:dask.bytes.s3.g->seekable_files[compression](f)
dask.bytes.s3.getsize(path,compression,s3)
dask.bytes.s3.open_files(path,s3=None,**s3_params)
dask.bytes.s3.read_block_from_s3(filename,offset,length,s3_params=None,delimiter=None,compression=None)
dask.bytes.s3.read_bytes(path,s3=None,delimiter=None,not_zero=False,blocksize=2**27,sample=True,compression=None,**s3_params)
dask.bytes.s3.s3_open_file(path,s3_params)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/bytes/compression.py----------------------------------------
dask.bytes.compression.noop_file(file,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/bytes/tests/test_bytes_utils.py----------------------------------------
A:dask.bytes.tests.test_bytes_utils.data->delimiter.join([b'123', b'456', b'789'])
A:dask.bytes.tests.test_bytes_utils.f->io.BytesIO(b'123\n456')
dask.bytes.tests.test_bytes_utils.test_read_block()
dask.bytes.tests.test_bytes_utils.test_seek_delimiter_endline()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/bytes/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/bytes/tests/test_s3.py----------------------------------------
A:dask.bytes.tests.test_s3.compute->partial(compute, get=get)
A:dask.bytes.tests.test_s3.client->boto3.client('s3')
A:dask.bytes.tests.test_s3.m->moto.mock_s3()
A:dask.bytes.tests.test_s3.(sample, values)->read_bytes('compress/test/accounts.*', s3=s3, compression=fmt, blocksize=blocksize)
A:dask.bytes.tests.test_s3.results->compute(*concat(values))
A:dask.bytes.tests.test_s3.(_, values)->read_bytes(test_bucket_name + '/test/accounts*', blocksize=blocksize, delimiter=d, s3=s3)
A:dask.bytes.tests.test_s3.(_, L)->read_bytes('dask-data/nyc-taxi/2014/*.csv', blocksize=None)
A:dask.bytes.tests.test_s3.(_, vals)->read_bytes(test_bucket_name + '/test/account*', blocksize=blocksize, s3=s3)
A:dask.bytes.tests.test_s3.ourlines->b''.join(res).split(b'\n')
A:dask.bytes.tests.test_s3.testlines->b''.join((files[k] for k in sorted(files))).split(b'\n')
A:dask.bytes.tests.test_s3.(_, values2)->read_bytes(test_bucket_name + '/test/accounts*', blocksize=blocksize, delimiter=b'foo', s3=s3)
A:dask.bytes.tests.test_s3.ours->b''.join(res)
A:dask.bytes.tests.test_s3.test->b''.join((files[v] for v in sorted(files)))
A:dask.bytes.tests.test_s3.myfiles->open_files(test_bucket_name + '/test/accounts.*', s3=s3)
A:dask.bytes.tests.test_s3.data->compute(*[file.read() for file in myfiles])
A:dask.bytes.tests.test_s3.(_, a)->read_bytes('compress/test/accounts.*', s3=s3)
A:dask.bytes.tests.test_s3.(_, b)->read_bytes('compress/test/accounts.*', s3=s3)
A:dask.bytes.tests.test_s3.(_, c)->read_bytes('compress/test/accounts.*', s3=s3)
A:dask.bytes.tests.test_s3.a->open_files('compress/test/accounts.*', s3=s3)
A:dask.bytes.tests.test_s3.b->open_files('compress/test/accounts.*', s3=s3)
A:dask.bytes.tests.test_s3.c->open_files('compress/test/accounts.*', s3=s3)
A:dask.bytes.tests.test_s3.dd->pytest.importorskip('dask.dataframe')
A:dask.bytes.tests.test_s3.df->pytest.importorskip('dask.dataframe').read_csv('s3://csv/*.csv', storage_options={'s3': s3})
dask.bytes.tests.test_s3.s3()
dask.bytes.tests.test_s3.s3_context(bucket,files)
dask.bytes.tests.test_s3.test_compression(s3,fmt,blocksize)
dask.bytes.tests.test_s3.test_files(s3)
dask.bytes.tests.test_s3.test_getsize(fmt)
dask.bytes.tests.test_s3.test_modification_time_open_files()
dask.bytes.tests.test_s3.test_modification_time_read_bytes()
dask.bytes.tests.test_s3.test_read_bytes(s3)
dask.bytes.tests.test_s3.test_read_bytes_block(s3,blocksize)
dask.bytes.tests.test_s3.test_read_bytes_blocksize_none(s3)
dask.bytes.tests.test_s3.test_read_bytes_blocksize_on_large_data()
dask.bytes.tests.test_s3.test_read_bytes_delimited(s3,blocksize)
dask.bytes.tests.test_s3.test_read_csv_passes_through_options()
dask.bytes.tests.test_s3.test_registered(s3)
dask.bytes.tests.test_s3.test_registered_open_files(s3)
dask.bytes.tests.test_s3.test_registered_open_text_files(s3)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/bytes/tests/test_local.py----------------------------------------
A:dask.bytes.tests.test_local.compute->partial(compute, get=get)
A:dask.bytes.tests.test_local.(sample, values)->func('.test.accounts.*', compression='not-found')
A:dask.bytes.tests.test_local.results->compute(*concat(values))
A:dask.bytes.tests.test_local.(sample, vals)->read_bytes('.test.account*', blocksize=bs)
A:dask.bytes.tests.test_local.ourlines->b''.join(res).split(b'\n')
A:dask.bytes.tests.test_local.testlines->b''.join((files[k] for k in sorted(files))).split(b'\n')
A:dask.bytes.tests.test_local.(_, values)->read_bytes('.test.accounts*', blocksize=bs, delimiter=d)
A:dask.bytes.tests.test_local.(_, values2)->read_bytes('.test.accounts*', blocksize=bs, delimiter=b'foo')
A:dask.bytes.tests.test_local.ours->b''.join(res)
A:dask.bytes.tests.test_local.test->b''.join((files[v] for v in sorted(files)))
A:dask.bytes.tests.test_local.files2->valmap(compression.compress[fmt], files)
A:dask.bytes.tests.test_local.myfiles->open_text_files('.test.accounts.*', compression=fmt)
A:dask.bytes.tests.test_local.data->compute(*[file.read() for file in myfiles])
A:dask.bytes.tests.test_local.(_, a)->read_bytes('.test.accounts.*')
A:dask.bytes.tests.test_local.(_, b)->read_bytes('.test.accounts.*')
A:dask.bytes.tests.test_local.a->open_files('.test.accounts.*')
A:dask.bytes.tests.test_local.b->open_files('.test.accounts.*')
A:dask.bytes.tests.test_local.(_, c)->read_bytes('.test.accounts.*')
A:dask.bytes.tests.test_local.c->open_files('.test.accounts.*')
dask.bytes.tests.test_local.test_bad_compression()
dask.bytes.tests.test_local.test_compression(fmt,blocksize)
dask.bytes.tests.test_local.test_compression_binary(fmt)
dask.bytes.tests.test_local.test_compression_text(fmt)
dask.bytes.tests.test_local.test_getsize(fmt)
dask.bytes.tests.test_local.test_modification_time_open_files(open_files)
dask.bytes.tests.test_local.test_names()
dask.bytes.tests.test_local.test_not_found()
dask.bytes.tests.test_local.test_open_files()
dask.bytes.tests.test_local.test_read_bytes()
dask.bytes.tests.test_local.test_read_bytes_block()
dask.bytes.tests.test_local.test_read_bytes_blocksize_none()
dask.bytes.tests.test_local.test_read_bytes_delimited()
dask.bytes.tests.test_local.test_registered_open_files()
dask.bytes.tests.test_local.test_registered_open_text_files(encoding)
dask.bytes.tests.test_local.test_registered_read_bytes()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask0.10.0/lib/python3.6/site-packages/dask/bytes/tests/test_compression.py----------------------------------------
A:dask.bytes.tests.test_compression.b->BytesIO(compressed)
A:dask.bytes.tests.test_compression.c->decomp(b)
A:dask.bytes.tests.test_compression.out->BytesIO()
A:dask.bytes.tests.test_compression.f->File(out, mode='wb')
A:dask.bytes.tests.test_compression.compressed->BytesIO().read()
A:dask.bytes.tests.test_compression.g->File(b, mode='rb')
A:dask.bytes.tests.test_compression.data2->File(b, mode='rb').read()
dask.bytes.tests.test_compression.test_compression()
dask.bytes.tests.test_compression.test_files(fmt,File)

