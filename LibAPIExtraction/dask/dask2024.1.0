
----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/_compatibility.py----------------------------------------
A:dask._compatibility.PY_VERSION->parse_version('.'.join(map(str, sys.version_info[:3])))
dask._compatibility.entry_points(group=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/__main__.py----------------------------------------
dask.__main__.main()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/threaded.py----------------------------------------
A:dask.threaded.main_thread->current_thread()
A:dask.threaded.pools_lock->Lock()
A:dask.threaded.thread->current_thread()
A:dask.threaded.default_pool->ThreadPoolExecutor(CPU_COUNT)
A:dask.threaded.pool->MultiprocessingPoolExecutor(pool)
A:dask.threaded.results->get_async(pool.submit, pool._max_workers, dsk, keys, cache=cache, get_id=_thread_get_id, pack_exception=pack_exception, **kwargs)
A:dask.threaded.active_threads->set(threading.enumerate())
dask.threaded._thread_get_id()
dask.threaded.get(dsk:Mapping,keys:Sequence[Key]|Key,cache=None,num_workers=None,pool=None,**kwargs)
dask.threaded.pack_exception(e,dumps)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/system.py----------------------------------------
A:dask.system.quota->int(f.read())
A:dask.system.period->int(f.read())
A:dask.system.(quota, period)->_try_extract_cgroup_cpu_quota()
A:dask.system.count->min(count, cgroups_count)
A:dask.system.affinity_count->len(psutil.Process().cpu_affinity())
A:dask.system.cgroups_count->math.ceil(quota / period)
A:dask.system.CPU_COUNT->cpu_count()
dask.system._try_extract_cgroup_cpu_quota()
dask.system.cpu_count()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/multiprocessing.py----------------------------------------
A:dask.multiprocessing._dumps->partial(cloudpickle.dumps, protocol=pickle.HIGHEST_PROTOCOL)
A:dask.multiprocessing.typ->type(exc.__class__.__name__, (RemoteException, type(exc)), {'exception_type': type(exc)})
A:dask.multiprocessing.exc->remote_exception(exc, tb)
A:dask.multiprocessing.(exc_type, exc_value, exc_traceback)->sys.exc_info()
A:dask.multiprocessing.tb->_pack_traceback(exc_traceback)
A:dask.multiprocessing.result->get_async(pool.submit, pool._max_workers, dsk3, keys, get_id=_process_get_id, dumps=dumps, loads=loads, pack_exception=pack_exception, raise_exception=reraise, chunksize=chunksize, **kwargs)
A:dask.multiprocessing.context_name->dask.config.get('multiprocessing.context', 'spawn')
A:dask.multiprocessing.context->get_context()
A:dask.multiprocessing.initializer->partial(initialize_worker_process, user_initializer=initializer)
A:dask.multiprocessing.pool->MultiprocessingPoolExecutor(pool)
A:dask.multiprocessing.dsk->ensure_dict(dsk)
A:dask.multiprocessing.(dsk2, dependencies)->cull(dsk, keys)
A:dask.multiprocessing.(dsk3, dependencies)->fuse(dsk2, keys, dependencies)
A:dask.multiprocessing.np->sys.modules.get('numpy')
dask.multiprocessing.RemoteException(self,exception,traceback)
dask.multiprocessing.RemoteException.__dir__(self)
dask.multiprocessing.RemoteException.__getattr__(self,key)
dask.multiprocessing.RemoteException.__init__(self,exception,traceback)
dask.multiprocessing.RemoteException.__str__(self)
dask.multiprocessing._process_get_id()
dask.multiprocessing._reduce_method_descriptor(m)
dask.multiprocessing.default_initializer()
dask.multiprocessing.get(dsk:Mapping,keys:Sequence[Key]|Key,num_workers=None,func_loads=None,func_dumps=None,optimize_graph=True,pool=None,initializer=None,chunksize=None,**kwargs)
dask.multiprocessing.get_context()
dask.multiprocessing.initialize_worker_process(user_initializer=None)
dask.multiprocessing.pack_exception(e,dumps)
dask.multiprocessing.remote_exception(exc:Exception,tb)->Exception


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/__init__.py----------------------------------------
A:dask.__init__.versions->get_versions()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/local.py----------------------------------------
A:dask.local.cache->dict()
A:dask.local.data_keys->set()
A:dask.local.dsk2->dict(dsk).copy()
A:dask.local.dependents->reverse_dict(dependencies)
A:dask.local.ready->sorted(ready_set, key=sortkey, reverse=True)
A:dask.local.(task, data)->loads(task_info)
A:dask.local.result->pack_exception(e, dumps)
A:dask.local.id->get_id()
A:dask.local.queue->Queue()
A:dask.local.result_flat->set(flatten(result))
A:dask.local.results->set(result_flat)
A:dask.local.dsk->dict(dsk)
A:dask.local.(_, _, pretask_cbs, posttask_cbs, _)->unpack_callbacks(callbacks)
A:dask.local.keyorder->order(dsk)
A:dask.local.state->start_state_from_dask(dsk, cache=cache, sortkey=keyorder.get)
A:dask.local.rerun_exceptions_locally->dask.config.get('rerun_exceptions_locally', False)
A:dask.local.nready->len(state['ready'])
A:dask.local.avail_workers->max(num_workers - used_workers, 0)
A:dask.local.ntasks->min(nready, chunksize * avail_workers)
A:dask.local.key->state['ready'].pop()
A:dask.local.fut->Future()
A:dask.local.(exc, tb)->loads(res_info)
A:dask.local.(res, worker_id)->loads(res_info)
A:dask.local.synchronous_executor->SynchronousExecutor()
A:dask.local.self._max_workers->len(pool._pool)
dask.get(dsk:Mapping,keys:Sequence[Key]|Key,**kwargs)
dask.local.MultiprocessingPoolExecutor(self,pool)
dask.local.MultiprocessingPoolExecutor.__init__(self,pool)
dask.local.MultiprocessingPoolExecutor.submit(self,fn,*args,**kwargs)
dask.local.SynchronousExecutor(Executor)
dask.local.SynchronousExecutor.submit(self,fn,*args,**kwargs)
dask.local.batch_execute_tasks(it)
dask.local.default_get_id()
dask.local.default_pack_exception(e,dumps)
dask.local.execute_task(key,task_info,dumps,loads,get_id,pack_exception)
dask.local.finish_task(dsk,key,state,results,sortkey,delete=True,release_data=release_data)
dask.local.get_apply_async(apply_async,num_workers,*args,**kwargs)
dask.local.get_async(submit,num_workers,dsk,result,cache=None,get_id=default_get_id,rerun_exceptions_locally=None,pack_exception=default_pack_exception,raise_exception=reraise,callbacks=None,dumps=identity,loads=identity,chunksize=None,**kwargs)
dask.local.get_sync(dsk:Mapping,keys:Sequence[Key]|Key,**kwargs)
dask.local.identity(x)
dask.local.nested_get(ind,coll)
dask.local.release_data(key,state,delete=True)
dask.local.reraise(exc,tb=None)
dask.local.sortkey(item)
dask.local.start_state_from_dask(dsk,cache=None,sortkey=None)
dask.local.submit_apply_async(apply_async,fn,*args,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/highlevelgraph.py----------------------------------------
A:dask.highlevelgraph.seen->set()
A:dask.highlevelgraph.work->keys.copy()
A:dask.highlevelgraph.k->set(layers).pop()
A:dask.highlevelgraph.ret_deps[k]->self.get_dependencies(k, all_hlg_keys)
A:dask.highlevelgraph.typ->type(o)
A:dask.highlevelgraph.key->_get_some_layer_name(collection)
A:dask.highlevelgraph.value->clone_value(value)
A:dask.highlevelgraph.obj->type(self).__new__(self.__class__)
A:dask.highlevelgraph.shortname->key_split(self.name)
A:dask.highlevelgraph.chunks->self.collection_annotations.get('chunks')
A:dask.highlevelgraph.svg_repr->svg(chunks)
A:dask.highlevelgraph.info[key]->html.escape(str(val))
A:dask.highlevelgraph.graph->collection.__dask_graph__()
A:dask.highlevelgraph.layers->ensure_dict(graph.layers, copy=True)
A:dask.highlevelgraph.deps->ensure_dict(graph.dependencies, copy=True)
A:dask.highlevelgraph.deps[name]->set(collection.__dask_layers__())
A:dask.highlevelgraph.deps[key]->set()
A:dask.highlevelgraph.outself._to_dict->ensure_dict(self)
A:dask.highlevelgraph.all_keys->self.keys()
A:dask.highlevelgraph.self.key_dependencies[k]->ready.pop().get_dependencies(k, all_keys)
A:dask.highlevelgraph.dependencies[str(id(g))]->set()
A:dask.highlevelgraph.g->import_required('graphviz', 'Drawing dask graphs with the graphviz visualization engine requires the `graphviz` python library and the `graphviz` system library.\n\nPlease either conda or pip install as follows:\n\n  conda install python-graphviz     # either conda install\n  python -m pip install graphviz    # or pip install and follow installation instructions').Digraph(graph_attr=graph_attr, node_attr=node_attr, edge_attr=edge_attr)
A:dask.highlevelgraph.layer->ready.pop()
A:dask.highlevelgraph.keys_set->set(flatten(keys))
A:dask.highlevelgraph.all_ext_keys->self.get_all_external_keys()
A:dask.highlevelgraph.output_keys->set(flatten(keys)).intersection(layer.get_output_keys())
A:dask.highlevelgraph.(culled_layer, culled_deps)->ready.pop().cull(output_keys, all_ext_keys)
A:dask.highlevelgraph.external_deps->set()
A:dask.highlevelgraph.ret_layers_keys->set(ret_layers.keys())
A:dask.highlevelgraph.to_visit->set(layers)
A:dask.highlevelgraph.dependencies->compute_layer_dependencies(self.layers)
A:dask.highlevelgraph.dep_key1->self.dependencies.keys()
A:dask.highlevelgraph.dep_key2->compute_layer_dependencies(self.layers).keys()
A:dask.highlevelgraph.graphviz->import_required('graphviz', 'Drawing dask graphs with the graphviz visualization engine requires the `graphviz` python library and the `graphviz` system library.\n\nPlease either conda or pip install as follows:\n\n  conda install python-graphviz     # either conda install\n  python -m pip install graphviz    # or pip install and follow installation instructions')
A:dask.highlevelgraph.n_tasks[layer]->len(hg.layers[layer])
A:dask.highlevelgraph.min_tasks->min(n_tasks.values())
A:dask.highlevelgraph.max_tasks->max(n_tasks.values())
A:dask.highlevelgraph.color->kwargs.get('color')
A:dask.highlevelgraph.layer_name->name(layer)
A:dask.highlevelgraph.attrs->data_attributes.get(legend_title, {})
A:dask.highlevelgraph.node_label->label(layer, cache=cache)
A:dask.highlevelgraph.layer_type->str(type(hg.layers[layer]).__name__)
A:dask.highlevelgraph.cols->layer_ca.get('columns')
A:dask.highlevelgraph.dep_name->name(dep)
A:dask.highlevelgraph.(name,)->collection.__dask_layers__()
dask.highlevelgraph.HighLevelGraph(self,layers:Mapping[str,Graph],dependencies:Mapping[str,set[str]],key_dependencies:dict[Key,set[Key]]|None=None)
dask.highlevelgraph.HighLevelGraph.__getitem__(self,key:Key)->Any
dask.highlevelgraph.HighLevelGraph.__init__(self,layers:Mapping[str,Graph],dependencies:Mapping[str,set[str]],key_dependencies:dict[Key,set[Key]]|None=None)
dask.highlevelgraph.HighLevelGraph.__iter__(self)->Iterator[Key]
dask.highlevelgraph.HighLevelGraph.__len__(self)->int
dask.highlevelgraph.HighLevelGraph.__repr__(self)->str
dask.highlevelgraph.HighLevelGraph._from_collection(cls,name,layer,collection)
dask.highlevelgraph.HighLevelGraph._repr_html_(self)->str
dask.highlevelgraph.HighLevelGraph._toposort_layers(self)->list[str]
dask.highlevelgraph.HighLevelGraph.copy(self)->HighLevelGraph
dask.highlevelgraph.HighLevelGraph.cull(self,keys:Iterable[Key])->HighLevelGraph
dask.highlevelgraph.HighLevelGraph.cull_layers(self,layers:Iterable[str])->HighLevelGraph
dask.highlevelgraph.HighLevelGraph.dependents(self)->dict[str, set[str]]
dask.highlevelgraph.HighLevelGraph.from_collections(cls,name:str,layer:Graph,dependencies:Sequence[DaskCollection]=())->HighLevelGraph
dask.highlevelgraph.HighLevelGraph.get_all_dependencies(self)->dict[Key, set[Key]]
dask.highlevelgraph.HighLevelGraph.get_all_external_keys(self)->set[Key]
dask.highlevelgraph.HighLevelGraph.items(self)->ItemsView[Key, Any]
dask.highlevelgraph.HighLevelGraph.keys(self)->KeysView
dask.highlevelgraph.HighLevelGraph.merge(cls,*graphs:Graph)->HighLevelGraph
dask.highlevelgraph.HighLevelGraph.to_dict(self)->dict[Key, Any]
dask.highlevelgraph.HighLevelGraph.validate(self)->None
dask.highlevelgraph.HighLevelGraph.values(self)->ValuesView[Any]
dask.highlevelgraph.HighLevelGraph.visualize(self,filename='dask-hlg.svg',format=None,**kwargs)
dask.highlevelgraph.Layer(self,annotations:Mapping[str,Any]|None=None,collection_annotations:Mapping[str,Any]|None=None)
dask.highlevelgraph.Layer.__copy__(self)
dask.highlevelgraph.Layer.__init__(self,annotations:Mapping[str,Any]|None=None,collection_annotations:Mapping[str,Any]|None=None)
dask.highlevelgraph.Layer._repr_html_(self,layer_index='',highlevelgraph_key='',dependencies=())
dask.highlevelgraph.Layer.clone(self,keys:set,seed:Hashable,bind_to:Key|None=None)->tuple[Layer, bool]
dask.highlevelgraph.Layer.cull(self,keys:set[Key],all_hlg_keys:Collection[Key])->tuple[Layer, Mapping[Key, set[Key]]]
dask.highlevelgraph.Layer.get_dependencies(self,key:Key,all_hlg_keys:Collection[Key])->set
dask.highlevelgraph.Layer.get_output_keys(self)->Set[Key]
dask.highlevelgraph.Layer.is_materialized(self)->bool
dask.highlevelgraph.Layer.layer_info_dict(self)
dask.highlevelgraph.MaterializedLayer(self,mapping:Mapping,annotations=None,collection_annotations=None)
dask.highlevelgraph.MaterializedLayer.__contains__(self,k)
dask.highlevelgraph.MaterializedLayer.__getitem__(self,k)
dask.highlevelgraph.MaterializedLayer.__init__(self,mapping:Mapping,annotations=None,collection_annotations=None)
dask.highlevelgraph.MaterializedLayer.__iter__(self)
dask.highlevelgraph.MaterializedLayer.__len__(self)
dask.highlevelgraph.MaterializedLayer.get_output_keys(self)
dask.highlevelgraph.MaterializedLayer.is_materialized(self)
dask.highlevelgraph._get_some_layer_name(collection)->str
dask.highlevelgraph.compute_layer_dependencies(layers)
dask.highlevelgraph.to_graphviz(hg,data_attributes=None,function_attributes=None,rankdir='BT',graph_attr=None,node_attr=None,edge_attr=None,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/blockwise.py----------------------------------------
A:dask.blockwise.arrind_pairs->list(arrind_pairs)
A:dask.blockwise.output_indices->set(layers[dep].output_indices)
A:dask.blockwise.val->tuple(args)
A:dask.blockwise.new_tokens->tuple((blockwise_token(i) for i in range(len(inputs), len(inputs) + len(new_keys))))
A:dask.blockwise.sub->dict(zip(inputs[dep].output_indices, current_dep_indices))
A:dask.blockwise.kwargs->subs(kwargs, sub)
A:dask.blockwise.keys->tuple(map(blockwise_token, range(len(self.indices))))
A:dask.blockwise._keys->list(keys)
A:dask.blockwise.subgraph->Blockwise(output, output_indices, subgraph, indices, numblocks=numblocks, concatenate=concatenate, new_axes=new_axes)
A:dask.blockwise.self.output_indices->tuple(output_indices)
A:dask.blockwise.numblocks->tlz.merge([inp.numblocks for inp in inputs.values()])
A:dask.blockwise.io_deps->ensure_dict(io_deps or {}, copy=True)
A:dask.blockwise.name->tokenize(dep)
A:dask.blockwise.self.indices->tuple(_tmp_indices)
A:dask.blockwise.output_indices_set->set(self.output_indices)
A:dask.blockwise.self._dims->_make_dims(self.indices, self.numblocks, self.new_axes)
A:dask.blockwise.(dsk, _)->fuse(self.dsk, [self.output])
A:dask.blockwise.func->SubgraphCallable(dsk, self.output, keys)
A:dask.blockwise.dsk->subs(dsk, sub)
A:dask.blockwise.(coord_maps, concat_axes, dummies)->_get_coord_mapping(dims, output, out_indices, numblocks, argpairs, concatenate)
A:dask.blockwise.const_deps->set()
A:dask.blockwise.deps->set(blockwise_layers)
A:dask.blockwise.arg_coords->tuple((coords[c] for c in cmap))
A:dask.blockwise.tups->lol_product((arg,), arg_coords)
A:dask.blockwise.culled_deps->self._cull_dependencies(all_hlg_keys, output_blocks)
A:dask.blockwise.culled_layer->self._cull(output_blocks)
A:dask.blockwise.k->clone_key(k, seed)
A:dask.blockwise.block_names->set()
A:dask.blockwise.all_indices->set()
A:dask.blockwise.dummies->tuple(itertools.chain.from_iterable(_dummies_list))
A:dask.blockwise.argpairs->list(toolz.partition(2, arrind_pairs))
A:dask.blockwise.out->_optimize_blockwise(graph, keys=keys)
A:dask.blockwise.dependents->reverse_dict(dependencies)
A:dask.blockwise.stack->list(roots)
A:dask.blockwise.seen->set()
A:dask.blockwise.io_names->set()
A:dask.blockwise.layer->list(roots).pop()
A:dask.blockwise.dep->set(blockwise_layers).pop()
A:dask.blockwise.new_layer->rewrite_blockwise([layers[l] for l in blockwise_layers])
A:dask.blockwise.new_deps->set()
A:dask.blockwise.dependencies[layer]->full_graph.dependencies.get(layer, set())
A:dask.blockwise.annotations->tlz.merge(*args)
A:dask.blockwise.annotations['retries']->max(retries)
A:dask.blockwise.annotations['priority']->max(priorities)
A:dask.blockwise.annotations['resources']->tlz.merge_with(max, *resources)
A:dask.blockwise.annotations['workers']->list(set.intersection(*[set(w) for w in workers]))
A:dask.blockwise.annotations['allow_other_workers']->all(allow_other_workers)
A:dask.blockwise.fused_annotations->_fuse_annotations(*[i.annotations for i in inputs if i.annotations])
A:dask.blockwise.indices->list(inputs[root].indices)
A:dask.blockwise.(_, current_dep_indices)->list(inputs[root].indices).pop(i)
A:dask.blockwise.extra->dict(zip(contracted, new_index_iter))
A:dask.blockwise.sub[blockwise_token(ii)]->blockwise_token(len(indices))
A:dask.blockwise.index_map[id_key]->len(indices)
A:dask.blockwise.new_dsk->subs(inputs[dep].dsk, sub)
A:dask.blockwise.new_dsk[local_dep]->subs(inputs[dep].dsk, sub).pop(dep)
A:dask.blockwise.seen[x]->len(new_indices)
A:dask.blockwise.sub[i]->len(new_indices)
A:dask.blockwise.L->tlz.concat([zip(inds, dims) for ((x, inds), (x, dims)) in toolz.join(toolz.first, argpairs2, toolz.first, numblocks.items())])
A:dask.blockwise.g->tlz.groupby(0, L)
A:dask.blockwise.dims->broadcast_dimensions(indices, numblocks)
A:dask.blockwise.layers->ensure_dict(graph.layers, copy=True)
A:dask.blockwise.dependencies->ensure_dict(graph.dependencies, copy=True)
A:dask.blockwise.new->tlz.merge(layer, *[layers[dep] for dep in deps])
A:dask.blockwise.(new, _)->fuse(new, keys, ave_width=len(deps))
A:dask.blockwise.dependencies[name]->set()
dask.blockwise.BlockIndex(self,numblocks:tuple[int,...])
dask.blockwise.BlockIndex.__getitem__(self,idx:tuple[int,...])->tuple[int, ...]
dask.blockwise.BlockIndex.__init__(self,numblocks:tuple[int,...])
dask.blockwise.Blockwise(self,output:str,output_indices:Iterable[str],dsk:Graph,indices:Iterable[tuple[str|BlockwiseDep,Iterable[str]|None]],numblocks:Mapping[str,Sequence[int]],concatenate:bool|None=None,new_axes:Mapping[str,int]|None=None,output_blocks:set[tuple[int,...]]|None=None,annotations:Mapping[str,Any]|None=None,io_deps:Mapping[str,BlockwiseDep]|None=None)
dask.blockwise.Blockwise.__getitem__(self,key)
dask.blockwise.Blockwise.__init__(self,output:str,output_indices:Iterable[str],dsk:Graph,indices:Iterable[tuple[str|BlockwiseDep,Iterable[str]|None]],numblocks:Mapping[str,Sequence[int]],concatenate:bool|None=None,new_axes:Mapping[str,int]|None=None,output_blocks:set[tuple[int,...]]|None=None,annotations:Mapping[str,Any]|None=None,io_deps:Mapping[str,BlockwiseDep]|None=None)
dask.blockwise.Blockwise.__iter__(self)
dask.blockwise.Blockwise.__len__(self)->int
dask.blockwise.Blockwise.__repr__(self)
dask.blockwise.Blockwise._cull(self,output_blocks)
dask.blockwise.Blockwise._cull_dependencies(self,all_hlg_keys,output_blocks)
dask.blockwise.Blockwise._dict(self)
dask.blockwise.Blockwise.clone(self,keys:set[Key],seed:Hashable,bind_to:Key|None=None)->tuple[Layer, bool]
dask.blockwise.Blockwise.cull(self,keys:set,all_hlg_keys:Iterable)->tuple[Layer, Mapping[Key, set[Key]]]
dask.blockwise.Blockwise.dims(self)
dask.blockwise.Blockwise.get_output_keys(self)
dask.blockwise.Blockwise.is_materialized(self)
dask.blockwise.BlockwiseDep
dask.blockwise.BlockwiseDep.__getitem__(self,idx:tuple[int,...])->Any
dask.blockwise.BlockwiseDep.__repr__(self)->str
dask.blockwise.BlockwiseDep.get(self,idx:tuple[int,...],default)->Any
dask.blockwise.BlockwiseDep.produces_keys(self)->bool
dask.blockwise.BlockwiseDepDict(self,mapping:dict,numblocks:tuple[int,...]|None=None,produces_tasks:bool=False,produces_keys:bool=False)
dask.blockwise.BlockwiseDepDict.__getitem__(self,idx:tuple[int,...])->Any
dask.blockwise.BlockwiseDepDict.__init__(self,mapping:dict,numblocks:tuple[int,...]|None=None,produces_tasks:bool=False,produces_keys:bool=False)
dask.blockwise.BlockwiseDepDict.__len__(self)->int
dask.blockwise.BlockwiseDepDict.produces_keys(self)->bool
dask.blockwise._can_fuse_annotations(a:dict|None,b:dict|None)->bool
dask.blockwise._fuse_annotations(*args:dict)->dict
dask.blockwise._get_coord_mapping(dims,output,out_indices,numblocks,argpairs,concatenate)
dask.blockwise._make_dims(indices,numblocks,new_axes)
dask.blockwise._optimize_blockwise(full_graph,keys=())
dask.blockwise._unique_dep(dep,ind)
dask.blockwise.blockwise(func,output,output_indices,*arrind_pairs,numblocks=None,concatenate=None,new_axes=None,dependencies=(),**kwargs)
dask.blockwise.blockwise_token(i,prefix=_BLOCKWISE_DEFAULT_PREFIX)
dask.blockwise.broadcast_dimensions(argpairs,numblocks,sentinels=(1,(1,)),consolidate=None)
dask.blockwise.fuse_roots(graph:HighLevelGraph,keys:list)
dask.blockwise.index_subs(ind,substitution)
dask.blockwise.lol_product(head,values)
dask.blockwise.lol_tuples(head,ind,values,dummies)
dask.blockwise.make_blockwise_graph(func,output,out_indices,*arrind_pairs,numblocks=None,concatenate=None,new_axes=None,output_blocks=None,dims=None,deserializing=False,func_future_args=None,return_key_deps=False,io_deps=None)
dask.blockwise.optimize_blockwise(graph,keys=())
dask.blockwise.rewrite_blockwise(inputs)
dask.blockwise.subs(task,substitution)
dask.blockwise.zero_broadcast_dimensions(lol,nblocks)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/compatibility.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/order.py----------------------------------------
A:dask.order.dsk->dict(dsk)
A:dask.order.expected_len->len(dsk)
A:dask.order.dependencies->copy.deepcopy(dependencies)
A:dask.order.dependents->reverse_dict(dependencies)
A:dask.order.requires_data_task->defaultdict(set)
A:dask.order.result[leaf]->Order(prio, -1)
A:dask.order.(num_needed, total_dependencies)->ndependencies(dependencies, dependents)
A:dask.order.cycle->getcycle(dsk, None)
A:dask.order.(roots_connected, max_dependents)->_connecting_to_roots(dependencies, dependents)
A:dask.order.(leafs_connected, _)->_connecting_to_roots(dependents, dependencies)
A:dask.order.runnable_hull->set()
A:dask.order.reachable_hull->set()
A:dask.order.item->path_pop()
A:dask.order.result[item]->Order(i, crit_path_counter - _crit_path_counter_offset)
A:dask.order.candidates->runnable.copy()
A:dask.order.key->current_pop()
A:dask.order.branches->deque([path])
A:dask.order.path->pruned_branches.popleft()
A:dask.order.branch->pruned_branches.popleft().copy()
A:dask.order.known_runnable_paths[current]->list(pruned_branches)
A:dask.order.size->len(leafs_connected[r])
A:dask.order.longest_path->use_longest_path()
A:dask.order.occurences_grouped->defaultdict(set)
A:dask.order.occurences_grouped_sorted[k]->sorted(v, key=sort_key, reverse=True)
A:dask.order.get_target->_build_get_target()
A:dask.order.leaf_nodes_sorted->sorted(leaf_nodes, key=sort_key, reverse=False)
A:dask.order.target->get_target()
A:dask.order.deps->dependencies[item].difference(result)
A:dask.order.roots->set()
A:dask.order.max_dependents[k]->len(deps)
A:dask.order.new_set->result_first.copy()
A:dask.order.max_dependents[key]->max(max_dependents[child], max_dependents[key])
A:dask.order.num_needed[k]->len(v)
A:dask.order.num_dependencies->num_needed.copy()
A:dask.order.OrderInfo->namedtuple('OrderInfo', ('order', 'age', 'num_data_when_run', 'num_data_when_released', 'num_dependencies_freed'))
A:dask.order.(dependencies, dependents)->get_deps(dsk)
A:dask.order.o->order(dsk, dependencies=dependencies, return_stats=False)
A:dask.order.new[new_key]->_convert_task(values)
dask.order.Order(NamedTuple)
dask.order._connecting_to_roots(dependencies:Mapping[Key,set[Key]],dependents:Mapping[Key,set[Key]])->tuple[dict[Key, set[Key]], dict[Key, int]]
dask.order._convert_task(task:Any)->Any
dask.order._f()->None
dask.order.diagnostics(dsk:MutableMapping[Key,Any],o:Mapping[Key,int]|None=None,dependencies:MutableMapping[Key,set[Key]]|None=None)->tuple[dict[Key, OrderInfo], list[int]]
dask.order.ndependencies(dependencies:Mapping[Key,set[Key]],dependents:Mapping[Key,set[Key]])->tuple[dict[Key, int], dict[Key, int]]
dask.order.order(dsk:Mapping[Key,Any],dependencies:Mapping[Key,set[Key]]|None=None,*,return_stats:bool=False)->dict[Key, Order] | dict[Key, int]
dask.order.sanitize_dsk(dsk:MutableMapping[Key,Any])->dict


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/layers.py----------------------------------------
A:dask.layers.self.numblocks->tuple((len(chunk) for chunk in chunks))
A:dask.layers.self.starts->tuple((cached_cumsum(c, initial_zero=True) for c in chunks))
A:dask.layers.loc->tuple(((start[i], start[i + 1]) for (i, start) in zip(idx, self.starts)))
A:dask.layers.dsk->self._construct_graph()
A:dask.layers.ind->len(args)
A:dask.layers.self._cached_keysresult->keys()
A:dask.layers.dask_keys->self._dask_keys()
A:dask.layers.concatenate3->CallableLazyImport('dask.array.core.concatenate3')
A:dask.layers.dims->list(map(len, chunks))
A:dask.layers.expand_key2->functools.partial(_expand_keys_around_center, dims=dims, axes=axes)
A:dask.layers.interior_keys->tlz.pipe(dask_keys, flatten, map(expand_key2), map(flatten), toolz.concat, list)
A:dask.layers.frac_slice->fractional_slice((name,) + k, axes)
A:dask.layers.seq->list(product(*args))
A:dask.layers.result->reshapelist(shape2, seq)
A:dask.layers.n->int(len(seq) / shape[0])
A:dask.layers.depth->axes.get(i, 0)
A:dask.layers.index->tuple(index)
A:dask.layers.parts->math.ceil(parts / self.split_every)
A:dask.layers.deps->defaultdict(set)
A:dask.layers.parts_out->self._keys_to_parts(keys)
A:dask.layers.culled_deps->self._cull_dependencies(keys, parts_out=parts_out)
A:dask.layers.culled_layer->self._cull(output_partitions)
A:dask.layers.concat_func->CallableLazyImport('dask.dataframe.multi._concat_wrapper')
A:dask.layers.shuffle_group_func->CallableLazyImport('dask.dataframe.shuffle.shuffle_group')
A:dask.layers._inp->insert(out, self.stage, i)
A:dask.layers.self.how->self.merge_kwargs.get('how')
A:dask.layers.split_partition_func->CallableLazyImport('dask.dataframe.multi._split_partition')
A:dask.layers.merge_chunk_func->CallableLazyImport('dask.dataframe.multi._merge_chunk_wrapper')
A:dask.layers.io_arg_map->BlockwiseDepDict({(i,): inp for (i, inp) in enumerate(self.inputs)}, produces_tasks=self.produces_tasks)
A:dask.layers.columns->list(columns)
A:dask.layers.io_func->self.io_func.project_columns(columns)
A:dask.layers.layer->DataFrameIOLayer((self.label or 'subset') + '-' + tokenize(self.name, columns), columns, self.inputs, io_func, label=self.label, produces_tasks=self.produces_tasks, annotations=self.annotations)
A:dask.layers.self.height->len(self.widths)
A:dask.layers.lstop->min(lstart + self.split_every, p_max)
A:dask.layers.dsk[self.name, s]->self._define_task(input_keys, final_task=True)
A:dask.layers.dsk[self._make_key(self.tree_node_name, group, depth, split=s)]->self._define_task(input_keys, final_task=False)
A:dask.layers.output_keys->self._output_keys()
A:dask.layers.splits->set()
A:dask.layers.output_partitions->self._keys_to_output_partitions(keys)
dask.layers.ArrayBlockwiseDep(self,chunks:tuple[tuple[int,...],...])
dask.layers.ArrayBlockwiseDep.__getitem__(self,idx:tuple[int,...])
dask.layers.ArrayBlockwiseDep.__init__(self,chunks:tuple[tuple[int,...],...])
dask.layers.ArrayChunkShapeDep(ArrayBlockwiseDep)
dask.layers.ArrayChunkShapeDep.__getitem__(self,idx:tuple[int,...])
dask.layers.ArrayOverlapLayer(self,name,axes,chunks,numblocks,token)
dask.layers.ArrayOverlapLayer.__getitem__(self,key)
dask.layers.ArrayOverlapLayer.__init__(self,name,axes,chunks,numblocks,token)
dask.layers.ArrayOverlapLayer.__iter__(self)
dask.layers.ArrayOverlapLayer.__len__(self)
dask.layers.ArrayOverlapLayer.__repr__(self)
dask.layers.ArrayOverlapLayer._construct_graph(self,deserializing=False)
dask.layers.ArrayOverlapLayer._dask_keys(self)
dask.layers.ArrayOverlapLayer._dict(self)
dask.layers.ArrayOverlapLayer.get_output_keys(self)
dask.layers.ArrayOverlapLayer.is_materialized(self)
dask.layers.ArraySliceDep(self,chunks:tuple[tuple[int,...],...])
dask.layers.ArraySliceDep.__getitem__(self,idx:tuple)
dask.layers.ArraySliceDep.__init__(self,chunks:tuple[tuple[int,...],...])
dask.layers.BroadcastJoinLayer(self,name,npartitions,lhs_name,lhs_npartitions,rhs_name,rhs_npartitions,parts_out=None,annotations=None,left_on=None,right_on=None,**merge_kwargs)
dask.layers.BroadcastJoinLayer.__getitem__(self,key)
dask.layers.BroadcastJoinLayer.__init__(self,name,npartitions,lhs_name,lhs_npartitions,rhs_name,rhs_npartitions,parts_out=None,annotations=None,left_on=None,right_on=None,**merge_kwargs)
dask.layers.BroadcastJoinLayer.__iter__(self)
dask.layers.BroadcastJoinLayer.__len__(self)
dask.layers.BroadcastJoinLayer.__repr__(self)
dask.layers.BroadcastJoinLayer._broadcast_plan(self)
dask.layers.BroadcastJoinLayer._construct_graph(self,deserializing=False)
dask.layers.BroadcastJoinLayer._cull(self,parts_out)
dask.layers.BroadcastJoinLayer._cull_dependencies(self,keys,parts_out=None)
dask.layers.BroadcastJoinLayer._dict(self)
dask.layers.BroadcastJoinLayer._keys_to_parts(self,keys)
dask.layers.BroadcastJoinLayer.cull(self,keys,all_keys)
dask.layers.BroadcastJoinLayer.get_output_keys(self)
dask.layers.BroadcastJoinLayer.is_materialized(self)
dask.layers.CallableLazyImport(self,function_path)
dask.layers.CallableLazyImport.__init__(self,function_path)
dask.layers.DataFrameIOLayer(self,name,columns,inputs,io_func,label=None,produces_tasks=False,creation_info=None,annotations=None)
dask.layers.DataFrameIOLayer.__init__(self,name,columns,inputs,io_func,label=None,produces_tasks=False,creation_info=None,annotations=None)
dask.layers.DataFrameIOLayer.__repr__(self)
dask.layers.DataFrameIOLayer.columns(self)
dask.layers.DataFrameIOLayer.project_columns(self,columns)
dask.layers.DataFrameTreeReduction(self,name:str,name_input:str,npartitions_input:int,concat_func:Callable,tree_node_func:Callable,finalize_func:Callable|None=None,split_every:int=32,split_out:int|None=None,output_partitions:list[int]|None=None,tree_node_name:str|None=None,annotations:dict[str,Any]|None=None)
dask.layers.DataFrameTreeReduction.__getitem__(self,key)
dask.layers.DataFrameTreeReduction.__init__(self,name:str,name_input:str,npartitions_input:int,concat_func:Callable,tree_node_func:Callable,finalize_func:Callable|None=None,split_every:int=32,split_out:int|None=None,output_partitions:list[int]|None=None,tree_node_name:str|None=None,annotations:dict[str,Any]|None=None)
dask.layers.DataFrameTreeReduction.__iter__(self)
dask.layers.DataFrameTreeReduction.__len__(self)
dask.layers.DataFrameTreeReduction.__repr__(self)
dask.layers.DataFrameTreeReduction._construct_graph(self)
dask.layers.DataFrameTreeReduction._cull(self,output_partitions)
dask.layers.DataFrameTreeReduction._define_task(self,input_keys,final_task=False)
dask.layers.DataFrameTreeReduction._dict(self)
dask.layers.DataFrameTreeReduction._keys_to_output_partitions(self,keys)
dask.layers.DataFrameTreeReduction._make_key(self,*name_parts,split=0)
dask.layers.DataFrameTreeReduction._output_keys(self)
dask.layers.DataFrameTreeReduction.cull(self,keys,all_keys)
dask.layers.DataFrameTreeReduction.get_output_keys(self)
dask.layers.DataFrameTreeReduction.is_materialized(self)
dask.layers.ShuffleLayer(self,name,column,inputs,stage,npartitions,npartitions_input,nsplits,ignore_index,name_input,meta_input,parts_out=None,annotations=None)
dask.layers.ShuffleLayer.__init__(self,name,column,inputs,stage,npartitions,npartitions_input,nsplits,ignore_index,name_input,meta_input,parts_out=None,annotations=None)
dask.layers.ShuffleLayer.__repr__(self)
dask.layers.ShuffleLayer._construct_graph(self,deserializing=False)
dask.layers.ShuffleLayer._cull(self,parts_out)
dask.layers.ShuffleLayer._cull_dependencies(self,keys,parts_out=None)
dask.layers.SimpleShuffleLayer(self,name,column,npartitions,npartitions_input,ignore_index,name_input,meta_input,parts_out=None,annotations=None)
dask.layers.SimpleShuffleLayer.__getitem__(self,key)
dask.layers.SimpleShuffleLayer.__init__(self,name,column,npartitions,npartitions_input,ignore_index,name_input,meta_input,parts_out=None,annotations=None)
dask.layers.SimpleShuffleLayer.__iter__(self)
dask.layers.SimpleShuffleLayer.__len__(self)
dask.layers.SimpleShuffleLayer.__repr__(self)
dask.layers.SimpleShuffleLayer._construct_graph(self,deserializing=False)
dask.layers.SimpleShuffleLayer._cull(self,parts_out)
dask.layers.SimpleShuffleLayer._cull_dependencies(self,keys,parts_out=None)
dask.layers.SimpleShuffleLayer._dict(self)
dask.layers.SimpleShuffleLayer._key_priority(self,key)
dask.layers.SimpleShuffleLayer._keys_to_parts(self,keys)
dask.layers.SimpleShuffleLayer.cull(self,keys,all_keys)
dask.layers.SimpleShuffleLayer.get_output_keys(self)
dask.layers.SimpleShuffleLayer.is_materialized(self)
dask.layers._expand_keys_around_center(k,dims,name=None,axes=None)
dask.layers.fractional_slice(task,axes)
dask.layers.reshapelist(shape,seq)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/core.py----------------------------------------
A:dask.core.result->lists_to_tuples(result, out)
A:dask.core.typ->type(key)
A:dask.core.dependents->reverse_dict({k: inplay.intersection(dependencies[k]) for k in inplay})
A:dask.core.T_->TypeVar('T_')
A:dask.core.type_task->type(task)
A:dask.core.type_arg->type(arg)
A:dask.core.arg->subs(arg, key, val)
A:dask.core.completed->set()
A:dask.core.seen->set()
A:dask.core.inplay->set(priorities)
A:dask.core.prev->min(deps, key=priorities.__getitem__)
A:dask.core.cycle->'->'.join((str(x) for x in cycle))
dask.core._execute_task(arg,cache,dsk=None)
dask.core._toposort(dsk,keys=None,returncycle=False,dependencies=None)
dask.core.flatten(seq,container=list)
dask.core.get(dsk,out,cache=None)
dask.core.get_dependencies(dsk:Graph,key:Key|None=None,task:Key|NoDefault=no_default,as_list:bool=False)->set[Key] | list[Key]
dask.core.get_deps(dsk:Graph)->tuple[dict[Key, set[Key]], dict[Key, set[Key]]]
dask.core.getcycle(d,keys)
dask.core.has_tasks(dsk,x)
dask.core.isdag(d,keys)
dask.core.ishashable(x)
dask.core.iskey(key:object)->bool
dask.core.istask(x)
dask.core.keys_in_tasks(keys:Collection[Key],tasks:Iterable[Any],as_list:bool=False)
dask.core.lists_to_tuples(res,keys)
dask.core.literal(self,data)
dask.core.literal.__init__(self,data)
dask.core.literal.__reduce__(self)
dask.core.literal.__repr__(self)
dask.core.preorder_traversal(task)
dask.core.quote(x)
dask.core.reverse_dict(d:Mapping[T_,Iterable[T_]])->dict[T_, set[T_]]
dask.core.subs(task,key,val)
dask.core.toposort(dsk,dependencies=None)
dask.core.validate_key(key:object)->None
dask.istask(x)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/utils.py----------------------------------------
A:dask.utils.K->TypeVar('K')
A:dask.utils.V->TypeVar('V')
A:dask.utils.T->TypeVar('T')
A:dask.utils.F->TypeVar('F', bound=FuncType)
A:dask.utils.system_encoding->sys.getdefaultencoding()
A:dask.utils.old_arg_value->kwargs.pop(old_arg_name, None)
A:dask.utils.new_arg_value->mapping.get(old_arg_value, old_arg_value)
A:dask.utils.extension->extension.lstrip('.').lstrip('.')
A:dask.utils.(handle, filename)->tempfile.mkstemp(extension, dir=dir)
A:dask.utils.dirname->tempfile.mkdtemp(dir=dir)
A:dask.utils.f->open(filename, 'w' + mode)
A:dask.utils.old_cwd->os.getcwd()
A:dask.utils.seq->list(map(concrete, seq))
A:dask.utils.p->list(p)
A:dask.utils.cp->numpy.cumsum([0] + p)
A:dask.utils.random_state->numpy.random.RandomState(random_state)
A:dask.utils.x->numpy.random.RandomState(random_state).random_sample(n)
A:dask.utils.out->numpy.empty(n, dtype='i1')
A:dask.utils.random_data->numpy.random.RandomState(random_state).bytes(624 * n * 4)
A:dask.utils.l->list(np.frombuffer(random_data, dtype=np.uint32).reshape((n, -1)))
A:dask.utils.func->object.__new__(cls).dispatch(object)
A:dask.utils.spec->getargspec(func)
A:dask.utils.s->s.replace(' ', '').replace(' ', '')
A:dask.utils.(toplevel, _, _)->cls2.__module__.partition('.')
A:dask.utils.meth->object.__new__(cls).dispatch(type(arg))
A:dask.utils.stripped->line.strip()
A:dask.utils.lines->extra_titles(doc).split('\n')
A:dask.utils.seen->set()
A:dask.utils.lines[i]->lines[i].replace(title, new_title).replace(title, new_title)
A:dask.utils.lines[i + 1]->lines[i + 1].replace('-' * len(title), '-' * len(new_title)).replace('-' * len(title), '-' * len(new_title))
A:dask.utils.l1->'This docstring was copied from {}.{}.{}.\n\n'.format(cls.__module__, cls.__name__, name)
A:dask.utils.i->tail.find('\n\n')
A:dask.utils.indent->re.match('\\s*', tail).group(0)
A:dask.utils.doc->extra_titles(doc)
A:dask.utils.original_method->getattr(cls, method.__name__)
A:dask.utils.obj_method->getattr(obj, method.__name__, None)
A:dask.utils.method_args->get_named_args(method)
A:dask.utils.original_args->get_named_args(original_method)
A:dask.utils.method.__doc__->_derived_from(original_klass, method, ua_args=ua_args, extra=extra, skipblocks=skipblocks, inconsistencies=inconsistencies)
A:dask.utils.(module, *_)->type(obj).__module__.split('.')
A:dask.utils.L->list(tup)
A:dask.utils.(deps, _)->get_deps(dsk)
A:dask.utils.columns->tuple((str(i) for i in columns))
A:dask.utils.widths->tuple((max(max(map(len, x)), len(c)) for (x, c) in zip(zip(*rows), columns)))
A:dask.utils.data->'\n'.join((row_template % r for r in rows))
A:dask.utils.self->object.__new__(cls)
A:dask.utils.M->MethodCache()
A:dask.utils.self.lock->Lock()
A:dask.utils.actual_get->get_scheduler(collections=[collection], scheduler=scheduler)
A:dask.utils.client->get_client()
A:dask.utils.function->kwargs.pop('function')
A:dask.utils.other->kwargs.pop('other')
A:dask.utils.args2->list(args)
A:dask.utils.n->float(prefix)
A:dask.utils.d->int(n / 3600 / 24)
A:dask.utils.h->int(n / 3600)
A:dask.utils.m->int(n / 60)
A:dask.utils.dur->int(units[unit](diff))
A:dask.utils.valid_units->', '.join(timedelta_sizes.keys())
A:dask.utils.result->_cumsum(tuple(seq), initial_zero)
A:dask.utils.iter_sizes->iter(sizes)
A:dask.utils.size->next(iter_sizes, None)
A:dask.utils.hex_pattern->re.compile('[a-f]+')
A:dask.utils.words->s.replace(' ', '').replace(' ', '').split('-')
A:dask.utils.typ->type(obj)
A:dask.utils.result[modname]->version(modname)
A:dask.utils.exc_val.__traceback__->object.__new__(cls).shorten(exc_tb)
A:dask.utils.paths->dask.config.get('admin.traceback.shorten')
A:dask.utils.exp->re.compile('.*(' + '|'.join(paths) + ')')
dask.utils.Dispatch(self,name=None)
dask.utils.Dispatch.__doc__(self)
dask.utils.Dispatch.__init__(self,name=None)
dask.utils.Dispatch.dispatch(self,cls)
dask.utils.Dispatch.register(self,type,func=None)
dask.utils.Dispatch.register_lazy(self,toplevel,func=None)
dask.utils.IndexCallable(self,fn)
dask.utils.IndexCallable.__getitem__(self,key)
dask.utils.IndexCallable.__init__(self,fn)
dask.utils.MethodCache
dask.utils.MethodCache.__dir__(self)
dask.utils.MethodCache.__getattr__(self,item)
dask.utils.OperatorMethodMixin
dask.utils.OperatorMethodMixin._bind_operator(cls,op)
dask.utils.OperatorMethodMixin._get_binary_operator(cls,op,inv=False)
dask.utils.OperatorMethodMixin._get_unary_operator(cls,op)
dask.utils.SerializableLock(self,token:Hashable|None=None)
dask.utils.SerializableLock.__enter__(self)
dask.utils.SerializableLock.__exit__(self,*args)
dask.utils.SerializableLock.__getstate__(self)
dask.utils.SerializableLock.__init__(self,token:Hashable|None=None)
dask.utils.SerializableLock.__setstate__(self,token)
dask.utils.SerializableLock.__str__(self)
dask.utils.SerializableLock.acquire(self,*args,**kwargs)
dask.utils.SerializableLock.locked(self)
dask.utils.SerializableLock.release(self,*args,**kwargs)
dask.utils._HashIdWrapper(self,wrapped)
dask.utils._HashIdWrapper.__eq__(self,other)
dask.utils._HashIdWrapper.__hash__(self)
dask.utils._HashIdWrapper.__init__(self,wrapped)
dask.utils._HashIdWrapper.__ne__(self,other)
dask.utils._cumsum(seq,initial_zero)
dask.utils._deprecated(*,version:str|None=None,after_version:str|None=None,message:str|None=None,use_instead:str|None=None,category:type[Warning]=FutureWarning)
dask.utils._deprecated_kwarg(old_arg_name:str,new_arg_name:str|None,mapping:Mapping[Any,Any]|Callable[[Any],Any]|None=None,stacklevel:int=2)->Callable[[F], F]
dask.utils._derived_from(cls,method,ua_args=None,extra='',skipblocks=0,inconsistencies=None)
dask.utils._skip_doctest(line)
dask.utils.apply(func,args,kwargs=None)
dask.utils.asciitable(columns,rows)
dask.utils.cached_cumsum(seq,initial_zero=False)
dask.utils.cached_property(functools.cached_property)
dask.utils.cached_property.__set__(self,instance,val)
dask.utils.changed_cwd(new_cwd)
dask.utils.concrete(seq)
dask.utils.deepmap(func,*seqs)
dask.utils.dependency_depth(dsk)
dask.utils.derived_from(original_klass,version=None,ua_args=None,skipblocks=0,inconsistencies=None)
dask.utils.digit(n,k,base)
dask.utils.ensure_bytes(s)->bytes
dask.utils.ensure_dict(d:Mapping[K,V],*,copy:bool=False)->dict[K, V]
dask.utils.ensure_not_exists(filename)->None
dask.utils.ensure_set(s:Set[T],*,copy:bool=False)->set[T]
dask.utils.ensure_unicode(s)->str
dask.utils.extra_titles(doc)
dask.utils.filetext(text,extension='',open=open,mode='w')
dask.utils.filetexts(d,open=open,mode='t',use_tmpdir=True)
dask.utils.format_bytes(n:int)->str
dask.utils.format_time(n:float)->str
dask.utils.format_time_ago(n:datetime)->str
dask.utils.funcname(func)->str
dask.utils.get_default_shuffle_method()->str
dask.utils.get_meta_library(like)
dask.utils.get_named_args(func)->list[str]
dask.utils.get_scheduler_lock(collection=None,scheduler=None)
dask.utils.getargspec(func)
dask.utils.has_keyword(func,keyword)
dask.utils.homogeneous_deepmap(func,seq)
dask.utils.ignore_warning(doc,cls,name,extra='',skipblocks=0,inconsistencies=None)
dask.utils.import_required(mod_name,error_msg)
dask.utils.insert(tup,loc,val)
dask.utils.is_arraylike(x)->bool
dask.utils.is_cupy_type(x)->bool
dask.utils.is_dataframe_like(df)->bool
dask.utils.is_index_like(s)->bool
dask.utils.is_integer(i)->bool
dask.utils.is_namedtuple_instance(obj:Any)->bool
dask.utils.is_series_like(s)->bool
dask.utils.itemgetter(self,index)
dask.utils.itemgetter.__eq__(self,other)
dask.utils.itemgetter.__init__(self,index)
dask.utils.itemgetter.__reduce__(self)
dask.utils.iter_chunks(sizes,max_size)
dask.utils.key_split(s)
dask.utils.maybe_pluralize(count,noun,plural_form=None)
dask.utils.memory_repr(num)
dask.utils.methodcaller(cls,method:str)
dask.utils.methodcaller.__new__(cls,method:str)
dask.utils.methodcaller.__reduce__(self)
dask.utils.methodcaller.__str__(self)
dask.utils.methodcaller.func(self)->str
dask.utils.natural_sort_key(s:str)->list[str | int]
dask.utils.ndeepmap(n,func,seq)
dask.utils.ndimlist(seq)
dask.utils.parse_bytes(s:float|str)->int
dask.utils.parse_timedelta(s,default='seconds')
dask.utils.partial_by_order(*args,**kwargs)
dask.utils.pseudorandom(n:int,p,random_state=None)
dask.utils.put_lines(buf,lines)
dask.utils.random_state_data(n:int,random_state=None)->list
dask.utils.shorten_traceback
dask.utils.shorten_traceback.__enter__(self)->None
dask.utils.shorten_traceback.__exit__(self,exc_type:type[BaseException]|None,exc_val:BaseException|None,exc_tb:types.TracebackType|None)->None
dask.utils.shorten_traceback.shorten(exc_tb:types.TracebackType)->types.TracebackType
dask.utils.show_versions()->None
dask.utils.skip_doctest(doc)
dask.utils.stringify(obj,exclusive:Iterable|None=None)
dask.utils.stringify_collection_keys(obj)
dask.utils.takes_multiple_arguments(func,varargs=True)
dask.utils.tmp_cwd(dir=None)
dask.utils.tmpdir(dir=None)
dask.utils.tmpfile(extension='',dir=None)
dask.utils.typename(typ:Any,short:bool=False)->str
dask.utils.unsupported_arguments(doc,args)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/optimization.py----------------------------------------
A:dask.optimization.seen->set()
A:dask.optimization.dependencies->dict()
A:dask.optimization.work->list(set(flatten(keys)))
A:dask.optimization.dependencies_k->get_dependencies(dsk, k, as_list=True)
A:dask.optimization.typ->type(first_key)
A:dask.optimization.keys->set(flatten(keys))
A:dask.optimization.unfusible->set()
A:dask.optimization.parent2child->dict(map(reversed, child2parent.items()))
A:dask.optimization.(child, parent)->child2parent.popitem()
A:dask.optimization.parent->child2parent.pop(parent)
A:dask.optimization.child->dict(map(reversed, child2parent.items())).pop(child)
A:dask.optimization.fused->set()
A:dask.optimization.aliases->set()
A:dask.optimization.new_key->key_renamer(chain)
A:dask.optimization.val->subs(val, cur_child, child_info[1])
A:dask.optimization.rv[key]->subs(rv[key], old_key, new_key)
A:dask.optimization.replaceorder->toposort({k: dsk[k] for k in keys if k in dsk}, dependencies=dependencies)
A:dask.optimization.dsk2->keysubs.copy()
A:dask.optimization.output->set(output)
A:dask.optimization.fast_functions->set(fast_functions)
A:dask.optimization.dependents->reverse_dict(dependencies)
A:dask.optimization.dsk->inline(dsk, keys, inline_constants=inline_constants, dependencies=dependencies)
A:dask.optimization.funcs->set()
A:dask.optimization.it->reversed(keys)
A:dask.optimization.first_key->next(it)
A:dask.optimization.first_name->dask.utils.key_split(first_key)
A:dask.optimization.names->sorted(names)
A:dask.optimization.concatenated_name->'-'.join(names)
A:dask.optimization.ave_width->dask.config.get('optimization.fuse.ave-width')
A:dask.optimization.max_height->dask.config.get('optimization.fuse.max-height')
A:dask.optimization.max_depth_new_edges->dask.config.get('optimization.fuse.max-depth-new-edges')
A:dask.optimization.max_width->dask.config.get('optimization.fuse.max-width')
A:dask.optimization.fuse_subgraphs->dask.config.get('optimization.fuse.subgraphs')
A:dask.optimization.rename_keys->dask.config.get('optimization.fuse.rename-keys')
A:dask.optimization.deps[k]->set(vals)
A:dask.optimization.rv->inline(dsk, keys, inline_constants=inline_constants, dependencies=dependencies).copy()
A:dask.optimization.num_children->len(children)
A:dask.optimization.(child_key, child_task, child_keys, height, width, num_nodes, fudge, children_edges)->info_stack_pop()
A:dask.optimization.num_children_edges->len(children_edges)
A:dask.optimization.fudge->int(ave_width - 1)
A:dask.optimization.children_edges->set()
A:dask.optimization.max_num_edges->len(cur_edges)
A:dask.optimization.children_deps->set()
A:dask.optimization.alias->key_renamer(fused_keys)
A:dask.optimization.inkeys->tuple(inkeys_set)
A:dask.optimization.subchain->fused_trees.pop(k, False)
dask.optimization.Default(Enum)
dask.optimization.Default.__repr__(self)->str
dask.optimization.SubgraphCallable(self,dsk,outkey,inkeys,name=None)
dask.optimization.SubgraphCallable.__eq__(self,other)
dask.optimization.SubgraphCallable.__hash__(self)
dask.optimization.SubgraphCallable.__init__(self,dsk,outkey,inkeys,name=None)
dask.optimization.SubgraphCallable.__ne__(self,other)
dask.optimization.SubgraphCallable.__reduce__(self)
dask.optimization.SubgraphCallable.__repr__(self)
dask.optimization._flat_set(x)
dask.optimization._inplace_fuse_subgraphs(dsk,keys,dependencies,fused_trees,rename_keys)
dask.optimization.cull(dsk,keys)
dask.optimization.default_fused_keys_renamer(keys,max_fused_key_length=120)
dask.optimization.default_fused_linear_keys_renamer(keys)
dask.optimization.functions_of(task)
dask.optimization.fuse(dsk,keys=None,dependencies=None,ave_width=_default,max_width=_default,max_height=_default,max_depth_new_edges=_default,rename_keys=_default,fuse_subgraphs=_default)
dask.optimization.fuse_linear(dsk,keys=None,dependencies=None,rename_keys=True)
dask.optimization.inline(dsk,keys=None,inline_constants=True,dependencies=None)
dask.optimization.inline_functions(dsk,output,fast_functions=None,inline_constants=False,dependencies=None)
dask.optimization.unwrap_partial(func)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/_version.py----------------------------------------
dask._version.get_versions()
dask.get_versions()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/graph_manipulation.py----------------------------------------
A:dask.graph_manipulation.T->TypeVar('T')
A:dask.graph_manipulation.split_every->int(split_every)
A:dask.graph_manipulation.(collections, _)->unpack_collections(*collections)
A:dask.graph_manipulation.tok->tokenize(coll, blocker)
A:dask.graph_manipulation.keys_iter->flatten(collection.__dask_keys__())
A:dask.graph_manipulation.dsk->dask.highlevelgraph.HighLevelGraph.merge(*dsks)
A:dask.graph_manipulation.map_names->set()
A:dask.graph_manipulation.map_layer->_build_map_layer(chunks.checkpoint, prev_name, map_name, collection)
A:dask.graph_manipulation.indices->tuple((i for (i, _) in enumerate(numblocks)))
A:dask.graph_manipulation.dep_keys->tuple((d.key for d in dependencies))
A:dask.graph_manipulation.(omit, _)->unpack_collections(omit)
A:dask.graph_manipulation.omit_keys->set()
A:dask.graph_manipulation.omit_layers->set()
A:dask.graph_manipulation.(unpacked_children, repack)->unpack_collections(children)
A:dask.graph_manipulation.prev_coll_names->get_collection_names(child)
A:dask.graph_manipulation.layers_to_clone->get_collection_names(child).copy()
A:dask.graph_manipulation.hlg_name->tokenize(*prev_coll_names)
A:dask.graph_manipulation.blocker_dsk->checkpoint(*collections, split_every=split_every).__dask_graph__()
A:dask.graph_manipulation.layers_to_copy_verbatim->set()
A:dask.graph_manipulation.prev_layer_name->get_collection_names(child).copy().pop()
A:dask.graph_manipulation.new_layer_name->clone_key(prev_layer_name, seed=seed)
A:dask.graph_manipulation.(new_layers[new_layer_name], is_bound)->_build_map_layer(chunks.bind, prev_name, new_name, coll, dependencies=(blocker,)).clone(keys=clone_keys, seed=seed, bind_to=blocker_key)
A:dask.graph_manipulation.layer_name->set().pop()
A:dask.graph_manipulation.(rebuild, args)->coll.__dask_postpersist__()
A:dask.graph_manipulation.out->repack([block_one(coll) for coll in unpacked])
A:dask.graph_manipulation.blocker->checkpoint(*collections, split_every=split_every)
A:dask.graph_manipulation.layer->_build_map_layer(chunks.bind, prev_name, new_name, coll, dependencies=(blocker,))
A:dask.graph_manipulation.(unpacked, repack)->unpack_collections(*collections)
dask.graph_manipulation._bind_one(child:T,blocker:Delayed|None,omit_layers:set[str],omit_keys:set[Key],seed:Hashable)->T
dask.graph_manipulation._build_map_layer(func:Callable,prev_name:str,new_name:str,collection,dependencies:tuple[Delayed,...]=())->Layer
dask.graph_manipulation._can_apply_blockwise(collection)->bool
dask.graph_manipulation._checkpoint_one(collection,split_every)->Delayed
dask.graph_manipulation.bind(children:T,parents,*,omit=None,seed:Hashable|None=None,assume_layers:bool=True,split_every:float|Literal[False]|None=None)->T
dask.graph_manipulation.checkpoint(*collections,split_every:float|Literal[False]|None=None)->Delayed
dask.graph_manipulation.chunks
dask.graph_manipulation.chunks.bind(node:T,*args,**kwargs)->T
dask.graph_manipulation.chunks.checkpoint(*args,**kwargs)->None
dask.graph_manipulation.clone(*collections,omit=None,seed:Hashable=None,assume_layers:bool=True)
dask.graph_manipulation.wait_on(*collections,split_every:float|Literal[False]|None=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/context.py----------------------------------------
A:dask.context.thread_state->threading.local()
dask.context.GlobalMethod(self,default,key,falsey=None)
dask.context.GlobalMethod.__get__(self,instance,owner=None)
dask.context.GlobalMethod.__init__(self,default,key,falsey=None)
dask.context.globalmethod(default=None,key=None,falsey=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/config.py----------------------------------------
A:dask.config.paths->_get_paths()
A:dask.config.PATH->os.path.join(os.path.expanduser('~'), '.config', 'dask')
A:dask.config.config_lock->threading.Lock()
A:dask.config.k->canonical_name(k, result)
A:dask.config.config->_load_config_file(path)
A:dask.config.d->d.setdefault(key, {}).setdefault(key, {})
A:dask.config.varname->name[5:].lower().replace('__', '.')
A:dask.config.d[varname]->ast.literal_eval(value)
A:dask.config.destination->os.path.join(directory, os.path.basename(source))
A:dask.config.lines->list(f)
A:dask.config.key->canonical_name(key, config=config)
A:dask.config.configs->collect_yaml(paths=paths)
A:dask.config.keys->canonical_name(key, config=config).split('.')
A:dask.config.current_defaults->merge(*defaults)
A:dask.config.value->pop(key, config=config)
A:dask.config.new->check_deprecations(key, deprecations)
A:dask.config.old->canonical_name(key, config=config).replace('_', '-')
A:dask.config.fn->os.path.join(os.path.dirname(__file__), 'dask.yaml')
A:dask.config._defaults->yaml.safe_load(f)
dask.config._get_paths()
dask.config._initialize()->None
dask.config._load_config_file(path:str)->dict | None
dask.config.canonical_name(k:str,config:dict)->str
dask.config.check_deprecations(key:str,deprecations:Mapping[str,str|None]=deprecations)->str
dask.config.collect(paths:list[str]=paths,env:Mapping[str,str]|None=None)->dict
dask.config.collect_env(env:Mapping[str,str]|None=None)->dict
dask.config.collect_yaml(paths:Sequence[str]=paths)->list[dict]
dask.config.deserialize(data:str)->Any
dask.config.ensure_file(source:str,destination:str|None=None,comment:bool=True)->None
dask.config.expand_environment_variables(config:Any)->Any
dask.config.get(key:str,default:Any=no_default,config:dict=config,override_with:Any=None)->Any
dask.config.merge(*dicts:Mapping)->dict
dask.config.pop(key:str,default:Any=no_default,config:dict=config)->Any
dask.config.refresh(config:dict=config,defaults:list[Mapping]=defaults,**kwargs)->None
dask.config.rename(deprecations:Mapping[str,str|None]=deprecations,config:dict=config)->None
dask.config.serialize(data:Any)->str
dask.config.set(self,arg:Mapping|None=None,config:dict=config,lock:threading.Lock=config_lock,**kwargs)
dask.config.set.__enter__(self)
dask.config.set.__exit__(self,type,value,traceback)
dask.config.set.__init__(self,arg:Mapping|None=None,config:dict=config,lock:threading.Lock=config_lock,**kwargs)
dask.config.set._assign(self,keys:Sequence[str],value:Any,d:dict,path:tuple[str,...]=(),record:bool=True)->None
dask.config.update(old:dict,new:Mapping,priority:Literal['old','new','new-defaults']='new',defaults:Mapping|None=None)->dict
dask.config.update_defaults(new:Mapping,config:dict=config,defaults:list[Mapping]=defaults)->None


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/hashing.py----------------------------------------
A:dask.hashing.h->hash_buffer(buf, hasher)
A:dask.hashing.s->binascii.b2a_hex(h)
dask.hashing._hash_sha1(buf)
dask.hashing.hash_buffer(buf,hasher=None)
dask.hashing.hash_buffer_hex(buf,hasher=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/callbacks.py----------------------------------------
A:dask.callbacks.self._cm->add_callbacks(self)
dask.callbacks.Callback(self,start=None,start_state=None,pretask=None,posttask=None,finish=None)
dask.callbacks.Callback.__enter__(self)
dask.callbacks.Callback.__exit__(self,*args)
dask.callbacks.Callback.__init__(self,start=None,start_state=None,pretask=None,posttask=None,finish=None)
dask.callbacks.Callback._callback(self)->tuple[Callable | None, ...]
dask.callbacks.Callback.register(self)->None
dask.callbacks.Callback.unregister(self)->None
dask.callbacks.add_callbacks(self,*callbacks)
dask.callbacks.add_callbacks.__enter__(self)
dask.callbacks.add_callbacks.__exit__(self,type,value,traceback)
dask.callbacks.add_callbacks.__init__(self,*callbacks)
dask.callbacks.local_callbacks(callbacks=None)
dask.callbacks.normalize_callback(cb)
dask.callbacks.unpack_callbacks(cbs)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dot.py----------------------------------------
A:dask.dot.head->funcname(func)
A:dask.dot._HASHPAT->re.compile('([0-9a-z]{32})')
A:dask.dot._UUIDPAT->re.compile('([0-9a-z]{8}-[0-9a-z]{4}-[0-9a-z]{4}-[0-9a-z]{4}-[0-9a-z]{12})')
A:dask.dot.s->s.replace(h, label).replace(h, label)
A:dask.dot.m->re.search(pattern, s)
A:dask.dot.n->cache.get(h, len(cache))
A:dask.dot.graphviz->import_required('graphviz', 'Drawing dask graphs with the graphviz engine requires the `graphviz` python library and the `graphviz` system library.\n\nPlease either conda or pip install as follows:\n\n  conda install python-graphviz     # either conda install\n  python -m pip install graphviz    # or pip install and follow installation instructions')
A:dask.dot.g->import_required('ipycytoscape', 'Drawing dask graphs with the cytoscape engine requires the `ipycytoscape` python library.\n\nPlease either conda or pip install as follows:\n\n  conda install ipycytoscape            # either conda install\n  python -m pip install ipycytoscape    # or pip install').CytoscapeWidget(layout={'height': '400px'})
A:dask.dot.seen->set()
A:dask.dot.connected->set()
A:dask.dot.k_name->name(k)
A:dask.dot.attrs->data_attributes.get(k, {}).copy()
A:dask.dot.dep_name->name(dep)
A:dask.dot.v_name->name(v)
A:dask.dot.IPYTHON_IMAGE_FORMATS->frozenset(['jpeg', 'png'])
A:dask.dot.IPYTHON_NO_DISPLAY_FORMATS->frozenset(['dot', 'pdf'])
A:dask.dot.(filename, format)->os.path.splitext(filename)
A:dask.dot.format->format[1:].lower()
A:dask.dot.data->_to_cytoscape_json(dsk, **kwargs)
A:dask.dot.display_cls->_get_display_cls(format)
A:dask.dot.full_filename->'.'.join([filename, format])
A:dask.dot.ipycytoscape->import_required('ipycytoscape', 'Drawing dask graphs with the cytoscape engine requires the `ipycytoscape` python library.\n\nPlease either conda or pip install as follows:\n\n  conda install ipycytoscape            # either conda install\n  python -m pip install ipycytoscape    # or pip install')
dask.dot._get_display_cls(format)
dask.dot._to_cytoscape_json(dsk,data_attributes=None,function_attributes=None,collapse_outputs=False,verbose=False,**kwargs)
dask.dot.box_label(key,verbose=False)
dask.dot.cytoscape_graph(dsk,filename:str|None='mydask',format:str|None=None,*,rankdir:str='BT',node_sep:float=10,edge_sep:float=10,spacing_factor:float=1,node_style:dict[str,str]|None=None,edge_style:dict[str,str]|None=None,**kwargs)
dask.dot.dot_graph(dsk,filename='mydask',format=None,**kwargs)
dask.dot.graphviz_to_file(g,filename,format)
dask.dot.has_sub_tasks(task)
dask.dot.label(x,cache=None)
dask.dot.name(x)
dask.dot.task_label(task)
dask.dot.to_graphviz(dsk,data_attributes=None,function_attributes=None,rankdir='BT',graph_attr=None,node_attr=None,edge_attr=None,collapse_outputs=False,verbose=False,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/distributed.py----------------------------------------
dask.distributed.__getattr__(value)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/ml.py----------------------------------------
dask.ml.__getattr__(value)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/backends.py----------------------------------------
A:dask.backends.BackendFuncParams->ParamSpec('BackendFuncParams')
A:dask.backends.BackendFuncReturn->TypeVar('BackendFuncReturn')
A:dask.backends.BackendEntrypointType->TypeVar('BackendEntrypointType', bound='DaskBackendEntrypoint')
A:dask.backends.entrypoints->detect_entrypoints(f'dask.{self._module_name}.backends')
A:dask.backends.dispatcher->self.dispatch(backend)
A:dask.backends.func->getattr(self, dispatch_name)
A:dask.backends.backend->self.dispatch(self.backend)
dask.backends.CreationDispatch(self,module_name:str,default:str,entrypoint_class:type[BackendEntrypointType],name:str|None=None)
dask.backends.CreationDispatch.__getattr__(self,item:str)
dask.backends.CreationDispatch.__init__(self,module_name:str,default:str,entrypoint_class:type[BackendEntrypointType],name:str|None=None)
dask.backends.CreationDispatch.backend(self)->str
dask.backends.CreationDispatch.backend(self,value:str)
dask.backends.CreationDispatch.dispatch(self,backend:str)
dask.backends.CreationDispatch.register_backend(self,name:str,backend:BackendEntrypointType)->BackendEntrypointType
dask.backends.CreationDispatch.register_inplace(self,backend:str,name:str|None=None)->Callable[[Callable[BackendFuncParams, BackendFuncReturn]], Callable[BackendFuncParams, BackendFuncReturn]]
dask.backends.DaskBackendEntrypoint
dask.backends.DaskBackendEntrypoint.to_backend(data)
dask.backends.DaskBackendEntrypoint.to_backend_dispatch(cls)
dask.backends.detect_entrypoints(entry_point_name)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/sizeof.py----------------------------------------
A:dask.sizeof.sizeof->Dispatch(name='sizeof')
A:dask.sizeof.logger->logging.getLogger(__name__)
A:dask.sizeof.num_items->len(seq)
A:dask.sizeof.samples->random.sample(seq, num_samples)
A:dask.sizeof.ncells->sum((len(x) for x in xs))
A:dask.sizeof.sample->numpy.random.choice(x, size=100, replace=True)
A:dask.sizeof.sample_nbytes->sum((sizeof(i) for i in unique_samples.values()))
A:dask.sizeof.p->sizeof(table.schema.metadata)
A:dask.sizeof.registrar->entry_point.load()
dask.sizeof.SimpleSizeof
dask.sizeof._register_entry_point_plugins()
dask.sizeof.register_cupy()
dask.sizeof.register_numba()
dask.sizeof.register_numpy()
dask.sizeof.register_pandas()
dask.sizeof.register_pyarrow()
dask.sizeof.register_rmm()
dask.sizeof.register_spmatrix()
dask.sizeof.sizeof_array(o)
dask.sizeof.sizeof_blocked(d)
dask.sizeof.sizeof_bytes(o)
dask.sizeof.sizeof_default(o)
dask.sizeof.sizeof_memoryview(o)
dask.sizeof.sizeof_python_collection(seq)
dask.sizeof.sizeof_python_dict(d)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/base.py----------------------------------------
A:dask.base.annotations['workers']->list(annotations['workers'])
A:dask.base.token->_annotations.set(merge(_annotations.get(), annotations))
A:dask.base.(result,)->compute(self, traverse=False, **kwargs)
A:dask.base.schedule->get_scheduler(scheduler=scheduler, collections=collections)
A:dask.base.dsk2->optimization_function(cls)(dsk, keys, **kwargs)
A:dask.base.groups->groupby(optimization_function, collections)
A:dask.base.(dsk, keys)->_extract_graph_and_keys(val)
A:dask.base.dsk->collections_to_dsk(collections, optimize_graph, **kwargs)
A:dask.base.(dsk, _)->_extract_graph_and_keys(collections)
A:dask.base.graph->merge(*map(ensure_dict, graphs))
A:dask.base.tok->tokenize(expr)
A:dask.base.tsk->quote(expr)
A:dask.base.dsk[collections_token]->quote(results)
A:dask.base.(collections, repack)->unpack_collections(*args, traverse=traverse)
A:dask.base.(r, s)->a.__dask_postpersist__()
A:dask.base.results->schedule(dsk, keys, **kwargs)
A:dask.base.(args, _)->unpack_collections(*args, traverse=traverse)
A:dask.base.color->kwargs.get('color')
A:dask.base.o_stats->order(dsk, return_stats=True)
A:dask.base.cmap->getattr(plt.cm, cmap)
A:dask.base.maxval->max(1, max(values.values()))
A:dask.base.client->default_client()
A:dask.base.a_keys->list(flatten(a.__dask_keys__()))
A:dask.base.(rebuild, state)->a.__dask_postpersist__()
A:dask.base.d->dict(zip(keys, results))
A:dask.base.hasher->_md5(str(tuple(map(normalize_token, args))).encode())
A:dask.base.normalize_token->Dispatch()
A:dask.base.method->getattr(o, '__dask_tokenize__', None)
A:dask.base.function_cache_lock->threading.Lock()
A:dask.base.result->pickle.dumps(func, protocol=4)
A:dask.base.first->getattr(func, 'first', None)
A:dask.base.args->tuple((normalize_token(i) for i in func.args))
A:dask.base.kws->tuple(((k, normalize_token(v)) for (k, v) in sorted(func.keywords.items())))
A:dask.base.data->hash_buffer_hex(x.copy().ravel(order='K').view('i1'))
A:dask.base.i->sum((v * 256 ** (len(t) - i - 1) for (i, v) in enumerate(t)))
A:dask.base.h->hex(int(i))[2:].upper()
A:dask.base.get_err_msg->"\nThe get= keyword has been removed.\n\nPlease use the scheduler= keyword instead with the name of\nthe desired scheduler like 'threads' or 'processes'\n\n    x.compute(scheduler='single-threaded')\n    x.compute(scheduler='threads')\n    x.compute(scheduler='processes')\n\nor with a function that takes the graph and keys\n\n    x.compute(scheduler=my_scheduler_function)\n\nor with a Dask client\n\n    x.compute(scheduler=client)\n".strip()
A:dask.base.scheduler->scheduler.lower().lower()
A:dask.base.num_workers->dask.config.get('num_workers', CPU_COUNT)
A:dask.base.KeyOrStrT->TypeVar('KeyOrStrT', Key, str)
A:dask.base.prefix->key_split(key)
dask.annotate(**annotations:Any)->Iterator[None]
dask.base.DaskMethodsMixin
dask.base.DaskMethodsMixin.__await__(self)
dask.base.DaskMethodsMixin.compute(self,**kwargs)
dask.base.DaskMethodsMixin.persist(self,**kwargs)
dask.base.DaskMethodsMixin.visualize(self,filename='mydask',format=None,optimize_graph=False,**kwargs)
dask.base._HashFactory(self,string:ReadableBuffer=b'',*,usedforsecurity:bool=True)
dask.base._HashFactory.__call__(self,string:ReadableBuffer=b'',*,usedforsecurity:bool=True)
dask.base._colorize(t)
dask.base._distributed_available()->bool
dask.base._extract_graph_and_keys(vals)
dask.base._md5(x:ReadableBuffer,_hashlib_md5:_HashFactory=hashlib.md5)->hashlib._Hash
dask.base._normalize_function(func:Callable)->tuple | str | bytes
dask.base._normalize_seq_func(seq)
dask.base.annotate(**annotations:Any)->Iterator[None]
dask.base.clone_key(key:KeyOrStrT,seed:Hashable)->KeyOrStrT
dask.base.collections_to_dsk(collections,optimize_graph=True,optimizations=(),**kwargs)
dask.base.compute(*args,traverse=True,optimize_graph=True,scheduler=None,get=None,**kwargs)
dask.base.compute_as_if_collection(cls,dsk,keys,scheduler=None,get=None,**kwargs)
dask.base.dont_optimize(dsk,keys,**kwargs)
dask.base.get_annotations()->dict[str, Any]
dask.base.get_collection_names(collection)->set[str]
dask.base.get_name_from_key(key:Key)->str
dask.base.get_scheduler(get=None,scheduler=None,collections=None,cls=None)
dask.base.is_dask_collection(x)->bool
dask.base.normalize_dataclass(obj)
dask.base.normalize_dict(d)
dask.base.normalize_enum(e)
dask.base.normalize_function(func:Callable)->Callable | tuple | str | bytes
dask.base.normalize_literal(lit)
dask.base.normalize_object(o)
dask.base.normalize_ordered_dict(d)
dask.base.normalize_range(r)
dask.base.normalize_seq(seq)
dask.base.normalize_set(s)
dask.base.optimization_function(x)
dask.base.optimize(*args,traverse=True,**kwargs)
dask.base.persist(*args,traverse=True,optimize_graph=True,scheduler=None,**kwargs)
dask.base.register_numpy()
dask.base.register_pandas()
dask.base.register_scipy()
dask.base.replace_name_in_key(key:KeyOrStrT,rename:Mapping[str,str])->KeyOrStrT
dask.base.tokenize(*args,**kwargs)
dask.base.unpack_collections(*args,traverse=True)
dask.base.visualize(*args,filename='mydask',traverse=True,optimize_graph=False,maxval=None,engine:Literal['cytoscape','ipycytoscape','graphviz']|None=None,**kwargs)
dask.base.visualize_dsk(dsk,filename='mydask',traverse=True,optimize_graph=False,maxval=None,o=None,engine:Literal['cytoscape','ipycytoscape','graphviz']|None=None,limit=None,**kwargs)
dask.base.wait(x,timeout=None,return_when='ALL_COMPLETED')
dask.compute(*args,traverse=True,optimize_graph=True,scheduler=None,get=None,**kwargs)
dask.compute_as_if_collection(cls,dsk,keys,scheduler=None,get=None,**kwargs)
dask.get_annotations()->dict[str, Any]
dask.is_dask_collection(x)->bool
dask.optimize(*args,traverse=True,**kwargs)
dask.persist(*args,traverse=True,optimize_graph=True,scheduler=None,**kwargs)
dask.visualize(*args,filename='mydask',traverse=True,optimize_graph=False,maxval=None,engine:Literal['cytoscape','ipycytoscape','graphviz']|None=None,**kwargs)
dask.visualize_dsk(dsk,filename='mydask',traverse=True,optimize_graph=False,maxval=None,o=None,engine:Literal['cytoscape','ipycytoscape','graphviz']|None=None,limit=None,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/rewrite.py----------------------------------------
A:dask.rewrite.self._stack->deque([END])
A:dask.rewrite.subterms->args(self.term)
A:dask.rewrite.self.term->self._stack.pop()
A:dask.rewrite.VAR->Token('?')
A:dask.rewrite.END->Token('end')
A:dask.rewrite.self.vars->tuple(sorted(set(self._varlist)))
A:dask.rewrite.term->rule.subs(sd)
A:dask.rewrite.self._net->Node()
A:dask.rewrite.ind->len(self.rules)
A:dask.rewrite.curr_node.edges[t]->Node()
A:dask.rewrite.S->Traverser(term)
A:dask.rewrite.subs->_process_match(rule, syms)
A:dask.rewrite.stack->deque()
A:dask.rewrite.n->N.edges.get(VAR, None)
A:dask.rewrite.(S, N, matches)->deque().pop()
dask.rewrite.Node(cls,edges=None,patterns=None)
dask.rewrite.Node.__new__(cls,edges=None,patterns=None)
dask.rewrite.Node.edges(self)
dask.rewrite.Node.patterns(self)
dask.rewrite.RewriteRule(self,lhs,rhs,vars=())
dask.rewrite.RewriteRule.__init__(self,lhs,rhs,vars=())
dask.rewrite.RewriteRule.__repr__(self)
dask.rewrite.RewriteRule.__str__(self)
dask.rewrite.RewriteRule._apply(self,sub_dict)
dask.rewrite.RuleSet(self,*rules)
dask.rewrite.RuleSet.__init__(self,*rules)
dask.rewrite.RuleSet._rewrite(self,term)
dask.rewrite.RuleSet.add(self,rule)
dask.rewrite.RuleSet.iter_matches(self,term)
dask.rewrite.RuleSet.rewrite(self,task,strategy='bottom_up')
dask.rewrite.Token(self,name)
dask.rewrite.Token.__init__(self,name)
dask.rewrite.Token.__repr__(self)
dask.rewrite.Traverser(self,term,stack=None)
dask.rewrite.Traverser.__init__(self,term,stack=None)
dask.rewrite.Traverser.__iter__(self)
dask.rewrite.Traverser.copy(self)
dask.rewrite.Traverser.current(self)
dask.rewrite.Traverser.next(self)
dask.rewrite.Traverser.skip(self)
dask.rewrite._bottom_up(net,term)
dask.rewrite._match(S,N)
dask.rewrite._process_match(rule,syms)
dask.rewrite._top_level(net,term)
dask.rewrite.args(task)
dask.rewrite.head(task)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/cache.py----------------------------------------
A:dask.cache.cache->cachey.Cache(cache, *args, **kwargs)
A:dask.cache.self.starttimes->dict()
A:dask.cache.self.durations->dict()
A:dask.cache.self.starttimes[key]->default_timer()
dask.cache.Cache(self,cache,*args,**kwargs)
dask.cache.Cache.__init__(self,cache,*args,**kwargs)
dask.cache.Cache._finish(self,dsk,state,errored)
dask.cache.Cache._posttask(self,key,value,dsk,state,id)
dask.cache.Cache._pretask(self,key,dsk,state)
dask.cache.Cache._start(self,dsk)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/typing.py----------------------------------------
A:dask.typing.CollType->TypeVar('CollType', bound='DaskCollection')
A:dask.typing.CollType_co->TypeVar('CollType_co', bound='DaskCollection', covariant=True)
dask.typing.DaskCollection(Protocol)
dask.typing.DaskCollection.__dask_graph__(self)->Graph
dask.typing.DaskCollection.__dask_keys__(self)->NestedKeys
dask.typing.DaskCollection.__dask_postcompute__(self)->tuple[PostComputeCallable, tuple]
dask.typing.DaskCollection.__dask_postpersist__(self)->tuple[PostPersistCallable, tuple]
dask.typing.DaskCollection.__dask_tokenize__(self)->Hashable
dask.typing.DaskCollection.compute(self,**kwargs:Any)->Any
dask.typing.DaskCollection.persist(self:CollType,**kwargs:Any)->CollType
dask.typing.DaskCollection.visualize(self,filename:str='mydask',format:str|None=None,optimize_graph:bool=False,**kwargs:Any)->DisplayObject | None
dask.typing.HLGDaskCollection(DaskCollection,Protocol)
dask.typing.HLGDaskCollection.__dask_layers__(self)->Sequence[str]
dask.typing.PostPersistCallable(self,dsk:Graph,*args:Any,rename:Mapping[str,str]|None=None)
dask.typing.PostPersistCallable.__call__(self,dsk:Graph,*args:Any,rename:Mapping[str,str]|None=None)
dask.typing.SchedulerGetCallable(self,dsk:Graph,keys:Sequence[Key]|Key,**kwargs:Any)
dask.typing.SchedulerGetCallable.__call__(self,dsk:Graph,keys:Sequence[Key]|Key,**kwargs:Any)
dask.typing._NoDefault(Enum)
dask.typing._NoDefault.__repr__(self)->str


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/delayed.py----------------------------------------
A:dask.delayed.DEFAULT_GET->dask.base.named_schedulers.get('threads', named_schedulers['sync'])
A:dask.delayed.out->list(zip(*ls))
A:dask.delayed.keys->set(expr).__dask_keys__()
A:dask.delayed.(finalize, args)->set(expr).__dask_postcompute__()
A:dask.delayed.graph->dask.highlevelgraph.HighLevelGraph.from_collections(name, {name: task}, dependencies=collections)
A:dask.delayed.finalized->finalize(expr)
A:dask.delayed.expr->set(expr)
A:dask.delayed.typ->type(expr)
A:dask.delayed.(args, collections)->unpack_collections([v for v in expr])
A:dask.delayed.args->list(args)
A:dask.delayed.collections->list(concat(collections))
A:dask.delayed.opt->getattr(expr, '__dask_optimize__', dont_optimize)
A:dask.delayed.(args, dasks)->unzip((to_task_dask(e) for e in expr), 2)
A:dask.delayed.dsk->dsk.cull(set(flatten(keys))).cull(set(flatten(keys)))
A:dask.delayed.(args, dsk)->to_task_dask([expr.start, expr.stop, expr.step])
A:dask.delayed.pure->kwargs.pop('pure', pure)
A:dask.delayed.(task, collections)->unpack_collections(obj)
A:dask.delayed.task->quote(obj)
A:dask.delayed.token->tokenize(obj, nout, pure=pure)
A:dask.delayed.__dask_scheduler__->staticmethod(DEFAULT_GET)
A:dask.delayed.__dask_optimize__->globalmethod(optimize, key='delayed_optimize')
A:dask.delayed.layer->next(iter(dsk.layers))
A:dask.delayed.func->delayed(apply, pure=pure)
A:dask.delayed.method->delayed(right(op) if inv else op, pure=True)
A:dask.delayed.dask_key_name->kwargs.pop('dask_key_name', None)
A:dask.delayed.name->'{}-{}'.format(funcname(func), tokenize(func_token, *args, pure=pure, **kwargs))
A:dask.delayed.(args2, collections)->unzip(map(unpack_collections, args), 2)
A:dask.delayed.(dask_kwargs, collections2)->unpack_collections(kwargs)
dask.delayed(obj,name=None,pure=None,nout=None,traverse=True)
dask.delayed.Delayed(self,key,dsk,length=None,layer=None)
dask.delayed.Delayed.__bool__(self)
dask.delayed.Delayed.__dask_graph__(self)->Graph
dask.delayed.Delayed.__dask_keys__(self)->NestedKeys
dask.delayed.Delayed.__dask_layers__(self)->Sequence[str]
dask.delayed.Delayed.__dask_postcompute__(self)
dask.delayed.Delayed.__dask_postpersist__(self)
dask.delayed.Delayed.__dask_tokenize__(self)
dask.delayed.Delayed.__dir__(self)
dask.delayed.Delayed.__get__(self,instance,cls)
dask.delayed.Delayed.__getattr__(self,attr)
dask.delayed.Delayed.__hash__(self)
dask.delayed.Delayed.__init__(self,key,dsk,length=None,layer=None)
dask.delayed.Delayed.__iter__(self)
dask.delayed.Delayed.__len__(self)
dask.delayed.Delayed.__repr__(self)
dask.delayed.Delayed.__setattr__(self,attr,val)
dask.delayed.Delayed.__setitem__(self,index,val)
dask.delayed.Delayed._get_binary_operator(cls,op,inv=False)
dask.delayed.Delayed._rebuild(self,dsk,*,rename=None)
dask.delayed.Delayed.dask(self)
dask.delayed.Delayed.key(self)
dask.delayed.DelayedAttr(self,obj,attr)
dask.delayed.DelayedAttr.__getattr__(self,attr)
dask.delayed.DelayedAttr.__init__(self,obj,attr)
dask.delayed.DelayedAttr.dask(self)
dask.delayed.DelayedLeaf(self,obj,key,pure=None,nout=None)
dask.delayed.DelayedLeaf.__doc__(self)
dask.delayed.DelayedLeaf.__init__(self,obj,key,pure=None,nout=None)
dask.delayed.DelayedLeaf.__name__(self)
dask.delayed.DelayedLeaf.__wrapped__(self)
dask.delayed.DelayedLeaf.dask(self)
dask.delayed._swap(method,self,other)
dask.delayed.call_function(func,func_token,args,kwargs,pure=None,nout=None)
dask.delayed.delayed(obj,name=None,pure=None,nout=None,traverse=True)
dask.delayed.finalize(collection)
dask.delayed.optimize(dsk,keys,**kwargs)
dask.delayed.right(method)
dask.delayed.single_key(seq)
dask.delayed.to_task_dask(expr)
dask.delayed.tokenize(*args,pure=None,**kwargs)
dask.delayed.unpack_collections(expr)
dask.delayed.unzip(ls,nout)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/utils_test.py----------------------------------------
A:dask.utils_test.result->self.get(d, [['badkey'], 'y'])
A:dask.utils_test.graph->HighLevelGraph(layers, dependencies)
dask.utils_test.GetFunctionTestMixin
dask.utils_test.GetFunctionTestMixin.test_badkey(self)
dask.utils_test.GetFunctionTestMixin.test_data_not_in_dict_is_ok(self)
dask.utils_test.GetFunctionTestMixin.test_get(self)
dask.utils_test.GetFunctionTestMixin.test_get_stack_limit(self)
dask.utils_test.GetFunctionTestMixin.test_get_with_list(self)
dask.utils_test.GetFunctionTestMixin.test_get_with_list_top_level(self)
dask.utils_test.GetFunctionTestMixin.test_get_with_nested_list(self)
dask.utils_test.GetFunctionTestMixin.test_get_works_with_unhashables_in_values(self)
dask.utils_test.GetFunctionTestMixin.test_nested_badkey(self)
dask.utils_test.GetFunctionTestMixin.test_nested_tasks(self)
dask.utils_test.GetFunctionTestMixin.test_with_HighLevelGraph(self)
dask.utils_test._check_warning(condition:bool,category:type[Warning],message:str)
dask.utils_test.add(x,y)
dask.utils_test.dec(x)
dask.utils_test.hlg_layer(hlg:HighLevelGraph,prefix:str)->Layer
dask.utils_test.hlg_layer_topological(hlg:HighLevelGraph,i:int)->Layer
dask.utils_test.import_or_none(name)
dask.utils_test.inc(x)
dask.utils_test.slowadd(a,b,delay=0.1)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/cli.py----------------------------------------
A:dask.cli.data->reduce(lambda d, k: d[k], key.split('.'), dask.config.config)
A:dask.cli.command->entry_point.load()
dask.cli._register_command_ep(interface,entry_point)
dask.cli.cli()
dask.cli.config()
dask.cli.config_get(key=None)
dask.cli.config_list()
dask.cli.docs()
dask.cli.info()
dask.cli.run_cli()
dask.cli.versions()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/datasets.py----------------------------------------
A:dask.datasets.field->Field(seed=seed, **field)
A:dask.datasets.schema->Schema(schema=lambda : schema_description(field), **schema_kwargs)
A:dask.datasets.random_state->random.Random(seed)
dask.datasets._generate_mimesis(field,schema_description,records_per_partition,seed)
dask.datasets._make_mimesis(field,schema,npartitions,records_per_partition,seed=None)
dask.datasets.make_people(npartitions=10,records_per_partition=1000,seed=None,locale='en')
dask.datasets.timeseries(start='2000-01-01',end='2000-01-31',freq='1s',partition_freq='1d',dtypes=None,seed=None,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/overlap.py----------------------------------------
A:dask.array.overlap.depth->dict(zip(range(ndim), depth))
A:dask.array.overlap.token->tokenize(x, axes)
A:dask.array.overlap.graph->dask.highlevelgraph.HighLevelGraph.from_collections(name, graph, dependencies=[x])
A:dask.array.overlap.chunks->list(x.chunks)
A:dask.array.overlap.axes->coerce_depth(x.ndim, depth)
A:dask.array.overlap.boundary->dict(zip(range(ndim), boundary))
A:dask.array.overlap.bdy->dict(zip(range(ndim), boundary)).get(i, 'none')
A:dask.array.overlap.overlap->coerce_depth(x.ndim, depth).get(i, 0)
A:dask.array.overlap.ind->tuple((slice(front, back) for (front, back) in zip(trim_front, trim_back)))
A:dask.array.overlap.(l, r)->_remove_overlap_boundaries(l, r, axis, depth)
A:dask.array.overlap.l->l.rechunk(tuple(lchunks)).rechunk(tuple(lchunks))
A:dask.array.overlap.r->r.rechunk(tuple(rchunks)).rechunk(tuple(rchunks))
A:dask.array.overlap.c->full_like(x, value, shape=tuple(map(sum, chunks)), chunks=tuple(chunks), dtype=x.dtype)
A:dask.array.overlap.lchunks->list(l.chunks)
A:dask.array.overlap.rchunks->list(r.chunks)
A:dask.array.overlap.d->dict(zip(range(ndim), depth)).get(k, 0)
A:dask.array.overlap.this_kind->kind.get(i, 'none')
A:dask.array.overlap.x->x.rechunk(safe_chunks).rechunk(safe_chunks)
A:dask.array.overlap.depth2->coerce_depth(x.ndim, depth)
A:dask.array.overlap.boundary2->coerce_boundary(x.ndim, boundary)
A:dask.array.overlap.new_chunks->tuple((ensure_minimum_chunksize(size, c) for (size, c) in zip(depths, x.chunks)))
A:dask.array.overlap.x1->x.rechunk(safe_chunks).rechunk(safe_chunks).rechunk(new_chunks)
A:dask.array.overlap.original_chunks_too_small->any([min(c) < d for (d, c) in zip(depths, x.chunks)])
A:dask.array.overlap.x2->boundaries(x1, depth2, boundary2)
A:dask.array.overlap.x3->overlap_internal(x2, depth2)
A:dask.array.overlap.x4->dask.array.chunk.trim(x3, trim)
A:dask.array.overlap.empty_shape->list(x.shape)
A:dask.array.overlap.empty_chunks->list(x.chunks)
A:dask.array.overlap.empty->empty_like(getattr(x, '_meta', x), shape=empty_shape, chunks=empty_chunks, dtype=x.dtype)
A:dask.array.overlap.out_chunks->list(x.chunks)
A:dask.array.overlap.ax_chunks->list(out_chunks[k])
A:dask.array.overlap.out_chunks[k]->tuple(ax_chunks)
A:dask.array.overlap.trim->get(sig.index('trim'), args, trim)
A:dask.array.overlap.(_, args)->unify_chunks(*list(concat(zip(args, inds))), warn=False)
A:dask.array.overlap.drop_axis->kwargs.pop('drop_axis', None)
A:dask.array.overlap.ndim_out->max((a.ndim for a in args if isinstance(a, Array)))
A:dask.array.overlap.kept_axes->tuple((ax for ax in range(args[i].ndim) if ax not in drop_axis))
A:dask.array.overlap.depth[i]->int(depth[i])
A:dask.array.overlap.window_shape_array->numpy.array(window_shape)
A:dask.array.overlap.axis->normalize_axis_tuple(axis, x.ndim, allow_duplicate=True)
A:dask.array.overlap.safe_chunks->tuple((ensure_minimum_chunksize(d + 1, c) for (d, c) in zip(depths, x.chunks)))
dask.array.map_overlap(func,*args,depth=None,boundary=None,trim=True,align_arrays=True,allow_rechunk=True,**kwargs)
dask.array.overlap._overlap_internal_chunks(original_chunks,axes)
dask.array.overlap._remove_overlap_boundaries(l,r,axis,depth)
dask.array.overlap._trim(x,axes,boundary,block_info)
dask.array.overlap.add_dummy_padding(x,depth,boundary)
dask.array.overlap.boundaries(x,depth=None,kind=None)
dask.array.overlap.coerce_boundary(ndim,boundary)
dask.array.overlap.coerce_depth(ndim,depth)
dask.array.overlap.coerce_depth_type(ndim,depth)
dask.array.overlap.constant(x,axis,depth,value)
dask.array.overlap.ensure_minimum_chunksize(size,chunks)
dask.array.overlap.map_overlap(func,*args,depth=None,boundary=None,trim=True,align_arrays=True,allow_rechunk=True,**kwargs)
dask.array.overlap.nearest(x,axis,depth)
dask.array.overlap.overlap(x,depth,boundary,*,allow_rechunk=True)
dask.array.overlap.overlap_internal(x,axes)
dask.array.overlap.periodic(x,axis,depth)
dask.array.overlap.reflect(x,axis,depth)
dask.array.overlap.sliding_window_view(x,window_shape,axis=None)
dask.array.overlap.trim_internal(x,axes,boundary=None)
dask.array.overlap.trim_overlap(x,depth,boundary=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/chunk.py----------------------------------------
A:dask.array.chunk.r->a_callable(x, *args, axis=axis, **kwargs)
A:dask.array.chunk.axes->range(x.ndim)
A:dask.array.chunk.r_slice->tuple()
A:dask.array.chunk.argmin->keepdims_wrapper(np.argmin)
A:dask.array.chunk.nanargmin->keepdims_wrapper(np.nanargmin)
A:dask.array.chunk.argmax->keepdims_wrapper(np.argmax)
A:dask.array.chunk.nanargmax->keepdims_wrapper(np.nanargmax)
A:dask.array.chunk.ind->tuple((slice(0, -(d % axes[i])) if d % axes[i] else slice(None, None) for (i, d) in enumerate(x.shape)))
A:dask.array.chunk.newshape->tuple(concat([(x.shape[i] // axes[i], axes[i]) for i in range(x.ndim)]))
A:dask.array.chunk.a->numpy.concatenate([ai for (ai, _) in a_plus_idx], axis)
A:dask.array.chunk.a_plus_idx->list(flatten(a_plus_idx))
A:dask.array.chunk.idx->numpy.where(idx < 0, idx + sum(x_chunks), idx)
A:dask.array.chunk.idx2->numpy.argsort(a, axis=axis)
A:dask.array.chunk.(a, idx)->argtopk(a_plus_idx, k, axis, keepdims)
A:dask.array.chunk.res->arange_safe(start, stop, step, dtype, like=like)
A:dask.array.chunk.start->start.compute().compute()
A:dask.array.chunk.stop->stop.compute().compute()
A:dask.array.chunk.x->numpy.asfortranarray(x)
A:dask.array.chunk.idx_final->numpy.zeros_like(idx)
A:dask.array.chunk.idx_cum->numpy.cumsum(idx_filter)
A:dask.array.chunk.result->result.copy().copy()
dask.array.chunk.arange(start,stop,step,length,dtype,like=None)
dask.array.chunk.argtopk(a_plus_idx,k,axis,keepdims)
dask.array.chunk.argtopk_aggregate(a_plus_idx,k,axis,keepdims)
dask.array.chunk.argtopk_preprocess(a,idx)
dask.array.chunk.astype(x,astype_dtype=None,**kwargs)
dask.array.chunk.coarsen(reduction,x,axes,trim_excess=False,**kwargs)
dask.array.chunk.getitem(obj,index)
dask.array.chunk.keepdims_wrapper(a_callable)
dask.array.chunk.linspace(start,stop,num,endpoint=True,dtype=None)
dask.array.chunk.slice_with_int_dask_array(x,idx,offset,x_size,axis)
dask.array.chunk.slice_with_int_dask_array_aggregate(idx,chunk_outputs,x_chunks,axis)
dask.array.chunk.topk(a,k,axis,keepdims)
dask.array.chunk.topk_aggregate(a,k,axis,keepdims)
dask.array.chunk.trim(x,axes=None)
dask.array.chunk.view(x,dtype,order='C')


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/einsumfuncs.py----------------------------------------
A:dask.array.einsumfuncs.einsum_symbols_set->set(einsum_symbols)
A:dask.array.einsumfuncs.subscripts->'->'.join((inputs, outputs))
A:dask.array.einsumfuncs.ncontract_inds->len(contract_inds)
A:dask.array.einsumfuncs.dtype->numpy.result_type(*[o.dtype for o in ops])
A:dask.array.einsumfuncs.einsum->dask.array.core.einsum_lookup.dispatch(type(operands[0]))
A:dask.array.einsumfuncs.chunk->einsum(subscripts, *operands, dtype=dtype, **kwargs)
A:dask.array.einsumfuncs.tmp_operands->list(operands)
A:dask.array.einsumfuncs.used->'->'.join((inputs, outputs)).replace('.', '').replace(',', '').replace('->', '')
A:dask.array.einsumfuncs.unused->list(einsum_symbols_set - set(used))
A:dask.array.einsumfuncs.ellipse_inds->''.join(unused)
A:dask.array.einsumfuncs.(input_tmp, output_sub)->'->'.join((inputs, outputs)).split('->')
A:dask.array.einsumfuncs.split_subscripts->'->'.join((inputs, outputs)).split(',')
A:dask.array.einsumfuncs.ellipse_count->max(operands[num].ndim, 1)
A:dask.array.einsumfuncs.split_subscripts[num]->sub.replace('...', rep_inds)
A:dask.array.einsumfuncs.tmp_subscripts->'->'.join((inputs, outputs)).replace(',', '')
A:dask.array.einsumfuncs.normal_inds->''.join(sorted(set(output_subscript) - set(out_ellipse)))
A:dask.array.einsumfuncs.(input_subscripts, output_subscript)->'->'.join((inputs, outputs)).split('->')
A:dask.array.einsumfuncs.(inputs, outputs, ops)->parse_einsum_input(operands)
A:dask.array.einsumfuncs.(optimize, _)->numpy.einsum_path(subscripts, *fake_ops, optimize=optimize)
A:dask.array.einsumfuncs.result->blockwise(chunk_einsum, tuple(outputs) + tuple(contract_inds), *(a for ap in zip(ops, inputs) for a in ap), adjust_chunks={ind: 1 for ind in contract_inds}, dtype=dtype, subscripts=subscripts, kernel_dtype=einsum_dtype, ncontract_inds=ncontract_inds, optimize=optimize, **kwargs)
A:dask.array.einsumfuncs.size->len(outputs)
dask.array.einsumfuncs.chunk_einsum(*operands,**kwargs)
dask.array.einsumfuncs.einsum(*operands,dtype=None,optimize=False,split_every=None,**kwargs)
dask.array.einsumfuncs.parse_einsum_input(operands)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/image.py----------------------------------------
A:dask.array.image.filenames->sorted(glob(filename))
A:dask.array.image.sample->preprocess(sample)
A:dask.array.image.dsk->dict(zip(keys, values))
dask.array.image.add_leading_dimension(x)
dask.array.image.imread(filename,imread=None,preprocess=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/rechunk.py----------------------------------------
A:dask.array.rechunk.slc->slice(last_o_end, last_o_end)
A:dask.array.rechunk.cmos->cumdims_label(old_known, 'o')
A:dask.array.rechunk.cmns->cumdims_label(new_known, 'n')
A:dask.array.rechunk.sliced[i]->_intersect_1d(_breakpoints(cmo, cmn))
A:dask.array.rechunk.cross1->product(*old_to_new(old_chunks, new_chunks))
A:dask.array.rechunk.cross->chain((tuple(product(*cr)) for cr in cross1))
A:dask.array.rechunk.old_shapes->tuple(map(sum, old_chunks))
A:dask.array.rechunk.new_shapes->tuple(map(sum, new_chunks))
A:dask.array.rechunk.chunks->find_split_rechunk(current_chunks, new_chunks, graph_size * threshold)
A:dask.array.rechunk.steps->plan_rechunk(x.chunks, chunks, x.dtype.itemsize, threshold, block_size_limit)
A:dask.array.rechunk.x->_compute_rechunk(x, c)
A:dask.array.rechunk.crossed_size->reduce(mul, (len(oc) + len(nc) - 1 if oc != nc else len(oc) for (oc, nc) in zip(old_chunks, new_chunks)))
A:dask.array.rechunk.nb_divides->int(np.ceil(c / max_width))
A:dask.array.rechunk.distinct->set(desired_chunks)
A:dask.array.rechunk.w->set(desired_chunks).pop()
A:dask.array.rechunk.n->len(desired_chunks)
A:dask.array.rechunk.(width, i, j)->heapq.heappop(heap)
A:dask.array.rechunk.ndim->len(old_chunks)
A:dask.array.rechunk.sorted_candidates->sorted(merge_candidates, key=key)
A:dask.array.rechunk.largest_block_size->reduce(mul, old_largest_width)
A:dask.array.rechunk.chunk_limit->int(block_size_limit * largest_width / largest_block_size)
A:dask.array.rechunk.c->merge_to_number(new_chunks[dim], max_number)
A:dask.array.rechunk.graph_size->estimate_graph_size(current_chunks, new_chunks)
A:dask.array.rechunk.max_number->int(len(old_chunks[dim]) * graph_size_limit / graph_size)
A:dask.array.rechunk.block_size_limit->max([block_size_limit, largest_old_block, largest_new_block])
A:dask.array.rechunk.largest_old_block->_largest_block_size(old_chunks)
A:dask.array.rechunk.largest_new_block->_largest_block_size(new_chunks)
A:dask.array.rechunk.(chunks, memory_limit_hit)->find_merge_rechunk(chunks, new_chunks, block_size_limit)
A:dask.array.rechunk.crossed->intersect_chunks(x.chunks, chunks)
A:dask.array.rechunk.x2->dict()
A:dask.array.rechunk.intermediates->dict()
A:dask.array.rechunk.token->tokenize(x, chunks)
A:dask.array.rechunk.split_name_suffixes->count()
A:dask.array.rechunk.old_blocks->numpy.empty([len(c) for c in x.chunks], dtype='O')
A:dask.array.rechunk.new_index->product(*(range(len(c)) for c in chunks))
A:dask.array.rechunk.rec_cat_arg->numpy.empty(subdims1, dtype='O')
A:dask.array.rechunk.(old_block_index, slices)->zip(*ind_slices)
A:dask.array.rechunk.layer->tlz.merge(x2, intermediates)
A:dask.array.rechunk.graph->dask.highlevelgraph.HighLevelGraph.from_collections(merge_name, layer, dependencies=[x])
A:dask.array.rechunk.median_len->numpy.median(chunks).astype(int)
A:dask.array.rechunk.n_chunks->len(chunks)
A:dask.array.rechunk.best_chunk_size->numpy.argmin(diffs)
dask.array.rechunk(x,chunks='auto',threshold=None,block_size_limit=None,balance=False,method=None)
dask.array.rechunk._PrettyBlocks(self,blocks)
dask.array.rechunk._PrettyBlocks.__init__(self,blocks)
dask.array.rechunk._PrettyBlocks.__str__(self)
dask.array.rechunk._balance_chunksizes(chunks:tuple[int,...])->tuple[int, ...]
dask.array.rechunk._breakpoints(cumold,cumnew)
dask.array.rechunk._compute_rechunk(x,chunks)
dask.array.rechunk._get_chunks(n,chunksize)
dask.array.rechunk._intersect_1d(breaks)
dask.array.rechunk._largest_block_size(chunks)
dask.array.rechunk._number_of_blocks(chunks)
dask.array.rechunk._validate_rechunk(old_chunks,new_chunks)
dask.array.rechunk.cumdims_label(chunks,const)
dask.array.rechunk.divide_to_width(desired_chunks,max_width)
dask.array.rechunk.estimate_graph_size(old_chunks,new_chunks)
dask.array.rechunk.find_merge_rechunk(old_chunks,new_chunks,block_size_limit)
dask.array.rechunk.find_split_rechunk(old_chunks,new_chunks,graph_size_limit)
dask.array.rechunk.format_blocks(blocks)
dask.array.rechunk.format_chunks(chunks)
dask.array.rechunk.format_plan(plan)
dask.array.rechunk.intersect_chunks(old_chunks,new_chunks)
dask.array.rechunk.merge_to_number(desired_chunks,max_number)
dask.array.rechunk.old_to_new(old_chunks,new_chunks)
dask.array.rechunk.plan_rechunk(old_chunks,new_chunks,itemsize,threshold=None,block_size_limit=None)
dask.array.rechunk.rechunk(x,chunks='auto',threshold=None,block_size_limit=None,balance=False,method=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/blockwise.py----------------------------------------
A:dask.array.blockwise.(chunkss, arrays)->unify_chunks(*args)
A:dask.array.blockwise.arginds->zip(arrays, args[1::2])
A:dask.array.blockwise.arg->normalize_arg(arg)
A:dask.array.blockwise.(arg, collections)->unpack_collections(arg)
A:dask.array.blockwise.v->normalize_arg(v)
A:dask.array.blockwise.(v, collections)->unpack_collections(v)
A:dask.array.blockwise.out->'{}-{}'.format(token or utils.funcname(func).strip('_'), base.tokenize(func, out_ind, argindsstr, dtype, **kwargs))
A:dask.array.blockwise.graph->dask.highlevelgraph.HighLevelGraph.from_collections(out, graph, dependencies=arrays + dependencies)
A:dask.array.blockwise.chunks[i]->tuple(adjust_chunks[ind])
A:dask.array.blockwise.chunks->tuple(chunks)
A:dask.array.blockwise.meta->compute_meta(func, dtype, *args[::2], **kwargs)
dask.array.atop(*args,**kwargs)
dask.array.blockwise(func,out_ind,*args,name=None,token=None,dtype=None,adjust_chunks=None,new_axes=None,align_arrays=True,concatenate=None,meta=None,**kwargs)
dask.array.blockwise.atop(*args,**kwargs)
dask.array.blockwise.blockwise(func,out_ind,*args,name=None,token=None,dtype=None,adjust_chunks=None,new_axes=None,align_arrays=True,concatenate=None,meta=None,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/stats.py----------------------------------------
A:dask.array.stats.F_onewayResult->namedtuple('F_onewayResult', ('statistic', 'pvalue'))
A:dask.array.stats.KurtosistestResult->namedtuple('KurtosistestResult', ('statistic', 'pvalue'))
A:dask.array.stats.NormaltestResult->namedtuple('NormaltestResult', ('statistic', 'pvalue'))
A:dask.array.stats.Power_divergenceResult->namedtuple('Power_divergenceResult', ('statistic', 'pvalue'))
A:dask.array.stats.SkewtestResult->namedtuple('SkewtestResult', ('statistic', 'pvalue'))
A:dask.array.stats.Ttest_1sampResult->namedtuple('Ttest_1sampResult', ('statistic', 'pvalue'))
A:dask.array.stats.Ttest_indResult->namedtuple('Ttest_indResult', ('statistic', 'pvalue'))
A:dask.array.stats.Ttest_relResult->namedtuple('Ttest_relResult', ('statistic', 'pvalue'))
A:dask.array.stats.v1->dask.array.var(a, axis, ddof=1)
A:dask.array.stats.v2->dask.array.var(b, axis, ddof=1)
A:dask.array.stats.(df, denom)->_unequal_var_ttest_denom(v1, n1, v2, n2)
A:dask.array.stats.res->_ttest_ind_from_stats(da.mean(a, axis), da.mean(b, axis), denom, df)
A:dask.array.stats.v->dask.array.var(d, axis, ddof=1)
A:dask.array.stats.denom->dask.array.sqrt(vn1 + vn2)
A:dask.array.stats.t->dask.array.divide(d, denom)
A:dask.array.stats.(t, prob)->_ttest_finish(df, t)
A:dask.array.stats.df->dask.array.where(da.isnan(df), 1, df)
A:dask.array.stats.d->(a - b).astype(np.float64)
A:dask.array.stats.dm->dask.array.mean(d, axis)
A:dask.array.stats.f_exp->f_obs.mean(axis=axis, keepdims=True)
A:dask.array.stats.stat->terms.sum(axis=axis)
A:dask.array.stats.num_obs->_count(terms, axis=axis)
A:dask.array.stats.p->delayed(distributions.chi2.sf)(stat, num_obs - 1 - ddof)
A:dask.array.stats.m2->moment(a, 2, axis)
A:dask.array.stats.m3->moment(a, 3, axis)
A:dask.array.stats.vals->dask.array.where(zero, 0, m4 / m2 ** 2.0)
A:dask.array.stats.b2->kurtosis(a, axis, fisher=False)
A:dask.array.stats.n->float(a.shape[axis])
A:dask.array.stats.alpha->math.sqrt(2.0 / (W2 - 1))
A:dask.array.stats.y->numpy.where(y == 0, 1, y)
A:dask.array.stats.m4->moment(a, 4, axis)
A:dask.array.stats.olderr->numpy.seterr(all='ignore')
A:dask.array.stats.term2->numpy.where(denom < 0, term1, np.power((1 - 2.0 / A) / denom, 1 / 3.0))
A:dask.array.stats.Z->numpy.where(denom == 99, 0, Z)
A:dask.array.stats.(s, _)->skewtest(a, axis)
A:dask.array.stats.(k, _)->kurtosistest(a, axis)
A:dask.array.stats.num_groups->len(args)
A:dask.array.stats.alldata->dask.array.concatenate(args)
A:dask.array.stats.bign->len(alldata)
A:dask.array.stats.offset->dask.array.concatenate(args).mean()
A:dask.array.stats.prob->_fdtrc(dfbn, dfwn, f)
A:dask.array.stats._xlogy->wrap_elemwise(special.xlogy, source=special)
A:dask.array.stats._fdtrc->wrap_elemwise(special.fdtrc, source=special)
A:dask.array.stats.s->dask.array.sum(a, axis)
dask.array.stats._count(x,axis=None)
dask.array.stats._equal_var_ttest_denom(v1,n1,v2,n2)
dask.array.stats._square_of_sums(a,axis=0)
dask.array.stats._sum_of_squares(a,axis=0)
dask.array.stats._ttest_finish(df,t)
dask.array.stats._ttest_ind_from_stats(mean1,mean2,denom,df)
dask.array.stats._unequal_var_ttest_denom(v1,n1,v2,n2)
dask.array.stats.chisquare(f_obs,f_exp=None,ddof=0,axis=0)
dask.array.stats.f_oneway(*args)
dask.array.stats.kurtosis(a,axis=0,fisher=True,bias=True,nan_policy='propagate')
dask.array.stats.kurtosistest(a,axis=0,nan_policy='propagate')
dask.array.stats.moment(a,moment=1,axis=0,nan_policy='propagate')
dask.array.stats.normaltest(a,axis=0,nan_policy='propagate')
dask.array.stats.power_divergence(f_obs,f_exp=None,ddof=0,axis=0,lambda_=None)
dask.array.stats.skew(a,axis=0,bias=True,nan_policy='propagate')
dask.array.stats.skewtest(a,axis=0,nan_policy='propagate')
dask.array.stats.ttest_1samp(a,popmean,axis=0,nan_policy='propagate')
dask.array.stats.ttest_ind(a,b,axis=0,equal_var=True)
dask.array.stats.ttest_rel(a,b,axis=0,nan_policy='propagate')


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/creation.py----------------------------------------
A:dask.array.creation.backend_entrypoint->dask.array.backends.array_creation_dispatch.dispatch(backend)
A:dask.array.creation.a->asarray(a)
A:dask.array.creation.(shape, chunks)->_get_like_function_shapes_chunks(a, chunks, shape)
A:dask.array.creation.num->int(max(np.ceil((stop - start) / step), 0))
A:dask.array.creation.chunks->tuple(chunks)
A:dask.array.creation.sparse->bool(sparse)
A:dask.array.creation.s[i]->slice(None)
A:dask.array.creation.s->tuple(s)
A:dask.array.creation.grid->empty((len(dimensions),) + dimensions, dtype=dtype, chunks=(1,) + chunks)
A:dask.array.creation.dimensions->tuple(dimensions)
A:dask.array.creation.dtype->numpy.dtype(start.dtype)
A:dask.array.creation.(vchunks, hchunks)->normalize_chunks(chunks, shape=(N, M), dtype=dtype)
A:dask.array.creation.token->tokenize(N, chunks, M, k, dtype)
A:dask.array.creation.meta->meta_from_array(a, ndims_free + 1)
A:dask.array.creation.m->m.astype(dtype, copy=False).astype(dtype, copy=False)
A:dask.array.creation.kdiag_row_start->max(0, -k)
A:dask.array.creation.kdiag_row_stop->min(a.shape[axis1], a.shape[axis2] - k)
A:dask.array.creation.graph->dask.highlevelgraph.HighLevelGraph.from_collections(name, dsk, dependencies=[a])
A:dask.array.creation.blocks->v.__dask_keys__()
A:dask.array.creation.axis1->_axis_fmt(axis1, 'axis1', a.ndim)
A:dask.array.creation.axis2->_axis_fmt(axis2, 'axis2', a.ndim)
A:dask.array.creation.free_indices->list(product(*(range(a.numblocks[i]) for i in free_axes)))
A:dask.array.creation.ndims_free->len(free_axes)
A:dask.array.creation.kdiag_col_start->max(0, k)
A:dask.array.creation.dsk->dict()
A:dask.array.creation.shape->tuple(shape)
A:dask.array.creation.row_stops_->numpy.cumsum(a.chunks[axis1])
A:dask.array.creation.row_starts->numpy.roll(row_stops_, 1)
A:dask.array.creation.col_stops_->numpy.cumsum(a.chunks[axis2])
A:dask.array.creation.col_starts->numpy.roll(col_stops_, 1)
A:dask.array.creation.row_blockid->numpy.arange(a.numblocks[axis1])
A:dask.array.creation.col_blockid->numpy.arange(a.numblocks[axis2])
A:dask.array.creation.kdiag_row_end->min(nrows, ncols - k)
A:dask.array.creation.inds->tuple(range(len(shape)))
A:dask.array.creation.arrs->meshgrid(*arrs, indexing='ij')
A:dask.array.creation.args->sum(zip(arrs, itertools.repeat(inds)), ())
A:dask.array.creation.res->blockwise(func, inds, *args, token='fromfunction', **kwargs)
A:dask.array.creation.cchunks->cached_cumsum(a.chunks[axis], initial_zero=True)
A:dask.array.creation.ls->numpy.linspace(c_start, c_stop, repeats).round(0)
A:dask.array.creation.all_slice->slice(None, None, None)
A:dask.array.creation.result->result.rechunk(chunks).rechunk(chunks)
A:dask.array.creation.tup->tuple(reps)
A:dask.array.creation.c->asarray(A)
A:dask.array.creation.d->len(tup)
A:dask.array.creation.shape_out->tuple((s * t for (s, t) in zip(shape, tup)))
A:dask.array.creation.pad_value->tuple((tuple(pw) for pw in pad_value))
A:dask.array.creation.j->tuple(j)
A:dask.array.creation.j[dim]->slice(None)
A:dask.array.creation.(pad_shapes, pad_chunks)->get_pad_shapes_chunks(result, pad_width, (d,))
A:dask.array.creation.pad_slices[0][d]->slice(None, 1, None)
A:dask.array.creation.pad_slices[1][d]->slice(-1, None, None)
A:dask.array.creation.reflect_type->kwargs.get('reflect', 'even')
A:dask.array.creation.select->tuple(select)
A:dask.array.creation.orient->tuple(orient)
A:dask.array.creation.idx->tuple((2 - i for i in idx))
A:dask.array.creation.stat_length->kwargs.get('stat_length', tuple(((n, n) for n in array.shape)))
A:dask.array.creation.axes->tuple(axes)
A:dask.array.creation.pad_shape->tuple(pad_shape)
A:dask.array.creation.pad_chunks->tuple(pad_chunks)
A:dask.array.creation.result_idx->result_idx.astype(array.dtype).astype(array.dtype)
A:dask.array.creation.result[i]->pad_func(array[i], iaxis_pad_width, iaxis, pad_func_kwargs)
A:dask.array.creation.array->asarray(array)
A:dask.array.creation.pad_width->expand_pad_value(array, pad_width)
dask.array.arange(*args,chunks='auto',like=None,dtype=None,**kwargs)
dask.array.creation._get_like_function_shapes_chunks(a,chunks,shape)
dask.array.creation.arange(*args,chunks='auto',like=None,dtype=None,**kwargs)
dask.array.creation.diag(v,k=0)
dask.array.creation.diagonal(a,offset=0,axis1=0,axis2=1)
dask.array.creation.empty_like(a,dtype=None,order='C',chunks=None,name=None,shape=None)
dask.array.creation.expand_pad_value(array,pad_value)
dask.array.creation.eye(N,chunks='auto',M=None,k=0,dtype=float)
dask.array.creation.fromfunction(func,chunks='auto',shape=None,dtype=None,**kwargs)
dask.array.creation.full_like(a,fill_value,order='C',dtype=None,chunks=None,name=None,shape=None)
dask.array.creation.get_pad_shapes_chunks(array,pad_width,axes)
dask.array.creation.indices(dimensions,dtype=int,chunks='auto')
dask.array.creation.linear_ramp_chunk(start,stop,num,dim,step)
dask.array.creation.linspace(start,stop,num=50,endpoint=True,retstep=False,chunks='auto',dtype=None)
dask.array.creation.meshgrid(*xi,sparse=False,indexing='xy',**kwargs)
dask.array.creation.ones_like(a,dtype=None,order='C',chunks=None,name=None,shape=None)
dask.array.creation.pad(array,pad_width,mode='constant',**kwargs)
dask.array.creation.pad_edge(array,pad_width,mode,**kwargs)
dask.array.creation.pad_reuse(array,pad_width,mode,**kwargs)
dask.array.creation.pad_stats(array,pad_width,mode,stat_length)
dask.array.creation.pad_udf(array,pad_width,mode,**kwargs)
dask.array.creation.repeat(a,repeats,axis=None)
dask.array.creation.tile(A,reps)
dask.array.creation.to_backend(x:Array,backend:str|None=None,**kwargs)
dask.array.creation.tri(N,M=None,k=0,dtype=float,chunks='auto',*,like=None)
dask.array.creation.wrapped_pad_func(array,pad_func,iaxis_pad_width,iaxis,pad_func_kwargs)
dask.array.creation.zeros_like(a,dtype=None,order='C',chunks=None,name=None,shape=None)
dask.array.diag(v,k=0)
dask.array.diagonal(a,offset=0,axis1=0,axis2=1)
dask.array.empty_like(a,dtype=None,order='C',chunks=None,name=None,shape=None)
dask.array.eye(N,chunks='auto',M=None,k=0,dtype=float)
dask.array.fromfunction(func,chunks='auto',shape=None,dtype=None,**kwargs)
dask.array.full_like(a,fill_value,order='C',dtype=None,chunks=None,name=None,shape=None)
dask.array.indices(dimensions,dtype=int,chunks='auto')
dask.array.linspace(start,stop,num=50,endpoint=True,retstep=False,chunks='auto',dtype=None)
dask.array.meshgrid(*xi,sparse=False,indexing='xy',**kwargs)
dask.array.ones_like(a,dtype=None,order='C',chunks=None,name=None,shape=None)
dask.array.pad(array,pad_width,mode='constant',**kwargs)
dask.array.pad_edge(array,pad_width,mode,**kwargs)
dask.array.pad_reuse(array,pad_width,mode,**kwargs)
dask.array.pad_stats(array,pad_width,mode,stat_length)
dask.array.pad_udf(array,pad_width,mode,**kwargs)
dask.array.repeat(a,repeats,axis=None)
dask.array.tile(A,reps)
dask.array.tri(N,M=None,k=0,dtype=float,chunks='auto',*,like=None)
dask.array.zeros_like(a,dtype=None,order='C',chunks=None,name=None,shape=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/dispatch.py----------------------------------------
A:dask.array.dispatch.concatenate_lookup->Dispatch('concatenate')
A:dask.array.dispatch.tensordot_lookup->Dispatch('tensordot')
A:dask.array.dispatch.einsum_lookup->Dispatch('einsum')
A:dask.array.dispatch.empty_lookup->Dispatch('empty')
A:dask.array.dispatch.divide_lookup->Dispatch('divide')
A:dask.array.dispatch.percentile_lookup->Dispatch('percentile')
A:dask.array.dispatch.numel_lookup->Dispatch('numel')
A:dask.array.dispatch.nannumel_lookup->Dispatch('nannumel')
A:dask.array.dispatch.to_numpy_dispatch->Dispatch('to_numpy_dispatch')
A:dask.array.dispatch.to_cupy_dispatch->Dispatch('to_cupy_dispatch')


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/fft.py----------------------------------------
A:dask.array.fft.chunks->out_chunk_fn(a, s, axes)
A:dask.array.fft.s->list(s)
A:dask.array.fft.a->asarray(a)
A:dask.array.fft.axes->list(range(x.ndim))
A:dask.array.fft.sample->numpy.ones(a.ndim * (8,), dtype=a.dtype)
A:dask.array.fft.func_mod->inspect.getmodule(fft_func)
A:dask.array.fft.func.__doc__->skip_doctest(func.__doc__)
A:dask.array.fft.fft->fft_wrap(np.fft.fft)
A:dask.array.fft.fft2->fft_wrap(np.fft.fft2)
A:dask.array.fft.fftn->fft_wrap(np.fft.fftn)
A:dask.array.fft.ifft->fft_wrap(np.fft.ifft)
A:dask.array.fft.ifft2->fft_wrap(np.fft.ifft2)
A:dask.array.fft.ifftn->fft_wrap(np.fft.ifftn)
A:dask.array.fft.rfft->fft_wrap(np.fft.rfft)
A:dask.array.fft.rfft2->fft_wrap(np.fft.rfft2)
A:dask.array.fft.rfftn->fft_wrap(np.fft.rfftn)
A:dask.array.fft.irfft->fft_wrap(np.fft.irfft)
A:dask.array.fft.irfft2->fft_wrap(np.fft.irfft2)
A:dask.array.fft.irfftn->fft_wrap(np.fft.irfftn)
A:dask.array.fft.hfft->fft_wrap(np.fft.hfft)
A:dask.array.fft.ihfft->fft_wrap(np.fft.ihfft)
A:dask.array.fft.r->tuple(r)
A:dask.array.fft.n->int(n)
A:dask.array.fft.d->float(d)
A:dask.array.fft.l[i]->slice(None, n_2)
A:dask.array.fft.l->tuple(l)
A:dask.array.fft.r[i]->slice(n_2, None)
A:dask.array.fft.y->y.rechunk({i: x.chunks[i]}).rechunk({i: x.chunks[i]})
dask.array.fft._fft_out_chunks(a,s,axes)
dask.array.fft._fftfreq_block(i,n,d)
dask.array.fft._fftshift_helper(x,axes=None,inverse=False)
dask.array.fft._hfft_out_chunks(a,s,axes)
dask.array.fft._ihfft_out_chunks(a,s,axes)
dask.array.fft._irfft_out_chunks(a,s,axes)
dask.array.fft._rfft_out_chunks(a,s,axes)
dask.array.fft.fft_wrap(fft_func,kind=None,dtype=None)
dask.array.fft.fftfreq(n,d=1.0,chunks='auto')
dask.array.fft.fftshift(x,axes=None)
dask.array.fft.ifftshift(x,axes=None)
dask.array.fft.rfftfreq(n,d=1.0,chunks='auto')


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/core.py----------------------------------------
A:dask.array.core.DEFAULT_GET->dask.base.named_schedulers.get('threads', named_schedulers['sync'])
A:dask.array.core.b2->tuple((x for x in b if x is not None))
A:dask.array.core.b3->tuple((None if x is None else slice(None, None) for x in b if not isinstance(x, Integral)))
A:dask.array.core.c->super().__new__(cls).copy()
A:dask.array.core.chunks->tuple((tuple(np.array(c)[i].tolist()) for (c, i) in zip(self._array.chunks, index)))
A:dask.array.core.out_ind->tuple(out_ind)
A:dask.array.core.layer->dict(zip(keys, values))
A:dask.array.core.layers[original_name]->MaterializedLayer({original_name: arr})
A:dask.array.core.layers[name]->core_blockwise(getitem, name, out_ind, original_name, None, ArraySliceDep(chunks), out_ind, numblocks={}, **kwargs)
A:dask.array.core.A->TypeVar('A', bound=ArrayLike)
A:dask.array.core.B->map(rightfunc, B)
A:dask.array.core.arrays->concrete(arrays)
A:dask.array.core.concatenate->dask.array.dispatch.concatenate_lookup.dispatch(type(max(arrays, key=lambda x: getattr(x, '__array_priority__', 0))))
A:dask.array.core.keys->product(*(range(len(c)) for c in chunks))
A:dask.array.core.ret->dict()
A:dask.array.core.ret[k]->concatenate(list((a[k] for a in arrays)), axis=axes[0])
A:dask.array.core.o->func(*args, **kwargs)
A:dask.array.core.(exc_type, exc_value, exc_traceback)->sys.exc_info()
A:dask.array.core.tb->''.join(traceback.format_tb(exc_traceback))
A:dask.array.core.meta->numpy.stack([meta_from_array(a) for a in seq], axis=axis)
A:dask.array.core.dtype->kwargs.pop('enforce_dtype')
A:dask.array.core.ndim_out->len(out_ind)
A:dask.array.core.new_axis->list(range(self.ndim, self.ndim + len(dt.shape)))
A:dask.array.core.adjust_chunks->dict(zip(out_ind, chunks))
A:dask.array.core.out->_elemwise_normalize_out(out)
A:dask.array.core.block_id_array->Array(block_id_dsk, block_id_name, chunks=tuple(((1,) * len(c) for c in out.chunks)), dtype=np.object_)
A:dask.array.core.num_chunks[i]->tuple((len(s) - 1 for s in starts[i]))
A:dask.array.core.arr_k->tuple((location.get(ind, 0) if num_chunks[i][j] > 1 else 0 for (j, ind) in enumerate(argpairs[i][1])))
A:dask.array.core.block_info->Array(block_info_dsk, block_info_name, chunks=tuple(((1,) * len(c) for c in out.chunks)), dtype=np.object_)
A:dask.array.core.func->partial(func, *args, **kwargs)
A:dask.array.core.expected_ndim->kwargs.pop('expected_ndim')
A:dask.array.core.out_ndim->len(broadcast_shapes(*shapes))
A:dask.array.core.n->sum(map(len, locations))
A:dask.array.core.targets->cast('Collection[ArrayLike | Delayed]', targets)
A:dask.array.core.regions_list->list(regions)
A:dask.array.core.sources_hlg->dask.highlevelgraph.HighLevelGraph.merge(*[e.__dask_graph__() for e in sources])
A:dask.array.core.sources_layer->Array.__dask_optimize__(sources_hlg, list(core.flatten([e.__dask_keys__() for e in sources])))
A:dask.array.core.targets_hlg->dask.highlevelgraph.HighLevelGraph.merge(*targets_dsks)
A:dask.array.core.targets_layer->dask.delayed.Delayed.__dask_optimize__(targets_hlg, targets_keys)
A:dask.array.core.dependencies[targets_name]->set()
A:dask.array.core.map_layer->insert_to_ooc(keys=s.__dask_keys__(), chunks=s.chunks, out=t.key if isinstance(t, Delayed) else t, name=n, lock=lock, region=r, return_stored=return_stored, load_stored=load_stored)
A:dask.array.core.store_dsk->HighLevelGraph(layers, dependencies)
A:dask.array.core.store_dlyds->persist(*store_dlyds, **kwargs)
A:dask.array.core.store_dsk_2->dask.highlevelgraph.HighLevelGraph.merge(*[e.dask for e in store_dlyds])
A:dask.array.core.load_store_dsk->retrieve_from_ooc(map_keys, store_dsk, store_dsk_2)
A:dask.array.core.dependencies[key]->set(map_names)
A:dask.array.core.shape->tuple(shape)
A:dask.array.core.CHUNKS_NONE_ERROR_MESSAGE->'\nYou must specify a chunks= keyword argument.\nThis specifies the chunksize of your array blocks.\n\nSee the following documentation page for details:\n  https://docs.dask.org/en/latest/array-creation.html#chunks\n'.strip()
A:dask.array.core.self->super().__new__(cls)
A:dask.array.core.dask->dask.highlevelgraph.HighLevelGraph.from_collections(name, dask, dependencies=())
A:dask.array.core.self._name->str(name)
A:dask.array.core.self._chunks->normalize_chunks(chunks, shape, dtype=dt)
A:dask.array.core.self._meta->meta_from_array(meta, ndim=self.ndim, dtype=dtype)
A:dask.array.core.result->numpy.empty(shape=shape, dtype=dtype(deepfirst(arrays)))
A:dask.array.core.ind->numpy.array(ind, copy=True)
A:dask.array.core.self._cached_keysresult->keys()
A:dask.array.core.__dask_optimize__->globalmethod(optimize, key='array_optimize', falsey=dont_optimize)
A:dask.array.core.__dask_scheduler__->staticmethod(DEFAULT_GET)
A:dask.array.core.name->rename.get(name, name)
A:dask.array.core.chunk_shapes->numpy.empty_like(values[0], dtype=dtype, shape=shape).map_blocks(_get_chunk_shape, dtype=int, chunks=tuple((len(c) * (1,) for c in x.chunks)) + ((x.ndim,),), new_axis=x.ndim)
A:dask.array.core.s[i]->slice(None)
A:dask.array.core.s->numpy.asarray(a.shape, dtype=int)
A:dask.array.core.x._chunks->tuple((tuple((int(chunk) for chunk in chunks)) for chunks in compute(tuple(c))[0]))
A:dask.array.core.da_ufunc->getattr(ufunc, numpy_ufunc.__name__)
A:dask.array.core.chunksize->str(self.chunksize)
A:dask.array.core.grid->super().__new__(cls).to_svg(size=config.get('array.svg.size', 120))
A:dask.array.core.nbytes->format_bytes(self.nbytes)
A:dask.array.core.cbytes->format_bytes(math.prod(self.chunksize) * self.dtype.itemsize)
A:dask.array.core.x->numpy.empty_like(values[0], dtype=dtype, shape=shape)
A:dask.array.core.(args, kwargs)->compute(args, kwargs)
A:dask.array.core.module->getattr(module, submodule)
A:dask.array.core.da_func->getattr(module, func.__name__)
A:dask.array.core.r->store([self], [target], **kwargs)
A:dask.array.core.value->delayed(value)
A:dask.array.core.y->Array(graph, out, chunks=self.chunks, dtype=self.dtype, meta=meta)
A:dask.array.core.dsk->dict(zip(keys, values))
A:dask.array.core.graph->dask.highlevelgraph.HighLevelGraph.from_collections(name, dsk, dependencies=[xx])
A:dask.array.core.dt->numpy.dtype({'names': index, 'formats': [self.dtype.fields[name][0] for name in index], 'offsets': [self.dtype.fields[name][1] for name in index], 'itemsize': self.dtype.itemsize})
A:dask.array.core.index2->normalize_index(index, self.shape)
A:dask.array.core.(self, index2)->slice_with_bool_dask_array(self, index2)
A:dask.array.core.(dsk, chunks)->slice_array(out, self.name, self.chunks, index2, self.itemsize)
A:dask.array.core.casting->kwargs.get('casting', 'unsafe')
A:dask.array.core.L->ndeepmap(self.ndim, lambda k: Delayed(k, graph, layer=layer), keys)
A:dask.array.core.i->int(f)
A:dask.array.core.parsed->parse_bytes(c)
A:dask.array.core.allints->all((isinstance(c, int) for c in chunks))
A:dask.array.core.previous_chunks->getattr(x, 'chunks', None)
A:dask.array.core.limit->max(1, limit)
A:dask.array.core.largest_block->math.prod((cs if isinstance(cs, Number) else max(cs) for cs in chunks if cs != 'auto'))
A:dask.array.core.chunk_frequencies->frequencies(previous_chunks[i])
A:dask.array.core.(mode, count)->max(chunk_frequencies.items(), key=lambda kv: kv[1])
A:dask.array.core.multiplier->_compute_multiplier(limit, dtype, largest_block, result)
A:dask.array.core.last_autos->set(autos)
A:dask.array.core.result[a]->round_to(proposed, ideal_shape[a])
A:dask.array.core.chunks[i]->round_to(size, shape[i])
A:dask.array.core.token->tokenize(x, flat_indexes)
A:dask.array.core.lock->Lock()
A:dask.array.core.is_single_block->all((len(c) == 1 for c in chunks))
A:dask.array.core.slices->slices_from_chunks(chunks)
A:dask.array.core.url->os.fspath(url)
A:dask.array.core.store->zarr.storage.FSStore(url, **storage_options)
A:dask.array.core.z->zarr.create(shape=arr.shape, chunks=chunks, dtype=arr.dtype, store=store, path=component, overwrite=overwrite, **kwargs)
A:dask.array.core.arr->arr.rechunk(chunks).rechunk(chunks)
A:dask.array.core.old_chunks->normalize_chunks(z.chunks, z.shape)
A:dask.array.core.index->tuple((slice(k, k + 1) if isinstance(k, Number) else k for k in index))
A:dask.array.core.total->sum(first(non_trivial_dims))
A:dask.array.core.m->min((c[-1] for c in rchunks))
A:dask.array.core.warn->kwargs.get('warn', True)
A:dask.array.core.(arrays, inds)->zip(*arginds)
A:dask.array.core.blockdim_dict->dict()
A:dask.array.core.max_parts->max(max_parts, a.npartitions)
A:dask.array.core.chunkss->broadcast_dimensions(nameinds, blockdim_dict, consolidate=common_blockdim)
A:dask.array.core.nparts->math.prod(map(len, chunkss.values()))
A:dask.array.core.diff->max(ndim - x.ndim, 0)
A:dask.array.core.rec->_Recurser(recurse_if=lambda x: type(x) is list)
A:dask.array.core.curr_depth->len(index)
A:dask.array.core.elem_ndim->_Recurser(recurse_if=lambda x: type(x) is list).map_reduce(arrays, f_map=lambda xi: xi.ndim, f_reduce=max)
A:dask.array.core.ndim->len(indexes)
A:dask.array.core._concatenate->dask.array.dispatch.concatenate_lookup.dispatch(type(max(seq_metas, key=lambda x: getattr(x, '__array_priority__', 0))))
A:dask.array.core.uc_args->list(concat(((x, ind) for x in seq2)))
A:dask.array.core.(_, seq2)->unify_chunks(*uc_args)
A:dask.array.core.out[index]->numpy.asanyarray(x)
A:dask.array.core.a->numpy.asanyarray(a, like=like_meta, dtype=dtype, order=order)
A:dask.array.core.like_meta->meta_from_array(like)
A:dask.array.core.maybe_shape->getattr(arg, 'shape', None)
A:dask.array.core.where->_elemwise_normalize_where(where)
A:dask.array.core.need_enforce_dtype->any((not is_scalar_for_elemwise(a) and a.ndim == 0 for a in args))
A:dask.array.core.blockwise_kwargs->dict(dtype=dtype, name=name, token=funcname(op).strip('_'))
A:dask.array.core.function->kwargs.pop('enforce_dtype_function')
A:dask.array.core.enumerated_chunks->product(*(enumerate(bds) for bds in chunks))
A:dask.array.core.old_index->tuple((0 if bd == (1,) else i for (bd, i) in zip(x.chunks, new_index[ndim_new:])))
A:dask.array.core.subok->bool(subok)
A:dask.array.core.args->tuple((to_array(e) for e in args))
A:dask.array.core.(_, args)->unify_chunks(*uc_args, warn=False)
A:dask.array.core.args2->list(map(add, args, offset))
A:dask.array.core.idx->first((i for i in enumerate(seq) if i[1].shape != seq[0].shape))
A:dask.array.core.NDARRAY_ARRAY_FUNCTION->getattr(np.ndarray, '__array_function__', None)
A:dask.array.core.advanced->max(core.flatten(arrays, container=(list, tuple)), key=lambda x: getattr(x, '__array_priority__', 0))
A:dask.array.core.extradims->max(0, deepfirst(arrays).ndim - (max(axes) + 1))
A:dask.array.core.indexes->replace_ellipsis(x.ndim, indexes)
A:dask.array.core.nonfancy_indexes->tuple(nonfancy_indexes)
A:dask.array.core.reduced_indexes->tuple(reduced_indexes)
A:dask.array.core.broadcast_indexes->numpy.broadcast_arrays(*dict_indexes.values())
A:dask.array.core.shapes_str->' '.join((str(a.shape) for a in dict_indexes.values()))
A:dask.array.core.lookup->dict(zip(dict_indexes, broadcast_indexes))
A:dask.array.core.axis->_get_axis(flat_indexes)
A:dask.array.core.points->list()
A:dask.array.core.per_block->groupby(1, points)
A:dask.array.core.other_blocks->list(product(*[list(range(len(c))) if i is None else [None] for (i, c) in zip(flat_indexes, x.chunks)]))
A:dask.array.core.result_1d->empty(tuple(map(sum, chunks)), chunks=chunks, dtype=x.dtype, name=out_name)
A:dask.array.core.locations->list(map(list, locations))
A:dask.array.core.values->list(values)
A:dask.array.core.xx->numpy.empty_like(values[0], dtype=dtype, shape=shape).rechunk(chunks)
A:dask.array.core.info->pickle.load(f)
A:dask.array.core.hlg->dask.highlevelgraph.HighLevelGraph.from_collections(name, graph, dependencies=[self._array])
dask.array.Array(cls,dask,name,chunks,dtype=None,meta=None,shape=None)
dask.array.Array.A(self)
dask.array.Array.T(self)
dask.array.Array.__abs__(self)
dask.array.Array.__add__(self,other)
dask.array.Array.__and__(self,other)
dask.array.Array.__array__(self,dtype=None,**kwargs)
dask.array.Array.__array_function__(self,func,types,args,kwargs)
dask.array.Array.__array_ufunc__(self,numpy_ufunc,method,*inputs,**kwargs)
dask.array.Array.__bool__(self)
dask.array.Array.__complex__(self)
dask.array.Array.__dask_graph__(self)->Graph
dask.array.Array.__dask_keys__(self)->NestedKeys
dask.array.Array.__dask_layers__(self)->Sequence[str]
dask.array.Array.__dask_postcompute__(self)
dask.array.Array.__dask_postpersist__(self)
dask.array.Array.__dask_tokenize__(self)
dask.array.Array.__deepcopy__(self,memo)
dask.array.Array.__div__(self,other)
dask.array.Array.__divmod__(self,other)
dask.array.Array.__eq__(self,other)
dask.array.Array.__float__(self)
dask.array.Array.__floordiv__(self,other)
dask.array.Array.__ge__(self,other)
dask.array.Array.__getitem__(self,index)
dask.array.Array.__gt__(self,other)
dask.array.Array.__index__(self)
dask.array.Array.__int__(self)
dask.array.Array.__invert__(self)
dask.array.Array.__iter__(self)
dask.array.Array.__le__(self,other)
dask.array.Array.__len__(self)
dask.array.Array.__lshift__(self,other)
dask.array.Array.__lt__(self,other)
dask.array.Array.__matmul__(self,other)
dask.array.Array.__mod__(self,other)
dask.array.Array.__mul__(self,other)
dask.array.Array.__ne__(self,other)
dask.array.Array.__neg__(self)
dask.array.Array.__or__(self,other)
dask.array.Array.__pos__(self)
dask.array.Array.__pow__(self,other)
dask.array.Array.__radd__(self,other)
dask.array.Array.__rand__(self,other)
dask.array.Array.__rdiv__(self,other)
dask.array.Array.__rdivmod__(self,other)
dask.array.Array.__reduce__(self)
dask.array.Array.__repr__(self)
dask.array.Array.__rfloordiv__(self,other)
dask.array.Array.__rlshift__(self,other)
dask.array.Array.__rmatmul__(self,other)
dask.array.Array.__rmod__(self,other)
dask.array.Array.__rmul__(self,other)
dask.array.Array.__ror__(self,other)
dask.array.Array.__rpow__(self,other)
dask.array.Array.__rrshift__(self,other)
dask.array.Array.__rshift__(self,other)
dask.array.Array.__rsub__(self,other)
dask.array.Array.__rtruediv__(self,other)
dask.array.Array.__rxor__(self,other)
dask.array.Array.__setitem__(self,key,value)
dask.array.Array.__sub__(self,other)
dask.array.Array.__truediv__(self,other)
dask.array.Array.__xor__(self,other)
dask.array.Array._chunks(self)
dask.array.Array._chunks(self,chunks)
dask.array.Array._elemwise(self)
dask.array.Array._key_array(self)
dask.array.Array._name(self)
dask.array.Array._name(self,val)
dask.array.Array._rebuild(self,dsk,*,rename=None)
dask.array.Array._repr_html_(self)
dask.array.Array._reset_cache(self,key=None)
dask.array.Array._scalarfunc(self,cast_type)
dask.array.Array._vindex(self,key)
dask.array.Array.all(self,axis=None,keepdims=False,split_every=None,out=None)
dask.array.Array.any(self,axis=None,keepdims=False,split_every=None,out=None)
dask.array.Array.argmax(self,axis=None,*,keepdims=False,split_every=None,out=None)
dask.array.Array.argmin(self,axis=None,*,keepdims=False,split_every=None,out=None)
dask.array.Array.argtopk(self,k,axis=-1,split_every=None)
dask.array.Array.astype(self,dtype,**kwargs)
dask.array.Array.blocks(self)
dask.array.Array.choose(self,choices)
dask.array.Array.chunks(self)
dask.array.Array.chunks(self,chunks)
dask.array.Array.chunksize(self)->tuple[T_IntOrNaN, ...]
dask.array.Array.clip(self,min=None,max=None)
dask.array.Array.compute_chunk_sizes(self)
dask.array.Array.conj(self)
dask.array.Array.copy(self)
dask.array.Array.cumprod(self,axis,dtype=None,out=None,*,method='sequential')
dask.array.Array.cumsum(self,axis,dtype=None,out=None,*,method='sequential')
dask.array.Array.dot(self,other)
dask.array.Array.dtype(self)
dask.array.Array.imag(self)
dask.array.Array.itemsize(self)->int
dask.array.Array.map_blocks(self,func,*args,**kwargs)
dask.array.Array.map_overlap(self,func,depth,boundary=None,trim=True,**kwargs)
dask.array.Array.max(self,axis=None,keepdims=False,split_every=None,out=None)
dask.array.Array.mean(self,axis=None,dtype=None,keepdims=False,split_every=None,out=None)
dask.array.Array.min(self,axis=None,keepdims=False,split_every=None,out=None)
dask.array.Array.moment(self,order,axis=None,dtype=None,keepdims=False,ddof=0,split_every=None,out=None)
dask.array.Array.name(self)
dask.array.Array.name(self,val)
dask.array.Array.nbytes(self)->T_IntOrNaN
dask.array.Array.ndim(self)->int
dask.array.Array.nonzero(self)
dask.array.Array.npartitions(self)
dask.array.Array.numblocks(self)
dask.array.Array.partitions(self)
dask.array.Array.prod(self,axis=None,dtype=None,keepdims=False,split_every=None,out=None)
dask.array.Array.ravel(self)
dask.array.Array.real(self)
dask.array.Array.rechunk(self,chunks='auto',threshold=None,block_size_limit=None,balance=False,method=None)
dask.array.Array.repeat(self,repeats,axis=None)
dask.array.Array.reshape(self,*shape,merge_chunks=True,limit=None)
dask.array.Array.round(self,decimals=0)
dask.array.Array.shape(self)->tuple[T_IntOrNaN, ...]
dask.array.Array.size(self)->T_IntOrNaN
dask.array.Array.squeeze(self,axis=None)
dask.array.Array.std(self,axis=None,dtype=None,keepdims=False,ddof=0,split_every=None,out=None)
dask.array.Array.store(self,target,**kwargs)
dask.array.Array.sum(self,axis=None,dtype=None,keepdims=False,split_every=None,out=None)
dask.array.Array.swapaxes(self,axis1,axis2)
dask.array.Array.to_backend(self,backend:str|None=None,**kwargs)
dask.array.Array.to_dask_dataframe(self,columns=None,index=None,meta=None)
dask.array.Array.to_delayed(self,optimize_graph=True)
dask.array.Array.to_hdf5(self,filename,datapath,**kwargs)
dask.array.Array.to_svg(self,size=500)
dask.array.Array.to_tiledb(self,uri,*args,**kwargs)
dask.array.Array.to_zarr(self,*args,**kwargs)
dask.array.Array.topk(self,k,axis=-1,split_every=None)
dask.array.Array.trace(self,offset=0,axis1=0,axis2=1,dtype=None)
dask.array.Array.transpose(self,*axes)
dask.array.Array.var(self,axis=None,dtype=None,keepdims=False,ddof=0,split_every=None,out=None)
dask.array.Array.view(self,dtype=None,order='C')
dask.array.Array.vindex(self)
dask.array.PerformanceWarning(Warning)
dask.array.asanyarray(a,dtype=None,order=None,*,like=None,inline_array=False)
dask.array.asarray(a,allow_unknown_chunksizes=False,dtype=None,order=None,*,like=None,**kwargs)
dask.array.block(arrays,allow_unknown_chunksizes=False)
dask.array.blockdims_from_blockshape(shape,chunks)
dask.array.broadcast_arrays(*args,subok=False)
dask.array.broadcast_to(x,shape,chunks=None,meta=None)
dask.array.concatenate(seq,axis=0,allow_unknown_chunksizes=False)
dask.array.concatenate3(arrays)
dask.array.concatenate_axes(arrays,axes)
dask.array.core.Array(cls,dask,name,chunks,dtype=None,meta=None,shape=None)
dask.array.core.Array.A(self)
dask.array.core.Array.T(self)
dask.array.core.Array.__abs__(self)
dask.array.core.Array.__add__(self,other)
dask.array.core.Array.__and__(self,other)
dask.array.core.Array.__array__(self,dtype=None,**kwargs)
dask.array.core.Array.__array_function__(self,func,types,args,kwargs)
dask.array.core.Array.__array_ufunc__(self,numpy_ufunc,method,*inputs,**kwargs)
dask.array.core.Array.__bool__(self)
dask.array.core.Array.__complex__(self)
dask.array.core.Array.__dask_graph__(self)->Graph
dask.array.core.Array.__dask_keys__(self)->NestedKeys
dask.array.core.Array.__dask_layers__(self)->Sequence[str]
dask.array.core.Array.__dask_postcompute__(self)
dask.array.core.Array.__dask_postpersist__(self)
dask.array.core.Array.__dask_tokenize__(self)
dask.array.core.Array.__deepcopy__(self,memo)
dask.array.core.Array.__div__(self,other)
dask.array.core.Array.__divmod__(self,other)
dask.array.core.Array.__eq__(self,other)
dask.array.core.Array.__float__(self)
dask.array.core.Array.__floordiv__(self,other)
dask.array.core.Array.__ge__(self,other)
dask.array.core.Array.__getitem__(self,index)
dask.array.core.Array.__gt__(self,other)
dask.array.core.Array.__index__(self)
dask.array.core.Array.__int__(self)
dask.array.core.Array.__invert__(self)
dask.array.core.Array.__iter__(self)
dask.array.core.Array.__le__(self,other)
dask.array.core.Array.__len__(self)
dask.array.core.Array.__lshift__(self,other)
dask.array.core.Array.__lt__(self,other)
dask.array.core.Array.__matmul__(self,other)
dask.array.core.Array.__mod__(self,other)
dask.array.core.Array.__mul__(self,other)
dask.array.core.Array.__ne__(self,other)
dask.array.core.Array.__neg__(self)
dask.array.core.Array.__new__(cls,dask,name,chunks,dtype=None,meta=None,shape=None)
dask.array.core.Array.__or__(self,other)
dask.array.core.Array.__pos__(self)
dask.array.core.Array.__pow__(self,other)
dask.array.core.Array.__radd__(self,other)
dask.array.core.Array.__rand__(self,other)
dask.array.core.Array.__rdiv__(self,other)
dask.array.core.Array.__rdivmod__(self,other)
dask.array.core.Array.__reduce__(self)
dask.array.core.Array.__repr__(self)
dask.array.core.Array.__rfloordiv__(self,other)
dask.array.core.Array.__rlshift__(self,other)
dask.array.core.Array.__rmatmul__(self,other)
dask.array.core.Array.__rmod__(self,other)
dask.array.core.Array.__rmul__(self,other)
dask.array.core.Array.__ror__(self,other)
dask.array.core.Array.__rpow__(self,other)
dask.array.core.Array.__rrshift__(self,other)
dask.array.core.Array.__rshift__(self,other)
dask.array.core.Array.__rsub__(self,other)
dask.array.core.Array.__rtruediv__(self,other)
dask.array.core.Array.__rxor__(self,other)
dask.array.core.Array.__setitem__(self,key,value)
dask.array.core.Array.__sub__(self,other)
dask.array.core.Array.__truediv__(self,other)
dask.array.core.Array.__xor__(self,other)
dask.array.core.Array._chunks(self)
dask.array.core.Array._chunks(self,chunks)
dask.array.core.Array._elemwise(self)
dask.array.core.Array._key_array(self)
dask.array.core.Array._name(self)
dask.array.core.Array._name(self,val)
dask.array.core.Array._rebuild(self,dsk,*,rename=None)
dask.array.core.Array._repr_html_(self)
dask.array.core.Array._reset_cache(self,key=None)
dask.array.core.Array._scalarfunc(self,cast_type)
dask.array.core.Array._vindex(self,key)
dask.array.core.Array.all(self,axis=None,keepdims=False,split_every=None,out=None)
dask.array.core.Array.any(self,axis=None,keepdims=False,split_every=None,out=None)
dask.array.core.Array.argmax(self,axis=None,*,keepdims=False,split_every=None,out=None)
dask.array.core.Array.argmin(self,axis=None,*,keepdims=False,split_every=None,out=None)
dask.array.core.Array.argtopk(self,k,axis=-1,split_every=None)
dask.array.core.Array.astype(self,dtype,**kwargs)
dask.array.core.Array.blocks(self)
dask.array.core.Array.choose(self,choices)
dask.array.core.Array.chunks(self)
dask.array.core.Array.chunks(self,chunks)
dask.array.core.Array.chunksize(self)->tuple[T_IntOrNaN, ...]
dask.array.core.Array.clip(self,min=None,max=None)
dask.array.core.Array.compute_chunk_sizes(self)
dask.array.core.Array.conj(self)
dask.array.core.Array.copy(self)
dask.array.core.Array.cumprod(self,axis,dtype=None,out=None,*,method='sequential')
dask.array.core.Array.cumsum(self,axis,dtype=None,out=None,*,method='sequential')
dask.array.core.Array.dot(self,other)
dask.array.core.Array.dtype(self)
dask.array.core.Array.imag(self)
dask.array.core.Array.itemsize(self)->int
dask.array.core.Array.map_blocks(self,func,*args,**kwargs)
dask.array.core.Array.map_overlap(self,func,depth,boundary=None,trim=True,**kwargs)
dask.array.core.Array.max(self,axis=None,keepdims=False,split_every=None,out=None)
dask.array.core.Array.mean(self,axis=None,dtype=None,keepdims=False,split_every=None,out=None)
dask.array.core.Array.min(self,axis=None,keepdims=False,split_every=None,out=None)
dask.array.core.Array.moment(self,order,axis=None,dtype=None,keepdims=False,ddof=0,split_every=None,out=None)
dask.array.core.Array.name(self)
dask.array.core.Array.name(self,val)
dask.array.core.Array.nbytes(self)->T_IntOrNaN
dask.array.core.Array.ndim(self)->int
dask.array.core.Array.nonzero(self)
dask.array.core.Array.npartitions(self)
dask.array.core.Array.numblocks(self)
dask.array.core.Array.partitions(self)
dask.array.core.Array.prod(self,axis=None,dtype=None,keepdims=False,split_every=None,out=None)
dask.array.core.Array.ravel(self)
dask.array.core.Array.real(self)
dask.array.core.Array.rechunk(self,chunks='auto',threshold=None,block_size_limit=None,balance=False,method=None)
dask.array.core.Array.repeat(self,repeats,axis=None)
dask.array.core.Array.reshape(self,*shape,merge_chunks=True,limit=None)
dask.array.core.Array.round(self,decimals=0)
dask.array.core.Array.shape(self)->tuple[T_IntOrNaN, ...]
dask.array.core.Array.size(self)->T_IntOrNaN
dask.array.core.Array.squeeze(self,axis=None)
dask.array.core.Array.std(self,axis=None,dtype=None,keepdims=False,ddof=0,split_every=None,out=None)
dask.array.core.Array.store(self,target,**kwargs)
dask.array.core.Array.sum(self,axis=None,dtype=None,keepdims=False,split_every=None,out=None)
dask.array.core.Array.swapaxes(self,axis1,axis2)
dask.array.core.Array.to_backend(self,backend:str|None=None,**kwargs)
dask.array.core.Array.to_dask_dataframe(self,columns=None,index=None,meta=None)
dask.array.core.Array.to_delayed(self,optimize_graph=True)
dask.array.core.Array.to_hdf5(self,filename,datapath,**kwargs)
dask.array.core.Array.to_svg(self,size=500)
dask.array.core.Array.to_tiledb(self,uri,*args,**kwargs)
dask.array.core.Array.to_zarr(self,*args,**kwargs)
dask.array.core.Array.topk(self,k,axis=-1,split_every=None)
dask.array.core.Array.trace(self,offset=0,axis1=0,axis2=1,dtype=None)
dask.array.core.Array.transpose(self,*axes)
dask.array.core.Array.var(self,axis=None,dtype=None,keepdims=False,ddof=0,split_every=None,out=None)
dask.array.core.Array.view(self,dtype=None,order='C')
dask.array.core.Array.vindex(self)
dask.array.core.BlockView(self,array:Array)
dask.array.core.BlockView.__eq__(self,other:Any)->bool
dask.array.core.BlockView.__getitem__(self,index:Any)->Array
dask.array.core.BlockView.__init__(self,array:Array)
dask.array.core.BlockView.ravel(self)->list[Array]
dask.array.core.BlockView.shape(self)->tuple[int, ...]
dask.array.core.BlockView.size(self)->int
dask.array.core.PerformanceWarning(Warning)
dask.array.core._check_regular_chunks(chunkset)
dask.array.core._compute_multiplier(limit:int,dtype,largest_block:int,result)
dask.array.core._concatenate2(arrays,axes=None)
dask.array.core._elemwise_handle_where(*args,**kwargs)
dask.array.core._elemwise_normalize_out(out)
dask.array.core._elemwise_normalize_where(where)
dask.array.core._enforce_dtype(*args,**kwargs)
dask.array.core._get_axis(indexes)
dask.array.core._get_chunk_shape(a)
dask.array.core._pass_extra_kwargs(func,keys,*args,**kwargs)
dask.array.core._should_delegate(self,other)->bool
dask.array.core._vindex(x,*indexes)
dask.array.core._vindex_array(x,dict_indexes)
dask.array.core._vindex_merge(locations,values)
dask.array.core._vindex_slice(block,points)
dask.array.core._vindex_transpose(block,axis)
dask.array.core.apply_and_enforce(*args,**kwargs)
dask.array.core.apply_infer_dtype(func,args,kwargs,funcname,suggest_dtype='dtype',nout=None)
dask.array.core.asanyarray(a,dtype=None,order=None,*,like=None,inline_array=False)
dask.array.core.asarray(a,allow_unknown_chunksizes=False,dtype=None,order=None,*,like=None,**kwargs)
dask.array.core.auto_chunks(chunks,shape,limit,dtype,previous_chunks=None)
dask.array.core.block(arrays,allow_unknown_chunksizes=False)
dask.array.core.blockdims_from_blockshape(shape,chunks)
dask.array.core.broadcast_arrays(*args,subok=False)
dask.array.core.broadcast_chunks(*chunkss)
dask.array.core.broadcast_shapes(*shapes)
dask.array.core.broadcast_to(x,shape,chunks=None,meta=None)
dask.array.core.check_if_handled_given_other(f)
dask.array.core.chunks_from_arrays(arrays)
dask.array.core.common_blockdim(blockdims)
dask.array.core.concatenate(seq,axis=0,allow_unknown_chunksizes=False)
dask.array.core.concatenate3(arrays)
dask.array.core.concatenate_axes(arrays,axes)
dask.array.core.deepfirst(seq)
dask.array.core.dotmany(A,B,leftfunc=None,rightfunc=None,**kwargs)
dask.array.core.elemwise(op,*args,out=None,where=True,dtype=None,name=None,**kwargs)
dask.array.core.ensure_int(f)
dask.array.core.finalize(results)
dask.array.core.from_array(x,chunks='auto',name=None,lock=False,asarray=None,fancy=True,getitem=None,meta=None,inline_array=False)
dask.array.core.from_delayed(value,shape,dtype=None,meta=None,name=None)
dask.array.core.from_func(func,shape,dtype=None,name=None,args=(),kwargs=None)
dask.array.core.from_npy_stack(dirname,mmap_mode='r')
dask.array.core.from_zarr(url,component=None,storage_options=None,chunks=None,name=None,inline_array=False,**kwargs)
dask.array.core.getter(a,b,asarray=True,lock=None)
dask.array.core.getter_inline(a,b,asarray=True,lock=None)
dask.array.core.getter_nofancy(a,b,asarray=True,lock=None)
dask.array.core.graph_from_arraylike(arr,chunks,shape,name,getitem=getter,lock=False,asarray=True,dtype=None,inline_array=False)->HighLevelGraph
dask.array.core.handle_out(out,result)
dask.array.core.implements(*numpy_functions)
dask.array.core.insert_to_ooc(keys:list,chunks:tuple[tuple[int,...],...],out:ArrayLike,name:str,*,lock:Lock|bool=True,region:tuple[slice,...]|slice|None=None,return_stored:bool=False,load_stored:bool=False)->dict
dask.array.core.interleave_none(a,b)
dask.array.core.is_scalar_for_elemwise(arg)
dask.array.core.keyname(name,i,okey)
dask.array.core.load_chunk(out:A,index:slice,lock:Any)->A
dask.array.core.load_store_chunk(x:Any,out:Any,index:slice,lock:Any,return_stored:bool,load_stored:bool)
dask.array.core.map_blocks(func,*args,name=None,token=None,dtype=None,chunks=None,drop_axis=None,new_axis=None,enforce_ndim=False,meta=None,**kwargs)
dask.array.core.new_da_object(dsk,name,chunks,meta=None,dtype=None)
dask.array.core.normalize_arg(x)
dask.array.core.normalize_chunks(chunks,shape=None,limit=None,dtype=None,previous_chunks=None)
dask.array.core.offset_func(func,offset,*args)
dask.array.core.retrieve_from_ooc(keys:Collection[Key],dsk_pre:Graph,dsk_post:Graph)->dict[tuple, Any]
dask.array.core.round_to(c,s)
dask.array.core.shapelist(a)
dask.array.core.slices_from_chunks(chunks)
dask.array.core.stack(seq,axis=0,allow_unknown_chunksizes=False)
dask.array.core.store(sources:Array|Collection[Array],targets:ArrayLike|Delayed|Collection[ArrayLike|Delayed],lock:bool|Lock=True,regions:tuple[slice,...]|Collection[tuple[slice,...]]|None=None,compute:bool=True,return_stored:bool=False,**kwargs)
dask.array.core.store_chunk(x:ArrayLike,out:ArrayLike,index:slice,lock:Any,return_stored:bool)
dask.array.core.to_hdf5(filename,*args,chunks=True,**kwargs)
dask.array.core.to_npy_stack(dirname,x,axis=0)
dask.array.core.to_zarr(arr,url,component=None,storage_options=None,overwrite=False,region=None,compute=True,return_stored=False,**kwargs)
dask.array.core.transposelist(arrays,axes,extradims=0)
dask.array.core.unify_chunks(*args,**kwargs)
dask.array.core.unpack_singleton(x)
dask.array.from_array(x,chunks='auto',name=None,lock=False,asarray=None,fancy=True,getitem=None,meta=None,inline_array=False)
dask.array.from_delayed(value,shape,dtype=None,meta=None,name=None)
dask.array.from_npy_stack(dirname,mmap_mode='r')
dask.array.from_zarr(url,component=None,storage_options=None,chunks=None,name=None,inline_array=False,**kwargs)
dask.array.map_blocks(func,*args,name=None,token=None,dtype=None,chunks=None,drop_axis=None,new_axis=None,enforce_ndim=False,meta=None,**kwargs)
dask.array.stack(seq,axis=0,allow_unknown_chunksizes=False)
dask.array.store(sources:Array|Collection[Array],targets:ArrayLike|Delayed|Collection[ArrayLike|Delayed],lock:bool|Lock=True,regions:tuple[slice,...]|Collection[tuple[slice,...]]|None=None,compute:bool=True,return_stored:bool=False,**kwargs)
dask.array.store_chunk(x:ArrayLike,out:ArrayLike,index:slice,lock:Any,return_stored:bool)
dask.array.to_hdf5(filename,*args,chunks=True,**kwargs)
dask.array.to_npy_stack(dirname,x,axis=0)
dask.array.to_zarr(arr,url,component=None,storage_options=None,overwrite=False,region=None,compute=True,return_stored=False,**kwargs)
dask.array.unify_chunks(*args,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/utils.py----------------------------------------
A:dask.array.utils.x->numpy.array(x, dtype='O')
A:dask.array.utils.meta->numpy.array(meta)
A:dask.array.utils.a->a.compute(scheduler='sync').compute(scheduler='sync')
A:dask.array.utils.b->numpy.array(b)
A:dask.array.utils.freqs->frequencies(concat(dsk.layers.values()))
A:dask.array.utils.key_collisions->set()
A:dask.array.utils.chunk->numpy.array(chunk, dtype='O')
A:dask.array.utils.expected_shape->tuple((c[i] for (c, i) in zip(x.chunks, idx)))
A:dask.array.utils.x_meta->getattr(x, '_meta', None)
A:dask.array.utils.adt->getattr(x, 'dtype', None)
A:dask.array.utils.(a, adt, a_meta, a_computed)->_get_dt_meta_computed(a, check_shape=check_shape, check_graph=check_graph, check_chunks=check_chunks, check_ndim=check_ndim, scheduler=scheduler)
A:dask.array.utils.(b, bdt, b_meta, b_computed)->_get_dt_meta_computed(b, check_shape=check_shape, check_graph=check_graph, check_chunks=check_chunks, check_ndim=check_ndim, scheduler=scheduler)
A:dask.array.utils.signs->numpy.sum(u, axis=0, keepdims=True)
A:dask.array.utils.func->getattr(scipy.linalg, func_name)
dask.array.assert_eq(a,b,check_shape=True,check_graph=True,check_meta=True,check_chunks=True,check_ndim=True,check_type=True,check_dtype=True,equal_nan=True,scheduler='sync',**kwargs)
dask.array.assert_eq_shape(a,b,check_ndim=True,check_nan=True)
dask.array.utils.__getattr__(name)
dask.array.utils._array_like_safe(np_func,da_func,a,like,**kwargs)
dask.array.utils._check_chunks(x,check_ndim=True,scheduler=None)
dask.array.utils._check_dsk(dsk)
dask.array.utils._dtype_of(a)
dask.array.utils._get_dt_meta_computed(x,check_shape=True,check_graph=True,check_chunks=True,check_ndim=True,scheduler=None)
dask.array.utils._not_empty(x)
dask.array.utils.allclose(a,b,equal_nan=False,**kwargs)
dask.array.utils.arange_safe(*args,like,**kwargs)
dask.array.utils.array_safe(a,like,**kwargs)
dask.array.utils.asanyarray_safe(a,like,**kwargs)
dask.array.utils.asarray_safe(a,like,**kwargs)
dask.array.utils.assert_eq(a,b,check_shape=True,check_graph=True,check_meta=True,check_chunks=True,check_ndim=True,check_type=True,check_dtype=True,equal_nan=True,scheduler='sync',**kwargs)
dask.array.utils.assert_eq_shape(a,b,check_ndim=True,check_nan=True)
dask.array.utils.compute_meta(func,_dtype,*args,**kwargs)
dask.array.utils.meta_from_array(x,ndim=None,dtype=None)
dask.array.utils.normalize_to_array(x)
dask.array.utils.safe_wraps(wrapped,assigned=functools.WRAPPER_ASSIGNMENTS)
dask.array.utils.same_keys(a,b)
dask.array.utils.scipy_linalg_safe(func_name,*args,**kwargs)
dask.array.utils.solve_triangular_safe(a,b,lower=False)
dask.array.utils.svd_flip(u,v,u_based_decision=False)
dask.array.utils.validate_axis(axis,ndim)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/ma.py----------------------------------------
A:dask.array.ma.data->asanyarray(data)
A:dask.array.ma.mask->mask.reshape((1,) * data.ndim).reshape((1,) * data.ndim)
A:dask.array.ma.fill_value->numpy.ma.core._check_fill_value(fill_value, a.dtype)
A:dask.array.ma.a->asanyarray(a)
A:dask.array.ma.value->asanyarray(value)
A:dask.array.ma.oinds->max(ainds, vinds, key=len)
A:dask.array.ma.masked_greater->_wrap_masked(np.ma.masked_greater)
A:dask.array.ma.masked_greater_equal->_wrap_masked(np.ma.masked_greater_equal)
A:dask.array.ma.masked_less->_wrap_masked(np.ma.masked_less)
A:dask.array.ma.masked_less_equal->_wrap_masked(np.ma.masked_less_equal)
A:dask.array.ma.masked_not_equal->_wrap_masked(np.ma.masked_not_equal)
A:dask.array.ma.inds->tuple(range(data.ndim))
A:dask.array.ma.x->x.copy().copy()
A:dask.array.ma.cshape->getattr(condition, 'shape', ())
A:dask.array.ma.condition->asanyarray(condition)
A:dask.array.ma.ainds->tuple(range(a.ndim))
A:dask.array.ma.cinds->tuple(range(condition.ndim))
A:dask.array.ma.res->asanyarray(a).map_blocks(_set_fill_value, fill_value)
dask.array.ma._chunk_count(x,axis=None,keepdims=None)
dask.array.ma._masked_array(data,mask=np.ma.nomask,masked_dtype=None,**kwargs)
dask.array.ma._set_fill_value(x,fill_value)
dask.array.ma._wrap_masked(f)
dask.array.ma.average(a,axis=None,weights=None,returned=False,keepdims=False)
dask.array.ma.count(a,axis=None,keepdims=False,split_every=None)
dask.array.ma.empty_like(a,**kwargs)
dask.array.ma.filled(a,fill_value=None)
dask.array.ma.fix_invalid(a,fill_value=None)
dask.array.ma.getdata(a)
dask.array.ma.getmaskarray(a)
dask.array.ma.masked_array(data,mask=np.ma.nomask,fill_value=None,**kwargs)
dask.array.ma.masked_equal(a,value)
dask.array.ma.masked_inside(x,v1,v2)
dask.array.ma.masked_invalid(a)
dask.array.ma.masked_outside(x,v1,v2)
dask.array.ma.masked_values(x,value,rtol=1e-05,atol=1e-08,shrink=True)
dask.array.ma.masked_where(condition,a)
dask.array.ma.nonzero(a)
dask.array.ma.normalize_masked_array(x)
dask.array.ma.ones_like(a,**kwargs)
dask.array.ma.set_fill_value(a,fill_value)
dask.array.ma.where(condition,x=None,y=None)
dask.array.ma.zeros_like(a,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/optimization.py----------------------------------------
A:dask.array.optimization.keys->list(flatten(keys))
A:dask.array.optimization.dsk->dsk.copy().copy()
A:dask.array.optimization.dependencies->dsk.copy().copy().get_all_dependencies()
A:dask.array.optimization.hold->hold_keys(dsk, dependencies)
A:dask.array.optimization.(dsk, dependencies)->fuse(dsk, hold + keys + (fuse_keys or []), dependencies, rename_keys=rename_fused_keys)
A:dask.array.optimization.dependents->reverse_dict(dependencies)
A:dask.array.optimization.hold_keys->list(data)
A:dask.array.optimization.new_dep->next(iter(dependents[dep]))
A:dask.array.optimization.v->next(iter(first.dsk.values()))
A:dask.array.optimization.length->len(value)
A:dask.array.optimization.c_index->fuse_slice(b_index, a_index)
A:dask.array.optimization.a->normalize_slice(a)
A:dask.array.optimization.b->normalize_slice(b)
A:dask.array.optimization.stop->min(a.stop, stop)
A:dask.array.optimization.a_has_lists->any((isinstance(item, list) for item in a))
A:dask.array.optimization.b_has_lists->any((isinstance(item, list) for item in b))
A:dask.array.optimization.result->list()
dask.array.optimization._is_getter_task(value)->tuple[Callable, Any, Any, bool, bool | None] | None
dask.array.optimization.check_for_nonfusible_fancy_indexing(fancy,normal)
dask.array.optimization.fuse_slice(a,b)
dask.array.optimization.hold_keys(dsk,dependencies)
dask.array.optimization.normalize_slice(s)
dask.array.optimization.optimize(dsk,keys,fuse_keys=None,fast_functions=None,inline_functions_fast_functions=(getter_inline,),rename_fused_keys=True,**kwargs)
dask.array.optimization.optimize_slices(dsk)
dask.array.optimize(dsk,keys,fuse_keys=None,fast_functions=None,inline_functions_fast_functions=(getter_inline,),rename_fused_keys=True,**kwargs)
dask.array.optimize_slices(dsk)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/reshape.py----------------------------------------
A:dask.array.reshape.chunk_reduction->reduce(mul, map(len, inchunks[ileft + 1:ii + 1]))
A:dask.array.reshape.result_inchunks[ileft]->expand_tuple(inchunks[ileft], chunk_reduction)
A:dask.array.reshape.prod->reduce(mul, inshape[ileft + 1:ii + 1])
A:dask.array.reshape.result_outchunks[oi]->tuple((prod * c for c in result_inchunks[ileft]))
A:dask.array.reshape.cs->reduce(mul, outshape[oleft + 1:oi + 1])
A:dask.array.reshape.result_inchunks[ii]->contract_tuple(inchunks[ii], cs)
A:dask.array.reshape.result_outchunks[oleft]->tuple((c // cs for c in result_inchunks[ii]))
A:dask.array.reshape.part->max(x / factor, 1)
A:dask.array.reshape.shape->tuple((missing_size if s == -1 else s for s in shape))
A:dask.array.reshape.missing_size->sanitize_index(x.size / reduce(mul, known_sizes, 1))
A:dask.array.reshape.meta->meta_from_array(x, len(shape))
A:dask.array.reshape.key->next(flatten(x.__dask_keys__()))
A:dask.array.reshape.chunks->tuple(((d,) for d in shape))
A:dask.array.reshape.graph->dask.highlevelgraph.HighLevelGraph.from_collections(name, dsk, dependencies=[x2])
A:dask.array.reshape.din->len(x.shape)
A:dask.array.reshape.dout->len(shape)
A:dask.array.reshape.x->x.rechunk({i: 1 for i in range(din - dout)}).rechunk({i: 1 for i in range(din - dout)})
A:dask.array.reshape.(inchunks, outchunks)->reshape_rechunk(x.shape, shape, x.chunks)
A:dask.array.reshape.limit->parse_bytes(limit)
A:dask.array.reshape.split->dask.config.get('array.slicing.split-large-chunks', None)
A:dask.array.reshape.outchunks->normalize_chunks(chunk_plan, shape=shape, limit=limit, dtype=x.dtype, previous_chunks=inchunks)
A:dask.array.reshape.x2->x.rechunk({i: 1 for i in range(din - dout)}).rechunk({i: 1 for i in range(din - dout)}).rechunk(inchunks)
A:dask.array.reshape.in_keys->list(product([x2.name], *[range(len(c)) for c in inchunks]))
A:dask.array.reshape.out_keys->list(product([name], *[range(len(c)) for c in outchunks]))
A:dask.array.reshape.shapes->list(product(*outchunks))
dask.array.reshape(x,shape,merge_chunks=True,limit=None)
dask.array.reshape.contract_tuple(chunks,factor)
dask.array.reshape.expand_tuple(chunks,factor)
dask.array.reshape.reshape(x,shape,merge_chunks=True,limit=None)
dask.array.reshape.reshape_rechunk(inshape,outshape,inchunks)
dask.array.reshape_rechunk(inshape,outshape,inchunks)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/numpy_compat.py----------------------------------------
A:dask.array.numpy_compat._np_version->parse_version(np.__version__)
A:dask.array.numpy_compat.x->x.astype(dtype).astype(dtype)
A:dask.array.numpy_compat.ma_divide->numpy.ma.core._DomainedBinaryOperation(divide, np.ma.core._DomainSafeDivide(), 0, 1)
A:dask.array.numpy_compat.next_kwargs->f_kwargs(**kwargs)
A:dask.array.numpy_compat.do_recurse->self.recurse_if(x)
A:dask.array.numpy_compat.source->normalize_axis_tuple(source, a.ndim, 'source')
A:dask.array.numpy_compat.destination->normalize_axis_tuple(destination, a.ndim, 'destination')
A:dask.array.numpy_compat.result->a.transpose(order)
A:dask.array.numpy_compat.axis->normalize_axis_index(axis, n)
A:dask.array.numpy_compat.axes->list(range(0, n))
dask.array.moveaxis(a,source,destination)
dask.array.numpy_compat._Recurser(self,recurse_if)
dask.array.numpy_compat._Recurser.__init__(self,recurse_if)
dask.array.numpy_compat._Recurser.map_reduce(self,x,f_map=lambdax,**kwargs:x,f_reduce=lambdax,**kwargs:x,f_kwargs=lambda**kwargs:kwargs,**kwargs)
dask.array.numpy_compat._Recurser.walk(self,x,index=())
dask.array.numpy_compat.moveaxis(a,source,destination)
dask.array.numpy_compat.percentile(a,q,method='linear')
dask.array.numpy_compat.rollaxis(a,axis,start=0)
dask.array.rollaxis(a,axis,start=0)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/gufunc.py----------------------------------------
A:dask.array.gufunc._CORE_DIMENSION_LIST->'(?:{0:}(?:,{0:})*,?)?'.format(_DIMENSION_NAME)
A:dask.array.gufunc._INPUT_ARGUMENTS->'(?:{0:}(?:,{0:})*,?)?'.format(_ARGUMENT)
A:dask.array.gufunc._OUTPUT_ARGUMENTS->'{0:}(?:,{0:})*'.format(_ARGUMENT)
A:dask.array.gufunc.signature->re.sub('\\s+', '', signature)
A:dask.array.gufunc.(in_txt, out_txt)->re.sub('\\s+', '', signature).split('->')
A:dask.array.gufunc.nin->len(input_coredimss)
A:dask.array.gufunc.filtered_core_dims->list(filter(len, input_coredimss))
A:dask.array.gufunc.nr_outputs_with_coredims->len([True for x in output_coredimss if len(x) > 0])
A:dask.array.gufunc.(input_coredimss, output_coredimss)->_parse_gufunc_signature(signature)
A:dask.array.gufunc.tempfunc->numpy.vectorize(func, signature=signature)
A:dask.array.gufunc.output_dtypes->apply_infer_dtype(tempfunc, args, kwargs, 'apply_gufunc', 'output_dtypes', nout)
A:dask.array.gufunc.meta->meta_from_array(meta, len(output_shape))
A:dask.array.gufunc.func->numpy.vectorize(func, signature=signature, otypes=otypes)
A:dask.array.gufunc.(input_axes, output_axes)->_validate_normalize_axes(axes, axis, keepdims, input_coredimss, output_coredimss)
A:dask.array.gufunc.iax->tuple((a if a < 0 else a - len(shape) for a in iax))
A:dask.array.gufunc.transposed_arg->arg.transpose(tidc)
A:dask.array.gufunc.core_shapes->merge(*core_input_shapes)
A:dask.array.gufunc.dimsizes->dimsizess.get(dim, [])
A:dask.array.gufunc.chunksizes_->chunksizess.get(dim, [])
A:dask.array.gufunc.relevant_chunksizes->list(unique((c for (s, c) in zip(sizes, chunksizes) if s > 1)))
A:dask.array.gufunc.arginds->list(concat(zip(args, input_dimss)))
A:dask.array.gufunc.tmp->blockwise(func, loop_output_dims, *arginds, concatenate=True, meta=meta, **kwargs)
A:dask.array.gufunc.keys->list(flatten(tmp.__dask_keys__()))
A:dask.array.gufunc.(name, token)->keys[0][0].split('-')
A:dask.array.gufunc.core_output_shape->tuple((core_shapes[d] for d in ocd))
A:dask.array.gufunc.graph->dask.highlevelgraph.HighLevelGraph.from_collections(leaf_name, leaf_dsk, dependencies=[tmp])
A:dask.array.gufunc.leaf_arr->leaf_arr.transpose(tidcs).transpose(tidcs)
A:dask.array.gufunc.self.__doc__->"\n        Bound ``dask.array.gufunc``\n        func: ``{func}``\n        signature: ``'{signature}'``\n\n        Parameters\n        ----------\n        *args : numpy/dask arrays or scalars\n            Arrays to which to apply to ``func``. Core dimensions as specified in\n            ``signature`` need to come last.\n        **kwargs : dict\n            Extra keyword arguments to pass to ``func``\n\n        Returns\n        -------\n        Single dask.array.Array or tuple of dask.array.Array\n        ".format(func=str(self.pyfunc), signature=self.signature)
A:dask.array.gufunc._as_gufunc.__doc__->"\n        Decorator to make ``dask.array.gufunc``\n        signature: ``'{signature}'``\n\n        Parameters\n        ----------\n        pyfunc : callable\n            Function matching signature ``'{signature}'``.\n\n        Returns\n        -------\n        ``dask.array.gufunc``\n        ".format(signature=signature)
dask.array.apply_gufunc(func,signature,*args,axes=None,axis=None,keepdims=False,output_dtypes=None,output_sizes=None,vectorize=None,allow_rechunk=False,meta=None,**kwargs)
dask.array.as_gufunc(signature=None,**kwargs)
dask.array.gufunc(self,pyfunc,*,signature=None,vectorize=False,axes=None,axis=None,keepdims=False,output_sizes=None,output_dtypes=None,allow_rechunk=False,meta=None)
dask.array.gufunc._parse_gufunc_signature(signature)
dask.array.gufunc._validate_normalize_axes(axes,axis,keepdims,input_coredimss,output_coredimss)
dask.array.gufunc.apply_gufunc(func,signature,*args,axes=None,axis=None,keepdims=False,output_dtypes=None,output_sizes=None,vectorize=None,allow_rechunk=False,meta=None,**kwargs)
dask.array.gufunc.as_gufunc(signature=None,**kwargs)
dask.array.gufunc.gufunc(self,pyfunc,*,signature=None,vectorize=False,axes=None,axis=None,keepdims=False,output_sizes=None,output_dtypes=None,allow_rechunk=False,meta=None)
dask.array.gufunc.gufunc.__init__(self,pyfunc,*,signature=None,vectorize=False,axes=None,axis=None,keepdims=False,output_sizes=None,output_dtypes=None,allow_rechunk=False,meta=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/wrap.py----------------------------------------
A:dask.array.wrap.shape->kwargs.get('shape', x.shape)
A:dask.array.wrap.name->kwargs.pop('name', None)
A:dask.array.wrap.chunks->normalize_chunks(chunks, shape, dtype=dtype)
A:dask.array.wrap.dtype->numpy.dtype(dtype)
A:dask.array.wrap.parsed->_parse_wrap_args(func, args, kwargs, shape)
A:dask.array.wrap.func->partial(func, dtype=dtype, **kwargs)
A:dask.array.wrap.out_inddep_ind->tuple(range(len(shape)))
A:dask.array.wrap.graph->core_blockwise(func, name, out_ind, ArrayChunkShapeDep(chunks), dep_ind, numblocks={})
A:dask.array.wrap.meta->meta_from_array(x)
A:dask.array.wrap.keys->product([name], *[range(len(bd)) for bd in chunks])
A:dask.array.wrap.shapes->list(shapes)
A:dask.array.wrap.dsk->dict(zip(keys, vals))
A:dask.array.wrap.f->partial(wrap_func, func_like, **kwargs)
A:dask.array.wrap.w->wrap(wrap_func_shape_as_first_arg)
A:dask.array.wrap.inner->_broadcast_trick_inner(func)
A:dask.array.wrap.ones->dask.array.backends.array_creation_dispatch.register_inplace(backend='numpy', name='ones')(w(broadcast_trick(np.ones_like), dtype='f8'))
A:dask.array.wrap.zeros->dask.array.backends.array_creation_dispatch.register_inplace(backend='numpy', name='zeros')(w(broadcast_trick(np.zeros_like), dtype='f8'))
A:dask.array.wrap.empty->dask.array.backends.array_creation_dispatch.register_inplace(backend='numpy', name='empty')(w(broadcast_trick(np.empty_like), dtype='f8'))
A:dask.array.wrap.w_like->wrap(wrap_func_like)
A:dask.array.wrap.empty_like->w_like(np.empty, func_like=np.empty_like)
A:dask.array.wrap._full->dask.array.backends.array_creation_dispatch.register_inplace(backend='numpy', name='full')(w(broadcast_trick(np.full_like)))
A:dask.array.wrap._full_like->w_like(np.full, func_like=np.full_like)
A:dask.array.wrap._full.__doc__->dask.array.backends.array_creation_dispatch.register_inplace(backend='numpy', name='full')(w(broadcast_trick(np.full_like))).__doc__.replace('>>> np.full_like(y, [0, 0, 255])', '>>> np.full_like(y, [0, 0, 255])  # doctest: +NORMALIZE_WHITESPACE')
A:dask.array.wrap.kwargs['dtype']->type(fill_value)
dask.array.full(shape,fill_value,*args,**kwargs)
dask.array.full_like(a,fill_value,*args,**kwargs)
dask.array.wrap._broadcast_trick_inner(func,shape,meta=(),*args,**kwargs)
dask.array.wrap._parse_wrap_args(func,args,kwargs,shape)
dask.array.wrap.broadcast_trick(func)
dask.array.wrap.full(shape,fill_value,*args,**kwargs)
dask.array.wrap.full_like(a,fill_value,*args,**kwargs)
dask.array.wrap.wrap(wrap_func,func,func_like=None,**kwargs)
dask.array.wrap.wrap_func_like(func,*args,**kwargs)
dask.array.wrap.wrap_func_shape_as_first_arg(func,*args,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/routines.py----------------------------------------
A:dask.array.routines.x->x.rechunk(new_chunks).rechunk(new_chunks)
A:dask.array.routines.tup->tuple((atleast_3d(x) for x in tup))
A:dask.array.routines.ind->argwhere(a)
A:dask.array.routines.out->Array(graph, name, chunks, out_dtype)
A:dask.array.routines.axes->tuple(axes)
A:dask.array.routines.m->asarray_safe(m, like=m)
A:dask.array.routines.axis->validate_axis(axis, arr.ndim)
A:dask.array.routines.sl[ax]->slice(None, None, -1)
A:dask.array.routines.sl->tuple((0 if i in axis else slice(None) for (i, s) in enumerate(a.shape)))
A:dask.array.routines.axes_list->list(range(0, m.ndim))
A:dask.array.routines.tensordot->dask.array.core.tensordot_lookup.dispatch(type(x))
A:dask.array.routines.lhs->from_array(lhs)
A:dask.array.routines.rhs->from_array(rhs)
A:dask.array.routines.left_axes->tuple(left_axes)
A:dask.array.routines.right_axes->tuple(right_axes)
A:dask.array.routines.dt->numpy.promote_types(lhs.dtype, rhs.dtype)
A:dask.array.routines.left_index->list(range(lhs.ndim))
A:dask.array.routines.right_index->list(range(lhs.ndim, lhs.ndim + rhs.ndim))
A:dask.array.routines.intermediate->blockwise(_tensordot, out_index, lhs, left_index, rhs, right_index, dtype=dt, concatenate=concatenate, adjust_chunks=adjust_chunks, axes=(left_axes, right_axes), is_sparse=is_sparse)
A:dask.array.routines.dtype->getattr(values, 'dtype', type(values))
A:dask.array.routines.chunk->numpy.array(f.chunks[ax])
A:dask.array.routines.a->asanyarray(a)
A:dask.array.routines.b->b.flatten().flatten()
A:dask.array.routines.out_ind->tuple(range(a.ndim + 1))
A:dask.array.routines.lhs_ind->tuple(range(a.ndim))
A:dask.array.routines.arr->arr.ravel().ravel()
A:dask.array.routines.test_data->numpy.ones((1,), dtype=arr.dtype)
A:dask.array.routines.test_result->numpy.array(func1d(test_data, *args, **kwargs))
A:dask.array.routines.result->result.reshape(array.shape).reshape(array.shape)
A:dask.array.routines.n->blockwise(_isin_kernel, element_axes + test_axes, element, element_axes, test_elements, test_axes, adjust_chunks={axis: lambda _: 1 for axis in test_axes}, dtype=bool, assume_unique=assume_unique).sum(axis=0)
A:dask.array.routines.prepend->broadcast_to(prepend, tuple(shape))
A:dask.array.routines.shape->broadcast_shapes(x.shape, y.shape)
A:dask.array.routines.append->numpy.broadcast_to(append, tuple(shape))
A:dask.array.routines.sl_1[axis]->slice(1, None)
A:dask.array.routines.sl_2[axis]->slice(None, -1)
A:dask.array.routines.sl_1->tuple(sl_1)
A:dask.array.routines.sl_2->tuple(sl_2)
A:dask.array.routines.ary->asarray(ary)
A:dask.array.routines.aryf->asarray(ary).flatten()
A:dask.array.routines.r->numpy.empty(u.shape, dtype=dt)
A:dask.array.routines.grad->numpy.gradient(x, coord, axis=axis, **grad_kwargs)
A:dask.array.routines.f->f.astype(float).astype(float)
A:dask.array.routines.kwargs['edge_order']->math.ceil(kwargs.get('edge_order', 1))
A:dask.array.routines.token->tokenize(sample, bins, range, weights, density)
A:dask.array.routines.meta->reduction(np.empty((1,) * x.ndim, dtype=x.dtype), **kwargs)
A:dask.array.routines.chunked_counts->blockwise(partial(np.bincount, minlength=minlength), 'i', *args, token=token, meta=meta)
A:dask.array.routines.output->_tree_reduce(chunked_counts, aggregate=partial(_bincount_agg, dtype=meta.dtype), axis=(0,), keepdims=True, dtype=meta.dtype, split_every=split_every, concatenate=False)
A:dask.array.routines.bins->asarray(bins)
A:dask.array.routines.res->numpy.searchsorted(x, y, side=side)
A:dask.array.routines.a_chunk_sizes->array_safe((0, *a.chunks[0]), like=meta_from_array(a))
A:dask.array.routines.a_offsets->asarray(a_chunk_offsets, chunks=1)
A:dask.array.routines.((start_ref, stop_ref, num_ref), deps)->unpack_collections([start, stop, num])
A:dask.array.routines.linspace_graph->dask.highlevelgraph.HighLevelGraph.from_collections(linspace_name, linspace_dsk, dependencies=deps)
A:dask.array.routines.((bins_ref, range_ref), deps)->unpack_collections([bins, range])
A:dask.array.routines.a_keys->flatten(a.__dask_keys__())
A:dask.array.routines.w_keys->flatten(weights.__dask_keys__())
A:dask.array.routines.graph->dask.highlevelgraph.HighLevelGraph.from_collections(name, dsk, dependencies=[x])
A:dask.array.routines.nchunks->len(list(flatten(a.__dask_keys__())))
A:dask.array.routines.mapped->blockwise(_isin_kernel, element_axes + test_axes, element, element_axes, test_elements, test_axes, adjust_chunks={axis: lambda _: 1 for axis in test_axes}, dtype=bool, assume_unique=assume_unique)
A:dask.array.routines.db->asarray(np.diff(bins).astype(float), chunks=n.chunks)
A:dask.array.routines.(counts, edges)->histogramdd((x, y), bins=bins, range=range, normed=normed, weights=weights, density=density)
A:dask.array.routines.dc_bins->is_dask_collection(bins)
A:dask.array.routines.D->len(sample)
A:dask.array.routines.deps->tuple(sample)
A:dask.array.routines.column_zeros->tuple((0 for _ in _range(D)))
A:dask.array.routines.sample_keys->flatten(sample.__dask_keys__())
A:dask.array.routines.all_nbins->tuple(((b.size - 1,) for b in edges))
A:dask.array.routines.width_divider->asarray(width_divider, chunks=n.chunks)
A:dask.array.routines.y->asarray(y)
A:dask.array.routines.X->concatenate((X, y), axis)
A:dask.array.routines.fact->float(N - ddof)
A:dask.array.routines.c->cov(x, y, rowvar)
A:dask.array.routines.d->d.reshape((d.shape[0], 1)).reshape((d.shape[0], 1))
A:dask.array.routines.sqr_d->sqrt(d)
A:dask.array.routines.u->numpy.unique(ar)
A:dask.array.routines.r['inverse']->numpy.arange(len(r), dtype=np.intp)
A:dask.array.routines.ar->ar.ravel().ravel()
A:dask.array.routines.out._chunks->tuple(((np.nan,) * len(c) for c in out.chunks))
A:dask.array.routines.mtches->(ar[:, None] == out['values'][None, :]).astype(np.intp)
A:dask.array.routines.values->ravel(asanyarray(values))
A:dask.array.routines.element->asarray(element)
A:dask.array.routines.test_elements->asarray(test_elements)
A:dask.array.routines.element_axes->tuple(range(element.ndim))
A:dask.array.routines.test_axes->tuple((i + element.ndim for i in range(test_elements.ndim)))
A:dask.array.routines.sl1[i]->slice(s, None)
A:dask.array.routines.sl2[i]->slice(None, s)
A:dask.array.routines.sl1->tuple(sl1)
A:dask.array.routines.sl2->tuple(sl2)
A:dask.array.routines.shape_it->iter(a.shape)
A:dask.array.routines.condition->asarray(condition).astype(bool)
A:dask.array.routines.func->partial(np.isclose, rtol=rtol, atol=atol, equal_nan=equal_nan)
A:dask.array.routines._isnonzero_vec->numpy.vectorize(_isnonzero_vec, otypes=[bool])
A:dask.array.routines.nz->isnonzero(a).flatten()
A:dask.array.routines.unraveled_indices->tuple((empty((0,), dtype=np.intp, chunks=1) for i in shape))
A:dask.array.routines.index_stack->stack(multi_index_arrs)
A:dask.array.routines.multi_index_arrs->broadcast_arrays(*multi_index)
A:dask.array.routines.intermediate_dtype->result_type(*choicelist)
A:dask.array.routines.blockwise_shape->tuple(range(choicelist[0].ndim))
A:dask.array.routines.excess->overflow.sum()
A:dask.array.routines.(partitioned_excess, remainder)->_partition(excess, multiple)
A:dask.array.routines.new_chunks->numpy.array([*new_chunks, *remainder])
A:dask.array.routines.reduction->getattr(np, reduction.__name__)
A:dask.array.routines.aligned->aligned_coarsen_chunks(x.chunks[i], div)
A:dask.array.routines.chunks->tuple((tuple((coarsen_dim(bd, i) for bd in bds if coarsen_dim(bd, i) > 0)) for (i, bds) in enumerate(x.chunks)))
A:dask.array.routines.padded_breaks->concat([[None], breaks, [None]])
A:dask.array.routines.obj->numpy.unique(obj)
A:dask.array.routines.split_arr->split_at_breaks(arr, np.unique(obj), axis)
A:dask.array.routines.values_shape->tuple((len(obj) if axis == n else s for (n, s) in enumerate(arr.shape)))
A:dask.array.routines.values_chunks->tuple((values_bd if axis == n else arr_bd for (n, (arr_bd, values_bd)) in enumerate(zip(arr.chunks, values.chunks))))
A:dask.array.routines.values_breaks->numpy.cumsum(counts[counts > 0])
A:dask.array.routines.split_values->split_at_breaks(values, values_breaks, axis)
A:dask.array.routines.interleaved->list(interleave([split_arr, split_values]))
A:dask.array.routines.tmp->numpy.arange(*obj.indices(arr.shape[axis]))
A:dask.array.routines.target_arr->split_at_breaks(arr, obj, axis)
A:dask.array.routines.avg->asanyarray(a).mean(axis, keepdims=keepdims)
A:dask.array.routines.scl->broadcast_to(scl, avg.shape).copy()
A:dask.array.routines.wgt->wgt.swapaxes(-1, axis).swapaxes(-1, axis)
A:dask.array.routines.result_dtype->result_type(a.dtype, wgt.dtype)
A:dask.array.routines.mask->tri(*m.shape[-2:], k=k - 1, dtype=bool, chunks=m.chunks[-2:], like=meta_from_array(m))
dask.array.allclose(arr1,arr2,rtol=1e-05,atol=1e-08,equal_nan=False)
dask.array.append(arr,values,axis=None)
dask.array.apply_along_axis(func1d,axis,arr,*args,dtype=None,shape=None,**kwargs)
dask.array.apply_over_axes(func,a,axes)
dask.array.argwhere(a)
dask.array.around(x,decimals=0)
dask.array.array(x,dtype=None,ndmin=None,*,like=None)
dask.array.atleast_1d(*arys)
dask.array.atleast_2d(*arys)
dask.array.atleast_3d(*arys)
dask.array.average(a,axis=None,weights=None,returned=False,keepdims=False)
dask.array.bincount(x,weights=None,minlength=0,split_every=None)
dask.array.choose(a,choices)
dask.array.coarsen(reduction,x,axes,trim_excess=False,**kwargs)
dask.array.compress(condition,a,axis=None)
dask.array.corrcoef(x,y=None,rowvar=1)
dask.array.count_nonzero(a,axis=None)
dask.array.cov(m,y=None,rowvar=1,bias=0,ddof=None)
dask.array.delete(arr,obj,axis)
dask.array.diff(a,n=1,axis=-1,prepend=None,append=None)
dask.array.digitize(a,bins,right=False)
dask.array.dot(a,b)
dask.array.dstack(tup,allow_unknown_chunksizes=False)
dask.array.ediff1d(ary,to_end=None,to_begin=None)
dask.array.expand_dims(a,axis)
dask.array.extract(condition,arr)
dask.array.flatnonzero(a)
dask.array.flip(m,axis=None)
dask.array.fliplr(m)
dask.array.flipud(m)
dask.array.gradient(f,*varargs,axis=None,**kwargs)
dask.array.histogram(a,bins=None,range=None,normed=False,weights=None,density=None)
dask.array.histogram2d(x,y,bins=10,range=None,normed=None,weights=None,density=None)
dask.array.histogramdd(sample,bins,range=None,normed=None,weights=None,density=None)
dask.array.hstack(tup,allow_unknown_chunksizes=False)
dask.array.insert(arr,obj,values,axis)
dask.array.isclose(arr1,arr2,rtol=1e-05,atol=1e-08,equal_nan=False)
dask.array.isin(element,test_elements,assume_unique=False,invert=False)
dask.array.isnull(values)
dask.array.matmul(a,b)
dask.array.ndim(a)
dask.array.nonzero(a)
dask.array.notnull(values)
dask.array.outer(a,b)
dask.array.piecewise(x,condlist,funclist,*args,**kw)
dask.array.ptp(a,axis=None)
dask.array.ravel(array_like)
dask.array.ravel_multi_index(multi_index,dims,mode='raise',order='C')
dask.array.result_type(*args)
dask.array.roll(array,shift,axis=None)
dask.array.rot90(m,k=1,axes=(0,1))
dask.array.round(a,decimals=0)
dask.array.routines._asarray_isnull(values)
dask.array.routines._average(a,axis=None,weights=None,returned=False,is_masked=False,keepdims=False)
dask.array.routines._bincount_agg(bincounts,dtype,**kwargs)
dask.array.routines._block_hist(x,bins,range=None,weights=None)
dask.array.routines._block_histogramdd_multiarg(*args)
dask.array.routines._block_histogramdd_rect(sample,bins,range,weights)
dask.array.routines._chunk_sum(a,axis=None,dtype=None,keepdims=None)
dask.array.routines._gradient_kernel(x,block_id,coord,axis,array_locs,grad_kwargs)
dask.array.routines._inner_apply_along_axis(arr,func1d,func1d_axis,func1d_args,func1d_kwargs)
dask.array.routines._int_piecewise(x,*condlist,**kwargs)
dask.array.routines._isin_kernel(element,test_elements,assume_unique=False)
dask.array.routines._isnonzero_vec(v)
dask.array.routines._linspace_from_delayed(start,stop,num=50)
dask.array.routines._matmul(a,b)
dask.array.routines._partition(total:int,divisor:int)->tuple[tuple[int, ...], tuple[int, ...]]
dask.array.routines._searchsorted_block(x,y,side)
dask.array.routines._select(*args,**kwargs)
dask.array.routines._sum_wo_cat(a,axis=None,dtype=None)
dask.array.routines._take_dask_array_from_numpy(a,indices,axis)
dask.array.routines._tensordot(a,b,axes,is_sparse)
dask.array.routines._tensordot_is_sparse(x)
dask.array.routines._unique_internal(ar,indices,counts,return_inverse=False)
dask.array.routines._unravel_index_kernel(indices,func_kwargs)
dask.array.routines.aligned_coarsen_chunks(chunks:list[int],multiple:int)->tuple[int, ...]
dask.array.routines.allclose(arr1,arr2,rtol=1e-05,atol=1e-08,equal_nan=False)
dask.array.routines.append(arr,values,axis=None)
dask.array.routines.apply_along_axis(func1d,axis,arr,*args,dtype=None,shape=None,**kwargs)
dask.array.routines.apply_over_axes(func,a,axes)
dask.array.routines.argwhere(a)
dask.array.routines.around(x,decimals=0)
dask.array.routines.array(x,dtype=None,ndmin=None,*,like=None)
dask.array.routines.atleast_1d(*arys)
dask.array.routines.atleast_2d(*arys)
dask.array.routines.atleast_3d(*arys)
dask.array.routines.average(a,axis=None,weights=None,returned=False,keepdims=False)
dask.array.routines.bincount(x,weights=None,minlength=0,split_every=None)
dask.array.routines.choose(a,choices)
dask.array.routines.coarsen(reduction,x,axes,trim_excess=False,**kwargs)
dask.array.routines.compress(condition,a,axis=None)
dask.array.routines.corrcoef(x,y=None,rowvar=1)
dask.array.routines.count_nonzero(a,axis=None)
dask.array.routines.cov(m,y=None,rowvar=1,bias=0,ddof=None)
dask.array.routines.delete(arr,obj,axis)
dask.array.routines.diff(a,n=1,axis=-1,prepend=None,append=None)
dask.array.routines.digitize(a,bins,right=False)
dask.array.routines.dot(a,b)
dask.array.routines.dstack(tup,allow_unknown_chunksizes=False)
dask.array.routines.ediff1d(ary,to_end=None,to_begin=None)
dask.array.routines.expand_dims(a,axis)
dask.array.routines.extract(condition,arr)
dask.array.routines.flatnonzero(a)
dask.array.routines.flip(m,axis=None)
dask.array.routines.fliplr(m)
dask.array.routines.flipud(m)
dask.array.routines.gradient(f,*varargs,axis=None,**kwargs)
dask.array.routines.histogram(a,bins=None,range=None,normed=False,weights=None,density=None)
dask.array.routines.histogram2d(x,y,bins=10,range=None,normed=None,weights=None,density=None)
dask.array.routines.histogramdd(sample,bins,range=None,normed=None,weights=None,density=None)
dask.array.routines.hstack(tup,allow_unknown_chunksizes=False)
dask.array.routines.insert(arr,obj,values,axis)
dask.array.routines.isclose(arr1,arr2,rtol=1e-05,atol=1e-08,equal_nan=False)
dask.array.routines.iscomplexobj(x)
dask.array.routines.isin(element,test_elements,assume_unique=False,invert=False)
dask.array.routines.isnonzero(a)
dask.array.routines.isnull(values)
dask.array.routines.matmul(a,b)
dask.array.routines.ndim(a)
dask.array.routines.nonzero(a)
dask.array.routines.notnull(values)
dask.array.routines.outer(a,b)
dask.array.routines.piecewise(x,condlist,funclist,*args,**kw)
dask.array.routines.ptp(a,axis=None)
dask.array.routines.ravel(array_like)
dask.array.routines.ravel_multi_index(multi_index,dims,mode='raise',order='C')
dask.array.routines.result_type(*args)
dask.array.routines.roll(array,shift,axis=None)
dask.array.routines.rot90(m,k=1,axes=(0,1))
dask.array.routines.round(a,decimals=0)
dask.array.routines.searchsorted(a,v,side='left',sorter=None)
dask.array.routines.select(condlist,choicelist,default=0)
dask.array.routines.shape(array)
dask.array.routines.split_at_breaks(array,breaks,axis=0)
dask.array.routines.squeeze(a,axis=None)
dask.array.routines.swapaxes(a,axis1,axis2)
dask.array.routines.take(a,indices,axis=0)
dask.array.routines.tensordot(lhs,rhs,axes=2)
dask.array.routines.transpose(a,axes=None)
dask.array.routines.tril(m,k=0)
dask.array.routines.tril_indices(n,k=0,m=None,chunks='auto')
dask.array.routines.tril_indices_from(arr,k=0)
dask.array.routines.triu(m,k=0)
dask.array.routines.triu_indices(n,k=0,m=None,chunks='auto')
dask.array.routines.triu_indices_from(arr,k=0)
dask.array.routines.union1d(ar1,ar2)
dask.array.routines.unique(ar,return_index=False,return_inverse=False,return_counts=False)
dask.array.routines.unique_no_structured_arr(ar,return_index=False,return_inverse=False,return_counts=False)
dask.array.routines.unravel_index(indices,shape,order='C')
dask.array.routines.variadic_choose(a,*choices)
dask.array.routines.vdot(a,b)
dask.array.routines.vstack(tup,allow_unknown_chunksizes=False)
dask.array.routines.where(condition,x=None,y=None)
dask.array.searchsorted(a,v,side='left',sorter=None)
dask.array.select(condlist,choicelist,default=0)
dask.array.shape(array)
dask.array.squeeze(a,axis=None)
dask.array.swapaxes(a,axis1,axis2)
dask.array.take(a,indices,axis=0)
dask.array.tensordot(lhs,rhs,axes=2)
dask.array.transpose(a,axes=None)
dask.array.tril(m,k=0)
dask.array.tril_indices(n,k=0,m=None,chunks='auto')
dask.array.tril_indices_from(arr,k=0)
dask.array.triu(m,k=0)
dask.array.triu_indices(n,k=0,m=None,chunks='auto')
dask.array.triu_indices_from(arr,k=0)
dask.array.union1d(ar1,ar2)
dask.array.unique(ar,return_index=False,return_inverse=False,return_counts=False)
dask.array.unique_no_structured_arr(ar,return_index=False,return_inverse=False,return_counts=False)
dask.array.unravel_index(indices,shape,order='C')
dask.array.vdot(a,b)
dask.array.vstack(tup,allow_unknown_chunksizes=False)
dask.array.where(condition,x=None,y=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/linalg.py----------------------------------------
A:dask.array.linalg.k_0->min([m, n])
A:dask.array.linalg.(qq, rr)->numpy.linalg.qr(np.ones(shape=(1, 1), dtype=data.dtype))
A:dask.array.linalg.layers->data.__dask_graph__().layers.copy()
A:dask.array.linalg.dependencies->data.__dask_graph__().dependencies.copy()
A:dask.array.linalg.dsk_qr_st1->blockwise(_wrapped_qr, name_qr_st1, 'ij', data.name, 'ij', numblocks={data.name: numblocks})
A:dask.array.linalg.dependencies[name_qr_st1]->set(data.__dask_layers__())
A:dask.array.linalg.prospective_blocks->numpy.ceil(single_core_compute_m / cr_max)
A:dask.array.linalg.n_q->min(m_q, cc)
A:dask.array.linalg.vchunks_rstacked->tuple((sum(map(lambda x: x[1], sub_block_info)) for sub_block_info in all_blocks))
A:dask.array.linalg.graph->dask.highlevelgraph.HighLevelGraph.from_collections(sname, sdsk, dependencies=[rt, r])
A:dask.array.linalg.r_stacked_meta->meta_from_array(data, len((sum(vchunks_rstacked), n)), dtype=rr.dtype)
A:dask.array.linalg.r_stacked->Array(graph, name_r_stacked, shape=(sum(vchunks_rstacked), n), chunks=(vchunks_rstacked, n), meta=r_stacked_meta)
A:dask.array.linalg.(q_inner, r_inner)->tsqr(r_stacked, _max_vchunk_size=cr_max)
A:dask.array.linalg.dependencies[name_q_st2]->set(q_inner.__dask_layers__())
A:dask.array.linalg.dependencies[name_r_st2]->set(r_inner.__dask_layers__())
A:dask.array.linalg.dsk_q_st3->blockwise(np.dot, name_q_st3, 'ij', name_q_st1, 'ij', name_q_st2, 'ij', numblocks={name_q_st1: numblocks, name_q_st2: numblocks})
A:dask.array.linalg.dsk_qr_st2->blockwise(np.linalg.qr, name_qr_st2, 'ij', name_r_st1_stacked, 'ij', numblocks={name_r_st1_stacked: (1, 1)})
A:dask.array.linalg.deps->set()
A:dask.array.linalg.dsk_q_blockslices->tlz.merge(dsk_n, dsk_q2_shapes, dsk_q2_cumsum, dsk_block_slices)
A:dask.array.linalg.q_meta->meta_from_array(data, len(q_shape), qq.dtype)
A:dask.array.linalg.r_meta->meta_from_array(data, len(r_shape), rr.dtype)
A:dask.array.linalg.q->q.persist().persist()
A:dask.array.linalg.r->svd(x)[1][None].min(keepdims=keepdims)
A:dask.array.linalg.dsk_svd_st2->blockwise(np.linalg.svd, name_svd_st2, 'ij', name_r_st2, 'ij', numblocks={name_r_st2: (1, 1)})
A:dask.array.linalg.dsk_u_st4->blockwise(dotmany, name_u_st4, 'ij', name_q_st3, 'ik', name_u_st2, 'kj', numblocks={name_q_st3: numblocks, name_u_st2: (1, 1)})
A:dask.array.linalg.(uu, ss, vvh)->numpy.linalg.svd(np.ones(shape=(1, 1), dtype=data.dtype))
A:dask.array.linalg.k->min(a.shape)
A:dask.array.linalg.d_vh->max(m_vh, n_vh)
A:dask.array.linalg.u_meta->meta_from_array(data, len((m_u, n_u)), uu.dtype)
A:dask.array.linalg.s_meta->meta_from_array(data, len((n_s,)), ss.dtype)
A:dask.array.linalg.vh_meta->meta_from_array(data, len((d_vh, d_vh)), vvh.dtype)
A:dask.array.linalg.u->Array(graph, name_u, shape=a.shape, chunks=a.chunks, meta=uu_meta)
A:dask.array.linalg.s->Array(graph, sname, shape=(r.shape[0],), chunks=r.shape[0], meta=meta)
A:dask.array.linalg.vh->Array(graph, name_v_st2, shape=(d_vh, d_vh), chunks=((n,), (n,)), meta=vh_meta)
A:dask.array.linalg.dependencies[name_A_1]->set(data.__dask_layers__())
A:dask.array.linalg.dependencies[name_A_rest]->set()
A:dask.array.linalg.Q_meta->meta_from_array(data, len((m, min(m, n))), dtype=qq.dtype)
A:dask.array.linalg.R_1_meta->meta_from_array(data, len((min(m, n), cc)), dtype=rr.dtype)
A:dask.array.linalg.Q->Array(graph, name_Q, shape=(m, min(m, n)), chunks=(m, min(m, n)), meta=Q_meta)
A:dask.array.linalg.R_1->Array(graph, name_R_1, shape=(min(m, n), cc), chunks=(cr, cc), meta=R_1_meta)
A:dask.array.linalg.A_rest_meta->meta_from_array(data, len((min(m, n), n - cc)), dtype=rr.dtype)
A:dask.array.linalg.A_rest->Array(graph, name_A_rest, shape=(min(m, n), n - cc), chunks=(cr, data.chunks[1][1:]), meta=A_rest_meta)
A:dask.array.linalg.R->concatenate(Rs, axis=1)
A:dask.array.linalg.comp_level->compression_level(min(m, n), q, n_oversamples=n_oversamples)
A:dask.array.linalg.state->default_rng(seed)
A:dask.array.linalg.omega->default_rng(seed).standard_normal(size=(n, comp_level), chunks=(data.chunks[1], (comp_level,))).astype(datatype, copy=False)
A:dask.array.linalg.mat_h->data.dot(tmp)
A:dask.array.linalg.tmp->tmp.persist().persist()
A:dask.array.linalg.(q, _)->tsqr(data.dot(q))
A:dask.array.linalg.comp->comp.persist().persist()
A:dask.array.linalg.a_compressed->comp.persist().persist().dot(a)
A:dask.array.linalg.(v, s, u)->tsqr(a_compressed.T, compute_svd=True)
A:dask.array.linalg.(u, v)->svd_flip(u, v)
A:dask.array.linalg.(mu, ms, mv)->numpy.linalg.svd(np.ones_like(a._meta, shape=(1, 1), dtype=a._meta.dtype))
A:dask.array.linalg.(u, s, v)->tsqr(a, compute_svd=True)
A:dask.array.linalg.v->from_delayed(v, shape=(k, n), meta=mv)
A:dask.array.linalg.(vt, s, ut)->tsqr(a.T, compute_svd=True)
A:dask.array.linalg.vdim->len(a.chunks[0])
A:dask.array.linalg.hdim->len(a.chunks[1])
A:dask.array.linalg.token->tokenize(a, b)
A:dask.array.linalg.(pp, ll, uu)->scipy.linalg.lu(np.ones(shape=(1, 1), dtype=a.dtype))
A:dask.array.linalg.pp_meta->meta_from_array(a, dtype=pp.dtype)
A:dask.array.linalg.ll_meta->meta_from_array(a, dtype=ll.dtype)
A:dask.array.linalg.uu_meta->meta_from_array(a, dtype=uu.dtype)
A:dask.array.linalg.p->Array(graph, name_p, shape=a.shape, chunks=a.chunks, meta=pp_meta)
A:dask.array.linalg.l->Array(graph, name_l, shape=a.shape, chunks=a.chunks, meta=ll_meta)
A:dask.array.linalg.vchunks->len(a.chunks[1])
A:dask.array.linalg.target->_b_init(i, j)
A:dask.array.linalg.a_meta->meta_from_array(a)
A:dask.array.linalg.b_meta->meta_from_array(b)
A:dask.array.linalg.res->_solve_triangular_lower(array_safe([[1, 0], [1, 2]], dtype=a.dtype, like=a_meta), array_safe([0, 1], dtype=b.dtype, like=b_meta))
A:dask.array.linalg.meta->meta_from_array(residuals, 1)
A:dask.array.linalg.(l, u)->_cholesky(a)
A:dask.array.linalg.(p, l, u)->lu(a)
A:dask.array.linalg.b->Array(graph, name_p, shape=a.shape, chunks=a.chunks, meta=pp_meta).T.dot(b)
A:dask.array.linalg.uy->solve_triangular(l, b, lower=True)
A:dask.array.linalg.graph_upper->dask.highlevelgraph.HighLevelGraph.from_collections(name_upper, dsk, dependencies=[a])
A:dask.array.linalg.graph_lower->dask.highlevelgraph.HighLevelGraph.from_collections(name, dsk, dependencies=[a])
A:dask.array.linalg.cho->numpy.linalg.cholesky(array_safe([[1, 2], [2, 5]], dtype=a.dtype, like=a_meta))
A:dask.array.linalg.lower->Array(graph_lower, name, shape=a.shape, chunks=a.chunks, meta=meta)
A:dask.array.linalg.upper->Array(graph_upper, name_upper, shape=a.shape, chunks=a.chunks, meta=meta)
A:dask.array.linalg.(q, r)->qr(a)
A:dask.array.linalg.x->solve_triangular(r, q.T.conj().dot(b))
A:dask.array.linalg.residuals->abs(residuals ** 2).sum(axis=0, keepdims=b.ndim == 1)
A:dask.array.linalg.rank->Array(graph, rname, shape=(), chunks=(), dtype=int)
A:dask.array.linalg.rt->svd(x)[1][None].min(keepdims=keepdims).T.conj()
A:dask.array.linalg.axis->tuple(axis)
dask.array.linalg._cholesky(a)
dask.array.linalg._cholesky_lower(a)
dask.array.linalg._cumsum_blocks(it)
dask.array.linalg._cumsum_part(last,new)
dask.array.linalg._nanmin(m,n)
dask.array.linalg._reverse(x)
dask.array.linalg._solve_triangular_lower(a,b)
dask.array.linalg._wrapped_qr(a)
dask.array.linalg.cholesky(a,lower=False)
dask.array.linalg.compression_level(n,q,n_oversamples=10,min_subspace_size=20)
dask.array.linalg.compression_matrix(data,q,iterator='power',n_power_iter=0,n_oversamples=10,seed=None,compute=False)
dask.array.linalg.inv(a)
dask.array.linalg.lstsq(a,b)
dask.array.linalg.lu(a)
dask.array.linalg.norm(x,ord=None,axis=None,keepdims=False)
dask.array.linalg.qr(a)
dask.array.linalg.sfqr(data,name=None)
dask.array.linalg.solve(a,b,sym_pos=None,assume_a='gen')
dask.array.linalg.solve_triangular(a,b,lower=False)
dask.array.linalg.svd(a,coerce_signs=True)
dask.array.linalg.svd_compressed(a,k,iterator='power',n_power_iter=0,n_oversamples=10,seed=None,compute=False,coerce_signs=True)
dask.array.linalg.tsqr(data,compute_svd=False,_max_vchunk_size=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/ufunc.py----------------------------------------
A:dask.array.ufunc.self._ufunc->numpy.frompyfunc(func, nin, nout)
A:dask.array.ufunc.self._name->funcname(func)
A:dask.array.ufunc.o->set(dir(type(self)))
A:dask.array.ufunc.result->dsk._elemwise(self._ufunc, *args, **kwargs)
A:dask.array.ufunc.A_is_dask->is_dask_collection(A)
A:dask.array.ufunc.B_is_dask->is_dask_collection(B)
A:dask.array.ufunc.A->asarray(A)
A:dask.array.ufunc.B->asarray(B)
A:dask.array.ufunc.out_inds->tuple(range(ndim))
A:dask.array.ufunc.dtype->apply_infer_dtype(self._ufunc.outer, [A, B], kwargs, 'ufunc.outer', suggest_dtype=False)
A:dask.array.ufunc.func->partial(self._ufunc.outer, dtype=kwargs.pop('dtype'))
A:dask.array.ufunc.add->ufunc(np.add)
A:dask.array.ufunc.subtract->ufunc(np.subtract)
A:dask.array.ufunc.multiply->ufunc(np.multiply)
A:dask.array.ufunc.divide->ufunc(np.divide)
A:dask.array.ufunc.logaddexp->ufunc(np.logaddexp)
A:dask.array.ufunc.logaddexp2->ufunc(np.logaddexp2)
A:dask.array.ufunc.true_divide->ufunc(np.true_divide)
A:dask.array.ufunc.floor_divide->ufunc(np.floor_divide)
A:dask.array.ufunc.negative->ufunc(np.negative)
A:dask.array.ufunc.positive->ufunc(np.positive)
A:dask.array.ufunc.power->ufunc(np.power)
A:dask.array.ufunc.float_power->ufunc(np.float_power)
A:dask.array.ufunc.remainder->ufunc(np.remainder)
A:dask.array.ufunc.mod->ufunc(np.mod)
A:dask.array.ufunc.conjconjugate->ufunc(np.conjugate)
A:dask.array.ufunc.exp->ufunc(np.exp)
A:dask.array.ufunc.exp2->ufunc(np.exp2)
A:dask.array.ufunc.log->ufunc(np.log)
A:dask.array.ufunc.log2->ufunc(np.log2)
A:dask.array.ufunc.log10->ufunc(np.log10)
A:dask.array.ufunc.log1p->ufunc(np.log1p)
A:dask.array.ufunc.expm1->ufunc(np.expm1)
A:dask.array.ufunc.sqrt->ufunc(np.sqrt)
A:dask.array.ufunc.square->ufunc(np.square)
A:dask.array.ufunc.cbrt->ufunc(np.cbrt)
A:dask.array.ufunc.reciprocal->ufunc(np.reciprocal)
A:dask.array.ufunc.sin->ufunc(np.sin)
A:dask.array.ufunc.cos->ufunc(np.cos)
A:dask.array.ufunc.tan->ufunc(np.tan)
A:dask.array.ufunc.arcsin->ufunc(np.arcsin)
A:dask.array.ufunc.arccos->ufunc(np.arccos)
A:dask.array.ufunc.arctan->ufunc(np.arctan)
A:dask.array.ufunc.arctan2->ufunc(np.arctan2)
A:dask.array.ufunc.hypot->ufunc(np.hypot)
A:dask.array.ufunc.sinh->ufunc(np.sinh)
A:dask.array.ufunc.cosh->ufunc(np.cosh)
A:dask.array.ufunc.tanh->ufunc(np.tanh)
A:dask.array.ufunc.arcsinh->ufunc(np.arcsinh)
A:dask.array.ufunc.arccosh->ufunc(np.arccosh)
A:dask.array.ufunc.arctanh->ufunc(np.arctanh)
A:dask.array.ufunc.deg2rad->ufunc(np.deg2rad)
A:dask.array.ufunc.rad2deg->ufunc(np.rad2deg)
A:dask.array.ufunc.greater->ufunc(np.greater)
A:dask.array.ufunc.greater_equal->ufunc(np.greater_equal)
A:dask.array.ufunc.less->ufunc(np.less)
A:dask.array.ufunc.less_equal->ufunc(np.less_equal)
A:dask.array.ufunc.not_equal->ufunc(np.not_equal)
A:dask.array.ufunc.equal->ufunc(np.equal)
A:dask.array.ufunc.isneginf->partial(equal, -np.inf)
A:dask.array.ufunc.isposinf->partial(equal, np.inf)
A:dask.array.ufunc.logical_and->ufunc(np.logical_and)
A:dask.array.ufunc.logical_or->ufunc(np.logical_or)
A:dask.array.ufunc.logical_xor->ufunc(np.logical_xor)
A:dask.array.ufunc.logical_not->ufunc(np.logical_not)
A:dask.array.ufunc.maximum->ufunc(np.maximum)
A:dask.array.ufunc.minimum->ufunc(np.minimum)
A:dask.array.ufunc.fmax->ufunc(np.fmax)
A:dask.array.ufunc.fmin->ufunc(np.fmin)
A:dask.array.ufunc.bitwise_and->ufunc(np.bitwise_and)
A:dask.array.ufunc.bitwise_or->ufunc(np.bitwise_or)
A:dask.array.ufunc.bitwise_xor->ufunc(np.bitwise_xor)
A:dask.array.ufunc.bitwise_not->ufunc(np.bitwise_not)
A:dask.array.ufunc.left_shift->ufunc(np.left_shift)
A:dask.array.ufunc.right_shift->ufunc(np.right_shift)
A:dask.array.ufunc.isfinite->ufunc(np.isfinite)
A:dask.array.ufunc.isinf->ufunc(np.isinf)
A:dask.array.ufunc.isnan->ufunc(np.isnan)
A:dask.array.ufunc.signbit->ufunc(np.signbit)
A:dask.array.ufunc.copysign->ufunc(np.copysign)
A:dask.array.ufunc.nextafter->ufunc(np.nextafter)
A:dask.array.ufunc.spacing->ufunc(np.spacing)
A:dask.array.ufunc.ldexp->ufunc(np.ldexp)
A:dask.array.ufunc.fmod->ufunc(np.fmod)
A:dask.array.ufunc.floor->ufunc(np.floor)
A:dask.array.ufunc.ceil->ufunc(np.ceil)
A:dask.array.ufunc.trunc->ufunc(np.trunc)
A:dask.array.ufunc.degrees->ufunc(np.degrees)
A:dask.array.ufunc.radians->ufunc(np.radians)
A:dask.array.ufunc.rint->ufunc(np.rint)
A:dask.array.ufunc.fabs->ufunc(np.fabs)
A:dask.array.ufunc.sign->ufunc(np.sign)
A:dask.array.ufunc.absolute->ufunc(np.absolute)
A:dask.array.ufunc.clip->wrap_elemwise(np.clip)
A:dask.array.ufunc.isreal->wrap_elemwise(np.isreal)
A:dask.array.ufunc.iscomplex->wrap_elemwise(np.iscomplex)
A:dask.array.ufunc.real->wrap_elemwise(np.real)
A:dask.array.ufunc.imag->wrap_elemwise(np.imag)
A:dask.array.ufunc.fix->wrap_elemwise(np.fix)
A:dask.array.ufunc.i0->wrap_elemwise(np.i0)
A:dask.array.ufunc.sinc->wrap_elemwise(np.sinc)
A:dask.array.ufunc.nan_to_num->wrap_elemwise(np.nan_to_num)
A:dask.array.ufunc.deg->bool(deg)
A:dask.array.ufunc.tmp->elemwise(np.modf, x, dtype=object)
A:dask.array.ufunc.a->numpy.ones_like(getattr(x, '_meta', x), shape=(1,) * x.ndim, dtype=x.dtype)
A:dask.array.ufunc.(l, r)->numpy.modf(a)
A:dask.array.ufunc.graph->dask.highlevelgraph.HighLevelGraph.from_collections(right, rdsk, dependencies=[tmp])
A:dask.array.ufunc.L->Array(graph, left, chunks=tmp.chunks, meta=l)
A:dask.array.ufunc.R->Array(graph, right, chunks=tmp.chunks, meta=r)
dask.array.angle(x,deg=0)
dask.array.divmod(x,y)
dask.array.frexp(x)
dask.array.frompyfunc(func,nin,nout)
dask.array.modf(x)
dask.array.ufunc.angle(x,deg=0)
dask.array.ufunc.da_frompyfunc(self,func,nin,nout)
dask.array.ufunc.da_frompyfunc.__dask_tokenize__(self)
dask.array.ufunc.da_frompyfunc.__dir__(self)
dask.array.ufunc.da_frompyfunc.__getattr__(self,a)
dask.array.ufunc.da_frompyfunc.__init__(self,func,nin,nout)
dask.array.ufunc.da_frompyfunc.__reduce__(self)
dask.array.ufunc.da_frompyfunc.__repr__(self)
dask.array.ufunc.divmod(x,y)
dask.array.ufunc.frexp(x)
dask.array.ufunc.frompyfunc(func,nin,nout)
dask.array.ufunc.modf(x)
dask.array.ufunc.ufunc(self,ufunc)
dask.array.ufunc.ufunc.__dir__(self)
dask.array.ufunc.ufunc.__getattr__(self,key)
dask.array.ufunc.ufunc.__init__(self,ufunc)
dask.array.ufunc.ufunc.__repr__(self)
dask.array.ufunc.ufunc.outer(self,A,B,**kwargs)
dask.array.ufunc.wrap_elemwise(numpy_ufunc,source=np)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/chunk_types.py----------------------------------------
dask.array.chunk_types.is_valid_array_chunk(array)
dask.array.chunk_types.is_valid_chunk_type(type)
dask.array.chunk_types.register_chunk_type(type)
dask.array.register_chunk_type(type)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/slicing.py----------------------------------------
A:dask.array.slicing.colon->slice(None, None, None)
A:dask.array.slicing.ind2->int(ind)
A:dask.array.slicing.index_array->asanyarray_safe(ind, like=ind)
A:dask.array.slicing.nonzero->numpy.nonzero(index_array)
A:dask.array.slicing.int_index->asanyarray_safe(ind, like=ind).astype(np.intp)
A:dask.array.slicing.check_int->numpy.isclose(index_array, int_index)
A:dask.array.slicing.blockdims->tuple(map(tuple, blockdims))
A:dask.array.slicing.suffixes->product(*[range(len(bd)) for bd in blockdims])
A:dask.array.slicing.not_none_count->sum((i is not None for i in index))
A:dask.array.slicing.(dsk_out, bd_out)->slice_with_newaxes(out_name, in_name, blockdims, index, itemsize)
A:dask.array.slicing.bd_out->tuple(map(tuple, bd_out))
A:dask.array.slicing.index2->numpy.empty_like(index)
A:dask.array.slicing.where_none_orig->list(where_none)
A:dask.array.slicing.n->sum((isinstance(ind, Integral) for ind in index[:x]))
A:dask.array.slicing.(dsk, blockdims2)->slice_slices_and_integers(tmp, in_name, blockdims, index_without_list)
A:dask.array.slicing.expand->expander(where_none)
A:dask.array.slicing.expand_orig->expander(where_none_orig)
A:dask.array.slicing.dsk3->merge(dsk, dsk2)
A:dask.array.slicing.blockdims3->expand(blockdims2, (1,))
A:dask.array.slicing.index->numpy.where(index < 0, index + size, index)
A:dask.array.slicing.index[where_list.pop()]->slice(0, 0, 1)
A:dask.array.slicing.index_without_list->tuple((slice(None, None, None) if is_arraylike(i) else i for i in index))
A:dask.array.slicing.(blockdims2, dsk3)->take(out_name, in_name, blockdims, index[where_list[0]], itemsize, axis=axis)
A:dask.array.slicing.(blockdims2, dsk2)->take(out_name, tmp, blockdims2, index[axis], 8, axis=axis2)
A:dask.array.slicing.shape->tuple((cached_cumsum(dim, initial_zero=True)[-1] for dim in blockdims))
A:dask.array.slicing.block_slices->list(map(_slice_1d, shape, blockdims, index))
A:dask.array.slicing.in_names->list(product([in_name], *[pluck(0, s) for s in sorted_block_slices]))
A:dask.array.slicing.out_names->list(product([out_name], *[range(len(d))[::-1] if i.step and i.step < 0 else range(len(d)) for (d, i) in zip(block_slices, index) if not isinstance(i, Integral)]))
A:dask.array.slicing.all_slices->list(product(*[pluck(1, s) for s in sorted_block_slices]))
A:dask.array.slicing.chunk_boundaries->cached_cumsum(lengths)
A:dask.array.slicing.i->concatenate_array_chunks(i)
A:dask.array.slicing.d->dict()
A:dask.array.slicing.istart->min(istart + 1, len(chunk_boundaries) - 1)
A:dask.array.slicing.istop->max(istop - 1, -1)
A:dask.array.slicing.d[i]->slice(rstart - chunk_stop, max(chunk_start - chunk_stop - 1, stop - chunk_stop), step)
A:dask.array.slicing.d[k]->slice(None, None, None)
A:dask.array.slicing.d[0]->slice(0, 0, 1)
A:dask.array.slicing.seq->numpy.asanyarray(seq)
A:dask.array.slicing.left->''.join(left)
A:dask.array.slicing.right->numpy.cumsum(sizes, out=left[1:])
A:dask.array.slicing.locations->numpy.empty(len(sizes) + 1, dtype=int)
A:dask.array.slicing.locations[1:]->numpy.searchsorted(seq, right)
A:dask.array.slicing.cum_chunks->asarray_safe(cum_chunks, like=index)
A:dask.array.slicing.chunk_locations->chunk_locations.tolist().tolist()
A:dask.array.slicing.extra->asarray_safe([0], like=where)
A:dask.array.slicing.c_loc->asarray_safe([len(chunk_locations)], like=where)
A:dask.array.slicing.where->numpy.concatenate([extra, where, c_loc])
A:dask.array.slicing.plan->slicing_plan(chunks[axis], index)
A:dask.array.slicing.factor->math.ceil(len(plan) / len(chunks[axis]))
A:dask.array.slicing.nbytes->dask.utils.parse_bytes(config.get('array.chunk-size'))
A:dask.array.slicing.other_numel->math.prod((max(x) for x in other_chunks))
A:dask.array.slicing.maxsize->math.ceil(nbytes / (other_numel * itemsize))
A:dask.array.slicing.split->dask.config.get('array.slicing.split-large-chunks', None)
A:dask.array.slicing.index_length->len(index_list)
A:dask.array.slicing.index_sublist->numpy.array_split(index_list, math.ceil(index_length / maxsize))
A:dask.array.slicing.index_list->numpy.array(index_list)
A:dask.array.slicing.indims->list(dims)
A:dask.array.slicing.indims[axis]->list(range(len(where_index)))
A:dask.array.slicing.keys->list(product([outname], *indims))
A:dask.array.slicing.outdims->list(dims)
A:dask.array.slicing.slices->slices_from_chunks(chunks)
A:dask.array.slicing.inkeys->list(product([inname], *outdims))
A:dask.array.slicing.chunks2->list(chunks)
A:dask.array.slicing.chunks2[axis]->tuple(map(len, index_lists))
A:dask.array.slicing.dsk->merge(dict(v.dask), dsk)
A:dask.array.slicing.ind->numpy.asanyarray(ind)
A:dask.array.slicing.decl->decl.format(**locals()).format(**locals())
A:dask.array.slicing.pairs->sorted(_slice_1d(dim_shape, lengths, index).items(), key=itemgetter(0))
A:dask.array.slicing.(start, stop, step)->slice(start, stop, -1).indices(size)
A:dask.array.slicing.idx->posify_index(none_shape, idx)
A:dask.array.slicing.x->x.copy().copy()
A:dask.array.slicing.offset->Array(offset.dask, offset.name, (x.chunks[axis],), offset.dtype, meta=x._meta)
A:dask.array.slicing.x_axes->tuple(range(x.ndim))
A:dask.array.slicing.p->blockwise(chunk.slice_with_int_dask_array, p_axes, x, x_axes, idx, idx_axes, offset, offset_axes, x_size=x.shape[axis], axis=axis, dtype=x.dtype, meta=x._meta)
A:dask.array.slicing.y->elemwise(getitem, x, *index, dtype=x.dtype)
A:dask.array.slicing.graph->dask.highlevelgraph.HighLevelGraph.from_collections(name, d, dependencies=[x])
A:dask.array.slicing.arginds->list(concat(arginds))
A:dask.array.slicing.out->blockwise(getitem_variadic, tuple(range(x.ndim)), x, tuple(range(x.ndim)), *arginds, dtype=x.dtype)
A:dask.array.slicing.out._chunks->tuple(chunks)
A:dask.array.slicing.offsets->numpy.roll(np.cumsum(chunks[0]), 1)
A:dask.array.slicing.index3->numpy.empty_like(index)
A:dask.array.slicing.b->numpy.sort(a)
A:dask.array.slicing.(index2, index3)->make_block_sorted_slices(index, chunks1)
A:dask.array.slicing.parsed_indices->list(normalize_index(indices, shape))
A:dask.array.slicing.is_slice->isinstance(index, slice)
A:dask.array.slicing.(div, mod)->divmod(stop - start, step)
A:dask.array.slicing.value_ndim->len(value_shape)
A:dask.array.slicing.(indices, implied_shape, reverse, implied_shape_positions)->parse_assignment_indices(indices, array_shape)
A:dask.array.slicing.array_locations->product(*array_locations)
A:dask.array.slicing.in_keys->list(flatten(array.__dask_keys__()))
A:dask.array.slicing.integer_index->isinstance(index, int)
A:dask.array.slicing.block_index->block_index_from_1d_index(dim, loc0, loc1, is_bool)
A:dask.array.slicing.(block_index_size, rem)->divmod(stop - start, step)
A:dask.array.slicing.pre->numpy.where(index < 0, index + size, index).indices(loc0)
A:dask.array.slicing.(n_preceeding, rem)->divmod(pre[1] - pre[0], step)
A:dask.array.slicing.block_index_size->block_index_shape_from_1d_bool_index(dim, loc0, loc1)
A:dask.array.slicing.n_preceeding->n_preceeding_from_1d_bool_index(dim, loc0)
A:dask.array.slicing.value_indices[i]->slice(start, stop, -1)
A:dask.array.slicing.v->concatenate_array_chunks(v)
A:dask.array.slicing.v_key->next(flatten(v.__dask_keys__()))
dask.array.slicing._expander(where)
dask.array.slicing._sanitize_index_element(ind)
dask.array.slicing._slice_1d(dim_shape,lengths,index)
dask.array.slicing.check_index(axis,ind,dimension)
dask.array.slicing.concatenate_array_chunks(x)
dask.array.slicing.expander(where)
dask.array.slicing.getitem_variadic(x,*index)
dask.array.slicing.issorted(seq)
dask.array.slicing.make_block_sorted_slices(index,chunks)
dask.array.slicing.new_blockdim(dim_shape,lengths,index)
dask.array.slicing.normalize_index(idx,shape)
dask.array.slicing.normalize_slice(idx,dim)
dask.array.slicing.parse_assignment_indices(indices,shape)
dask.array.slicing.partition_by_size(sizes,seq)
dask.array.slicing.posify_index(shape,ind)
dask.array.slicing.replace_ellipsis(n,index)
dask.array.slicing.sanitize_index(ind)
dask.array.slicing.setitem(x,v,indices)
dask.array.slicing.setitem_array(out_name,array,indices,value)
dask.array.slicing.shuffle_slice(x,index)
dask.array.slicing.slice_array(out_name,in_name,blockdims,index,itemsize)
dask.array.slicing.slice_slices_and_integers(out_name,in_name,blockdims,index)
dask.array.slicing.slice_with_bool_dask_array(x,index)
dask.array.slicing.slice_with_int_dask_array(x,index)
dask.array.slicing.slice_with_int_dask_array_on_axis(x,idx,axis)
dask.array.slicing.slice_with_newaxes(out_name,in_name,blockdims,index,itemsize)
dask.array.slicing.slice_wrap_lists(out_name,in_name,blockdims,index,itemsize)
dask.array.slicing.slicing_plan(chunks,index)
dask.array.slicing.take(outname,inname,chunks,index,itemsize,axis=0)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/tiledb_io.py----------------------------------------
A:dask.array.tiledb_io.key->tiledb_config.pop('key', None)
A:dask.array.tiledb_io.tdb->tiledb.empty_like(uri, darray, tile=chunks, config=tiledb_config, key=key, **kwargs)
dask.array.from_tiledb(uri,attribute=None,chunks=None,storage_options=None,**kwargs)
dask.array.tiledb_io._tiledb_to_chunks(tiledb_array)
dask.array.tiledb_io.from_tiledb(uri,attribute=None,chunks=None,storage_options=None,**kwargs)
dask.array.tiledb_io.to_tiledb(darray,uri,compute=True,return_stored=False,storage_options=None,key=None,**kwargs)
dask.array.to_tiledb(darray,uri,compute=True,return_stored=False,storage_options=None,key=None,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/backends.py----------------------------------------
A:dask.array.backends.out->numpy.ma.concatenate(arrays, axis=axis)
A:dask.array.backends.fill_values->numpy.unique(fill_values)
A:dask.array.backends.axes_a->list(axes_a)
A:dask.array.backends.axes_b->list(axes_b)
A:dask.array.backends.na->len(axes_a)
A:dask.array.backends.nb->len(axes_b)
A:dask.array.backends.at->a.transpose(newaxes_a).reshape(newshape_a)
A:dask.array.backends.bt->b.transpose(newaxes_b).reshape(newshape_b)
A:dask.array.backends.res->numpy.ma.dot(at, bt)
A:dask.array.backends.keepdims->kwargs.get('keepdims', False)
A:dask.array.backends.axis->kwargs.get('axis', None)
A:dask.array.backends.dtype->kwargs.get('dtype', np.float64)
A:dask.array.backends.prod->math.prod((shape[dim] for dim in axis))
A:dask.array.backends.new_shape->tuple((shape[dim] for dim in range(len(shape)) if dim not in axis))
A:dask.array.backends.n->_nannumel(x, **kwargs)
A:dask.array.backends.array_creation_dispatch->CreationDispatch(module_name='array', default='numpy', entrypoint_class=ArrayBackendEntrypoint, name='array_creation_dispatch')
dask.array.backends.ArrayBackendEntrypoint(DaskBackendEntrypoint)
dask.array.backends.ArrayBackendEntrypoint.RandomState(self)
dask.array.backends.ArrayBackendEntrypoint.arange(start,/,stop=None,step=1,*,dtype=None,meta=None,**kwargs)
dask.array.backends.ArrayBackendEntrypoint.default_bit_generator(self)
dask.array.backends.ArrayBackendEntrypoint.empty(shape,*,dtype=None,meta=None,**kwargs)
dask.array.backends.ArrayBackendEntrypoint.full(shape,fill_value,*,dtype=None,meta=None,**kwargs)
dask.array.backends.ArrayBackendEntrypoint.ones(shape,*,dtype=None,meta=None,**kwargs)
dask.array.backends.ArrayBackendEntrypoint.zeros(shape,*,dtype=None,meta=None,**kwargs)
dask.array.backends.NumpyBackendEntrypoint(ArrayBackendEntrypoint)
dask.array.backends.NumpyBackendEntrypoint.RandomState(self)
dask.array.backends.NumpyBackendEntrypoint.default_bit_generator(self)
dask.array.backends.NumpyBackendEntrypoint.to_backend(cls,data:Array,**kwargs)
dask.array.backends.NumpyBackendEntrypoint.to_backend_dispatch(cls)
dask.array.backends._concatenate(arrays,axis=0)
dask.array.backends._nannumel(x,**kwargs)
dask.array.backends._nannumel_sparse(x,**kwargs)
dask.array.backends._numel(x,coerce_np_ndarray:bool,**kwargs)
dask.array.backends._numel_arraylike(x,**kwargs)
dask.array.backends._numel_masked(x,**kwargs)
dask.array.backends._numel_ndarray(x,**kwargs)
dask.array.backends._tensordot(a,b,axes=2)
dask.array.backends._tensordot_scipy_sparse(a,b,axes)
dask.array.backends.percentile(a,q,method='linear')
dask.array.backends.register_cupy()
dask.array.backends.register_cupyx()
dask.array.backends.register_scipy_sparse()
dask.array.backends.register_sparse()
dask.array.backends.to_numpy_dispatch_from_numpy(data,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/random.py----------------------------------------
A:dask.array.random.(a, size, replace, p, axis, chunks, meta, dependencies)->_choice_validate_params(self, a, size, replace, p, 0, chunks)
A:dask.array.random.sizes->list(product(*chunks))
A:dask.array.random.bitgens->random_state_data(len(sizes), rng._numpy_state)
A:dask.array.random.keys->product([name], *[range(len(bd)) for bd in chunks] + [[0]] * len(extra_chunks))
A:dask.array.random.graph->dask.highlevelgraph.HighLevelGraph.from_collections(name, dsk, dependencies=dependencies)
A:dask.array.random.x->arange(x, chunks='auto')
A:dask.array.random.index->numpy.arange(len(x))
A:dask.array.random.self._numpy_state->numpy.random.RandomState(seed)
A:dask.array.random.state_data->random_state_data(len(sizes), self._numpy_state)
A:dask.array.random.backend_lib->importlib.import_module(backend_name)
A:dask.array.random.bit_generator->type(bit_generator)()
A:dask.array.random.state->dask.array.backends.array_creation_dispatch.RandomState(state_data)
A:dask.array.random.seeds->rng(bitgen)._seed_seq.spawn(n_bitgens)
A:dask.array.random.bitgen->rng(bitgen)
A:dask.array.random.rng->_rng_from_bitgen(bitgen)
A:dask.array.random.func->getattr(state, funcname)
A:dask.array.random.meta->func_applier(gen, funcname, bitgen, (0,) * len(size), small_args, small_kwargs)
A:dask.array.random.a->a.rechunk(a.shape).rechunk(a.shape)
A:dask.array.random.len_a->len(a)
A:dask.array.random.p->p.rechunk(p.shape).rechunk(p.shape)
A:dask.array.random.chunks->normalize_chunks(chunks, size, dtype=kwargs.get('dtype', np.float64))
A:dask.array.random.shapes->list({ar.shape for ar in chain(args, kwargs.values()) if isinstance(ar, (Array, np.ndarray))})
A:dask.array.random.size->broadcast_shapes(*shapes)
A:dask.array.random.slices->slices_from_chunks(chunks)
A:dask.array.random.res->_broadcast_any(ar, size, chunks)
A:dask.array.random.bitgen_token->tokenize(bitgens)
A:dask.array.random.gen->type(rng._bit_generator)
A:dask.array.random.token->tokenize(bitgen_token, size, chunks, args, kwargs)
A:dask.array.random.blocks->product(*[range(len(bd)) for bd in chunks])
A:dask.array.random._cached_states_lock->Lock()
A:dask.array.random._cached_states[key]state->RandomState()
A:dask.array.random.seed->_make_api('seed')
A:dask.array.random.beta->_make_api('beta')
A:dask.array.random.binomial->_make_api('binomial')
A:dask.array.random.chisquare->_make_api('chisquare')
A:dask.array.random.choice->_make_api('choice')
A:dask.array.random.exponential->_make_api('exponential')
A:dask.array.random.f->_make_api('f')
A:dask.array.random.gamma->_make_api('gamma')
A:dask.array.random.geometric->_make_api('geometric')
A:dask.array.random.gumbel->_make_api('gumbel')
A:dask.array.random.hypergeometric->_make_api('hypergeometric')
A:dask.array.random.laplace->_make_api('laplace')
A:dask.array.random.logistic->_make_api('logistic')
A:dask.array.random.lognormal->_make_api('lognormal')
A:dask.array.random.logseries->_make_api('logseries')
A:dask.array.random.multinomial->_make_api('multinomial')
A:dask.array.random.negative_binomial->_make_api('negative_binomial')
A:dask.array.random.noncentral_chisquare->_make_api('noncentral_chisquare')
A:dask.array.random.noncentral_f->_make_api('noncentral_f')
A:dask.array.random.normal->_make_api('normal')
A:dask.array.random.pareto->_make_api('pareto')
A:dask.array.random.permutation->_make_api('permutation')
A:dask.array.random.poisson->_make_api('poisson')
A:dask.array.random.power->_make_api('power')
A:dask.array.random.random_sample->_make_api('random_sample')
A:dask.array.random.random->_make_api('random_sample')
A:dask.array.random.randint->_make_api('randint')
A:dask.array.random.random_integers->_make_api('random_integers')
A:dask.array.random.rayleigh->_make_api('rayleigh')
A:dask.array.random.standard_cauchy->_make_api('standard_cauchy')
A:dask.array.random.standard_exponential->_make_api('standard_exponential')
A:dask.array.random.standard_gamma->_make_api('standard_gamma')
A:dask.array.random.standard_normal->_make_api('standard_normal')
A:dask.array.random.standard_t->_make_api('standard_t')
A:dask.array.random.triangular->_make_api('triangular')
A:dask.array.random.uniform->_make_api('uniform')
A:dask.array.random.vonmises->_make_api('vonmises')
A:dask.array.random.wald->_make_api('wald')
A:dask.array.random.weibull->_make_api('weibull')
A:dask.array.random.zipf->_make_api('zipf')
dask.array.random.Generator(self,bit_generator)
dask.array.random.Generator.__init__(self,bit_generator)
dask.array.random.Generator.__str__(self)
dask.array.random.Generator._backend(self)
dask.array.random.Generator._backend_name(self)
dask.array.random.Generator.beta(self,a,b,size=None,chunks='auto',**kwargs)
dask.array.random.Generator.binomial(self,n,p,size=None,chunks='auto',**kwargs)
dask.array.random.Generator.chisquare(self,df,size=None,chunks='auto',**kwargs)
dask.array.random.Generator.choice(self,a,size=None,replace=True,p=None,axis=0,shuffle=True,chunks='auto')
dask.array.random.Generator.exponential(self,scale=1.0,size=None,chunks='auto',**kwargs)
dask.array.random.Generator.f(self,dfnum,dfden,size=None,chunks='auto',**kwargs)
dask.array.random.Generator.gamma(self,shape,scale=1.0,size=None,chunks='auto',**kwargs)
dask.array.random.Generator.geometric(self,p,size=None,chunks='auto',**kwargs)
dask.array.random.Generator.gumbel(self,loc=0.0,scale=1.0,size=None,chunks='auto',**kwargs)
dask.array.random.Generator.hypergeometric(self,ngood,nbad,nsample,size=None,chunks='auto',**kwargs)
dask.array.random.Generator.integers(self,low,high=None,size=None,dtype=np.int64,endpoint=False,chunks='auto',**kwargs)
dask.array.random.Generator.laplace(self,loc=0.0,scale=1.0,size=None,chunks='auto',**kwargs)
dask.array.random.Generator.logistic(self,loc=0.0,scale=1.0,size=None,chunks='auto',**kwargs)
dask.array.random.Generator.lognormal(self,mean=0.0,sigma=1.0,size=None,chunks='auto',**kwargs)
dask.array.random.Generator.logseries(self,p,size=None,chunks='auto',**kwargs)
dask.array.random.Generator.multinomial(self,n,pvals,size=None,chunks='auto',**kwargs)
dask.array.random.Generator.multivariate_hypergeometric(self,colors,nsample,size=None,method='marginals',chunks='auto',**kwargs)
dask.array.random.Generator.negative_binomial(self,n,p,size=None,chunks='auto',**kwargs)
dask.array.random.Generator.noncentral_chisquare(self,df,nonc,size=None,chunks='auto',**kwargs)
dask.array.random.Generator.noncentral_f(self,dfnum,dfden,nonc,size=None,chunks='auto',**kwargs)
dask.array.random.Generator.normal(self,loc=0.0,scale=1.0,size=None,chunks='auto',**kwargs)
dask.array.random.Generator.pareto(self,a,size=None,chunks='auto',**kwargs)
dask.array.random.Generator.permutation(self,x)
dask.array.random.Generator.poisson(self,lam=1.0,size=None,chunks='auto',**kwargs)
dask.array.random.Generator.power(self,a,size=None,chunks='auto',**kwargs)
dask.array.random.Generator.random(self,size=None,dtype=np.float64,out=None,chunks='auto',**kwargs)
dask.array.random.Generator.rayleigh(self,scale=1.0,size=None,chunks='auto',**kwargs)
dask.array.random.Generator.standard_cauchy(self,size=None,chunks='auto',**kwargs)
dask.array.random.Generator.standard_exponential(self,size=None,chunks='auto',**kwargs)
dask.array.random.Generator.standard_gamma(self,shape,size=None,chunks='auto',**kwargs)
dask.array.random.Generator.standard_normal(self,size=None,chunks='auto',**kwargs)
dask.array.random.Generator.standard_t(self,df,size=None,chunks='auto',**kwargs)
dask.array.random.Generator.triangular(self,left,mode,right,size=None,chunks='auto',**kwargs)
dask.array.random.Generator.uniform(self,low=0.0,high=1.0,size=None,chunks='auto',**kwargs)
dask.array.random.Generator.vonmises(self,mu,kappa,size=None,chunks='auto',**kwargs)
dask.array.random.Generator.wald(self,mean,scale,size=None,chunks='auto',**kwargs)
dask.array.random.Generator.weibull(self,a,size=None,chunks='auto',**kwargs)
dask.array.random.Generator.zipf(self,a,size=None,chunks='auto',**kwargs)
dask.array.random.RandomState(self,seed=None,RandomState=None)
dask.array.random.RandomState.__init__(self,seed=None,RandomState=None)
dask.array.random.RandomState._backend(self)
dask.array.random.RandomState.beta(self,a,b,size=None,chunks='auto',**kwargs)
dask.array.random.RandomState.binomial(self,n,p,size=None,chunks='auto',**kwargs)
dask.array.random.RandomState.chisquare(self,df,size=None,chunks='auto',**kwargs)
dask.array.random.RandomState.exponential(self,scale=1.0,size=None,chunks='auto',**kwargs)
dask.array.random.RandomState.f(self,dfnum,dfden,size=None,chunks='auto',**kwargs)
dask.array.random.RandomState.gamma(self,shape,scale=1.0,size=None,chunks='auto',**kwargs)
dask.array.random.RandomState.geometric(self,p,size=None,chunks='auto',**kwargs)
dask.array.random.RandomState.gumbel(self,loc=0.0,scale=1.0,size=None,chunks='auto',**kwargs)
dask.array.random.RandomState.hypergeometric(self,ngood,nbad,nsample,size=None,chunks='auto',**kwargs)
dask.array.random.RandomState.laplace(self,loc=0.0,scale=1.0,size=None,chunks='auto',**kwargs)
dask.array.random.RandomState.logistic(self,loc=0.0,scale=1.0,size=None,chunks='auto',**kwargs)
dask.array.random.RandomState.lognormal(self,mean=0.0,sigma=1.0,size=None,chunks='auto',**kwargs)
dask.array.random.RandomState.logseries(self,p,size=None,chunks='auto',**kwargs)
dask.array.random.RandomState.multinomial(self,n,pvals,size=None,chunks='auto',**kwargs)
dask.array.random.RandomState.negative_binomial(self,n,p,size=None,chunks='auto',**kwargs)
dask.array.random.RandomState.noncentral_chisquare(self,df,nonc,size=None,chunks='auto',**kwargs)
dask.array.random.RandomState.noncentral_f(self,dfnum,dfden,nonc,size=None,chunks='auto',**kwargs)
dask.array.random.RandomState.normal(self,loc=0.0,scale=1.0,size=None,chunks='auto',**kwargs)
dask.array.random.RandomState.pareto(self,a,size=None,chunks='auto',**kwargs)
dask.array.random.RandomState.permutation(self,x)
dask.array.random.RandomState.poisson(self,lam=1.0,size=None,chunks='auto',**kwargs)
dask.array.random.RandomState.power(self,a,size=None,chunks='auto',**kwargs)
dask.array.random.RandomState.randint(self,low,high=None,size=None,chunks='auto',dtype='l',**kwargs)
dask.array.random.RandomState.random_integers(self,low,high=None,size=None,chunks='auto',**kwargs)
dask.array.random.RandomState.random_sample(self,size=None,chunks='auto',**kwargs)
dask.array.random.RandomState.rayleigh(self,scale=1.0,size=None,chunks='auto',**kwargs)
dask.array.random.RandomState.seed(self,seed=None)
dask.array.random.RandomState.standard_cauchy(self,size=None,chunks='auto',**kwargs)
dask.array.random.RandomState.standard_exponential(self,size=None,chunks='auto',**kwargs)
dask.array.random.RandomState.standard_gamma(self,shape,size=None,chunks='auto',**kwargs)
dask.array.random.RandomState.standard_normal(self,size=None,chunks='auto',**kwargs)
dask.array.random.RandomState.standard_t(self,df,size=None,chunks='auto',**kwargs)
dask.array.random.RandomState.tomaxint(self,size=None,chunks='auto',**kwargs)
dask.array.random.RandomState.triangular(self,left,mode,right,size=None,chunks='auto',**kwargs)
dask.array.random.RandomState.uniform(self,low=0.0,high=1.0,size=None,chunks='auto',**kwargs)
dask.array.random.RandomState.vonmises(self,mu,kappa,size=None,chunks='auto',**kwargs)
dask.array.random.RandomState.wald(self,mean,scale,size=None,chunks='auto',**kwargs)
dask.array.random.RandomState.weibull(self,a,size=None,chunks='auto',**kwargs)
dask.array.random.RandomState.zipf(self,a,size=None,chunks='auto',**kwargs)
dask.array.random._apply_random(RandomState,funcname,state_data,size,args,kwargs)
dask.array.random._apply_random_func(rng,funcname,bitgen,size,args,kwargs)
dask.array.random._choice_rng(state_data,a,size,replace,p,axis,shuffle)
dask.array.random._choice_rs(state_data,a,size,replace,p)
dask.array.random._choice_validate_params(state,a,size,replace,p,axis,chunks)
dask.array.random._make_api(attr)
dask.array.random._rng_from_bitgen(bitgen)
dask.array.random._shuffle(bit_generator,x,axis=0)
dask.array.random._spawn_bitgens(bitgen,n_bitgens)
dask.array.random._wrap_func(rng,funcname,*args,size=None,chunks='auto',extra_chunks=(),**kwargs)
dask.array.random.default_rng(seed=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/cupy_entry_point.py----------------------------------------
dask.array.cupy_entry_point.CupyBackendEntrypoint(self)
dask.array.cupy_entry_point.CupyBackendEntrypoint.RandomState(self)
dask.array.cupy_entry_point.CupyBackendEntrypoint.__init__(self)
dask.array.cupy_entry_point.CupyBackendEntrypoint.arange(*args,like=None,**kwargs)
dask.array.cupy_entry_point.CupyBackendEntrypoint.default_bit_generator(self)
dask.array.cupy_entry_point.CupyBackendEntrypoint.empty(*args,**kwargs)
dask.array.cupy_entry_point.CupyBackendEntrypoint.full(*args,**kwargs)
dask.array.cupy_entry_point.CupyBackendEntrypoint.ones(*args,**kwargs)
dask.array.cupy_entry_point.CupyBackendEntrypoint.to_backend(cls,data:Array,**kwargs)
dask.array.cupy_entry_point.CupyBackendEntrypoint.to_backend_dispatch(cls)
dask.array.cupy_entry_point.CupyBackendEntrypoint.zeros(*args,**kwargs)
dask.array.cupy_entry_point._cupy(strict=True)
dask.array.cupy_entry_point._da_with_cupy_meta(attr,*args,meta=None,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/percentile.py----------------------------------------
A:dask.array.percentile.n->len(a)
A:dask.array.percentile.q->array_safe(q, like=meta_from_array(a))
A:dask.array.percentile.result->merge_percentiles(finalq, qs, [v.codes for v in vals], method, Ns, raise_on_nan)
A:dask.array.percentile.a2->values.view('i8')
A:dask.array.percentile.result[0]->min(result[0], values.min())
A:dask.array.percentile.t->TDigest()
A:dask.array.percentile.method->kwargs.pop('interpolation')
A:dask.array.percentile.token->tokenize(a, q, method)
A:dask.array.percentile.meta->meta_from_array(a, dtype=dtype)
A:dask.array.percentile.calc_q->numpy.pad(q, 1, mode='constant')
A:dask.array.percentile.dsk->merge(dsk, dsk2)
A:dask.array.percentile.graph->dask.highlevelgraph.HighLevelGraph.from_collections(name2, dsk, dependencies=[a])
A:dask.array.percentile.finalq->array_safe(finalq, like=combined_vals)
A:dask.array.percentile.qs->list(map(list, qs))
A:dask.array.percentile.vals->list(vals)
A:dask.array.percentile.(vals, Ns)->zip(*vals)
A:dask.array.percentile.Ns->list(Ns)
A:dask.array.percentile.L->list(zip(*[(q, val, N) for (q, val, N) in zip(qs, vals, Ns) if N]))
A:dask.array.percentile.count->numpy.empty_like(finalq, shape=len(q))
A:dask.array.percentile.count[1:]->numpy.diff(array_safe(q, like=q[0]))
A:dask.array.percentile.combined_vals->numpy.take(combined_vals, sort_order)
A:dask.array.percentile.combined_counts->numpy.take(combined_counts, sort_order)
A:dask.array.percentile.sort_order->numpy.argsort(combined_vals)
A:dask.array.percentile.combined_q->numpy.cumsum(combined_counts)
A:dask.array.percentile.rv->numpy.interp(desired_q, combined_q, combined_vals)
A:dask.array.percentile.left->numpy.searchsorted(combined_q, desired_q, side='left')
A:dask.array.percentile.lower->numpy.minimum(left, right)
A:dask.array.percentile.upper->numpy.maximum(left, right)
A:dask.array.percentile.lower_residual->numpy.abs(combined_q[lower] - desired_q)
A:dask.array.percentile.upper_residual->numpy.abs(combined_q[upper] - desired_q)
dask.array.percentile(a,q,method='linear',internal_method='default',**kwargs)
dask.array.percentile._percentile(a,q,method='linear')
dask.array.percentile._percentiles_from_tdigest(qs,digests)
dask.array.percentile._tdigest_chunk(a)
dask.array.percentile.merge_percentiles(finalq,qs,vals,method='lower',Ns=None,raise_on_nan=True)
dask.array.percentile.percentile(a,q,method='linear',internal_method='default',**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/reductions.py----------------------------------------
A:dask.array.reductions.f->dask.array.dispatch.divide_lookup.dispatch(type(builtins.max(a, b, key=key)))
A:dask.array.reductions.axis->validate_axis(axis, a.ndim)
A:dask.array.reductions.chunk->partial(chunk, dtype=dtype)
A:dask.array.reductions.aggregate->partial(chunk.argtopk_aggregate, k=k)
A:dask.array.reductions.inds->tuple(inds)
A:dask.array.reductions.wgt->broadcast_to(wgt, x.shape)
A:dask.array.reductions.tmp->Array(graph, name, chunks, dtype=dtype, meta=meta)
A:dask.array.reductions.tmp._chunks->tuple(((output_size,) * len(c) if i in axis else c for (i, c) in enumerate(tmp.chunks)))
A:dask.array.reductions.reduced_meta->compute_meta(chunk, x.dtype, x._meta, axis=axis, keepdims=True)
A:dask.array.reductions.result->handle_out(out, result)
A:dask.array.reductions.result._chunks->tuple(((output_size,) if i in axis else c for (i, c) in enumerate(tmp.chunks)))
A:dask.array.reductions.n->_concatenate2(ns, axes=axis).sum(axis=axis, **keepdim_kw)
A:dask.array.reductions.split_every->dict.fromkeys(axis, n)
A:dask.array.reductions.depth->int(builtins.max(depth, math.ceil(math.log(n, split_every[i]))))
A:dask.array.reductions.func->compose(func, partial(_concatenate2, axes=sorted(axis)))
A:dask.array.reductions.x->x.flatten().rechunk(chunks=x.npartitions).flatten().rechunk(chunks=x.npartitions)
A:dask.array.reductions.keys->list(product(*map(range, x.numblocks)))
A:dask.array.reductions.out_chunks->list(getter(out_chunks))
A:dask.array.reductions.dummy->dict((i for i in enumerate(p) if i[0] in split_every))
A:dask.array.reductions.g->lol_tuples((x.name,), range(x.ndim), free, dummy)
A:dask.array.reductions.graph->dask.highlevelgraph.HighLevelGraph.from_collections(name, dsk, dependencies=[m])
A:dask.array.reductions.meta->a.rechunk({ax: -1 if ax in axis else 'auto' for ax in range(a.ndim)})._meta.astype(np.intp).reshape((0,) * (a.ndim - naxis + 1))
A:dask.array.reductions.dtype->getattr(func(np.ones((0,), dtype=x.dtype)), 'dtype', object)
A:dask.array.reductions.dt->getattr(np.var(np.ones(shape=(1,), dtype=a.dtype)), 'dtype', object)
A:dask.array.reductions.total->_concatenate2(deepmap(lambda pair: pair['total'], pairs), axes=axis).sum(axis=axis, **kwargs)
A:dask.array.reductions.totals->_concatenate2(deepmap(lambda pair: pair['total'], pairs), axes=axis)
A:dask.array.reductions.d->numpy.abs(d)
A:dask.array.reductions.M->_moment_helper(Ms, ns, inner_term, order, sum, axis, kwargs)
A:dask.array.reductions.ns->_concatenate2(ns, axes=axis)
A:dask.array.reductions.Ms->_concatenate2(deepmap(lambda pair: pair['M'], pairs), axes=axis)
A:dask.array.reductions.mu->divide(totals.sum(axis=axis, **keepdim_kw), n)
A:dask.array.reductions.inner_term->numpy.abs(divide(totals, ns) - mu)
A:dask.array.reductions.keepdim_kw->kwargs.copy()
A:dask.array.reductions.reduced->a.rechunk({ax: -1 if ax in axis else 'auto' for ax in range(a.ndim)}).sum(axis=axis)
A:dask.array.reductions.o->numpy.sqrt(a)
A:dask.array.reductions.local_args->argfunc(vals, axis=axis)
A:dask.array.reductions.vals->numpy.ma.filled(vals, fill_value)
A:dask.array.reductions.arg->argfunc(x, axis=arg_axis, keepdims=True)
A:dask.array.reductions.ind->numpy.unravel_index(arg.ravel()[0], x.shape)
A:dask.array.reductions.total_ind->tuple((o + i for (o, i) in zip(offset, ind)))
A:dask.array.reductions.arg[:]->numpy.ravel_multi_index(total_ind, total_shape)
A:dask.array.reductions.fill_value->numpy.ma.maximum_fill_value(vals)
A:dask.array.reductions.(arg, vals)->_arg_combine(data, axis, argfunc, keepdims=keepdims)
A:dask.array.reductions.offsets->list(product(*(accumulate(operator.add, bd[:-1], 0) for bd in x.chunks)))
A:dask.array.reductions.offset_info->pluck(axis[0], offsets)
A:dask.array.reductions.chunks->tuple(((1,) * len(c) if i in axis else c for (i, c) in enumerate(x.chunks)))
A:dask.array.reductions.batches->x.flatten().rechunk(chunks=x.npartitions).flatten().rechunk(chunks=x.npartitions).map_blocks(preop, axis=axis, keepdims=True, dtype=dtype)
A:dask.array.reductions.n_vals->len(prefix_vals)
A:dask.array.reductions.stride2->builtins.max(2, 2 ** math.ceil(math.log2(n_vals // 2)))
A:dask.array.reductions.m->x.flatten().rechunk(chunks=x.npartitions).flatten().rechunk(chunks=x.npartitions).map_blocks(func, axis=axis, dtype=dtype)
A:dask.array.reductions.full->slice(None, None, None)
A:dask.array.reductions.indices->list(product(*[range(nb) if ii != axis else [i] for (ii, nb) in enumerate(x.numblocks)]))
A:dask.array.reductions.dsk->dict()
A:dask.array.reductions.shape->tuple((x.chunks[i][ii] if i != axis else 1 for (i, ii) in enumerate(ind)))
A:dask.array.reductions.chunk_combine->partial(chunk.argtopk, k=k)
A:dask.array.reductions.idx->arange(a.shape[axis], chunks=(a.chunks[axis],), dtype=np.intp)
A:dask.array.reductions.a_plus_idx->a.rechunk({ax: -1 if ax in axis else 'auto' for ax in range(a.ndim)}).map_blocks(chunk.argtopk_preprocess, idx, dtype=object)
A:dask.array.reductions.naxis->len(axis)
A:dask.array.reductions.a->a.rechunk({ax: -1 if ax in axis else 'auto' for ax in range(a.ndim)}).rechunk({ax: -1 if ax in axis else 'auto' for ax in range(a.ndim)})
dask.array.all(a,axis=None,keepdims=False,split_every=None,out=None)
dask.array.any(a,axis=None,keepdims=False,split_every=None,out=None)
dask.array.argmax(a,axis=None,keepdims=False,split_every=None,out=None)
dask.array.argmin(a,axis=None,keepdims=False,split_every=None,out=None)
dask.array.argtopk(a,k,axis=-1,split_every=None)
dask.array.cumprod(x,axis=None,dtype=None,out=None,method='sequential')
dask.array.cumsum(x,axis=None,dtype=None,out=None,method='sequential')
dask.array.max(a,axis=None,keepdims=False,split_every=None,out=None)
dask.array.mean(a,axis=None,dtype=None,keepdims=False,split_every=None,out=None)
dask.array.mean_agg(pairs,dtype='f8',axis=None,computing_meta=False,**kwargs)
dask.array.mean_chunk(x,sum=chunk.sum,numel=numel,dtype='f8',computing_meta=False,**kwargs)
dask.array.mean_combine(pairs,sum=chunk.sum,numel=numel,dtype='f8',axis=None,computing_meta=False,**kwargs)
dask.array.median(a,axis=None,keepdims=False,out=None)
dask.array.min(a,axis=None,keepdims=False,split_every=None,out=None)
dask.array.moment(a,order,axis=None,dtype=None,keepdims=False,ddof=0,split_every=None,out=None)
dask.array.moment_agg(pairs,order=2,ddof=0,dtype='f8',sum=np.sum,axis=None,computing_meta=False,**kwargs)
dask.array.moment_chunk(A,order=2,sum=chunk.sum,numel=numel,dtype='f8',computing_meta=False,implicit_complex_dtype=False,**kwargs)
dask.array.moment_combine(pairs,order=2,ddof=0,dtype='f8',sum=np.sum,axis=None,computing_meta=False,**kwargs)
dask.array.nanargmax(a,axis=None,keepdims=False,split_every=None,out=None)
dask.array.nanargmin(a,axis=None,keepdims=False,split_every=None,out=None)
dask.array.nancumprod(x,axis,dtype=None,out=None,*,method='sequential')
dask.array.nancumsum(x,axis,dtype=None,out=None,*,method='sequential')
dask.array.nanmax(a,axis=None,keepdims=False,split_every=None,out=None)
dask.array.nanmean(a,axis=None,dtype=None,keepdims=False,split_every=None,out=None)
dask.array.nanmedian(a,axis=None,keepdims=False,out=None)
dask.array.nanmin(a,axis=None,keepdims=False,split_every=None,out=None)
dask.array.nanprod(a,axis=None,dtype=None,keepdims=False,split_every=None,out=None)
dask.array.nanstd(a,axis=None,dtype=None,keepdims=False,ddof=0,split_every=None,out=None)
dask.array.nansum(a,axis=None,dtype=None,keepdims=False,split_every=None,out=None)
dask.array.nanvar(a,axis=None,dtype=None,keepdims=False,ddof=0,split_every=None,out=None)
dask.array.prod(a,axis=None,dtype=None,keepdims=False,split_every=None,out=None)
dask.array.reduction(x,chunk,aggregate,axis=None,keepdims=False,dtype=None,split_every=None,combine=None,name=None,out=None,concatenate=True,output_size=1,meta=None,weights=None)
dask.array.reductions._arg_combine(data,axis,argfunc,keepdims=False)
dask.array.reductions._cumprod_merge(a,b)
dask.array.reductions._cumsum_merge(a,b)
dask.array.reductions._moment_helper(Ms,ns,inner_term,order,sum,axis,kwargs)
dask.array.reductions._nanargmax(x,axis,**kwargs)
dask.array.reductions._nanargmin(x,axis,**kwargs)
dask.array.reductions._nanmax_skip(x_chunk,axis,keepdims)
dask.array.reductions._nanmin_skip(x_chunk,axis,keepdims)
dask.array.reductions._prefixscan_combine(func,binop,pre,x,axis,dtype)
dask.array.reductions._prefixscan_first(func,x,axis,dtype)
dask.array.reductions._sqrt(a)
dask.array.reductions._tree_reduce(x,aggregate,axis,keepdims,dtype,split_every=None,combine=None,name=None,concatenate=True,reduced_meta=None)
dask.array.reductions.all(a,axis=None,keepdims=False,split_every=None,out=None)
dask.array.reductions.any(a,axis=None,keepdims=False,split_every=None,out=None)
dask.array.reductions.arg_agg(argfunc,data,axis=None,keepdims=False,**kwargs)
dask.array.reductions.arg_chunk(func,argfunc,x,axis,offset_info)
dask.array.reductions.arg_combine(argfunc,data,axis=None,**kwargs)
dask.array.reductions.arg_reduction(x,chunk,combine,agg,axis=None,keepdims=False,split_every=None,out=None)
dask.array.reductions.argmax(a,axis=None,keepdims=False,split_every=None,out=None)
dask.array.reductions.argmin(a,axis=None,keepdims=False,split_every=None,out=None)
dask.array.reductions.argtopk(a,k,axis=-1,split_every=None)
dask.array.reductions.chunk_max(x,axis=None,keepdims=None)
dask.array.reductions.chunk_min(x,axis=None,keepdims=None)
dask.array.reductions.cumprod(x,axis=None,dtype=None,out=None,method='sequential')
dask.array.reductions.cumreduction(func,binop,ident,x,axis=None,dtype=None,out=None,method='sequential',preop=None)
dask.array.reductions.cumsum(x,axis=None,dtype=None,out=None,method='sequential')
dask.array.reductions.divide(a,b,dtype=None)
dask.array.reductions.max(a,axis=None,keepdims=False,split_every=None,out=None)
dask.array.reductions.mean(a,axis=None,dtype=None,keepdims=False,split_every=None,out=None)
dask.array.reductions.mean_agg(pairs,dtype='f8',axis=None,computing_meta=False,**kwargs)
dask.array.reductions.mean_chunk(x,sum=chunk.sum,numel=numel,dtype='f8',computing_meta=False,**kwargs)
dask.array.reductions.mean_combine(pairs,sum=chunk.sum,numel=numel,dtype='f8',axis=None,computing_meta=False,**kwargs)
dask.array.reductions.median(a,axis=None,keepdims=False,out=None)
dask.array.reductions.min(a,axis=None,keepdims=False,split_every=None,out=None)
dask.array.reductions.moment(a,order,axis=None,dtype=None,keepdims=False,ddof=0,split_every=None,out=None)
dask.array.reductions.moment_agg(pairs,order=2,ddof=0,dtype='f8',sum=np.sum,axis=None,computing_meta=False,**kwargs)
dask.array.reductions.moment_chunk(A,order=2,sum=chunk.sum,numel=numel,dtype='f8',computing_meta=False,implicit_complex_dtype=False,**kwargs)
dask.array.reductions.moment_combine(pairs,order=2,ddof=0,dtype='f8',sum=np.sum,axis=None,computing_meta=False,**kwargs)
dask.array.reductions.nanarg_agg(argfunc,data,axis=None,keepdims=False,**kwargs)
dask.array.reductions.nanargmax(a,axis=None,keepdims=False,split_every=None,out=None)
dask.array.reductions.nanargmin(a,axis=None,keepdims=False,split_every=None,out=None)
dask.array.reductions.nancumprod(x,axis,dtype=None,out=None,*,method='sequential')
dask.array.reductions.nancumsum(x,axis,dtype=None,out=None,*,method='sequential')
dask.array.reductions.nanmax(a,axis=None,keepdims=False,split_every=None,out=None)
dask.array.reductions.nanmean(a,axis=None,dtype=None,keepdims=False,split_every=None,out=None)
dask.array.reductions.nanmedian(a,axis=None,keepdims=False,out=None)
dask.array.reductions.nanmin(a,axis=None,keepdims=False,split_every=None,out=None)
dask.array.reductions.nannumel(x,**kwargs)
dask.array.reductions.nanprod(a,axis=None,dtype=None,keepdims=False,split_every=None,out=None)
dask.array.reductions.nanstd(a,axis=None,dtype=None,keepdims=False,ddof=0,split_every=None,out=None)
dask.array.reductions.nansum(a,axis=None,dtype=None,keepdims=False,split_every=None,out=None)
dask.array.reductions.nanvar(a,axis=None,dtype=None,keepdims=False,ddof=0,split_every=None,out=None)
dask.array.reductions.numel(x,**kwargs)
dask.array.reductions.partial_reduce(func,x,split_every,keepdims=False,dtype=None,name=None,reduced_meta=None)
dask.array.reductions.prefixscan_blelloch(func,preop,binop,x,axis=None,dtype=None,out=None)
dask.array.reductions.prod(a,axis=None,dtype=None,keepdims=False,split_every=None,out=None)
dask.array.reductions.reduction(x,chunk,aggregate,axis=None,keepdims=False,dtype=None,split_every=None,combine=None,name=None,out=None,concatenate=True,output_size=1,meta=None,weights=None)
dask.array.reductions.safe_sqrt(a)
dask.array.reductions.std(a,axis=None,dtype=None,keepdims=False,ddof=0,split_every=None,out=None)
dask.array.reductions.sum(a,axis=None,dtype=None,keepdims=False,split_every=None,out=None)
dask.array.reductions.topk(a,k,axis=-1,split_every=None)
dask.array.reductions.trace(a,offset=0,axis1=0,axis2=1,dtype=None)
dask.array.reductions.var(a,axis=None,dtype=None,keepdims=False,ddof=0,split_every=None,out=None)
dask.array.std(a,axis=None,dtype=None,keepdims=False,ddof=0,split_every=None,out=None)
dask.array.sum(a,axis=None,dtype=None,keepdims=False,split_every=None,out=None)
dask.array.topk(a,k,axis=-1,split_every=None)
dask.array.trace(a,offset=0,axis1=0,axis2=1,dtype=None)
dask.array.var(a,axis=None,dtype=None,keepdims=False,ddof=0,split_every=None,out=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/svg.py----------------------------------------
A:dask.array.svg.shape->tuple(map(sum, chunks))
A:dask.array.svg.(y, x)->grid_points(chunks, sizes)
A:dask.array.svg.(lines, (min_x, max_x, min_y, max_y))->svg_grid(x, y, offset=offset, skew=skew, size=size)
A:dask.array.svg.(x, y, z)->grid_points(chunks, sizes)
A:dask.array.svg.(xy, (mnx, mxx, mny, mxy))->svg_grid(x / 1.7, y, offset=(ox + 10, oy + 0), skew=(1, 0), size=size)
A:dask.array.svg.(zx, (_, _, _, max_x))->svg_grid(z, x / 1.7, offset=(ox + 10, oy + 0), skew=(0, 1), size=size)
A:dask.array.svg.(zy, (min_z, max_z, min_y, max_y))->svg_grid(z, y, offset=(ox + max_x + 10, oy + max_x), skew=(0, 0), size=size)
A:dask.array.svg.sizes->draw_sizes(shape, size=size)
A:dask.array.svg.o->'\n'.join(lines[1:-1])
A:dask.array.svg.lines->'\n'.join(lines[1:-1]).split('\n')
A:dask.array.svg.height->float(re.search('height="(\\d*\\.?\\d*)"', header).groups()[0])
A:dask.array.svg.total_height->max(total_height, height)
A:dask.array.svg.width->float(re.search('width="(\\d*\\.?\\d*)"', header).groups()[0])
A:dask.array.svg.n->len(x1)
A:dask.array.svg.indices->range(n)
A:dask.array.svg.lines[0]->lines[0].replace(' /', ' style="stroke-width:2" /').replace(' /', ' style="stroke-width:2" /')
A:dask.array.svg.lines[-1]->lines[-1].replace(' /', ' style="stroke-width:2" /').replace(' /', ' style="stroke-width:2" /')
A:dask.array.svg.min_x->min(x1.min(), x2.min())
A:dask.array.svg.min_y->min(y1.min(), y2.min())
A:dask.array.svg.max_x->max(x1.max(), x2.max())
A:dask.array.svg.max_y->max(y1.max(), y2.max())
A:dask.array.svg.mx->max(shape)
dask.array.svg.draw_sizes(shape,size=200)
dask.array.svg.grid_points(chunks,sizes)
dask.array.svg.ratio_response(x)
dask.array.svg.svg(chunks,size=200,**kwargs)
dask.array.svg.svg_1d(chunks,sizes=None,**kwargs)
dask.array.svg.svg_2d(chunks,offset=(0,0),skew=(0,0),size=200,sizes=None)
dask.array.svg.svg_3d(chunks,size=200,sizes=None,offset=(0,0))
dask.array.svg.svg_grid(x,y,offset=(0,0),skew=(0,0),size=200)
dask.array.svg.svg_lines(x1,y1,x2,y2,max_n=20)
dask.array.svg.svg_nd(chunks,size=200)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/tests/test_linalg.py----------------------------------------
A:dask.array.tests.test_linalg.mat->numpy.random.default_rng().random((m, n))
A:dask.array.tests.test_linalg.data->dask.array.from_array(mat, chunks=(10, n), name='A')
A:dask.array.tests.test_linalg.n_q->min(m, n)
A:dask.array.tests.test_linalg.n_u->min(m, n)
A:dask.array.tests.test_linalg.d_vh->max(m_vh, n_vh)
A:dask.array.tests.test_linalg.(q, r)->qr(data)
A:dask.array.tests.test_linalg.(u, s, vh)->tsqr(data, compute_svd=True)
A:dask.array.tests.test_linalg.c0->dask.array.from_array(_c0, chunks=m_min, name='c')
A:dask.array.tests.test_linalg.r0->dask.array.from_array(_r0, chunks=n_max, name='r')
A:dask.array.tests.test_linalg.q->q.compute().compute()
A:dask.array.tests.test_linalg.r->min(m, n, k)
A:dask.array.tests.test_linalg.u->u.compute().compute()
A:dask.array.tests.test_linalg.s->s.compute().compute()
A:dask.array.tests.test_linalg.vh->vh.compute().compute()
A:dask.array.tests.test_linalg.x->dask.array.random.default_rng().random((10,) * ndim, chunks=(-1,) * ndim)
A:dask.array.tests.test_linalg.mat2->numpy.vstack([mat, -np.ones((10, 5))])
A:dask.array.tests.test_linalg.x2->dask.array.from_array(mat2, chunks=5)
A:dask.array.tests.test_linalg.c->dask.array.from_array(v2, chunks=5)
A:dask.array.tests.test_linalg.(q1, r1)->qr(data)
A:dask.array.tests.test_linalg.(q2, r2)->qr(data)
A:dask.array.tests.test_linalg.(u1, s1, v1)->svd(data)
A:dask.array.tests.test_linalg.(u2, s2, v2)->svd(data)
A:dask.array.tests.test_linalg.a->numpy.random.default_rng().random(shape)
A:dask.array.tests.test_linalg.d_a->numpy.linalg.norm(a, ord=norm, axis=axis, keepdims=keepdims)
A:dask.array.tests.test_linalg.(d_u, d_s, d_vt)->dask.array.linalg.svd(d_a)
A:dask.array.tests.test_linalg.(u, s, vt)->svd_compressed(x, 3, seed=1234)
A:dask.array.tests.test_linalg.(u, s, v)->dask.array.linalg.svd(x)
A:dask.array.tests.test_linalg.(uu, ss, vv)->dask.array.linalg.svd_compressed(x, k=2, iterator=iterator, n_power_iter=1, seed=123)
A:dask.array.tests.test_linalg.s_true->scipy.linalg.svd(a.compute(), compute_uv=False)
A:dask.array.tests.test_linalg.norm->scipy.linalg.norm((a - u[:, :r] * s[:r] @ vt[:r, :]).compute(), 2)
A:dask.array.tests.test_linalg.(u2, s2, vt2)->svd_compressed(x, 3, seed=1234)
A:dask.array.tests.test_linalg.A1->numpy.array([[7, 3, -1, 2], [3, 8, 1, -4], [-1, 1, 4, -1], [2, -4, -1, 6]])
A:dask.array.tests.test_linalg.A2->numpy.array([[7, 0, 0, 0, 0, 0], [0, 8, 0, 0, 0, 0], [0, 0, 4, 0, 0, 0], [0, 0, 0, 6, 0, 0], [0, 0, 0, 0, 3, 0], [0, 0, 0, 0, 0, 5]])
A:dask.array.tests.test_linalg.dA->dask.array.from_array(A, (chunk, ncol))
A:dask.array.tests.test_linalg.(p, l, u)->scipy.linalg.lu(A)
A:dask.array.tests.test_linalg.(dp, dl, du)->dask.array.linalg.lu(dA)
A:dask.array.tests.test_linalg.A3->numpy.array([[7, 3, 2, 1, 4, 1], [7, 11, 5, 2, 5, 2], [21, 25, 16, 10, 16, 5], [21, 41, 18, 13, 16, 11], [14, 46, 23, 24, 21, 22], [0, 56, 29, 17, 14, 8]])
A:dask.array.tests.test_linalg.rng->numpy.random.default_rng(1)
A:dask.array.tests.test_linalg.A->numpy.random.default_rng(1).integers(1, 20, (nrow, ncol))
A:dask.array.tests.test_linalg.b->numpy.random.default_rng(1).integers(1, 20, nrow)
A:dask.array.tests.test_linalg.Au->numpy.triu(A)
A:dask.array.tests.test_linalg.dAu->dask.array.from_array(Au, (chunk, chunk))
A:dask.array.tests.test_linalg.db->dask.array.from_array(b, chunk)
A:dask.array.tests.test_linalg.res->dask.array.linalg.solve(dA, db, sym_pos=False)
A:dask.array.tests.test_linalg.Al->numpy.tril(A)
A:dask.array.tests.test_linalg.dAl->dask.array.from_array(Al, (chunk, chunk))
A:dask.array.tests.test_linalg.lA->numpy.tril(A)
A:dask.array.tests.test_linalg.(x, r, rank, s)->numpy.linalg.lstsq(A, b2D, rcond=-1)
A:dask.array.tests.test_linalg.(dx, dr, drank, ds)->dask.array.linalg.lstsq(dA, db2D)
A:dask.array.tests.test_linalg.b2D->numpy.random.default_rng(1).integers(1, 20, (nrow, ncol // 2))
A:dask.array.tests.test_linalg.db2D->dask.array.from_array(b2D, (chunk, ncol // 2))
A:dask.array.tests.test_linalg.dx->dask.array.from_array(x, chunks=chunks)
A:dask.array.tests.test_linalg.(du, ds, dv)->dask.array.linalg.svd(dx)
A:dask.array.tests.test_linalg.(uf, vf)->svd_flip(u, v)
A:dask.array.tests.test_linalg.(uc, vc)->svd_flip(*da.compute(u, v))
A:dask.array.tests.test_linalg.(u, v)->svd_flip(x, x.T, u_based_decision=u_based)
A:dask.array.tests.test_linalg.y->dask.array.random.default_rng().random((10,) * ndim, chunks=(-1,) * ndim).copy()
A:dask.array.tests.test_linalg.(du, dv)->svd_flip(du, dv)
A:dask.array.tests.test_linalg.(nu, ns, nv)->numpy.linalg.svd(x, full_matrices=False)
A:dask.array.tests.test_linalg.(nu, nv)->svd_flip(nu, nv)
A:dask.array.tests.test_linalg.d->dask.array.from_array(a, chunks=chunks)
A:dask.array.tests.test_linalg.a_r->numpy.linalg.norm(a, ord=norm, axis=axis, keepdims=keepdims)
A:dask.array.tests.test_linalg.d_r->dask.array.linalg.norm(d, ord=norm, axis=axis, keepdims=keepdims)
dask.array.tests.test_linalg._check_lu_result(p,l,u,A)
dask.array.tests.test_linalg._get_symmat(size)
dask.array.tests.test_linalg._scipy_linalg_solve(a,b,assume_a)
dask.array.tests.test_linalg.test_cholesky(shape,chunk)
dask.array.tests.test_linalg.test_dask_svd_self_consistent(m,n)
dask.array.tests.test_linalg.test_inv(shape,chunk)
dask.array.tests.test_linalg.test_linalg_consistent_names()
dask.array.tests.test_linalg.test_lstsq(nrow,ncol,chunk,iscomplex)
dask.array.tests.test_linalg.test_lu_1()
dask.array.tests.test_linalg.test_lu_2(size)
dask.array.tests.test_linalg.test_lu_3(size)
dask.array.tests.test_linalg.test_lu_errors()
dask.array.tests.test_linalg.test_no_chunks_svd()
dask.array.tests.test_linalg.test_norm_1dim(shape,chunks,axis,norm,keepdims)
dask.array.tests.test_linalg.test_norm_2dim(shape,chunks,axis,norm,keepdims)
dask.array.tests.test_linalg.test_norm_any_ndim(shape,chunks,axis,norm,keepdims)
dask.array.tests.test_linalg.test_norm_any_prec(norm,keepdims,precision,isreal)
dask.array.tests.test_linalg.test_norm_any_slice(shape,chunks,norm,keepdims)
dask.array.tests.test_linalg.test_norm_implemented_errors(shape,chunks,axis,norm,keepdims)
dask.array.tests.test_linalg.test_qr(m,n,chunks,error_type)
dask.array.tests.test_linalg.test_sfqr(m,n,chunks,error_type)
dask.array.tests.test_linalg.test_solve(shape,chunk)
dask.array.tests.test_linalg.test_solve_assume_a(shape,chunk)
dask.array.tests.test_linalg.test_solve_triangular_errors()
dask.array.tests.test_linalg.test_solve_triangular_matrix(shape,chunk)
dask.array.tests.test_linalg.test_solve_triangular_matrix2(shape,chunk)
dask.array.tests.test_linalg.test_solve_triangular_vector(shape,chunk)
dask.array.tests.test_linalg.test_svd_compressed(iterator)
dask.array.tests.test_linalg.test_svd_compressed_compute(iterator)
dask.array.tests.test_linalg.test_svd_compressed_deterministic()
dask.array.tests.test_linalg.test_svd_compressed_dtype_preservation(input_dtype,output_dtype)
dask.array.tests.test_linalg.test_svd_compressed_shapes(m,n,k,chunks)
dask.array.tests.test_linalg.test_svd_dtype_preservation(chunks,dtype)
dask.array.tests.test_linalg.test_svd_flip_correction(shape,chunks,dtype)
dask.array.tests.test_linalg.test_svd_flip_sign(dtype,u_based)
dask.array.tests.test_linalg.test_svd_incompatible_chunking()
dask.array.tests.test_linalg.test_svd_incompatible_dimensions(ndim)
dask.array.tests.test_linalg.test_svd_supported_array_shapes(chunks,shape)
dask.array.tests.test_linalg.test_tsqr(m,n,chunks,error_type)
dask.array.tests.test_linalg.test_tsqr_uncertain(m_min,n_max,chunks,vary_rows,vary_cols,error_type)
dask.array.tests.test_linalg.test_tsqr_zero_height_chunks()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/tests/test_routines.py----------------------------------------
A:dask.array.tests.test_routines.np->pytest.importorskip('numpy')
A:dask.array.tests.test_routines.x->pytest.importorskip('numpy').ones(5, np.float32)
A:dask.array.tests.test_routines.d->np.random.default_rng().random((7, 7))
A:dask.array.tests.test_routines.y->np.random.default_rng().integers(10, size=(5, 10))
A:dask.array.tests.test_routines.dx->dask.array.from_array(x, chunks=10)
A:dask.array.tests.test_routines.np_func->getattr(np, funcname)
A:dask.array.tests.test_routines.da_func->getattr(da, funcname)
A:dask.array.tests.test_routines.np_r_n->np_func(*np_a_n)
A:dask.array.tests.test_routines.da_r_n->da_func(*da_a_n)
A:dask.array.tests.test_routines.np_a->pytest.importorskip('numpy').random.default_rng().random(shape)
A:dask.array.tests.test_routines.da_a->dask.array.from_array(np_a, chunks=1)
A:dask.array.tests.test_routines.np_r->np_func(np_a, k=k, **kwargs)
A:dask.array.tests.test_routines.da_r->da_func(da_a, k=k, **kwargs)
A:dask.array.tests.test_routines.np_a_1->pytest.importorskip('numpy').random.default_rng().random(shape1)
A:dask.array.tests.test_routines.da_a_1->dask.array.from_array(np_a_1, chunks=tuple((c // 2 for c in shape1)))
A:dask.array.tests.test_routines.np_a_2->pytest.importorskip('numpy').random.default_rng().random(shape2)
A:dask.array.tests.test_routines.da_a_2->dask.array.from_array(np_a_2, chunks=tuple((c // 2 for c in shape2)))
A:dask.array.tests.test_routines.a->dask.array.from_array(np.array([1, 2 + 0j]), 2)
A:dask.array.tests.test_routines.result->dask.array.isnull(arr[0]).compute()
A:dask.array.tests.test_routines.axis->tuple((i for (i, c) in enumerate(d.chunks) if i in axis and len(c) == 1))
A:dask.array.tests.test_routines.axes->dict(return_index=True, return_inverse=True, return_counts=True).get('axes', (0, 1))
A:dask.array.tests.test_routines.rng->pytest.importorskip('numpy').random.default_rng()
A:dask.array.tests.test_routines.b->pytest.importorskip('numpy').random.default_rng().random((4, 1, 6))
A:dask.array.tests.test_routines.expected->pytest.importorskip('numpy').union1d(x1, x2)
A:dask.array.tests.test_routines.u->dask.array.from_array(x, chunks=3)
A:dask.array.tests.test_routines.v->pytest.importorskip('numpy').random.default_rng().random(n)
A:dask.array.tests.test_routines.z->pytest.importorskip('numpy').random.default_rng().integers(10, size=(1, 2))
A:dask.array.tests.test_routines.r1->dask.array.dot(x, x).compute()
A:dask.array.tests.test_routines.r2->dask.array.dot(y, y).compute()
A:dask.array.tests.test_routines.rr->pytest.importorskip('numpy').dot(z, z)
A:dask.array.tests.test_routines.slices[axis]->slice(None)
A:dask.array.tests.test_routines.slices->tuple(slices)
A:dask.array.tests.test_routines.sample->pytest.importorskip('numpy').array(func1d(a[slices]))
A:dask.array.tests.test_routines.d_a->dask.array.arange(11, chunks=2)
A:dask.array.tests.test_routines.r_a->pytest.importorskip('numpy').isin(a1, test_elements, assume_unique=assume_unique)
A:dask.array.tests.test_routines.r_d_a->dask.array.gradient(d_a, *varargs, axis=axis, edge_order=edge_order)
A:dask.array.tests.test_routines.e->dask.array.from_array(y, chunks=(4, 5))
A:dask.array.tests.test_routines.expected_output->pytest.importorskip('numpy').array([0, 2, 2, 0, 0, 1], dtype=e.dtype)
A:dask.array.tests.test_routines.dweights->dask.array.from_array(weights, chunks=2)
A:dask.array.tests.test_routines.bins->pytest.importorskip('numpy').array([0, 0.2, 0.5, 0.8, 1])
A:dask.array.tests.test_routines.ad->dask.array.asarray(a, chunks=a_chunks)
A:dask.array.tests.test_routines.vd->dask.array.from_array(v, chunks=10)
A:dask.array.tests.test_routines.out->dask.array.searchsorted(ad, vd, side)
A:dask.array.tests.test_routines.(a1, b1)->dask.array.histogramdd(data, bins=5, range=((0, 1),) * 3)
A:dask.array.tests.test_routines.(a2, b2)->pytest.importorskip('numpy').histogramdd(data.compute(), bins=5, range=((0, 1),) * 3)
A:dask.array.tests.test_routines.data->dask.array.random.default_rng().random(size=(10, 3), chunks=(5, 3))
A:dask.array.tests.test_routines.err_msg->str(info.value)
A:dask.array.tests.test_routines.weights->pytest.importorskip('numpy').array([0.25, 0.75])
A:dask.array.tests.test_routines.weights_d->dask.array.from_array(weights, chunks=vd.chunks)
A:dask.array.tests.test_routines.d_range[non_delayed_i]->d_range[non_delayed_i].compute().compute()
A:dask.array.tests.test_routines.(hist_d, bins_d)->dask.array.histogram(vd, bins=da.array(n) if delay_n_bins and (not density) else n, range=d_range, density=density, weights=weights_d if weighted else None)
A:dask.array.tests.test_routines.(hist, bins)->pytest.importorskip('numpy').histogram(v, bins=bins, range=[bins[0], bins[-1]], density=density, weights=weights if weighted else None)
A:dask.array.tests.test_routines.bins_d->dask.array.from_array(bins, chunks=2)
A:dask.array.tests.test_routines.(hist_d, bins_d2)->dask.array.histogram(vd, bins=bins_d, range=[bins_d[0], bins_d[-1]], density=density, weights=weights_d if weighted else None)
A:dask.array.tests.test_routines.(a1, b1x, b1y)->dask.array.histogram2d(x, y, bins=b, density=density, weights=w)
A:dask.array.tests.test_routines.(a2, b2x, b2y)->pytest.importorskip('numpy').histogram2d(x, y, bins=b, density=density, weights=w)
A:dask.array.tests.test_routines.(a3, b3x, b3y)->pytest.importorskip('numpy').histogram2d(x.compute(), y.compute(), bins=b, density=density, weights=w.compute() if weights else None)
A:dask.array.tests.test_routines.(a3, b3)->dask.array.histogramdd(x, bins=bins, range=ranges, weights=w, normed=True)
A:dask.array.tests.test_routines.w->pytest.importorskip('numpy').random.default_rng().random(size=(10,), chunks=5)
A:dask.array.tests.test_routines.(a4, b4)->pytest.importorskip('numpy').histogramdd(x.compute(), bins=bins, density=True)
A:dask.array.tests.test_routines.kwargs->dict(return_index=True, return_inverse=True, return_counts=True)
A:dask.array.tests.test_routines.r_d->dask.array.isin(d1, test_elements, assume_unique=assume_unique)
A:dask.array.tests.test_routines.a1->pytest.importorskip('numpy').arange(10)
A:dask.array.tests.test_routines.d1->dask.array.from_array(a1, chunks=(5,))
A:dask.array.tests.test_routines.d2->dask.array.from_array(a2, chunks=test_chunks)
A:dask.array.tests.test_routines.test_elements->pytest.importorskip('numpy').arange(0, 10, 2)
A:dask.array.tests.test_routines.actual->dask.array.roll(da.zeros(0), 0)
A:dask.array.tests.test_routines.x1->dask.array.arange(np.sum(chunks), chunks=5)
A:dask.array.tests.test_routines.x2->dask.array.arange(np.sum(chunks), chunks=5).rechunk(tuple(chunks))
A:dask.array.tests.test_routines.dx1->dask.array.from_array(x1)
A:dask.array.tests.test_routines.dx2->dask.array.from_array(x2)
A:dask.array.tests.test_routines.a_flat->dask.array.from_array(np.array([1, 2 + 0j]), 2).ravel()
A:dask.array.tests.test_routines.a_e->pytest.importorskip('numpy').expand_dims(a, axis=axis)
A:dask.array.tests.test_routines.d_e->dask.array.expand_dims(d, axis=axis)
A:dask.array.tests.test_routines.a_s->pytest.importorskip('numpy').squeeze(a)
A:dask.array.tests.test_routines.d_s->dask.array.squeeze(d)
A:dask.array.tests.test_routines.exp_d_s_chunks->tuple((c for (i, c) in enumerate(d.chunks) if i not in axis))
A:dask.array.tests.test_routines.tmp->list(x._chunks)
A:dask.array.tests.test_routines.x._chunks->tuple(tmp)
A:dask.array.tests.test_routines.np_stacked->np_func((y, y))
A:dask.array.tests.test_routines.dsk_stacked->dsk_func((x, x), allow_unknown_chunksizes=True)
A:dask.array.tests.test_routines.c1->pytest.importorskip('numpy').array([1, 0, 1])
A:dask.array.tests.test_routines.c2->dask.array.from_array(c1, chunks=2)
A:dask.array.tests.test_routines.dc1->dask.array.from_array(c1, chunks=3)
A:dask.array.tests.test_routines.dc2->dask.array.from_array(c2, chunks=(2, 1))
A:dask.array.tests.test_routines.res->dask.array.extract(dc, a)
A:dask.array.tests.test_routines.c3->pytest.importorskip('numpy').array([True, False])
A:dask.array.tests.test_routines.dc3->dask.array.from_array(c3, chunks=2)
A:dask.array.tests.test_routines.arr->pytest.importorskip('numpy').asarray(arr)
A:dask.array.tests.test_routines.n_a->pytest.importorskip('numpy').array([0, np.nan, 1, 1.5])
A:dask.array.tests.test_routines.n_b->pytest.importorskip('numpy').array([1e-09, np.nan, 1, 2])
A:dask.array.tests.test_routines.d_b->dask.array.from_array(b, chunks=((2, 2), 1, (4, 2)))
A:dask.array.tests.test_routines.n_r->pytest.importorskip('numpy').allclose(n_a, n_b, equal_nan=True)
A:dask.array.tests.test_routines.d_r->dask.array.allclose(d_a, d_b, equal_nan=True)
A:dask.array.tests.test_routines.d_conditions->dask.array.from_array(conditions)
A:dask.array.tests.test_routines.d_choices->dask.array.from_array(choices)
A:dask.array.tests.test_routines.res_x->pytest.importorskip('numpy').select([x < 0, x > 2, x > 1], [x, x * 2, x * 3], default=1)
A:dask.array.tests.test_routines.res_y->dask.array.select([y < 0, y > 2, y > 1], [y, y * 2, y * 3], default=1)
A:dask.array.tests.test_routines.m->pytest.importorskip('numpy').isnan(d)
A:dask.array.tests.test_routines.d_d->dask.array.from_array(d, chunks=(7, 3))
A:dask.array.tests.test_routines.d_m->dask.array.isnan(d_d)
A:dask.array.tests.test_routines.x_nz->pytest.importorskip('numpy').ones(5, np.float32).nonzero()
A:dask.array.tests.test_routines.d_nz->np.random.default_rng().random((7, 7)).nonzero()
A:dask.array.tests.test_routines.w1->dask.array.where(c, d, e)
A:dask.array.tests.test_routines.w2->pytest.importorskip('numpy').where(c, x, y)
A:dask.array.tests.test_routines.y1->pytest.importorskip('numpy').random.default_rng().integers(10, size=(10, 5))
A:dask.array.tests.test_routines.y2->dask.array.from_array(y1, chunks=2)
A:dask.array.tests.test_routines.w3->pytest.importorskip('numpy').where(True, x, y1)
A:dask.array.tests.test_routines.w4->dask.array.where(True, x, y1)
A:dask.array.tests.test_routines.x_w->pytest.importorskip('numpy').where(x)
A:dask.array.tests.test_routines.d_w->dask.array.where(d)
A:dask.array.tests.test_routines.x_c->pytest.importorskip('numpy').count_nonzero(x)
A:dask.array.tests.test_routines.d_c->dask.array.from_array(c, chunks=((2, 3), (4, 2)))
A:dask.array.tests.test_routines.x_fnz->pytest.importorskip('numpy').flatnonzero(x)
A:dask.array.tests.test_routines.d_fnz->dask.array.flatnonzero(d)
A:dask.array.tests.test_routines.shape->tuple()
A:dask.array.tests.test_routines.findices->pytest.importorskip('numpy').random.default_rng().integers(np.prod(shape, dtype=int), size=nindices)
A:dask.array.tests.test_routines.d_findices->dask.array.from_array(findices, chunks=1)
A:dask.array.tests.test_routines.indices->pytest.importorskip('numpy').unravel_index(findices, shape, order)
A:dask.array.tests.test_routines.d_indices->dask.array.unravel_index(d_findices, shape, order)
A:dask.array.tests.test_routines.darr->dask.array.from_array(arr, chunks=1)
A:dask.array.tests.test_routines.input->dask.array.from_array(arr, chunks=chunks)
A:dask.array.tests.test_routines.arrinput->asarray(arr)
A:dask.array.tests.test_routines.multi_index->dask.array.from_array([[3, 6, 6], [4, 5, 1], [-1, -1, -1]])
A:dask.array.tests.test_routines.multi_index_np->dask.array.from_array([[3, 6, 6], [4, 5, 1], [-1, -1, -1]]).compute()
A:dask.array.tests.test_routines.multi_index1->dask.array.from_array([2, -1, 3, -1], chunks=2)
A:dask.array.tests.test_routines.multi_index2->dask.array.from_array([[1, 2], [-1, -1], [3, 4], [5, 6], [7, 8], [-1, -1]], chunks=(2, 1))
A:dask.array.tests.test_routines.aligned_chunks->acc(chunks, divisor)
A:dask.array.tests.test_routines.b1->dask.array.from_array(y1, chunks=(4, 4))
A:dask.array.tests.test_routines.y0->pytest.importorskip('numpy').random.default_rng().integers(10, size=(5, 10))
A:dask.array.tests.test_routines.b0->dask.array.from_array(y0, chunks=(4, 4))
A:dask.array.tests.test_routines.c->pytest.importorskip('numpy').random.default_rng().random((5, 6))
A:dask.array.tests.test_routines.input_sigs->sig.split('->')[0].split(',')
A:dask.array.tests.test_routines.(np_inputs, da_inputs)->_numpy_and_dask_inputs('a')
A:dask.array.tests.test_routines.(_, da_inputs)->_numpy_and_dask_inputs('a')
A:dask.array.tests.test_routines.np_res->pytest.importorskip('numpy').einsum('ajk,kbl,jl,ab->ab', a, b, c, d)
A:dask.array.tests.test_routines.da_res->dask.array.einsum('ajk,kbl,jl,ab->ab', d_a, d_b, d_c, d_d)
A:dask.array.tests.test_routines.np_avg->pytest.importorskip('numpy').average(a, keepdims=True)
A:dask.array.tests.test_routines.da_avg->dask.array.average(d_a, weights=d_weights, axis=1, keepdims=keepdims)
A:dask.array.tests.test_routines.d_weights->dask.array.from_array(weights, chunks=2)
A:dask.array.tests.test_routines.A->pytest.importorskip('numpy').random.default_rng().integers(0, 11, (30, 35))
A:dask.array.tests.test_routines.dA->dask.array.from_array(A, chunks=(5, 5))
dask.array.tests.test_routines._maybe_len(l)
dask.array.tests.test_routines._numpy_and_dask_inputs(input_sigs)
dask.array.tests.test_routines.test_aligned_coarsen_chunks(chunks,divisor)
dask.array.tests.test_routines.test_allclose()
dask.array.tests.test_routines.test_append()
dask.array.tests.test_routines.test_apply_along_axis(func1d_name,func1d,specify_output_props,input_shape,axis)
dask.array.tests.test_routines.test_apply_over_axes(func_name,func,shape,axes)
dask.array.tests.test_routines.test_argwhere()
dask.array.tests.test_routines.test_argwhere_obj()
dask.array.tests.test_routines.test_argwhere_str()
dask.array.tests.test_routines.test_array()
dask.array.tests.test_routines.test_array_return_type()
dask.array.tests.test_routines.test_atleast_nd_no_args(funcname)
dask.array.tests.test_routines.test_atleast_nd_one_arg(funcname,shape,chunks)
dask.array.tests.test_routines.test_atleast_nd_two_args(funcname,shape1,shape2)
dask.array.tests.test_routines.test_average(a,returned)
dask.array.tests.test_routines.test_average_keepdims(a)
dask.array.tests.test_routines.test_average_raises()
dask.array.tests.test_routines.test_average_weights(keepdims)
dask.array.tests.test_routines.test_bincount()
dask.array.tests.test_routines.test_bincount_unspecified_minlength()
dask.array.tests.test_routines.test_bincount_with_weights(weights)
dask.array.tests.test_routines.test_choose()
dask.array.tests.test_routines.test_coarsen()
dask.array.tests.test_routines.test_coarsen_bad_chunks(chunks)
dask.array.tests.test_routines.test_coarsen_with_excess()
dask.array.tests.test_routines.test_compress()
dask.array.tests.test_routines.test_corrcoef()
dask.array.tests.test_routines.test_count_nonzero()
dask.array.tests.test_routines.test_count_nonzero_axis(axis)
dask.array.tests.test_routines.test_count_nonzero_obj()
dask.array.tests.test_routines.test_count_nonzero_obj_axis(axis)
dask.array.tests.test_routines.test_count_nonzero_str()
dask.array.tests.test_routines.test_cov()
dask.array.tests.test_routines.test_delete()
dask.array.tests.test_routines.test_derived_docstrings()
dask.array.tests.test_routines.test_diff(shape,n,axis)
dask.array.tests.test_routines.test_diff_append(n)
dask.array.tests.test_routines.test_diff_negative_order()
dask.array.tests.test_routines.test_diff_prepend(n)
dask.array.tests.test_routines.test_digitize()
dask.array.tests.test_routines.test_dot_method()
dask.array.tests.test_routines.test_dot_persist_equivalence()
dask.array.tests.test_routines.test_dstack()
dask.array.tests.test_routines.test_ediff1d(shape,to_end,to_begin)
dask.array.tests.test_routines.test_einsum(einsum_signature)
dask.array.tests.test_routines.test_einsum_broadcasting_contraction()
dask.array.tests.test_routines.test_einsum_broadcasting_contraction2()
dask.array.tests.test_routines.test_einsum_broadcasting_contraction3()
dask.array.tests.test_routines.test_einsum_casting(casting)
dask.array.tests.test_routines.test_einsum_invalid_args()
dask.array.tests.test_routines.test_einsum_optimize(optimize_opts)
dask.array.tests.test_routines.test_einsum_order(order)
dask.array.tests.test_routines.test_einsum_split_every(split_every)
dask.array.tests.test_routines.test_expand_dims(axis)
dask.array.tests.test_routines.test_extract()
dask.array.tests.test_routines.test_flatnonzero()
dask.array.tests.test_routines.test_flip(funcname,kwargs,shape)
dask.array.tests.test_routines.test_gradient(shape,varargs,axis,edge_order)
dask.array.tests.test_routines.test_histogram()
dask.array.tests.test_routines.test_histogram2d(weights,density,bins)
dask.array.tests.test_routines.test_histogram2d_array_bins(weights,density)
dask.array.tests.test_routines.test_histogram_alternative_bins_range()
dask.array.tests.test_routines.test_histogram_bin_range_raises(bins,hist_range)
dask.array.tests.test_routines.test_histogram_bins_range_with_nan_array()
dask.array.tests.test_routines.test_histogram_delayed_bins(density,weighted)
dask.array.tests.test_routines.test_histogram_delayed_n_bins_raises_with_density()
dask.array.tests.test_routines.test_histogram_delayed_range(density,weighted,non_delayed_i,delay_n_bins)
dask.array.tests.test_routines.test_histogram_extra_args_and_shapes()
dask.array.tests.test_routines.test_histogram_normed_deprecation()
dask.array.tests.test_routines.test_histogram_return_type()
dask.array.tests.test_routines.test_histogramdd()
dask.array.tests.test_routines.test_histogramdd_alternative_bins_range()
dask.array.tests.test_routines.test_histogramdd_density()
dask.array.tests.test_routines.test_histogramdd_edges()
dask.array.tests.test_routines.test_histogramdd_raise_incompat_shape()
dask.array.tests.test_routines.test_histogramdd_raise_normed_and_density()
dask.array.tests.test_routines.test_histogramdd_raises_incompat_bins_or_range()
dask.array.tests.test_routines.test_histogramdd_raises_incompat_multiarg_chunks()
dask.array.tests.test_routines.test_histogramdd_raises_incompat_sample_chunks()
dask.array.tests.test_routines.test_histogramdd_raises_incompat_weight_chunks()
dask.array.tests.test_routines.test_histogramdd_seq_of_arrays()
dask.array.tests.test_routines.test_histogramdd_weighted()
dask.array.tests.test_routines.test_histogramdd_weighted_density()
dask.array.tests.test_routines.test_hstack()
dask.array.tests.test_routines.test_insert()
dask.array.tests.test_routines.test_isclose()
dask.array.tests.test_routines.test_iscomplexobj()
dask.array.tests.test_routines.test_isin_assume_unique(assume_unique)
dask.array.tests.test_routines.test_isin_rand(seed,low,high,elements_shape,elements_chunks,test_shape,test_chunks,invert)
dask.array.tests.test_routines.test_isnull()
dask.array.tests.test_routines.test_isnull_result_is_an_array()
dask.array.tests.test_routines.test_matmul(x_shape,y_shape,x_chunks,y_chunks)
dask.array.tests.test_routines.test_moveaxis_rollaxis(funcname,shape)
dask.array.tests.test_routines.test_moveaxis_rollaxis_keyword()
dask.array.tests.test_routines.test_moveaxis_rollaxis_numpy_api()
dask.array.tests.test_routines.test_multi_insert()
dask.array.tests.test_routines.test_nonzero()
dask.array.tests.test_routines.test_nonzero_method()
dask.array.tests.test_routines.test_outer(shape1,shape2)
dask.array.tests.test_routines.test_piecewise()
dask.array.tests.test_routines.test_piecewise_otherwise()
dask.array.tests.test_routines.test_ptp(shape,axis)
dask.array.tests.test_routines.test_ravel()
dask.array.tests.test_routines.test_ravel_1D_no_op()
dask.array.tests.test_routines.test_ravel_multi_index(asarray,arr,chunks,kwargs)
dask.array.tests.test_routines.test_ravel_multi_index_delayed_dims(dims,wrap_in_list)
dask.array.tests.test_routines.test_ravel_multi_index_non_int_dtype()
dask.array.tests.test_routines.test_ravel_multi_index_unknown_shape()
dask.array.tests.test_routines.test_ravel_multi_index_unknown_shape_fails()
dask.array.tests.test_routines.test_ravel_with_array_like()
dask.array.tests.test_routines.test_result_type()
dask.array.tests.test_routines.test_roll(chunks,shift,axis)
dask.array.tests.test_routines.test_roll_always_results_in_a_new_array()
dask.array.tests.test_routines.test_roll_works_even_if_shape_is_0()
dask.array.tests.test_routines.test_rot90(kwargs,shape)
dask.array.tests.test_routines.test_round()
dask.array.tests.test_routines.test_searchsorted(a,a_chunks,v,v_chunks,side)
dask.array.tests.test_routines.test_searchsorted_sorter_not_implemented()
dask.array.tests.test_routines.test_select()
dask.array.tests.test_routines.test_select_broadcasting()
dask.array.tests.test_routines.test_select_multidimension()
dask.array.tests.test_routines.test_select_return_dtype()
dask.array.tests.test_routines.test_shape_and_ndim(shape)
dask.array.tests.test_routines.test_squeeze(is_func,axis)
dask.array.tests.test_routines.test_squeeze_1d_array(shape)
dask.array.tests.test_routines.test_stack_unknown_chunk_sizes(np_func,dsk_func,nan_chunk)
dask.array.tests.test_routines.test_swapaxes()
dask.array.tests.test_routines.test_take()
dask.array.tests.test_routines.test_take_dask_from_numpy()
dask.array.tests.test_routines.test_tensordot()
dask.array.tests.test_routines.test_tensordot_2(axes)
dask.array.tests.test_routines.test_tensordot_double_contraction_neq2(chunks)
dask.array.tests.test_routines.test_tensordot_double_contraction_ngt2()
dask.array.tests.test_routines.test_tensordot_more_than_26_dims()
dask.array.tests.test_routines.test_transpose()
dask.array.tests.test_routines.test_transpose_negative_axes()
dask.array.tests.test_routines.test_transpose_skip_when_possible()
dask.array.tests.test_routines.test_tril_ndims()
dask.array.tests.test_routines.test_tril_triu()
dask.array.tests.test_routines.test_tril_triu_indices(n,k,m,chunks)
dask.array.tests.test_routines.test_tril_triu_non_square_arrays()
dask.array.tests.test_routines.test_union1d(shape,reverse)
dask.array.tests.test_routines.test_unique_kwargs(return_index,return_inverse,return_counts)
dask.array.tests.test_routines.test_unique_rand(seed,low,high,shape,chunks)
dask.array.tests.test_routines.test_unravel_index()
dask.array.tests.test_routines.test_unravel_index_empty()
dask.array.tests.test_routines.test_vdot(shape,chunks)
dask.array.tests.test_routines.test_vstack()
dask.array.tests.test_routines.test_where()
dask.array.tests.test_routines.test_where_bool_optimization()
dask.array.tests.test_routines.test_where_incorrect_args()
dask.array.tests.test_routines.test_where_nonzero()
dask.array.tests.test_routines.test_where_scalar_dtype()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/tests/test_array_utils.py----------------------------------------
A:dask.array.tests.test_array_utils.x->dask.array.Array({('x', 0, 0): (np.ones, (5, 5))}, name='x', chunks=(5, 5), shape=(5, 5), meta=np.ndarray, dtype=float)
dask.array.tests.test_array_utils.test_assert_eq_checks_dtype(a,b)
dask.array.tests.test_array_utils.test_assert_eq_scheduler(a,b)
dask.array.tests.test_array_utils.test_meta_from_array(asarray)
dask.array.tests.test_array_utils.test_meta_from_array_literal(meta,dtype)
dask.array.tests.test_array_utils.test_meta_from_array_type_inputs()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/tests/test_optimization.py----------------------------------------
A:dask.array.tests.test_optimization.result->dask.array.matmul(xx, xx).compute(optimize_graph=optimize_graph)
A:dask.array.tests.test_optimization.lock1->SerializableLock()
A:dask.array.tests.test_optimization.lock2->SerializableLock()
A:dask.array.tests.test_optimization.nil->slice(None)
A:dask.array.tests.test_optimization.x->dask.array.ones(10, chunks=(2,))
A:dask.array.tests.test_optimization.y->dask.array.zeros(10, chunks=(2,))
A:dask.array.tests.test_optimization.dsk->dask.array.matmul(xx, xx).__dask_optimize__(z.dask, z.__dask_keys__())
A:dask.array.tests.test_optimization.dsk2->optimize_blockwise(dsk1)
A:dask.array.tests.test_optimization.fused_key->(dsk2.keys() - {'x', ('dx2', 0)}).pop()
A:dask.array.tests.test_optimization.s->str(v)
A:dask.array.tests.test_optimization.n_getters->len([v for v in dsk.values() if _is_getter_task(v)])
A:dask.array.tests.test_optimization.null->slice(0, None)
A:dask.array.tests.test_optimization.a->dask.array.zeros(10, chunks=(2,)).__dask_optimize__(y.dask, y.__dask_keys__())
A:dask.array.tests.test_optimization.b->dask.array.zeros(10, chunks=(2,)).__dask_optimize__(y.dask, y.__dask_keys__())
A:dask.array.tests.test_optimization.dsk1->dask.array.zeros(10, chunks=(2,)).__dask_optimize__(y.dask, y.__dask_keys__()).__dask_graph__()
A:dask.array.tests.test_optimization.z->dask.array.matmul(xx, xx)
A:dask.array.tests.test_optimization.d->dask.array.from_array(x, chunks=(4, 4))
A:dask.array.tests.test_optimization.X->dask.array.dot(X, X.T)
A:dask.array.tests.test_optimization.(zz,)->dask.optimize(z)
A:dask.array.tests.test_optimization.hlg->dask.blockwise.optimize_blockwise(z.dask)
A:dask.array.tests.test_optimization.za->dask.array.Array(hlg, z.name, z.chunks, z.dtype)
A:dask.array.tests.test_optimization.xx->dask.array.from_array(np.array([[1, 1], [2, 2]]), chunks=1)
dask.array.tests.test_optimization._assert_getter_dsk_eq(a,b)
dask.array.tests.test_optimization._check_get_task_eq(a,b)->bool
dask.array.tests.test_optimization._wrap_getter(func,wrap)
dask.array.tests.test_optimization.getitem(request)
dask.array.tests.test_optimization.getter(request)
dask.array.tests.test_optimization.getter_nofancy(request)
dask.array.tests.test_optimization.test_array_creation_blockwise_fusion()
dask.array.tests.test_optimization.test_disable_lowlevel_fusion()
dask.array.tests.test_optimization.test_dont_fuse_fancy_indexing_in_getter_nofancy(getitem,getter_nofancy)
dask.array.tests.test_optimization.test_dont_fuse_numpy_arrays()
dask.array.tests.test_optimization.test_dont_remove_no_op_slices_for_getter_or_getter_nofancy(which,getitem,getter,getter_nofancy)
dask.array.tests.test_optimization.test_double_dependencies()
dask.array.tests.test_optimization.test_fuse_getitem(getter,getter_nofancy,getitem)
dask.array.tests.test_optimization.test_fuse_getitem_lock(getter,getter_nofancy,getitem)
dask.array.tests.test_optimization.test_fuse_getter_with_asarray(chunks)
dask.array.tests.test_optimization.test_fuse_roots()
dask.array.tests.test_optimization.test_fuse_roots_annotations()
dask.array.tests.test_optimization.test_fuse_slice()
dask.array.tests.test_optimization.test_fuse_slice_with_lists()
dask.array.tests.test_optimization.test_fuse_slices_with_alias(getter,getitem)
dask.array.tests.test_optimization.test_gh3937()
dask.array.tests.test_optimization.test_hard_fuse_slice_cases(getter)
dask.array.tests.test_optimization.test_nonfusible_fancy_indexing()
dask.array.tests.test_optimization.test_optimize_blockwise_duplicate_dependency(optimize_graph)
dask.array.tests.test_optimization.test_optimize_slicing(getter)
dask.array.tests.test_optimization.test_optimize_with_getitem_fusion(getter)
dask.array.tests.test_optimization.test_remove_no_op_slices_for_getitem(getitem)
dask.array.tests.test_optimization.test_turn_off_fusion()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/tests/test_rechunk.py----------------------------------------
A:dask.array.tests.test_rechunk.np->pytest.importorskip('numpy')
A:dask.array.tests.test_rechunk.new->cumdims_label(((1, 1, 2), (1, 5, 1, 0)), 'n')
A:dask.array.tests.test_rechunk.old->cumdims_label(((4,), (1,) * 5), 'o')
A:dask.array.tests.test_rechunk.breaks->tuple((_breakpoints(o, n) for (o, n) in zip(old, new)))
A:dask.array.tests.test_rechunk.cross->list(intersect_chunks(old_chunks=old, new_chunks=new))
A:dask.array.tests.test_rechunk.a->pytest.importorskip('dask.array').ones((8, 8), chunks=(4, 4))
A:dask.array.tests.test_rechunk.x->pytest.importorskip('dask.array').from_array(np.random.default_rng().uniform(size=N))
A:dask.array.tests.test_rechunk.x2->pytest.importorskip('dask.array').from_array(np.random.default_rng().uniform(size=N)).rechunk(chunks=new)
A:dask.array.tests.test_rechunk.y->pytest.importorskip('dask.array').from_array(np.random.default_rng().uniform(size=N)).rechunk(chunks=len(x) // nchunks, balance=True)
A:dask.array.tests.test_rechunk.orig->pytest.importorskip('numpy').random.default_rng().uniform(0, 1, a ** b).reshape((a,) * b)
A:dask.array.tests.test_rechunk.new_blockdims->normalize_chunks(new_chunks, new_shape)
A:dask.array.tests.test_rechunk.check1->rechunk(x, chunks=new_chunks)
A:dask.array.tests.test_rechunk.dd->pytest.importorskip('dask.dataframe')
A:dask.array.tests.test_rechunk.result->old_to_new(old, new)
A:dask.array.tests.test_rechunk.z->pytest.importorskip('dask.array').from_array(np.random.default_rng().uniform(size=N)).rechunk(chunks=len(x) // nchunks, balance=True).rechunk({1: 'auto'})
A:dask.array.tests.test_rechunk.chunks->merge_to_number((1, 1, 1, 1, 3, 1, 1), 1)
A:dask.array.tests.test_rechunk.steps->plan_rechunk(a, b, itemsize=8)
A:dask.array.tests.test_rechunk.steps2->_plan((f, c), (c, f), block_size_limit=3999, itemsize=10)
A:dask.array.tests.test_rechunk.dsk->pytest.importorskip('dask.array').from_array(np.random.default_rng().uniform(size=N)).rechunk(chunks=len(x) // nchunks, balance=True).__dask_graph__()
A:dask.array.tests.test_rechunk.pd->pytest.importorskip('pandas')
A:dask.array.tests.test_rechunk.arr->pytest.importorskip('numpy').random.default_rng().standard_normal((50, 10))
A:dask.array.tests.test_rechunk.expected->pytest.importorskip('dask.array').ones((8, 8), chunks=((4, 4), (4, 0, 0, 4)))
A:dask.array.tests.test_rechunk.xx->pytest.importorskip('dask.array').concatenate([x, x])
A:dask.array.tests.test_rechunk.da->pytest.importorskip('dask.array')
A:dask.array.tests.test_rechunk.balanced->pytest.importorskip('dask.array').from_array(np.random.default_rng().uniform(size=N)).rechunk(chunks=('10MB', -1), balance=True)
A:dask.array.tests.test_rechunk.unbalanced->pytest.importorskip('dask.array').from_array(np.random.default_rng().uniform(size=N)).rechunk(chunks=('10MB', -1), balance=False)
dask.array.tests.test_rechunk._assert_steps(steps,expected)
dask.array.tests.test_rechunk._plan(old_chunks,new_chunks,itemsize=1,block_size_limit=10000000.0,threshold=4)
dask.array.tests.test_rechunk.assert_chunks_match(left,right)
dask.array.tests.test_rechunk.test_balance_2d_negative_dimension()
dask.array.tests.test_rechunk.test_balance_basics()
dask.array.tests.test_rechunk.test_balance_basics_2d()
dask.array.tests.test_rechunk.test_balance_chunks_unchanged()
dask.array.tests.test_rechunk.test_balance_different_inputs()
dask.array.tests.test_rechunk.test_balance_n_chunks_size()
dask.array.tests.test_rechunk.test_balance_raises()
dask.array.tests.test_rechunk.test_balance_small()
dask.array.tests.test_rechunk.test_balance_split_into_n_chunks()
dask.array.tests.test_rechunk.test_divide_to_width()
dask.array.tests.test_rechunk.test_dont_concatenate_single_chunks(shape,chunks)
dask.array.tests.test_rechunk.test_dtype()
dask.array.tests.test_rechunk.test_intersect_1()
dask.array.tests.test_rechunk.test_intersect_2()
dask.array.tests.test_rechunk.test_intersect_chunks_with_nonzero()
dask.array.tests.test_rechunk.test_intersect_chunks_with_zero()
dask.array.tests.test_rechunk.test_intersect_nan()
dask.array.tests.test_rechunk.test_intersect_nan_long()
dask.array.tests.test_rechunk.test_intersect_nan_single()
dask.array.tests.test_rechunk.test_merge_to_number()
dask.array.tests.test_rechunk.test_old_to_new()
dask.array.tests.test_rechunk.test_old_to_new_known()
dask.array.tests.test_rechunk.test_old_to_new_large()
dask.array.tests.test_rechunk.test_old_to_new_single()
dask.array.tests.test_rechunk.test_old_to_new_with_zero()
dask.array.tests.test_rechunk.test_plan_rechunk()
dask.array.tests.test_rechunk.test_plan_rechunk_5d()
dask.array.tests.test_rechunk.test_plan_rechunk_asymmetric()
dask.array.tests.test_rechunk.test_plan_rechunk_heterogeneous()
dask.array.tests.test_rechunk.test_rechunk_0d()
dask.array.tests.test_rechunk.test_rechunk_1d()
dask.array.tests.test_rechunk.test_rechunk_2d()
dask.array.tests.test_rechunk.test_rechunk_4d()
dask.array.tests.test_rechunk.test_rechunk_auto_1d(shape,chunks,bs,expected)
dask.array.tests.test_rechunk.test_rechunk_auto_2d()
dask.array.tests.test_rechunk.test_rechunk_auto_3d()
dask.array.tests.test_rechunk.test_rechunk_auto_image_stack(n)
dask.array.tests.test_rechunk.test_rechunk_avoid_needless_chunking()
dask.array.tests.test_rechunk.test_rechunk_bad_keys()
dask.array.tests.test_rechunk.test_rechunk_blockshape()
dask.array.tests.test_rechunk.test_rechunk_down()
dask.array.tests.test_rechunk.test_rechunk_empty()
dask.array.tests.test_rechunk.test_rechunk_empty_array(arr)
dask.array.tests.test_rechunk.test_rechunk_empty_chunks()
dask.array.tests.test_rechunk.test_rechunk_expand()
dask.array.tests.test_rechunk.test_rechunk_expand2()
dask.array.tests.test_rechunk.test_rechunk_intermediates()
dask.array.tests.test_rechunk.test_rechunk_internals_1()
dask.array.tests.test_rechunk.test_rechunk_method()
dask.array.tests.test_rechunk.test_rechunk_minus_one()
dask.array.tests.test_rechunk.test_rechunk_same()
dask.array.tests.test_rechunk.test_rechunk_same_fully_unknown()
dask.array.tests.test_rechunk.test_rechunk_same_fully_unknown_floats()
dask.array.tests.test_rechunk.test_rechunk_same_partially_unknown()
dask.array.tests.test_rechunk.test_rechunk_unknown_from_array()
dask.array.tests.test_rechunk.test_rechunk_unknown_from_pandas()
dask.array.tests.test_rechunk.test_rechunk_unknown_raises()
dask.array.tests.test_rechunk.test_rechunk_warning()
dask.array.tests.test_rechunk.test_rechunk_with_dict()
dask.array.tests.test_rechunk.test_rechunk_with_empty_input()
dask.array.tests.test_rechunk.test_rechunk_with_fully_unknown_dimension(x,chunks)
dask.array.tests.test_rechunk.test_rechunk_with_fully_unknown_dimension_explicit(new_chunks)
dask.array.tests.test_rechunk.test_rechunk_with_integer()
dask.array.tests.test_rechunk.test_rechunk_with_null_dimensions()
dask.array.tests.test_rechunk.test_rechunk_with_partially_unknown_dimension(x,chunks)
dask.array.tests.test_rechunk.test_rechunk_with_partially_unknown_dimension_explicit(new_chunks)
dask.array.tests.test_rechunk.test_rechunk_with_zero()
dask.array.tests.test_rechunk.test_rechunk_with_zero_placeholders()
dask.array.tests.test_rechunk.test_rechunk_zero()
dask.array.tests.test_rechunk.test_rechunk_zero_dim()
dask.array.tests.test_rechunk.test_rechunk_zero_dim_array()
dask.array.tests.test_rechunk.test_rechunk_zero_dim_array_II()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/tests/test_ufunc.py----------------------------------------
A:dask.array.tests.test_ufunc.np->pytest.importorskip('numpy')
A:dask.array.tests.test_ufunc.disclaimer->DISCLAIMER.format(name=name)
A:dask.array.tests.test_ufunc.ufunc->getattr(da, name)
A:dask.array.tests.test_ufunc.dafunc->getattr(da, func)
A:dask.array.tests.test_ufunc.npfunc->getattr(np, func)
A:dask.array.tests.test_ufunc.arr->pytest.importorskip('numpy').random.randint(1, 100, size=(20, 20))
A:dask.array.tests.test_ufunc.darr->dask.array.from_array(arr, 3)
A:dask.array.tests.test_ufunc.arr1->pytest.importorskip('numpy').array([1, 2, 3])
A:dask.array.tests.test_ufunc.darr1->dask.array.from_array(arr1)
A:dask.array.tests.test_ufunc.arr2->pytest.importorskip('numpy').array([4, 5, 6])
A:dask.array.tests.test_ufunc.darr2->dask.array.from_array(arr2)
A:dask.array.tests.test_ufunc.real->pytest.importorskip('numpy').random.randint(1, 100, size=(20, 20))
A:dask.array.tests.test_ufunc.dareal->dask.array.from_array(real, 3)
A:dask.array.tests.test_ufunc.daimag->dask.array.from_array(imag, 3)
A:dask.array.tests.test_ufunc.dacomp->dask.array.from_array(comp, 3)
A:dask.array.tests.test_ufunc.(res1, res2)->dafunc(arr)
A:dask.array.tests.test_ufunc.(exp1, exp2)->npfunc(arr)
A:dask.array.tests.test_ufunc.x->dask.array.arange(10, chunks=(5,))
A:dask.array.tests.test_ufunc.d->dask.array.from_array(x, chunks=(2, 2))
A:dask.array.tests.test_ufunc.myadd->dask.array.frompyfunc(add, 2, 1)
A:dask.array.tests.test_ufunc.np_myadd->pytest.importorskip('numpy').frompyfunc(add, 2, 1)
A:dask.array.tests.test_ufunc.dx->dask.array.from_array(x, chunks=(3, 4))
A:dask.array.tests.test_ufunc.y->dask.array.arange(15, chunks=(5,))
A:dask.array.tests.test_ufunc.dy->dask.array.from_array(y, chunks=2)
A:dask.array.tests.test_ufunc.f->da_frompyfunc(add, 2, 1)
A:dask.array.tests.test_ufunc.np_f->pytest.importorskip('numpy').frompyfunc(add, 2, 1)
A:dask.array.tests.test_ufunc.f2->pickle.loads(pickle.dumps(f))
A:dask.array.tests.test_ufunc.empty->pytest.importorskip('numpy').empty(10, dtype=x.dtype)
A:dask.array.tests.test_ufunc.result->dask.array.add(left, right, where=where, out=out)
A:dask.array.tests.test_ufunc.expected->pytest.importorskip('numpy').add(left, right, where=where)
A:dask.array.tests.test_ufunc.left->dask.array.from_array(np.arange(4, dtype='i8'), chunks=2)
A:dask.array.tests.test_ufunc.right->dask.array.from_array(np.arange(4, 8, dtype='i8'), chunks=2)
A:dask.array.tests.test_ufunc.out->dask.array.from_array(out_np, chunks=2)
A:dask.array.tests.test_ufunc.d_out->dask.array.zeros(where.shape, dtype=left.dtype)
A:dask.array.tests.test_ufunc.d_wherewhere->pytest.importorskip('numpy').array([False, True, True, False])
A:dask.array.tests.test_ufunc.d_where->dask.array.from_array(where, chunks=2)
A:dask.array.tests.test_ufunc.where->dask.array.from_array(np.array([1, 0, 0, 1], dtype='bool'), chunks=2)
A:dask.array.tests.test_ufunc.d_left->dask.array.from_array(left, chunks=2)
A:dask.array.tests.test_ufunc.d_right->dask.array.from_array(right, chunks=2)
A:dask.array.tests.test_ufunc.expected_masked->pytest.importorskip('numpy').where(where, expected, 0)
A:dask.array.tests.test_ufunc.result_masked->pytest.importorskip('numpy').where(where, expected, 0)
A:dask.array.tests.test_ufunc.expected_no_where->pytest.importorskip('numpy').add(left, right)
A:dask.array.tests.test_ufunc.out_np->pytest.importorskip('numpy').zeros(4, dtype='i8')
dask.array.tests.test_ufunc.test_angle()
dask.array.tests.test_ufunc.test_array_ufunc()
dask.array.tests.test_ufunc.test_array_ufunc_binop()
dask.array.tests.test_ufunc.test_array_ufunc_out()
dask.array.tests.test_ufunc.test_binary_ufunc(ufunc)
dask.array.tests.test_ufunc.test_clip()
dask.array.tests.test_ufunc.test_complex(ufunc)
dask.array.tests.test_ufunc.test_divmod()
dask.array.tests.test_ufunc.test_dtype_kwarg(dt)
dask.array.tests.test_ufunc.test_frompyfunc()
dask.array.tests.test_ufunc.test_frompyfunc_wrapper()
dask.array.tests.test_ufunc.test_issignedinf()
dask.array.tests.test_ufunc.test_non_ufunc_others(func)
dask.array.tests.test_ufunc.test_out_numpy()
dask.array.tests.test_ufunc.test_out_shape_mismatch()
dask.array.tests.test_ufunc.test_ufunc()
dask.array.tests.test_ufunc.test_ufunc_2results(ufunc)
dask.array.tests.test_ufunc.test_ufunc_meta(name)
dask.array.tests.test_ufunc.test_ufunc_outer()
dask.array.tests.test_ufunc.test_ufunc_where(dtype,left_is_da,right_is_da,where_kind)
dask.array.tests.test_ufunc.test_ufunc_where_broadcasts(left_is_da,right_is_da,where_is_da)
dask.array.tests.test_ufunc.test_ufunc_where_doesnt_mutate_out()
dask.array.tests.test_ufunc.test_ufunc_where_no_out()
dask.array.tests.test_ufunc.test_unary_ufunc(ufunc)
dask.array.tests.test_ufunc.test_unsupported_ufunc_methods()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/tests/test_cupy_creation.py----------------------------------------
A:dask.array.tests.test_cupy_creation.cupy->pytest.importorskip('cupy')
A:dask.array.tests.test_cupy_creation.v->dask.array.from_array(v, chunks=(1, (1, 2), (1, 2, 1), (2, 1, 2), (5, 1)), asarray=False)
A:dask.array.tests.test_cupy_creation.dv->dask.array.from_array(v, chunks=(4,), asarray=False)
A:dask.array.tests.test_cupy_creation.darr->dask.array.diag(dv)
A:dask.array.tests.test_cupy_creation.cupyarr->pytest.importorskip('cupy').diag(v)
A:dask.array.tests.test_cupy_creation.x->dask.array.from_array(cupy.arange(11), chunks=(4,))
A:dask.array.tests.test_cupy_creation.dx->dask.array.from_array(x, chunks=(4, 4), asarray=False)
A:dask.array.tests.test_cupy_creation.np_a->numpy.random.default_rng().random(shape)
A:dask.array.tests.test_cupy_creation.da_a->dask.array.from_array(cupy.array(np_a), chunks=chunks)
A:dask.array.tests.test_cupy_creation.np_r->numpy.pad(np_a, pad_width, mode, **kwargs)
A:dask.array.tests.test_cupy_creation.da_r->dask.array.pad(da_a, pad_width, mode, **kwargs)
A:dask.array.tests.test_cupy_creation.cp_a->pytest.importorskip('cupy').tri(*args)
A:dask.array.tests.test_cupy_creation.xp_a->xp.tri(*args, like=da.from_array(cupy.array(())))
A:dask.array.tests.test_cupy_creation.x_new->dask.array.from_array(cupy.arange(11), chunks=(4,)).to_backend()
dask.array.tests.test_cupy_creation.test_diag()
dask.array.tests.test_cupy_creation.test_diagonal()
dask.array.tests.test_cupy_creation.test_pad(shape,chunks,pad_width,mode,kwargs)
dask.array.tests.test_cupy_creation.test_to_backend_cupy()
dask.array.tests.test_cupy_creation.test_tri_like(xp,N,M,k,dtype,chunks)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/tests/test_dispatch.py----------------------------------------
A:dask.array.tests.test_dispatch.a->dask.array.from_array(['a', 'b', '.', 'd'])
A:dask.array.tests.test_dispatch.arrs->tuple((arr if not isinstance(arr, type(self)) else arr.arr for arr in arrs))
A:dask.array.tests.test_dispatch.t->tuple((ti for ti in t if not issubclass(ti, type(self))))
A:dask.array.tests.test_dispatch.__getitem__->wrap('__getitem__')
A:dask.array.tests.test_dispatch.__setitem__->wrap('__setitem__')
A:dask.array.tests.test_dispatch.inputs->tuple(self._downcast_args(inputs))
A:dask.array.tests.test_dispatch.shape->dispatch_property('shape')
A:dask.array.tests.test_dispatch.ndim->dispatch_property('ndim')
A:dask.array.tests.test_dispatch.dtype->dispatch_property('dtype')
A:dask.array.tests.test_dispatch.astype->wrap('astype')
A:dask.array.tests.test_dispatch.sum->wrap('sum')
A:dask.array.tests.test_dispatch.prod->wrap('prod')
A:dask.array.tests.test_dispatch.reshape->wrap('reshape')
A:dask.array.tests.test_dispatch.squeeze->wrap('squeeze')
A:dask.array.tests.test_dispatch.args->tuple(self._downcast_args(args))
A:dask.array.tests.test_dispatch.b->WrappedArray(np.arange(4))
A:dask.array.tests.test_dispatch.outputs->kwargs.get('out', ())
A:dask.array.tests.test_dispatch.s->UnknownScalar()
dask.array.tests.test_dispatch.EncapsulateNDArray(self,arr)
dask.array.tests.test_dispatch.EncapsulateNDArray.__array__(self,*args,**kwargs)
dask.array.tests.test_dispatch.EncapsulateNDArray.__array_function__(self,f,t,arrs,kw)
dask.array.tests.test_dispatch.EncapsulateNDArray.__array_ufunc__(self,ufunc,method,*inputs,**kwargs)
dask.array.tests.test_dispatch.EncapsulateNDArray.__init__(self,arr)
dask.array.tests.test_dispatch.UnknownScalar
dask.array.tests.test_dispatch.UnknownScalar.__mul__(self,other)
dask.array.tests.test_dispatch.UnknownScalarThatUnderstandsArrayOps(np.lib.mixins.NDArrayOperatorsMixin)
dask.array.tests.test_dispatch.UnknownScalarThatUnderstandsArrayOps.__array_ufunc__(self,ufunc,method,*inputs,**kwargs)
dask.array.tests.test_dispatch.WrappedArray(self,arr,**attrs)
dask.array.tests.test_dispatch.WrappedArray.__array__(self,*args,**kwargs)
dask.array.tests.test_dispatch.WrappedArray.__array_function__(self,func,types,args,kwargs)
dask.array.tests.test_dispatch.WrappedArray.__array_ufunc__(self,ufunc,method,*inputs,**kwargs)
dask.array.tests.test_dispatch.WrappedArray.__dask_graph__(self)
dask.array.tests.test_dispatch.WrappedArray.__getitem__(self,key)
dask.array.tests.test_dispatch.WrappedArray.__init__(self,arr,**attrs)
dask.array.tests.test_dispatch.WrappedArray.__setitem__(self,key,value)
dask.array.tests.test_dispatch.WrappedArray._downcast_args(self,args)
dask.array.tests.test_dispatch.dispatch_property(prop_name)
dask.array.tests.test_dispatch.test_binary_operation_type_precedence(op,arr_upcast,arr_downcast)
dask.array.tests.test_dispatch.test_delegation_specific_cases()
dask.array.tests.test_dispatch.test_delegation_unknown_scalar(arr)
dask.array.tests.test_dispatch.test_delegation_unknown_scalar_that_understands_arr_ops(arr)
dask.array.tests.test_dispatch.test_direct_deferral_wrapping_override()
dask.array.tests.test_dispatch.test_is_valid_array_chunk(arr,result)
dask.array.tests.test_dispatch.test_is_valid_chunk_type(arr_type,result)
dask.array.tests.test_dispatch.wrap(func_name)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/tests/test_percentiles.py----------------------------------------
A:dask.array.tests.test_percentiles.percentile_internal_methods->pytest.mark.parametrize('internal_method', [pytest.param('tdigest', marks=pytest.mark.skipif(not crick, reason='Requires crick')), 'dask'])
A:dask.array.tests.test_percentiles.d->dask.array.ones((16,), chunks=(4,))
A:dask.array.tests.test_percentiles.x->dask.array.random.default_rng().random(1000, chunks=(100,))
A:dask.array.tests.test_percentiles.result->dask.array.percentile(x, 50, internal_method=internal_method).compute()
A:dask.array.tests.test_percentiles.x0->pandas.Categorical(['Alice', 'Bob', 'Charlie', 'Dennis', 'Alice', 'Alice'])
A:dask.array.tests.test_percentiles.x1->pandas.Categorical(['Alice', 'Bob', 'Charlie', 'Dennis', 'Alice', 'Alice'])
A:dask.array.tests.test_percentiles.p->dask.array.percentile(x, [50])
A:dask.array.tests.test_percentiles.(a, b)->dask.array.percentile(x, [40, 60], internal_method=internal_method).compute()
dask.array.tests.test_percentiles.test_percentile(internal_method)
dask.array.tests.test_percentiles.test_percentile_with_categoricals()
dask.array.tests.test_percentiles.test_percentiles_with_empty_arrays(internal_method)
dask.array.tests.test_percentiles.test_percentiles_with_empty_q(internal_method)
dask.array.tests.test_percentiles.test_percentiles_with_scaler_percentile(internal_method,q)
dask.array.tests.test_percentiles.test_unknown_chunk_sizes(internal_method)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/tests/test_cupy_sparse.py----------------------------------------
A:dask.array.tests.test_cupy_sparse.cupy->pytest.importorskip('cupy')
A:dask.array.tests.test_cupy_sparse.cupyx->pytest.importorskip('cupyx')
A:dask.array.tests.test_cupy_sparse.x->cupy.random.default_rng().random(x_shape, dtype=dtype)
A:dask.array.tests.test_cupy_sparse.sp->sp.map_blocks(cupyx.scipy.sparse.csr_matrix, dtype=cupy.float32).map_blocks(cupyx.scipy.sparse.csr_matrix, dtype=cupy.float32)
A:dask.array.tests.test_cupy_sparse.y->cupy.random.default_rng().random(y_shape, dtype=dtype)
A:dask.array.tests.test_cupy_sparse.rng->pytest.importorskip('cupy').random.default_rng()
A:dask.array.tests.test_cupy_sparse.meta->pytest.importorskip('cupyx').scipy.sparse.csr_matrix((0, 0))
A:dask.array.tests.test_cupy_sparse.z->cupy.random.default_rng().random(x_shape, dtype=dtype).dot(y)
A:dask.array.tests.test_cupy_sparse.z_expected->sp_concatenate([cupyx.scipy.sparse.csr_matrix(e.compute()) for e in xs])
A:dask.array.tests.test_cupy_sparse.da_x->da_x.map_blocks(sp_matrix, meta=sp_matrix(cupy.array([0], dtype=dtype))).map_blocks(sp_matrix, meta=sp_matrix(cupy.array([0], dtype=dtype)))
A:dask.array.tests.test_cupy_sparse.da_y->da_y.map_blocks(sp_matrix, meta=sp_matrix(cupy.array([0], dtype=dtype))).map_blocks(sp_matrix, meta=sp_matrix(cupy.array([0], dtype=dtype)))
A:dask.array.tests.test_cupy_sparse.da_z->dask.array.dot(da_x, da_y).compute()
dask.array.tests.test_cupy_sparse.test_sparse_concatenate(axis)
dask.array.tests.test_cupy_sparse.test_sparse_dot(sp_format,input_sizes)
dask.array.tests.test_cupy_sparse.test_sparse_hstack_vstack_csr()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/tests/test_cupy_random.py----------------------------------------
A:dask.array.tests.test_cupy_random.cupy->pytest.importorskip('cupy')
A:dask.array.tests.test_cupy_random.a->func(*args, **kwargs)
A:dask.array.tests.test_cupy_random.rs->dask.array.random.RandomState(RandomState=cupy.random.RandomState)
A:dask.array.tests.test_cupy_random.rng->dask.array.random.default_rng(cupy.random.default_rng())
A:dask.array.tests.test_cupy_random.state->dask.array.random.default_rng(5)
A:dask.array.tests.test_cupy_random.x->dask.array.random.default_rng(cupy.random.default_rng()).poisson(1.0, size=shape, chunks=3)
A:dask.array.tests.test_cupy_random.y->dask.array.random.default_rng(5).standard_normal(size=(2, 3), chunks=3)
dask.array.tests.test_cupy_random.test_cupy_unsupported()
dask.array.tests.test_cupy_random.test_random_Generator_processes(backend)
dask.array.tests.test_cupy_random.test_random_all_Generator(backend,gen,shape)
dask.array.tests.test_cupy_random.test_random_all_RandomState(backend,rs)
dask.array.tests.test_cupy_random.test_random_all_direct_calls(backend)
dask.array.tests.test_cupy_random.test_random_shapes(shape)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/tests/test_slicing.py----------------------------------------
A:dask.array.tests.test_slicing.np->pytest.importorskip('numpy')
A:dask.array.tests.test_slicing.result->dask.array.random.default_rng().integers(0, 1000, size=size, chunks=chunks).compute()
A:dask.array.tests.test_slicing.(result, chunks)->slice_array('y', 'x', ([5, 5], [5, 5]), (slice(0, 7), 1), itemsize=8)
A:dask.array.tests.test_slicing.(chunks, dsk)->take('y', 'x', [(20, 20, 20, 20), (20, 20)], [1, 3, 5, 37], itemsize=8, axis=1)
A:dask.array.tests.test_slicing.expected->pytest.importorskip('numpy').arange(13, 24)
A:dask.array.tests.test_slicing.(a, bd1)->slice_array('y', 'x', ((3, 3, 3, 1), (3, 3, 3, 1)), (np.array([1, 2, 9]), slice(None, None, None)), itemsize=8)
A:dask.array.tests.test_slicing.(b, bd2)->slice_array('y', 'x', ((3, 3, 3, 1), (3, 3, 3, 1)), (np.array([1, 2, 9]), slice(None, None, None)), itemsize=8)
A:dask.array.tests.test_slicing.index->pytest.importorskip('numpy').arange(len(x))
A:dask.array.tests.test_slicing.(c, bd3)->slice_array('y', 'x', ((3, 3, 3, 1), (3, 3, 3, 1)), index, itemsize=8)
A:dask.array.tests.test_slicing.o->dask.array.ones((24, 16), chunks=((4, 8, 8, 4), (2, 6, 6, 2)))
A:dask.array.tests.test_slicing.a->pytest.importorskip('numpy').arange(4 * 500 * 500).reshape(4, 500, 500)
A:dask.array.tests.test_slicing.x->dask.array.random.default_rng().integers(0, 1000, size=size, chunks=chunks)
A:dask.array.tests.test_slicing.I->ReturnItem()
A:dask.array.tests.test_slicing.ind->dask.array.from_array(ind, chunks=2)
A:dask.array.tests.test_slicing.dx->dask.array.from_array(x, chunks=2)
A:dask.array.tests.test_slicing.pd->pytest.importorskip('pandas')
A:dask.array.tests.test_slicing.(dsk_out, bd_out)->slice_array('in', 'out', blockdims, index, itemsize=8)
A:dask.array.tests.test_slicing.idx->pytest.importorskip('numpy').array([0, 0, 1, 1])
A:dask.array.tests.test_slicing.expect->pytest.importorskip('numpy').array([[40, 10, 20], [90, 60, 70], [140, 110, 120]])
A:dask.array.tests.test_slicing.idx0->dask.array.from_array(1, chunks=1)
A:dask.array.tests.test_slicing.d->dask.array.from_array(x, chunks=shape)
A:dask.array.tests.test_slicing.rng->pytest.importorskip('numpy').random.default_rng()
A:dask.array.tests.test_slicing.dind->dask.array.from_array(ind, chunks=4)
A:dask.array.tests.test_slicing.X->dask.array.random.default_rng().random(size=(100, 2), chunks=(2, 2))
A:dask.array.tests.test_slicing.y->dask.array.core.asarray(x)
A:dask.array.tests.test_slicing.(result,)->normalize_index([-5, -2, 1], (np.nan,))
A:dask.array.tests.test_slicing.plan->slicing_plan(chunks, index=index)
A:dask.array.tests.test_slicing.arr->dask.array.from_array(a, chunks=(1, 500, 500))
A:dask.array.tests.test_slicing.(chunks2, dsk)->take('a', 'b', chunks, index, itemsize)
A:dask.array.tests.test_slicing.mask->dask.array.zeros(array_size, chunks=chunk_size2)
A:dask.array.tests.test_slicing.(a, b)->make_block_sorted_slices(index, x.chunks)
A:dask.array.tests.test_slicing.index2->pytest.importorskip('numpy').array([0, 2, 4, 6, 1, 3, 5, 7])
A:dask.array.tests.test_slicing.index3->pytest.importorskip('numpy').array([3, 0, 2, 1, 7, 4, 6, 5])
A:dask.array.tests.test_slicing.b->shuffle_slice(x, index)
A:dask.array.tests.test_slicing.a1->dask.array.from_array(np.zeros(3), chunks=1, asarray=asarray, lock=lock, fancy=fancy)
A:dask.array.tests.test_slicing.a2->dask.array.from_array(np.ones(3), chunks=1, asarray=asarray, lock=lock, fancy=fancy)
A:dask.array.tests.test_slicing.al->dask.array.stack([a1, a2])
A:dask.array.tests.test_slicing.array->dask.array.from_array(np.zeros((3, 0)))
A:dask.array.tests.test_slicing.actual->array[mask].compute()
dask.array.tests.test_slicing.ReturnItem
dask.array.tests.test_slicing.ReturnItem.__getitem__(self,key)
dask.array.tests.test_slicing.test_None_overlap_int()
dask.array.tests.test_slicing.test_boolean_list_slicing()
dask.array.tests.test_slicing.test_boolean_numpy_array_slicing()
dask.array.tests.test_slicing.test_cull()
dask.array.tests.test_slicing.test_empty_list()
dask.array.tests.test_slicing.test_empty_slice()
dask.array.tests.test_slicing.test_getitem_avoids_large_chunks()
dask.array.tests.test_slicing.test_getitem_avoids_large_chunks_missing()
dask.array.tests.test_slicing.test_gh3579()
dask.array.tests.test_slicing.test_gh4043(lock,asarray,fancy)
dask.array.tests.test_slicing.test_index_with_bool_dask_array()
dask.array.tests.test_slicing.test_index_with_bool_dask_array_2()
dask.array.tests.test_slicing.test_index_with_int_dask_array(x_chunks,idx_chunks)
dask.array.tests.test_slicing.test_index_with_int_dask_array_0d(chunks)
dask.array.tests.test_slicing.test_index_with_int_dask_array_dtypes(dtype)
dask.array.tests.test_slicing.test_index_with_int_dask_array_indexerror(chunks)
dask.array.tests.test_slicing.test_index_with_int_dask_array_nanchunks(chunks)
dask.array.tests.test_slicing.test_index_with_int_dask_array_negindex(chunks)
dask.array.tests.test_slicing.test_index_with_int_dask_array_nocompute()
dask.array.tests.test_slicing.test_make_blockwise_sorted_slice()
dask.array.tests.test_slicing.test_multiple_list_slicing()
dask.array.tests.test_slicing.test_negative_list_slicing()
dask.array.tests.test_slicing.test_negative_n_slicing()
dask.array.tests.test_slicing.test_new_blockdim()
dask.array.tests.test_slicing.test_normalize_index()
dask.array.tests.test_slicing.test_oob_check()
dask.array.tests.test_slicing.test_pathological_unsorted_slicing()
dask.array.tests.test_slicing.test_permit_oob_slices()
dask.array.tests.test_slicing.test_sanitize_index()
dask.array.tests.test_slicing.test_sanitize_index_element()
dask.array.tests.test_slicing.test_setitem_with_different_chunks_preserves_shape(params)
dask.array.tests.test_slicing.test_shuffle_slice(size,chunks)
dask.array.tests.test_slicing.test_slice_1d()
dask.array.tests.test_slicing.test_slice_array_1d()
dask.array.tests.test_slicing.test_slice_array_2d()
dask.array.tests.test_slicing.test_slice_array_3d_with_bool_numpy_array()
dask.array.tests.test_slicing.test_slice_array_null_dimension()
dask.array.tests.test_slicing.test_slice_list_then_None()
dask.array.tests.test_slicing.test_slice_optimizations()
dask.array.tests.test_slicing.test_slice_singleton_value_on_boundary()
dask.array.tests.test_slicing.test_slice_stop_0()
dask.array.tests.test_slicing.test_slicing_and_chunks()
dask.array.tests.test_slicing.test_slicing_and_unknown_chunks()
dask.array.tests.test_slicing.test_slicing_chunks()
dask.array.tests.test_slicing.test_slicing_consistent_names()
dask.array.tests.test_slicing.test_slicing_consistent_names_after_normalization()
dask.array.tests.test_slicing.test_slicing_exhaustively()
dask.array.tests.test_slicing.test_slicing_identities()
dask.array.tests.test_slicing.test_slicing_integer_no_warnings()
dask.array.tests.test_slicing.test_slicing_none_int_ellipes()
dask.array.tests.test_slicing.test_slicing_plan(chunks,index,expected)
dask.array.tests.test_slicing.test_slicing_with_Nones(shape,index)
dask.array.tests.test_slicing.test_slicing_with_negative_step_flops_keys()
dask.array.tests.test_slicing.test_slicing_with_newaxis()
dask.array.tests.test_slicing.test_slicing_with_numpy_arrays()
dask.array.tests.test_slicing.test_slicing_with_singleton_indices()
dask.array.tests.test_slicing.test_take()
dask.array.tests.test_slicing.test_take_avoids_large_chunks()
dask.array.tests.test_slicing.test_take_semi_sorted()
dask.array.tests.test_slicing.test_take_sorted()
dask.array.tests.test_slicing.test_take_uses_config()
dask.array.tests.test_slicing.test_uneven_blockdims()
dask.array.tests.test_slicing.test_uneven_chunks()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/tests/test_cupy_overlap.py----------------------------------------
A:dask.array.tests.test_cupy_overlap.cupy->pytest.importorskip('cupy')
A:dask.array.tests.test_cupy_overlap.cupy_version->parse_version(cupy.__version__)
A:dask.array.tests.test_cupy_overlap.x->pytest.importorskip('cupy').arange(64).reshape((8, 8))
A:dask.array.tests.test_cupy_overlap.d->dask.array.from_array(x, chunks=(4, 4), asarray=False)
A:dask.array.tests.test_cupy_overlap.g->dask.array.overlap.overlap_internal(d, {0: 2, 1: 1})
A:dask.array.tests.test_cupy_overlap.expected->numpy.array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [7, 0, 1, 2, 3, 4, 5, 6, 7, 0], [15, 8, 9, 10, 11, 12, 13, 14, 15, 8], [23, 16, 17, 18, 19, 20, 21, 22, 23, 16], [31, 24, 25, 26, 27, 28, 29, 30, 31, 24], [39, 32, 33, 34, 35, 36, 37, 38, 39, 32], [47, 40, 41, 42, 43, 44, 45, 46, 47, 40], [55, 48, 49, 50, 51, 52, 53, 54, 55, 48], [63, 56, 57, 58, 59, 60, 61, 62, 63, 56], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])
A:dask.array.tests.test_cupy_overlap.e->dask.array.overlap.boundaries(d, {0: 2, 1: 1}, {0: 0, 1: 'periodic'})
dask.array.tests.test_cupy_overlap.test_boundaries()
dask.array.tests.test_cupy_overlap.test_constant()
dask.array.tests.test_cupy_overlap.test_nearest()
dask.array.tests.test_cupy_overlap.test_overlap_internal()
dask.array.tests.test_cupy_overlap.test_periodic()
dask.array.tests.test_cupy_overlap.test_reflect()
dask.array.tests.test_cupy_overlap.test_trim_internal()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/tests/test_cupy_slicing.py----------------------------------------
A:dask.array.tests.test_cupy_slicing.cupy->pytest.importorskip('cupy')
A:dask.array.tests.test_cupy_slicing.x->dask.array.arange(5, chunks=-1, like=cupy.array(()))
A:dask.array.tests.test_cupy_slicing.idx->dask.array.Array({('x', 0): (crash,)}, name='x', chunks=((2,),), dtype=np.int64)
A:dask.array.tests.test_cupy_slicing.expect->pytest.importorskip('cupy').array([[40, 10, 20], [90, 60, 70], [140, 110, 120]])
A:dask.array.tests.test_cupy_slicing.orig_idx->pytest.importorskip('cupy').array(orig_idx)
A:dask.array.tests.test_cupy_slicing.idx0->dask.array.from_array(cupy.array(1), chunks=1)
A:dask.array.tests.test_cupy_slicing.a->dask.array.from_array(cupy.array([10, 20, 30, 40]), chunks=-1)
dask.array.tests.test_cupy_slicing.test_index_with_int_dask_array(x_chunks,idx_chunks)
dask.array.tests.test_cupy_slicing.test_index_with_int_dask_array_0d(chunks)
dask.array.tests.test_cupy_slicing.test_index_with_int_dask_array_dtypes(dtype)
dask.array.tests.test_cupy_slicing.test_index_with_int_dask_array_indexerror(chunks)
dask.array.tests.test_cupy_slicing.test_index_with_int_dask_array_nanchunks(chunks)
dask.array.tests.test_cupy_slicing.test_index_with_int_dask_array_negindex(chunks)
dask.array.tests.test_cupy_slicing.test_index_with_int_dask_array_nep35(x_chunks,idx_chunks)
dask.array.tests.test_cupy_slicing.test_index_with_int_dask_array_nocompute()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/tests/test_xarray.py----------------------------------------
A:dask.array.tests.test_xarray.xr->pytest.importorskip('xarray')
A:dask.array.tests.test_xarray.y->dask.array.asarray(xr.DataArray([1, 2, 3.0]))
A:dask.array.tests.test_xarray.coord->dask.array.arange(8, chunks=-1)
A:dask.array.tests.test_xarray.x->pytest.importorskip('xarray').DataArray(data, coords={'x': coord, 'y': coord}, dims=['x', 'y'])
A:dask.array.tests.test_xarray.result->dask.array.fft.fft(x)
A:dask.array.tests.test_xarray.expected->dask.array.fft.fft(x.data)
dask.array.tests.test_xarray.test_asanyarray()
dask.array.tests.test_xarray.test_asarray()
dask.array.tests.test_xarray.test_asarray_xarray_intersphinx_workaround()
dask.array.tests.test_xarray.test_fft()
dask.array.tests.test_xarray.test_mean()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/tests/test_creation.py----------------------------------------
A:dask.array.tests.test_creation.backend_lib->pytest.importorskip(backend)
A:dask.array.tests.test_creation.np_func->getattr(np, funcname)
A:dask.array.tests.test_creation.da_func->getattr(da, funcname)
A:dask.array.tests.test_creation.shape->cast_shape(shape)
A:dask.array.tests.test_creation.chunks->cast_chunks(chunks)
A:dask.array.tests.test_creation.dtype->numpy.dtype(dtype)
A:dask.array.tests.test_creation.a->dask.array.asarray([0, 1, 2])
A:dask.array.tests.test_creation.np_r->numpy.pad(np_a, pad_width, udf_pad, **kwargs)
A:dask.array.tests.test_creation.da_r->dask.array.pad(da_a, pad_width, udf_pad, **kwargs)
A:dask.array.tests.test_creation.darr->dask.array.diag(v, k)
A:dask.array.tests.test_creation.nparr->numpy.diag(v, k)
A:dask.array.tests.test_creation.(darr, dstep)->dask.array.linspace(6, 49, endpoint=endpoint, chunks=5, retstep=True)
A:dask.array.tests.test_creation.(nparr, npstep)->numpy.linspace(6, 49, endpoint=endpoint, retstep=True)
A:dask.array.tests.test_creation.x->dask.array.ones((8, 8), chunks=(4, 4))
A:dask.array.tests.test_creation.a_np->numpy.empty_like(y2, dtype=dtype)
A:dask.array.tests.test_creation.a_da->dask.array.empty_like(y1, dtype=dtype).compute()
A:dask.array.tests.test_creation.expected->numpy.ones((8,))
A:dask.array.tests.test_creation.result->dask.array.indices(shape, chunks='auto')
A:dask.array.tests.test_creation.xi_d_e->dask.array.from_array(xi_a[-1], chunks=each_chunk)
A:dask.array.tests.test_creation.xi_d_ef->dask.array.from_array(xi_a[-1], chunks=each_chunk).flatten()
A:dask.array.tests.test_creation.do->list(range(len(xi_dc)))
A:dask.array.tests.test_creation.xi_dc->tuple(xi_dc)
A:dask.array.tests.test_creation.r_a->numpy.meshgrid(*xi_a, indexing=indexing, sparse=sparse)
A:dask.array.tests.test_creation.r_d->dask.array.meshgrid(*xi_d, indexing=indexing, sparse=sparse)
A:dask.array.tests.test_creation.b->numpy.array([4, 5, 6, 7])
A:dask.array.tests.test_creation.(x, y)->numpy.meshgrid(a, b, indexing='ij')
A:dask.array.tests.test_creation.(x_d, y_d)->dask.array.meshgrid(a, b, indexing='ij')
A:dask.array.tests.test_creation.v->dask.array.from_array(v, chunks=(4, 4, 2))
A:dask.array.tests.test_creation.d->dask.array.diagonal(dd)
A:dask.array.tests.test_creation.y->numpy.arange(5 * 8).reshape((5, 8))
A:dask.array.tests.test_creation.d2->dask.array.fromfunction(func, shape=(5, 5), chunks=(2, 2), dtype=dtype, **kwargs)
A:dask.array.tests.test_creation.np_a->numpy.random.random(shape)
A:dask.array.tests.test_creation.da_a->dask.array.from_array(np_a, chunks=chunks)
A:dask.array.tests.test_creation.scaler->inner_kwargs.get('scaler', 1)
A:dask.array.tests.test_creation.dd->dask.array.ones((8, 8), chunks=(4, 4))
A:dask.array.tests.test_creation.dafn->getattr(da, fn)
A:dask.array.tests.test_creation.npfn->getattr(np, fn)
A:dask.array.tests.test_creation.x1->dask.array.random.standard_normal(size=shape, chunks=chunks)
A:dask.array.tests.test_creation.x2->dask.array.random.standard_normal(size=shape, chunks=chunks).compute()
dask.array.tests.test_creation.test_arange()
dask.array.tests.test_creation.test_arange_cast_float_int_step()
dask.array.tests.test_creation.test_arange_dtypes(start,stop,step,dtype)
dask.array.tests.test_creation.test_arange_float_step()
dask.array.tests.test_creation.test_arr_like(funcname,shape,cast_shape,dtype,cast_chunks,chunks,name,order,backend)
dask.array.tests.test_creation.test_arr_like_shape(funcname,kwargs,shape,dtype,chunks,out_shape)
dask.array.tests.test_creation.test_auto_chunks()
dask.array.tests.test_creation.test_diag_2d_array_creation(k)
dask.array.tests.test_creation.test_diag_bad_input(k)
dask.array.tests.test_creation.test_diag_extraction(k)
dask.array.tests.test_creation.test_diagonal()
dask.array.tests.test_creation.test_diagonal_zero_chunks()
dask.array.tests.test_creation.test_empty_indices()
dask.array.tests.test_creation.test_eye()
dask.array.tests.test_creation.test_fromfunction(func,dtype,kwargs)
dask.array.tests.test_creation.test_indices()
dask.array.tests.test_creation.test_indices_dimensions_chunks()
dask.array.tests.test_creation.test_indices_wrong_chunks()
dask.array.tests.test_creation.test_linspace(endpoint)
dask.array.tests.test_creation.test_meshgrid(shapes,chunks,indexing,sparse)
dask.array.tests.test_creation.test_meshgrid_inputcoercion()
dask.array.tests.test_creation.test_nan_empty_like(shape_chunks,dtype)
dask.array.tests.test_creation.test_nan_full_like(val,shape_chunks,dtype)
dask.array.tests.test_creation.test_nan_zeros_ones_like(fn,shape_chunks,dtype)
dask.array.tests.test_creation.test_pad(shape,chunks,pad_width,mode,kwargs)
dask.array.tests.test_creation.test_pad_0_width(shape,chunks,pad_width,mode,kwargs)
dask.array.tests.test_creation.test_pad_3d_data(dtype,pad_widths,mode)
dask.array.tests.test_creation.test_pad_constant_values(np_a,pad_value)
dask.array.tests.test_creation.test_pad_udf(kwargs)
dask.array.tests.test_creation.test_repeat()
dask.array.tests.test_creation.test_string_auto_chunk()
dask.array.tests.test_creation.test_tile_basic(reps)
dask.array.tests.test_creation.test_tile_chunks(shape,chunks,reps)
dask.array.tests.test_creation.test_tile_empty_array(shape,chunks,reps)
dask.array.tests.test_creation.test_tile_neg_reps(shape,chunks,reps)
dask.array.tests.test_creation.test_tile_np_kroncompare_examples(shape,reps)
dask.array.tests.test_creation.test_tile_zero_reps(shape,chunks,reps)
dask.array.tests.test_creation.test_tri(N,M,k,dtype,chunks)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/tests/test_atop.py----------------------------------------
A:dask.array.tests.test_atop.result->dask.array.blockwise(lambda x, y: x + y, 'i', d, 'i', y=name, dtype=object)
A:dask.array.tests.test_atop.x->dask.array.ones(10, chunks=(5,))
A:dask.array.tests.test_atop.dsk->optimize_blockwise(w.dask)
A:dask.array.tests.test_atop.a->dask.array.from_array(np.ones(1, np.float64), chunks=(1,))
A:dask.array.tests.test_atop.layer->hlg_layer_topological(dsk, 0)
A:dask.array.tests.test_atop.annotations->collections.Counter((tuple(a.items()) if type(a) is dict else a for a in annotations))
A:dask.array.tests.test_atop.w->dask.array.blockwise(sum, '', z, '', dtype=y.dtype)
A:dask.array.tests.test_atop.z_top_before->tuple(z.dask.layers[z.name].indices)
A:dask.array.tests.test_atop.(zz,)->dask.optimize(z)
A:dask.array.tests.test_atop.z_top_after->tuple(z.dask.layers[z.name].indices)
A:dask.array.tests.test_atop.b->dask.array.from_array(np.zeros(1, np.float64), chunks=(1,))
A:dask.array.tests.test_atop.d->dask.array.from_array(x, chunks=2)
A:dask.array.tests.test_atop.y->dask.array.atop(inc, 'i', x, 'i', dtype=x.dtype)
A:dask.array.tests.test_atop.z->dask.array.blockwise(inc, 'i', x, 'i', dtype=x.dtype)
A:dask.array.tests.test_atop.(x,)->dask.array.from_array(np.zeros(1, np.float64), chunks=(1,)).map_partitions(np.asarray).to_delayed()
A:dask.array.tests.test_atop.(u, s, v)->dask.array.linalg.svd(y)
A:dask.array.tests.test_atop.A->dask.array.random.default_rng().random((20, 20), chunks=(10, 10))
A:dask.array.tests.test_atop.B->dask.array.blockwise(f, ('d1', 'd2'), A, ('d1', 'd2'), x=tup, dtype=A.dtype)
A:dask.array.tests.test_atop.a.dask->dict(a.dask)
dask.array.tests.test_atop.test_args_delayed()
dask.array.tests.test_atop.test_atop_legacy()
dask.array.tests.test_atop.test_bag_array_conversion()
dask.array.tests.test_atop.test_blockwise_chunks()
dask.array.tests.test_atop.test_blockwise_diamond_fusion()
dask.array.tests.test_atop.test_blockwise_kwargs()
dask.array.tests.test_atop.test_blockwise_names()
dask.array.tests.test_atop.test_blockwise_new_axes()
dask.array.tests.test_atop.test_blockwise_new_axes_2()
dask.array.tests.test_atop.test_blockwise_new_axes_chunked()
dask.array.tests.test_atop.test_blockwise_no_args()
dask.array.tests.test_atop.test_blockwise_no_array_args()
dask.array.tests.test_atop.test_blockwise_non_blockwise_output()
dask.array.tests.test_atop.test_blockwise_numpy_arg()
dask.array.tests.test_atop.test_blockwise_stacked_new_axes(concatenate)
dask.array.tests.test_atop.test_blockwise_stacked_new_axes_front(concatenate)
dask.array.tests.test_atop.test_blockwise_stacked_new_axes_same_dim(concatenate)
dask.array.tests.test_atop.test_common_token_names_args(name)
dask.array.tests.test_atop.test_common_token_names_kwargs(name)
dask.array.tests.test_atop.test_dont_merge_before_reductions()
dask.array.tests.test_atop.test_index_subs()
dask.array.tests.test_atop.test_inner_compute()
dask.array.tests.test_atop.test_namedtuple(tup)
dask.array.tests.test_atop.test_non_hlg()
dask.array.tests.test_atop.test_optimize_blockwise()
dask.array.tests.test_atop.test_optimize_blockwise_control_annotations()
dask.array.tests.test_atop.test_optimize_blockwise_custom_annotations()
dask.array.tests.test_atop.test_rewrite(inputs,expected)
dask.array.tests.test_atop.test_svd()
dask.array.tests.test_atop.test_top_len()
dask.array.tests.test_atop.test_validate_top_inputs()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/tests/test_overlap.py----------------------------------------
A:dask.array.tests.test_overlap.x->dask.array.from_array(expected, chunks=5)
A:dask.array.tests.test_overlap.d->dask.array.from_array(x, chunks=(2, 2))
A:dask.array.tests.test_overlap.g->overlap(darr, depth=0, boundary=0)
A:dask.array.tests.test_overlap.result->trim_internal(constant, depth, boundary=42)
A:dask.array.tests.test_overlap.expected->numpy.lib.stride_tricks.sliding_window_view(arr.compute(), window_shape, axis)
A:dask.array.tests.test_overlap.e->boundaries(d, {0: 2, 1: 1}, {0: 0, 1: 'periodic'})
A:dask.array.tests.test_overlap.u_depth->numpy.uint16([2, 1])
A:dask.array.tests.test_overlap.arr->dask.array.zeros((4, 3))
A:dask.array.tests.test_overlap.y->dask.array.from_array(expected, chunks=5).map_overlap(lambda x: x, depth=2, boundary=0)
A:dask.array.tests.test_overlap.z->dask.array.map_overlap(func, x, y, chunks=(3, 3), depth=1, trim=True, boundary='reflect')
A:dask.array.tests.test_overlap.exp1->dask.array.from_array(x, chunks=(2, 2)).map_overlap(lambda x: x + x.size, depth=1, dtype=d.dtype, boundary='reflect')
A:dask.array.tests.test_overlap.exp2->dask.array.from_array(x, chunks=(2, 2)).map_overlap(lambda x: x + x.size, depth={0: 1, 1: 1}, boundary={0: 'reflect', 1: 'none'}, dtype=d.dtype)
A:dask.array.tests.test_overlap.exp3->dask.array.from_array(x, chunks=(2, 2)).map_overlap(lambda x: x + x.size, depth={1: 1}, boundary={1: 'reflect'}, dtype=d.dtype)
A:dask.array.tests.test_overlap.exp4->dask.array.from_array(x, chunks=(2, 2)).map_overlap(lambda x: x + x.size, depth={1: (1, 0)}, boundary={0: 'none', 1: 'none'}, dtype=d.dtype)
A:dask.array.tests.test_overlap.size_per_slice->sum((np.pad(x[:4], 1, mode='constant').size for x in xs))
A:dask.array.tests.test_overlap.depth->tuple((0 if i in _drop_axis else d for (i, d) in enumerate(depth)))
A:dask.array.tests.test_overlap.x1->dask.array.ones((10,), chunks=(5, 5))
A:dask.array.tests.test_overlap.x2->dask.array.ones((10,), chunks=(5, 5)).rechunk(10)
A:dask.array.tests.test_overlap.z1->dask.array.map_overlap(oversum, x1, depth=2, trim=False, boundary='none')
A:dask.array.tests.test_overlap.z2->dask.array.map_overlap(oversum, x2, depth=2, trim=False, boundary='none')
A:dask.array.tests.test_overlap.a->dask.array.from_array(expected, chunks=5).map_overlap(lambda x: x, depth={0: 1}, boundary='none')
A:dask.array.tests.test_overlap.darr->dask.array.from_array(a, chunks=(3, 5))
A:dask.array.tests.test_overlap.garr->overlap(darr, depth={0: 5, 1: 5}, boundary={0: 'nearest', 1: 'nearest'})
A:dask.array.tests.test_overlap.tarr->trim_internal(garr, {0: 5, 1: 5}, boundary='nearest')
A:dask.array.tests.test_overlap.reflected->overlap(darr, depth=depth, boundary='reflect')
A:dask.array.tests.test_overlap.nearest->overlap(darr, depth=depth, boundary='nearest')
A:dask.array.tests.test_overlap.periodic->overlap(darr, depth=depth, boundary='periodic')
A:dask.array.tests.test_overlap.constant->overlap(darr, depth=depth, boundary=42)
A:dask.array.tests.test_overlap.b->dask.array.from_array(expected, chunks=5).map_overlap(lambda x: x, depth={1: 1}, boundary='none')
A:dask.array.tests.test_overlap.output->overlap(darr, depth=depth, boundary=1)
A:dask.array.tests.test_overlap.exp->boundaries(x, 2, {0: 'none', 1: 33})
A:dask.array.tests.test_overlap.res->numpy.array([[33, 33, 0, 1, 2, 3, 33, 33], [33, 33, 4, 5, 6, 7, 33, 33], [33, 33, 8, 9, 10, 11, 33, 33], [33, 33, 12, 13, 14, 15, 33, 33]])
A:dask.array.tests.test_overlap.rng->dask.array.random.default_rng(0)
A:dask.array.tests.test_overlap.c->dask.array.from_array(expected, chunks=5).map_overlap(lambda x: x, depth={0: 1, 1: 1}, boundary='none')
A:dask.array.tests.test_overlap.x_overlaped->dask.array.overlap.overlap(x, 2, boundary=boundary)
A:dask.array.tests.test_overlap.x_trimmed->dask.array.overlap.trim_overlap(x_overlaped, 2, boundary=boundary)
A:dask.array.tests.test_overlap.rand->dask.array.random.default_rng().random((860, 1024, 1024), chunks=(1, 1024, 1024))
A:dask.array.tests.test_overlap.filtered->dask.array.random.default_rng().random((860, 1024, 1024), chunks=(1, 1024, 1024)).map_overlap(lambda arr: arr, depth=(2, 2, 2), boundary='reflect')
A:dask.array.tests.test_overlap.actual->sliding_window_view(arr, window_shape, axis)
dask.array.tests.test_overlap.test_asymmetric_overlap_boundary_exception()
dask.array.tests.test_overlap.test_boundaries()
dask.array.tests.test_overlap.test_constant()
dask.array.tests.test_overlap.test_constant_boundaries()
dask.array.tests.test_overlap.test_depth_greater_than_dim()
dask.array.tests.test_overlap.test_depth_greater_than_smallest_chunk_combines_chunks(chunks)
dask.array.tests.test_overlap.test_different_depths_and_boundary_combinations(depth)
dask.array.tests.test_overlap.test_ensure_minimum_chunksize(chunks,expected)
dask.array.tests.test_overlap.test_ensure_minimum_chunksize_raises_error()
dask.array.tests.test_overlap.test_map_overlap()
dask.array.tests.test_overlap.test_map_overlap_assumes_shape_matches_first_array_if_trim_is_false()
dask.array.tests.test_overlap.test_map_overlap_deprecated_signature()
dask.array.tests.test_overlap.test_map_overlap_escapes_to_map_blocks_when_depth_is_zero()
dask.array.tests.test_overlap.test_map_overlap_multiarray()
dask.array.tests.test_overlap.test_map_overlap_multiarray_block_broadcast()
dask.array.tests.test_overlap.test_map_overlap_multiarray_defaults()
dask.array.tests.test_overlap.test_map_overlap_multiarray_different_depths()
dask.array.tests.test_overlap.test_map_overlap_multiarray_uneven_numblocks_exception()
dask.array.tests.test_overlap.test_map_overlap_multiarray_variadic()
dask.array.tests.test_overlap.test_map_overlap_no_depth(boundary)
dask.array.tests.test_overlap.test_map_overlap_rechunks_array_along_multiple_dims_if_needed()
dask.array.tests.test_overlap.test_map_overlap_rechunks_array_if_needed()
dask.array.tests.test_overlap.test_map_overlap_trim_using_drop_axis_and_different_depths(drop_axis)
dask.array.tests.test_overlap.test_nearest()
dask.array.tests.test_overlap.test_nearest_overlap()
dask.array.tests.test_overlap.test_no_shared_keys_with_different_depths()
dask.array.tests.test_overlap.test_none_boundaries()
dask.array.tests.test_overlap.test_one_chunk_along_axis()
dask.array.tests.test_overlap.test_overlap()
dask.array.tests.test_overlap.test_overlap_allow_rechunk_kwarg()
dask.array.tests.test_overlap.test_overlap_few_dimensions()
dask.array.tests.test_overlap.test_overlap_few_dimensions_small()
dask.array.tests.test_overlap.test_overlap_internal()
dask.array.tests.test_overlap.test_overlap_internal_asymmetric()
dask.array.tests.test_overlap.test_overlap_internal_asymmetric_small()
dask.array.tests.test_overlap.test_overlap_small()
dask.array.tests.test_overlap.test_periodic()
dask.array.tests.test_overlap.test_reflect()
dask.array.tests.test_overlap.test_sliding_window_errors(window_shape,axis)
dask.array.tests.test_overlap.test_sliding_window_view(shape,chunks,window_shape,axis)
dask.array.tests.test_overlap.test_trim_boundary(boundary)
dask.array.tests.test_overlap.test_trim_internal()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/tests/test_reshape.py----------------------------------------
A:dask.array.tests.test_reshape.(result_in, result_out)->reshape_rechunk(inshape, outshape, prechunks)
A:dask.array.tests.test_reshape.a->dask.array.from_array(base, chunks=inchunks)
A:dask.array.tests.test_reshape.A->dask.array.from_array(a, chunks=(5, 2, 3))
A:dask.array.tests.test_reshape.a2->dask.array.from_array(base, chunks=inchunks).reshape((60, -1))
A:dask.array.tests.test_reshape.A2->dask.array.from_array(a, chunks=(5, 2, 3)).reshape((60, -1))
A:dask.array.tests.test_reshape.base->numpy.arange(np.prod(inshape)).reshape(inshape)
A:dask.array.tests.test_reshape.(inchunks2, outchunks2)->reshape_rechunk(a.shape, outshape, inchunks)
A:dask.array.tests.test_reshape.result->dask.array.from_array(base, chunks=inchunks).reshape(outshape, merge_chunks=False)
dask.array.tests.test_reshape.test_contract_tuple()
dask.array.tests.test_reshape.test_expand_tuple()
dask.array.tests.test_reshape.test_reshape_all_chunked_no_merge(inshape,inchunks,outshape,outchunks)
dask.array.tests.test_reshape.test_reshape_all_not_chunked_merge(inshape,inchunks,expected_inchunks,outshape,outchunks)
dask.array.tests.test_reshape.test_reshape_merge_chunks(inshape,inchunks,outshape,outchunks)
dask.array.tests.test_reshape.test_reshape_rechunk(inshape,outshape,prechunks,inchunks,outchunks)
dask.array.tests.test_reshape.test_reshape_unknown_sizes()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/tests/test_array_function.py----------------------------------------
A:dask.array.tests.test_array_function.x->dask.array.from_array(np.array([1, 2, 4, 3]), chunks=(2,))
A:dask.array.tests.test_array_function.y->dask.array.from_array(x, chunks=(50, 50))
A:dask.array.tests.test_array_function.res_x->func(x)
A:dask.array.tests.test_array_function.res_y->func(y)
A:dask.array.tests.test_array_function.sparse->pytest.importorskip('sparse')
A:dask.array.tests.test_array_function.rng->numpy.random.default_rng()
A:dask.array.tests.test_array_function.xx->func(x)
A:dask.array.tests.test_array_function.yy->func(y)
A:dask.array.tests.test_array_function.cupy->pytest.importorskip('cupy')
A:dask.array.tests.test_array_function.(u_base, s_base, v_base)->dask.array.linalg.svd(y)
A:dask.array.tests.test_array_function.(u, s, v)->numpy.linalg.svd(y)
dask.array.tests.test_array_function.test_array_function_cupy_svd(chunks)
dask.array.tests.test_array_function.test_array_function_dask(func)
dask.array.tests.test_array_function.test_array_function_fft(func)
dask.array.tests.test_array_function.test_array_function_sparse(func)
dask.array.tests.test_array_function.test_array_function_sparse_tensordot()
dask.array.tests.test_array_function.test_array_notimpl_function_dask(func)
dask.array.tests.test_array_function.test_binary_function_type_precedence(func,arr_upcast,arr_downcast)
dask.array.tests.test_array_function.test_like_raises(func)
dask.array.tests.test_array_function.test_like_with_numpy_func(func)
dask.array.tests.test_array_function.test_like_with_numpy_func_and_dtype(func)
dask.array.tests.test_array_function.test_non_existent_func()
dask.array.tests.test_array_function.test_stack_functions_require_sequence_of_arrays(func)
dask.array.tests.test_array_function.test_unregistered_func(func)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/tests/test_sparse.py----------------------------------------
A:dask.array.tests.test_sparse.sparse->pytest.importorskip('sparse')
A:dask.array.tests.test_sparse.SPARSE_VERSION->parse_version(sparse.__version__)
A:dask.array.tests.test_sparse.x->numpy.random.random((2, 3, 4))
A:dask.array.tests.test_sparse.y->dask.array.utils.meta_from_array(x, ndim=2)
A:dask.array.tests.test_sparse.xx->numpy.random.random((2, 3, 4)).map_blocks(sparse.COO.from_numpy)
A:dask.array.tests.test_sparse.yy->dask.array.utils.meta_from_array(x, ndim=2).map_blocks(sparse.COO.from_numpy)
A:dask.array.tests.test_sparse.zz->dask.array.utils.meta_from_array(x, ndim=2).map_blocks(sparse.COO.from_numpy).compute()
A:dask.array.tests.test_sparse.z->pytest.importorskip('sparse').COO.from_numpy(y.compute())
A:dask.array.tests.test_sparse.text->dask.array.utils.meta_from_array(x, ndim=2)._repr_html_()
A:dask.array.tests.test_sparse.d->dask.array.from_array(x, chunks=(5, 5))
A:dask.array.tests.test_sparse.xs->pytest.importorskip('sparse').COO.from_numpy(x, fill_value=0.0)
dask.array.tests.test_sparse.test_basic(func)
dask.array.tests.test_sparse.test_from_array()
dask.array.tests.test_sparse.test_from_delayed_meta()
dask.array.tests.test_sparse.test_html_repr()
dask.array.tests.test_sparse.test_map_blocks()
dask.array.tests.test_sparse.test_meta_from_array()
dask.array.tests.test_sparse.test_metadata()
dask.array.tests.test_sparse.test_numel(numel,axis,keepdims)
dask.array.tests.test_sparse.test_tensordot()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/tests/test_gufunc.py----------------------------------------
A:dask.array.tests.test_gufunc.a->dask.array.random.default_rng().normal(size=(10, 20, 30), chunks=(5, 5, 30))
A:dask.array.tests.test_gufunc.b->dask.array.random.default_rng().normal(size=(20, 30), chunks=30)
A:dask.array.tests.test_gufunc.(i, o)->_validate_normalize_axes(None, 0, True, [('i',)], ())
A:dask.array.tests.test_gufunc.result->array_and_max(stack)
A:dask.array.tests.test_gufunc.(mean, std)->apply_gufunc(stats, '(i)->(),()', a, output_dtypes=2 * (a.dtype,), allow_rechunk=True)
A:dask.array.tests.test_gufunc.mean->apply_gufunc(stats, '(i)->()', a, output_dtypes='f', vectorize=vectorize)
A:dask.array.tests.test_gufunc.(mean, std, min)->apply_gufunc(stats, '(i)->(),(),()', a, output_dtypes=('f', 'f', 'f'), vectorize=vectorize)
A:dask.array.tests.test_gufunc.ret->apply_gufunc(foo, '()->()', 1.0, output_dtypes=float, bar=2)
A:dask.array.tests.test_gufunc.rng->dask.array.random.default_rng()
A:dask.array.tests.test_gufunc.c->apply_gufunc(outer_product, '(i),(j)->(i,j)', a, b, vectorize=True)
A:dask.array.tests.test_gufunc.x->dask.array.random.default_rng().normal(size=(10, 20, 30), chunks=(5, 5, 30)).sum(axis=0, keepdims=True)
A:dask.array.tests.test_gufunc.z->foo(dx, dy, 1, cast='f8')
A:dask.array.tests.test_gufunc.(z1, z2)->apply_gufunc(addmul, '(),()->(),()', a, b, output_dtypes=2 * (a.dtype,))
A:dask.array.tests.test_gufunc.(x, y)->numpy.broadcast_arrays(x, y)
A:dask.array.tests.test_gufunc.y->mysum(a, axis=0, keepdims=True)
A:dask.array.tests.test_gufunc.dy->dask.array.from_array(y, chunks=5)
A:dask.array.tests.test_gufunc.gufoo->gufunc(foo, signature='(i)->()', axis=-1, keepdims=False, output_dtypes=float, vectorize=True)
A:dask.array.tests.test_gufunc.valy->mysum(a, axis=0, keepdims=True).compute()
A:dask.array.tests.test_gufunc.(x, y, z)->apply_gufunc(foo, '(i),(i)->(i),(i),(i)', a, b, output_dtypes=3 * (float,), vectorize=False)
A:dask.array.tests.test_gufunc.dx->dask.array.from_array(x, chunks=5)
A:dask.array.tests.test_gufunc.cast->kwargs.pop('cast', 'i8')
A:dask.array.tests.test_gufunc.dz->apply_gufunc(foo, '(),(),()->()', dx, dy, 1, cast='f8', output_dtypes='f8')
A:dask.array.tests.test_gufunc.msg->str(e.value)
A:dask.array.tests.test_gufunc.(z0, z1)->apply_gufunc(foo, '(),()->(),()', dx, dy)
A:dask.array.tests.test_gufunc.da_->dask.array.from_array(a, chunks=2)
A:dask.array.tests.test_gufunc.m->numpy.einsum('jiu,juk->uik', a, b)
A:dask.array.tests.test_gufunc.dm->apply_gufunc(matmul, '(i,j),(j,k)->(i,k)', da_, db, axes=[(1, 0), (0, -1), (-2, -1)], allow_rechunk=True)
A:dask.array.tests.test_gufunc.nx->numpy.fft.ifft(y, axis=axis)
A:dask.array.tests.test_gufunc.db_->dask.array.from_array(b, chunks=2)
A:dask.array.tests.test_gufunc.db->dask.array.from_array(b, chunks=3)
A:dask.array.tests.test_gufunc.numba->pytest.importorskip('numba')
A:dask.array.tests.test_gufunc.sparse->pytest.importorskip('sparse')
A:dask.array.tests.test_gufunc.(sum, mean)->apply_gufunc(stats, '(i)->(),()', a, output_dtypes=2 * (a.dtype,))
A:dask.array.tests.test_gufunc.expected->stats(a.compute())
A:dask.array.tests.test_gufunc.stack->dask.array.ones((1, 50, 60), chunks=(1, -1, -1))
dask.array.tests.test_gufunc.test__parse_gufunc_signature()
dask.array.tests.test_gufunc.test__validate_normalize_axes_01()
dask.array.tests.test_gufunc.test__validate_normalize_axes_02()
dask.array.tests.test_gufunc.test__validate_normalize_axes_03()
dask.array.tests.test_gufunc.test_apply_gufunc_01()
dask.array.tests.test_gufunc.test_apply_gufunc_01b()
dask.array.tests.test_gufunc.test_apply_gufunc_02()
dask.array.tests.test_gufunc.test_apply_gufunc_axes_01(axes)
dask.array.tests.test_gufunc.test_apply_gufunc_axes_02()
dask.array.tests.test_gufunc.test_apply_gufunc_axes_args_validation()
dask.array.tests.test_gufunc.test_apply_gufunc_axes_input_validation_01()
dask.array.tests.test_gufunc.test_apply_gufunc_axes_two_kept_coredims()
dask.array.tests.test_gufunc.test_apply_gufunc_axis_01(keepdims)
dask.array.tests.test_gufunc.test_apply_gufunc_axis_02()
dask.array.tests.test_gufunc.test_apply_gufunc_axis_02b()
dask.array.tests.test_gufunc.test_apply_gufunc_axis_03()
dask.array.tests.test_gufunc.test_apply_gufunc_axis_keepdims(axis)
dask.array.tests.test_gufunc.test_apply_gufunc_broadcasting_loopdims()
dask.array.tests.test_gufunc.test_apply_gufunc_check_coredim_chunksize()
dask.array.tests.test_gufunc.test_apply_gufunc_check_inhomogeneous_chunksize()
dask.array.tests.test_gufunc.test_apply_gufunc_check_same_dimsizes()
dask.array.tests.test_gufunc.test_apply_gufunc_elemwise_01()
dask.array.tests.test_gufunc.test_apply_gufunc_elemwise_01b()
dask.array.tests.test_gufunc.test_apply_gufunc_elemwise_02()
dask.array.tests.test_gufunc.test_apply_gufunc_elemwise_core()
dask.array.tests.test_gufunc.test_apply_gufunc_elemwise_loop()
dask.array.tests.test_gufunc.test_apply_gufunc_infer_dtype()
dask.array.tests.test_gufunc.test_apply_gufunc_output_dtypes(output_dtypes)
dask.array.tests.test_gufunc.test_apply_gufunc_output_dtypes_string(vectorize)
dask.array.tests.test_gufunc.test_apply_gufunc_output_dtypes_string_many_outputs(vectorize)
dask.array.tests.test_gufunc.test_apply_gufunc_pass_additional_kwargs()
dask.array.tests.test_gufunc.test_apply_gufunc_scalar_output()
dask.array.tests.test_gufunc.test_apply_gufunc_two_mixed_outputs()
dask.array.tests.test_gufunc.test_apply_gufunc_two_scalar_output()
dask.array.tests.test_gufunc.test_apply_gufunc_via_numba_01()
dask.array.tests.test_gufunc.test_apply_gufunc_via_numba_02()
dask.array.tests.test_gufunc.test_apply_gufunc_with_meta()
dask.array.tests.test_gufunc.test_as_gufunc()
dask.array.tests.test_gufunc.test_as_gufunc_with_meta()
dask.array.tests.test_gufunc.test_gufunc()
dask.array.tests.test_gufunc.test_gufunc_mixed_inputs()
dask.array.tests.test_gufunc.test_gufunc_mixed_inputs_vectorize()
dask.array.tests.test_gufunc.test_gufunc_two_inputs()
dask.array.tests.test_gufunc.test_gufunc_vector_output()
dask.array.tests.test_gufunc.test_gufunc_vectorize_whitespace()
dask.array.tests.test_gufunc.test_preserve_meta_type()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/tests/test_cupy_gufunc.py----------------------------------------
A:dask.array.tests.test_cupy_gufunc.cupy->pytest.importorskip('cupy')
A:dask.array.tests.test_cupy_gufunc.a->pytest.importorskip('cupy').random.default_rng().standard_normal((3, 6, 4))
A:dask.array.tests.test_cupy_gufunc.da_->dask.array.from_array(a, chunks=2, asarray=False)
A:dask.array.tests.test_cupy_gufunc.m->numpy.diff(a, axis=1)
A:dask.array.tests.test_cupy_gufunc.dm->apply_gufunc(mydiff, '(i)->(i)', da_, axis=1, output_sizes={'i': 5}, allow_rechunk=True)
dask.array.tests.test_cupy_gufunc.test_apply_gufunc_axis()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/tests/test_stats.py----------------------------------------
A:dask.array.tests.test_stats.scipy->pytest.importorskip('scipy')
A:dask.array.tests.test_stats.x->numpy.random.random(size=(30, 2))
A:dask.array.tests.test_stats.y->dask.array.from_array(x, 3)
A:dask.array.tests.test_stats.dfunc->getattr(dask.array.stats, kind)
A:dask.array.tests.test_stats.sfunc->getattr(scipy.stats, kind)
A:dask.array.tests.test_stats.expected->pytest.importorskip('scipy').stats.f_oneway(*np_args)
A:dask.array.tests.test_stats.result->dask.array.stats.kurtosis(dask_array).compute()
A:dask.array.tests.test_stats.a->dask.array.ones((7,), chunks=(7,))
A:dask.array.tests.test_stats.a_->dask.array.from_array(a, 3)
A:dask.array.tests.test_stats.dask_test->getattr(dask.array.stats, kind)
A:dask.array.tests.test_stats.scipy_test->getattr(scipy.stats, kind)
A:dask.array.tests.test_stats.b_->dask.array.from_array(b, 3)
A:dask.array.tests.test_stats.numpy_array->numpy.random.random(size=(30,))
A:dask.array.tests.test_stats.dask_array->dask.array.from_array(numpy_array, 3)
A:dask.array.tests.test_stats.result_non_fisher->dask.array.stats.kurtosis(dask_array, fisher=False).compute()
dask.array.tests.test_stats.test_anova()
dask.array.tests.test_stats.test_bias_raises()
dask.array.tests.test_stats.test_kurtosis_single_return_type()
dask.array.tests.test_stats.test_measures(kind,kwargs,single_dim)
dask.array.tests.test_stats.test_moments(k)
dask.array.tests.test_stats.test_nan_raises(func,nargs,nan_policy)
dask.array.tests.test_stats.test_one(kind)
dask.array.tests.test_stats.test_power_divergence_invalid()
dask.array.tests.test_stats.test_skew_raises()
dask.array.tests.test_stats.test_skew_single_return_type()
dask.array.tests.test_stats.test_two(kind,kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/tests/test_fft.py----------------------------------------
A:dask.array.tests.test_fft.nparr->numpy.arange(100).reshape(10, 10)
A:dask.array.tests.test_fft.darr->dask.array.from_array(nparr, chunks=(1, 10))
A:dask.array.tests.test_fft.darr2->dask.array.from_array(nparr, chunks=(10, 1))
A:dask.array.tests.test_fft.darr3->dask.array.from_array(nparr, chunks=(10, 10))
A:dask.array.tests.test_fft.da_fft->getattr(da.fft, funcname)
A:dask.array.tests.test_fft.bad_darr->dask.array.from_array(nparr, chunks=(5, 5))
A:dask.array.tests.test_fft.np_fft->getattr(np.fft, funcname)
A:dask.array.tests.test_fft.a->numpy.arange(np.prod(shape)).reshape(shape)
A:dask.array.tests.test_fft.d->dask.array.from_array(a, chunks=chunks)
A:dask.array.tests.test_fft.cs->list(chunk_size)
A:dask.array.tests.test_fft.d2->dask.array.from_array(a, chunks=chunks).rechunk(cs)
A:dask.array.tests.test_fft.r->da_fft(d2, axes=axes)
A:dask.array.tests.test_fft.er->np_fft(a, axes=axes)
A:dask.array.tests.test_fft.fft_mod->pytest.importorskip(modname)
A:dask.array.tests.test_fft.func->getattr(fft_mod, funcname)
A:dask.array.tests.test_fft.darrc->dask.array.from_array(nparr, chunks=(1, 10)).astype(dtype).rechunk(darr.shape)
A:dask.array.tests.test_fft.darr2c->dask.array.from_array(nparr, chunks=(10, 1)).astype(dtype).rechunk(darr2.shape)
A:dask.array.tests.test_fft.nparrc->numpy.arange(100).reshape(10, 10).astype(dtype)
A:dask.array.tests.test_fft.wfunc->fft_wrap(func)
A:dask.array.tests.test_fft.c->c(n)
A:dask.array.tests.test_fft.r1->numpy.fft.rfftfreq(n, d)
A:dask.array.tests.test_fft.r2->dask.array.fft.rfftfreq(n, d, chunks=c)
A:dask.array.tests.test_fft.np_func->getattr(np.fft, funcname)
A:dask.array.tests.test_fft.da_func->getattr(da.fft, funcname)
A:dask.array.tests.test_fft.a_r->np_func(a, axes)
A:dask.array.tests.test_fft.d_r->da_func1(da_func2(d, axes), axes)
A:dask.array.tests.test_fft.da_func1->getattr(da.fft, funcname1)
A:dask.array.tests.test_fft.da_func2->getattr(da.fft, funcname2)
dask.array.tests.test_fft.test_cant_fft_chunked_axis(funcname)
dask.array.tests.test_fft.test_fft(funcname)
dask.array.tests.test_fft.test_fft2n_shapes(funcname)
dask.array.tests.test_fft.test_fft_consistent_names(funcname)
dask.array.tests.test_fft.test_fft_n_kwarg(funcname)
dask.array.tests.test_fft.test_fftfreq(n,d,c)
dask.array.tests.test_fft.test_fftshift(funcname,shape,chunks,axes)
dask.array.tests.test_fft.test_fftshift_identity(funcname1,funcname2,shape,chunks,axes)
dask.array.tests.test_fft.test_nd_ffts_axes(funcname,dtype)
dask.array.tests.test_fft.test_rfftfreq(n,d,c)
dask.array.tests.test_fft.test_wrap_bad_kind()
dask.array.tests.test_fft.test_wrap_fftns(modname,funcname,dtype)
dask.array.tests.test_fft.test_wrap_ffts(modname,funcname,dtype)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/tests/test_masked.py----------------------------------------
A:dask.array.tests.test_masked.m->numpy.ma.masked_array([1, 2, 3], mask=[True, True, False], fill_value=10)
A:dask.array.tests.test_masked.m2->numpy.ma.masked_array([1, 2, 3], mask=[True, True, False], fill_value=0)
A:dask.array.tests.test_masked.m3->numpy.ma.masked_array([1, 2, 3], mask=False, fill_value=10)
A:dask.array.tests.test_masked.dm->dask.array.from_array(m, chunks=(2,), asarray=False)
A:dask.array.tests.test_masked.t->numpy.ma.masked_array([1, 2], mask=[0, 1])
A:dask.array.tests.test_masked.x->numpy.ma.array(x, mask=mask)
A:dask.array.tests.test_masked.y->numpy.random.default_rng().integers(10, size=15).astype(np.uint8)
A:dask.array.tests.test_masked.y2->deepcopy(x, memo=memo)
A:dask.array.tests.test_masked.xx->dask.array.ma.masked_equal(x, 0)
A:dask.array.tests.test_masked.yy->dask.array.ma.masked_equal(y, 0)
A:dask.array.tests.test_masked.zz->dask.array.concatenate([x, y], axis=1).compute()
A:dask.array.tests.test_masked.rng->numpy.random.default_rng()
A:dask.array.tests.test_masked.d->dask.array.ma.masked_array(x, mask=mask, chunks=(4, 5))
A:dask.array.tests.test_masked.s->dask.array.ma.masked_array(x, mask=mask, chunks=(4, 5)).map_blocks(fn)
A:dask.array.tests.test_masked.dd->func(d)
A:dask.array.tests.test_masked.ss->func(s)
A:dask.array.tests.test_masked.z->dask.array.concatenate([x, y], axis=1)
A:dask.array.tests.test_masked.dx->dask.array.from_array(data, chunks=(2, 3))
A:dask.array.tests.test_masked.dy->dask.array.from_array(y, chunks=5)
A:dask.array.tests.test_masked.sol->numpy.ma.where(x)
A:dask.array.tests.test_masked.my->numpy.ma.masked_greater(y, 0)
A:dask.array.tests.test_masked.dmy->dask.array.ma.masked_greater(dy, 0)
A:dask.array.tests.test_masked.mx->numpy.ma.masked_greater(x, 3)
A:dask.array.tests.test_masked.mdx->dask.array.ma.masked_greater(dx, 5)
A:dask.array.tests.test_masked.res->dask.array.ma.where(d)
A:dask.array.tests.test_masked.a->numpy.ma.array(data, mask=mask)
A:dask.array.tests.test_masked.b->numpy.ma.filled(b)
A:dask.array.tests.test_masked.dfunc->getattr(da, reduction)
A:dask.array.tests.test_masked.func->getattr(np, reduction)
A:dask.array.tests.test_masked.dmx->dask.array.ma.masked_greater(dx, 3)
A:dask.array.tests.test_masked.f1->dask.array.from_array(np.array(1), chunks=())
A:dask.array.tests.test_masked.mask->numpy.random.default_rng().choice(a=[False, True], size=(15, 14), p=[0.5, 0.5])
A:dask.array.tests.test_masked.data->numpy.arange(9).reshape((3, 3))
A:dask.array.tests.test_masked.d_a->dask.array.ma.masked_array(data=data, mask=mask, chunks=2)
A:dask.array.tests.test_masked.weights->numpy.array([0.25, 0.75])
A:dask.array.tests.test_masked.d_weights->dask.array.from_array(weights, chunks=2)
A:dask.array.tests.test_masked.da_avg->dask.array.ma.average(d_a, weights=d_weights, axis=1, keepdims=keepdims)
A:dask.array.tests.test_masked.masked->numpy.ma.array(x, mask=mask)
A:dask.array.tests.test_masked.da_func->getattr(da.ma, funcname)
A:dask.array.tests.test_masked.np_func->getattr(np.ma.core, funcname)
A:dask.array.tests.test_masked.e->dask.array.from_array(y, chunks=(4,))
A:dask.array.tests.test_masked.w1->dask.array.ma.where(c1, d, b1)
A:dask.array.tests.test_masked.w2->numpy.ma.where(c2, x, b2)
dask.array.tests.test_masked.assert_eq_ma(a,b)
dask.array.tests.test_masked.test_accessors()
dask.array.tests.test_masked.test_arg_reductions(reduction)
dask.array.tests.test_masked.test_arithmetic_results_in_masked()
dask.array.tests.test_masked.test_average_weights_with_masked_array(keepdims)
dask.array.tests.test_masked.test_basic(func)
dask.array.tests.test_masked.test_copy_deepcopy()
dask.array.tests.test_masked.test_count()
dask.array.tests.test_masked.test_creation_functions()
dask.array.tests.test_masked.test_cumulative()
dask.array.tests.test_masked.test_filled()
dask.array.tests.test_masked.test_from_array_masked_array()
dask.array.tests.test_masked.test_like_funcs(funcname)
dask.array.tests.test_masked.test_masked_array()
dask.array.tests.test_masked.test_mixed_concatenate(func)
dask.array.tests.test_masked.test_mixed_output_type()
dask.array.tests.test_masked.test_mixed_random(func)
dask.array.tests.test_masked.test_nonzero()
dask.array.tests.test_masked.test_reductions(dtype,reduction)
dask.array.tests.test_masked.test_reductions_allmasked(dtype,reduction)
dask.array.tests.test_masked.test_set_fill_value()
dask.array.tests.test_masked.test_tensordot()
dask.array.tests.test_masked.test_tokenize_masked_array()
dask.array.tests.test_masked.test_where()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/tests/test_testing.py----------------------------------------
dask.array.tests.test_testing.test_assert_eq_checks_scalars()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/tests/test_image.py----------------------------------------
A:dask.array.tests.test_image.fn->os.path.join(dirname, 'image.%d.png' % i)
A:dask.array.tests.test_image.x->numpy.random.randint(0, 255, size=shape).astype('u1')
A:dask.array.tests.test_image.im->da_imread(globstring, preprocess=preprocess)
dask.array.tests.test_image.random_images(n,shape)
dask.array.tests.test_image.test_imread()
dask.array.tests.test_image.test_imread_with_custom_function()
dask.array.tests.test_image.test_preprocess()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/tests/test_chunk.py----------------------------------------
A:dask.array.tests.test_chunk.summer_wrapped->keepdims_wrapper(summer)
A:dask.array.tests.test_chunk.a->numpy.arange(24).reshape(1, 2, 3, 4)
A:dask.array.tests.test_chunk.r->summer(a, axis=(1, 3))
A:dask.array.tests.test_chunk.rw->summer_wrapped(a, axis=(1, 3), keepdims=True)
A:dask.array.tests.test_chunk.rwf->summer_wrapped(a, axis=(1, 3), keepdims=False)
A:dask.array.tests.test_chunk.x->numpy.random.rand(1000000)
A:dask.array.tests.test_chunk.y->getitem(x, slice(120, 122))
A:dask.array.tests.test_chunk.res->dask.array.coarsen(np.mean, x, {0: 3}, trim_excess=True)
A:dask.array.tests.test_chunk.y_op->operator.getitem(x, slice(120, 122))
dask.array.tests.test_chunk.test_coarsen()
dask.array.tests.test_chunk.test_coarsen_unaligned_shape()
dask.array.tests.test_chunk.test_getitem()
dask.array.tests.test_chunk.test_integer_input()
dask.array.tests.test_chunk.test_keepdims_wrapper_no_axis()
dask.array.tests.test_chunk.test_keepdims_wrapper_one_axis()
dask.array.tests.test_chunk.test_keepdims_wrapper_two_axes()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/tests/test_cupy_routines.py----------------------------------------
A:dask.array.tests.test_cupy_routines.cupy->pytest.importorskip('cupy')
A:dask.array.tests.test_cupy_routines.cupy_version->parse_version(cupy.__version__)
A:dask.array.tests.test_cupy_routines.x->pytest.importorskip('cupy').random.default_rng().random(size=(100, 100))
A:dask.array.tests.test_cupy_routines.d->dask.array.from_array(a, chunks=chunks)
A:dask.array.tests.test_cupy_routines.e->dask.array.bincount(d, minlength=6)
A:dask.array.tests.test_cupy_routines.carr->pytest.importorskip('cupy').random.default_rng().integers(0, 3, size=(10, 10))
A:dask.array.tests.test_cupy_routines.darr->dask.array.from_array(carr, chunks=(20, 5))
A:dask.array.tests.test_cupy_routines.c->pytest.importorskip('cupy').asarray([True])
A:dask.array.tests.test_cupy_routines.res->dask.array.compress(c, darr, axis=0)
A:dask.array.tests.test_cupy_routines.a->cupy.random.default_rng(seed).integers(low, high, size=shape)
A:dask.array.tests.test_cupy_routines.bins->bins_type.random.default_rng().random(size=13)
A:dask.array.tests.test_cupy_routines.bins_cupy->pytest.importorskip('cupy').array(bins)
A:dask.array.tests.test_cupy_routines.A->pytest.importorskip('cupy').random.default_rng().integers(0, 11, (30, 35))
A:dask.array.tests.test_cupy_routines.dA->dask.array.from_array(A, chunks=(5, 5), asarray=False)
A:dask.array.tests.test_cupy_routines.kwargs->dict(return_index=return_index, return_inverse=return_inverse, return_counts=return_counts)
A:dask.array.tests.test_cupy_routines.r_a->numpy.unique(a)
A:dask.array.tests.test_cupy_routines.r_d->dask.array.unique(d)
A:dask.array.tests.test_cupy_routines.rng->pytest.importorskip('cupy').random.default_rng(seed)
dask.array.tests.test_cupy_routines.test_bincount()
dask.array.tests.test_cupy_routines.test_compress()
dask.array.tests.test_cupy_routines.test_diff(shape,n,axis)
dask.array.tests.test_cupy_routines.test_diff_append(n)
dask.array.tests.test_cupy_routines.test_diff_prepend(n)
dask.array.tests.test_cupy_routines.test_digitize(bins_type)
dask.array.tests.test_cupy_routines.test_tril_triu()
dask.array.tests.test_cupy_routines.test_tril_triu_non_square_arrays()
dask.array.tests.test_cupy_routines.test_unique_kwargs(return_index,return_inverse,return_counts)
dask.array.tests.test_cupy_routines.test_unique_rand(seed,low,high,shape,chunks)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/tests/test_cupy_linalg.py----------------------------------------
A:dask.array.tests.test_cupy_linalg.cupy->pytest.importorskip('cupy')
A:dask.array.tests.test_cupy_linalg.cupy_version->parse_version(cupy.__version__)
A:dask.array.tests.test_cupy_linalg.mat->numpy.random.default_rng().random((m, n))
A:dask.array.tests.test_cupy_linalg.data->dask.array.from_array(mat, chunks=chunks, name='A')
A:dask.array.tests.test_cupy_linalg.n_q->min(m, n)
A:dask.array.tests.test_cupy_linalg.n_u->min(m, n)
A:dask.array.tests.test_cupy_linalg.d_vh->max(m_vh, n_vh)
A:dask.array.tests.test_cupy_linalg.(q, r)->dask.array.linalg.sfqr(data)
A:dask.array.tests.test_cupy_linalg.(u, s, vh)->dask.array.linalg.tsqr(data, compute_svd=True)
A:dask.array.tests.test_cupy_linalg.c0->dask.array.from_array(_c0, chunks=m_min, name='c', asarray=False)
A:dask.array.tests.test_cupy_linalg.r0->dask.array.from_array(_r0, chunks=n_max, name='r', asarray=False)
A:dask.array.tests.test_cupy_linalg.q->q.compute().compute()
A:dask.array.tests.test_cupy_linalg.r->r.compute().compute()
A:dask.array.tests.test_cupy_linalg.u->u.compute().compute()
A:dask.array.tests.test_cupy_linalg.s->s.compute().compute()
A:dask.array.tests.test_cupy_linalg.vh->vh.compute().compute()
A:dask.array.tests.test_cupy_linalg.rng->pytest.importorskip('cupy').random.default_rng(1)
A:dask.array.tests.test_cupy_linalg.A->_get_symmat(shape)
A:dask.array.tests.test_cupy_linalg.b->pytest.importorskip('cupy').random.default_rng(1).integers(1, 20, nrow)
A:dask.array.tests.test_cupy_linalg.dA->dask.array.from_array(A, (chunk, chunk))
A:dask.array.tests.test_cupy_linalg.db->dask.array.from_array(b, chunk)
A:dask.array.tests.test_cupy_linalg.(x, r, rank, s)->pytest.importorskip('cupy').linalg.lstsq(A, b2D, rcond=-1)
A:dask.array.tests.test_cupy_linalg.(dx, dr, drank, ds)->dask.array.linalg.lstsq(dA, db2D)
A:dask.array.tests.test_cupy_linalg.b2D->pytest.importorskip('cupy').random.default_rng(1).integers(1, 20, (nrow, ncol // 2))
A:dask.array.tests.test_cupy_linalg.db2D->dask.array.from_array(b2D, (chunk, ncol // 2))
A:dask.array.tests.test_cupy_linalg.lA->pytest.importorskip('cupy').tril(A)
A:dask.array.tests.test_cupy_linalg.scipy_linalg->pytest.importorskip('scipy.linalg')
dask.array.tests.test_cupy_linalg._get_symmat(size)
dask.array.tests.test_cupy_linalg.test_cholesky(shape,chunk)
dask.array.tests.test_cupy_linalg.test_lstsq(nrow,ncol,chunk,iscomplex)
dask.array.tests.test_cupy_linalg.test_sfqr(m,n,chunks,error_type)
dask.array.tests.test_cupy_linalg.test_tsqr(m,n,chunks,error_type)
dask.array.tests.test_cupy_linalg.test_tsqr_uncertain(m_min,n_max,chunks,vary_rows,vary_cols,error_type)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/tests/test_wrap.py----------------------------------------
A:dask.array.tests.test_wrap.a->dask.array.full(shape=(3, 3), fill_value=100, dtype=None)
A:dask.array.tests.test_wrap.x->numpy.full((3, 3), 1, dtype='i8')
dask.array.tests.test_wrap.test_can_make_really_big_array_of_ones()
dask.array.tests.test_wrap.test_full()
dask.array.tests.test_wrap.test_full_detects_da_dtype()
dask.array.tests.test_wrap.test_full_error_nonscalar_fill_value()
dask.array.tests.test_wrap.test_full_like_error_nonscalar_fill_value()
dask.array.tests.test_wrap.test_full_none_dtype()
dask.array.tests.test_wrap.test_kwargs()
dask.array.tests.test_wrap.test_ones()
dask.array.tests.test_wrap.test_singleton_size()
dask.array.tests.test_wrap.test_size_as_list()
dask.array.tests.test_wrap.test_wrap_consistent_names()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/tests/test_cupy_reductions.py----------------------------------------
A:dask.array.tests.test_cupy_reductions.cupy->pytest.importorskip('cupy')
A:dask.array.tests.test_cupy_reductions.x->pytest.importorskip('cupy').random.default_rng().random((10, 10, 10))
A:dask.array.tests.test_cupy_reductions.a->pytest.importorskip('cupy').ones((10, 10))
A:dask.array.tests.test_cupy_reductions.x2->pytest.importorskip('cupy').arange(10)
A:dask.array.tests.test_cupy_reductions.a2->dask.array.from_array(x2, chunks=3)
A:dask.array.tests.test_cupy_reductions.b->dask.array.from_array(a, chunks=(4, 4))
A:dask.array.tests.test_cupy_reductions.result->func(b, axis=0)
dask.array.tests.test_cupy_reductions.test_arg_reductions(dfunc,func)
dask.array.tests.test_cupy_reductions.test_cumreduction_with_cupy(func)
dask.array.tests.test_cupy_reductions.test_nanarg_reductions(dfunc,func)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/tests/test_cupy_percentile.py----------------------------------------
A:dask.array.tests.test_cupy_percentile.cupy->pytest.importorskip('cupy')
A:dask.array.tests.test_cupy_percentile.d->dask.array.from_array(cupy.ones((16,)), chunks=(4,))
A:dask.array.tests.test_cupy_percentile.qs->numpy.array([0, 50, 100])
A:dask.array.tests.test_cupy_percentile.result->dask.array.percentile(x, 50, method='midpoint').compute()
A:dask.array.tests.test_cupy_percentile.x->dask.array.random.default_rng(cupy.random.default_rng()).random(1000, chunks=(100,))
A:dask.array.tests.test_cupy_percentile.rng->dask.array.random.default_rng(cupy.random.default_rng())
A:dask.array.tests.test_cupy_percentile.(a, b)->dask.array.percentile(x, [40, 60], method='midpoint').compute()
dask.array.tests.test_cupy_percentile.test_percentile()
dask.array.tests.test_cupy_percentile.test_percentile_tokenize()
dask.array.tests.test_cupy_percentile.test_percentiles_with_empty_arrays()
dask.array.tests.test_cupy_percentile.test_percentiles_with_empty_q()
dask.array.tests.test_cupy_percentile.test_percentiles_with_scaler_percentile(q)
dask.array.tests.test_cupy_percentile.test_percentiles_with_unknown_chunk_sizes()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/tests/test_reductions.py----------------------------------------
A:dask.array.tests.test_reductions.np->pytest.importorskip('numpy')
A:dask.array.tests.test_reductions.x->dask.array.reduction(dx, w_sum, np.sum, dtype='f8', weights=w)
A:dask.array.tests.test_reductions.y->dask.array.ones((10, 10), chunks=(4, 4))
A:dask.array.tests.test_reductions.n->pytest.importorskip('numpy').sum(np.ma.masked_where(np.isnan(arr), arr), **kwargs)
A:dask.array.tests.test_reductions.ssub->pytest.importorskip('numpy').random.default_rng().shuffle(list(sub))
A:dask.array.tests.test_reductions.expected->getattr(np, func)(x, axis=axis)
A:dask.array.tests.test_reductions.actual->getattr(da, func)(d, axis=axis)
A:dask.array.tests.test_reductions.a->pytest.importorskip('xarray').DataArray(da.from_array(np.full((10, 10), np.nan)))
A:dask.array.tests.test_reductions.a1->dask.array.from_array(x1, chunks=1)
A:dask.array.tests.test_reductions.a2->dask.array.from_array(x2, chunks=((5, 0, 5),))
A:dask.array.tests.test_reductions.b->pytest.importorskip('numpy').arange(8).reshape((2, 2, 2))
A:dask.array.tests.test_reductions.x2->dask.array.ones((0, 0, 0), chunks=4).compute()
A:dask.array.tests.test_reductions.x3->pytest.importorskip('numpy').array([[1, 1, 2, 3], [1, 1, 4, 0]])
A:dask.array.tests.test_reductions.a3->dask.array.from_array(x3, chunks=1)
A:dask.array.tests.test_reductions.x1->dask.array.ones((10, 0, 5), chunks=4).compute()
A:dask.array.tests.test_reductions.a4->dask.array.arange(10)
A:dask.array.tests.test_reductions.x[:2, :2]->pytest.importorskip('numpy').array([[1, 2], [3, 4]])
A:dask.array.tests.test_reductions.d->dask.array.from_array(x, chunks=2)
A:dask.array.tests.test_reductions.dx1->dask.array.ones((10, 0, 5), chunks=4)
A:dask.array.tests.test_reductions.dx2->dask.array.ones((0, 0, 0), chunks=4)
A:dask.array.tests.test_reductions.(dependencies, dependents)->get_deps(x.dask)
A:dask.array.tests.test_reductions.(names, tokens)->list(zip_longest(*[key[0].rsplit('-', 1) for key in a.dask]))
A:dask.array.tests.test_reductions.np_func->getattr(np, func)
A:dask.array.tests.test_reductions.da_func->getattr(da, func)
A:dask.array.tests.test_reductions.a_r->np_func(a, axis=axis)
A:dask.array.tests.test_reductions.d_r->da_func(d, axis=axis, method=method)
A:dask.array.tests.test_reductions.rng->pytest.importorskip('numpy').random.default_rng()
A:dask.array.tests.test_reductions.npa->pytest.importorskip('numpy').random.default_rng().random((10,))
A:dask.array.tests.test_reductions.npb->pytest.importorskip('numpy').random.default_rng().random((10, 20, 30))
A:dask.array.tests.test_reductions.arr->dask.array.ones(1).astype(object)
A:dask.array.tests.test_reductions.result->getattr(arr, method)().compute()
A:dask.array.tests.test_reductions.block_lens->pytest.importorskip('numpy').array([len(x.compute()) for x in d.blocks])
A:dask.array.tests.test_reductions.xr->pytest.importorskip('xarray')
A:dask.array.tests.test_reductions.shape->tuple((np.sum(s) for s in chunks))
A:dask.array.tests.test_reductions.np_array->pytest.importorskip('numpy').arange(np.prod(shape)).reshape(*shape)
A:dask.array.tests.test_reductions.reduced_x->dask.array.reduction(x, lambda x, axis, keepdims: x, lambda x, axis, keepdims: x, keepdims=True, axis=axes, split_every=split_every, dtype=x.dtype, meta=x._meta)
A:dask.array.tests.test_reductions.dx->dask.array.from_array(a, chunks=(4, 5))
A:dask.array.tests.test_reductions.w->pytest.importorskip('numpy').linspace(1, 2, 6).reshape(6, 1)
dask.array.tests.test_reduction_0d_test(da_func,darr,np_func,narr)
dask.array.tests.test_reduction_1d_test(da_func,darr,np_func,narr,use_dtype=True,split_every=True)
dask.array.tests.test_reduction_2d_test(da_func,darr,np_func,narr,use_dtype=True,split_every=True)
dask.array.tests.test_reductions.assert_max_deps(x,n,eq=True)
dask.array.tests.test_reductions.reduction_0d_test(da_func,darr,np_func,narr)
dask.array.tests.test_reductions.reduction_1d_test(da_func,darr,np_func,narr,use_dtype=True,split_every=True)
dask.array.tests.test_reductions.reduction_2d_test(da_func,darr,np_func,narr,use_dtype=True,split_every=True)
dask.array.tests.test_reductions.test_0d_array()
dask.array.tests.test_reductions.test_arg_reductions(dfunc,func)
dask.array.tests.test_reductions.test_arg_reductions_unknown_chunksize(func)
dask.array.tests.test_reductions.test_arg_reductions_unknown_chunksize_2d(func)
dask.array.tests.test_reductions.test_arg_reductions_unknown_single_chunksize(func)
dask.array.tests.test_reductions.test_array_cumreduction_axis(func,use_nan,axis,method)
dask.array.tests.test_reductions.test_array_cumreduction_out(func)
dask.array.tests.test_reductions.test_array_reduction_out(func)
dask.array.tests.test_reductions.test_chunk_structure_independence(axes,split_every,chunks)
dask.array.tests.test_reductions.test_empty_chunk_nanmin_nanmax(func)
dask.array.tests.test_reductions.test_empty_chunk_nanmin_nanmax_raise(func)
dask.array.tests.test_reductions.test_general_reduction_names()
dask.array.tests.test_reductions.test_mean_func_does_not_warn()
dask.array.tests.test_reductions.test_median(axis,keepdims,func)
dask.array.tests.test_reductions.test_median_does_not_rechunk_if_whole_axis_in_one_chunk(axis,func)
dask.array.tests.test_reductions.test_min_max_empty_chunks(dfunc,func)
dask.array.tests.test_reductions.test_moment()
dask.array.tests.test_reductions.test_nan()
dask.array.tests.test_reductions.test_nan_func_does_not_warn(func)
dask.array.tests.test_reductions.test_nan_object(func)
dask.array.tests.test_reductions.test_nan_reduction_warnings(dfunc,func)
dask.array.tests.test_reductions.test_nanarg_reductions(dfunc,func)
dask.array.tests.test_reductions.test_numel(dtype,keepdims,nan)
dask.array.tests.test_reductions.test_object_reduction(method)
dask.array.tests.test_reductions.test_reduction_errors()
dask.array.tests.test_reductions.test_reduction_names()
dask.array.tests.test_reductions.test_reduction_on_scalar()
dask.array.tests.test_reductions.test_reductions_0D()
dask.array.tests.test_reductions.test_reductions_1D(dtype)
dask.array.tests.test_reductions.test_reductions_2D(dtype)
dask.array.tests.test_reductions.test_reductions_2D_nans()
dask.array.tests.test_reductions.test_reductions_with_empty_array()
dask.array.tests.test_reductions.test_reductions_with_negative_axes()
dask.array.tests.test_reductions.test_regres_3940(func,method)
dask.array.tests.test_reductions.test_topk_argtopk1(npfunc,daskfunc,split_every)
dask.array.tests.test_reductions.test_topk_argtopk2(npfunc,daskfunc,split_every,chunksize)
dask.array.tests.test_reductions.test_topk_argtopk3()
dask.array.tests.test_reductions.test_trace()
dask.array.tests.test_reductions.test_tree_reduce_depth()
dask.array.tests.test_reductions.test_tree_reduce_set_options()
dask.array.tests.test_reductions.test_weighted_reduction()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/tests/test_svg.py----------------------------------------
A:dask.array.tests.test_svg.cleaned->dask.array.ones((10, 10, 10, 10, 10)).to_svg().replace('&rarr;', '')
A:dask.array.tests.test_svg.x->dask.array.ones((3000, 10000, 50), chunks=(1000, 1000, 10))
A:dask.array.tests.test_svg.text->dask.array.ones((10, 10, 10, 10, 10)).to_svg()
A:dask.array.tests.test_svg.(a, b, c)->draw_sizes((1000, 100, 10))
A:dask.array.tests.test_svg.data->dask.array.ones((16000, 2400, 3600), chunks=(1, 2400, 3600))
dask.array.tests.test_svg.parses(text)
dask.array.tests.test_svg.test_3d()
dask.array.tests.test_svg.test_basic()
dask.array.tests.test_svg.test_draw_sizes()
dask.array.tests.test_svg.test_errors()
dask.array.tests.test_svg.test_repr_html()
dask.array.tests.test_svg.test_repr_html_size_units()
dask.array.tests.test_svg.test_too_many_lines_fills_sides_darker()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/tests/test_cupy_core.py----------------------------------------
A:dask.array.tests.test_cupy_core.cupy->pytest.importorskip('cupy')
A:dask.array.tests.test_cupy_core.cupy_version->parse_version(cupy.__version__)
A:dask.array.tests.test_cupy_core.c->pytest.importorskip('cupy').random.default_rng().random((2, 3, 4), dtype=dtype)
A:dask.array.tests.test_cupy_core.n->pytest.importorskip('cupy').random.default_rng().random((2, 3, 4), dtype=dtype).get()
A:dask.array.tests.test_cupy_core.dc->dask.array.from_array(c, chunks=(1, 2, 2), asarray=False)
A:dask.array.tests.test_cupy_core.dn->dask.array.from_array(n, chunks=(1, 2, 2))
A:dask.array.tests.test_cupy_core.ddc->func(dc)
A:dask.array.tests.test_cupy_core.ddn->func(dn)
A:dask.array.tests.test_cupy_core.a->dask.array.utils.asanyarray_safe(arr, like=like)
A:dask.array.tests.test_cupy_core.x_np->numpy.arange(56).reshape((7, 8))
A:dask.array.tests.test_cupy_core.x_cp->pytest.importorskip('cupy').arange(56).reshape((7, 8))
A:dask.array.tests.test_cupy_core.d_np->dask.array.from_array(x_np, chunks=(3, 4))
A:dask.array.tests.test_cupy_core.d_cp->dask.array.from_array(x_cp, chunks=(3, 4))
A:dask.array.tests.test_cupy_core.res_np->dask.array.core._vindex(d_np, [0, 1, 6, 0], [0, 1, 0, 7])
A:dask.array.tests.test_cupy_core.res_cp->dask.array.core._vindex(d_cp, [0, 1, 6, 0], [0, 1, 0, 7])
A:dask.array.tests.test_cupy_core.x->pytest.importorskip('cupy').arange(12).reshape((3, 4))
A:dask.array.tests.test_cupy_core.d->dask.array.from_array(cupy.ones((10, 10)), chunks=(2, 2))
A:dask.array.tests.test_cupy_core.result->dask.array.core.getter(cupy.arange(5), (None, slice(None, None)))
A:dask.array.tests.test_cupy_core.r->dask.get(*args, **kwargs)
A:dask.array.tests.test_cupy_core.at->pytest.importorskip('cupy').zeros(shape=(10, 10))
A:dask.array.tests.test_cupy_core.dx->dask.array.from_array(x, chunks=(2, 3))
A:dask.array.tests.test_cupy_core.index->dask.array.from_array([0, -1], chunks=(1,))
A:dask.array.tests.test_cupy_core.dx[...]->pytest.importorskip('cupy').arange(24).reshape((2, 1, 3, 4))
A:dask.array.tests.test_cupy_core.v->pytest.importorskip('cupy').arange(12).reshape((3, 4)).reshape((1, 1) + x.shape)
A:dask.array.tests.test_cupy_core.y->dask.array.from_array(cupy.array(1))
A:dask.array.tests.test_cupy_core.cp_func->getattr(cupy, array_func)
A:dask.array.tests.test_cupy_core.xp_func->getattr(xp, array_func)
A:dask.array.tests.test_cupy_core.cp_a->cp_func([1, 2, 3])
A:dask.array.tests.test_cupy_core.xp_a->xp_func(orig_arr([1, 2, 3]), like=da.from_array(cupy.array(())))
dask.array.tests.test_cupy_core.test_array_like(xp,orig_arr,array_func)
dask.array.tests.test_cupy_core.test_asanyarray(arr,like)
dask.array.tests.test_cupy_core.test_basic(func)
dask.array.tests.test_cupy_core.test_getter()
dask.array.tests.test_cupy_core.test_setitem_1d()
dask.array.tests.test_cupy_core.test_setitem_2d()
dask.array.tests.test_cupy_core.test_setitem_errs()
dask.array.tests.test_cupy_core.test_setitem_extended_API_0d()
dask.array.tests.test_cupy_core.test_setitem_extended_API_1d(index,value)
dask.array.tests.test_cupy_core.test_setitem_extended_API_2d(index,value)
dask.array.tests.test_cupy_core.test_setitem_extended_API_2d_rhs_func_of_lhs()
dask.array.tests.test_cupy_core.test_setitem_on_read_only_blocks()
dask.array.tests.test_cupy_core.test_sizeof(dtype)
dask.array.tests.test_cupy_core.test_store_kwargs()
dask.array.tests.test_cupy_core.test_view()
dask.array.tests.test_cupy_core.test_view_fortran()
dask.array.tests.test_cupy_core.test_vindex()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/tests/test_numpy_compat.py----------------------------------------
A:dask.array.tests.test_numpy_compat.x->numpy.ones((5, 3), dtype=dtype)
A:dask.array.tests.test_numpy_compat.dx->dask.array.ones((5, 3), dtype=dtype, chunks=3)
A:dask.array.tests.test_numpy_compat.image->dask.array.from_array(np.array([[0, 1], [1, 2]]), chunks=(1, 2))
dask.array.tests.test_numpy_compat.dtype(request)
dask.array.tests.test_numpy_compat.index(request)
dask.array.tests.test_numpy_compat.test_basic()
dask.array.tests.test_numpy_compat.test_min_max_round_funcs()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/tests/test_random.py----------------------------------------
A:dask.array.tests.test_random.state->generator_class(5)
A:dask.array.tests.test_random.x->dask.array.random.default_rng(cupy.random.default_rng()).standard_normal((10, 5), dtype=np.float32)
A:dask.array.tests.test_random.y->generator_class().permutation(x)
A:dask.array.tests.test_random.rng->dask.array.random.default_rng(seed=sd)
A:dask.array.tests.test_random.samples_1->generator_class(42).normal(size=1000, chunks=10)
A:dask.array.tests.test_random.samples_2->generator_class(42).normal(size=1000, chunks=10)
A:dask.array.tests.test_random.state1->generator_class(42)
A:dask.array.tests.test_random.state2->generator_class(42)
A:dask.array.tests.test_random.a->generator_class(0)
A:dask.array.tests.test_random.b->generator_class(0)
A:dask.array.tests.test_random.x1->generator_class(123).random(20, chunks=20)
A:dask.array.tests.test_random.arr->numpy.arange(6).reshape((2, 3))
A:dask.array.tests.test_random.daones->dask.array.ones((2, 3, 4), chunks=3)
A:dask.array.tests.test_random.z->generator_class().normal(y, 0.01, chunks=(10,))
A:dask.array.tests.test_random.res->dask.array.random.default_rng(cupy.random.default_rng()).standard_normal((10, 5), dtype=np.float32).compute()
A:dask.array.tests.test_random.np_a->numpy.array(py_a, dtype='f8')
A:dask.array.tests.test_random.da_a->dask.array.from_array(np_a, chunks=2)
A:dask.array.tests.test_random.np_p->numpy.array([0, 0.2, 0.2, 0.3, 0.3])
A:dask.array.tests.test_random.da_p->dask.array.from_array(np_p, chunks=2)
A:dask.array.tests.test_random.r1->generator_class(0).permutation(x)
A:dask.array.tests.test_random.r2->generator_class(0).permutation(x)
A:dask.array.tests.test_random.cupy->pytest.importorskip('cupy')
A:dask.array.tests.test_random.rs->dask.array.random.default_rng(cupy.random.default_rng())
dask.array.tests.test_random.generator_class(request)
dask.array.tests.test_random.test_Generator_only_funcs(sz)
dask.array.tests.test_random.test_RandomState_only_funcs()
dask.array.tests.test_random.test_array_broadcasting(generator_class)
dask.array.tests.test_random.test_auto_chunks(generator_class)
dask.array.tests.test_random.test_can_make_really_big_random_array(generator_class)
dask.array.tests.test_random.test_choice(generator_class)
dask.array.tests.test_random.test_concurrency(generator_class)
dask.array.tests.test_random.test_consistent_across_sizes(generator_class)
dask.array.tests.test_random.test_create_with_auto_dimensions()
dask.array.tests.test_random.test_default_rng(sd)
dask.array.tests.test_random.test_determinisim_through_dask_values(generator_class)
dask.array.tests.test_random.test_doc_generator()
dask.array.tests.test_random.test_doc_randomstate(generator_class)
dask.array.tests.test_random.test_docs(generator_class)
dask.array.tests.test_random.test_generator_consistent_names(generator_class)
dask.array.tests.test_random.test_generators(generator_class)
dask.array.tests.test_random.test_kwargs(generator_class)
dask.array.tests.test_random.test_multinomial(generator_class)
dask.array.tests.test_random.test_names()
dask.array.tests.test_random.test_parametrized_random_function(generator_class)
dask.array.tests.test_random.test_permutation(generator_class)
dask.array.tests.test_random.test_raises_bad_kwarg(generator_class)
dask.array.tests.test_random.test_randint_dtype()
dask.array.tests.test_random.test_random(generator_class)
dask.array.tests.test_random.test_random_all(sz)
dask.array.tests.test_random.test_random_all_with_class_methods(generator_class,sz)
dask.array.tests.test_random.test_random_seed()
dask.array.tests.test_random.test_randomstate_kwargs()
dask.array.tests.test_random.test_serializability(generator_class)
dask.array.tests.test_random.test_unique_names(generator_class)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/tests/test_array_core.py----------------------------------------
A:dask.array.tests.test_array_core.np->pytest.importorskip('numpy')
A:dask.array.tests.test_array_core.shape->tuple((d * n for n in chunk))
A:dask.array.tests.test_array_core.arr->dask.array.from_zarr(d, chunks=(5,))
A:dask.array.tests.test_array_core.dsk->dict(da.from_zarr(a, inline_array=inline_array).dask)
A:dask.array.tests.test_array_core.x->dask.array.ones(10)
A:dask.array.tests.test_array_core.z->x.copy().compute_chunk_sizes()
A:dask.array.tests.test_array_core.o->pytest.importorskip('numpy').ones((20, 20))
A:dask.array.tests.test_array_core.getx->graph_from_arraylike(x, (5, 5), shape=(20, 20), name='x')
A:dask.array.tests.test_array_core.geto->graph_from_arraylike(o, (5, 5), shape=(20, 20), name='o')
A:dask.array.tests.test_array_core.result->normalize_chunks(((2, 3, 5), 'auto'), (10, 10), limit=10, dtype=np.uint8)
A:dask.array.tests.test_array_core.out->dask.array.from_array([1] + [2, 2] + [3, 3, 3], chunks=((1, 2, 3),)).map_blocks(lambda x: 5, chunks=out_chunks)
A:dask.array.tests.test_array_core.comp->top(f, 'out', 'ij', 'x', 'ji', numblocks={'x': (4, 4)})
A:dask.array.tests.test_array_core.a->dask.array.random.default_rng().random(size=(20, 2))
A:dask.array.tests.test_array_core.dx->dask.array.from_array(x, chunks=chunks)
A:dask.array.tests.test_array_core.d->h5py.File(fn1, mode='a').create_dataset('/x', shape=(1000, 1000), chunks=(32, 64), dtype='float64')
A:dask.array.tests.test_array_core.nx->dask.array.ones(10).compute()
A:dask.array.tests.test_array_core.ny->pytest.importorskip('numpy').linalg._umath_linalg.inv(nx)
A:dask.array.tests.test_array_core.y->dask.array.ones(10).copy()
A:dask.array.tests.test_array_core.(nw, nv)->pytest.importorskip('numpy').linalg._umath_linalg.eig(nx)
A:dask.array.tests.test_array_core.(w, v)->pytest.importorskip('numpy').linalg._umath_linalg.eig(x)
A:dask.array.tests.test_array_core.s->dask.array.ones(10).map_blocks(pd.DataFrame)
A:dask.array.tests.test_array_core.colon->slice(None, None, None)
A:dask.array.tests.test_array_core.s2->stack([a, b, c], axis=2)
A:dask.array.tests.test_array_core.result_np->pytest.importorskip('numpy').concatenate([np.zeros(0), np.zeros(0)])
A:dask.array.tests.test_array_core.result_da->dask.array.concatenate([da.zeros(0), da.zeros(0)])
A:dask.array.tests.test_array_core.chunks->normalize_chunks('1800B', shape=(10, 20, 30), dtype='float64')
A:dask.array.tests.test_array_core.i->pytest.importorskip('numpy').arange(10, dtype='i4')
A:dask.array.tests.test_array_core.f->pytest.importorskip('h5py').File(fn1, mode='a')
A:dask.array.tests.test_array_core.di->dask.array.from_array(i, chunks=5)
A:dask.array.tests.test_array_core.df->pytest.importorskip('pandas').DataFrame({'x': [1, 2, 3]})
A:dask.array.tests.test_array_core.res->dask.array.core.map_blocks(func, d, f, dtype=d.dtype)
A:dask.array.tests.test_array_core.rng->dask.array.random.default_rng()
A:dask.array.tests.test_array_core.dd->pytest.importorskip('dask.dataframe')
A:dask.array.tests.test_array_core.pd->pytest.importorskip('pandas')
A:dask.array.tests.test_array_core.a_df->pytest.importorskip('pandas').DataFrame({'x': np.arange(12)})
A:dask.array.tests.test_array_core.b_df->pytest.importorskip('pandas').DataFrame({'y': np.arange(12) * 10})
A:dask.array.tests.test_array_core.a_ddf->pytest.importorskip('dask.dataframe').from_pandas(a_df, sort=False, npartitions=3)
A:dask.array.tests.test_array_core.b_ddf->pytest.importorskip('dask.dataframe').from_pandas(b_df, sort=False, npartitions=3)
A:dask.array.tests.test_array_core.c_x->dask.array.concatenate([a_x, b_x], axis=1, allow_unknown_chunksizes=True)
A:dask.array.tests.test_array_core.m_df->pytest.importorskip('pandas').DataFrame({'m': np.arange(12) * 100})
A:dask.array.tests.test_array_core.n_df->pytest.importorskip('pandas').DataFrame({'n': np.arange(12) * 1000})
A:dask.array.tests.test_array_core.m_ddf->pytest.importorskip('dask.dataframe').from_pandas(m_df, sort=False, npartitions=3)
A:dask.array.tests.test_array_core.n_ddf->pytest.importorskip('dask.dataframe').from_pandas(n_df, sort=False, npartitions=3)
A:dask.array.tests.test_array_core.b->pytest.importorskip('numpy').ones(1000000)
A:dask.array.tests.test_array_core.a1->pytest.importorskip('numpy').array([1, 1, 1])
A:dask.array.tests.test_array_core.d1->dask.array.store(x, y, regions=(0,), compute=False)
A:dask.array.tests.test_array_core.d2->dask.array.store(x, y, regions=(1,), compute=False)
A:dask.array.tests.test_array_core.expected->pytest.importorskip('numpy').array([])
A:dask.array.tests.test_array_core.a2->dask.array.from_zarr(z)
A:dask.array.tests.test_array_core.a3->dask.array.ma.masked_equal(array, 0)
A:dask.array.tests.test_array_core.a4->loads(dumps(a3))
A:dask.array.tests.test_array_core.a5->pytest.importorskip('numpy').array(5)
A:dask.array.tests.test_array_core.a6->pytest.importorskip('numpy').array([6, 6, 6, 6, 6])
A:dask.array.tests.test_array_core.a7->pytest.importorskip('numpy').zeros((2, 6))
A:dask.array.tests.test_array_core.d3->dask.array.asarray(a3)
A:dask.array.tests.test_array_core.d4->dask.array.asarray(a4)
A:dask.array.tests.test_array_core.d5->dask.array.asarray(a5)
A:dask.array.tests.test_array_core.d6->dask.array.asarray(a6)
A:dask.array.tests.test_array_core.d7->dask.array.asarray(a7)
A:dask.array.tests.test_array_core.d000->dask.array.asarray(a000)
A:dask.array.tests.test_array_core.d100->dask.array.asarray(a100)
A:dask.array.tests.test_array_core.d010->dask.array.asarray(a010)
A:dask.array.tests.test_array_core.d001->dask.array.asarray(a001)
A:dask.array.tests.test_array_core.d011->dask.array.asarray(a011)
A:dask.array.tests.test_array_core.d101->dask.array.asarray(a101)
A:dask.array.tests.test_array_core.d110->dask.array.asarray(a110)
A:dask.array.tests.test_array_core.d111->dask.array.asarray(a111)
A:dask.array.tests.test_array_core.c->dask.array.blockwise(add, 'i', a, 'i', b, None, dtype=a.dtype)
A:dask.array.tests.test_array_core.data->pytest.importorskip('numpy').ones((100, 50), dtype=dtype)
A:dask.array.tests.test_array_core.list_vec->list(range(1, 6))
A:dask.array.tests.test_array_core.xb->pytest.importorskip('numpy').broadcast_to(x, shape)
A:dask.array.tests.test_array_core.ab->broadcast_to(a, shape, chunks=chunks)
A:dask.array.tests.test_array_core.d_a->dask.array.from_array(a, chunks=tuple((s // 2 for s in a.shape)))
A:dask.array.tests.test_array_core.a_r->pytest.importorskip('numpy').broadcast_arrays(a_0, a_1)
A:dask.array.tests.test_array_core.d_r->dask.array.broadcast_arrays(d_a_0, d_a_1)
A:dask.array.tests.test_array_core.u->dask.array.random.default_rng().random(u_shape)
A:dask.array.tests.test_array_core.v->dask.array.ones(10).reshape((1, 1) + x.shape)
A:dask.array.tests.test_array_core.d_u->from_array(u, chunks=1)
A:dask.array.tests.test_array_core.d_v->from_array(v, chunks=1)
A:dask.array.tests.test_array_core.xr->dask.array.ones(10).reshape(new_shape)
A:dask.array.tests.test_array_core.ar->dask.array.random.default_rng().random(size=(20, 2)).reshape(new_shape)
A:dask.array.tests.test_array_core.array->dask.array.from_array([1] + [2, 2] + [3, 3, 3], chunks=((1, 2, 3),))
A:dask.array.tests.test_array_core.limit->parse_bytes(dask.config.get('array.chunk-size'))
A:dask.array.tests.test_array_core.e->h5py.File(fn1, mode='a').create_dataset('/x', shape=(1000, 1000), chunks=(32, 64), dtype='float64').map_blocks(lambda b: b.sum(axis=0), chunks=(4,), drop_axis=drop_axis, dtype=d.dtype)
A:dask.array.tests.test_array_core.values->dask.compute(*vs, scheduler='single-threaded')
A:dask.array.tests.test_array_core.dy->dask.array.ones(11, chunks=(3,))
A:dask.array.tests.test_array_core.dz->dask.array.map_blocks(np.add, dx, dy, chunks=dx.chunks)
A:dask.array.tests.test_array_core.cast->kwargs.pop('cast', 'i8')
A:dask.array.tests.test_array_core.msg->str(e.value)
A:dask.array.tests.test_array_core.input_arr->dask.array.zeros((3, 4), chunks=((3,), (4,)), dtype=np.float32)
A:dask.array.tests.test_array_core.optimized->optimize_blockwise(dsk)
A:dask.array.tests.test_array_core.sparse->pytest.importorskip('sparse')
A:dask.array.tests.test_array_core.d0->dask.array.from_array(np.array(1), chunks=(1,))
A:dask.array.tests.test_array_core.darr->dask.array.from_array(arr, chunks=(10, 10))
A:dask.array.tests.test_array_core.r->broadcast_chunks(a, b)
A:dask.array.tests.test_array_core.at->delayed(at)
A:dask.array.tests.test_array_core.atd->delayed(make_target)('at')
A:dask.array.tests.test_array_core.btd->delayed(make_target)('bt')
A:dask.array.tests.test_array_core.st->dask.array.random.default_rng().random(size=(20, 2)).store(at, scheduler='processes', num_workers=10)
A:dask.array.tests.test_array_core.bt->pytest.importorskip('numpy').zeros(shape=(10, 10))
A:dask.array.tests.test_array_core.(ar, br)->dask.array.compute(ar, br)
A:dask.array.tests.test_array_core.(dat, dbt)->store([a, b], [at, bt], compute=False, return_stored=True)
A:dask.array.tests.test_array_core.self.max_concurrent_uses->max(self.concurrent_uses, self.max_concurrent_uses)
A:dask.array.tests.test_array_core.self.lock->Lock(*args, **kwargs)
A:dask.array.tests.test_array_core._Lock->type(Lock())
A:dask.array.tests.test_array_core.lock->CounterLock()
A:dask.array.tests.test_array_core.nchunks->sum((math.prod(map(len, e.chunks)) for e in (a, b)))
A:dask.array.tests.test_array_core.st1->dask.array.random.default_rng().random(size=(20, 2)).store(at, return_stored=return_stored, compute=False)
A:dask.array.tests.test_array_core.st2->dask.array.random.default_rng().random(size=(20, 2)).store(at, return_stored=return_stored, compute=False)
A:dask.array.tests.test_array_core.h5py->pytest.importorskip('h5py')
A:dask.array.tests.test_array_core.result_a->dask.array.random.default_rng().random(size=(20, 2)).astype(float, order='F')
A:dask.array.tests.test_array_core.result_b->result_b.compute().compute()
A:dask.array.tests.test_array_core.(l1, l2)->dask.array.modf(a)
A:dask.array.tests.test_array_core.(r1, r2)->pytest.importorskip('numpy').modf(x)
A:dask.array.tests.test_array_core.dtype->pytest.importorskip('numpy').dtype('i8')
A:dask.array.tests.test_array_core.self.ndim->len(x.shape)
A:dask.array.tests.test_array_core.y_c->dask.array.ones(10).copy().copy()
A:dask.array.tests.test_array_core.ddf->pytest.importorskip('dask.dataframe').from_pandas(df, npartitions=2)
A:dask.array.tests.test_array_core.plain_np_result->concatenate3(concat)
A:dask.array.tests.test_array_core.axis->_get_axis(ind)
A:dask.array.tests.test_array_core.k->len(next((i for i in ind if isinstance(i, (np.ndarray, list)))))
A:dask.array.tests.test_array_core.inds->pytest.importorskip('numpy').int64(3)
A:dask.array.tests.test_array_core.target->pytest.importorskip('numpy').memmap(fn_1, shape=x.shape, mode='w+', dtype=x.dtype)
A:dask.array.tests.test_array_core.stackdir->os.path.join(dirname, 'test')
A:dask.array.tests.test_array_core.g->pytest.importorskip('h5py').File(fn2, mode='a')
A:dask.array.tests.test_array_core.f['x']->pytest.importorskip('numpy').arange(10).astype(float)
A:dask.array.tests.test_array_core.g['x']->pytest.importorskip('numpy').ones(10).astype(float)
A:dask.array.tests.test_array_core.[[a, b], [c, d]]->dask.array.ones(10).copy().to_delayed()
A:dask.array.tests.test_array_core.y2->copy.deepcopy(x, memo=memo)
A:dask.array.tests.test_array_core.xx->dask.array.ones(10).compute()
A:dask.array.tests.test_array_core.yy->dask.array.ones(10).copy().compute(scheduler='single-threaded')
A:dask.array.tests.test_array_core.zz->x.copy().compute_chunk_sizes().compute(scheduler='single-threaded')
A:dask.array.tests.test_array_core.X->dask.array.from_array(X, chunks=(4, 4, 4))
A:dask.array.tests.test_array_core.dind->dask.array.from_array(ind, (2, 2))
A:dask.array.tests.test_array_core.index->dask.array.from_array([0, -1], chunks=(1,))
A:dask.array.tests.test_array_core.val->pytest.importorskip('numpy').array([0, 0], dtype=int)
A:dask.array.tests.test_array_core.dx[...]->pytest.importorskip('numpy').arange(24).reshape((2, 1, 3, 4))
A:dask.array.tests.test_array_core.dx2->dask.array.from_array(x, chunks=chunks, name=False)
A:dask.array.tests.test_array_core.dx3->dask.array.from_array(x, chunks=chunks, name=False)
A:dask.array.tests.test_array_core.zarr->pytest.importorskip('zarr')
A:dask.array.tests.test_array_core.path->pathlib.Path(d)
A:dask.array.tests.test_array_core.mapper->pytest.importorskip('zarr').storage.DirectoryStore(d)
A:dask.array.tests.test_array_core.group->pytest.importorskip('zarr').open_group(d, mode='r')
A:dask.array.tests.test_array_core.tiledb->pytest.importorskip('tiledb')
A:dask.array.tests.test_array_core.tdb->pytest.importorskip('tiledb').DenseArray(uri, 'r')
A:dask.array.tests.test_array_core.tdb2->dask.array.from_tiledb(t)
A:dask.array.tests.test_array_core.dom->pytest.importorskip('tiledb').Domain(tiledb.Dim('x', (0, 1000), tile=100), tiledb.Dim('y', (0, 1000), tile=100))
A:dask.array.tests.test_array_core.schema->pytest.importorskip('tiledb').ArraySchema(attrs=(tiledb.Attr('attr1'), tiledb.Attr('attr2')), domain=dom)
A:dask.array.tests.test_array_core.ar1->dask.array.random.default_rng().standard_normal(tdb.schema.shape)
A:dask.array.tests.test_array_core.ar2->dask.array.random.default_rng().standard_normal(tdb.schema.shape)
A:dask.array.tests.test_array_core.blockview->BlockView(x)
A:dask.array.tests.test_array_core.vs->dask.array.ones(10).copy().to_delayed().flatten().tolist()
A:dask.array.tests.test_array_core.z_expected->sp_concatenate([scipy.sparse.csr_matrix(e.compute()) for e in xs])
A:dask.array.tests.test_array_core.Y->dask.array.random.default_rng().random((10, 10), chunks='auto')
A:dask.array.tests.test_array_core.meta->pytest.importorskip('sparse').COO.from_numpy(x)
A:dask.array.tests.test_array_core.Z->dask.array.random.default_rng().random((10, 10), chunks='auto').compute_chunk_sizes()
A:dask.array.tests.test_array_core.actual->load_store_chunk(x=np.array([]), out=np.array([]), index=2, lock=False, return_stored=True, load_stored=False)
dask.array.tests.test_array_core.CounterLock(self,*args,**kwargs)
dask.array.tests.test_array_core.CounterLock.__init__(self,*args,**kwargs)
dask.array.tests.test_array_core.CounterLock.acquire(self,*args,**kwargs)
dask.array.tests.test_array_core.CounterLock.release(self,*args,**kwargs)
dask.array.tests.test_array_core.MyArray(self,x)
dask.array.tests.test_array_core.MyArray.__getitem__(self,i)
dask.array.tests.test_array_core.MyArray.__init__(self,x)
dask.array.tests.test_array_core.NonthreadSafeStore(self)
dask.array.tests.test_array_core.NonthreadSafeStore.__init__(self)
dask.array.tests.test_array_core.NonthreadSafeStore.__setitem__(self,key,value)
dask.array.tests.test_array_core.ThreadSafeStore(self)
dask.array.tests.test_array_core.ThreadSafeStore.__init__(self)
dask.array.tests.test_array_core.ThreadSafeStore.__setitem__(self,key,value)
dask.array.tests.test_array_core.ThreadSafetyError(Exception)
dask.array.tests.test_array_core._known(num=50)
dask.array.tests.test_array_core.test_3851()
dask.array.tests.test_array_core.test_3925()
dask.array.tests.test_array_core.test_A_property()
dask.array.tests.test_array_core.test_Array()
dask.array.tests.test_array_core.test_Array_computation()
dask.array.tests.test_array_core.test_Array_normalizes_dtype()
dask.array.tests.test_array_core.test_Array_numpy_gufunc_call__array_ufunc__01()
dask.array.tests.test_array_core.test_Array_numpy_gufunc_call__array_ufunc__02()
dask.array.tests.test_array_core.test_T()
dask.array.tests.test_array_core.test_align_chunks_to_previous_chunks()
dask.array.tests.test_array_core.test_arithmetic()
dask.array.tests.test_array_core.test_array_compute_forward_kwargs()
dask.array.tests.test_array_core.test_array_copy_noop(chunks)
dask.array.tests.test_array_core.test_array_picklable(array)
dask.array.tests.test_array_core.test_asanyarray()
dask.array.tests.test_array_core.test_asanyarray_dataframe()
dask.array.tests.test_array_core.test_asanyarray_datetime64()
dask.array.tests.test_array_core.test_asarray(asarray)
dask.array.tests.test_array_core.test_asarray_chunks()
dask.array.tests.test_array_core.test_asarray_dask_dataframe(asarray)
dask.array.tests.test_array_core.test_asarray_h5py(asarray,inline_array)
dask.array.tests.test_array_core.test_astype()
dask.array.tests.test_array_core.test_astype_gh1151()
dask.array.tests.test_array_core.test_astype_gh9316()
dask.array.tests.test_array_core.test_astype_gh9318()
dask.array.tests.test_array_core.test_auto_chunks_h5py()
dask.array.tests.test_array_core.test_block_3d()
dask.array.tests.test_array_core.test_block_complicated()
dask.array.tests.test_array_core.test_block_empty_lists()
dask.array.tests.test_array_core.test_block_invalid_nesting()
dask.array.tests.test_array_core.test_block_mixed_1d_and_2d()
dask.array.tests.test_array_core.test_block_nested()
dask.array.tests.test_array_core.test_block_no_lists()
dask.array.tests.test_array_core.test_block_simple_column_wise()
dask.array.tests.test_array_core.test_block_simple_row_wise()
dask.array.tests.test_array_core.test_block_tuple()
dask.array.tests.test_array_core.test_block_with_1d_arrays_column_wise()
dask.array.tests.test_array_core.test_block_with_1d_arrays_multiple_rows()
dask.array.tests.test_array_core.test_block_with_1d_arrays_row_wise()
dask.array.tests.test_array_core.test_block_with_mismatched_shape()
dask.array.tests.test_array_core.test_blockdims_from_blockshape()
dask.array.tests.test_array_core.test_blocks_indexer()
dask.array.tests.test_array_core.test_blockview()
dask.array.tests.test_array_core.test_blockwise_1_in_shape_I()
dask.array.tests.test_array_core.test_blockwise_1_in_shape_II()
dask.array.tests.test_array_core.test_blockwise_1_in_shape_III()
dask.array.tests.test_array_core.test_blockwise_concatenate()
dask.array.tests.test_array_core.test_blockwise_large_inputs_delayed()
dask.array.tests.test_array_core.test_blockwise_literals()
dask.array.tests.test_array_core.test_blockwise_with_numpy_arrays()
dask.array.tests.test_array_core.test_blockwise_zero_shape()
dask.array.tests.test_array_core.test_blockwise_zero_shape_new_axes()
dask.array.tests.test_array_core.test_bool()
dask.array.tests.test_array_core.test_broadcast_against_zero_shape()
dask.array.tests.test_array_core.test_broadcast_arrays()
dask.array.tests.test_array_core.test_broadcast_arrays_uneven_chunks()
dask.array.tests.test_array_core.test_broadcast_chunks()
dask.array.tests.test_array_core.test_broadcast_dimensions()
dask.array.tests.test_array_core.test_broadcast_dimensions_works_with_singleton_dimensions()
dask.array.tests.test_array_core.test_broadcast_operator(u_shape,v_shape)
dask.array.tests.test_array_core.test_broadcast_shapes()
dask.array.tests.test_array_core.test_broadcast_to()
dask.array.tests.test_array_core.test_broadcast_to_array()
dask.array.tests.test_array_core.test_broadcast_to_chunks()
dask.array.tests.test_array_core.test_broadcast_to_scalar()
dask.array.tests.test_array_core.test_chunk_assignment_invalidates_cached_properties()
dask.array.tests.test_array_core.test_chunk_non_array_like()
dask.array.tests.test_array_core.test_chunk_shape_broadcast(ndim)
dask.array.tests.test_array_core.test_chunked_dot_product()
dask.array.tests.test_array_core.test_chunked_transpose_plus_one()
dask.array.tests.test_array_core.test_chunks_error()
dask.array.tests.test_array_core.test_chunks_is_immutable()
dask.array.tests.test_array_core.test_coerce()
dask.array.tests.test_array_core.test_common_blockdim()
dask.array.tests.test_array_core.test_compute_chunk_sizes()
dask.array.tests.test_array_core.test_compute_chunk_sizes_2d_array()
dask.array.tests.test_array_core.test_compute_chunk_sizes_3d_array(N=8)
dask.array.tests.test_array_core.test_compute_chunk_sizes_warning_fixes_concatenate()
dask.array.tests.test_array_core.test_compute_chunk_sizes_warning_fixes_rechunk(unknown)
dask.array.tests.test_array_core.test_compute_chunk_sizes_warning_fixes_reduction(unknown)
dask.array.tests.test_array_core.test_compute_chunk_sizes_warning_fixes_reshape(unknown)
dask.array.tests.test_array_core.test_compute_chunk_sizes_warning_fixes_slicing()
dask.array.tests.test_array_core.test_compute_chunk_sizes_warning_fixes_to_svg(unknown)
dask.array.tests.test_array_core.test_compute_chunk_sizes_warning_fixes_to_zarr(unknown)
dask.array.tests.test_array_core.test_concatenate()
dask.array.tests.test_array_core.test_concatenate3_2()
dask.array.tests.test_array_core.test_concatenate3_nep18_dispatching(mock_concatenate2,one_d)
dask.array.tests.test_array_core.test_concatenate3_on_scalars()
dask.array.tests.test_array_core.test_concatenate_axes()
dask.array.tests.test_array_core.test_concatenate_errs()
dask.array.tests.test_array_core.test_concatenate_fixlen_strings()
dask.array.tests.test_array_core.test_concatenate_flatten()
dask.array.tests.test_array_core.test_concatenate_rechunk()
dask.array.tests.test_array_core.test_concatenate_stack_dont_warn()
dask.array.tests.test_array_core.test_concatenate_types(dtypes)
dask.array.tests.test_array_core.test_concatenate_unknown_axes()
dask.array.tests.test_array_core.test_concatenate_zero_size()
dask.array.tests.test_array_core.test_constructor_plugin()
dask.array.tests.test_array_core.test_constructors_chunks_dict()
dask.array.tests.test_array_core.test_copy_mutate()
dask.array.tests.test_array_core.test_cumulative()
dask.array.tests.test_array_core.test_dask_array_holds_scipy_sparse_containers()
dask.array.tests.test_array_core.test_dask_layers()
dask.array.tests.test_array_core.test_delayed_array_key_hygeine()
dask.array.tests.test_array_core.test_dont_dealias_outputs()
dask.array.tests.test_array_core.test_dont_fuse_outputs()
dask.array.tests.test_array_core.test_dtype()
dask.array.tests.test_array_core.test_dtype_complex()
dask.array.tests.test_array_core.test_elemwise_consistent_names()
dask.array.tests.test_array_core.test_elemwise_differently_chunked()
dask.array.tests.test_array_core.test_elemwise_dtype()
dask.array.tests.test_array_core.test_elemwise_name()
dask.array.tests.test_array_core.test_elemwise_on_scalars()
dask.array.tests.test_array_core.test_elemwise_uneven_chunks()
dask.array.tests.test_array_core.test_elemwise_with_lists(chunks,other)
dask.array.tests.test_array_core.test_elemwise_with_ndarrays()
dask.array.tests.test_array_core.test_ellipsis_slicing()
dask.array.tests.test_array_core.test_empty_array()
dask.array.tests.test_array_core.test_empty_chunks_in_array_len()
dask.array.tests.test_array_core.test_field_access()
dask.array.tests.test_array_core.test_field_access_with_shape()
dask.array.tests.test_array_core.test_from_array_chunks_dict()
dask.array.tests.test_array_core.test_from_array_dask_array()
dask.array.tests.test_array_core.test_from_array_dask_collection_warns()
dask.array.tests.test_array_core.test_from_array_getitem(wrap,inline_array)
dask.array.tests.test_array_core.test_from_array_inline()
dask.array.tests.test_array_core.test_from_array_list(x)
dask.array.tests.test_array_core.test_from_array_meta()
dask.array.tests.test_array_core.test_from_array_minus_one()
dask.array.tests.test_array_core.test_from_array_name()
dask.array.tests.test_array_core.test_from_array_names()
dask.array.tests.test_array_core.test_from_array_ndarray_getitem()
dask.array.tests.test_array_core.test_from_array_ndarray_onechunk(x)
dask.array.tests.test_array_core.test_from_array_no_asarray(asarray,cls,inline_array)
dask.array.tests.test_array_core.test_from_array_raises_on_bad_chunks()
dask.array.tests.test_array_core.test_from_array_scalar(type_)
dask.array.tests.test_array_core.test_from_array_tasks_always_call_getter(x,chunks,inline_array)
dask.array.tests.test_array_core.test_from_array_with_lock(inline_array)
dask.array.tests.test_array_core.test_from_array_with_missing_chunks()
dask.array.tests.test_array_core.test_from_delayed()
dask.array.tests.test_array_core.test_from_delayed_meta()
dask.array.tests.test_array_core.test_from_func()
dask.array.tests.test_array_core.test_from_zarr_name()
dask.array.tests.test_array_core.test_from_zarr_unique_name()
dask.array.tests.test_array_core.test_full()
dask.array.tests.test_array_core.test_getter()
dask.array.tests.test_array_core.test_graph_from_arraylike(inline_array)
dask.array.tests.test_array_core.test_h5py_newaxis()
dask.array.tests.test_array_core.test_h5py_tokenize()
dask.array.tests.test_array_core.test_index_array_with_array_1d()
dask.array.tests.test_array_core.test_index_array_with_array_2d()
dask.array.tests.test_array_core.test_index_array_with_array_3d_2d()
dask.array.tests.test_array_core.test_index_with_integer_types()
dask.array.tests.test_array_core.test_itemsize()
dask.array.tests.test_array_core.test_keys()
dask.array.tests.test_array_core.test_len_object_with_unknown_size()
dask.array.tests.test_array_core.test_load_store_chunk()
dask.array.tests.test_array_core.test_long_slice()
dask.array.tests.test_array_core.test_map_blocks()
dask.array.tests.test_array_core.test_map_blocks2()
dask.array.tests.test_array_core.test_map_blocks3()
dask.array.tests.test_array_core.test_map_blocks_block_info()
dask.array.tests.test_array_core.test_map_blocks_block_info_with_broadcast()
dask.array.tests.test_array_core.test_map_blocks_block_info_with_drop_axis()
dask.array.tests.test_array_core.test_map_blocks_block_info_with_new_axis()
dask.array.tests.test_array_core.test_map_blocks_chunks()
dask.array.tests.test_array_core.test_map_blocks_dataframe()
dask.array.tests.test_array_core.test_map_blocks_delayed()
dask.array.tests.test_array_core.test_map_blocks_dtype_inference()
dask.array.tests.test_array_core.test_map_blocks_infer_chunks_broadcast()
dask.array.tests.test_array_core.test_map_blocks_infer_newaxis()
dask.array.tests.test_array_core.test_map_blocks_large_inputs_delayed()
dask.array.tests.test_array_core.test_map_blocks_name()
dask.array.tests.test_array_core.test_map_blocks_no_array_args()
dask.array.tests.test_array_core.test_map_blocks_optimize_blockwise(func)
dask.array.tests.test_array_core.test_map_blocks_series()
dask.array.tests.test_array_core.test_map_blocks_token_deprecated()
dask.array.tests.test_array_core.test_map_blocks_unique_name_chunks_dtype()
dask.array.tests.test_array_core.test_map_blocks_unique_name_drop_axis()
dask.array.tests.test_array_core.test_map_blocks_unique_name_new_axis()
dask.array.tests.test_array_core.test_map_blocks_with_changed_dimension()
dask.array.tests.test_array_core.test_map_blocks_with_changed_dimension_and_broadcast_chunks()
dask.array.tests.test_array_core.test_map_blocks_with_chunks()
dask.array.tests.test_array_core.test_map_blocks_with_constants()
dask.array.tests.test_array_core.test_map_blocks_with_invalid_drop_axis()
dask.array.tests.test_array_core.test_map_blocks_with_kwargs()
dask.array.tests.test_array_core.test_map_blocks_with_negative_drop_axis()
dask.array.tests.test_array_core.test_matmul()
dask.array.tests.test_array_core.test_matmul_array_ufunc()
dask.array.tests.test_array_core.test_memmap()
dask.array.tests.test_array_core.test_meta(dtype)
dask.array.tests.test_array_core.test_nbytes()
dask.array.tests.test_array_core.test_nbytes_auto()
dask.array.tests.test_array_core.test_no_chunks()
dask.array.tests.test_array_core.test_no_chunks_2d()
dask.array.tests.test_array_core.test_no_chunks_slicing_2d()
dask.array.tests.test_array_core.test_no_chunks_yes_chunks()
dask.array.tests.test_array_core.test_no_warnings_from_blockwise()
dask.array.tests.test_array_core.test_no_warnings_on_metadata()
dask.array.tests.test_array_core.test_normalize_chunks()
dask.array.tests.test_array_core.test_normalize_chunks_auto_1d(shape,limit,expected)
dask.array.tests.test_array_core.test_normalize_chunks_auto_2d(shape,chunks,limit,expected)
dask.array.tests.test_array_core.test_normalize_chunks_auto_3d()
dask.array.tests.test_array_core.test_normalize_chunks_nan()
dask.array.tests.test_array_core.test_normalize_chunks_object_dtype(dtype)
dask.array.tests.test_array_core.test_normalize_chunks_tuples_of_tuples()
dask.array.tests.test_array_core.test_np_array_with_zero_dimensions()
dask.array.tests.test_array_core.test_npartitions()
dask.array.tests.test_array_core.test_numblocks_suppoorts_singleton_block_dims()
dask.array.tests.test_array_core.test_operator_dtype_promotion()
dask.array.tests.test_array_core.test_operators()
dask.array.tests.test_array_core.test_optimize()
dask.array.tests.test_array_core.test_pandas_from_dask_array()
dask.array.tests.test_array_core.test_partitions_indexer()
dask.array.tests.test_array_core.test_point_slicing()
dask.array.tests.test_array_core.test_point_slicing_with_full_slice()
dask.array.tests.test_array_core.test_raise_informative_errors_no_chunks()
dask.array.tests.test_array_core.test_raise_on_bad_kwargs()
dask.array.tests.test_array_core.test_raise_on_no_chunks()
dask.array.tests.test_array_core.test_read_zarr_chunks()
dask.array.tests.test_array_core.test_rechunk_auto()
dask.array.tests.test_array_core.test_regular_chunks(data)
dask.array.tests.test_array_core.test_repr()
dask.array.tests.test_array_core.test_repr_html_array_highlevelgraph()
dask.array.tests.test_array_core.test_repr_meta()
dask.array.tests.test_array_core.test_reshape(original_shape,new_shape,chunks)
dask.array.tests.test_array_core.test_reshape_avoids_large_chunks(limit,shape,chunks,reshape_size)
dask.array.tests.test_array_core.test_reshape_exceptions()
dask.array.tests.test_array_core.test_reshape_not_implemented_error()
dask.array.tests.test_array_core.test_reshape_splat()
dask.array.tests.test_array_core.test_reshape_unknown_dimensions()
dask.array.tests.test_array_core.test_reshape_warns_by_default_if_it_is_producing_large_chunks()
dask.array.tests.test_array_core.test_scipy_sparse_concatenate(axis)
dask.array.tests.test_array_core.test_setitem_1d()
dask.array.tests.test_array_core.test_setitem_2d()
dask.array.tests.test_array_core.test_setitem_errs()
dask.array.tests.test_array_core.test_setitem_extended_API_0d()
dask.array.tests.test_array_core.test_setitem_extended_API_1d(index,value)
dask.array.tests.test_array_core.test_setitem_extended_API_2d(index,value)
dask.array.tests.test_array_core.test_setitem_extended_API_2d_mask(index,value)
dask.array.tests.test_array_core.test_setitem_extended_API_2d_rhs_func_of_lhs()
dask.array.tests.test_array_core.test_setitem_hardmask()
dask.array.tests.test_array_core.test_setitem_on_read_only_blocks()
dask.array.tests.test_array_core.test_setitem_slice_twice()
dask.array.tests.test_array_core.test_short_stack()
dask.array.tests.test_array_core.test_size()
dask.array.tests.test_array_core.test_slice_reversed()
dask.array.tests.test_array_core.test_slice_with_floats()
dask.array.tests.test_array_core.test_slice_with_integer_types()
dask.array.tests.test_array_core.test_slicing_flexible_type()
dask.array.tests.test_array_core.test_slicing_with_ellipsis()
dask.array.tests.test_array_core.test_slicing_with_ndarray()
dask.array.tests.test_array_core.test_slicing_with_non_ndarrays()
dask.array.tests.test_array_core.test_slicing_with_object_dtype()
dask.array.tests.test_array_core.test_stack()
dask.array.tests.test_array_core.test_stack_errs()
dask.array.tests.test_array_core.test_stack_promote_type()
dask.array.tests.test_array_core.test_stack_rechunk()
dask.array.tests.test_array_core.test_stack_scalars()
dask.array.tests.test_array_core.test_stack_unknown_chunksizes()
dask.array.tests.test_array_core.test_stack_zero_size()
dask.array.tests.test_array_core.test_store()
dask.array.tests.test_array_core.test_store_compute_false()
dask.array.tests.test_array_core.test_store_delayed_target()
dask.array.tests.test_array_core.test_store_deterministic_keys(return_stored,delayed_target)
dask.array.tests.test_array_core.test_store_kwargs()
dask.array.tests.test_array_core.test_store_locks()
dask.array.tests.test_array_core.test_store_method_return()
dask.array.tests.test_array_core.test_store_multiprocessing_lock()
dask.array.tests.test_array_core.test_store_nocompute_regions()
dask.array.tests.test_array_core.test_store_regions()
dask.array.tests.test_array_core.test_tiledb_multiattr()
dask.array.tests.test_array_core.test_tiledb_roundtrip()
dask.array.tests.test_array_core.test_timedelta_op()
dask.array.tests.test_array_core.test_to_backend()
dask.array.tests.test_array_core.test_to_dask_dataframe()
dask.array.tests.test_array_core.test_to_delayed()
dask.array.tests.test_array_core.test_to_delayed_optimize_graph()
dask.array.tests.test_array_core.test_to_hdf5()
dask.array.tests.test_array_core.test_to_npy_stack()
dask.array.tests.test_array_core.test_to_zarr_accepts_empty_array_without_exception_raised()
dask.array.tests.test_array_core.test_to_zarr_unknown_chunks_raises()
dask.array.tests.test_array_core.test_top()
dask.array.tests.test_array_core.test_top_literals()
dask.array.tests.test_array_core.test_top_supports_broadcasting_rules()
dask.array.tests.test_array_core.test_uneven_chunks()
dask.array.tests.test_array_core.test_uneven_chunks_blockwise()
dask.array.tests.test_array_core.test_uneven_chunks_that_fit_neatly()
dask.array.tests.test_array_core.test_view()
dask.array.tests.test_array_core.test_view_fortran()
dask.array.tests.test_array_core.test_vindex_basic()
dask.array.tests.test_array_core.test_vindex_errors()
dask.array.tests.test_array_core.test_vindex_identity()
dask.array.tests.test_array_core.test_vindex_merge()
dask.array.tests.test_array_core.test_vindex_nd()
dask.array.tests.test_array_core.test_vindex_negative()
dask.array.tests.test_array_core.test_warn_bad_rechunking()
dask.array.tests.test_array_core.test_zarr_existing_array()
dask.array.tests.test_array_core.test_zarr_group()
dask.array.tests.test_array_core.test_zarr_inline_array(inline_array)
dask.array.tests.test_array_core.test_zarr_nocompute()
dask.array.tests.test_array_core.test_zarr_pass_mapper()
dask.array.tests.test_array_core.test_zarr_regions()
dask.array.tests.test_array_core.test_zarr_return_stored(compute)
dask.array.tests.test_array_core.test_zarr_roundtrip()
dask.array.tests.test_array_core.test_zarr_roundtrip_with_path_like()
dask.array.tests.test_array_core.test_zero_sized_array_rechunk()
dask.array.tests.test_array_core.test_zero_slice_dtypes()
dask.array.tests.test_array_core.unknown()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/lib/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/array/lib/stride_tricks.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/__init__.py----------------------------------------
A:dask.dataframe.__init__.use_dask_expr->dask.config.get('dataframe.query-planning')
A:dask.dataframe.__init__._Frame->raise_not_implemented_error('_Frame')
A:dask.dataframe.__init__.Aggregation->raise_not_implemented_error('Aggregation')
A:dask.dataframe.__init__.read_fwf->raise_not_implemented_error('read_fwf')
A:dask.dataframe.__init__.merge_asof->raise_not_implemented_error('merge_asof')
A:dask.dataframe.__init__.melt->raise_not_implemented_error('melt')
dask.dataframe.__init__._dask_expr_enabled()->bool


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/extensions.py----------------------------------------
A:dask.dataframe.extensions.make_array_nonempty->Dispatch('make_array_nonempty')
A:dask.dataframe.extensions.make_scalar->Dispatch('make_scalar')


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/categorical.py----------------------------------------
A:dask.dataframe.categorical.df->df.copy().copy()
A:dask.dataframe.categorical.df[col]->df[col].astype(cat_dtype).astype(cat_dtype)
A:dask.dataframe.categorical.cat_dtype->categorical_dtype(meta=df.index, categories=index, ordered=False)
A:dask.dataframe.categorical.ind->df.copy().copy().index.astype(dtype=cat_dtype)
A:dask.dataframe.categorical.res[col]->x.dropna().drop_duplicates()
A:dask.dataframe.categorical.res->defaultdict(list)
A:dask.dataframe.categorical.columns->list(meta.select_dtypes(['object', 'string', 'category']).columns)
A:dask.dataframe.categorical.token->tokenize(df, columns, index, split_every)
A:dask.dataframe.categorical.graph->dask.highlevelgraph.HighLevelGraph.from_collections(prefix, dsk, dependencies=[df])
A:dask.dataframe.categorical.(categories, index)->compute_as_if_collection(df.__class__, graph, (prefix, 0), **kwargs)
A:dask.dataframe.categorical.categories->self._property_map('categories').unique().compute(**kwargs)
A:dask.dataframe.categorical.out->self._series.copy()
A:dask.dataframe.categorical.out._meta->clear_known_categories(out._meta)
A:dask.dataframe.categorical.present->pandas.Index(present.compute())
A:dask.dataframe.categorical.(ordered, mask)->pandas.Index(present.compute()).reindex(meta_cat.categories)
A:dask.dataframe.categorical.meta->meta_cat.set_categories(new_categories, ordered=meta_cat.ordered)
dask.dataframe.categorical.CategoricalAccessor(Accessor)
dask.dataframe.categorical.CategoricalAccessor.as_known(self,**kwargs)
dask.dataframe.categorical.CategoricalAccessor.as_unknown(self)
dask.dataframe.categorical.CategoricalAccessor.categories(self)
dask.dataframe.categorical.CategoricalAccessor.codes(self)
dask.dataframe.categorical.CategoricalAccessor.known(self)
dask.dataframe.categorical.CategoricalAccessor.ordered(self)
dask.dataframe.categorical.CategoricalAccessor.remove_unused_categories(self)
dask.dataframe.categorical._categorize_block(df,categories,index)
dask.dataframe.categorical._get_categories(df,columns,index)
dask.dataframe.categorical._get_categories_agg(parts)
dask.dataframe.categorical.categorize(df,columns=None,index=None,split_every=None,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/partitionquantiles.py----------------------------------------
A:dask.dataframe.partitionquantiles.random_state->numpy.random.RandomState(state)
A:dask.dataframe.partitionquantiles.q_fixed->numpy.linspace(0, 100, num_fixed)
A:dask.dataframe.partitionquantiles.qs->numpy.linspace(0, 1, npartitions + 1)
A:dask.dataframe.partitionquantiles.group_size->int(math.log(N))
A:dask.dataframe.partitionquantiles.prev_width->len(keys)
A:dask.dataframe.partitionquantiles.prev_keys->iter(keys)
A:dask.dataframe.partitionquantiles.width->tree_width(prev_width)
A:dask.dataframe.partitionquantiles.groups->tree_groups(prev_width, width)
A:dask.dataframe.partitionquantiles.diff->numpy.ediff1d(qs, 0.0, 0.0)
A:dask.dataframe.partitionquantiles.it->merge_sorted(*[zip(x, y) for (x, y) in vals_and_weights])
A:dask.dataframe.partitionquantiles.(val, weight)(prev_val, prev_weight)->next(it)
A:dask.dataframe.partitionquantiles.vals->numpy.round(vals).astype(data.dtype)
A:dask.dataframe.partitionquantiles.weights->numpy.array(weights)
A:dask.dataframe.partitionquantiles.q_weights->numpy.cumsum(trimmed_weights)
A:dask.dataframe.partitionquantiles.q_target->numpy.linspace(0, q_weights[-1], trimmed_npartitions + 1)
A:dask.dataframe.partitionquantiles.rv->pandas.array(rv, dtype=dtype)
A:dask.dataframe.partitionquantiles.duplicated_index->numpy.linspace(0, len(vals) - 1, npartitions - len(vals) + 1, dtype=int)
A:dask.dataframe.partitionquantiles.left->numpy.searchsorted(q_weights, q_target, side='left')
A:dask.dataframe.partitionquantiles.lower->numpy.minimum(left, right)
A:dask.dataframe.partitionquantiles.length->len(df)
A:dask.dataframe.partitionquantiles.(vals, _)->_percentile(array_safe(data, like=data.values), qs, interpolation)
A:dask.dataframe.partitionquantiles.vals[0]->data.min()
A:dask.dataframe.partitionquantiles.vals_and_weights->percentiles_to_weights(qs, vals, length)
A:dask.dataframe.partitionquantiles.token->tokenize(df, qs, upsample)
A:dask.dataframe.partitionquantiles.state_data->random_state_data(df.npartitions, random_state)
A:dask.dataframe.partitionquantiles.df_keys->df.__dask_keys__()
A:dask.dataframe.partitionquantiles.merge_dsk->create_merge_tree(merge_and_compress_summaries, sorted(val_dsk), name2)
A:dask.dataframe.partitionquantiles.merged_key->max(merge_dsk)
A:dask.dataframe.partitionquantiles.dsk->merge(df.dask, dtype_dsk, val_dsk, merge_dsk, last_dsk)
dask.dataframe.partitionquantiles.create_merge_tree(func,keys,token,level=0)
dask.dataframe.partitionquantiles.dtype_info(df)
dask.dataframe.partitionquantiles.merge_and_compress_summaries(vals_and_weights)
dask.dataframe.partitionquantiles.partition_quantiles(df,npartitions,upsample=1.0,random_state=None)
dask.dataframe.partitionquantiles.percentiles_summary(df,num_old,num_new,upsample,state)
dask.dataframe.partitionquantiles.percentiles_to_weights(qs,vals,length)
dask.dataframe.partitionquantiles.process_val_weights(vals_and_weights,npartitions,dtype_info)
dask.dataframe.partitionquantiles.sample_percentiles(num_old,num_new,chunk_length,upsample=1.0,random_state=None)
dask.dataframe.partitionquantiles.tree_groups(N,num_groups)
dask.dataframe.partitionquantiles.tree_width(N,to_binary=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/optimize.py----------------------------------------
A:dask.dataframe.optimize.keys->list(core.flatten(keys))
A:dask.dataframe.optimize.dsk->ensure_dict(dsk)
A:dask.dataframe.optimize.dependencies->ensure_dict(dsk).dependencies.copy()
A:dask.dataframe.optimize.fuse_subgraphs->dask.config.get('optimization.fuse.subgraphs')
A:dask.dataframe.optimize.(dsk, _)->cull(dsk, keys)
A:dask.dataframe.optimize.layers->ensure_dict(dsk).layers.copy()
A:dask.dataframe.optimize.columns->set()
A:dask.dataframe.optimize.row_select_layer->row_select_layers.pop()
A:dask.dataframe.optimize.new->old.project_columns(columns)
A:dask.dataframe.optimize.new_block->Blockwise(block.output, block.output_indices, block.dsk, new_indices, numblocks, block.concatenate, block.new_axes)
A:dask.dataframe.optimize.dependencies[new.name]->ensure_dict(dsk).dependencies.copy().pop(io_layer_name)
A:dask.dataframe.optimize.new_hlg->HighLevelGraph(layers, dependencies)
dask.dataframe.optimize(dsk,keys,**kwargs)
dask.dataframe.optimize.optimize(dsk,keys,**kwargs)
dask.dataframe.optimize.optimize_dataframe_getitem(dsk,keys)
dask.dataframe.optimize_dataframe_getitem(dsk,keys)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/dispatch.py----------------------------------------
A:dask.dataframe.dispatch.make_meta_dispatch->Dispatch('make_meta_dispatch')
A:dask.dataframe.dispatch.make_meta_obj->Dispatch('make_meta_obj')
A:dask.dataframe.dispatch.meta_nonempty->Dispatch('meta_nonempty')
A:dask.dataframe.dispatch.meta_lib_from_array->Dispatch('meta_lib_from_array')
A:dask.dataframe.dispatch.hash_object_dispatch->Dispatch('hash_object_dispatch')
A:dask.dataframe.dispatch.group_split_dispatch->Dispatch('group_split_dispatch')
A:dask.dataframe.dispatch.get_parallel_type->Dispatch('get_parallel_type')
A:dask.dataframe.dispatch.categorical_dtype_dispatch->Dispatch('CategoricalDtype')
A:dask.dataframe.dispatch.concat_dispatch->Dispatch('concat')
A:dask.dataframe.dispatch.tolist_dispatch->Dispatch('tolist')
A:dask.dataframe.dispatch.is_categorical_dtype_dispatch->Dispatch('is_categorical_dtype')
A:dask.dataframe.dispatch.union_categoricals_dispatch->Dispatch('union_categoricals')
A:dask.dataframe.dispatch.grouper_dispatch->Dispatch('grouper')
A:dask.dataframe.dispatch.partd_encode_dispatch->Dispatch('partd_encode_dispatch')
A:dask.dataframe.dispatch.pyarrow_schema_dispatch->Dispatch('pyarrow_schema_dispatch')
A:dask.dataframe.dispatch.from_pyarrow_table_dispatch->Dispatch('from_pyarrow_table_dispatch')
A:dask.dataframe.dispatch.to_pyarrow_table_dispatch->Dispatch('to_pyarrow_table_dispatch')
A:dask.dataframe.dispatch.to_pandas_dispatch->Dispatch('to_pandas_dispatch')
A:dask.dataframe.dispatch.func->Dispatch('union_categoricals').dispatch(type(to_union[0]))
A:dask.dataframe.dispatch.obj->getattr(obj, 'dtype', obj)
dask.dataframe.dispatch.categorical_dtype(meta,categories=None,ordered=False)
dask.dataframe.dispatch.concat(dfs,axis=0,join='outer',uniform=False,filter_warning=True,ignore_index=False,**kwargs)
dask.dataframe.dispatch.is_categorical_dtype(obj)
dask.dataframe.dispatch.make_meta(x,index=None,parent_meta=None)
dask.dataframe.dispatch.tolist(obj)
dask.dataframe.dispatch.union_categoricals(to_union,sort_categories=False,ignore_order=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/shuffle.py----------------------------------------
A:dask.dataframe.shuffle.logger->logging.getLogger(__name__)
A:dask.dataframe.shuffle.divisions->divisions.drop_duplicates().drop_duplicates()
A:dask.dataframe.shuffle.mins->mins.bfill().tolist().bfill().tolist()
A:dask.dataframe.shuffle.maxes->maxes.bfill().tolist().bfill().tolist()
A:dask.dataframe.shuffle.(divisions, sizes, mins, maxes)->compute(divisions, sizes, mins, maxes)
A:dask.dataframe.shuffle.empty_dataframe_detected->pandas.isna(divisions).all()
A:dask.dataframe.shuffle.total->sum(sizes)
A:dask.dataframe.shuffle.npartitions->max(100, df.npartitions)
A:dask.dataframe.shuffle.indexes->numpy.linspace(0, n - 1, npartitions + 1).astype(int)
A:dask.dataframe.shuffle.maxes2->(maxes.iloc[:n - 1] if ascending else maxes.iloc[1:]).reset_index(drop=True)
A:dask.dataframe.shuffle.mins2->(mins.iloc[1:] if ascending else mins.iloc[:n - 1]).reset_index(drop=True)
A:dask.dataframe.shuffle.(divisions, _, _, presorted)->_calculate_divisions(df, sort_by_col, repartition, npartitions, upsample, partition_size, ascending)
A:dask.dataframe.shuffle.df->df.astype(cast_dtype, errors='ignore').astype(cast_dtype, errors='ignore')
A:dask.dataframe.shuffle.(divisions, mins, maxes, presorted)->_calculate_divisions(df, index2, repartition, npartitions, upsample, partition_size)
A:dask.dataframe.shuffle.result->map_partitions(M.set_index, df, index, drop=drop, meta=meta, align_dataframes=False, transform_divisions=False)
A:dask.dataframe.shuffle.meta->df.astype(cast_dtype, errors='ignore').astype(cast_dtype, errors='ignore')._meta.set_index(index, drop=drop)
A:dask.dataframe.shuffle.partitions->numpy.empty(len(s), dtype='int32')
A:dask.dataframe.shuffle.df2->df.astype(cast_dtype, errors='ignore').astype(cast_dtype, errors='ignore').drop('_partitions', axis=1).set_index('_index', drop=True)
A:dask.dataframe.shuffle.df3->rearrange_by_column(df2, '_partitions', max_branch=max_branch, npartitions=len(divisions) - 1, shuffle_method=shuffle_method)
A:dask.dataframe.shuffle.df4->rearrange_by_column(df2, '_partitions', max_branch=max_branch, npartitions=len(divisions) - 1, shuffle_method=shuffle_method).map_partitions(set_index_post_series, index_name=index.name, drop=drop, column_dtype=df.columns.dtype)
A:dask.dataframe.shuffle.df4.divisions->tuple((i if not pd.isna(i) else np.nan for i in divisions))
A:dask.dataframe.shuffle.index->index.to_frame().to_frame()
A:dask.dataframe.shuffle.df2._meta->df.astype(cast_dtype, errors='ignore').astype(cast_dtype, errors='ignore').drop('_partitions', axis=1).set_index('_index', drop=True)._meta.reset_index(drop=True)
A:dask.dataframe.shuffle.self.compression->dask.config.get('dataframe.shuffle.compression', None)
A:dask.dataframe.shuffle.path->tempfile.mkdtemp(suffix='.partd', dir=self.tempdir)
A:dask.dataframe.shuffle.file->partd_compression(file)
A:dask.dataframe.shuffle.token->tokenize(df, npartitions)
A:dask.dataframe.shuffle.encode_cls->partd_encode_dispatch(df._meta)
A:dask.dataframe.shuffle.graph->dask.highlevelgraph.HighLevelGraph.from_collections(name, dsk, dependencies=[ddf])
A:dask.dataframe.shuffle.(pp, values)->compute_as_if_collection(DataFrame, graph, keys)
A:dask.dataframe.shuffle.dsk2->dict(zip(sorted(dsk2), values))
A:dask.dataframe.shuffle.layer->tlz.merge(dsk1, dsk2, dsk3, dsk4)
A:dask.dataframe.shuffle.shuffle_layer->SimpleShuffleLayer(shuffle_name, column, npartitions, df.npartitions, ignore_index, df._name, df._meta)
A:dask.dataframe.shuffle.stages->int(math.ceil(math.log(n) / math.log(max_branch)))
A:dask.dataframe.shuffle.k->int(math.ceil(n ** (1 / stages)))
A:dask.dataframe.shuffle.stage_layer->ShuffleLayer(stage_name, column, inputs, stage, npartitions, n, k, ignore_index, df._name, df._meta)
A:dask.dataframe.shuffle.graph2->dask.highlevelgraph.HighLevelGraph.from_collections(repartition_get_name, dsk, dependencies=[df])
A:dask.dataframe.shuffle.res->p.get(part)
A:dask.dataframe.shuffle.not_null->s.notna()
A:dask.dataframe.shuffle.nas->getattr(nas, 'values', nas)
A:dask.dataframe.shuffle.ind->hash_object_dispatch(df[cols] if cols else df, index=False)
A:dask.dataframe.shuffle.result2->group_split_dispatch(df, ind, n, ignore_index=ignore_index)
A:dask.dataframe.shuffle.typ->numpy.min_scalar_type(npartitions * 2)
A:dask.dataframe.shuffle.g->df.astype(cast_dtype, errors='ignore').astype(cast_dtype, errors='ignore').groupby(col)
A:dask.dataframe.shuffle.df2.columns->df.astype(cast_dtype, errors='ignore').astype(cast_dtype, errors='ignore').drop('_partitions', axis=1).set_index('_index', drop=True).columns.astype(column_dtype)
A:dask.dataframe.shuffle.ddf_keys->list(dsk.values())
A:dask.dataframe.shuffle.lens->dask.dataframe.methods.tolist(lens)
A:dask.dataframe.shuffle.(mins, maxes, lens)->_compute_partition_stats(df.index, allow_overlap=True, **kwargs)
A:dask.dataframe.shuffle.(mins, maxes, _)->_compute_partition_stats(column, allow_overlap=False, **kwargs)
A:dask.dataframe.shuffle.result.divisions->tuple(divisions)
dask.dataframe.shuffle._calculate_divisions(df:DataFrame,partition_col:Series,repartition:bool,npartitions:int,upsample:float=1.0,partition_size:float=128000000.0,ascending:bool=True)->tuple[list, list, list, bool]
dask.dataframe.shuffle._compute_partition_stats(column:Series,allow_overlap:bool=False,**kwargs)->tuple[list, list, list[int]]
dask.dataframe.shuffle._noop(x,cleanup_token)
dask.dataframe.shuffle.barrier(args)
dask.dataframe.shuffle.cleanup_partd_files(p,keys)
dask.dataframe.shuffle.collect(p,part,meta,barrier_token)
dask.dataframe.shuffle.compute_and_set_divisions(df:DataFrame,**kwargs)->DataFrame
dask.dataframe.shuffle.compute_divisions(df:DataFrame,col:Any|None=None,**kwargs)->tuple
dask.dataframe.shuffle.drop_overlap(df,index)
dask.dataframe.shuffle.ensure_cleanup_on_exception(p)
dask.dataframe.shuffle.fix_overlap(ddf,mins,maxes,lens)
dask.dataframe.shuffle.get_overlap(df,index)
dask.dataframe.shuffle.maybe_buffered_partd(self,encode_cls=None,buffer=True,tempdir=None)
dask.dataframe.shuffle.maybe_buffered_partd.__init__(self,encode_cls=None,buffer=True,tempdir=None)
dask.dataframe.shuffle.maybe_buffered_partd.__reduce__(self)
dask.dataframe.shuffle.partitioning_index(df,npartitions,cast_dtype=None)
dask.dataframe.shuffle.rearrange_by_column(df,col,npartitions=None,max_branch=None,shuffle_method=None,compute=None,ignore_index=False)
dask.dataframe.shuffle.rearrange_by_column_disk(df,column,npartitions=None,compute=False)
dask.dataframe.shuffle.rearrange_by_column_tasks(df,column,max_branch=32,npartitions=None,ignore_index=False)
dask.dataframe.shuffle.rearrange_by_divisions(df,column,divisions,max_branch=None,shuffle_method=None,ascending=True,na_position='last',duplicates=True)
dask.dataframe.shuffle.set_index(df:DataFrame,index:str|Series,npartitions:int|Literal['auto']|None=None,shuffle_method:str|None=None,compute:bool=False,drop:bool=True,upsample:float=1.0,divisions:Sequence|None=None,partition_size:float=128000000.0,sort:bool=True,**kwargs)->DataFrame
dask.dataframe.shuffle.set_index_post_scalar(df,index_name,drop,column_dtype)
dask.dataframe.shuffle.set_index_post_series(df,index_name,drop,column_dtype)
dask.dataframe.shuffle.set_partition(df:DataFrame,index:str|Series,divisions:Sequence,max_branch:int=32,drop:bool=True,shuffle_method:str|None=None,compute:bool|None=None)->DataFrame
dask.dataframe.shuffle.set_partitions_pre(s,divisions,ascending=True,na_position='last')
dask.dataframe.shuffle.set_sorted_index(df:DataFrame,index:str|Series,drop:bool=True,divisions:Sequence|None=None,**kwargs)->DataFrame
dask.dataframe.shuffle.shuffle(df,index,shuffle_method=None,npartitions=None,max_branch=32,ignore_index=False,compute=None)
dask.dataframe.shuffle.shuffle_group(df,cols,stage,k,npartitions,ignore_index,nfinal)
dask.dataframe.shuffle.shuffle_group_2(df,cols,ignore_index,nparts)
dask.dataframe.shuffle.shuffle_group_3(df,col,npartitions,p)
dask.dataframe.shuffle.shuffle_group_get(g_head,i)
dask.dataframe.shuffle.sort_values(df:DataFrame,by:str|list[str],npartitions:int|Literal['auto']|None=None,shuffle_method:str|None=None,ascending:bool|list[bool]=True,na_position:Literal['first']|Literal['last']='last',upsample:float=1.0,partition_size:float=128000000.0,sort_function:Callable[[pd.DataFrame],pd.DataFrame]|None=None,sort_function_kwargs:Mapping[str,Any]|None=None)->DataFrame


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/core.py----------------------------------------
A:dask.dataframe.core.DEFAULT_GET->dask.base.named_schedulers.get('threads', named_schedulers['sync'])
A:dask.dataframe.core.self->self._get_numeric_data()._get_numeric_data()
A:dask.dataframe.core.numerics->self._get_numeric_data()._get_numeric_data()._meta._get_numeric_data()
A:dask.dataframe.core.dsk->dict()
A:dask.dataframe.core.self._parent_meta->pandas.Series(dtype='float64')
A:dask.dataframe.core.meta->make_meta(meta)
A:dask.dataframe.core.__dask_optimize__->globalmethod(optimize, key='dataframe_optimize', falsey=dont_optimize)
A:dask.dataframe.core.__dask_scheduler__->staticmethod(DEFAULT_GET)
A:dask.dataframe.core.name->funcname(func)
A:dask.dataframe.core.o->aca_combine(x, **kwargs)
A:dask.dataframe.core.graph->dask.highlevelgraph.HighLevelGraph.from_collections(final_prefix, dsk, dependencies=dependencies)
A:dask.dataframe.core.return_type->get_parallel_type(other)
A:dask.dataframe.core.other_meta->make_meta(other, parent_meta=self._parent_meta)
A:dask.dataframe.core.other_meta_nonempty->meta_nonempty(other_meta)
A:dask.dataframe.core.self.divisions->tuple(divisions)
A:dask.dataframe.core.result->new_dd_object(graph, _name, meta, divisions)
A:dask.dataframe.core.n->len(divisions)
A:dask.dataframe.core.self._meta.attrs->dict(value)
A:dask.dataframe.core.self._computed->self._get_numeric_data()._get_numeric_data().compute()
A:dask.dataframe.core.x->x.set_index('idx').set_index('idx')
A:dask.dataframe.core.out->_cov_corr_combine(data, corr)
A:dask.dataframe.core.divisions->list(base_series.divisions)
A:dask.dataframe.core.data->self._get_numeric_data()._get_numeric_data()._repr_data().to_html(max_rows=5, show_dimensions=False, notebook=True)
A:dask.dataframe.core.real_min->_split_partitions(df, nsplits, split_name).index.min()
A:dask.dataframe.core.real_max->_split_partitions(df, nsplits, split_name).index.max()
A:dask.dataframe.core.df->_split_partitions(df, nsplits, split_name)
A:dask.dataframe.core.shuffle_npartitions->max(df.npartitions // (split_every or df.npartitions), split_out)
A:dask.dataframe.core.deduplicated->_split_partitions(df, nsplits, split_name).map_partitions(chunk, token='drop-duplicates-chunk', meta=df._meta, ignore_index=ignore_index, enforce_metadata=False, transform_divisions=False, **kwargs).shuffle(kwargs.get('subset', None) or list(df.columns), ignore_index=ignore_index, npartitions=shuffle_npartitions, shuffle_method=shuffle_method).map_partitions(chunk, meta=df._meta, ignore_index=ignore_index, token='drop-duplicates-agg', transform_divisions=False, **kwargs)
A:dask.dataframe.core.shuffle->_determine_split_out_shuffle(shuffle, split_out)
A:dask.dataframe.core.state_data->random_state_data(self.npartitions, random_state)
A:dask.dataframe.core.token->tokenize(df, divisions)
A:dask.dataframe.core.out_df->type(self)(graph, name2, self._meta, self.divisions)
A:dask.dataframe.core.index->pseudorandom(len(df), p, random_state)
A:dask.dataframe.core.new_keys->numpy.array(self.__dask_keys__(), dtype=object)[index].tolist()
A:dask.dataframe.core.axis->self._get_numeric_data()._get_numeric_data()._validate_axis(axis)
A:dask.dataframe.core.parts->self._get_numeric_data()._get_numeric_data()._limit_fillna('bfill', limit=limit, skip_check=self.npartitions - 1, meta=meta)
A:dask.dataframe.core.random_state->numpy.random.RandomState(random_state)
A:dask.dataframe.core.lengths->tuple(self.map_partitions(len).compute())
A:dask.dataframe.core.chunks->self._get_numeric_data()._get_numeric_data()._validate_chunks(records, lengths)
A:dask.dataframe.core.keys->self._get_numeric_data()._get_numeric_data().__dask_keys__()
A:dask.dataframe.core.res->res.clear_divisions().clear_divisions()
A:dask.dataframe.core.res.divisions->tuple((str(division) + suffix for division in self.divisions))
A:dask.dataframe.core.numeric_only_kwargs->get_numeric_only_kwargs(numeric_only)
A:dask.dataframe.core.mode_series->value_count_series[value_count_series == max_val].index.to_series().sort_values().reset_index(drop=True)
A:dask.dataframe.core.num->self._get_numeric_data()._get_numeric_data()._repr_data().to_html(max_rows=5, show_dimensions=False, notebook=True)._get_numeric_data()
A:dask.dataframe.core.s->self._get_numeric_data()._get_numeric_data().get_partition(i).compute()
A:dask.dataframe.core.array_values->self._get_numeric_data()._get_numeric_data()._repr_data().to_html(max_rows=5, show_dimensions=False, notebook=True)._get_numeric_data().values.astype('f8')
A:dask.dataframe.core.array_var->var(column.values, axis=0, ddof=ddof, split_every=split_every)
A:dask.dataframe.core.meta_computation->self._get_numeric_data()._get_numeric_data()._repr_data().to_html(max_rows=5, show_dimensions=False, notebook=True)._get_numeric_data()._meta_nonempty.var(axis=0)
A:dask.dataframe.core.is_timedelta->is_timedelta64_dtype(column._meta)
A:dask.dataframe.core.is_nan->column.astype('f8').isna()
A:dask.dataframe.core.column->column.astype('f8').astype('f8')
A:dask.dataframe.core.numeric_kwargs->get_numeric_only_kwargs(numeric_only)
A:dask.dataframe.core.is_df_like->is_dataframe_like(self._meta)
A:dask.dataframe.core.(numeric_dd, needs_time_conversion)->self._get_numeric_data()._get_numeric_data()._convert_time_cols_to_numeric(time_cols, axis, meta, skipna)
A:dask.dataframe.core.needs_time_conversion->is_datetime64_any_dtype(self._meta)
A:dask.dataframe.core.numeric_dd->from_pandas(meta_frame_constructor(self)({'_': meta_series_constructor(self)([np.nan])}, index=self.index), npartitions=self.npartitions)
A:dask.dataframe.core.v->normalize_arg(v)
A:dask.dataframe.core.numeric_dd[col]->_convert_to_numeric(numeric_dd[col], skipna)
A:dask.dataframe.core.array_skew->dask.array.stats.skew(array_values, axis=0, bias=bias, nan_policy=nan_policy)
A:dask.dataframe.core.array_kurtosis->dask.array.stats.kurtosis(array_values, axis=0, fisher=fisher, bias=bias, nan_policy=nan_policy)
A:dask.dataframe.core.quantiles->tuple((quantile(num.iloc[:, i], q, method) for i in range(len(num.columns))))
A:dask.dataframe.core.output->self._get_numeric_data()._get_numeric_data()._describe_1d(self, split_every, percentiles, percentiles_method, datetime_is_numeric)
A:dask.dataframe.core.chosen_columns_indexes->self._get_numeric_data()._get_numeric_data()._get_columns_indexes_based_on_dtypes(data)
A:dask.dataframe.core.bools_and_times->self._get_numeric_data()._get_numeric_data()._meta.select_dtypes(include=_include)
A:dask.dataframe.core.percentiles->list(percentiles)
A:dask.dataframe.core.vcounts->self._get_numeric_data()._get_numeric_data()._repr_data().to_html(max_rows=5, show_dimensions=False, notebook=True).value_counts(split_every=split_every)
A:dask.dataframe.core.min_ts->to_numeric(data.dropna()).min(split_every=split_every)
A:dask.dataframe.core.max_ts->to_numeric(data.dropna()).max(split_every=split_every)
A:dask.dataframe.core.cumpart->map_partitions(chunk, self, token=name1, meta=self, **chunk_kwargs)
A:dask.dataframe.core.cumlast->map_partitions(_take_last, cumpart, skipna, meta=meta_series_constructor(self)([], dtype='float'), token=name2)
A:dask.dataframe.core.suffix->tokenize(self)
A:dask.dataframe.core.values->numpy.fromiter(values, dtype=object)
A:dask.dataframe.core.(meta1, meta2)->_emulate(M.align, self, other, join, axis=axis, fill_value=fill_value)
A:dask.dataframe.core.aligned->self._get_numeric_data()._get_numeric_data().map_partitions(M.align, other, join=join, axis=axis, fill_value=fill_value, enforce_metadata=False)
A:dask.dataframe.core.result1->new_dd_object(dsk1, name1, meta1, aligned.divisions)
A:dask.dataframe.core.result2->new_dd_object(dsk2, name2, meta2, aligned.divisions)
A:dask.dataframe.core.offset->pandas.tseries.frequencies.to_offset(freq)
A:dask.dataframe.core.end->self._get_numeric_data()._get_numeric_data().loc._get_partitions(date)
A:dask.dataframe.core.is_anchored->pandas.tseries.frequencies.to_offset(freq).is_anchored()
A:dask.dataframe.core.start->_split_partitions(df, nsplits, split_name).divisions[0].ceil(freq)
A:dask.dataframe.core._is_partition_type->staticmethod(is_dataframe_like)
A:dask.dataframe.core.renamed->_rename_dask(self, columns)
A:dask.dataframe.core.dt->CachedAccessor('dt', DatetimeAccessor)
A:dask.dataframe.core.cat->CachedAccessor('cat', CategoricalAccessor)
A:dask.dataframe.core.str->CachedAccessor('str', StringAccessor)
A:dask.dataframe.core.old->pandas.Series(range(self.npartitions + 1), index=self.divisions)
A:dask.dataframe.core.res._divisions->tuple(methods.tolist(new))
A:dask.dataframe.core.df.divisions->tuple(pd.Index(self.divisions).to_timestamp(how=how, freq=freq))
A:dask.dataframe.core.uniqs->self._get_numeric_data()._get_numeric_data().drop_duplicates(split_every=split_every)
A:dask.dataframe.core.op->partial(comparison, fill_value=fill_value)
A:dask.dataframe.core.applied->applied.clear_divisions().clear_divisions()
A:dask.dataframe.core.applied.divisions->tuple(pd.Series(self.divisions).map(arg, na_action=na_action))
A:dask.dataframe.core.repartition_npartitions->max(self.npartitions // (split_every or self.npartitions), split_out)
A:dask.dataframe.core.is_integer_slice->any((isinstance(i, Integral) for i in (key.start, key.step, key.stop)))
A:dask.dataframe.core.(self, key)->_maybe_align_partitions([self, key])
A:dask.dataframe.core.key->list(key)
A:dask.dataframe.core.col_size->len(self.columns)
A:dask.dataframe.core.row_size->delayed(int)(self.size / col_size)
A:dask.dataframe.core.cs->self._get_numeric_data()._get_numeric_data()._meta.select_dtypes(include=include, exclude=exclude)
A:dask.dataframe.core.indexer->self._get_numeric_data()._get_numeric_data()._get_columns_indexes_based_on_dtypes(cs)
A:dask.dataframe.core.kwargs[k]->from_dask_array(v, index=data.index, meta=data._meta)
A:dask.dataframe.core.df2->self._get_numeric_data()._get_numeric_data()._repr_data().to_html(max_rows=5, show_dimensions=False, notebook=True)._meta_nonempty.assign(**_extract_meta({k: kwargs[k]}, nonempty=True))
A:dask.dataframe.core.other->_recursive_pairwise_outer_join(other, on=on, lsuffix=lsuffix, rsuffix=rsuffix, npartitions=npartitions, shuffle_method=shuffle_method)
A:dask.dataframe.core.ddf->new_dd_object(graph, name, meta, divisions=(None, None))
A:dask.dataframe.core.computations->dict(zip(computations.keys(), da.compute(*computations.values())))
A:dask.dataframe.core.column_width->max(space, 7)
A:dask.dataframe.core.header->textwrap.dedent('             #   {{column:<{column_width}}} Non-Null Count  Dtype\n            ---  {{underl:<{column_width}}} --------------  -----').format(column_width=column_width).format(column='Column', underl='------')
A:dask.dataframe.core.column_template->textwrap.dedent('            {{i:^3}}  {{name:<{column_width}}} {{count}} non-null      {{dtype}}'.format(column_width=column_width))
A:dask.dataframe.core.memory_int->computations['memory_usage'].sum()
A:dask.dataframe.core.records->to_records(self)
A:dask.dataframe.core.series_df->pandas.concat([_repr_data_series(s, index=index) for (_, s) in meta.items()], axis=1)
A:dask.dataframe.core.selected_df->selected_df.assign(_index=self.index).assign(_index=self.index)
A:dask.dataframe.core.meth->getattr(pd.Series, name)
A:dask.dataframe.core.args->_maybe_align_partitions(args)
A:dask.dataframe.core.deps->dasks.copy()
A:dask.dataframe.core.a->a.rechunk({i + 1: d for (i, d) in enumerate(a.shape[1:])}).rechunk({i + 1: d for (i, d) in enumerate(a.shape[1:])})
A:dask.dataframe.core._is_broadcastable->partial(is_broadcastable, dfs)
A:dask.dataframe.core.dfs->shard_df_on_index(df, divisions[1:-1])
A:dask.dataframe.core.h->meta_frame_constructor(df)([], index=h).reset_index()
A:dask.dataframe.core.chunk_kwargs->dict()
A:dask.dataframe.core.aggregate_kwargs->dict()
A:dask.dataframe.core.combine_kwargs->dict()
A:dask.dataframe.core.npartitions->npartitions.pop().pop()
A:dask.dataframe.core.token_key->tokenize(base_series, map_series)
A:dask.dataframe.core.chunked->chunked.map_partitions(hash_shard, split_out, split_out_setup, split_out_setup_kwargs, ignore_index, token='split-%s' % token_key).map_partitions(hash_shard, split_out, split_out_setup, split_out_setup_kwargs, ignore_index, token='split-%s' % token_key)
A:dask.dataframe.core.layer->DataFrameTreeReduction(final_name, chunked.name, npartitions, partial(_concat, ignore_index=ignore_index), partial(combine, **combine_kwargs) if combine_kwargs else combine, finalize_func=partial(aggregate, **aggregate_kwargs) if aggregate_kwargs else aggregate, split_every=split_every, split_out=split_out if split_out and split_out > 1 else None, tree_node_name=f'{token or funcname(combine)}-combine-{token_key}')
A:dask.dataframe.core.meta_chunk->_emulate(chunk, *args, udf=True, **chunk_kwargs)
A:dask.dataframe.core.res[k]->_extract_meta(x[k], nonempty)
A:dask.dataframe.core.parent_meta->kwargs.pop('parent_meta', None)
A:dask.dataframe.core.arg->normalize_arg(arg)
A:dask.dataframe.core.(arg2, collections)->unpack_collections(arg)
A:dask.dataframe.core.(v, collections)->unpack_collections(v)
A:dask.dataframe.core.func->kwargs.pop('_func')
A:dask.dataframe.core.columns->pandas.Index(columns)
A:dask.dataframe.core.metadata->_rename(names, df._meta)
A:dask.dataframe.core.q_ndarray->numpy.array(q)
A:dask.dataframe.core.empty_index->pandas.Index([], dtype=float)
A:dask.dataframe.core.calc_qs->numpy.pad(qs, 1, mode='constant')
A:dask.dataframe.core.sums->numpy.nan_to_num(data['sum'])
A:dask.dataframe.core.counts->numpy.zeros_like(df.values, shape=shape)
A:dask.dataframe.core.mask->_split_partitions(df, nsplits, split_name).iloc[:, idx].notnull()
A:dask.dataframe.core.cov->numpy.full_like(sums, np.nan)
A:dask.dataframe.core.m->numpy.nansum(data['m'] + counts * (sums / counts_na - mu) ** 2, axis=0)
A:dask.dataframe.core.m[idx]->numpy.nansum(mu_discrepancy, axis=0)
A:dask.dataframe.core.data[k]->numpy.concatenate(data[k]).reshape((len(data[k]),) + data[k][0].shape)
A:dask.dataframe.core.cum_sums->numpy.cumsum(sums, 0)
A:dask.dataframe.core.cum_counts->numpy.cumsum(counts, 0)
A:dask.dataframe.core.nobs->numpy.where(cum_counts[-1], cum_counts[-1], np.nan)
A:dask.dataframe.core.counts_na->numpy.where(counts, counts, np.nan)
A:dask.dataframe.core.den->numpy.sqrt(m2 * m2.T)
A:dask.dataframe.core.p->list(p)
A:dask.dataframe.core.series_typ->type(a.iloc[0:1, 0])
A:dask.dataframe.core.d->dict()
A:dask.dataframe.core.last_elem->_is_single_last_div(c)
A:dask.dataframe.core.freq->pandas.tseries.frequencies.to_offset(freq)
A:dask.dataframe.core.new_offset_type->getattr(pd.tseries.offsets, new_offset)
A:dask.dataframe.core.(_, anchor)->pandas.tseries.frequencies.to_offset(freq).split('-')
A:dask.dataframe.core.size->int(size)
A:dask.dataframe.core.mem_usages->pandas.Series(split_mem_usages)
A:dask.dataframe.core.new_npartitions->list(map(len, iter_chunks(mem_usages, size)))
A:dask.dataframe.core.new_partitions_boundaries->list(new_partitions_boundaries)
A:dask.dataframe.core.mem_usage->mem_usage.sum().sum()
A:dask.dataframe.core.original_divisionsdivisions->pandas.Series(df.divisions).drop_duplicates()
A:dask.dataframe.core.(div, mod)->divmod(npartitions, df.npartitions)
A:dask.dataframe.core.idx->getattr(x, fn)(skipna=skipna, **numeric_only_kwargs)
A:dask.dataframe.core.value->x.set_index('idx').set_index('idx').value.infer_objects()
A:dask.dataframe.core.idxvalue->meta_series_constructor(x)([], dtype='i8')
A:dask.dataframe.core.value_count_series->_split_partitions(df, nsplits, split_name).sum()
A:dask.dataframe.core.max_val->_split_partitions(df, nsplits, split_name).sum().max(skipna=dropna)
A:dask.dataframe.core.r->dask.utils.M.head(df, n)
A:dask.dataframe.core.is_offset->isinstance(freq, pd.DateOffset)
A:dask.dataframe.core.divs->pandas.Series(range(len(df.divisions)), index=df.divisions)
A:dask.dataframe.core.meta.index->make_meta(meta).index.astype(arg.index.dtype)
A:dask.dataframe.core.dtype->str(s.dtype)
A:dask.dataframe.core.layer[(name, i) + suffix]->DataFrameTreeReduction(final_name, chunked.name, npartitions, partial(_concat, ignore_index=ignore_index), partial(combine, **combine_kwargs) if combine_kwargs else combine, finalize_func=partial(aggregate, **aggregate_kwargs) if aggregate_kwargs else aggregate, split_every=split_every, split_out=split_out if split_out and split_out > 1 else None, tree_node_name=f'{token or funcname(combine)}-combine-{token_key}').pop((name, i))
A:dask.dataframe.core.final_series->pseudorandom(len(df), p, random_state).to_series().map(final_series)
A:dask.dataframe.core.base_token_key->tokenize(base_series, split_out)
A:dask.dataframe.core.map_token_key->tokenize(map_series)
A:dask.dataframe.core.sqrt->sqrt.astype(dtype).astype(dtype)
A:dask.dataframe.core.time_col_mask->sqrt.astype(dtype).astype(dtype).index.isin(time_cols)
A:dask.dataframe.core.sqrt[time_col]->pandas.to_timedelta(matching_val)
dask.dataframe.DataFrame(self,dsk,name,meta,divisions)
dask.dataframe.DataFrame.__array_wrap__(self,array,context=None)
dask.dataframe.DataFrame.__contains__(self,key)
dask.dataframe.DataFrame.__delitem__(self,key)
dask.dataframe.DataFrame.__dir__(self)
dask.dataframe.DataFrame.__getattr__(self,key)
dask.dataframe.DataFrame.__getitem__(self,key)
dask.dataframe.DataFrame.__iter__(self)
dask.dataframe.DataFrame.__len__(self)
dask.dataframe.DataFrame.__setattr__(self,key,value)
dask.dataframe.DataFrame.__setitem__(self,key,value)
dask.dataframe.DataFrame._bind_comparison_method(cls,name,comparison,original=pd.DataFrame)
dask.dataframe.DataFrame._bind_operator_method(cls,name,op,original=pd.DataFrame)
dask.dataframe.DataFrame._get_numeric_data(self,how='any',subset=None)
dask.dataframe.DataFrame._ipython_key_completions_(self)
dask.dataframe.DataFrame._is_column_label_reference(self,key)
dask.dataframe.DataFrame._repr_data(self)
dask.dataframe.DataFrame._repr_html_(self)
dask.dataframe.DataFrame._select_columns_or_index(self,columns_or_index)
dask.dataframe.DataFrame._validate_axis(cls,axis=0,none_is_zero:bool=True)->None | Literal[0, 1]
dask.dataframe.DataFrame.apply(self,func,axis=0,broadcast=None,raw=False,reduce=None,args=(),meta=no_default,result_type=None,**kwds)
dask.dataframe.DataFrame.applymap(self,func,meta=no_default)
dask.dataframe.DataFrame.assign(self,**kwargs)
dask.dataframe.DataFrame.axes(self)
dask.dataframe.DataFrame.categorize(self,columns=None,index=None,split_every=None,**kwargs)
dask.dataframe.DataFrame.clip(self,lower=None,upper=None,out=None,axis=None)
dask.dataframe.DataFrame.columns(self)
dask.dataframe.DataFrame.columns(self,columns)
dask.dataframe.DataFrame.corr(self,method='pearson',min_periods=None,numeric_only=no_default,split_every=False)
dask.dataframe.DataFrame.cov(self,min_periods=None,numeric_only=no_default,split_every=False)
dask.dataframe.DataFrame.drop(self,labels=None,axis=0,columns=None,errors='raise')
dask.dataframe.DataFrame.dropna(self,how=no_default,subset=None,thresh=no_default)
dask.dataframe.DataFrame.dtypes(self)
dask.dataframe.DataFrame.empty(self)
dask.dataframe.DataFrame.eval(self,expr,inplace=None,**kwargs)
dask.dataframe.DataFrame.explode(self,column)
dask.dataframe.DataFrame.from_dict(cls,data,*,npartitions,orient='columns',dtype=None,columns=None)
dask.dataframe.DataFrame.groupby(self,by=None,group_keys=GROUP_KEYS_DEFAULT,sort=None,observed=None,dropna=None,**kwargs)
dask.dataframe.DataFrame.iloc(self)
dask.dataframe.DataFrame.info(self,buf=None,verbose=False,memory_usage=False)
dask.dataframe.DataFrame.items(self)
dask.dataframe.DataFrame.iterrows(self)
dask.dataframe.DataFrame.itertuples(self,index=True,name='Pandas')
dask.dataframe.DataFrame.join(self,other,on=None,how='left',lsuffix='',rsuffix='',npartitions=None,shuffle_method=None)
dask.dataframe.DataFrame.map(self,func,meta=no_default,na_action=None)
dask.dataframe.DataFrame.melt(self,id_vars=None,value_vars=None,var_name=None,value_name='value',col_level=None)
dask.dataframe.DataFrame.memory_usage(self,index=True,deep=False)
dask.dataframe.DataFrame.merge(self,right,how='inner',on=None,left_on=None,right_on=None,left_index=False,right_index=False,suffixes=('_x','_y'),indicator=False,npartitions=None,shuffle_method=None,broadcast=None)
dask.dataframe.DataFrame.mode(self,dropna=True,split_every=False,numeric_only=False)
dask.dataframe.DataFrame.ndim(self)
dask.dataframe.DataFrame.nlargest(self,n=5,columns=no_default,split_every=None)
dask.dataframe.DataFrame.nsmallest(self,n=5,columns=no_default,split_every=None)
dask.dataframe.DataFrame.nunique(self,split_every=False,dropna=True,axis=0)
dask.dataframe.DataFrame.pivot_table(self,index=None,columns=None,values=None,aggfunc='mean')
dask.dataframe.DataFrame.pop(self,item)
dask.dataframe.DataFrame.query(self,expr,**kwargs)
dask.dataframe.DataFrame.rename(self,index=None,columns=None)
dask.dataframe.DataFrame.round(self,decimals=0)
dask.dataframe.DataFrame.select_dtypes(self,include=None,exclude=None)
dask.dataframe.DataFrame.set_index(self,other:str|Series,drop:bool=True,sorted:bool=False,npartitions:int|Literal['auto']|None=None,divisions:Sequence|None=None,inplace:bool=False,sort:bool=True,**kwargs)
dask.dataframe.DataFrame.shape(self)
dask.dataframe.DataFrame.sort_values(self,by:str|list[str],npartitions:int|Literal['auto']|None=None,ascending:bool=True,na_position:Literal['first']|Literal['last']='last',sort_function:Callable[[pd.DataFrame],pd.DataFrame]|None=None,sort_function_kwargs:Mapping[str,Any]|None=None,**kwargs)->DataFrame
dask.dataframe.DataFrame.squeeze(self,axis=None)
dask.dataframe.DataFrame.to_bag(self,index=False,format='tuple')
dask.dataframe.DataFrame.to_html(self,max_rows=5)
dask.dataframe.DataFrame.to_orc(self,path,*args,**kwargs)
dask.dataframe.DataFrame.to_parquet(self,path,*args,**kwargs)
dask.dataframe.DataFrame.to_records(self,index=False,lengths=None)
dask.dataframe.DataFrame.to_string(self,max_rows=5)
dask.dataframe.DataFrame.to_timestamp(self,freq=None,how='start',axis=0)
dask.dataframe.Index(Series)
dask.dataframe.Index.__array_wrap__(self,array,context=None)
dask.dataframe.Index.__dir__(self)
dask.dataframe.Index.__getattr__(self,key)
dask.dataframe.Index.count(self,split_every=False)
dask.dataframe.Index.drop_duplicates(self,split_every=None,split_out=1,shuffle=None,**kwargs)
dask.dataframe.Index.head(self,n=5,compute=True)
dask.dataframe.Index.index(self)
dask.dataframe.Index.is_monotonic_decreasing(self)
dask.dataframe.Index.is_monotonic_increasing(self)
dask.dataframe.Index.map(self,arg,na_action=None,meta=no_default,is_monotonic=False)
dask.dataframe.Index.max(self,split_every=False)
dask.dataframe.Index.memory_usage(self,deep=False)
dask.dataframe.Index.min(self,split_every=False)
dask.dataframe.Index.shift(self,periods=1,freq=None)
dask.dataframe.Index.to_frame(self,index=True,name=None)
dask.dataframe.Index.to_series(self)
dask.dataframe.Series(_Frame)
dask.dataframe.Series.__array_wrap__(self,array,context=None)
dask.dataframe.Series.__contains__(self,key)
dask.dataframe.Series.__dir__(self)
dask.dataframe.Series.__divmod__(self,other)
dask.dataframe.Series.__getitem__(self,key)
dask.dataframe.Series.__iter__(self)
dask.dataframe.Series.__rdivmod__(self,other)
dask.dataframe.Series.__repr__(self)
dask.dataframe.Series._bind_comparison_method(cls,name,comparison,original=pd.Series)
dask.dataframe.Series._bind_operator_method(cls,name,op,original=pd.Series)
dask.dataframe.Series._get_numeric_data(self,how='any',subset=None)
dask.dataframe.Series._repartition_quantiles(self,npartitions,upsample=1.0)
dask.dataframe.Series._repr_data(self)
dask.dataframe.Series._validate_axis(cls,axis=0,none_is_zero:bool=True)->None | Literal[0, 1]
dask.dataframe.Series._view(self,dtype)
dask.dataframe.Series.align(self,other,join='outer',axis=None,fill_value=None)
dask.dataframe.Series.apply(self,func,convert_dtype=no_default,meta=no_default,args=(),**kwds)
dask.dataframe.Series.autocorr(self,lag=1,split_every=False)
dask.dataframe.Series.axes(self)
dask.dataframe.Series.between(self,left,right,inclusive='both')
dask.dataframe.Series.clip(self,lower=None,upper=None,out=None,axis=None)
dask.dataframe.Series.combine(self,other,func,fill_value=None)
dask.dataframe.Series.combine_first(self,other)
dask.dataframe.Series.corr(self,other,method='pearson',min_periods=None,split_every=False)
dask.dataframe.Series.count(self,split_every=False)
dask.dataframe.Series.cov(self,other,min_periods=None,split_every=False)
dask.dataframe.Series.dropna(self)
dask.dataframe.Series.dtype(self)
dask.dataframe.Series.explode(self)
dask.dataframe.Series.groupby(self,by=None,group_keys=GROUP_KEYS_DEFAULT,sort=None,observed=None,dropna=None,**kwargs)
dask.dataframe.Series.is_monotonic_decreasing(self)
dask.dataframe.Series.is_monotonic_increasing(self)
dask.dataframe.Series.isin(self,values)
dask.dataframe.Series.map(self,arg,na_action=None,meta=no_default)
dask.dataframe.Series.median(self,method='default')
dask.dataframe.Series.median_approximate(self,method='default')
dask.dataframe.Series.memory_usage(self,index=True,deep=False)
dask.dataframe.Series.mode(self,dropna=True,split_every=False)
dask.dataframe.Series.name(self)
dask.dataframe.Series.name(self,name)
dask.dataframe.Series.nbytes(self)
dask.dataframe.Series.ndim(self)
dask.dataframe.Series.nlargest(self,n=5,split_every=None)
dask.dataframe.Series.nsmallest(self,n=5,split_every=None)
dask.dataframe.Series.nunique(self,split_every=None,dropna=True)
dask.dataframe.Series.quantile(self,q=0.5,method='default')
dask.dataframe.Series.rename(self,index=None,inplace=no_default,sorted_index=False)
dask.dataframe.Series.round(self,decimals=0)
dask.dataframe.Series.shape(self)
dask.dataframe.Series.squeeze(self)
dask.dataframe.Series.to_bag(self,index=False,format='tuple')
dask.dataframe.Series.to_frame(self,name=None)
dask.dataframe.Series.to_string(self,max_rows=5)
dask.dataframe.Series.to_timestamp(self,freq=None,how='start',axis=0)
dask.dataframe.Series.unique(self,split_every=None,split_out=1)
dask.dataframe.Series.value_counts(self,sort=None,ascending=False,dropna=True,normalize=False,split_every=None,split_out=1)
dask.dataframe.Series.view(self,dtype)
dask.dataframe._Frame(self,dsk,name,meta,divisions)
dask.dataframe._Frame.__array__(self,dtype=None,**kwargs)
dask.dataframe._Frame.__array_ufunc__(self,numpy_ufunc,method,*inputs,**kwargs)
dask.dataframe._Frame.__array_wrap__(self,array,context=None)
dask.dataframe._Frame.__bool__(self)
dask.dataframe._Frame.__complex__(self)
dask.dataframe._Frame.__dask_graph__(self)->Graph
dask.dataframe._Frame.__dask_keys__(self)->NestedKeys
dask.dataframe._Frame.__dask_layers__(self)->Sequence[str]
dask.dataframe._Frame.__dask_postcompute__(self)
dask.dataframe._Frame.__dask_postpersist__(self)
dask.dataframe._Frame.__dask_tokenize__(self)
dask.dataframe._Frame.__float__(self)
dask.dataframe._Frame.__getstate__(self)
dask.dataframe._Frame.__int__(self)
dask.dataframe._Frame.__len__(self)
dask.dataframe._Frame.__repr__(self)
dask.dataframe._Frame.__setstate__(self,state)
dask.dataframe._Frame._args(self)
dask.dataframe._Frame._bind_operator_method(cls,name,op,original=pd.DataFrame)
dask.dataframe._Frame._constructor(self)
dask.dataframe._Frame._contains_index_name(self,columns_or_index)
dask.dataframe._Frame._convert_time_cols_to_numeric(self,time_cols,axis,meta,skipna)
dask.dataframe._Frame._cum_agg(self,op_name,chunk,aggregate,axis,skipna=True,chunk_kwargs=None,out=None)
dask.dataframe._Frame._describe_1d(self,data,split_every=False,percentiles=None,percentiles_method='default',datetime_is_numeric=False)
dask.dataframe._Frame._describe_nonnumeric_1d(self,data,split_every=False,datetime_is_numeric=False)
dask.dataframe._Frame._describe_numeric(self,data,split_every=False,percentiles=None,percentiles_method='default',is_timedelta_column=False,is_datetime_column=False)
dask.dataframe._Frame._drop_duplicates_shuffle(self,split_out,split_every,shuffle_method,ignore_index,**kwargs)
dask.dataframe._Frame._elemwise(self)
dask.dataframe._Frame._get_binary_operator(cls,op,inv=False)
dask.dataframe._Frame._get_columns_indexes_based_on_dtypes(self,subset)
dask.dataframe._Frame._get_unary_operator(cls,op)
dask.dataframe._Frame._head(self,n,npartitions,compute,safe)
dask.dataframe._Frame._is_index_level_reference(self,key)
dask.dataframe._Frame._kurtosis_1d(self,column,fisher=True,bias=True,nan_policy='propagate')
dask.dataframe._Frame._kurtosis_numeric(self,fisher=True,bias=True,nan_policy='propagate')
dask.dataframe._Frame._limit_fillna(self,method=None,*,limit=None,skip_check=None,meta=None)
dask.dataframe._Frame._meta_nonempty(self)
dask.dataframe._Frame._partitions(self,index)
dask.dataframe._Frame._rebuild(self,dsk,*,rename=None)
dask.dataframe._Frame._reduction_agg(self,name,axis=None,skipna=True,split_every=False,out=None,numeric_only=None,none_is_zero=True)
dask.dataframe._Frame._repr_data(self)
dask.dataframe._Frame._repr_divisions(self)
dask.dataframe._Frame._scalarfunc(self,cast_type)
dask.dataframe._Frame._skew_1d(self,column,bias=True,nan_policy='propagate')
dask.dataframe._Frame._skew_numeric(self,bias=True,nan_policy='propagate')
dask.dataframe._Frame._validate_chunks(self,arr,lengths)
dask.dataframe._Frame._validate_condition(self,cond)
dask.dataframe._Frame._var_1d(self,column,skipna=True,ddof=1,split_every=False)
dask.dataframe._Frame._var_numeric(self,skipna=True,ddof=1,split_every=False)
dask.dataframe._Frame.abs(self)
dask.dataframe._Frame.add_prefix(self,prefix)
dask.dataframe._Frame.add_suffix(self,suffix)
dask.dataframe._Frame.align(self,other,join='outer',axis=None,fill_value=None)
dask.dataframe._Frame.all(self,axis=None,skipna=True,split_every=False,out=None)
dask.dataframe._Frame.any(self,axis=None,skipna=True,split_every=False,out=None)
dask.dataframe._Frame.astype(self,dtype)
dask.dataframe._Frame.attrs(self)
dask.dataframe._Frame.attrs(self,value)
dask.dataframe._Frame.bfill(self,axis=None,limit=None)
dask.dataframe._Frame.clear_divisions(self)
dask.dataframe._Frame.combine(self,other,func,fill_value=None,overwrite=True)
dask.dataframe._Frame.combine_first(self,other)
dask.dataframe._Frame.compute_current_divisions(self,col=None)
dask.dataframe._Frame.copy(self,deep=False)
dask.dataframe._Frame.count(self,axis=None,split_every=False,numeric_only=False)
dask.dataframe._Frame.cummax(self,axis=None,skipna=True,out=None)
dask.dataframe._Frame.cummin(self,axis=None,skipna=True,out=None)
dask.dataframe._Frame.cumprod(self,axis=None,skipna=True,dtype=None,out=None)
dask.dataframe._Frame.cumsum(self,axis=None,skipna=True,dtype=None,out=None)
dask.dataframe._Frame.describe(self,split_every=False,percentiles=None,percentiles_method='default',include=None,exclude=None,datetime_is_numeric=no_default)
dask.dataframe._Frame.diff(self,periods=1,axis=0)
dask.dataframe._Frame.divisions(self)
dask.dataframe._Frame.divisions(self,value)
dask.dataframe._Frame.dot(self,other,meta=no_default)
dask.dataframe._Frame.drop_duplicates(self,subset=None,split_every=None,split_out=1,shuffle=None,ignore_index=False,**kwargs)
dask.dataframe._Frame.enforce_runtime_divisions(self)
dask.dataframe._Frame.ffill(self,axis=None,limit=None)
dask.dataframe._Frame.fillna(self,value=None,method=None,limit=None,axis=None)
dask.dataframe._Frame.first(self,offset)
dask.dataframe._Frame.get_partition(self,n)
dask.dataframe._Frame.head(self,n=5,npartitions=1,compute=True)
dask.dataframe._Frame.idxmax(self,axis=None,skipna=True,split_every=False,numeric_only=no_default)
dask.dataframe._Frame.idxmin(self,axis=None,skipna=True,split_every=False,numeric_only=no_default)
dask.dataframe._Frame.index(self)
dask.dataframe._Frame.index(self,value)
dask.dataframe._Frame.isin(self,values)
dask.dataframe._Frame.isna(self)
dask.dataframe._Frame.isnull(self)
dask.dataframe._Frame.known_divisions(self)
dask.dataframe._Frame.kurtosis(self,axis=0,fisher=True,bias=True,nan_policy='propagate',out=None,numeric_only=no_default)
dask.dataframe._Frame.last(self,offset)
dask.dataframe._Frame.loc(self)
dask.dataframe._Frame.map_overlap(self,func,before,after,*args,**kwargs)
dask.dataframe._Frame.map_partitions(self,func,*args,**kwargs)
dask.dataframe._Frame.mask(self,cond,other=np.nan)
dask.dataframe._Frame.max(self,axis=0,skipna=True,split_every=False,out=None,numeric_only=None)
dask.dataframe._Frame.mean(self,axis=0,skipna=True,split_every=False,dtype=None,out=None,numeric_only=None)
dask.dataframe._Frame.median(self,axis=None,method='default')
dask.dataframe._Frame.median_approximate(self,axis=None,method='default')
dask.dataframe._Frame.memory_usage_per_partition(self,index=True,deep=False)
dask.dataframe._Frame.min(self,axis=0,skipna=True,split_every=False,out=None,numeric_only=None)
dask.dataframe._Frame.mode(self,dropna=True,split_every=False)
dask.dataframe._Frame.notnull(self)
dask.dataframe._Frame.npartitions(self)->int
dask.dataframe._Frame.nunique_approx(self,split_every=None)
dask.dataframe._Frame.partitions(self)
dask.dataframe._Frame.pipe(self,func,*args,**kwargs)
dask.dataframe._Frame.prod(self,axis=None,skipna=True,split_every=False,dtype=None,out=None,min_count=None,numeric_only=None)
dask.dataframe._Frame.quantile(self,q=0.5,axis=0,numeric_only=no_default,method='default')
dask.dataframe._Frame.random_split(self,frac,random_state=None,shuffle=False)
dask.dataframe._Frame.reduction(self,chunk,aggregate=None,combine=None,meta=no_default,token=None,split_every=None,chunk_kwargs=None,aggregate_kwargs=None,combine_kwargs=None,**kwargs)
dask.dataframe._Frame.repartition(self,divisions=None,npartitions=None,partition_size=None,freq=None,force=False)
dask.dataframe._Frame.replace(self,to_replace=None,value=None,regex=False)
dask.dataframe._Frame.resample(self,rule,closed=None,label=None)
dask.dataframe._Frame.reset_index(self,drop=False)
dask.dataframe._Frame.rolling(self,window,min_periods=None,center=False,win_type=None,axis=no_default)
dask.dataframe._Frame.sample(self,n=None,frac=None,replace=False,random_state=None)
dask.dataframe._Frame.sem(self,axis=None,skipna=True,ddof=1,split_every=False,numeric_only=None)
dask.dataframe._Frame.shift(self,periods=1,freq=None,axis=0)
dask.dataframe._Frame.shuffle(self,on,npartitions=None,max_branch=None,shuffle_method=None,ignore_index=False,compute=None)
dask.dataframe._Frame.size(self)
dask.dataframe._Frame.skew(self,axis=0,bias=True,nan_policy='propagate',out=None,numeric_only=no_default)
dask.dataframe._Frame.std(self,axis=None,skipna=True,ddof=1,split_every=False,dtype=None,out=None,numeric_only=no_default)
dask.dataframe._Frame.sum(self,axis=None,skipna=True,split_every=False,dtype=None,out=None,min_count=None,numeric_only=None)
dask.dataframe._Frame.tail(self,n=5,compute=True)
dask.dataframe._Frame.to_backend(self,backend:str|None=None,**kwargs)
dask.dataframe._Frame.to_csv(self,filename,**kwargs)
dask.dataframe._Frame.to_dask_array(self,lengths=None,meta=None)
dask.dataframe._Frame.to_delayed(self,optimize_graph=True)
dask.dataframe._Frame.to_hdf(self,path_or_buf,key,mode='a',append=False,**kwargs)
dask.dataframe._Frame.to_json(self,filename,*args,**kwargs)
dask.dataframe._Frame.to_sql(self,name:str,uri:str,schema=None,if_exists:str='fail',index:bool=True,index_label=None,chunksize=None,dtype=None,method=None,compute=True,parallel=False,engine_kwargs=None)
dask.dataframe._Frame.values(self)
dask.dataframe._Frame.var(self,axis=None,skipna=True,ddof=1,split_every=False,dtype=None,out=None,numeric_only=no_default)
dask.dataframe._Frame.where(self,cond,other=np.nan)
dask.dataframe.core.DataFrame(self,dsk,name,meta,divisions)
dask.dataframe.core.DataFrame.__array_wrap__(self,array,context=None)
dask.dataframe.core.DataFrame.__contains__(self,key)
dask.dataframe.core.DataFrame.__delitem__(self,key)
dask.dataframe.core.DataFrame.__dir__(self)
dask.dataframe.core.DataFrame.__getattr__(self,key)
dask.dataframe.core.DataFrame.__getitem__(self,key)
dask.dataframe.core.DataFrame.__init__(self,dsk,name,meta,divisions)
dask.dataframe.core.DataFrame.__iter__(self)
dask.dataframe.core.DataFrame.__len__(self)
dask.dataframe.core.DataFrame.__setattr__(self,key,value)
dask.dataframe.core.DataFrame.__setitem__(self,key,value)
dask.dataframe.core.DataFrame._bind_comparison_method(cls,name,comparison,original=pd.DataFrame)
dask.dataframe.core.DataFrame._bind_operator_method(cls,name,op,original=pd.DataFrame)
dask.dataframe.core.DataFrame._get_numeric_data(self,how='any',subset=None)
dask.dataframe.core.DataFrame._ipython_key_completions_(self)
dask.dataframe.core.DataFrame._is_column_label_reference(self,key)
dask.dataframe.core.DataFrame._repr_data(self)
dask.dataframe.core.DataFrame._repr_html_(self)
dask.dataframe.core.DataFrame._select_columns_or_index(self,columns_or_index)
dask.dataframe.core.DataFrame._validate_axis(cls,axis=0,none_is_zero:bool=True)->None | Literal[0, 1]
dask.dataframe.core.DataFrame.apply(self,func,axis=0,broadcast=None,raw=False,reduce=None,args=(),meta=no_default,result_type=None,**kwds)
dask.dataframe.core.DataFrame.applymap(self,func,meta=no_default)
dask.dataframe.core.DataFrame.assign(self,**kwargs)
dask.dataframe.core.DataFrame.axes(self)
dask.dataframe.core.DataFrame.categorize(self,columns=None,index=None,split_every=None,**kwargs)
dask.dataframe.core.DataFrame.clip(self,lower=None,upper=None,out=None,axis=None)
dask.dataframe.core.DataFrame.columns(self)
dask.dataframe.core.DataFrame.columns(self,columns)
dask.dataframe.core.DataFrame.corr(self,method='pearson',min_periods=None,numeric_only=no_default,split_every=False)
dask.dataframe.core.DataFrame.cov(self,min_periods=None,numeric_only=no_default,split_every=False)
dask.dataframe.core.DataFrame.drop(self,labels=None,axis=0,columns=None,errors='raise')
dask.dataframe.core.DataFrame.dropna(self,how=no_default,subset=None,thresh=no_default)
dask.dataframe.core.DataFrame.dtypes(self)
dask.dataframe.core.DataFrame.empty(self)
dask.dataframe.core.DataFrame.eval(self,expr,inplace=None,**kwargs)
dask.dataframe.core.DataFrame.explode(self,column)
dask.dataframe.core.DataFrame.from_dict(cls,data,*,npartitions,orient='columns',dtype=None,columns=None)
dask.dataframe.core.DataFrame.groupby(self,by=None,group_keys=GROUP_KEYS_DEFAULT,sort=None,observed=None,dropna=None,**kwargs)
dask.dataframe.core.DataFrame.iloc(self)
dask.dataframe.core.DataFrame.info(self,buf=None,verbose=False,memory_usage=False)
dask.dataframe.core.DataFrame.items(self)
dask.dataframe.core.DataFrame.iterrows(self)
dask.dataframe.core.DataFrame.itertuples(self,index=True,name='Pandas')
dask.dataframe.core.DataFrame.join(self,other,on=None,how='left',lsuffix='',rsuffix='',npartitions=None,shuffle_method=None)
dask.dataframe.core.DataFrame.map(self,func,meta=no_default,na_action=None)
dask.dataframe.core.DataFrame.melt(self,id_vars=None,value_vars=None,var_name=None,value_name='value',col_level=None)
dask.dataframe.core.DataFrame.memory_usage(self,index=True,deep=False)
dask.dataframe.core.DataFrame.merge(self,right,how='inner',on=None,left_on=None,right_on=None,left_index=False,right_index=False,suffixes=('_x','_y'),indicator=False,npartitions=None,shuffle_method=None,broadcast=None)
dask.dataframe.core.DataFrame.mode(self,dropna=True,split_every=False,numeric_only=False)
dask.dataframe.core.DataFrame.ndim(self)
dask.dataframe.core.DataFrame.nlargest(self,n=5,columns=no_default,split_every=None)
dask.dataframe.core.DataFrame.nsmallest(self,n=5,columns=no_default,split_every=None)
dask.dataframe.core.DataFrame.nunique(self,split_every=False,dropna=True,axis=0)
dask.dataframe.core.DataFrame.pivot_table(self,index=None,columns=None,values=None,aggfunc='mean')
dask.dataframe.core.DataFrame.pop(self,item)
dask.dataframe.core.DataFrame.query(self,expr,**kwargs)
dask.dataframe.core.DataFrame.rename(self,index=None,columns=None)
dask.dataframe.core.DataFrame.round(self,decimals=0)
dask.dataframe.core.DataFrame.select_dtypes(self,include=None,exclude=None)
dask.dataframe.core.DataFrame.set_index(self,other:str|Series,drop:bool=True,sorted:bool=False,npartitions:int|Literal['auto']|None=None,divisions:Sequence|None=None,inplace:bool=False,sort:bool=True,**kwargs)
dask.dataframe.core.DataFrame.shape(self)
dask.dataframe.core.DataFrame.sort_values(self,by:str|list[str],npartitions:int|Literal['auto']|None=None,ascending:bool=True,na_position:Literal['first']|Literal['last']='last',sort_function:Callable[[pd.DataFrame],pd.DataFrame]|None=None,sort_function_kwargs:Mapping[str,Any]|None=None,**kwargs)->DataFrame
dask.dataframe.core.DataFrame.squeeze(self,axis=None)
dask.dataframe.core.DataFrame.to_bag(self,index=False,format='tuple')
dask.dataframe.core.DataFrame.to_html(self,max_rows=5)
dask.dataframe.core.DataFrame.to_orc(self,path,*args,**kwargs)
dask.dataframe.core.DataFrame.to_parquet(self,path,*args,**kwargs)
dask.dataframe.core.DataFrame.to_records(self,index=False,lengths=None)
dask.dataframe.core.DataFrame.to_string(self,max_rows=5)
dask.dataframe.core.DataFrame.to_timestamp(self,freq=None,how='start',axis=0)
dask.dataframe.core.Index(Series)
dask.dataframe.core.Index.__array_wrap__(self,array,context=None)
dask.dataframe.core.Index.__dir__(self)
dask.dataframe.core.Index.__getattr__(self,key)
dask.dataframe.core.Index.count(self,split_every=False)
dask.dataframe.core.Index.drop_duplicates(self,split_every=None,split_out=1,shuffle=None,**kwargs)
dask.dataframe.core.Index.head(self,n=5,compute=True)
dask.dataframe.core.Index.index(self)
dask.dataframe.core.Index.is_monotonic_decreasing(self)
dask.dataframe.core.Index.is_monotonic_increasing(self)
dask.dataframe.core.Index.map(self,arg,na_action=None,meta=no_default,is_monotonic=False)
dask.dataframe.core.Index.max(self,split_every=False)
dask.dataframe.core.Index.memory_usage(self,deep=False)
dask.dataframe.core.Index.min(self,split_every=False)
dask.dataframe.core.Index.shift(self,periods=1,freq=None)
dask.dataframe.core.Index.to_frame(self,index=True,name=None)
dask.dataframe.core.Index.to_series(self)
dask.dataframe.core.Scalar(self,dsk,name,meta,divisions=None)
dask.dataframe.core.Scalar.__array__(self)
dask.dataframe.core.Scalar.__bool__(self)
dask.dataframe.core.Scalar.__dask_graph__(self)->Graph
dask.dataframe.core.Scalar.__dask_keys__(self)->NestedKeys
dask.dataframe.core.Scalar.__dask_layers__(self)->Sequence[str]
dask.dataframe.core.Scalar.__dask_postcompute__(self)
dask.dataframe.core.Scalar.__dask_postpersist__(self)
dask.dataframe.core.Scalar.__dask_tokenize__(self)
dask.dataframe.core.Scalar.__dir__(self)
dask.dataframe.core.Scalar.__getstate__(self)
dask.dataframe.core.Scalar.__init__(self,dsk,name,meta,divisions=None)
dask.dataframe.core.Scalar.__repr__(self)
dask.dataframe.core.Scalar.__setstate__(self,state)
dask.dataframe.core.Scalar._args(self)
dask.dataframe.core.Scalar._get_binary_operator(cls,op,inv=False)
dask.dataframe.core.Scalar._get_unary_operator(cls,op)
dask.dataframe.core.Scalar._meta_nonempty(self)
dask.dataframe.core.Scalar._rebuild(self,dsk,*,rename=None)
dask.dataframe.core.Scalar.divisions(self)
dask.dataframe.core.Scalar.dtype(self)
dask.dataframe.core.Scalar.key(self)
dask.dataframe.core.Scalar.to_delayed(self,optimize_graph=True)
dask.dataframe.core.Series(_Frame)
dask.dataframe.core.Series.__array_wrap__(self,array,context=None)
dask.dataframe.core.Series.__contains__(self,key)
dask.dataframe.core.Series.__dir__(self)
dask.dataframe.core.Series.__divmod__(self,other)
dask.dataframe.core.Series.__getitem__(self,key)
dask.dataframe.core.Series.__iter__(self)
dask.dataframe.core.Series.__rdivmod__(self,other)
dask.dataframe.core.Series.__repr__(self)
dask.dataframe.core.Series._bind_comparison_method(cls,name,comparison,original=pd.Series)
dask.dataframe.core.Series._bind_operator_method(cls,name,op,original=pd.Series)
dask.dataframe.core.Series._get_numeric_data(self,how='any',subset=None)
dask.dataframe.core.Series._repartition_quantiles(self,npartitions,upsample=1.0)
dask.dataframe.core.Series._repr_data(self)
dask.dataframe.core.Series._validate_axis(cls,axis=0,none_is_zero:bool=True)->None | Literal[0, 1]
dask.dataframe.core.Series._view(self,dtype)
dask.dataframe.core.Series.align(self,other,join='outer',axis=None,fill_value=None)
dask.dataframe.core.Series.apply(self,func,convert_dtype=no_default,meta=no_default,args=(),**kwds)
dask.dataframe.core.Series.autocorr(self,lag=1,split_every=False)
dask.dataframe.core.Series.axes(self)
dask.dataframe.core.Series.between(self,left,right,inclusive='both')
dask.dataframe.core.Series.clip(self,lower=None,upper=None,out=None,axis=None)
dask.dataframe.core.Series.combine(self,other,func,fill_value=None)
dask.dataframe.core.Series.combine_first(self,other)
dask.dataframe.core.Series.corr(self,other,method='pearson',min_periods=None,split_every=False)
dask.dataframe.core.Series.count(self,split_every=False)
dask.dataframe.core.Series.cov(self,other,min_periods=None,split_every=False)
dask.dataframe.core.Series.dropna(self)
dask.dataframe.core.Series.dtype(self)
dask.dataframe.core.Series.explode(self)
dask.dataframe.core.Series.groupby(self,by=None,group_keys=GROUP_KEYS_DEFAULT,sort=None,observed=None,dropna=None,**kwargs)
dask.dataframe.core.Series.is_monotonic_decreasing(self)
dask.dataframe.core.Series.is_monotonic_increasing(self)
dask.dataframe.core.Series.isin(self,values)
dask.dataframe.core.Series.map(self,arg,na_action=None,meta=no_default)
dask.dataframe.core.Series.median(self,method='default')
dask.dataframe.core.Series.median_approximate(self,method='default')
dask.dataframe.core.Series.memory_usage(self,index=True,deep=False)
dask.dataframe.core.Series.mode(self,dropna=True,split_every=False)
dask.dataframe.core.Series.name(self)
dask.dataframe.core.Series.name(self,name)
dask.dataframe.core.Series.nbytes(self)
dask.dataframe.core.Series.ndim(self)
dask.dataframe.core.Series.nlargest(self,n=5,split_every=None)
dask.dataframe.core.Series.nsmallest(self,n=5,split_every=None)
dask.dataframe.core.Series.nunique(self,split_every=None,dropna=True)
dask.dataframe.core.Series.quantile(self,q=0.5,method='default')
dask.dataframe.core.Series.rename(self,index=None,inplace=no_default,sorted_index=False)
dask.dataframe.core.Series.round(self,decimals=0)
dask.dataframe.core.Series.shape(self)
dask.dataframe.core.Series.squeeze(self)
dask.dataframe.core.Series.to_bag(self,index=False,format='tuple')
dask.dataframe.core.Series.to_frame(self,name=None)
dask.dataframe.core.Series.to_string(self,max_rows=5)
dask.dataframe.core.Series.to_timestamp(self,freq=None,how='start',axis=0)
dask.dataframe.core.Series.unique(self,split_every=None,split_out=1)
dask.dataframe.core.Series.value_counts(self,sort=None,ascending=False,dropna=True,normalize=False,split_every=None,split_out=1)
dask.dataframe.core.Series.view(self,dtype)
dask.dataframe.core._Frame(self,dsk,name,meta,divisions)
dask.dataframe.core._Frame.__array__(self,dtype=None,**kwargs)
dask.dataframe.core._Frame.__array_ufunc__(self,numpy_ufunc,method,*inputs,**kwargs)
dask.dataframe.core._Frame.__array_wrap__(self,array,context=None)
dask.dataframe.core._Frame.__bool__(self)
dask.dataframe.core._Frame.__complex__(self)
dask.dataframe.core._Frame.__dask_graph__(self)->Graph
dask.dataframe.core._Frame.__dask_keys__(self)->NestedKeys
dask.dataframe.core._Frame.__dask_layers__(self)->Sequence[str]
dask.dataframe.core._Frame.__dask_postcompute__(self)
dask.dataframe.core._Frame.__dask_postpersist__(self)
dask.dataframe.core._Frame.__dask_tokenize__(self)
dask.dataframe.core._Frame.__float__(self)
dask.dataframe.core._Frame.__getstate__(self)
dask.dataframe.core._Frame.__init__(self,dsk,name,meta,divisions)
dask.dataframe.core._Frame.__int__(self)
dask.dataframe.core._Frame.__len__(self)
dask.dataframe.core._Frame.__repr__(self)
dask.dataframe.core._Frame.__setstate__(self,state)
dask.dataframe.core._Frame._args(self)
dask.dataframe.core._Frame._bind_operator_method(cls,name,op,original=pd.DataFrame)
dask.dataframe.core._Frame._constructor(self)
dask.dataframe.core._Frame._contains_index_name(self,columns_or_index)
dask.dataframe.core._Frame._convert_time_cols_to_numeric(self,time_cols,axis,meta,skipna)
dask.dataframe.core._Frame._cum_agg(self,op_name,chunk,aggregate,axis,skipna=True,chunk_kwargs=None,out=None)
dask.dataframe.core._Frame._describe_1d(self,data,split_every=False,percentiles=None,percentiles_method='default',datetime_is_numeric=False)
dask.dataframe.core._Frame._describe_nonnumeric_1d(self,data,split_every=False,datetime_is_numeric=False)
dask.dataframe.core._Frame._describe_numeric(self,data,split_every=False,percentiles=None,percentiles_method='default',is_timedelta_column=False,is_datetime_column=False)
dask.dataframe.core._Frame._drop_duplicates_shuffle(self,split_out,split_every,shuffle_method,ignore_index,**kwargs)
dask.dataframe.core._Frame._elemwise(self)
dask.dataframe.core._Frame._get_binary_operator(cls,op,inv=False)
dask.dataframe.core._Frame._get_columns_indexes_based_on_dtypes(self,subset)
dask.dataframe.core._Frame._get_unary_operator(cls,op)
dask.dataframe.core._Frame._head(self,n,npartitions,compute,safe)
dask.dataframe.core._Frame._is_index_level_reference(self,key)
dask.dataframe.core._Frame._kurtosis_1d(self,column,fisher=True,bias=True,nan_policy='propagate')
dask.dataframe.core._Frame._kurtosis_numeric(self,fisher=True,bias=True,nan_policy='propagate')
dask.dataframe.core._Frame._limit_fillna(self,method=None,*,limit=None,skip_check=None,meta=None)
dask.dataframe.core._Frame._meta_nonempty(self)
dask.dataframe.core._Frame._partitions(self,index)
dask.dataframe.core._Frame._rebuild(self,dsk,*,rename=None)
dask.dataframe.core._Frame._reduction_agg(self,name,axis=None,skipna=True,split_every=False,out=None,numeric_only=None,none_is_zero=True)
dask.dataframe.core._Frame._repr_data(self)
dask.dataframe.core._Frame._repr_divisions(self)
dask.dataframe.core._Frame._scalarfunc(self,cast_type)
dask.dataframe.core._Frame._skew_1d(self,column,bias=True,nan_policy='propagate')
dask.dataframe.core._Frame._skew_numeric(self,bias=True,nan_policy='propagate')
dask.dataframe.core._Frame._validate_chunks(self,arr,lengths)
dask.dataframe.core._Frame._validate_condition(self,cond)
dask.dataframe.core._Frame._var_1d(self,column,skipna=True,ddof=1,split_every=False)
dask.dataframe.core._Frame._var_numeric(self,skipna=True,ddof=1,split_every=False)
dask.dataframe.core._Frame.abs(self)
dask.dataframe.core._Frame.add_prefix(self,prefix)
dask.dataframe.core._Frame.add_suffix(self,suffix)
dask.dataframe.core._Frame.align(self,other,join='outer',axis=None,fill_value=None)
dask.dataframe.core._Frame.all(self,axis=None,skipna=True,split_every=False,out=None)
dask.dataframe.core._Frame.any(self,axis=None,skipna=True,split_every=False,out=None)
dask.dataframe.core._Frame.astype(self,dtype)
dask.dataframe.core._Frame.attrs(self)
dask.dataframe.core._Frame.attrs(self,value)
dask.dataframe.core._Frame.bfill(self,axis=None,limit=None)
dask.dataframe.core._Frame.clear_divisions(self)
dask.dataframe.core._Frame.combine(self,other,func,fill_value=None,overwrite=True)
dask.dataframe.core._Frame.combine_first(self,other)
dask.dataframe.core._Frame.compute_current_divisions(self,col=None)
dask.dataframe.core._Frame.copy(self,deep=False)
dask.dataframe.core._Frame.count(self,axis=None,split_every=False,numeric_only=False)
dask.dataframe.core._Frame.cummax(self,axis=None,skipna=True,out=None)
dask.dataframe.core._Frame.cummin(self,axis=None,skipna=True,out=None)
dask.dataframe.core._Frame.cumprod(self,axis=None,skipna=True,dtype=None,out=None)
dask.dataframe.core._Frame.cumsum(self,axis=None,skipna=True,dtype=None,out=None)
dask.dataframe.core._Frame.describe(self,split_every=False,percentiles=None,percentiles_method='default',include=None,exclude=None,datetime_is_numeric=no_default)
dask.dataframe.core._Frame.diff(self,periods=1,axis=0)
dask.dataframe.core._Frame.divisions(self)
dask.dataframe.core._Frame.divisions(self,value)
dask.dataframe.core._Frame.dot(self,other,meta=no_default)
dask.dataframe.core._Frame.drop_duplicates(self,subset=None,split_every=None,split_out=1,shuffle=None,ignore_index=False,**kwargs)
dask.dataframe.core._Frame.enforce_runtime_divisions(self)
dask.dataframe.core._Frame.ffill(self,axis=None,limit=None)
dask.dataframe.core._Frame.fillna(self,value=None,method=None,limit=None,axis=None)
dask.dataframe.core._Frame.first(self,offset)
dask.dataframe.core._Frame.get_partition(self,n)
dask.dataframe.core._Frame.head(self,n=5,npartitions=1,compute=True)
dask.dataframe.core._Frame.idxmax(self,axis=None,skipna=True,split_every=False,numeric_only=no_default)
dask.dataframe.core._Frame.idxmin(self,axis=None,skipna=True,split_every=False,numeric_only=no_default)
dask.dataframe.core._Frame.index(self)
dask.dataframe.core._Frame.index(self,value)
dask.dataframe.core._Frame.isin(self,values)
dask.dataframe.core._Frame.isna(self)
dask.dataframe.core._Frame.isnull(self)
dask.dataframe.core._Frame.known_divisions(self)
dask.dataframe.core._Frame.kurtosis(self,axis=0,fisher=True,bias=True,nan_policy='propagate',out=None,numeric_only=no_default)
dask.dataframe.core._Frame.last(self,offset)
dask.dataframe.core._Frame.loc(self)
dask.dataframe.core._Frame.map_overlap(self,func,before,after,*args,**kwargs)
dask.dataframe.core._Frame.map_partitions(self,func,*args,**kwargs)
dask.dataframe.core._Frame.mask(self,cond,other=np.nan)
dask.dataframe.core._Frame.max(self,axis=0,skipna=True,split_every=False,out=None,numeric_only=None)
dask.dataframe.core._Frame.mean(self,axis=0,skipna=True,split_every=False,dtype=None,out=None,numeric_only=None)
dask.dataframe.core._Frame.median(self,axis=None,method='default')
dask.dataframe.core._Frame.median_approximate(self,axis=None,method='default')
dask.dataframe.core._Frame.memory_usage_per_partition(self,index=True,deep=False)
dask.dataframe.core._Frame.min(self,axis=0,skipna=True,split_every=False,out=None,numeric_only=None)
dask.dataframe.core._Frame.mode(self,dropna=True,split_every=False)
dask.dataframe.core._Frame.notnull(self)
dask.dataframe.core._Frame.npartitions(self)->int
dask.dataframe.core._Frame.nunique_approx(self,split_every=None)
dask.dataframe.core._Frame.partitions(self)
dask.dataframe.core._Frame.pipe(self,func,*args,**kwargs)
dask.dataframe.core._Frame.prod(self,axis=None,skipna=True,split_every=False,dtype=None,out=None,min_count=None,numeric_only=None)
dask.dataframe.core._Frame.quantile(self,q=0.5,axis=0,numeric_only=no_default,method='default')
dask.dataframe.core._Frame.random_split(self,frac,random_state=None,shuffle=False)
dask.dataframe.core._Frame.reduction(self,chunk,aggregate=None,combine=None,meta=no_default,token=None,split_every=None,chunk_kwargs=None,aggregate_kwargs=None,combine_kwargs=None,**kwargs)
dask.dataframe.core._Frame.repartition(self,divisions=None,npartitions=None,partition_size=None,freq=None,force=False)
dask.dataframe.core._Frame.replace(self,to_replace=None,value=None,regex=False)
dask.dataframe.core._Frame.resample(self,rule,closed=None,label=None)
dask.dataframe.core._Frame.reset_index(self,drop=False)
dask.dataframe.core._Frame.rolling(self,window,min_periods=None,center=False,win_type=None,axis=no_default)
dask.dataframe.core._Frame.sample(self,n=None,frac=None,replace=False,random_state=None)
dask.dataframe.core._Frame.sem(self,axis=None,skipna=True,ddof=1,split_every=False,numeric_only=None)
dask.dataframe.core._Frame.shift(self,periods=1,freq=None,axis=0)
dask.dataframe.core._Frame.shuffle(self,on,npartitions=None,max_branch=None,shuffle_method=None,ignore_index=False,compute=None)
dask.dataframe.core._Frame.size(self)
dask.dataframe.core._Frame.skew(self,axis=0,bias=True,nan_policy='propagate',out=None,numeric_only=no_default)
dask.dataframe.core._Frame.std(self,axis=None,skipna=True,ddof=1,split_every=False,dtype=None,out=None,numeric_only=no_default)
dask.dataframe.core._Frame.sum(self,axis=None,skipna=True,split_every=False,dtype=None,out=None,min_count=None,numeric_only=None)
dask.dataframe.core._Frame.tail(self,n=5,compute=True)
dask.dataframe.core._Frame.to_backend(self,backend:str|None=None,**kwargs)
dask.dataframe.core._Frame.to_csv(self,filename,**kwargs)
dask.dataframe.core._Frame.to_dask_array(self,lengths=None,meta=None)
dask.dataframe.core._Frame.to_delayed(self,optimize_graph=True)
dask.dataframe.core._Frame.to_hdf(self,path_or_buf,key,mode='a',append=False,**kwargs)
dask.dataframe.core._Frame.to_json(self,filename,*args,**kwargs)
dask.dataframe.core._Frame.to_sql(self,name:str,uri:str,schema=None,if_exists:str='fail',index:bool=True,index_label=None,chunksize=None,dtype=None,method=None,compute=True,parallel=False,engine_kwargs=None)
dask.dataframe.core._Frame.values(self)
dask.dataframe.core._Frame.var(self,axis=None,skipna=True,ddof=1,split_every=False,dtype=None,out=None,numeric_only=no_default)
dask.dataframe.core._Frame.where(self,cond,other=np.nan)
dask.dataframe.core._concat(args,ignore_index=False)
dask.dataframe.core._convert_to_numeric(series,skipna)
dask.dataframe.core._count_aggregate(x)
dask.dataframe.core._cov_corr(df,min_periods=None,corr=False,scalar=False,numeric_only=no_default,split_every=False)
dask.dataframe.core._cov_corr_agg(data,cols,min_periods=2,corr=False,scalar=False,like_df=None)
dask.dataframe.core._cov_corr_chunk(df,corr=False)
dask.dataframe.core._cov_corr_combine(data_in,corr=False)
dask.dataframe.core._determine_split_out_shuffle(shuffle,split_out)
dask.dataframe.core._emulate(func,*args,udf=False,**kwargs)
dask.dataframe.core._extract_meta(x,nonempty=False)
dask.dataframe.core._get_divisions_map_partitions(align_dataframes,transform_divisions,dfs,func,args,kwargs)
dask.dataframe.core._get_meta_map_partitions(args,dfs,func,kwargs,meta,parent_meta)
dask.dataframe.core._getattr_numeric_only(*args,_dask_method_name,**kwargs)
dask.dataframe.core._map_freq_to_period_start(freq)
dask.dataframe.core._maybe_from_pandas(dfs)
dask.dataframe.core._mode_aggregate(df,dropna)
dask.dataframe.core._numeric_data(func)
dask.dataframe.core._numeric_only(func)
dask.dataframe.core._numeric_only_maybe_warn(df,numeric_only,default=None)
dask.dataframe.core._raise_if_not_series_or_dataframe(x,funcname)
dask.dataframe.core._raise_if_object_series(x,funcname)
dask.dataframe.core._reduction_aggregate(x,aca_aggregate=None,**kwargs)
dask.dataframe.core._reduction_chunk(x,aca_chunk=None,**kwargs)
dask.dataframe.core._reduction_combine(x,aca_combine=None,**kwargs)
dask.dataframe.core._rename(columns,df)
dask.dataframe.core._rename_dask(df,names)
dask.dataframe.core._repartition_from_boundaries(df,new_partitions_boundaries,new_name)
dask.dataframe.core._repr_data_series(s,index)
dask.dataframe.core._scalar_binary(op,self,other,inv=False)
dask.dataframe.core._split_partitions(df,nsplits,new_name)
dask.dataframe.core._sqrt_and_convert_to_timedelta(partition,axis,dtype=None,*args,**kwargs)
dask.dataframe.core._take_last(a,skipna=True)
dask.dataframe.core.apply_and_enforce(*args,**kwargs)
dask.dataframe.core.apply_concat_apply(args,chunk=None,aggregate=None,combine=None,meta=no_default,token=None,chunk_kwargs=None,aggregate_kwargs=None,combine_kwargs=None,split_every=None,split_out=None,split_out_setup=None,split_out_setup_kwargs=None,sort=None,ignore_index=False,**kwargs)
dask.dataframe.core.check_divisions(divisions)
dask.dataframe.core.elemwise(op,*args,meta=no_default,out=None,transform_divisions=True,**kwargs)
dask.dataframe.core.finalize(results)
dask.dataframe.core.handle_out(out,result)
dask.dataframe.core.has_parallel_type(x)
dask.dataframe.core.hash_shard(df,nparts,split_out_setup=None,split_out_setup_kwargs=None,ignore_index=False)
dask.dataframe.core.idxmaxmin_agg(x,fn=None,skipna=True,scalar=False,numeric_only=no_default)
dask.dataframe.core.idxmaxmin_chunk(x,fn=None,skipna=True,numeric_only=False)
dask.dataframe.core.idxmaxmin_combine(x,fn=None,skipna=True)
dask.dataframe.core.idxmaxmin_row(x,fn=None,skipna=True)
dask.dataframe.core.is_broadcastable(dfs,s)
dask.dataframe.core.map_partitions(func,*args,meta=no_default,enforce_metadata=True,transform_divisions=True,align_dataframes=True,**kwargs)
dask.dataframe.core.mapseries(base_chunk,concat_map)
dask.dataframe.core.mapseries_combine(index,concat_result)
dask.dataframe.core.maybe_shift_divisions(df,periods,freq)
dask.dataframe.core.meta_warning(df)
dask.dataframe.core.new_dd_object(dsk,name,meta,divisions,parent_meta=None)
dask.dataframe.core.partitionwise_graph(func,layer_name,*args,**kwargs)
dask.dataframe.core.pd_split(df,p,random_state=None,shuffle=False)
dask.dataframe.core.prefix_reduction(f,ddf,identity,**kwargs)
dask.dataframe.core.quantile(df,q,method='default')
dask.dataframe.core.repartition(df,divisions=None,force=False)
dask.dataframe.core.repartition_divisions(a,b,name,out1,out2,force=False)
dask.dataframe.core.repartition_freq(df,freq=None)
dask.dataframe.core.repartition_npartitions(df,npartitions)
dask.dataframe.core.repartition_size(df,size)
dask.dataframe.core.safe_head(df,n)
dask.dataframe.core.series_map(base_series,map_series)
dask.dataframe.core.split_evenly(df,k)
dask.dataframe.core.split_out_on_cols(df,cols=None)
dask.dataframe.core.split_out_on_index(df)
dask.dataframe.core.suffix_reduction(f,ddf,identity,**kwargs)
dask.dataframe.core.to_datetime(arg,meta=None,**kwargs)
dask.dataframe.core.to_timedelta(arg,unit=None,errors='raise')
dask.dataframe.core.total_mem_usage(df,index=True,deep=False)
dask.dataframe.map_partitions(func,*args,meta=no_default,enforce_metadata=True,transform_divisions=True,align_dataframes=True,**kwargs)
dask.dataframe.repartition(df,divisions=None,force=False)
dask.dataframe.repartition_divisions(a,b,name,out1,out2,force=False)
dask.dataframe.repartition_freq(df,freq=None)
dask.dataframe.repartition_npartitions(df,npartitions)
dask.dataframe.repartition_size(df,size)
dask.dataframe.to_datetime(arg,meta=None,**kwargs)
dask.dataframe.to_timedelta(arg,unit=None,errors='raise')


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/groupby.py----------------------------------------
A:dask.dataframe.groupby.columns->sorted((col for col in columns if col.startswith(prefix)))
A:dask.dataframe.groupby.by->dict(name=df.columns[0], levels=_determine_levels(by)).get('by', None)
A:dask.dataframe.groupby.g->SeriesGroupBy(self.obj, by=self.by, slice=key, sort=self.sort, **self.dropna, **self.observed)
A:dask.dataframe.groupby.df->df.to_frame('__series__').to_frame('__series__')
A:dask.dataframe.groupby.result->result['__series__'].rename(series_name)
A:dask.dataframe.groupby.grouped->sample.groupby(by_meta, group_keys=self.group_keys, **self.observed, **self.dropna)
A:dask.dataframe.groupby.numeric_only->dict(name=df.columns[0], levels=_determine_levels(by)).get('numeric_only', no_default)
A:dask.dataframe.groupby.key->list(key)
A:dask.dataframe.groupby.full_index->pandas.MultiIndex.from_product(result.index.levels, names=result.index.names)
A:dask.dataframe.groupby.empty->pandas.DataFrame(empty_data)
A:dask.dataframe.groupby.func->funcname(known_np_funcs.get(func, func))
A:dask.dataframe.groupby.numeric_only_kwargs->get_numeric_only_kwargs(numeric_only)
A:dask.dataframe.groupby.x->SeriesGroupBy(self.obj, by=self.by, slice=key, sort=self.sort, **self.dropna, **self.observed).sum(**numeric_only_kwargs)
A:dask.dataframe.groupby.n->g[x.columns].count().rename(columns=lambda c: f'{c}-count')
A:dask.dataframe.groupby.g2->_groupby_raise_unaligned(df, by=by, observed=observed, dropna=dropna)
A:dask.dataframe.groupby.x2->g[g.columns[nc // 3:2 * nc // 3]].rename(columns=lambda c: c[0])
A:dask.dataframe.groupby.nc->len(g.columns)
A:dask.dataframe.groupby.num_elements->len(list(it.product(cols, repeat=2)))
A:dask.dataframe.groupby.num_cols->len(cols)
A:dask.dataframe.groupby.vals->list(range(num_elements))
A:dask.dataframe.groupby.col_idx_mapping->dict(zip(cols, range(num_cols)))
A:dask.dataframe.groupby.sqrt_val->numpy.sqrt(std_val_i * std_val_j)
A:dask.dataframe.groupby.index->pandas.MultiIndex.from_product([level_1, level_1])
A:dask.dataframe.groupby._df->df.to_frame('__series__').to_frame('__series__').__class__()
A:dask.dataframe.groupby._df.index->numpy.zeros(len(_df), dtype=int)
A:dask.dataframe.groupby.dt_df->df.to_frame('__series__').to_frame('__series__').select_dtypes(include=['datetime', 'timedelta'])
A:dask.dataframe.groupby.df[col]->_convert_to_numeric(dt_df[col], True)
A:dask.dataframe.groupby.col_mapping->collections.OrderedDict()
A:dask.dataframe.groupby.col_mapping[c]->str(i)
A:dask.dataframe.groupby.is_mask->any((is_series_like(s) for s in self.by))
A:dask.dataframe.groupby.cols->set(chunked.columns)
A:dask.dataframe.groupby.mul->SeriesGroupBy(self.obj, by=self.by, slice=key, sort=self.sort, **self.dropna, **self.observed).apply(_mul_cols, cols=cols).reset_index(level=-1, drop=True)
A:dask.dataframe.groupby.t->list(_t)
A:dask.dataframe.groupby.total_sums->concat(sums).groupby(level=levels, sort=sort).sum()
A:dask.dataframe.groupby.total_muls->concat(muls).groupby(level=levels, sort=sort).sum()
A:dask.dataframe.groupby.total_counts->concat(counts).groupby(level=levels).sum()
A:dask.dataframe.groupby.idx_mapping->list()
A:dask.dataframe.groupby.idx_vals->list(inv_col_mapping.keys() - set(total_sums.columns))
A:dask.dataframe.groupby.idx_name->inv_col_mapping.get(val, val)
A:dask.dataframe.groupby.keys->list(col_mapping.keys())
A:dask.dataframe.groupby.result.columns->result['__series__'].rename(series_name).columns.set_levels(keys, level=level)
A:dask.dataframe.groupby.s_result->result['__series__'].rename(series_name).stack(dropna=False)
A:dask.dataframe.groupby.result.index->numpy.zeros(len(result), dtype=int)
A:dask.dataframe.groupby.name->dict(name=df.columns[0], levels=_determine_levels(by)).pop('name')
A:dask.dataframe.groupby.kwargs->dict(name=df.columns[0], levels=_determine_levels(by))
A:dask.dataframe.groupby.spec->_normalize_spec({None: arg}, [])
A:dask.dataframe.groupby.impls->_build_agg_args_var(result_column, func, func_args, func_kwargs, input_column)
A:dask.dataframe.groupby.chunks->sorted(chunks.values())
A:dask.dataframe.groupby.aggs->sorted(aggs.values())
A:dask.dataframe.groupby.intermediate->_make_agg_id('list', input_column)
A:dask.dataframe.groupby.int_sum->_make_agg_id('sum', input_column)
A:dask.dataframe.groupby.int_sum2->_make_agg_id('sum2', input_column)
A:dask.dataframe.groupby.int_count->_make_agg_id('count', input_column)
A:dask.dataframe.groupby.col->_make_agg_id(funcname(func), input_column)
A:dask.dataframe.groupby.funcs->dict(name=df.columns[0], levels=_determine_levels(by)).pop('funcs')
A:dask.dataframe.groupby.r->func(grouped, **func_kwargs)
A:dask.dataframe.groupby.result[result_column]->func(df, **finalize_kwargs)
A:dask.dataframe.groupby.ddof->dict(name=df.columns[0], levels=_determine_levels(by)).get('ddof', 1)
A:dask.dataframe.groupby.align->cum_last.reindex(part.set_index(index).index, fill_value=initial)
A:dask.dataframe.groupby.union->a.index.union(b.index)
A:dask.dataframe.groupby.projection->set(by_).union({self._slice} if np.isscalar(self._slice) or isinstance(self._slice, str) else self._slice)
A:dask.dataframe.groupby.self.by->_normalize_by(df, by)
A:dask.dataframe.groupby.partitions_aligned->all((item.npartitions == df.npartitions if isinstance(item, Series) else True for item in (self.by if isinstance(self.by, (tuple, list)) else [self.by])))
A:dask.dataframe.groupby.self._meta->self.obj._meta.groupby(by_meta, group_keys=group_keys, **self.observed, **self.dropna)
A:dask.dataframe.groupby.shuffle->_determine_split_out_shuffle(shuffle, split_out)
A:dask.dataframe.groupby.meta->self._meta_nonempty.apply(_drop_apply, by=self.by, what='bfill', limit=limit)
A:dask.dataframe.groupby.levels->dict(name=df.columns[0], levels=_determine_levels(by)).pop('index_levels')
A:dask.dataframe.groupby.suffix->str(uuid.uuid4())
A:dask.dataframe.groupby.self.obj->self.obj.assign(**{col + suffix: self.obj[col]})
A:dask.dataframe.groupby.cumpart_raw->map_partitions(_apply_chunk, self.obj, *by, chunk=chunk, columns=columns, token=name_part, meta=meta, **self.dropna)
A:dask.dataframe.groupby.cumpart_ext->cumpart_raw_frame.assign(**{i: self.obj[i] if np.isscalar(i) and i in getattr(self.obj, 'columns', []) else self.obj.index for i in by})
A:dask.dataframe.groupby.grouper->grouper_dispatch(self._meta.obj)
A:dask.dataframe.groupby.cumlast->map_partitions(_apply_chunk, cumpart_ext, *by_groupers, columns=0 if columns is None else columns, chunk=M.last, meta=meta, token=name_last, **self.dropna)
A:dask.dataframe.groupby._hash->tokenize(self, token, chunk, aggregate, initial)
A:dask.dataframe.groupby.graph->dask.highlevelgraph.HighLevelGraph.from_collections(name, dask, dependencies=dependencies)
A:dask.dataframe.groupby.df2->df.to_frame('__series__').to_frame('__series__').assign(_by=self.by)
A:dask.dataframe.groupby.df3->df3.reset_index().set_index(index_name or 'index').reset_index().set_index(index_name or 'index')
A:dask.dataframe.groupby.df4->df4['__series__'].rename(self.obj.name)
A:dask.dataframe.groupby.axis->self._normalize_axis(axis, 'fillna')
A:dask.dataframe.groupby.numeric_kwargs->get_numeric_only_kwargs(numeric_only)
A:dask.dataframe.groupby.chunk_kwargs->dict()
A:dask.dataframe.groupby.s->self.sum(split_every=split_every, split_out=split_out, shuffle=shuffle, numeric_only=numeric_only)
A:dask.dataframe.groupby.c->self.count(split_every=split_every, split_out=split_out, shuffle=shuffle)
A:dask.dataframe.groupby.v->self.var(ddof, split_every=split_every, split_out=split_out, numeric_only=numeric_only)
A:dask.dataframe.groupby.(relabeling, arg, columns, order)->reconstruct_func(arg, **kwargs)
A:dask.dataframe.groupby.(columns, arg)->validate_func_kwargs(kwargs)
A:dask.dataframe.groupby.group_columns->set()
A:dask.dataframe.groupby.column_projection->set().union(arg.keys()).intersection(self.obj.columns)
A:dask.dataframe.groupby.(chunk_funcs, aggregate_funcs, finalizers)->_build_agg_args(spec)
A:dask.dataframe.groupby.has_median->any((s[1] in ('median', np.median) for s in spec))
A:dask.dataframe.groupby.(meta_args, meta_kwargs)->_extract_meta((args, kwargs), nonempty=True)
A:dask.dataframe.groupby.(df2, by)->self._shuffle(meta)
A:dask.dataframe.groupby.df3.index->df3.reset_index().set_index(index_name or 'index').reset_index().set_index(index_name or 'index').index.rename(index_name)
A:dask.dataframe.groupby.meta_kwargs->_extract_meta(kwargs, nonempty=True)
A:dask.dataframe.groupby.numerics->self.obj._meta._get_numeric_data()
A:dask.dataframe.groupby.ret->type(series_gb.obj)(data, name=name)
A:dask.dataframe.groupby.ret.index->type(series_gb.obj)(data, name=name).index.astype(series_gb.obj.index.dtype, copy=False)
A:dask.dataframe.groupby.res->pandas.concat(data, names=series_gb.obj.index.names)
A:dask.dataframe.groupby.res.index->pandas.concat(data, names=series_gb.obj.index.names).index.set_levels(typed_levels.values(), level=typed_levels.keys(), verify_integrity=False)
A:dask.dataframe.groupby.aggregate_kwargs->dict()
A:dask.dataframe.groupby.npartitions->npartitions.pop().pop()
A:dask.dataframe.groupby.chunked->chunked.reset_index().reset_index()
A:dask.dataframe.groupby.shuffle_npartitions->max(chunked.npartitions // split_every, split_out)
A:dask.dataframe.groupby.index_cols->sorted(set(chunked.columns) - cols)
dask.dataframe.Aggregation(self,name,chunk,agg,finalize=None)
dask.dataframe.groupby.Aggregation(self,name,chunk,agg,finalize=None)
dask.dataframe.groupby.Aggregation.__init__(self,name,chunk,agg,finalize=None)
dask.dataframe.groupby.DataFrameGroupBy(_GroupBy)
dask.dataframe.groupby.DataFrameGroupBy.__dir__(self)
dask.dataframe.groupby.DataFrameGroupBy.__getattr__(self,key)
dask.dataframe.groupby.DataFrameGroupBy.__getitem__(self,key)
dask.dataframe.groupby.DataFrameGroupBy._all_numeric(self)
dask.dataframe.groupby.DataFrameGroupBy.agg(self,arg=None,split_every=None,split_out=1,shuffle=None,**kwargs)
dask.dataframe.groupby.DataFrameGroupBy.aggregate(self,arg=None,split_every=None,split_out=1,shuffle=None,**kwargs)
dask.dataframe.groupby.SeriesGroupBy(self,df,by=None,slice=None,observed=None,**kwargs)
dask.dataframe.groupby.SeriesGroupBy.__init__(self,df,by=None,slice=None,observed=None,**kwargs)
dask.dataframe.groupby.SeriesGroupBy.agg(self,arg=None,split_every=None,split_out=1,shuffle=None,**kwargs)
dask.dataframe.groupby.SeriesGroupBy.aggregate(self,arg=None,split_every=None,split_out=1,shuffle=None,**kwargs)
dask.dataframe.groupby.SeriesGroupBy.head(self,n=5,split_every=None,split_out=1,shuffle=None)
dask.dataframe.groupby.SeriesGroupBy.nunique(self,split_every=None,split_out=1)
dask.dataframe.groupby.SeriesGroupBy.tail(self,n=5,split_every=None,split_out=1,shuffle=None)
dask.dataframe.groupby.SeriesGroupBy.unique(self,split_every=None,split_out=1,shuffle=None)
dask.dataframe.groupby.SeriesGroupBy.value_counts(self,split_every=None,split_out=1,shuffle=None)
dask.dataframe.groupby._GroupBy(self,df,by=None,slice=None,group_keys=GROUP_KEYS_DEFAULT,dropna=None,sort=True,observed=None)
dask.dataframe.groupby._GroupBy.__init__(self,df,by=None,slice=None,group_keys=GROUP_KEYS_DEFAULT,dropna=None,sort=True,observed=None)
dask.dataframe.groupby._GroupBy.__iter__(self)
dask.dataframe.groupby._GroupBy._cum_agg(self,token,chunk,aggregate,initial,numeric_only=no_default)
dask.dataframe.groupby._GroupBy._groupby_kwargs(self)
dask.dataframe.groupby._GroupBy._meta_nonempty(self)
dask.dataframe.groupby._GroupBy._normalize_axis(self,axis,method:str)
dask.dataframe.groupby._GroupBy._shuffle(self,meta)
dask.dataframe.groupby._GroupBy._single_agg(self,token,func,aggfunc=None,meta=None,split_every=None,split_out=1,shuffle=None,chunk_kwargs=None,aggregate_kwargs=None,columns=None)
dask.dataframe.groupby._GroupBy.aggregate(self,arg=None,split_every=None,split_out=1,shuffle=None,**kwargs)
dask.dataframe.groupby._GroupBy.apply(self,func,*args,**kwargs)
dask.dataframe.groupby._GroupBy.bfill(self,limit=None)
dask.dataframe.groupby._GroupBy.compute(self,**kwargs)
dask.dataframe.groupby._GroupBy.corr(self,ddof=1,split_every=None,split_out=1,numeric_only=no_default)
dask.dataframe.groupby._GroupBy.count(self,split_every=None,split_out=1,shuffle=None)
dask.dataframe.groupby._GroupBy.cov(self,ddof=1,split_every=None,split_out=1,std=False,numeric_only=no_default)
dask.dataframe.groupby._GroupBy.cumcount(self,axis=no_default)
dask.dataframe.groupby._GroupBy.cumprod(self,axis=no_default,numeric_only=no_default)
dask.dataframe.groupby._GroupBy.cumsum(self,axis=no_default,numeric_only=no_default)
dask.dataframe.groupby._GroupBy.ffill(self,limit=None)
dask.dataframe.groupby._GroupBy.fillna(self,value=None,method=None,limit=None,axis=no_default)
dask.dataframe.groupby._GroupBy.first(self,split_every=None,split_out=1,shuffle=None,numeric_only=no_default)
dask.dataframe.groupby._GroupBy.get_group(self,key)
dask.dataframe.groupby._GroupBy.idxmax(self,split_every=None,split_out=1,shuffle=None,axis=no_default,skipna=True,numeric_only=no_default)
dask.dataframe.groupby._GroupBy.idxmin(self,split_every=None,split_out=1,shuffle=None,axis=no_default,skipna=True,numeric_only=no_default)
dask.dataframe.groupby._GroupBy.index(self)
dask.dataframe.groupby._GroupBy.index(self,value)
dask.dataframe.groupby._GroupBy.last(self,split_every=None,split_out=1,shuffle=None,numeric_only=no_default)
dask.dataframe.groupby._GroupBy.max(self,split_every=None,split_out=1,shuffle=None,numeric_only=no_default)
dask.dataframe.groupby._GroupBy.mean(self,split_every=None,split_out=1,shuffle=None,numeric_only=no_default)
dask.dataframe.groupby._GroupBy.median(self,split_every=None,split_out=1,shuffle=None,numeric_only=no_default)
dask.dataframe.groupby._GroupBy.min(self,split_every=None,split_out=1,shuffle=None,numeric_only=no_default)
dask.dataframe.groupby._GroupBy.prod(self,split_every=None,split_out=1,shuffle=None,min_count=None,numeric_only=no_default)
dask.dataframe.groupby._GroupBy.rolling(self,window,min_periods=None,center=False,win_type=None,axis=0)
dask.dataframe.groupby._GroupBy.shift(self,periods=1,freq=no_default,axis=no_default,fill_value=no_default,meta=no_default)
dask.dataframe.groupby._GroupBy.size(self,split_every=None,split_out=1,shuffle=None)
dask.dataframe.groupby._GroupBy.std(self,ddof=1,split_every=None,split_out=1,numeric_only=no_default)
dask.dataframe.groupby._GroupBy.sum(self,split_every=None,split_out=1,shuffle=None,min_count=None,numeric_only=no_default)
dask.dataframe.groupby._GroupBy.transform(self,func,*args,**kwargs)
dask.dataframe.groupby._GroupBy.var(self,ddof=1,split_every=None,split_out=1,numeric_only=no_default)
dask.dataframe.groupby._agg_finalize(df,aggregate_funcs,finalize_funcs,level,sort=False,**kwargs)
dask.dataframe.groupby._aggregate_docstring(based_on=None)
dask.dataframe.groupby._apply_chunk(df,*by,dropna=None,observed=None,**kwargs)
dask.dataframe.groupby._apply_func_to_column(df_like,column,func)
dask.dataframe.groupby._apply_func_to_columns(df_like,prefix,func)
dask.dataframe.groupby._build_agg_args(spec)
dask.dataframe.groupby._build_agg_args_custom(result_column,func,input_column)
dask.dataframe.groupby._build_agg_args_list(result_column,func,input_column)
dask.dataframe.groupby._build_agg_args_mean(result_column,func,input_column)
dask.dataframe.groupby._build_agg_args_simple(result_column,func,input_column,impl_pair)
dask.dataframe.groupby._build_agg_args_single(result_column,func,func_args,func_kwargs,input_column)
dask.dataframe.groupby._build_agg_args_std(result_column,func,func_args,func_kwargs,input_column)
dask.dataframe.groupby._build_agg_args_var(result_column,func,func_args,func_kwargs,input_column)
dask.dataframe.groupby._compute_sum_of_squares(grouped,column)
dask.dataframe.groupby._cov_agg(_t,levels,ddof,std=False,sort=False)
dask.dataframe.groupby._cov_chunk(df,*by,numeric_only=no_default)
dask.dataframe.groupby._cov_combine(g,levels)
dask.dataframe.groupby._cov_finalizer(df,cols,std=False)
dask.dataframe.groupby._cum_agg_aligned(part,cum_last,index,columns,func,initial)
dask.dataframe.groupby._cum_agg_filled(a,b,func,initial)
dask.dataframe.groupby._cumcount_aggregate(a,b,fill_value=None)
dask.dataframe.groupby._determine_levels(by)
dask.dataframe.groupby._drop_apply(group,*,by,what,**kwargs)
dask.dataframe.groupby._drop_duplicates_reindex(df)
dask.dataframe.groupby._finalize_mean(df,sum_column,count_column)
dask.dataframe.groupby._finalize_std(df,count_column,sum_column,sum2_column,**kwargs)
dask.dataframe.groupby._finalize_var(df,count_column,sum_column,sum2_column,**kwargs)
dask.dataframe.groupby._groupby_aggregate(df,aggfunc=None,levels=None,dropna=None,sort=False,observed=None,**kwargs)
dask.dataframe.groupby._groupby_aggregate_spec(df,spec,levels=None,dropna=None,sort=False,observed=None,**kwargs)
dask.dataframe.groupby._groupby_apply_funcs(df,*by,**kwargs)
dask.dataframe.groupby._groupby_get_group(df,by_key,get_key,columns)
dask.dataframe.groupby._groupby_raise_unaligned(df,**kwargs)
dask.dataframe.groupby._groupby_slice_apply(df,grouper,key,func,*args,group_keys=GROUP_KEYS_DEFAULT,dropna=None,observed=None,**kwargs)
dask.dataframe.groupby._groupby_slice_shift(df,grouper,key,shuffled,group_keys=GROUP_KEYS_DEFAULT,dropna=None,observed=None,**kwargs)
dask.dataframe.groupby._groupby_slice_transform(df,grouper,key,func,*args,group_keys=GROUP_KEYS_DEFAULT,dropna=None,observed=None,**kwargs)
dask.dataframe.groupby._head_aggregate(series_gb,**kwargs)
dask.dataframe.groupby._head_chunk(series_gb,**kwargs)
dask.dataframe.groupby._is_aligned(df,by)
dask.dataframe.groupby._make_agg_id(func,column)
dask.dataframe.groupby._maybe_slice(grouped,columns)
dask.dataframe.groupby._median_aggregate(series_gb,**kwargs)
dask.dataframe.groupby._mul_cols(df,cols)
dask.dataframe.groupby._non_agg_chunk(df,*by,key,dropna=None,observed=None,**kwargs)
dask.dataframe.groupby._normalize_by(df,by)
dask.dataframe.groupby._normalize_spec(spec,non_group_columns)
dask.dataframe.groupby._nunique_df_aggregate(df,levels,name,sort=False)
dask.dataframe.groupby._nunique_df_chunk(df,*by,**kwargs)
dask.dataframe.groupby._nunique_df_combine(df,levels,sort=False)
dask.dataframe.groupby._nunique_series_chunk(df,*by,**_ignored_)
dask.dataframe.groupby._shuffle_aggregate(args,chunk=None,aggregate=None,token=None,chunk_kwargs=None,aggregate_kwargs=None,split_every=None,split_out=1,sort=True,ignore_index=False,shuffle_method='tasks')
dask.dataframe.groupby._tail_aggregate(series_gb,**kwargs)
dask.dataframe.groupby._tail_chunk(series_gb,**kwargs)
dask.dataframe.groupby._unique_aggregate(series_gb,name=None)
dask.dataframe.groupby._value_counts(x,**kwargs)
dask.dataframe.groupby._value_counts_aggregate(series_gb)
dask.dataframe.groupby._var_agg(g,levels,ddof,sort=False,numeric_only=no_default,observed=False,dropna=True)
dask.dataframe.groupby._var_chunk(df,*by,numeric_only=no_default,observed=False,dropna=True)
dask.dataframe.groupby._var_combine(g,levels,sort=False)
dask.dataframe.groupby.numeric_only_deprecate_default(func)
dask.dataframe.groupby.numeric_only_not_implemented(func)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/utils.py----------------------------------------
A:dask.dataframe.utils.dtype->getattr(t, 'dtype', t)
A:dask.dataframe.utils.divisions->list(divisions)
A:dask.dataframe.utils.df->df.sort_index().sort_index()
A:dask.dataframe.utils.index->index.as_ordered().as_ordered()
A:dask.dataframe.utils.indices->index.as_ordered().as_ordered().searchsorted(divisions)
A:dask.dataframe.utils.T->TypeVar('T', bound=Callable)
A:dask.dataframe.utils.body->textwrap.wrap(_META_DESCRIPTION, initial_indent=indent, subsequent_indent=indent, width=78)
A:dask.dataframe.utils.descr->'{}\n{}'.format(_META_TYPES, '\n'.join(body))
A:dask.dataframe.utils.f.__doc__->'{}{}{}\n{}{}\n\n{}'.format(first, parameter_header, parameters, indent[4:], descr, rest)
A:dask.dataframe.utils.(first, last)->re.split('Parameters\\n[ ]*----------', f.__doc__)
A:dask.dataframe.utils.(parameters, rest)->last.split('\n\n', 1)
A:dask.dataframe.utils.(exc_type, exc_value, exc_traceback)->sys.exc_info()
A:dask.dataframe.utils.tb->''.join(traceback.format_tb(exc_traceback))
A:dask.dataframe.utils.msg->msg.format(f' in `{funcname}`' if funcname else '', repr(e), tb).format(f' in `{funcname}`' if funcname else '', repr(e), tb)
A:dask.dataframe.utils.x->x.set_categories([UNKNOWN_CATEGORIES]).set_categories([UNKNOWN_CATEGORIES])
A:dask.dataframe.utils.x[c]->x[c].cat.set_categories([UNKNOWN_CATEGORIES]).cat.set_categories([UNKNOWN_CATEGORIES])
A:dask.dataframe.utils.x.index->x.set_categories([UNKNOWN_CATEGORIES]).set_categories([UNKNOWN_CATEGORIES]).index.set_categories([UNKNOWN_CATEGORIES])
A:dask.dataframe.utils.s.index->make_meta(index)
A:dask.dataframe.utils.errmsg->'Partition type: `{}`\n{}'.format(typename(type(meta)), asciitable(['', 'dtype'], [('Found', x.dtype), ('Expected', meta.dtype)]))
A:dask.dataframe.utils.dtypes->pandas.concat([x.dtypes, meta.dtypes], axis=1, sort=True)
A:dask.dataframe.utils.extra->dask.dataframe.methods.tolist(actual.columns.difference(meta.columns))
A:dask.dataframe.utils.missing->dask.dataframe.methods.tolist(meta.columns.difference(actual.columns))
A:dask.dataframe.utils.n->len(idx)
A:dask.dataframe.utils.graph->dsk.__dask_graph__()
A:dask.dataframe.utils.result->dsk.compute(scheduler=scheduler)
A:dask.dataframe.utils.a->a.reset_index(drop=True).reset_index(drop=True)
A:dask.dataframe.utils.b->numpy.dtype(type(res))
A:dask.dataframe.utils.(a, b)->_maybe_convert_string(a, b)
A:dask.dataframe.utils.at->type(np.asarray(a.divisions).tolist()[0])
A:dask.dataframe.utils.bt->type(np.asarray(b.divisions).tolist()[0])
A:dask.dataframe.utils.get->get_scheduler(scheduler=scheduler, collections=[type(ddf)])
A:dask.dataframe.utils.results->get(ddf.dask, ddf.__dask_keys__())
A:dask.dataframe.utils.(dependencies, dependents)->get_deps(x.dask)
A:dask.dataframe.utils.df2->df.sort_index().sort_index().copy(deep=False)
A:dask.dataframe.utils.convert_string->dask.config.get('dataframe.convert-string')
dask.dataframe.assert_eq(a,b,check_names=True,check_dtype=True,check_divisions=True,check_index=True,sort_results=True,scheduler='sync',**kwargs)
dask.dataframe.assert_eq_dtypes(a,b)
dask.dataframe.utils.AttributeNotImplementedError(NotImplementedError,AttributeError)
dask.dataframe.utils._check_dask(dsk,check_names=True,check_dtypes=True,result=None,scheduler=None)
dask.dataframe.utils._empty_series(name,dtype,index=None)
dask.dataframe.utils._maybe_convert_string(a,b)
dask.dataframe.utils._maybe_sort(a,check_index:bool)
dask.dataframe.utils._nonempty_scalar(x)
dask.dataframe.utils._scalar_from_dtype(dtype)
dask.dataframe.utils.assert_dask_dtypes(ddf,res,numeric_equal=True)
dask.dataframe.utils.assert_dask_graph(dask,label)
dask.dataframe.utils.assert_divisions(ddf,scheduler=None)
dask.dataframe.utils.assert_eq(a,b,check_names=True,check_dtype=True,check_divisions=True,check_index=True,sort_results=True,scheduler='sync',**kwargs)
dask.dataframe.utils.assert_eq_dtypes(a,b)
dask.dataframe.utils.assert_max_deps(x,n,eq=True)
dask.dataframe.utils.assert_sane_keynames(ddf)
dask.dataframe.utils.check_matching_columns(meta,actual)
dask.dataframe.utils.check_meta(x,meta,funcname=None,numeric_equal=True)
dask.dataframe.utils.check_numeric_only_valid(numeric_only:bool|NoDefault,name:str)->dict
dask.dataframe.utils.clear_known_categories(x,cols=None,index=True,dtype_backend=None)
dask.dataframe.utils.drop_by_shallow_copy(df,columns,errors='raise')
dask.dataframe.utils.get_numeric_only_kwargs(numeric_only:bool|NoDefault)->dict
dask.dataframe.utils.get_string_dtype()
dask.dataframe.utils.has_known_categories(x)
dask.dataframe.utils.index_summary(idx,name=None)
dask.dataframe.utils.insert_meta_param_description(*args,**kwargs)
dask.dataframe.utils.is_float_na_dtype(t)
dask.dataframe.utils.is_integer_na_dtype(t)
dask.dataframe.utils.meta_frame_constructor(like)
dask.dataframe.utils.meta_series_constructor(like)
dask.dataframe.utils.pyarrow_strings_enabled()->bool
dask.dataframe.utils.raise_on_meta_error(funcname=None,udf=False)
dask.dataframe.utils.shard_df_on_index(df,divisions)
dask.dataframe.utils.strip_unknown_categories(x,just_drop_unknown=False)
dask.dataframe.utils.valid_divisions(divisions)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/hyperloglog.py----------------------------------------
A:dask.dataframe.hyperloglog.bits->bits.cumsum(axis=1).astype(bool).cumsum(axis=1).astype(bool)
A:dask.dataframe.hyperloglog.hashes->hashes.astype(np.uint32).astype(np.uint32)
A:dask.dataframe.hyperloglog.first_bit->compute_first_bit(hashes)
A:dask.dataframe.hyperloglog.df->pandas.DataFrame({'j': j, 'first_bit': first_bit})
A:dask.dataframe.hyperloglog.Ms->Ms.reshape(len(Ms) // m, m).reshape(len(Ms) // m, m)
A:dask.dataframe.hyperloglog.M->reduce_state(Ms, b)
A:dask.dataframe.hyperloglog.V->(M == 0).sum()
dask.dataframe.hyperloglog.compute_first_bit(a)
dask.dataframe.hyperloglog.compute_hll_array(obj,b)
dask.dataframe.hyperloglog.estimate_count(Ms,b)
dask.dataframe.hyperloglog.reduce_state(Ms,b)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/_compat.py----------------------------------------
A:dask.dataframe._compat.PANDAS_VERSION->Version(pd.__version__)
A:dask.dataframe._compat.left_na->pandas.isna(left)
A:dask.dataframe._compat.right_na->pandas.isna(right)
A:dask.dataframe._compat.data->numpy.where(data > 1, np.nan, data)
A:dask.dataframe._compat.data.index->makeDateIndex()
A:dask.dataframe._compat.df->pandas.DataFrame({'A': [0.0, 1, 2, 3, 4], 'B': [0.0, 1, 0, 1, 0], 'C': [f'foo{i}' for i in range(5)], 'D': pd.date_range('2009-01-01', periods=5)})
dask.dataframe._compat.assert_categorical_equal(left,right,*args,**kwargs)
dask.dataframe._compat.assert_numpy_array_equal(left,right)
dask.dataframe._compat.check_apply_dataframe_deprecation()
dask.dataframe._compat.check_applymap_dataframe_deprecation()
dask.dataframe._compat.check_axis_keyword_deprecation()
dask.dataframe._compat.check_convert_dtype_deprecation()
dask.dataframe._compat.check_groupby_axis_deprecation()
dask.dataframe._compat.check_nuisance_columns_warning()
dask.dataframe._compat.check_numeric_only_deprecation(name=None,show_nuisance_warning:bool=False)
dask.dataframe._compat.check_observed_deprecation()
dask.dataframe._compat.check_reductions_runtime_warning()
dask.dataframe._compat.check_to_pydatetime_deprecation(catch_deprecation_warnings:bool)
dask.dataframe._compat.is_any_real_numeric_dtype(arr_or_dtype)->bool
dask.dataframe._compat.is_string_dtype(arr_or_dtype)->bool
dask.dataframe._compat.makeDataFrame()
dask.dataframe._compat.makeDateIndex(k=30,freq='B')
dask.dataframe._compat.makeMissingDataframe()
dask.dataframe._compat.makeMixedDataFrame()
dask.dataframe._compat.makeTimeDataFrame()
dask.dataframe._compat.makeTimeSeries()
dask.dataframe._compat.makeTimedeltaIndex(k=30,freq='D')


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/_pyarrow_compat.py----------------------------------------
A:dask.dataframe._pyarrow_compat.array->pyarrow.chunked_array(chunks)
dask.dataframe._pyarrow_compat.rebuild_arrowextensionarray(type_,chunks)
dask.dataframe._pyarrow_compat.reduce_arrowextensionarray(x)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/reshape.py----------------------------------------
A:dask.dataframe.reshape.columns_contents->pandas.CategoricalIndex(df[columns].cat.categories, name=columns)
A:dask.dataframe.reshape.new_columns->pandas.MultiIndex.from_product((sorted(values), columns_contents), names=[None, columns])
A:dask.dataframe.reshape.meta->pandas.DataFrame(columns=new_columns, dtype=np.float64, index=pd.Index(df._meta[index]))
A:dask.dataframe.reshape.meta[value_col]->meta[value_col].astype(df[values].dtypes[value_col]).astype(df[values].dtypes[value_col])
A:dask.dataframe.reshape.pv_sum->apply_concat_apply([df], chunk=methods.pivot_sum, aggregate=methods.pivot_agg, meta=meta, token='pivot_table_sum', chunk_kwargs=kwargs)
A:dask.dataframe.reshape.pv_count->apply_concat_apply([df], chunk=methods.pivot_count, aggregate=methods.pivot_agg, meta=meta, token='pivot_table_count', chunk_kwargs=kwargs)
dask.dataframe.get_dummies(data,prefix=None,prefix_sep='_',dummy_na=False,columns=None,sparse=False,drop_first=False,dtype=_get_dummies_dtype_default,**kwargs)
dask.dataframe.melt(frame,id_vars=None,value_vars=None,var_name=None,value_name='value',col_level=None)
dask.dataframe.pivot_table(df,index=None,columns=None,values=None,aggfunc='mean')
dask.dataframe.reshape.get_dummies(data,prefix=None,prefix_sep='_',dummy_na=False,columns=None,sparse=False,drop_first=False,dtype=_get_dummies_dtype_default,**kwargs)
dask.dataframe.reshape.melt(frame,id_vars=None,value_vars=None,var_name=None,value_name='value',col_level=None)
dask.dataframe.reshape.pivot_table(df,index=None,columns=None,values=None,aggfunc='mean')


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/_dtypes.py----------------------------------------
A:dask.dataframe._dtypes.data->dtype.empty(2)
dask.dataframe._dtypes._(dtype)
dask.dataframe._dtypes._(dtype)
dask.dataframe._dtypes._(dtype)
dask.dataframe._dtypes._(x)
dask.dataframe._dtypes._(x)
dask.dataframe._dtypes._(x)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/multi.py----------------------------------------
A:dask.dataframe.multi._is_broadcastable->partial(is_broadcastable, args)
A:dask.dataframe.multi.divisions->tuple(divisions[min(present):max(present) + 2])
A:dask.dataframe.multi.result->result.map_partitions(M.rename_axis, ixname).map_partitions(M.rename_axis, ixname)
A:dask.dataframe.multi.L->list()
A:dask.dataframe.multi.dfs2->iter(align_partitions(*dfs)[0])
A:dask.dataframe.multi.parts->tuple(parts[min(present):max(present) + 1])
A:dask.dataframe.multi.left_index->dict(how=how, left_on=left_on, right_on=right_on, left_index=left_index, right_index=right_index, suffixes=suffixes, indicator=indicator).get('left_index', False)
A:dask.dataframe.multi.right_index->dict(how=how, left_on=left_on, right_on=right_on, left_index=left_index, right_index=right_index, suffixes=suffixes, indicator=indicator).get('right_index', False)
A:dask.dataframe.multi.lhs.index->left.set_index(left_on, sorted=True).astype(dtype)
A:dask.dataframe.multi.lhs->lhs.repartition(npartitions=npartitions).repartition(npartitions=npartitions)
A:dask.dataframe.multi.rhs.index->right.set_index(right_on, drop=left_on == right_on, sorted=True).astype(dtype)
A:dask.dataframe.multi.rhs->rhs.repartition(npartitions=npartitions).repartition(npartitions=npartitions)
A:dask.dataframe.multi.out->lhs.repartition(npartitions=npartitions).repartition(npartitions=npartitions).merge(rhs, *args, **kwargs)
A:dask.dataframe.multi.out.index->lhs.repartition(npartitions=npartitions).repartition(npartitions=npartitions).merge(rhs, *args, **kwargs).index.astype(empty_index_dtype)
A:dask.dataframe.multi.how->dict(how=how, left_on=left_on, right_on=right_on, left_index=left_index, right_index=right_index, suffixes=suffixes, indicator=indicator).get('how', 'left')
A:dask.dataframe.multi.((lhs, rhs), divisions, parts)->align_partitions(lhs, rhs)
A:dask.dataframe.multi.(divisions, parts)->require(divisions, parts, required[how])
A:dask.dataframe.multi.meta->lhs.repartition(npartitions=npartitions).repartition(npartitions=npartitions)._meta_nonempty.merge(rhs._meta_nonempty, **merge_kwargs)
A:dask.dataframe.multi.dsk->dict()
A:dask.dataframe.multi.graph->dask.highlevelgraph.HighLevelGraph.from_collections(name, broadcast_join_layer, dependencies=[lhs_dep, rhs_dep])
A:dask.dataframe.multi.shuffle_method->get_default_shuffle_method()
A:dask.dataframe.multi.npartitions->max(lhs.npartitions, rhs.npartitions)
A:dask.dataframe.multi.lhs2->shuffle_func(lhs, left_on, shuffle_method='tasks')
A:dask.dataframe.multi.rhs2->shuffle_func(rhs, right_on, shuffle_method='tasks')
A:dask.dataframe.multi.kwargs->dict(how=how, left_on=left_on, right_on=right_on, left_index=left_index, right_index=right_index, suffixes=suffixes, indicator=indicator)
A:dask.dataframe.multi.joined->map_partitions(merge_chunk, left, right, meta=meta, enforce_metadata=False, transform_divisions=False, align_dataframes=False, **kwargs)
A:dask.dataframe.multi.meta.index->lhs.repartition(npartitions=npartitions).repartition(npartitions=npartitions)._meta_nonempty.merge(rhs._meta_nonempty, **merge_kwargs).index.astype('int64')
A:dask.dataframe.multi.joined.divisions->tuple(divisions)
A:dask.dataframe.multi.col_tb->asciitable(('Merge columns', 'left dtype', 'right dtype'), dtype_mism)
A:dask.dataframe.multi.left->left.set_index(left_on, sorted=True).set_index(left_on, sorted=True)
A:dask.dataframe.multi.right->right.set_index(right_on, drop=left_on == right_on, sorted=True).set_index(right_on, drop=left_on == right_on, sorted=True)
A:dask.dataframe.multi.broadcast_bias->float(broadcast)
A:dask.dataframe.multi.n_small->min(left.npartitions, right.npartitions)
A:dask.dataframe.multi.n_big->max(left.npartitions, right.npartitions)
A:dask.dataframe.multi.partition->max(0, min(m - 1, j))
A:dask.dataframe.multi.frame->pandas.concat(frames)
A:dask.dataframe.multi.order->new_columns.take(order)
A:dask.dataframe.multi.tails->compute_tails(right, by=kwargs['right_by'])
A:dask.dataframe.multi.heads->compute_heads(right, by=kwargs['right_by'])
A:dask.dataframe.multi.(dfs2, divisions, parts)->align_partitions(*dfs)
A:dask.dataframe.multi.empty->strip_unknown_categories(meta)
A:dask.dataframe.multi.shared_columns->_concat(dfs, False).columns.intersection(meta.columns)
A:dask.dataframe.multi.df->_concat(dfs, False)
A:dask.dataframe.multi.df[needs_astype]->df[needs_astype].astype(meta[needs_astype].dtypes).astype(meta[needs_astype].dtypes)
A:dask.dataframe.multi.axis->dask.dataframe.core.DataFrame._validate_axis(axis)
A:dask.dataframe.multi.dfs->_maybe_from_pandas(dfs)
A:dask.dataframe.multi.selected_df->selected_df.assign(_index=df.index).assign(_index=df.index)
A:dask.dataframe.multi.on->_select_columns_or_index(df, on)
A:dask.dataframe.multi.nset->set(on)
A:dask.dataframe.multi.ind->hash_object_dispatch(df[on].astype(dtypes), index=False)
A:dask.dataframe.multi.partitions->partitioning_index(on, nsplits, cast_dtype=dtypes)
A:dask.dataframe.multi.df2->_concat(dfs, False).assign(_partitions=partitions)
A:dask.dataframe.multi.merge_kwargs->dict(how=how, left_on=left_on, right_on=right_on, left_index=left_index, right_index=right_index, suffixes=suffixes, indicator=indicator)
A:dask.dataframe.multi._index_names->set(lhs._meta_nonempty.index.names)
A:dask.dataframe.multi.token->tokenize(lhs, rhs, npartitions, **merge_kwargs)
A:dask.dataframe.multi.broadcast_join_layer->BroadcastJoinLayer(name, npartitions, lhs_name, lhs.npartitions, rhs_name, rhs.npartitions, parts_out=parts_out, **merge_kwargs)
A:dask.dataframe.multi.number_of_dataframes_to_merge->len(dataframes_to_merge)
A:dask.dataframe.multi.merged_ddf->_recursive_pairwise_outer_join([_recursive_pairwise_outer_join(dataframes_to_merge[:middle_index], **merge_options), _recursive_pairwise_outer_join(dataframes_to_merge[middle_index:], **merge_options)], **merge_options)
dask.dataframe.concat(dfs,axis=0,join='outer',interleave_partitions=False,ignore_unknown_divisions=False,ignore_order=False,**kwargs)
dask.dataframe.concat_and_check(dfs,ignore_order=False)
dask.dataframe.concat_indexed_dataframes(dfs,axis=0,join='outer',ignore_order=False,**kwargs)
dask.dataframe.concat_unindexed_dataframes(dfs,ignore_order=False,**kwargs)
dask.dataframe.merge(left,right,how='inner',on=None,left_on=None,right_on=None,left_index=False,right_index=False,suffixes=('_x','_y'),indicator=False,npartitions=None,shuffle_method=None,max_branch=None,broadcast=None)
dask.dataframe.merge_asof(left,right,on=None,left_on=None,right_on=None,left_index=False,right_index=False,by=None,left_by=None,right_by=None,suffixes=('_x','_y'),tolerance=None,allow_exact_matches=True,direction='backward')
dask.dataframe.merge_asof_indexed(left,right,**kwargs)
dask.dataframe.merge_asof_padded(left,right,prev=None,next=None,**kwargs)
dask.dataframe.merge_chunk(lhs,*args,result_meta,**kwargs)
dask.dataframe.merge_indexed_dataframes(lhs,rhs,left_index=True,right_index=True,**kwargs)
dask.dataframe.multi._concat_wrapper(dfs)
dask.dataframe.multi._contains_index_name(df,columns_or_index)
dask.dataframe.multi._maybe_align_partitions(args)
dask.dataframe.multi._merge_chunk_wrapper(*args,**kwargs)
dask.dataframe.multi._recursive_pairwise_outer_join(dataframes_to_merge,on,lsuffix,rsuffix,npartitions,shuffle_method)
dask.dataframe.multi._select_columns_or_index(df,columns_or_index)
dask.dataframe.multi._split_partition(df,on,nsplits)
dask.dataframe.multi.align_partitions(*dfs)
dask.dataframe.multi.broadcast_join(lhs,left_on,rhs,right_on,how='inner',npartitions=None,suffixes=('_x','_y'),shuffle_method=None,indicator=False,parts_out=None)
dask.dataframe.multi.compute_heads(ddf,by=None)
dask.dataframe.multi.compute_tails(ddf,by=None)
dask.dataframe.multi.concat(dfs,axis=0,join='outer',interleave_partitions=False,ignore_unknown_divisions=False,ignore_order=False,**kwargs)
dask.dataframe.multi.concat_and_check(dfs,ignore_order=False)
dask.dataframe.multi.concat_indexed_dataframes(dfs,axis=0,join='outer',ignore_order=False,**kwargs)
dask.dataframe.multi.concat_unindexed_dataframes(dfs,ignore_order=False,**kwargs)
dask.dataframe.multi.get_unsorted_columns(frames)
dask.dataframe.multi.hash_join(lhs,left_on,rhs,right_on,how='inner',npartitions=None,suffixes=('_x','_y'),shuffle_method=None,indicator=False,max_branch=None)
dask.dataframe.multi.merge(left,right,how='inner',on=None,left_on=None,right_on=None,left_index=False,right_index=False,suffixes=('_x','_y'),indicator=False,npartitions=None,shuffle_method=None,max_branch=None,broadcast=None)
dask.dataframe.multi.merge_asof(left,right,on=None,left_on=None,right_on=None,left_index=False,right_index=False,by=None,left_by=None,right_by=None,suffixes=('_x','_y'),tolerance=None,allow_exact_matches=True,direction='backward')
dask.dataframe.multi.merge_asof_indexed(left,right,**kwargs)
dask.dataframe.multi.merge_asof_padded(left,right,prev=None,next=None,**kwargs)
dask.dataframe.multi.merge_chunk(lhs,*args,result_meta,**kwargs)
dask.dataframe.multi.merge_indexed_dataframes(lhs,rhs,left_index=True,right_index=True,**kwargs)
dask.dataframe.multi.most_recent_head(left,right)
dask.dataframe.multi.most_recent_head_summary(left,right,by=None)
dask.dataframe.multi.most_recent_tail(left,right)
dask.dataframe.multi.most_recent_tail_summary(left,right,by=None)
dask.dataframe.multi.pair_partitions(L,R)
dask.dataframe.multi.require(divisions,parts,required=None)
dask.dataframe.multi.single_partition_join(left,right,**kwargs)
dask.dataframe.multi.stack_partitions(dfs,divisions,join='outer',ignore_order=False,**kwargs)
dask.dataframe.multi.warn_dtype_mismatch(left,right,left_on,right_on)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/rolling.py----------------------------------------
A:dask.dataframe.rolling.CombinedOutput->type('CombinedOutput', (tuple,), {})
A:dask.dataframe.rolling.combined->dask.dataframe.methods.concat(parts)
A:dask.dataframe.rolling.out->func(*args, **kwargs)
A:dask.dataframe.rolling.before->pandas.Timedelta(self.window)
A:dask.dataframe.rolling.after->pandas.to_timedelta(after)
A:dask.dataframe.rolling.name->super()._rolling_kwargs().pop('token', None)
A:dask.dataframe.rolling.parent_meta->super()._rolling_kwargs().pop('parent_meta', None)
A:dask.dataframe.rolling.token->tokenize(func, meta, before, after, *args, **kwargs)
A:dask.dataframe.rolling.args->_maybe_align_partitions(args)
A:dask.dataframe.rolling.meta->self.pandas_rolling_method(self.obj._meta_nonempty, rolling_kwargs, method_name, *args, **kwargs)
A:dask.dataframe.rolling.graph->dask.highlevelgraph.HighLevelGraph.from_collections(name, dsk, dependencies=dependencies)
A:dask.dataframe.rolling.divisions->_get_divisions_map_partitions(align_dataframes, transform_divisions, dfs, func, args, kwargs)
A:dask.dataframe.rolling.(prevs_parts_dsk, prevs)->_get_previous_partitions(arg, before)
A:dask.dataframe.rolling.(nexts_parts_dsk, nexts)->_get_nexts_partitions(arg, after)
A:dask.dataframe.rolling.arg->normalize_arg(arg)
A:dask.dataframe.rolling.(arg2, collections)->unpack_collections(arg)
A:dask.dataframe.rolling.v->normalize_arg(v)
A:dask.dataframe.rolling.(v, collections)->unpack_collections(v)
A:dask.dataframe.rolling.dsk->partitionwise_graph(overlap_chunk, name, func, before, after, *args2, **kwargs4, dependencies=dependencies)
A:dask.dataframe.rolling.divs->pandas.Series(df.divisions)
A:dask.dataframe.rolling.lb->max(pt_i - before, pt_z)
A:dask.dataframe.rolling.selected->dask.dataframe.methods.concat([prev[prev.index > current.index.min() - before] for prev in prevs])
A:dask.dataframe.rolling.rolling->df.groupby(**groupby_kwargs).rolling(**rolling_kwargs)
A:dask.dataframe.rolling.rolling_kwargs->self._rolling_kwargs()
A:dask.dataframe.rolling.sliced_plus->list(self._groupby_slice)
A:dask.dataframe.rolling.kwargs->super()._rolling_kwargs()
A:dask.dataframe.rolling.groupby->df.groupby(**groupby_kwargs)
dask.dataframe.map_overlap(func,df,before,after,*args,meta=no_default,enforce_metadata=True,transform_divisions=True,align_dataframes=True,**kwargs)
dask.dataframe.rolling.Rolling(self,obj,window=None,min_periods=None,center=False,win_type=None,axis=no_default)
dask.dataframe.rolling.Rolling.__init__(self,obj,window=None,min_periods=None,center=False,win_type=None,axis=no_default)
dask.dataframe.rolling.Rolling.__repr__(self)
dask.dataframe.rolling.Rolling._call_method(self,method_name,*args,**kwargs)
dask.dataframe.rolling.Rolling._has_single_partition(self)
dask.dataframe.rolling.Rolling._rolling_kwargs(self)
dask.dataframe.rolling.Rolling.aggregate(self,func,*args,**kwargs)
dask.dataframe.rolling.Rolling.apply(self,func,raw=False,engine='cython',engine_kwargs=None,args=None,kwargs=None)
dask.dataframe.rolling.Rolling.count(self)
dask.dataframe.rolling.Rolling.cov(self)
dask.dataframe.rolling.Rolling.kurt(self)
dask.dataframe.rolling.Rolling.max(self)
dask.dataframe.rolling.Rolling.mean(self)
dask.dataframe.rolling.Rolling.median(self)
dask.dataframe.rolling.Rolling.min(self)
dask.dataframe.rolling.Rolling.pandas_rolling_method(df,rolling_kwargs,name,*args,**kwargs)
dask.dataframe.rolling.Rolling.quantile(self,quantile)
dask.dataframe.rolling.Rolling.skew(self)
dask.dataframe.rolling.Rolling.std(self,ddof=1)
dask.dataframe.rolling.Rolling.sum(self)
dask.dataframe.rolling.Rolling.var(self,ddof=1)
dask.dataframe.rolling.RollingGroupby(self,groupby,window=None,min_periods=None,center=False,win_type=None,axis=0)
dask.dataframe.rolling.RollingGroupby.__init__(self,groupby,window=None,min_periods=None,center=False,win_type=None,axis=0)
dask.dataframe.rolling.RollingGroupby._call_method(self,method_name,*args,**kwargs)
dask.dataframe.rolling.RollingGroupby._rolling_kwargs(self)
dask.dataframe.rolling.RollingGroupby.pandas_rolling_method(df,rolling_kwargs,name,*args,groupby_kwargs=None,groupby_slice=None,**kwargs)
dask.dataframe.rolling._combined_parts(prev_part,current_part,next_part,before,after)
dask.dataframe.rolling._get_nexts_partitions(df,after)
dask.dataframe.rolling._get_previous_partitions(df,before)
dask.dataframe.rolling._head_timedelta(current,next_,after)
dask.dataframe.rolling._tail_timedelta(prevs,current,before)
dask.dataframe.rolling.map_overlap(func,df,before,after,*args,meta=no_default,enforce_metadata=True,transform_divisions=True,align_dataframes=True,**kwargs)
dask.dataframe.rolling.overlap_chunk(func,before,after,*args,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/numeric.py----------------------------------------
A:dask.dataframe.numeric.is_series->isinstance(arg, Series)
A:dask.dataframe.numeric.is_array->isinstance(arg, Array)
A:dask.dataframe.numeric.is_scalar->pd_is_scalar(arg)
A:dask.dataframe.numeric.meta->pandas.to_numeric(arg._meta)
dask.dataframe.numeric.to_numeric(arg,errors='raise',meta=None)
dask.dataframe.to_numeric(arg,errors='raise',meta=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/indexing.py----------------------------------------
A:dask.dataframe.indexing.meta->self._make_meta(iindexer, cindexer)
A:dask.dataframe.indexing.iindexer->_maybe_partial_time_string(idx, iindexer)
A:dask.dataframe.indexing.idx->meta_nonempty(self.obj._meta.index)
A:dask.dataframe.indexing.iindexer_series->_maybe_partial_time_string(idx, iindexer).to_dask_dataframe('_', self.obj.index)
A:dask.dataframe.indexing.parts->self._get_partitions(iindexer)
A:dask.dataframe.indexing.items->sorted(parts.items())
A:dask.dataframe.indexing.graph->dask.highlevelgraph.HighLevelGraph.from_collections(name, dsk, dependencies=[self.obj])
A:dask.dataframe.indexing.part->self._get_partitions(iindexer)
A:dask.dataframe.indexing.start->index._maybe_cast_slice_bound(indexer, 'left')
A:dask.dataframe.indexing.stop->index._maybe_cast_slice_bound(indexer, 'right')
A:dask.dataframe.indexing.istart->self._coerce_loc_index(iindexer.start)
A:dask.dataframe.indexing.istop->self._coerce_loc_index(iindexer.stop)
A:dask.dataframe.indexing.div_start->max(istart, self.obj.divisions[start])
A:dask.dataframe.indexing.div_stop->min(istop, self.obj.divisions[stop + 1])
A:dask.dataframe.indexing.val->_coerce_loc_index(divisions, val)
A:dask.dataframe.indexing.i->bisect.bisect_right(divisions, val)
A:dask.dataframe.indexing.results->defaultdict(list)
A:dask.dataframe.indexing.div->min(len(divisions) - 2, max(0, i - 1))
dask.dataframe.indexing._IndexerBase(self,obj)
dask.dataframe.indexing._IndexerBase.__dask_tokenize__(self)
dask.dataframe.indexing._IndexerBase.__init__(self,obj)
dask.dataframe.indexing._IndexerBase._make_meta(self,iindexer,cindexer)
dask.dataframe.indexing._IndexerBase._meta_indexer(self)
dask.dataframe.indexing._IndexerBase._name(self)
dask.dataframe.indexing._LocIndexer(_IndexerBase)
dask.dataframe.indexing._LocIndexer.__getitem__(self,key)
dask.dataframe.indexing._LocIndexer._coerce_loc_index(self,key)
dask.dataframe.indexing._LocIndexer._get_partitions(self,keys)
dask.dataframe.indexing._LocIndexer._loc(self,iindexer,cindexer)
dask.dataframe.indexing._LocIndexer._loc_array(self,iindexer,cindexer)
dask.dataframe.indexing._LocIndexer._loc_element(self,iindexer,cindexer)
dask.dataframe.indexing._LocIndexer._loc_list(self,iindexer,cindexer)
dask.dataframe.indexing._LocIndexer._loc_series(self,iindexer,cindexer)
dask.dataframe.indexing._LocIndexer._loc_slice(self,iindexer,cindexer)
dask.dataframe.indexing._LocIndexer._maybe_partial_time_string(self,iindexer)
dask.dataframe.indexing._LocIndexer._meta_indexer(self)
dask.dataframe.indexing._coerce_loc_index(divisions,o)
dask.dataframe.indexing._iLocIndexer(_IndexerBase)
dask.dataframe.indexing._iLocIndexer.__getitem__(self,key)
dask.dataframe.indexing._iLocIndexer._iloc(self,iindexer,cindexer)
dask.dataframe.indexing._iLocIndexer._meta_indexer(self)
dask.dataframe.indexing._maybe_partial_time_string(index,indexer)
dask.dataframe.indexing._partition_of_index_value(divisions,val)
dask.dataframe.indexing._partitions_of_index_values(divisions,values)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/methods.py----------------------------------------
A:dask.dataframe.methods.right_index->pandas.Series(data, index=index, dtype=dtype, name=name).index.get_slice_bound(stop, 'left', **kind_opts)
A:dask.dataframe.methods.left_index->pandas.Series(data, index=index, dtype=dtype, name=name).index.get_slice_bound(start, 'right', **kind_opts)
A:dask.dataframe.methods.vars->pandas.concat([numeric_var, timedelta_var])
A:dask.dataframe.methods.values_indexes->sorted((x.index for x in values), key=len)
A:dask.dataframe.methods.typ->type(q)
A:dask.dataframe.methods.mean->pandas.to_timedelta(mean)
A:dask.dataframe.methods.std->pandas.to_timedelta(std)
A:dask.dataframe.methods.min->pandas.to_datetime(min)
A:dask.dataframe.methods.max->pandas.to_datetime(max)
A:dask.dataframe.methods.q->q.to_frame().to_frame()
A:dask.dataframe.methods.part1->typ([count, mean, std, min], index=['count', 'mean', 'std', 'min'])
A:dask.dataframe.methods.part3->typ([max], index=['max'])
A:dask.dataframe.methods.result->pandas.Series(data, index=index, dtype=dtype, name=name)
A:dask.dataframe.methods.args_len->len(stats)
A:dask.dataframe.methods.top->top.tz_localize(tz).tz_localize(tz)
A:dask.dataframe.methods.first->pandas.Timestamp(min_ts, tz=tz)
A:dask.dataframe.methods.last->pandas.Timestamp(max_ts, tz=tz)
A:dask.dataframe.methods.pairs->dict(partition(2, pairs))
A:dask.dataframe.methods.df->df.copy().copy()
A:dask.dataframe.methods.out->df.copy().copy().fillna()
A:dask.dataframe.methods.values->values.astype(object).astype(object)
A:dask.dataframe.methods.rs->numpy.random.RandomState(state)
A:dask.dataframe.methods.df.columns->df.copy().copy().columns.astype(dtype)
A:dask.dataframe.methods.s->pandas.Series(concatenated[['first', 'last']].to_numpy().ravel())
A:dask.dataframe.methods.monotonic_increasing_chunk->partial(_monotonic_chunk, prop='is_monotonic_increasing')
A:dask.dataframe.methods.monotonic_decreasing_chunk->partial(_monotonic_chunk, prop='is_monotonic_decreasing')
A:dask.dataframe.methods.monotonic_increasing_combine->partial(_monotonic_combine, prop='is_monotonic_increasing')
A:dask.dataframe.methods.monotonic_decreasing_combine->partial(_monotonic_combine, prop='is_monotonic_decreasing')
A:dask.dataframe.methods.monotonic_increasing_aggregate->partial(_monotonic_aggregate, prop='is_monotonic_increasing')
A:dask.dataframe.methods.monotonic_decreasing_aggregate->partial(_monotonic_aggregate, prop='is_monotonic_decreasing')
dask.dataframe.methods._cum_aggregate_apply(aggregate,x,y)
dask.dataframe.methods._monotonic_aggregate(concatenated,prop)
dask.dataframe.methods._monotonic_chunk(x,prop)
dask.dataframe.methods._monotonic_combine(concatenated,prop)
dask.dataframe.methods.apply(df,*args,**kwargs)
dask.dataframe.methods.applymap(df,*args,**kwargs)
dask.dataframe.methods.assign(df,*pairs)
dask.dataframe.methods.assign_index(df,ind)
dask.dataframe.methods.boundary_slice(df,start,stop,right_boundary=True,left_boundary=True,kind=None)
dask.dataframe.methods.cummax_aggregate(x,y)
dask.dataframe.methods.cummin_aggregate(x,y)
dask.dataframe.methods.cumprod_aggregate(x,y)
dask.dataframe.methods.cumsum_aggregate(x,y)
dask.dataframe.methods.describe_aggregate(values)
dask.dataframe.methods.describe_nonnumeric_aggregate(stats,name)
dask.dataframe.methods.describe_numeric_aggregate(stats,name=None,is_timedelta_col=False,is_datetime_col=False)
dask.dataframe.methods.drop_columns(df,columns,dtype)
dask.dataframe.methods.fillna_check(df,method,check=True)
dask.dataframe.methods.iloc(df,cindexer=None)
dask.dataframe.methods.index_count(x)
dask.dataframe.methods.loc(df,iindexer,cindexer=None)
dask.dataframe.methods.mean_aggregate(s,n)
dask.dataframe.methods.nbytes(x)
dask.dataframe.methods.pivot_agg(df)
dask.dataframe.methods.pivot_agg_first(df)
dask.dataframe.methods.pivot_agg_last(df)
dask.dataframe.methods.pivot_count(df,index,columns,values)
dask.dataframe.methods.pivot_first(df,index,columns,values)
dask.dataframe.methods.pivot_last(df,index,columns,values)
dask.dataframe.methods.pivot_sum(df,index,columns,values)
dask.dataframe.methods.sample(df,state,frac,replace)
dask.dataframe.methods.size(x)
dask.dataframe.methods.try_loc(df,iindexer,cindexer=None)
dask.dataframe.methods.unique(x,series_name=None)
dask.dataframe.methods.value_counts_aggregate(x,total_length=None,sort=True,ascending=False,normalize=False,**groupby_kwargs)
dask.dataframe.methods.value_counts_combine(x,sort=True,ascending=False,**groupby_kwargs)
dask.dataframe.methods.values(df)
dask.dataframe.methods.var_mixed_concat(numeric_var,timedelta_var,columns)
dask.dataframe.methods.wrap_kurtosis_reduction(array_kurtosis,index)
dask.dataframe.methods.wrap_skew_reduction(array_skew,index)
dask.dataframe.methods.wrap_var_reduction(array_var,index)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/_pyarrow.py----------------------------------------
A:dask.dataframe._pyarrow.string_dtype->pandas.StringDtype('pyarrow')
A:dask.dataframe._pyarrow.df->df.astype(dtypes, copy=False).astype(dtypes, copy=False)
A:dask.dataframe._pyarrow.df.index->df.astype(dtypes, copy=False).astype(dtypes, copy=False).index.astype(string_dtype)
A:dask.dataframe._pyarrow.to_pyarrow_string->partial(_to_string_dtype, dtype_check=is_object_string_dtype, index_check=is_object_string_index, string_dtype='pyarrow')
A:dask.dataframe._pyarrow.to_object_string->partial(_to_string_dtype, dtype_check=is_pyarrow_string_dtype, index_check=is_pyarrow_string_index, string_dtype=object)
dask.dataframe._pyarrow._to_string_dtype(df,dtype_check,index_check,string_dtype)
dask.dataframe._pyarrow.check_pyarrow_string_supported()
dask.dataframe._pyarrow.is_object_string_dataframe(x)
dask.dataframe._pyarrow.is_object_string_dtype(dtype)
dask.dataframe._pyarrow.is_object_string_index(x)
dask.dataframe._pyarrow.is_object_string_series(x)
dask.dataframe._pyarrow.is_pyarrow_string_dtype(dtype)
dask.dataframe._pyarrow.is_pyarrow_string_index(x)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/backends.py----------------------------------------
A:dask.dataframe.backends.dataframe_creation_dispatch->CreationDispatch(module_name='dataframe', default='pandas', entrypoint_class=DataFrameBackendEntrypoint, name='dataframe_creation_dispatch')
A:dask.dataframe.backends.out->pandas.concat(dfs2, join=join, **kwargs)
A:dask.dataframe.backends.out.index->pandas.concat(dfs2, join=join, **kwargs).index.copy(deep=True)
A:dask.dataframe.backends.types_mapper->kwargs.pop('types_mapper', default_types_mapper)
A:dask.dataframe.backends.index->make_meta_dispatch(index)
A:dask.dataframe.backends.dtype->numpy.dtype(x)
A:dask.dataframe.backends.idx->_nonempty_index(s.index)
A:dask.dataframe.backends.dt_s_dict->dict()
A:dask.dataframe.backends.data->pandas.Categorical.from_codes(codes, sample.cat.categories, sample.cat.ordered)
A:dask.dataframe.backends.dt_s_dict[dt]->_nonempty_series(x.iloc[:, i], idx=idx)
A:dask.dataframe.backends.res->pandas.DataFrame(data, index=idx, columns=np.arange(len(x.columns)))
A:dask.dataframe.backends.typ->type(idx)
A:dask.dataframe.backends.start->numpy.timedelta64(1, 'D')
A:dask.dataframe.backends.entry->_scalar_from_dtype(dtype)
A:dask.dataframe.backends.total_size->super().__sizeof__()
A:dask.dataframe.backends.(indexer, locations)->pandas._libs.algos.groupsort_indexer(c.astype(np.intp, copy=False), k)
A:dask.dataframe.backends.df2->df.take(indexer)
A:dask.dataframe.backends.locations->locations.cumsum().cumsum()
A:dask.dataframe.backends.ignore_order->kwargs.pop('ignore_order', False)
A:dask.dataframe.backends.dfs[i]->dfs[i].astype('category').astype('category')
A:dask.dataframe.backends.new_tuples->numpy.concatenate(to_concat)
A:dask.dataframe.backends.ind->concat([df.index for df in dfs2])
A:dask.dataframe.backends.cat_mask->pandas.concat([(df.dtypes == 'category').to_frame().T for df in dfs3], join=join, **kwargs).any()
A:dask.dataframe.backends.sample->df.get(col)
A:dask.dataframe.backends.codes->numpy.full(len(df), -1, dtype='i8')
A:dask.dataframe.backends.out[col]->union_categoricals(parts, ignore_order=ignore_order)
dask.dataframe.backends.DataFrameBackendEntrypoint(DaskBackendEntrypoint)
dask.dataframe.backends.DataFrameBackendEntrypoint.from_dict(data:dict,*,npartitions:int,**kwargs)
dask.dataframe.backends.DataFrameBackendEntrypoint.read_csv(urlpath:str|list,**kwargs)
dask.dataframe.backends.DataFrameBackendEntrypoint.read_hdf(pattern:str|list,key:str,**kwargs)
dask.dataframe.backends.DataFrameBackendEntrypoint.read_json(url_path:str|list,**kwargs)
dask.dataframe.backends.DataFrameBackendEntrypoint.read_orc(path:str|list,**kwargs)
dask.dataframe.backends.DataFrameBackendEntrypoint.read_parquet(path:str|list,**kwargs)
dask.dataframe.backends.PandasBackendEntrypoint(DataFrameBackendEntrypoint)
dask.dataframe.backends.PandasBackendEntrypoint.to_backend(cls,data:_Frame,**kwargs)
dask.dataframe.backends.PandasBackendEntrypoint.to_backend_dispatch(cls)
dask.dataframe.backends.ShuffleGroupResult(SimpleSizeof,dict)
dask.dataframe.backends.ShuffleGroupResult.__sizeof__(self)->int
dask.dataframe.backends._(dtype)
dask.dataframe.backends._(x)
dask.dataframe.backends._(x,index=None)
dask.dataframe.backends._(x,index=None)
dask.dataframe.backends._meta_lib_from_array_da(x)
dask.dataframe.backends._meta_lib_from_array_numpy(x)
dask.dataframe.backends._nonempty_index(idx)
dask.dataframe.backends._nonempty_series(s,idx=None)
dask.dataframe.backends._register_cudf()
dask.dataframe.backends._register_cupy_to_cudf()
dask.dataframe.backends.categorical_dtype_pandas(categories=None,ordered=False)
dask.dataframe.backends.concat_pandas(dfs,axis=0,join='outer',uniform=False,filter_warning=True,ignore_index=False,**kwargs)
dask.dataframe.backends.get_grouper_pandas(obj)
dask.dataframe.backends.get_pandas_dataframe_from_pyarrow(meta,table,**kwargs)
dask.dataframe.backends.get_parallel_type_dataframe(_)
dask.dataframe.backends.get_parallel_type_frame(o)
dask.dataframe.backends.get_parallel_type_index(_)
dask.dataframe.backends.get_parallel_type_object(_)
dask.dataframe.backends.get_parallel_type_series(_)
dask.dataframe.backends.get_pyarrow_schema_pandas(obj,preserve_index=None)
dask.dataframe.backends.get_pyarrow_table_from_pandas(obj,**kwargs)
dask.dataframe.backends.group_split_pandas(df,c,k,ignore_index=False)
dask.dataframe.backends.hash_object_pandas(obj,index=True,encoding='utf8',hash_key=None,categorize=True)
dask.dataframe.backends.is_categorical_dtype_pandas(obj)
dask.dataframe.backends.make_meta_object(x,index=None)
dask.dataframe.backends.make_meta_pandas_datetime_tz(x,index=None)
dask.dataframe.backends.meta_nonempty_dataframe(x)
dask.dataframe.backends.meta_nonempty_object(x)
dask.dataframe.backends.partd_pandas_blocks(_)
dask.dataframe.backends.percentile(a,q,interpolation='linear')
dask.dataframe.backends.to_pandas_dispatch_from_pandas(data,**kwargs)
dask.dataframe.backends.tolist_numpy_or_pandas(obj)
dask.dataframe.backends.union_categoricals_pandas(to_union,sort_categories=False,ignore_order=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/accessor.py----------------------------------------
A:dask.dataframe.accessor.func.__wrapped__->getattr(pd_cls, attr)
A:dask.dataframe.accessor.series_meta->series_meta.to_series().to_series()
A:dask.dataframe.accessor.meta->getattr(meta.str, method)(n=n, expand=expand, pat=pat)
A:dask.dataframe.accessor.pd_cls->getattr(pd.Series, cls._accessor_name)
A:dask.dataframe.accessor.out->getattr(getattr(obj, accessor, obj), attr)(*args, **kwargs)
A:dask.dataframe.accessor.x->x.compute().compute()
A:dask.dataframe.accessor.accessor_obj->self._accessor(obj)
dask.dataframe.accessor.Accessor(self,series)
dask.dataframe.accessor.Accessor.__init__(self,series)
dask.dataframe.accessor.Accessor.__init_subclass__(cls,**kwargs)
dask.dataframe.accessor.Accessor._delegate_method(obj,accessor,attr,args,kwargs,catch_deprecation_warnings:bool=False)
dask.dataframe.accessor.Accessor._delegate_property(obj,accessor,attr)
dask.dataframe.accessor.Accessor._function_map(self,attr,*args,**kwargs)
dask.dataframe.accessor.Accessor._property_map(self,attr)
dask.dataframe.accessor.CachedAccessor(self,name,accessor)
dask.dataframe.accessor.CachedAccessor.__get__(self,obj,cls)
dask.dataframe.accessor.CachedAccessor.__init__(self,name,accessor)
dask.dataframe.accessor.DatetimeAccessor(Accessor)
dask.dataframe.accessor.StringAccessor(Accessor)
dask.dataframe.accessor.StringAccessor.__getitem__(self,index)
dask.dataframe.accessor.StringAccessor._split(self,method,pat=None,n=-1,expand=False)
dask.dataframe.accessor.StringAccessor.cat(self,others=None,sep=None,na_rep=None)
dask.dataframe.accessor.StringAccessor.extractall(self,pat,flags=0)
dask.dataframe.accessor.StringAccessor.rsplit(self,pat=None,n=-1,expand=False)
dask.dataframe.accessor.StringAccessor.split(self,pat=None,n=-1,expand=False)
dask.dataframe.accessor._bind_method(cls,pd_cls,attr,min_version=None)
dask.dataframe.accessor._bind_property(cls,pd_cls,attr,min_version=None)
dask.dataframe.accessor._register_accessor(name,cls)
dask.dataframe.accessor.maybe_wrap_pandas(obj,x)
dask.dataframe.accessor.register_dataframe_accessor(name)
dask.dataframe.accessor.register_index_accessor(name)
dask.dataframe.accessor.register_series_accessor(name)
dask.dataframe.accessor.str_cat(self,*others,**kwargs)
dask.dataframe.accessor.str_extractall(series,pat,flags)
dask.dataframe.accessor.str_get(series,index)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/io/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/io/sql.py----------------------------------------
A:dask.dataframe.io.sql.engine->sqlalchemy.create_engine(uri, **engine_kwargs)
A:dask.dataframe.io.sql.q->d.to_sql(con=engine, **kwargs)
A:dask.dataframe.io.sql.head->to_pyarrow_string(head)
A:dask.dataframe.io.sql.minmax->pandas.read_sql(q, engine)
A:dask.dataframe.io.sql.divisions->numpy.linspace(mini, maxi, npartitions + 1, dtype=dtype).tolist()
A:dask.dataframe.io.sql.table_name->sqlalchemy.Table(table_name, m, autoload_with=engine, schema=schema)
A:dask.dataframe.io.sql.con->dict(name=name, uri=uri, engine_kwargs=engine_kwargs, schema=schema, if_exists=if_exists, index=index, index_label=index_label, chunksize=chunksize, dtype=dtype, method=method).pop('uri')
A:dask.dataframe.io.sql.m->sqlalchemy.MetaData()
A:dask.dataframe.io.sql.query->sqlalchemy.sql.select(*columns).select_from(table_name)
A:dask.dataframe.io.sql.df->pandas.read_sql(q, engine, **kwargs)
A:dask.dataframe.io.sql.kwargs->dict(name=name, uri=uri, engine_kwargs=engine_kwargs, schema=schema, if_exists=if_exists, index=index, index_label=index_label, chunksize=chunksize, dtype=dtype, method=method)
A:dask.dataframe.io.sql.meta_task->delayed(_to_sql_chunk)(df._meta, **kwargs)
A:dask.dataframe.io.sql.worker_kwargs->dict(kwargs, if_exists='append')
A:dask.dataframe.io.sql.result->delayed(result)
dask.dataframe.io.sql._extra_deps(func,*args,extras=None,**kwargs)
dask.dataframe.io.sql._read_sql_chunk(q,uri,meta,engine_kwargs=None,**kwargs)
dask.dataframe.io.sql._to_sql_chunk(d,uri,engine_kwargs=None,**kwargs)
dask.dataframe.io.sql.read_sql(sql,con,index_col,**kwargs)
dask.dataframe.io.sql.read_sql_query(sql,con,index_col,divisions=None,npartitions=None,limits=None,bytes_per_chunk='256MiB',head_rows=5,meta=None,engine_kwargs=None,**kwargs)
dask.dataframe.io.sql.read_sql_table(table_name,con,index_col,divisions=None,npartitions=None,limits=None,columns=None,bytes_per_chunk='256MiB',head_rows=5,schema=None,meta=None,engine_kwargs=None,**kwargs)
dask.dataframe.io.sql.to_sql(df,name:str,uri:str,schema=None,if_exists:str='fail',index:bool=True,index_label=None,chunksize=None,dtype=None,method=None,compute=True,parallel=False,engine_kwargs=None)
dask.dataframe.read_sql(sql,con,index_col,**kwargs)
dask.dataframe.read_sql_query(sql,con,index_col,divisions=None,npartitions=None,limits=None,bytes_per_chunk='256MiB',head_rows=5,meta=None,engine_kwargs=None,**kwargs)
dask.dataframe.read_sql_table(table_name,con,index_col,divisions=None,npartitions=None,limits=None,columns=None,bytes_per_chunk='256MiB',head_rows=5,schema=None,meta=None,engine_kwargs=None,**kwargs)
dask.dataframe.to_sql(df,name:str,uri:str,schema=None,if_exists:str='fail',index:bool=True,index_label=None,chunksize=None,dtype=None,method=None,compute=True,parallel=False,engine_kwargs=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/io/utils.py----------------------------------------
A:dask.dataframe.io.utils.pandas_metadata->json.loads(schema.metadata[b'pandas'].decode('utf8'))
A:dask.dataframe.io.utils.numpy_dtype->type_mapper(field.type)
A:dask.dataframe.io.utils.index->pandas.MultiIndex.from_arrays(indexes, names=index_cols)
A:dask.dataframe.io.utils.df->pandas.DataFrame(data, index=index)
A:dask.dataframe.io.utils.precache_options->(precache_options or {}).copy()
A:dask.dataframe.io.utils.precache->(precache_options or {}).copy().pop('method', None)
A:dask.dataframe.io.utils.cache_type->kwargs.pop('cache_type', 'parts')
dask.dataframe.io.utils.DataFrameIOFunction(self,*args,**kwargs)
dask.dataframe.io.utils.DataFrameIOFunction.__call__(self,*args,**kwargs)
dask.dataframe.io.utils.DataFrameIOFunction.columns(self)
dask.dataframe.io.utils.DataFrameIOFunction.project_columns(self,columns)
dask.dataframe.io.utils.SupportsLock(Protocol)
dask.dataframe.io.utils.SupportsLock.acquire(self)->object
dask.dataframe.io.utils.SupportsLock.release(self)->object
dask.dataframe.io.utils._get_pyarrow_dtypes(schema,categories,dtype_backend=None)
dask.dataframe.io.utils._guid()
dask.dataframe.io.utils._is_local_fs(fs)
dask.dataframe.io.utils._is_local_fs_pyarrow(fs)
dask.dataframe.io.utils._meta_from_dtypes(to_read_columns,file_dtypes,index_cols,column_index_names)
dask.dataframe.io.utils._open_input_files(paths,fs=None,context_stack=None,open_file_func=None,precache_options=None,**kwargs)
dask.dataframe.io.utils._set_context(obj,stack)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/io/csv.py----------------------------------------
A:dask.dataframe.io.csv.rest_kwargs->self.kwargs.copy()
A:dask.dataframe.io.csv.df->df.assign(**{colname: pd.Categorical.from_codes(np.full(len(df), code), paths)}).assign(**{colname: pd.Categorical.from_codes(np.full(len(df), code), paths)})
A:dask.dataframe.io.csv.bio->BytesIO()
A:dask.dataframe.io.csv.code->paths.index(path)
A:dask.dataframe.io.csv.df[c]->df[c].astype(dtypes[c]).astype(dtypes[c])
A:dask.dataframe.io.csv.ex->'\n'.join((f'- {c}\n  {e!r}' for (c, e) in sorted(errors, key=lambda x: str(x[0]))))
A:dask.dataframe.io.csv.bad_dtypes->sorted(bad_dtypes, key=lambda x: str(x[0]))
A:dask.dataframe.io.csv.table->asciitable(['Column', 'Found', 'Expected'], bad_dtypes)
A:dask.dataframe.io.csv.dtype_msg->"{table}\n\n{exceptions}Usually this is due to dask's dtype inference failing, and\n*may* be fixed by specifying dtypes manually by adding:\n\n{dtype_kw}\n\nto the call to `read_csv`/`read_table`.{extra}".format(table=table, exceptions=exceptions, dtype_kw=dtype_kw, extra=extra)
A:dask.dataframe.io.csv.cols->'\n'.join(('- %s' % c for c in bad_dates))
A:dask.dataframe.io.csv.date_msg->"The following columns{also}failed to properly parse as dates:\n\n{cols}\n\nThis is usually due to an invalid value in that column. To\ndiagnose and fix it's recommended to drop these columns from the\n`parse_dates` keyword, and manually convert them to dates later\nusing `dd.to_datetime`.".format(also=also, cols=cols)
A:dask.dataframe.io.csv.dtypes->reader(BytesIO(b_sample), nrows=sample_rows, **head_kwargs).dtypes.to_dict()
A:dask.dataframe.io.csv.unknown_categoricals->categoricals.difference(known_categoricals)
A:dask.dataframe.io.csv.columns->list(head.columns)
A:dask.dataframe.io.csv.blocks->tuple(flatten(block_lists))
A:dask.dataframe.io.csv.is_first->tuple(block_mask(block_lists))
A:dask.dataframe.io.csv.is_last->tuple(block_mask_last(block_lists))
A:dask.dataframe.io.csv.head->reader(BytesIO(b_sample), nrows=sample_rows, **head_kwargs)
A:dask.dataframe.io.csv.blocksize->parse_bytes(blocksize)
A:dask.dataframe.io.csv.cpu->psutil.cpu_count()
A:dask.dataframe.io.csv.AUTO_BLOCKSIZE->_infer_block_size()
A:dask.dataframe.io.csv.lastskiprowfirstrow->kwargs.get('skiprows')
A:dask.dataframe.io.csv.skiprows->set(kwargs.get('skiprows'))
A:dask.dataframe.io.csv.lastskiprow->max(skiprows)
A:dask.dataframe.io.csv.firstrow->min(set(range(len(skiprows) + 1)) - set(skiprows))
A:dask.dataframe.io.csv.path_converter->kwargs.get('converters').get(include_path_column, None)
A:dask.dataframe.io.csv.compression->infer_compression(paths[0])
A:dask.dataframe.io.csv.b_lineterminator->lineterminator.encode()
A:dask.dataframe.io.csv.b_out->read_bytes(urlpath, delimiter=b_lineterminator, blocksize=blocksize, sample=sample, compression=compression, include_path=include_path_column, **storage_options or {})
A:dask.dataframe.io.csv.b_sample->values[0][0].compute()
A:dask.dataframe.io.csv.names->kwargs.get('names', None)
A:dask.dataframe.io.csv.header->kwargs.get('header', 'infer' if names is None else None)
A:dask.dataframe.io.csv.split_comment->part.decode().split(kwargs.get('comment'))
A:dask.dataframe.io.csv.parts->values[0][0].compute().split(b_lineterminator, max(lastskiprow + need, firstrow + need))
A:dask.dataframe.io.csv.head_kwargs->kwargs.copy()
A:dask.dataframe.io.csv.specified_dtypes->kwargs.get('dtype', {})
A:dask.dataframe.io.csv.head[c]->head[c].astype(float).astype(float)
A:dask.dataframe.io.csv.read.__doc__->READ_DOC_TEMPLATE.format(reader=reader_name, file_type=file_type)
A:dask.dataframe.io.csv.read_csv->dask.dataframe.backends.dataframe_creation_dispatch.register_inplace(backend='pandas', name='read_csv')(make_reader(pd.read_csv, 'read_csv', 'CSV'))
A:dask.dataframe.io.csv.read_table->make_reader(pd.read_table, 'read_table', 'delimited')
A:dask.dataframe.io.csv.read_fwf->make_reader(pd.read_fwf, 'read_fwf', 'fixed-width')
A:dask.dataframe.io.csv.file_options->dict(compression=compression, encoding=encoding, newline='', **storage_options or {})
A:dask.dataframe.io.csv.to_csv_chunk->delayed(_write_csv, pure=False)
A:dask.dataframe.io.csv.dfs->df.assign(**{colname: pd.Categorical.from_codes(np.full(len(df), code), paths)}).assign(**{colname: pd.Categorical.from_codes(np.full(len(df), code), paths)}).to_delayed()
A:dask.dataframe.io.csv.first_file->open_file(filename, mode=mode, **file_options)
A:dask.dataframe.io.csv.value->to_csv_chunk(d, append_file, depend_on=value, **kwargs)
A:dask.dataframe.io.csv.append_mode->append_mode.replace('w', '').replace('x', '').replace('w', '').replace('x', '')
A:dask.dataframe.io.csv.append_file->open_file(filename, mode=append_mode, **file_options)
A:dask.dataframe.io.csv.files->open_files(filename, mode=mode, name_function=name_function, num=df.npartitions, **file_options)
A:dask.dataframe.io.csv.compute_kwargs->dict()
dask.dataframe.io.csv.CSVFunctionWrapper(self,full_columns,columns,colname,head,header,reader,dtypes,enforce,kwargs)
dask.dataframe.io.csv.CSVFunctionWrapper.__init__(self,full_columns,columns,colname,head,header,reader,dtypes,enforce,kwargs)
dask.dataframe.io.csv.CSVFunctionWrapper.columns(self)
dask.dataframe.io.csv.CSVFunctionWrapper.project_columns(self,columns)
dask.dataframe.io.csv._infer_block_size()
dask.dataframe.io.csv._write_csv(df,fil,*,depend_on=None,**kwargs)
dask.dataframe.io.csv.auto_blocksize(total_memory,cpu_count)
dask.dataframe.io.csv.block_mask(block_lists)
dask.dataframe.io.csv.block_mask_last(block_lists)
dask.dataframe.io.csv.coerce_dtypes(df,dtypes)
dask.dataframe.io.csv.make_reader(reader,reader_name,file_type)
dask.dataframe.io.csv.pandas_read_text(reader,b,header,kwargs,dtypes=None,columns=None,write_header=True,enforce=False,path=None)
dask.dataframe.io.csv.read_pandas(reader,urlpath,blocksize='default',lineterminator=None,compression='infer',sample=256000,sample_rows=10,enforce=False,assume_missing=False,storage_options=None,include_path_column=False,**kwargs)
dask.dataframe.io.csv.text_blocks_to_pandas(reader,block_lists,header,head,kwargs,enforce=False,specified_dtypes=None,path=None,blocksize=None,urlpath=None)
dask.dataframe.io.csv.to_csv(df,filename,single_file=False,encoding='utf-8',mode='wt',name_function=None,compression=None,compute=True,scheduler=None,storage_options=None,header_first_partition_only=None,compute_kwargs=None,**kwargs)
dask.dataframe.to_csv(df,filename,single_file=False,encoding='utf-8',mode='wt',name_function=None,compression=None,compute=True,scheduler=None,storage_options=None,header_first_partition_only=None,compute_kwargs=None,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/io/hdf.py----------------------------------------
A:dask.dataframe.io.hdf.MP_GET->dask.base.named_schedulers.get('processes', object())
A:dask.dataframe.io.hdf.path->stringify_path(path)
A:dask.dataframe.io.hdf.name_function->build_name_function(df.npartitions - 1)
A:dask.dataframe.io.hdf._actual_get->get_scheduler(collections=[df], scheduler=scheduler)
A:dask.dataframe.io.hdf.lock->get_scheduler_lock()
A:dask.dataframe.io.hdf.dsk->dict()
A:dask.dataframe.io.hdf.i_name->name_function(i)
A:dask.dataframe.io.hdf.kwargs2->kwargs.copy()
A:dask.dataframe.io.hdf.kwargs2['key']->key.replace('*', i_name)
A:dask.dataframe.io.hdf.graph->dask.highlevelgraph.HighLevelGraph.from_collections((name, 0), dsk, dependencies=[df])
A:dask.dataframe.io.hdf.self.common_kwargs->merge(common_kwargs, {'columns': columns})
A:dask.dataframe.io.hdf.result->pandas.read_hdf(path, key, **merge(self.common_kwargs, kwargs))
A:dask.dataframe.io.hdf.pattern->stringify_path(pattern)
A:dask.dataframe.io.hdf.paths->sorted(glob(pattern))
A:dask.dataframe.io.hdf.exists->os.path.exists(path)
A:dask.dataframe.io.hdf.meta->pandas.read_hdf(hdf, meta_key)
A:dask.dataframe.io.hdf.(parts, divisions)->_build_parts(paths, key, start, stop, chunksize, sorted_index, mode)
A:dask.dataframe.io.hdf.(keys, stops, divisions)->_get_keys_stops_divisions(path, key, stop, sorted_index, chunksize, mode)
A:dask.dataframe.io.hdf.keys->_expand_key(key, hdf)
A:dask.dataframe.io.hdf.storer->hdf.get_storer(k)
dask.dataframe.io.hdf.HDFFunctionWrapper(self,columns,dim,lock,common_kwargs)
dask.dataframe.io.hdf.HDFFunctionWrapper.__init__(self,columns,dim,lock,common_kwargs)
dask.dataframe.io.hdf.HDFFunctionWrapper.columns(self)
dask.dataframe.io.hdf.HDFFunctionWrapper.project_columns(self,columns)
dask.dataframe.io.hdf._build_parts(paths,key,start,stop,chunksize,sorted_index,mode)
dask.dataframe.io.hdf._expand_key(key,hdf)
dask.dataframe.io.hdf._get_keys_stops_divisions(path,key,stop,sorted_index,chunksize,mode)
dask.dataframe.io.hdf._one_path_one_key(path,key,start,stop,chunksize)
dask.dataframe.io.hdf._pd_to_hdf(pd_to_hdf,lock,args,kwargs=None)
dask.dataframe.io.hdf.read_hdf(pattern,key,start=0,stop=None,columns=None,chunksize=1000000,sorted_index=False,lock=True,mode='r')
dask.dataframe.io.hdf.to_hdf(df,path,key,mode='a',append=False,scheduler=None,name_function=None,compute=True,lock=None,dask_kwargs=None,**kwargs)
dask.dataframe.read_hdf(pattern,key,start=0,stop=None,columns=None,chunksize=1000000,sorted_index=False,lock=True,mode='r')
dask.dataframe.to_hdf(df,path,key,mode='a',append=False,scheduler=None,name_function=None,compute=True,lock=None,dask_kwargs=None,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/io/json.py----------------------------------------
A:dask.dataframe.io.json.outfiles->open_files(url_path, 'wt', encoding=encoding, errors=errors, name_function=name_function, num=df.npartitions, compression=compression, **storage_options or {})
A:dask.dataframe.io.json.compute_kwargs->dict()
A:dask.dataframe.io.json.engine->partial(pd.read_json, engine=engine)
A:dask.dataframe.io.json.b_out->read_bytes(url_path, b'\n', blocksize=blocksize, sample=sample, compression=compression, include_path=include_path_column, **storage_options)
A:dask.dataframe.io.json.first_path->path_converter(paths[0])
A:dask.dataframe.io.json.path_dtype->pandas.CategoricalDtype((path_converter(f.path) for f in files))
A:dask.dataframe.io.json.flat_paths->flatten(([path_converter(p)] * len(chunk) for (p, chunk) in zip(paths, chunks)))
A:dask.dataframe.io.json.flat_chunks->flatten(chunks)
A:dask.dataframe.io.json.meta->make_meta(meta)
A:dask.dataframe.io.json.files->open_files(url_path, 'rt', encoding=encoding, errors=errors, compression=compression, **storage_options)
A:dask.dataframe.io.json.s->io.StringIO(chunk.decode(encoding, errors))
A:dask.dataframe.io.json.df->add_path_column(df, column_name, path, path_dtype)
dask.dataframe.io.json.add_path_column(df,column_name,path,dtype)
dask.dataframe.io.json.read_json(url_path,orient='records',lines=None,storage_options=None,blocksize=None,sample=2**20,encoding='utf-8',errors='strict',compression='infer',meta=None,engine=pd.read_json,include_path_column=False,path_converter=None,**kwargs)
dask.dataframe.io.json.read_json_chunk(chunk,encoding,errors,engine,column_name,path,path_dtype,kwargs,meta=None)
dask.dataframe.io.json.read_json_file(f,orient,lines,engine,column_name,path,path_dtype,kwargs)
dask.dataframe.io.json.to_json(df,url_path,orient='records',lines=None,storage_options=None,compute=True,encoding='utf-8',errors='strict',compression=None,compute_kwargs=None,name_function=None,**kwargs)
dask.dataframe.io.json.write_json_partition(df,openfile,kwargs)
dask.dataframe.read_json(url_path,orient='records',lines=None,storage_options=None,blocksize=None,sample=2**20,encoding='utf-8',errors='strict',compression='infer',meta=None,engine=pd.read_json,include_path_column=False,path_converter=None,**kwargs)
dask.dataframe.read_json_chunk(chunk,encoding,errors,engine,column_name,path,path_dtype,kwargs,meta=None)
dask.dataframe.read_json_file(f,orient,lines,engine,column_name,path,path_dtype,kwargs)
dask.dataframe.to_json(df,url_path,orient='records',lines=None,storage_options=None,compute=True,encoding='utf-8',errors='strict',compression=None,compute_kwargs=None,name_function=None,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/io/demo.py----------------------------------------
A:dask.dataframe.io.demo.(handler_args, handler_kwargs)->_with_defaults(method)
A:dask.dataframe.io.demo.handler_kwargs->handler_kwargs.copy().copy()
A:dask.dataframe.io.demo.data->method(*args, state=rstate, size=n, **kwargs)
A:dask.dataframe.io.demo.handler->getattr(rstate, method)
A:dask.dataframe.io.demo.choices->list(string.ascii_letters + string.digits + string.punctuation + ' ')
A:dask.dataframe.io.demo.cat_len->len(str(nunique))
A:dask.dataframe.io.demo.state->numpy.random.RandomState(state_data)
A:dask.dataframe.io.demo.index->pandas.RangeIndex(start=start, stop=end + step, step=step).astype(index_dtype)
A:dask.dataframe.io.demo.step->kwargs.get('freq')
A:dask.dataframe.io.demo.df->df.astype(update_dtypes, copy=False).astype(update_dtypes, copy=False)
A:dask.dataframe.io.demo.result->make[dt](len(index), state, **kws)
A:dask.dataframe.io.demo.divisions->list(pd.RangeIndex(0, stop=end, step=partition_freq))
A:dask.dataframe.io.demo.state_data->random_state_data(npartitions, seed)
A:dask.dataframe.io.demo.(meta_start, meta_end)->list(pd.date_range(start='2000', freq=freq, periods=2))
A:dask.dataframe.io.demo.start->pandas.Timestamp(spec.index_spec.start)
A:dask.dataframe.io.demo.prefix->re.sub('[^a-zA-Z0-9]', '_', f'{col.dtype}').rstrip('_')
dask.dataframe.demo.ColumnSpec
dask.dataframe.demo.DatasetSpec
dask.dataframe.demo.DatetimeIndexSpec
dask.dataframe.demo.MakeDataframePart(self,index_dtype,dtypes,kwargs,columns=None)
dask.dataframe.demo.MakeDataframePart.columns(self)
dask.dataframe.demo.MakeDataframePart.project_columns(self,columns)
dask.dataframe.demo.RangeIndexSpec
dask.dataframe.demo.make_categorical(n,rstate,choices=None,nunique=None,**kwargs)
dask.dataframe.demo.make_dataframe_part(index_dtype,start,end,dtypes,columns,state_data,kwargs)
dask.dataframe.demo.make_float(n,rstate,random=False,**kwargs)
dask.dataframe.demo.make_int(n:int,rstate:Any,random:bool=False,dtype:str|type=int,method:str|Callable='poisson',args:tuple[Any,...]=(),**kwargs)
dask.dataframe.demo.make_partition(columns:list,dtypes:dict[str,type|str],index,kwargs,state)
dask.dataframe.demo.make_random_string(n,rstate,length:int=25)->list[str]
dask.dataframe.demo.make_string(n,rstate,choices=None,random=False,length=None,**kwargs)
dask.dataframe.demo.make_timeseries(start='2000-01-01',end='2000-12-31',dtypes=None,freq='10s',partition_freq=f'1{_ME}',seed=None,**kwargs)
dask.dataframe.demo.same_astype(a:str|type,b:str|type)
dask.dataframe.demo.with_spec(spec:DatasetSpec,seed:int|None=None)
dask.dataframe.io.demo.ColumnSpec
dask.dataframe.io.demo.DatasetSpec
dask.dataframe.io.demo.DatetimeIndexSpec
dask.dataframe.io.demo.MakeDataframePart(self,index_dtype,dtypes,kwargs,columns=None)
dask.dataframe.io.demo.MakeDataframePart.__init__(self,index_dtype,dtypes,kwargs,columns=None)
dask.dataframe.io.demo.MakeDataframePart.columns(self)
dask.dataframe.io.demo.MakeDataframePart.project_columns(self,columns)
dask.dataframe.io.demo.RangeIndexSpec
dask.dataframe.io.demo.make_categorical(n,rstate,choices=None,nunique=None,**kwargs)
dask.dataframe.io.demo.make_dataframe_part(index_dtype,start,end,dtypes,columns,state_data,kwargs)
dask.dataframe.io.demo.make_float(n,rstate,random=False,**kwargs)
dask.dataframe.io.demo.make_int(n:int,rstate:Any,random:bool=False,dtype:str|type=int,method:str|Callable='poisson',args:tuple[Any,...]=(),**kwargs)
dask.dataframe.io.demo.make_partition(columns:list,dtypes:dict[str,type|str],index,kwargs,state)
dask.dataframe.io.demo.make_random_string(n,rstate,length:int=25)->list[str]
dask.dataframe.io.demo.make_string(n,rstate,choices=None,random=False,length=None,**kwargs)
dask.dataframe.io.demo.make_timeseries(start='2000-01-01',end='2000-12-31',dtypes=None,freq='10s',partition_freq=f'1{_ME}',seed=None,**kwargs)
dask.dataframe.io.demo.same_astype(a:str|type,b:str|type)
dask.dataframe.io.demo.with_spec(spec:DatasetSpec,seed:int|None=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/io/io.py----------------------------------------
A:dask.dataframe.io.io.lock->Lock()
A:dask.dataframe.io.io.meta->make_meta(meta)
A:dask.dataframe.io.io.columns->list(x.dtype.names)
A:dask.dataframe.io.io.extra->sorted(set(columns).difference(x.dtype.names))
A:dask.dataframe.io.io.divisions->tuple(range(0, len(x), chunksize))
A:dask.dataframe.io.io.token->tokenize(x, chunksize, columns)
A:dask.dataframe.io.io.nrows->len(data)
A:dask.dataframe.io.io.data->data.copy().copy()
A:dask.dataframe.io.io.(divisions, locations)->sorted_division_locations(data.index, npartitions=npartitions, chunksize=chunksize)
A:dask.dataframe.io.io.chunksize->int(ceil(nrows / npartitions))
A:dask.dataframe.io.io.index->pandas.RangeIndex(*index)
A:dask.dataframe.io.io.msg->'The index and array have different numbers of blocks. ({} != {})'.format(index.npartitions, x.numblocks[0])
A:dask.dataframe.io.io.n_elements->sum(x.chunks[0])
A:dask.dataframe.io.io.blk->blockwise(_partition_from_array, name, 'i', *arrays_and_indices, numblocks=numblocks, concatenate=True, **kwargs)
A:dask.dataframe.io.io.graph->dask.highlevelgraph.HighLevelGraph.from_collections(name, layer, dependencies=[])
A:dask.dataframe.io.io.divs->list(divisions)
A:dask.dataframe.io.io.layer->DataFrameIOLayer(name, column_projection, inputs, io_func, label=label, produces_tasks=produces_tasks, creation_info=creation_info)
A:dask.dataframe.io.io.df->new_dd_object(HighLevelGraph.from_collections(name, layer, dfs), name, meta, divs)
A:dask.dataframe.io.io.i->chunksizes(0)
A:dask.dataframe.io.io.ind->int((seq_unique == seq[i]).nonzero()[0][0])
A:dask.dataframe.io.io.pos->int(offsets[ind])
A:dask.dataframe.io.io.self.is_dataframe_io_func->isinstance(self.func, DataFrameIOFunction)
A:dask.dataframe.io.io.lengths->set()
A:dask.dataframe.io.io.iterables->list(iterables)
A:dask.dataframe.io.io.iterables[i]->list(iterable)
A:dask.dataframe.io.io.produces_tasks->kwargs.pop('produces_tasks', False)
A:dask.dataframe.io.io.creation_info->kwargs.pop('creation_info', None)
A:dask.dataframe.io.io.inputs->list(zip(*iterables))
A:dask.dataframe.io.io.io_func->_PackedArgCallable(func, args=args, kwargs=kwargs, meta=meta if enforce_metadata else None, enforce_metadata=enforce_metadata, packed=packed)
A:dask.dataframe.io.io.backend_entrypoint->dask.dataframe.backends.dataframe_creation_dispatch.dispatch(backend)
dask.dataframe.from_array(x,chunksize=50000,columns=None,meta=None)
dask.dataframe.from_dask_array(x,columns=None,index=None,meta=None)
dask.dataframe.from_delayed(dfs:Delayed|distributed.Future|Iterable[Delayed|distributed.Future],meta=None,divisions:tuple|Literal['sorted']|None=None,prefix:str='from-delayed',verify_meta:bool=True)->DataFrame | Series
dask.dataframe.from_dict(data,npartitions,orient='columns',dtype=None,columns=None,constructor=pd.DataFrame)
dask.dataframe.from_map(func,*iterables,args=None,meta=None,divisions=None,label=None,token=None,enforce_metadata=True,**kwargs)
dask.dataframe.from_pandas(data:pd.DataFrame|pd.Series,npartitions:int|None=None,chunksize:int|None=None,sort:bool=True,name:str|None=None)->DataFrame | Series
dask.dataframe.io.io._PackedArgCallable(self,func,args=None,kwargs=None,meta=None,packed=False,enforce_metadata=False)
dask.dataframe.io.io._PackedArgCallable.__init__(self,func,args=None,kwargs=None,meta=None,packed=False,enforce_metadata=False)
dask.dataframe.io.io._PackedArgCallable.columns(self)
dask.dataframe.io.io._PackedArgCallable.project_columns(self,columns)
dask.dataframe.io.io._df_to_bag(df,index=False,format='tuple')
dask.dataframe.io.io._link(token,result)
dask.dataframe.io.io._meta_from_array(x,columns=None,index=None,meta=None)
dask.dataframe.io.io._partition_from_array(data,index=None,initializer=None,**kwargs)
dask.dataframe.io.io.from_array(x,chunksize=50000,columns=None,meta=None)
dask.dataframe.io.io.from_dask_array(x,columns=None,index=None,meta=None)
dask.dataframe.io.io.from_delayed(dfs:Delayed|distributed.Future|Iterable[Delayed|distributed.Future],meta=None,divisions:tuple|Literal['sorted']|None=None,prefix:str='from-delayed',verify_meta:bool=True)->DataFrame | Series
dask.dataframe.io.io.from_dict(data,npartitions,orient='columns',dtype=None,columns=None,constructor=pd.DataFrame)
dask.dataframe.io.io.from_map(func,*iterables,args=None,meta=None,divisions=None,label=None,token=None,enforce_metadata=True,**kwargs)
dask.dataframe.io.io.from_pandas(data:pd.DataFrame|pd.Series,npartitions:int|None=None,chunksize:int|None=None,sort:bool=True,name:str|None=None)->DataFrame | Series
dask.dataframe.io.io.sorted_division_locations(seq,npartitions=None,chunksize=None)
dask.dataframe.io.io.to_backend(ddf:_Frame,backend:str|None=None,**kwargs)
dask.dataframe.io.io.to_bag(df,index=False,format='tuple')
dask.dataframe.io.io.to_records(df)
dask.dataframe.io.to_backend(ddf:_Frame,backend:str|None=None,**kwargs)
dask.dataframe.to_bag(df,index=False,format='tuple')
dask.dataframe.to_records(df)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/io/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/io/tests/test_parquet.py----------------------------------------
A:dask.dataframe.io.tests.test_parquet.fastparquet_version->parse_version(fastparquet.__version__)
A:dask.dataframe.io.tests.test_parquet.pyarrow_version->parse_version('0')
A:dask.dataframe.io.tests.test_parquet.FASTPARQUET_MARK->pytest.mark.skipif(SKIP_FASTPARQUET, reason='fastparquet not found or pyarrow strings are enabled')
A:dask.dataframe.io.tests.test_parquet.PYARROW_MARK->pytest.mark.skipif(SKIP_PYARROW, reason=SKIP_PYARROW_REASON)
A:dask.dataframe.io.tests.test_parquet.df->pandas.DataFrame({'a': [1, 2], 'b': [{'a': 2}, {'a': 4}]})
A:dask.dataframe.io.tests.test_parquet.ddf->dask.dataframe.read_parquet(tmpdir, dataset={'partitioning': partitioning}, filters=filters)
A:dask.dataframe.io.tests.test_parquet._engine_fixture->pytest.fixture(params=[pytest.param('fastparquet', marks=[FASTPARQUET_MARK, pytest.mark.filterwarnings('ignore::FutureWarning')]), pytest.param('pyarrow', marks=[PYARROW_MARK])])
A:dask.dataframe.io.tests.test_parquet.DASK_EXPR_ENABLED->dask.dataframe._dask_expr_enabled()
A:dask.dataframe.io.tests.test_parquet.tmp->str(tmpdir)
A:dask.dataframe.io.tests.test_parquet.data->pandas.DataFrame({'myindex': np.arange(size), 'A': np.random.random(size=size), 'B': pd.Categorical(b)}).set_index('myindex')
A:dask.dataframe.io.tests.test_parquet.files->os.listdir(fn)
A:dask.dataframe.io.tests.test_parquet.df2->dask.dataframe.from_delayed([df_c, df_d])
A:dask.dataframe.io.tests.test_parquet.out->dask.dataframe.read_parquet(path, engine=engine, gather_statistics=True)
A:dask.dataframe.io.tests.test_parquet.fn->str(tmpdir)
A:dask.dataframe.io.tests.test_parquet.read_df->dask.dataframe.read_parquet(fn, split_row_groups=split_row_groups, filters=[('a', '==', 5)])
A:dask.dataframe.io.tests.test_parquet.tmp_path->os.path.join(tmp_path, '../', 'files')
A:dask.dataframe.io.tests.test_parquet.ddf2->dask.dataframe.read_parquet(tmp_path, engine=engine, dtype_backend=dtype_backend)
A:dask.dataframe.io.tests.test_parquet.tmpdir->str(tmpdir)
A:dask.dataframe.io.tests.test_parquet.index->pandas.MultiIndex.from_arrays([np.arange(10), np.arange(10) + 1], names=['x0', 'x1'])
A:dask.dataframe.io.tests.test_parquet.d->dask.dataframe.read_parquet(path, engine=engine).compute()
A:dask.dataframe.io.tests.test_parquet.dfp->pandas.DataFrame({'a': [1, 2], 'b': [{'a': 2}, {'a': 4}]}).copy()
A:dask.dataframe.io.tests.test_parquet.table->pyarrow.Table.from_pandas(df).replace_schema_metadata({})
A:dask.dataframe.io.tests.test_parquet.partitions->dask.dataframe.read_parquet(tmpdir, dataset={'partitioning': partitioning}, filters=filters).to_delayed()
A:dask.dataframe.io.tests.test_parquet.result->dask.dataframe.read_parquet(tmpdir + 'test.parquet').compute()
A:dask.dataframe.io.tests.test_parquet.expected->pandas.DataFrame({'b': ['x', 'y'], 'a': pd.Categorical(pd.Index([1, 2], dtype='int32'))}, index=[0, 0])
A:dask.dataframe.io.tests.test_parquet.expected.index->pandas.DataFrame({'b': ['x', 'y'], 'a': pd.Categorical(pd.Index([1, 2], dtype='int32'))}, index=[0, 0]).index.astype('string[pyarrow]')
A:dask.dataframe.io.tests.test_parquet.ddf1->dask.dataframe.from_pandas(df1, npartitions=2)
A:dask.dataframe.io.tests.test_parquet.metadata1->dask.dataframe.read_parquet(tmp_path, engine=read_engine, filters=[('y', '=', 'c')]).read()
A:dask.dataframe.io.tests.test_parquet.metadata2->dask.dataframe.read_parquet(tmp_path, engine=read_engine, filters=[('y', '=', 'c')]).read()
A:dask.dataframe.io.tests.test_parquet.ddf3->dask.dataframe.read_parquet(tmp, engine=engine, calculate_divisions=True, split_row_groups=True, filters=filters)
A:dask.dataframe.io.tests.test_parquet.df0->pandas.DataFrame({'a': range(10)})
A:dask.dataframe.io.tests.test_parquet.df1->pandas.DataFrame({'a': range(100), 'b': ['cat', 'dog'] * 50})
A:dask.dataframe.io.tests.test_parquet.df0['lat']->df0['lat'].astype('Int64').astype('Int64')
A:dask.dataframe.io.tests.test_parquet.df1['lat']->df1['lat'].astype('Int64').astype('Int64')
A:dask.dataframe.io.tests.test_parquet.dd_df0->dask.dataframe.from_pandas(df0, npartitions=1)
A:dask.dataframe.io.tests.test_parquet.dd_df1->dask.dataframe.from_pandas(df1, npartitions=1)
A:dask.dataframe.io.tests.test_parquet.out['lon']->dask.dataframe.read_parquet(path, engine=engine, gather_statistics=True).lon.astype('int64')
A:dask.dataframe.io.tests.test_parquet.df2.index->dask.dataframe.from_delayed([df_c, df_d]).index.astype(df1.index.dtype)
A:dask.dataframe.io.tests.test_parquet.res->dask.dataframe.read_parquet(tmpdir)
A:dask.dataframe.io.tests.test_parquet.sol->pandas.concat([df1, df2])
A:dask.dataframe.io.tests.test_parquet.df3->fastparquet.ParquetFile(fn).to_pandas(filters=[('at', '=', 'aa')])
A:dask.dataframe.io.tests.test_parquet.dts->pandas.date_range('2020-01-01', '2021-01-01')
A:dask.dataframe.io.tests.test_parquet.expect->pandas.read_parquet(fn)
A:dask.dataframe.io.tests.test_parquet.fns->glob.glob(os.path.join(tmpdir, '*.parquet'))
A:dask.dataframe.io.tests.test_parquet.oe->write_kwargs.pop('object_encoding', None)
A:dask.dataframe.io.tests.test_parquet.ddf['y']->dask.dataframe.read_parquet(tmpdir, dataset={'partitioning': partitioning}, filters=filters).y.astype('category')
A:dask.dataframe.io.tests.test_parquet.cats_set->dask.dataframe.read_parquet(tmp_path, engine=engine, dtype_backend=dtype_backend).map_partitions(lambda x: x.y.cat.categories.sort_values()).compute()
A:dask.dataframe.io.tests.test_parquet.tstamp->pandas.Timestamp(1513393355, unit='s')
A:dask.dataframe.io.tests.test_parquet.tz_tstamp->pandas.Timestamp(1513393355, unit='s', tz=timezone)
A:dask.dataframe.io.tests.test_parquet.schema->pyarrow.schema([('b', pa.string())])
A:dask.dataframe.io.tests.test_parquet.ddf_after_write->dask.dataframe.read_parquet(str(tmpdir), calculate_divisions=False).compute().reset_index(drop=True)
A:dask.dataframe.io.tests.test_parquet.df_out->dask.dataframe.read_parquet(tmpdir, calculate_divisions=True)
A:dask.dataframe.io.tests.test_parquet.msg->str(rec.value)
A:dask.dataframe.io.tests.test_parquet.dftest->pandas.DataFrame({'dummy': [1, 1, 1, 1], 'DatePart': pd.Categorical(cats, categories=cats, ordered=True)})
A:dask.dataframe.io.tests.test_parquet.ddftest->dask.dataframe.from_pandas(dftest, npartitions=4).set_index('dummy')
A:dask.dataframe.io.tests.test_parquet.ddftest_read->dask.dataframe.read_parquet(tmpdir, index='dummy', engine=read_engine, filters=[('DatePart', '<=', '2018-01-02')], calculate_divisions=True)
A:dask.dataframe.io.tests.test_parquet.a->dask.dataframe.read_parquet(tmp_path, engine=read_engine, filters=[('x', '>', 4)])
A:dask.dataframe.io.tests.test_parquet.b->numpy.arange(npartitions, dtype='int32').repeat(size // npartitions)
A:dask.dataframe.io.tests.test_parquet.c->dask.dataframe.read_parquet(tmp_path, engine=read_engine, filters=[('y', '==', 'c'), ('x', '>', 6)])
A:dask.dataframe.io.tests.test_parquet.e->dask.dataframe.read_parquet(tmp_path, engine=read_engine, filters=[('x', 'in', (0, 9))])
A:dask.dataframe.io.tests.test_parquet.f->dask.dataframe.read_parquet(tmp_path, engine=read_engine, filters=[('y', '=', 'c')])
A:dask.dataframe.io.tests.test_parquet.g->dask.dataframe.read_parquet(tmp_path, engine=read_engine, filters=[('x', '!=', 1)])
A:dask.dataframe.io.tests.test_parquet.ddf_out->dask.dataframe.read_parquet(files, calculate_divisions=True, engine=engine, filters=[('x', '>', 3)])
A:dask.dataframe.io.tests.test_parquet.value->dask.dataframe.read_parquet(tmpdir, dataset={'partitioning': partitioning}, filters=filters).to_parquet(tmpdir, compute=False, engine=engine)
A:dask.dataframe.io.tests.test_parquet.invalidate_cache->MagicMock()
A:dask.dataframe.io.tests.test_parquet.path->str(tmpdir)
A:dask.dataframe.io.tests.test_parquet.pf->fastparquet.ParquetFile(fn)
A:dask.dataframe.io.tests.test_parquet.dn->os.path.join(fn, d)
A:dask.dataframe.io.tests.test_parquet.rddf->dask.dataframe.read_parquet(key2, filesystem=fs)
A:dask.dataframe.io.tests.test_parquet.metadata->pyarrow.parquet.read_metadata(os.path.join(filename, '_metadata'))
A:dask.dataframe.io.tests.test_parquet.row_group->pyarrow.parquet.read_metadata(os.path.join(filename, '_metadata')).row_group(i)
A:dask.dataframe.io.tests.test_parquet.column->pyarrow.parquet.read_metadata(os.path.join(filename, '_metadata')).row_group(i).column(j)
A:dask.dataframe.io.tests.test_parquet.(index_names, column_names, mapping, column_index_names)->_parse_pandas_metadata(md)
A:dask.dataframe.io.tests.test_parquet.(index_names, column_names, storage_name_mapping, column_index_names)->_parse_pandas_metadata(md)
A:dask.dataframe.io.tests.test_parquet.path1->os.path.join(path1, 'data.parquet')
A:dask.dataframe.io.tests.test_parquet.path2->os.path.join(path2, 'data.parquet')
A:dask.dataframe.io.tests.test_parquet.df_partitioned->dask.dataframe.read_parquet(fn, engine=engine)
A:dask.dataframe.io.tests.test_parquet.pdf->pandas.read_parquet(tmpdir, partitioning=pd_partitioning(**partitioning), filters=filters)
A:dask.dataframe.io.tests.test_parquet.df['x']->df['x'].astype('category').astype('category')
A:dask.dataframe.io.tests.test_parquet.numbers->numpy.random.randint(0, 800000, size=1000000)
A:dask.dataframe.io.tests.test_parquet.df.name->pandas.DataFrame({'a': [1, 2], 'b': [{'a': 2}, {'a': 4}]}).name.astype('category')
A:dask.dataframe.io.tests.test_parquet.paths->glob.glob(os.path.join(tmp_path, '*.parquet'))
A:dask.dataframe.io.tests.test_parquet.ddf2.name->dask.dataframe.read_parquet(tmp_path, engine=engine, dtype_backend=dtype_backend).name.where(ddf2.timestamp == '2000-01-01', None)
A:dask.dataframe.io.tests.test_parquet.ddf_read->dask.dataframe.read_parquet(str(tmpdir), dtype_backend='numpy_nullable', dataset={'partitioning': {'flavor': 'hive', 'schema': pa.schema([('id', pa.int64())])}})
A:dask.dataframe.io.tests.test_parquet.tmp_path_rd->str(tmpdir.mkdir('read'))
A:dask.dataframe.io.tests.test_parquet.tmp_path_wt->str(tmpdir.mkdir('write'))
A:dask.dataframe.io.tests.test_parquet.dsk->optimize_dataframe_getitem(ddf.dask, keys=[(ddf._name, 0)])
A:dask.dataframe.io.tests.test_parquet.subgraph_rd->hlg_layer(dsk, 'read-parquet')
A:dask.dataframe.io.tests.test_parquet.subgraph_wt->hlg_layer(dsk, 'to-parquet')
A:dask.dataframe.io.tests.test_parquet.subgraph->next((l for l in dsk.layers.values() if isinstance(l, DataFrameIOLayer)))
A:dask.dataframe.io.tests.test_parquet.(a1, a2, a3)->dask.compute(a, b, c)
A:dask.dataframe.io.tests.test_parquet.(b1, b2, b3)->dask.compute(a, b, c, optimize_graph=False)
A:dask.dataframe.io.tests.test_parquet.kwargs->numpy.iinfo(np.dtype('int64')).get('kwargs', {})
A:dask.dataframe.io.tests.test_parquet.layer->hlg_layer(ddf2.dask, 'read-parquet')
A:dask.dataframe.io.tests.test_parquet.graph->optimize_blockwise(ddf.__dask_graph__(), keys)
A:dask.dataframe.io.tests.test_parquet.expected_rg_cout->int(half_size / row_group_size)
A:dask.dataframe.io.tests.test_parquet.npartitions_expected->math.ceil(size / row_group_size / split_row_groups)
A:dask.dataframe.io.tests.test_parquet.result_isna->dask.dataframe.read_parquet(path, filters=[('a', 'is', np.nan)], split_row_groups=split_row_groups)
A:dask.dataframe.io.tests.test_parquet.result_notna->dask.dataframe.read_parquet(path, filters=[('a', 'is not', np.nan)], split_row_groups=split_row_groups)
A:dask.dataframe.io.tests.test_parquet.df2a->df2['a'].groupby(df2['c']).first().to_delayed()
A:dask.dataframe.io.tests.test_parquet.df2b->df2['b'].groupby(df2['c']).first().to_delayed()
A:dask.dataframe.io.tests.test_parquet.df2c->df2[['a', 'b']].rolling(2).max().to_delayed()
A:dask.dataframe.io.tests.test_parquet.df2d->dask.dataframe.from_delayed([df_c, df_d]).rolling(2).max().to_delayed()
A:dask.dataframe.io.tests.test_parquet.(result,)->dask.compute(df2a + df2b + df2c + df2d)
A:dask.dataframe.io.tests.test_parquet.dirname->str(tmpdir)
A:dask.dataframe.io.tests.test_parquet.outpath->os.path.join(str(tmpdir), 'out')
A:dask.dataframe.io.tests.test_parquet.sizep0->sum([md.row_group(rg).total_byte_size for rg in range(md.num_row_groups)])
A:dask.dataframe.io.tests.test_parquet.(a, b)->dask.optimize(ddf['A'], ddf)
A:dask.dataframe.io.tests.test_parquet.df_write->pandas.DataFrame({'id': [1, 2, 3, 4] * 4, 'time': np.arange(16), 'random': np.random.choice(['cat', 'dog'], size=16)})
A:dask.dataframe.io.tests.test_parquet.ddf_write->dask.dataframe.from_pandas(df_write, npartitions=4)
A:dask.dataframe.io.tests.test_parquet.df_read->dask.dataframe.read_parquet(str(tmpdir), dtype_backend='numpy_nullable', dataset={'partitioning': {'flavor': 'hive', 'schema': pa.schema([('id', pa.int64())])}}).compute()
A:dask.dataframe.io.tests.test_parquet.info->numpy.iinfo(np.dtype('int64'))
A:dask.dataframe.io.tests.test_parquet.ctx->contextlib.nullcontext()
A:dask.dataframe.io.tests.test_parquet.arr_numeric->numpy.linspace(start=info.min + 2, stop=info.max, num=1024, dtype='int64')
A:dask.dataframe.io.tests.test_parquet.arr_dates->numpy.linspace(start=info.min + 2, stop=info.max, num=1024, dtype='int64').astype('datetime64[ms]')
A:dask.dataframe.io.tests.test_parquet.new_array->new_array.cast(original_type).cast(original_type)
A:dask.dataframe.io.tests.test_parquet.fixed_arrow_table->cls.clamp_arrow_datetimes(arrow_table)
A:dask.dataframe.io.tests.test_parquet.got->pytest.importorskip('dask_cudf').read_parquet(fn)
A:dask.dataframe.io.tests.test_parquet.path0->os.path.join(path0, 'data.parquet')
A:dask.dataframe.io.tests.test_parquet._df1->pandas.DataFrame({'part': 'a', 'col': range(5)})
A:dask.dataframe.io.tests.test_parquet._df2->pandas.DataFrame({'part': 'b', 'col': range(5)})
A:dask.dataframe.io.tests.test_parquet.t1->pyarrow.Table.from_pandas(_df1[write_cols], preserve_index=False).replace_schema_metadata(metadata={})
A:dask.dataframe.io.tests.test_parquet.t2->pyarrow.Table.from_pandas(_df2[write_cols], preserve_index=False).replace_schema_metadata(metadata={})
A:dask.dataframe.io.tests.test_parquet.result['part']->result['part'].astype('object').astype('object')
A:dask.dataframe.io.tests.test_parquet.expect['B']->expect['B'].astype(pd.CategoricalDtype(expect['B'].dtype.categories.astype('int64'))).astype(pd.CategoricalDtype(expect['B'].dtype.categories.astype('int64')))
A:dask.dataframe.io.tests.test_parquet.df['b']->df['b'].astype('category').astype('category')
A:dask.dataframe.io.tests.test_parquet.ddf.index->dask.dataframe.read_parquet(tmpdir, dataset={'partitioning': partitioning}, filters=filters).index.astype('Int64')
A:dask.dataframe.io.tests.test_parquet.read_df_1->dask.dataframe.read_parquet(fn, filters=[('b', '==', 'a')], read_from_paths=False)
A:dask.dataframe.io.tests.test_parquet.read_df_2->dask.dataframe.read_parquet(fn, filters=[('b', '==', 'a')])
A:dask.dataframe.io.tests.test_parquet.read_ddf->dask.dataframe.read_parquet(tmpdir, engine=engine, filters=[[('part', '==', 'c')]])
A:dask.dataframe.io.tests.test_parquet.read_ddf['part']->read_ddf['part'].astype('object').astype('object')
A:dask.dataframe.io.tests.test_parquet.df_a->dask.delayed(pd.DataFrame.from_dict)({'x': [1, 1, 2, 2], 'y': [1, 0, 1, 0]}, dtype=('int64', 'int64'))
A:dask.dataframe.io.tests.test_parquet.df_b->dask.delayed(pd.DataFrame.from_dict)({'x': [1, 2, 1, 2], 'y': [2, 0, 2, 0]}, dtype=('int64', 'int64'))
A:dask.dataframe.io.tests.test_parquet.df_c->dask.delayed(pd.DataFrame.from_dict)({'x': [], 'y': []}, dtype=('int64', 'int64'))
A:dask.dataframe.io.tests.test_parquet.schema_common->pyarrow.parquet.ParquetFile(os.path.join(tmpdir, '_common_metadata')).schema.to_arrow_schema()
A:dask.dataframe.io.tests.test_parquet.df_d->dask.delayed(pd.DataFrame.from_dict)({'x': [3, 3, 4, 4], 'y': [1, 0, 1, 0]}, dtype=('int64', 'int64'))
A:dask.dataframe.io.tests.test_parquet.ddf2.a->dask.dataframe.read_parquet(tmp_path, engine=engine, dtype_backend=dtype_backend).a.astype('object')
A:dask.dataframe.io.tests.test_parquet.fmd->dask.dataframe.io.parquet.create_metadata_file(fns, engine='pyarrow', split_every=3, out_dir=False)
A:dask.dataframe.io.tests.test_parquet.files_->Path(tmpdir).rglob('*')
A:dask.dataframe.io.tests.test_parquet.files2_->Path(tmpdir).rglob('*')
A:dask.dataframe.io.tests.test_parquet.path_new->os.path.join(str(tmpdir), 'path_new')
A:dask.dataframe.io.tests.test_parquet.subdir->str(tmpdir).mkdir('subdir')
A:dask.dataframe.io.tests.test_parquet.ddf2['year']->dask.dataframe.read_parquet(tmp_path, engine=engine, dtype_backend=dtype_backend).year.astype('int64')
A:dask.dataframe.io.tests.test_parquet.dataset_with_bad_metadata->os.path.join(tmpdir, 'data1')
A:dask.dataframe.io.tests.test_parquet.dataset_without_metadata->os.path.join(tmpdir, 'data2')
A:dask.dataframe.io.tests.test_parquet.ddf2a->dask.dataframe.read_parquet(str(tmpdir), engine=engine, calculate_divisions=True)
A:dask.dataframe.io.tests.test_parquet.ddf2b->dask.dataframe.read_parquet(str(tmpdir), engine=engine, calculate_divisions=True, metadata_task_size=metadata_task_size)
A:dask.dataframe.io.tests.test_parquet.ddf2c->dask.dataframe.read_parquet(str(tmpdir), engine=engine, calculate_divisions=True)
A:dask.dataframe.io.tests.test_parquet.ddf0->dask.dataframe.from_pandas(pd.DataFrame({'a': range(10)}), 1)
A:dask.dataframe.io.tests.test_parquet.expected_pdf->pandas.DataFrame({'num1': [1, 2, 3, 4, 33], 'num2': [7, 8, 9, 10, 44]})
A:dask.dataframe.io.tests.test_parquet.actual->dask.dataframe.read_parquet(fn, engine=engine, index=False)
A:dask.dataframe.io.tests.test_parquet.dask_path->str(tmpdir).mkdir('foo-dask')
A:dask.dataframe.io.tests.test_parquet.pa_path->str(tmpdir).mkdir('foo-pyarrow')
A:dask.dataframe.io.tests.test_parquet.df_read_dask->dask.dataframe.read_parquet(pa_path, engine=engine)
A:dask.dataframe.io.tests.test_parquet.df_read_pa->pyarrow.parquet.read_table(pa_path).to_pandas()
A:dask.dataframe.io.tests.test_parquet.cudf->pytest.importorskip('cudf')
A:dask.dataframe.io.tests.test_parquet.dask_cudf->pytest.importorskip('dask_cudf')
A:dask.dataframe.io.tests.test_parquet.scalar->dask.dataframe.read_parquet(tmpdir, dataset={'partitioning': partitioning}, filters=filters).to_parquet(remote_fn, compute=False, storage_options=storage_options)
A:dask.dataframe.io.tests.test_parquet.key2->str(tmp_path / 'write1')
A:dask.dataframe.io.tests.test_parquet.fs->get_filesystem_class('memory')(use_instance_cache=False)
A:dask.dataframe.io.tests.test_parquet.partitioning->dict(flavor='hive', schema=schema)
dask.dataframe.io.tests.test_parquet.check_compression(engine,filename,compression)
dask.dataframe.io.tests.test_parquet.engine(request)
dask.dataframe.io.tests.test_parquet.pandas_metadata(request)
dask.dataframe.io.tests.test_parquet.read_engine(request)
dask.dataframe.io.tests.test_parquet.test_append(tmpdir,engine,metadata_file)
dask.dataframe.io.tests.test_parquet.test_append_cat_fp(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_append_create(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_append_dict_column(tmpdir)
dask.dataframe.io.tests.test_parquet.test_append_different_columns(tmpdir,engine,metadata_file)
dask.dataframe.io.tests.test_parquet.test_append_known_divisions_to_unknown_divisions_works(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_append_overlapping_divisions(tmpdir,engine,metadata_file,index,offset)
dask.dataframe.io.tests.test_parquet.test_append_with_partition(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_append_wo_index(tmpdir,engine,metadata_file)
dask.dataframe.io.tests.test_parquet.test_arrow_partitioning(tmpdir)
dask.dataframe.io.tests.test_parquet.test_arrow_to_pandas(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_blocksize(tmpdir,blocksize,engine,metadata)
dask.dataframe.io.tests.test_parquet.test_blockwise_parquet_annotations(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_calculate_divisions_false(tmpdir,write_engine,read_engine)
dask.dataframe.io.tests.test_parquet.test_calculate_divisions_no_index(tmpdir,write_engine,read_engine)
dask.dataframe.io.tests.test_parquet.test_categorical(tmpdir,write_engine,read_engine)
dask.dataframe.io.tests.test_parquet.test_categories(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_categories_large(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_categories_unnamed_index(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_columns_auto_index(tmpdir,write_engine,read_engine)
dask.dataframe.io.tests.test_parquet.test_columns_index(tmpdir,write_engine,read_engine)
dask.dataframe.io.tests.test_parquet.test_columns_index_with_multi_index(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_columns_name(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_columns_no_index(tmpdir,write_engine,read_engine)
dask.dataframe.io.tests.test_parquet.test_create_metadata_file(tmpdir,write_engine,read_engine,partition_on)
dask.dataframe.io.tests.test_parquet.test_custom_filename(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_custom_filename_with_partition(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_custom_filename_works_with_pyarrow_when_append_is_true(tmpdir)
dask.dataframe.io.tests.test_parquet.test_custom_metadata(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_datasets_timeseries(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_delayed_no_metadata(tmpdir,write_engine,read_engine)
dask.dataframe.io.tests.test_parquet.test_deprecate_gather_statistics(tmp_path,engine)
dask.dataframe.io.tests.test_parquet.test_dir_filter(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_divisions_are_known_read_with_filters(tmpdir)
dask.dataframe.io.tests.test_parquet.test_divisions_read_with_filters(tmpdir)
dask.dataframe.io.tests.test_parquet.test_divisions_with_null_partition(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_drill_scheme(tmpdir)
dask.dataframe.io.tests.test_parquet.test_dtype_backend(tmp_path,dtype_backend,engine)
dask.dataframe.io.tests.test_parquet.test_dtype_backend_categoricals(tmp_path)
dask.dataframe.io.tests.test_parquet.test_empty(tmpdir,write_engine,read_engine,index)
dask.dataframe.io.tests.test_parquet.test_empty_partition(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_extra_file(tmpdir,engine,partition_on)
dask.dataframe.io.tests.test_parquet.test_filesystem_option(tmp_path,engine,fs)
dask.dataframe.io.tests.test_parquet.test_filter_isna(tmpdir,split_row_groups)
dask.dataframe.io.tests.test_parquet.test_filter_nonpartition_columns(tmpdir,write_engine,read_engine,calculate_divisions)
dask.dataframe.io.tests.test_parquet.test_filter_nulls(tmpdir,filters,op,length,split_row_groups,engine)
dask.dataframe.io.tests.test_parquet.test_filtering_pyarrow_dataset(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_filters(tmpdir,write_engine,read_engine)
dask.dataframe.io.tests.test_parquet.test_filters_categorical(tmpdir,write_engine,read_engine)
dask.dataframe.io.tests.test_parquet.test_filters_file_list(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_filters_v0(tmpdir,write_engine,read_engine)
dask.dataframe.io.tests.test_parquet.test_from_pandas_preserve_none_index(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_from_pandas_preserve_none_rangeindex(tmpdir,write_engine,read_engine)
dask.dataframe.io.tests.test_parquet.test_fsspec_to_parquet_filesystem_option(tmp_path)
dask.dataframe.io.tests.test_parquet.test_get_engine_fastparquet()
dask.dataframe.io.tests.test_parquet.test_get_engine_fastparquet_only()
dask.dataframe.io.tests.test_parquet.test_get_engine_invalid()
dask.dataframe.io.tests.test_parquet.test_get_engine_no_engine()
dask.dataframe.io.tests.test_parquet.test_get_engine_pyarrow()
dask.dataframe.io.tests.test_parquet.test_get_engine_third_party()
dask.dataframe.io.tests.test_parquet.test_getitem_optimization(tmpdir,engine,preserve_index,index)
dask.dataframe.io.tests.test_parquet.test_getitem_optimization_after_filter(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_getitem_optimization_after_filter_complex(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_getitem_optimization_empty(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_getitem_optimization_multi(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_gpu_write_parquet_simple(tmpdir)
dask.dataframe.io.tests.test_parquet.test_graph_size_pyarrow(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_ignore_metadata_file(tmpdir,engine,calculate_divisions)
dask.dataframe.io.tests.test_parquet.test_illegal_column_name(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_in_predicate_can_use_iterables(tmp_path,engine,filter_value)
dask.dataframe.io.tests.test_parquet.test_in_predicate_requires_an_iterable(tmp_path,engine,filter_value)
dask.dataframe.io.tests.test_parquet.test_informative_error_messages()
dask.dataframe.io.tests.test_parquet.test_layer_creation_info(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_local(tmpdir,write_engine,read_engine,has_metadata)
dask.dataframe.io.tests.test_parquet.test_metadata_task_size(tmpdir,engine,write_metadata_file,metadata_task_size)
dask.dataframe.io.tests.test_parquet.test_multi_partition_none_index_false(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_names(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_no_index(tmpdir,write_engine,read_engine)
dask.dataframe.io.tests.test_parquet.test_non_categorical_partitioning_pyarrow(tmpdir,filters)
dask.dataframe.io.tests.test_parquet.test_nonsense_column(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_not_in_predicate(tmp_path,engine)
dask.dataframe.io.tests.test_parquet.test_null_partition_pyarrow(tmpdir,scheduler)
dask.dataframe.io.tests.test_parquet.test_optimize_and_not(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_optimize_blockwise_parquet(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_optimize_getitem_and_nonblockwise(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_ordering(tmpdir,write_engine,read_engine)
dask.dataframe.io.tests.test_parquet.test_pandas_metadata_nullable_pyarrow(tmpdir)
dask.dataframe.io.tests.test_parquet.test_pandas_timestamp_overflow_pyarrow(tmpdir)
dask.dataframe.io.tests.test_parquet.test_parquet_pyarrow_write_empty_metadata(tmpdir)
dask.dataframe.io.tests.test_parquet.test_parquet_pyarrow_write_empty_metadata_append(tmpdir)
dask.dataframe.io.tests.test_parquet.test_parquet_select_cats(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_parse_pandas_metadata(pandas_metadata)
dask.dataframe.io.tests.test_parquet.test_parse_pandas_metadata_column_with_index_name()
dask.dataframe.io.tests.test_parquet.test_parse_pandas_metadata_duplicate_index_columns()
dask.dataframe.io.tests.test_parquet.test_parse_pandas_metadata_null_index()
dask.dataframe.io.tests.test_parquet.test_partition_on(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_partition_on_cats(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_partition_on_cats_2(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_partition_on_cats_pyarrow(tmpdir,stats,meta)
dask.dataframe.io.tests.test_parquet.test_partition_on_duplicates(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_partition_on_string(tmpdir,partition_on)
dask.dataframe.io.tests.test_parquet.test_partition_parallel_metadata(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_partitioned_column_overlap(tmpdir,engine,write_cols)
dask.dataframe.io.tests.test_parquet.test_partitioned_no_pandas_metadata(tmpdir,engine,write_cols)
dask.dataframe.io.tests.test_parquet.test_partitioned_preserve_index(tmpdir,write_engine,read_engine)
dask.dataframe.io.tests.test_parquet.test_pathlib_path(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_pyarrow_dataset_filter_on_partitioned(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_pyarrow_dataset_filter_partitioned(tmpdir,split_row_groups)
dask.dataframe.io.tests.test_parquet.test_pyarrow_dataset_partitioned(tmpdir,engine,test_filter)
dask.dataframe.io.tests.test_parquet.test_pyarrow_dataset_read_from_paths(tmpdir)
dask.dataframe.io.tests.test_parquet.test_pyarrow_dataset_simple(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_pyarrow_directory_partitioning(tmpdir)
dask.dataframe.io.tests.test_parquet.test_pyarrow_filesystem_option(tmp_path,fs)
dask.dataframe.io.tests.test_parquet.test_pyarrow_filesystem_option_real_data()
dask.dataframe.io.tests.test_parquet.test_pyarrow_filter_divisions(tmpdir)
dask.dataframe.io.tests.test_parquet.test_pyarrow_schema_inference(tmpdir,index,schema)
dask.dataframe.io.tests.test_parquet.test_pyarrow_schema_mismatch_error(tmpdir)
dask.dataframe.io.tests.test_parquet.test_pyarrow_schema_mismatch_explicit_schema_none(tmpdir)
dask.dataframe.io.tests.test_parquet.test_read_dir_nometa(tmpdir,write_engine,read_engine,divisions,remove_common)
dask.dataframe.io.tests.test_parquet.test_read_glob(tmpdir,write_engine,read_engine)
dask.dataframe.io.tests.test_parquet.test_read_glob_no_meta(tmpdir,write_engine,read_engine)
dask.dataframe.io.tests.test_parquet.test_read_glob_yes_meta(tmpdir,write_engine,read_engine)
dask.dataframe.io.tests.test_parquet.test_read_list(tmpdir,write_engine,read_engine)
dask.dataframe.io.tests.test_parquet.test_read_no_metadata(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_read_pandas_fastparquet_partitioned(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_read_parquet_convert_string(tmp_path,convert_string,engine)
dask.dataframe.io.tests.test_parquet.test_read_parquet_convert_string_nullable_mapper(tmp_path,engine)
dask.dataframe.io.tests.test_parquet.test_read_parquet_custom_columns(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_read_parquet_getitem_skip_when_getting_read_parquet(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_read_parquet_lists_not_converting(tmpdir)
dask.dataframe.io.tests.test_parquet.test_read_parquet_preserve_categorical_column_dtype(tmp_path)
dask.dataframe.io.tests.test_parquet.test_read_series(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_read_write_overwrite_is_true(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_read_write_partition_on_overwrite_is_true(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_retries_on_remote_filesystem(tmpdir)
dask.dataframe.io.tests.test_parquet.test_roundtrip(tmpdir,df,write_kwargs,read_kwargs,engine)
dask.dataframe.io.tests.test_parquet.test_roundtrip_arrow(tmpdir,df)
dask.dataframe.io.tests.test_parquet.test_roundtrip_date_dtype(tmpdir)
dask.dataframe.io.tests.test_parquet.test_roundtrip_decimal_dtype(tmpdir)
dask.dataframe.io.tests.test_parquet.test_roundtrip_from_pandas(tmpdir,write_engine,read_engine)
dask.dataframe.io.tests.test_parquet.test_roundtrip_nullable_dtypes(tmp_path)
dask.dataframe.io.tests.test_parquet.test_roundtrip_pandas_blocksize(tmpdir,write_engine,read_engine)
dask.dataframe.io.tests.test_parquet.test_roundtrip_partitioned_pyarrow_dataset(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_roundtrip_rename_columns(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_select_filtered_column(tmp_path,engine)
dask.dataframe.io.tests.test_parquet.test_select_filtered_column_no_stats(tmp_path,engine)
dask.dataframe.io.tests.test_parquet.test_select_partitioned_column(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_simple(tmpdir,write_engine,read_engine)
dask.dataframe.io.tests.test_parquet.test_split_adaptive_aggregate_files(tmpdir,write_engine,read_engine,aggregate_files)
dask.dataframe.io.tests.test_parquet.test_split_adaptive_blocksize(tmpdir,blocksize,engine,metadata)
dask.dataframe.io.tests.test_parquet.test_split_adaptive_empty(tmpdir,write_engine,read_engine)
dask.dataframe.io.tests.test_parquet.test_split_adaptive_files(tmpdir,blocksize,partition_on,write_engine,read_engine,metadata)
dask.dataframe.io.tests.test_parquet.test_split_row_groups(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_split_row_groups_filter(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_split_row_groups_int(tmpdir,split_row_groups,calculate_divisions,engine)
dask.dataframe.io.tests.test_parquet.test_split_row_groups_int_aggregate_files(tmpdir,engine,split_row_groups)
dask.dataframe.io.tests.test_parquet.test_statistics_nometa(tmpdir,write_engine,read_engine)
dask.dataframe.io.tests.test_parquet.test_throws_error_if_custom_filename_is_invalid(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_timeseries_nulls_in_schema(tmpdir,engine,schema)
dask.dataframe.io.tests.test_parquet.test_timestamp96(tmpdir)
dask.dataframe.io.tests.test_parquet.test_timestamp_index(tmpdir,engine,write_metadata)
dask.dataframe.io.tests.test_parquet.test_to_parquet_calls_invalidate_cache(tmpdir,monkeypatch,compute)
dask.dataframe.io.tests.test_parquet.test_to_parquet_errors_non_string_column_names(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_to_parquet_fastparquet_default_writes_nulls(tmpdir)
dask.dataframe.io.tests.test_parquet.test_to_parquet_lazy(tmpdir,scheduler,engine)
dask.dataframe.io.tests.test_parquet.test_to_parquet_overwrite_adaptive_round_trip(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_to_parquet_overwrite_files_from_read_parquet_in_same_call_raises(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_to_parquet_overwrite_raises(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_to_parquet_pyarrow_w_inconsistent_schema_by_partition_succeeds_w_manual_schema(tmpdir)
dask.dataframe.io.tests.test_parquet.test_to_parquet_with_get(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_unsupported_extension_dir(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_unsupported_extension_file(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_use_nullable_dtypes(tmp_path,dtype_backend,engine)
dask.dataframe.io.tests.test_parquet.test_use_nullable_dtypes_with_types_mapper(tmp_path,engine)
dask.dataframe.io.tests.test_parquet.test_with_tz(tmpdir)
dask.dataframe.io.tests.test_parquet.test_writing_parquet_with_compression(tmpdir,compression,engine)
dask.dataframe.io.tests.test_parquet.test_writing_parquet_with_kwargs(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.test_writing_parquet_with_partition_on_and_compression(tmpdir,compression,engine)
dask.dataframe.io.tests.test_parquet.test_writing_parquet_with_unknown_kwargs(tmpdir,engine)
dask.dataframe.io.tests.test_parquet.write_engine(request)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/io/tests/test_demo.py----------------------------------------
A:dask.dataframe.io.tests.test_demo.df->dask.dataframe.demo.make_timeseries()
A:dask.dataframe.io.tests.test_demo.a->dask.dataframe.demo.make_timeseries('2000', '2015', {'A': float, 'B': int, 'C': str}, freq='2D', partition_freq=f'6{ME}', seed=123)
A:dask.dataframe.io.tests.test_demo.b->dask.dataframe.demo.make_timeseries('2000', '2015', {'A': float, 'B': int, 'C': str}, freq='2D', partition_freq=f'6{ME}', seed=123)
A:dask.dataframe.io.tests.test_demo.c->dask.dataframe.demo.make_timeseries('2000', '2015', {'A': float, 'B': int, 'C': str}, freq='2D', partition_freq=f'6{ME}', seed=456)
A:dask.dataframe.io.tests.test_demo.d->dask.dataframe.demo.make_timeseries('2000', '2015', {'A': float, 'B': int, 'C': str}, freq='2D', partition_freq=f'3{ME}', seed=123)
A:dask.dataframe.io.tests.test_demo.e->dask.dataframe.demo.make_timeseries('2000', '2015', {'A': float, 'B': int, 'C': str}, freq='1D', partition_freq=f'6{ME}', seed=123)
A:dask.dataframe.io.tests.test_demo.graph->optimize_blockwise(df.__dask_graph__(), keys)
A:dask.dataframe.io.tests.test_demo.a_cardinality->dask.dataframe.demo.make_timeseries().A_B.nunique()
A:dask.dataframe.io.tests.test_demo.b_cardinality->dask.dataframe.demo.make_timeseries().B_.nunique()
A:dask.dataframe.io.tests.test_demo.(aa, bb)->dask.compute(a_cardinality, b_cardinality, scheduler='single-threaded')
A:dask.dataframe.io.tests.test_demo.df3->df2.compute()
A:dask.dataframe.io.tests.test_demo.ddf->with_spec(spec, seed=42)
A:dask.dataframe.io.tests.test_demo.spec->DatasetSpec(nrecords=10, index_spec=DatetimeIndexSpec(dtype='datetime64[ns]', freq='1h', start='2023-01-02', partition_freq='1D'), column_specs=[ColumnSpec(dtype=int)])
A:dask.dataframe.io.tests.test_demo.res->with_spec(spec, seed=42).compute()
dask.dataframe.io.tests.test_demo.test_make_timeseries()
dask.dataframe.io.tests.test_demo.test_make_timeseries_blockwise()
dask.dataframe.io.tests.test_demo.test_make_timeseries_column_projection()
dask.dataframe.io.tests.test_demo.test_make_timeseries_fancy_keywords()
dask.dataframe.io.tests.test_demo.test_make_timeseries_getitem_compute()
dask.dataframe.io.tests.test_demo.test_make_timeseries_keywords()
dask.dataframe.io.tests.test_demo.test_make_timeseries_no_args()
dask.dataframe.io.tests.test_demo.test_no_overlaps()
dask.dataframe.io.tests.test_demo.test_same_prefix_col_numbering(seed)
dask.dataframe.io.tests.test_demo.test_with_spec(seed)
dask.dataframe.io.tests.test_demo.test_with_spec_category_nunique()
dask.dataframe.io.tests.test_demo.test_with_spec_datetime_index()
dask.dataframe.io.tests.test_demo.test_with_spec_default_integer(seed)
dask.dataframe.io.tests.test_demo.test_with_spec_integer_method()
dask.dataframe.io.tests.test_demo.test_with_spec_non_default(seed)
dask.dataframe.io.tests.test_demo.test_with_spec_pyarrow()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/io/tests/test_orc.py----------------------------------------
A:dask.dataframe.io.tests.test_orc.pa->pytest.importorskip('pyarrow')
A:dask.dataframe.io.tests.test_orc.d->dask.dataframe.read_orc(orc_files[0])
A:dask.dataframe.io.tests.test_orc.requests->pytest.importorskip('requests')
A:dask.dataframe.io.tests.test_orc.d2->dask.dataframe.read_orc(os.path.dirname(orc_files[0]) + '/*.orc')
A:dask.dataframe.io.tests.test_orc.graph->optimize_dataframe_getitem(d3.__dask_graph__(), keys)
A:dask.dataframe.io.tests.test_orc.tmp->str(tmpdir)
A:dask.dataframe.io.tests.test_orc.data->pandas.DataFrame({'a': np.arange(100, dtype=np.float64), 'b': np.random.choice(['cat', 'dog', 'mouse'], size=100)})
A:dask.dataframe.io.tests.test_orc.df->pandas.DataFrame(np.random.randn(100, 4), columns=['a', 'b', 'c', 'd'])
A:dask.dataframe.io.tests.test_orc.df2->dask.dataframe.read_orc(orc_files[:2], split_stripes=11, aggregate_files=True)
A:dask.dataframe.io.tests.test_orc.out->pandas.DataFrame(np.random.randn(100, 4), columns=['a', 'b', 'c', 'd']).to_orc(tmp_path, compute=False)
A:dask.dataframe.io.tests.test_orc.ddf->dask.dataframe.from_pandas(df, npartitions=4)
A:dask.dataframe.io.tests.test_orc.eager_path->os.path.join(tmp_path, 'eager_orc_dataset')
A:dask.dataframe.io.tests.test_orc.delayed_path->os.path.join(tmp_path, 'delayed_orc_dataset')
A:dask.dataframe.io.tests.test_orc.dataset->dask.dataframe.from_pandas(df, npartitions=4).to_orc(delayed_path, compute=False)
dask.dataframe.io.tests.test_orc.orc_files()
dask.dataframe.io.tests.test_orc.test_orc_aggregate_files_offset(orc_files)
dask.dataframe.io.tests.test_orc.test_orc_multiple(orc_files)
dask.dataframe.io.tests.test_orc.test_orc_names(orc_files,tmp_path)
dask.dataframe.io.tests.test_orc.test_orc_roundtrip(tmpdir,index,columns)
dask.dataframe.io.tests.test_orc.test_orc_roundtrip_aggregate_files(tmpdir,split_stripes)
dask.dataframe.io.tests.test_orc.test_orc_single(orc_files,split_stripes)
dask.dataframe.io.tests.test_orc.test_orc_with_backend()
dask.dataframe.io.tests.test_orc.test_to_orc_delayed(tmp_path)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/io/tests/test_csv.py----------------------------------------
A:dask.dataframe.io.tests.test_csv.pd->pytest.importorskip('pandas')
A:dask.dataframe.io.tests.test_csv.dd->pytest.importorskip('dask.dataframe')
A:dask.dataframe.io.tests.test_csv.csv_text->'\nname,amount\nAlice,100\nBob,-200\nCharlie,300\nDennis,400\nEdith,-500\nFrank,600\nAlice,200\nFrank,-200\nBob,600\nAlice,400\nFrank,200\nAlice,300\nEdith,600\n'.strip()
A:dask.dataframe.io.tests.test_csv.tsv_text->'\nname,amount\nAlice,100\nBob,-200\nCharlie,300\nDennis,400\nEdith,-500\nFrank,600\nAlice,200\nFrank,-200\nBob,600\nAlice,400\nFrank,200\nAlice,300\nEdith,600\n'.strip().replace(',', '\t')
A:dask.dataframe.io.tests.test_csv.tsv_text2->'\nname   amount\nAlice    100\nBob     -200\nCharlie  300\nDennis   400\nEdith   -500\nFrank    600\nAlice    200\nFrank   -200\nBob      600\nAlice    400\nFrank    200\nAlice    300\nEdith    600\n'.strip()
A:dask.dataframe.io.tests.test_csv.timeseries->'\nDate,Open,High,Low,Close,Volume,Adj Close\n2015-08-28,198.50,199.839996,197.919998,199.240005,143298900,199.240005\n2015-08-27,197.020004,199.419998,195.210007,199.160004,266244700,199.160004\n2015-08-26,192.080002,194.789993,188.369995,194.679993,328058100,194.679993\n2015-08-25,195.429993,195.449997,186.919998,187.229996,353966700,187.229996\n2015-08-24,197.630005,197.630005,182.399994,189.550003,478672400,189.550003\n2015-08-21,201.729996,203.940002,197.520004,197.630005,328271500,197.630005\n2015-08-20,206.509995,208.289993,203.899994,204.009995,185865600,204.009995\n2015-08-19,209.089996,210.009995,207.350006,208.279999,167316300,208.279999\n2015-08-18,210.259995,210.679993,209.699997,209.929993,70043800,209.929993\n'.strip()
A:dask.dataframe.io.tests.test_csv.df->pytest.importorskip('pandas').read_csv(path, header=0, names=names, usecols=usecols)
A:dask.dataframe.io.tests.test_csv.tsv_units_row->csv_units_row.replace(b',', b'\t')
A:dask.dataframe.io.tests.test_csv.csv_and_table->pytest.mark.parametrize('reader,files', [(pd.read_csv, csv_files), (pd.read_table, tsv_files), (pd.read_fwf, fwf_files)])
A:dask.dataframe.io.tests.test_csv.(header, b)->dd.read_csv(fn, blocksize='20kB').split(b'\n', 1)
A:dask.dataframe.io.tests.test_csv.head->reader(BytesIO(blocks[0][0]), header=0)
A:dask.dataframe.io.tests.test_csv.values->list(range(len(columns)))
A:dask.dataframe.io.tests.test_csv.result->pytest.importorskip('dask.dataframe').read_csv(os.path.join(tdir, '*.csv'), include_path_column=True, converters={'path': parse_filename}, names=['A', 'B', 'C']).compute()
A:dask.dataframe.io.tests.test_csv.expected->pytest.importorskip('pandas').read_csv(BytesIO(data))
A:dask.dataframe.io.tests.test_csv.lines->dd.read_csv(fn, blocksize='20kB').split(b'\n')
A:dask.dataframe.io.tests.test_csv.skip->len(comment_footer.splitlines())
A:dask.dataframe.io.tests.test_csv.expected_df->read_files_with(files, pd_read, skiprows=skip)
A:dask.dataframe.io.tests.test_csv.dfs->dd_read('2014-01-*.csv', blocksize='10B', include_path_column=True)
A:dask.dataframe.io.tests.test_csv.f->gzip.open(os.path.join(tdir, 'b.csv.gz'), 'wb')
A:dask.dataframe.io.tests.test_csv.ddf->pytest.importorskip('dask.dataframe').read_csv(path, header=0, names=names, usecols=usecols, blocksize=60)
A:dask.dataframe.io.tests.test_csv.df_pyarrow->pytest.importorskip('pandas').read_csv(path, header=0, names=names, usecols=usecols).astype({'name': 'string[pyarrow]'})
A:dask.dataframe.io.tests.test_csv.actual->dd_read(fn, blocksize=200, skiprows=skip, names=names).compute()
A:dask.dataframe.io.tests.test_csv.expected2->pd_read(BytesIO(files[fn]))
A:dask.dataframe.io.tests.test_csv.sol->pytest.importorskip('pandas').read_csv(fn)
A:dask.dataframe.io.tests.test_csv.res->pytest.importorskip('dask.dataframe').read_csv(fn, sample=50, assume_missing=True, dtype=None)
A:dask.dataframe.io.tests.test_csv.filenames->pytest.importorskip('pandas').read_csv(path, header=0, names=names, usecols=usecols).filename.compute().unique()
A:dask.dataframe.io.tests.test_csv.blocks->compute_as_if_collection(dd.DataFrame, f.dask, f.__dask_keys__(), scheduler='sync')
A:dask.dataframe.io.tests.test_csv.a->pytest.importorskip('dask.dataframe').from_pandas(df, npartitions)
A:dask.dataframe.io.tests.test_csv.b->pytest.importorskip('dask.dataframe').read_csv(fn, blocksize='20kB')
A:dask.dataframe.io.tests.test_csv.c->pytest.importorskip('dask.dataframe').read_csv(fn, skiprows=1, na_values=[0])
A:dask.dataframe.io.tests.test_csv.text->'a b c-d\n1 2 3\n4 5 6'.replace(' ', '\t')
A:dask.dataframe.io.tests.test_csv.text1->normalize_text('\n    A,B\n    a,a\n    b,b\n    a,a\n    ')
A:dask.dataframe.io.tests.test_csv.text2->normalize_text('\n    A,B\n    a,a\n    b,b\n    c,c\n    ')
A:dask.dataframe.io.tests.test_csv.string_dtype->get_string_dtype()
A:dask.dataframe.io.tests.test_csv.dtype->pytest.importorskip('pandas').api.types.CategoricalDtype(['a', 'b', 'c'], ordered=True)
A:dask.dataframe.io.tests.test_csv.expected['A']->expected['A'].cat.as_ordered().cat.as_ordered()
A:dask.dataframe.io.tests.test_csv.suffix->{'gzip': '.gz', 'bz2': '.bz2', 'zip': '.zip', 'xz': '.xz'}.get(fmt, '')
A:dask.dataframe.io.tests.test_csv.files2->valmap(compress['gzip'], csv_files)
A:dask.dataframe.io.tests.test_csv.msg->str(w[0].message)
A:dask.dataframe.io.tests.test_csv.psutil->pytest.importorskip('psutil')
A:dask.dataframe.io.tests.test_csv.blocksize->auto_blocksize(1000000000000, 3)
A:dask.dataframe.io.tests.test_csv.cpu_count->pytest.importorskip('psutil').cpu_count()
A:dask.dataframe.io.tests.test_csv.mock_read_bytes->unittest.mock.Mock(wraps=read_bytes)
A:dask.dataframe.io.tests.test_csv.expected_block_size->auto_blocksize(total_memory, cpu_count)
A:dask.dataframe.io.tests.test_csv.xfail_pandas_100->pytest.mark.xfail(reason='https://github.com/dask/dask/issues/5787')
A:dask.dataframe.io.tests.test_csv.ar->pytest.importorskip('pandas').Series(range(0, 100))
A:dask.dataframe.io.tests.test_csv.test_df->pytest.importorskip('pandas').DataFrame({'a': ar, 'b': br, 'c': cr, 'd': dr})
A:dask.dataframe.io.tests.test_csv.d->d.compute().compute()
A:dask.dataframe.io.tests.test_csv.d.index->range(len(d.index))
A:dask.dataframe.io.tests.test_csv.pdmc_text->normalize_text('\n    ID,date,time\n    10,2003-11-04,180036\n    11,2003-11-05,125640\n    12,2003-11-01,2519\n    13,2003-10-22,142559\n    14,2003-10-24,163113\n    15,2003-10-20,170133\n    16,2003-11-11,160448\n    17,2003-11-03,171759\n    18,2003-11-07,190928\n    19,2003-10-21,84623\n    20,2003-10-25,192207\n    21,2003-11-13,180156\n    22,2003-11-15,131037\n    ')
A:dask.dataframe.io.tests.test_csv.sep_text->normalize_text('\n    a,b\n    1,2\n    ')
A:dask.dataframe.io.tests.test_csv.files->csv_files.copy()
A:dask.dataframe.io.tests.test_csv.files[k]->files[k].replace(b'name', b'address').replace(b'name', b'address')
A:dask.dataframe.io.tests.test_csv.r->pytest.importorskip('dask.dataframe').from_pandas(df, npartitions).to_csv(fn, index=False, compute=False, single_file=True)
A:dask.dataframe.io.tests.test_csv.paths->pytest.importorskip('dask.dataframe').read_csv(path, header=0, names=names, usecols=usecols, blocksize=60).to_csv('foo*.csv')
A:dask.dataframe.io.tests.test_csv.fn->os.path.join(dn, 'test.csv.gz')
A:dask.dataframe.io.tests.test_csv.df16->pytest.importorskip('pandas').DataFrame({'x': ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p'], 'y': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]})
A:dask.dataframe.io.tests.test_csv.df0->pytest.importorskip('pandas').Series(['a', 'b', 'c', 'd'], index=[1.0, 2.0, 3.0, 4.0])
A:dask.dataframe.io.tests.test_csv.csv_path->os.path.join(dir, 'test.csv')
A:dask.dataframe.io.tests.test_csv.dir0->os.path.join(str(dir), 'createme')
A:dask.dataframe.io.tests.test_csv.dir->str(dir)
A:dask.dataframe.io.tests.test_csv.df1->pytest.importorskip('pandas').DataFrame([{c: v for (c, v) in zip(columns, values)}])
A:dask.dataframe.io.tests.test_csv.dfe->pytest.importorskip('pandas').DataFrame({'x': [], 'y': []})
A:dask.dataframe.io.tests.test_csv.ddfe->pytest.importorskip('dask.dataframe').from_pandas(dfe, npartitions=1)
A:dask.dataframe.io.tests.test_csv.filename->os.path.join(dn, 'foo0.csv')
A:dask.dataframe.io.tests.test_csv.line->fp.readline()
A:dask.dataframe.io.tests.test_csv.raw->gzip.open(os.path.join(tdir, 'b.csv.gz'), 'wb').read()
A:dask.dataframe.io.tests.test_csv.mask->list(block_mask(block_lists))
A:dask.dataframe.io.tests.test_csv.df['path']->df['path'].astype('category').astype('category')
A:dask.dataframe.io.tests.test_csv.path->os.path.join(str(tmpdir), 'input.csv')
A:dask.dataframe.io.tests.test_csv.ddf1->pytest.importorskip('dask.dataframe').read_csv(path)
A:dask.dataframe.io.tests.test_csv.ddfs->pytest.importorskip('dask.dataframe').read_csv(path).groupby('foo')
A:dask.dataframe.io.tests.test_csv.columns->list('hczzkylaape')
A:dask.dataframe.io.tests.test_csv.df2->pytest.importorskip('dask.dataframe').read_csv(path)[columns].head(1)
A:dask.dataframe.io.tests.test_csv.expect->pytest.importorskip('pandas').read_csv(fn)
A:dask.dataframe.io.tests.test_csv.dsk->optimize_dataframe_getitem(ddf.dask, keys=[ddf._name])
A:dask.dataframe.io.tests.test_csv.subgraph_rd->hlg_layer(dsk, 'read-csv')
A:dask.dataframe.io.tests.test_csv.old_csv_path->os.path.join(str(tmpdir), 'old.csv')
A:dask.dataframe.io.tests.test_csv.new_csv_path->os.path.join(str(tmpdir), 'new_csv')
A:dask.dataframe.io.tests.test_csv.new_df->pytest.importorskip('dask.dataframe').read_csv(new_csv_path, header=None, delimiter=',', dtype=str, blocksize=None)
A:dask.dataframe.io.tests.test_csv.old_df->pytest.importorskip('dask.dataframe').read_csv(old_csv_path, header=None, delimiter=',', dtype=str, blocksize=None)
A:dask.dataframe.io.tests.test_csv.csv->StringIO('    city1,1992-09-13,10\n    city2,1992-09-13,14\n    city3,1992-09-13,98\n    city4,1992-09-13,13\n    city5,1992-09-13,45\n    city6,1992-09-13,64\n    ')
dask.dataframe.io.tests.test_csv.normalize_text(s)
dask.dataframe.io.tests.test_csv.parse_filename(path)
dask.dataframe.io.tests.test_csv.read_files(file_names=csv_files)
dask.dataframe.io.tests.test_csv.read_files_with(file_names,handler,**kwargs)
dask.dataframe.io.tests.test_csv.test__infer_block_size(monkeypatch)
dask.dataframe.io.tests.test_csv.test_assume_missing()
dask.dataframe.io.tests.test_csv.test_auto_blocksize()
dask.dataframe.io.tests.test_csv.test_auto_blocksize_csv(monkeypatch)
dask.dataframe.io.tests.test_csv.test_auto_blocksize_max64mb()
dask.dataframe.io.tests.test_csv.test_block_mask(block_lists)
dask.dataframe.io.tests.test_csv.test_categorical_dtypes()
dask.dataframe.io.tests.test_csv.test_categorical_known()
dask.dataframe.io.tests.test_csv.test_comment(dd_read,pd_read,files)
dask.dataframe.io.tests.test_csv.test_compression_multiple_files(compression)
dask.dataframe.io.tests.test_csv.test_consistent_dtypes()
dask.dataframe.io.tests.test_csv.test_consistent_dtypes_2()
dask.dataframe.io.tests.test_csv.test_csv_getitem_column_order(tmpdir)
dask.dataframe.io.tests.test_csv.test_csv_name_should_be_different_even_if_head_is_same(tmpdir)
dask.dataframe.io.tests.test_csv.test_csv_parse_fail(tmpdir)
dask.dataframe.io.tests.test_csv.test_csv_with_integer_names()
dask.dataframe.io.tests.test_csv.test_different_columns_are_allowed()
dask.dataframe.io.tests.test_csv.test_empty_csv_file()
dask.dataframe.io.tests.test_csv.test_encoding_gh601(encoding)
dask.dataframe.io.tests.test_csv.test_enforce_columns(reader,blocks)
dask.dataframe.io.tests.test_csv.test_enforce_dtypes(reader,blocks)
dask.dataframe.io.tests.test_csv.test_error_if_sample_is_too_small()
dask.dataframe.io.tests.test_csv.test_getitem_optimization_after_filter()
dask.dataframe.io.tests.test_csv.test_head_partial_line_fix()
dask.dataframe.io.tests.test_csv.test_header_None()
dask.dataframe.io.tests.test_csv.test_header_int(header)
dask.dataframe.io.tests.test_csv.test_index_col()
dask.dataframe.io.tests.test_csv.test_late_dtypes()
dask.dataframe.io.tests.test_csv.test_multiple_read_csv_has_deterministic_name()
dask.dataframe.io.tests.test_csv.test_names_with_header_0(tmpdir,use_names)
dask.dataframe.io.tests.test_csv.test_none_usecols()
dask.dataframe.io.tests.test_csv.test_pandas_read_text(reader,files)
dask.dataframe.io.tests.test_csv.test_pandas_read_text_dtype_coercion(reader,files)
dask.dataframe.io.tests.test_csv.test_pandas_read_text_kwargs(reader,files)
dask.dataframe.io.tests.test_csv.test_pandas_read_text_with_header(reader,files)
dask.dataframe.io.tests.test_csv.test_parse_dates_multi_column()
dask.dataframe.io.tests.test_csv.test_read_csv(dd_read,pd_read,text,sep)
dask.dataframe.io.tests.test_csv.test_read_csv_arrow_engine()
dask.dataframe.io.tests.test_csv.test_read_csv_compression(fmt,blocksize)
dask.dataframe.io.tests.test_csv.test_read_csv_convert_string_config()
dask.dataframe.io.tests.test_csv.test_read_csv_files(dd_read,pd_read,files)
dask.dataframe.io.tests.test_csv.test_read_csv_files_list(dd_read,pd_read,files)
dask.dataframe.io.tests.test_csv.test_read_csv_groupby_get_group(tmpdir)
dask.dataframe.io.tests.test_csv.test_read_csv_has_deterministic_name()
dask.dataframe.io.tests.test_csv.test_read_csv_has_different_names_based_on_blocksize()
dask.dataframe.io.tests.test_csv.test_read_csv_header_issue_823()
dask.dataframe.io.tests.test_csv.test_read_csv_include_path_column(dd_read,files)
dask.dataframe.io.tests.test_csv.test_read_csv_include_path_column_as_str(dd_read,files)
dask.dataframe.io.tests.test_csv.test_read_csv_include_path_column_is_dtype_category(dd_read,files)
dask.dataframe.io.tests.test_csv.test_read_csv_include_path_column_with_duplicate_name(dd_read,files)
dask.dataframe.io.tests.test_csv.test_read_csv_include_path_column_with_multiple_partitions_per_file(dd_read,files)
dask.dataframe.io.tests.test_csv.test_read_csv_index()
dask.dataframe.io.tests.test_csv.test_read_csv_large_skiprows(dd_read,pd_read,text,skip)
dask.dataframe.io.tests.test_csv.test_read_csv_names_not_none()
dask.dataframe.io.tests.test_csv.test_read_csv_no_sample()
dask.dataframe.io.tests.test_csv.test_read_csv_raises_on_no_files()
dask.dataframe.io.tests.test_csv.test_read_csv_sensitive_to_enforce()
dask.dataframe.io.tests.test_csv.test_read_csv_sep()
dask.dataframe.io.tests.test_csv.test_read_csv_singleton_dtype()
dask.dataframe.io.tests.test_csv.test_read_csv_skiprows_only_in_first_partition(dd_read,pd_read,text,skip)
dask.dataframe.io.tests.test_csv.test_read_csv_skiprows_range()
dask.dataframe.io.tests.test_csv.test_read_csv_slash_r()
dask.dataframe.io.tests.test_csv.test_read_csv_with_datetime_index_partitions_n()
dask.dataframe.io.tests.test_csv.test_read_csv_with_datetime_index_partitions_one()
dask.dataframe.io.tests.test_csv.test_reading_empty_csv_files_with_path()
dask.dataframe.io.tests.test_csv.test_robust_column_mismatch()
dask.dataframe.io.tests.test_csv.test_select_with_include_path_column(tmpdir)
dask.dataframe.io.tests.test_csv.test_skipfooter(dd_read,pd_read,files)
dask.dataframe.io.tests.test_csv.test_skipinitialspace()
dask.dataframe.io.tests.test_csv.test_skiprows(dd_read,pd_read,files)
dask.dataframe.io.tests.test_csv.test_skiprows_as_list(dd_read,pd_read,files,units)
dask.dataframe.io.tests.test_csv.test_string_blocksize()
dask.dataframe.io.tests.test_csv.test_text_blocks_to_pandas_blocked(reader,files)
dask.dataframe.io.tests.test_csv.test_text_blocks_to_pandas_kwargs(reader,files)
dask.dataframe.io.tests.test_csv.test_text_blocks_to_pandas_simple(reader,files)
dask.dataframe.io.tests.test_csv.test_to_csv()
dask.dataframe.io.tests.test_csv.test_to_csv_errors_using_multiple_scheduler_args()
dask.dataframe.io.tests.test_csv.test_to_csv_gzip()
dask.dataframe.io.tests.test_csv.test_to_csv_header(header,header_first_partition_only,expected_first,expected_next)
dask.dataframe.io.tests.test_csv.test_to_csv_header_empty_dataframe(header,expected)
dask.dataframe.io.tests.test_csv.test_to_csv_keeps_all_non_scheduler_compute_kwargs()
dask.dataframe.io.tests.test_csv.test_to_csv_line_ending()
dask.dataframe.io.tests.test_csv.test_to_csv_multiple_files_cornercases()
dask.dataframe.io.tests.test_csv.test_to_csv_nodir()
dask.dataframe.io.tests.test_csv.test_to_csv_paths()
dask.dataframe.io.tests.test_csv.test_to_csv_series()
dask.dataframe.io.tests.test_csv.test_to_csv_simple()
dask.dataframe.io.tests.test_csv.test_to_csv_single_file_exlusive_mode_no_overwrite()
dask.dataframe.io.tests.test_csv.test_to_csv_warns_using_scheduler_argument()
dask.dataframe.io.tests.test_csv.test_to_csv_with_get()
dask.dataframe.io.tests.test_csv.test_to_csv_with_single_file_and_append_mode()
dask.dataframe.io.tests.test_csv.test_to_csv_with_single_file_and_exclusive_mode()
dask.dataframe.io.tests.test_csv.test_to_single_csv()
dask.dataframe.io.tests.test_csv.test_to_single_csv_gzip()
dask.dataframe.io.tests.test_csv.test_to_single_csv_with_header_first_partition_only()
dask.dataframe.io.tests.test_csv.test_to_single_csv_with_name_function()
dask.dataframe.io.tests.test_csv.test_usecols()
dask.dataframe.io.tests.test_csv.test_warn_non_seekable_files()
dask.dataframe.io.tests.test_csv.test_windows_line_terminator()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/io/tests/test_hdf.py----------------------------------------
A:dask.dataframe.io.tests.test_hdf.df->pandas.DataFrame({'A': [], 'B': []}, index=[])
A:dask.dataframe.io.tests.test_hdf.a->dask.dataframe.from_pandas(df, 16)
A:dask.dataframe.io.tests.test_hdf.out->dask.dataframe.read_hdf(fn, '/data*')
A:dask.dataframe.io.tests.test_hdf.r->dask.dataframe.read_hdf(fn, '/data*', sorted_index=True)
A:dask.dataframe.io.tests.test_hdf.df16->pandas.DataFrame({'x': ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p'], 'y': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]}, index=[1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0])
A:dask.dataframe.io.tests.test_hdf.b->dask.dataframe.from_pandas(df16, 16)
A:dask.dataframe.io.tests.test_hdf.dsk->optimize_dataframe_getitem(out.dask, keys=out.__dask_keys__())
A:dask.dataframe.io.tests.test_hdf.fn->os.path.join(dn, 'data_*.csv')
A:dask.dataframe.io.tests.test_hdf.fn1->os.path.join(dn, 'data_1.h5')
A:dask.dataframe.io.tests.test_hdf.fn2->os.path.join(dn, 'data_2.h5')
A:dask.dataframe.io.tests.test_hdf.d->dask.dataframe.from_pandas(df, 16).to_hdf(fn, '/data', compute=False)
A:dask.dataframe.io.tests.test_hdf.ddf->dask.dataframe.from_pandas(df, npartitions=2)
A:dask.dataframe.io.tests.test_hdf.df2->pandas.read_hdf(fn, 'foo4')
A:dask.dataframe.io.tests.test_hdf.sorted_data->data.sort_index()
A:dask.dataframe.io.tests.test_hdf.res->pandas.read_hdf(path, '/data')
A:dask.dataframe.io.tests.test_hdf.expected->pandas.read_hdf(os.path.join(tdir, 'one.h5'), '/foo/data', start=1, stop=3)
A:dask.dataframe.io.tests.test_hdf.path->pathlib.Path(fn)
A:dask.dataframe.io.tests.test_hdf.tables->pytest.importorskip('tables')
A:dask.dataframe.io.tests.test_hdf.value1->pytest.importorskip('tables').Float32Col()
A:dask.dataframe.io.tests.test_hdf.value2->pytest.importorskip('tables').Float32Col()
A:dask.dataframe.io.tests.test_hdf.value3->pytest.importorskip('tables').Float32Col()
A:dask.dataframe.io.tests.test_hdf.group->h5file.create_group('/', 'group')
A:dask.dataframe.io.tests.test_hdf.t->h5file.create_table(group, 'table3', Table3, 'Table 3')
A:dask.dataframe.io.tests.test_hdf.bar->pandas.DataFrame(np.random.randn(10, 4))
dask.dataframe.io.tests.test_hdf.test_hdf_empty_dataframe(tmp_path)
dask.dataframe.io.tests.test_hdf.test_hdf_file_list()
dask.dataframe.io.tests.test_hdf.test_hdf_filenames()
dask.dataframe.io.tests.test_hdf.test_hdf_globbing()
dask.dataframe.io.tests.test_hdf.test_hdf_nonpandas_keys()
dask.dataframe.io.tests.test_hdf.test_hdf_path_exceptions()
dask.dataframe.io.tests.test_hdf.test_read_hdf(data,compare)
dask.dataframe.io.tests.test_hdf.test_read_hdf_doesnt_segfault()
dask.dataframe.io.tests.test_hdf.test_read_hdf_multiple()
dask.dataframe.io.tests.test_hdf.test_read_hdf_multiply_open()
dask.dataframe.io.tests.test_hdf.test_read_hdf_pattern_pathlike()
dask.dataframe.io.tests.test_hdf.test_read_hdf_start_stop_values()
dask.dataframe.io.tests.test_hdf.test_to_fmt_warns()
dask.dataframe.io.tests.test_hdf.test_to_hdf()
dask.dataframe.io.tests.test_hdf.test_to_hdf_exceptions()
dask.dataframe.io.tests.test_hdf.test_to_hdf_kwargs()
dask.dataframe.io.tests.test_hdf.test_to_hdf_link_optimizations()
dask.dataframe.io.tests.test_hdf.test_to_hdf_lock_delays()
dask.dataframe.io.tests.test_hdf.test_to_hdf_modes_multiple_files()
dask.dataframe.io.tests.test_hdf.test_to_hdf_modes_multiple_nodes()
dask.dataframe.io.tests.test_hdf.test_to_hdf_multiple_files()
dask.dataframe.io.tests.test_hdf.test_to_hdf_multiple_nodes()
dask.dataframe.io.tests.test_hdf.test_to_hdf_path_pathlike()
dask.dataframe.io.tests.test_hdf.test_to_hdf_schedulers(scheduler,npartitions)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/io/tests/test_io.py----------------------------------------
A:dask.dataframe.io.tests.test_io.DASK_EXPR_ENABLED->dask.dataframe._dask_expr_enabled()
A:dask.dataframe.io.tests.test_io.x->dask.array.ones((10,), chunks=(5,))
A:dask.dataframe.io.tests.test_io.res->_meta_from_array(x, columns=['b', 'a'])
A:dask.dataframe.io.tests.test_io.d->dask.dataframe.from_array(x, chunksize=4)
A:dask.dataframe.io.tests.test_io.a->pandas.DataFrame({'x': [1, 2]}, index=[1, 10])
A:dask.dataframe.io.tests.test_io.df->pandas.DataFrame({'A': [x] * size, 'B': [10] * size})
A:dask.dataframe.io.tests.test_io.ddf->dask.dataframe.from_map(MyFunc(), enumerate([0, 1, 2]), label='myfunc', enforce_metadata=True)
A:dask.dataframe.io.tests.test_io.s->dask.dataframe.from_delayed([d.a for d in dfs], meta=meta.a, divisions=divisions)
A:dask.dataframe.io.tests.test_io.ds->dask.dataframe.from_pandas(s, npartitions=2)
A:dask.dataframe.io.tests.test_io.df.Date->pandas.DataFrame({'A': [x] * size, 'B': [10] * size}).Date.astype('datetime64[ns]')
A:dask.dataframe.io.tests.test_io.num_rows->list(ddf.map_partitions(len).compute())
A:dask.dataframe.io.tests.test_io.s_pyarrow->dask.dataframe.from_delayed([d.a for d in dfs], meta=meta.a, divisions=divisions).astype('string[pyarrow]')
A:dask.dataframe.io.tests.test_io.s_pyarrow.index->dask.dataframe.from_delayed([d.a for d in dfs], meta=meta.a, divisions=divisions).astype('string[pyarrow]').index.astype('string[pyarrow]')
A:dask.dataframe.io.tests.test_io.df_pyarrow->pandas.DataFrame({'A': [x] * size, 'B': [10] * size}).astype({'z': 'string[pyarrow]'})
A:dask.dataframe.io.tests.test_io.df_pyarrow.index->pandas.DataFrame({'A': [x] * size, 'B': [10] * size}).astype({'z': 'string[pyarrow]'}).index.astype('string[pyarrow]')
A:dask.dataframe.io.tests.test_io.pdf->pandas.DataFrame(np.ones((10, 3)) * 2, columns=['a', 'b', 'c'])
A:dask.dataframe.io.tests.test_io.expected->pytest.importorskip(backend).DataFrame(data)
A:dask.dataframe.io.tests.test_io.cudf->pytest.importorskip('cudf')
A:dask.dataframe.io.tests.test_io.df2->dask.dataframe.from_array(x, columns=['a', 'b', 'c'])
A:dask.dataframe.io.tests.test_io.pser->pandas.Series(np.ones(10))
A:dask.dataframe.io.tests.test_io.ser->dask.dataframe.from_map(func, iterable, divisions=divisions)
A:dask.dataframe.io.tests.test_io.ser2->dask.dataframe.from_array(x)
A:dask.dataframe.io.tests.test_io.result->ddf.to_dask_array().compute()
A:dask.dataframe.io.tests.test_io.b->pandas.DataFrame({'x': [4, 1]}, index=[100, 200])
A:dask.dataframe.io.tests.test_io.y->dask.array.from_array(x, chunks=(1,))
A:dask.dataframe.io.tests.test_io.d1->dask.dataframe.from_dask_array(x, columns=['name'])
A:dask.dataframe.io.tests.test_io.p1->pandas.DataFrame(y, columns=['name'])
A:dask.dataframe.io.tests.test_io.d2->dask.dataframe.from_array(y, columns=['name'])
A:dask.dataframe.io.tests.test_io.dx->dask.array.ones((10,), chunks=(5,)).to_delayed()
A:dask.dataframe.io.tests.test_io.monotonic_index->dask.array.from_array(np.arange(6), chunks=chunksizes)
A:dask.dataframe.io.tests.test_io.array_lib->pytest.importorskip(array_backend)
A:dask.dataframe.io.tests.test_io.df_lib->pytest.importorskip(df_backend)
A:dask.dataframe.io.tests.test_io.darr->dask.array.ones(10)
A:dask.dataframe.io.tests.test_io.ddf1->dask.dataframe.from_array(darr)
A:dask.dataframe.io.tests.test_io.ddf2->dask.dataframe.from_dask_array(x, index=ddf.index, columns='val2')
A:dask.dataframe.io.tests.test_io.bagdf->dask.dataframe.from_map(MyFunc(), enumerate([0, 1, 2]), label='myfunc', enforce_metadata=True).to_bag(format='frame')
A:dask.dataframe.io.tests.test_io.meta->dfs[0].compute()
A:dask.dataframe.io.tests.test_io.check_ddf->dask.dataframe.from_delayed(dfs, meta=meta2)
A:dask.dataframe.io.tests.test_io.arr->dask.dataframe.from_map(MyFunc(), enumerate([0, 1, 2]), label='myfunc', enforce_metadata=True).to_dask_array()
A:dask.dataframe.io.tests.test_io.dsk->optimize_blockwise(arr.dask, keys=keys)
A:dask.dataframe.io.tests.test_io.misordered_meta->pandas.DataFrame(columns=['date', 'ent', 'val', '(1)', '(2)'], data=[range(5)])
A:dask.dataframe.io.tests.test_io.A->dask.dataframe.from_delayed([delayed(a), delayed(b)], divisions='sorted')
A:dask.dataframe.io.tests.test_io.(a, b)->dask.dataframe.from_map(MyFunc(), enumerate([0, 1, 2]), label='myfunc', enforce_metadata=True).to_delayed()
A:dask.dataframe.io.tests.test_io.dx2->dask.array.ones((10,), chunks=(5,)).to_delayed(optimize_graph=False)
A:dask.dataframe.io.tests.test_io.expect->pandas.DataFrame({'A': [0, 1, 1, 2, 2, 2], 'B': [10] * 6}, index=[0, 0, 1, 0, 1, 2])
A:dask.dataframe.io.tests.test_io.index->numpy.array([0, 1, 0, 1], dtype='int64')
A:dask.dataframe.io.tests.test_io.string_dtype->get_string_dtype()
A:dask.dataframe.io.tests.test_io._lib->pytest.importorskip(backend)
A:dask.dataframe.io.tests.test_io.got->dask.dataframe.from_dict(data, npartitions=2)
A:dask.dataframe.io.tests.test_io.got_classmethod->dask.dataframe.from_dict(data, npartitions=2).from_dict(data, npartitions=2)
dask.dataframe.io.tests.test_io._generator()
dask.dataframe.io.tests.test_io.test_DataFrame_from_dask_array()
dask.dataframe.io.tests.test_io.test_DataFrame_from_dask_array_with_blockwise_ops()
dask.dataframe.io.tests.test_io.test_Series_from_dask_array()
dask.dataframe.io.tests.test_io.test_from_array()
dask.dataframe.io.tests.test_io.test_from_array_1d_list_of_columns_gives_dataframe()
dask.dataframe.io.tests.test_io.test_from_array_1d_with_column_names()
dask.dataframe.io.tests.test_io.test_from_array_dispatching(array_backend,df_backend)
dask.dataframe.io.tests.test_io.test_from_array_raises_more_than_2D()
dask.dataframe.io.tests.test_io.test_from_array_with_column_names()
dask.dataframe.io.tests.test_io.test_from_array_with_record_dtype()
dask.dataframe.io.tests.test_io.test_from_array_wrong_column_shape_error()
dask.dataframe.io.tests.test_io.test_from_dask_array_compat_numpy_array()
dask.dataframe.io.tests.test_io.test_from_dask_array_compat_numpy_array_1d()
dask.dataframe.io.tests.test_io.test_from_dask_array_empty_chunks(chunksizes,expected_divisions)
dask.dataframe.io.tests.test_io.test_from_dask_array_index(as_frame)
dask.dataframe.io.tests.test_io.test_from_dask_array_index_dtype()
dask.dataframe.io.tests.test_io.test_from_dask_array_index_raises()
dask.dataframe.io.tests.test_io.test_from_dask_array_struct_dtype()
dask.dataframe.io.tests.test_io.test_from_dask_array_unknown_chunks()
dask.dataframe.io.tests.test_io.test_from_dask_array_unknown_width_error()
dask.dataframe.io.tests.test_io.test_from_delayed()
dask.dataframe.io.tests.test_io.test_from_delayed_misordered_meta()
dask.dataframe.io.tests.test_io.test_from_delayed_optimize_fusion()
dask.dataframe.io.tests.test_io.test_from_delayed_preserves_hlgs()
dask.dataframe.io.tests.test_io.test_from_delayed_sorted()
dask.dataframe.io.tests.test_io.test_from_delayed_to_dask_array()
dask.dataframe.io.tests.test_io.test_from_dict_backends(backend)
dask.dataframe.io.tests.test_io.test_from_map_args()
dask.dataframe.io.tests.test_io.test_from_map_column_projection()
dask.dataframe.io.tests.test_io.test_from_map_custom_name()
dask.dataframe.io.tests.test_io.test_from_map_divisions()
dask.dataframe.io.tests.test_io.test_from_map_meta()
dask.dataframe.io.tests.test_io.test_from_map_multi()
dask.dataframe.io.tests.test_io.test_from_map_other_iterables(iterable)
dask.dataframe.io.tests.test_io.test_from_map_simple(vals)
dask.dataframe.io.tests.test_io.test_from_pandas_chunksize_one()
dask.dataframe.io.tests.test_io.test_from_pandas_convert_string_config()
dask.dataframe.io.tests.test_io.test_from_pandas_convert_string_config_raises()
dask.dataframe.io.tests.test_io.test_from_pandas_dataframe()
dask.dataframe.io.tests.test_io.test_from_pandas_immutable(sort,index)
dask.dataframe.io.tests.test_io.test_from_pandas_non_sorted()
dask.dataframe.io.tests.test_io.test_from_pandas_npartitions_duplicates(index)
dask.dataframe.io.tests.test_io.test_from_pandas_npartitions_is_accurate(n)
dask.dataframe.io.tests.test_io.test_from_pandas_series()
dask.dataframe.io.tests.test_io.test_from_pandas_single_row()
dask.dataframe.io.tests.test_io.test_from_pandas_small()
dask.dataframe.io.tests.test_io.test_from_pandas_with_datetime_index()
dask.dataframe.io.tests.test_io.test_from_pandas_with_index_nulls(null_value)
dask.dataframe.io.tests.test_io.test_from_pandas_with_wrong_args()
dask.dataframe.io.tests.test_io.test_gpu_from_pandas_npartitions_duplicates()
dask.dataframe.io.tests.test_io.test_meta_from_1darray()
dask.dataframe.io.tests.test_io.test_meta_from_array()
dask.dataframe.io.tests.test_io.test_meta_from_recarray()
dask.dataframe.io.tests.test_io.test_to_bag()
dask.dataframe.io.tests.test_io.test_to_bag_frame()
dask.dataframe.io.tests.test_io.test_to_delayed()
dask.dataframe.io.tests.test_io.test_to_delayed_optimize_graph()
dask.dataframe.io.tests.test_io.test_to_records()
dask.dataframe.io.tests.test_io.test_to_records_raises()
dask.dataframe.io.tests.test_io.test_to_records_with_lengths(lengths)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/io/tests/test_sql.py----------------------------------------
A:dask.dataframe.io.tests.test_sql.pd->pytest.importorskip('pandas')
A:dask.dataframe.io.tests.test_sql.dd->pytest.importorskip('dask.dataframe')
A:dask.dataframe.io.tests.test_sql.np->pytest.importorskip('numpy')
A:dask.dataframe.io.tests.test_sql.df->pytest.importorskip('pandas').DataFrame({'a': list('ghjkl'), 'b': [now + i * d for i in range(2, -3, -1)]})
A:dask.dataframe.io.tests.test_sql.metadata->MetaData()
A:dask.dataframe.io.tests.test_sql.engine->create_engine(db_url)
A:dask.dataframe.io.tests.test_sql.table->Table(table_name, metadata, Column('id', Integer, primary_key=True), Column('col2', Integer), schema=schema_name)
A:dask.dataframe.io.tests.test_sql.dask_df->read_sql_table(table.name, db_url, index_col='id', schema=table.schema, npartitions=1)
A:dask.dataframe.io.tests.test_sql.pd_dataframe->read_sql_table(table.name, db_url, index_col='id', schema=table.schema, npartitions=1).compute()
A:dask.dataframe.io.tests.test_sql.test_data->pytest.importorskip('pandas').DataFrame({'id': list(range(50))}).set_index('id')
A:dask.dataframe.io.tests.test_sql.ddf->pytest.importorskip('dask.dataframe').from_pandas(df, 2)
A:dask.dataframe.io.tests.test_sql.now->datetime.datetime.now()
A:dask.dataframe.io.tests.test_sql.d->datetime.timedelta(seconds=1)
A:dask.dataframe.io.tests.test_sql.string_dtype->get_string_dtype()
A:dask.dataframe.io.tests.test_sql.data->read_sql_table('test', db, npartitions=2, index_col='number').compute()
A:dask.dataframe.io.tests.test_sql.df2->pytest.importorskip('pandas').DataFrame({'a': list('ghjkl'), 'b': [now + i * d for i in range(2, -3, -1)]}).set_index('b')
A:dask.dataframe.io.tests.test_sql.part->read_sql_table('test', db, npartitions=2, index_col='number').compute().get_partition(1).compute()
A:dask.dataframe.io.tests.test_sql.data_1->read_sql_table('test', db, columns=list(df.columns), bytes_per_chunk=2 ** 30, index_col='number', head_rows=1)
A:dask.dataframe.io.tests.test_sql.out->read_sql(s, db, npartitions=2, index_col='number')
A:dask.dataframe.io.tests.test_sql.m->read_sql(s, db, npartitions=2, index_col='number').memory_usage_per_partition(deep=True, index=True).compute()
A:dask.dataframe.io.tests.test_sql.s1->sqlalchemy.sql.select(sql.column('number'), sql.column('name'), sql.column('age')).select_from(sql.table('test'))
A:dask.dataframe.io.tests.test_sql.s2->sqlalchemy.sql.select(sa.cast(sql.column('number'), sa.types.BigInteger).label('number'), sql.column('name')).where(sql.column('number') >= 5).select_from(sql.table('test'))
A:dask.dataframe.io.tests.test_sql.number->sqlalchemy.sql.column('number')
A:dask.dataframe.io.tests.test_sql.name->sqlalchemy.sql.column('name')
A:dask.dataframe.io.tests.test_sql.lenname_df->lenname_df.reset_index().set_index('lenname').reset_index().set_index('lenname')
A:dask.dataframe.io.tests.test_sql.lenname_df['lenname']->lenname_df['name'].str.len()
A:dask.dataframe.io.tests.test_sql.index->pytest.importorskip('pandas').Index([], name='number', dtype='int')
A:dask.dataframe.io.tests.test_sql.meta->pytest.importorskip('pandas').DataFrame(data, index=index)
A:dask.dataframe.io.tests.test_sql.s->sqlalchemy.sql.select(sql.column('number'), sql.column('name')).select_from(sql.table('test'))
A:dask.dataframe.io.tests.test_sql.df_by_age->pytest.importorskip('pandas').DataFrame({'a': list('ghjkl'), 'b': [now + i * d for i in range(2, -3, -1)]}).set_index('age')
A:dask.dataframe.io.tests.test_sql.df_appended->pytest.importorskip('pandas').concat([df, df])
A:dask.dataframe.io.tests.test_sql.ddf_by_age->pytest.importorskip('dask.dataframe').from_pandas(df, 2).set_index('age')
A:dask.dataframe.io.tests.test_sql.result->pytest.importorskip('dask.dataframe').from_pandas(df, 2).to_sql('test', uri, parallel=parallel, compute=False)
A:dask.dataframe.io.tests.test_sql.actual->len(result.compute())
A:dask.dataframe.io.tests.test_sql.logs->'\n'.join((r.message for r in caplog.records))
dask.dataframe.io.tests.test_sql.db()
dask.dataframe.io.tests.test_sql.test_datetimes()
dask.dataframe.io.tests.test_sql.test_division_or_partition(db)
dask.dataframe.io.tests.test_sql.test_divisions(db)
dask.dataframe.io.tests.test_sql.test_empty(db)
dask.dataframe.io.tests.test_sql.test_empty_other_schema()
dask.dataframe.io.tests.test_sql.test_extra_connection_engine_keywords(caplog,db)
dask.dataframe.io.tests.test_sql.test_limits(db)
dask.dataframe.io.tests.test_sql.test_meta(db)
dask.dataframe.io.tests.test_sql.test_meta_no_head_rows(db)
dask.dataframe.io.tests.test_sql.test_needs_rational(db)
dask.dataframe.io.tests.test_sql.test_no_character_index_without_divisions(db)
dask.dataframe.io.tests.test_sql.test_no_meta_no_head_rows(db)
dask.dataframe.io.tests.test_sql.test_npartitions(db)
dask.dataframe.io.tests.test_sql.test_passing_engine_as_uri_raises_helpful_error(db)
dask.dataframe.io.tests.test_sql.test_query(db)
dask.dataframe.io.tests.test_sql.test_query_index_from_query(db)
dask.dataframe.io.tests.test_sql.test_query_with_meta(db)
dask.dataframe.io.tests.test_sql.test_read_sql(db)
dask.dataframe.io.tests.test_sql.test_simple(db)
dask.dataframe.io.tests.test_sql.test_single_column(db,use_head)
dask.dataframe.io.tests.test_sql.test_to_sql(npartitions,parallel)
dask.dataframe.io.tests.test_sql.test_to_sql_engine_kwargs(caplog)
dask.dataframe.io.tests.test_sql.test_to_sql_kwargs()
dask.dataframe.io.tests.test_sql.tmp_db_uri()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/io/tests/test_json.py----------------------------------------
A:dask.dataframe.io.tests.test_json.df->pandas.DataFrame({'x': ['a', 'b', 'c', 'd'], 'y': [1, 2, 3, 4]})
A:dask.dataframe.io.tests.test_json.ddf->dask.dataframe.from_pandas(df, npartitions=2)
A:dask.dataframe.io.tests.test_json.actual->dask.dataframe.read_json(fn)
A:dask.dataframe.io.tests.test_json.actual_pd->pandas.read_json(f, orient='records', lines=False)
A:dask.dataframe.io.tests.test_json.actual_pd['path']->pandas.Series((f.replace(os.sep, '/'),) * len(actual_pd), dtype='category')
A:dask.dataframe.io.tests.test_json.actual_pd[path_column_name]->pandas.Series((path_converter(f),) * len(actual_pd), dtype='category')
A:dask.dataframe.io.tests.test_json.fil1->str(tmpdir.join('fil1.json')).replace(os.sep, '/')
A:dask.dataframe.io.tests.test_json.fil2->str(tmpdir.join('fil2.json')).replace(os.sep, '/')
A:dask.dataframe.io.tests.test_json.df2->pandas.DataFrame({'x': ['a', 'b', 'c', 'd'], 'y': [1, 2, 3, 4]}).assign(x=df.x + 0.5)
A:dask.dataframe.io.tests.test_json.path_dtype->pandas.CategoricalDtype((fil1, fil2))
A:dask.dataframe.io.tests.test_json.df['path']->pandas.Series((fil1,) * len(df), dtype=path_dtype)
A:dask.dataframe.io.tests.test_json.df2['path']->pandas.Series((fil2,) * len(df2), dtype=path_dtype)
A:dask.dataframe.io.tests.test_json.sol->pandas.concat([df, df2])
A:dask.dataframe.io.tests.test_json.res->dask.dataframe.read_json(str(tmpdir.join('fil*.json')), orient=orient, meta=meta, lines=True, blocksize=50)
A:dask.dataframe.io.tests.test_json.actual.columns->list(df.columns)
A:dask.dataframe.io.tests.test_json.got->dask.dataframe.read_json(f, engine=engine, lines=False)
A:dask.dataframe.io.tests.test_json.fn->os.path.join(path, '*.json.gz')
A:dask.dataframe.io.tests.test_json.result->dask.dataframe.read_json(os.path.join(dn, '*'))
A:dask.dataframe.io.tests.test_json.d->dask.dataframe.read_json(fn, blocksize=block, sample=10)
A:dask.dataframe.io.tests.test_json.paths->dask.compute(*list_of_delayed)
A:dask.dataframe.io.tests.test_json.list_of_delayed->dask.dataframe.from_pandas(df, npartitions=2).to_json(f, compute=False)
dask.dataframe.io.tests.test_json.test_json_compressed(compression)
dask.dataframe.io.tests.test_json.test_read_chunked(block)
dask.dataframe.io.tests.test_json.test_read_json_basic(orient)
dask.dataframe.io.tests.test_json.test_read_json_engine_str(engine)
dask.dataframe.io.tests.test_json.test_read_json_error()
dask.dataframe.io.tests.test_json.test_read_json_fkeyword(fkeyword)
dask.dataframe.io.tests.test_json.test_read_json_inferred_compression()
dask.dataframe.io.tests.test_json.test_read_json_meta(orient,tmpdir)
dask.dataframe.io.tests.test_json.test_read_json_multiple_files_with_path_column(blocksize,tmpdir)
dask.dataframe.io.tests.test_json.test_read_json_path_column_with_duplicate_name_is_error()
dask.dataframe.io.tests.test_json.test_read_json_with_path_column(orient)
dask.dataframe.io.tests.test_json.test_read_json_with_path_converter()
dask.dataframe.io.tests.test_json.test_read_orient_not_records_and_lines()
dask.dataframe.io.tests.test_json.test_to_json_results()
dask.dataframe.io.tests.test_json.test_to_json_with_get()
dask.dataframe.io.tests.test_json.test_write_json_basic(orient)
dask.dataframe.io.tests.test_json.test_write_orient_not_records_and_lines()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/io/orc/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/io/orc/core.py----------------------------------------
A:dask.dataframe.io.orc.core.func->copy.deepcopy(self)
A:dask.dataframe.io.orc.core._df->self.engine.read_partition(self.fs, parts, self.schema, self.columns)
A:dask.dataframe.io.orc.core.engine->_get_engine(engine)
A:dask.dataframe.io.orc.core.(fs, fs_token, paths)->get_fs_token_paths(path, mode='rb', storage_options=storage_options)
A:dask.dataframe.io.orc.core.(parts, schema, meta)->_get_engine(engine).read_metadata(fs, paths, columns, index, split_stripes, aggregate_files)
A:dask.dataframe.io.orc.core.path->fs._strip_protocol(path)
A:dask.dataframe.io.orc.core.(fs, _, _)->get_fs_token_paths(path, mode='wb', storage_options=storage_options)
A:dask.dataframe.io.orc.core.df->df.reset_index(drop=True).reset_index(drop=True)
A:dask.dataframe.io.orc.core.part_tasks->list(dsk.keys())
A:dask.dataframe.io.orc.core.graph->dask.highlevelgraph.HighLevelGraph.from_collections((final_name, 0), dsk, dependencies=[df])
A:dask.dataframe.io.orc.core.compute_kwargs->dict()
dask.dataframe.io.orc.core.ORCFunctionWrapper(self,fs,columns,schema,engine,index)
dask.dataframe.io.orc.core.ORCFunctionWrapper.__init__(self,fs,columns,schema,engine,index)
dask.dataframe.io.orc.core.ORCFunctionWrapper.columns(self)
dask.dataframe.io.orc.core.ORCFunctionWrapper.project_columns(self,columns)
dask.dataframe.io.orc.core._get_engine(engine:Literal['pyarrow']|ORCEngine)->type[ArrowORCEngine] | ORCEngine
dask.dataframe.io.orc.core.read_orc(path,engine='pyarrow',columns=None,index=None,split_stripes=1,aggregate_files=None,storage_options=None)
dask.dataframe.io.orc.core.to_orc(df,path,engine='pyarrow',write_index=True,storage_options=None,compute=True,compute_kwargs=None)
dask.dataframe.read_orc(path,engine='pyarrow',columns=None,index=None,split_stripes=1,aggregate_files=None,storage_options=None)
dask.dataframe.to_orc(df,path,engine='pyarrow',write_index=True,storage_options=None,compute=True,compute_kwargs=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/io/orc/utils.py----------------------------------------
dask.dataframe.io.orc.utils.ORCEngine
dask.dataframe.io.orc.utils.ORCEngine.read_metadata(cls,fs,paths,columns,index,split_stripes,aggregate_files,**kwargs)
dask.dataframe.io.orc.utils.ORCEngine.read_partition(cls,fs,part,columns,**kwargs)
dask.dataframe.io.orc.utils.ORCEngine.write_partition(cls,df,path,fs,filename,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/io/orc/arrow.py----------------------------------------
A:dask.dataframe.io.orc.arrow.paths->fs.find(paths[0])
A:dask.dataframe.io.orc.arrow.o->pyarrow.orc.ORCFile(f)
A:dask.dataframe.io.orc.arrow._stripes->list(range(o.nstripes))
A:dask.dataframe.io.orc.arrow.schema->_get_pyarrow_dtypes(schema, categories=None)
A:dask.dataframe.io.orc.arrow.parts->cls._aggregate_files(aggregate_files, split_stripes, parts)
A:dask.dataframe.io.orc.arrow.meta->_meta_from_dtypes(columns, schema, index, [])
A:dask.dataframe.io.orc.arrow.nstripes->len(new_part[0][1])
A:dask.dataframe.io.orc.arrow.next_nstripes->len(part[0][1])
A:dask.dataframe.io.orc.arrow.table->pyarrow.Table.from_pandas(df)
A:dask.dataframe.io.orc.arrow.columns->list(schema)
dask.dataframe.io.orc.arrow.ArrowORCEngine
dask.dataframe.io.orc.arrow.ArrowORCEngine._aggregate_files(cls,aggregate_files,split_stripes,parts)
dask.dataframe.io.orc.arrow.ArrowORCEngine.read_metadata(cls,fs,paths,columns,index,split_stripes,aggregate_files,**kwargs)
dask.dataframe.io.orc.arrow.ArrowORCEngine.read_partition(cls,fs,parts,schema,columns,**kwargs)
dask.dataframe.io.orc.arrow.ArrowORCEngine.write_partition(cls,df,path,fs,filename,**kwargs)
dask.dataframe.io.orc.arrow._read_orc_stripes(fs,path,stripes,schema,columns)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/io/parquet/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/io/parquet/core.py----------------------------------------
A:dask.dataframe.io.parquet.core.self.common_kwargs->tlz.merge(common_kwargs, kwargs or {})
A:dask.dataframe.io.parquet.core.blocksize->parse_bytes(blocksize)
A:dask.dataframe.io.parquet.core.calculate_divisions->bool(calculate_divisions)
A:dask.dataframe.io.parquet.core.parquet_file_extension->kwargs['dataset'].pop('require_extension')
A:dask.dataframe.io.parquet.core.df->df.reset_index(drop=True).reset_index(drop=True)
A:dask.dataframe.io.parquet.core.columns->list(columns)
A:dask.dataframe.io.parquet.core.engine->get_engine(engine)
A:dask.dataframe.io.parquet.core.path->stringify_path(path)
A:dask.dataframe.io.parquet.core.(dataset_options, read_options, open_file_options, other_options)->_split_user_options(**kwargs)
A:dask.dataframe.io.parquet.core.(fs, paths, dataset_options, open_file_options)->get_engine(engine).extract_filesystem(path, filesystem, dataset_options, open_file_options, storage_options)
A:dask.dataframe.io.parquet.core.paths->sorted(paths, key=natural_sort_key)
A:dask.dataframe.io.parquet.core.read_metadata_result->get_engine(engine).read_metadata(fs, paths, categories=categories, index=index, use_nullable_dtypes=use_nullable_dtypes, dtype_backend=dtype_backend, gather_statistics=calculate_divisions, filters=filters, split_row_groups=split_row_groups, blocksize=blocksize, aggregate_files=aggregate_files, ignore_metadata_file=ignore_metadata_file, metadata_task_size=metadata_task_size, parquet_file_extension=parquet_file_extension, dataset=dataset_options, read=read_options, **other_options)
A:dask.dataframe.io.parquet.core.common_kwargs->parts[0].pop('common_kwargs', {})
A:dask.dataframe.io.parquet.core.aggregation_depth->parts[0].pop('aggregation_depth', aggregation_depth)
A:dask.dataframe.io.parquet.core.split_row_groups->parts[0].pop('split_row_groups', split_row_groups)
A:dask.dataframe.io.parquet.core.(parts, divisions, index)->process_statistics(parts, statistics, filters, index, blocksize if split_row_groups is True else None, split_row_groups, fs, aggregation_depth)
A:dask.dataframe.io.parquet.core.(meta, index, columns)->set_index_columns(meta, index, columns, auto_index_allowed)
A:dask.dataframe.io.parquet.core.io_func->ParquetFunctionWrapper(engine, fs, meta, columns, index, dtype_backend, {}, common_kwargs)
A:dask.dataframe.io.parquet.core.annotations->dask.get_annotations()
A:dask.dataframe.io.parquet.core.ctx->contextlib.nullcontext()
A:dask.dataframe.io.parquet.core.(fs, _paths, _, _)->get_engine(engine).extract_filesystem(path, filesystem=filesystem, dataset_options={}, open_file_options={}, storage_options=storage_options)
A:dask.dataframe.io.parquet.core.real_cols->set(df.columns)
A:dask.dataframe.io.parquet.core.(i_offset, fmd, metadata_file_exists, extra_write_kwargs)->get_engine(engine).initialize_write(df, fs, path, append=append, ignore_divisions=ignore_divisions, partition_on=partition_on, division_info=division_info, index_cols=index_cols, schema=schema, custom_metadata=custom_metadata, **kwargs)
A:dask.dataframe.io.parquet.core.data_write->df.reset_index(drop=True).reset_index(drop=True).map_partitions(ToParquetFunctionWrapper(engine, path, fs, partition_on, write_metadata_file, i_offset, name_function, toolz.merge(kwargs, {'compression': compression, 'custom_metadata': custom_metadata}, extra_write_kwargs)), BlockIndex((df.npartitions,)), meta=df._meta, enforce_metadata=False, transform_divisions=False, align_dataframes=False)
A:dask.dataframe.io.parquet.core.graph->dask.highlevelgraph.HighLevelGraph.from_collections(name, dsk, dependencies=[])
A:dask.dataframe.io.parquet.core.out->out.compute(**compute_kwargs).compute(**compute_kwargs)
A:dask.dataframe.io.parquet.core.(fs, _, paths)->get_fs_token_paths(paths, mode='rb', storage_options=storage_options)
A:dask.dataframe.io.parquet.core.(paths, root_dir, fns)->_sort_and_analyze_paths(paths, fs, **ap_kwargs)
A:dask.dataframe.io.parquet.core.parts->math.ceil(parts / split_every)
A:dask.dataframe.io.parquet.core.height->len(widths)
A:dask.dataframe.io.parquet.core.lstop->min(lstart + split_every, p_max)
A:dask.dataframe.io.parquet.core.compute_kwargs->dict()
A:dask.dataframe.io.parquet.core.null_count->c.get('null_count', None)
A:dask.dataframe.io.parquet.core.(out_parts, out_statistics)->apply_conjunction(parts, statistics, conjunction)
A:dask.dataframe.io.parquet.core.result->list(zip(*[(part, stats) for (part, stats) in zip(parts, statistics) if stats['num-rows'] > 0]))
A:dask.dataframe.io.parquet.core.(parts, statistics)->aggregate_row_groups(parts, statistics, blocksize, split_row_groups, fs, aggregation_depth)
A:dask.dataframe.io.parquet.core.use_blocksize_criteria->bool(blocksize)
A:dask.dataframe.io.parquet.core.rgs->set(list(part['piece'][1]) + list(next_part[-1]['piece'][1]))
A:dask.dataframe.io.parquet.core.stat['num-row-groups']->stat.get('num-row-groups', 1)
A:dask.dataframe.io.parquet.core.next_stat['num-row-groups']->next_stat.get('num-row-groups', 1)
A:dask.dataframe.io.parquet.core.col['min']->min(col['min'], col_add['min'])
A:dask.dataframe.io.parquet.core.col['max']->max(col['max'], col_add['max'])
dask.dataframe.io.parquet.core.ParquetFunctionWrapper(self,engine,fs,meta,columns,index,dtype_backend,kwargs,common_kwargs)
dask.dataframe.io.parquet.core.ParquetFunctionWrapper.__init__(self,engine,fs,meta,columns,index,dtype_backend,kwargs,common_kwargs)
dask.dataframe.io.parquet.core.ParquetFunctionWrapper.columns(self)
dask.dataframe.io.parquet.core.ParquetFunctionWrapper.project_columns(self,columns)
dask.dataframe.io.parquet.core.ToParquetFunctionWrapper(self,engine,path,fs,partition_on,write_metadata_file,i_offset,name_function,kwargs_pass)
dask.dataframe.io.parquet.core.ToParquetFunctionWrapper.__dask_tokenize__(self)
dask.dataframe.io.parquet.core.ToParquetFunctionWrapper.__init__(self,engine,path,fs,partition_on,write_metadata_file,i_offset,name_function,kwargs_pass)
dask.dataframe.io.parquet.core.aggregate_row_groups(parts,stats,blocksize,split_row_groups,fs,aggregation_depth)
dask.dataframe.io.parquet.core.apply_filters(parts,statistics,filters)
dask.dataframe.io.parquet.core.check_multi_support(engine)
dask.dataframe.io.parquet.core.create_metadata_file(paths,root_dir=None,out_dir=None,engine='pyarrow',storage_options=None,split_every=32,compute=True,compute_kwargs=None,fs=None)
dask.dataframe.io.parquet.core.get_engine(engine:Literal['auto','pyarrow']|type[Engine])->type[Engine]
dask.dataframe.io.parquet.core.process_statistics(parts,statistics,filters,index,blocksize,split_row_groups,fs,aggregation_depth)
dask.dataframe.io.parquet.core.read_parquet(path,columns=None,filters=None,categories=None,index=None,storage_options=None,engine='auto',use_nullable_dtypes:bool|None=None,dtype_backend=None,calculate_divisions=None,ignore_metadata_file=False,metadata_task_size=None,split_row_groups='infer',blocksize='default',aggregate_files=None,parquet_file_extension=('.parq','.parquet','.pq'),filesystem=None,**kwargs)
dask.dataframe.io.parquet.core.read_parquet_part(fs,engine,meta,part,columns,index,kwargs)
dask.dataframe.io.parquet.core.set_index_columns(meta,index,columns,auto_index_allowed)
dask.dataframe.io.parquet.core.sorted_columns(statistics,columns=None)
dask.dataframe.io.parquet.core.to_parquet(df,path,engine='auto',compression='snappy',write_index=True,append=False,overwrite=False,ignore_divisions=False,partition_on=None,storage_options=None,custom_metadata=None,write_metadata_file=None,compute=True,compute_kwargs=None,schema='infer',name_function=None,filesystem=None,**kwargs)
dask.dataframe.io.parquet.create_metadata_file(paths,root_dir=None,out_dir=None,engine='pyarrow',storage_options=None,split_every=32,compute=True,compute_kwargs=None,fs=None)
dask.dataframe.read_parquet(path,columns=None,filters=None,categories=None,index=None,storage_options=None,engine='auto',use_nullable_dtypes:bool|None=None,dtype_backend=None,calculate_divisions=None,ignore_metadata_file=False,metadata_task_size=None,split_row_groups='infer',blocksize='default',aggregate_files=None,parquet_file_extension=('.parq','.parquet','.pq'),filesystem=None,**kwargs)
dask.dataframe.read_parquet_part(fs,engine,meta,part,columns,index,kwargs)
dask.dataframe.to_parquet(df,path,engine='auto',compression='snappy',write_index=True,append=False,overwrite=False,ignore_divisions=False,partition_on=None,storage_options=None,custom_metadata=None,write_metadata_file=None,compute=True,compute_kwargs=None,schema='infer',name_function=None,filesystem=None,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/io/parquet/utils.py----------------------------------------
A:dask.dataframe.io.parquet.utils.fs->dataset_options.pop('fs', 'fsspec')
A:dask.dataframe.io.parquet.utils.(fs, _, paths)->get_fs_token_paths(urlpath, mode='rb', storage_options=storage_options)
A:dask.dataframe.io.parquet.utils.paths->expand_paths_if_needed(urlpath, 'rb', 1, fs, None)
A:dask.dataframe.io.parquet.utils.index_name_xpr->re.compile('__index_level_\\d+__')
A:dask.dataframe.io.parquet.utils.column_index_names->pandas_metadata.get('column_indexes', [{'name': None}])
A:dask.dataframe.io.parquet.utils.index_names->list(index_storage_names)
A:dask.dataframe.io.parquet.utils.index_storage_names2->set(index_storage_names)
A:dask.dataframe.io.parquet.utils.storage_name_mapping->dict(pairs2)
A:dask.dataframe.io.parquet.utils.user_columns->list(user_columns)
A:dask.dataframe.io.parquet.utils.user_index->list(user_index)
A:dask.dataframe.io.parquet.utils.file_list->sorted(file_list, key=natural_sort_key)
A:dask.dataframe.io.parquet.utils.(base, fns)->_analyze_paths(file_list, fs, root=root)
A:dask.dataframe.io.parquet.utils.p->p.replace(fs.sep, '/').replace(fs.sep, '/')
A:dask.dataframe.io.parquet.utils.path->list(path)
A:dask.dataframe.io.parquet.utils.l->len(basepath)
A:dask.dataframe.io.parquet.utils.basepath->_join_path(root).split('/')
A:dask.dataframe.io.parquet.utils.df_rgs->pandas.DataFrame(file_row_group_stats)
A:dask.dataframe.io.parquet.utils.df_cols->pandas.DataFrame(file_row_group_column_stats)
A:dask.dataframe.io.parquet.utils.minval->pandas.DataFrame(file_row_group_column_stats).iloc[:, i].dropna().min()
A:dask.dataframe.io.parquet.utils.maxval->pandas.DataFrame(file_row_group_column_stats).iloc[:, i + 1].dropna().max()
A:dask.dataframe.io.parquet.utils.null_count->pandas.DataFrame(file_row_group_column_stats).iloc[:, i + 2].sum()
A:dask.dataframe.io.parquet.utils.split_row_groups->int(split_row_groups)
A:dask.dataframe.io.parquet.utils.row_group_count->len(row_groups)
A:dask.dataframe.io.parquet.utils._rgs->list(range(residual, row_group_count, split_row_groups))
A:dask.dataframe.io.parquet.utils.part->make_part_func(filename, row_groups, **make_part_kwargs)
A:dask.dataframe.io.parquet.utils.stat->_aggregate_stats(filename, file_row_group_stats[filename], file_row_group_column_stats[filename], stat_col_indices)
A:dask.dataframe.io.parquet.utils.open_file_options->kwargs.copy().pop('open_file_options', {}).copy()
A:dask.dataframe.io.parquet.utils.precache_options->kwargs.copy().pop('open_file_options', {}).copy().pop('precache_options', {}).copy()
A:dask.dataframe.io.parquet.utils.open_file_options['cache_type']->kwargs.copy().pop('open_file_options', {}).copy().get('cache_type', default_cache)
A:dask.dataframe.io.parquet.utils.open_file_options['mode']->kwargs.copy().pop('open_file_options', {}).copy().get('mode', 'rb')
A:dask.dataframe.io.parquet.utils.user_kwargs->kwargs.copy()
A:dask.dataframe.io.parquet.utils.read_options->kwargs.copy().pop('read', {}).copy()
A:dask.dataframe.io.parquet.utils.blocksize->parse_bytes(blocksize)
dask.dataframe.io.parquet.utils.Engine
dask.dataframe.io.parquet.utils.Engine.aggregate_metadata(cls,meta_list,fs,out_path)
dask.dataframe.io.parquet.utils.Engine.collect_file_metadata(cls,path,fs,file_path)
dask.dataframe.io.parquet.utils.Engine.default_blocksize(cls)
dask.dataframe.io.parquet.utils.Engine.extract_filesystem(cls,urlpath,filesystem,dataset_options,open_file_options,storage_options)
dask.dataframe.io.parquet.utils.Engine.initialize_write(cls,df,fs,path,append=False,partition_on=None,ignore_divisions=False,division_info=None,**kwargs)
dask.dataframe.io.parquet.utils.Engine.read_metadata(cls,fs,paths,categories=None,index=None,use_nullable_dtypes=None,dtype_backend=None,gather_statistics=None,filters=None,**kwargs)
dask.dataframe.io.parquet.utils.Engine.read_partition(cls,fs,piece,columns,index,use_nullable_dtypes=False,**kwargs)
dask.dataframe.io.parquet.utils.Engine.write_metadata(cls,parts,meta,fs,path,append=False,**kwargs)
dask.dataframe.io.parquet.utils.Engine.write_partition(cls,df,path,fs,filename,partition_on,return_metadata,**kwargs)
dask.dataframe.io.parquet.utils._aggregate_stats(file_path,file_row_group_stats,file_row_group_column_stats,stat_col_indices)
dask.dataframe.io.parquet.utils._analyze_paths(file_list,fs,root=False)
dask.dataframe.io.parquet.utils._get_aggregation_depth(aggregate_files,partition_names)
dask.dataframe.io.parquet.utils._infer_split_row_groups(row_group_sizes,blocksize,aggregate_files=False)
dask.dataframe.io.parquet.utils._normalize_index_columns(user_columns,data_columns,user_index,data_index)
dask.dataframe.io.parquet.utils._parse_pandas_metadata(pandas_metadata)
dask.dataframe.io.parquet.utils._process_open_file_options(open_file_options,metadata=None,columns=None,row_groups=None,default_engine=None,default_cache='readahead',allow_precache=True)
dask.dataframe.io.parquet.utils._row_groups_to_parts(gather_statistics,split_row_groups,aggregation_depth,file_row_groups,file_row_group_stats,file_row_group_column_stats,stat_col_indices,make_part_func,make_part_kwargs)
dask.dataframe.io.parquet.utils._set_gather_statistics(gather_statistics,blocksize,split_row_groups,aggregation_depth,filter_columns,stat_columns)
dask.dataframe.io.parquet.utils._set_metadata_task_size(metadata_task_size,fs)
dask.dataframe.io.parquet.utils._sort_and_analyze_paths(file_list,fs,root=False)
dask.dataframe.io.parquet.utils._split_user_options(**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/io/parquet/fastparquet.py----------------------------------------
A:dask.dataframe.io.parquet.fastparquet._FP_FILE_LOCK->threading.RLock()
A:dask.dataframe.io.parquet.fastparquet.cats->OrderedDict([(key, list(v)) for (key, v) in cats.items()])
A:dask.dataframe.io.parquet.fastparquet.raw_cats->OrderedDict()
A:dask.dataframe.io.parquet.fastparquet.s->ex_from_sep('/')
A:dask.dataframe.io.parquet.fastparquet.paths->tlz.unique(paths)
A:dask.dataframe.io.parquet.fastparquet.partitions->tlz.unique(((k, v) for path in paths for (k, v) in s.findall(path)))
A:dask.dataframe.io.parquet.fastparquet.i_val->tlz.unique(((i, val) for path in paths for (i, val) in enumerate(path.split('/')[:-1])))
A:dask.dataframe.io.parquet.fastparquet.conflicts_by_value->OrderedDict()
A:dask.dataframe.io.parquet.fastparquet.vals_by_type->groupby_types(v)
A:dask.dataframe.io.parquet.fastparquet.pqpartitions->list(pf.cats)
A:dask.dataframe.io.parquet.fastparquet.pf.row_groups->sorted(pf.row_groups, key=lambda x: natural_sort_key(x.columns[0].file_path))
A:dask.dataframe.io.parquet.fastparquet.pandas_type[c['field_name']]->c.get('pandas_type', None)
A:dask.dataframe.io.parquet.fastparquet.file_row_groups->defaultdict(list)
A:dask.dataframe.io.parquet.fastparquet.file_row_group_stats->defaultdict(list)
A:dask.dataframe.io.parquet.fastparquet.file_row_group_column_stats->defaultdict(list)
A:dask.dataframe.io.parquet.fastparquet.cmin->pandas.Timestamp(cmin, tz=tz)
A:dask.dataframe.io.parquet.fastparquet.cmax->pandas.Timestamp(cmax, tz=tz)
A:dask.dataframe.io.parquet.fastparquet.null_count->null_count.decode('utf-8').decode('utf-8')
A:dask.dataframe.io.parquet.fastparquet.tz->getattr(dtypes[name], 'tz', None)
A:dask.dataframe.io.parquet.fastparquet.last->cmax_last.get(name, None)
A:dask.dataframe.io.parquet.fastparquet.real_row_groups->cls._get_thrift_row_groups(pf, filename, rg_list)
A:dask.dataframe.io.parquet.fastparquet.full_path->fs.sep.join([p for p in [base_path, filename] if p != ''])
A:dask.dataframe.io.parquet.fastparquet.dataset_kwargs->kwargs.pop('dataset', {})
A:dask.dataframe.io.parquet.fastparquet._metadata_exists->fs.isfile(fs.sep.join([base, '_metadata']))
A:dask.dataframe.io.parquet.fastparquet.(paths, base, fns)->_sort_and_analyze_paths(paths, fs)
A:dask.dataframe.io.parquet.fastparquet.pf->fastparquet.api.ParquetFile(path, open_with=fs.open)
A:dask.dataframe.io.parquet.fastparquet.len0->len(paths)
A:dask.dataframe.io.parquet.fastparquet.scheme->get_file_scheme(fns)
A:dask.dataframe.io.parquet.fastparquet.pf.cats->paths_to_cats(fns, scheme)
A:dask.dataframe.io.parquet.fastparquet.parts->tlz.unique(paths).copy()
A:dask.dataframe.io.parquet.fastparquet.aggregation_depth->_get_aggregation_depth(aggregate_files, list(pf.cats))
A:dask.dataframe.io.parquet.fastparquet.pf_sample->ParquetFile(paths[0], open_with=fs.open, **dataset_kwargs)
A:dask.dataframe.io.parquet.fastparquet.split_row_groups->_infer_split_row_groups([rg.total_byte_size for rg in pf_sample.row_groups], blocksize, bool(aggregate_files))
A:dask.dataframe.io.parquet.fastparquet.(index_names, column_names, storage_name_mapping, column_index_names)->_parse_pandas_metadata(pandas_md)
A:dask.dataframe.io.parquet.fastparquet.(column_names, index_names)->_normalize_index_columns(columns, column_names, index, index_names)
A:dask.dataframe.io.parquet.fastparquet.categories->list(categories)
A:dask.dataframe.io.parquet.fastparquet.dtypes->fastparquet.api.ParquetFile(path, open_with=fs.open)._dtypes(categories)
A:dask.dataframe.io.parquet.fastparquet.dtypes[cat]->pandas.CategoricalDtype(categories=[UNKNOWN_CATEGORIES])
A:dask.dataframe.io.parquet.fastparquet.dtypes[catcol]->pandas.CategoricalDtype(categories=pf.cats[catcol])
A:dask.dataframe.io.parquet.fastparquet.meta->cls._create_dd_meta(dataset_info)
A:dask.dataframe.io.parquet.fastparquet.metadata_task_size->_set_metadata_task_size(dataset_info['metadata_task_size'], fs)
A:dask.dataframe.io.parquet.fastparquet.gather_statistics->_set_gather_statistics(gather_statistics, blocksize, split_row_groups, aggregation_depth, filter_columns, set(stat_col_indices) | filter_columns)
A:dask.dataframe.io.parquet.fastparquet.(parts, stats)->_row_groups_to_parts(gather_statistics, split_row_groups, aggregation_depth, file_row_groups, file_row_group_stats, file_row_group_column_stats, stat_col_indices, cls._make_part, make_part_kwargs={'fs': fs, 'pf': pf, 'base_path': base_path, 'partitions': list(pf.cats)})
A:dask.dataframe.io.parquet.fastparquet.base_path->dataset_info_kwargs.get('base_path', None)
A:dask.dataframe.io.parquet.fastparquet.root_cats->dataset_info_kwargs.get('root_cats', None)
A:dask.dataframe.io.parquet.fastparquet.root_file_scheme->dataset_info_kwargs.get('root_file_scheme', None)
A:dask.dataframe.io.parquet.fastparquet.(file_row_groups, file_row_group_stats, file_row_group_column_stats, gather_statistics, base_path)->cls._organize_row_groups(pf, split_row_groups, gather_statistics, stat_col_indices, filters, dtypes, base_path, has_metadata_file, blocksize, aggregation_depth)
A:dask.dataframe.io.parquet.fastparquet.dataset_info->cls._collect_dataset_info(paths, fs, categories, index, gather_statistics, filters, split_row_groups, blocksize, aggregate_files, ignore_metadata_file, metadata_task_size, parquet_file_extension, kwargs)
A:dask.dataframe.io.parquet.fastparquet.(parts, stats, common_kwargs)->cls._construct_collection_plan(dataset_info)
A:dask.dataframe.io.parquet.fastparquet.parquet_file->ParquetFile([p[0] for p in pieces], open_with=fs.open, root=base_path or False, **kwargs.get('dataset', {}))
A:dask.dataframe.io.parquet.fastparquet.n_local_row_groups->len(_pf.row_groups)
A:dask.dataframe.io.parquet.fastparquet.rgs->partition_on_columns(df, partition_on, path, filename, fmd, fs.sep, compression, fs.open, mkdirs)
A:dask.dataframe.io.parquet.fastparquet.chunk.file_path->ex_from_sep('/').decode()
A:dask.dataframe.io.parquet.fastparquet.size->sum((rg.num_rows for rg in rgs))
A:dask.dataframe.io.parquet.fastparquet.(df, views)->fastparquet.api.ParquetFile(path, open_with=fs.open).pre_allocate(size, columns, categories, index)
A:dask.dataframe.io.parquet.fastparquet.df.columns->pandas.Index([], dtype=object)
A:dask.dataframe.io.parquet.fastparquet.fn_rg_map->defaultdict(list)
A:dask.dataframe.io.parquet.fastparquet.fn->fs.sep.join([path, '_common_metadata'])
A:dask.dataframe.io.parquet.fastparquet.(precache_options, open_file_options)->_process_open_file_options(open_file_options, **{'allow_precache': False, 'default_cache': 'readahead'} if _is_local_fs(fs) else {'metadata': pf, 'columns': list(set(columns).intersection(pf.columns)), 'row_groups': [rgs for rgs in fn_rg_map.values()], 'default_engine': 'fastparquet', 'default_cache': 'readahead'})
A:dask.dataframe.io.parquet.fastparquet.metadata_file_exists->fs.exists(fs.sep.join([path, '_metadata']))
A:dask.dataframe.io.parquet.fastparquet.i_offset->fastparquet.writer.find_max_part(fmd.row_groups)
A:dask.dataframe.io.parquet.fastparquet.minmax->fastparquet.api.sorted_partitioned_columns(pf)
A:dask.dataframe.io.parquet.fastparquet.fmd->copy.copy(fmd)
A:dask.dataframe.io.parquet.fastparquet.s.name->ex_from_sep('/').name.decode()
A:dask.dataframe.io.parquet.fastparquet.fmd.num_rows->len(df)
A:dask.dataframe.io.parquet.fastparquet.rg->make_part_file(fil, df, fmd.schema, compression=compression, fmd=fmd)
A:dask.dataframe.io.parquet.fastparquet._meta->copy.copy(meta)
dask.dataframe.io.parquet.fastparquet.FastParquetEngine(Engine)
dask.dataframe.io.parquet.fastparquet.FastParquetEngine._collect_dataset_info(cls,paths,fs,categories,index,gather_statistics,filters,split_row_groups,blocksize,aggregate_files,ignore_metadata_file,metadata_task_size,parquet_file_extension,kwargs)
dask.dataframe.io.parquet.fastparquet.FastParquetEngine._collect_file_parts(cls,pf_or_files,dataset_info_kwargs)
dask.dataframe.io.parquet.fastparquet.FastParquetEngine._construct_collection_plan(cls,dataset_info)
dask.dataframe.io.parquet.fastparquet.FastParquetEngine._create_dd_meta(cls,dataset_info)
dask.dataframe.io.parquet.fastparquet.FastParquetEngine._get_thrift_row_groups(cls,pf,filename,row_groups)
dask.dataframe.io.parquet.fastparquet.FastParquetEngine._make_part(cls,filename,rg_list,fs=None,pf=None,base_path=None,partitions=None)
dask.dataframe.io.parquet.fastparquet.FastParquetEngine._organize_row_groups(cls,pf,split_row_groups,gather_statistics,stat_col_indices,filters,dtypes,base_path,has_metadata_file,blocksize,aggregation_depth)
dask.dataframe.io.parquet.fastparquet.FastParquetEngine.initialize_write(cls,df,fs,path,append=False,partition_on=None,ignore_divisions=False,division_info=None,schema='infer',object_encoding='utf8',index_cols=None,custom_metadata=None,**kwargs)
dask.dataframe.io.parquet.fastparquet.FastParquetEngine.multi_support(cls)
dask.dataframe.io.parquet.fastparquet.FastParquetEngine.pf_to_pandas(cls,pf,fs=None,columns=None,categories=None,index=None,open_file_options=None,**kwargs)
dask.dataframe.io.parquet.fastparquet.FastParquetEngine.read_metadata(cls,fs,paths,categories=None,index=None,use_nullable_dtypes=None,dtype_backend=None,gather_statistics=None,filters=None,split_row_groups='adaptive',blocksize=None,aggregate_files=None,ignore_metadata_file=False,metadata_task_size=None,parquet_file_extension=None,**kwargs)
dask.dataframe.io.parquet.fastparquet.FastParquetEngine.read_partition(cls,fs,pieces,columns,index,dtype_backend=None,categories=(),root_cats=None,root_file_scheme=None,base_path=None,**kwargs)
dask.dataframe.io.parquet.fastparquet.FastParquetEngine.write_metadata(cls,parts,meta,fs,path,append=False,**kwargs)
dask.dataframe.io.parquet.fastparquet.FastParquetEngine.write_partition(cls,df,path,fs,filename,partition_on,return_metadata,fmd=None,compression=None,custom_metadata=None,**kwargs)
dask.dataframe.io.parquet.fastparquet._paths_to_cats(paths,file_scheme)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/io/parquet/arrow.py----------------------------------------
A:dask.dataframe.io.parquet.arrow.data_df->df.set_index(index).drop(partition_cols, axis='columns')
A:dask.dataframe.io.parquet.arrow.data_cols->df.set_index(index).columns.drop(partition_cols)
A:dask.dataframe.io.parquet.arrow.subschema->subschema.remove(subschema.get_field_index(col)).remove(subschema.get_field_index(col))
A:dask.dataframe.io.parquet.arrow.gb->df.set_index(index).drop(partition_cols, axis='columns').groupby(partition_keys, dropna=False, observed=False)
A:dask.dataframe.io.parquet.arrow.subdir->type(fs)(**region, **storage_options).sep.join([_hive_dirname(name, val) for (name, val) in zip(partition_cols, keys)])
A:dask.dataframe.io.parquet.arrow.subtable->pandas_to_arrow_table(subgroup, preserve_index=preserve_index, schema=subschema)
A:dask.dataframe.io.parquet.arrow.prefix->type(fs)(**region, **storage_options).sep.join([root_path, subdir])
A:dask.dataframe.io.parquet.arrow.full_path->type(fs)(**region, **storage_options).sep.join([p for p in [data_path, filename] if p != ''])
A:dask.dataframe.io.parquet.arrow.self.keys->pandas.Index(keys.sort_values(), copy=False)
A:dask.dataframe.io.parquet.arrow.read_kwargs->kwargs.get('read', {}).copy()
A:dask.dataframe.io.parquet.arrow.(precache_options, open_file_options)->_process_open_file_options(read_kwargs.pop('open_file_options', {}), **{'allow_precache': False, 'default_cache': 'none'} if _is_local_fs(fs) else {'columns': columns, 'row_groups': row_groups if row_groups == [None] else [row_groups], 'default_engine': 'pyarrow', 'default_cache': 'none'})
A:dask.dataframe.io.parquet.arrow.col->list(row_group).metadata.column(row_group_schema[column_name])
A:dask.dataframe.io.parquet.arrow.field_index->list(row_group).schema.get_field_index(name)
A:dask.dataframe.io.parquet.arrow.field->pyarrow.dataset.field(col)
A:dask.dataframe.io.parquet.arrow.expr->pyarrow.dataset.field(col).isin(val)
A:dask.dataframe.io.parquet.arrow.fs->type(fs)(**region, **storage_options)
A:dask.dataframe.io.parquet.arrow.fsspec_fs->ArrowFSWrapper(fs)
A:dask.dataframe.io.parquet.arrow.fs_strip->LocalFileSystem()
A:dask.dataframe.io.parquet.arrow.paths->type(fs)(**region, **storage_options).sep.join([base, fns[0]])
A:dask.dataframe.io.parquet.arrow.config_backend->dask.config.get('dataframe.dtype_backend', None)
A:dask.dataframe.io.parquet.arrow.dataset_info->cls._collect_dataset_info(paths, fs, categories, index, gather_statistics, filters, split_row_groups, blocksize, aggregate_files, ignore_metadata_file, metadata_task_size, parquet_file_extension, kwargs)
A:dask.dataframe.io.parquet.arrow.meta->clear_known_categories(meta, cols=[c for c in categories if c not in meta.index.names], dtype_backend=dtype_backend)
A:dask.dataframe.io.parquet.arrow.(parts, stats, common_kwargs)->cls._construct_collection_plan(dataset_info)
A:dask.dataframe.io.parquet.arrow.columns_and_parts->list(set(columns_and_parts) - set(df.index.names))
A:dask.dataframe.io.parquet.arrow.row_group->list(row_group)
A:dask.dataframe.io.parquet.arrow.arrow_table->arrow_table.append_column(partition.name, arr).append_column(partition.name, arr)
A:dask.dataframe.io.parquet.arrow.df->df.set_index(index).set_index(index)
A:dask.dataframe.io.parquet.arrow.df[partition.name]->pandas.Series(pd.Categorical(categories=partition.keys, values=df[partition.name].values), index=df.index)
A:dask.dataframe.io.parquet.arrow.index_in_columns_and_parts->set(df.index.names).issubset(set(columns_and_parts))
A:dask.dataframe.io.parquet.arrow.inferred_schema->inferred_schema.set(i, schema.field(j)).set(i, schema.field(j))
A:dask.dataframe.io.parquet.arrow.schema->parts[0][0].get('schema', None)
A:dask.dataframe.io.parquet.arrow.i->inferred_schema.set(i, schema.field(j)).set(i, schema.field(j)).get_field_index(name)
A:dask.dataframe.io.parquet.arrow.j->parts[0][0].get('schema', None).get_field_index(name)
A:dask.dataframe.io.parquet.arrow.ds->pyarrow.dataset.dataset(path_or_frag, filesystem=_wrapped_fs(fs), **_process_kwargs(**kwargs.get('dataset', {})))
A:dask.dataframe.io.parquet.arrow.i_offset->len(ds.files)
A:dask.dataframe.io.parquet.arrow.full_metadata->pyarrow.parquet.read_metadata(fil)
A:dask.dataframe.io.parquet.arrow.tail_metadata->pyarrow.parquet.read_metadata(fil)
A:dask.dataframe.io.parquet.arrow.arrow_schema->pyarrow.parquet.read_metadata(fil).schema.to_arrow_schema()
A:dask.dataframe.io.parquet.arrow.pandas_metadata->json.loads(arrow_schema.metadata[b'pandas'].decode('utf8'))
A:dask.dataframe.io.parquet.arrow.dtypes->_get_pyarrow_dtypes(arrow_schema, categories)
A:dask.dataframe.io.parquet.arrow.index_col_i->names.index(division_info['name'])
A:dask.dataframe.io.parquet.arrow.column->list(row_group).column(index_col_i)
A:dask.dataframe.io.parquet.arrow.df_schema->pyarrow.Schema.from_pandas(df)
A:dask.dataframe.io.parquet.arrow.expected->textwrap.indent(schema.to_string(show_schema_metadata=False), '    ')
A:dask.dataframe.io.parquet.arrow.actual->textwrap.indent(df_schema.to_string(show_schema_metadata=False), '    ')
A:dask.dataframe.io.parquet.arrow.t->t.replace_schema_metadata(metadata=_md).replace_schema_metadata(metadata=_md)
A:dask.dataframe.io.parquet.arrow.md_list->_write_partitioned(t, df, path, filename, partition_on, fs, cls._pandas_to_arrow_table, preserve_index, index_cols=index_cols, compression=compression, return_metadata=return_metadata, **kwargs)
A:dask.dataframe.io.parquet.arrow.common_metadata_path->type(fs)(**region, **storage_options).sep.join([path, '_common_metadata'])
A:dask.dataframe.io.parquet.arrow.metadata_path->type(fs)(**region, **storage_options).sep.join([out_path, '_metadata'])
A:dask.dataframe.io.parquet.arrow._dataset_kwargs->kwargs.pop('dataset', {})
A:dask.dataframe.io.parquet.arrow._dataset_kwargs['format']->pyarrow.dataset.ParquetFileFormat()
A:dask.dataframe.io.parquet.arrow._processed_dataset_kwargs->_process_kwargs(**_dataset_kwargs)
A:dask.dataframe.io.parquet.arrow.(paths, base, fns)->_sort_and_analyze_paths(paths, fs)
A:dask.dataframe.io.parquet.arrow.meta_path->type(fs)(**region, **storage_options).sep.join([base, '_metadata'])
A:dask.dataframe.io.parquet.arrow.len0->len(paths)
A:dask.dataframe.io.parquet.arrow.file_frag->next(ds.get_fragments())
A:dask.dataframe.io.parquet.arrow.split_row_groups->_infer_split_row_groups([rg.total_byte_size for rg in file_frag.row_groups], blocksize, bool(aggregate_files))
A:dask.dataframe.io.parquet.arrow.partition_names->list(ds.partitioning.schema.names)
A:dask.dataframe.io.parquet.arrow.aggregation_depth->_get_aggregation_depth(aggregate_files, partition_names)
A:dask.dataframe.io.parquet.arrow.arrow_to_pandas->dataset_info['kwargs'].get('arrow_to_pandas', {}).copy()
A:dask.dataframe.io.parquet.arrow.index_names->list(meta.index.names)
A:dask.dataframe.io.parquet.arrow.column_names->list(meta.columns)
A:dask.dataframe.io.parquet.arrow.(column_names, index_names)->_normalize_index_columns(columns, column_names + partitions, index, index_names)
A:dask.dataframe.io.parquet.arrow.meta.index->pandas.CategoricalIndex([], categories=partition.keys, name=meta.index.name)
A:dask.dataframe.io.parquet.arrow.meta[partition.name]->pandas.Series(pd.Categorical(categories=partition.keys, values=[]), index=meta.index)
A:dask.dataframe.io.parquet.arrow.metadata_task_size->_set_metadata_task_size(dataset_info['metadata_task_size'], fs)
A:dask.dataframe.io.parquet.arrow.filter_columns->set()
A:dask.dataframe.io.parquet.arrow.gather_statistics->_set_gather_statistics(gather_statistics, blocksize, split_row_groups, aggregation_depth, filter_columns, set(stat_col_indices))
A:dask.dataframe.io.parquet.arrow.ds_filters->_filters_to_expression(filters)
A:dask.dataframe.io.parquet.arrow.file_frags->list(pa_ds.dataset(files_or_frags, filesystem=_wrapped_fs(fs), **_process_kwargs(**dataset_options)).get_fragments())
A:dask.dataframe.io.parquet.arrow.(parts, stats)->Delayed('final-' + name, gather_parts_dsk).compute()
A:dask.dataframe.io.parquet.arrow.all_files->sorted(ds.files, key=natural_sort_key)
A:dask.dataframe.io.parquet.arrow.file_row_groups->defaultdict(list)
A:dask.dataframe.io.parquet.arrow.file_row_group_stats->defaultdict(list)
A:dask.dataframe.io.parquet.arrow.file_row_group_column_stats->defaultdict(list)
A:dask.dataframe.io.parquet.arrow.raw_keys->pyarrow.dataset._get_partition_keys(frag.partition_expression)
A:dask.dataframe.io.parquet.arrow.statistics->_get_rg_statistics(row_group, list(stat_col_indices))
A:dask.dataframe.io.parquet.arrow.last->cmax_last.get(name, None)
A:dask.dataframe.io.parquet.arrow.pkeys->partition_keys.get(full_path, None)
A:dask.dataframe.io.parquet.arrow.partitioning->kwargs.get('dataset', {}).get('partitioning', None)
A:dask.dataframe.io.parquet.arrow.frags->list(ds.get_fragments())
A:dask.dataframe.io.parquet.arrow.cat->keys_dict.get(partition.name, None)
A:dask.dataframe.io.parquet.arrow.arr->pyarrow.DictionaryArray.from_arrays(cat_ind, pa.array(partition.keys))
A:dask.dataframe.io.parquet.arrow.cat_ind->numpy.full(len(arrow_table), partition.keys.get_loc(cat), dtype='i4')
A:dask.dataframe.io.parquet.arrow.user_mapper->kwargs.get('arrow_to_pandas', {}).get('types_mapper')
A:dask.dataframe.io.parquet.arrow.converted_type->type_converter(pyarrow_dtype)
A:dask.dataframe.io.parquet.arrow._kwargs->kwargs.get('arrow_to_pandas', {})
A:dask.dataframe.io.parquet.arrow.types_mapper->cls._determine_type_mapper(dtype_backend=dtype_backend, convert_string=convert_string, **kwargs)
A:dask.dataframe.io.parquet.arrow.res->arrow_table.append_column(partition.name, arr).append_column(partition.name, arr).to_pandas(categories=categories, **_kwargs)
A:dask.dataframe.io.parquet.arrow.res.index->arrow_table.append_column(partition.name, arr).append_column(partition.name, arr).to_pandas(categories=categories, **_kwargs).index.astype(pd.StringDtype('pyarrow'))
dask.dataframe.io.parquet.arrow.ArrowDatasetEngine(Engine)
dask.dataframe.io.parquet.arrow.ArrowDatasetEngine._arrow_table_to_pandas(cls,arrow_table:pa.Table,categories,dtype_backend=None,convert_string=False,**kwargs)->pd.DataFrame
dask.dataframe.io.parquet.arrow.ArrowDatasetEngine._collect_dataset_info(cls,paths,fs,categories,index,gather_statistics,filters,split_row_groups,blocksize,aggregate_files,ignore_metadata_file,metadata_task_size,parquet_file_extension,kwargs)
dask.dataframe.io.parquet.arrow.ArrowDatasetEngine._collect_file_parts(cls,files_or_frags,dataset_info_kwargs)
dask.dataframe.io.parquet.arrow.ArrowDatasetEngine._construct_collection_plan(cls,dataset_info)
dask.dataframe.io.parquet.arrow.ArrowDatasetEngine._create_dd_meta(cls,dataset_info)
dask.dataframe.io.parquet.arrow.ArrowDatasetEngine._determine_type_mapper(cls,*,dtype_backend=None,convert_string=False,**kwargs)
dask.dataframe.io.parquet.arrow.ArrowDatasetEngine._make_part(cls,filename,rg_list,fs=None,partition_keys=None,partition_obj=None,data_path=None)
dask.dataframe.io.parquet.arrow.ArrowDatasetEngine._pandas_to_arrow_table(cls,df:pd.DataFrame,preserve_index=False,schema=None)->pa.Table
dask.dataframe.io.parquet.arrow.ArrowDatasetEngine._read_table(cls,path_or_frag,fs,row_groups,columns,schema,filters,partitions,partition_keys,**kwargs)
dask.dataframe.io.parquet.arrow.ArrowDatasetEngine.aggregate_metadata(cls,meta_list,fs,out_path)
dask.dataframe.io.parquet.arrow.ArrowDatasetEngine.collect_file_metadata(cls,path,fs,file_path)
dask.dataframe.io.parquet.arrow.ArrowDatasetEngine.extract_filesystem(cls,urlpath,filesystem,dataset_options,open_file_options,storage_options)
dask.dataframe.io.parquet.arrow.ArrowDatasetEngine.initialize_write(cls,df,fs,path,append=False,partition_on=None,ignore_divisions=False,division_info=None,schema='infer',index_cols=None,**kwargs)
dask.dataframe.io.parquet.arrow.ArrowDatasetEngine.multi_support(cls)
dask.dataframe.io.parquet.arrow.ArrowDatasetEngine.read_metadata(cls,fs,paths,categories=None,index=None,use_nullable_dtypes=None,dtype_backend=None,gather_statistics=None,filters=None,split_row_groups='adaptive',blocksize=None,aggregate_files=None,ignore_metadata_file=False,metadata_task_size=0,parquet_file_extension=None,**kwargs)
dask.dataframe.io.parquet.arrow.ArrowDatasetEngine.read_partition(cls,fs,pieces,columns,index,dtype_backend=None,categories=(),partitions=(),filters=None,schema=None,**kwargs)
dask.dataframe.io.parquet.arrow.ArrowDatasetEngine.write_metadata(cls,parts,meta,fs,path,append=False,**kwargs)
dask.dataframe.io.parquet.arrow.ArrowDatasetEngine.write_partition(cls,df,path,fs,filename,partition_on,return_metadata,fmd=None,compression=None,index_cols=None,schema=None,head=False,custom_metadata=None,**kwargs)
dask.dataframe.io.parquet.arrow.PartitionObj(self,name,keys)
dask.dataframe.io.parquet.arrow.PartitionObj.__init__(self,name,keys)
dask.dataframe.io.parquet.arrow._append_row_groups(metadata,md)
dask.dataframe.io.parquet.arrow._filters_to_expression(filters,propagate_null=False,nan_is_null=True)
dask.dataframe.io.parquet.arrow._frag_subset(old_frag,row_groups)
dask.dataframe.io.parquet.arrow._get_pandas_metadata(schema)
dask.dataframe.io.parquet.arrow._get_rg_statistics(row_group,col_names)
dask.dataframe.io.parquet.arrow._hive_dirname(name,val)
dask.dataframe.io.parquet.arrow._index_in_schema(index,schema)
dask.dataframe.io.parquet.arrow._need_filtering(filters,partition_keys)
dask.dataframe.io.parquet.arrow._process_kwargs(partitioning=None,**kwargs)
dask.dataframe.io.parquet.arrow._read_table_from_path(path,fs,row_groups,columns,schema,filters,**kwargs)
dask.dataframe.io.parquet.arrow._wrapped_fs(fs)
dask.dataframe.io.parquet.arrow._write_partitioned(table,df,root_path,filename,partition_cols,fs,pandas_to_arrow_table,preserve_index,index_cols=(),return_metadata=True,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/tests/test_ufunc.py----------------------------------------
A:dask.dataframe.tests.test_ufunc.pd->pytest.importorskip('pandas')
A:dask.dataframe.tests.test_ufunc.dafunc->getattr(da, ufunc)
A:dask.dataframe.tests.test_ufunc.npfunc->getattr(np, ufunc)
A:dask.dataframe.tests.test_ufunc.dask_input->dask.dataframe.from_pandas(pandas_input, 3)
A:dask.dataframe.tests.test_ufunc.s->pytest.importorskip('pandas').Series(np.random.randint(1, 100, size=20), index=list('abcdefghijklmnopqrst'))
A:dask.dataframe.tests.test_ufunc.ds->dask.dataframe.from_pandas(s, 3)
A:dask.dataframe.tests.test_ufunc.df->pytest.importorskip('pandas').DataFrame(input_matrix, columns=['A', 'B'])
A:dask.dataframe.tests.test_ufunc.ddf->dask.dataframe.from_pandas(df, 3)
A:dask.dataframe.tests.test_ufunc.pandas1->make_pandas_input()
A:dask.dataframe.tests.test_ufunc.pandas2->make_pandas_input()
A:dask.dataframe.tests.test_ufunc.dask1->dask.dataframe.from_pandas(pandas1, 3)
A:dask.dataframe.tests.test_ufunc.dask2->dask.dataframe.from_pandas(pandas2, 4)
A:dask.dataframe.tests.test_ufunc.dask->dask.dataframe.from_pandas(pandas, 3)
A:dask.dataframe.tests.test_ufunc.input_matrix->numpy.random.randint(1, 100, size=(20, 2))
A:dask.dataframe.tests.test_ufunc.df_out->pytest.importorskip('pandas').DataFrame(np.random.randint(1, 100, size=(20, 2)), columns=['X', 'Y'])
A:dask.dataframe.tests.test_ufunc.ddf_out_np->dask.dataframe.from_pandas(df_out, 3)
A:dask.dataframe.tests.test_ufunc.ddf_out_da->dask.dataframe.from_pandas(df_out, 3)
A:dask.dataframe.tests.test_ufunc.expected->pytest.importorskip('pandas').DataFrame(np.sin(input_matrix) + 10, columns=['A', 'B'])
A:dask.dataframe.tests.test_ufunc.ddf_out->dask.dataframe.from_pandas(df_out, 3)
A:dask.dataframe.tests.test_ufunc.np_redfunc->getattr(np, redfunc)
A:dask.dataframe.tests.test_ufunc.np_ufunc->getattr(np, ufunc)
dask.dataframe.tests.test_ufunc.test_2args_with_array(ufunc,pandas,darray)
dask.dataframe.tests.test_ufunc.test_clip(pandas,min,max)
dask.dataframe.tests.test_ufunc.test_frame_2ufunc_out()
dask.dataframe.tests.test_ufunc.test_frame_ufunc_out(ufunc)
dask.dataframe.tests.test_ufunc.test_mixed_types(ufunc,arg1,arg2)
dask.dataframe.tests.test_ufunc.test_ufunc(pandas_input,ufunc)
dask.dataframe.tests.test_ufunc.test_ufunc_numpy_scalar_comparison(pandas,scalar)
dask.dataframe.tests.test_ufunc.test_ufunc_with_2args(ufunc,make_pandas_input)
dask.dataframe.tests.test_ufunc.test_ufunc_with_reduction(redfunc,ufunc,pandas)
dask.dataframe.tests.test_ufunc.test_ufunc_wrapped(ufunc)
dask.dataframe.tests.test_ufunc.test_ufunc_wrapped_not_implemented()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/tests/test_accessors.py----------------------------------------
A:dask.dataframe.tests.test_accessors.pd->pytest.importorskip('pandas')
A:dask.dataframe.tests.test_accessors.before->set(dir(obj))
A:dask.dataframe.tests.test_accessors.instance->dask.dataframe.from_pandas(obj._partition_type([], dtype=float), 2)
A:dask.dataframe.tests.test_accessors.after->set(dir(obj))
A:dask.dataframe.tests.test_accessors.a->pytest.importorskip('pandas').Series([1, 2])
A:dask.dataframe.tests.test_accessors.b->dask.dataframe.from_pandas(a, 2)
A:dask.dataframe.tests.test_accessors.df->pytest.importorskip('pandas').DataFrame({'a': ['a\nb']}, index=index)
A:dask.dataframe.tests.test_accessors.df['string_col']->df['str_col'].astype('string')
A:dask.dataframe.tests.test_accessors.ddf->dask.dataframe.from_pandas(df, npartitions=1)
A:dask.dataframe.tests.test_accessors.ddf_result->dask.dataframe.from_pandas(df, npartitions=1).dt_col.dt.to_pydatetime()
A:dask.dataframe.tests.test_accessors.pd_result->pytest.importorskip('pandas').Series(df.dt_col.dt.to_pydatetime(), index=df.index, dtype=object)
A:dask.dataframe.tests.test_accessors.ctx->pytest.warns(pd.errors.PerformanceWarning, match='Falling back on a non-pyarrow')
A:dask.dataframe.tests.test_accessors.df.str_col->to_pyarrow_string(df.str_col)
A:dask.dataframe.tests.test_accessors.expected->pytest.importorskip('pandas').DataFrame({'a': ['a\nb']}, index=index).str_col.str.contains('d', case=False)
A:dask.dataframe.tests.test_accessors.sol->pytest.importorskip('pandas').DataFrame({'a': ['a\nb']}, index=index).str_col.str.cat(df.str_col.str.upper(), sep=':')
A:dask.dataframe.tests.test_accessors.s->pytest.importorskip('pandas').Series(['a b c', 'aa bb cc', 'aaa bbb ccc'])
A:dask.dataframe.tests.test_accessors.ds->dask.dataframe.from_pandas(s, npartitions=2)
A:dask.dataframe.tests.test_accessors.pd_a->df['a'].str.split('\n', n=1, expand=True)
A:dask.dataframe.tests.test_accessors.dd_a->ddf['a'].str.split('\n', n=1, expand=True)
dask.dataframe.tests.test_accessors.MyAccessor(self,obj)
dask.dataframe.tests.test_accessors.MyAccessor.__init__(self,obj)
dask.dataframe.tests.test_accessors.MyAccessor.method(self)
dask.dataframe.tests.test_accessors.MyAccessor.prop(self)
dask.dataframe.tests.test_accessors.df_ddf()
dask.dataframe.tests.test_accessors.ensure_removed(obj,attr)
dask.dataframe.tests.test_accessors.test_accessor_works()
dask.dataframe.tests.test_accessors.test_dt_accessor(df_ddf)
dask.dataframe.tests.test_accessors.test_dt_accessor_not_available(df_ddf)
dask.dataframe.tests.test_accessors.test_register(obj,registrar)
dask.dataframe.tests.test_accessors.test_str_accessor(df_ddf)
dask.dataframe.tests.test_accessors.test_str_accessor_cat(df_ddf)
dask.dataframe.tests.test_accessors.test_str_accessor_cat_none()
dask.dataframe.tests.test_accessors.test_str_accessor_extractall(df_ddf)
dask.dataframe.tests.test_accessors.test_str_accessor_getitem(df_ddf)
dask.dataframe.tests.test_accessors.test_str_accessor_not_available(df_ddf)
dask.dataframe.tests.test_accessors.test_str_accessor_removeprefix_removesuffix(df_ddf,method)
dask.dataframe.tests.test_accessors.test_str_accessor_split_expand(method)
dask.dataframe.tests.test_accessors.test_str_accessor_split_expand_more_columns()
dask.dataframe.tests.test_accessors.test_str_accessor_split_noexpand(method)
dask.dataframe.tests.test_accessors.test_str_split_no_warning(index)
dask.dataframe.tests.test_accessors.test_string_nullable_types(df_ddf)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/tests/test_optimize_dataframe.py----------------------------------------
A:dask.dataframe.tests.test_optimize_dataframe.dfs->list(dsk.values())
A:dask.dataframe.tests.test_optimize_dataframe.df->pandas.DataFrame({'x': range(10), 'y': range(10)})
A:dask.dataframe.tests.test_optimize_dataframe.a->s.__dask_optimize__(s.dask, s.__dask_keys__())
A:dask.dataframe.tests.test_optimize_dataframe.b->s.__dask_optimize__(s.dask, s.__dask_keys__())
A:dask.dataframe.tests.test_optimize_dataframe.ddf->dask.dataframe.from_pandas(df, npartitions=2)
A:dask.dataframe.tests.test_optimize_dataframe.graph->optimize_blockwise(ddf.dask)
dask.dataframe.tests.test_optimize_dataframe.test_fuse_ave_width()
dask.dataframe.tests.test_optimize_dataframe.test_optimize_blockwise()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/tests/test_boolean.py----------------------------------------
A:dask.dataframe.tests.test_boolean.values->pandas.array([True, False, None], dtype='boolean')
A:dask.dataframe.tests.test_boolean.ds->dask.dataframe.from_pandas(pd.Series(values), 2)
A:dask.dataframe.tests.test_boolean.ddf->dask.dataframe.from_pandas(pd.DataFrame({'A': values}), 2)
A:dask.dataframe.tests.test_boolean.s1->pandas.Series(pd.array([True, False, None] * 3, dtype='boolean'))
A:dask.dataframe.tests.test_boolean.s2->pandas.Series(pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype='boolean'))
A:dask.dataframe.tests.test_boolean.ds1->dask.dataframe.from_pandas(s1, 2)
A:dask.dataframe.tests.test_boolean.ds2->dask.dataframe.from_pandas(s2, 2)
dask.dataframe.tests.test_boolean.test_meta()
dask.dataframe.tests.test_boolean.test_ops()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/tests/test_rolling.py----------------------------------------
A:dask.dataframe.tests.test_rolling.df->pandas.DataFrame({'B': [0, 1, 2, np.nan, 4, 5, 6], 'C': ['a', 'a', 'a', 'b', 'b', 'a', 'b']})
A:dask.dataframe.tests.test_rolling.ddf->dask.dataframe.from_pandas(df, npartitions=2)
A:dask.dataframe.tests.test_rolling.ts->pandas.DataFrame(ts_data, index=idx)
A:dask.dataframe.tests.test_rolling.ts_constant_freq->pandas.DataFrame(ts_data, index=idx_constant_freq)
A:dask.dataframe.tests.test_rolling.dts->dask.dataframe.from_pandas(ts, 3)
A:dask.dataframe.tests.test_rolling.a->dask.dataframe.from_pandas(df, npartitions=2)
A:dask.dataframe.tests.test_rolling.b->pandas.DataFrame({'B': [0, 1, 2, np.nan, 4, 5, 6], 'C': ['a', 'a', 'a', 'b', 'b', 'a', 'b']}).shift(-after)
A:dask.dataframe.tests.test_rolling.res->repr(dts.rolling('4s'))
A:dask.dataframe.tests.test_rolling.sol->pandas.DataFrame({'B': [0, 1, 2, np.nan, 4, 5, 6], 'C': ['a', 'a', 'a', 'b', 'b', 'a', 'b']}).rolling(2).sum()
A:dask.dataframe.tests.test_rolling.ddf2->dask.dataframe.from_pandas(ddf2, 2 if align_dataframes else npartitions)
A:dask.dataframe.tests.test_rolling.res2->dask.dataframe.from_pandas(df, npartitions=2).map_overlap(shifted_sum, 0, 3, 0, 3, c=2, align_dataframes=align_dataframes, transform_divisions=transform_divisions, enforce_metadata=enforce_metadata)
A:dask.dataframe.tests.test_rolling.res3->dask.dataframe.from_pandas(df, npartitions=2).map_overlap(shifted_sum, 0, 3, 0, 3, c=3, align_dataframes=align_dataframes, transform_divisions=transform_divisions, enforce_metadata=enforce_metadata)
A:dask.dataframe.tests.test_rolling.res4->dask.dataframe.from_pandas(df, npartitions=2).map_overlap(shifted_sum, 3, 0, 0, 3, c=2)
A:dask.dataframe.tests.test_rolling.prolling->pandas.DataFrame(ts_data, index=idx).b.rolling(window)
A:dask.dataframe.tests.test_rolling.drolling->dask.dataframe.from_pandas(ts, 3).b.rolling(window)
A:dask.dataframe.tests.test_rolling.ctx->pytest.warns(FutureWarning, match='Using axis=1 in Rolling')
A:dask.dataframe.tests.test_rolling.expected->pandas.DataFrame({'B': [0, 1, 2, np.nan, 4, 5, 6], 'C': ['a', 'a', 'a', 'b', 'b', 'a', 'b']}).groupby('group1').column1.rolling('15D').mean()
A:dask.dataframe.tests.test_rolling.result->dask.dataframe.from_pandas(ts, 3).map_overlap(lambda x: x.rolling(window).count(), before, after)
A:dask.dataframe.tests.test_rolling.before->pandas.Timedelta(before)
A:dask.dataframe.tests.test_rolling.after->pandas.Timedelta(after)
A:dask.dataframe.tests.test_rolling.actual->dask.dataframe.from_pandas(df, npartitions=2).groupby('group1').column1.rolling('15D').mean()
dask.dataframe.tests.test_rolling.mad(x)
dask.dataframe.tests.test_rolling.shifted_sum(df,before,after,c=0)
dask.dataframe.tests.test_rolling.test_groupby_rolling()
dask.dataframe.tests.test_rolling.test_groupby_rolling_with_integer_window_raises()
dask.dataframe.tests.test_rolling.test_map_overlap(npartitions,use_dask_input)
dask.dataframe.tests.test_rolling.test_map_overlap_errors()
dask.dataframe.tests.test_rolling.test_map_overlap_multiple_dataframes(use_dask_input,npartitions,enforce_metadata,transform_divisions,align_dataframes,overlap_setup)
dask.dataframe.tests.test_rolling.test_map_overlap_names(npartitions,enforce_metadata,transform_divisions,align_dataframes)
dask.dataframe.tests.test_rolling.test_map_overlap_provide_meta()
dask.dataframe.tests.test_rolling.test_rolling_agg_aggregate()
dask.dataframe.tests.test_rolling.test_rolling_axis(kwargs)
dask.dataframe.tests.test_rolling.test_rolling_cov(window,center)
dask.dataframe.tests.test_rolling.test_rolling_methods(method,args,window,center,check_less_precise)
dask.dataframe.tests.test_rolling.test_rolling_names()
dask.dataframe.tests.test_rolling.test_rolling_numba_engine()
dask.dataframe.tests.test_rolling.test_rolling_partition_size()
dask.dataframe.tests.test_rolling.test_rolling_raises()
dask.dataframe.tests.test_rolling.test_rolling_repr()
dask.dataframe.tests.test_rolling.test_time_rolling(before,after)
dask.dataframe.tests.test_rolling.test_time_rolling_constructor()
dask.dataframe.tests.test_rolling.test_time_rolling_cov(window)
dask.dataframe.tests.test_rolling.test_time_rolling_large_window_fixed_chunks(window,N)
dask.dataframe.tests.test_rolling.test_time_rolling_large_window_variable_chunks(window)
dask.dataframe.tests.test_rolling.test_time_rolling_methods(method,args,window,check_less_precise)
dask.dataframe.tests.test_rolling.test_time_rolling_repr()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/tests/test_dataframe.py----------------------------------------
A:dask.dataframe.tests.test_dataframe.DASK_EXPR_ENABLED->dask.dataframe._dask_expr_enabled()
A:dask.dataframe.tests.test_dataframe.meta->pandas.DataFrame(np.random.randn(10, 3), columns=list('abc')).groupby('grouper').transform(func)
A:dask.dataframe.tests.test_dataframe.d->pandas.DataFrame({'g': [0, 0, 1] * 3, 'b': [1, 2, 3] * 3})
A:dask.dataframe.tests.test_dataframe.full->pandas.DataFrame({'g': [0, 0, 1] * 3, 'b': [1, 2, 3] * 3}).compute(scheduler='sync')
A:dask.dataframe.tests.test_dataframe.expected->pandas.DataFrame(np.random.randn(10, 3), columns=list('abc')).where(cond=cond, other=5)
A:dask.dataframe.tests.test_dataframe.d2->dask.dataframe.from_pandas(pd.DataFrame({'x': [1, 2, 3]}), npartitions=1)
A:dask.dataframe.tests.test_dataframe.ddf->dask.dataframe.from_pandas(pdf, 5)
A:dask.dataframe.tests.test_dataframe.pdf->pandas.DataFrame({'x': range(50)})
A:dask.dataframe.tests.test_dataframe.df->pandas.DataFrame(np.random.randn(10, 3), columns=list('abc'))
A:dask.dataframe.tests.test_dataframe.ps->pandas.Series([1.123, 2.123, 3.123, 1.234, 2.234, 3.234], name='a')
A:dask.dataframe.tests.test_dataframe.ds->dask.dataframe.from_pandas(s, npartitions=2)
A:dask.dataframe.tests.test_dataframe.val->numpy.int64(1)
A:dask.dataframe.tests.test_dataframe.s->pandas.Series(range(10))
A:dask.dataframe.tests.test_dataframe.idx->pandas.Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], name='x')
A:dask.dataframe.tests.test_dataframe.res->pxy['b'].groupby(pxy['g']).sum()
A:dask.dataframe.tests.test_dataframe.ds2->dask.dataframe.from_pandas(s, npartitions=2).repartition(divisions=type_ctor(ds.divisions))
A:dask.dataframe.tests.test_dataframe.df['a']->df['a'].astype(get_string_dtype()).astype(get_string_dtype())
A:dask.dataframe.tests.test_dataframe.actual->f(ddf.A, offset)
A:dask.dataframe.tests.test_dataframe.df_none->pandas.DataFrame({'A': [None, None]})
A:dask.dataframe.tests.test_dataframe.ddf_none->dask.dataframe.from_pandas(df_none, 2)
A:dask.dataframe.tests.test_dataframe.df_len0->pandas.DataFrame({'A': []})
A:dask.dataframe.tests.test_dataframe.ddf_len0->dask.dataframe.from_pandas(df_len0, 2)
A:dask.dataframe.tests.test_dataframe.ddf_nocols->dask.dataframe.from_pandas(pd.DataFrame({}), 2)
A:dask.dataframe.tests.test_dataframe.A->pytest.importorskip('dask.array').arange(0, 101)
A:dask.dataframe.tests.test_dataframe.r->ddf['a'].quantile(0.5).compute()
A:dask.dataframe.tests.test_dataframe.df_out->pandas.DataFrame(np.random.randn(100, 5), columns=list('abcde'), index=index)
A:dask.dataframe.tests.test_dataframe.ddf_out->dask.dataframe.from_pandas(df_out, 5)
A:dask.dataframe.tests.test_dataframe.l->pandas.Series({'a': 2, 'b': 3})
A:dask.dataframe.tests.test_dataframe.u->pandas.Series({'a': 7, 'b': 5})
A:dask.dataframe.tests.test_dataframe.dl->dask.dataframe.from_pandas(l, 3)
A:dask.dataframe.tests.test_dataframe.du->dask.dataframe.from_pandas(u, 3)
A:dask.dataframe.tests.test_dataframe.df2->from_pyarrow_table_dispatch(df1, to_pyarrow_table_dispatch(df1))
A:dask.dataframe.tests.test_dataframe.ddf2->dask.dataframe.from_pandas(pdf2, 2)
A:dask.dataframe.tests.test_dataframe.pdf1->pandas.DataFrame({'a': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'b': [3, 5, 2, 5, 7, 2, 4, 2, 4]})
A:dask.dataframe.tests.test_dataframe.ddf1->dask.dataframe.from_pandas(pdf1, 2)
A:dask.dataframe.tests.test_dataframe.pdf2->pandas.DataFrame({'a': [True, False, True] * 3, 'b': [False, False, True] * 3})
A:dask.dataframe.tests.test_dataframe.pdf3->pandas.DataFrame({'a': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'b': [3, 5, 2, 5, 7, 2, 4, 2, 4]}, index=[0, 1, 2, 3, 4, 5, 6, 7, 8])
A:dask.dataframe.tests.test_dataframe.ddf3->dask.dataframe.repartition(ddf, divisions=type_ctor(ddf.divisions))
A:dask.dataframe.tests.test_dataframe.pdf4->pandas.DataFrame({'a': [True, False, True] * 3, 'b': [False, False, True] * 3}, index=[5, 6, 7, 8, 9, 10, 11, 12, 13])
A:dask.dataframe.tests.test_dataframe.ddf4->dask.dataframe.from_pandas(pdf4, 2)
A:dask.dataframe.tests.test_dataframe.pdf5->pandas.DataFrame({'a': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'b': [9, 4, 2, 6, 2, 3, 1, 6, 2], 'c': [5, 6, 7, 8, 9, 10, 11, 12, 13]}, index=[0, 1, 2, 3, 4, 5, 6, 7, 8])
A:dask.dataframe.tests.test_dataframe.ddf5->dask.dataframe.from_pandas(pdf5, 2)
A:dask.dataframe.tests.test_dataframe.pdf6->pandas.DataFrame({'a': [True, False, True] * 3, 'b': [False, False, True] * 3, 'c': [False] * 9, 'd': [True] * 9}, index=[5, 6, 7, 8, 9, 10, 11, 12, 13])
A:dask.dataframe.tests.test_dataframe.ddf6->dask.dataframe.from_pandas(pdf6, 2)
A:dask.dataframe.tests.test_dataframe.result->dask.dataframe.from_pandas(pdf, 5).where(cond=dd_cond, other=5)
A:dask.dataframe.tests.test_dataframe.layer->hlg_layer(df.dask, 'f-')
A:dask.dataframe.tests.test_dataframe.x->timeseries().shuffle('id', shuffle_method='tasks').head(compute=False)
A:dask.dataframe.tests.test_dataframe.a->a.map_partitions(f, meta=a._meta).map_partitions(f, meta=a._meta)
A:dask.dataframe.tests.test_dataframe.b->dask.dataframe.from_pandas(pdf, 5).map_partitions(lambda x, y: x, y=big)
A:dask.dataframe.tests.test_dataframe.index->numpy.arange(100)
A:dask.dataframe.tests.test_dataframe.sol->pandas.DataFrame(np.random.randn(10, 3), columns=list('abc')).drop_duplicates(subset=subset, keep=keep)
A:dask.dataframe.tests.test_dataframe.df1->pytest.importorskip('cudf').DataFrame(np.random.randn(10, 3), columns=list('abc'))
A:dask.dataframe.tests.test_dataframe.res2->f(ddf.a, m, 5, split_every=2)
A:dask.dataframe.tests.test_dataframe._d->pandas.DataFrame({'g': [0, 0, 1] * 3, 'b': [1, 2, 3] * 3}).clear_divisions()
A:dask.dataframe.tests.test_dataframe.div1->dask.dataframe.from_pandas(pdf, 5).a.get_partition(0)
A:dask.dataframe.tests.test_dataframe.div2->dask.dataframe.from_pandas(pdf, 5).a.get_partition(1)
A:dask.dataframe.tests.test_dataframe.div3->dask.dataframe.from_pandas(pdf, 5).a.get_partition(2)
A:dask.dataframe.tests.test_dataframe.result2->dask.dataframe.from_pandas(pdf, 5).A.corr(ddf.B.rename('A'), split_every=2)
A:dask.dataframe.tests.test_dataframe.result3->dask.dataframe.from_pandas(pdf, 5).x.value_counts(split_out=2, dropna=False, normalize=normalize)
A:dask.dataframe.tests.test_dataframe.result4->dask.dataframe.from_pandas(pdf, 5).x.value_counts(dropna=True, normalize=normalize, split_out=2)
A:dask.dataframe.tests.test_dataframe.expected4->pandas.DataFrame(np.random.randn(10, 3), columns=list('abc')).x.value_counts(dropna=True, normalize=normalize)
A:dask.dataframe.tests.test_dataframe.f_series->pandas.Series(f_list)
A:dask.dataframe.tests.test_dataframe.array->pytest.importorskip('dask.array').random.exponential(1, 10000, chunks=10000 // 2)
A:dask.dataframe.tests.test_dataframe.exp->pandas.DataFrame({'g': [0, 0, 1] * 3, 'b': [1, 2, 3] * 3}).compute(scheduler='sync').b.quantile([])
A:dask.dataframe.tests.test_dataframe.ctx->contextlib.nullcontext()
A:dask.dataframe.tests.test_dataframe.ddf_unknown->dask.dataframe.from_pandas(df, npartitions=5, sort=False)
A:dask.dataframe.tests.test_dataframe.res_unknown->dask.dataframe.from_pandas(df, npartitions=5, sort=False).assign(c=1, d='string', e=ddf_unknown.a.sum(), f=ddf_unknown.a + ddf_unknown.b, g=lambda x: x.a + x.c, dt=pd.Timestamp(2018, 2, 13))
A:dask.dataframe.tests.test_dataframe.string_dtype->get_string_dtype()
A:dask.dataframe.tests.test_dataframe.lk->pandas.Series(lk)
A:dask.dataframe.tests.test_dataframe.e->pandas.DataFrame({'g': [0, 0, 1] * 3, 'b': [1, 2, 3] * 3}).assign(c=d.a + 1)
A:dask.dataframe.tests.test_dataframe.f->type(e)(*e._args)
A:dask.dataframe.tests.test_dataframe.df1a->pandas.DataFrame({'A': np.random.randn(10), 'B': np.random.randn(10), 'C': np.random.randn(10)}, index=[1, 12, 5, 6, 3, 9, 10, 4, 13, 11])
A:dask.dataframe.tests.test_dataframe.df1b->pandas.DataFrame({'B': np.random.randn(10), 'C': np.random.randn(10), 'D': np.random.randn(10)}, index=[0, 3, 2, 10, 5, 6, 7, 8, 12, 13])
A:dask.dataframe.tests.test_dataframe.ddf1a->dask.dataframe.from_pandas(df1a, 3)
A:dask.dataframe.tests.test_dataframe.ddf1b->dask.dataframe.from_pandas(df1b, 3)
A:dask.dataframe.tests.test_dataframe.(res1, res2)->dask.dataframe.from_pandas(df1a, 3).align(ddf1b, join=join, axis='columns')
A:dask.dataframe.tests.test_dataframe.(exp1, exp2)->pandas.DataFrame({'A': np.random.randn(10), 'B': np.random.randn(10), 'C': np.random.randn(10)}, index=[1, 12, 5, 6, 3, 9, 10, 4, 13, 11]).align(df1b, join=join, axis='columns')
A:dask.dataframe.tests.test_dataframe.a2->cp_loads(cp_dumps(df.A))
A:dask.dataframe.tests.test_dataframe.i2->cp_loads(cp_dumps(df.index))
A:dask.dataframe.tests.test_dataframe.s2->pandas.Series([np.nan, np.nan, np.nan, np.nan])
A:dask.dataframe.tests.test_dataframe.(a, b)->pandas.DataFrame({'g': [0, 0, 1] * 3, 'b': [1, 2, 3] * 3}).random_split([0.5, 0.5], 42, True)
A:dask.dataframe.tests.test_dataframe.(a2, b2)->pandas.DataFrame({'g': [0, 0, 1] * 3, 'b': [1, 2, 3] * 3}).random_split([0.5, 0.5], 42, True)
A:dask.dataframe.tests.test_dataframe.parts->dask.get(b.dask, b.__dask_keys__())
A:dask.dataframe.tests.test_dataframe.keys->sorted(keys)
A:dask.dataframe.tests.test_dataframe.sp->pandas.concat([compute_as_if_collection(dd.DataFrame, d.dask, k) for k in keys])
A:dask.dataframe.tests.test_dataframe.rddf->dask.dataframe.from_pandas(pdf, 5).repartition(divisions=div, force=True)
A:dask.dataframe.tests.test_dataframe.rds->dask.dataframe.from_pandas(s, npartitions=2).repartition(divisions=div, force=True)
A:dask.dataframe.tests.test_dataframe.s.index->pandas.to_datetime([datetime(2020, 1, 1, 12, 0) + timedelta(minutes=x) for x in s], utc=True)
A:dask.dataframe.tests.test_dataframe.start->pandas.Timestamp(start)
A:dask.dataframe.tests.test_dataframe.end->pandas.Timestamp(end)
A:dask.dataframe.tests.test_dataframe.ind->pandas.date_range(start=start, end=end, freq='60s')
A:dask.dataframe.tests.test_dataframe.ts->pandas.date_range('2015-01-01 00:00', '2015-05-01 23:50', freq='10min')
A:dask.dataframe.tests.test_dataframe.ds3->dask.dataframe.repartition(ds, divisions=type_ctor(ds.divisions))
A:dask.dataframe.tests.test_dataframe.new_freq->_map_freq_to_period_start(freq)
A:dask.dataframe.tests.test_dataframe.delayed->pytest.importorskip('cudf').DataFrame(np.random.randn(10, 3), columns=list('abc')).to_delayed(optimize_graph=optimize)
A:dask.dataframe.tests.test_dataframe.dm->pandas.DataFrame({'g': [0, 0, 1] * 3, 'b': [1, 2, 3] * 3}).a.mean().to_delayed(optimize_graph=optimize)
A:dask.dataframe.tests.test_dataframe.df3->dask.dataframe.from_delayed(delayed2, meta=df1, divisions=df1.divisions)
A:dask.dataframe.tests.test_dataframe.ddf.b->dask.dataframe.from_pandas(pdf, 5).b.fillna(ddf.a)
A:dask.dataframe.tests.test_dataframe.fill_value->pandas.Series([1, 10], index=['A', 'C'])
A:dask.dataframe.tests.test_dataframe.c->a.map_partitions(f, meta=a._meta).map_partitions(f, meta=a._meta).copy(deep=False)
A:dask.dataframe.tests.test_dataframe.bb->dask.dataframe.from_pandas(pdf, 5).map_partitions(lambda x, y: x, y=big).index.compute()
A:dask.dataframe.tests.test_dataframe.p->pandas.DataFrame({'x': [1, 2, 3, 4], 'y': [5, 6, 7, 8]})
A:dask.dataframe.tests.test_dataframe.deprecate_ctx->pytest.warns(FutureWarning, match='`inplace` is deprecated')
A:dask.dataframe.tests.test_dataframe.df['cstr']->df['cstr'].astype(get_string_dtype()).astype(get_string_dtype())
A:dask.dataframe.tests.test_dataframe.r3->f(3)
A:dask.dataframe.tests.test_dataframe.r4->f(4)
A:dask.dataframe.tests.test_dataframe.arr->numpy.array(np.random.normal(size=50))
A:dask.dataframe.tests.test_dataframe.ddf_index_only->dask.dataframe.from_pandas(pdf, 5).set_index('x')
A:dask.dataframe.tests.test_dataframe.ddf_result->dask.dataframe.from_pandas(pdf, 5).applymap(lambda x: (x, x))
A:dask.dataframe.tests.test_dataframe.pdf_result->pandas.DataFrame(np.random.randn(10, 3), columns=list('abc')).applymap(lambda x: (x, x))
A:dask.dataframe.tests.test_dataframe.res3->pytest.importorskip('dask.array').corr(db, min_periods=10)
A:dask.dataframe.tests.test_dataframe.res4->pytest.importorskip('dask.array').corr(db, min_periods=10, split_every=2)
A:dask.dataframe.tests.test_dataframe.sol2->pytest.importorskip('dask.array').corr(db, min_periods=10)
A:dask.dataframe.tests.test_dataframe.da->pytest.importorskip('dask.array')
A:dask.dataframe.tests.test_dataframe.db->dask.dataframe.from_pandas(b, npartitions=6)
A:dask.dataframe.tests.test_dataframe.cudf->pytest.importorskip('cudf')
A:dask.dataframe.tests.test_dataframe.df['categorical_nans']->df['categorical_nans'].replace('c', np.nan).replace('c', np.nan)
A:dask.dataframe.tests.test_dataframe.df['categorical_binary']->df['categorical_binary'].astype('category').astype('category')
A:dask.dataframe.tests.test_dataframe.df['unique_id']->df['unique_id'].astype(str).astype(str)
A:dask.dataframe.tests.test_dataframe.dx->(ddf.values + 1).astype(float)
A:dask.dataframe.tests.test_dataframe.i->dask.dataframe._compat.makeTimeSeries()
A:dask.dataframe.tests.test_dataframe.pd_items->df['x'].iteritems()
A:dask.dataframe.tests.test_dataframe.dd_items->ddf['x'].iteritems()
A:dask.dataframe.tests.test_dataframe.abc->pandas.api.types.CategoricalDtype(['a', 'b', 'c'], ordered=False)
A:dask.dataframe.tests.test_dataframe.category->pandas.api.types.CategoricalDtype(ordered=False)
A:dask.dataframe.tests.test_dataframe.dx2->dask.dataframe.from_pandas(pdf, 5).x.astype(dtype)
A:dask.dataframe.tests.test_dataframe.stdout_pd->buf_pd.getvalue()
A:dask.dataframe.tests.test_dataframe.stdout_da->stdout_da.replace(str(type(ddf)), str(type(df))).replace(str(type(ddf)), str(type(df)))
A:dask.dataframe.tests.test_dataframe.buf->StringIO()
A:dask.dataframe.tests.test_dataframe.g->dask.dataframe.from_pandas(pdf, 5).groupby(['A', 'B']).agg(['count', 'sum'])
A:dask.dataframe.tests.test_dataframe.memory_usage->float(ddf.memory_usage().sum().compute())
A:dask.dataframe.tests.test_dataframe.orig->dask.dataframe.from_pandas(pdf, 5).copy()
A:dask.dataframe.tests.test_dataframe.darr->pytest.importorskip('dask.array').from_array(arr, chunks=10)
A:dask.dataframe.tests.test_dataframe.column_1->string_subclass('column_1')
A:dask.dataframe.tests.test_dataframe.cols->col_type(['C', 'A', 'B'])
A:dask.dataframe.tests.test_dataframe.completions->dask.dataframe.from_pandas(pdf, 5)._ipython_key_completions_()
A:dask.dataframe.tests.test_dataframe.dropped->dask.dataframe.from_pandas(s, npartitions=2).unique(split_every=split_every, split_out=split_out)
A:dask.dataframe.tests.test_dataframe.dsk->dask.dataframe.from_pandas(s, npartitions=2).unique(split_every=split_every, split_out=split_out).__dask_optimize__(dropped.dask, dropped.__dask_keys__())
A:dask.dataframe.tests.test_dataframe.(dependencies, dependents)->get_deps(dsk)
A:dask.dataframe.tests.test_dataframe.y->pandas.DataFrame(np.random.randn(10, 3), columns=list('abc')).map_partitions(sparse.csr_matrix)
A:dask.dataframe.tests.test_dataframe.rs->numpy.random.RandomState(1)
A:dask.dataframe.tests.test_dataframe.ddf_single->dask.dataframe.from_pandas(df, npartitions=1)
A:dask.dataframe.tests.test_dataframe.dtRange->pandas.date_range('01.01.2015', '05.05.2015')
A:dask.dataframe.tests.test_dataframe.d[c + 'cs']->d[c].cumsum()
A:dask.dataframe.tests.test_dataframe.d[c + 'cmin']->d[c].cummin()
A:dask.dataframe.tests.test_dataframe.d[c + 'cmax']->d[c].cummax()
A:dask.dataframe.tests.test_dataframe.d[c + 'cp']->d[c].cumprod()
A:dask.dataframe.tests.test_dataframe.sparse->pytest.importorskip('scipy.sparse')
A:dask.dataframe.tests.test_dataframe.computed->dask.dataframe.from_pandas(pdf, 5).where(cond=dd_cond, other=5).compute()
A:dask.dataframe.tests.test_dataframe.base->pandas.Series([''.join(np.random.choice(['a', 'b', 'c'], size=3)) for x in range(100)])
A:dask.dataframe.tests.test_dataframe.offsets->pandas.Series([pd.offsets.DateOffset(years=o) for o in range(3)])
A:dask.dataframe.tests.test_dataframe.dask_base->dask.dataframe.from_pandas(base, npartitions=base_npart, sort=False)
A:dask.dataframe.tests.test_dataframe.dask_offsets->dask.dataframe.from_pandas(offsets, npartitions=1)
A:dask.dataframe.tests.test_dataframe.dask_offsets._meta->pandas.Series([pd.offsets.DateOffset(years=o) for o in range(3)]).head()
A:dask.dataframe.tests.test_dataframe.vs->pandas.DataFrame(np.random.randn(10, 3), columns=list('abc')).map_partitions(sparse.csr_matrix).to_delayed().flatten().tolist()
A:dask.dataframe.tests.test_dataframe.values->dask.compute(*vs, scheduler='single-threaded')
A:dask.dataframe.tests.test_dataframe.big->numpy.ones(1000000)
A:dask.dataframe.tests.test_dataframe.cleared->dask.dataframe.from_pandas(pdf, 5).index.map(lambda x: x * 10)
A:dask.dataframe.tests.test_dataframe.applied->dask.dataframe.from_pandas(pdf, 5).index.map(lambda x: x * 10, is_monotonic=True)
A:dask.dataframe.tests.test_dataframe.ddf_copy->dask.dataframe.from_pandas(pdf, 5).copy()
A:dask.dataframe.tests.test_dataframe.L->list(range(100))
A:dask.dataframe.tests.test_dataframe.out->dask.dataframe.from_pandas(pdf, 5).map_partitions(lambda x, y: x + sum(y), L)
A:dask.dataframe.tests.test_dataframe.mapper->pandas.Series(np.random.randint(50, size=len(map_index)), index=map_index)
A:dask.dataframe.tests.test_dataframe.map_index->numpy.array(map_index)
A:dask.dataframe.tests.test_dataframe.dask_map->dask.dataframe.from_pandas(mapper, npartitions=map_npart, sort=False)
A:dask.dataframe.tests.test_dataframe.exploded_df->pandas.DataFrame(np.random.randn(10, 3), columns=list('abc')).explode('A')
A:dask.dataframe.tests.test_dataframe.exploded_ddf->dask.dataframe.from_pandas(pdf, 5).explode('A')
A:dask.dataframe.tests.test_dataframe.exploded_s->pandas.Series(range(10)).explode()
A:dask.dataframe.tests.test_dataframe.exploded_ds->dask.dataframe.from_pandas(s, npartitions=2).explode()
A:dask.dataframe.tests.test_dataframe.[v]->task[0].dsk.values()
A:dask.dataframe.tests.test_dataframe.hlg->fuse_roots(res.__dask_graph__(), keys=res.__dask_keys__())
A:dask.dataframe.tests.test_dataframe.expected_df->dask.dataframe.from_pandas(df.join(df['x'], lsuffix='_'), npartitions=1)
A:dask.dataframe.tests.test_dataframe.actual_df->dask.dataframe.from_pandas(pdf, 5).join(ddf['x'], lsuffix='_')
A:dask.dataframe.tests.test_dataframe.ddi->dds.min()
A:dask.dataframe.tests.test_dataframe.df_pandas->pandas.DataFrame({'a': [1.1]}, dtype='Float64')
A:dask.dataframe.tests.test_dataframe.s1->pandas.Series([1, 2, 3, 4])
A:dask.dataframe.tests.test_dataframe.dask_s1->dask.dataframe.from_pandas(s1, npartitions=1)
A:dask.dataframe.tests.test_dataframe.dask_df->dask.dataframe.from_pandas(df, npartitions=1)
A:dask.dataframe.tests.test_dataframe.dask_s2->dask.dataframe.from_pandas(s2, npartitions=1)
A:dask.dataframe.tests.test_dataframe.partitioned_s1->dask.dataframe.from_pandas(s1, npartitions=2)
A:dask.dataframe.tests.test_dataframe.partitioned_df->dask.dataframe.from_pandas(df, npartitions=2)
A:dask.dataframe.tests.test_dataframe.partitioned_s2->dask.dataframe.from_pandas(s2, npartitions=2)
A:dask.dataframe.tests.test_dataframe.df_pxy->weakref.proxy(df)
A:dask.dataframe.tests.test_dataframe.ser->pandas.Series({'data': [1, 2, 3]})
A:dask.dataframe.tests.test_dataframe.ser_pxy->weakref.proxy(ser)
A:dask.dataframe.tests.test_dataframe.pxy->weakref.proxy(a)
A:dask.dataframe.tests.test_dataframe.pds->pandas.Series(series, index=series)
A:dask.dataframe.tests.test_dataframe.s_2->pandas.Series(list(reversed(s)))
A:dask.dataframe.tests.test_dataframe.ds_2->dask.dataframe.from_pandas(s_2, npartitions=5, sort=False)
A:dask.dataframe.tests.test_dataframe.merged->mapped.copy()
A:dask.dataframe.tests.test_dataframe.pa->pytest.importorskip('pyarrow')
A:dask.dataframe.tests.test_dataframe.pa_dtype->pytest.importorskip('pyarrow').decimal128(precision=7, scale=3)
A:dask.dataframe.tests.test_dataframe.data->pytest.importorskip('pyarrow').array([decimal.Decimal('8093.234'), decimal.Decimal('8094.234'), decimal.Decimal('8095.234'), decimal.Decimal('8096.234'), decimal.Decimal('8097.234'), decimal.Decimal('8098.234')], type=pa_dtype)
A:dask.dataframe.tests.test_dataframe.dd_cond->pandas.DataFrame(cond, index=df.index, columns=df.columns)
A:dask.dataframe.tests.test_dataframe.df['d']->pandas.Series(['cat', 'dog'] * 5, dtype='string[pyarrow]')
A:dask.dataframe.tests.test_dataframe.table->to_pyarrow_table_dispatch(df, preserve_index=preserve_index)
A:dask.dataframe.tests.test_dataframe.schema->pyarrow_schema_dispatch(df, preserve_index=preserve_index)
A:dask.dataframe.tests.test_dataframe.df1['d']->pandas.Series(['cat', 'dog'] * 5, dtype='string[pyarrow]')
A:dask.dataframe.tests.test_dataframe.divisions->list(ddf.divisions)
A:dask.dataframe.tests.test_dataframe.ddf.divisions->tuple(divisions)
dask.dataframe.tests.test_dataframe._assert_info(df,ddf,memory_usage=True)
dask.dataframe.tests.test_dataframe._drop_mean(df,col=None)
dask.dataframe.tests.test_dataframe.assert_numeric_only_default_warning(numeric_only,func=None)
dask.dataframe.tests.test_dataframe.test_Dataframe()
dask.dataframe.tests.test_dataframe.test_Index()
dask.dataframe.tests.test_dataframe.test_Scalar()
dask.dataframe.tests.test_dataframe.test_Series()
dask.dataframe.tests.test_dataframe.test_abs()
dask.dataframe.tests.test_dataframe.test_aca_meta_infer()
dask.dataframe.tests.test_dataframe.test_aca_split_every()
dask.dataframe.tests.test_dataframe.test_add_prefix()
dask.dataframe.tests.test_dataframe.test_add_suffix()
dask.dataframe.tests.test_dataframe.test_align(join)
dask.dataframe.tests.test_dataframe.test_align_axis(join)
dask.dataframe.tests.test_dataframe.test_align_dataframes()
dask.dataframe.tests.test_dataframe.test_apply()
dask.dataframe.tests.test_dataframe.test_apply_convert_dtype(convert_dtype)
dask.dataframe.tests.test_dataframe.test_apply_infer_columns()
dask.dataframe.tests.test_dataframe.test_apply_warns()
dask.dataframe.tests.test_dataframe.test_apply_warns_with_invalid_meta()
dask.dataframe.tests.test_dataframe.test_applymap()
dask.dataframe.tests.test_dataframe.test_args()
dask.dataframe.tests.test_dataframe.test_array_assignment()
dask.dataframe.tests.test_dataframe.test_assign()
dask.dataframe.tests.test_dataframe.test_assign_callable()
dask.dataframe.tests.test_dataframe.test_assign_dtypes()
dask.dataframe.tests.test_dataframe.test_assign_index()
dask.dataframe.tests.test_dataframe.test_assign_na_float_columns()
dask.dataframe.tests.test_dataframe.test_assign_no_warning_fragmented()
dask.dataframe.tests.test_dataframe.test_assign_pandas_series()
dask.dataframe.tests.test_dataframe.test_astype()
dask.dataframe.tests.test_dataframe.test_astype_categoricals()
dask.dataframe.tests.test_dataframe.test_astype_categoricals_known()
dask.dataframe.tests.test_dataframe.test_attribute_assignment()
dask.dataframe.tests.test_dataframe.test_attributes()
dask.dataframe.tests.test_dataframe.test_attrs_dataframe()
dask.dataframe.tests.test_dataframe.test_attrs_series()
dask.dataframe.tests.test_dataframe.test_autocorr()
dask.dataframe.tests.test_dataframe.test_axes()
dask.dataframe.tests.test_dataframe.test_better_errors_object_reductions()
dask.dataframe.tests.test_dataframe.test_bfill()
dask.dataframe.tests.test_dataframe.test_bool()
dask.dataframe.tests.test_dataframe.test_boundary_slice_empty()
dask.dataframe.tests.test_dataframe.test_boundary_slice_nonmonotonic()
dask.dataframe.tests.test_dataframe.test_boundary_slice_same(index,left,right)
dask.dataframe.tests.test_dataframe.test_broadcast()
dask.dataframe.tests.test_dataframe.test_categorize_info()
dask.dataframe.tests.test_dataframe.test_clip(lower,upper)
dask.dataframe.tests.test_dataframe.test_clip_axis_0()
dask.dataframe.tests.test_dataframe.test_clip_axis_1()
dask.dataframe.tests.test_dataframe.test_coerce()
dask.dataframe.tests.test_dataframe.test_column_assignment()
dask.dataframe.tests.test_dataframe.test_column_names()
dask.dataframe.tests.test_dataframe.test_columns_assignment()
dask.dataframe.tests.test_dataframe.test_columns_named_divisions_and_meta()
dask.dataframe.tests.test_dataframe.test_combine()
dask.dataframe.tests.test_dataframe.test_combine_first()
dask.dataframe.tests.test_dataframe.test_concat()
dask.dataframe.tests.test_dataframe.test_contains_frame()
dask.dataframe.tests.test_dataframe.test_contains_series_raises_deprecated_warning_preserves_behavior()
dask.dataframe.tests.test_dataframe.test_copy()
dask.dataframe.tests.test_dataframe.test_corr()
dask.dataframe.tests.test_dataframe.test_corr_gpu()
dask.dataframe.tests.test_dataframe.test_corr_same_name()
dask.dataframe.tests.test_dataframe.test_cov_corr_meta(chunksize)
dask.dataframe.tests.test_dataframe.test_cov_corr_mixed(numeric_only)
dask.dataframe.tests.test_dataframe.test_cov_corr_stable()
dask.dataframe.tests.test_dataframe.test_cov_dataframe(numeric_only)
dask.dataframe.tests.test_dataframe.test_cov_gpu(numeric_only)
dask.dataframe.tests.test_dataframe.test_cov_series()
dask.dataframe.tests.test_dataframe.test_cumulative()
dask.dataframe.tests.test_dataframe.test_cumulative_empty_partitions(func)
dask.dataframe.tests.test_dataframe.test_cumulative_multiple_columns()
dask.dataframe.tests.test_dataframe.test_custom_map_reduce()
dask.dataframe.tests.test_dataframe.test_dask_dataframe_holds_scipy_sparse_containers()
dask.dataframe.tests.test_dataframe.test_dask_layers()
dask.dataframe.tests.test_dataframe.test_dataframe_compute_forward_kwargs()
dask.dataframe.tests.test_dataframe.test_dataframe_doc()
dask.dataframe.tests.test_dataframe.test_dataframe_doc_from_non_pandas()
dask.dataframe.tests.test_dataframe.test_dataframe_explode()
dask.dataframe.tests.test_dataframe.test_dataframe_groupby_cumprod_agg_empty_partitions()
dask.dataframe.tests.test_dataframe.test_dataframe_groupby_cumsum_agg_empty_partitions()
dask.dataframe.tests.test_dataframe.test_dataframe_items(columns)
dask.dataframe.tests.test_dataframe.test_dataframe_iterrows()
dask.dataframe.tests.test_dataframe.test_dataframe_itertuples()
dask.dataframe.tests.test_dataframe.test_dataframe_itertuples_with_index_false()
dask.dataframe.tests.test_dataframe.test_dataframe_itertuples_with_name_none()
dask.dataframe.tests.test_dataframe.test_dataframe_map(na_action)
dask.dataframe.tests.test_dataframe.test_dataframe_map_raises()
dask.dataframe.tests.test_dataframe.test_dataframe_mode()
dask.dataframe.tests.test_dataframe.test_dataframe_picklable()
dask.dataframe.tests.test_dataframe.test_dataframe_quantile(method,expected,numeric_only)
dask.dataframe.tests.test_dataframe.test_dataframe_reductions_arithmetic(reduction)
dask.dataframe.tests.test_dataframe.test_datetime_loc_open_slicing()
dask.dataframe.tests.test_dataframe.test_del()
dask.dataframe.tests.test_dataframe.test_delayed_roundtrip(optimize)
dask.dataframe.tests.test_dataframe.test_describe(include,exclude,percentiles,subset)
dask.dataframe.tests.test_dataframe.test_describe_empty()
dask.dataframe.tests.test_dataframe.test_describe_empty_tdigest()
dask.dataframe.tests.test_dataframe.test_describe_for_possibly_unsorted_q()
dask.dataframe.tests.test_dataframe.test_describe_numeric(method,test_values)
dask.dataframe.tests.test_dataframe.test_describe_without_datetime_is_numeric()
dask.dataframe.tests.test_dataframe.test_deterministic_apply_concat_apply_names()
dask.dataframe.tests.test_dataframe.test_diff()
dask.dataframe.tests.test_dataframe.test_dot()
dask.dataframe.tests.test_dataframe.test_dot_nan()
dask.dataframe.tests.test_dataframe.test_drop_axis_1()
dask.dataframe.tests.test_dataframe.test_drop_columns(columns)
dask.dataframe.tests.test_dataframe.test_drop_duplicates(shuffle)
dask.dataframe.tests.test_dataframe.test_drop_duplicates_subset()
dask.dataframe.tests.test_dataframe.test_drop_meta_mismatch()
dask.dataframe.tests.test_dataframe.test_dropna()
dask.dataframe.tests.test_dataframe.test_dtype()
dask.dataframe.tests.test_dataframe.test_dtype_cast()
dask.dataframe.tests.test_dataframe.test_duplicate_columns(func,kwargs)
dask.dataframe.tests.test_dataframe.test_embarrassingly_parallel_operations()
dask.dataframe.tests.test_dataframe.test_empty()
dask.dataframe.tests.test_dataframe.test_empty_max()
dask.dataframe.tests.test_dataframe.test_empty_quantile(method)
dask.dataframe.tests.test_dataframe.test_enforce_runtime_divisions()
dask.dataframe.tests.test_dataframe.test_eval()
dask.dataframe.tests.test_dataframe.test_ffill()
dask.dataframe.tests.test_dataframe.test_ffill_bfill()
dask.dataframe.tests.test_dataframe.test_fillna()
dask.dataframe.tests.test_dataframe.test_fillna_dask_dataframe_input()
dask.dataframe.tests.test_dataframe.test_fillna_duplicate_index()
dask.dataframe.tests.test_dataframe.test_fillna_multi_dataframe()
dask.dataframe.tests.test_dataframe.test_fillna_series_types()
dask.dataframe.tests.test_dataframe.test_first_and_last(method)
dask.dataframe.tests.test_dataframe.test_from_delayed_empty_meta_provided()
dask.dataframe.tests.test_dataframe.test_from_delayed_lazy_if_meta_provided()
dask.dataframe.tests.test_dataframe.test_from_dict(dtype,orient,npartitions)
dask.dataframe.tests.test_dataframe.test_from_dict_raises()
dask.dataframe.tests.test_dataframe.test_fuse_roots()
dask.dataframe.tests.test_dataframe.test_get_partition()
dask.dataframe.tests.test_dataframe.test_getitem_column_types(col_type)
dask.dataframe.tests.test_dataframe.test_getitem_meta()
dask.dataframe.tests.test_dataframe.test_getitem_multilevel()
dask.dataframe.tests.test_dataframe.test_getitem_string_subclass()
dask.dataframe.tests.test_dataframe.test_getitem_with_bool_dataframe_as_key()
dask.dataframe.tests.test_dataframe.test_getitem_with_non_series()
dask.dataframe.tests.test_dataframe.test_gh580()
dask.dataframe.tests.test_dataframe.test_gh6305()
dask.dataframe.tests.test_dataframe.test_gh_1301()
dask.dataframe.tests.test_dataframe.test_gh_517()
dask.dataframe.tests.test_dataframe.test_groupby_callable()
dask.dataframe.tests.test_dataframe.test_groupby_multilevel_info()
dask.dataframe.tests.test_dataframe.test_has_parallel_type()
dask.dataframe.tests.test_dataframe.test_hash_split_unique(npartitions,split_every,split_out)
dask.dataframe.tests.test_dataframe.test_head_npartitions()
dask.dataframe.tests.test_dataframe.test_head_npartitions_warn()
dask.dataframe.tests.test_dataframe.test_head_tail()
dask.dataframe.tests.test_dataframe.test_idxmaxmin(idx,skipna)
dask.dataframe.tests.test_dataframe.test_idxmaxmin_empty_partitions()
dask.dataframe.tests.test_dataframe.test_idxmaxmin_numeric_only(func)
dask.dataframe.tests.test_dataframe.test_index()
dask.dataframe.tests.test_dataframe.test_index_divisions()
dask.dataframe.tests.test_dataframe.test_index_errors()
dask.dataframe.tests.test_dataframe.test_index_head()
dask.dataframe.tests.test_dataframe.test_index_is_monotonic_deprecated()
dask.dataframe.tests.test_dataframe.test_index_is_monotonic_dt64()
dask.dataframe.tests.test_dataframe.test_index_names()
dask.dataframe.tests.test_dataframe.test_index_nulls(null_value)
dask.dataframe.tests.test_dataframe.test_index_time_properties()
dask.dataframe.tests.test_dataframe.test_info()
dask.dataframe.tests.test_dataframe.test_inplace_operators()
dask.dataframe.tests.test_dataframe.test_ipython_completion()
dask.dataframe.tests.test_dataframe.test_is_monotonic_deprecated()
dask.dataframe.tests.test_dataframe.test_is_monotonic_dt64()
dask.dataframe.tests.test_dataframe.test_is_monotonic_empty_partitions()
dask.dataframe.tests.test_dataframe.test_is_monotonic_numeric(series,reverse,cls)
dask.dataframe.tests.test_dataframe.test_isin()
dask.dataframe.tests.test_dataframe.test_isna(values)
dask.dataframe.tests.test_dataframe.test_iter()
dask.dataframe.tests.test_dataframe.test_join_series()
dask.dataframe.tests.test_dataframe.test_known_divisions()
dask.dataframe.tests.test_dataframe.test_len()
dask.dataframe.tests.test_dataframe.test_map()
dask.dataframe.tests.test_dataframe.test_map_freq_to_period_start(freq,expected_freq)
dask.dataframe.tests.test_dataframe.test_map_index()
dask.dataframe.tests.test_dataframe.test_map_partition_array(func)
dask.dataframe.tests.test_dataframe.test_map_partition_sparse()
dask.dataframe.tests.test_dataframe.test_map_partitions()
dask.dataframe.tests.test_dataframe.test_map_partitions_column_info()
dask.dataframe.tests.test_dataframe.test_map_partitions_delays_large_inputs()
dask.dataframe.tests.test_dataframe.test_map_partitions_delays_lists()
dask.dataframe.tests.test_dataframe.test_map_partitions_keeps_kwargs_readable()
dask.dataframe.tests.test_dataframe.test_map_partitions_method_names()
dask.dataframe.tests.test_dataframe.test_map_partitions_multi_argument()
dask.dataframe.tests.test_dataframe.test_map_partitions_names()
dask.dataframe.tests.test_dataframe.test_map_partitions_partition_info()
dask.dataframe.tests.test_dataframe.test_map_partitions_propagates_index_metadata()
dask.dataframe.tests.test_dataframe.test_map_partitions_type()
dask.dataframe.tests.test_dataframe.test_map_partitions_with_delayed_collection()
dask.dataframe.tests.test_dataframe.test_mask_where_array_like(df,cond)
dask.dataframe.tests.test_dataframe.test_mask_where_callable()
dask.dataframe.tests.test_dataframe.test_median()
dask.dataframe.tests.test_dataframe.test_median_approximate(method)
dask.dataframe.tests.test_dataframe.test_memory_usage_dataframe(index,deep)
dask.dataframe.tests.test_dataframe.test_memory_usage_index(deep)
dask.dataframe.tests.test_dataframe.test_memory_usage_per_partition(index,deep)
dask.dataframe.tests.test_dataframe.test_memory_usage_series(index,deep)
dask.dataframe.tests.test_dataframe.test_meta_error_message()
dask.dataframe.tests.test_dataframe.test_meta_nonempty_uses_meta_value_if_provided()
dask.dataframe.tests.test_dataframe.test_meta_raises()
dask.dataframe.tests.test_dataframe.test_metadata_inference_single_partition_aligned_args()
dask.dataframe.tests.test_dataframe.test_methods_tokenize_differently()
dask.dataframe.tests.test_dataframe.test_mixed_dask_array_multi_dimensional()
dask.dataframe.tests.test_dataframe.test_mixed_dask_array_operations()
dask.dataframe.tests.test_dataframe.test_mixed_dask_array_operations_errors()
dask.dataframe.tests.test_dataframe.test_mod_eq()
dask.dataframe.tests.test_dataframe.test_mode_numeric_only()
dask.dataframe.tests.test_dataframe.test_nbytes()
dask.dataframe.tests.test_dataframe.test_ndim()
dask.dataframe.tests.test_dataframe.test_nlargest_nsmallest()
dask.dataframe.tests.test_dataframe.test_nlargest_nsmallest_raises()
dask.dataframe.tests.test_dataframe.test_nunique(dropna,axis)
dask.dataframe.tests.test_dataframe.test_partitions_indexer()
dask.dataframe.tests.test_dataframe.test_pipe()
dask.dataframe.tests.test_dataframe.test_pop()
dask.dataframe.tests.test_dataframe.test_pyarrow_conversion_dispatch(self_destruct)
dask.dataframe.tests.test_dataframe.test_pyarrow_conversion_dispatch_cudf()
dask.dataframe.tests.test_dataframe.test_pyarrow_decimal_extension_dtype()
dask.dataframe.tests.test_dataframe.test_pyarrow_extension_dtype(dtype)
dask.dataframe.tests.test_dataframe.test_pyarrow_schema_dispatch()
dask.dataframe.tests.test_dataframe.test_pyarrow_schema_dispatch_preserves_index(preserve_index)
dask.dataframe.tests.test_dataframe.test_quantile(method,quantile)
dask.dataframe.tests.test_dataframe.test_quantile_datetime_numeric_only_false()
dask.dataframe.tests.test_dataframe.test_quantile_for_possibly_unsorted_q()
dask.dataframe.tests.test_dataframe.test_quantile_missing(method)
dask.dataframe.tests.test_dataframe.test_quantile_tiny_partitions()
dask.dataframe.tests.test_dataframe.test_quantile_trivial_partitions()
dask.dataframe.tests.test_dataframe.test_query()
dask.dataframe.tests.test_dataframe.test_random_partitions()
dask.dataframe.tests.test_dataframe.test_reduction_method()
dask.dataframe.tests.test_dataframe.test_reduction_method_split_every()
dask.dataframe.tests.test_dataframe.test_rename_columns()
dask.dataframe.tests.test_dataframe.test_rename_dict()
dask.dataframe.tests.test_dataframe.test_rename_function()
dask.dataframe.tests.test_dataframe.test_rename_index()
dask.dataframe.tests.test_dataframe.test_rename_series()
dask.dataframe.tests.test_dataframe.test_rename_series_method()
dask.dataframe.tests.test_dataframe.test_rename_series_method_2()
dask.dataframe.tests.test_dataframe.test_repartition()
dask.dataframe.tests.test_dataframe.test_repartition_datetime_tz_index()
dask.dataframe.tests.test_dataframe.test_repartition_divisions()
dask.dataframe.tests.test_dataframe.test_repartition_freq(npartitions,freq,start,end)
dask.dataframe.tests.test_dataframe.test_repartition_freq_day()
dask.dataframe.tests.test_dataframe.test_repartition_freq_divisions()
dask.dataframe.tests.test_dataframe.test_repartition_freq_errors()
dask.dataframe.tests.test_dataframe.test_repartition_freq_month()
dask.dataframe.tests.test_dataframe.test_repartition_input_errors()
dask.dataframe.tests.test_dataframe.test_repartition_noop(type_ctor)
dask.dataframe.tests.test_dataframe.test_repartition_npartitions(use_index,n,k,dtype,transform)
dask.dataframe.tests.test_dataframe.test_repartition_npartitions_numeric_edge_case()
dask.dataframe.tests.test_dataframe.test_repartition_npartitions_same_limits()
dask.dataframe.tests.test_dataframe.test_repartition_object_index()
dask.dataframe.tests.test_dataframe.test_repartition_on_pandas_dataframe()
dask.dataframe.tests.test_dataframe.test_repartition_partition_size(use_index,n,partition_size,transform)
dask.dataframe.tests.test_dataframe.test_repartition_partition_size_arg()
dask.dataframe.tests.test_dataframe.test_replace()
dask.dataframe.tests.test_dataframe.test_repr_html_dataframe_highlevelgraph()
dask.dataframe.tests.test_dataframe.test_repr_materialize()
dask.dataframe.tests.test_dataframe.test_reset_index()
dask.dataframe.tests.test_dataframe.test_round()
dask.dataframe.tests.test_dataframe.test_sample()
dask.dataframe.tests.test_dataframe.test_sample_empty_partitions()
dask.dataframe.tests.test_dataframe.test_sample_raises()
dask.dataframe.tests.test_dataframe.test_sample_without_replacement()
dask.dataframe.tests.test_dataframe.test_scalar_raises()
dask.dataframe.tests.test_dataframe.test_scalar_with_array()
dask.dataframe.tests.test_dataframe.test_select_dtypes(include,exclude)
dask.dataframe.tests.test_dataframe.test_series_axes()
dask.dataframe.tests.test_dataframe.test_series_explode()
dask.dataframe.tests.test_dataframe.test_series_iter()
dask.dataframe.tests.test_dataframe.test_series_iteritems()
dask.dataframe.tests.test_dataframe.test_series_map(base_npart,map_npart,sorted_index,sorted_map_index)
dask.dataframe.tests.test_dataframe.test_series_round()
dask.dataframe.tests.test_dataframe.test_set_index_with_index()
dask.dataframe.tests.test_dataframe.test_setitem()
dask.dataframe.tests.test_dataframe.test_setitem_triggering_realign()
dask.dataframe.tests.test_dataframe.test_setitem_with_bool_dataframe_as_key()
dask.dataframe.tests.test_dataframe.test_setitem_with_bool_series_as_key()
dask.dataframe.tests.test_dataframe.test_setitem_with_numeric_column_name_raises_not_implemented()
dask.dataframe.tests.test_dataframe.test_shape()
dask.dataframe.tests.test_dataframe.test_shift()
dask.dataframe.tests.test_dataframe.test_shift_with_freq_DatetimeIndex(data_freq,divs1)
dask.dataframe.tests.test_dataframe.test_shift_with_freq_PeriodIndex(data_freq,divs)
dask.dataframe.tests.test_dataframe.test_shift_with_freq_TimedeltaIndex()
dask.dataframe.tests.test_dataframe.test_shift_with_freq_errors()
dask.dataframe.tests.test_dataframe.test_simple_map_partitions()
dask.dataframe.tests.test_dataframe.test_size()
dask.dataframe.tests.test_dataframe.test_slice_on_filtered_boundary(drop)
dask.dataframe.tests.test_dataframe.test_split_out_drop_duplicates(split_every)
dask.dataframe.tests.test_dataframe.test_split_out_value_counts(split_every)
dask.dataframe.tests.test_dataframe.test_squeeze()
dask.dataframe.tests.test_dataframe.test_timeseries_sorted()
dask.dataframe.tests.test_dataframe.test_to_backend()
dask.dataframe.tests.test_dataframe.test_to_dask_array(meta,as_frame,lengths)
dask.dataframe.tests.test_dataframe.test_to_dask_array_raises(as_frame)
dask.dataframe.tests.test_dataframe.test_to_dask_array_unknown(as_frame)
dask.dataframe.tests.test_dataframe.test_to_datetime(gpu)
dask.dataframe.tests.test_dataframe.test_to_frame()
dask.dataframe.tests.test_dataframe.test_to_timedelta()
dask.dataframe.tests.test_dataframe.test_to_timestamp()
dask.dataframe.tests.test_dataframe.test_transform_getitem_works(func)
dask.dataframe.tests.test_dataframe.test_unique()
dask.dataframe.tests.test_dataframe.test_unknown_divisions()
dask.dataframe.tests.test_dataframe.test_use_of_weakref_proxy()
dask.dataframe.tests.test_dataframe.test_value_counts()
dask.dataframe.tests.test_dataframe.test_value_counts_not_sorted()
dask.dataframe.tests.test_dataframe.test_value_counts_with_dropna()
dask.dataframe.tests.test_dataframe.test_value_counts_with_normalize()
dask.dataframe.tests.test_dataframe.test_value_counts_with_normalize_and_dropna(normalize)
dask.dataframe.tests.test_dataframe.test_values()
dask.dataframe.tests.test_dataframe.test_values_extension_dtypes()
dask.dataframe.tests.test_dataframe.test_view()
dask.dataframe.tests.test_dataframe.test_where_mask()
dask.dataframe.tests.test_dataframe.test_with_boundary(start,stop,right_boundary,left_boundary,drop)
dask.dataframe.tests.test_dataframe.test_with_min_count()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/tests/test_groupby.py----------------------------------------
A:dask.dataframe.tests.test_groupby.DASK_EXPR_ENABLED->dask.dataframe._dask_expr_enabled()
A:dask.dataframe.tests.test_groupby.pdf->pandas.DataFrame({'ints': [4, 4, 5, 5, 5], 'ints2': [1, 2, 3, 4, 1], 'dates': pd.date_range('2015-01-01', periods=5, freq='1min'), 'strings': ['q', 'c', 'k', 'a', 'l']})
A:dask.dataframe.tests.test_groupby.ddf->dask.dataframe.from_pandas(df, npartitions=3)
A:dask.dataframe.tests.test_groupby.gp->pandas.DataFrame({'ints': [4, 4, 5, 5, 5], 'ints2': [1, 2, 3, 4, 1], 'dates': pd.date_range('2015-01-01', periods=5, freq='1min'), 'strings': ['q', 'c', 'k', 'a', 'l']}).groupby('y')
A:dask.dataframe.tests.test_groupby.dp->dask.dataframe.from_pandas(df, npartitions=3).groupby('y')
A:dask.dataframe.tests.test_groupby.df->pandas.DataFrame({'a': [11, 12, 31, 1, 2, 3, 4, 5, 6, 10], 'b': pd.Categorical(values=[1] * 9 + [np.nan], categories=[1, 2])})
A:dask.dataframe.tests.test_groupby.expected->getattr(pdf.groupby('ints'), func)(**kwargs)
A:dask.dataframe.tests.test_groupby.c->pandas.DataFrame({'a': [11, 12, 31, 1, 2, 3, 4, 5, 6, 10], 'b': pd.Categorical(values=[1] * 9 + [np.nan], categories=[1, 2])}).a.sum()
A:dask.dataframe.tests.test_groupby.d->pandas.DataFrame({'g0': [0, 0, 0, 1, 1] * 3, 'g1': [0, 0, 0, 1, 1] * 3, 'cc': [4, 5, 4, 6, 6] * 3})
A:dask.dataframe.tests.test_groupby.c_scalar->dask.dataframe.from_pandas(df, npartitions=3).a.sum()
A:dask.dataframe.tests.test_groupby.d_scalar->dask.dataframe.from_pandas(df, npartitions=3).b.mean()
A:dask.dataframe.tests.test_groupby.c_delayed->dask.delayed(lambda : c)()
A:dask.dataframe.tests.test_groupby.d_delayed->dask.delayed(lambda : d)()
A:dask.dataframe.tests.test_groupby.meta->group_and_slice(ddf, grouper)._meta.first()
A:dask.dataframe.tests.test_groupby.g->dask.dataframe.from_pandas(df, npartitions=3).groupby('a')
A:dask.dataframe.tests.test_groupby.ddf2->dask.dataframe.from_pandas(df, npartitions=3).set_index('a')
A:dask.dataframe.tests.test_groupby.pdf2->pandas.DataFrame({'ints': [4, 4, 5, 5, 5], 'ints2': [1, 2, 3, 4, 1], 'dates': pd.date_range('2015-01-01', periods=5, freq='1min'), 'strings': ['q', 'c', 'k', 'a', 'l']}).set_index('a')
A:dask.dataframe.tests.test_groupby.agg->dask.dataframe.from_pandas(df, npartitions=3).set_index('a').groupby('a').agg({'b': 'mean'})
A:dask.dataframe.tests.test_groupby.dask_group->dask.dataframe.from_pandas(s, npartitions=2).groupby(ss)
A:dask.dataframe.tests.test_groupby.pandas_group->grouper(df)
A:dask.dataframe.tests.test_groupby.dask_agg->getattr(dask_group, agg_func)
A:dask.dataframe.tests.test_groupby.pandas_agg->getattr(pandas_group, agg_func)
A:dask.dataframe.tests.test_groupby.a->dask.dataframe.from_pandas(d, npartitions=5)
A:dask.dataframe.tests.test_groupby.b->dask.dataframe.from_pandas(df, npartitions=3).groupby(ind(ddf)).B.apply(len)
A:dask.dataframe.tests.test_groupby.sol->call(pdf.a.groupby(pdf.b), 'var', ddof=2)
A:dask.dataframe.tests.test_groupby.res->res.add(i, fill_value=0).add(i, fill_value=0)
A:dask.dataframe.tests.test_groupby.ctx->contextlib.nullcontext()
A:dask.dataframe.tests.test_groupby.ddgrouped->dask.dataframe.from_pandas(df, npartitions=3).groupby(by(ddf))
A:dask.dataframe.tests.test_groupby.pdgrouped->pandas.DataFrame({'ints': [4, 4, 5, 5, 5], 'ints2': [1, 2, 3, 4, 1], 'dates': pd.date_range('2015-01-01', periods=5, freq='1min'), 'strings': ['q', 'c', 'k', 'a', 'l']}).groupby(by(pdf))
A:dask.dataframe.tests.test_groupby.strings->list('aaabbccccdddeee')
A:dask.dataframe.tests.test_groupby.data->numpy.random.randn(rows, cols)
A:dask.dataframe.tests.test_groupby.ps->pandas.DataFrame(dict(strings=strings, data=data))
A:dask.dataframe.tests.test_groupby.s->pandas.Series([1, 2, 2, 1, 1])
A:dask.dataframe.tests.test_groupby.result->getattr(ddf.groupby('ints'), func)(**kwargs)
A:dask.dataframe.tests.test_groupby.pd_group->pandas.Series([1, 2, 2, 1, 1]).groupby(s)
A:dask.dataframe.tests.test_groupby.ss->dask.dataframe.from_pandas(s, npartitions=2)
A:dask.dataframe.tests.test_groupby.pd_group2->pandas.Series([1, 2, 2, 1, 1]).groupby(s + 1)
A:dask.dataframe.tests.test_groupby.dask_group2->dask.dataframe.from_pandas(s, npartitions=2).groupby(ss + 1)
A:dask.dataframe.tests.test_groupby.sss->dask.dataframe.from_pandas(s, npartitions=5)
A:dask.dataframe.tests.test_groupby.full->pandas.DataFrame({'a': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'b': [4, 5, 6, 3, 2, 1, 0, 0, 0]}, index=[0, 1, 3, 5, 6, 8, 9, 9, 9])
A:dask.dataframe.tests.test_groupby.(no_mean_chunks, no_mean_aggs, no_mean_finalizers)->_build_agg_args(no_mean_spec)
A:dask.dataframe.tests.test_groupby.(with_mean_chunks, with_mean_aggs, with_mean_finalizers)->_build_agg_args(with_mean_spec)
A:dask.dataframe.tests.test_groupby.dask_holder->collections.namedtuple('dask_holder', ['dask'])
A:dask.dataframe.tests.test_groupby.result1->dask.dataframe.from_pandas(df, npartitions=3).groupby(['a', 'b']).agg(spec, split_every=2)
A:dask.dataframe.tests.test_groupby.result2->dask.dataframe.from_pandas(df, npartitions=3).groupby(['a', 'b']).agg(spec, split_every=2)
A:dask.dataframe.tests.test_groupby.agg_dask1->get_agg_dask(result1)
A:dask.dataframe.tests.test_groupby.agg_dask2->get_agg_dask(result2)
A:dask.dataframe.tests.test_groupby.other->dask.dataframe.from_pandas(df, npartitions=3).groupby(['a', 'b']).agg(other_spec, split_every=2)
A:dask.dataframe.tests.test_groupby.expect->pandas.DataFrame({'ints': [4, 4, 5, 5, 5], 'ints2': [1, 2, 3, 4, 1], 'dates': pd.date_range('2015-01-01', periods=5, freq='1min'), 'strings': ['q', 'c', 'k', 'a', 'l']}).groupby(by)[slice_key].count()
A:dask.dataframe.tests.test_groupby.expect['b', 'mean']->expect['b', 'mean'].astype(result['b', 'mean'].dtype).astype(result['b', 'mean'].dtype)
A:dask.dataframe.tests.test_groupby.actual->caller(ddf.groupby(['A', ddf['A'].eq('a1')])['B'])
A:dask.dataframe.tests.test_groupby.deprecated_ctx->pytest.warns(FutureWarning, match='Please use `ffill`/`bfill` or `fillna` without a GroupBy')
A:dask.dataframe.tests.test_groupby.cols->sorted(list(dddf.columns))
A:dask.dataframe.tests.test_groupby.dddf->dddf.sort_index().sort_index()
A:dask.dataframe.tests.test_groupby.meta_nonempty->group_and_slice(ddf, grouper)._meta_nonempty.first().head(0)
A:dask.dataframe.tests.test_groupby.ddf3->dask.dataframe.from_pandas(pdf, npartitions=3)
A:dask.dataframe.tests.test_groupby.ddf7->dask.dataframe.from_pandas(pdf, npartitions=7)
A:dask.dataframe.tests.test_groupby.dsk->getattr(ddf.groupby('ints'), func)(**kwargs).__dask_optimize__(result.dask, result.__dask_keys__())
A:dask.dataframe.tests.test_groupby.(dependencies, dependents)->get_deps(dsk)
A:dask.dataframe.tests.test_groupby.deprecate_ctx->pytest.warns(warn, match='`axis` parameter is deprecated')
A:dask.dataframe.tests.test_groupby.ddf_group->dask.dataframe.from_pandas(df, npartitions=3).groupby('foo')
A:dask.dataframe.tests.test_groupby.ds_group->dfiltered.b.groupby(ddf.a)
A:dask.dataframe.tests.test_groupby.df_group->filtered.groupby(df.a)
A:dask.dataframe.tests.test_groupby.ddf0->getattr(ddf.groupby(['a']), op)()
A:dask.dataframe.tests.test_groupby.ddf1->getattr(ddf1.groupby(['a']), op)()
A:dask.dataframe.tests.test_groupby.(res0_a, res1_a)->dask.compute(getattr(dcum['ones'], op)(), getattr(dcum['twos'], op)())
A:dask.dataframe.tests.test_groupby.dcum->dask.dataframe.from_pandas(df, npartitions=3).groupby(['a'])
A:dask.dataframe.tests.test_groupby.cum->pandas.DataFrame({'a': [11, 12, 31, 1, 2, 3, 4, 5, 6, 10], 'b': pd.Categorical(values=[1] * 9 + [np.nan], categories=[1, 2])}).groupby(['a'])
A:dask.dataframe.tests.test_groupby.ddf_no_divs->dask.dataframe.from_pandas(df, npartitions=df.index.nunique(), sort=False).clear_divisions()
A:dask.dataframe.tests.test_groupby.custom_mean->dask.dataframe.Aggregation('mean', lambda s: (s.count(), s.sum()), lambda s0, s1: (s0.sum(), s1.sum()), lambda s0, s1: s1 / s0)
A:dask.dataframe.tests.test_groupby.custom_sum->dask.dataframe.Aggregation('sum', lambda s: s.sum(), lambda s0: s0.sum())
A:dask.dataframe.tests.test_groupby.agg_func->dask.dataframe.Aggregation('custom_mode', lambda s: s.apply(lambda s: [s.value_counts()]), agg_mode, lambda s: s.map(lambda i: i[0].idxmax()))
A:dask.dataframe.tests.test_groupby.by->dask.array.from_array([0, 1]).to_dask_dataframe()
A:dask.dataframe.tests.test_groupby.result.columns->getattr(ddf.groupby('ints'), func)(**kwargs).columns.astype(np.dtype('O'))
A:dask.dataframe.tests.test_groupby.result_pd->agg(df.groupby('x', sort=sort))
A:dask.dataframe.tests.test_groupby.result_dd->dask.dataframe.from_pandas(df, npartitions=3).groupby('group')['value'].idxmax(skipna=skipna)
A:dask.dataframe.tests.test_groupby.rng->numpy.random.RandomState(42)
A:dask.dataframe.tests.test_groupby.pd_gb->pandas.DataFrame({'a': [11, 12, 31, 1, 2, 3, 4, 5, 6, 10], 'b': pd.Categorical(values=[1] * 9 + [np.nan], categories=[1, 2])}).groupby(by).baz.value_counts()
A:dask.dataframe.tests.test_groupby.dd_gb->dask.dataframe.from_pandas(df, npartitions=3).groupby(by).baz.value_counts()
A:dask.dataframe.tests.test_groupby.counts->pandas.DataFrame({'a': [11, 12, 31, 1, 2, 3, 4, 5, 6, 10], 'b': pd.Categorical(values=[1] * 9 + [np.nan], categories=[1, 2])}).groupby('x')['y'].value_counts()
A:dask.dataframe.tests.test_groupby.dcounts->dask.dataframe.from_pandas(df, npartitions=3).groupby('x')['y'].value_counts()
A:dask.dataframe.tests.test_groupby.delayed_periods->dask.delayed(lambda : 1)()
A:dask.dataframe.tests.test_groupby.df_result->pandas.DataFrame({'ints': [4, 4, 5, 5, 5], 'ints2': [1, 2, 3, 4, 1], 'dates': pd.date_range('2015-01-01', periods=5, freq='1min'), 'strings': ['q', 'c', 'k', 'a', 'l']}).groupby('b').shift(periods=-2, freq='D')
A:dask.dataframe.tests.test_groupby.dask_result->dask_result.compute().compute()
A:dask.dataframe.tests.test_groupby.pd_result->getattr(df.groupby('A'), func)()
A:dask.dataframe.tests.test_groupby.cudf->pytest.importorskip('cudf')
A:dask.dataframe.tests.test_groupby.dask_cudf->pytest.importorskip('dask_cudf')
A:dask.dataframe.tests.test_groupby.df['d']->df['c'].astype('category')
A:dask.dataframe.tests.test_groupby.cudf_result->pandas.DataFrame({'a': [11, 12, 31, 1, 2, 3, 4, 5, 6, 10], 'b': pd.Categorical(values=[1] * 9 + [np.nan], categories=[1, 2])}).groupby(by, dropna=dropna, group_keys=group_keys).e.sum()
A:dask.dataframe.tests.test_groupby.gdf->pytest.importorskip('cudf').from_pandas(pdf)
A:dask.dataframe.tests.test_groupby.pd_grouper->grouper_dispatch(pdf)(key=key)
A:dask.dataframe.tests.test_groupby.gd_grouper->grouper_dispatch(gdf)(key=key)
A:dask.dataframe.tests.test_groupby.got->dask.dataframe.from_pandas(df, npartitions=3).groupby(by)[slice_key].count()
A:dask.dataframe.tests.test_groupby.dcdf->dask.dataframe.from_pandas(df, npartitions=3).to_backend('cudf')
A:dask.dataframe.tests.test_groupby.res_pd->pandas.DataFrame({'a': [11, 12, 31, 1, 2, 3, 4, 5, 6, 10], 'b': pd.Categorical(values=[1] * 9 + [np.nan], categories=[1, 2])}).groupby('a', group_keys=group_keys).apply(func)
A:dask.dataframe.tests.test_groupby.res_dd->dask.dataframe.from_pandas(df, npartitions=3).groupby('a', group_keys=group_keys).apply(func, meta=res_pd)
A:dask.dataframe.tests.test_groupby.res_dc->dask.dataframe.from_pandas(df, npartitions=3).to_backend('cudf').groupby('a', group_keys=group_keys).apply(func, meta=cudf.from_pandas(res_pd))
A:dask.dataframe.tests.test_groupby.df['e']->df['c'].astype('category').astype('category')
A:dask.dataframe.tests.test_groupby.result_so1->dask.dataframe.from_pandas(df, npartitions=3).groupby(column, sort=False).a.mean(split_out=1).compute().dropna()
A:dask.dataframe.tests.test_groupby.data_source->pytest.importorskip(backend)
A:dask.dataframe.tests.test_groupby.series->pytest.importorskip(backend).Series(np.concatenate([sqrt * np.arange(5), np.arange(35)])).astype('int64')
A:dask.dataframe.tests.test_groupby.gb->dask.dataframe.from_pandas(df, npartitions=3).groupby(by, sort=sort)
A:dask.dataframe.tests.test_groupby.gb_pd->pandas.DataFrame({'a': [11, 12, 31, 1, 2, 3, 4, 5, 6, 10], 'b': pd.Categorical(values=[1] * 9 + [np.nan], categories=[1, 2])}).groupby(by, sort=sort)
A:dask.dataframe.tests.test_groupby.result_1->getattr(gb, agg)
A:dask.dataframe.tests.test_groupby.result_1_pd->getattr(gb_pd, agg)
A:dask.dataframe.tests.test_groupby.result_2->getattr(gb.e, agg)
A:dask.dataframe.tests.test_groupby.result_2_pd->getattr(gb_pd.e, agg)
A:dask.dataframe.tests.test_groupby.result_3->dask.dataframe.from_pandas(df, npartitions=3).groupby(by, sort=sort).agg({'e': agg})
A:dask.dataframe.tests.test_groupby.result_3_pd->pandas.DataFrame({'a': [11, 12, 31, 1, 2, 3, 4, 5, 6, 10], 'b': pd.Categorical(values=[1] * 9 + [np.nan], categories=[1, 2])}).groupby(by, sort=sort).agg({'e': agg})
A:dask.dataframe.tests.test_groupby.ddf['cat_1']->ddf['cat_1'].cat.as_unknown().cat.as_unknown()
A:dask.dataframe.tests.test_groupby.ddf['cat_2']->ddf['cat_2'].cat.as_unknown().cat.as_unknown()
A:dask.dataframe.tests.test_groupby.caller->operator.methodcaller(operation)
A:dask.dataframe.tests.test_groupby.ctx_warn->pytest.warns(FutureWarning, match='The default value of numeric_only')
A:dask.dataframe.tests.test_groupby.ctx_error->pytest.raises(NotImplementedError, match="'numeric_only=False' is not implemented in Dask")
A:dask.dataframe.tests.test_groupby.ddf_result->getattr(ddf.groupby('A'), func)(numeric_only=True)
A:dask.dataframe.tests.test_groupby.pdf_result->getattr(df.groupby('b', observed=observed, dropna=dropna), func)()
A:dask.dataframe.tests.test_groupby.dd_result->getattr(ddf.groupby('b', observed=observed, dropna=dropna), func)()
dask.dataframe.tests.test_groupby.agg_func(request)
dask.dataframe.tests.test_groupby.auto_shuffle_method(shuffle_method)
dask.dataframe.tests.test_groupby.groupby_axis_and_meta(axis=0)
dask.dataframe.tests.test_groupby.groupby_axis_deprecated(*contexts,dask_op=True)
dask.dataframe.tests.test_groupby.record_numeric_only_warnings()
dask.dataframe.tests.test_groupby.test_aggregate__single_element_groups(agg_func)
dask.dataframe.tests.test_groupby.test_aggregate_build_agg_args__reuse_of_intermediates()
dask.dataframe.tests.test_groupby.test_aggregate_dask()
dask.dataframe.tests.test_groupby.test_aggregate_median(spec,keys,shuffle_method)
dask.dataframe.tests.test_groupby.test_apply_or_transform_shuffle(grouped,func)
dask.dataframe.tests.test_groupby.test_apply_or_transform_shuffle_multilevel(grouper,func)
dask.dataframe.tests.test_groupby.test_bfill(group_keys,limit)
dask.dataframe.tests.test_groupby.test_cumulative(func,key,sel)
dask.dataframe.tests.test_groupby.test_cumulative_axis(func)
dask.dataframe.tests.test_groupby.test_dataframe_aggregations_multilevel(grouper,agg_func,split_out)
dask.dataframe.tests.test_groupby.test_dataframe_groupby_agg_custom_sum(pandas_spec,dask_spec,check_dtype)
dask.dataframe.tests.test_groupby.test_dataframe_groupby_nunique()
dask.dataframe.tests.test_groupby.test_dataframe_groupby_nunique_across_group_same_value()
dask.dataframe.tests.test_groupby.test_dataframe_named_agg(shuffle)
dask.dataframe.tests.test_groupby.test_df_groupby_idx_axis(func,axis)
dask.dataframe.tests.test_groupby.test_df_groupby_idxmax()
dask.dataframe.tests.test_groupby.test_df_groupby_idxmax_skipna(skipna)
dask.dataframe.tests.test_groupby.test_df_groupby_idxmin()
dask.dataframe.tests.test_groupby.test_df_groupby_idxmin_skipna(skipna)
dask.dataframe.tests.test_groupby.test_empty_partitions_with_value_counts(by)
dask.dataframe.tests.test_groupby.test_ffill(group_keys,limit)
dask.dataframe.tests.test_groupby.test_fillna(axis,group_keys,limit)
dask.dataframe.tests.test_groupby.test_full_groupby()
dask.dataframe.tests.test_groupby.test_full_groupby_apply_multiarg()
dask.dataframe.tests.test_groupby.test_full_groupby_multilevel(grouper,reverse)
dask.dataframe.tests.test_groupby.test_groupby_None_split_out_warns()
dask.dataframe.tests.test_groupby.test_groupby_agg_custom__mode()
dask.dataframe.tests.test_groupby.test_groupby_agg_custom__name_clash_with_internal_different_column()
dask.dataframe.tests.test_groupby.test_groupby_agg_custom__name_clash_with_internal_same_column()
dask.dataframe.tests.test_groupby.test_groupby_agg_grouper_multiple(slice_)
dask.dataframe.tests.test_groupby.test_groupby_agg_grouper_single()
dask.dataframe.tests.test_groupby.test_groupby_aggregate_categorical_observed(known_cats,ordered_cats,agg_func,groupby,observed)
dask.dataframe.tests.test_groupby.test_groupby_aggregate_categoricals(grouping,agg)
dask.dataframe.tests.test_groupby.test_groupby_aggregate_partial_function(agg)
dask.dataframe.tests.test_groupby.test_groupby_aggregate_partial_function_unexpected_args(agg)
dask.dataframe.tests.test_groupby.test_groupby_aggregate_partial_function_unexpected_kwargs(agg)
dask.dataframe.tests.test_groupby.test_groupby_apply_cudf(group_keys)
dask.dataframe.tests.test_groupby.test_groupby_apply_tasks(shuffle_method)
dask.dataframe.tests.test_groupby.test_groupby_column_and_index_agg_funcs(agg_func)
dask.dataframe.tests.test_groupby.test_groupby_column_and_index_apply(group_args,apply_func)
dask.dataframe.tests.test_groupby.test_groupby_cov(columns)
dask.dataframe.tests.test_groupby.test_groupby_cov_non_numeric_grouping_column()
dask.dataframe.tests.test_groupby.test_groupby_dataframe_cum_caching(op)
dask.dataframe.tests.test_groupby.test_groupby_dir()
dask.dataframe.tests.test_groupby.test_groupby_dropna_cudf(dropna,by,group_keys)
dask.dataframe.tests.test_groupby.test_groupby_dropna_pandas(dropna)
dask.dataframe.tests.test_groupby.test_groupby_dropna_with_agg(sort)
dask.dataframe.tests.test_groupby.test_groupby_empty_partitions_with_rows_operation(operation)
dask.dataframe.tests.test_groupby.test_groupby_error()
dask.dataframe.tests.test_groupby.test_groupby_get_group(categoricals,by)
dask.dataframe.tests.test_groupby.test_groupby_group_keys(group_keys)
dask.dataframe.tests.test_groupby.test_groupby_grouper_dispatch(key)
dask.dataframe.tests.test_groupby.test_groupby_index_array()
dask.dataframe.tests.test_groupby.test_groupby_internal_repr()
dask.dataframe.tests.test_groupby.test_groupby_internal_repr_xfail()
dask.dataframe.tests.test_groupby.test_groupby_iter_fails()
dask.dataframe.tests.test_groupby.test_groupby_large_ints_exception(backend)
dask.dataframe.tests.test_groupby.test_groupby_meta_content(group_and_slice,grouper)
dask.dataframe.tests.test_groupby.test_groupby_multi_index_with_row_operations(operation)
dask.dataframe.tests.test_groupby.test_groupby_multilevel_agg()
dask.dataframe.tests.test_groupby.test_groupby_multilevel_getitem(grouper,agg_func)
dask.dataframe.tests.test_groupby.test_groupby_multiprocessing()
dask.dataframe.tests.test_groupby.test_groupby_normalize_by()
dask.dataframe.tests.test_groupby.test_groupby_not_supported()
dask.dataframe.tests.test_groupby.test_groupby_numeric_column()
dask.dataframe.tests.test_groupby.test_groupby_numeric_only_None_column_name()
dask.dataframe.tests.test_groupby.test_groupby_numeric_only_false(func)
dask.dataframe.tests.test_groupby.test_groupby_numeric_only_false_cov_corr(func)
dask.dataframe.tests.test_groupby.test_groupby_numeric_only_not_implemented(func,numeric_only)
dask.dataframe.tests.test_groupby.test_groupby_numeric_only_supported(func,numeric_only)
dask.dataframe.tests.test_groupby.test_groupby_numeric_only_true(func)
dask.dataframe.tests.test_groupby.test_groupby_observed_with_agg()
dask.dataframe.tests.test_groupby.test_groupby_on_index(scheduler)
dask.dataframe.tests.test_groupby.test_groupby_reduction_split(keyword,agg_func,shuffle_method)
dask.dataframe.tests.test_groupby.test_groupby_select_column_agg(func)
dask.dataframe.tests.test_groupby.test_groupby_series_cum_caching()
dask.dataframe.tests.test_groupby.test_groupby_set_index()
dask.dataframe.tests.test_groupby.test_groupby_shift_basic_input(npartitions,period,axis)
dask.dataframe.tests.test_groupby.test_groupby_shift_lazy_input()
dask.dataframe.tests.test_groupby.test_groupby_shift_series()
dask.dataframe.tests.test_groupby.test_groupby_shift_with_freq(shuffle_method)
dask.dataframe.tests.test_groupby.test_groupby_shift_within_partition_sorting()
dask.dataframe.tests.test_groupby.test_groupby_slice_agg_reduces()
dask.dataframe.tests.test_groupby.test_groupby_slice_getitem(by,slice_key)
dask.dataframe.tests.test_groupby.test_groupby_sort_argument(by,agg,sort)
dask.dataframe.tests.test_groupby.test_groupby_sort_argument_agg(agg,sort)
dask.dataframe.tests.test_groupby.test_groupby_sort_true_split_out()
dask.dataframe.tests.test_groupby.test_groupby_split_out_multiindex(split_out,column)
dask.dataframe.tests.test_groupby.test_groupby_split_out_num()
dask.dataframe.tests.test_groupby.test_groupby_string_label()
dask.dataframe.tests.test_groupby.test_groupby_transform_funcs(transformation)
dask.dataframe.tests.test_groupby.test_groupby_transform_ufunc_partitioning(npartitions,indexed)
dask.dataframe.tests.test_groupby.test_groupby_unaligned_index()
dask.dataframe.tests.test_groupby.test_groupby_unique(int_dtype)
dask.dataframe.tests.test_groupby.test_groupby_value_counts(by,int_dtype)
dask.dataframe.tests.test_groupby.test_groupby_value_counts_10322()
dask.dataframe.tests.test_groupby.test_groupby_var_dropna_observed(dropna,observed,func)
dask.dataframe.tests.test_groupby.test_groupby_with_pd_grouper()
dask.dataframe.tests.test_groupby.test_groupby_with_row_operations(operation)
dask.dataframe.tests.test_groupby.test_groupy_non_aligned_index()
dask.dataframe.tests.test_groupby.test_groupy_series_wrong_grouper()
dask.dataframe.tests.test_groupby.test_hash_groupby_aggregate(npartitions,split_every,split_out)
dask.dataframe.tests.test_groupby.test_numeric_column_names()
dask.dataframe.tests.test_groupby.test_rounding_negative_var()
dask.dataframe.tests.test_groupby.test_series_aggregations_multilevel(grouper,split_out,agg_func)
dask.dataframe.tests.test_groupby.test_series_groupby()
dask.dataframe.tests.test_groupby.test_series_groupby_agg_custom_mean(pandas_spec,dask_spec)
dask.dataframe.tests.test_groupby.test_series_groupby_cumfunc_with_named_index(npartitions,func)
dask.dataframe.tests.test_groupby.test_series_groupby_errors()
dask.dataframe.tests.test_groupby.test_series_groupby_idxmax()
dask.dataframe.tests.test_groupby.test_series_groupby_idxmax_skipna(skipna)
dask.dataframe.tests.test_groupby.test_series_groupby_idxmin()
dask.dataframe.tests.test_groupby.test_series_groupby_idxmin_skipna(skipna)
dask.dataframe.tests.test_groupby.test_series_groupby_multi_character_column_name()
dask.dataframe.tests.test_groupby.test_series_groupby_propagates_names()
dask.dataframe.tests.test_groupby.test_series_named_agg(shuffle,agg)
dask.dataframe.tests.test_groupby.test_shuffle_aggregate(shuffle_method,split_out,split_every)
dask.dataframe.tests.test_groupby.test_shuffle_aggregate_defaults(shuffle_method)
dask.dataframe.tests.test_groupby.test_shuffle_aggregate_sort(shuffle_method,sort)
dask.dataframe.tests.test_groupby.test_split_apply_combine_on_series(empty)
dask.dataframe.tests.test_groupby.test_split_out_multi_column_groupby()
dask.dataframe.tests.test_groupby.test_std_columns_int()
dask.dataframe.tests.test_groupby.test_std_object_dtype(func)
dask.dataframe.tests.test_groupby.test_timeseries()
dask.dataframe.tests.test_groupby.test_with_min_count(min_count)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/tests/test_categorical.py----------------------------------------
A:dask.dataframe.tests.test_categorical.s->pandas.Series(range(6), index=pd.Categorical(list('bacbac')))
A:dask.dataframe.tests.test_categorical.ds->dask.dataframe.from_pandas(s, npartitions=2)
A:dask.dataframe.tests.test_categorical.a->pandas.Series([1, 2, 3], dtype='category')
A:dask.dataframe.tests.test_categorical.b->pandas.Series([1, 2, 3], dtype='category').set_index('y', divisions=['a', 'b', 'c'], npartitions=a.npartitions)
A:dask.dataframe.tests.test_categorical.c->pandas.DataFrame({'v': list('klmno'), 'w': list('zzzzz'), 'x': np.arange(10, 15), 'y': list('bcbcc'), 'z': np.arange(10, 15, dtype='f8')})
A:dask.dataframe.tests.test_categorical.df.w->pandas.DataFrame({'A': ['a', 'b', 'c']}).w.astype('category')
A:dask.dataframe.tests.test_categorical.df.y->pandas.DataFrame({'A': ['a', 'b', 'c']}).y.astype('category')
A:dask.dataframe.tests.test_categorical.ddf->dask.dataframe.from_pandas(df, npartitions=2)
A:dask.dataframe.tests.test_categorical.df->pandas.DataFrame({'A': ['a', 'b', 'c']})
A:dask.dataframe.tests.test_categorical.expected->pandas.Series([1, 2, 3], dtype='category').str.upper()
A:dask.dataframe.tests.test_categorical.result->dask.dataframe.from_pandas(a, 2).str.upper()
A:dask.dataframe.tests.test_categorical.pdf->pandas.DataFrame({'id': categories * rows_per_category, 'value': np.random.random(n_rows)})
A:dask.dataframe.tests.test_categorical.meta->clear_known_categories(pdf).rename(columns={'y': 'y_'})
A:dask.dataframe.tests.test_categorical.pdf.index->pandas.DataFrame({'id': categories * rows_per_category, 'value': np.random.random(n_rows)}).index.astype('category')
A:dask.dataframe.tests.test_categorical.ddf['w']->dask.dataframe.from_pandas(df, npartitions=2).w.cat.as_unknown()
A:dask.dataframe.tests.test_categorical.ddf['y_']->dask.dataframe.from_pandas(df, npartitions=2).y_.cat.as_unknown()
A:dask.dataframe.tests.test_categorical.ddf.index->dask.dataframe.from_pandas(df, npartitions=2).index.cat.as_unknown()
A:dask.dataframe.tests.test_categorical.ddf2->dask.dataframe.from_pandas(df2, npartitions=2, sort=False)
A:dask.dataframe.tests.test_categorical.ddf_known_index->dask.dataframe.from_pandas(df, npartitions=2).categorize(columns=[], index=True)
A:dask.dataframe.tests.test_categorical.cat_dtype->dask.dataframe.categorical.categorical_dtype(meta=a, categories=[1, 100, 200], ordered=True)
A:dask.dataframe.tests.test_categorical.df['y']->df['y'].astype('category').astype('category')
A:dask.dataframe.tests.test_categorical.ddf['id']->ddf['id'].astype('category').cat.as_ordered().astype('category').cat.as_ordered()
A:dask.dataframe.tests.test_categorical.ddf['y']->ddf['y'].astype('category').astype('category')
A:dask.dataframe.tests.test_categorical.df.x->pandas.DataFrame({'A': ['a', 'b', 'c']}).x.astype('category')
A:dask.dataframe.tests.test_categorical.df2->pandas.DataFrame({'A': ['a', 'b', 'c']}).set_index(df.x)
A:dask.dataframe.tests.test_categorical.df['A']->df['A'].astype('category').astype('category')
A:dask.dataframe.tests.test_categorical.dask_df->dask.dataframe.from_pandas(df, 2)
A:dask.dataframe.tests.test_categorical.ret_type->dask.dataframe.from_pandas(df, 2).A.cat.as_known()
A:dask.dataframe.tests.test_categorical.op->operator.methodcaller(method, **kwargs)
A:dask.dataframe.tests.test_categorical.da->dask.dataframe.from_pandas(a, 2)
A:dask.dataframe.tests.test_categorical.db->dask.dataframe.from_pandas(a, 2).cat.as_known()
A:dask.dataframe.tests.test_categorical.res->dask.dataframe.from_pandas(a, 2).cat.as_known().compute()
dask.dataframe.tests.test_categorical.TestCategoricalAccessor
dask.dataframe.tests.test_categorical.TestCategoricalAccessor.test_callable(self,series,method,kwargs)
dask.dataframe.tests.test_categorical.TestCategoricalAccessor.test_categorical_empty(self)
dask.dataframe.tests.test_categorical.TestCategoricalAccessor.test_categorical_non_string_raises(self)
dask.dataframe.tests.test_categorical.TestCategoricalAccessor.test_categorical_string_ops(self)
dask.dataframe.tests.test_categorical.TestCategoricalAccessor.test_properties(self,series,prop,compare)
dask.dataframe.tests.test_categorical.TestCategoricalAccessor.test_unknown_categories(self,series)
dask.dataframe.tests.test_categorical.assert_array_index_eq(left,right,check_divisions=False)
dask.dataframe.tests.test_categorical.get_cat(x)
dask.dataframe.tests.test_categorical.test_categorical_accessor_presence()
dask.dataframe.tests.test_categorical.test_categorical_dtype()
dask.dataframe.tests.test_categorical.test_categorical_set_index(shuffle_method)
dask.dataframe.tests.test_categorical.test_categorical_set_index_npartitions_vs_ncategories(npartitions,ncategories)
dask.dataframe.tests.test_categorical.test_categorize()
dask.dataframe.tests.test_categorical.test_categorize_index()
dask.dataframe.tests.test_categorical.test_categorize_nan()
dask.dataframe.tests.test_categorical.test_concat_unions_categoricals()
dask.dataframe.tests.test_categorical.test_repartition_on_categoricals(npartitions)
dask.dataframe.tests.test_categorical.test_return_type_known_categories()
dask.dataframe.tests.test_categorical.test_unknown_categoricals(shuffle_method,numeric_only,npartitions,split_out,request)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/tests/test_pyarrow.py----------------------------------------
A:dask.dataframe.tests.test_pyarrow.pa->pytest.importorskip('pyarrow')
A:dask.dataframe.tests.test_pyarrow.dtype->pandas.ArrowDtype(dtype)
dask.dataframe.tests.test_pyarrow.test_is_object_string_dtype(dtype,expected)
dask.dataframe.tests.test_pyarrow.test_is_object_string_index(index,expected)
dask.dataframe.tests.test_pyarrow.test_is_object_string_series(series,expected)
dask.dataframe.tests.test_pyarrow.test_is_pyarrow_string_dtype(dtype,expected)
dask.dataframe.tests.test_pyarrow.tests_is_object_string_dataframe(series,expected)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/tests/test_reshape.py----------------------------------------
A:dask.dataframe.tests.test_reshape.exp->pandas.pivot_table(df, index='A', columns='B', values='C', aggfunc='count', observed=False).astype(np.float64)
A:dask.dataframe.tests.test_reshape.ddata->dask.dataframe.from_pandas(data, 2)
A:dask.dataframe.tests.test_reshape.res->dask.dataframe.pivot_table(ddf, index='A', columns='B', values='C', aggfunc='count')
A:dask.dataframe.tests.test_reshape.df->pandas.DataFrame({'A': np.random.choice(list('abc'), size=10), 'B': np.random.randn(10), 'C': np.random.choice(list('abc'), size=10)})
A:dask.dataframe.tests.test_reshape.ddf->dask.dataframe.from_pandas(df, 2)
A:dask.dataframe.tests.test_reshape.res_p->pandas.get_dummies(df.astype('category'))
A:dask.dataframe.tests.test_reshape.res_d->dask.dataframe.get_dummies(ddf)
A:dask.dataframe.tests.test_reshape.s->pandas.Series([1, 1, 1, 2, 2, 1, 3, 4])
A:dask.dataframe.tests.test_reshape.ds->dask.dataframe.from_pandas(s, 2)
A:dask.dataframe.tests.test_reshape.ddf._meta->make_meta({'x': 'category', 'y': 'category'}, parent_meta=pd.DataFrame())
A:dask.dataframe.tests.test_reshape.exp_index->pandas.CategoricalIndex(['A', 'B'], name='B')
A:dask.dataframe.tests.test_reshape.ddf['C']->dask.dataframe.from_pandas(df, 2).C.cat.as_unknown()
dask.dataframe.tests.test_reshape.check_pandas_issue_45618_warning(test_func)
dask.dataframe.tests.test_reshape.ignore_numpy_bool8_deprecation()
dask.dataframe.tests.test_reshape.test_get_dummies(data)
dask.dataframe.tests.test_reshape.test_get_dummies_categories_order()
dask.dataframe.tests.test_reshape.test_get_dummies_dtype()
dask.dataframe.tests.test_reshape.test_get_dummies_errors()
dask.dataframe.tests.test_reshape.test_get_dummies_kwargs()
dask.dataframe.tests.test_reshape.test_get_dummies_object()
dask.dataframe.tests.test_reshape.test_get_dummies_sparse()
dask.dataframe.tests.test_reshape.test_get_dummies_sparse_mix()
dask.dataframe.tests.test_reshape.test_pivot_table(values,aggfunc)
dask.dataframe.tests.test_reshape.test_pivot_table_dtype()
dask.dataframe.tests.test_reshape.test_pivot_table_errors()
dask.dataframe.tests.test_reshape.test_pivot_table_firstlast(values,aggfunc)
dask.dataframe.tests.test_reshape.test_pivot_table_index_dtype()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/tests/test_merge_column_and_index.py----------------------------------------
A:dask.dataframe.tests.test_merge_column_and_index.partition_sizes->numpy.array([4, 2, 5, 3, 2, 5, 9, 4, 7, 4, 8])
A:dask.dataframe.tests.test_merge_column_and_index.vi->range(len(k))
A:dask.dataframe.tests.test_merge_column_and_index.expected->df_left.merge(df_right, on=on, how=how)
A:dask.dataframe.tests.test_merge_column_and_index.result->ddf_left_double.merge(ddf_right, on=on, how=how, broadcast=broadcast, shuffle_method=shuffle_method)
A:dask.dataframe.tests.test_merge_column_and_index.pdf1->pandas.DataFrame({'x': range(20), 'y': range(20)})
A:dask.dataframe.tests.test_merge_column_and_index.df1->pandas.DataFrame({'a': ['0', '0', None, None, None, None, '5', '7', '15', '33']})
A:dask.dataframe.tests.test_merge_column_and_index.pdf2->pandas.DataFrame({'z': range(10)}, index=pd.Index(range(10), name='a'))
A:dask.dataframe.tests.test_merge_column_and_index.df2->pandas.DataFrame({'c': ['1', '2', '3', '4'], 'b': ['0', '5', '7', '15']})
A:dask.dataframe.tests.test_merge_column_and_index.df3->pandas.DataFrame({'a': ['0', '0', None, None, None, None, '5', '7', '15', '33']}).join(df2.clear_divisions(), on='x', how=how, shuffle_method=shuffle_method)
A:dask.dataframe.tests.test_merge_column_and_index.expect->pandas.DataFrame({'x': range(20), 'y': range(20)}).join(pdf2, on='x', how=how)
A:dask.dataframe.tests.test_merge_column_and_index.df1_d->dask.dataframe.from_pandas(df1, npartitions=4)
A:dask.dataframe.tests.test_merge_column_and_index.df2_d->df2_d.repartition(npartitions=repartition).repartition(npartitions=repartition)
A:dask.dataframe.tests.test_merge_column_and_index.pandas_result->pandas.DataFrame({'a': ['0', '0', None, None, None, None, '5', '7', '15', '33']}).merge(df2.set_index('b'), how='left', left_on='a', right_index=True)
A:dask.dataframe.tests.test_merge_column_and_index.dask_result->dask.dataframe.from_pandas(df1, npartitions=4).merge(df2_d, how='left', left_on='a', right_index=True)
dask.dataframe.tests.test_merge_column_and_index.ddf_left(df_left)
dask.dataframe.tests.test_merge_column_and_index.ddf_left_double(df_left)
dask.dataframe.tests.test_merge_column_and_index.ddf_left_single(df_left)
dask.dataframe.tests.test_merge_column_and_index.ddf_left_unknown(ddf_left)
dask.dataframe.tests.test_merge_column_and_index.ddf_right(df_right)
dask.dataframe.tests.test_merge_column_and_index.ddf_right_double(df_right)
dask.dataframe.tests.test_merge_column_and_index.ddf_right_single(df_right)
dask.dataframe.tests.test_merge_column_and_index.ddf_right_unknown(ddf_right)
dask.dataframe.tests.test_merge_column_and_index.df_left()
dask.dataframe.tests.test_merge_column_and_index.df_right()
dask.dataframe.tests.test_merge_column_and_index.how(request)
dask.dataframe.tests.test_merge_column_and_index.on(request)
dask.dataframe.tests.test_merge_column_and_index.test_join(how,shuffle_method)
dask.dataframe.tests.test_merge_column_and_index.test_merge_column_with_nulls(repartition)
dask.dataframe.tests.test_merge_column_and_index.test_merge_known_to_double_bcast_left(df_left,df_right,ddf_left_double,ddf_right,on,shuffle_method,how,broadcast)
dask.dataframe.tests.test_merge_column_and_index.test_merge_known_to_double_bcast_right(df_left,df_right,ddf_left,ddf_right_double,on,how,shuffle_method)
dask.dataframe.tests.test_merge_column_and_index.test_merge_known_to_known(df_left,df_right,ddf_left,ddf_right,on,how,shuffle_method)
dask.dataframe.tests.test_merge_column_and_index.test_merge_known_to_single(df_left,df_right,ddf_left,ddf_right_single,on,how,shuffle_method)
dask.dataframe.tests.test_merge_column_and_index.test_merge_known_to_unknown(df_left,df_right,ddf_left,ddf_right_unknown,on,how,shuffle_method)
dask.dataframe.tests.test_merge_column_and_index.test_merge_single_to_known(df_left,df_right,ddf_left_single,ddf_right,on,how,shuffle_method)
dask.dataframe.tests.test_merge_column_and_index.test_merge_unknown_to_known(df_left,df_right,ddf_left_unknown,ddf_right,on,how,shuffle_method)
dask.dataframe.tests.test_merge_column_and_index.test_merge_unknown_to_unknown(df_left,df_right,ddf_left_unknown,ddf_right_unknown,on,how,shuffle_method)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/tests/test_methods.py----------------------------------------
A:dask.dataframe.tests.test_methods.df->pandas.DataFrame({'a': [1, 2, 3], 'b': 1.5})
A:dask.dataframe.tests.test_methods.result->dask.dataframe.methods.assign(df, 'a', 5)
dask.dataframe.tests.test_methods.test_assign_not_modifying_array_inplace()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/tests/test_hashing.py----------------------------------------
A:dask.dataframe.tests.test_hashing.a->hash_pandas_object(obj)
A:dask.dataframe.tests.test_hashing.b->hash_pandas_object(obj)
A:dask.dataframe.tests.test_hashing.s2->s1.astype('category').cat.set_categories(s1)
A:dask.dataframe.tests.test_hashing.s3->s1.astype('category').cat.set_categories(s1).cat.set_categories(list(reversed(s1)))
A:dask.dataframe.tests.test_hashing.h1->hash_pandas_object(s1, categorize=categorize)
A:dask.dataframe.tests.test_hashing.h2->hash_pandas_object(s.iloc[:3])
A:dask.dataframe.tests.test_hashing.h3->hash_pandas_object(s3, categorize=categorize)
A:dask.dataframe.tests.test_hashing.s->pandas.Series(['a', 'b', 'c', None])
A:dask.dataframe.tests.test_hashing.result->dask.dataframe.dispatch.hash_object_dispatch(obj)
A:dask.dataframe.tests.test_hashing.expected->pandas.util.hash_pandas_object(obj)
dask.dataframe.tests.test_hashing.test_categorical_consistency()
dask.dataframe.tests.test_hashing.test_hash_object_dispatch(obj)
dask.dataframe.tests.test_hashing.test_hash_pandas_object(obj)
dask.dataframe.tests.test_hashing.test_object_missing_values()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/tests/test_pyarrow_compat.py----------------------------------------
A:dask.dataframe.tests.test_pyarrow_compat.pa->pytest.importorskip('pyarrow')
A:dask.dataframe.tests.test_pyarrow_compat.pa_dtype->pytest.importorskip('pyarrow').string()
A:dask.dataframe.tests.test_pyarrow_compat.expected->pandas.Series(map(str, range(1000)), dtype=string_dtype)
A:dask.dataframe.tests.test_pyarrow_compat.expected_sliced->pandas.Series(map(str, range(1000)), dtype=string_dtype).head(2)
A:dask.dataframe.tests.test_pyarrow_compat.full_pickled->pickle.dumps(expected)
A:dask.dataframe.tests.test_pyarrow_compat.sliced_pickled->pickle.dumps(expected_sliced)
A:dask.dataframe.tests.test_pyarrow_compat.result->pickle.loads(full_pickled)
A:dask.dataframe.tests.test_pyarrow_compat.result_sliced->pickle.loads(sliced_pickled)
A:dask.dataframe.tests.test_pyarrow_compat.string_dtype->pandas.ArrowDtype(pa.string())
dask.dataframe.tests.test_pyarrow_compat.data(dtype)
dask.dataframe.tests.test_pyarrow_compat.dtype(request)
dask.dataframe.tests.test_pyarrow_compat.test_pickle_roundtrip(data)
dask.dataframe.tests.test_pyarrow_compat.test_pickle_roundtrip_pyarrow_string_implementations(string_dtype)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/tests/test_arithmetics_reduction.py----------------------------------------
A:dask.dataframe.tests.test_arithmetics_reduction.DASK_EXPR_ENABLED->dask.dataframe._dask_expr_enabled()
A:dask.dataframe.tests.test_arithmetics_reduction.meta->make_meta({'a': 'i8', 'b': 'i8'}, index=pd.Index([], 'i8'), parent_meta=pd.DataFrame())
A:dask.dataframe.tests.test_arithmetics_reduction.ddf1->dask.dataframe.from_pandas(df1, npartitions=3)
A:dask.dataframe.tests.test_arithmetics_reduction.pdf1->dask.dataframe.from_pandas(df1, npartitions=3).compute()
A:dask.dataframe.tests.test_arithmetics_reduction.pdf2->pandas.DataFrame({'dt1': [pd.NaT] + [datetime.fromtimestamp(1636426704 + i * 250000) for i in range(10)] + [pd.NaT], 'dt2': [datetime.fromtimestamp(1636426704 + i * 250000) for i in range(12)], 'dt3': [datetime.fromtimestamp(1636426704 + i * 282616) for i in range(12)]})
A:dask.dataframe.tests.test_arithmetics_reduction.pdf3->pandas.DataFrame({'a': [1, 2, 3, 4, 5], 'b': [3, 5, 2, 5, 7]}, index=[1, 2, 3, 4, 5])
A:dask.dataframe.tests.test_arithmetics_reduction.ddf2->dask.dataframe.from_pandas(pdf2, 3)
A:dask.dataframe.tests.test_arithmetics_reduction.ddf3->dask.dataframe.from_pandas(pdf3, 2)
A:dask.dataframe.tests.test_arithmetics_reduction.ddf4->dask.dataframe.from_pandas(pdf4, 2)
A:dask.dataframe.tests.test_arithmetics_reduction.pdf4->pandas.DataFrame({'a': [3, 2, 6, 7, 8], 'b': [9, 4, 2, 6, 2]}, index=[10, 11, 12, 13, 14])
A:dask.dataframe.tests.test_arithmetics_reduction.pdf5->pandas.DataFrame({'a': [1, 2, 3, 4, 5], 'b': [3, 5, 2, 5, 7]}, index=[1, 3, 5, 7, 9])
A:dask.dataframe.tests.test_arithmetics_reduction.ddf5->dask.dataframe.from_pandas(pdf5, 2)
A:dask.dataframe.tests.test_arithmetics_reduction.pdf6->pandas.DataFrame({'a': [3, 2, 6, 7, 8], 'b': [9, 4, 2, 6, 2]}, index=[2, 3, 4, 5, 6])
A:dask.dataframe.tests.test_arithmetics_reduction.ddf6->dask.dataframe.from_pandas(pdf6, 2)
A:dask.dataframe.tests.test_arithmetics_reduction.pdf7->pandas.DataFrame({'a': [1, 2, 3, 4, 5, 6, 7, 8], 'b': [5, 6, 7, 8, 1, 2, 3, 4]}, index=[0, 2, 4, 8, 9, 10, 11, 13])
A:dask.dataframe.tests.test_arithmetics_reduction.pdf8->pandas.DataFrame({'a': [5, 6, 7, 8, 4, 3, 2, 1], 'b': [2, 4, 5, 3, 4, 2, 1, 0]}, index=[1, 3, 4, 8, 9, 11, 12, 13])
A:dask.dataframe.tests.test_arithmetics_reduction.ddf7->dask.dataframe.from_pandas(pdf7, 3)
A:dask.dataframe.tests.test_arithmetics_reduction.ddf8->dask.dataframe.from_pandas(pdf8, 2)
A:dask.dataframe.tests.test_arithmetics_reduction.pdf9->pandas.DataFrame({'a': [1, 2, 3, 4, 5, 6, 7, 8], 'b': [5, 6, 7, 8, 1, 2, 3, 4]}, index=[0, 2, 4, 8, 9, 10, 11, 13])
A:dask.dataframe.tests.test_arithmetics_reduction.pdf10->pandas.DataFrame({'a': [5, 6, 7, 8, 4, 3, 2, 1], 'b': [2, 4, 5, 3, 4, 2, 1, 0]}, index=[0, 3, 4, 8, 9, 11, 12, 13])
A:dask.dataframe.tests.test_arithmetics_reduction.ddf9->dask.dataframe.from_pandas(pdf9, 3)
A:dask.dataframe.tests.test_arithmetics_reduction.ddf10->dask.dataframe.from_pandas(pdf10, 2)
A:dask.dataframe.tests.test_arithmetics_reduction.df->pandas.DataFrame([[1]], columns=['a'])
A:dask.dataframe.tests.test_arithmetics_reduction.a->pandas.Series([1, None, 2], dtype=pd.Int32Dtype())
A:dask.dataframe.tests.test_arithmetics_reduction.el->numpy.int64(10)
A:dask.dataframe.tests.test_arithmetics_reduction.er->numpy.int64(4)
A:dask.dataframe.tests.test_arithmetics_reduction.l->dask.dataframe.core.Scalar({('l', 0): el}, 'l', 'i8')
A:dask.dataframe.tests.test_arithmetics_reduction.r->dask.dataframe.core.Scalar({('r', 0): er}, 'r', 'i8')
A:dask.dataframe.tests.test_arithmetics_reduction.s->pandas.Series([1, 2, 3, 4, 5, 6, 7])
A:dask.dataframe.tests.test_arithmetics_reduction.pds->pandas.Series(pd.timedelta_range('1 days', freq='D', periods=5))
A:dask.dataframe.tests.test_arithmetics_reduction.dds->dask.dataframe.from_pandas(pds, 2)
A:dask.dataframe.tests.test_arithmetics_reduction.pdf->pandas.DataFrame({'dt1': [datetime.fromtimestamp(1636426704 + i * 250000) for i in range(10)], 'dt2': [datetime.fromtimestamp(1636426704 + i * 217790) for i in range(10)], 'nums': [i for i in range(10)]})
A:dask.dataframe.tests.test_arithmetics_reduction.ddf->dask.dataframe.from_pandas(pdf, 3)
A:dask.dataframe.tests.test_arithmetics_reduction.ps3->pandas.Series(np.random.randn(10), index=list('ABCDXabcde'))
A:dask.dataframe.tests.test_arithmetics_reduction.nans1->pandas.Series([1] + [np.nan] * 4 + [2] + [np.nan] * 3)
A:dask.dataframe.tests.test_arithmetics_reduction.nands1->dask.dataframe.from_pandas(nans1, 2)
A:dask.dataframe.tests.test_arithmetics_reduction.nans2->pandas.Series([1] + [np.nan] * 8)
A:dask.dataframe.tests.test_arithmetics_reduction.nands2->dask.dataframe.from_pandas(nans2, 2)
A:dask.dataframe.tests.test_arithmetics_reduction.nans3->pandas.Series([np.nan] * 9)
A:dask.dataframe.tests.test_arithmetics_reduction.nands3->dask.dataframe.from_pandas(nans3, 2)
A:dask.dataframe.tests.test_arithmetics_reduction.bools->pandas.Series([True, False, True, False, True], dtype=bool)
A:dask.dataframe.tests.test_arithmetics_reduction.boolds->dask.dataframe.from_pandas(bools, 2)
A:dask.dataframe.tests.test_arithmetics_reduction.ds->dask.dataframe.from_pandas(ser, npartitions=2)
A:dask.dataframe.tests.test_arithmetics_reduction.dsk_in->dask.dataframe.from_pandas(frame, 3)
A:dask.dataframe.tests.test_arithmetics_reduction.dsk_out->dask.dataframe.from_pandas(out, 3)
A:dask.dataframe.tests.test_arithmetics_reduction.np_redfunc->getattr(np, redfunc)
A:dask.dataframe.tests.test_arithmetics_reduction.pd_redfunc->getattr(frame.__class__, redfunc)
A:dask.dataframe.tests.test_arithmetics_reduction.dsk_redfunc->getattr(dsk_in.__class__, redfunc)
A:dask.dataframe.tests.test_arithmetics_reduction.ctx->pytest.warns(FutureWarning, match='numeric_only')
A:dask.dataframe.tests.test_arithmetics_reduction.ddf_out_axis_default->dask.dataframe.from_pandas(pd.Series([False, False, False, False, False], index=['A', 'B', 'C', 'D', 'E']), 10)
A:dask.dataframe.tests.test_arithmetics_reduction.ddf_out_axis1->dask.dataframe.from_pandas(pd.Series(np.random.choice([True, False], size=(100,))), 10)
A:dask.dataframe.tests.test_arithmetics_reduction.result->dask.dataframe.from_pandas(pdf2, 3).std(axis=1, **kwargs)
A:dask.dataframe.tests.test_arithmetics_reduction.expected->pandas.DataFrame({'dt1': [pd.NaT] + [datetime.fromtimestamp(1636426704 + i * 250000) for i in range(10)] + [pd.NaT], 'dt2': [datetime.fromtimestamp(1636426704 + i * 250000) for i in range(12)], 'dt3': [datetime.fromtimestamp(1636426704 + i * 282616) for i in range(12)]}).std(axis=1, **kwargs)
A:dask.dataframe.tests.test_arithmetics_reduction.actual->getattr(ddf, func)(**kwargs)
A:dask.dataframe.tests.test_arithmetics_reduction.pd_result->getattr(ser, func)()
A:dask.dataframe.tests.test_arithmetics_reduction.dd_result->getattr(ds, func)()
A:dask.dataframe.tests.test_arithmetics_reduction.s_nan->pandas.Series([1, -1, 8, np.nan, 5, 6, 2.4])
A:dask.dataframe.tests.test_arithmetics_reduction.ds_nan->dask.dataframe.from_pandas(s_nan, 3)
A:dask.dataframe.tests.test_arithmetics_reduction.comparison_pd->getattr(s, comparison)
A:dask.dataframe.tests.test_arithmetics_reduction.comparison_dd->getattr(ds, comparison)
A:dask.dataframe.tests.test_arithmetics_reduction.b->dask.dataframe.from_pandas(a, 2)
A:dask.dataframe.tests.test_arithmetics_reduction.df1->pandas.Series(np.random.rand(10))
A:dask.dataframe.tests.test_arithmetics_reduction.df2->pandas.Series(np.random.rand(10))
A:dask.dataframe.tests.test_arithmetics_reduction.dsk_func->getattr(ddf.__class__, func)
A:dask.dataframe.tests.test_arithmetics_reduction.pd_func->getattr(pdf.__class__, func)
A:dask.dataframe.tests.test_arithmetics_reduction.idx->pandas.date_range('2000', periods=4)
A:dask.dataframe.tests.test_arithmetics_reduction.func->getattr(ddf['a'], method)
A:dask.dataframe.tests.test_arithmetics_reduction.t1->pandas.Series([t1])
A:dask.dataframe.tests.test_arithmetics_reduction.t2->pandas.Series([t2])
A:dask.dataframe.tests.test_arithmetics_reduction.dt1->pandas.concat([pd.Series([pd.NaT] * 15, index=range(15)), pd.to_datetime(pd.Series([datetime.fromtimestamp(1636426704 + i * 250000) for i in range(num_rows - 15)], index=range(15, 250)))], ignore_index=False)
A:dask.dataframe.tests.test_arithmetics_reduction.pdf[f'dt{i}']->pandas.to_datetime(pd.Series([int(x + 0.12 * i) for x in base_numbers]))
A:dask.dataframe.tests.test_arithmetics_reduction.pa->pytest.importorskip('pyarrow')
A:dask.dataframe.tests.test_arithmetics_reduction.ser->pandas.Series([1, 2, 3, 4], dtype=dtype)
dask.dataframe.tests.test_arithmetics_reduction.assert_near_timedeltas(t1,t2,atol=2000)
dask.dataframe.tests.test_arithmetics_reduction.check_frame_arithmetics(l,r,el,er,allow_comparison_ops=True)
dask.dataframe.tests.test_arithmetics_reduction.check_series_arithmetics(l,r,el,er,allow_comparison_ops=True)
dask.dataframe.tests.test_arithmetics_reduction.test_allany(split_every)
dask.dataframe.tests.test_arithmetics_reduction.test_arithmetics()
dask.dataframe.tests.test_arithmetics_reduction.test_arithmetics_different_index()
dask.dataframe.tests.test_arithmetics_reduction.test_count_numeric_only_axis_one()
dask.dataframe.tests.test_arithmetics_reduction.test_datetime_std_across_axis1_null_results(skipna,numeric_only)
dask.dataframe.tests.test_arithmetics_reduction.test_datetime_std_creates_copy_cols(axis,numeric_only)
dask.dataframe.tests.test_arithmetics_reduction.test_datetime_std_with_larger_dataset(axis,skipna,numeric_only)
dask.dataframe.tests.test_arithmetics_reduction.test_deterministic_arithmetic_names()
dask.dataframe.tests.test_arithmetics_reduction.test_deterministic_reduction_names(split_every)
dask.dataframe.tests.test_arithmetics_reduction.test_divmod()
dask.dataframe.tests.test_arithmetics_reduction.test_empty_df_reductions(func)
dask.dataframe.tests.test_arithmetics_reduction.test_frame_series_arithmetic_methods()
dask.dataframe.tests.test_arithmetics_reduction.test_moment()
dask.dataframe.tests.test_arithmetics_reduction.test_reduction_series_invalid_axis()
dask.dataframe.tests.test_arithmetics_reduction.test_reductions(split_every)
dask.dataframe.tests.test_arithmetics_reduction.test_reductions_frame(split_every)
dask.dataframe.tests.test_arithmetics_reduction.test_reductions_frame_dtypes(func,kwargs,numeric_only)
dask.dataframe.tests.test_arithmetics_reduction.test_reductions_frame_dtypes_numeric_only(func)
dask.dataframe.tests.test_arithmetics_reduction.test_reductions_frame_dtypes_numeric_only_supported(func)
dask.dataframe.tests.test_arithmetics_reduction.test_reductions_frame_nan(split_every)
dask.dataframe.tests.test_arithmetics_reduction.test_reductions_non_numeric_dtypes()
dask.dataframe.tests.test_arithmetics_reduction.test_reductions_out(frame,axis,out,redfunc)
dask.dataframe.tests.test_arithmetics_reduction.test_reductions_timedelta(split_every)
dask.dataframe.tests.test_arithmetics_reduction.test_reductions_with_pandas_and_arrow_ea(dtype,func)
dask.dataframe.tests.test_arithmetics_reduction.test_scalar_arithmetics()
dask.dataframe.tests.test_arithmetics_reduction.test_scalar_arithmetics_with_dask_instances()
dask.dataframe.tests.test_arithmetics_reduction.test_series_agg_with_min_count(method,min_count)
dask.dataframe.tests.test_arithmetics_reduction.test_series_comparison_nan(comparison)
dask.dataframe.tests.test_arithmetics_reduction.test_skew_kurt_numeric_only_false(func)
dask.dataframe.tests.test_arithmetics_reduction.test_std_raises_on_index()
dask.dataframe.tests.test_arithmetics_reduction.test_std_raises_with_arrow_string_ea()
dask.dataframe.tests.test_arithmetics_reduction.test_sum_intna()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/tests/test_shuffle.py----------------------------------------
A:dask.dataframe.tests.test_shuffle.DASK_EXPR_ENABLED->dask.dataframe._dask_expr_enabled()
A:dask.dataframe.tests.test_shuffle.meta->pandas.Series(dtype=int, index=pd.Index([], dtype=bool, name='A'), name='A')
A:dask.dataframe.tests.test_shuffle.d->dask.dataframe.from_pandas(df, npartitions=1)
A:dask.dataframe.tests.test_shuffle.full->dask.dataframe.from_pandas(df, npartitions=1).compute()
A:dask.dataframe.tests.test_shuffle.s->shuffle(ddf, ddf.x, npartitions=6, shuffle_method=shuffle_method)
A:dask.dataframe.tests.test_shuffle.x->dask.get(s.dask, (s._name, 0))
A:dask.dataframe.tests.test_shuffle.y->dask.get(s.dask, (s._name, 1))
A:dask.dataframe.tests.test_shuffle.result->dask.dataframe.merge(ddf1, ddf2, how='outer', on='B').groupby('A').apply(lambda df: len(df), meta=meta)
A:dask.dataframe.tests.test_shuffle.df->pandas.DataFrame.from_records([[pd.Timestamp('2002-01-11 21:00:01+0000', tz='UTC'), 4223, 54719.0], [pd.Timestamp('2002-01-14 21:00:01+0000', tz='UTC'), 6942, 19223.0], [pd.Timestamp('2002-01-15 21:00:01+0000', tz='UTC'), 12551, 72865.0], [pd.Timestamp('2002-01-23 21:00:01+0000', tz='UTC'), 6005, 57670.0], [pd.Timestamp('2002-01-29 21:00:01+0000', tz='UTC'), 2043, 58600.0], [pd.Timestamp('2002-02-01 21:00:01+0000', tz='UTC'), 6909, 8459.0], [pd.Timestamp('2002-01-14 21:00:01+0000', tz='UTC'), 5326, 77339.0], [pd.Timestamp('2002-01-14 21:00:01+0000', tz='UTC'), 4711, 54135.0], [pd.Timestamp('2002-01-22 21:00:01+0000', tz='UTC'), 103, 57627.0], [pd.Timestamp('2002-01-30 21:00:01+0000', tz='UTC'), 16862, 54458.0], [pd.Timestamp('2002-01-31 21:00:01+0000', tz='UTC'), 4143, 56280.0]], columns=['time', 'id1', 'id2'])
A:dask.dataframe.tests.test_shuffle.ddf->dask.dataframe.from_pandas(pdf, npartitions=2)
A:dask.dataframe.tests.test_shuffle.sc->shuffle(ddf, ddf.x, npartitions=6, shuffle_method=shuffle_method).compute()
A:dask.dataframe.tests.test_shuffle.res1->shuffle(d, d[['b']], shuffle_method=shuffle_method).compute()
A:dask.dataframe.tests.test_shuffle.res2->partitioning_index(df2, 5)
A:dask.dataframe.tests.test_shuffle.res3->shuffle(d, 'b', shuffle_method=shuffle_method).compute()
A:dask.dataframe.tests.test_shuffle.a->d[d.a > 3].set_index('a', sorted=True)
A:dask.dataframe.tests.test_shuffle.b->d[d.a > 3].set_index('a', sorted=True).set_index('key', divisions=divisions)
A:dask.dataframe.tests.test_shuffle.parts->get(result.dask, result.__dask_keys__())
A:dask.dataframe.tests.test_shuffle.df2->pandas.DataFrame([[2, 3], [109, 2], [345, 3], [50, 7], [95, 1]], columns=['B', 'C'])
A:dask.dataframe.tests.test_shuffle.res->dask.dataframe.from_pandas(pdf, npartitions=2).set_index(ddf.d2 + one_day)
A:dask.dataframe.tests.test_shuffle.df.a->pandas.DataFrame.from_records([[pd.Timestamp('2002-01-11 21:00:01+0000', tz='UTC'), 4223, 54719.0], [pd.Timestamp('2002-01-14 21:00:01+0000', tz='UTC'), 6942, 19223.0], [pd.Timestamp('2002-01-15 21:00:01+0000', tz='UTC'), 12551, 72865.0], [pd.Timestamp('2002-01-23 21:00:01+0000', tz='UTC'), 6005, 57670.0], [pd.Timestamp('2002-01-29 21:00:01+0000', tz='UTC'), 2043, 58600.0], [pd.Timestamp('2002-02-01 21:00:01+0000', tz='UTC'), 6909, 8459.0], [pd.Timestamp('2002-01-14 21:00:01+0000', tz='UTC'), 5326, 77339.0], [pd.Timestamp('2002-01-14 21:00:01+0000', tz='UTC'), 4711, 54135.0], [pd.Timestamp('2002-01-22 21:00:01+0000', tz='UTC'), 103, 57627.0], [pd.Timestamp('2002-01-30 21:00:01+0000', tz='UTC'), 16862, 54458.0], [pd.Timestamp('2002-01-31 21:00:01+0000', tz='UTC'), 4143, 56280.0]], columns=['time', 'id1', 'id2']).a.astype('category')
A:dask.dataframe.tests.test_shuffle.df2.a->pandas.DataFrame([[2, 3], [109, 2], [345, 3], [50, 7], [95, 1]], columns=['B', 'C']).a.cat.set_categories(list(reversed(df2.a.cat.categories)))
A:dask.dataframe.tests.test_shuffle.ddf2->dask.dataframe.from_pandas(df2, npartitions=1)
A:dask.dataframe.tests.test_shuffle.df_result->pandas.DataFrame.from_records([[pd.Timestamp('2002-01-11 21:00:01+0000', tz='UTC'), 4223, 54719.0], [pd.Timestamp('2002-01-14 21:00:01+0000', tz='UTC'), 6942, 19223.0], [pd.Timestamp('2002-01-15 21:00:01+0000', tz='UTC'), 12551, 72865.0], [pd.Timestamp('2002-01-23 21:00:01+0000', tz='UTC'), 6005, 57670.0], [pd.Timestamp('2002-01-29 21:00:01+0000', tz='UTC'), 2043, 58600.0], [pd.Timestamp('2002-02-01 21:00:01+0000', tz='UTC'), 6909, 8459.0], [pd.Timestamp('2002-01-14 21:00:01+0000', tz='UTC'), 5326, 77339.0], [pd.Timestamp('2002-01-14 21:00:01+0000', tz='UTC'), 4711, 54135.0], [pd.Timestamp('2002-01-22 21:00:01+0000', tz='UTC'), 103, 57627.0], [pd.Timestamp('2002-01-30 21:00:01+0000', tz='UTC'), 16862, 54458.0], [pd.Timestamp('2002-01-31 21:00:01+0000', tz='UTC'), 4143, 56280.0]], columns=['time', 'id1', 'id2']).set_index('col1', drop=drop, append=append)
A:dask.dataframe.tests.test_shuffle.ddf_result->dask.dataframe.from_pandas(pdf, npartitions=2).set_index('arrow_col')
A:dask.dataframe.tests.test_shuffle.get->dask.base.get_scheduler(scheduler=scheduler)
A:dask.dataframe.tests.test_shuffle.tmpdir->tempfile.mkdtemp()
A:dask.dataframe.tests.test_shuffle.A->pandas.DataFrame({'key': [1, 2, 3, 4, 4, 5, 6, 7], 'value': list('abcd' * 2)})
A:dask.dataframe.tests.test_shuffle.f->maybe_buffered_partd()
A:dask.dataframe.tests.test_shuffle.p1->maybe_buffered_partd(buffer=False, tempdir=None)()
A:dask.dataframe.tests.test_shuffle.f2->pickle.loads(pickle.dumps(f))
A:dask.dataframe.tests.test_shuffle.p2->pandas.DataFrame({'x': [13, 14, 15], 'y': ['b', 'b', 'c']})
A:dask.dataframe.tests.test_shuffle.f3->maybe_buffered_partd(tempdir=tmp_path)
A:dask.dataframe.tests.test_shuffle.p3->pandas.DataFrame({'x': [16, 17, 18], 'y': ['d', 'e', 'e']})
A:dask.dataframe.tests.test_shuffle.contents->list(tmp_path.iterdir())
A:dask.dataframe.tests.test_shuffle.f4->pickle.loads(pickle.dumps(f3))
A:dask.dataframe.tests.test_shuffle.deprecated->pytest.warns(FutureWarning, match="the 'compute' keyword is deprecated")
A:dask.dataframe.tests.test_shuffle.d2->dask.dataframe.from_pandas(df, npartitions=1).set_index('tz', npartitions=1)
A:dask.dataframe.tests.test_shuffle.d3->dask.dataframe.from_pandas(df, npartitions=1).set_index(d.b, npartitions=3)
A:dask.dataframe.tests.test_shuffle.d4->dask.dataframe.from_pandas(df, npartitions=1).set_index('b')
A:dask.dataframe.tests.test_shuffle.d5->dask.dataframe.from_pandas(df, npartitions=1).set_index(['b'])
A:dask.dataframe.tests.test_shuffle.exp->dask.dataframe.from_pandas(df, npartitions=1).compute().copy()
A:dask.dataframe.tests.test_shuffle.ctx->multiprocessing.get_context('spawn')
A:dask.dataframe.tests.test_shuffle.func->partial(_set_index, df=ddf, idx='x')
A:dask.dataframe.tests.test_shuffle.divisions_set->set(pool.map(func, range(100)))
A:dask.dataframe.tests.test_shuffle.n->int(nbytes / (nparts * 8))
A:dask.dataframe.tests.test_shuffle.vals->pandas.to_datetime(vals, unit='ns')
A:dask.dataframe.tests.test_shuffle.lo->sum(breaks[:i])
A:dask.dataframe.tests.test_shuffle.hi->sum(breaks[i:i + 1])
A:dask.dataframe.tests.test_shuffle.dask_cudf->pytest.importorskip('dask_cudf')
A:dask.dataframe.tests.test_shuffle.expected->pandas.merge(df1, df2, how='outer', on='B').groupby('A').apply(lambda df: len(df))
A:dask.dataframe.tests.test_shuffle.cudf->pytest.importorskip('cudf')
A:dask.dataframe.tests.test_shuffle.gdf->pytest.importorskip('cudf').from_pandas(df)
A:dask.dataframe.tests.test_shuffle.d1->dask.dataframe.from_pandas(df, npartitions=1).set_index('notz', npartitions=1)
A:dask.dataframe.tests.test_shuffle.L->sorted(list(range(0, 200, 10)) * 2)
A:dask.dataframe.tests.test_shuffle.s_naive->pandas.Series(pd.date_range('20130101', periods=3))
A:dask.dataframe.tests.test_shuffle.s_aware->pandas.Series(pd.date_range('20130101', periods=3, tz='US/Eastern'))
A:dask.dataframe.tests.test_shuffle.s1->pandas.DatetimeIndex(s_naive.values, dtype=s_naive.dtype)
A:dask.dataframe.tests.test_shuffle.s2->pandas.DatetimeIndex(s_aware, dtype=s_aware.dtype)
A:dask.dataframe.tests.test_shuffle.s2badtype->pandas.DatetimeIndex(s_aware.values, dtype=s_naive.dtype)
A:dask.dataframe.tests.test_shuffle.data->pandas.DataFrame(index=pd.Index(['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'C'], name='index'))
A:dask.dataframe.tests.test_shuffle.output->pandas.DataFrame(index=pd.Index(['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'C'], name='index')).reset_index().set_index('index', npartitions=1)
A:dask.dataframe.tests.test_shuffle.df.ts->pandas.to_datetime(df.ts, unit=unit)
A:dask.dataframe.tests.test_shuffle.pdf->pandas.DataFrame({'a': 1, 'arrow_col': pd.Series(data, dtype=dtype)})
A:dask.dataframe.tests.test_shuffle.aa->dask.delayed(a)
A:dask.dataframe.tests.test_shuffle.bb->dask.delayed(b)
A:dask.dataframe.tests.test_shuffle.actual->dask.dataframe.from_pandas(pdf, npartitions=2).set_index('ts', sorted=True)
A:dask.dataframe.tests.test_shuffle.order->list(reversed(string.ascii_letters))
A:dask.dataframe.tests.test_shuffle.values->list(string.ascii_letters)
A:dask.dataframe.tests.test_shuffle.dtype->pandas.ArrowDtype(pa.decimal128(10, 2))
A:dask.dataframe.tests.test_shuffle.divisions->d[d.a > 3].set_index('a', sorted=True).compute_current_divisions('key')
A:dask.dataframe.tests.test_shuffle.df['c']->numpy.arange(100, nelem + 100).astype(str)
A:dask.dataframe.tests.test_shuffle.large->pandas.DataFrame({'KEY': np.arange(0, 50000)})
A:dask.dataframe.tests.test_shuffle.small->pandas.DataFrame({'KEY': np.arange(25, 500)})
A:dask.dataframe.tests.test_shuffle.dd_left->dask.dataframe.from_pandas(small, npartitions=3)
A:dask.dataframe.tests.test_shuffle.dd_right->dask.dataframe.from_pandas(large, npartitions=257)
A:dask.dataframe.tests.test_shuffle.dd_merged->dask.dataframe.from_pandas(small, npartitions=3).merge(dd_right, how='inner', on='KEY')
A:dask.dataframe.tests.test_shuffle.count->itertools.count()
A:dask.dataframe.tests.test_shuffle.ntimes->next(count)
A:dask.dataframe.tests.test_shuffle.ddf_new_div->dask.dataframe.from_pandas(pdf, npartitions=2).set_index('A', divisions=divisions)
A:dask.dataframe.tests.test_shuffle.pdf_result->pandas.DataFrame({'a': 1, 'arrow_col': pd.Series(data, dtype=dtype)}).set_index('arrow_col')
A:dask.dataframe.tests.test_shuffle.df1->pandas.DataFrame([[True], [False]] * 50, columns=['A'])
A:dask.dataframe.tests.test_shuffle.df1['b']->(df1['a'] * 123).astype(str)
A:dask.dataframe.tests.test_shuffle.uncompressed_data->generate_raw_partd_file(compression=None)
A:dask.dataframe.tests.test_shuffle.compressed_data->generate_raw_partd_file(compression='BZ2')
A:dask.dataframe.tests.test_shuffle.df_in->dask.datasets.timeseries('2000', '2001', types={'value': float, 'name': str, 'id': int}, freq='2h', partition_freq=f'1{ME}', seed=1)
A:dask.dataframe.tests.test_shuffle.ext_on->df_in[on].copy()
A:dask.dataframe.tests.test_shuffle.df_out_1->dask.datasets.timeseries('2000', '2001', types={'value': float, 'name': str, 'id': int}, freq='2h', partition_freq=f'1{ME}', seed=1).shuffle(on, shuffle_method=shuffle_method, ignore_index=ignore_index, max_branch=max_branch)
A:dask.dataframe.tests.test_shuffle.df_out_2->dask.datasets.timeseries('2000', '2001', types={'value': float, 'name': str, 'id': int}, freq='2h', partition_freq=f'1{ME}', seed=1).shuffle(ext_on, shuffle_method=shuffle_method, ignore_index=ignore_index)
A:dask.dataframe.tests.test_shuffle.ddf1->dask.dataframe.from_pandas(df1, npartitions=10)
A:dask.dataframe.tests.test_shuffle.ddf_shuffled->dask.dataframe.from_pandas(pdf, npartitions=2).shuffle('a', max_branch=3, shuffle_method='tasks')
A:dask.dataframe.tests.test_shuffle.dsk->dask.dataframe.from_pandas(pdf, npartitions=2).__dask_graph__()
A:dask.dataframe.tests.test_shuffle.dsk_culled->dask.dataframe.from_pandas(pdf, npartitions=2).__dask_graph__().cull(set(keys))
A:dask.dataframe.tests.test_shuffle.dsk_dict->dict(dsk_culled)
A:dask.dataframe.tests.test_shuffle.(dsk_dict_culled, _)->cull(dsk_dict, keys)
A:dask.dataframe.tests.test_shuffle.layer_roundtrip->pickle.loads(pickle.dumps(layer))
A:dask.dataframe.tests.test_shuffle.day_index->dask.dataframe.from_pandas(pdf, npartitions=2).index.dt.floor('D')
A:dask.dataframe.tests.test_shuffle.day_df->dask.dataframe.from_pandas(pdf, npartitions=2).set_index(day_index)
A:dask.dataframe.tests.test_shuffle.one_day->pandas.Timedelta(days=1)
A:dask.dataframe.tests.test_shuffle.next_day_df->dask.dataframe.from_pandas(pdf, npartitions=2).set_index(ddf.index + one_day)
A:dask.dataframe.tests.test_shuffle.no_dates->dask.dataframe.from_pandas(pd.DataFrame(values), npartitions=3)
A:dask.dataframe.tests.test_shuffle.range_df->dask.dataframe.from_pandas(pdf, npartitions=2).set_index(no_dates.index)
A:dask.dataframe.tests.test_shuffle.df['a']->numpy.ascontiguousarray(np.arange(nelem)[::-1])
A:dask.dataframe.tests.test_shuffle.df['b']->numpy.arange(100, nelem + 100)
A:dask.dataframe.tests.test_shuffle.got->dask.dataframe.from_pandas(pdf, npartitions=2).sort_values(by=by[0], sort_function=f, sort_function_kwargs={'by_columns': by})
A:dask.dataframe.tests.test_shuffle.expect->pandas.DataFrame.from_records([[pd.Timestamp('2002-01-11 21:00:01+0000', tz='UTC'), 4223, 54719.0], [pd.Timestamp('2002-01-14 21:00:01+0000', tz='UTC'), 6942, 19223.0], [pd.Timestamp('2002-01-15 21:00:01+0000', tz='UTC'), 12551, 72865.0], [pd.Timestamp('2002-01-23 21:00:01+0000', tz='UTC'), 6005, 57670.0], [pd.Timestamp('2002-01-29 21:00:01+0000', tz='UTC'), 2043, 58600.0], [pd.Timestamp('2002-02-01 21:00:01+0000', tz='UTC'), 6909, 8459.0], [pd.Timestamp('2002-01-14 21:00:01+0000', tz='UTC'), 5326, 77339.0], [pd.Timestamp('2002-01-14 21:00:01+0000', tz='UTC'), 4711, 54135.0], [pd.Timestamp('2002-01-22 21:00:01+0000', tz='UTC'), 103, 57627.0], [pd.Timestamp('2002-01-30 21:00:01+0000', tz='UTC'), 16862, 54458.0], [pd.Timestamp('2002-01-31 21:00:01+0000', tz='UTC'), 4143, 56280.0]], columns=['time', 'id1', 'id2']).sort_values(by=by)
A:dask.dataframe.tests.test_shuffle.(divisions, mins, maxes, presorted)->_calculate_divisions(ddf, ddf['x'], False, 4)
A:dask.dataframe.tests.test_shuffle.df1['B']->list(range(100))
dask.dataframe.tests.test_shuffle._set_index(i,df,idx)
dask.dataframe.tests.test_shuffle.make_part(n)
dask.dataframe.tests.test_shuffle.mock_shuffle_group_3(df,col,npartitions,p)
dask.dataframe.tests.test_shuffle.test_calculate_divisions(pdf,expected)
dask.dataframe.tests.test_shuffle.test_compute_current_divisions_nan_partition()
dask.dataframe.tests.test_shuffle.test_compute_current_divisions_overlap()
dask.dataframe.tests.test_shuffle.test_compute_current_divisions_overlap_2()
dask.dataframe.tests.test_shuffle.test_compute_divisions()
dask.dataframe.tests.test_shuffle.test_dataframe_shuffle_on_arg(on,ignore_index,max_branch,shuffle_method)
dask.dataframe.tests.test_shuffle.test_default_partitions()
dask.dataframe.tests.test_shuffle.test_disk_shuffle_check_actual_compression()
dask.dataframe.tests.test_shuffle.test_disk_shuffle_with_compression_option(compression)
dask.dataframe.tests.test_shuffle.test_disk_shuffle_with_unknown_compression()
dask.dataframe.tests.test_shuffle.test_empty_partitions()
dask.dataframe.tests.test_shuffle.test_gh_2730()
dask.dataframe.tests.test_shuffle.test_index_with_dataframe(shuffle_method)
dask.dataframe.tests.test_shuffle.test_index_with_non_series(shuffle_method)
dask.dataframe.tests.test_shuffle.test_maybe_buffered_partd(tmp_path)
dask.dataframe.tests.test_shuffle.test_noop()
dask.dataframe.tests.test_shuffle.test_npartitions_auto_raises_deprecation_warning()
dask.dataframe.tests.test_shuffle.test_partitioning_index()
dask.dataframe.tests.test_shuffle.test_partitioning_index_categorical_on_values()
dask.dataframe.tests.test_shuffle.test_rearrange(shuffle_method,scheduler)
dask.dataframe.tests.test_shuffle.test_rearrange_by_column_with_narrow_divisions()
dask.dataframe.tests.test_shuffle.test_rearrange_cleanup()
dask.dataframe.tests.test_shuffle.test_rearrange_disk_cleanup_with_exception()
dask.dataframe.tests.test_shuffle.test_set_index(engine)
dask.dataframe.tests.test_shuffle.test_set_index_2(shuffle_method)
dask.dataframe.tests.test_shuffle.test_set_index_3(shuffle_method)
dask.dataframe.tests.test_shuffle.test_set_index_categorical()
dask.dataframe.tests.test_shuffle.test_set_index_consistent_divisions()
dask.dataframe.tests.test_shuffle.test_set_index_datetime_precision(unit)
dask.dataframe.tests.test_shuffle.test_set_index_deprecated_shuffle_keyword(shuffle_method)
dask.dataframe.tests.test_shuffle.test_set_index_detects_sorted_data(shuffle_method)
dask.dataframe.tests.test_shuffle.test_set_index_divisions_2()
dask.dataframe.tests.test_shuffle.test_set_index_divisions_compute()
dask.dataframe.tests.test_shuffle.test_set_index_divisions_sorted()
dask.dataframe.tests.test_shuffle.test_set_index_does_not_repeat_work_due_to_optimizations()
dask.dataframe.tests.test_shuffle.test_set_index_doesnt_increase_partitions(shuffle_method)
dask.dataframe.tests.test_shuffle.test_set_index_drop(drop)
dask.dataframe.tests.test_shuffle.test_set_index_ea_dtype()
dask.dataframe.tests.test_shuffle.test_set_index_empty_partition()
dask.dataframe.tests.test_shuffle.test_set_index_errors_with_inplace_kwarg()
dask.dataframe.tests.test_shuffle.test_set_index_general(npartitions,shuffle_method)
dask.dataframe.tests.test_shuffle.test_set_index_interpolate(engine)
dask.dataframe.tests.test_shuffle.test_set_index_interpolate_int(engine)
dask.dataframe.tests.test_shuffle.test_set_index_interpolate_large_uint(engine)
dask.dataframe.tests.test_shuffle.test_set_index_names(shuffle_method)
dask.dataframe.tests.test_shuffle.test_set_index_nan_partition()
dask.dataframe.tests.test_shuffle.test_set_index_no_sort(drop,append)
dask.dataframe.tests.test_shuffle.test_set_index_npartitions()
dask.dataframe.tests.test_shuffle.test_set_index_on_empty(converter)
dask.dataframe.tests.test_shuffle.test_set_index_overlap()
dask.dataframe.tests.test_shuffle.test_set_index_overlap_2()
dask.dataframe.tests.test_shuffle.test_set_index_overlap_does_not_drop_rows_when_divisions_overlap()
dask.dataframe.tests.test_shuffle.test_set_index_partitions_meta_dtype()
dask.dataframe.tests.test_shuffle.test_set_index_pyarrow_dtype(data,dtype)
dask.dataframe.tests.test_shuffle.test_set_index_raises_error_on_bad_input()
dask.dataframe.tests.test_shuffle.test_set_index_self_index(shuffle_method)
dask.dataframe.tests.test_shuffle.test_set_index_sorted_min_max_same()
dask.dataframe.tests.test_shuffle.test_set_index_sorted_single_partition()
dask.dataframe.tests.test_shuffle.test_set_index_sorted_true()
dask.dataframe.tests.test_shuffle.test_set_index_sorts()
dask.dataframe.tests.test_shuffle.test_set_index_string(shuffle_method,string_dtype)
dask.dataframe.tests.test_shuffle.test_set_index_timestamp()
dask.dataframe.tests.test_shuffle.test_set_index_timezone()
dask.dataframe.tests.test_shuffle.test_set_index_with_dask_dt_index()
dask.dataframe.tests.test_shuffle.test_set_index_with_empty_and_overlap()
dask.dataframe.tests.test_shuffle.test_set_index_with_empty_divisions()
dask.dataframe.tests.test_shuffle.test_set_index_with_explicit_divisions()
dask.dataframe.tests.test_shuffle.test_set_index_with_series_uses_fastpath()
dask.dataframe.tests.test_shuffle.test_shuffle(shuffle_method)
dask.dataframe.tests.test_shuffle.test_shuffle_by_as_list()
dask.dataframe.tests.test_shuffle.test_shuffle_deprecated_shuffle_keyword(shuffle_method)
dask.dataframe.tests.test_shuffle.test_shuffle_empty_partitions(shuffle_method)
dask.dataframe.tests.test_shuffle.test_shuffle_from_one_partition_to_one_other(shuffle_method)
dask.dataframe.tests.test_shuffle.test_shuffle_hlg_layer()
dask.dataframe.tests.test_shuffle.test_shuffle_hlg_layer_serialize(npartitions)
dask.dataframe.tests.test_shuffle.test_shuffle_npartitions(shuffle_method)
dask.dataframe.tests.test_shuffle.test_shuffle_npartitions_lt_input_partitions(shuffle_method)
dask.dataframe.tests.test_shuffle.test_shuffle_nulls_introduced()
dask.dataframe.tests.test_shuffle.test_shuffle_partitions_meta_dtype()
dask.dataframe.tests.test_shuffle.test_shuffle_sort(shuffle_method)
dask.dataframe.tests.test_shuffle.test_shuffle_values_raises()
dask.dataframe.tests.test_shuffle.test_sort_values(nelem,by,ascending)
dask.dataframe.tests.test_shuffle.test_sort_values_bool_ascending()
dask.dataframe.tests.test_shuffle.test_sort_values_custom_function(by,nparts)
dask.dataframe.tests.test_shuffle.test_sort_values_deprecated_shuffle_keyword(shuffle_method)
dask.dataframe.tests.test_shuffle.test_sort_values_partitions_meta_dtype_with_divisions()
dask.dataframe.tests.test_shuffle.test_sort_values_single_partition(nelem,by,ascending)
dask.dataframe.tests.test_shuffle.test_sort_values_tasks_backend(backend,by,ascending)
dask.dataframe.tests.test_shuffle.test_sort_values_timestamp(npartitions)
dask.dataframe.tests.test_shuffle.test_sort_values_with_nulls(data,nparts,by,ascending,na_position)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/tests/test_format.py----------------------------------------
A:dask.dataframe.tests.test_format.df->pandas.DataFrame()
A:dask.dataframe.tests.test_format.ddf->dask.dataframe.from_pandas(df, npartitions=1)
A:dask.dataframe.tests.test_format.s1->repr(ddf)
A:dask.dataframe.tests.test_format.ddf.b->dask.dataframe.from_pandas(df, npartitions=1).b.astype('category')
A:dask.dataframe.tests.test_format.string_dtype->_format_string_dtype()
A:dask.dataframe.tests.test_format.footer->_format_footer('-index', 2)
A:dask.dataframe.tests.test_format.exp->'<div><strong>Dask DataFrame Structure:</strong></div>\n<div>\n{style}{exp_table}\n</div>\n<div>Dask Name: from_pandas, 1 graph layer</div>'.format(style=style, exp_table=exp_table)
A:dask.dataframe.tests.test_format.s->pandas.Series(['a', 'b', 'c']).astype('category')
A:dask.dataframe.tests.test_format.ds->dask.dataframe.from_pandas(s, 3)
A:dask.dataframe.tests.test_format.known->dask.dataframe.from_pandas(s, npartitions=1)
A:dask.dataframe.tests.test_format.unknown->dask.dataframe.from_pandas(s, npartitions=1).cat.as_unknown()
A:dask.dataframe.tests.test_format.arr->dask.array.from_array(np.arange(10).reshape(5, 2), chunks=(5, 2))
A:dask.dataframe.tests.test_format.frame->dask.dataframe.from_dask_array(arr, columns=['a', 'a'])
dask.dataframe.tests.test_format._format_footer(suffix='',layers=1)
dask.dataframe.tests.test_format._format_string_dtype()
dask.dataframe.tests.test_format.test_categorical_format()
dask.dataframe.tests.test_format.test_dataframe_format()
dask.dataframe.tests.test_format.test_dataframe_format_long()
dask.dataframe.tests.test_format.test_dataframe_format_unknown_divisions()
dask.dataframe.tests.test_format.test_dataframe_format_with_index()
dask.dataframe.tests.test_format.test_duplicate_columns_repr()
dask.dataframe.tests.test_format.test_empty_repr()
dask.dataframe.tests.test_format.test_index_format()
dask.dataframe.tests.test_format.test_repr()
dask.dataframe.tests.test_format.test_repr_meta_mutation()
dask.dataframe.tests.test_format.test_series_format()
dask.dataframe.tests.test_format.test_series_format_long()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/tests/test_extensions.py----------------------------------------
A:dask.dataframe.tests.test_extensions.pd->pytest.importorskip('pandas')
A:dask.dataframe.tests.test_extensions.arr->pandas.tests.extension.decimal.array.DecimalArray._from_sequence([Decimal('1.0')] * 10)
A:dask.dataframe.tests.test_extensions.ser->pytest.importorskip('pandas').Series(DecimalArray._from_sequence([Decimal('0'), Decimal('1')]))
A:dask.dataframe.tests.test_extensions.dser->dask.dataframe.from_pandas(ser, 2)
A:dask.dataframe.tests.test_extensions.df->pytest.importorskip('pandas').DataFrame({'A': ser})
A:dask.dataframe.tests.test_extensions.ddf->dask.dataframe.from_pandas(df, 2)
A:dask.dataframe.tests.test_extensions.result->dask.dataframe.utils.make_meta(Decimal('1.0'), parent_meta=pd.DataFrame())
dask.dataframe.tests.test_extensions._(dtype)
dask.dataframe.tests.test_extensions._(x)
dask.dataframe.tests.test_extensions.test_reduction()
dask.dataframe.tests.test_extensions.test_register_extension_type()
dask.dataframe.tests.test_extensions.test_scalar()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/tests/test_numeric.py----------------------------------------
A:dask.dataframe.tests.test_numeric.output->to_numeric(arg, meta=pd.Series([], dtype='float64'))
A:dask.dataframe.tests.test_numeric.arg->from_pandas(df, npartitions=2)
A:dask.dataframe.tests.test_numeric.expected->pandas.to_numeric(s)
A:dask.dataframe.tests.test_numeric.s->pandas.Series(['1.0', '2', -3, -5.1])
A:dask.dataframe.tests.test_numeric.df->pandas.DataFrame({'a': s, 'b': s})
dask.dataframe.tests.test_numeric.test_to_numeric_on_dask_array()
dask.dataframe.tests.test_numeric.test_to_numeric_on_dask_dataframe_dataframe_raises_error()
dask.dataframe.tests.test_numeric.test_to_numeric_on_dask_dataframe_series()
dask.dataframe.tests.test_numeric.test_to_numeric_on_dask_dataframe_series_with_meta()
dask.dataframe.tests.test_numeric.test_to_numeric_on_scalars(arg)
dask.dataframe.tests.test_numeric.test_to_numeric_raises()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/tests/test_utils_dataframe.py----------------------------------------
A:dask.dataframe.tests.test_utils_dataframe.df->pandas.DataFrame({'x': [1, 2, 3, 4]})
A:dask.dataframe.tests.test_utils_dataframe.result->list(shard_df_on_index(df, [20, 50]))
A:dask.dataframe.tests.test_utils_dataframe.meta->pandas.DataFrame(columns=['A', 'D'], index=[0])
A:dask.dataframe.tests.test_utils_dataframe.ddf->dask.dataframe.from_pandas(df, npartitions=2)
A:dask.dataframe.tests.test_utils_dataframe.idx->pandas.Index([1], name='foo', dtype='uint64')
A:dask.dataframe.tests.test_utils_dataframe.x->pandas.DatetimeTZDtype(tz='UTC')
A:dask.dataframe.tests.test_utils_dataframe.df1->pandas.DataFrame({'A': pd.Categorical(['Alice', 'Bob', 'Carol']), 'B': list('abc'), 'C': 'bar', 'D': np.float32(1), 'E': np.int32(1), 'F': pd.Timestamp('2016-01-01'), 'G': pd.date_range('2016-01-01', periods=3, tz='America/New_York'), 'H': pd.Timedelta('1 hours'), 'I': np.void(b' '), 'J': pd.Categorical([UNKNOWN_CATEGORIES] * 3), 'K': pd.Categorical([None, None, None])}, columns=list('DCBAHGFEIJK'))
A:dask.dataframe.tests.test_utils_dataframe.df3->meta_nonempty(df2)
A:dask.dataframe.tests.test_utils_dataframe.s->pandas.Index([1], name='foo', dtype='uint64').to_series()
A:dask.dataframe.tests.test_utils_dataframe.res->meta_nonempty(idx)
A:dask.dataframe.tests.test_utils_dataframe.exp->pandas.DataFrame([['foo', 'foo', 'foo'], ['foo', 'foo', 'foo']], index=meta_nonempty(df.index), columns=['A', 'A', 'B'])
A:dask.dataframe.tests.test_utils_dataframe.wrap->IndexWrapper()
A:dask.dataframe.tests.test_utils_dataframe.ser->pandas.Series([], dtype='Float64')
A:dask.dataframe.tests.test_utils_dataframe.non_empty->meta_nonempty(ser)
A:dask.dataframe.tests.test_utils_dataframe.df_s->pandas.DataFrame({'x': [1, 2, 3, 4]}).sort_values('B')
A:dask.dataframe.tests.test_utils_dataframe.df_sr->pandas.DataFrame({'x': [1, 2, 3, 4]}).sort_values('B').reset_index(drop=True)
A:dask.dataframe.tests.test_utils_dataframe.ddf_s->dask.dataframe.from_pandas(df, npartitions=2).sort_values(['B'])
A:dask.dataframe.tests.test_utils_dataframe.ddf_sr->dask.dataframe.from_pandas(df, npartitions=2).sort_values(['B']).reset_index(drop=True)
A:dask.dataframe.tests.test_utils_dataframe.ddf2->dask.dataframe.from_pandas(df, npartitions=2).map_partitions(check_custom_scheduler, meta=ddf)
dask.dataframe.tests.test_utils_dataframe.test_apply_and_enforce_message()
dask.dataframe.tests.test_utils_dataframe.test_assert_eq_scheduler()
dask.dataframe.tests.test_utils_dataframe.test_assert_eq_sorts()
dask.dataframe.tests.test_utils_dataframe.test_check_matching_columns_raises_appropriate_errors()
dask.dataframe.tests.test_utils_dataframe.test_check_meta()
dask.dataframe.tests.test_utils_dataframe.test_check_meta_typename()
dask.dataframe.tests.test_utils_dataframe.test_is_dataframe_like(monkeypatch,frame_value_counts)
dask.dataframe.tests.test_utils_dataframe.test_make_meta()
dask.dataframe.tests.test_utils_dataframe.test_meta_constructor_utilities(data)
dask.dataframe.tests.test_utils_dataframe.test_meta_constructor_utilities_raise(data)
dask.dataframe.tests.test_utils_dataframe.test_meta_duplicated()
dask.dataframe.tests.test_utils_dataframe.test_meta_nonempty()
dask.dataframe.tests.test_utils_dataframe.test_meta_nonempty_empty_categories()
dask.dataframe.tests.test_utils_dataframe.test_meta_nonempty_index()
dask.dataframe.tests.test_utils_dataframe.test_meta_nonempty_scalar()
dask.dataframe.tests.test_utils_dataframe.test_meta_nonempty_uint64index()
dask.dataframe.tests.test_utils_dataframe.test_nonempty_series_nullable_float()
dask.dataframe.tests.test_utils_dataframe.test_nonempty_series_sparse()
dask.dataframe.tests.test_utils_dataframe.test_pyarrow_strings_enabled()
dask.dataframe.tests.test_utils_dataframe.test_raise_on_meta_error()
dask.dataframe.tests.test_utils_dataframe.test_shard_df_on_index()
dask.dataframe.tests.test_utils_dataframe.test_valid_divisions(divisions,valid)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/tests/test_indexing.py----------------------------------------
A:dask.dataframe.tests.test_indexing.meta->make_meta({'a': 'i8', 'b': 'i8'}, index=pd.Index([], 'i8'), parent_meta=pd.DataFrame())
A:dask.dataframe.tests.test_indexing.d->dask.dataframe.repartition(pd.concat(dsk.values()), divisions=[0, 5, 9, 9])
A:dask.dataframe.tests.test_indexing.full->dask.dataframe.repartition(pd.concat(dsk.values()), divisions=[0, 5, 9, 9]).compute()
A:dask.dataframe.tests.test_indexing.df->pytest.importorskip('cudf').DataFrame({'a': range(8), 'index': index}).set_index('index')
A:dask.dataframe.tests.test_indexing.ddf->dask.dataframe.from_pandas(df, npartitions=3)
A:dask.dataframe.tests.test_indexing.s->pandas.Series([1, 2, 3])
A:dask.dataframe.tests.test_indexing.ds->dask.dataframe.from_pandas(s, 2)
A:dask.dataframe.tests.test_indexing.ctx->pytest.warns(FutureWarning, match='float-dtype index')
A:dask.dataframe.tests.test_indexing.a->dask.dataframe.from_pandas(df, 2)
A:dask.dataframe.tests.test_indexing.a.divisions->tuple(map(pd.Timestamp, a.divisions))
A:dask.dataframe.tests.test_indexing.datetime_index->pandas.date_range('2016-01-01', '2016-01-31', freq='12h')
A:dask.dataframe.tests.test_indexing.slice_->slice('2016-01-03', '2016-01-05')
A:dask.dataframe.tests.test_indexing.expected->pytest.importorskip('cudf').DataFrame({'a': range(8), 'index': index}).set_index('index').index.to_frame()
A:dask.dataframe.tests.test_indexing.actual->dask.dataframe.from_pandas(df, npartitions=3).index.to_frame()
A:dask.dataframe.tests.test_indexing.(a1, b1, c1)->dask.compute(a, b, c)
A:dask.dataframe.tests.test_indexing.s1->pandas.Series([0, 1, 2])
A:dask.dataframe.tests.test_indexing.s2->pandas.Series([True, False, pd.NA], dtype='boolean')
A:dask.dataframe.tests.test_indexing.ddf1->dask.dataframe.from_pandas(s1, npartitions=1)
A:dask.dataframe.tests.test_indexing.ddf2->dask.dataframe.from_pandas(s2, npartitions=1)
A:dask.dataframe.tests.test_indexing.obj->pandas.DataFrame([[0, 1, 2, 3], [4, 5, 6, 7]], columns=['a', 'b', 'c', 'c'])
A:dask.dataframe.tests.test_indexing.dask_df->dask.dataframe.from_pandas(obj, npartitions=1)
A:dask.dataframe.tests.test_indexing.cudf->pytest.importorskip('cudf')
A:dask.dataframe.tests.test_indexing.cupy->pytest.importorskip('cupy')
A:dask.dataframe.tests.test_indexing.cdf_index->pytest.importorskip('cudf').Series([1, 100, 300])
A:dask.dataframe.tests.test_indexing.cupy_index->pytest.importorskip('cupy').array([1, 100, 300])
dask.dataframe.tests.test_indexing.test_coerce_loc_index()
dask.dataframe.tests.test_indexing.test_deterministic_hashing_dataframe()
dask.dataframe.tests.test_indexing.test_deterministic_hashing_series()
dask.dataframe.tests.test_indexing.test_getitem()
dask.dataframe.tests.test_indexing.test_getitem_integer_slice()
dask.dataframe.tests.test_indexing.test_getitem_period_str()
dask.dataframe.tests.test_indexing.test_getitem_slice()
dask.dataframe.tests.test_indexing.test_getitem_timestamp_str()
dask.dataframe.tests.test_indexing.test_gpu_loc()
dask.dataframe.tests.test_indexing.test_iloc(indexer)
dask.dataframe.tests.test_indexing.test_iloc_dispatch_to_getitem()
dask.dataframe.tests.test_indexing.test_iloc_duplicate_columns()
dask.dataframe.tests.test_indexing.test_iloc_out_of_order_selection()
dask.dataframe.tests.test_indexing.test_iloc_raises()
dask.dataframe.tests.test_indexing.test_iloc_series()
dask.dataframe.tests.test_indexing.test_loc()
dask.dataframe.tests.test_indexing.test_loc2d()
dask.dataframe.tests.test_indexing.test_loc2d_duplicated_columns()
dask.dataframe.tests.test_indexing.test_loc2d_with_known_divisions()
dask.dataframe.tests.test_indexing.test_loc2d_with_unknown_divisions()
dask.dataframe.tests.test_indexing.test_loc_datetime_no_freq()
dask.dataframe.tests.test_indexing.test_loc_non_informative_index()
dask.dataframe.tests.test_indexing.test_loc_on_numpy_datetimes()
dask.dataframe.tests.test_indexing.test_loc_on_pandas_datetimes()
dask.dataframe.tests.test_indexing.test_loc_period_str()
dask.dataframe.tests.test_indexing.test_loc_timestamp_str()
dask.dataframe.tests.test_indexing.test_loc_with_array()
dask.dataframe.tests.test_indexing.test_loc_with_array_different_partition()
dask.dataframe.tests.test_indexing.test_loc_with_function()
dask.dataframe.tests.test_indexing.test_loc_with_non_boolean_series()
dask.dataframe.tests.test_indexing.test_loc_with_series()
dask.dataframe.tests.test_indexing.test_loc_with_series_different_partition()
dask.dataframe.tests.test_indexing.test_loc_with_text_dates()
dask.dataframe.tests.test_indexing.test_pandas_nullable_boolean_data_type()
dask.dataframe.tests.test_indexing.test_to_frame(index)
dask.dataframe.tests.test_indexing.test_to_series(index)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/tests/test_hyperloglog.py----------------------------------------
A:dask.dataframe.tests.test_hyperloglog.rs->numpy.random.RandomState(96)
A:dask.dataframe.tests.test_hyperloglog.ddf->dask.dataframe.from_pandas(df, npartitions=npartitions)
A:dask.dataframe.tests.test_hyperloglog.approx->dask.dataframe.from_pandas(df, npartitions=npartitions).nunique_approx(split_every=split_every).compute(scheduler='sync')
A:dask.dataframe.tests.test_hyperloglog.exact->len(df.drop_duplicates())
A:dask.dataframe.tests.test_hyperloglog.df->dask.dataframe.demo.make_timeseries('2000-01-01', '2000-04-01', {'value': float, 'id': int}, freq='10s', partition_freq='1D', seed=1)
dask.dataframe.tests.test_hyperloglog.test_basic(df,npartitions)
dask.dataframe.tests.test_hyperloglog.test_larger_data()
dask.dataframe.tests.test_hyperloglog.test_split_every(split_every,npartitions)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/tests/test_multi.py----------------------------------------
A:dask.dataframe.tests.test_multi.DASK_EXPR_ENABLED->dask.dataframe._dask_expr_enabled()
A:dask.dataframe.tests.test_multi.A->pandas.DataFrame({'x': [1, 2, 3, 4, 5, 6], 'y': [1, 1, 2, 2, 3, 4]})
A:dask.dataframe.tests.test_multi.a->pandas.DataFrame({'A': [0, 1, 2, 3], 'B': [4, 5, 6, 7]})
A:dask.dataframe.tests.test_multi.B->pandas.DataFrame({'y': [1, 3, 4, 4, 5, 6], 'z': [6, 5, 4, 3, 2, 1]})
A:dask.dataframe.tests.test_multi.b->pandas.DataFrame({'A': [0, 1, 2, 4], 'C': [4, 5, 7, 7]})
A:dask.dataframe.tests.test_multi.s->pandas.Series([7, 8], name=6, index=['a', 'b'])
A:dask.dataframe.tests.test_multi.((aa, bb), divisions, L)->align_partitions(a, b)
A:dask.dataframe.tests.test_multi.((aa, ss, bb), divisions, L)->align_partitions(a, s, b)
A:dask.dataframe.tests.test_multi.ldf->pandas.DataFrame({'a': [1, 2, 3, 4, 5, 6, 7], 'b': [7, 6, 5, 4, 3, 2, 1]}, index=list('abcdefg'))
A:dask.dataframe.tests.test_multi.rdf->pandas.DataFrame({'c': [1, 2, 3, 4, 5, 6, 7], 'd': [7, 6, 5, 4, 3, 2, 1]}, index=list('fghijkl'))
A:dask.dataframe.tests.test_multi.((lresult, rresult), div, parts)->align_partitions(lhs, rhs)
A:dask.dataframe.tests.test_multi.df->pandas.DataFrame({f'{i}A': [5, 6, 7, 8], f'{i}B': [4, 3, 2, 1]}, index=[0, 1, 2, 3])
A:dask.dataframe.tests.test_multi.ddf->dask.dataframe.from_pandas(df, npartitions_other)
A:dask.dataframe.tests.test_multi.ddf2->dask.dataframe.from_pandas(df2, npartitions=2)
A:dask.dataframe.tests.test_multi.(a, b)->_maybe_align_partitions([ddf, ddf2])
A:dask.dataframe.tests.test_multi.c->dask.dataframe.DataFrame(dsk, 'y', meta, [None, None])
A:dask.dataframe.tests.test_multi.result->dask.dataframe.merge(left, right, broadcast=True, shuffle_method=shuffle)
A:dask.dataframe.tests.test_multi.expected->pandas.merge(df2, df1, left_index=True, right_on='A')
A:dask.dataframe.tests.test_multi.df1->pandas.DataFrame(data={'A': ['a', 'b', 'c']}, index=['s', 'v', 'w'])
A:dask.dataframe.tests.test_multi.df2->df2.astype({'B': 'category'}).astype({'B': 'category'})
A:dask.dataframe.tests.test_multi.df3->dask.dataframe.DataFrame(dsk, 'y', meta, [None, None]).compute()
A:dask.dataframe.tests.test_multi.join_pd->pandas.DataFrame(data={'A': ['a', 'b', 'c']}, index=['s', 'v', 'w']).join(df2, how='inner', lsuffix='_l', rsuffix='_r')
A:dask.dataframe.tests.test_multi.multi_join_pd->pandas.DataFrame(data={'A': ['a', 'b', 'c']}, index=['s', 'v', 'w']).join(df2, how='inner', lsuffix='_l', rsuffix='_r').join(df3, how='inner', lsuffix='_l', rsuffix='_r')
A:dask.dataframe.tests.test_multi.ddf1->dask.dataframe.from_pandas(df1, npartitions=1)
A:dask.dataframe.tests.test_multi.ddf3->dask.dataframe.DataFrame(dsk, 'y', meta, [None, None])
A:dask.dataframe.tests.test_multi.join_dd->dask.dataframe.from_pandas(df1, npartitions=1).join(ddf2, how='inner', lsuffix='_l', rsuffix='_r')
A:dask.dataframe.tests.test_multi.multi_join_dd->dask.dataframe.from_pandas(df1, npartitions=1).join(ddf2, how='inner', lsuffix='_l', rsuffix='_r').join(ddf3, how='inner', lsuffix='_l', rsuffix='_r')
A:dask.dataframe.tests.test_multi.C->pandas.merge_asof(B, A, on='time', by='ticker', tolerance=pd.Timedelta('10ms'), allow_exact_matches=False)
A:dask.dataframe.tests.test_multi.good_df->pandas.DataFrame({'index_col': list(range(10)), 'good_val': list(range(10))})
A:dask.dataframe.tests.test_multi.good_dd->dask.dataframe.from_pandas(good_df, npartitions=2)
A:dask.dataframe.tests.test_multi.empty_df->dask.dataframe.from_pandas(pd.DataFrame(), npartitions=10)
A:dask.dataframe.tests.test_multi.empty_dd->dask.dataframe.from_pandas(empty_df, npartitions=2)
A:dask.dataframe.tests.test_multi.result_dd->dask.dataframe.merge_asof(dd.from_pandas(df1, npartitions=2), dd.from_pandas(df2, npartitions=2), left_on=left_col, right_on=right_col)
A:dask.dataframe.tests.test_multi.result_df->pandas.merge_asof(df1, df2, left_on=left_col, right_on=right_col)
A:dask.dataframe.tests.test_multi.pdf1->pandas.DataFrame({'x': pd.Categorical(['a', 'b', 'c', 'a'], categories=['a', 'b', 'c'], ordered=ordered)})
A:dask.dataframe.tests.test_multi.pdf2->pandas.DataFrame({'x': pd.Categorical(['c', 'b', 'a'], categories=['c', 'b', 'a'], ordered=ordered)})
A:dask.dataframe.tests.test_multi.pdf3->pandas.DataFrame(np.random.randn(7, 6), columns=list('FGHIJK'), index=list('cdefghi'))
A:dask.dataframe.tests.test_multi.pdf2['x']->range(len(pdf2['y']))
A:dask.dataframe.tests.test_multi.df_concat->pandas.concat([df, empty_df])
A:dask.dataframe.tests.test_multi.empty_ddf->dask.dataframe.from_pandas(empty_df, npartitions=1)
A:dask.dataframe.tests.test_multi.ddf_concat->dask.dataframe.concat([ddf, empty_ddf])
A:dask.dataframe.tests.test_multi.empty_df_with_col->pandas.DataFrame([], columns=['x'], dtype='int64')
A:dask.dataframe.tests.test_multi.df_concat_with_col->pandas.concat([df, empty_df_with_col])
A:dask.dataframe.tests.test_multi.empty_ddf_with_col->dask.dataframe.from_pandas(empty_df_with_col, npartitions=1)
A:dask.dataframe.tests.test_multi.ddf_concat_with_col->dask.dataframe.concat([ddf, empty_ddf_with_col])
A:dask.dataframe.tests.test_multi.ddf_concat_with_col._meta.index->dask.dataframe.concat([ddf, empty_ddf_with_col])._meta.index.astype('int64')
A:dask.dataframe.tests.test_multi.df_1->pandas.DataFrame({'x': [value_1]})
A:dask.dataframe.tests.test_multi.df_2->pandas.DataFrame({'x': [value_2]})
A:dask.dataframe.tests.test_multi.ddf_1->dask.dataframe.from_pandas(df_1, npartitions=1)
A:dask.dataframe.tests.test_multi.ddf_2->dask.dataframe.from_pandas(df_2, npartitions=1)
A:dask.dataframe.tests.test_multi.dask_dtypes->list(ddf.map_partitions(lambda x: x.dtypes).compute())
A:dask.dataframe.tests.test_multi.warned->any(('merge column data type mismatches' in str(r) for r in record))
A:dask.dataframe.tests.test_multi.has_nans->dask.dataframe.merge(left, right, broadcast=True, shuffle_method=shuffle).isna().values.any()
A:dask.dataframe.tests.test_multi.left->dask.dataframe.from_dict({'a': [1, 2] * 80, 'b_left': range(160)}, npartitions=16)
A:dask.dataframe.tests.test_multi.right->dask.dataframe.from_dict({'a': [2, 1] * 10, 'b_right': range(20)}, npartitions=2)
A:dask.dataframe.tests.test_multi.dd_left->dask.dataframe.from_pandas(left, npartitions=4)
A:dask.dataframe.tests.test_multi.dd_right->dask.dataframe.from_pandas(right, npartitions=4)
A:dask.dataframe.tests.test_multi.merged->dask.dataframe.merge(ddf, rhs, on='B').compute()
A:dask.dataframe.tests.test_multi.cudf->pytest.importorskip('cudf')
A:dask.dataframe.tests.test_multi.dask_cudf->pytest.importorskip('dask_cudf')
A:dask.dataframe.tests.test_multi.emp->pytest.importorskip('cudf').from_pandas(emp)
A:dask.dataframe.tests.test_multi.skills->pytest.importorskip('cudf').from_pandas(skills)
A:dask.dataframe.tests.test_multi.dd_emp->dask.dataframe.from_pandas(emp, npartitions=parts[0])
A:dask.dataframe.tests.test_multi.dd_skills->dask.dataframe.from_pandas(skills, npartitions=parts[1])
A:dask.dataframe.tests.test_multi.expect->pandas.merge(df1, df2, on='a')
A:dask.dataframe.tests.test_multi.aa->dask.dataframe.from_pandas(a, npartitions=2, sort=False)
A:dask.dataframe.tests.test_multi.bb->dask.dataframe.from_pandas(b, npartitions=2, sort=False)
A:dask.dataframe.tests.test_multi.cc->dask.dataframe.from_pandas(b, npartitions=1, sort=False)
A:dask.dataframe.tests.test_multi.pdf1l->pandas.DataFrame({'a': list('abcdefghij'), 'b': list('abcdefghij'), 'c': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}, index=list('abcdefghij'))
A:dask.dataframe.tests.test_multi.pdf1r->pandas.DataFrame({'d': list('abcdefghij'), 'e': list('abcdefghij'), 'f': [10, 9, 8, 7, 6, 5, 4, 3, 2, 1]}, index=list('abcdefghij'))
A:dask.dataframe.tests.test_multi.pdf2l->pandas.DataFrame({'a': list('abcdeabcde'), 'b': list('abcabcabca'), 'c': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}, index=list('abcdefghij'))
A:dask.dataframe.tests.test_multi.pdf2r->pandas.DataFrame({'d': list('edcbaedcba'), 'e': list('aaabbbcccd'), 'f': [10, 9, 8, 7, 6, 5, 4, 3, 2, 1]}, index=list('fghijklmno'))
A:dask.dataframe.tests.test_multi.pdf3r->pandas.DataFrame({'d': list('aaabbbccaa'), 'e': list('abbbbbbbbb'), 'f': [10, 9, 8, 7, 6, 5, 4, 3, 2, 1]}, index=list('ABCDEFGHIJ'))
A:dask.dataframe.tests.test_multi.pdf4r->pandas.DataFrame({'c': list('abda'), 'd': [5, 4, 3, 2]}, index=list('abdg'))
A:dask.dataframe.tests.test_multi.pdf5l->pandas.DataFrame({'a': list('lmnopqr'), 'b': [7, 6, 5, 4, 3, 2, 1]}, index=list('lmnopqr'))
A:dask.dataframe.tests.test_multi.pdf5r->pandas.DataFrame({'c': list('abcd'), 'd': [5, 4, 3, 2]}, index=list('abcd'))
A:dask.dataframe.tests.test_multi.pdf6l->pandas.DataFrame({'a': list('cdefghi'), 'b': [7, 6, 5, 4, 3, 2, 1]}, index=list('cdefghi'))
A:dask.dataframe.tests.test_multi.pdf6r->pandas.DataFrame({'c': list('abab'), 'd': [5, 4, 3, 2]}, index=list('abcd'))
A:dask.dataframe.tests.test_multi.pdf7l->pandas.DataFrame({'a': list('aabbccd'), 'b': [7, 6, 5, 4, 3, 2, 1]}, index=list('abcdefg'))
A:dask.dataframe.tests.test_multi.pdf7r->pandas.DataFrame({'c': list('aabb'), 'd': [5, 4, 3, 2]}, index=list('fghi'))
A:dask.dataframe.tests.test_multi.ddl->dask.dataframe.from_pandas(pdl, lpart)
A:dask.dataframe.tests.test_multi.ddr->dask.dataframe.from_pandas(pdr, rpart)
A:dask.dataframe.tests.test_multi.pdf4l->pandas.DataFrame({'a': list('abcabce'), 'b': [7, 6, 5, 4, 3, 2, 1]}, index=list('abcdefg'))
A:dask.dataframe.tests.test_multi.right_df->pandas.DataFrame({'b': [1.0, 2.0, 3.0]}, index=['a', 'b', 'c'])
A:dask.dataframe.tests.test_multi.actual->df1.merge(df2, on='A').compute()
A:dask.dataframe.tests.test_multi.pdf3l->pandas.DataFrame({'a': list('aaaaaaaaaa'), 'b': list('aaaaaaaaaa'), 'c': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}, index=list('abcdefghij'))
A:dask.dataframe.tests.test_multi.pdf->pdf.astype({'s1': 'string[pyarrow]', 's2': 'string[pyarrow]'}).astype({'s1': 'string[pyarrow]', 's2': 'string[pyarrow]'})
A:dask.dataframe.tests.test_multi.da->dask.dataframe.from_pandas(a, 2).cat.as_unknown().to_frame('A')
A:dask.dataframe.tests.test_multi.dc->dask.dataframe.from_pandas(a, 2).cat.as_unknown().to_frame('A').merge(b, on='x', how='inner')
A:dask.dataframe.tests.test_multi.output_layers->dask.dataframe.from_pandas(b, npartitions=1, sort=False).dask.layers.keys()
A:dask.dataframe.tests.test_multi.expected.index->pandas.merge(df2, df1, left_index=True, right_on='A').index.astype('int64')
A:dask.dataframe.tests.test_multi.lhs->pandas.DataFrame({'A': [1, 2, 3], 'B': list('abc'), 'C': 'foo', 'D': 1.0}, columns=list('DCBA'))
A:dask.dataframe.tests.test_multi.rhs->pandas.DataFrame({'G': [4, 5], 'H': 6.0, 'I': 'bar', 'B': list('ab')}, columns=list('GHIB'))
A:dask.dataframe.tests.test_multi.meta->make_meta({'b': 'i8', 'c': 'i8'}, parent_meta=pd.DataFrame())
A:dask.dataframe.tests.test_multi.d->dask.dataframe.DataFrame(dsk, 'y', meta, [0, 3, 5])
A:dask.dataframe.tests.test_multi.pdf4->pandas.DataFrame(np.random.randn(7, 5), columns=list('FGHAB'), index=list('cdefghi'))
A:dask.dataframe.tests.test_multi.pdf5->pandas.DataFrame(np.random.randn(7, 5), columns=list('FGHAB'), index=list('fklmnop'))
A:dask.dataframe.tests.test_multi.ddf4->dask.dataframe.from_pandas(pdf4, 2)
A:dask.dataframe.tests.test_multi.ddf5->dask.dataframe.from_pandas(pdf5, 3)
A:dask.dataframe.tests.test_multi.df.w->pandas.DataFrame({f'{i}A': [5, 6, 7, 8], f'{i}B': [4, 3, 2, 1]}, index=[0, 1, 2, 3]).w.astype('category')
A:dask.dataframe.tests.test_multi.df.y->pandas.DataFrame({f'{i}A': [5, 6, 7, 8], f'{i}B': [4, 3, 2, 1]}, index=[0, 1, 2, 3]).y.astype('category')
A:dask.dataframe.tests.test_multi.dframes[0]['y']->dframes[0]['y'].cat.as_unknown().cat.as_unknown()
A:dask.dataframe.tests.test_multi.dframes[0].index->dframes[0].index.cat.as_unknown()
A:dask.dataframe.tests.test_multi.dframes[0]._meta->clear_known_categories(dframes[0]._meta, ['y'], index=True)
A:dask.dataframe.tests.test_multi.sol->concat(dfs, join=join)
A:dask.dataframe.tests.test_multi.res->concat([grouped_d1, grouped_d2], axis=1)
A:dask.dataframe.tests.test_multi.parts->compute_as_if_collection(dd.DataFrame, res.dask, res.__dask_keys__())
A:dask.dataframe.tests.test_multi.db->dask.dataframe.from_pandas(b, 2).to_frame('A')
A:dask.dataframe.tests.test_multi.b2->pandas.DataFrame({'x': ['a']}, index=pd.DatetimeIndex(['2015-03-24 00:00:16'], dtype='datetime64[ns]'))
A:dask.dataframe.tests.test_multi.b3->pandas.DataFrame({'x': ['c']}, index=pd.DatetimeIndex(['2015-03-29 00:00:44'], dtype='datetime64[ns]'))
A:dask.dataframe.tests.test_multi.b2['x']->pandas.DataFrame({'x': ['a']}, index=pd.DatetimeIndex(['2015-03-24 00:00:16'], dtype='datetime64[ns]')).x.astype('category').cat.set_categories(['a', 'c'])
A:dask.dataframe.tests.test_multi.b3['x']->pandas.DataFrame({'x': ['c']}, index=pd.DatetimeIndex(['2015-03-29 00:00:44'], dtype='datetime64[ns]')).x.astype('category').cat.set_categories(['a', 'c'])
A:dask.dataframe.tests.test_multi.db2->dask.dataframe.from_pandas(b2, 1)
A:dask.dataframe.tests.test_multi.db3->dask.dataframe.from_pandas(b3, 1)
A:dask.dataframe.tests.test_multi.df2.y->df2.astype({'B': 'category'}).astype({'B': 'category'}).y.cat.set_categories(list('abc'))
A:dask.dataframe.tests.test_multi.joined->dask.dataframe.from_pandas(df2, npartitions=2).join(ddf2, rsuffix='r')
A:dask.dataframe.tests.test_multi.r1->dask.dataframe.from_pandas(df1, npartitions=1).merge(ddf2, how='left', left_index=True, right_index=True)
A:dask.dataframe.tests.test_multi.sf1->pandas.DataFrame(data={'A': ['a', 'b', 'c']}, index=['s', 'v', 'w']).set_index('x')
A:dask.dataframe.tests.test_multi.sf2->df2.astype({'B': 'category'}).astype({'B': 'category'}).set_index('x')
A:dask.dataframe.tests.test_multi.r2->pandas.DataFrame(data={'A': ['a', 'b', 'c']}, index=['s', 'v', 'w']).set_index('x').merge(sf2, how='left', left_index=True, right_index=True)
A:dask.dataframe.tests.test_multi.d1->pytest.importorskip('cudf').from_pandas(d1)
A:dask.dataframe.tests.test_multi.d2->pytest.importorskip('cudf').from_pandas(d2)
A:dask.dataframe.tests.test_multi.dd1->dask.dataframe.from_pandas(d1, npartitions)
A:dask.dataframe.tests.test_multi.dd2->dask.dataframe.from_pandas(d2, npartitions)
A:dask.dataframe.tests.test_multi.grouped_d1->pytest.importorskip('cudf').from_pandas(d1).groupby(['a']).sum()
A:dask.dataframe.tests.test_multi.grouped_d2->pytest.importorskip('cudf').from_pandas(d2).groupby(['c']).sum()
A:dask.dataframe.tests.test_multi.grouped_dd1->dask.dataframe.from_pandas(d1, npartitions).groupby(['a']).sum()
A:dask.dataframe.tests.test_multi.grouped_dd2->dask.dataframe.from_pandas(d2, npartitions).groupby(['c']).sum()
A:dask.dataframe.tests.test_multi.res_dd->dask.dataframe.concat([grouped_dd1, grouped_dd2], axis=1)
A:dask.dataframe.tests.test_multi.expected['x']->expected['x'].astype('category').astype('category')
A:dask.dataframe.tests.test_multi.df1['a']->df1['a'].astype(dtype).astype(dtype)
A:dask.dataframe.tests.test_multi.df2['a']->df2['a'].astype(dtype).astype(dtype)
A:dask.dataframe.tests.test_multi.ddf['join_col']->ddf['join_col'].astype('category').astype('category')
A:dask.dataframe.tests.test_multi.ddf2.index->dask.dataframe.from_pandas(df2, npartitions=2).index.astype('category')
A:dask.dataframe.tests.test_multi.actual_dask->pandas.DataFrame(data={'A': ['a', 'b', 'c']}, index=['s', 'v', 'w']).merge(df2, on='A')
A:dask.dataframe.tests.test_multi.df1['A']->pandas.DataFrame(data={'A': ['a', 'b', 'c']}, index=['s', 'v', 'w']).A.astype('category')
A:dask.dataframe.tests.test_multi.df2['A']->df2.astype({'B': 'category'}).astype({'B': 'category'}).A.astype('category')
A:dask.dataframe.tests.test_multi.q->dask.dataframe.from_pandas(df1, npartitions=1).join(df2)
A:dask.dataframe.tests.test_multi.lg->pandas.DataFrame({'x': np.random.choice(np.arange(100), size_lg), 'y': np.arange(size_lg)})
A:dask.dataframe.tests.test_multi.ddf_lg->dask.dataframe.from_pandas(lg, npartitions=npartitions_lg)
A:dask.dataframe.tests.test_multi.sm->pandas.DataFrame({'x': np.random.choice(np.arange(100), size_sm), 'y': np.arange(size_sm)})
A:dask.dataframe.tests.test_multi.ddf_sm->dask.dataframe.from_pandas(sm, npartitions=npartitions_sm)
A:dask.dataframe.tests.test_multi.dd_result->dask.dataframe.merge(ddf_left, ddf_right, on='y', how=how, npartitions=npartitions, broadcast=broadcast_bias, shuffle_method='tasks')
A:dask.dataframe.tests.test_multi.pd_result->pandas.merge(left, right, on='y', how=how)
A:dask.dataframe.tests.test_multi.dd_result['y']->dd_result['y'].astype(np.int32).astype(np.int32)
A:dask.dataframe.tests.test_multi.pd_result['y']->pd_result['y'].astype(np.int32).astype(np.int32)
A:dask.dataframe.tests.test_multi.base_df->dask.dataframe.from_pandas(pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6]}, index=[0, 1, 3]), 3)
A:dask.dataframe.tests.test_multi.ddf_loop->ddf_loop.join(ddf, how=how).join(ddf, how=how)
A:dask.dataframe.tests.test_multi.ddf_pairwise->ddf_pairwise.join(dfs_to_merge, how=how).join(dfs_to_merge, how=how)
dask.dataframe.tests.test_multi.check_append_with_warning(dask_obj,dask_append,pandas_obj,pandas_append)
dask.dataframe.tests.test_multi.list_eq(aa,bb)
dask.dataframe.tests.test_multi.test__maybe_align_partitions()
dask.dataframe.tests.test_multi.test_align_partitions()
dask.dataframe.tests.test_multi.test_align_partitions_unknown_divisions()
dask.dataframe.tests.test_multi.test_append()
dask.dataframe.tests.test_multi.test_append2()
dask.dataframe.tests.test_multi.test_append_categorical()
dask.dataframe.tests.test_multi.test_append_lose_divisions()
dask.dataframe.tests.test_multi.test_broadcast_true(shuffle)
dask.dataframe.tests.test_multi.test_categorical_join()
dask.dataframe.tests.test_multi.test_categorical_merge_does_not_raise_setting_with_copy_warning()
dask.dataframe.tests.test_multi.test_categorical_merge_retains_category_dtype()
dask.dataframe.tests.test_multi.test_categorical_merge_with_columns_missing_from_left()
dask.dataframe.tests.test_multi.test_categorical_merge_with_merge_column_cat_in_one_and_not_other_upcasts()
dask.dataframe.tests.test_multi.test_cheap_inner_merge_with_pandas_object()
dask.dataframe.tests.test_multi.test_cheap_single_parition_merge_left_right(how,flip)
dask.dataframe.tests.test_multi.test_cheap_single_partition_merge(flip)
dask.dataframe.tests.test_multi.test_cheap_single_partition_merge_divisions()
dask.dataframe.tests.test_multi.test_cheap_single_partition_merge_on_index()
dask.dataframe.tests.test_multi.test_concat(join)
dask.dataframe.tests.test_multi.test_concat2()
dask.dataframe.tests.test_multi.test_concat3()
dask.dataframe.tests.test_multi.test_concat4_interleave_partitions()
dask.dataframe.tests.test_multi.test_concat5()
dask.dataframe.tests.test_multi.test_concat_categorical(known,cat_index,divisions)
dask.dataframe.tests.test_multi.test_concat_categorical_mixed_simple()
dask.dataframe.tests.test_multi.test_concat_dataframe_empty()
dask.dataframe.tests.test_multi.test_concat_datetimeindex()
dask.dataframe.tests.test_multi.test_concat_different_dtypes(value_1,value_2)
dask.dataframe.tests.test_multi.test_concat_ignore_order(ordered)
dask.dataframe.tests.test_multi.test_concat_one_series()
dask.dataframe.tests.test_multi.test_concat_series(join)
dask.dataframe.tests.test_multi.test_concat_unknown_divisions()
dask.dataframe.tests.test_multi.test_concat_unknown_divisions_errors()
dask.dataframe.tests.test_multi.test_concat_with_operation_remains_hlg()
dask.dataframe.tests.test_multi.test_dtype_equality_warning()
dask.dataframe.tests.test_multi.test_errors_for_merge_on_frame_columns()
dask.dataframe.tests.test_multi.test_groupby_concat_cudf(engine)
dask.dataframe.tests.test_multi.test_half_indexed_dataframe_avoids_shuffle()
dask.dataframe.tests.test_multi.test_hash_join(how,shuffle_method)
dask.dataframe.tests.test_multi.test_indexed_concat(join)
dask.dataframe.tests.test_multi.test_join_by_index_patterns(how,shuffle_method)
dask.dataframe.tests.test_multi.test_join_gives_proper_divisions()
dask.dataframe.tests.test_multi.test_melt(kwargs)
dask.dataframe.tests.test_multi.test_merge(how,shuffle_method)
dask.dataframe.tests.test_multi.test_merge_asof_by_leftby_rightby_error()
dask.dataframe.tests.test_multi.test_merge_asof_indexed()
dask.dataframe.tests.test_multi.test_merge_asof_indexed_two_partitions()
dask.dataframe.tests.test_multi.test_merge_asof_left_on_right_index(allow_exact_matches,direction,unknown_divisions)
dask.dataframe.tests.test_multi.test_merge_asof_on(allow_exact_matches,direction)
dask.dataframe.tests.test_multi.test_merge_asof_on_basic()
dask.dataframe.tests.test_multi.test_merge_asof_on_by()
dask.dataframe.tests.test_multi.test_merge_asof_on_by_tolerance()
dask.dataframe.tests.test_multi.test_merge_asof_on_by_tolerance_no_exact_matches()
dask.dataframe.tests.test_multi.test_merge_asof_on_left_right(left_col,right_col)
dask.dataframe.tests.test_multi.test_merge_asof_on_lefton_righton_error()
dask.dataframe.tests.test_multi.test_merge_asof_unsorted_raises()
dask.dataframe.tests.test_multi.test_merge_asof_with_empty()
dask.dataframe.tests.test_multi.test_merge_asof_with_various_npartitions()
dask.dataframe.tests.test_multi.test_merge_by_index_patterns(how,shuffle_method)
dask.dataframe.tests.test_multi.test_merge_by_multiple_columns(how,shuffle_method)
dask.dataframe.tests.test_multi.test_merge_columns_dtypes(how,on_index)
dask.dataframe.tests.test_multi.test_merge_deprecated_shuffle_keyword(shuffle_method)
dask.dataframe.tests.test_multi.test_merge_empty_left_df(shuffle_method,how)
dask.dataframe.tests.test_multi.test_merge_how_raises()
dask.dataframe.tests.test_multi.test_merge_index_without_divisions(shuffle_method)
dask.dataframe.tests.test_multi.test_merge_indexed_dataframe_to_indexed_dataframe()
dask.dataframe.tests.test_multi.test_merge_maintains_columns()
dask.dataframe.tests.test_multi.test_merge_outer_empty()
dask.dataframe.tests.test_multi.test_merge_tasks_large_to_small(how,npartitions,base)
dask.dataframe.tests.test_multi.test_merge_tasks_passes_through()
dask.dataframe.tests.test_multi.test_merge_tasks_semi_anti_cudf(engine,how,parts)
dask.dataframe.tests.test_multi.test_multi_duplicate_divisions()
dask.dataframe.tests.test_multi.test_nullable_types_merge(dtype)
dask.dataframe.tests.test_multi.test_pairwise_merge_results_in_identical_output_df(how,npartitions_base,npartitions_other)
dask.dataframe.tests.test_multi.test_pairwise_rejects_unsupported_join_types(how)
dask.dataframe.tests.test_multi.test_repartition_repeated_divisions()
dask.dataframe.tests.test_multi.test_sequential_joins()
dask.dataframe.tests.test_multi.test_singleton_divisions()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/tseries/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/tseries/resample.py----------------------------------------
A:dask.dataframe.tseries.resample.out->getattr(series.resample(rule, **resample_kwargs), how)(*how_args, **how_kwargs)
A:dask.dataframe.tseries.resample.new_index->pandas.date_range(start.tz_localize(None), end.tz_localize(None), freq=rule, **closed_kwargs, name=out.index.name).tz_localize(start.tz, nonexistent='shift_forward')
A:dask.dataframe.tseries.resample.rule->pandas.tseries.frequencies.to_offset(rule)
A:dask.dataframe.tseries.resample.g->pandas.Grouper(freq=rule, how='count', closed=closed, label=label)
A:dask.dataframe.tseries.resample.divs->pandas.Series(range(len(divisions)), index=divisions)
A:dask.dataframe.tseries.resample.temp->pandas.Series(range(len(divisions)), index=divisions).resample(rule, closed=closed, label='left').count()
A:dask.dataframe.tseries.resample.newdivs->dask.dataframe.methods.tolist(newdivs)
A:dask.dataframe.tseries.resample.outdivs->dask.dataframe.methods.tolist(outdivs)
A:dask.dataframe.tseries.resample.self._rule->pandas.tseries.frequencies.to_offset(rule)
A:dask.dataframe.tseries.resample.(newdivs, outdivs)->_resample_bin_and_out_divs(self.obj.divisions, rule, **kwargs)
A:dask.dataframe.tseries.resample.partitioned->self.obj.repartition(newdivs, force=True)
A:dask.dataframe.tseries.resample.keys->self.obj.repartition(newdivs, force=True).__dask_keys__()
A:dask.dataframe.tseries.resample.args->zip(keys, outdivs, outdivs[1:], ['left'] * (len(keys) - 1) + [None])
A:dask.dataframe.tseries.resample.meta_r->self.obj._meta_nonempty.resample(self._rule, **self._kwargs)
A:dask.dataframe.tseries.resample.meta->getattr(meta_r, how)(*how_args, **how_kwargs)
A:dask.dataframe.tseries.resample.graph->dask.highlevelgraph.HighLevelGraph.from_collections(name, dsk, dependencies=[partitioned])
dask.dataframe.tseries.resample.Resampler(self,obj,rule,**kwargs)
dask.dataframe.tseries.resample.Resampler.__init__(self,obj,rule,**kwargs)
dask.dataframe.tseries.resample.Resampler._agg(self,how,meta=None,fill_value=np.nan,how_args=(),how_kwargs=None)
dask.dataframe.tseries.resample.Resampler.agg(self,agg_funcs,*args,**kwargs)
dask.dataframe.tseries.resample.Resampler.count(self)
dask.dataframe.tseries.resample.Resampler.first(self)
dask.dataframe.tseries.resample.Resampler.last(self)
dask.dataframe.tseries.resample.Resampler.max(self)
dask.dataframe.tseries.resample.Resampler.mean(self)
dask.dataframe.tseries.resample.Resampler.median(self)
dask.dataframe.tseries.resample.Resampler.min(self)
dask.dataframe.tseries.resample.Resampler.nunique(self)
dask.dataframe.tseries.resample.Resampler.ohlc(self)
dask.dataframe.tseries.resample.Resampler.prod(self)
dask.dataframe.tseries.resample.Resampler.quantile(self)
dask.dataframe.tseries.resample.Resampler.sem(self)
dask.dataframe.tseries.resample.Resampler.size(self)
dask.dataframe.tseries.resample.Resampler.std(self)
dask.dataframe.tseries.resample.Resampler.sum(self)
dask.dataframe.tseries.resample.Resampler.var(self)
dask.dataframe.tseries.resample._resample_bin_and_out_divs(divisions,rule,closed='left',label='left')
dask.dataframe.tseries.resample._resample_series(series,start,end,reindex_closed,rule,resample_kwargs,how,fill_value,how_args,how_kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/tseries/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/dataframe/tseries/tests/test_resample.py----------------------------------------
A:dask.dataframe.tseries.tests.test_resample.index->pandas.date_range('2000-01-01', '2000-02-15', freq='h')
A:dask.dataframe.tseries.tests.test_resample.ps->pandas.Series(range(len(index)), index=index)
A:dask.dataframe.tseries.tests.test_resample.ds->dask.dataframe.from_pandas(ps, npartitions=2)
A:dask.dataframe.tseries.tests.test_resample.result->dask.dataframe.from_pandas(df, npartitions=1).resample('1D').mean()
A:dask.dataframe.tseries.tests.test_resample.expected->f(ds.resample('1d'))
A:dask.dataframe.tseries.tests.test_resample.df->pandas.DataFrame([[1], [2], [3]], index=index)
A:dask.dataframe.tseries.tests.test_resample.df['Time']->pandas.to_datetime(df['Time'], utc=True)
A:dask.dataframe.tseries.tests.test_resample.ddf->dask.dataframe.from_pandas(df, npartitions=1)
A:dask.dataframe.tseries.tests.test_resample.actual->dask.dataframe.from_pandas(df, npartitions=1).resample(freq).size().compute()
A:dask.dataframe.tseries.tests.test_resample.s->pandas.Series(range(len(index)), index=index)
A:dask.dataframe.tseries.tests.test_resample.date_today->datetime.datetime.now()
A:dask.dataframe.tseries.tests.test_resample.days->pandas.date_range(date_today, date_today + timedelta(20), freq='D')
A:dask.dataframe.tseries.tests.test_resample.data->numpy.random.randint(1, high=100, size=len(days))
A:dask.dataframe.tseries.tests.test_resample.df.index->pandas.DataFrame([[1], [2], [3]], index=index).index.tz_localize('America/Sao_Paulo')
A:dask.dataframe.tseries.tests.test_resample.res->f(ps.resample('1d'))
dask.dataframe.tseries.tests.test_resample.resample(df,freq,how='mean',**kwargs)
dask.dataframe.tseries.tests.test_resample.test_common_aggs(agg)
dask.dataframe.tseries.tests.test_resample.test_resample_agg()
dask.dataframe.tseries.tests.test_resample.test_resample_agg_passes_kwargs()
dask.dataframe.tseries.tests.test_resample.test_resample_does_not_evenly_divide_day()
dask.dataframe.tseries.tests.test_resample.test_resample_has_correct_fill_value(method)
dask.dataframe.tseries.tests.test_resample.test_resample_index_name()
dask.dataframe.tseries.tests.test_resample.test_resample_pads_last_division_to_avoid_off_by_one()
dask.dataframe.tseries.tests.test_resample.test_resample_throws_error_when_parition_index_does_not_match_index()
dask.dataframe.tseries.tests.test_resample.test_series_resample(obj,method,npartitions,freq,closed,label)
dask.dataframe.tseries.tests.test_resample.test_series_resample_does_not_evenly_divide_day()
dask.dataframe.tseries.tests.test_resample.test_series_resample_non_existent_datetime()
dask.dataframe.tseries.tests.test_resample.test_unknown_divisions_error()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/widgets/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/widgets/widgets.py----------------------------------------
A:dask.widgets.widgets.loader->FileSystemLoader(TEMPLATE_PATHS)
A:dask.widgets.widgets.environment->Environment(loader=loader)
dask.widgets.get_environment()->Environment
dask.widgets.get_template(name:str)->Template
dask.widgets.widgets.get_environment()->Environment
dask.widgets.widgets.get_template(name:str)->Template


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/widgets/tests/test_widgets.py----------------------------------------
A:dask.widgets.tests.test_widgets.jinja2->pytest.importorskip('jinja2')
A:dask.widgets.tests.test_widgets.template->get_template('custom_filter.html.j2')
A:dask.widgets.tests.test_widgets.rendered->get_template('custom_filter.html.j2').render(foo='bar')
A:dask.widgets.tests.test_widgets.environment->get_environment()
dask.widgets.tests.test_widgets.setup_testing()
dask.widgets.tests.test_widgets.test_environment()
dask.widgets.tests.test_widgets.test_filters()
dask.widgets.tests.test_widgets.test_unknown_template()
dask.widgets.tests.test_widgets.test_widgets()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/tests/test_rewrite.py----------------------------------------
A:dask.tests.test_rewrite.t->Traverser(term)
A:dask.tests.test_rewrite.t2->Traverser(term).copy()
A:dask.tests.test_rewrite.rule1->RewriteRule((add, 'a', 1), (inc, 'a'), vars)
A:dask.tests.test_rewrite.rule2->RewriteRule((add, 'a', 'a'), (double, 'a'), vars)
A:dask.tests.test_rewrite.rule3->RewriteRule((add, (inc, 'a'), (inc, 'a')), (add, (double, 'a'), 2), vars)
A:dask.tests.test_rewrite.rule4->RewriteRule((add, (inc, 'b'), (inc, 'a')), (add, (add, 'a', 'b'), 2), vars)
A:dask.tests.test_rewrite.rule5->RewriteRule((sum, ['c', 'b', 'a']), (add, (add, 'a', 'b'), 'c'), vars)
A:dask.tests.test_rewrite.rule6->RewriteRule((list, 'x'), repl_list, ('x',))
A:dask.tests.test_rewrite.rs->RuleSet(*rules)
A:dask.tests.test_rewrite.matches->list(rs.iter_matches(term))
A:dask.tests.test_rewrite.new_term->RuleSet(*rules).rewrite(new_term)
dask.tests.test_rewrite.double(x)
dask.tests.test_rewrite.repl_list(sd)
dask.tests.test_rewrite.test_RewriteRule()
dask.tests.test_rewrite.test_RewriteRuleSubs()
dask.tests.test_rewrite.test_RuleSet()
dask.tests.test_rewrite.test_args()
dask.tests.test_rewrite.test_head()
dask.tests.test_rewrite.test_matches()
dask.tests.test_rewrite.test_rewrite()
dask.tests.test_rewrite.test_traverser()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/tests/test_optimization.py----------------------------------------
A:dask.tests.test_optimization.(culled, dependencies)->cull(d, 'out')
A:dask.tests.test_optimization.rv1->fuse_linear(*args, **kwargs)
A:dask.tests.test_optimization.rv2->fuse(*args, **kwargs)
A:dask.tests.test_optimization.result->inline_functions(dsk, [], fast_functions={inc})
A:dask.tests.test_optimization.nohash->NonHashableCallable()
A:dask.tests.test_optimization.(d2, dependencies)->cull(d, ['d', 'e'])
A:dask.tests.test_optimization.expected->with_deps(d)
A:dask.tests.test_optimization.rv->fuse(d, keys=keys, ave_width=2, rename_keys=True)
A:dask.tests.test_optimization.f->SubgraphCallable(dsk, 'h', ['in1', 'in2'], name='test')
A:dask.tests.test_optimization.f2->SubgraphCallable(dsk2, 'c', ['d', 'e'])
A:dask.tests.test_optimization.f3->SubgraphCallable(dsk2, 'c', ['d', 'f'], name=f1.name)
A:dask.tests.test_optimization.dsk2->dsk.copy()
A:dask.tests.test_optimization.np->pytest.importorskip('numpy')
A:dask.tests.test_optimization.f1->SubgraphCallable(dsk1, 'c', ['d', 'e'])
A:dask.tests.test_optimization.f4->SubgraphCallable(dsk2, 'a', ['d', 'e'], name=f1.name)
A:dask.tests.test_optimization.f5->SubgraphCallable(dsk1, 'c', ['e', 'd'], name=f1.name)
A:dask.tests.test_optimization.unnamed1->SubgraphCallable(dsk1, 'c', ['d', 'e'], name='first')
A:dask.tests.test_optimization.unnamed2->SubgraphCallable(dsk1, 'c', ['d', 'e'], name='second')
A:dask.tests.test_optimization.res->fuse(dsk, 'add-5', fuse_subgraphs=True)
A:dask.tests.test_optimization.sol->with_deps({'add-x-1': (SubgraphCallable({'x-1': 1, 'add-1': (add, 'x-1', 'x-1'), 'add-2': (add, 'add-1', 'add-1'), 'add-3': (add, 'add-2', 'add-2'), 'add-4': (add, 'add-3', 'add-3'), 'add-5': (add, 'add-4', 'add-4')}, 'add-5', ()),), 'add-5': 'add-x-1'})
A:dask.tests.test_optimization.(fused, deps)->fuse(d, rename_keys=True)
dask.tests.test_optimization._subgraph_callables_eq(self,other)
dask.tests.test_optimization.compare_subgraph_callables(monkeypatch)
dask.tests.test_optimization.double(x)
dask.tests.test_optimization.func_with_kwargs(a,b,c=2)
dask.tests.test_optimization.fuse2(*args,**kwargs)
dask.tests.test_optimization.test_SubgraphCallable()
dask.tests.test_optimization.test_SubgraphCallable_eq()
dask.tests.test_optimization.test_SubgraphCallable_with_numpy()
dask.tests.test_optimization.test_cull()
dask.tests.test_optimization.test_donot_substitute_same_key_multiple_times()
dask.tests.test_optimization.test_dont_fuse_numpy_arrays()
dask.tests.test_optimization.test_functions_of()
dask.tests.test_optimization.test_fuse()
dask.tests.test_optimization.test_fuse_config()
dask.tests.test_optimization.test_fuse_keys()
dask.tests.test_optimization.test_fuse_reductions_multiple_input()
dask.tests.test_optimization.test_fuse_reductions_single_input()
dask.tests.test_optimization.test_fuse_stressed()
dask.tests.test_optimization.test_fuse_subgraphs(compare_subgraph_callables)
dask.tests.test_optimization.test_fuse_subgraphs_linear_chains_of_duplicate_deps(compare_subgraph_callables)
dask.tests.test_optimization.test_fused_keys_max_length()
dask.tests.test_optimization.test_inline()
dask.tests.test_optimization.test_inline_cull_dependencies()
dask.tests.test_optimization.test_inline_doesnt_shrink_fast_functions_at_top()
dask.tests.test_optimization.test_inline_functions()
dask.tests.test_optimization.test_inline_functions_non_hashable()
dask.tests.test_optimization.test_inline_functions_protects_output_keys()
dask.tests.test_optimization.test_inline_ignores_curries_and_partials()
dask.tests.test_optimization.test_inline_traverses_lists()
dask.tests.test_optimization.with_deps(dsk)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/tests/test_base.py----------------------------------------
A:dask.tests.test_base.da->pytest.importorskip('dask.array')
A:dask.tests.test_base.dd->import_or_none('dask.dataframe')
A:dask.tests.test_base.np->import_or_none('numpy')
A:dask.tests.test_base.sp->import_or_none('scipy.sparse')
A:dask.tests.test_base.pd->pytest.importorskip('pandas')
A:dask.tests.test_base.a->dask.bag.from_sequence([1, 2, 3]).min()
A:dask.tests.test_base.x->pytest.importorskip('dask.array').arange(10)
A:dask.tests.test_base.fn->str(tmpdir.join('demo_data'))
A:dask.tests.test_base.mmap1->import_or_none('numpy').memmap(f, dtype=np.uint8, mode='r', offset=0, shape=5)
A:dask.tests.test_base.mmap2->import_or_none('numpy').memmap(f, dtype=np.uint8, mode='r', offset=5, shape=5)
A:dask.tests.test_base.y->dask.delayed(inc)(x)
A:dask.tests.test_base.z->Tuple(dsk3, ['z'])
A:dask.tests.test_base.mm->import_or_none('numpy').load(fn, mmap_mode='r')
A:dask.tests.test_base.mm2->import_or_none('numpy').load(fn, mmap_mode='r')
A:dask.tests.test_base.b->dask.bag.range(10, npartitions=2)
A:dask.tests.test_base.c->OrderedDict([('b', 2), ('a', 1)])
A:dask.tests.test_base.d->Delayed(key, {key: 1})
A:dask.tests.test_base.inc->import_or_none('numpy').frompyfunc(lambda x: x + 1, 1, 1)
A:dask.tests.test_base.f->delayed(pure=False)(time.sleep)
A:dask.tests.test_base.res->persist([a, b], c, traverse=False)
A:dask.tests.test_base.o->dask.bag.range(10, npartitions=2).map(add1).map(mul2)
A:dask.tests.test_base.a['z']->dask.bag.from_sequence([1, 2, 3]).min().y.astype('category')
A:dask.tests.test_base.b['z']->dask.bag.from_sequence([1, 2, 3]).min().y.astype('category')
A:dask.tests.test_base.df->pytest.importorskip('pandas').DataFrame(data=np.random.random((3, 1)), columns=['\ud83d'])
A:dask.tests.test_base.idx->pytest.importorskip('pandas').MultiIndex.from_product([['a', 'b'], [0, 1]])
A:dask.tests.test_base.before->tokenize(a)
A:dask.tests.test_base.after->tokenize(a)
A:dask.tests.test_base.a1->ADataClass(1)
A:dask.tests.test_base.a2->ADataClass(2)
A:dask.tests.test_base.b1->BDataClass(1)
A:dask.tests.test_base.ADataClassRedefinedDifferently->dataclasses.make_dataclass('ADataClass', [('a', Union[int, str])])
A:dask.tests.test_base.rng->import_or_none('numpy').random.RandomState(1234)
A:dask.tests.test_base.b.row[:10]->import_or_none('numpy').arange(10)
A:dask.tests.test_base.cycle->dict(a=None)
A:dask.tests.test_base.dx->pytest.importorskip('dask_expr')
A:dask.tests.test_base.dxf->pytest.importorskip('dask_expr').from_pandas(df)
A:dask.tests.test_base.coll->pytest.importorskip('dask_expr')._collection.new_collection(DoNotMaterialize())
A:dask.tests.test_base.args->build(a, b, c, (i for i in [a, b, c]))
A:dask.tests.test_base.(collections, repack)->unpack_collections(a, (fail, 1), [(fail, 2, 3)], traverse=False)
A:dask.tests.test_base.result->persist({'a': a, 'b': [1, 2, b]}, (c, 2))
A:dask.tests.test_base.sol->build('~a', '~b', '~c', ['~a', '~b', '~c'])
A:dask.tests.test_base.__dask_scheduler__->staticmethod(dask.threaded.get)
A:dask.tests.test_base.w->Tuple({}, [])
A:dask.tests.test_base.t2->normalize_token(f).persist()
A:dask.tests.test_base.(w2, x2, y2, z2)->dask.persist(w, x, y, z)
A:dask.tests.test_base.(rebuild, args)->dask.bag.from_sequence([1, 2, 3]).min().__dask_postpersist__()
A:dask.tests.test_base.w3->rebuild({}, *args, rename={'w': 'w3'})
A:dask.tests.test_base.x3->delayed(inc)(x2)
A:dask.tests.test_base.z3->rebuild({'z3': 70}, *args, rename={'z': 'z3'})
A:dask.tests.test_base.add1->partial(add, 1)
A:dask.tests.test_base.mul2->partial(mul, 2)
A:dask.tests.test_base.arr->import_or_none('numpy').arange(100).reshape((10, 10))
A:dask.tests.test_base.darr->pytest.importorskip('dask.array').from_array(arr, chunks=(5, 5))
A:dask.tests.test_base.(out1, out2)->compute(ddf1, ddf2)
A:dask.tests.test_base.ddf->import_or_none('dask.dataframe').from_pandas(df, npartitions=2)
A:dask.tests.test_base.ddf2->rebuild(dsk, *args, rename={ddf1._name: 'x'})
A:dask.tests.test_base.ds->pytest.importorskip('pandas').Series([1, 2, 3, 4])
A:dask.tests.test_base.dds2->rebuild({('x', 0): 5}, *args, rename={dds1._name: 'x'})
A:dask.tests.test_base.dds1->import_or_none('dask.dataframe').from_pandas(ds1, npartitions=2).min()
A:dask.tests.test_base.df1->pytest.importorskip('pandas').DataFrame({'a': [1, 2, 3, 4], 'b': [5, 6, 7, 8]})
A:dask.tests.test_base.df2->pytest.importorskip('pandas').DataFrame({'a': [2, 3, 5, 6], 'b': [6, 7, 9, 10]})
A:dask.tests.test_base.ddf1->import_or_none('dask.dataframe').from_pandas(df1, npartitions=2)
A:dask.tests.test_base.ds1->pytest.importorskip('pandas').Series([1, 2, 3, 4])
A:dask.tests.test_base.ds2->pytest.importorskip('pandas').Series([5, 6, 7, 8])
A:dask.tests.test_base.(arr_out, df_out)->compute(darr, ddf)
A:dask.tests.test_base.(xx, bb)->dask.compute(x + 1, b.map(inc), scheduler='single-threaded')
A:dask.tests.test_base.(xx, yy)->compute(x, y)
A:dask.tests.test_base.graphviz->pytest.importorskip('graphviz')
A:dask.tests.test_base.viz->pytest.importorskip('dask.array').arange(10).dask.visualize(filename=os.path.join(d, 'mydask.png'))
A:dask.tests.test_base.text->delayed(pure=False)(time.sleep).read()
A:dask.tests.test_base.defn->dedent('\n    def inc():\n        return x\n    ')
A:dask.tests.test_base.t->normalize_token(f)
A:dask.tests.test_base.dsk->dict(x2.dask)
A:dask.tests.test_base.(x2, y2, z2, constant)->optimize(x, y, z, 1)
A:dask.tests.test_base.sols->dask.compute(x, y, z, optimizations=[inc_to_dec])
A:dask.tests.test_base.(x3, y3, z3)->optimize(x, y, z, optimizations=[inc_to_dec])
A:dask.tests.test_base.(x4, y4, z4)->optimize(x, y, z)
A:dask.tests.test_base.out->subprocess.check_output([sys.executable, '-c', code])
A:dask.tests.test_base.modules->set(eval(out.decode()))
A:dask.tests.test_base.x1->delayed(1)
A:dask.tests.test_base.x2->delayed(inc)(x1)
A:dask.tests.test_base.(xx,)->persist(x)
A:dask.tests.test_base.dp->rebuild({new_key: 2}, *args, rename=rename)
A:dask.tests.test_base.A->dataclasses.make_dataclass('A', [('param', float, dataclasses.field(repr=False))], namespace={'__dask_tokenize__': lambda self: self.param})
A:dask.tests.test_base.dsk1->collections_to_dsk([x])
A:dask.tests.test_base.dsk2->collections_to_dsk([x])
A:dask.tests.test_base.ctx->nullcontext()
A:dask.tests.test_base.mod->importlib.import_module(module)
A:dask.tests.test_base.proc->subprocess.run([sys.executable, '-c', inspect.getsource(check_default_scheduler) + f'check_default_scheduler({params})\n'])
dask.tests.test_base.ADataClass
dask.tests.test_base.BDataClass
dask.tests.test_base.MyExecutor(Executor)
dask.tests.test_base.Tuple(self,dsk,keys)
dask.tests.test_base.Tuple.__add__(self,other)
dask.tests.test_base.Tuple.__dask_graph__(self)
dask.tests.test_base.Tuple.__dask_keys__(self)
dask.tests.test_base.Tuple.__dask_layers__(self)
dask.tests.test_base.Tuple.__dask_postcompute__(self)
dask.tests.test_base.Tuple.__dask_postpersist__(self)
dask.tests.test_base.Tuple.__dask_tokenize__(self)
dask.tests.test_base.Tuple.__init__(self,dsk,keys)
dask.tests.test_base.Tuple._rebuild(dsk,keys,*,rename=None)
dask.tests.test_base.check_default_scheduler(module,collection,expected,emscripten)
dask.tests.test_base.f1(a,b,c=1)
dask.tests.test_base.f2(a,b=1,c=2)
dask.tests.test_base.f3(a)
dask.tests.test_base.inc_to_dec(dsk,keys)
dask.tests.test_base.test_callable_scheduler()
dask.tests.test_base.test_clone_key()
dask.tests.test_base.test_compute_array()
dask.tests.test_base.test_compute_array_bag()
dask.tests.test_base.test_compute_array_dataframe()
dask.tests.test_base.test_compute_as_if_collection_low_level_task_graph()
dask.tests.test_base.test_compute_dataframe()
dask.tests.test_base.test_compute_dataframe_invalid_unicode()
dask.tests.test_base.test_compute_dataframe_valid_unicode_in_bytes()
dask.tests.test_base.test_compute_nested()
dask.tests.test_base.test_compute_no_opt()
dask.tests.test_base.test_compute_with_literal()
dask.tests.test_base.test_custom_collection()
dask.tests.test_base.test_default_imports()
dask.tests.test_base.test_emscripten_default_scheduler(params)
dask.tests.test_base.test_get_collection_names()
dask.tests.test_base.test_get_name_from_key()
dask.tests.test_base.test_get_scheduler()
dask.tests.test_base.test_is_dask_collection()
dask.tests.test_base.test_is_dask_collection_dask_expr()
dask.tests.test_base.test_is_dask_collection_dask_expr_does_not_materialize()
dask.tests.test_base.test_normalize_base()
dask.tests.test_base.test_normalize_function()
dask.tests.test_base.test_normalize_function_dataclass_field_no_repr()
dask.tests.test_base.test_normalize_function_limited_size()
dask.tests.test_base.test_num_workers_config(scheduler)
dask.tests.test_base.test_optimizations_ctd()
dask.tests.test_base.test_optimizations_keyword()
dask.tests.test_base.test_optimize()
dask.tests.test_base.test_optimize_None()
dask.tests.test_base.test_optimize_globals()
dask.tests.test_base.test_optimize_nested()
dask.tests.test_base.test_persist_array()
dask.tests.test_base.test_persist_array_bag()
dask.tests.test_base.test_persist_array_rename()
dask.tests.test_base.test_persist_bag()
dask.tests.test_base.test_persist_bag_rename()
dask.tests.test_base.test_persist_dataframe()
dask.tests.test_base.test_persist_dataframe_rename()
dask.tests.test_base.test_persist_delayed()
dask.tests.test_base.test_persist_delayed_custom_key(key)
dask.tests.test_base.test_persist_delayed_rename(key,rename,new_key)
dask.tests.test_base.test_persist_delayedattr()
dask.tests.test_base.test_persist_delayedleaf()
dask.tests.test_base.test_persist_item()
dask.tests.test_base.test_persist_item_change_name()
dask.tests.test_base.test_persist_literals()
dask.tests.test_base.test_persist_nested()
dask.tests.test_base.test_persist_scalar()
dask.tests.test_base.test_persist_scalar_rename()
dask.tests.test_base.test_persist_series()
dask.tests.test_base.test_persist_series_rename()
dask.tests.test_base.test_raise_get_keyword()
dask.tests.test_base.test_replace_name_in_keys()
dask.tests.test_base.test_scheduler_keyword()
dask.tests.test_base.test_tokenize()
dask.tests.test_base.test_tokenize_base_types(x)
dask.tests.test_base.test_tokenize_callable()
dask.tests.test_base.test_tokenize_dataclass()
dask.tests.test_base.test_tokenize_datetime_date()
dask.tests.test_base.test_tokenize_datetime_datetime()
dask.tests.test_base.test_tokenize_datetime_time()
dask.tests.test_base.test_tokenize_dense_sparse_array(cls_name)
dask.tests.test_base.test_tokenize_dict()
dask.tests.test_base.test_tokenize_discontiguous_numpy_array()
dask.tests.test_base.test_tokenize_enum(enum_type)
dask.tests.test_base.test_tokenize_function_cloudpickle()
dask.tests.test_base.test_tokenize_kwargs()
dask.tests.test_base.test_tokenize_literal()
dask.tests.test_base.test_tokenize_method()
dask.tests.test_base.test_tokenize_na()
dask.tests.test_base.test_tokenize_numpy_array_consistent_on_values()
dask.tests.test_base.test_tokenize_numpy_array_on_object_dtype()
dask.tests.test_base.test_tokenize_numpy_array_supports_uneven_sizes()
dask.tests.test_base.test_tokenize_numpy_datetime()
dask.tests.test_base.test_tokenize_numpy_matrix()
dask.tests.test_base.test_tokenize_numpy_memmap()
dask.tests.test_base.test_tokenize_numpy_memmap_no_filename()
dask.tests.test_base.test_tokenize_numpy_memmap_offset(tmpdir)
dask.tests.test_base.test_tokenize_numpy_scalar()
dask.tests.test_base.test_tokenize_numpy_scalar_string_rep()
dask.tests.test_base.test_tokenize_numpy_ufunc_consistent()
dask.tests.test_base.test_tokenize_object()
dask.tests.test_base.test_tokenize_object_array_with_nans()
dask.tests.test_base.test_tokenize_object_with_recursion_error()
dask.tests.test_base.test_tokenize_offset()
dask.tests.test_base.test_tokenize_ordered_dict()
dask.tests.test_base.test_tokenize_pandas()
dask.tests.test_base.test_tokenize_pandas_extension_array()
dask.tests.test_base.test_tokenize_pandas_index()
dask.tests.test_base.test_tokenize_pandas_invalid_unicode()
dask.tests.test_base.test_tokenize_pandas_mixed_unicode_bytes()
dask.tests.test_base.test_tokenize_pandas_no_pickle()
dask.tests.test_base.test_tokenize_partial_func_args_kwargs_consistent()
dask.tests.test_base.test_tokenize_range()
dask.tests.test_base.test_tokenize_same_repr()
dask.tests.test_base.test_tokenize_sequences()
dask.tests.test_base.test_tokenize_set()
dask.tests.test_base.test_tokenize_timedelta()
dask.tests.test_base.test_unpack_collections()
dask.tests.test_base.test_use_cloudpickle_to_tokenize_functions_in__main__()
dask.tests.test_base.test_visualize()
dask.tests.test_base.test_visualize_highlevelgraph()
dask.tests.test_base.test_visualize_order()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/tests/test_threaded.py----------------------------------------
A:dask.tests.test_threaded.result->get({'x': (lambda : i,)}, 'x', num_workers=2)
A:dask.tests.test_threaded.before->threading.active_count()
A:dask.tests.test_threaded.t->threading.Thread(target=test_f)
A:dask.tests.test_threaded.start->time()
A:dask.tests.test_threaded.after->threading.active_count()
A:dask.tests.test_threaded.main_thread->threading.get_ident()
A:dask.tests.test_threaded.in_clog_event->threading.Event()
A:dask.tests.test_threaded.clog_event->threading.Event()
A:dask.tests.test_threaded.interrupter->threading.Thread(target=interrupt, args=(in_clog_event,))
dask.tests.test_threaded.bad(x)
dask.tests.test_threaded.test_broken_callback()
dask.tests.test_threaded.test_dont_spawn_too_many_threads()
dask.tests.test_threaded.test_dont_spawn_too_many_threads_CPU_COUNT()
dask.tests.test_threaded.test_exceptions_rise_to_top()
dask.tests.test_threaded.test_get()
dask.tests.test_threaded.test_get_without_computation()
dask.tests.test_threaded.test_interrupt()
dask.tests.test_threaded.test_nested_get()
dask.tests.test_threaded.test_pool_kwarg(pool_typ)
dask.tests.test_threaded.test_reuse_pool(pool_typ)
dask.tests.test_threaded.test_thread_safety()
dask.tests.test_threaded.test_threaded_within_thread()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/tests/test_context.py----------------------------------------
A:dask.tests.test_context.da->pytest.importorskip('dask.array')
A:dask.tests.test_context.x->Foo()
A:dask.tests.test_context.g->globalmethod(foo, key='g', falsey=bar)
dask.tests.test_context.Foo
dask.tests.test_context.Foo.f()
dask.tests.test_context.bar()
dask.tests.test_context.foo()
dask.tests.test_context.test_globalmethod()
dask.tests.test_context.test_with_get()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/tests/test_local.py----------------------------------------
A:dask.tests.test_local.result->start_state_from_dask(dsk)
A:dask.tests.test_local.state->start_state_from_dask(dsk)
A:dask.tests.test_local.get->staticmethod(get_sync)
A:dask.tests.test_local.x_keys->sorted(dsk)
A:dask.tests.test_local.da->pytest.importorskip('dask.array')
A:dask.tests.test_local.x->pytest.importorskip('dask.array').random.normal(size=(20, 20), chunks=(-1, -1))
A:dask.tests.test_local.res->(x.dot(x.T) - x.mean(axis=0)).std()
A:dask.tests.test_local.dsk->dict(res.__dask_graph__())
A:dask.tests.test_local.exp_order_dict->order(dsk)
A:dask.tests.test_local.exp_order->sorted(exp_order_dict.keys(), key=exp_order_dict.get)
dask.tests.test_local.TestGetAsync(GetFunctionTestMixin)
dask.tests.test_local.TestGetAsync.test_get_sync_num_workers(self)
dask.tests.test_local.test_cache_options()
dask.tests.test_local.test_callback()
dask.tests.test_local.test_complex_ordering()
dask.tests.test_local.test_exceptions_propagate()
dask.tests.test_local.test_finish_task()
dask.tests.test_local.test_ordering()
dask.tests.test_local.test_sort_key()
dask.tests.test_local.test_start_state()
dask.tests.test_local.test_start_state_looks_at_cache()
dask.tests.test_local.test_start_state_with_independent_but_runnable_tasks()
dask.tests.test_local.test_start_state_with_redirects()
dask.tests.test_local.test_start_state_with_tasks_no_deps()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/tests/test_docs.py----------------------------------------
dask.tests.test_docs.test_development_guidelines_matches_ci()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/tests/test_graph_manipulation.py----------------------------------------
A:dask.tests.test_graph_manipulation.da->import_or_none('dask.array')
A:dask.tests.test_graph_manipulation.dd->import_or_none('dask.dataframe')
A:dask.tests.test_graph_manipulation.pd->import_or_none('pandas')
A:dask.tests.test_graph_manipulation.zarr->import_or_none('zarr')
A:dask.tests.test_graph_manipulation.dsk1->HighLevelGraph({'a-1': dsk1}, {'a-1': set()})
A:dask.tests.test_graph_manipulation.dsk2->HighLevelGraph({'a-1': dsk1, 'b-1': dsk2}, {'a-1': set(), 'b-1': {'a-1'}})
A:dask.tests.test_graph_manipulation.dsko->omit.__dask_graph__()
A:dask.tests.test_graph_manipulation.cnt->NodeCounter()
A:dask.tests.test_graph_manipulation.df->import_or_none('pandas').DataFrame({'x': list(range(10))})
A:dask.tests.test_graph_manipulation.(t1, t2, cnt)->demo_tuples(layers)
A:dask.tests.test_graph_manipulation.cp->checkpoint(*colls)
A:dask.tests.test_graph_manipulation.(colls, cnt)->collections_with_node_counters()
A:dask.tests.test_graph_manipulation.(t1, _, cnt)->demo_tuples(layers)
A:dask.tests.test_graph_manipulation.t1w->wait_on(t1)
A:dask.tests.test_graph_manipulation.out->clone({'x': [t2]}, omit={'y': [t1, t3]}, assume_layers=layers)
A:dask.tests.test_graph_manipulation.colls2->wait_on(*colls)
A:dask.tests.test_graph_manipulation.dsk3->HighLevelGraph({'a-1': dsk1, 'b-1': dsk2, 'c-1': dsk3}, {'a-1': set(), 'b-1': {'a-1'}, 'c-1': {'b-1'}})
A:dask.tests.test_graph_manipulation.t1->Tuple(dsk, list(dsk))
A:dask.tests.test_graph_manipulation.t2->wait_on(t1, split_every=split_every)
A:dask.tests.test_graph_manipulation.t3->Tuple(dsk3, list(dsk3))
A:dask.tests.test_graph_manipulation.c1->clone(t2, seed=1, assume_layers=layers)
A:dask.tests.test_graph_manipulation.c2->clone(t2, seed=1, assume_layers=layers)
A:dask.tests.test_graph_manipulation.c3->clone(t2, seed=2, assume_layers=layers)
A:dask.tests.test_graph_manipulation.c4->clone(c1, seed=1, assume_layers=layers)
A:dask.tests.test_graph_manipulation.c5->clone(t2, assume_layers=layers)
A:dask.tests.test_graph_manipulation.c6->clone(t2, assume_layers=layers)
A:dask.tests.test_graph_manipulation.c7->clone(t2, omit=t1, seed=1, assume_layers=layers)
A:dask.tests.test_graph_manipulation.arr->import_or_none('dask.array').ones(10, chunks=1)
A:dask.tests.test_graph_manipulation.blk->import_or_none('dask.array').from_zarr(zarr.ones(10))
A:dask.tests.test_graph_manipulation.cln->clone(blk)
A:dask.tests.test_graph_manipulation.dsk4->HighLevelGraph({'d-1': dsk4, 'e': dsk4b}, {'d-1': set(), 'e': set()})
A:dask.tests.test_graph_manipulation.t4->bind(t3, t1, split_every=split_every, assume_layers=False)
A:dask.tests.test_graph_manipulation.bound1->bind(t3, t4, seed=1, assume_layers=layers)
A:dask.tests.test_graph_manipulation.cloned_a_name->clone_key('a-1', seed=1)
A:dask.tests.test_graph_manipulation.bound2->bind(t3, t4, omit=t2, seed=1, assume_layers=layers)
A:dask.tests.test_graph_manipulation.cloned_c_name->clone_key('c-1', seed=1)
A:dask.tests.test_graph_manipulation.bound3->bind(t4, t3, seed=1, assume_layers=layers)
A:dask.tests.test_graph_manipulation.cloned_d_name->clone_key('d-1', seed=1)
A:dask.tests.test_graph_manipulation.cloned_e_name->clone_key('e', seed=1)
A:dask.tests.test_graph_manipulation.d1->double(2)
A:dask.tests.test_graph_manipulation.d2->double(d1)
A:dask.tests.test_graph_manipulation.a1->import_or_none('dask.array').ones((10, 10), chunks=5)
A:dask.tests.test_graph_manipulation.b1->dask.bag.from_sequence([1, 2], npartitions=2)
A:dask.tests.test_graph_manipulation.b2->dask.bag.from_sequence([1, 2], npartitions=2).map(lambda x: x * 2)
A:dask.tests.test_graph_manipulation.b3->dask.bag.from_sequence([1, 2], npartitions=2).map(lambda x: x * 2).map(lambda x: x + 1)
A:dask.tests.test_graph_manipulation.b4->dask.bag.from_sequence([1, 2], npartitions=2).map(lambda x: x * 2).map(lambda x: x + 1).min()
A:dask.tests.test_graph_manipulation.ddf1->import_or_none('dask.dataframe').from_pandas(df, npartitions=2)
A:dask.tests.test_graph_manipulation.ddf2->import_or_none('dask.dataframe').from_pandas(df, npartitions=2).map_partitions(lambda x: x * 2)
A:dask.tests.test_graph_manipulation.ddf3->import_or_none('dask.dataframe').from_pandas(df, npartitions=2).map_partitions(lambda x: x * 2).map_partitions(lambda x: x + 1)
A:dask.tests.test_graph_manipulation.ddf5->ddf4.min()
A:dask.tests.test_graph_manipulation.parent->import_or_none('dask.array').ones((10, 10), chunks=5).map_blocks(cnt.f)
A:dask.tests.test_graph_manipulation.(d2c, a3c, b3c, b4c, ddf3c, ddf4c, ddf5c)->clone(d2, a3, b3, b4, ddf3, ddf4, ddf5, omit=(d1, a1, b2, ddf2), seed=0)
A:dask.tests.test_graph_manipulation.c->checkpoint(t1, split_every=split_every)
A:dask.tests.test_graph_manipulation.t->Tuple({'a': 1, 'b': 2}, ['a', 'b'])
dask.tests.test_graph_manipulation.NodeCounter(self)
dask.tests.test_graph_manipulation.NodeCounter.__init__(self)
dask.tests.test_graph_manipulation.NodeCounter.f(self,x)
dask.tests.test_graph_manipulation.assert_did_not_materialize(cloned,orig)
dask.tests.test_graph_manipulation.assert_no_common_keys(a,b,omit=None,*,layers:bool)->None
dask.tests.test_graph_manipulation.collections_with_node_counters()
dask.tests.test_graph_manipulation.demo_tuples(layers:bool)->tuple[Tuple, Tuple, NodeCounter]
dask.tests.test_graph_manipulation.test_bind(layers)
dask.tests.test_graph_manipulation.test_bind_clone_collections(func)
dask.tests.test_graph_manipulation.test_blockwise_clone_with_literals(literal)
dask.tests.test_graph_manipulation.test_blockwise_clone_with_no_indices()
dask.tests.test_graph_manipulation.test_checkpoint(layers)
dask.tests.test_graph_manipulation.test_checkpoint_collections()
dask.tests.test_graph_manipulation.test_clone(layers)
dask.tests.test_graph_manipulation.test_split_every(split_every,nkeys)
dask.tests.test_graph_manipulation.test_split_every_invalid()
dask.tests.test_graph_manipulation.test_wait_on_collections()
dask.tests.test_graph_manipulation.test_wait_on_many(layers)
dask.tests.test_graph_manipulation.test_wait_on_one(layers)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/tests/test_ml.py----------------------------------------
dask.tests.test_ml.test_basic()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/tests/test_traceback.py----------------------------------------
A:dask.tests.test_traceback.frames->list(traceback.walk_tb(e.tb))
A:dask.tests.test_traceback.d->dask.delayed(f3)()
A:dask.tests.test_traceback.distributed->pytest.importorskip('distributed')
A:dask.tests.test_traceback.(dp1,)->dask.persist(d)
A:dask.tests.test_traceback.dp2->dask.delayed(f3)().persist()
A:dask.tests.test_traceback.actual->dask.config.get('admin.traceback.shorten', config=d)
dask.tests.test_traceback.assert_tb_levels(expect)
dask.tests.test_traceback.f1()
dask.tests.test_traceback.f2()
dask.tests.test_traceback.f3()
dask.tests.test_traceback.test_compute_shorten_traceback(scheduler)
dask.tests.test_traceback.test_deprecated_config(tmp_path)
dask.tests.test_traceback.test_distributed_shorten_traceback()
dask.tests.test_traceback.test_persist_shorten_traceback(scheduler)
dask.tests.test_traceback.test_shorten_traceback(regexes,expect)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/tests/test_utils_test.py----------------------------------------
A:dask.tests.test_utils_test.hg->HighLevelGraph(layers, dependencies)
dask.tests.test_utils_test.test__check_warning()
dask.tests.test_utils_test.test_hlg_layer()
dask.tests.test_utils_test.test_hlg_layer_topological()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/tests/test_delayed.py----------------------------------------
A:dask.tests.test_delayed.__dask_scheduler__->staticmethod(dask.threaded.get)
A:dask.tests.test_delayed.a->Series(['a', 'b', 'a'], dtype='category')
A:dask.tests.test_delayed.b->Series(['a', 'c', 'a'], dtype='category')
A:dask.tests.test_delayed.(task, dask)->to_task_dask(x)
A:dask.tests.test_delayed.f->delayed(foo)
A:dask.tests.test_delayed.x->delayed(123)
A:dask.tests.test_delayed.add2->delayed(add)
A:dask.tests.test_delayed.literal->dask.delayed(3)
A:dask.tests.test_delayed.with_class->delayed({'data': ADataClass(a=literal)})
A:dask.tests.test_delayed.final->delayed(return_nested)(with_class)
A:dask.tests.test_delayed.data->delayed([1, 2, 3])
A:dask.tests.test_delayed.c->pytest.importorskip('dask.dataframe').from_delayed([da, db], verify_meta=False)
A:dask.tests.test_delayed.d->Delayed('b', graph)
A:dask.tests.test_delayed.o->Series(['a', 'b', 'a'], dtype='category').index(1)
A:dask.tests.test_delayed.np->pytest.importorskip('numpy')
A:dask.tests.test_delayed.z->dask.delayed(inc)(1)
A:dask.tests.test_delayed.(x2,)->dask.optimize(x)
A:dask.tests.test_delayed.res->delayed(x, traverse=False).compute()
A:dask.tests.test_delayed.v1->delayed(add, pure=True)(1, 2)
A:dask.tests.test_delayed.v2->delayed(add, pure=True)(1, 2)
A:dask.tests.test_delayed.myrand->delayed(random)
A:dask.tests.test_delayed.func->delayed(identity, pure=True)
A:dask.tests.test_delayed.length->len(x)
A:dask.tests.test_delayed.dmysum->delayed(mysum, pure=True)
A:dask.tests.test_delayed.ten->dmysum(1, 2, c=c, four=dmysum(2, 2))
A:dask.tests.test_delayed.x2->delayed(add, pure=True)(x, (4, 5, 6))
A:dask.tests.test_delayed.n->delayed(len, pure=True)(x)
A:dask.tests.test_delayed.da->delayed(lambda x: x)(a)
A:dask.tests.test_delayed.arr->pytest.importorskip('numpy').arange(100).reshape((10, 10))
A:dask.tests.test_delayed.darr->delayed(lambda x: x)(a).from_array(arr, chunks=(5, 5))
A:dask.tests.test_delayed.val->delayed(sum)([arr, darr, 1])
A:dask.tests.test_delayed.(task, dsk)->to_task_dask(darr)
A:dask.tests.test_delayed.delayed_arr->delayed(darr)
A:dask.tests.test_delayed.arr1->pytest.importorskip('numpy').arange(100).reshape((10, 10))
A:dask.tests.test_delayed.arr2->pytest.importorskip('numpy').arange(100).reshape((10, 10)).dot(arr1.T)
A:dask.tests.test_delayed.darr1->delayed(lambda x: x)(a).from_array(arr1, chunks=(5, 5))
A:dask.tests.test_delayed.darr2->delayed(lambda x: x)(a).from_array(arr2, chunks=(5, 5))
A:dask.tests.test_delayed.out->delayed(sum)([i.sum() for i in seq])
A:dask.tests.test_delayed.y->pickle.loads(pickle.dumps(x))
A:dask.tests.test_delayed.v->delayed([x])
A:dask.tests.test_delayed.foo->Foo(1)
A:dask.tests.test_delayed.X->delayed(lambda x: x)(a).ones((10, 10), chunks=5).to_delayed().flatten()
A:dask.tests.test_delayed.dd->pytest.importorskip('dask.dataframe')
A:dask.tests.test_delayed.db->delayed(lambda x: x)(b)
A:dask.tests.test_delayed.d1->delayed(1)
A:dask.tests.test_delayed.d2->modlevel_delayed1(d1)
A:dask.tests.test_delayed.hlg->dask.highlevelgraph.HighLevelGraph.from_collections('foo', {'alias': d2.key}, dependencies=[d2])
A:dask.tests.test_delayed.explicit->Delayed('alias', hlg, layer='foo')
A:dask.tests.test_delayed.graph->dask.highlevelgraph.HighLevelGraph.from_collections('b', {'a': 1, 'b': (inc, 'a'), 'c': (inc, 'b')}, [])
A:dask.tests.test_delayed.(d_opt,)->dask.optimize(d)
dask.tests.test_delayed.AFrozenDataClass
dask.tests.test_delayed.ANonFrozenDataClass
dask.tests.test_delayed.Tuple(self,dsk,keys)
dask.tests.test_delayed.Tuple.__dask_graph__(self)
dask.tests.test_delayed.Tuple.__dask_keys__(self)
dask.tests.test_delayed.Tuple.__dask_postcompute__(self)
dask.tests.test_delayed.Tuple.__dask_tokenize__(self)
dask.tests.test_delayed.Tuple.__init__(self,dsk,keys)
dask.tests.test_delayed.identity(x)
dask.tests.test_delayed.modlevel_delayed1(x)
dask.tests.test_delayed.modlevel_delayed2(x)
dask.tests.test_delayed.modlevel_eager(x)
dask.tests.test_delayed.test_annotations_survive_optimization()
dask.tests.test_delayed.test_array_bag_delayed()
dask.tests.test_delayed.test_array_delayed()
dask.tests.test_delayed.test_attribute_of_attribute()
dask.tests.test_delayed.test_attributes()
dask.tests.test_delayed.test_callable_obj()
dask.tests.test_delayed.test_check_meta_flag()
dask.tests.test_delayed.test_cloudpickle(f)
dask.tests.test_delayed.test_common_subexpressions()
dask.tests.test_delayed.test_custom_delayed()
dask.tests.test_delayed.test_dask_layers()
dask.tests.test_delayed.test_delayed()
dask.tests.test_delayed.test_delayed_callable()
dask.tests.test_delayed.test_delayed_compute_forward_kwargs()
dask.tests.test_delayed.test_delayed_decorator_on_method()
dask.tests.test_delayed.test_delayed_errors()
dask.tests.test_delayed.test_delayed_function_attributes_forwarded()
dask.tests.test_delayed.test_delayed_method_descriptor()
dask.tests.test_delayed.test_delayed_name()
dask.tests.test_delayed.test_delayed_name_on_call()
dask.tests.test_delayed.test_delayed_optimize()
dask.tests.test_delayed.test_delayed_picklable()
dask.tests.test_delayed.test_delayed_visualise_warn()
dask.tests.test_delayed.test_delayed_with_dataclass(cls)
dask.tests.test_delayed.test_delayed_with_dataclass_with_custom_init()
dask.tests.test_delayed.test_delayed_with_dataclass_with_eager_custom_init()
dask.tests.test_delayed.test_delayed_with_dataclass_with_set_init_false_field()
dask.tests.test_delayed.test_delayed_with_dataclass_with_unset_init_false_field()
dask.tests.test_delayed.test_delayed_with_eager_dataclass_with_set_init_false_field()
dask.tests.test_delayed.test_delayed_with_namedtuple()
dask.tests.test_delayed.test_finalize_name()
dask.tests.test_delayed.test_iterators(typ)
dask.tests.test_delayed.test_keys_from_array()
dask.tests.test_delayed.test_kwargs()
dask.tests.test_delayed.test_lists()
dask.tests.test_delayed.test_lists_are_concrete()
dask.tests.test_delayed.test_literates()
dask.tests.test_delayed.test_literates_keys()
dask.tests.test_delayed.test_method_getattr_call_same_task()
dask.tests.test_delayed.test_methods()
dask.tests.test_delayed.test_name_consistent_across_instances()
dask.tests.test_delayed.test_nout()
dask.tests.test_delayed.test_nout_with_tasks(x)
dask.tests.test_delayed.test_np_dtype_of_delayed()
dask.tests.test_delayed.test_operators()
dask.tests.test_delayed.test_pickle(f)
dask.tests.test_delayed.test_pure()
dask.tests.test_delayed.test_pure_global_setting()
dask.tests.test_delayed.test_sensitive_to_partials()
dask.tests.test_delayed.test_to_task_dask()
dask.tests.test_delayed.test_traverse_false()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/tests/test_ci.py----------------------------------------
A:dask.tests.test_ci.v->Version(importlib_metadata.version(package))
dask.tests.test_ci.test_upstream_packages_installed()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/tests/test_cli.py----------------------------------------
A:dask.tests.test_cli.runner->CliRunner()
A:dask.tests.test_cli.result->CliRunner().invoke(dask.cli.versions)
A:dask.tests.test_cli.table->json.loads(result.output)
A:dask.tests.test_cli.bad_ep->importlib_metadata.EntryPoint(name='bad', value='dask.tests.test_cli:bad_command', group='dask_cli')
A:dask.tests.test_cli.good_ep->importlib_metadata.EntryPoint(name='good', value='dask.tests.test_cli:good_command', group='dask_cli')
A:dask.tests.test_cli.one->importlib_metadata.EntryPoint(name='one', value='dask.tests.test_cli:good_command', group='dask_cli')
A:dask.tests.test_cli.two->importlib_metadata.EntryPoint(name='two', value='dask.tests.test_cli:good_command_2', group='dask_cli')
dask.tests.test_cli.bad_command()
dask.tests.test_cli.dummy_cli()
dask.tests.test_cli.dummy_cli_2()
dask.tests.test_cli.good_command()
dask.tests.test_cli.good_command_2()
dask.tests.test_cli.test_config_get()
dask.tests.test_cli.test_config_get_bad_value()
dask.tests.test_cli.test_config_get_value()
dask.tests.test_cli.test_config_list()
dask.tests.test_cli.test_info_versions()
dask.tests.test_cli.test_register_command_ep()
dask.tests.test_cli.test_repeated_name_registration_warn()
dask.tests.test_cli.test_version()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/tests/test_distributed.py----------------------------------------
A:dask.tests.test_distributed.distributed->pytest.importorskip('distributed')
A:dask.tests.test_distributed.gen_cluster->partial(gen_cluster, should_check_state=False)
A:dask.tests.test_distributed.cluster->partial(cluster, should_check_state=False)
A:dask.tests.test_distributed.pytestmark->pytest.mark.skipif(sys.platform == 'win32', reason='The teardown of distributed.utils_test.cluster_fixture fails on windows CI currently')
A:dask.tests.test_distributed.ignore_sync_scheduler_warning->pytest.mark.filterwarnings('ignore:Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.')
A:dask.tests.test_distributed.x->pytest.importorskip('dask.array', reason='Requires dask.array').from_array(np.array([0, 1, 2]))
A:dask.tests.test_distributed.(x2,)->persist(x)
A:dask.tests.test_distributed.y->pytest.importorskip('dask.array', reason='Requires dask.array').from_array(np.array([[0, 1, 2]]))
A:dask.tests.test_distributed.(y2, one)->persist(y, 1)
A:dask.tests.test_distributed.result->dd.from_pandas(series, npartitions=34).reduction(chunk=lambda x: x, aggregate=lambda x: SomeObject(x.sum().sum()), split_every=False, token='commit-dataset', meta=object)
A:dask.tests.test_distributed.res->get_scheduler_lock(collection, scheduler='distributed')
A:dask.tests.test_distributed.pd->pytest.importorskip('pandas')
A:dask.tests.test_distributed.dd->pytest.importorskip('dask.dataframe', reason='Requires dask.dataframe')
A:dask.tests.test_distributed.df->pytest.importorskip('pandas').DataFrame({'a': range(10), 'b': range(10)})
A:dask.tests.test_distributed.futures->distributed.utils_test.client.scatter([x, x])
A:dask.tests.test_distributed.ddf->dask.datasets.timeseries(start='2000-01-01', end='2000-07-01', freq='12h')
A:dask.tests.test_distributed.df1->pytest.importorskip('pandas').DataFrame({'x': range(size), 'y': range(size)})
A:dask.tests.test_distributed.df2->pytest.importorskip('pandas').DataFrame({'x': range(size), 'z': range(size)})
A:dask.tests.test_distributed.ddfm->pytest.importorskip('dask.dataframe', reason='Requires dask.dataframe').merge(dfl, dfr, on=on, broadcast=broadcast, shuffle_method='tasks')
A:dask.tests.test_distributed.dfm->pytest.importorskip('dask.dataframe', reason='Requires dask.dataframe').merge(dfl, dfr, on=on, broadcast=broadcast, shuffle_method='tasks').compute()
A:dask.tests.test_distributed.pdfl->pytest.importorskip('pandas').DataFrame({'a': [1, 2] * 2, 'b_left': range(4)})
A:dask.tests.test_distributed.pdfr->pytest.importorskip('pandas').DataFrame({'a': [2, 1], 'b_right': range(2)})
A:dask.tests.test_distributed.dfl->pytest.importorskip('dask.dataframe', reason='Requires dask.dataframe').from_pandas(pdfl, npartitions=4)
A:dask.tests.test_distributed.dfr->pytest.importorskip('dask.dataframe', reason='Requires dask.dataframe').from_pandas(pdfr, npartitions=2)
A:dask.tests.test_distributed.num_update_graphs->distributed.utils_test.client.run_on_scheduler(lambda dask_scheduler: dask_scheduler._update_graph_count)
A:dask.tests.test_distributed.b->pytest.importorskip('dask.bag', reason='Requires dask.bag').range(100, npartitions=10)
A:dask.tests.test_distributed.da->pytest.importorskip('dask.array', reason='Requires dask.array')
A:dask.tests.test_distributed.np->pytest.importorskip('numpy')
A:dask.tests.test_distributed.A->pytest.importorskip('dask.array', reason='Requires dask.array').concatenate([da.from_delayed(f, shape=x.shape, dtype=x.dtype) for f in futures], axis=0)
A:dask.tests.test_distributed.xx->delayed(add)(x, x)
A:dask.tests.test_distributed.yy->delayed(add)(y, y)
A:dask.tests.test_distributed.xxyy->delayed(add)(xx, yy)
A:dask.tests.test_distributed.xxyy2->distributed.utils_test.client.persist(xxyy)
A:dask.tests.test_distributed.xxyy3->delayed(add)(xxyy2, 10)
A:dask.tests.test_distributed.a->pytest.importorskip('dask.array', reason='Requires dask.array').ones((3, 3), chunks=chunks)
A:dask.tests.test_distributed.a2->pytest.importorskip('dask.array', reason='Requires dask.array').from_zarr(d)
A:dask.tests.test_distributed.zarr->pytest.importorskip('zarr')
A:dask.tests.test_distributed.z->pytest.importorskip('dask.array', reason='Requires dask.array').blockwise(f, 'i', x, 'i', y, 'ij', dtype=x.dtype, concatenate=True)
A:dask.tests.test_distributed.scale->varying([ZeroDivisionError('one'), ZeroDivisionError('two'), 2, 2])
A:dask.tests.test_distributed.darr->pytest.importorskip('dask.array', reason='Requires dask.array').ones((100,))
A:dask.tests.test_distributed.narr->pytest.importorskip('numpy').full(shape, 10)
A:dask.tests.test_distributed.dsk->dask.dataframe.optimize(ddf.dask, ddf.__dask_keys__())
A:dask.tests.test_distributed.futs->distributed.utils_test.client.scatter(parts)
A:dask.tests.test_distributed.ddf0->pytest.importorskip('dask.dataframe', reason='Requires dask.dataframe').from_pandas(df, npartitions=3)
A:dask.tests.test_distributed.fn->str(tmpdir.join('h5'))
A:dask.tests.test_distributed.series_len->len(series)
A:dask.tests.test_distributed.arr->pytest.importorskip('dask.array', reason='Requires dask.array').ones((1,), chunks=1).persist()
A:dask.tests.test_distributed.u->pytest.importorskip('dask.array', reason='Requires dask.array').from_array(np.arange(3))
A:dask.tests.test_distributed.v->pytest.importorskip('dask.array', reason='Requires dask.array').from_array(np.array([10 + 2j, 7 - 3j, 8 + 1j]))
A:dask.tests.test_distributed.cv->pytest.importorskip('dask.array', reason='Requires dask.array').from_array(np.array([10 + 2j, 7 - 3j, 8 + 1j])).conj()
A:dask.tests.test_distributed.(cv,)->dask.optimize(cv)
A:dask.tests.test_distributed.expected->pytest.importorskip('numpy').array([0 + 0j, 7 + 3j, 16 - 2j])
A:dask.tests.test_distributed.x_value->pytest.importorskip('dask.array', reason='Requires dask.array').from_array(np.array([0, 1, 2])).compute()
A:dask.tests.test_distributed.y_value->pytest.importorskip('dask.array', reason='Requires dask.array').from_array(np.array([[0, 1, 2]])).compute()
A:dask.tests.test_distributed.ddf['day']->dask.datasets.timeseries(start='2000-01-01', end='2000-07-01', freq='12h').enter_time.dt.day_name()
A:dask.tests.test_distributed.ddf2->dask.datasets.timeseries(start='2000-01-01', end='2000-07-01', freq='12h').shuffle('a', shuffle_method='tasks', max_branch=max_branch)
A:dask.tests.test_distributed.datasets->pytest.importorskip('dask.datasets')
A:dask.tests.test_distributed.item_df->pytest.importorskip('dask.dataframe', reason='Requires dask.dataframe').from_pandas(pd.DataFrame({'a': range(10)}), npartitions=1)
A:dask.tests.test_distributed.merged_df->merged_df.shuffle(on='b', shuffle_method='tasks').shuffle(on='b', shuffle_method='tasks')
A:dask.tests.test_distributed.series->pytest.importorskip('pandas').Series(data=[1] * N, index=range(2, N + 2))
A:dask.tests.test_distributed.dask_series->pytest.importorskip('dask.dataframe', reason='Requires dask.dataframe').from_pandas(series, npartitions=34)
A:dask.tests.test_distributed.db->pytest.importorskip('dask.bag', reason='Requires dask.bag')
A:dask.tests.test_distributed.dbag->pytest.importorskip('dask.bag', reason='Requires dask.bag').range(100, npartitions=2)
A:dask.tests.test_distributed.sc->dask.base.get_scheduler()
A:dask.tests.test_distributed.b2->pytest.importorskip('dask.bag', reason='Requires dask.bag').range(100, npartitions=10).groupby(lambda x: x % 13)
dask.tests.test_distributed.test_annotations_blockwise_unpack(c,s,a,b)
dask.tests.test_distributed.test_await(c,s,a,b)
dask.tests.test_distributed.test_bag_groupby_default(c,s,a,b)
dask.tests.test_distributed.test_blockwise_array_creation(c,io,fuse)
dask.tests.test_distributed.test_blockwise_concatenate(c)
dask.tests.test_distributed.test_blockwise_dataframe_io(c,tmpdir,io,fuse,from_futures)
dask.tests.test_distributed.test_blockwise_different_optimization(c)
dask.tests.test_distributed.test_blockwise_fusion_after_compute(c)
dask.tests.test_distributed.test_blockwise_numpy_args(c,s,a,b)
dask.tests.test_distributed.test_blockwise_numpy_kwargs(c,s,a,b)
dask.tests.test_distributed.test_can_import_client()
dask.tests.test_distributed.test_can_import_nested_things()
dask.tests.test_distributed.test_combo_of_layer_types(c,s,a,b)
dask.tests.test_distributed.test_dataframe_broadcast_merge(c,on,broadcast)
dask.tests.test_distributed.test_default_scheduler_on_worker(c,computation,use_distributed,scheduler)
dask.tests.test_distributed.test_from_delayed_dataframe(c)
dask.tests.test_distributed.test_fused_blockwise_dataframe_merge(c,fuse)
dask.tests.test_distributed.test_futures_in_graph(c)
dask.tests.test_distributed.test_futures_in_subgraphs(c,s,a,b)
dask.tests.test_distributed.test_futures_to_delayed_array(c)
dask.tests.test_distributed.test_futures_to_delayed_bag(c)
dask.tests.test_distributed.test_futures_to_delayed_dataframe(c)
dask.tests.test_distributed.test_get_scheduler_default_client_config_interleaving(s)
dask.tests.test_distributed.test_get_scheduler_lock(scheduler,expected_classes)
dask.tests.test_distributed.test_get_scheduler_lock_distributed(c,multiprocessing_method)
dask.tests.test_distributed.test_get_scheduler_with_distributed_active(c)
dask.tests.test_distributed.test_get_scheduler_with_distributed_active_reset_config(c)
dask.tests.test_distributed.test_get_scheduler_without_distributed_raises()
dask.tests.test_distributed.test_local_get_with_distributed_active(c,s,a,b)
dask.tests.test_distributed.test_local_scheduler()
dask.tests.test_distributed.test_map_partitions_da_input(c,s,a,b)
dask.tests.test_distributed.test_map_partitions_df_input()
dask.tests.test_distributed.test_map_partitions_partition_info(c,s,a,b)
dask.tests.test_distributed.test_non_recursive_df_reduce(c,s,a,b)
dask.tests.test_distributed.test_persist(c,s,a,b)
dask.tests.test_distributed.test_persist_nested(c)
dask.tests.test_distributed.test_scheduler_equals_client(c)
dask.tests.test_distributed.test_serializable_groupby_agg(c,s,a,b)
dask.tests.test_distributed.test_set_index_no_resursion_error(c)
dask.tests.test_distributed.test_shuffle_priority(c,s,a,b,max_branch,expected_layer_type)
dask.tests.test_distributed.test_to_hdf_distributed(c)
dask.tests.test_distributed.test_to_hdf_scheduler_distributed(npartitions,c)
dask.tests.test_distributed.test_to_sql_engine_kwargs(c,s,a,b)
dask.tests.test_distributed.test_write_single_hdf(c,lock_param)
dask.tests.test_distributed.test_zarr_distributed_roundtrip(c)
dask.tests.test_distributed.test_zarr_in_memory_distributed_err(c)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/tests/test_compatibility.py----------------------------------------
dask.tests.test_compatibility.test_deprecation()
dask.tests.test_compatibility.test_entry_points()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/tests/test_datasets.py----------------------------------------
A:dask.tests.test_datasets.b->dask.datasets.make_people(seed=123)
A:dask.tests.test_datasets.a->dask.datasets.make_people(seed=123)
dask.tests.test_datasets.test_deterministic()
dask.tests.test_datasets.test_full_dataset()
dask.tests.test_datasets.test_make_dataset_with_processes()
dask.tests.test_datasets.test_mimesis()
dask.tests.test_datasets.test_no_mimesis()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/tests/test_core.py----------------------------------------
A:dask.tests.test_core.np->pytest.importorskip('numpy')
A:dask.tests.test_core.one->pytest.importorskip('numpy').int64(1)
A:dask.tests.test_core.f->namedtuple('f', ['x', 'y'])
A:dask.tests.test_core.get->staticmethod(get)
A:dask.tests.test_core.custom_testget->TestCustomGetPass()
A:dask.tests.test_core.s->get_dependencies(dsk, task=[], as_list=True)
A:dask.tests.test_core.(dependencies, dependents)->get_deps(dsk)
A:dask.tests.test_core.a->MutateOnEq()
A:dask.tests.test_core.task->F()
A:dask.tests.test_core.df->pandas.DataFrame()
A:dask.tests.test_core.l->literal((add, 1, 2))
dask.tests.test_core.MutateOnEq
dask.tests.test_core.MutateOnEq.__eq__(self,other)
dask.tests.test_core.TestGet(GetFunctionTestMixin)
dask.tests.test_core.contains(a,b)
dask.tests.test_core.test_GetFunctionTestMixin_class()
dask.tests.test_core.test_flatten()
dask.tests.test_core.test_get_dependencies_empty()
dask.tests.test_core.test_get_dependencies_list()
dask.tests.test_core.test_get_dependencies_many()
dask.tests.test_core.test_get_dependencies_nested()
dask.tests.test_core.test_get_dependencies_nothing()
dask.tests.test_core.test_get_dependencies_task()
dask.tests.test_core.test_get_dependencies_task_none()
dask.tests.test_core.test_get_deps()
dask.tests.test_core.test_getcycle()
dask.tests.test_core.test_has_tasks()
dask.tests.test_core.test_ishashable()
dask.tests.test_core.test_iskey()
dask.tests.test_core.test_iskey_numpy_types()
dask.tests.test_core.test_istask()
dask.tests.test_core.test_literal_serializable()
dask.tests.test_core.test_preorder_traversal()
dask.tests.test_core.test_quote()
dask.tests.test_core.test_subs()
dask.tests.test_core.test_subs_arbitrary_key()
dask.tests.test_core.test_subs_no_key_data_eq()
dask.tests.test_core.test_subs_with_surprisingly_friendly_eq()
dask.tests.test_core.test_subs_with_unfriendly_eq()
dask.tests.test_core.test_validate_key()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/tests/test_layers.py----------------------------------------
A:dask.tests.test_layers.self.start_modules->set()
A:dask.tests.test_layers.dac->pytest.importorskip('dask.array.core')
A:dask.tests.test_layers.shape->tuple((d * n for n in chunk))
A:dask.tests.test_layers.chunks->pytest.importorskip('dask.array.core').normalize_chunks(chunk, shape)
A:dask.tests.test_layers.array_deps->ArraySliceDep(chunks)
A:dask.tests.test_layers.pd->pytest.importorskip('pandas')
A:dask.tests.test_layers.dd->pytest.importorskip('dask.dataframe')
A:dask.tests.test_layers.df->pytest.importorskip('dask.dataframe').from_pandas(pd.DataFrame({'a': range(10)}), npartitions=2)
A:dask.tests.test_layers.ddf1->pytest.importorskip('dask.dataframe').read_parquet(str(tmpdir), filters=filters)
A:dask.tests.test_layers.ddf2->pytest.importorskip('dask.dataframe').from_pandas(df, npartitions=1)
A:dask.tests.test_layers.da->pytest.importorskip('dask.array')
A:dask.tests.test_layers.array->pytest.importorskip('dask.array').ones((100,))
A:dask.tests.test_layers.fs->fractional_slice(('x', 4.9), {0: 2})
A:dask.tests.test_layers.end_modules->c.run_on_scheduler(lambda : set(sys.modules))
A:dask.tests.test_layers.start_modules->c.run_on_scheduler(lambda dask_scheduler: dask_scheduler.plugins[SchedulerImportCheck.name].start_modules)
A:dask.tests.test_layers.datasets->pytest.importorskip('dask.datasets')
A:dask.tests.test_layers.result->pytest.importorskip('dask.dataframe').core.new_dd_object(dsk, name, ddf._meta, ddf.divisions)
A:dask.tests.test_layers.culled_graph->graph.cull(culled_keys)
A:dask.tests.test_layers.ddf->pytest.importorskip('dask.datasets').timeseries(end='2000-01-15')
A:dask.tests.test_layers.dsk->dask.highlevelgraph.HighLevelGraph.from_collections(name, dsk, dependencies=[ddf])
A:dask.tests.test_layers.cached_deps->graph.cull(culled_keys).key_dependencies.copy()
A:dask.tests.test_layers.deps->graph.cull(culled_keys).get_all_dependencies()
A:dask.tests.test_layers.deps0->graph.get_all_dependencies()
dask.tests.test_layers.SchedulerImportCheck(self,pattern)
dask.tests.test_layers.SchedulerImportCheck.__init__(self,pattern)
dask.tests.test_layers.SchedulerImportCheck.start(self,scheduler)
dask.tests.test_layers._array_creation(tmpdir)
dask.tests.test_layers._array_map_overlap(tmpdir)
dask.tests.test_layers._dataframe_broadcast_join(tmpdir)
dask.tests.test_layers._dataframe_shuffle(tmpdir)
dask.tests.test_layers._dataframe_tree_reduction(tmpdir)
dask.tests.test_layers._groupby_op(ddf)
dask.tests.test_layers._pq_fastparquet(tmpdir)
dask.tests.test_layers._pq_pyarrow(tmpdir)
dask.tests.test_layers._read_csv(tmpdir)
dask.tests.test_layers._shuffle_op(ddf)
dask.tests.test_layers.test_array_chunk_shape_dep()
dask.tests.test_layers.test_array_slice_deps()
dask.tests.test_layers.test_dataframe_cull_key_dependencies(op)
dask.tests.test_layers.test_dataframe_cull_key_dependencies_materialized()
dask.tests.test_layers.test_fractional_slice()
dask.tests.test_layers.test_scheduler_highlevel_graph_unpack_import(op,lib,optimize_graph,loop,tmpdir)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/tests/test_order.py----------------------------------------
A:dask.tests.test_order.dsk->dict(graph)
A:dask.tests.test_order.o->order(dsk, dependencies=dependencies)
A:dask.tests.test_order.(dependencies, dependents)->get_deps(dsk)
A:dask.tests.test_order.x_keys->sorted(dsk)
A:dask.tests.test_order.da->pytest.importorskip('dask.array')
A:dask.tests.test_order.origx->pytest.importorskip('dask.array').random.normal(size=(A, B), chunks=(1, None))
A:dask.tests.test_order.y->(x[:, None, :] * x[:, :, None]).cumsum(axis=0)
A:dask.tests.test_order.x->pytest.importorskip('dask.array').blockwise(random, 'yx', new_axes={'y': (10,) * n_reducers, 'x': (10,) * n_reducers}, dtype=float, **trivial_deps)
A:dask.tests.test_order.w->(y * x[:, None]).sum(axis=(1, 2))
A:dask.tests.test_order.As->sorted((val for ((letter, _), val) in o.items() if letter == a))
A:dask.tests.test_order.Bs->sorted((val for ((letter, _), val) in o.items() if letter == b))
A:dask.tests.test_order.actual->sorted((v for ((letter, _), v) in o.items() if letter in {d, dd, e, ee}))
A:dask.tests.test_order.val->abs(o[x, 5, i, 1] - o[x, 6, i, 0])
A:dask.tests.test_order.dsk2->dict(dsk)
A:dask.tests.test_order.dsk3->dict(dsk)
A:dask.tests.test_order.dsk4->dict(dsk3)
A:dask.tests.test_order.zarr->pytest.importorskip('zarr')
A:dask.tests.test_order.store->pytest.importorskip('zarr').DirectoryStore(tmpdir)
A:dask.tests.test_order.root->pytest.importorskip('zarr').group(store, overwrite=True)
A:dask.tests.test_order.dest->pytest.importorskip('zarr').group(store, overwrite=True).empty_like(name='dest', data=x, chunks=x.chunksize, overwrite=True)
A:dask.tests.test_order.d->pytest.importorskip('dask.array').blockwise(random, 'yx', new_axes={'y': (10,) * n_reducers, 'x': (10,) * n_reducers}, dtype=float, **trivial_deps).store(dest, lock=False, compute=False)
A:dask.tests.test_order.first_store->min(stores, key=lambda k: o[k])
A:dask.tests.test_order.connected_max->max((v for (k, v) in o.items() if k in connected_stores))
A:dask.tests.test_order.disconnected_min->min((v for (k, v) in o.items() if k in disconnected_stores))
A:dask.tests.test_order.(_, pressure)->diagnostics(dsk)
A:dask.tests.test_order.(info, memory_over_time)->diagnostics(dsk)
A:dask.tests.test_order.(a, b, c, d, e)->list('abcde')
A:dask.tests.test_order.xr->pytest.importorskip('xarray')
A:dask.tests.test_order.ds->pytest.importorskip('xarray').Dataset(data_vars=dict(a=(('x', 'y'), np.arange(80).reshape(8, 10)), b=('y', np.arange(10)))).chunk(x=1)
A:dask.tests.test_order.mean->quad.mean('time')
A:dask.tests.test_order.diag_array->diagnostics(collections_to_dsk([mean], optimize_graph=optimize))
A:dask.tests.test_order.diag_df->diagnostics(collections_to_dsk([mean.to_dask_dataframe()], optimize_graph=optimize))
A:dask.tests.test_order.np->pytest.importorskip('numpy')
A:dask.tests.test_order.data->pytest.importorskip('dask.array').random.random((200, 1), chunks=(1, -1))
A:dask.tests.test_order.arr->pytest.importorskip('xarray').DataArray(data, dims=['time', 'x'], coords={'day': ('time', np.arange(data.shape[0]) % ngroups)})
A:dask.tests.test_order.clim->pytest.importorskip('xarray').DataArray(data, dims=['time', 'x'], coords={'day': ('time', np.arange(data.shape[0]) % ngroups)}).groupby('day').mean(dim='time')
A:dask.tests.test_order.anom_mean->anom.mean(dim='time')
A:dask.tests.test_order.graph->pytest.importorskip('xarray').Dataset(data_vars=dict(a=(('x', 'y'), np.arange(80).reshape(8, 10)), b=('y', np.arange(10)))).chunk(x=1).map_blocks(f).__dask_graph__()
A:dask.tests.test_order.(diags, pressure)->diagnostics(graph)
A:dask.tests.test_order.count_dependents->defaultdict(set)
A:dask.tests.test_order.n_splits->max(count_dependents)
A:dask.tests.test_order.max_age_mean_chunks->max(ages_mean_chunks.values())
A:dask.tests.test_order.max_age_transpose->max(ages_tranpose.values())
A:dask.tests.test_order.first_pressure->max(diagnostics(first)[1])
A:dask.tests.test_order.second_pressure->max(diagnostics(other)[1])
A:dask.tests.test_order.of1->list((o[g, 1, ix] for ix in range(3)))
A:dask.tests.test_order.of2->list((o[g, 2, ix] for ix in range(3)))
A:dask.tests.test_order.before->len(dsk)
A:dask.tests.test_order.before_dsk->dict(graph).copy()
A:dask.tests.test_order.(con_r, _)->_connecting_to_roots(dependencies, dependents)
A:dask.tests.test_order.final_nodes->sorted([(d, ix, jx) for ix in range(2) for jx in range(2)], key=o.__getitem__)
A:dask.tests.test_order.shared_root->shared_roots.pop()
A:dask.tests.test_order.(connected_roots, max_dependents)->_connecting_to_roots(dependencies, dependents)
A:dask.tests.test_order.(dependencies, __build_class__)->get_deps(dsk)
A:dask.tests.test_order.dependencies_copy->dependencies.copy()
A:dask.tests.test_order.dsk_copy->dict(graph).copy()
dask.tests.test_order.abcde(request)
dask.tests.test_order.assert_topological_sort(dsk,order)
dask.tests.test_order.f(*args)
dask.tests.test_order.test_anom_mean()
dask.tests.test_order.test_anom_mean_raw(abcde)
dask.tests.test_order.test_array_store_final_order(tmpdir)
dask.tests.test_order.test_array_vs_dataframe(optimize)
dask.tests.test_order.test_avoid_broker_nodes(abcde)
dask.tests.test_order.test_avoid_upwards_branching(abcde)
dask.tests.test_order.test_avoid_upwards_branching_complex(abcde)
dask.tests.test_order.test_base_of_reduce_preferred(abcde)
dask.tests.test_order.test_break_ties_by_str(abcde)
dask.tests.test_order.test_connecting_to_roots_asym()
dask.tests.test_order.test_connecting_to_roots_single_root()
dask.tests.test_order.test_connecting_to_roots_tree_reduction()
dask.tests.test_order.test_deep_bases_win_over_dependents(abcde)
dask.tests.test_order.test_diagnostics(abcde)
dask.tests.test_order.test_do_not_mutate_input()
dask.tests.test_order.test_dont_run_all_dependents_too_early(abcde)
dask.tests.test_order.test_doublediff(abcde)
dask.tests.test_order.test_eager_to_compute_dependent_to_free_parent()
dask.tests.test_order.test_favor_longest_critical_path(abcde)
dask.tests.test_order.test_flaky_array_reduction()
dask.tests.test_order.test_flox_reduction(abcde)
dask.tests.test_order.test_gh_3055()
dask.tests.test_order.test_gh_3055_explicit(abcde)
dask.tests.test_order.test_local_parents_of_reduction(abcde)
dask.tests.test_order.test_many_branches_use_ndependencies(abcde)
dask.tests.test_order.test_map_overlap(abcde)
dask.tests.test_order.test_nearest_neighbor(abcde)
dask.tests.test_order.test_order_cycle()
dask.tests.test_order.test_order_doesnt_fail_on_mixed_type_keys(abcde)
dask.tests.test_order.test_order_empty()
dask.tests.test_order.test_order_flox_reduction_2(abcde)
dask.tests.test_order.test_order_with_equal_dependents(abcde)
dask.tests.test_order.test_ordering_keeps_groups_together(abcde)
dask.tests.test_order.test_prefer_deep(abcde)
dask.tests.test_order.test_prefer_short_ancestor(abcde)
dask.tests.test_order.test_prefer_short_narrow(abcde)
dask.tests.test_order.test_recursion_depth_long_linear_chains()
dask.tests.test_order.test_reduce_with_many_common_dependents(optimize,keep_self,ndeps,n_reducers)
dask.tests.test_order.test_run_smaller_sections(abcde)
dask.tests.test_order.test_stacklimit(abcde)
dask.tests.test_order.test_string_ordering()
dask.tests.test_order.test_string_ordering_dependents()
dask.tests.test_order.test_terminal_node_backtrack()
dask.tests.test_order.test_type_comparisions_ok(abcde)
dask.tests.test_order.test_use_structure_not_keys(abcde)
dask.tests.test_order.test_xarray_8414()
dask.tests.test_order.test_xarray_like_reduction()
dask.tests.test_order.visualize(dsk,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/tests/test_highgraph.py----------------------------------------
A:dask.tests.test_highgraph.da->pytest.importorskip('dask.array')
A:dask.tests.test_highgraph.fn->str(tmpdir)
A:dask.tests.test_highgraph.a->pytest.importorskip('dask.array').ones((1000, 1000), chunks=(100, 100))
A:dask.tests.test_highgraph.hg->HighLevelGraph(layers, dependencies)
A:dask.tests.test_highgraph.h1->HighLevelGraph({'a': {'a': 'b'}, 'b': {'b': 1}}, {'a': {'b'}, 'b': set()})
A:dask.tests.test_highgraph.h2->HighLevelGraph({'a': {'a': 'b'}, 'b': {'b': 1}}, {'a': {'b'}, 'b': set()}).copy()
A:dask.tests.test_highgraph.v1->getattr(h1, k)
A:dask.tests.test_highgraph.v2->getattr(h2, k)
A:dask.tests.test_highgraph.culled_by_x->HighLevelGraph(layers, dependencies).cull({'x'})
A:dask.tests.test_highgraph.culled_by_y->HighLevelGraph(layers, dependencies).cull([[['y']]])
A:dask.tests.test_highgraph.expect->HighLevelGraph({k: dict(v) for (k, v) in hg.layers.items() if k != 'c'}, {k: set(v) for (k, v) in hg.dependencies.items() if k != 'c'})
A:dask.tests.test_highgraph.culled->HighLevelGraph(layers, dependencies).cull_layers(['a', 'b'])
A:dask.tests.test_highgraph.A->pytest.importorskip('dask.array').ones((10, 10), chunks=(5, 5))
A:dask.tests.test_highgraph.layer->MaterializedLayer({'a': 42, 'b': 3.14}, annotations={'foo': 'bar'})
A:dask.tests.test_highgraph.(culled_layer, _)->MaterializedLayer({'a': 42, 'b': 3.14}, annotations={'foo': 'bar'}).cull({'a'}, [])
A:dask.tests.test_highgraph.b1->threading.Barrier(2)
A:dask.tests.test_highgraph.b2->threading.Barrier(2)
A:dask.tests.test_highgraph.out->dask.get_annotations()
A:dask.tests.test_highgraph.f1->ex.submit(f, 1)
A:dask.tests.test_highgraph.f2->ex.submit(f, 2)
A:dask.tests.test_highgraph.np->pytest.importorskip('numpy')
A:dask.tests.test_highgraph.x->pytest.importorskip('dask.array').outer(x, y).transpose()
A:dask.tests.test_highgraph.y->pytest.importorskip('dask.array').from_array(np.arange(10).reshape((10,)), (4,))
A:dask.tests.test_highgraph.dsk->pytest.importorskip('dask.array').outer(x, y).transpose().__dask_graph__()
A:dask.tests.test_highgraph.dsk_cull->pytest.importorskip('dask.array').outer(x, y).transpose().__dask_graph__().cull(keys)
A:dask.tests.test_highgraph.out_keys->MaterializedLayer({'a': 42, 'b': 3.14}, annotations={'foo': 'bar'}).get_output_keys()
A:dask.tests.test_highgraph.b->Blockwise(output='b', output_indices=tuple('ij'), dsk={'b': [[blockwise_token(0)]]}, indices=(), numblocks={}, new_axes={'i': (1, 1, 1), 'j': (1, 1)})
A:dask.tests.test_highgraph.c->Blockwise(output='b', output_indices=tuple('ij'), dsk={'b': [[blockwise_token(0)]]}, indices=(), numblocks={}, new_axes={'i': (1, 1, 1), 'j': (1, 1)}).sum(axis=1)
A:dask.tests.test_highgraph.g->to_graphviz(hg)
A:dask.tests.test_highgraph.end->MaterializedLayer({'a': 42, 'b': 3.14}, annotations={'foo': 'bar'}).find('"', start)
dask.tests.test_highgraph.annot_map_fn(key)
dask.tests.test_highgraph.test_annotation_cleared_on_error()
dask.tests.test_highgraph.test_annotations_leak()
dask.tests.test_highgraph.test_basic()
dask.tests.test_highgraph.test_blockwise_cull(flat)
dask.tests.test_highgraph.test_copy()
dask.tests.test_highgraph.test_cull()
dask.tests.test_highgraph.test_cull_layers()
dask.tests.test_highgraph.test_getitem()
dask.tests.test_highgraph.test_keys_values_items_to_dict_methods()
dask.tests.test_highgraph.test_len_does_not_materialize()
dask.tests.test_highgraph.test_materializedlayer_cull_preserves_annotations()
dask.tests.test_highgraph.test_multiple_annotations()
dask.tests.test_highgraph.test_node_tooltips_exist()
dask.tests.test_highgraph.test_repr_html_hlg_layers()
dask.tests.test_highgraph.test_single_annotation(annotation)
dask.tests.test_highgraph.test_visualize(tmpdir)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/tests/test_hashing.py----------------------------------------
A:dask.tests.test_hashing.np->pytest.importorskip('numpy')
A:dask.tests.test_hashing.h->hasher(x)
dask.tests.test_hashing.test_hash_buffer(x)
dask.tests.test_hashing.test_hash_buffer_hex(x)
dask.tests.test_hashing.test_hashers(hasher)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/tests/test_config.py----------------------------------------
A:dask.tests.test_config.c->merge(a, b)
A:dask.tests.test_config.config->collect_env({'DASK_INTERNAL_INHERIT_CONFIG': serialize({'array': {'svg': {'size': 150}}})})
A:dask.tests.test_config.perm_orig->stat.S_IMODE(os.stat(path).st_mode)
A:dask.tests.test_config.dir_path->str(tmpdir)
A:dask.tests.test_config.a_path->os.path.join(dir_path, 'a.yaml')
A:dask.tests.test_config.b_path->os.path.join(dir_path, 'b.yaml')
A:dask.tests.test_config.fil_path->os.path.join(dir_path, 'a.yaml')
A:dask.tests.test_config.res->collect_env(env)
A:dask.tests.test_config.source->os.path.join(str(tmpdir), 'source.yaml')
A:dask.tests.test_config.dest->os.path.join(str(tmpdir), 'dest')
A:dask.tests.test_config.destination->os.path.join(str(tmpdir), 'dask')
A:dask.tests.test_config.result->yaml.safe_load(f)
A:dask.tests.test_config.text->f.read()
A:dask.tests.test_config.[fn]->os.listdir(destination)
A:dask.tests.test_config.jsonschema->pytest.importorskip('jsonschema')
A:dask.tests.test_config.config_fn->os.path.join(os.path.dirname(__file__), '..', 'dask.yaml')
A:dask.tests.test_config.schema_fn->os.path.join(os.path.dirname(__file__), '..', 'dask-schema.yaml')
A:dask.tests.test_config.schema->yaml.safe_load(f)
A:dask.tests.test_config.serialized->serialize({'array': {'svg': {'size': 150}}})
A:dask.tests.test_config.paths->_get_paths()
A:dask.tests.test_config.prefix->os.path.join('include', 'this', 'path')
dask.tests.test_config.no_read_permissions(path)
dask.tests.test_config.test__get_paths(monkeypatch)
dask.tests.test_config.test_canonical_name()
dask.tests.test_config.test_collect()
dask.tests.test_config.test_collect_env_none(monkeypatch)
dask.tests.test_config.test_collect_yaml_dir()
dask.tests.test_config.test_collect_yaml_malformed_file(tmpdir)
dask.tests.test_config.test_collect_yaml_no_top_level_dict(tmpdir)
dask.tests.test_config.test_collect_yaml_paths()
dask.tests.test_config.test_collect_yaml_permission_errors(tmpdir,kind)
dask.tests.test_config.test_config_inheritance()
dask.tests.test_config.test_config_serialization()
dask.tests.test_config.test_core_file()
dask.tests.test_config.test_default_search_paths()
dask.tests.test_config.test_deprecations_on_env_variables(monkeypatch)
dask.tests.test_config.test_deprecations_on_set(args,kwargs)
dask.tests.test_config.test_deprecations_on_yaml(tmp_path,key)
dask.tests.test_config.test_ensure_file(tmpdir)
dask.tests.test_config.test_ensure_file_defaults_to_DASK_CONFIG_directory(tmpdir)
dask.tests.test_config.test_ensure_file_directory(mkdir,tmpdir)
dask.tests.test_config.test_env()
dask.tests.test_config.test_env_none_values()
dask.tests.test_config.test_env_var_canonical_name(monkeypatch)
dask.tests.test_config.test_expand_environment_variables(monkeypatch,inp,out)
dask.tests.test_config.test_get()
dask.tests.test_config.test_get_override_with()
dask.tests.test_config.test_get_set_canonical_name()
dask.tests.test_config.test_get_set_roundtrip(key)
dask.tests.test_config.test_merge()
dask.tests.test_config.test_merge_None_to_dict()
dask.tests.test_config.test_pop()
dask.tests.test_config.test_refresh()
dask.tests.test_config.test_rename()
dask.tests.test_config.test_schema()
dask.tests.test_config.test_schema_is_complete()
dask.tests.test_config.test_set()
dask.tests.test_config.test_set_hard_to_copyables()
dask.tests.test_config.test_set_kwargs()
dask.tests.test_config.test_set_nested()
dask.tests.test_config.test_update()
dask.tests.test_config.test_update_defaults()
dask.tests.test_config.test_update_dict_to_list()
dask.tests.test_config.test_update_list_to_dict()
dask.tests.test_config.test_update_new_defaults()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/tests/warning_aliases.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/tests/test_callbacks.py----------------------------------------
A:dask.tests.test_callbacks.inner_callback->MyCallback()
A:dask.tests.test_callbacks.outer_callback->MyCallback()
dask.tests.test_callbacks.test_add_remove_mutates_not_replaces()
dask.tests.test_callbacks.test_finish_always_called()
dask.tests.test_callbacks.test_nested_schedulers()
dask.tests.test_callbacks.test_start_callback()
dask.tests.test_callbacks.test_start_state_callback()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/tests/test_system.py----------------------------------------
A:dask.tests.test_system.psutil->pytest.importorskip('psutil')
A:dask.tests.test_system.count->cpu_count()
dask.tests.test_system.test_cpu_count()
dask.tests.test_system.test_cpu_count_cgroups(dirname,monkeypatch)
dask.tests.test_system.test_cpu_count_cgroups_v2(quota,group_name,monkeypatch)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/tests/test_backends.py----------------------------------------
A:dask.tests.test_backends.dd->pytest.importorskip('dask.dataframe')
A:dask.tests.test_backends.msg->str(excinfo.value)
dask.tests.test_backends.test_CreationDispatch_error_informative_message(backend)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/tests/test_utils.py----------------------------------------
A:dask.tests.test_utils.result->parse_timedelta(text)
A:dask.tests.test_utils.np->pytest.importorskip('numpy')
A:dask.tests.test_utils.pa->pytest.importorskip('pyarrow')
A:dask.tests.test_utils.buf->pytest.importorskip('pyarrow').py_buffer(b'123')
A:dask.tests.test_utils.a->SerializableLock('a')
A:dask.tests.test_utils.func2->functools.partial(func, 2)
A:dask.tests.test_utils.foo->Dispatch()
A:dask.tests.test_utils.b->SerializableLock('b')
A:dask.tests.test_utils.state->pytest.importorskip('numpy').random.RandomState(seed)
A:dask.tests.test_utils.states->random_state_data(10, 1234)
A:dask.tests.test_utils.states2->random_state_data(n, state)
A:dask.tests.test_utils.f->methodcaller('count')
A:dask.tests.test_utils.res->stringify_collection_keys(obj)
A:dask.tests.test_utils.a2->pickle.loads(pickle.dumps(a))
A:dask.tests.test_utils.a3->pickle.loads(pickle.dumps(a))
A:dask.tests.test_utils.a4->pickle.loads(pickle.dumps(a2))
A:dask.tests.test_utils.b2->pickle.loads(pickle.dumps(b))
A:dask.tests.test_utils.b3->pickle.loads(pickle.dumps(b2))
A:dask.tests.test_utils.c->SerializableLock('a')
A:dask.tests.test_utils.d->SerializableLock()
A:dask.tests.test_utils.md->pytest.importorskip('multipledispatch')
A:dask.tests.test_utils.vfunc->pytest.importorskip('numpy').vectorize(func)
A:dask.tests.test_utils.func->functools.partial(np.add, out=None)
A:dask.tests.test_utils.d2->ensure_dict(d, copy=True)
A:dask.tests.test_utils.d3->ensure_dict(HighLevelGraph.from_collections('x', d))
A:dask.tests.test_utils.d4->ensure_dict(mydict(d))
A:dask.tests.test_utils.s2->ensure_set(s, copy=True)
A:dask.tests.test_utils.s3->ensure_set(myset(s))
A:dask.tests.test_utils.g->itemgetter(1)
A:dask.tests.test_utils.g2->pickle.loads(pickle.dumps(g))
A:dask.tests.test_utils.bar->functools.partial(foo, a=1)
A:dask.tests.test_utils.dd->pytest.importorskip('dask.dataframe')
A:dask.tests.test_utils.keys->list(dsk)
A:dask.tests.test_utils.dsk->stringify(dsk, exclusive=set(dsk) | {('z', 1)})
A:dask.tests.test_utils.msg->str(record[0].message)
A:dask.tests.test_utils.instance->MyType()
A:dask.tests.test_utils.x->cached_cumsum(a)
A:dask.tests.test_utils.y->cached_cumsum(a, initial_zero=True)
A:dask.tests.test_utils.pd->pytest.importorskip('pandas')
A:dask.tests.test_utils.da->pytest.importorskip('dask.array')
A:dask.tests.test_utils.cp->pytest.importorskip('cupy')
A:dask.tests.test_utils.cudf->pytest.importorskip('cudf')
dask.tests.test_utils.MyType
dask.tests.test_utils.test_SerializableLock()
dask.tests.test_utils.test_SerializableLock_acquire_blocking()
dask.tests.test_utils.test_SerializableLock_locked()
dask.tests.test_utils.test_SerializableLock_name_collision()
dask.tests.test_utils.test_asciitable()
dask.tests.test_utils.test_cached_cumsum()
dask.tests.test_utils.test_cached_cumsum_nan()
dask.tests.test_utils.test_cached_cumsum_non_tuple()
dask.tests.test_utils.test_deprecated()
dask.tests.test_utils.test_deprecated_after_version()
dask.tests.test_utils.test_deprecated_category()
dask.tests.test_utils.test_deprecated_message()
dask.tests.test_utils.test_deprecated_version()
dask.tests.test_utils.test_derived_from()
dask.tests.test_utils.test_derived_from_dask_dataframe()
dask.tests.test_utils.test_derived_from_func()
dask.tests.test_utils.test_derived_from_prop_cached_prop(decorator)
dask.tests.test_utils.test_dispatch()
dask.tests.test_utils.test_dispatch_kwargs()
dask.tests.test_utils.test_dispatch_lazy()
dask.tests.test_utils.test_dispatch_lazy_walks_mro()
dask.tests.test_utils.test_dispatch_variadic_on_first_argument()
dask.tests.test_utils.test_ensure_bytes()
dask.tests.test_utils.test_ensure_bytes_ndarray()
dask.tests.test_utils.test_ensure_bytes_pyarrow_buffer()
dask.tests.test_utils.test_ensure_dict()
dask.tests.test_utils.test_ensure_set()
dask.tests.test_utils.test_ensure_unicode()
dask.tests.test_utils.test_ensure_unicode_ndarray()
dask.tests.test_utils.test_ensure_unicode_pyarrow_buffer()
dask.tests.test_utils.test_extra_titles()
dask.tests.test_utils.test_format_bytes(n,expect)
dask.tests.test_utils.test_format_time()
dask.tests.test_utils.test_funcname()
dask.tests.test_utils.test_funcname_long()
dask.tests.test_utils.test_funcname_multipledispatch()
dask.tests.test_utils.test_funcname_numpy_vectorize()
dask.tests.test_utils.test_funcname_toolz()
dask.tests.test_utils.test_get_meta_library()
dask.tests.test_utils.test_get_meta_library_gpu()
dask.tests.test_utils.test_getargspec()
dask.tests.test_utils.test_has_keyword()
dask.tests.test_utils.test_is_arraylike()
dask.tests.test_utils.test_itemgetter()
dask.tests.test_utils.test_iter_chunks()
dask.tests.test_utils.test_memory_repr()
dask.tests.test_utils.test_method_caller()
dask.tests.test_utils.test_ndeepmap()
dask.tests.test_utils.test_parse_bytes()
dask.tests.test_utils.test_parse_timedelta()
dask.tests.test_utils.test_partial_by_order()
dask.tests.test_utils.test_random_state_data()
dask.tests.test_utils.test_skip_doctest()
dask.tests.test_utils.test_stringify()
dask.tests.test_utils.test_stringify_collection_keys()
dask.tests.test_utils.test_takes_multiple_arguments()
dask.tests.test_utils.test_tmpfile_naming()
dask.tests.test_utils.test_typename()
dask.tests.test_utils.test_typename_on_instances()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/tests/test_dot.py----------------------------------------
A:dask.tests.test_dot.ipycytoscape->pytest.importorskip('ipycytoscape')
A:dask.tests.test_dot.pytestmark->pytest.mark.skipif(True, reason='graphviz exception with Python -OO flag')
A:dask.tests.test_dot.ipython_not_installed_mark->pytest.mark.skipif(ipython_not_installed, reason='IPython not installed')
A:dask.tests.test_dot.label_re->re.compile('.*\\[label=(.*?) shape=(.*?)\\]')
A:dask.tests.test_dot.m->re.compile('.*\\[label=(.*?) shape=(.*?)\\]').match(line)
A:dask.tests.test_dot.result->dot_graph(dsk, filename=str(tmpdir.join(filename)), format=format)
A:dask.tests.test_dot.g->to_graphviz(dsk, verbose=True, collapse_outputs=True)
A:dask.tests.test_dot.labels->list(filter(None, map(get_label, g.body)))
A:dask.tests.test_dot.shapes->list(map(lambda x: x['data']['shape'], data['nodes']))
A:dask.tests.test_dot.data->_to_cytoscape_json(dsk, collapse_outputs=True, verbose=True)
A:dask.tests.test_dot.dsk2->dsk.copy()
A:dask.tests.test_dot.filename->str(tmpdir.join('$(touch should_not_get_created.txt)'))
A:dask.tests.test_dot.target->str(tmpdir.join('mydask.html'))
A:dask.tests.test_dot.before->tmpdir.listdir()
A:dask.tests.test_dot.after->tmpdir.listdir()
A:dask.tests.test_dot.x->delayed(f)(1, y=2)
A:dask.tests.test_dot.label->task_label(x.dask[x.key])
A:dask.tests.test_dot.attrs_func_test->copy.deepcopy(attrs_func)
A:dask.tests.test_dot.attrs_data_test->copy.deepcopy(attrs_data)
dask.tests.test_dot.get_label(line)
dask.tests.test_dot.get_shape(line)
dask.tests.test_dot.test__to_cytoscape_json_collapse_outputs()
dask.tests.test_dot.test__to_cytoscape_json_collapse_outputs_and_verbose()
dask.tests.test_dot.test__to_cytoscape_json_verbose()
dask.tests.test_dot.test_aliases()
dask.tests.test_dot.test_cytoscape_graph(tmpdir)
dask.tests.test_dot.test_cytoscape_graph_color()
dask.tests.test_dot.test_cytoscape_graph_custom(tmp_path)
dask.tests.test_dot.test_delayed_kwargs_apply()
dask.tests.test_dot.test_dot_graph(tmpdir,format,typ)
dask.tests.test_dot.test_dot_graph_defaults()
dask.tests.test_dot.test_dot_graph_no_filename(tmpdir,format,typ)
dask.tests.test_dot.test_filenames_and_formats(tmpdir,filename,format,target,expected_result_type)
dask.tests.test_dot.test_immutable_attributes(viz_func)
dask.tests.test_dot.test_label()
dask.tests.test_dot.test_task_label()
dask.tests.test_dot.test_to_graphviz()
dask.tests.test_dot.test_to_graphviz_attributes()
dask.tests.test_dot.test_to_graphviz_collapse_outputs()
dask.tests.test_dot.test_to_graphviz_collapse_outputs_and_verbose()
dask.tests.test_dot.test_to_graphviz_custom()
dask.tests.test_dot.test_to_graphviz_verbose()
dask.tests.test_dot.test_to_graphviz_with_unconnected_node()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/tests/test_typing.py----------------------------------------
A:dask.tests.test_typing.da->pytest.importorskip('dask.array')
A:dask.tests.test_typing.db->pytest.importorskip('dask.bag')
A:dask.tests.test_typing.dds->pytest.importorskip('dask.datasets')
A:dask.tests.test_typing.dd->pytest.importorskip('dask.dataframe')
A:dask.tests.test_typing.__dask_scheduler__->staticmethod(get2)
A:dask.tests.test_typing.__dask_optimize__->globalmethod(dont_optimize, key='collection_optim', falsey=dont_optimize)
A:dask.tests.test_typing.arr->pytest.importorskip('dask.array').ones(10)
A:dask.tests.test_typing.bag->pytest.importorskip('dask.bag').from_sequence([1, 2, 3, 4, 5], npartitions=2)
A:dask.tests.test_typing.df->pytest.importorskip('dask.datasets').timeseries()
A:dask.tests.test_typing.dobj->increment(2)
A:dask.tests.test_typing.a->pytest.importorskip('dask.array').ones(10)
A:dask.tests.test_typing.hlgc->HLGCollection(a)
A:dask.tests.test_typing.nhlgc->NotHLGCollection(a)
dask.tests.test_typing.HLGCollection(self,based_on:HLGDaskCollection)
dask.tests.test_typing.HLGCollection.__dask_graph__(self)->Graph
dask.tests.test_typing.HLGCollection.__dask_keys__(self)->NestedKeys
dask.tests.test_typing.HLGCollection.__dask_layers__(self)->Sequence[str]
dask.tests.test_typing.HLGCollection.__dask_postcompute__(self)->tuple[PostComputeCallable, tuple]
dask.tests.test_typing.HLGCollection.__dask_postpersist__(self)->tuple[PostPersistCallable, tuple]
dask.tests.test_typing.HLGCollection.__dask_tokenize__(self)->Hashable
dask.tests.test_typing.HLGCollection.__init__(self,based_on:HLGDaskCollection)
dask.tests.test_typing.Inheriting(self,based_on:DaskCollection)
dask.tests.test_typing.Inheriting.__dask_graph__(self)->Graph
dask.tests.test_typing.Inheriting.__dask_keys__(self)->NestedKeys
dask.tests.test_typing.Inheriting.__dask_postcompute__(self)->tuple[PostComputeCallable, tuple]
dask.tests.test_typing.Inheriting.__dask_postpersist__(self)->tuple[PostPersistCallable, tuple]
dask.tests.test_typing.Inheriting.__dask_tokenize__(self)->Hashable
dask.tests.test_typing.Inheriting.__init__(self,based_on:DaskCollection)
dask.tests.test_typing.Inheriting.compute(self,**kwargs)->Any
dask.tests.test_typing.Inheriting.persist(self,**kwargs)->Inheriting
dask.tests.test_typing.Inheriting.visualize(self,filename:str='mydask',format:str|None=None,optimize_graph:bool=False,**kwargs:Any)->DisplayObject | None
dask.tests.test_typing.NotHLGCollection(self,based_on:DaskCollection)
dask.tests.test_typing.NotHLGCollection.__dask_graph__(self)->Graph
dask.tests.test_typing.NotHLGCollection.__dask_keys__(self)->NestedKeys
dask.tests.test_typing.NotHLGCollection.__dask_postcompute__(self)->tuple[PostComputeCallable, tuple]
dask.tests.test_typing.NotHLGCollection.__dask_postpersist__(self)->tuple[PostPersistCallable, tuple]
dask.tests.test_typing.NotHLGCollection.__dask_tokenize__(self)->Hashable
dask.tests.test_typing.NotHLGCollection.__init__(self,based_on:DaskCollection)
dask.tests.test_typing.assert_isinstance(coll:DaskCollection,protocol:Any)->None
dask.tests.test_typing.compute(coll:DaskCollection)->Any
dask.tests.test_typing.compute2(coll:DaskCollection)->Any
dask.tests.test_typing.finalize(x:Sequence[Any])->Any
dask.tests.test_typing.get1(dsk:Mapping,keys:Sequence[Key]|Key,**kwargs:Any)->Any
dask.tests.test_typing.get2(dsk:Mapping,keys:Sequence[Key]|Key,**kwargs:Any)->Any
dask.tests.test_typing.increment_(x:int)->int
dask.tests.test_typing.test_inheriting_class()->None
dask.tests.test_typing.test_isinstance_core(protocol)
dask.tests.test_typing.test_isinstance_custom()->None
dask.tests.test_typing.test_parameter_passing()->None


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/tests/test_sizeof.py----------------------------------------
A:dask.tests.test_sizeof.requires_pandas->pytest.mark.skipif(pd is None, reason='requires pandas')
A:dask.tests.test_sizeof.np->pytest.importorskip('numpy')
A:dask.tests.test_sizeof.dt->pytest.importorskip('numpy').dtype('f8')
A:dask.tests.test_sizeof.x->pytest.importorskip('numpy').ones(10000)
A:dask.tests.test_sizeof.df->pandas.DataFrame({'x': [1, 2, 3], 'y': ['a' * 100, 'b' * 100, 'c' * 100]}, index=[10, 20, 30])
A:dask.tests.test_sizeof.df1->pandas.DataFrame([objs * 3] * 1000, dtype=dtype)
A:dask.tests.test_sizeof.df2->pandas.DataFrame([[x, y], [z, w]], dtype=dtype)
A:dask.tests.test_sizeof.index->pandas.MultiIndex.from_product([range(5), ['a', 'b', 'c', 'd', 'e']])
A:dask.tests.test_sizeof.actual_size->sys.getsizeof(index)
A:dask.tests.test_sizeof.df3->pandas.DataFrame([[x, y], [z, x]], dtype=dtype)
A:dask.tests.test_sizeof.sparse->pytest.importorskip('scipy.sparse')
A:dask.tests.test_sizeof.sp->pytest.importorskip('scipy.sparse').eye(10)
A:dask.tests.test_sizeof.cls->class_impl.Impl(size)
A:dask.tests.test_sizeof.s1->cls([f'x{i:3d}' for i in range(1000)], dtype=dtype)
A:dask.tests.test_sizeof.s2->cls([x, y, z, w] * 1000, dtype=dtype)
A:dask.tests.test_sizeof.s3->cls([x, y, z, w], dtype=dtype)
A:dask.tests.test_sizeof.s4->cls([x, y, z, x], dtype=dtype)
A:dask.tests.test_sizeof.s5->cls([x, x, x, x], dtype=dtype)
A:dask.tests.test_sizeof.df4->pandas.DataFrame([[x, x], [x, x]], dtype=dtype)
A:dask.tests.test_sizeof.s->cls(['x' * 100000, 'y' * 50000], dtype='string[pyarrow]')
A:dask.tests.test_sizeof.empty->pytest.importorskip('pyarrow').Table.from_pandas(df.head(0))
A:dask.tests.test_sizeof.pa->pytest.importorskip('pyarrow')
A:dask.tests.test_sizeof.table->pytest.importorskip('pyarrow').Table.from_pandas(df)
dask.tests.test_sizeof._get_sizeof_on_path(path,size)
dask.tests.test_sizeof.test_base()
dask.tests.test_sizeof.test_bytes_like()
dask.tests.test_sizeof.test_containers()
dask.tests.test_sizeof.test_dataframe_object_dtype(dtype)
dask.tests.test_sizeof.test_dict()
dask.tests.test_sizeof.test_name()
dask.tests.test_sizeof.test_numpy()
dask.tests.test_sizeof.test_numpy_0_strided()
dask.tests.test_sizeof.test_pandas()
dask.tests.test_sizeof.test_pandas_contiguous_dtypes()
dask.tests.test_sizeof.test_pandas_empty()
dask.tests.test_sizeof.test_pandas_multiindex()
dask.tests.test_sizeof.test_pandas_object_dtype(dtype,cls_name)
dask.tests.test_sizeof.test_pandas_repeated_column()
dask.tests.test_sizeof.test_pandas_string_arrow_dtype(cls_name)
dask.tests.test_sizeof.test_pyarrow_table()
dask.tests.test_sizeof.test_register_backend_entrypoint(tmp_path)
dask.tests.test_sizeof.test_sparse_matrix()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/tests/test_spark_compat.py----------------------------------------
A:dask.tests.test_spark_compat.dd->pytest.importorskip('dask.dataframe')
A:dask.tests.test_spark_compat.pyspark->pytest.importorskip('pyspark')
A:dask.tests.test_spark_compat.pa->pytest.importorskip('pyarrow')
A:dask.tests.test_spark_compat.pdf->pandas.DataFrame({'a': range(size), 'b': decimal_data})
A:dask.tests.test_spark_compat.pdf.index->pandas.DataFrame({'a': range(size), 'b': decimal_data}).index.tz_localize('UTC')
A:dask.tests.test_spark_compat.prev->signal.getsignal(signal.SIGINT)
A:dask.tests.test_spark_compat.spark->pytest.importorskip('pyspark').sql.SparkSession.builder.master('local').appName('Dask Testing').config('spark.sql.session.timeZone', 'UTC').getOrCreate()
A:dask.tests.test_spark_compat.tmpdir->str(tmpdir)
A:dask.tests.test_spark_compat.sdf->sdf.withColumn('b', sdf['b'].cast(pyspark.sql.types.DecimalType(7, 3))).withColumn('b', sdf['b'].cast(pyspark.sql.types.DecimalType(7, 3)))
A:dask.tests.test_spark_compat.ddf->pytest.importorskip('dask.dataframe').read_parquet(tmpdir, engine='pyarrow', dtype_backend='pyarrow')
A:dask.tests.test_spark_compat.expected->pandas.DataFrame({'a': range(size), 'b': decimal_data}).astype({'a': 'int64[pyarrow]', 'b': pd.ArrowDtype(pa.decimal128(7, 3))})
dask.tests.test_spark_compat.engine(request)
dask.tests.test_spark_compat.spark_session()
dask.tests.test_spark_compat.test_read_decimal_dtype_pyarrow(spark_session,tmpdir)
dask.tests.test_spark_compat.test_roundtrip_hive_parquet_spark_to_dask(spark_session,tmpdir,engine)
dask.tests.test_spark_compat.test_roundtrip_parquet_dask_to_spark(spark_session,npartitions,tmpdir,engine)
dask.tests.test_spark_compat.test_roundtrip_parquet_spark_to_dask(spark_session,npartitions,tmpdir,engine)
dask.tests.test_spark_compat.test_roundtrip_parquet_spark_to_dask_extension_dtypes(spark_session,tmpdir)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/tests/test_cache.py----------------------------------------
A:dask.tests.test_cache.cachey->pytest.importorskip('cachey')
A:dask.tests.test_cache.c->Cache(10000)
A:dask.tests.test_cache.cc->Cache(c)
A:dask.tests.test_cache.da->pytest.importorskip('dask.array')
A:dask.tests.test_cache.z->pytest.importorskip('dask.array').from_array(zeros(1), chunks=10)
A:dask.tests.test_cache.o->pytest.importorskip('dask.array').from_array(ones(1), chunks=10)
dask.tests.test_cache.f(duration,size,*args)
dask.tests.test_cache.inc(x)
dask.tests.test_cache.test_cache()
dask.tests.test_cache.test_cache_correctness()
dask.tests.test_cache.test_cache_with_number()
dask.tests.test_cache.test_prefer_cheap_dependent()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/tests/test_multiprocessing.py----------------------------------------
A:dask.tests.test_multiprocessing.np->pytest.importorskip('numpy')
A:dask.tests.test_multiprocessing.b->remote_exception(e, 'traceback-body')
A:dask.tests.test_multiprocessing.a->NotUnpickleable()
A:dask.tests.test_multiprocessing.a2->_loads(b, buffers=l)
A:dask.tests.test_multiprocessing.e->TypeError('hello')
A:dask.tests.test_multiprocessing.f->get(dsk, 'x')
A:dask.tests.test_multiprocessing.x->delayed(lambda x: x)(NoIndex(1))
A:dask.tests.test_multiprocessing.(res,)->get(x.dask, x.__dask_keys__())
A:dask.tests.test_multiprocessing.(results,)->compute([f() for _ in range(10)])
A:dask.tests.test_multiprocessing.(results2,)->compute([f() for _ in range(10)], scheduler=scheduler, initializer=initializer)
A:dask.tests.test_multiprocessing.result->get({'x': (check_for_pytest,)}, 'x')
dask.tests.test_multiprocessing.NotUnpickleable
dask.tests.test_multiprocessing.NotUnpickleable.__getstate__(self)
dask.tests.test_multiprocessing.NotUnpickleable.__setstate__(self,state)
dask.tests.test_multiprocessing.bad()
dask.tests.test_multiprocessing.check_for_pytest()
dask.tests.test_multiprocessing.global_
dask.tests.test_multiprocessing.lambda_result()
dask.tests.test_multiprocessing.my_small_function_global(a,b)
dask.tests.test_multiprocessing.proc_init()
dask.tests.test_multiprocessing.test_custom_context_ignored_elsewhere()
dask.tests.test_multiprocessing.test_custom_context_used_python3_posix()
dask.tests.test_multiprocessing.test_dumps_loads()
dask.tests.test_multiprocessing.test_errors_propagate()
dask.tests.test_multiprocessing.test_fuse_doesnt_clobber_intermediates()
dask.tests.test_multiprocessing.test_get_context_always_default()
dask.tests.test_multiprocessing.test_get_context_using_python3_posix()
dask.tests.test_multiprocessing.test_lambda_results_with_cloudpickle()
dask.tests.test_multiprocessing.test_lambda_with_cloudpickle()
dask.tests.test_multiprocessing.test_optimize_graph_false()
dask.tests.test_multiprocessing.test_out_of_band_pickling()
dask.tests.test_multiprocessing.test_pickle_globals()
dask.tests.test_multiprocessing.test_pickle_locals()
dask.tests.test_multiprocessing.test_process_initializer(scheduler,initializer,expected_results)
dask.tests.test_multiprocessing.test_random_seeds(random)
dask.tests.test_multiprocessing.test_remote_exception()
dask.tests.test_multiprocessing.test_reuse_pool(pool_typ)
dask.tests.test_multiprocessing.test_unpicklable_args_generate_errors()
dask.tests.test_multiprocessing.test_works_with_highlevel_graph()
dask.tests.test_multiprocessing.unrelated_function_global(a)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/diagnostics/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/diagnostics/profile_visualize.py----------------------------------------
A:dask.diagnostics.profile_visualize.head->funcname(task[0])
A:dask.diagnostics.profile_visualize.label_size2->int((label_size - 2 - 2 * len(task)) // len(task))
A:dask.diagnostics.profile_visualize.args->', '.join((pprint_task(t, keys, label_size2) for t in task))
A:dask.diagnostics.profile_visualize.result->pprint_task(task[:3], keys, label_size)
A:dask.diagnostics.profile_visualize.palettes->import_required('bokeh.palettes', _BOKEH_MISSING_MSG)
A:dask.diagnostics.profile_visualize.unique_funcs->sorted(unique(funcs))
A:dask.diagnostics.profile_visualize.n_funcs->len(unique_funcs)
A:dask.diagnostics.profile_visualize.keys->list(sorted(palette_lookup.keys()))
A:dask.diagnostics.profile_visualize.palette->list(unique(palette))
A:dask.diagnostics.profile_visualize.color_lookup->dict(zip(unique_funcs, cycle(palette)))
A:dask.diagnostics.profile_visualize.bp->import_required('bokeh.plotting', _BOKEH_MISSING_MSG)
A:dask.diagnostics.profile_visualize.filename->kwargs.pop('file_path')
A:dask.diagnostics.profile_visualize.p->import_required('bokeh.plotting', _BOKEH_MISSING_MSG).figure(y_range=[0, 10], x_range=[0, 10], **defaults)
A:dask.diagnostics.profile_visualize.defaults->dict(title='Profile Results', tools='hover,save,reset,wheel_zoom,xpan', toolbar_location='above', width=800, height=300)
A:dask.diagnostics.profile_visualize.kwargs['width']->kwargs.pop('plot_width')
A:dask.diagnostics.profile_visualize.kwargs['height']->kwargs.pop('plot_height')
A:dask.diagnostics.profile_visualize.(keys, tasks, starts, ends, ids)->zip(*results)
A:dask.diagnostics.profile_visualize.id_group->groupby(itemgetter(4), results)
A:dask.diagnostics.profile_visualize.data['color']->get_colors(palette, funcs)
A:dask.diagnostics.profile_visualize.source->import_required('bokeh.plotting', _BOKEH_MISSING_MSG).ColumnDataSource(data=data)
A:dask.diagnostics.profile_visualize.hover->import_required('bokeh.plotting', _BOKEH_MISSING_MSG).figure(y_range=[0, 10], x_range=[0, 10], **defaults).select(HoverTool)
A:dask.diagnostics.profile_visualize.(t, mem, cpu)->zip(*results)
A:dask.diagnostics.profile_visualize.tics->sorted(unique(starts + ends))
A:dask.diagnostics.profile_visualize.groups->groupby(lambda d: pprint_task(d[1], dsk, label_size), results)
A:dask.diagnostics.profile_visualize.cnts->dict.fromkeys(tics, 0)
dask.diagnostics.profile_visualize.BOKEH_VERSION()
dask.diagnostics.profile_visualize.fix_bounds(start,end,min_span)
dask.diagnostics.profile_visualize.get_colors(palette,funcs)
dask.diagnostics.profile_visualize.plot_cache(results,dsk,start_time,end_time,metric_name,palette='Viridis',label_size=60,**kwargs)
dask.diagnostics.profile_visualize.plot_resources(results,start_time,end_time,palette='Viridis',**kwargs)
dask.diagnostics.profile_visualize.plot_tasks(results,dsk,start_time,end_time,palette='Viridis',label_size=60,**kwargs)
dask.diagnostics.profile_visualize.pprint_task(task,keys,label_size=60)
dask.diagnostics.profile_visualize.unquote(expr)
dask.diagnostics.profile_visualize.visualize(profilers,filename='profile.html',show=True,save=None,mode=None,**kwargs)
dask.diagnostics.visualize(profilers,filename='profile.html',show=True,save=None,mode=None,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/diagnostics/profile.py----------------------------------------
A:dask.diagnostics.profile.TaskData->namedtuple('TaskData', ('key', 'task', 'start_time', 'end_time', 'worker_id'))
A:dask.diagnostics.profile.self.start_time->default_timer()
A:dask.diagnostics.profile.self.end_time->default_timer()
A:dask.diagnostics.profile.start->default_timer()
A:dask.diagnostics.profile.end->default_timer()
A:dask.diagnostics.profile.ResourceData->namedtuple('ResourceData', ('time', 'mem', 'cpu'))
A:dask.diagnostics.profile.self._tracker->_Tracker(self._dt)
A:dask.diagnostics.profile.(self.parent_conn, self.child_conn)->Pipe()
A:dask.diagnostics.profile.psutil->import_required('psutil', 'Tracking resource usage requires `psutil` to be installed')
A:dask.diagnostics.profile.self.parent->import_required('psutil', 'Tracking resource usage requires `psutil` to be installed').Process(self.parent_pid)
A:dask.diagnostics.profile.pid->current_process()
A:dask.diagnostics.profile.msg->self.child_conn.recv()
A:dask.diagnostics.profile.ps->self._update_pids(pid)
A:dask.diagnostics.profile.tic->default_timer()
A:dask.diagnostics.profile.cpu2->p.cpu_percent()
A:dask.diagnostics.profile.CacheData->namedtuple('CacheData', ('key', 'task', 'metric', 'cache_time', 'free_time'))
A:dask.diagnostics.profile.t->default_timer()
A:dask.diagnostics.profile.(metric, start)->self._cache.pop(k)
dask.diagnostics.CacheProfiler(self,metric=None,metric_name=None)
dask.diagnostics.CacheProfiler.__enter__(self)
dask.diagnostics.CacheProfiler.__exit__(self,*args)
dask.diagnostics.CacheProfiler._finish(self,dsk,state,failed)
dask.diagnostics.CacheProfiler._plot(self,**kwargs)
dask.diagnostics.CacheProfiler._posttask(self,key,value,dsk,state,id)
dask.diagnostics.CacheProfiler._start(self,dsk)
dask.diagnostics.CacheProfiler.clear(self)
dask.diagnostics.CacheProfiler.visualize(self,**kwargs)
dask.diagnostics.Profiler(self)
dask.diagnostics.Profiler.__enter__(self)
dask.diagnostics.Profiler.__exit__(self,*args)
dask.diagnostics.Profiler._finish(self,dsk,state,failed)
dask.diagnostics.Profiler._plot(self,**kwargs)
dask.diagnostics.Profiler._posttask(self,key,value,dsk,state,id)
dask.diagnostics.Profiler._pretask(self,key,dsk,state)
dask.diagnostics.Profiler._start(self,dsk)
dask.diagnostics.Profiler.clear(self)
dask.diagnostics.Profiler.visualize(self,**kwargs)
dask.diagnostics.ResourceProfiler(self,dt=1)
dask.diagnostics.ResourceProfiler.__enter__(self)
dask.diagnostics.ResourceProfiler.__exit__(self,*args)
dask.diagnostics.ResourceProfiler._finish(self,dsk,state,failed)
dask.diagnostics.ResourceProfiler._is_running(self)
dask.diagnostics.ResourceProfiler._plot(self,**kwargs)
dask.diagnostics.ResourceProfiler._start(self,dsk)
dask.diagnostics.ResourceProfiler._start_collect(self)
dask.diagnostics.ResourceProfiler._stop_collect(self)
dask.diagnostics.ResourceProfiler.clear(self)
dask.diagnostics.ResourceProfiler.close(self)
dask.diagnostics.ResourceProfiler.visualize(self,**kwargs)
dask.diagnostics.profile.CacheProfiler(self,metric=None,metric_name=None)
dask.diagnostics.profile.CacheProfiler.__enter__(self)
dask.diagnostics.profile.CacheProfiler.__exit__(self,*args)
dask.diagnostics.profile.CacheProfiler.__init__(self,metric=None,metric_name=None)
dask.diagnostics.profile.CacheProfiler._finish(self,dsk,state,failed)
dask.diagnostics.profile.CacheProfiler._plot(self,**kwargs)
dask.diagnostics.profile.CacheProfiler._posttask(self,key,value,dsk,state,id)
dask.diagnostics.profile.CacheProfiler._start(self,dsk)
dask.diagnostics.profile.CacheProfiler.clear(self)
dask.diagnostics.profile.CacheProfiler.visualize(self,**kwargs)
dask.diagnostics.profile.Profiler(self)
dask.diagnostics.profile.Profiler.__enter__(self)
dask.diagnostics.profile.Profiler.__exit__(self,*args)
dask.diagnostics.profile.Profiler.__init__(self)
dask.diagnostics.profile.Profiler._finish(self,dsk,state,failed)
dask.diagnostics.profile.Profiler._plot(self,**kwargs)
dask.diagnostics.profile.Profiler._posttask(self,key,value,dsk,state,id)
dask.diagnostics.profile.Profiler._pretask(self,key,dsk,state)
dask.diagnostics.profile.Profiler._start(self,dsk)
dask.diagnostics.profile.Profiler.clear(self)
dask.diagnostics.profile.Profiler.visualize(self,**kwargs)
dask.diagnostics.profile.ResourceProfiler(self,dt=1)
dask.diagnostics.profile.ResourceProfiler.__enter__(self)
dask.diagnostics.profile.ResourceProfiler.__exit__(self,*args)
dask.diagnostics.profile.ResourceProfiler.__init__(self,dt=1)
dask.diagnostics.profile.ResourceProfiler._finish(self,dsk,state,failed)
dask.diagnostics.profile.ResourceProfiler._is_running(self)
dask.diagnostics.profile.ResourceProfiler._plot(self,**kwargs)
dask.diagnostics.profile.ResourceProfiler._start(self,dsk)
dask.diagnostics.profile.ResourceProfiler._start_collect(self)
dask.diagnostics.profile.ResourceProfiler._stop_collect(self)
dask.diagnostics.profile.ResourceProfiler.clear(self)
dask.diagnostics.profile.ResourceProfiler.close(self)
dask.diagnostics.profile.ResourceProfiler.visualize(self,**kwargs)
dask.diagnostics.profile._Tracker(self,dt=1)
dask.diagnostics.profile._Tracker.__init__(self,dt=1)
dask.diagnostics.profile._Tracker._update_pids(self,pid)
dask.diagnostics.profile._Tracker.run(self)
dask.diagnostics.profile._Tracker.shutdown(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/diagnostics/progress.py----------------------------------------
A:dask.diagnostics.progress.(m, s)->divmod(t, 60)
A:dask.diagnostics.progress.(h, m)->divmod(m, 60)
A:dask.diagnostics.progress.self._start_time->default_timer()
A:dask.diagnostics.progress.self._timer->threading.Thread(target=self._timer_func)
A:dask.diagnostics.progress.ndone->len(s['finished'])
A:dask.diagnostics.progress.percent->int(100 * frac)
A:dask.diagnostics.progress.elapsed->format_time(elapsed)
A:dask.diagnostics.progress.msg->'\r[{0:<{1}}] | {2}% Completed | {3}'.format(bar, self._width, percent, elapsed)
dask.diagnostics.ProgressBar(self,minimum=0,width=40,dt=0.1,out=None)
dask.diagnostics.ProgressBar._draw_bar(self,frac,elapsed)
dask.diagnostics.ProgressBar._finish(self,dsk,state,errored)
dask.diagnostics.ProgressBar._pretask(self,key,dsk,state)
dask.diagnostics.ProgressBar._start(self,dsk)
dask.diagnostics.ProgressBar._timer_func(self)
dask.diagnostics.ProgressBar._update_bar(self,elapsed)
dask.diagnostics.progress.ProgressBar(self,minimum=0,width=40,dt=0.1,out=None)
dask.diagnostics.progress.ProgressBar.__init__(self,minimum=0,width=40,dt=0.1,out=None)
dask.diagnostics.progress.ProgressBar._draw_bar(self,frac,elapsed)
dask.diagnostics.progress.ProgressBar._finish(self,dsk,state,errored)
dask.diagnostics.progress.ProgressBar._pretask(self,key,dsk,state)
dask.diagnostics.progress.ProgressBar._start(self,dsk)
dask.diagnostics.progress.ProgressBar._timer_func(self)
dask.diagnostics.progress.ProgressBar._update_bar(self,elapsed)
dask.diagnostics.progress.format_time(t)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/diagnostics/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/diagnostics/tests/test_profiler.py----------------------------------------
A:dask.diagnostics.tests.test_profiler.prof->profiler()
A:dask.diagnostics.tests.test_profiler.in_context_time->default_timer()
A:dask.diagnostics.tests.test_profiler.out->get(dsk, 'e')
A:dask.diagnostics.tests.test_profiler.prof_data->sorted(prof.results, key=lambda d: d.key)
A:dask.diagnostics.tests.test_profiler.n->len(prof.results)
A:dask.diagnostics.tests.test_profiler.m->len(prof.results)
A:dask.diagnostics.tests.test_profiler.p->visualize([prof, rprof], label_size=50, title='Not the default', show=False, save=False)
A:dask.diagnostics.tests.test_profiler.funcs->list(range(300))
A:dask.diagnostics.tests.test_profiler.cmap->get_colors('Viridis', funcs)
A:dask.diagnostics.tests.test_profiler.lk->dict(zip(funcs, Blues5))
dask.diagnostics.tests.test_profiler.test_cache_profiler()
dask.diagnostics.tests.test_profiler.test_cache_profiler_plot()
dask.diagnostics.tests.test_profiler.test_cache_profiler_plot_with_invalid_bokeh_kwarg_raises_error()
dask.diagnostics.tests.test_profiler.test_get_colors()
dask.diagnostics.tests.test_profiler.test_plot_multiple()
dask.diagnostics.tests.test_profiler.test_pprint_task()
dask.diagnostics.tests.test_profiler.test_profiler()
dask.diagnostics.tests.test_profiler.test_profiler_plot()
dask.diagnostics.tests.test_profiler.test_profiler_works_under_error()
dask.diagnostics.tests.test_profiler.test_register(profiler)
dask.diagnostics.tests.test_profiler.test_resource_profiler()
dask.diagnostics.tests.test_profiler.test_resource_profiler_multiple_gets()
dask.diagnostics.tests.test_profiler.test_resource_profiler_plot()
dask.diagnostics.tests.test_profiler.test_saves_file()
dask.diagnostics.tests.test_profiler.test_saves_file_path_deprecated()
dask.diagnostics.tests.test_profiler.test_two_gets()
dask.diagnostics.tests.test_profiler.test_unquote()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/diagnostics/tests/test_progress.py----------------------------------------
A:dask.diagnostics.tests.test_progress.(out, err)->capsys.readouterr()
A:dask.diagnostics.tests.test_progress.da->pytest.importorskip('dask.array')
A:dask.diagnostics.tests.test_progress.data->pytest.importorskip('dask.array').ones((100, 100), dtype='f4', chunks=(100, 100))
A:dask.diagnostics.tests.test_progress.out->get_threaded(dsk, 'e')
A:dask.diagnostics.tests.test_progress.p->ProgressBar()
A:dask.diagnostics.tests.test_progress.cachey->pytest.importorskip('cachey')
A:dask.diagnostics.tests.test_progress.c->pytest.importorskip('cachey').Cache(10000)
A:dask.diagnostics.tests.test_progress.cc->Cache(c)
dask.diagnostics.tests.test_progress.check_bar_completed(capsys,width=40)
dask.diagnostics.tests.test_progress.test_array_compute(capsys)
dask.diagnostics.tests.test_progress.test_clean_exit(get)
dask.diagnostics.tests.test_progress.test_format_time()
dask.diagnostics.tests.test_progress.test_minimum_time(capsys)
dask.diagnostics.tests.test_progress.test_no_tasks(capsys)
dask.diagnostics.tests.test_progress.test_progressbar(capsys)
dask.diagnostics.tests.test_progress.test_register(capsys)
dask.diagnostics.tests.test_progress.test_store_time()
dask.diagnostics.tests.test_progress.test_with_alias(capsys)
dask.diagnostics.tests.test_progress.test_with_cache(capsys)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/bag/chunk.py----------------------------------------
A:dask.bag.chunk.(squares, totals, counts)->list(zip(*x))
dask.bag.chunk.barrier(*args)
dask.bag.chunk.foldby_combine2(combine,acc,x)
dask.bag.chunk.getitem(x,key)
dask.bag.chunk.groupby_tasks_group_hash(x,hash,grouper)
dask.bag.chunk.var_aggregate(x,ddof)
dask.bag.chunk.var_chunk(seq)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/bag/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/bag/text.py----------------------------------------
A:dask.bag.text.delayed->delayed(pure=True)
A:dask.bag.text.blocksize->parse_bytes(blocksize)
A:dask.bag.text.files->open_files(urlpath, mode='rt', encoding=encoding, errors=errors, compression=compression, newline=newline, **storage_options or {})
A:dask.bag.text.block_lines->delayed(concat)(delayed(map)(partial(file_to_blocks, include_path, delimiter=linedelimiter), block_files))
A:dask.bag.text.o->read_bytes(urlpath, delimiter=linedelimiter.encode() if linedelimiter is not None else b'\n', blocksize=blocksize, sample=False, compression=compression, include_path=include_path, **storage_options or {})
A:dask.bag.text.paths->list(concat([[path] * len(raw_blocks[i]) for (i, path) in enumerate(o[2])]))
A:dask.bag.text.blocks->from_delayed(blocks)
A:dask.bag.text.text->block.decode(encoding, errors)
A:dask.bag.text.parts->block.decode(encoding, errors).split(line_delimiter)
A:dask.bag.text.lines->io.StringIO(text, newline=line_delimiter)
dask.bag.read_text(urlpath,blocksize=None,compression='infer',encoding=system_encoding,errors='strict',linedelimiter=None,collection=True,storage_options=None,files_per_partition=None,include_path=False)
dask.bag.text.attach_path(block,path)
dask.bag.text.decode(block,encoding,errors,line_delimiter)
dask.bag.text.file_to_blocks(include_path,lazy_file,delimiter=None)
dask.bag.text.read_text(urlpath,blocksize=None,compression='infer',encoding=system_encoding,errors='strict',linedelimiter=None,collection=True,storage_options=None,files_per_partition=None,include_path=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/bag/core.py----------------------------------------
A:dask.bag.core.DEFAULT_GET->dask.base.named_schedulers.get('processes', named_schedulers['sync'])
A:dask.bag.core.no_result->type('no_result', (object,), {'__slots__': (), '__reduce__': lambda self: 'no_result'})
A:dask.bag.core.dependents->reverse_dict(dependencies)
A:dask.bag.core.dsk->merge(dsk1, dsk2, dsk3, dsk4)
A:dask.bag.core.(dsk2, dependencies)->cull(dsk, keys)
A:dask.bag.core.(dsk3, dependencies)->fuse(dsk2, keys + (fuse_keys or []), dependencies, **kwargs)
A:dask.bag.core.dsk4->inline_singleton_lists(dsk3, keys, dependencies)
A:dask.bag.core.dsk5->lazify(dsk4)
A:dask.bag.core.files->open_files(path, compression=compression, mode=mode, encoding=encoding, name_function=name_function, num=b.npartitions, **storage_options or {})
A:dask.bag.core.graph->dask.highlevelgraph.HighLevelGraph.from_collections(new_name, dsk, dependencies=[bag])
A:dask.bag.core.out->defaultdict(int)
A:dask.bag.core.results->list(results)
A:dask.bag.core.func->partial(topk, k)
A:dask.bag.core.__dask_optimize__->globalmethod(optimize, key='bag_optimize', falsey=dont_optimize)
A:dask.bag.core.__dask_scheduler__->staticmethod(DEFAULT_GET)
A:dask.bag.core.value->delayed(value)
A:dask.bag.core.name->'{}-{}'.format(name, tokenize(func, 'map-partitions', *args, **kwargs))
A:dask.bag.core.str->property(fget=StringAccessor)
A:dask.bag.core.(kwargs, collections)->unpack_scalar_dask_kwargs(kwargs)
A:dask.bag.core.random_state->Random(random_state)
A:dask.bag.core.state_data->random_state_data_python(self.npartitions, random_state)
A:dask.bag.core.key->partial(chunk.getitem, key=key)
A:dask.bag.core.result->result.map_partitions(sorted, key=second, reverse=True).map_partitions(sorted, key=second, reverse=True)
A:dask.bag.core.token->tokenize(b, grouper, npartitions, blocksize)
A:dask.bag.core.dsk[fmt]->merge(dsk1, dsk2, dsk3, dsk4).pop((fmt, 0))
A:dask.bag.core.(totals, counts)->list(zip(*x))
A:dask.bag.core.combine2->partial(chunk.foldby_combine2, combine)
A:dask.bag.core.b->Bag(graph, name, 1)
A:dask.bag.core.shuffle->get_default_shuffle_method()
A:dask.bag.core.head->self.take(1, warn=False)
A:dask.bag.core.meta->dask.dataframe.utils.make_meta(meta, parent_meta=pd.DataFrame())
A:dask.bag.core.cols->list(meta.columns)
A:dask.bag.core.dtypes->dask.dataframe.utils.make_meta(meta, parent_meta=pd.DataFrame()).dtypes.to_dict()
A:dask.bag.core.dfs->self.map_partitions(to_dataframe, cols, dtypes)
A:dask.bag.core.keys->self.__dask_keys__()
A:dask.bag.core.binop_name->funcname(binop)
A:dask.bag.core.res->pandas.DataFrame(seq, columns=list(columns))
A:dask.bag.core.d->groupby(grouper, p.get(group, lock=False))
A:dask.bag.core.d2->defaultdict(list)
A:dask.bag.core.seq->list(seq)
A:dask.bag.core.partition_size->max(1, math.ceil(math.sqrt(len(seq)) / math.sqrt(100)))
A:dask.bag.core.parts->list(partition_all(partition_size, seq))
A:dask.bag.core.counter->itertools.count(0)
A:dask.bag.core.seqs->list(seqs)
A:dask.bag.core.ijs->list(enumerate(take(npartitions, range(0, n, size))))
A:dask.bag.core.f->partial(f, **kwargs)
A:dask.bag.core.self.nkws->len(self.kwarg_keys)
A:dask.bag.core.kwargs->dict(zip(self.kwarg_keys, vals[-self.nkws:]))
A:dask.bag.core.(vv, collections)->unpack_collections(v)
A:dask.bag.core.(other_kwargs, collections)->unpack_scalar_dask_kwargs(other_kwargs)
A:dask.bag.core.npartitions->npartitions.pop().pop()
A:dask.bag.core.iter_kwarg_keys->list(bag_kwargs)
A:dask.bag.core.return_type->set(map(type, bags))
A:dask.bag.core.other_kwargs->other_kwargs[0](other_kwargs[1][0](*other_kwargs[1][1:]))
A:dask.bag.core.k->int(math.ceil(n ** (1 / stages)))
A:dask.bag.core.b2->Bag(graph, name, 1).map(partial(chunk.groupby_tasks_group_hash, hash=hash, grouper=grouper))
A:dask.bag.core.dirname->dask.config.get('temporary_directory', None)
A:dask.bag.core.(_, part)->peek(part)
A:dask.bag.core.r->list(take(n, b))
A:dask.bag.core.np_rng->numpy.random.default_rng(random_state)
A:dask.bag.core.random_data->numpy.random.default_rng(random_state).bytes(624 * n * 4)
A:dask.bag.core.arr->numpy.frombuffer(random_data, dtype=np.uint32).reshape((n, -1))
A:dask.bag.core.(div, mod)->divmod(npartitions, bag.npartitions)
A:dask.bag.core.partition->reify(deepcopy(partition))
A:dask.bag.core.size->int(size)
A:dask.bag.core.mem_usages->_split_partitions(bag, nsplits, split_name).map_partitions(total_mem_usage).compute()
A:dask.bag.core.bag->_split_partitions(bag, nsplits, split_name)
A:dask.bag.core.new_npartitions->list(map(len, iter_chunks(mem_usages, size)))
A:dask.bag.core.new_partitions_boundaries->list(new_partitions_boundaries)
dask.bag.Bag(self,dsk:Graph,name:str,npartitions:int)
dask.bag.Bag.__dask_graph__(self)->Graph
dask.bag.Bag.__dask_keys__(self)->NestedKeys
dask.bag.Bag.__dask_layers__(self)->Sequence[str]
dask.bag.Bag.__dask_postcompute__(self)
dask.bag.Bag.__dask_postpersist__(self)
dask.bag.Bag.__dask_tokenize__(self)
dask.bag.Bag.__getstate__(self)
dask.bag.Bag.__iter__(self)
dask.bag.Bag.__setstate__(self,state)
dask.bag.Bag.__str__(self)
dask.bag.Bag._args(self)
dask.bag.Bag._rebuild(self,dsk,*,rename=None)
dask.bag.Bag.accumulate(self,binop,initial=no_default)
dask.bag.Bag.all(self,split_every=None)
dask.bag.Bag.any(self,split_every=None)
dask.bag.Bag.count(self,split_every=None)
dask.bag.Bag.distinct(self,key=None)
dask.bag.Bag.filter(self,predicate)
dask.bag.Bag.flatten(self)
dask.bag.Bag.fold(self,binop,combine=None,initial=no_default,split_every=None,out_type=Item)
dask.bag.Bag.foldby(self,key,binop,initial=no_default,combine=None,combine_initial=no_default,split_every=None)
dask.bag.Bag.frequencies(self,split_every=None,sort=False)
dask.bag.Bag.groupby(self,grouper,method=None,npartitions=None,blocksize=2**20,max_branch=None,shuffle=None)
dask.bag.Bag.join(self,other,on_self,on_other=None)
dask.bag.Bag.map(self,func,*args,**kwargs)
dask.bag.Bag.map_partitions(self,func,*args,**kwargs)
dask.bag.Bag.max(self,split_every=None)
dask.bag.Bag.mean(self)
dask.bag.Bag.min(self,split_every=None)
dask.bag.Bag.pluck(self,key,default=no_default)
dask.bag.Bag.product(self,other)
dask.bag.Bag.random_sample(self,prob,random_state=None)
dask.bag.Bag.reduction(self,perpartition,aggregate,split_every=None,out_type=Item,name=None)
dask.bag.Bag.remove(self,predicate)
dask.bag.Bag.repartition(self,npartitions=None,partition_size=None)
dask.bag.Bag.starmap(self,func,**kwargs)
dask.bag.Bag.std(self,ddof=0)
dask.bag.Bag.sum(self,split_every=None)
dask.bag.Bag.take(self,k,npartitions=1,compute=True,warn=True)
dask.bag.Bag.to_avro(self,filename,schema,name_function=None,storage_options=None,codec='null',sync_interval=16000,metadata=None,compute=True,**kwargs)
dask.bag.Bag.to_dataframe(self,meta=None,columns=None,optimize_graph=True)
dask.bag.Bag.to_delayed(self,optimize_graph=True)
dask.bag.Bag.to_textfiles(self,path,name_function=None,compression='infer',encoding=system_encoding,compute=True,storage_options=None,last_endline=False,**kwargs)
dask.bag.Bag.topk(self,k,key=None,split_every=None)
dask.bag.Bag.unzip(self,n)
dask.bag.Bag.var(self,ddof=0)
dask.bag.Item(self,dsk,key,layer=None)
dask.bag.Item.__dask_graph__(self)->Graph
dask.bag.Item.__dask_keys__(self)->NestedKeys
dask.bag.Item.__dask_layers__(self)->Sequence[str]
dask.bag.Item.__dask_postcompute__(self)
dask.bag.Item.__dask_postpersist__(self)
dask.bag.Item.__dask_tokenize__(self)
dask.bag.Item.__getstate__(self)
dask.bag.Item.__setstate__(self,state)
dask.bag.Item._args(self)
dask.bag.Item._rebuild(self,dsk,*,rename=None)
dask.bag.Item.apply(self,func)
dask.bag.Item.from_delayed(value)
dask.bag.Item.to_delayed(self,optimize_graph=True)
dask.bag.concat(bags)
dask.bag.core.Bag(self,dsk:Graph,name:str,npartitions:int)
dask.bag.core.Bag.__dask_graph__(self)->Graph
dask.bag.core.Bag.__dask_keys__(self)->NestedKeys
dask.bag.core.Bag.__dask_layers__(self)->Sequence[str]
dask.bag.core.Bag.__dask_postcompute__(self)
dask.bag.core.Bag.__dask_postpersist__(self)
dask.bag.core.Bag.__dask_tokenize__(self)
dask.bag.core.Bag.__getstate__(self)
dask.bag.core.Bag.__init__(self,dsk:Graph,name:str,npartitions:int)
dask.bag.core.Bag.__iter__(self)
dask.bag.core.Bag.__setstate__(self,state)
dask.bag.core.Bag.__str__(self)
dask.bag.core.Bag._args(self)
dask.bag.core.Bag._rebuild(self,dsk,*,rename=None)
dask.bag.core.Bag.accumulate(self,binop,initial=no_default)
dask.bag.core.Bag.all(self,split_every=None)
dask.bag.core.Bag.any(self,split_every=None)
dask.bag.core.Bag.count(self,split_every=None)
dask.bag.core.Bag.distinct(self,key=None)
dask.bag.core.Bag.filter(self,predicate)
dask.bag.core.Bag.flatten(self)
dask.bag.core.Bag.fold(self,binop,combine=None,initial=no_default,split_every=None,out_type=Item)
dask.bag.core.Bag.foldby(self,key,binop,initial=no_default,combine=None,combine_initial=no_default,split_every=None)
dask.bag.core.Bag.frequencies(self,split_every=None,sort=False)
dask.bag.core.Bag.groupby(self,grouper,method=None,npartitions=None,blocksize=2**20,max_branch=None,shuffle=None)
dask.bag.core.Bag.join(self,other,on_self,on_other=None)
dask.bag.core.Bag.map(self,func,*args,**kwargs)
dask.bag.core.Bag.map_partitions(self,func,*args,**kwargs)
dask.bag.core.Bag.max(self,split_every=None)
dask.bag.core.Bag.mean(self)
dask.bag.core.Bag.min(self,split_every=None)
dask.bag.core.Bag.pluck(self,key,default=no_default)
dask.bag.core.Bag.product(self,other)
dask.bag.core.Bag.random_sample(self,prob,random_state=None)
dask.bag.core.Bag.reduction(self,perpartition,aggregate,split_every=None,out_type=Item,name=None)
dask.bag.core.Bag.remove(self,predicate)
dask.bag.core.Bag.repartition(self,npartitions=None,partition_size=None)
dask.bag.core.Bag.starmap(self,func,**kwargs)
dask.bag.core.Bag.std(self,ddof=0)
dask.bag.core.Bag.sum(self,split_every=None)
dask.bag.core.Bag.take(self,k,npartitions=1,compute=True,warn=True)
dask.bag.core.Bag.to_avro(self,filename,schema,name_function=None,storage_options=None,codec='null',sync_interval=16000,metadata=None,compute=True,**kwargs)
dask.bag.core.Bag.to_dataframe(self,meta=None,columns=None,optimize_graph=True)
dask.bag.core.Bag.to_delayed(self,optimize_graph=True)
dask.bag.core.Bag.to_textfiles(self,path,name_function=None,compression='infer',encoding=system_encoding,compute=True,storage_options=None,last_endline=False,**kwargs)
dask.bag.core.Bag.topk(self,k,key=None,split_every=None)
dask.bag.core.Bag.unzip(self,n)
dask.bag.core.Bag.var(self,ddof=0)
dask.bag.core.Item(self,dsk,key,layer=None)
dask.bag.core.Item.__dask_graph__(self)->Graph
dask.bag.core.Item.__dask_keys__(self)->NestedKeys
dask.bag.core.Item.__dask_layers__(self)->Sequence[str]
dask.bag.core.Item.__dask_postcompute__(self)
dask.bag.core.Item.__dask_postpersist__(self)
dask.bag.core.Item.__dask_tokenize__(self)
dask.bag.core.Item.__getstate__(self)
dask.bag.core.Item.__init__(self,dsk,key,layer=None)
dask.bag.core.Item.__setstate__(self,state)
dask.bag.core.Item._args(self)
dask.bag.core.Item._rebuild(self,dsk,*,rename=None)
dask.bag.core.Item.apply(self,func)
dask.bag.core.Item.from_delayed(value)
dask.bag.core.Item.to_delayed(self,optimize_graph=True)
dask.bag.core.StringAccessor(self,bag)
dask.bag.core.StringAccessor.__dir__(self)
dask.bag.core.StringAccessor.__getattr__(self,key)
dask.bag.core.StringAccessor.__init__(self,bag)
dask.bag.core.StringAccessor._strmap(self,key,*args,**kwargs)
dask.bag.core.StringAccessor.match(self,pattern)
dask.bag.core._MapChunk(self,f,iters,kwarg_keys=None)
dask.bag.core._MapChunk.__init__(self,f,iters,kwarg_keys=None)
dask.bag.core._MapChunk.__next__(self)
dask.bag.core._MapChunk.check_all_iterators_consumed(self)
dask.bag.core._reduce(binop,sequence,initial=no_default)
dask.bag.core._repartition_from_boundaries(bag,new_partitions_boundaries,new_name)
dask.bag.core._split_partitions(bag,nsplits,new_name)
dask.bag.core._to_textfiles_chunk(data,lazy_file,last_endline)
dask.bag.core.accumulate_part(binop,seq,initial,is_first=False)
dask.bag.core.bag_map(func,*args,**kwargs)
dask.bag.core.bag_range(n,npartitions)
dask.bag.core.bag_zip(*bags)
dask.bag.core.chunk_distinct(seq,key=None)
dask.bag.core.collect(grouper,group,p,barrier_token)
dask.bag.core.concat(bags)
dask.bag.core.dictitems(d)
dask.bag.core.empty_safe_aggregate(func,parts,is_last)
dask.bag.core.empty_safe_apply(func,part,is_last)
dask.bag.core.finalize(results)
dask.bag.core.finalize_item(results)
dask.bag.core.from_delayed(values)
dask.bag.core.from_sequence(seq,partition_size=None,npartitions=None)
dask.bag.core.from_url(urls)
dask.bag.core.groupby_disk(b,grouper,npartitions=None,blocksize=2**20)
dask.bag.core.groupby_tasks(b,grouper,hash=hash,max_branch=32)
dask.bag.core.inline_singleton_lists(dsk,keys,dependencies=None)
dask.bag.core.lazify(dsk)
dask.bag.core.lazify_task(task,start=True)
dask.bag.core.make_group(k,stage)
dask.bag.core.map_chunk(f,iters,iter_kwarg_keys=None,kwargs=None)
dask.bag.core.map_partitions(func,*args,**kwargs)
dask.bag.core.merge_distinct(seqs,key=None)
dask.bag.core.merge_frequencies(seqs)
dask.bag.core.optimize(dsk,keys,fuse_keys=None,rename_fused_keys=None,**kwargs)
dask.bag.core.partition(grouper,sequence,npartitions,p,nelements=2**20)
dask.bag.core.random_sample(x,state_data,prob)
dask.bag.core.random_state_data_python(n:int,random_state:int|Random|None=None)->list[tuple[int, tuple[int, ...], None]]
dask.bag.core.reify(seq)
dask.bag.core.repartition_npartitions(bag,npartitions)
dask.bag.core.repartition_size(bag,size)
dask.bag.core.robust_wraps(wrapper)
dask.bag.core.safe_take(n,b,warn=True)
dask.bag.core.split(seq,n)
dask.bag.core.starmap_chunk(f,x,kwargs)
dask.bag.core.to_dataframe(seq,columns,dtypes)
dask.bag.core.to_textfiles(b,path,name_function=None,compression='infer',encoding=system_encoding,compute=True,storage_options=None,last_endline=False,**kwargs)
dask.bag.core.total_mem_usage(partition)
dask.bag.core.unpack_scalar_dask_kwargs(kwargs)
dask.bag.from_delayed(values)
dask.bag.from_sequence(seq,partition_size=None,npartitions=None)
dask.bag.from_url(urls)
dask.bag.map(func,*args,**kwargs)
dask.bag.map_partitions(func,*args,**kwargs)
dask.bag.range(n,npartitions)
dask.bag.to_textfiles(b,path,name_function=None,compression='infer',encoding=system_encoding,compute=True,storage_options=None,last_endline=False,**kwargs)
dask.bag.zip(*bags)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/bag/utils.py----------------------------------------
A:dask.bag.utils.a->a.compute(scheduler=scheduler).compute(scheduler=scheduler)
A:dask.bag.utils.b->b.compute(scheduler=scheduler).compute(scheduler=scheduler)
dask.bag.assert_eq(a,b,scheduler='sync')
dask.bag.utils.assert_eq(a,b,scheduler='sync')


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/bag/avro.py----------------------------------------
A:dask.bag.avro.c->fo.read(1)
A:dask.bag.avro.b->ord(fo.read(1))
A:dask.bag.avro.size->read_long(fo)
A:dask.bag.avro.n_keys->read_long(fo)
A:dask.bag.avro.out['sync']->fo.read(SYNC_SIZE)
A:dask.bag.avro.out['header_size']->fo.tell()
A:dask.bag.avro.out['head_bytes']->fo.read(out['header_size'])
A:dask.bag.avro.head->read_header(f)
A:dask.bag.avro.(fs, fs_token, paths)->get_fs_token_paths(urlpath, mode='rb', storage_options=storage_options)
A:dask.bag.avro.dhead->delayed(open_head)
A:dask.bag.avro.out->type(b)(graph, name, b.npartitions)
A:dask.bag.avro.(heads, sizes)->zip(*out)
A:dask.bag.avro.dread->delayed(read_file)
A:dask.bag.avro.off->list(range(0, size, blocksize))
A:dask.bag.avro.f->OpenFile(fs, path, compression=compression)
A:dask.bag.avro.token->fs_tokenize(fs_token, delimiter, path, fs.ukey(path), compression, offset)
A:dask.bag.avro.files->open_files(filename, 'wb', name_function=name_function, num=b.npartitions, **storage_options)
A:dask.bag.avro.chunk->read_block(f, off, l, head['sync'])
A:dask.bag.avro.i->io.BytesIO(chunk)
A:dask.bag.avro.graph->dask.highlevelgraph.HighLevelGraph.from_collections(name, dsk, dependencies=[b])
dask.bag.avro._verify_schema(s)
dask.bag.avro._write_avro_part(part,f,schema,codec,sync_interval,metadata)
dask.bag.avro.open_head(fs,path,compression)
dask.bag.avro.read_avro(urlpath,blocksize=100000000,storage_options=None,compression=None)
dask.bag.avro.read_bytes(fo)
dask.bag.avro.read_chunk(fobj,off,l,head)
dask.bag.avro.read_file(fo)
dask.bag.avro.read_header(fo)
dask.bag.avro.read_long(fo)
dask.bag.avro.to_avro(b,filename,schema,name_function=None,storage_options=None,codec='null',sync_interval=16000,metadata=None,compute=True,**kwargs)
dask.bag.read_avro(urlpath,blocksize=100000000,storage_options=None,compression=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/bag/random.py----------------------------------------
A:dask.bag.random.res->_sample_with_replacement(population=population, k=k, split_every=split_every)
A:dask.bag.random.k_i->len(s_i)
A:dask.bag.random.stream->iter(population)
A:dask.bag.random.w->math.exp(math.log(rnd.random()) / k)
A:dask.bag.random.e->next(stream)
A:dask.bag.random.min_nxt->min(nxt)
dask.bag.random._finalize_sample(reduce_iter,k)
dask.bag.random._geometric(p)
dask.bag.random._sample(population,k,split_every)
dask.bag.random._sample_map_partitions(population,k)
dask.bag.random._sample_reduce(reduce_iter,k,replace)
dask.bag.random._sample_with_replacement(population,k,split_every)
dask.bag.random._sample_with_replacement_map_partitions(population,k)
dask.bag.random._weighted_sampling_without_replacement(population,weights,k)
dask.bag.random.choices(population,k=1,split_every=None)
dask.bag.random.sample(population,k,split_every=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/bag/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/bag/tests/test_text.py----------------------------------------
A:dask.bag.tests.test_text.compute->partial(compute, scheduler='sync')
A:dask.bag.tests.test_text.expected->''.join([files[v] for v in sorted(files)])
A:dask.bag.tests.test_text.b->read_text('*.txt', files_per_partition=10, include_path=True)
A:dask.bag.tests.test_text.(L,)->compute(b)
A:dask.bag.tests.test_text.o->read_text(sorted(files), compression=fmt, blocksize=bs, encoding=encoding, include_path=include_path)
A:dask.bag.tests.test_text.(paths,)->compute(o.pluck(1))
A:dask.bag.tests.test_text.expected_paths->list(concat([[k] * v.count('\n') for (k, v) in files.items()]))
A:dask.bag.tests.test_text.blocks->read_text('.test.accounts.*.json', compression=fmt, blocksize=bs, encoding=encoding, collection=False)
A:dask.bag.tests.test_text.L->compute(*blocks)
A:dask.bag.tests.test_text.f->read_text(fn, collection=False)
A:dask.bag.tests.test_text.result->result.compute(scheduler='sync').compute(scheduler='sync')
A:dask.bag.tests.test_text.l->len(b.take(100, npartitions=1))
A:dask.bag.tests.test_text.p->read_text('*.txt', files_per_partition=10, include_path=True).take(100, npartitions=1)
A:dask.bag.tests.test_text.p_unique_paths->set(p_paths)
A:dask.bag.tests.test_text.b_unique_paths->set(b_paths)
A:dask.bag.tests.test_text.vals->read_text('.test.delim.txt', linedelimiter='$$$$', blocksize=2).compute()
dask.bag.tests.test_text.test_complex_delimiter()
dask.bag.tests.test_text.test_errors()
dask.bag.tests.test_text.test_files_per_partition()
dask.bag.tests.test_text.test_read_text(fmt,bs,encoding,include_path)
dask.bag.tests.test_text.test_read_text_unicode_no_collection(tmp_path)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/bag/tests/test_avro.py----------------------------------------
A:dask.bag.tests.test_avro.fastavro->pytest.importorskip('fastavro')
A:dask.bag.tests.test_avro.tmpdir->str(tmpdir)
A:dask.bag.tests.test_avro.fn->os.path.join(tmpdir, 'out*.avro')
A:dask.bag.tests.test_avro.b->dask.bag.from_sequence(expected, npartitions=3)
A:dask.bag.tests.test_avro.fn1->os.path.join(tmpdir, 'one.avro')
A:dask.bag.tests.test_avro.fn2->os.path.join(tmpdir, 'two.avro')
A:dask.bag.tests.test_avro.out->dask.bag.from_sequence(expected, npartitions=3).to_avro(fn, schema)
A:dask.bag.tests.test_avro.b2->dask.bag.read_avro(fn)
dask.bag.tests.test_avro.test_invalid_schema(tmpdir)
dask.bag.tests.test_avro.test_onefile_oneblock(tmpdir)
dask.bag.tests.test_avro.test_roundtrip(tmpdir,codec)
dask.bag.tests.test_avro.test_roundtrip_simple(tmpdir)
dask.bag.tests.test_avro.test_twofile_multiblock(tmpdir)
dask.bag.tests.test_avro.test_twofile_oneblock(tmpdir)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/bag/tests/test_bag.py----------------------------------------
A:dask.bag.tests.test_bag.b->dask.bag.from_sequence(range(1, 100), npartitions=3)
A:dask.bag.tests.test_bag.result->dask.bag.from_sequence(range(1, 100), npartitions=3).compute(scheduler='sync')
A:dask.bag.tests.test_bag.b2->dask.bag.from_sequence(range(1, 100), npartitions=3).groupby(lambda x: x % 2)
A:dask.bag.tests.test_bag.x->dask.bag.from_sequence([{'name': 'test1', 'v1': 1}, {'name': 'test2', 'v1': 2}], npartitions=2)
A:dask.bag.tests.test_bag.x2->dask.bag.from_sequence(range(1, 100), npartitions=3).groupby(lambda x: x % 2).compute()
A:dask.bag.tests.test_bag.x_sum->sum(x)
A:dask.bag.tests.test_bag.a->dask.bag.from_sequence([1, 2], npartitions=2)
A:dask.bag.tests.test_bag.fewer_parts->dask.bag.from_sequence(range(100), npartitions=5)
A:dask.bag.tests.test_bag.unequal->dask.bag.from_sequence(range(110), npartitions=10)
A:dask.bag.tests.test_bag.max_second->dask.bag.from_sequence(range(1, 100), npartitions=3).pluck(1).max()
A:dask.bag.tests.test_bag.c->dask.bag.from_sequence(range(1, 100), npartitions=3).map(lambda x: x)
A:dask.bag.tests.test_bag.expected->list(unique(seq, key=lambda x: x['a']))
A:dask.bag.tests.test_bag.(one, two, three)->dask.bag.from_sequence(range(1, 100), npartitions=3).unzip(3)
A:dask.bag.tests.test_bag.c2->dask.bag.from_sequence(range(1, 100), npartitions=3).topk(4, key=lambda x: -x, split_every=2)
A:dask.bag.tests.test_bag.acc->acc.copy().copy()
A:dask.bag.tests.test_bag.d->y.map(lambda a: dict(**a, v4=a['v3'] + 1)).to_dataframe()
A:dask.bag.tests.test_bag.e->dask.bag.read_text('s3://tip-data/t*.gz', storage_options=dict(anon=True))
A:dask.bag.tests.test_bag.bag->dask.bag.from_delayed(obj)
A:dask.bag.tests.test_bag.bag2->dask.bag.from_delayed(obj).filter(None).frequencies(split_every=2)
A:dask.bag.tests.test_bag.L->list(range(1001))
A:dask.bag.tests.test_bag.np->pytest.importorskip('numpy')
A:dask.bag.tests.test_bag.data->list(range(100))
A:dask.bag.tests.test_bag.other->transform([1, 2, 3])
A:dask.bag.tests.test_bag.dsk->y.map(inc).__dask_optimize__(z.dask, z.__dask_keys__(), fuse_keys=y.__dask_keys__())
A:dask.bag.tests.test_bag.f->myopen(os.path.join(dir, '1.' + ext), 'rb')
A:dask.bag.tests.test_bag.g->dask.bag.from_sequence(range(1, 100), npartitions=3).foldby(iseven, add, 0, add, 0, split_every=m)
A:dask.bag.tests.test_bag.results->dask.get(c.dask, c.__dask_keys__())
A:dask.bag.tests.test_bag.dx->dask.bag.from_sequence(x, npartitions=10)
A:dask.bag.tests.test_bag.dy->dask.bag.from_sequence(y, npartitions=10)
A:dask.bag.tests.test_bag.sol->pytest.importorskip('pandas').DataFrame({'a': range(100)})
A:dask.bag.tests.test_bag.dy_mean->dask.delayed(dy_mean)
A:dask.bag.tests.test_bag.layer->hlg_layer(b.map_partitions(lambda x: x, token='test-string').dask, 'test-string')
A:dask.bag.tests.test_bag.outp->optimize(inp, ['c'], rename_fused_keys=False)
A:dask.bag.tests.test_bag.b_enc->dask.bag.from_sequence(range(1, 100), npartitions=3).str.strip().map(lambda x: x.encode('utf-8'))
A:dask.bag.tests.test_bag.c_enc->dask.bag.from_sequence(range(1, 100), npartitions=3).map(lambda x: x).str.strip().map(lambda x: x.encode('utf-8'))
A:dask.bag.tests.test_bag.df->dask.bag.from_sequence(range(1, 100), npartitions=3).map_partitions(f).to_dataframe(meta=sol)
A:dask.bag.tests.test_bag.y->y.map(lambda a: dict(**a, v4=a['v3'] + 1)).map(lambda a: dict(**a, v4=a['v3'] + 1))
A:dask.bag.tests.test_bag.z->y.map(lambda a: dict(**a, v4=a['v3'] + 1)).map(lambda a: dict(**a, v4=a['v3'] + 1)).map(inc)
A:dask.bag.tests.test_bag.result2->dict(result)
A:dask.bag.tests.test_bag.dd->pytest.importorskip('dask.dataframe')
A:dask.bag.tests.test_bag.pd->pytest.importorskip('pandas')
A:dask.bag.tests.test_bag.text->str(dict(d.dask))
A:dask.bag.tests.test_bag.out->dask.bag.from_sequence(range(1, 100), npartitions=3).groupby(lambda x: x % 2834, max_branch=24, shuffle='tasks')
A:dask.bag.tests.test_bag.B->dask.bag.from_sequence(['abc', '123', 'xyz'], npartitions=2)
A:dask.bag.tests.test_bag.dictbag->BagOfDicts(*db.from_sequence([{'a': {'b': 'c'}}])._args)
A:dask.bag.tests.test_bag.bin_data->'€'.encode()
A:dask.bag.tests.test_bag.(a, b, c)->dask.bag.from_sequence(range(1, 100), npartitions=3).map(inc).to_delayed()
A:dask.bag.tests.test_bag.t->dask.bag.from_sequence(range(1, 100), npartitions=3).sum().to_delayed()
A:dask.bag.tests.test_bag.[d]->dask.bag.from_sequence(range(1, 100), npartitions=3).groupby(lambda x: x % 2).to_textfiles(str(tmpdir), compute=False)
A:dask.bag.tests.test_bag.[d2]->dask.bag.from_sequence(range(1, 100), npartitions=3).groupby(lambda x: x % 2).to_delayed(optimize_graph=False)
A:dask.bag.tests.test_bag.d2->y.map(lambda a: dict(**a, v4=a['v3'] + 1)).map(lambda a: dict(**a, v4=a['v3'] + 1)).to_dataframe(optimize_graph=False)
A:dask.bag.tests.test_bag.bb->from_delayed([a, b, c])
A:dask.bag.tests.test_bag.asum_value->delayed(sum)(a)
A:dask.bag.tests.test_bag.asum_item->dask.bag.Item.from_delayed(asum_value)
A:dask.bag.tests.test_bag.delayed_records->delayed(lazy_records, pure=False)
A:dask.bag.tests.test_bag.evens->dask.bag.from_sequence(range(0, hi, 2), npartitions=npartitions)
A:dask.bag.tests.test_bag.odds->dask.bag.from_sequence(range(1, hi, 2), npartitions=npartitions)
A:dask.bag.tests.test_bag.pairs->dask.bag.zip(evens, odds)
A:dask.bag.tests.test_bag.total_mem->sum(b.map_partitions(total_mem_usage).compute())
A:dask.bag.tests.test_bag.r->dask.bag.from_sequence(range(1, 100), npartitions=3).accumulate(add)
A:dask.bag.tests.test_bag.partitions->dask.get(out.dask, out.__dask_keys__())
A:dask.bag.tests.test_bag.vals->dask.bag.compute(b.min(split_every=2), b.max(split_every=2), scheduler='sync')
A:dask.bag.tests.test_bag.sp->pytest.importorskip('scipy.sparse')
A:dask.bag.tests.test_bag.s->dask.bag.from_sequence(range(1, 100), npartitions=3).sum()
A:dask.bag.tests.test_bag.s2->loads(dumps(s))
A:dask.bag.tests.test_bag.res->dask.bag.from_sequence(range(1, 100), npartitions=3).reduction(func, sum)
A:dask.bag.tests.test_bag.paths->dask.bag.from_sequence(range(1, 100), npartitions=3).to_textfiles('foo*')
A:dask.bag.tests.test_bag.mybag->dask.bag.from_sequence(['a', 'b', 'c'])
A:dask.bag.tests.test_bag.in_memory->weakref.WeakSet()
A:dask.bag.tests.test_bag.o->C(i)
A:dask.bag.tests.test_bag.da->pytest.importorskip('dask.array')
A:dask.bag.tests.test_bag.i->dask.bag.Item.from_delayed(delayed)
A:dask.bag.tests.test_bag.back->dask.bag.Item.from_delayed(delayed).to_delayed(optimize_graph=optimize)
A:dask.bag.tests.test_bag.ntasks->len(db.from_sequence(range(nitems)).dask)
A:dask.bag.tests.test_bag.total_mem_b->sum(b.map_partitions(total_mem_usage).compute())
A:dask.bag.tests.test_bag.total_mem_c->sum(c.map_partitions(total_mem_usage).compute())
dask.bag.tests.test_bag.BagOfDicts(db.Bag)
dask.bag.tests.test_bag.BagOfDicts.get(self,key,default=None)
dask.bag.tests.test_bag.BagOfDicts.set(self,key,value)
dask.bag.tests.test_bag.StrictReal(int)
dask.bag.tests.test_bag.StrictReal.__eq__(self,other)
dask.bag.tests.test_bag.StrictReal.__ne__(self,other)
dask.bag.tests.test_bag.iseven(x)
dask.bag.tests.test_bag.isodd(x)
dask.bag.tests.test_bag.test_Bag()
dask.bag.tests.test_bag.test_accumulate()
dask.bag.tests.test_bag.test_aggregation(npartitions)
dask.bag.tests.test_bag.test_args()
dask.bag.tests.test_bag.test_bag_class_extend()
dask.bag.tests.test_bag.test_bag_compute_forward_kwargs()
dask.bag.tests.test_bag.test_bag_groupby_normal_hash()
dask.bag.tests.test_bag.test_bag_groupby_pure_hash()
dask.bag.tests.test_bag.test_bag_map()
dask.bag.tests.test_bag.test_bag_paths()
dask.bag.tests.test_bag.test_bag_picklable()
dask.bag.tests.test_bag.test_bag_with_single_callable()
dask.bag.tests.test_bag.test_bagged_array_delayed()
dask.bag.tests.test_bag.test_can_use_dict_to_make_concrete()
dask.bag.tests.test_bag.test_concat()
dask.bag.tests.test_bag.test_concat_after_map()
dask.bag.tests.test_bag.test_dask_layers()
dask.bag.tests.test_bag.test_dask_layers_to_delayed(optimize)
dask.bag.tests.test_bag.test_default_partitioning_worker_saturation(nworkers)
dask.bag.tests.test_bag.test_distinct()
dask.bag.tests.test_bag.test_distinct_with_key()
dask.bag.tests.test_bag.test_empty()
dask.bag.tests.test_bag.test_empty_bag()
dask.bag.tests.test_bag.test_ensure_compute_output_is_concrete()
dask.bag.tests.test_bag.test_filter()
dask.bag.tests.test_bag.test_flatten()
dask.bag.tests.test_bag.test_fold()
dask.bag.tests.test_bag.test_fold_bag()
dask.bag.tests.test_bag.test_foldby()
dask.bag.tests.test_bag.test_foldby_tree_reduction()
dask.bag.tests.test_bag.test_frequencies()
dask.bag.tests.test_bag.test_frequencies_sorted()
dask.bag.tests.test_bag.test_from_delayed()
dask.bag.tests.test_bag.test_from_delayed_iterator()
dask.bag.tests.test_bag.test_from_empty_sequence()
dask.bag.tests.test_bag.test_from_long_sequence()
dask.bag.tests.test_bag.test_from_s3()
dask.bag.tests.test_bag.test_from_sequence()
dask.bag.tests.test_bag.test_from_url()
dask.bag.tests.test_bag.test_gh715()
dask.bag.tests.test_bag.test_groupby()
dask.bag.tests.test_bag.test_groupby_tasks()
dask.bag.tests.test_bag.test_groupby_tasks_2(size,npartitions,groups)
dask.bag.tests.test_bag.test_groupby_tasks_3()
dask.bag.tests.test_bag.test_groupby_tasks_names()
dask.bag.tests.test_bag.test_groupby_with_indexer()
dask.bag.tests.test_bag.test_groupby_with_npartitions_changed()
dask.bag.tests.test_bag.test_groupby_with_scheduler_func()
dask.bag.tests.test_bag.test_inline_singleton_lists()
dask.bag.tests.test_bag.test_iter()
dask.bag.tests.test_bag.test_join(transform)
dask.bag.tests.test_bag.test_keys()
dask.bag.tests.test_bag.test_lambdas()
dask.bag.tests.test_bag.test_lazify()
dask.bag.tests.test_bag.test_lazify_task()
dask.bag.tests.test_bag.test_map_is_lazy()
dask.bag.tests.test_bag.test_map_keynames()
dask.bag.tests.test_bag.test_map_method()
dask.bag.tests.test_bag.test_map_partitions()
dask.bag.tests.test_bag.test_map_partitions_arg()
dask.bag.tests.test_bag.test_map_partitions_args_kwargs()
dask.bag.tests.test_bag.test_map_partitions_blockwise()
dask.bag.tests.test_bag.test_map_releases_element_references_as_soon_as_possible()
dask.bag.tests.test_bag.test_map_total_mem_usage()
dask.bag.tests.test_bag.test_map_with_iterator_function()
dask.bag.tests.test_bag.test_msgpack_unicode()
dask.bag.tests.test_bag.test_multiple_repartition_partition_size()
dask.bag.tests.test_bag.test_non_splittable_reductions(npartitions)
dask.bag.tests.test_bag.test_npartitions_saturation(nworkers)
dask.bag.tests.test_bag.test_optimize_fuse_keys()
dask.bag.tests.test_bag.test_partition_collect()
dask.bag.tests.test_bag.test_pluck()
dask.bag.tests.test_bag.test_pluck_with_default()
dask.bag.tests.test_bag.test_product()
dask.bag.tests.test_bag.test_random_sample_different_definitions()
dask.bag.tests.test_bag.test_random_sample_prob_range()
dask.bag.tests.test_bag.test_random_sample_random_state()
dask.bag.tests.test_bag.test_random_sample_repeated_computation()
dask.bag.tests.test_bag.test_random_sample_size()
dask.bag.tests.test_bag.test_range()
dask.bag.tests.test_bag.test_read_text()
dask.bag.tests.test_bag.test_read_text_encoding()
dask.bag.tests.test_bag.test_read_text_large()
dask.bag.tests.test_bag.test_read_text_large_gzip()
dask.bag.tests.test_bag.test_reduction_empty()
dask.bag.tests.test_bag.test_reduction_empty_aggregate(npartitions)
dask.bag.tests.test_bag.test_reduction_names()
dask.bag.tests.test_bag.test_reduction_with_non_comparable_objects()
dask.bag.tests.test_bag.test_reduction_with_sparse_matrices()
dask.bag.tests.test_bag.test_reductions()
dask.bag.tests.test_bag.test_reductions_are_lazy()
dask.bag.tests.test_bag.test_remove()
dask.bag.tests.test_bag.test_rename_fused_keys_bag()
dask.bag.tests.test_bag.test_repartition_input_errors()
dask.bag.tests.test_bag.test_repartition_names()
dask.bag.tests.test_bag.test_repartition_npartitions(nin,nout)
dask.bag.tests.test_bag.test_repartition_partition_size(nin,nout)
dask.bag.tests.test_bag.test_repartition_partition_size_complex_dtypes()
dask.bag.tests.test_bag.test_repeated_groupby()
dask.bag.tests.test_bag.test_repr(func)
dask.bag.tests.test_bag.test_starmap()
dask.bag.tests.test_bag.test_std()
dask.bag.tests.test_bag.test_str_empty_split()
dask.bag.tests.test_bag.test_string_namespace()
dask.bag.tests.test_bag.test_string_namespace_with_unicode()
dask.bag.tests.test_bag.test_take()
dask.bag.tests.test_bag.test_take_npartitions()
dask.bag.tests.test_bag.test_take_npartitions_warn()
dask.bag.tests.test_bag.test_temporary_directory(tmpdir)
dask.bag.tests.test_bag.test_to_dataframe()
dask.bag.tests.test_bag.test_to_dataframe_optimize_graph()
dask.bag.tests.test_bag.test_to_delayed()
dask.bag.tests.test_bag.test_to_delayed_optimize_graph(tmpdir)
dask.bag.tests.test_bag.test_to_textfiles(ext,myopen)
dask.bag.tests.test_bag.test_to_textfiles_empty_partitions()
dask.bag.tests.test_bag.test_to_textfiles_encoding()
dask.bag.tests.test_bag.test_to_textfiles_endlines()
dask.bag.tests.test_bag.test_to_textfiles_inputs()
dask.bag.tests.test_bag.test_to_textfiles_name_function_preserves_order()
dask.bag.tests.test_bag.test_to_textfiles_name_function_warn()
dask.bag.tests.test_bag.test_topk()
dask.bag.tests.test_bag.test_topk_with_multiarg_lambda()
dask.bag.tests.test_bag.test_topk_with_non_callable_key(npartitions)
dask.bag.tests.test_bag.test_tree_reductions()
dask.bag.tests.test_bag.test_unzip()
dask.bag.tests.test_bag.test_var()
dask.bag.tests.test_bag.test_zip(npartitions,hi=1000)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/bag/tests/test_random.py----------------------------------------
A:dask.bag.tests.test_random.seq->list(range(N))
A:dask.bag.tests.test_random.sut->dask.bag.from_sequence(seq, npartitions=3)
A:dask.bag.tests.test_random.li->list(random.sample(sut, k=10).compute())
A:dask.bag.tests.test_random.bag->dask.bag.from_sequence(range(10), partition_size=3)
A:dask.bag.tests.test_random.bag2->dask.bag.random.sample(bag, k=8, split_every=2)
A:dask.bag.tests.test_random.population->range(4)
A:dask.bag.tests.test_random.sampled->dask.bag.random._weighted_sampling_without_replacement(population=population, weights=p, k=k)
A:dask.bag.tests.test_random.A->dask.bag.from_sequence([[1, 2], [3, 4, 5], [6], [7]])
A:dask.bag.tests.test_random.B->dask.bag.from_sequence(['a', 'b', 'c', 'd'])
A:dask.bag.tests.test_random.a->dask.bag.random.choices(A.flatten(), k=B.count().compute()).repartition(4)
A:dask.bag.tests.test_random.C->dask.bag.zip(B, a).compute()
A:dask.bag.tests.test_random.(picks, _)->dask.bag.random._sample_with_replacement_map_partitions(seq, k)
dask.bag.tests.test_random.bhattacharyya(h1,h2)
dask.bag.tests.test_random.test_choices_empty_partition()
dask.bag.tests.test_random.test_choices_k_bigger_than_bag_size()
dask.bag.tests.test_random.test_choices_k_bigger_than_smallest_partition_size()
dask.bag.tests.test_random.test_choices_k_equal_bag_size_with_unbalanced_partitions()
dask.bag.tests.test_random.test_choices_size_exactly_k()
dask.bag.tests.test_random.test_choices_with_more_bag_partitons()
dask.bag.tests.test_random.test_partitions_are_coerced_to_lists()
dask.bag.tests.test_random.test_reservoir_sample_map_partitions_correctness()
dask.bag.tests.test_random.test_reservoir_sample_with_replacement_map_partitions_correctness()
dask.bag.tests.test_random.test_sample_empty_partition()
dask.bag.tests.test_random.test_sample_k_bigger_than_bag_size()
dask.bag.tests.test_random.test_sample_k_equal_bag_size_with_unbalanced_partitions()
dask.bag.tests.test_random.test_sample_k_larger_than_partitions()
dask.bag.tests.test_random.test_sample_return_bag()
dask.bag.tests.test_random.test_sample_size_exactly_k()
dask.bag.tests.test_random.test_sample_size_k_bigger_than_smallest_partition_size()
dask.bag.tests.test_random.test_sample_with_more_bag_partitons()
dask.bag.tests.test_random.test_weighted_sampling_without_replacement()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/bytes/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/bytes/core.py----------------------------------------
A:dask.bytes.core.(fs, fs_token, paths)->get_fs_token_paths(urlpath, mode='rb', storage_options=kwargs)
A:dask.bytes.core.blocksize->int(blocksize)
A:dask.bytes.core.comp->infer_compression(path)
A:dask.bytes.core.delayed_read->delayed(read_block_from_file)
A:dask.bytes.core.token->tokenize(fs_token, delimiter, path, fs.ukey(path), compression, offset)
A:dask.bytes.core.sample->f.read(sample)
A:dask.bytes.core.sample_buff->f.read(sample)
A:dask.bytes.core.new->f.read(sample)
dask.bytes.core.read_block_from_file(lazy_file,off,bs,delimiter)
dask.bytes.core.read_bytes(urlpath,delimiter=None,not_zero=False,blocksize='128MiB',sample='10kiB',compression=None,include_path=False,**kwargs)
dask.bytes.read_bytes(urlpath,delimiter=None,not_zero=False,blocksize='128MiB',sample='10kiB',compression=None,include_path=False,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/bytes/utils.py----------------------------------------
A:dask.bytes.utils.out->io.BytesIO()
dask.bytes.utils.zip_compress(data)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/bytes/tests/test_bytes_utils.py----------------------------------------
A:dask.bytes.tests.test_bytes_utils.data->delimiter.join([b'123', b'456', b'789'])
A:dask.bytes.tests.test_bytes_utils.f->io.BytesIO(b'123\n456')
A:dask.bytes.tests.test_bytes_utils.so->infer_storage_options(urlpath)
A:dask.bytes.tests.test_bytes_utils.options->infer_storage_options('%s://Bucket-name.com/test.csv' % protocol)
A:dask.bytes.tests.test_bytes_utils.test_filepath->os.path.join('path', 'to', 'file.txt')
A:dask.bytes.tests.test_bytes_utils.path->CustomFSPath(test_filepath)
dask.bytes.tests.test_bytes_utils.test_infer_storage_options()
dask.bytes.tests.test_bytes_utils.test_infer_storage_options_c(urlpath,expected_path)
dask.bytes.tests.test_bytes_utils.test_read_block()
dask.bytes.tests.test_bytes_utils.test_seek_delimiter_endline()
dask.bytes.tests.test_bytes_utils.test_stringify_path()


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/bytes/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/bytes/tests/test_s3.py----------------------------------------
A:dask.bytes.tests.test_s3.s3fs->pytest.importorskip('s3fs')
A:dask.bytes.tests.test_s3.boto3->pytest.importorskip('boto3')
A:dask.bytes.tests.test_s3.moto->pytest.importorskip('moto', minversion='1.3.14')
A:dask.bytes.tests.test_s3.requests->pytest.importorskip('requests')
A:dask.bytes.tests.test_s3.compute->partial(compute, scheduler='sync')
A:dask.bytes.tests.test_s3.saved_environ->dict(os.environ)
A:dask.bytes.tests.test_s3.proc->subprocess.Popen(shlex.split('moto_server s3 -p 5555'), stdout=subprocess.DEVNULL)
A:dask.bytes.tests.test_s3.r->pytest.importorskip('requests').get(endpoint_uri)
A:dask.bytes.tests.test_s3.client->pytest.importorskip('boto3').client('s3', endpoint_url='http://127.0.0.1:5555/')
A:dask.bytes.tests.test_s3.fs->pytest.importorskip('s3fs').S3FileSystem(anon=True, client_kwargs={'endpoint_url': 'http://127.0.0.1:5555/'})
A:dask.bytes.tests.test_s3.np->pytest.importorskip('numpy')
A:dask.bytes.tests.test_s3.pd->pytest.importorskip('pandas')
A:dask.bytes.tests.test_s3.sample->pytest.importorskip('pandas').DataFrame(data)
A:dask.bytes.tests.test_s3.df->pytest.importorskip('dask.dataframe').from_pandas(data, chunksize=500)
A:dask.bytes.tests.test_s3.file->io.BytesIO()
A:dask.bytes.tests.test_s3.sfile->io.TextIOWrapper(file)
A:dask.bytes.tests.test_s3.s3->DaskS3FileSystem(username='key', password='secret')
A:dask.bytes.tests.test_s3.fils->open_files(paths, mode='wb', **s3so)
A:dask.bytes.tests.test_s3.(sample, values)->read_bytes('s3://compress/test/accounts.*', compression=fmt, blocksize=blocksize, **s3so)
A:dask.bytes.tests.test_s3.results->compute(*concat(values))
A:dask.bytes.tests.test_s3.(_, values)->read_bytes('s3://' + test_bucket_name + '/test/accounts*', blocksize=blocksize, delimiter=d, **s3so)
A:dask.bytes.tests.test_s3.(_, L)->read_bytes(f's3://{test_bucket_name}/nyc-taxi/2014/*.csv', blocksize=None, anon=True, **s3so)
A:dask.bytes.tests.test_s3.(_, vals)->read_bytes('s3://' + test_bucket_name + '/test/account*', blocksize=blocksize, **s3so)
A:dask.bytes.tests.test_s3.ourlines->b''.join(res).split(b'\n')
A:dask.bytes.tests.test_s3.testlines->b''.join((files[k] for k in sorted(files))).split(b'\n')
A:dask.bytes.tests.test_s3.(_, values2)->read_bytes('s3://' + test_bucket_name + '/test/accounts*', blocksize=blocksize, delimiter=b'foo', **s3so)
A:dask.bytes.tests.test_s3.ours->b''.join(res)
A:dask.bytes.tests.test_s3.test->b''.join((files[v] for v in sorted(files)))
A:dask.bytes.tests.test_s3.myfiles->open_files('s3://' + test_bucket_name + '/test/accounts.*', mode=mode, **s3so)
A:dask.bytes.tests.test_s3.data->pytest.importorskip('pandas').DataFrame({'i32': np.array([0, 5, 2, 5])})
A:dask.bytes.tests.test_s3.(_, a)->read_bytes('s3://compress/test/accounts.*', anon=True, **s3so)
A:dask.bytes.tests.test_s3.(_, b)->read_bytes('s3://compress/test/accounts.*', anon=True, **s3so)
A:dask.bytes.tests.test_s3.(_, c)->read_bytes('s3://compress/test/accounts.*', anon=True, **s3so)
A:dask.bytes.tests.test_s3.dd->pytest.importorskip('dask.dataframe')
A:dask.bytes.tests.test_s3.df2->pytest.importorskip('dask.dataframe').read_parquet(url, index=False, engine=engine, storage_options=s3so)
A:dask.bytes.tests.test_s3.df3->pytest.importorskip('dask.dataframe').read_parquet(url, engine=engine, storage_options=s3so, open_file_options={'open_file_func': _open})
A:dask.bytes.tests.test_s3.df4->pytest.importorskip('dask.dataframe').read_parquet(url, engine=engine, storage_options=s3so, open_file_options={'cache_type': 'all'})
dask.bytes.tests.test_s3.engine(request)
dask.bytes.tests.test_s3.ensure_safe_environment_variables()
dask.bytes.tests.test_s3.s3(s3_base)
dask.bytes.tests.test_s3.s3_base()
dask.bytes.tests.test_s3.s3_context(bucket=test_bucket_name,files=files)
dask.bytes.tests.test_s3.s3_with_yellow_tripdata(s3)
dask.bytes.tests.test_s3.s3so()
dask.bytes.tests.test_s3.test_compression(s3,fmt,blocksize,s3so)
dask.bytes.tests.test_s3.test_get_s3()
dask.bytes.tests.test_s3.test_modification_time_read_bytes(s3,s3so)
dask.bytes.tests.test_s3.test_open_files(s3,mode,s3so)
dask.bytes.tests.test_s3.test_open_files_write(s3,s3so)
dask.bytes.tests.test_s3.test_parquet(s3,engine,s3so,metadata_file)
dask.bytes.tests.test_s3.test_parquet_append(s3,engine,s3so)
dask.bytes.tests.test_s3.test_parquet_wstoragepars(s3,s3so,engine)
dask.bytes.tests.test_s3.test_read_bytes(s3,s3so)
dask.bytes.tests.test_s3.test_read_bytes_block(s3,blocksize,s3so)
dask.bytes.tests.test_s3.test_read_bytes_blocksize_none(s3,s3so)
dask.bytes.tests.test_s3.test_read_bytes_blocksize_on_large_data(s3_with_yellow_tripdata,s3so)
dask.bytes.tests.test_s3.test_read_bytes_delimited(s3,blocksize,s3so)
dask.bytes.tests.test_s3.test_read_bytes_non_existing_glob(s3,s3so)
dask.bytes.tests.test_s3.test_read_bytes_sample_delimiter(s3,s3so)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/bytes/tests/test_local.py----------------------------------------
A:dask.bytes.tests.test_local.compute->partial(compute, scheduler='sync')
A:dask.bytes.tests.test_local.(sample, values)->func('.test.accounts.*', compression='not-found')
A:dask.bytes.tests.test_local.results->compute(*concat(values))
A:dask.bytes.tests.test_local.(sample, _)->read_bytes('.test.accounts.1.json', sample=False)
A:dask.bytes.tests.test_local.(sample, vals)->read_bytes('.test.account*', blocksize=bs)
A:dask.bytes.tests.test_local.ourlines->b''.join(res).split(b'\n')
A:dask.bytes.tests.test_local.testlines->b''.join((files[k] for k in sorted(files))).split(b'\n')
A:dask.bytes.tests.test_local.(_, _, paths)->read_bytes('.test.accounts.*', include_path=True)
A:dask.bytes.tests.test_local.url->pathlib.Path('file://.test.accounts.*')
A:dask.bytes.tests.test_local.(_, values)->read_bytes('.test.accounts*', blocksize=bs, delimiter=d)
A:dask.bytes.tests.test_local.(_, values2)->read_bytes('.test.accounts*', blocksize=bs, delimiter=b'foo')
A:dask.bytes.tests.test_local.ours->b''.join(res)
A:dask.bytes.tests.test_local.test->b''.join((files[v] for v in sorted(files)))
A:dask.bytes.tests.test_local.files2->valmap(compress[fmt], files)
A:dask.bytes.tests.test_local.myfiles->open_files('.test.accounts.*')
A:dask.bytes.tests.test_local.x->f.read()
A:dask.bytes.tests.test_local.(_, a)->read_bytes('.test.accounts.*')
A:dask.bytes.tests.test_local.(_, b)->read_bytes('.test.accounts.*')
A:dask.bytes.tests.test_local.a->list(concat(a))
A:dask.bytes.tests.test_local.b->list(concat(b))
A:dask.bytes.tests.test_local.(_, c)->read_bytes('.test.accounts.*')
A:dask.bytes.tests.test_local.c->list(concat(c))
A:dask.bytes.tests.test_local.tmpdir->str(tmpdir)
A:dask.bytes.tests.test_local.files->open_files(fn, compression='gzip', mode='rt')
A:dask.bytes.tests.test_local.d->f.read()
A:dask.bytes.tests.test_local.myfiles2->cloudpickle.loads(cloudpickle.dumps(myfiles))
A:dask.bytes.tests.test_local.fn->str(tmpdir / 'myfile.txt.gz')
A:dask.bytes.tests.test_local.here->os.getcwd()
A:dask.bytes.tests.test_local.out->LocalFileSystem().glob('*')
A:dask.bytes.tests.test_local.fs->LocalFileSystem()
A:dask.bytes.tests.test_local.res->f.read()
dask.bytes.tests.test_local.test_abs_paths(tmpdir)
dask.bytes.tests.test_local.test_bad_compression()
dask.bytes.tests.test_local.test_compression(fmt,blocksize)
dask.bytes.tests.test_local.test_names()
dask.bytes.tests.test_local.test_not_found()
dask.bytes.tests.test_local.test_open_files()
dask.bytes.tests.test_local.test_open_files_compression(mode,fmt)
dask.bytes.tests.test_local.test_open_files_text_mode(encoding)
dask.bytes.tests.test_local.test_open_files_write(tmpdir,compression_opener)
dask.bytes.tests.test_local.test_parse_sample_bytes()
dask.bytes.tests.test_local.test_pickability_of_lazy_files(tmpdir)
dask.bytes.tests.test_local.test_py2_local_bytes(tmpdir)
dask.bytes.tests.test_local.test_read_bytes()
dask.bytes.tests.test_local.test_read_bytes_block()
dask.bytes.tests.test_local.test_read_bytes_blocksize_float_errs()
dask.bytes.tests.test_local.test_read_bytes_blocksize_none()
dask.bytes.tests.test_local.test_read_bytes_blocksize_types(blocksize)
dask.bytes.tests.test_local.test_read_bytes_delimited()
dask.bytes.tests.test_local.test_read_bytes_include_path()
dask.bytes.tests.test_local.test_read_bytes_no_sample()
dask.bytes.tests.test_local.test_read_bytes_sample_delimiter()
dask.bytes.tests.test_local.test_unordered_urlpath_errors()
dask.bytes.tests.test_local.test_with_paths()
dask.bytes.tests.test_local.test_with_urls()
dask.bytes.tests.test_local.to_uri(path)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/bytes/tests/test_compression.py----------------------------------------
A:dask.bytes.tests.test_compression.compressed->compress[fmt](data)
A:dask.bytes.tests.test_compression.b->BytesIO(compressed)
A:dask.bytes.tests.test_compression.g->File(b, mode='rb')
A:dask.bytes.tests.test_compression.data2->File(b, mode='rb').read()
dask.bytes.tests.test_compression.test_files(fmt,File)


----------------------------------------/dataset/nuaa/anaconda3/envs/dask2024.1.0/lib/python3.9/site-packages/dask/bytes/tests/test_http.py----------------------------------------
A:dask.bytes.tests.test_http.requests->pytest.importorskip('requests')
A:dask.bytes.tests.test_http.aiohttp->pytest.importorskip('aiohttp')
A:dask.bytes.tests.test_http.p->subprocess.Popen(cmd, cwd=d)
A:dask.bytes.tests.test_http.data->f.read(3)
A:dask.bytes.tests.test_http.expected->expected.read().read()
A:dask.bytes.tests.test_http.fs->open_files(root + '*')
A:dask.bytes.tests.test_http.dd->pytest.importorskip('dask.dataframe')
A:dask.bytes.tests.test_http.df->pytest.importorskip('dask.dataframe').read_parquet(['https://github.com/Parquet/parquet-compatibility/raw/master/parquet-testdata/impala/1.1.1-NONE/nation.impala.parquet'], engine=engine).compute()
A:dask.bytes.tests.test_http.b->pytest.importorskip('dask.dataframe').read_csv(url)
dask.bytes.tests.test_http.dir_server()
dask.bytes.tests.test_http.test_bag()
dask.bytes.tests.test_http.test_errors(dir_server)
dask.bytes.tests.test_http.test_fetch_range_with_headers(dir_server)
dask.bytes.tests.test_http.test_files(dir_server)
dask.bytes.tests.test_http.test_loc(dir_server)
dask.bytes.tests.test_http.test_open_glob(dir_server)
dask.bytes.tests.test_http.test_ops(dir_server,block_size)
dask.bytes.tests.test_http.test_ops_blocksize(dir_server)
dask.bytes.tests.test_http.test_parquet(engine)
dask.bytes.tests.test_http.test_read_csv()
dask.bytes.tests.test_http.test_simple(dir_server)

