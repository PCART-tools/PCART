
----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/kernel_approximation.py----------------------------------------
A:sklearn.kernel_approximation.X->check_array(X, accept_sparse='csr')
A:sklearn.kernel_approximation.random_state->check_random_state(self.random_state)
A:sklearn.kernel_approximation.self.random_offset_->check_random_state(self.random_state).uniform(0, 2 * np.pi, size=self.n_components)
A:sklearn.kernel_approximation.projection->safe_sparse_dot(X, self.random_weights_)
A:sklearn.kernel_approximation.uniform->check_random_state(self.random_state).uniform(size=(n_features, self.n_components))
A:sklearn.kernel_approximation.sparse->scipy.sparse.issparse(X)
A:sklearn.kernel_approximation.X_step->scipy.sparse.csr_matrix((data_step, indices, indptr), shape=X.shape, dtype=X.dtype, copy=False)
A:sklearn.kernel_approximation.X_step[non_zero]->numpy.sqrt(X_nz * self.sample_interval_)
A:sklearn.kernel_approximation.factor_nz->numpy.sqrt(step_nz / np.cosh(np.pi * j * self.sample_interval_))
A:sklearn.kernel_approximation.indices->check_array(X, accept_sparse='csr').indices.copy()
A:sklearn.kernel_approximation.indptr->check_array(X, accept_sparse='csr').indptr.copy()
A:sklearn.kernel_approximation.data_step->numpy.sqrt(X.data * self.sample_interval_)
A:sklearn.kernel_approximation.rnd->check_random_state(self.random_state)
A:sklearn.kernel_approximation.n_components->min(n_samples, n_components)
A:sklearn.kernel_approximation.inds->check_random_state(self.random_state).permutation(n_samples)
A:sklearn.kernel_approximation.basis_kernel->pairwise_kernels(basis, metric=self.kernel, filter_params=True, **self._get_kernel_params())
A:sklearn.kernel_approximation.(U, S, V)->svd(basis_kernel)
A:sklearn.kernel_approximation.S->numpy.maximum(S, 1e-12)
A:sklearn.kernel_approximation.self.normalization_->numpy.dot(U / np.sqrt(S), V)
A:sklearn.kernel_approximation.kernel_params->self._get_kernel_params()
A:sklearn.kernel_approximation.embedded->pairwise_kernels(X, self.components_, metric=self.kernel, filter_params=True, **kernel_params)
A:sklearn.kernel_approximation.params[param]->getattr(self, param)
sklearn.kernel_approximation.AdditiveChi2Sampler(self,sample_steps=2,sample_interval=None)
sklearn.kernel_approximation.AdditiveChi2Sampler.__init__(self,sample_steps=2,sample_interval=None)
sklearn.kernel_approximation.AdditiveChi2Sampler._transform_dense(self,X)
sklearn.kernel_approximation.AdditiveChi2Sampler._transform_sparse(self,X)
sklearn.kernel_approximation.AdditiveChi2Sampler.fit(self,X,y=None)
sklearn.kernel_approximation.AdditiveChi2Sampler.transform(self,X)
sklearn.kernel_approximation.Nystroem(self,kernel='rbf',gamma=None,coef0=None,degree=None,kernel_params=None,n_components=100,random_state=None)
sklearn.kernel_approximation.Nystroem.__init__(self,kernel='rbf',gamma=None,coef0=None,degree=None,kernel_params=None,n_components=100,random_state=None)
sklearn.kernel_approximation.Nystroem._get_kernel_params(self)
sklearn.kernel_approximation.Nystroem.fit(self,X,y=None)
sklearn.kernel_approximation.Nystroem.transform(self,X)
sklearn.kernel_approximation.RBFSampler(self,gamma=1.0,n_components=100,random_state=None)
sklearn.kernel_approximation.RBFSampler.__init__(self,gamma=1.0,n_components=100,random_state=None)
sklearn.kernel_approximation.RBFSampler.fit(self,X,y=None)
sklearn.kernel_approximation.RBFSampler.transform(self,X)
sklearn.kernel_approximation.SkewedChi2Sampler(self,skewedness=1.0,n_components=100,random_state=None)
sklearn.kernel_approximation.SkewedChi2Sampler.__init__(self,skewedness=1.0,n_components=100,random_state=None)
sklearn.kernel_approximation.SkewedChi2Sampler.fit(self,X,y=None)
sklearn.kernel_approximation.SkewedChi2Sampler.transform(self,X)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/naive_bayes.py----------------------------------------
A:sklearn.naive_bayes.jll->safe_sparse_dot(X, (self.feature_log_prob_ - neg_prob).T)
A:sklearn.naive_bayes.log_prob_x->logsumexp(jll, axis=1)
A:sklearn.naive_bayes.(X, y)->check_X_y(X, y, 'csr')
A:sklearn.naive_bayes.n_new->float(sample_weight.sum())
A:sklearn.naive_bayes.new_mu->numpy.mean(X, axis=0)
A:sklearn.naive_bayes.new_var->numpy.var(X, axis=0)
A:sklearn.naive_bayes.n_total->float(n_past + n_new)
A:sklearn.naive_bayes.sample_weight->numpy.atleast_2d(sample_weight)
A:sklearn.naive_bayes.n_classes->len(self.classes_)
A:sklearn.naive_bayes.self.theta_->numpy.zeros((n_classes, n_features))
A:sklearn.naive_bayes.self.sigma_->numpy.zeros((n_classes, n_features))
A:sklearn.naive_bayes.self.class_count_->numpy.zeros(n_effective_classes, dtype=np.float64)
A:sklearn.naive_bayes.priors->numpy.asarray(self.priors)
A:sklearn.naive_bayes.self.class_prior_->numpy.zeros(len(self.classes_), dtype=np.float64)
A:sklearn.naive_bayes.unique_y->numpy.unique(y)
A:sklearn.naive_bayes.unique_y_in_classes->numpy.in1d(unique_y, classes)
A:sklearn.naive_bayes.i->classes.searchsorted(y_i)
A:sklearn.naive_bayes.N_i->sw_i.sum()
A:sklearn.naive_bayes.(new_theta, new_sigma)->self._update_mean_variance(self.class_count_[i], self.theta_[i, :], self.sigma_[i, :], X_i, sw_i)
A:sklearn.naive_bayes.X->binarize(X, threshold=self.binarize)
A:sklearn.naive_bayes.jointi->numpy.log(self.class_prior_[i])
A:sklearn.naive_bayes.self.class_log_prior_->numpy.log(class_prior)
A:sklearn.naive_bayes.self.feature_count_->numpy.zeros((n_effective_classes, n_features), dtype=np.float64)
A:sklearn.naive_bayes.Y->Y.astype(np.float64).astype(np.float64)
A:sklearn.naive_bayes.alpha->self._check_alpha()
A:sklearn.naive_bayes.labelbin->LabelBinarizer()
A:sklearn.naive_bayes.coef_->property(_get_coef)
A:sklearn.naive_bayes.intercept_->property(_get_intercept)
A:sklearn.naive_bayes.smoothed_cc->smoothed_fc.sum(axis=1)
A:sklearn.naive_bayes.neg_prob->numpy.log(1 - np.exp(self.feature_log_prob_))
sklearn.naive_bayes.BaseDiscreteNB(BaseNB)
sklearn.naive_bayes.BaseDiscreteNB._check_alpha(self)
sklearn.naive_bayes.BaseDiscreteNB._get_coef(self)
sklearn.naive_bayes.BaseDiscreteNB._get_intercept(self)
sklearn.naive_bayes.BaseDiscreteNB._update_class_log_prior(self,class_prior=None)
sklearn.naive_bayes.BaseDiscreteNB.fit(self,X,y,sample_weight=None)
sklearn.naive_bayes.BaseDiscreteNB.partial_fit(self,X,y,classes=None,sample_weight=None)
sklearn.naive_bayes.BaseNB(six.with_metaclass(ABCMeta,BaseEstimator,ClassifierMixin))
sklearn.naive_bayes.BaseNB._joint_log_likelihood(self,X)
sklearn.naive_bayes.BaseNB.predict(self,X)
sklearn.naive_bayes.BaseNB.predict_log_proba(self,X)
sklearn.naive_bayes.BaseNB.predict_proba(self,X)
sklearn.naive_bayes.BernoulliNB(self,alpha=1.0,binarize=0.0,fit_prior=True,class_prior=None)
sklearn.naive_bayes.BernoulliNB.__init__(self,alpha=1.0,binarize=0.0,fit_prior=True,class_prior=None)
sklearn.naive_bayes.BernoulliNB._count(self,X,Y)
sklearn.naive_bayes.BernoulliNB._joint_log_likelihood(self,X)
sklearn.naive_bayes.BernoulliNB._update_feature_log_prob(self,alpha)
sklearn.naive_bayes.GaussianNB(self,priors=None)
sklearn.naive_bayes.GaussianNB.__init__(self,priors=None)
sklearn.naive_bayes.GaussianNB._joint_log_likelihood(self,X)
sklearn.naive_bayes.GaussianNB._partial_fit(self,X,y,classes=None,_refit=False,sample_weight=None)
sklearn.naive_bayes.GaussianNB._update_mean_variance(n_past,mu,var,X,sample_weight=None)
sklearn.naive_bayes.GaussianNB.fit(self,X,y,sample_weight=None)
sklearn.naive_bayes.GaussianNB.partial_fit(self,X,y,classes=None,sample_weight=None)
sklearn.naive_bayes.MultinomialNB(self,alpha=1.0,fit_prior=True,class_prior=None)
sklearn.naive_bayes.MultinomialNB.__init__(self,alpha=1.0,fit_prior=True,class_prior=None)
sklearn.naive_bayes.MultinomialNB._count(self,X,Y)
sklearn.naive_bayes.MultinomialNB._joint_log_likelihood(self,X)
sklearn.naive_bayes.MultinomialNB._update_feature_log_prob(self,alpha)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/discriminant_analysis.py----------------------------------------
A:sklearn.discriminant_analysis.sc->StandardScaler()
A:sklearn.discriminant_analysis.X->check_array(X)
A:sklearn.discriminant_analysis.s->shrunk_covariance(empirical_covariance(X), shrinkage)
A:sklearn.discriminant_analysis.classes->numpy.unique(y)
A:sklearn.discriminant_analysis.self.means_->numpy.asarray(means)
A:sklearn.discriminant_analysis.self.covariance_->_class_cov(X, y, self.priors_)
A:sklearn.discriminant_analysis.St->_cov(X, shrinkage)
A:sklearn.discriminant_analysis.(evals, evecs)->scipy.linalg.eigh(Sb, Sw)
A:sklearn.discriminant_analysis.self.coef_->numpy.array(self.coef_[1, :] - self.coef_[0, :], ndmin=2)
A:sklearn.discriminant_analysis.n_classes->len(self.classes_)
A:sklearn.discriminant_analysis.self.xbar_->numpy.dot(self.priors_, self.means_)
A:sklearn.discriminant_analysis.Xc->numpy.concatenate(Xc, axis=0)
A:sklearn.discriminant_analysis.std->numpy.concatenate(Xc, axis=0).std(axis=0)
A:sklearn.discriminant_analysis.(U, S, V)->scipy.linalg.svd(X, full_matrices=False)
A:sklearn.discriminant_analysis.rank->numpy.sum(S > self.tol)
A:sklearn.discriminant_analysis.(_, S, V)->scipy.linalg.svd(X, full_matrices=0)
A:sklearn.discriminant_analysis.self.scalings_->numpy.dot(scalings, V.T[:, :rank])
A:sklearn.discriminant_analysis.coef->numpy.dot(self.means_ - self.xbar_, self.scalings_)
A:sklearn.discriminant_analysis.(X, y)->check_X_y(X, y)
A:sklearn.discriminant_analysis.self.classes_->unique_labels(y)
A:sklearn.discriminant_analysis.(_, y_t)->numpy.unique(y, return_inverse=True)
A:sklearn.discriminant_analysis.self.priors_->numpy.asarray(self.priors)
A:sklearn.discriminant_analysis.self._max_components->min(len(self.classes_) - 1, self.n_components)
A:sklearn.discriminant_analysis.self.intercept_->numpy.array(self.intercept_[1] - self.intercept_[0], ndmin=1)
A:sklearn.discriminant_analysis.X_new->numpy.dot(X, self.scalings_)
A:sklearn.discriminant_analysis.prob->self.decision_function(X)
A:sklearn.discriminant_analysis.(self.classes_, y)->numpy.unique(y, return_inverse=True)
A:sklearn.discriminant_analysis.meang->Xg.mean(0)
A:sklearn.discriminant_analysis.(U, S, Vt)->numpy.linalg.svd(Xgc, full_matrices=False)
A:sklearn.discriminant_analysis.X2->numpy.dot(Xm, R * S ** (-0.5))
A:sklearn.discriminant_analysis.u->numpy.asarray([np.sum(np.log(s)) for s in self.scalings_])
A:sklearn.discriminant_analysis.dec_func->self._decision_function(X)
A:sklearn.discriminant_analysis.d->self._decision_function(X)
A:sklearn.discriminant_analysis.y_pred->self.classes_.take(d.argmax(1))
A:sklearn.discriminant_analysis.values->self._decision_function(X)
A:sklearn.discriminant_analysis.likelihood->numpy.exp(values - values.max(axis=1)[:, np.newaxis])
A:sklearn.discriminant_analysis.probas_->self.predict_proba(X)
sklearn.discriminant_analysis.LinearDiscriminantAnalysis(self,solver='svd',shrinkage=None,priors=None,n_components=None,store_covariance=False,tol=0.0001)
sklearn.discriminant_analysis.LinearDiscriminantAnalysis.__init__(self,solver='svd',shrinkage=None,priors=None,n_components=None,store_covariance=False,tol=0.0001)
sklearn.discriminant_analysis.LinearDiscriminantAnalysis._solve_eigen(self,X,y,shrinkage)
sklearn.discriminant_analysis.LinearDiscriminantAnalysis._solve_lsqr(self,X,y,shrinkage)
sklearn.discriminant_analysis.LinearDiscriminantAnalysis._solve_svd(self,X,y)
sklearn.discriminant_analysis.LinearDiscriminantAnalysis.fit(self,X,y)
sklearn.discriminant_analysis.LinearDiscriminantAnalysis.predict_log_proba(self,X)
sklearn.discriminant_analysis.LinearDiscriminantAnalysis.predict_proba(self,X)
sklearn.discriminant_analysis.LinearDiscriminantAnalysis.transform(self,X)
sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis(self,priors=None,reg_param=0.0,store_covariance=False,tol=0.0001,store_covariances=None)
sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.__init__(self,priors=None,reg_param=0.0,store_covariance=False,tol=0.0001,store_covariances=None)
sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis._decision_function(self,X)
sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.covariances_(self)
sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.decision_function(self,X)
sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.fit(self,X,y)
sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.predict(self,X)
sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.predict_log_proba(self,X)
sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.predict_proba(self,X)
sklearn.discriminant_analysis._class_cov(X,y,priors=None,shrinkage=None)
sklearn.discriminant_analysis._class_means(X,y)
sklearn.discriminant_analysis._cov(X,shrinkage=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/__init__.py----------------------------------------
A:sklearn.__init__.logger->logging.getLogger(__name__)
A:sklearn.__init__._ASSUME_FINITE->bool(os.environ.get('SKLEARN_ASSUME_FINITE', False))
A:sklearn.__init__.old_config->get_config().copy()
A:sklearn.__init__._random_seed->int(_random_seed)
sklearn.__init__.config_context(**new_config)
sklearn.__init__.get_config()
sklearn.__init__.set_config(assume_finite=None)
sklearn.__init__.setup_module(module)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/exceptions.py----------------------------------------
sklearn.exceptions.ChangedBehaviorWarning(UserWarning)
sklearn.exceptions.ConvergenceWarning(UserWarning)
sklearn.exceptions.DataConversionWarning(UserWarning)
sklearn.exceptions.DataDimensionalityWarning(UserWarning)
sklearn.exceptions.EfficiencyWarning(UserWarning)
sklearn.exceptions.FitFailedWarning(RuntimeWarning)
sklearn.exceptions.NonBLASDotWarning(EfficiencyWarning)
sklearn.exceptions.NotFittedError(ValueError,AttributeError)
sklearn.exceptions.SkipTestWarning(UserWarning)
sklearn.exceptions.UndefinedMetricWarning(UserWarning)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/kernel_ridge.py----------------------------------------
A:sklearn.kernel_ridge.(X, y)->check_X_y(X, y, accept_sparse=('csr', 'csc'), multi_output=True, y_numeric=True)
A:sklearn.kernel_ridge.sample_weight->check_array(sample_weight, ensure_2d=False)
A:sklearn.kernel_ridge.K->self._get_kernel(X, self.X_fit_)
A:sklearn.kernel_ridge.alpha->numpy.atleast_1d(self.alpha)
A:sklearn.kernel_ridge.y->y.reshape(-1, 1).reshape(-1, 1)
A:sklearn.kernel_ridge.self.dual_coef_->self.dual_coef_.ravel()
sklearn.kernel_ridge.KernelRidge(self,alpha=1,kernel='linear',gamma=None,degree=3,coef0=1,kernel_params=None)
sklearn.kernel_ridge.KernelRidge.__init__(self,alpha=1,kernel='linear',gamma=None,degree=3,coef0=1,kernel_params=None)
sklearn.kernel_ridge.KernelRidge._get_kernel(self,X,Y=None)
sklearn.kernel_ridge.KernelRidge._pairwise(self)
sklearn.kernel_ridge.KernelRidge.fit(self,X,y=None,sample_weight=None)
sklearn.kernel_ridge.KernelRidge.predict(self,X)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/isotonic.py----------------------------------------
A:sklearn.isotonic.(rho, _)->spearmanr(x, y)
A:sklearn.isotonic.rho_0->math.tanh(F - 1.96 * F_se)
A:sklearn.isotonic.rho_1->math.tanh(F + 1.96 * F_se)
A:sklearn.isotonic.y->as_float_array(y)
A:sklearn.isotonic.sample_weight->numpy.ones(len(y))
A:sklearn.isotonic.self.f_->scipy.interpolate.interp1d(X, y, kind='linear', bounds_error=bounds_error)
A:sklearn.isotonic.self.increasing_->check_increasing(X, y)
A:sklearn.isotonic.order->numpy.lexsort((y, X))
A:sklearn.isotonic.(unique_X, unique_y, unique_sample_weight)->_make_unique(X, y, sample_weight)
A:sklearn.isotonic.self._y_y->isotonic_regression(unique_y, unique_sample_weight, self.y_min, self.y_max, increasing=self.increasing_)
A:sklearn.isotonic.keep_data->numpy.ones((len(y),), dtype=bool)
A:sklearn.isotonic.keep_data[1:-1]->numpy.logical_or(np.not_equal(y[1:-1], y[:-2]), np.not_equal(y[1:-1], y[2:]))
A:sklearn.isotonic.(X, y)->self._build_y(X, y, sample_weight)
A:sklearn.isotonic.T->numpy.clip(T, self.X_min_, self.X_max_)
A:sklearn.isotonic.state->super(IsotonicRegression, self).__getstate__()
sklearn.isotonic.IsotonicRegression(self,y_min=None,y_max=None,increasing=True,out_of_bounds='nan')
sklearn.isotonic.IsotonicRegression.X_(self)
sklearn.isotonic.IsotonicRegression.X_(self)
sklearn.isotonic.IsotonicRegression.X_(self,value)
sklearn.isotonic.IsotonicRegression.__getstate__(self)
sklearn.isotonic.IsotonicRegression.__init__(self,y_min=None,y_max=None,increasing=True,out_of_bounds='nan')
sklearn.isotonic.IsotonicRegression.__setstate__(self,state)
sklearn.isotonic.IsotonicRegression._build_f(self,X,y)
sklearn.isotonic.IsotonicRegression._build_y(self,X,y,sample_weight,trim_duplicates=True)
sklearn.isotonic.IsotonicRegression._check_fit_data(self,X,y,sample_weight=None)
sklearn.isotonic.IsotonicRegression.fit(self,X,y,sample_weight=None)
sklearn.isotonic.IsotonicRegression.predict(self,T)
sklearn.isotonic.IsotonicRegression.transform(self,T)
sklearn.isotonic.IsotonicRegression.y_(self)
sklearn.isotonic.IsotonicRegression.y_(self)
sklearn.isotonic.IsotonicRegression.y_(self,value)
sklearn.isotonic.check_increasing(x,y)
sklearn.isotonic.isotonic_regression(y,sample_weight=None,y_min=None,y_max=None,increasing=True)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/grid_search.py----------------------------------------
A:sklearn.grid_search.items->sorted(self.param_distributions.items())
A:sklearn.grid_search.(keys, values)->zip(*items)
A:sklearn.grid_search.params->dict()
A:sklearn.grid_search.product->partial(reduce, operator.mul)
A:sklearn.grid_search.(keys, values_lists)->zip(*sorted(sub_grid.items())[::-1])
A:sklearn.grid_search.total->numpy.product(sizes)
A:sklearn.grid_search.(ind, offset)->divmod(ind, n)
A:sklearn.grid_search.all_lists->numpy.all([not hasattr(v, 'rvs') for v in self.param_distributions.values()])
A:sklearn.grid_search.rnd->check_random_state(self.random_state)
A:sklearn.grid_search.param_grid->ParameterGrid(self.param_distributions)
A:sklearn.grid_search.grid_size->len(param_grid)
A:sklearn.grid_search.params[k]->v.rvs()
A:sklearn.grid_search.(score, n_samples_test, _)->_fit_and_score(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, error_score)
A:sklearn.grid_search.self.scorer_->check_scoring(self.estimator, scoring=self.scoring)
A:sklearn.grid_search.n_samples->_num_samples(X)
A:sklearn.grid_search.(X, y)->indexable(X, y)
A:sklearn.grid_search.cv->check_cv(cv, X, y, classifier=is_classifier(estimator))
A:sklearn.grid_search.n_candidates->len(parameter_iterable)
A:sklearn.grid_search.base_estimator->clone(self.estimator)
A:sklearn.grid_search.out->Parallel(n_jobs=self.n_jobs, verbose=self.verbose, pre_dispatch=pre_dispatch)((delayed(_fit_and_score)(clone(base_estimator), X, y, self.scorer_, train, test, self.verbose, parameters, self.fit_params, return_parameters=True, error_score=self.error_score) for parameters in parameter_iterable for (train, test) in cv))
A:sklearn.grid_search.n_fits->len(out)
A:sklearn.grid_search.n_folds->len(cv)
A:sklearn.grid_search.scores->list()
A:sklearn.grid_search.grid_scores->list()
A:sklearn.grid_search.best_estimator->clone(base_estimator).set_params(**best.parameters)
A:sklearn.grid_search.sampled_params->ParameterSampler(self.param_distributions, self.n_iter, random_state=self.random_state)
sklearn.grid_search.BaseSearchCV(self,estimator,scoring=None,fit_params=None,n_jobs=1,iid=True,refit=True,cv=None,verbose=0,pre_dispatch='2*n_jobs',error_score='raise')
sklearn.grid_search.BaseSearchCV.__init__(self,estimator,scoring=None,fit_params=None,n_jobs=1,iid=True,refit=True,cv=None,verbose=0,pre_dispatch='2*n_jobs',error_score='raise')
sklearn.grid_search.BaseSearchCV._estimator_type(self)
sklearn.grid_search.BaseSearchCV._fit(self,X,y,parameter_iterable)
sklearn.grid_search.BaseSearchCV.classes_(self)
sklearn.grid_search.BaseSearchCV.decision_function(self,X)
sklearn.grid_search.BaseSearchCV.inverse_transform(self,Xt)
sklearn.grid_search.BaseSearchCV.predict(self,X)
sklearn.grid_search.BaseSearchCV.predict_log_proba(self,X)
sklearn.grid_search.BaseSearchCV.predict_proba(self,X)
sklearn.grid_search.BaseSearchCV.score(self,X,y=None)
sklearn.grid_search.BaseSearchCV.transform(self,X)
sklearn.grid_search.GridSearchCV(self,estimator,param_grid,scoring=None,fit_params=None,n_jobs=1,iid=True,refit=True,cv=None,verbose=0,pre_dispatch='2*n_jobs',error_score='raise')
sklearn.grid_search.GridSearchCV.__init__(self,estimator,param_grid,scoring=None,fit_params=None,n_jobs=1,iid=True,refit=True,cv=None,verbose=0,pre_dispatch='2*n_jobs',error_score='raise')
sklearn.grid_search.GridSearchCV.fit(self,X,y=None)
sklearn.grid_search.ParameterGrid(self,param_grid)
sklearn.grid_search.ParameterGrid.__getitem__(self,ind)
sklearn.grid_search.ParameterGrid.__init__(self,param_grid)
sklearn.grid_search.ParameterGrid.__iter__(self)
sklearn.grid_search.ParameterGrid.__len__(self)
sklearn.grid_search.ParameterSampler(self,param_distributions,n_iter,random_state=None)
sklearn.grid_search.ParameterSampler.__init__(self,param_distributions,n_iter,random_state=None)
sklearn.grid_search.ParameterSampler.__iter__(self)
sklearn.grid_search.ParameterSampler.__len__(self)
sklearn.grid_search.RandomizedSearchCV(self,estimator,param_distributions,n_iter=10,scoring=None,fit_params=None,n_jobs=1,iid=True,refit=True,cv=None,verbose=0,pre_dispatch='2*n_jobs',random_state=None,error_score='raise')
sklearn.grid_search.RandomizedSearchCV.__init__(self,estimator,param_distributions,n_iter=10,scoring=None,fit_params=None,n_jobs=1,iid=True,refit=True,cv=None,verbose=0,pre_dispatch='2*n_jobs',random_state=None,error_score='raise')
sklearn.grid_search.RandomizedSearchCV.fit(self,X,y=None)
sklearn.grid_search._CVScoreTuple(namedtuple('_CVScoreTuple',('parameters','mean_validation_score','cv_validation_scores')))
sklearn.grid_search._CVScoreTuple.__repr__(self)
sklearn.grid_search._check_param_grid(param_grid)
sklearn.grid_search.fit_grid_point(X,y,estimator,parameters,train,test,scorer,verbose,error_score='raise',**fit_params)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/multioutput.py----------------------------------------
A:sklearn.multioutput.estimator->clone(estimator)
A:sklearn.multioutput.(X, y)->check_X_y(X, y, multi_output=True, accept_sparse=True)
A:sklearn.multioutput.self.estimators_->Parallel(n_jobs=self.n_jobs)((delayed(_fit_estimator)(self.estimator, X, y[:, i], sample_weight) for i in range(y.shape[1])))
A:sklearn.multioutput.X->check_array(X, accept_sparse=True)
A:sklearn.multioutput.y->Parallel(n_jobs=self.n_jobs)((delayed(parallel_helper)(e, 'predict', X) for e in self.estimators_))
A:sklearn.multioutput.n_outputs_->len(self.estimators_)
A:sklearn.multioutput.y_pred->self.predict(X)
A:sklearn.multioutput.(X, Y)->check_X_y(X, Y, multi_output=True, accept_sparse=True)
A:sklearn.multioutput.random_state->check_random_state(self.random_state)
A:sklearn.multioutput.self.order_->check_random_state(self.random_state).permutation(Y.shape[1])
A:sklearn.multioutput.X_aug->numpy.hstack((X, previous_predictions))
A:sklearn.multioutput.Y_pred_chain->numpy.zeros((X.shape[0], len(self.estimators_)))
A:sklearn.multioutput.cv_result->cross_val_predict(self.base_estimator, X_aug[:, :col_idx], y=y, cv=self.cv)
A:sklearn.multioutput.X_aug[:, col_idx]->numpy.expand_dims(cv_result, 1)
A:sklearn.multioutput.Y_pred_chain[:, chain_idx]->clone(estimator).predict(X_aug)
A:sklearn.multioutput.inv_order->numpy.empty_like(self.order_)
A:sklearn.multioutput.inv_order[self.order_]->numpy.arange(len(self.order_))
A:sklearn.multioutput.Y_prob_chain->numpy.zeros((X.shape[0], len(self.estimators_)))
A:sklearn.multioutput.Y_decision_chain->numpy.zeros((X.shape[0], len(self.estimators_)))
A:sklearn.multioutput.Y_decision_chain[:, chain_idx]->clone(estimator).decision_function(X_aug)
sklearn.multioutput.ClassifierChain(self,base_estimator,order=None,cv=None,random_state=None)
sklearn.multioutput.ClassifierChain.__init__(self,base_estimator,order=None,cv=None,random_state=None)
sklearn.multioutput.ClassifierChain.decision_function(self,X)
sklearn.multioutput.ClassifierChain.fit(self,X,Y)
sklearn.multioutput.ClassifierChain.predict(self,X)
sklearn.multioutput.ClassifierChain.predict_proba(self,X)
sklearn.multioutput.MultiOutputClassifier(self,estimator,n_jobs=1)
sklearn.multioutput.MultiOutputClassifier.__init__(self,estimator,n_jobs=1)
sklearn.multioutput.MultiOutputClassifier.predict_proba(self,X)
sklearn.multioutput.MultiOutputClassifier.score(self,X,y)
sklearn.multioutput.MultiOutputEstimator(self,estimator,n_jobs=1)
sklearn.multioutput.MultiOutputEstimator.__init__(self,estimator,n_jobs=1)
sklearn.multioutput.MultiOutputEstimator.fit(self,X,y,sample_weight=None)
sklearn.multioutput.MultiOutputEstimator.partial_fit(self,X,y,classes=None,sample_weight=None)
sklearn.multioutput.MultiOutputEstimator.predict(self,X)
sklearn.multioutput.MultiOutputRegressor(self,estimator,n_jobs=1)
sklearn.multioutput.MultiOutputRegressor.__init__(self,estimator,n_jobs=1)
sklearn.multioutput.MultiOutputRegressor.partial_fit(self,X,y,sample_weight=None)
sklearn.multioutput.MultiOutputRegressor.score(self,X,y,sample_weight=None)
sklearn.multioutput._fit_estimator(estimator,X,y,sample_weight=None)
sklearn.multioutput._partial_fit_estimator(estimator,X,y,classes=None,sample_weight=None,first_time=True)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/pipeline.py----------------------------------------
A:sklearn.pipeline.(names, estimators)->zip(*self.steps)
A:sklearn.pipeline.self.steps->list(self.steps)
A:sklearn.pipeline.memory->kwargs.pop('memory', None)
A:sklearn.pipeline.fit_transform_one_cached->kwargs.pop('memory', None).cache(_fit_transform_one)
A:sklearn.pipeline.fit_params_steps->dict(((name, {}) for (name, step) in self.steps if step is not None))
A:sklearn.pipeline.(step, param)->pname.split('__', 1)
A:sklearn.pipeline.cloned_transformer->clone(transformer)
A:sklearn.pipeline.(Xt, fitted_transformer)->fit_transform_one_cached(cloned_transformer, None, Xt, y, **fit_params_steps[name])
A:sklearn.pipeline.(Xt, fit_params)->self._fit(X, y, **fit_params)
A:sklearn.pipeline.Xt->transform.transform(Xt)
A:sklearn.pipeline.namecount->defaultdict(int)
A:sklearn.pipeline.res->transformer.fit(X, y, **fit_params).transform(X)
A:sklearn.pipeline.(names, transformers)->zip(*self.transformer_list)
A:sklearn.pipeline.self.transformer_list->list(self.transformer_list)
A:sklearn.pipeline.transformers->iter(transformers)
A:sklearn.pipeline.result->Parallel(n_jobs=self.n_jobs)((delayed(_fit_transform_one)(trans, weight, X, y, **fit_params) for (name, trans, weight) in self._iter()))
A:sklearn.pipeline.(Xs, transformers)->zip(*result)
A:sklearn.pipeline.Xs->numpy.hstack(Xs)
A:sklearn.pipeline.n_jobs->kwargs.pop('n_jobs', 1)
sklearn.pipeline.FeatureUnion(self,transformer_list,n_jobs=1,transformer_weights=None)
sklearn.pipeline.FeatureUnion.__init__(self,transformer_list,n_jobs=1,transformer_weights=None)
sklearn.pipeline.FeatureUnion._iter(self)
sklearn.pipeline.FeatureUnion._update_transformer_list(self,transformers)
sklearn.pipeline.FeatureUnion._validate_transformers(self)
sklearn.pipeline.FeatureUnion.fit(self,X,y=None)
sklearn.pipeline.FeatureUnion.fit_transform(self,X,y=None,**fit_params)
sklearn.pipeline.FeatureUnion.get_feature_names(self)
sklearn.pipeline.FeatureUnion.get_params(self,deep=True)
sklearn.pipeline.FeatureUnion.set_params(self,**kwargs)
sklearn.pipeline.FeatureUnion.transform(self,X)
sklearn.pipeline.Pipeline(self,steps,memory=None)
sklearn.pipeline.Pipeline.__init__(self,steps,memory=None)
sklearn.pipeline.Pipeline._estimator_type(self)
sklearn.pipeline.Pipeline._final_estimator(self)
sklearn.pipeline.Pipeline._fit(self,X,y=None,**fit_params)
sklearn.pipeline.Pipeline._inverse_transform(self,X)
sklearn.pipeline.Pipeline._pairwise(self)
sklearn.pipeline.Pipeline._transform(self,X)
sklearn.pipeline.Pipeline._validate_steps(self)
sklearn.pipeline.Pipeline.classes_(self)
sklearn.pipeline.Pipeline.decision_function(self,X)
sklearn.pipeline.Pipeline.fit(self,X,y=None,**fit_params)
sklearn.pipeline.Pipeline.fit_predict(self,X,y=None,**fit_params)
sklearn.pipeline.Pipeline.fit_transform(self,X,y=None,**fit_params)
sklearn.pipeline.Pipeline.get_params(self,deep=True)
sklearn.pipeline.Pipeline.inverse_transform(self)
sklearn.pipeline.Pipeline.named_steps(self)
sklearn.pipeline.Pipeline.predict(self,X)
sklearn.pipeline.Pipeline.predict_log_proba(self,X)
sklearn.pipeline.Pipeline.predict_proba(self,X)
sklearn.pipeline.Pipeline.score(self,X,y=None,sample_weight=None)
sklearn.pipeline.Pipeline.set_params(self,**kwargs)
sklearn.pipeline.Pipeline.transform(self)
sklearn.pipeline._fit_one_transformer(transformer,X,y)
sklearn.pipeline._fit_transform_one(transformer,weight,X,y,**fit_params)
sklearn.pipeline._name_estimators(estimators)
sklearn.pipeline._transform_one(transformer,weight,X)
sklearn.pipeline.make_pipeline(*steps,**kwargs)
sklearn.pipeline.make_union(*transformers,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/setup.py----------------------------------------
A:sklearn.setup.config->Configuration('sklearn', parent_package, top_path)
A:sklearn.setup.blas_info->get_info('blas_opt', 0)
sklearn.setup.configuration(parent_package='',top_path=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/multiclass.py----------------------------------------
A:sklearn.multiclass.unique_y->numpy.unique(y)
A:sklearn.multiclass.estimator->clone(estimator)
A:sklearn.multiclass.score->numpy.ravel(estimator.decision_function(X))
A:sklearn.multiclass.self.label_binarizer_->LabelBinarizer(sparse_output=True)
A:sklearn.multiclass.Y->numpy.array([self.code_book_[classes_index[y[i]]] for i in range(X.shape[0])], dtype=np.int)
A:sklearn.multiclass.self.estimators_->Parallel(n_jobs=self.n_jobs)((delayed(_fit_binary)(self.estimator, X, Y[:, i]) for i in range(Y.shape[1])))
A:sklearn.multiclass.n_samples->_num_samples(X)
A:sklearn.multiclass.maxima->numpy.empty(n_samples, dtype=float)
A:sklearn.multiclass.argmaxima->numpy.zeros(n_samples, dtype=int)
A:sklearn.multiclass.pred->euclidean_distances(Y, self.code_book_).argmin(axis=1)
A:sklearn.multiclass.indices->array.array('i')
A:sklearn.multiclass.indptr->array.array('i', [0])
A:sklearn.multiclass.data->numpy.ones(len(indices), dtype=int)
A:sklearn.multiclass.indicator->scipy.sparse.csc_matrix((data, indices, indptr), shape=(n_samples, len(self.estimators_)))
A:sklearn.multiclass.cond->numpy.logical_or(y == i, y == j)
A:sklearn.multiclass.y_binary->numpy.zeros_like(y)
A:sklearn.multiclass.(X, y)->check_X_y(X, y)
A:sklearn.multiclass.self.classes_->numpy.unique(y)
A:sklearn.multiclass.estimators_indices->list(zip(*Parallel(n_jobs=self.n_jobs)((delayed(_fit_ovo_binary)(self.estimator, X, y, self.classes_[i], self.classes_[j]) for i in range(n_classes) for j in range(i + 1, n_classes)))))
A:sklearn.multiclass.combinations->itertools.combinations(range(self.n_classes_), 2)
A:sklearn.multiclass.random_state->check_random_state(self.random_state)
A:sklearn.multiclass.code_size_->int(n_classes * self.code_size)
A:sklearn.multiclass.self.code_book_->check_random_state(self.random_state).random_sample((n_classes, code_size_))
A:sklearn.multiclass.classes_index->dict(((c, i) for (i, c) in enumerate(self.classes_)))
A:sklearn.multiclass.X->check_array(X)
sklearn.multiclass.OneVsOneClassifier(self,estimator,n_jobs=1)
sklearn.multiclass.OneVsOneClassifier.__init__(self,estimator,n_jobs=1)
sklearn.multiclass.OneVsOneClassifier._pairwise(self)
sklearn.multiclass.OneVsOneClassifier.decision_function(self,X)
sklearn.multiclass.OneVsOneClassifier.fit(self,X,y)
sklearn.multiclass.OneVsOneClassifier.n_classes_(self)
sklearn.multiclass.OneVsOneClassifier.partial_fit(self,X,y,classes=None)
sklearn.multiclass.OneVsOneClassifier.predict(self,X)
sklearn.multiclass.OneVsRestClassifier(self,estimator,n_jobs=1)
sklearn.multiclass.OneVsRestClassifier.__init__(self,estimator,n_jobs=1)
sklearn.multiclass.OneVsRestClassifier._first_estimator(self)
sklearn.multiclass.OneVsRestClassifier._pairwise(self)
sklearn.multiclass.OneVsRestClassifier.coef_(self)
sklearn.multiclass.OneVsRestClassifier.decision_function(self,X)
sklearn.multiclass.OneVsRestClassifier.fit(self,X,y)
sklearn.multiclass.OneVsRestClassifier.intercept_(self)
sklearn.multiclass.OneVsRestClassifier.multilabel_(self)
sklearn.multiclass.OneVsRestClassifier.n_classes_(self)
sklearn.multiclass.OneVsRestClassifier.partial_fit(self,X,y,classes=None)
sklearn.multiclass.OneVsRestClassifier.predict(self,X)
sklearn.multiclass.OneVsRestClassifier.predict_proba(self,X)
sklearn.multiclass.OutputCodeClassifier(self,estimator,code_size=1.5,random_state=None,n_jobs=1)
sklearn.multiclass.OutputCodeClassifier.__init__(self,estimator,code_size=1.5,random_state=None,n_jobs=1)
sklearn.multiclass.OutputCodeClassifier.fit(self,X,y)
sklearn.multiclass.OutputCodeClassifier.predict(self,X)
sklearn.multiclass._ConstantPredictor(BaseEstimator)
sklearn.multiclass._ConstantPredictor.decision_function(self,X)
sklearn.multiclass._ConstantPredictor.fit(self,X,y)
sklearn.multiclass._ConstantPredictor.predict(self,X)
sklearn.multiclass._ConstantPredictor.predict_proba(self,X)
sklearn.multiclass._check_estimator(estimator)
sklearn.multiclass._fit_binary(estimator,X,y,classes=None)
sklearn.multiclass._fit_ovo_binary(estimator,X,y,i,j)
sklearn.multiclass._partial_fit_binary(estimator,X,y)
sklearn.multiclass._partial_fit_ovo_binary(estimator,X,y,i,j)
sklearn.multiclass._predict_binary(estimator,X)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/dummy.py----------------------------------------
A:sklearn.dummy.X->check_array(X, accept_sparse=['csr', 'csc', 'coo'], force_all_finite=False)
A:sklearn.dummy.y->numpy.ravel(y)
A:sklearn.dummy.self.sparse_output_->scipy.sparse.issparse(y)
A:sklearn.dummy.constant->numpy.reshape(np.atleast_1d(self.constant), (-1, 1))
A:sklearn.dummy.(self.classes_, self.n_classes_, self.class_prior_)->class_distribution(y, sample_weight)
A:sklearn.dummy.n_samples->int(X.shape[0])
A:sklearn.dummy.rs->check_random_state(self.random_state)
A:sklearn.dummy.proba->self.predict_proba(X)
A:sklearn.dummy.ind->numpy.where(classes_[k] == constant[k])
A:sklearn.dummy.out->numpy.zeros((n_samples, n_classes_[k]), dtype=np.float64)
A:sklearn.dummy.self.constant_->numpy.reshape(self.constant_, (1, -1))
A:sklearn.dummy.self.constant->check_array(self.constant, accept_sparse=['csr', 'csc', 'coo'], ensure_2d=False, ensure_min_samples=0)
sklearn.dummy.DummyClassifier(self,strategy='stratified',random_state=None,constant=None)
sklearn.dummy.DummyClassifier.__init__(self,strategy='stratified',random_state=None,constant=None)
sklearn.dummy.DummyClassifier.fit(self,X,y,sample_weight=None)
sklearn.dummy.DummyClassifier.predict(self,X)
sklearn.dummy.DummyClassifier.predict_log_proba(self,X)
sklearn.dummy.DummyClassifier.predict_proba(self,X)
sklearn.dummy.DummyRegressor(self,strategy='mean',constant=None,quantile=None)
sklearn.dummy.DummyRegressor.__init__(self,strategy='mean',constant=None,quantile=None)
sklearn.dummy.DummyRegressor.fit(self,X,y,sample_weight=None)
sklearn.dummy.DummyRegressor.predict(self,X)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/learning_curve.py----------------------------------------
A:sklearn.learning_curve.(X, y)->indexable(X, y)
A:sklearn.learning_curve.cv->check_cv(cv, X, y, classifier=is_classifier(estimator))
A:sklearn.learning_curve.scorer->check_scoring(estimator, scoring=scoring)
A:sklearn.learning_curve.n_max_training_samples->len(cv[0][0])
A:sklearn.learning_curve.train_sizes_abs->numpy.unique(train_sizes_abs)
A:sklearn.learning_curve.parallel->Parallel(n_jobs=n_jobs, pre_dispatch=pre_dispatch, verbose=verbose)
A:sklearn.learning_curve.out->out.reshape(n_cv_folds, n_params, 2).transpose((2, 1, 0)).reshape(n_cv_folds, n_params, 2).transpose((2, 1, 0))
A:sklearn.learning_curve.n_min_required_samples->numpy.min(train_sizes_abs)
A:sklearn.learning_curve.n_max_required_samples->numpy.max(train_sizes_abs)
A:sklearn.learning_curve.partitions->zip(train_sizes, np.split(train, train_sizes)[:-1])
A:sklearn.learning_curve.(X_train, y_train)->_safe_split(estimator, X, y, train_subset)
A:sklearn.learning_curve.(X_partial_train, y_partial_train)->_safe_split(estimator, X, y, partial_train)
A:sklearn.learning_curve.(X_test, y_test)->_safe_split(estimator, X, y, test, train_subset)
A:sklearn.learning_curve.n_params->len(param_range)
sklearn.learning_curve._incremental_fit_estimator(estimator,X,y,classes,train,test,train_sizes,scorer,verbose)
sklearn.learning_curve._translate_train_sizes(train_sizes,n_max_training_samples)
sklearn.learning_curve.learning_curve(estimator,X,y,train_sizes=np.linspace(0.1,1.0,5),cv=None,scoring=None,exploit_incremental_learning=False,n_jobs=1,pre_dispatch='all',verbose=0,error_score='raise')
sklearn.learning_curve.validation_curve(estimator,X,y,param_name,param_range,cv=None,scoring=None,n_jobs=1,pre_dispatch='all',verbose=0)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/calibration.py----------------------------------------
A:sklearn.calibration.(X, y)->indexable(X, y)
A:sklearn.calibration.le->LabelBinarizer().fit(y)
A:sklearn.calibration.base_estimator->LinearSVC(random_state=0)
A:sklearn.calibration.calibrated_classifier->_CalibratedClassifier(this_estimator, method=self.method, classes=self.classes_)
A:sklearn.calibration.cv->check_cv(self.cv, y, classifier=True)
A:sklearn.calibration.sample_weight->check_array(sample_weight, ensure_2d=False)
A:sklearn.calibration.this_estimator->clone(base_estimator)
A:sklearn.calibration.X->column_or_1d(X)
A:sklearn.calibration.mean_proba->numpy.zeros((X.shape[0], len(self.classes_)))
A:sklearn.calibration.proba->numpy.zeros((X.shape[0], n_classes))
A:sklearn.calibration.n_classes->len(self.classes_)
A:sklearn.calibration.df->column_or_1d(df)
A:sklearn.calibration.idx_pos_class->self.label_encoder_.transform(self.base_estimator.classes_)
A:sklearn.calibration.self.label_encoder_->LabelEncoder()
A:sklearn.calibration.Y->label_binarize(y, self.classes_)
A:sklearn.calibration.(df, idx_pos_class)->self._preproc(X)
A:sklearn.calibration.calibrator->_SigmoidCalibration()
A:sklearn.calibration.proba[:, k]->_SigmoidCalibration().predict(this_df)
A:sklearn.calibration.y->column_or_1d(y)
A:sklearn.calibration.prior0->float(np.sum(y <= 0))
A:sklearn.calibration.T->column_or_1d(T)
A:sklearn.calibration.E->numpy.exp(AB[0] * F + AB[1])
A:sklearn.calibration.dA->numpy.dot(TEP_minus_T1P, F)
A:sklearn.calibration.dB->numpy.sum(TEP_minus_T1P)
A:sklearn.calibration.AB0->numpy.array([0.0, log((prior0 + 1.0) / (prior1 + 1.0))])
A:sklearn.calibration.AB_->fmin_bfgs(objective, AB0, fprime=grad, disp=False)
A:sklearn.calibration.(self.a_, self.b_)->_sigmoid_calibration(X, y, sample_weight)
A:sklearn.calibration.y_true->_check_binary_probabilistic_predictions(y_true, y_prob)
A:sklearn.calibration.y_prob->column_or_1d(y_prob)
A:sklearn.calibration.bins->numpy.linspace(0.0, 1.0 + 1e-08, n_bins + 1)
A:sklearn.calibration.bin_sums->numpy.bincount(binids, weights=y_prob, minlength=len(bins))
A:sklearn.calibration.bin_true->numpy.bincount(binids, weights=y_true, minlength=len(bins))
A:sklearn.calibration.bin_total->numpy.bincount(binids, minlength=len(bins))
sklearn.calibration.CalibratedClassifierCV(self,base_estimator=None,method='sigmoid',cv=3)
sklearn.calibration.CalibratedClassifierCV.__init__(self,base_estimator=None,method='sigmoid',cv=3)
sklearn.calibration.CalibratedClassifierCV.fit(self,X,y,sample_weight=None)
sklearn.calibration.CalibratedClassifierCV.predict(self,X)
sklearn.calibration.CalibratedClassifierCV.predict_proba(self,X)
sklearn.calibration._CalibratedClassifier(self,base_estimator,method='sigmoid',classes=None)
sklearn.calibration._CalibratedClassifier.__init__(self,base_estimator,method='sigmoid',classes=None)
sklearn.calibration._CalibratedClassifier._preproc(self,X)
sklearn.calibration._CalibratedClassifier.fit(self,X,y,sample_weight=None)
sklearn.calibration._CalibratedClassifier.predict_proba(self,X)
sklearn.calibration._SigmoidCalibration(BaseEstimator,RegressorMixin)
sklearn.calibration._SigmoidCalibration.fit(self,X,y,sample_weight=None)
sklearn.calibration._SigmoidCalibration.predict(self,T)
sklearn.calibration._sigmoid_calibration(df,y,sample_weight=None)
sklearn.calibration.calibration_curve(y_true,y_prob,normalize=False,n_bins=5)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/base.py----------------------------------------
A:sklearn.base.estimator_type->type(estimator)
A:sklearn.base.new_object_params->estimator.get_params(deep=False)
A:sklearn.base.new_object_params[name]->clone(param, safe=False)
A:sklearn.base.new_object->klass(**new_object_params)
A:sklearn.base.params_set->klass(**new_object_params).get_params(deep=False)
A:sklearn.base.equality_test->numpy.all(param1 == param2)
A:sklearn.base.options->numpy.get_printoptions()
A:sklearn.base.params_list->list()
A:sklearn.base.this_line_length->len(line_sep)
A:sklearn.base.lines->'\n'.join((l.rstrip(' ') for l in lines.split('\n')))
A:sklearn.base.init->getattr(cls.__init__, 'deprecated_original', cls.__init__)
A:sklearn.base.init_signature->signature(init)
A:sklearn.base.out->dict()
A:sklearn.base.value->getattr(self, key, None)
A:sklearn.base.deep_items->getattr(self, key, None).get_params().items()
A:sklearn.base.valid_params->self.get_params(deep=True)
A:sklearn.base.nested_params->defaultdict(dict)
A:sklearn.base.(key, delim, sub_key)->key.partition('__')
A:sklearn.base.state->self.__dict__.copy()
A:sklearn.base.pickle_version->self.__dict__.copy().pop('_sklearn_version', 'pre-0.18')
A:sklearn.base.indices->self.get_indices(i)
A:sklearn.base.data->check_array(data, accept_sparse='csr')
A:sklearn.base.(row_ind, col_ind)->self.get_indices(i)
sklearn.base.BaseEstimator(object)
sklearn.base.BaseEstimator.__getstate__(self)
sklearn.base.BaseEstimator.__repr__(self)
sklearn.base.BaseEstimator.__setstate__(self,state)
sklearn.base.BaseEstimator._get_param_names(cls)
sklearn.base.BaseEstimator.get_params(self,deep=True)
sklearn.base.BaseEstimator.set_params(self,**params)
sklearn.base.BiclusterMixin(object)
sklearn.base.BiclusterMixin.biclusters_(self)
sklearn.base.BiclusterMixin.get_indices(self,i)
sklearn.base.BiclusterMixin.get_shape(self,i)
sklearn.base.BiclusterMixin.get_submatrix(self,i,data)
sklearn.base.ClassifierMixin(object)
sklearn.base.ClassifierMixin.score(self,X,y,sample_weight=None)
sklearn.base.ClusterMixin(object)
sklearn.base.ClusterMixin.fit_predict(self,X,y=None)
sklearn.base.DensityMixin(object)
sklearn.base.DensityMixin.score(self,X,y=None)
sklearn.base.MetaEstimatorMixin(object)
sklearn.base.RegressorMixin(object)
sklearn.base.RegressorMixin.score(self,X,y,sample_weight=None)
sklearn.base.TransformerMixin(object)
sklearn.base.TransformerMixin.fit_transform(self,X,y=None,**fit_params)
sklearn.base._first_and_last_element(arr)
sklearn.base._pprint(params,offset=0,printer=repr)
sklearn.base.clone(estimator,safe=True)
sklearn.base.is_classifier(estimator)
sklearn.base.is_regressor(estimator)
sklearn.clone(estimator,safe=True)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/random_projection.py----------------------------------------
A:sklearn.random_projection.eps->numpy.asarray(eps)
A:sklearn.random_projection.n_samples->numpy.asarray(n_samples)
A:sklearn.random_projection.rng->check_random_state(random_state)
A:sklearn.random_projection.components->scipy.sparse.csr_matrix((data, indices, indptr), shape=(n_components, n_features))
A:sklearn.random_projection.density->_check_density(density, n_features)
A:sklearn.random_projection.n_nonzero_i->check_random_state(random_state).binomial(n_features, density)
A:sklearn.random_projection.indices_i->sample_without_replacement(n_features, n_nonzero_i, random_state=rng)
A:sklearn.random_projection.indices->numpy.concatenate(indices)
A:sklearn.random_projection.X->check_array(X, accept_sparse=['csr', 'csc'])
A:sklearn.random_projection.self.n_components_->johnson_lindenstrauss_min_dim(n_samples=n_samples, eps=self.eps)
A:sklearn.random_projection.self.components_->self._make_random_matrix(self.n_components_, n_features)
A:sklearn.random_projection.X_new->safe_sparse_dot(X, self.components_.T, dense_output=self.dense_output)
A:sklearn.random_projection.random_state->check_random_state(self.random_state)
A:sklearn.random_projection.self.density_->_check_density(self.density, n_features)
sklearn.random_projection.BaseRandomProjection(self,n_components='auto',eps=0.1,dense_output=False,random_state=None)
sklearn.random_projection.BaseRandomProjection.__init__(self,n_components='auto',eps=0.1,dense_output=False,random_state=None)
sklearn.random_projection.BaseRandomProjection._make_random_matrix(self,n_components,n_features)
sklearn.random_projection.BaseRandomProjection.fit(self,X,y=None)
sklearn.random_projection.BaseRandomProjection.transform(self,X)
sklearn.random_projection.GaussianRandomProjection(self,n_components='auto',eps=0.1,random_state=None)
sklearn.random_projection.GaussianRandomProjection.__init__(self,n_components='auto',eps=0.1,random_state=None)
sklearn.random_projection.GaussianRandomProjection._make_random_matrix(self,n_components,n_features)
sklearn.random_projection.SparseRandomProjection(self,n_components='auto',density='auto',eps=0.1,dense_output=False,random_state=None)
sklearn.random_projection.SparseRandomProjection.__init__(self,n_components='auto',density='auto',eps=0.1,dense_output=False,random_state=None)
sklearn.random_projection.SparseRandomProjection._make_random_matrix(self,n_components,n_features)
sklearn.random_projection._check_density(density,n_features)
sklearn.random_projection._check_input_size(n_components,n_features)
sklearn.random_projection.gaussian_random_matrix(n_components,n_features,random_state=None)
sklearn.random_projection.johnson_lindenstrauss_min_dim(n_samples,eps=0.1)
sklearn.random_projection.sparse_random_matrix(n_components,n_features,density='auto',random_state=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/cross_validation.py----------------------------------------
A:sklearn.cross_validation.self.n->int(n)
A:sklearn.cross_validation.ind->numpy.arange(len(labels))
A:sklearn.cross_validation.train_index->numpy.logical_not(test_index)
A:sklearn.cross_validation.test_mask->self._empty_mask()
A:sklearn.cross_validation.self.n_foldsn_folds->int(n_folds)
A:sklearn.cross_validation.self.idxs->numpy.arange(n)
A:sklearn.cross_validation.rng->check_random_state(self.random_state)
A:sklearn.cross_validation.(unique_labels, labels)->numpy.unique(labels, return_inverse=True)
A:sklearn.cross_validation.n_labels->len(unique_labels)
A:sklearn.cross_validation.n_samples_per_label->numpy.bincount(labels)
A:sklearn.cross_validation.n_samples_per_fold->numpy.zeros(n_folds)
A:sklearn.cross_validation.label_to_fold->numpy.zeros(len(unique_labels))
A:sklearn.cross_validation.lightest_fold->numpy.argmin(n_samples_per_fold)
A:sklearn.cross_validation.y->numpy.asarray(y)
A:sklearn.cross_validation.(unique_labels, y_inversed)->numpy.unique(y, return_inverse=True)
A:sklearn.cross_validation.label_counts->numpy.bincount(y_inversed)
A:sklearn.cross_validation.min_labels->numpy.min(label_counts)
A:sklearn.cross_validation.test_folds->numpy.zeros(n_samples, dtype=np.int)
A:sklearn.cross_validation.self.labels->numpy.array(labels, copy=True)
A:sklearn.cross_validation.self.unique_labels->numpy.unique(labels)
A:sklearn.cross_validation.self.n_unique_labels->len(self.unique_labels)
A:sklearn.cross_validation.comb->combinations(range(self.n_unique_labels), self.p)
A:sklearn.cross_validation.test_index->self._empty_mask()
A:sklearn.cross_validation.idx->numpy.array(idx)
A:sklearn.cross_validation.(self.n_train, self.n_test)->_validate_shuffle_split(n, test_size, train_size)
A:sklearn.cross_validation.permutation->check_random_state(self.random_state).permutation(cls_count[i])
A:sklearn.cross_validation.n_test->float(test_size)
A:sklearn.cross_validation.n_train->float(train_size)
A:sklearn.cross_validation.floored->numpy.floor(continuous)
A:sklearn.cross_validation.need_to_add->int(n_draws - floored.sum())
A:sklearn.cross_validation.(inds,)->numpy.where(remainder == value)
A:sklearn.cross_validation.add_now->min(len(inds), need_to_add)
A:sklearn.cross_validation.inds->check_random_state(self.random_state).choice(inds, size=add_now, replace=False)
A:sklearn.cross_validation.self.y->numpy.array(y)
A:sklearn.cross_validation.(self.classes, self.y_indices)->numpy.unique(y, return_inverse=True)
A:sklearn.cross_validation.cls_count->numpy.bincount(self.y_indices)
A:sklearn.cross_validation.n_i->_approximate_mode(cls_count, self.n_train, rng)
A:sklearn.cross_validation.t_i->_approximate_mode(class_counts_remaining, self.n_test, rng)
A:sklearn.cross_validation.train->numpy.flatnonzero(np.in1d(self.label_indices, label_train))
A:sklearn.cross_validation.test->numpy.flatnonzero(np.in1d(self.label_indices, label_test))
A:sklearn.cross_validation.self.test_fold->column_or_1d(self.test_fold)
A:sklearn.cross_validation.self.unique_folds->numpy.unique(self.test_fold)
A:sklearn.cross_validation.(classes, label_indices)->numpy.unique(labels, return_inverse=True)
A:sklearn.cross_validation.v->v.tocsr().tocsr()
A:sklearn.cross_validation.(X, y)->indexable(X, y)
A:sklearn.cross_validation.cv->ShuffleSplit(n_samples, test_size=test_size, train_size=train_size, random_state=random_state)
A:sklearn.cross_validation.parallel->Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)
A:sklearn.cross_validation.preds_blocks->parallel((delayed(_fit_and_predict)(clone(estimator), X, y, train, test, verbose, fit_params) for (train, test) in cv))
A:sklearn.cross_validation.locs->numpy.concatenate([loc for (_, loc) in preds_blocks])
A:sklearn.cross_validation.inv_locs->numpy.empty(len(locs), dtype=int)
A:sklearn.cross_validation.inv_locs[locs]->numpy.arange(len(locs))
A:sklearn.cross_validation.preds->estimator.predict(X_test)
A:sklearn.cross_validation.fit_params->dict([(k, _index_param_value(X, v, train)) for (k, v) in fit_params.items()])
A:sklearn.cross_validation.(X_train, y_train)->_safe_split(estimator, X, y, train)
A:sklearn.cross_validation.(X_test, _)->_safe_split(estimator, X, y, test, train)
A:sklearn.cross_validation.hit->numpy.zeros(n, bool)
A:sklearn.cross_validation.scorer->check_scoring(estimator, scoring=scoring)
A:sklearn.cross_validation.scores->parallel((delayed(_fit_and_score)(clone(estimator), X, y, scorer, train, test, verbose, None, fit_params) for (train, test) in cv))
A:sklearn.cross_validation.start_time->time.time()
A:sklearn.cross_validation.(X_test, y_test)->_safe_split(estimator, X, y, test, train)
A:sklearn.cross_validation.test_score->_score(estimator, X_test, y_test, scorer)
A:sklearn.cross_validation.train_score->_score(estimator, X_train, y_train, scorer)
A:sklearn.cross_validation.X_subset->safe_indexing(X, indices)
A:sklearn.cross_validation.y_subset->safe_indexing(y, indices)
A:sklearn.cross_validation.score->_permutation_test_score(clone(estimator), X, y, cv, scorer)
A:sklearn.cross_validation.ind[this_mask]->options.pop('random_state', None).permutation(ind[this_mask])
A:sklearn.cross_validation.is_sparse->scipy.sparse.issparse(X)
A:sklearn.cross_validation.n_samples->_num_samples(arrays[0])
A:sklearn.cross_validation.random_state->options.pop('random_state', None)
A:sklearn.cross_validation.permutation_scores->numpy.array(permutation_scores)
A:sklearn.cross_validation.n_arrays->len(arrays)
A:sklearn.cross_validation.test_size->options.pop('test_size', None)
A:sklearn.cross_validation.train_size->options.pop('train_size', None)
A:sklearn.cross_validation.stratify->options.pop('stratify', None)
A:sklearn.cross_validation.arrays->indexable(*arrays)
A:sklearn.cross_validation.(train, test)->next(iter(cv))
sklearn.cross_validation.BaseShuffleSplit(self,n,n_iter=10,test_size=0.1,train_size=None,random_state=None)
sklearn.cross_validation.BaseShuffleSplit.__init__(self,n,n_iter=10,test_size=0.1,train_size=None,random_state=None)
sklearn.cross_validation.BaseShuffleSplit.__iter__(self)
sklearn.cross_validation.BaseShuffleSplit._iter_indices(self)
sklearn.cross_validation.KFold(self,n,n_folds=3,shuffle=False,random_state=None)
sklearn.cross_validation.KFold.__init__(self,n,n_folds=3,shuffle=False,random_state=None)
sklearn.cross_validation.KFold.__len__(self)
sklearn.cross_validation.KFold.__repr__(self)
sklearn.cross_validation.KFold._iter_test_indices(self)
sklearn.cross_validation.LabelKFold(self,labels,n_folds=3)
sklearn.cross_validation.LabelKFold.__init__(self,labels,n_folds=3)
sklearn.cross_validation.LabelKFold.__len__(self)
sklearn.cross_validation.LabelKFold.__repr__(self)
sklearn.cross_validation.LabelKFold._iter_test_indices(self)
sklearn.cross_validation.LabelShuffleSplit(self,labels,n_iter=5,test_size=0.2,train_size=None,random_state=None)
sklearn.cross_validation.LabelShuffleSplit.__init__(self,labels,n_iter=5,test_size=0.2,train_size=None,random_state=None)
sklearn.cross_validation.LabelShuffleSplit.__len__(self)
sklearn.cross_validation.LabelShuffleSplit.__repr__(self)
sklearn.cross_validation.LabelShuffleSplit._iter_indices(self)
sklearn.cross_validation.LeaveOneLabelOut(self,labels)
sklearn.cross_validation.LeaveOneLabelOut.__init__(self,labels)
sklearn.cross_validation.LeaveOneLabelOut.__len__(self)
sklearn.cross_validation.LeaveOneLabelOut.__repr__(self)
sklearn.cross_validation.LeaveOneLabelOut._iter_test_masks(self)
sklearn.cross_validation.LeaveOneOut(_PartitionIterator)
sklearn.cross_validation.LeaveOneOut.__len__(self)
sklearn.cross_validation.LeaveOneOut.__repr__(self)
sklearn.cross_validation.LeaveOneOut._iter_test_indices(self)
sklearn.cross_validation.LeavePLabelOut(self,labels,p)
sklearn.cross_validation.LeavePLabelOut.__init__(self,labels,p)
sklearn.cross_validation.LeavePLabelOut.__len__(self)
sklearn.cross_validation.LeavePLabelOut.__repr__(self)
sklearn.cross_validation.LeavePLabelOut._iter_test_masks(self)
sklearn.cross_validation.LeavePOut(self,n,p)
sklearn.cross_validation.LeavePOut.__init__(self,n,p)
sklearn.cross_validation.LeavePOut.__len__(self)
sklearn.cross_validation.LeavePOut.__repr__(self)
sklearn.cross_validation.LeavePOut._iter_test_indices(self)
sklearn.cross_validation.PredefinedSplit(self,test_fold)
sklearn.cross_validation.PredefinedSplit.__init__(self,test_fold)
sklearn.cross_validation.PredefinedSplit.__len__(self)
sklearn.cross_validation.PredefinedSplit.__repr__(self)
sklearn.cross_validation.PredefinedSplit._iter_test_indices(self)
sklearn.cross_validation.ShuffleSplit(BaseShuffleSplit)
sklearn.cross_validation.ShuffleSplit.__len__(self)
sklearn.cross_validation.ShuffleSplit.__repr__(self)
sklearn.cross_validation.ShuffleSplit._iter_indices(self)
sklearn.cross_validation.StratifiedKFold(self,y,n_folds=3,shuffle=False,random_state=None)
sklearn.cross_validation.StratifiedKFold.__init__(self,y,n_folds=3,shuffle=False,random_state=None)
sklearn.cross_validation.StratifiedKFold.__len__(self)
sklearn.cross_validation.StratifiedKFold.__repr__(self)
sklearn.cross_validation.StratifiedKFold._iter_test_masks(self)
sklearn.cross_validation.StratifiedShuffleSplit(self,y,n_iter=10,test_size=0.1,train_size=None,random_state=None)
sklearn.cross_validation.StratifiedShuffleSplit.__init__(self,y,n_iter=10,test_size=0.1,train_size=None,random_state=None)
sklearn.cross_validation.StratifiedShuffleSplit.__len__(self)
sklearn.cross_validation.StratifiedShuffleSplit.__repr__(self)
sklearn.cross_validation.StratifiedShuffleSplit._iter_indices(self)
sklearn.cross_validation._BaseKFold(self,n,n_folds,shuffle,random_state)
sklearn.cross_validation._BaseKFold.__init__(self,n,n_folds,shuffle,random_state)
sklearn.cross_validation._PartitionIterator(self,n)
sklearn.cross_validation._PartitionIterator.__init__(self,n)
sklearn.cross_validation._PartitionIterator.__iter__(self)
sklearn.cross_validation._PartitionIterator._empty_mask(self)
sklearn.cross_validation._PartitionIterator._iter_test_indices(self)
sklearn.cross_validation._PartitionIterator._iter_test_masks(self)
sklearn.cross_validation._approximate_mode(class_counts,n_draws,rng)
sklearn.cross_validation._check_is_partition(locs,n)
sklearn.cross_validation._fit_and_predict(estimator,X,y,train,test,verbose,fit_params)
sklearn.cross_validation._fit_and_score(estimator,X,y,scorer,train,test,verbose,parameters,fit_params,return_train_score=False,return_parameters=False,error_score='raise')
sklearn.cross_validation._index_param_value(X,v,indices)
sklearn.cross_validation._permutation_test_score(estimator,X,y,cv,scorer)
sklearn.cross_validation._safe_split(estimator,X,y,indices,train_indices=None)
sklearn.cross_validation._score(estimator,X_test,y_test,scorer)
sklearn.cross_validation._shuffle(y,labels,random_state)
sklearn.cross_validation._validate_shuffle_split(n,test_size,train_size)
sklearn.cross_validation.check_cv(cv,X=None,y=None,classifier=False)
sklearn.cross_validation.cross_val_predict(estimator,X,y=None,cv=None,n_jobs=1,verbose=0,fit_params=None,pre_dispatch='2*n_jobs')
sklearn.cross_validation.cross_val_score(estimator,X,y=None,scoring=None,cv=None,n_jobs=1,verbose=0,fit_params=None,pre_dispatch='2*n_jobs')
sklearn.cross_validation.permutation_test_score(estimator,X,y,cv=None,n_permutations=100,n_jobs=1,labels=None,random_state=0,verbose=0,scoring=None)
sklearn.cross_validation.train_test_split(*arrays,**options)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/_build_utils/__init__.py----------------------------------------
A:sklearn._build_utils.__init__.def_macros->get_info('blas_opt', 0).get('define_macros', [])
A:sklearn._build_utils.__init__.blas_info->get_info('blas_opt', 0)
A:sklearn._build_utils.__init__.cblas_libs->get_info('blas_opt', 0).pop('libraries', [])
A:sklearn._build_utils.__init__.(path, ext)->os.path.splitext(sfile)
A:sklearn._build_utils.__init__.is_release->os.path.exists(os.path.join(top_path, 'PKG-INFO'))
A:sklearn._build_utils.__init__.message->'Please install cython with a version >= {0} in order to build a scikit-learn development version.'.format(CYTHON_MIN_VERSION)
A:sklearn._build_utils.__init__.config.ext_modules->cythonize(config.ext_modules)
sklearn._build_utils.__init__.build_from_c_and_cpp_files(extensions)
sklearn._build_utils.__init__.get_blas_info()
sklearn._build_utils.__init__.maybe_cythonize_extensions(top_path,config)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py----------------------------------------
A:sklearn.decomposition.online_lda.is_sparse_x->scipy.sparse.issparse(X)
A:sklearn.decomposition.online_lda.doc_topic_distr->self._unnormalized_transform(X)
A:sklearn.decomposition.online_lda.exp_doc_topic->numpy.exp(_dirichlet_expectation_2d(doc_topic_distr))
A:sklearn.decomposition.online_lda.exp_doc_topic_d->exp_doc_topic[idx_d, :].copy()
A:sklearn.decomposition.online_lda.self.random_state_->check_random_state(self.random_state)
A:sklearn.decomposition.online_lda.self.components_->self.random_state_.gamma(init_gamma, init_var, (self._n_components, n_features))
A:sklearn.decomposition.online_lda.self.exp_dirichlet_component_->numpy.exp(_dirichlet_expectation_2d(self.components_))
A:sklearn.decomposition.online_lda.n_jobs->_get_n_jobs(self.n_jobs)
A:sklearn.decomposition.online_lda.parallel->Parallel(n_jobs=n_jobs, verbose=max(0, self.verbose - 1))
A:sklearn.decomposition.online_lda.results->parallel((delayed(_update_doc_distribution)(X[idx_slice, :], self.exp_dirichlet_component_, self.doc_topic_prior_, self.max_doc_update_iter, self.mean_change_tol, cal_sstats, random_state) for idx_slice in gen_even_slices(X.shape[0], n_jobs)))
A:sklearn.decomposition.online_lda.(doc_topics, sstats_list)->zip(*results)
A:sklearn.decomposition.online_lda.suff_stats->numpy.zeros(self.components_.shape)
A:sklearn.decomposition.online_lda.(_, suff_stats)->self._e_step(X, cal_sstats=True, random_init=True, parallel=parallel)
A:sklearn.decomposition.online_lda.weight->numpy.power(self.learning_offset + self.n_batch_iter_, -self.learning_decay)
A:sklearn.decomposition.online_lda.X->self._check_non_neg_array(X, 'LatentDirichletAllocation.perplexity')
A:sklearn.decomposition.online_lda.(doc_topics_distr, _)->self._e_step(X, cal_sstats=False, random_init=False, parallel=parallel)
A:sklearn.decomposition.online_lda.bound->self._approx_bound(X, doc_topic_distr, sub_sampling)
A:sklearn.decomposition.online_lda.self.bound_->self._perplexity_precomp_distr(X, doc_topics_distr, sub_sampling=False)
A:sklearn.decomposition.online_lda.(doc_topic_distr, _)->self._e_step(X, cal_sstats=False, random_init=False)
A:sklearn.decomposition.online_lda.score->self._approx_bound(X, doc_topic_distr, sub_sampling=False)
A:sklearn.decomposition.online_lda.dirichlet_doc_topic->_dirichlet_expectation_2d(doc_topic_distr)
A:sklearn.decomposition.online_lda.dirichlet_component_->_dirichlet_expectation_2d(self.components_)
A:sklearn.decomposition.online_lda.norm_phi->logsumexp(temp, axis=0)
A:sklearn.decomposition.online_lda.word_cnt->self._check_non_neg_array(X, 'LatentDirichletAllocation.perplexity').sum()
sklearn.decomposition.LatentDirichletAllocation(self,n_components=10,doc_topic_prior=None,topic_word_prior=None,learning_method=None,learning_decay=0.7,learning_offset=10.0,max_iter=10,batch_size=128,evaluate_every=-1,total_samples=1000000.0,perp_tol=0.1,mean_change_tol=0.001,max_doc_update_iter=100,n_jobs=1,verbose=0,random_state=None,n_topics=None)
sklearn.decomposition.LatentDirichletAllocation._approx_bound(self,X,doc_topic_distr,sub_sampling)
sklearn.decomposition.LatentDirichletAllocation._check_non_neg_array(self,X,whom)
sklearn.decomposition.LatentDirichletAllocation._check_params(self)
sklearn.decomposition.LatentDirichletAllocation._e_step(self,X,cal_sstats,random_init,parallel=None)
sklearn.decomposition.LatentDirichletAllocation._em_step(self,X,total_samples,batch_update,parallel=None)
sklearn.decomposition.LatentDirichletAllocation._init_latent_vars(self,n_features)
sklearn.decomposition.LatentDirichletAllocation._perplexity_precomp_distr(self,X,doc_topic_distr=None,sub_sampling=False)
sklearn.decomposition.LatentDirichletAllocation._unnormalized_transform(self,X)
sklearn.decomposition.LatentDirichletAllocation.fit(self,X,y=None)
sklearn.decomposition.LatentDirichletAllocation.partial_fit(self,X,y=None)
sklearn.decomposition.LatentDirichletAllocation.perplexity(self,X,doc_topic_distr='deprecated',sub_sampling=False)
sklearn.decomposition.LatentDirichletAllocation.score(self,X,y=None)
sklearn.decomposition.LatentDirichletAllocation.transform(self,X)
sklearn.decomposition.online_lda.LatentDirichletAllocation(self,n_components=10,doc_topic_prior=None,topic_word_prior=None,learning_method=None,learning_decay=0.7,learning_offset=10.0,max_iter=10,batch_size=128,evaluate_every=-1,total_samples=1000000.0,perp_tol=0.1,mean_change_tol=0.001,max_doc_update_iter=100,n_jobs=1,verbose=0,random_state=None,n_topics=None)
sklearn.decomposition.online_lda.LatentDirichletAllocation.__init__(self,n_components=10,doc_topic_prior=None,topic_word_prior=None,learning_method=None,learning_decay=0.7,learning_offset=10.0,max_iter=10,batch_size=128,evaluate_every=-1,total_samples=1000000.0,perp_tol=0.1,mean_change_tol=0.001,max_doc_update_iter=100,n_jobs=1,verbose=0,random_state=None,n_topics=None)
sklearn.decomposition.online_lda.LatentDirichletAllocation._approx_bound(self,X,doc_topic_distr,sub_sampling)
sklearn.decomposition.online_lda.LatentDirichletAllocation._check_non_neg_array(self,X,whom)
sklearn.decomposition.online_lda.LatentDirichletAllocation._check_params(self)
sklearn.decomposition.online_lda.LatentDirichletAllocation._e_step(self,X,cal_sstats,random_init,parallel=None)
sklearn.decomposition.online_lda.LatentDirichletAllocation._em_step(self,X,total_samples,batch_update,parallel=None)
sklearn.decomposition.online_lda.LatentDirichletAllocation._init_latent_vars(self,n_features)
sklearn.decomposition.online_lda.LatentDirichletAllocation._perplexity_precomp_distr(self,X,doc_topic_distr=None,sub_sampling=False)
sklearn.decomposition.online_lda.LatentDirichletAllocation._unnormalized_transform(self,X)
sklearn.decomposition.online_lda.LatentDirichletAllocation.fit(self,X,y=None)
sklearn.decomposition.online_lda.LatentDirichletAllocation.partial_fit(self,X,y=None)
sklearn.decomposition.online_lda.LatentDirichletAllocation.perplexity(self,X,doc_topic_distr='deprecated',sub_sampling=False)
sklearn.decomposition.online_lda.LatentDirichletAllocation.score(self,X,y=None)
sklearn.decomposition.online_lda.LatentDirichletAllocation.transform(self,X)
sklearn.decomposition.online_lda._update_doc_distribution(X,exp_topic_word_distr,doc_topic_prior,max_iters,mean_change_tol,cal_sstats,random_state)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/decomposition/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/decomposition/factor_analysis.py----------------------------------------
A:sklearn.decomposition.factor_analysis.X->check_array(X)
A:sklearn.decomposition.factor_analysis.self.mean_->numpy.mean(X, axis=0)
A:sklearn.decomposition.factor_analysis.nsqrt->sqrt(n_samples)
A:sklearn.decomposition.factor_analysis.var->numpy.var(X, axis=0)
A:sklearn.decomposition.factor_analysis.psi->numpy.maximum(var - np.sum(W ** 2, axis=0), SMALL)
A:sklearn.decomposition.factor_analysis.(_, s, V)->randomized_svd(X, n_components, random_state=random_state, n_iter=self.iterated_power)
A:sklearn.decomposition.factor_analysis.random_state->check_random_state(self.random_state)
A:sklearn.decomposition.factor_analysis.(s, V, unexp_var)->my_svd(X / (sqrt_psi * nsqrt))
A:sklearn.decomposition.factor_analysis.Ih->numpy.eye(len(self.components_))
A:sklearn.decomposition.factor_analysis.cov_z->scipy.linalg.inv(Ih + np.dot(Wpsi, self.components_.T))
A:sklearn.decomposition.factor_analysis.tmp->numpy.dot(X_transformed, Wpsi.T)
A:sklearn.decomposition.factor_analysis.X_transformed->numpy.dot(tmp, cov_z)
A:sklearn.decomposition.factor_analysis.cov->numpy.dot(self.components_.T, self.components_)
A:sklearn.decomposition.factor_analysis.precision->self.get_precision()
A:sklearn.decomposition.factor_analysis.log_like->numpy.zeros(X.shape[0])
sklearn.decomposition.FactorAnalysis(self,n_components=None,tol=0.01,copy=True,max_iter=1000,noise_variance_init=None,svd_method='randomized',iterated_power=3,random_state=0)
sklearn.decomposition.FactorAnalysis.fit(self,X,y=None)
sklearn.decomposition.FactorAnalysis.get_covariance(self)
sklearn.decomposition.FactorAnalysis.get_precision(self)
sklearn.decomposition.FactorAnalysis.score(self,X,y=None)
sklearn.decomposition.FactorAnalysis.score_samples(self,X)
sklearn.decomposition.FactorAnalysis.transform(self,X)
sklearn.decomposition.factor_analysis.FactorAnalysis(self,n_components=None,tol=0.01,copy=True,max_iter=1000,noise_variance_init=None,svd_method='randomized',iterated_power=3,random_state=0)
sklearn.decomposition.factor_analysis.FactorAnalysis.__init__(self,n_components=None,tol=0.01,copy=True,max_iter=1000,noise_variance_init=None,svd_method='randomized',iterated_power=3,random_state=0)
sklearn.decomposition.factor_analysis.FactorAnalysis.fit(self,X,y=None)
sklearn.decomposition.factor_analysis.FactorAnalysis.get_covariance(self)
sklearn.decomposition.factor_analysis.FactorAnalysis.get_precision(self)
sklearn.decomposition.factor_analysis.FactorAnalysis.score(self,X,y=None)
sklearn.decomposition.factor_analysis.FactorAnalysis.score_samples(self,X)
sklearn.decomposition.factor_analysis.FactorAnalysis.transform(self,X)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/decomposition/pca.py----------------------------------------
A:sklearn.decomposition.pca.pl->numpy.sum(np.log(spectrum[:rank]))
A:sklearn.decomposition.pca.spectrum_->spectrum.copy()
A:sklearn.decomposition.pca.n_spectrum->len(spectrum)
A:sklearn.decomposition.pca.ll->numpy.empty(n_spectrum)
A:sklearn.decomposition.pca.ll[rank]->_assess_dimension_(spectrum, rank, n_samples, n_features)
A:sklearn.decomposition.pca.(U, S, V)->randomized_svd(X, n_components, n_iter=self.iterated_power, random_state=random_state)
A:sklearn.decomposition.pca.X->self._fit(X)
A:sklearn.decomposition.pca.self.mean_->numpy.mean(X, axis=0)
A:sklearn.decomposition.pca.(U, V)->svd_flip(U[:, ::-1], V[::-1])
A:sklearn.decomposition.pca.total_var->numpy.var(X, ddof=1, axis=0)
A:sklearn.decomposition.pca.singular_values_->S.copy()
A:sklearn.decomposition.pca.n_components->_infer_dimension_(explained_variance_, n_samples, n_features)
A:sklearn.decomposition.pca.ratio_cumsum->stable_cumsum(explained_variance_ratio_)
A:sklearn.decomposition.pca.self.noise_variance_->explained_variance_[n_components:].mean()
A:sklearn.decomposition.pca.random_state->check_random_state(self.random_state)
A:sklearn.decomposition.pca.v0->check_random_state(self.random_state).uniform(-1, 1, size=min(X.shape))
A:sklearn.decomposition.pca.self.singular_values_->S.copy()
A:sklearn.decomposition.pca.log_like->numpy.zeros(X.shape[0])
A:sklearn.decomposition.pca.precision->self.get_precision()
A:sklearn.decomposition.pca.full_var->numpy.var(X, ddof=1, axis=0).sum()
A:sklearn.decomposition.pca.X_original->numpy.dot(X, self.components_)
sklearn.decomposition.PCA(self,n_components=None,copy=True,whiten=False,svd_solver='auto',tol=0.0,iterated_power='auto',random_state=None)
sklearn.decomposition.PCA._fit(self,X)
sklearn.decomposition.PCA._fit_full(self,X,n_components)
sklearn.decomposition.PCA._fit_truncated(self,X,n_components,svd_solver)
sklearn.decomposition.PCA.fit(self,X,y=None)
sklearn.decomposition.PCA.fit_transform(self,X,y=None)
sklearn.decomposition.PCA.score(self,X,y=None)
sklearn.decomposition.PCA.score_samples(self,X)
sklearn.decomposition.RandomizedPCA(self,n_components=None,copy=True,iterated_power=2,whiten=False,random_state=None)
sklearn.decomposition.RandomizedPCA._fit(self,X)
sklearn.decomposition.RandomizedPCA.fit(self,X,y=None)
sklearn.decomposition.RandomizedPCA.fit_transform(self,X,y=None)
sklearn.decomposition.RandomizedPCA.inverse_transform(self,X)
sklearn.decomposition.RandomizedPCA.transform(self,X)
sklearn.decomposition.pca.PCA(self,n_components=None,copy=True,whiten=False,svd_solver='auto',tol=0.0,iterated_power='auto',random_state=None)
sklearn.decomposition.pca.PCA.__init__(self,n_components=None,copy=True,whiten=False,svd_solver='auto',tol=0.0,iterated_power='auto',random_state=None)
sklearn.decomposition.pca.PCA._fit(self,X)
sklearn.decomposition.pca.PCA._fit_full(self,X,n_components)
sklearn.decomposition.pca.PCA._fit_truncated(self,X,n_components,svd_solver)
sklearn.decomposition.pca.PCA.fit(self,X,y=None)
sklearn.decomposition.pca.PCA.fit_transform(self,X,y=None)
sklearn.decomposition.pca.PCA.score(self,X,y=None)
sklearn.decomposition.pca.PCA.score_samples(self,X)
sklearn.decomposition.pca.RandomizedPCA(self,n_components=None,copy=True,iterated_power=2,whiten=False,random_state=None)
sklearn.decomposition.pca.RandomizedPCA.__init__(self,n_components=None,copy=True,iterated_power=2,whiten=False,random_state=None)
sklearn.decomposition.pca.RandomizedPCA._fit(self,X)
sklearn.decomposition.pca.RandomizedPCA.fit(self,X,y=None)
sklearn.decomposition.pca.RandomizedPCA.fit_transform(self,X,y=None)
sklearn.decomposition.pca.RandomizedPCA.inverse_transform(self,X)
sklearn.decomposition.pca.RandomizedPCA.transform(self,X)
sklearn.decomposition.pca._assess_dimension_(spectrum,rank,n_samples,n_features)
sklearn.decomposition.pca._infer_dimension_(spectrum,n_samples,n_features)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/decomposition/truncated_svd.py----------------------------------------
A:sklearn.decomposition.truncated_svd.X->check_array(X)
A:sklearn.decomposition.truncated_svd.random_state->check_random_state(self.random_state)
A:sklearn.decomposition.truncated_svd.(U, Sigma, VT)->randomized_svd(X, self.n_components, n_iter=self.n_iter, random_state=random_state)
A:sklearn.decomposition.truncated_svd.(U, VT)->svd_flip(U[:, ::-1], VT[::-1])
A:sklearn.decomposition.truncated_svd.self.explained_variance_exp_var->numpy.var(X_transformed, axis=0)
A:sklearn.decomposition.truncated_svd.(_, full_var)->mean_variance_axis(X, axis=0)
A:sklearn.decomposition.truncated_svd.full_var->numpy.var(X, axis=0).sum()
sklearn.decomposition.TruncatedSVD(self,n_components=2,algorithm='randomized',n_iter=5,random_state=None,tol=0.0)
sklearn.decomposition.TruncatedSVD.fit(self,X,y=None)
sklearn.decomposition.TruncatedSVD.fit_transform(self,X,y=None)
sklearn.decomposition.TruncatedSVD.inverse_transform(self,X)
sklearn.decomposition.TruncatedSVD.transform(self,X)
sklearn.decomposition.truncated_svd.TruncatedSVD(self,n_components=2,algorithm='randomized',n_iter=5,random_state=None,tol=0.0)
sklearn.decomposition.truncated_svd.TruncatedSVD.__init__(self,n_components=2,algorithm='randomized',n_iter=5,random_state=None,tol=0.0)
sklearn.decomposition.truncated_svd.TruncatedSVD.fit(self,X,y=None)
sklearn.decomposition.truncated_svd.TruncatedSVD.fit_transform(self,X,y=None)
sklearn.decomposition.truncated_svd.TruncatedSVD.inverse_transform(self,X)
sklearn.decomposition.truncated_svd.TruncatedSVD.transform(self,X)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/decomposition/sparse_pca.py----------------------------------------
A:sklearn.decomposition.sparse_pca.random_state->check_random_state(self.random_state)
A:sklearn.decomposition.sparse_pca.X->check_array(X)
A:sklearn.decomposition.sparse_pca.(Vt, _, E, self.n_iter_)->dict_learning(X.T, n_components, self.alpha, tol=self.tol, max_iter=self.max_iter, method=self.method, n_jobs=self.n_jobs, verbose=self.verbose, random_state=random_state, code_init=code_init, dict_init=dict_init, return_n_iter=True)
A:sklearn.decomposition.sparse_pca.U->ridge_regression(self.components_.T, X.T, ridge_alpha, solver='cholesky')
A:sklearn.decomposition.sparse_pca.s->numpy.sqrt((U ** 2).sum(axis=0))
A:sklearn.decomposition.sparse_pca.(Vt, _, self.n_iter_)->dict_learning_online(X.T, n_components, alpha=self.alpha, n_iter=self.n_iter, return_code=True, dict_init=None, verbose=self.verbose, callback=self.callback, batch_size=self.batch_size, shuffle=self.shuffle, n_jobs=self.n_jobs, method=self.method, random_state=random_state, return_n_iter=True)
sklearn.decomposition.MiniBatchSparsePCA(self,n_components=None,alpha=1,ridge_alpha=0.01,n_iter=100,callback=None,batch_size=3,verbose=False,shuffle=True,n_jobs=1,method='lars',random_state=None)
sklearn.decomposition.MiniBatchSparsePCA.fit(self,X,y=None)
sklearn.decomposition.SparsePCA(self,n_components=None,alpha=1,ridge_alpha=0.01,max_iter=1000,tol=1e-08,method='lars',n_jobs=1,U_init=None,V_init=None,verbose=False,random_state=None)
sklearn.decomposition.SparsePCA.fit(self,X,y=None)
sklearn.decomposition.SparsePCA.transform(self,X,ridge_alpha='deprecated')
sklearn.decomposition.sparse_pca.MiniBatchSparsePCA(self,n_components=None,alpha=1,ridge_alpha=0.01,n_iter=100,callback=None,batch_size=3,verbose=False,shuffle=True,n_jobs=1,method='lars',random_state=None)
sklearn.decomposition.sparse_pca.MiniBatchSparsePCA.__init__(self,n_components=None,alpha=1,ridge_alpha=0.01,n_iter=100,callback=None,batch_size=3,verbose=False,shuffle=True,n_jobs=1,method='lars',random_state=None)
sklearn.decomposition.sparse_pca.MiniBatchSparsePCA.fit(self,X,y=None)
sklearn.decomposition.sparse_pca.SparsePCA(self,n_components=None,alpha=1,ridge_alpha=0.01,max_iter=1000,tol=1e-08,method='lars',n_jobs=1,U_init=None,V_init=None,verbose=False,random_state=None)
sklearn.decomposition.sparse_pca.SparsePCA.__init__(self,n_components=None,alpha=1,ridge_alpha=0.01,max_iter=1000,tol=1e-08,method='lars',n_jobs=1,U_init=None,V_init=None,verbose=False,random_state=None)
sklearn.decomposition.sparse_pca.SparsePCA.fit(self,X,y=None)
sklearn.decomposition.sparse_pca.SparsePCA.transform(self,X,ridge_alpha='deprecated')


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/decomposition/fastica_.py----------------------------------------
A:sklearn.decomposition.fastica_.(s, u)->scipy.linalg.eigh(np.dot(W, W.T))
A:sklearn.decomposition.fastica_.W->_sym_decorrelation(w_init)
A:sklearn.decomposition.fastica_.w->w_init[j, :].copy()
A:sklearn.decomposition.fastica_.(gwtx, g_wtx)->g(np.dot(W, X), fun_args)
A:sklearn.decomposition.fastica_.lim->max(abs(abs(np.diag(np.dot(W1, W.T))) - 1))
A:sklearn.decomposition.fastica_.p_->float(X.shape[1])
A:sklearn.decomposition.fastica_.W1->_sym_decorrelation(np.dot(gwtx, X.T) / p_ - g_wtx[:, np.newaxis] * W)
A:sklearn.decomposition.fastica_.alpha->fun_args.get('alpha', 1.0)
A:sklearn.decomposition.fastica_.gx->numpy.tanh(x, x)
A:sklearn.decomposition.fastica_.g_x->numpy.empty(x.shape[0])
A:sklearn.decomposition.fastica_.g_x[i]->(alpha * (1 - gx_i ** 2)).mean()
A:sklearn.decomposition.fastica_.exp->numpy.exp(-x ** 2 / 2)
A:sklearn.decomposition.fastica_.random_state->check_random_state(random_state)
A:sklearn.decomposition.fastica_.n_components->min(n, p)
A:sklearn.decomposition.fastica_.X_mean->numpy.dot(X, self.mixing_.T).mean(axis=-1)
A:sklearn.decomposition.fastica_.(u, d, _)->scipy.linalg.svd(X, full_matrices=False)
A:sklearn.decomposition.fastica_.X1->as_float_array(X, copy=False)
A:sklearn.decomposition.fastica_.w_init->numpy.asarray(w_init)
A:sklearn.decomposition.fastica_.(W, n_iter)->_ica_def(X1, **kwargs)
A:sklearn.decomposition.fastica_.(whitening, unmixing, sources, X_mean, self.n_iter_)->fastica(X=X, n_components=self.n_components, algorithm=self.algorithm, whiten=self.whiten, fun=self.fun, fun_args=fun_args, max_iter=self.max_iter, tol=self.tol, w_init=self.w_init, random_state=self.random_state, return_X_mean=True, compute_sources=compute_sources, return_n_iter=True)
A:sklearn.decomposition.fastica_.self.components_->numpy.dot(unmixing, whitening)
A:sklearn.decomposition.fastica_.self.mixing_->scipy.linalg.pinv(self.components_)
A:sklearn.decomposition.fastica_.X->numpy.dot(X, self.mixing_.T)
sklearn.decomposition.FastICA(self,n_components=None,algorithm='parallel',whiten=True,fun='logcosh',fun_args=None,max_iter=200,tol=0.0001,w_init=None,random_state=None)
sklearn.decomposition.FastICA._fit(self,X,compute_sources=False)
sklearn.decomposition.FastICA.fit(self,X,y=None)
sklearn.decomposition.FastICA.fit_transform(self,X,y=None)
sklearn.decomposition.FastICA.inverse_transform(self,X,copy=True)
sklearn.decomposition.FastICA.transform(self,X,y='deprecated',copy=True)
sklearn.decomposition.fastica(X,n_components=None,algorithm='parallel',whiten=True,fun='logcosh',fun_args=None,max_iter=200,tol=0.0001,w_init=None,random_state=None,return_X_mean=False,compute_sources=True,return_n_iter=False)
sklearn.decomposition.fastica_.FastICA(self,n_components=None,algorithm='parallel',whiten=True,fun='logcosh',fun_args=None,max_iter=200,tol=0.0001,w_init=None,random_state=None)
sklearn.decomposition.fastica_.FastICA.__init__(self,n_components=None,algorithm='parallel',whiten=True,fun='logcosh',fun_args=None,max_iter=200,tol=0.0001,w_init=None,random_state=None)
sklearn.decomposition.fastica_.FastICA._fit(self,X,compute_sources=False)
sklearn.decomposition.fastica_.FastICA.fit(self,X,y=None)
sklearn.decomposition.fastica_.FastICA.fit_transform(self,X,y=None)
sklearn.decomposition.fastica_.FastICA.inverse_transform(self,X,copy=True)
sklearn.decomposition.fastica_.FastICA.transform(self,X,y='deprecated',copy=True)
sklearn.decomposition.fastica_._cube(x,fun_args)
sklearn.decomposition.fastica_._exp(x,fun_args)
sklearn.decomposition.fastica_._gs_decorrelation(w,W,j)
sklearn.decomposition.fastica_._ica_def(X,tol,g,fun_args,max_iter,w_init)
sklearn.decomposition.fastica_._ica_par(X,tol,g,fun_args,max_iter,w_init)
sklearn.decomposition.fastica_._logcosh(x,fun_args=None)
sklearn.decomposition.fastica_._sym_decorrelation(W)
sklearn.decomposition.fastica_.fastica(X,n_components=None,algorithm='parallel',whiten=True,fun='logcosh',fun_args=None,max_iter=200,tol=0.0001,w_init=None,random_state=None,return_X_mean=False,compute_sources=True,return_n_iter=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/decomposition/setup.py----------------------------------------
A:sklearn.decomposition.setup.config->Configuration('decomposition', parent_package, top_path)
sklearn.decomposition.setup.configuration(parent_package='',top_path=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/decomposition/incremental_pca.py----------------------------------------
A:sklearn.decomposition.incremental_pca.X->numpy.vstack((self.singular_values_.reshape((-1, 1)) * self.components_, X, mean_correction))
A:sklearn.decomposition.incremental_pca.(col_mean, col_var, n_total_samples)->_incremental_mean_and_var(X, last_mean=self.mean_, last_variance=self.var_, last_sample_count=self.n_samples_seen_)
A:sklearn.decomposition.incremental_pca.col_batch_mean->numpy.mean(X, axis=0)
A:sklearn.decomposition.incremental_pca.(U, S, V)->scipy.linalg.svd(X, full_matrices=False)
A:sklearn.decomposition.incremental_pca.(U, V)->svd_flip(U, V, u_based_decision=False)
A:sklearn.decomposition.incremental_pca.self.noise_variance_->explained_variance[self.n_components_:].mean()
sklearn.decomposition.IncrementalPCA(self,n_components=None,whiten=False,copy=True,batch_size=None)
sklearn.decomposition.IncrementalPCA.fit(self,X,y=None)
sklearn.decomposition.IncrementalPCA.partial_fit(self,X,y=None,check_input=True)
sklearn.decomposition.incremental_pca.IncrementalPCA(self,n_components=None,whiten=False,copy=True,batch_size=None)
sklearn.decomposition.incremental_pca.IncrementalPCA.__init__(self,n_components=None,whiten=False,copy=True,batch_size=None)
sklearn.decomposition.incremental_pca.IncrementalPCA.fit(self,X,y=None)
sklearn.decomposition.incremental_pca.IncrementalPCA.partial_fit(self,X,y=None,check_input=True)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/decomposition/nmf.py----------------------------------------
A:sklearn.decomposition.nmf.A->check_array(A)
A:sklearn.decomposition.nmf.beta->_beta_loss_to_float(beta)
A:sklearn.decomposition.nmf.X->check_array(X, accept_sparse=('csr', 'csc'), dtype=float)
A:sklearn.decomposition.nmf.W->numpy.zeros((n_samples, n_components))
A:sklearn.decomposition.nmf.H->numpy.atleast_2d(H)
A:sklearn.decomposition.nmf.norm_X->numpy.dot(X.data, X.data)
A:sklearn.decomposition.nmf.norm_WH->trace_dot(np.dot(np.dot(W.T, W), H), H)
A:sklearn.decomposition.nmf.cross_prod->trace_dot(X * H.T, W)
A:sklearn.decomposition.nmf.WH->_special_sparse_dot(W, H, X).copy()
A:sklearn.decomposition.nmf.WH_data->_special_sparse_dot(W, H, X).copy().ravel()
A:sklearn.decomposition.nmf.X_data->check_array(X, accept_sparse=('csr', 'csc'), dtype=float).ravel()
A:sklearn.decomposition.nmf.sum_WH->numpy.dot(np.sum(W, axis=0), np.sum(H, axis=1))
A:sklearn.decomposition.nmf.res->numpy.dot(X_data, np.log(div))
A:sklearn.decomposition.nmf.sum_WH_beta->numpy.sum(WH ** beta)
A:sklearn.decomposition.nmf.sum_X_WH->numpy.dot(X_data, WH_data ** (beta - 1))
A:sklearn.decomposition.nmf.(ii, jj)->check_array(X, accept_sparse=('csr', 'csc'), dtype=float).nonzero()
A:sklearn.decomposition.nmf.dot_vals->numpy.multiply(W[ii, :], H.T[jj, :]).sum(axis=1)
A:sklearn.decomposition.nmf.alpha_H->float(alpha)
A:sklearn.decomposition.nmf.alpha_W->float(alpha)
A:sklearn.decomposition.nmf.beta_loss->_check_string_param(solver, regularization, beta_loss, init)
A:sklearn.decomposition.nmf.avg->numpy.sqrt(X.mean() / n_components)
A:sklearn.decomposition.nmf.rng->check_random_state(random_state)
A:sklearn.decomposition.nmf.(U, S, V)->randomized_svd(X, n_components, random_state=random_state)
A:sklearn.decomposition.nmf.lbd->numpy.sqrt(S[j] * sigma)
A:sklearn.decomposition.nmf.W[W == 0]->abs(avg * rng.randn(len(W[W == 0])) / 100)
A:sklearn.decomposition.nmf.H[H == 0]->abs(avg * rng.randn(len(H[H == 0])) / 100)
A:sklearn.decomposition.nmf.HHt->numpy.dot(H, H.T)
A:sklearn.decomposition.nmf.XHt->safe_sparse_dot(X, H.T)
A:sklearn.decomposition.nmf.permutation->numpy.asarray(permutation, dtype=np.intp)
A:sklearn.decomposition.nmf.Ht->check_array(H.T, order='C')
A:sklearn.decomposition.nmf.numerator->safe_sparse_dot(W.T, WH_safe_X)
A:sklearn.decomposition.nmf.denominator->numpy.dot(np.dot(W.T, W), H)
A:sklearn.decomposition.nmf.WH_safe_X->_special_sparse_dot(W, H, X)
A:sklearn.decomposition.nmf.H_sum->numpy.sum(H, axis=1)
A:sklearn.decomposition.nmf.WHHt->numpy.dot(WH, H.T)
A:sklearn.decomposition.nmf.WHi->numpy.dot(W, H[:, i])
A:sklearn.decomposition.nmf.WHHt[i, :]->numpy.dot(WHi, H.T)
A:sklearn.decomposition.nmf.W_sum->numpy.sum(W, axis=0)
A:sklearn.decomposition.nmf.WtWH->numpy.dot(W.T, WH)
A:sklearn.decomposition.nmf.WtWH[:, i]->numpy.dot(W.T, WHi)
A:sklearn.decomposition.nmf.start_time->time.time()
A:sklearn.decomposition.nmf.error_at_init->_beta_divergence(X, W, H, beta_loss, square_root=True)
A:sklearn.decomposition.nmf.(delta_W, H_sum, HHt, XHt)->_multiplicative_update_w(X, W, H, beta_loss, l1_reg_W, l2_reg_W, gamma, H_sum, HHt, XHt, update_H)
A:sklearn.decomposition.nmf.delta_H->_multiplicative_update_h(X, W, H, beta_loss, l1_reg_H, l2_reg_H, gamma)
A:sklearn.decomposition.nmf.error->_beta_divergence(X, W, H, beta_loss, square_root=True)
A:sklearn.decomposition.nmf.iter_time->time.time()
A:sklearn.decomposition.nmf.end_time->time.time()
A:sklearn.decomposition.nmf.(W, H)->_initialize_nmf(X, n_components, init=init, random_state=random_state)
A:sklearn.decomposition.nmf.(l1_reg_W, l1_reg_H, l2_reg_W, l2_reg_H)->_compute_regularization(alpha, l1_ratio, regularization)
A:sklearn.decomposition.nmf.(W, H, n_iter)->_fit_multiplicative_update(X, W, H, beta_loss, max_iter, tol, l1_reg_W, l1_reg_H, l2_reg_W, l2_reg_H, update_H, verbose)
A:sklearn.decomposition.nmf.(W, H, n_iter_)->non_negative_factorization(X=X, W=W, H=H, n_components=self.n_components, init=self.init, update_H=True, solver=self.solver, beta_loss=self.beta_loss, tol=self.tol, max_iter=self.max_iter, alpha=self.alpha, l1_ratio=self.l1_ratio, regularization='both', random_state=self.random_state, verbose=self.verbose, shuffle=self.shuffle)
A:sklearn.decomposition.nmf.self.reconstruction_err_->_beta_divergence(X, W, H, self.beta_loss, square_root=True)
A:sklearn.decomposition.nmf.(W, _, n_iter_)->non_negative_factorization(X=X, W=None, H=self.components_, n_components=self.n_components_, init=self.init, update_H=False, solver=self.solver, beta_loss=self.beta_loss, tol=self.tol, max_iter=self.max_iter, alpha=self.alpha, l1_ratio=self.l1_ratio, regularization='both', random_state=self.random_state, verbose=self.verbose, shuffle=self.shuffle)
sklearn.decomposition.NMF(self,n_components=None,init=None,solver='cd',beta_loss='frobenius',tol=0.0001,max_iter=200,random_state=None,alpha=0.0,l1_ratio=0.0,verbose=0,shuffle=False)
sklearn.decomposition.NMF.fit(self,X,y=None,**params)
sklearn.decomposition.NMF.fit_transform(self,X,y=None,W=None,H=None)
sklearn.decomposition.NMF.inverse_transform(self,W)
sklearn.decomposition.NMF.transform(self,X)
sklearn.decomposition.nmf.NMF(self,n_components=None,init=None,solver='cd',beta_loss='frobenius',tol=0.0001,max_iter=200,random_state=None,alpha=0.0,l1_ratio=0.0,verbose=0,shuffle=False)
sklearn.decomposition.nmf.NMF.__init__(self,n_components=None,init=None,solver='cd',beta_loss='frobenius',tol=0.0001,max_iter=200,random_state=None,alpha=0.0,l1_ratio=0.0,verbose=0,shuffle=False)
sklearn.decomposition.nmf.NMF.fit(self,X,y=None,**params)
sklearn.decomposition.nmf.NMF.fit_transform(self,X,y=None,W=None,H=None)
sklearn.decomposition.nmf.NMF.inverse_transform(self,W)
sklearn.decomposition.nmf.NMF.transform(self,X)
sklearn.decomposition.nmf._beta_divergence(X,W,H,beta,square_root=False)
sklearn.decomposition.nmf._beta_loss_to_float(beta_loss)
sklearn.decomposition.nmf._check_init(A,shape,whom)
sklearn.decomposition.nmf._check_string_param(solver,regularization,beta_loss,init)
sklearn.decomposition.nmf._compute_regularization(alpha,l1_ratio,regularization)
sklearn.decomposition.nmf._fit_coordinate_descent(X,W,H,tol=0.0001,max_iter=200,l1_reg_W=0,l1_reg_H=0,l2_reg_W=0,l2_reg_H=0,update_H=True,verbose=0,shuffle=False,random_state=None)
sklearn.decomposition.nmf._fit_multiplicative_update(X,W,H,beta_loss='frobenius',max_iter=200,tol=0.0001,l1_reg_W=0,l1_reg_H=0,l2_reg_W=0,l2_reg_H=0,update_H=True,verbose=0)
sklearn.decomposition.nmf._initialize_nmf(X,n_components,init=None,eps=1e-06,random_state=None)
sklearn.decomposition.nmf._multiplicative_update_h(X,W,H,beta_loss,l1_reg_H,l2_reg_H,gamma)
sklearn.decomposition.nmf._multiplicative_update_w(X,W,H,beta_loss,l1_reg_W,l2_reg_W,gamma,H_sum=None,HHt=None,XHt=None,update_H=True)
sklearn.decomposition.nmf._special_sparse_dot(W,H,X)
sklearn.decomposition.nmf._update_coordinate_descent(X,W,Ht,l1_reg,l2_reg,shuffle,random_state)
sklearn.decomposition.nmf.non_negative_factorization(X,W=None,H=None,n_components=None,init='random',update_H=True,solver='cd',beta_loss='frobenius',tol=0.0001,max_iter=200,alpha=0.0,l1_ratio=0.0,regularization=None,random_state=None,verbose=0,shuffle=False)
sklearn.decomposition.nmf.norm(x)
sklearn.decomposition.nmf.trace_dot(X,Y)
sklearn.decomposition.non_negative_factorization(X,W=None,H=None,n_components=None,init='random',update_H=True,solver='cd',beta_loss='frobenius',tol=0.0001,max_iter=200,alpha=0.0,l1_ratio=0.0,regularization=None,random_state=None,verbose=0,shuffle=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/decomposition/base.py----------------------------------------
A:sklearn.decomposition.base.exp_var_diff->numpy.maximum(exp_var - self.noise_variance_, 0.0)
A:sklearn.decomposition.base.cov->numpy.dot(components_.T * exp_var_diff, components_)
A:sklearn.decomposition.base.precision->numpy.dot(components_.T, np.dot(linalg.inv(precision), components_))
A:sklearn.decomposition.base.X->check_array(X)
A:sklearn.decomposition.base.X_transformed->numpy.dot(X, self.components_.T)
sklearn.decomposition.base._BasePCA(six.with_metaclass(ABCMeta,BaseEstimator,TransformerMixin))
sklearn.decomposition.base._BasePCA.fit(X,y=None)
sklearn.decomposition.base._BasePCA.get_covariance(self)
sklearn.decomposition.base._BasePCA.get_precision(self)
sklearn.decomposition.base._BasePCA.inverse_transform(self,X)
sklearn.decomposition.base._BasePCA.transform(self,X)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/decomposition/dict_learning.py----------------------------------------
A:sklearn.decomposition.dict_learning.cov->numpy.dot(dictionary, X.T)
A:sklearn.decomposition.dict_learning.err_mgt->numpy.seterr(all='ignore')
A:sklearn.decomposition.dict_learning.lasso_lars->LassoLars(alpha=alpha, fit_intercept=False, verbose=verbose, normalize=False, precompute=gram, fit_path=False)
A:sklearn.decomposition.dict_learning.clf->Lasso(alpha=alpha, fit_intercept=False, normalize=False, precompute=gram, max_iter=max_iter, warm_start=True)
A:sklearn.decomposition.dict_learning.lars->Lars(fit_intercept=False, verbose=verbose, normalize=False, precompute=gram, n_nonzero_coefs=int(regularization), fit_path=False)
A:sklearn.decomposition.dict_learning.dictionary->_update_dict(dictionary, B, A, verbose=verbose, random_state=random_state)
A:sklearn.decomposition.dict_learning.X->check_array(X)
A:sklearn.decomposition.dict_learning.gram->numpy.dot(dictionary, dictionary.T)
A:sklearn.decomposition.dict_learning.regularization->min(max(n_features / 10, 1), n_components)
A:sklearn.decomposition.dict_learning.code->sparse_encode(X, self.components_, algorithm=self.transform_algorithm, n_nonzero_coefs=self.transform_n_nonzero_coefs, alpha=self.transform_alpha, n_jobs=self.n_jobs)
A:sklearn.decomposition.dict_learning.slices->list(gen_even_slices(n_samples, _get_n_jobs(n_jobs)))
A:sklearn.decomposition.dict_learning.code_views->Parallel(n_jobs=n_jobs, verbose=verbose)((delayed(_sparse_encode)(X[this_slice], dictionary, gram, cov[:, this_slice] if cov is not None else None, algorithm, regularization=regularization, copy_cov=copy_cov, init=init[this_slice] if init is not None else None, max_iter=max_iter, check_input=False) for this_slice in slices))
A:sklearn.decomposition.dict_learning.n_components->len(code)
A:sklearn.decomposition.dict_learning.random_state->check_random_state(self.random_state)
A:sklearn.decomposition.dict_learning.R->numpy.sum(R)
A:sklearn.decomposition.dict_learning.(ger,)->scipy.linalg.get_blas_funcs(('ger',), (dictionary, code))
A:sklearn.decomposition.dict_learning.dictionary[:, k]->check_random_state(self.random_state).randn(n_samples)
A:sklearn.decomposition.dict_learning.atom_norm_square->numpy.dot(dictionary[:, k], dictionary[:, k])
A:sklearn.decomposition.dict_learning.t0->time.time()
A:sklearn.decomposition.dict_learning.alpha->float(alpha)
A:sklearn.decomposition.dict_learning.n_jobs->cpu_count()
A:sklearn.decomposition.dict_learning.(code, S, dictionary)->scipy.linalg.svd(X, full_matrices=False)
A:sklearn.decomposition.dict_learning.r->len(dictionary)
A:sklearn.decomposition.dict_learning.(dictionary, residuals)->_update_dict(dictionary.T, X.T, code.T, verbose=verbose, return_r2=True, random_state=random_state)
A:sklearn.decomposition.dict_learning.(_, S, dictionary)->randomized_svd(X, n_components, random_state=random_state)
A:sklearn.decomposition.dict_learning.X_train->check_array(X_train, order='C', dtype=np.float64, copy=False)
A:sklearn.decomposition.dict_learning.batches->itertools.cycle(batches)
A:sklearn.decomposition.dict_learning.A->inner_stats[0].copy()
A:sklearn.decomposition.dict_learning.B->inner_stats[1].copy()
A:sklearn.decomposition.dict_learning.theta->float(batch_size ** 2 + ii + 1 - batch_size)
A:sklearn.decomposition.dict_learning.split_code->numpy.empty((n_samples, 2 * n_features))
A:sklearn.decomposition.dict_learning.split_code[:, :n_features]->numpy.maximum(code, 0)
A:sklearn.decomposition.dict_learning.(V, U, E, self.n_iter_)->dict_learning(X, n_components, self.alpha, tol=self.tol, max_iter=self.max_iter, method=self.fit_algorithm, n_jobs=self.n_jobs, code_init=self.code_init, dict_init=self.dict_init, verbose=self.verbose, random_state=random_state, return_n_iter=True)
A:sklearn.decomposition.dict_learning.(U, (A, B), self.n_iter_)->dict_learning_online(X, self.n_components, self.alpha, n_iter=self.n_iter, return_code=False, method=self.fit_algorithm, n_jobs=self.n_jobs, dict_init=self.dict_init, batch_size=self.batch_size, shuffle=self.shuffle, verbose=self.verbose, random_state=random_state, return_inner_stats=True, return_n_iter=True)
A:sklearn.decomposition.dict_learning.self.random_state_->check_random_state(self.random_state)
A:sklearn.decomposition.dict_learning.inner_stats->getattr(self, 'inner_stats_', None)
A:sklearn.decomposition.dict_learning.iter_offset->getattr(self, 'iter_offset_', 0)
A:sklearn.decomposition.dict_learning.(U, (A, B))->dict_learning_online(X, self.n_components, self.alpha, n_iter=self.n_iter, method=self.fit_algorithm, n_jobs=self.n_jobs, dict_init=dict_init, batch_size=len(X), shuffle=False, verbose=self.verbose, return_code=False, iter_offset=iter_offset, random_state=self.random_state_, return_inner_stats=True, inner_stats=inner_stats)
sklearn.decomposition.DictionaryLearning(self,n_components=None,alpha=1,max_iter=1000,tol=1e-08,fit_algorithm='lars',transform_algorithm='omp',transform_n_nonzero_coefs=None,transform_alpha=None,n_jobs=1,code_init=None,dict_init=None,verbose=False,split_sign=False,random_state=None)
sklearn.decomposition.DictionaryLearning.fit(self,X,y=None)
sklearn.decomposition.MiniBatchDictionaryLearning(self,n_components=None,alpha=1,n_iter=1000,fit_algorithm='lars',n_jobs=1,batch_size=3,shuffle=True,dict_init=None,transform_algorithm='omp',transform_n_nonzero_coefs=None,transform_alpha=None,verbose=False,split_sign=False,random_state=None)
sklearn.decomposition.MiniBatchDictionaryLearning.fit(self,X,y=None)
sklearn.decomposition.MiniBatchDictionaryLearning.partial_fit(self,X,y=None,iter_offset=None)
sklearn.decomposition.SparseCoder(self,dictionary,transform_algorithm='omp',transform_n_nonzero_coefs=None,transform_alpha=None,split_sign=False,n_jobs=1)
sklearn.decomposition.SparseCoder.fit(self,X,y=None)
sklearn.decomposition.dict_learning(X,n_components,alpha,max_iter=100,tol=1e-08,method='lars',n_jobs=1,dict_init=None,code_init=None,callback=None,verbose=False,random_state=None,return_n_iter=False)
sklearn.decomposition.dict_learning.DictionaryLearning(self,n_components=None,alpha=1,max_iter=1000,tol=1e-08,fit_algorithm='lars',transform_algorithm='omp',transform_n_nonzero_coefs=None,transform_alpha=None,n_jobs=1,code_init=None,dict_init=None,verbose=False,split_sign=False,random_state=None)
sklearn.decomposition.dict_learning.DictionaryLearning.__init__(self,n_components=None,alpha=1,max_iter=1000,tol=1e-08,fit_algorithm='lars',transform_algorithm='omp',transform_n_nonzero_coefs=None,transform_alpha=None,n_jobs=1,code_init=None,dict_init=None,verbose=False,split_sign=False,random_state=None)
sklearn.decomposition.dict_learning.DictionaryLearning.fit(self,X,y=None)
sklearn.decomposition.dict_learning.MiniBatchDictionaryLearning(self,n_components=None,alpha=1,n_iter=1000,fit_algorithm='lars',n_jobs=1,batch_size=3,shuffle=True,dict_init=None,transform_algorithm='omp',transform_n_nonzero_coefs=None,transform_alpha=None,verbose=False,split_sign=False,random_state=None)
sklearn.decomposition.dict_learning.MiniBatchDictionaryLearning.__init__(self,n_components=None,alpha=1,n_iter=1000,fit_algorithm='lars',n_jobs=1,batch_size=3,shuffle=True,dict_init=None,transform_algorithm='omp',transform_n_nonzero_coefs=None,transform_alpha=None,verbose=False,split_sign=False,random_state=None)
sklearn.decomposition.dict_learning.MiniBatchDictionaryLearning.fit(self,X,y=None)
sklearn.decomposition.dict_learning.MiniBatchDictionaryLearning.partial_fit(self,X,y=None,iter_offset=None)
sklearn.decomposition.dict_learning.SparseCoder(self,dictionary,transform_algorithm='omp',transform_n_nonzero_coefs=None,transform_alpha=None,split_sign=False,n_jobs=1)
sklearn.decomposition.dict_learning.SparseCoder.__init__(self,dictionary,transform_algorithm='omp',transform_n_nonzero_coefs=None,transform_alpha=None,split_sign=False,n_jobs=1)
sklearn.decomposition.dict_learning.SparseCoder.fit(self,X,y=None)
sklearn.decomposition.dict_learning.SparseCodingMixin(TransformerMixin)
sklearn.decomposition.dict_learning.SparseCodingMixin._set_sparse_coding_params(self,n_components,transform_algorithm='omp',transform_n_nonzero_coefs=None,transform_alpha=None,split_sign=False,n_jobs=1)
sklearn.decomposition.dict_learning.SparseCodingMixin.transform(self,X)
sklearn.decomposition.dict_learning._sparse_encode(X,dictionary,gram,cov=None,algorithm='lasso_lars',regularization=None,copy_cov=True,init=None,max_iter=1000,check_input=True,verbose=0)
sklearn.decomposition.dict_learning._update_dict(dictionary,Y,code,verbose=False,return_r2=False,random_state=None)
sklearn.decomposition.dict_learning.dict_learning(X,n_components,alpha,max_iter=100,tol=1e-08,method='lars',n_jobs=1,dict_init=None,code_init=None,callback=None,verbose=False,random_state=None,return_n_iter=False)
sklearn.decomposition.dict_learning.dict_learning_online(X,n_components=2,alpha=1,n_iter=100,return_code=True,dict_init=None,callback=None,batch_size=3,verbose=False,shuffle=True,n_jobs=1,method='lars',iter_offset=0,random_state=None,return_inner_stats=False,inner_stats=None,return_n_iter=False)
sklearn.decomposition.dict_learning.sparse_encode(X,dictionary,gram=None,cov=None,algorithm='lasso_lars',n_nonzero_coefs=None,alpha=None,copy_cov=True,init=None,max_iter=1000,n_jobs=1,check_input=True,verbose=0)
sklearn.decomposition.dict_learning_online(X,n_components=2,alpha=1,n_iter=100,return_code=True,dict_init=None,callback=None,batch_size=3,verbose=False,shuffle=True,n_jobs=1,method='lars',iter_offset=0,random_state=None,return_inner_stats=False,inner_stats=None,return_n_iter=False)
sklearn.decomposition.sparse_encode(X,dictionary,gram=None,cov=None,algorithm='lasso_lars',n_nonzero_coefs=None,alpha=None,copy_cov=True,init=None,max_iter=1000,n_jobs=1,check_input=True,verbose=0)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/decomposition/kernel_pca.py----------------------------------------
A:sklearn.decomposition.kernel_pca.self._centerer->KernelCenterer()
A:sklearn.decomposition.kernel_pca.K->self._get_kernel(X, self.X_transformed_fit_)
A:sklearn.decomposition.kernel_pca.n_components->min(K.shape[0], self.n_components)
A:sklearn.decomposition.kernel_pca.(self.lambdas_, self.alphas_)->eigsh(K, n_components, which='LA', tol=self.tol, maxiter=self.max_iter, v0=v0)
A:sklearn.decomposition.kernel_pca.random_state->check_random_state(self.random_state)
A:sklearn.decomposition.kernel_pca.v0->check_random_state(self.random_state).uniform(-1, 1, K.shape[0])
A:sklearn.decomposition.kernel_pca.self.dual_coef_->scipy.linalg.solve(K, X, sym_pos=True, overwrite_a=True)
A:sklearn.decomposition.kernel_pca.X->check_array(X, accept_sparse='csr', copy=self.copy_X)
A:sklearn.decomposition.kernel_pca.sqrt_lambdas->numpy.diag(np.sqrt(self.lambdas_))
A:sklearn.decomposition.kernel_pca.X_transformed->numpy.dot(self.alphas_, sqrt_lambdas)
sklearn.decomposition.KernelPCA(self,n_components=None,kernel='linear',gamma=None,degree=3,coef0=1,kernel_params=None,alpha=1.0,fit_inverse_transform=False,eigen_solver='auto',tol=0,max_iter=None,remove_zero_eig=False,random_state=None,copy_X=True,n_jobs=1)
sklearn.decomposition.KernelPCA._fit_inverse_transform(self,X_transformed,X)
sklearn.decomposition.KernelPCA._fit_transform(self,K)
sklearn.decomposition.KernelPCA._get_kernel(self,X,Y=None)
sklearn.decomposition.KernelPCA._pairwise(self)
sklearn.decomposition.KernelPCA.fit(self,X,y=None)
sklearn.decomposition.KernelPCA.fit_transform(self,X,y=None,**params)
sklearn.decomposition.KernelPCA.inverse_transform(self,X)
sklearn.decomposition.KernelPCA.transform(self,X)
sklearn.decomposition.kernel_pca.KernelPCA(self,n_components=None,kernel='linear',gamma=None,degree=3,coef0=1,kernel_params=None,alpha=1.0,fit_inverse_transform=False,eigen_solver='auto',tol=0,max_iter=None,remove_zero_eig=False,random_state=None,copy_X=True,n_jobs=1)
sklearn.decomposition.kernel_pca.KernelPCA.__init__(self,n_components=None,kernel='linear',gamma=None,degree=3,coef0=1,kernel_params=None,alpha=1.0,fit_inverse_transform=False,eigen_solver='auto',tol=0,max_iter=None,remove_zero_eig=False,random_state=None,copy_X=True,n_jobs=1)
sklearn.decomposition.kernel_pca.KernelPCA._fit_inverse_transform(self,X_transformed,X)
sklearn.decomposition.kernel_pca.KernelPCA._fit_transform(self,K)
sklearn.decomposition.kernel_pca.KernelPCA._get_kernel(self,X,Y=None)
sklearn.decomposition.kernel_pca.KernelPCA._pairwise(self)
sklearn.decomposition.kernel_pca.KernelPCA.fit(self,X,y=None)
sklearn.decomposition.kernel_pca.KernelPCA.fit_transform(self,X,y=None,**params)
sklearn.decomposition.kernel_pca.KernelPCA.inverse_transform(self,X)
sklearn.decomposition.kernel_pca.KernelPCA.transform(self,X)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/decomposition/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/decomposition/tests/test_pca.py----------------------------------------
A:sklearn.decomposition.tests.test_pca.iris->sklearn.datasets.load_iris()
A:sklearn.decomposition.tests.test_pca.pca->PCA(n_components=3, svd_solver='bad_argument')
A:sklearn.decomposition.tests.test_pca.X_r->PCA(n_components=3, svd_solver='bad_argument').transform(X)
A:sklearn.decomposition.tests.test_pca.X_r2->PCA(n_components=3, svd_solver='bad_argument').fit_transform(X)
A:sklearn.decomposition.tests.test_pca.cov->PCA(n_components=3, svd_solver='bad_argument').get_covariance()
A:sklearn.decomposition.tests.test_pca.precision->PCA(n_components=3, svd_solver='bad_argument').get_precision()
A:sklearn.decomposition.tests.test_pca.X->numpy.random.RandomState(0).rand(5, 4)
A:sklearn.decomposition.tests.test_pca.rng->numpy.random.RandomState(0)
A:sklearn.decomposition.tests.test_pca.X_->numpy.random.RandomState(0).rand(5, 4).copy()
A:sklearn.decomposition.tests.test_pca.X_whitened->PCA(n_components=3, svd_solver='bad_argument').fit_transform(X_.copy())
A:sklearn.decomposition.tests.test_pca.X_whitened2->PCA(n_components=3, svd_solver='bad_argument').transform(X_)
A:sklearn.decomposition.tests.test_pca.X_unwhitened->PCA(n_components=3, svd_solver='bad_argument').transform(X_)
A:sklearn.decomposition.tests.test_pca.apca->PCA(n_components=3, svd_solver='arpack', random_state=rng)
A:sklearn.decomposition.tests.test_pca.rpca->RandomizedPCA(random_state=0)
A:sklearn.decomposition.tests.test_pca.X_pca->PCA(n_components=3, svd_solver='bad_argument').fit_transform(X)
A:sklearn.decomposition.tests.test_pca.X_rpca->RandomizedPCA(random_state=0).transform(X)
A:sklearn.decomposition.tests.test_pca.X_apca->PCA(n_components=3, svd_solver='arpack', random_state=rng).transform(X)
A:sklearn.decomposition.tests.test_pca.X_hat->numpy.dot(X_pca, pca.components_)
A:sklearn.decomposition.tests.test_pca.Yt->PCA(n_components=2, svd_solver='randomized', random_state=0).fit(X).transform(Xt)
A:sklearn.decomposition.tests.test_pca.Y->RandomizedPCA(random_state=0).fit_transform(X)
A:sklearn.decomposition.tests.test_pca.Y_inverse->PCA(n_components=3, svd_solver='bad_argument').inverse_transform(Y)
A:sklearn.decomposition.tests.test_pca.X_transformed->PCA(n_components=1, svd_solver='randomized', random_state=0).fit(X).transform(X)
A:sklearn.decomposition.tests.test_pca.relative_max_delta->(np.abs(X - Y_inverse) / np.abs(X).mean()).max()
A:sklearn.decomposition.tests.test_pca.ll->numpy.zeros(p)
A:sklearn.decomposition.tests.test_pca.ll1->PCA(n_components=3, svd_solver='bad_argument').score(X)
A:sklearn.decomposition.tests.test_pca.ll2->PCA(n_components=3, svd_solver='bad_argument').score(X)
A:sklearn.decomposition.tests.test_pca.ll[k]->PCA(n_components=3, svd_solver='bad_argument').score(Xt)
A:sklearn.decomposition.tests.test_pca.digits->sklearn.datasets.load_digits()
A:sklearn.decomposition.tests.test_pca.pca_test->PCA(n_components=10, svd_solver='randomized', random_state=0)
A:sklearn.decomposition.tests.test_pca.Y_pca->PCA(svd_solver='randomized', random_state=0).fit_transform(X)
A:sklearn.decomposition.tests.test_pca.X_64->numpy.random.RandomState(0).rand(1000, 4).astype(np.float64)
A:sklearn.decomposition.tests.test_pca.X_32->numpy.random.RandomState(0).rand(1000, 4).astype(np.float64).astype(np.float32)
A:sklearn.decomposition.tests.test_pca.pca_64->PCA(n_components=3, svd_solver=svd_solver, random_state=0).fit(X_i64)
A:sklearn.decomposition.tests.test_pca.pca_32->PCA(n_components=3, svd_solver=svd_solver, random_state=0).fit(X_i32)
A:sklearn.decomposition.tests.test_pca.X_i64->X_i64.astype(np.int64).astype(np.int64)
A:sklearn.decomposition.tests.test_pca.X_i32->X_i64.astype(np.int64).astype(np.int64).astype(np.int32)
sklearn.decomposition.tests.test_pca.check_pca_float_dtype_preservation(svd_solver)
sklearn.decomposition.tests.test_pca.check_pca_int_dtype_upcast_to_double(svd_solver)
sklearn.decomposition.tests.test_pca.test_deprecation_randomized_pca()
sklearn.decomposition.tests.test_pca.test_explained_variance()
sklearn.decomposition.tests.test_pca.test_infer_dim_1()
sklearn.decomposition.tests.test_pca.test_infer_dim_2()
sklearn.decomposition.tests.test_pca.test_infer_dim_3()
sklearn.decomposition.tests.test_pca.test_infer_dim_by_explained_variance()
sklearn.decomposition.tests.test_pca.test_no_empty_slice_warning()
sklearn.decomposition.tests.test_pca.test_pca()
sklearn.decomposition.tests.test_pca.test_pca_arpack_solver()
sklearn.decomposition.tests.test_pca.test_pca_bad_solver()
sklearn.decomposition.tests.test_pca.test_pca_check_projection()
sklearn.decomposition.tests.test_pca.test_pca_dim()
sklearn.decomposition.tests.test_pca.test_pca_dtype_preservation()
sklearn.decomposition.tests.test_pca.test_pca_inverse()
sklearn.decomposition.tests.test_pca.test_pca_randomized_solver()
sklearn.decomposition.tests.test_pca.test_pca_score()
sklearn.decomposition.tests.test_pca.test_pca_score2()
sklearn.decomposition.tests.test_pca.test_pca_score3()
sklearn.decomposition.tests.test_pca.test_pca_score_with_different_solvers()
sklearn.decomposition.tests.test_pca.test_pca_sparse_input()
sklearn.decomposition.tests.test_pca.test_pca_validation()
sklearn.decomposition.tests.test_pca.test_pca_zero_noise_variance_edge_cases()
sklearn.decomposition.tests.test_pca.test_randomized_pca_check_list()
sklearn.decomposition.tests.test_pca.test_randomized_pca_check_projection()
sklearn.decomposition.tests.test_pca.test_randomized_pca_inverse()
sklearn.decomposition.tests.test_pca.test_singular_values()
sklearn.decomposition.tests.test_pca.test_svd_solver_auto()
sklearn.decomposition.tests.test_pca.test_whitening()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/decomposition/tests/test_fastica.py----------------------------------------
A:sklearn.decomposition.tests.test_fastica.x->numpy.rollaxis(x, axis)
A:sklearn.decomposition.tests.test_fastica.rng->numpy.random.RandomState(0)
A:sklearn.decomposition.tests.test_fastica.(W, _, _)->numpy.linalg.svd(rng.randn(10, 10))
A:sklearn.decomposition.tests.test_fastica.w->numpy.random.RandomState(0).randn(10)
A:sklearn.decomposition.tests.test_fastica.u->_gs_decorrelation(w, W, 5)
A:sklearn.decomposition.tests.test_fastica.tmp->numpy.dot(u, W.T)
A:sklearn.decomposition.tests.test_fastica.s2->numpy.ceil(np.sin(np.pi * t))
A:sklearn.decomposition.tests.test_fastica.mixing->numpy.random.RandomState(0).randn(6, 2)
A:sklearn.decomposition.tests.test_fastica.m->numpy.dot(mixing, s)
A:sklearn.decomposition.tests.test_fastica.(k_, mixing_, s_)->fastica(m.T, n_components=2, random_state=rng)
A:sklearn.decomposition.tests.test_fastica.X->numpy.random.RandomState(0).random_sample((n_samples, n_features))
A:sklearn.decomposition.tests.test_fastica.(_, _, sources_fun)->fastica(m.T, fun=nl, algorithm=algo, random_state=0)
A:sklearn.decomposition.tests.test_fastica.ica->FastICA(n_components=n_components, random_state=rng, whiten=whiten)
A:sklearn.decomposition.tests.test_fastica.sources->FastICA(n_components=n_components, random_state=rng, whiten=whiten).fit_transform(m.T)
A:sklearn.decomposition.tests.test_fastica.t->numpy.linspace(0, 100, n_samples)
A:sklearn.decomposition.tests.test_fastica.s1->numpy.sin(t)
A:sklearn.decomposition.tests.test_fastica.Xt->FastICA(n_components=n_components, random_state=rng, whiten=whiten).fit_transform(X)
A:sklearn.decomposition.tests.test_fastica.Xt2->FastICA(n_components=n_components, random_state=rng, whiten=whiten).transform(X)
A:sklearn.decomposition.tests.test_fastica.X2->FastICA(n_components=n_components, random_state=rng, whiten=whiten).inverse_transform(Xt)
sklearn.decomposition.tests.test_fastica.center_and_norm(x,axis=-1)
sklearn.decomposition.tests.test_fastica.test_fastica_nowhiten()
sklearn.decomposition.tests.test_fastica.test_fastica_simple(add_noise=False)
sklearn.decomposition.tests.test_fastica.test_fit_transform()
sklearn.decomposition.tests.test_fastica.test_gs()
sklearn.decomposition.tests.test_fastica.test_inverse_transform()
sklearn.decomposition.tests.test_fastica.test_non_square_fastica(add_noise=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/decomposition/tests/test_truncated_svd.py----------------------------------------
A:sklearn.decomposition.tests.test_truncated_svd.rng->numpy.random.RandomState(0)
A:sklearn.decomposition.tests.test_truncated_svd.X->numpy.random.RandomState(0).randn(n_samples, n_features)
A:sklearn.decomposition.tests.test_truncated_svd.svd_a->TruncatedSVD(30, algorithm='arpack')
A:sklearn.decomposition.tests.test_truncated_svd.svd_r->TruncatedSVD(30, algorithm='randomized', random_state=42)
A:sklearn.decomposition.tests.test_truncated_svd.comp_a->numpy.abs(svd_a.components_)
A:sklearn.decomposition.tests.test_truncated_svd.comp_r->numpy.abs(svd_r.components_)
A:sklearn.decomposition.tests.test_truncated_svd.tsvd->TruncatedSVD(n_components=6)
A:sklearn.decomposition.tests.test_truncated_svd.Xtrans->TruncatedSVD(n_components=6).fit_transform(Xint)
A:sklearn.decomposition.tests.test_truncated_svd.Xt->TruncatedSVD(n_components=6).fit_transform(X)
A:sklearn.decomposition.tests.test_truncated_svd.Xinv->TruncatedSVD(n_components=6).inverse_transform(Xt)
A:sklearn.decomposition.tests.test_truncated_svd.Xint->numpy.random.RandomState(0).randn(n_samples, n_features).astype(np.int64)
A:sklearn.decomposition.tests.test_truncated_svd.svd_a_10_sp->TruncatedSVD(10, algorithm='arpack')
A:sklearn.decomposition.tests.test_truncated_svd.svd_r_10_sp->TruncatedSVD(10, algorithm='randomized', random_state=42)
A:sklearn.decomposition.tests.test_truncated_svd.svd_a_20_sp->TruncatedSVD(20, algorithm='arpack')
A:sklearn.decomposition.tests.test_truncated_svd.svd_r_20_sp->TruncatedSVD(20, algorithm='randomized', random_state=42)
A:sklearn.decomposition.tests.test_truncated_svd.X_trans_a_10_sp->TruncatedSVD(10, algorithm='arpack').fit_transform(X)
A:sklearn.decomposition.tests.test_truncated_svd.X_trans_r_10_sp->TruncatedSVD(10, algorithm='randomized', random_state=42).fit_transform(X)
A:sklearn.decomposition.tests.test_truncated_svd.X_trans_a_20_sp->TruncatedSVD(20, algorithm='arpack').fit_transform(X)
A:sklearn.decomposition.tests.test_truncated_svd.X_trans_r_20_sp->TruncatedSVD(20, algorithm='randomized', random_state=42).fit_transform(X)
A:sklearn.decomposition.tests.test_truncated_svd.svd_a_10_de->TruncatedSVD(10, algorithm='arpack')
A:sklearn.decomposition.tests.test_truncated_svd.svd_r_10_de->TruncatedSVD(10, algorithm='randomized', random_state=42)
A:sklearn.decomposition.tests.test_truncated_svd.svd_a_20_de->TruncatedSVD(20, algorithm='arpack')
A:sklearn.decomposition.tests.test_truncated_svd.svd_r_20_de->TruncatedSVD(20, algorithm='randomized', random_state=42)
A:sklearn.decomposition.tests.test_truncated_svd.X_trans_a_10_de->TruncatedSVD(10, algorithm='arpack').fit_transform(X.toarray())
A:sklearn.decomposition.tests.test_truncated_svd.X_trans_r_10_de->TruncatedSVD(10, algorithm='randomized', random_state=42).fit_transform(X.toarray())
A:sklearn.decomposition.tests.test_truncated_svd.X_trans_a_20_de->TruncatedSVD(20, algorithm='arpack').fit_transform(X.toarray())
A:sklearn.decomposition.tests.test_truncated_svd.X_trans_r_20_de->TruncatedSVD(20, algorithm='randomized', random_state=42).fit_transform(X.toarray())
A:sklearn.decomposition.tests.test_truncated_svd.total_variance->numpy.var(X.toarray(), axis=0).sum()
A:sklearn.decomposition.tests.test_truncated_svd.variances->numpy.var(transformed, axis=0)
A:sklearn.decomposition.tests.test_truncated_svd.apca->TruncatedSVD(n_components=3, algorithm='arpack', random_state=rng)
A:sklearn.decomposition.tests.test_truncated_svd.rpca->TruncatedSVD(n_components=3, algorithm='randomized', random_state=rng)
A:sklearn.decomposition.tests.test_truncated_svd.X_apca->TruncatedSVD(n_components=3, algorithm='arpack', random_state=rng).fit_transform(X)
A:sklearn.decomposition.tests.test_truncated_svd.X_rpca->TruncatedSVD(n_components=3, algorithm='randomized', random_state=rng).fit_transform(X)
A:sklearn.decomposition.tests.test_truncated_svd.X_hat_apca->numpy.dot(X_apca, apca.components_)
A:sklearn.decomposition.tests.test_truncated_svd.X_hat_rpca->numpy.dot(X_rpca, rpca.components_)
sklearn.decomposition.tests.test_truncated_svd.test_algorithms()
sklearn.decomposition.tests.test_truncated_svd.test_attributes()
sklearn.decomposition.tests.test_truncated_svd.test_explained_variance()
sklearn.decomposition.tests.test_truncated_svd.test_integers()
sklearn.decomposition.tests.test_truncated_svd.test_inverse_transform()
sklearn.decomposition.tests.test_truncated_svd.test_singular_values()
sklearn.decomposition.tests.test_truncated_svd.test_sparse_formats()
sklearn.decomposition.tests.test_truncated_svd.test_too_many_components()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/decomposition/tests/test_sparse_pca.py----------------------------------------
A:sklearn.decomposition.tests.test_sparse_pca.rng->numpy.random.RandomState(0)
A:sklearn.decomposition.tests.test_sparse_pca.U->MiniBatchSparsePCA(n_components=13, random_state=rng).fit_transform(X)
A:sklearn.decomposition.tests.test_sparse_pca.V->numpy.random.RandomState(0).randn(n_components, n_features)
A:sklearn.decomposition.tests.test_sparse_pca.img->numpy.zeros(image_size)
A:sklearn.decomposition.tests.test_sparse_pca.V[k, :]->numpy.zeros(image_size).ravel()
A:sklearn.decomposition.tests.test_sparse_pca.Y->numpy.dot(U, V)
A:sklearn.decomposition.tests.test_sparse_pca.X->numpy.random.RandomState(0).randn(12, 10)
A:sklearn.decomposition.tests.test_sparse_pca.spca->SparsePCA(n_components=3, n_jobs=2, method='lars', alpha=alpha, random_state=0).fit(Y)
A:sklearn.decomposition.tests.test_sparse_pca.(Y, _, _)->generate_toy_data(3, 10, (8, 8), random_state=rng)
A:sklearn.decomposition.tests.test_sparse_pca.spca_lars->MiniBatchSparsePCA(n_components=3, random_state=0, alpha=alpha).fit(Y)
A:sklearn.decomposition.tests.test_sparse_pca.spca_lasso->MiniBatchSparsePCA(n_components=3, method='cd', alpha=alpha, random_state=0).fit(Y)
A:sklearn.decomposition.tests.test_sparse_pca.U1->MiniBatchSparsePCA(n_components=3, random_state=0, alpha=alpha).fit(Y).transform(Y)
A:sklearn.decomposition.tests.test_sparse_pca.U2->MiniBatchSparsePCA(n_components=3, n_jobs=2, alpha=alpha, random_state=0).fit(Y).transform(Y)
A:sklearn.decomposition.tests.test_sparse_pca.estimator->SparsePCA(n_components=8)
A:sklearn.decomposition.tests.test_sparse_pca.U_init->numpy.random.RandomState(0).randn(5, 3)
A:sklearn.decomposition.tests.test_sparse_pca.V_init->numpy.random.RandomState(0).randn(3, 4)
A:sklearn.decomposition.tests.test_sparse_pca.model->SparsePCA(n_components=3, U_init=U_init, V_init=V_init, max_iter=0, random_state=rng)
A:sklearn.decomposition.tests.test_sparse_pca.pca->MiniBatchSparsePCA(n_components=13, random_state=rng)
sklearn.decomposition.tests.test_sparse_pca.generate_toy_data(n_components,n_samples,image_size,random_state=None)
sklearn.decomposition.tests.test_sparse_pca.test_correct_shapes()
sklearn.decomposition.tests.test_sparse_pca.test_fit_transform()
sklearn.decomposition.tests.test_sparse_pca.test_fit_transform_parallel()
sklearn.decomposition.tests.test_sparse_pca.test_fit_transform_tall()
sklearn.decomposition.tests.test_sparse_pca.test_initialization()
sklearn.decomposition.tests.test_sparse_pca.test_mini_batch_correct_shapes()
sklearn.decomposition.tests.test_sparse_pca.test_mini_batch_fit_transform()
sklearn.decomposition.tests.test_sparse_pca.test_transform_nan()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/decomposition/tests/test_dict_learning.py----------------------------------------
A:sklearn.decomposition.tests.test_dict_learning.rng_global->numpy.random.RandomState(0)
A:sklearn.decomposition.tests.test_dict_learning.X->numpy.random.RandomState(0).randn(100, 64)
A:sklearn.decomposition.tests.test_dict_learning.rng->numpy.random.RandomState(0)
A:sklearn.decomposition.tests.test_dict_learning.X_->numpy.random.RandomState(0).randn(n_samples, n_features)
A:sklearn.decomposition.tests.test_dict_learning.dictionary->numpy.random.RandomState(0).randn(n_components, n_features)
A:sklearn.decomposition.tests.test_dict_learning.code->SparseCoder(dictionary=V, transform_algorithm='lasso_lars', transform_alpha=0.001).transform(X)
A:sklearn.decomposition.tests.test_dict_learning.dico->MiniBatchDictionaryLearning(n_components, n_iter=0, dict_init=V, random_state=0).fit(X)
A:sklearn.decomposition.tests.test_dict_learning.split_code->MiniBatchDictionaryLearning(n_components, n_iter=0, dict_init=V, random_state=0).fit(X).transform(X)
A:sklearn.decomposition.tests.test_dict_learning.(code, dictionary)->dict_learning_online(X, n_components=n_components, alpha=1, random_state=rng)
A:sklearn.decomposition.tests.test_dict_learning.sys.stdout->StringIO()
A:sklearn.decomposition.tests.test_dict_learning.V->numpy.random.RandomState(0).randn(n_components, n_features)
A:sklearn.decomposition.tests.test_dict_learning.dict1->MiniBatchDictionaryLearning(n_components, n_iter=10 * len(X), batch_size=1, alpha=1, shuffle=False, dict_init=V, random_state=0).fit(X)
A:sklearn.decomposition.tests.test_dict_learning.dict2->MiniBatchDictionaryLearning(n_components, alpha=1, n_iter=1, dict_init=V, random_state=0)
A:sklearn.decomposition.tests.test_dict_learning.Xf->check_array(X, order='F')
A:sklearn.decomposition.tests.test_dict_learning.a->sparse_encode(X, V, algorithm=algo)
A:sklearn.decomposition.tests.test_dict_learning.b->sparse_encode(Xf, V, algorithm=algo)
A:sklearn.decomposition.tests.test_dict_learning.D->numpy.random.RandomState(0).randn(2, 64)
sklearn.decomposition.tests.test_dict_learning.test_dict_learning_lassocd_readonly_data()
sklearn.decomposition.tests.test_dict_learning.test_dict_learning_nonzero_coefs()
sklearn.decomposition.tests.test_dict_learning.test_dict_learning_online_estimator_shapes()
sklearn.decomposition.tests.test_dict_learning.test_dict_learning_online_initialization()
sklearn.decomposition.tests.test_dict_learning.test_dict_learning_online_overcomplete()
sklearn.decomposition.tests.test_dict_learning.test_dict_learning_online_partial_fit()
sklearn.decomposition.tests.test_dict_learning.test_dict_learning_online_shapes()
sklearn.decomposition.tests.test_dict_learning.test_dict_learning_online_verbosity()
sklearn.decomposition.tests.test_dict_learning.test_dict_learning_overcomplete()
sklearn.decomposition.tests.test_dict_learning.test_dict_learning_reconstruction()
sklearn.decomposition.tests.test_dict_learning.test_dict_learning_reconstruction_parallel()
sklearn.decomposition.tests.test_dict_learning.test_dict_learning_shapes()
sklearn.decomposition.tests.test_dict_learning.test_dict_learning_split()
sklearn.decomposition.tests.test_dict_learning.test_dict_learning_unknown_fit_algorithm()
sklearn.decomposition.tests.test_dict_learning.test_sparse_coder_estimator()
sklearn.decomposition.tests.test_dict_learning.test_sparse_encode_error()
sklearn.decomposition.tests.test_dict_learning.test_sparse_encode_error_default_sparsity()
sklearn.decomposition.tests.test_dict_learning.test_sparse_encode_input()
sklearn.decomposition.tests.test_dict_learning.test_sparse_encode_shapes()
sklearn.decomposition.tests.test_dict_learning.test_sparse_encode_shapes_omp()
sklearn.decomposition.tests.test_dict_learning.test_unknown_method()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/decomposition/tests/test_online_lda.py----------------------------------------
A:sklearn.decomposition.tests.test_online_lda.X->numpy.random.randint(4, size=(n_samples, 10))
A:sklearn.decomposition.tests.test_online_lda.(n_components, X)->_build_sparse_mtx()
A:sklearn.decomposition.tests.test_online_lda.lda_1->LatentDirichletAllocation(n_components=n_components, max_iter=1, learning_method=method, total_samples=100, random_state=0)
A:sklearn.decomposition.tests.test_online_lda.lda_2->LatentDirichletAllocation(n_components=n_components, max_iter=10, learning_method=method, total_samples=100, random_state=0)
A:sklearn.decomposition.tests.test_online_lda.topic_distr_1->LatentDirichletAllocation(n_components=n_components, max_iter=1, learning_method=method, total_samples=100, random_state=0).fit_transform(X)
A:sklearn.decomposition.tests.test_online_lda.topic_distr_2->LatentDirichletAllocation(n_components=n_components, max_iter=10, learning_method=method, total_samples=100, random_state=0).fit_transform(X)
A:sklearn.decomposition.tests.test_online_lda.rng->numpy.random.RandomState(0)
A:sklearn.decomposition.tests.test_online_lda.lda->LatentDirichletAllocation(n_topics=10, learning_method='batch')
A:sklearn.decomposition.tests.test_online_lda.top_idx->set(c.argsort()[-3:][::-1])
A:sklearn.decomposition.tests.test_online_lda.X_trans->LatentDirichletAllocation(n_topics=10, learning_method='batch').transform(X)
A:sklearn.decomposition.tests.test_online_lda.X_fit->LatentDirichletAllocation(n_topics=10, learning_method='batch').fit_transform(X)
A:sklearn.decomposition.tests.test_online_lda.n_components->numpy.random.RandomState(0).randint(3, 6)
A:sklearn.decomposition.tests.test_online_lda.n_col->numpy.random.RandomState(0).randint(6, 10)
A:sklearn.decomposition.tests.test_online_lda.X_1->numpy.random.randint(4, size=(10, n_col))
A:sklearn.decomposition.tests.test_online_lda.X_2->numpy.random.RandomState(0).randint(4, size=(10, 8))
A:sklearn.decomposition.tests.test_online_lda.n_samples->numpy.random.RandomState(0).randint(6, 10)
A:sklearn.decomposition.tests.test_online_lda.invalid_n_samples->numpy.random.RandomState(0).randint(4, size=(n_samples + 1, n_components))
A:sklearn.decomposition.tests.test_online_lda.invalid_n_components->numpy.random.RandomState(0).randint(4, size=(n_samples, n_components + 1))
A:sklearn.decomposition.tests.test_online_lda.perp_1->LatentDirichletAllocation(n_topics=10, learning_method='batch').perplexity(X)
A:sklearn.decomposition.tests.test_online_lda.perp_2->LatentDirichletAllocation(n_topics=10, learning_method='batch').perplexity(X.toarray())
A:sklearn.decomposition.tests.test_online_lda.perp_1_subsampling->LatentDirichletAllocation(n_components=n_components, max_iter=1, learning_method=method, total_samples=100, random_state=0).perplexity(X, sub_sampling=True)
A:sklearn.decomposition.tests.test_online_lda.perp_2_subsampling->LatentDirichletAllocation(n_components=n_components, max_iter=10, learning_method=method, total_samples=100, random_state=0).perplexity(X, sub_sampling=True)
A:sklearn.decomposition.tests.test_online_lda.score_1->LatentDirichletAllocation(n_components=n_components, max_iter=1, learning_method=method, total_samples=100, random_state=0).score(X)
A:sklearn.decomposition.tests.test_online_lda.score_2->LatentDirichletAllocation(n_components=n_components, max_iter=10, learning_method=method, total_samples=100, random_state=0).score(X)
A:sklearn.decomposition.tests.test_online_lda.perplexity_1->LatentDirichletAllocation(n_topics=10, learning_method='batch').perplexity(X, sub_sampling=False)
A:sklearn.decomposition.tests.test_online_lda.score->LatentDirichletAllocation(n_topics=10, learning_method='batch').score(X)
A:sklearn.decomposition.tests.test_online_lda.perplexity_2->numpy.exp(-1.0 * (score / np.sum(X.data)))
A:sklearn.decomposition.tests.test_online_lda.perplexity2->LatentDirichletAllocation(n_topics=10, learning_method='batch').perplexity(X)
A:sklearn.decomposition.tests.test_online_lda.distr1->LatentDirichletAllocation(n_topics=10, learning_method='batch').fit_transform(X)
A:sklearn.decomposition.tests.test_online_lda.Z->numpy.zeros((5, 4))
A:sklearn.decomposition.tests.test_online_lda.x->x.reshape(100, 100).reshape(100, 100)
A:sklearn.decomposition.tests.test_online_lda.expectation->numpy.empty_like(x)
A:sklearn.decomposition.tests.test_online_lda.out->StringIO()
A:sklearn.decomposition.tests.test_online_lda.n_lines->StringIO().getvalue().count('\n')
A:sklearn.decomposition.tests.test_online_lda.n_perplexity->StringIO().getvalue().count('perplexity')
sklearn.decomposition.tests.test_online_lda._build_sparse_mtx()
sklearn.decomposition.tests.test_online_lda.check_verbosity(verbose,evaluate_every,expected_lines,expected_perplexities)
sklearn.decomposition.tests.test_online_lda.test_dirichlet_expectation()
sklearn.decomposition.tests.test_online_lda.test_doc_topic_distr_deprecation()
sklearn.decomposition.tests.test_online_lda.test_invalid_params()
sklearn.decomposition.tests.test_online_lda.test_lda_default_prior_params()
sklearn.decomposition.tests.test_online_lda.test_lda_dense_input()
sklearn.decomposition.tests.test_online_lda.test_lda_empty_docs()
sklearn.decomposition.tests.test_online_lda.test_lda_fit_batch()
sklearn.decomposition.tests.test_online_lda.test_lda_fit_online()
sklearn.decomposition.tests.test_online_lda.test_lda_fit_perplexity()
sklearn.decomposition.tests.test_online_lda.test_lda_fit_transform()
sklearn.decomposition.tests.test_online_lda.test_lda_multi_jobs()
sklearn.decomposition.tests.test_online_lda.test_lda_n_topics_deprecation()
sklearn.decomposition.tests.test_online_lda.test_lda_negative_input()
sklearn.decomposition.tests.test_online_lda.test_lda_no_component_error()
sklearn.decomposition.tests.test_online_lda.test_lda_partial_fit()
sklearn.decomposition.tests.test_online_lda.test_lda_partial_fit_dim_mismatch()
sklearn.decomposition.tests.test_online_lda.test_lda_partial_fit_multi_jobs()
sklearn.decomposition.tests.test_online_lda.test_lda_perplexity()
sklearn.decomposition.tests.test_online_lda.test_lda_preplexity_mismatch()
sklearn.decomposition.tests.test_online_lda.test_lda_score()
sklearn.decomposition.tests.test_online_lda.test_lda_score_perplexity()
sklearn.decomposition.tests.test_online_lda.test_lda_transform()
sklearn.decomposition.tests.test_online_lda.test_lda_transform_mismatch()
sklearn.decomposition.tests.test_online_lda.test_perplexity_input_format()
sklearn.decomposition.tests.test_online_lda.test_verbosity()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/decomposition/tests/test_factor_analysis.py----------------------------------------
A:sklearn.decomposition.tests.test_factor_analysis.rng->numpy.random.RandomState(0)
A:sklearn.decomposition.tests.test_factor_analysis.W->numpy.random.RandomState(0).randn(n_components, n_features)
A:sklearn.decomposition.tests.test_factor_analysis.h->numpy.random.RandomState(0).randn(n_samples, n_components)
A:sklearn.decomposition.tests.test_factor_analysis.fa_fail->FactorAnalysis()
A:sklearn.decomposition.tests.test_factor_analysis.fa->FactorAnalysis(n_components=n_components, noise_variance_init=np.ones(n_features))
A:sklearn.decomposition.tests.test_factor_analysis.X_t->FactorAnalysis(n_components=n_components, noise_variance_init=np.ones(n_features)).transform(X)
A:sklearn.decomposition.tests.test_factor_analysis.diff->numpy.all(np.diff(fa.loglike_))
A:sklearn.decomposition.tests.test_factor_analysis.scov->numpy.cov(X, rowvar=0.0, bias=1.0)
A:sklearn.decomposition.tests.test_factor_analysis.mcov->FactorAnalysis(n_components=n_components, noise_variance_init=np.ones(n_features)).get_covariance()
A:sklearn.decomposition.tests.test_factor_analysis.cov->FactorAnalysis(n_components=n_components, noise_variance_init=np.ones(n_features)).get_covariance()
A:sklearn.decomposition.tests.test_factor_analysis.precision->FactorAnalysis(n_components=n_components, noise_variance_init=np.ones(n_features)).get_precision()
sklearn.decomposition.tests.test_factor_analysis.test_factor_analysis()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/decomposition/tests/test_incremental_pca.py----------------------------------------
A:sklearn.decomposition.tests.test_incremental_pca.iris->sklearn.datasets.load_iris()
A:sklearn.decomposition.tests.test_incremental_pca.ipca->IncrementalPCA(whiten=True, n_components=nc, batch_size=250).fit(X)
A:sklearn.decomposition.tests.test_incremental_pca.pca->PCA(whiten=True, n_components=nc).fit(X)
A:sklearn.decomposition.tests.test_incremental_pca.X_transformed->IncrementalPCA(whiten=True, n_components=nc, batch_size=250).fit(X).fit_transform(X)
A:sklearn.decomposition.tests.test_incremental_pca.cov->IncrementalPCA(whiten=True, n_components=nc, batch_size=250).fit(X).get_covariance()
A:sklearn.decomposition.tests.test_incremental_pca.precision->IncrementalPCA(whiten=True, n_components=nc, batch_size=250).fit(X).get_precision()
A:sklearn.decomposition.tests.test_incremental_pca.rng->numpy.random.RandomState(0)
A:sklearn.decomposition.tests.test_incremental_pca.Yt->IncrementalPCA(n_components=2).fit(X).transform(Xt)
A:sklearn.decomposition.tests.test_incremental_pca.X->sklearn.datasets.make_low_rank_matrix(1000, 10, tail_strength=0.0, effective_rank=2, random_state=1999)
A:sklearn.decomposition.tests.test_incremental_pca.Y->IncrementalPCA(whiten=True, n_components=nc, batch_size=250).fit(X).transform(X)
A:sklearn.decomposition.tests.test_incremental_pca.Y_inverse->IncrementalPCA(whiten=True, n_components=nc, batch_size=250).fit(X).inverse_transform(Y)
A:sklearn.decomposition.tests.test_incremental_pca.X2->numpy.random.RandomState(0).randn(n_samples, 50)
A:sklearn.decomposition.tests.test_incremental_pca.X3->numpy.random.RandomState(0).randn(n_samples, n_features)
A:sklearn.decomposition.tests.test_incremental_pca.batch_sizes->numpy.arange(20, 40, 3)
A:sklearn.decomposition.tests.test_incremental_pca.pipca->IncrementalPCA(n_components=2, batch_size=batch_size)
A:sklearn.decomposition.tests.test_incremental_pca.batch_itr->numpy.arange(0, n + 1, batch_size)
A:sklearn.decomposition.tests.test_incremental_pca.Y_pca->PCA(n_components=3).fit_transform(X)
A:sklearn.decomposition.tests.test_incremental_pca.Y_ipca->IncrementalPCA(n_components=3, batch_size=25).fit_transform(X)
A:sklearn.decomposition.tests.test_incremental_pca.X_pca->PCA(whiten=True, n_components=nc).fit(X).fit_transform(X)
A:sklearn.decomposition.tests.test_incremental_pca.X_ipca->IncrementalPCA(whiten=True, n_components=nc, batch_size=250).fit(X).transform(X)
A:sklearn.decomposition.tests.test_incremental_pca.X_hat->numpy.dot(X_pca, pca.components_)
A:sklearn.decomposition.tests.test_incremental_pca.Xt_pca->PCA(whiten=True, n_components=nc).fit(X).transform(X)
A:sklearn.decomposition.tests.test_incremental_pca.Xt_ipca->IncrementalPCA(whiten=True, n_components=nc, batch_size=250).fit(X).transform(X)
A:sklearn.decomposition.tests.test_incremental_pca.Xinv_ipca->IncrementalPCA(whiten=True, n_components=nc, batch_size=250).fit(X).inverse_transform(Xt_ipca)
A:sklearn.decomposition.tests.test_incremental_pca.Xinv_pca->PCA(whiten=True, n_components=nc).fit(X).inverse_transform(Xt_pca)
sklearn.decomposition.tests.test_incremental_pca.test_explained_variances()
sklearn.decomposition.tests.test_incremental_pca.test_incremental_pca()
sklearn.decomposition.tests.test_incremental_pca.test_incremental_pca_against_pca_iris()
sklearn.decomposition.tests.test_incremental_pca.test_incremental_pca_against_pca_random_data()
sklearn.decomposition.tests.test_incremental_pca.test_incremental_pca_batch_signs()
sklearn.decomposition.tests.test_incremental_pca.test_incremental_pca_batch_values()
sklearn.decomposition.tests.test_incremental_pca.test_incremental_pca_check_projection()
sklearn.decomposition.tests.test_incremental_pca.test_incremental_pca_inverse()
sklearn.decomposition.tests.test_incremental_pca.test_incremental_pca_num_features_change()
sklearn.decomposition.tests.test_incremental_pca.test_incremental_pca_partial_fit()
sklearn.decomposition.tests.test_incremental_pca.test_incremental_pca_set_params()
sklearn.decomposition.tests.test_incremental_pca.test_incremental_pca_validation()
sklearn.decomposition.tests.test_incremental_pca.test_singular_values()
sklearn.decomposition.tests.test_incremental_pca.test_whitening()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/decomposition/tests/test_kernel_pca.py----------------------------------------
A:sklearn.decomposition.tests.test_kernel_pca.rng->numpy.random.RandomState(0)
A:sklearn.decomposition.tests.test_kernel_pca.X_fit->numpy.random.RandomState(0).random_sample((2, 4))
A:sklearn.decomposition.tests.test_kernel_pca.X_pred->numpy.random.RandomState(0).random_sample((2, 4))
A:sklearn.decomposition.tests.test_kernel_pca.kpca->KernelPCA(kernel='rbf', n_components=2, fit_inverse_transform=True, gamma=2.0)
A:sklearn.decomposition.tests.test_kernel_pca.X_fit_transformed->KernelPCA(kernel='rbf', n_components=2, fit_inverse_transform=True, gamma=2.0).fit_transform(X_fit)
A:sklearn.decomposition.tests.test_kernel_pca.X_fit_transformed2->KernelPCA(kernel='rbf', n_components=2, fit_inverse_transform=True, gamma=2.0).fit(X_fit).transform(X_fit)
A:sklearn.decomposition.tests.test_kernel_pca.X_pred_transformed->KernelPCA(kernel='rbf', n_components=2, fit_inverse_transform=True, gamma=2.0).transform(X_pred)
A:sklearn.decomposition.tests.test_kernel_pca.X_pred2->KernelPCA(kernel='rbf', n_components=2, fit_inverse_transform=True, gamma=2.0).inverse_transform(X_pred_transformed)
A:sklearn.decomposition.tests.test_kernel_pca.state->numpy.random.RandomState(0)
A:sklearn.decomposition.tests.test_kernel_pca.X->numpy.array([[1 - 1e-30, 1], [1, 1], [1, 1 - 1e-20]])
A:sklearn.decomposition.tests.test_kernel_pca.transformed1->KernelPCA(kernel='rbf', n_components=2, fit_inverse_transform=True, gamma=2.0).transform(X)
A:sklearn.decomposition.tests.test_kernel_pca.X_copy->numpy.array([[1 - 1e-30, 1], [1, 1], [1, 1 - 1e-20]]).copy()
A:sklearn.decomposition.tests.test_kernel_pca.transformed2->KernelPCA(kernel='rbf', n_components=2, fit_inverse_transform=True, gamma=2.0).transform(X_copy)
A:sklearn.decomposition.tests.test_kernel_pca.Xt->KernelPCA(kernel='rbf', n_components=2, fit_inverse_transform=True, gamma=2.0).fit_transform(X)
A:sklearn.decomposition.tests.test_kernel_pca.X_kpca->KernelPCA(kernel='rbf', n_components=2, fit_inverse_transform=True, gamma=2.0).fit_transform(X)
A:sklearn.decomposition.tests.test_kernel_pca.X_kpca2->KernelPCA(4, eigen_solver=eigen_solver, kernel='precomputed').fit(np.dot(X_fit, X_fit.T)).transform(np.dot(X_pred, X_fit.T))
A:sklearn.decomposition.tests.test_kernel_pca.X_kpca_train->KernelPCA(4, eigen_solver=eigen_solver, kernel='precomputed').fit_transform(np.dot(X_fit, X_fit.T))
A:sklearn.decomposition.tests.test_kernel_pca.X_kpca_train2->KernelPCA(4, eigen_solver=eigen_solver, kernel='precomputed').fit(np.dot(X_fit, X_fit.T)).transform(np.dot(X_fit, X_fit.T))
A:sklearn.decomposition.tests.test_kernel_pca.(X, y)->make_circles(n_samples=400, factor=0.3, noise=0.05, random_state=0)
A:sklearn.decomposition.tests.test_kernel_pca.pipeline->Pipeline([('kernel_pca', kpca), ('Perceptron', Perceptron(max_iter=5))])
A:sklearn.decomposition.tests.test_kernel_pca.param_grid->dict(Perceptron__max_iter=np.arange(1, 5))
A:sklearn.decomposition.tests.test_kernel_pca.grid_search->GridSearchCV(pipeline, cv=3, param_grid=param_grid)
A:sklearn.decomposition.tests.test_kernel_pca.X_kernel->rbf_kernel(X, gamma=2.0)
A:sklearn.decomposition.tests.test_kernel_pca.train_score->Perceptron(max_iter=5).fit(X_kpca, y).score(X_kpca, y)
sklearn.decomposition.tests.test_kernel_pca.test_gridsearch_pipeline()
sklearn.decomposition.tests.test_kernel_pca.test_gridsearch_pipeline_precomputed()
sklearn.decomposition.tests.test_kernel_pca.test_kernel_pca()
sklearn.decomposition.tests.test_kernel_pca.test_kernel_pca_consistent_transform()
sklearn.decomposition.tests.test_kernel_pca.test_kernel_pca_invalid_kernel()
sklearn.decomposition.tests.test_kernel_pca.test_kernel_pca_invalid_parameters()
sklearn.decomposition.tests.test_kernel_pca.test_kernel_pca_linear_kernel()
sklearn.decomposition.tests.test_kernel_pca.test_kernel_pca_n_components()
sklearn.decomposition.tests.test_kernel_pca.test_kernel_pca_precomputed()
sklearn.decomposition.tests.test_kernel_pca.test_kernel_pca_sparse()
sklearn.decomposition.tests.test_kernel_pca.test_nested_circles()
sklearn.decomposition.tests.test_kernel_pca.test_remove_zero_eig()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/decomposition/tests/test_nmf.py----------------------------------------
A:sklearn.decomposition.tests.test_nmf.rng->numpy.random.mtrand.RandomState(42)
A:sklearn.decomposition.tests.test_nmf.data->numpy.abs(rng.randn(10, 10))
A:sklearn.decomposition.tests.test_nmf.(W, H)->sklearn.decomposition.nmf._initialize_nmf(X, n_components, init='random', random_state=42)
A:sklearn.decomposition.tests.test_nmf.A->numpy.ones((2, 2))
A:sklearn.decomposition.tests.test_nmf.clf->NMF(2, tol=0.1).fit(A)
A:sklearn.decomposition.tests.test_nmf.error->scipy.linalg.norm(np.dot(W, H) - A)
A:sklearn.decomposition.tests.test_nmf.sdev->scipy.linalg.norm(A - A.mean())
A:sklearn.decomposition.tests.test_nmf.(W0, H0)->sklearn.decomposition.nmf._initialize_nmf(X, n_components, init='random', random_state=42)
A:sklearn.decomposition.tests.test_nmf.(Wa, Ha)->sklearn.decomposition.nmf._initialize_nmf(data, 10, init='nndsvda')
A:sklearn.decomposition.tests.test_nmf.(War, Har)->sklearn.decomposition.nmf._initialize_nmf(data, 10, init='nndsvdar', random_state=0)
A:sklearn.decomposition.tests.test_nmf.model->sklearn.decomposition.nmf.NMF(n_components=n_components, solver=solver, alpha=0.0, l1_ratio=l1_ratio, random_state=42)
A:sklearn.decomposition.tests.test_nmf.transf->sklearn.decomposition.nmf.NMF(n_components=n_components, solver=solver, alpha=0.0, l1_ratio=l1_ratio, random_state=42).fit_transform(A)
A:sklearn.decomposition.tests.test_nmf.pnmf->NMF(5, solver=solver, init='nndsvdar', random_state=0, max_iter=600)
A:sklearn.decomposition.tests.test_nmf.X->numpy.random.mtrand.RandomState(42).randn(n_samples, n_features)
A:sklearn.decomposition.tests.test_nmf.m->NMF(solver=solver, n_components=4, init='random', random_state=0, max_iter=1000)
A:sklearn.decomposition.tests.test_nmf.ft->NMF(solver=solver, n_components=4, init='random', random_state=0, max_iter=1000).fit_transform(A)
A:sklearn.decomposition.tests.test_nmf.t->NMF(solver=solver, n_components=4, init='random', random_state=0, max_iter=1000).transform(A)
A:sklearn.decomposition.tests.test_nmf.random_state->numpy.random.RandomState(0)
A:sklearn.decomposition.tests.test_nmf.avg->numpy.sqrt(A.mean() / n_components)
A:sklearn.decomposition.tests.test_nmf.H_init->numpy.abs(avg * random_state.randn(n_components, 5))
A:sklearn.decomposition.tests.test_nmf.W_init->numpy.abs(avg * random_state.randn(6, n_components))
A:sklearn.decomposition.tests.test_nmf.A_new->NMF(solver=solver, n_components=4, init='random', random_state=0, max_iter=1000).inverse_transform(ft)
A:sklearn.decomposition.tests.test_nmf.A_sparse->csc_matrix(A)
A:sklearn.decomposition.tests.test_nmf.est1->NMF(solver=solver, n_components=5, init='random', random_state=0, tol=0.01)
A:sklearn.decomposition.tests.test_nmf.est2->clone(est1)
A:sklearn.decomposition.tests.test_nmf.W1->NMF(solver=solver, n_components=5, init='random', random_state=0, tol=0.01).fit_transform(A)
A:sklearn.decomposition.tests.test_nmf.W2->clone(est1).fit_transform(A_sparse)
A:sklearn.decomposition.tests.test_nmf.A_fit_tr->sklearn.decomposition.nmf.NMF(n_components=n_components, solver=solver, alpha=0.0, l1_ratio=l1_ratio, random_state=42).fit_transform(A)
A:sklearn.decomposition.tests.test_nmf.A_tr->sklearn.decomposition.nmf.NMF(n_components=n_components, solver=solver, alpha=0.0, l1_ratio=l1_ratio, random_state=42).transform(A)
A:sklearn.decomposition.tests.test_nmf.(W_nmf, H, _)->non_negative_factorization(A, solver=solver, random_state=1, tol=0.01)
A:sklearn.decomposition.tests.test_nmf.(W_nmf_2, _, _)->non_negative_factorization(A, H=H, update_H=False, solver=solver, random_state=1, tol=0.01)
A:sklearn.decomposition.tests.test_nmf.model_class->NMF(solver=solver, random_state=1, tol=0.01)
A:sklearn.decomposition.tests.test_nmf.W_cls->NMF(solver=solver, random_state=1, tol=0.01).fit_transform(A)
A:sklearn.decomposition.tests.test_nmf.W_cls_2->NMF(solver=solver, random_state=1, tol=0.01).transform(A)
A:sklearn.decomposition.tests.test_nmf.W->numpy.abs(rng.randn(n_samples, n_components))
A:sklearn.decomposition.tests.test_nmf.H->numpy.abs(rng.randn(n_components, n_features))
A:sklearn.decomposition.tests.test_nmf.WH->sklearn.decomposition.nmf._special_sparse_dot(W, H, X)
A:sklearn.decomposition.tests.test_nmf.res->(X_nonzero ** beta).sum()
A:sklearn.decomposition.tests.test_nmf.X_csr->scipy.sparse.csr_matrix(X)
A:sklearn.decomposition.tests.test_nmf.ref->_beta_divergence_dense(X, W, H, beta)
A:sklearn.decomposition.tests.test_nmf.loss->sklearn.decomposition.nmf._beta_divergence(X, W, H, beta_loss)
A:sklearn.decomposition.tests.test_nmf.loss_csr->sklearn.decomposition.nmf._beta_divergence(X_csr, W, H, beta)
A:sklearn.decomposition.tests.test_nmf.WH_safe->sklearn.decomposition.nmf._special_sparse_dot(W, H, X_csr)
A:sklearn.decomposition.tests.test_nmf.(ii, jj)->scipy.sparse.csr_matrix(X).nonzero()
A:sklearn.decomposition.tests.test_nmf.WH_safe_data->numpy.asarray(WH_safe[ii, jj]).ravel()
A:sklearn.decomposition.tests.test_nmf.(W1, H1, _)->non_negative_factorization(X, W, H, n_components, init='custom', update_H=True, solver='mu', beta_loss=beta_loss, max_iter=n_iter, alpha=alpha, l1_ratio=l1_ratio, regularization='both', random_state=42)
A:sklearn.decomposition.tests.test_nmf.(W2, H2, _)->non_negative_factorization(X_csr, W, H, n_components, init='custom', update_H=True, solver='mu', beta_loss=beta_loss, max_iter=n_iter, alpha=alpha, l1_ratio=l1_ratio, regularization='both', random_state=42)
A:sklearn.decomposition.tests.test_nmf.(W3, H3, _)->non_negative_factorization(X_csr, W, H, n_components, init='custom', update_H=True, solver='mu', beta_loss=beta_loss, max_iter=n_iter, alpha=alpha, l1_ratio=l1_ratio, regularization='both', random_state=42)
A:sklearn.decomposition.tests.test_nmf.(W, H, _)->non_negative_factorization(X, W, H, beta_loss=beta_loss, init='custom', n_components=n_components, max_iter=1, alpha=alpha, solver=solver, tol=tol, l1_ratio=l1_ratio, verbose=0, regularization='both', random_state=0, update_H=True)
A:sklearn.decomposition.tests.test_nmf.regul->sklearn.decomposition.nmf.NMF(n_components=n_components, solver=solver, alpha=0.5, l1_ratio=l1_ratio, random_state=42)
A:sklearn.decomposition.tests.test_nmf.W_regul->sklearn.decomposition.nmf.NMF(n_components=n_components, solver=solver, alpha=0.5, l1_ratio=l1_ratio, random_state=42).fit_transform(X)
A:sklearn.decomposition.tests.test_nmf.W_model->sklearn.decomposition.nmf.NMF(n_components=n_components, solver=solver, alpha=0.0, l1_ratio=l1_ratio, random_state=42).fit_transform(X)
sklearn.decomposition.tests.test_nmf._beta_divergence_dense(X,W,H,beta)
sklearn.decomposition.tests.test_nmf.test_beta_divergence()
sklearn.decomposition.tests.test_nmf.test_initialize_close()
sklearn.decomposition.tests.test_nmf.test_initialize_nn_output()
sklearn.decomposition.tests.test_nmf.test_initialize_variants()
sklearn.decomposition.tests.test_nmf.test_n_components_greater_n_features()
sklearn.decomposition.tests.test_nmf.test_nmf_decreasing()
sklearn.decomposition.tests.test_nmf.test_nmf_fit_close()
sklearn.decomposition.tests.test_nmf.test_nmf_fit_nn_output()
sklearn.decomposition.tests.test_nmf.test_nmf_inverse_transform()
sklearn.decomposition.tests.test_nmf.test_nmf_multiplicative_update_sparse()
sklearn.decomposition.tests.test_nmf.test_nmf_negative_beta_loss()
sklearn.decomposition.tests.test_nmf.test_nmf_regularization()
sklearn.decomposition.tests.test_nmf.test_nmf_sparse_input()
sklearn.decomposition.tests.test_nmf.test_nmf_sparse_transform()
sklearn.decomposition.tests.test_nmf.test_nmf_transform()
sklearn.decomposition.tests.test_nmf.test_nmf_transform_custom_init()
sklearn.decomposition.tests.test_nmf.test_non_negative_factorization_checking()
sklearn.decomposition.tests.test_nmf.test_non_negative_factorization_consistency()
sklearn.decomposition.tests.test_nmf.test_parameter_checking()
sklearn.decomposition.tests.test_nmf.test_special_sparse_dot()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/manifold/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/manifold/t_sne.py----------------------------------------
A:sklearn.manifold.t_sne.distances->pairwise_distances(X, metric=self.metric)
A:sklearn.manifold.t_sne.conditional_P->_utils._binary_search_perplexity(distances, neighbors, desired_perplexity, verbose)
A:sklearn.manifold.t_sne.sum_P->numpy.maximum(P.sum(), MACHINE_EPSILON)
A:sklearn.manifold.t_sne.P->_joint_probabilities_nn(distances_nn, neighbors_nn, self.perplexity, self.verbose)
A:sklearn.manifold.t_sne.t0->time()
A:sklearn.manifold.t_sne.neighbors->_joint_probabilities_nn(distances_nn, neighbors_nn, self.perplexity, self.verbose).indices.astype(np.int64, copy=False)
A:sklearn.manifold.t_sne.X_embedded->X_embedded.ravel().reshape(n_samples, self.n_components)
A:sklearn.manifold.t_sne.dist->pdist(X_embedded, 'sqeuclidean')
A:sklearn.manifold.t_sne.Q->numpy.maximum(dist / (2.0 * np.sum(dist)), MACHINE_EPSILON)
A:sklearn.manifold.t_sne.grad->grad.ravel().ravel()
A:sklearn.manifold.t_sne.PQd->squareform((P - Q) * dist)
A:sklearn.manifold.t_sne.grad[i]->numpy.dot(np.ravel(PQd[i], order='K'), X_embedded[i] - X_embedded)
A:sklearn.manifold.t_sne.params->X_embedded.ravel().reshape(n_samples, self.n_components).ravel()
A:sklearn.manifold.t_sne.val_P->_joint_probabilities_nn(distances_nn, neighbors_nn, self.perplexity, self.verbose).data.astype(np.float32, copy=False)
A:sklearn.manifold.t_sne.indptr->_joint_probabilities_nn(distances_nn, neighbors_nn, self.perplexity, self.verbose).indptr.astype(np.int64, copy=False)
A:sklearn.manifold.t_sne.error->_barnes_hut_tsne.gradient(val_P, X_embedded, neighbors, indptr, grad, angle, n_components, verbose, dof=degrees_of_freedom)
A:sklearn.manifold.t_sne.p->p0.copy().ravel()
A:sklearn.manifold.t_sne.update->numpy.zeros_like(p)
A:sklearn.manifold.t_sne.gains->numpy.ones_like(p)
A:sklearn.manifold.t_sne.tic->time()
A:sklearn.manifold.t_sne.(error, grad)->objective(p, *args, **kwargs)
A:sklearn.manifold.t_sne.grad_norm->scipy.linalg.norm(grad)
A:sklearn.manifold.t_sne.dec->numpy.invert(inc)
A:sklearn.manifold.t_sne.toc->time()
A:sklearn.manifold.t_sne.dist_X->pairwise_distances(X, squared=True)
A:sklearn.manifold.t_sne.dist_X_embedded->pairwise_distances(X_embedded, squared=True)
A:sklearn.manifold.t_sne.ind_X->numpy.argsort(dist_X, axis=1)
A:sklearn.manifold.t_sne.ranks->numpy.zeros(n_neighbors)
A:sklearn.manifold.t_sne.X->check_array(X, accept_sparse=['csr', 'csc', 'coo'], dtype=[np.float32, np.float64])
A:sklearn.manifold.t_sne.random_state->check_random_state(self.random_state)
A:sklearn.manifold.t_sne.k->min(n_samples - 1, int(3.0 * self.perplexity + 1))
A:sklearn.manifold.t_sne.knn->NearestNeighbors(algorithm='auto', n_neighbors=k, metric=self.metric)
A:sklearn.manifold.t_sne.(distances_nn, neighbors_nn)->NearestNeighbors(algorithm='auto', n_neighbors=k, metric=self.metric).kneighbors(None, n_neighbors=k)
A:sklearn.manifold.t_sne.pca->PCA(n_components=self.n_components, svd_solver='randomized', random_state=random_state)
A:sklearn.manifold.t_sne.degrees_of_freedom->max(self.n_components - 1.0, 1)
A:sklearn.manifold.t_sne.(params, kl_divergence, it)->_gradient_descent(obj_func, params, **opt_args)
A:sklearn.manifold.t_sne.embedding->self._fit(X)
sklearn.manifold.TSNE(self,n_components=2,perplexity=30.0,early_exaggeration=12.0,learning_rate=200.0,n_iter=1000,n_iter_without_progress=300,min_grad_norm=1e-07,metric='euclidean',init='random',verbose=0,random_state=None,method='barnes_hut',angle=0.5)
sklearn.manifold.TSNE._fit(self,X,skip_num_points=0)
sklearn.manifold.TSNE._tsne(self,P,degrees_of_freedom,n_samples,X_embedded,neighbors=None,skip_num_points=0)
sklearn.manifold.TSNE.fit(self,X,y=None)
sklearn.manifold.TSNE.fit_transform(self,X,y=None)
sklearn.manifold.TSNE.n_iter_final(self)
sklearn.manifold.t_sne.TSNE(self,n_components=2,perplexity=30.0,early_exaggeration=12.0,learning_rate=200.0,n_iter=1000,n_iter_without_progress=300,min_grad_norm=1e-07,metric='euclidean',init='random',verbose=0,random_state=None,method='barnes_hut',angle=0.5)
sklearn.manifold.t_sne.TSNE.__init__(self,n_components=2,perplexity=30.0,early_exaggeration=12.0,learning_rate=200.0,n_iter=1000,n_iter_without_progress=300,min_grad_norm=1e-07,metric='euclidean',init='random',verbose=0,random_state=None,method='barnes_hut',angle=0.5)
sklearn.manifold.t_sne.TSNE._fit(self,X,skip_num_points=0)
sklearn.manifold.t_sne.TSNE._tsne(self,P,degrees_of_freedom,n_samples,X_embedded,neighbors=None,skip_num_points=0)
sklearn.manifold.t_sne.TSNE.fit(self,X,y=None)
sklearn.manifold.t_sne.TSNE.fit_transform(self,X,y=None)
sklearn.manifold.t_sne.TSNE.n_iter_final(self)
sklearn.manifold.t_sne._gradient_descent(objective,p0,it,n_iter,n_iter_check=1,n_iter_without_progress=300,momentum=0.8,learning_rate=200.0,min_gain=0.01,min_grad_norm=1e-07,verbose=0,args=None,kwargs=None)
sklearn.manifold.t_sne._joint_probabilities(distances,desired_perplexity,verbose)
sklearn.manifold.t_sne._joint_probabilities_nn(distances,neighbors,desired_perplexity,verbose)
sklearn.manifold.t_sne._kl_divergence(params,P,degrees_of_freedom,n_samples,n_components,skip_num_points=0)
sklearn.manifold.t_sne._kl_divergence_bh(params,P,degrees_of_freedom,n_samples,n_components,angle=0.5,skip_num_points=0,verbose=False)
sklearn.manifold.t_sne.trustworthiness(X,X_embedded,n_neighbors=5,precomputed=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/manifold/spectral_embedding_.py----------------------------------------
A:sklearn.manifold.spectral_embedding_.graph->graph.tocsr().tocsr()
A:sklearn.manifold.spectral_embedding_.connected_nodes->numpy.zeros(n_node, dtype=np.bool)
A:sklearn.manifold.spectral_embedding_.nodes_to_explore->numpy.zeros(n_node, dtype=np.bool)
A:sklearn.manifold.spectral_embedding_.last_num_component->numpy.zeros(n_node, dtype=np.bool).sum()
A:sklearn.manifold.spectral_embedding_.neighbors->graph[i].toarray().ravel()
A:sklearn.manifold.spectral_embedding_.(n_connected_components, _)->connected_components(graph)
A:sklearn.manifold.spectral_embedding_.laplacian->_set_diag(laplacian, 1, norm_laplacian)
A:sklearn.manifold.spectral_embedding_.adjacency->check_symmetric(adjacency)
A:sklearn.manifold.spectral_embedding_.random_state->check_random_state(self.random_state)
A:sklearn.manifold.spectral_embedding_.(laplacian, dd)->scipy.sparse.csgraph.laplacian(adjacency, normed=norm_laplacian, return_diag=True)
A:sklearn.manifold.spectral_embedding_.v0->check_random_state(self.random_state).uniform(-1, 1, laplacian.shape[0])
A:sklearn.manifold.spectral_embedding_.(lambdas, diffusion_map)->lobpcg(laplacian, X, tol=1e-15, largest=False, maxiter=2000)
A:sklearn.manifold.spectral_embedding_.ml->smoothed_aggregation_solver(check_array(laplacian, 'csr'))
A:sklearn.manifold.spectral_embedding_.M->smoothed_aggregation_solver(check_array(laplacian, 'csr')).aspreconditioner()
A:sklearn.manifold.spectral_embedding_.X->check_array(X, ensure_min_samples=2, estimator=self)
A:sklearn.manifold.spectral_embedding_.X[:, 0]->dd.ravel()
A:sklearn.manifold.spectral_embedding_.embedding->_deterministic_vector_sign_flip(embedding)
A:sklearn.manifold.spectral_embedding_.self.affinity_matrix_->self.affinity(X)
A:sklearn.manifold.spectral_embedding_.affinity_matrix->self._get_affinity_matrix(X)
A:sklearn.manifold.spectral_embedding_.self.embedding_->spectral_embedding(affinity_matrix, n_components=self.n_components, eigen_solver=self.eigen_solver, random_state=random_state)
sklearn.manifold.SpectralEmbedding(self,n_components=2,affinity='nearest_neighbors',gamma=None,random_state=None,eigen_solver=None,n_neighbors=None,n_jobs=1)
sklearn.manifold.SpectralEmbedding._get_affinity_matrix(self,X,Y=None)
sklearn.manifold.SpectralEmbedding._pairwise(self)
sklearn.manifold.SpectralEmbedding.fit(self,X,y=None)
sklearn.manifold.SpectralEmbedding.fit_transform(self,X,y=None)
sklearn.manifold.spectral_embedding(adjacency,n_components=8,eigen_solver=None,random_state=None,eigen_tol=0.0,norm_laplacian=True,drop_first=True)
sklearn.manifold.spectral_embedding_.SpectralEmbedding(self,n_components=2,affinity='nearest_neighbors',gamma=None,random_state=None,eigen_solver=None,n_neighbors=None,n_jobs=1)
sklearn.manifold.spectral_embedding_.SpectralEmbedding.__init__(self,n_components=2,affinity='nearest_neighbors',gamma=None,random_state=None,eigen_solver=None,n_neighbors=None,n_jobs=1)
sklearn.manifold.spectral_embedding_.SpectralEmbedding._get_affinity_matrix(self,X,Y=None)
sklearn.manifold.spectral_embedding_.SpectralEmbedding._pairwise(self)
sklearn.manifold.spectral_embedding_.SpectralEmbedding.fit(self,X,y=None)
sklearn.manifold.spectral_embedding_.SpectralEmbedding.fit_transform(self,X,y=None)
sklearn.manifold.spectral_embedding_._graph_connected_component(graph,node_id)
sklearn.manifold.spectral_embedding_._graph_is_connected(graph)
sklearn.manifold.spectral_embedding_._set_diag(laplacian,value,norm_laplacian)
sklearn.manifold.spectral_embedding_.spectral_embedding(adjacency,n_components=8,eigen_solver=None,random_state=None,eigen_tol=0.0,norm_laplacian=True,drop_first=True)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/manifold/mds.py----------------------------------------
A:sklearn.manifold.mds.dissimilarities->check_array(dissimilarities)
A:sklearn.manifold.mds.random_state->check_random_state(random_state)
A:sklearn.manifold.mds.sim_flat->((1 - np.tri(n_samples)) * dissimilarities).ravel()
A:sklearn.manifold.mds.X->check_array(X)
A:sklearn.manifold.mds.ir->IsotonicRegression()
A:sklearn.manifold.mds.dis->numpy.sqrt((X ** 2).sum(axis=1)).sum()
A:sklearn.manifold.mds.dis_flat->numpy.sqrt((X ** 2).sum(axis=1)).sum().ravel()
A:sklearn.manifold.mds.disparities_flat->IsotonicRegression().fit_transform(sim_flat_w, dis_flat_w)
A:sklearn.manifold.mds.disparities->disparities.reshape((n_samples, n_samples)).reshape((n_samples, n_samples))
A:sklearn.manifold.mds.init->numpy.asarray(init).copy()
A:sklearn.manifold.mds.(pos, stress, n_iter_)->_smacof_single(dissimilarities, metric=metric, n_components=n_components, init=init, max_iter=max_iter, verbose=verbose, eps=eps, random_state=random_state)
A:sklearn.manifold.mds.best_pos->pos.copy()
A:sklearn.manifold.mds.seeds->check_random_state(random_state).randint(np.iinfo(np.int32).max, size=n_init)
A:sklearn.manifold.mds.results->Parallel(n_jobs=n_jobs, verbose=max(verbose - 1, 0))((delayed(_smacof_single)(dissimilarities, metric=metric, n_components=n_components, init=init, max_iter=max_iter, verbose=verbose, eps=eps, random_state=seed) for seed in seeds))
A:sklearn.manifold.mds.(positions, stress, n_iters)->zip(*results)
A:sklearn.manifold.mds.best->numpy.argmin(stress)
A:sklearn.manifold.mds.self.dissimilarity_matrix_->euclidean_distances(X)
A:sklearn.manifold.mds.(self.embedding_, self.stress_, self.n_iter_)->smacof(self.dissimilarity_matrix_, metric=self.metric, n_components=self.n_components, init=init, n_init=self.n_init, n_jobs=self.n_jobs, max_iter=self.max_iter, verbose=self.verbose, eps=self.eps, random_state=self.random_state, return_n_iter=True)
sklearn.manifold.MDS(self,n_components=2,metric=True,n_init=4,max_iter=300,verbose=0,eps=0.001,n_jobs=1,random_state=None,dissimilarity='euclidean')
sklearn.manifold.MDS._pairwise(self)
sklearn.manifold.MDS.fit(self,X,y=None,init=None)
sklearn.manifold.MDS.fit_transform(self,X,y=None,init=None)
sklearn.manifold.mds.MDS(self,n_components=2,metric=True,n_init=4,max_iter=300,verbose=0,eps=0.001,n_jobs=1,random_state=None,dissimilarity='euclidean')
sklearn.manifold.mds.MDS.__init__(self,n_components=2,metric=True,n_init=4,max_iter=300,verbose=0,eps=0.001,n_jobs=1,random_state=None,dissimilarity='euclidean')
sklearn.manifold.mds.MDS._pairwise(self)
sklearn.manifold.mds.MDS.fit(self,X,y=None,init=None)
sklearn.manifold.mds.MDS.fit_transform(self,X,y=None,init=None)
sklearn.manifold.mds._smacof_single(dissimilarities,metric=True,n_components=2,init=None,max_iter=300,verbose=0,eps=0.001,random_state=None)
sklearn.manifold.mds.smacof(dissimilarities,metric=True,n_components=2,init=None,n_init=8,n_jobs=1,max_iter=300,verbose=0,eps=0.001,random_state=None,return_n_iter=False)
sklearn.manifold.smacof(dissimilarities,metric=True,n_components=2,init=None,n_init=8,n_jobs=1,max_iter=300,verbose=0,eps=0.001,random_state=None,return_n_iter=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/manifold/setup.py----------------------------------------
A:sklearn.manifold.setup.config->Configuration('manifold', parent_package, top_path)
A:sklearn.manifold.setup.(cblas_libs, blas_info)->get_blas_info()
A:sklearn.manifold.setup.eca->blas_info.pop('extra_compile_args', [])
sklearn.manifold.setup.configuration(parent_package='',top_path=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/manifold/isomap.py----------------------------------------
A:sklearn.manifold.isomap.X->check_array(X)
A:sklearn.manifold.isomap.self.nbrs_->NearestNeighbors(n_neighbors=self.n_neighbors, algorithm=self.neighbors_algorithm, n_jobs=self.n_jobs)
A:sklearn.manifold.isomap.self.kernel_pca_->KernelPCA(n_components=self.n_components, kernel='precomputed', eigen_solver=self.eigen_solver, tol=self.tol, max_iter=self.max_iter, n_jobs=self.n_jobs)
A:sklearn.manifold.isomap.kng->kneighbors_graph(self.nbrs_, self.n_neighbors, mode='distance', n_jobs=self.n_jobs)
A:sklearn.manifold.isomap.self.dist_matrix_->graph_shortest_path(kng, method=self.path_method, directed=False)
A:sklearn.manifold.isomap.self.embedding_->self.kernel_pca_.fit_transform(G)
A:sklearn.manifold.isomap.G_center->KernelCenterer().fit_transform(G)
A:sklearn.manifold.isomap.(distances, indices)->self.nbrs_.kneighbors(X, return_distance=True)
A:sklearn.manifold.isomap.G_X->numpy.zeros((X.shape[0], self.training_data_.shape[0]))
A:sklearn.manifold.isomap.G_X[i]->numpy.min(self.dist_matrix_[indices[i]] + distances[i][:, None], 0)
sklearn.manifold.Isomap(self,n_neighbors=5,n_components=2,eigen_solver='auto',tol=0,max_iter=None,path_method='auto',neighbors_algorithm='auto',n_jobs=1)
sklearn.manifold.Isomap._fit_transform(self,X)
sklearn.manifold.Isomap.fit(self,X,y=None)
sklearn.manifold.Isomap.fit_transform(self,X,y=None)
sklearn.manifold.Isomap.reconstruction_error(self)
sklearn.manifold.Isomap.transform(self,X)
sklearn.manifold.isomap.Isomap(self,n_neighbors=5,n_components=2,eigen_solver='auto',tol=0,max_iter=None,path_method='auto',neighbors_algorithm='auto',n_jobs=1)
sklearn.manifold.isomap.Isomap.__init__(self,n_neighbors=5,n_components=2,eigen_solver='auto',tol=0,max_iter=None,path_method='auto',neighbors_algorithm='auto',n_jobs=1)
sklearn.manifold.isomap.Isomap._fit_transform(self,X)
sklearn.manifold.isomap.Isomap.fit(self,X,y=None)
sklearn.manifold.isomap.Isomap.fit_transform(self,X,y=None)
sklearn.manifold.isomap.Isomap.reconstruction_error(self)
sklearn.manifold.isomap.Isomap.transform(self,X)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/manifold/locally_linear.py----------------------------------------
A:sklearn.manifold.locally_linear.X->check_array(X)
A:sklearn.manifold.locally_linear.Z->check_array(Z, dtype=FLOAT_DTYPES, allow_nd=True)
A:sklearn.manifold.locally_linear.B->numpy.empty((n_samples, n_neighbors), dtype=X.dtype)
A:sklearn.manifold.locally_linear.v->numpy.ones(n_neighbors, dtype=X.dtype)
A:sklearn.manifold.locally_linear.G->numpy.dot(C, C.T)
A:sklearn.manifold.locally_linear.trace->numpy.trace(G)
A:sklearn.manifold.locally_linear.w->solve(G, v, sym_pos=True)
A:sklearn.manifold.locally_linear.knn->NearestNeighbors(n_neighbors + 1, n_jobs=n_jobs).fit(X)
A:sklearn.manifold.locally_linear.data->barycenter_weights(X, X[ind], reg=reg)
A:sklearn.manifold.locally_linear.indptr->numpy.arange(0, n_samples * n_neighbors + 1, n_neighbors)
A:sklearn.manifold.locally_linear.random_state->check_random_state(self.random_state)
A:sklearn.manifold.locally_linear.v0->check_random_state(self.random_state).uniform(-1, 1, M.shape[0])
A:sklearn.manifold.locally_linear.(eigen_values, eigen_vectors)->eigh(M, eigvals=(k_skip, k + k_skip - 1), overwrite_a=True)
A:sklearn.manifold.locally_linear.M->numpy.zeros((N, N))
A:sklearn.manifold.locally_linear.index->numpy.argsort(np.abs(eigen_values))
A:sklearn.manifold.locally_linear.nbrs->NearestNeighbors(n_neighbors=n_neighbors + 1, n_jobs=n_jobs)
A:sklearn.manifold.locally_linear.W->barycenter_kneighbors_graph(nbrs, n_neighbors=n_neighbors, reg=reg, n_jobs=n_jobs)
A:sklearn.manifold.locally_linear.neighbors->NearestNeighbors(n_neighbors=n_neighbors + 1, n_jobs=n_jobs).kneighbors(X, n_neighbors=n_neighbors + 1, return_distance=False)
A:sklearn.manifold.locally_linear.Yi->numpy.empty((n_neighbors, 1 + n_components + dp), dtype=np.float64)
A:sklearn.manifold.locally_linear.Ci->numpy.dot(Xi, Xi.T)
A:sklearn.manifold.locally_linear.(Q, R)->qr(Yi)
A:sklearn.manifold.locally_linear.S->solve(G, v, sym_pos=True).sum(0)
A:sklearn.manifold.locally_linear.(nbrs_x, nbrs_y)->numpy.meshgrid(neighbors[i], neighbors[i])
A:sklearn.manifold.locally_linear.V->numpy.zeros((N, n_neighbors, n_neighbors))
A:sklearn.manifold.locally_linear.nev->min(d_in, n_neighbors)
A:sklearn.manifold.locally_linear.evals->numpy.zeros([N, nev])
A:sklearn.manifold.locally_linear.(V[i], evals[i], _)->svd(X_nbrs, full_matrices=True)
A:sklearn.manifold.locally_linear.C_nbrs->numpy.dot(X_nbrs, X_nbrs.T)
A:sklearn.manifold.locally_linear.(evi, vi)->eigh(C_nbrs)
A:sklearn.manifold.locally_linear.tmp->numpy.dot(V.transpose(0, 2, 1), np.ones(n_neighbors))
A:sklearn.manifold.locally_linear.w_reg->numpy.zeros((N, n_neighbors))
A:sklearn.manifold.locally_linear.w_reg[i]->numpy.dot(V[i], tmp[i])
A:sklearn.manifold.locally_linear.eta->numpy.median(rho)
A:sklearn.manifold.locally_linear.s_range->numpy.zeros(N, dtype=int)
A:sklearn.manifold.locally_linear.evals_cumsum->stable_cumsum(evals, 1)
A:sklearn.manifold.locally_linear.s_range[i]->numpy.searchsorted(eta_range[i, ::-1], eta)
A:sklearn.manifold.locally_linear.norm_h->numpy.linalg.norm(h)
A:sklearn.manifold.locally_linear.Wi_sum1->Wi.sum(1)
A:sklearn.manifold.locally_linear.Gi->numpy.zeros((n_neighbors, n_components + 1))
A:sklearn.manifold.locally_linear.GiGiT->numpy.dot(Gi, Gi.T)
A:sklearn.manifold.locally_linear.self.nbrs_->NearestNeighbors(self.n_neighbors, algorithm=self.neighbors_algorithm, n_jobs=self.n_jobs)
A:sklearn.manifold.locally_linear.(self.embedding_, self.reconstruction_error_)->locally_linear_embedding(self.nbrs_, self.n_neighbors, self.n_components, eigen_solver=self.eigen_solver, tol=self.tol, max_iter=self.max_iter, method=self.method, hessian_tol=self.hessian_tol, modified_tol=self.modified_tol, random_state=random_state, reg=self.reg, n_jobs=self.n_jobs)
A:sklearn.manifold.locally_linear.ind->self.nbrs_.kneighbors(X, n_neighbors=self.n_neighbors, return_distance=False)
A:sklearn.manifold.locally_linear.weights->barycenter_weights(X, self.nbrs_._fit_X[ind], reg=self.reg)
A:sklearn.manifold.locally_linear.X_new->numpy.empty((X.shape[0], self.n_components))
A:sklearn.manifold.locally_linear.X_new[i]->numpy.dot(self.embedding_[ind[i]].T, weights[i])
sklearn.manifold.LocallyLinearEmbedding(self,n_neighbors=5,n_components=2,reg=0.001,eigen_solver='auto',tol=1e-06,max_iter=100,method='standard',hessian_tol=0.0001,modified_tol=1e-12,neighbors_algorithm='auto',random_state=None,n_jobs=1)
sklearn.manifold.LocallyLinearEmbedding._fit_transform(self,X)
sklearn.manifold.LocallyLinearEmbedding.fit(self,X,y=None)
sklearn.manifold.LocallyLinearEmbedding.fit_transform(self,X,y=None)
sklearn.manifold.LocallyLinearEmbedding.transform(self,X)
sklearn.manifold.locally_linear.LocallyLinearEmbedding(self,n_neighbors=5,n_components=2,reg=0.001,eigen_solver='auto',tol=1e-06,max_iter=100,method='standard',hessian_tol=0.0001,modified_tol=1e-12,neighbors_algorithm='auto',random_state=None,n_jobs=1)
sklearn.manifold.locally_linear.LocallyLinearEmbedding.__init__(self,n_neighbors=5,n_components=2,reg=0.001,eigen_solver='auto',tol=1e-06,max_iter=100,method='standard',hessian_tol=0.0001,modified_tol=1e-12,neighbors_algorithm='auto',random_state=None,n_jobs=1)
sklearn.manifold.locally_linear.LocallyLinearEmbedding._fit_transform(self,X)
sklearn.manifold.locally_linear.LocallyLinearEmbedding.fit(self,X,y=None)
sklearn.manifold.locally_linear.LocallyLinearEmbedding.fit_transform(self,X,y=None)
sklearn.manifold.locally_linear.LocallyLinearEmbedding.transform(self,X)
sklearn.manifold.locally_linear.barycenter_kneighbors_graph(X,n_neighbors,reg=0.001,n_jobs=1)
sklearn.manifold.locally_linear.barycenter_weights(X,Z,reg=0.001)
sklearn.manifold.locally_linear.locally_linear_embedding(X,n_neighbors,n_components,reg=0.001,eigen_solver='auto',tol=1e-06,max_iter=100,method='standard',hessian_tol=0.0001,modified_tol=1e-12,random_state=None,n_jobs=1)
sklearn.manifold.locally_linear.null_space(M,k,k_skip=1,eigen_solver='arpack',tol=1e-06,max_iter=100,random_state=None)
sklearn.manifold.locally_linear_embedding(X,n_neighbors,n_components,reg=0.001,eigen_solver='auto',tol=1e-06,max_iter=100,method='standard',hessian_tol=0.0001,modified_tol=1e-12,random_state=None,n_jobs=1)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/manifold/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/manifold/tests/test_locally_linear.py----------------------------------------
A:sklearn.manifold.tests.test_locally_linear.X->numpy.random.RandomState(0).randint(0, 100, size=(20, 3))
A:sklearn.manifold.tests.test_locally_linear.A->barycenter_kneighbors_graph(X, 2)
A:sklearn.manifold.tests.test_locally_linear.pred->numpy.dot(A.toarray(), X)
A:sklearn.manifold.tests.test_locally_linear.rng->numpy.random.RandomState(0)
A:sklearn.manifold.tests.test_locally_linear.clf->sklearn.manifold.LocallyLinearEmbedding(method=method, n_neighbors=10)
A:sklearn.manifold.tests.test_locally_linear.N->barycenter_kneighbors_graph(X, clf.n_neighbors).toarray()
A:sklearn.manifold.tests.test_locally_linear.reconstruction_error->scipy.linalg.norm(np.dot(N, X) - X)
A:sklearn.manifold.tests.test_locally_linear.X_reembedded->sklearn.manifold.LocallyLinearEmbedding(method=method, n_neighbors=10).transform(X + noise)
A:sklearn.manifold.tests.test_locally_linear.(X, y)->sklearn.datasets.make_blobs(random_state=0)
A:sklearn.manifold.tests.test_locally_linear.M->numpy.ones((10, 3))
A:sklearn.manifold.tests.test_locally_linear.rand->numpy.random.RandomState(0)
sklearn.manifold.tests.test_locally_linear.test_barycenter_kneighbors_graph()
sklearn.manifold.tests.test_locally_linear.test_integer_input()
sklearn.manifold.tests.test_locally_linear.test_lle_init_parameters()
sklearn.manifold.tests.test_locally_linear.test_lle_manifold()
sklearn.manifold.tests.test_locally_linear.test_lle_simple_grid()
sklearn.manifold.tests.test_locally_linear.test_pipeline()
sklearn.manifold.tests.test_locally_linear.test_singular_matrix()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/manifold/tests/test_t_sne.py----------------------------------------
A:sklearn.manifold.tests.test_t_sne.x->numpy.linspace(0, 1, 10)
A:sklearn.manifold.tests.test_t_sne.(xx, yy)->numpy.meshgrid(x, x)
A:sklearn.manifold.tests.test_t_sne.X_2d_grid->numpy.hstack([xx.ravel().reshape(-1, 1), yy.ravel().reshape(-1, 1)])
A:sklearn.manifold.tests.test_t_sne.sys.stdout->StringIO()
A:sklearn.manifold.tests.test_t_sne.(_, error, it)->_gradient_descent(ObjectiveSmallGradient(), np.zeros(1), 0, n_iter=11, n_iter_without_progress=100, momentum=0.0, learning_rate=0.0, min_gain=0.0, min_grad_norm=0.0, verbose=2)
A:sklearn.manifold.tests.test_t_sne.out->sys.stdout.getvalue()
A:sklearn.manifold.tests.test_t_sne.random_state->check_random_state(0)
A:sklearn.manifold.tests.test_t_sne.distances->abs(distances.dot(distances.T))
A:sklearn.manifold.tests.test_t_sne.P->squareform(P)
A:sklearn.manifold.tests.test_t_sne.mean_perplexity->numpy.mean([np.exp(-np.sum(P[i] * np.log(P[i]))) for i in range(P.shape[0])])
A:sklearn.manifold.tests.test_t_sne.P1->P1.toarray().toarray()
A:sklearn.manifold.tests.test_t_sne.neighbors_nn->numpy.argsort(distances, axis=1)[:, :k].astype(np.int64)
A:sklearn.manifold.tests.test_t_sne.distances_nn->numpy.array([distances[i, neighbors_nn[i]] for i in range(n_samples)])
A:sklearn.manifold.tests.test_t_sne.P2->_binary_search_perplexity(distances_nn, neighbors_nn, desired_perplexity, verbose=0)
A:sklearn.manifold.tests.test_t_sne.P_nn->numpy.array([P1[k, neighbors_nn[k]] for k in range(n_samples)])
A:sklearn.manifold.tests.test_t_sne.k->int(k)
A:sklearn.manifold.tests.test_t_sne.P2k->_binary_search_perplexity(distances_nn, neighbors_nn, desired_perplexity, verbose=0)
A:sklearn.manifold.tests.test_t_sne.X_embedded->TSNE(n_components=2, method=method, learning_rate=1.0, init='random', random_state=0, n_iter=251, perplexity=30.0, angle=0).fit_transform(X)
A:sklearn.manifold.tests.test_t_sne.X->check_random_state(0).randn(50, n_components_original).astype(np.float32)
A:sklearn.manifold.tests.test_t_sne.tsne->TSNE(n_components=2, method=method, learning_rate=1.0, init='random', random_state=0, n_iter=251, perplexity=30.0, angle=0)
A:sklearn.manifold.tests.test_t_sne.t->trustworthiness(D, X_embedded, n_neighbors=1, precomputed=True)
A:sklearn.manifold.tests.test_t_sne.(X, _)->make_blobs(n_features=3, random_state=random_state)
A:sklearn.manifold.tests.test_t_sne.X_csr->scipy.sparse.csr_matrix(X)
A:sklearn.manifold.tests.test_t_sne.D->squareform(pdist(X), 'sqeuclidean')
A:sklearn.manifold.tests.test_t_sne.bad_dist->numpy.array([[0.0, -1.0], [1.0, 0.0]])
A:sklearn.manifold.tests.test_t_sne.X_embedded1->TSNE(n_components=2, method=method, learning_rate=1.0, init='random', random_state=0, n_iter=251, perplexity=30.0, angle=0).fit_transform(X)
A:sklearn.manifold.tests.test_t_sne.X_embedded2->TSNE(n_components=2, method=method, learning_rate=1.0, init='random', random_state=0, n_iter=251, perplexity=30.0, angle=0).fit_transform(X)
A:sklearn.manifold.tests.test_t_sne.pos_input->numpy.array([[1.0, 0.0], [0.0, 1.0], [5.0, 2.0], [7.3, 2.2]])
A:sklearn.manifold.tests.test_t_sne.pos_output->pos_output.astype(np.float32).astype(np.float32)
A:sklearn.manifold.tests.test_t_sne.neighbors->squareform(P).indices.astype(np.int64)
A:sklearn.manifold.tests.test_t_sne.grad_output->numpy.array([[0.0, 0.0], [0.0, 0.0], [4.24275173e-08, -3.69569698e-08], [-2.58720939e-09, 7.52706374e-09]])
A:sklearn.manifold.tests.test_t_sne.pij_input->squareform(pij_input).astype(np.float32)
A:sklearn.manifold.tests.test_t_sne.grad_bh->numpy.zeros(pos_output.shape, dtype=np.float32)
A:sklearn.manifold.tests.test_t_sne.indptr->squareform(P).indptr.astype(np.int64)
A:sklearn.manifold.tests.test_t_sne.degrees_of_freedom->float(n_components - 1.0)
A:sklearn.manifold.tests.test_t_sne.params->check_random_state(0).randn(n_samples, n_components)
A:sklearn.manifold.tests.test_t_sne.(kl_exact, grad_exact)->_kl_divergence(params, P, degrees_of_freedom, n_samples, n_components)
A:sklearn.manifold.tests.test_t_sne.bt->BallTree(distances)
A:sklearn.manifold.tests.test_t_sne.(distances_nn, neighbors_nn)->BallTree(distances).query(distances, k=k + 1)
A:sklearn.manifold.tests.test_t_sne.P_bh->P_bh.toarray().toarray()
A:sklearn.manifold.tests.test_t_sne.(kl_bh, grad_bh)->_kl_divergence_bh(params, P_bh, degrees_of_freedom, n_samples, n_components, angle=angle, skip_num_points=0, verbose=0)
A:sklearn.manifold.tests.test_t_sne.lines_out->sys.stdout.getvalue().split('\n')
A:sklearn.manifold.tests.test_t_sne.start_grad_norm->line.find('gradient norm')
A:sklearn.manifold.tests.test_t_sne.gradient_norm_values->numpy.array(gradient_norm_values)
A:sklearn.manifold.tests.test_t_sne.n_smaller_gradient_norms->len(gradient_norm_values[gradient_norm_values <= min_grad_norm])
A:sklearn.manifold.tests.test_t_sne.(_, _, error)->line.partition('error = ')
A:sklearn.manifold.tests.test_t_sne.(error, _, _)->error.partition(',')
A:sklearn.manifold.tests.test_t_sne.Y->TSNE(n_components=2, method=method, learning_rate=1.0, init='random', random_state=0, n_iter=251, perplexity=30.0, angle=0).fit_transform(X_2d_grid)
A:sklearn.manifold.tests.test_t_sne.try_name->'{}_{}'.format(method, seed)
A:sklearn.manifold.tests.test_t_sne.nn->NearestNeighbors(n_neighbors=1).fit(Y)
A:sklearn.manifold.tests.test_t_sne.dist_to_nn->NearestNeighbors(n_neighbors=1).fit(Y).kneighbors(return_distance=True)[0].ravel()
A:sklearn.manifold.tests.test_t_sne.X_embeddeds[method]->TSNE(n_components=2, method=method, learning_rate=1.0, init='random', random_state=0, n_iter=251, perplexity=30.0, angle=0).fit_transform(X)
A:sklearn.manifold.tests.test_t_sne.X_transformed_tsne->TSNE(metric=metric, n_components=n_components_embedding, random_state=0).fit_transform(X)
A:sklearn.manifold.tests.test_t_sne.X_transformed_tsne_precomputed->TSNE(metric='precomputed', n_components=n_components_embedding, random_state=0).fit_transform(dist_func(X))
sklearn.manifold.tests.test_t_sne._run_answer_test(pos_input,pos_output,neighbors,grad_output,verbose=False,perplexity=0.1,skip_num_points=0)
sklearn.manifold.tests.test_t_sne.assert_uniform_grid(Y,try_name=None)
sklearn.manifold.tests.test_t_sne.check_uniform_grid(method,seeds=[0,1,2],n_iter=1000)
sklearn.manifold.tests.test_t_sne.test_64bit()
sklearn.manifold.tests.test_t_sne.test_accessible_kl_divergence()
sklearn.manifold.tests.test_t_sne.test_angle_out_of_range_checks()
sklearn.manifold.tests.test_t_sne.test_answer_gradient_four_points()
sklearn.manifold.tests.test_t_sne.test_answer_gradient_two_points()
sklearn.manifold.tests.test_t_sne.test_barnes_hut_angle()
sklearn.manifold.tests.test_t_sne.test_bh_match_exact()
sklearn.manifold.tests.test_t_sne.test_binary_perplexity_stability()
sklearn.manifold.tests.test_t_sne.test_binary_search()
sklearn.manifold.tests.test_t_sne.test_binary_search_neighbors()
sklearn.manifold.tests.test_t_sne.test_chebyshev_metric()
sklearn.manifold.tests.test_t_sne.test_distance_not_available()
sklearn.manifold.tests.test_t_sne.test_early_exaggeration_too_small()
sklearn.manifold.tests.test_t_sne.test_early_exaggeration_used()
sklearn.manifold.tests.test_t_sne.test_fit_csr_matrix()
sklearn.manifold.tests.test_t_sne.test_gradient()
sklearn.manifold.tests.test_t_sne.test_gradient_descent_stops()
sklearn.manifold.tests.test_t_sne.test_init_ndarray()
sklearn.manifold.tests.test_t_sne.test_init_ndarray_precomputed()
sklearn.manifold.tests.test_t_sne.test_init_not_available()
sklearn.manifold.tests.test_t_sne.test_method_not_available()
sklearn.manifold.tests.test_t_sne.test_min_grad_norm()
sklearn.manifold.tests.test_t_sne.test_n_components_range()
sklearn.manifold.tests.test_t_sne.test_n_iter_used()
sklearn.manifold.tests.test_t_sne.test_n_iter_without_progress()
sklearn.manifold.tests.test_t_sne.test_no_sparse_on_barnes_hut()
sklearn.manifold.tests.test_t_sne.test_non_positive_computed_distances()
sklearn.manifold.tests.test_t_sne.test_non_positive_precomputed_distances()
sklearn.manifold.tests.test_t_sne.test_non_square_precomputed_distances()
sklearn.manifold.tests.test_t_sne.test_optimization_minimizes_kl_divergence()
sklearn.manifold.tests.test_t_sne.test_pca_initialization_not_compatible_with_precomputed_kernel()
sklearn.manifold.tests.test_t_sne.test_preserve_trustworthiness_approximately()
sklearn.manifold.tests.test_t_sne.test_preserve_trustworthiness_approximately_with_precomputed_distances()
sklearn.manifold.tests.test_t_sne.test_reduction_to_one_component()
sklearn.manifold.tests.test_t_sne.test_skip_num_points_gradient()
sklearn.manifold.tests.test_t_sne.test_too_few_iterations()
sklearn.manifold.tests.test_t_sne.test_trustworthiness()
sklearn.manifold.tests.test_t_sne.test_tsne_with_different_distance_metrics()
sklearn.manifold.tests.test_t_sne.test_uniform_grid()
sklearn.manifold.tests.test_t_sne.test_verbose()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/manifold/tests/test_spectral_embedding.py----------------------------------------
A:sklearn.manifold.tests.test_spectral_embedding.centers->numpy.array([[0.0, 5.0, 0.0, 0.0, 0.0], [0.0, 0.0, 4.0, 0.0, 0.0], [1.0, 0.0, 0.0, 5.0, 1.0]])
A:sklearn.manifold.tests.test_spectral_embedding.(S, true_labels)->make_blobs(n_samples=n_samples, centers=centers, cluster_std=1.0, random_state=42)
A:sklearn.manifold.tests.test_spectral_embedding.rng->numpy.random.RandomState(42)
A:sklearn.manifold.tests.test_spectral_embedding.p->numpy.random.RandomState(42).permutation(n_samples)
A:sklearn.manifold.tests.test_spectral_embedding.source->numpy.random.RandomState(42).randint(min_idx, max_idx, size=n_random_connections)
A:sklearn.manifold.tests.test_spectral_embedding.target->numpy.random.RandomState(42).randint(min_idx, max_idx, size=n_random_connections)
A:sklearn.manifold.tests.test_spectral_embedding.(row_idx, column_idx)->tuple(np.array(connections).T)
A:sklearn.manifold.tests.test_spectral_embedding.data->numpy.random.RandomState(36).randn(10, 30)
A:sklearn.manifold.tests.test_spectral_embedding.affinity->numpy.zeros(shape=[n_sample * 2, n_sample * 2])
A:sklearn.manifold.tests.test_spectral_embedding.component_1->_graph_connected_component(affinity, p[start])
A:sklearn.manifold.tests.test_spectral_embedding.component_2->_graph_connected_component(affinity, p[stop - 1])
A:sklearn.manifold.tests.test_spectral_embedding.random_state->numpy.random.RandomState(36)
A:sklearn.manifold.tests.test_spectral_embedding.component->_graph_connected_component(affinity, -1)
A:sklearn.manifold.tests.test_spectral_embedding.true_label->numpy.zeros(shape=2 * n_sample)
A:sklearn.manifold.tests.test_spectral_embedding.se_precomp->SpectralEmbedding(n_components=2, affinity='precomputed', random_state=np.random.RandomState(seed))
A:sklearn.manifold.tests.test_spectral_embedding.embedded_coordinate->SpectralEmbedding(n_components=2, affinity='precomputed', random_state=np.random.RandomState(seed)).fit_transform(affinity.astype(np.float32))
A:sklearn.manifold.tests.test_spectral_embedding.label_->numpy.array(embedded_coordinate.ravel() < 0, dtype='float')
A:sklearn.manifold.tests.test_spectral_embedding.se_rbf->SpectralEmbedding(n_components=n_clusters, affinity='rbf', random_state=random_state)
A:sklearn.manifold.tests.test_spectral_embedding.embed_precomp->SpectralEmbedding(n_components=2, affinity='precomputed', random_state=np.random.RandomState(seed)).fit_transform(rbf_kernel(S, gamma=gamma))
A:sklearn.manifold.tests.test_spectral_embedding.embed_rbf->SpectralEmbedding(n_components=n_clusters, affinity='rbf', random_state=random_state).fit_transform(S)
A:sklearn.manifold.tests.test_spectral_embedding.kern->rbf_kernel(S, gamma=gamma)
A:sklearn.manifold.tests.test_spectral_embedding.se_callable->SpectralEmbedding(n_components=2, affinity=lambda x: rbf_kernel(x, gamma=gamma), gamma=gamma, random_state=np.random.RandomState(seed))
A:sklearn.manifold.tests.test_spectral_embedding.embed_callable->SpectralEmbedding(n_components=2, affinity=lambda x: rbf_kernel(x, gamma=gamma), gamma=gamma, random_state=np.random.RandomState(seed)).fit_transform(S)
A:sklearn.manifold.tests.test_spectral_embedding.se_amg->SpectralEmbedding(n_components=2, affinity='nearest_neighbors', eigen_solver='amg', n_neighbors=5, random_state=np.random.RandomState(seed))
A:sklearn.manifold.tests.test_spectral_embedding.se_arpack->SpectralEmbedding(n_components=2, affinity='nearest_neighbors', eigen_solver='arpack', n_neighbors=5, random_state=np.random.RandomState(seed))
A:sklearn.manifold.tests.test_spectral_embedding.embed_amg->SpectralEmbedding(n_components=2, affinity='nearest_neighbors', eigen_solver='amg', n_neighbors=5, random_state=np.random.RandomState(seed)).fit_transform(S)
A:sklearn.manifold.tests.test_spectral_embedding.embed_arpack->SpectralEmbedding(n_components=2, affinity='nearest_neighbors', eigen_solver='arpack', n_neighbors=5, random_state=np.random.RandomState(seed)).fit_transform(S)
A:sklearn.manifold.tests.test_spectral_embedding.se_knn->SpectralEmbedding(n_components=n_clusters, affinity='nearest_neighbors', n_neighbors=5, random_state=random_state)
A:sklearn.manifold.tests.test_spectral_embedding.km->KMeans(n_clusters=n_clusters, random_state=random_state)
A:sklearn.manifold.tests.test_spectral_embedding.se->SpectralEmbedding(n_components=1, affinity='<unknown>', random_state=np.random.RandomState(seed))
A:sklearn.manifold.tests.test_spectral_embedding.graph->numpy.array([[1, 1, 0, 0, 0], [1, 1, 1, 0, 0], [0, 1, 1, 1, 0], [0, 0, 1, 1, 1], [0, 0, 0, 1, 1]])
A:sklearn.manifold.tests.test_spectral_embedding.sims->rbf_kernel(data)
A:sklearn.manifold.tests.test_spectral_embedding.embedding_1->spectral_embedding(sims, norm_laplacian=False, n_components=n_components, drop_first=False)
A:sklearn.manifold.tests.test_spectral_embedding.embedding_2->spectral_embedding(sims)
A:sklearn.manifold.tests.test_spectral_embedding.(laplacian, dd)->scipy.sparse.csgraph.laplacian(sims, normed=False, return_diag=True)
A:sklearn.manifold.tests.test_spectral_embedding.(_, diffusion_map)->eigh(laplacian)
sklearn.manifold.tests.test_spectral_embedding._check_with_col_sign_flipping(A,B,tol=0.0)
sklearn.manifold.tests.test_spectral_embedding.test_connectivity(seed=36)
sklearn.manifold.tests.test_spectral_embedding.test_pipeline_spectral_clustering(seed=36)
sklearn.manifold.tests.test_spectral_embedding.test_sparse_graph_connected_component()
sklearn.manifold.tests.test_spectral_embedding.test_spectral_embedding_amg_solver(seed=36)
sklearn.manifold.tests.test_spectral_embedding.test_spectral_embedding_callable_affinity(seed=36)
sklearn.manifold.tests.test_spectral_embedding.test_spectral_embedding_deterministic()
sklearn.manifold.tests.test_spectral_embedding.test_spectral_embedding_precomputed_affinity(seed=36)
sklearn.manifold.tests.test_spectral_embedding.test_spectral_embedding_two_components(seed=36)
sklearn.manifold.tests.test_spectral_embedding.test_spectral_embedding_unknown_affinity(seed=36)
sklearn.manifold.tests.test_spectral_embedding.test_spectral_embedding_unknown_eigensolver(seed=36)
sklearn.manifold.tests.test_spectral_embedding.test_spectral_embedding_unnormalized()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/manifold/tests/test_mds.py----------------------------------------
A:sklearn.manifold.tests.test_mds.sim->numpy.array([[0, 5, 3, 4], [5, 0, 2, 2], [3, 2, 0, 1], [4, 2, 1, 0]])
A:sklearn.manifold.tests.test_mds.Z->numpy.array([[-0.266, -0.539], [0.016, -0.238], [-0.2, 0.524]])
A:sklearn.manifold.tests.test_mds.(X, _)->sklearn.manifold.mds.smacof(sim, init=Z, n_components=2, max_iter=1, n_init=1)
A:sklearn.manifold.tests.test_mds.X_true->numpy.array([[-1.415, -2.471], [1.633, 1.107], [0.249, -0.067], [-0.468, 1.431]])
A:sklearn.manifold.tests.test_mds.mds_clf->sklearn.manifold.mds.MDS(metric=False, n_jobs=3, dissimilarity='precomputed')
sklearn.manifold.tests.test_mds.test_MDS()
sklearn.manifold.tests.test_mds.test_smacof()
sklearn.manifold.tests.test_mds.test_smacof_error()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/manifold/tests/test_isomap.py----------------------------------------
A:sklearn.manifold.tests.test_isomap.X->numpy.concatenate((X, noise), 1)
A:sklearn.manifold.tests.test_isomap.G->sklearn.neighbors.kneighbors_graph(X, n_neighbors, mode='distance').toarray()
A:sklearn.manifold.tests.test_isomap.clf->sklearn.pipeline.Pipeline([('isomap', manifold.Isomap()), ('clf', neighbors.KNeighborsClassifier())])
A:sklearn.manifold.tests.test_isomap.G_iso->sklearn.neighbors.kneighbors_graph(clf.embedding_, n_neighbors, mode='distance').toarray()
A:sklearn.manifold.tests.test_isomap.rng->numpy.random.RandomState(0)
A:sklearn.manifold.tests.test_isomap.centerer->sklearn.preprocessing.KernelCenterer()
A:sklearn.manifold.tests.test_isomap.K->sklearn.preprocessing.KernelCenterer().fit_transform(-0.5 * G ** 2)
A:sklearn.manifold.tests.test_isomap.K_iso->sklearn.preprocessing.KernelCenterer().fit_transform(-0.5 * G_iso ** 2)
A:sklearn.manifold.tests.test_isomap.(X, y)->sklearn.datasets.make_blobs(random_state=0)
A:sklearn.manifold.tests.test_isomap.iso->sklearn.manifold.Isomap(n_components, 2)
A:sklearn.manifold.tests.test_isomap.X_iso->sklearn.manifold.Isomap(n_components, 2).fit_transform(X)
A:sklearn.manifold.tests.test_isomap.X_iso2->sklearn.manifold.Isomap(n_components, 2).transform(X + noise)
A:sklearn.manifold.tests.test_isomap.model->sklearn.manifold.Isomap()
sklearn.manifold.tests.test_isomap.test_isomap_clone_bug()
sklearn.manifold.tests.test_isomap.test_isomap_reconstruction_error()
sklearn.manifold.tests.test_isomap.test_isomap_simple_grid()
sklearn.manifold.tests.test_isomap.test_pipeline()
sklearn.manifold.tests.test_isomap.test_transform()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/preprocessing/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/preprocessing/_function_transformer.py----------------------------------------
A:sklearn.preprocessing._function_transformer.X->check_array(X, self.accept_sparse)
sklearn.preprocessing.FunctionTransformer(self,func=None,inverse_func=None,validate=True,accept_sparse=False,pass_y='deprecated',kw_args=None,inv_kw_args=None)
sklearn.preprocessing.FunctionTransformer._transform(self,X,y=None,func=None,kw_args=None)
sklearn.preprocessing.FunctionTransformer.fit(self,X,y=None)
sklearn.preprocessing.FunctionTransformer.inverse_transform(self,X,y='deprecated')
sklearn.preprocessing.FunctionTransformer.transform(self,X,y='deprecated')
sklearn.preprocessing._function_transformer.FunctionTransformer(self,func=None,inverse_func=None,validate=True,accept_sparse=False,pass_y='deprecated',kw_args=None,inv_kw_args=None)
sklearn.preprocessing._function_transformer.FunctionTransformer.__init__(self,func=None,inverse_func=None,validate=True,accept_sparse=False,pass_y='deprecated',kw_args=None,inv_kw_args=None)
sklearn.preprocessing._function_transformer.FunctionTransformer._transform(self,X,y=None,func=None,kw_args=None)
sklearn.preprocessing._function_transformer.FunctionTransformer.fit(self,X,y=None)
sklearn.preprocessing._function_transformer.FunctionTransformer.inverse_transform(self,X,y='deprecated')
sklearn.preprocessing._function_transformer.FunctionTransformer.transform(self,X,y='deprecated')
sklearn.preprocessing._function_transformer._identity(X)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/preprocessing/label.py----------------------------------------
A:sklearn.preprocessing.label.y->y.toarray().toarray()
A:sklearn.preprocessing.label.self.classes_->numpy.empty(len(classes), dtype=dtype)
A:sklearn.preprocessing.label.(self.classes_, y)->numpy.unique(y, return_inverse=True)
A:sklearn.preprocessing.label.classes->sorted(set(itertools.chain.from_iterable(y)))
A:sklearn.preprocessing.label.diff->numpy.setdiff1d(y, np.arange(len(self.classes_)))
A:sklearn.preprocessing.label.self.y_type_->type_of_target(y)
A:sklearn.preprocessing.label.self.sparse_input_->scipy.sparse.issparse(y)
A:sklearn.preprocessing.label.y_is_multilabel->type_of_target(y).startswith('multilabel')
A:sklearn.preprocessing.label.y_inv->y_inv.toarray().toarray()
A:sklearn.preprocessing.label.y_type->type_of_target(y)
A:sklearn.preprocessing.label.n_classes->len(classes)
A:sklearn.preprocessing.label.Y->Y[:, -1].reshape((-1, 1))
A:sklearn.preprocessing.label.sorted_class->numpy.sort(classes)
A:sklearn.preprocessing.label.y_in_classes->numpy.in1d(y, classes)
A:sklearn.preprocessing.label.indices->array.array('i')
A:sklearn.preprocessing.label.indptr->array.array('i', [0])
A:sklearn.preprocessing.label.data->numpy.ones(len(indices), dtype=int)
A:sklearn.preprocessing.label.Y.data->Y[:, -1].reshape((-1, 1)).data.astype(int, copy=False)
A:sklearn.preprocessing.label.outputs->numpy.arange(n_outputs)
A:sklearn.preprocessing.label.row_nnz->numpy.diff(y.indptr)
A:sklearn.preprocessing.label.y_data_repeated_max->numpy.repeat(row_max, row_nnz)
A:sklearn.preprocessing.label.y_i_all_argmax->numpy.append(y_i_all_argmax, [len(y.data)])
A:sklearn.preprocessing.label.index_first_argmax->numpy.searchsorted(y_i_all_argmax, y.indptr[:-1])
A:sklearn.preprocessing.label.y_ind_ext->numpy.append(y.indices, [0])
A:sklearn.preprocessing.label.y.data->numpy.array(y.data > threshold, dtype=np.int)
A:sklearn.preprocessing.label.class_mapping->numpy.empty(len(tmp), dtype=dtype)
A:sklearn.preprocessing.label.yt->yt.tocsr().tocsr()
A:sklearn.preprocessing.label.tmp->sorted(class_mapping, key=class_mapping.get)
A:sklearn.preprocessing.label.(self.classes_, inverse)->numpy.unique(class_mapping, return_inverse=True)
A:sklearn.preprocessing.label.yt.indices->numpy.array(inverse[yt.indices], dtype=yt.indices.dtype, copy=False)
A:sklearn.preprocessing.label.class_to_index->dict(zip(self.classes_, range(len(self.classes_))))
A:sklearn.preprocessing.label.unexpected->numpy.setdiff1d(yt, [0, 1])
sklearn.preprocessing.LabelBinarizer(self,neg_label=0,pos_label=1,sparse_output=False)
sklearn.preprocessing.LabelBinarizer.fit(self,y)
sklearn.preprocessing.LabelBinarizer.fit_transform(self,y)
sklearn.preprocessing.LabelBinarizer.inverse_transform(self,Y,threshold=None)
sklearn.preprocessing.LabelBinarizer.transform(self,y)
sklearn.preprocessing.LabelEncoder(BaseEstimator,TransformerMixin)
sklearn.preprocessing.LabelEncoder.fit(self,y)
sklearn.preprocessing.LabelEncoder.fit_transform(self,y)
sklearn.preprocessing.LabelEncoder.inverse_transform(self,y)
sklearn.preprocessing.LabelEncoder.transform(self,y)
sklearn.preprocessing.MultiLabelBinarizer(self,classes=None,sparse_output=False)
sklearn.preprocessing.MultiLabelBinarizer._transform(self,y,class_mapping)
sklearn.preprocessing.MultiLabelBinarizer.fit(self,y)
sklearn.preprocessing.MultiLabelBinarizer.fit_transform(self,y)
sklearn.preprocessing.MultiLabelBinarizer.inverse_transform(self,yt)
sklearn.preprocessing.MultiLabelBinarizer.transform(self,y)
sklearn.preprocessing.label.LabelBinarizer(self,neg_label=0,pos_label=1,sparse_output=False)
sklearn.preprocessing.label.LabelBinarizer.__init__(self,neg_label=0,pos_label=1,sparse_output=False)
sklearn.preprocessing.label.LabelBinarizer.fit(self,y)
sklearn.preprocessing.label.LabelBinarizer.fit_transform(self,y)
sklearn.preprocessing.label.LabelBinarizer.inverse_transform(self,Y,threshold=None)
sklearn.preprocessing.label.LabelBinarizer.transform(self,y)
sklearn.preprocessing.label.LabelEncoder(BaseEstimator,TransformerMixin)
sklearn.preprocessing.label.LabelEncoder.fit(self,y)
sklearn.preprocessing.label.LabelEncoder.fit_transform(self,y)
sklearn.preprocessing.label.LabelEncoder.inverse_transform(self,y)
sklearn.preprocessing.label.LabelEncoder.transform(self,y)
sklearn.preprocessing.label.MultiLabelBinarizer(self,classes=None,sparse_output=False)
sklearn.preprocessing.label.MultiLabelBinarizer.__init__(self,classes=None,sparse_output=False)
sklearn.preprocessing.label.MultiLabelBinarizer._transform(self,y,class_mapping)
sklearn.preprocessing.label.MultiLabelBinarizer.fit(self,y)
sklearn.preprocessing.label.MultiLabelBinarizer.fit_transform(self,y)
sklearn.preprocessing.label.MultiLabelBinarizer.inverse_transform(self,yt)
sklearn.preprocessing.label.MultiLabelBinarizer.transform(self,y)
sklearn.preprocessing.label._inverse_binarize_multiclass(y,classes)
sklearn.preprocessing.label._inverse_binarize_thresholding(y,output_type,classes,threshold)
sklearn.preprocessing.label.label_binarize(y,classes,neg_label=0,pos_label=1,sparse_output=False)
sklearn.preprocessing.label_binarize(y,classes,neg_label=0,pos_label=1,sparse_output=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/preprocessing/data.py----------------------------------------
A:sklearn.preprocessing.data.scale->scale.copy().copy()
A:sklearn.preprocessing.data.X->self._check_inputs(X, accept_sparse_negative=True)
A:sklearn.preprocessing.data.(_, var)->mean_variance_axis(X, axis=0)
A:sklearn.preprocessing.data.var->_handle_zeros_in_scale(var, copy=False)
A:sklearn.preprocessing.data.mean_->numpy.mean(X, axis)
A:sklearn.preprocessing.data.scale_->_handle_zeros_in_scale(scale_, copy=False)
A:sklearn.preprocessing.data.Xr->numpy.rollaxis(X, axis)
A:sklearn.preprocessing.data.mean_1->numpy.rollaxis(X, axis).mean(axis=0)
A:sklearn.preprocessing.data.mean_2->numpy.rollaxis(X, axis).mean(axis=0)
A:sklearn.preprocessing.data.data_min->numpy.minimum(self.data_min_, data_min)
A:sklearn.preprocessing.data.data_max->numpy.maximum(self.data_max_, data_max)
A:sklearn.preprocessing.data.s->RobustScaler(with_centering=with_centering, with_scaling=with_scaling, quantile_range=quantile_range, copy=copy)
A:sklearn.preprocessing.data.(self.mean_, self.var_)->mean_variance_axis(X, axis=0)
A:sklearn.preprocessing.data.(self.mean_, self.var_, self.n_samples_seen_)->_incremental_mean_and_var(X, self.mean_, self.var_, self.n_samples_seen_)
A:sklearn.preprocessing.data.self.scale_->_handle_zeros_in_scale(self.scale_, copy=False)
A:sklearn.preprocessing.data.(mins, maxs)->min_max_axis(X, axis=0)
A:sklearn.preprocessing.data.max_abs->numpy.maximum(self.max_abs_, max_abs)
A:sklearn.preprocessing.data.self.center_->numpy.median(X, axis=0)
A:sklearn.preprocessing.data.q->numpy.percentile(X, self.quantile_range, axis=0)
A:sklearn.preprocessing.data.start->int(not include_bias)
A:sklearn.preprocessing.data.combinations->self._combinations(n_features, self.degree, self.interaction_only, self.include_bias)
A:sklearn.preprocessing.data.name->' '.join(('%s^%d' % (input_features[ind], exp) if exp != 1 else input_features[ind] for (ind, exp) in zip(inds, row[inds])))
A:sklearn.preprocessing.data.self.n_output_features_->sum((1 for _ in combinations))
A:sklearn.preprocessing.data.XP->numpy.empty((n_samples, self.n_output_features_), dtype=X.dtype)
A:sklearn.preprocessing.data.XP[:, i]->X[:, c].prod(1)
A:sklearn.preprocessing.data.(_, norms)->min_max_axis(X, 1)
A:sklearn.preprocessing.data.norms_elementwise->_handle_zeros_in_scale(norms, copy=False).repeat(np.diff(X.indptr))
A:sklearn.preprocessing.data.norms->_handle_zeros_in_scale(norms, copy=False)
A:sklearn.preprocessing.data.not_cond->numpy.logical_not(cond)
A:sklearn.preprocessing.data.K->check_array(K, copy=copy, dtype=FLOAT_DTYPES)
A:sklearn.preprocessing.data.col->col.take(subsample_idx, mode='clip').take(subsample_idx, mode='clip')
A:sklearn.preprocessing.data.row->numpy.concatenate((np.arange(n_samples), X.row))
A:sklearn.preprocessing.data.data->numpy.ones(np.sum(mask))
A:sklearn.preprocessing.data.indptr->numpy.concatenate((np.array([0]), indptr))
A:sklearn.preprocessing.data.indices->numpy.cumsum(n_values)
A:sklearn.preprocessing.data.ind->numpy.arange(n_features)
A:sklearn.preprocessing.data.sel->numpy.zeros(n_features, dtype=bool)
A:sklearn.preprocessing.data.not_sel->numpy.logical_not(sel)
A:sklearn.preprocessing.data.n_selected->numpy.sum(sel)
A:sklearn.preprocessing.data.X_sel->transform(X[:, ind[sel]])
A:sklearn.preprocessing.data.n_values->numpy.hstack([[0], n_values])
A:sklearn.preprocessing.data.column_indices->(X + indices[:-1]).ravel()
A:sklearn.preprocessing.data.row_indices->numpy.repeat(np.arange(n_samples, dtype=np.int32), n_features)
A:sklearn.preprocessing.data.out->scipy.sparse.coo_matrix((data, (row_indices, column_indices)), shape=(n_samples, indices[-1]), dtype=self.dtype).tocsr()
A:sklearn.preprocessing.data.mask->(X < self.n_values_).ravel()
A:sklearn.preprocessing.data.references->list(map(lambda x: x * 100, self.references_))
A:sklearn.preprocessing.data.subsample_idx->random_state.choice(n_samples, size=self.subsample, replace=False)
A:sklearn.preprocessing.data.self.quantiles_->numpy.transpose(self.quantiles_)
A:sklearn.preprocessing.data.column_data->numpy.zeros(shape=n_samples, dtype=X.dtype)
A:sklearn.preprocessing.data.column_data[:column_subsample]->random_state.choice(column_nnz_data, size=column_subsample, replace=False)
A:sklearn.preprocessing.data.rng->check_random_state(self.random_state)
A:sklearn.preprocessing.data.self.references_->numpy.linspace(0, 1, self.n_quantiles, endpoint=True)
A:sklearn.preprocessing.data.output_distribution->getattr(stats, output_distribution)
A:sklearn.preprocessing.data.X_col->numpy.clip(X_col, clip_min, clip_max)
A:sklearn.preprocessing.data.clip_min->getattr(stats, output_distribution).ppf(BOUNDS_THRESHOLD - np.spacing(1))
A:sklearn.preprocessing.data.clip_max->getattr(stats, output_distribution).ppf(1 - (BOUNDS_THRESHOLD - np.spacing(1)))
A:sklearn.preprocessing.data.column_slice->slice(X.indptr[feature_idx], X.indptr[feature_idx + 1])
A:sklearn.preprocessing.data.X.data[column_slice]->self._transform_col(X.data[column_slice], self.quantiles_[:, feature_idx], inverse)
A:sklearn.preprocessing.data.X[:, feature_idx]->self._transform_col(X[:, feature_idx], self.quantiles_[:, feature_idx], inverse)
A:sklearn.preprocessing.data.n->QuantileTransformer(n_quantiles=n_quantiles, output_distribution=output_distribution, subsample=subsample, ignore_implicit_zeros=ignore_implicit_zeros, random_state=random_state, copy=copy)
sklearn.preprocessing.Binarizer(self,threshold=0.0,copy=True)
sklearn.preprocessing.Binarizer.fit(self,X,y=None)
sklearn.preprocessing.Binarizer.transform(self,X,y='deprecated',copy=None)
sklearn.preprocessing.KernelCenterer(BaseEstimator,TransformerMixin)
sklearn.preprocessing.KernelCenterer._pairwise(self)
sklearn.preprocessing.KernelCenterer.fit(self,K,y=None)
sklearn.preprocessing.KernelCenterer.transform(self,K,y='deprecated',copy=True)
sklearn.preprocessing.MaxAbsScaler(self,copy=True)
sklearn.preprocessing.MaxAbsScaler._reset(self)
sklearn.preprocessing.MaxAbsScaler.fit(self,X,y=None)
sklearn.preprocessing.MaxAbsScaler.inverse_transform(self,X)
sklearn.preprocessing.MaxAbsScaler.partial_fit(self,X,y=None)
sklearn.preprocessing.MaxAbsScaler.transform(self,X)
sklearn.preprocessing.MinMaxScaler(self,feature_range=(0,1),copy=True)
sklearn.preprocessing.MinMaxScaler._reset(self)
sklearn.preprocessing.MinMaxScaler.fit(self,X,y=None)
sklearn.preprocessing.MinMaxScaler.inverse_transform(self,X)
sklearn.preprocessing.MinMaxScaler.partial_fit(self,X,y=None)
sklearn.preprocessing.MinMaxScaler.transform(self,X)
sklearn.preprocessing.Normalizer(self,norm='l2',copy=True)
sklearn.preprocessing.Normalizer.fit(self,X,y=None)
sklearn.preprocessing.Normalizer.transform(self,X,y='deprecated',copy=None)
sklearn.preprocessing.OneHotEncoder(self,n_values='auto',categorical_features='all',dtype=np.float64,sparse=True,handle_unknown='error')
sklearn.preprocessing.OneHotEncoder._fit_transform(self,X)
sklearn.preprocessing.OneHotEncoder._transform(self,X)
sklearn.preprocessing.OneHotEncoder.fit(self,X,y=None)
sklearn.preprocessing.OneHotEncoder.fit_transform(self,X,y=None)
sklearn.preprocessing.OneHotEncoder.transform(self,X)
sklearn.preprocessing.PolynomialFeatures(self,degree=2,interaction_only=False,include_bias=True)
sklearn.preprocessing.PolynomialFeatures._combinations(n_features,degree,interaction_only,include_bias)
sklearn.preprocessing.PolynomialFeatures.fit(self,X,y=None)
sklearn.preprocessing.PolynomialFeatures.get_feature_names(self,input_features=None)
sklearn.preprocessing.PolynomialFeatures.powers_(self)
sklearn.preprocessing.PolynomialFeatures.transform(self,X)
sklearn.preprocessing.QuantileTransformer(self,n_quantiles=1000,output_distribution='uniform',ignore_implicit_zeros=False,subsample=int(100000.0),random_state=None,copy=True)
sklearn.preprocessing.QuantileTransformer._check_inputs(self,X,accept_sparse_negative=False)
sklearn.preprocessing.QuantileTransformer._check_is_fitted(self,X)
sklearn.preprocessing.QuantileTransformer._dense_fit(self,X,random_state)
sklearn.preprocessing.QuantileTransformer._sparse_fit(self,X,random_state)
sklearn.preprocessing.QuantileTransformer._transform(self,X,inverse=False)
sklearn.preprocessing.QuantileTransformer._transform_col(self,X_col,quantiles,inverse)
sklearn.preprocessing.QuantileTransformer.fit(self,X,y=None)
sklearn.preprocessing.QuantileTransformer.inverse_transform(self,X)
sklearn.preprocessing.QuantileTransformer.transform(self,X)
sklearn.preprocessing.RobustScaler(self,with_centering=True,with_scaling=True,quantile_range=(25.0,75.0),copy=True)
sklearn.preprocessing.RobustScaler._check_array(self,X,copy)
sklearn.preprocessing.RobustScaler.fit(self,X,y=None)
sklearn.preprocessing.RobustScaler.inverse_transform(self,X)
sklearn.preprocessing.RobustScaler.transform(self,X)
sklearn.preprocessing.StandardScaler(self,copy=True,with_mean=True,with_std=True)
sklearn.preprocessing.StandardScaler._reset(self)
sklearn.preprocessing.StandardScaler.fit(self,X,y=None)
sklearn.preprocessing.StandardScaler.inverse_transform(self,X,copy=None)
sklearn.preprocessing.StandardScaler.partial_fit(self,X,y=None)
sklearn.preprocessing.StandardScaler.transform(self,X,y='deprecated',copy=None)
sklearn.preprocessing.add_dummy_feature(X,value=1.0)
sklearn.preprocessing.binarize(X,threshold=0.0,copy=True)
sklearn.preprocessing.data.Binarizer(self,threshold=0.0,copy=True)
sklearn.preprocessing.data.Binarizer.__init__(self,threshold=0.0,copy=True)
sklearn.preprocessing.data.Binarizer.fit(self,X,y=None)
sklearn.preprocessing.data.Binarizer.transform(self,X,y='deprecated',copy=None)
sklearn.preprocessing.data.KernelCenterer(BaseEstimator,TransformerMixin)
sklearn.preprocessing.data.KernelCenterer._pairwise(self)
sklearn.preprocessing.data.KernelCenterer.fit(self,K,y=None)
sklearn.preprocessing.data.KernelCenterer.transform(self,K,y='deprecated',copy=True)
sklearn.preprocessing.data.MaxAbsScaler(self,copy=True)
sklearn.preprocessing.data.MaxAbsScaler.__init__(self,copy=True)
sklearn.preprocessing.data.MaxAbsScaler._reset(self)
sklearn.preprocessing.data.MaxAbsScaler.fit(self,X,y=None)
sklearn.preprocessing.data.MaxAbsScaler.inverse_transform(self,X)
sklearn.preprocessing.data.MaxAbsScaler.partial_fit(self,X,y=None)
sklearn.preprocessing.data.MaxAbsScaler.transform(self,X)
sklearn.preprocessing.data.MinMaxScaler(self,feature_range=(0,1),copy=True)
sklearn.preprocessing.data.MinMaxScaler.__init__(self,feature_range=(0,1),copy=True)
sklearn.preprocessing.data.MinMaxScaler._reset(self)
sklearn.preprocessing.data.MinMaxScaler.fit(self,X,y=None)
sklearn.preprocessing.data.MinMaxScaler.inverse_transform(self,X)
sklearn.preprocessing.data.MinMaxScaler.partial_fit(self,X,y=None)
sklearn.preprocessing.data.MinMaxScaler.transform(self,X)
sklearn.preprocessing.data.Normalizer(self,norm='l2',copy=True)
sklearn.preprocessing.data.Normalizer.__init__(self,norm='l2',copy=True)
sklearn.preprocessing.data.Normalizer.fit(self,X,y=None)
sklearn.preprocessing.data.Normalizer.transform(self,X,y='deprecated',copy=None)
sklearn.preprocessing.data.OneHotEncoder(self,n_values='auto',categorical_features='all',dtype=np.float64,sparse=True,handle_unknown='error')
sklearn.preprocessing.data.OneHotEncoder.__init__(self,n_values='auto',categorical_features='all',dtype=np.float64,sparse=True,handle_unknown='error')
sklearn.preprocessing.data.OneHotEncoder._fit_transform(self,X)
sklearn.preprocessing.data.OneHotEncoder._transform(self,X)
sklearn.preprocessing.data.OneHotEncoder.fit(self,X,y=None)
sklearn.preprocessing.data.OneHotEncoder.fit_transform(self,X,y=None)
sklearn.preprocessing.data.OneHotEncoder.transform(self,X)
sklearn.preprocessing.data.PolynomialFeatures(self,degree=2,interaction_only=False,include_bias=True)
sklearn.preprocessing.data.PolynomialFeatures.__init__(self,degree=2,interaction_only=False,include_bias=True)
sklearn.preprocessing.data.PolynomialFeatures._combinations(n_features,degree,interaction_only,include_bias)
sklearn.preprocessing.data.PolynomialFeatures.fit(self,X,y=None)
sklearn.preprocessing.data.PolynomialFeatures.get_feature_names(self,input_features=None)
sklearn.preprocessing.data.PolynomialFeatures.powers_(self)
sklearn.preprocessing.data.PolynomialFeatures.transform(self,X)
sklearn.preprocessing.data.QuantileTransformer(self,n_quantiles=1000,output_distribution='uniform',ignore_implicit_zeros=False,subsample=int(100000.0),random_state=None,copy=True)
sklearn.preprocessing.data.QuantileTransformer.__init__(self,n_quantiles=1000,output_distribution='uniform',ignore_implicit_zeros=False,subsample=int(100000.0),random_state=None,copy=True)
sklearn.preprocessing.data.QuantileTransformer._check_inputs(self,X,accept_sparse_negative=False)
sklearn.preprocessing.data.QuantileTransformer._check_is_fitted(self,X)
sklearn.preprocessing.data.QuantileTransformer._dense_fit(self,X,random_state)
sklearn.preprocessing.data.QuantileTransformer._sparse_fit(self,X,random_state)
sklearn.preprocessing.data.QuantileTransformer._transform(self,X,inverse=False)
sklearn.preprocessing.data.QuantileTransformer._transform_col(self,X_col,quantiles,inverse)
sklearn.preprocessing.data.QuantileTransformer.fit(self,X,y=None)
sklearn.preprocessing.data.QuantileTransformer.inverse_transform(self,X)
sklearn.preprocessing.data.QuantileTransformer.transform(self,X)
sklearn.preprocessing.data.RobustScaler(self,with_centering=True,with_scaling=True,quantile_range=(25.0,75.0),copy=True)
sklearn.preprocessing.data.RobustScaler.__init__(self,with_centering=True,with_scaling=True,quantile_range=(25.0,75.0),copy=True)
sklearn.preprocessing.data.RobustScaler._check_array(self,X,copy)
sklearn.preprocessing.data.RobustScaler.fit(self,X,y=None)
sklearn.preprocessing.data.RobustScaler.inverse_transform(self,X)
sklearn.preprocessing.data.RobustScaler.transform(self,X)
sklearn.preprocessing.data.StandardScaler(self,copy=True,with_mean=True,with_std=True)
sklearn.preprocessing.data.StandardScaler.__init__(self,copy=True,with_mean=True,with_std=True)
sklearn.preprocessing.data.StandardScaler._reset(self)
sklearn.preprocessing.data.StandardScaler.fit(self,X,y=None)
sklearn.preprocessing.data.StandardScaler.inverse_transform(self,X,copy=None)
sklearn.preprocessing.data.StandardScaler.partial_fit(self,X,y=None)
sklearn.preprocessing.data.StandardScaler.transform(self,X,y='deprecated',copy=None)
sklearn.preprocessing.data._handle_zeros_in_scale(scale,copy=True)
sklearn.preprocessing.data._transform_selected(X,transform,selected='all',copy=True)
sklearn.preprocessing.data.add_dummy_feature(X,value=1.0)
sklearn.preprocessing.data.binarize(X,threshold=0.0,copy=True)
sklearn.preprocessing.data.maxabs_scale(X,axis=0,copy=True)
sklearn.preprocessing.data.minmax_scale(X,feature_range=(0,1),axis=0,copy=True)
sklearn.preprocessing.data.normalize(X,norm='l2',axis=1,copy=True,return_norm=False)
sklearn.preprocessing.data.quantile_transform(X,axis=0,n_quantiles=1000,output_distribution='uniform',ignore_implicit_zeros=False,subsample=int(100000.0),random_state=None,copy=False)
sklearn.preprocessing.data.robust_scale(X,axis=0,with_centering=True,with_scaling=True,quantile_range=(25.0,75.0),copy=True)
sklearn.preprocessing.data.scale(X,axis=0,with_mean=True,with_std=True,copy=True)
sklearn.preprocessing.maxabs_scale(X,axis=0,copy=True)
sklearn.preprocessing.minmax_scale(X,feature_range=(0,1),axis=0,copy=True)
sklearn.preprocessing.normalize(X,norm='l2',axis=1,copy=True,return_norm=False)
sklearn.preprocessing.quantile_transform(X,axis=0,n_quantiles=1000,output_distribution='uniform',ignore_implicit_zeros=False,subsample=int(100000.0),random_state=None,copy=False)
sklearn.preprocessing.robust_scale(X,axis=0,with_centering=True,with_scaling=True,quantile_range=(25.0,75.0),copy=True)
sklearn.preprocessing.scale(X,axis=0,with_mean=True,with_std=True,copy=True)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/preprocessing/imputation.py----------------------------------------
A:sklearn.preprocessing.imputation.mode->scipy.stats.mode(array)
A:sklearn.preprocessing.imputation.X->X.toarray().toarray()
A:sklearn.preprocessing.imputation.self.statistics_->self._dense_fit(X, self.strategy, self.missing_values, self.axis)
A:sklearn.preprocessing.imputation.n_zeros_axis->numpy.zeros(X.shape[not axis], dtype=int)
A:sklearn.preprocessing.imputation.mask_missing_values->_get_mask(X.data, missing_values)
A:sklearn.preprocessing.imputation.mask_valids->numpy.hsplit(np.logical_not(mask_missing_values), X.indptr[1:-1])
A:sklearn.preprocessing.imputation.new_data->X.toarray().toarray().data.copy()
A:sklearn.preprocessing.imputation.sums->X.toarray().toarray().sum(axis=axis)
A:sklearn.preprocessing.imputation.mask_non_zeros->scipy.sparse.csc_matrix((mask_valids.astype(np.float64), X.indices, X.indptr), copy=False)
A:sklearn.preprocessing.imputation.s->scipy.sparse.csc_matrix((mask_valids.astype(np.float64), X.indices, X.indptr), copy=False).sum(axis=0)
A:sklearn.preprocessing.imputation.n_non_missing->numpy.diff(X.indptr)
A:sklearn.preprocessing.imputation.columns_all->numpy.hsplit(X.data, X.indptr[1:-1])
A:sklearn.preprocessing.imputation.median->numpy.ma.getdata(median_masked)
A:sklearn.preprocessing.imputation.median[i]->_get_median(column, n_zeros_axis[i])
A:sklearn.preprocessing.imputation.most_frequent->numpy.empty(X.shape[0])
A:sklearn.preprocessing.imputation.most_frequent[i]->_most_frequent(row, np.nan, 0)
A:sklearn.preprocessing.imputation.mask->_get_mask(X, self.missing_values)
A:sklearn.preprocessing.imputation.masked_X->numpy.ma.masked_array(X, mask=mask)
A:sklearn.preprocessing.imputation.mean_masked->numpy.ma.mean(masked_X, axis=axis)
A:sklearn.preprocessing.imputation.mean->numpy.ma.getdata(mean_masked)
A:sklearn.preprocessing.imputation.masked_X.mask->numpy.logical_or(masked_X.mask, np.isnan(X))
A:sklearn.preprocessing.imputation.median_masked->numpy.ma.median(masked_X, axis=axis)
A:sklearn.preprocessing.imputation.row_mask->numpy.logical_not(row_mask).astype(np.bool)
A:sklearn.preprocessing.imputation.statistics->self._dense_fit(X, self.strategy, self.missing_values, self.axis)
A:sklearn.preprocessing.imputation.invalid_mask->numpy.isnan(statistics)
A:sklearn.preprocessing.imputation.valid_mask->numpy.logical_not(invalid_mask)
A:sklearn.preprocessing.imputation.X.data[mask]->valid_statistics[indexes].astype(X.dtype, copy=False)
A:sklearn.preprocessing.imputation.n_missing->numpy.sum(mask, axis=self.axis)
A:sklearn.preprocessing.imputation.values->numpy.repeat(valid_statistics, n_missing)
sklearn.preprocessing.Imputer(self,missing_values='NaN',strategy='mean',axis=0,verbose=0,copy=True)
sklearn.preprocessing.Imputer._dense_fit(self,X,strategy,missing_values,axis)
sklearn.preprocessing.Imputer._sparse_fit(self,X,strategy,missing_values,axis)
sklearn.preprocessing.Imputer.fit(self,X,y=None)
sklearn.preprocessing.Imputer.transform(self,X)
sklearn.preprocessing.imputation.Imputer(self,missing_values='NaN',strategy='mean',axis=0,verbose=0,copy=True)
sklearn.preprocessing.imputation.Imputer.__init__(self,missing_values='NaN',strategy='mean',axis=0,verbose=0,copy=True)
sklearn.preprocessing.imputation.Imputer._dense_fit(self,X,strategy,missing_values,axis)
sklearn.preprocessing.imputation.Imputer._sparse_fit(self,X,strategy,missing_values,axis)
sklearn.preprocessing.imputation.Imputer.fit(self,X,y=None)
sklearn.preprocessing.imputation.Imputer.transform(self,X)
sklearn.preprocessing.imputation._get_mask(X,value_to_mask)
sklearn.preprocessing.imputation._most_frequent(array,extra_value,n_repeat)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/preprocessing/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/preprocessing/tests/test_label.py----------------------------------------
A:sklearn.preprocessing.tests.test_label.iris->sklearn.datasets.load_iris()
A:sklearn.preprocessing.tests.test_label.a->a.toarray().toarray()
A:sklearn.preprocessing.tests.test_label.lb->LabelBinarizer(neg_label=neg_label, pos_label=pos_label, sparse_output=sparse_output)
A:sklearn.preprocessing.tests.test_label.got->_inverse_binarize_multiclass(csr_matrix([[0, 1, 0], [-1, 0, -1], [0, 0, 0]]), np.arange(3))
A:sklearn.preprocessing.tests.test_label.to_invert->numpy.array([[1, 0], [0, 1], [0, 1], [1, 0]])
A:sklearn.preprocessing.tests.test_label.expected->numpy.array([[3, 0], [0, 3], [3, 0]])[:, 1].reshape((-1, 1))
A:sklearn.preprocessing.tests.test_label.inp->iter(inp)
A:sklearn.preprocessing.tests.test_label.one_class->numpy.array([0, 0, 0, 0])
A:sklearn.preprocessing.tests.test_label.le->LabelEncoder()
A:sklearn.preprocessing.tests.test_label.ret->LabelEncoder().fit_transform(['paris', 'paris', 'tokyo', 'amsterdam'])
A:sklearn.preprocessing.tests.test_label.indicator_mat->numpy.array([[1, 1]])
A:sklearn.preprocessing.tests.test_label.inverse->inputs[0]()
A:sklearn.preprocessing.tests.test_label.mlb->MultiLabelBinarizer()
A:sklearn.preprocessing.tests.test_label.Y->numpy.array([[1, 1], [1, 0], [0, 0]])
A:sklearn.preprocessing.tests.test_label.tuple_classes->numpy.empty(3, dtype=object)
A:sklearn.preprocessing.tests.test_label.out->label_binarize([0, 1, 2, 3], classes=[3, 2, 0, 1])
A:sklearn.preprocessing.tests.test_label.binarized->LabelBinarizer(neg_label=neg_label, pos_label=pos_label, sparse_output=sparse_output).fit_transform(y)
A:sklearn.preprocessing.tests.test_label.y_type->type_of_target(y)
A:sklearn.preprocessing.tests.test_label.inversed->_inverse_binarize_thresholding(binarized, output_type=y_type, classes=classes, threshold=(neg_label + pos_label) / 2.0)
A:sklearn.preprocessing.tests.test_label.inverse_output->LabelBinarizer(neg_label=neg_label, pos_label=pos_label, sparse_output=sparse_output).inverse_transform(binarized)
A:sklearn.preprocessing.tests.test_label.y_ind->numpy.array([[0, 1, 0], [1, 1, 1], [0, 0, 0]])
sklearn.preprocessing.tests.test_label.check_binarized_results(y,classes,pos_label,neg_label,expected)
sklearn.preprocessing.tests.test_label.test_invalid_input_label_binarize()
sklearn.preprocessing.tests.test_label.test_inverse_binarize_multiclass()
sklearn.preprocessing.tests.test_label.test_label_binarize_binary()
sklearn.preprocessing.tests.test_label.test_label_binarize_multiclass()
sklearn.preprocessing.tests.test_label.test_label_binarize_multilabel()
sklearn.preprocessing.tests.test_label.test_label_binarize_with_class_order()
sklearn.preprocessing.tests.test_label.test_label_binarizer()
sklearn.preprocessing.tests.test_label.test_label_binarizer_errors()
sklearn.preprocessing.tests.test_label.test_label_binarizer_set_label_encoding()
sklearn.preprocessing.tests.test_label.test_label_binarizer_unseen_labels()
sklearn.preprocessing.tests.test_label.test_label_encoder()
sklearn.preprocessing.tests.test_label.test_label_encoder_errors()
sklearn.preprocessing.tests.test_label.test_label_encoder_fit_transform()
sklearn.preprocessing.tests.test_label.test_multilabel_binarizer()
sklearn.preprocessing.tests.test_label.test_multilabel_binarizer_empty_sample()
sklearn.preprocessing.tests.test_label.test_multilabel_binarizer_given_classes()
sklearn.preprocessing.tests.test_label.test_multilabel_binarizer_inverse_validation()
sklearn.preprocessing.tests.test_label.test_multilabel_binarizer_non_integer_labels()
sklearn.preprocessing.tests.test_label.test_multilabel_binarizer_non_unique()
sklearn.preprocessing.tests.test_label.test_multilabel_binarizer_same_length_sequence()
sklearn.preprocessing.tests.test_label.test_multilabel_binarizer_unknown_class()
sklearn.preprocessing.tests.test_label.test_sparse_output_multilabel_binarizer()
sklearn.preprocessing.tests.test_label.toarray(a)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/preprocessing/tests/test_function_transformer.py----------------------------------------
A:sklearn.preprocessing.tests.test_function_transformer.X->numpy.array([1, 4, 9, 16]).reshape((2, 2))
A:sklearn.preprocessing.tests.test_function_transformer.y->object()
A:sklearn.preprocessing.tests.test_function_transformer.transformed->assert_warns_message(DeprecationWarning, 'pass_y is deprecated', FunctionTransformer(_make_func(args_store, kwargs_store), pass_y=True).transform, X, y)
A:sklearn.preprocessing.tests.test_function_transformer.F->FunctionTransformer(func=np.sqrt, inverse_func=np.around, inv_kw_args=dict(decimals=3))
A:sklearn.preprocessing.tests.test_function_transformer.F.kw_args->dict(decimals=1)
sklearn.preprocessing.tests.test_function_transformer._make_func(args_store,kwargs_store,func=lambdaX,*a,**k:X)
sklearn.preprocessing.tests.test_function_transformer.test_delegate_to_func()
sklearn.preprocessing.tests.test_function_transformer.test_inverse_transform()
sklearn.preprocessing.tests.test_function_transformer.test_kw_arg()
sklearn.preprocessing.tests.test_function_transformer.test_kw_arg_reset()
sklearn.preprocessing.tests.test_function_transformer.test_kw_arg_update()
sklearn.preprocessing.tests.test_function_transformer.test_np_log()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/preprocessing/tests/test_imputation.py----------------------------------------
A:sklearn.preprocessing.tests.test_imputation.imputer->Imputer(missing_values=0, strategy='mean', copy=False, axis=1)
A:sklearn.preprocessing.tests.test_imputation.X_trans->X_trans.toarray().toarray()
A:sklearn.preprocessing.tests.test_imputation.X->sparse_random_matrix(5, 5, density=0.75, random_state=0).copy()
A:sklearn.preprocessing.tests.test_imputation.X_imputed->Imputer(missing_values=0, strategy='mean', copy=False, axis=1).fit_transform(sparse.csr_matrix(X))
A:sklearn.preprocessing.tests.test_imputation.X_imputed_mean->numpy.array([[3, 5], [1, 3], [2, 7], [6, 13]])
A:sklearn.preprocessing.tests.test_imputation.X_imputed_median->numpy.array([[0, 0, 0], [5, 5, 5], [0, 0, 0], [-5, 0, -2.5], [0, 5, 2.5], [4, 5, 4.5], [-4, -5, -4.5], [-1, 2, 0.5]]).transpose()
A:sklearn.preprocessing.tests.test_imputation.rng->numpy.random.RandomState(0)
A:sklearn.preprocessing.tests.test_imputation.zeros->numpy.zeros(shape[0])
A:sklearn.preprocessing.tests.test_imputation.values->numpy.arange(1, shape[0] + 1)
A:sklearn.preprocessing.tests.test_imputation.X_true->numpy.array([[2, 0, 5], [2, 3, 3], [1, 3, 3], [2, 3, 7]])
A:sklearn.preprocessing.tests.test_imputation.true_statistics->numpy.empty(shape[1])
A:sklearn.preprocessing.tests.test_imputation.nb_missing_values->max(shape[0] + dec * dec - (j + dec) * (j + dec), 0)
A:sklearn.preprocessing.tests.test_imputation.p->numpy.repeat(test_missing_values, nb_missing_values)
A:sklearn.preprocessing.tests.test_imputation.true_statistics[j]->true_value_fun(z, v, p)
A:sklearn.preprocessing.tests.test_imputation.X[:, j]->numpy.hstack((v, z, p))
A:sklearn.preprocessing.tests.test_imputation.X_true[:, j]->numpy.hstack((v, z, np.repeat(true_statistics[j], nb_missing_values)))
A:sklearn.preprocessing.tests.test_imputation.pipeline->Pipeline([('imputer', Imputer(missing_values=0)), ('tree', tree.DecisionTreeRegressor(random_state=0))])
A:sklearn.preprocessing.tests.test_imputation.Y->sparse_random_matrix(l, 1, density=0.1).toarray()
A:sklearn.preprocessing.tests.test_imputation.gs->GridSearchCV(pipeline, parameters)
A:sklearn.preprocessing.tests.test_imputation.imputer_pickled->pickle.loads(pickle.dumps(imputer))
A:sklearn.preprocessing.tests.test_imputation.X_orig->sparse_random_matrix(5, 5, density=0.75, random_state=0)
A:sklearn.preprocessing.tests.test_imputation.Xt->Imputer(missing_values=0, strategy='mean', copy=False, axis=1).fit(X).transform(X)
sklearn.preprocessing.tests.test_imputation._check_statistics(X,X_true,strategy,statistics,missing_values)
sklearn.preprocessing.tests.test_imputation.safe_mean(arr,*args,**kwargs)
sklearn.preprocessing.tests.test_imputation.safe_median(arr,*args,**kwargs)
sklearn.preprocessing.tests.test_imputation.test_imputation_copy()
sklearn.preprocessing.tests.test_imputation.test_imputation_mean_median()
sklearn.preprocessing.tests.test_imputation.test_imputation_mean_median_only_zero()
sklearn.preprocessing.tests.test_imputation.test_imputation_median_special_cases()
sklearn.preprocessing.tests.test_imputation.test_imputation_most_frequent()
sklearn.preprocessing.tests.test_imputation.test_imputation_pickle()
sklearn.preprocessing.tests.test_imputation.test_imputation_pipeline_grid_search()
sklearn.preprocessing.tests.test_imputation.test_imputation_shape()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/preprocessing/tests/test_data.py----------------------------------------
A:sklearn.preprocessing.tests.test_data.iris->sklearn.datasets.load_iris()
A:sklearn.preprocessing.tests.test_data.rng->numpy.random.RandomState(0)
A:sklearn.preprocessing.tests.test_data.offsets->numpy.random.RandomState(0).uniform(-1000000000000000.0, 1000000000000000.0, size=n_features)
A:sklearn.preprocessing.tests.test_data.scales->numpy.random.RandomState(0).uniform(1000.0, 1000000.0, size=n_features)
A:sklearn.preprocessing.tests.test_data.X_1row->X_2d[0, :].reshape(1, n_features)
A:sklearn.preprocessing.tests.test_data.X_1col->X_2d[:, 0].reshape(n_samples, 1)
A:sklearn.preprocessing.tests.test_data.X_list_1row->X_2d[0, :].reshape(1, n_features).tolist()
A:sklearn.preprocessing.tests.test_data.X_list_1col->X_2d[:, 0].reshape(n_samples, 1).tolist()
A:sklearn.preprocessing.tests.test_data.a->a.toarray().toarray()
A:sklearn.preprocessing.tests.test_data.P1->numpy.hstack([np.ones_like(X1), X1, X1 ** 2, X1 ** 3])
A:sklearn.preprocessing.tests.test_data.X2->numpy.array([[1, 1, 1]])
A:sklearn.preprocessing.tests.test_data.P2->numpy.hstack([x1 ** 0 * x2 ** 0, x1 ** 1 * x2 ** 0, x1 ** 0 * x2 ** 1, x1 ** 2 * x2 ** 0, x1 ** 1 * x2 ** 1, x1 ** 0 * x2 ** 2])
A:sklearn.preprocessing.tests.test_data.P_test->PolynomialFeatures(deg, include_bias=False).fit_transform(X)
A:sklearn.preprocessing.tests.test_data.interact->PolynomialFeatures(2, interaction_only=True, include_bias=True)
A:sklearn.preprocessing.tests.test_data.X_poly->PolynomialFeatures(2, interaction_only=True, include_bias=True).fit_transform(X)
A:sklearn.preprocessing.tests.test_data.X->numpy.array([[0, 25, 50, 75, 100], [2, 4, 6, 8, 10], [2.6, 4.1, 2.3, 9.5, 0.1]])
A:sklearn.preprocessing.tests.test_data.poly->PolynomialFeatures(degree=1, include_bias=True).fit(X)
A:sklearn.preprocessing.tests.test_data.feature_names->PolynomialFeatures(degree=1, include_bias=True).fit(X).get_feature_names([u'\x01F40D', u'☮', u'א'])
A:sklearn.preprocessing.tests.test_data.scaler->StandardScaler(with_std=False)
A:sklearn.preprocessing.tests.test_data.X_scaled->StandardScaler(with_std=False).fit(X).transform(X)
A:sklearn.preprocessing.tests.test_data.X_scaled_back->StandardScaler(with_std=False).inverse_transform(X_scaled)
A:sklearn.preprocessing.tests.test_data.X_arr->numpy.array(X_list)
A:sklearn.preprocessing.tests.test_data.x_scaled->assert_warns_message(UserWarning, w, scale, x)
A:sklearn.preprocessing.tests.test_data.x_small_scaled->assert_no_warnings(scale, x)
A:sklearn.preprocessing.tests.test_data.x_big_scaled->assert_warns_message(UserWarning, w, scale, x_big)
A:sklearn.preprocessing.tests.test_data.x_big_centered->assert_warns_message(UserWarning, w, scale, x_big, with_std=False)
A:sklearn.preprocessing.tests.test_data.s1->numpy.array([0, 1, 2, 3])
A:sklearn.preprocessing.tests.test_data.s2->_handle_zeros_in_scale(s1, copy=True)
A:sklearn.preprocessing.tests.test_data.scaler_batch->MaxAbsScaler().fit(X)
A:sklearn.preprocessing.tests.test_data.scaler_incr->scaler_incr.partial_fit(X[batch]).partial_fit(X[batch])
A:sklearn.preprocessing.tests.test_data.batch0->slice(0, chunk_size)
A:sklearn.preprocessing.tests.test_data.X_csr->scipy.sparse.csr_matrix(X[batch])
A:sklearn.preprocessing.tests.test_data.X_csc->scipy.sparse.csc_matrix(X[batch])
A:sklearn.preprocessing.tests.test_data.null_transform->StandardScaler(with_mean=False, with_std=False, copy=True)
A:sklearn.preprocessing.tests.test_data.X_null->StandardScaler(with_mean=False, with_std=False, copy=True).fit_transform(X_csr)
A:sklearn.preprocessing.tests.test_data.X_orig->StandardScaler(with_mean=False, with_std=False, copy=True).inverse_transform(X_null)
A:sklearn.preprocessing.tests.test_data.chunks_copy->X_sofar.copy()
A:sklearn.preprocessing.tests.test_data.scaled_batch->StandardScaler().fit_transform(X_sofar)
A:sklearn.preprocessing.tests.test_data.scaled_incr->scaler_incr.partial_fit(X[batch]).partial_fit(X[batch]).transform(X_sofar)
A:sklearn.preprocessing.tests.test_data.right_input->scaler_incr.partial_fit(X[batch]).partial_fit(X[batch]).inverse_transform(scaled_incr)
A:sklearn.preprocessing.tests.test_data.zero->numpy.zeros(X.shape[1])
A:sklearn.preprocessing.tests.test_data.epsilon->numpy.nextafter(0, 1)
A:sklearn.preprocessing.tests.test_data.X_trans->OneHotEncoder(categorical_features=cat).fit_transform(X)
A:sklearn.preprocessing.tests.test_data.X_trans_inv->StandardScaler(with_std=False).inverse_transform(X_trans)
A:sklearn.preprocessing.tests.test_data.X_trans_new->StandardScaler(with_std=False).transform(X_new)
A:sklearn.preprocessing.tests.test_data.X_1d->X_2d[0, :].reshape(1, n_features).ravel()
A:sklearn.preprocessing.tests.test_data.min_->X_2d[0, :].reshape(1, n_features).ravel().min()
A:sklearn.preprocessing.tests.test_data.max_->X_2d[0, :].reshape(1, n_features).ravel().max()
A:sklearn.preprocessing.tests.test_data.scaler_csr->StandardScaler(with_mean=False).fit(X_csr)
A:sklearn.preprocessing.tests.test_data.X_csr_scaled->scale(X_csr, with_mean=False, with_std=False, copy=True)
A:sklearn.preprocessing.tests.test_data.scaler_csc->StandardScaler(with_mean=False).fit(X_csc)
A:sklearn.preprocessing.tests.test_data.X_csc_scaled->scale(X_csr.tocsc(), with_mean=False)
A:sklearn.preprocessing.tests.test_data.(X_csr_scaled_mean, X_csr_scaled_std)->mean_variance_axis(X_csr_scaled, 0)
A:sklearn.preprocessing.tests.test_data.X_csr_scaled_back->StandardScaler(with_mean=False).fit(X_csr).inverse_transform(X_csr_scaled)
A:sklearn.preprocessing.tests.test_data.X_csc_scaled_back->StandardScaler(with_mean=False).fit(X_csr).inverse_transform(X_csc_scaled.tocsc())
A:sklearn.preprocessing.tests.test_data.X_copy->numpy.array([[0, 25, 50, 75, 100], [2, 4, 6, 8, 10], [2.6, 4.1, 2.3, 9.5, 0.1]]).copy()
A:sklearn.preprocessing.tests.test_data.X_csr_copy->scipy.sparse.csr_matrix(X[batch]).copy()
A:sklearn.preprocessing.tests.test_data.X_csc_copy->scipy.sparse.csc_matrix(X[batch]).copy()
A:sklearn.preprocessing.tests.test_data.X_transformed_csr->scipy.sparse.csr_matrix(scaler.transform(X))
A:sklearn.preprocessing.tests.test_data.X_transformed_csc->scipy.sparse.csc_matrix(scaler.transform(X))
A:sklearn.preprocessing.tests.test_data.single_row->numpy.array([[0.1, 1.0, 2.0, 0.0, -1.0]])
A:sklearn.preprocessing.tests.test_data.row_trans->StandardScaler(with_std=False).transform(sparse.csr_matrix(single_row))
A:sklearn.preprocessing.tests.test_data.row_scaled_back->StandardScaler(with_std=False).inverse_transform(row_trans)
A:sklearn.preprocessing.tests.test_data.q->numpy.percentile(X_trans, q=(25, 75), axis=1)
A:sklearn.preprocessing.tests.test_data.transformer->QuantileTransformer(n_quantiles=1000, random_state=0)
A:sklearn.preprocessing.tests.test_data.X_sparse->scipy.sparse.csr_matrix(X_dense)
A:sklearn.preprocessing.tests.test_data.X_sparse_tran->QuantileTransformer(n_quantiles=1000, random_state=0).fit_transform(X_sparse)
A:sklearn.preprocessing.tests.test_data.X_sparse_tran_inv->QuantileTransformer(n_quantiles=1000, random_state=0).inverse_transform(X_sparse_tran)
A:sklearn.preprocessing.tests.test_data.X_neg->scipy.sparse.csc_matrix(X_neg)
A:sklearn.preprocessing.tests.test_data.X_bad_feat->numpy.transpose([[0, 25, 50, 0, 0, 0, 75, 0, 0, 100], [0, 0, 2.6, 4.1, 0, 0, 2.3, 0, 9.5, 0.1]])
A:sklearn.preprocessing.tests.test_data.X_tran->QuantileTransformer(n_quantiles=1000, random_state=0).transform(X)
A:sklearn.preprocessing.tests.test_data.X_expected->scipy.sparse.csr_matrix([[1.0, 1.0, 1.0]])
A:sklearn.preprocessing.tests.test_data.X_data->numpy.array([-1, -1, 1, 0, 0, 0, 1, -1, 1])
A:sklearn.preprocessing.tests.test_data.X_col->numpy.array([0, 0, 1, 1, 1, 1, 1, 1, 1])
A:sklearn.preprocessing.tests.test_data.X_row->numpy.array([0, 4, 0, 1, 2, 3, 4, 5, 6])
A:sklearn.preprocessing.tests.test_data.X_test->numpy.array([[-1, 1, 0], [101, 11, 10]])
A:sklearn.preprocessing.tests.test_data.inf_norm->numpy.max(np.abs(diff))
A:sklearn.preprocessing.tests.test_data.transformer_dense->QuantileTransformer(n_quantiles=10).fit(X.toarray())
A:sklearn.preprocessing.tests.test_data.X_trans_a0->quantile_transform(X.T, axis=0, n_quantiles=5)
A:sklearn.preprocessing.tests.test_data.X_trans_a1->quantile_transform(X, axis=1, n_quantiles=5)
A:sklearn.preprocessing.tests.test_data.X_dense->numpy.array([[3.0, 0, 4.0], [1.0, 0.0, 0.0], [2.0, 3.0, 0.0]])
A:sklearn.preprocessing.tests.test_data.X_trans_sp->QuantileTransformer(n_quantiles=3, random_state=0).fit_transform(X_sparse)
A:sklearn.preprocessing.tests.test_data.X1->numpy.array([[0, 0.1], [0, 0.5], [1, 0.1]])
A:sklearn.preprocessing.tests.test_data.X_trans_csr->StandardScaler(with_std=False).fit_transform(X_csr)
A:sklearn.preprocessing.tests.test_data.X_trans_csc->StandardScaler(with_std=False).fit_transform(X_csc)
A:sklearn.preprocessing.tests.test_data.X_trans_csr_inv->StandardScaler(with_std=False).inverse_transform(X_trans_csr)
A:sklearn.preprocessing.tests.test_data.X_trans_csc_inv->StandardScaler(with_std=False).inverse_transform(X_trans_csc)
A:sklearn.preprocessing.tests.test_data.max_abs->numpy.abs(X_1d).max()
A:sklearn.preprocessing.tests.test_data.scaler_incr_csr->scaler_incr_csr.partial_fit(X_csr).partial_fit(X_csr)
A:sklearn.preprocessing.tests.test_data.scaler_incr_csc->scaler_incr_csc.partial_fit(X_csc).partial_fit(X_csc)
A:sklearn.preprocessing.tests.test_data.X_sparse_unpruned->scipy.sparse.csr_matrix(X_dense)
A:sklearn.preprocessing.tests.test_data.X_sparse_pruned->scipy.sparse.csr_matrix(X_dense)
A:sklearn.preprocessing.tests.test_data.normalizer->Normalizer(norm='max', copy=False)
A:sklearn.preprocessing.tests.test_data.X_norm->toarray(X_norm)
A:sklearn.preprocessing.tests.test_data.X_norm1->toarray(X_norm1)
A:sklearn.preprocessing.tests.test_data.X_norm2->toarray(X_norm2)
A:sklearn.preprocessing.tests.test_data.row_sums->X_norm_squared.sum(axis=1)
A:sklearn.preprocessing.tests.test_data.X_normnormalizer->Normalizer(norm='l2', copy=False).transform(X)
A:sklearn.preprocessing.tests.test_data.row_maxs->toarray(X_norm).max(axis=1)
A:sklearn.preprocessing.tests.test_data.rs->numpy.random.RandomState(0)
A:sklearn.preprocessing.tests.test_data.ones->numpy.ones(10)
A:sklearn.preprocessing.tests.test_data.(_, norms)->normalize(X_sparse, norm='max', return_norm=True)
A:sklearn.preprocessing.tests.test_data.X_->numpy.array([[1, 0, 5], [2, 3, -1]])
A:sklearn.preprocessing.tests.test_data.binarizer->Binarizer(threshold=-0.5, copy=True)
A:sklearn.preprocessing.tests.test_data.X_bin->Binarizer(threshold=-0.5, copy=True).transform(X)
A:sklearn.preprocessing.tests.test_data.X_float->numpy.array([[1, 0, 5], [2, 3, -1]], dtype=np.float64)
A:sklearn.preprocessing.tests.test_data.X_fit->numpy.random.RandomState(0).random_sample((5, 4))
A:sklearn.preprocessing.tests.test_data.X_fit_centered->StandardScaler(with_std=False).transform(X_fit)
A:sklearn.preprocessing.tests.test_data.K_fit->numpy.dot(X_fit, X_fit.T)
A:sklearn.preprocessing.tests.test_data.centerer->KernelCenterer()
A:sklearn.preprocessing.tests.test_data.K_fit_centered->numpy.dot(X_fit_centered, X_fit_centered.T)
A:sklearn.preprocessing.tests.test_data.K_fit_centered2->KernelCenterer().fit_transform(K_fit)
A:sklearn.preprocessing.tests.test_data.X_pred->numpy.random.RandomState(0).random_sample((2, 4))
A:sklearn.preprocessing.tests.test_data.K_pred->numpy.dot(X_pred, X_fit.T)
A:sklearn.preprocessing.tests.test_data.X_pred_centered->StandardScaler(with_std=False).transform(X_pred)
A:sklearn.preprocessing.tests.test_data.K_pred_centered->numpy.dot(X_pred_centered, X_fit_centered.T)
A:sklearn.preprocessing.tests.test_data.K_pred_centered2->KernelCenterer().transform(K_pred)
A:sklearn.preprocessing.tests.test_data.y_true->numpy.ones((4,))
A:sklearn.preprocessing.tests.test_data.K->numpy.array([[0, 25, 50, 75, 100], [2, 4, 6, 8, 10], [2.6, 4.1, 2.3, 9.5, 0.1]]).dot(X.T)
A:sklearn.preprocessing.tests.test_data.kcent->KernelCenterer()
A:sklearn.preprocessing.tests.test_data.pipeline->Pipeline([('kernel_centerer', kcent), ('svr', SVR())])
A:sklearn.preprocessing.tests.test_data.y_pred->cross_val_predict(pipeline, K, y_true, cv=2)
A:sklearn.preprocessing.tests.test_data.X_transformed->obj.fit(X).transform(X)
A:sklearn.preprocessing.tests.test_data.X_transformed2->obj.fit_transform(X)
A:sklearn.preprocessing.tests.test_data.enc->OneHotEncoder(categorical_features=cat)
A:sklearn.preprocessing.tests.test_data.Xtr->OneHotEncoder(categorical_features=cat).fit_transform(X)
A:sklearn.preprocessing.tests.test_data.original_X->numpy.asarray([[1, 2], [3, 4]])
A:sklearn.preprocessing.tests.test_data.X2tr->OneHotEncoder(categorical_features=cat).transform(X2)
A:sklearn.preprocessing.tests.test_data.(A, B)->_run_one_hot(X, X2, cat)
A:sklearn.preprocessing.tests.test_data.(C, D)->_run_one_hot(X, X2, ind)
A:sklearn.preprocessing.tests.test_data.y->numpy.array([[4, 1, 1]])
A:sklearn.preprocessing.tests.test_data.oh->OneHotEncoder(handle_unknown='42')
sklearn.preprocessing.tests.test_data._check_dim_1axis(a)
sklearn.preprocessing.tests.test_data._check_one_hot(X,X2,cat,n_features)
sklearn.preprocessing.tests.test_data._check_transform_selected(X,X_expected,sel)
sklearn.preprocessing.tests.test_data._run_one_hot(X,X2,cat)
sklearn.preprocessing.tests.test_data.assert_correct_incr(i,batch_start,batch_stop,n,chunk_size,n_samples_seen)
sklearn.preprocessing.tests.test_data.test_add_dummy_feature()
sklearn.preprocessing.tests.test_data.test_add_dummy_feature_coo()
sklearn.preprocessing.tests.test_data.test_add_dummy_feature_csc()
sklearn.preprocessing.tests.test_data.test_add_dummy_feature_csr()
sklearn.preprocessing.tests.test_data.test_binarizer()
sklearn.preprocessing.tests.test_data.test_center_kernel()
sklearn.preprocessing.tests.test_data.test_cv_pipeline_precomputed()
sklearn.preprocessing.tests.test_data.test_fit_cold_start()
sklearn.preprocessing.tests.test_data.test_fit_transform()
sklearn.preprocessing.tests.test_data.test_handle_zeros_in_scale()
sklearn.preprocessing.tests.test_data.test_maxabs_scaler_1d()
sklearn.preprocessing.tests.test_data.test_maxabs_scaler_large_negative_value()
sklearn.preprocessing.tests.test_data.test_maxabs_scaler_partial_fit()
sklearn.preprocessing.tests.test_data.test_maxabs_scaler_transform_one_row_csr()
sklearn.preprocessing.tests.test_data.test_maxabs_scaler_zero_variance_features()
sklearn.preprocessing.tests.test_data.test_min_max_scaler_1d()
sklearn.preprocessing.tests.test_data.test_min_max_scaler_iris()
sklearn.preprocessing.tests.test_data.test_min_max_scaler_zero_variance_features()
sklearn.preprocessing.tests.test_data.test_minmax_scale_axis1()
sklearn.preprocessing.tests.test_data.test_minmax_scaler_partial_fit()
sklearn.preprocessing.tests.test_data.test_normalize()
sklearn.preprocessing.tests.test_data.test_normalizer_l1()
sklearn.preprocessing.tests.test_data.test_normalizer_l2()
sklearn.preprocessing.tests.test_data.test_normalizer_max()
sklearn.preprocessing.tests.test_data.test_one_hot_encoder_categorical_features()
sklearn.preprocessing.tests.test_data.test_one_hot_encoder_dense()
sklearn.preprocessing.tests.test_data.test_one_hot_encoder_sparse()
sklearn.preprocessing.tests.test_data.test_one_hot_encoder_unknown_transform()
sklearn.preprocessing.tests.test_data.test_partial_fit_sparse_input()
sklearn.preprocessing.tests.test_data.test_polynomial_feature_names()
sklearn.preprocessing.tests.test_data.test_polynomial_features()
sklearn.preprocessing.tests.test_data.test_quantile_transform_and_inverse()
sklearn.preprocessing.tests.test_data.test_quantile_transform_axis1()
sklearn.preprocessing.tests.test_data.test_quantile_transform_bounds()
sklearn.preprocessing.tests.test_data.test_quantile_transform_check_error()
sklearn.preprocessing.tests.test_data.test_quantile_transform_dense_toy()
sklearn.preprocessing.tests.test_data.test_quantile_transform_iris()
sklearn.preprocessing.tests.test_data.test_quantile_transform_sparse_ignore_zeros()
sklearn.preprocessing.tests.test_data.test_quantile_transform_sparse_toy()
sklearn.preprocessing.tests.test_data.test_quantile_transform_subsampling()
sklearn.preprocessing.tests.test_data.test_quantile_transform_valid_axis()
sklearn.preprocessing.tests.test_data.test_robust_scale_axis1()
sklearn.preprocessing.tests.test_data.test_robust_scaler_2d_arrays()
sklearn.preprocessing.tests.test_data.test_robust_scaler_invalid_range()
sklearn.preprocessing.tests.test_data.test_robust_scaler_iris()
sklearn.preprocessing.tests.test_data.test_robust_scaler_iris_quantiles()
sklearn.preprocessing.tests.test_data.test_robust_scaler_transform_one_row_csr()
sklearn.preprocessing.tests.test_data.test_robust_scaler_zero_variance_features()
sklearn.preprocessing.tests.test_data.test_scale_1d()
sklearn.preprocessing.tests.test_data.test_scale_function_without_centering()
sklearn.preprocessing.tests.test_data.test_scale_input_finiteness_validation()
sklearn.preprocessing.tests.test_data.test_scale_sparse_with_mean_raise_exception()
sklearn.preprocessing.tests.test_data.test_scaler_2d_arrays()
sklearn.preprocessing.tests.test_data.test_scaler_int()
sklearn.preprocessing.tests.test_data.test_scaler_without_centering()
sklearn.preprocessing.tests.test_data.test_scaler_without_copy()
sklearn.preprocessing.tests.test_data.test_standard_scaler_1d()
sklearn.preprocessing.tests.test_data.test_standard_scaler_numerical_stability()
sklearn.preprocessing.tests.test_data.test_standard_scaler_partial_fit()
sklearn.preprocessing.tests.test_data.test_standard_scaler_partial_fit_numerical_stability()
sklearn.preprocessing.tests.test_data.test_standard_scaler_trasform_with_partial_fit()
sklearn.preprocessing.tests.test_data.test_transform_selected()
sklearn.preprocessing.tests.test_data.test_transform_selected_copy_arg()
sklearn.preprocessing.tests.test_data.test_warning_scaling_integers()
sklearn.preprocessing.tests.test_data.toarray(a)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/neural_network/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py----------------------------------------
A:sklearn.neural_network.multilayer_perceptron.self.coefs_[i]->numpy.reshape(packed_parameters[start:end], shape)
A:sklearn.neural_network.multilayer_perceptron.activations[i + 1]->output_activation(activations[i + 1])
A:sklearn.neural_network.multilayer_perceptron.coef_grads[layer]->safe_sparse_dot(activations[layer].T, deltas[layer])
A:sklearn.neural_network.multilayer_perceptron.intercept_grads[layer]->numpy.mean(deltas[layer], 0)
A:sklearn.neural_network.multilayer_perceptron.(loss, coef_grads, intercept_grads)->self._backprop(X, y, activations, deltas, coef_grads, intercept_grads)
A:sklearn.neural_network.multilayer_perceptron.grad->_pack(coef_grads, intercept_grads)
A:sklearn.neural_network.multilayer_perceptron.activations->self._forward_pass(activations)
A:sklearn.neural_network.multilayer_perceptron.loss->LOSS_FUNCTIONS[loss_func_name](y, activations[-1])
A:sklearn.neural_network.multilayer_perceptron.values->numpy.sum(np.array([np.dot(s.ravel(), s.ravel()) for s in self.coefs_]))
A:sklearn.neural_network.multilayer_perceptron.(coef_grads, intercept_grads)->self._compute_loss_grad(i - 1, n_samples, activations, deltas, coef_grads, intercept_grads)
A:sklearn.neural_network.multilayer_perceptron.deltas[i - 1]->safe_sparse_dot(deltas[i], self.coefs_[i].T)
A:sklearn.neural_network.multilayer_perceptron.self.n_layers_->len(layer_units)
A:sklearn.neural_network.multilayer_perceptron.(coef_init, intercept_init)->self._init_coef(layer_units[i], layer_units[i + 1])
A:sklearn.neural_network.multilayer_perceptron.init_bound->numpy.sqrt(6.0 / (fan_in + fan_out))
A:sklearn.neural_network.multilayer_perceptron.coef_init->self._random_state.uniform(-init_bound, init_bound, (fan_in, fan_out))
A:sklearn.neural_network.multilayer_perceptron.intercept_init->self._random_state.uniform(-init_bound, init_bound, fan_out)
A:sklearn.neural_network.multilayer_perceptron.hidden_layer_sizes->list(hidden_layer_sizes)
A:sklearn.neural_network.multilayer_perceptron.(X, y)->check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'], multi_output=True, y_numeric=True)
A:sklearn.neural_network.multilayer_perceptron.y->column_or_1d(y, warn=True)
A:sklearn.neural_network.multilayer_perceptron.self._random_state->check_random_state(self.random_state)
A:sklearn.neural_network.multilayer_perceptron.batch_size->numpy.clip(self.batch_size, 1, n_samples)
A:sklearn.neural_network.multilayer_perceptron.packed_coef_inter->_pack(self.coefs_, self.intercepts_)
A:sklearn.neural_network.multilayer_perceptron.(optimal_parameters, self.loss_, d)->fmin_l_bfgs_b(x0=packed_coef_inter, func=self._loss_grad_lbfgs, maxfun=self.max_iter, iprint=iprint, pgtol=self.tol, args=(X, y, activations, deltas, coef_grads, intercept_grads))
A:sklearn.neural_network.multilayer_perceptron.self._optimizer->AdamOptimizer(params, self.learning_rate_init, self.beta_1, self.beta_2, self.epsilon)
A:sklearn.neural_network.multilayer_perceptron.(X, X_val, y, y_val)->train_test_split(X, y, random_state=self._random_state, test_size=self.validation_fraction)
A:sklearn.neural_network.multilayer_perceptron.y_val->self._label_binarizer.inverse_transform(y_val)
A:sklearn.neural_network.multilayer_perceptron.(batch_loss, coef_grads, intercept_grads)->self._backprop(X[batch_slice], y[batch_slice], activations, deltas, coef_grads, intercept_grads)
A:sklearn.neural_network.multilayer_perceptron.is_stopping->self._optimizer.trigger_stopping(msg, self.verbose)
A:sklearn.neural_network.multilayer_perceptron.X->check_array(X, accept_sparse=['csr', 'csc', 'coo'])
A:sklearn.neural_network.multilayer_perceptron.sup->super(MLPRegressor, self)
A:sklearn.neural_network.multilayer_perceptron.self._label_binarizer->LabelBinarizer()
A:sklearn.neural_network.multilayer_perceptron.classes->unique_labels(y)
A:sklearn.neural_network.multilayer_perceptron.y_pred->self._predict(X)
A:sklearn.neural_network.multilayer_perceptron.y_prob->self.predict_proba(X)
sklearn.neural_network.MLPClassifier(self,hidden_layer_sizes=(100,),activation='relu',solver='adam',alpha=0.0001,batch_size='auto',learning_rate='constant',learning_rate_init=0.001,power_t=0.5,max_iter=200,shuffle=True,random_state=None,tol=0.0001,verbose=False,warm_start=False,momentum=0.9,nesterovs_momentum=True,early_stopping=False,validation_fraction=0.1,beta_1=0.9,beta_2=0.999,epsilon=1e-08)
sklearn.neural_network.MLPClassifier._partial_fit(self,X,y,classes=None)
sklearn.neural_network.MLPClassifier._validate_input(self,X,y,incremental)
sklearn.neural_network.MLPClassifier.fit(self,X,y)
sklearn.neural_network.MLPClassifier.partial_fit(self)
sklearn.neural_network.MLPClassifier.predict(self,X)
sklearn.neural_network.MLPClassifier.predict_log_proba(self,X)
sklearn.neural_network.MLPClassifier.predict_proba(self,X)
sklearn.neural_network.MLPRegressor(self,hidden_layer_sizes=(100,),activation='relu',solver='adam',alpha=0.0001,batch_size='auto',learning_rate='constant',learning_rate_init=0.001,power_t=0.5,max_iter=200,shuffle=True,random_state=None,tol=0.0001,verbose=False,warm_start=False,momentum=0.9,nesterovs_momentum=True,early_stopping=False,validation_fraction=0.1,beta_1=0.9,beta_2=0.999,epsilon=1e-08)
sklearn.neural_network.MLPRegressor._validate_input(self,X,y,incremental)
sklearn.neural_network.MLPRegressor.predict(self,X)
sklearn.neural_network.multilayer_perceptron.BaseMultilayerPerceptron(self,hidden_layer_sizes,activation,solver,alpha,batch_size,learning_rate,learning_rate_init,power_t,max_iter,loss,shuffle,random_state,tol,verbose,warm_start,momentum,nesterovs_momentum,early_stopping,validation_fraction,beta_1,beta_2,epsilon)
sklearn.neural_network.multilayer_perceptron.BaseMultilayerPerceptron.__init__(self,hidden_layer_sizes,activation,solver,alpha,batch_size,learning_rate,learning_rate_init,power_t,max_iter,loss,shuffle,random_state,tol,verbose,warm_start,momentum,nesterovs_momentum,early_stopping,validation_fraction,beta_1,beta_2,epsilon)
sklearn.neural_network.multilayer_perceptron.BaseMultilayerPerceptron._backprop(self,X,y,activations,deltas,coef_grads,intercept_grads)
sklearn.neural_network.multilayer_perceptron.BaseMultilayerPerceptron._compute_loss_grad(self,layer,n_samples,activations,deltas,coef_grads,intercept_grads)
sklearn.neural_network.multilayer_perceptron.BaseMultilayerPerceptron._fit(self,X,y,incremental=False)
sklearn.neural_network.multilayer_perceptron.BaseMultilayerPerceptron._fit_lbfgs(self,X,y,activations,deltas,coef_grads,intercept_grads,layer_units)
sklearn.neural_network.multilayer_perceptron.BaseMultilayerPerceptron._fit_stochastic(self,X,y,activations,deltas,coef_grads,intercept_grads,layer_units,incremental)
sklearn.neural_network.multilayer_perceptron.BaseMultilayerPerceptron._forward_pass(self,activations)
sklearn.neural_network.multilayer_perceptron.BaseMultilayerPerceptron._init_coef(self,fan_in,fan_out)
sklearn.neural_network.multilayer_perceptron.BaseMultilayerPerceptron._initialize(self,y,layer_units)
sklearn.neural_network.multilayer_perceptron.BaseMultilayerPerceptron._loss_grad_lbfgs(self,packed_coef_inter,X,y,activations,deltas,coef_grads,intercept_grads)
sklearn.neural_network.multilayer_perceptron.BaseMultilayerPerceptron._partial_fit(self,X,y)
sklearn.neural_network.multilayer_perceptron.BaseMultilayerPerceptron._predict(self,X)
sklearn.neural_network.multilayer_perceptron.BaseMultilayerPerceptron._unpack(self,packed_parameters)
sklearn.neural_network.multilayer_perceptron.BaseMultilayerPerceptron._update_no_improvement_count(self,early_stopping,X_val,y_val)
sklearn.neural_network.multilayer_perceptron.BaseMultilayerPerceptron._validate_hyperparameters(self)
sklearn.neural_network.multilayer_perceptron.BaseMultilayerPerceptron.fit(self,X,y)
sklearn.neural_network.multilayer_perceptron.BaseMultilayerPerceptron.partial_fit(self)
sklearn.neural_network.multilayer_perceptron.MLPClassifier(self,hidden_layer_sizes=(100,),activation='relu',solver='adam',alpha=0.0001,batch_size='auto',learning_rate='constant',learning_rate_init=0.001,power_t=0.5,max_iter=200,shuffle=True,random_state=None,tol=0.0001,verbose=False,warm_start=False,momentum=0.9,nesterovs_momentum=True,early_stopping=False,validation_fraction=0.1,beta_1=0.9,beta_2=0.999,epsilon=1e-08)
sklearn.neural_network.multilayer_perceptron.MLPClassifier.__init__(self,hidden_layer_sizes=(100,),activation='relu',solver='adam',alpha=0.0001,batch_size='auto',learning_rate='constant',learning_rate_init=0.001,power_t=0.5,max_iter=200,shuffle=True,random_state=None,tol=0.0001,verbose=False,warm_start=False,momentum=0.9,nesterovs_momentum=True,early_stopping=False,validation_fraction=0.1,beta_1=0.9,beta_2=0.999,epsilon=1e-08)
sklearn.neural_network.multilayer_perceptron.MLPClassifier._partial_fit(self,X,y,classes=None)
sklearn.neural_network.multilayer_perceptron.MLPClassifier._validate_input(self,X,y,incremental)
sklearn.neural_network.multilayer_perceptron.MLPClassifier.fit(self,X,y)
sklearn.neural_network.multilayer_perceptron.MLPClassifier.partial_fit(self)
sklearn.neural_network.multilayer_perceptron.MLPClassifier.predict(self,X)
sklearn.neural_network.multilayer_perceptron.MLPClassifier.predict_log_proba(self,X)
sklearn.neural_network.multilayer_perceptron.MLPClassifier.predict_proba(self,X)
sklearn.neural_network.multilayer_perceptron.MLPRegressor(self,hidden_layer_sizes=(100,),activation='relu',solver='adam',alpha=0.0001,batch_size='auto',learning_rate='constant',learning_rate_init=0.001,power_t=0.5,max_iter=200,shuffle=True,random_state=None,tol=0.0001,verbose=False,warm_start=False,momentum=0.9,nesterovs_momentum=True,early_stopping=False,validation_fraction=0.1,beta_1=0.9,beta_2=0.999,epsilon=1e-08)
sklearn.neural_network.multilayer_perceptron.MLPRegressor.__init__(self,hidden_layer_sizes=(100,),activation='relu',solver='adam',alpha=0.0001,batch_size='auto',learning_rate='constant',learning_rate_init=0.001,power_t=0.5,max_iter=200,shuffle=True,random_state=None,tol=0.0001,verbose=False,warm_start=False,momentum=0.9,nesterovs_momentum=True,early_stopping=False,validation_fraction=0.1,beta_1=0.9,beta_2=0.999,epsilon=1e-08)
sklearn.neural_network.multilayer_perceptron.MLPRegressor._validate_input(self,X,y,incremental)
sklearn.neural_network.multilayer_perceptron.MLPRegressor.predict(self,X)
sklearn.neural_network.multilayer_perceptron._pack(coefs_,intercepts_)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/neural_network/_stochastic_optimizers.py----------------------------------------
A:sklearn.neural_network._stochastic_optimizers.self.learning_rate->float(learning_rate_init)
A:sklearn.neural_network._stochastic_optimizers.updates->self._get_updates(grads)
sklearn.neural_network._stochastic_optimizers.AdamOptimizer(self,params,learning_rate_init=0.001,beta_1=0.9,beta_2=0.999,epsilon=1e-08)
sklearn.neural_network._stochastic_optimizers.AdamOptimizer.__init__(self,params,learning_rate_init=0.001,beta_1=0.9,beta_2=0.999,epsilon=1e-08)
sklearn.neural_network._stochastic_optimizers.AdamOptimizer._get_updates(self,grads)
sklearn.neural_network._stochastic_optimizers.BaseOptimizer(self,params,learning_rate_init=0.1)
sklearn.neural_network._stochastic_optimizers.BaseOptimizer.__init__(self,params,learning_rate_init=0.1)
sklearn.neural_network._stochastic_optimizers.BaseOptimizer.iteration_ends(self,time_step)
sklearn.neural_network._stochastic_optimizers.BaseOptimizer.trigger_stopping(self,msg,verbose)
sklearn.neural_network._stochastic_optimizers.BaseOptimizer.update_params(self,grads)
sklearn.neural_network._stochastic_optimizers.SGDOptimizer(self,params,learning_rate_init=0.1,lr_schedule='constant',momentum=0.9,nesterov=True,power_t=0.5)
sklearn.neural_network._stochastic_optimizers.SGDOptimizer.__init__(self,params,learning_rate_init=0.1,lr_schedule='constant',momentum=0.9,nesterov=True,power_t=0.5)
sklearn.neural_network._stochastic_optimizers.SGDOptimizer._get_updates(self,grads)
sklearn.neural_network._stochastic_optimizers.SGDOptimizer.iteration_ends(self,time_step)
sklearn.neural_network._stochastic_optimizers.SGDOptimizer.trigger_stopping(self,msg,verbose)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/neural_network/rbm.py----------------------------------------
A:sklearn.neural_network.rbm.X->check_array(X, accept_sparse='csr', dtype=np.float64)
A:sklearn.neural_network.rbm.p->numpy.dot(h, self.components_)
A:sklearn.neural_network.rbm.self.random_state_->check_random_state(self.random_state)
A:sklearn.neural_network.rbm.h_->self._sample_hiddens(v, self.random_state_)
A:sklearn.neural_network.rbm.v_->check_array(X, accept_sparse='csr').copy()
A:sklearn.neural_network.rbm.self.components_->numpy.asarray(rng.normal(0, 0.01, (self.n_components, X.shape[1])), order='F')
A:sklearn.neural_network.rbm.self.intercept_hidden_->numpy.zeros(self.n_components)
A:sklearn.neural_network.rbm.self.intercept_visible_->numpy.zeros(X.shape[1])
A:sklearn.neural_network.rbm.self.h_samples_->numpy.zeros((self.batch_size, self.n_components))
A:sklearn.neural_network.rbm.h_pos->self._mean_hiddens(v_pos)
A:sklearn.neural_network.rbm.v_neg->self._sample_visibles(self.h_samples_, rng)
A:sklearn.neural_network.rbm.h_neg->self._mean_hiddens(v_neg)
A:sklearn.neural_network.rbm.v->check_array(X, accept_sparse='csr')
A:sklearn.neural_network.rbm.rng->check_random_state(self.random_state)
A:sklearn.neural_network.rbm.fe->self._free_energy(v)
A:sklearn.neural_network.rbm.fe_->self._free_energy(v_)
A:sklearn.neural_network.rbm.n_batches->int(np.ceil(float(n_samples) / self.batch_size))
A:sklearn.neural_network.rbm.batch_slices->list(gen_even_slices(n_batches * self.batch_size, n_batches, n_samples))
A:sklearn.neural_network.rbm.begin->time.time()
A:sklearn.neural_network.rbm.end->time.time()
sklearn.neural_network.BernoulliRBM(self,n_components=256,learning_rate=0.1,batch_size=10,n_iter=10,verbose=0,random_state=None)
sklearn.neural_network.BernoulliRBM._fit(self,v_pos,rng)
sklearn.neural_network.BernoulliRBM._free_energy(self,v)
sklearn.neural_network.BernoulliRBM._mean_hiddens(self,v)
sklearn.neural_network.BernoulliRBM._sample_hiddens(self,v,rng)
sklearn.neural_network.BernoulliRBM._sample_visibles(self,h,rng)
sklearn.neural_network.BernoulliRBM.fit(self,X,y=None)
sklearn.neural_network.BernoulliRBM.gibbs(self,v)
sklearn.neural_network.BernoulliRBM.partial_fit(self,X,y=None)
sklearn.neural_network.BernoulliRBM.score_samples(self,X)
sklearn.neural_network.BernoulliRBM.transform(self,X)
sklearn.neural_network.rbm.BernoulliRBM(self,n_components=256,learning_rate=0.1,batch_size=10,n_iter=10,verbose=0,random_state=None)
sklearn.neural_network.rbm.BernoulliRBM.__init__(self,n_components=256,learning_rate=0.1,batch_size=10,n_iter=10,verbose=0,random_state=None)
sklearn.neural_network.rbm.BernoulliRBM._fit(self,v_pos,rng)
sklearn.neural_network.rbm.BernoulliRBM._free_energy(self,v)
sklearn.neural_network.rbm.BernoulliRBM._mean_hiddens(self,v)
sklearn.neural_network.rbm.BernoulliRBM._sample_hiddens(self,v,rng)
sklearn.neural_network.rbm.BernoulliRBM._sample_visibles(self,h,rng)
sklearn.neural_network.rbm.BernoulliRBM.fit(self,X,y=None)
sklearn.neural_network.rbm.BernoulliRBM.gibbs(self,v)
sklearn.neural_network.rbm.BernoulliRBM.partial_fit(self,X,y=None)
sklearn.neural_network.rbm.BernoulliRBM.score_samples(self,X)
sklearn.neural_network.rbm.BernoulliRBM.transform(self,X)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/neural_network/_base.py----------------------------------------
A:sklearn.neural_network._base.y_prob->numpy.clip(y_prob, 1e-10, 1 - 1e-10)
A:sklearn.neural_network._base.y_true->numpy.append(1 - y_true, y_true, axis=1)
sklearn.neural_network._base.binary_log_loss(y_true,y_prob)
sklearn.neural_network._base.identity(X)
sklearn.neural_network._base.inplace_identity_derivative(Z,delta)
sklearn.neural_network._base.inplace_logistic_derivative(Z,delta)
sklearn.neural_network._base.inplace_relu_derivative(Z,delta)
sklearn.neural_network._base.inplace_tanh_derivative(Z,delta)
sklearn.neural_network._base.log_loss(y_true,y_prob)
sklearn.neural_network._base.logistic(X)
sklearn.neural_network._base.relu(X)
sklearn.neural_network._base.softmax(X)
sklearn.neural_network._base.squared_loss(y_true,y_pred)
sklearn.neural_network._base.tanh(X)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/neural_network/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/neural_network/tests/test_stochastic_optimizers.py----------------------------------------
A:sklearn.neural_network.tests.test_stochastic_optimizers.optimizer->AdamOptimizer(params, lr, beta_1, beta_2, epsilon)
sklearn.neural_network.tests.test_stochastic_optimizers.test_adam_optimizer()
sklearn.neural_network.tests.test_stochastic_optimizers.test_base_optimizer()
sklearn.neural_network.tests.test_stochastic_optimizers.test_sgd_optimizer_momentum()
sklearn.neural_network.tests.test_stochastic_optimizers.test_sgd_optimizer_nesterovs_momentum()
sklearn.neural_network.tests.test_stochastic_optimizers.test_sgd_optimizer_no_momentum()
sklearn.neural_network.tests.test_stochastic_optimizers.test_sgd_optimizer_trigger_stopping()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/neural_network/tests/test_rbm.py----------------------------------------
A:sklearn.neural_network.tests.test_rbm.X->csc_matrix([[0.0], [1.0]])
A:sklearn.neural_network.tests.test_rbm.rbm->BernoulliRBM(n_components=2, batch_size=2, n_iter=1, random_state=42, verbose=True)
A:sklearn.neural_network.tests.test_rbm.n_batches->int(np.ceil(float(n_samples) / rbm.batch_size))
A:sklearn.neural_network.tests.test_rbm.batch_slices->numpy.array_split(X, n_batches)
A:sklearn.neural_network.tests.test_rbm.rbm1->BernoulliRBM(n_components=10, batch_size=2, n_iter=10, random_state=rng)
A:sklearn.neural_network.tests.test_rbm.Xt1->BernoulliRBM(n_components=10, batch_size=2, n_iter=10, random_state=rng).transform(X)
A:sklearn.neural_network.tests.test_rbm.Xt2->BernoulliRBM(n_components=10, batch_size=2, n_iter=10, random_state=rng)._mean_hiddens(X)
A:sklearn.neural_network.tests.test_rbm.X_sparse->sparse(Xdigits[:100])
A:sklearn.neural_network.tests.test_rbm.rbm2->BernoulliRBM(n_components=2, batch_size=2, n_iter=42, random_state=rng)
A:sklearn.neural_network.tests.test_rbm.rng->numpy.random.RandomState(42)
A:sklearn.neural_network.tests.test_rbm.h->BernoulliRBM(n_components=10, batch_size=2, n_iter=10, random_state=rng)._mean_hiddens(X[0])
A:sklearn.neural_network.tests.test_rbm.hs->numpy.mean([rbm1._sample_hiddens(X[0], rng) for i in range(100)], 0)
A:sklearn.neural_network.tests.test_rbm.X_sampled->BernoulliRBM(n_components=10, batch_size=2, n_iter=10, random_state=rng).gibbs(X)
A:sklearn.neural_network.tests.test_rbm.X_sampled2->BernoulliRBM(n_components=10, batch_size=2, n_iter=10, random_state=rng).gibbs(X)
A:sklearn.neural_network.tests.test_rbm.d_score->BernoulliRBM(n_components=10, batch_size=2, n_iter=10, random_state=rng).score_samples(X)
A:sklearn.neural_network.tests.test_rbm.s_score->BernoulliRBM(n_components=10, batch_size=2, n_iter=10, random_state=rng).score_samples(lil_matrix(X))
A:sklearn.neural_network.tests.test_rbm.sys.stdout->StringIO()
A:sklearn.neural_network.tests.test_rbm.s->sys.stdout.getvalue()
sklearn.neural_network.tests.test_rbm.test_fit()
sklearn.neural_network.tests.test_rbm.test_fit_gibbs()
sklearn.neural_network.tests.test_rbm.test_fit_gibbs_sparse()
sklearn.neural_network.tests.test_rbm.test_gibbs_smoke()
sklearn.neural_network.tests.test_rbm.test_partial_fit()
sklearn.neural_network.tests.test_rbm.test_rbm_verbose()
sklearn.neural_network.tests.test_rbm.test_sample_hiddens()
sklearn.neural_network.tests.test_rbm.test_score_samples()
sklearn.neural_network.tests.test_rbm.test_small_sparse()
sklearn.neural_network.tests.test_rbm.test_small_sparse_partial_fit()
sklearn.neural_network.tests.test_rbm.test_sparse_and_verbose()
sklearn.neural_network.tests.test_rbm.test_transform()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/neural_network/tests/test_mlp.py----------------------------------------
A:sklearn.neural_network.tests.test_mlp.digits_dataset_multi->load_digits(n_class=3)
A:sklearn.neural_network.tests.test_mlp.X_digits_multi->MinMaxScaler().fit_transform(digits_dataset_multi.data[:200])
A:sklearn.neural_network.tests.test_mlp.digits_dataset_binary->load_digits(n_class=2)
A:sklearn.neural_network.tests.test_mlp.X_digits_binary->MinMaxScaler().fit_transform(digits_dataset_binary.data[:200])
A:sklearn.neural_network.tests.test_mlp.boston->load_boston()
A:sklearn.neural_network.tests.test_mlp.iris->load_iris()
A:sklearn.neural_network.tests.test_mlp.alpha_values->numpy.arange(2)
A:sklearn.neural_network.tests.test_mlp.mlp->MLPClassifier(solver='lbfgs', hidden_layer_sizes=15, random_state=1)
A:sklearn.neural_network.tests.test_mlp.X->numpy.random.random((n_samples, n_features))
A:sklearn.neural_network.tests.test_mlp.y->numpy.array([0])
A:sklearn.neural_network.tests.test_mlp.mlp.coefs_[0]->numpy.array([[0.1, 0.2], [0.3, 0.1], [0.5, 0]])
A:sklearn.neural_network.tests.test_mlp.mlp.coefs_[1]->numpy.array([[0.1], [0.2]])
A:sklearn.neural_network.tests.test_mlp.mlp.intercepts_[0]->numpy.array([0.1, 0.1])
A:sklearn.neural_network.tests.test_mlp.mlp.intercepts_[1]->numpy.array([1.0])
A:sklearn.neural_network.tests.test_mlp.Y->LabelBinarizer().fit_transform(y)
A:sklearn.neural_network.tests.test_mlp.theta->numpy.hstack([l.ravel() for l in mlp.coefs_ + mlp.intercepts_])
A:sklearn.neural_network.tests.test_mlp.[value, grad]->loss_grad_fun(theta)
A:sklearn.neural_network.tests.test_mlp.numgrad->numpy.zeros(np.size(theta))
A:sklearn.neural_network.tests.test_mlp.n->numpy.size(theta, 0)
A:sklearn.neural_network.tests.test_mlp.E->numpy.eye(n)
A:sklearn.neural_network.tests.test_mlp.y_predict->MLPClassifier(solver='lbfgs', hidden_layer_sizes=15, random_state=1).predict(X_test)
A:sklearn.neural_network.tests.test_mlp.(X, y)->make_regression(n_samples=200, n_targets=5)
A:sklearn.neural_network.tests.test_mlp.clf->MLPClassifier(hidden_layer_sizes=2, solver='lbfgs', warm_start=True).fit(X, y)
A:sklearn.neural_network.tests.test_mlp.pred1->MLPClassifier(solver='lbfgs', hidden_layer_sizes=15, random_state=1).predict(X)
A:sklearn.neural_network.tests.test_mlp.pred2->MLPClassifier(solver='lbfgs', hidden_layer_sizes=15, random_state=1).predict(X_sparse)
A:sklearn.neural_network.tests.test_mlp.score->MLPClassifier(solver='lbfgs', hidden_layer_sizes=15, random_state=1).score(X, y)
A:sklearn.neural_network.tests.test_mlp.y_proba->MLPClassifier(hidden_layer_sizes=2, solver='lbfgs', warm_start=True).fit(X, y).predict_proba(X)
A:sklearn.neural_network.tests.test_mlp.y_log_proba->MLPClassifier(hidden_layer_sizes=2, solver='lbfgs', warm_start=True).fit(X, y).predict_log_proba(X)
A:sklearn.neural_network.tests.test_mlp.proba_max->MLPClassifier(hidden_layer_sizes=2, solver='lbfgs', warm_start=True).fit(X, y).predict_proba(X).argmax(axis=1)
A:sklearn.neural_network.tests.test_mlp.proba_log_max->MLPClassifier(hidden_layer_sizes=2, solver='lbfgs', warm_start=True).fit(X, y).predict_log_proba(X).argmax(axis=1)
A:sklearn.neural_network.tests.test_mlp.(X, Y)->make_multilabel_classification(n_samples=50, random_state=0, return_indicator=True)
A:sklearn.neural_network.tests.test_mlp.X_sparse->csr_matrix(X)
A:sklearn.neural_network.tests.test_mlp.sys.stdoutoutput->StringIO()
A:sklearn.neural_network.tests.test_mlp.y_2classes->numpy.array([0] * 75 + [1] * 75)
A:sklearn.neural_network.tests.test_mlp.y_3classes->numpy.array([0] * 40 + [1] * 40 + [2] * 70)
A:sklearn.neural_network.tests.test_mlp.y_3classes_alt->numpy.array([0] * 50 + [1] * 50 + [3] * 50)
A:sklearn.neural_network.tests.test_mlp.y_4classes->numpy.array([0] * 37 + [1] * 37 + [2] * 38 + [3] * 38)
A:sklearn.neural_network.tests.test_mlp.y_5classes->numpy.array([0] * 30 + [1] * 30 + [2] * 30 + [3] * 30 + [4] * 30)
sklearn.neural_network.tests.test_mlp.test_adaptive_learning_rate()
sklearn.neural_network.tests.test_mlp.test_alpha()
sklearn.neural_network.tests.test_mlp.test_early_stopping()
sklearn.neural_network.tests.test_mlp.test_fit()
sklearn.neural_network.tests.test_mlp.test_gradient()
sklearn.neural_network.tests.test_mlp.test_lbfgs_classification()
sklearn.neural_network.tests.test_mlp.test_lbfgs_regression()
sklearn.neural_network.tests.test_mlp.test_learning_rate_warmstart()
sklearn.neural_network.tests.test_mlp.test_multilabel_classification()
sklearn.neural_network.tests.test_mlp.test_multioutput_regression()
sklearn.neural_network.tests.test_mlp.test_params_errors()
sklearn.neural_network.tests.test_mlp.test_partial_fit_classes_error()
sklearn.neural_network.tests.test_mlp.test_partial_fit_classification()
sklearn.neural_network.tests.test_mlp.test_partial_fit_errors()
sklearn.neural_network.tests.test_mlp.test_partial_fit_regression()
sklearn.neural_network.tests.test_mlp.test_partial_fit_unseen_classes()
sklearn.neural_network.tests.test_mlp.test_predict_proba_binary()
sklearn.neural_network.tests.test_mlp.test_predict_proba_multiclass()
sklearn.neural_network.tests.test_mlp.test_predict_proba_multilabel()
sklearn.neural_network.tests.test_mlp.test_sparse_matrices()
sklearn.neural_network.tests.test_mlp.test_tolerance()
sklearn.neural_network.tests.test_mlp.test_verbose_sgd()
sklearn.neural_network.tests.test_mlp.test_warm_start()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/feature_selection/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/feature_selection/from_model.py----------------------------------------
A:sklearn.feature_selection.from_model.importances->numpy.linalg.norm(estimator.coef_, axis=0, ord=norm_order)
A:sklearn.feature_selection.from_model.(scale, reference)->_calculate_threshold(estimator, scores, self.threshold).split('*')
A:sklearn.feature_selection.from_model.scale->float(scale.strip())
A:sklearn.feature_selection.from_model.reference->numpy.mean(importances)
A:sklearn.feature_selection.from_model.threshold->_calculate_threshold(estimator, scores, self.threshold)
A:sklearn.feature_selection.from_model.scores->_get_feature_importances(self.estimator_, self.norm_order)
A:sklearn.feature_selection.from_model.self.estimator_->clone(self.estimator)
sklearn.feature_selection.SelectFromModel(self,estimator,threshold=None,prefit=False,norm_order=1)
sklearn.feature_selection.SelectFromModel._get_support_mask(self)
sklearn.feature_selection.SelectFromModel.fit(self,X,y=None,**fit_params)
sklearn.feature_selection.SelectFromModel.partial_fit(self,X,y=None,**fit_params)
sklearn.feature_selection.SelectFromModel.threshold_(self)
sklearn.feature_selection.from_model.SelectFromModel(self,estimator,threshold=None,prefit=False,norm_order=1)
sklearn.feature_selection.from_model.SelectFromModel.__init__(self,estimator,threshold=None,prefit=False,norm_order=1)
sklearn.feature_selection.from_model.SelectFromModel._get_support_mask(self)
sklearn.feature_selection.from_model.SelectFromModel.fit(self,X,y=None,**fit_params)
sklearn.feature_selection.from_model.SelectFromModel.partial_fit(self,X,y=None,**fit_params)
sklearn.feature_selection.from_model.SelectFromModel.threshold_(self)
sklearn.feature_selection.from_model._calculate_threshold(estimator,importances,threshold)
sklearn.feature_selection.from_model._get_feature_importances(estimator,norm_order=1)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/feature_selection/mutual_info_.py----------------------------------------
A:sklearn.feature_selection.mutual_info_.x->numpy.zeros(X.shape[0])
A:sklearn.feature_selection.mutual_info_.y->scale(y, with_mean=False)
A:sklearn.feature_selection.mutual_info_.xy->numpy.hstack((x, y))
A:sklearn.feature_selection.mutual_info_.nn->NearestNeighbors()
A:sklearn.feature_selection.mutual_info_.radius->numpy.empty(n_samples)
A:sklearn.feature_selection.mutual_info_.ind->NearestNeighbors().radius_neighbors(radius=radius, return_distance=False)
A:sklearn.feature_selection.mutual_info_.nx->numpy.array([i.size for i in ind])
A:sklearn.feature_selection.mutual_info_.ny->numpy.array([i.size for i in ind])
A:sklearn.feature_selection.mutual_info_.c->c.reshape((-1, 1)).reshape((-1, 1))
A:sklearn.feature_selection.mutual_info_.label_counts->numpy.empty(n_samples)
A:sklearn.feature_selection.mutual_info_.k_all->numpy.empty(n_samples)
A:sklearn.feature_selection.mutual_info_.count->numpy.sum(mask)
A:sklearn.feature_selection.mutual_info_.k->min(n_neighbors, count - 1)
A:sklearn.feature_selection.mutual_info_.radius[mask]->numpy.nextafter(r[:, -1], 0)
A:sklearn.feature_selection.mutual_info_.n_samples->numpy.sum(mask)
A:sklearn.feature_selection.mutual_info_.m_all->numpy.array([i.size for i in ind])
A:sklearn.feature_selection.mutual_info_.columns->range(X.shape[1])
A:sklearn.feature_selection.mutual_info_.(X, y)->check_X_y(X, y, accept_sparse='csc', y_numeric=not discrete_target)
A:sklearn.feature_selection.mutual_info_.discrete_features->numpy.asarray(discrete_features)
A:sklearn.feature_selection.mutual_info_.discrete_mask->numpy.zeros(n_features, dtype=bool)
A:sklearn.feature_selection.mutual_info_.rng->check_random_state(random_state)
A:sklearn.feature_selection.mutual_info_.X->X.astype(float).astype(float)
A:sklearn.feature_selection.mutual_info_.X[:, continuous_mask]->scale(X[:, continuous_mask], with_mean=False, copy=False)
A:sklearn.feature_selection.mutual_info_.means->numpy.maximum(1, np.mean(np.abs(X[:, continuous_mask]), axis=0))
sklearn.feature_selection.mutual_info_._compute_mi(x,y,x_discrete,y_discrete,n_neighbors=3)
sklearn.feature_selection.mutual_info_._compute_mi_cc(x,y,n_neighbors)
sklearn.feature_selection.mutual_info_._compute_mi_cd(c,d,n_neighbors)
sklearn.feature_selection.mutual_info_._estimate_mi(X,y,discrete_features='auto',discrete_target=False,n_neighbors=3,copy=True,random_state=None)
sklearn.feature_selection.mutual_info_._iterate_columns(X,columns=None)
sklearn.feature_selection.mutual_info_.mutual_info_classif(X,y,discrete_features='auto',n_neighbors=3,copy=True,random_state=None)
sklearn.feature_selection.mutual_info_.mutual_info_regression(X,y,discrete_features='auto',n_neighbors=3,copy=True,random_state=None)
sklearn.feature_selection.mutual_info_classif(X,y,discrete_features='auto',n_neighbors=3,copy=True,random_state=None)
sklearn.feature_selection.mutual_info_regression(X,y,discrete_features='auto',n_neighbors=3,copy=True,random_state=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/feature_selection/rfe.py----------------------------------------
A:sklearn.feature_selection.rfe.(X_train, y_train)->_safe_split(estimator, X, y, train)
A:sklearn.feature_selection.rfe.(X_test, y_test)->_safe_split(estimator, X, y, test, train)
A:sklearn.feature_selection.rfe.(X, y)->check_X_y(X, y, 'csr')
A:sklearn.feature_selection.rfe.step->int(self.step)
A:sklearn.feature_selection.rfe.support_->numpy.ones(n_features, dtype=np.bool)
A:sklearn.feature_selection.rfe.ranking_->numpy.ones(n_features, dtype=np.int)
A:sklearn.feature_selection.rfe.estimator->clone(self.estimator)
A:sklearn.feature_selection.rfe.coefs->getattr(estimator, 'feature_importances_', None)
A:sklearn.feature_selection.rfe.ranks->numpy.ravel(ranks)
A:sklearn.feature_selection.rfe.threshold->min(step, np.sum(support_) - n_features_to_select)
A:sklearn.feature_selection.rfe.self.estimator_->clone(self.estimator)
A:sklearn.feature_selection.rfe.self.n_features_->numpy.ones(n_features, dtype=np.bool).sum()
A:sklearn.feature_selection.rfe.cv->check_cv(self.cv, y, is_classifier(self.estimator))
A:sklearn.feature_selection.rfe.scorer->check_scoring(self.estimator, scoring=self.scoring)
A:sklearn.feature_selection.rfe.rfe->RFE(estimator=self.estimator, n_features_to_select=n_features_to_select, step=self.step)
A:sklearn.feature_selection.rfe.scores->numpy.sum(scores, axis=0)
A:sklearn.feature_selection.rfe.n_features_to_select->max(n_features - np.argmax(scores) * step, n_features_to_select)
sklearn.feature_selection.RFE(self,estimator,n_features_to_select=None,step=1,verbose=0)
sklearn.feature_selection.RFE._estimator_type(self)
sklearn.feature_selection.RFE._fit(self,X,y,step_score=None)
sklearn.feature_selection.RFE._get_support_mask(self)
sklearn.feature_selection.RFE.decision_function(self,X)
sklearn.feature_selection.RFE.fit(self,X,y)
sklearn.feature_selection.RFE.predict(self,X)
sklearn.feature_selection.RFE.predict_log_proba(self,X)
sklearn.feature_selection.RFE.predict_proba(self,X)
sklearn.feature_selection.RFE.score(self,X,y)
sklearn.feature_selection.RFECV(self,estimator,step=1,cv=None,scoring=None,verbose=0,n_jobs=1)
sklearn.feature_selection.RFECV.fit(self,X,y)
sklearn.feature_selection.rfe.RFE(self,estimator,n_features_to_select=None,step=1,verbose=0)
sklearn.feature_selection.rfe.RFE.__init__(self,estimator,n_features_to_select=None,step=1,verbose=0)
sklearn.feature_selection.rfe.RFE._estimator_type(self)
sklearn.feature_selection.rfe.RFE._fit(self,X,y,step_score=None)
sklearn.feature_selection.rfe.RFE._get_support_mask(self)
sklearn.feature_selection.rfe.RFE.decision_function(self,X)
sklearn.feature_selection.rfe.RFE.fit(self,X,y)
sklearn.feature_selection.rfe.RFE.predict(self,X)
sklearn.feature_selection.rfe.RFE.predict_log_proba(self,X)
sklearn.feature_selection.rfe.RFE.predict_proba(self,X)
sklearn.feature_selection.rfe.RFE.score(self,X,y)
sklearn.feature_selection.rfe.RFECV(self,estimator,step=1,cv=None,scoring=None,verbose=0,n_jobs=1)
sklearn.feature_selection.rfe.RFECV.__init__(self,estimator,step=1,cv=None,scoring=None,verbose=0,n_jobs=1)
sklearn.feature_selection.rfe.RFECV.fit(self,X,y)
sklearn.feature_selection.rfe._rfe_single_fit(rfe,estimator,X,y,train,test,scorer)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py----------------------------------------
A:sklearn.feature_selection.univariate_selection.scores->_clean_nans(self.scores_)
A:sklearn.feature_selection.univariate_selection.n_classes->len(args)
A:sklearn.feature_selection.univariate_selection.n_samples_per_class->numpy.array([a.shape[0] for a in args])
A:sklearn.feature_selection.univariate_selection.n_samples->numpy.sum(n_samples_per_class)
A:sklearn.feature_selection.univariate_selection.ss_alldata->sum((safe_sqr(a).sum(axis=0) for a in args))
A:sklearn.feature_selection.univariate_selection.f->numpy.asarray(f).ravel()
A:sklearn.feature_selection.univariate_selection.prob->scipy.special.fdtrc(dfbn, dfwn, f)
A:sklearn.feature_selection.univariate_selection.(X, y)->check_X_y(X, y, ['csr', 'csc'], multi_output=True)
A:sklearn.feature_selection.univariate_selection.f_obs->numpy.asarray(f_obs, dtype=np.float64)
A:sklearn.feature_selection.univariate_selection.k->len(f_obs)
A:sklearn.feature_selection.univariate_selection.chisq->chisq.sum(axis=0).sum(axis=0)
A:sklearn.feature_selection.univariate_selection.X->check_array(X, accept_sparse='csr')
A:sklearn.feature_selection.univariate_selection.Y->numpy.append(1 - Y, Y, axis=1)
A:sklearn.feature_selection.univariate_selection.observed->safe_sparse_dot(Y.T, X)
A:sklearn.feature_selection.univariate_selection.feature_count->check_array(X, accept_sparse='csr').sum(axis=0).reshape(1, -1)
A:sklearn.feature_selection.univariate_selection.class_prob->numpy.append(1 - Y, Y, axis=1).mean(axis=0).reshape(1, -1)
A:sklearn.feature_selection.univariate_selection.expected->numpy.dot(class_prob.T, feature_count)
A:sklearn.feature_selection.univariate_selection.X_means->check_array(X, accept_sparse='csr').mean(axis=0)
A:sklearn.feature_selection.univariate_selection.X_norms->row_norms(X.T)
A:sklearn.feature_selection.univariate_selection.corr->safe_sparse_dot(y, X)
A:sklearn.feature_selection.univariate_selection.pv->scipy.stats.f.sf(F, 1, degrees_of_freedom)
A:sklearn.feature_selection.univariate_selection.score_func_ret->self.score_func(X, y)
A:sklearn.feature_selection.univariate_selection.self.pvalues_->numpy.asarray(self.pvalues_)
A:sklearn.feature_selection.univariate_selection.self.scores_->numpy.asarray(self.scores_)
A:sklearn.feature_selection.univariate_selection.treshold->scipy.stats.scoreatpercentile(scores, 100 - self.percentile)
A:sklearn.feature_selection.univariate_selection.max_feats->int(len(scores) * self.percentile / 100)
A:sklearn.feature_selection.univariate_selection.mask->numpy.zeros(scores.shape, dtype=bool)
A:sklearn.feature_selection.univariate_selection.n_features->len(self.pvalues_)
A:sklearn.feature_selection.univariate_selection.sv->numpy.sort(self.pvalues_)
A:sklearn.feature_selection.univariate_selection.selector->self._make_selector()
A:sklearn.feature_selection.univariate_selection.possible_params->self._make_selector()._get_param_names()
sklearn.feature_selection.GenericUnivariateSelect(self,score_func=f_classif,mode='percentile',param=1e-05)
sklearn.feature_selection.GenericUnivariateSelect._check_params(self,X,y)
sklearn.feature_selection.GenericUnivariateSelect._get_support_mask(self)
sklearn.feature_selection.GenericUnivariateSelect._make_selector(self)
sklearn.feature_selection.SelectFdr(self,score_func=f_classif,alpha=0.05)
sklearn.feature_selection.SelectFdr._get_support_mask(self)
sklearn.feature_selection.SelectFpr(self,score_func=f_classif,alpha=0.05)
sklearn.feature_selection.SelectFpr._get_support_mask(self)
sklearn.feature_selection.SelectFwe(self,score_func=f_classif,alpha=0.05)
sklearn.feature_selection.SelectFwe._get_support_mask(self)
sklearn.feature_selection.SelectKBest(self,score_func=f_classif,k=10)
sklearn.feature_selection.SelectKBest._check_params(self,X,y)
sklearn.feature_selection.SelectKBest._get_support_mask(self)
sklearn.feature_selection.SelectPercentile(self,score_func=f_classif,percentile=10)
sklearn.feature_selection.SelectPercentile._check_params(self,X,y)
sklearn.feature_selection.SelectPercentile._get_support_mask(self)
sklearn.feature_selection.chi2(X,y)
sklearn.feature_selection.f_classif(X,y)
sklearn.feature_selection.f_oneway(*args)
sklearn.feature_selection.f_regression(X,y,center=True)
sklearn.feature_selection.univariate_selection.GenericUnivariateSelect(self,score_func=f_classif,mode='percentile',param=1e-05)
sklearn.feature_selection.univariate_selection.GenericUnivariateSelect.__init__(self,score_func=f_classif,mode='percentile',param=1e-05)
sklearn.feature_selection.univariate_selection.GenericUnivariateSelect._check_params(self,X,y)
sklearn.feature_selection.univariate_selection.GenericUnivariateSelect._get_support_mask(self)
sklearn.feature_selection.univariate_selection.GenericUnivariateSelect._make_selector(self)
sklearn.feature_selection.univariate_selection.SelectFdr(self,score_func=f_classif,alpha=0.05)
sklearn.feature_selection.univariate_selection.SelectFdr.__init__(self,score_func=f_classif,alpha=0.05)
sklearn.feature_selection.univariate_selection.SelectFdr._get_support_mask(self)
sklearn.feature_selection.univariate_selection.SelectFpr(self,score_func=f_classif,alpha=0.05)
sklearn.feature_selection.univariate_selection.SelectFpr.__init__(self,score_func=f_classif,alpha=0.05)
sklearn.feature_selection.univariate_selection.SelectFpr._get_support_mask(self)
sklearn.feature_selection.univariate_selection.SelectFwe(self,score_func=f_classif,alpha=0.05)
sklearn.feature_selection.univariate_selection.SelectFwe.__init__(self,score_func=f_classif,alpha=0.05)
sklearn.feature_selection.univariate_selection.SelectFwe._get_support_mask(self)
sklearn.feature_selection.univariate_selection.SelectKBest(self,score_func=f_classif,k=10)
sklearn.feature_selection.univariate_selection.SelectKBest.__init__(self,score_func=f_classif,k=10)
sklearn.feature_selection.univariate_selection.SelectKBest._check_params(self,X,y)
sklearn.feature_selection.univariate_selection.SelectKBest._get_support_mask(self)
sklearn.feature_selection.univariate_selection.SelectPercentile(self,score_func=f_classif,percentile=10)
sklearn.feature_selection.univariate_selection.SelectPercentile.__init__(self,score_func=f_classif,percentile=10)
sklearn.feature_selection.univariate_selection.SelectPercentile._check_params(self,X,y)
sklearn.feature_selection.univariate_selection.SelectPercentile._get_support_mask(self)
sklearn.feature_selection.univariate_selection._BaseFilter(self,score_func)
sklearn.feature_selection.univariate_selection._BaseFilter.__init__(self,score_func)
sklearn.feature_selection.univariate_selection._BaseFilter._check_params(self,X,y)
sklearn.feature_selection.univariate_selection._BaseFilter.fit(self,X,y)
sklearn.feature_selection.univariate_selection._chisquare(f_obs,f_exp)
sklearn.feature_selection.univariate_selection._clean_nans(scores)
sklearn.feature_selection.univariate_selection.chi2(X,y)
sklearn.feature_selection.univariate_selection.f_classif(X,y)
sklearn.feature_selection.univariate_selection.f_oneway(*args)
sklearn.feature_selection.univariate_selection.f_regression(X,y,center=True)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/feature_selection/variance_threshold.py----------------------------------------
A:sklearn.feature_selection.variance_threshold.X->check_array(X, ('csr', 'csc'), dtype=np.float64)
A:sklearn.feature_selection.variance_threshold.(_, self.variances_)->mean_variance_axis(X, axis=0)
A:sklearn.feature_selection.variance_threshold.self.variances_->numpy.var(X, axis=0)
sklearn.feature_selection.VarianceThreshold(self,threshold=0.0)
sklearn.feature_selection.VarianceThreshold._get_support_mask(self)
sklearn.feature_selection.VarianceThreshold.fit(self,X,y=None)
sklearn.feature_selection.variance_threshold.VarianceThreshold(self,threshold=0.0)
sklearn.feature_selection.variance_threshold.VarianceThreshold.__init__(self,threshold=0.0)
sklearn.feature_selection.variance_threshold.VarianceThreshold._get_support_mask(self)
sklearn.feature_selection.variance_threshold.VarianceThreshold.fit(self,X,y=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/feature_selection/base.py----------------------------------------
A:sklearn.feature_selection.base.mask->self.get_support()
A:sklearn.feature_selection.base.X->check_array(X)
A:sklearn.feature_selection.base.it->self.inverse_transform(np.diff(X.indptr).reshape(1, -1))
A:sklearn.feature_selection.base.col_nonzeros->self.inverse_transform(np.diff(X.indptr).reshape(1, -1)).ravel()
A:sklearn.feature_selection.base.indptr->numpy.concatenate([[0], np.cumsum(col_nonzeros)])
A:sklearn.feature_selection.base.Xt->numpy.zeros((X.shape[0], support.size), dtype=X.dtype)
A:sklearn.feature_selection.base.support->self.get_support()
sklearn.feature_selection.base.SelectorMixin(six.with_metaclass(ABCMeta,TransformerMixin))
sklearn.feature_selection.base.SelectorMixin._get_support_mask(self)
sklearn.feature_selection.base.SelectorMixin.get_support(self,indices=False)
sklearn.feature_selection.base.SelectorMixin.inverse_transform(self,X)
sklearn.feature_selection.base.SelectorMixin.transform(self,X)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/feature_selection/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/feature_selection/tests/test_base.py----------------------------------------
A:sklearn.feature_selection.tests.test_base.X->numpy.arange(20).reshape(2, 10)
A:sklearn.feature_selection.tests.test_base.mask->numpy.zeros(self.n_input_feats, dtype=bool)
A:sklearn.feature_selection.tests.test_base.Xt->numpy.arange(0, 20, 2).reshape(2, 5)
A:sklearn.feature_selection.tests.test_base.Xinv->numpy.arange(20).reshape(2, 10).copy()
A:sklearn.feature_selection.tests.test_base.feature_names->list('ABCDEFGHIJ')
A:sklearn.feature_selection.tests.test_base.feature_names_inv->numpy.array(feature_names)
A:sklearn.feature_selection.tests.test_base.sel->StepSelector()
A:sklearn.feature_selection.tests.test_base.Xt_actual->StepSelector().fit(sparse(X)).transform(sparse(X))
A:sklearn.feature_selection.tests.test_base.Xt_actual2->StepSelector().fit_transform(sparse(X))
A:sklearn.feature_selection.tests.test_base.names_t_actual->StepSelector().transform([feature_names])
A:sklearn.feature_selection.tests.test_base.Xinv_actual->StepSelector().fit(sparse(X)).inverse_transform(sparse(Xt))
A:sklearn.feature_selection.tests.test_base.names_inv_actual->StepSelector().inverse_transform([feature_names_t])
sklearn.feature_selection.tests.test_base.StepSelector(self,step=2)
sklearn.feature_selection.tests.test_base.StepSelector.__init__(self,step=2)
sklearn.feature_selection.tests.test_base.StepSelector._get_support_mask(self)
sklearn.feature_selection.tests.test_base.StepSelector.fit(self,X,y=None)
sklearn.feature_selection.tests.test_base.test_get_support()
sklearn.feature_selection.tests.test_base.test_inverse_transform_dense()
sklearn.feature_selection.tests.test_base.test_inverse_transform_sparse()
sklearn.feature_selection.tests.test_base.test_transform_dense()
sklearn.feature_selection.tests.test_base.test_transform_sparse()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/feature_selection/tests/test_mutual_info.py----------------------------------------
A:sklearn.feature_selection.tests.test_mutual_info.x->numpy.hstack((x, 2))
A:sklearn.feature_selection.tests.test_mutual_info.y->numpy.array([0, 1, 2, 2, 1], dtype=float)
A:sklearn.feature_selection.tests.test_mutual_info.mean->numpy.zeros(4)
A:sklearn.feature_selection.tests.test_mutual_info.cov->numpy.array([[1, 0.5, 2, 1], [0, 1, 0.1, 0.0], [0, 0.1, 1, 0.1], [0, 0.1, 0.1, 1]]).dot(T.T)
A:sklearn.feature_selection.tests.test_mutual_info.rng->check_random_state(0)
A:sklearn.feature_selection.tests.test_mutual_info.Z->check_random_state(0).multivariate_normal(mean, cov, size=1000)
A:sklearn.feature_selection.tests.test_mutual_info.I_computed->_compute_mi(x, y, True, False, n_neighbors)
A:sklearn.feature_selection.tests.test_mutual_info.y[mask]->numpy.random.uniform(-1, 1, size=np.sum(mask))
A:sklearn.feature_selection.tests.test_mutual_info.y[~mask]->numpy.random.uniform(0, 2, size=np.sum(~mask))
A:sklearn.feature_selection.tests.test_mutual_info.mi_1->mutual_info(X, y, discrete_features='auto', random_state=0)
A:sklearn.feature_selection.tests.test_mutual_info.mi_2->mutual_info(X, y, discrete_features=False, random_state=0)
A:sklearn.feature_selection.tests.test_mutual_info.X->numpy.array([[0, 0, 0], [1, 1, 0], [2, 0, 1], [2, 0, 1], [2, 0, 1]], dtype=float)
A:sklearn.feature_selection.tests.test_mutual_info.mi->mutual_info_classif(X, y, discrete_features=[2], n_neighbors=3, random_state=0)
A:sklearn.feature_selection.tests.test_mutual_info.T->numpy.array([[1, 0.5, 2, 1], [0, 1, 0.1, 0.0], [0, 0.1, 1, 0.1], [0, 0.1, 0.1, 1]])
A:sklearn.feature_selection.tests.test_mutual_info.mi_nn->mutual_info_classif(X, y, discrete_features=[2], n_neighbors=n_neighbors, random_state=0)
A:sklearn.feature_selection.tests.test_mutual_info.X_csr->csr_matrix(X)
A:sklearn.feature_selection.tests.test_mutual_info.mi_3->mutual_info(X_csr, y, discrete_features='auto', random_state=0)
A:sklearn.feature_selection.tests.test_mutual_info.mi_4->mutual_info(X_csr, y, discrete_features=True, random_state=0)
sklearn.feature_selection.tests.test_mutual_info.test_compute_mi_cc()
sklearn.feature_selection.tests.test_mutual_info.test_compute_mi_cd()
sklearn.feature_selection.tests.test_mutual_info.test_compute_mi_cd_unique_label()
sklearn.feature_selection.tests.test_mutual_info.test_compute_mi_dd()
sklearn.feature_selection.tests.test_mutual_info.test_mutual_info_classif_discrete()
sklearn.feature_selection.tests.test_mutual_info.test_mutual_info_classif_mixed()
sklearn.feature_selection.tests.test_mutual_info.test_mutual_info_options()
sklearn.feature_selection.tests.test_mutual_info.test_mutual_info_regression()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/feature_selection/tests/test_variance_threshold.py----------------------------------------
A:sklearn.feature_selection.tests.test_variance_threshold.sel->VarianceThreshold().fit(X)
A:sklearn.feature_selection.tests.test_variance_threshold.X->VarianceThreshold(threshold=0.4).fit_transform(X)
sklearn.feature_selection.tests.test_variance_threshold.test_variance_threshold()
sklearn.feature_selection.tests.test_variance_threshold.test_zero_variance()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/feature_selection/tests/test_rfe.py----------------------------------------
A:sklearn.feature_selection.tests.test_rfe.self.coef_->numpy.ones(X.shape[1], dtype=np.float64)
A:sklearn.feature_selection.tests.test_rfe.generator->check_random_state(0)
A:sklearn.feature_selection.tests.test_rfe.iris->load_iris()
A:sklearn.feature_selection.tests.test_rfe.clf->MockClassifier()
A:sklearn.feature_selection.tests.test_rfe.rfe->RFE(estimator=SVC(kernel='linear'), n_features_to_select=n_features_to_select, step=step)
A:sklearn.feature_selection.tests.test_rfe.clf_svc->SVC(kernel='linear')
A:sklearn.feature_selection.tests.test_rfe.rfe_svc->RFE(estimator=clf_svc, n_features_to_select=4, step=0.1)
A:sklearn.feature_selection.tests.test_rfe.X_sparse->scipy.sparse.csr_matrix(X)
A:sklearn.feature_selection.tests.test_rfe.X_r->RFECV(estimator=SVC(kernel='linear')).transform(X)
A:sklearn.feature_selection.tests.test_rfe.clf_sparse->SVC(kernel='linear')
A:sklearn.feature_selection.tests.test_rfe.rfe_sparse->RFE(estimator=clf_sparse, n_features_to_select=4, step=0.1)
A:sklearn.feature_selection.tests.test_rfe.X_r_sparse->RFECV(estimator=SVC(kernel='linear'), step=0.2, cv=5).transform(X_sparse)
A:sklearn.feature_selection.tests.test_rfe.y->check_random_state(0).rand(100).round()
A:sklearn.feature_selection.tests.test_rfe.rfecv->RFECV(estimator=SVC(kernel='linear'))
A:sklearn.feature_selection.tests.test_rfe.rfecv_sparse->RFECV(estimator=SVC(kernel='linear'), step=0.2, cv=5)
A:sklearn.feature_selection.tests.test_rfe.scoring->make_scorer(zero_one_loss, greater_is_better=False)
A:sklearn.feature_selection.tests.test_rfe.scorer->get_scorer('accuracy')
A:sklearn.feature_selection.tests.test_rfe.sys.stdout->StringIO()
A:sklearn.feature_selection.tests.test_rfe.score->cross_val_score(rfe, iris.data, iris.target)
A:sklearn.feature_selection.tests.test_rfe.(X, y)->make_friedman1(n_samples=50, n_features=n_features, random_state=0)
A:sklearn.feature_selection.tests.test_rfe.estimator->SVR(kernel='linear')
A:sklearn.feature_selection.tests.test_rfe.selector->RFE(estimator, step=5)
A:sklearn.feature_selection.tests.test_rfe.sel->RFE(estimator, step=5).fit(X, y)
A:sklearn.feature_selection.tests.test_rfe.X->check_random_state(0).normal(size=(100, n_features))
sklearn.feature_selection.tests.test_rfe.MockClassifier(self,foo_param=0)
sklearn.feature_selection.tests.test_rfe.MockClassifier.__init__(self,foo_param=0)
sklearn.feature_selection.tests.test_rfe.MockClassifier.fit(self,X,Y)
sklearn.feature_selection.tests.test_rfe.MockClassifier.get_params(self,deep=True)
sklearn.feature_selection.tests.test_rfe.MockClassifier.predict(self,T)
sklearn.feature_selection.tests.test_rfe.MockClassifier.score(self,X=None,Y=None)
sklearn.feature_selection.tests.test_rfe.MockClassifier.set_params(self,**params)
sklearn.feature_selection.tests.test_rfe.test_number_of_subsets_of_features()
sklearn.feature_selection.tests.test_rfe.test_rfe()
sklearn.feature_selection.tests.test_rfe.test_rfe_cv_n_jobs()
sklearn.feature_selection.tests.test_rfe.test_rfe_estimator_tags()
sklearn.feature_selection.tests.test_rfe.test_rfe_features_importance()
sklearn.feature_selection.tests.test_rfe.test_rfe_min_step()
sklearn.feature_selection.tests.test_rfe.test_rfe_mockclassifier()
sklearn.feature_selection.tests.test_rfe.test_rfecv()
sklearn.feature_selection.tests.test_rfe.test_rfecv_mockclassifier()
sklearn.feature_selection.tests.test_rfe.test_rfecv_verbose_output()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/feature_selection/tests/test_from_model.py----------------------------------------
A:sklearn.feature_selection.tests.test_from_model.iris->sklearn.datasets.load_iris()
A:sklearn.feature_selection.tests.test_from_model.rng->numpy.random.RandomState(0)
A:sklearn.feature_selection.tests.test_from_model.clf->SGDClassifier(alpha=0.1, max_iter=10, shuffle=True, random_state=0, tol=None)
A:sklearn.feature_selection.tests.test_from_model.model->SelectFromModel(clf, threshold='0.1 * mean')
A:sklearn.feature_selection.tests.test_from_model.est->RandomForestClassifier(n_estimators=50, random_state=0)
A:sklearn.feature_selection.tests.test_from_model.transformer->SelectFromModel(estimator=est)
A:sklearn.feature_selection.tests.test_from_model.(X, y)->sklearn.datasets.make_classification(n_samples=1000, n_features=10, n_informative=3, n_redundant=0, n_repeated=0, shuffle=False, random_state=0, n_classes=4)
A:sklearn.feature_selection.tests.test_from_model.X_new->SelectFromModel(estimator=est).transform(X)
A:sklearn.feature_selection.tests.test_from_model.sample_weight->numpy.ones(y.shape)
A:sklearn.feature_selection.tests.test_from_model.mask->SelectFromModel(estimator=est)._get_support_mask()
A:sklearn.feature_selection.tests.test_from_model.weighted_mask->SelectFromModel(estimator=est)._get_support_mask()
A:sklearn.feature_selection.tests.test_from_model.reweighted_mask->SelectFromModel(estimator=est)._get_support_mask()
A:sklearn.feature_selection.tests.test_from_model.importances->numpy.linalg.norm(est.coef_, axis=0, ord=order)
A:sklearn.feature_selection.tests.test_from_model.X_transform->SelectFromModel(clf, threshold='0.1 * mean').transform(data)
sklearn.feature_selection.tests.test_from_model.test_2d_coef()
sklearn.feature_selection.tests.test_from_model.test_calling_fit_reinitializes()
sklearn.feature_selection.tests.test_from_model.test_coef_default_threshold()
sklearn.feature_selection.tests.test_from_model.test_feature_importances()
sklearn.feature_selection.tests.test_from_model.test_input_estimator_unchanged()
sklearn.feature_selection.tests.test_from_model.test_invalid_input()
sklearn.feature_selection.tests.test_from_model.test_partial_fit()
sklearn.feature_selection.tests.test_from_model.test_prefit()
sklearn.feature_selection.tests.test_from_model.test_sample_weight()
sklearn.feature_selection.tests.test_from_model.test_threshold_string()
sklearn.feature_selection.tests.test_from_model.test_threshold_without_refitting()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/feature_selection/tests/test_chi2.py----------------------------------------
A:sklearn.feature_selection.tests.test_chi2.chi2->mkchi2(k=2).fit(Xsp, y)
A:sklearn.feature_selection.tests.test_chi2.Xsp->csr_matrix(X, dtype=np.float64)
A:sklearn.feature_selection.tests.test_chi2.Xtrans->Xtrans.toarray().toarray()
A:sklearn.feature_selection.tests.test_chi2.Xtrans2->mkchi2(k=2).fit_transform(Xsp, y).toarray()
A:sklearn.feature_selection.tests.test_chi2.Xcoo->coo_matrix(X)
A:sklearn.feature_selection.tests.test_chi2.(chi, p)->chi2([[1, 0], [0, 0]], [1, 0])
A:sklearn.feature_selection.tests.test_chi2.obs->numpy.array([[2.0, 2.0], [1.0, 1.0]])
A:sklearn.feature_selection.tests.test_chi2.exp->numpy.array([[1.5, 1.5], [1.5, 1.5]])
A:sklearn.feature_selection.tests.test_chi2.(chi_scp, p_scp)->scipy.stats.chisquare(obs, exp)
A:sklearn.feature_selection.tests.test_chi2.(chi_our, p_our)->_chisquare(obs, exp)
sklearn.feature_selection.tests.test_chi2.mkchi2(k)
sklearn.feature_selection.tests.test_chi2.test_chi2()
sklearn.feature_selection.tests.test_chi2.test_chi2_coo()
sklearn.feature_selection.tests.test_chi2.test_chi2_negative()
sklearn.feature_selection.tests.test_chi2.test_chi2_unused_feature()
sklearn.feature_selection.tests.test_chi2.test_chisquare()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/feature_selection/tests/test_feature_select.py----------------------------------------
A:sklearn.feature_selection.tests.test_feature_select.rng->numpy.random.RandomState(0)
A:sklearn.feature_selection.tests.test_feature_select.X1->ignore_warnings(sel.fit_transform)([X], y)
A:sklearn.feature_selection.tests.test_feature_select.(f, pv)->scipy.stats.f_oneway(X1, X2)
A:sklearn.feature_selection.tests.test_feature_select.(f2, pv2)->f_oneway(X1, X2)
A:sklearn.feature_selection.tests.test_feature_select.X->numpy.random.RandomState(0).rand(40, 10)
A:sklearn.feature_selection.tests.test_feature_select.y->numpy.random.RandomState(0).randint(0, 4, size=40)
A:sklearn.feature_selection.tests.test_feature_select.(fint, pint)->f_oneway(X, y)
A:sklearn.feature_selection.tests.test_feature_select.(f, p)->f_oneway(X.astype(np.float), y)
A:sklearn.feature_selection.tests.test_feature_select.(X, y)->make_regression(n_samples=100, n_features=10, n_informative=2, shuffle=False, random_state=0, noise=10)
A:sklearn.feature_selection.tests.test_feature_select.(F, pv)->f_classif(X, y)
A:sklearn.feature_selection.tests.test_feature_select.(F_sparse, pv_sparse)->f_regression(sparse.csr_matrix(X), y, center=False)
A:sklearn.feature_selection.tests.test_feature_select.(F1, pv1)->f_regression(X, y)
A:sklearn.feature_selection.tests.test_feature_select.(F2, pv2)->f_regression(X, y.astype(np.float))
A:sklearn.feature_selection.tests.test_feature_select.Y->numpy.ones(n_samples)
A:sklearn.feature_selection.tests.test_feature_select.(F1, _)->f_regression(X, Y, center=True)
A:sklearn.feature_selection.tests.test_feature_select.(F2, _)->f_regression(X, Y, center=False)
A:sklearn.feature_selection.tests.test_feature_select.univariate_filter->SelectPercentile(mutual_info_regression, percentile=20)
A:sklearn.feature_selection.tests.test_feature_select.X_r->SelectPercentile(mutual_info_regression, percentile=20).fit(X, y).transform(X)
A:sklearn.feature_selection.tests.test_feature_select.X_r2->GenericUnivariateSelect(mutual_info_regression, mode='percentile', param=20).fit(X, y).transform(X)
A:sklearn.feature_selection.tests.test_feature_select.support->SelectPercentile(mutual_info_regression, percentile=20).get_support()
A:sklearn.feature_selection.tests.test_feature_select.gtruth->numpy.zeros(10)
A:sklearn.feature_selection.tests.test_feature_select.X_r2inv->SelectPercentile(mutual_info_regression, percentile=20).inverse_transform(X_r2)
A:sklearn.feature_selection.tests.test_feature_select.support_mask->safe_mask(X_r2inv, support)
A:sklearn.feature_selection.tests.test_feature_select.X_selected->assert_warns_message(UserWarning, 'No features were selected', selector.transform, X)
A:sklearn.feature_selection.tests.test_feature_select.X_2->numpy.random.RandomState(0).rand(40, 10).copy()
A:sklearn.feature_selection.tests.test_feature_select.(scores, pvalues)->chi2(X, y)
A:sklearn.feature_selection.tests.test_feature_select.filter_fdr->SelectFdr(chi2, alpha=0.1)
A:sklearn.feature_selection.tests.test_feature_select.support_fdr->SelectFdr(chi2, alpha=0.1).get_support()
A:sklearn.feature_selection.tests.test_feature_select.filter_kbest->SelectKBest(chi2, k=1)
A:sklearn.feature_selection.tests.test_feature_select.support_kbest->SelectKBest(chi2, k=1).get_support()
A:sklearn.feature_selection.tests.test_feature_select.filter_percentile->SelectPercentile(chi2, percentile=50)
A:sklearn.feature_selection.tests.test_feature_select.support_percentile->SelectPercentile(chi2, percentile=50).get_support()
A:sklearn.feature_selection.tests.test_feature_select.filter_fpr->SelectFpr(chi2, alpha=0.1)
A:sklearn.feature_selection.tests.test_feature_select.support_fpr->SelectFpr(chi2, alpha=0.1).get_support()
A:sklearn.feature_selection.tests.test_feature_select.filter_fwe->SelectFwe(chi2, alpha=0.1)
A:sklearn.feature_selection.tests.test_feature_select.support_fwe->SelectFwe(chi2, alpha=0.1).get_support()
A:sklearn.feature_selection.tests.test_feature_select.num_false_positives->numpy.sum(support[n_informative:] == 1)
A:sklearn.feature_selection.tests.test_feature_select.num_true_positives->numpy.sum(support[:n_informative] == 1)
A:sklearn.feature_selection.tests.test_feature_select.false_discovery_rate->numpy.mean([single_fdr(alpha, n_informative, random_state) for random_state in range(100)])
A:sklearn.feature_selection.tests.test_feature_select.sel->SelectKBest(chi2, k=n_features).fit(X_train, y_train)
A:sklearn.feature_selection.tests.test_feature_select.X2->ignore_warnings(sel.fit_transform)([X], y)
A:sklearn.feature_selection.tests.test_feature_select.X0->numpy.array([[10000, 9999, 9998], [1, 1, 1]])
A:sklearn.feature_selection.tests.test_feature_select.Xt->SelectPercentile(chi2, percentile=67).fit_transform(X, y)
A:sklearn.feature_selection.tests.test_feature_select.X_train->numpy.array([[0, 0, 0], [1, 1, 1]])
A:sklearn.feature_selection.tests.test_feature_select.X_test->SelectKBest(chi2, k=n_features).fit(X_train, y_train).transform([[0, 1, 2]])
sklearn.feature_selection.tests.test_feature_select.assert_best_scores_kept(score_filter)
sklearn.feature_selection.tests.test_feature_select.test_boundary_case_ch2()
sklearn.feature_selection.tests.test_feature_select.test_f_classif()
sklearn.feature_selection.tests.test_feature_select.test_f_classif_constant_feature()
sklearn.feature_selection.tests.test_feature_select.test_f_classif_multi_class()
sklearn.feature_selection.tests.test_feature_select.test_f_oneway_ints()
sklearn.feature_selection.tests.test_feature_select.test_f_oneway_vs_scipy_stats()
sklearn.feature_selection.tests.test_feature_select.test_f_regression()
sklearn.feature_selection.tests.test_feature_select.test_f_regression_center()
sklearn.feature_selection.tests.test_feature_select.test_f_regression_input_dtype()
sklearn.feature_selection.tests.test_feature_select.test_invalid_k()
sklearn.feature_selection.tests.test_feature_select.test_invalid_percentile()
sklearn.feature_selection.tests.test_feature_select.test_mutual_info_classif()
sklearn.feature_selection.tests.test_feature_select.test_mutual_info_regression()
sklearn.feature_selection.tests.test_feature_select.test_nans()
sklearn.feature_selection.tests.test_feature_select.test_no_feature_selected()
sklearn.feature_selection.tests.test_feature_select.test_score_func_error()
sklearn.feature_selection.tests.test_feature_select.test_scorefunc_multilabel()
sklearn.feature_selection.tests.test_feature_select.test_select_fdr_regression()
sklearn.feature_selection.tests.test_feature_select.test_select_fwe_regression()
sklearn.feature_selection.tests.test_feature_select.test_select_heuristics_classif()
sklearn.feature_selection.tests.test_feature_select.test_select_heuristics_regression()
sklearn.feature_selection.tests.test_feature_select.test_select_kbest_all()
sklearn.feature_selection.tests.test_feature_select.test_select_kbest_classif()
sklearn.feature_selection.tests.test_feature_select.test_select_kbest_regression()
sklearn.feature_selection.tests.test_feature_select.test_select_kbest_zero()
sklearn.feature_selection.tests.test_feature_select.test_select_percentile_classif()
sklearn.feature_selection.tests.test_feature_select.test_select_percentile_classif_sparse()
sklearn.feature_selection.tests.test_feature_select.test_select_percentile_regression()
sklearn.feature_selection.tests.test_feature_select.test_select_percentile_regression_full()
sklearn.feature_selection.tests.test_feature_select.test_selectkbest_tiebreaking()
sklearn.feature_selection.tests.test_feature_select.test_selectpercentile_tiebreaking()
sklearn.feature_selection.tests.test_feature_select.test_tied_pvalues()
sklearn.feature_selection.tests.test_feature_select.test_tied_scores()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/externals/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/externals/funcsigs.py----------------------------------------
A:sklearn.externals.funcsigs._WrapperDescriptor->type(type.__call__)
A:sklearn.externals.funcsigs._MethodWrapper->type(all.__call__)
A:sklearn.externals.funcsigs.meth->getattr(meth, name, meth)
A:sklearn.externals.funcsigs.sig->signature(call)
A:sklearn.externals.funcsigs.first->signature(call).parameters.values()[0].replace(kind=_POSITIONAL_ONLY)
A:sklearn.externals.funcsigs.new_params->OrderedDict(sig.parameters.items())
A:sklearn.externals.funcsigs.ba->signature(call).bind_partial(*partial_args, **partial_keywords)
A:sklearn.externals.funcsigs.msg->msg.format(arg=param.name).format(arg=param.name)
A:sklearn.externals.funcsigs.new_params[arg_name]->next(parameters).replace(default=arg_value, _partial_kwarg=True)
A:sklearn.externals.funcsigs.call->_get_user_defined_method(type(obj), '__call__', 'im_func')
A:sklearn.externals.funcsigs.new->_get_user_defined_method(obj, '__new__')
A:sklearn.externals.funcsigs.init->_get_user_defined_method(obj, '__init__')
A:sklearn.externals.funcsigs.obj->int.__new__(self, *args)
A:sklearn.externals.funcsigs._POSITIONAL_ONLY->_ParameterKind(0, name='POSITIONAL_ONLY')
A:sklearn.externals.funcsigs._POSITIONAL_OR_KEYWORD->_ParameterKind(1, name='POSITIONAL_OR_KEYWORD')
A:sklearn.externals.funcsigs._VAR_POSITIONAL->_ParameterKind(2, name='VAR_POSITIONAL')
A:sklearn.externals.funcsigs._KEYWORD_ONLY->_ParameterKind(3, name='KEYWORD_ONLY')
A:sklearn.externals.funcsigs._VAR_KEYWORD->_ParameterKind(4, name='VAR_KEYWORD')
A:sklearn.externals.funcsigs.name->str(idx)
A:sklearn.externals.funcsigs.formatted->str(param)
A:sklearn.externals.funcsigs.params->OrderedDict(((param.name, param) for param in parameters))
A:sklearn.externals.funcsigs.param->next(parameters)
A:sklearn.externals.funcsigs.positional->tuple(arg_names[:pos_count])
A:sklearn.externals.funcsigs.keyword_only_count->getattr(func_code, 'co_kwonlyargcount', 0)
A:sklearn.externals.funcsigs.annotations->getattr(func, '__annotations__', {})
A:sklearn.externals.funcsigs.kwdefaults->getattr(func, '__kwdefaults__', None)
A:sklearn.externals.funcsigs.pos_default_count->len(defaults)
A:sklearn.externals.funcsigs.annotation->getattr(func, '__annotations__', {}).get(name, _empty)
A:sklearn.externals.funcsigs.default->getattr(func, '__kwdefaults__', None).get(name, _empty)
A:sklearn.externals.funcsigs.parameters->iter(self.parameters.values())
A:sklearn.externals.funcsigs.other_positions->dict(((param, idx) for (idx, param) in enumerate(other.parameters.keys())))
A:sklearn.externals.funcsigs.arguments->OrderedDict()
A:sklearn.externals.funcsigs.arg_vals->iter(args)
A:sklearn.externals.funcsigs.arg_val->kwargs.pop(param_name)
A:sklearn.externals.funcsigs.arguments[param.name]->tuple(values)
A:sklearn.externals.funcsigs.rendered->'({0})'.format(', '.join(result))
A:sklearn.externals.funcsigs.anno->formatannotation(self.return_annotation)
sklearn.externals.funcsigs.BoundArguments(self,signature,arguments)
sklearn.externals.funcsigs.BoundArguments.__eq__(self,other)
sklearn.externals.funcsigs.BoundArguments.__hash__(self)
sklearn.externals.funcsigs.BoundArguments.__init__(self,signature,arguments)
sklearn.externals.funcsigs.BoundArguments.__ne__(self,other)
sklearn.externals.funcsigs.BoundArguments.args(self)
sklearn.externals.funcsigs.BoundArguments.kwargs(self)
sklearn.externals.funcsigs.BoundArguments.signature(self)
sklearn.externals.funcsigs.Parameter(self,name,kind,default=_empty,annotation=_empty,_partial_kwarg=False)
sklearn.externals.funcsigs.Parameter.__eq__(self,other)
sklearn.externals.funcsigs.Parameter.__hash__(self)
sklearn.externals.funcsigs.Parameter.__init__(self,name,kind,default=_empty,annotation=_empty,_partial_kwarg=False)
sklearn.externals.funcsigs.Parameter.__ne__(self,other)
sklearn.externals.funcsigs.Parameter.__repr__(self)
sklearn.externals.funcsigs.Parameter.__str__(self)
sklearn.externals.funcsigs.Parameter.annotation(self)
sklearn.externals.funcsigs.Parameter.default(self)
sklearn.externals.funcsigs.Parameter.kind(self)
sklearn.externals.funcsigs.Parameter.name(self)
sklearn.externals.funcsigs.Parameter.replace(self,name=_void,kind=_void,annotation=_void,default=_void,_partial_kwarg=_void)
sklearn.externals.funcsigs.Signature(self,parameters=None,return_annotation=_empty,__validate_parameters__=True)
sklearn.externals.funcsigs.Signature.__eq__(self,other)
sklearn.externals.funcsigs.Signature.__hash__(self)
sklearn.externals.funcsigs.Signature.__init__(self,parameters=None,return_annotation=_empty,__validate_parameters__=True)
sklearn.externals.funcsigs.Signature.__ne__(self,other)
sklearn.externals.funcsigs.Signature.__str__(self)
sklearn.externals.funcsigs.Signature._bind(self,args,kwargs,partial=False)
sklearn.externals.funcsigs.Signature.bind(self,*args,**kwargs)
sklearn.externals.funcsigs.Signature.bind_partial(self,*args,**kwargs)
sklearn.externals.funcsigs.Signature.from_function(cls,func)
sklearn.externals.funcsigs.Signature.parameters(self)
sklearn.externals.funcsigs.Signature.replace(self,parameters=_void,return_annotation=_void)
sklearn.externals.funcsigs.Signature.return_annotation(self)
sklearn.externals.funcsigs._ParameterKind(self,*args,**kwargs)
sklearn.externals.funcsigs._ParameterKind.__new__(self,*args,**kwargs)
sklearn.externals.funcsigs._ParameterKind.__repr__(self)
sklearn.externals.funcsigs._ParameterKind.__str__(self)
sklearn.externals.funcsigs._empty(object)
sklearn.externals.funcsigs._get_user_defined_method(cls,method_name,*nested)
sklearn.externals.funcsigs._void(object)
sklearn.externals.funcsigs.formatannotation(annotation,base_module=None)
sklearn.externals.funcsigs.signature(obj)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/externals/test_externals_setup.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/externals/six.py----------------------------------------
A:sklearn.externals.six.MAXSIZE->int((1 << 63) - 1)
A:sklearn.externals.six.result->self._resolve()
A:sklearn.externals.six.module->_import_module(self.mod)
A:sklearn.externals.six.movessys.modules[__name__ + '.moves']->_MovedItems(__name__ + '.moves')
A:sklearn.externals.six.sys.modules[__name__ + '.moves.urllib_parse']->Module_six_moves_urllib_parse(__name__ + '.moves.urllib_parse')
A:sklearn.externals.six.sys.modules[__name__ + '.moves.urllib.parse']->Module_six_moves_urllib_parse(__name__ + '.moves.urllib.parse')
A:sklearn.externals.six.sys.modules[__name__ + '.moves.urllib_error']->Module_six_moves_urllib_error(__name__ + '.moves.urllib_error')
A:sklearn.externals.six.sys.modules[__name__ + '.moves.urllib.error']->Module_six_moves_urllib_error(__name__ + '.moves.urllib.error')
A:sklearn.externals.six.sys.modules[__name__ + '.moves.urllib_request']->Module_six_moves_urllib_request(__name__ + '.moves.urllib_request')
A:sklearn.externals.six.sys.modules[__name__ + '.moves.urllib.request']->Module_six_moves_urllib_request(__name__ + '.moves.urllib.request')
A:sklearn.externals.six.sys.modules[__name__ + '.moves.urllib_response']->Module_six_moves_urllib_response(__name__ + '.moves.urllib_response')
A:sklearn.externals.six.sys.modules[__name__ + '.moves.urllib.response']->Module_six_moves_urllib_response(__name__ + '.moves.urllib.response')
A:sklearn.externals.six.sys.modules[__name__ + '.moves.urllib_robotparser']->Module_six_moves_urllib_robotparser(__name__ + '.moves.urllib_robotparser')
A:sklearn.externals.six.sys.modules[__name__ + '.moves.urllib.robotparser']->Module_six_moves_urllib_robotparser(__name__ + '.moves.urllib.robotparser')
A:sklearn.externals.six.sys.modules[__name__ + '.moves.urllib']->Module_six_moves_urllib(__name__ + '.moves.urllib')
A:sklearn.externals.six.get_method_function->operator.attrgetter(_meth_func)
A:sklearn.externals.six.get_method_self->operator.attrgetter(_meth_self)
A:sklearn.externals.six.get_function_closure->operator.attrgetter(_func_closure)
A:sklearn.externals.six.get_function_code->operator.attrgetter(_func_code)
A:sklearn.externals.six.get_function_defaults->operator.attrgetter(_func_defaults)
A:sklearn.externals.six.get_function_globals->operator.attrgetter(_func_globals)
A:sklearn.externals.six.int2byte->operator.methodcaller('to_bytes', 1, 'big')
A:sklearn.externals.six.byte2int->operator.itemgetter(0)
A:sklearn.externals.six.exec_->getattr(builtins, 'exec')
A:sklearn.externals.six.print_->getattr(builtins, 'print')
A:sklearn.externals.six.frame->sys._getframe(1)
A:sklearn.externals.six.fp->kwargs.pop('file', sys.stdout)
A:sklearn.externals.six.data->str(data)
A:sklearn.externals.six.sep->kwargs.pop('sep', None)
A:sklearn.externals.six.end->kwargs.pop('end', None)
A:sklearn.externals.six.newline->unicode('\n')
A:sklearn.externals.six.space->unicode(' ')
A:sklearn.externals.six.orig_vars->cls.__dict__.copy()
sklearn.externals.six.Module_six_moves_urllib(types.ModuleType)
sklearn.externals.six.Module_six_moves_urllib_error(types.ModuleType)
sklearn.externals.six.Module_six_moves_urllib_parse(types.ModuleType)
sklearn.externals.six.Module_six_moves_urllib_request(types.ModuleType)
sklearn.externals.six.Module_six_moves_urllib_response(types.ModuleType)
sklearn.externals.six.Module_six_moves_urllib_robotparser(types.ModuleType)
sklearn.externals.six.MovedAttribute(self,name,old_mod,new_mod,old_attr=None,new_attr=None)
sklearn.externals.six.MovedAttribute.__init__(self,name,old_mod,new_mod,old_attr=None,new_attr=None)
sklearn.externals.six.MovedAttribute._resolve(self)
sklearn.externals.six.MovedModule(self,name,old,new=None)
sklearn.externals.six.MovedModule.__init__(self,name,old,new=None)
sklearn.externals.six.MovedModule._resolve(self)
sklearn.externals.six._LazyDescr(self,name)
sklearn.externals.six._LazyDescr.__get__(self,obj,tp)
sklearn.externals.six._LazyDescr.__init__(self,name)
sklearn.externals.six._MovedItems(types.ModuleType)
sklearn.externals.six._add_doc(func,doc)
sklearn.externals.six._import_module(name)
sklearn.externals.six.add_metaclass(metaclass)
sklearn.externals.six.add_move(move)
sklearn.externals.six.iteritems(d,**kw)
sklearn.externals.six.iterkeys(d,**kw)
sklearn.externals.six.iterlists(d,**kw)
sklearn.externals.six.itervalues(d,**kw)
sklearn.externals.six.remove_move(name)
sklearn.externals.six.with_metaclass(meta,*bases)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/externals/setup.py----------------------------------------
A:sklearn.externals.setup.config->Configuration('externals', parent_package, top_path)
sklearn.externals.setup.configuration(parent_package='',top_path=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/externals/joblib/numpy_pickle.py----------------------------------------
A:sklearn.externals.joblib.numpy_pickle.buffersize->max(16 * 1024 ** 2 // array.itemsize, 1)
A:sklearn.externals.joblib.numpy_pickle.count->NumpyUnpickler(filename, fobj, mmap_mode=mmap_mode).np.multiply.reduce(self.shape)
A:sklearn.externals.joblib.numpy_pickle.array->self.read_array(unpickler)
A:sklearn.externals.joblib.numpy_pickle.read_count->min(max_read_count, count - i)
A:sklearn.externals.joblib.numpy_pickle.read_size->int(read_count * self.dtype.itemsize)
A:sklearn.externals.joblib.numpy_pickle.data->_read_bytes(unpickler.file_handle, read_size, 'array data')
A:sklearn.externals.joblib.numpy_pickle.array[i:i + read_count]->NumpyUnpickler(filename, fobj, mmap_mode=mmap_mode).np.frombuffer(data, dtype=self.dtype, count=read_count)
A:sklearn.externals.joblib.numpy_pickle.offset->NumpyUnpickler(filename, fobj, mmap_mode=mmap_mode).file_handle.tell()
A:sklearn.externals.joblib.numpy_pickle.marray->make_memmap(unpickler.filename, dtype=self.dtype, shape=self.shape, order=self.order, mode=unpickler.mmap_mode, offset=offset)
A:sklearn.externals.joblib.numpy_pickle.new_array->NumpyUnpickler(filename, fobj, mmap_mode=mmap_mode).np.core.multiarray._reconstruct(self.subclass, (0,), 'b')
A:sklearn.externals.joblib.numpy_pickle.dispatch->numpy_pickle_utils.Unpickler.dispatch.copy()
A:sklearn.externals.joblib.numpy_pickle.self.buffered->isinstance(self.file_handle, BinaryZlibFile)
A:sklearn.externals.joblib.numpy_pickle.wrapper->self._create_array_wrapper(obj)
A:sklearn.externals.joblib.numpy_pickle.obj->_unpickle(fobj, filename, mmap_mode)
A:sklearn.externals.joblib.numpy_pickle.self._dirname->os.path.dirname(filename)
A:sklearn.externals.joblib.numpy_pickle.array_wrapper->self.stack.pop()
A:sklearn.externals.joblib.numpy_pickle.filename->getattr(fobj, 'name', '')
A:sklearn.externals.joblib.numpy_pickle.is_filename->isinstance(filename, _basestring)
A:sklearn.externals.joblib.numpy_pickle.is_fileobj->hasattr(filename, 'write')
A:sklearn.externals.joblib.numpy_pickle.unpickler->NumpyUnpickler(filename, fobj, mmap_mode=mmap_mode)
A:sklearn.externals.joblib.numpy_pickle.new_exc->ValueError('You may be trying to read with python 3 a joblib pickle generated with python 2. This feature is not supported by joblib.')
sklearn.externals.joblib.dump(value,filename,compress=0,protocol=None,cache_size=None)
sklearn.externals.joblib.load(filename,mmap_mode=None)
sklearn.externals.joblib.numpy_pickle.NumpyArrayWrapper(self,subclass,shape,order,dtype,allow_mmap=False)
sklearn.externals.joblib.numpy_pickle.NumpyArrayWrapper.__init__(self,subclass,shape,order,dtype,allow_mmap=False)
sklearn.externals.joblib.numpy_pickle.NumpyArrayWrapper.read(self,unpickler)
sklearn.externals.joblib.numpy_pickle.NumpyArrayWrapper.read_array(self,unpickler)
sklearn.externals.joblib.numpy_pickle.NumpyArrayWrapper.read_mmap(self,unpickler)
sklearn.externals.joblib.numpy_pickle.NumpyArrayWrapper.write_array(self,array,pickler)
sklearn.externals.joblib.numpy_pickle.NumpyPickler(self,fp,protocol=None)
sklearn.externals.joblib.numpy_pickle.NumpyPickler.__init__(self,fp,protocol=None)
sklearn.externals.joblib.numpy_pickle.NumpyPickler._create_array_wrapper(self,array)
sklearn.externals.joblib.numpy_pickle.NumpyPickler.save(self,obj)
sklearn.externals.joblib.numpy_pickle.NumpyUnpickler(self,filename,file_handle,mmap_mode=None)
sklearn.externals.joblib.numpy_pickle.NumpyUnpickler.__init__(self,filename,file_handle,mmap_mode=None)
sklearn.externals.joblib.numpy_pickle.NumpyUnpickler.load_build(self)
sklearn.externals.joblib.numpy_pickle._unpickle(fobj,filename='',mmap_mode=None)
sklearn.externals.joblib.numpy_pickle.dump(value,filename,compress=0,protocol=None,cache_size=None)
sklearn.externals.joblib.numpy_pickle.load(filename,mmap_mode=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/externals/joblib/_multiprocessing_helpers.py----------------------------------------
A:sklearn.externals.joblib._multiprocessing_helpers._sem->multiprocessing.Semaphore()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/externals/joblib/logger.py----------------------------------------
A:sklearn.externals.joblib.logger.t->_squeeze_time(t)
A:sklearn.externals.joblib.logger.print_options->numpy.get_printoptions()
A:sklearn.externals.joblib.logger.out->pprint.pformat(obj, depth=depth, indent=indent)
A:sklearn.externals.joblib.logger.self.last_time->time.time()
A:sklearn.externals.joblib.logger.logfile->os.path.join(logdir, 'joblib.log')
sklearn.externals.joblib.Logger(self,depth=3)
sklearn.externals.joblib.Logger.debug(self,msg)
sklearn.externals.joblib.Logger.format(self,obj,indent=0)
sklearn.externals.joblib.Logger.warn(self,msg)
sklearn.externals.joblib.PrintTime(self,logfile=None,logdir=None)
sklearn.externals.joblib.logger.Logger(self,depth=3)
sklearn.externals.joblib.logger.Logger.__init__(self,depth=3)
sklearn.externals.joblib.logger.Logger.debug(self,msg)
sklearn.externals.joblib.logger.Logger.format(self,obj,indent=0)
sklearn.externals.joblib.logger.Logger.warn(self,msg)
sklearn.externals.joblib.logger.PrintTime(self,logfile=None,logdir=None)
sklearn.externals.joblib.logger.PrintTime.__init__(self,logfile=None,logdir=None)
sklearn.externals.joblib.logger._squeeze_time(t)
sklearn.externals.joblib.logger.format_time(t)
sklearn.externals.joblib.logger.pformat(obj,indent=0,depth=3)
sklearn.externals.joblib.logger.short_format_time(t)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py----------------------------------------
A:sklearn.externals.joblib._parallel_backends.result->ImmediateResult(func)
A:sklearn.externals.joblib._parallel_backends.n_jobs->self.effective_n_jobs(n_jobs)
A:sklearn.externals.joblib._parallel_backends.ideal_batch_size->int(old_batch_size * self.MIN_IDEAL_BATCH_DURATION / batch_duration)
A:sklearn.externals.joblib._parallel_backends.batch_size->max(2 * ideal_batch_size, 1)
A:sklearn.externals.joblib._parallel_backends.self._pool->MemmapingPool(n_jobs, **backend_args)
A:sklearn.externals.joblib._parallel_backends.already_forked->int(os.environ.get(self.JOBLIB_SPAWNED_PROCESS, 0))
A:sklearn.externals.joblib._parallel_backends.self.results->batch()
A:sklearn.externals.joblib._parallel_backends.(e_type, e_value, e_tb)->sys.exc_info()
A:sklearn.externals.joblib._parallel_backends.text->format_exc(e_type, e_value, e_tb, context=10, tb_offset=1)
sklearn.externals.joblib._parallel_backends.AutoBatchingMixin(object)
sklearn.externals.joblib._parallel_backends.AutoBatchingMixin.batch_completed(self,batch_size,duration)
sklearn.externals.joblib._parallel_backends.AutoBatchingMixin.compute_batch_size(self)
sklearn.externals.joblib._parallel_backends.FallbackToBackend(self,backend)
sklearn.externals.joblib._parallel_backends.FallbackToBackend.__init__(self,backend)
sklearn.externals.joblib._parallel_backends.ImmediateResult(self,batch)
sklearn.externals.joblib._parallel_backends.ImmediateResult.__init__(self,batch)
sklearn.externals.joblib._parallel_backends.ImmediateResult.get(self)
sklearn.externals.joblib._parallel_backends.MultiprocessingBackend(PoolManagerMixin,AutoBatchingMixin,ParallelBackendBase)
sklearn.externals.joblib._parallel_backends.MultiprocessingBackend.configure(self,n_jobs=1,parallel=None,**backend_args)
sklearn.externals.joblib._parallel_backends.MultiprocessingBackend.effective_n_jobs(self,n_jobs)
sklearn.externals.joblib._parallel_backends.MultiprocessingBackend.terminate(self)
sklearn.externals.joblib._parallel_backends.ParallelBackendBase(with_metaclass(ABCMeta))
sklearn.externals.joblib._parallel_backends.ParallelBackendBase.abort_everything(self,ensure_ready=True)
sklearn.externals.joblib._parallel_backends.ParallelBackendBase.apply_async(self,func,callback=None)
sklearn.externals.joblib._parallel_backends.ParallelBackendBase.batch_completed(self,batch_size,duration)
sklearn.externals.joblib._parallel_backends.ParallelBackendBase.compute_batch_size(self)
sklearn.externals.joblib._parallel_backends.ParallelBackendBase.configure(self,n_jobs=1,parallel=None,**backend_args)
sklearn.externals.joblib._parallel_backends.ParallelBackendBase.effective_n_jobs(self,n_jobs)
sklearn.externals.joblib._parallel_backends.ParallelBackendBase.get_exceptions(self)
sklearn.externals.joblib._parallel_backends.ParallelBackendBase.terminate(self)
sklearn.externals.joblib._parallel_backends.PoolManagerMixin(object)
sklearn.externals.joblib._parallel_backends.PoolManagerMixin.abort_everything(self,ensure_ready=True)
sklearn.externals.joblib._parallel_backends.PoolManagerMixin.apply_async(self,func,callback=None)
sklearn.externals.joblib._parallel_backends.PoolManagerMixin.effective_n_jobs(self,n_jobs)
sklearn.externals.joblib._parallel_backends.PoolManagerMixin.terminate(self)
sklearn.externals.joblib._parallel_backends.SafeFunction(self,func)
sklearn.externals.joblib._parallel_backends.SafeFunction.__init__(self,func)
sklearn.externals.joblib._parallel_backends.SequentialBackend(ParallelBackendBase)
sklearn.externals.joblib._parallel_backends.SequentialBackend.apply_async(self,func,callback=None)
sklearn.externals.joblib._parallel_backends.SequentialBackend.effective_n_jobs(self,n_jobs)
sklearn.externals.joblib._parallel_backends.ThreadingBackend(PoolManagerMixin,ParallelBackendBase)
sklearn.externals.joblib._parallel_backends.ThreadingBackend.configure(self,n_jobs=1,parallel=None,**backend_args)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/externals/joblib/_compat.py----------------------------------------
sklearn.externals.joblib._compat.with_metaclass(meta,*bases)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/externals/joblib/backports.py----------------------------------------
A:sklearn.externals.joblib.backports.mm->numpy.memmap(filename, dtype=dtype, mode=mode, offset=offset, shape=shape, order=order)
A:sklearn.externals.joblib.backports.src->unicode(src, sys.getfilesystemencoding())
A:sklearn.externals.joblib.backports.dst->unicode(dst, sys.getfilesystemencoding())
A:sklearn.externals.joblib.backports.return_value->ctypes.windll.kernel32.MoveFileExW(src, dst, movefile_replace_existing)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/externals/joblib/format_stack.py----------------------------------------
A:sklearn.externals.joblib.format_stack.name->getattr(value, '__name__', None)
A:sklearn.externals.joblib.format_stack.klass->getattr(value, '__class__', None)
A:sklearn.externals.joblib.format_stack.unique_set->set()
A:sklearn.externals.joblib.format_stack.better_fn->frame.f_globals.get('__file__', None)
A:sklearn.externals.joblib.format_stack.records->inspect.getouterframes(inspect.currentframe())
A:sklearn.externals.joblib.format_stack.aux->traceback.extract_tb(etb)
A:sklearn.externals.joblib.format_stack.start->max(maybe_start, 0)
A:sklearn.externals.joblib.format_stack.buf->list(records[i])
A:sklearn.externals.joblib.format_stack.records[i]->tuple(buf)
A:sklearn.externals.joblib.format_stack.(args, varargs, varkw, locals)->inspect.getargvalues(frame)
A:sklearn.externals.joblib.format_stack.line->getline(file, lnum[0])
A:sklearn.externals.joblib.format_stack.unique_names->uniq_stable(names)
A:sklearn.externals.joblib.format_stack.value->safe_repr(eval(name_full, locals))
A:sklearn.externals.joblib.format_stack.date->time.ctime(time.time())
A:sklearn.externals.joblib.format_stack.(etype_str, evalue_str)->map(str, (etype, evalue))
A:sklearn.externals.joblib.format_stack.frames->format_records(records)
A:sklearn.externals.joblib.format_stack.output->list()
sklearn.externals.joblib.format_stack._fixed_getframes(etb,context=1,tb_offset=0)
sklearn.externals.joblib.format_stack._format_traceback_lines(lnum,index,lines,lvals=None)
sklearn.externals.joblib.format_stack.eq_repr(value,repr=safe_repr)
sklearn.externals.joblib.format_stack.fix_frame_records_filenames(records)
sklearn.externals.joblib.format_stack.format_exc(etype,evalue,etb,context=5,tb_offset=0)
sklearn.externals.joblib.format_stack.format_outer_frames(context=5,stack_start=None,stack_end=None,ignore_ipython=True)
sklearn.externals.joblib.format_stack.format_records(records)
sklearn.externals.joblib.format_stack.safe_repr(value)
sklearn.externals.joblib.format_stack.uniq_stable(elems)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/externals/joblib/hashing.py----------------------------------------
A:sklearn.externals.joblib.hashing.self._sequence->sorted((hash(e) for e in set_sequence))
A:sklearn.externals.joblib.hashing.self.stream->io.BytesIO()
A:sklearn.externals.joblib.hashing.self._hash->hashlib.new(hash_name)
A:sklearn.externals.joblib.hashing.dumps->self.stream.getvalue()
A:sklearn.externals.joblib.hashing.obj->_MyHash(func_name, inst, cls)
A:sklearn.externals.joblib.hashing.kwargs->dict(name=name, pack=pack)
A:sklearn.externals.joblib.hashing.module->getattr(obj, '__module__', None)
A:sklearn.externals.joblib.hashing.dispatch->Pickler.dispatch.copy()
A:sklearn.externals.joblib.hashing.obj_c_contiguous->_MyHash(func_name, inst, cls).flatten()
A:sklearn.externals.joblib.hashing.hasher->Hasher(hash_name=hash_name)
sklearn.externals.joblib.hash(obj,hash_name='md5',coerce_mmap=False)
sklearn.externals.joblib.hashing.Hasher(self,hash_name='md5')
sklearn.externals.joblib.hashing.Hasher.__init__(self,hash_name='md5')
sklearn.externals.joblib.hashing.Hasher._batch_setitems(self,items)
sklearn.externals.joblib.hashing.Hasher.hash(self,obj,return_digest=True)
sklearn.externals.joblib.hashing.Hasher.memoize(self,obj)
sklearn.externals.joblib.hashing.Hasher.save(self,obj)
sklearn.externals.joblib.hashing.Hasher.save_global(self,obj,name=None,pack=struct.pack)
sklearn.externals.joblib.hashing.Hasher.save_set(self,set_items)
sklearn.externals.joblib.hashing.NumpyHasher(self,hash_name='md5',coerce_mmap=False)
sklearn.externals.joblib.hashing.NumpyHasher.__init__(self,hash_name='md5',coerce_mmap=False)
sklearn.externals.joblib.hashing.NumpyHasher.save(self,obj)
sklearn.externals.joblib.hashing._ConsistentSet(self,set_sequence)
sklearn.externals.joblib.hashing._ConsistentSet.__init__(self,set_sequence)
sklearn.externals.joblib.hashing._MyHash(self,*args)
sklearn.externals.joblib.hashing._MyHash.__init__(self,*args)
sklearn.externals.joblib.hashing.hash(obj,hash_name='md5',coerce_mmap=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/externals/joblib/disk.py----------------------------------------
A:sklearn.externals.joblib.disk.stat->os.stat(os.path.join(path, file))
A:sklearn.externals.joblib.disk.units->dict(K=kilo, M=kilo ** 2, G=kilo ** 3)
A:sklearn.externals.joblib.disk.size->int(units[text[-1]] * float(text[:-1]))
A:sklearn.externals.joblib.disk.names->os.listdir(path)
A:sklearn.externals.joblib.disk.fullname->os.path.join(path, name)
sklearn.externals.joblib.disk.disk_used(path)
sklearn.externals.joblib.disk.memstr_to_bytes(text)
sklearn.externals.joblib.disk.mkdirp(d)
sklearn.externals.joblib.disk.rm_subdirs(path,onerror=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/externals/joblib/pool.py----------------------------------------
A:sklearn.externals.joblib.pool.WindowsError->type(None)
A:sklearn.externals.joblib.pool.b->getattr(a, 'base', None)
A:sklearn.externals.joblib.pool.base->make_memmap(filename, dtype=dtype, shape=total_buffer_len, mode=mode, offset=offset, order=order)
A:sklearn.externals.joblib.pool.(a_start, a_end)->numpy.byte_bounds(a)
A:sklearn.externals.joblib.pool.m->_get_backing_memmap(a)
A:sklearn.externals.joblib.pool.self.verbose->int(verbose)
A:sklearn.externals.joblib.pool.filename->os.path.join(self._temp_folder, basename)
A:sklearn.externals.joblib.pool.self.dispatch->pickle.Pickler.dispatch.copy()
A:sklearn.externals.joblib.pool.self.dispatch_table->copyreg.dispatch_table.copy()
A:sklearn.externals.joblib.pool.reduced->reduce_func(obj)
A:sklearn.externals.joblib.pool.(self._reader, self._writer)->getattr(self, '_ctx', mp).Pipe(duplex=False)
A:sklearn.externals.joblib.pool.self._rlock->getattr(self, '_ctx', mp).Lock()
A:sklearn.externals.joblib.pool.self._wlock->getattr(self, '_ctx', mp).Lock()
A:sklearn.externals.joblib.pool.buffer->BytesIO()
A:sklearn.externals.joblib.pool.forward_reducers->dict()
A:sklearn.externals.joblib.pool.backward_reducers->dict()
A:sklearn.externals.joblib.pool.poolargs->dict(processes=processes, forward_reducers=forward_reducers, backward_reducers=backward_reducers)
A:sklearn.externals.joblib.pool.context->getattr(self, '_ctx', mp)
A:sklearn.externals.joblib.pool.self._inqueue->CustomizablePicklingQueue(context, self._forward_reducers)
A:sklearn.externals.joblib.pool.self._outqueue->CustomizablePicklingQueue(context, self._backward_reducers)
A:sklearn.externals.joblib.pool.temp_folder->os.path.abspath(os.path.expanduser(temp_folder))
A:sklearn.externals.joblib.pool.pool_folder->os.path.join(temp_folder, pool_folder_name)
A:sklearn.externals.joblib.pool.pool_module_name->whichmodule(delete_folder, 'delete_folder')
A:sklearn.externals.joblib.pool.forward_reduce_ndarray->ArrayMemmapReducer(max_nbytes, pool_folder, mmap_mode, verbose, prewarm=prewarm)
A:sklearn.externals.joblib.pool.backward_reduce_ndarray->ArrayMemmapReducer(None, pool_folder, mmap_mode, verbose)
sklearn.externals.joblib.pool.ArrayMemmapReducer(self,max_nbytes,temp_folder,mmap_mode,verbose=0,context_id=None,prewarm=True)
sklearn.externals.joblib.pool.ArrayMemmapReducer.__init__(self,max_nbytes,temp_folder,mmap_mode,verbose=0,context_id=None,prewarm=True)
sklearn.externals.joblib.pool.CustomizablePickler(self,writer,reducers=None,protocol=HIGHEST_PROTOCOL)
sklearn.externals.joblib.pool.CustomizablePickler.__init__(self,writer,reducers=None,protocol=HIGHEST_PROTOCOL)
sklearn.externals.joblib.pool.CustomizablePickler.register(self,type,reduce_func)
sklearn.externals.joblib.pool.CustomizablePicklingQueue(self,context,reducers=None)
sklearn.externals.joblib.pool.CustomizablePicklingQueue.__getstate__(self)
sklearn.externals.joblib.pool.CustomizablePicklingQueue.__init__(self,context,reducers=None)
sklearn.externals.joblib.pool.CustomizablePicklingQueue.__setstate__(self,state)
sklearn.externals.joblib.pool.CustomizablePicklingQueue._make_methods(self)
sklearn.externals.joblib.pool.CustomizablePicklingQueue.empty(self)
sklearn.externals.joblib.pool.MemmapingPool(self,processes=None,temp_folder=None,max_nbytes=1000000.0,mmap_mode='r',forward_reducers=None,backward_reducers=None,verbose=0,context_id=None,prewarm=False,**kwargs)
sklearn.externals.joblib.pool.MemmapingPool.__init__(self,processes=None,temp_folder=None,max_nbytes=1000000.0,mmap_mode='r',forward_reducers=None,backward_reducers=None,verbose=0,context_id=None,prewarm=False,**kwargs)
sklearn.externals.joblib.pool.MemmapingPool.terminate(self)
sklearn.externals.joblib.pool.PicklingPool(self,processes=None,forward_reducers=None,backward_reducers=None,**kwargs)
sklearn.externals.joblib.pool.PicklingPool.__init__(self,processes=None,forward_reducers=None,backward_reducers=None,**kwargs)
sklearn.externals.joblib.pool.PicklingPool._setup_queues(self)
sklearn.externals.joblib.pool._get_backing_memmap(a)
sklearn.externals.joblib.pool._reduce_memmap_backed(a,m)
sklearn.externals.joblib.pool._strided_from_memmap(filename,dtype,mode,offset,order,shape,strides,total_buffer_len)
sklearn.externals.joblib.pool.delete_folder(folder_path)
sklearn.externals.joblib.pool.has_shareable_memory(a)
sklearn.externals.joblib.pool.reduce_memmap(a)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/externals/joblib/memory.py----------------------------------------
A:sklearn.externals.joblib.memory.CacheItemInfo->collections.namedtuple('CacheItemInfo', 'path size last_access')
A:sklearn.externals.joblib.memory.func_code->'\n'.join(func_code[1:])
A:sklearn.externals.joblib.memory.first_line->int(func_code[0][len(FIRST_LINE_TEXT):])
A:sklearn.externals.joblib.memory.(modules, funcname)->get_func_name(func)
A:sklearn.externals.joblib.memory.args->', '.join(['%s=%s' % (name, value) for (name, value) in metadata['input_args'].items()])
A:sklearn.externals.joblib.memory.signature->os.path.basename(func_name)
A:sklearn.externals.joblib.memory.filename->os.path.join(output_dir, 'metadata.json')
A:sklearn.externals.joblib.memory.result->numpy_pickle.load(filename, mmap_mode=mmap_mode)
A:sklearn.externals.joblib.memory.is_cache_hash_dir->re.match('[a-f0-9]{32}', os.path.basename(dirpath))
A:sklearn.externals.joblib.memory.output_filename->os.path.join(dirpath, 'output.pkl')
A:sklearn.externals.joblib.memory.last_access->datetime.datetime.fromtimestamp(last_access)
A:sklearn.externals.joblib.memory.dirsize->sum((os.path.getsize(fn) for fn in full_filenames))
A:sklearn.externals.joblib.memory.bytes_limit->memstr_to_bytes(bytes_limit)
A:sklearn.externals.joblib.memory.cache_items->_get_cache_items(root_path)
A:sklearn.externals.joblib.memory.cache_size->sum((item.size for item in cache_items))
A:sklearn.externals.joblib.memory.thread_id->id(threading.current_thread())
A:sklearn.externals.joblib.memory.temporary_filename->'{}.thread-{}-pid-{}'.format(filename, thread_id, os.getpid())
A:sklearn.externals.joblib.memory._FUNCTION_HASHES->weakref.WeakKeyDictionary()
A:sklearn.externals.joblib.memory.self.func->_get_func_fullname(func)
A:sklearn.externals.joblib.memory.self._output_dir->_cache_key_to_dir(cachedir, self.func, argument_hash)
A:sklearn.externals.joblib.memory.self.metadata->json.load(f)
A:sklearn.externals.joblib.memory.self.duration->self.metadata.get('duration', None)
A:sklearn.externals.joblib.memory.timestamp->time.time()
A:sklearn.externals.joblib.memory.doc->re.sub('\x08.', '', doc)
A:sklearn.externals.joblib.memory.(output_dir, argument_hash)->self._get_output_dir(*args, **kwargs)
A:sklearn.externals.joblib.memory.output_pickle_path->os.path.join(output_dir, 'output.pkl')
A:sklearn.externals.joblib.memory.(_, name)->get_func_name(self.func)
A:sklearn.externals.joblib.memory.(out, metadata)->self.call(*args, **kwargs)
A:sklearn.externals.joblib.memory.out->_load_output(output_dir, _get_func_fullname(self.func), timestamp=self.timestamp, metadata=metadata, mmap_mode=self.mmap_mode, verbose=self._verbose)
A:sklearn.externals.joblib.memory.t0->time.time()
A:sklearn.externals.joblib.memory.(_, signature)->format_signature(self.func, *args, **kwargs)
A:sklearn.externals.joblib.memory.(_, argument_hash, metadata)->self._cached_call(args, kwargs)
A:sklearn.externals.joblib.memory.argument_hash->self._get_argument_hash(*args, **kwargs)
A:sklearn.externals.joblib.memory.output_dir->os.path.join(self._get_func_dir(self.func), argument_hash)
A:sklearn.externals.joblib.memory.func_dir->self._get_func_dir(mkdir=False)
A:sklearn.externals.joblib.memory.func_code_h->hash(getattr(self.func, '__code__', None))
A:sklearn.externals.joblib.memory.func_hash->self._hash_func()
A:sklearn.externals.joblib.memory.(func_code, source_file, first_line)->get_func_code(self.func)
A:sklearn.externals.joblib.memory.func_code_file->os.path.join(func_dir, 'func_code.py')
A:sklearn.externals.joblib.memory.(old_func_code, old_first_line)->extract_first_line(infile.read())
A:sklearn.externals.joblib.memory.(_, func_name)->get_func_name(self.func, resolv_alias=False)
A:sklearn.externals.joblib.memory.num_lines->len(func_code.split('\n'))
A:sklearn.externals.joblib.memory.on_disk_func_code->''.join(on_disk_func_code)
A:sklearn.externals.joblib.memory.possible_collision->source_file.startswith('<doctest ')
A:sklearn.externals.joblib.memory.(func_code, _, first_line)->get_func_code(self.func)
A:sklearn.externals.joblib.memory.start_time->time.time()
A:sklearn.externals.joblib.memory.(output_dir, _)->self._get_output_dir(*args, **kwargs)
A:sklearn.externals.joblib.memory.output->self.func(*args, **kwargs)
A:sklearn.externals.joblib.memory.metadata->self._persist_input(output_dir, duration, args, kwargs)
A:sklearn.externals.joblib.memory.write_func->functools.partial(numpy_pickle.dump, compress=self.compress)
A:sklearn.externals.joblib.memory.argument_dict->filter_args(self.func, self.ignore, args, kwargs)
A:sklearn.externals.joblib.memory.input_repr->dict(((k, repr(v)) for (k, v) in argument_dict.items()))
A:sklearn.externals.joblib.memory.self.timestamp->time.time()
A:sklearn.externals.joblib.memory.self.cachedir->os.path.join(cachedir, 'joblib')
A:sklearn.externals.joblib.memory.cache_items_to_delete->_get_cache_items_to_delete(self.cachedir, self.bytes_limit)
sklearn.externals.joblib.MemorizedResult(self,cachedir,func,argument_hash,mmap_mode=None,verbose=0,timestamp=None,metadata=None)
sklearn.externals.joblib.MemorizedResult.__reduce__(self)
sklearn.externals.joblib.MemorizedResult.__repr__(self)
sklearn.externals.joblib.MemorizedResult.clear(self)
sklearn.externals.joblib.MemorizedResult.get(self)
sklearn.externals.joblib.Memory(self,cachedir,mmap_mode=None,compress=False,verbose=1,bytes_limit=None)
sklearn.externals.joblib.Memory.__reduce__(self)
sklearn.externals.joblib.Memory.__repr__(self)
sklearn.externals.joblib.Memory.cache(self,func=None,ignore=None,verbose=None,mmap_mode=False)
sklearn.externals.joblib.Memory.clear(self,warn=True)
sklearn.externals.joblib.Memory.eval(self,func,*args,**kwargs)
sklearn.externals.joblib.Memory.reduce_size(self)
sklearn.externals.joblib.memory.JobLibCollisionWarning(UserWarning)
sklearn.externals.joblib.memory.MemorizedFunc(self,func,cachedir,ignore=None,mmap_mode=None,compress=False,verbose=1,timestamp=None)
sklearn.externals.joblib.memory.MemorizedFunc.__init__(self,func,cachedir,ignore=None,mmap_mode=None,compress=False,verbose=1,timestamp=None)
sklearn.externals.joblib.memory.MemorizedFunc.__reduce__(self)
sklearn.externals.joblib.memory.MemorizedFunc.__repr__(self)
sklearn.externals.joblib.memory.MemorizedFunc._cached_call(self,args,kwargs)
sklearn.externals.joblib.memory.MemorizedFunc._check_previous_func_code(self,stacklevel=2)
sklearn.externals.joblib.memory.MemorizedFunc._get_argument_hash(self,*args,**kwargs)
sklearn.externals.joblib.memory.MemorizedFunc._get_func_dir(self,mkdir=True)
sklearn.externals.joblib.memory.MemorizedFunc._get_output_dir(self,*args,**kwargs)
sklearn.externals.joblib.memory.MemorizedFunc._hash_func(self)
sklearn.externals.joblib.memory.MemorizedFunc._persist_input(self,output_dir,duration,args,kwargs,this_duration_limit=0.5)
sklearn.externals.joblib.memory.MemorizedFunc._persist_output(self,output,dir)
sklearn.externals.joblib.memory.MemorizedFunc._write_func_code(self,filename,func_code,first_line)
sklearn.externals.joblib.memory.MemorizedFunc.call(self,*args,**kwargs)
sklearn.externals.joblib.memory.MemorizedFunc.call_and_shelve(self,*args,**kwargs)
sklearn.externals.joblib.memory.MemorizedFunc.clear(self,warn=True)
sklearn.externals.joblib.memory.MemorizedResult(self,cachedir,func,argument_hash,mmap_mode=None,verbose=0,timestamp=None,metadata=None)
sklearn.externals.joblib.memory.MemorizedResult.__init__(self,cachedir,func,argument_hash,mmap_mode=None,verbose=0,timestamp=None,metadata=None)
sklearn.externals.joblib.memory.MemorizedResult.__reduce__(self)
sklearn.externals.joblib.memory.MemorizedResult.__repr__(self)
sklearn.externals.joblib.memory.MemorizedResult.clear(self)
sklearn.externals.joblib.memory.MemorizedResult.get(self)
sklearn.externals.joblib.memory.Memory(self,cachedir,mmap_mode=None,compress=False,verbose=1,bytes_limit=None)
sklearn.externals.joblib.memory.Memory.__init__(self,cachedir,mmap_mode=None,compress=False,verbose=1,bytes_limit=None)
sklearn.externals.joblib.memory.Memory.__reduce__(self)
sklearn.externals.joblib.memory.Memory.__repr__(self)
sklearn.externals.joblib.memory.Memory.cache(self,func=None,ignore=None,verbose=None,mmap_mode=False)
sklearn.externals.joblib.memory.Memory.clear(self,warn=True)
sklearn.externals.joblib.memory.Memory.eval(self,func,*args,**kwargs)
sklearn.externals.joblib.memory.Memory.reduce_size(self)
sklearn.externals.joblib.memory.NotMemorizedFunc(self,func)
sklearn.externals.joblib.memory.NotMemorizedFunc.__init__(self,func)
sklearn.externals.joblib.memory.NotMemorizedFunc.__reduce__(self)
sklearn.externals.joblib.memory.NotMemorizedFunc.__repr__(self)
sklearn.externals.joblib.memory.NotMemorizedFunc.call_and_shelve(self,*args,**kwargs)
sklearn.externals.joblib.memory.NotMemorizedFunc.clear(self,warn=True)
sklearn.externals.joblib.memory.NotMemorizedResult(self,value)
sklearn.externals.joblib.memory.NotMemorizedResult.__getstate__(self)
sklearn.externals.joblib.memory.NotMemorizedResult.__init__(self,value)
sklearn.externals.joblib.memory.NotMemorizedResult.__repr__(self)
sklearn.externals.joblib.memory.NotMemorizedResult.__setstate__(self,state)
sklearn.externals.joblib.memory.NotMemorizedResult.clear(self)
sklearn.externals.joblib.memory.NotMemorizedResult.get(self)
sklearn.externals.joblib.memory._cache_key_to_dir(cachedir,func,argument_hash)
sklearn.externals.joblib.memory._get_cache_items(root_path)
sklearn.externals.joblib.memory._get_cache_items_to_delete(root_path,bytes_limit)
sklearn.externals.joblib.memory._get_func_fullname(func)
sklearn.externals.joblib.memory._load_output(output_dir,func_name,timestamp=None,metadata=None,mmap_mode=None,verbose=0)
sklearn.externals.joblib.memory.concurrency_safe_write(to_write,filename,write_func)
sklearn.externals.joblib.memory.extract_first_line(func_code)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py----------------------------------------
A:sklearn.externals.joblib.parallel._backend->threading.local()
A:sklearn.externals.joblib.parallel.active_backend_and_jobs->getattr(_backend, 'backend_and_jobs', None)
A:sklearn.externals.joblib.parallel.active_backend->BACKENDS[DEFAULT_BACKEND]()
A:sklearn.externals.joblib.parallel.backend->backend_factory()
A:sklearn.externals.joblib.parallel.old_backend_and_jobs->getattr(_backend, 'backend_and_jobs', None)
A:sklearn.externals.joblib.parallel.DEFAULT_MP_CONTEXT->_multiprocessing_helpers.mp.get_context(method=method)
A:sklearn.externals.joblib.parallel.self.items->list(iterator_slice)
A:sklearn.externals.joblib.parallel.self._size->len(self.items)
A:sklearn.externals.joblib.parallel.scale->sqrt(index / verbose)
A:sklearn.externals.joblib.parallel.next_scale->sqrt((index + 1) / verbose)
A:sklearn.externals.joblib.parallel.delayed_function->functools.wraps(function)(delayed_function)
A:sklearn.externals.joblib.parallel.(backend, _)->get_active_backend()
A:sklearn.externals.joblib.parallel.(active_backend, default_n_jobs)->get_active_backend()
A:sklearn.externals.joblib.parallel.max_nbytes->memstr_to_bytes(max_nbytes)
A:sklearn.externals.joblib.parallel.self._backend_args->dict(max_nbytes=max_nbytes, mmap_mode=mmap_mode, temp_folder=temp_folder, verbose=max(0, self.verbose - 50))
A:sklearn.externals.joblib.parallel.self._jobs->list()
A:sklearn.externals.joblib.parallel.self._lock->threading.Lock()
A:sklearn.externals.joblib.parallel.n_jobs->self._effective_n_jobs()
A:sklearn.externals.joblib.parallel.dispatch_timestamp->time.time()
A:sklearn.externals.joblib.parallel.cb->BatchCompletionCallBack(dispatch_timestamp, len(batch), self)
A:sklearn.externals.joblib.parallel.job->self._jobs.pop(0)
A:sklearn.externals.joblib.parallel.batch_size->self._backend.compute_batch_size()
A:sklearn.externals.joblib.parallel.tasks->BatchedCalls(itertools.islice(iterator, batch_size))
A:sklearn.externals.joblib.parallel.self._output->list()
A:sklearn.externals.joblib.parallel.this_report->format_outer_frames(context=10, stack_start=1)
A:sklearn.externals.joblib.parallel.exception->exception_type(report)
A:sklearn.externals.joblib.parallel.iterator->itertools.islice(iterator, pre_dispatch)
A:sklearn.externals.joblib.parallel.pre_dispatch->eval(pre_dispatch)
A:sklearn.externals.joblib.parallel.self._pre_dispatch_amountpre_dispatch->int(pre_dispatch)
A:sklearn.externals.joblib.parallel.self._start_time->time.time()
sklearn.externals.joblib.Parallel(self,n_jobs=1,backend=None,verbose=0,timeout=None,pre_dispatch='2*n_jobs',batch_size='auto',temp_folder=None,max_nbytes='1M',mmap_mode='r')
sklearn.externals.joblib.Parallel.__enter__(self)
sklearn.externals.joblib.Parallel.__exit__(self,exc_type,exc_value,traceback)
sklearn.externals.joblib.Parallel.__repr__(self)
sklearn.externals.joblib.Parallel._dispatch(self,batch)
sklearn.externals.joblib.Parallel._effective_n_jobs(self)
sklearn.externals.joblib.Parallel._initialize_backend(self)
sklearn.externals.joblib.Parallel._print(self,msg,msg_args)
sklearn.externals.joblib.Parallel._terminate_backend(self)
sklearn.externals.joblib.Parallel.dispatch_next(self)
sklearn.externals.joblib.Parallel.dispatch_one_batch(self,iterator)
sklearn.externals.joblib.Parallel.print_progress(self)
sklearn.externals.joblib.Parallel.retrieve(self)
sklearn.externals.joblib.cpu_count()
sklearn.externals.joblib.delayed(function,check_pickle=True)
sklearn.externals.joblib.effective_n_jobs(n_jobs=-1)
sklearn.externals.joblib.parallel.BatchCompletionCallBack(self,dispatch_timestamp,batch_size,parallel)
sklearn.externals.joblib.parallel.BatchCompletionCallBack.__init__(self,dispatch_timestamp,batch_size,parallel)
sklearn.externals.joblib.parallel.BatchedCalls(self,iterator_slice)
sklearn.externals.joblib.parallel.BatchedCalls.__init__(self,iterator_slice)
sklearn.externals.joblib.parallel.BatchedCalls.__len__(self)
sklearn.externals.joblib.parallel.Parallel(self,n_jobs=1,backend=None,verbose=0,timeout=None,pre_dispatch='2*n_jobs',batch_size='auto',temp_folder=None,max_nbytes='1M',mmap_mode='r')
sklearn.externals.joblib.parallel.Parallel.__enter__(self)
sklearn.externals.joblib.parallel.Parallel.__exit__(self,exc_type,exc_value,traceback)
sklearn.externals.joblib.parallel.Parallel.__init__(self,n_jobs=1,backend=None,verbose=0,timeout=None,pre_dispatch='2*n_jobs',batch_size='auto',temp_folder=None,max_nbytes='1M',mmap_mode='r')
sklearn.externals.joblib.parallel.Parallel.__repr__(self)
sklearn.externals.joblib.parallel.Parallel._dispatch(self,batch)
sklearn.externals.joblib.parallel.Parallel._effective_n_jobs(self)
sklearn.externals.joblib.parallel.Parallel._initialize_backend(self)
sklearn.externals.joblib.parallel.Parallel._print(self,msg,msg_args)
sklearn.externals.joblib.parallel.Parallel._terminate_backend(self)
sklearn.externals.joblib.parallel.Parallel.dispatch_next(self)
sklearn.externals.joblib.parallel.Parallel.dispatch_one_batch(self,iterator)
sklearn.externals.joblib.parallel.Parallel.print_progress(self)
sklearn.externals.joblib.parallel.Parallel.retrieve(self)
sklearn.externals.joblib.parallel._verbosity_filter(index,verbose)
sklearn.externals.joblib.parallel.cpu_count()
sklearn.externals.joblib.parallel.delayed(function,check_pickle=True)
sklearn.externals.joblib.parallel.effective_n_jobs(n_jobs=-1)
sklearn.externals.joblib.parallel.get_active_backend()
sklearn.externals.joblib.parallel.parallel_backend(backend,n_jobs=-1,**backend_params)
sklearn.externals.joblib.parallel.register_parallel_backend(name,factory,make_default=False)
sklearn.externals.joblib.parallel_backend(backend,n_jobs=-1,**backend_params)
sklearn.externals.joblib.register_parallel_backend(name,factory,make_default=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/externals/joblib/func_inspect.py----------------------------------------
A:sklearn.externals.joblib.func_inspect.source_code->''.join(inspect.getsourcelines(func)[0])
A:sklearn.externals.joblib.func_inspect.(source_file, line_no)->re.match('\\<doctest (.*\\.rst)\\[(.*)\\]\\>', source_file).groups()
A:sklearn.externals.joblib.func_inspect.line_no->int(line_no)
A:sklearn.externals.joblib.func_inspect.source_lines->list(islice(source_file_obj, first_line - 1, None))
A:sklearn.externals.joblib.func_inspect.string->string.replace(char, quote(char)).replace(char, quote(char))
A:sklearn.externals.joblib.func_inspect.module->module.split('.').split('.')
A:sklearn.externals.joblib.func_inspect.filename->'-'.join(parts)
A:sklearn.externals.joblib.func_inspect.parts->'-'.join(parts).split(os.sep)
A:sklearn.externals.joblib.func_inspect.name->_clean_win_chars(name)
A:sklearn.externals.joblib.func_inspect.arg_spec->getfullargspec(func)
A:sklearn.externals.joblib.func_inspect.tuple_type->collections.namedtuple('FullArgSpec', tuple_fields)
A:sklearn.externals.joblib.func_inspect.arg_spec_str->inspect.formatargspec(*arg_spec_for_format)
A:sklearn.externals.joblib.func_inspect.kwargs_str->', '.join(('%s=%s' % (k, v) for (k, v) in kwargs.items()))
A:sklearn.externals.joblib.func_inspect.args->list(args)
A:sklearn.externals.joblib.func_inspect.(_, name)->get_func_name(func, resolv_alias=False)
A:sklearn.externals.joblib.func_inspect.arg_dict->dict()
A:sklearn.externals.joblib.func_inspect.arg_dict[arg_name]->kwargs.pop(arg_name)
A:sklearn.externals.joblib.func_inspect.varkwargs->dict()
A:sklearn.externals.joblib.func_inspect.formatted_arg->_format_arg(arg)
A:sklearn.externals.joblib.func_inspect.(module, name)->get_func_name(func)
A:sklearn.externals.joblib.func_inspect.module_path->'.'.join(module)
A:sklearn.externals.joblib.func_inspect.arg_str->', '.join(arg_str)
A:sklearn.externals.joblib.func_inspect.previous_length->len(formatted_arg)
A:sklearn.externals.joblib.func_inspect.(path, signature)->format_signature(func, *args, **kwargs)
sklearn.externals.joblib.func_inspect._clean_win_chars(string)
sklearn.externals.joblib.func_inspect._format_arg(arg)
sklearn.externals.joblib.func_inspect._function_called_str(function_name,args,kwargs)
sklearn.externals.joblib.func_inspect._signature_str(function_name,arg_spec)
sklearn.externals.joblib.func_inspect.filter_args(func,ignore_lst,args=(),kwargs=dict())
sklearn.externals.joblib.func_inspect.format_call(func,args,kwargs,object_name='Memory')
sklearn.externals.joblib.func_inspect.format_signature(func,*args,**kwargs)
sklearn.externals.joblib.func_inspect.get_func_code(func)
sklearn.externals.joblib.func_inspect.get_func_name(func,resolv_alias=True,win_characters=True)
sklearn.externals.joblib.func_inspect.getfullargspec(func)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/externals/joblib/_memory_helpers.py----------------------------------------
A:sklearn.externals.joblib._memory_helpers.cookie_re->re.compile('coding[:=]\\s*([-\\w.]+)')
A:sklearn.externals.joblib._memory_helpers.enc->orig_enc[:12].lower().replace('_', '-')
A:sklearn.externals.joblib._memory_helpers.line_string->line.decode('ascii')
A:sklearn.externals.joblib._memory_helpers.matches->re.compile('coding[:=]\\s*([-\\w.]+)').findall(line_string)
A:sklearn.externals.joblib._memory_helpers.encoding->find_cookie(second)
A:sklearn.externals.joblib._memory_helpers.codec->lookup(encoding)
A:sklearn.externals.joblib._memory_helpers.first->read_or_stop()
A:sklearn.externals.joblib._memory_helpers.second->read_or_stop()
A:sklearn.externals.joblib._memory_helpers.buffer->open(filename, 'rb')
A:sklearn.externals.joblib._memory_helpers.(encoding, lines)->_detect_encoding(buffer.readline)
A:sklearn.externals.joblib._memory_helpers.text->TextIOWrapper(buffer, encoding, line_buffering=True)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/externals/joblib/my_exceptions.py----------------------------------------
A:sklearn.externals.joblib.my_exceptions._exception_mapping->dict()
A:sklearn.externals.joblib.my_exceptions.this_exception->type(this_name, (JoblibException, exception), {})
A:sklearn.externals.joblib.my_exceptions.namespace->dict()
A:sklearn.externals.joblib.my_exceptions.common_exceptions->dir(_builtin_exceptions)
A:sklearn.externals.joblib.my_exceptions.obj->getattr(_builtin_exceptions, name)
A:sklearn.externals.joblib.my_exceptions.(this_obj, this_name)->_mk_exception(obj, name=name)
sklearn.externals.joblib.my_exceptions.JoblibException(self,*args)
sklearn.externals.joblib.my_exceptions.JoblibException.__init__(self,*args)
sklearn.externals.joblib.my_exceptions.JoblibException.__repr__(self)
sklearn.externals.joblib.my_exceptions.TransportableException(self,message,etype)
sklearn.externals.joblib.my_exceptions.TransportableException.__init__(self,message,etype)
sklearn.externals.joblib.my_exceptions.WorkerInterrupt(Exception)
sklearn.externals.joblib.my_exceptions._mk_common_exceptions()
sklearn.externals.joblib.my_exceptions._mk_exception(exception,name=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/externals/joblib/numpy_pickle_compat.py----------------------------------------
A:sklearn.externals.joblib.numpy_pickle_compat._MAX_LEN->len(hex_str(2 ** 64))
A:sklearn.externals.joblib.numpy_pickle_compat.length->hex_str(len(data))
A:sklearn.externals.joblib.numpy_pickle_compat.next_byte->file_handle.read(1)
A:sklearn.externals.joblib.numpy_pickle_compat.data->read_zfile(f)
A:sklearn.externals.joblib.numpy_pickle_compat.filename->os.path.join(unpickler._dirname, self.filename)
A:sklearn.externals.joblib.numpy_pickle_compat.allow_mmap->getattr(self, 'allow_mmap', True)
A:sklearn.externals.joblib.numpy_pickle_compat.array->self.stack.pop().read(self)
A:sklearn.externals.joblib.numpy_pickle_compat.new_array->ZipNumpyUnpickler(filename, file_handle=file_handle).np.core.multiarray._reconstruct(self.subclass, (0,), 'b')
A:sklearn.externals.joblib.numpy_pickle_compat.dispatch->numpy_pickle_utils.Unpickler.dispatch.copy()
A:sklearn.externals.joblib.numpy_pickle_compat.self._filename->os.path.basename(filename)
A:sklearn.externals.joblib.numpy_pickle_compat.self._dirname->os.path.dirname(filename)
A:sklearn.externals.joblib.numpy_pickle_compat.self.file_handle->self._open_pickle(file_handle)
A:sklearn.externals.joblib.numpy_pickle_compat.nd_array_wrapper->self.stack.pop()
A:sklearn.externals.joblib.numpy_pickle_compat.unpickler->ZipNumpyUnpickler(filename, file_handle=file_handle)
A:sklearn.externals.joblib.numpy_pickle_compat.obj->ZipNumpyUnpickler(filename, file_handle=file_handle).load()
A:sklearn.externals.joblib.numpy_pickle_compat.new_exc->ValueError('You may be trying to read with python 3 a joblib pickle generated with python 2. This feature is not supported by joblib.')
sklearn.externals.joblib.numpy_pickle_compat.NDArrayWrapper(self,filename,subclass,allow_mmap=True)
sklearn.externals.joblib.numpy_pickle_compat.NDArrayWrapper.__init__(self,filename,subclass,allow_mmap=True)
sklearn.externals.joblib.numpy_pickle_compat.NDArrayWrapper.read(self,unpickler)
sklearn.externals.joblib.numpy_pickle_compat.ZNDArrayWrapper(self,filename,init_args,state)
sklearn.externals.joblib.numpy_pickle_compat.ZNDArrayWrapper.__init__(self,filename,init_args,state)
sklearn.externals.joblib.numpy_pickle_compat.ZNDArrayWrapper.read(self,unpickler)
sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler(self,filename,file_handle,mmap_mode=None)
sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler.__init__(self,filename,file_handle,mmap_mode=None)
sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler._open_pickle(self,file_handle)
sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler.load_build(self)
sklearn.externals.joblib.numpy_pickle_compat.hex_str(an_int)
sklearn.externals.joblib.numpy_pickle_compat.load_compatibility(filename)
sklearn.externals.joblib.numpy_pickle_compat.read_zfile(file_handle)
sklearn.externals.joblib.numpy_pickle_compat.write_zfile(file_handle,data,compress=1)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/externals/joblib/numpy_pickle_utils.py----------------------------------------
A:sklearn.externals.joblib.numpy_pickle_utils._MAX_PREFIX_LEN->max((len(prefix) for prefix in (_ZFILE_PREFIX, _GZIP_PREFIX, _BZ2_PREFIX, _XZ_PREFIX, _LZMA_PREFIX)))
A:sklearn.externals.joblib.numpy_pickle_utils.fileobj->_buffered_read_file(lzma.LZMAFile(fileobj, 'rb'))
A:sklearn.externals.joblib.numpy_pickle_utils.first_bytes->_buffered_read_file(lzma.LZMAFile(fileobj, 'rb')).read(_MAX_PREFIX_LEN)
A:sklearn.externals.joblib.numpy_pickle_utils.compressor->_detect_compressor(fileobj)
A:sklearn.externals.joblib.numpy_pickle_utils.self._lock->RLock()
A:sklearn.externals.joblib.numpy_pickle_utils.self._decompressor->zlib.decompressobj(self.wbits)
A:sklearn.externals.joblib.numpy_pickle_utils.self._compressor->zlib.compressobj(compresslevel, zlib.DEFLATED, self.wbits, zlib.DEF_MEM_LEVEL, 0)
A:sklearn.externals.joblib.numpy_pickle_utils.self._fp->io.open(filename, mode)
A:sklearn.externals.joblib.numpy_pickle_utils.fname->getattr(self._fp, 'name', None)
A:sklearn.externals.joblib.numpy_pickle_utils.self._buffer->self._decompressor.decompress(rawblock)
A:sklearn.externals.joblib.numpy_pickle_utils.data->bytes()
A:sklearn.externals.joblib.numpy_pickle_utils.compressed->self._compressor.compress(data)
A:sklearn.externals.joblib.numpy_pickle_utils.r->fp.read(size - len(data))
sklearn.externals.joblib.numpy_pickle_utils.BinaryGzipFile(BinaryZlibFile)
sklearn.externals.joblib.numpy_pickle_utils.BinaryZlibFile(self,filename,mode='rb',compresslevel=9)
sklearn.externals.joblib.numpy_pickle_utils.BinaryZlibFile.__init__(self,filename,mode='rb',compresslevel=9)
sklearn.externals.joblib.numpy_pickle_utils.BinaryZlibFile._check_can_read(self)
sklearn.externals.joblib.numpy_pickle_utils.BinaryZlibFile._check_can_seek(self)
sklearn.externals.joblib.numpy_pickle_utils.BinaryZlibFile._check_can_write(self)
sklearn.externals.joblib.numpy_pickle_utils.BinaryZlibFile._check_not_closed(self)
sklearn.externals.joblib.numpy_pickle_utils.BinaryZlibFile._fill_buffer(self)
sklearn.externals.joblib.numpy_pickle_utils.BinaryZlibFile._read_all(self,return_data=True)
sklearn.externals.joblib.numpy_pickle_utils.BinaryZlibFile._read_block(self,n_bytes,return_data=True)
sklearn.externals.joblib.numpy_pickle_utils.BinaryZlibFile._rewind(self)
sklearn.externals.joblib.numpy_pickle_utils.BinaryZlibFile.close(self)
sklearn.externals.joblib.numpy_pickle_utils.BinaryZlibFile.closed(self)
sklearn.externals.joblib.numpy_pickle_utils.BinaryZlibFile.fileno(self)
sklearn.externals.joblib.numpy_pickle_utils.BinaryZlibFile.read(self,size=-1)
sklearn.externals.joblib.numpy_pickle_utils.BinaryZlibFile.readable(self)
sklearn.externals.joblib.numpy_pickle_utils.BinaryZlibFile.readinto(self,b)
sklearn.externals.joblib.numpy_pickle_utils.BinaryZlibFile.seek(self,offset,whence=0)
sklearn.externals.joblib.numpy_pickle_utils.BinaryZlibFile.seekable(self)
sklearn.externals.joblib.numpy_pickle_utils.BinaryZlibFile.tell(self)
sklearn.externals.joblib.numpy_pickle_utils.BinaryZlibFile.writable(self)
sklearn.externals.joblib.numpy_pickle_utils.BinaryZlibFile.write(self,data)
sklearn.externals.joblib.numpy_pickle_utils._buffered_read_file(fobj)
sklearn.externals.joblib.numpy_pickle_utils._buffered_write_file(fobj)
sklearn.externals.joblib.numpy_pickle_utils._detect_compressor(fileobj)
sklearn.externals.joblib.numpy_pickle_utils._is_raw_file(fileobj)
sklearn.externals.joblib.numpy_pickle_utils._read_bytes(fp,size,error_template='ranoutofdata')
sklearn.externals.joblib.numpy_pickle_utils._read_fileobject(fileobj,filename,mmap_mode=None)
sklearn.externals.joblib.numpy_pickle_utils._write_fileobject(filename,compress=('zlib',3))


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/datasets/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/datasets/species_distributions.py----------------------------------------
A:sklearn.datasets.species_distributions.SAMPLES->RemoteFileMetadata(filename='samples.zip', url='https://ndownloader.figshare.com/files/5976075', checksum='abb07ad284ac50d9e6d20f1c4211e0fd3c098f7f85955e89d321ee8efe37ac28')
A:sklearn.datasets.species_distributions.COVERAGES->RemoteFileMetadata(filename='coverages.zip', url='https://ndownloader.figshare.com/files/5976078', checksum='4d862674d72e79d6cee77e63b98651ec7926043ba7d39dcb31329cf3f6073807')
A:sklearn.datasets.species_distributions.logger->logging.getLogger(__name__)
A:sklearn.datasets.species_distributions.header->dict([make_tuple(line) for line in header])
A:sklearn.datasets.species_distributions.M->numpy.loadtxt(F, dtype=dtype)
A:sklearn.datasets.species_distributions.nodata->int(header[b'NODATA_value'])
A:sklearn.datasets.species_distributions.names->F.readline().strip().split(',')
A:sklearn.datasets.species_distributions.rec->numpy.loadtxt(F, skiprows=0, delimiter=',', dtype='a22,f4,f4')
A:sklearn.datasets.species_distributions.xgrid->numpy.arange(xmin, xmax, batch.grid_size)
A:sklearn.datasets.species_distributions.ygrid->numpy.arange(ymin, ymax, batch.grid_size)
A:sklearn.datasets.species_distributions.data_home->get_data_home(data_home)
A:sklearn.datasets.species_distributions.extra_params->dict(x_left_lower_corner=-94.8, Nx=1212, y_left_lower_corner=-56.05, Ny=1592, grid_size=0.05)
A:sklearn.datasets.species_distributions.archive_path->_pkl_filepath(data_home, DATA_ARCHIVE_NAME)
A:sklearn.datasets.species_distributions.samples_path->_fetch_remote(SAMPLES, dirname=data_home)
A:sklearn.datasets.species_distributions.fhandle->BytesIO(X[f])
A:sklearn.datasets.species_distributions.train->_load_csv(fhandle)
A:sklearn.datasets.species_distributions.test->_load_csv(fhandle)
A:sklearn.datasets.species_distributions.coverages_path->_fetch_remote(COVERAGES, dirname=data_home)
A:sklearn.datasets.species_distributions.coverages->numpy.asarray(coverages, dtype=dtype)
A:sklearn.datasets.species_distributions.bunch->sklearn.externals.joblib.load(archive_path)
sklearn.datasets.fetch_species_distributions(data_home=None,download_if_missing=True)
sklearn.datasets.species_distributions._load_coverage(F,header_length=6,dtype=np.int16)
sklearn.datasets.species_distributions._load_csv(F)
sklearn.datasets.species_distributions.construct_grids(batch)
sklearn.datasets.species_distributions.fetch_species_distributions(data_home=None,download_if_missing=True)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/datasets/olivetti_faces.py----------------------------------------
A:sklearn.datasets.olivetti_faces.FACES->RemoteFileMetadata(filename='olivettifaces.mat', url='https://ndownloader.figshare.com/files/5976027', checksum='b612fb967f2dc77c9c62d3e1266e0c73d5fca46a4b8906c18e454d41af987794')
A:sklearn.datasets.olivetti_faces.data_home->get_data_home(data_home=data_home)
A:sklearn.datasets.olivetti_faces.filepath->_pkl_filepath(data_home, 'olivetti.pkz')
A:sklearn.datasets.olivetti_faces.mat_path->_fetch_remote(FACES, dirname=data_home)
A:sklearn.datasets.olivetti_faces.mfile->loadmat(file_name=mat_path)
A:sklearn.datasets.olivetti_faces.faces->faces.reshape((400, 64, 64)).transpose(0, 2, 1).reshape((400, 64, 64)).transpose(0, 2, 1)
A:sklearn.datasets.olivetti_faces.target->numpy.array([i // 10 for i in range(400)])
A:sklearn.datasets.olivetti_faces.random_state->check_random_state(random_state)
A:sklearn.datasets.olivetti_faces.order->check_random_state(random_state).permutation(len(faces))
sklearn.datasets.fetch_olivetti_faces(data_home=None,shuffle=False,random_state=0,download_if_missing=True)
sklearn.datasets.olivetti_faces.fetch_olivetti_faces(data_home=None,shuffle=False,random_state=0,download_if_missing=True)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/datasets/covtype.py----------------------------------------
A:sklearn.datasets.covtype.ARCHIVE->RemoteFileMetadata(filename='covtype.data.gz', url='https://ndownloader.figshare.com/files/5976039', checksum='614360d0257557dd1792834a85a1cdebfadc3c4f30b011d56afee7ffb5b15771')
A:sklearn.datasets.covtype.logger->logging.getLogger(__name__)
A:sklearn.datasets.covtype.data_home->get_data_home(data_home=data_home)
A:sklearn.datasets.covtype.covtype_dir->join(data_home, 'covertype')
A:sklearn.datasets.covtype.samples_path->_pkl_filepath(covtype_dir, 'samples')
A:sklearn.datasets.covtype.targets_path->_pkl_filepath(covtype_dir, 'targets')
A:sklearn.datasets.covtype.available->exists(samples_path)
A:sklearn.datasets.covtype.archive_path->_fetch_remote(ARCHIVE, dirname=covtype_dir)
A:sklearn.datasets.covtype.Xy->numpy.genfromtxt(GzipFile(filename=archive_path), delimiter=',')
A:sklearn.datasets.covtype.y->externals.joblib.load(targets_path)
A:sklearn.datasets.covtype.X->externals.joblib.load(samples_path)
A:sklearn.datasets.covtype.ind->numpy.arange(X.shape[0])
A:sklearn.datasets.covtype.rng->check_random_state(random_state)
sklearn.datasets.covtype.fetch_covtype(data_home=None,download_if_missing=True,random_state=None,shuffle=False)
sklearn.datasets.fetch_covtype(data_home=None,download_if_missing=True,random_state=None,shuffle=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/datasets/svmlight_format.py----------------------------------------
A:sklearn.datasets.svmlight_format.(_, ext)->os.path.splitext(f)
A:sklearn.datasets.svmlight_format.(actual_dtype, data, ind, indptr, labels, query)->_load_svmlight_file(f, dtype, multilabel, zero_based, query_id, offset, length)
A:sklearn.datasets.svmlight_format.labels->numpy.frombuffer(labels, np.float64)
A:sklearn.datasets.svmlight_format.data->numpy.asarray(data, dtype=dtype)
A:sklearn.datasets.svmlight_format.indices->numpy.frombuffer(ind, np.intc)
A:sklearn.datasets.svmlight_format.indptr->numpy.frombuffer(indptr, dtype=np.intc)
A:sklearn.datasets.svmlight_format.query->numpy.frombuffer(query, np.int64)
A:sklearn.datasets.svmlight_format.X->check_array(X, accept_sparse='csr').sorted_indices()
A:sklearn.datasets.svmlight_format.X_is_sp->int(hasattr(X, 'tocsr'))
A:sklearn.datasets.svmlight_format.y_is_sp->int(hasattr(y, 'tocsr'))
A:sklearn.datasets.svmlight_format.value_pattern->u('%d:%.16g')
A:sklearn.datasets.svmlight_format.label_pattern->u('%.16g')
A:sklearn.datasets.svmlight_format.line_pattern->u('%s')
A:sklearn.datasets.svmlight_format.span->slice(X.indptr[i], X.indptr[i + 1])
A:sklearn.datasets.svmlight_format.row->zip(np.where(nz)[0], X[i, nz])
A:sklearn.datasets.svmlight_format.s->' '.join((value_pattern % (j + one_based, x) for (j, x) in row))
A:sklearn.datasets.svmlight_format.labels_str->','.join((label_pattern % j for j in nz_labels))
A:sklearn.datasets.svmlight_format.comment->comment.encode('utf-8').encode('utf-8')
A:sklearn.datasets.svmlight_format.yval->check_array(y, accept_sparse='csr', ensure_2d=False)
A:sklearn.datasets.svmlight_format.Xval->check_array(X, accept_sparse='csr')
A:sklearn.datasets.svmlight_format.y->check_array(y, accept_sparse='csr', ensure_2d=False).sorted_indices()
A:sklearn.datasets.svmlight_format.query_id->numpy.asarray(query_id)
sklearn.datasets.dump_svmlight_file(X,y,f,zero_based=True,comment=None,query_id=None,multilabel=False)
sklearn.datasets.load_svmlight_file(f,n_features=None,dtype=np.float64,multilabel=False,zero_based='auto',query_id=False,offset=0,length=-1)
sklearn.datasets.load_svmlight_files(files,n_features=None,dtype=np.float64,multilabel=False,zero_based='auto',query_id=False,offset=0,length=-1)
sklearn.datasets.svmlight_format._dump_svmlight(X,y,f,multilabel,one_based,comment,query_id)
sklearn.datasets.svmlight_format._gen_open(f)
sklearn.datasets.svmlight_format._open_and_load(f,dtype,multilabel,zero_based,query_id,offset=0,length=-1)
sklearn.datasets.svmlight_format.dump_svmlight_file(X,y,f,zero_based=True,comment=None,query_id=None,multilabel=False)
sklearn.datasets.svmlight_format.load_svmlight_file(f,n_features=None,dtype=np.float64,multilabel=False,zero_based='auto',query_id=False,offset=0,length=-1)
sklearn.datasets.svmlight_format.load_svmlight_files(files,n_features=None,dtype=np.float64,multilabel=False,zero_based='auto',query_id=False,offset=0,length=-1)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/datasets/lfw.py----------------------------------------
A:sklearn.datasets.lfw.logger->logging.getLogger(__name__)
A:sklearn.datasets.lfw.ARCHIVE->RemoteFileMetadata(filename='lfw.tgz', url='https://ndownloader.figshare.com/files/5976018', checksum='055f7d9c632d7370e6fb4afc7468d40f970c34a80d4c6f50ffec63f5a8d536c0')
A:sklearn.datasets.lfw.FUNNELED_ARCHIVE->RemoteFileMetadata(filename='lfw-funneled.tgz', url='https://ndownloader.figshare.com/files/5976015', checksum='b47c8422c8cded889dc5a13418c4bc2abbda121092b3533a83306f90d900100a')
A:sklearn.datasets.lfw.data_home->get_data_home(data_home=data_home)
A:sklearn.datasets.lfw.lfw_home->join(data_home, 'lfw_home')
A:sklearn.datasets.lfw.target_filepath->join(lfw_home, target.filename)
A:sklearn.datasets.lfw.data_folder_path->join(lfw_home, 'lfw')
A:sklearn.datasets.lfw.archive_path->join(lfw_home, archive.filename)
A:sklearn.datasets.lfw.slice_->tuple((s or ds for (s, ds) in zip(slice_, default_slice)))
A:sklearn.datasets.lfw.resize->float(resize)
A:sklearn.datasets.lfw.h->int(resize * h)
A:sklearn.datasets.lfw.w->int(resize * w)
A:sklearn.datasets.lfw.n_faces->list(pairs.shape).pop(0)
A:sklearn.datasets.lfw.faces->_load_imgs(file_paths, slice_, color, resize)
A:sklearn.datasets.lfw.img->imread(file_path)
A:sklearn.datasets.lfw.face->face.mean(axis=2).mean(axis=2)
A:sklearn.datasets.lfw.folder_path->join(data_folder_path, person_name)
A:sklearn.datasets.lfw.n_pictures->len(paths)
A:sklearn.datasets.lfw.person_name->person_name.replace('_', ' ').replace('_', ' ')
A:sklearn.datasets.lfw.target_names->numpy.unique(person_names)
A:sklearn.datasets.lfw.target->numpy.zeros(n_pairs, dtype=np.int)
A:sklearn.datasets.lfw.indices->numpy.arange(n_faces)
A:sklearn.datasets.lfw.(lfw_home, data_folder_path)->check_fetch_lfw(data_home=data_home, funneled=funneled, download_if_missing=download_if_missing)
A:sklearn.datasets.lfw.m->Memory(cachedir=lfw_home, compress=6, verbose=0)
A:sklearn.datasets.lfw.load_func->Memory(cachedir=lfw_home, compress=6, verbose=0).cache(_fetch_lfw_pairs)
A:sklearn.datasets.lfw.(faces, target, target_names)->load_func(data_folder_path, resize=resize, min_faces_per_person=min_faces_per_person, color=color, slice_=slice_)
A:sklearn.datasets.lfw.n_pairs->len(pair_specs)
A:sklearn.datasets.lfw.file_paths->list()
A:sklearn.datasets.lfw.person_folder->join(data_folder_path, str(name, 'UTF-8'))
A:sklearn.datasets.lfw.filenames->list(sorted(listdir(person_folder)))
A:sklearn.datasets.lfw.file_path->join(person_folder, filenames[idx])
A:sklearn.datasets.lfw.pairs->_load_imgs(file_paths, slice_, color, resize)
A:sklearn.datasets.lfw.shape->list(pairs.shape)
A:sklearn.datasets.lfw.index_file_path->join(lfw_home, label_filenames[subset])
A:sklearn.datasets.lfw.(pairs, target, target_names)->load_func(index_file_path, data_folder_path, resize=resize, color=color, slice_=slice_)
sklearn.datasets.fetch_lfw_pairs(subset='train',data_home=None,funneled=True,resize=0.5,color=False,slice_=(slice(70,195),slice(78,172)),download_if_missing=True)
sklearn.datasets.fetch_lfw_people(data_home=None,funneled=True,resize=0.5,min_faces_per_person=0,color=False,slice_=(slice(70,195),slice(78,172)),download_if_missing=True)
sklearn.datasets.lfw._fetch_lfw_pairs(index_file_path,data_folder_path,slice_=None,color=False,resize=None)
sklearn.datasets.lfw._fetch_lfw_people(data_folder_path,slice_=None,color=False,resize=None,min_faces_per_person=0)
sklearn.datasets.lfw._load_imgs(file_paths,slice_,color,resize)
sklearn.datasets.lfw.check_fetch_lfw(data_home=None,funneled=True,download_if_missing=True)
sklearn.datasets.lfw.fetch_lfw_pairs(subset='train',data_home=None,funneled=True,resize=0.5,color=False,slice_=(slice(70,195),slice(78,172)),download_if_missing=True)
sklearn.datasets.lfw.fetch_lfw_people(data_home=None,funneled=True,resize=0.5,min_faces_per_person=0,color=False,slice_=(slice(70,195),slice(78,172)),download_if_missing=True)
sklearn.datasets.lfw.scale_face(face)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/datasets/california_housing.py----------------------------------------
A:sklearn.datasets.california_housing.ARCHIVE->RemoteFileMetadata(filename='cal_housing.tgz', url='https://ndownloader.figshare.com/files/5976036', checksum='aaa5c9a6afe2225cc2aed2723682ae403280c4a3695a2ddda4ffb5d8215ea681')
A:sklearn.datasets.california_housing.logger->logging.getLogger(__name__)
A:sklearn.datasets.california_housing.data_home->get_data_home(data_home=data_home)
A:sklearn.datasets.california_housing.filepath->_pkl_filepath(data_home, 'cal_housing.pkz')
A:sklearn.datasets.california_housing.archive_path->_fetch_remote(ARCHIVE, dirname=data_home)
A:sklearn.datasets.california_housing.cal_housing->externals.joblib.load(filepath)
sklearn.datasets.california_housing.fetch_california_housing(data_home=None,download_if_missing=True)
sklearn.datasets.fetch_california_housing(data_home=None,download_if_missing=True)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/datasets/rcv1.py----------------------------------------
A:sklearn.datasets.rcv1.TOPICS_METADATA->RemoteFileMetadata(url='https://ndownloader.figshare.com/files/5976048', checksum='2a98e5e5d8b770bded93afc8930d88299474317fe14181aee1466cc754d0d1c1', filename='rcv1v2.topics.qrels.gz')
A:sklearn.datasets.rcv1.logger->logging.getLogger(__name__)
A:sklearn.datasets.rcv1.data_home->get_data_home(data_home=data_home)
A:sklearn.datasets.rcv1.rcv1_dir->join(data_home, 'RCV1')
A:sklearn.datasets.rcv1.samples_path->_pkl_filepath(rcv1_dir, 'samples.pkl')
A:sklearn.datasets.rcv1.sample_id_path->_pkl_filepath(rcv1_dir, 'sample_id.pkl')
A:sklearn.datasets.rcv1.sample_topics_path->_pkl_filepath(rcv1_dir, 'sample_topics.pkl')
A:sklearn.datasets.rcv1.topics_path->_pkl_filepath(rcv1_dir, 'topics_names.pkl')
A:sklearn.datasets.rcv1.file_path->_fetch_remote(each, dirname=rcv1_dir)
A:sklearn.datasets.rcv1.Xy->load_svmlight_files(files, n_features=N_FEATURES)
A:sklearn.datasets.rcv1.X->externals.joblib.load(samples_path)
A:sklearn.datasets.rcv1.sample_id->externals.joblib.load(sample_id_path)
A:sklearn.datasets.rcv1.topics_archive_path->_fetch_remote(TOPICS_METADATA, dirname=rcv1_dir)
A:sklearn.datasets.rcv1.y->externals.joblib.load(sample_topics_path)
A:sklearn.datasets.rcv1.sample_id_bis->numpy.zeros(N_SAMPLES, dtype=np.int32)
A:sklearn.datasets.rcv1.line_components->line.decode('ascii').split(u' ')
A:sklearn.datasets.rcv1.doc->int(doc)
A:sklearn.datasets.rcv1.permutation->_find_permutation(sample_id_bis, sample_id)
A:sklearn.datasets.rcv1.categories->externals.joblib.load(topics_path)
A:sklearn.datasets.rcv1.order->numpy.argsort(categories)
A:sklearn.datasets.rcv1.(X, y, sample_id)->shuffle_(X, y, sample_id, random_state=random_state)
A:sklearn.datasets.rcv1.s->numpy.zeros(n, dtype=np.int32)
A:sklearn.datasets.rcv1.i->numpy.arange(n, dtype=np.int32)
A:sklearn.datasets.rcv1.t->numpy.argsort(a)
A:sklearn.datasets.rcv1.u->numpy.argsort(b)
A:sklearn.datasets.rcv1.u_->_inverse_permutation(u)
sklearn.datasets.fetch_rcv1(data_home=None,subset='all',download_if_missing=True,random_state=None,shuffle=False)
sklearn.datasets.rcv1._find_permutation(a,b)
sklearn.datasets.rcv1._inverse_permutation(p)
sklearn.datasets.rcv1.fetch_rcv1(data_home=None,subset='all',download_if_missing=True,random_state=None,shuffle=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/datasets/mldata.py----------------------------------------
A:sklearn.datasets.mldata.dataname->mldata_filename(dataname)
A:sklearn.datasets.mldata.data_home->join(data_home, 'mldata')
A:sklearn.datasets.mldata.filename->join(data_home, matlab_name)
A:sklearn.datasets.mldata.mldata_url->urlopen(urlname)
A:sklearn.datasets.mldata.matlab_dict->scipy.io.loadmat(matlab_file, struct_as_record=True)
A:sklearn.datasets.mldata.dataset['target']->dataset['target'].squeeze().squeeze()
sklearn.datasets.fetch_mldata(dataname,target_name='label',data_name='data',transpose_data=True,data_home=None)
sklearn.datasets.mldata.fetch_mldata(dataname,target_name='label',data_name='data',transpose_data=True,data_home=None)
sklearn.datasets.mldata.mldata_filename(dataname)
sklearn.datasets.mldata.setup_module(module)
sklearn.datasets.mldata.teardown_module(module)
sklearn.datasets.mldata_filename(dataname)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/datasets/samples_generator.py----------------------------------------
A:sklearn.datasets.samples_generator.out->sample_without_replacement(2 ** dimensions, samples, random_state=rng).astype(dtype='>u4', copy=False)
A:sklearn.datasets.samples_generator.generator->check_random_state(random_state)
A:sklearn.datasets.samples_generator.X->check_random_state(random_state).multivariate_normal(mean, cov * np.identity(n_features), (n_samples,))
A:sklearn.datasets.samples_generator.y->numpy.hstack([np.repeat(np.arange(n_classes), step), np.repeat(n_classes - 1, n_samples - step * n_classes)])
A:sklearn.datasets.samples_generator.centroids->_generate_hypercube(n_clusters, n_informative, generator).astype(float)
A:sklearn.datasets.samples_generator.X[:, :n_informative]->check_random_state(random_state).randn(n_samples, n_informative)
A:sklearn.datasets.samples_generator.X_k[...]->numpy.dot(X_k, A)
A:sklearn.datasets.samples_generator.X[:, n_informative:n_informative + n_redundant]->numpy.dot(X[:, :n_informative], B)
A:sklearn.datasets.samples_generator.indices->numpy.arange(n_samples)
A:sklearn.datasets.samples_generator.X[:, -n_useless:]->check_random_state(random_state).randn(n_samples, n_useless)
A:sklearn.datasets.samples_generator.y[flip_mask]->check_random_state(random_state).randint(n_classes, size=flip_mask.sum())
A:sklearn.datasets.samples_generator.(X, y)->util_shuffle(X, y, random_state=generator)
A:sklearn.datasets.samples_generator.p_c->check_random_state(random_state).rand(n_classes)
A:sklearn.datasets.samples_generator.cumulative_p_c->numpy.cumsum(p_c)
A:sklearn.datasets.samples_generator.p_w_c->check_random_state(random_state).rand(n_features, n_classes)
A:sklearn.datasets.samples_generator.y_size->check_random_state(random_state).poisson(n_labels)
A:sklearn.datasets.samples_generator.c->numpy.searchsorted(cumulative_p_c, generator.rand(y_size - len(y)))
A:sklearn.datasets.samples_generator.n_words->check_random_state(random_state).poisson(length)
A:sklearn.datasets.samples_generator.words->numpy.searchsorted(cumulative_p_w_sample, generator.rand(n_words))
A:sklearn.datasets.samples_generator.cumulative_p_w_sample->check_random_state(random_state).rand(n_features, n_classes).take(y, axis=1).sum(axis=1).cumsum()
A:sklearn.datasets.samples_generator.X_indices->array.array('i')
A:sklearn.datasets.samples_generator.X_indptr->array.array('i', [0])
A:sklearn.datasets.samples_generator.(words, y)->sample_example()
A:sklearn.datasets.samples_generator.X_data->numpy.ones(len(X_indices), dtype=np.float64)
A:sklearn.datasets.samples_generator.lb->MultiLabelBinarizer(sparse_output=return_indicator == 'sparse')
A:sklearn.datasets.samples_generator.Y->numpy.dot(D, X)
A:sklearn.datasets.samples_generator.rs->check_random_state(random_state)
A:sklearn.datasets.samples_generator.n_informative->min(n_features, n_informative)
A:sklearn.datasets.samples_generator.ground_truth->numpy.zeros((n_features, n_targets))
A:sklearn.datasets.samples_generator.outer_circ_x->numpy.cos(np.linspace(0, np.pi, n_samples_out))
A:sklearn.datasets.samples_generator.outer_circ_y->numpy.sin(np.linspace(0, np.pi, n_samples_out))
A:sklearn.datasets.samples_generator.centers->check_array(centers)
A:sklearn.datasets.samples_generator.n->min(n_samples, n_features)
A:sklearn.datasets.samples_generator.(u, _)->scipy.linalg.qr(generator.randn(n_samples, n), mode='economic')
A:sklearn.datasets.samples_generator.(v, _)->scipy.linalg.qr(generator.randn(n_features, n), mode='economic')
A:sklearn.datasets.samples_generator.singular_ind->numpy.arange(n, dtype=np.float64)
A:sklearn.datasets.samples_generator.D->check_random_state(random_state).randn(n_features, n_components)
A:sklearn.datasets.samples_generator.idx->numpy.argsort(np.sum((X - mean[np.newaxis, :]) ** 2, axis=1))
A:sklearn.datasets.samples_generator.X[idx, i]->check_random_state(random_state).randn(n_nonzero_coefs)
A:sklearn.datasets.samples_generator.A->check_random_state(random_state).rand(n_dim, n_dim)
A:sklearn.datasets.samples_generator.(U, s, V)->scipy.linalg.svd(np.dot(A.T, A))
A:sklearn.datasets.samples_generator.random_state->check_random_state(random_state)
A:sklearn.datasets.samples_generator.aux->numpy.tril(aux, k=-1)
A:sklearn.datasets.samples_generator.permutation->check_random_state(random_state).permutation(dim)
A:sklearn.datasets.samples_generator.prec->numpy.dot(chol.T, chol)
A:sklearn.datasets.samples_generator.d->numpy.diag(prec).reshape(1, prec.shape[0])
A:sklearn.datasets.samples_generator.t->numpy.squeeze(t)
A:sklearn.datasets.samples_generator.x->numpy.sin(t)
A:sklearn.datasets.samples_generator.mean->numpy.array(mean)
A:sklearn.datasets.samples_generator.row_idx->check_random_state(random_state).permutation(n_rows)
A:sklearn.datasets.samples_generator.col_idx->check_random_state(random_state).permutation(n_cols)
A:sklearn.datasets.samples_generator.consts->check_random_state(random_state).uniform(minval, maxval, n_clusters)
A:sklearn.datasets.samples_generator.row_sizes->check_random_state(random_state).multinomial(n_rows, np.repeat(1.0 / n_row_clusters, n_row_clusters))
A:sklearn.datasets.samples_generator.col_sizes->check_random_state(random_state).multinomial(n_cols, np.repeat(1.0 / n_col_clusters, n_col_clusters))
A:sklearn.datasets.samples_generator.row_labels->numpy.hstack(list((np.repeat(val, rep) for (val, rep) in zip(range(n_row_clusters), row_sizes))))
A:sklearn.datasets.samples_generator.col_labels->numpy.hstack(list((np.repeat(val, rep) for (val, rep) in zip(range(n_col_clusters), col_sizes))))
A:sklearn.datasets.samples_generator.result->numpy.zeros(shape, dtype=np.float64)
A:sklearn.datasets.samples_generator.selector->numpy.outer(row_labels == i, col_labels == j)
A:sklearn.datasets.samples_generator.(result, row_idx, col_idx)->_shuffle(result, random_state)
A:sklearn.datasets.samples_generator.rows->numpy.vstack((row_labels == label for label in range(n_row_clusters) for _ in range(n_col_clusters)))
A:sklearn.datasets.samples_generator.cols->numpy.vstack((col_labels == label for _ in range(n_row_clusters) for label in range(n_col_clusters)))
sklearn.datasets.make_biclusters(shape,n_clusters,noise=0.0,minval=10,maxval=100,shuffle=True,random_state=None)
sklearn.datasets.make_blobs(n_samples=100,n_features=2,centers=3,cluster_std=1.0,center_box=(-10.0,10.0),shuffle=True,random_state=None)
sklearn.datasets.make_checkerboard(shape,n_clusters,noise=0.0,minval=10,maxval=100,shuffle=True,random_state=None)
sklearn.datasets.make_circles(n_samples=100,shuffle=True,noise=None,random_state=None,factor=0.8)
sklearn.datasets.make_classification(n_samples=100,n_features=20,n_informative=2,n_redundant=2,n_repeated=0,n_classes=2,n_clusters_per_class=2,weights=None,flip_y=0.01,class_sep=1.0,hypercube=True,shift=0.0,scale=1.0,shuffle=True,random_state=None)
sklearn.datasets.make_friedman1(n_samples=100,n_features=10,noise=0.0,random_state=None)
sklearn.datasets.make_friedman2(n_samples=100,noise=0.0,random_state=None)
sklearn.datasets.make_friedman3(n_samples=100,noise=0.0,random_state=None)
sklearn.datasets.make_gaussian_quantiles(mean=None,cov=1.0,n_samples=100,n_features=2,n_classes=3,shuffle=True,random_state=None)
sklearn.datasets.make_hastie_10_2(n_samples=12000,random_state=None)
sklearn.datasets.make_low_rank_matrix(n_samples=100,n_features=100,effective_rank=10,tail_strength=0.5,random_state=None)
sklearn.datasets.make_moons(n_samples=100,shuffle=True,noise=None,random_state=None)
sklearn.datasets.make_multilabel_classification(n_samples=100,n_features=20,n_classes=5,n_labels=2,length=50,allow_unlabeled=True,sparse=False,return_indicator='dense',return_distributions=False,random_state=None)
sklearn.datasets.make_regression(n_samples=100,n_features=100,n_informative=10,n_targets=1,bias=0.0,effective_rank=None,tail_strength=0.5,noise=0.0,shuffle=True,coef=False,random_state=None)
sklearn.datasets.make_s_curve(n_samples=100,noise=0.0,random_state=None)
sklearn.datasets.make_sparse_coded_signal(n_samples,n_components,n_features,n_nonzero_coefs,random_state=None)
sklearn.datasets.make_sparse_spd_matrix(dim=1,alpha=0.95,norm_diag=False,smallest_coef=0.1,largest_coef=0.9,random_state=None)
sklearn.datasets.make_sparse_uncorrelated(n_samples=100,n_features=10,random_state=None)
sklearn.datasets.make_spd_matrix(n_dim,random_state=None)
sklearn.datasets.make_swiss_roll(n_samples=100,noise=0.0,random_state=None)
sklearn.datasets.samples_generator._generate_hypercube(samples,dimensions,rng)
sklearn.datasets.samples_generator._shuffle(data,random_state=None)
sklearn.datasets.samples_generator.make_biclusters(shape,n_clusters,noise=0.0,minval=10,maxval=100,shuffle=True,random_state=None)
sklearn.datasets.samples_generator.make_blobs(n_samples=100,n_features=2,centers=3,cluster_std=1.0,center_box=(-10.0,10.0),shuffle=True,random_state=None)
sklearn.datasets.samples_generator.make_checkerboard(shape,n_clusters,noise=0.0,minval=10,maxval=100,shuffle=True,random_state=None)
sklearn.datasets.samples_generator.make_circles(n_samples=100,shuffle=True,noise=None,random_state=None,factor=0.8)
sklearn.datasets.samples_generator.make_classification(n_samples=100,n_features=20,n_informative=2,n_redundant=2,n_repeated=0,n_classes=2,n_clusters_per_class=2,weights=None,flip_y=0.01,class_sep=1.0,hypercube=True,shift=0.0,scale=1.0,shuffle=True,random_state=None)
sklearn.datasets.samples_generator.make_friedman1(n_samples=100,n_features=10,noise=0.0,random_state=None)
sklearn.datasets.samples_generator.make_friedman2(n_samples=100,noise=0.0,random_state=None)
sklearn.datasets.samples_generator.make_friedman3(n_samples=100,noise=0.0,random_state=None)
sklearn.datasets.samples_generator.make_gaussian_quantiles(mean=None,cov=1.0,n_samples=100,n_features=2,n_classes=3,shuffle=True,random_state=None)
sklearn.datasets.samples_generator.make_hastie_10_2(n_samples=12000,random_state=None)
sklearn.datasets.samples_generator.make_low_rank_matrix(n_samples=100,n_features=100,effective_rank=10,tail_strength=0.5,random_state=None)
sklearn.datasets.samples_generator.make_moons(n_samples=100,shuffle=True,noise=None,random_state=None)
sklearn.datasets.samples_generator.make_multilabel_classification(n_samples=100,n_features=20,n_classes=5,n_labels=2,length=50,allow_unlabeled=True,sparse=False,return_indicator='dense',return_distributions=False,random_state=None)
sklearn.datasets.samples_generator.make_regression(n_samples=100,n_features=100,n_informative=10,n_targets=1,bias=0.0,effective_rank=None,tail_strength=0.5,noise=0.0,shuffle=True,coef=False,random_state=None)
sklearn.datasets.samples_generator.make_s_curve(n_samples=100,noise=0.0,random_state=None)
sklearn.datasets.samples_generator.make_sparse_coded_signal(n_samples,n_components,n_features,n_nonzero_coefs,random_state=None)
sklearn.datasets.samples_generator.make_sparse_spd_matrix(dim=1,alpha=0.95,norm_diag=False,smallest_coef=0.1,largest_coef=0.9,random_state=None)
sklearn.datasets.samples_generator.make_sparse_uncorrelated(n_samples=100,n_features=10,random_state=None)
sklearn.datasets.samples_generator.make_spd_matrix(n_dim,random_state=None)
sklearn.datasets.samples_generator.make_swiss_roll(n_samples=100,noise=0.0,random_state=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/datasets/setup.py----------------------------------------
A:sklearn.datasets.setup.config->Configuration('datasets', parent_package, top_path)
sklearn.datasets.setup.configuration(parent_package='',top_path=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/datasets/mlcomp.py----------------------------------------
A:sklearn.datasets.mlcomp.dataset_path->os.path.join(mlcomp_root, dataset)
A:sklearn.datasets.mlcomp.mlcomp_root->os.path.normpath(mlcomp_root)
A:sklearn.datasets.mlcomp.metadata_file->os.path.join(dataset_path, 'metadata')
A:sklearn.datasets.mlcomp.metadata->dict()
A:sklearn.datasets.mlcomp.(key, value)->line.split(':', 1)
A:sklearn.datasets.mlcomp.metadata[key.strip()]->value.strip()
A:sklearn.datasets.mlcomp.format->dict().get('format', 'unknow')
A:sklearn.datasets.mlcomp.loader->LOADERS.get(format)
sklearn.datasets.load_mlcomp(name_or_id,set_='raw',mlcomp_root=None,**kwargs)
sklearn.datasets.mlcomp._load_document_classification(dataset_path,metadata,set_=None,**kwargs)
sklearn.datasets.mlcomp.load_mlcomp(name_or_id,set_='raw',mlcomp_root=None,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/datasets/twenty_newsgroups.py----------------------------------------
A:sklearn.datasets.twenty_newsgroups.logger->logging.getLogger(__name__)
A:sklearn.datasets.twenty_newsgroups.ARCHIVE->RemoteFileMetadata(filename='20news-bydate.tar.gz', url='https://ndownloader.figshare.com/files/5975967', checksum='8f1b2514ca22a5ade8fbb9cfa5727df95fa587f4c87b786e15c759fa66d95610')
A:sklearn.datasets.twenty_newsgroups.train_path->os.path.join(target_dir, TRAIN_FOLDER)
A:sklearn.datasets.twenty_newsgroups.test_path->os.path.join(target_dir, TEST_FOLDER)
A:sklearn.datasets.twenty_newsgroups.archive_path->_fetch_remote(ARCHIVE, dirname=target_dir)
A:sklearn.datasets.twenty_newsgroups.cache->download_20newsgroups(target_dir=twenty_home, cache_path=cache_path)
A:sklearn.datasets.twenty_newsgroups.compressed_content->f.read()
A:sklearn.datasets.twenty_newsgroups.(_before, _blankline, after)->text.partition('\n\n')
A:sklearn.datasets.twenty_newsgroups._QUOTE_RE->re.compile('(writes in|writes:|wrote:|says:|said:|^In article|^Quoted from|^\\||^>)')
A:sklearn.datasets.twenty_newsgroups.lines->text.strip().split('\n')
A:sklearn.datasets.twenty_newsgroups.data_home->get_data_home(data_home=data_home)
A:sklearn.datasets.twenty_newsgroups.cache_path->_pkl_filepath(data_home, CACHE_NAME)
A:sklearn.datasets.twenty_newsgroups.twenty_home->os.path.join(data_home, '20news_home')
A:sklearn.datasets.twenty_newsgroups.uncompressed_content->codecs.decode(compressed_content, 'zlib_codec')
A:sklearn.datasets.twenty_newsgroups.data_lst->numpy.array(data.data, dtype=object)
A:sklearn.datasets.twenty_newsgroups.target->numpy.concatenate((data_train.target, data_test.target))
A:sklearn.datasets.twenty_newsgroups.filenames->list()
A:sklearn.datasets.twenty_newsgroups.data.target->numpy.searchsorted(labels, data.target)
A:sklearn.datasets.twenty_newsgroups.data.filenames->numpy.array(filenames)
A:sklearn.datasets.twenty_newsgroups.(labels, categories)->zip(*labels)
A:sklearn.datasets.twenty_newsgroups.mask->numpy.in1d(data.target, labels)
A:sklearn.datasets.twenty_newsgroups.data.target_names->list(categories)
A:sklearn.datasets.twenty_newsgroups.data.data->numpy.array(data.data, dtype=object).tolist()
A:sklearn.datasets.twenty_newsgroups.random_state->check_random_state(random_state)
A:sklearn.datasets.twenty_newsgroups.indices->numpy.arange(data.target.shape[0])
A:sklearn.datasets.twenty_newsgroups.target_file->_pkl_filepath(data_home, filebase + '.pkl')
A:sklearn.datasets.twenty_newsgroups.data_train->fetch_20newsgroups(data_home=data_home, subset='train', categories=None, shuffle=True, random_state=12, remove=remove, download_if_missing=download_if_missing)
A:sklearn.datasets.twenty_newsgroups.data_test->fetch_20newsgroups(data_home=data_home, subset='test', categories=None, shuffle=True, random_state=12, remove=remove, download_if_missing=download_if_missing)
A:sklearn.datasets.twenty_newsgroups.(X_train, X_test)->externals.joblib.load(target_file)
A:sklearn.datasets.twenty_newsgroups.vectorizer->CountVectorizer(dtype=np.int16)
A:sklearn.datasets.twenty_newsgroups.X_train->X_train.astype(np.float64).astype(np.float64)
A:sklearn.datasets.twenty_newsgroups.X_test->X_test.astype(np.float64).astype(np.float64)
A:sklearn.datasets.twenty_newsgroups.data->scipy.sparse.vstack((X_train, X_test)).tocsr()
sklearn.datasets.fetch_20newsgroups(data_home=None,subset='train',categories=None,shuffle=True,random_state=42,remove=(),download_if_missing=True)
sklearn.datasets.fetch_20newsgroups_vectorized(subset='train',remove=(),data_home=None,download_if_missing=True)
sklearn.datasets.twenty_newsgroups.download_20newsgroups(target_dir,cache_path)
sklearn.datasets.twenty_newsgroups.fetch_20newsgroups(data_home=None,subset='train',categories=None,shuffle=True,random_state=42,remove=(),download_if_missing=True)
sklearn.datasets.twenty_newsgroups.fetch_20newsgroups_vectorized(subset='train',remove=(),data_home=None,download_if_missing=True)
sklearn.datasets.twenty_newsgroups.strip_newsgroup_footer(text)
sklearn.datasets.twenty_newsgroups.strip_newsgroup_header(text)
sklearn.datasets.twenty_newsgroups.strip_newsgroup_quoting(text)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/datasets/kddcup99.py----------------------------------------
A:sklearn.datasets.kddcup99.ARCHIVE->RemoteFileMetadata(filename='kddcup99_data', url='https://ndownloader.figshare.com/files/5976045', checksum='3b6c942aa0356c0ca35b7b595a26c89d343652c9db428893e7494f837b274292')
A:sklearn.datasets.kddcup99.ARCHIVE_10_PERCENT->RemoteFileMetadata(filename='kddcup99_10_data', url='https://ndownloader.figshare.com/files/5976042', checksum='8045aca0d84e70e622d1148d7df782496f6333bf6eb979a1b0837c42a9fd9561')
A:sklearn.datasets.kddcup99.logger->logging.getLogger(__name__)
A:sklearn.datasets.kddcup99.data_home->get_data_home(data_home=data_home)
A:sklearn.datasets.kddcup99.kddcup99->_fetch_brute_kddcup99(data_home=data_home, shuffle=shuffle, percent10=percent10, download_if_missing=download_if_missing)
A:sklearn.datasets.kddcup99.t->numpy.logical_not(s)
A:sklearn.datasets.kddcup99.random_state->check_random_state(random_state)
A:sklearn.datasets.kddcup99.r->check_random_state(random_state).randint(0, n_samples_abnormal, 3377)
A:sklearn.datasets.kddcup99.data[:, 0]->numpy.log((data[:, 0] + 0.1).astype(float))
A:sklearn.datasets.kddcup99.data[:, 4]->numpy.log((data[:, 4] + 0.1).astype(float))
A:sklearn.datasets.kddcup99.data[:, 5]->numpy.log((data[:, 5] + 0.1).astype(float))
A:sklearn.datasets.kddcup99.kddcup_dir->join(data_home, 'kddcup99' + dir_suffix)
A:sklearn.datasets.kddcup99.samples_path->join(kddcup_dir, 'samples')
A:sklearn.datasets.kddcup99.targets_path->join(kddcup_dir, 'targets')
A:sklearn.datasets.kddcup99.available->exists(samples_path)
A:sklearn.datasets.kddcup99.DT->numpy.dtype(dt)
A:sklearn.datasets.kddcup99.archive_path->join(kddcup_dir, archive.filename)
A:sklearn.datasets.kddcup99.file_->GzipFile(filename=archive_path, mode='r')
A:sklearn.datasets.kddcup99.line->line.decode().decode()
A:sklearn.datasets.kddcup99.Xy->numpy.asarray(Xy, dtype=object)
A:sklearn.datasets.kddcup99.Xy[:, j]->Xy[:, j].astype(DT[j]).astype(DT[j])
A:sklearn.datasets.kddcup99.X->externals.joblib.load(samples_path)
A:sklearn.datasets.kddcup99.y->externals.joblib.load(targets_path)
A:sklearn.datasets.kddcup99.(X, y)->shuffle_method(X, y, random_state=random_state)
sklearn.datasets.fetch_kddcup99(subset=None,data_home=None,shuffle=False,random_state=None,percent10=True,download_if_missing=True)
sklearn.datasets.kddcup99._fetch_brute_kddcup99(data_home=None,download_if_missing=True,random_state=None,shuffle=False,percent10=True)
sklearn.datasets.kddcup99._mkdirp(d)
sklearn.datasets.kddcup99.fetch_kddcup99(subset=None,data_home=None,shuffle=False,random_state=None,percent10=True,download_if_missing=True)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/datasets/base.py----------------------------------------
A:sklearn.datasets.base.RemoteFileMetadata->namedtuple('RemoteFileMetadata', ['filename', 'url', 'checksum'])
A:sklearn.datasets.base.data_home->get_data_home(data_home)
A:sklearn.datasets.base.folder_path->join(container_path, folder)
A:sklearn.datasets.base.filenames->numpy.array(filenames)
A:sklearn.datasets.base.target->numpy.empty((n_samples,))
A:sklearn.datasets.base.random_state->check_random_state(random_state)
A:sklearn.datasets.base.indices->numpy.arange(filenames.shape[0])
A:sklearn.datasets.base.data_file->csv.reader(f)
A:sklearn.datasets.base.temp->next(data_file)
A:sklearn.datasets.base.n_samples->int(temp[0])
A:sklearn.datasets.base.n_features->int(temp[1])
A:sklearn.datasets.base.target_names->numpy.array(temp[2:])
A:sklearn.datasets.base.data->numpy.empty((n_samples, n_features))
A:sklearn.datasets.base.data[i]->numpy.asarray(d[:-1], dtype=np.float64)
A:sklearn.datasets.base.target[i]->numpy.asarray(d[-1], dtype=np.float64)
A:sklearn.datasets.base.module_path->join(dirname(__file__), 'images')
A:sklearn.datasets.base.(data, target, target_names)->load_data(module_path, 'breast_cancer.csv')
A:sklearn.datasets.base.fdescr->rst_file.read()
A:sklearn.datasets.base.feature_names->numpy.array(temp)
A:sklearn.datasets.base.descr->f.read()
A:sklearn.datasets.base.images->load_sample_images()
A:sklearn.datasets.base.base_dir->join(dirname(__file__), 'data/')
A:sklearn.datasets.base.data_exercise->numpy.loadtxt(base_dir + 'linnerud_exercise.csv', skiprows=1)
A:sklearn.datasets.base.data_physiological->numpy.loadtxt(base_dir + 'linnerud_physiological.csv', skiprows=1)
A:sklearn.datasets.base.header_exercise->f.readline().split()
A:sklearn.datasets.base.header_physiological->f.readline().split()
A:sklearn.datasets.base.fdescr_name->join(module_path, 'descr', 'boston_house_prices.rst')
A:sklearn.datasets.base.descr_text->f.read()
A:sklearn.datasets.base.data_file_name->join(module_path, 'data', 'boston_house_prices.csv')
A:sklearn.datasets.base.py3_suffix->kwargs.get('py3_suffix', '_py3')
A:sklearn.datasets.base.(basename, ext)->splitext(args[-1])
A:sklearn.datasets.base.sha256hash->hashlib.sha256()
A:sklearn.datasets.base.buffer->f.read(chunk_size)
A:sklearn.datasets.base.checksum->_sha256(file_path)
sklearn.datasets.base._fetch_remote(remote,dirname=None)
sklearn.datasets.base._pkl_filepath(*args,**kwargs)
sklearn.datasets.base._sha256(path)
sklearn.datasets.base.clear_data_home(data_home=None)
sklearn.datasets.base.get_data_home(data_home=None)
sklearn.datasets.base.load_boston(return_X_y=False)
sklearn.datasets.base.load_breast_cancer(return_X_y=False)
sklearn.datasets.base.load_data(module_path,data_file_name)
sklearn.datasets.base.load_diabetes(return_X_y=False)
sklearn.datasets.base.load_digits(n_class=10,return_X_y=False)
sklearn.datasets.base.load_files(container_path,description=None,categories=None,load_content=True,shuffle=True,encoding=None,decode_error='strict',random_state=0)
sklearn.datasets.base.load_iris(return_X_y=False)
sklearn.datasets.base.load_linnerud(return_X_y=False)
sklearn.datasets.base.load_sample_image(image_name)
sklearn.datasets.base.load_sample_images()
sklearn.datasets.base.load_wine(return_X_y=False)
sklearn.datasets.clear_data_home(data_home=None)
sklearn.datasets.get_data_home(data_home=None)
sklearn.datasets.load_boston(return_X_y=False)
sklearn.datasets.load_breast_cancer(return_X_y=False)
sklearn.datasets.load_diabetes(return_X_y=False)
sklearn.datasets.load_digits(n_class=10,return_X_y=False)
sklearn.datasets.load_files(container_path,description=None,categories=None,load_content=True,shuffle=True,encoding=None,decode_error='strict',random_state=0)
sklearn.datasets.load_iris(return_X_y=False)
sklearn.datasets.load_linnerud(return_X_y=False)
sklearn.datasets.load_sample_image(image_name)
sklearn.datasets.load_sample_images()
sklearn.datasets.load_wine(return_X_y=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/datasets/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/datasets/tests/test_base.py----------------------------------------
A:sklearn.datasets.tests.test_base.DATA_HOME->tempfile.mkdtemp(prefix='scikit_learn_data_home_test_')
A:sklearn.datasets.tests.test_base.LOAD_FILES_ROOT->tempfile.mkdtemp(prefix='scikit_learn_load_files_test_')
A:sklearn.datasets.tests.test_base.TEST_CATEGORY_DIR1->tempfile.mkdtemp(dir=LOAD_FILES_ROOT)
A:sklearn.datasets.tests.test_base.TEST_CATEGORY_DIR2->tempfile.mkdtemp(dir=LOAD_FILES_ROOT)
A:sklearn.datasets.tests.test_base.sample_file->tempfile.NamedTemporaryFile(dir=TEST_CATEGORY_DIR1, delete=False)
A:sklearn.datasets.tests.test_base.data_home->get_data_home(data_home=DATA_HOME)
A:sklearn.datasets.tests.test_base.res->load_boston()
A:sklearn.datasets.tests.test_base.category->os.path.abspath(TEST_CATEGORY_DIR1).split('/').pop()
A:sklearn.datasets.tests.test_base.digits->load_digits(9)
A:sklearn.datasets.tests.test_base.X_y_tuple->load_boston(return_X_y=True)
A:sklearn.datasets.tests.test_base.bunch->Bunch(key='original')
A:sklearn.datasets.tests.test_base.china->load_sample_image('china.jpg')
A:sklearn.datasets.tests.test_base.bunch_from_pkl->loads(dumps(bunch))
A:sklearn.datasets.tests.test_base.data->load_iris()
sklearn.datasets.tests.test_base._remove_dir(path)
sklearn.datasets.tests.test_base.setup_load_files()
sklearn.datasets.tests.test_base.teardown_load_files()
sklearn.datasets.tests.test_base.teardown_module()
sklearn.datasets.tests.test_base.test_bunch_dir()
sklearn.datasets.tests.test_base.test_bunch_pickle_generated_with_0_16_and_read_with_0_17()
sklearn.datasets.tests.test_base.test_data_home()
sklearn.datasets.tests.test_base.test_default_empty_load_files()
sklearn.datasets.tests.test_base.test_default_load_files()
sklearn.datasets.tests.test_base.test_load_boston()
sklearn.datasets.tests.test_base.test_load_breast_cancer()
sklearn.datasets.tests.test_base.test_load_diabetes()
sklearn.datasets.tests.test_base.test_load_digits()
sklearn.datasets.tests.test_base.test_load_digits_n_class_lt_10()
sklearn.datasets.tests.test_base.test_load_files_w_categories_desc_and_encoding()
sklearn.datasets.tests.test_base.test_load_files_wo_load_content()
sklearn.datasets.tests.test_base.test_load_iris()
sklearn.datasets.tests.test_base.test_load_linnerud()
sklearn.datasets.tests.test_base.test_load_missing_sample_image_error()
sklearn.datasets.tests.test_base.test_load_sample_image()
sklearn.datasets.tests.test_base.test_load_sample_images()
sklearn.datasets.tests.test_base.test_load_wine()
sklearn.datasets.tests.test_base.test_loads_dumps_bunch()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/datasets/tests/test_rcv1.py----------------------------------------
A:sklearn.datasets.tests.test_rcv1.data1->fetch_rcv1(shuffle=False, download_if_missing=False)
A:sklearn.datasets.tests.test_rcv1.j->cat_list.index(cat)
A:sklearn.datasets.tests.test_rcv1.data2->fetch_rcv1(shuffle=True, subset='train', random_state=77, download_if_missing=False)
A:sklearn.datasets.tests.test_rcv1.idx1->s1.tolist().index(sample_id)
A:sklearn.datasets.tests.test_rcv1.idx2->s2.tolist().index(sample_id)
A:sklearn.datasets.tests.test_rcv1.feature_values_1->X1[idx1, :].toarray()
A:sklearn.datasets.tests.test_rcv1.feature_values_2->X2[idx2, :].toarray()
A:sklearn.datasets.tests.test_rcv1.target_values_1->Y1[idx1, :].toarray()
A:sklearn.datasets.tests.test_rcv1.target_values_2->Y2[idx2, :].toarray()
sklearn.datasets.tests.test_rcv1.test_fetch_rcv1()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/datasets/tests/test_samples_generator.py----------------------------------------
A:sklearn.datasets.tests.test_samples_generator.(X, y)->make_moons(3, shuffle=False)
A:sklearn.datasets.tests.test_samples_generator.make->partial(make_classification, class_sep=class_sep, n_redundant=0, n_repeated=0, flip_y=0, shift=0, scale=1, shuffle=False)
A:sklearn.datasets.tests.test_samples_generator.n_classes->len(weights)
A:sklearn.datasets.tests.test_samples_generator.signs->signs.view(dtype='|S{0}'.format(signs.strides[0])).view(dtype='|S{0}'.format(signs.strides[0]))
A:sklearn.datasets.tests.test_samples_generator.(unique_signs, cluster_index)->numpy.unique(signs, return_inverse=True)
A:sklearn.datasets.tests.test_samples_generator.clusters_by_class->defaultdict(set)
A:sklearn.datasets.tests.test_samples_generator.centroid->X[cluster_index == cluster].mean(axis=0)
A:sklearn.datasets.tests.test_samples_generator.(X, Y)->make_multilabel_classification(n_samples=25, n_features=20, n_classes=3, random_state=0, return_indicator='sparse', allow_unlabeled=allow_unlabeled)
A:sklearn.datasets.tests.test_samples_generator.(X2, Y2, p_c, p_w_c)->make_multilabel_classification(n_samples=25, n_features=20, n_classes=3, random_state=0, allow_unlabeled=allow_unlabeled, return_distributions=True)
A:sklearn.datasets.tests.test_samples_generator.(X, y, c)->make_regression(n_samples=100, n_features=10, n_informative=3, n_targets=3, coef=True, noise=1.0, random_state=0)
A:sklearn.datasets.tests.test_samples_generator.cluster_stds->numpy.array([0.05, 0.2, 0.4])
A:sklearn.datasets.tests.test_samples_generator.cluster_centers->numpy.array([[0.0, 0.0], [1.0, 1.0], [0.0, 1.0]])
A:sklearn.datasets.tests.test_samples_generator.X->make_spd_matrix(n_dim=5, random_state=0)
A:sklearn.datasets.tests.test_samples_generator.(u, s, v)->svd(X)
A:sklearn.datasets.tests.test_samples_generator.(Y, D, X)->make_sparse_coded_signal(n_samples=5, n_components=8, n_features=10, n_nonzero_coefs=3, random_state=0)
A:sklearn.datasets.tests.test_samples_generator.(eigenvalues, _)->eig(X)
A:sklearn.datasets.tests.test_samples_generator.(X, t)->make_s_curve(n_samples=5, noise=0.0, random_state=0)
A:sklearn.datasets.tests.test_samples_generator.(X, rows, cols)->make_checkerboard(shape=(100, 100), n_clusters=2, shuffle=True, random_state=0)
A:sklearn.datasets.tests.test_samples_generator.(X2, _, _)->make_checkerboard(shape=(100, 100), n_clusters=2, shuffle=True, random_state=0)
A:sklearn.datasets.tests.test_samples_generator.(X1, _, _)->make_checkerboard(shape=(100, 100), n_clusters=2, shuffle=True, random_state=0)
A:sklearn.datasets.tests.test_samples_generator.dist_sqr->((x - center) ** 2).sum()
sklearn.datasets.tests.test_samples_generator.test_make_biclusters()
sklearn.datasets.tests.test_samples_generator.test_make_blobs()
sklearn.datasets.tests.test_samples_generator.test_make_checkerboard()
sklearn.datasets.tests.test_samples_generator.test_make_classification()
sklearn.datasets.tests.test_samples_generator.test_make_classification_informative_features()
sklearn.datasets.tests.test_samples_generator.test_make_friedman1()
sklearn.datasets.tests.test_samples_generator.test_make_friedman2()
sklearn.datasets.tests.test_samples_generator.test_make_friedman3()
sklearn.datasets.tests.test_samples_generator.test_make_hastie_10_2()
sklearn.datasets.tests.test_samples_generator.test_make_low_rank_matrix()
sklearn.datasets.tests.test_samples_generator.test_make_moons()
sklearn.datasets.tests.test_samples_generator.test_make_multilabel_classification_return_indicator()
sklearn.datasets.tests.test_samples_generator.test_make_multilabel_classification_return_indicator_sparse()
sklearn.datasets.tests.test_samples_generator.test_make_multilabel_classification_return_sequences()
sklearn.datasets.tests.test_samples_generator.test_make_regression()
sklearn.datasets.tests.test_samples_generator.test_make_regression_multitarget()
sklearn.datasets.tests.test_samples_generator.test_make_s_curve()
sklearn.datasets.tests.test_samples_generator.test_make_sparse_coded_signal()
sklearn.datasets.tests.test_samples_generator.test_make_sparse_uncorrelated()
sklearn.datasets.tests.test_samples_generator.test_make_spd_matrix()
sklearn.datasets.tests.test_samples_generator.test_make_swiss_roll()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/datasets/tests/test_20news.py----------------------------------------
A:sklearn.datasets.tests.test_20news.data->sklearn.datasets.fetch_20newsgroups(subset='all')
A:sklearn.datasets.tests.test_20news.data2cats->sklearn.datasets.fetch_20newsgroups(subset='all', categories=data.target_names[-1:-3:-1], shuffle=False)
A:sklearn.datasets.tests.test_20news.label->sklearn.datasets.fetch_20newsgroups(subset='all').target_names.index(category)
A:sklearn.datasets.tests.test_20news.bunch->sklearn.datasets.fetch_20newsgroups_vectorized(subset='all')
sklearn.datasets.tests.test_20news.test_20news()
sklearn.datasets.tests.test_20news.test_20news_length_consistency()
sklearn.datasets.tests.test_20news.test_20news_vectorized()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/datasets/tests/test_mldata.py----------------------------------------
A:sklearn.datasets.tests.test_mldata.tmpdir->tempfile.mkdtemp()
A:sklearn.datasets.tests.test_mldata.datasets.mldata.urlopen->mock_mldata_urlopen({dataname: ({'y': y, 'x': x, 'z': z}, ['z', 'x', 'y'])})
A:sklearn.datasets.tests.test_mldata.mock->fetch_mldata('mock', data_home=tmpdir)
A:sklearn.datasets.tests.test_mldata.x->scipy.arange(6).reshape(2, 3)
A:sklearn.datasets.tests.test_mldata.dset->fetch_mldata(dataname, target_name='y', data_name='z', data_home=tmpdir)
A:sklearn.datasets.tests.test_mldata.y->scipy.array([1, -1])
A:sklearn.datasets.tests.test_mldata.z->scipy.arange(12).reshape(4, 3)
sklearn.datasets.tests.test_mldata.setup_tmpdata()
sklearn.datasets.tests.test_mldata.teardown_tmpdata()
sklearn.datasets.tests.test_mldata.test_download()
sklearn.datasets.tests.test_mldata.test_fetch_multiple_column()
sklearn.datasets.tests.test_mldata.test_fetch_one_column()
sklearn.datasets.tests.test_mldata.test_mldata_filename()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/datasets/tests/test_lfw.py----------------------------------------
A:sklearn.datasets.tests.test_lfw.SCIKIT_LEARN_DATA->tempfile.mkdtemp(prefix='scikit_learn_lfw_test_')
A:sklearn.datasets.tests.test_lfw.SCIKIT_LEARN_EMPTY_DATA->tempfile.mkdtemp(prefix='scikit_learn_empty_test_')
A:sklearn.datasets.tests.test_lfw.LFW_HOME->os.path.join(SCIKIT_LEARN_DATA, 'lfw_home')
A:sklearn.datasets.tests.test_lfw.random_state->random.Random(42)
A:sklearn.datasets.tests.test_lfw.np_rng->numpy.random.RandomState(42)
A:sklearn.datasets.tests.test_lfw.folder_name->os.path.join(LFW_HOME, 'lfw_funneled', name)
A:sklearn.datasets.tests.test_lfw.n_faces->numpy.random.RandomState(42).randint(1, 5)
A:sklearn.datasets.tests.test_lfw.file_path->os.path.join(folder_name, name + '_%04d.jpg' % i)
A:sklearn.datasets.tests.test_lfw.uniface->numpy.random.RandomState(42).randint(0, 255, size=(250, 250, 3))
A:sklearn.datasets.tests.test_lfw.name->random.Random(42).choice(more_than_two)
A:sklearn.datasets.tests.test_lfw.(first, second)->random.Random(42).sample(range(counts[name]), 2)
A:sklearn.datasets.tests.test_lfw.(first_name, second_name)->random.Random(42).sample(FAKE_NAMES, 2)
A:sklearn.datasets.tests.test_lfw.first_index->random.Random(42).choice(np.arange(counts[first_name]))
A:sklearn.datasets.tests.test_lfw.second_index->random.Random(42).choice(np.arange(counts[second_name]))
A:sklearn.datasets.tests.test_lfw.lfw_people->fetch_lfw_people(data_home=SCIKIT_LEARN_DATA, resize=None, slice_=None, color=True, download_if_missing=False)
A:sklearn.datasets.tests.test_lfw.lfw_pairs_train->fetch_lfw_pairs(data_home=SCIKIT_LEARN_DATA, resize=None, slice_=None, color=True, download_if_missing=False)
sklearn.datasets.tests.test_lfw.setup_module()
sklearn.datasets.tests.test_lfw.teardown_module()
sklearn.datasets.tests.test_lfw.test_load_empty_lfw_pairs()
sklearn.datasets.tests.test_lfw.test_load_empty_lfw_people()
sklearn.datasets.tests.test_lfw.test_load_fake_lfw_pairs()
sklearn.datasets.tests.test_lfw.test_load_fake_lfw_people()
sklearn.datasets.tests.test_lfw.test_load_fake_lfw_people_too_restrictive()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/datasets/tests/test_svmlight_format.py----------------------------------------
A:sklearn.datasets.tests.test_svmlight_format.currdir->os.path.dirname(os.path.abspath(__file__))
A:sklearn.datasets.tests.test_svmlight_format.datafile->os.path.join(currdir, 'data', 'svmlight_classification.txt')
A:sklearn.datasets.tests.test_svmlight_format.multifile->os.path.join(currdir, 'data', 'svmlight_multilabel.txt')
A:sklearn.datasets.tests.test_svmlight_format.invalidfile->os.path.join(currdir, 'data', 'svmlight_invalid.txt')
A:sklearn.datasets.tests.test_svmlight_format.invalidfile2->os.path.join(currdir, 'data', 'svmlight_invalid_order.txt')
A:sklearn.datasets.tests.test_svmlight_format.(X, y)->load_svmlight_file(f, n_features=4, zero_based=zero_based)
A:sklearn.datasets.tests.test_svmlight_format.(X1, y1)->load_svmlight_file(datafile)
A:sklearn.datasets.tests.test_svmlight_format.fd->os.open(datafile, os.O_RDONLY)
A:sklearn.datasets.tests.test_svmlight_format.(X2, y2)->load_svmlight_file(f, zero_based=False)
A:sklearn.datasets.tests.test_svmlight_format.(X_train, y_train, X_test, y_test)->load_svmlight_files([datafile] * 2, dtype=np.float32)
A:sklearn.datasets.tests.test_svmlight_format.(X1, y1, X2, y2, X3, y3)->load_svmlight_files([datafile] * 3, dtype=np.float64)
A:sklearn.datasets.tests.test_svmlight_format.(Xgz, ygz)->load_svmlight_file(tmp.name)
A:sklearn.datasets.tests.test_svmlight_format.(Xbz, ybz)->load_svmlight_file(tmp.name)
A:sklearn.datasets.tests.test_svmlight_format.f->BytesIO()
A:sklearn.datasets.tests.test_svmlight_format.data1->b('-1 1:1 2:2 3:3\n')
A:sklearn.datasets.tests.test_svmlight_format.data2->b('-1 0:0 1:1\n')
A:sklearn.datasets.tests.test_svmlight_format.f1->BytesIO(data1)
A:sklearn.datasets.tests.test_svmlight_format.f2->BytesIO(data2)
A:sklearn.datasets.tests.test_svmlight_format.(X1, y1, X2, y2)->load_svmlight_files([f1, f2], zero_based='auto')
A:sklearn.datasets.tests.test_svmlight_format.data->b('\n    1 qid:0 0:1 1:2 2:3\n    0 qid:72048431380967004 0:1440446648 1:72048431380967004 2:236784985\n    0 qid:-9223372036854775807 0:1440446648 1:72048431380967004 2:236784985\n    3 qid:9223372036854775807  0:1440446648 1:72048431380967004 2:236784985')
A:sklearn.datasets.tests.test_svmlight_format.res1->load_svmlight_files([BytesIO(data)], query_id=True)
A:sklearn.datasets.tests.test_svmlight_format.res2->load_svmlight_file(BytesIO(data), query_id=True)
A:sklearn.datasets.tests.test_svmlight_format.(X_sparse, y_dense)->load_svmlight_file(datafile)
A:sklearn.datasets.tests.test_svmlight_format.X_dense->X_sparse.toarray()
A:sklearn.datasets.tests.test_svmlight_format.y_sparse->scipy.sparse.csr_matrix(y_dense)
A:sklearn.datasets.tests.test_svmlight_format.comment->str(comment, 'utf-8')
A:sklearn.datasets.tests.test_svmlight_format.X2_dense->X2.toarray()
A:sklearn.datasets.tests.test_svmlight_format.X->scipy.sparse.csr_matrix(X)
A:sklearn.datasets.tests.test_svmlight_format.utf8_comment->b('It is true that\nÂ½Â² = Â¼')
A:sklearn.datasets.tests.test_svmlight_format.unicode_comment->b('It is true that\nÂ½Â² = Â¼').decode('utf-8')
A:sklearn.datasets.tests.test_svmlight_format.(X1, y1, query_id1)->load_svmlight_file(f, query_id=True, zero_based=True)
A:sklearn.datasets.tests.test_svmlight_format.(X, y, qid)->load_svmlight_file(f, query_id=True, zero_based=True)
A:sklearn.datasets.tests.test_svmlight_format.true_X->scipy.sparse.csr_matrix(np.zeros(shape=(3, 4)))
A:sklearn.datasets.tests.test_svmlight_format.true_y->numpy.array([0, 1, 0])
A:sklearn.datasets.tests.test_svmlight_format.rng->numpy.random.RandomState(0)
A:sklearn.datasets.tests.test_svmlight_format.y->numpy.random.RandomState(0).randint(low=0, high=2, size=n_samples)
A:sklearn.datasets.tests.test_svmlight_format.size->len(f.getvalue())
A:sklearn.datasets.tests.test_svmlight_format.(X_0, y_0)->load_svmlight_file(f, n_features=n_features, offset=mark_0, length=length_0)
A:sklearn.datasets.tests.test_svmlight_format.(X_1, y_1)->load_svmlight_file(f, n_features=n_features, offset=mark_1, length=length_1)
A:sklearn.datasets.tests.test_svmlight_format.(X_2, y_2)->load_svmlight_file(f, n_features=n_features, offset=mark_2)
A:sklearn.datasets.tests.test_svmlight_format.y_concat->numpy.concatenate([y_0, y_1])
A:sklearn.datasets.tests.test_svmlight_format.X_concat->scipy.sparse.vstack([X_0, X_1])
A:sklearn.datasets.tests.test_svmlight_format.(X_0, y_0, q_0)->load_svmlight_file(f, n_features=n_features, query_id=True, offset=0, length=mark)
A:sklearn.datasets.tests.test_svmlight_format.(X_1, y_1, q_1)->load_svmlight_file(f, n_features=n_features, query_id=True, offset=mark, length=-1)
A:sklearn.datasets.tests.test_svmlight_format.q_concat->numpy.concatenate([q_0, q_1])
sklearn.datasets.tests.test_svmlight_format.test_dump()
sklearn.datasets.tests.test_svmlight_format.test_dump_comment()
sklearn.datasets.tests.test_svmlight_format.test_dump_concise()
sklearn.datasets.tests.test_svmlight_format.test_dump_invalid()
sklearn.datasets.tests.test_svmlight_format.test_dump_multilabel()
sklearn.datasets.tests.test_svmlight_format.test_dump_query_id()
sklearn.datasets.tests.test_svmlight_format.test_invalid_filename()
sklearn.datasets.tests.test_svmlight_format.test_load_compressed()
sklearn.datasets.tests.test_svmlight_format.test_load_invalid_file()
sklearn.datasets.tests.test_svmlight_format.test_load_invalid_file2()
sklearn.datasets.tests.test_svmlight_format.test_load_invalid_order_file()
sklearn.datasets.tests.test_svmlight_format.test_load_offset_exhaustive_splits()
sklearn.datasets.tests.test_svmlight_format.test_load_svmlight_file()
sklearn.datasets.tests.test_svmlight_format.test_load_svmlight_file_fd()
sklearn.datasets.tests.test_svmlight_format.test_load_svmlight_file_multilabel()
sklearn.datasets.tests.test_svmlight_format.test_load_svmlight_file_n_features()
sklearn.datasets.tests.test_svmlight_format.test_load_svmlight_files()
sklearn.datasets.tests.test_svmlight_format.test_load_with_long_qid()
sklearn.datasets.tests.test_svmlight_format.test_load_with_offsets()
sklearn.datasets.tests.test_svmlight_format.test_load_with_offsets_error()
sklearn.datasets.tests.test_svmlight_format.test_load_with_qid()
sklearn.datasets.tests.test_svmlight_format.test_load_zero_based()
sklearn.datasets.tests.test_svmlight_format.test_load_zero_based_auto()
sklearn.datasets.tests.test_svmlight_format.test_load_zeros()
sklearn.datasets.tests.test_svmlight_format.test_not_a_filename()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/datasets/tests/test_kddcup99.py----------------------------------------
A:sklearn.datasets.tests.test_kddcup99.data->fetch_kddcup99('smtp')
A:sklearn.datasets.tests.test_kddcup99.data_shuffled->fetch_kddcup99(shuffle=True, random_state=0)
sklearn.datasets.tests.test_kddcup99.test_percent10()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/datasets/tests/test_covtype.py----------------------------------------
A:sklearn.datasets.tests.test_covtype.data1->fetch(shuffle=True, random_state=42)
A:sklearn.datasets.tests.test_covtype.data2->fetch(shuffle=True, random_state=37)
sklearn.datasets.tests.test_covtype.fetch(*args,**kwargs)
sklearn.datasets.tests.test_covtype.test_fetch()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/gaussian_process/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py----------------------------------------
A:sklearn.gaussian_process.gpr.self.kernel_->clone(self.kernel)
A:sklearn.gaussian_process.gpr.self._rng->check_random_state(self.random_state)
A:sklearn.gaussian_process.gpr.(X, y)->check_X_y(X, y, multi_output=True, y_numeric=True)
A:sklearn.gaussian_process.gpr.self._y_train_mean->numpy.zeros(1)
A:sklearn.gaussian_process.gpr.(lml, grad)->self.log_marginal_likelihood(theta, eval_gradient=True)
A:sklearn.gaussian_process.gpr.theta_initial->self._rng.uniform(bounds[:, 0], bounds[:, 1])
A:sklearn.gaussian_process.gpr.lml_values->list(map(itemgetter(1), optima))
A:sklearn.gaussian_process.gpr.self.log_marginal_likelihood_value_->self.log_marginal_likelihood(self.kernel_.theta)
A:sklearn.gaussian_process.gpr.K->kernel(self.X_train_)
A:sklearn.gaussian_process.gpr.self.L_->cholesky(K, lower=True)
A:sklearn.gaussian_process.gpr.self.alpha_->cho_solve((self.L_, True), self.y_train_)
A:sklearn.gaussian_process.gpr.X->check_array(X)
A:sklearn.gaussian_process.gpr.y_mean->self.kernel_(X, self.X_train_).dot(self.alpha_)
A:sklearn.gaussian_process.gpr.y_cov->kernel(X)
A:sklearn.gaussian_process.gpr.y_var->self.kernel_.diag(X)
A:sklearn.gaussian_process.gpr.K_trans->self.kernel_(X, self.X_train_)
A:sklearn.gaussian_process.gpr.v->cho_solve((self.L_, True), K_trans.T)
A:sklearn.gaussian_process.gpr.L_inv->solve_triangular(self.L_.T, np.eye(self.L_.shape[0]))
A:sklearn.gaussian_process.gpr.K_inv->solve_triangular(self.L_.T, np.eye(self.L_.shape[0])).dot(L_inv.T)
A:sklearn.gaussian_process.gpr.rng->check_random_state(random_state)
A:sklearn.gaussian_process.gpr.(y_mean, y_cov)->self.predict(X, return_cov=True)
A:sklearn.gaussian_process.gpr.y_samples->numpy.hstack(y_samples)
A:sklearn.gaussian_process.gpr.kernel->self.kernel_.clone_with_theta(theta)
A:sklearn.gaussian_process.gpr.(K, K_gradient)->kernel(self.X_train_, eval_gradient=True)
A:sklearn.gaussian_process.gpr.L->cholesky(K, lower=True)
A:sklearn.gaussian_process.gpr.alpha->cho_solve((L, True), y_train)
A:sklearn.gaussian_process.gpr.log_likelihood->log_likelihood_dims.sum(-1)
A:sklearn.gaussian_process.gpr.tmp->numpy.einsum('ik,jk->ijk', alpha, alpha)
A:sklearn.gaussian_process.gpr.log_likelihood_gradient->log_likelihood_gradient_dims.sum(-1)
A:sklearn.gaussian_process.gpr.(theta_opt, func_min, convergence_dict)->fmin_l_bfgs_b(obj_func, initial_theta, bounds=bounds)
A:sklearn.gaussian_process.gpr.(theta_opt, func_min)->self.optimizer(obj_func, initial_theta, bounds=bounds)
sklearn.gaussian_process.GaussianProcessRegressor(self,kernel=None,alpha=1e-10,optimizer='fmin_l_bfgs_b',n_restarts_optimizer=0,normalize_y=False,copy_X_train=True,random_state=None)
sklearn.gaussian_process.GaussianProcessRegressor._constrained_optimization(self,obj_func,initial_theta,bounds)
sklearn.gaussian_process.GaussianProcessRegressor.fit(self,X,y)
sklearn.gaussian_process.GaussianProcessRegressor.log_marginal_likelihood(self,theta=None,eval_gradient=False)
sklearn.gaussian_process.GaussianProcessRegressor.predict(self,X,return_std=False,return_cov=False)
sklearn.gaussian_process.GaussianProcessRegressor.rng(self)
sklearn.gaussian_process.GaussianProcessRegressor.sample_y(self,X,n_samples=1,random_state=0)
sklearn.gaussian_process.GaussianProcessRegressor.y_train_mean(self)
sklearn.gaussian_process.gpr.GaussianProcessRegressor(self,kernel=None,alpha=1e-10,optimizer='fmin_l_bfgs_b',n_restarts_optimizer=0,normalize_y=False,copy_X_train=True,random_state=None)
sklearn.gaussian_process.gpr.GaussianProcessRegressor.__init__(self,kernel=None,alpha=1e-10,optimizer='fmin_l_bfgs_b',n_restarts_optimizer=0,normalize_y=False,copy_X_train=True,random_state=None)
sklearn.gaussian_process.gpr.GaussianProcessRegressor._constrained_optimization(self,obj_func,initial_theta,bounds)
sklearn.gaussian_process.gpr.GaussianProcessRegressor.fit(self,X,y)
sklearn.gaussian_process.gpr.GaussianProcessRegressor.log_marginal_likelihood(self,theta=None,eval_gradient=False)
sklearn.gaussian_process.gpr.GaussianProcessRegressor.predict(self,X,return_std=False,return_cov=False)
sklearn.gaussian_process.gpr.GaussianProcessRegressor.rng(self)
sklearn.gaussian_process.gpr.GaussianProcessRegressor.sample_y(self,X,n_samples=1,random_state=0)
sklearn.gaussian_process.gpr.GaussianProcessRegressor.y_train_mean(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/gaussian_process/gaussian_process.py----------------------------------------
A:sklearn.gaussian_process.gaussian_process.X->check_array(X)
A:sklearn.gaussian_process.gaussian_process.ij->numpy.zeros((n_nonzero_cross_dist, 2), dtype=np.int)
A:sklearn.gaussian_process.gaussian_process.D->numpy.zeros((n_nonzero_cross_dist, n_features))
A:sklearn.gaussian_process.gaussian_process.ij[ll_0:ll_1, 1]->numpy.arange(k + 1, n_samples)
A:sklearn.gaussian_process.gaussian_process.D[ll_0:ll_1]->numpy.abs(X[k] - X[k + 1:n_samples])
A:sklearn.gaussian_process.gaussian_process.self.random_state->check_random_state(self.random_state)
A:sklearn.gaussian_process.gaussian_process.(X, y)->check_X_y(X, y, multi_output=True, y_numeric=True)
A:sklearn.gaussian_process.gaussian_process.X_mean->numpy.zeros(1)
A:sklearn.gaussian_process.gaussian_process.X_std->numpy.ones(1)
A:sklearn.gaussian_process.gaussian_process.y_mean->numpy.zeros(1)
A:sklearn.gaussian_process.gaussian_process.y_std->numpy.ones(1)
A:sklearn.gaussian_process.gaussian_process.(D, ij)->l1_cross_distances(self.X)
A:sklearn.gaussian_process.gaussian_process.F->self.regr(self.X)
A:sklearn.gaussian_process.gaussian_process.(self.theta_, self.reduced_likelihood_function_value_, par)->self._arg_max_reduced_likelihood_function()
A:sklearn.gaussian_process.gaussian_process.(self.reduced_likelihood_function_value_, par)->self.reduced_likelihood_function()
A:sklearn.gaussian_process.gaussian_process.y->numpy.zeros(n_eval)
A:sklearn.gaussian_process.gaussian_process.MSE->MSE.ravel().ravel()
A:sklearn.gaussian_process.gaussian_process.dx->manhattan_distances(X, Y=self.X, sum_over_features=False)
A:sklearn.gaussian_process.gaussian_process.f->self.regr(X)
A:sklearn.gaussian_process.gaussian_process.r->self.corr(theta, D)
A:sklearn.gaussian_process.gaussian_process.(reduced_likelihood_function_value, par)->self.reduced_likelihood_function()
A:sklearn.gaussian_process.gaussian_process.rt->scipy.linalg.solve_triangular(self.C, r.T, lower=True)
A:sklearn.gaussian_process.gaussian_process.u->numpy.zeros((n_targets, n_eval))
A:sklearn.gaussian_process.gaussian_process.batch_to->min([(k + 1) * batch_size + 1, n_eval + 1])
A:sklearn.gaussian_process.gaussian_process.(y[batch_from:batch_to], MSE[batch_from:batch_to])->self.predict(X[batch_from:batch_to], eval_MSE=eval_MSE, batch_size=None)
A:sklearn.gaussian_process.gaussian_process.y[batch_from:batch_to]->self.predict(X[batch_from:batch_to], eval_MSE=eval_MSE, batch_size=None)
A:sklearn.gaussian_process.gaussian_process.C->scipy.linalg.cholesky(R, lower=True)
A:sklearn.gaussian_process.gaussian_process.Ft->scipy.linalg.solve_triangular(C, F, lower=True)
A:sklearn.gaussian_process.gaussian_process.(Q, G)->scipy.linalg.qr(Ft, mode='economic')
A:sklearn.gaussian_process.gaussian_process.sv->scipy.linalg.svd(F, compute_uv=False)
A:sklearn.gaussian_process.gaussian_process.Yt->scipy.linalg.solve_triangular(C, self.y, lower=True)
A:sklearn.gaussian_process.gaussian_process.beta->numpy.array(self.beta0)
A:sklearn.gaussian_process.gaussian_process.detR->(np.diag(C) ** (2.0 / n_samples)).prod()
A:sklearn.gaussian_process.gaussian_process.par['gamma']->scipy.linalg.solve_triangular(C.T, rho)
A:sklearn.gaussian_process.gaussian_process.log10_optimal_theta->scipy.optimize.fmin_cobyla(minus_reduced_likelihood_function, np.log10(theta0).ravel(), constraints, disp=0)
A:sklearn.gaussian_process.gaussian_process.(optimal_rlf_value, optimal_par)->self.reduced_likelihood_function(theta=optimal_theta)
A:sklearn.gaussian_process.gaussian_process.self.theta0->numpy.atleast_2d(self.theta0)
A:sklearn.gaussian_process.gaussian_process.self.thetaL->numpy.atleast_2d(self.thetaL)
A:sklearn.gaussian_process.gaussian_process.self.thetaU->numpy.atleast_2d(self.thetaU)
A:sklearn.gaussian_process.gaussian_process.(theta_iso, optimal_rlf_value_iso, par_iso)->self._arg_max_reduced_likelihood_function()
A:sklearn.gaussian_process.gaussian_process.(optimal_theta[0, i], optimal_rlf_value, optimal_par)->self._arg_max_reduced_likelihood_function()
A:sklearn.gaussian_process.gaussian_process.self.beta0->numpy.atleast_2d(self.beta0)
A:sklearn.gaussian_process.gaussian_process.self.verbose->bool(self.verbose)
A:sklearn.gaussian_process.gaussian_process.self.normalize->bool(self.normalize)
A:sklearn.gaussian_process.gaussian_process.self.nugget->numpy.asarray(self.nugget)
A:sklearn.gaussian_process.gaussian_process.self.random_start->int(self.random_start)
sklearn.gaussian_process.GaussianProcess(self,regr='constant',corr='squared_exponential',beta0=None,storage_mode='full',verbose=False,theta0=0.1,thetaL=None,thetaU=None,optimizer='fmin_cobyla',random_start=1,normalize=True,nugget=10.0*MACHINE_EPSILON,random_state=None)
sklearn.gaussian_process.GaussianProcess._arg_max_reduced_likelihood_function(self)
sklearn.gaussian_process.GaussianProcess._check_params(self,n_samples=None)
sklearn.gaussian_process.GaussianProcess.fit(self,X,y)
sklearn.gaussian_process.GaussianProcess.predict(self,X,eval_MSE=False,batch_size=None)
sklearn.gaussian_process.GaussianProcess.reduced_likelihood_function(self,theta=None)
sklearn.gaussian_process.gaussian_process.GaussianProcess(self,regr='constant',corr='squared_exponential',beta0=None,storage_mode='full',verbose=False,theta0=0.1,thetaL=None,thetaU=None,optimizer='fmin_cobyla',random_start=1,normalize=True,nugget=10.0*MACHINE_EPSILON,random_state=None)
sklearn.gaussian_process.gaussian_process.GaussianProcess.__init__(self,regr='constant',corr='squared_exponential',beta0=None,storage_mode='full',verbose=False,theta0=0.1,thetaL=None,thetaU=None,optimizer='fmin_cobyla',random_start=1,normalize=True,nugget=10.0*MACHINE_EPSILON,random_state=None)
sklearn.gaussian_process.gaussian_process.GaussianProcess._arg_max_reduced_likelihood_function(self)
sklearn.gaussian_process.gaussian_process.GaussianProcess._check_params(self,n_samples=None)
sklearn.gaussian_process.gaussian_process.GaussianProcess.fit(self,X,y)
sklearn.gaussian_process.gaussian_process.GaussianProcess.predict(self,X,eval_MSE=False,batch_size=None)
sklearn.gaussian_process.gaussian_process.GaussianProcess.reduced_likelihood_function(self,theta=None)
sklearn.gaussian_process.gaussian_process.l1_cross_distances(X)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/gaussian_process/kernels.py----------------------------------------
A:sklearn.gaussian_process.kernels.length_scale->_check_length_scale(X, self.length_scale)
A:sklearn.gaussian_process.kernels.bounds->numpy.repeat(bounds, n_elements, 0)
A:sklearn.gaussian_process.kernels.params->dict(kernel=self.kernel, exponent=self.exponent)
A:sklearn.gaussian_process.kernels.init->getattr(cls.__init__, 'deprecated_original', cls.__init__)
A:sklearn.gaussian_process.kernels.init_sign->signature(init)
A:sklearn.gaussian_process.kernels.params[arg]->getattr(self, arg, None)
A:sklearn.gaussian_process.kernels.valid_params->self.get_params(deep=True)
A:sklearn.gaussian_process.kernels.split->key.split('__', 1)
A:sklearn.gaussian_process.kernels.cloned->clone(self)
A:sklearn.gaussian_process.kernels.params[hyperparameter.name]->numpy.exp(theta[i])
A:sklearn.gaussian_process.kernels.params_a->self.get_params()
A:sklearn.gaussian_process.kernels.params_b->b.get_params()
A:sklearn.gaussian_process.kernels.(K_single, K_grad_single)->kernel(X, Y, eval_gradient)
A:sklearn.gaussian_process.kernels.deep_items->self.kernel.get_params().items()
A:sklearn.gaussian_process.kernels.(K1, K1_gradient)->self.k1(X, Y, eval_gradient=True)
A:sklearn.gaussian_process.kernels.(K2, K2_gradient)->self.k2(X, Y, eval_gradient=True)
A:sklearn.gaussian_process.kernels.(K, K_gradient)->self.kernel(X, Y, eval_gradient=True)
A:sklearn.gaussian_process.kernels.K->pairwise_kernels(X, Y, metric=self.metric, gamma=self.gamma, filter_params=True, **pairwise_kernels_kwargs)
A:sklearn.gaussian_process.kernels.X->numpy.atleast_2d(X)
A:sklearn.gaussian_process.kernels.dists->cdist(X, Y, metric='euclidean')
A:sklearn.gaussian_process.kernels.K_gradient->numpy.empty((K.shape[0], K.shape[1], 1))
A:sklearn.gaussian_process.kernels.length_scale_gradient->numpy.empty((K.shape[0], K.shape[1], 0))
A:sklearn.gaussian_process.kernels.alpha_gradient->numpy.empty((K.shape[0], K.shape[1], 0))
A:sklearn.gaussian_process.kernels.sin_of_arg->numpy.sin(arg)
A:sklearn.gaussian_process.kernels.cos_of_arg->numpy.cos(arg)
A:sklearn.gaussian_process.kernels.periodicity_gradient->numpy.empty((K.shape[0], K.shape[1], 0))
A:sklearn.gaussian_process.kernels.f0->f(*(xk,) + args)
A:sklearn.gaussian_process.kernels.grad->numpy.zeros((f0.shape[0], f0.shape[1], len(xk)), float)
A:sklearn.gaussian_process.kernels.ei->numpy.zeros((len(xk),), float)
sklearn.gaussian_process.kernels.CompoundKernel(self,kernels)
sklearn.gaussian_process.kernels.CompoundKernel.__eq__(self,b)
sklearn.gaussian_process.kernels.CompoundKernel.__init__(self,kernels)
sklearn.gaussian_process.kernels.CompoundKernel.bounds(self)
sklearn.gaussian_process.kernels.CompoundKernel.diag(self,X)
sklearn.gaussian_process.kernels.CompoundKernel.get_params(self,deep=True)
sklearn.gaussian_process.kernels.CompoundKernel.is_stationary(self)
sklearn.gaussian_process.kernels.CompoundKernel.theta(self)
sklearn.gaussian_process.kernels.CompoundKernel.theta(self,theta)
sklearn.gaussian_process.kernels.ConstantKernel(self,constant_value=1.0,constant_value_bounds=(1e-05,100000.0))
sklearn.gaussian_process.kernels.ConstantKernel.__init__(self,constant_value=1.0,constant_value_bounds=(1e-05,100000.0))
sklearn.gaussian_process.kernels.ConstantKernel.__repr__(self)
sklearn.gaussian_process.kernels.ConstantKernel.diag(self,X)
sklearn.gaussian_process.kernels.ConstantKernel.hyperparameter_constant_value(self)
sklearn.gaussian_process.kernels.DotProduct(self,sigma_0=1.0,sigma_0_bounds=(1e-05,100000.0))
sklearn.gaussian_process.kernels.DotProduct.__init__(self,sigma_0=1.0,sigma_0_bounds=(1e-05,100000.0))
sklearn.gaussian_process.kernels.DotProduct.__repr__(self)
sklearn.gaussian_process.kernels.DotProduct.diag(self,X)
sklearn.gaussian_process.kernels.DotProduct.hyperparameter_sigma_0(self)
sklearn.gaussian_process.kernels.DotProduct.is_stationary(self)
sklearn.gaussian_process.kernels.ExpSineSquared(self,length_scale=1.0,periodicity=1.0,length_scale_bounds=(1e-05,100000.0),periodicity_bounds=(1e-05,100000.0))
sklearn.gaussian_process.kernels.ExpSineSquared.__init__(self,length_scale=1.0,periodicity=1.0,length_scale_bounds=(1e-05,100000.0),periodicity_bounds=(1e-05,100000.0))
sklearn.gaussian_process.kernels.ExpSineSquared.__repr__(self)
sklearn.gaussian_process.kernels.ExpSineSquared.hyperparameter_length_scale(self)
sklearn.gaussian_process.kernels.ExpSineSquared.hyperparameter_periodicity(self)
sklearn.gaussian_process.kernels.Exponentiation(self,kernel,exponent)
sklearn.gaussian_process.kernels.Exponentiation.__eq__(self,b)
sklearn.gaussian_process.kernels.Exponentiation.__init__(self,kernel,exponent)
sklearn.gaussian_process.kernels.Exponentiation.__repr__(self)
sklearn.gaussian_process.kernels.Exponentiation.bounds(self)
sklearn.gaussian_process.kernels.Exponentiation.diag(self,X)
sklearn.gaussian_process.kernels.Exponentiation.get_params(self,deep=True)
sklearn.gaussian_process.kernels.Exponentiation.hyperparameters(self)
sklearn.gaussian_process.kernels.Exponentiation.is_stationary(self)
sklearn.gaussian_process.kernels.Exponentiation.theta(self)
sklearn.gaussian_process.kernels.Exponentiation.theta(self,theta)
sklearn.gaussian_process.kernels.Hyperparameter(cls,name,value_type,bounds,n_elements=1,fixed=None)
sklearn.gaussian_process.kernels.Hyperparameter.__eq__(self,other)
sklearn.gaussian_process.kernels.Hyperparameter.__new__(cls,name,value_type,bounds,n_elements=1,fixed=None)
sklearn.gaussian_process.kernels.Kernel(self,X,Y=None,eval_gradient=False)
sklearn.gaussian_process.kernels.Kernel.__add__(self,b)
sklearn.gaussian_process.kernels.Kernel.__call__(self,X,Y=None,eval_gradient=False)
sklearn.gaussian_process.kernels.Kernel.__eq__(self,b)
sklearn.gaussian_process.kernels.Kernel.__mul__(self,b)
sklearn.gaussian_process.kernels.Kernel.__pow__(self,b)
sklearn.gaussian_process.kernels.Kernel.__radd__(self,b)
sklearn.gaussian_process.kernels.Kernel.__repr__(self)
sklearn.gaussian_process.kernels.Kernel.__rmul__(self,b)
sklearn.gaussian_process.kernels.Kernel.bounds(self)
sklearn.gaussian_process.kernels.Kernel.clone_with_theta(self,theta)
sklearn.gaussian_process.kernels.Kernel.diag(self,X)
sklearn.gaussian_process.kernels.Kernel.get_params(self,deep=True)
sklearn.gaussian_process.kernels.Kernel.hyperparameters(self)
sklearn.gaussian_process.kernels.Kernel.is_stationary(self)
sklearn.gaussian_process.kernels.Kernel.n_dims(self)
sklearn.gaussian_process.kernels.Kernel.set_params(self,**params)
sklearn.gaussian_process.kernels.Kernel.theta(self)
sklearn.gaussian_process.kernels.Kernel.theta(self,theta)
sklearn.gaussian_process.kernels.KernelOperator(self,k1,k2)
sklearn.gaussian_process.kernels.KernelOperator.__eq__(self,b)
sklearn.gaussian_process.kernels.KernelOperator.__init__(self,k1,k2)
sklearn.gaussian_process.kernels.KernelOperator.bounds(self)
sklearn.gaussian_process.kernels.KernelOperator.get_params(self,deep=True)
sklearn.gaussian_process.kernels.KernelOperator.hyperparameters(self)
sklearn.gaussian_process.kernels.KernelOperator.is_stationary(self)
sklearn.gaussian_process.kernels.KernelOperator.theta(self)
sklearn.gaussian_process.kernels.KernelOperator.theta(self,theta)
sklearn.gaussian_process.kernels.Matern(self,length_scale=1.0,length_scale_bounds=(1e-05,100000.0),nu=1.5)
sklearn.gaussian_process.kernels.Matern.__init__(self,length_scale=1.0,length_scale_bounds=(1e-05,100000.0),nu=1.5)
sklearn.gaussian_process.kernels.Matern.__repr__(self)
sklearn.gaussian_process.kernels.NormalizedKernelMixin(object)
sklearn.gaussian_process.kernels.NormalizedKernelMixin.diag(self,X)
sklearn.gaussian_process.kernels.PairwiseKernel(self,gamma=1.0,gamma_bounds=(1e-05,100000.0),metric='linear',pairwise_kernels_kwargs=None)
sklearn.gaussian_process.kernels.PairwiseKernel.__init__(self,gamma=1.0,gamma_bounds=(1e-05,100000.0),metric='linear',pairwise_kernels_kwargs=None)
sklearn.gaussian_process.kernels.PairwiseKernel.__repr__(self)
sklearn.gaussian_process.kernels.PairwiseKernel.diag(self,X)
sklearn.gaussian_process.kernels.PairwiseKernel.hyperparameter_gamma(self)
sklearn.gaussian_process.kernels.PairwiseKernel.is_stationary(self)
sklearn.gaussian_process.kernels.Product(self,X,Y=None,eval_gradient=False)
sklearn.gaussian_process.kernels.Product.__call__(self,X,Y=None,eval_gradient=False)
sklearn.gaussian_process.kernels.Product.__repr__(self)
sklearn.gaussian_process.kernels.Product.diag(self,X)
sklearn.gaussian_process.kernels.RBF(self,length_scale=1.0,length_scale_bounds=(1e-05,100000.0))
sklearn.gaussian_process.kernels.RBF.__init__(self,length_scale=1.0,length_scale_bounds=(1e-05,100000.0))
sklearn.gaussian_process.kernels.RBF.__repr__(self)
sklearn.gaussian_process.kernels.RBF.anisotropic(self)
sklearn.gaussian_process.kernels.RBF.hyperparameter_length_scale(self)
sklearn.gaussian_process.kernels.RationalQuadratic(self,length_scale=1.0,alpha=1.0,length_scale_bounds=(1e-05,100000.0),alpha_bounds=(1e-05,100000.0))
sklearn.gaussian_process.kernels.RationalQuadratic.__init__(self,length_scale=1.0,alpha=1.0,length_scale_bounds=(1e-05,100000.0),alpha_bounds=(1e-05,100000.0))
sklearn.gaussian_process.kernels.RationalQuadratic.__repr__(self)
sklearn.gaussian_process.kernels.RationalQuadratic.hyperparameter_alpha(self)
sklearn.gaussian_process.kernels.RationalQuadratic.hyperparameter_length_scale(self)
sklearn.gaussian_process.kernels.StationaryKernelMixin(object)
sklearn.gaussian_process.kernels.StationaryKernelMixin.is_stationary(self)
sklearn.gaussian_process.kernels.Sum(self,X,Y=None,eval_gradient=False)
sklearn.gaussian_process.kernels.Sum.__call__(self,X,Y=None,eval_gradient=False)
sklearn.gaussian_process.kernels.Sum.__repr__(self)
sklearn.gaussian_process.kernels.Sum.diag(self,X)
sklearn.gaussian_process.kernels.WhiteKernel(self,noise_level=1.0,noise_level_bounds=(1e-05,100000.0))
sklearn.gaussian_process.kernels.WhiteKernel.__init__(self,noise_level=1.0,noise_level_bounds=(1e-05,100000.0))
sklearn.gaussian_process.kernels.WhiteKernel.__repr__(self)
sklearn.gaussian_process.kernels.WhiteKernel.diag(self,X)
sklearn.gaussian_process.kernels.WhiteKernel.hyperparameter_noise_level(self)
sklearn.gaussian_process.kernels._approx_fprime(xk,f,epsilon,args=())
sklearn.gaussian_process.kernels._check_length_scale(X,length_scale)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/gaussian_process/regression_models.py----------------------------------------
A:sklearn.gaussian_process.regression_models.x->numpy.asarray(x, dtype=np.float64)
A:sklearn.gaussian_process.regression_models.f->numpy.hstack([f, x[:, k, np.newaxis] * x[:, k:]])
sklearn.gaussian_process.regression_models.constant(x)
sklearn.gaussian_process.regression_models.linear(x)
sklearn.gaussian_process.regression_models.quadratic(x)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/gaussian_process/correlation_models.py----------------------------------------
A:sklearn.gaussian_process.correlation_models.theta->numpy.asarray(theta, dtype=np.float64)
A:sklearn.gaussian_process.correlation_models.d->numpy.asarray(d, dtype=np.float64)
A:sklearn.gaussian_process.correlation_models.r->numpy.prod(ss, 1)
sklearn.gaussian_process.correlation_models.absolute_exponential(theta,d)
sklearn.gaussian_process.correlation_models.cubic(theta,d)
sklearn.gaussian_process.correlation_models.generalized_exponential(theta,d)
sklearn.gaussian_process.correlation_models.linear(theta,d)
sklearn.gaussian_process.correlation_models.pure_nugget(theta,d)
sklearn.gaussian_process.correlation_models.squared_exponential(theta,d)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/gaussian_process/gpc.py----------------------------------------
A:sklearn.gaussian_process.gpc.self.kernel_->clone(self.kernel)
A:sklearn.gaussian_process.gpc.self.rng->check_random_state(self.random_state)
A:sklearn.gaussian_process.gpc.label_encoder->LabelEncoder()
A:sklearn.gaussian_process.gpc.self.y_train_->LabelEncoder().fit_transform(y)
A:sklearn.gaussian_process.gpc.(lml, grad)->self.log_marginal_likelihood(theta, eval_gradient=True)
A:sklearn.gaussian_process.gpc.theta_initial->numpy.exp(self.rng.uniform(bounds[:, 0], bounds[:, 1]))
A:sklearn.gaussian_process.gpc.lml_values->list(map(itemgetter(1), optima))
A:sklearn.gaussian_process.gpc.self.log_marginal_likelihood_value_->self.base_estimator_.log_marginal_likelihood()
A:sklearn.gaussian_process.gpc.K->kernel(self.X_train_)
A:sklearn.gaussian_process.gpc.(_, (self.pi_, self.W_sr_, self.L_, _, _))->self._posterior_mode(K, return_temporaries=True)
A:sklearn.gaussian_process.gpc.K_star->self.kernel_(self.X_train_, X)
A:sklearn.gaussian_process.gpc.f_star->self.kernel_(self.X_train_, X).T.dot(self.y_train_ - self.pi_)
A:sklearn.gaussian_process.gpc.v->solve(self.L_, self.W_sr_[:, np.newaxis] * K_star)
A:sklearn.gaussian_process.gpc.kernel->self.kernel_.clone_with_theta(theta)
A:sklearn.gaussian_process.gpc.(K, K_gradient)->kernel(self.X_train_, eval_gradient=True)
A:sklearn.gaussian_process.gpc.(Z, (pi, W_sr, L, b, a))->self._posterior_mode(K, return_temporaries=True)
A:sklearn.gaussian_process.gpc.d_Z->numpy.empty(theta.shape[0])
A:sklearn.gaussian_process.gpc.C->solve(L, W_sr[:, np.newaxis] * K)
A:sklearn.gaussian_process.gpc.b->solve(L, W_sr[:, np.newaxis] * K).dot(self.y_train_ - pi)
A:sklearn.gaussian_process.gpc.f->kernel(self.X_train_).dot(a)
A:sklearn.gaussian_process.gpc.pi->expit(f)
A:sklearn.gaussian_process.gpc.W_sr->numpy.sqrt(W)
A:sklearn.gaussian_process.gpc.L->cholesky(B, lower=True)
A:sklearn.gaussian_process.gpc.(theta_opt, func_min, convergence_dict)->fmin_l_bfgs_b(obj_func, initial_theta, bounds=bounds)
A:sklearn.gaussian_process.gpc.(theta_opt, func_min)->self.optimizer(obj_func, initial_theta, bounds=bounds)
A:sklearn.gaussian_process.gpc.(X, y)->check_X_y(X, y, multi_output=False)
A:sklearn.gaussian_process.gpc.self.base_estimator_->OneVsOneClassifier(self.base_estimator_, n_jobs=self.n_jobs)
A:sklearn.gaussian_process.gpc.self.classes_->numpy.unique(y)
A:sklearn.gaussian_process.gpc.X->check_array(X)
A:sklearn.gaussian_process.gpc.theta->numpy.asarray(theta)
sklearn.gaussian_process.GaussianProcessClassifier(self,kernel=None,optimizer='fmin_l_bfgs_b',n_restarts_optimizer=0,max_iter_predict=100,warm_start=False,copy_X_train=True,random_state=None,multi_class='one_vs_rest',n_jobs=1)
sklearn.gaussian_process.GaussianProcessClassifier.fit(self,X,y)
sklearn.gaussian_process.GaussianProcessClassifier.kernel_(self)
sklearn.gaussian_process.GaussianProcessClassifier.log_marginal_likelihood(self,theta=None,eval_gradient=False)
sklearn.gaussian_process.GaussianProcessClassifier.predict(self,X)
sklearn.gaussian_process.GaussianProcessClassifier.predict_proba(self,X)
sklearn.gaussian_process.gpc.GaussianProcessClassifier(self,kernel=None,optimizer='fmin_l_bfgs_b',n_restarts_optimizer=0,max_iter_predict=100,warm_start=False,copy_X_train=True,random_state=None,multi_class='one_vs_rest',n_jobs=1)
sklearn.gaussian_process.gpc.GaussianProcessClassifier.__init__(self,kernel=None,optimizer='fmin_l_bfgs_b',n_restarts_optimizer=0,max_iter_predict=100,warm_start=False,copy_X_train=True,random_state=None,multi_class='one_vs_rest',n_jobs=1)
sklearn.gaussian_process.gpc.GaussianProcessClassifier.fit(self,X,y)
sklearn.gaussian_process.gpc.GaussianProcessClassifier.kernel_(self)
sklearn.gaussian_process.gpc.GaussianProcessClassifier.log_marginal_likelihood(self,theta=None,eval_gradient=False)
sklearn.gaussian_process.gpc.GaussianProcessClassifier.predict(self,X)
sklearn.gaussian_process.gpc.GaussianProcessClassifier.predict_proba(self,X)
sklearn.gaussian_process.gpc._BinaryGaussianProcessClassifierLaplace(self,kernel=None,optimizer='fmin_l_bfgs_b',n_restarts_optimizer=0,max_iter_predict=100,warm_start=False,copy_X_train=True,random_state=None)
sklearn.gaussian_process.gpc._BinaryGaussianProcessClassifierLaplace.__init__(self,kernel=None,optimizer='fmin_l_bfgs_b',n_restarts_optimizer=0,max_iter_predict=100,warm_start=False,copy_X_train=True,random_state=None)
sklearn.gaussian_process.gpc._BinaryGaussianProcessClassifierLaplace._constrained_optimization(self,obj_func,initial_theta,bounds)
sklearn.gaussian_process.gpc._BinaryGaussianProcessClassifierLaplace._posterior_mode(self,K,return_temporaries=False)
sklearn.gaussian_process.gpc._BinaryGaussianProcessClassifierLaplace.fit(self,X,y)
sklearn.gaussian_process.gpc._BinaryGaussianProcessClassifierLaplace.log_marginal_likelihood(self,theta=None,eval_gradient=False)
sklearn.gaussian_process.gpc._BinaryGaussianProcessClassifierLaplace.predict(self,X)
sklearn.gaussian_process.gpc._BinaryGaussianProcessClassifierLaplace.predict_proba(self,X)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/gaussian_process/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/gaussian_process/tests/test_gpr.py----------------------------------------
A:sklearn.gaussian_process.tests.test_gpr.y->numpy.ones(6)
A:sklearn.gaussian_process.tests.test_gpr.fixed_kernel->RBF(length_scale=1.0, length_scale_bounds='fixed')
A:sklearn.gaussian_process.tests.test_gpr.gpr->GaussianProcessRegressor(kernel=kernel, alpha=0.0)
A:sklearn.gaussian_process.tests.test_gpr.(y_pred, y_cov)->GaussianProcessRegressor(kernel=kernel, alpha=0.0).predict(X, return_cov=True)
A:sklearn.gaussian_process.tests.test_gpr.(lml, lml_gradient)->GaussianProcessRegressor(kernel=kernel, alpha=0.0).log_marginal_likelihood(kernel.theta, True)
A:sklearn.gaussian_process.tests.test_gpr.lml_gradient_approx->approx_fprime(kernel.theta, lambda theta: gpr.log_marginal_likelihood(theta, False), 1e-10)
A:sklearn.gaussian_process.tests.test_gpr.(y_mean, y_cov)->GaussianProcessRegressor(kernel=kernel, alpha=0.0).predict(X2, return_cov=True)
A:sklearn.gaussian_process.tests.test_gpr.samples->GaussianProcessRegressor(kernel=kernel, alpha=0.0).sample_y(X2, 300000)
A:sklearn.gaussian_process.tests.test_gpr.kernel->DotProduct()
A:sklearn.gaussian_process.tests.test_gpr.(y_mean, y_std)->GaussianProcessRegressor(kernel=kernel, alpha=0.0).predict(X2, return_std=True)
A:sklearn.gaussian_process.tests.test_gpr.rng->numpy.random.RandomState(0)
A:sklearn.gaussian_process.tests.test_gpr.X->numpy.arange(12).reshape(6, -1)
A:sklearn.gaussian_process.tests.test_gpr.gp->GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=n_restarts_optimizer, random_state=0).fit(X, y)
A:sklearn.gaussian_process.tests.test_gpr.lml->GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=n_restarts_optimizer, random_state=0).fit(X, y).log_marginal_likelihood(gp.kernel_.theta)
A:sklearn.gaussian_process.tests.test_gpr.y_mean->numpy.ones(6).mean(0)
A:sklearn.gaussian_process.tests.test_gpr.gpr_norm->GaussianProcessRegressor(kernel=kernel, normalize_y=True)
A:sklearn.gaussian_process.tests.test_gpr.(y_pred, y_pred_std)->GaussianProcessRegressor(kernel=kernel, alpha=0.0).predict(X2, return_std=True)
A:sklearn.gaussian_process.tests.test_gpr.(y_pred_norm, y_pred_std_norm)->GaussianProcessRegressor(kernel=kernel, normalize_y=True).predict(X2, return_std=True)
A:sklearn.gaussian_process.tests.test_gpr.(_, y_cov)->GaussianProcessRegressor(kernel=kernel, alpha=0.0).predict(X2, return_cov=True)
A:sklearn.gaussian_process.tests.test_gpr.(_, y_cov_norm)->GaussianProcessRegressor(kernel=kernel, normalize_y=True).predict(X2, return_cov=True)
A:sklearn.gaussian_process.tests.test_gpr.gpr_2d->GaussianProcessRegressor(kernel=kernel, normalize_y=True)
A:sklearn.gaussian_process.tests.test_gpr.(y_pred_1d, y_std_1d)->GaussianProcessRegressor(kernel=kernel, alpha=0.0).predict(X2, return_std=True)
A:sklearn.gaussian_process.tests.test_gpr.(y_pred_2d, y_std_2d)->GaussianProcessRegressor(kernel=kernel, normalize_y=True).predict(X2, return_std=True)
A:sklearn.gaussian_process.tests.test_gpr.(_, y_cov_1d)->GaussianProcessRegressor(kernel=kernel, alpha=0.0).predict(X2, return_cov=True)
A:sklearn.gaussian_process.tests.test_gpr.(_, y_cov_2d)->GaussianProcessRegressor(kernel=kernel, normalize_y=True).predict(X2, return_cov=True)
A:sklearn.gaussian_process.tests.test_gpr.y_sample_1d->GaussianProcessRegressor(kernel=kernel, alpha=0.0).sample_y(X2, n_samples=10)
A:sklearn.gaussian_process.tests.test_gpr.y_sample_2d->GaussianProcessRegressor(kernel=kernel, normalize_y=True).sample_y(X2, n_samples=10)
A:sklearn.gaussian_process.tests.test_gpr.theta->numpy.atleast_1d(rng.uniform(np.maximum(-2, bounds[:, 0]), np.minimum(1, bounds[:, 1])))
A:sklearn.gaussian_process.tests.test_gpr.f->obj_func(theta, eval_gradient=False)
A:sklearn.gaussian_process.tests.test_gpr.gpr_equal_inputs->GaussianProcessRegressor(kernel=kernel, alpha=0.01)
A:sklearn.gaussian_process.tests.test_gpr.gpr_similar_inputs->GaussianProcessRegressor(kernel=kernel, alpha=0.01)
A:sklearn.gaussian_process.tests.test_gpr.X_->numpy.vstack((X, X[0] + 1e-15))
A:sklearn.gaussian_process.tests.test_gpr.y_->numpy.hstack((y, y[0] + 1))
A:sklearn.gaussian_process.tests.test_gpr.(y_pred_equal, y_std_equal)->GaussianProcessRegressor(kernel=kernel, alpha=0.01).predict(X_test, return_std=True)
A:sklearn.gaussian_process.tests.test_gpr.(y_pred_similar, y_std_similar)->GaussianProcessRegressor(kernel=kernel, alpha=0.01).predict(X_test, return_std=True)
A:sklearn.gaussian_process.tests.test_gpr.gpr1->GaussianProcessRegressor()
A:sklearn.gaussian_process.tests.test_gpr.(_, y_std1)->GaussianProcessRegressor().predict(X, return_std=True)
A:sklearn.gaussian_process.tests.test_gpr.(_, y_cov1)->GaussianProcessRegressor().predict(X, return_cov=True)
A:sklearn.gaussian_process.tests.test_gpr.gpr2->GaussianProcessRegressor(kernel=default_kernel)
A:sklearn.gaussian_process.tests.test_gpr.(_, y_std2)->GaussianProcessRegressor(kernel=default_kernel).predict(X, return_std=True)
A:sklearn.gaussian_process.tests.test_gpr.(_, y_cov2)->GaussianProcessRegressor(kernel=default_kernel).predict(X, return_cov=True)
sklearn.gaussian_process.tests.test_gpr.f(x)
sklearn.gaussian_process.tests.test_gpr.test_anisotropic_kernel()
sklearn.gaussian_process.tests.test_gpr.test_converged_to_local_maximum()
sklearn.gaussian_process.tests.test_gpr.test_custom_optimizer()
sklearn.gaussian_process.tests.test_gpr.test_duplicate_input()
sklearn.gaussian_process.tests.test_gpr.test_gpr_correct_error_message()
sklearn.gaussian_process.tests.test_gpr.test_gpr_interpolation()
sklearn.gaussian_process.tests.test_gpr.test_lml_gradient()
sklearn.gaussian_process.tests.test_gpr.test_lml_improving()
sklearn.gaussian_process.tests.test_gpr.test_lml_precomputed()
sklearn.gaussian_process.tests.test_gpr.test_no_fit_default_predict()
sklearn.gaussian_process.tests.test_gpr.test_no_optimizer()
sklearn.gaussian_process.tests.test_gpr.test_predict_cov_vs_std()
sklearn.gaussian_process.tests.test_gpr.test_prior()
sklearn.gaussian_process.tests.test_gpr.test_random_starts()
sklearn.gaussian_process.tests.test_gpr.test_sample_statistics()
sklearn.gaussian_process.tests.test_gpr.test_solution_inside_bounds()
sklearn.gaussian_process.tests.test_gpr.test_y_multioutput()
sklearn.gaussian_process.tests.test_gpr.test_y_normalization()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/gaussian_process/tests/test_gaussian_process.py----------------------------------------
A:sklearn.gaussian_process.tests.test_gaussian_process.y->f(X)
A:sklearn.gaussian_process.tests.test_gaussian_process.gp->GaussianProcess(corr='absolute_exponential', theta0=0.0001, thetaL=1e-12, thetaU=0.01, nugget=0.01, optimizer='Welch', regr='linear', random_state=0)
A:sklearn.gaussian_process.tests.test_gaussian_process.(y_pred, MSE)->GaussianProcess(corr='absolute_exponential', theta0=0.0001, thetaL=1e-12, thetaU=0.01, nugget=0.01, optimizer='Welch', regr='linear', random_state=0).predict(X, eval_MSE=True)
A:sklearn.gaussian_process.tests.test_gaussian_process.(y2_pred, MSE2)->GaussianProcess(corr='absolute_exponential', theta0=0.0001, thetaL=1e-12, thetaU=0.01, nugget=0.01, optimizer='Welch', regr='linear', random_state=0).predict(X2, eval_MSE=True)
A:sklearn.gaussian_process.tests.test_gaussian_process.X->numpy.array([[-4.61611719, -6.00099547], [4.10469096, 5.32782448], [0.0, -0.5], [-6.17289014, -4.6984743], [1.3109306, -6.93271427], [-5.03823144, 3.10584743], [-2.87600388, 6.74310541], [5.21301203, 4.26386883]])
A:sklearn.gaussian_process.tests.test_gaussian_process.y_pred->GaussianProcess(corr='absolute_exponential', theta0=0.0001, thetaL=1e-12, thetaU=0.01, nugget=0.01, optimizer='Welch', regr='linear', random_state=0).predict(X)
A:sklearn.gaussian_process.tests.test_gaussian_process.rng->numpy.random.RandomState(0)
A:sklearn.gaussian_process.tests.test_gaussian_process.(X, y)->make_regression(n_informative=3, n_features=60, noise=50, random_state=0, effective_rank=1)
sklearn.gaussian_process.tests.test_gaussian_process.test_1d(regr=regression.constant,corr=correlation.squared_exponential,random_start=10,beta0=None)
sklearn.gaussian_process.tests.test_gaussian_process.test_2d(regr=regression.constant,corr=correlation.squared_exponential,random_start=10,beta0=None)
sklearn.gaussian_process.tests.test_gaussian_process.test_2d_2d(regr=regression.constant,corr=correlation.squared_exponential,random_start=10,beta0=None)
sklearn.gaussian_process.tests.test_gaussian_process.test_batch_size()
sklearn.gaussian_process.tests.test_gaussian_process.test_more_builtin_correlation_models(random_start=1)
sklearn.gaussian_process.tests.test_gaussian_process.test_mse_solving()
sklearn.gaussian_process.tests.test_gaussian_process.test_no_normalize()
sklearn.gaussian_process.tests.test_gaussian_process.test_ordinary_kriging()
sklearn.gaussian_process.tests.test_gaussian_process.test_random_starts()
sklearn.gaussian_process.tests.test_gaussian_process.test_wrong_number_of_outputs()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/gaussian_process/tests/test_gpc.py----------------------------------------
A:sklearn.gaussian_process.tests.test_gpc.y->numpy.array(f(X).ravel() > 0, dtype=int)
A:sklearn.gaussian_process.tests.test_gpc.fX->f(X).ravel()
A:sklearn.gaussian_process.tests.test_gpc.y_mc->numpy.empty(y.shape, dtype=int)
A:sklearn.gaussian_process.tests.test_gpc.fixed_kernel->RBF(length_scale=1.0, length_scale_bounds='fixed')
A:sklearn.gaussian_process.tests.test_gpc.gpc->GaussianProcessClassifier(kernel=kernel)
A:sklearn.gaussian_process.tests.test_gpc.(lml, lml_gradient)->GaussianProcessClassifier(kernel=kernel).log_marginal_likelihood(kernel.theta, True)
A:sklearn.gaussian_process.tests.test_gpc.lml_gradient_approx->approx_fprime(kernel.theta, lambda theta: gpc.log_marginal_likelihood(theta, False), 1e-10)
A:sklearn.gaussian_process.tests.test_gpc.rng->numpy.random.RandomState(0)
A:sklearn.gaussian_process.tests.test_gpc.gp->GaussianProcessClassifier(kernel=kernel, n_restarts_optimizer=n_restarts_optimizer, random_state=0).fit(X, y)
A:sklearn.gaussian_process.tests.test_gpc.lml->GaussianProcessClassifier(kernel=kernel, n_restarts_optimizer=n_restarts_optimizer, random_state=0).fit(X, y).log_marginal_likelihood(gp.kernel_.theta)
A:sklearn.gaussian_process.tests.test_gpc.theta->numpy.atleast_1d(rng.uniform(np.maximum(-2, bounds[:, 0]), np.minimum(1, bounds[:, 1])))
A:sklearn.gaussian_process.tests.test_gpc.f->obj_func(theta, eval_gradient=False)
A:sklearn.gaussian_process.tests.test_gpc.y_prob->GaussianProcessClassifier(kernel=kernel).predict_proba(X2)
A:sklearn.gaussian_process.tests.test_gpc.y_pred->GaussianProcessClassifier(kernel=kernel).predict(X2)
A:sklearn.gaussian_process.tests.test_gpc.gpc_2->GaussianProcessClassifier(kernel=kernel, n_jobs=2)
A:sklearn.gaussian_process.tests.test_gpc.y_prob_2->GaussianProcessClassifier(kernel=kernel, n_jobs=2).predict_proba(X2)
sklearn.gaussian_process.tests.test_gpc.f(x)
sklearn.gaussian_process.tests.test_gpc.test_converged_to_local_maximum()
sklearn.gaussian_process.tests.test_gpc.test_custom_optimizer()
sklearn.gaussian_process.tests.test_gpc.test_lml_gradient()
sklearn.gaussian_process.tests.test_gpc.test_lml_improving()
sklearn.gaussian_process.tests.test_gpc.test_lml_precomputed()
sklearn.gaussian_process.tests.test_gpc.test_multi_class()
sklearn.gaussian_process.tests.test_gpc.test_multi_class_n_jobs()
sklearn.gaussian_process.tests.test_gpc.test_predict_consistent()
sklearn.gaussian_process.tests.test_gpc.test_random_starts()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/gaussian_process/tests/test_kernels.py----------------------------------------
A:sklearn.gaussian_process.tests.test_kernels.X->numpy.random.RandomState(0).normal(0, 1, (5, 2))
A:sklearn.gaussian_process.tests.test_kernels.Y->numpy.random.RandomState(0).normal(0, 1, (6, 2))
A:sklearn.gaussian_process.tests.test_kernels.(K, K_gradient)->kernel(X, eval_gradient=True)
A:sklearn.gaussian_process.tests.test_kernels.kernel_clone->kernel.clone_with_theta(theta)
A:sklearn.gaussian_process.tests.test_kernels.K->Matern(nu=0.5, length_scale=1.0)(X)
A:sklearn.gaussian_process.tests.test_kernels.K_gradient_approx->_approx_fprime(kernel.theta, eval_kernel_for_theta, 1e-10)
A:sklearn.gaussian_process.tests.test_kernels.(_, K_gradient)->kernel(X, eval_gradient=True)
A:sklearn.gaussian_process.tests.test_kernels.init_sign->signature(kernel.__class__.__init__).parameters.values()
A:sklearn.gaussian_process.tests.test_kernels.theta_vars->map(lambda s: s[0:-len('_bounds')], filter(lambda s: s.endswith('_bounds'), args))
A:sklearn.gaussian_process.tests.test_kernels.params->kernel.get_params()
A:sklearn.gaussian_process.tests.test_kernels.new_kernel->kernel_class(**params)
A:sklearn.gaussian_process.tests.test_kernels.(_, K_gradient_new)->new_kernel(X, eval_gradient=True)
A:sklearn.gaussian_process.tests.test_kernels.theta[i]->numpy.log(42)
A:sklearn.gaussian_process.tests.test_kernels.K_auto->kernel(X)
A:sklearn.gaussian_process.tests.test_kernels.K_cross->kernel(X, X)
A:sklearn.gaussian_process.tests.test_kernels.K_call_diag->numpy.diag(kernel(X))
A:sklearn.gaussian_process.tests.test_kernels.K_diag->kernel.diag(X)
A:sklearn.gaussian_process.tests.test_kernels.X1->numpy.array(X)
A:sklearn.gaussian_process.tests.test_kernels.X2->numpy.array(X)
A:sklearn.gaussian_process.tests.test_kernels.attr_value1->getattr(kernel1, attr)
A:sklearn.gaussian_process.tests.test_kernels.attr_value2->getattr(kernel2, attr)
A:sklearn.gaussian_process.tests.test_kernels.kernel_cloned->clone(kernel)
A:sklearn.gaussian_process.tests.test_kernels.kernel_cloned_clone->clone(kernel_cloned)
A:sklearn.gaussian_process.tests.test_kernels.K_absexp->numpy.exp(-euclidean_distances(X, X, squared=False))
A:sklearn.gaussian_process.tests.test_kernels.K1->kernel(X, Y)
A:sklearn.gaussian_process.tests.test_kernels.K2->pairwise_kernels(X, Y, metric=kernel)
sklearn.gaussian_process.tests.test_kernels.check_hyperparameters_equal(kernel1,kernel2)
sklearn.gaussian_process.tests.test_kernels.test_auto_vs_cross()
sklearn.gaussian_process.tests.test_kernels.test_kernel_anisotropic()
sklearn.gaussian_process.tests.test_kernels.test_kernel_clone()
sklearn.gaussian_process.tests.test_kernels.test_kernel_clone_after_set_params()
sklearn.gaussian_process.tests.test_kernels.test_kernel_diag()
sklearn.gaussian_process.tests.test_kernels.test_kernel_gradient()
sklearn.gaussian_process.tests.test_kernels.test_kernel_operator_commutative()
sklearn.gaussian_process.tests.test_kernels.test_kernel_stationary()
sklearn.gaussian_process.tests.test_kernels.test_kernel_theta()
sklearn.gaussian_process.tests.test_kernels.test_kernel_versus_pairwise()
sklearn.gaussian_process.tests.test_kernels.test_matern_kernel()
sklearn.gaussian_process.tests.test_kernels.test_repr_kernels()
sklearn.gaussian_process.tests.test_kernels.test_set_get_params()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/cluster/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/cluster/k_means_.py----------------------------------------
A:sklearn.cluster.k_means_.centers->centers.toarray().toarray()
A:sklearn.cluster.k_means_.center_id->check_random_state(self.random_state).randint(n_samples)
A:sklearn.cluster.k_means_.centers[0]->X[center_id].toarray()
A:sklearn.cluster.k_means_.closest_dist_sq->euclidean_distances(centers[0, np.newaxis], X, Y_norm_squared=x_squared_norms, squared=True)
A:sklearn.cluster.k_means_.current_pot->euclidean_distances(centers[0, np.newaxis], X, Y_norm_squared=x_squared_norms, squared=True).sum()
A:sklearn.cluster.k_means_.candidate_ids->numpy.searchsorted(stable_cumsum(closest_dist_sq), rand_vals)
A:sklearn.cluster.k_means_.distance_to_candidates->euclidean_distances(X[candidate_ids], X, Y_norm_squared=x_squared_norms, squared=True)
A:sklearn.cluster.k_means_.new_dist_sq->numpy.minimum(closest_dist_sq, distance_to_candidates[trial])
A:sklearn.cluster.k_means_.new_pot->numpy.minimum(closest_dist_sq, distance_to_candidates[trial]).sum()
A:sklearn.cluster.k_means_.centers[c]->X[best_candidate].toarray()
A:sklearn.cluster.k_means_.variances->numpy.var(X, axis=0)
A:sklearn.cluster.k_means_.random_state->check_random_state(self.random_state)
A:sklearn.cluster.k_means_.X->self._check_test_data(X)
A:sklearn.cluster.k_means_.tol->_tolerance(X, self.tol)
A:sklearn.cluster.k_means_.init->check_array(init, dtype=X.dtype.type, copy=True)
A:sklearn.cluster.k_means_.X_mean->self._check_test_data(X).mean(axis=0)
A:sklearn.cluster.k_means_.x_squared_norms->row_norms(X, squared=True)
A:sklearn.cluster.k_means_.(labels, inertia, centers, n_iter_)->kmeans_single(X, n_clusters, max_iter=max_iter, init=init, verbose=verbose, precompute_distances=precompute_distances, tol=tol, x_squared_norms=x_squared_norms, random_state=random_state)
A:sklearn.cluster.k_means_.best_labels->labels.astype(np.int32).copy()
A:sklearn.cluster.k_means_.best_centers->centers.toarray().toarray().copy()
A:sklearn.cluster.k_means_.seeds->check_random_state(self.random_state).randint(np.iinfo(np.int32).max, size=n_init)
A:sklearn.cluster.k_means_.results->Parallel(n_jobs=n_jobs, verbose=0)((delayed(kmeans_single)(X, n_clusters, max_iter=max_iter, init=init, verbose=verbose, tol=tol, precompute_distances=precompute_distances, x_squared_norms=x_squared_norms, random_state=seed) for seed in seeds))
A:sklearn.cluster.k_means_.(labels, inertia, centers, n_iters)->zip(*results)
A:sklearn.cluster.k_means_.best->numpy.argmin(inertia)
A:sklearn.cluster.k_means_.(centers, labels, n_iter)->k_means_elkan(X, n_clusters, centers, tol=tol, max_iter=max_iter, verbose=verbose)
A:sklearn.cluster.k_means_.inertia->_k_means._assign_labels_array(X, x_squared_norms, centers, labels, distances=distances)
A:sklearn.cluster.k_means_.distances->numpy.zeros(X.shape[0], dtype=X.dtype)
A:sklearn.cluster.k_means_.centers_old->centers.toarray().toarray().copy()
A:sklearn.cluster.k_means_.(labels, inertia)->zip(*results)
A:sklearn.cluster.k_means_.center_shift_total->squared_norm(centers_old - centers)
A:sklearn.cluster.k_means_.(best_labels, best_inertia)->_labels_inertia(X, x_squared_norms, best_centers, precompute_distances=precompute_distances, distances=distances)
A:sklearn.cluster.k_means_.(labels, mindist)->pairwise_distances_argmin_min(X=X, Y=centers, metric='euclidean', metric_kwargs={'squared': True})
A:sklearn.cluster.k_means_.labels->labels.astype(np.int32).astype(np.int32)
A:sklearn.cluster.k_means_.init_indices->check_random_state(self.random_state).randint(0, n_samples, init_size)
A:sklearn.cluster.k_means_.(self.cluster_centers_, self.labels_, self.inertia_, self.n_iter_)->k_means(X, n_clusters=self.n_clusters, init=self.init, n_init=self.n_init, max_iter=self.max_iter, verbose=self.verbose, precompute_distances=self.precompute_distances, tol=self.tol, random_state=random_state, copy_x=self.copy_x, n_jobs=self.n_jobs, algorithm=self.algorithm, return_n_iter=True)
A:sklearn.cluster.k_means_.(nearest_center, inertia)->_labels_inertia(X, x_squared_norms, centers, distances=distances)
A:sklearn.cluster.k_means_.n_reassigns->to_reassign.sum()
A:sklearn.cluster.k_means_.new_centers->check_random_state(self.random_state).choice(X.shape[0], replace=False, size=n_reassigns)
A:sklearn.cluster.k_means_.counts[to_reassign]->numpy.min(counts[~to_reassign])
A:sklearn.cluster.k_means_.count->center_mask.sum()
A:sklearn.cluster.k_means_.ewa_diff->context.get('ewa_diff')
A:sklearn.cluster.k_means_.ewa_inertia->context.get('ewa_inertia')
A:sklearn.cluster.k_means_.ewa_inertia_min->context.get('ewa_inertia_min')
A:sklearn.cluster.k_means_.no_improvement->context.get('no_improvement', 0)
A:sklearn.cluster.k_means_.self.init->numpy.ascontiguousarray(self.init, dtype=X.dtype)
A:sklearn.cluster.k_means_.old_center_buffer->numpy.zeros(0, dtype=X.dtype)
A:sklearn.cluster.k_means_.n_batches->int(np.ceil(float(n_samples) / self.batch_size))
A:sklearn.cluster.k_means_.n_iter->int(self.max_iter * n_batches)
A:sklearn.cluster.k_means_.validation_indices->check_random_state(self.random_state).randint(0, n_samples, init_size)
A:sklearn.cluster.k_means_.counts->numpy.zeros(self.n_clusters, dtype=np.int32)
A:sklearn.cluster.k_means_.cluster_centers->_init_centroids(X, self.n_clusters, self.init, random_state=random_state, x_squared_norms=x_squared_norms, init_size=init_size)
A:sklearn.cluster.k_means_.(batch_inertia, centers_squared_diff)->_mini_batch_step(X[minibatch_indices], x_squared_norms[minibatch_indices], self.cluster_centers_, self.counts_, old_center_buffer, tol > 0.0, distances=distances, random_reassign=(iteration_idx + 1) % (10 + self.counts_.min()) == 0, random_state=random_state, reassignment_ratio=self.reassignment_ratio, verbose=self.verbose)
A:sklearn.cluster.k_means_.(_, inertia)->_labels_inertia(X_valid, x_squared_norms_valid, cluster_centers)
A:sklearn.cluster.k_means_.minibatch_indices->check_random_state(self.random_state).randint(0, n_samples, self.batch_size)
A:sklearn.cluster.k_means_.(self.labels_, self.inertia_)->_labels_inertia(X, x_squared_norms, self.cluster_centers_)
A:sklearn.cluster.k_means_.slices->gen_batches(X.shape[0], self.batch_size)
A:sklearn.cluster.k_means_.self.random_state_->getattr(self, 'random_state_', check_random_state(self.random_state))
A:sklearn.cluster.k_means_.self.cluster_centers_->_init_centroids(X, self.n_clusters, self.init, random_state=self.random_state_, x_squared_norms=x_squared_norms, init_size=self.init_size)
A:sklearn.cluster.k_means_.self.counts_->numpy.zeros(self.n_clusters, dtype=np.int32)
sklearn.cluster.KMeans(self,n_clusters=8,init='k-means++',n_init=10,max_iter=300,tol=0.0001,precompute_distances='auto',verbose=0,random_state=None,copy_x=True,n_jobs=1,algorithm='auto')
sklearn.cluster.KMeans._check_fit_data(self,X)
sklearn.cluster.KMeans._check_test_data(self,X)
sklearn.cluster.KMeans._transform(self,X)
sklearn.cluster.KMeans.fit(self,X,y=None)
sklearn.cluster.KMeans.fit_predict(self,X,y=None)
sklearn.cluster.KMeans.fit_transform(self,X,y=None)
sklearn.cluster.KMeans.predict(self,X)
sklearn.cluster.KMeans.score(self,X,y=None)
sklearn.cluster.KMeans.transform(self,X)
sklearn.cluster.MiniBatchKMeans(self,n_clusters=8,init='k-means++',max_iter=100,batch_size=100,verbose=0,compute_labels=True,random_state=None,tol=0.0,max_no_improvement=10,init_size=None,n_init=3,reassignment_ratio=0.01)
sklearn.cluster.MiniBatchKMeans._labels_inertia_minibatch(self,X)
sklearn.cluster.MiniBatchKMeans.fit(self,X,y=None)
sklearn.cluster.MiniBatchKMeans.partial_fit(self,X,y=None)
sklearn.cluster.MiniBatchKMeans.predict(self,X)
sklearn.cluster.k_means(X,n_clusters,init='k-means++',precompute_distances='auto',n_init=10,max_iter=300,verbose=False,tol=0.0001,random_state=None,copy_x=True,n_jobs=1,algorithm='auto',return_n_iter=False)
sklearn.cluster.k_means_.KMeans(self,n_clusters=8,init='k-means++',n_init=10,max_iter=300,tol=0.0001,precompute_distances='auto',verbose=0,random_state=None,copy_x=True,n_jobs=1,algorithm='auto')
sklearn.cluster.k_means_.KMeans.__init__(self,n_clusters=8,init='k-means++',n_init=10,max_iter=300,tol=0.0001,precompute_distances='auto',verbose=0,random_state=None,copy_x=True,n_jobs=1,algorithm='auto')
sklearn.cluster.k_means_.KMeans._check_fit_data(self,X)
sklearn.cluster.k_means_.KMeans._check_test_data(self,X)
sklearn.cluster.k_means_.KMeans._transform(self,X)
sklearn.cluster.k_means_.KMeans.fit(self,X,y=None)
sklearn.cluster.k_means_.KMeans.fit_predict(self,X,y=None)
sklearn.cluster.k_means_.KMeans.fit_transform(self,X,y=None)
sklearn.cluster.k_means_.KMeans.predict(self,X)
sklearn.cluster.k_means_.KMeans.score(self,X,y=None)
sklearn.cluster.k_means_.KMeans.transform(self,X)
sklearn.cluster.k_means_.MiniBatchKMeans(self,n_clusters=8,init='k-means++',max_iter=100,batch_size=100,verbose=0,compute_labels=True,random_state=None,tol=0.0,max_no_improvement=10,init_size=None,n_init=3,reassignment_ratio=0.01)
sklearn.cluster.k_means_.MiniBatchKMeans.__init__(self,n_clusters=8,init='k-means++',max_iter=100,batch_size=100,verbose=0,compute_labels=True,random_state=None,tol=0.0,max_no_improvement=10,init_size=None,n_init=3,reassignment_ratio=0.01)
sklearn.cluster.k_means_.MiniBatchKMeans._labels_inertia_minibatch(self,X)
sklearn.cluster.k_means_.MiniBatchKMeans.fit(self,X,y=None)
sklearn.cluster.k_means_.MiniBatchKMeans.partial_fit(self,X,y=None)
sklearn.cluster.k_means_.MiniBatchKMeans.predict(self,X)
sklearn.cluster.k_means_._init_centroids(X,k,init,random_state=None,x_squared_norms=None,init_size=None)
sklearn.cluster.k_means_._k_init(X,n_clusters,x_squared_norms,random_state,n_local_trials=None)
sklearn.cluster.k_means_._kmeans_single_elkan(X,n_clusters,max_iter=300,init='k-means++',verbose=False,x_squared_norms=None,random_state=None,tol=0.0001,precompute_distances=True)
sklearn.cluster.k_means_._kmeans_single_lloyd(X,n_clusters,max_iter=300,init='k-means++',verbose=False,x_squared_norms=None,random_state=None,tol=0.0001,precompute_distances=True)
sklearn.cluster.k_means_._labels_inertia(X,x_squared_norms,centers,precompute_distances=True,distances=None)
sklearn.cluster.k_means_._labels_inertia_precompute_dense(X,x_squared_norms,centers,distances)
sklearn.cluster.k_means_._mini_batch_convergence(model,iteration_idx,n_iter,tol,n_samples,centers_squared_diff,batch_inertia,context,verbose=0)
sklearn.cluster.k_means_._mini_batch_step(X,x_squared_norms,centers,counts,old_center_buffer,compute_squared_diff,distances,random_reassign=False,random_state=None,reassignment_ratio=0.01,verbose=False)
sklearn.cluster.k_means_._tolerance(X,tol)
sklearn.cluster.k_means_._validate_center_shape(X,n_centers,centers)
sklearn.cluster.k_means_.k_means(X,n_clusters,init='k-means++',precompute_distances='auto',n_init=10,max_iter=300,verbose=False,tol=0.0001,random_state=None,copy_x=True,n_jobs=1,algorithm='auto',return_n_iter=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/cluster/_feature_agglomeration.py----------------------------------------
A:sklearn.cluster._feature_agglomeration.X->check_array(X)
A:sklearn.cluster._feature_agglomeration.(unil, inverse)->numpy.unique(self.labels_, return_inverse=True)
sklearn.cluster._feature_agglomeration.AgglomerationTransform(TransformerMixin)
sklearn.cluster._feature_agglomeration.AgglomerationTransform.inverse_transform(self,Xred)
sklearn.cluster._feature_agglomeration.AgglomerationTransform.transform(self,X)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/cluster/birch.py----------------------------------------
A:sklearn.cluster.birch.row->numpy.zeros(X.shape[1])
A:sklearn.cluster.birch.new_subcluster1->_CFSubcluster()
A:sklearn.cluster.birch.new_subcluster2->_CFSubcluster()
A:sklearn.cluster.birch.new_node1->_CFNode(threshold, branching_factor, is_leaf=node.is_leaf, n_features=node.n_features)
A:sklearn.cluster.birch.new_node2->_CFNode(threshold, branching_factor, is_leaf=node.is_leaf, n_features=node.n_features)
A:sklearn.cluster.birch.dist->euclidean_distances(node.centroids_, Y_norm_squared=node.squared_norm_, squared=True)
A:sklearn.cluster.birch.farthest_idx->numpy.unravel_index(dist.argmax(), (n_clusters, n_clusters))
A:sklearn.cluster.birch.self.init_centroids_->numpy.zeros((branching_factor + 1, n_features))
A:sklearn.cluster.birch.self.init_sq_norm_->numpy.zeros(branching_factor + 1)
A:sklearn.cluster.birch.n_samples->len(self.subclusters_)
A:sklearn.cluster.birch.ind->self.subclusters_.index(subcluster)
A:sklearn.cluster.birch.dist_matrix->numpy.dot(self.centroids_, subcluster.centroid_)
A:sklearn.cluster.birch.closest_index->numpy.argmin(dist_matrix)
A:sklearn.cluster.birch.split_child->closest_subcluster.child_.insert_cf_subcluster(subcluster)
A:sklearn.cluster.birch.(new_subcluster1, new_subcluster2)->_split_node(self.root_, threshold, branching_factor)
A:sklearn.cluster.birch.merged->closest_subcluster.merge_subcluster(subcluster, self.threshold)
A:sklearn.cluster.birch.self.squared_sum_self.sq_norm_->numpy.dot(self.linear_sum_, self.linear_sum_)
A:sklearn.cluster.birch.self.sq_norm_->numpy.dot(self.centroid_, self.centroid_)
A:sklearn.cluster.birch.new_norm->numpy.dot(new_centroid, new_centroid)
A:sklearn.cluster.birch.X->check_array(X, accept_sparse='csr')
A:sklearn.cluster.birch.partial_fit->getattr(self, 'partial_fit_')
A:sklearn.cluster.birch.has_root->getattr(self, 'root_', None)
A:sklearn.cluster.birch.self.root_->_CFNode(threshold, branching_factor, is_leaf=False, n_features=n_features)
A:sklearn.cluster.birch.self.dummy_leaf_->_CFNode(threshold, branching_factor, is_leaf=True, n_features=n_features)
A:sklearn.cluster.birch.subcluster->_CFSubcluster(linear_sum=sample)
A:sklearn.cluster.birch.split->self.root_.insert_cf_subcluster(subcluster)
A:sklearn.cluster.birch.centroids->numpy.concatenate([leaf.centroids_ for leaf in self._get_leaves()])
A:sklearn.cluster.birch.is_fitted->hasattr(self, 'subcluster_centers_')
A:sklearn.cluster.birch.has_partial_fit->hasattr(self, 'partial_fit_')
A:sklearn.cluster.birch.reduced_distance->safe_sparse_dot(X, self.subcluster_centers_.T)
A:sklearn.cluster.birch.clusterer->AgglomerativeClustering(n_clusters=self.n_clusters)
A:sklearn.cluster.birch.self._subcluster_norms->row_norms(self.subcluster_centers_, squared=True)
A:sklearn.cluster.birch.self.subcluster_labels_->AgglomerativeClustering(n_clusters=self.n_clusters).fit_predict(self.subcluster_centers_)
A:sklearn.cluster.birch.self.labels_->self.predict(X)
sklearn.cluster.Birch(self,threshold=0.5,branching_factor=50,n_clusters=3,compute_labels=True,copy=True)
sklearn.cluster.Birch._check_fit(self,X)
sklearn.cluster.Birch._fit(self,X)
sklearn.cluster.Birch._get_leaves(self)
sklearn.cluster.Birch._global_clustering(self,X=None)
sklearn.cluster.Birch.fit(self,X,y=None)
sklearn.cluster.Birch.partial_fit(self,X=None,y=None)
sklearn.cluster.Birch.predict(self,X)
sklearn.cluster.Birch.transform(self,X)
sklearn.cluster.birch.Birch(self,threshold=0.5,branching_factor=50,n_clusters=3,compute_labels=True,copy=True)
sklearn.cluster.birch.Birch.__init__(self,threshold=0.5,branching_factor=50,n_clusters=3,compute_labels=True,copy=True)
sklearn.cluster.birch.Birch._check_fit(self,X)
sklearn.cluster.birch.Birch._fit(self,X)
sklearn.cluster.birch.Birch._get_leaves(self)
sklearn.cluster.birch.Birch._global_clustering(self,X=None)
sklearn.cluster.birch.Birch.fit(self,X,y=None)
sklearn.cluster.birch.Birch.partial_fit(self,X=None,y=None)
sklearn.cluster.birch.Birch.predict(self,X)
sklearn.cluster.birch.Birch.transform(self,X)
sklearn.cluster.birch._CFNode(self,threshold,branching_factor,is_leaf,n_features)
sklearn.cluster.birch._CFNode.__init__(self,threshold,branching_factor,is_leaf,n_features)
sklearn.cluster.birch._CFNode.append_subcluster(self,subcluster)
sklearn.cluster.birch._CFNode.insert_cf_subcluster(self,subcluster)
sklearn.cluster.birch._CFNode.update_split_subclusters(self,subcluster,new_subcluster1,new_subcluster2)
sklearn.cluster.birch._CFSubcluster(self,linear_sum=None)
sklearn.cluster.birch._CFSubcluster.__init__(self,linear_sum=None)
sklearn.cluster.birch._CFSubcluster.merge_subcluster(self,nominee_cluster,threshold)
sklearn.cluster.birch._CFSubcluster.radius(self)
sklearn.cluster.birch._CFSubcluster.update(self,subcluster)
sklearn.cluster.birch._iterate_sparse_X(X)
sklearn.cluster.birch._split_node(node,threshold,branching_factor)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/cluster/mean_shift_.py----------------------------------------
A:sklearn.cluster.mean_shift_.X->check_array(X)
A:sklearn.cluster.mean_shift_.random_state->check_random_state(random_state)
A:sklearn.cluster.mean_shift_.nbrs->NearestNeighbors(n_neighbors=1, n_jobs=n_jobs).fit(cluster_centers)
A:sklearn.cluster.mean_shift_.(d, _)->NearestNeighbors(n_neighbors=1, n_jobs=n_jobs).fit(cluster_centers).kneighbors(X[batch, :], return_distance=True)
A:sklearn.cluster.mean_shift_.my_mean->numpy.mean(points_within, axis=0)
A:sklearn.cluster.mean_shift_.bandwidth->estimate_bandwidth(X, n_jobs=n_jobs)
A:sklearn.cluster.mean_shift_.seeds->get_bin_seeds(X, bandwidth, min_bin_freq)
A:sklearn.cluster.mean_shift_.all_res->Parallel(n_jobs=n_jobs)((delayed(_mean_shift_single_seed)(seed, X, nbrs, max_iter) for seed in seeds))
A:sklearn.cluster.mean_shift_.sorted_by_intensity->sorted(center_intensity_dict.items(), key=lambda tup: tup[1], reverse=True)
A:sklearn.cluster.mean_shift_.sorted_centers->numpy.array([tup[0] for tup in sorted_by_intensity])
A:sklearn.cluster.mean_shift_.unique->numpy.ones(len(sorted_centers), dtype=np.bool)
A:sklearn.cluster.mean_shift_.labels->idxs.flatten()
A:sklearn.cluster.mean_shift_.(distances, idxs)->NearestNeighbors(n_neighbors=1, n_jobs=n_jobs).fit(cluster_centers).kneighbors(X)
A:sklearn.cluster.mean_shift_.bin_sizes->defaultdict(int)
A:sklearn.cluster.mean_shift_.binned_point->numpy.round(point / bin_size)
A:sklearn.cluster.mean_shift_.bin_seeds->numpy.array([point for (point, freq) in six.iteritems(bin_sizes) if freq >= min_bin_freq], dtype=np.float32)
A:sklearn.cluster.mean_shift_.(self.cluster_centers_, self.labels_)->mean_shift(X, bandwidth=self.bandwidth, seeds=self.seeds, min_bin_freq=self.min_bin_freq, bin_seeding=self.bin_seeding, cluster_all=self.cluster_all, n_jobs=self.n_jobs)
sklearn.cluster.MeanShift(self,bandwidth=None,seeds=None,bin_seeding=False,min_bin_freq=1,cluster_all=True,n_jobs=1)
sklearn.cluster.MeanShift.fit(self,X,y=None)
sklearn.cluster.MeanShift.predict(self,X)
sklearn.cluster.estimate_bandwidth(X,quantile=0.3,n_samples=None,random_state=0,n_jobs=1)
sklearn.cluster.get_bin_seeds(X,bin_size,min_bin_freq=1)
sklearn.cluster.mean_shift(X,bandwidth=None,seeds=None,bin_seeding=False,min_bin_freq=1,cluster_all=True,max_iter=300,n_jobs=1)
sklearn.cluster.mean_shift_.MeanShift(self,bandwidth=None,seeds=None,bin_seeding=False,min_bin_freq=1,cluster_all=True,n_jobs=1)
sklearn.cluster.mean_shift_.MeanShift.__init__(self,bandwidth=None,seeds=None,bin_seeding=False,min_bin_freq=1,cluster_all=True,n_jobs=1)
sklearn.cluster.mean_shift_.MeanShift.fit(self,X,y=None)
sklearn.cluster.mean_shift_.MeanShift.predict(self,X)
sklearn.cluster.mean_shift_._mean_shift_single_seed(my_mean,X,nbrs,max_iter)
sklearn.cluster.mean_shift_.estimate_bandwidth(X,quantile=0.3,n_samples=None,random_state=0,n_jobs=1)
sklearn.cluster.mean_shift_.get_bin_seeds(X,bin_size,min_bin_freq=1)
sklearn.cluster.mean_shift_.mean_shift(X,bandwidth=None,seeds=None,bin_seeding=False,min_bin_freq=1,cluster_all=True,max_iter=300,n_jobs=1)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/cluster/spectral.py----------------------------------------
A:sklearn.cluster.spectral.random_state->check_random_state(self.random_state)
A:sklearn.cluster.spectral.vectors->as_float_array(vectors, copy=copy)
A:sklearn.cluster.spectral.norm_ones->numpy.sqrt(n_samples)
A:sklearn.cluster.spectral.rotation->numpy.dot(Vh.T, U.T)
A:sklearn.cluster.spectral.c->numpy.zeros(n_samples)
A:sklearn.cluster.spectral.t_discrete->numpy.dot(vectors, rotation)
A:sklearn.cluster.spectral.labels->discretize(maps, random_state=random_state)
A:sklearn.cluster.spectral.vectors_discrete->csc_matrix((np.ones(len(labels)), (np.arange(0, n_samples), labels)), shape=(n_samples, n_components))
A:sklearn.cluster.spectral.(U, S, Vh)->numpy.linalg.svd(t_svd)
A:sklearn.cluster.spectral.maps->spectral_embedding(affinity, n_components=n_components, eigen_solver=eigen_solver, random_state=random_state, eigen_tol=eigen_tol, drop_first=False)
A:sklearn.cluster.spectral.(_, labels, _)->k_means(maps, n_clusters, random_state=random_state, n_init=n_init)
A:sklearn.cluster.spectral.X->check_array(X, accept_sparse=['csr', 'csc', 'coo'], dtype=np.float64)
A:sklearn.cluster.spectral.connectivity->kneighbors_graph(X, n_neighbors=self.n_neighbors, include_self=True, n_jobs=self.n_jobs)
A:sklearn.cluster.spectral.self.affinity_matrix_->pairwise_kernels(X, metric=self.affinity, filter_params=True, **params)
A:sklearn.cluster.spectral.self.labels_->spectral_clustering(self.affinity_matrix_, n_clusters=self.n_clusters, eigen_solver=self.eigen_solver, random_state=random_state, n_init=self.n_init, eigen_tol=self.eigen_tol, assign_labels=self.assign_labels)
sklearn.cluster.SpectralClustering(self,n_clusters=8,eigen_solver=None,random_state=None,n_init=10,gamma=1.0,affinity='rbf',n_neighbors=10,eigen_tol=0.0,assign_labels='kmeans',degree=3,coef0=1,kernel_params=None,n_jobs=1)
sklearn.cluster.SpectralClustering._pairwise(self)
sklearn.cluster.SpectralClustering.fit(self,X,y=None)
sklearn.cluster.spectral.SpectralClustering(self,n_clusters=8,eigen_solver=None,random_state=None,n_init=10,gamma=1.0,affinity='rbf',n_neighbors=10,eigen_tol=0.0,assign_labels='kmeans',degree=3,coef0=1,kernel_params=None,n_jobs=1)
sklearn.cluster.spectral.SpectralClustering.__init__(self,n_clusters=8,eigen_solver=None,random_state=None,n_init=10,gamma=1.0,affinity='rbf',n_neighbors=10,eigen_tol=0.0,assign_labels='kmeans',degree=3,coef0=1,kernel_params=None,n_jobs=1)
sklearn.cluster.spectral.SpectralClustering._pairwise(self)
sklearn.cluster.spectral.SpectralClustering.fit(self,X,y=None)
sklearn.cluster.spectral.discretize(vectors,copy=True,max_svd_restarts=30,n_iter_max=20,random_state=None)
sklearn.cluster.spectral.spectral_clustering(affinity,n_clusters=8,n_components=None,eigen_solver=None,random_state=None,n_init=10,eigen_tol=0.0,assign_labels='kmeans')
sklearn.cluster.spectral_clustering(affinity,n_clusters=8,n_components=None,eigen_solver=None,random_state=None,n_init=10,eigen_tol=0.0,assign_labels='kmeans')


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/cluster/affinity_propagation_.py----------------------------------------
A:sklearn.cluster.affinity_propagation_.S->as_float_array(S, copy=copy)
A:sklearn.cluster.affinity_propagation_.preference->numpy.median(S)
A:sklearn.cluster.affinity_propagation_.random_state->numpy.random.RandomState(0)
A:sklearn.cluster.affinity_propagation_.A->numpy.zeros((n_samples, n_samples))
A:sklearn.cluster.affinity_propagation_.R->numpy.zeros((n_samples, n_samples))
A:sklearn.cluster.affinity_propagation_.tmp->numpy.zeros((n_samples, n_samples))
A:sklearn.cluster.affinity_propagation_.e->numpy.zeros((n_samples, convergence_iter))
A:sklearn.cluster.affinity_propagation_.ind->numpy.arange(n_samples)
A:sklearn.cluster.affinity_propagation_.I->numpy.argmax(tmp, axis=1)
A:sklearn.cluster.affinity_propagation_.Y2->numpy.max(tmp, axis=1)
A:sklearn.cluster.affinity_propagation_.dA->numpy.diag(tmp).copy()
A:sklearn.cluster.affinity_propagation_.K->numpy.sum(E, axis=0)
A:sklearn.cluster.affinity_propagation_.se->numpy.sum(e, axis=1)
A:sklearn.cluster.affinity_propagation_.c->numpy.argmax(S[:, I], axis=1)
A:sklearn.cluster.affinity_propagation_.c[I]->numpy.arange(K)
A:sklearn.cluster.affinity_propagation_.j->numpy.argmax(np.sum(S[ii[:, np.newaxis], ii], axis=0))
A:sklearn.cluster.affinity_propagation_.cluster_centers_indices->numpy.unique(labels)
A:sklearn.cluster.affinity_propagation_.labels->numpy.empty((n_samples, 1))
A:sklearn.cluster.affinity_propagation_.X->check_array(X, accept_sparse='csr')
A:sklearn.cluster.affinity_propagation_.(self.cluster_centers_indices_, self.labels_, self.n_iter_)->affinity_propagation(self.affinity_matrix_, self.preference, max_iter=self.max_iter, convergence_iter=self.convergence_iter, damping=self.damping, copy=self.copy, verbose=self.verbose, return_n_iter=True)
A:sklearn.cluster.affinity_propagation_.self.cluster_centers_->X[self.cluster_centers_indices_].copy()
sklearn.cluster.AffinityPropagation(self,damping=0.5,max_iter=200,convergence_iter=15,copy=True,preference=None,affinity='euclidean',verbose=False)
sklearn.cluster.AffinityPropagation._pairwise(self)
sklearn.cluster.AffinityPropagation.fit(self,X,y=None)
sklearn.cluster.AffinityPropagation.predict(self,X)
sklearn.cluster.affinity_propagation(S,preference=None,convergence_iter=15,max_iter=200,damping=0.5,copy=True,verbose=False,return_n_iter=False)
sklearn.cluster.affinity_propagation_.AffinityPropagation(self,damping=0.5,max_iter=200,convergence_iter=15,copy=True,preference=None,affinity='euclidean',verbose=False)
sklearn.cluster.affinity_propagation_.AffinityPropagation.__init__(self,damping=0.5,max_iter=200,convergence_iter=15,copy=True,preference=None,affinity='euclidean',verbose=False)
sklearn.cluster.affinity_propagation_.AffinityPropagation._pairwise(self)
sklearn.cluster.affinity_propagation_.AffinityPropagation.fit(self,X,y=None)
sklearn.cluster.affinity_propagation_.AffinityPropagation.predict(self,X)
sklearn.cluster.affinity_propagation_.affinity_propagation(S,preference=None,convergence_iter=15,max_iter=200,damping=0.5,copy=True,verbose=False,return_n_iter=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/cluster/dbscan_.py----------------------------------------
A:sklearn.cluster.dbscan_.X->check_array(X, accept_sparse='csr')
A:sklearn.cluster.dbscan_.sample_weight->numpy.asarray(sample_weight)
A:sklearn.cluster.dbscan_.neighborhoods->NearestNeighbors(radius=eps, algorithm=algorithm, leaf_size=leaf_size, metric=metric, metric_params=metric_params, p=p, n_jobs=n_jobs).radius_neighbors(X, eps, return_distance=False)
A:sklearn.cluster.dbscan_.masked_indices->numpy.insert(masked_indices, masked_indptr, np.arange(X.shape[0]))
A:sklearn.cluster.dbscan_.neighborhoods[:]->numpy.split(masked_indices, masked_indptr)
A:sklearn.cluster.dbscan_.neighbors_model->NearestNeighbors(radius=eps, algorithm=algorithm, leaf_size=leaf_size, metric=metric, metric_params=metric_params, p=p, n_jobs=n_jobs)
A:sklearn.cluster.dbscan_.n_neighbors->numpy.array([np.sum(sample_weight[neighbors]) for neighbors in neighborhoods])
A:sklearn.cluster.dbscan_.core_samples->numpy.asarray(n_neighbors >= min_samples, dtype=np.uint8)
A:sklearn.cluster.dbscan_.clust->dbscan(X, sample_weight=sample_weight, **self.get_params())
A:sklearn.cluster.dbscan_.self.components_->numpy.empty((0, X.shape[1]))
sklearn.cluster.DBSCAN(self,eps=0.5,min_samples=5,metric='euclidean',metric_params=None,algorithm='auto',leaf_size=30,p=None,n_jobs=1)
sklearn.cluster.DBSCAN.fit(self,X,y=None,sample_weight=None)
sklearn.cluster.DBSCAN.fit_predict(self,X,y=None,sample_weight=None)
sklearn.cluster.dbscan(X,eps=0.5,min_samples=5,metric='minkowski',metric_params=None,algorithm='auto',leaf_size=30,p=2,sample_weight=None,n_jobs=1)
sklearn.cluster.dbscan_.DBSCAN(self,eps=0.5,min_samples=5,metric='euclidean',metric_params=None,algorithm='auto',leaf_size=30,p=None,n_jobs=1)
sklearn.cluster.dbscan_.DBSCAN.__init__(self,eps=0.5,min_samples=5,metric='euclidean',metric_params=None,algorithm='auto',leaf_size=30,p=None,n_jobs=1)
sklearn.cluster.dbscan_.DBSCAN.fit(self,X,y=None,sample_weight=None)
sklearn.cluster.dbscan_.DBSCAN.fit_predict(self,X,y=None,sample_weight=None)
sklearn.cluster.dbscan_.dbscan(X,eps=0.5,min_samples=5,metric='minkowski',metric_params=None,algorithm='auto',leaf_size=30,p=2,sample_weight=None,n_jobs=1)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/cluster/setup.py----------------------------------------
A:sklearn.cluster.setup.(cblas_libs, blas_info)->get_blas_info()
A:sklearn.cluster.setup.config->Configuration('cluster', parent_package, top_path)
sklearn.cluster.setup.configuration(parent_package='',top_path=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/cluster/bicluster.py----------------------------------------
A:sklearn.cluster.bicluster.X->check_array(X, accept_sparse='csr', dtype=np.float64)
A:sklearn.cluster.bicluster.row_diag->numpy.where(np.isnan(row_diag), 0, row_diag)
A:sklearn.cluster.bicluster.col_diag->numpy.where(np.isnan(col_diag), 0, col_diag)
A:sklearn.cluster.bicluster.r->dia_matrix((row_diag, [0]), shape=(n_rows, n_rows))
A:sklearn.cluster.bicluster.c->dia_matrix((col_diag, [0]), shape=(n_cols, n_cols))
A:sklearn.cluster.bicluster.(X_new, _, _)->_scale_normalize(X_scaled)
A:sklearn.cluster.bicluster.dist->norm(X_scaled - X_new)
A:sklearn.cluster.bicluster.L->numpy.log(X)
A:sklearn.cluster.bicluster.col_avg->numpy.log(X).mean(axis=0)
A:sklearn.cluster.bicluster.avg->numpy.log(X).mean()
A:sklearn.cluster.bicluster.(u, _, vt)->svds(array, k=n_components, ncv=self.n_svd_vecs)
A:sklearn.cluster.bicluster.A->safe_sparse_dot(array, array.T)
A:sklearn.cluster.bicluster.random_state->check_random_state(self.random_state)
A:sklearn.cluster.bicluster.v0->check_random_state(self.random_state).uniform(-1, 1, A.shape[0])
A:sklearn.cluster.bicluster.(_, v)->eigsh(A, ncv=self.n_svd_vecs, v0=v0)
A:sklearn.cluster.bicluster.(_, u)->eigsh(A, ncv=self.n_svd_vecs, v0=v0)
A:sklearn.cluster.bicluster.model->KMeans(n_clusters, init=self.init, n_init=self.n_init, n_jobs=self.n_jobs, random_state=self.random_state)
A:sklearn.cluster.bicluster.(normalized_data, row_diag, col_diag)->_scale_normalize(X)
A:sklearn.cluster.bicluster.(u, v)->self._svd(normalized_data, n_sv, n_discard)
A:sklearn.cluster.bicluster.z->numpy.vstack((row_diag[:, np.newaxis] * u, col_diag[:, np.newaxis] * v))
A:sklearn.cluster.bicluster.(_, labels)->self._k_means(projected, n_clusters)
A:sklearn.cluster.bicluster.self.rows_->numpy.vstack((self.row_labels_ == label for label in range(n_row_clusters) for _ in range(n_col_clusters)))
A:sklearn.cluster.bicluster.self.columns_->numpy.vstack((self.column_labels_ == label for _ in range(n_row_clusters) for label in range(n_col_clusters)))
A:sklearn.cluster.bicluster.normalized_data->_log_normalize(X)
A:sklearn.cluster.bicluster.(normalized_data, _, _)->_scale_normalize(X)
A:sklearn.cluster.bicluster.best_ut->self._fit_best_piecewise(ut, self.n_best, n_row_clusters)
A:sklearn.cluster.bicluster.best_vt->self._fit_best_piecewise(vt, self.n_best, n_col_clusters)
A:sklearn.cluster.bicluster.self.row_labels_->self._project_and_cluster(X, best_vt.T, n_row_clusters)
A:sklearn.cluster.bicluster.self.column_labels_->self._project_and_cluster(X.T, best_ut.T, n_col_clusters)
A:sklearn.cluster.bicluster.(centroid, labels)->self._k_means(v.reshape(-1, 1), n_clusters)
A:sklearn.cluster.bicluster.piecewise_vectors->numpy.apply_along_axis(make_piecewise, axis=1, arr=vectors)
A:sklearn.cluster.bicluster.dists->numpy.apply_along_axis(norm, axis=1, arr=vectors - piecewise_vectors)
A:sklearn.cluster.bicluster.projected->safe_sparse_dot(data, vectors)
sklearn.cluster.SpectralBiclustering(self,n_clusters=3,method='bistochastic',n_components=6,n_best=3,svd_method='randomized',n_svd_vecs=None,mini_batch=False,init='k-means++',n_init=10,n_jobs=1,random_state=None)
sklearn.cluster.SpectralBiclustering._check_parameters(self)
sklearn.cluster.SpectralBiclustering._fit(self,X)
sklearn.cluster.SpectralBiclustering._fit_best_piecewise(self,vectors,n_best,n_clusters)
sklearn.cluster.SpectralBiclustering._project_and_cluster(self,data,vectors,n_clusters)
sklearn.cluster.SpectralCoclustering(self,n_clusters=3,svd_method='randomized',n_svd_vecs=None,mini_batch=False,init='k-means++',n_init=10,n_jobs=1,random_state=None)
sklearn.cluster.SpectralCoclustering._fit(self,X)
sklearn.cluster.bicluster.BaseSpectral(self,n_clusters=3,svd_method='randomized',n_svd_vecs=None,mini_batch=False,init='k-means++',n_init=10,n_jobs=1,random_state=None)
sklearn.cluster.bicluster.BaseSpectral.__init__(self,n_clusters=3,svd_method='randomized',n_svd_vecs=None,mini_batch=False,init='k-means++',n_init=10,n_jobs=1,random_state=None)
sklearn.cluster.bicluster.BaseSpectral._check_parameters(self)
sklearn.cluster.bicluster.BaseSpectral._k_means(self,data,n_clusters)
sklearn.cluster.bicluster.BaseSpectral._svd(self,array,n_components,n_discard)
sklearn.cluster.bicluster.BaseSpectral.fit(self,X,y=None)
sklearn.cluster.bicluster.SpectralBiclustering(self,n_clusters=3,method='bistochastic',n_components=6,n_best=3,svd_method='randomized',n_svd_vecs=None,mini_batch=False,init='k-means++',n_init=10,n_jobs=1,random_state=None)
sklearn.cluster.bicluster.SpectralBiclustering.__init__(self,n_clusters=3,method='bistochastic',n_components=6,n_best=3,svd_method='randomized',n_svd_vecs=None,mini_batch=False,init='k-means++',n_init=10,n_jobs=1,random_state=None)
sklearn.cluster.bicluster.SpectralBiclustering._check_parameters(self)
sklearn.cluster.bicluster.SpectralBiclustering._fit(self,X)
sklearn.cluster.bicluster.SpectralBiclustering._fit_best_piecewise(self,vectors,n_best,n_clusters)
sklearn.cluster.bicluster.SpectralBiclustering._project_and_cluster(self,data,vectors,n_clusters)
sklearn.cluster.bicluster.SpectralCoclustering(self,n_clusters=3,svd_method='randomized',n_svd_vecs=None,mini_batch=False,init='k-means++',n_init=10,n_jobs=1,random_state=None)
sklearn.cluster.bicluster.SpectralCoclustering.__init__(self,n_clusters=3,svd_method='randomized',n_svd_vecs=None,mini_batch=False,init='k-means++',n_init=10,n_jobs=1,random_state=None)
sklearn.cluster.bicluster.SpectralCoclustering._fit(self,X)
sklearn.cluster.bicluster._bistochastic_normalize(X,max_iter=1000,tol=1e-05)
sklearn.cluster.bicluster._log_normalize(X)
sklearn.cluster.bicluster._scale_normalize(X)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/cluster/hierarchical.py----------------------------------------
A:sklearn.cluster.hierarchical.connectivity->check_array(connectivity, accept_sparse=['csr', 'coo', 'lil'])
A:sklearn.cluster.hierarchical.(n_components, labels)->connected_components(connectivity)
A:sklearn.cluster.hierarchical.D->pairwise_distances(Xi, Xj, metric=affinity)
A:sklearn.cluster.hierarchical.(ii, jj)->numpy.where(D == np.min(D))
A:sklearn.cluster.hierarchical.X->check_array(X, accept_sparse=['csr', 'csc', 'coo'], ensure_min_features=2, estimator=self)
A:sklearn.cluster.hierarchical.out->scipy.cluster.hierarchy.linkage(X, method=linkage, metric=affinity)
A:sklearn.cluster.hierarchical.children_->out[:, :2].astype(np.int)
A:sklearn.cluster.hierarchical.(connectivity, n_components)->_fix_connectivity(X, connectivity, affinity=affinity)
A:sklearn.cluster.hierarchical.coord_row->numpy.empty(coord_col.shape, dtype=np.intp, order='C')
A:sklearn.cluster.hierarchical.coord_col->join_func(A[i], A[j], used_node, n_i, n_j)
A:sklearn.cluster.hierarchical.moments_1->numpy.zeros(n_nodes, order='C')
A:sklearn.cluster.hierarchical.moments_2->numpy.zeros((n_nodes, n_features), order='C')
A:sklearn.cluster.hierarchical.inertia->list()
A:sklearn.cluster.hierarchical.parent->numpy.arange(n_nodes, dtype=np.intp)
A:sklearn.cluster.hierarchical.used_node->numpy.ones(n_nodes, dtype=np.intp)
A:sklearn.cluster.hierarchical.distances->numpy.empty(n_nodes - n_samples)
A:sklearn.cluster.hierarchical.not_visited->numpy.empty(n_nodes, dtype=np.int8, order='C')
A:sklearn.cluster.hierarchical.(inert, i, j)->heappop(inertia)
A:sklearn.cluster.hierarchical.n_additions->len(coord_row)
A:sklearn.cluster.hierarchical.ini->numpy.empty(n_additions, dtype=np.float64, order='C')
A:sklearn.cluster.hierarchical.children->numpy.array(children)
A:sklearn.cluster.hierarchical.(i, j)->numpy.triu_indices(X.shape[0], k=1)
A:sklearn.cluster.hierarchical.A->numpy.empty(n_nodes, dtype=object)
A:sklearn.cluster.hierarchical.A[ind]->IntFloatDict(np.asarray(row, dtype=np.intp), np.asarray(data, dtype=np.float64))
A:sklearn.cluster.hierarchical.edge->heappop(inertia)
A:sklearn.cluster.hierarchical._TREE_BUILDERS->dict(ward=ward_tree, complete=_complete_linkage, average=_average_linkage)
A:sklearn.cluster.hierarchical.label->numpy.zeros(n_leaves, dtype=np.intp)
A:sklearn.cluster.hierarchical.memory->check_memory(self.memory)
A:sklearn.cluster.hierarchical.n_samples->len(X)
A:sklearn.cluster.hierarchical.(self.children_, self.n_components_, self.n_leaves_, parents)->check_memory(self.memory).cache(tree_builder)(X, connectivity, n_clusters=n_clusters, **kwargs)
A:sklearn.cluster.hierarchical.self.labels_->numpy.searchsorted(np.unique(labels), labels)
A:sklearn.cluster.hierarchical.labels->numpy.copy(labels[:n_samples])
sklearn.cluster.AgglomerativeClustering(self,n_clusters=2,affinity='euclidean',memory=None,connectivity=None,compute_full_tree='auto',linkage='ward',pooling_func=np.mean)
sklearn.cluster.AgglomerativeClustering.fit(self,X,y=None)
sklearn.cluster.FeatureAgglomeration(AgglomerativeClustering,AgglomerationTransform)
sklearn.cluster.FeatureAgglomeration.fit(self,X,y=None,**params)
sklearn.cluster.FeatureAgglomeration.fit_predict(self)
sklearn.cluster.hierarchical.AgglomerativeClustering(self,n_clusters=2,affinity='euclidean',memory=None,connectivity=None,compute_full_tree='auto',linkage='ward',pooling_func=np.mean)
sklearn.cluster.hierarchical.AgglomerativeClustering.__init__(self,n_clusters=2,affinity='euclidean',memory=None,connectivity=None,compute_full_tree='auto',linkage='ward',pooling_func=np.mean)
sklearn.cluster.hierarchical.AgglomerativeClustering.fit(self,X,y=None)
sklearn.cluster.hierarchical.FeatureAgglomeration(AgglomerativeClustering,AgglomerationTransform)
sklearn.cluster.hierarchical.FeatureAgglomeration.fit(self,X,y=None,**params)
sklearn.cluster.hierarchical.FeatureAgglomeration.fit_predict(self)
sklearn.cluster.hierarchical._average_linkage(*args,**kwargs)
sklearn.cluster.hierarchical._complete_linkage(*args,**kwargs)
sklearn.cluster.hierarchical._fix_connectivity(X,connectivity,affinity)
sklearn.cluster.hierarchical._hc_cut(n_clusters,children,n_leaves)
sklearn.cluster.hierarchical.linkage_tree(X,connectivity=None,n_components='deprecated',n_clusters=None,linkage='complete',affinity='euclidean',return_distance=False)
sklearn.cluster.hierarchical.ward_tree(X,connectivity=None,n_clusters=None,return_distance=False)
sklearn.cluster.linkage_tree(X,connectivity=None,n_components='deprecated',n_clusters=None,linkage='complete',affinity='euclidean',return_distance=False)
sklearn.cluster.ward_tree(X,connectivity=None,n_clusters=None,return_distance=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/cluster/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/cluster/tests/test_k_means.py----------------------------------------
A:sklearn.cluster.tests.test_k_means.centers->numpy.array([[0, 0], [0, 1]])
A:sklearn.cluster.tests.test_k_means.(X, true_labels)->make_blobs(n_samples=n_samples, centers=centers, cluster_std=1.0, random_state=42)
A:sklearn.cluster.tests.test_k_means.X_csr->scipy.sparse.csr_matrix(X)
A:sklearn.cluster.tests.test_k_means.rnd->numpy.random.RandomState(42)
A:sklearn.cluster.tests.test_k_means.X_normal->numpy.random.RandomState(42).normal(size=(50, 10))
A:sklearn.cluster.tests.test_k_means.(X_blobs, _)->make_blobs(random_state=0)
A:sklearn.cluster.tests.test_k_means.km_full->KMeans(algorithm='full', n_clusters=5, random_state=0, n_init=1)
A:sklearn.cluster.tests.test_k_means.km_elkan->KMeans(algorithm='elkan', n_clusters=5, random_state=0, n_init=1)
A:sklearn.cluster.tests.test_k_means.rng->numpy.random.RandomState(42)
A:sklearn.cluster.tests.test_k_means.mindist->numpy.minimum(dist, mindist)
A:sklearn.cluster.tests.test_k_means.dist->numpy.sum((X - noisy_centers[center_id]) ** 2, axis=1)
A:sklearn.cluster.tests.test_k_means.inertia_gold->numpy.minimum(dist, mindist).sum()
A:sklearn.cluster.tests.test_k_means.x_squared_norms->(X ** 2).sum(axis=1)
A:sklearn.cluster.tests.test_k_means.(labels_array, inertia_array)->_labels_inertia(X, x_squared_norms, noisy_centers)
A:sklearn.cluster.tests.test_k_means.x_squared_norms_from_csr->row_norms(X_csr, squared=True)
A:sklearn.cluster.tests.test_k_means.(labels_csr, inertia_csr)->_labels_inertia(X_csr, x_squared_norms_from_csr, noisy_centers)
A:sklearn.cluster.tests.test_k_means.new_centers->old_centers.copy()
A:sklearn.cluster.tests.test_k_means.new_centers_csr->old_centers.copy()
A:sklearn.cluster.tests.test_k_means.counts->numpy.zeros(new_centers.shape[0], dtype=np.int32)
A:sklearn.cluster.tests.test_k_means.counts_csr->numpy.zeros(new_centers.shape[0], dtype=np.int32)
A:sklearn.cluster.tests.test_k_means.x_squared_norms_csr->row_norms(X_csr, squared=True)
A:sklearn.cluster.tests.test_k_means.buffer->numpy.zeros(centers.shape[1], dtype=np.double)
A:sklearn.cluster.tests.test_k_means.buffer_csr->numpy.zeros(centers.shape[1], dtype=np.double)
A:sklearn.cluster.tests.test_k_means.(old_inertia, incremental_diff)->_mini_batch_step(X_mb, x_mb_squared_norms, new_centers, counts, buffer, 1, None, random_reassign=False)
A:sklearn.cluster.tests.test_k_means.(labels, new_inertia)->_labels_inertia(X_mb, x_mb_squared_norms, new_centers)
A:sklearn.cluster.tests.test_k_means.effective_diff->numpy.sum((new_centers_csr - old_centers) ** 2)
A:sklearn.cluster.tests.test_k_means.(old_inertia_csr, incremental_diff_csr)->_mini_batch_step(X_mb_csr, x_mb_squared_norms_csr, new_centers_csr, counts_csr, buffer_csr, 1, None, random_reassign=False)
A:sklearn.cluster.tests.test_k_means.(labels_csr, new_inertia_csr)->_labels_inertia(X_mb_csr, x_mb_squared_norms_csr, new_centers_csr)
A:sklearn.cluster.tests.test_k_means.km->KMeans(init=init_centers_test, n_clusters=3, n_init=1)
A:sklearn.cluster.tests.test_k_means.X->numpy.random.RandomState(42).uniform(size=(n_samples, 10))
A:sklearn.cluster.tests.test_k_means.bad_centers->numpy.array([[+0, 1, 0, 0], [0.2, 0, 0.2, 0.2], [+0, 0, 0, 0]])
A:sklearn.cluster.tests.test_k_means.labels->KMeans(init=init_centers_test, n_clusters=3, n_init=1).predict(X)
A:sklearn.cluster.tests.test_k_means.mb_k_means->MiniBatchKMeans(n_clusters=n_clusters, init='random', n_init=10).fit(X_csr)
A:sklearn.cluster.tests.test_k_means.sys.stdout->StringIO()
A:sklearn.cluster.tests.test_k_means.(zeroed_X, true_labels)->make_blobs(n_samples=n_samples, centers=5, cluster_std=1.0, random_state=42)
A:sklearn.cluster.tests.test_k_means.score_before->MiniBatchKMeans(n_clusters=n_clusters, init='random', n_init=10).fit(X_csr).score(this_X)
A:sklearn.cluster.tests.test_k_means.my_X->numpy.array([[1.1, 1.1], [0.9, 1.1], [1.1, 0.9], [0.9, 1.1]])
A:sklearn.cluster.tests.test_k_means.array_init->numpy.array([[1.0, 1.0], [5.0, 5.0], [-5.0, -5.0]])
A:sklearn.cluster.tests.test_k_means.pred->MiniBatchKMeans(n_clusters=n_clusters, init='random', n_init=10).fit(X_csr).predict(mb_k_means.cluster_centers_)
A:sklearn.cluster.tests.test_k_means.km1->KMeans(algorithm='full', random_state=13)
A:sklearn.cluster.tests.test_k_means.s1->KMeans(algorithm='full', random_state=13).fit(X).score(X)
A:sklearn.cluster.tests.test_k_means.km2->KMeans(algorithm='elkan', random_state=13)
A:sklearn.cluster.tests.test_k_means.s2->KMeans(algorithm='elkan', random_state=13).fit(X).score(X)
A:sklearn.cluster.tests.test_k_means.X_int->numpy.array(X_list, dtype=dtype)
A:sklearn.cluster.tests.test_k_means.X_int_csr->scipy.sparse.csr_matrix(X_int)
A:sklearn.cluster.tests.test_k_means.scores->numpy.array([v_measure_score(expected_labels, km.labels_) for km in fitted_models])
A:sklearn.cluster.tests.test_k_means.X_new->KMeans(init=init_centers_test, n_clusters=3, n_init=1).transform(km.cluster_centers_)
A:sklearn.cluster.tests.test_k_means.X1->KMeans(n_clusters=3, random_state=51).fit(X).transform(X)
A:sklearn.cluster.tests.test_k_means.X2->KMeans(n_clusters=3, random_state=51).fit_transform(X)
A:sklearn.cluster.tests.test_k_means.inertia->inertia.mean(axis=1).mean(axis=1)
A:sklearn.cluster.tests.test_k_means.(cluster_centers, labels, inertia)->k_means(X, n_clusters=n_clusters, verbose=True)
A:sklearn.cluster.tests.test_k_means.X_norms->numpy.sum(X ** 2, axis=1)
A:sklearn.cluster.tests.test_k_means.precompute->_init_centroids(X, 3, 'k-means++', random_state=0, x_squared_norms=X_norms)
A:sklearn.cluster.tests.test_k_means.mb_km->MiniBatchKMeans(n_init=1, random_state=30)
A:sklearn.cluster.tests.test_k_means.X_test->dtype(X_small)
A:sklearn.cluster.tests.test_k_means.X_new[dtype]->estimator.transform(X_test)
A:sklearn.cluster.tests.test_k_means.X_small->numpy.array([[1.1, 1.1], [-7.5, -7.5], [-1.1, -1.1], [7.5, 7.5]])
A:sklearn.cluster.tests.test_k_means.init_centers->numpy.array([[0.0, 0.0], [5.0, 5.0], [-5.0, -5.0]])
A:sklearn.cluster.tests.test_k_means.init_centers_test->dtype(init_centers)
A:sklearn.cluster.tests.test_k_means.iris->load_iris()
A:sklearn.cluster.tests.test_k_means.X_sparse->scipy.sparse.csr_matrix(X)
A:sklearn.cluster.tests.test_k_means.classifier->KMeans(n_clusters=3, init=centers, n_init=1)
sklearn.cluster.tests.test_k_means._check_fitted_model(km)
sklearn.cluster.tests.test_k_means.test_elkan_results()
sklearn.cluster.tests.test_k_means.test_fit_transform()
sklearn.cluster.tests.test_k_means.test_float_precision()
sklearn.cluster.tests.test_k_means.test_full_vs_elkan()
sklearn.cluster.tests.test_k_means.test_int_input()
sklearn.cluster.tests.test_k_means.test_k_means_copyx()
sklearn.cluster.tests.test_k_means.test_k_means_explicit_init_shape()
sklearn.cluster.tests.test_k_means.test_k_means_fortran_aligned_data()
sklearn.cluster.tests.test_k_means.test_k_means_function()
sklearn.cluster.tests.test_k_means.test_k_means_init_centers()
sklearn.cluster.tests.test_k_means.test_k_means_invalid_init()
sklearn.cluster.tests.test_k_means.test_k_means_n_init()
sklearn.cluster.tests.test_k_means.test_k_means_new_centers()
sklearn.cluster.tests.test_k_means.test_k_means_non_collapsed()
sklearn.cluster.tests.test_k_means.test_k_means_perfect_init()
sklearn.cluster.tests.test_k_means.test_k_means_plus_plus_init()
sklearn.cluster.tests.test_k_means.test_k_means_plus_plus_init_2_jobs()
sklearn.cluster.tests.test_k_means.test_k_means_plus_plus_init_not_precomputed()
sklearn.cluster.tests.test_k_means.test_k_means_plus_plus_init_sparse()
sklearn.cluster.tests.test_k_means.test_k_means_precompute_distances_flag()
sklearn.cluster.tests.test_k_means.test_k_means_random_init()
sklearn.cluster.tests.test_k_means.test_k_means_random_init_not_precomputed()
sklearn.cluster.tests.test_k_means.test_k_means_random_init_sparse()
sklearn.cluster.tests.test_k_means.test_labels_assignment_and_inertia()
sklearn.cluster.tests.test_k_means.test_max_iter_error()
sklearn.cluster.tests.test_k_means.test_mb_k_means_plus_plus_init_dense_array()
sklearn.cluster.tests.test_k_means.test_mb_k_means_plus_plus_init_sparse_matrix()
sklearn.cluster.tests.test_k_means.test_mb_kmeans_verbose()
sklearn.cluster.tests.test_k_means.test_mini_batch_k_means_random_init_partial_fit()
sklearn.cluster.tests.test_k_means.test_mini_match_k_means_invalid_init()
sklearn.cluster.tests.test_k_means.test_minibatch_default_init_size()
sklearn.cluster.tests.test_k_means.test_minibatch_init_with_large_k()
sklearn.cluster.tests.test_k_means.test_minibatch_k_means_init_multiple_runs_with_explicit_centers()
sklearn.cluster.tests.test_k_means.test_minibatch_k_means_perfect_init_dense_array()
sklearn.cluster.tests.test_k_means.test_minibatch_k_means_perfect_init_sparse_csr()
sklearn.cluster.tests.test_k_means.test_minibatch_k_means_random_init_dense_array()
sklearn.cluster.tests.test_k_means.test_minibatch_k_means_random_init_sparse_csr()
sklearn.cluster.tests.test_k_means.test_minibatch_reassign()
sklearn.cluster.tests.test_k_means.test_minibatch_sensible_reassign_fit()
sklearn.cluster.tests.test_k_means.test_minibatch_sensible_reassign_partial_fit()
sklearn.cluster.tests.test_k_means.test_minibatch_set_init_size()
sklearn.cluster.tests.test_k_means.test_minibatch_tol()
sklearn.cluster.tests.test_k_means.test_minibatch_update_consistency()
sklearn.cluster.tests.test_k_means.test_minibatch_with_many_reassignments()
sklearn.cluster.tests.test_k_means.test_n_init()
sklearn.cluster.tests.test_k_means.test_predict()
sklearn.cluster.tests.test_k_means.test_predict_equal_labels()
sklearn.cluster.tests.test_k_means.test_predict_minibatch_dense_input()
sklearn.cluster.tests.test_k_means.test_predict_minibatch_kmeanspp_init_sparse_input()
sklearn.cluster.tests.test_k_means.test_predict_minibatch_random_init_sparse_input()
sklearn.cluster.tests.test_k_means.test_score()
sklearn.cluster.tests.test_k_means.test_sparse_k_means_init_centers()
sklearn.cluster.tests.test_k_means.test_sparse_mb_k_means_callable_init()
sklearn.cluster.tests.test_k_means.test_sparse_validate_centers()
sklearn.cluster.tests.test_k_means.test_transform()
sklearn.cluster.tests.test_k_means.test_x_squared_norms_init_centroids()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/cluster/tests/test_birch.py----------------------------------------
A:sklearn.cluster.tests.test_birch.(X, y)->make_blobs(n_samples=80, centers=4)
A:sklearn.cluster.tests.test_birch.brc->Birch(threshold=5.0, n_clusters=None)
A:sklearn.cluster.tests.test_birch.n_samples_root->sum([sc.n_samples_ for sc in brc.root_.subclusters_])
A:sklearn.cluster.tests.test_birch.n_samples_leaves->sum([sc.n_samples_ for leaf in brc._get_leaves() for sc in leaf.subclusters_])
A:sklearn.cluster.tests.test_birch.brc_partial->Birch(n_clusters=None)
A:sklearn.cluster.tests.test_birch.rng->numpy.random.RandomState(0)
A:sklearn.cluster.tests.test_birch.X->generate_clustered_data(n_clusters=3, n_features=3, n_samples_per_cluster=10)
A:sklearn.cluster.tests.test_birch.shuffle_indices->numpy.arange(30)
A:sklearn.cluster.tests.test_birch.nearest_centroid->pairwise_distances_argmin(X_shuffle, centroids)
A:sklearn.cluster.tests.test_birch.brc1->Birch(n_clusters=10)
A:sklearn.cluster.tests.test_birch.gc->AgglomerativeClustering(n_clusters=10)
A:sklearn.cluster.tests.test_birch.brc2->Birch(n_clusters=gc)
A:sklearn.cluster.tests.test_birch.clf->ElasticNet()
A:sklearn.cluster.tests.test_birch.brc3->Birch(n_clusters=clf)
A:sklearn.cluster.tests.test_birch.brc4->Birch(threshold=10000.0)
A:sklearn.cluster.tests.test_birch.csr->scipy.sparse.csr_matrix(X)
A:sklearn.cluster.tests.test_birch.brc_sparse->Birch(n_clusters=10)
sklearn.cluster.tests.test_birch.check_branching_factor(node,branching_factor)
sklearn.cluster.tests.test_birch.check_threshold(birch_instance,threshold)
sklearn.cluster.tests.test_birch.test_birch_predict()
sklearn.cluster.tests.test_birch.test_branching_factor()
sklearn.cluster.tests.test_birch.test_n_clusters()
sklearn.cluster.tests.test_birch.test_n_samples_leaves_roots()
sklearn.cluster.tests.test_birch.test_partial_fit()
sklearn.cluster.tests.test_birch.test_sparse_X()
sklearn.cluster.tests.test_birch.test_threshold()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/cluster/tests/test_dbscan.py----------------------------------------
A:sklearn.cluster.tests.test_dbscan.X->numpy.zeros((10, 10))
A:sklearn.cluster.tests.test_dbscan.D->pairwise_distances(X)
A:sklearn.cluster.tests.test_dbscan.(core_samples, labels)->dbscan(X, algorithm=algorithm, eps=1, min_samples=4)
A:sklearn.cluster.tests.test_dbscan.db->DBSCAN(leaf_size=20, eps=eps, min_samples=min_samples, algorithm='ball_tree')
A:sklearn.cluster.tests.test_dbscan.(core_sparse, labels_sparse)->dbscan(D_sparse, eps=0.8, min_samples=10, metric='precomputed')
A:sklearn.cluster.tests.test_dbscan.(core_dense, labels_dense)->dbscan(D, eps=0.8, min_samples=10, metric='precomputed')
A:sklearn.cluster.tests.test_dbscan.nn->NearestNeighbors(radius=0.9).fit(X)
A:sklearn.cluster.tests.test_dbscan.D_sparse->NearestNeighbors(radius=0.9).fit(X).radius_neighbors_graph(mode='distance')
A:sklearn.cluster.tests.test_dbscan.rng->numpy.random.RandomState(42)
A:sklearn.cluster.tests.test_dbscan.obj->DBSCAN()
A:sklearn.cluster.tests.test_dbscan.s->pickle.dumps(obj)
A:sklearn.cluster.tests.test_dbscan.(core, _)->dbscan([[0], [1], [1]], eps=0.99, min_samples=2)
A:sklearn.cluster.tests.test_dbscan.sample_weight->numpy.random.RandomState(42).randint(0, 5, X.shape[0])
A:sklearn.cluster.tests.test_dbscan.(core1, label1)->dbscan(X, sample_weight=sample_weight)
A:sklearn.cluster.tests.test_dbscan.X_repeated->numpy.repeat(X, sample_weight, axis=0)
A:sklearn.cluster.tests.test_dbscan.(core_repeated, label_repeated)->dbscan(X_repeated)
A:sklearn.cluster.tests.test_dbscan.core_repeated_mask->numpy.zeros(X_repeated.shape[0], dtype=bool)
A:sklearn.cluster.tests.test_dbscan.core_mask->numpy.zeros(X.shape[0], dtype=bool)
A:sklearn.cluster.tests.test_dbscan.(core3, label3)->dbscan(D, sample_weight=sample_weight, metric='precomputed')
A:sklearn.cluster.tests.test_dbscan.est->DBSCAN()
A:sklearn.cluster.tests.test_dbscan.label5->DBSCAN().fit_predict(X, sample_weight=sample_weight)
A:sklearn.cluster.tests.test_dbscan.n_samples->len(X)
A:sklearn.cluster.tests.test_dbscan.ar->numpy.array([[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0], [0.0, 0.0, 0.1, 0.1, 0.0, 0.0, 0.3], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1], [0.0, 0.0, 0.0, 0.0, 0.3, 0.1, 0.0]])
A:sklearn.cluster.tests.test_dbscan.matrix->scipy.sparse.csr_matrix(ar)
sklearn.cluster.tests.test_dbscan.test_boundaries()
sklearn.cluster.tests.test_dbscan.test_dbscan_badargs()
sklearn.cluster.tests.test_dbscan.test_dbscan_balltree()
sklearn.cluster.tests.test_dbscan.test_dbscan_callable()
sklearn.cluster.tests.test_dbscan.test_dbscan_core_samples_toy()
sklearn.cluster.tests.test_dbscan.test_dbscan_feature()
sklearn.cluster.tests.test_dbscan.test_dbscan_metric_params()
sklearn.cluster.tests.test_dbscan.test_dbscan_no_core_samples()
sklearn.cluster.tests.test_dbscan.test_dbscan_precomputed_metric_with_degenerate_input_arrays()
sklearn.cluster.tests.test_dbscan.test_dbscan_precomputed_metric_with_initial_rows_zero()
sklearn.cluster.tests.test_dbscan.test_dbscan_similarity()
sklearn.cluster.tests.test_dbscan.test_dbscan_sparse()
sklearn.cluster.tests.test_dbscan.test_dbscan_sparse_precomputed()
sklearn.cluster.tests.test_dbscan.test_input_validation()
sklearn.cluster.tests.test_dbscan.test_pickle()
sklearn.cluster.tests.test_dbscan.test_weighted_dbscan()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/cluster/tests/test_affinity_propagation.py----------------------------------------
A:sklearn.cluster.tests.test_affinity_propagation.(X, _)->make_blobs(n_samples=60, n_features=2, centers=centers, cluster_std=0.4, shuffle=True, random_state=0)
A:sklearn.cluster.tests.test_affinity_propagation.(cluster_centers_indices, labels)->affinity_propagation(S, preference=preference)
A:sklearn.cluster.tests.test_affinity_propagation.n_clusters_->len(cluster_centers_indices)
A:sklearn.cluster.tests.test_affinity_propagation.af->AffinityPropagation(affinity='precomputed')
A:sklearn.cluster.tests.test_affinity_propagation.(_, labels_no_copy)->affinity_propagation(S, preference=preference, copy=False)
A:sklearn.cluster.tests.test_affinity_propagation.labels->AffinityPropagation(affinity='precomputed').fit_predict(X)
A:sklearn.cluster.tests.test_affinity_propagation.labels2->AffinityPropagation(affinity='precomputed').predict(X)
A:sklearn.cluster.tests.test_affinity_propagation.S->numpy.dot(X, X.T)
sklearn.cluster.tests.test_affinity_propagation.test_affinity_propagation()
sklearn.cluster.tests.test_affinity_propagation.test_affinity_propagation_predict()
sklearn.cluster.tests.test_affinity_propagation.test_affinity_propagation_predict_error()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/cluster/tests/test_spectral.py----------------------------------------
A:sklearn.cluster.tests.test_spectral.S->scipy.sparse.coo_matrix(S)
A:sklearn.cluster.tests.test_spectral.model->SpectralClustering(random_state=0, n_clusters=2, affinity='precomputed', eigen_solver=eigen_solver, assign_labels=assign_labels).fit(mat)
A:sklearn.cluster.tests.test_spectral.model_copy->loads(dumps(model))
A:sklearn.cluster.tests.test_spectral.centers->numpy.array([[0.0, 0.0, 0.0], [10.0, 10.0, 10.0], [20.0, 20.0, 20.0]])
A:sklearn.cluster.tests.test_spectral.(X, true_labels)->make_blobs(n_samples=100, centers=centers, cluster_std=1.0, random_state=42)
A:sklearn.cluster.tests.test_spectral.D->pairwise_distances(X)
A:sklearn.cluster.tests.test_spectral.labels->spectral_clustering(S, n_clusters=len(centers), random_state=0, eigen_solver='amg')
A:sklearn.cluster.tests.test_spectral.(X, y)->make_blobs(n_samples=20, random_state=0, centers=[[1, 1], [-1, -1]], cluster_std=0.01)
A:sklearn.cluster.tests.test_spectral.sp->SpectralClustering(n_clusters=2, affinity='<unknown>')
A:sklearn.cluster.tests.test_spectral.kernels_available->kernel_metrics()
A:sklearn.cluster.tests.test_spectral.random_state->numpy.random.RandomState(seed)
A:sklearn.cluster.tests.test_spectral.y_true->numpy.array(y_true, np.float)
A:sklearn.cluster.tests.test_spectral.y_indicator->scipy.sparse.coo_matrix((np.ones(n_samples), (np.arange(n_samples), y_true)), shape=(n_samples, n_class + 1))
A:sklearn.cluster.tests.test_spectral.y_pred->discretize(y_true_noisy, random_state)
sklearn.cluster.tests.test_spectral.test_affinities()
sklearn.cluster.tests.test_spectral.test_discretize(seed=8)
sklearn.cluster.tests.test_spectral.test_spectral_amg_mode()
sklearn.cluster.tests.test_spectral.test_spectral_clustering()
sklearn.cluster.tests.test_spectral.test_spectral_clustering_sparse()
sklearn.cluster.tests.test_spectral.test_spectral_unknown_assign_labels()
sklearn.cluster.tests.test_spectral.test_spectral_unknown_mode()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/cluster/tests/common.py----------------------------------------
A:sklearn.cluster.tests.common.prng->numpy.random.RandomState(seed)
A:sklearn.cluster.tests.common.X->numpy.empty((0, n_features))
sklearn.cluster.tests.common.generate_clustered_data(seed=0,n_clusters=3,n_features=2,n_samples_per_cluster=20,std=0.4)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/cluster/tests/test_hierarchical.py----------------------------------------
A:sklearn.cluster.tests.test_hierarchical.rng->numpy.random.RandomState(0)
A:sklearn.cluster.tests.test_hierarchical.X->numpy.random.RandomState(0).randn(size, size)
A:sklearn.cluster.tests.test_hierarchical.(children, n_nodes, n_leaves, parent)->linkage_func(X.T, connectivity)
A:sklearn.cluster.tests.test_hierarchical.(children_t, n_nodes_t, n_leaves_t, parent_t)->linkage_tree(X.T)
A:sklearn.cluster.tests.test_hierarchical.dis->cosine_distances(X)
A:sklearn.cluster.tests.test_hierarchical.res->linkage_tree(X, affinity=manhattan_distances)
A:sklearn.cluster.tests.test_hierarchical.mask->numpy.array([True, False, False, True])
A:sklearn.cluster.tests.test_hierarchical.connectivity->grid_to_graph(n_x=size, n_y=size, mask=mask, return_as=np.ndarray)
A:sklearn.cluster.tests.test_hierarchical.(children, n_components, n_leaves, parent)->tree_builder(X.T, connectivity)
A:sklearn.cluster.tests.test_hierarchical.clustering->AgglomerativeClustering(n_clusters=10, connectivity=connectivity, linkage='complete')
A:sklearn.cluster.tests.test_hierarchical.tempdir->mkdtemp()
A:sklearn.cluster.tests.test_hierarchical.clustering2->AgglomerativeClustering(n_clusters=10, connectivity=connectivity, affinity='precomputed', linkage='complete')
A:sklearn.cluster.tests.test_hierarchical.X_dist->pairwise_distances(X)
A:sklearn.cluster.tests.test_hierarchical.agglo->FeatureAgglomeration(n_clusters=5, connectivity=connectivity)
A:sklearn.cluster.tests.test_hierarchical.X_red->FeatureAgglomeration(n_clusters=5, connectivity=connectivity).transform(X)
A:sklearn.cluster.tests.test_hierarchical.X_full->FeatureAgglomeration(n_clusters=5, connectivity=connectivity).inverse_transform(X_red)
A:sklearn.cluster.tests.test_hierarchical.n->len(cut)
A:sklearn.cluster.tests.test_hierarchical.ecut->numpy.zeros((n, k))
A:sklearn.cluster.tests.test_hierarchical.out->scipy.cluster.hierarchy.linkage(X, method=linkage)
A:sklearn.cluster.tests.test_hierarchical.children_->out[:, :2].astype(np.int)
A:sklearn.cluster.tests.test_hierarchical.(children, _, n_leaves, _)->_TREE_BUILDERS[linkage](X, connectivity)
A:sklearn.cluster.tests.test_hierarchical.cut->_hc_cut(k, children, n_leaves)
A:sklearn.cluster.tests.test_hierarchical.cut_->_hc_cut(k, children_, n_leaves)
A:sklearn.cluster.tests.test_hierarchical.ward->AgglomerativeClustering(n_clusters=4, connectivity=connectivity, linkage='ward')
A:sklearn.cluster.tests.test_hierarchical.out_unstructured->ward_tree(X, return_distance=True)
A:sklearn.cluster.tests.test_hierarchical.out_structured->ward_tree(X, connectivity=connectivity, return_distance=True)
A:sklearn.cluster.tests.test_hierarchical.linkage_X_ward->numpy.array([[3.0, 4.0, 0.36265956, 2.0], [1.0, 5.0, 1.77045373, 2.0], [0.0, 2.0, 2.55760419, 2.0], [6.0, 8.0, 9.10208346, 4.0], [7.0, 9.0, 24.7784379, 6.0]])
A:sklearn.cluster.tests.test_hierarchical.linkage_X_complete->numpy.array([[3.0, 4.0, 0.36265956, 2.0], [1.0, 5.0, 1.77045373, 2.0], [0.0, 2.0, 2.55760419, 2.0], [6.0, 8.0, 6.96742194, 4.0], [7.0, 9.0, 18.77445997, 6.0]])
A:sklearn.cluster.tests.test_hierarchical.linkage_X_average->numpy.array([[3.0, 4.0, 0.36265956, 2.0], [1.0, 5.0, 1.77045373, 2.0], [0.0, 2.0, 2.55760419, 2.0], [6.0, 8.0, 6.55832839, 4.0], [7.0, 9.0, 15.44089605, 6.0]])
A:sklearn.cluster.tests.test_hierarchical.(n_samples, n_features)->numpy.shape(X)
A:sklearn.cluster.tests.test_hierarchical.connectivity_X->numpy.ones((n_samples, n_samples))
A:sklearn.cluster.tests.test_hierarchical.out_X_unstructured->linkage_tree(X, return_distance=True, linkage=linkage)
A:sklearn.cluster.tests.test_hierarchical.out_X_structured->linkage_tree(X, connectivity=connectivity_X, linkage=linkage, return_distance=True)
A:sklearn.cluster.tests.test_hierarchical.x->numpy.array([[0, 0], [1, 1]])
A:sklearn.cluster.tests.test_hierarchical.m->numpy.array([[True, False], [False, True]])
A:sklearn.cluster.tests.test_hierarchical.c->grid_to_graph(n_x=2, n_y=2, mask=m)
A:sklearn.cluster.tests.test_hierarchical.w->AgglomerativeClustering(connectivity=c, linkage='ward')
A:sklearn.cluster.tests.test_hierarchical.keys->numpy.unique(rng.randint(100, size=10).astype(np.intp))
A:sklearn.cluster.tests.test_hierarchical.values->numpy.random.RandomState(0).rand(len(keys))
A:sklearn.cluster.tests.test_hierarchical.d->IntFloatDict(keys, values)
A:sklearn.cluster.tests.test_hierarchical.other->IntFloatDict(other_keys, other_values)
A:sklearn.cluster.tests.test_hierarchical.aglc1->AgglomerativeClustering(connectivity=connectivity)
A:sklearn.cluster.tests.test_hierarchical.aglc2->AgglomerativeClustering(connectivity=connectivity_include_self)
A:sklearn.cluster.tests.test_hierarchical.connectivity_include_self->kneighbors_graph(X, 3, include_self=True)
A:sklearn.cluster.tests.test_hierarchical.agc->AgglomerativeClustering(n_clusters=n_clus)
A:sklearn.cluster.tests.test_hierarchical.fa->FakeAffinity()
sklearn.cluster.tests.test_hierarchical.assess_same_labelling(cut1,cut2)
sklearn.cluster.tests.test_hierarchical.test_affinity_passed_to_fix_connectivity()
sklearn.cluster.tests.test_hierarchical.test_agg_n_clusters()
sklearn.cluster.tests.test_hierarchical.test_agglomerative_clustering()
sklearn.cluster.tests.test_hierarchical.test_agglomerative_clustering_wrong_arg_memory()
sklearn.cluster.tests.test_hierarchical.test_compute_full_tree()
sklearn.cluster.tests.test_hierarchical.test_connectivity_callable()
sklearn.cluster.tests.test_hierarchical.test_connectivity_fixing_non_lil()
sklearn.cluster.tests.test_hierarchical.test_connectivity_ignores_diagonal()
sklearn.cluster.tests.test_hierarchical.test_connectivity_propagation()
sklearn.cluster.tests.test_hierarchical.test_deprecation_of_n_components_in_linkage_tree()
sklearn.cluster.tests.test_hierarchical.test_height_linkage_tree()
sklearn.cluster.tests.test_hierarchical.test_int_float_dict()
sklearn.cluster.tests.test_hierarchical.test_linkage_misc()
sklearn.cluster.tests.test_hierarchical.test_n_components()
sklearn.cluster.tests.test_hierarchical.test_scikit_vs_scipy()
sklearn.cluster.tests.test_hierarchical.test_structured_linkage_tree()
sklearn.cluster.tests.test_hierarchical.test_unstructured_linkage_tree()
sklearn.cluster.tests.test_hierarchical.test_ward_agglomeration()
sklearn.cluster.tests.test_hierarchical.test_ward_linkage_tree_return_distance()
sklearn.cluster.tests.test_hierarchical.test_ward_tree_children_order()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/cluster/tests/test_bicluster.py----------------------------------------
A:sklearn.cluster.tests.test_bicluster.data->numpy.arange(27).reshape((3, 3, 3))
A:sklearn.cluster.tests.test_bicluster.model->SpectralBiclustering()
A:sklearn.cluster.tests.test_bicluster.submatrix->submatrix.toarray().toarray()
A:sklearn.cluster.tests.test_bicluster.X->numpy.random.RandomState(0).rand(100, 100)
A:sklearn.cluster.tests.test_bicluster.(m, n)->SpectralBiclustering().get_shape(i)
A:sklearn.cluster.tests.test_bicluster.(i_ind, j_ind)->SpectralBiclustering().get_indices(i)
A:sklearn.cluster.tests.test_bicluster.(S, rows, cols)->make_checkerboard((30, 40), 3, noise=0, random_state=0)
A:sklearn.cluster.tests.test_bicluster.S->numpy.where(S < 1, 0, S)
A:sklearn.cluster.tests.test_bicluster.row_sum->numpy.asarray(row_sum).squeeze()
A:sklearn.cluster.tests.test_bicluster.col_sum->numpy.asarray(col_sum).squeeze()
A:sklearn.cluster.tests.test_bicluster.generator->numpy.random.RandomState(0)
A:sklearn.cluster.tests.test_bicluster.(scaled, _, _)->_scale_normalize(mat)
A:sklearn.cluster.tests.test_bicluster.scaled->_bistochastic_normalize(mat)
A:sklearn.cluster.tests.test_bicluster.mat->numpy.random.RandomState(0).rand(100, 100)
A:sklearn.cluster.tests.test_bicluster.vectors->numpy.array([[1, 0], [0, 1], [0, 0]])
A:sklearn.cluster.tests.test_bicluster.best->SpectralBiclustering()._fit_best_piecewise(vectors, n_best=2, n_clusters=2)
A:sklearn.cluster.tests.test_bicluster.labels->SpectralBiclustering()._project_and_cluster(data, vectors, n_clusters=2)
sklearn.cluster.tests.test_bicluster.MockBiclustering(self)
sklearn.cluster.tests.test_bicluster.MockBiclustering.__init__(self)
sklearn.cluster.tests.test_bicluster.MockBiclustering.get_indices(self,i)
sklearn.cluster.tests.test_bicluster._do_bistochastic_test(scaled)
sklearn.cluster.tests.test_bicluster._do_scale_test(scaled)
sklearn.cluster.tests.test_bicluster._test_shape_indices(model)
sklearn.cluster.tests.test_bicluster.test_bistochastic_normalize()
sklearn.cluster.tests.test_bicluster.test_errors()
sklearn.cluster.tests.test_bicluster.test_fit_best_piecewise()
sklearn.cluster.tests.test_bicluster.test_get_submatrix()
sklearn.cluster.tests.test_bicluster.test_log_normalize()
sklearn.cluster.tests.test_bicluster.test_perfect_checkerboard()
sklearn.cluster.tests.test_bicluster.test_project_and_cluster()
sklearn.cluster.tests.test_bicluster.test_scale_normalize()
sklearn.cluster.tests.test_bicluster.test_spectral_biclustering()
sklearn.cluster.tests.test_bicluster.test_spectral_coclustering()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/cluster/tests/test_mean_shift.py----------------------------------------
A:sklearn.cluster.tests.test_mean_shift.(X, _)->make_blobs(n_samples=100, n_features=2, centers=[[0, 0], [1, 1]], cluster_std=0.1, random_state=0)
A:sklearn.cluster.tests.test_mean_shift.bandwidth->estimate_bandwidth(X, n_samples=200)
A:sklearn.cluster.tests.test_mean_shift.ms->MeanShift()
A:sklearn.cluster.tests.test_mean_shift.labels_unique->numpy.unique(labels)
A:sklearn.cluster.tests.test_mean_shift.n_clusters_->len(labels_unique)
A:sklearn.cluster.tests.test_mean_shift.(cluster_centers, labels)->mean_shift(X, bandwidth=bandwidth)
A:sklearn.cluster.tests.test_mean_shift.X->numpy.array([[1.0, 1.0], [1.4, 1.4], [1.8, 1.2], [2.0, 1.0], [2.1, 1.1], [0.0, 0.0]])
A:sklearn.cluster.tests.test_mean_shift.ms1->MeanShift(n_jobs=2)
A:sklearn.cluster.tests.test_mean_shift.ms2->MeanShift()
A:sklearn.cluster.tests.test_mean_shift.labels->MeanShift().fit_predict(X)
A:sklearn.cluster.tests.test_mean_shift.labels2->MeanShift().predict(X)
A:sklearn.cluster.tests.test_mean_shift.ground_truth->set([(1.0, 1.0), (2.0, 1.0)])
A:sklearn.cluster.tests.test_mean_shift.test_bins->get_bin_seeds(X, 1)
A:sklearn.cluster.tests.test_mean_shift.test_result->set([tuple(p) for p in test_bins])
sklearn.cluster.tests.test_mean_shift.test_bin_seeds()
sklearn.cluster.tests.test_mean_shift.test_estimate_bandwidth()
sklearn.cluster.tests.test_mean_shift.test_estimate_bandwidth_with_sparse_matrix()
sklearn.cluster.tests.test_mean_shift.test_mean_shift()
sklearn.cluster.tests.test_mean_shift.test_meanshift_all_orphans()
sklearn.cluster.tests.test_mean_shift.test_meanshift_predict()
sklearn.cluster.tests.test_mean_shift.test_parallel()
sklearn.cluster.tests.test_mean_shift.test_unfitted()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/metrics/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/metrics/scorer.py----------------------------------------
A:sklearn.metrics.scorer.kwargs_string->''.join([', %s=%s' % (str(k), str(v)) for (k, v) in self._kwargs.items()])
A:sklearn.metrics.scorer.y_pred->clf.predict_proba(X)
A:sklearn.metrics.scorer.y_type->type_of_target(y)
A:sklearn.metrics.scorer.module->getattr(scoring, '__module__', None)
A:sklearn.metrics.scorer.keys->set(scoring)
A:sklearn.metrics.scorer.explained_variance_scorer->make_scorer(explained_variance_score)
A:sklearn.metrics.scorer.r2_scorer->make_scorer(r2_score)
A:sklearn.metrics.scorer.neg_mean_squared_error_scorer->make_scorer(mean_squared_error, greater_is_better=False)
A:sklearn.metrics.scorer.mean_squared_error_scorer->make_scorer(mean_squared_error, greater_is_better=False)
A:sklearn.metrics.scorer.neg_mean_squared_log_error_scorer->make_scorer(mean_squared_log_error, greater_is_better=False)
A:sklearn.metrics.scorer.neg_mean_absolute_error_scorer->make_scorer(mean_absolute_error, greater_is_better=False)
A:sklearn.metrics.scorer.mean_absolute_error_scorer->make_scorer(mean_absolute_error, greater_is_better=False)
A:sklearn.metrics.scorer.neg_median_absolute_error_scorer->make_scorer(median_absolute_error, greater_is_better=False)
A:sklearn.metrics.scorer.median_absolute_error_scorer->make_scorer(median_absolute_error, greater_is_better=False)
A:sklearn.metrics.scorer.accuracy_scorer->make_scorer(accuracy_score)
A:sklearn.metrics.scorer.f1_scorer->make_scorer(f1_score)
A:sklearn.metrics.scorer.roc_auc_scorer->make_scorer(roc_auc_score, greater_is_better=True, needs_threshold=True)
A:sklearn.metrics.scorer.average_precision_scorer->make_scorer(average_precision_score, needs_threshold=True)
A:sklearn.metrics.scorer.precision_scorer->make_scorer(precision_score)
A:sklearn.metrics.scorer.recall_scorer->make_scorer(recall_score)
A:sklearn.metrics.scorer.neg_log_loss_scorer->make_scorer(log_loss, greater_is_better=False, needs_proba=True)
A:sklearn.metrics.scorer.log_loss_scorer->make_scorer(log_loss, greater_is_better=False, needs_proba=True)
A:sklearn.metrics.scorer.adjusted_rand_scorer->make_scorer(adjusted_rand_score)
A:sklearn.metrics.scorer.homogeneity_scorer->make_scorer(homogeneity_score)
A:sklearn.metrics.scorer.completeness_scorer->make_scorer(completeness_score)
A:sklearn.metrics.scorer.v_measure_scorer->make_scorer(v_measure_score)
A:sklearn.metrics.scorer.mutual_info_scorer->make_scorer(mutual_info_score)
A:sklearn.metrics.scorer.adjusted_mutual_info_scorer->make_scorer(adjusted_mutual_info_score)
A:sklearn.metrics.scorer.normalized_mutual_info_scorer->make_scorer(normalized_mutual_info_score)
A:sklearn.metrics.scorer.fowlkes_mallows_scorer->make_scorer(fowlkes_mallows_score)
A:sklearn.metrics.scorer.SCORERS->dict(explained_variance=explained_variance_scorer, r2=r2_scorer, neg_median_absolute_error=neg_median_absolute_error_scorer, neg_mean_absolute_error=neg_mean_absolute_error_scorer, neg_mean_squared_error=neg_mean_squared_error_scorer, neg_mean_squared_log_error=neg_mean_squared_log_error_scorer, median_absolute_error=median_absolute_error_scorer, mean_absolute_error=mean_absolute_error_scorer, mean_squared_error=mean_squared_error_scorer, accuracy=accuracy_scorer, roc_auc=roc_auc_scorer, average_precision=average_precision_scorer, log_loss=log_loss_scorer, neg_log_loss=neg_log_loss_scorer, adjusted_rand_score=adjusted_rand_scorer, homogeneity_score=homogeneity_scorer, completeness_score=completeness_scorer, v_measure_score=v_measure_scorer, mutual_info_score=mutual_info_scorer, adjusted_mutual_info_score=adjusted_mutual_info_scorer, normalized_mutual_info_score=normalized_mutual_info_scorer, fowlkes_mallows_score=fowlkes_mallows_scorer)
A:sklearn.metrics.scorer.SCORERS[name]->make_scorer(metric)
A:sklearn.metrics.scorer.qualified_name->'{0}_{1}'.format(name, average)
A:sklearn.metrics.scorer.SCORERS[qualified_name]->make_scorer(metric, pos_label=None, average=average)
sklearn.metrics.get_scorer(scoring)
sklearn.metrics.make_scorer(score_func,greater_is_better=True,needs_proba=False,needs_threshold=False,**kwargs)
sklearn.metrics.scorer._BaseScorer(self,score_func,sign,kwargs)
sklearn.metrics.scorer._BaseScorer.__init__(self,score_func,sign,kwargs)
sklearn.metrics.scorer._BaseScorer.__repr__(self)
sklearn.metrics.scorer._BaseScorer._factory_args(self)
sklearn.metrics.scorer._PredictScorer(self,estimator,X,y_true,sample_weight=None)
sklearn.metrics.scorer._PredictScorer.__call__(self,estimator,X,y_true,sample_weight=None)
sklearn.metrics.scorer._ProbaScorer(self,clf,X,y,sample_weight=None)
sklearn.metrics.scorer._ProbaScorer.__call__(self,clf,X,y,sample_weight=None)
sklearn.metrics.scorer._ProbaScorer._factory_args(self)
sklearn.metrics.scorer._ThresholdScorer(self,clf,X,y,sample_weight=None)
sklearn.metrics.scorer._ThresholdScorer.__call__(self,clf,X,y,sample_weight=None)
sklearn.metrics.scorer._ThresholdScorer._factory_args(self)
sklearn.metrics.scorer._check_multimetric_scoring(estimator,scoring=None)
sklearn.metrics.scorer._passthrough_scorer(estimator,*args,**kwargs)
sklearn.metrics.scorer.check_scoring(estimator,scoring=None,allow_none=False)
sklearn.metrics.scorer.get_scorer(scoring)
sklearn.metrics.scorer.make_scorer(score_func,greater_is_better=True,needs_proba=False,needs_threshold=False,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/metrics/classification.py----------------------------------------
A:sklearn.metrics.classification.type_true->type_of_target(y_true)
A:sklearn.metrics.classification.type_pred->type_of_target(y_pred)
A:sklearn.metrics.classification.y_type->y_type.pop().pop()
A:sklearn.metrics.classification.y_true->_check_binary_probabilistic_predictions(y_true, y_prob)
A:sklearn.metrics.classification.y_pred->numpy.append(1 - y_pred, y_pred, axis=1)
A:sklearn.metrics.classification.unique_values->numpy.union1d(y_true, y_pred)
A:sklearn.metrics.classification.(y_type, y_true, y_pred)->_check_targets(y_true, y_pred)
A:sklearn.metrics.classification.differing_labels->count_nonzero(y_true - y_pred, axis=1)
A:sklearn.metrics.classification.labels->numpy.unique(y_true)
A:sklearn.metrics.classification.sample_weight->numpy.asarray(sample_weight)
A:sklearn.metrics.classification.label_to_ind->dict(((y, x) for (x, y) in enumerate(labels)))
A:sklearn.metrics.classification.ind->numpy.logical_and(y_pred < n_labels, y_true < n_labels)
A:sklearn.metrics.classification.CM->coo_matrix((sample_weight, (y_true, y_pred)), shape=(n_labels, n_labels), dtype=dtype).toarray()
A:sklearn.metrics.classification.confusion->confusion_matrix(y1, y2, labels=labels, sample_weight=sample_weight)
A:sklearn.metrics.classification.sum0->numpy.sum(confusion, axis=0)
A:sklearn.metrics.classification.sum1->numpy.sum(confusion, axis=1)
A:sklearn.metrics.classification.w_mat->numpy.abs(w_mat - w_mat.T)
A:sklearn.metrics.classification.pred_or_true->count_nonzero(y_true + y_pred, axis=1)
A:sklearn.metrics.classification.pred_and_true->count_nonzero(y_true.multiply(y_pred), axis=1)
A:sklearn.metrics.classification.lb->LabelBinarizer()
A:sklearn.metrics.classification.C->confusion_matrix(y_true, y_pred, sample_weight=sample_weight)
A:sklearn.metrics.classification.t_sum->confusion_matrix(y_true, y_pred, sample_weight=sample_weight).sum(axis=1, dtype=np.float64)
A:sklearn.metrics.classification.p_sum->confusion_matrix(y_true, y_pred, sample_weight=sample_weight).sum(axis=0, dtype=np.float64)
A:sklearn.metrics.classification.n_correct->numpy.trace(C, dtype=np.float64)
A:sklearn.metrics.classification.n_samples->_num_samples(y_true)
A:sklearn.metrics.classification.score->accuracy_score(y_true, y_pred, normalize=normalize, sample_weight=sample_weight)
A:sklearn.metrics.classification.(_, _, f, _)->precision_recall_fscore_support(y_true, y_pred, beta=beta, labels=labels, pos_label=pos_label, average=average, warn_for=('f-score',), sample_weight=sample_weight)
A:sklearn.metrics.classification.msg_start->'{0} is'.format(metric.title())
A:sklearn.metrics.classification.msg->msg.format('in {0}s with'.format(axis1)).format('in {0}s with'.format(axis1))
A:sklearn.metrics.classification.present_labels->unique_labels(y_true, y_pred)
A:sklearn.metrics.classification.n_labels->len(labels)
A:sklearn.metrics.classification.true_and_pred->_check_binary_probabilistic_predictions(y_true, y_prob).multiply(y_pred)
A:sklearn.metrics.classification.tp_sum->numpy.array([tp_sum.sum()])
A:sklearn.metrics.classification.pred_sum->numpy.array([pred_sum.sum()])
A:sklearn.metrics.classification.true_sum->numpy.array([true_sum.sum()])
A:sklearn.metrics.classification.le->LabelEncoder()
A:sklearn.metrics.classification.true_sumpred_sumtp_sum->numpy.zeros(len(labels))
A:sklearn.metrics.classification.indices->numpy.searchsorted(sorted_labels, labels[:n_labels])
A:sklearn.metrics.classification.precision->numpy.average(precision, weights=weights)
A:sklearn.metrics.classification.recall->numpy.average(recall, weights=weights)
A:sklearn.metrics.classification.f_score->numpy.average(f_score, weights=weights)
A:sklearn.metrics.classification.(p, _, _, _)->precision_recall_fscore_support(y_true, y_pred, labels=labels, pos_label=pos_label, average=average, warn_for=('precision',), sample_weight=sample_weight)
A:sklearn.metrics.classification.(_, r, _, _)->precision_recall_fscore_support(y_true, y_pred, labels=labels, pos_label=pos_label, average=average, warn_for=('recall',), sample_weight=sample_weight)
A:sklearn.metrics.classification.name_width->max((len(cn) for cn in target_names))
A:sklearn.metrics.classification.width->max(name_width, len(last_line_heading), digits)
A:sklearn.metrics.classification.report->head_fmt.format(u'', *headers, width=width)
A:sklearn.metrics.classification.(p, r, f1, s)->precision_recall_fscore_support(y_true, y_pred, labels=labels, average=None, sample_weight=sample_weight)
A:sklearn.metrics.classification.rows->zip(target_names, p, r, f1, s)
A:sklearn.metrics.classification.weight_average->numpy.mean(sample_weight)
A:sklearn.metrics.classification.n_differences->count_nonzero(y_true - y_pred, sample_weight=sample_weight)
A:sklearn.metrics.classification.transformed_labels->check_array(transformed_labels)
A:sklearn.metrics.classification.pred_decision->numpy.ravel(pred_decision)
A:sklearn.metrics.classification.y_true_unique->numpy.unique(y_true)
A:sklearn.metrics.classification.mask->numpy.ones_like(pred_decision, dtype=bool)
A:sklearn.metrics.classification.lbin->LabelBinarizer(neg_label=-1)
A:sklearn.metrics.classification.y_prob->column_or_1d(y_prob)
A:sklearn.metrics.classification.pos_label->_check_binary_probabilistic_predictions(y_true, y_prob).max()
sklearn.metrics.accuracy_score(y_true,y_pred,normalize=True,sample_weight=None)
sklearn.metrics.brier_score_loss(y_true,y_prob,sample_weight=None,pos_label=None)
sklearn.metrics.classification._check_binary_probabilistic_predictions(y_true,y_prob)
sklearn.metrics.classification._check_targets(y_true,y_pred)
sklearn.metrics.classification._prf_divide(numerator,denominator,metric,modifier,average,warn_for)
sklearn.metrics.classification._weighted_sum(sample_score,sample_weight,normalize=False)
sklearn.metrics.classification.accuracy_score(y_true,y_pred,normalize=True,sample_weight=None)
sklearn.metrics.classification.brier_score_loss(y_true,y_prob,sample_weight=None,pos_label=None)
sklearn.metrics.classification.classification_report(y_true,y_pred,labels=None,target_names=None,sample_weight=None,digits=2)
sklearn.metrics.classification.cohen_kappa_score(y1,y2,labels=None,weights=None,sample_weight=None)
sklearn.metrics.classification.confusion_matrix(y_true,y_pred,labels=None,sample_weight=None)
sklearn.metrics.classification.f1_score(y_true,y_pred,labels=None,pos_label=1,average='binary',sample_weight=None)
sklearn.metrics.classification.fbeta_score(y_true,y_pred,beta,labels=None,pos_label=1,average='binary',sample_weight=None)
sklearn.metrics.classification.hamming_loss(y_true,y_pred,labels=None,sample_weight=None,classes=None)
sklearn.metrics.classification.hinge_loss(y_true,pred_decision,labels=None,sample_weight=None)
sklearn.metrics.classification.jaccard_similarity_score(y_true,y_pred,normalize=True,sample_weight=None)
sklearn.metrics.classification.log_loss(y_true,y_pred,eps=1e-15,normalize=True,sample_weight=None,labels=None)
sklearn.metrics.classification.matthews_corrcoef(y_true,y_pred,sample_weight=None)
sklearn.metrics.classification.precision_recall_fscore_support(y_true,y_pred,beta=1.0,labels=None,pos_label=1,average=None,warn_for=('precision','recall','f-score'),sample_weight=None)
sklearn.metrics.classification.precision_score(y_true,y_pred,labels=None,pos_label=1,average='binary',sample_weight=None)
sklearn.metrics.classification.recall_score(y_true,y_pred,labels=None,pos_label=1,average='binary',sample_weight=None)
sklearn.metrics.classification.zero_one_loss(y_true,y_pred,normalize=True,sample_weight=None)
sklearn.metrics.classification_report(y_true,y_pred,labels=None,target_names=None,sample_weight=None,digits=2)
sklearn.metrics.cohen_kappa_score(y1,y2,labels=None,weights=None,sample_weight=None)
sklearn.metrics.confusion_matrix(y_true,y_pred,labels=None,sample_weight=None)
sklearn.metrics.f1_score(y_true,y_pred,labels=None,pos_label=1,average='binary',sample_weight=None)
sklearn.metrics.fbeta_score(y_true,y_pred,beta,labels=None,pos_label=1,average='binary',sample_weight=None)
sklearn.metrics.hamming_loss(y_true,y_pred,labels=None,sample_weight=None,classes=None)
sklearn.metrics.hinge_loss(y_true,pred_decision,labels=None,sample_weight=None)
sklearn.metrics.jaccard_similarity_score(y_true,y_pred,normalize=True,sample_weight=None)
sklearn.metrics.log_loss(y_true,y_pred,eps=1e-15,normalize=True,sample_weight=None,labels=None)
sklearn.metrics.matthews_corrcoef(y_true,y_pred,sample_weight=None)
sklearn.metrics.precision_recall_fscore_support(y_true,y_pred,beta=1.0,labels=None,pos_label=1,average=None,warn_for=('precision','recall','f-score'),sample_weight=None)
sklearn.metrics.precision_score(y_true,y_pred,labels=None,pos_label=1,average='binary',sample_weight=None)
sklearn.metrics.recall_score(y_true,y_pred,labels=None,pos_label=1,average='binary',sample_weight=None)
sklearn.metrics.zero_one_loss(y_true,y_pred,normalize=True,sample_weight=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/metrics/setup.py----------------------------------------
A:sklearn.metrics.setup.config->Configuration('metrics', parent_package, top_path)
A:sklearn.metrics.setup.(cblas_libs, blas_info)->get_blas_info()
sklearn.metrics.setup.configuration(parent_package='',top_path=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/metrics/base.py----------------------------------------
A:sklearn.metrics.base.y_type->type_of_target(y_true)
A:sklearn.metrics.base.y_true->y_true.reshape((-1, 1)).reshape((-1, 1))
A:sklearn.metrics.base.y_score->y_score.reshape((-1, 1)).reshape((-1, 1))
A:sklearn.metrics.base.score_weight->numpy.repeat(score_weight, y_true.shape[1])
A:sklearn.metrics.base.average_weight->numpy.sum(y_true, axis=0)
A:sklearn.metrics.base.score->numpy.zeros((n_classes,))
A:sklearn.metrics.base.y_true_c->y_true.reshape((-1, 1)).reshape((-1, 1)).take([c], axis=not_average_axis).ravel()
A:sklearn.metrics.base.y_score_c->y_score.reshape((-1, 1)).reshape((-1, 1)).take([c], axis=not_average_axis).ravel()
A:sklearn.metrics.base.score[c]->binary_metric(y_true_c, y_score_c, sample_weight=score_weight)
sklearn.metrics.base._average_binary_score(binary_metric,y_true,y_score,average,sample_weight=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/metrics/ranking.py----------------------------------------
A:sklearn.metrics.ranking.x->column_or_1d(x)
A:sklearn.metrics.ranking.y->column_or_1d(y)
A:sklearn.metrics.ranking.order->numpy.lexsort((y, x))
A:sklearn.metrics.ranking.dx->numpy.diff(x)
A:sklearn.metrics.ranking.area->area.dtype.type(area).dtype.type(area)
A:sklearn.metrics.ranking.(precision, recall, thresholds)->precision_recall_curve(y_true, y_score, sample_weight=sample_weight)
A:sklearn.metrics.ranking.(fpr, tpr, tresholds)->roc_curve(y_true, y_score, sample_weight=sample_weight)
A:sklearn.metrics.ranking.y_type->type_of_target(y_true)
A:sklearn.metrics.ranking.y_true->csr_matrix(y_true)
A:sklearn.metrics.ranking.y_score->check_array(y_score, ensure_2d=False)
A:sklearn.metrics.ranking.sample_weight->column_or_1d(sample_weight)
A:sklearn.metrics.ranking.classes->numpy.unique(y_true)
A:sklearn.metrics.ranking.(fps, tps, thresholds)->_binary_clf_curve(y_true, y_score, pos_label=pos_label, sample_weight=sample_weight)
A:sklearn.metrics.ranking.last_ind->tps.searchsorted(tps[-1])
A:sklearn.metrics.ranking.sl->slice(last_ind, None, -1)
A:sklearn.metrics.ranking.fpr->numpy.repeat(np.nan, fps.shape)
A:sklearn.metrics.ranking.tpr->numpy.repeat(np.nan, tps.shape)
A:sklearn.metrics.ranking.L->rankdata(scores_i[relevant], 'max')
A:sklearn.metrics.ranking.y_score_mask->numpy.ma.masked_array(y_score, mask=np.logical_not(y_true))
A:sklearn.metrics.ranking.y_min_relevant->numpy.ma.masked_array(y_score, mask=np.logical_not(y_true)).min(axis=1).reshape((-1, 1))
A:sklearn.metrics.ranking.coverage->coverage.filled(0).filled(0)
A:sklearn.metrics.ranking.loss->numpy.zeros(n_samples)
A:sklearn.metrics.ranking.(unique_scores, unique_inverse)->numpy.unique(y_score[i], return_inverse=True)
A:sklearn.metrics.ranking.true_at_reversed_rank->numpy.bincount(unique_inverse[y_true.indices[start:stop]], minlength=len(unique_scores))
A:sklearn.metrics.ranking.all_at_reversed_rank->numpy.bincount(unique_inverse, minlength=len(unique_scores))
A:sklearn.metrics.ranking.loss[i]->numpy.dot(true_at_reversed_rank.cumsum(), false_at_reversed_rank)
A:sklearn.metrics.ranking.n_positives->count_nonzero(y_true, axis=1)
sklearn.metrics.auc(x,y,reorder=False)
sklearn.metrics.average_precision_score(y_true,y_score,average='macro',sample_weight=None)
sklearn.metrics.coverage_error(y_true,y_score,sample_weight=None)
sklearn.metrics.label_ranking_average_precision_score(y_true,y_score)
sklearn.metrics.label_ranking_loss(y_true,y_score,sample_weight=None)
sklearn.metrics.precision_recall_curve(y_true,probas_pred,pos_label=None,sample_weight=None)
sklearn.metrics.ranking._binary_clf_curve(y_true,y_score,pos_label=None,sample_weight=None)
sklearn.metrics.ranking.auc(x,y,reorder=False)
sklearn.metrics.ranking.average_precision_score(y_true,y_score,average='macro',sample_weight=None)
sklearn.metrics.ranking.coverage_error(y_true,y_score,sample_weight=None)
sklearn.metrics.ranking.label_ranking_average_precision_score(y_true,y_score)
sklearn.metrics.ranking.label_ranking_loss(y_true,y_score,sample_weight=None)
sklearn.metrics.ranking.precision_recall_curve(y_true,probas_pred,pos_label=None,sample_weight=None)
sklearn.metrics.ranking.roc_auc_score(y_true,y_score,average='macro',sample_weight=None)
sklearn.metrics.ranking.roc_curve(y_true,y_score,pos_label=None,sample_weight=None,drop_intermediate=True)
sklearn.metrics.roc_auc_score(y_true,y_score,average='macro',sample_weight=None)
sklearn.metrics.roc_curve(y_true,y_score,pos_label=None,sample_weight=None,drop_intermediate=True)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/metrics/regression.py----------------------------------------
A:sklearn.metrics.regression.y_true->y_true.reshape((-1, 1)).reshape((-1, 1))
A:sklearn.metrics.regression.y_pred->y_pred.reshape((-1, 1)).reshape((-1, 1))
A:sklearn.metrics.regression.multioutput->check_array(multioutput, ensure_2d=False)
A:sklearn.metrics.regression.(y_type, y_true, y_pred, multioutput)->_check_reg_targets(y_true, y_pred, multioutput)
A:sklearn.metrics.regression.output_errors->numpy.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)
A:sklearn.metrics.regression.(y_type, y_true, y_pred, _)->_check_reg_targets(y_true, y_pred, 'uniform_average')
A:sklearn.metrics.regression.y_diff_avg->numpy.average(y_true - y_pred, weights=sample_weight, axis=0)
A:sklearn.metrics.regression.numerator->(weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)
A:sklearn.metrics.regression.y_true_avg->numpy.average(y_true, weights=sample_weight, axis=0)
A:sklearn.metrics.regression.denominator->(weight * (y_true - np.average(y_true, axis=0, weights=sample_weight)) ** 2).sum(axis=0, dtype=np.float64)
A:sklearn.metrics.regression.output_scores->numpy.ones([y_true.shape[1]])
A:sklearn.metrics.regression.sample_weight->column_or_1d(sample_weight)
sklearn.metrics.explained_variance_score(y_true,y_pred,sample_weight=None,multioutput='uniform_average')
sklearn.metrics.mean_absolute_error(y_true,y_pred,sample_weight=None,multioutput='uniform_average')
sklearn.metrics.mean_squared_error(y_true,y_pred,sample_weight=None,multioutput='uniform_average')
sklearn.metrics.mean_squared_log_error(y_true,y_pred,sample_weight=None,multioutput='uniform_average')
sklearn.metrics.median_absolute_error(y_true,y_pred)
sklearn.metrics.r2_score(y_true,y_pred,sample_weight=None,multioutput='uniform_average')
sklearn.metrics.regression._check_reg_targets(y_true,y_pred,multioutput)
sklearn.metrics.regression.explained_variance_score(y_true,y_pred,sample_weight=None,multioutput='uniform_average')
sklearn.metrics.regression.mean_absolute_error(y_true,y_pred,sample_weight=None,multioutput='uniform_average')
sklearn.metrics.regression.mean_squared_error(y_true,y_pred,sample_weight=None,multioutput='uniform_average')
sklearn.metrics.regression.mean_squared_log_error(y_true,y_pred,sample_weight=None,multioutput='uniform_average')
sklearn.metrics.regression.median_absolute_error(y_true,y_pred)
sklearn.metrics.regression.r2_score(y_true,y_pred,sample_weight=None,multioutput='uniform_average')


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/metrics/pairwise.py----------------------------------------
A:sklearn.metrics.pairwise.X->csr_matrix(X, copy=False)
A:sklearn.metrics.pairwise.Y->csr_matrix(Y, copy=False)
A:sklearn.metrics.pairwise.(X, Y, dtype_float)->_return_float_dtype(X, Y)
A:sklearn.metrics.pairwise.XY->check_array(X, accept_sparse='csr', dtype=dtype, warn_on_dtype=warn_on_dtype, estimator=estimator)
A:sklearn.metrics.pairwise.(X, Y)->check_pairwise_arrays(X, Y, dtype=dtype)
A:sklearn.metrics.pairwise.XX->check_array(X_norm_squared)
A:sklearn.metrics.pairwise.YY->numpy.atleast_2d(Y_norm_squared)
A:sklearn.metrics.pairwise.distances->numpy.zeros(len(X))
A:sklearn.metrics.pairwise.indices->numpy.empty(X.shape[0], dtype=np.intp)
A:sklearn.metrics.pairwise.values->numpy.empty(X.shape[0])
A:sklearn.metrics.pairwise.d_chunk->pairwise_distances(X_chunk, Y_chunk, metric=metric, **metric_kwargs)
A:sklearn.metrics.pairwise.min_indices->pairwise_distances(X_chunk, Y_chunk, metric=metric, **metric_kwargs).argmin(axis=1)
A:sklearn.metrics.pairwise.D->numpy.abs(D, D)
A:sklearn.metrics.pairwise.S->cosine_similarity(X, Y)
A:sklearn.metrics.pairwise.diff.data->numpy.abs(diff.data)
A:sklearn.metrics.pairwise.distances[i]->metric(X[i], Y[i])
A:sklearn.metrics.pairwise.K->additive_chi2_kernel(X, Y)
A:sklearn.metrics.pairwise.X_normalized->normalize(X, copy=True)
A:sklearn.metrics.pairwise.Y_normalized->normalize(Y, copy=True)
A:sklearn.metrics.pairwise.result->numpy.zeros((X.shape[0], Y.shape[0]), dtype=X.dtype)
A:sklearn.metrics.pairwise.n_jobs->max(cpu_count() + 1 + n_jobs, 1)
A:sklearn.metrics.pairwise.fd->delayed(func)
A:sklearn.metrics.pairwise.ret->Parallel(n_jobs=n_jobs, verbose=0)((fd(X, Y[s], **kwds) for s in gen_even_slices(Y.shape[0], n_jobs)))
A:sklearn.metrics.pairwise.out->numpy.empty((X.shape[0], Y.shape[0]), dtype='float')
A:sklearn.metrics.pairwise.iterator->itertools.product(range(X.shape[0]), range(Y.shape[0]))
A:sklearn.metrics.pairwise.out[i, j]->metric(X[i], Y[j], **kwds)
A:sklearn.metrics.pairwise.out[i, i]->metric(x, x, **kwds)
A:sklearn.metrics.pairwise.(X, _)->check_pairwise_arrays(X, Y, precomputed=True)
A:sklearn.metrics.pairwise.func->partial(_pairwise_callable, metric=metric, **kwds)
A:sklearn.metrics.pairwise.kwds->dict(((k, kwds[k]) for k in kwds if k in KERNEL_PARAMS[metric]))
sklearn.metrics.euclidean_distances(X,Y=None,Y_norm_squared=None,squared=False,X_norm_squared=None)
sklearn.metrics.pairwise._pairwise_callable(X,Y,metric,**kwds)
sklearn.metrics.pairwise._parallel_pairwise(X,Y,func,n_jobs,**kwds)
sklearn.metrics.pairwise._return_float_dtype(X,Y)
sklearn.metrics.pairwise.additive_chi2_kernel(X,Y=None)
sklearn.metrics.pairwise.check_paired_arrays(X,Y)
sklearn.metrics.pairwise.check_pairwise_arrays(X,Y,precomputed=False,dtype=None)
sklearn.metrics.pairwise.chi2_kernel(X,Y=None,gamma=1.0)
sklearn.metrics.pairwise.cosine_distances(X,Y=None)
sklearn.metrics.pairwise.cosine_similarity(X,Y=None,dense_output=True)
sklearn.metrics.pairwise.distance_metrics()
sklearn.metrics.pairwise.euclidean_distances(X,Y=None,Y_norm_squared=None,squared=False,X_norm_squared=None)
sklearn.metrics.pairwise.kernel_metrics()
sklearn.metrics.pairwise.laplacian_kernel(X,Y=None,gamma=None)
sklearn.metrics.pairwise.linear_kernel(X,Y=None)
sklearn.metrics.pairwise.manhattan_distances(X,Y=None,sum_over_features=True,size_threshold=None)
sklearn.metrics.pairwise.paired_cosine_distances(X,Y)
sklearn.metrics.pairwise.paired_distances(X,Y,metric='euclidean',**kwds)
sklearn.metrics.pairwise.paired_euclidean_distances(X,Y)
sklearn.metrics.pairwise.paired_manhattan_distances(X,Y)
sklearn.metrics.pairwise.pairwise_distances(X,Y=None,metric='euclidean',n_jobs=1,**kwds)
sklearn.metrics.pairwise.pairwise_distances_argmin(X,Y,axis=1,metric='euclidean',batch_size=500,metric_kwargs=None)
sklearn.metrics.pairwise.pairwise_distances_argmin_min(X,Y,axis=1,metric='euclidean',batch_size=500,metric_kwargs=None)
sklearn.metrics.pairwise.pairwise_kernels(X,Y=None,metric='linear',filter_params=False,n_jobs=1,**kwds)
sklearn.metrics.pairwise.polynomial_kernel(X,Y=None,degree=3,gamma=None,coef0=1)
sklearn.metrics.pairwise.rbf_kernel(X,Y=None,gamma=None)
sklearn.metrics.pairwise.sigmoid_kernel(X,Y=None,gamma=None,coef0=1)
sklearn.metrics.pairwise_distances(X,Y=None,metric='euclidean',n_jobs=1,**kwds)
sklearn.metrics.pairwise_distances_argmin(X,Y,axis=1,metric='euclidean',batch_size=500,metric_kwargs=None)
sklearn.metrics.pairwise_distances_argmin_min(X,Y,axis=1,metric='euclidean',batch_size=500,metric_kwargs=None)
sklearn.metrics.pairwise_kernels(X,Y=None,metric='linear',filter_params=False,n_jobs=1,**kwds)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/metrics/cluster/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/metrics/cluster/supervised.py----------------------------------------
A:sklearn.metrics.cluster.supervised.labels_true->numpy.asarray(labels_true)
A:sklearn.metrics.cluster.supervised.labels_pred->numpy.asarray(labels_pred)
A:sklearn.metrics.cluster.supervised.(classes, class_idx)->numpy.unique(labels_true, return_inverse=True)
A:sklearn.metrics.cluster.supervised.(clusters, cluster_idx)->numpy.unique(labels_pred, return_inverse=True)
A:sklearn.metrics.cluster.supervised.contingency->contingency.astype(np.float64).astype(np.float64)
A:sklearn.metrics.cluster.supervised.(labels_true, labels_pred)->check_clusterings(labels_true, labels_pred)
A:sklearn.metrics.cluster.supervised.sum_comb_c->sum((comb2(n_c) for n_c in np.ravel(contingency.sum(axis=1))))
A:sklearn.metrics.cluster.supervised.sum_comb_k->sum((comb2(n_k) for n_k in np.ravel(contingency.sum(axis=0))))
A:sklearn.metrics.cluster.supervised.sum_comb->sum((comb2(n_ij) for n_ij in contingency.data))
A:sklearn.metrics.cluster.supervised.entropy_C->entropy(labels_true)
A:sklearn.metrics.cluster.supervised.entropy_K->entropy(labels_pred)
A:sklearn.metrics.cluster.supervised.MI->mutual_info_score(None, None, contingency=contingency)
A:sklearn.metrics.cluster.supervised.(nzx, nzy)->numpy.nonzero(contingency)
A:sklearn.metrics.cluster.supervised.(nzx, nzy, nz_val)->scipy.sparse.find(contingency)
A:sklearn.metrics.cluster.supervised.contingency_sum->contingency.astype(np.float64).astype(np.float64).sum()
A:sklearn.metrics.cluster.supervised.pi->numpy.bincount(label_idx).astype(np.float64)
A:sklearn.metrics.cluster.supervised.pj->numpy.ravel(contingency.sum(axis=0))
A:sklearn.metrics.cluster.supervised.log_contingency_nm->numpy.log(nz_val)
A:sklearn.metrics.cluster.supervised.classes->numpy.unique(labels_true)
A:sklearn.metrics.cluster.supervised.clusters->numpy.unique(labels_pred)
A:sklearn.metrics.cluster.supervised.mi->mutual_info_score(labels_true, labels_pred, contingency=contingency)
A:sklearn.metrics.cluster.supervised.emi->expected_mutual_information(contingency, n_samples)
A:sklearn.metrics.cluster.supervised.c->contingency_matrix(labels_true, labels_pred, sparse=True)
A:sklearn.metrics.cluster.supervised.pi_sum->numpy.sum(pi)
sklearn.metrics.adjusted_mutual_info_score(labels_true,labels_pred)
sklearn.metrics.adjusted_rand_score(labels_true,labels_pred)
sklearn.metrics.cluster.contingency_matrix(labels_true,labels_pred,eps=None,sparse=False)
sklearn.metrics.cluster.entropy(labels)
sklearn.metrics.cluster.supervised.adjusted_mutual_info_score(labels_true,labels_pred)
sklearn.metrics.cluster.supervised.adjusted_rand_score(labels_true,labels_pred)
sklearn.metrics.cluster.supervised.check_clusterings(labels_true,labels_pred)
sklearn.metrics.cluster.supervised.comb2(n)
sklearn.metrics.cluster.supervised.completeness_score(labels_true,labels_pred)
sklearn.metrics.cluster.supervised.contingency_matrix(labels_true,labels_pred,eps=None,sparse=False)
sklearn.metrics.cluster.supervised.entropy(labels)
sklearn.metrics.cluster.supervised.fowlkes_mallows_score(labels_true,labels_pred,sparse=False)
sklearn.metrics.cluster.supervised.homogeneity_completeness_v_measure(labels_true,labels_pred)
sklearn.metrics.cluster.supervised.homogeneity_score(labels_true,labels_pred)
sklearn.metrics.cluster.supervised.mutual_info_score(labels_true,labels_pred,contingency=None)
sklearn.metrics.cluster.supervised.normalized_mutual_info_score(labels_true,labels_pred)
sklearn.metrics.cluster.supervised.v_measure_score(labels_true,labels_pred)
sklearn.metrics.completeness_score(labels_true,labels_pred)
sklearn.metrics.fowlkes_mallows_score(labels_true,labels_pred,sparse=False)
sklearn.metrics.homogeneity_completeness_v_measure(labels_true,labels_pred)
sklearn.metrics.homogeneity_score(labels_true,labels_pred)
sklearn.metrics.mutual_info_score(labels_true,labels_pred,contingency=None)
sklearn.metrics.normalized_mutual_info_score(labels_true,labels_pred)
sklearn.metrics.v_measure_score(labels_true,labels_pred)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/metrics/cluster/setup.py----------------------------------------
A:sklearn.metrics.cluster.setup.config->Configuration('metrics/cluster', parent_package, top_path)
sklearn.metrics.cluster.setup.configuration(parent_package='',top_path=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/metrics/cluster/bicluster.py----------------------------------------
A:sklearn.metrics.cluster.bicluster.(a_rows, a_cols)->map(checks, a)
A:sklearn.metrics.cluster.bicluster.(b_rows, b_cols)->map(checks, b)
A:sklearn.metrics.cluster.bicluster.(a_rows, a_cols, b_rows, b_cols)->_check_rows_and_columns(a, b)
A:sklearn.metrics.cluster.bicluster.result->numpy.array(list((list((similarity(a_rows[i], a_cols[i], b_rows[j], b_cols[j]) for j in range(n_b))) for i in range(n_a))))
A:sklearn.metrics.cluster.bicluster.matrix->_pairwise_similarity(a, b, similarity)
A:sklearn.metrics.cluster.bicluster.indices->linear_assignment(1.0 - matrix)
A:sklearn.metrics.cluster.bicluster.n_a->len(a[0])
A:sklearn.metrics.cluster.bicluster.n_b->len(b[0])
sklearn.metrics.cluster.bicluster._check_rows_and_columns(a,b)
sklearn.metrics.cluster.bicluster._jaccard(a_rows,a_cols,b_rows,b_cols)
sklearn.metrics.cluster.bicluster._pairwise_similarity(a,b,similarity)
sklearn.metrics.cluster.bicluster.consensus_score(a,b,similarity='jaccard')
sklearn.metrics.consensus_score(a,b,similarity='jaccard')


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/metrics/cluster/unsupervised.py----------------------------------------
A:sklearn.metrics.cluster.unsupervised.(X, labels)->check_X_y(X, labels)
A:sklearn.metrics.cluster.unsupervised.random_state->check_random_state(random_state)
A:sklearn.metrics.cluster.unsupervised.le->LabelEncoder()
A:sklearn.metrics.cluster.unsupervised.labels->LabelEncoder().fit_transform(labels)
A:sklearn.metrics.cluster.unsupervised.distances->pairwise_distances(X, metric=metric, **kwds)
A:sklearn.metrics.cluster.unsupervised.n_samples_per_label->numpy.bincount(labels, minlength=len(unique_labels))
A:sklearn.metrics.cluster.unsupervised.intra_clust_dists->numpy.zeros(distances.shape[0], dtype=distances.dtype)
A:sklearn.metrics.cluster.unsupervised.other_distances->numpy.mean(current_distances[:, other_mask], axis=1)
A:sklearn.metrics.cluster.unsupervised.inter_clust_dists[mask]->numpy.minimum(inter_clust_dists[mask], other_distances)
A:sklearn.metrics.cluster.unsupervised.n_labels->len(le.classes_)
A:sklearn.metrics.cluster.unsupervised.mean->numpy.mean(X, axis=0)
A:sklearn.metrics.cluster.unsupervised.mean_k->numpy.mean(cluster_k, axis=0)
sklearn.metrics.calinski_harabaz_score(X,labels)
sklearn.metrics.cluster.unsupervised.calinski_harabaz_score(X,labels)
sklearn.metrics.cluster.unsupervised.check_number_of_labels(n_labels,n_samples)
sklearn.metrics.cluster.unsupervised.silhouette_samples(X,labels,metric='euclidean',**kwds)
sklearn.metrics.cluster.unsupervised.silhouette_score(X,labels,metric='euclidean',sample_size=None,random_state=None,**kwds)
sklearn.metrics.silhouette_samples(X,labels,metric='euclidean',**kwds)
sklearn.metrics.silhouette_score(X,labels,metric='euclidean',sample_size=None,random_state=None,**kwds)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/metrics/cluster/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/metrics/cluster/tests/test_unsupervised.py----------------------------------------
A:sklearn.metrics.cluster.tests.test_unsupervised.dataset->sklearn.datasets.load_iris()
A:sklearn.metrics.cluster.tests.test_unsupervised.X_csr->csr_matrix(X_dense)
A:sklearn.metrics.cluster.tests.test_unsupervised.X_dok->scipy.sparse.dok_matrix(X_dense)
A:sklearn.metrics.cluster.tests.test_unsupervised.X_lil->scipy.sparse.lil_matrix(X_dense)
A:sklearn.metrics.cluster.tests.test_unsupervised.D->pairwise_distances(X, metric='euclidean')
A:sklearn.metrics.cluster.tests.test_unsupervised.score_precomputed->silhouette_score(D, y, metric='precomputed', sample_size=int(X.shape[0] / 2), random_state=0)
A:sklearn.metrics.cluster.tests.test_unsupervised.score_euclidean->silhouette_score(X, y, metric='euclidean', sample_size=int(X.shape[0] / 2), random_state=0)
A:sklearn.metrics.cluster.tests.test_unsupervised.labels->numpy.array([0, 1, 1, 1, 2, 2])
A:sklearn.metrics.cluster.tests.test_unsupervised.silhouette->silhouette_score(X, labels)
A:sklearn.metrics.cluster.tests.test_unsupervised.ss->silhouette_samples(X, labels)
A:sklearn.metrics.cluster.tests.test_unsupervised.y->numpy.zeros(X.shape[0])
A:sklearn.metrics.cluster.tests.test_unsupervised.rng->numpy.random.RandomState(seed=0)
sklearn.metrics.cluster.tests.test_unsupervised.test_calinski_harabaz_score()
sklearn.metrics.cluster.tests.test_unsupervised.test_cluster_size_1()
sklearn.metrics.cluster.tests.test_unsupervised.test_correct_labelsize()
sklearn.metrics.cluster.tests.test_unsupervised.test_non_encoded_labels()
sklearn.metrics.cluster.tests.test_unsupervised.test_non_numpy_labels()
sklearn.metrics.cluster.tests.test_unsupervised.test_silhouette()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/metrics/cluster/tests/test_supervised.py----------------------------------------
A:sklearn.metrics.cluster.tests.test_supervised.(h, c, v)->homogeneity_completeness_v_measure([0, 0, 0, 1, 1, 1], [0, 4, 0, 4, 2, 2])
A:sklearn.metrics.cluster.tests.test_supervised.ari_1->adjusted_rand_score([0, 0, 0, 1, 1, 1], [0, 1, 0, 1, 2, 2])
A:sklearn.metrics.cluster.tests.test_supervised.ari_2->adjusted_rand_score([0, 0, 0, 1, 1, 1], [0, 4, 0, 4, 2, 2])
A:sklearn.metrics.cluster.tests.test_supervised.scores->uniform_labelings_scores(adjusted_rand_score, n_samples, n_clusters_range, n_runs)
A:sklearn.metrics.cluster.tests.test_supervised.labels_a->numpy.array([0, 0, 0, 1, 1, 2])
A:sklearn.metrics.cluster.tests.test_supervised.labels_b->numpy.array([1, 1, 2, 2, 0, 0])
A:sklearn.metrics.cluster.tests.test_supervised.scores[i, j]->score_func(labels_a, labels_b)
A:sklearn.metrics.cluster.tests.test_supervised.max_abs_scores->numpy.abs(scores).max(axis=1)
A:sklearn.metrics.cluster.tests.test_supervised.mi->mutual_info_score(labels_a, labels_b, contingency=C)
A:sklearn.metrics.cluster.tests.test_supervised.C->contingency_matrix(labels_a, labels_b)
A:sklearn.metrics.cluster.tests.test_supervised.n_samples->contingency_matrix(labels_a, labels_b).sum()
A:sklearn.metrics.cluster.tests.test_supervised.emi->expected_mutual_information(C, n_samples)
A:sklearn.metrics.cluster.tests.test_supervised.ami->adjusted_mutual_info_score(a110, b110)
A:sklearn.metrics.cluster.tests.test_supervised.a110->numpy.array([list(labels_a) * 110]).flatten()
A:sklearn.metrics.cluster.tests.test_supervised.b110->numpy.array([list(labels_b) * 110]).flatten()
A:sklearn.metrics.cluster.tests.test_supervised.ent->entropy([0, 0, 42.0])
A:sklearn.metrics.cluster.tests.test_supervised.C_sparse->assert_raise_message(ValueError, "Cannot set 'eps' when sparse=True", contingency_matrix, labels_a, labels_b, eps=1e-10, sparse=True)
A:sklearn.metrics.cluster.tests.test_supervised.random_state->numpy.random.RandomState(seed)
A:sklearn.metrics.cluster.tests.test_supervised.score->fowlkes_mallows_score([0, 0, 0, 1, 1, 1], [0, 0, 1, 1, 2, 2])
A:sklearn.metrics.cluster.tests.test_supervised.perfect_score->fowlkes_mallows_score([0, 0, 0, 1, 1, 1], [1, 1, 1, 0, 0, 0])
A:sklearn.metrics.cluster.tests.test_supervised.worst_score->fowlkes_mallows_score([0, 0, 0, 0, 0, 0], [0, 1, 2, 3, 4, 5])
A:sklearn.metrics.cluster.tests.test_supervised.score_original->fowlkes_mallows_score(labels_a, labels_b)
A:sklearn.metrics.cluster.tests.test_supervised.score_symmetric->fowlkes_mallows_score(labels_b, labels_a)
A:sklearn.metrics.cluster.tests.test_supervised.score_permuted->fowlkes_mallows_score((labels_a + 1) % 3, labels_b)
A:sklearn.metrics.cluster.tests.test_supervised.score_both->fowlkes_mallows_score(labels_b, (labels_a + 2) % 3)
sklearn.metrics.cluster.tests.test_supervised.test_adjusted_mutual_info_score()
sklearn.metrics.cluster.tests.test_supervised.test_adjustment_for_chance()
sklearn.metrics.cluster.tests.test_supervised.test_complete_but_not_homogeneous_labeling()
sklearn.metrics.cluster.tests.test_supervised.test_contingency_matrix()
sklearn.metrics.cluster.tests.test_supervised.test_contingency_matrix_sparse()
sklearn.metrics.cluster.tests.test_supervised.test_entropy()
sklearn.metrics.cluster.tests.test_supervised.test_error_messages_on_wrong_input()
sklearn.metrics.cluster.tests.test_supervised.test_exactly_zero_info_score()
sklearn.metrics.cluster.tests.test_supervised.test_expected_mutual_info_overflow()
sklearn.metrics.cluster.tests.test_supervised.test_fowlkes_mallows_score()
sklearn.metrics.cluster.tests.test_supervised.test_fowlkes_mallows_score_properties()
sklearn.metrics.cluster.tests.test_supervised.test_homogeneous_but_not_complete_labeling()
sklearn.metrics.cluster.tests.test_supervised.test_non_consicutive_labels()
sklearn.metrics.cluster.tests.test_supervised.test_not_complete_and_not_homogeneous_labeling()
sklearn.metrics.cluster.tests.test_supervised.test_perfect_matches()
sklearn.metrics.cluster.tests.test_supervised.test_v_measure_and_mutual_information(seed=36)
sklearn.metrics.cluster.tests.test_supervised.uniform_labelings_scores(score_func,n_samples,k_range,n_runs=10,seed=42)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/metrics/cluster/tests/test_bicluster.py----------------------------------------
A:sklearn.metrics.cluster.tests.test_bicluster.a1->numpy.array([True, True, False, False])
A:sklearn.metrics.cluster.tests.test_bicluster.a2->numpy.array([True, True, True, True])
A:sklearn.metrics.cluster.tests.test_bicluster.a3->numpy.array([False, True, True, False])
A:sklearn.metrics.cluster.tests.test_bicluster.a4->numpy.array([False, False, True, True])
A:sklearn.metrics.cluster.tests.test_bicluster.a_rows->numpy.array([[True, True, False, False], [False, False, True, True], [False, False, False, True]])
A:sklearn.metrics.cluster.tests.test_bicluster.a_cols->numpy.array([[True, True, False, False], [False, False, True, True], [False, False, False, True]])
A:sklearn.metrics.cluster.tests.test_bicluster.s->consensus_score((a_rows, a_cols), (a_rows[idx], a_cols[idx]))
sklearn.metrics.cluster.tests.test_bicluster.test_consensus_score()
sklearn.metrics.cluster.tests.test_bicluster.test_consensus_score_issue2445()
sklearn.metrics.cluster.tests.test_bicluster.test_jaccard()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/metrics/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/metrics/tests/test_classification.py----------------------------------------
A:sklearn.metrics.tests.test_classification.dataset->sklearn.datasets.load_iris()
A:sklearn.metrics.tests.test_classification.p->numpy.arange(n_samples)
A:sklearn.metrics.tests.test_classification.rng->numpy.random.RandomState(20170906)
A:sklearn.metrics.tests.test_classification.half->int(n_samples / 2)
A:sklearn.metrics.tests.test_classification.clf->sklearn.svm.SVC(kernel='linear', probability=True, random_state=0)
A:sklearn.metrics.tests.test_classification.probas_pred->sklearn.svm.SVC(kernel='linear', probability=True, random_state=0).fit(X[:half], y[:half]).predict_proba(X[half:])
A:sklearn.metrics.tests.test_classification.y_pred->numpy.array([0.1, 0.8, 0.9, 0.3, 1.0, 0.95])
A:sklearn.metrics.tests.test_classification.y1->numpy.array([[0, 1, 1], [1, 0, 1]])
A:sklearn.metrics.tests.test_classification.y2->numpy.array([[0, 0, 1], [1, 0, 1]])
A:sklearn.metrics.tests.test_classification.(y_true, y_pred, _)->make_prediction(binary=False)
A:sklearn.metrics.tests.test_classification.(p, r, f, s)->assert_warns(UndefinedMetricWarning, precision_recall_fscore_support, y_true, y_pred, average=average, beta=beta)
A:sklearn.metrics.tests.test_classification.ps->precision_score(y_true, y_pred, average='weighted')
A:sklearn.metrics.tests.test_classification.rs->recall_score(y_true, y_pred, average='weighted')
A:sklearn.metrics.tests.test_classification.fs->f1_score(y_true, y_pred, average='weighted')
A:sklearn.metrics.tests.test_classification.y_true_bin->label_binarize(y_true, classes=np.arange(5))
A:sklearn.metrics.tests.test_classification.y_pred_bin->label_binarize(y_pred, classes=np.arange(5))
A:sklearn.metrics.tests.test_classification.actual->recall_score(y_true, y_pred, labels=[0, 1, 2, 3, 4], average='macro')
A:sklearn.metrics.tests.test_classification.recall_13->partial(recall_score, y_true, y_pred, labels=[1, 3])
A:sklearn.metrics.tests.test_classification.recall_all->partial(recall_score, y_true, y_pred, labels=None)
A:sklearn.metrics.tests.test_classification.y_true->numpy.array([0, 1, 1, 0, 1, 1])
A:sklearn.metrics.tests.test_classification.cm->confusion_matrix(y, y, sample_weight=weight)
A:sklearn.metrics.tests.test_classification.(tp, fp, fn, tn)->confusion_matrix(y, y, sample_weight=weight).flatten()
A:sklearn.metrics.tests.test_classification.den->numpy.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))
A:sklearn.metrics.tests.test_classification.mcc->assert_warns_message(RuntimeWarning, 'invalid value encountered', matthews_corrcoef, y_true, y_pred, sample_weight)
A:sklearn.metrics.tests.test_classification.kappa->cohen_kappa_score(y1, y2)
A:sklearn.metrics.tests.test_classification.sample_weight->numpy.random.RandomState(20170906).rand(20)
A:sklearn.metrics.tests.test_classification.C->confusion_matrix(y_true, y_pred, sample_weight=sample_weight)
A:sklearn.metrics.tests.test_classification.N->len(C)
A:sklearn.metrics.tests.test_classification.cov_ytyp->sum([C[k, k] * C[m, l] - C[l, k] * C[k, m] for k in range(N) for m in range(N) for l in range(N)])
A:sklearn.metrics.tests.test_classification.cov_ytyt->sum([C[:, k].sum() * np.sum([C[g, f] for f in range(N) for g in range(N) if f != k]) for k in range(N)])
A:sklearn.metrics.tests.test_classification.cov_ypyp->numpy.sum([C[k, :].sum() * np.sum([C[f, g] for f in range(N) for g in range(N) if f != k]) for k in range(N)])
A:sklearn.metrics.tests.test_classification.mcc_ours->matthews_corrcoef(y_true, y_pred, sample_weight)
A:sklearn.metrics.tests.test_classification.y_true_inv2->numpy.where(y_true_inv2, 'a', 'b')
A:sklearn.metrics.tests.test_classification.ord_a->ord('a')
A:sklearn.metrics.tests.test_classification.conf_matrix->confusion_matrix(y_true, y_pred)
A:sklearn.metrics.tests.test_classification.n_points->len(y_true)
A:sklearn.metrics.tests.test_classification.x_true->numpy.random.RandomState(20170906).random_sample(n_points)
A:sklearn.metrics.tests.test_classification.arr->numpy.repeat([0.0, 1.0, 2.0], n_points)
A:sklearn.metrics.tests.test_classification.(y_true, y_pred)->random_ys(n_points)
A:sklearn.metrics.tests.test_classification.(ps, rs, fs, _)->precision_recall_fscore_support(y_true, y_pred, average=None)
A:sklearn.metrics.tests.test_classification.(p, r, f, _)->precision_recall_fscore_support(y_true, y_pred, average='weighted')
A:sklearn.metrics.tests.test_classification.support->numpy.bincount(y_true)
A:sklearn.metrics.tests.test_classification.old_error_settings->numpy.seterr(all='raise')
A:sklearn.metrics.tests.test_classification.weight->numpy.ones(len(y))
A:sklearn.metrics.tests.test_classification.iris->sklearn.datasets.load_iris()
A:sklearn.metrics.tests.test_classification.report->classification_report(y_true, y_pred)
A:sklearn.metrics.tests.test_classification.labels->numpy.array([0, 1, 2, 3])
A:sklearn.metrics.tests.test_classification.(_, y_true)->make_multilabel_classification(n_features=1, n_samples=n_samples, n_classes=n_classes, random_state=0)
A:sklearn.metrics.tests.test_classification.(_, y_pred)->make_multilabel_classification(n_features=1, n_samples=n_samples, n_classes=n_classes, random_state=1)
A:sklearn.metrics.tests.test_classification.w->numpy.array([1, 3])
A:sklearn.metrics.tests.test_classification.f2->fbeta_score(y_true, y_pred, beta=2, average=None)
A:sklearn.metrics.tests.test_classification.fbeta->assert_warns(UndefinedMetricWarning, fbeta_score, y_true, y_pred, beta=beta, average=average)
A:sklearn.metrics.tests.test_classification.y_true_ind->numpy.array([[0, 1, 1], [1, 0, 0], [0, 0, 1]])
A:sklearn.metrics.tests.test_classification.y_pred_ind->numpy.array([[0, 1, 0], [1, 0, 0], [0, 0, 1]])
A:sklearn.metrics.tests.test_classification.(merged_type, y1out, y2out)->_check_targets(y1, y2)
A:sklearn.metrics.tests.test_classification.pred_decision->numpy.array([[+0.36, -0.17, -0.58, -0.99], [-0.55, -0.38, -0.48, -0.58], [-1.45, -0.58, -0.38, -0.17], [-0.55, -0.38, -0.48, -0.58], [-1.45, -0.58, -0.38, -0.17]])
A:sklearn.metrics.tests.test_classification.dummy_losses->numpy.array([1 - pred_decision[0][0] + pred_decision[0][1], 1 - pred_decision[1][1] + pred_decision[1][2], 1 - pred_decision[2][2] + pred_decision[2][3], 1 - pred_decision[3][1] + pred_decision[3][2], 1 - pred_decision[4][3] + pred_decision[4][2], 1 - pred_decision[5][2] + pred_decision[5][3]])
A:sklearn.metrics.tests.test_classification.dummy_hinge_loss->numpy.mean(dummy_losses)
A:sklearn.metrics.tests.test_classification.loss->log_loss(y_true, y_pred)
A:sklearn.metrics.tests.test_classification.y_score->numpy.array([[0.1, 0.9], [0.1, 0.9]])
A:sklearn.metrics.tests.test_classification.calculated_log_loss->log_loss(y_true, y_score, labels=[1, 2])
A:sklearn.metrics.tests.test_classification.y_tr->numpy.array(['ham', 'spam', 'spam', 'ham'])
A:sklearn.metrics.tests.test_classification.y_pr->numpy.array([[0.2, 0.7], [0.6, 0.5], [0.4, 0.1], [0.7, 0.2]])
sklearn.metrics.tests.test_classification.make_prediction(dataset=None,binary=False)
sklearn.metrics.tests.test_classification.test__check_targets()
sklearn.metrics.tests.test_classification.test__check_targets_multiclass_with_both_y_true_and_y_pred_binary()
sklearn.metrics.tests.test_classification.test_average_precision_score_duplicate_values()
sklearn.metrics.tests.test_classification.test_average_precision_score_score_non_binary_class()
sklearn.metrics.tests.test_classification.test_average_precision_score_tied_values()
sklearn.metrics.tests.test_classification.test_brier_score_loss()
sklearn.metrics.tests.test_classification.test_classification_report_labels_target_names_unequal_length()
sklearn.metrics.tests.test_classification.test_classification_report_multiclass()
sklearn.metrics.tests.test_classification.test_classification_report_multiclass_with_digits()
sklearn.metrics.tests.test_classification.test_classification_report_multiclass_with_long_string_label()
sklearn.metrics.tests.test_classification.test_classification_report_multiclass_with_string_label()
sklearn.metrics.tests.test_classification.test_classification_report_multiclass_with_unicode_label()
sklearn.metrics.tests.test_classification.test_cohen_kappa()
sklearn.metrics.tests.test_classification.test_confusion_matrix_binary()
sklearn.metrics.tests.test_classification.test_confusion_matrix_dtype()
sklearn.metrics.tests.test_classification.test_confusion_matrix_multiclass()
sklearn.metrics.tests.test_classification.test_confusion_matrix_multiclass_subset_labels()
sklearn.metrics.tests.test_classification.test_confusion_matrix_sample_weight()
sklearn.metrics.tests.test_classification.test_fscore_warnings()
sklearn.metrics.tests.test_classification.test_hinge_loss_binary()
sklearn.metrics.tests.test_classification.test_hinge_loss_multiclass()
sklearn.metrics.tests.test_classification.test_hinge_loss_multiclass_invariance_lists()
sklearn.metrics.tests.test_classification.test_hinge_loss_multiclass_missing_labels_with_labels_none()
sklearn.metrics.tests.test_classification.test_hinge_loss_multiclass_with_missing_labels()
sklearn.metrics.tests.test_classification.test_log_loss()
sklearn.metrics.tests.test_classification.test_log_loss_pandas_input()
sklearn.metrics.tests.test_classification.test_matthews_corrcoef()
sklearn.metrics.tests.test_classification.test_matthews_corrcoef_against_jurman()
sklearn.metrics.tests.test_classification.test_matthews_corrcoef_against_numpy_corrcoef()
sklearn.metrics.tests.test_classification.test_matthews_corrcoef_multiclass()
sklearn.metrics.tests.test_classification.test_matthews_corrcoef_nan()
sklearn.metrics.tests.test_classification.test_matthews_corrcoef_overflow()
sklearn.metrics.tests.test_classification.test_multilabel_accuracy_score_subset_accuracy()
sklearn.metrics.tests.test_classification.test_multilabel_classification_report()
sklearn.metrics.tests.test_classification.test_multilabel_hamming_loss()
sklearn.metrics.tests.test_classification.test_multilabel_jaccard_similarity_score()
sklearn.metrics.tests.test_classification.test_multilabel_zero_one_loss_subset()
sklearn.metrics.tests.test_classification.test_precision_recall_f1_no_labels()
sklearn.metrics.tests.test_classification.test_precision_recall_f1_score_binary()
sklearn.metrics.tests.test_classification.test_precision_recall_f1_score_binary_averaged()
sklearn.metrics.tests.test_classification.test_precision_recall_f1_score_multiclass()
sklearn.metrics.tests.test_classification.test_precision_recall_f1_score_multilabel_1()
sklearn.metrics.tests.test_classification.test_precision_recall_f1_score_multilabel_2()
sklearn.metrics.tests.test_classification.test_precision_recall_f1_score_with_an_empty_prediction()
sklearn.metrics.tests.test_classification.test_precision_recall_f_binary_single_class()
sklearn.metrics.tests.test_classification.test_precision_recall_f_extra_labels()
sklearn.metrics.tests.test_classification.test_precision_recall_f_ignored_labels()
sklearn.metrics.tests.test_classification.test_precision_recall_f_unused_pos_label()
sklearn.metrics.tests.test_classification.test_precision_recall_fscore_support_errors()
sklearn.metrics.tests.test_classification.test_precision_refcall_f1_score_multilabel_unordered_labels()
sklearn.metrics.tests.test_classification.test_precision_warnings()
sklearn.metrics.tests.test_classification.test_prf_average_binary_data_non_binary()
sklearn.metrics.tests.test_classification.test_prf_warnings()
sklearn.metrics.tests.test_classification.test_recall_warnings()
sklearn.metrics.tests.test_classification.test_zero_precision_recall()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/metrics/tests/test_ranking.py----------------------------------------
A:sklearn.metrics.tests.test_ranking.dataset->sklearn.datasets.load_iris()
A:sklearn.metrics.tests.test_ranking.p->numpy.sum(y_true)
A:sklearn.metrics.tests.test_ranking.rng->check_random_state(404)
A:sklearn.metrics.tests.test_ranking.half->int(n_samples / 2)
A:sklearn.metrics.tests.test_ranking.clf->sklearn.svm.SVC(kernel='linear', probability=True, random_state=0)
A:sklearn.metrics.tests.test_ranking.probas_pred->sklearn.svm.SVC(kernel='linear', probability=True, random_state=0).fit(X[:half], y[:half]).predict_proba(X[half:])
A:sklearn.metrics.tests.test_ranking.y_pred->check_random_state(404).rand(10)
A:sklearn.metrics.tests.test_ranking.n_correct->numpy.sum(diff_matrix > 0)
A:sklearn.metrics.tests.test_ranking.n_pos->numpy.sum(y_true == pos_label)
A:sklearn.metrics.tests.test_ranking.(precision, recall, threshold)->precision_recall_curve(y_true, y_score)
A:sklearn.metrics.tests.test_ranking.precision->list(reversed(precision))
A:sklearn.metrics.tests.test_ranking.recall->list(reversed(recall))
A:sklearn.metrics.tests.test_ranking.(y_true, _, probas_pred)->make_prediction(binary=True)
A:sklearn.metrics.tests.test_ranking.expected_auc->_auc(y_true, probas_pred)
A:sklearn.metrics.tests.test_ranking.(fpr, tpr, thresholds)->assert_warns(w, roc_curve, [1 - x for x in y_true], y_pred)
A:sklearn.metrics.tests.test_ranking.roc_auc->roc_auc_score(y_true, probas_pred)
A:sklearn.metrics.tests.test_ranking.y_true->check_array(y_true)
A:sklearn.metrics.tests.test_ranking.(fpr, tpr, thr)->roc_curve(y_true, y_pred, drop_intermediate=True)
A:sklearn.metrics.tests.test_ranking.tp->numpy.sum((probas_pred >= t) & y_true)
A:sklearn.metrics.tests.test_ranking.(y_true, pred, probas_pred)->make_prediction(binary=True)
A:sklearn.metrics.tests.test_ranking.trivial_pred->numpy.zeros(y_true.shape)
A:sklearn.metrics.tests.test_ranking.(tpr, fpr, _)->assert_warns(UndefinedMetricWarning, roc_curve, y_true, y_score)
A:sklearn.metrics.tests.test_ranking.y_score->check_random_state(random_state).uniform(size=(n_samples, n_classes))
A:sklearn.metrics.tests.test_ranking.(tpr, fpr, thresholds)->roc_curve(y_true, y_score, drop_intermediate=True)
A:sklearn.metrics.tests.test_ranking.y_true_copy->check_array(y_true).copy()
A:sklearn.metrics.tests.test_ranking.(p, r, t)->precision_recall_curve(labels, predict_probas)
A:sklearn.metrics.tests.test_ranking.(p, r, thresholds)->precision_recall_curve(y_true, np.zeros_like(probas_pred))
A:sklearn.metrics.tests.test_ranking.(p2, r2, thresholds2)->precision_recall_curve(y_true == pos_label, probas_pred[:, pos_label])
A:sklearn.metrics.tests.test_ranking.precision_recall_auc->_average_precision_slow(y_true, probas_pred)
A:sklearn.metrics.tests.test_ranking.(p, r, _)->precision_recall_curve(y_true, y_score)
A:sklearn.metrics.tests.test_ranking.auc_prc->average_precision_score(y_true, y_score)
A:sklearn.metrics.tests.test_ranking.roc_auc_scaled_up->roc_auc_score(y_true, 100 * probas_pred)
A:sklearn.metrics.tests.test_ranking.roc_auc_scaled_down->roc_auc_score(y_true, 1e-06 * probas_pred)
A:sklearn.metrics.tests.test_ranking.roc_auc_shifted->roc_auc_score(y_true, probas_pred - 10)
A:sklearn.metrics.tests.test_ranking.pr_auc->average_precision_score(y_true, probas_pred)
A:sklearn.metrics.tests.test_ranking.pr_auc_scaled_up->average_precision_score(y_true, 100 * probas_pred)
A:sklearn.metrics.tests.test_ranking.pr_auc_scaled_down->average_precision_score(y_true, 1e-06 * probas_pred)
A:sklearn.metrics.tests.test_ranking.pr_auc_shifted->average_precision_score(y_true, probas_pred - 10)
A:sklearn.metrics.tests.test_ranking.random_state->check_random_state(random_state)
A:sklearn.metrics.tests.test_ranking.y_score_ties->numpy.zeros_like(y_score)
A:sklearn.metrics.tests.test_ranking.score->numpy.empty((n_samples,))
A:sklearn.metrics.tests.test_ranking.(unique_rank, inv_rank)->numpy.unique(y_score[i], return_inverse=True)
A:sklearn.metrics.tests.test_ranking.corr_rank->numpy.bincount(rank, minlength=n_ranks + 1).cumsum()
A:sklearn.metrics.tests.test_ranking.n_ranked_above->sum((rank[r] <= rank[label] for r in relevant))
A:sklearn.metrics.tests.test_ranking.(_, y_true)->make_multilabel_classification(n_features=1, allow_unlabeled=False, random_state=random_state, n_classes=n_classes, n_samples=n_samples)
A:sklearn.metrics.tests.test_ranking.score_lrap->label_ranking_average_precision_score(y_true, y_score)
A:sklearn.metrics.tests.test_ranking.score_my_lrap->_my_lrap(y_true, y_score)
sklearn.metrics.tests.test_ranking._auc(y_true,y_score)
sklearn.metrics.tests.test_ranking._average_precision(y_true,y_score)
sklearn.metrics.tests.test_ranking._average_precision_slow(y_true,y_score)
sklearn.metrics.tests.test_ranking._my_lrap(y_true,y_score)
sklearn.metrics.tests.test_ranking._test_precision_recall_curve(y_true,probas_pred)
sklearn.metrics.tests.test_ranking.check_alternative_lrap_implementation(lrap_score,n_classes=5,n_samples=20,random_state=0)
sklearn.metrics.tests.test_ranking.check_lrap_error_raised(lrap_score)
sklearn.metrics.tests.test_ranking.check_lrap_only_ties(lrap_score)
sklearn.metrics.tests.test_ranking.check_lrap_toy(lrap_score)
sklearn.metrics.tests.test_ranking.check_lrap_without_tie_and_increasing_score(lrap_score)
sklearn.metrics.tests.test_ranking.check_zero_or_all_relevant_labels(lrap_score)
sklearn.metrics.tests.test_ranking.make_prediction(dataset=None,binary=False)
sklearn.metrics.tests.test_ranking.test_auc()
sklearn.metrics.tests.test_ranking.test_auc_duplicate_values()
sklearn.metrics.tests.test_ranking.test_auc_errors()
sklearn.metrics.tests.test_ranking.test_auc_score_non_binary_class()
sklearn.metrics.tests.test_ranking.test_average_precision_constant_values()
sklearn.metrics.tests.test_ranking.test_binary_clf_curve()
sklearn.metrics.tests.test_ranking.test_coverage_error()
sklearn.metrics.tests.test_ranking.test_coverage_tie_handling()
sklearn.metrics.tests.test_ranking.test_label_ranking_avp()
sklearn.metrics.tests.test_ranking.test_label_ranking_loss()
sklearn.metrics.tests.test_ranking.test_precision_recall_curve()
sklearn.metrics.tests.test_ranking.test_precision_recall_curve_errors()
sklearn.metrics.tests.test_ranking.test_precision_recall_curve_pos_label()
sklearn.metrics.tests.test_ranking.test_precision_recall_curve_toydata()
sklearn.metrics.tests.test_ranking.test_ranking_appropriate_input_shape()
sklearn.metrics.tests.test_ranking.test_ranking_loss_ties_handling()
sklearn.metrics.tests.test_ranking.test_roc_curve()
sklearn.metrics.tests.test_ranking.test_roc_curve_confidence()
sklearn.metrics.tests.test_ranking.test_roc_curve_drop_intermediate()
sklearn.metrics.tests.test_ranking.test_roc_curve_end_points()
sklearn.metrics.tests.test_ranking.test_roc_curve_hard()
sklearn.metrics.tests.test_ranking.test_roc_curve_multi()
sklearn.metrics.tests.test_ranking.test_roc_curve_one_label()
sklearn.metrics.tests.test_ranking.test_roc_curve_toydata()
sklearn.metrics.tests.test_ranking.test_roc_returns_consistency()
sklearn.metrics.tests.test_ranking.test_score_scale_invariance()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/metrics/tests/test_common.py----------------------------------------
A:sklearn.metrics.tests.test_common.ALL_METRICS->dict()
A:sklearn.metrics.tests.test_common.METRIC_UNDEFINED_BINARY_MULTICLASS->set(METRIC_UNDEFINED_BINARY).union(set(METRIC_UNDEFINED_MULTICLASS))
A:sklearn.metrics.tests.test_common.random_state->check_random_state(0)
A:sklearn.metrics.tests.test_common.y_true->numpy.vstack([ya, yb])
A:sklearn.metrics.tests.test_common.y_pred->numpy.vstack([ya, ya])
A:sklearn.metrics.tests.test_common.(y_true_shuffle, y_pred_shuffle)->shuffle(y_true, y_pred, random_state=0)
A:sklearn.metrics.tests.test_common.y_score->check_random_state(0).randint(1, 4, size=y_true.shape)
A:sklearn.metrics.tests.test_common.(y_true_shuffle, y_pred_shuffle, y_score_shuffle)->shuffle(y_true, y_pred, y_score, random_state=0)
A:sklearn.metrics.tests.test_common.y1->numpy.vstack([y1, [[0] * n_classes]])
A:sklearn.metrics.tests.test_common.y2->numpy.vstack([y2, [[0] * n_classes]])
A:sklearn.metrics.tests.test_common.y1_list->list(y1)
A:sklearn.metrics.tests.test_common.y2_list->list(y2)
A:sklearn.metrics.tests.test_common.y1_column->numpy.reshape(y1_1d, (-1, 1))
A:sklearn.metrics.tests.test_common.y2_column->numpy.reshape(y2_1d, (-1, 1))
A:sklearn.metrics.tests.test_common.y1_row->numpy.reshape(y1_1d, (1, -1))
A:sklearn.metrics.tests.test_common.y2_row->numpy.reshape(y2_1d, (1, -1))
A:sklearn.metrics.tests.test_common.measure->metrics(y_true, y_pred, normalize=True)
A:sklearn.metrics.tests.test_common.measure_with_number->metric(y1, y2)
A:sklearn.metrics.tests.test_common.metric_str->partial(metric_str, pos_label=pos_label_str)
A:sklearn.metrics.tests.test_common.measure_with_str->metric_str(y1_str, y2)
A:sklearn.metrics.tests.test_common.measure_with_strobj->metric(y1_str.astype('O'), y2)
A:sklearn.metrics.tests.test_common.METRICS->dict()
A:sklearn.metrics.tests.test_common.error->metric(y_true, y_pred)
A:sklearn.metrics.tests.test_common.perm->check_random_state(0).permutation(y_true.shape[1])
A:sklearn.metrics.tests.test_common.(_, y1)->make_multilabel_classification(n_features=1, n_classes=n_classes, random_state=0, n_samples=n_samples, allow_unlabeled=True)
A:sklearn.metrics.tests.test_common.(_, y2)->make_multilabel_classification(n_features=1, n_classes=n_classes, random_state=1, n_samples=n_samples, allow_unlabeled=True)
A:sklearn.metrics.tests.test_common.y1_sparse_indicator->scipy.sparse.coo_matrix(y1)
A:sklearn.metrics.tests.test_common.y2_sparse_indicator->scipy.sparse.coo_matrix(y2)
A:sklearn.metrics.tests.test_common.(_, y_true)->make_multilabel_classification(n_features=1, n_classes=n_classes, random_state=0, allow_unlabeled=True, n_samples=n_samples)
A:sklearn.metrics.tests.test_common.(_, y_pred)->make_multilabel_classification(n_features=1, n_classes=n_classes, random_state=1, allow_unlabeled=True, n_samples=n_samples)
A:sklearn.metrics.tests.test_common.label_measure->metric(y_true, y_pred, average=None)
A:sklearn.metrics.tests.test_common.micro_measure->metric(y_true, y_pred, average='micro')
A:sklearn.metrics.tests.test_common.macro_measure->metric(y_true, y_pred, average='macro')
A:sklearn.metrics.tests.test_common.weights->numpy.sum(y_true_binarize, axis=0, dtype=int)
A:sklearn.metrics.tests.test_common.weighted_measure->metric(y_true, y_pred, average='weighted')
A:sklearn.metrics.tests.test_common.sample_measure->metric(y_true, y_pred, average='samples')
A:sklearn.metrics.tests.test_common.is_multilabel->type_of_target(y_true).startswith('multilabel')
A:sklearn.metrics.tests.test_common.lb->LabelBinarizer().fit(y_true)
A:sklearn.metrics.tests.test_common.y_true_binarize->LabelBinarizer().fit(y_true).transform(y_true)
A:sklearn.metrics.tests.test_common.y_pred_binarize->LabelBinarizer().fit(y_true).transform(y_pred)
A:sklearn.metrics.tests.test_common.(_, y)->make_multilabel_classification(n_features=1, n_classes=n_classes, random_state=5, n_samples=n_samples, allow_unlabeled=False)
A:sklearn.metrics.tests.test_common.rng->numpy.random.RandomState(0)
A:sklearn.metrics.tests.test_common.sample_weight->numpy.random.RandomState(0).randint(1, 10, size=len(y1))
A:sklearn.metrics.tests.test_common.unweighted_score->metric(y1, y2, sample_weight=None)
A:sklearn.metrics.tests.test_common.weighted_score->metric(y1, y2, sample_weight=sample_weight)
A:sklearn.metrics.tests.test_common.weighted_score_list->metric(y1, y2, sample_weight=sample_weight.tolist())
A:sklearn.metrics.tests.test_common.repeat_weighted_score->metric(np.repeat(y1, sample_weight, axis=0), np.repeat(y2, sample_weight, axis=0), sample_weight=None)
A:sklearn.metrics.tests.test_common.sample_weight_zeroed->numpy.copy(sample_weight)
A:sklearn.metrics.tests.test_common.weighted_score_subset->metric(y1_subset, y2_subset, sample_weight=sample_weight_subset)
A:sklearn.metrics.tests.test_common.weighted_score_zeroed->metric(y1, y2, sample_weight=sample_weight_zeroed)
A:sklearn.metrics.tests.test_common.(_, ya)->make_multilabel_classification(n_features=1, n_classes=20, random_state=0, n_samples=100, allow_unlabeled=False)
A:sklearn.metrics.tests.test_common.(_, yb)->make_multilabel_classification(n_features=1, n_classes=20, random_state=1, n_samples=100, allow_unlabeled=False)
A:sklearn.metrics.tests.test_common.y_true_multilabel->numpy.array([[1, 1, 0, 0], [1, 1, 0, 0]])
A:sklearn.metrics.tests.test_common.y_pred_multilabel->numpy.array([[0, 0, 1, 1], [0, 1, 1, 0]])
A:sklearn.metrics.tests.test_common.y_true_multiclass->numpy.array([0, 1, 2])
A:sklearn.metrics.tests.test_common.y_pred_multiclass->numpy.array([0, 2, 3])
A:sklearn.metrics.tests.test_common.labels->numpy.array([3, 0, 1, 2])
A:sklearn.metrics.tests.test_common.(_, inverse_labels)->numpy.unique(labels, return_inverse=True)
A:sklearn.metrics.tests.test_common.score_labels->metric(y_true, y_pred, labels=labels, average=None)
A:sklearn.metrics.tests.test_common.score->metric(y_true, y_pred, average=None)
sklearn.metrics.tests.test_common._check_averaging(metric,y_true,y_pred,y_true_binarize,y_pred_binarize,is_multilabel)
sklearn.metrics.tests.test_common.check_averaging(name,y_true,y_true_binarize,y_pred,y_pred_binarize,y_score)
sklearn.metrics.tests.test_common.check_sample_weight_invariance(name,metric,y1,y2)
sklearn.metrics.tests.test_common.check_single_sample(name)
sklearn.metrics.tests.test_common.check_single_sample_multioutput(name)
sklearn.metrics.tests.test_common.test_averaging_multiclass(n_samples=50,n_classes=3)
sklearn.metrics.tests.test_common.test_averaging_multilabel(n_classes=5,n_samples=40)
sklearn.metrics.tests.test_common.test_averaging_multilabel_all_ones()
sklearn.metrics.tests.test_common.test_averaging_multilabel_all_zeroes()
sklearn.metrics.tests.test_common.test_format_invariance_with_1d_vectors()
sklearn.metrics.tests.test_common.test_inf_nan_input()
sklearn.metrics.tests.test_common.test_invariance_string_vs_numbers_labels()
sklearn.metrics.tests.test_common.test_multilabel_representation_invariance()
sklearn.metrics.tests.test_common.test_multioutput_number_of_output_differ()
sklearn.metrics.tests.test_common.test_multioutput_regression_invariance_to_dimension_shuffling()
sklearn.metrics.tests.test_common.test_no_averaging_labels()
sklearn.metrics.tests.test_common.test_normalize_option_binary_classification(n_samples=20)
sklearn.metrics.tests.test_common.test_normalize_option_multiclass_classification()
sklearn.metrics.tests.test_common.test_normalize_option_multilabel_classification()
sklearn.metrics.tests.test_common.test_raise_value_error_multilabel_sequences()
sklearn.metrics.tests.test_common.test_sample_order_invariance()
sklearn.metrics.tests.test_common.test_sample_order_invariance_multilabel_and_multioutput()
sklearn.metrics.tests.test_common.test_sample_weight_invariance(n_samples=50)
sklearn.metrics.tests.test_common.test_single_sample()
sklearn.metrics.tests.test_common.test_symmetry()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/metrics/tests/test_regression.py----------------------------------------
A:sklearn.metrics.tests.test_regression.y_true->numpy.array([[0.5, 1], [1, 2], [7, 6]])
A:sklearn.metrics.tests.test_regression.y_pred->numpy.array([[0.5, 2], [1, 2.5], [8, 8]])
A:sklearn.metrics.tests.test_regression.error->r2_score(y_true, y_pred, multioutput='uniform_average')
A:sklearn.metrics.tests.test_regression.(y_type, y_check1, y_check2, multioutput)->_check_reg_targets(y1, y2, None)
A:sklearn.metrics.tests.test_regression.expected_message->"Allowed 'multioutput' string values are.+You provided multioutput={!r}".format(invalid_multioutput)
A:sklearn.metrics.tests.test_regression.mse->mean_squared_error(y_true, y_pred, multioutput='raw_values')
A:sklearn.metrics.tests.test_regression.mae->mean_absolute_error(y_true, y_pred, multioutput='raw_values')
A:sklearn.metrics.tests.test_regression.r->r2_score([[0, -1], [0, 1]], [[2, 2], [1, 1]], multioutput='raw_values')
A:sklearn.metrics.tests.test_regression.evs->explained_variance_score(y_true, y_pred, multioutput='raw_values')
A:sklearn.metrics.tests.test_regression.r2->r2_score(y_true, y_pred, multioutput='raw_values')
A:sklearn.metrics.tests.test_regression.msle->mean_squared_log_error(y_true, y_pred, multioutput=[0.3, 0.7])
A:sklearn.metrics.tests.test_regression.msle2->mean_squared_error(np.log(1 + y_true), np.log(1 + y_pred), multioutput=[0.3, 0.7])
A:sklearn.metrics.tests.test_regression.msew->mean_squared_error(y_true, y_pred, multioutput=[0.4, 0.6])
A:sklearn.metrics.tests.test_regression.maew->mean_absolute_error(y_true, y_pred, multioutput=[0.4, 0.6])
A:sklearn.metrics.tests.test_regression.rw->r2_score(y_true, y_pred, multioutput=[0.4, 0.6])
A:sklearn.metrics.tests.test_regression.evsw->explained_variance_score(y_true, y_pred, multioutput=[0.4, 0.6])
sklearn.metrics.tests.test_regression.test__check_reg_targets()
sklearn.metrics.tests.test_regression.test__check_reg_targets_exception()
sklearn.metrics.tests.test_regression.test_multioutput_regression()
sklearn.metrics.tests.test_regression.test_regression_custom_weights()
sklearn.metrics.tests.test_regression.test_regression_metrics(n_samples=50)
sklearn.metrics.tests.test_regression.test_regression_metrics_at_limits()
sklearn.metrics.tests.test_regression.test_regression_multioutput_array()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/metrics/tests/test_pairwise.py----------------------------------------
A:sklearn.metrics.tests.test_pairwise.rng->numpy.random.RandomState(0)
A:sklearn.metrics.tests.test_pairwise.X->numpy.random.RandomState(0).random_sample((5, 4))
A:sklearn.metrics.tests.test_pairwise.S->paired_distances(X, Y, metric='manhattan')
A:sklearn.metrics.tests.test_pairwise.S2->paired_distances(X, Y, metric=lambda x, y: np.abs(x - y).sum(axis=0))
A:sklearn.metrics.tests.test_pairwise.Y->numpy.random.RandomState(0).random_sample((3, 4))
A:sklearn.metrics.tests.test_pairwise.X_tuples->tuple([tuple([v for v in row]) for row in X])
A:sklearn.metrics.tests.test_pairwise.Y_tuples->tuple([tuple([v for v in row]) for row in Y])
A:sklearn.metrics.tests.test_pairwise.X_sparse->csr_matrix(X)
A:sklearn.metrics.tests.test_pairwise.Y_sparse->csr_matrix(Y)
A:sklearn.metrics.tests.test_pairwise.res->pairwise_distances(X, Z, metric=metric)
A:sklearn.metrics.tests.test_pairwise.K->laplacian_kernel(X, X)
A:sklearn.metrics.tests.test_pairwise.K1->pairwise_kernels(X_, Y=Y_, metric='cosine')
A:sklearn.metrics.tests.test_pairwise.K2->pairwise_kernels(X_, Y=Y_, metric='linear')
A:sklearn.metrics.tests.test_pairwise.S3->func(csr_matrix(X), csr_matrix(Y))
A:sklearn.metrics.tests.test_pairwise.distances->numpy.diag(distances)
A:sklearn.metrics.tests.test_pairwise.Xsp->dok_matrix(X)
A:sklearn.metrics.tests.test_pairwise.Ysp->csr_matrix(Y, dtype=np.float32)
A:sklearn.metrics.tests.test_pairwise.(D, E)->pairwise_distances_argmin_min(X, Y, metric='minkowski', metric_kwargs={'p': 2})
A:sklearn.metrics.tests.test_pairwise.D2->cosine_distances(XB)
A:sklearn.metrics.tests.test_pairwise.(Dsp, Esp)->pairwise_distances_argmin_min(Xsp, Ysp, metric='euclidean')
A:sklearn.metrics.tests.test_pairwise.dist->pairwise_distances(X, Y, metric='manhattan')
A:sklearn.metrics.tests.test_pairwise.dist_orig_ind->pairwise_distances(X, Y, metric='manhattan').argmin(axis=0)
A:sklearn.metrics.tests.test_pairwise.(dist_chunked_ind, dist_chunked_val)->pairwise_distances_argmin_min(X, Y, axis=0, metric='manhattan', batch_size=50)
A:sklearn.metrics.tests.test_pairwise.D->paired_manhattan_distances(X, Y)
A:sklearn.metrics.tests.test_pairwise.X_norm_sq->(X ** 2).sum(axis=1).reshape(1, -1)
A:sklearn.metrics.tests.test_pairwise.Y_norm_sq->(Y ** 2).sum(axis=1).reshape(1, -1)
A:sklearn.metrics.tests.test_pairwise.D1->euclidean_distances(X, Y)
A:sklearn.metrics.tests.test_pairwise.D3->euclidean_distances(X, Y, Y_norm_squared=Y_norm_sq)
A:sklearn.metrics.tests.test_pairwise.D4->euclidean_distances(X, Y, X_norm_squared=X_norm_sq, Y_norm_squared=Y_norm_sq)
A:sklearn.metrics.tests.test_pairwise.wrong_D->euclidean_distances(X, Y, X_norm_squared=np.zeros_like(X_norm_sq), Y_norm_squared=np.zeros_like(Y_norm_sq))
A:sklearn.metrics.tests.test_pairwise.x->numpy.abs(rng.rand(910))
A:sklearn.metrics.tests.test_pairwise.XA->numpy.resize(np.arange(40), (5, 8)).astype(np.float32)
A:sklearn.metrics.tests.test_pairwise.XB->numpy.resize(np.arange(40), (5, 8)).astype(np.float32)
A:sklearn.metrics.tests.test_pairwise.K_add->additive_chi2_kernel(X, Y)
A:sklearn.metrics.tests.test_pairwise.chi2_exp->numpy.exp(gamma * chi2)
A:sklearn.metrics.tests.test_pairwise.Xcsr->csr_matrix(X)
A:sklearn.metrics.tests.test_pairwise.Ycsr->csr_matrix(Y)
A:sklearn.metrics.tests.test_pairwise.X_->normalize(X_)
A:sklearn.metrics.tests.test_pairwise.Y_->normalize(Y_)
A:sklearn.metrics.tests.test_pairwise.(XA_checked, XB_checked)->check_pairwise_arrays(XA, XB.astype(np.float))
A:sklearn.metrics.tests.test_pairwise.XA_sparse->csr_matrix(XA)
A:sklearn.metrics.tests.test_pairwise.XB_sparse->csr_matrix(XB)
A:sklearn.metrics.tests.test_pairwise.(XA_checked, XA_2_checked)->check_pairwise_arrays(XA_sparse, XA_sparse)
A:sklearn.metrics.tests.test_pairwise.XA_tuples->tuplify(XA)
A:sklearn.metrics.tests.test_pairwise.XB_tuples->tuplify(XB)
sklearn.metrics.tests.test_pairwise.callable_rbf_kernel(x,y,**kwds)
sklearn.metrics.tests.test_pairwise.check_pairwise_parallel(func,metric,kwds)
sklearn.metrics.tests.test_pairwise.test_check_XB_returned()
sklearn.metrics.tests.test_pairwise.test_check_dense_matrices()
sklearn.metrics.tests.test_pairwise.test_check_different_dimensions()
sklearn.metrics.tests.test_pairwise.test_check_invalid_dimensions()
sklearn.metrics.tests.test_pairwise.test_check_preserve_type()
sklearn.metrics.tests.test_pairwise.test_check_sparse_arrays()
sklearn.metrics.tests.test_pairwise.test_check_tuple_input()
sklearn.metrics.tests.test_pairwise.test_chi_square_kernel()
sklearn.metrics.tests.test_pairwise.test_cosine_distances()
sklearn.metrics.tests.test_pairwise.test_cosine_similarity()
sklearn.metrics.tests.test_pairwise.test_cosine_similarity_sparse_output()
sklearn.metrics.tests.test_pairwise.test_euclidean_distances()
sklearn.metrics.tests.test_pairwise.test_kernel_sparse()
sklearn.metrics.tests.test_pairwise.test_kernel_symmetry()
sklearn.metrics.tests.test_pairwise.test_laplacian_kernel()
sklearn.metrics.tests.test_pairwise.test_linear_kernel()
sklearn.metrics.tests.test_pairwise.test_paired_distances()
sklearn.metrics.tests.test_pairwise.test_paired_euclidean_distances()
sklearn.metrics.tests.test_pairwise.test_paired_manhattan_distances()
sklearn.metrics.tests.test_pairwise.test_pairwise_boolean_distance()
sklearn.metrics.tests.test_pairwise.test_pairwise_callable_nonstrict_metric()
sklearn.metrics.tests.test_pairwise.test_pairwise_distances()
sklearn.metrics.tests.test_pairwise.test_pairwise_distances_argmin_min()
sklearn.metrics.tests.test_pairwise.test_pairwise_kernels()
sklearn.metrics.tests.test_pairwise.test_pairwise_kernels_filter_param()
sklearn.metrics.tests.test_pairwise.test_pairwise_parallel()
sklearn.metrics.tests.test_pairwise.test_pairwise_precomputed()
sklearn.metrics.tests.test_pairwise.test_rbf_kernel()
sklearn.metrics.tests.test_pairwise.tuplify(X)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/metrics/tests/test_score_objects.py----------------------------------------
A:sklearn.metrics.tests.test_score_objects.sensible_regr->DecisionTreeRegressor(random_state=0)
A:sklearn.metrics.tests.test_score_objects.sensible_clf->DecisionTreeClassifier(random_state=0)
A:sklearn.metrics.tests.test_score_objects.sensible_ml_clf->DecisionTreeClassifier(random_state=0)
A:sklearn.metrics.tests.test_score_objects.TEMP_FOLDER->tempfile.mkdtemp(prefix='sklearn_test_score_objects_')
A:sklearn.metrics.tests.test_score_objects.(X, y)->make_blobs(random_state=0, centers=2)
A:sklearn.metrics.tests.test_score_objects.(_, y_ml)->make_multilabel_classification(n_samples=X.shape[0], random_state=0)
A:sklearn.metrics.tests.test_score_objects.filename->os.path.join(TEMP_FOLDER, 'test_data.pkl')
A:sklearn.metrics.tests.test_score_objects.(X_mm, y_mm, y_ml_mm)->sklearn.externals.joblib.load(filename, mmap_mode='r')
A:sklearn.metrics.tests.test_score_objects.ESTIMATORS->_make_estimators(X_mm, y_mm, y_ml_mm)
A:sklearn.metrics.tests.test_score_objects.estimator->_make_estimators(X_train, y_train, y_ml_train)
A:sklearn.metrics.tests.test_score_objects.scorer->make_scorer(fbeta_score, beta=2)
A:sklearn.metrics.tests.test_score_objects.(scorers, is_multi)->_check_multimetric_scoring(estimator, scoring)
A:sklearn.metrics.tests.test_score_objects.(names, scorers)->zip(*scorers.items())
A:sklearn.metrics.tests.test_score_objects.grid->GridSearchCV(LinearSVC(), param_grid={'C': [0.1, 1]})
A:sklearn.metrics.tests.test_score_objects.pipe->make_pipeline(LinearSVC())
A:sklearn.metrics.tests.test_score_objects.scores->cross_val_score(EstimatorWithFit(), [[1], [2], [3]], [1, 0, 1], scoring=DummyScorer())
A:sklearn.metrics.tests.test_score_objects.(X_train, X_test, y_train, y_test)->train_test_split(X, y, random_state=0)
A:sklearn.metrics.tests.test_score_objects.clf->LogisticRegression(random_state=0)
A:sklearn.metrics.tests.test_score_objects.score1->get_scorer(name)(km, X_test, y_test)
A:sklearn.metrics.tests.test_score_objects.score2->getattr(cluster_module, name)(y_test, km.predict(X_test))
A:sklearn.metrics.tests.test_score_objects.unpickled_scorer->pickle.loads(pickle.dumps(scorer))
A:sklearn.metrics.tests.test_score_objects.score3->roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])
A:sklearn.metrics.tests.test_score_objects.diabetes->load_diabetes()
A:sklearn.metrics.tests.test_score_objects.logscore->get_scorer('neg_log_loss')(clf, X_test, y_test)
A:sklearn.metrics.tests.test_score_objects.logloss->log_loss(y_test, clf.predict_proba(X_test))
A:sklearn.metrics.tests.test_score_objects.reg->DecisionTreeRegressor()
A:sklearn.metrics.tests.test_score_objects.y_proba->LogisticRegression(random_state=0).decision_function(X_test)
A:sklearn.metrics.tests.test_score_objects.km->KMeans(n_clusters=3)
A:sklearn.metrics.tests.test_score_objects.f1_scorer_no_average->make_scorer(f1_score, average=None)
A:sklearn.metrics.tests.test_score_objects.grid_search->GridSearchCV(clf, scoring=f1_scorer_no_average, param_grid={'max_depth': [1, 2]})
A:sklearn.metrics.tests.test_score_objects.split->train_test_split(X, y, y_ml, random_state=0)
A:sklearn.metrics.tests.test_score_objects.sample_weight->numpy.ones_like(y_test)
A:sklearn.metrics.tests.test_score_objects.weighted->scorer(estimator[name], X_test, target, sample_weight=sample_weight)
A:sklearn.metrics.tests.test_score_objects.ignored->scorer(estimator[name], X_test[10:], target[10:])
A:sklearn.metrics.tests.test_score_objects.unweighted->scorer(estimator[name], X_test, target)
A:sklearn.metrics.tests.test_score_objects.score->scorer(estimator, X_mm, y_mm)
sklearn.metrics.tests.test_score_objects.DummyScorer(self,est,X,y)
sklearn.metrics.tests.test_score_objects.DummyScorer.__call__(self,est,X,y)
sklearn.metrics.tests.test_score_objects.EstimatorWithFit(BaseEstimator)
sklearn.metrics.tests.test_score_objects.EstimatorWithFit.fit(self,X,y)
sklearn.metrics.tests.test_score_objects.EstimatorWithFitAndPredict(object)
sklearn.metrics.tests.test_score_objects.EstimatorWithFitAndPredict.fit(self,X,y)
sklearn.metrics.tests.test_score_objects.EstimatorWithFitAndPredict.predict(self,X)
sklearn.metrics.tests.test_score_objects.EstimatorWithFitAndScore(object)
sklearn.metrics.tests.test_score_objects.EstimatorWithFitAndScore.fit(self,X,y)
sklearn.metrics.tests.test_score_objects.EstimatorWithFitAndScore.score(self,X,y)
sklearn.metrics.tests.test_score_objects.EstimatorWithoutFit(object)
sklearn.metrics.tests.test_score_objects._make_estimators(X_train,y_train,y_ml_train)
sklearn.metrics.tests.test_score_objects.check_multimetric_scoring_single_metric_wrapper(*args,**kwargs)
sklearn.metrics.tests.test_score_objects.check_scorer_memmap(scorer_name)
sklearn.metrics.tests.test_score_objects.check_scoring_validator_for_single_metric_usecases(scoring_validator)
sklearn.metrics.tests.test_score_objects.setup_module()
sklearn.metrics.tests.test_score_objects.teardown_module()
sklearn.metrics.tests.test_score_objects.test_all_scorers_repr()
sklearn.metrics.tests.test_score_objects.test_check_scoring_and_check_multimetric_scoring()
sklearn.metrics.tests.test_score_objects.test_check_scoring_gridsearchcv()
sklearn.metrics.tests.test_score_objects.test_classification_scores()
sklearn.metrics.tests.test_score_objects.test_deprecated_names()
sklearn.metrics.tests.test_score_objects.test_make_scorer()
sklearn.metrics.tests.test_score_objects.test_raises_on_score_list()
sklearn.metrics.tests.test_score_objects.test_regression_scorers()
sklearn.metrics.tests.test_score_objects.test_scorer_memmap_input()
sklearn.metrics.tests.test_score_objects.test_scorer_sample_weight()
sklearn.metrics.tests.test_score_objects.test_scoring_is_not_metric()
sklearn.metrics.tests.test_score_objects.test_supervised_cluster_scorers()
sklearn.metrics.tests.test_score_objects.test_thresholded_scorers()
sklearn.metrics.tests.test_score_objects.test_thresholded_scorers_multilabel_indicator_data()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/cross_decomposition/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/cross_decomposition/pls_.py----------------------------------------
A:sklearn.cross_decomposition.pls_.X_pinv->pinv2(X, check_finite=False)
A:sklearn.cross_decomposition.pls_.x_weights->numpy.dot(X_pinv, y_score)
A:sklearn.cross_decomposition.pls_.x_score->numpy.dot(X, x_weights)
A:sklearn.cross_decomposition.pls_.Y_pinv->pinv2(Y, check_finite=False)
A:sklearn.cross_decomposition.pls_.y_weights->numpy.dot(Y_pinv, x_score)
A:sklearn.cross_decomposition.pls_.C->numpy.dot(X.T, Y)
A:sklearn.cross_decomposition.pls_.(U, s, Vh)->svd(C, full_matrices=False)
A:sklearn.cross_decomposition.pls_.x_mean->check_array(X, dtype=np.float64).mean(axis=0)
A:sklearn.cross_decomposition.pls_.y_mean->Y.reshape(-1, 1).mean(axis=0)
A:sklearn.cross_decomposition.pls_.x_std->numpy.ones(X.shape[1])
A:sklearn.cross_decomposition.pls_.y_std->numpy.ones(Y.shape[1])
A:sklearn.cross_decomposition.pls_.X->check_array(X, dtype=np.float64)
A:sklearn.cross_decomposition.pls_.Y->Y.reshape(-1, 1).reshape(-1, 1)
A:sklearn.cross_decomposition.pls_.(X, Y, self.x_mean_, self.y_mean_, self.x_std_, self.y_std_)->_center_scale_xy(X, Y, self.scale)
A:sklearn.cross_decomposition.pls_.self.x_scores_->numpy.dot(X, U)
A:sklearn.cross_decomposition.pls_.self.y_scores_->numpy.dot(Y, V)
A:sklearn.cross_decomposition.pls_.self.x_weights_->numpy.zeros((p, self.n_components))
A:sklearn.cross_decomposition.pls_.self.y_weights_->numpy.zeros((q, self.n_components))
A:sklearn.cross_decomposition.pls_.self.x_loadings_->numpy.zeros((p, self.n_components))
A:sklearn.cross_decomposition.pls_.self.y_loadings_->numpy.zeros((q, self.n_components))
A:sklearn.cross_decomposition.pls_.(x_weights, y_weights, n_iter_)->_nipals_twoblocks_inner_loop(X=Xk, Y=Yk, mode=self.mode, max_iter=self.max_iter, tol=self.tol, norm_y_weights=self.norm_y_weights)
A:sklearn.cross_decomposition.pls_.(x_weights, y_weights)->svd_flip(x_weights, y_weights.T)
A:sklearn.cross_decomposition.pls_.x_scores->numpy.dot(Xr, self.x_weights_)
A:sklearn.cross_decomposition.pls_.y_ss->numpy.dot(y_weights.T, y_weights)
A:sklearn.cross_decomposition.pls_.self.x_scores_[:, k]->numpy.dot(Xr, self.x_weights_).ravel()
A:sklearn.cross_decomposition.pls_.self.y_scores_[:, k]->numpy.dot(Yr, self.y_weights_).ravel()
A:sklearn.cross_decomposition.pls_.self.x_weights_[:, k]->numpy.dot(X_pinv, y_score).ravel()
A:sklearn.cross_decomposition.pls_.self.y_weights_[:, k]->numpy.dot(Y_pinv, x_score).ravel()
A:sklearn.cross_decomposition.pls_.self.x_loadings_[:, k]->x_loadings.ravel()
A:sklearn.cross_decomposition.pls_.self.y_loadings_[:, k]->y_loadings.ravel()
A:sklearn.cross_decomposition.pls_.self.x_rotations_->numpy.dot(self.x_weights_, pinv2(np.dot(self.x_loadings_.T, self.x_weights_), check_finite=False))
A:sklearn.cross_decomposition.pls_.self.y_rotations_->numpy.ones(1)
A:sklearn.cross_decomposition.pls_.self.coef_->numpy.dot(self.x_rotations_, self.y_loadings_.T)
A:sklearn.cross_decomposition.pls_.y_scores->numpy.dot(Yr, self.y_weights_)
A:sklearn.cross_decomposition.pls_.Ypred->numpy.dot(X, self.coef_)
A:sklearn.cross_decomposition.pls_.(U, s, V)->svds(C, k=self.n_components)
A:sklearn.cross_decomposition.pls_.(U, V)->svd_flip(U, V)
sklearn.cross_decomposition.PLSCanonical(self,n_components=2,scale=True,algorithm='nipals',max_iter=500,tol=1e-06,copy=True)
sklearn.cross_decomposition.PLSRegression(self,n_components=2,scale=True,max_iter=500,tol=1e-06,copy=True)
sklearn.cross_decomposition.PLSSVD(self,n_components=2,scale=True,copy=True)
sklearn.cross_decomposition.PLSSVD.fit(self,X,Y)
sklearn.cross_decomposition.PLSSVD.fit_transform(self,X,y=None)
sklearn.cross_decomposition.PLSSVD.transform(self,X,Y=None)
sklearn.cross_decomposition._PLS(self,n_components=2,scale=True,deflation_mode='regression',mode='A',algorithm='nipals',norm_y_weights=False,max_iter=500,tol=1e-06,copy=True)
sklearn.cross_decomposition._PLS.fit(self,X,Y)
sklearn.cross_decomposition._PLS.fit_transform(self,X,y=None)
sklearn.cross_decomposition._PLS.predict(self,X,copy=True)
sklearn.cross_decomposition._PLS.transform(self,X,Y=None,copy=True)
sklearn.cross_decomposition._center_scale_xy(X,Y,scale=True)
sklearn.cross_decomposition._nipals_twoblocks_inner_loop(X,Y,mode='A',max_iter=500,tol=1e-06,norm_y_weights=False)
sklearn.cross_decomposition._svd_cross_product(X,Y)
sklearn.cross_decomposition.pls_.PLSCanonical(self,n_components=2,scale=True,algorithm='nipals',max_iter=500,tol=1e-06,copy=True)
sklearn.cross_decomposition.pls_.PLSCanonical.__init__(self,n_components=2,scale=True,algorithm='nipals',max_iter=500,tol=1e-06,copy=True)
sklearn.cross_decomposition.pls_.PLSRegression(self,n_components=2,scale=True,max_iter=500,tol=1e-06,copy=True)
sklearn.cross_decomposition.pls_.PLSRegression.__init__(self,n_components=2,scale=True,max_iter=500,tol=1e-06,copy=True)
sklearn.cross_decomposition.pls_.PLSSVD(self,n_components=2,scale=True,copy=True)
sklearn.cross_decomposition.pls_.PLSSVD.__init__(self,n_components=2,scale=True,copy=True)
sklearn.cross_decomposition.pls_.PLSSVD.fit(self,X,Y)
sklearn.cross_decomposition.pls_.PLSSVD.fit_transform(self,X,y=None)
sklearn.cross_decomposition.pls_.PLSSVD.transform(self,X,Y=None)
sklearn.cross_decomposition.pls_._PLS(self,n_components=2,scale=True,deflation_mode='regression',mode='A',algorithm='nipals',norm_y_weights=False,max_iter=500,tol=1e-06,copy=True)
sklearn.cross_decomposition.pls_._PLS.__init__(self,n_components=2,scale=True,deflation_mode='regression',mode='A',algorithm='nipals',norm_y_weights=False,max_iter=500,tol=1e-06,copy=True)
sklearn.cross_decomposition.pls_._PLS.fit(self,X,Y)
sklearn.cross_decomposition.pls_._PLS.fit_transform(self,X,y=None)
sklearn.cross_decomposition.pls_._PLS.predict(self,X,copy=True)
sklearn.cross_decomposition.pls_._PLS.transform(self,X,Y=None,copy=True)
sklearn.cross_decomposition.pls_._center_scale_xy(X,Y,scale=True)
sklearn.cross_decomposition.pls_._nipals_twoblocks_inner_loop(X,Y,mode='A',max_iter=500,tol=1e-06,norm_y_weights=False)
sklearn.cross_decomposition.pls_._svd_cross_product(X,Y)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/cross_decomposition/cca_.py----------------------------------------
sklearn.cross_decomposition.CCA(self,n_components=2,scale=True,max_iter=500,tol=1e-06,copy=True)
sklearn.cross_decomposition.cca_.CCA(self,n_components=2,scale=True,max_iter=500,tol=1e-06,copy=True)
sklearn.cross_decomposition.cca_.CCA.__init__(self,n_components=2,scale=True,max_iter=500,tol=1e-06,copy=True)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/cross_decomposition/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/cross_decomposition/tests/test_pls.py----------------------------------------
A:sklearn.cross_decomposition.tests.test_pls.d->load_linnerud()
A:sklearn.cross_decomposition.tests.test_pls.pls_bynipals->sklearn.cross_decomposition.pls_.PLSCanonical(n_components=X.shape[1])
A:sklearn.cross_decomposition.tests.test_pls.pls_bysvd->sklearn.cross_decomposition.pls_.PLSCanonical(algorithm='svd', n_components=X.shape[1])
A:sklearn.cross_decomposition.tests.test_pls.plsca->sklearn.cross_decomposition.pls_.PLSCanonical(n_components=X.shape[1])
A:sklearn.cross_decomposition.tests.test_pls.K->numpy.dot(M.T, M)
A:sklearn.cross_decomposition.tests.test_pls.(Xc, Yc, x_mean, y_mean, x_std, y_std)->sklearn.cross_decomposition.pls_._center_scale_xy(X.copy(), Y.copy(), scale=True)
A:sklearn.cross_decomposition.tests.test_pls.Xr->sklearn.cross_decomposition.pls_.PLSCanonical(n_components=X.shape[1]).transform(X)
A:sklearn.cross_decomposition.tests.test_pls.(Xr, Yr)->sklearn.cross_decomposition.pls_.PLSCanonical(n_components=X.shape[1]).transform(X, Y)
A:sklearn.cross_decomposition.tests.test_pls.pls_ca->sklearn.cross_decomposition.pls_.PLSCanonical(n_components=3)
A:sklearn.cross_decomposition.tests.test_pls.x_weights->numpy.array([[0.65803719, 0.19197924, 0.21769083], [0.7009113, 0.13303969, -0.15376699], [0.13528197, -0.68636408, 0.13856546], [0.16854574, -0.66788088, -0.12485304], [-0.03232333, -0.04189855, 0.40690153], [0.1148816, -0.09643158, 0.1613305], [0.04792138, -0.02384992, 0.17175319], [-0.06781, -0.01666137, -0.18556747], [-0.00266945, -0.00160224, 0.11893098], [-0.00849528, -0.07706095, 0.1570547], [-0.00949471, -0.02964127, 0.34657036], [-0.03572177, 0.0945091, 0.3414855], [0.05584937, -0.02028961, -0.57682568], [0.05744254, -0.01482333, -0.17431274]])
A:sklearn.cross_decomposition.tests.test_pls.x_rotations->numpy.array([[-0.61330704, 0.41591889, -0.62297525], [-0.74697144, 0.31388326, 0.77368233], [-0.25668686, -0.89237972, -0.24121788]])
A:sklearn.cross_decomposition.tests.test_pls.y_weights->numpy.array([[0.66101097, 0.18672553, 0.22826092], [0.69347861, 0.18463471, -0.23995597], [0.14462724, -0.66504085, 0.17082434], [0.22247955, -0.6932605, -0.09832993], [0.07035859, 0.00714283, 0.67810124], [0.07765351, -0.0105204, -0.44108074], [-0.00917056, 0.04322147, 0.10062478], [-0.01909512, 0.06182718, 0.28830475], [0.01756709, 0.04797666, 0.32225745]])
A:sklearn.cross_decomposition.tests.test_pls.y_rotations->numpy.array([[+0.58989127, 0.7168115, 0.30665872], [+0.77134053, -0.70791757, 0.19786539], [-0.2388767, -0.00343595, 0.94162826]])
A:sklearn.cross_decomposition.tests.test_pls.pls_2->sklearn.cross_decomposition.pls_.PLSRegression(n_components=X.shape[1])
A:sklearn.cross_decomposition.tests.test_pls.x_loadings->numpy.array([[0.65649254, 0.1847647, 0.15270699], [0.67554234, 0.15237508, -0.09182247], [0.19219925, -0.67750975, 0.08673128], [0.2133631, -0.67034809, -0.08835483], [-0.03178912, -0.06668336, 0.43395268], [0.15684588, -0.13350241, 0.20578984], [0.03337736, -0.03807306, 0.09871553], [-0.06199844, 0.01559854, -0.1881785], [0.00406146, -0.00587025, 0.16413253], [-0.00374239, -0.05848466, 0.19140336], [0.00139214, -0.01033161, 0.32239136], [-0.05292828, 0.0953533, 0.31916881], [0.04031924, -0.01961045, -0.65174036], [0.06172484, -0.06597366, -0.1244497]])
A:sklearn.cross_decomposition.tests.test_pls.y_loadings->numpy.array([[0.68568625, 0.1674376, 0.0969508], [0.68782064, 0.20375837, -0.1164448], [0.11712173, -0.68046903, 0.12001505], [0.17860457, -0.6798319, -0.05089681], [0.06265739, -0.0277703, 0.74729584], [0.0914178, 0.00403751, -0.5135078], [-0.02196918, -0.01377169, 0.09564505], [-0.03288952, 0.09039729, 0.31858973], [0.04287624, 0.05254676, 0.27836841]])
A:sklearn.cross_decomposition.tests.test_pls.rng->check_random_state(0)
A:sklearn.cross_decomposition.tests.test_pls.l1->check_random_state(0).normal(size=n)
A:sklearn.cross_decomposition.tests.test_pls.l2->check_random_state(0).normal(size=n)
A:sklearn.cross_decomposition.tests.test_pls.X->numpy.concatenate((X, rng.normal(size=p_noise * n).reshape(n, p_noise)), axis=1)
A:sklearn.cross_decomposition.tests.test_pls.Y->check_random_state(0).randn(n_samples, n_targets)
A:sklearn.cross_decomposition.tests.test_pls.pls->sklearn.cross_decomposition.pls_.PLSRegression(n_components=5, scale=True)
A:sklearn.cross_decomposition.tests.test_pls.clf->sklearn.cross_decomposition.pls_.PLSCanonical()
A:sklearn.cross_decomposition.tests.test_pls.X_copy->numpy.concatenate((X, rng.normal(size=p_noise * n).reshape(n, p_noise)), axis=1).copy()
A:sklearn.cross_decomposition.tests.test_pls.Y_copy->check_random_state(0).randn(n_samples, n_targets).copy()
A:sklearn.cross_decomposition.tests.test_pls.X2->numpy.array([[0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [2.0, 2.0, 2.0], [3.0, 5.0, 4.0]])
A:sklearn.cross_decomposition.tests.test_pls.Y2->numpy.array([[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]])
A:sklearn.cross_decomposition.tests.test_pls.X_std->numpy.concatenate((X, rng.normal(size=p_noise * n).reshape(n, p_noise)), axis=1).std(axis=0, ddof=1)
A:sklearn.cross_decomposition.tests.test_pls.Y_std->check_random_state(0).randn(n_samples, n_targets).std(axis=0, ddof=1)
A:sklearn.cross_decomposition.tests.test_pls.(X_score, Y_score)->sklearn.cross_decomposition.pls_.PLSCanonical().fit_transform(X_s, Y_s)
A:sklearn.cross_decomposition.tests.test_pls.(X_s_score, Y_s_score)->sklearn.cross_decomposition.pls_.PLSCanonical().fit_transform(X_s, Y_s)
A:sklearn.cross_decomposition.tests.test_pls.Q->check_random_state(0).randn(n_targets, n_features)
A:sklearn.cross_decomposition.tests.test_pls.X_scaled->StandardScaler().fit_transform(X)
A:sklearn.cross_decomposition.tests.test_pls.score->sklearn.cross_decomposition.pls_.PLSRegression(n_components=5, scale=True).score(X, Y)
A:sklearn.cross_decomposition.tests.test_pls.score_scaled->sklearn.cross_decomposition.pls_.PLSRegression(n_components=5, scale=True).score(X_scaled, Y)
sklearn.cross_decomposition.tests.test_pls.test_PLSSVD()
sklearn.cross_decomposition.tests.test_pls.test_pls()
sklearn.cross_decomposition.tests.test_pls.test_pls_errors()
sklearn.cross_decomposition.tests.test_pls.test_pls_scaling()
sklearn.cross_decomposition.tests.test_pls.test_predict_transform_copy()
sklearn.cross_decomposition.tests.test_pls.test_scale_and_stability()
sklearn.cross_decomposition.tests.test_pls.test_univariate_pls_regression()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/semi_supervised/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/semi_supervised/label_propagation.py----------------------------------------
A:sklearn.semi_supervised.label_propagation.self.nn_fit->NearestNeighbors(self.n_neighbors, n_jobs=self.n_jobs).fit(X)
A:sklearn.semi_supervised.label_propagation.probas->self.predict_proba(X)
A:sklearn.semi_supervised.label_propagation.X_2d->check_array(X, accept_sparse=['csc', 'csr', 'coo', 'dok', 'bsr', 'lil', 'dia'])
A:sklearn.semi_supervised.label_propagation.weight_matrices->self._get_kernel(self.X_, X_2d)
A:sklearn.semi_supervised.label_propagation.ine->numpy.sum(self.label_distributions_[weight_matrix], axis=0)
A:sklearn.semi_supervised.label_propagation.probabilities->numpy.dot(weight_matrices, self.label_distributions_)
A:sklearn.semi_supervised.label_propagation.(X, y)->check_X_y(X, y)
A:sklearn.semi_supervised.label_propagation.graph_matrix->graph_matrix.tocsr().tocsr()
A:sklearn.semi_supervised.label_propagation.classes->numpy.unique(y)
A:sklearn.semi_supervised.label_propagation.y->numpy.asarray(y)
A:sklearn.semi_supervised.label_propagation.self.label_distributions_->numpy.where(unlabeled, self.label_distributions_, y_static)
A:sklearn.semi_supervised.label_propagation.y_static->numpy.copy(self.label_distributions_)
A:sklearn.semi_supervised.label_propagation.l_previous->numpy.zeros((self.X_.shape[0], n_classes))
A:sklearn.semi_supervised.label_propagation.self.transduction_->transduction.ravel()
A:sklearn.semi_supervised.label_propagation.affinity_matrix->self._get_kernel(self.X_)
A:sklearn.semi_supervised.label_propagation.normalizer->self._get_kernel(self.X_).sum(axis=0)
A:sklearn.semi_supervised.label_propagation.laplacian->scipy.sparse.csgraph.laplacian(affinity_matrix, normed=True)
sklearn.semi_supervised.LabelPropagation(self,kernel='rbf',gamma=20,n_neighbors=7,alpha=None,max_iter=1000,tol=0.001,n_jobs=1)
sklearn.semi_supervised.LabelPropagation._build_graph(self)
sklearn.semi_supervised.LabelPropagation.fit(self,X,y)
sklearn.semi_supervised.LabelSpreading(self,kernel='rbf',gamma=20,n_neighbors=7,alpha=0.2,max_iter=30,tol=0.001,n_jobs=1)
sklearn.semi_supervised.LabelSpreading._build_graph(self)
sklearn.semi_supervised.label_propagation.BaseLabelPropagation(self,kernel='rbf',gamma=20,n_neighbors=7,alpha=1,max_iter=30,tol=0.001,n_jobs=1)
sklearn.semi_supervised.label_propagation.BaseLabelPropagation.__init__(self,kernel='rbf',gamma=20,n_neighbors=7,alpha=1,max_iter=30,tol=0.001,n_jobs=1)
sklearn.semi_supervised.label_propagation.BaseLabelPropagation._build_graph(self)
sklearn.semi_supervised.label_propagation.BaseLabelPropagation._get_kernel(self,X,y=None)
sklearn.semi_supervised.label_propagation.BaseLabelPropagation.fit(self,X,y)
sklearn.semi_supervised.label_propagation.BaseLabelPropagation.predict(self,X)
sklearn.semi_supervised.label_propagation.BaseLabelPropagation.predict_proba(self,X)
sklearn.semi_supervised.label_propagation.LabelPropagation(self,kernel='rbf',gamma=20,n_neighbors=7,alpha=None,max_iter=1000,tol=0.001,n_jobs=1)
sklearn.semi_supervised.label_propagation.LabelPropagation.__init__(self,kernel='rbf',gamma=20,n_neighbors=7,alpha=None,max_iter=1000,tol=0.001,n_jobs=1)
sklearn.semi_supervised.label_propagation.LabelPropagation._build_graph(self)
sklearn.semi_supervised.label_propagation.LabelPropagation.fit(self,X,y)
sklearn.semi_supervised.label_propagation.LabelSpreading(self,kernel='rbf',gamma=20,n_neighbors=7,alpha=0.2,max_iter=30,tol=0.001,n_jobs=1)
sklearn.semi_supervised.label_propagation.LabelSpreading.__init__(self,kernel='rbf',gamma=20,n_neighbors=7,alpha=0.2,max_iter=30,tol=0.001,n_jobs=1)
sklearn.semi_supervised.label_propagation.LabelSpreading._build_graph(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/semi_supervised/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/semi_supervised/tests/test_label_propagation.py----------------------------------------
A:sklearn.semi_supervised.tests.test_label_propagation.clf->sklearn.semi_supervised.label_propagation.LabelPropagation(max_iter=10000, gamma=0.1)
A:sklearn.semi_supervised.tests.test_label_propagation.(X, y)->make_classification(n_classes=n_classes, n_samples=200, random_state=0)
A:sklearn.semi_supervised.tests.test_label_propagation.lp_default->sklearn.semi_supervised.label_propagation.LabelPropagation(kernel='rbf', gamma=0.1)
A:sklearn.semi_supervised.tests.test_label_propagation.lp_0->sklearn.semi_supervised.label_propagation.LabelPropagation(alpha=0, kernel='rbf', gamma=0.1)
A:sklearn.semi_supervised.tests.test_label_propagation.S->sklearn.semi_supervised.label_propagation.LabelPropagation(max_iter=10000, gamma=0.1)._build_graph()
A:sklearn.semi_supervised.tests.test_label_propagation.Y->numpy.zeros((len(y), n_classes + 1))
A:sklearn.semi_supervised.tests.test_label_propagation.expected->numpy.zeros((len(y), n_classes + 1)).copy()
A:sklearn.semi_supervised.tests.test_label_propagation.T_bar->sklearn.semi_supervised.label_propagation.LabelPropagation(max_iter=10000, gamma=0.1)._build_graph()
A:sklearn.semi_supervised.tests.test_label_propagation.Y_u->numpy.dot(np.dot(np.linalg.inv(np.eye(Tuu.shape[0]) - Tuu), Tul), Y_l)
A:sklearn.semi_supervised.tests.test_label_propagation.X->numpy.array([[1.0, 0.0], [0.0, 1.0], [1.0, 2.5]])
A:sklearn.semi_supervised.tests.test_label_propagation.y->numpy.array([0, 1, -1])
A:sklearn.semi_supervised.tests.test_label_propagation.mdl->sklearn.semi_supervised.label_propagation.LabelPropagation(kernel='rbf', max_iter=500)
sklearn.semi_supervised.tests.test_label_propagation.test_alpha_deprecation()
sklearn.semi_supervised.tests.test_label_propagation.test_convergence_speed()
sklearn.semi_supervised.tests.test_label_propagation.test_convergence_warning()
sklearn.semi_supervised.tests.test_label_propagation.test_distribution()
sklearn.semi_supervised.tests.test_label_propagation.test_fit_transduction()
sklearn.semi_supervised.tests.test_label_propagation.test_label_propagation_closed_form()
sklearn.semi_supervised.tests.test_label_propagation.test_label_spreading_closed_form()
sklearn.semi_supervised.tests.test_label_propagation.test_predict()
sklearn.semi_supervised.tests.test_label_propagation.test_predict_proba()
sklearn.semi_supervised.tests.test_label_propagation.test_valid_alpha()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/tests/test_kernel_approximation.py----------------------------------------
A:sklearn.tests.test_kernel_approximation.rng->numpy.random.RandomState(0)
A:sklearn.tests.test_kernel_approximation.X->list(X)
A:sklearn.tests.test_kernel_approximation.Y->numpy.random.RandomState(0).random_sample(size=(300, 50))
A:sklearn.tests.test_kernel_approximation.kernel->rbf_kernel(X, Y, gamma=gamma)
A:sklearn.tests.test_kernel_approximation.transform->SkewedChi2Sampler(skewedness=c, n_components=1000, random_state=42)
A:sklearn.tests.test_kernel_approximation.X_trans->RBFSampler(gamma=gamma, n_components=1000, random_state=42).fit_transform(X)
A:sklearn.tests.test_kernel_approximation.Y_trans->RBFSampler(gamma=gamma, n_components=1000, random_state=42).transform(Y)
A:sklearn.tests.test_kernel_approximation.kernel_approx->numpy.dot(X_trans, Y_trans.T)
A:sklearn.tests.test_kernel_approximation.X_sp_trans->SkewedChi2Sampler(skewedness=c, n_components=1000, random_state=42).fit_transform(csr_matrix(X))
A:sklearn.tests.test_kernel_approximation.Y_sp_trans->SkewedChi2Sampler(skewedness=c, n_components=1000, random_state=42).transform(csr_matrix(Y))
A:sklearn.tests.test_kernel_approximation.Y_neg->numpy.random.RandomState(0).random_sample(size=(300, 50)).copy()
A:sklearn.tests.test_kernel_approximation.rbf_transform->RBFSampler(gamma=gamma, n_components=1000, random_state=42)
A:sklearn.tests.test_kernel_approximation.rnd->numpy.random.RandomState(42)
A:sklearn.tests.test_kernel_approximation.X_transformed->Nystroem(kernel='polynomial', n_components=X.shape[0], degree=3.1, coef0=0.1).fit_transform(X)
A:sklearn.tests.test_kernel_approximation.K->polynomial_kernel(X, degree=3.1, coef0=0.1)
A:sklearn.tests.test_kernel_approximation.trans->Nystroem(n_components=2, kernel=kern, random_state=rnd)
A:sklearn.tests.test_kernel_approximation.kernels_available->kernel_metrics()
A:sklearn.tests.test_kernel_approximation.nystroem->Nystroem(kernel='polynomial', n_components=X.shape[0], degree=3.1, coef0=0.1)
A:sklearn.tests.test_kernel_approximation.K2->numpy.dot(X_transformed, X_transformed.T)
A:sklearn.tests.test_kernel_approximation.N->Nystroem(gamma=gamma, n_components=X.shape[0]).fit(X)
A:sklearn.tests.test_kernel_approximation.ny->Nystroem(kernel=linear_kernel, **param)
sklearn.tests.test_kernel_approximation.test_additive_chi2_sampler()
sklearn.tests.test_kernel_approximation.test_input_validation()
sklearn.tests.test_kernel_approximation.test_nystroem_approximation()
sklearn.tests.test_kernel_approximation.test_nystroem_callable()
sklearn.tests.test_kernel_approximation.test_nystroem_default_parameters()
sklearn.tests.test_kernel_approximation.test_nystroem_poly_kernel_params()
sklearn.tests.test_kernel_approximation.test_nystroem_singular_kernel()
sklearn.tests.test_kernel_approximation.test_rbf_sampler()
sklearn.tests.test_kernel_approximation.test_skewed_chi2_sampler()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/tests/test_base.py----------------------------------------
A:sklearn.tests.test_base.self.a->a.copy()
A:sklearn.tests.test_base.selector->SelectFpr(f_classif, alpha=0.1)
A:sklearn.tests.test_base.new_selector->clone(selector)
A:sklearn.tests.test_base.buggy->Buggy()
A:sklearn.tests.test_base.no_estimator->NoEstimator()
A:sklearn.tests.test_base.varg_est->VargEstimator()
A:sklearn.tests.test_base.clf->Pipeline([('svc', SVC())])
A:sklearn.tests.test_base.clf2->clone(clf)
A:sklearn.tests.test_base.est->DeprecatedAttributeEstimator(a=1)
A:sklearn.tests.test_base.sparse_matrix->cls(np.eye(5))
A:sklearn.tests.test_base.clf_cloned->clone(clf)
A:sklearn.tests.test_base.my_estimator->MyEstimator()
A:sklearn.tests.test_base.test->T(K(), K())
A:sklearn.tests.test_base.some_est->T(a=['long_params'] * 1000)
A:sklearn.tests.test_base.svc->SVC()
A:sklearn.tests.test_base.rng->numpy.random.RandomState(0)
A:sklearn.tests.test_base.sample_weight->numpy.random.RandomState(0).randint(1, 10, size=len(ds.target))
A:sklearn.tests.test_base.d->numpy.arange(10)
A:sklearn.tests.test_base.df->MockDataFrame(d)
A:sklearn.tests.test_base.e->DummyEstimator(df, scalar_param=1)
A:sklearn.tests.test_base.cloned_e->clone(e)
A:sklearn.tests.test_base.iris->sklearn.datasets.load_iris()
A:sklearn.tests.test_base.tree->TreeNoVersion().fit(iris.data, iris.target)
A:sklearn.tests.test_base.tree_pickle->pickle.dumps(tree)
A:sklearn.tests.test_base.tree_restored->assert_no_warnings(pickle.loads, tree_pickle)
A:sklearn.tests.test_base.score_of_original->TreeNoVersion().fit(iris.data, iris.target).score(iris.data, iris.target)
A:sklearn.tests.test_base.score_of_restored->assert_no_warnings(pickle.loads, tree_pickle).score(iris.data, iris.target)
A:sklearn.tests.test_base.tree_pickle_other->pickle.dumps(tree)
A:sklearn.tests.test_base.message->pickle_error_message.format(estimator='TreeNoVersion', old_version='pre-0.18', current_version=sklearn.__version__)
A:sklearn.tests.test_base.tree_pickle_noversion->pickle.dumps(tree)
A:sklearn.tests.test_base.data->self.__dict__.copy()
A:sklearn.tests.test_base.estimator->SingleInheritanceEstimator()
A:sklearn.tests.test_base.serialized->pickle.dumps(estimator)
A:sklearn.tests.test_base.estimator_restored->pickle.loads(serialized)
sklearn.tests.test_base.Buggy(self,a=None)
sklearn.tests.test_base.Buggy.__init__(self,a=None)
sklearn.tests.test_base.DeprecatedAttributeEstimator(self,a=None,b=None)
sklearn.tests.test_base.DeprecatedAttributeEstimator.__init__(self,a=None,b=None)
sklearn.tests.test_base.DeprecatedAttributeEstimator.b(self)
sklearn.tests.test_base.DontPickleAttributeMixin(object)
sklearn.tests.test_base.DontPickleAttributeMixin.__getstate__(self)
sklearn.tests.test_base.DontPickleAttributeMixin.__setstate__(self,state)
sklearn.tests.test_base.K(self,c=None,d=None)
sklearn.tests.test_base.K.__init__(self,c=None,d=None)
sklearn.tests.test_base.ModifyInitParams(self,a=np.array([0]))
sklearn.tests.test_base.ModifyInitParams.__init__(self,a=np.array([0]))
sklearn.tests.test_base.MultiInheritanceEstimator(self,attribute_pickled=5)
sklearn.tests.test_base.MultiInheritanceEstimator.__init__(self,attribute_pickled=5)
sklearn.tests.test_base.MyEstimator(self,l1=0,empty=None)
sklearn.tests.test_base.MyEstimator.__init__(self,l1=0,empty=None)
sklearn.tests.test_base.NoEstimator(self)
sklearn.tests.test_base.NoEstimator.__init__(self)
sklearn.tests.test_base.NoEstimator.fit(self,X=None,y=None)
sklearn.tests.test_base.NoEstimator.predict(self,X=None)
sklearn.tests.test_base.SingleInheritanceEstimator(self,attribute_pickled=5)
sklearn.tests.test_base.SingleInheritanceEstimator.__getstate__(self)
sklearn.tests.test_base.SingleInheritanceEstimator.__init__(self,attribute_pickled=5)
sklearn.tests.test_base.T(self,a=None,b=None)
sklearn.tests.test_base.T.__init__(self,a=None,b=None)
sklearn.tests.test_base.TreeBadVersion(DecisionTreeClassifier)
sklearn.tests.test_base.TreeBadVersion.__getstate__(self)
sklearn.tests.test_base.TreeNoVersion(DecisionTreeClassifier)
sklearn.tests.test_base.TreeNoVersion.__getstate__(self)
sklearn.tests.test_base.VargEstimator(self,*vargs)
sklearn.tests.test_base.VargEstimator.__init__(self,*vargs)
sklearn.tests.test_base.test_clone()
sklearn.tests.test_base.test_clone_2()
sklearn.tests.test_base.test_clone_buggy()
sklearn.tests.test_base.test_clone_copy_init_params()
sklearn.tests.test_base.test_clone_empty_array()
sklearn.tests.test_base.test_clone_nan()
sklearn.tests.test_base.test_clone_pandas_dataframe()
sklearn.tests.test_base.test_clone_sparse_matrices()
sklearn.tests.test_base.test_get_params()
sklearn.tests.test_base.test_get_params_deprecated()
sklearn.tests.test_base.test_is_classifier()
sklearn.tests.test_base.test_pickle_version_no_warning_is_issued_with_non_sklearn_estimator()
sklearn.tests.test_base.test_pickle_version_warning_is_issued_upon_different_version()
sklearn.tests.test_base.test_pickle_version_warning_is_issued_when_no_version_info_in_pickle()
sklearn.tests.test_base.test_pickle_version_warning_is_not_raised_with_matching_version()
sklearn.tests.test_base.test_pickling_when_getstate_is_overwritten_by_mixin()
sklearn.tests.test_base.test_pickling_when_getstate_is_overwritten_by_mixin_outside_of_sklearn()
sklearn.tests.test_base.test_pickling_works_when_getstate_is_overwritten_in_the_child_class()
sklearn.tests.test_base.test_repr()
sklearn.tests.test_base.test_score_sample_weight()
sklearn.tests.test_base.test_set_params()
sklearn.tests.test_base.test_set_params_passes_all_parameters()
sklearn.tests.test_base.test_str()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/tests/test_calibration.py----------------------------------------
A:sklearn.tests.test_calibration.(X, y)->make_classification(n_samples=10, n_features=5, n_classes=num_classes)
A:sklearn.tests.test_calibration.sample_weight->numpy.random.RandomState(seed=42).uniform(size=y.size)
A:sklearn.tests.test_calibration.clf->LinearSVC(C=1.0)
A:sklearn.tests.test_calibration.pc_clf->CalibratedClassifierCV(clf, method=method, cv='prefit')
A:sklearn.tests.test_calibration.clf_invalid_method->CalibratedClassifierCV(clf, method='foo')
A:sklearn.tests.test_calibration.clf_base_regressor->CalibratedClassifierCV(RandomForestRegressor(), method='sigmoid')
A:sklearn.tests.test_calibration.base_estimator->LinearSVC(random_state=42)
A:sklearn.tests.test_calibration.calibrated_clf->CalibratedClassifierCV(base_estimator, method=method)
A:sklearn.tests.test_calibration.probs_with_sw->CalibratedClassifierCV(base_estimator, method=method).predict_proba(X_test)
A:sklearn.tests.test_calibration.probs_without_sw->CalibratedClassifierCV(base_estimator, method=method).predict_proba(X_test)
A:sklearn.tests.test_calibration.diff->numpy.linalg.norm(probs_with_sw - probs_without_sw)
A:sklearn.tests.test_calibration.(X, y_idx)->make_blobs(n_samples=100, n_features=2, random_state=42, centers=3, cluster_std=3.0)
A:sklearn.tests.test_calibration.target_names->numpy.array(['a', 'b', 'c'])
A:sklearn.tests.test_calibration.cal_clf->CalibratedClassifierCV(clf, method='sigmoid', cv=LeaveOneOut())
A:sklearn.tests.test_calibration.probas->CalibratedClassifierCV(clf, method='sigmoid', cv=LeaveOneOut()).predict_proba(X_test)
A:sklearn.tests.test_calibration.e->numpy.exp(-y_pred)
A:sklearn.tests.test_calibration.uncalibrated_log_loss->log_loss(y_test, softmax(clf.decision_function(X_test)))
A:sklearn.tests.test_calibration.calibrated_log_loss->log_loss(y_test, probas)
A:sklearn.tests.test_calibration.clf_probs->LinearSVC(C=1.0).predict_proba(X_test)
A:sklearn.tests.test_calibration.loss->log_loss(y_test, clf_probs)
A:sklearn.tests.test_calibration.cal_clf_probs->CalibratedClassifierCV(clf, method='sigmoid', cv=LeaveOneOut()).predict_proba(X_test)
A:sklearn.tests.test_calibration.cal_loss->log_loss(y_test, cal_clf_probs)
A:sklearn.tests.test_calibration.y_prob->CalibratedClassifierCV(clf, method=method, cv='prefit').predict_proba(this_X_test)
A:sklearn.tests.test_calibration.y_pred->numpy.array([0.0, 0.1, 0.2, 0.8, 0.9, 1.0])
A:sklearn.tests.test_calibration.exF->numpy.array([5, -4, 1.0])
A:sklearn.tests.test_calibration.exY->numpy.array([1, -1, -1])
A:sklearn.tests.test_calibration.AB_lin_libsvm->numpy.array([-0.20261354391187855, 0.6523631498001051])
A:sklearn.tests.test_calibration.sk_prob->_SigmoidCalibration().fit(exF, exY).predict(exF)
A:sklearn.tests.test_calibration.y_true->numpy.array([0, 0, 0, 1, 1, 1])
A:sklearn.tests.test_calibration.(prob_true, prob_pred)->calibration_curve(y_true, y_pred, n_bins=2)
A:sklearn.tests.test_calibration.(prob_true_unnormalized, prob_pred_unnormalized)->calibration_curve(y_true, y_pred * 2, n_bins=2, normalize=True)
A:sklearn.tests.test_calibration.clf_c->CalibratedClassifierCV(clf, cv=2, method='isotonic')
A:sklearn.tests.test_calibration.clf_prob->CalibratedClassifierCV(clf, method='sigmoid', cv=LeaveOneOut())
A:sklearn.tests.test_calibration.probs->CalibratedClassifierCV(clf, method='sigmoid', cv=LeaveOneOut()).predict_proba(X)
A:sklearn.tests.test_calibration.X->numpy.random.randn(10, 5)
A:sklearn.tests.test_calibration.y->numpy.arange(10)
A:sklearn.tests.test_calibration.proba->calibrated_classifier.predict_proba(X)
sklearn.tests.test_calibration.test_calibration()
sklearn.tests.test_calibration.test_calibration_curve()
sklearn.tests.test_calibration.test_calibration_less_classes()
sklearn.tests.test_calibration.test_calibration_multiclass()
sklearn.tests.test_calibration.test_calibration_nan_imputer()
sklearn.tests.test_calibration.test_calibration_prefit()
sklearn.tests.test_calibration.test_calibration_prob_sum()
sklearn.tests.test_calibration.test_sample_weight()
sklearn.tests.test_calibration.test_sigmoid_calibration()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/tests/test_discriminant_analysis.py----------------------------------------
A:sklearn.tests.test_discriminant_analysis.X->(means[:, np.newaxis, :] + scatter[np.newaxis, :, :]).reshape((-1, 3))
A:sklearn.tests.test_discriminant_analysis.y->numpy.repeat(np.arange(means.shape[0]), scatter.shape[0])
A:sklearn.tests.test_discriminant_analysis.y3->numpy.array([1, 1, 2, 2, 3, 3])
A:sklearn.tests.test_discriminant_analysis.X1->numpy.array([[-2], [-1], [-1], [1], [1], [2]], dtype='f')
A:sklearn.tests.test_discriminant_analysis.X6->numpy.array([[0, 0], [-2, -2], [-2, -1], [-1, -1], [-1, -2], [1, 3], [1, 2], [2, 1], [2, 2]])
A:sklearn.tests.test_discriminant_analysis.y6->numpy.array([1, 1, 1, 1, 1, 2, 2, 2, 2])
A:sklearn.tests.test_discriminant_analysis.y7->numpy.array([1, 2, 3, 2, 3, 1, 2, 3, 1])
A:sklearn.tests.test_discriminant_analysis.X7->numpy.array([[-3], [-2], [-1], [-1], [0], [1], [1], [2], [3]])
A:sklearn.tests.test_discriminant_analysis.X2->numpy.array([[-3, 0], [-2, 0], [-1, 0], [-1, 0], [0, 0], [1, 0], [1, 0], [2, 0], [3, 0]])
A:sklearn.tests.test_discriminant_analysis.y4->numpy.array([1, 1, 1, 1, 1, 1, 1, 1, 2])
A:sklearn.tests.test_discriminant_analysis.y5->numpy.array([0, 0, 0, 0, 0, 1, 1, 1])
A:sklearn.tests.test_discriminant_analysis.clf->QuadraticDiscriminantAnalysis(reg_param=0.1)
A:sklearn.tests.test_discriminant_analysis.y_pred->QuadraticDiscriminantAnalysis(reg_param=0.1).predict(X2)
A:sklearn.tests.test_discriminant_analysis.y_pred1->QuadraticDiscriminantAnalysis(reg_param=0.1).fit(X7, y6).predict(X7)
A:sklearn.tests.test_discriminant_analysis.y_proba_pred1->QuadraticDiscriminantAnalysis(reg_param=0.1).predict_proba(X7)
A:sklearn.tests.test_discriminant_analysis.y_log_proba_pred1->QuadraticDiscriminantAnalysis(reg_param=0.1).predict_log_proba(X7)
A:sklearn.tests.test_discriminant_analysis.y_pred3->QuadraticDiscriminantAnalysis(reg_param=0.1).fit(X6, y7).predict(X6)
A:sklearn.tests.test_discriminant_analysis.priors->numpy.array([0.5, 0.6])
A:sklearn.tests.test_discriminant_analysis.prior_norm->numpy.array([0.45, 0.55])
A:sklearn.tests.test_discriminant_analysis.(X, y)->make_blobs(n_samples=n_samples, n_features=n_features, centers=n_classes, random_state=11)
A:sklearn.tests.test_discriminant_analysis.clf_lda_svd->LinearDiscriminantAnalysis(solver='svd')
A:sklearn.tests.test_discriminant_analysis.clf_lda_lsqr->LinearDiscriminantAnalysis(solver='lsqr')
A:sklearn.tests.test_discriminant_analysis.clf_lda_eigen->LinearDiscriminantAnalysis(solver='eigen')
A:sklearn.tests.test_discriminant_analysis.X_transformed->QuadraticDiscriminantAnalysis(reg_param=0.1).fit(X, y).transform(X)
A:sklearn.tests.test_discriminant_analysis.state->numpy.random.RandomState(0)
A:sklearn.tests.test_discriminant_analysis.means->numpy.array([[0, 0, -1], [0, 2, 0], [0, -2, 0], [0, 0, 5]])
A:sklearn.tests.test_discriminant_analysis.scatter->numpy.array([[0.1, 0, 0], [-0.1, 0, 0], [0, 0.1, 0], [0, -0.1, 0], [0, 0, 0.1], [0, 0, -0.1]])
A:sklearn.tests.test_discriminant_analysis.means_transformed->QuadraticDiscriminantAnalysis(reg_param=0.1).transform(means)
A:sklearn.tests.test_discriminant_analysis.rng->numpy.random.RandomState(1234)
A:sklearn.tests.test_discriminant_analysis.n_pos->numpy.sum(y_pred == 2)
A:sklearn.tests.test_discriminant_analysis.n_pos2->numpy.sum(y_pred == 2)
A:sklearn.tests.test_discriminant_analysis.y_pred5->QuadraticDiscriminantAnalysis(reg_param=0.1).predict(X5)
A:sklearn.tests.test_discriminant_analysis.(x, y)->make_blobs(n_samples=100, n_features=5, centers=1, random_state=42)
A:sklearn.tests.test_discriminant_analysis.x->numpy.dot(x, np.arange(x.shape[1] ** 2).reshape(x.shape[1], x.shape[1]))
A:sklearn.tests.test_discriminant_analysis.c_e->_cov(x, 'empirical')
A:sklearn.tests.test_discriminant_analysis.c_s->_cov(x, 'auto')
sklearn.tests.test_discriminant_analysis.test_covariance()
sklearn.tests.test_discriminant_analysis.test_lda_coefs()
sklearn.tests.test_discriminant_analysis.test_lda_explained_variance_ratio()
sklearn.tests.test_discriminant_analysis.test_lda_orthogonality()
sklearn.tests.test_discriminant_analysis.test_lda_predict()
sklearn.tests.test_discriminant_analysis.test_lda_priors()
sklearn.tests.test_discriminant_analysis.test_lda_scaling()
sklearn.tests.test_discriminant_analysis.test_lda_store_covariance()
sklearn.tests.test_discriminant_analysis.test_lda_transform()
sklearn.tests.test_discriminant_analysis.test_qda()
sklearn.tests.test_discriminant_analysis.test_qda_deprecation()
sklearn.tests.test_discriminant_analysis.test_qda_priors()
sklearn.tests.test_discriminant_analysis.test_qda_regularization()
sklearn.tests.test_discriminant_analysis.test_qda_store_covariance()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/tests/test_kernel_ridge.py----------------------------------------
A:sklearn.tests.test_kernel_ridge.(X, y)->make_regression(n_features=10)
A:sklearn.tests.test_kernel_ridge.Xcsr->scipy.sparse.csr_matrix(X)
A:sklearn.tests.test_kernel_ridge.Xcsc->scipy.sparse.csc_matrix(X)
A:sklearn.tests.test_kernel_ridge.pred->Ridge(alpha=1, fit_intercept=False).fit(X, Y).predict(X)
A:sklearn.tests.test_kernel_ridge.pred2->KernelRidge(kernel='linear', alpha=1).fit(X, Y).predict(X)
A:sklearn.tests.test_kernel_ridge.kr->KernelRidge(kernel='linear', alpha=0)
A:sklearn.tests.test_kernel_ridge.K->numpy.dot(X, X.T)
A:sklearn.tests.test_kernel_ridge.K2->numpy.dot(X, X.T).copy()
A:sklearn.tests.test_kernel_ridge.sw->numpy.random.RandomState(0).rand(X.shape[0])
A:sklearn.tests.test_kernel_ridge.pred3->KernelRidge(kernel='linear', alpha=1).fit(X, y).predict(X)
sklearn.tests.test_kernel_ridge.test_kernel_ridge()
sklearn.tests.test_kernel_ridge.test_kernel_ridge_csc()
sklearn.tests.test_kernel_ridge.test_kernel_ridge_csr()
sklearn.tests.test_kernel_ridge.test_kernel_ridge_multi_output()
sklearn.tests.test_kernel_ridge.test_kernel_ridge_precomputed()
sklearn.tests.test_kernel_ridge.test_kernel_ridge_precomputed_kernel_unchanged()
sklearn.tests.test_kernel_ridge.test_kernel_ridge_sample_weights()
sklearn.tests.test_kernel_ridge.test_kernel_ridge_singular_kernel()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/tests/test_learning_curve.py----------------------------------------
A:sklearn.tests.test_learning_curve.(X, y)->make_classification(n_samples=2, n_features=1, n_informative=1, n_redundant=0, n_classes=2, n_clusters_per_class=1, random_state=0)
A:sklearn.tests.test_learning_curve.estimator->MockImprovingEstimator(20)
A:sklearn.tests.test_learning_curve.(train_sizes, train_scores, test_scores)->learning_curve(estimator, X, y, cv=cv, train_sizes=np.linspace(0.1, 1.0, 10))
A:sklearn.tests.test_learning_curve.(X, _)->make_classification(n_samples=30, n_features=1, n_informative=1, n_redundant=0, n_classes=2, n_clusters_per_class=1, random_state=0)
A:sklearn.tests.test_learning_curve.sys.stdout->StringIO()
A:sklearn.tests.test_learning_curve.out->sys.stdout.getvalue()
A:sklearn.tests.test_learning_curve.(_, _, test_scores)->learning_curve(estimator, X, y, cv=3, error_score=0)
A:sklearn.tests.test_learning_curve.train_sizes->numpy.linspace(0.2, 1.0, 5)
A:sklearn.tests.test_learning_curve.(train_sizes_inc, train_scores_inc, test_scores_inc)->learning_curve(estimator, X, y, train_sizes=train_sizes, cv=3, exploit_incremental_learning=True)
A:sklearn.tests.test_learning_curve.(train_sizes_batch, train_scores_batch, test_scores_batch)->learning_curve(estimator, X, y, cv=3, train_sizes=train_sizes, exploit_incremental_learning=False)
A:sklearn.tests.test_learning_curve.(train_sizes, _, _)->assert_warns(RuntimeWarning, learning_curve, estimator, X, y, cv=3, train_sizes=np.linspace(0.33, 1.0, 3))
A:sklearn.tests.test_learning_curve.cv->KFold(n=30, n_folds=3)
A:sklearn.tests.test_learning_curve.param_range->numpy.linspace(1, 0, 10)
A:sklearn.tests.test_learning_curve.(train_scores, test_scores)->validation_curve(MockEstimatorWithParameter(), X, y, param_name='param', param_range=param_range, cv=2)
A:sklearn.tests.test_learning_curve.(_, _)->validation_curve(MockEstimatorWithSingleFitCallAllowed(), X, y, param_name='param', param_range=param_range, cv=2)
sklearn.tests.test_learning_curve.MockEstimatorFailing(BaseEstimator)
sklearn.tests.test_learning_curve.MockEstimatorFailing.fit(self,X_subset,y_subset)
sklearn.tests.test_learning_curve.MockEstimatorFailing.score(self,X=None,y=None)
sklearn.tests.test_learning_curve.MockEstimatorWithParameter(self,param=0.5)
sklearn.tests.test_learning_curve.MockEstimatorWithParameter.__init__(self,param=0.5)
sklearn.tests.test_learning_curve.MockEstimatorWithParameter._is_training_data(self,X)
sklearn.tests.test_learning_curve.MockEstimatorWithParameter.fit(self,X_subset,y_subset)
sklearn.tests.test_learning_curve.MockEstimatorWithParameter.predict(self,X)
sklearn.tests.test_learning_curve.MockEstimatorWithParameter.score(self,X=None,y=None)
sklearn.tests.test_learning_curve.MockEstimatorWithSingleFitCallAllowed(MockEstimatorWithParameter)
sklearn.tests.test_learning_curve.MockEstimatorWithSingleFitCallAllowed.fit(self,X_subset,y_subset)
sklearn.tests.test_learning_curve.MockImprovingEstimator(self,n_max_train_sizes)
sklearn.tests.test_learning_curve.MockImprovingEstimator.__init__(self,n_max_train_sizes)
sklearn.tests.test_learning_curve.MockImprovingEstimator._is_training_data(self,X)
sklearn.tests.test_learning_curve.MockImprovingEstimator.fit(self,X_subset,y_subset=None)
sklearn.tests.test_learning_curve.MockImprovingEstimator.predict(self,X)
sklearn.tests.test_learning_curve.MockImprovingEstimator.score(self,X=None,Y=None)
sklearn.tests.test_learning_curve.MockIncrementalImprovingEstimator(self,n_max_train_sizes)
sklearn.tests.test_learning_curve.MockIncrementalImprovingEstimator.__init__(self,n_max_train_sizes)
sklearn.tests.test_learning_curve.MockIncrementalImprovingEstimator._is_training_data(self,X)
sklearn.tests.test_learning_curve.MockIncrementalImprovingEstimator.partial_fit(self,X,y=None,**params)
sklearn.tests.test_learning_curve.test_learning_curve()
sklearn.tests.test_learning_curve.test_learning_curve_batch_and_incremental_learning_are_equal()
sklearn.tests.test_learning_curve.test_learning_curve_error_score()
sklearn.tests.test_learning_curve.test_learning_curve_error_score_default_raise()
sklearn.tests.test_learning_curve.test_learning_curve_incremental_learning()
sklearn.tests.test_learning_curve.test_learning_curve_incremental_learning_not_possible()
sklearn.tests.test_learning_curve.test_learning_curve_incremental_learning_unsupervised()
sklearn.tests.test_learning_curve.test_learning_curve_n_sample_range_out_of_bounds()
sklearn.tests.test_learning_curve.test_learning_curve_remove_duplicate_sample_sizes()
sklearn.tests.test_learning_curve.test_learning_curve_unsupervised()
sklearn.tests.test_learning_curve.test_learning_curve_verbose()
sklearn.tests.test_learning_curve.test_learning_curve_with_boolean_indices()
sklearn.tests.test_learning_curve.test_validation_curve()
sklearn.tests.test_learning_curve.test_validation_curve_clone_estimator()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/tests/test_random_projection.py----------------------------------------
A:sklearn.tests.test_random_projection.all_random_matrix->set(all_sparse_random_matrix + all_dense_random_matrix)
A:sklearn.tests.test_random_projection.all_RandomProjection->set(all_SparseRandomProjection + all_DenseRandomProjection)
A:sklearn.tests.test_random_projection.rng->numpy.random.RandomState(0)
A:sklearn.tests.test_random_projection.data_coo->scipy.sparse.coo_matrix((rng.randn(n_nonzeros), (rng.randint(n_samples, size=n_nonzeros), rng.randint(n_features, size=n_nonzeros))), shape=(n_samples, n_features))
A:sklearn.tests.test_random_projection.n_nonzeros->int(n_samples * n_features / 100.0)
A:sklearn.tests.test_random_projection.(data, data_csr)->make_sparse_random_data(n_samples, n_features, n_nonzeros)
A:sklearn.tests.test_random_projection.A->densify(A)
A:sklearn.tests.test_random_projection.values->numpy.unique(A)
A:sklearn.tests.test_random_projection.(data, _)->make_sparse_random_data(5, n_features, int(n_features / 4))
A:sklearn.tests.test_random_projection.rp->RandomProjection(n_components=100, density=0.001, random_state=0)
A:sklearn.tests.test_random_projection.original_distances->original_distances.ravel().ravel()
A:sklearn.tests.test_random_projection.projected->RandomProjection(n_components=100, density=0.001, random_state=0).fit_transform(data)
A:sklearn.tests.test_random_projection.projected_distances->projected_distances.ravel().ravel()
A:sklearn.tests.test_random_projection.sparse_data->scipy.sparse.csr_matrix(data)
A:sklearn.tests.test_random_projection.projected_1->RandomProjection(n_components=100, density=0.001, random_state=0).transform(data)
A:sklearn.tests.test_random_projection.projected_2->RandomProjection(n_components=100, density=0.001, random_state=0).transform(data)
A:sklearn.tests.test_random_projection.rp2->RandomProjection(random_state=0, eps=0.5)
A:sklearn.tests.test_random_projection.projected_3->RandomProjection(random_state=0, eps=0.5).fit_transform(data)
A:sklearn.tests.test_random_projection.rp_dense->RandomProjection(n_components=3, random_state=1).fit(data)
A:sklearn.tests.test_random_projection.rp_sparse->RandomProjection(n_components=3, random_state=1).fit(sp.csr_matrix(data))
sklearn.tests.test_random_projection.check_input_size_random_matrix(random_matrix)
sklearn.tests.test_random_projection.check_input_with_sparse_random_matrix(random_matrix)
sklearn.tests.test_random_projection.check_size_generated(random_matrix)
sklearn.tests.test_random_projection.check_zero_mean_and_unit_norm(random_matrix)
sklearn.tests.test_random_projection.densify(matrix)
sklearn.tests.test_random_projection.make_sparse_random_data(n_samples,n_features,n_nonzeros)
sklearn.tests.test_random_projection.test_SparseRandomProjection_output_representation()
sklearn.tests.test_random_projection.test_basic_property_of_random_matrix()
sklearn.tests.test_random_projection.test_correct_RandomProjection_dimensions_embedding()
sklearn.tests.test_random_projection.test_gaussian_random_matrix()
sklearn.tests.test_random_projection.test_input_size_jl_min_dim()
sklearn.tests.test_random_projection.test_invalid_jl_domain()
sklearn.tests.test_random_projection.test_random_projection_embedding_quality()
sklearn.tests.test_random_projection.test_random_projection_transformer_invalid_input()
sklearn.tests.test_random_projection.test_sparse_random_matrix()
sklearn.tests.test_random_projection.test_sparse_random_projection_transformer_invalid_density()
sklearn.tests.test_random_projection.test_too_many_samples_to_find_a_safe_embedding()
sklearn.tests.test_random_projection.test_try_to_transform_before_fit()
sklearn.tests.test_random_projection.test_warning_n_components_greater_than_n_features()
sklearn.tests.test_random_projection.test_works_with_sparse_data()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/tests/test_multioutput.py----------------------------------------
A:sklearn.tests.test_multioutput.(X, y)->make_classification(n_samples=1000, n_features=100, n_classes=16, n_informative=10, random_state=0)
A:sklearn.tests.test_multioutput.references->numpy.zeros_like(y_test)
A:sklearn.tests.test_multioutput.rgr->MultiOutputRegressor(GradientBoostingRegressor(random_state=0))
A:sklearn.tests.test_multioutput.references[:, n]->MultiOutputRegressor(SGDRegressor(random_state=0, max_iter=5)).predict(X_test)
A:sklearn.tests.test_multioutput.y_pred->MultiOutputRegressor(SGDRegressor(random_state=0, max_iter=5)).predict(X_test)
A:sklearn.tests.test_multioutput.sgr->MultiOutputRegressor(SGDRegressor(random_state=0, max_iter=5))
A:sklearn.tests.test_multioutput.rgr_sparse->MultiOutputRegressor(Lasso(random_state=0))
A:sklearn.tests.test_multioutput.rgr_w->MultiOutputRegressor(GradientBoostingRegressor(random_state=0))
A:sklearn.tests.test_multioutput.iris->sklearn.datasets.load_iris()
A:sklearn.tests.test_multioutput.y2->numpy.array(['d', 'e', 'f', 'e', 'd']).reshape(5, 1)
A:sklearn.tests.test_multioutput.y3->shuffle(y1, random_state=2)
A:sklearn.tests.test_multioutput.y->numpy.column_stack((y1, y2, y3))
A:sklearn.tests.test_multioutput.n_classes->len(np.unique(y1))
A:sklearn.tests.test_multioutput.classes->list(map(np.unique, (y1, y2, y3)))
A:sklearn.tests.test_multioutput.sgd_linear_clf->SGDClassifier(random_state=1, max_iter=5)
A:sklearn.tests.test_multioutput.mor->MultiOutputClassifier(sgd_linear_clf, n_jobs=-1)
A:sklearn.tests.test_multioutput.multi_target_linear->MultiOutputClassifier(sgd_linear_clf)
A:sklearn.tests.test_multioutput.first_predictions->MultiOutputClassifier(sgd_linear_clf).predict(X)
A:sklearn.tests.test_multioutput.second_predictions->MultiOutputClassifier(sgd_linear_clf).predict(X)
A:sklearn.tests.test_multioutput.forest->RandomForestClassifier(n_estimators=10, random_state=1)
A:sklearn.tests.test_multioutput.multi_target_forest->MultiOutputClassifier(forest)
A:sklearn.tests.test_multioutput.predictions->MultiOutputClassifier(multi_class_svc).predict(X)
A:sklearn.tests.test_multioutput.predict_proba->MultiOutputClassifier(forest).predict_proba(X)
A:sklearn.tests.test_multioutput.forest_->clone(forest)
A:sklearn.tests.test_multioutput.svc->LinearSVC(random_state=0)
A:sklearn.tests.test_multioutput.multi_class_svc->OneVsRestClassifier(svc)
A:sklearn.tests.test_multioutput.multi_target_svc->MultiOutputClassifier(multi_class_svc)
A:sklearn.tests.test_multioutput.multi_class_svc_->clone(multi_class_svc)
A:sklearn.tests.test_multioutput.rng->numpy.random.RandomState(seed)
A:sklearn.tests.test_multioutput.X->numpy.random.RandomState(seed).normal(size=(5, 5))
A:sklearn.tests.test_multioutput.y1->numpy.array(['b', 'a', 'a', 'b', 'a']).reshape(5, 1)
A:sklearn.tests.test_multioutput.Y->numpy.concatenate([y1, y2], axis=1)
A:sklearn.tests.test_multioutput.clf->MultiOutputClassifier(sgd_linear_clf)
A:sklearn.tests.test_multioutput.y_result->MultiOutputClassifier(sgd_linear_clf).predict_proba(X)
A:sklearn.tests.test_multioutput.w->numpy.asarray([2.0, 1.0, 1.0])
A:sklearn.tests.test_multioutput.clf_w->MultiOutputClassifier(sgd_linear_clf)
A:sklearn.tests.test_multioutput.moc->MultiOutputClassifier(LinearSVC(random_state=0))
A:sklearn.tests.test_multioutput.y_new->numpy.column_stack((y1, y2))
A:sklearn.tests.test_multioutput.Y_multi->numpy.array([[int(yyy) for yyy in format(yy, '#06b')[2:]] for yy in y])
A:sklearn.tests.test_multioutput.(X, Y)->generate_multilabel_dataset_with_correlations()
A:sklearn.tests.test_multioutput.classifier_chain->ClassifierChain(LogisticRegression())
A:sklearn.tests.test_multioutput.Y_pred->ClassifierChain(LogisticRegression()).predict(X)
A:sklearn.tests.test_multioutput.Y_prob->ClassifierChain(LogisticRegression()).predict_proba(X)
A:sklearn.tests.test_multioutput.Y_decision->ClassifierChain(LogisticRegression()).decision_function(X)
A:sklearn.tests.test_multioutput.X_sparse->scipy.sparse.csr_matrix(X)
A:sklearn.tests.test_multioutput.Y_pred_sparse->ClassifierChain(LogisticRegression()).predict(X_sparse)
A:sklearn.tests.test_multioutput.Y_pred_dense->ClassifierChain(LogisticRegression()).predict(X)
A:sklearn.tests.test_multioutput.classifier_chain_random->ClassifierChain(LogisticRegression(), order='random', random_state=42)
A:sklearn.tests.test_multioutput.Y_pred_random->ClassifierChain(LogisticRegression(), order='random', random_state=42).predict(X)
A:sklearn.tests.test_multioutput.classifier_chain_fixed->ClassifierChain(LogisticRegression(), order=classifier_chain_random.order_)
A:sklearn.tests.test_multioutput.Y_pred_fixed->ClassifierChain(LogisticRegression(), order=classifier_chain_random.order_).predict(X)
A:sklearn.tests.test_multioutput.classifier_chain_cv->ClassifierChain(LogisticRegression(), cv=3)
A:sklearn.tests.test_multioutput.Y_pred_cv->ClassifierChain(LogisticRegression(), cv=3).predict(X)
A:sklearn.tests.test_multioutput.ovr->OneVsRestClassifier(LogisticRegression())
A:sklearn.tests.test_multioutput.Y_pred_ovr->OneVsRestClassifier(LogisticRegression()).predict(X_test)
A:sklearn.tests.test_multioutput.chain->ClassifierChain(LogisticRegression())
A:sklearn.tests.test_multioutput.Y_pred_chain->ClassifierChain(LogisticRegression()).predict(X_test)
sklearn.tests.test_multioutput.generate_multilabel_dataset_with_correlations()
sklearn.tests.test_multioutput.test_classifier_chain_crossval_fit_and_predict()
sklearn.tests.test_multioutput.test_classifier_chain_fit_and_predict_with_linear_svc()
sklearn.tests.test_multioutput.test_classifier_chain_fit_and_predict_with_logistic_regression()
sklearn.tests.test_multioutput.test_classifier_chain_fit_and_predict_with_sparse_data()
sklearn.tests.test_multioutput.test_classifier_chain_fit_and_predict_with_sparse_data_and_cv()
sklearn.tests.test_multioutput.test_classifier_chain_random_order()
sklearn.tests.test_multioutput.test_classifier_chain_vs_independent_models()
sklearn.tests.test_multioutput.test_multi_output_classification()
sklearn.tests.test_multioutput.test_multi_output_classification_partial_fit()
sklearn.tests.test_multioutput.test_multi_output_classification_partial_fit_parallelism()
sklearn.tests.test_multioutput.test_multi_output_classification_partial_fit_sample_weights()
sklearn.tests.test_multioutput.test_multi_output_classification_sample_weights()
sklearn.tests.test_multioutput.test_multi_output_exceptions()
sklearn.tests.test_multioutput.test_multi_target_regression()
sklearn.tests.test_multioutput.test_multi_target_regression_one_target()
sklearn.tests.test_multioutput.test_multi_target_regression_partial_fit()
sklearn.tests.test_multioutput.test_multi_target_sample_weight_partial_fit()
sklearn.tests.test_multioutput.test_multi_target_sample_weights()
sklearn.tests.test_multioutput.test_multi_target_sample_weights_api()
sklearn.tests.test_multioutput.test_multi_target_sparse_regression()
sklearn.tests.test_multioutput.test_multiclass_multioutput_estimator()
sklearn.tests.test_multioutput.test_multiclass_multioutput_estimator_predict_proba()
sklearn.tests.test_multioutput.test_mutli_output_classifiation_partial_fit_no_first_classes_exception()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/tests/test_dummy.py----------------------------------------
A:sklearn.tests.test_dummy.proba->DummyRegressor().predict_proba(X)
A:sklearn.tests.test_dummy.log_proba->DummyRegressor().predict_log_proba(X)
A:sklearn.tests.test_dummy.y->numpy.random.RandomState(seed=1).rand(n_samples)
A:sklearn.tests.test_dummy.n_samples->len(X)
A:sklearn.tests.test_dummy.X->numpy.random.RandomState(seed=1).randn(10, 10)
A:sklearn.tests.test_dummy.est->DummyRegressor(strategy='quantile', quantile=0.95).fit(X, y, sample_weight)
A:sklearn.tests.test_dummy.y_pred->DummyRegressor().predict(X)
A:sklearn.tests.test_dummy.clf->DummyRegressor()
A:sklearn.tests.test_dummy.random_state->numpy.random.RandomState(seed=1)
A:sklearn.tests.test_dummy.reg->DummyRegressor(strategy='constant', constant=43)
A:sklearn.tests.test_dummy.X_learn->numpy.random.RandomState(seed=1).randn(10, 10)
A:sklearn.tests.test_dummy.y_learn->numpy.random.RandomState(seed=1).randn(10, 5)
A:sklearn.tests.test_dummy.mean->numpy.mean(y_learn, axis=0).reshape((1, -1))
A:sklearn.tests.test_dummy.X_test->numpy.random.RandomState(seed=1).randn(20, 10)
A:sklearn.tests.test_dummy.y_test->numpy.random.RandomState(seed=1).randn(20, 5)
A:sklearn.tests.test_dummy.y_pred_learn->DummyRegressor(strategy='quantile', quantile=0.95).fit(X, y, sample_weight).predict(X_learn)
A:sklearn.tests.test_dummy.y_pred_test->DummyRegressor(strategy='quantile', quantile=0.95).fit(X, y, sample_weight).predict(X_test)
A:sklearn.tests.test_dummy.median->numpy.median(y_learn, axis=0).reshape((1, -1))
A:sklearn.tests.test_dummy.quantile_values->numpy.percentile(y_learn, axis=0, q=80).reshape((1, -1))
A:sklearn.tests.test_dummy.constants->numpy.random.RandomState(seed=1).randn(5)
A:sklearn.tests.test_dummy.y_expected->numpy.hstack([np.ones((n_samples, 1)), np.zeros((n_samples, 1))])
A:sklearn.tests.test_dummy.sample_weight->numpy.random.RandomState(seed=1).rand(n_samples)
sklearn.tests.test_dummy._check_behavior_2d(clf)
sklearn.tests.test_dummy._check_behavior_2d_for_constant(clf)
sklearn.tests.test_dummy._check_equality_regressor(statistic,y_learn,y_pred_learn,y_test,y_pred_test)
sklearn.tests.test_dummy._check_predict_proba(clf,X,y)
sklearn.tests.test_dummy.test_classification_sample_weight()
sklearn.tests.test_dummy.test_classifier_exceptions()
sklearn.tests.test_dummy.test_constant_size_multioutput_regressor()
sklearn.tests.test_dummy.test_constant_strategy()
sklearn.tests.test_dummy.test_constant_strategy_exceptions()
sklearn.tests.test_dummy.test_constant_strategy_multioutput()
sklearn.tests.test_dummy.test_constant_strategy_multioutput_regressor()
sklearn.tests.test_dummy.test_constant_strategy_regressor()
sklearn.tests.test_dummy.test_constant_strategy_sparse_target()
sklearn.tests.test_dummy.test_constants_not_specified_regressor()
sklearn.tests.test_dummy.test_dummy_classifier_on_nan_value()
sklearn.tests.test_dummy.test_dummy_regressor_on_nan_value()
sklearn.tests.test_dummy.test_dummy_regressor_sample_weight(n_samples=10)
sklearn.tests.test_dummy.test_mean_strategy_multioutput_regressor()
sklearn.tests.test_dummy.test_mean_strategy_regressor()
sklearn.tests.test_dummy.test_median_strategy_multioutput_regressor()
sklearn.tests.test_dummy.test_median_strategy_regressor()
sklearn.tests.test_dummy.test_most_frequent_and_prior_strategy()
sklearn.tests.test_dummy.test_most_frequent_and_prior_strategy_multioutput()
sklearn.tests.test_dummy.test_most_frequent_and_prior_strategy_sparse_target()
sklearn.tests.test_dummy.test_quantile_invalid()
sklearn.tests.test_dummy.test_quantile_strategy_empty_train()
sklearn.tests.test_dummy.test_quantile_strategy_multioutput_regressor()
sklearn.tests.test_dummy.test_quantile_strategy_regressor()
sklearn.tests.test_dummy.test_regressor_exceptions()
sklearn.tests.test_dummy.test_stratified_strategy()
sklearn.tests.test_dummy.test_stratified_strategy_multioutput()
sklearn.tests.test_dummy.test_stratified_strategy_sparse_target()
sklearn.tests.test_dummy.test_string_labels()
sklearn.tests.test_dummy.test_uniform_strategy()
sklearn.tests.test_dummy.test_uniform_strategy_multioutput()
sklearn.tests.test_dummy.test_uniform_strategy_sparse_target_warning()
sklearn.tests.test_dummy.test_unknown_strategey_regressor()
sklearn.tests.test_dummy.test_y_mean_attribute_regressor()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/tests/test_cross_validation.py----------------------------------------
A:sklearn.tests.test_cross_validation.X->numpy.array([[-3, 4], [2, 4], [3, 3], [0, 2], [-3, 1], [-2, 1], [0, 0], [-2, -1], [-1, -2], [1, -2]])
A:sklearn.tests.test_cross_validation.T->T.reshape(len(T), -1).reshape(len(T), -1)
A:sklearn.tests.test_cross_validation.X_sparse->csr_matrix(X)
A:sklearn.tests.test_cross_validation.W_sparse->coo_matrix((np.array([1]), (np.array([1]), np.array([0]))), shape=(10, 1))
A:sklearn.tests.test_cross_validation.P_sparse->coo_matrix(np.eye(5))
A:sklearn.tests.test_cross_validation.expected_n_iter->len(cv)
A:sklearn.tests.test_cross_validation.collected_test_samples->set()
A:sklearn.tests.test_cross_validation.cv->sklearn.cross_validation.LeaveOneOut(len(y))
A:sklearn.tests.test_cross_validation.kf->sklearn.cross_validation.KFold(4, 2)
A:sklearn.tests.test_cross_validation.splits->sklearn.cross_validation.StratifiedShuffleSplit(labels, n_iter=1, test_size=0.5, random_state=0)
A:sklearn.tests.test_cross_validation.(train, test)->sklearn.cross_validation.train_test_split(y, test_size=test_size, stratify=y, random_state=0)
A:sklearn.tests.test_cross_validation.labels->numpy.array([1, 2, 3, 4])
A:sklearn.tests.test_cross_validation.ind->numpy.arange(300)
A:sklearn.tests.test_cross_validation.all_folds->numpy.concatenate((all_folds, ind[test]))
A:sklearn.tests.test_cross_validation.kf0->list(cval.StratifiedKFold(labels, 5, shuffle=True, random_state=0))
A:sklearn.tests.test_cross_validation.kf1->list(cval.StratifiedKFold(labels, 5, shuffle=True, random_state=1))
A:sklearn.tests.test_cross_validation.digits->load_digits()
A:sklearn.tests.test_cross_validation.model->SVC(C=10, gamma=0.005)
A:sklearn.tests.test_cross_validation.n->len(y)
A:sklearn.tests.test_cross_validation.mean_score->sklearn.cross_validation.cross_val_score(model, X, y, cv=cv).mean()
A:sklearn.tests.test_cross_validation.rng->numpy.random.RandomState(0)
A:sklearn.tests.test_cross_validation.n_labels->len(np.unique(labels))
A:sklearn.tests.test_cross_validation.n_samples->len(labels)
A:sklearn.tests.test_cross_validation.ss1->sklearn.cross_validation.ShuffleSplit(10, test_size=0.2, random_state=0)
A:sklearn.tests.test_cross_validation.ss2->sklearn.cross_validation.ShuffleSplit(10, test_size=2, random_state=0)
A:sklearn.tests.test_cross_validation.ss3->sklearn.cross_validation.ShuffleSplit(10, test_size=np.int32(2), random_state=0)
A:sklearn.tests.test_cross_validation.ss4->sklearn.cross_validation.ShuffleSplit(10, test_size=typ(2), random_state=0)
A:sklearn.tests.test_cross_validation.y->numpy.array([[1, 1], [0, 1], [0, 1], [0, 1], [1, 1], [0, 1], [1, 0], [1, 1], [1, 0], [0, 0]])
A:sklearn.tests.test_cross_validation.sss->sklearn.cross_validation.StratifiedShuffleSplit(y, 6, test_size=0.33, random_state=0)
A:sklearn.tests.test_cross_validation.test_size->numpy.ceil(0.33 * len(y))
A:sklearn.tests.test_cross_validation.bf->scipy.stats.binom(n_splits, p)
A:sklearn.tests.test_cross_validation.p->numpy.arange(100)
A:sklearn.tests.test_cross_validation.label_counts->numpy.unique(labels)
A:sklearn.tests.test_cross_validation.ps->sklearn.cross_validation.PredefinedSplit([1, 1, 2, 2])
A:sklearn.tests.test_cross_validation.slo->sklearn.cross_validation.LabelShuffleSplit(y, n_iter, test_size=test_size, random_state=0)
A:sklearn.tests.test_cross_validation.y_unique->numpy.unique(y)
A:sklearn.tests.test_cross_validation.y_train_unique->numpy.unique(y[train])
A:sklearn.tests.test_cross_validation.y_test_unique->numpy.unique(y[test])
A:sklearn.tests.test_cross_validation.labels_changing->numpy.array(labels, copy=True)
A:sklearn.tests.test_cross_validation.lolo->sklearn.cross_validation.LeaveOneLabelOut(labels)
A:sklearn.tests.test_cross_validation.lolo_changing->sklearn.cross_validation.LeaveOneLabelOut(labels_changing)
A:sklearn.tests.test_cross_validation.lplo->sklearn.cross_validation.LeavePLabelOut(labels, p=2)
A:sklearn.tests.test_cross_validation.lplo_changing->sklearn.cross_validation.LeavePLabelOut(labels_changing, p=2)
A:sklearn.tests.test_cross_validation.clf->MockClassifier()
A:sklearn.tests.test_cross_validation.scores->sklearn.cross_validation.cross_val_score(reg, X, y, cv=5)
A:sklearn.tests.test_cross_validation.svm->SVC(kernel='linear')
A:sklearn.tests.test_cross_validation.iris->load_iris()
A:sklearn.tests.test_cross_validation.cv_indices->sklearn.cross_validation.KFold(len(y), 5)
A:sklearn.tests.test_cross_validation.scores_indices->sklearn.cross_validation.cross_val_score(svm, X, y, cv=cv_indices)
A:sklearn.tests.test_cross_validation.mask_train->numpy.zeros(len(y), dtype=np.bool)
A:sklearn.tests.test_cross_validation.mask_test->numpy.zeros(len(y), dtype=np.bool)
A:sklearn.tests.test_cross_validation.scores_masks->sklearn.cross_validation.cross_val_score(svm, X, y, cv=cv_masks)
A:sklearn.tests.test_cross_validation.linear_kernel->numpy.dot(X, X.T)
A:sklearn.tests.test_cross_validation.score_precomputed->sklearn.cross_validation.cross_val_score(svm, linear_kernel, y)
A:sklearn.tests.test_cross_validation.score_linear->sklearn.cross_validation.cross_val_score(svm, X, y)
A:sklearn.tests.test_cross_validation.n_classes->len(np.unique(y))
A:sklearn.tests.test_cross_validation.DUMMY_OBJ->object()
A:sklearn.tests.test_cross_validation.scoring->make_scorer(explained_variance_score)
A:sklearn.tests.test_cross_validation.score->sklearn.cross_validation.cross_val_score(clf, X, y, scoring=scoring)
A:sklearn.tests.test_cross_validation.X_s->coo_matrix(X)
A:sklearn.tests.test_cross_validation.split->sklearn.cross_validation.train_test_split(X_4d, y_3d)
A:sklearn.tests.test_cross_validation.X_4d->numpy.arange(10 * 5 * 3 * 2).reshape(10, 5, 3, 2)
A:sklearn.tests.test_cross_validation.y_3d->numpy.arange(10 * 7 * 11).reshape(10, 7, 11)
A:sklearn.tests.test_cross_validation.X_df->MockDataFrame(X)
A:sklearn.tests.test_cross_validation.(X_train, X_test)->sklearn.cross_validation.train_test_split(X_df)
A:sklearn.tests.test_cross_validation.zo_scores->sklearn.cross_validation.cross_val_score(clf, iris.data, iris.target, scoring='accuracy', cv=5)
A:sklearn.tests.test_cross_validation.f1_scores->sklearn.cross_validation.cross_val_score(clf, iris.data, iris.target, scoring='f1_weighted', cv=5)
A:sklearn.tests.test_cross_validation.(X, y)->make_multilabel_classification(n_classes=2, n_labels=1, allow_unlabeled=False, return_indicator=True, random_state=1)
A:sklearn.tests.test_cross_validation.reg->Ridge()
A:sklearn.tests.test_cross_validation.r2_scores->sklearn.cross_validation.cross_val_score(reg, X, y, scoring='r2', cv=5)
A:sklearn.tests.test_cross_validation.neg_mse_scores->sklearn.cross_validation.cross_val_score(reg, X, y, cv=5, scoring='neg_mean_squared_error')
A:sklearn.tests.test_cross_validation.expected_neg_mse->numpy.array([-763.07, -553.16, -274.38, -273.26, -1681.99])
A:sklearn.tests.test_cross_validation.ev_scores->sklearn.cross_validation.cross_val_score(reg, X, y, cv=5, scoring=scoring)
A:sklearn.tests.test_cross_validation.(score, scores, pvalue)->sklearn.cross_validation.permutation_test_score(svm, X, y, n_permutations=30, cv=cv, scoring='accuracy')
A:sklearn.tests.test_cross_validation.(score_label, _, pvalue_label)->sklearn.cross_validation.permutation_test_score(svm_sparse, X_sparse, y, n_permutations=30, cv=cv_sparse, scoring='accuracy', labels=np.ones(y.size), random_state=0)
A:sklearn.tests.test_cross_validation.svm_sparse->SVC(kernel='linear')
A:sklearn.tests.test_cross_validation.cv_sparse->sklearn.cross_validation.StratifiedKFold(y, 2)
A:sklearn.tests.test_cross_validation.scorer->make_scorer(custom_score)
A:sklearn.tests.test_cross_validation.(score, _, pvalue)->sklearn.cross_validation.permutation_test_score(svm, X, y, n_permutations=100, scoring=scorer, cv=cv, random_state=0)
A:sklearn.tests.test_cross_validation.loo->sklearn.cross_validation.LeaveOneOut(4)
A:sklearn.tests.test_cross_validation.lpo->sklearn.cross_validation.LeavePOut(4, 2)
A:sklearn.tests.test_cross_validation.skf->sklearn.cross_validation.StratifiedKFold(y, 2)
A:sklearn.tests.test_cross_validation.lopo->sklearn.cross_validation.LeavePLabelOut(labels, 2)
A:sklearn.tests.test_cross_validation.ss->sklearn.cross_validation.ShuffleSplit(10, random_state=21)
A:sklearn.tests.test_cross_validation.clfp->SVC(kernel='precomputed')
A:sklearn.tests.test_cross_validation.K->numpy.dot(X, X.T)
A:sklearn.tests.test_cross_validation.(X_tr, y_tr)->sklearn.cross_validation._safe_split(clf, X, y, tr)
A:sklearn.tests.test_cross_validation.(K_tr, y_tr2)->sklearn.cross_validation._safe_split(clfp, K, y, tr)
A:sklearn.tests.test_cross_validation.(X_te, y_te)->sklearn.cross_validation._safe_split(clf, X, y, te, tr)
A:sklearn.tests.test_cross_validation.(K_te, y_te2)->sklearn.cross_validation._safe_split(clfp, K, y, te, tr)
A:sklearn.tests.test_cross_validation.y_binary->numpy.array([0, 1, 0, 1, 0, 0, 1, 1, 1])
A:sklearn.tests.test_cross_validation.y_multiclass->numpy.array([0, 1, 0, 1, 2, 1, 2, 0, 2])
A:sklearn.tests.test_cross_validation.y_multioutput->numpy.array([[1, 2], [0, 3], [0, 0], [3, 1], [2, 0]])
A:sklearn.tests.test_cross_validation.scoring_micro->make_scorer(precision_score, average='micro')
A:sklearn.tests.test_cross_validation.scoring_macro->make_scorer(precision_score, average='macro')
A:sklearn.tests.test_cross_validation.scoring_samples->make_scorer(precision_score, average='samples')
A:sklearn.tests.test_cross_validation.score_micro->sklearn.cross_validation.cross_val_score(clf, X, y, scoring=scoring_micro, cv=5)
A:sklearn.tests.test_cross_validation.score_macro->sklearn.cross_validation.cross_val_score(clf, X, y, scoring=scoring_macro, cv=5)
A:sklearn.tests.test_cross_validation.score_samples->sklearn.cross_validation.cross_val_score(clf, X, y, scoring=scoring_samples, cv=5)
A:sklearn.tests.test_cross_validation.boston->load_boston()
A:sklearn.tests.test_cross_validation.est->Ridge()
A:sklearn.tests.test_cross_validation.preds2->numpy.zeros_like(y)
A:sklearn.tests.test_cross_validation.preds2[test]->Ridge().predict(X[test])
A:sklearn.tests.test_cross_validation.preds->sklearn.cross_validation.cross_val_predict(classif, X, y, cv=10)
A:sklearn.tests.test_cross_validation.Xsp->coo_matrix(Xsp)
A:sklearn.tests.test_cross_validation.predictions->sklearn.cross_validation.cross_val_predict(clf, X_3d, y)
A:sklearn.tests.test_cross_validation.a->sklearn.cross_validation.cross_val_score(clf, X, y, fit_params=fit_params)
A:sklearn.tests.test_cross_validation.y_sparse->csr_matrix(y)
A:sklearn.tests.test_cross_validation.classif->OneVsRestClassifier(SVC(kernel='linear'))
A:sklearn.tests.test_cross_validation.preds_sparse->preds_sparse.toarray().toarray()
sklearn.tests.test_cross_validation.MockClassifier(self,a=0,allow_nd=False)
sklearn.tests.test_cross_validation.MockClassifier.__init__(self,a=0,allow_nd=False)
sklearn.tests.test_cross_validation.MockClassifier.fit(self,X,Y=None,sample_weight=None,class_prior=None,sparse_sample_weight=None,sparse_param=None,dummy_int=None,dummy_str=None,dummy_obj=None,callback=None)
sklearn.tests.test_cross_validation.MockClassifier.get_params(self,deep=False)
sklearn.tests.test_cross_validation.MockClassifier.predict(self,T)
sklearn.tests.test_cross_validation.MockClassifier.score(self,X=None,Y=None)
sklearn.tests.test_cross_validation.check_cv_coverage(cv,expected_n_iter=None,n_samples=None)
sklearn.tests.test_cross_validation.check_valid_split(train,test,n_samples=None)
sklearn.tests.test_cross_validation.test_check_cv_return_types()
sklearn.tests.test_cross_validation.test_check_is_partition()
sklearn.tests.test_cross_validation.test_cross_val_generator_with_default_indices()
sklearn.tests.test_cross_validation.test_cross_val_generator_with_indices()
sklearn.tests.test_cross_validation.test_cross_val_predict()
sklearn.tests.test_cross_validation.test_cross_val_predict_input_types()
sklearn.tests.test_cross_validation.test_cross_val_predict_pandas()
sklearn.tests.test_cross_validation.test_cross_val_predict_sparse_prediction()
sklearn.tests.test_cross_validation.test_cross_val_score()
sklearn.tests.test_cross_validation.test_cross_val_score_allow_nans()
sklearn.tests.test_cross_validation.test_cross_val_score_errors()
sklearn.tests.test_cross_validation.test_cross_val_score_fit_params()
sklearn.tests.test_cross_validation.test_cross_val_score_mask()
sklearn.tests.test_cross_validation.test_cross_val_score_multilabel()
sklearn.tests.test_cross_validation.test_cross_val_score_pandas()
sklearn.tests.test_cross_validation.test_cross_val_score_precomputed()
sklearn.tests.test_cross_validation.test_cross_val_score_score_func()
sklearn.tests.test_cross_validation.test_cross_val_score_with_score_func_classification()
sklearn.tests.test_cross_validation.test_cross_val_score_with_score_func_regression()
sklearn.tests.test_cross_validation.test_kfold_balance()
sklearn.tests.test_cross_validation.test_kfold_can_detect_dependent_samples_on_digits()
sklearn.tests.test_cross_validation.test_kfold_indices()
sklearn.tests.test_cross_validation.test_kfold_no_shuffle()
sklearn.tests.test_cross_validation.test_kfold_valueerrors()
sklearn.tests.test_cross_validation.test_label_kfold()
sklearn.tests.test_cross_validation.test_label_shuffle_split()
sklearn.tests.test_cross_validation.test_leave_label_out_changing_labels()
sklearn.tests.test_cross_validation.test_permutation_score()
sklearn.tests.test_cross_validation.test_permutation_test_score_allow_nans()
sklearn.tests.test_cross_validation.test_predefinedsplit_with_kfold_split()
sklearn.tests.test_cross_validation.test_safe_split_with_precomputed_kernel()
sklearn.tests.test_cross_validation.test_shuffle_kfold()
sklearn.tests.test_cross_validation.test_shuffle_split()
sklearn.tests.test_cross_validation.test_shuffle_stratifiedkfold()
sklearn.tests.test_cross_validation.test_shufflesplit_errors()
sklearn.tests.test_cross_validation.test_shufflesplit_reproducible()
sklearn.tests.test_cross_validation.test_sparse_fit_params()
sklearn.tests.test_cross_validation.test_stratified_kfold_no_shuffle()
sklearn.tests.test_cross_validation.test_stratified_kfold_ratios()
sklearn.tests.test_cross_validation.test_stratified_shuffle_split_even()
sklearn.tests.test_cross_validation.test_stratified_shuffle_split_init()
sklearn.tests.test_cross_validation.test_stratified_shuffle_split_iter()
sklearn.tests.test_cross_validation.test_stratified_shuffle_split_overlap_train_test_bug()
sklearn.tests.test_cross_validation.test_stratifiedkfold_balance()
sklearn.tests.test_cross_validation.test_train_test_split()
sklearn.tests.test_cross_validation.test_train_test_split_allow_nans()
sklearn.tests.test_cross_validation.test_train_test_split_errors()
sklearn.tests.test_cross_validation.train_test_split_mock_pandas()
sklearn.tests.test_cross_validation.train_test_split_pandas()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/tests/test_isotonic.py----------------------------------------
A:sklearn.tests.test_isotonic.ir->IsotonicRegression()
A:sklearn.tests.test_isotonic.(x_s, y_s, sample_weight_s)->shuffle(x, y, sample_weight, random_state=0)
A:sklearn.tests.test_isotonic.y_transformed->IsotonicRegression().fit_transform(x, y, sample_weight=sample_weight)
A:sklearn.tests.test_isotonic.y_transformed_s->IsotonicRegression().fit(x_s, y_s, sample_weight=sample_weight_s).transform(x)
A:sklearn.tests.test_isotonic.is_increasing->assert_warns_message(UserWarning, 'interval', check_increasing, x, y)
A:sklearn.tests.test_isotonic.y->isotonic_regression(x, y_min=0.0, increasing=False)
A:sklearn.tests.test_isotonic.y_->IsotonicRegression().fit_transform(x, y)
A:sklearn.tests.test_isotonic.x->numpy.linspace(-3, 3, n_samples)
A:sklearn.tests.test_isotonic.perm->numpy.random.permutation(len(y))
A:sklearn.tests.test_isotonic.rng->numpy.random.RandomState(123)
A:sklearn.tests.test_isotonic.weights->numpy.random.RandomState(123).rand(n_samples)
A:sklearn.tests.test_isotonic.y_set_value->IsotonicRegression().fit_transform(x, y, sample_weight=weights)
A:sklearn.tests.test_isotonic.y_default_value->IsotonicRegression().fit_transform(x, y)
A:sklearn.tests.test_isotonic.y_result->numpy.round(ir.fit_transform(x, y))
A:sklearn.tests.test_isotonic.received_y->IsotonicRegression().fit_transform(x, y, sample_weight=sample_weight)
A:sklearn.tests.test_isotonic.y1->IsotonicRegression().predict([min(x) - 10, max(x) + 10])
A:sklearn.tests.test_isotonic.y2->IsotonicRegression().predict(x)
A:sklearn.tests.test_isotonic.ir_ser->pickle.dumps(ir, pickle.HIGHEST_PROTOCOL)
A:sklearn.tests.test_isotonic.ir2->pickle.loads(ir_ser)
A:sklearn.tests.test_isotonic.all_predictions_finite->numpy.all(np.isfinite(ir.predict(x)))
A:sklearn.tests.test_isotonic.regression->IsotonicRegression()
A:sklearn.tests.test_isotonic.w->numpy.random.RandomState(123).uniform(size=n_samples)
A:sklearn.tests.test_isotonic.y_train->numpy.less(rng.rand(n_samples), 1.0 / (1.0 + np.exp(-X_train))).astype('int64')
A:sklearn.tests.test_isotonic.slow_model->IsotonicRegression(y_min=0, y_max=1, out_of_bounds='clip')
A:sklearn.tests.test_isotonic.fast_model->IsotonicRegression(y_min=0, y_max=1, out_of_bounds='clip')
A:sklearn.tests.test_isotonic.(X_train_fit, y_train_fit)->IsotonicRegression(y_min=0, y_max=1, out_of_bounds='clip')._build_y(X_train, y_train, sample_weight=weights, trim_duplicates=False)
A:sklearn.tests.test_isotonic.y_pred_slow->IsotonicRegression(y_min=0, y_max=1, out_of_bounds='clip').predict(X_test)
A:sklearn.tests.test_isotonic.y_pred_fast->IsotonicRegression(y_min=0, y_max=1, out_of_bounds='clip').predict(X_test)
sklearn.tests.test_isotonic.test_assert_raises_exceptions()
sklearn.tests.test_isotonic.test_check_ci_warn()
sklearn.tests.test_isotonic.test_check_increasing_down()
sklearn.tests.test_isotonic.test_check_increasing_down_extreme()
sklearn.tests.test_isotonic.test_check_increasing_small_number_of_samples()
sklearn.tests.test_isotonic.test_check_increasing_up()
sklearn.tests.test_isotonic.test_check_increasing_up_extreme()
sklearn.tests.test_isotonic.test_fast_predict()
sklearn.tests.test_isotonic.test_isotonic_copy_before_fit()
sklearn.tests.test_isotonic.test_isotonic_duplicate_min_entry()
sklearn.tests.test_isotonic.test_isotonic_min_max_boundaries()
sklearn.tests.test_isotonic.test_isotonic_regression()
sklearn.tests.test_isotonic.test_isotonic_regression_auto_decreasing()
sklearn.tests.test_isotonic.test_isotonic_regression_auto_increasing()
sklearn.tests.test_isotonic.test_isotonic_regression_oob_bad()
sklearn.tests.test_isotonic.test_isotonic_regression_oob_bad_after()
sklearn.tests.test_isotonic.test_isotonic_regression_oob_clip()
sklearn.tests.test_isotonic.test_isotonic_regression_oob_nan()
sklearn.tests.test_isotonic.test_isotonic_regression_oob_raise()
sklearn.tests.test_isotonic.test_isotonic_regression_pickle()
sklearn.tests.test_isotonic.test_isotonic_regression_reversed()
sklearn.tests.test_isotonic.test_isotonic_regression_ties_max()
sklearn.tests.test_isotonic.test_isotonic_regression_ties_min()
sklearn.tests.test_isotonic.test_isotonic_regression_ties_secondary_()
sklearn.tests.test_isotonic.test_isotonic_sample_weight()
sklearn.tests.test_isotonic.test_isotonic_sample_weight_parameter_default_value()
sklearn.tests.test_isotonic.test_isotonic_ymin_ymax()
sklearn.tests.test_isotonic.test_isotonic_zero_weight_loop()
sklearn.tests.test_isotonic.test_permutation_invariance()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/tests/test_check_build.py----------------------------------------
sklearn.tests.test_check_build.test_raise_build_error()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/tests/test_config.py----------------------------------------
sklearn.tests.test_config.test_config_context()
sklearn.tests.test_config.test_config_context_exception()
sklearn.tests.test_config.test_set_config()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/tests/test_multiclass.py----------------------------------------
A:sklearn.tests.test_multiclass.iris->sklearn.datasets.load_iris()
A:sklearn.tests.test_multiclass.rng->numpy.random.RandomState(0)
A:sklearn.tests.test_multiclass.perm->numpy.random.RandomState(0).permutation(iris.target.size)
A:sklearn.tests.test_multiclass.ovr->OneVsOneClassifier(SVC())
A:sklearn.tests.test_multiclass.y->numpy.array(['a'] * 4)
A:sklearn.tests.test_multiclass.msg->type_of_target(y)
A:sklearn.tests.test_multiclass.pred->OneVsOneClassifier(LinearSVC(random_state=0)).estimators_[k].predict(iris.data)
A:sklearn.tests.test_multiclass.clf->Pipeline([('tree', DecisionTreeClassifier())])
A:sklearn.tests.test_multiclass.pred2->OneVsOneClassifier(MultinomialNB()).fit(X, y).predict(X)
A:sklearn.tests.test_multiclass.(X, y)->shuffle(iris.data, iris.target, random_state=0)
A:sklearn.tests.test_multiclass.ovr2->OneVsRestClassifier(MultinomialNB())
A:sklearn.tests.test_multiclass.X->numpy.eye(4)
A:sklearn.tests.test_multiclass.ovr1->OneVsRestClassifier(SGDClassifier(max_iter=1, tol=None, shuffle=False, random_state=0))
A:sklearn.tests.test_multiclass.pred1->OneVsOneClassifier(MultinomialNB()).predict(X)
A:sklearn.tests.test_multiclass.base_clf->MultinomialNB(alpha=1)
A:sklearn.tests.test_multiclass.(X, Y)->sklearn.datasets.make_classification(n_samples=100, n_features=20, random_state=0)
A:sklearn.tests.test_multiclass.Y_pred->Pipeline([('tree', DecisionTreeClassifier())]).predict(X_test)
A:sklearn.tests.test_multiclass.clf_sprs->OneVsRestClassifier(svm.SVC()).fit(X_train, sparse(Y_train))
A:sklearn.tests.test_multiclass.Y_pred_sprs->OneVsRestClassifier(svm.SVC()).fit(X_train, sparse(Y_train)).predict(X_test)
A:sklearn.tests.test_multiclass.Y_proba->Pipeline([('tree', DecisionTreeClassifier())]).predict_proba(X_test)
A:sklearn.tests.test_multiclass.dec_pred->(clf_sprs.decision_function(X_test) > 0).astype(int)
A:sklearn.tests.test_multiclass.y_pred->OneVsOneClassifier(SVC()).predict_proba(X)
A:sklearn.tests.test_multiclass.Y->numpy.array([[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0]])
A:sklearn.tests.test_multiclass.classes->set('eggs spam'.split())
A:sklearn.tests.test_multiclass.dec->Pipeline([('tree', DecisionTreeClassifier())]).decision_function(X)
A:sklearn.tests.test_multiclass.X_test->numpy.array([[0, 0, 4]])
A:sklearn.tests.test_multiclass.probabilities->Pipeline([('tree', DecisionTreeClassifier())]).predict_proba(X_test)
A:sklearn.tests.test_multiclass.decision_only->OneVsRestClassifier(svm.SVR()).fit(X_train, Y_train)
A:sklearn.tests.test_multiclass.gs->GridSearchCV(svm.SVC(probability=False), param_grid={'probability': [True]})
A:sklearn.tests.test_multiclass.proba_after_fit->OneVsRestClassifier(gs)
A:sklearn.tests.test_multiclass.cv->GridSearchCV(ecoc, {'estimator__C': Cs})
A:sklearn.tests.test_multiclass.ovr_pipe->OneVsRestClassifier(clf)
A:sklearn.tests.test_multiclass.ovo->OutputCodeClassifier(LinearSVC(), code_size=-1)
A:sklearn.tests.test_multiclass.prediction_from_array->OutputCodeClassifier(LinearSVC(), code_size=-1).fit(iris.data, iris.target).predict(iris.data)
A:sklearn.tests.test_multiclass.prediction_from_list->OutputCodeClassifier(LinearSVC(), code_size=-1).fit(iris_data_list, list(iris.target)).predict(iris_data_list)
A:sklearn.tests.test_multiclass.temp->sklearn.datasets.load_iris()
A:sklearn.tests.test_multiclass.ovo1->OneVsOneClassifier(MultinomialNB())
A:sklearn.tests.test_multiclass.ovo2->OneVsOneClassifier(MultinomialNB())
A:sklearn.tests.test_multiclass.message_re->escape('Mini-batch contains {0} while it must be subset of {1}'.format(np.unique(error_y), np.unique(y)))
A:sklearn.tests.test_multiclass.ovo_clf->OneVsOneClassifier(LinearSVC(random_state=0))
A:sklearn.tests.test_multiclass.decisions->OneVsOneClassifier(LinearSVC(random_state=0)).decision_function(iris.data)
A:sklearn.tests.test_multiclass.votes->numpy.round(ovo_decision)
A:sklearn.tests.test_multiclass.multi_clf->OneVsOneClassifier(Perceptron(shuffle=False, max_iter=4, tol=None))
A:sklearn.tests.test_multiclass.ovo_prediction->OneVsOneClassifier(Perceptron(shuffle=False, max_iter=4, tol=None)).fit(X, y).predict(X)
A:sklearn.tests.test_multiclass.ovo_decision->OneVsOneClassifier(Perceptron(shuffle=False, max_iter=4, tol=None)).decision_function(X)
A:sklearn.tests.test_multiclass.y_ref->numpy.array([2, 0, 1, 2])
A:sklearn.tests.test_multiclass.ecoc->OutputCodeClassifier(LinearSVC(random_state=0), random_state=0)
A:sklearn.tests.test_multiclass.clf_precomputed->sklearn.svm.SVC(kernel='precomputed')
A:sklearn.tests.test_multiclass.ovr_false->MultiClassClassifier(clf_notprecomputed)
A:sklearn.tests.test_multiclass.linear_kernel->numpy.dot(X, X.T)
A:sklearn.tests.test_multiclass.n_estimators->len(ovr_false.estimators_)
A:sklearn.tests.test_multiclass.clf_notprecomputed->sklearn.svm.SVC(kernel='linear')
A:sklearn.tests.test_multiclass.ovr_true->MultiClassClassifier(clf_precomputed)
A:sklearn.tests.test_multiclass.score_precomputed->cross_val_score(ovr_true, linear_kernel, y)
A:sklearn.tests.test_multiclass.score_linear->cross_val_score(ovr_false, X, y)
sklearn.tests.test_multiclass.test_check_classification_targets()
sklearn.tests.test_multiclass.test_ecoc_exceptions()
sklearn.tests.test_multiclass.test_ecoc_fit_predict()
sklearn.tests.test_multiclass.test_ecoc_float_y()
sklearn.tests.test_multiclass.test_ecoc_gridsearch()
sklearn.tests.test_multiclass.test_ovo_decision_function()
sklearn.tests.test_multiclass.test_ovo_exceptions()
sklearn.tests.test_multiclass.test_ovo_fit_on_list()
sklearn.tests.test_multiclass.test_ovo_fit_predict()
sklearn.tests.test_multiclass.test_ovo_float_y()
sklearn.tests.test_multiclass.test_ovo_gridsearch()
sklearn.tests.test_multiclass.test_ovo_one_class()
sklearn.tests.test_multiclass.test_ovo_partial_fit_predict()
sklearn.tests.test_multiclass.test_ovo_string_y()
sklearn.tests.test_multiclass.test_ovo_ties()
sklearn.tests.test_multiclass.test_ovo_ties2()
sklearn.tests.test_multiclass.test_ovr_always_present()
sklearn.tests.test_multiclass.test_ovr_binary()
sklearn.tests.test_multiclass.test_ovr_coef_()
sklearn.tests.test_multiclass.test_ovr_coef_exceptions()
sklearn.tests.test_multiclass.test_ovr_exceptions()
sklearn.tests.test_multiclass.test_ovr_fit_predict()
sklearn.tests.test_multiclass.test_ovr_fit_predict_sparse()
sklearn.tests.test_multiclass.test_ovr_fit_predict_svc()
sklearn.tests.test_multiclass.test_ovr_gridsearch()
sklearn.tests.test_multiclass.test_ovr_multiclass()
sklearn.tests.test_multiclass.test_ovr_multilabel()
sklearn.tests.test_multiclass.test_ovr_multilabel_dataset()
sklearn.tests.test_multiclass.test_ovr_multilabel_decision_function()
sklearn.tests.test_multiclass.test_ovr_multilabel_predict_proba()
sklearn.tests.test_multiclass.test_ovr_ovo_regressor()
sklearn.tests.test_multiclass.test_ovr_partial_fit()
sklearn.tests.test_multiclass.test_ovr_partial_fit_exceptions()
sklearn.tests.test_multiclass.test_ovr_pipeline()
sklearn.tests.test_multiclass.test_ovr_single_label_decision_function()
sklearn.tests.test_multiclass.test_ovr_single_label_predict_proba()
sklearn.tests.test_multiclass.test_pairwise_attribute()
sklearn.tests.test_multiclass.test_pairwise_cross_val_score()
sklearn.tests.test_multiclass.test_pairwise_indices()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/tests/test_common.py----------------------------------------
A:sklearn.tests.test_common.msg->'Base estimators such as {0} should not be included in all_estimators'.format(name)
A:sklearn.tests.test_common.estimators->all_estimators()
A:sklearn.tests.test_common.estimator->Estimator()
A:sklearn.tests.test_common.cwd->os.getcwd()
A:sklearn.tests.test_common.setup_path->os.path.abspath(os.path.join(sklearn.__path__[0], '..'))
A:sklearn.tests.test_common.setup_filename->os.path.join(setup_path, 'setup.py')
A:sklearn.tests.test_common.classifiers->all_estimators(type_filter='classifier')
A:sklearn.tests.test_common.pkgs->pkgutil.walk_packages(path=sklearn.__path__, prefix='sklearn.', onerror=lambda _: None)
A:sklearn.tests.test_common.package->__import__(modname, fromlist='dummy')
A:sklearn.tests.test_common.HAS_TESTS_EXCEPTIONS->re.compile('(?x)\n                                      \\.externals(\\.|$)|\n                                      \\.tests(\\.|$)|\n                                      \\._\n                                      ')
A:sklearn.tests.test_common.lookup->dict(((name, ispkg) for (_, name, ispkg) in pkgutil.walk_packages(sklearn.__path__, prefix='sklearn.')))
sklearn.tests.test_common.test_all_estimator_no_base_class()
sklearn.tests.test_common.test_all_estimators()
sklearn.tests.test_common.test_all_tests_are_importable()
sklearn.tests.test_common.test_class_weight_balanced_linear_classifiers()
sklearn.tests.test_common.test_configure()
sklearn.tests.test_common.test_import_all_consistency()
sklearn.tests.test_common.test_non_meta_estimators()
sklearn.tests.test_common.test_root_import_all_completeness()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/tests/test_metaestimators.py----------------------------------------
A:sklearn.tests.test_metaestimators.self.coef_->numpy.arange(X.shape[1])
A:sklearn.tests.test_metaestimators.delegate->SubEstimator(hidden_method=method)
A:sklearn.tests.test_metaestimators.delegator->delegator_data.construct(delegate)
sklearn.tests.test_metaestimators.DelegatorData(self,name,construct,skip_methods=(),fit_args=make_classification())
sklearn.tests.test_metaestimators.DelegatorData.__init__(self,name,construct,skip_methods=(),fit_args=make_classification())
sklearn.tests.test_metaestimators.test_metaestimator_delegation()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/tests/test_init.py----------------------------------------
sklearn.tests.test_init.test_import_skl()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/tests/test_naive_bayes.py----------------------------------------
A:sklearn.tests.test_naive_bayes.X->numpy.array([[1, 0], [1, 1]])
A:sklearn.tests.test_naive_bayes.y->numpy.array([0, 1])
A:sklearn.tests.test_naive_bayes.rng->numpy.random.RandomState(0)
A:sklearn.tests.test_naive_bayes.X1->numpy.random.RandomState(0).normal(size=(10, 3))
A:sklearn.tests.test_naive_bayes.y1->(rng.normal(size=10) > 0).astype(np.int)
A:sklearn.tests.test_naive_bayes.X2->numpy.random.RandomState(0).randint(5, size=(6, 100))
A:sklearn.tests.test_naive_bayes.y2->numpy.array([1, 1, 2, 2, 3, 3])
A:sklearn.tests.test_naive_bayes.clf->BernoulliNB(alpha=1.0)
A:sklearn.tests.test_naive_bayes.y_pred->BernoulliNB(alpha=1.0).predict(X2)
A:sklearn.tests.test_naive_bayes.y_pred_proba->BernoulliNB(alpha=1.0).predict_proba(X)
A:sklearn.tests.test_naive_bayes.y_pred_log_proba->BernoulliNB(alpha=1.0).predict_log_proba(X)
A:sklearn.tests.test_naive_bayes.sw->numpy.random.RandomState(0).rand(y.shape[0])
A:sklearn.tests.test_naive_bayes.clf_sw->GaussianNB().fit(X, y, sample_weight)
A:sklearn.tests.test_naive_bayes.clf1->cls()
A:sklearn.tests.test_naive_bayes.clf2->pickle.load(BytesIO(store.getvalue()))
A:sklearn.tests.test_naive_bayes.ind->numpy.random.RandomState(0).randint(0, X.shape[0], 20)
A:sklearn.tests.test_naive_bayes.sample_weight->numpy.array([1, 1, 2, 2], dtype=np.float64)
A:sklearn.tests.test_naive_bayes.clf_dupl->GaussianNB().fit(X[ind], y[ind])
A:sklearn.tests.test_naive_bayes.x_empty->numpy.empty((0, X.shape[1]))
A:sklearn.tests.test_naive_bayes.(tmean, tvar)->sklearn.naive_bayes.GaussianNB._update_mean_variance(prev_points, mean, var, x_empty)
A:sklearn.tests.test_naive_bayes.y_pred2->pickle.load(BytesIO(store.getvalue())).predict(X)
A:sklearn.tests.test_naive_bayes.y_pred_proba2->pickle.load(BytesIO(store.getvalue())).predict_proba(X)
A:sklearn.tests.test_naive_bayes.y_pred_log_proba2->pickle.load(BytesIO(store.getvalue())).predict_log_proba(X)
A:sklearn.tests.test_naive_bayes.clf3->cls()
A:sklearn.tests.test_naive_bayes.y_pred3->cls().predict(X)
A:sklearn.tests.test_naive_bayes.y_pred_proba3->cls().predict_proba(X)
A:sklearn.tests.test_naive_bayes.y_pred_log_proba3->cls().predict_log_proba(X)
A:sklearn.tests.test_naive_bayes.clf_pf->GaussianNB().partial_fit(X, y, np.unique(y))
A:sklearn.tests.test_naive_bayes.clf_pf2->GaussianNB().partial_fit(X[0::2, :], y[0::2], np.unique(y))
A:sklearn.tests.test_naive_bayes.store->BytesIO()
A:sklearn.tests.test_naive_bayes.prior->numpy.exp(clf.class_log_prior_)
A:sklearn.tests.test_naive_bayes.iris->load_iris()
A:sklearn.tests.test_naive_bayes.(iris_data1, iris_data2, iris_target1, iris_target2)->train_test_split(iris.data, iris.target, test_size=0.4, random_state=415)
A:sklearn.tests.test_naive_bayes.clf_full->cls(class_prior=prior)
A:sklearn.tests.test_naive_bayes.clf_partial->cls(class_prior=prior)
A:sklearn.tests.test_naive_bayes.positive_prior->numpy.exp(clf.intercept_[0])
A:sklearn.tests.test_naive_bayes.digits->load_digits()
A:sklearn.tests.test_naive_bayes.binary_3v8->numpy.logical_or(digits.target == 3, digits.target == 8)
A:sklearn.tests.test_naive_bayes.scores->cross_val_score(GaussianNB(), X_3v8, y_3v8, cv=10)
A:sklearn.tests.test_naive_bayes.Y->numpy.array([0, 0, 0, 1])
A:sklearn.tests.test_naive_bayes.num->numpy.log(clf.feature_count_ + 1.0)
A:sklearn.tests.test_naive_bayes.class_prior->numpy.array([0.75, 0.25])
A:sklearn.tests.test_naive_bayes.feature_prob->numpy.array([[0.4, 0.8, 0.2, 0.4, 0.4, 0.2], [1 / 3.0, 2 / 3.0, 2 / 3.0, 1 / 3.0, 1 / 3.0, 2 / 3.0]])
A:sklearn.tests.test_naive_bayes.X_test->numpy.array([[0, 1, 1, 0, 0, 1]])
A:sklearn.tests.test_naive_bayes.unnorm_predict_proba->numpy.array([[0.005183999999999999, 0.02194787379972565]])
A:sklearn.tests.test_naive_bayes.nb->MultinomialNB(alpha=0.0)
A:sklearn.tests.test_naive_bayes.prob->numpy.array([[2.0 / 3, 1.0 / 3], [0, 1]])
A:sklearn.tests.test_naive_bayes.b_nb->BernoulliNB(alpha=-0.1)
A:sklearn.tests.test_naive_bayes.m_nb->MultinomialNB(alpha=-0.1)
sklearn.tests.test_naive_bayes.check_partial_fit(cls)
sklearn.tests.test_naive_bayes.check_sample_weight_multiclass(cls)
sklearn.tests.test_naive_bayes.test_alpha()
sklearn.tests.test_naive_bayes.test_bnb()
sklearn.tests.test_naive_bayes.test_check_accuracy_on_digits()
sklearn.tests.test_naive_bayes.test_check_update_with_no_data()
sklearn.tests.test_naive_bayes.test_coef_intercept_shape()
sklearn.tests.test_naive_bayes.test_discrete_prior()
sklearn.tests.test_naive_bayes.test_discretenb_partial_fit()
sklearn.tests.test_naive_bayes.test_discretenb_pickle()
sklearn.tests.test_naive_bayes.test_discretenb_predict_proba()
sklearn.tests.test_naive_bayes.test_discretenb_provide_prior()
sklearn.tests.test_naive_bayes.test_discretenb_provide_prior_with_partial_fit()
sklearn.tests.test_naive_bayes.test_discretenb_uniform_prior()
sklearn.tests.test_naive_bayes.test_feature_log_prob_bnb()
sklearn.tests.test_naive_bayes.test_gnb()
sklearn.tests.test_naive_bayes.test_gnb_neg_priors()
sklearn.tests.test_naive_bayes.test_gnb_partial_fit()
sklearn.tests.test_naive_bayes.test_gnb_pfit_wrong_nb_features()
sklearn.tests.test_naive_bayes.test_gnb_prior()
sklearn.tests.test_naive_bayes.test_gnb_prior_greater_one()
sklearn.tests.test_naive_bayes.test_gnb_prior_large_bias()
sklearn.tests.test_naive_bayes.test_gnb_priors()
sklearn.tests.test_naive_bayes.test_gnb_sample_weight()
sklearn.tests.test_naive_bayes.test_gnb_wrong_nb_priors()
sklearn.tests.test_naive_bayes.test_input_check_fit()
sklearn.tests.test_naive_bayes.test_input_check_partial_fit()
sklearn.tests.test_naive_bayes.test_mnnb()
sklearn.tests.test_naive_bayes.test_naive_bayes_scale_invariance()
sklearn.tests.test_naive_bayes.test_sample_weight_mnb()
sklearn.tests.test_naive_bayes.test_sample_weight_multiclass()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/tests/test_grid_search.py----------------------------------------
A:sklearn.tests.test_grid_search.X->numpy.arange(100).reshape(10, 10)
A:sklearn.tests.test_grid_search.y->numpy.array([0] * 5 + [1] * 5)
A:sklearn.tests.test_grid_search.grid1->ParameterGrid(params1)
A:sklearn.tests.test_grid_search.grid2->ParameterGrid(params2)
A:sklearn.tests.test_grid_search.points->set((tuple(chain(*sorted(p.items()))) for p in grid2))
A:sklearn.tests.test_grid_search.empty->ParameterGrid({})
A:sklearn.tests.test_grid_search.has_empty->ParameterGrid([{'C': [1, 10]}, {}, {'C': [0.5]}])
A:sklearn.tests.test_grid_search.clf->FailingClassifier()
A:sklearn.tests.test_grid_search.grid_search->GridSearchCV(Ridge(), {'alpha': [1.0, 2.0]})
A:sklearn.tests.test_grid_search.sys.stdout->StringIO()
A:sklearn.tests.test_grid_search.X_round_trip->GridSearchCV(Ridge(), {'alpha': [1.0, 2.0]}).inverse_transform(grid_search.transform(X))
A:sklearn.tests.test_grid_search.(X, y)->make_classification(n_samples=20, n_features=10, random_state=0)
A:sklearn.tests.test_grid_search.clf_no_score->LinearSVCNoScore(random_state=0)
A:sklearn.tests.test_grid_search.grid_search_no_score->GridSearchCV(clf_no_score, {'C': Cs})
A:sklearn.tests.test_grid_search.search_no_scoring->GridSearchCV(clf, grid, scoring=None).fit(X, y)
A:sklearn.tests.test_grid_search.search_accuracy->GridSearchCV(clf, grid, scoring='accuracy').fit(X, y)
A:sklearn.tests.test_grid_search.search_no_score_method_auc->GridSearchCV(LinearSVCNoScore(), grid, scoring='roc_auc').fit(X, y)
A:sklearn.tests.test_grid_search.search_auc->GridSearchCV(clf, grid, scoring='roc_auc').fit(X, y)
A:sklearn.tests.test_grid_search.score_no_scoring->assert_no_warnings(search_no_scoring.score, X, y)
A:sklearn.tests.test_grid_search.score_accuracy->assert_no_warnings(search_accuracy.score, X, y)
A:sklearn.tests.test_grid_search.score_no_score_auc->assert_no_warnings(search_no_score_method_auc.score, X, y)
A:sklearn.tests.test_grid_search.score_auc->assert_no_warnings(search_auc.score, X, y)
A:sklearn.tests.test_grid_search.random_search->RandomizedSearchCV(est, est_parameters, cv=cv, n_iter=3)
A:sklearn.tests.test_grid_search.(X_, y_)->make_classification(n_samples=200, n_features=100, random_state=0)
A:sklearn.tests.test_grid_search.cv->KFold(y.shape[0], random_state=0)
A:sklearn.tests.test_grid_search.mask->numpy.ones(X.shape[0], dtype=np.bool)
A:sklearn.tests.test_grid_search.svm->SVC(kernel='linear')
A:sklearn.tests.test_grid_search.y_pred->KFold(y.shape[0], random_state=0).predict(K_test)
A:sklearn.tests.test_grid_search.X_->scipy.sparse.csr_matrix(X_)
A:sklearn.tests.test_grid_search.y_pred2->KFold(y.shape[0], random_state=0).predict(X_[180:])
A:sklearn.tests.test_grid_search.F1Loss->make_scorer(f1_loss, greater_is_better=False)
A:sklearn.tests.test_grid_search.y_pred3->KFold(y.shape[0], random_state=0).predict(X_[180:])
A:sklearn.tests.test_grid_search.K_train->numpy.zeros((10, 20))
A:sklearn.tests.test_grid_search.K_test->numpy.dot(X_[180:], X_[:180].T)
A:sklearn.tests.test_grid_search.y_train->numpy.ones((10,))
A:sklearn.tests.test_grid_search.X_4d->numpy.arange(10 * 5 * 3 * 2).reshape(10, 5, 3, 2)
A:sklearn.tests.test_grid_search.y_3d->numpy.arange(10 * 7 * 11).reshape(10, 7, 11)
A:sklearn.tests.test_grid_search.km->KMeans(random_state=0)
A:sklearn.tests.test_grid_search.(X, _)->make_blobs(cluster_std=0.1, random_state=1, centers=[[0, 1], [1, 0], [0, 0]])
A:sklearn.tests.test_grid_search.search->RandomizedSearchCV(SVC(), n_iter=n_search_iter, cv=n_cv_iter, param_distributions=params, iid=False)
A:sklearn.tests.test_grid_search.sampler->ParameterSampler(params_distribution, n_iter=7)
A:sklearn.tests.test_grid_search.params->dict(C=expon(scale=10), gamma=expon(scale=0.1))
A:sklearn.tests.test_grid_search.sorted_grid_scores->list(sorted(search.grid_scores_, key=lambda x: x.mean_validation_score))
A:sklearn.tests.test_grid_search.correct_score->est.score(X[test], y[test])
A:sklearn.tests.test_grid_search.dec->FailingClassifier().decision_function(X[test])
A:sklearn.tests.test_grid_search.gs->GridSearchCV(clf, [{'parameter': [0, 1, 2]}], scoring='accuracy', refit=False, error_score='raise')
A:sklearn.tests.test_grid_search.p->Pipeline([('imputer', Imputer(strategy='mean', missing_values='NaN')), ('classifier', MockClassifier())])
A:sklearn.tests.test_grid_search.samples->list(sampler)
sklearn.tests.test_grid_search.BrokenClassifier(self,parameter=None)
sklearn.tests.test_grid_search.BrokenClassifier.__init__(self,parameter=None)
sklearn.tests.test_grid_search.BrokenClassifier.fit(self,X,y)
sklearn.tests.test_grid_search.BrokenClassifier.predict(self,X)
sklearn.tests.test_grid_search.FailingClassifier(self,parameter=None)
sklearn.tests.test_grid_search.FailingClassifier.__init__(self,parameter=None)
sklearn.tests.test_grid_search.FailingClassifier.fit(self,X,y=None)
sklearn.tests.test_grid_search.FailingClassifier.predict(self,X)
sklearn.tests.test_grid_search.LinearSVCNoScore(LinearSVC)
sklearn.tests.test_grid_search.LinearSVCNoScore.score(self)
sklearn.tests.test_grid_search.MockClassifier(self,foo_param=0)
sklearn.tests.test_grid_search.MockClassifier.__init__(self,foo_param=0)
sklearn.tests.test_grid_search.MockClassifier.fit(self,X,Y)
sklearn.tests.test_grid_search.MockClassifier.get_params(self,deep=False)
sklearn.tests.test_grid_search.MockClassifier.inverse_transform(self,X)
sklearn.tests.test_grid_search.MockClassifier.predict(self,T)
sklearn.tests.test_grid_search.MockClassifier.score(self,X=None,Y=None)
sklearn.tests.test_grid_search.MockClassifier.set_params(self,**params)
sklearn.tests.test_grid_search.MockClassifier.transform(self,X)
sklearn.tests.test_grid_search.assert_grid_iter_equals_getitem(grid)
sklearn.tests.test_grid_search.test_X_as_list()
sklearn.tests.test_grid_search.test_classes__property()
sklearn.tests.test_grid_search.test_grid_search()
sklearn.tests.test_grid_search.test_grid_search_allows_nans()
sklearn.tests.test_grid_search.test_grid_search_bad_param_grid()
sklearn.tests.test_grid_search.test_grid_search_error()
sklearn.tests.test_grid_search.test_grid_search_failing_classifier()
sklearn.tests.test_grid_search.test_grid_search_failing_classifier_raise()
sklearn.tests.test_grid_search.test_grid_search_iid()
sklearn.tests.test_grid_search.test_grid_search_no_score()
sklearn.tests.test_grid_search.test_grid_search_one_grid_point()
sklearn.tests.test_grid_search.test_grid_search_precomputed_kernel()
sklearn.tests.test_grid_search.test_grid_search_precomputed_kernel_error_kernel_function()
sklearn.tests.test_grid_search.test_grid_search_precomputed_kernel_error_nonsquare()
sklearn.tests.test_grid_search.test_grid_search_score_consistency()
sklearn.tests.test_grid_search.test_grid_search_score_method()
sklearn.tests.test_grid_search.test_grid_search_sparse()
sklearn.tests.test_grid_search.test_grid_search_sparse_scoring()
sklearn.tests.test_grid_search.test_grid_search_with_multioutput_data()
sklearn.tests.test_grid_search.test_gridsearch_nd()
sklearn.tests.test_grid_search.test_gridsearch_no_predict()
sklearn.tests.test_grid_search.test_no_refit()
sklearn.tests.test_grid_search.test_pandas_input()
sklearn.tests.test_grid_search.test_param_sampler()
sklearn.tests.test_grid_search.test_parameter_grid()
sklearn.tests.test_grid_search.test_parameters_sampler_replacement()
sklearn.tests.test_grid_search.test_pickle()
sklearn.tests.test_grid_search.test_predict_proba_disabled()
sklearn.tests.test_grid_search.test_randomized_search_grid_scores()
sklearn.tests.test_grid_search.test_refit()
sklearn.tests.test_grid_search.test_transform_inverse_transform_round_trip()
sklearn.tests.test_grid_search.test_trivial_grid_scores()
sklearn.tests.test_grid_search.test_unsupervised_grid_search()
sklearn.tests.test_grid_search.test_y_as_list()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/tests/test_pipeline.py----------------------------------------
A:sklearn.tests.test_pipeline.self.means_->numpy.mean(X, axis=0)
A:sklearn.tests.test_pipeline.self.timestamp_->time.time()
A:sklearn.tests.test_pipeline.clf->SVC(probability=True, random_state=0)
A:sklearn.tests.test_pipeline.pipe->Pipeline([('transf', clone(transf)), ('svc', clf)])
A:sklearn.tests.test_pipeline.filter1->SelectKBest(f_classif, k=2)
A:sklearn.tests.test_pipeline.pipe2->assert_no_warnings(clone, pipe)
A:sklearn.tests.test_pipeline.params->Pipeline([('transf', clone(transf)), ('svc', clf)]).get_params(deep=True)
A:sklearn.tests.test_pipeline.params2->assert_no_warnings(clone, pipe).get_params(deep=True)
A:sklearn.tests.test_pipeline.X->numpy.array([[1, 2]])
A:sklearn.tests.test_pipeline.iris->load_iris()
A:sklearn.tests.test_pipeline.pca->PCA(n_components=2, svd_solver='randomized', random_state=0)
A:sklearn.tests.test_pipeline.n_classes->len(np.unique(y))
A:sklearn.tests.test_pipeline.scaler->StandardScaler()
A:sklearn.tests.test_pipeline.predict->Pipeline([('transf', clone(transf)), ('svc', clf)]).predict(X)
A:sklearn.tests.test_pipeline.proba->Pipeline([('transf', clone(transf)), ('svc', clf)]).predict_proba(X)
A:sklearn.tests.test_pipeline.log_proba->Pipeline([('transf', clone(transf)), ('svc', clf)]).predict_log_proba(X)
A:sklearn.tests.test_pipeline.decision_function->Pipeline([('transf', clone(transf)), ('svc', clf)]).decision_function(X)
A:sklearn.tests.test_pipeline.km->KMeans(random_state=0)
A:sklearn.tests.test_pipeline.scaler_for_pipeline->StandardScaler()
A:sklearn.tests.test_pipeline.km_for_pipeline->KMeans(random_state=0)
A:sklearn.tests.test_pipeline.scaled->StandardScaler().fit_transform(iris.data)
A:sklearn.tests.test_pipeline.separate_pred->KMeans(random_state=0).fit_predict(scaled)
A:sklearn.tests.test_pipeline.pipeline_pred->Pipeline([('transf', clone(transf)), ('svc', clf)]).fit_predict(iris.data)
A:sklearn.tests.test_pipeline.svd->TruncatedSVD(n_components=2, random_state=0)
A:sklearn.tests.test_pipeline.select->SelectKBest(k=1)
A:sklearn.tests.test_pipeline.fs->FeatureUnion([('words', CountVectorizer(analyzer='word')), ('chars', CountVectorizer(analyzer='char'))])
A:sklearn.tests.test_pipeline.X_transformed->FeatureUnion([('words', CountVectorizer(analyzer='word')), ('chars', CountVectorizer(analyzer='char'))]).transform(X)
A:sklearn.tests.test_pipeline.X_sp->scipy.sparse.csr_matrix(X)
A:sklearn.tests.test_pipeline.X_sp_transformed->FeatureUnion([('words', CountVectorizer(analyzer='word')), ('chars', CountVectorizer(analyzer='char'))]).fit_transform(X_sp, y)
A:sklearn.tests.test_pipeline.fs2->assert_no_warnings(clone, fs)
A:sklearn.tests.test_pipeline.mock->Transf()
A:sklearn.tests.test_pipeline.fu->make_union(pca, mock, n_jobs=3)
A:sklearn.tests.test_pipeline.(names, transformers)->zip(*fu.transformer_list)
A:sklearn.tests.test_pipeline.pipeline->make_pipeline(DummyTransf(), SVC())
A:sklearn.tests.test_pipeline.X_trans->make_pipeline(DummyTransf(), SVC()).fit_transform(X, y)
A:sklearn.tests.test_pipeline.X_trans2->DummyTransf().fit(X, y).transform(X)
A:sklearn.tests.test_pipeline.X_trans3->PCA(n_components=2, svd_solver='randomized', random_state=0).fit_transform(X)
A:sklearn.tests.test_pipeline.X_back->make_pipeline(DummyTransf(), SVC()).inverse_transform(X_trans)
A:sklearn.tests.test_pipeline.X_back2->PCA(n_components=2, svd_solver='randomized', random_state=0).inverse_transform(X_trans)
A:sklearn.tests.test_pipeline.transf->DummyTransf()
A:sklearn.tests.test_pipeline.transf1->Transf()
A:sklearn.tests.test_pipeline.transf2->Transf()
A:sklearn.tests.test_pipeline.mult2->Mult(2)
A:sklearn.tests.test_pipeline.y->numpy.array([1])
A:sklearn.tests.test_pipeline.mult3->Mult(3)
A:sklearn.tests.test_pipeline.mult5->Mult(5)
A:sklearn.tests.test_pipeline.t1->Transf()
A:sklearn.tests.test_pipeline.t2->Transf()
A:sklearn.tests.test_pipeline.X_fit_transformed->FeatureUnion([('words', CountVectorizer(analyzer='word')), ('chars', CountVectorizer(analyzer='char'))]).fit_transform(X, y)
A:sklearn.tests.test_pipeline.X_fit_transformed_wo_method->FeatureUnion([('words', CountVectorizer(analyzer='word')), ('chars', CountVectorizer(analyzer='char'))]).fit_transform(X, y)
A:sklearn.tests.test_pipeline.fs_parallel->FeatureUnion([('words', CountVectorizer(analyzer='word')), ('chars', CountVectorizer(analyzer='char'))], n_jobs=2)
A:sklearn.tests.test_pipeline.fs_parallel2->FeatureUnion([('words', CountVectorizer(analyzer='word')), ('chars', CountVectorizer(analyzer='char'))], n_jobs=2)
A:sklearn.tests.test_pipeline.X_transformed_parallel->FeatureUnion([('words', CountVectorizer(analyzer='word')), ('chars', CountVectorizer(analyzer='char'))], n_jobs=2).transform(X)
A:sklearn.tests.test_pipeline.X_transformed_parallel2->FeatureUnion([('words', CountVectorizer(analyzer='word')), ('chars', CountVectorizer(analyzer='char'))], n_jobs=2).transform(X)
A:sklearn.tests.test_pipeline.word_vect->CountVectorizer(analyzer='word')
A:sklearn.tests.test_pipeline.char_vect->CountVectorizer(analyzer='char_wb', ngram_range=(3, 3))
A:sklearn.tests.test_pipeline.ft->FeatureUnion([('m2', mult2), ('m3', mult3)])
A:sklearn.tests.test_pipeline.feature_names->FeatureUnion([('m2', mult2), ('m3', mult3)]).get_feature_names()
A:sklearn.tests.test_pipeline.reg->make_pipeline(SelectKBest(k=1), LinearRegression())
A:sklearn.tests.test_pipeline.est->cls(**{param: [('a', Mult(1))]})
A:sklearn.tests.test_pipeline.estimator->Pipeline([('a', Pipeline([('b', DummyRegressor())]))])
A:sklearn.tests.test_pipeline.cached_pipe->Pipeline([('transf', transf), ('svc', clf)], memory=memory)
A:sklearn.tests.test_pipeline.dummy->WrongDummyMemory()
A:sklearn.tests.test_pipeline.cachedir->mkdtemp()
A:sklearn.tests.test_pipeline.memory->Memory(cachedir=cachedir)
A:sklearn.tests.test_pipeline.clf_2->SVC(probability=True, random_state=0)
A:sklearn.tests.test_pipeline.transf_2->DummyTransf()
A:sklearn.tests.test_pipeline.cached_pipe_2->Pipeline([('transf_2', transf_2), ('svc', clf_2)], memory=memory)
sklearn.tests.test_pipeline.DummyMemory(object)
sklearn.tests.test_pipeline.DummyMemory.cache(self,func)
sklearn.tests.test_pipeline.DummyTransf(Transf)
sklearn.tests.test_pipeline.DummyTransf.fit(self,X,y)
sklearn.tests.test_pipeline.FitParamT(self)
sklearn.tests.test_pipeline.FitParamT.__init__(self)
sklearn.tests.test_pipeline.FitParamT.fit(self,X,y,should_succeed=False)
sklearn.tests.test_pipeline.FitParamT.fit_predict(self,X,y,should_succeed=False)
sklearn.tests.test_pipeline.FitParamT.predict(self,X)
sklearn.tests.test_pipeline.FitParamT.score(self,X,y=None,sample_weight=None)
sklearn.tests.test_pipeline.Mult(self,mult=1)
sklearn.tests.test_pipeline.Mult.__init__(self,mult=1)
sklearn.tests.test_pipeline.Mult.fit(self,X,y)
sklearn.tests.test_pipeline.Mult.inverse_transform(self,X)
sklearn.tests.test_pipeline.Mult.predict(self,X)
sklearn.tests.test_pipeline.Mult.score(self,X,y=None)
sklearn.tests.test_pipeline.Mult.transform(self,X)
sklearn.tests.test_pipeline.NoFit(self,a=None,b=None)
sklearn.tests.test_pipeline.NoFit.__init__(self,a=None,b=None)
sklearn.tests.test_pipeline.NoInvTransf(NoTrans)
sklearn.tests.test_pipeline.NoInvTransf.transform(self,X)
sklearn.tests.test_pipeline.NoTrans(NoFit)
sklearn.tests.test_pipeline.NoTrans.fit(self,X,y)
sklearn.tests.test_pipeline.NoTrans.get_params(self,deep=False)
sklearn.tests.test_pipeline.NoTrans.set_params(self,**params)
sklearn.tests.test_pipeline.Transf(NoInvTransf)
sklearn.tests.test_pipeline.Transf.inverse_transform(self,X)
sklearn.tests.test_pipeline.Transf.transform(self,X)
sklearn.tests.test_pipeline.TransfFitParams(Transf)
sklearn.tests.test_pipeline.TransfFitParams.fit(self,X,y,**fit_params)
sklearn.tests.test_pipeline.WrongDummyMemory(object)
sklearn.tests.test_pipeline.test_classes_property()
sklearn.tests.test_pipeline.test_feature_union()
sklearn.tests.test_pipeline.test_feature_union_feature_names()
sklearn.tests.test_pipeline.test_feature_union_parallel()
sklearn.tests.test_pipeline.test_feature_union_weights()
sklearn.tests.test_pipeline.test_fit_predict_on_pipeline()
sklearn.tests.test_pipeline.test_fit_predict_on_pipeline_without_fit_predict()
sklearn.tests.test_pipeline.test_fit_predict_with_intermediate_fit_params()
sklearn.tests.test_pipeline.test_make_pipeline()
sklearn.tests.test_pipeline.test_make_pipeline_memory()
sklearn.tests.test_pipeline.test_make_union()
sklearn.tests.test_pipeline.test_make_union_kwargs()
sklearn.tests.test_pipeline.test_pipeline_ducktyping()
sklearn.tests.test_pipeline.test_pipeline_fit_params()
sklearn.tests.test_pipeline.test_pipeline_fit_transform()
sklearn.tests.test_pipeline.test_pipeline_init()
sklearn.tests.test_pipeline.test_pipeline_init_tuple()
sklearn.tests.test_pipeline.test_pipeline_memory()
sklearn.tests.test_pipeline.test_pipeline_methods_anova()
sklearn.tests.test_pipeline.test_pipeline_methods_pca_svm()
sklearn.tests.test_pipeline.test_pipeline_methods_preprocessing_svm()
sklearn.tests.test_pipeline.test_pipeline_named_steps()
sklearn.tests.test_pipeline.test_pipeline_raise_set_params_error()
sklearn.tests.test_pipeline.test_pipeline_sample_weight_supported()
sklearn.tests.test_pipeline.test_pipeline_sample_weight_unsupported()
sklearn.tests.test_pipeline.test_pipeline_transform()
sklearn.tests.test_pipeline.test_pipeline_with_cache_attribute()
sklearn.tests.test_pipeline.test_pipeline_wrong_memory()
sklearn.tests.test_pipeline.test_set_feature_union_step_none()
sklearn.tests.test_pipeline.test_set_feature_union_steps()
sklearn.tests.test_pipeline.test_set_params_nested_pipeline()
sklearn.tests.test_pipeline.test_set_pipeline_step_none()
sklearn.tests.test_pipeline.test_set_pipeline_steps()
sklearn.tests.test_pipeline.test_step_name_validation()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/tests/test_docstring_parameters.py----------------------------------------
A:sklearn.tests.test_docstring_parameters.PUBLIC_MODULES->set(['sklearn.' + modname for (_, modname, _) in walk_packages(sklearn.__path__) if not modname.startswith('_') and '.tests.' not in modname])
A:sklearn.tests.test_docstring_parameters.module->importlib.import_module(name)
A:sklearn.tests.test_docstring_parameters.classes->inspect.getmembers(module, inspect.isclass)
A:sklearn.tests.test_docstring_parameters.cdoc->numpydoc.docscrape.ClassDoc(cls)
A:sklearn.tests.test_docstring_parameters.cls_init->getattr(cls, '__init__', None)
A:sklearn.tests.test_docstring_parameters.method->getattr(cls, method_name)
A:sklearn.tests.test_docstring_parameters.sig->signature(method)
A:sklearn.tests.test_docstring_parameters.result->check_docstring_parameters(method, ignore=param_ignore, class_name=cname)
A:sklearn.tests.test_docstring_parameters.functions->inspect.getmembers(module, inspect.isfunction)
A:sklearn.tests.test_docstring_parameters.name_->_get_func_name(func)
A:sklearn.tests.test_docstring_parameters.mod->importlib.import_module(modname)
A:sklearn.tests.test_docstring_parameters.source->getsource(mod)
sklearn.tests.test_docstring_parameters.test_docstring_parameters()
sklearn.tests.test_docstring_parameters.test_tabs()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/__check_build/__init__.py----------------------------------------
A:sklearn.__check_build.__init__.dir_content->list()
sklearn.__check_build.__init__.raise_build_error(e)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/__check_build/setup.py----------------------------------------
A:sklearn.__check_build.setup.config->Configuration('__check_build', parent_package, top_path)
sklearn.__check_build.setup.configuration(parent_package='',top_path=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/tree/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/tree/setup.py----------------------------------------
A:sklearn.tree.setup.config->Configuration('tree', parent_package, top_path)
sklearn.tree.setup.configuration(parent_package='',top_path=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/tree/tree.py----------------------------------------
A:sklearn.tree.tree.random_state->check_random_state(self.random_state)
A:sklearn.tree.tree.X->self._validate_X_predict(X, check_input)
A:sklearn.tree.tree.y->numpy.ascontiguousarray(y, dtype=DOUBLE)
A:sklearn.tree.tree.is_classification->is_classifier(self)
A:sklearn.tree.tree.y_original->numpy.copy(y)
A:sklearn.tree.tree.y_encoded->numpy.zeros(y.shape, dtype=np.int)
A:sklearn.tree.tree.(classes_k, y_encoded[:, k])->numpy.unique(y[:, k], return_inverse=True)
A:sklearn.tree.tree.expanded_class_weight->compute_sample_weight(self.class_weight, y_original)
A:sklearn.tree.tree.self.n_classes_->numpy.array(self.n_classes_, dtype=np.intp)
A:sklearn.tree.tree.min_samples_leaf->int(ceil(self.min_samples_leaf * n_samples))
A:sklearn.tree.tree.min_samples_split->max(min_samples_split, 2 * min_samples_leaf)
A:sklearn.tree.tree.max_features->max(1, int(self.max_features * self.n_features_))
A:sklearn.tree.tree.sample_weight->numpy.ascontiguousarray(sample_weight, dtype=DOUBLE)
A:sklearn.tree.tree.X_idx_sorted->numpy.asfortranarray(np.argsort(X, axis=0), dtype=np.int32)
A:sklearn.tree.tree.criterion->CRITERIA_REG[self.criterion](self.n_outputs_, n_samples)
A:sklearn.tree.tree.splitter->SPLITTERS[self.splitter](criterion, self.max_features_, min_samples_leaf, min_weight_leaf, random_state, self.presort)
A:sklearn.tree.tree.self.tree_->Tree(self.n_features_, self.n_classes_, self.n_outputs_)
A:sklearn.tree.tree.builder->BestFirstTreeBuilder(splitter, min_samples_split, min_samples_leaf, min_weight_leaf, max_depth, max_leaf_nodes, self.min_impurity_decrease, min_impurity_split)
A:sklearn.tree.tree.proba->self.predict_proba(X)
A:sklearn.tree.tree.predictions->numpy.zeros((n_samples, self.n_outputs_))
A:sklearn.tree.tree.predictions[:, k]->self.classes_[k].take(np.argmax(proba[:, k], axis=1), axis=0)
A:sklearn.tree.tree.proba[k]->numpy.log(proba[k])
sklearn.tree.DecisionTreeClassifier(self,criterion='gini',splitter='best',max_depth=None,min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_features=None,random_state=None,max_leaf_nodes=None,min_impurity_decrease=0.0,min_impurity_split=None,class_weight=None,presort=False)
sklearn.tree.DecisionTreeClassifier.fit(self,X,y,sample_weight=None,check_input=True,X_idx_sorted=None)
sklearn.tree.DecisionTreeClassifier.predict_log_proba(self,X)
sklearn.tree.DecisionTreeClassifier.predict_proba(self,X,check_input=True)
sklearn.tree.DecisionTreeRegressor(self,criterion='mse',splitter='best',max_depth=None,min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_features=None,random_state=None,max_leaf_nodes=None,min_impurity_decrease=0.0,min_impurity_split=None,presort=False)
sklearn.tree.DecisionTreeRegressor.fit(self,X,y,sample_weight=None,check_input=True,X_idx_sorted=None)
sklearn.tree.ExtraTreeClassifier(self,criterion='gini',splitter='random',max_depth=None,min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_features='auto',random_state=None,max_leaf_nodes=None,min_impurity_decrease=0.0,min_impurity_split=None,class_weight=None)
sklearn.tree.ExtraTreeRegressor(self,criterion='mse',splitter='random',max_depth=None,min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_features='auto',random_state=None,min_impurity_decrease=0.0,min_impurity_split=None,max_leaf_nodes=None)
sklearn.tree.tree.BaseDecisionTree(self,criterion,splitter,max_depth,min_samples_split,min_samples_leaf,min_weight_fraction_leaf,max_features,max_leaf_nodes,random_state,min_impurity_decrease,min_impurity_split,class_weight=None,presort=False)
sklearn.tree.tree.BaseDecisionTree.__init__(self,criterion,splitter,max_depth,min_samples_split,min_samples_leaf,min_weight_fraction_leaf,max_features,max_leaf_nodes,random_state,min_impurity_decrease,min_impurity_split,class_weight=None,presort=False)
sklearn.tree.tree.BaseDecisionTree._validate_X_predict(self,X,check_input)
sklearn.tree.tree.BaseDecisionTree.apply(self,X,check_input=True)
sklearn.tree.tree.BaseDecisionTree.decision_path(self,X,check_input=True)
sklearn.tree.tree.BaseDecisionTree.feature_importances_(self)
sklearn.tree.tree.BaseDecisionTree.fit(self,X,y,sample_weight=None,check_input=True,X_idx_sorted=None)
sklearn.tree.tree.BaseDecisionTree.predict(self,X,check_input=True)
sklearn.tree.tree.DecisionTreeClassifier(self,criterion='gini',splitter='best',max_depth=None,min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_features=None,random_state=None,max_leaf_nodes=None,min_impurity_decrease=0.0,min_impurity_split=None,class_weight=None,presort=False)
sklearn.tree.tree.DecisionTreeClassifier.__init__(self,criterion='gini',splitter='best',max_depth=None,min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_features=None,random_state=None,max_leaf_nodes=None,min_impurity_decrease=0.0,min_impurity_split=None,class_weight=None,presort=False)
sklearn.tree.tree.DecisionTreeClassifier.fit(self,X,y,sample_weight=None,check_input=True,X_idx_sorted=None)
sklearn.tree.tree.DecisionTreeClassifier.predict_log_proba(self,X)
sklearn.tree.tree.DecisionTreeClassifier.predict_proba(self,X,check_input=True)
sklearn.tree.tree.DecisionTreeRegressor(self,criterion='mse',splitter='best',max_depth=None,min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_features=None,random_state=None,max_leaf_nodes=None,min_impurity_decrease=0.0,min_impurity_split=None,presort=False)
sklearn.tree.tree.DecisionTreeRegressor.__init__(self,criterion='mse',splitter='best',max_depth=None,min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_features=None,random_state=None,max_leaf_nodes=None,min_impurity_decrease=0.0,min_impurity_split=None,presort=False)
sklearn.tree.tree.DecisionTreeRegressor.fit(self,X,y,sample_weight=None,check_input=True,X_idx_sorted=None)
sklearn.tree.tree.ExtraTreeClassifier(self,criterion='gini',splitter='random',max_depth=None,min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_features='auto',random_state=None,max_leaf_nodes=None,min_impurity_decrease=0.0,min_impurity_split=None,class_weight=None)
sklearn.tree.tree.ExtraTreeClassifier.__init__(self,criterion='gini',splitter='random',max_depth=None,min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_features='auto',random_state=None,max_leaf_nodes=None,min_impurity_decrease=0.0,min_impurity_split=None,class_weight=None)
sklearn.tree.tree.ExtraTreeRegressor(self,criterion='mse',splitter='random',max_depth=None,min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_features='auto',random_state=None,min_impurity_decrease=0.0,min_impurity_split=None,max_leaf_nodes=None)
sklearn.tree.tree.ExtraTreeRegressor.__init__(self,criterion='mse',splitter='random',max_depth=None,min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_features='auto',random_state=None,min_impurity_decrease=0.0,min_impurity_split=None,max_leaf_nodes=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/tree/export.py----------------------------------------
A:sklearn.tree.export.SENTINEL->Sentinel()
A:sklearn.tree.export.color->list(colors['rgb'][0])
A:sklearn.tree.export.sorted_values->sorted(value, reverse=True)
A:sklearn.tree.export.alpha->int(np.round(255 * ((value - colors['bounds'][0]) / (colors['bounds'][1] - colors['bounds'][0])), 0))
A:sklearn.tree.export.value_text->value_text.replace('\n ', characters[4]).replace('\n ', characters[4])
A:sklearn.tree.export.colors['rgb']->_color_brew(tree.n_classes[0])
A:sklearn.tree.export.out_file->externals.six.StringIO()
sklearn.tree.export.Sentinel(object)
sklearn.tree.export.Sentinel.__repr__(self)
sklearn.tree.export._color_brew(n)
sklearn.tree.export.export_graphviz(decision_tree,out_file=SENTINEL,max_depth=None,feature_names=None,class_names=None,label='all',filled=False,leaves_parallel=False,impurity=True,node_ids=False,proportion=False,rotate=False,rounded=False,special_characters=False,precision=3)
sklearn.tree.export_graphviz(decision_tree,out_file=SENTINEL,max_depth=None,feature_names=None,class_names=None,label='all',filled=False,leaves_parallel=False,impurity=True,node_ids=False,proportion=False,rotate=False,rounded=False,special_characters=False,precision=3)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/tree/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/tree/tests/test_export.py----------------------------------------
A:sklearn.tree.tests.test_export.clf->GradientBoostingClassifier(n_estimators=2, random_state=0)
A:sklearn.tree.tests.test_export.contents1->export_graphviz(clf, filled=True, out_file=None)
A:sklearn.tree.tests.test_export.out->StringIO()
A:sklearn.tree.tests.test_export.dot_data->export_graphviz(clf, out_file=None, precision=precision, proportion=True)
A:sklearn.tree.tests.test_export.rng_reg->RandomState(2)
A:sklearn.tree.tests.test_export.rng_clf->RandomState(8)
sklearn.tree.tests.test_export.test_friedman_mse_in_graphviz()
sklearn.tree.tests.test_export.test_graphviz_errors()
sklearn.tree.tests.test_export.test_graphviz_toy()
sklearn.tree.tests.test_export.test_precision()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/tree/tests/test_tree.py----------------------------------------
A:sklearn.tree.tests.test_tree.ALL_TREES->dict()
A:sklearn.tree.tests.test_tree.X_small->numpy.array([[0, 0, 4, 0, 0, 0, 1, -14, 0, -4, 0, 0, 0, 0], [0, 0, 5, 3, 0, -4, 0, 0, 1, -5, 0.2, 0, 4, 1], [-1, -1, 0, 0, -4.5, 0, 0, 2.1, 1, 0, 0, -4.5, 0, 1], [-1, -1, 0, -1.2, 0, 0, 0, 0, 0, 0, 0.2, 0, 0, 1], [-1, -1, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 1], [-1, -2, 0, 4, -3, 10, 4, 0, -3.2, 0, 4, 3, -4, 1], [2.11, 0, -6, -0.5, 0, 11, 0, 0, -3.2, 6, 0.5, 0, -3, 1], [2.11, 0, -6, -0.5, 0, 11, 0, 0, -3.2, 6, 0, 0, -2, 1], [2.11, 8, -6, -0.5, 0, 11, 0, 0, -3.2, 6, 0, 0, -2, 1], [2.11, 8, -6, -0.5, 0, 11, 0, 0, -3.2, 6, 0.5, 0, -1, 0], [2, 8, 5, 1, 0.5, -4, 10, 0, 1, -5, 3, 0, 2, 0], [2, 0, 1, 1, 1, -1, 1, 0, 0, -2, 3, 0, 1, 0], [2, 0, 1, 2, 3, -1, 10, 2, 0, -1, 1, 2, 2, 0], [1, 1, 0, 2, 2, -1, 1, 2, 0, -5, 1, 2, 3, 0], [3, 1, 0, 3, 0, -4, 10, 0, 1, -5, 3, 0, 3, 1], [2.11, 8, -6, -0.5, 0, 1, 0, 0, -3.2, 6, 0.5, 0, -3, 1], [2.11, 8, -6, -0.5, 0, 1, 0, 0, -3.2, 6, 1.5, 1, -1, -1], [2.11, 8, -6, -0.5, 0, 10, 0, 0, -3.2, 6, 0.5, 0, -1, -1], [2, 0, 5, 1, 0.5, -2, 10, 0, 1, -5, 3, 1, 0, -1], [2, 0, 1, 1, 1, -2, 1, 0, 0, -2, 0, 0, 0, 1], [2, 1, 1, 1, 2, -1, 10, 2, 0, -1, 0, 2, 1, 1], [1, 1, 0, 0, 1, -3, 1, 2, 0, -5, 1, 2, 1, 1], [3, 1, 0, 1, 0, -4, 1, 0, 1, -2, 0, 0, 1, 0]])
A:sklearn.tree.tests.test_tree.iris->sklearn.datasets.load_iris()
A:sklearn.tree.tests.test_tree.rng->numpy.random.RandomState(1)
A:sklearn.tree.tests.test_tree.perm->numpy.random.RandomState(1).permutation(digits.target.size)
A:sklearn.tree.tests.test_tree.boston->sklearn.datasets.load_boston()
A:sklearn.tree.tests.test_tree.digits->sklearn.datasets.load_digits()
A:sklearn.tree.tests.test_tree.random_state->check_random_state(0)
A:sklearn.tree.tests.test_tree.(X_multilabel, y_multilabel)->sklearn.datasets.make_multilabel_classification(random_state=0, n_samples=30, n_features=10)
A:sklearn.tree.tests.test_tree.X_sparse_pos->check_random_state(0).uniform(size=(20, 5))
A:sklearn.tree.tests.test_tree.y_random->check_random_state(0).randint(0, 4, size=(20,))
A:sklearn.tree.tests.test_tree.X_sparse_mix->sparse_random_matrix(20, 10, density=0.25, random_state=0)
A:sklearn.tree.tests.test_tree.DATASETS[name]['X_sparse']->csc_matrix(DATASETS[name]['X'])
A:sklearn.tree.tests.test_tree.internal->numpy.logical_not(external)
A:sklearn.tree.tests.test_tree.clf->DecisionTreeClassifier(splitter='best', max_leaf_nodes=huge)
A:sklearn.tree.tests.test_tree.reg->TreeRegressor(random_state=0)
A:sklearn.tree.tests.test_tree.y->check_random_state(0).randint(0, 3, size=(n_samples,))
A:sklearn.tree.tests.test_tree.(gridx, gridy)->numpy.indices(y.shape)
A:sklearn.tree.tests.test_tree.score->TreeEstimator(random_state=0, max_depth=2).score(X, y)
A:sklearn.tree.tests.test_tree.prob_predict->DecisionTreeClassifier(splitter='best', max_leaf_nodes=huge).predict_proba(iris.data)
A:sklearn.tree.tests.test_tree.X->numpy.array([[0], [0], [0], [0], [1]])
A:sklearn.tree.tests.test_tree.(X, y)->sklearn.datasets.make_hastie_10_2(n_samples=100, random_state=1)
A:sklearn.tree.tests.test_tree.n_important->numpy.sum(importances > 0.1)
A:sklearn.tree.tests.test_tree.clf2->TreeClassifier(class_weight=class_weight, random_state=0)
A:sklearn.tree.tests.test_tree.est->TreeEstimator(random_state=0, max_depth=2)
A:sklearn.tree.tests.test_tree.Xf->numpy.asfortranarray(X)
A:sklearn.tree.tests.test_tree.t->numpy.asarray(T)
A:sklearn.tree.tests.test_tree.out->TreeEstimator(random_state=0, max_depth=2).tree_.apply(X)
A:sklearn.tree.tests.test_tree.node_counts->numpy.bincount(out)
A:sklearn.tree.tests.test_tree.weights->numpy.random.RandomState(1).rand(X.shape[0])
A:sklearn.tree.tests.test_tree.total_weight->numpy.sum(weights)
A:sklearn.tree.tests.test_tree.node_weights->numpy.bincount(out)
A:sklearn.tree.tests.test_tree.est1->TreeEstimator(max_leaf_nodes=max_leaf_nodes, random_state=0)
A:sklearn.tree.tests.test_tree.est2->pickle.loads(serialized_object)
A:sklearn.tree.tests.test_tree.est3->TreeEstimator(max_leaf_nodes=max_leaf_nodes, min_impurity_decrease=0.0001, random_state=0)
A:sklearn.tree.tests.test_tree.est4->TreeEstimator(max_leaf_nodes=max_leaf_nodes, min_impurity_decrease=0.1, random_state=0)
A:sklearn.tree.tests.test_tree.fitted_attribute->dict()
A:sklearn.tree.tests.test_tree.fitted_attribute[attribute]->getattr(est.tree_, attribute)
A:sklearn.tree.tests.test_tree.serialized_object->pickle.dumps(est)
A:sklearn.tree.tests.test_tree.score2->pickle.loads(serialized_object).score(X, y)
A:sklearn.tree.tests.test_tree.y_hat->TreeRegressor(random_state=0).fit(X, y).predict(T)
A:sklearn.tree.tests.test_tree.proba->DecisionTreeClassifier(splitter='best', max_leaf_nodes=huge).predict_proba(T)
A:sklearn.tree.tests.test_tree.log_proba->DecisionTreeClassifier(splitter='best', max_leaf_nodes=huge).predict_log_proba(T)
A:sklearn.tree.tests.test_tree.sample_weight->numpy.ones(iris.target.shape)
A:sklearn.tree.tests.test_tree.duplicates->numpy.random.RandomState(1).randint(0, X.shape[0], 100)
A:sklearn.tree.tests.test_tree.clf1->TreeClassifier(random_state=0)
A:sklearn.tree.tests.test_tree.clf3->TreeClassifier(class_weight=[{0: 2.0, 1: 2.0, 2: 1.0}, {0: 2.0, 1: 1.0, 2: 2.0}, {0: 1.0, 1: 2.0, 2: 2.0}], random_state=0)
A:sklearn.tree.tests.test_tree.clf4->TreeClassifier(class_weight='balanced', random_state=0)
A:sklearn.tree.tests.test_tree.value->getattr(DecisionTreeClassifier().fit([[0], [1]], [0, 1]).tree_, attr)
A:sklearn.tree.tests.test_tree.X_sparse->csc_matrix((data, indices, indptr), shape=(n_samples, n_features))
A:sklearn.tree.tests.test_tree.d->TreeEstimator(random_state=0, max_depth=max_depth).fit(X, y)
A:sklearn.tree.tests.test_tree.s->TreeEstimator(random_state=0, max_depth=max_depth).fit(X_sparse, y)
A:sklearn.tree.tests.test_tree.y_pred->TreeEstimator(random_state=0, max_depth=max_depth).fit(X, y).predict(X)
A:sklearn.tree.tests.test_tree.y_proba->TreeEstimator(random_state=0, max_depth=max_depth).fit(X, y).predict_proba(X)
A:sklearn.tree.tests.test_tree.y_log_proba->TreeEstimator(random_state=0, max_depth=max_depth).fit(X, y).predict_log_proba(X)
A:sklearn.tree.tests.test_tree.X_sparse_test->X_sparse_test.copy().copy()
A:sklearn.tree.tests.test_tree.samples->numpy.arange(n_samples)
A:sklearn.tree.tests.test_tree.n_nonzero_i->check_random_state(0).binomial(n_samples, 0.5)
A:sklearn.tree.tests.test_tree.indices->numpy.concatenate(indices)
A:sklearn.tree.tests.test_tree.data->numpy.array(np.concatenate(data), dtype=np.float32)
A:sklearn.tree.tests.test_tree.X_test->X_sparse_test.copy().copy().toarray()
A:sklearn.tree.tests.test_tree.X_2d->sklearn.datasets.load_iris().data[:, 0].reshape((-1, 1))
A:sklearn.tree.tests.test_tree.X_small32->csr_matrix(X_small.astype(tree._tree.DTYPE))
A:sklearn.tree.tests.test_tree.(y, X)->sklearn.datasets.make_multilabel_classification(random_state=0, n_samples=50, n_features=1, n_classes=20)
A:sklearn.tree.tests.test_tree.node_indicator->est.decision_path(X).toarray()
A:sklearn.tree.tests.test_tree.node_indicator_csr->TreeEstimator(random_state=0, max_depth=2).decision_path(X)
A:sklearn.tree.tests.test_tree.leaves->TreeEstimator(random_state=0, max_depth=2).apply(X)
A:sklearn.tree.tests.test_tree.max_depth->est.decision_path(X).toarray().sum(axis=1).max()
A:sklearn.tree.tests.test_tree.dt_mae->DecisionTreeRegressor(random_state=0, criterion='mae', max_leaf_nodes=2)
A:sklearn.tree.tests.test_tree.n_classes->numpy.arange(3, dtype=np.intp)
A:sklearn.tree.tests.test_tree.criteria->typename(n_outputs, n_samples)
A:sklearn.tree.tests.test_tree.result->copy_func(criteria).__reduce__()
sklearn.tree.tests.test_tree._check_min_weight_leaf_split_level(TreeEstimator,X,y,sample_weight)
sklearn.tree.tests.test_tree.assert_tree_equal(d,s,message)
sklearn.tree.tests.test_tree.check_class_weight_errors(name)
sklearn.tree.tests.test_tree.check_class_weights(name)
sklearn.tree.tests.test_tree.check_decision_path(name)
sklearn.tree.tests.test_tree.check_explicit_sparse_zeros(tree,max_depth=3,n_features=10)
sklearn.tree.tests.test_tree.check_min_weight_fraction_leaf(name,datasets,sparse=False)
sklearn.tree.tests.test_tree.check_min_weight_fraction_leaf_with_min_samples_leaf(name,datasets,sparse=False)
sklearn.tree.tests.test_tree.check_min_weight_leaf_split_level(name)
sklearn.tree.tests.test_tree.check_no_sparse_y_support(name)
sklearn.tree.tests.test_tree.check_presort_sparse(est,X,y)
sklearn.tree.tests.test_tree.check_public_apply(name)
sklearn.tree.tests.test_tree.check_public_apply_sparse(name)
sklearn.tree.tests.test_tree.check_raise_error_on_1d_input(name)
sklearn.tree.tests.test_tree.check_sparse_criterion(tree,dataset)
sklearn.tree.tests.test_tree.check_sparse_input(tree,dataset,max_depth=None)
sklearn.tree.tests.test_tree.check_sparse_parameters(tree,dataset)
sklearn.tree.tests.test_tree.test_1d_input()
sklearn.tree.tests.test_tree.test_arrayrepr()
sklearn.tree.tests.test_tree.test_arrays_persist()
sklearn.tree.tests.test_tree.test_behaviour_constant_feature_after_splits()
sklearn.tree.tests.test_tree.test_big_input()
sklearn.tree.tests.test_tree.test_boston()
sklearn.tree.tests.test_tree.test_class_weight_errors()
sklearn.tree.tests.test_tree.test_class_weights()
sklearn.tree.tests.test_tree.test_classes_shape()
sklearn.tree.tests.test_tree.test_classification_toy()
sklearn.tree.tests.test_tree.test_criterion_copy()
sklearn.tree.tests.test_tree.test_decision_path()
sklearn.tree.tests.test_tree.test_decision_path_hardcoded()
sklearn.tree.tests.test_tree.test_error()
sklearn.tree.tests.test_tree.test_explicit_sparse_zeros()
sklearn.tree.tests.test_tree.test_huge_allocations()
sklearn.tree.tests.test_tree.test_importances()
sklearn.tree.tests.test_tree.test_importances_gini_equal_mse()
sklearn.tree.tests.test_tree.test_importances_raises()
sklearn.tree.tests.test_tree.test_iris()
sklearn.tree.tests.test_tree.test_mae()
sklearn.tree.tests.test_tree.test_max_features()
sklearn.tree.tests.test_tree.test_max_leaf_nodes()
sklearn.tree.tests.test_tree.test_max_leaf_nodes_max_depth()
sklearn.tree.tests.test_tree.test_memory_layout()
sklearn.tree.tests.test_tree.test_min_impurity_decrease()
sklearn.tree.tests.test_tree.test_min_impurity_split()
sklearn.tree.tests.test_tree.test_min_samples_leaf()
sklearn.tree.tests.test_tree.test_min_samples_split()
sklearn.tree.tests.test_tree.test_min_weight_fraction_leaf()
sklearn.tree.tests.test_tree.test_min_weight_fraction_leaf_with_min_samples_leaf()
sklearn.tree.tests.test_tree.test_min_weight_leaf_split_level()
sklearn.tree.tests.test_tree.test_multioutput()
sklearn.tree.tests.test_tree.test_no_sparse_y_support()
sklearn.tree.tests.test_tree.test_numerical_stability()
sklearn.tree.tests.test_tree.test_only_constant_features()
sklearn.tree.tests.test_tree.test_presort_sparse()
sklearn.tree.tests.test_tree.test_probability()
sklearn.tree.tests.test_tree.test_public_apply()
sklearn.tree.tests.test_tree.test_pure_set()
sklearn.tree.tests.test_tree.test_realloc()
sklearn.tree.tests.test_tree.test_regression_toy()
sklearn.tree.tests.test_tree.test_sample_weight()
sklearn.tree.tests.test_tree.test_sample_weight_invalid()
sklearn.tree.tests.test_tree.test_sparse_criterion()
sklearn.tree.tests.test_tree.test_sparse_input()
sklearn.tree.tests.test_tree.test_sparse_parameters()
sklearn.tree.tests.test_tree.test_unbalanced_iris()
sklearn.tree.tests.test_tree.test_weighted_classification_toy()
sklearn.tree.tests.test_tree.test_with_only_one_non_constant_features()
sklearn.tree.tests.test_tree.test_xor()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/covariance/robust_covariance.py----------------------------------------
A:sklearn.covariance.robust_covariance.X->check_array(X, ensure_min_samples=2, estimator='MinCovDet')
A:sklearn.covariance.robust_covariance.random_state->check_random_state(self.random_state)
A:sklearn.covariance.robust_covariance.support->numpy.zeros(n_samples, dtype=bool)
A:sklearn.covariance.robust_covariance.precision->scipy.linalg.pinvh(raw_covariance)
A:sklearn.covariance.robust_covariance.dist->numpy.zeros(n_samples)
A:sklearn.covariance.robust_covariance.location->numpy.asarray([np.mean(X)])
A:sklearn.covariance.robust_covariance.covariance->numpy.asarray([[np.var(X)]])
A:sklearn.covariance.robust_covariance.det->fast_logdet(covariance)
A:sklearn.covariance.robust_covariance.(all_locs_sub, all_covs_sub, all_dets_sub, all_supports_sub, all_ds_sub)->zip(*all_estimates)
A:sklearn.covariance.robust_covariance.n_support->int(support_fraction * n_samples)
A:sklearn.covariance.robust_covariance.X_sorted->numpy.sort(np.ravel(X))
A:sklearn.covariance.robust_covariance.samples_shuffle->check_random_state(self.random_state).permutation(n_samples)
A:sklearn.covariance.robust_covariance.h_subset->int(np.ceil(n_samples_subsets * (n_support / float(n_samples))))
A:sklearn.covariance.robust_covariance.n_trials->max(10, n_trials_tot // n_subsets)
A:sklearn.covariance.robust_covariance.all_best_locations->numpy.zeros((n_best_tot, n_features))
A:sklearn.covariance.robust_covariance.all_best_covariances->numpy.zeros((n_best_tot, n_features, n_features))
A:sklearn.covariance.robust_covariance.(best_locations_sub, best_covariances_sub, _, _)->select_candidates(current_subset, h_subset, n_trials, select=n_best_sub, n_iter=2, cov_computation_method=cov_computation_method, random_state=random_state)
A:sklearn.covariance.robust_covariance.subset_slice->numpy.arange(i * n_best_sub, (i + 1) * n_best_sub)
A:sklearn.covariance.robust_covariance.n_samples_merged->min(1500, n_samples)
A:sklearn.covariance.robust_covariance.h_merged->int(np.ceil(n_samples_merged * (n_support / float(n_samples))))
A:sklearn.covariance.robust_covariance.(locations_merged, covariances_merged, supports_merged, d)->select_candidates(X[selection], h_merged, n_trials=(all_best_locations, all_best_covariances), select=n_best_merged, cov_computation_method=cov_computation_method, random_state=random_state)
A:sklearn.covariance.robust_covariance.(locations_full, covariances_full, supports_full, d)->select_candidates(X, n_support, n_trials=(locations_best, covariances_best), select=1, cov_computation_method=cov_computation_method, random_state=random_state)
A:sklearn.covariance.robust_covariance.(locations_best, covariances_best, _, _)->select_candidates(X, n_support, n_trials=n_trials, select=n_best, n_iter=2, cov_computation_method=cov_computation_method, random_state=random_state)
A:sklearn.covariance.robust_covariance._nonrobust_covariance->staticmethod(empirical_covariance)
A:sklearn.covariance.robust_covariance.(raw_location, raw_covariance, raw_support, raw_dist)->fast_mcd(X, support_fraction=self.support_fraction, cov_computation_method=self._nonrobust_covariance, random_state=random_state)
A:sklearn.covariance.robust_covariance.raw_location->numpy.zeros(n_features)
A:sklearn.covariance.robust_covariance.raw_covariance->self._nonrobust_covariance(X[raw_support], assume_centered=True)
A:sklearn.covariance.robust_covariance.raw_dist->numpy.sum(np.dot(X, precision) * X, 1)
A:sklearn.covariance.robust_covariance.location_reweighted->data[mask].mean(0)
A:sklearn.covariance.robust_covariance.covariance_reweighted->self._nonrobust_covariance(data[mask], assume_centered=self.assume_centered)
A:sklearn.covariance.robust_covariance.support_reweighted->numpy.zeros(n_samples, dtype=bool)
A:sklearn.covariance.robust_covariance.self.dist_->numpy.sum(np.dot(X_centered, self.get_precision()) * X_centered, 1)
sklearn.covariance.MinCovDet(self,store_precision=True,assume_centered=False,support_fraction=None,random_state=None)
sklearn.covariance.MinCovDet.correct_covariance(self,data)
sklearn.covariance.MinCovDet.fit(self,X,y=None)
sklearn.covariance.MinCovDet.reweight_covariance(self,data)
sklearn.covariance.fast_mcd(X,support_fraction=None,cov_computation_method=empirical_covariance,random_state=None)
sklearn.covariance.robust_covariance.MinCovDet(self,store_precision=True,assume_centered=False,support_fraction=None,random_state=None)
sklearn.covariance.robust_covariance.MinCovDet.__init__(self,store_precision=True,assume_centered=False,support_fraction=None,random_state=None)
sklearn.covariance.robust_covariance.MinCovDet.correct_covariance(self,data)
sklearn.covariance.robust_covariance.MinCovDet.fit(self,X,y=None)
sklearn.covariance.robust_covariance.MinCovDet.reweight_covariance(self,data)
sklearn.covariance.robust_covariance._c_step(X,n_support,random_state,remaining_iterations=30,initial_estimates=None,verbose=False,cov_computation_method=empirical_covariance)
sklearn.covariance.robust_covariance.c_step(X,n_support,remaining_iterations=30,initial_estimates=None,verbose=False,cov_computation_method=empirical_covariance,random_state=None)
sklearn.covariance.robust_covariance.fast_mcd(X,support_fraction=None,cov_computation_method=empirical_covariance,random_state=None)
sklearn.covariance.robust_covariance.select_candidates(X,n_support,n_trials,select=1,n_iter=30,verbose=False,cov_computation_method=empirical_covariance,random_state=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/covariance/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/covariance/shrunk_covariance_.py----------------------------------------
A:sklearn.covariance.shrunk_covariance_.emp_cov->empirical_covariance(X, assume_centered=assume_centered)
A:sklearn.covariance.shrunk_covariance_.X->check_array(X)
A:sklearn.covariance.shrunk_covariance_.self.location_->check_array(X).mean(0)
A:sklearn.covariance.shrunk_covariance_.covariance->shrunk_covariance(covariance, self.shrinkage)
A:sklearn.covariance.shrunk_covariance_.n_splits->int(n_features / block_size)
A:sklearn.covariance.shrunk_covariance_.rows->slice(block_size * i, block_size * (i + 1))
A:sklearn.covariance.shrunk_covariance_.cols->slice(block_size * j, block_size * (j + 1))
A:sklearn.covariance.shrunk_covariance_.beta->min(beta, delta)
A:sklearn.covariance.shrunk_covariance_.shrinkage->ledoit_wolf_shrinkage(X, assume_centered=assume_centered, block_size=block_size)
A:sklearn.covariance.shrunk_covariance_.(covariance, shrinkage)->oas(X - self.location_, assume_centered=True)
A:sklearn.covariance.shrunk_covariance_.alpha->numpy.mean(emp_cov ** 2)
sklearn.covariance.LedoitWolf(self,store_precision=True,assume_centered=False,block_size=1000)
sklearn.covariance.LedoitWolf.fit(self,X,y=None)
sklearn.covariance.OAS(EmpiricalCovariance)
sklearn.covariance.OAS.fit(self,X,y=None)
sklearn.covariance.ShrunkCovariance(self,store_precision=True,assume_centered=False,shrinkage=0.1)
sklearn.covariance.ShrunkCovariance.fit(self,X,y=None)
sklearn.covariance.ledoit_wolf(X,assume_centered=False,block_size=1000)
sklearn.covariance.ledoit_wolf_shrinkage(X,assume_centered=False,block_size=1000)
sklearn.covariance.oas(X,assume_centered=False)
sklearn.covariance.shrunk_covariance(emp_cov,shrinkage=0.1)
sklearn.covariance.shrunk_covariance_.LedoitWolf(self,store_precision=True,assume_centered=False,block_size=1000)
sklearn.covariance.shrunk_covariance_.LedoitWolf.__init__(self,store_precision=True,assume_centered=False,block_size=1000)
sklearn.covariance.shrunk_covariance_.LedoitWolf.fit(self,X,y=None)
sklearn.covariance.shrunk_covariance_.OAS(EmpiricalCovariance)
sklearn.covariance.shrunk_covariance_.OAS.fit(self,X,y=None)
sklearn.covariance.shrunk_covariance_.ShrunkCovariance(self,store_precision=True,assume_centered=False,shrinkage=0.1)
sklearn.covariance.shrunk_covariance_.ShrunkCovariance.__init__(self,store_precision=True,assume_centered=False,shrinkage=0.1)
sklearn.covariance.shrunk_covariance_.ShrunkCovariance.fit(self,X,y=None)
sklearn.covariance.shrunk_covariance_.ledoit_wolf(X,assume_centered=False,block_size=1000)
sklearn.covariance.shrunk_covariance_.ledoit_wolf_shrinkage(X,assume_centered=False,block_size=1000)
sklearn.covariance.shrunk_covariance_.oas(X,assume_centered=False)
sklearn.covariance.shrunk_covariance_.shrunk_covariance(emp_cov,shrinkage=0.1)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/covariance/outlier_detection.py----------------------------------------
A:sklearn.covariance.outlier_detection.self.threshold_->scipy.stats.scoreatpercentile(self.dist_, 100.0 * (1.0 - self.contamination))
A:sklearn.covariance.outlier_detection.X->check_array(X)
A:sklearn.covariance.outlier_detection.mahal_dist->self.mahalanobis(X)
A:sklearn.covariance.outlier_detection.values->self.decision_function(X, raw_values=True)
sklearn.covariance.EllipticEnvelope(self,store_precision=True,assume_centered=False,support_fraction=None,contamination=0.1,random_state=None)
sklearn.covariance.EllipticEnvelope.decision_function(self,X,raw_values=False)
sklearn.covariance.EllipticEnvelope.fit(self,X,y=None)
sklearn.covariance.EllipticEnvelope.predict(self,X)
sklearn.covariance.EllipticEnvelope.score(self,X,y,sample_weight=None)
sklearn.covariance.outlier_detection.EllipticEnvelope(self,store_precision=True,assume_centered=False,support_fraction=None,contamination=0.1,random_state=None)
sklearn.covariance.outlier_detection.EllipticEnvelope.__init__(self,store_precision=True,assume_centered=False,support_fraction=None,contamination=0.1,random_state=None)
sklearn.covariance.outlier_detection.EllipticEnvelope.decision_function(self,X,raw_values=False)
sklearn.covariance.outlier_detection.EllipticEnvelope.fit(self,X,y=None)
sklearn.covariance.outlier_detection.EllipticEnvelope.predict(self,X)
sklearn.covariance.outlier_detection.EllipticEnvelope.score(self,X,y,sample_weight=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/covariance/graph_lasso_.py----------------------------------------
A:sklearn.covariance.graph_lasso_.gap->numpy.sum(emp_cov * precision_)
A:sklearn.covariance.graph_lasso_.A->numpy.copy(emp_cov)
A:sklearn.covariance.graph_lasso_.precision_->scipy.linalg.pinvh(covariance_)
A:sklearn.covariance.graph_lasso_.covariance_->empirical_covariance(X, assume_centered=self.assume_centered).copy()
A:sklearn.covariance.graph_lasso_.indices->numpy.arange(n_features)
A:sklearn.covariance.graph_lasso_.costs->list()
A:sklearn.covariance.graph_lasso_.errors->dict(invalid='raise')
A:sklearn.covariance.graph_lasso_.sub_covariance->numpy.ascontiguousarray(covariance_[indices != idx].T[indices != idx])
A:sklearn.covariance.graph_lasso_.(coefs, _, _, _)->linear_model.cd_fast.enet_coordinate_descent_gram(coefs, alpha, 0, sub_covariance, row, row, max_iter, enet_tol, check_random_state(None), False)
A:sklearn.covariance.graph_lasso_.(_, _, coefs)->lars_path(sub_covariance, row, Xy=row, Gram=sub_covariance, alpha_min=alpha / (n_features - 1), copy_Gram=True, eps=eps, method='lars', return_path=False)
A:sklearn.covariance.graph_lasso_.coefs->numpy.dot(sub_covariance, coefs)
A:sklearn.covariance.graph_lasso_.d_gap->_dual_gap(emp_cov, precision_, alpha)
A:sklearn.covariance.graph_lasso_.cost->_objective(emp_cov, precision_, alpha)
A:sklearn.covariance.graph_lasso_.X->check_array(X, ensure_min_features=2, estimator=self)
A:sklearn.covariance.graph_lasso_.self.location_->check_array(X, ensure_min_features=2, estimator=self).mean(0)
A:sklearn.covariance.graph_lasso_.emp_cov->empirical_covariance(X, assume_centered=self.assume_centered)
A:sklearn.covariance.graph_lasso_.(self.covariance_, self.precision_, self.n_iter_)->graph_lasso(emp_cov, alpha=best_alpha, mode=self.mode, tol=self.tol, enet_tol=self.enet_tol, max_iter=self.max_iter, verbose=inner_verbose, return_n_iter=True)
A:sklearn.covariance.graph_lasso_.inner_verbose->max(0, self.verbose - 1)
A:sklearn.covariance.graph_lasso_.covariances_->list()
A:sklearn.covariance.graph_lasso_.precisions_->list()
A:sklearn.covariance.graph_lasso_.scores_->list()
A:sklearn.covariance.graph_lasso_.test_emp_cov->empirical_covariance(X_test)
A:sklearn.covariance.graph_lasso_.(covariance_, precision_)->graph_lasso(emp_cov, alpha=alpha, cov_init=covariance_, mode=mode, tol=tol, enet_tol=enet_tol, max_iter=max_iter, verbose=inner_verbose)
A:sklearn.covariance.graph_lasso_.this_score->numpy.mean(scores)
A:sklearn.covariance.graph_lasso_.cv->check_cv(self.cv, y, classifier=False)
A:sklearn.covariance.graph_lasso_.path->list(zip(*path))
A:sklearn.covariance.graph_lasso_.alpha_1->alpha_max(emp_cov)
A:sklearn.covariance.graph_lasso_.t0->time.time()
A:sklearn.covariance.graph_lasso_.this_path->Parallel(n_jobs=self.n_jobs, verbose=self.verbose)((delayed(graph_lasso_path)(X[train], alphas=alphas, X_test=X[test], mode=self.mode, tol=self.tol, enet_tol=self.enet_tol, max_iter=int(0.1 * self.max_iter), verbose=inner_verbose) for (train, test) in cv.split(X, y)))
A:sklearn.covariance.graph_lasso_.(covs, _, scores)->zip(*this_path)
A:sklearn.covariance.graph_lasso_.covs->zip(*covs)
A:sklearn.covariance.graph_lasso_.scores->zip(*scores)
A:sklearn.covariance.graph_lasso_.alphas->list(path[0])
A:sklearn.covariance.graph_lasso_.grid_scores->list(path[1])
A:sklearn.covariance.graph_lasso_.self.grid_scores_->numpy.array(grid_scores)
sklearn.covariance.GraphLasso(self,alpha=0.01,mode='cd',tol=0.0001,enet_tol=0.0001,max_iter=100,verbose=False,assume_centered=False)
sklearn.covariance.GraphLasso.fit(self,X,y=None)
sklearn.covariance.GraphLassoCV(self,alphas=4,n_refinements=4,cv=None,tol=0.0001,enet_tol=0.0001,max_iter=100,mode='cd',n_jobs=1,verbose=False,assume_centered=False)
sklearn.covariance.GraphLassoCV.fit(self,X,y=None)
sklearn.covariance.GraphLassoCV.grid_scores(self)
sklearn.covariance.graph_lasso(emp_cov,alpha,cov_init=None,mode='cd',tol=0.0001,enet_tol=0.0001,max_iter=100,verbose=False,return_costs=False,eps=np.finfo(np.float64).eps,return_n_iter=False)
sklearn.covariance.graph_lasso_.GraphLasso(self,alpha=0.01,mode='cd',tol=0.0001,enet_tol=0.0001,max_iter=100,verbose=False,assume_centered=False)
sklearn.covariance.graph_lasso_.GraphLasso.__init__(self,alpha=0.01,mode='cd',tol=0.0001,enet_tol=0.0001,max_iter=100,verbose=False,assume_centered=False)
sklearn.covariance.graph_lasso_.GraphLasso.fit(self,X,y=None)
sklearn.covariance.graph_lasso_.GraphLassoCV(self,alphas=4,n_refinements=4,cv=None,tol=0.0001,enet_tol=0.0001,max_iter=100,mode='cd',n_jobs=1,verbose=False,assume_centered=False)
sklearn.covariance.graph_lasso_.GraphLassoCV.__init__(self,alphas=4,n_refinements=4,cv=None,tol=0.0001,enet_tol=0.0001,max_iter=100,mode='cd',n_jobs=1,verbose=False,assume_centered=False)
sklearn.covariance.graph_lasso_.GraphLassoCV.fit(self,X,y=None)
sklearn.covariance.graph_lasso_.GraphLassoCV.grid_scores(self)
sklearn.covariance.graph_lasso_._dual_gap(emp_cov,precision_,alpha)
sklearn.covariance.graph_lasso_._objective(mle,precision_,alpha)
sklearn.covariance.graph_lasso_.alpha_max(emp_cov)
sklearn.covariance.graph_lasso_.graph_lasso(emp_cov,alpha,cov_init=None,mode='cd',tol=0.0001,enet_tol=0.0001,max_iter=100,verbose=False,return_costs=False,eps=np.finfo(np.float64).eps,return_n_iter=False)
sklearn.covariance.graph_lasso_.graph_lasso_path(X,alphas,cov_init=None,X_test=None,mode='cd',tol=0.0001,enet_tol=0.0001,max_iter=100,verbose=False)
sklearn.covariance.graph_lasso_path(X,alphas,cov_init=None,X_test=None,mode='cd',tol=0.0001,enet_tol=0.0001,max_iter=100,verbose=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/covariance/empirical_covariance_.py----------------------------------------
A:sklearn.covariance.empirical_covariance_.X->check_array(X)
A:sklearn.covariance.empirical_covariance_.covariance->empirical_covariance(X, assume_centered=self.assume_centered)
A:sklearn.covariance.empirical_covariance_.self.precision_->scipy.linalg.pinvh(covariance)
A:sklearn.covariance.empirical_covariance_.precision->self.get_precision()
A:sklearn.covariance.empirical_covariance_.self.location_->check_array(X).mean(0)
A:sklearn.covariance.empirical_covariance_.test_cov->empirical_covariance(X_test - self.location_, assume_centered=True)
A:sklearn.covariance.empirical_covariance_.res->log_likelihood(test_cov, self.get_precision())
A:sklearn.covariance.empirical_covariance_.squared_norm->numpy.amax(linalg.svdvals(np.dot(error.T, error)))
A:sklearn.covariance.empirical_covariance_.result->numpy.sqrt(squared_norm)
A:sklearn.covariance.empirical_covariance_.mahalanobis_dist->numpy.sum(np.dot(centered_obs, precision) * centered_obs, 1)
sklearn.covariance.EmpiricalCovariance(self,store_precision=True,assume_centered=False)
sklearn.covariance.EmpiricalCovariance._set_covariance(self,covariance)
sklearn.covariance.EmpiricalCovariance.error_norm(self,comp_cov,norm='frobenius',scaling=True,squared=True)
sklearn.covariance.EmpiricalCovariance.fit(self,X,y=None)
sklearn.covariance.EmpiricalCovariance.get_precision(self)
sklearn.covariance.EmpiricalCovariance.mahalanobis(self,observations)
sklearn.covariance.EmpiricalCovariance.score(self,X_test,y=None)
sklearn.covariance.empirical_covariance(X,assume_centered=False)
sklearn.covariance.empirical_covariance_.EmpiricalCovariance(self,store_precision=True,assume_centered=False)
sklearn.covariance.empirical_covariance_.EmpiricalCovariance.__init__(self,store_precision=True,assume_centered=False)
sklearn.covariance.empirical_covariance_.EmpiricalCovariance._set_covariance(self,covariance)
sklearn.covariance.empirical_covariance_.EmpiricalCovariance.error_norm(self,comp_cov,norm='frobenius',scaling=True,squared=True)
sklearn.covariance.empirical_covariance_.EmpiricalCovariance.fit(self,X,y=None)
sklearn.covariance.empirical_covariance_.EmpiricalCovariance.get_precision(self)
sklearn.covariance.empirical_covariance_.EmpiricalCovariance.mahalanobis(self,observations)
sklearn.covariance.empirical_covariance_.EmpiricalCovariance.score(self,X_test,y=None)
sklearn.covariance.empirical_covariance_.empirical_covariance(X,assume_centered=False)
sklearn.covariance.empirical_covariance_.log_likelihood(emp_cov,precision)
sklearn.covariance.log_likelihood(emp_cov,precision)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/covariance/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/covariance/tests/test_robust_covariance.py----------------------------------------
A:sklearn.covariance.tests.test_robust_covariance.X->numpy.random.RandomState(0).randn(100, 10)
A:sklearn.covariance.tests.test_robust_covariance.mcd->MinCovDet()
A:sklearn.covariance.tests.test_robust_covariance.rand_gen->numpy.random.RandomState(0)
A:sklearn.covariance.tests.test_robust_covariance.data->numpy.hstack((data, np.zeros((data.shape[0], 1))))
A:sklearn.covariance.tests.test_robust_covariance.inliers_mask->numpy.ones(n_samples).astype(bool)
A:sklearn.covariance.tests.test_robust_covariance.mcd_fit->MinCovDet(random_state=rand_gen).fit(data)
A:sklearn.covariance.tests.test_robust_covariance.error_location->numpy.mean((pure_data.mean(0) - T) ** 2)
A:sklearn.covariance.tests.test_robust_covariance.error_cov->numpy.mean((empirical_covariance(pure_data) - S) ** 2)
A:sklearn.covariance.tests.test_robust_covariance.rnd->numpy.random.RandomState(0)
A:sklearn.covariance.tests.test_robust_covariance.data_values->numpy.linspace(-5, 5, 10).tolist()
A:sklearn.covariance.tests.test_robust_covariance.clf->EllipticEnvelope(contamination=0.1)
A:sklearn.covariance.tests.test_robust_covariance.y_pred->EllipticEnvelope(contamination=0.1).predict(X)
A:sklearn.covariance.tests.test_robust_covariance.decision->EllipticEnvelope(contamination=0.1).decision_function(X, raw_values=True)
A:sklearn.covariance.tests.test_robust_covariance.decision_transformed->EllipticEnvelope(contamination=0.1).decision_function(X, raw_values=False)
sklearn.covariance.tests.test_robust_covariance.launch_mcd_on_dataset(n_samples,n_features,n_outliers,tol_loc,tol_cov,tol_support)
sklearn.covariance.tests.test_robust_covariance.test_fast_mcd_on_invalid_input()
sklearn.covariance.tests.test_robust_covariance.test_mcd()
sklearn.covariance.tests.test_robust_covariance.test_mcd_class_on_invalid_input()
sklearn.covariance.tests.test_robust_covariance.test_mcd_issue1127()
sklearn.covariance.tests.test_robust_covariance.test_mcd_issue3367()
sklearn.covariance.tests.test_robust_covariance.test_outlier_detection()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/covariance/tests/test_covariance.py----------------------------------------
A:sklearn.covariance.tests.test_covariance.cov->ShrunkCovariance(shrinkage=0.5, store_precision=False)
A:sklearn.covariance.tests.test_covariance.emp_cov->empirical_covariance(X, assume_centered=False)
A:sklearn.covariance.tests.test_covariance.mahal_dist->ShrunkCovariance(shrinkage=0.5, store_precision=False).mahalanobis(X)
A:sklearn.covariance.tests.test_covariance.X_1d->X[:, 0].reshape((-1, 1))
A:sklearn.covariance.tests.test_covariance.X_1sample->numpy.arange(5).reshape(1, 5)
A:sklearn.covariance.tests.test_covariance.X_integer->numpy.asarray([[0, 1], [1, 0]])
A:sklearn.covariance.tests.test_covariance.result->numpy.asarray([[0.25, -0.25], [-0.25, 0.25]])
A:sklearn.covariance.tests.test_covariance.lw->LedoitWolf(block_size=25).fit(X)
A:sklearn.covariance.tests.test_covariance.score_->OAS(store_precision=False).score(X_centered)
A:sklearn.covariance.tests.test_covariance.(lw_cov_from_mle, lw_shrinkage_from_mle)->ledoit_wolf(X_1d)
A:sklearn.covariance.tests.test_covariance.scov->ShrunkCovariance(shrinkage=oa.shrinkage_)
A:sklearn.covariance.tests.test_covariance.delta_->empirical_covariance(X, assume_centered=False).copy()
A:sklearn.covariance.tests.test_covariance.beta->min(beta_, delta)
A:sklearn.covariance.tests.test_covariance.rng->numpy.random.RandomState(0)
A:sklearn.covariance.tests.test_covariance.X->numpy.random.RandomState(0).normal(size=(10, 20))
A:sklearn.covariance.tests.test_covariance.oa->OAS(store_precision=False)
A:sklearn.covariance.tests.test_covariance.(oa_cov_from_mle, oa_shrinkage_from_mle)->oas(X_1d)
sklearn.covariance.tests.test_covariance._naive_ledoit_wolf_shrinkage(X)
sklearn.covariance.tests.test_covariance.test_covariance()
sklearn.covariance.tests.test_covariance.test_ledoit_wolf()
sklearn.covariance.tests.test_covariance.test_ledoit_wolf_large()
sklearn.covariance.tests.test_covariance.test_ledoit_wolf_small()
sklearn.covariance.tests.test_covariance.test_oas()
sklearn.covariance.tests.test_covariance.test_shrunk_covariance()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/covariance/tests/test_graph_lasso.py----------------------------------------
A:sklearn.covariance.tests.test_graph_lasso.random_state->check_random_state(random_state)
A:sklearn.covariance.tests.test_graph_lasso.prec->make_sparse_spd_matrix(dim, alpha=0.96, random_state=random_state)
A:sklearn.covariance.tests.test_graph_lasso.cov->scipy.linalg.inv(prec)
A:sklearn.covariance.tests.test_graph_lasso.X->check_random_state(random_state).multivariate_normal(np.zeros(dim), cov, size=n_samples)
A:sklearn.covariance.tests.test_graph_lasso.emp_cov->empirical_covariance(X)
A:sklearn.covariance.tests.test_graph_lasso.covs->dict()
A:sklearn.covariance.tests.test_graph_lasso.icovs->dict()
A:sklearn.covariance.tests.test_graph_lasso.(cov_, icov_, costs)->graph_lasso(emp_cov, alpha=alpha, mode=method, return_costs=True)
A:sklearn.covariance.tests.test_graph_lasso.model->GraphLasso(alpha=0.25).fit(X)
A:sklearn.covariance.tests.test_graph_lasso.precs->list()
A:sklearn.covariance.tests.test_graph_lasso.cov_R->numpy.array([[0.08, 0.056666662595, 0.00229729713223, 0.00153153142149], [0.056666662595, 0.082222222222, 0.00333333333333, 0.00222222222222], [0.002297297132, 0.003333333333, 0.00666666666667, 9.009009009e-05], [0.001531531421, 0.002222222222, 9.009009009e-05, 0.00222222222222]])
A:sklearn.covariance.tests.test_graph_lasso.icov_R->numpy.array([[24.42244057, -16.831679593, 0.0, 0.0], [-16.83168201, 24.351841681, -6.206896552, -12.5], [0.0, -6.206896171, 153.103448276, 0.0], [0.0, -12.499999143, 0.0, 462.5]])
A:sklearn.covariance.tests.test_graph_lasso.(cov, icov)->graph_lasso(emp_cov, alpha=0.01, return_costs=False, mode=method)
A:sklearn.covariance.tests.test_graph_lasso.indices->numpy.arange(10, 13)
A:sklearn.covariance.tests.test_graph_lasso.sys.stdout->StringIO()
A:sklearn.covariance.tests.test_graph_lasso.graph_lasso->GraphLassoCV(alphas=[0.8, 0.5], tol=0.1, n_jobs=1)
sklearn.covariance.tests.test_graph_lasso.test_deprecated_grid_scores(random_state=1)
sklearn.covariance.tests.test_graph_lasso.test_graph_lasso(random_state=0)
sklearn.covariance.tests.test_graph_lasso.test_graph_lasso_cv(random_state=1)
sklearn.covariance.tests.test_graph_lasso.test_graph_lasso_iris()
sklearn.covariance.tests.test_graph_lasso.test_graph_lasso_iris_singular()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/feature_extraction/image.py----------------------------------------
A:sklearn.feature_extraction.image.vertices->numpy.arange(n_x * n_y * n_z).reshape((n_x, n_y, n_z))
A:sklearn.feature_extraction.image.edges_deep->numpy.vstack((vertices[:, :, :-1].ravel(), vertices[:, :, 1:].ravel()))
A:sklearn.feature_extraction.image.edges_right->numpy.vstack((vertices[:, :-1].ravel(), vertices[:, 1:].ravel()))
A:sklearn.feature_extraction.image.edges_down->numpy.vstack((vertices[:-1].ravel(), vertices[1:].ravel()))
A:sklearn.feature_extraction.image.edges->_mask_edges_weights(mask, edges)
A:sklearn.feature_extraction.image.gradient->numpy.abs(img[edges[0] // (n_y * n_z), edges[0] % (n_y * n_z) // n_z, edges[0] % (n_y * n_z) % n_z] - img[edges[1] // (n_y * n_z), edges[1] % (n_y * n_z) // n_z, edges[1] % (n_y * n_z) % n_z])
A:sklearn.feature_extraction.image.inds->numpy.arange(mask.size)
A:sklearn.feature_extraction.image.ind_mask->numpy.logical_and(np.in1d(edges[0], inds), np.in1d(edges[1], inds))
A:sklearn.feature_extraction.image.maxval->_mask_edges_weights(mask, edges).max()
A:sklearn.feature_extraction.image.order->numpy.searchsorted(np.unique(edges.ravel()), np.arange(maxval + 1))
A:sklearn.feature_extraction.image.img->numpy.zeros(image_size)
A:sklearn.feature_extraction.image.weights->numpy.ones(edges.shape[1], dtype=dtype)
A:sklearn.feature_extraction.image.(edges, weights)->_mask_edges_weights(mask, edges, weights)
A:sklearn.feature_extraction.image.diag->numpy.ones(n_voxels, dtype=dtype)
A:sklearn.feature_extraction.image.mask->numpy.asarray(mask, dtype=np.bool)
A:sklearn.feature_extraction.image.n_voxels->numpy.sum(mask)
A:sklearn.feature_extraction.image.diag_idx->numpy.arange(n_voxels)
A:sklearn.feature_extraction.image.i_idx->numpy.hstack((edges[0], edges[1]))
A:sklearn.feature_extraction.image.j_idx->numpy.hstack((edges[1], edges[0]))
A:sklearn.feature_extraction.image.graph->scipy.sparse.coo_matrix((np.hstack((weights, weights, diag)), (np.hstack((i_idx, diag_idx)), np.hstack((j_idx, diag_idx)))), (n_voxels, n_voxels), dtype=dtype)
A:sklearn.feature_extraction.image.patch_shape->tuple([patch_shape] * arr_ndim)
A:sklearn.feature_extraction.image.extraction_step->tuple([extraction_step] * arr_ndim)
A:sklearn.feature_extraction.image.shape->tuple(list(patch_indices_shape) + list(patch_shape))
A:sklearn.feature_extraction.image.strides->tuple(list(indexing_strides) + list(patch_strides))
A:sklearn.feature_extraction.image.patches->numpy.empty(patches_shape)
A:sklearn.feature_extraction.image.image->image.reshape((i_h, i_w, -1)).reshape((i_h, i_w, -1))
A:sklearn.feature_extraction.image.extracted_patches->extract_patches(image, patch_shape=(p_h, p_w, n_colors), extraction_step=1)
A:sklearn.feature_extraction.image.n_patches->_compute_n_patches(i_h, i_w, p_h, p_w, self.max_patches)
A:sklearn.feature_extraction.image.rng->check_random_state(random_state)
A:sklearn.feature_extraction.image.i_s->check_random_state(random_state).randint(i_h - p_h + 1, size=n_patches)
A:sklearn.feature_extraction.image.j_s->check_random_state(random_state).randint(i_w - p_w + 1, size=n_patches)
A:sklearn.feature_extraction.image.self.random_state->check_random_state(self.random_state)
A:sklearn.feature_extraction.image.X->numpy.reshape(X, (n_images, i_h, i_w, -1))
A:sklearn.feature_extraction.image.patches[ii * n_patches:(ii + 1) * n_patches]->extract_patches_2d(image, patch_size, self.max_patches, self.random_state)
sklearn.feature_extraction.grid_to_graph(n_x,n_y,n_z=1,mask=None,return_as=sparse.coo_matrix,dtype=np.int)
sklearn.feature_extraction.image.PatchExtractor(self,patch_size=None,max_patches=None,random_state=None)
sklearn.feature_extraction.image.PatchExtractor.__init__(self,patch_size=None,max_patches=None,random_state=None)
sklearn.feature_extraction.image.PatchExtractor.fit(self,X,y=None)
sklearn.feature_extraction.image.PatchExtractor.transform(self,X)
sklearn.feature_extraction.image._compute_gradient_3d(edges,img)
sklearn.feature_extraction.image._compute_n_patches(i_h,i_w,p_h,p_w,max_patches=None)
sklearn.feature_extraction.image._make_edges_3d(n_x,n_y,n_z=1)
sklearn.feature_extraction.image._mask_edges_weights(mask,edges,weights=None)
sklearn.feature_extraction.image._to_graph(n_x,n_y,n_z,mask=None,img=None,return_as=sparse.coo_matrix,dtype=None)
sklearn.feature_extraction.image.extract_patches(arr,patch_shape=8,extraction_step=1)
sklearn.feature_extraction.image.extract_patches_2d(image,patch_size,max_patches=None,random_state=None)
sklearn.feature_extraction.image.grid_to_graph(n_x,n_y,n_z=1,mask=None,return_as=sparse.coo_matrix,dtype=np.int)
sklearn.feature_extraction.image.img_to_graph(img,mask=None,return_as=sparse.coo_matrix,dtype=None)
sklearn.feature_extraction.image.reconstruct_from_patches_2d(patches,image_size)
sklearn.feature_extraction.img_to_graph(img,mask=None,return_as=sparse.coo_matrix,dtype=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/feature_extraction/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/feature_extraction/dict_vectorizer.py----------------------------------------
A:sklearn.feature_extraction.dict_vectorizer.vocab[f]->len(vocab)
A:sklearn.feature_extraction.dict_vectorizer.vocab->dict(((f, i) for (i, f) in enumerate(feature_names)))
A:sklearn.feature_extraction.dict_vectorizer.indices->numpy.frombuffer(indices, dtype=np.intc)
A:sklearn.feature_extraction.dict_vectorizer.indptr->numpy.frombuffer(indptr, dtype=np.intc)
A:sklearn.feature_extraction.dict_vectorizer.result_matrix->result_matrix.toarray().toarray()
A:sklearn.feature_extraction.dict_vectorizer.map_index->numpy.empty(len(feature_names), dtype=np.int32)
A:sklearn.feature_extraction.dict_vectorizer.X->_tosequence(X)
A:sklearn.feature_extraction.dict_vectorizer.Xa->numpy.zeros((len(X), len(vocab)), dtype=dtype)
A:sklearn.feature_extraction.dict_vectorizer.Xa[i, vocab[f]]->dtype(v)
A:sklearn.feature_extraction.dict_vectorizer.new_vocab[names[i]]->len(new_vocab)
sklearn.feature_extraction.DictVectorizer(self,dtype=np.float64,separator='=',sparse=True,sort=True)
sklearn.feature_extraction.DictVectorizer._transform(self,X,fitting)
sklearn.feature_extraction.DictVectorizer.fit(self,X,y=None)
sklearn.feature_extraction.DictVectorizer.fit_transform(self,X,y=None)
sklearn.feature_extraction.DictVectorizer.get_feature_names(self)
sklearn.feature_extraction.DictVectorizer.inverse_transform(self,X,dict_type=dict)
sklearn.feature_extraction.DictVectorizer.restrict(self,support,indices=False)
sklearn.feature_extraction.DictVectorizer.transform(self,X)
sklearn.feature_extraction.dict_vectorizer.DictVectorizer(self,dtype=np.float64,separator='=',sparse=True,sort=True)
sklearn.feature_extraction.dict_vectorizer.DictVectorizer.__init__(self,dtype=np.float64,separator='=',sparse=True,sort=True)
sklearn.feature_extraction.dict_vectorizer.DictVectorizer._transform(self,X,fitting)
sklearn.feature_extraction.dict_vectorizer.DictVectorizer.fit(self,X,y=None)
sklearn.feature_extraction.dict_vectorizer.DictVectorizer.fit_transform(self,X,y=None)
sklearn.feature_extraction.dict_vectorizer.DictVectorizer.get_feature_names(self)
sklearn.feature_extraction.dict_vectorizer.DictVectorizer.inverse_transform(self,X,dict_type=dict)
sklearn.feature_extraction.dict_vectorizer.DictVectorizer.restrict(self,support,indices=False)
sklearn.feature_extraction.dict_vectorizer.DictVectorizer.transform(self,X)
sklearn.feature_extraction.dict_vectorizer._tosequence(X)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/feature_extraction/text.py----------------------------------------
A:sklearn.feature_extraction.text.normalized->unicodedata.normalize('NFKD', s)
A:sklearn.feature_extraction.text.nkfd_form->unicodedata.normalize('NFKD', s)
A:sklearn.feature_extraction.text._white_spaces->re.compile('\\s\\s+')
A:sklearn.feature_extraction.text.doc->doc.decode(self.encoding, self.decode_error).decode(self.encoding, self.decode_error)
A:sklearn.feature_extraction.text.tokens->list(original_tokens)
A:sklearn.feature_extraction.text.n_original_tokens->len(original_tokens)
A:sklearn.feature_extraction.text.text_document->self._white_spaces.sub(' ', text_document)
A:sklearn.feature_extraction.text.text_len->len(text_document)
A:sklearn.feature_extraction.text.ngrams->list(text_document)
A:sklearn.feature_extraction.text.w_len->len(w)
A:sklearn.feature_extraction.text.token_pattern->re.compile(self.token_pattern)
A:sklearn.feature_extraction.text.preprocess->self.build_preprocessor()
A:sklearn.feature_extraction.text.stop_words->self.get_stop_words()
A:sklearn.feature_extraction.text.tokenize->self.build_tokenizer()
A:sklearn.feature_extraction.text.vocabulary->dict(vocabulary)
A:sklearn.feature_extraction.text.indices->numpy.array(list(self.vocabulary_.values()))
A:sklearn.feature_extraction.text.self.vocabulary_->dict(vocabulary)
A:sklearn.feature_extraction.text.analyzer->self.build_analyzer()
A:sklearn.feature_extraction.text.X->super(TfidfVectorizer, self).transform(raw_documents)
A:sklearn.feature_extraction.text.sorted_features->sorted(six.iteritems(vocabulary))
A:sklearn.feature_extraction.text.map_index->numpy.empty(len(sorted_features), dtype=np.int32)
A:sklearn.feature_extraction.text.X.indices->numpy.empty(len(sorted_features), dtype=np.int32).take(X.indices, mode='clip')
A:sklearn.feature_extraction.text.dfs->_document_frequency(X)
A:sklearn.feature_extraction.text.tfs->numpy.asarray(X.sum(axis=0)).ravel()
A:sklearn.feature_extraction.text.mask->numpy.ones(len(dfs), dtype=bool)
A:sklearn.feature_extraction.text.new_mask->numpy.zeros(len(dfs), dtype=bool)
A:sklearn.feature_extraction.text.removed_terms->set()
A:sklearn.feature_extraction.text.analyze->self.build_analyzer()
A:sklearn.feature_extraction.text.indptr->numpy.frombuffer(indptr, dtype=np.intc)
A:sklearn.feature_extraction.text.values->numpy.frombuffer(values, dtype=np.intc)
A:sklearn.feature_extraction.text.j_indices->numpy.asarray(j_indices, dtype=np.intc)
A:sklearn.feature_extraction.text.(vocabulary, X)->self._count_vocab(raw_documents, self.fixed_vocabulary_)
A:sklearn.feature_extraction.text.(X, self.stop_words_)->self._limit_features(X, vocabulary, max_doc_count, min_doc_count, max_features)
A:sklearn.feature_extraction.text.(_, X)->self._count_vocab(raw_documents, fixed_vocab=True)
A:sklearn.feature_extraction.text.terms->numpy.array(list(self.vocabulary_.keys()))
A:sklearn.feature_extraction.text.df->_document_frequency(X)
A:sklearn.feature_extraction.text.self._idf_diag->scipy.sparse.spdiags(idf, diags=0, m=n_features, n=n_features, format='csr')
A:sklearn.feature_extraction.text.self._tfidf->TfidfTransformer(norm=norm, use_idf=use_idf, smooth_idf=smooth_idf, sublinear_tf=sublinear_tf)
sklearn.feature_extraction.text.CountVectorizer(self,input='content',encoding='utf-8',decode_error='strict',strip_accents=None,lowercase=True,preprocessor=None,tokenizer=None,stop_words=None,token_pattern='(?u)\\b\\w\\w+\\b',ngram_range=(1,1),analyzer='word',max_df=1.0,min_df=1,max_features=None,vocabulary=None,binary=False,dtype=np.int64)
sklearn.feature_extraction.text.CountVectorizer.__init__(self,input='content',encoding='utf-8',decode_error='strict',strip_accents=None,lowercase=True,preprocessor=None,tokenizer=None,stop_words=None,token_pattern='(?u)\\b\\w\\w+\\b',ngram_range=(1,1),analyzer='word',max_df=1.0,min_df=1,max_features=None,vocabulary=None,binary=False,dtype=np.int64)
sklearn.feature_extraction.text.CountVectorizer._count_vocab(self,raw_documents,fixed_vocab)
sklearn.feature_extraction.text.CountVectorizer._limit_features(self,X,vocabulary,high=None,low=None,limit=None)
sklearn.feature_extraction.text.CountVectorizer._sort_features(self,X,vocabulary)
sklearn.feature_extraction.text.CountVectorizer.fit(self,raw_documents,y=None)
sklearn.feature_extraction.text.CountVectorizer.fit_transform(self,raw_documents,y=None)
sklearn.feature_extraction.text.CountVectorizer.get_feature_names(self)
sklearn.feature_extraction.text.CountVectorizer.inverse_transform(self,X)
sklearn.feature_extraction.text.CountVectorizer.transform(self,raw_documents)
sklearn.feature_extraction.text.HashingVectorizer(self,input='content',encoding='utf-8',decode_error='strict',strip_accents=None,lowercase=True,preprocessor=None,tokenizer=None,stop_words=None,token_pattern='(?u)\\b\\w\\w+\\b',ngram_range=(1,1),analyzer='word',n_features=2**20,binary=False,norm='l2',alternate_sign=True,non_negative=False,dtype=np.float64)
sklearn.feature_extraction.text.HashingVectorizer.__init__(self,input='content',encoding='utf-8',decode_error='strict',strip_accents=None,lowercase=True,preprocessor=None,tokenizer=None,stop_words=None,token_pattern='(?u)\\b\\w\\w+\\b',ngram_range=(1,1),analyzer='word',n_features=2**20,binary=False,norm='l2',alternate_sign=True,non_negative=False,dtype=np.float64)
sklearn.feature_extraction.text.HashingVectorizer._get_hasher(self)
sklearn.feature_extraction.text.HashingVectorizer.fit(self,X,y=None)
sklearn.feature_extraction.text.HashingVectorizer.partial_fit(self,X,y=None)
sklearn.feature_extraction.text.HashingVectorizer.transform(self,X)
sklearn.feature_extraction.text.TfidfTransformer(self,norm='l2',use_idf=True,smooth_idf=True,sublinear_tf=False)
sklearn.feature_extraction.text.TfidfTransformer.__init__(self,norm='l2',use_idf=True,smooth_idf=True,sublinear_tf=False)
sklearn.feature_extraction.text.TfidfTransformer.fit(self,X,y=None)
sklearn.feature_extraction.text.TfidfTransformer.idf_(self)
sklearn.feature_extraction.text.TfidfTransformer.transform(self,X,copy=True)
sklearn.feature_extraction.text.TfidfVectorizer(self,input='content',encoding='utf-8',decode_error='strict',strip_accents=None,lowercase=True,preprocessor=None,tokenizer=None,analyzer='word',stop_words=None,token_pattern='(?u)\\b\\w\\w+\\b',ngram_range=(1,1),max_df=1.0,min_df=1,max_features=None,vocabulary=None,binary=False,dtype=np.int64,norm='l2',use_idf=True,smooth_idf=True,sublinear_tf=False)
sklearn.feature_extraction.text.TfidfVectorizer.__init__(self,input='content',encoding='utf-8',decode_error='strict',strip_accents=None,lowercase=True,preprocessor=None,tokenizer=None,analyzer='word',stop_words=None,token_pattern='(?u)\\b\\w\\w+\\b',ngram_range=(1,1),max_df=1.0,min_df=1,max_features=None,vocabulary=None,binary=False,dtype=np.int64,norm='l2',use_idf=True,smooth_idf=True,sublinear_tf=False)
sklearn.feature_extraction.text.TfidfVectorizer.fit(self,raw_documents,y=None)
sklearn.feature_extraction.text.TfidfVectorizer.fit_transform(self,raw_documents,y=None)
sklearn.feature_extraction.text.TfidfVectorizer.idf_(self)
sklearn.feature_extraction.text.TfidfVectorizer.norm(self)
sklearn.feature_extraction.text.TfidfVectorizer.norm(self,value)
sklearn.feature_extraction.text.TfidfVectorizer.smooth_idf(self)
sklearn.feature_extraction.text.TfidfVectorizer.smooth_idf(self,value)
sklearn.feature_extraction.text.TfidfVectorizer.sublinear_tf(self)
sklearn.feature_extraction.text.TfidfVectorizer.sublinear_tf(self,value)
sklearn.feature_extraction.text.TfidfVectorizer.transform(self,raw_documents,copy=True)
sklearn.feature_extraction.text.TfidfVectorizer.use_idf(self)
sklearn.feature_extraction.text.TfidfVectorizer.use_idf(self,value)
sklearn.feature_extraction.text.VectorizerMixin(object)
sklearn.feature_extraction.text.VectorizerMixin._char_ngrams(self,text_document)
sklearn.feature_extraction.text.VectorizerMixin._char_wb_ngrams(self,text_document)
sklearn.feature_extraction.text.VectorizerMixin._check_vocabulary(self)
sklearn.feature_extraction.text.VectorizerMixin._validate_vocabulary(self)
sklearn.feature_extraction.text.VectorizerMixin._word_ngrams(self,tokens,stop_words=None)
sklearn.feature_extraction.text.VectorizerMixin.build_analyzer(self)
sklearn.feature_extraction.text.VectorizerMixin.build_preprocessor(self)
sklearn.feature_extraction.text.VectorizerMixin.build_tokenizer(self)
sklearn.feature_extraction.text.VectorizerMixin.decode(self,doc)
sklearn.feature_extraction.text.VectorizerMixin.get_stop_words(self)
sklearn.feature_extraction.text._check_stop_list(stop)
sklearn.feature_extraction.text._document_frequency(X)
sklearn.feature_extraction.text._make_int_array()
sklearn.feature_extraction.text.strip_accents_ascii(s)
sklearn.feature_extraction.text.strip_accents_unicode(s)
sklearn.feature_extraction.text.strip_tags(s)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py----------------------------------------
A:sklearn.feature_extraction.hashing.raw_X->iter(raw_X)
A:sklearn.feature_extraction.hashing.(indices, indptr, values)->_hashing.transform(raw_X, self.n_features, self.dtype, self.alternate_sign)
A:sklearn.feature_extraction.hashing.X->scipy.sparse.csr_matrix((values, indices, indptr), dtype=self.dtype, shape=(n_samples, self.n_features))
sklearn.feature_extraction.FeatureHasher(self,n_features=2**20,input_type='dict',dtype=np.float64,alternate_sign=True,non_negative=False)
sklearn.feature_extraction.FeatureHasher._validate_params(n_features,input_type)
sklearn.feature_extraction.FeatureHasher.fit(self,X=None,y=None)
sklearn.feature_extraction.FeatureHasher.transform(self,raw_X)
sklearn.feature_extraction.hashing.FeatureHasher(self,n_features=2**20,input_type='dict',dtype=np.float64,alternate_sign=True,non_negative=False)
sklearn.feature_extraction.hashing.FeatureHasher.__init__(self,n_features=2**20,input_type='dict',dtype=np.float64,alternate_sign=True,non_negative=False)
sklearn.feature_extraction.hashing.FeatureHasher._validate_params(n_features,input_type)
sklearn.feature_extraction.hashing.FeatureHasher.fit(self,X=None,y=None)
sklearn.feature_extraction.hashing.FeatureHasher.transform(self,raw_X)
sklearn.feature_extraction.hashing._iteritems(d)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/feature_extraction/setup.py----------------------------------------
A:sklearn.feature_extraction.setup.config->Configuration('feature_extraction', parent_package, top_path)
sklearn.feature_extraction.setup.configuration(parent_package='',top_path=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/feature_extraction/stop_words.py----------------------------------------
A:sklearn.feature_extraction.stop_words.ENGLISH_STOP_WORDS->frozenset(['a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amoungst', 'amount', 'an', 'and', 'another', 'any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere', 'are', 'around', 'as', 'at', 'back', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'between', 'beyond', 'bill', 'both', 'bottom', 'but', 'by', 'call', 'can', 'cannot', 'cant', 'co', 'con', 'could', 'couldnt', 'cry', 'de', 'describe', 'detail', 'do', 'done', 'down', 'due', 'during', 'each', 'eg', 'eight', 'either', 'eleven', 'else', 'elsewhere', 'empty', 'enough', 'etc', 'even', 'ever', 'every', 'everyone', 'everything', 'everywhere', 'except', 'few', 'fifteen', 'fifty', 'fill', 'find', 'fire', 'first', 'five', 'for', 'former', 'formerly', 'forty', 'found', 'four', 'from', 'front', 'full', 'further', 'get', 'give', 'go', 'had', 'has', 'hasnt', 'have', 'he', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'however', 'hundred', 'i', 'ie', 'if', 'in', 'inc', 'indeed', 'interest', 'into', 'is', 'it', 'its', 'itself', 'keep', 'last', 'latter', 'latterly', 'least', 'less', 'ltd', 'made', 'many', 'may', 'me', 'meanwhile', 'might', 'mill', 'mine', 'more', 'moreover', 'most', 'mostly', 'move', 'much', 'must', 'my', 'myself', 'name', 'namely', 'neither', 'never', 'nevertheless', 'next', 'nine', 'no', 'nobody', 'none', 'noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'of', 'off', 'often', 'on', 'once', 'one', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'part', 'per', 'perhaps', 'please', 'put', 'rather', 're', 'same', 'see', 'seem', 'seemed', 'seeming', 'seems', 'serious', 'several', 'she', 'should', 'show', 'side', 'since', 'sincere', 'six', 'sixty', 'so', 'some', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhere', 'still', 'such', 'system', 'take', 'ten', 'than', 'that', 'the', 'their', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', 'therefore', 'therein', 'thereupon', 'these', 'they', 'thick', 'thin', 'third', 'this', 'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'top', 'toward', 'towards', 'twelve', 'twenty', 'two', 'un', 'under', 'until', 'up', 'upon', 'us', 'very', 'via', 'was', 'we', 'well', 'were', 'what', 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', 'whom', 'whose', 'why', 'will', 'with', 'within', 'without', 'would', 'yet', 'you', 'your', 'yours', 'yourself', 'yourselves'])


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/feature_extraction/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/feature_extraction/tests/test_text.py----------------------------------------
A:sklearn.feature_extraction.tests.test_text.wa->CountVectorizer(ngram_range=(1, 2), encoding='ascii').build_analyzer()
A:sklearn.feature_extraction.tests.test_text.text->StringIO('A test with a file-like object!')
A:sklearn.feature_extraction.tests.test_text.text_bytes->StringIO('A test with a file-like object!').encode('utf-8')
A:sklearn.feature_extraction.tests.test_text.ca->CountVectorizer(analyzer='char', ngram_range=(3, 6), encoding='ascii').build_analyzer()
A:sklearn.feature_extraction.tests.test_text.cnga->CountVectorizer(analyzer='word', strip_accents='unicode', ngram_range=(3, 6)).build_analyzer()
A:sklearn.feature_extraction.tests.test_text.cnga_file->CountVectorizer(input='file', analyzer='word', ngram_range=(3, 6)).build_analyzer()
A:sklearn.feature_extraction.tests.test_text.file->StringIO(text)
A:sklearn.feature_extraction.tests.test_text.terms->numpy.sort(np.unique(analyze(doc)))
A:sklearn.feature_extraction.tests.test_text.v->TfidfVectorizer(binary=True, use_idf=False, norm=None)
A:sklearn.feature_extraction.tests.test_text.vect->TfidfVectorizer(use_idf=True)
A:sklearn.feature_extraction.tests.test_text.X->TfidfVectorizer(binary=True, use_idf=False, norm=None).fit_transform(['hello world', 'hello hello']).toarray()
A:sklearn.feature_extraction.tests.test_text.pipe->Pipeline([('count', CountVectorizer(vocabulary=what_we_like)), ('tfidf', TfidfTransformer())])
A:sklearn.feature_extraction.tests.test_text.cv->CountVectorizer(vocabulary=vocab_dict)
A:sklearn.feature_extraction.tests.test_text.X1->CountVectorizer(vocabulary=vocab_dict).fit_transform(ALL_FOOD_DOCS[:5])
A:sklearn.feature_extraction.tests.test_text.X2->TfidfVectorizer(binary=True, use_idf=False, norm=None).transform(['hello world', 'hello hello']).toarray()
A:sklearn.feature_extraction.tests.test_text.tr->TfidfTransformer(sublinear_tf=True, use_idf=False, norm=None)
A:sklearn.feature_extraction.tests.test_text.tfidf->TfidfTransformer(norm='l1').fit(counts_train).transform(counts_train).toarray()
A:sklearn.feature_extraction.tests.test_text.train_data->iter(ALL_FOOD_DOCS[:-1])
A:sklearn.feature_extraction.tests.test_text.v1->CountVectorizer(max_df=0.5)
A:sklearn.feature_extraction.tests.test_text.counts_train->counts_train.tocsr().tocsr()
A:sklearn.feature_extraction.tests.test_text.v2->CountVectorizer(vocabulary=v1.vocabulary_)
A:sklearn.feature_extraction.tests.test_text.counts_test->counts_test.tocsr().tocsr()
A:sklearn.feature_extraction.tests.test_text.t1->TfidfTransformer(norm='l1')
A:sklearn.feature_extraction.tests.test_text.tfidf_test->TfidfTransformer(norm='l1').transform(counts_test).toarray()
A:sklearn.feature_extraction.tests.test_text.t2->TfidfTransformer(norm='l1', use_idf=False)
A:sklearn.feature_extraction.tests.test_text.tf->TfidfTransformer(norm='l1', use_idf=False).fit(counts_train).transform(counts_train).toarray()
A:sklearn.feature_extraction.tests.test_text.t3->TfidfTransformer(use_idf=True)
A:sklearn.feature_extraction.tests.test_text.tv->TfidfVectorizer(norm='l2', use_idf=False, smooth_idf=False, sublinear_tf=False)
A:sklearn.feature_extraction.tests.test_text.tfidf2->TfidfVectorizer(norm='l2', use_idf=False, smooth_idf=False, sublinear_tf=False).fit_transform(train_data).toarray()
A:sklearn.feature_extraction.tests.test_text.tfidf_test2->TfidfVectorizer(norm='l2', use_idf=False, smooth_idf=False, sublinear_tf=False).transform(test_data).toarray()
A:sklearn.feature_extraction.tests.test_text.v3->CountVectorizer(vocabulary=None)
A:sklearn.feature_extraction.tests.test_text.feature_names->CountVectorizer(vocabulary=vocab_dict).get_feature_names()
A:sklearn.feature_extraction.tests.test_text.expected_vocabulary->set(['burger', 'beer', 'salad', 'pizza'])
A:sklearn.feature_extraction.tests.test_text.expected_stop_words->set([u'celeri', u'tomato', u'copyright', u'coke', u'sparkling', u'water', u'the'])
A:sklearn.feature_extraction.tests.test_text.vectorizer->vec_factory(max_df=0.6, max_features=4)
A:sklearn.feature_extraction.tests.test_text.cv_1->CountVectorizer(max_features=1)
A:sklearn.feature_extraction.tests.test_text.cv_3->CountVectorizer(max_features=3)
A:sklearn.feature_extraction.tests.test_text.cv_None->CountVectorizer(max_features=None)
A:sklearn.feature_extraction.tests.test_text.counts_1->CountVectorizer(max_features=1).fit_transform(JUNK_FOOD_DOCS).sum(axis=0)
A:sklearn.feature_extraction.tests.test_text.counts_3->CountVectorizer(max_features=3).fit_transform(JUNK_FOOD_DOCS).sum(axis=0)
A:sklearn.feature_extraction.tests.test_text.counts_None->CountVectorizer(max_features=None).fit_transform(JUNK_FOOD_DOCS).sum(axis=0)
A:sklearn.feature_extraction.tests.test_text.features_1->CountVectorizer(max_features=1).get_feature_names()
A:sklearn.feature_extraction.tests.test_text.features_3->CountVectorizer(max_features=3).get_feature_names()
A:sklearn.feature_extraction.tests.test_text.features_None->CountVectorizer(max_features=None).get_feature_names()
A:sklearn.feature_extraction.tests.test_text.X_sparse->TfidfVectorizer(use_idf=True).fit_transform(test_data)
A:sklearn.feature_extraction.tests.test_text.transformed_data->transformed_data.toarray().toarray()
A:sklearn.feature_extraction.tests.test_text.inversed_data->vec_factory(max_df=0.6, max_features=4).inverse_transform(transformed_data)
A:sklearn.feature_extraction.tests.test_text.analyze->vec_factory(max_df=0.6, max_features=4).build_analyzer()
A:sklearn.feature_extraction.tests.test_text.inversed_terms->numpy.sort(np.unique(inversed_terms))
A:sklearn.feature_extraction.tests.test_text.inversed_data2->vec_factory(max_df=0.6, max_features=4).inverse_transform(transformed_data)
A:sklearn.feature_extraction.tests.test_text.(train_data, test_data, target_train, target_test)->train_test_split(data, target, test_size=0.1, random_state=0)
A:sklearn.feature_extraction.tests.test_text.pipeline->Pipeline([('vect', TfidfVectorizer()), ('svc', LinearSVC())])
A:sklearn.feature_extraction.tests.test_text.grid_search->GridSearchCV(pipeline, parameters, n_jobs=1)
A:sklearn.feature_extraction.tests.test_text.pred->GridSearchCV(pipeline, parameters, n_jobs=1).fit(train_data, target_train).predict(test_data)
A:sklearn.feature_extraction.tests.test_text.cv_scores->cross_val_score(pipeline, data, target, cv=3)
A:sklearn.feature_extraction.tests.test_text.X_counted->TfidfVectorizer(use_idf=True).fit_transform([document])
A:sklearn.feature_extraction.tests.test_text.X_hashed->TfidfVectorizer(use_idf=True).transform([document])
A:sklearn.feature_extraction.tests.test_text.X_1->TfidfVectorizer(use_idf=True).fit_transform(ALL_FOOD_DOCS)
A:sklearn.feature_extraction.tests.test_text.X_2->TfidfVectorizer(use_idf=True).transform(ALL_FOOD_DOCS)
A:sklearn.feature_extraction.tests.test_text.s->pickle.dumps(orig)
A:sklearn.feature_extraction.tests.test_text.copy->pickle.loads(s)
A:sklearn.feature_extraction.tests.test_text.rng->numpy.random.RandomState(0)
A:sklearn.feature_extraction.tests.test_text.vocab_words->numpy.array(['beer', 'burger', 'celeri', 'coke', 'pizza', 'salad', 'sparkling', 'tomato', 'water'])
A:sklearn.feature_extraction.tests.test_text.vocab_set->set(rng.choice(vocab_words, size=5, replace=False))
A:sklearn.feature_extraction.tests.test_text.unpickled_cv->pickle.loads(pickle.dumps(cv))
A:sklearn.feature_extraction.tests.test_text.vocab_dict->dict()
A:sklearn.feature_extraction.tests.test_text.words->numpy.random.RandomState(0).choice(vocab_words, size=5, replace=False)
A:sklearn.feature_extraction.tests.test_text.vect_transform->TfidfVectorizer(use_idf=True).transform(JUNK_FOOD_DOCS).toarray()
A:sklearn.feature_extraction.tests.test_text.stop_None_transform->TfidfVectorizer(use_idf=True).transform(JUNK_FOOD_DOCS).toarray()
A:sklearn.feature_extraction.tests.test_text.stop_del_transform->TfidfVectorizer(use_idf=True).transform(JUNK_FOOD_DOCS).toarray()
A:sklearn.feature_extraction.tests.test_text.orig->TfidfTransformer().fit(X)
A:sklearn.feature_extraction.tests.test_text.hv->HashingVectorizer()
A:sklearn.feature_extraction.tests.test_text.vect_vocab->TfidfVectorizer(vocabulary=['the'])
A:sklearn.feature_extraction.tests.test_text.vect_vocab_clone->clone(vect_vocab)
sklearn.feature_extraction.tests.test_text.lazy_analyze(s)
sklearn.feature_extraction.tests.test_text.split_tokenize(s)
sklearn.feature_extraction.tests.test_text.strip_eacute(s)
sklearn.feature_extraction.tests.test_text.test_char_ngram_analyzer()
sklearn.feature_extraction.tests.test_text.test_char_wb_ngram_analyzer()
sklearn.feature_extraction.tests.test_text.test_count_binary_occurrences()
sklearn.feature_extraction.tests.test_text.test_count_vectorizer_max_features()
sklearn.feature_extraction.tests.test_text.test_count_vectorizer_pipeline_grid_selection()
sklearn.feature_extraction.tests.test_text.test_countvectorizer_custom_vocabulary()
sklearn.feature_extraction.tests.test_text.test_countvectorizer_custom_vocabulary_gap_index()
sklearn.feature_extraction.tests.test_text.test_countvectorizer_custom_vocabulary_pipeline()
sklearn.feature_extraction.tests.test_text.test_countvectorizer_custom_vocabulary_repeated_indeces()
sklearn.feature_extraction.tests.test_text.test_countvectorizer_empty_vocabulary()
sklearn.feature_extraction.tests.test_text.test_countvectorizer_stop_words()
sklearn.feature_extraction.tests.test_text.test_countvectorizer_vocab_dicts_when_pickling()
sklearn.feature_extraction.tests.test_text.test_countvectorizer_vocab_sets_when_pickling()
sklearn.feature_extraction.tests.test_text.test_feature_names()
sklearn.feature_extraction.tests.test_text.test_fit_countvectorizer_twice()
sklearn.feature_extraction.tests.test_text.test_hashed_binary_occurrences()
sklearn.feature_extraction.tests.test_text.test_hashing_vectorizer()
sklearn.feature_extraction.tests.test_text.test_hashingvectorizer_nan_in_docs()
sklearn.feature_extraction.tests.test_text.test_non_unique_vocab()
sklearn.feature_extraction.tests.test_text.test_pickling_transformer()
sklearn.feature_extraction.tests.test_text.test_pickling_vectorizer()
sklearn.feature_extraction.tests.test_text.test_stop_words_removal()
sklearn.feature_extraction.tests.test_text.test_strip_accents()
sklearn.feature_extraction.tests.test_text.test_sublinear_tf()
sklearn.feature_extraction.tests.test_text.test_tf_idf_smoothing()
sklearn.feature_extraction.tests.test_text.test_tfidf_no_smoothing()
sklearn.feature_extraction.tests.test_text.test_tfidf_vectorizer_setters()
sklearn.feature_extraction.tests.test_text.test_tfidf_vectorizer_with_fixed_vocabulary()
sklearn.feature_extraction.tests.test_text.test_tfidfvectorizer_binary()
sklearn.feature_extraction.tests.test_text.test_tfidfvectorizer_export_idf()
sklearn.feature_extraction.tests.test_text.test_to_ascii()
sklearn.feature_extraction.tests.test_text.test_unicode_decode_error()
sklearn.feature_extraction.tests.test_text.test_vectorizer()
sklearn.feature_extraction.tests.test_text.test_vectorizer_inverse_transform()
sklearn.feature_extraction.tests.test_text.test_vectorizer_max_df()
sklearn.feature_extraction.tests.test_text.test_vectorizer_max_features()
sklearn.feature_extraction.tests.test_text.test_vectorizer_min_df()
sklearn.feature_extraction.tests.test_text.test_vectorizer_pipeline_cross_validation()
sklearn.feature_extraction.tests.test_text.test_vectorizer_pipeline_grid_selection()
sklearn.feature_extraction.tests.test_text.test_vectorizer_string_object_as_input()
sklearn.feature_extraction.tests.test_text.test_vectorizer_unicode()
sklearn.feature_extraction.tests.test_text.test_vectorizer_vocab_clone()
sklearn.feature_extraction.tests.test_text.test_word_analyzer_unigrams()
sklearn.feature_extraction.tests.test_text.test_word_analyzer_unigrams_and_bigrams()
sklearn.feature_extraction.tests.test_text.test_word_ngram_analyzer()
sklearn.feature_extraction.tests.test_text.uppercase(s)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/feature_extraction/tests/test_feature_hasher.py----------------------------------------
A:sklearn.feature_extraction.tests.test_feature_hasher.h->FeatureHasher(n_features=np.uint16(2 ** 6))
A:sklearn.feature_extraction.tests.test_feature_hasher.X1->FeatureHasher(n_features=16).transform(raw_X)
A:sklearn.feature_extraction.tests.test_feature_hasher.X2->FeatureHasher(n_features=16, input_type='pair').transform(gen)
A:sklearn.feature_extraction.tests.test_feature_hasher.X->FeatureHasher().transform([{'foo': 0}])
A:sklearn.feature_extraction.tests.test_feature_hasher.(x1, x2)->FeatureHasher(n_features=np.uint16(2 ** 6)).transform(raw_X).toarray()
A:sklearn.feature_extraction.tests.test_feature_hasher.x1_nz->numpy.abs(x1[x1 != 0])
A:sklearn.feature_extraction.tests.test_feature_hasher.x2_nz->numpy.abs(x2[x2 != 0])
A:sklearn.feature_extraction.tests.test_feature_hasher.hasher->FeatureHasher()
A:sklearn.feature_extraction.tests.test_feature_hasher.Xt->FeatureHasher(alternate_sign=True, non_negative=True, input_type='pair').fit_transform(X)
A:sklearn.feature_extraction.tests.test_feature_hasher.Xt_2->FeatureHasher(alternate_sign=False, non_negative=False, input_type='string').fit_transform(X)
sklearn.feature_extraction.tests.test_feature_hasher.test_feature_hasher_dicts()
sklearn.feature_extraction.tests.test_feature_hasher.test_feature_hasher_pairs()
sklearn.feature_extraction.tests.test_feature_hasher.test_feature_hasher_pairs_with_string_values()
sklearn.feature_extraction.tests.test_feature_hasher.test_feature_hasher_strings()
sklearn.feature_extraction.tests.test_feature_hasher.test_hash_collisions()
sklearn.feature_extraction.tests.test_feature_hasher.test_hash_empty_input()
sklearn.feature_extraction.tests.test_feature_hasher.test_hasher_alternate_sign()
sklearn.feature_extraction.tests.test_feature_hasher.test_hasher_invalid_input()
sklearn.feature_extraction.tests.test_feature_hasher.test_hasher_negative()
sklearn.feature_extraction.tests.test_feature_hasher.test_hasher_set_params()
sklearn.feature_extraction.tests.test_feature_hasher.test_hasher_zeros()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/feature_extraction/tests/test_image.py----------------------------------------
A:sklearn.feature_extraction.tests.test_image.grad_x->img_to_graph(x)
A:sklearn.feature_extraction.tests.test_image.grad_y->img_to_graph(y)
A:sklearn.feature_extraction.tests.test_image.mask->numpy.ones((size, size))
A:sklearn.feature_extraction.tests.test_image.A->grid_to_graph(n_x=size, n_y=size, n_z=size, mask=mask, dtype=np.float64)
A:sklearn.feature_extraction.tests.test_image.face->face.astype(np.float32).astype(np.float32)
A:sklearn.feature_extraction.tests.test_image.graph->grid_to_graph(*face.shape, mask=mask, dtype=None)
A:sklearn.feature_extraction.tests.test_image.face_color->numpy.zeros(face.shape + (3,))
A:sklearn.feature_extraction.tests.test_image.images->numpy.zeros((3,) + face.shape)
A:sklearn.feature_extraction.tests.test_image.downsampled_face->_downsampled_face()
A:sklearn.feature_extraction.tests.test_image.orange_face->_orange_face(downsampled_face)
A:sklearn.feature_extraction.tests.test_image.face_collection->_make_images(downsampled_face)
A:sklearn.feature_extraction.tests.test_image.patches->extract_patches(face, patch_shape=p)
A:sklearn.feature_extraction.tests.test_image.expected_n_patches->int(0.5 * (i_h - p_h + 1) * (i_w - p_w + 1))
A:sklearn.feature_extraction.tests.test_image.face_reconstructed->reconstruct_from_patches_2d(patches, face.shape)
A:sklearn.feature_extraction.tests.test_image.extr->PatchExtractor(patch_size=(p_h, p_w), random_state=0)
A:sklearn.feature_extraction.tests.test_image.faces->_make_images(orange_face)
A:sklearn.feature_extraction.tests.test_image.image->numpy.arange(np.prod(image_shape)).reshape(image_shape)
A:sklearn.feature_extraction.tests.test_image.ndim->len(image_shape)
A:sklearn.feature_extraction.tests.test_image.x->numpy.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
sklearn.feature_extraction.tests.test_image._downsampled_face()
sklearn.feature_extraction.tests.test_image._make_images(face=None)
sklearn.feature_extraction.tests.test_image._orange_face(face=None)
sklearn.feature_extraction.tests.test_image.test_connect_regions()
sklearn.feature_extraction.tests.test_image.test_connect_regions_with_grid()
sklearn.feature_extraction.tests.test_image.test_extract_patches_all()
sklearn.feature_extraction.tests.test_image.test_extract_patches_all_color()
sklearn.feature_extraction.tests.test_image.test_extract_patches_all_rect()
sklearn.feature_extraction.tests.test_image.test_extract_patches_max_patches()
sklearn.feature_extraction.tests.test_image.test_extract_patches_square()
sklearn.feature_extraction.tests.test_image.test_extract_patches_strided()
sklearn.feature_extraction.tests.test_image.test_grid_to_graph()
sklearn.feature_extraction.tests.test_image.test_img_to_graph()
sklearn.feature_extraction.tests.test_image.test_patch_extractor_all_patches()
sklearn.feature_extraction.tests.test_image.test_patch_extractor_color()
sklearn.feature_extraction.tests.test_image.test_patch_extractor_fit()
sklearn.feature_extraction.tests.test_image.test_patch_extractor_max_patches()
sklearn.feature_extraction.tests.test_image.test_patch_extractor_max_patches_default()
sklearn.feature_extraction.tests.test_image.test_reconstruct_patches_perfect()
sklearn.feature_extraction.tests.test_image.test_reconstruct_patches_perfect_color()
sklearn.feature_extraction.tests.test_image.test_width_patch()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/feature_extraction/tests/test_dict_vectorizer.py----------------------------------------
A:sklearn.feature_extraction.tests.test_dict_vectorizer.v->DictVectorizer(sparse=sparse).fit(D)
A:sklearn.feature_extraction.tests.test_dict_vectorizer.X->X.toarray().toarray()
A:sklearn.feature_extraction.tests.test_dict_vectorizer.d1->dict([('useless%d' % i, 10) for i in range(20)], useful1=1, useful2=20)
A:sklearn.feature_extraction.tests.test_dict_vectorizer.d2->dict([('useless%d' % i, 10) for i in range(20)], useful1=20, useful2=1)
A:sklearn.feature_extraction.tests.test_dict_vectorizer.sel->SelectKBest(chi2, k=2).fit(X, [0, 1])
A:sklearn.feature_extraction.tests.test_dict_vectorizer.D_out->DictVectorizer(sparse=sparse).fit(D).inverse_transform(X)
A:sklearn.feature_extraction.tests.test_dict_vectorizer.names->DictVectorizer(sparse=sparse).fit(D).get_feature_names()
A:sklearn.feature_extraction.tests.test_dict_vectorizer.rng->Random(42)
A:sklearn.feature_extraction.tests.test_dict_vectorizer.d_sorted->dict(items)
A:sklearn.feature_extraction.tests.test_dict_vectorizer.d_shuffled->dict(items)
A:sklearn.feature_extraction.tests.test_dict_vectorizer.v_1->DictVectorizer().fit([d_sorted])
A:sklearn.feature_extraction.tests.test_dict_vectorizer.v_2->DictVectorizer().fit([d_shuffled])
sklearn.feature_extraction.tests.test_dict_vectorizer.test_deterministic_vocabulary()
sklearn.feature_extraction.tests.test_dict_vectorizer.test_dictvectorizer()
sklearn.feature_extraction.tests.test_dict_vectorizer.test_feature_selection()
sklearn.feature_extraction.tests.test_dict_vectorizer.test_one_of_k()
sklearn.feature_extraction.tests.test_dict_vectorizer.test_unseen_or_no_features()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/model_selection/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/model_selection/_search.py----------------------------------------
A:sklearn.model_selection._search.items->sorted(self.param_distributions.items())
A:sklearn.model_selection._search.(keys, values)->zip(*items)
A:sklearn.model_selection._search.params->dict()
A:sklearn.model_selection._search.product->partial(reduce, operator.mul)
A:sklearn.model_selection._search.(keys, values_lists)->zip(*sorted(sub_grid.items())[::-1])
A:sklearn.model_selection._search.total->numpy.product(sizes)
A:sklearn.model_selection._search.(ind, offset)->divmod(ind, n)
A:sklearn.model_selection._search.all_lists->numpy.all([not hasattr(v, 'rvs') for v in self.param_distributions.values()])
A:sklearn.model_selection._search.rnd->check_random_state(self.random_state)
A:sklearn.model_selection._search.param_grid->ParameterGrid(self.param_distributions)
A:sklearn.model_selection._search.grid_size->len(param_grid)
A:sklearn.model_selection._search.params[k]->v.rvs(random_state=rnd)
A:sklearn.model_selection._search.(scores, n_samples_test)->_fit_and_score(estimator, X, y, scorer, train, test, verbose, parameters, fit_params=fit_params, return_n_test_samples=True, error_score=error_score)
A:sklearn.model_selection._search.cv->check_cv(self.cv, y, classifier=is_classifier(estimator))
A:sklearn.model_selection._search.(scorers, self.multimetric_)->_check_multimetric_scoring(self.estimator, scoring=self.scoring)
A:sklearn.model_selection._search.(X, y, groups)->indexable(X, y, groups)
A:sklearn.model_selection._search.n_splits->check_cv(self.cv, y, classifier=is_classifier(estimator)).get_n_splits(X, y, groups)
A:sklearn.model_selection._search.candidate_params->list(self._get_param_iterator())
A:sklearn.model_selection._search.n_candidates->len(candidate_params)
A:sklearn.model_selection._search.base_estimator->clone(self.estimator)
A:sklearn.model_selection._search.out->Parallel(n_jobs=self.n_jobs, verbose=self.verbose, pre_dispatch=pre_dispatch)((delayed(_fit_and_score)(clone(base_estimator), X, y, scorers, train, test, self.verbose, parameters, fit_params=fit_params, return_train_score=self.return_train_score, return_n_test_samples=True, return_times=True, return_parameters=False, error_score=self.error_score) for (parameters, (train, test)) in product(candidate_params, cv.split(X, y, groups))))
A:sklearn.model_selection._search.(train_score_dicts, test_score_dicts, test_sample_counts, fit_time, score_time)->zip(*out)
A:sklearn.model_selection._search.(test_score_dicts, test_sample_counts, fit_time, score_time)->zip(*out)
A:sklearn.model_selection._search.test_scores->_aggregate_score_dicts(test_score_dicts)
A:sklearn.model_selection._search.train_scores->_aggregate_score_dicts(train_score_dicts)
A:sklearn.model_selection._search.array->numpy.array(array, dtype=np.float64).reshape(n_candidates, n_splits)
A:sklearn.model_selection._search.array_means->numpy.average(array, axis=1, weights=weights)
A:sklearn.model_selection._search.array_stds->numpy.sqrt(np.average((array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights))
A:sklearn.model_selection._search.results['rank_%s' % key_name]->numpy.asarray(rankdata(-array_means, method='min'), dtype=np.int32)
A:sklearn.model_selection._search.param_results->defaultdict(partial(MaskedArray, np.empty(n_candidates), mask=True, dtype=object))
A:sklearn.model_selection._search.test_sample_counts->numpy.array(test_sample_counts[:n_splits], dtype=np.int)
A:sklearn.model_selection._search.prev_keys->set(results.keys())
A:sklearn.model_selection._search.message->'You are accessing a training score ({!r}), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True'.format(key)
A:sklearn.model_selection._search.self.best_index_->results['rank_test_%s' % refit_metric].argmin()
A:sklearn.model_selection._search.self.best_estimator_->clone(base_estimator).set_params(**self.best_params_)
A:sklearn.model_selection._search.grid_scores->list()
A:sklearn.model_selection._search.scores->numpy.array(list((self.cv_results_['split%d_test_score' % s][i] for s in range(self.n_splits_))), dtype=np.float64)
sklearn.model_selection.GridSearchCV(self,estimator,param_grid,scoring=None,fit_params=None,n_jobs=1,iid=True,refit=True,cv=None,verbose=0,pre_dispatch='2*n_jobs',error_score='raise',return_train_score='warn')
sklearn.model_selection.GridSearchCV._get_param_iterator(self)
sklearn.model_selection.ParameterGrid(self,param_grid)
sklearn.model_selection.ParameterGrid.__getitem__(self,ind)
sklearn.model_selection.ParameterGrid.__iter__(self)
sklearn.model_selection.ParameterGrid.__len__(self)
sklearn.model_selection.ParameterSampler(self,param_distributions,n_iter,random_state=None)
sklearn.model_selection.ParameterSampler.__iter__(self)
sklearn.model_selection.ParameterSampler.__len__(self)
sklearn.model_selection.RandomizedSearchCV(self,estimator,param_distributions,n_iter=10,scoring=None,fit_params=None,n_jobs=1,iid=True,refit=True,cv=None,verbose=0,pre_dispatch='2*n_jobs',random_state=None,error_score='raise',return_train_score='warn')
sklearn.model_selection.RandomizedSearchCV._get_param_iterator(self)
sklearn.model_selection._search.BaseSearchCV(self,estimator,scoring=None,fit_params=None,n_jobs=1,iid=True,refit=True,cv=None,verbose=0,pre_dispatch='2*n_jobs',error_score='raise',return_train_score=True)
sklearn.model_selection._search.BaseSearchCV.__init__(self,estimator,scoring=None,fit_params=None,n_jobs=1,iid=True,refit=True,cv=None,verbose=0,pre_dispatch='2*n_jobs',error_score='raise',return_train_score=True)
sklearn.model_selection._search.BaseSearchCV._check_is_fitted(self,method_name)
sklearn.model_selection._search.BaseSearchCV._estimator_type(self)
sklearn.model_selection._search.BaseSearchCV.classes_(self)
sklearn.model_selection._search.BaseSearchCV.decision_function(self,X)
sklearn.model_selection._search.BaseSearchCV.fit(self,X,y=None,groups=None,**fit_params)
sklearn.model_selection._search.BaseSearchCV.grid_scores_(self)
sklearn.model_selection._search.BaseSearchCV.inverse_transform(self,Xt)
sklearn.model_selection._search.BaseSearchCV.predict(self,X)
sklearn.model_selection._search.BaseSearchCV.predict_log_proba(self,X)
sklearn.model_selection._search.BaseSearchCV.predict_proba(self,X)
sklearn.model_selection._search.BaseSearchCV.score(self,X,y=None)
sklearn.model_selection._search.BaseSearchCV.transform(self,X)
sklearn.model_selection._search.GridSearchCV(self,estimator,param_grid,scoring=None,fit_params=None,n_jobs=1,iid=True,refit=True,cv=None,verbose=0,pre_dispatch='2*n_jobs',error_score='raise',return_train_score='warn')
sklearn.model_selection._search.GridSearchCV.__init__(self,estimator,param_grid,scoring=None,fit_params=None,n_jobs=1,iid=True,refit=True,cv=None,verbose=0,pre_dispatch='2*n_jobs',error_score='raise',return_train_score='warn')
sklearn.model_selection._search.GridSearchCV._get_param_iterator(self)
sklearn.model_selection._search.ParameterGrid(self,param_grid)
sklearn.model_selection._search.ParameterGrid.__getitem__(self,ind)
sklearn.model_selection._search.ParameterGrid.__init__(self,param_grid)
sklearn.model_selection._search.ParameterGrid.__iter__(self)
sklearn.model_selection._search.ParameterGrid.__len__(self)
sklearn.model_selection._search.ParameterSampler(self,param_distributions,n_iter,random_state=None)
sklearn.model_selection._search.ParameterSampler.__init__(self,param_distributions,n_iter,random_state=None)
sklearn.model_selection._search.ParameterSampler.__iter__(self)
sklearn.model_selection._search.ParameterSampler.__len__(self)
sklearn.model_selection._search.RandomizedSearchCV(self,estimator,param_distributions,n_iter=10,scoring=None,fit_params=None,n_jobs=1,iid=True,refit=True,cv=None,verbose=0,pre_dispatch='2*n_jobs',random_state=None,error_score='raise',return_train_score='warn')
sklearn.model_selection._search.RandomizedSearchCV.__init__(self,estimator,param_distributions,n_iter=10,scoring=None,fit_params=None,n_jobs=1,iid=True,refit=True,cv=None,verbose=0,pre_dispatch='2*n_jobs',random_state=None,error_score='raise',return_train_score='warn')
sklearn.model_selection._search.RandomizedSearchCV._get_param_iterator(self)
sklearn.model_selection._search._CVScoreTuple(namedtuple('_CVScoreTuple',('parameters','mean_validation_score','cv_validation_scores')))
sklearn.model_selection._search._CVScoreTuple.__repr__(self)
sklearn.model_selection._search._check_param_grid(param_grid)
sklearn.model_selection._search.fit_grid_point(X,y,estimator,parameters,train,test,scorer,verbose,error_score='raise',**fit_params)
sklearn.model_selection.fit_grid_point(X,y,estimator,parameters,train,test,scorer,verbose,error_score='raise',**fit_params)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/model_selection/_split.py----------------------------------------
A:sklearn.model_selection._split.(X, y, groups)->indexable(X, y, groups)
A:sklearn.model_selection._split.indices->numpy.arange(n_samples)
A:sklearn.model_selection._split.test_mask->numpy.zeros(len(self.test_fold), dtype=np.bool)
A:sklearn.model_selection._split.n_splits->int(n_splits)
A:sklearn.model_selection._split.n_samples->_num_samples(arrays[0])
A:sklearn.model_selection._split.groups->check_array(groups, ensure_2d=False, dtype=None)
A:sklearn.model_selection._split.(unique_groups, groups)->numpy.unique(groups, return_inverse=True)
A:sklearn.model_selection._split.n_groups->len(unique_groups)
A:sklearn.model_selection._split.n_samples_per_group->numpy.bincount(groups)
A:sklearn.model_selection._split.n_samples_per_fold->numpy.zeros(self.n_splits)
A:sklearn.model_selection._split.group_to_fold->numpy.zeros(len(unique_groups))
A:sklearn.model_selection._split.lightest_fold->numpy.argmin(n_samples_per_fold)
A:sklearn.model_selection._split.y->check_array(y, ensure_2d=False, dtype=None)
A:sklearn.model_selection._split.type_of_target_y->type_of_target(y)
A:sklearn.model_selection._split.(unique_y, y_inversed)->numpy.unique(y, return_inverse=True)
A:sklearn.model_selection._split.y_counts->numpy.bincount(y_inversed)
A:sklearn.model_selection._split.min_groups->numpy.min(y_counts)
A:sklearn.model_selection._split.test_folds->self._make_test_folds(X, y)
A:sklearn.model_selection._split.test_starts->range(test_size + n_samples % n_folds, n_samples, test_size)
A:sklearn.model_selection._split.unique_groups->numpy.unique(groups)
A:sklearn.model_selection._split.combi->combinations(range(len(unique_groups)), self.n_groups)
A:sklearn.model_selection._split.test_index->numpy.zeros(_num_samples(X), dtype=np.bool)
A:sklearn.model_selection._split.rng->check_random_state(self.random_state)
A:sklearn.model_selection._split.cv->CVClass(test_size=test_size, train_size=train_size, random_state=random_state)
A:sklearn.model_selection._split.(n_train, n_test)->_validate_shuffle_split(n_samples, test_size, train_size)
A:sklearn.model_selection._split.permutation->check_random_state(self.random_state).permutation(class_counts[i])
A:sklearn.model_selection._split.(classes, group_indices)->numpy.unique(groups, return_inverse=True)
A:sklearn.model_selection._split.train->numpy.arange(n_train)
A:sklearn.model_selection._split.test->numpy.arange(n_train, n_train + n_test)
A:sklearn.model_selection._split.floored->numpy.floor(continuous)
A:sklearn.model_selection._split.need_to_add->int(n_draws - floored.sum())
A:sklearn.model_selection._split.(inds,)->numpy.where(remainder == value)
A:sklearn.model_selection._split.add_now->min(len(inds), need_to_add)
A:sklearn.model_selection._split.inds->check_random_state(self.random_state).choice(inds, size=add_now, replace=False)
A:sklearn.model_selection._split.(classes, y_indices)->numpy.unique(y, return_inverse=True)
A:sklearn.model_selection._split.class_counts->numpy.bincount(y_indices)
A:sklearn.model_selection._split.class_indices->numpy.split(np.argsort(y_indices, kind='mergesort'), np.cumsum(class_counts)[:-1])
A:sklearn.model_selection._split.n_i->_approximate_mode(class_counts, n_train, rng)
A:sklearn.model_selection._split.t_i->_approximate_mode(class_counts_remaining, n_test, rng)
A:sklearn.model_selection._split.perm_indices_class_i->class_indices[i].take(permutation, mode='clip')
A:sklearn.model_selection._split.n_test->float(test_size)
A:sklearn.model_selection._split.n_train->float(train_size)
A:sklearn.model_selection._split.self.test_fold->column_or_1d(self.test_fold)
A:sklearn.model_selection._split.self.unique_folds->numpy.unique(self.test_fold)
A:sklearn.model_selection._split.ind->numpy.arange(len(self.test_fold))
A:sklearn.model_selection._split.self.cv->list(cv)
A:sklearn.model_selection._split.n_arrays->len(arrays)
A:sklearn.model_selection._split.test_size->options.pop('test_size', 'default')
A:sklearn.model_selection._split.train_size->options.pop('train_size', None)
A:sklearn.model_selection._split.random_state->options.pop('random_state', None)
A:sklearn.model_selection._split.stratify->options.pop('stratify', None)
A:sklearn.model_selection._split.shuffle->options.pop('shuffle', True)
A:sklearn.model_selection._split.arrays->indexable(*arrays)
A:sklearn.model_selection._split.(train, test)->next(cv.split(X=arrays[0], y=stratify))
A:sklearn.model_selection._split.init->getattr(cls.__init__, 'deprecated_original', cls.__init__)
A:sklearn.model_selection._split.init_signature->signature(init)
A:sklearn.model_selection._split.args->sorted([p.name for p in init_signature.parameters.values() if p.name != 'self' and p.kind != p.VAR_KEYWORD])
A:sklearn.model_selection._split.params->dict()
A:sklearn.model_selection._split.value->getattr(self, key, None)
sklearn.model_selection.BaseCrossValidator(self)
sklearn.model_selection.BaseCrossValidator.__repr__(self)
sklearn.model_selection.BaseCrossValidator._iter_test_indices(self,X=None,y=None,groups=None)
sklearn.model_selection.BaseCrossValidator._iter_test_masks(self,X=None,y=None,groups=None)
sklearn.model_selection.BaseCrossValidator.get_n_splits(self,X=None,y=None,groups=None)
sklearn.model_selection.BaseCrossValidator.split(self,X,y=None,groups=None)
sklearn.model_selection.GroupKFold(self,n_splits=3)
sklearn.model_selection.GroupKFold._iter_test_indices(self,X,y,groups)
sklearn.model_selection.GroupShuffleSplit(self,n_splits=5,test_size='default',train_size=None,random_state=None)
sklearn.model_selection.GroupShuffleSplit._iter_indices(self,X,y,groups)
sklearn.model_selection.KFold(self,n_splits=3,shuffle=False,random_state=None)
sklearn.model_selection.KFold._iter_test_indices(self,X,y=None,groups=None)
sklearn.model_selection.LeaveOneGroupOut(BaseCrossValidator)
sklearn.model_selection.LeaveOneGroupOut._iter_test_masks(self,X,y,groups)
sklearn.model_selection.LeaveOneGroupOut.get_n_splits(self,X=None,y=None,groups=None)
sklearn.model_selection.LeaveOneOut(BaseCrossValidator)
sklearn.model_selection.LeaveOneOut._iter_test_indices(self,X,y=None,groups=None)
sklearn.model_selection.LeaveOneOut.get_n_splits(self,X,y=None,groups=None)
sklearn.model_selection.LeavePGroupsOut(self,n_groups)
sklearn.model_selection.LeavePGroupsOut._iter_test_masks(self,X,y,groups)
sklearn.model_selection.LeavePGroupsOut.get_n_splits(self,X=None,y=None,groups=None)
sklearn.model_selection.LeavePOut(self,p)
sklearn.model_selection.LeavePOut._iter_test_indices(self,X,y=None,groups=None)
sklearn.model_selection.LeavePOut.get_n_splits(self,X,y=None,groups=None)
sklearn.model_selection.PredefinedSplit(self,test_fold)
sklearn.model_selection.PredefinedSplit._iter_test_masks(self)
sklearn.model_selection.PredefinedSplit.get_n_splits(self,X=None,y=None,groups=None)
sklearn.model_selection.PredefinedSplit.split(self,X=None,y=None,groups=None)
sklearn.model_selection.RepeatedKFold(self,n_splits=5,n_repeats=10,random_state=None)
sklearn.model_selection.RepeatedStratifiedKFold(self,n_splits=5,n_repeats=10,random_state=None)
sklearn.model_selection.ShuffleSplit(BaseShuffleSplit)
sklearn.model_selection.ShuffleSplit._iter_indices(self,X,y=None,groups=None)
sklearn.model_selection.StratifiedKFold(self,n_splits=3,shuffle=False,random_state=None)
sklearn.model_selection.StratifiedKFold._iter_test_masks(self,X,y=None,groups=None)
sklearn.model_selection.StratifiedKFold._make_test_folds(self,X,y=None)
sklearn.model_selection.StratifiedKFold.split(self,X,y,groups=None)
sklearn.model_selection.StratifiedShuffleSplit(self,n_splits=10,test_size='default',train_size=None,random_state=None)
sklearn.model_selection.StratifiedShuffleSplit._iter_indices(self,X,y,groups=None)
sklearn.model_selection.StratifiedShuffleSplit.split(self,X,y,groups=None)
sklearn.model_selection.TimeSeriesSplit(self,n_splits=3,max_train_size=None)
sklearn.model_selection.TimeSeriesSplit.split(self,X,y=None,groups=None)
sklearn.model_selection._split.BaseCrossValidator(self)
sklearn.model_selection._split.BaseCrossValidator.__init__(self)
sklearn.model_selection._split.BaseCrossValidator.__repr__(self)
sklearn.model_selection._split.BaseCrossValidator._iter_test_indices(self,X=None,y=None,groups=None)
sklearn.model_selection._split.BaseCrossValidator._iter_test_masks(self,X=None,y=None,groups=None)
sklearn.model_selection._split.BaseCrossValidator.get_n_splits(self,X=None,y=None,groups=None)
sklearn.model_selection._split.BaseCrossValidator.split(self,X,y=None,groups=None)
sklearn.model_selection._split.BaseShuffleSplit(self,n_splits=10,test_size='default',train_size=None,random_state=None)
sklearn.model_selection._split.BaseShuffleSplit.__init__(self,n_splits=10,test_size='default',train_size=None,random_state=None)
sklearn.model_selection._split.BaseShuffleSplit.__repr__(self)
sklearn.model_selection._split.BaseShuffleSplit._iter_indices(self,X,y=None,groups=None)
sklearn.model_selection._split.BaseShuffleSplit.get_n_splits(self,X=None,y=None,groups=None)
sklearn.model_selection._split.BaseShuffleSplit.split(self,X,y=None,groups=None)
sklearn.model_selection._split.GroupKFold(self,n_splits=3)
sklearn.model_selection._split.GroupKFold.__init__(self,n_splits=3)
sklearn.model_selection._split.GroupKFold._iter_test_indices(self,X,y,groups)
sklearn.model_selection._split.GroupShuffleSplit(self,n_splits=5,test_size='default',train_size=None,random_state=None)
sklearn.model_selection._split.GroupShuffleSplit.__init__(self,n_splits=5,test_size='default',train_size=None,random_state=None)
sklearn.model_selection._split.GroupShuffleSplit._iter_indices(self,X,y,groups)
sklearn.model_selection._split.KFold(self,n_splits=3,shuffle=False,random_state=None)
sklearn.model_selection._split.KFold.__init__(self,n_splits=3,shuffle=False,random_state=None)
sklearn.model_selection._split.KFold._iter_test_indices(self,X,y=None,groups=None)
sklearn.model_selection._split.LeaveOneGroupOut(BaseCrossValidator)
sklearn.model_selection._split.LeaveOneGroupOut._iter_test_masks(self,X,y,groups)
sklearn.model_selection._split.LeaveOneGroupOut.get_n_splits(self,X=None,y=None,groups=None)
sklearn.model_selection._split.LeaveOneOut(BaseCrossValidator)
sklearn.model_selection._split.LeaveOneOut._iter_test_indices(self,X,y=None,groups=None)
sklearn.model_selection._split.LeaveOneOut.get_n_splits(self,X,y=None,groups=None)
sklearn.model_selection._split.LeavePGroupsOut(self,n_groups)
sklearn.model_selection._split.LeavePGroupsOut.__init__(self,n_groups)
sklearn.model_selection._split.LeavePGroupsOut._iter_test_masks(self,X,y,groups)
sklearn.model_selection._split.LeavePGroupsOut.get_n_splits(self,X=None,y=None,groups=None)
sklearn.model_selection._split.LeavePOut(self,p)
sklearn.model_selection._split.LeavePOut.__init__(self,p)
sklearn.model_selection._split.LeavePOut._iter_test_indices(self,X,y=None,groups=None)
sklearn.model_selection._split.LeavePOut.get_n_splits(self,X,y=None,groups=None)
sklearn.model_selection._split.PredefinedSplit(self,test_fold)
sklearn.model_selection._split.PredefinedSplit.__init__(self,test_fold)
sklearn.model_selection._split.PredefinedSplit._iter_test_masks(self)
sklearn.model_selection._split.PredefinedSplit.get_n_splits(self,X=None,y=None,groups=None)
sklearn.model_selection._split.PredefinedSplit.split(self,X=None,y=None,groups=None)
sklearn.model_selection._split.RepeatedKFold(self,n_splits=5,n_repeats=10,random_state=None)
sklearn.model_selection._split.RepeatedKFold.__init__(self,n_splits=5,n_repeats=10,random_state=None)
sklearn.model_selection._split.RepeatedStratifiedKFold(self,n_splits=5,n_repeats=10,random_state=None)
sklearn.model_selection._split.RepeatedStratifiedKFold.__init__(self,n_splits=5,n_repeats=10,random_state=None)
sklearn.model_selection._split.ShuffleSplit(BaseShuffleSplit)
sklearn.model_selection._split.ShuffleSplit._iter_indices(self,X,y=None,groups=None)
sklearn.model_selection._split.StratifiedKFold(self,n_splits=3,shuffle=False,random_state=None)
sklearn.model_selection._split.StratifiedKFold.__init__(self,n_splits=3,shuffle=False,random_state=None)
sklearn.model_selection._split.StratifiedKFold._iter_test_masks(self,X,y=None,groups=None)
sklearn.model_selection._split.StratifiedKFold._make_test_folds(self,X,y=None)
sklearn.model_selection._split.StratifiedKFold.split(self,X,y,groups=None)
sklearn.model_selection._split.StratifiedShuffleSplit(self,n_splits=10,test_size='default',train_size=None,random_state=None)
sklearn.model_selection._split.StratifiedShuffleSplit.__init__(self,n_splits=10,test_size='default',train_size=None,random_state=None)
sklearn.model_selection._split.StratifiedShuffleSplit._iter_indices(self,X,y,groups=None)
sklearn.model_selection._split.StratifiedShuffleSplit.split(self,X,y,groups=None)
sklearn.model_selection._split.TimeSeriesSplit(self,n_splits=3,max_train_size=None)
sklearn.model_selection._split.TimeSeriesSplit.__init__(self,n_splits=3,max_train_size=None)
sklearn.model_selection._split.TimeSeriesSplit.split(self,X,y=None,groups=None)
sklearn.model_selection._split._BaseKFold(self,n_splits,shuffle,random_state)
sklearn.model_selection._split._BaseKFold.__init__(self,n_splits,shuffle,random_state)
sklearn.model_selection._split._BaseKFold.get_n_splits(self,X=None,y=None,groups=None)
sklearn.model_selection._split._BaseKFold.split(self,X,y=None,groups=None)
sklearn.model_selection._split._CVIterableWrapper(self,cv)
sklearn.model_selection._split._CVIterableWrapper.__init__(self,cv)
sklearn.model_selection._split._CVIterableWrapper.get_n_splits(self,X=None,y=None,groups=None)
sklearn.model_selection._split._CVIterableWrapper.split(self,X=None,y=None,groups=None)
sklearn.model_selection._split._RepeatedSplits(self,cv,n_repeats=10,random_state=None,**cvargs)
sklearn.model_selection._split._RepeatedSplits.__init__(self,cv,n_repeats=10,random_state=None,**cvargs)
sklearn.model_selection._split._RepeatedSplits.get_n_splits(self,X=None,y=None,groups=None)
sklearn.model_selection._split._RepeatedSplits.split(self,X,y=None,groups=None)
sklearn.model_selection._split._approximate_mode(class_counts,n_draws,rng)
sklearn.model_selection._split._build_repr(self)
sklearn.model_selection._split._validate_shuffle_split(n_samples,test_size,train_size)
sklearn.model_selection._split._validate_shuffle_split_init(test_size,train_size)
sklearn.model_selection._split.check_cv(cv=3,y=None,classifier=False)
sklearn.model_selection._split.train_test_split(*arrays,**options)
sklearn.model_selection.check_cv(cv=3,y=None,classifier=False)
sklearn.model_selection.train_test_split(*arrays,**options)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/model_selection/_validation.py----------------------------------------
A:sklearn.model_selection._validation.(X, y, groups)->indexable(X, y, groups)
A:sklearn.model_selection._validation.cv->check_cv(cv, y, classifier=is_classifier(estimator))
A:sklearn.model_selection._validation.(scorers, _)->_check_multimetric_scoring(estimator, scoring=scoring)
A:sklearn.model_selection._validation.parallel->Parallel(n_jobs=n_jobs, pre_dispatch=pre_dispatch, verbose=verbose)
A:sklearn.model_selection._validation.scores->parallel((delayed(_fit_and_score)(clone(estimator), X, y, scorers, train, test, verbose, None, fit_params, return_train_score=return_train_score, return_times=True) for (train, test) in cv.split(X, y, groups)))
A:sklearn.model_selection._validation.(train_scores, test_scores, fit_times, score_times)->zip(*scores)
A:sklearn.model_selection._validation.train_scores->_score(estimator, X_train, y_train, scorer, is_multimetric)
A:sklearn.model_selection._validation.(test_scores, fit_times, score_times)->zip(*scores)
A:sklearn.model_selection._validation.test_scores->_score(estimator, X_test, y_test, scorer, is_multimetric)
A:sklearn.model_selection._validation.ret['fit_time']->numpy.array(fit_times)
A:sklearn.model_selection._validation.ret['score_time']->numpy.array(score_times)
A:sklearn.model_selection._validation.ret['test_%s' % name]->numpy.array(test_scores[name])
A:sklearn.model_selection._validation.ret[key]->numpy.array(train_scores[name])
A:sklearn.model_selection._validation.message->'You are accessing a training score ({!r}), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True'.format(key)
A:sklearn.model_selection._validation.scorer->check_scoring(estimator, scoring=scoring)
A:sklearn.model_selection._validation.cv_results->cross_validate(estimator=estimator, X=X, y=y, groups=groups, scoring={'score': scorer}, cv=cv, return_train_score=False, n_jobs=n_jobs, verbose=verbose, fit_params=fit_params, pre_dispatch=pre_dispatch)
A:sklearn.model_selection._validation.fit_params->dict([(k, _index_param_value(X, v, train)) for (k, v) in fit_params.items()])
A:sklearn.model_selection._validation.start_time->time.time()
A:sklearn.model_selection._validation.(X_train, y_train)->_safe_split(estimator, X, y, train_subset)
A:sklearn.model_selection._validation.(X_test, y_test)->_safe_split(estimator, X, y, test, train_subset)
A:sklearn.model_selection._validation.score->_permutation_test_score(clone(estimator), X, y, groups, cv, scorer)
A:sklearn.model_selection._validation.le->LabelEncoder()
A:sklearn.model_selection._validation.y->LabelEncoder().fit_transform(y)
A:sklearn.model_selection._validation.prediction_blocks->parallel((delayed(_fit_and_predict)(clone(estimator), X, y, train, test, verbose, fit_params, method) for (train, test) in cv.split(X, y, groups)))
A:sklearn.model_selection._validation.test_indices->numpy.concatenate([indices_i for (_, indices_i) in prediction_blocks])
A:sklearn.model_selection._validation.inv_test_indices->numpy.empty(len(test_indices), dtype=int)
A:sklearn.model_selection._validation.inv_test_indices[test_indices]->numpy.arange(len(test_indices))
A:sklearn.model_selection._validation.predictions->func(X_test)
A:sklearn.model_selection._validation.(X_test, _)->_safe_split(estimator, X, y, test, train)
A:sklearn.model_selection._validation.func->getattr(estimator, method)
A:sklearn.model_selection._validation.n_classes->len(set(y))
A:sklearn.model_selection._validation.predictions_for_all_classes->numpy.full((_num_samples(predictions), n_classes), default_values[method])
A:sklearn.model_selection._validation.hit->numpy.zeros(n_samples, dtype=bool)
A:sklearn.model_selection._validation.v->v.tocsr().tocsr()
A:sklearn.model_selection._validation.random_state->check_random_state(random_state)
A:sklearn.model_selection._validation.permutation_scores->numpy.array(permutation_scores)
A:sklearn.model_selection._validation.indices->numpy.arange(len(groups))
A:sklearn.model_selection._validation.indices[this_mask]->check_random_state(random_state).permutation(indices[this_mask])
A:sklearn.model_selection._validation.cv_iter->list(cv.split(X, y, groups))
A:sklearn.model_selection._validation.n_max_training_samples->len(cv_iter[0][0])
A:sklearn.model_selection._validation.train_sizes_abs->numpy.unique(train_sizes_abs)
A:sklearn.model_selection._validation.rng->check_random_state(random_state)
A:sklearn.model_selection._validation.out->out.reshape(n_cv_folds, n_params, 2).transpose((2, 1, 0)).reshape(n_cv_folds, n_params, 2).transpose((2, 1, 0))
A:sklearn.model_selection._validation.n_min_required_samples->numpy.min(train_sizes_abs)
A:sklearn.model_selection._validation.n_max_required_samples->numpy.max(train_sizes_abs)
A:sklearn.model_selection._validation.partitions->zip(train_sizes, np.split(train, train_sizes)[:-1])
A:sklearn.model_selection._validation.(X_partial_train, y_partial_train)->_safe_split(estimator, X, y, partial_train)
A:sklearn.model_selection._validation.n_params->len(param_range)
A:sklearn.model_selection._validation.out[key]->numpy.asarray([score[key] for score in scores])
sklearn.model_selection._validation._aggregate_score_dicts(scores)
sklearn.model_selection._validation._check_is_permutation(indices,n_samples)
sklearn.model_selection._validation._fit_and_predict(estimator,X,y,train,test,verbose,fit_params,method)
sklearn.model_selection._validation._fit_and_score(estimator,X,y,scorer,train,test,verbose,parameters,fit_params,return_train_score=False,return_parameters=False,return_n_test_samples=False,return_times=False,error_score='raise')
sklearn.model_selection._validation._incremental_fit_estimator(estimator,X,y,classes,train,test,train_sizes,scorer,verbose)
sklearn.model_selection._validation._index_param_value(X,v,indices)
sklearn.model_selection._validation._multimetric_score(estimator,X_test,y_test,scorers)
sklearn.model_selection._validation._permutation_test_score(estimator,X,y,groups,cv,scorer)
sklearn.model_selection._validation._score(estimator,X_test,y_test,scorer,is_multimetric=False)
sklearn.model_selection._validation._shuffle(y,groups,random_state)
sklearn.model_selection._validation._translate_train_sizes(train_sizes,n_max_training_samples)
sklearn.model_selection._validation.cross_val_predict(estimator,X,y=None,groups=None,cv=None,n_jobs=1,verbose=0,fit_params=None,pre_dispatch='2*n_jobs',method='predict')
sklearn.model_selection._validation.cross_val_score(estimator,X,y=None,groups=None,scoring=None,cv=None,n_jobs=1,verbose=0,fit_params=None,pre_dispatch='2*n_jobs')
sklearn.model_selection._validation.cross_validate(estimator,X,y=None,groups=None,scoring=None,cv=None,n_jobs=1,verbose=0,fit_params=None,pre_dispatch='2*n_jobs',return_train_score='warn')
sklearn.model_selection._validation.learning_curve(estimator,X,y,groups=None,train_sizes=np.linspace(0.1,1.0,5),cv=None,scoring=None,exploit_incremental_learning=False,n_jobs=1,pre_dispatch='all',verbose=0,shuffle=False,random_state=None)
sklearn.model_selection._validation.permutation_test_score(estimator,X,y,groups=None,cv=None,n_permutations=100,n_jobs=1,random_state=0,verbose=0,scoring=None)
sklearn.model_selection._validation.validation_curve(estimator,X,y,param_name,param_range,groups=None,cv=None,scoring=None,n_jobs=1,pre_dispatch='all',verbose=0)
sklearn.model_selection.cross_val_predict(estimator,X,y=None,groups=None,cv=None,n_jobs=1,verbose=0,fit_params=None,pre_dispatch='2*n_jobs',method='predict')
sklearn.model_selection.cross_val_score(estimator,X,y=None,groups=None,scoring=None,cv=None,n_jobs=1,verbose=0,fit_params=None,pre_dispatch='2*n_jobs')
sklearn.model_selection.cross_validate(estimator,X,y=None,groups=None,scoring=None,cv=None,n_jobs=1,verbose=0,fit_params=None,pre_dispatch='2*n_jobs',return_train_score='warn')
sklearn.model_selection.learning_curve(estimator,X,y,groups=None,train_sizes=np.linspace(0.1,1.0,5),cv=None,scoring=None,exploit_incremental_learning=False,n_jobs=1,pre_dispatch='all',verbose=0,shuffle=False,random_state=None)
sklearn.model_selection.permutation_test_score(estimator,X,y,groups=None,cv=None,n_permutations=100,n_jobs=1,random_state=0,verbose=0,scoring=None)
sklearn.model_selection.validation_curve(estimator,X,y,param_name,param_range,groups=None,cv=None,scoring=None,n_jobs=1,pre_dispatch='all',verbose=0)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/model_selection/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/model_selection/tests/test_validation.py----------------------------------------
A:sklearn.model_selection.tests.test_validation.X->numpy.arange(200).reshape(100, 2)
A:sklearn.model_selection.tests.test_validation.T->T.reshape(len(T), -1).reshape(len(T), -1)
A:sklearn.model_selection.tests.test_validation.X_sparse->csr_matrix(X)
A:sklearn.model_selection.tests.test_validation.y->LabelEncoder().fit_transform(y)
A:sklearn.model_selection.tests.test_validation.y2->numpy.array([1, 1, 1, 2, 2, 2, 3, 3, 3, 3])
A:sklearn.model_selection.tests.test_validation.P_sparse->coo_matrix(np.eye(5))
A:sklearn.model_selection.tests.test_validation.clf->CheckingClassifier(check_X=check_df, check_y=check_series)
A:sklearn.model_selection.tests.test_validation.scores->numpy.memmap(tf.name, dtype=np.float64)
A:sklearn.model_selection.tests.test_validation.multioutput_y->numpy.column_stack([y, y[::-1]])
A:sklearn.model_selection.tests.test_validation.(X, y)->shuffle(X, y, random_state=0)
A:sklearn.model_selection.tests.test_validation.estimator->PassiveAggressiveClassifier(max_iter=5, tol=None, shuffle=False)
A:sklearn.model_selection.tests.test_validation.multiclass_scorer->make_scorer(precision_recall_fscore_support)
A:sklearn.model_selection.tests.test_validation.multivalued_scorer->make_scorer(confusion_matrix)
A:sklearn.model_selection.tests.test_validation.cv->GroupKFold(n_splits=2)
A:sklearn.model_selection.tests.test_validation.(X_reg, y_reg)->make_regression(n_samples=30, random_state=0)
A:sklearn.model_selection.tests.test_validation.reg->Ridge()
A:sklearn.model_selection.tests.test_validation.(X_clf, y_clf)->make_classification(n_samples=30, random_state=0)
A:sklearn.model_selection.tests.test_validation.mse_scorer->check_scoring(est, 'neg_mean_squared_error')
A:sklearn.model_selection.tests.test_validation.r2_scorer->check_scoring(est, 'r2')
A:sklearn.model_selection.tests.test_validation.est->LogisticRegression()
A:sklearn.model_selection.tests.test_validation.train_mse_scores->numpy.array(train_mse_scores)
A:sklearn.model_selection.tests.test_validation.test_mse_scores->numpy.array(test_mse_scores)
A:sklearn.model_selection.tests.test_validation.train_r2_scores->numpy.array(train_r2_scores)
A:sklearn.model_selection.tests.test_validation.test_r2_scores->numpy.array(test_r2_scores)
A:sklearn.model_selection.tests.test_validation.result[val]->assert_no_warnings(cross_validate, estimator, X, y, return_train_score=val)
A:sklearn.model_selection.tests.test_validation.msg->'You are accessing a training score ({!r}), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True'.format('train_score')
A:sklearn.model_selection.tests.test_validation.train_score->assert_warns_message(FutureWarning, msg, result['warn'].get, 'train_score')
A:sklearn.model_selection.tests.test_validation.mse_scores_dict->cross_validate(clf, X, y, cv=5, scoring='neg_mean_squared_error', return_train_score=False)
A:sklearn.model_selection.tests.test_validation.r2_scores_dict->cross_validate(clf, X, y, cv=5, scoring=['r2'], return_train_score=False)
A:sklearn.model_selection.tests.test_validation.keys_sans_train->set(('test_r2', 'test_neg_mean_squared_error', 'fit_time', 'score_time'))
A:sklearn.model_selection.tests.test_validation.keys_with_train->set(('test_r2', 'test_neg_mean_squared_error', 'fit_time', 'score_time')).union(set(('train_r2', 'train_neg_mean_squared_error')))
A:sklearn.model_selection.tests.test_validation.cv_results->cross_validate(clf, X, y, cv=5, scoring=scoring, return_train_score=False)
A:sklearn.model_selection.tests.test_validation.svm->SVC(kernel='linear')
A:sklearn.model_selection.tests.test_validation.iris->load_iris()
A:sklearn.model_selection.tests.test_validation.kfold->KFold()
A:sklearn.model_selection.tests.test_validation.scores_indices->cross_val_score(svm, X, y, cv=kfold)
A:sklearn.model_selection.tests.test_validation.mask_train->numpy.zeros(len(y), dtype=np.bool)
A:sklearn.model_selection.tests.test_validation.mask_test->numpy.zeros(len(y), dtype=np.bool)
A:sklearn.model_selection.tests.test_validation.scores_masks->cross_val_score(svm, X, y, cv=cv_masks)
A:sklearn.model_selection.tests.test_validation.linear_kernel->numpy.dot(X, X.T)
A:sklearn.model_selection.tests.test_validation.score_precomputed->cross_val_score(svm, linear_kernel, y)
A:sklearn.model_selection.tests.test_validation.score_linear->cross_val_score(svm, X, y)
A:sklearn.model_selection.tests.test_validation.score_callable->cross_val_score(svm, X, y)
A:sklearn.model_selection.tests.test_validation.n_classes->len(np.unique(y))
A:sklearn.model_selection.tests.test_validation.W_sparse->coo_matrix((np.array([1]), (np.array([1]), np.array([0]))), shape=(10, 1))
A:sklearn.model_selection.tests.test_validation.DUMMY_OBJ->object()
A:sklearn.model_selection.tests.test_validation.scoring->make_scorer(explained_variance_score)
A:sklearn.model_selection.tests.test_validation.score->numpy.memmap(tf.name, shape=(), mode='r', dtype=np.float64)
A:sklearn.model_selection.tests.test_validation.zo_scores->cross_val_score(clf, iris.data, iris.target, scoring='accuracy', cv=5)
A:sklearn.model_selection.tests.test_validation.f1_scores->cross_val_score(clf, iris.data, iris.target, scoring='f1_weighted', cv=5)
A:sklearn.model_selection.tests.test_validation.r2_scores->cross_val_score(reg, X, y, scoring='r2', cv=5)
A:sklearn.model_selection.tests.test_validation.neg_mse_scores->cross_val_score(reg, X, y, cv=5, scoring='neg_mean_squared_error')
A:sklearn.model_selection.tests.test_validation.expected_neg_mse->numpy.array([-763.07, -553.16, -274.38, -273.26, -1681.99])
A:sklearn.model_selection.tests.test_validation.ev_scores->cross_val_score(reg, X, y, cv=5, scoring=scoring)
A:sklearn.model_selection.tests.test_validation.(score, scores, pvalue)->permutation_test_score(svm, X, y, n_permutations=30, cv=cv, scoring='accuracy')
A:sklearn.model_selection.tests.test_validation.(score_group, _, pvalue_group)->permutation_test_score(svm_sparse, X_sparse, y, n_permutations=30, cv=cv_sparse, scoring='accuracy', groups=np.ones(y.size), random_state=0)
A:sklearn.model_selection.tests.test_validation.svm_sparse->SVC(kernel='linear')
A:sklearn.model_selection.tests.test_validation.cv_sparse->StratifiedKFold(2)
A:sklearn.model_selection.tests.test_validation.scorer->make_scorer(custom_score)
A:sklearn.model_selection.tests.test_validation.(score, _, pvalue)->permutation_test_score(svm, X, y, n_permutations=100, scoring=scorer, cv=cv, random_state=0)
A:sklearn.model_selection.tests.test_validation.p->numpy.arange(100)
A:sklearn.model_selection.tests.test_validation.scoring_micro->make_scorer(precision_score, average='micro')
A:sklearn.model_selection.tests.test_validation.scoring_macro->make_scorer(precision_score, average='macro')
A:sklearn.model_selection.tests.test_validation.scoring_samples->make_scorer(precision_score, average='samples')
A:sklearn.model_selection.tests.test_validation.score_micro->cross_val_score(clf, X, y, scoring=scoring_micro, cv=5)
A:sklearn.model_selection.tests.test_validation.score_macro->cross_val_score(clf, X, y, scoring=scoring_macro, cv=5)
A:sklearn.model_selection.tests.test_validation.score_samples->cross_val_score(clf, X, y, scoring=scoring_samples, cv=5)
A:sklearn.model_selection.tests.test_validation.boston->load_boston()
A:sklearn.model_selection.tests.test_validation.preds2->numpy.zeros_like(y)
A:sklearn.model_selection.tests.test_validation.preds2[test]->LogisticRegression().predict(X[test])
A:sklearn.model_selection.tests.test_validation.preds->cross_val_predict(classif, X, y, cv=10)
A:sklearn.model_selection.tests.test_validation.Xsp->coo_matrix(Xsp)
A:sklearn.model_selection.tests.test_validation.ind->numpy.argsort(y)
A:sklearn.model_selection.tests.test_validation.predictions->cross_val_predict(est, X, y, method=method, cv=kfold3)
A:sklearn.model_selection.tests.test_validation.a->cross_val_score(clf, X, y, fit_params=fit_params)
A:sklearn.model_selection.tests.test_validation.(train_sizes, train_scores, test_scores)->learning_curve(estimator, X, y, cv=cv, train_sizes=np.linspace(0.1, 1.0, 10))
A:sklearn.model_selection.tests.test_validation.(train_sizes2, train_scores2, test_scores2)->learning_curve(estimator, X, y, cv=OneTimeSplitter(n_splits=n_splits, n_samples=n_samples), train_sizes=np.linspace(0.1, 1.0, 10), shuffle=shuffle_train)
A:sklearn.model_selection.tests.test_validation.(X, _)->make_classification(n_samples=30, n_features=1, n_informative=1, n_redundant=0, n_classes=2, n_clusters_per_class=1, random_state=0)
A:sklearn.model_selection.tests.test_validation.sys.stdout->StringIO()
A:sklearn.model_selection.tests.test_validation.out->sys.stdout.getvalue()
A:sklearn.model_selection.tests.test_validation.train_sizes->numpy.linspace(0.2, 1.0, 5)
A:sklearn.model_selection.tests.test_validation.(train_sizes_inc, train_scores_inc, test_scores_inc)->learning_curve(estimator, X, y, cv=cv, n_jobs=1, train_sizes=np.linspace(0.3, 1.0, 3), groups=groups, shuffle=True, random_state=2, exploit_incremental_learning=True)
A:sklearn.model_selection.tests.test_validation.(train_sizes_batch, train_scores_batch, test_scores_batch)->learning_curve(estimator, X, y, cv=cv, n_jobs=1, train_sizes=np.linspace(0.3, 1.0, 3), groups=groups, shuffle=True, random_state=2)
A:sklearn.model_selection.tests.test_validation.(train_sizes, _, _)->assert_warns(RuntimeWarning, learning_curve, estimator, X, y, cv=3, train_sizes=np.linspace(0.33, 1.0, 3))
A:sklearn.model_selection.tests.test_validation.groups->numpy.array([1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 4, 4, 4, 4])
A:sklearn.model_selection.tests.test_validation.param_range->numpy.linspace(1, 0, 10)
A:sklearn.model_selection.tests.test_validation.(train_scores, test_scores)->validation_curve(MockEstimatorWithParameter(), X, y, param_name='param', param_range=param_range, cv=2)
A:sklearn.model_selection.tests.test_validation.(_, _)->validation_curve(MockEstimatorWithSingleFitCallAllowed(), X, y, param_name='param', param_range=param_range, cv=2)
A:sklearn.model_selection.tests.test_validation.scores1->validation_curve(SVC(kernel='linear', random_state=0), X, y, 'C', [0.1, 0.1, 0.2, 0.2], cv=OneTimeSplitter(n_splits=n_splits, n_samples=n_samples))
A:sklearn.model_selection.tests.test_validation.scores2->validation_curve(SVC(kernel='linear', random_state=0), X, y, 'C', [0.1, 0.1, 0.2, 0.2], cv=KFold(n_splits=n_splits, shuffle=True))
A:sklearn.model_selection.tests.test_validation.scores3->validation_curve(SVC(kernel='linear', random_state=0), X, y, 'C', [0.1, 0.1, 0.2, 0.2], cv=KFold(n_splits=n_splits))
A:sklearn.model_selection.tests.test_validation.rng->numpy.random.RandomState(0)
A:sklearn.model_selection.tests.test_validation.y_sparse->csr_matrix(y)
A:sklearn.model_selection.tests.test_validation.classif->OneVsRestClassifier(SVC(kernel='linear'))
A:sklearn.model_selection.tests.test_validation.preds_sparse->preds_sparse.toarray().toarray()
A:sklearn.model_selection.tests.test_validation.classes->len(set(y))
A:sklearn.model_selection.tests.test_validation.expected_predictions->get_expected_predictions(X, y, kfold3, classes, est, method)
A:sklearn.model_selection.tests.test_validation.func->getattr(est, method)
A:sklearn.model_selection.tests.test_validation.expected_predictions[test]->func(X[test])
A:sklearn.model_selection.tests.test_validation.predictions_y1->cross_val_predict(est, X, y + 1, method=method, cv=kfold)
A:sklearn.model_selection.tests.test_validation.predictions_y2->cross_val_predict(est, X, y - 2, method=method, cv=kfold)
A:sklearn.model_selection.tests.test_validation.predictions_ystr->cross_val_predict(est, X, y.astype('str'), method=method, cv=kfold)
A:sklearn.model_selection.tests.test_validation.expected_predictions_->func(X[test])
A:sklearn.model_selection.tests.test_validation.exp_pred_test->numpy.full((len(test), classes), np.finfo(expected_predictions.dtype).min)
A:sklearn.model_selection.tests.test_validation.kfold3->KFold(n_splits=3)
A:sklearn.model_selection.tests.test_validation.kfold4->KFold(n_splits=4)
A:sklearn.model_selection.tests.test_validation.le->LabelEncoder()
A:sklearn.model_selection.tests.test_validation.tf->tempfile.NamedTemporaryFile(mode='wb', delete=False)
sklearn.model_selection.tests.test_validation.MockClassifier(self,a=0,allow_nd=False)
sklearn.model_selection.tests.test_validation.MockClassifier.__init__(self,a=0,allow_nd=False)
sklearn.model_selection.tests.test_validation.MockClassifier.fit(self,X,Y=None,sample_weight=None,class_prior=None,sparse_sample_weight=None,sparse_param=None,dummy_int=None,dummy_str=None,dummy_obj=None,callback=None)
sklearn.model_selection.tests.test_validation.MockClassifier.get_params(self,deep=False)
sklearn.model_selection.tests.test_validation.MockClassifier.predict(self,T)
sklearn.model_selection.tests.test_validation.MockClassifier.score(self,X=None,Y=None)
sklearn.model_selection.tests.test_validation.MockEstimatorWithParameter(self,param=0.5)
sklearn.model_selection.tests.test_validation.MockEstimatorWithParameter.__init__(self,param=0.5)
sklearn.model_selection.tests.test_validation.MockEstimatorWithParameter._is_training_data(self,X)
sklearn.model_selection.tests.test_validation.MockEstimatorWithParameter.fit(self,X_subset,y_subset)
sklearn.model_selection.tests.test_validation.MockEstimatorWithParameter.predict(self,X)
sklearn.model_selection.tests.test_validation.MockEstimatorWithParameter.score(self,X=None,y=None)
sklearn.model_selection.tests.test_validation.MockEstimatorWithSingleFitCallAllowed(MockEstimatorWithParameter)
sklearn.model_selection.tests.test_validation.MockEstimatorWithSingleFitCallAllowed.fit(self,X_subset,y_subset)
sklearn.model_selection.tests.test_validation.MockEstimatorWithSingleFitCallAllowed.predict(self,X)
sklearn.model_selection.tests.test_validation.MockImprovingEstimator(self,n_max_train_sizes)
sklearn.model_selection.tests.test_validation.MockImprovingEstimator.__init__(self,n_max_train_sizes)
sklearn.model_selection.tests.test_validation.MockImprovingEstimator._is_training_data(self,X)
sklearn.model_selection.tests.test_validation.MockImprovingEstimator.fit(self,X_subset,y_subset=None)
sklearn.model_selection.tests.test_validation.MockImprovingEstimator.predict(self,X)
sklearn.model_selection.tests.test_validation.MockImprovingEstimator.score(self,X=None,Y=None)
sklearn.model_selection.tests.test_validation.MockIncrementalImprovingEstimator(self,n_max_train_sizes)
sklearn.model_selection.tests.test_validation.MockIncrementalImprovingEstimator.__init__(self,n_max_train_sizes)
sklearn.model_selection.tests.test_validation.MockIncrementalImprovingEstimator._is_training_data(self,X)
sklearn.model_selection.tests.test_validation.MockIncrementalImprovingEstimator.partial_fit(self,X,y=None,**params)
sklearn.model_selection.tests.test_validation.check_cross_val_predict_with_method(est)
sklearn.model_selection.tests.test_validation.check_cross_validate_multi_metric(clf,X,y,scores)
sklearn.model_selection.tests.test_validation.check_cross_validate_single_metric(clf,X,y,scores)
sklearn.model_selection.tests.test_validation.get_expected_predictions(X,y,cv,classes,est,method)
sklearn.model_selection.tests.test_validation.test_check_is_permutation()
sklearn.model_selection.tests.test_validation.test_cross_val_predict()
sklearn.model_selection.tests.test_validation.test_cross_val_predict_class_subset()
sklearn.model_selection.tests.test_validation.test_cross_val_predict_decision_function_shape()
sklearn.model_selection.tests.test_validation.test_cross_val_predict_input_types()
sklearn.model_selection.tests.test_validation.test_cross_val_predict_method_checking()
sklearn.model_selection.tests.test_validation.test_cross_val_predict_pandas()
sklearn.model_selection.tests.test_validation.test_cross_val_predict_predict_log_proba_shape()
sklearn.model_selection.tests.test_validation.test_cross_val_predict_predict_proba_shape()
sklearn.model_selection.tests.test_validation.test_cross_val_predict_sparse_prediction()
sklearn.model_selection.tests.test_validation.test_cross_val_predict_with_method()
sklearn.model_selection.tests.test_validation.test_cross_val_score()
sklearn.model_selection.tests.test_validation.test_cross_val_score_allow_nans()
sklearn.model_selection.tests.test_validation.test_cross_val_score_errors()
sklearn.model_selection.tests.test_validation.test_cross_val_score_fit_params()
sklearn.model_selection.tests.test_validation.test_cross_val_score_mask()
sklearn.model_selection.tests.test_validation.test_cross_val_score_multilabel()
sklearn.model_selection.tests.test_validation.test_cross_val_score_pandas()
sklearn.model_selection.tests.test_validation.test_cross_val_score_precomputed()
sklearn.model_selection.tests.test_validation.test_cross_val_score_predict_groups()
sklearn.model_selection.tests.test_validation.test_cross_val_score_score_func()
sklearn.model_selection.tests.test_validation.test_cross_val_score_sparse_fit_params()
sklearn.model_selection.tests.test_validation.test_cross_val_score_with_score_func_classification()
sklearn.model_selection.tests.test_validation.test_cross_val_score_with_score_func_regression()
sklearn.model_selection.tests.test_validation.test_cross_validate()
sklearn.model_selection.tests.test_validation.test_cross_validate_invalid_scoring_param()
sklearn.model_selection.tests.test_validation.test_cross_validate_return_train_score_warn()
sklearn.model_selection.tests.test_validation.test_gridsearchcv_cross_val_predict_with_method()
sklearn.model_selection.tests.test_validation.test_learning_curve()
sklearn.model_selection.tests.test_validation.test_learning_curve_batch_and_incremental_learning_are_equal()
sklearn.model_selection.tests.test_validation.test_learning_curve_incremental_learning()
sklearn.model_selection.tests.test_validation.test_learning_curve_incremental_learning_not_possible()
sklearn.model_selection.tests.test_validation.test_learning_curve_incremental_learning_unsupervised()
sklearn.model_selection.tests.test_validation.test_learning_curve_n_sample_range_out_of_bounds()
sklearn.model_selection.tests.test_validation.test_learning_curve_remove_duplicate_sample_sizes()
sklearn.model_selection.tests.test_validation.test_learning_curve_unsupervised()
sklearn.model_selection.tests.test_validation.test_learning_curve_verbose()
sklearn.model_selection.tests.test_validation.test_learning_curve_with_boolean_indices()
sklearn.model_selection.tests.test_validation.test_learning_curve_with_shuffle()
sklearn.model_selection.tests.test_validation.test_permutation_score()
sklearn.model_selection.tests.test_validation.test_permutation_test_score_allow_nans()
sklearn.model_selection.tests.test_validation.test_permutation_test_score_pandas()
sklearn.model_selection.tests.test_validation.test_score_memmap()
sklearn.model_selection.tests.test_validation.test_validation_curve()
sklearn.model_selection.tests.test_validation.test_validation_curve_clone_estimator()
sklearn.model_selection.tests.test_validation.test_validation_curve_cv_splits_consistency()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/model_selection/tests/test_split.py----------------------------------------
A:sklearn.model_selection.tests.test_split.X->numpy.zeros((6, 1))
A:sklearn.model_selection.tests.test_split.P_sparse->coo_matrix(np.eye(5))
A:sklearn.model_selection.tests.test_split.digits->load_digits()
A:sklearn.model_selection.tests.test_split.T->T.reshape(len(T), -1).reshape(len(T), -1)
A:sklearn.model_selection.tests.test_split.X_1d->numpy.array([1, 2, 3, 4])
A:sklearn.model_selection.tests.test_split.y->numpy.repeat([0, 1], X.shape[0] / 2)
A:sklearn.model_selection.tests.test_split.groups->numpy.random.RandomState(0).randint(0, 5, 15)
A:sklearn.model_selection.tests.test_split.loo->LeaveOneOut()
A:sklearn.model_selection.tests.test_split.lpo->LeavePOut(p)
A:sklearn.model_selection.tests.test_split.kf->KFold(3, shuffle=True)
A:sklearn.model_selection.tests.test_split.skf->StratifiedKFold(3, shuffle=True)
A:sklearn.model_selection.tests.test_split.lolo->LeaveOneGroupOut().split(X, groups=groups)
A:sklearn.model_selection.tests.test_split.lopo->LeavePGroupsOut(p)
A:sklearn.model_selection.tests.test_split.ss->ShuffleSplit(random_state=21)
A:sklearn.model_selection.tests.test_split.ps->PredefinedSplit(folds)
A:sklearn.model_selection.tests.test_split.rng->numpy.random.RandomState(0)
A:sklearn.model_selection.tests.test_split.y_2d->numpy.repeat([0, 1], X.shape[0] / 2).reshape(-1, 1)
A:sklearn.model_selection.tests.test_split.y_multilabel->numpy.array([[0, 0, 0, 0], [0, 1, 1, 0], [0, 0, 0, 1], [1, 1, 0, 1], [0, 0, 1, 0]])
A:sklearn.model_selection.tests.test_split.msg->'The groups parameter contains fewer than (or equal to) n_groups (3) numbers of unique groups ({}). LeavePGroupsOut expects that at least n_groups + 1 (4) unique groups be present'.format(groups)
A:sklearn.model_selection.tests.test_split.n_samples->len(groups)
A:sklearn.model_selection.tests.test_split.expected_n_splits->OldSKF(y_multiclass, n_folds=3).get_n_splits(X, y, groups)
A:sklearn.model_selection.tests.test_split.collected_test_samples->set()
A:sklearn.model_selection.tests.test_split.X1->numpy.ones(18)
A:sklearn.model_selection.tests.test_split.X2->numpy.ones(16)
A:sklearn.model_selection.tests.test_split.skf_3->StratifiedKFold(3)
A:sklearn.model_selection.tests.test_split.splits->TimeSeriesSplit(n_splits=3).split(X)
A:sklearn.model_selection.tests.test_split.(train, test)->next(splits)
A:sklearn.model_selection.tests.test_split.cv->OldSKF(y_multiclass, n_folds=3)
A:sklearn.model_selection.tests.test_split.kf2->KFold(3, shuffle=True, random_state=0)
A:sklearn.model_selection.tests.test_split.kf3->KFold(3, shuffle=True, random_state=1)
A:sklearn.model_selection.tests.test_split.all_folds->numpy.zeros(300)
A:sklearn.model_selection.tests.test_split.X_40->numpy.ones(40)
A:sklearn.model_selection.tests.test_split.kf0->StratifiedKFold(5, shuffle=True, random_state=0)
A:sklearn.model_selection.tests.test_split.kf1->StratifiedKFold(5, shuffle=True, random_state=1)
A:sklearn.model_selection.tests.test_split.model->SVC(C=10, gamma=0.005)
A:sklearn.model_selection.tests.test_split.mean_score->cross_val_score(model, X, y, cv=cv).mean()
A:sklearn.model_selection.tests.test_split.ss1->ShuffleSplit(test_size=0.2, random_state=0).split(X)
A:sklearn.model_selection.tests.test_split.ss2->ShuffleSplit(test_size=2, random_state=0).split(X)
A:sklearn.model_selection.tests.test_split.ss3->ShuffleSplit(test_size=np.int32(2), random_state=0).split(X)
A:sklearn.model_selection.tests.test_split.ss4->ShuffleSplit(test_size=typ(2), random_state=0).split(X)
A:sklearn.model_selection.tests.test_split.sss->StratifiedShuffleSplit(test_size=2, random_state=42)
A:sklearn.model_selection.tests.test_split.test_size->numpy.ceil(0.33 * len(y))
A:sklearn.model_selection.tests.test_split.bf->scipy.stats.binom(n_splits, p)
A:sklearn.model_selection.tests.test_split.prob->scipy.stats.binom(n_splits, p).pmf(count)
A:sklearn.model_selection.tests.test_split.(n_train, n_test)->_validate_shuffle_split(n_samples, test_size=1.0 / n_folds, train_size=1.0 - 1.0 / n_folds)
A:sklearn.model_selection.tests.test_split.group_counts->numpy.unique(groups)
A:sklearn.model_selection.tests.test_split.expected_ratio->numpy.mean(y[:, 4])
A:sklearn.model_selection.tests.test_split.Xy->numpy.ones(len(groups))
A:sklearn.model_selection.tests.test_split.slo->GroupShuffleSplit(n_splits, test_size=test_size, random_state=0)
A:sklearn.model_selection.tests.test_split.l_unique->numpy.unique(groups_i)
A:sklearn.model_selection.tests.test_split.l->numpy.asarray(groups_i)
A:sklearn.model_selection.tests.test_split.l_train_unique->numpy.unique(l[train])
A:sklearn.model_selection.tests.test_split.l_test_unique->numpy.unique(l[test])
A:sklearn.model_selection.tests.test_split.logo->LeaveOneGroupOut()
A:sklearn.model_selection.tests.test_split.lpgo_1->LeavePGroupsOut(n_groups=1)
A:sklearn.model_selection.tests.test_split.lpgo_2->LeavePGroupsOut(n_groups=2)
A:sklearn.model_selection.tests.test_split.n_groups->len(np.unique(groups))
A:sklearn.model_selection.tests.test_split.groups_arr->numpy.asarray(groups_i)
A:sklearn.model_selection.tests.test_split.groups_changing->numpy.array(groups, copy=True)
A:sklearn.model_selection.tests.test_split.lolo_changing->LeaveOneGroupOut().split(X, groups=groups)
A:sklearn.model_selection.tests.test_split.lplo->LeavePGroupsOut(n_groups=2).split(X, groups=groups)
A:sklearn.model_selection.tests.test_split.lplo_changing->LeavePGroupsOut(n_groups=2).split(X, groups=groups)
A:sklearn.model_selection.tests.test_split.Xygroups->numpy.arange(3)
A:sklearn.model_selection.tests.test_split.rkf->RepeatedKFold(n_splits, n_repeats)
A:sklearn.model_selection.tests.test_split.rskf->RepeatedStratifiedKFold(n_splits=2, n_repeats=2, random_state=random_state)
A:sklearn.model_selection.tests.test_split.X_s->InputFeatureType(X)
A:sklearn.model_selection.tests.test_split.split->train_test_split(X_4d, y_3d)
A:sklearn.model_selection.tests.test_split.X_4d->numpy.arange(10 * 5 * 3 * 2).reshape(10, 5, 3, 2)
A:sklearn.model_selection.tests.test_split.y_3d->numpy.arange(10 * 7 * 11).reshape(10, 7, 11)
A:sklearn.model_selection.tests.test_split.X_df->MockDataFrame(X)
A:sklearn.model_selection.tests.test_split.(X_train, X_test)->train_test_split(X_df)
A:sklearn.model_selection.tests.test_split.(X_train_arr, X_test_arr)->train_test_split(X_df)
A:sklearn.model_selection.tests.test_split.y2->numpy.hstack((np.ones(4), np.zeros(3)))
A:sklearn.model_selection.tests.test_split.y3->numpy.hstack((np.ones(4), np.zeros(3))).tolist()
A:sklearn.model_selection.tests.test_split.(X_train1, X_test1, y_train1, y_test1)->train_test_split(X, y1, stratify=y1 if stratify else None, random_state=0)
A:sklearn.model_selection.tests.test_split.(X_train2, X_test2, y_train2, y_test2)->train_test_split(X, y2, stratify=y2 if stratify else None, random_state=0)
A:sklearn.model_selection.tests.test_split.(X_train3, X_test3, y_train3, y_test3)->train_test_split(X, y3, stratify=y3 if stratify else None, random_state=0)
A:sklearn.model_selection.tests.test_split.y_binary->numpy.array([0, 1, 0, 1, 0, 0, 1, 1, 1])
A:sklearn.model_selection.tests.test_split.y_multiclass->numpy.array([0, 1, 0, 1, 2, 1, 2, 0, 2])
A:sklearn.model_selection.tests.test_split.y_multiclass_2d->numpy.array([0, 1, 0, 1, 2, 1, 2, 0, 2]).reshape(-1, 1)
A:sklearn.model_selection.tests.test_split.y_multioutput->numpy.array([[1, 2], [0, 3], [0, 0], [3, 1], [2, 0]])
A:sklearn.model_selection.tests.test_split.cv1->check_cv(3, y_multiclass, classifier=True)
A:sklearn.model_selection.tests.test_split.cv2->check_cv(OldSKF(y_multiclass, n_folds=3))
A:sklearn.model_selection.tests.test_split.wrapped_old_skf->_CVIterableWrapper(cv)
A:sklearn.model_selection.tests.test_split.kf_iter->KFold(n_splits=5).split(X, y)
A:sklearn.model_selection.tests.test_split.kf_iter_wrapped->check_cv(kf_iter)
A:sklearn.model_selection.tests.test_split.kf_randomized_iter->KFold(n_splits=5, shuffle=True).split(X, y)
A:sklearn.model_selection.tests.test_split.kf_randomized_iter_wrapped->check_cv(kf_randomized_iter)
A:sklearn.model_selection.tests.test_split.folds->numpy.zeros(n_samples)
A:sklearn.model_selection.tests.test_split.lkf->GroupKFold(n_splits=n_splits)
A:sklearn.model_selection.tests.test_split.cv_iter->list(lkf.split(X, y, groups.tolist()))
A:sklearn.model_selection.tests.test_split.tscv->TimeSeriesSplit(2)
A:sklearn.model_selection.tests.test_split.n_splits_actual->len(list(splits))
A:sklearn.model_selection.tests.test_split.suffix_start->max(len(train) - max_train_size, 0)
A:sklearn.model_selection.tests.test_split.check_splits->TimeSeriesSplit(n_splits=3, max_train_size=5).split(X)
A:sklearn.model_selection.tests.test_split.(X, y)->make_classification(n_samples=15, n_classes=2, random_state=0)
A:sklearn.model_selection.tests.test_split.gs->GridSearchCV(Ridge(), param_grid={'alpha': [1, 0.1]}, cv=inner_cv)
sklearn.model_selection.tests.test_split.MockClassifier(self,a=0,allow_nd=False)
sklearn.model_selection.tests.test_split.MockClassifier.__init__(self,a=0,allow_nd=False)
sklearn.model_selection.tests.test_split.MockClassifier.fit(self,X,Y=None,sample_weight=None,class_prior=None,sparse_sample_weight=None,sparse_param=None,dummy_int=None,dummy_str=None,dummy_obj=None,callback=None)
sklearn.model_selection.tests.test_split.MockClassifier.get_params(self,deep=False)
sklearn.model_selection.tests.test_split.MockClassifier.predict(self,T)
sklearn.model_selection.tests.test_split.MockClassifier.score(self,X=None,Y=None)
sklearn.model_selection.tests.test_split._check_time_series_max_train_size(splits,check_splits,max_train_size)
sklearn.model_selection.tests.test_split.check_cv_coverage(cv,X,y,groups,expected_n_splits=None)
sklearn.model_selection.tests.test_split.check_valid_split(train,test,n_samples=None)
sklearn.model_selection.tests.test_split.test_2d_y()
sklearn.model_selection.tests.test_split.test_build_repr()
sklearn.model_selection.tests.test_split.test_check_cv()
sklearn.model_selection.tests.test_split.test_cross_validator_with_default_params()
sklearn.model_selection.tests.test_split.test_cv_iterable_wrapper()
sklearn.model_selection.tests.test_split.test_get_n_splits_for_repeated_kfold()
sklearn.model_selection.tests.test_split.test_get_n_splits_for_repeated_stratified_kfold()
sklearn.model_selection.tests.test_split.test_group_kfold()
sklearn.model_selection.tests.test_split.test_group_shuffle_split()
sklearn.model_selection.tests.test_split.test_kfold_balance()
sklearn.model_selection.tests.test_split.test_kfold_can_detect_dependent_samples_on_digits()
sklearn.model_selection.tests.test_split.test_kfold_indices()
sklearn.model_selection.tests.test_split.test_kfold_no_shuffle()
sklearn.model_selection.tests.test_split.test_kfold_valueerrors()
sklearn.model_selection.tests.test_split.test_leave_group_out_changing_groups()
sklearn.model_selection.tests.test_split.test_leave_one_p_group_out()
sklearn.model_selection.tests.test_split.test_leave_one_p_group_out_error_on_fewer_number_of_groups()
sklearn.model_selection.tests.test_split.test_nested_cv()
sklearn.model_selection.tests.test_split.test_predefinedsplit_with_kfold_split()
sklearn.model_selection.tests.test_split.test_repeated_cv_value_errors()
sklearn.model_selection.tests.test_split.test_repeated_kfold_determinstic_split()
sklearn.model_selection.tests.test_split.test_repeated_stratified_kfold_determinstic_split()
sklearn.model_selection.tests.test_split.test_shuffle_kfold()
sklearn.model_selection.tests.test_split.test_shuffle_kfold_stratifiedkfold_reproducibility()
sklearn.model_selection.tests.test_split.test_shuffle_split()
sklearn.model_selection.tests.test_split.test_shuffle_stratifiedkfold()
sklearn.model_selection.tests.test_split.test_shufflesplit_errors()
sklearn.model_selection.tests.test_split.test_shufflesplit_reproducible()
sklearn.model_selection.tests.test_split.test_stratified_kfold_no_shuffle()
sklearn.model_selection.tests.test_split.test_stratified_kfold_ratios()
sklearn.model_selection.tests.test_split.test_stratified_shuffle_split_even()
sklearn.model_selection.tests.test_split.test_stratified_shuffle_split_init()
sklearn.model_selection.tests.test_split.test_stratified_shuffle_split_iter()
sklearn.model_selection.tests.test_split.test_stratified_shuffle_split_multilabel()
sklearn.model_selection.tests.test_split.test_stratified_shuffle_split_multilabel_many_labels()
sklearn.model_selection.tests.test_split.test_stratified_shuffle_split_overlap_train_test_bug()
sklearn.model_selection.tests.test_split.test_stratified_shuffle_split_respects_test_size()
sklearn.model_selection.tests.test_split.test_stratifiedkfold_balance()
sklearn.model_selection.tests.test_split.test_stratifiedshufflesplit_list_input()
sklearn.model_selection.tests.test_split.test_time_series_cv()
sklearn.model_selection.tests.test_split.test_time_series_max_train_size()
sklearn.model_selection.tests.test_split.test_train_test_default_warning()
sklearn.model_selection.tests.test_split.test_train_test_split()
sklearn.model_selection.tests.test_split.test_train_test_split_allow_nans()
sklearn.model_selection.tests.test_split.test_train_test_split_errors()
sklearn.model_selection.tests.test_split.train_test_split_list_input()
sklearn.model_selection.tests.test_split.train_test_split_mock_pandas()
sklearn.model_selection.tests.test_split.train_test_split_pandas()
sklearn.model_selection.tests.test_split.train_test_split_sparse()
sklearn.model_selection.tests.testcheck_cv_coverage(cv,X,y,groups,expected_n_splits=None)
sklearn.model_selection.tests.testtrain_test_split_list_input()
sklearn.model_selection.tests.testtrain_test_split_mock_pandas()
sklearn.model_selection.tests.testtrain_test_split_pandas()
sklearn.model_selection.tests.testtrain_test_split_sparse()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/model_selection/tests/test_search.py----------------------------------------
A:sklearn.model_selection.tests.test_search.self.classes_->numpy.unique(Y)
A:sklearn.model_selection.tests.test_search.X->numpy.arange(6).reshape(6, -1)
A:sklearn.model_selection.tests.test_search.y->numpy.array([0] * 5 + [1] * 5)
A:sklearn.model_selection.tests.test_search.grid1->ParameterGrid(params1)
A:sklearn.model_selection.tests.test_search.grid2->ParameterGrid(params2)
A:sklearn.model_selection.tests.test_search.points->set((tuple(chain(*sorted(p.items()))) for p in grid2))
A:sklearn.model_selection.tests.test_search.empty->ParameterGrid({})
A:sklearn.model_selection.tests.test_search.has_empty->ParameterGrid([{'C': [1, 10]}, {}, {'C': [0.5]}])
A:sklearn.model_selection.tests.test_search.clf->MockClassifier()
A:sklearn.model_selection.tests.test_search.grid_search->GridSearchCV(clf, {'foo_param': [1, 2, 3]}, verbose=3)
A:sklearn.model_selection.tests.test_search.sys.stdout->StringIO()
A:sklearn.model_selection.tests.test_search.searcher->klass(clf, {'foo_param': [1, 2, 3]}, cv=2, **klass_kwargs)
A:sklearn.model_selection.tests.test_search.(X, y)->make_classification(n_samples=n_samples, random_state=0)
A:sklearn.model_selection.tests.test_search.clf_no_score->LinearSVCNoScore(random_state=0)
A:sklearn.model_selection.tests.test_search.grid_search_no_score->GridSearchCV(clf_no_score, {'C': Cs})
A:sklearn.model_selection.tests.test_search.search_no_scoring->GridSearchCV(clf, grid, scoring=None).fit(X, y)
A:sklearn.model_selection.tests.test_search.search_accuracy->GridSearchCV(clf, grid, scoring='accuracy').fit(X, y)
A:sklearn.model_selection.tests.test_search.search_no_score_method_auc->GridSearchCV(LinearSVCNoScore(), grid, scoring='roc_auc').fit(X, y)
A:sklearn.model_selection.tests.test_search.search_auc->GridSearchCV(clf, grid, scoring='roc_auc').fit(X, y)
A:sklearn.model_selection.tests.test_search.score_no_scoring->GridSearchCV(clf, grid, scoring=None).fit(X, y).score(X, y)
A:sklearn.model_selection.tests.test_search.score_accuracy->GridSearchCV(clf, grid, scoring='accuracy').fit(X, y).score(X, y)
A:sklearn.model_selection.tests.test_search.score_no_score_auc->GridSearchCV(LinearSVCNoScore(), grid, scoring='roc_auc').fit(X, y).score(X, y)
A:sklearn.model_selection.tests.test_search.score_auc->GridSearchCV(clf, grid, scoring='roc_auc').fit(X, y).score(X, y)
A:sklearn.model_selection.tests.test_search.rng->numpy.random.RandomState(0)
A:sklearn.model_selection.tests.test_search.groups->numpy.random.RandomState(0).randint(0, 3, 15)
A:sklearn.model_selection.tests.test_search.gs->GridSearchCV(LinearSVC(random_state=0), param_grid={'C': [0.1, 0.1, 0.2, 0.2]}, cv=KFold(n_splits=n_splits, shuffle=True))
A:sklearn.model_selection.tests.test_search.msg->'You are accessing a training score ({!r}), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True'.format(key)
A:sklearn.model_selection.tests.test_search.train_score->assert_warns_message(FutureWarning, msg, result['warn'].get, key)
A:sklearn.model_selection.tests.test_search.random_search->RandomizedSearchCV(est, est_parameters, cv=cv, n_iter=3)
A:sklearn.model_selection.tests.test_search.(X_, y_)->make_classification(n_samples=200, n_features=100, random_state=0)
A:sklearn.model_selection.tests.test_search.cv->KFold(random_state=0)
A:sklearn.model_selection.tests.test_search.y_pred->KFold(random_state=0).predict(K_test)
A:sklearn.model_selection.tests.test_search.X_->scipy.sparse.csr_matrix(X_)
A:sklearn.model_selection.tests.test_search.y_pred2->KFold(random_state=0).predict(X_[180:])
A:sklearn.model_selection.tests.test_search.F1Loss->make_scorer(f1_loss, greater_is_better=False)
A:sklearn.model_selection.tests.test_search.y_pred3->KFold(random_state=0).predict(X_[180:])
A:sklearn.model_selection.tests.test_search.K_train->numpy.zeros((10, 20))
A:sklearn.model_selection.tests.test_search.K_test->numpy.dot(X_[180:], X_[:180].T)
A:sklearn.model_selection.tests.test_search.y_train->numpy.ones((10,))
A:sklearn.model_selection.tests.test_search.X_4d->numpy.arange(10 * 5 * 3 * 2).reshape(10, 5, 3, 2)
A:sklearn.model_selection.tests.test_search.y_3d->numpy.arange(10 * 7 * 11).reshape(10, 7, 11)
A:sklearn.model_selection.tests.test_search.km->KMeans(random_state=0)
A:sklearn.model_selection.tests.test_search.(X, _)->make_blobs(cluster_std=0.1, random_state=1, centers=[[0, 1], [1, 0], [0, 0]])
A:sklearn.model_selection.tests.test_search.search->RandomizedSearchCV(SVC(), n_iter=n_search_iter, cv=n_splits, iid=iid, param_distributions=params)
A:sklearn.model_selection.tests.test_search.sampler->ParameterSampler(params_distribution, n_iter=7)
A:sklearn.model_selection.tests.test_search.n_cand->len(res_params)
A:sklearn.model_selection.tests.test_search.grid_scores->assert_warns(DeprecationWarning, getattr, search, 'grid_scores_')
A:sklearn.model_selection.tests.test_search.n_candidates->len(gs.cv_results_['params'])
A:sklearn.model_selection.tests.test_search.params->dict(C=np.logspace(-10, 1), gamma=np.logspace(-5, 0, base=0.1))
A:sklearn.model_selection.tests.test_search.mask->numpy.ones(X.shape[0], dtype=np.bool)
A:sklearn.model_selection.tests.test_search.test_cv_scores->numpy.array(list((search.cv_results_['split%d_test_score' % s][0] for s in range(search.n_splits_))))
A:sklearn.model_selection.tests.test_search.train_cv_scores->numpy.array(list((search.cv_results_['split%d_train_score' % s][0] for s in range(search.n_splits_))))
A:sklearn.model_selection.tests.test_search.expected_test_std->numpy.sqrt(1.0 / 4 * (expected_test_mean - 1) ** 2 + 3.0 / 4 * (expected_test_mean - 1.0 / 3.0) ** 2)
A:sklearn.model_selection.tests.test_search.svc->LinearSVC(random_state=0)
A:sklearn.model_selection.tests.test_search.rs->RandomizedSearchCV(svc, {'C': [0, 1]}, cv=2, error_score=0, n_iter=2)
A:sklearn.model_selection.tests.test_search.result_keys->list(cv_results.keys())
A:sklearn.model_selection.tests.test_search.cv_scores->numpy.array(list((grid_search.cv_results_['split%d_test_score' % s][candidate_i] for s in range(n_splits))))
A:sklearn.model_selection.tests.test_search.correct_score->clone(svc).set_params(**params).score(X[test], y[test])
A:sklearn.model_selection.tests.test_search.dec->MockClassifier().decision_function(X[test])
A:sklearn.model_selection.tests.test_search.scorer->make_scorer(accuracy_score)
A:sklearn.model_selection.tests.test_search.(this_scores, this_params, n_test_samples)->fit_grid_point(X, y, clone(svc), params, train, test, scorer, verbose=False)
A:sklearn.model_selection.tests.test_search.est->clone(svc).set_params(**params)
A:sklearn.model_selection.tests.test_search.expected_score->scorer(est, X[test], y[test])
A:sklearn.model_selection.tests.test_search.grid_search_pickled->pickle.loads(pickle.dumps(grid_search))
A:sklearn.model_selection.tests.test_search.random_search_pickled->pickle.loads(pickle.dumps(random_search))
A:sklearn.model_selection.tests.test_search.p->Pipeline([('imputer', Imputer(strategy='mean', missing_values='NaN')), ('classifier', MockClassifier())])
A:sklearn.model_selection.tests.test_search.samples->list(sampler)
A:sklearn.model_selection.tests.test_search.gs2->GridSearchCV(LinearSVC(random_state=0), param_grid={'C': [0.1, 0.2, 0.3]}, cv=KFold(n_splits=n_splits))
A:sklearn.model_selection.tests.test_search.gs3->GridSearchCV(LinearSVC(random_state=0), param_grid={'C': [0.1, 0.2, 0.3]}, cv=KFold(n_splits=n_splits, shuffle=True, random_state=0).split(X, y))
A:sklearn.model_selection.tests.test_search.gs4->GridSearchCV(LinearSVC(random_state=0), param_grid={'C': [0.1, 0.2, 0.3]}, cv=KFold(n_splits=n_splits, shuffle=True, random_state=0))
A:sklearn.model_selection.tests.test_search.per_param_scores[param_i]->list((gs.cv_results_['split%d_%s_score' % (s, score_type)][param_i] for s in range(5)))
A:sklearn.model_selection.tests.test_search.X_round_trip->GridSearchCV(clf, {'foo_param': [1, 2, 3]}, verbose=3).inverse_transform(grid_search.transform(X))
sklearn.model_selection.tests.test_search.BrokenClassifier(self,parameter=None)
sklearn.model_selection.tests.test_search.BrokenClassifier.__init__(self,parameter=None)
sklearn.model_selection.tests.test_search.BrokenClassifier.fit(self,X,y)
sklearn.model_selection.tests.test_search.BrokenClassifier.predict(self,X)
sklearn.model_selection.tests.test_search.FailingClassifier(self,parameter=None)
sklearn.model_selection.tests.test_search.FailingClassifier.__init__(self,parameter=None)
sklearn.model_selection.tests.test_search.FailingClassifier.fit(self,X,y=None)
sklearn.model_selection.tests.test_search.FailingClassifier.predict(self,X)
sklearn.model_selection.tests.test_search.LinearSVCNoScore(LinearSVC)
sklearn.model_selection.tests.test_search.LinearSVCNoScore.score(self)
sklearn.model_selection.tests.test_search.MockClassifier(self,foo_param=0)
sklearn.model_selection.tests.test_search.MockClassifier.__init__(self,foo_param=0)
sklearn.model_selection.tests.test_search.MockClassifier.fit(self,X,Y)
sklearn.model_selection.tests.test_search.MockClassifier.get_params(self,deep=False)
sklearn.model_selection.tests.test_search.MockClassifier.inverse_transform(self,X)
sklearn.model_selection.tests.test_search.MockClassifier.predict(self,T)
sklearn.model_selection.tests.test_search.MockClassifier.score(self,X=None,Y=None)
sklearn.model_selection.tests.test_search.MockClassifier.set_params(self,**params)
sklearn.model_selection.tests.test_search.MockClassifier.transform(self,X)
sklearn.model_selection.tests.test_search.assert_grid_iter_equals_getitem(grid)
sklearn.model_selection.tests.test_search.check_cv_results_array_types(search,param_keys,score_keys)
sklearn.model_selection.tests.test_search.check_cv_results_grid_scores_consistency(search)
sklearn.model_selection.tests.test_search.check_cv_results_keys(cv_results,param_keys,score_keys,n_cand)
sklearn.model_selection.tests.test_search.check_hyperparameter_searcher_with_fit_params(klass,**klass_kwargs)
sklearn.model_selection.tests.test_search.compare_cv_results_multimetric_with_single(search_multi,search_acc,search_rec,iid)
sklearn.model_selection.tests.test_search.compare_refit_methods_when_refit_with_acc(search_multi,search_acc,refit)
sklearn.model_selection.tests.test_search.test_X_as_list()
sklearn.model_selection.tests.test_search.test_classes__property()
sklearn.model_selection.tests.test_search.test_fit_grid_point()
sklearn.model_selection.tests.test_search.test_grid_search()
sklearn.model_selection.tests.test_search.test_grid_search_allows_nans()
sklearn.model_selection.tests.test_search.test_grid_search_bad_param_grid()
sklearn.model_selection.tests.test_search.test_grid_search_correct_score_results()
sklearn.model_selection.tests.test_search.test_grid_search_cv_results()
sklearn.model_selection.tests.test_search.test_grid_search_cv_results_multimetric()
sklearn.model_selection.tests.test_search.test_grid_search_cv_splits_consistency()
sklearn.model_selection.tests.test_search.test_grid_search_error()
sklearn.model_selection.tests.test_search.test_grid_search_failing_classifier()
sklearn.model_selection.tests.test_search.test_grid_search_failing_classifier_raise()
sklearn.model_selection.tests.test_search.test_grid_search_fit_params_deprecation()
sklearn.model_selection.tests.test_search.test_grid_search_fit_params_two_places()
sklearn.model_selection.tests.test_search.test_grid_search_groups()
sklearn.model_selection.tests.test_search.test_grid_search_no_score()
sklearn.model_selection.tests.test_search.test_grid_search_one_grid_point()
sklearn.model_selection.tests.test_search.test_grid_search_precomputed_kernel()
sklearn.model_selection.tests.test_search.test_grid_search_precomputed_kernel_error_nonsquare()
sklearn.model_selection.tests.test_search.test_grid_search_score_method()
sklearn.model_selection.tests.test_search.test_grid_search_sparse()
sklearn.model_selection.tests.test_search.test_grid_search_sparse_scoring()
sklearn.model_selection.tests.test_search.test_grid_search_when_param_grid_includes_range()
sklearn.model_selection.tests.test_search.test_grid_search_with_fit_params()
sklearn.model_selection.tests.test_search.test_grid_search_with_multioutput_data()
sklearn.model_selection.tests.test_search.test_gridsearch_nd()
sklearn.model_selection.tests.test_search.test_gridsearch_no_predict()
sklearn.model_selection.tests.test_search.test_no_refit()
sklearn.model_selection.tests.test_search.test_pandas_input()
sklearn.model_selection.tests.test_search.test_param_sampler()
sklearn.model_selection.tests.test_search.test_parameter_grid()
sklearn.model_selection.tests.test_search.test_parameters_sampler_replacement()
sklearn.model_selection.tests.test_search.test_pickle()
sklearn.model_selection.tests.test_search.test_predict_proba_disabled()
sklearn.model_selection.tests.test_search.test_random_search_cv_results()
sklearn.model_selection.tests.test_search.test_random_search_cv_results_multimetric()
sklearn.model_selection.tests.test_search.test_random_search_with_fit_params()
sklearn.model_selection.tests.test_search.test_refit()
sklearn.model_selection.tests.test_search.test_return_train_score_warn()
sklearn.model_selection.tests.test_search.test_search_cv_results_none_param()
sklearn.model_selection.tests.test_search.test_search_cv_results_rank_tie_breaking()
sklearn.model_selection.tests.test_search.test_search_cv_timing()
sklearn.model_selection.tests.test_search.test_search_iid_param()
sklearn.model_selection.tests.test_search.test_search_train_scores_set_to_false()
sklearn.model_selection.tests.test_search.test_stochastic_gradient_loss_param()
sklearn.model_selection.tests.test_search.test_transform_inverse_transform_round_trip()
sklearn.model_selection.tests.test_search.test_trivial_cv_results_attr()
sklearn.model_selection.tests.test_search.test_unsupervised_grid_search()
sklearn.model_selection.tests.test_search.test_y_as_list()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/model_selection/tests/common.py----------------------------------------
A:sklearn.model_selection.tests.common.self.indices->iter(KFold(n_splits=n_splits).split(np.ones(n_samples)))
sklearn.model_selection.tests.common.OneTimeSplitter(self,n_splits=4,n_samples=99)
sklearn.model_selection.tests.common.OneTimeSplitter.__init__(self,n_splits=4,n_samples=99)
sklearn.model_selection.tests.common.OneTimeSplitter.get_n_splits(self,X=None,y=None,groups=None)
sklearn.model_selection.tests.common.OneTimeSplitter.split(self,X=None,y=None,groups=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/ensemble/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/ensemble/bagging.py----------------------------------------
A:sklearn.ensemble.bagging.indices->sample_without_replacement(n_population, n_samples, random_state=random_state)
A:sklearn.ensemble.bagging.random_state->numpy.random.RandomState(seed)
A:sklearn.ensemble.bagging.feature_indices->_generate_indices(random_state, bootstrap_features, n_features, max_features)
A:sklearn.ensemble.bagging.sample_indices->_generate_indices(random_state, bootstrap_samples, n_samples, max_samples)
A:sklearn.ensemble.bagging.support_sample_weight->has_fit_parameter(ensemble.base_estimator_, 'sample_weight')
A:sklearn.ensemble.bagging.estimator->ensemble._make_estimator(append=False, random_state=random_state)
A:sklearn.ensemble.bagging.(features, indices)->_generate_bagging_indices(random_state, bootstrap_features, bootstrap, n_features, n_samples, max_features, max_samples)
A:sklearn.ensemble.bagging.curr_sample_weight->check_array(sample_weight, ensure_2d=False).copy()
A:sklearn.ensemble.bagging.sample_counts->numpy.bincount(indices, minlength=n_samples)
A:sklearn.ensemble.bagging.proba->numpy.zeros((n_samples, n_classes))
A:sklearn.ensemble.bagging.proba_estimator->ensemble._make_estimator(append=False, random_state=random_state).predict_proba(X[:, features])
A:sklearn.ensemble.bagging.predictions->numpy.zeros((n_samples,))
A:sklearn.ensemble.bagging.log_proba->numpy.logaddexp(log_proba, all_log_proba[j])
A:sklearn.ensemble.bagging.all_classes->numpy.arange(n_classes, dtype=np.int)
A:sklearn.ensemble.bagging.log_proba_estimator->ensemble._make_estimator(append=False, random_state=random_state).predict_log_proba(X[:, features])
A:sklearn.ensemble.bagging.log_proba[:, estimator.classes_]->numpy.logaddexp(log_proba[:, estimator.classes_], log_proba_estimator[:, range(len(estimator.classes_))])
A:sklearn.ensemble.bagging.missing->numpy.setdiff1d(all_classes, estimator.classes_)
A:sklearn.ensemble.bagging.log_proba[:, missing]->numpy.logaddexp(log_proba[:, missing], -np.inf)
A:sklearn.ensemble.bagging.(X, y)->check_X_y(X, y, ['csr', 'csc'])
A:sklearn.ensemble.bagging.sample_weight->check_array(sample_weight, ensure_2d=False)
A:sklearn.ensemble.bagging.y->column_or_1d(y, warn=True)
A:sklearn.ensemble.bagging.max_samples->int(max_samples * X.shape[0])
A:sklearn.ensemble.bagging.max_features->int(self.max_features * self.n_features_)
A:sklearn.ensemble.bagging.(n_jobs, n_estimators, starts)->_partition_estimators(self.n_estimators, self.n_jobs)
A:sklearn.ensemble.bagging.total_n_estimators->sum(n_estimators)
A:sklearn.ensemble.bagging.seeds->numpy.random.RandomState(seed).randint(MAX_INT, size=n_more_estimators)
A:sklearn.ensemble.bagging.all_results->Parallel(n_jobs=n_jobs, verbose=self.verbose)((delayed(_parallel_build_estimators)(n_estimators[i], self, X, y, sample_weight, seeds[starts[i]:starts[i + 1]], total_n_estimators, verbose=self.verbose) for i in range(n_jobs)))
A:sklearn.ensemble.bagging.(feature_indices, sample_indices)->_generate_bagging_indices(random_state, self.bootstrap_features, self.bootstrap, self.n_features_, self._n_samples, self._max_features, self._max_samples)
A:sklearn.ensemble.bagging.mask->indices_to_mask(sample_indices, self._n_samples)
A:sklearn.ensemble.bagging.p->ensemble._make_estimator(append=False, random_state=random_state).predict(X[mask, :][:, features])
A:sklearn.ensemble.bagging.oob_score->accuracy_score(y, np.argmax(predictions, axis=1))
A:sklearn.ensemble.bagging.(self.classes_, y)->numpy.unique(y, return_inverse=True)
A:sklearn.ensemble.bagging.self.n_classes_->len(self.classes_)
A:sklearn.ensemble.bagging.predicted_probabilitiy->self.predict_proba(X)
A:sklearn.ensemble.bagging.X->check_array(X, accept_sparse=['csr', 'csc'])
A:sklearn.ensemble.bagging.all_proba->Parallel(n_jobs=n_jobs, verbose=self.verbose)((delayed(_parallel_predict_proba)(self.estimators_[starts[i]:starts[i + 1]], self.estimators_features_[starts[i]:starts[i + 1]], X, self.n_classes_) for i in range(n_jobs)))
A:sklearn.ensemble.bagging.all_log_proba->Parallel(n_jobs=n_jobs, verbose=self.verbose)((delayed(_parallel_predict_log_proba)(self.estimators_[starts[i]:starts[i + 1]], self.estimators_features_[starts[i]:starts[i + 1]], X, self.n_classes_) for i in range(n_jobs)))
A:sklearn.ensemble.bagging.all_decisions->Parallel(n_jobs=n_jobs, verbose=self.verbose)((delayed(_parallel_decision_function)(self.estimators_[starts[i]:starts[i + 1]], self.estimators_features_[starts[i]:starts[i + 1]], X) for i in range(n_jobs)))
A:sklearn.ensemble.bagging.all_y_hat->Parallel(n_jobs=n_jobs, verbose=self.verbose)((delayed(_parallel_predict_regression)(self.estimators_[starts[i]:starts[i + 1]], self.estimators_features_[starts[i]:starts[i + 1]], X) for i in range(n_jobs)))
A:sklearn.ensemble.bagging.n_predictions->numpy.zeros((n_samples,))
A:sklearn.ensemble.bagging.self.oob_score_->r2_score(y, predictions)
sklearn.ensemble.BaggingClassifier(self,base_estimator=None,n_estimators=10,max_samples=1.0,max_features=1.0,bootstrap=True,bootstrap_features=False,oob_score=False,warm_start=False,n_jobs=1,random_state=None,verbose=0)
sklearn.ensemble.BaggingClassifier._set_oob_score(self,X,y)
sklearn.ensemble.BaggingClassifier._validate_estimator(self)
sklearn.ensemble.BaggingClassifier._validate_y(self,y)
sklearn.ensemble.BaggingClassifier.decision_function(self,X)
sklearn.ensemble.BaggingClassifier.predict(self,X)
sklearn.ensemble.BaggingClassifier.predict_log_proba(self,X)
sklearn.ensemble.BaggingClassifier.predict_proba(self,X)
sklearn.ensemble.BaggingRegressor(self,base_estimator=None,n_estimators=10,max_samples=1.0,max_features=1.0,bootstrap=True,bootstrap_features=False,oob_score=False,warm_start=False,n_jobs=1,random_state=None,verbose=0)
sklearn.ensemble.BaggingRegressor._set_oob_score(self,X,y)
sklearn.ensemble.BaggingRegressor._validate_estimator(self)
sklearn.ensemble.BaggingRegressor.predict(self,X)
sklearn.ensemble.bagging.BaggingClassifier(self,base_estimator=None,n_estimators=10,max_samples=1.0,max_features=1.0,bootstrap=True,bootstrap_features=False,oob_score=False,warm_start=False,n_jobs=1,random_state=None,verbose=0)
sklearn.ensemble.bagging.BaggingClassifier.__init__(self,base_estimator=None,n_estimators=10,max_samples=1.0,max_features=1.0,bootstrap=True,bootstrap_features=False,oob_score=False,warm_start=False,n_jobs=1,random_state=None,verbose=0)
sklearn.ensemble.bagging.BaggingClassifier._set_oob_score(self,X,y)
sklearn.ensemble.bagging.BaggingClassifier._validate_estimator(self)
sklearn.ensemble.bagging.BaggingClassifier._validate_y(self,y)
sklearn.ensemble.bagging.BaggingClassifier.decision_function(self,X)
sklearn.ensemble.bagging.BaggingClassifier.predict(self,X)
sklearn.ensemble.bagging.BaggingClassifier.predict_log_proba(self,X)
sklearn.ensemble.bagging.BaggingClassifier.predict_proba(self,X)
sklearn.ensemble.bagging.BaggingRegressor(self,base_estimator=None,n_estimators=10,max_samples=1.0,max_features=1.0,bootstrap=True,bootstrap_features=False,oob_score=False,warm_start=False,n_jobs=1,random_state=None,verbose=0)
sklearn.ensemble.bagging.BaggingRegressor.__init__(self,base_estimator=None,n_estimators=10,max_samples=1.0,max_features=1.0,bootstrap=True,bootstrap_features=False,oob_score=False,warm_start=False,n_jobs=1,random_state=None,verbose=0)
sklearn.ensemble.bagging.BaggingRegressor._set_oob_score(self,X,y)
sklearn.ensemble.bagging.BaggingRegressor._validate_estimator(self)
sklearn.ensemble.bagging.BaggingRegressor.predict(self,X)
sklearn.ensemble.bagging.BaseBagging(self,base_estimator=None,n_estimators=10,max_samples=1.0,max_features=1.0,bootstrap=True,bootstrap_features=False,oob_score=False,warm_start=False,n_jobs=1,random_state=None,verbose=0)
sklearn.ensemble.bagging.BaseBagging.__init__(self,base_estimator=None,n_estimators=10,max_samples=1.0,max_features=1.0,bootstrap=True,bootstrap_features=False,oob_score=False,warm_start=False,n_jobs=1,random_state=None,verbose=0)
sklearn.ensemble.bagging.BaseBagging._fit(self,X,y,max_samples=None,max_depth=None,sample_weight=None)
sklearn.ensemble.bagging.BaseBagging._get_estimators_indices(self)
sklearn.ensemble.bagging.BaseBagging._set_oob_score(self,X,y)
sklearn.ensemble.bagging.BaseBagging._validate_y(self,y)
sklearn.ensemble.bagging.BaseBagging.estimators_samples_(self)
sklearn.ensemble.bagging.BaseBagging.fit(self,X,y,sample_weight=None)
sklearn.ensemble.bagging._generate_bagging_indices(random_state,bootstrap_features,bootstrap_samples,n_features,n_samples,max_features,max_samples)
sklearn.ensemble.bagging._generate_indices(random_state,bootstrap,n_population,n_samples)
sklearn.ensemble.bagging._parallel_build_estimators(n_estimators,ensemble,X,y,sample_weight,seeds,total_n_estimators,verbose)
sklearn.ensemble.bagging._parallel_decision_function(estimators,estimators_features,X)
sklearn.ensemble.bagging._parallel_predict_log_proba(estimators,estimators_features,X,n_classes)
sklearn.ensemble.bagging._parallel_predict_proba(estimators,estimators_features,X,n_classes)
sklearn.ensemble.bagging._parallel_predict_regression(estimators,estimators_features,X)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py----------------------------------------
A:sklearn.ensemble.weight_boosting.(X, y)->check_X_y(X, y, accept_sparse=accept_sparse, dtype=dtype, y_numeric=is_regressor(self))
A:sklearn.ensemble.weight_boosting.sample_weight->check_array(sample_weight, ensure_2d=False)
A:sklearn.ensemble.weight_boosting.self.estimator_weights_->numpy.zeros(self.n_estimators, dtype=np.float64)
A:sklearn.ensemble.weight_boosting.self.estimator_errors_->numpy.ones(self.n_estimators, dtype=np.float64)
A:sklearn.ensemble.weight_boosting.random_state->check_random_state(self.random_state)
A:sklearn.ensemble.weight_boosting.(sample_weight, estimator_weight, estimator_error)->self._boost(iboost, X, y, sample_weight, random_state)
A:sklearn.ensemble.weight_boosting.sample_weight_sum->numpy.sum(sample_weight)
A:sklearn.ensemble.weight_boosting.norm->self.estimator_weights_.sum()
A:sklearn.ensemble.weight_boosting.X->self._validate_X_predict(X)
A:sklearn.ensemble.weight_boosting.proba->numpy.exp(1.0 / (n_classes - 1) * proba)
A:sklearn.ensemble.weight_boosting.log_proba->numpy.log(proba)
A:sklearn.ensemble.weight_boosting.estimator->self._make_estimator(random_state=random_state)
A:sklearn.ensemble.weight_boosting.y_predict_proba->self._make_estimator(random_state=random_state).predict_proba(X)
A:sklearn.ensemble.weight_boosting.self.classes_->getattr(estimator, 'classes_', None)
A:sklearn.ensemble.weight_boosting.self.n_classes_->len(self.classes_)
A:sklearn.ensemble.weight_boosting.y_predict->self._make_estimator(random_state=random_state).predict(X)
A:sklearn.ensemble.weight_boosting.estimator_error->(sample_weight * error_vect).sum()
A:sklearn.ensemble.weight_boosting.y_codes->numpy.array([-1.0 / (n_classes - 1), 1.0])
A:sklearn.ensemble.weight_boosting.y_coding->numpy.array([-1.0 / (n_classes - 1), 1.0]).take(classes == y[:, np.newaxis])
A:sklearn.ensemble.weight_boosting.pred->sum(((estimator.predict(X) == classes).T * w for (estimator, w) in zip(self.estimators_, self.estimator_weights_)))
A:sklearn.ensemble.weight_boosting.current_pred->self._make_estimator(random_state=random_state).predict(X)
A:sklearn.ensemble.weight_boosting.tmp_pred->numpy.copy(pred)
A:sklearn.ensemble.weight_boosting.current_proba->_samme_proba(estimator, n_classes, X)
A:sklearn.ensemble.weight_boosting.real_proba->numpy.exp(1.0 / (n_classes - 1) * (proba / norm))
A:sklearn.ensemble.weight_boosting.cdf->stable_cumsum(sample_weight)
A:sklearn.ensemble.weight_boosting.uniform_samples->check_random_state(self.random_state).random_sample(X.shape[0])
A:sklearn.ensemble.weight_boosting.bootstrap_idx->numpy.array(bootstrap_idx, copy=False)
A:sklearn.ensemble.weight_boosting.error_vect->numpy.abs(y_predict - y)
A:sklearn.ensemble.weight_boosting.error_max->numpy.abs(y_predict - y).max()
A:sklearn.ensemble.weight_boosting.sorted_idx->numpy.argsort(predictions, axis=1)
A:sklearn.ensemble.weight_boosting.weight_cdf->stable_cumsum(self.estimator_weights_[sorted_idx], axis=1)
A:sklearn.ensemble.weight_boosting.median_idx->median_or_above.argmax(axis=1)
sklearn.ensemble.AdaBoostClassifier(self,base_estimator=None,n_estimators=50,learning_rate=1.0,algorithm='SAMME.R',random_state=None)
sklearn.ensemble.AdaBoostClassifier._boost(self,iboost,X,y,sample_weight,random_state)
sklearn.ensemble.AdaBoostClassifier._boost_discrete(self,iboost,X,y,sample_weight,random_state)
sklearn.ensemble.AdaBoostClassifier._boost_real(self,iboost,X,y,sample_weight,random_state)
sklearn.ensemble.AdaBoostClassifier._validate_estimator(self)
sklearn.ensemble.AdaBoostClassifier.decision_function(self,X)
sklearn.ensemble.AdaBoostClassifier.fit(self,X,y,sample_weight=None)
sklearn.ensemble.AdaBoostClassifier.predict(self,X)
sklearn.ensemble.AdaBoostClassifier.predict_log_proba(self,X)
sklearn.ensemble.AdaBoostClassifier.predict_proba(self,X)
sklearn.ensemble.AdaBoostClassifier.staged_decision_function(self,X)
sklearn.ensemble.AdaBoostClassifier.staged_predict(self,X)
sklearn.ensemble.AdaBoostClassifier.staged_predict_proba(self,X)
sklearn.ensemble.AdaBoostRegressor(self,base_estimator=None,n_estimators=50,learning_rate=1.0,loss='linear',random_state=None)
sklearn.ensemble.AdaBoostRegressor._boost(self,iboost,X,y,sample_weight,random_state)
sklearn.ensemble.AdaBoostRegressor._get_median_predict(self,X,limit)
sklearn.ensemble.AdaBoostRegressor._validate_estimator(self)
sklearn.ensemble.AdaBoostRegressor.fit(self,X,y,sample_weight=None)
sklearn.ensemble.AdaBoostRegressor.predict(self,X)
sklearn.ensemble.AdaBoostRegressor.staged_predict(self,X)
sklearn.ensemble.weight_boosting.AdaBoostClassifier(self,base_estimator=None,n_estimators=50,learning_rate=1.0,algorithm='SAMME.R',random_state=None)
sklearn.ensemble.weight_boosting.AdaBoostClassifier.__init__(self,base_estimator=None,n_estimators=50,learning_rate=1.0,algorithm='SAMME.R',random_state=None)
sklearn.ensemble.weight_boosting.AdaBoostClassifier._boost(self,iboost,X,y,sample_weight,random_state)
sklearn.ensemble.weight_boosting.AdaBoostClassifier._boost_discrete(self,iboost,X,y,sample_weight,random_state)
sklearn.ensemble.weight_boosting.AdaBoostClassifier._boost_real(self,iboost,X,y,sample_weight,random_state)
sklearn.ensemble.weight_boosting.AdaBoostClassifier._validate_estimator(self)
sklearn.ensemble.weight_boosting.AdaBoostClassifier.decision_function(self,X)
sklearn.ensemble.weight_boosting.AdaBoostClassifier.fit(self,X,y,sample_weight=None)
sklearn.ensemble.weight_boosting.AdaBoostClassifier.predict(self,X)
sklearn.ensemble.weight_boosting.AdaBoostClassifier.predict_log_proba(self,X)
sklearn.ensemble.weight_boosting.AdaBoostClassifier.predict_proba(self,X)
sklearn.ensemble.weight_boosting.AdaBoostClassifier.staged_decision_function(self,X)
sklearn.ensemble.weight_boosting.AdaBoostClassifier.staged_predict(self,X)
sklearn.ensemble.weight_boosting.AdaBoostClassifier.staged_predict_proba(self,X)
sklearn.ensemble.weight_boosting.AdaBoostRegressor(self,base_estimator=None,n_estimators=50,learning_rate=1.0,loss='linear',random_state=None)
sklearn.ensemble.weight_boosting.AdaBoostRegressor.__init__(self,base_estimator=None,n_estimators=50,learning_rate=1.0,loss='linear',random_state=None)
sklearn.ensemble.weight_boosting.AdaBoostRegressor._boost(self,iboost,X,y,sample_weight,random_state)
sklearn.ensemble.weight_boosting.AdaBoostRegressor._get_median_predict(self,X,limit)
sklearn.ensemble.weight_boosting.AdaBoostRegressor._validate_estimator(self)
sklearn.ensemble.weight_boosting.AdaBoostRegressor.fit(self,X,y,sample_weight=None)
sklearn.ensemble.weight_boosting.AdaBoostRegressor.predict(self,X)
sklearn.ensemble.weight_boosting.AdaBoostRegressor.staged_predict(self,X)
sklearn.ensemble.weight_boosting.BaseWeightBoosting(self,base_estimator=None,n_estimators=50,estimator_params=tuple(),learning_rate=1.0,random_state=None)
sklearn.ensemble.weight_boosting.BaseWeightBoosting.__init__(self,base_estimator=None,n_estimators=50,estimator_params=tuple(),learning_rate=1.0,random_state=None)
sklearn.ensemble.weight_boosting.BaseWeightBoosting._boost(self,iboost,X,y,sample_weight,random_state)
sklearn.ensemble.weight_boosting.BaseWeightBoosting._validate_X_predict(self,X)
sklearn.ensemble.weight_boosting.BaseWeightBoosting.feature_importances_(self)
sklearn.ensemble.weight_boosting.BaseWeightBoosting.fit(self,X,y,sample_weight=None)
sklearn.ensemble.weight_boosting.BaseWeightBoosting.staged_score(self,X,y,sample_weight=None)
sklearn.ensemble.weight_boosting._samme_proba(estimator,n_classes,X)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/ensemble/partial_dependence.py----------------------------------------
A:sklearn.ensemble.partial_dependence.emp_percentiles->mquantiles(X, prob=percentiles, axis=0)
A:sklearn.ensemble.partial_dependence.uniques->numpy.unique(X[:, col])
A:sklearn.ensemble.partial_dependence.axis->numpy.linspace(emp_percentiles[0, col], emp_percentiles[1, col], num=grid_resolution, endpoint=True)
A:sklearn.ensemble.partial_dependence.target_variables->numpy.asarray(target_variables, dtype=np.int32, order='C').ravel()
A:sklearn.ensemble.partial_dependence.X->check_array(X, dtype=DTYPE, order='C')
A:sklearn.ensemble.partial_dependence.(grid, axes)->_grid_from_X(X[:, target_variables], percentiles, grid_resolution)
A:sklearn.ensemble.partial_dependence.grid->numpy.asarray(grid, dtype=DTYPE, order='C')
A:sklearn.ensemble.partial_dependence.pdp->numpy.zeros((n_trees_per_stage, grid.shape[0]), dtype=np.float64, order='C')
A:sklearn.ensemble.partial_dependence.label_idx->numpy.searchsorted(gbrt.classes_, label)
A:sklearn.ensemble.partial_dependence.feature_names->feature_names.tolist().tolist()
A:sklearn.ensemble.partial_dependence.fx->feature_names.tolist().tolist().index(fx)
A:sklearn.ensemble.partial_dependence.fxs->numpy.array([convert_feature(fx) for fx in fxs], dtype=np.int32)
A:sklearn.ensemble.partial_dependence.pd_result->Parallel(n_jobs=n_jobs, verbose=verbose)((delayed(partial_dependence)(gbrt, fxs, X=X, grid_resolution=grid_resolution, percentiles=percentiles) for fxs in features))
A:sklearn.ensemble.partial_dependence.n_fx->len(axes)
A:sklearn.ensemble.partial_dependence.(old_min_pd, old_max_pd)->pdp_lim.get(n_fx, (min_pd, max_pd))
A:sklearn.ensemble.partial_dependence.min_pd->min(min_pd, old_min_pd)
A:sklearn.ensemble.partial_dependence.max_pd->max(max_pd, old_max_pd)
A:sklearn.ensemble.partial_dependence.Z_level->numpy.linspace(*pdp_lim[2], num=8)
A:sklearn.ensemble.partial_dependence.fig->fig.add_subplot(n_rows, n_cols, i + 1).get_figure()
A:sklearn.ensemble.partial_dependence.n_cols->min(n_cols, len(features))
A:sklearn.ensemble.partial_dependence.n_rows->int(np.ceil(len(features) / float(n_cols)))
A:sklearn.ensemble.partial_dependence.ax->fig.add_subplot(n_rows, n_cols, i + 1).get_figure().add_subplot(n_rows, n_cols, i + 1)
A:sklearn.ensemble.partial_dependence.(XX, YY)->numpy.meshgrid(axes[0], axes[1])
A:sklearn.ensemble.partial_dependence.CS->fig.add_subplot(n_rows, n_cols, i + 1).get_figure().add_subplot(n_rows, n_cols, i + 1).contour(XX, YY, Z, levels=Z_level, linewidths=0.5, colors='k')
A:sklearn.ensemble.partial_dependence.deciles->mquantiles(X[:, fx[1]], prob=np.arange(0.1, 1.0, 0.1))
A:sklearn.ensemble.partial_dependence.trans->matplotlib.transforms.blended_transform_factory(ax.transAxes, ax.transData)
A:sklearn.ensemble.partial_dependence.ylim->fig.add_subplot(n_rows, n_cols, i + 1).get_figure().add_subplot(n_rows, n_cols, i + 1).get_ylim()
A:sklearn.ensemble.partial_dependence.tick_formatter->ScalarFormatter()
A:sklearn.ensemble.partial_dependence.xlim->fig.add_subplot(n_rows, n_cols, i + 1).get_figure().add_subplot(n_rows, n_cols, i + 1).get_xlim()
sklearn.ensemble.partial_dependence._grid_from_X(X,percentiles=(0.05,0.95),grid_resolution=100)
sklearn.ensemble.partial_dependence.partial_dependence(gbrt,target_variables,grid=None,X=None,percentiles=(0.05,0.95),grid_resolution=100)
sklearn.ensemble.partial_dependence.plot_partial_dependence(gbrt,X,features,feature_names=None,label=None,n_cols=3,grid_resolution=100,percentiles=(0.05,0.95),n_jobs=1,verbose=0,ax=None,line_kw=None,contour_kw=None,**fig_kw)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py----------------------------------------
A:sklearn.ensemble.gradient_boosting.self.quantile->_weighted_percentile(y, sample_weight, self.alpha * 100.0)
A:sklearn.ensemble.gradient_boosting.y->y.astype(np.float64).astype(np.float64)
A:sklearn.ensemble.gradient_boosting.self.mean->numpy.average(y, weights=sample_weight)
A:sklearn.ensemble.gradient_boosting.pos->numpy.sum(sample_weight * y)
A:sklearn.ensemble.gradient_boosting.neg->numpy.sum(sample_weight * (1 - y))
A:sklearn.ensemble.gradient_boosting.sample_weight->column_or_1d(sample_weight, warn=True)
A:sklearn.ensemble.gradient_boosting.class_counts->numpy.bincount(y, weights=sample_weight)
A:sklearn.ensemble.gradient_boosting.terminal_regions->DecisionTreeRegressor(criterion=self.criterion, splitter='best', max_depth=self.max_depth, min_samples_split=self.min_samples_split, min_samples_leaf=self.min_samples_leaf, min_weight_fraction_leaf=self.min_weight_fraction_leaf, min_impurity_decrease=self.min_impurity_decrease, min_impurity_split=self.min_impurity_split, max_features=self.max_features, max_leaf_nodes=self.max_leaf_nodes, random_state=random_state, presort=self.presort).apply(X)
A:sklearn.ensemble.gradient_boosting.masked_terminal_regions->DecisionTreeRegressor(criterion=self.criterion, splitter='best', max_depth=self.max_depth, min_samples_split=self.min_samples_split, min_samples_leaf=self.min_samples_leaf, min_weight_fraction_leaf=self.min_weight_fraction_leaf, min_impurity_decrease=self.min_impurity_decrease, min_impurity_split=self.min_impurity_split, max_features=self.max_features, max_leaf_nodes=self.max_leaf_nodes, random_state=random_state, presort=self.presort).apply(X).copy()
A:sklearn.ensemble.gradient_boosting.pred->pred.take(terminal_region, axis=0).take(terminal_region, axis=0)
A:sklearn.ensemble.gradient_boosting.tree.value[leaf, 0, 0]->_weighted_percentile(diff, sample_weight, percentile=50)
A:sklearn.ensemble.gradient_boosting.gamma->_weighted_percentile(np.abs(diff), sample_weight, self.alpha * 100)
A:sklearn.ensemble.gradient_boosting.sq_loss->numpy.sum(0.5 * sample_weight[gamma_mask] * diff[gamma_mask] ** 2.0)
A:sklearn.ensemble.gradient_boosting.lin_loss->numpy.sum(gamma * sample_weight[~gamma_mask] * (np.abs(diff[~gamma_mask]) - gamma / 2.0))
A:sklearn.ensemble.gradient_boosting.residual->loss.negative_gradient(y, y_pred, k=k, sample_weight=sample_weight)
A:sklearn.ensemble.gradient_boosting.median->_weighted_percentile(diff, sample_weight, percentile=50)
A:sklearn.ensemble.gradient_boosting.val->_weighted_percentile(diff, sample_weight, self.percentile)
A:sklearn.ensemble.gradient_boosting.numerator->numpy.sum(y_ * sample_weight * np.exp(-y_ * pred))
A:sklearn.ensemble.gradient_boosting.denominator->numpy.sum(sample_weight * np.exp(-y_ * pred))
A:sklearn.ensemble.gradient_boosting.proba->self.predict_proba(X)
A:sklearn.ensemble.gradient_boosting.proba[:, 1]->expit(2.0 * score.ravel())
A:sklearn.ensemble.gradient_boosting.Y->numpy.zeros((y.shape[0], self.K), dtype=np.float64)
A:sklearn.ensemble.gradient_boosting.self.verbose_fmt->' '.join(verbose_fmt)
A:sklearn.ensemble.gradient_boosting.self.start_time->time()
A:sklearn.ensemble.gradient_boosting.remaining_time->'{0:.2f}s'.format(remaining_time)
A:sklearn.ensemble.gradient_boosting.tree->DecisionTreeRegressor(criterion=self.criterion, splitter='best', max_depth=self.max_depth, min_samples_split=self.min_samples_split, min_samples_leaf=self.min_samples_leaf, min_weight_fraction_leaf=self.min_weight_fraction_leaf, min_impurity_decrease=self.min_impurity_decrease, min_impurity_split=self.min_impurity_split, max_features=self.max_features, max_leaf_nodes=self.max_leaf_nodes, random_state=random_state, presort=self.presort)
A:sklearn.ensemble.gradient_boosting.self.loss_->loss_class(self.n_classes_)
A:sklearn.ensemble.gradient_boosting.max_features->max(int(self.max_features * self.n_features_), 1)
A:sklearn.ensemble.gradient_boosting.self.init_->INIT_ESTIMATORS[self.init]()
A:sklearn.ensemble.gradient_boosting.self.estimators_->numpy.empty((0, 0), dtype=np.object)
A:sklearn.ensemble.gradient_boosting.self.train_score_->numpy.zeros((self.n_estimators,), dtype=np.float64)
A:sklearn.ensemble.gradient_boosting.self.oob_improvement_->numpy.zeros((total_n_estimators,), dtype=np.float64)
A:sklearn.ensemble.gradient_boosting.(X, y)->check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'], dtype=DTYPE)
A:sklearn.ensemble.gradient_boosting.random_state->check_random_state(self.random_state)
A:sklearn.ensemble.gradient_boosting.y_pred->self._fit_stage(i, X, y, y_pred, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc, X_csr)
A:sklearn.ensemble.gradient_boosting.X_idx_sorted->numpy.asfortranarray(np.argsort(X, axis=0), dtype=np.int32)
A:sklearn.ensemble.gradient_boosting.n_stages->self._fit_stages(X, y, y_pred, sample_weight, random_state, begin_at_stage, monitor, X_idx_sorted)
A:sklearn.ensemble.gradient_boosting.sample_mask->_random_sample_mask(n_samples, n_inbag, random_state)
A:sklearn.ensemble.gradient_boosting.n_inbag->max(1, int(self.subsample * n_samples))
A:sklearn.ensemble.gradient_boosting.verbose_reporter->VerboseReporter(self.verbose)
A:sklearn.ensemble.gradient_boosting.old_oob_score->loss_(y[~sample_mask], y_pred[~sample_mask], sample_weight[~sample_mask])
A:sklearn.ensemble.gradient_boosting.self.train_score_[i]->loss_(y, y_pred, sample_weight)
A:sklearn.ensemble.gradient_boosting.early_stopping->monitor(i, self, locals())
A:sklearn.ensemble.gradient_boosting.X->check_array(X, dtype=DTYPE, order='C', accept_sparse='csr')
A:sklearn.ensemble.gradient_boosting.score->self.decision_function(X)
A:sklearn.ensemble.gradient_boosting.total_sum->numpy.zeros((self.n_features_,), dtype=np.float64)
A:sklearn.ensemble.gradient_boosting.leaves->leaves.reshape(X.shape[0], self.estimators_.shape[0]).reshape(X.shape[0], self.estimators_.shape[0])
A:sklearn.ensemble.gradient_boosting.leaves[:, i, j]->estimator.apply(X, check_input=False)
A:sklearn.ensemble.gradient_boosting.(self.classes_, y)->numpy.unique(y, return_inverse=True)
A:sklearn.ensemble.gradient_boosting.self.n_classes_->len(self.classes_)
A:sklearn.ensemble.gradient_boosting.decisions->self.loss_._score_to_decision(score)
sklearn.ensemble.GradientBoostingClassifier(self,loss='deviance',learning_rate=0.1,n_estimators=100,subsample=1.0,criterion='friedman_mse',min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_depth=3,min_impurity_decrease=0.0,min_impurity_split=None,init=None,random_state=None,max_features=None,verbose=0,max_leaf_nodes=None,warm_start=False,presort='auto')
sklearn.ensemble.GradientBoostingClassifier._validate_y(self,y)
sklearn.ensemble.GradientBoostingClassifier.decision_function(self,X)
sklearn.ensemble.GradientBoostingClassifier.predict(self,X)
sklearn.ensemble.GradientBoostingClassifier.predict_log_proba(self,X)
sklearn.ensemble.GradientBoostingClassifier.predict_proba(self,X)
sklearn.ensemble.GradientBoostingClassifier.staged_decision_function(self,X)
sklearn.ensemble.GradientBoostingClassifier.staged_predict(self,X)
sklearn.ensemble.GradientBoostingClassifier.staged_predict_proba(self,X)
sklearn.ensemble.GradientBoostingRegressor(self,loss='ls',learning_rate=0.1,n_estimators=100,subsample=1.0,criterion='friedman_mse',min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_depth=3,min_impurity_decrease=0.0,min_impurity_split=None,init=None,random_state=None,max_features=None,alpha=0.9,verbose=0,max_leaf_nodes=None,warm_start=False,presort='auto')
sklearn.ensemble.GradientBoostingRegressor.apply(self,X)
sklearn.ensemble.GradientBoostingRegressor.predict(self,X)
sklearn.ensemble.GradientBoostingRegressor.staged_predict(self,X)
sklearn.ensemble.gradient_boosting.BaseGradientBoosting(self,loss,learning_rate,n_estimators,criterion,min_samples_split,min_samples_leaf,min_weight_fraction_leaf,max_depth,min_impurity_decrease,min_impurity_split,init,subsample,max_features,random_state,alpha=0.9,verbose=0,max_leaf_nodes=None,warm_start=False,presort='auto')
sklearn.ensemble.gradient_boosting.BaseGradientBoosting.__init__(self,loss,learning_rate,n_estimators,criterion,min_samples_split,min_samples_leaf,min_weight_fraction_leaf,max_depth,min_impurity_decrease,min_impurity_split,init,subsample,max_features,random_state,alpha=0.9,verbose=0,max_leaf_nodes=None,warm_start=False,presort='auto')
sklearn.ensemble.gradient_boosting.BaseGradientBoosting._check_initialized(self)
sklearn.ensemble.gradient_boosting.BaseGradientBoosting._check_params(self)
sklearn.ensemble.gradient_boosting.BaseGradientBoosting._clear_state(self)
sklearn.ensemble.gradient_boosting.BaseGradientBoosting._decision_function(self,X)
sklearn.ensemble.gradient_boosting.BaseGradientBoosting._fit_stage(self,i,X,y,y_pred,sample_weight,sample_mask,random_state,X_idx_sorted,X_csc=None,X_csr=None)
sklearn.ensemble.gradient_boosting.BaseGradientBoosting._fit_stages(self,X,y,y_pred,sample_weight,random_state,begin_at_stage=0,monitor=None,X_idx_sorted=None)
sklearn.ensemble.gradient_boosting.BaseGradientBoosting._init_decision_function(self,X)
sklearn.ensemble.gradient_boosting.BaseGradientBoosting._init_state(self)
sklearn.ensemble.gradient_boosting.BaseGradientBoosting._is_initialized(self)
sklearn.ensemble.gradient_boosting.BaseGradientBoosting._make_estimator(self,append=True)
sklearn.ensemble.gradient_boosting.BaseGradientBoosting._resize_state(self)
sklearn.ensemble.gradient_boosting.BaseGradientBoosting._staged_decision_function(self,X)
sklearn.ensemble.gradient_boosting.BaseGradientBoosting._validate_y(self,y)
sklearn.ensemble.gradient_boosting.BaseGradientBoosting.apply(self,X)
sklearn.ensemble.gradient_boosting.BaseGradientBoosting.feature_importances_(self)
sklearn.ensemble.gradient_boosting.BaseGradientBoosting.fit(self,X,y,sample_weight=None,monitor=None)
sklearn.ensemble.gradient_boosting.BaseGradientBoosting.n_features(self)
sklearn.ensemble.gradient_boosting.BinomialDeviance(self,n_classes)
sklearn.ensemble.gradient_boosting.BinomialDeviance.__init__(self,n_classes)
sklearn.ensemble.gradient_boosting.BinomialDeviance._score_to_decision(self,score)
sklearn.ensemble.gradient_boosting.BinomialDeviance._score_to_proba(self,score)
sklearn.ensemble.gradient_boosting.BinomialDeviance._update_terminal_region(self,tree,terminal_regions,leaf,X,y,residual,pred,sample_weight)
sklearn.ensemble.gradient_boosting.BinomialDeviance.init_estimator(self)
sklearn.ensemble.gradient_boosting.BinomialDeviance.negative_gradient(self,y,pred,**kargs)
sklearn.ensemble.gradient_boosting.ClassificationLossFunction(six.with_metaclass(ABCMeta,LossFunction))
sklearn.ensemble.gradient_boosting.ClassificationLossFunction._score_to_decision(self,score)
sklearn.ensemble.gradient_boosting.ClassificationLossFunction._score_to_proba(self,score)
sklearn.ensemble.gradient_boosting.ExponentialLoss(self,n_classes)
sklearn.ensemble.gradient_boosting.ExponentialLoss.__init__(self,n_classes)
sklearn.ensemble.gradient_boosting.ExponentialLoss._score_to_decision(self,score)
sklearn.ensemble.gradient_boosting.ExponentialLoss._score_to_proba(self,score)
sklearn.ensemble.gradient_boosting.ExponentialLoss._update_terminal_region(self,tree,terminal_regions,leaf,X,y,residual,pred,sample_weight)
sklearn.ensemble.gradient_boosting.ExponentialLoss.init_estimator(self)
sklearn.ensemble.gradient_boosting.ExponentialLoss.negative_gradient(self,y,pred,**kargs)
sklearn.ensemble.gradient_boosting.GradientBoostingClassifier(self,loss='deviance',learning_rate=0.1,n_estimators=100,subsample=1.0,criterion='friedman_mse',min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_depth=3,min_impurity_decrease=0.0,min_impurity_split=None,init=None,random_state=None,max_features=None,verbose=0,max_leaf_nodes=None,warm_start=False,presort='auto')
sklearn.ensemble.gradient_boosting.GradientBoostingClassifier.__init__(self,loss='deviance',learning_rate=0.1,n_estimators=100,subsample=1.0,criterion='friedman_mse',min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_depth=3,min_impurity_decrease=0.0,min_impurity_split=None,init=None,random_state=None,max_features=None,verbose=0,max_leaf_nodes=None,warm_start=False,presort='auto')
sklearn.ensemble.gradient_boosting.GradientBoostingClassifier._validate_y(self,y)
sklearn.ensemble.gradient_boosting.GradientBoostingClassifier.decision_function(self,X)
sklearn.ensemble.gradient_boosting.GradientBoostingClassifier.predict(self,X)
sklearn.ensemble.gradient_boosting.GradientBoostingClassifier.predict_log_proba(self,X)
sklearn.ensemble.gradient_boosting.GradientBoostingClassifier.predict_proba(self,X)
sklearn.ensemble.gradient_boosting.GradientBoostingClassifier.staged_decision_function(self,X)
sklearn.ensemble.gradient_boosting.GradientBoostingClassifier.staged_predict(self,X)
sklearn.ensemble.gradient_boosting.GradientBoostingClassifier.staged_predict_proba(self,X)
sklearn.ensemble.gradient_boosting.GradientBoostingRegressor(self,loss='ls',learning_rate=0.1,n_estimators=100,subsample=1.0,criterion='friedman_mse',min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_depth=3,min_impurity_decrease=0.0,min_impurity_split=None,init=None,random_state=None,max_features=None,alpha=0.9,verbose=0,max_leaf_nodes=None,warm_start=False,presort='auto')
sklearn.ensemble.gradient_boosting.GradientBoostingRegressor.__init__(self,loss='ls',learning_rate=0.1,n_estimators=100,subsample=1.0,criterion='friedman_mse',min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_depth=3,min_impurity_decrease=0.0,min_impurity_split=None,init=None,random_state=None,max_features=None,alpha=0.9,verbose=0,max_leaf_nodes=None,warm_start=False,presort='auto')
sklearn.ensemble.gradient_boosting.GradientBoostingRegressor.apply(self,X)
sklearn.ensemble.gradient_boosting.GradientBoostingRegressor.predict(self,X)
sklearn.ensemble.gradient_boosting.GradientBoostingRegressor.staged_predict(self,X)
sklearn.ensemble.gradient_boosting.HuberLossFunction(self,n_classes,alpha=0.9)
sklearn.ensemble.gradient_boosting.HuberLossFunction.__init__(self,n_classes,alpha=0.9)
sklearn.ensemble.gradient_boosting.HuberLossFunction._update_terminal_region(self,tree,terminal_regions,leaf,X,y,residual,pred,sample_weight)
sklearn.ensemble.gradient_boosting.HuberLossFunction.init_estimator(self)
sklearn.ensemble.gradient_boosting.HuberLossFunction.negative_gradient(self,y,pred,sample_weight=None,**kargs)
sklearn.ensemble.gradient_boosting.LeastAbsoluteError(self,y,pred,sample_weight=None)
sklearn.ensemble.gradient_boosting.LeastAbsoluteError.__call__(self,y,pred,sample_weight=None)
sklearn.ensemble.gradient_boosting.LeastAbsoluteError._update_terminal_region(self,tree,terminal_regions,leaf,X,y,residual,pred,sample_weight)
sklearn.ensemble.gradient_boosting.LeastAbsoluteError.init_estimator(self)
sklearn.ensemble.gradient_boosting.LeastAbsoluteError.negative_gradient(self,y,pred,**kargs)
sklearn.ensemble.gradient_boosting.LeastSquaresError(self,y,pred,sample_weight=None)
sklearn.ensemble.gradient_boosting.LeastSquaresError.__call__(self,y,pred,sample_weight=None)
sklearn.ensemble.gradient_boosting.LeastSquaresError._update_terminal_region(self,tree,terminal_regions,leaf,X,y,residual,pred,sample_weight)
sklearn.ensemble.gradient_boosting.LeastSquaresError.init_estimator(self)
sklearn.ensemble.gradient_boosting.LeastSquaresError.negative_gradient(self,y,pred,**kargs)
sklearn.ensemble.gradient_boosting.LeastSquaresError.update_terminal_regions(self,tree,X,y,residual,y_pred,sample_weight,sample_mask,learning_rate=1.0,k=0)
sklearn.ensemble.gradient_boosting.LogOddsEstimator(object)
sklearn.ensemble.gradient_boosting.LogOddsEstimator.fit(self,X,y,sample_weight=None)
sklearn.ensemble.gradient_boosting.LogOddsEstimator.predict(self,X)
sklearn.ensemble.gradient_boosting.LossFunction(self,n_classes)
sklearn.ensemble.gradient_boosting.LossFunction.__init__(self,n_classes)
sklearn.ensemble.gradient_boosting.LossFunction._update_terminal_region(self,tree,terminal_regions,leaf,X,y,residual,pred,sample_weight)
sklearn.ensemble.gradient_boosting.LossFunction.init_estimator(self)
sklearn.ensemble.gradient_boosting.LossFunction.negative_gradient(self,y,y_pred,**kargs)
sklearn.ensemble.gradient_boosting.LossFunction.update_terminal_regions(self,tree,X,y,residual,y_pred,sample_weight,sample_mask,learning_rate=1.0,k=0)
sklearn.ensemble.gradient_boosting.MeanEstimator(object)
sklearn.ensemble.gradient_boosting.MeanEstimator.fit(self,X,y,sample_weight=None)
sklearn.ensemble.gradient_boosting.MeanEstimator.predict(self,X)
sklearn.ensemble.gradient_boosting.MultinomialDeviance(self,n_classes)
sklearn.ensemble.gradient_boosting.MultinomialDeviance.__init__(self,n_classes)
sklearn.ensemble.gradient_boosting.MultinomialDeviance._score_to_decision(self,score)
sklearn.ensemble.gradient_boosting.MultinomialDeviance._score_to_proba(self,score)
sklearn.ensemble.gradient_boosting.MultinomialDeviance._update_terminal_region(self,tree,terminal_regions,leaf,X,y,residual,pred,sample_weight)
sklearn.ensemble.gradient_boosting.MultinomialDeviance.init_estimator(self)
sklearn.ensemble.gradient_boosting.MultinomialDeviance.negative_gradient(self,y,pred,k=0,**kwargs)
sklearn.ensemble.gradient_boosting.PriorProbabilityEstimator(object)
sklearn.ensemble.gradient_boosting.PriorProbabilityEstimator.fit(self,X,y,sample_weight=None)
sklearn.ensemble.gradient_boosting.PriorProbabilityEstimator.predict(self,X)
sklearn.ensemble.gradient_boosting.QuantileEstimator(self,alpha=0.9)
sklearn.ensemble.gradient_boosting.QuantileEstimator.__init__(self,alpha=0.9)
sklearn.ensemble.gradient_boosting.QuantileEstimator.fit(self,X,y,sample_weight=None)
sklearn.ensemble.gradient_boosting.QuantileEstimator.predict(self,X)
sklearn.ensemble.gradient_boosting.QuantileLossFunction(self,n_classes,alpha=0.9)
sklearn.ensemble.gradient_boosting.QuantileLossFunction.__init__(self,n_classes,alpha=0.9)
sklearn.ensemble.gradient_boosting.QuantileLossFunction._update_terminal_region(self,tree,terminal_regions,leaf,X,y,residual,pred,sample_weight)
sklearn.ensemble.gradient_boosting.QuantileLossFunction.init_estimator(self)
sklearn.ensemble.gradient_boosting.QuantileLossFunction.negative_gradient(self,y,pred,**kargs)
sklearn.ensemble.gradient_boosting.RegressionLossFunction(self,n_classes)
sklearn.ensemble.gradient_boosting.RegressionLossFunction.__init__(self,n_classes)
sklearn.ensemble.gradient_boosting.ScaledLogOddsEstimator(LogOddsEstimator)
sklearn.ensemble.gradient_boosting.VerboseReporter(self,verbose)
sklearn.ensemble.gradient_boosting.VerboseReporter.__init__(self,verbose)
sklearn.ensemble.gradient_boosting.VerboseReporter.init(self,est,begin_at_stage=0)
sklearn.ensemble.gradient_boosting.VerboseReporter.update(self,j,est)
sklearn.ensemble.gradient_boosting.ZeroEstimator(object)
sklearn.ensemble.gradient_boosting.ZeroEstimator.fit(self,X,y,sample_weight=None)
sklearn.ensemble.gradient_boosting.ZeroEstimator.predict(self,X)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/ensemble/setup.py----------------------------------------
A:sklearn.ensemble.setup.config->Configuration('ensemble', parent_package, top_path)
sklearn.ensemble.setup.configuration(parent_package='',top_path=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/ensemble/forest.py----------------------------------------
A:sklearn.ensemble.forest.random_instance->check_random_state(random_state)
A:sklearn.ensemble.forest.sample_indices->_generate_sample_indices(random_state, n_samples)
A:sklearn.ensemble.forest.sample_counts->numpy.bincount(indices, minlength=n_samples)
A:sklearn.ensemble.forest.indices_range->numpy.arange(n_samples)
A:sklearn.ensemble.forest.curr_sample_weight->check_array(sample_weight, ensure_2d=False).copy()
A:sklearn.ensemble.forest.indices->_generate_sample_indices(tree.random_state, n_samples)
A:sklearn.ensemble.forest.X->check_array(X, accept_sparse=['csc'])
A:sklearn.ensemble.forest.results->Parallel(n_jobs=self.n_jobs, verbose=self.verbose, backend='threading')((delayed(parallel_helper)(tree, 'apply', X, check_input=False) for tree in self.estimators_))
A:sklearn.ensemble.forest.indicators->Parallel(n_jobs=self.n_jobs, verbose=self.verbose, backend='threading')((delayed(parallel_helper)(tree, 'decision_path', X, check_input=False) for tree in self.estimators_))
A:sklearn.ensemble.forest.n_nodes_ptr->numpy.array(n_nodes).cumsum()
A:sklearn.ensemble.forest.y->check_random_state(self.random_state).uniform(size=X.shape[0])
A:sklearn.ensemble.forest.sample_weight->check_array(sample_weight, ensure_2d=False)
A:sklearn.ensemble.forest.(y, expanded_class_weight)->self._validate_y_class_weight(y)
A:sklearn.ensemble.forest.random_state->check_random_state(self.random_state)
A:sklearn.ensemble.forest.tree->self._make_estimator(append=False, random_state=random_state)
A:sklearn.ensemble.forest.trees->Parallel(n_jobs=self.n_jobs, verbose=self.verbose, backend='threading')((delayed(_parallel_build_trees)(t, self, X, y, sample_weight, i, len(trees), verbose=self.verbose, class_weight=self.class_weight) for (i, t) in enumerate(trees)))
A:sklearn.ensemble.forest.all_importances->Parallel(n_jobs=self.n_jobs, backend='threading')((delayed(getattr)(tree, 'feature_importances_') for tree in self.estimators_))
A:sklearn.ensemble.forest.prediction->predict(X, check_input=False)
A:sklearn.ensemble.forest.unsampled_indices->_generate_unsampled_indices(estimator.random_state, n_samples)
A:sklearn.ensemble.forest.p_estimator->estimator.predict(X[unsampled_indices, :], check_input=False)
A:sklearn.ensemble.forest.y_original->numpy.copy(y)
A:sklearn.ensemble.forest.y_store_unique_indices->numpy.zeros(y.shape, dtype=np.int)
A:sklearn.ensemble.forest.(classes_k, y_store_unique_indices[:, k])->numpy.unique(y[:, k], return_inverse=True)
A:sklearn.ensemble.forest.expanded_class_weight->compute_sample_weight(class_weight, y_original)
A:sklearn.ensemble.forest.proba->self.predict_proba(X)
A:sklearn.ensemble.forest.predictions->numpy.zeros((n_samples, self.n_outputs_))
A:sklearn.ensemble.forest.predictions[:, k]->self.classes_[k].take(np.argmax(proba[k], axis=1), axis=0)
A:sklearn.ensemble.forest.(n_jobs, _, _)->_partition_estimators(self.n_estimators, self.n_jobs)
A:sklearn.ensemble.forest.lock->threading.Lock()
A:sklearn.ensemble.forest.proba[k]->numpy.log(proba[k])
A:sklearn.ensemble.forest.y_hat->numpy.zeros(X.shape[0], dtype=np.float64)
A:sklearn.ensemble.forest.n_predictions->numpy.zeros((n_samples, self.n_outputs_))
A:sklearn.ensemble.forest.self.oob_prediction_->self.oob_prediction_.reshape((n_samples,))
A:sklearn.ensemble.forest.rnd->check_random_state(self.random_state)
A:sklearn.ensemble.forest.self.one_hot_encoder_->OneHotEncoder(sparse=self.sparse_output)
sklearn.ensemble.ExtraTreesClassifier(self,n_estimators=10,criterion='gini',max_depth=None,min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_features='auto',max_leaf_nodes=None,min_impurity_decrease=0.0,min_impurity_split=None,bootstrap=False,oob_score=False,n_jobs=1,random_state=None,verbose=0,warm_start=False,class_weight=None)
sklearn.ensemble.ExtraTreesRegressor(self,n_estimators=10,criterion='mse',max_depth=None,min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_features='auto',max_leaf_nodes=None,min_impurity_decrease=0.0,min_impurity_split=None,bootstrap=False,oob_score=False,n_jobs=1,random_state=None,verbose=0,warm_start=False)
sklearn.ensemble.RandomForestClassifier(self,n_estimators=10,criterion='gini',max_depth=None,min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_features='auto',max_leaf_nodes=None,min_impurity_decrease=0.0,min_impurity_split=None,bootstrap=True,oob_score=False,n_jobs=1,random_state=None,verbose=0,warm_start=False,class_weight=None)
sklearn.ensemble.RandomForestRegressor(self,n_estimators=10,criterion='mse',max_depth=None,min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_features='auto',max_leaf_nodes=None,min_impurity_decrease=0.0,min_impurity_split=None,bootstrap=True,oob_score=False,n_jobs=1,random_state=None,verbose=0,warm_start=False)
sklearn.ensemble.RandomTreesEmbedding(self,n_estimators=10,max_depth=5,min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_leaf_nodes=None,min_impurity_decrease=0.0,min_impurity_split=None,sparse_output=True,n_jobs=1,random_state=None,verbose=0,warm_start=False)
sklearn.ensemble.RandomTreesEmbedding._set_oob_score(self,X,y)
sklearn.ensemble.RandomTreesEmbedding.fit(self,X,y=None,sample_weight=None)
sklearn.ensemble.RandomTreesEmbedding.fit_transform(self,X,y=None,sample_weight=None)
sklearn.ensemble.RandomTreesEmbedding.transform(self,X)
sklearn.ensemble.forest.BaseForest(self,base_estimator,n_estimators=10,estimator_params=tuple(),bootstrap=False,oob_score=False,n_jobs=1,random_state=None,verbose=0,warm_start=False,class_weight=None)
sklearn.ensemble.forest.BaseForest.__init__(self,base_estimator,n_estimators=10,estimator_params=tuple(),bootstrap=False,oob_score=False,n_jobs=1,random_state=None,verbose=0,warm_start=False,class_weight=None)
sklearn.ensemble.forest.BaseForest._set_oob_score(self,X,y)
sklearn.ensemble.forest.BaseForest._validate_X_predict(self,X)
sklearn.ensemble.forest.BaseForest._validate_y_class_weight(self,y)
sklearn.ensemble.forest.BaseForest.apply(self,X)
sklearn.ensemble.forest.BaseForest.decision_path(self,X)
sklearn.ensemble.forest.BaseForest.feature_importances_(self)
sklearn.ensemble.forest.BaseForest.fit(self,X,y,sample_weight=None)
sklearn.ensemble.forest.ExtraTreesClassifier(self,n_estimators=10,criterion='gini',max_depth=None,min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_features='auto',max_leaf_nodes=None,min_impurity_decrease=0.0,min_impurity_split=None,bootstrap=False,oob_score=False,n_jobs=1,random_state=None,verbose=0,warm_start=False,class_weight=None)
sklearn.ensemble.forest.ExtraTreesClassifier.__init__(self,n_estimators=10,criterion='gini',max_depth=None,min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_features='auto',max_leaf_nodes=None,min_impurity_decrease=0.0,min_impurity_split=None,bootstrap=False,oob_score=False,n_jobs=1,random_state=None,verbose=0,warm_start=False,class_weight=None)
sklearn.ensemble.forest.ExtraTreesRegressor(self,n_estimators=10,criterion='mse',max_depth=None,min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_features='auto',max_leaf_nodes=None,min_impurity_decrease=0.0,min_impurity_split=None,bootstrap=False,oob_score=False,n_jobs=1,random_state=None,verbose=0,warm_start=False)
sklearn.ensemble.forest.ExtraTreesRegressor.__init__(self,n_estimators=10,criterion='mse',max_depth=None,min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_features='auto',max_leaf_nodes=None,min_impurity_decrease=0.0,min_impurity_split=None,bootstrap=False,oob_score=False,n_jobs=1,random_state=None,verbose=0,warm_start=False)
sklearn.ensemble.forest.ForestClassifier(self,base_estimator,n_estimators=10,estimator_params=tuple(),bootstrap=False,oob_score=False,n_jobs=1,random_state=None,verbose=0,warm_start=False,class_weight=None)
sklearn.ensemble.forest.ForestClassifier.__init__(self,base_estimator,n_estimators=10,estimator_params=tuple(),bootstrap=False,oob_score=False,n_jobs=1,random_state=None,verbose=0,warm_start=False,class_weight=None)
sklearn.ensemble.forest.ForestClassifier._set_oob_score(self,X,y)
sklearn.ensemble.forest.ForestClassifier._validate_y_class_weight(self,y)
sklearn.ensemble.forest.ForestClassifier.predict(self,X)
sklearn.ensemble.forest.ForestClassifier.predict_log_proba(self,X)
sklearn.ensemble.forest.ForestClassifier.predict_proba(self,X)
sklearn.ensemble.forest.ForestRegressor(self,base_estimator,n_estimators=10,estimator_params=tuple(),bootstrap=False,oob_score=False,n_jobs=1,random_state=None,verbose=0,warm_start=False)
sklearn.ensemble.forest.ForestRegressor.__init__(self,base_estimator,n_estimators=10,estimator_params=tuple(),bootstrap=False,oob_score=False,n_jobs=1,random_state=None,verbose=0,warm_start=False)
sklearn.ensemble.forest.ForestRegressor._set_oob_score(self,X,y)
sklearn.ensemble.forest.ForestRegressor.predict(self,X)
sklearn.ensemble.forest.RandomForestClassifier(self,n_estimators=10,criterion='gini',max_depth=None,min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_features='auto',max_leaf_nodes=None,min_impurity_decrease=0.0,min_impurity_split=None,bootstrap=True,oob_score=False,n_jobs=1,random_state=None,verbose=0,warm_start=False,class_weight=None)
sklearn.ensemble.forest.RandomForestClassifier.__init__(self,n_estimators=10,criterion='gini',max_depth=None,min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_features='auto',max_leaf_nodes=None,min_impurity_decrease=0.0,min_impurity_split=None,bootstrap=True,oob_score=False,n_jobs=1,random_state=None,verbose=0,warm_start=False,class_weight=None)
sklearn.ensemble.forest.RandomForestRegressor(self,n_estimators=10,criterion='mse',max_depth=None,min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_features='auto',max_leaf_nodes=None,min_impurity_decrease=0.0,min_impurity_split=None,bootstrap=True,oob_score=False,n_jobs=1,random_state=None,verbose=0,warm_start=False)
sklearn.ensemble.forest.RandomForestRegressor.__init__(self,n_estimators=10,criterion='mse',max_depth=None,min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_features='auto',max_leaf_nodes=None,min_impurity_decrease=0.0,min_impurity_split=None,bootstrap=True,oob_score=False,n_jobs=1,random_state=None,verbose=0,warm_start=False)
sklearn.ensemble.forest.RandomTreesEmbedding(self,n_estimators=10,max_depth=5,min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_leaf_nodes=None,min_impurity_decrease=0.0,min_impurity_split=None,sparse_output=True,n_jobs=1,random_state=None,verbose=0,warm_start=False)
sklearn.ensemble.forest.RandomTreesEmbedding.__init__(self,n_estimators=10,max_depth=5,min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_leaf_nodes=None,min_impurity_decrease=0.0,min_impurity_split=None,sparse_output=True,n_jobs=1,random_state=None,verbose=0,warm_start=False)
sklearn.ensemble.forest.RandomTreesEmbedding._set_oob_score(self,X,y)
sklearn.ensemble.forest.RandomTreesEmbedding.fit(self,X,y=None,sample_weight=None)
sklearn.ensemble.forest.RandomTreesEmbedding.fit_transform(self,X,y=None,sample_weight=None)
sklearn.ensemble.forest.RandomTreesEmbedding.transform(self,X)
sklearn.ensemble.forest._generate_sample_indices(random_state,n_samples)
sklearn.ensemble.forest._generate_unsampled_indices(random_state,n_samples)
sklearn.ensemble.forest._parallel_build_trees(tree,forest,X,y,sample_weight,tree_idx,n_trees,verbose=0,class_weight=None)
sklearn.ensemble.forest.accumulate_prediction(predict,X,out,lock)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/ensemble/iforest.py----------------------------------------
A:sklearn.ensemble.iforest.X->check_array(X, accept_sparse='csr')
A:sklearn.ensemble.iforest.rnd->check_random_state(self.random_state)
A:sklearn.ensemble.iforest.y->check_random_state(self.random_state).uniform(size=X.shape[0])
A:sklearn.ensemble.iforest.max_samples->int(self.max_samples * X.shape[0])
A:sklearn.ensemble.iforest.max_depth->int(np.ceil(np.log2(max(max_samples, 2))))
A:sklearn.ensemble.iforest.is_inlier->numpy.ones(X.shape[0], dtype=int)
A:sklearn.ensemble.iforest.n_samples_leaf->n_samples_leaf.reshape((1, -1)).reshape((1, -1))
A:sklearn.ensemble.iforest.depths->numpy.zeros((n_samples, self.n_estimators), order='f')
A:sklearn.ensemble.iforest.leaves_index->tree.apply(X_subset)
A:sklearn.ensemble.iforest.node_indicator->tree.decision_path(X_subset)
A:sklearn.ensemble.iforest.depths[:, i]->numpy.ravel(node_indicator.sum(axis=1))
A:sklearn.ensemble.iforest.average_path_length->numpy.zeros(n_samples_leaf.shape)
A:sklearn.ensemble.iforest.not_mask->numpy.logical_not(mask)
sklearn.ensemble.IsolationForest(self,n_estimators=100,max_samples='auto',contamination=0.1,max_features=1.0,bootstrap=False,n_jobs=1,random_state=None,verbose=0)
sklearn.ensemble.IsolationForest._set_oob_score(self,X,y)
sklearn.ensemble.IsolationForest.decision_function(self,X)
sklearn.ensemble.IsolationForest.fit(self,X,y=None,sample_weight=None)
sklearn.ensemble.IsolationForest.predict(self,X)
sklearn.ensemble.iforest.IsolationForest(self,n_estimators=100,max_samples='auto',contamination=0.1,max_features=1.0,bootstrap=False,n_jobs=1,random_state=None,verbose=0)
sklearn.ensemble.iforest.IsolationForest.__init__(self,n_estimators=100,max_samples='auto',contamination=0.1,max_features=1.0,bootstrap=False,n_jobs=1,random_state=None,verbose=0)
sklearn.ensemble.iforest.IsolationForest._set_oob_score(self,X,y)
sklearn.ensemble.iforest.IsolationForest.decision_function(self,X)
sklearn.ensemble.iforest.IsolationForest.fit(self,X,y=None,sample_weight=None)
sklearn.ensemble.iforest.IsolationForest.predict(self,X)
sklearn.ensemble.iforest._average_path_length(n_samples_leaf)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/ensemble/voting_classifier.py----------------------------------------
A:sklearn.ensemble.voting_classifier.(names, clfs)->zip(*self.estimators)
A:sklearn.ensemble.voting_classifier.n_isnone->numpy.sum([clf is None for (_, clf) in self.estimators])
A:sklearn.ensemble.voting_classifier.self.le_->LabelEncoder().fit(y)
A:sklearn.ensemble.voting_classifier.transformed_y->self.le_.transform(y)
A:sklearn.ensemble.voting_classifier.self.estimators_->Parallel(n_jobs=self.n_jobs)((delayed(_parallel_fit_estimator)(clone(clf), X, transformed_y, sample_weight=sample_weight) for clf in clfs if clf is not None))
A:sklearn.ensemble.voting_classifier.maj->self.le_.inverse_transform(maj)
A:sklearn.ensemble.voting_classifier.predictions->self._predict(X)
A:sklearn.ensemble.voting_classifier.avg->numpy.average(self._collect_probas(X), axis=0, weights=self._weights_not_none)
A:sklearn.ensemble.voting_classifier.probas->self._collect_probas(X)
sklearn.ensemble.VotingClassifier(self,estimators,voting='hard',weights=None,n_jobs=1,flatten_transform=None)
sklearn.ensemble.VotingClassifier._collect_probas(self,X)
sklearn.ensemble.VotingClassifier._predict(self,X)
sklearn.ensemble.VotingClassifier._predict_proba(self,X)
sklearn.ensemble.VotingClassifier._weights_not_none(self)
sklearn.ensemble.VotingClassifier.fit(self,X,y,sample_weight=None)
sklearn.ensemble.VotingClassifier.get_params(self,deep=True)
sklearn.ensemble.VotingClassifier.named_estimators(self)
sklearn.ensemble.VotingClassifier.predict(self,X)
sklearn.ensemble.VotingClassifier.predict_proba(self)
sklearn.ensemble.VotingClassifier.set_params(self,**params)
sklearn.ensemble.VotingClassifier.transform(self,X)
sklearn.ensemble.voting_classifier.VotingClassifier(self,estimators,voting='hard',weights=None,n_jobs=1,flatten_transform=None)
sklearn.ensemble.voting_classifier.VotingClassifier.__init__(self,estimators,voting='hard',weights=None,n_jobs=1,flatten_transform=None)
sklearn.ensemble.voting_classifier.VotingClassifier._collect_probas(self,X)
sklearn.ensemble.voting_classifier.VotingClassifier._predict(self,X)
sklearn.ensemble.voting_classifier.VotingClassifier._predict_proba(self,X)
sklearn.ensemble.voting_classifier.VotingClassifier._weights_not_none(self)
sklearn.ensemble.voting_classifier.VotingClassifier.fit(self,X,y,sample_weight=None)
sklearn.ensemble.voting_classifier.VotingClassifier.get_params(self,deep=True)
sklearn.ensemble.voting_classifier.VotingClassifier.named_estimators(self)
sklearn.ensemble.voting_classifier.VotingClassifier.predict(self,X)
sklearn.ensemble.voting_classifier.VotingClassifier.predict_proba(self)
sklearn.ensemble.voting_classifier.VotingClassifier.set_params(self,**params)
sklearn.ensemble.voting_classifier.VotingClassifier.transform(self,X)
sklearn.ensemble.voting_classifier._parallel_fit_estimator(estimator,X,y,sample_weight=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/ensemble/base.py----------------------------------------
A:sklearn.ensemble.base.random_state->check_random_state(random_state)
A:sklearn.ensemble.base.to_set[key]->check_random_state(random_state).randint(MAX_RAND_SEED)
A:sklearn.ensemble.base.estimator->clone(self.base_estimator_)
A:sklearn.ensemble.base.n_jobs->min(_get_n_jobs(n_jobs), n_estimators)
A:sklearn.ensemble.base.starts->numpy.cumsum(n_estimators_per_job)
sklearn.ensemble.BaseEnsemble(self,base_estimator,n_estimators=10,estimator_params=tuple())
sklearn.ensemble.BaseEnsemble.__getitem__(self,index)
sklearn.ensemble.BaseEnsemble.__iter__(self)
sklearn.ensemble.BaseEnsemble.__len__(self)
sklearn.ensemble.BaseEnsemble._make_estimator(self,append=True,random_state=None)
sklearn.ensemble.BaseEnsemble._validate_estimator(self,default=None)
sklearn.ensemble.base.BaseEnsemble(self,base_estimator,n_estimators=10,estimator_params=tuple())
sklearn.ensemble.base.BaseEnsemble.__getitem__(self,index)
sklearn.ensemble.base.BaseEnsemble.__init__(self,base_estimator,n_estimators=10,estimator_params=tuple())
sklearn.ensemble.base.BaseEnsemble.__iter__(self)
sklearn.ensemble.base.BaseEnsemble.__len__(self)
sklearn.ensemble.base.BaseEnsemble._make_estimator(self,append=True,random_state=None)
sklearn.ensemble.base.BaseEnsemble._validate_estimator(self,default=None)
sklearn.ensemble.base._partition_estimators(n_estimators,n_jobs)
sklearn.ensemble.base._set_random_states(estimator,random_state=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/ensemble/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/ensemble/tests/test_base.py----------------------------------------
A:sklearn.ensemble.tests.test_base.ensemble->BaggingClassifier(base_estimator=Perceptron(tol=0.001), n_estimators=0)
A:sklearn.ensemble.tests.test_base.iris->load_iris()
A:sklearn.ensemble.tests.test_base.random_state->numpy.random.RandomState(3)
A:sklearn.ensemble.tests.test_base.np_int_ensemble->BaggingClassifier(base_estimator=Perceptron(tol=0.001), n_estimators=np.int32(3))
A:sklearn.ensemble.tests.test_base.string_ensemble->BaggingClassifier(base_estimator=Perceptron(tol=0.001), n_estimators='3')
A:sklearn.ensemble.tests.test_base.float_ensemble->BaggingClassifier(base_estimator=Perceptron(tol=0.001), n_estimators=3.0)
A:sklearn.ensemble.tests.test_base.clf1->Perceptron(tol=0.001, random_state=None)
A:sklearn.ensemble.tests.test_base.clf2->Perceptron(tol=0.001, random_state=None)
A:sklearn.ensemble.tests.test_base.est1->Pipeline(make_steps())
A:sklearn.ensemble.tests.test_base.params->sklearn.pipeline.Pipeline.get_params(self, *args, **kwargs).items()
A:sklearn.ensemble.tests.test_base.est2->cls(make_steps())
sklearn.ensemble.tests.test_base.test_base()
sklearn.ensemble.tests.test_base.test_base_not_int_n_estimators()
sklearn.ensemble.tests.test_base.test_base_zero_n_estimators()
sklearn.ensemble.tests.test_base.test_set_random_states()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/ensemble/tests/test_forest.py----------------------------------------
A:sklearn.ensemble.tests.test_forest.(X_large, y_large)->sklearn.datasets.make_classification(n_samples=500, n_features=10, n_informative=3, n_redundant=0, n_repeated=0, shuffle=False, random_state=0)
A:sklearn.ensemble.tests.test_forest.iris->sklearn.datasets.load_iris()
A:sklearn.ensemble.tests.test_forest.rng->numpy.random.RandomState(0)
A:sklearn.ensemble.tests.test_forest.perm->numpy.random.RandomState(0).permutation(boston.target.size)
A:sklearn.ensemble.tests.test_forest.boston->sklearn.datasets.load_boston()
A:sklearn.ensemble.tests.test_forest.(hastie_X, hastie_y)->sklearn.datasets.make_hastie_10_2(n_samples=20, random_state=1)
A:sklearn.ensemble.tests.test_forest.hastie_X->hastie_X.astype(np.float32).astype(np.float32)
A:sklearn.ensemble.tests.test_forest.FOREST_ESTIMATORS->dict()
A:sklearn.ensemble.tests.test_forest.clf->ForestEstimator(n_estimators=15, max_depth=3, warm_start=False, random_state=1, bootstrap=True, oob_score=True)
A:sklearn.ensemble.tests.test_forest.leaf_indices->ForestEstimator(n_estimators=15, max_depth=3, warm_start=False, random_state=1, bootstrap=True, oob_score=True).apply(X)
A:sklearn.ensemble.tests.test_forest.score->ForestEstimator(random_state=0).score(X, y)
A:sklearn.ensemble.tests.test_forest.r->FOREST_REGRESSORS[name](random_state=0)
A:sklearn.ensemble.tests.test_forest.X->numpy.eye(n_classes)
A:sklearn.ensemble.tests.test_forest.y->numpy.random.RandomState(0).rand(1000)
A:sklearn.ensemble.tests.test_forest.est->Estimator(min_impurity_decrease=0.1)
A:sklearn.ensemble.tests.test_forest.n_important->numpy.sum(importances > 0.1)
A:sklearn.ensemble.tests.test_forest.sample_weight->numpy.ones(iris.target.shape)
A:sklearn.ensemble.tests.test_forest.n_samples->len(samples)
A:sklearn.ensemble.tests.test_forest.features->list(range(n_features))
A:sklearn.ensemble.tests.test_forest.mask_b->numpy.ones(n_samples, dtype=np.bool)
A:sklearn.ensemble.tests.test_forest.n_samples_b->len(X_)
A:sklearn.ensemble.tests.test_forest.data->numpy.array([[0, 0, 1, 0, 0, 1, 0, 1], [1, 0, 1, 1, 1, 0, 1, 2], [1, 0, 1, 1, 0, 1, 1, 3], [0, 1, 1, 1, 0, 1, 0, 4], [1, 1, 0, 1, 0, 1, 1, 5], [1, 1, 0, 1, 1, 1, 1, 6], [1, 0, 1, 0, 0, 1, 0, 7], [1, 1, 1, 1, 1, 1, 1, 8], [1, 1, 1, 1, 0, 1, 1, 9], [1, 1, 1, 0, 1, 1, 1, 0]])
A:sklearn.ensemble.tests.test_forest.true_importances->numpy.zeros(n_features)
A:sklearn.ensemble.tests.test_forest.true_importances[i]->mdi_importance(i, X, y)
A:sklearn.ensemble.tests.test_forest.test_score->Estimator(min_impurity_decrease=0.1).score(X[n_samples // 2:, :], y[n_samples // 2:])
A:sklearn.ensemble.tests.test_forest.forest->ForestEstimator(n_estimators=10, n_jobs=3, random_state=0)
A:sklearn.ensemble.tests.test_forest.y1->ForestEstimator(n_estimators=10, n_jobs=3, random_state=0).predict(X)
A:sklearn.ensemble.tests.test_forest.y2->ForestEstimator(n_estimators=10, n_jobs=3, random_state=0).predict(X)
A:sklearn.ensemble.tests.test_forest.obj->ForestEstimator(random_state=0)
A:sklearn.ensemble.tests.test_forest.pickle_object->pickle.dumps(obj)
A:sklearn.ensemble.tests.test_forest.obj2->pickle.loads(pickle_object)
A:sklearn.ensemble.tests.test_forest.score2->pickle.loads(pickle_object).score(X, y)
A:sklearn.ensemble.tests.test_forest.y_pred->Estimator(min_impurity_decrease=0.1).fit(X_train, y_train).predict(X_test)
A:sklearn.ensemble.tests.test_forest.proba->Estimator(min_impurity_decrease=0.1).predict_proba(X_test)
A:sklearn.ensemble.tests.test_forest.log_proba->Estimator(min_impurity_decrease=0.1).predict_log_proba(X_test)
A:sklearn.ensemble.tests.test_forest.hasher->RandomTreesEmbedding(n_estimators=30, random_state=1)
A:sklearn.ensemble.tests.test_forest.(X, y)->sklearn.datasets.make_hastie_10_2(n_samples=100, random_state=1)
A:sklearn.ensemble.tests.test_forest.X_transformed->RandomTreesEmbedding(n_estimators=30, random_state=1).fit_transform(X)
A:sklearn.ensemble.tests.test_forest.hasher_dense->RandomTreesEmbedding(n_estimators=10, sparse_output=False, random_state=0)
A:sklearn.ensemble.tests.test_forest.hasher_sparse->RandomTreesEmbedding(n_estimators=10, sparse_output=True, random_state=0)
A:sklearn.ensemble.tests.test_forest.X_transformed_dense->RandomTreesEmbedding(n_estimators=10, sparse_output=False, random_state=0).fit_transform(X)
A:sklearn.ensemble.tests.test_forest.X_transformed_sparse->RandomTreesEmbedding(n_estimators=30, random_state=1).fit_transform(csc_matrix(X))
A:sklearn.ensemble.tests.test_forest.svd->TruncatedSVD(n_components=2)
A:sklearn.ensemble.tests.test_forest.X_reduced->TruncatedSVD(n_components=2).fit_transform(X_transformed)
A:sklearn.ensemble.tests.test_forest.linear_clf->LinearSVC()
A:sklearn.ensemble.tests.test_forest.X_train->numpy.random.RandomState(0).randn(n_samples, n_features)
A:sklearn.ensemble.tests.test_forest.y_train->numpy.random.RandomState(0).randint(0, 2, n_samples)
A:sklearn.ensemble.tests.test_forest.X_test->numpy.random.RandomState(0).randn(n_samples, n_features)
A:sklearn.ensemble.tests.test_forest.uniques->defaultdict(int)
A:sklearn.ensemble.tests.test_forest.tree->''.join(('%d,%d/' % (f, int(t)) if f >= 0 else '-' for (f, t) in zip(tree.tree_.feature, tree.tree_.threshold)))
A:sklearn.ensemble.tests.test_forest.X[:, 0]->numpy.random.randint(0, 2, 1000)
A:sklearn.ensemble.tests.test_forest.X[:, 1]->numpy.random.randint(0, 3, 1000)
A:sklearn.ensemble.tests.test_forest.out->Estimator(min_impurity_decrease=0.1).estimators_[0].tree_.apply(X)
A:sklearn.ensemble.tests.test_forest.node_counts->numpy.bincount(out)
A:sklearn.ensemble.tests.test_forest.weights->numpy.random.RandomState(0).rand(X.shape[0])
A:sklearn.ensemble.tests.test_forest.total_weight->numpy.sum(weights)
A:sklearn.ensemble.tests.test_forest.node_weights->numpy.bincount(out, weights=weights)
A:sklearn.ensemble.tests.test_forest.dense->ForestEstimator(random_state=0, max_depth=2).fit(X, y)
A:sklearn.ensemble.tests.test_forest.sparse->ForestEstimator(random_state=0, max_depth=2).fit(X_sparse, y)
A:sklearn.ensemble.tests.test_forest.X_2d->sklearn.datasets.load_iris().data[:, 0].reshape((-1, 1))
A:sklearn.ensemble.tests.test_forest.clf1->ForestClassifier(random_state=0)
A:sklearn.ensemble.tests.test_forest.clf2->ForestClassifier(class_weight=class_weight, random_state=0)
A:sklearn.ensemble.tests.test_forest.clf3->ForestClassifier(class_weight=[{0: 2.0, 1: 2.0, 2: 1.0}, {0: 2.0, 1: 1.0, 2: 2.0}, {0: 1.0, 1: 2.0, 2: 2.0}], random_state=0)
A:sklearn.ensemble.tests.test_forest.clf4->ForestClassifier(class_weight='balanced', random_state=0)
A:sklearn.ensemble.tests.test_forest.clf_ws->ForestEstimator(n_estimators=n_estimators, random_state=random_state, warm_start=True)
A:sklearn.ensemble.tests.test_forest.clf_no_ws->ForestEstimator(n_estimators=10, random_state=random_state, warm_start=False)
A:sklearn.ensemble.tests.test_forest.clf_2->ForestEstimator(n_estimators=5, max_depth=3, warm_start=False, random_state=1, bootstrap=True, oob_score=False)
A:sklearn.ensemble.tests.test_forest.clf_3->ForestEstimator(n_estimators=15, max_depth=3, warm_start=True, random_state=1, bootstrap=True, oob_score=False)
A:sklearn.ensemble.tests.test_forest.classifier->RandomForestClassifier(random_state=0, bootstrap=False)
A:sklearn.ensemble.tests.test_forest.result->RandomForestClassifier(random_state=0, bootstrap=False).fit(X, y).predict(X)
A:sklearn.ensemble.tests.test_forest.(indicator, n_nodes_ptr)->Estimator(min_impurity_decrease=0.1).decision_path(X)
A:sklearn.ensemble.tests.test_forest.leaves->Estimator(min_impurity_decrease=0.1).apply(X)
sklearn.ensemble.tests.test_forest.check_1d_input(name,X,X_2d,y)
sklearn.ensemble.tests.test_forest.check_boston_criterion(name,criterion)
sklearn.ensemble.tests.test_forest.check_class_weight_balanced_and_bootstrap_multi_output(name)
sklearn.ensemble.tests.test_forest.check_class_weight_errors(name)
sklearn.ensemble.tests.test_forest.check_class_weights(name)
sklearn.ensemble.tests.test_forest.check_classes_shape(name)
sklearn.ensemble.tests.test_forest.check_classification_toy(name)
sklearn.ensemble.tests.test_forest.check_decision_path(name)
sklearn.ensemble.tests.test_forest.check_gridsearch(name)
sklearn.ensemble.tests.test_forest.check_importances(name,criterion,dtype,tolerance)
sklearn.ensemble.tests.test_forest.check_iris_criterion(name,criterion)
sklearn.ensemble.tests.test_forest.check_max_leaf_nodes_max_depth(name)
sklearn.ensemble.tests.test_forest.check_memory_layout(name,dtype)
sklearn.ensemble.tests.test_forest.check_min_samples_leaf(name)
sklearn.ensemble.tests.test_forest.check_min_samples_split(name)
sklearn.ensemble.tests.test_forest.check_min_weight_fraction_leaf(name)
sklearn.ensemble.tests.test_forest.check_multioutput(name)
sklearn.ensemble.tests.test_forest.check_oob_score(name,X,y,n_estimators=20)
sklearn.ensemble.tests.test_forest.check_oob_score_raise_error(name)
sklearn.ensemble.tests.test_forest.check_parallel(name,X,y)
sklearn.ensemble.tests.test_forest.check_pickle(name,X,y)
sklearn.ensemble.tests.test_forest.check_probability(name)
sklearn.ensemble.tests.test_forest.check_regressor_attributes(name)
sklearn.ensemble.tests.test_forest.check_sparse_input(name,X,X_sparse,y)
sklearn.ensemble.tests.test_forest.check_unfitted_feature_importances(name)
sklearn.ensemble.tests.test_forest.check_warm_start(name,random_state=42)
sklearn.ensemble.tests.test_forest.check_warm_start_clear(name)
sklearn.ensemble.tests.test_forest.check_warm_start_equal_n_estimators(name)
sklearn.ensemble.tests.test_forest.check_warm_start_oob(name)
sklearn.ensemble.tests.test_forest.check_warm_start_smaller_n_estimators(name)
sklearn.ensemble.tests.test_forest.test_1d_input()
sklearn.ensemble.tests.test_forest.test_boston()
sklearn.ensemble.tests.test_forest.test_class_weight_balanced_and_bootstrap_multi_output()
sklearn.ensemble.tests.test_forest.test_class_weight_errors()
sklearn.ensemble.tests.test_forest.test_class_weights()
sklearn.ensemble.tests.test_forest.test_classes_shape()
sklearn.ensemble.tests.test_forest.test_classification_toy()
sklearn.ensemble.tests.test_forest.test_decision_path()
sklearn.ensemble.tests.test_forest.test_distribution()
sklearn.ensemble.tests.test_forest.test_dtype_convert(n_classes=15)
sklearn.ensemble.tests.test_forest.test_gridsearch()
sklearn.ensemble.tests.test_forest.test_importances()
sklearn.ensemble.tests.test_forest.test_importances_asymptotic()
sklearn.ensemble.tests.test_forest.test_iris()
sklearn.ensemble.tests.test_forest.test_max_leaf_nodes_max_depth()
sklearn.ensemble.tests.test_forest.test_memory_layout()
sklearn.ensemble.tests.test_forest.test_min_impurity_decrease()
sklearn.ensemble.tests.test_forest.test_min_impurity_split()
sklearn.ensemble.tests.test_forest.test_min_samples_leaf()
sklearn.ensemble.tests.test_forest.test_min_samples_split()
sklearn.ensemble.tests.test_forest.test_min_weight_fraction_leaf()
sklearn.ensemble.tests.test_forest.test_multioutput()
sklearn.ensemble.tests.test_forest.test_oob_score()
sklearn.ensemble.tests.test_forest.test_oob_score_raise_error()
sklearn.ensemble.tests.test_forest.test_parallel()
sklearn.ensemble.tests.test_forest.test_parallel_train()
sklearn.ensemble.tests.test_forest.test_pickle()
sklearn.ensemble.tests.test_forest.test_probability()
sklearn.ensemble.tests.test_forest.test_random_hasher()
sklearn.ensemble.tests.test_forest.test_random_hasher_sparse_data()
sklearn.ensemble.tests.test_forest.test_random_trees_dense_equal()
sklearn.ensemble.tests.test_forest.test_random_trees_dense_type()
sklearn.ensemble.tests.test_forest.test_regressor_attributes()
sklearn.ensemble.tests.test_forest.test_sparse_input()
sklearn.ensemble.tests.test_forest.test_unfitted_feature_importances()
sklearn.ensemble.tests.test_forest.test_warm_start()
sklearn.ensemble.tests.test_forest.test_warm_start_clear()
sklearn.ensemble.tests.test_forest.test_warm_start_equal_n_estimators()
sklearn.ensemble.tests.test_forest.test_warm_start_oob()
sklearn.ensemble.tests.test_forest.test_warm_start_smaller_n_estimators()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/ensemble/tests/test_gradient_boosting.py----------------------------------------
A:sklearn.ensemble.tests.test_gradient_boosting.rng->numpy.random.RandomState(0)
A:sklearn.ensemble.tests.test_gradient_boosting.boston->sklearn.datasets.load_boston()
A:sklearn.ensemble.tests.test_gradient_boosting.perm->numpy.random.RandomState(0).permutation(iris.target.size)
A:sklearn.ensemble.tests.test_gradient_boosting.iris->sklearn.datasets.load_iris()
A:sklearn.ensemble.tests.test_gradient_boosting.clf->GradientBoostingClassifier(loss='exponential', n_estimators=100, random_state=1)
A:sklearn.ensemble.tests.test_gradient_boosting.leaves->GradientBoostingClassifier(loss='exponential', n_estimators=100, random_state=1).apply(iris.data)
A:sklearn.ensemble.tests.test_gradient_boosting.(X, y)->sklearn.datasets.make_hastie_10_2(n_samples=100, random_state=1)
A:sklearn.ensemble.tests.test_gradient_boosting.gbrt->GradientBoostingRegressor(n_estimators=1, max_features=0.01 / X.shape[1])
A:sklearn.ensemble.tests.test_gradient_boosting.ones->numpy.ones(len(iris.target))
A:sklearn.ensemble.tests.test_gradient_boosting.y_pred->GradientBoostingClassifier(loss='exponential', n_estimators=100, random_state=1).classes_.take(y_proba.argmax(axis=1), axis=0)
A:sklearn.ensemble.tests.test_gradient_boosting.mse->mean_squared_error(boston.target, y_pred)
A:sklearn.ensemble.tests.test_gradient_boosting.score->GradientBoostingClassifier(loss='exponential', n_estimators=100, random_state=1).decision_function(T).ravel()
A:sklearn.ensemble.tests.test_gradient_boosting.random_state->check_random_state(1)
A:sklearn.ensemble.tests.test_gradient_boosting.X->numpy.random.RandomState(0).uniform(size=(10, 3))
A:sklearn.ensemble.tests.test_gradient_boosting.y->numpy.array(iris.target)
A:sklearn.ensemble.tests.test_gradient_boosting.y_proba->GradientBoostingClassifier(loss='exponential', n_estimators=100, random_state=1).predict_proba(T)
A:sklearn.ensemble.tests.test_gradient_boosting.x->numpy.array([[]])
A:sklearn.ensemble.tests.test_gradient_boosting.deviance->GradientBoostingRegressor(n_estimators=1, max_features=0.01 / X.shape[1]).loss_(y_test, gbrt.decision_function(X_test))
A:sklearn.ensemble.tests.test_gradient_boosting.staged_func->getattr(estimator, 'staged_' + func, None)
A:sklearn.ensemble.tests.test_gradient_boosting.staged_result->list(staged_func(X))
A:sklearn.ensemble.tests.test_gradient_boosting.serialized_clf->pickle.dumps(clf, protocol=pickle.HIGHEST_PROTOCOL)
A:sklearn.ensemble.tests.test_gradient_boosting.clf_quantile->GradientBoostingRegressor(n_estimators=100, loss='quantile', max_depth=4, alpha=0.5, random_state=7)
A:sklearn.ensemble.tests.test_gradient_boosting.y_quantile->GradientBoostingRegressor(n_estimators=100, loss='quantile', max_depth=4, alpha=0.5, random_state=7).predict(boston.data)
A:sklearn.ensemble.tests.test_gradient_boosting.clf_lad->GradientBoostingRegressor(n_estimators=100, loss='lad', max_depth=4, random_state=7)
A:sklearn.ensemble.tests.test_gradient_boosting.y_lad->GradientBoostingRegressor(n_estimators=100, loss='lad', max_depth=4, random_state=7).predict(boston.data)
A:sklearn.ensemble.tests.test_gradient_boosting.symbol_y->tosequence(map(str, y))
A:sklearn.ensemble.tests.test_gradient_boosting.float_y->numpy.asarray(y, dtype=np.float32)
A:sklearn.ensemble.tests.test_gradient_boosting.y_->numpy.asfortranarray(y_)
A:sklearn.ensemble.tests.test_gradient_boosting.X_->numpy.ascontiguousarray(X)
A:sklearn.ensemble.tests.test_gradient_boosting.sys.stdout->StringIO()
A:sklearn.ensemble.tests.test_gradient_boosting.header->verbose_output.readline().rstrip()
A:sklearn.ensemble.tests.test_gradient_boosting.n_lines->sum((1 for l in verbose_output.readlines()))
A:sklearn.ensemble.tests.test_gradient_boosting.est->GBEstimator(min_impurity_decrease=0.1)
A:sklearn.ensemble.tests.test_gradient_boosting.est_ws->Cls(n_estimators=100, max_depth=1, subsample=0.5, random_state=1, warm_start=True)
A:sklearn.ensemble.tests.test_gradient_boosting.est_2->Cls(n_estimators=100, max_depth=1, warm_start=True)
A:sklearn.ensemble.tests.test_gradient_boosting.est2->clone(est)
A:sklearn.ensemble.tests.test_gradient_boosting.gb->GradientBoostingClassifier(n_estimators=5, loss=loss)
A:sklearn.ensemble.tests.test_gradient_boosting.dense->EstimatorClass(n_estimators=10, random_state=0, max_depth=2).fit(X, y)
A:sklearn.ensemble.tests.test_gradient_boosting.sparse->EstimatorClass(n_estimators=10, random_state=0, max_depth=2, presort=False).fit(X_sparse, y)
A:sklearn.ensemble.tests.test_gradient_boosting.auto->EstimatorClass(n_estimators=10, random_state=0, max_depth=2, presort='auto').fit(X_sparse, y)
A:sklearn.ensemble.tests.test_gradient_boosting.(y, X)->sklearn.datasets.make_multilabel_classification(random_state=0, n_samples=50, n_features=1, n_classes=20)
sklearn.ensemble.tests.test_gradient_boosting.check_boston(presort,loss,subsample)
sklearn.ensemble.tests.test_gradient_boosting.check_classification_synthetic(presort,loss)
sklearn.ensemble.tests.test_gradient_boosting.check_classification_toy(presort,loss)
sklearn.ensemble.tests.test_gradient_boosting.check_iris(presort,subsample,sample_weight)
sklearn.ensemble.tests.test_gradient_boosting.check_sparse_input(EstimatorClass,X,X_sparse,y)
sklearn.ensemble.tests.test_gradient_boosting.early_stopping_monitor(i,est,locals)
sklearn.ensemble.tests.test_gradient_boosting.test_boston()
sklearn.ensemble.tests.test_gradient_boosting.test_check_inputs()
sklearn.ensemble.tests.test_gradient_boosting.test_check_inputs_predict()
sklearn.ensemble.tests.test_gradient_boosting.test_check_max_features()
sklearn.ensemble.tests.test_gradient_boosting.test_classification_synthetic()
sklearn.ensemble.tests.test_gradient_boosting.test_classification_toy()
sklearn.ensemble.tests.test_gradient_boosting.test_complete_classification()
sklearn.ensemble.tests.test_gradient_boosting.test_complete_regression()
sklearn.ensemble.tests.test_gradient_boosting.test_degenerate_targets()
sklearn.ensemble.tests.test_gradient_boosting.test_feature_importances()
sklearn.ensemble.tests.test_gradient_boosting.test_float_class_labels()
sklearn.ensemble.tests.test_gradient_boosting.test_iris()
sklearn.ensemble.tests.test_gradient_boosting.test_loss_function()
sklearn.ensemble.tests.test_gradient_boosting.test_max_feature_auto()
sklearn.ensemble.tests.test_gradient_boosting.test_max_feature_regression()
sklearn.ensemble.tests.test_gradient_boosting.test_max_leaf_nodes_max_depth()
sklearn.ensemble.tests.test_gradient_boosting.test_mem_layout()
sklearn.ensemble.tests.test_gradient_boosting.test_min_impurity_decrease()
sklearn.ensemble.tests.test_gradient_boosting.test_min_impurity_split()
sklearn.ensemble.tests.test_gradient_boosting.test_monitor_early_stopping()
sklearn.ensemble.tests.test_gradient_boosting.test_more_verbose_output()
sklearn.ensemble.tests.test_gradient_boosting.test_non_uniform_weights_toy_edge_case_clf()
sklearn.ensemble.tests.test_gradient_boosting.test_non_uniform_weights_toy_edge_case_reg()
sklearn.ensemble.tests.test_gradient_boosting.test_oob_improvement()
sklearn.ensemble.tests.test_gradient_boosting.test_oob_improvement_raise()
sklearn.ensemble.tests.test_gradient_boosting.test_oob_multilcass_iris()
sklearn.ensemble.tests.test_gradient_boosting.test_parameter_checks()
sklearn.ensemble.tests.test_gradient_boosting.test_probability_exponential()
sklearn.ensemble.tests.test_gradient_boosting.test_probability_log()
sklearn.ensemble.tests.test_gradient_boosting.test_quantile_loss()
sklearn.ensemble.tests.test_gradient_boosting.test_regression_synthetic()
sklearn.ensemble.tests.test_gradient_boosting.test_serialization()
sklearn.ensemble.tests.test_gradient_boosting.test_shape_y()
sklearn.ensemble.tests.test_gradient_boosting.test_sparse_input()
sklearn.ensemble.tests.test_gradient_boosting.test_staged_functions_defensive()
sklearn.ensemble.tests.test_gradient_boosting.test_staged_predict()
sklearn.ensemble.tests.test_gradient_boosting.test_staged_predict_proba()
sklearn.ensemble.tests.test_gradient_boosting.test_symbol_labels()
sklearn.ensemble.tests.test_gradient_boosting.test_verbose_output()
sklearn.ensemble.tests.test_gradient_boosting.test_warm_start()
sklearn.ensemble.tests.test_gradient_boosting.test_warm_start_clear()
sklearn.ensemble.tests.test_gradient_boosting.test_warm_start_equal_n_estimators()
sklearn.ensemble.tests.test_gradient_boosting.test_warm_start_max_depth()
sklearn.ensemble.tests.test_gradient_boosting.test_warm_start_n_estimators()
sklearn.ensemble.tests.test_gradient_boosting.test_warm_start_oob()
sklearn.ensemble.tests.test_gradient_boosting.test_warm_start_oob_switch()
sklearn.ensemble.tests.test_gradient_boosting.test_warm_start_smaller_n_estimators()
sklearn.ensemble.tests.test_gradient_boosting.test_warm_start_wo_nestimators_change()
sklearn.ensemble.tests.test_gradient_boosting.test_warm_start_zero_n_estimators()
sklearn.ensemble.tests.test_gradient_boosting.test_zero_estimator_clf()
sklearn.ensemble.tests.test_gradient_boosting.test_zero_estimator_reg()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py----------------------------------------
A:sklearn.ensemble.tests.test_gradient_boosting_loss_functions.bd->BinomialDeviance(2)
A:sklearn.ensemble.tests.test_gradient_boosting_loss_functions.est->LogOddsEstimator()
A:sklearn.ensemble.tests.test_gradient_boosting_loss_functions.rng->check_random_state(13)
A:sklearn.ensemble.tests.test_gradient_boosting_loss_functions.y->numpy.empty(102, dtype=np.float64)
A:sklearn.ensemble.tests.test_gradient_boosting_loss_functions.pred->check_random_state(13).rand(100)
A:sklearn.ensemble.tests.test_gradient_boosting_loss_functions.loss->Loss(k)
A:sklearn.ensemble.tests.test_gradient_boosting_loss_functions.loss_wo_sw->loss(y, pred)
A:sklearn.ensemble.tests.test_gradient_boosting_loss_functions.loss_w_sw->loss(y, pred, np.ones(pred.shape[0], dtype=np.float32))
A:sklearn.ensemble.tests.test_gradient_boosting_loss_functions.X->check_random_state(13).rand(100, 2)
A:sklearn.ensemble.tests.test_gradient_boosting_loss_functions.sample_weight->numpy.ones(100)
A:sklearn.ensemble.tests.test_gradient_boosting_loss_functions.reg_y->check_random_state(13).rand(100)
A:sklearn.ensemble.tests.test_gradient_boosting_loss_functions.clf_y->check_random_state(13).randint(0, 2, size=100)
A:sklearn.ensemble.tests.test_gradient_boosting_loss_functions.init_est->Loss(k).init_estimator()
A:sklearn.ensemble.tests.test_gradient_boosting_loss_functions.out->Loss(k).init_estimator().predict(X)
A:sklearn.ensemble.tests.test_gradient_boosting_loss_functions.sw_init_est->Loss(k).init_estimator()
A:sklearn.ensemble.tests.test_gradient_boosting_loss_functions.sw_out->Loss(k).init_estimator().predict(X)
A:sklearn.ensemble.tests.test_gradient_boosting_loss_functions.sw->numpy.ones(102, dtype=np.float64)
A:sklearn.ensemble.tests.test_gradient_boosting_loss_functions.score->_weighted_percentile(y, sw, 50)
A:sklearn.ensemble.tests.test_gradient_boosting_loss_functions.x->numpy.asarray([-1.0, 0.0, 1.0])
A:sklearn.ensemble.tests.test_gradient_boosting_loss_functions.y_found->QuantileLossFunction(1, 0.9)(x, np.zeros_like(x))
A:sklearn.ensemble.tests.test_gradient_boosting_loss_functions.y_expected->numpy.asarray([0.1, 0.0, 0.9]).mean()
A:sklearn.ensemble.tests.test_gradient_boosting_loss_functions.mclf_y->check_random_state(13).randint(0, 3, size=100)
A:sklearn.ensemble.tests.test_gradient_boosting_loss_functions.p->numpy.zeros((y.shape[0], k), dtype=np.float64)
A:sklearn.ensemble.tests.test_gradient_boosting_loss_functions.deviance_w_w->loss(y, p, sample_weight)
A:sklearn.ensemble.tests.test_gradient_boosting_loss_functions.deviance_wo_w->loss(y, p)
sklearn.ensemble.tests.test_gradient_boosting_loss_functions.test_binomial_deviance()
sklearn.ensemble.tests.test_gradient_boosting_loss_functions.test_log_odds_estimator()
sklearn.ensemble.tests.test_gradient_boosting_loss_functions.test_quantile_loss_function()
sklearn.ensemble.tests.test_gradient_boosting_loss_functions.test_sample_weight_deviance()
sklearn.ensemble.tests.test_gradient_boosting_loss_functions.test_sample_weight_init_estimators()
sklearn.ensemble.tests.test_gradient_boosting_loss_functions.test_sample_weight_smoke()
sklearn.ensemble.tests.test_gradient_boosting_loss_functions.test_weighted_percentile()
sklearn.ensemble.tests.test_gradient_boosting_loss_functions.test_weighted_percentile_equal()
sklearn.ensemble.tests.test_gradient_boosting_loss_functions.test_weighted_percentile_zero_weight()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/ensemble/tests/test_iforest.py----------------------------------------
A:sklearn.ensemble.tests.test_iforest.rng->check_random_state(0)
A:sklearn.ensemble.tests.test_iforest.iris->load_iris()
A:sklearn.ensemble.tests.test_iforest.perm->check_random_state(0).permutation(boston.target.size)
A:sklearn.ensemble.tests.test_iforest.boston->load_boston()
A:sklearn.ensemble.tests.test_iforest.X_train->numpy.array([[0, 1], [1, 2]])
A:sklearn.ensemble.tests.test_iforest.X_test->numpy.array([[2, 1], [1, 1]])
A:sklearn.ensemble.tests.test_iforest.grid->ParameterGrid({'max_samples': [0.5, 1.0], 'bootstrap': [True, False]})
A:sklearn.ensemble.tests.test_iforest.(X_train, X_test, y_train, y_test)->train_test_split(boston.data[:50], boston.target[:50], random_state=rng)
A:sklearn.ensemble.tests.test_iforest.X_train_sparse->sparse_format(X_train)
A:sklearn.ensemble.tests.test_iforest.X_test_sparse->sparse_format(X_test)
A:sklearn.ensemble.tests.test_iforest.sparse_classifier->IsolationForest(n_estimators=10, random_state=1, **params).fit(X_train_sparse)
A:sklearn.ensemble.tests.test_iforest.sparse_results->IsolationForest(n_estimators=10, random_state=1, **params).fit(X_train_sparse).predict(X_test_sparse)
A:sklearn.ensemble.tests.test_iforest.dense_classifier->IsolationForest(n_estimators=10, random_state=1, **params).fit(X_train)
A:sklearn.ensemble.tests.test_iforest.dense_results->IsolationForest(n_estimators=10, random_state=1, **params).fit(X_train).predict(X_test)
A:sklearn.ensemble.tests.test_iforest.clf->IsolationForest(max_features=0.8)
A:sklearn.ensemble.tests.test_iforest.ensemble->IsolationForest(n_jobs=1, random_state=0).fit(X_train)
A:sklearn.ensemble.tests.test_iforest.y1->IsolationForest(n_jobs=1, random_state=0).fit(X_train).predict(X_test)
A:sklearn.ensemble.tests.test_iforest.y2->IsolationForest(n_jobs=1, random_state=0).fit(X_train).predict(X_test)
A:sklearn.ensemble.tests.test_iforest.y3->IsolationForest(n_jobs=1, random_state=0).fit(X_train).predict(X_test)
A:sklearn.ensemble.tests.test_iforest.X_outliers->check_random_state(0).uniform(low=-4, high=4, size=(20, 2))
A:sklearn.ensemble.tests.test_iforest.y_test->numpy.array([0] * 20 + [1] * 20)
A:sklearn.ensemble.tests.test_iforest.pred->IsolationForest(max_features=0.8).predict(X)
sklearn.ensemble.tests.test_iforest.test_iforest()
sklearn.ensemble.tests.test_iforest.test_iforest_average_path_length()
sklearn.ensemble.tests.test_iforest.test_iforest_error()
sklearn.ensemble.tests.test_iforest.test_iforest_parallel_regression()
sklearn.ensemble.tests.test_iforest.test_iforest_performance()
sklearn.ensemble.tests.test_iforest.test_iforest_sparse()
sklearn.ensemble.tests.test_iforest.test_iforest_subsampled_features()
sklearn.ensemble.tests.test_iforest.test_iforest_works()
sklearn.ensemble.tests.test_iforest.test_max_samples_attribute()
sklearn.ensemble.tests.test_iforest.test_max_samples_consistency()
sklearn.ensemble.tests.test_iforest.test_recalculate_max_depth()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/ensemble/tests/test_bagging.py----------------------------------------
A:sklearn.ensemble.tests.test_bagging.rng->check_random_state(0)
A:sklearn.ensemble.tests.test_bagging.iris->load_iris()
A:sklearn.ensemble.tests.test_bagging.perm->check_random_state(0).permutation(boston.target.size)
A:sklearn.ensemble.tests.test_bagging.boston->load_boston()
A:sklearn.ensemble.tests.test_bagging.(X_train, X_test, y_train, y_test)->train_test_split(X, y, random_state=43)
A:sklearn.ensemble.tests.test_bagging.grid->ParameterGrid({'max_samples': [0.5, 1.0], 'max_features': [0.5, 1.0], 'bootstrap': [True, False], 'bootstrap_features': [True, False]})
A:sklearn.ensemble.tests.test_bagging.self.data_type_->type(X)
A:sklearn.ensemble.tests.test_bagging.X_train_sparse->sparse_format(X_train)
A:sklearn.ensemble.tests.test_bagging.X_test_sparse->sparse_format(X_test)
A:sklearn.ensemble.tests.test_bagging.sparse_classifier->BaggingRegressor(base_estimator=CustomSVR(), random_state=1, **params).fit(X_train_sparse, y_train)
A:sklearn.ensemble.tests.test_bagging.sparse_results->BaggingRegressor(base_estimator=CustomSVR(), random_state=1, **params).fit(X_train_sparse, y_train).predict(X_test_sparse)
A:sklearn.ensemble.tests.test_bagging.dense_classifier->BaggingClassifier(base_estimator=CustomSVC(decision_function_shape='ovr'), random_state=1, **params).fit(X_train, y_train)
A:sklearn.ensemble.tests.test_bagging.dense_results->BaggingRegressor(base_estimator=CustomSVR(), random_state=1, **params).fit(X_train, y_train).predict(X_test)
A:sklearn.ensemble.tests.test_bagging.sparse_type->type(X_train_sparse)
A:sklearn.ensemble.tests.test_bagging.base_estimator->DecisionTreeRegressor().fit(X_train, y_train)
A:sklearn.ensemble.tests.test_bagging.ensemble->BaggingRegressor(SVR(), n_jobs=3, random_state=0).fit(X_train, y_train)
A:sklearn.ensemble.tests.test_bagging.clf->BaggingClassifier(n_estimators=50, oob_score=True)
A:sklearn.ensemble.tests.test_bagging.test_score->BaggingClassifier(n_estimators=50, oob_score=True).score(X_test, y_test)
A:sklearn.ensemble.tests.test_bagging.clf1->BaggingRegressor(base_estimator=KNeighborsRegressor(), n_estimators=1, bootstrap=False, bootstrap_features=False, random_state=rng).fit(X_train, y_train)
A:sklearn.ensemble.tests.test_bagging.clf2->KNeighborsRegressor().fit(X_train, y_train)
A:sklearn.ensemble.tests.test_bagging.base->DecisionTreeClassifier()
A:sklearn.ensemble.tests.test_bagging.y1->BaggingClassifier(n_estimators=5, warm_start=True, random_state=3141).predict(X_test)
A:sklearn.ensemble.tests.test_bagging.y2->BaggingClassifier(n_estimators=50, oob_score=True).predict(X_test)
A:sklearn.ensemble.tests.test_bagging.y3->BaggingRegressor(SVR(), n_jobs=3, random_state=0).fit(X_train, y_train).predict(X_test)
A:sklearn.ensemble.tests.test_bagging.decisions1->BaggingRegressor(SVR(), n_jobs=3, random_state=0).fit(X_train, y_train).decision_function(X_test)
A:sklearn.ensemble.tests.test_bagging.decisions2->BaggingRegressor(SVR(), n_jobs=3, random_state=0).fit(X_train, y_train).decision_function(X_test)
A:sklearn.ensemble.tests.test_bagging.X_err->numpy.hstack((X_test, np.zeros((X_test.shape[0], 1))))
A:sklearn.ensemble.tests.test_bagging.decisions3->BaggingRegressor(SVR(), n_jobs=3, random_state=0).fit(X_train, y_train).decision_function(X_test)
A:sklearn.ensemble.tests.test_bagging.estimator->BaggingClassifier(DummyZeroEstimator())
A:sklearn.ensemble.tests.test_bagging.self.classes_->numpy.unique(y)
A:sklearn.ensemble.tests.test_bagging.(X, y)->make_hastie_10_2(n_samples=2 * max_samples, random_state=1)
A:sklearn.ensemble.tests.test_bagging.clf_ws->BaggingClassifier(n_estimators=5, warm_start=True, random_state=3141)
A:sklearn.ensemble.tests.test_bagging.clf_no_ws->BaggingClassifier(n_estimators=10, random_state=random_state, warm_start=False)
A:sklearn.ensemble.tests.test_bagging.y_pred->BaggingClassifier(n_estimators=50, oob_score=True).predict(X_test)
A:sklearn.ensemble.tests.test_bagging.bagging->BaggingClassifier(KNeighborsClassifier(), max_samples=max_samples, max_features=0.5, random_state=1)
sklearn.ensemble.tests.test_bagging.DummyZeroEstimator(BaseEstimator)
sklearn.ensemble.tests.test_bagging.DummyZeroEstimator.fit(self,X,y)
sklearn.ensemble.tests.test_bagging.DummyZeroEstimator.predict(self,X)
sklearn.ensemble.tests.test_bagging.test_bagging_sample_weight_unsupported_but_passed()
sklearn.ensemble.tests.test_bagging.test_bagging_with_pipeline()
sklearn.ensemble.tests.test_bagging.test_base_estimator()
sklearn.ensemble.tests.test_bagging.test_bootstrap_features()
sklearn.ensemble.tests.test_bagging.test_bootstrap_samples()
sklearn.ensemble.tests.test_bagging.test_classification()
sklearn.ensemble.tests.test_bagging.test_error()
sklearn.ensemble.tests.test_bagging.test_estimators_samples()
sklearn.ensemble.tests.test_bagging.test_gridsearch()
sklearn.ensemble.tests.test_bagging.test_max_samples_consistency()
sklearn.ensemble.tests.test_bagging.test_oob_score_classification()
sklearn.ensemble.tests.test_bagging.test_oob_score_consistency()
sklearn.ensemble.tests.test_bagging.test_oob_score_regression()
sklearn.ensemble.tests.test_bagging.test_oob_score_removed_on_warm_start()
sklearn.ensemble.tests.test_bagging.test_parallel_classification()
sklearn.ensemble.tests.test_bagging.test_parallel_regression()
sklearn.ensemble.tests.test_bagging.test_probability()
sklearn.ensemble.tests.test_bagging.test_regression()
sklearn.ensemble.tests.test_bagging.test_set_oob_score_label_encoding()
sklearn.ensemble.tests.test_bagging.test_single_estimator()
sklearn.ensemble.tests.test_bagging.test_sparse_classification()
sklearn.ensemble.tests.test_bagging.test_sparse_regression()
sklearn.ensemble.tests.test_bagging.test_warm_start(random_state=42)
sklearn.ensemble.tests.test_bagging.test_warm_start_equal_n_estimators()
sklearn.ensemble.tests.test_bagging.test_warm_start_equivalence()
sklearn.ensemble.tests.test_bagging.test_warm_start_smaller_n_estimators()
sklearn.ensemble.tests.test_bagging.test_warm_start_with_oob_score_fails()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/ensemble/tests/test_partial_dependence.py----------------------------------------
A:sklearn.ensemble.tests.test_partial_dependence.boston->sklearn.datasets.load_boston()
A:sklearn.ensemble.tests.test_partial_dependence.iris->sklearn.datasets.load_iris()
A:sklearn.ensemble.tests.test_partial_dependence.clf->GradientBoostingClassifier(n_estimators=10, random_state=1)
A:sklearn.ensemble.tests.test_partial_dependence.(pdp, axes)->partial_dependence(clf, [0], X=boston.data, grid_resolution=grid_resolution)
A:sklearn.ensemble.tests.test_partial_dependence.X_->numpy.asarray(X)
A:sklearn.ensemble.tests.test_partial_dependence.grid->numpy.random.rand(10, 2, 1)
A:sklearn.ensemble.tests.test_partial_dependence.(pdp_2, axes)->partial_dependence(clf, [0], grid=grid)
A:sklearn.ensemble.tests.test_partial_dependence.(fig, axs)->plot_partial_dependence(clf, iris.data, [0, 1], label='setosa', grid_resolution=grid_resolution)
A:sklearn.ensemble.tests.test_partial_dependence.feature_names->sklearn.datasets.load_boston().feature_names.tolist()
sklearn.ensemble.tests.test_partial_dependence.test_partial_dependecy_input()
sklearn.ensemble.tests.test_partial_dependence.test_partial_dependence_classifier()
sklearn.ensemble.tests.test_partial_dependence.test_partial_dependence_multiclass()
sklearn.ensemble.tests.test_partial_dependence.test_partial_dependence_regressor()
sklearn.ensemble.tests.test_partial_dependence.test_plot_partial_dependence()
sklearn.ensemble.tests.test_partial_dependence.test_plot_partial_dependence_input()
sklearn.ensemble.tests.test_partial_dependence.test_plot_partial_dependence_multiclass()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/ensemble/tests/test_weight_boosting.py----------------------------------------
A:sklearn.ensemble.tests.test_weight_boosting.rng->numpy.random.RandomState(0)
A:sklearn.ensemble.tests.test_weight_boosting.iris->sklearn.datasets.load_iris()
A:sklearn.ensemble.tests.test_weight_boosting.perm->numpy.random.RandomState(0).permutation(iris.target.size)
A:sklearn.ensemble.tests.test_weight_boosting.(iris.data, iris.target)->shuffle(iris.data, iris.target, random_state=rng)
A:sklearn.ensemble.tests.test_weight_boosting.boston->sklearn.datasets.load_boston()
A:sklearn.ensemble.tests.test_weight_boosting.(boston.data, boston.target)->shuffle(boston.data, boston.target, random_state=rng)
A:sklearn.ensemble.tests.test_weight_boosting.probs->numpy.array([[1, 1e-06, 0], [0.19, 0.6, 0.2], [-999, 0.51, 0.5], [1e-06, 1, 1e-09]])
A:sklearn.ensemble.tests.test_weight_boosting.mock->MockEstimator()
A:sklearn.ensemble.tests.test_weight_boosting.samme_proba->sklearn.ensemble.weight_boosting._samme_proba(mock, 3, np.ones_like(probs))
A:sklearn.ensemble.tests.test_weight_boosting.y_t->numpy.ones(len(X))
A:sklearn.ensemble.tests.test_weight_boosting.clf->AdaBoostRegressor(KMeans())
A:sklearn.ensemble.tests.test_weight_boosting.classes->numpy.unique(iris.target)
A:sklearn.ensemble.tests.test_weight_boosting.proba->AdaBoostRegressor(KMeans()).predict_proba(iris.data)
A:sklearn.ensemble.tests.test_weight_boosting.score->AdaBoostRegressor(random_state=0).score(boston.data, boston.target)
A:sklearn.ensemble.tests.test_weight_boosting.reg->AdaBoostRegressor(random_state=0)
A:sklearn.ensemble.tests.test_weight_boosting.iris_weights->numpy.random.RandomState(0).randint(10, size=iris.target.shape)
A:sklearn.ensemble.tests.test_weight_boosting.boston_weights->numpy.random.RandomState(0).randint(10, size=boston.target.shape)
A:sklearn.ensemble.tests.test_weight_boosting.predictions->AdaBoostRegressor(KMeans()).predict(boston.data)
A:sklearn.ensemble.tests.test_weight_boosting.boost->AdaBoostRegressor(DummyEstimator(), n_estimators=3)
A:sklearn.ensemble.tests.test_weight_boosting.obj->AdaBoostRegressor(random_state=0)
A:sklearn.ensemble.tests.test_weight_boosting.s->pickle.dumps(obj)
A:sklearn.ensemble.tests.test_weight_boosting.obj2->pickle.loads(s)
A:sklearn.ensemble.tests.test_weight_boosting.score2->pickle.loads(s).score(boston.data, boston.target)
A:sklearn.ensemble.tests.test_weight_boosting.(X, y)->sklearn.datasets.make_regression(n_samples=15, n_features=50, n_targets=1, random_state=42)
A:sklearn.ensemble.tests.test_weight_boosting.self.data_type_->type(X)
A:sklearn.ensemble.tests.test_weight_boosting.y->numpy.ravel(y)
A:sklearn.ensemble.tests.test_weight_boosting.(X_train, X_test, y_train, y_test)->train_test_split(X, y, random_state=0)
A:sklearn.ensemble.tests.test_weight_boosting.X_train_sparse->sparse_format(X_train)
A:sklearn.ensemble.tests.test_weight_boosting.X_test_sparse->sparse_format(X_test)
A:sklearn.ensemble.tests.test_weight_boosting.sparse_classifier->AdaBoostRegressor(base_estimator=CustomSVR(), random_state=1).fit(X_train_sparse, y_train)
A:sklearn.ensemble.tests.test_weight_boosting.dense_classifier->AdaBoostClassifier(base_estimator=CustomSVC(probability=True), random_state=1, algorithm='SAMME').fit(X_train, y_train)
A:sklearn.ensemble.tests.test_weight_boosting.sparse_results->AdaBoostRegressor(base_estimator=CustomSVR(), random_state=1).fit(X_train_sparse, y_train).staged_predict(X_test_sparse)
A:sklearn.ensemble.tests.test_weight_boosting.dense_results->AdaBoostClassifier(base_estimator=CustomSVC(probability=True), random_state=1, algorithm='SAMME').fit(X_train, y_train).staged_predict(X_test)
A:sklearn.ensemble.tests.test_weight_boosting.dense_classifierdense_results->AdaBoostRegressor(base_estimator=CustomSVR(), random_state=1).fit(X_train, y_train)
sklearn.ensemble.tests.test_weight_boosting.test_base_estimator()
sklearn.ensemble.tests.test_weight_boosting.test_boston()
sklearn.ensemble.tests.test_weight_boosting.test_classification_toy()
sklearn.ensemble.tests.test_weight_boosting.test_error()
sklearn.ensemble.tests.test_weight_boosting.test_gridsearch()
sklearn.ensemble.tests.test_weight_boosting.test_importances()
sklearn.ensemble.tests.test_weight_boosting.test_iris()
sklearn.ensemble.tests.test_weight_boosting.test_oneclass_adaboost_proba()
sklearn.ensemble.tests.test_weight_boosting.test_pickle()
sklearn.ensemble.tests.test_weight_boosting.test_regression_toy()
sklearn.ensemble.tests.test_weight_boosting.test_samme_proba()
sklearn.ensemble.tests.test_weight_boosting.test_sample_weight_adaboost_regressor()
sklearn.ensemble.tests.test_weight_boosting.test_sample_weight_missing()
sklearn.ensemble.tests.test_weight_boosting.test_sparse_classification()
sklearn.ensemble.tests.test_weight_boosting.test_sparse_regression()
sklearn.ensemble.tests.test_weight_boosting.test_staged_predict()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/ensemble/tests/test_voting_classifier.py----------------------------------------
A:sklearn.ensemble.tests.test_voting_classifier.iris->sklearn.datasets.load_iris()
A:sklearn.ensemble.tests.test_voting_classifier.eclf->VotingClassifier(estimators=[('mock', clf)], voting='soft')
A:sklearn.ensemble.tests.test_voting_classifier.clf->MockClassifier()
A:sklearn.ensemble.tests.test_voting_classifier.clf1->LogisticRegression(random_state=123)
A:sklearn.ensemble.tests.test_voting_classifier.clf2->RandomForestClassifier(random_state=123)
A:sklearn.ensemble.tests.test_voting_classifier.clf3->GaussianNB()
A:sklearn.ensemble.tests.test_voting_classifier.scores->cross_val_score(eclf, X, y, cv=5, scoring='accuracy')
A:sklearn.ensemble.tests.test_voting_classifier.X->numpy.array([[-1.1, -1.5], [-1.2, -1.4], [-3.4, -2.2], [1.1, 1.2]])
A:sklearn.ensemble.tests.test_voting_classifier.y->numpy.array([1, 1, 2, 2])
A:sklearn.ensemble.tests.test_voting_classifier.clf1_res->numpy.array([[0.59790391, 0.40209609], [0.57622162, 0.42377838], [0.50728456, 0.49271544], [0.40241774, 0.59758226]])
A:sklearn.ensemble.tests.test_voting_classifier.clf2_res->numpy.array([[0.8, 0.2], [0.8, 0.2], [0.2, 0.8], [0.3, 0.7]])
A:sklearn.ensemble.tests.test_voting_classifier.clf3_res->numpy.array([[0.9985082, 0.0014918], [0.99845843, 0.00154157], [0.0, 1.0], [0.0, 1.0]])
A:sklearn.ensemble.tests.test_voting_classifier.eclf_res->VotingClassifier(estimators=[('mock', clf)], voting='soft').fit(X, y).predict_proba(X)
A:sklearn.ensemble.tests.test_voting_classifier.(X, y)->make_multilabel_classification(n_classes=2, n_labels=1, allow_unlabeled=False, random_state=123)
A:sklearn.ensemble.tests.test_voting_classifier.grid->GridSearchCV(estimator=eclf, param_grid=params, cv=5)
A:sklearn.ensemble.tests.test_voting_classifier.eclf1->VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='soft').fit(X, y)
A:sklearn.ensemble.tests.test_voting_classifier.eclf2->VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='soft', flatten_transform=True).fit(X, y)
A:sklearn.ensemble.tests.test_voting_classifier.sample_weight->numpy.random.RandomState(123).uniform(size=(len(y),))
A:sklearn.ensemble.tests.test_voting_classifier.eclf3->VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='soft', flatten_transform=False).fit(X, y)
A:sklearn.ensemble.tests.test_voting_classifier.clf4->KNeighborsClassifier()
A:sklearn.ensemble.tests.test_voting_classifier.X1->numpy.array([[1], [2]])
A:sklearn.ensemble.tests.test_voting_classifier.y1->numpy.array([1, 2])
A:sklearn.ensemble.tests.test_voting_classifier.res->assert_warns_message(DeprecationWarning, warn_msg, eclf1.transform, X)
sklearn.ensemble.tests.test_voting_classifier.test_estimator_init()
sklearn.ensemble.tests.test_voting_classifier.test_estimator_weights_format()
sklearn.ensemble.tests.test_voting_classifier.test_gridsearch()
sklearn.ensemble.tests.test_voting_classifier.test_majority_label_iris()
sklearn.ensemble.tests.test_voting_classifier.test_multilabel()
sklearn.ensemble.tests.test_voting_classifier.test_notfitted()
sklearn.ensemble.tests.test_voting_classifier.test_parallel_fit()
sklearn.ensemble.tests.test_voting_classifier.test_predict_on_toy_problem()
sklearn.ensemble.tests.test_voting_classifier.test_predict_proba_on_toy_problem()
sklearn.ensemble.tests.test_voting_classifier.test_predictproba_hardvoting()
sklearn.ensemble.tests.test_voting_classifier.test_sample_weight()
sklearn.ensemble.tests.test_voting_classifier.test_sample_weight_kwargs()
sklearn.ensemble.tests.test_voting_classifier.test_set_estimator_none()
sklearn.ensemble.tests.test_voting_classifier.test_set_params()
sklearn.ensemble.tests.test_voting_classifier.test_tie_situation()
sklearn.ensemble.tests.test_voting_classifier.test_transform()
sklearn.ensemble.tests.test_voting_classifier.test_weights_iris()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/linear_model/omp.py----------------------------------------
A:sklearn.linear_model.omp.X->as_float_array(X, copy=False, force_all_finite=False)
A:sklearn.linear_model.omp.(nrm2, swap)->scipy.linalg.get_blas_funcs(('nrm2', 'swap'), (Gram,))
A:sklearn.linear_model.omp.(potrs,)->get_lapack_funcs(('potrs',), (Gram,))
A:sklearn.linear_model.omp.alpha->numpy.dot(X.T, y)
A:sklearn.linear_model.omp.gamma->numpy.empty(0)
A:sklearn.linear_model.omp.indices->numpy.arange(len(Gram))
A:sklearn.linear_model.omp.L->numpy.zeros((max_features, max_features), dtype=Gram.dtype)
A:sklearn.linear_model.omp.coefs->orthogonal_mp(X_train, y_train, n_nonzero_coefs=max_iter, tol=None, precompute=False, copy_X=False, return_path=True)
A:sklearn.linear_model.omp.lam->numpy.argmax(np.abs(alpha))
A:sklearn.linear_model.omp.L[n_active, :n_active]->numpy.dot(X[:, :n_active].T, X[:, lam])
A:sklearn.linear_model.omp.L[n_active, n_active]->numpy.sqrt(1 - v)
A:sklearn.linear_model.omp.(X.T[n_active], X.T[lam])->swap(X.T[n_active], X.T[lam])
A:sklearn.linear_model.omp.(gamma, _)->potrs(L[:n_active, :n_active], Xy[:n_active], lower=True, overwrite_b=False)
A:sklearn.linear_model.omp.Xy->numpy.asarray(Xy)
A:sklearn.linear_model.omp.(Gram[n_active], Gram[lam])->swap(Gram[n_active], Gram[lam])
A:sklearn.linear_model.omp.(Gram.T[n_active], Gram.T[lam])->swap(Gram.T[n_active], Gram.T[lam])
A:sklearn.linear_model.omp.beta->numpy.dot(Gram[:, :n_active], gamma)
A:sklearn.linear_model.omp.delta->numpy.inner(gamma, beta[:n_active])
A:sklearn.linear_model.omp.y->check_array(y)
A:sklearn.linear_model.omp.n_nonzero_coefs->int(0.1 * len(Gram))
A:sklearn.linear_model.omp.G->numpy.asfortranarray(G)
A:sklearn.linear_model.omp.norms_squared->numpy.sum(y ** 2, axis=0)
A:sklearn.linear_model.omp.coef->numpy.zeros((len(Gram), Xy.shape[1]))
A:sklearn.linear_model.omp.out->_gram_omp(Gram, Xy[:, k], n_nonzero_coefs, norms_squared[k] if tol is not None else None, tol, copy_Gram=copy_Gram, copy_Xy=copy_Xy, return_path=return_path)
A:sklearn.linear_model.omp.Gram->check_array(Gram, order='F', copy=copy_Gram)
A:sklearn.linear_model.omp.(X, y)->check_X_y(X, y, y_numeric=True, ensure_min_features=2, estimator=self)
A:sklearn.linear_model.omp.(X, y, X_offset, y_offset, X_scale, Gram, Xy)->_pre_fit(X, y, None, self.precompute, self.normalize, self.fit_intercept, copy=True)
A:sklearn.linear_model.omp.self.n_nonzero_coefs_->max(int(0.1 * n_features), 1)
A:sklearn.linear_model.omp.(coef_, self.n_iter_)->orthogonal_mp_gram(Gram, Xy=Xy, n_nonzero_coefs=self.n_nonzero_coefs_, tol=self.tol, norms_squared=norms_sq, copy_Gram=True, copy_Xy=True, return_n_iter=True)
A:sklearn.linear_model.omp.X_train->X_train.copy().copy()
A:sklearn.linear_model.omp.y_train->as_float_array(y_train, copy=False)
A:sklearn.linear_model.omp.X_test->X_test.copy().copy()
A:sklearn.linear_model.omp.y_test->as_float_array(y_test, copy=False)
A:sklearn.linear_model.omp.X_mean->X_train.copy().copy().mean(axis=0)
A:sklearn.linear_model.omp.y_mean->as_float_array(y_train, copy=False).mean(axis=0)
A:sklearn.linear_model.omp.norms->numpy.sqrt(np.sum(X_train ** 2, axis=0))
A:sklearn.linear_model.omp.nonzeros->numpy.flatnonzero(norms)
A:sklearn.linear_model.omp.cv->check_cv(self.cv, classifier=False)
A:sklearn.linear_model.omp.cv_paths->Parallel(n_jobs=self.n_jobs, verbose=self.verbose)((delayed(_omp_path_residues)(X[train], y[train], X[test], y[test], self.copy, self.fit_intercept, self.normalize, max_iter) for (train, test) in cv.split(X)))
A:sklearn.linear_model.omp.min_early_stop->min((fold.shape[0] for fold in cv_paths))
A:sklearn.linear_model.omp.mse_folds->numpy.array([(fold[:min_early_stop] ** 2).mean(axis=1) for fold in cv_paths])
A:sklearn.linear_model.omp.omp->OrthogonalMatchingPursuit(n_nonzero_coefs=best_n_nonzero_coefs, fit_intercept=self.fit_intercept, normalize=self.normalize)
sklearn.linear_model.OrthogonalMatchingPursuit(self,n_nonzero_coefs=None,tol=None,fit_intercept=True,normalize=True,precompute='auto')
sklearn.linear_model.OrthogonalMatchingPursuit.fit(self,X,y)
sklearn.linear_model.OrthogonalMatchingPursuitCV(self,copy=True,fit_intercept=True,normalize=True,max_iter=None,cv=None,n_jobs=1,verbose=False)
sklearn.linear_model.OrthogonalMatchingPursuitCV.fit(self,X,y)
sklearn.linear_model.omp.OrthogonalMatchingPursuit(self,n_nonzero_coefs=None,tol=None,fit_intercept=True,normalize=True,precompute='auto')
sklearn.linear_model.omp.OrthogonalMatchingPursuit.__init__(self,n_nonzero_coefs=None,tol=None,fit_intercept=True,normalize=True,precompute='auto')
sklearn.linear_model.omp.OrthogonalMatchingPursuit.fit(self,X,y)
sklearn.linear_model.omp.OrthogonalMatchingPursuitCV(self,copy=True,fit_intercept=True,normalize=True,max_iter=None,cv=None,n_jobs=1,verbose=False)
sklearn.linear_model.omp.OrthogonalMatchingPursuitCV.__init__(self,copy=True,fit_intercept=True,normalize=True,max_iter=None,cv=None,n_jobs=1,verbose=False)
sklearn.linear_model.omp.OrthogonalMatchingPursuitCV.fit(self,X,y)
sklearn.linear_model.omp._cholesky_omp(X,y,n_nonzero_coefs,tol=None,copy_X=True,return_path=False)
sklearn.linear_model.omp._gram_omp(Gram,Xy,n_nonzero_coefs,tol_0=None,tol=None,copy_Gram=True,copy_Xy=True,return_path=False)
sklearn.linear_model.omp._omp_path_residues(X_train,y_train,X_test,y_test,copy=True,fit_intercept=True,normalize=True,max_iter=100)
sklearn.linear_model.omp.orthogonal_mp(X,y,n_nonzero_coefs=None,tol=None,precompute=False,copy_X=True,return_path=False,return_n_iter=False)
sklearn.linear_model.omp.orthogonal_mp_gram(Gram,Xy,n_nonzero_coefs=None,tol=None,norms_squared=None,copy_Gram=True,copy_Xy=True,return_path=False,return_n_iter=False)
sklearn.linear_model.orthogonal_mp(X,y,n_nonzero_coefs=None,tol=None,precompute=False,copy_X=True,return_path=False,return_n_iter=False)
sklearn.linear_model.orthogonal_mp_gram(Gram,Xy,n_nonzero_coefs=None,tol=None,norms_squared=None,copy_Gram=True,copy_Xy=True,return_path=False,return_n_iter=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/linear_model/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/linear_model/huber.py----------------------------------------
A:sklearn.linear_model.huber.X_is_sparse->scipy.sparse.issparse(X)
A:sklearn.linear_model.huber.n_samples->numpy.sum(sample_weight)
A:sklearn.linear_model.huber.abs_linear_loss->numpy.abs(linear_loss)
A:sklearn.linear_model.huber.num_outliers->numpy.count_nonzero(outliers_mask)
A:sklearn.linear_model.huber.n_sw_outliers->numpy.sum(outliers_sw)
A:sklearn.linear_model.huber.weighted_loss->numpy.dot(weighted_non_outliers.T, non_outliers)
A:sklearn.linear_model.huber.grad->numpy.zeros(n_features + 1)
A:sklearn.linear_model.huber.signed_outliers->numpy.ones_like(outliers)
A:sklearn.linear_model.huber.X_outliers->axis0_safe_slice(X, outliers_mask, num_outliers)
A:sklearn.linear_model.huber.(X, y)->check_X_y(X, y, copy=False, accept_sparse=['csr'], y_numeric=True)
A:sklearn.linear_model.huber.sample_weight->numpy.ones_like(y)
A:sklearn.linear_model.huber.parameters->numpy.zeros(X.shape[1] + 1)
A:sklearn.linear_model.huber.bounds->numpy.tile([-np.inf, np.inf], (parameters.shape[0], 1))
A:sklearn.linear_model.huber.(parameters, f, dict_)->scipy.optimize.fmin_l_bfgs_b(_huber_loss_and_gradient, parameters, args=(X, y, self.epsilon, self.alpha, sample_weight), bounds=bounds)
A:sklearn.linear_model.huber.self.n_iter_->dict_.get('nit', None)
A:sklearn.linear_model.huber.residual->numpy.abs(y - safe_sparse_dot(X, self.coef_) - self.intercept_)
sklearn.linear_model.HuberRegressor(self,epsilon=1.35,max_iter=100,alpha=0.0001,warm_start=False,fit_intercept=True,tol=1e-05)
sklearn.linear_model.HuberRegressor.fit(self,X,y,sample_weight=None)
sklearn.linear_model.huber.HuberRegressor(self,epsilon=1.35,max_iter=100,alpha=0.0001,warm_start=False,fit_intercept=True,tol=1e-05)
sklearn.linear_model.huber.HuberRegressor.__init__(self,epsilon=1.35,max_iter=100,alpha=0.0001,warm_start=False,fit_intercept=True,tol=1e-05)
sklearn.linear_model.huber.HuberRegressor.fit(self,X,y,sample_weight=None)
sklearn.linear_model.huber._huber_loss_and_gradient(w,X,y,epsilon,alpha,sample_weight=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/linear_model/ransac.py----------------------------------------
A:sklearn.linear_model.ransac._EPSILON->numpy.spacing(1)
A:sklearn.linear_model.ransac.nom->max(_EPSILON, 1 - probability)
A:sklearn.linear_model.ransac.denom->max(_EPSILON, 1 - inlier_ratio ** min_samples)
A:sklearn.linear_model.ransac.X->check_array(X, accept_sparse='csr')
A:sklearn.linear_model.ransac.y->check_array(y, ensure_2d=False)
A:sklearn.linear_model.ransac.base_estimator->LinearRegression()
A:sklearn.linear_model.ransac.min_samples->numpy.ceil(self.min_samples * X.shape[0])
A:sklearn.linear_model.ransac.residual_threshold->numpy.median(np.abs(y - np.median(y)))
A:sklearn.linear_model.ransac.random_state->check_random_state(self.random_state)
A:sklearn.linear_model.ransac.estimator_fit_has_sample_weight->has_fit_parameter(base_estimator, 'sample_weight')
A:sklearn.linear_model.ransac.sample_weight->numpy.asarray(sample_weight)
A:sklearn.linear_model.ransac.sample_idxs->numpy.arange(n_samples)
A:sklearn.linear_model.ransac.subset_idxs->sample_without_replacement(n_samples, min_samples, random_state=random_state)
A:sklearn.linear_model.ransac.y_pred->LinearRegression().predict(X)
A:sklearn.linear_model.ransac.diff->diff.reshape(-1, 1).reshape(-1, 1)
A:sklearn.linear_model.ransac.residuals_subset->loss_function(y, y_pred)
A:sklearn.linear_model.ransac.n_inliers_subset->numpy.sum(inlier_mask_subset)
A:sklearn.linear_model.ransac.score_subset->LinearRegression().score(X_inlier_subset, y_inlier_subset)
A:sklearn.linear_model.ransac.max_trials->min(max_trials, _dynamic_max_trials(n_inliers_best, n_samples, min_samples, self.stop_probability))
sklearn.linear_model.RANSACRegressor(self,base_estimator=None,min_samples=None,residual_threshold=None,is_data_valid=None,is_model_valid=None,max_trials=100,max_skips=np.inf,stop_n_inliers=np.inf,stop_score=np.inf,stop_probability=0.99,residual_metric=None,loss='absolute_loss',random_state=None)
sklearn.linear_model.RANSACRegressor.fit(self,X,y,sample_weight=None)
sklearn.linear_model.RANSACRegressor.predict(self,X)
sklearn.linear_model.RANSACRegressor.score(self,X,y)
sklearn.linear_model.ransac.RANSACRegressor(self,base_estimator=None,min_samples=None,residual_threshold=None,is_data_valid=None,is_model_valid=None,max_trials=100,max_skips=np.inf,stop_n_inliers=np.inf,stop_score=np.inf,stop_probability=0.99,residual_metric=None,loss='absolute_loss',random_state=None)
sklearn.linear_model.ransac.RANSACRegressor.__init__(self,base_estimator=None,min_samples=None,residual_threshold=None,is_data_valid=None,is_model_valid=None,max_trials=100,max_skips=np.inf,stop_n_inliers=np.inf,stop_score=np.inf,stop_probability=0.99,residual_metric=None,loss='absolute_loss',random_state=None)
sklearn.linear_model.ransac.RANSACRegressor.fit(self,X,y,sample_weight=None)
sklearn.linear_model.ransac.RANSACRegressor.predict(self,X)
sklearn.linear_model.ransac.RANSACRegressor.score(self,X,y)
sklearn.linear_model.ransac._dynamic_max_trials(n_inliers,n_samples,min_samples,probability)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/linear_model/theil_sen.py----------------------------------------
A:sklearn.linear_model.theil_sen.diff_norm->numpy.sqrt(np.sum(diff ** 2, axis=1))
A:sklearn.linear_model.theil_sen.is_x_old_in_X->int(mask.sum() < X.shape[0])
A:sklearn.linear_model.theil_sen.quotient_norm->scipy.linalg.norm(np.sum(diff / diff_norm, axis=0))
A:sklearn.linear_model.theil_sen.spatial_median_old->numpy.mean(X, axis=0)
A:sklearn.linear_model.theil_sen.spatial_median->_modified_weiszfeld_step(X, spatial_median_old)
A:sklearn.linear_model.theil_sen.fit_intercept->int(fit_intercept)
A:sklearn.linear_model.theil_sen.weights->numpy.vstack(weights)
A:sklearn.linear_model.theil_sen.X_subpopulation->numpy.ones((n_subsamples, n_features))
A:sklearn.linear_model.theil_sen.y_subpopulation->numpy.zeros(max(n_subsamples, n_features))
A:sklearn.linear_model.theil_sen.(lstsq,)->get_lapack_funcs(('gelss',), (X_subpopulation, y_subpopulation))
A:sklearn.linear_model.theil_sen.self.max_subpopulation->int(max_subpopulation)
A:sklearn.linear_model.theil_sen.n_subsamples->min(n_dim, n_samples)
A:sklearn.linear_model.theil_sen.all_combinations->max(1, np.rint(binom(n_samples, n_subsamples)))
A:sklearn.linear_model.theil_sen.n_subpopulation->int(min(self.max_subpopulation, all_combinations))
A:sklearn.linear_model.theil_sen.random_state->check_random_state(self.random_state)
A:sklearn.linear_model.theil_sen.(X, y)->check_X_y(X, y, y_numeric=True)
A:sklearn.linear_model.theil_sen.(n_subsamples, self.n_subpopulation_)->self._check_subparams(n_samples, n_features)
A:sklearn.linear_model.theil_sen.self.breakdown_->_breakdown_point(n_samples, n_subsamples)
A:sklearn.linear_model.theil_sen.tol_outliers->int(self.breakdown_ * n_samples)
A:sklearn.linear_model.theil_sen.indices->list(combinations(range(n_samples), n_subsamples))
A:sklearn.linear_model.theil_sen.n_jobs->_get_n_jobs(self.n_jobs)
A:sklearn.linear_model.theil_sen.index_list->numpy.array_split(indices, n_jobs)
A:sklearn.linear_model.theil_sen.(self.n_iter_, coefs)->_spatial_median(weights, max_iter=self.max_iter, tol=self.tol)
sklearn.linear_model.TheilSenRegressor(self,fit_intercept=True,copy_X=True,max_subpopulation=10000.0,n_subsamples=None,max_iter=300,tol=0.001,random_state=None,n_jobs=1,verbose=False)
sklearn.linear_model.TheilSenRegressor._check_subparams(self,n_samples,n_features)
sklearn.linear_model.TheilSenRegressor.fit(self,X,y)
sklearn.linear_model.theil_sen.TheilSenRegressor(self,fit_intercept=True,copy_X=True,max_subpopulation=10000.0,n_subsamples=None,max_iter=300,tol=0.001,random_state=None,n_jobs=1,verbose=False)
sklearn.linear_model.theil_sen.TheilSenRegressor.__init__(self,fit_intercept=True,copy_X=True,max_subpopulation=10000.0,n_subsamples=None,max_iter=300,tol=0.001,random_state=None,n_jobs=1,verbose=False)
sklearn.linear_model.theil_sen.TheilSenRegressor._check_subparams(self,n_samples,n_features)
sklearn.linear_model.theil_sen.TheilSenRegressor.fit(self,X,y)
sklearn.linear_model.theil_sen._breakdown_point(n_samples,n_subsamples)
sklearn.linear_model.theil_sen._lstsq(X,y,indices,fit_intercept)
sklearn.linear_model.theil_sen._modified_weiszfeld_step(X,x_old)
sklearn.linear_model.theil_sen._spatial_median(X,max_iter=300,tol=0.001)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/linear_model/logistic.py----------------------------------------
A:sklearn.linear_model.logistic.grad->numpy.zeros((n_classes, n_features + bool(fit_intercept)), dtype=X.dtype)
A:sklearn.linear_model.logistic.(w, c, yz)->_intercept_dot(w, X, y)
A:sklearn.linear_model.logistic.sample_weight->check_array(sample_weight, ensure_2d=False)
A:sklearn.linear_model.logistic.z->expit(yz)
A:sklearn.linear_model.logistic.grad[-1]->z0.sum()
A:sklearn.linear_model.logistic.dX->safe_sparse_dot(sparse.dia_matrix((d, 0), shape=(n_samples, n_samples)), X)
A:sklearn.linear_model.logistic.dd_intercept->numpy.squeeze(np.array(dX.sum(axis=0)))
A:sklearn.linear_model.logistic.ret->numpy.empty_like(s)
A:sklearn.linear_model.logistic.ret[:n_features]->check_array(X, accept_sparse='csr', dtype=np.float64).T.dot(dX.dot(s[:n_features]))
A:sklearn.linear_model.logistic.ret[-1]->numpy.squeeze(np.array(dX.sum(axis=0))).dot(s[:n_features])
A:sklearn.linear_model.logistic.w->numpy.mean([coefs_paths[i][best_indices[i]] for i in range(len(folds))], axis=0)
A:sklearn.linear_model.logistic.p->numpy.exp(p, p)
A:sklearn.linear_model.logistic.(loss, p, w)->_multinomial_loss(w, X, Y, alpha, sample_weight)
A:sklearn.linear_model.logistic.grad[:, :n_features]->safe_sparse_dot(diff.T, X)
A:sklearn.linear_model.logistic.grad[:, -1]->diff.sum(axis=0)
A:sklearn.linear_model.logistic.(loss, grad, p)->_multinomial_loss_grad(w, X, Y, alpha, sample_weight)
A:sklearn.linear_model.logistic.v->v.reshape(n_classes, -1).reshape(n_classes, -1)
A:sklearn.linear_model.logistic.r_yhat->safe_sparse_dot(X, v.T)
A:sklearn.linear_model.logistic.hessProd->numpy.zeros((n_classes, n_features + bool(fit_intercept)))
A:sklearn.linear_model.logistic.hessProd[:, :n_features]->safe_sparse_dot(r_yhat.T, X)
A:sklearn.linear_model.logistic.hessProd[:, -1]->safe_sparse_dot(X, v.T).sum(axis=0)
A:sklearn.linear_model.logistic.Cs->numpy.logspace(-4, 4, Cs)
A:sklearn.linear_model.logistic.X->check_array(X, accept_sparse='csr', dtype=np.float64)
A:sklearn.linear_model.logistic.y->LabelEncoder().fit(y).transform(y)
A:sklearn.linear_model.logistic.classes->numpy.unique(y)
A:sklearn.linear_model.logistic.random_state->check_random_state(random_state)
A:sklearn.linear_model.logistic.le->LabelEncoder()
A:sklearn.linear_model.logistic.class_weight_->compute_class_weight(class_weight, mask_classes, y_bin)
A:sklearn.linear_model.logistic.w0->coef_.ravel()
A:sklearn.linear_model.logistic.mask_classes->numpy.array([-1, 1])
A:sklearn.linear_model.logistic.y_bin->numpy.ones(y.shape, dtype=X.dtype)
A:sklearn.linear_model.logistic.lbin->LabelBinarizer()
A:sklearn.linear_model.logistic.Y_multi->LabelEncoder().fit_transform(y).astype(X.dtype, copy=False)
A:sklearn.linear_model.logistic.coefs->list()
A:sklearn.linear_model.logistic.n_iter->numpy.zeros(len(Cs), dtype=np.int32)
A:sklearn.linear_model.logistic.(w0, loss, info)->scipy.optimize.fmin_l_bfgs_b(func, w0, fprime=None, args=(X, target, 1.0 / C, sample_weight), iprint=(verbose > 0) - 1, pgtol=tol)
A:sklearn.linear_model.logistic.(w0, n_iter_i)->newton_cg(hess, func, grad, w0, args=args, maxiter=max_iter, tol=tol)
A:sklearn.linear_model.logistic.(coef_, intercept_, n_iter_i)->_fit_liblinear(X, target, C, fit_intercept, intercept_scaling, None, penalty, dual, verbose, max_iter, tol, random_state, sample_weight=sample_weight)
A:sklearn.linear_model.logistic.target->target.astype(np.float64).astype(np.float64)
A:sklearn.linear_model.logistic.(w0, n_iter_i, warm_start_sag)->sag_solver(X, target, sample_weight, loss, alpha, beta, max_iter, tol, verbose, random_state, False, max_squared_sum, warm_start_sag, is_saga=solver == 'saga')
A:sklearn.linear_model.logistic.multi_w0->numpy.reshape(w0, (classes.size, -1))
A:sklearn.linear_model.logistic.(coefs, Cs, n_iter)->logistic_regression_path(X_train, y_train, Cs=Cs, fit_intercept=fit_intercept, solver=solver, max_iter=max_iter, class_weight=class_weight, pos_class=pos_class, multi_class=multi_class, tol=tol, verbose=verbose, dual=dual, penalty=penalty, intercept_scaling=intercept_scaling, random_state=random_state, check_input=False, max_squared_sum=max_squared_sum, sample_weight=sample_weight)
A:sklearn.linear_model.logistic.log_reg->LogisticRegression(fit_intercept=fit_intercept)
A:sklearn.linear_model.logistic.log_reg.classes_->numpy.unique(y_train)
A:sklearn.linear_model.logistic.y_test->numpy.ones(y_test.shape, dtype=np.float64)
A:sklearn.linear_model.logistic.scores->numpy.reshape(scores, (n_classes, len(folds), -1))
A:sklearn.linear_model.logistic.(X, y)->check_X_y(X, y, accept_sparse='csr', dtype=np.float64, order='C')
A:sklearn.linear_model.logistic.self.classes_->numpy.unique(y)
A:sklearn.linear_model.logistic.(self.coef_, self.intercept_, n_iter_)->_fit_liblinear(X, y, self.C, self.fit_intercept, self.intercept_scaling, self.class_weight, self.penalty, self.dual, self.verbose, self.max_iter, self.tol, self.random_state, sample_weight=sample_weight)
A:sklearn.linear_model.logistic.self.n_iter_->numpy.reshape(n_iter_, (n_classes, len(folds), len(self.Cs_)))
A:sklearn.linear_model.logistic.max_squared_sum->row_norms(X, squared=True).max()
A:sklearn.linear_model.logistic.n_classes->len(encoded_labels)
A:sklearn.linear_model.logistic.warm_start_coef->numpy.append(warm_start_coef, self.intercept_[:, np.newaxis], axis=1)
A:sklearn.linear_model.logistic.self.coef_->numpy.empty((n_classes, X.shape[1]))
A:sklearn.linear_model.logistic.self.intercept_->numpy.zeros(n_classes)
A:sklearn.linear_model.logistic.path_func->delayed(_log_reg_scoring_path)
A:sklearn.linear_model.logistic.fold_coefs_->Parallel(n_jobs=self.n_jobs, verbose=self.verbose, backend=backend)((path_func(X, y, train, test, pos_class=label, Cs=self.Cs, fit_intercept=self.fit_intercept, penalty=self.penalty, dual=self.dual, solver=self.solver, tol=self.tol, max_iter=self.max_iter, verbose=self.verbose, class_weight=class_weight, scoring=self.scoring, multi_class=self.multi_class, intercept_scaling=self.intercept_scaling, random_state=self.random_state, max_squared_sum=max_squared_sum, sample_weight=sample_weight) for label in iter_encoded_labels for (train, test) in folds))
A:sklearn.linear_model.logistic.(fold_coefs_, _, n_iter_)->zip(*fold_coefs_)
A:sklearn.linear_model.logistic.label_encoder->LabelEncoder().fit(y)
A:sklearn.linear_model.logistic.class_weight->dict(enumerate(class_weight))
A:sklearn.linear_model.logistic.encoded_labels->LabelEncoder().fit(y).transform(label_encoder.classes_)
A:sklearn.linear_model.logistic.cv->check_cv(self.cv, y, classifier=True)
A:sklearn.linear_model.logistic.folds->list(cv.split(X, y))
A:sklearn.linear_model.logistic.(multi_coefs_paths, Cs, multi_scores, n_iter_)->zip(*fold_coefs_)
A:sklearn.linear_model.logistic.multi_coefs_paths->numpy.asarray(multi_coefs_paths)
A:sklearn.linear_model.logistic.multi_scores->numpy.asarray(multi_scores)
A:sklearn.linear_model.logistic.coefs_paths->numpy.reshape(coefs_paths, (n_classes, len(folds), len(self.Cs_), -1))
A:sklearn.linear_model.logistic.(coefs_paths, Cs, scores, n_iter_)->zip(*fold_coefs_)
A:sklearn.linear_model.logistic.self.coefs_paths_->dict(zip(classes, coefs_paths))
A:sklearn.linear_model.logistic.self.scores_->dict(zip(classes, scores))
A:sklearn.linear_model.logistic.self.C_->numpy.asarray(self.C_)
A:sklearn.linear_model.logistic.best_index->numpy.reshape(scores, (n_classes, len(folds), -1)).sum(axis=0).argmax()
A:sklearn.linear_model.logistic.coef_init->numpy.mean(coefs_paths[:, best_index, :], axis=0)
A:sklearn.linear_model.logistic.(w, _, _)->logistic_regression_path(X, y, pos_class=encoded_label, Cs=[C_], solver=self.solver, fit_intercept=self.fit_intercept, coef=coef_init, max_iter=self.max_iter, tol=self.tol, penalty=self.penalty, class_weight=class_weight, multi_class=self.multi_class, verbose=max(0, self.verbose - 1), random_state=self.random_state, check_input=False, max_squared_sum=max_squared_sum, sample_weight=sample_weight)
A:sklearn.linear_model.logistic.best_indices->numpy.argmax(scores, axis=1)
sklearn.linear_model.LogisticRegression(self,penalty='l2',dual=False,tol=0.0001,C=1.0,fit_intercept=True,intercept_scaling=1,class_weight=None,random_state=None,solver='liblinear',max_iter=100,multi_class='ovr',verbose=0,warm_start=False,n_jobs=1)
sklearn.linear_model.LogisticRegression.fit(self,X,y,sample_weight=None)
sklearn.linear_model.LogisticRegression.predict_log_proba(self,X)
sklearn.linear_model.LogisticRegression.predict_proba(self,X)
sklearn.linear_model.LogisticRegressionCV(self,Cs=10,fit_intercept=True,cv=None,dual=False,penalty='l2',scoring=None,solver='lbfgs',tol=0.0001,max_iter=100,class_weight=None,n_jobs=1,verbose=0,refit=True,intercept_scaling=1.0,multi_class='ovr',random_state=None)
sklearn.linear_model.LogisticRegressionCV.fit(self,X,y,sample_weight=None)
sklearn.linear_model.logistic.LogisticRegression(self,penalty='l2',dual=False,tol=0.0001,C=1.0,fit_intercept=True,intercept_scaling=1,class_weight=None,random_state=None,solver='liblinear',max_iter=100,multi_class='ovr',verbose=0,warm_start=False,n_jobs=1)
sklearn.linear_model.logistic.LogisticRegression.__init__(self,penalty='l2',dual=False,tol=0.0001,C=1.0,fit_intercept=True,intercept_scaling=1,class_weight=None,random_state=None,solver='liblinear',max_iter=100,multi_class='ovr',verbose=0,warm_start=False,n_jobs=1)
sklearn.linear_model.logistic.LogisticRegression.fit(self,X,y,sample_weight=None)
sklearn.linear_model.logistic.LogisticRegression.predict_log_proba(self,X)
sklearn.linear_model.logistic.LogisticRegression.predict_proba(self,X)
sklearn.linear_model.logistic.LogisticRegressionCV(self,Cs=10,fit_intercept=True,cv=None,dual=False,penalty='l2',scoring=None,solver='lbfgs',tol=0.0001,max_iter=100,class_weight=None,n_jobs=1,verbose=0,refit=True,intercept_scaling=1.0,multi_class='ovr',random_state=None)
sklearn.linear_model.logistic.LogisticRegressionCV.__init__(self,Cs=10,fit_intercept=True,cv=None,dual=False,penalty='l2',scoring=None,solver='lbfgs',tol=0.0001,max_iter=100,class_weight=None,n_jobs=1,verbose=0,refit=True,intercept_scaling=1.0,multi_class='ovr',random_state=None)
sklearn.linear_model.logistic.LogisticRegressionCV.fit(self,X,y,sample_weight=None)
sklearn.linear_model.logistic._check_solver_option(solver,multi_class,penalty,dual)
sklearn.linear_model.logistic._intercept_dot(w,X,y)
sklearn.linear_model.logistic._log_reg_scoring_path(X,y,train,test,pos_class=None,Cs=10,scoring=None,fit_intercept=False,max_iter=100,tol=0.0001,class_weight=None,verbose=0,solver='lbfgs',penalty='l2',dual=False,intercept_scaling=1.0,multi_class='ovr',random_state=None,max_squared_sum=None,sample_weight=None)
sklearn.linear_model.logistic._logistic_grad_hess(w,X,y,alpha,sample_weight=None)
sklearn.linear_model.logistic._logistic_loss(w,X,y,alpha,sample_weight=None)
sklearn.linear_model.logistic._logistic_loss_and_grad(w,X,y,alpha,sample_weight=None)
sklearn.linear_model.logistic._multinomial_grad_hess(w,X,Y,alpha,sample_weight)
sklearn.linear_model.logistic._multinomial_loss(w,X,Y,alpha,sample_weight)
sklearn.linear_model.logistic._multinomial_loss_grad(w,X,Y,alpha,sample_weight)
sklearn.linear_model.logistic.logistic_regression_path(X,y,pos_class=None,Cs=10,fit_intercept=True,max_iter=100,tol=0.0001,verbose=0,solver='lbfgs',coef=None,class_weight=None,dual=False,penalty='l2',intercept_scaling=1.0,multi_class='ovr',random_state=None,check_input=True,max_squared_sum=None,sample_weight=None)
sklearn.linear_model.logistic_regression_path(X,y,pos_class=None,Cs=10,fit_intercept=True,max_iter=100,tol=0.0001,verbose=0,solver='lbfgs',coef=None,class_weight=None,dual=False,penalty='l2',intercept_scaling=1.0,multi_class='ovr',random_state=None,check_input=True,max_squared_sum=None,sample_weight=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/linear_model/randomized_l1.py----------------------------------------
A:sklearn.linear_model.randomized_l1.random_state->check_random_state(random_state)
A:sklearn.linear_model.randomized_l1._preprocess_data->staticmethod(_preprocess_data)
A:sklearn.linear_model.randomized_l1.(X, y)->check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'])
A:sklearn.linear_model.randomized_l1.X->as_float_array(X, copy=False)
A:sklearn.linear_model.randomized_l1.(X, y, X_offset, y_offset, X_scale)->self._preprocess_data(X, y, self.fit_intercept, self.normalize)
A:sklearn.linear_model.randomized_l1.(estimator_func, params)->self._make_estimator_and_params(X, y)
A:sklearn.linear_model.randomized_l1.memory->Memory(cachedir=memory, verbose=0)
A:sklearn.linear_model.randomized_l1.scores_->Memory(cachedir=memory, verbose=0).cache(_resample_model, ignore=['verbose', 'n_jobs', 'pre_dispatch'])(estimator_func, X, y, scaling=self.scaling, n_resampling=self.n_resampling, n_jobs=self.n_jobs, verbose=self.verbose, pre_dispatch=self.pre_dispatch, random_state=self.random_state, sample_fraction=self.sample_fraction, **params)
A:sklearn.linear_model.randomized_l1.self.scores_->numpy.max(self.all_scores_, axis=1)
A:sklearn.linear_model.randomized_l1.alpha->numpy.atleast_1d(np.asarray(alpha, dtype=np.float64))
A:sklearn.linear_model.randomized_l1.(alphas_, _, coef_)->lars_path(X, y, Gram=precompute, copy_X=False, copy_Gram=False, alpha_min=np.min(alpha), method='lasso', verbose=verbose, max_iter=max_iter, eps=eps)
A:sklearn.linear_model.randomized_l1.interpolator->interp1d(alphas_[::-1], coef_[:, ::-1], bounds_error=False, fill_value=0.0)
A:sklearn.linear_model.randomized_l1.scores->numpy.zeros((X.shape[1], len(C)), dtype=np.bool)
A:sklearn.linear_model.randomized_l1.model->LassoLarsIC(precompute=self.precompute, criterion=self.alpha, max_iter=self.max_iter, eps=self.eps)
A:sklearn.linear_model.randomized_l1.size->len(weights)
A:sklearn.linear_model.randomized_l1.weight_dia->scipy.sparse.dia_matrix((1 - weights, 0), (size, size))
A:sklearn.linear_model.randomized_l1.C->numpy.atleast_1d(np.asarray(C, dtype=np.float64))
A:sklearn.linear_model.randomized_l1.clf->LogisticRegression(C=this_C, tol=tol, penalty='l1', dual=False, fit_intercept=fit_intercept)
A:sklearn.linear_model.randomized_l1.this_scores[:]->numpy.any(np.abs(clf.coef_) > 10 * np.finfo(np.float).eps, axis=0)
A:sklearn.linear_model.randomized_l1.params->dict(C=self.C, tol=self.tol, fit_intercept=self.fit_intercept)
A:sklearn.linear_model.randomized_l1.(X, _, X_offset, _, X_scale)->_preprocess_data(X, y, fit_intercept, normalize=normalize)
A:sklearn.linear_model.randomized_l1.(alphas, _, coefs)->lars_path(X, y, method='lasso', verbose=False, alpha_min=alpha_min)
A:sklearn.linear_model.randomized_l1.rng->check_random_state(random_state)
A:sklearn.linear_model.randomized_l1.paths->Parallel(n_jobs=n_jobs, verbose=verbose)((delayed(_lasso_stability_path)(X, y, mask=rng.rand(n_samples) < sample_fraction, weights=1.0 - scaling * rng.randint(0, 2, size=(n_features,)), eps=eps) for k in range(n_resampling)))
A:sklearn.linear_model.randomized_l1.all_alphas->numpy.array(all_alphas)
A:sklearn.linear_model.randomized_l1.stride->int(max(1, int(len(all_alphas) / float(n_grid))))
A:sklearn.linear_model.randomized_l1.scores_path->numpy.zeros((n_features, len(all_alphas)))
sklearn.linear_model.RandomizedLasso(self,alpha='aic',scaling=0.5,sample_fraction=0.75,n_resampling=200,selection_threshold=0.25,fit_intercept=True,verbose=False,normalize=True,precompute='auto',max_iter=500,eps=np.finfo(np.float).eps,random_state=None,n_jobs=1,pre_dispatch='3*n_jobs',memory=None)
sklearn.linear_model.RandomizedLasso._make_estimator_and_params(self,X,y)
sklearn.linear_model.RandomizedLogisticRegression(self,C=1,scaling=0.5,sample_fraction=0.75,n_resampling=200,selection_threshold=0.25,tol=0.001,fit_intercept=True,verbose=False,normalize=True,random_state=None,n_jobs=1,pre_dispatch='3*n_jobs',memory=None)
sklearn.linear_model.RandomizedLogisticRegression._make_estimator_and_params(self,X,y)
sklearn.linear_model.RandomizedLogisticRegression._preprocess_data(self,X,y,fit_intercept,normalize=False)
sklearn.linear_model.lasso_stability_path(X,y,scaling=0.5,random_state=None,n_resampling=200,n_grid=100,sample_fraction=0.75,eps=4*np.finfo(np.float).eps,n_jobs=1,verbose=False)
sklearn.linear_model.randomized_l1.BaseRandomizedLinearModel(self)
sklearn.linear_model.randomized_l1.BaseRandomizedLinearModel.__init__(self)
sklearn.linear_model.randomized_l1.BaseRandomizedLinearModel._get_support_mask(self)
sklearn.linear_model.randomized_l1.BaseRandomizedLinearModel._make_estimator_and_params(self,X,y)
sklearn.linear_model.randomized_l1.BaseRandomizedLinearModel.fit(self,X,y)
sklearn.linear_model.randomized_l1.RandomizedLasso(self,alpha='aic',scaling=0.5,sample_fraction=0.75,n_resampling=200,selection_threshold=0.25,fit_intercept=True,verbose=False,normalize=True,precompute='auto',max_iter=500,eps=np.finfo(np.float).eps,random_state=None,n_jobs=1,pre_dispatch='3*n_jobs',memory=None)
sklearn.linear_model.randomized_l1.RandomizedLasso.__init__(self,alpha='aic',scaling=0.5,sample_fraction=0.75,n_resampling=200,selection_threshold=0.25,fit_intercept=True,verbose=False,normalize=True,precompute='auto',max_iter=500,eps=np.finfo(np.float).eps,random_state=None,n_jobs=1,pre_dispatch='3*n_jobs',memory=None)
sklearn.linear_model.randomized_l1.RandomizedLasso._make_estimator_and_params(self,X,y)
sklearn.linear_model.randomized_l1.RandomizedLogisticRegression(self,C=1,scaling=0.5,sample_fraction=0.75,n_resampling=200,selection_threshold=0.25,tol=0.001,fit_intercept=True,verbose=False,normalize=True,random_state=None,n_jobs=1,pre_dispatch='3*n_jobs',memory=None)
sklearn.linear_model.randomized_l1.RandomizedLogisticRegression.__init__(self,C=1,scaling=0.5,sample_fraction=0.75,n_resampling=200,selection_threshold=0.25,tol=0.001,fit_intercept=True,verbose=False,normalize=True,random_state=None,n_jobs=1,pre_dispatch='3*n_jobs',memory=None)
sklearn.linear_model.randomized_l1.RandomizedLogisticRegression._make_estimator_and_params(self,X,y)
sklearn.linear_model.randomized_l1.RandomizedLogisticRegression._preprocess_data(self,X,y,fit_intercept,normalize=False)
sklearn.linear_model.randomized_l1._lasso_stability_path(X,y,mask,weights,eps)
sklearn.linear_model.randomized_l1._randomized_lasso(X,y,weights,mask,alpha=1.0,verbose=False,precompute=False,eps=np.finfo(np.float).eps,max_iter=500)
sklearn.linear_model.randomized_l1._randomized_logistic(X,y,weights,mask,C=1.0,verbose=False,fit_intercept=True,tol=0.001)
sklearn.linear_model.randomized_l1._resample_model(estimator_func,X,y,scaling=0.5,n_resampling=200,n_jobs=1,verbose=False,pre_dispatch='3*n_jobs',random_state=None,sample_fraction=0.75,**params)
sklearn.linear_model.randomized_l1.lasso_stability_path(X,y,scaling=0.5,random_state=None,n_resampling=200,n_grid=100,sample_fraction=0.75,eps=4*np.finfo(np.float).eps,n_jobs=1,verbose=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/linear_model/perceptron.py----------------------------------------
sklearn.linear_model.Perceptron(self,penalty=None,alpha=0.0001,fit_intercept=True,max_iter=None,tol=None,shuffle=True,verbose=0,eta0=1.0,n_jobs=1,random_state=0,class_weight=None,warm_start=False,n_iter=None)
sklearn.linear_model.perceptron.Perceptron(self,penalty=None,alpha=0.0001,fit_intercept=True,max_iter=None,tol=None,shuffle=True,verbose=0,eta0=1.0,n_jobs=1,random_state=0,class_weight=None,warm_start=False,n_iter=None)
sklearn.linear_model.perceptron.Perceptron.__init__(self,penalty=None,alpha=0.0001,fit_intercept=True,max_iter=None,tol=None,shuffle=True,verbose=0,eta0=1.0,n_jobs=1,random_state=0,class_weight=None,warm_start=False,n_iter=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/linear_model/sag.py----------------------------------------
A:sklearn.linear_model.sag.mun->min(2 * n_samples * alpha_scaled, L)
A:sklearn.linear_model.sag.X->check_array(X, dtype=np.float64, accept_sparse='csr', order='C')
A:sklearn.linear_model.sag.y->check_array(y, dtype=np.float64, ensure_2d=False, order='C')
A:sklearn.linear_model.sag.sample_weight->numpy.ones(n_samples, dtype=np.float64, order='C')
A:sklearn.linear_model.sag.coef_init->numpy.vstack((coef_init, intercept_init))
A:sklearn.linear_model.sag.intercept_init->numpy.zeros(n_classes, dtype=np.float64)
A:sklearn.linear_model.sag.intercept_sum_gradient->numpy.zeros(n_classes, dtype=np.float64)
A:sklearn.linear_model.sag.gradient_memory_init->numpy.zeros((n_samples, n_classes), dtype=np.float64, order='C')
A:sklearn.linear_model.sag.sum_gradient_init->numpy.zeros((n_features, n_classes), dtype=np.float64, order='C')
A:sklearn.linear_model.sag.seen_init->numpy.zeros(n_samples, dtype=np.int32, order='C')
A:sklearn.linear_model.sag.(dataset, intercept_decay)->make_dataset(X, y, sample_weight, random_state)
A:sklearn.linear_model.sag.max_squared_sum->row_norms(X, squared=True).max()
A:sklearn.linear_model.sag.step_size->get_auto_step_size(max_squared_sum, alpha_scaled, loss, fit_intercept, n_samples=n_samples, is_saga=is_saga)
A:sklearn.linear_model.sag.(num_seen, n_iter_)->sag(dataset, coef_init, intercept_init, n_samples, n_features, n_classes, tol, max_iter, loss, step_size, alpha_scaled, beta_scaled, sum_gradient_init, gradient_memory_init, seen_init, num_seen_init, fit_intercept, intercept_sum_gradient, intercept_decay, is_saga, verbose)
sklearn.linear_model.sag.get_auto_step_size(max_squared_sum,alpha_scaled,loss,fit_intercept,n_samples=None,is_saga=False)
sklearn.linear_model.sag.sag_solver(X,y,sample_weight=None,loss='log',alpha=1.0,beta=0.0,max_iter=1000,tol=0.001,verbose=0,random_state=None,check_input=True,max_squared_sum=None,warm_start_mem=None,is_saga=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/linear_model/ridge.py----------------------------------------
A:sklearn.linear_model.ridge.X1->scipy.sparse.linalg.aslinearoperator(X)
A:sklearn.linear_model.ridge.coefs->numpy.empty([n_targets, n_features], dtype=X.dtype)
A:sklearn.linear_model.ridge.mv->create_mv(alpha[i])
A:sklearn.linear_model.ridge.C->scipy.sparse.linalg.LinearOperator((n_features, n_features), matvec=mv, dtype=X.dtype)
A:sklearn.linear_model.ridge.(coef, info)->scipy.sparse.linalg.cg(C, y_column, tol=tol)
A:sklearn.linear_model.ridge.coefs[i]->scipy.sparse.linalg.aslinearoperator(X).rmatvec(coef)
A:sklearn.linear_model.ridge.y_column->scipy.sparse.linalg.aslinearoperator(X).rmatvec(y_column)
A:sklearn.linear_model.ridge.(coefs[i], info)->scipy.sparse.linalg.cg(C, y_column, maxiter=max_iter, tol=tol)
A:sklearn.linear_model.ridge.n_iter->numpy.empty(y.shape[1], dtype=np.int32)
A:sklearn.linear_model.ridge.sqrt_alpha->numpy.sqrt(alpha)
A:sklearn.linear_model.ridge.info->scipy.sparse.linalg.lsqr(X, y_column, damp=sqrt_alpha[i], atol=tol, btol=tol, iter_lim=max_iter)
A:sklearn.linear_model.ridge.A->safe_sparse_dot(X.T, X, dense_output=True)
A:sklearn.linear_model.ridge.Xy->safe_sparse_dot(X.T, y, dense_output=True)
A:sklearn.linear_model.ridge.one_alpha->(alpha == alpha[0]).all()
A:sklearn.linear_model.ridge.coef[:]->scipy.linalg.solve(A, target, sym_pos=True, overwrite_a=False).ravel()
A:sklearn.linear_model.ridge.K->safe_sparse_dot(X, X.T, dense_output=True)
A:sklearn.linear_model.ridge.alpha->numpy.repeat(alpha, n_targets)
A:sklearn.linear_model.ridge.sw->numpy.sqrt(np.atleast_1d(sample_weight))
A:sklearn.linear_model.ridge.dual_coef->_solve_cholesky_kernel(K, y, alpha)
A:sklearn.linear_model.ridge.dual_coefs->numpy.empty([n_targets, n_samples], K.dtype)
A:sklearn.linear_model.ridge.dual_coef[:]->scipy.linalg.solve(K, target, sym_pos=True, overwrite_a=False).ravel()
A:sklearn.linear_model.ridge.(U, s, Vt)->scipy.linalg.svd(X, full_matrices=False)
A:sklearn.linear_model.ridge.UTy->numpy.dot(U.T, y)
A:sklearn.linear_model.ridge.d->numpy.zeros((s.size, alpha.size), dtype=X.dtype)
A:sklearn.linear_model.ridge.X->numpy.hstack((X, np.ones((X.shape[0], 1))))
A:sklearn.linear_model.ridge.y->column_or_1d(y, warn=True)
A:sklearn.linear_model.ridge.(X, y)->_rescale_data(X, y, sample_weight)
A:sklearn.linear_model.ridge.coef->coef.ravel().ravel()
A:sklearn.linear_model.ridge.(coef, n_iter)->_solve_lsqr(X, y, alpha, max_iter, tol)
A:sklearn.linear_model.ridge.max_squared_sum->row_norms(X, squared=True).max()
A:sklearn.linear_model.ridge.intercept->numpy.zeros((y.shape[1],))
A:sklearn.linear_model.ridge.(coef_, n_iter_, _)->sag_solver(X, target.ravel(), sample_weight, 'squared', alpha_i, 0, max_iter, tol, verbose, random_state, False, max_squared_sum, init, is_saga=solver == 'saga')
A:sklearn.linear_model.ridge.(X, y, X_offset, y_offset, X_scale)->base.LinearModel._preprocess_data(X, y, self.fit_intercept, self.normalize, self.copy_X, sample_weight=sample_weight)
A:sklearn.linear_model.ridge.(self.coef_, self.n_iter_, self.intercept_)->ridge_regression(X, y, alpha=self.alpha, sample_weight=sample_weight, max_iter=self.max_iter, tol=self.tol, solver=self.solver, random_state=self.random_state, return_n_iter=True, return_intercept=True)
A:sklearn.linear_model.ridge.(self.coef_, self.n_iter_)->ridge_regression(X, y, alpha=self.alpha, sample_weight=sample_weight, max_iter=self.max_iter, tol=self.tol, solver=self.solver, random_state=self.random_state, return_n_iter=True, return_intercept=False)
A:sklearn.linear_model.ridge.self._label_binarizer->LabelBinarizer(pos_label=1, neg_label=-1)
A:sklearn.linear_model.ridge.Y->self._label_binarizer.fit_transform(y)
A:sklearn.linear_model.ridge.self.alphas->numpy.asarray(alphas)
A:sklearn.linear_model.ridge.(v, Q)->scipy.linalg.eigh(K)
A:sklearn.linear_model.ridge.QT_y->numpy.dot(Q.T, y)
A:sklearn.linear_model.ridge.c->numpy.dot(Q, self._diag_dot(w, QT_y))
A:sklearn.linear_model.ridge.G_diag->self._decomp_diag(w, Q)
A:sklearn.linear_model.ridge.(G_diag, c)->self._errors_and_values_svd_helper(alpha, y, v, U, UT_y)
A:sklearn.linear_model.ridge.(U, s, _)->scipy.linalg.svd(X, full_matrices=0)
A:sklearn.linear_model.ridge.UT_y->numpy.dot(U.T, y)
A:sklearn.linear_model.ridge.sample_weight->check_array(sample_weight, ensure_2d=False)
A:sklearn.linear_model.ridge.with_sw->len(np.shape(sample_weight))
A:sklearn.linear_model.ridge.(v, Q, QT_y)->_pre_compute(X, y, centered_kernel)
A:sklearn.linear_model.ridge.cv_values->numpy.zeros((n_samples * n_y, len(self.alphas)))
A:sklearn.linear_model.ridge.scorer->check_scoring(self, scoring=self.scoring, allow_none=True)
A:sklearn.linear_model.ridge.(out, c)->_values(alpha, y, v, Q, QT_y)
A:sklearn.linear_model.ridge.cv_values[:, i]->out.ravel()
A:sklearn.linear_model.ridge.best->numpy.argmax(out)
A:sklearn.linear_model.ridge.self.coef_->safe_sparse_dot(self.dual_coef_.T, X)
A:sklearn.linear_model.ridge.self.cv_values_->numpy.zeros((n_samples * n_y, len(self.alphas))).reshape(cv_values_shape)
A:sklearn.linear_model.ridge.estimator->_RidgeGCV(self.alphas, fit_intercept=self.fit_intercept, normalize=self.normalize, scoring=self.scoring, gcv_mode=self.gcv_mode, store_cv_values=self.store_cv_values)
A:sklearn.linear_model.ridge.gs->GridSearchCV(Ridge(fit_intercept=self.fit_intercept, normalize=self.normalize), parameters, cv=self.cv, scoring=self.scoring)
sklearn.linear_model.Ridge(self,alpha=1.0,fit_intercept=True,normalize=False,copy_X=True,max_iter=None,tol=0.001,solver='auto',random_state=None)
sklearn.linear_model.Ridge.fit(self,X,y,sample_weight=None)
sklearn.linear_model.RidgeCV(_BaseRidgeCV,RegressorMixin)
sklearn.linear_model.RidgeClassifier(self,alpha=1.0,fit_intercept=True,normalize=False,copy_X=True,max_iter=None,tol=0.001,class_weight=None,solver='auto',random_state=None)
sklearn.linear_model.RidgeClassifier.classes_(self)
sklearn.linear_model.RidgeClassifier.fit(self,X,y,sample_weight=None)
sklearn.linear_model.RidgeClassifierCV(self,alphas=(0.1,1.0,10.0),fit_intercept=True,normalize=False,scoring=None,cv=None,class_weight=None)
sklearn.linear_model.RidgeClassifierCV.classes_(self)
sklearn.linear_model.RidgeClassifierCV.fit(self,X,y,sample_weight=None)
sklearn.linear_model.ridge.Ridge(self,alpha=1.0,fit_intercept=True,normalize=False,copy_X=True,max_iter=None,tol=0.001,solver='auto',random_state=None)
sklearn.linear_model.ridge.Ridge.__init__(self,alpha=1.0,fit_intercept=True,normalize=False,copy_X=True,max_iter=None,tol=0.001,solver='auto',random_state=None)
sklearn.linear_model.ridge.Ridge.fit(self,X,y,sample_weight=None)
sklearn.linear_model.ridge.RidgeCV(_BaseRidgeCV,RegressorMixin)
sklearn.linear_model.ridge.RidgeClassifier(self,alpha=1.0,fit_intercept=True,normalize=False,copy_X=True,max_iter=None,tol=0.001,class_weight=None,solver='auto',random_state=None)
sklearn.linear_model.ridge.RidgeClassifier.__init__(self,alpha=1.0,fit_intercept=True,normalize=False,copy_X=True,max_iter=None,tol=0.001,class_weight=None,solver='auto',random_state=None)
sklearn.linear_model.ridge.RidgeClassifier.classes_(self)
sklearn.linear_model.ridge.RidgeClassifier.fit(self,X,y,sample_weight=None)
sklearn.linear_model.ridge.RidgeClassifierCV(self,alphas=(0.1,1.0,10.0),fit_intercept=True,normalize=False,scoring=None,cv=None,class_weight=None)
sklearn.linear_model.ridge.RidgeClassifierCV.__init__(self,alphas=(0.1,1.0,10.0),fit_intercept=True,normalize=False,scoring=None,cv=None,class_weight=None)
sklearn.linear_model.ridge.RidgeClassifierCV.classes_(self)
sklearn.linear_model.ridge.RidgeClassifierCV.fit(self,X,y,sample_weight=None)
sklearn.linear_model.ridge._BaseRidge(self,alpha=1.0,fit_intercept=True,normalize=False,copy_X=True,max_iter=None,tol=0.001,solver='auto',random_state=None)
sklearn.linear_model.ridge._BaseRidge.__init__(self,alpha=1.0,fit_intercept=True,normalize=False,copy_X=True,max_iter=None,tol=0.001,solver='auto',random_state=None)
sklearn.linear_model.ridge._BaseRidge.fit(self,X,y,sample_weight=None)
sklearn.linear_model.ridge._BaseRidgeCV(self,alphas=(0.1,1.0,10.0),fit_intercept=True,normalize=False,scoring=None,cv=None,gcv_mode=None,store_cv_values=False)
sklearn.linear_model.ridge._BaseRidgeCV.__init__(self,alphas=(0.1,1.0,10.0),fit_intercept=True,normalize=False,scoring=None,cv=None,gcv_mode=None,store_cv_values=False)
sklearn.linear_model.ridge._BaseRidgeCV.fit(self,X,y,sample_weight=None)
sklearn.linear_model.ridge._RidgeGCV(self,alphas=(0.1,1.0,10.0),fit_intercept=True,normalize=False,scoring=None,copy_X=True,gcv_mode=None,store_cv_values=False)
sklearn.linear_model.ridge._RidgeGCV.__init__(self,alphas=(0.1,1.0,10.0),fit_intercept=True,normalize=False,scoring=None,copy_X=True,gcv_mode=None,store_cv_values=False)
sklearn.linear_model.ridge._RidgeGCV._decomp_diag(self,v_prime,Q)
sklearn.linear_model.ridge._RidgeGCV._diag_dot(self,D,B)
sklearn.linear_model.ridge._RidgeGCV._errors(self,alpha,y,v,Q,QT_y)
sklearn.linear_model.ridge._RidgeGCV._errors_and_values_helper(self,alpha,y,v,Q,QT_y)
sklearn.linear_model.ridge._RidgeGCV._errors_and_values_svd_helper(self,alpha,y,v,U,UT_y)
sklearn.linear_model.ridge._RidgeGCV._errors_svd(self,alpha,y,v,U,UT_y)
sklearn.linear_model.ridge._RidgeGCV._pre_compute(self,X,y,centered_kernel=True)
sklearn.linear_model.ridge._RidgeGCV._pre_compute_svd(self,X,y,centered_kernel=True)
sklearn.linear_model.ridge._RidgeGCV._values(self,alpha,y,v,Q,QT_y)
sklearn.linear_model.ridge._RidgeGCV._values_svd(self,alpha,y,v,U,UT_y)
sklearn.linear_model.ridge._RidgeGCV.fit(self,X,y,sample_weight=None)
sklearn.linear_model.ridge._solve_cholesky(X,y,alpha)
sklearn.linear_model.ridge._solve_cholesky_kernel(K,y,alpha,sample_weight=None,copy=False)
sklearn.linear_model.ridge._solve_lsqr(X,y,alpha,max_iter=None,tol=0.001)
sklearn.linear_model.ridge._solve_sparse_cg(X,y,alpha,max_iter=None,tol=0.001,verbose=0)
sklearn.linear_model.ridge._solve_svd(X,y,alpha)
sklearn.linear_model.ridge.ridge_regression(X,y,alpha,sample_weight=None,solver='auto',max_iter=None,tol=0.001,verbose=0,random_state=None,return_n_iter=False,return_intercept=False)
sklearn.linear_model.ridge_regression(X,y,alpha,sample_weight=None,solver='auto',max_iter=None,tol=0.001,verbose=0,random_state=None,return_n_iter=False,return_intercept=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py----------------------------------------
A:sklearn.linear_model.stochastic_gradient.penalty->str(penalty).lower()
A:sklearn.linear_model.stochastic_gradient.sample_weight->self._validate_sample_weight(sample_weight, n_samples)
A:sklearn.linear_model.stochastic_gradient.coef_init->coef_init.ravel().ravel()
A:sklearn.linear_model.stochastic_gradient.self.coef_->est.standard_coef_.ravel().reshape(1, -1)
A:sklearn.linear_model.stochastic_gradient.intercept_init->numpy.asarray(intercept_init, dtype=np.float64)
A:sklearn.linear_model.stochastic_gradient.self.intercept_->numpy.atleast_1d(self.intercept_)
A:sklearn.linear_model.stochastic_gradient.self.average_coef_->numpy.zeros(n_features, dtype=np.float64, order='C')
A:sklearn.linear_model.stochastic_gradient.self.average_intercept_->numpy.atleast_1d(self.average_intercept_)
A:sklearn.linear_model.stochastic_gradient.y_i->numpy.ones(y.shape, dtype=np.float64, order='C')
A:sklearn.linear_model.stochastic_gradient.coef->est.standard_coef_.ravel()
A:sklearn.linear_model.stochastic_gradient.average_coef->est.average_coef_.ravel()
A:sklearn.linear_model.stochastic_gradient.(y_i, coef, intercept, average_coef, average_intercept)->_prepare_fit_binary(est, y, i)
A:sklearn.linear_model.stochastic_gradient.(dataset, intercept_decay)->make_dataset(X, y, sample_weight)
A:sklearn.linear_model.stochastic_gradient.penalty_type->self._get_penalty_type(self.penalty)
A:sklearn.linear_model.stochastic_gradient.learning_rate_type->self._get_learning_rate_type(learning_rate)
A:sklearn.linear_model.stochastic_gradient.random_state->check_random_state(self.random_state)
A:sklearn.linear_model.stochastic_gradient.seed->check_random_state(self.random_state).randint(0, np.iinfo(np.int32).max)
A:sklearn.linear_model.stochastic_gradient.(standard_coef, standard_intercept, average_coef, average_intercept, n_iter_)->average_sgd(coef, intercept, average_coef, average_intercept, est.loss_function_, penalty_type, alpha, C, est.l1_ratio, dataset, max_iter, tol, int(est.fit_intercept), int(est.verbose), int(est.shuffle), seed, pos_weight, neg_weight, learning_rate_type, est.eta0, est.power_t, est.t_, intercept_decay, est.average)
A:sklearn.linear_model.stochastic_gradient.self.n_jobs->int(n_jobs)
A:sklearn.linear_model.stochastic_gradient.(X, y)->check_X_y(X, y, 'csr', copy=False, order='C', dtype=np.float64)
A:sklearn.linear_model.stochastic_gradient.self._expanded_class_weight->compute_class_weight(self.class_weight, self.classes_, y)
A:sklearn.linear_model.stochastic_gradient.self.loss_function_->self._get_loss_function(loss)
A:sklearn.linear_model.stochastic_gradient.classes->numpy.unique(y)
A:sklearn.linear_model.stochastic_gradient.(coef, intercept, n_iter_)->fit_binary(self, 1, X, y, alpha, C, learning_rate, max_iter, self._expanded_class_weight[1], self._expanded_class_weight[0], sample_weight)
A:sklearn.linear_model.stochastic_gradient.self.standard_intercept_->numpy.atleast_1d(self.standard_intercept_)
A:sklearn.linear_model.stochastic_gradient.result->Parallel(n_jobs=self.n_jobs, backend='threading', verbose=self.verbose)((delayed(fit_binary)(self, i, X, y, alpha, C, learning_rate, max_iter, self._expanded_class_weight[i], 1.0, sample_weight) for i in range(len(self.classes_))))
A:sklearn.linear_model.stochastic_gradient.n_iter_->max(n_iter_, n_iter_i)
A:sklearn.linear_model.stochastic_gradient.scores->self.decision_function(X)
A:sklearn.linear_model.stochastic_gradient.prob2->numpy.ones((scores.shape[0], 2))
A:sklearn.linear_model.stochastic_gradient.prob_sum->prob.sum(axis=1)
A:sklearn.linear_model.stochastic_gradient.prob_sum[all_zero]->len(self.classes_)
A:sklearn.linear_model.stochastic_gradient.y->y.astype(np.float64, copy=False).astype(np.float64, copy=False)
A:sklearn.linear_model.stochastic_gradient.X->check_array(X, accept_sparse='csr')
A:sklearn.linear_model.stochastic_gradient.loss_function->self._get_loss_function(loss)
A:sklearn.linear_model.stochastic_gradient.(self.standard_coef_, self.standard_intercept_, self.average_coef_, self.average_intercept_, self.n_iter_)->average_sgd(self.standard_coef_, self.standard_intercept_[0], self.average_coef_, self.average_intercept_[0], loss_function, penalty_type, alpha, C, self.l1_ratio, dataset, max_iter, tol, int(self.fit_intercept), int(self.verbose), int(self.shuffle), seed, 1.0, 1.0, learning_rate_type, self.eta0, self.power_t, self.t_, intercept_decay, self.average)
A:sklearn.linear_model.stochastic_gradient.(self.coef_, self.intercept_, self.n_iter_)->plain_sgd(self.coef_, self.intercept_[0], loss_function, penalty_type, alpha, C, self.l1_ratio, dataset, max_iter, tol, int(self.fit_intercept), int(self.verbose), int(self.shuffle), seed, 1.0, 1.0, learning_rate_type, self.eta0, self.power_t, self.t_, intercept_decay)
sklearn.linear_model.SGDClassifier(self,loss='hinge',penalty='l2',alpha=0.0001,l1_ratio=0.15,fit_intercept=True,max_iter=None,tol=None,shuffle=True,verbose=0,epsilon=DEFAULT_EPSILON,n_jobs=1,random_state=None,learning_rate='optimal',eta0=0.0,power_t=0.5,class_weight=None,warm_start=False,average=False,n_iter=None)
sklearn.linear_model.SGDClassifier._check_proba(self)
sklearn.linear_model.SGDClassifier._predict_log_proba(self,X)
sklearn.linear_model.SGDClassifier._predict_proba(self,X)
sklearn.linear_model.SGDClassifier.predict_log_proba(self)
sklearn.linear_model.SGDClassifier.predict_proba(self)
sklearn.linear_model.SGDRegressor(self,loss='squared_loss',penalty='l2',alpha=0.0001,l1_ratio=0.15,fit_intercept=True,max_iter=None,tol=None,shuffle=True,verbose=0,epsilon=DEFAULT_EPSILON,random_state=None,learning_rate='invscaling',eta0=0.01,power_t=0.25,warm_start=False,average=False,n_iter=None)
sklearn.linear_model.stochastic_gradient.BaseSGD(self,loss,penalty='l2',alpha=0.0001,C=1.0,l1_ratio=0.15,fit_intercept=True,max_iter=None,tol=None,shuffle=True,verbose=0,epsilon=0.1,random_state=None,learning_rate='optimal',eta0=0.0,power_t=0.5,warm_start=False,average=False,n_iter=None)
sklearn.linear_model.stochastic_gradient.BaseSGD.__init__(self,loss,penalty='l2',alpha=0.0001,C=1.0,l1_ratio=0.15,fit_intercept=True,max_iter=None,tol=None,shuffle=True,verbose=0,epsilon=0.1,random_state=None,learning_rate='optimal',eta0=0.0,power_t=0.5,warm_start=False,average=False,n_iter=None)
sklearn.linear_model.stochastic_gradient.BaseSGD._allocate_parameter_mem(self,n_classes,n_features,coef_init=None,intercept_init=None)
sklearn.linear_model.stochastic_gradient.BaseSGD._get_learning_rate_type(self,learning_rate)
sklearn.linear_model.stochastic_gradient.BaseSGD._get_loss_function(self,loss)
sklearn.linear_model.stochastic_gradient.BaseSGD._get_penalty_type(self,penalty)
sklearn.linear_model.stochastic_gradient.BaseSGD._validate_params(self,set_max_iter=True)
sklearn.linear_model.stochastic_gradient.BaseSGD._validate_sample_weight(self,sample_weight,n_samples)
sklearn.linear_model.stochastic_gradient.BaseSGD.fit(self,X,y)
sklearn.linear_model.stochastic_gradient.BaseSGD.set_params(self,*args,**kwargs)
sklearn.linear_model.stochastic_gradient.BaseSGDClassifier(self,loss='hinge',penalty='l2',alpha=0.0001,l1_ratio=0.15,fit_intercept=True,max_iter=None,tol=None,shuffle=True,verbose=0,epsilon=DEFAULT_EPSILON,n_jobs=1,random_state=None,learning_rate='optimal',eta0=0.0,power_t=0.5,class_weight=None,warm_start=False,average=False,n_iter=None)
sklearn.linear_model.stochastic_gradient.BaseSGDClassifier.__init__(self,loss='hinge',penalty='l2',alpha=0.0001,l1_ratio=0.15,fit_intercept=True,max_iter=None,tol=None,shuffle=True,verbose=0,epsilon=DEFAULT_EPSILON,n_jobs=1,random_state=None,learning_rate='optimal',eta0=0.0,power_t=0.5,class_weight=None,warm_start=False,average=False,n_iter=None)
sklearn.linear_model.stochastic_gradient.BaseSGDClassifier._fit(self,X,y,alpha,C,loss,learning_rate,coef_init=None,intercept_init=None,sample_weight=None)
sklearn.linear_model.stochastic_gradient.BaseSGDClassifier._fit_binary(self,X,y,alpha,C,sample_weight,learning_rate,max_iter)
sklearn.linear_model.stochastic_gradient.BaseSGDClassifier._fit_multiclass(self,X,y,alpha,C,learning_rate,sample_weight,max_iter)
sklearn.linear_model.stochastic_gradient.BaseSGDClassifier._partial_fit(self,X,y,alpha,C,loss,learning_rate,max_iter,classes,sample_weight,coef_init,intercept_init)
sklearn.linear_model.stochastic_gradient.BaseSGDClassifier.fit(self,X,y,coef_init=None,intercept_init=None,sample_weight=None)
sklearn.linear_model.stochastic_gradient.BaseSGDClassifier.loss_function(self)
sklearn.linear_model.stochastic_gradient.BaseSGDClassifier.partial_fit(self,X,y,classes=None,sample_weight=None)
sklearn.linear_model.stochastic_gradient.BaseSGDRegressor(self,loss='squared_loss',penalty='l2',alpha=0.0001,l1_ratio=0.15,fit_intercept=True,max_iter=None,tol=None,shuffle=True,verbose=0,epsilon=DEFAULT_EPSILON,random_state=None,learning_rate='invscaling',eta0=0.01,power_t=0.25,warm_start=False,average=False,n_iter=None)
sklearn.linear_model.stochastic_gradient.BaseSGDRegressor.__init__(self,loss='squared_loss',penalty='l2',alpha=0.0001,l1_ratio=0.15,fit_intercept=True,max_iter=None,tol=None,shuffle=True,verbose=0,epsilon=DEFAULT_EPSILON,random_state=None,learning_rate='invscaling',eta0=0.01,power_t=0.25,warm_start=False,average=False,n_iter=None)
sklearn.linear_model.stochastic_gradient.BaseSGDRegressor._decision_function(self,X)
sklearn.linear_model.stochastic_gradient.BaseSGDRegressor._fit(self,X,y,alpha,C,loss,learning_rate,coef_init=None,intercept_init=None,sample_weight=None)
sklearn.linear_model.stochastic_gradient.BaseSGDRegressor._fit_regressor(self,X,y,alpha,C,loss,learning_rate,sample_weight,max_iter)
sklearn.linear_model.stochastic_gradient.BaseSGDRegressor._partial_fit(self,X,y,alpha,C,loss,learning_rate,max_iter,sample_weight,coef_init,intercept_init)
sklearn.linear_model.stochastic_gradient.BaseSGDRegressor.fit(self,X,y,coef_init=None,intercept_init=None,sample_weight=None)
sklearn.linear_model.stochastic_gradient.BaseSGDRegressor.partial_fit(self,X,y,sample_weight=None)
sklearn.linear_model.stochastic_gradient.BaseSGDRegressor.predict(self,X)
sklearn.linear_model.stochastic_gradient.SGDClassifier(self,loss='hinge',penalty='l2',alpha=0.0001,l1_ratio=0.15,fit_intercept=True,max_iter=None,tol=None,shuffle=True,verbose=0,epsilon=DEFAULT_EPSILON,n_jobs=1,random_state=None,learning_rate='optimal',eta0=0.0,power_t=0.5,class_weight=None,warm_start=False,average=False,n_iter=None)
sklearn.linear_model.stochastic_gradient.SGDClassifier.__init__(self,loss='hinge',penalty='l2',alpha=0.0001,l1_ratio=0.15,fit_intercept=True,max_iter=None,tol=None,shuffle=True,verbose=0,epsilon=DEFAULT_EPSILON,n_jobs=1,random_state=None,learning_rate='optimal',eta0=0.0,power_t=0.5,class_weight=None,warm_start=False,average=False,n_iter=None)
sklearn.linear_model.stochastic_gradient.SGDClassifier._check_proba(self)
sklearn.linear_model.stochastic_gradient.SGDClassifier._predict_log_proba(self,X)
sklearn.linear_model.stochastic_gradient.SGDClassifier._predict_proba(self,X)
sklearn.linear_model.stochastic_gradient.SGDClassifier.predict_log_proba(self)
sklearn.linear_model.stochastic_gradient.SGDClassifier.predict_proba(self)
sklearn.linear_model.stochastic_gradient.SGDRegressor(self,loss='squared_loss',penalty='l2',alpha=0.0001,l1_ratio=0.15,fit_intercept=True,max_iter=None,tol=None,shuffle=True,verbose=0,epsilon=DEFAULT_EPSILON,random_state=None,learning_rate='invscaling',eta0=0.01,power_t=0.25,warm_start=False,average=False,n_iter=None)
sklearn.linear_model.stochastic_gradient.SGDRegressor.__init__(self,loss='squared_loss',penalty='l2',alpha=0.0001,l1_ratio=0.15,fit_intercept=True,max_iter=None,tol=None,shuffle=True,verbose=0,epsilon=DEFAULT_EPSILON,random_state=None,learning_rate='invscaling',eta0=0.01,power_t=0.25,warm_start=False,average=False,n_iter=None)
sklearn.linear_model.stochastic_gradient._prepare_fit_binary(est,y,i)
sklearn.linear_model.stochastic_gradient.fit_binary(est,i,X,y,alpha,C,learning_rate,max_iter,pos_weight,neg_weight,sample_weight)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/linear_model/setup.py----------------------------------------
A:sklearn.linear_model.setup.config->Configuration('linear_model', parent_package, top_path)
A:sklearn.linear_model.setup.(cblas_libs, blas_info)->get_blas_info()
sklearn.linear_model.setup.configuration(parent_package='',top_path=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/linear_model/passive_aggressive.py----------------------------------------
sklearn.linear_model.PassiveAggressiveClassifier(self,C=1.0,fit_intercept=True,max_iter=None,tol=None,shuffle=True,verbose=0,loss='hinge',n_jobs=1,random_state=None,warm_start=False,class_weight=None,average=False,n_iter=None)
sklearn.linear_model.PassiveAggressiveClassifier.fit(self,X,y,coef_init=None,intercept_init=None)
sklearn.linear_model.PassiveAggressiveClassifier.partial_fit(self,X,y,classes=None)
sklearn.linear_model.PassiveAggressiveRegressor(self,C=1.0,fit_intercept=True,max_iter=None,tol=None,shuffle=True,verbose=0,loss='epsilon_insensitive',epsilon=DEFAULT_EPSILON,random_state=None,warm_start=False,average=False,n_iter=None)
sklearn.linear_model.PassiveAggressiveRegressor.fit(self,X,y,coef_init=None,intercept_init=None)
sklearn.linear_model.PassiveAggressiveRegressor.partial_fit(self,X,y)
sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier(self,C=1.0,fit_intercept=True,max_iter=None,tol=None,shuffle=True,verbose=0,loss='hinge',n_jobs=1,random_state=None,warm_start=False,class_weight=None,average=False,n_iter=None)
sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier.__init__(self,C=1.0,fit_intercept=True,max_iter=None,tol=None,shuffle=True,verbose=0,loss='hinge',n_jobs=1,random_state=None,warm_start=False,class_weight=None,average=False,n_iter=None)
sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier.fit(self,X,y,coef_init=None,intercept_init=None)
sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier.partial_fit(self,X,y,classes=None)
sklearn.linear_model.passive_aggressive.PassiveAggressiveRegressor(self,C=1.0,fit_intercept=True,max_iter=None,tol=None,shuffle=True,verbose=0,loss='epsilon_insensitive',epsilon=DEFAULT_EPSILON,random_state=None,warm_start=False,average=False,n_iter=None)
sklearn.linear_model.passive_aggressive.PassiveAggressiveRegressor.__init__(self,C=1.0,fit_intercept=True,max_iter=None,tol=None,shuffle=True,verbose=0,loss='epsilon_insensitive',epsilon=DEFAULT_EPSILON,random_state=None,warm_start=False,average=False,n_iter=None)
sklearn.linear_model.passive_aggressive.PassiveAggressiveRegressor.fit(self,X,y,coef_init=None,intercept_init=None)
sklearn.linear_model.passive_aggressive.PassiveAggressiveRegressor.partial_fit(self,X,y)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py----------------------------------------
A:sklearn.linear_model.least_angle.max_features->min(max_iter, n_features)
A:sklearn.linear_model.least_angle.coefs->numpy.resize(coefs, (n_iter + add_features, n_features))
A:sklearn.linear_model.least_angle.alphas->numpy.resize(alphas, n_iter + add_features)
A:sklearn.linear_model.least_angle.sign_active->numpy.append(sign_active, 0.0)
A:sklearn.linear_model.least_angle.L->numpy.zeros((max_features, max_features), dtype=X.dtype)
A:sklearn.linear_model.least_angle.(swap, nrm2)->scipy.linalg.get_blas_funcs(('swap', 'nrm2'), (X,))
A:sklearn.linear_model.least_angle.(solve_cholesky,)->get_lapack_funcs(('potrs',), (X,))
A:sklearn.linear_model.least_angle.X->as_float_array(X, copy=self.copy_X)
A:sklearn.linear_model.least_angle.Gram->self._get_gram(self.precompute, X, y)
A:sklearn.linear_model.least_angle.Cov->Xy.copy()
A:sklearn.linear_model.least_angle.C_idx->numpy.argmax(np.abs(Cov))
A:sklearn.linear_model.least_angle.C->numpy.fabs(C_)
A:sklearn.linear_model.least_angle.sign_active[n_active]->numpy.sign(C_)
A:sklearn.linear_model.least_angle.(Cov[C_idx], Cov[0])->swap(Cov[C_idx], Cov[0])
A:sklearn.linear_model.least_angle.(X.T[n], X.T[m])->swap(X.T[n], X.T[m])
A:sklearn.linear_model.least_angle.L[n_active, :n_active]->numpy.dot(X.T[n_active], X.T[:n_active].T)
A:sklearn.linear_model.least_angle.(Gram[m], Gram[n])->swap(Gram[m], Gram[n])
A:sklearn.linear_model.least_angle.(Gram[:, m], Gram[:, n])->swap(Gram[:, m], Gram[:, n])
A:sklearn.linear_model.least_angle.v->numpy.dot(L[n_active, :n_active], L[n_active, :n_active])
A:sklearn.linear_model.least_angle.diag->max(np.sqrt(np.abs(c - v)), eps)
A:sklearn.linear_model.least_angle.(least_squares, info)->solve_cholesky(L_, sign_active[:n_active], lower=True)
A:sklearn.linear_model.least_angle.L_->L[:n_active, :n_active].copy()
A:sklearn.linear_model.least_angle.tmp->max(np.sum(least_squares * sign_active[:n_active]), eps)
A:sklearn.linear_model.least_angle.eq_dir->numpy.dot(X.T[:n_active].T, least_squares)
A:sklearn.linear_model.least_angle.corr_eq_dir->numpy.dot(Gram[:n_active, n_active:].T, least_squares)
A:sklearn.linear_model.least_angle.g1->utils.arrayfuncs.min_pos((C - Cov) / (AA - corr_eq_dir + tiny32))
A:sklearn.linear_model.least_angle.gamma_->min(g1, g2, C / AA)
A:sklearn.linear_model.least_angle.g2->utils.arrayfuncs.min_pos((C + Cov) / (AA + corr_eq_dir + tiny32))
A:sklearn.linear_model.least_angle.z_pos->utils.arrayfuncs.min_pos(z)
A:sklearn.linear_model.least_angle.coef->numpy.zeros_like(coef)
A:sklearn.linear_model.least_angle.(X.T[i], X.T[i + 1])->swap(X.T[i], X.T[i + 1])
A:sklearn.linear_model.least_angle.temp->numpy.dot(X.T[drop_idx], residual)
A:sklearn.linear_model.least_angle.(Gram[i], Gram[i + 1])->swap(Gram[i], Gram[i + 1])
A:sklearn.linear_model.least_angle.(Gram[:, i], Gram[:, i + 1])->swap(Gram[:, i], Gram[:, i + 1])
A:sklearn.linear_model.least_angle.precompute->numpy.dot(X.T, X)
A:sklearn.linear_model.least_angle.(X, y, X_offset, y_offset, X_scale)->self._preprocess_data(X, y, self.fit_intercept, self.normalize, self.copy_X)
A:sklearn.linear_model.least_angle.self.coef_->numpy.empty((n_targets, n_features))
A:sklearn.linear_model.least_angle.(alphas, active, coef_path, n_iter_)->lars_path(X, y[:, k], Gram=Gram, Xy=this_Xy, copy_X=self.copy_X, copy_Gram=True, alpha_min=alpha, method=self.method, verbose=max(0, self.verbose - 1), max_iter=max_iter, eps=self.eps, return_path=True, return_n_iter=True, positive=self.positive)
A:sklearn.linear_model.least_angle.(alphas, _, self.coef_[k], n_iter_)->lars_path(X, y[:, k], Gram=Gram, Xy=this_Xy, copy_X=self.copy_X, copy_Gram=True, alpha_min=alpha, method=self.method, verbose=max(0, self.verbose - 1), max_iter=max_iter, eps=self.eps, return_path=False, return_n_iter=True, positive=self.positive)
A:sklearn.linear_model.least_angle.(X, y)->check_X_y(X, y, y_numeric=True)
A:sklearn.linear_model.least_angle.alpha->getattr(self, 'alpha', 0.0)
A:sklearn.linear_model.least_angle.X_train->_check_copy_and_writeable(X_train, copy)
A:sklearn.linear_model.least_angle.y_train->as_float_array(y_train, copy=False)
A:sklearn.linear_model.least_angle.X_test->_check_copy_and_writeable(X_test, copy)
A:sklearn.linear_model.least_angle.y_test->as_float_array(y_test, copy=False)
A:sklearn.linear_model.least_angle.X_mean->_check_copy_and_writeable(X_train, copy).mean(axis=0)
A:sklearn.linear_model.least_angle.y_mean->as_float_array(y_train, copy=False).mean(axis=0)
A:sklearn.linear_model.least_angle.norms->numpy.sqrt(np.sum(X_train ** 2, axis=0))
A:sklearn.linear_model.least_angle.nonzeros->numpy.flatnonzero(norms)
A:sklearn.linear_model.least_angle.(alphas, active, coefs)->lars_path(X_train, y_train, Gram=Gram, copy_X=False, copy_Gram=False, method=method, verbose=max(0, verbose - 1), max_iter=max_iter, eps=eps, positive=positive)
A:sklearn.linear_model.least_angle.y->as_float_array(y, copy=self.copy_X)
A:sklearn.linear_model.least_angle.cv->check_cv(self.cv, classifier=False)
A:sklearn.linear_model.least_angle.cv_paths->Parallel(n_jobs=self.n_jobs, verbose=self.verbose)((delayed(_lars_path_residues)(X[train], y[train], X[test], y[test], Gram=Gram, copy=False, method=self.method, verbose=max(0, self.verbose - 1), normalize=self.normalize, fit_intercept=self.fit_intercept, max_iter=self.max_iter, eps=self.eps, positive=self.positive) for (train, test) in cv.split(X, y)))
A:sklearn.linear_model.least_angle.all_alphas->numpy.unique(all_alphas)
A:sklearn.linear_model.least_angle.stride->int(max(1, int(len(all_alphas) / float(self.max_n_alphas))))
A:sklearn.linear_model.least_angle.mse_path->numpy.empty((len(all_alphas), len(cv_paths)))
A:sklearn.linear_model.least_angle.this_residues->scipy.interpolate.interp1d(alphas, residues, axis=0)(all_alphas)
A:sklearn.linear_model.least_angle.mse_path[:, index]->numpy.mean(this_residues, axis=-1)
A:sklearn.linear_model.least_angle.mask->numpy.all(np.isfinite(mse_path), axis=-1)
A:sklearn.linear_model.least_angle.i_best_alpha->numpy.argmin(mse_path.mean(axis=-1))
A:sklearn.linear_model.least_angle.(X, y, Xmean, ymean, Xstd)->base.LinearModel._preprocess_data(X, y, self.fit_intercept, self.normalize, self.copy_X)
A:sklearn.linear_model.least_angle.(alphas_, active_, coef_path_, self.n_iter_)->lars_path(X, y, Gram=Gram, copy_X=copy_X, copy_Gram=True, alpha_min=0.0, method='lasso', verbose=self.verbose, max_iter=max_iter, eps=self.eps, return_n_iter=True, positive=self.positive)
A:sklearn.linear_model.least_angle.K->log(n_samples)
A:sklearn.linear_model.least_angle.mean_squared_error->numpy.mean(R ** 2, axis=0)
A:sklearn.linear_model.least_angle.sigma2->numpy.var(y)
A:sklearn.linear_model.least_angle.df->numpy.zeros(coef_path_.shape[1], dtype=np.int)
A:sklearn.linear_model.least_angle.df[k]->numpy.sum(mask)
A:sklearn.linear_model.least_angle.n_best->numpy.argmin(self.criterion_)
sklearn.linear_model.Lars(self,fit_intercept=True,verbose=False,normalize=True,precompute='auto',n_nonzero_coefs=500,eps=np.finfo(np.float).eps,copy_X=True,fit_path=True,positive=False)
sklearn.linear_model.Lars._fit(self,X,y,max_iter,alpha,fit_path,Xy=None)
sklearn.linear_model.Lars._get_gram(self,precompute,X,y)
sklearn.linear_model.Lars.fit(self,X,y,Xy=None)
sklearn.linear_model.LarsCV(self,fit_intercept=True,verbose=False,max_iter=500,normalize=True,precompute='auto',cv=None,max_n_alphas=1000,n_jobs=1,eps=np.finfo(np.float).eps,copy_X=True,positive=False)
sklearn.linear_model.LarsCV.alpha(self)
sklearn.linear_model.LarsCV.cv_mse_path_(self)
sklearn.linear_model.LarsCV.fit(self,X,y)
sklearn.linear_model.LassoLars(self,alpha=1.0,fit_intercept=True,verbose=False,normalize=True,precompute='auto',max_iter=500,eps=np.finfo(np.float).eps,copy_X=True,fit_path=True,positive=False)
sklearn.linear_model.LassoLarsCV(self,fit_intercept=True,verbose=False,max_iter=500,normalize=True,precompute='auto',cv=None,max_n_alphas=1000,n_jobs=1,eps=np.finfo(np.float).eps,copy_X=True,positive=False)
sklearn.linear_model.LassoLarsIC(self,criterion='aic',fit_intercept=True,verbose=False,normalize=True,precompute='auto',max_iter=500,eps=np.finfo(np.float).eps,copy_X=True,positive=False)
sklearn.linear_model.LassoLarsIC.fit(self,X,y,copy_X=True)
sklearn.linear_model.lars_path(X,y,Xy=None,Gram=None,max_iter=500,alpha_min=0,method='lar',copy_X=True,eps=np.finfo(np.float).eps,copy_Gram=True,verbose=0,return_path=True,return_n_iter=False,positive=False)
sklearn.linear_model.least_angle.Lars(self,fit_intercept=True,verbose=False,normalize=True,precompute='auto',n_nonzero_coefs=500,eps=np.finfo(np.float).eps,copy_X=True,fit_path=True,positive=False)
sklearn.linear_model.least_angle.Lars.__init__(self,fit_intercept=True,verbose=False,normalize=True,precompute='auto',n_nonzero_coefs=500,eps=np.finfo(np.float).eps,copy_X=True,fit_path=True,positive=False)
sklearn.linear_model.least_angle.Lars._fit(self,X,y,max_iter,alpha,fit_path,Xy=None)
sklearn.linear_model.least_angle.Lars._get_gram(self,precompute,X,y)
sklearn.linear_model.least_angle.Lars.fit(self,X,y,Xy=None)
sklearn.linear_model.least_angle.LarsCV(self,fit_intercept=True,verbose=False,max_iter=500,normalize=True,precompute='auto',cv=None,max_n_alphas=1000,n_jobs=1,eps=np.finfo(np.float).eps,copy_X=True,positive=False)
sklearn.linear_model.least_angle.LarsCV.__init__(self,fit_intercept=True,verbose=False,max_iter=500,normalize=True,precompute='auto',cv=None,max_n_alphas=1000,n_jobs=1,eps=np.finfo(np.float).eps,copy_X=True,positive=False)
sklearn.linear_model.least_angle.LarsCV.alpha(self)
sklearn.linear_model.least_angle.LarsCV.cv_mse_path_(self)
sklearn.linear_model.least_angle.LarsCV.fit(self,X,y)
sklearn.linear_model.least_angle.LassoLars(self,alpha=1.0,fit_intercept=True,verbose=False,normalize=True,precompute='auto',max_iter=500,eps=np.finfo(np.float).eps,copy_X=True,fit_path=True,positive=False)
sklearn.linear_model.least_angle.LassoLars.__init__(self,alpha=1.0,fit_intercept=True,verbose=False,normalize=True,precompute='auto',max_iter=500,eps=np.finfo(np.float).eps,copy_X=True,fit_path=True,positive=False)
sklearn.linear_model.least_angle.LassoLarsCV(self,fit_intercept=True,verbose=False,max_iter=500,normalize=True,precompute='auto',cv=None,max_n_alphas=1000,n_jobs=1,eps=np.finfo(np.float).eps,copy_X=True,positive=False)
sklearn.linear_model.least_angle.LassoLarsCV.__init__(self,fit_intercept=True,verbose=False,max_iter=500,normalize=True,precompute='auto',cv=None,max_n_alphas=1000,n_jobs=1,eps=np.finfo(np.float).eps,copy_X=True,positive=False)
sklearn.linear_model.least_angle.LassoLarsIC(self,criterion='aic',fit_intercept=True,verbose=False,normalize=True,precompute='auto',max_iter=500,eps=np.finfo(np.float).eps,copy_X=True,positive=False)
sklearn.linear_model.least_angle.LassoLarsIC.__init__(self,criterion='aic',fit_intercept=True,verbose=False,normalize=True,precompute='auto',max_iter=500,eps=np.finfo(np.float).eps,copy_X=True,positive=False)
sklearn.linear_model.least_angle.LassoLarsIC.fit(self,X,y,copy_X=True)
sklearn.linear_model.least_angle._check_copy_and_writeable(array,copy=False)
sklearn.linear_model.least_angle._lars_path_residues(X_train,y_train,X_test,y_test,Gram=None,copy=True,method='lars',verbose=False,fit_intercept=True,normalize=True,max_iter=500,eps=np.finfo(np.float).eps,positive=False)
sklearn.linear_model.least_angle.lars_path(X,y,Xy=None,Gram=None,max_iter=500,alpha_min=0,method='lar',copy_X=True,eps=np.finfo(np.float).eps,copy_Gram=True,verbose=0,return_path=True,return_n_iter=False,positive=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/linear_model/base.py----------------------------------------
A:sklearn.linear_model.base.rng->check_random_state(random_state)
A:sklearn.linear_model.base.seed->check_random_state(random_state).randint(1, np.iinfo(np.int32).max)
A:sklearn.linear_model.base.dataset->ArrayDataset(X, y, sample_weight, seed=seed)
A:sklearn.linear_model.base.X->check_array(X, accept_sparse='csr')
A:sklearn.linear_model.base.(X_offset, X_var)->mean_variance_axis(X, axis=0)
A:sklearn.linear_model.base.X_std->numpy.ones(X.shape[1])
A:sklearn.linear_model.base.y_offset->numpy.zeros(y.shape[1], dtype=X.dtype)
A:sklearn.linear_model.base.X_offset->numpy.zeros(X.shape[1], dtype=X.dtype)
A:sklearn.linear_model.base.y->safe_sparse_dot(sw_matrix, y)
A:sklearn.linear_model.base.X_offset[:]->check_array(X, accept_sparse='csr').dtype.type(0)
A:sklearn.linear_model.base.X_scale->numpy.ones(X.shape[1], dtype=X.dtype)
A:sklearn.linear_model.base.(X, X_scale)->f_normalize(X, axis=0, copy=False, return_norm=True)
A:sklearn.linear_model.base.sample_weight->numpy.sqrt(sample_weight)
A:sklearn.linear_model.base.sw_matrix->scipy.sparse.dia_matrix((sample_weight, 0), shape=(n_samples, n_samples))
A:sklearn.linear_model.base._preprocess_data->staticmethod(_preprocess_data)
A:sklearn.linear_model.base.scores->self.decision_function(X)
A:sklearn.linear_model.base.indices->self.decision_function(X).argmax(axis=1)
A:sklearn.linear_model.base.prob->self.decision_function(X)
A:sklearn.linear_model.base.self.coef_->numpy.ravel(self.coef_)
A:sklearn.linear_model.base.(X, y)->_rescale_data(X, y, sample_weight)
A:sklearn.linear_model.base.(X, y, X_offset, y_offset, X_scale)->_preprocess_data(X, y, fit_intercept=fit_intercept, normalize=normalize, copy=copy)
A:sklearn.linear_model.base.out->sparse_lsqr(X, y)
A:sklearn.linear_model.base.outs->Parallel(n_jobs=n_jobs_)((delayed(sparse_lsqr)(X, y[:, j].ravel()) for j in range(y.shape[1])))
A:sklearn.linear_model.base.self._residues->numpy.vstack((out[3] for out in outs))
A:sklearn.linear_model.base.(self.coef_, self._residues, self.rank_, self.singular_)->scipy.linalg.lstsq(X, y)
A:sklearn.linear_model.base.precompute->numpy.empty(shape=(n_features, n_features), dtype=X.dtype, order='C')
A:sklearn.linear_model.base.common_dtype->numpy.find_common_type([X.dtype, y.dtype], [])
A:sklearn.linear_model.base.Xy->numpy.empty(shape=(n_features, n_targets), dtype=common_dtype, order='F')
sklearn.linear_model.LinearRegression(self,fit_intercept=True,normalize=False,copy_X=True,n_jobs=1)
sklearn.linear_model.LinearRegression.fit(self,X,y,sample_weight=None)
sklearn.linear_model.base.LinearClassifierMixin(ClassifierMixin)
sklearn.linear_model.base.LinearClassifierMixin._predict_proba_lr(self,X)
sklearn.linear_model.base.LinearClassifierMixin.decision_function(self,X)
sklearn.linear_model.base.LinearClassifierMixin.predict(self,X)
sklearn.linear_model.base.LinearModel(six.with_metaclass(ABCMeta,BaseEstimator))
sklearn.linear_model.base.LinearModel._decision_function(self,X)
sklearn.linear_model.base.LinearModel._set_intercept(self,X_offset,y_offset,X_scale)
sklearn.linear_model.base.LinearModel.fit(self,X,y)
sklearn.linear_model.base.LinearModel.predict(self,X)
sklearn.linear_model.base.LinearRegression(self,fit_intercept=True,normalize=False,copy_X=True,n_jobs=1)
sklearn.linear_model.base.LinearRegression.__init__(self,fit_intercept=True,normalize=False,copy_X=True,n_jobs=1)
sklearn.linear_model.base.LinearRegression.fit(self,X,y,sample_weight=None)
sklearn.linear_model.base.SparseCoefMixin(object)
sklearn.linear_model.base.SparseCoefMixin.densify(self)
sklearn.linear_model.base.SparseCoefMixin.sparsify(self)
sklearn.linear_model.base._pre_fit(X,y,Xy,precompute,normalize,fit_intercept,copy)
sklearn.linear_model.base._preprocess_data(X,y,fit_intercept,normalize=False,copy=True,sample_weight=None,return_mean=False)
sklearn.linear_model.base._rescale_data(X,y,sample_weight)
sklearn.linear_model.base.center_data(X,y,fit_intercept,normalize=False,copy=True,sample_weight=None)
sklearn.linear_model.base.make_dataset(X,y,sample_weight,random_state=None)
sklearn.linear_model.base.sparse_center_data(X,y,fit_intercept,normalize=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py----------------------------------------
A:sklearn.linear_model.coordinate_descent.n_samples->len(y)
A:sklearn.linear_model.coordinate_descent.X_sparse->scipy.sparse.isspmatrix(X)
A:sklearn.linear_model.coordinate_descent.X->check_array(X, dtype=[np.float64, np.float32], order='F', copy=self.copy_X and self.fit_intercept)
A:sklearn.linear_model.coordinate_descent.(X, y, _, _, _)->_preprocess_data(X, y, fit_intercept, normalize, copy=False)
A:sklearn.linear_model.coordinate_descent.Xy->check_array(Xy, dtype=X.dtype.type, order='C', copy=False, ensure_2d=False)
A:sklearn.linear_model.coordinate_descent.(_, _, X_offset, _, X_scale)->_preprocess_data(X, y, fit_intercept, normalize, return_mean=True)
A:sklearn.linear_model.coordinate_descent.alphas->numpy.tile(np.sort(alphas)[::-1], (n_l1_ratio, 1))
A:sklearn.linear_model.coordinate_descent.y->check_array(y, dtype=X.dtype.type, ensure_2d=False)
A:sklearn.linear_model.coordinate_descent.X_sparse_scaling->numpy.zeros(n_features, dtype=X.dtype)
A:sklearn.linear_model.coordinate_descent.(X, y, X_offset, y_offset, X_scale, precompute, Xy)->_pre_fit(X, y, None, self.precompute, self.normalize, self.fit_intercept, copy=False)
A:sklearn.linear_model.coordinate_descent.n_alphas->len(alphas[0])
A:sklearn.linear_model.coordinate_descent.tol->params.get('tol', 0.0001)
A:sklearn.linear_model.coordinate_descent.max_iter->params.get('max_iter', 1000)
A:sklearn.linear_model.coordinate_descent.dual_gaps->numpy.empty(n_alphas)
A:sklearn.linear_model.coordinate_descent.rng->check_random_state(params.get('random_state', None))
A:sklearn.linear_model.coordinate_descent.selection->params.get('selection', 'cyclic')
A:sklearn.linear_model.coordinate_descent.coefs->numpy.empty((n_outputs, n_features, n_alphas), dtype=X.dtype)
A:sklearn.linear_model.coordinate_descent.coef_->numpy.zeros((n_targets, n_features), dtype=X.dtype, order='F')
A:sklearn.linear_model.coordinate_descent.model->MultiTaskLasso()
A:sklearn.linear_model.coordinate_descent.precompute->check_array(precompute, dtype=X.dtype.type, order='C')
A:sklearn.linear_model.coordinate_descent.path->staticmethod(lasso_path)
A:sklearn.linear_model.coordinate_descent.(X, y)->check_X_y(X, y, accept_sparse='csc', order='F', dtype=[np.float64, np.float32], copy=self.copy_X and self.fit_intercept, multi_output=True, y_numeric=True)
A:sklearn.linear_model.coordinate_descent.dual_gaps_->numpy.zeros(n_targets, dtype=X.dtype)
A:sklearn.linear_model.coordinate_descent.(_, this_coef, this_dual_gap, this_iter)->self.path(X, y[:, k], l1_ratio=self.l1_ratio, eps=None, n_alphas=None, alphas=[self.alpha], precompute=precompute, Xy=this_Xy, fit_intercept=False, normalize=False, copy_X=True, verbose=False, tol=self.tol, positive=self.positive, X_offset=X_offset, X_scale=X_scale, return_n_iter=True, coef_init=coef_[k], max_iter=self.max_iter, random_state=self.random_state, selection=self.selection, check_input=False)
A:sklearn.linear_model.coordinate_descent.(self.coef_, self.dual_gap_)->map(np.squeeze, [coef_, dual_gaps_])
A:sklearn.linear_model.coordinate_descent.self.coef_->numpy.asfortranarray(self.coef_)
A:sklearn.linear_model.coordinate_descent.(X_train, y_train, X_offset, y_offset, X_scale, precompute, Xy)->_pre_fit(X_train, y_train, None, precompute, normalize, fit_intercept, copy=False)
A:sklearn.linear_model.coordinate_descent.path_params->self.get_params()
A:sklearn.linear_model.coordinate_descent.X_train->check_array(X_train, 'csc', dtype=dtype, order=X_order)
A:sklearn.linear_model.coordinate_descent.(alphas, coefs, _)->path(X_train, y_train, **path_params)
A:sklearn.linear_model.coordinate_descent.y_offset->numpy.atleast_1d(y_offset)
A:sklearn.linear_model.coordinate_descent.nonzeros->numpy.flatnonzero(X_scale)
A:sklearn.linear_model.coordinate_descent.coefs_feature_major->numpy.rollaxis(coefs, 1)
A:sklearn.linear_model.coordinate_descent.feature_2d->numpy.reshape(coefs_feature_major, (n_features, -1))
A:sklearn.linear_model.coordinate_descent.X_test_coefs->safe_sparse_dot(X_test, coefs)
A:sklearn.linear_model.coordinate_descent.this_mses->(residues ** 2).mean(axis=0).mean(axis=0)
A:sklearn.linear_model.coordinate_descent.l1_ratios->numpy.atleast_1d(path_params['l1_ratio'])
A:sklearn.linear_model.coordinate_descent.n_l1_ratio->len(l1_ratios)
A:sklearn.linear_model.coordinate_descent.cv->check_cv(self.cv)
A:sklearn.linear_model.coordinate_descent.folds->list(cv.split(X, y))
A:sklearn.linear_model.coordinate_descent.mse_paths->numpy.reshape(mse_paths, (n_l1_ratio, len(folds), -1))
A:sklearn.linear_model.coordinate_descent.mean_mse->numpy.mean(mse_paths, axis=1)
A:sklearn.linear_model.coordinate_descent.self.mse_path_->numpy.squeeze(np.rollaxis(mse_paths, 2, 1))
A:sklearn.linear_model.coordinate_descent.i_best_alpha->numpy.argmin(mse_alphas)
A:sklearn.linear_model.coordinate_descent.self.alphas_->numpy.asarray(alphas[0])
A:sklearn.linear_model.coordinate_descent.common_params->dict(((name, value) for (name, value) in self.get_params().items() if name in model.get_params()))
A:sklearn.linear_model.coordinate_descent.(X, y, X_offset, y_offset, X_scale)->_preprocess_data(X, y, self.fit_intercept, self.normalize, copy=False)
A:sklearn.linear_model.coordinate_descent.(self.coef_, self.dual_gap_, self.eps_, self.n_iter_)->cd_fast.enet_coordinate_descent_multi_task(self.coef_, l1_reg, l2_reg, X, y, self.max_iter, self.tol, check_random_state(self.random_state), random)
sklearn.linear_model.ElasticNet(self,alpha=1.0,l1_ratio=0.5,fit_intercept=True,normalize=False,precompute=False,max_iter=1000,copy_X=True,tol=0.0001,warm_start=False,positive=False,random_state=None,selection='cyclic')
sklearn.linear_model.ElasticNet._decision_function(self,X)
sklearn.linear_model.ElasticNet.fit(self,X,y,check_input=True)
sklearn.linear_model.ElasticNet.sparse_coef_(self)
sklearn.linear_model.ElasticNetCV(self,l1_ratio=0.5,eps=0.001,n_alphas=100,alphas=None,fit_intercept=True,normalize=False,precompute='auto',max_iter=1000,tol=0.0001,cv=None,copy_X=True,verbose=0,n_jobs=1,positive=False,random_state=None,selection='cyclic')
sklearn.linear_model.Lasso(self,alpha=1.0,fit_intercept=True,normalize=False,precompute=False,copy_X=True,max_iter=1000,tol=0.0001,warm_start=False,positive=False,random_state=None,selection='cyclic')
sklearn.linear_model.LassoCV(self,eps=0.001,n_alphas=100,alphas=None,fit_intercept=True,normalize=False,precompute='auto',max_iter=1000,tol=0.0001,copy_X=True,cv=None,verbose=False,n_jobs=1,positive=False,random_state=None,selection='cyclic')
sklearn.linear_model.MultiTaskElasticNet(self,alpha=1.0,l1_ratio=0.5,fit_intercept=True,normalize=False,copy_X=True,max_iter=1000,tol=0.0001,warm_start=False,random_state=None,selection='cyclic')
sklearn.linear_model.MultiTaskElasticNet.fit(self,X,y)
sklearn.linear_model.MultiTaskElasticNetCV(self,l1_ratio=0.5,eps=0.001,n_alphas=100,alphas=None,fit_intercept=True,normalize=False,max_iter=1000,tol=0.0001,cv=None,copy_X=True,verbose=0,n_jobs=1,random_state=None,selection='cyclic')
sklearn.linear_model.MultiTaskLasso(self,alpha=1.0,fit_intercept=True,normalize=False,copy_X=True,max_iter=1000,tol=0.0001,warm_start=False,random_state=None,selection='cyclic')
sklearn.linear_model.MultiTaskLassoCV(self,eps=0.001,n_alphas=100,alphas=None,fit_intercept=True,normalize=False,max_iter=1000,tol=0.0001,copy_X=True,cv=None,verbose=False,n_jobs=1,random_state=None,selection='cyclic')
sklearn.linear_model.coordinate_descent.ElasticNet(self,alpha=1.0,l1_ratio=0.5,fit_intercept=True,normalize=False,precompute=False,max_iter=1000,copy_X=True,tol=0.0001,warm_start=False,positive=False,random_state=None,selection='cyclic')
sklearn.linear_model.coordinate_descent.ElasticNet.__init__(self,alpha=1.0,l1_ratio=0.5,fit_intercept=True,normalize=False,precompute=False,max_iter=1000,copy_X=True,tol=0.0001,warm_start=False,positive=False,random_state=None,selection='cyclic')
sklearn.linear_model.coordinate_descent.ElasticNet._decision_function(self,X)
sklearn.linear_model.coordinate_descent.ElasticNet.fit(self,X,y,check_input=True)
sklearn.linear_model.coordinate_descent.ElasticNet.sparse_coef_(self)
sklearn.linear_model.coordinate_descent.ElasticNetCV(self,l1_ratio=0.5,eps=0.001,n_alphas=100,alphas=None,fit_intercept=True,normalize=False,precompute='auto',max_iter=1000,tol=0.0001,cv=None,copy_X=True,verbose=0,n_jobs=1,positive=False,random_state=None,selection='cyclic')
sklearn.linear_model.coordinate_descent.ElasticNetCV.__init__(self,l1_ratio=0.5,eps=0.001,n_alphas=100,alphas=None,fit_intercept=True,normalize=False,precompute='auto',max_iter=1000,tol=0.0001,cv=None,copy_X=True,verbose=0,n_jobs=1,positive=False,random_state=None,selection='cyclic')
sklearn.linear_model.coordinate_descent.Lasso(self,alpha=1.0,fit_intercept=True,normalize=False,precompute=False,copy_X=True,max_iter=1000,tol=0.0001,warm_start=False,positive=False,random_state=None,selection='cyclic')
sklearn.linear_model.coordinate_descent.Lasso.__init__(self,alpha=1.0,fit_intercept=True,normalize=False,precompute=False,copy_X=True,max_iter=1000,tol=0.0001,warm_start=False,positive=False,random_state=None,selection='cyclic')
sklearn.linear_model.coordinate_descent.LassoCV(self,eps=0.001,n_alphas=100,alphas=None,fit_intercept=True,normalize=False,precompute='auto',max_iter=1000,tol=0.0001,copy_X=True,cv=None,verbose=False,n_jobs=1,positive=False,random_state=None,selection='cyclic')
sklearn.linear_model.coordinate_descent.LassoCV.__init__(self,eps=0.001,n_alphas=100,alphas=None,fit_intercept=True,normalize=False,precompute='auto',max_iter=1000,tol=0.0001,copy_X=True,cv=None,verbose=False,n_jobs=1,positive=False,random_state=None,selection='cyclic')
sklearn.linear_model.coordinate_descent.LinearModelCV(self,eps=0.001,n_alphas=100,alphas=None,fit_intercept=True,normalize=False,precompute='auto',max_iter=1000,tol=0.0001,copy_X=True,cv=None,verbose=False,n_jobs=1,positive=False,random_state=None,selection='cyclic')
sklearn.linear_model.coordinate_descent.LinearModelCV.__init__(self,eps=0.001,n_alphas=100,alphas=None,fit_intercept=True,normalize=False,precompute='auto',max_iter=1000,tol=0.0001,copy_X=True,cv=None,verbose=False,n_jobs=1,positive=False,random_state=None,selection='cyclic')
sklearn.linear_model.coordinate_descent.LinearModelCV.fit(self,X,y)
sklearn.linear_model.coordinate_descent.MultiTaskElasticNet(self,alpha=1.0,l1_ratio=0.5,fit_intercept=True,normalize=False,copy_X=True,max_iter=1000,tol=0.0001,warm_start=False,random_state=None,selection='cyclic')
sklearn.linear_model.coordinate_descent.MultiTaskElasticNet.__init__(self,alpha=1.0,l1_ratio=0.5,fit_intercept=True,normalize=False,copy_X=True,max_iter=1000,tol=0.0001,warm_start=False,random_state=None,selection='cyclic')
sklearn.linear_model.coordinate_descent.MultiTaskElasticNet.fit(self,X,y)
sklearn.linear_model.coordinate_descent.MultiTaskElasticNetCV(self,l1_ratio=0.5,eps=0.001,n_alphas=100,alphas=None,fit_intercept=True,normalize=False,max_iter=1000,tol=0.0001,cv=None,copy_X=True,verbose=0,n_jobs=1,random_state=None,selection='cyclic')
sklearn.linear_model.coordinate_descent.MultiTaskElasticNetCV.__init__(self,l1_ratio=0.5,eps=0.001,n_alphas=100,alphas=None,fit_intercept=True,normalize=False,max_iter=1000,tol=0.0001,cv=None,copy_X=True,verbose=0,n_jobs=1,random_state=None,selection='cyclic')
sklearn.linear_model.coordinate_descent.MultiTaskLasso(self,alpha=1.0,fit_intercept=True,normalize=False,copy_X=True,max_iter=1000,tol=0.0001,warm_start=False,random_state=None,selection='cyclic')
sklearn.linear_model.coordinate_descent.MultiTaskLasso.__init__(self,alpha=1.0,fit_intercept=True,normalize=False,copy_X=True,max_iter=1000,tol=0.0001,warm_start=False,random_state=None,selection='cyclic')
sklearn.linear_model.coordinate_descent.MultiTaskLassoCV(self,eps=0.001,n_alphas=100,alphas=None,fit_intercept=True,normalize=False,max_iter=1000,tol=0.0001,copy_X=True,cv=None,verbose=False,n_jobs=1,random_state=None,selection='cyclic')
sklearn.linear_model.coordinate_descent.MultiTaskLassoCV.__init__(self,eps=0.001,n_alphas=100,alphas=None,fit_intercept=True,normalize=False,max_iter=1000,tol=0.0001,copy_X=True,cv=None,verbose=False,n_jobs=1,random_state=None,selection='cyclic')
sklearn.linear_model.coordinate_descent._alpha_grid(X,y,Xy=None,l1_ratio=1.0,fit_intercept=True,eps=0.001,n_alphas=100,normalize=False,copy_X=True)
sklearn.linear_model.coordinate_descent._path_residuals(X,y,train,test,path,path_params,alphas=None,l1_ratio=1,X_order=None,dtype=None)
sklearn.linear_model.coordinate_descent.enet_path(X,y,l1_ratio=0.5,eps=0.001,n_alphas=100,alphas=None,precompute='auto',Xy=None,copy_X=True,coef_init=None,verbose=False,return_n_iter=False,positive=False,check_input=True,**params)
sklearn.linear_model.coordinate_descent.lasso_path(X,y,eps=0.001,n_alphas=100,alphas=None,precompute='auto',Xy=None,copy_X=True,coef_init=None,verbose=False,return_n_iter=False,positive=False,**params)
sklearn.linear_model.enet_path(X,y,l1_ratio=0.5,eps=0.001,n_alphas=100,alphas=None,precompute='auto',Xy=None,copy_X=True,coef_init=None,verbose=False,return_n_iter=False,positive=False,check_input=True,**params)
sklearn.linear_model.lasso_path(X,y,eps=0.001,n_alphas=100,alphas=None,precompute='auto',Xy=None,copy_X=True,coef_init=None,verbose=False,return_n_iter=False,positive=False,**params)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/linear_model/bayes.py----------------------------------------
A:sklearn.linear_model.bayes.(X, y)->check_X_y(X, y, dtype=np.float64, y_numeric=True)
A:sklearn.linear_model.bayes.(X, y, X_offset_, y_offset_, X_scale_)->self._preprocess_data(X, y, self.fit_intercept, self.normalize, self.copy_X)
A:sklearn.linear_model.bayes.self.scores_->list()
A:sklearn.linear_model.bayes.XT_y->numpy.dot(X.T, y)
A:sklearn.linear_model.bayes.(U, S, Vh)->scipy.linalg.svd(X, full_matrices=False)
A:sklearn.linear_model.bayes.coef_->numpy.zeros(n_features)
A:sklearn.linear_model.bayes.rmse_->numpy.sum((y - np.dot(X, coef_)) ** 2)
A:sklearn.linear_model.bayes.gamma_->numpy.sum(alpha_ * eigen_vals_ / (lambda_ + alpha_ * eigen_vals_))
A:sklearn.linear_model.bayes.coef_old_->numpy.copy(coef_)
A:sklearn.linear_model.bayes.sigma_->numpy.dot(sigma_, X[:, keep_lambda] * np.reshape(1.0 / lambda_[keep_lambda], [1, -1]))
A:sklearn.linear_model.bayes.y_mean->self._decision_function(X)
A:sklearn.linear_model.bayes.sigmas_squared_data->(np.dot(X, self.sigma_) * X).sum(axis=1)
A:sklearn.linear_model.bayes.y_std->numpy.sqrt(sigmas_squared_data + 1.0 / self.alpha_)
A:sklearn.linear_model.bayes.keep_lambda->numpy.ones(n_features, dtype=bool)
A:sklearn.linear_model.bayes.lambda_->numpy.ones(n_features)
A:sklearn.linear_model.bayes.s->(lambda_1 * np.log(lambda_) - lambda_2 * lambda_).sum()
sklearn.linear_model.ARDRegression(self,n_iter=300,tol=0.001,alpha_1=1e-06,alpha_2=1e-06,lambda_1=1e-06,lambda_2=1e-06,compute_score=False,threshold_lambda=10000.0,fit_intercept=True,normalize=False,copy_X=True,verbose=False)
sklearn.linear_model.ARDRegression.fit(self,X,y)
sklearn.linear_model.ARDRegression.predict(self,X,return_std=False)
sklearn.linear_model.BayesianRidge(self,n_iter=300,tol=0.001,alpha_1=1e-06,alpha_2=1e-06,lambda_1=1e-06,lambda_2=1e-06,compute_score=False,fit_intercept=True,normalize=False,copy_X=True,verbose=False)
sklearn.linear_model.BayesianRidge.fit(self,X,y)
sklearn.linear_model.BayesianRidge.predict(self,X,return_std=False)
sklearn.linear_model.bayes.ARDRegression(self,n_iter=300,tol=0.001,alpha_1=1e-06,alpha_2=1e-06,lambda_1=1e-06,lambda_2=1e-06,compute_score=False,threshold_lambda=10000.0,fit_intercept=True,normalize=False,copy_X=True,verbose=False)
sklearn.linear_model.bayes.ARDRegression.__init__(self,n_iter=300,tol=0.001,alpha_1=1e-06,alpha_2=1e-06,lambda_1=1e-06,lambda_2=1e-06,compute_score=False,threshold_lambda=10000.0,fit_intercept=True,normalize=False,copy_X=True,verbose=False)
sklearn.linear_model.bayes.ARDRegression.fit(self,X,y)
sklearn.linear_model.bayes.ARDRegression.predict(self,X,return_std=False)
sklearn.linear_model.bayes.BayesianRidge(self,n_iter=300,tol=0.001,alpha_1=1e-06,alpha_2=1e-06,lambda_1=1e-06,lambda_2=1e-06,compute_score=False,fit_intercept=True,normalize=False,copy_X=True,verbose=False)
sklearn.linear_model.bayes.BayesianRidge.__init__(self,n_iter=300,tol=0.001,alpha_1=1e-06,alpha_2=1e-06,lambda_1=1e-06,lambda_2=1e-06,compute_score=False,fit_intercept=True,normalize=False,copy_X=True,verbose=False)
sklearn.linear_model.bayes.BayesianRidge.fit(self,X,y)
sklearn.linear_model.bayes.BayesianRidge.predict(self,X,return_std=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/linear_model/tests/test_ridge.py----------------------------------------
A:sklearn.linear_model.tests.test_ridge.diabetes->sklearn.datasets.load_diabetes()
A:sklearn.linear_model.tests.test_ridge.ind->numpy.arange(X_diabetes.shape[0])
A:sklearn.linear_model.tests.test_ridge.rng->numpy.random.RandomState(0)
A:sklearn.linear_model.tests.test_ridge.iris->sklearn.datasets.load_iris()
A:sklearn.linear_model.tests.test_ridge.X_iris->scipy.sparse.csr_matrix(iris.data)
A:sklearn.linear_model.tests.test_ridge.y->numpy.random.RandomState(0).randn(n)
A:sklearn.linear_model.tests.test_ridge.X->numpy.eye(3)
A:sklearn.linear_model.tests.test_ridge.ridge->Ridge(alpha=1)
A:sklearn.linear_model.tests.test_ridge.coef->_solve_cholesky(X_diabetes, y, alpha=[0.01])
A:sklearn.linear_model.tests.test_ridge.K->numpy.dot(X_diabetes, X_diabetes.T)
A:sklearn.linear_model.tests.test_ridge.dual_coef->_solve_cholesky_kernel(K, y, alpha=[0.01])
A:sklearn.linear_model.tests.test_ridge.coefs->ridge_regression(X, y, alpha=alpha, sample_weight=sample_weight, solver=solver)
A:sklearn.linear_model.tests.test_ridge.coefs2->ridge_regression(X * np.sqrt(sample_weight)[:, np.newaxis], y * np.sqrt(sample_weight), alpha=alpha, solver=solver)
A:sklearn.linear_model.tests.test_ridge.param_grid->product((1.0, 0.01), (True, False), ('svd', 'cholesky', 'lsqr', 'sparse_cg'))
A:sklearn.linear_model.tests.test_ridge.est->Ridge(alpha=alpha, fit_intercept=intercept, solver=solver)
A:sklearn.linear_model.tests.test_ridge.W->numpy.diag(sample_weight)
A:sklearn.linear_model.tests.test_ridge.I->numpy.eye(n_features + 1)
A:sklearn.linear_model.tests.test_ridge.dummy_column->numpy.ones(shape=(n_samples, 1))
A:sklearn.linear_model.tests.test_ridge.X_aug->numpy.concatenate((dummy_column, X), axis=1)
A:sklearn.linear_model.tests.test_ridge.cf_coefs->scipy.linalg.solve(X_aug.T.dot(W).dot(X_aug) + alpha * I, X_aug.T.dot(W).dot(y))
A:sklearn.linear_model.tests.test_ridge.Y->numpy.array([1, 2])
A:sklearn.linear_model.tests.test_ridge.reg->Ridge(solver=solver, max_iter=1, tol=0.1)
A:sklearn.linear_model.tests.test_ridge.ols->LinearRegression(fit_intercept=False)
A:sklearn.linear_model.tests.test_ridge.penalties->numpy.arange(n_targets)
A:sklearn.linear_model.tests.test_ridge.coef_cholesky->numpy.array([Ridge(alpha=alpha, solver='cholesky').fit(X, target).coef_ for (alpha, target) in zip(penalties, y.T)])
A:sklearn.linear_model.tests.test_ridge.ridge_gcv->_RidgeGCV(fit_intercept=fit_intercept)
A:sklearn.linear_model.tests.test_ridge.decomp->_RidgeGCV(fit_intercept=fit_intercept)._pre_compute_svd(X_diabetes_, y_diabetes, fit_intercept)
A:sklearn.linear_model.tests.test_ridge.(errors, c)->_RidgeGCV(fit_intercept=fit_intercept)._errors(1.0, y_diabetes, *decomp)
A:sklearn.linear_model.tests.test_ridge.(values, c)->_RidgeGCV(fit_intercept=fit_intercept)._values(1.0, y_diabetes, *decomp)
A:sklearn.linear_model.tests.test_ridge.(errors3, c)->_RidgeGCV(fit_intercept=fit_intercept)._errors_svd(ridge.alpha, y_diabetes, *decomp)
A:sklearn.linear_model.tests.test_ridge.(values3, c)->_RidgeGCV(fit_intercept=fit_intercept)._values_svd(ridge.alpha, y_diabetes, *decomp)
A:sklearn.linear_model.tests.test_ridge.scoring->make_scorer(func)
A:sklearn.linear_model.tests.test_ridge.ridge_gcv2->RidgeCV(fit_intercept=False, scoring=scoring)
A:sklearn.linear_model.tests.test_ridge.ridge_gcv3->RidgeCV(fit_intercept=False, scoring=scoring)
A:sklearn.linear_model.tests.test_ridge.scorer->get_scorer('neg_mean_squared_error')
A:sklearn.linear_model.tests.test_ridge.ridge_gcv4->RidgeCV(fit_intercept=False, scoring=scorer)
A:sklearn.linear_model.tests.test_ridge.Y_pred->Ridge(alpha=1).predict(filter_(X_diabetes))
A:sklearn.linear_model.tests.test_ridge.y_pred->Ridge(solver=solver, max_iter=1, tol=0.1).predict(filter_(X_iris))
A:sklearn.linear_model.tests.test_ridge.ridge_cv->RidgeCV()
A:sklearn.linear_model.tests.test_ridge.gs->GridSearchCV(Ridge(), parameters, cv=cv)
A:sklearn.linear_model.tests.test_ridge.cv->KFold(5)
A:sklearn.linear_model.tests.test_ridge.score->Ridge(alpha=1).score(filter_(X_diabetes), y_diabetes)
A:sklearn.linear_model.tests.test_ridge.ridge2->Ridge(tol=0.001, fit_intercept=False)
A:sklearn.linear_model.tests.test_ridge.score2->Ridge(tol=0.001, fit_intercept=False).score(filter_(X_diabetes), y_diabetes)
A:sklearn.linear_model.tests.test_ridge.ret_dense->test_func(DENSE_FILTER)
A:sklearn.linear_model.tests.test_ridge.ret_sparse->test_func(SPARSE_FILTER)
A:sklearn.linear_model.tests.test_ridge.rega->RidgeClassifier(class_weight='balanced')
A:sklearn.linear_model.tests.test_ridge.reg1->reg()
A:sklearn.linear_model.tests.test_ridge.reg2->reg(class_weight=class_weight)
A:sklearn.linear_model.tests.test_ridge.sample_weight->numpy.ones(iris.target.shape)
A:sklearn.linear_model.tests.test_ridge.rngrng->numpy.random.RandomState(42)
A:sklearn.linear_model.tests.test_ridge.x->numpy.random.RandomState(0).randn(n_samples, n_features)
A:sklearn.linear_model.tests.test_ridge.n_alphas->len(alphas)
A:sklearn.linear_model.tests.test_ridge.r->RidgeCV(alphas=alphas, store_cv_values=True)
A:sklearn.linear_model.tests.test_ridge.ridgecv->_RidgeGCV()
A:sklearn.linear_model.tests.test_ridge.sparse_ridge->Ridge(alpha=1.0, fit_intercept=False)
A:sklearn.linear_model.tests.test_ridge.dense_ridge->Ridge(alpha=1.0, fit_intercept=False)
A:sklearn.linear_model.tests.test_ridge.X_sparse->sparse_converter(X)
A:sklearn.linear_model.tests.test_ridge.(X, y)->make_multilabel_classification(n_samples=10, random_state=0)
A:sklearn.linear_model.tests.test_ridge.X_csr->scipy.sparse.csr_matrix(X)
A:sklearn.linear_model.tests.test_ridge.dense->Ridge(alpha=1.0, tol=1e-15, solver=solver, fit_intercept=True)
A:sklearn.linear_model.tests.test_ridge.sparse->Ridge(alpha=1.0, tol=1e-15, solver='lsqr', fit_intercept=True)
A:sklearn.linear_model.tests.test_ridge.v->numpy.random.RandomState(0).randn(p)
A:sklearn.linear_model.tests.test_ridge.Q->numpy.random.RandomState(0).randn(len(v), len(v))
A:sklearn.linear_model.tests.test_ridge.QT_y->numpy.random.RandomState(0).randn(len(v), len(v)).T.dot(y)
A:sklearn.linear_model.tests.test_ridge.(G_diag, c)->_RidgeGCV()._errors_and_values_svd_helper(alpha, y, v, U, UT_y)
A:sklearn.linear_model.tests.test_ridge.(out, c_)->_RidgeGCV()._values_svd(alpha, y, v, U, UT_y)
A:sklearn.linear_model.tests.test_ridge.U->numpy.random.RandomState(0).randn(n, p)
A:sklearn.linear_model.tests.test_ridge.UT_y->numpy.random.RandomState(0).randn(n, p).T.dot(y)
A:sklearn.linear_model.tests.test_ridge.X_64->numpy.random.RandomState(0).randn(n_samples, n_features)
A:sklearn.linear_model.tests.test_ridge.y_64->numpy.random.RandomState(0).randn(n_samples, n_target)
A:sklearn.linear_model.tests.test_ridge.X_32->numpy.random.RandomState(0).randn(n_samples, n_features).astype(np.float32)
A:sklearn.linear_model.tests.test_ridge.y_32->numpy.random.RandomState(0).randn(n_samples, n_target).astype(np.float32)
A:sklearn.linear_model.tests.test_ridge.ridge_32->Ridge(alpha=alpha, solver='cholesky')
A:sklearn.linear_model.tests.test_ridge.ridge_64->Ridge(alpha=alpha, solver='cholesky')
sklearn.linear_model.tests.test_ridge._test_multi_ridge_diabetes(filter_)
sklearn.linear_model.tests.test_ridge._test_ridge_classifiers(filter_)
sklearn.linear_model.tests.test_ridge._test_ridge_cv(filter_)
sklearn.linear_model.tests.test_ridge._test_ridge_cv_normalize(filter_)
sklearn.linear_model.tests.test_ridge._test_ridge_diabetes(filter_)
sklearn.linear_model.tests.test_ridge._test_ridge_loo(filter_)
sklearn.linear_model.tests.test_ridge._test_tolerance(filter_)
sklearn.linear_model.tests.test_ridge.check_dense_sparse(test_func)
sklearn.linear_model.tests.test_ridge.test_class_weight_vs_sample_weight()
sklearn.linear_model.tests.test_ridge.test_class_weights()
sklearn.linear_model.tests.test_ridge.test_class_weights_cv()
sklearn.linear_model.tests.test_ridge.test_dense_sparse()
sklearn.linear_model.tests.test_ridge.test_dtype_match()
sklearn.linear_model.tests.test_ridge.test_dtype_match_cholesky()
sklearn.linear_model.tests.test_ridge.test_errors_and_values_helper()
sklearn.linear_model.tests.test_ridge.test_errors_and_values_svd_helper()
sklearn.linear_model.tests.test_ridge.test_n_iter()
sklearn.linear_model.tests.test_ridge.test_primal_dual_relationship()
sklearn.linear_model.tests.test_ridge.test_raises_value_error_if_sample_weights_greater_than_1d()
sklearn.linear_model.tests.test_ridge.test_raises_value_error_if_solver_not_supported()
sklearn.linear_model.tests.test_ridge.test_ridge()
sklearn.linear_model.tests.test_ridge.test_ridge_classifier_no_support_multilabel()
sklearn.linear_model.tests.test_ridge.test_ridge_cv_sparse_svd()
sklearn.linear_model.tests.test_ridge.test_ridge_fit_intercept_sparse()
sklearn.linear_model.tests.test_ridge.test_ridge_individual_penalties()
sklearn.linear_model.tests.test_ridge.test_ridge_intercept()
sklearn.linear_model.tests.test_ridge.test_ridge_regression_sample_weights()
sklearn.linear_model.tests.test_ridge.test_ridge_sample_weights()
sklearn.linear_model.tests.test_ridge.test_ridge_shapes()
sklearn.linear_model.tests.test_ridge.test_ridge_singular()
sklearn.linear_model.tests.test_ridge.test_ridge_sparse_svd()
sklearn.linear_model.tests.test_ridge.test_ridge_vs_lstsq()
sklearn.linear_model.tests.test_ridge.test_ridgecv_sample_weight()
sklearn.linear_model.tests.test_ridge.test_ridgecv_store_cv_values()
sklearn.linear_model.tests.test_ridge.test_sparse_cg_max_iter()
sklearn.linear_model.tests.test_ridge.test_sparse_design_with_sample_weights()
sklearn.linear_model.tests.test_ridge.test_toy_ridge_object()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/linear_model/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/linear_model/tests/test_base.py----------------------------------------
A:sklearn.linear_model.tests.test_base.rng->numpy.random.RandomState(0)
A:sklearn.linear_model.tests.test_base.reg->LinearRegression(fit_intercept=True)
A:sklearn.linear_model.tests.test_base.y->numpy.random.RandomState(0).rand(n_samples)
A:sklearn.linear_model.tests.test_base.X->scipy.sparse.csr_matrix(X)
A:sklearn.linear_model.tests.test_base.W->numpy.diag(sample_weight)
A:sklearn.linear_model.tests.test_base.dummy_column->numpy.ones(shape=(n_samples, 1))
A:sklearn.linear_model.tests.test_base.X_aug->numpy.concatenate((dummy_column, X), axis=1)
A:sklearn.linear_model.tests.test_base.coefs2->scipy.linalg.solve(X_aug.T.dot(W).dot(X_aug), X_aug.T.dot(W).dot(y))
A:sklearn.linear_model.tests.test_base.X2->numpy.array([[0.38349978, 0.61650022], [0.58853682, 0.41146318]])
A:sklearn.linear_model.tests.test_base.X3->numpy.array([[0.27677969, 0.70693172, 0.01628859], [0.08385139, 0.20692515, 0.70922346]])
A:sklearn.linear_model.tests.test_base.lr2_without_intercept->LinearRegression(fit_intercept=False).fit(X2, y)
A:sklearn.linear_model.tests.test_base.lr2_with_intercept->LinearRegression(fit_intercept=True).fit(X2, y)
A:sklearn.linear_model.tests.test_base.lr3_without_intercept->LinearRegression(fit_intercept=False).fit(X3, y)
A:sklearn.linear_model.tests.test_base.lr3_with_intercept->LinearRegression(fit_intercept=True).fit(X3, y)
A:sklearn.linear_model.tests.test_base.random_state->check_random_state(random_state)
A:sklearn.linear_model.tests.test_base.beta->check_random_state(random_state).rand(n)
A:sklearn.linear_model.tests.test_base.ols->LinearRegression()
A:sklearn.linear_model.tests.test_base.(X, y)->make_regression()
A:sklearn.linear_model.tests.test_base.Y_pred->LinearRegression().predict(X)
A:sklearn.linear_model.tests.test_base.y_pred->LinearRegression().predict(X)
A:sklearn.linear_model.tests.test_base.expected_X_mean->numpy.average(X, axis=0, weights=sample_weight)
A:sklearn.linear_model.tests.test_base.expected_y_mean->numpy.average(y, axis=0, weights=sample_weight)
A:sklearn.linear_model.tests.test_base.(Xt, yt, X_mean, y_mean, X_norm)->_preprocess_data(X, y, fit_intercept=True, normalize=True, return_mean=True)
A:sklearn.linear_model.tests.test_base.(_, yt, _, y_mean, _)->_preprocess_data(X, y, fit_intercept=True, normalize=True)
A:sklearn.linear_model.tests.test_base.sample_weight->numpy.random.RandomState(0).rand(n_samples)
A:sklearn.linear_model.tests.test_base.XA->scipy.sparse.csr_matrix(X).toarray()
A:sklearn.linear_model.tests.test_base.csr->scipy.sparse.csr_matrix(X)
A:sklearn.linear_model.tests.test_base.(csr_, y, _, _, _)->_preprocess_data(csr, y, True)
A:sklearn.linear_model.tests.test_base.X_32->numpy.asarray(X, dtype=np.float32)
A:sklearn.linear_model.tests.test_base.y_32->numpy.asarray(y, dtype=np.float32)
A:sklearn.linear_model.tests.test_base.X_64->numpy.asarray(X, dtype=np.float64)
A:sklearn.linear_model.tests.test_base.y_64->numpy.asarray(y, dtype=np.float64)
A:sklearn.linear_model.tests.test_base.(Xt_32, yt_32, X_mean_32, y_mean_32, X_norm_32)->_preprocess_data(X_32, y_32, fit_intercept=fit_intercept, normalize=normalize, return_mean=True)
A:sklearn.linear_model.tests.test_base.(Xt_64, yt_64, X_mean_64, y_mean_64, X_norm_64)->_preprocess_data(X_64, y_64, fit_intercept=fit_intercept, normalize=normalize, return_mean=True)
A:sklearn.linear_model.tests.test_base.(Xt_3264, yt_3264, X_mean_3264, y_mean_3264, X_norm_3264)->_preprocess_data(X_32, y_64, fit_intercept=fit_intercept, normalize=normalize, return_mean=True)
A:sklearn.linear_model.tests.test_base.(Xt_6432, yt_6432, X_mean_6432, y_mean_6432, X_norm_6432)->_preprocess_data(X_64, y_32, fit_intercept=fit_intercept, normalize=normalize, return_mean=True)
A:sklearn.linear_model.tests.test_base.(rescaled_X, rescaled_y)->_rescale_data(X, y, sample_weight)
A:sklearn.linear_model.tests.test_base.param_grid->product([True, False], [True, False], [True, False], [None, w])
A:sklearn.linear_model.tests.test_base.XX->scipy.sparse.csr_matrix(X).copy()
A:sklearn.linear_model.tests.test_base.(X1, y1, X1_mean, X1_var, y1_mean)->sparse_center_data(X, y, fit_intercept=fit_intercept, normalize=normalize)
A:sklearn.linear_model.tests.test_base.(X2, y2, X2_mean, X2_var, y2_mean)->_preprocess_data(X, y, fit_intercept=fit_intercept, normalize=normalize, return_mean=True)
sklearn.linear_model.tests.test_base.test_csr_preprocess_data()
sklearn.linear_model.tests.test_base.test_deprecation_center_data()
sklearn.linear_model.tests.test_base.test_dtype_preprocess_data()
sklearn.linear_model.tests.test_base.test_fit_intercept()
sklearn.linear_model.tests.test_base.test_linear_regression()
sklearn.linear_model.tests.test_base.test_linear_regression_multiple_outcome(random_state=0)
sklearn.linear_model.tests.test_base.test_linear_regression_sample_weights()
sklearn.linear_model.tests.test_base.test_linear_regression_sparse(random_state=0)
sklearn.linear_model.tests.test_base.test_linear_regression_sparse_multiple_outcome(random_state=0)
sklearn.linear_model.tests.test_base.test_preprocess_data()
sklearn.linear_model.tests.test_base.test_preprocess_data_multioutput()
sklearn.linear_model.tests.test_base.test_preprocess_data_weighted()
sklearn.linear_model.tests.test_base.test_raises_value_error_if_sample_weights_greater_than_1d()
sklearn.linear_model.tests.test_base.test_rescale_data()
sklearn.linear_model.tests.test_base.test_sparse_preprocess_data_with_return_mean()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/linear_model/tests/test_logistic.py----------------------------------------
A:sklearn.linear_model.tests.test_logistic.X_sp->scipy.sparse.csr_matrix(X_sp)
A:sklearn.linear_model.tests.test_logistic.iris->load_iris()
A:sklearn.linear_model.tests.test_logistic.n_samples->len(y)
A:sklearn.linear_model.tests.test_logistic.classes->numpy.unique(y)
A:sklearn.linear_model.tests.test_logistic.predicted->LogisticRegression(tol=0.0001, multi_class=multi_class, warm_start=warm_start, solver=solver, random_state=42, max_iter=100, fit_intercept=fit_intercept).fit(X, y).predict(X)
A:sklearn.linear_model.tests.test_logistic.probabilities->LogisticRegression(tol=0.0001, multi_class=multi_class, warm_start=warm_start, solver=solver, random_state=42, max_iter=100, fit_intercept=fit_intercept).predict_proba(iris.data)
A:sklearn.linear_model.tests.test_logistic.lr->LogisticRegression(max_iter=max_iter, tol=1e-15, multi_class=multi_class, random_state=0, solver=solver)
A:sklearn.linear_model.tests.test_logistic.pred->LogisticRegression(tol=0.0001, multi_class=multi_class, warm_start=warm_start, solver=solver, random_state=42, max_iter=100, fit_intercept=fit_intercept).predict(iris.data)
A:sklearn.linear_model.tests.test_logistic.target->(iris.target > 0).astype(np.intp)
A:sklearn.linear_model.tests.test_logistic.clf->LogisticRegression(tol=0.0001, multi_class=multi_class, warm_start=warm_start, solver=solver, random_state=42, max_iter=100, fit_intercept=fit_intercept)
A:sklearn.linear_model.tests.test_logistic.mlr->LogisticRegression(solver=solver, multi_class='multinomial', random_state=42, fit_intercept=False)
A:sklearn.linear_model.tests.test_logistic.pred_d_d->LogisticRegression(tol=0.0001, multi_class=multi_class, warm_start=warm_start, solver=solver, random_state=42, max_iter=100, fit_intercept=fit_intercept).decision_function(iris.data)
A:sklearn.linear_model.tests.test_logistic.pred_s_d->LogisticRegression(tol=0.0001, multi_class=multi_class, warm_start=warm_start, solver=solver, random_state=42, max_iter=100, fit_intercept=fit_intercept).decision_function(iris.data)
A:sklearn.linear_model.tests.test_logistic.sp_data->scipy.sparse.coo_matrix(iris.data)
A:sklearn.linear_model.tests.test_logistic.pred_s_s->LogisticRegression(tol=0.0001, multi_class=multi_class, warm_start=warm_start, solver=solver, random_state=42, max_iter=100, fit_intercept=fit_intercept).decision_function(sp_data)
A:sklearn.linear_model.tests.test_logistic.pred_d_s->LogisticRegression(tol=0.0001, multi_class=multi_class, warm_start=warm_start, solver=solver, random_state=42, max_iter=100, fit_intercept=fit_intercept).decision_function(sp_data)
A:sklearn.linear_model.tests.test_logistic.rng->numpy.random.RandomState(42)
A:sklearn.linear_model.tests.test_logistic.X_->numpy.hstack((X, np.ones(10)[:, np.newaxis]))
A:sklearn.linear_model.tests.test_logistic.y_->numpy.ones(X_.shape[0])
A:sklearn.linear_model.tests.test_logistic.Xnan->numpy.array(X, dtype=np.float64)
A:sklearn.linear_model.tests.test_logistic.X->numpy.concatenate([X] * 10)
A:sklearn.linear_model.tests.test_logistic.Cs->numpy.logspace(0, 4, 10)
A:sklearn.linear_model.tests.test_logistic.(coefs, Cs, _)->f(logistic_regression_path)(X, y, Cs=Cs, fit_intercept=True, tol=1e-06, solver=solver, intercept_scaling=10000.0, random_state=0)
A:sklearn.linear_model.tests.test_logistic.lr_coef->numpy.concatenate([lr.coef_.ravel(), lr.intercept_])
A:sklearn.linear_model.tests.test_logistic.(X, y)->make_classification(n_samples=10, n_features=20, random_state=0, n_classes=3, n_informative=10)
A:sklearn.linear_model.tests.test_logistic.lr1->LogisticRegression(random_state=0, dual=True, max_iter=1, tol=1e-15)
A:sklearn.linear_model.tests.test_logistic.lr2->LogisticRegression(random_state=0, dual=True, max_iter=1, tol=1e-15)
A:sklearn.linear_model.tests.test_logistic.lr3->LogisticRegression(random_state=8, dual=True, max_iter=1, tol=1e-15)
A:sklearn.linear_model.tests.test_logistic.(X_ref, y)->make_classification(n_samples=n_samples, n_features=n_features, n_classes=n_classes, n_informative=3, random_state=0)
A:sklearn.linear_model.tests.test_logistic.w->w.ravel().ravel()
A:sklearn.linear_model.tests.test_logistic.(loss, grad)->_logistic_loss_and_grad(w, X, y, alpha=1.0)
A:sklearn.linear_model.tests.test_logistic.approx_grad->scipy.optimize.approx_fprime(w, lambda w: _logistic_loss_and_grad(w, X, y, alpha=1.0)[0], 0.001)
A:sklearn.linear_model.tests.test_logistic.(loss_interp, grad_interp)->_logistic_loss_and_grad(w, X, y, alpha=1.0)
A:sklearn.linear_model.tests.test_logistic.X_ref->numpy.random.RandomState(42).randn(n_samples, n_features)
A:sklearn.linear_model.tests.test_logistic.y->numpy.concatenate([y] * 10)
A:sklearn.linear_model.tests.test_logistic.(grad_2, hess)->_logistic_grad_hess(w, X, y, alpha=1.0)
A:sklearn.linear_model.tests.test_logistic.vector->numpy.zeros_like(grad)
A:sklearn.linear_model.tests.test_logistic.hess_col->hessp(vec)
A:sklearn.linear_model.tests.test_logistic.d_x->numpy.linspace(-e, e, 30)
A:sklearn.linear_model.tests.test_logistic.d_grad->numpy.array([_multinomial_grad_hess(w + t * vec, X, Y, alpha=1.0, sample_weight=sample_weights)[0] for t in d_x])
A:sklearn.linear_model.tests.test_logistic.approx_hess_col->scipy.linalg.lstsq(d_x[:, np.newaxis], d_grad)[0].ravel()
A:sklearn.linear_model.tests.test_logistic.loss_interp_2->_logistic_loss(w, X, y, alpha=1.0)
A:sklearn.linear_model.tests.test_logistic.(grad_interp_2, hess)->_logistic_grad_hess(w, X, y, alpha=1.0)
A:sklearn.linear_model.tests.test_logistic.lr_cv->LogisticRegressionCV(penalty='l1', Cs=[1.0], solver='liblinear')
A:sklearn.linear_model.tests.test_logistic.coefs_paths->numpy.asarray(list(clf_multi.coefs_paths_.values()))
A:sklearn.linear_model.tests.test_logistic.scores->numpy.asarray(list(clf_multi.scores_.values()))
A:sklearn.linear_model.tests.test_logistic.y_str->LabelEncoder().fit(['bar', 'baz', 'foo']).inverse_transform(y)
A:sklearn.linear_model.tests.test_logistic.lr_str->LogisticRegression(solver='lbfgs', multi_class='multinomial')
A:sklearn.linear_model.tests.test_logistic.lr_cv_str->LogisticRegression(solver='lbfgs', class_weight={'bar': 1, 'baz': 2, 'foo': 0}, multi_class='multinomial').fit(X_ref, y_str)
A:sklearn.linear_model.tests.test_logistic.csr->scipy.sparse.csr_matrix(X)
A:sklearn.linear_model.tests.test_logistic.clfs->LogisticRegressionCV(fit_intercept=True)
A:sklearn.linear_model.tests.test_logistic.(grad_interp, hess_interp)->_logistic_grad_hess(w, X, y, alpha)
A:sklearn.linear_model.tests.test_logistic.loss_interp->_logistic_loss(w, X, y, alpha)
A:sklearn.linear_model.tests.test_logistic.(grad, hess)->_logistic_grad_hess(w, X_, y, alpha)
A:sklearn.linear_model.tests.test_logistic.loss->_logistic_loss(w, X_, y, alpha)
A:sklearn.linear_model.tests.test_logistic.grad->numpy.random.RandomState(42).rand(n_features + 1)
A:sklearn.linear_model.tests.test_logistic.hess_interp->hess_interp(grad)
A:sklearn.linear_model.tests.test_logistic.hess->hess(grad)
A:sklearn.linear_model.tests.test_logistic.cv->StratifiedKFold(n_cv)
A:sklearn.linear_model.tests.test_logistic.precomputed_folds->list(cv.split(train, target))
A:sklearn.linear_model.tests.test_logistic.clf1->LogisticRegression(solver=solver, multi_class='ovr', class_weight='balanced')
A:sklearn.linear_model.tests.test_logistic.target_copy->(iris.target > 0).astype(np.intp).copy()
A:sklearn.linear_model.tests.test_logistic.clf_multi->LogisticRegression(multi_class='multinomial', solver='lbfgs')
A:sklearn.linear_model.tests.test_logistic.multi_score->LogisticRegression(multi_class='multinomial', solver='lbfgs').score(train, target)
A:sklearn.linear_model.tests.test_logistic.ovr_score->LogisticRegression(tol=0.0001, multi_class=multi_class, warm_start=warm_start, solver=solver, random_state=42, max_iter=100, fit_intercept=fit_intercept).score(train, target)
A:sklearn.linear_model.tests.test_logistic.ncg->LogisticRegression(solver='newton-cg', fit_intercept=False, tol=tol)
A:sklearn.linear_model.tests.test_logistic.lbf->LogisticRegression(solver='lbfgs', fit_intercept=False, tol=tol)
A:sklearn.linear_model.tests.test_logistic.lib->LogisticRegression(fit_intercept=False, tol=tol)
A:sklearn.linear_model.tests.test_logistic.sag->LogisticRegression(solver='sag', fit_intercept=False, tol=tol, max_iter=1000, random_state=42)
A:sklearn.linear_model.tests.test_logistic.saga->LogisticRegression(C=1.0 / (n_samples * alpha), solver='saga', multi_class='ovr', max_iter=200, fit_intercept=False, penalty=penalty, random_state=0, tol=1e-24)
A:sklearn.linear_model.tests.test_logistic.n_classes->len(weight)
A:sklearn.linear_model.tests.test_logistic.clf_lbf->LogisticRegressionCV(solver='lbfgs', Cs=1, fit_intercept=False, class_weight=class_weight)
A:sklearn.linear_model.tests.test_logistic.clf_ncg->LogisticRegressionCV(solver='newton-cg', Cs=1, fit_intercept=False, class_weight=class_weight)
A:sklearn.linear_model.tests.test_logistic.clf_lib->LogisticRegression(solver='liblinear', max_iter=2, verbose=1)
A:sklearn.linear_model.tests.test_logistic.clf_sag->LogisticRegressionCV(solver='sag', Cs=1, fit_intercept=False, class_weight=class_weight, tol=1e-05, max_iter=10000, random_state=0)
A:sklearn.linear_model.tests.test_logistic.clf_saga->LogisticRegressionCV(solver='saga', Cs=1, fit_intercept=False, class_weight=class_weight, tol=1e-05, max_iter=10000, random_state=0)
A:sklearn.linear_model.tests.test_logistic.clf_sw_none->LR(solver=solver, fit_intercept=False, random_state=42)
A:sklearn.linear_model.tests.test_logistic.clf_sw_ones->LR(solver=solver, fit_intercept=False, random_state=42)
A:sklearn.linear_model.tests.test_logistic.clf_sw_lbfgs->LR(solver='lbfgs', fit_intercept=False, random_state=42)
A:sklearn.linear_model.tests.test_logistic.clf_sw_n->LR(solver='newton-cg', fit_intercept=False, random_state=42)
A:sklearn.linear_model.tests.test_logistic.clf_sw_sag->LR(solver='sag', fit_intercept=False, tol=1e-10, random_state=42)
A:sklearn.linear_model.tests.test_logistic.clf_sw_liblinear->LR(solver='liblinear', fit_intercept=False, random_state=42)
A:sklearn.linear_model.tests.test_logistic.clf_cw_12->LR(solver=solver, fit_intercept=False, class_weight={0: 1, 1: 2}, random_state=42)
A:sklearn.linear_model.tests.test_logistic.clf_sw_12->LR(solver=solver, fit_intercept=False, random_state=42)
A:sklearn.linear_model.tests.test_logistic.clf_cw->LogisticRegression(solver='liblinear', fit_intercept=False, class_weight={0: 1, 1: 2}, penalty='l2', dual=True, random_state=42)
A:sklearn.linear_model.tests.test_logistic.clf_sw->LogisticRegression(solver='liblinear', fit_intercept=False, penalty='l2', dual=True, random_state=42)
A:sklearn.linear_model.tests.test_logistic.class_weight->compute_class_weight('balanced', classes, y)
A:sklearn.linear_model.tests.test_logistic.class_weight_dict->_compute_class_weight_dictionary(y)
A:sklearn.linear_model.tests.test_logistic.clf2->LogisticRegression(solver=solver, multi_class='ovr', class_weight=class_weight_dict)
A:sklearn.linear_model.tests.test_logistic.ref_i->LogisticRegression(solver=solver, multi_class='multinomial')
A:sklearn.linear_model.tests.test_logistic.ref_w->LogisticRegression(solver=solver, multi_class='multinomial', fit_intercept=False)
A:sklearn.linear_model.tests.test_logistic.clf_i->LogisticRegression(solver=solver, multi_class='multinomial', random_state=42, max_iter=2000, tol=1e-07)
A:sklearn.linear_model.tests.test_logistic.clf_w->LogisticRegression(solver=solver, multi_class='multinomial', random_state=42, max_iter=2000, tol=1e-07, fit_intercept=False)
A:sklearn.linear_model.tests.test_logistic.clf_path->LogisticRegressionCV(solver=solver, max_iter=2000, tol=1e-06, multi_class='multinomial', Cs=[1.0])
A:sklearn.linear_model.tests.test_logistic.Y->numpy.zeros((n_samples, n_classes))
A:sklearn.linear_model.tests.test_logistic.ind->numpy.argmax(np.dot(X, w.T), axis=1)
A:sklearn.linear_model.tests.test_logistic.sample_weights->numpy.ones(X.shape[0])
A:sklearn.linear_model.tests.test_logistic.(grad, hessp)->_multinomial_grad_hess(w, X, Y, alpha=1.0, sample_weight=sample_weights)
A:sklearn.linear_model.tests.test_logistic.vec->numpy.zeros(n_features * n_classes)
A:sklearn.linear_model.tests.test_logistic.X_noise->numpy.random.RandomState(42).normal(scale=0.1, size=(n_samples, 3))
A:sklearn.linear_model.tests.test_logistic.X_constant->numpy.zeros(shape=(n_samples, 2))
A:sklearn.linear_model.tests.test_logistic.lr_liblinear->LogisticRegression(penalty='l1', C=1.0, solver='liblinear', fit_intercept=False, tol=1e-10)
A:sklearn.linear_model.tests.test_logistic.lr_saga->LogisticRegression(penalty='l1', C=1.0, solver='saga', fit_intercept=False, max_iter=1000, tol=1e-10)
A:sklearn.linear_model.tests.test_logistic.lr_saga_dense->LogisticRegression(penalty='l1', C=1.0, solver='saga', fit_intercept=False, max_iter=1000, tol=1e-10)
A:sklearn.linear_model.tests.test_logistic.clf_multi_loss->log_loss(y, clf_multi.predict_proba(X))
A:sklearn.linear_model.tests.test_logistic.clf_ovr->LogisticRegression(multi_class='ovr', solver='lbfgs')
A:sklearn.linear_model.tests.test_logistic.clf_ovr_loss->log_loss(y, clf_ovr.predict_proba(X))
A:sklearn.linear_model.tests.test_logistic.clf_wrong_loss->log_loss(y, clf_multi._predict_proba_lr(X))
A:sklearn.linear_model.tests.test_logistic.y_bin->numpy.concatenate([y] * 10).copy()
A:sklearn.linear_model.tests.test_logistic.cum_diff->numpy.sum(np.abs(coef_1 - clf.coef_))
A:sklearn.linear_model.tests.test_logistic.(X_sparse, y_sparse)->make_classification(n_samples=50, n_features=20, random_state=0)
A:sklearn.linear_model.tests.test_logistic.X_sparse->scipy.sparse.csr_matrix(X_sparse)
A:sklearn.linear_model.tests.test_logistic.liblinear->LogisticRegression(C=1.0 / (n_samples * alpha), solver='liblinear', multi_class='ovr', max_iter=200, fit_intercept=False, penalty=penalty, random_state=0, tol=1e-24)
A:sklearn.linear_model.tests.test_logistic.X_32->numpy.array(X).astype(np.float32)
A:sklearn.linear_model.tests.test_logistic.y_32->numpy.array(Y1).astype(np.float32)
A:sklearn.linear_model.tests.test_logistic.X_64->numpy.array(X).astype(np.float64)
A:sklearn.linear_model.tests.test_logistic.y_64->numpy.array(Y1).astype(np.float64)
A:sklearn.linear_model.tests.test_logistic.X_sparse_32->scipy.sparse.csr_matrix(X, dtype=np.float32)
A:sklearn.linear_model.tests.test_logistic.lr_32->LogisticRegression(solver=solver, multi_class=multi_class)
A:sklearn.linear_model.tests.test_logistic.lr_32_sparse->LogisticRegression(solver=solver, multi_class=multi_class)
A:sklearn.linear_model.tests.test_logistic.lr_64->LogisticRegression(solver=solver, multi_class=multi_class)
sklearn.linear_model.tests.test_logistic._compute_class_weight_dictionary(y)
sklearn.linear_model.tests.test_logistic.check_predictions(clf,X,y)
sklearn.linear_model.tests.test_logistic.test_check_solver_option()
sklearn.linear_model.tests.test_logistic.test_consistency_path()
sklearn.linear_model.tests.test_logistic.test_dtype_match()
sklearn.linear_model.tests.test_logistic.test_error()
sklearn.linear_model.tests.test_logistic.test_inconsistent_input()
sklearn.linear_model.tests.test_logistic.test_intercept_logistic_helper()
sklearn.linear_model.tests.test_logistic.test_liblinear_decision_function_zero()
sklearn.linear_model.tests.test_logistic.test_liblinear_dual_random_state()
sklearn.linear_model.tests.test_logistic.test_liblinear_logregcv_sparse()
sklearn.linear_model.tests.test_logistic.test_logistic_cv()
sklearn.linear_model.tests.test_logistic.test_logistic_cv_sparse()
sklearn.linear_model.tests.test_logistic.test_logistic_grad_hess()
sklearn.linear_model.tests.test_logistic.test_logistic_loss_and_grad()
sklearn.linear_model.tests.test_logistic.test_logistic_regression_class_weights()
sklearn.linear_model.tests.test_logistic.test_logistic_regression_convergence_warnings()
sklearn.linear_model.tests.test_logistic.test_logistic_regression_multinomial()
sklearn.linear_model.tests.test_logistic.test_logistic_regression_sample_weights()
sklearn.linear_model.tests.test_logistic.test_logistic_regression_solvers()
sklearn.linear_model.tests.test_logistic.test_logistic_regression_solvers_multiclass()
sklearn.linear_model.tests.test_logistic.test_logistic_regressioncv_class_weights()
sklearn.linear_model.tests.test_logistic.test_logreg_cv_penalty()
sklearn.linear_model.tests.test_logistic.test_logreg_intercept_scaling()
sklearn.linear_model.tests.test_logistic.test_logreg_intercept_scaling_zero()
sklearn.linear_model.tests.test_logistic.test_logreg_l1()
sklearn.linear_model.tests.test_logistic.test_logreg_l1_sparse_data()
sklearn.linear_model.tests.test_logistic.test_logreg_predict_proba_multinomial()
sklearn.linear_model.tests.test_logistic.test_lr_liblinear_warning()
sklearn.linear_model.tests.test_logistic.test_max_iter()
sklearn.linear_model.tests.test_logistic.test_multinomial_binary()
sklearn.linear_model.tests.test_logistic.test_multinomial_grad_hess()
sklearn.linear_model.tests.test_logistic.test_multinomial_logistic_regression_string_inputs()
sklearn.linear_model.tests.test_logistic.test_multinomial_validation()
sklearn.linear_model.tests.test_logistic.test_n_iter()
sklearn.linear_model.tests.test_logistic.test_nan()
sklearn.linear_model.tests.test_logistic.test_ovr_multinomial_iris()
sklearn.linear_model.tests.test_logistic.test_predict_2_classes()
sklearn.linear_model.tests.test_logistic.test_predict_3_classes()
sklearn.linear_model.tests.test_logistic.test_predict_iris()
sklearn.linear_model.tests.test_logistic.test_saga_sparse()
sklearn.linear_model.tests.test_logistic.test_saga_vs_liblinear()
sklearn.linear_model.tests.test_logistic.test_sparsify()
sklearn.linear_model.tests.test_logistic.test_warm_start()
sklearn.linear_model.tests.test_logistic.test_write_parameters()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/linear_model/tests/test_theil_sen.py----------------------------------------
A:sklearn.linear_model.tests.test_theil_sen.random_state->numpy.random.RandomState(0)
A:sklearn.linear_model.tests.test_theil_sen.x->numpy.random.RandomState(0).normal(size=n_samples)
A:sklearn.linear_model.tests.test_theil_sen.X->numpy.random.RandomState(0).normal(size=(n_samples, n_features))
A:sklearn.linear_model.tests.test_theil_sen.w->numpy.array([5.0, 10.0, 42.0, 7.0])
A:sklearn.linear_model.tests.test_theil_sen.ix->numpy.random.RandomState(0).randint(0, n_samples, size=n_outliers)
A:sklearn.linear_model.tests.test_theil_sen.new_y->_modified_weiszfeld_step(X, y)
A:sklearn.linear_model.tests.test_theil_sen.y->numpy.random.RandomState(0).normal(size=n_samples)
A:sklearn.linear_model.tests.test_theil_sen.(_, median)->_spatial_median(X, max_iter=100, tol=1e-06)
A:sklearn.linear_model.tests.test_theil_sen.true_median->numpy.median(X.ravel())
A:sklearn.linear_model.tests.test_theil_sen.dists->numpy.array([norm(x - y) for x in X])
A:sklearn.linear_model.tests.test_theil_sen.fermat_weber->fmin_bfgs(cost_func, median, disp=False)
A:sklearn.linear_model.tests.test_theil_sen.(X, y, w, c)->gen_toy_problem_2d()
A:sklearn.linear_model.tests.test_theil_sen.lstq->LinearRegression(fit_intercept=False).fit(X, y)
A:sklearn.linear_model.tests.test_theil_sen.theil_sen->TheilSenRegressor(fit_intercept=True, random_state=0).fit(X, y)
A:sklearn.linear_model.tests.test_theil_sen.bp->_breakdown_point(10000000000.0, 2)
A:sklearn.linear_model.tests.test_theil_sen.y_pred->TheilSenRegressor(fit_intercept=True, random_state=0).fit(X, y).predict(X)
sklearn.linear_model.tests.test_theil_sen.gen_toy_problem_1d(intercept=True)
sklearn.linear_model.tests.test_theil_sen.gen_toy_problem_2d()
sklearn.linear_model.tests.test_theil_sen.gen_toy_problem_4d()
sklearn.linear_model.tests.test_theil_sen.no_stdout_stderr()
sklearn.linear_model.tests.test_theil_sen.test_calc_breakdown_point()
sklearn.linear_model.tests.test_theil_sen.test_checksubparams_n_subsamples_if_less_samples_than_features()
sklearn.linear_model.tests.test_theil_sen.test_checksubparams_negative_subpopulation()
sklearn.linear_model.tests.test_theil_sen.test_checksubparams_too_few_subsamples()
sklearn.linear_model.tests.test_theil_sen.test_checksubparams_too_many_subsamples()
sklearn.linear_model.tests.test_theil_sen.test_less_samples_than_features()
sklearn.linear_model.tests.test_theil_sen.test_modweiszfeld_step_1d()
sklearn.linear_model.tests.test_theil_sen.test_modweiszfeld_step_2d()
sklearn.linear_model.tests.test_theil_sen.test_spatial_median_1d()
sklearn.linear_model.tests.test_theil_sen.test_spatial_median_2d()
sklearn.linear_model.tests.test_theil_sen.test_subpopulation()
sklearn.linear_model.tests.test_theil_sen.test_subsamples()
sklearn.linear_model.tests.test_theil_sen.test_theil_sen_1d()
sklearn.linear_model.tests.test_theil_sen.test_theil_sen_1d_no_intercept()
sklearn.linear_model.tests.test_theil_sen.test_theil_sen_2d()
sklearn.linear_model.tests.test_theil_sen.test_theil_sen_parallel()
sklearn.linear_model.tests.test_theil_sen.test_verbosity()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/linear_model/tests/test_huber.py----------------------------------------
A:sklearn.linear_model.tests.test_huber.rng->numpy.random.RandomState(1)
A:sklearn.linear_model.tests.test_huber.(X, y)->make_regression_with_outliers()
A:sklearn.linear_model.tests.test_huber.num_noise->int(0.1 * n_samples)
A:sklearn.linear_model.tests.test_huber.random_samples->numpy.random.RandomState(1).randint(0, n_samples, num_noise)
A:sklearn.linear_model.tests.test_huber.lr->LinearRegression(fit_intercept=True)
A:sklearn.linear_model.tests.test_huber.huber->HuberRegressor(fit_intercept=True, alpha=0.01, max_iter=100)
A:sklearn.linear_model.tests.test_huber.sample_weight->numpy.ones(X.shape[0])
A:sklearn.linear_model.tests.test_huber.w->numpy.random.RandomState(1).randn(n_features)
A:sklearn.linear_model.tests.test_huber.w[-1]->numpy.abs(w[-1])
A:sklearn.linear_model.tests.test_huber.grad_same->scipy.optimize.check_grad(loss_func, grad_func, w, X, y, 0.01, 0.1, sample_weight)
A:sklearn.linear_model.tests.test_huber.scale->max(np.mean(np.abs(huber.coef_)), np.mean(np.abs(huber.intercept_)))
A:sklearn.linear_model.tests.test_huber.X_new->numpy.vstack((X, np.vstack((X[1], X[1], X[3]))))
A:sklearn.linear_model.tests.test_huber.y_new->numpy.concatenate((y, [y[1]], [y[1]], [y[3]]))
A:sklearn.linear_model.tests.test_huber.X_csr->scipy.sparse.csr_matrix(X)
A:sklearn.linear_model.tests.test_huber.huber_sparse->HuberRegressor(fit_intercept=True, alpha=0.1)
A:sklearn.linear_model.tests.test_huber.sgdreg->SGDRegressor(alpha=0.0, loss='huber', shuffle=True, random_state=0, max_iter=10000, fit_intercept=False, epsilon=1.35, tol=None)
A:sklearn.linear_model.tests.test_huber.huber_warm->HuberRegressor(fit_intercept=True, alpha=1.0, max_iter=10000, warm_start=True, tol=0.1)
A:sklearn.linear_model.tests.test_huber.huber_warm_coef->HuberRegressor(fit_intercept=True, alpha=1.0, max_iter=10000, warm_start=True, tol=0.1).coef_.copy()
A:sklearn.linear_model.tests.test_huber.huber_score->HuberRegressor(fit_intercept=True, alpha=0.01, max_iter=100).score(X[mask], y[mask])
A:sklearn.linear_model.tests.test_huber.huber_outlier_score->HuberRegressor(fit_intercept=True, alpha=0.01, max_iter=100).score(X[~mask], y[~mask])
A:sklearn.linear_model.tests.test_huber.ridge->Ridge(fit_intercept=True, alpha=0.01)
A:sklearn.linear_model.tests.test_huber.ridge_score->Ridge(fit_intercept=True, alpha=0.01).score(X[mask], y[mask])
A:sklearn.linear_model.tests.test_huber.ridge_outlier_score->Ridge(fit_intercept=True, alpha=0.01).score(X[~mask], y[~mask])
sklearn.linear_model.tests.test_huber.make_regression_with_outliers(n_samples=50,n_features=20)
sklearn.linear_model.tests.test_huber.test_huber_and_sgd_same_results()
sklearn.linear_model.tests.test_huber.test_huber_better_r2_score()
sklearn.linear_model.tests.test_huber.test_huber_equals_lr_for_high_epsilon()
sklearn.linear_model.tests.test_huber.test_huber_gradient()
sklearn.linear_model.tests.test_huber.test_huber_sample_weights()
sklearn.linear_model.tests.test_huber.test_huber_scaling_invariant()
sklearn.linear_model.tests.test_huber.test_huber_sparse()
sklearn.linear_model.tests.test_huber.test_huber_warm_start()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/linear_model/tests/test_bayes.py----------------------------------------
A:sklearn.linear_model.tests.test_bayes.diabetes->sklearn.datasets.load_diabetes()
A:sklearn.linear_model.tests.test_bayes.clf->ARDRegression(compute_score=True)
A:sklearn.linear_model.tests.test_bayes.X->numpy.random.random((n_train, d))
A:sklearn.linear_model.tests.test_bayes.br_model->BayesianRidge(compute_score=True).fit(X, y)
A:sklearn.linear_model.tests.test_bayes.rr_model->Ridge(alpha=br_model.lambda_ / br_model.alpha_).fit(X, y)
A:sklearn.linear_model.tests.test_bayes.Y->numpy.array([1, 2, 3])
A:sklearn.linear_model.tests.test_bayes.w->numpy.array([1.0, 0.0, 1.0, -1.0, 0.0])
A:sklearn.linear_model.tests.test_bayes.X_test->numpy.random.random((n_test, d))
A:sklearn.linear_model.tests.test_bayes.y->f_noise(X, noise_mult)
A:sklearn.linear_model.tests.test_bayes.m1->BayesianRidge()
A:sklearn.linear_model.tests.test_bayes.(y_mean1, y_std1)->BayesianRidge().predict(X_test, return_std=True)
A:sklearn.linear_model.tests.test_bayes.m2->ARDRegression()
A:sklearn.linear_model.tests.test_bayes.(y_mean2, y_std2)->ARDRegression().predict(X_test, return_std=True)
sklearn.linear_model.tests.test_bayes.test_bayesian_on_diabetes()
sklearn.linear_model.tests.test_bayes.test_bayesian_ridge_parameter()
sklearn.linear_model.tests.test_bayes.test_return_std()
sklearn.linear_model.tests.test_bayes.test_toy_ard_object()
sklearn.linear_model.tests.test_bayes.test_toy_bayesian_ridge_object()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/linear_model/tests/test_omp.py----------------------------------------
A:sklearn.linear_model.tests.test_omp.(y, X, gamma)->make_sparse_coded_signal(n_targets, n_features, n_samples, n_nonzero_coefs, random_state=0)
A:sklearn.linear_model.tests.test_omp.gamma->numpy.zeros(n_features)
A:sklearn.linear_model.tests.test_omp.gamma_gram->orthogonal_mp_gram(G, Xy[:, 0], 5)
A:sklearn.linear_model.tests.test_omp.(idx,)->gamma[:, 0].nonzero()
A:sklearn.linear_model.tests.test_omp.gamma_rec->orthogonal_mp(X, y[:, 0], 5)
A:sklearn.linear_model.tests.test_omp.omp->OrthogonalMatchingPursuit(n_nonzero_coefs=n_features)
A:sklearn.linear_model.tests.test_omp.newX->rng.randn(n_samples, n_features).copy()
A:sklearn.linear_model.tests.test_omp.newy->numpy.dot(newX, gamma)
A:sklearn.linear_model.tests.test_omp.new_y->numpy.dot(X, gamma)
A:sklearn.linear_model.tests.test_omp.new_Xy->numpy.dot(X.T, new_y)
A:sklearn.linear_model.tests.test_omp.gamma_hat->orthogonal_mp(X, new_y, 2)
A:sklearn.linear_model.tests.test_omp.gamma_hat_gram->orthogonal_mp_gram(G, new_Xy, 2)
A:sklearn.linear_model.tests.test_omp.y_empty->numpy.zeros_like(y)
A:sklearn.linear_model.tests.test_omp.Xy_empty->numpy.dot(X.T, y_empty)
A:sklearn.linear_model.tests.test_omp.gamma_empty->ignore_warnings(orthogonal_mp)(X, y_empty, 1)
A:sklearn.linear_model.tests.test_omp.gamma_empty_gram->ignore_warnings(orthogonal_mp)(G, Xy_empty, 1)
A:sklearn.linear_model.tests.test_omp.path->orthogonal_mp(X, y, n_nonzero_coefs=5, return_path=True, precompute=True)
A:sklearn.linear_model.tests.test_omp.last->orthogonal_mp(X, y, n_nonzero_coefs=5, return_path=False, precompute=True)
A:sklearn.linear_model.tests.test_omp.ompcv->OrthogonalMatchingPursuitCV(normalize=True, fit_intercept=False, max_iter=10, cv=5)
A:sklearn.linear_model.tests.test_omp.rng->check_random_state(0)
A:sklearn.linear_model.tests.test_omp.X->check_random_state(0).randn(n_samples, n_features)
A:sklearn.linear_model.tests.test_omp.Y->check_random_state(0).randn(n_samples, n_targets)
A:sklearn.linear_model.tests.test_omp.lstsq->LinearRegression()
sklearn.linear_model.tests.test_omp.test_bad_input()
sklearn.linear_model.tests.test_omp.test_correct_shapes()
sklearn.linear_model.tests.test_omp.test_correct_shapes_gram()
sklearn.linear_model.tests.test_omp.test_estimator()
sklearn.linear_model.tests.test_omp.test_identical_regressors()
sklearn.linear_model.tests.test_omp.test_n_nonzero_coefs()
sklearn.linear_model.tests.test_omp.test_no_atoms()
sklearn.linear_model.tests.test_omp.test_omp_cv()
sklearn.linear_model.tests.test_omp.test_omp_path()
sklearn.linear_model.tests.test_omp.test_omp_reaches_least_squares()
sklearn.linear_model.tests.test_omp.test_omp_return_path_prop_with_gram()
sklearn.linear_model.tests.test_omp.test_perfect_signal_recovery()
sklearn.linear_model.tests.test_omp.test_swapped_regressors()
sklearn.linear_model.tests.test_omp.test_tol()
sklearn.linear_model.tests.test_omp.test_unreachable_accuracy()
sklearn.linear_model.tests.test_omp.test_with_without_gram()
sklearn.linear_model.tests.test_omp.test_with_without_gram_tol()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/linear_model/tests/test_sparse_coordinate_descent.py----------------------------------------
A:sklearn.linear_model.tests.test_sparse_coordinate_descent.clf->ElasticNetCV(n_alphas=n_alphas, eps=0.001, max_iter=max_iter, l1_ratio=0.5, fit_intercept=False)
A:sklearn.linear_model.tests.test_sparse_coordinate_descent.X->scipy.sparse.csc_matrix(X)
A:sklearn.linear_model.tests.test_sparse_coordinate_descent.clf_dense->ElasticNet(fit_intercept=True, normalize=True)
A:sklearn.linear_model.tests.test_sparse_coordinate_descent.clf_sparse->ElasticNet(fit_intercept=True, normalize=True)
A:sklearn.linear_model.tests.test_sparse_coordinate_descent.T->scipy.sparse.lil_matrix((3, 1))
A:sklearn.linear_model.tests.test_sparse_coordinate_descent.pred->ElasticNetCV(n_alphas=n_alphas, eps=0.001, max_iter=max_iter, l1_ratio=0.5, fit_intercept=False).predict(T)
A:sklearn.linear_model.tests.test_sparse_coordinate_descent.random_state->numpy.random.RandomState(seed)
A:sklearn.linear_model.tests.test_sparse_coordinate_descent.w->numpy.abs(w)
A:sklearn.linear_model.tests.test_sparse_coordinate_descent.rnd->numpy.random.RandomState(seed).uniform(size=(n_samples, n_features))
A:sklearn.linear_model.tests.test_sparse_coordinate_descent.y->numpy.ravel(y)
A:sklearn.linear_model.tests.test_sparse_coordinate_descent.(X, y)->make_sparse_data(n_samples=40, n_features=10)
A:sklearn.linear_model.tests.test_sparse_coordinate_descent.s_clf->Lasso(alpha=0.1, fit_intercept=False, max_iter=max_iter, tol=1e-07)
A:sklearn.linear_model.tests.test_sparse_coordinate_descent.d_clf->Lasso(alpha=0.1, fit_intercept=False, max_iter=max_iter, tol=1e-07)
A:sklearn.linear_model.tests.test_sparse_coordinate_descent.estimator->ElasticNet(alpha=0.01, fit_intercept=True, precompute=None)
A:sklearn.linear_model.tests.test_sparse_coordinate_descent.clfs->LassoCV(max_iter=100, cv=4, normalize=normalize)
A:sklearn.linear_model.tests.test_sparse_coordinate_descent.clfd->LassoCV(max_iter=100, cv=4, normalize=normalize)
A:sklearn.linear_model.tests.test_sparse_coordinate_descent.l->ElasticNet(normalize=normalize)
A:sklearn.linear_model.tests.test_sparse_coordinate_descent.sample->numpy.array([1, 2, 3, 4, 5]).reshape(1, -1)
A:sklearn.linear_model.tests.test_sparse_coordinate_descent.predict_dense->ElasticNet(normalize=normalize).predict(sample)
A:sklearn.linear_model.tests.test_sparse_coordinate_descent.l_sp->ElasticNet(normalize=normalize)
A:sklearn.linear_model.tests.test_sparse_coordinate_descent.X_sp->scipy.sparse.coo_matrix(X)
A:sklearn.linear_model.tests.test_sparse_coordinate_descent.sample_sparse->scipy.sparse.coo_matrix(sample)
A:sklearn.linear_model.tests.test_sparse_coordinate_descent.predict_sparse->ElasticNet(normalize=normalize).predict(sample_sparse)
sklearn.linear_model.tests.test_sparse_coordinate_descent._test_sparse_enet_not_as_toy_dataset(alpha,fit_intercept,positive)
sklearn.linear_model.tests.test_sparse_coordinate_descent.make_sparse_data(n_samples=100,n_features=100,n_informative=10,seed=42,positive=False,n_targets=1)
sklearn.linear_model.tests.test_sparse_coordinate_descent.test_enet_multitarget()
sklearn.linear_model.tests.test_sparse_coordinate_descent.test_enet_toy_explicit_sparse_input()
sklearn.linear_model.tests.test_sparse_coordinate_descent.test_enet_toy_list_input()
sklearn.linear_model.tests.test_sparse_coordinate_descent.test_lasso_zero()
sklearn.linear_model.tests.test_sparse_coordinate_descent.test_normalize_option()
sklearn.linear_model.tests.test_sparse_coordinate_descent.test_path_parameters()
sklearn.linear_model.tests.test_sparse_coordinate_descent.test_same_multiple_output_sparse_dense()
sklearn.linear_model.tests.test_sparse_coordinate_descent.test_same_output_sparse_dense_lasso_and_enet_cv()
sklearn.linear_model.tests.test_sparse_coordinate_descent.test_sparse_coef()
sklearn.linear_model.tests.test_sparse_coordinate_descent.test_sparse_enet_not_as_toy_dataset()
sklearn.linear_model.tests.test_sparse_coordinate_descent.test_sparse_lasso_not_as_toy_dataset()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/linear_model/tests/test_sag.py----------------------------------------
A:sklearn.linear_model.tests.test_sag.iris->load_iris()
A:sklearn.linear_model.tests.test_sag.w->check_random_state(42).normal(size=n_features)
A:sklearn.linear_model.tests.test_sag.pred->numpy.dot(myX, w)
A:sklearn.linear_model.tests.test_sag.p->loss(pred, myy)
A:sklearn.linear_model.tests.test_sag.weights->numpy.array([[0.1, 0.2, 0.3], [1.1, 1.2, -1.3]])
A:sklearn.linear_model.tests.test_sag.sum_gradient->numpy.zeros(n_features)
A:sklearn.linear_model.tests.test_sag.gradient_memory->numpy.zeros(n_samples)
A:sklearn.linear_model.tests.test_sag.intercept_gradient_memory->numpy.zeros(n_samples)
A:sklearn.linear_model.tests.test_sag.rng->check_random_state(42)
A:sklearn.linear_model.tests.test_sag.seen->set()
A:sklearn.linear_model.tests.test_sag.idx->int(rng.rand(1) * n_samples)
A:sklearn.linear_model.tests.test_sag.gradient->dloss(p, y[idx])
A:sklearn.linear_model.tests.test_sag.last_updated->numpy.zeros(n_features, dtype=np.int)
A:sklearn.linear_model.tests.test_sag.c_sum->numpy.zeros(n_iter * n_samples)
A:sklearn.linear_model.tests.test_sag.(X, y)->make_blobs(n_samples=n_samples, centers=3, random_state=0, cluster_std=0.1)
A:sklearn.linear_model.tests.test_sag.step_size->get_step_size(X, alpha, fit_intercept, classification=True)
A:sklearn.linear_model.tests.test_sag.clf->Ridge(fit_intercept=fit_intercept, tol=1e-11, solver='sag', alpha=alpha * n_samples, max_iter=n_iter)
A:sklearn.linear_model.tests.test_sag.(weights, intercept)->sag_sparse(X, y, step_size, alpha, n_iter=n_iter, dloss=log_dloss, fit_intercept=fit_intercept, saga=solver == 'saga')
A:sklearn.linear_model.tests.test_sag.(weights2, intercept2)->sag(X, y, step_size, alpha, n_iter=n_iter, dloss=squared_dloss, fit_intercept=fit_intercept)
A:sklearn.linear_model.tests.test_sag.intercept->numpy.array([1.0, 0, -0.2])
A:sklearn.linear_model.tests.test_sag.weights2->numpy.atleast_2d(weights2)
A:sklearn.linear_model.tests.test_sag.intercept2->numpy.array(intercept2)
A:sklearn.linear_model.tests.test_sag.X->numpy.array([[1.1, 2.2], [2.2, -4.4], [3.3, -2.2], [1.1, 1.1]])
A:sklearn.linear_model.tests.test_sag.true_w->check_random_state(42).normal(size=n_features)
A:sklearn.linear_model.tests.test_sag.y->numpy.array([0, 1, 2, 0])
A:sklearn.linear_model.tests.test_sag.(weights1, intercept1)->sag_sparse(X, y, step_size, alpha, n_iter=n_iter, dloss=squared_dloss, fit_intercept=fit_intercept)
A:sklearn.linear_model.tests.test_sag.clf1->LogisticRegression(solver='sag', C=1.0 / alpha, fit_intercept=fit_intercept)
A:sklearn.linear_model.tests.test_sag.clf2->Ridge(fit_intercept=fit_intercept, solver='sag', alpha=alpha)
A:sklearn.linear_model.tests.test_sag.clf3->Ridge(fit_intercept=fit_intercept, tol=1e-05, solver='lsqr', alpha=alpha, max_iter=n_iter, random_state=42)
A:sklearn.linear_model.tests.test_sag.pobj1->get_pobj(clf1.coef_, alpha, X, y, squared_loss)
A:sklearn.linear_model.tests.test_sag.pobj2->get_pobj(clf2.coef_, alpha, X, y, squared_loss)
A:sklearn.linear_model.tests.test_sag.pobj3->get_pobj(clf3.coef_, alpha, X, y, squared_loss)
A:sklearn.linear_model.tests.test_sag.(spweights1, spintercept1)->sag_sparse(X, y_encoded, step_size, alpha, n_iter=max_iter, dloss=log_dloss, sample_weight=sample_weight)
A:sklearn.linear_model.tests.test_sag.(spweights2, spintercept2)->sag_sparse(X, y_encoded, step_size, alpha, n_iter=max_iter, dloss=log_dloss, sample_weight=sample_weight, sparse=True)
A:sklearn.linear_model.tests.test_sag.max_squared_sum_->row_norms(X, squared=True).max()
A:sklearn.linear_model.tests.test_sag.mun_sqr->min(2 * n_samples * alpha, L_sqr)
A:sklearn.linear_model.tests.test_sag.mun_log->min(2 * n_samples * alpha, L_log)
A:sklearn.linear_model.tests.test_sag.step_size_sqr_->get_auto_step_size(max_squared_sum_, alpha, 'squared', fit_intercept, n_samples=n_samples, is_saga=saga)
A:sklearn.linear_model.tests.test_sag.step_size_log_->get_auto_step_size(max_squared_sum_, alpha, 'log', fit_intercept, n_samples=n_samples, is_saga=saga)
A:sklearn.linear_model.tests.test_sag.score1->LogisticRegression(solver='sag', C=1.0 / alpha, fit_intercept=fit_intercept).score(X, y)
A:sklearn.linear_model.tests.test_sag.score2->Ridge(fit_intercept=fit_intercept, solver='sag', alpha=alpha).score(X, y)
A:sklearn.linear_model.tests.test_sag.classes->numpy.unique(y)
A:sklearn.linear_model.tests.test_sag.y_tmp->numpy.ones(n_samples)
A:sklearn.linear_model.tests.test_sag.(spweights, spintercept)->sag_sparse(X, y, step_size, alpha, n_iter=n_iter, dloss=log_dloss, sample_weight=sample_weight, fit_intercept=fit_intercept)
A:sklearn.linear_model.tests.test_sag.y_encoded->numpy.ones(n_samples)
A:sklearn.linear_model.tests.test_sag.coef1->numpy.vstack(coef1)
A:sklearn.linear_model.tests.test_sag.intercept1->numpy.array(intercept1)
A:sklearn.linear_model.tests.test_sag.coef2->numpy.vstack(coef2)
A:sklearn.linear_model.tests.test_sag.pred1->LogisticRegression(solver='sag', C=1.0 / alpha, fit_intercept=fit_intercept).predict(X)
A:sklearn.linear_model.tests.test_sag.pred2->Ridge(fit_intercept=fit_intercept, solver='sag', alpha=alpha).predict(X)
A:sklearn.linear_model.tests.test_sag.le->LabelEncoder()
A:sklearn.linear_model.tests.test_sag.class_weight_->compute_class_weight(class_weight, np.unique(y), y)
A:sklearn.linear_model.tests.test_sag.n_classes->len(np.unique(y))
A:sklearn.linear_model.tests.test_sag.sample_weights->numpy.array([0.8, 1, 1, 0.8])
A:sklearn.linear_model.tests.test_sag.(dataset, _)->make_dataset(X, y, sample_weights, random_state=42)
A:sklearn.linear_model.tests.test_sag.(loss_1, grad_1)->_multinomial_grad_loss_all_samples(dataset, weights, intercept, n_samples, n_features, n_classes)
A:sklearn.linear_model.tests.test_sag.lbin->LabelBinarizer()
A:sklearn.linear_model.tests.test_sag.Y_bin->LabelBinarizer().fit_transform(y)
A:sklearn.linear_model.tests.test_sag.weights_intercept->numpy.vstack((weights, intercept)).T.ravel()
A:sklearn.linear_model.tests.test_sag.(loss_2, grad_2, _)->_multinomial_loss_grad(weights_intercept, X, Y_bin, 0.0, sample_weights)
A:sklearn.linear_model.tests.test_sag.grad_2->grad_2.reshape(n_classes, -1).reshape(n_classes, -1)
A:sklearn.linear_model.tests.test_sag.logsumexp_prediction->logsumexp(prediction, axis=1)
A:sklearn.linear_model.tests.test_sag.grad_1->numpy.dot(X.T, diff)
A:sklearn.linear_model.tests.test_sag.grad_gt->numpy.array([[-0.557487, -1.619151, +2.176638], [-0.903942, +5.258745, -4.354803]])
sklearn.linear_model.tests.test_sag.get_pobj(w,alpha,myX,myy,loss)
sklearn.linear_model.tests.test_sag.get_step_size(X,alpha,fit_intercept,classification=True)
sklearn.linear_model.tests.test_sag.log_dloss(p,y)
sklearn.linear_model.tests.test_sag.log_loss(p,y)
sklearn.linear_model.tests.test_sag.sag(X,y,step_size,alpha,n_iter=1,dloss=None,sparse=False,sample_weight=None,fit_intercept=True,saga=False)
sklearn.linear_model.tests.test_sag.sag_sparse(X,y,step_size,alpha,n_iter=1,dloss=None,sample_weight=None,sparse=False,fit_intercept=True,saga=False)
sklearn.linear_model.tests.test_sag.squared_dloss(p,y)
sklearn.linear_model.tests.test_sag.squared_loss(p,y)
sklearn.linear_model.tests.test_sag.test_binary_classifier_class_weight()
sklearn.linear_model.tests.test_sag.test_classifier_matching()
sklearn.linear_model.tests.test_sag.test_classifier_results()
sklearn.linear_model.tests.test_sag.test_classifier_single_class()
sklearn.linear_model.tests.test_sag.test_get_auto_step_size()
sklearn.linear_model.tests.test_sag.test_multiclass_classifier_class_weight()
sklearn.linear_model.tests.test_sag.test_multinomial_loss()
sklearn.linear_model.tests.test_sag.test_multinomial_loss_ground_truth()
sklearn.linear_model.tests.test_sag.test_regressor_matching()
sklearn.linear_model.tests.test_sag.test_sag_classifier_computed_correctly()
sklearn.linear_model.tests.test_sag.test_sag_multiclass_computed_correctly()
sklearn.linear_model.tests.test_sag.test_sag_pobj_matches_logistic_regression()
sklearn.linear_model.tests.test_sag.test_sag_pobj_matches_ridge_regression()
sklearn.linear_model.tests.test_sag.test_sag_regressor()
sklearn.linear_model.tests.test_sag.test_sag_regressor_computed_correctly()
sklearn.linear_model.tests.test_sag.test_step_size_alpha_error()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/linear_model/tests/test_coordinate_descent.py----------------------------------------
A:sklearn.linear_model.tests.test_coordinate_descent.clf->ElasticNet(alpha=0.5, max_iter=100, precompute=False, fit_intercept=fit_intercept, normalize=normalize)
A:sklearn.linear_model.tests.test_coordinate_descent.pred->ElasticNet(alpha=0.5, max_iter=100, precompute=False, fit_intercept=fit_intercept, normalize=normalize).predict(T)
A:sklearn.linear_model.tests.test_coordinate_descent.X->dtype(X)
A:sklearn.linear_model.tests.test_coordinate_descent.random_state->numpy.random.RandomState(0)
A:sklearn.linear_model.tests.test_coordinate_descent.w->numpy.random.RandomState(0).randn(n_features)
A:sklearn.linear_model.tests.test_coordinate_descent.y->numpy.array([12, 10, 11, 21, 5])
A:sklearn.linear_model.tests.test_coordinate_descent.X_test->numpy.random.RandomState(0).random_sample(size=(10, 3))
A:sklearn.linear_model.tests.test_coordinate_descent.y_test->numpy.dot(X_test, w)
A:sklearn.linear_model.tests.test_coordinate_descent.(X, y, X_test, y_test)->build_dataset(n_samples=20, n_features=10)
A:sklearn.linear_model.tests.test_coordinate_descent.lars->LassoLarsCV(normalize=False, max_iter=30).fit(X, y)
A:sklearn.linear_model.tests.test_coordinate_descent.mse_lars->scipy.interpolate.interp1d(lars.cv_alphas_, lars.mse_path_.T)
A:sklearn.linear_model.tests.test_coordinate_descent.diabetes->sklearn.datasets.load_diabetes()
A:sklearn.linear_model.tests.test_coordinate_descent.pipe->make_pipeline(StandardScaler(), LassoCV(cv=StratifiedKFold(n_splits=5)))
A:sklearn.linear_model.tests.test_coordinate_descent.clf_unconstrained->LassoCV(n_alphas=3, eps=0.1, max_iter=max_iter, cv=2, n_jobs=1)
A:sklearn.linear_model.tests.test_coordinate_descent.clf_constrained->LassoCV(n_alphas=3, eps=0.1, max_iter=max_iter, positive=True, cv=2, n_jobs=1)
A:sklearn.linear_model.tests.test_coordinate_descent.(alphas_lars, _, coef_path_lars)->lars_path(X, y, method='lasso')
A:sklearn.linear_model.tests.test_coordinate_descent.coef_path_cont_lars->scipy.interpolate.interp1d(alphas_lars[::-1], coef_path_lars[:, ::-1])
A:sklearn.linear_model.tests.test_coordinate_descent.(alphas_lasso2, coef_path_lasso2, _)->lasso_path(X, y, alphas=alphas, return_models=False)
A:sklearn.linear_model.tests.test_coordinate_descent.coef_path_cont_lasso->scipy.interpolate.interp1d(alphas_lasso2[::-1], coef_path_lasso2[:, ::-1])
A:sklearn.linear_model.tests.test_coordinate_descent.(X, y, _, _)->build_dataset(n_samples=20, n_features=10)
A:sklearn.linear_model.tests.test_coordinate_descent.clf1->LassoCV(n_alphas=5)
A:sklearn.linear_model.tests.test_coordinate_descent.clf2->ElasticNet(alpha=0.1, max_iter=10)
A:sklearn.linear_model.tests.test_coordinate_descent.lasso->LassoCV(fit_intercept=True, n_alphas=3)
A:sklearn.linear_model.tests.test_coordinate_descent.enet->ElasticNetCV(fit_intercept=True, n_alphas=3)
A:sklearn.linear_model.tests.test_coordinate_descent.enetcv_unconstrained->ElasticNetCV(n_alphas=3, eps=0.1, max_iter=max_iter, cv=2, n_jobs=1)
A:sklearn.linear_model.tests.test_coordinate_descent.enetcv_constrained->ElasticNetCV(n_alphas=3, eps=0.1, max_iter=max_iter, cv=2, positive=True, n_jobs=1)
A:sklearn.linear_model.tests.test_coordinate_descent.m_enet->MultiTaskElasticNetCV(fit_intercept=True, n_alphas=3)
A:sklearn.linear_model.tests.test_coordinate_descent.m_lasso->MultiTaskLassoCV(fit_intercept=True, n_alphas=3)
A:sklearn.linear_model.tests.test_coordinate_descent.rng->numpy.random.RandomState(0)
A:sklearn.linear_model.tests.test_coordinate_descent.X_train->numpy.random.RandomState(0).random_sample(size=(10, 3))
A:sklearn.linear_model.tests.test_coordinate_descent.y1->numpy.empty(10)
A:sklearn.linear_model.tests.test_coordinate_descent.y2->numpy.empty((10, 2))
A:sklearn.linear_model.tests.test_coordinate_descent.Y->numpy.array([-1, 0, 1])
A:sklearn.linear_model.tests.test_coordinate_descent.T->numpy.array([[2], [3], [4]])
A:sklearn.linear_model.tests.test_coordinate_descent.estimator->ElasticNet(alpha=0.01, fit_intercept=True)
A:sklearn.linear_model.tests.test_coordinate_descent.model->ElasticNet(alpha=0.001, tol=0.001).fit(X, y)
A:sklearn.linear_model.tests.test_coordinate_descent.boston->load_boston()
A:sklearn.linear_model.tests.test_coordinate_descent.low_reg_model->ElasticNet(alpha=final_alpha).fit(X, y)
A:sklearn.linear_model.tests.test_coordinate_descent.high_reg_model->ElasticNet(alpha=final_alpha * 10).fit(X, y)
A:sklearn.linear_model.tests.test_coordinate_descent.warm_low_reg_model->deepcopy(high_reg_model)
A:sklearn.linear_model.tests.test_coordinate_descent.clf_cyclic->MultiTaskElasticNet(selection='cyclic', tol=1e-08)
A:sklearn.linear_model.tests.test_coordinate_descent.clf_random->ElasticNet(selection='invalid')
A:sklearn.linear_model.tests.test_coordinate_descent.new_y->numpy.hstack((y[:, np.newaxis], y[:, np.newaxis]))
A:sklearn.linear_model.tests.test_coordinate_descent.(X, Y, _, _)->build_dataset(n_samples=50, n_features=50, n_targets=2)
A:sklearn.linear_model.tests.test_coordinate_descent.csr->scipy.sparse.csr_matrix(X)
A:sklearn.linear_model.tests.test_coordinate_descent.(_, coefs, _)->path(X, y, fit_intercept=False)
A:sklearn.linear_model.tests.test_coordinate_descent.(_, sparse_coefs, _)->path(csr, y, fit_intercept=False)
A:sklearn.linear_model.tests.test_coordinate_descent.Gram->dtype(X).T.dot(X)
A:sklearn.linear_model.tests.test_coordinate_descent.clf_float->model(fit_intercept=False)
A:sklearn.linear_model.tests.test_coordinate_descent.clf_precompute->ElasticNet(alpha=0.5, max_iter=100, precompute=Gram, fit_intercept=fit_intercept, normalize=normalize)
A:sklearn.linear_model.tests.test_coordinate_descent.multi_y->numpy.hstack((y[:, np.newaxis], y[:, np.newaxis]))
A:sklearn.linear_model.tests.test_coordinate_descent.clf_multioutput->MultiTaskElasticNet(alpha=0.5, max_iter=100, fit_intercept=fit_intercept, normalize=normalize)
A:sklearn.linear_model.tests.test_coordinate_descent.est_desired->MultiTaskElasticNetCV(l1_ratio=1e-05, **estkwds)
A:sklearn.linear_model.tests.test_coordinate_descent.est->MultiTaskElasticNetCV(l1_ratio=0, **estkwds)
sklearn.linear_model.tests.test_coordinate_descent.build_dataset(n_samples=50,n_features=200,n_informative_features=10,n_targets=1)
sklearn.linear_model.tests.test_coordinate_descent.test_1d_multioutput_enet_and_multitask_enet_cv()
sklearn.linear_model.tests.test_coordinate_descent.test_1d_multioutput_lasso_and_multitask_lasso_cv()
sklearn.linear_model.tests.test_coordinate_descent.test_check_input_false()
sklearn.linear_model.tests.test_coordinate_descent.test_enet_cv_positive_constraint()
sklearn.linear_model.tests.test_coordinate_descent.test_enet_float_precision()
sklearn.linear_model.tests.test_coordinate_descent.test_enet_l1_ratio()
sklearn.linear_model.tests.test_coordinate_descent.test_enet_multitarget()
sklearn.linear_model.tests.test_coordinate_descent.test_enet_path()
sklearn.linear_model.tests.test_coordinate_descent.test_enet_path_positive()
sklearn.linear_model.tests.test_coordinate_descent.test_enet_positive_constraint()
sklearn.linear_model.tests.test_coordinate_descent.test_enet_toy()
sklearn.linear_model.tests.test_coordinate_descent.test_lasso_alpha_warning()
sklearn.linear_model.tests.test_coordinate_descent.test_lasso_cv()
sklearn.linear_model.tests.test_coordinate_descent.test_lasso_cv_positive_constraint()
sklearn.linear_model.tests.test_coordinate_descent.test_lasso_cv_with_some_model_selection()
sklearn.linear_model.tests.test_coordinate_descent.test_lasso_non_float_y()
sklearn.linear_model.tests.test_coordinate_descent.test_lasso_path_return_models_vs_new_return_gives_same_coefficients()
sklearn.linear_model.tests.test_coordinate_descent.test_lasso_positive_constraint()
sklearn.linear_model.tests.test_coordinate_descent.test_lasso_readonly_data()
sklearn.linear_model.tests.test_coordinate_descent.test_lasso_toy()
sklearn.linear_model.tests.test_coordinate_descent.test_lasso_zero()
sklearn.linear_model.tests.test_coordinate_descent.test_multi_task_lasso_and_enet()
sklearn.linear_model.tests.test_coordinate_descent.test_multi_task_lasso_readonly_data()
sklearn.linear_model.tests.test_coordinate_descent.test_multioutput_enetcv_error()
sklearn.linear_model.tests.test_coordinate_descent.test_multitask_enet_and_lasso_cv()
sklearn.linear_model.tests.test_coordinate_descent.test_overrided_gram_matrix()
sklearn.linear_model.tests.test_coordinate_descent.test_path_parameters()
sklearn.linear_model.tests.test_coordinate_descent.test_precompute_invalid_argument()
sklearn.linear_model.tests.test_coordinate_descent.test_random_descent()
sklearn.linear_model.tests.test_coordinate_descent.test_sparse_dense_descent_paths()
sklearn.linear_model.tests.test_coordinate_descent.test_sparse_input_dtype_enet_and_lassocv()
sklearn.linear_model.tests.test_coordinate_descent.test_uniform_targets()
sklearn.linear_model.tests.test_coordinate_descent.test_warm_start()
sklearn.linear_model.tests.test_coordinate_descent.test_warm_start_convergence()
sklearn.linear_model.tests.test_coordinate_descent.test_warm_start_convergence_with_regularizer_decrement()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/linear_model/tests/test_sgd.py----------------------------------------
A:sklearn.linear_model.tests.test_sgd.X->StandardScaler().fit_transform(iris.data)
A:sklearn.linear_model.tests.test_sgd.T->numpy.array([[-1, -1], [2, 2], [3, 2]])
A:sklearn.linear_model.tests.test_sgd.X2->numpy.array([[-1, 1], [-0.75, 0.5], [-1.5, 1.5], [1, 1], [0.75, 0.5], [1.5, 1.5], [-1, -1], [0, -0.5], [1, -1]])
A:sklearn.linear_model.tests.test_sgd.T2->numpy.array([[-1.5, 0.5], [1, 2], [0, -2]])
A:sklearn.linear_model.tests.test_sgd.X3->numpy.array([[1, 1, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0], [0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 1, 1], [0, 0, 0, 1, 0, 0], [0, 0, 0, 1, 0, 0]])
A:sklearn.linear_model.tests.test_sgd.Y3->numpy.array([1, 1, 1, 1, 2, 2, 2, 2])
A:sklearn.linear_model.tests.test_sgd.X4->numpy.array([[1, 0.9, 0.8, 0, 0, 0], [1, 0.84, 0.98, 0, 0, 0], [1, 0.96, 0.88, 0, 0, 0], [1, 0.91, 0.99, 0, 0, 0], [0, 0, 0, 0.89, 0.91, 1], [0, 0, 0, 0.79, 0.84, 1], [0, 0, 0, 0.91, 0.95, 1], [0, 0, 0, 0.93, 1, 1]])
A:sklearn.linear_model.tests.test_sgd.Y4->numpy.array([1, 1, 1, 1, 2, 2, 2, 2])
A:sklearn.linear_model.tests.test_sgd.iris->sklearn.datasets.load_iris()
A:sklearn.linear_model.tests.test_sgd.X5->numpy.array([[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1]])
A:sklearn.linear_model.tests.test_sgd.weights->numpy.zeros(X.shape[1])
A:sklearn.linear_model.tests.test_sgd.average_weights->average_weights.reshape(1, -1).reshape(1, -1)
A:sklearn.linear_model.tests.test_sgd.p->self.factory(epsilon=0.9).predict_proba([x])
A:sklearn.linear_model.tests.test_sgd.clf->self.factory(epsilon=0.9)
A:sklearn.linear_model.tests.test_sgd.clf2->self.factory(alpha=0.1, max_iter=20)
A:sklearn.linear_model.tests.test_sgd.clf3->self.factory(alpha=0.01, eta0=0.01, shuffle=False, warm_start=True, learning_rate=lr)
A:sklearn.linear_model.tests.test_sgd.clf1->self.factory(alpha=0.1, max_iter=20, class_weight=class_weights)
A:sklearn.linear_model.tests.test_sgd.Y_encode->numpy.array(Y)
A:sklearn.linear_model.tests.test_sgd.(average_weights, average_intercept)->self.asgd(X3, Y3, eta, alpha)
A:sklearn.linear_model.tests.test_sgd.rng->numpy.random.RandomState(0)
A:sklearn.linear_model.tests.test_sgd.w->numpy.random.RandomState(0).normal(size=n_features)
A:sklearn.linear_model.tests.test_sgd.y->(np.dot(X_scaled, ground_truth) > 0.0).astype(np.int32)
A:sklearn.linear_model.tests.test_sgd.pred->self.factory(epsilon=0.9).predict(X)
A:sklearn.linear_model.tests.test_sgd.np_Y2->numpy.array(Y2)
A:sklearn.linear_model.tests.test_sgd.classes->numpy.unique(Y_)
A:sklearn.linear_model.tests.test_sgd.y_i->numpy.ones(np_Y2.shape[0])
A:sklearn.linear_model.tests.test_sgd.(average_coef, average_intercept)->self.asgd(X2, y_i, eta, alpha)
A:sklearn.linear_model.tests.test_sgd.d->self.factory(epsilon=0.9).decision_function([x])
A:sklearn.linear_model.tests.test_sgd.l->self.factory(epsilon=0.9).predict_log_proba([[-1, -1]])
A:sklearn.linear_model.tests.test_sgd.x->StandardScaler().fit_transform(iris.data).mean(axis=0)
A:sklearn.linear_model.tests.test_sgd.n->len(X4)
A:sklearn.linear_model.tests.test_sgd.idx->numpy.arange(X.shape[0])
A:sklearn.linear_model.tests.test_sgd.clf_weighted->self.factory(alpha=0.1, max_iter=1000, class_weight={0: 0.5, 1: 0.5})
A:sklearn.linear_model.tests.test_sgd.sample_weights->numpy.random.RandomState(0).random_sample(Y4.shape[0])
A:sklearn.linear_model.tests.test_sgd.multiplied_together->numpy.copy(sample_weights)
A:sklearn.linear_model.tests.test_sgd.f1->sklearn.metrics.f1_score(y, clf_balanced.predict(X), average='weighted')
A:sklearn.linear_model.tests.test_sgd.clf_balanced->self.factory(alpha=0.0001, max_iter=1000, class_weight='balanced', shuffle=False).fit(X, y)
A:sklearn.linear_model.tests.test_sgd.X_imbalanced->numpy.vstack([X] + [X_0] * 10)
A:sklearn.linear_model.tests.test_sgd.y_imbalanced->numpy.concatenate([y] + [y_0] * 10)
A:sklearn.linear_model.tests.test_sgd.y_pred->self.factory(epsilon=0.9).predict(T)
A:sklearn.linear_model.tests.test_sgd.id1->id(clf.coef_.data)
A:sklearn.linear_model.tests.test_sgd.id2->id(clf.coef_.data)
A:sklearn.linear_model.tests.test_sgd.y_pred2->self.factory(epsilon=0.9).predict(T)
A:sklearn.linear_model.tests.test_sgd.score->self.factory(epsilon=0.9).score(X, y)
A:sklearn.linear_model.tests.test_sgd.ground_truth_coef->numpy.random.RandomState(0).randn(n_features)
A:sklearn.linear_model.tests.test_sgd.cd->sklearn.linear_model.ElasticNet(alpha=alpha, l1_ratio=l1_ratio, fit_intercept=False)
A:sklearn.linear_model.tests.test_sgd.sgd->SGDClassifier(max_iter=max_iter, tol=tol, n_iter=n_iter)
A:sklearn.linear_model.tests.test_sgd.(X, y)->sklearn.datasets.make_classification(n_samples=1000, n_features=100, n_informative=20, random_state=1234)
A:sklearn.linear_model.tests.test_sgd.est_en->SGDClassifier(alpha=0.001, penalty='elasticnet', tol=None, max_iter=6, l1_ratio=1e-10, random_state=42).fit(X, y)
A:sklearn.linear_model.tests.test_sgd.est_l1->SGDClassifier(alpha=0.001, penalty='l1', max_iter=6, random_state=42, tol=None).fit(X, y)
A:sklearn.linear_model.tests.test_sgd.est_l2->SGDClassifier(alpha=0.001, penalty='l2', max_iter=6, random_state=42, tol=None).fit(X, y)
A:sklearn.linear_model.tests.test_sgd.X_scaled->MinMaxScaler().fit_transform(X)
A:sklearn.linear_model.tests.test_sgd.ground_truth->numpy.random.RandomState(0).normal(size=n_features)
A:sklearn.linear_model.tests.test_sgd.model->SGDClassifier(alpha=100000.0, learning_rate='constant', eta0=0.1, penalty=penalty, shuffle=False, tol=None, max_iter=6)
A:sklearn.linear_model.tests.test_sgd.model_0->SGDClassifier(tol=None, random_state=0, max_iter=max_iter)
A:sklearn.linear_model.tests.test_sgd.model_1->SGDClassifier(tol=0, random_state=0, max_iter=max_iter)
A:sklearn.linear_model.tests.test_sgd.model_2->SGDClassifier(tol=0.1, random_state=0, max_iter=max_iter)
A:sklearn.linear_model.tests.test_sgd.model_3->assert_warns(ConvergenceWarning, model_3.fit, X, y)
A:sklearn.linear_model.tests.test_sgd.est->SGDClassifier(max_iter=42, tol=0.01)
A:sklearn.linear_model.tests.test_sgd.loss->sklearn.linear_model.sgd_fast.SquaredEpsilonInsensitive(0.1)
sklearn.linear_model.tests.test_sgd.CommonTest(object)
sklearn.linear_model.tests.test_sgd.CommonTest._test_warm_start(self,X,Y,lr)
sklearn.linear_model.tests.test_sgd.CommonTest.asgd(self,X,y,eta,alpha,weight_init=None,intercept_init=0.0)
sklearn.linear_model.tests.test_sgd.CommonTest.factory(self,**kwargs)
sklearn.linear_model.tests.test_sgd.CommonTest.test_clone(self)
sklearn.linear_model.tests.test_sgd.CommonTest.test_input_format(self)
sklearn.linear_model.tests.test_sgd.CommonTest.test_late_onset_averaging_not_reached(self)
sklearn.linear_model.tests.test_sgd.CommonTest.test_late_onset_averaging_reached(self)
sklearn.linear_model.tests.test_sgd.CommonTest.test_plain_has_no_average_attr(self)
sklearn.linear_model.tests.test_sgd.CommonTest.test_sgd_bad_alpha_for_optimal_learning_rate(self)
sklearn.linear_model.tests.test_sgd.CommonTest.test_warm_start_constant(self)
sklearn.linear_model.tests.test_sgd.CommonTest.test_warm_start_invscaling(self)
sklearn.linear_model.tests.test_sgd.CommonTest.test_warm_start_optimal(self)
sklearn.linear_model.tests.test_sgd.DenseSGDClassifierTestCase(unittest.TestCase,CommonTest)
sklearn.linear_model.tests.test_sgd.DenseSGDClassifierTestCase._test_partial_fit_equal_fit(self,lr)
sklearn.linear_model.tests.test_sgd.DenseSGDClassifierTestCase.test_argument_coef(self)
sklearn.linear_model.tests.test_sgd.DenseSGDClassifierTestCase.test_average_binary_computed_correctly(self)
sklearn.linear_model.tests.test_sgd.DenseSGDClassifierTestCase.test_balanced_weight(self)
sklearn.linear_model.tests.test_sgd.DenseSGDClassifierTestCase.test_class_weights(self)
sklearn.linear_model.tests.test_sgd.DenseSGDClassifierTestCase.test_equal_class_weight(self)
sklearn.linear_model.tests.test_sgd.DenseSGDClassifierTestCase.test_fit_then_partial_fit(self)
sklearn.linear_model.tests.test_sgd.DenseSGDClassifierTestCase.test_multiple_fit(self)
sklearn.linear_model.tests.test_sgd.DenseSGDClassifierTestCase.test_partial_fit_binary(self)
sklearn.linear_model.tests.test_sgd.DenseSGDClassifierTestCase.test_partial_fit_equal_fit_constant(self)
sklearn.linear_model.tests.test_sgd.DenseSGDClassifierTestCase.test_partial_fit_equal_fit_invscaling(self)
sklearn.linear_model.tests.test_sgd.DenseSGDClassifierTestCase.test_partial_fit_equal_fit_optimal(self)
sklearn.linear_model.tests.test_sgd.DenseSGDClassifierTestCase.test_partial_fit_exception(self)
sklearn.linear_model.tests.test_sgd.DenseSGDClassifierTestCase.test_partial_fit_multiclass(self)
sklearn.linear_model.tests.test_sgd.DenseSGDClassifierTestCase.test_partial_fit_multiclass_average(self)
sklearn.linear_model.tests.test_sgd.DenseSGDClassifierTestCase.test_partial_fit_weight_class_balanced(self)
sklearn.linear_model.tests.test_sgd.DenseSGDClassifierTestCase.test_provide_coef(self)
sklearn.linear_model.tests.test_sgd.DenseSGDClassifierTestCase.test_regression_losses(self)
sklearn.linear_model.tests.test_sgd.DenseSGDClassifierTestCase.test_sample_weights(self)
sklearn.linear_model.tests.test_sgd.DenseSGDClassifierTestCase.test_set_coef_multiclass(self)
sklearn.linear_model.tests.test_sgd.DenseSGDClassifierTestCase.test_set_intercept(self)
sklearn.linear_model.tests.test_sgd.DenseSGDClassifierTestCase.test_set_intercept_binary(self)
sklearn.linear_model.tests.test_sgd.DenseSGDClassifierTestCase.test_set_intercept_to_intercept(self)
sklearn.linear_model.tests.test_sgd.DenseSGDClassifierTestCase.test_sgd(self)
sklearn.linear_model.tests.test_sgd.DenseSGDClassifierTestCase.test_sgd_at_least_two_labels(self)
sklearn.linear_model.tests.test_sgd.DenseSGDClassifierTestCase.test_sgd_bad_alpha(self)
sklearn.linear_model.tests.test_sgd.DenseSGDClassifierTestCase.test_sgd_bad_eta0(self)
sklearn.linear_model.tests.test_sgd.DenseSGDClassifierTestCase.test_sgd_bad_l1_ratio(self)
sklearn.linear_model.tests.test_sgd.DenseSGDClassifierTestCase.test_sgd_bad_learning_rate_schedule(self)
sklearn.linear_model.tests.test_sgd.DenseSGDClassifierTestCase.test_sgd_bad_loss(self)
sklearn.linear_model.tests.test_sgd.DenseSGDClassifierTestCase.test_sgd_bad_penalty(self)
sklearn.linear_model.tests.test_sgd.DenseSGDClassifierTestCase.test_sgd_l1(self)
sklearn.linear_model.tests.test_sgd.DenseSGDClassifierTestCase.test_sgd_max_iter_param(self)
sklearn.linear_model.tests.test_sgd.DenseSGDClassifierTestCase.test_sgd_multiclass(self)
sklearn.linear_model.tests.test_sgd.DenseSGDClassifierTestCase.test_sgd_multiclass_average(self)
sklearn.linear_model.tests.test_sgd.DenseSGDClassifierTestCase.test_sgd_multiclass_njobs(self)
sklearn.linear_model.tests.test_sgd.DenseSGDClassifierTestCase.test_sgd_multiclass_with_init_coef(self)
sklearn.linear_model.tests.test_sgd.DenseSGDClassifierTestCase.test_sgd_proba(self)
sklearn.linear_model.tests.test_sgd.DenseSGDClassifierTestCase.test_sgd_shuffle_param(self)
sklearn.linear_model.tests.test_sgd.DenseSGDClassifierTestCase.test_warm_start_multiclass(self)
sklearn.linear_model.tests.test_sgd.DenseSGDClassifierTestCase.test_weights_multiplied(self)
sklearn.linear_model.tests.test_sgd.DenseSGDClassifierTestCase.test_wrong_class_weight_format(self)
sklearn.linear_model.tests.test_sgd.DenseSGDClassifierTestCase.test_wrong_class_weight_label(self)
sklearn.linear_model.tests.test_sgd.DenseSGDClassifierTestCase.test_wrong_sample_weights(self)
sklearn.linear_model.tests.test_sgd.DenseSGDRegressorTestCase(unittest.TestCase,CommonTest)
sklearn.linear_model.tests.test_sgd.DenseSGDRegressorTestCase._test_partial_fit_equal_fit(self,lr)
sklearn.linear_model.tests.test_sgd.DenseSGDRegressorTestCase.test_average_sparse(self)
sklearn.linear_model.tests.test_sgd.DenseSGDRegressorTestCase.test_elasticnet_convergence(self)
sklearn.linear_model.tests.test_sgd.DenseSGDRegressorTestCase.test_loss_function_epsilon(self)
sklearn.linear_model.tests.test_sgd.DenseSGDRegressorTestCase.test_partial_fit(self)
sklearn.linear_model.tests.test_sgd.DenseSGDRegressorTestCase.test_partial_fit_equal_fit_constant(self)
sklearn.linear_model.tests.test_sgd.DenseSGDRegressorTestCase.test_partial_fit_equal_fit_invscaling(self)
sklearn.linear_model.tests.test_sgd.DenseSGDRegressorTestCase.test_partial_fit_equal_fit_optimal(self)
sklearn.linear_model.tests.test_sgd.DenseSGDRegressorTestCase.test_sgd(self)
sklearn.linear_model.tests.test_sgd.DenseSGDRegressorTestCase.test_sgd_averaged_computed_correctly(self)
sklearn.linear_model.tests.test_sgd.DenseSGDRegressorTestCase.test_sgd_averaged_partial_fit(self)
sklearn.linear_model.tests.test_sgd.DenseSGDRegressorTestCase.test_sgd_bad_loss(self)
sklearn.linear_model.tests.test_sgd.DenseSGDRegressorTestCase.test_sgd_bad_penalty(self)
sklearn.linear_model.tests.test_sgd.DenseSGDRegressorTestCase.test_sgd_epsilon_insensitive(self)
sklearn.linear_model.tests.test_sgd.DenseSGDRegressorTestCase.test_sgd_huber_fit(self)
sklearn.linear_model.tests.test_sgd.DenseSGDRegressorTestCase.test_sgd_least_squares_fit(self)
sklearn.linear_model.tests.test_sgd.SparseSGDClassifier(SGDClassifier)
sklearn.linear_model.tests.test_sgd.SparseSGDClassifier.decision_function(self,X)
sklearn.linear_model.tests.test_sgd.SparseSGDClassifier.fit(self,X,y,*args,**kw)
sklearn.linear_model.tests.test_sgd.SparseSGDClassifier.partial_fit(self,X,y,*args,**kw)
sklearn.linear_model.tests.test_sgd.SparseSGDClassifier.predict_proba(self,X)
sklearn.linear_model.tests.test_sgd.SparseSGDClassifierTestCase(DenseSGDClassifierTestCase)
sklearn.linear_model.tests.test_sgd.SparseSGDRegressor(SGDRegressor)
sklearn.linear_model.tests.test_sgd.SparseSGDRegressor.decision_function(self,X,*args,**kw)
sklearn.linear_model.tests.test_sgd.SparseSGDRegressor.fit(self,X,y,*args,**kw)
sklearn.linear_model.tests.test_sgd.SparseSGDRegressor.partial_fit(self,X,y,*args,**kw)
sklearn.linear_model.tests.test_sgd.SparseSGDRegressorTestCase(DenseSGDRegressorTestCase)
sklearn.linear_model.tests.test_sgd._test_gradient_common(loss_function,cases)
sklearn.linear_model.tests.test_sgd.test_future_and_deprecation_warnings()
sklearn.linear_model.tests.test_sgd.test_gradient_epsilon_insensitive()
sklearn.linear_model.tests.test_sgd.test_gradient_hinge()
sklearn.linear_model.tests.test_sgd.test_gradient_huber()
sklearn.linear_model.tests.test_sgd.test_gradient_log()
sklearn.linear_model.tests.test_sgd.test_gradient_modified_huber()
sklearn.linear_model.tests.test_sgd.test_gradient_squared_epsilon_insensitive()
sklearn.linear_model.tests.test_sgd.test_gradient_squared_hinge()
sklearn.linear_model.tests.test_sgd.test_gradient_squared_loss()
sklearn.linear_model.tests.test_sgd.test_l1_ratio()
sklearn.linear_model.tests.test_sgd.test_large_regularization()
sklearn.linear_model.tests.test_sgd.test_numerical_stability_large_gradient()
sklearn.linear_model.tests.test_sgd.test_tol_and_max_iter_default_values()
sklearn.linear_model.tests.test_sgd.test_tol_parameter()
sklearn.linear_model.tests.test_sgd.test_underflow_or_overlow()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/linear_model/tests/test_passive_aggressive.py----------------------------------------
A:sklearn.linear_model.tests.test_passive_aggressive.iris->load_iris()
A:sklearn.linear_model.tests.test_passive_aggressive.random_state->check_random_state(12)
A:sklearn.linear_model.tests.test_passive_aggressive.indices->numpy.arange(iris.data.shape[0])
A:sklearn.linear_model.tests.test_passive_aggressive.X_csr->scipy.sparse.csr_matrix(X)
A:sklearn.linear_model.tests.test_passive_aggressive.self.w->numpy.zeros(n_features, dtype=np.float64)
A:sklearn.linear_model.tests.test_passive_aggressive.p->self.project(X[i])
A:sklearn.linear_model.tests.test_passive_aggressive.loss->max(np.abs(p - y[i]) - self.epsilon, 0)
A:sklearn.linear_model.tests.test_passive_aggressive.sqnorm->numpy.dot(X[i], X[i])
A:sklearn.linear_model.tests.test_passive_aggressive.step->min(self.C, loss / sqnorm)
A:sklearn.linear_model.tests.test_passive_aggressive.clf->PassiveAggressiveClassifier(class_weight='the larch', max_iter=100)
A:sklearn.linear_model.tests.test_passive_aggressive.score->PassiveAggressiveClassifier(class_weight='the larch', max_iter=100).score(data, y)
A:sklearn.linear_model.tests.test_passive_aggressive.classes->numpy.unique(y)
A:sklearn.linear_model.tests.test_passive_aggressive.y_bin->y.copy()
A:sklearn.linear_model.tests.test_passive_aggressive.clf1->MyPassiveAggressive(C=1.0, loss=loss, fit_intercept=True, n_iter=2)
A:sklearn.linear_model.tests.test_passive_aggressive.clf2->PassiveAggressiveClassifier(C=1.0, loss=loss, fit_intercept=True, max_iter=2, shuffle=False, tol=None)
A:sklearn.linear_model.tests.test_passive_aggressive.X2->numpy.array([[-1.0, -1.0], [-1.0, 0], [-0.8, -1.0], [1.0, 1.0], [1.0, 0.0]])
A:sklearn.linear_model.tests.test_passive_aggressive.clf_balanced->PassiveAggressiveClassifier(C=0.1, max_iter=1000, tol=None, class_weight='balanced')
A:sklearn.linear_model.tests.test_passive_aggressive.clf_weighted->PassiveAggressiveClassifier(C=0.1, max_iter=1000, tol=None, class_weight={0: 0.5, 1: 0.5})
A:sklearn.linear_model.tests.test_passive_aggressive.reg->PassiveAggressiveRegressor(max_iter=100)
A:sklearn.linear_model.tests.test_passive_aggressive.pred->PassiveAggressiveRegressor(max_iter=100).predict(data)
A:sklearn.linear_model.tests.test_passive_aggressive.reg1->MyPassiveAggressive(C=1.0, loss=loss, fit_intercept=True, n_iter=2)
A:sklearn.linear_model.tests.test_passive_aggressive.reg2->PassiveAggressiveRegressor(C=1.0, tol=None, loss=loss, fit_intercept=True, max_iter=2, shuffle=False)
sklearn.linear_model.tests.test_passive_aggressive.MyPassiveAggressive(self,C=1.0,epsilon=0.01,loss='hinge',fit_intercept=True,n_iter=1,random_state=None)
sklearn.linear_model.tests.test_passive_aggressive.MyPassiveAggressive.__init__(self,C=1.0,epsilon=0.01,loss='hinge',fit_intercept=True,n_iter=1,random_state=None)
sklearn.linear_model.tests.test_passive_aggressive.MyPassiveAggressive.fit(self,X,y)
sklearn.linear_model.tests.test_passive_aggressive.MyPassiveAggressive.project(self,X)
sklearn.linear_model.tests.test_passive_aggressive.test_class_weights()
sklearn.linear_model.tests.test_passive_aggressive.test_classifier_accuracy()
sklearn.linear_model.tests.test_passive_aggressive.test_classifier_correctness()
sklearn.linear_model.tests.test_passive_aggressive.test_classifier_partial_fit()
sklearn.linear_model.tests.test_passive_aggressive.test_classifier_refit()
sklearn.linear_model.tests.test_passive_aggressive.test_classifier_undefined_methods()
sklearn.linear_model.tests.test_passive_aggressive.test_equal_class_weight()
sklearn.linear_model.tests.test_passive_aggressive.test_partial_fit_weight_class_balanced()
sklearn.linear_model.tests.test_passive_aggressive.test_regressor_correctness()
sklearn.linear_model.tests.test_passive_aggressive.test_regressor_mse()
sklearn.linear_model.tests.test_passive_aggressive.test_regressor_partial_fit()
sklearn.linear_model.tests.test_passive_aggressive.test_regressor_undefined_methods()
sklearn.linear_model.tests.test_passive_aggressive.test_wrong_class_weight_format()
sklearn.linear_model.tests.test_passive_aggressive.test_wrong_class_weight_label()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/linear_model/tests/test_randomized_l1.py----------------------------------------
A:sklearn.linear_model.tests.test_randomized_l1.diabetes->load_diabetes()
A:sklearn.linear_model.tests.test_randomized_l1.X->StandardScaler().fit_transform(X)
A:sklearn.linear_model.tests.test_randomized_l1.(F, _)->f_classif(X, y)
A:sklearn.linear_model.tests.test_randomized_l1.(coef_grid, scores_path)->lasso_stability_path(X, y, scaling=scaling, random_state=42, n_resampling=30)
A:sklearn.linear_model.tests.test_randomized_l1.clf->RandomizedLogisticRegression(verbose=False, C=1.0, random_state=42, scaling=scaling, n_resampling=50, tol=0.001)
A:sklearn.linear_model.tests.test_randomized_l1.tempdir->mkdtemp()
A:sklearn.linear_model.tests.test_randomized_l1.X_r->RandomizedLogisticRegression(verbose=False, C=1.0, random_state=42, scaling=scaling, n_resampling=50, tol=0.001).transform(X)
A:sklearn.linear_model.tests.test_randomized_l1.X_full->RandomizedLogisticRegression(verbose=False, C=1.0, random_state=42, scaling=scaling, n_resampling=50, tol=0.001).inverse_transform(X_r)
A:sklearn.linear_model.tests.test_randomized_l1.G->numpy.dot(X.T, X)
A:sklearn.linear_model.tests.test_randomized_l1.iris->load_iris()
A:sklearn.linear_model.tests.test_randomized_l1.X_orig->StandardScaler().fit_transform(X).copy()
A:sklearn.linear_model.tests.test_randomized_l1.(X, _, _, _, _)->_preprocess_data(X, y, True, True)
A:sklearn.linear_model.tests.test_randomized_l1.X_sp->scipy.sparse.csr_matrix(X)
sklearn.linear_model.tests.test_randomized_l1.test_lasso_stability_path()
sklearn.linear_model.tests.test_randomized_l1.test_randomized_lasso()
sklearn.linear_model.tests.test_randomized_l1.test_randomized_lasso_error_memory()
sklearn.linear_model.tests.test_randomized_l1.test_randomized_lasso_precompute()
sklearn.linear_model.tests.test_randomized_l1.test_randomized_logistic()
sklearn.linear_model.tests.test_randomized_l1.test_randomized_logistic_sparse()
sklearn.linear_model.tests.test_randomized_l1.test_warning_raised()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/linear_model/tests/test_perceptron.py----------------------------------------
A:sklearn.linear_model.tests.test_perceptron.iris->load_iris()
A:sklearn.linear_model.tests.test_perceptron.random_state->check_random_state(12)
A:sklearn.linear_model.tests.test_perceptron.indices->numpy.arange(iris.data.shape[0])
A:sklearn.linear_model.tests.test_perceptron.X_csr->scipy.sparse.csr_matrix(X)
A:sklearn.linear_model.tests.test_perceptron.self.w->numpy.zeros(n_features, dtype=np.float64)
A:sklearn.linear_model.tests.test_perceptron.X->numpy.atleast_2d(X)
A:sklearn.linear_model.tests.test_perceptron.clf->Perceptron(max_iter=100)
A:sklearn.linear_model.tests.test_perceptron.score->Perceptron(max_iter=100).score(data, y)
A:sklearn.linear_model.tests.test_perceptron.y_bin->y.copy()
A:sklearn.linear_model.tests.test_perceptron.clf1->MyPerceptron(n_iter=2)
A:sklearn.linear_model.tests.test_perceptron.clf2->Perceptron(max_iter=2, shuffle=False, tol=None)
sklearn.linear_model.tests.test_perceptron.MyPerceptron(self,n_iter=1)
sklearn.linear_model.tests.test_perceptron.MyPerceptron.__init__(self,n_iter=1)
sklearn.linear_model.tests.test_perceptron.MyPerceptron.fit(self,X,y)
sklearn.linear_model.tests.test_perceptron.MyPerceptron.predict(self,X)
sklearn.linear_model.tests.test_perceptron.MyPerceptron.project(self,X)
sklearn.linear_model.tests.test_perceptron.test_perceptron_accuracy()
sklearn.linear_model.tests.test_perceptron.test_perceptron_correctness()
sklearn.linear_model.tests.test_perceptron.test_undefined_methods()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/linear_model/tests/test_least_angle.py----------------------------------------
A:sklearn.linear_model.tests.test_least_angle.diabetes->sklearn.datasets.load_diabetes()
A:sklearn.linear_model.tests.test_least_angle.sys.stdout->StringIO()
A:sklearn.linear_model.tests.test_least_angle.(alphas_, active, coef_path_)->sklearn.linear_model.lars_path(X, y, method='lasso')
A:sklearn.linear_model.tests.test_least_angle.cov->numpy.dot(X.T, res)
A:sklearn.linear_model.tests.test_least_angle.C->numpy.max(abs(cov))
A:sklearn.linear_model.tests.test_least_angle.ocur->len(cov[C - eps < abs(cov)])
A:sklearn.linear_model.tests.test_least_angle.G->numpy.dot(X.T, X)
A:sklearn.linear_model.tests.test_least_angle.Xy->numpy.dot(X.T, y)
A:sklearn.linear_model.tests.test_least_angle.output->sklearn.linear_model.lars_path(X, y, method=method)
A:sklearn.linear_model.tests.test_least_angle.output_pre->sklearn.linear_model.lars_path(X, y, Gram=G, Xy=Xy, method=method)
A:sklearn.linear_model.tests.test_least_angle.clf->sklearn.linear_model.Lars(fit_intercept=False).fit(H, np.arange(n))
A:sklearn.linear_model.tests.test_least_angle.X->numpy.random.RandomState(42).randn(n, m)
A:sklearn.linear_model.tests.test_least_angle.y->numpy.array([-6.45006793, -3.51251449, -8.52445396, 6.12277822, -19.42109366])
A:sklearn.linear_model.tests.test_least_angle.rng->numpy.random.RandomState(42)
A:sklearn.linear_model.tests.test_least_angle.(_, _, coef_path_)->sklearn.linear_model.lars_path(X, y, Gram='auto', copy_X=False, copy_Gram=False, alpha_min=0.0, method='lasso', verbose=0, max_iter=500)
A:sklearn.linear_model.tests.test_least_angle.(alphas_, active_, coef_path_)->sklearn.linear_model.lars_path(X, y, method='lasso', Gram=G, Xy=Xy, alpha_min=0.9)
A:sklearn.linear_model.tests.test_least_angle.(alpha_, active, coef)->sklearn.linear_model.lars_path(X, y, method='lasso', Gram=G, Xy=Xy, alpha_min=0.9, return_path=False)
A:sklearn.linear_model.tests.test_least_angle.X1->numpy.array([[1, 1.0], [1.0, 1.0]])
A:sklearn.linear_model.tests.test_least_angle.y1->numpy.array([1, 1])
A:sklearn.linear_model.tests.test_least_angle.(alphas, active, coef_path)->sklearn.linear_model.lars_path(X1, y1)
A:sklearn.linear_model.tests.test_least_angle.lars->sklearn.linear_model.Lars(n_nonzero_coefs=6, verbose=verbose)
A:sklearn.linear_model.tests.test_least_angle.coord_descent->sklearn.linear_model.Lasso(alpha=alpha, tol=0.0001, normalize=False)
A:sklearn.linear_model.tests.test_least_angle.(alphas, _, lasso_path)->sklearn.linear_model.lars_path(X, y, method='lasso', positive=True)
A:sklearn.linear_model.tests.test_least_angle.lasso_cd->sklearn.linear_model.Lasso(fit_intercept=False, normalize=True, tol=1e-08, positive=True)
A:sklearn.linear_model.tests.test_least_angle.error->scipy.linalg.norm(c - lasso_cd.coef_)
A:sklearn.linear_model.tests.test_least_angle.clf1->sklearn.linear_model.LassoLars(fit_intercept=False, alpha=alpha, normalize=False, positive=True).fit(X, y)
A:sklearn.linear_model.tests.test_least_angle.clf2->sklearn.linear_model.Lasso(fit_intercept=False, alpha=alpha, tol=1e-08, normalize=False, positive=True).fit(X, y)
A:sklearn.linear_model.tests.test_least_angle.err->scipy.linalg.norm(clf1.coef_ - clf2.coef_)
A:sklearn.linear_model.tests.test_least_angle.lasso->sklearn.linear_model.LassoLars()
A:sklearn.linear_model.tests.test_least_angle.lasso2->sklearn.linear_model.LassoLars(alpha=lasso.alphas_[2])
A:sklearn.linear_model.tests.test_least_angle.w->numpy.zeros((m, 1))
A:sklearn.linear_model.tests.test_least_angle.i->numpy.arange(0, m)
A:sklearn.linear_model.tests.test_least_angle.(lars_alphas, _, lars_coef)->sklearn.linear_model.lars_path(X, y, method='lasso')
A:sklearn.linear_model.tests.test_least_angle.(_, lasso_coef2, _)->sklearn.linear_model.lasso_path(X, y, alphas=lars_alphas, tol=1e-06, fit_intercept=False)
A:sklearn.linear_model.tests.test_least_angle.lars_obj->objective_function(lars_coef_)
A:sklearn.linear_model.tests.test_least_angle.cd_obj->objective_function(cd_coef_)
A:sklearn.linear_model.tests.test_least_angle.Y_pred->getattr(linear_model, estname)(positive=True, **params).predict(X)
A:sklearn.linear_model.tests.test_least_angle.y_pred->getattr(linear_model, estname)(positive=True, **params).predict(X)
A:sklearn.linear_model.tests.test_least_angle.lars_cv->sklearn.linear_model.LassoLarsCV(max_iter=5)
A:sklearn.linear_model.tests.test_least_angle.x->numpy.array([[0.47299829, 0, 0, 0, 0], [0.08239882, 0.85784863, 0, 0, 0], [0.30114139, -0.07501577, 0.80895216, 0, 0], [-0.01460346, -0.1015233, 0.0407278, 0.80338378, 0], [-0.69363927, 0.06754067, 0.18064514, -0.0803561, 0.40427291]])
A:sklearn.linear_model.tests.test_least_angle.lars_bic->sklearn.linear_model.LassoLarsIC('bic')
A:sklearn.linear_model.tests.test_least_angle.lars_aic->sklearn.linear_model.LassoLarsIC('aic')
A:sklearn.linear_model.tests.test_least_angle.lars_broken->sklearn.linear_model.LassoLarsIC('<unknown>')
A:sklearn.linear_model.tests.test_least_angle.splitted_data->train_test_split(X, y, random_state=42)
A:sklearn.linear_model.tests.test_least_angle.(alpha, active, coefs)->sklearn.linear_model.lars_path(diabetes['data'], diabetes['target'], return_path=True, method=method, positive=True)
A:sklearn.linear_model.tests.test_least_angle.params->default_parameter.copy()
A:sklearn.linear_model.tests.test_least_angle.estimator->getattr(linear_model, estname)(positive=True, **params)
A:sklearn.linear_model.tests.test_least_angle.r->numpy.array([[0, 0, 0, 0, 0, -79.81036280949903, -83.52878873278283, -83.77765373919071, -83.78415693288893, -84.03339059175666], [0, 0, 0, 0, -0.476624256777266, 0, 0, 0, 0, 0.025219751009936], [0, -3.577397088285891, -4.702795355871871, -7.016748621359461, -7.614898471899412, -0.336938391359179, 0, 0, 0.001213370600853, 0.048162321585148], [0, 0, 0, 2.231558436628169, 2.723267514525966, 2.811549786389614, 2.813766976061531, 2.817462468949557, 2.817368178703816, 2.816221090636795], [0, 0, -1.218422599914637, -3.457726183014808, -4.02130452206071, -45.827461592423745, -47.776608869312305, -47.9115616107464, -47.914845922736234, -48.03956233426572]])
A:sklearn.linear_model.tests.test_least_angle.model_lasso_lars->sklearn.linear_model.LassoLars(alpha=0, fit_intercept=False, normalize=False)
A:sklearn.linear_model.tests.test_least_angle.r2->numpy.array([[0, 0, 0, 0, 0], [0, 0, 0, 8.371887668009453, 19.463768371044026], [0, 0, 0, 0, 9.901611055290553], [0, 7.495923132833733, 9.245133544334507, 17.389369207545062, 26.9716568156435], [0, 0, -1.569380717440311, -5.924804108067312, -7.996385265061972]])
A:sklearn.linear_model.tests.test_least_angle.model_lasso_lars2->sklearn.linear_model.LassoLars(alpha=0, fit_intercept=True, normalize=True)
A:sklearn.linear_model.tests.test_least_angle.normx->numpy.sqrt(np.sum(temp ** 2, axis=0))
sklearn.linear_model.tests.test_least_angle.test_all_precomputed()
sklearn.linear_model.tests.test_least_angle.test_collinearity()
sklearn.linear_model.tests.test_least_angle.test_estimatorclasses_positive_constraint()
sklearn.linear_model.tests.test_least_angle.test_lars_add_features()
sklearn.linear_model.tests.test_least_angle.test_lars_cv()
sklearn.linear_model.tests.test_least_angle.test_lars_cv_max_iter()
sklearn.linear_model.tests.test_least_angle.test_lars_lstsq()
sklearn.linear_model.tests.test_least_angle.test_lars_n_nonzero_coefs(verbose=False)
sklearn.linear_model.tests.test_least_angle.test_lars_path_positive_constraint()
sklearn.linear_model.tests.test_least_angle.test_lars_path_readonly_data()
sklearn.linear_model.tests.test_least_angle.test_lars_precompute()
sklearn.linear_model.tests.test_least_angle.test_lasso_gives_lstsq_solution()
sklearn.linear_model.tests.test_least_angle.test_lasso_lars_ic()
sklearn.linear_model.tests.test_least_angle.test_lasso_lars_path_length()
sklearn.linear_model.tests.test_least_angle.test_lasso_lars_vs_R_implementation()
sklearn.linear_model.tests.test_least_angle.test_lasso_lars_vs_lasso_cd(verbose=False)
sklearn.linear_model.tests.test_least_angle.test_lasso_lars_vs_lasso_cd_early_stopping(verbose=False)
sklearn.linear_model.tests.test_least_angle.test_lasso_lars_vs_lasso_cd_ill_conditioned()
sklearn.linear_model.tests.test_least_angle.test_lasso_lars_vs_lasso_cd_ill_conditioned2()
sklearn.linear_model.tests.test_least_angle.test_lasso_lars_vs_lasso_cd_positive(verbose=False)
sklearn.linear_model.tests.test_least_angle.test_multitarget()
sklearn.linear_model.tests.test_least_angle.test_no_path()
sklearn.linear_model.tests.test_least_angle.test_no_path_all_precomputed()
sklearn.linear_model.tests.test_least_angle.test_no_path_precomputed()
sklearn.linear_model.tests.test_least_angle.test_rank_deficient_design()
sklearn.linear_model.tests.test_least_angle.test_simple()
sklearn.linear_model.tests.test_least_angle.test_simple_precomputed()
sklearn.linear_model.tests.test_least_angle.test_singular_matrix()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/linear_model/tests/test_ransac.py----------------------------------------
A:sklearn.linear_model.tests.test_ransac.X->numpy.random.RandomState(0).rand(10, 2)
A:sklearn.linear_model.tests.test_ransac.data->numpy.column_stack([X, y])
A:sklearn.linear_model.tests.test_ransac.rng->numpy.random.RandomState(0)
A:sklearn.linear_model.tests.test_ransac.outliers->numpy.unique(rng.randint(len(X), size=200))
A:sklearn.linear_model.tests.test_ransac.base_estimator->Lasso()
A:sklearn.linear_model.tests.test_ransac.ransac_estimator->RANSACRegressor(base_estimator)
A:sklearn.linear_model.tests.test_ransac.ref_inlier_mask->numpy.ones_like(ransac_estimator.inlier_mask_).astype(np.bool_)
A:sklearn.linear_model.tests.test_ransac.y->numpy.zeros((100,))
A:sklearn.linear_model.tests.test_ransac.max_trials->_dynamic_max_trials(len(X) - len(outliers), X.shape[0], 2, 1 - 1e-09)
A:sklearn.linear_model.tests.test_ransac.X_sparse->scipy.sparse.csc_matrix(X)
A:sklearn.linear_model.tests.test_ransac.ransac_none_estimator->RANSACRegressor(None, 2, 5, random_state=0)
A:sklearn.linear_model.tests.test_ransac.ransac_estimator1->RANSACRegressor(base_estimator, min_samples=2, residual_threshold=5, random_state=0, loss=loss_multi1)
A:sklearn.linear_model.tests.test_ransac.ransac_estimator2->RANSACRegressor(base_estimator, min_samples=2, residual_threshold=5, random_state=0, loss=loss_multi2)
A:sklearn.linear_model.tests.test_ransac.ransac_estimator3->RANSACRegressor(base_estimator, min_samples=2, residual_threshold=5, random_state=0, loss='squared_loss')
A:sklearn.linear_model.tests.test_ransac.ransac_estimator4->RANSACRegressor(base_estimator, min_samples=5.2, residual_threshold=5, random_state=0)
A:sklearn.linear_model.tests.test_ransac.ransac_estimator5->RANSACRegressor(base_estimator, min_samples=2.0, residual_threshold=5, random_state=0)
A:sklearn.linear_model.tests.test_ransac.ransac_estimator6->RANSACRegressor(base_estimator, residual_threshold=5, random_state=0)
A:sklearn.linear_model.tests.test_ransac.ransac_estimator7->RANSACRegressor(base_estimator, min_samples=X.shape[0] + 1, residual_threshold=5, random_state=0)
A:sklearn.linear_model.tests.test_ransac.yyy->numpy.column_stack([y, y, y])
A:sklearn.linear_model.tests.test_ransac.ransac_estimator0->RANSACRegressor(base_estimator, min_samples=2, residual_threshold=5, random_state=0)
A:sklearn.linear_model.tests.test_ransac.weights->numpy.ones(n_samples)
A:sklearn.linear_model.tests.test_ransac.random_state->check_random_state(0)
A:sklearn.linear_model.tests.test_ransac.X_->numpy.append(X_, outlier_X, axis=0)
A:sklearn.linear_model.tests.test_ransac.y_->numpy.append(y_, outlier_y)
A:sklearn.linear_model.tests.test_ransac.sample_weight->numpy.append(sample_weight, outlier_weight)
A:sklearn.linear_model.tests.test_ransac.outlier_X->check_random_state(0).randint(0, 1000, [1, 1])
A:sklearn.linear_model.tests.test_ransac.outlier_weight->check_random_state(0).randint(0, 10, 1)
A:sklearn.linear_model.tests.test_ransac.outlier_y->check_random_state(0).randint(-1000, 0, 1)
A:sklearn.linear_model.tests.test_ransac.X_flat->numpy.append(np.repeat(X_, sample_weight, axis=0), np.repeat(outlier_X, outlier_weight, axis=0), axis=0)
A:sklearn.linear_model.tests.test_ransac.y_flat->numpy.ndarray.flatten(np.append(np.repeat(y_, sample_weight, axis=0), np.repeat(outlier_y, outlier_weight, axis=0), axis=0))
sklearn.linear_model.tests.test_ransac.test_ransac_default_residual_threshold()
sklearn.linear_model.tests.test_ransac.test_ransac_dynamic_max_trials()
sklearn.linear_model.tests.test_ransac.test_ransac_exceed_max_skips()
sklearn.linear_model.tests.test_ransac.test_ransac_fit_sample_weight()
sklearn.linear_model.tests.test_ransac.test_ransac_inliers_outliers()
sklearn.linear_model.tests.test_ransac.test_ransac_is_data_valid()
sklearn.linear_model.tests.test_ransac.test_ransac_is_model_valid()
sklearn.linear_model.tests.test_ransac.test_ransac_max_trials()
sklearn.linear_model.tests.test_ransac.test_ransac_min_n_samples()
sklearn.linear_model.tests.test_ransac.test_ransac_multi_dimensional_targets()
sklearn.linear_model.tests.test_ransac.test_ransac_no_valid_data()
sklearn.linear_model.tests.test_ransac.test_ransac_no_valid_model()
sklearn.linear_model.tests.test_ransac.test_ransac_none_estimator()
sklearn.linear_model.tests.test_ransac.test_ransac_predict()
sklearn.linear_model.tests.test_ransac.test_ransac_resid_thresh_no_inliers()
sklearn.linear_model.tests.test_ransac.test_ransac_residual_loss()
sklearn.linear_model.tests.test_ransac.test_ransac_residual_metric()
sklearn.linear_model.tests.test_ransac.test_ransac_score()
sklearn.linear_model.tests.test_ransac.test_ransac_sparse_coo()
sklearn.linear_model.tests.test_ransac.test_ransac_sparse_csc()
sklearn.linear_model.tests.test_ransac.test_ransac_sparse_csr()
sklearn.linear_model.tests.test_ransac.test_ransac_stop_n_inliers()
sklearn.linear_model.tests.test_ransac.test_ransac_stop_score()
sklearn.linear_model.tests.test_ransac.test_ransac_warn_exceed_max_skips()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/utils/validation.py----------------------------------------
A:sklearn.utils.validation.X->check_array(X, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)
A:sklearn.utils.validation.x->numpy.asarray(x)
A:sklearn.utils.validation.joined->', '.join(('%d' % e for e in shape))
A:sklearn.utils.validation.memory->Memory(cachedir=memory, verbose=0)
A:sklearn.utils.validation.uniques->numpy.unique(lengths)
A:sklearn.utils.validation.spmatrix->spmatrix.copy().copy()
A:sklearn.utils.validation.dtype_orig->getattr(array, 'dtype', None)
A:sklearn.utils.validation.array->getattr(0.5 * (array + array.T), conversion)()
A:sklearn.utils.validation.shape_repr->_shape_repr(array.shape)
A:sklearn.utils.validation.n_samples->_num_samples(array)
A:sklearn.utils.validation.y->y.astype(np.float64).astype(np.float64)
A:sklearn.utils.validation.shape->numpy.shape(y)
A:sklearn.utils.validation.diff->diff.tocsr().tocsr()
A:sklearn.utils.validation.symmetric->numpy.allclose(array, array.T, atol=tol)
sklearn.utils.as_float_array(X,copy=True,force_all_finite=True)
sklearn.utils.assert_all_finite(X)
sklearn.utils.check_X_y(X,y,accept_sparse=False,dtype='numeric',order=None,copy=False,force_all_finite=True,ensure_2d=True,allow_nd=False,multi_output=False,ensure_min_samples=1,ensure_min_features=1,y_numeric=False,warn_on_dtype=False,estimator=None)
sklearn.utils.check_array(array,accept_sparse=False,dtype='numeric',order=None,copy=False,force_all_finite=True,ensure_2d=True,allow_nd=False,ensure_min_samples=1,ensure_min_features=1,warn_on_dtype=False,estimator=None)
sklearn.utils.check_consistent_length(*arrays)
sklearn.utils.check_random_state(seed)
sklearn.utils.check_symmetric(array,tol=1e-10,raise_warning=True,raise_exception=False)
sklearn.utils.column_or_1d(y,warn=False)
sklearn.utils.indexable(*iterables)
sklearn.utils.validation._assert_all_finite(X)
sklearn.utils.validation._ensure_sparse_format(spmatrix,accept_sparse,dtype,copy,force_all_finite)
sklearn.utils.validation._is_arraylike(x)
sklearn.utils.validation._num_samples(x)
sklearn.utils.validation._shape_repr(shape)
sklearn.utils.validation.as_float_array(X,copy=True,force_all_finite=True)
sklearn.utils.validation.assert_all_finite(X)
sklearn.utils.validation.check_X_y(X,y,accept_sparse=False,dtype='numeric',order=None,copy=False,force_all_finite=True,ensure_2d=True,allow_nd=False,multi_output=False,ensure_min_samples=1,ensure_min_features=1,y_numeric=False,warn_on_dtype=False,estimator=None)
sklearn.utils.validation.check_array(array,accept_sparse=False,dtype='numeric',order=None,copy=False,force_all_finite=True,ensure_2d=True,allow_nd=False,ensure_min_samples=1,ensure_min_features=1,warn_on_dtype=False,estimator=None)
sklearn.utils.validation.check_consistent_length(*arrays)
sklearn.utils.validation.check_is_fitted(estimator,attributes,msg=None,all_or_any=all)
sklearn.utils.validation.check_memory(memory)
sklearn.utils.validation.check_non_negative(X,whom)
sklearn.utils.validation.check_random_state(seed)
sklearn.utils.validation.check_symmetric(array,tol=1e-10,raise_warning=True,raise_exception=False)
sklearn.utils.validation.column_or_1d(y,warn=False)
sklearn.utils.validation.has_fit_parameter(estimator,parameter)
sklearn.utils.validation.indexable(*iterables)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/utils/__init__.py----------------------------------------
A:sklearn.utils.__init__.mask->numpy.zeros(mask_length, dtype=np.bool)
A:sklearn.utils.__init__.ind->numpy.arange(mask.shape[0])
A:sklearn.utils.__init__.random_state->check_random_state(options.pop('random_state', None))
A:sklearn.utils.__init__.replace->options.pop('replace', True)
A:sklearn.utils.__init__.max_n_samples->options.pop('n_samples', None)
A:sklearn.utils.__init__.indices->numpy.arange(n_samples)
A:sklearn.utils.__init__.X->X.copy().copy()
A:sklearn.utils.__init__.end->min(n_samples, end)
sklearn.utils.__init__.Bunch(self,**kwargs)
sklearn.utils.__init__.Bunch.__dir__(self)
sklearn.utils.__init__.Bunch.__getattr__(self,key)
sklearn.utils.__init__.Bunch.__init__(self,**kwargs)
sklearn.utils.__init__.Bunch.__setattr__(self,key,value)
sklearn.utils.__init__.Bunch.__setstate__(self,state)
sklearn.utils.__init__._get_n_jobs(n_jobs)
sklearn.utils.__init__.axis0_safe_slice(X,mask,len_mask)
sklearn.utils.__init__.gen_batches(n,batch_size)
sklearn.utils.__init__.gen_even_slices(n,n_packs,n_samples=None)
sklearn.utils.__init__.indices_to_mask(indices,mask_length)
sklearn.utils.__init__.resample(*arrays,**options)
sklearn.utils.__init__.safe_indexing(X,indices)
sklearn.utils.__init__.safe_mask(X,mask)
sklearn.utils.__init__.safe_sqr(X,copy=True)
sklearn.utils.__init__.shuffle(*arrays,**options)
sklearn.utils.__init__.tosequence(x)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/utils/fixes.py----------------------------------------
A:sklearn.utils.fixes.euler_gamma->getattr(np, 'euler_gamma', 0.5772156649015329)
A:sklearn.utils.fixes.np_version->_parse_version(np.__version__)
A:sklearn.utils.fixes.sp_version->_parse_version(scipy.__version__)
A:sklearn.utils.fixes.out->numpy.asscalar(out)
A:sklearn.utils.fixes.major_index->numpy.compress(mask, major_index)
A:sklearn.utils.fixes.value->numpy.compress(mask, value)
A:sklearn.utils.fixes.(major_index, value)->_minor_reduce(mat, min_or_max)
A:sklearn.utils.fixes.value[not_full]->min_or_max(value[not_full], 0)
A:sklearn.utils.fixes.res->coo_matrix((value, (major_index, np.zeros(len(value)))), dtype=X.dtype, shape=(M, 1))
A:sklearn.utils.fixes.zero->X.dtype.type(0)
A:sklearn.utils.fixes.m->min_or_max(zero, m)
sklearn.utils.fixes._parse_version(version_string)
sklearn.utils.fixes.parallel_helper(obj,methodname,*args,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/utils/mocking.py----------------------------------------
A:sklearn.utils.mocking.self.iloc->ArraySlicingWrapper(array)
A:sklearn.utils.mocking.self.classes_->numpy.unique(check_array(y, ensure_2d=False, allow_nd=True))
sklearn.utils.mocking.ArraySlicingWrapper(self,array)
sklearn.utils.mocking.ArraySlicingWrapper.__getitem__(self,aslice)
sklearn.utils.mocking.ArraySlicingWrapper.__init__(self,array)
sklearn.utils.mocking.CheckingClassifier(self,check_y=None,check_X=None,foo_param=0,expected_fit_params=None)
sklearn.utils.mocking.CheckingClassifier.__init__(self,check_y=None,check_X=None,foo_param=0,expected_fit_params=None)
sklearn.utils.mocking.CheckingClassifier.fit(self,X,y,**fit_params)
sklearn.utils.mocking.CheckingClassifier.predict(self,T)
sklearn.utils.mocking.CheckingClassifier.score(self,X=None,Y=None)
sklearn.utils.mocking.MockDataFrame(self,array)
sklearn.utils.mocking.MockDataFrame.__array__(self,dtype=None)
sklearn.utils.mocking.MockDataFrame.__eq__(self,other)
sklearn.utils.mocking.MockDataFrame.__init__(self,array)
sklearn.utils.mocking.MockDataFrame.__len__(self)
sklearn.utils.mocking.MockDataFrame.__ne__(self,other)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/utils/optimize.py----------------------------------------
A:sklearn.utils.optimize.ret->line_search_wolfe2(f, fprime, xk, pk, gfk, old_fval, old_old_fval, **kwargs)
A:sklearn.utils.optimize.xsupi->_cg(fhess_p, fgrad, maxiter=maxinner, tol=termcond)
A:sklearn.utils.optimize.dri0->numpy.dot(ri, ri)
A:sklearn.utils.optimize.Ap->fhess_p(psupi)
A:sklearn.utils.optimize.curv->numpy.dot(psupi, Ap)
A:sklearn.utils.optimize.dri1->numpy.dot(ri, ri)
A:sklearn.utils.optimize.x0->numpy.asarray(x0).flatten()
A:sklearn.utils.optimize.old_fval->func(x0, *args)
A:sklearn.utils.optimize.(fgrad, fhess_p)->grad_hess(xk, *args)
A:sklearn.utils.optimize.absgrad->numpy.abs(fgrad)
A:sklearn.utils.optimize.maggrad->numpy.sum(absgrad)
A:sklearn.utils.optimize.eta->min([0.5, np.sqrt(maggrad)])
A:sklearn.utils.optimize.(alphak, fc, gc, old_fval, old_old_fval, gfkp1)->_line_search_wolfe12(func, grad, xk, xsupi, fgrad, old_fval, old_old_fval, args=args)
sklearn.utils.optimize._LineSearchError(RuntimeError)
sklearn.utils.optimize._cg(fhess_p,fgrad,maxiter,tol)
sklearn.utils.optimize._line_search_wolfe12(f,fprime,xk,pk,gfk,old_fval,old_old_fval,**kwargs)
sklearn.utils.optimize.newton_cg(grad_hess,func,grad,x0,args=(),tol=0.0001,maxiter=100,maxinner=200,line_search=True,warn=True)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/utils/stats.py----------------------------------------
A:sklearn.utils.stats.sorted_idx->numpy.argsort(array)
A:sklearn.utils.stats.weight_cdf->stable_cumsum(sample_weight[sorted_idx])
A:sklearn.utils.stats.percentile_idx->numpy.searchsorted(weight_cdf, percentile / 100.0 * weight_cdf[-1])
sklearn.utils.stats._weighted_percentile(array,sample_weight,percentile=50)
sklearn.utils.stats.rankdata(*args,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/utils/testing.py----------------------------------------
A:sklearn.utils.testing._dummy->unittest.TestCase('__init__')
A:sklearn.utils.testing.result->func(*args, **kw)
A:sklearn.utils.testing.found->any((warning.category is warning_class for warning in w))
A:sklearn.utils.testing.msg->str(msg.args[0] if hasattr(msg, 'args') else msg)
A:sklearn.utils.testing.error_message->str(e)
A:sklearn.utils.testing.names->' or '.join((e.__name__ for e in exceptions))
A:sklearn.utils.testing.x->x.tocsr().tocsr()
A:sklearn.utils.testing.y->y.tocsr().tocsr()
A:sklearn.utils.testing.datasets->dict(columns_dict)
A:sklearn.utils.testing.ordering->sorted(list(datasets.keys()))
A:sklearn.utils.testing.datasets['mldata_descr_ordering']->scipy.empty((1, len(ordering)), dtype='object')
A:sklearn.utils.testing.matfile->BytesIO()
A:sklearn.utils.testing.datasets.mldata.urlopen->mock_mldata_urlopen(mock_datasets)
A:sklearn.utils.testing.module->inspect.getmodule(func)
A:sklearn.utils.testing.classes->inspect.getmembers(module, inspect.isclass)
A:sklearn.utils.testing.all_classes->set(all_classes)
A:sklearn.utils.testing.type_filter->list(type_filter)
A:sklearn.utils.testing.self.temp_folder->tempfile.mkdtemp(prefix='sklearn_testing_')
A:sklearn.utils.testing.fpath->os.path.join(self.temp_folder, 'data.pkl')
A:sklearn.utils.testing.data_read_only->sklearn.externals.joblib.load(fpath, mmap_mode=self.mmap_mode)
A:sklearn.utils.testing.with_network->with_setup(check_skip_network)
A:sklearn.utils.testing.with_travis->with_setup(check_skip_travis)
A:sklearn.utils.testing.self.description->'{0[1]}.{0[3]}:{1.__name__}({2})'.format(inspect.stack()[1], check, arg_text)
A:sklearn.utils.testing.func_name->_get_func_name(func, class_name=class_name)
A:sklearn.utils.testing.args->list(filter(lambda x: x not in ignore, _get_args(func)))
A:sklearn.utils.testing.doc->numpydoc.docscrape.FunctionDoc(func)
A:sklearn.utils.testing.param_name->name.lstrip()
A:sklearn.utils.testing.param_names->list(filter(lambda x: x not in ignore, param_names))
A:sklearn.utils.testing.bad->str(sorted(list(set(param_names) ^ set(args))))
sklearn.utils.testing.TempMemmap(self,data,mmap_mode='r')
sklearn.utils.testing.TempMemmap.__enter__(self)
sklearn.utils.testing.TempMemmap.__exit__(self,exc_type,exc_val,exc_tb)
sklearn.utils.testing.TempMemmap.__init__(self,data,mmap_mode='r')
sklearn.utils.testing._IgnoreWarnings(self,category)
sklearn.utils.testing._IgnoreWarnings.__enter__(self)
sklearn.utils.testing._IgnoreWarnings.__exit__(self,*exc_info)
sklearn.utils.testing._IgnoreWarnings.__init__(self,category)
sklearn.utils.testing._IgnoreWarnings.__repr__(self)
sklearn.utils.testing._assert_allclose(actual,desired,rtol=1e-07,atol=0,err_msg='',verbose=True)
sklearn.utils.testing._delete_folder(folder_path,warn=False)
sklearn.utils.testing._get_args(function,varargs=False)
sklearn.utils.testing._get_func_name(func,class_name=None)
sklearn.utils.testing._named_check(self,check,arg_text)
sklearn.utils.testing._named_check.__init__(self,check,arg_text)
sklearn.utils.testing.all_estimators(include_meta_estimators=False,include_other=False,type_filter=None,include_dont_test=False)
sklearn.utils.testing.assert_allclose_dense_sparse(x,y,rtol=1e-07,atol=1e-09,err_msg='')
sklearn.utils.testing.assert_no_warnings(func,*args,**kw)
sklearn.utils.testing.assert_raise_message(exceptions,message,function,*args,**kwargs)
sklearn.utils.testing.assert_warns(warning_class,func,*args,**kw)
sklearn.utils.testing.assert_warns_message(warning_class,message,func,*args,**kw)
sklearn.utils.testing.check_docstring_parameters(func,doc=None,ignore=None,class_name=None)
sklearn.utils.testing.check_skip_network()
sklearn.utils.testing.check_skip_travis()
sklearn.utils.testing.clean_warning_registry()
sklearn.utils.testing.fake_mldata(columns_dict,dataname,matfile,ordering=None)
sklearn.utils.testing.if_matplotlib(func)
sklearn.utils.testing.if_safe_multiprocessing_with_blas(func)
sklearn.utils.testing.ignore_warnings(obj=None,category=Warning)
sklearn.utils.testing.install_mldata_mock(mock_datasets)
sklearn.utils.testing.mock_mldata_urlopen(self,mock_datasets)
sklearn.utils.testing.mock_mldata_urlopen.__init__(self,mock_datasets)
sklearn.utils.testing.set_random_state(estimator,random_state=0)
sklearn.utils.testing.skip_if_32bit(func)
sklearn.utils.testing.uninstall_mldata_mock()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/utils/metaestimators.py----------------------------------------
A:sklearn.utils.metaestimators.out->super(_BaseComposition, self).get_params(deep=False)
A:sklearn.utils.metaestimators.estimators->getattr(self, attr)
A:sklearn.utils.metaestimators.(names, _)->zip(*getattr(self, attr))
A:sklearn.utils.metaestimators.new_estimators->list(getattr(self, attr))
A:sklearn.utils.metaestimators.invalid_names->set(names).intersection(self.get_params(deep=False))
A:sklearn.utils.metaestimators.delegate->tuple(delegate)
A:sklearn.utils.metaestimators.X_subset->safe_indexing(X, indices)
A:sklearn.utils.metaestimators.y_subset->safe_indexing(y, indices)
sklearn.utils.metaestimators._BaseComposition(self)
sklearn.utils.metaestimators._BaseComposition.__init__(self)
sklearn.utils.metaestimators._BaseComposition._get_params(self,attr,deep=True)
sklearn.utils.metaestimators._BaseComposition._replace_estimator(self,attr,name,new_val)
sklearn.utils.metaestimators._BaseComposition._set_params(self,attr,**params)
sklearn.utils.metaestimators._BaseComposition._validate_names(self,names)
sklearn.utils.metaestimators._IffHasAttrDescriptor(self,fn,delegate_names,attribute_name)
sklearn.utils.metaestimators._IffHasAttrDescriptor.__get__(self,obj,type=None)
sklearn.utils.metaestimators._IffHasAttrDescriptor.__init__(self,fn,delegate_names,attribute_name)
sklearn.utils.metaestimators._safe_split(estimator,X,y,indices,train_indices=None)
sklearn.utils.metaestimators.if_delegate_has_method(delegate)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/utils/arpack.py----------------------------------------
sklearn.utils.arpack.eigs(A,*args,**kwargs)
sklearn.utils.arpack.eigsh(A,*args,**kwargs)
sklearn.utils.arpack.svds(A,*args,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/utils/graph.py----------------------------------------
A:sklearn.utils.graph.graph->scipy.sparse.lil_matrix(graph)
A:sklearn.utils.graph.next_level->set()
sklearn.utils.graph.connected_components(*args,**kwargs)
sklearn.utils.graph.graph_laplacian(*args,**kwargs)
sklearn.utils.graph.single_source_shortest_path_length(graph,source,cutoff=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/utils/setup.py----------------------------------------
A:sklearn.utils.setup.config->Configuration('utils', parent_package, top_path)
A:sklearn.utils.setup.(cblas_libs, blas_info)->get_blas_info()
A:sklearn.utils.setup.cblas_compile_args->blas_info.pop('extra_compile_args', [])
sklearn.utils.setup.configuration(parent_package='',top_path=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/utils/extmath.py----------------------------------------
A:sklearn.utils.extmath.x->numpy.ravel(x, order='K')
A:sklearn.utils.extmath.X->numpy.copy(X)
A:sklearn.utils.extmath.norms->numpy.einsum('ij,ij->i', X, X)
A:sklearn.utils.extmath.(sign, ld)->numpy.linalg.slogdet(A)
A:sklearn.utils.extmath.ret->ret.toarray().toarray()
A:sklearn.utils.extmath.random_state->check_random_state(random_state)
A:sklearn.utils.extmath.Q->randomized_range_finder(M, n_random, n_iter, power_iteration_normalizer, random_state)
A:sklearn.utils.extmath.(Q, _)->scipy.linalg.qr(safe_sparse_dot(A, Q), mode='economic')
A:sklearn.utils.extmath.B->safe_sparse_dot(Q.T, M)
A:sklearn.utils.extmath.(Uhat, s, V)->scipy.linalg.svd(B, full_matrices=False)
A:sklearn.utils.extmath.U->numpy.dot(Q, Uhat)
A:sklearn.utils.extmath.(U, V)->svd_flip(U, V, u_based_decision=False)
A:sklearn.utils.extmath.a->numpy.asarray(a)
A:sklearn.utils.extmath.w->numpy.asarray(w)
A:sklearn.utils.extmath.scores->numpy.unique(np.ravel(a))
A:sklearn.utils.extmath.testshape->list(a.shape)
A:sklearn.utils.extmath.oldmostfreq->numpy.zeros(testshape)
A:sklearn.utils.extmath.oldcounts->numpy.maximum(counts, oldcounts)
A:sklearn.utils.extmath.template->numpy.zeros(a.shape)
A:sklearn.utils.extmath.counts->numpy.expand_dims(np.sum(template, axis), axis)
A:sklearn.utils.extmath.mostfrequent->numpy.where(counts > oldcounts, score, oldmostfreq)
A:sklearn.utils.extmath.ix->numpy.indices(shape)
A:sklearn.utils.extmath.out->numpy.cumsum(arr, axis=axis, dtype=np.float64)
A:sklearn.utils.extmath.max_abs_cols->numpy.argmax(np.abs(u), axis=0)
A:sklearn.utils.extmath.signs->numpy.sign(u[range(u.shape[0]), max_abs_rows])
A:sklearn.utils.extmath.max_abs_rows->numpy.argmax(np.abs(u), axis=1)
A:sklearn.utils.extmath.max_prob->numpy.max(X, axis=1).reshape((-1, 1))
A:sklearn.utils.extmath.sum_prob->numpy.sum(X, axis=1).reshape((-1, 1))
A:sklearn.utils.extmath.m->numpy.copy(X).data.min()
A:sklearn.utils.extmath.min_->safe_min(X)
A:sklearn.utils.extmath.new_sum->numpy.copy(X).sum(axis=0)
A:sklearn.utils.extmath.expected->numpy.sum(arr, axis=axis, dtype=np.float64)
sklearn.utils.extmath._deterministic_vector_sign_flip(u)
sklearn.utils.extmath._impose_f_order(X)
sklearn.utils.extmath._incremental_mean_and_var(X,last_mean=0.0,last_variance=None,last_sample_count=0)
sklearn.utils.extmath.cartesian(arrays,out=None)
sklearn.utils.extmath.density(w,**kwargs)
sklearn.utils.extmath.fast_dot(a,b,out=None)
sklearn.utils.extmath.fast_logdet(A)
sklearn.utils.extmath.log_logistic(X,out=None)
sklearn.utils.extmath.logsumexp(arr,axis=0)
sklearn.utils.extmath.make_nonnegative(X,min_value=0)
sklearn.utils.extmath.norm(x)
sklearn.utils.extmath.pinvh(a,cond=None,rcond=None,lower=True)
sklearn.utils.extmath.randomized_range_finder(A,size,n_iter,power_iteration_normalizer='auto',random_state=None)
sklearn.utils.extmath.randomized_svd(M,n_components,n_oversamples=10,n_iter='auto',power_iteration_normalizer='auto',transpose='auto',flip_sign=True,random_state=0)
sklearn.utils.extmath.row_norms(X,squared=False)
sklearn.utils.extmath.safe_min(X)
sklearn.utils.extmath.safe_sparse_dot(a,b,dense_output=False)
sklearn.utils.extmath.softmax(X,copy=True)
sklearn.utils.extmath.squared_norm(x)
sklearn.utils.extmath.stable_cumsum(arr,axis=None,rtol=1e-05,atol=1e-08)
sklearn.utils.extmath.svd_flip(u,v,u_based_decision=True)
sklearn.utils.extmath.weighted_mode(a,w,axis=0)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/utils/multiclass.py----------------------------------------
A:sklearn.utils.multiclass.ys_types->set(['multiclass'])
A:sklearn.utils.multiclass.label_type->set(['multiclass']).pop()
A:sklearn.utils.multiclass._unique_labels->_FN_UNIQUE_LABELS.get(label_type, None)
A:sklearn.utils.multiclass.ys_labels->set(chain.from_iterable((_unique_labels(y) for y in ys)))
A:sklearn.utils.multiclass.y->y.tocsc().tocsc()
A:sklearn.utils.multiclass.labels->numpy.unique(y)
A:sklearn.utils.multiclass.y_type->type_of_target(y)
A:sklearn.utils.multiclass.clf.classes_->unique_labels(classes)
A:sklearn.utils.multiclass.y_nnz->numpy.diff(y.indptr)
A:sklearn.utils.multiclass.(classes_k, y_k)->numpy.unique(y[:, k], return_inverse=True)
A:sklearn.utils.multiclass.class_prior_k->numpy.bincount(y_k, weights=sample_weight)
A:sklearn.utils.multiclass.classes_k->numpy.insert(classes_k, 0, 0)
A:sklearn.utils.multiclass.votes->numpy.zeros((n_samples, n_classes))
A:sklearn.utils.multiclass.sum_of_confidences->numpy.zeros((n_samples, n_classes))
A:sklearn.utils.multiclass.max_confidences->numpy.zeros((n_samples, n_classes)).max()
A:sklearn.utils.multiclass.min_confidences->numpy.zeros((n_samples, n_classes)).min()
A:sklearn.utils.multiclass.max_abs_confidence->max(abs(max_confidences), abs(min_confidences))
sklearn.utils.multiclass._check_partial_fit_first_call(clf,classes=None)
sklearn.utils.multiclass._is_integral_float(y)
sklearn.utils.multiclass._ovr_decision_function(predictions,confidences,n_classes)
sklearn.utils.multiclass._unique_indicator(y)
sklearn.utils.multiclass._unique_multiclass(y)
sklearn.utils.multiclass.check_classification_targets(y)
sklearn.utils.multiclass.class_distribution(y,sample_weight=None)
sklearn.utils.multiclass.is_multilabel(y)
sklearn.utils.multiclass.type_of_target(y)
sklearn.utils.multiclass.unique_labels(*ys)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/utils/bench.py----------------------------------------
sklearn.utils.bench.total_seconds(delta)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/utils/deprecation.py----------------------------------------
A:sklearn.utils.deprecation.wrapped.__doc__->self._update_doc(wrapped.__doc__)
A:sklearn.utils.deprecation.closures->getattr(func, '__closure__', [])
sklearn.utils.deprecated(self,extra='')
sklearn.utils.deprecated._decorate_class(self,cls)
sklearn.utils.deprecated._decorate_fun(self,fun)
sklearn.utils.deprecated._update_doc(self,olddoc)
sklearn.utils.deprecation.DeprecationDict(self,*args,**kwargs)
sklearn.utils.deprecation.DeprecationDict.__getitem__(self,key)
sklearn.utils.deprecation.DeprecationDict.__init__(self,*args,**kwargs)
sklearn.utils.deprecation.DeprecationDict.add_warning(self,key,*args,**kwargs)
sklearn.utils.deprecation.DeprecationDict.get(self,key,default=None)
sklearn.utils.deprecation._is_deprecated(func)
sklearn.utils.deprecation.deprecated(self,extra='')
sklearn.utils.deprecation.deprecated.__init__(self,extra='')
sklearn.utils.deprecation.deprecated._decorate_class(self,cls)
sklearn.utils.deprecation.deprecated._decorate_fun(self,fun)
sklearn.utils.deprecation.deprecated._update_doc(self,olddoc)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/utils/_scipy_sparse_lsqr_backport.py----------------------------------------
A:sklearn.utils._scipy_sparse_lsqr_backport.A->aslinearoperator(A)
A:sklearn.utils._scipy_sparse_lsqr_backport.b->b.squeeze().squeeze()
A:sklearn.utils._scipy_sparse_lsqr_backport.var->numpy.zeros(n)
A:sklearn.utils._scipy_sparse_lsqr_backport.__xm->numpy.zeros(m)
A:sklearn.utils._scipy_sparse_lsqr_backport.__xn->numpy.zeros(n)
A:sklearn.utils._scipy_sparse_lsqr_backport.v->aslinearoperator(A).rmatvec(u)
A:sklearn.utils._scipy_sparse_lsqr_backport.x->numpy.zeros(n)
A:sklearn.utils._scipy_sparse_lsqr_backport.beta->numpy.linalg.norm(u)
A:sklearn.utils._scipy_sparse_lsqr_backport.w->aslinearoperator(A).rmatvec(u).copy()
A:sklearn.utils._scipy_sparse_lsqr_backport.alfa->numpy.linalg.norm(v)
A:sklearn.utils._scipy_sparse_lsqr_backport.anorm->sqrt(anorm ** 2 + alfa ** 2 + beta ** 2 + damp ** 2)
A:sklearn.utils._scipy_sparse_lsqr_backport.rhobar1->sqrt(rhobar ** 2 + damp ** 2)
A:sklearn.utils._scipy_sparse_lsqr_backport.(cs, sn, rho)->_sym_ortho(rhobar1, beta)
A:sklearn.utils._scipy_sparse_lsqr_backport.xnorm->sqrt(xxnorm + zbar ** 2)
A:sklearn.utils._scipy_sparse_lsqr_backport.gamma->sqrt(gambar ** 2 + theta ** 2)
A:sklearn.utils._scipy_sparse_lsqr_backport.rnorm->sqrt(res1 + res2)
A:sklearn.utils._scipy_sparse_lsqr_backport.r1norm->sqrt(abs(r1sq))
sklearn.utils._scipy_sparse_lsqr_backport._sym_ortho(a,b)
sklearn.utils._scipy_sparse_lsqr_backport.lsqr(A,b,damp=0.0,atol=1e-08,btol=1e-08,conlim=100000000.0,iter_lim=None,show=False,calc_var=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/utils/class_weight.py----------------------------------------
A:sklearn.utils.class_weight.weight->numpy.ones(classes.shape[0], dtype=np.float64, order='C')
A:sklearn.utils.class_weight.le->LabelEncoder()
A:sklearn.utils.class_weight.y_ind->LabelEncoder().fit_transform(y)
A:sklearn.utils.class_weight.i->numpy.searchsorted(classes, c)
A:sklearn.utils.class_weight.y->numpy.reshape(y, (-1, 1))
A:sklearn.utils.class_weight.classes_full->numpy.unique(y_full)
A:sklearn.utils.class_weight.classes_subsample->numpy.unique(y_subsample)
A:sklearn.utils.class_weight.weight_k->compute_class_weight(class_weight_k, classes_full, y_full)
A:sklearn.utils.class_weight.expanded_class_weight->numpy.prod(expanded_class_weight, axis=0, dtype=np.float64)
sklearn.utils.class_weight.compute_class_weight(class_weight,classes,y)
sklearn.utils.class_weight.compute_sample_weight(class_weight,y,indices=None)
sklearn.utils.compute_class_weight(class_weight,classes,y)
sklearn.utils.compute_sample_weight(class_weight,y,indices=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/utils/random.py----------------------------------------
A:sklearn.utils.random.random_state->check_random_state(random_state)
A:sklearn.utils.random.data->array.array('i')
A:sklearn.utils.random.indices->array.array('i')
A:sklearn.utils.random.indptr->array.array('i', [0])
A:sklearn.utils.random.classes[j]->numpy.insert(classes[j], 0, 0)
A:sklearn.utils.random.class_prob_j->numpy.insert(class_prob_j, 0, 0.0)
A:sklearn.utils.random.rng->check_random_state(random_state)
A:sklearn.utils.random.nnz->int(n_samples * p_nonzero)
A:sklearn.utils.random.ind_sample->sample_without_replacement(n_population=n_samples, n_samples=nnz, random_state=random_state)
A:sklearn.utils.random.classes_ind->numpy.searchsorted(class_probability_nz_norm.cumsum(), rng.rand(nnz))
sklearn.utils.random.choice(a,size=None,replace=True,p=None,random_state=None)
sklearn.utils.random.random_choice_csc(n_samples,classes,class_probability=None,random_state=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/utils/linear_assignment_.py----------------------------------------
A:sklearn.utils.linear_assignment_.indices->numpy.array(indices, dtype=int)
A:sklearn.utils.linear_assignment_.cost_matrix->numpy.atleast_2d(cost_matrix)
A:sklearn.utils.linear_assignment_.self.C->numpy.atleast_2d(cost_matrix).copy()
A:sklearn.utils.linear_assignment_.self.row_uncovered->numpy.ones(n, dtype=np.bool)
A:sklearn.utils.linear_assignment_.self.col_uncovered->numpy.ones(m, dtype=np.bool)
A:sklearn.utils.linear_assignment_.self.path->numpy.zeros((n + m, 2), dtype=int)
A:sklearn.utils.linear_assignment_.self.marked->numpy.zeros((n, m), dtype=int)
A:sklearn.utils.linear_assignment_.col->numpy.argmax(state.marked[path[count, 0]] == 2)
A:sklearn.utils.linear_assignment_.state->_HungarianState(cost_matrix)
A:sklearn.utils.linear_assignment_.step->step(state)
A:sklearn.utils.linear_assignment_.C->(state.C == 0).astype(np.int)
A:sklearn.utils.linear_assignment_.(row, col)->numpy.unravel_index(np.argmax(covered_C), (n, m))
A:sklearn.utils.linear_assignment_.star_col->numpy.argmax(state.marked[row] == 1)
A:sklearn.utils.linear_assignment_.row->numpy.argmax(state.marked[:, path[count, 1]] == 1)
A:sklearn.utils.linear_assignment_.minval->numpy.min(minval[state.col_uncovered])
sklearn.utils.linear_assignment_._HungarianState(self,cost_matrix)
sklearn.utils.linear_assignment_._HungarianState.__init__(self,cost_matrix)
sklearn.utils.linear_assignment_._HungarianState._clear_covers(self)
sklearn.utils.linear_assignment_._HungarianState._find_prime_in_row(self,row)
sklearn.utils.linear_assignment_._hungarian(cost_matrix)
sklearn.utils.linear_assignment_._step1(state)
sklearn.utils.linear_assignment_._step3(state)
sklearn.utils.linear_assignment_._step4(state)
sklearn.utils.linear_assignment_._step5(state)
sklearn.utils.linear_assignment_._step6(state)
sklearn.utils.linear_assignment_.linear_assignment(X)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/utils/sparsefuncs.py----------------------------------------
A:sklearn.utils.sparsefuncs.X.indices->numpy.concatenate([X.indices[:m_start], X.indices[n_start:n_stop], X.indices[m_stop:n_start], X.indices[m_start:m_stop], X.indices[n_stop:]])
A:sklearn.utils.sparsefuncs.X.data->numpy.concatenate([X.data[:m_start], X.data[n_start:n_stop], X.data[m_stop:n_start], X.data[m_start:m_stop], X.data[n_stop:]])
A:sklearn.utils.sparsefuncs.out->numpy.diff(X.indptr)
A:sklearn.utils.sparsefuncs.weights->numpy.repeat(sample_weight, np.diff(X.indptr))
A:sklearn.utils.sparsefuncs.n_negative->numpy.count_nonzero(data < 0)
A:sklearn.utils.sparsefuncs.(middle, is_odd)->divmod(n_elems, 2)
A:sklearn.utils.sparsefuncs.median->numpy.zeros(n_features)
A:sklearn.utils.sparsefuncs.data->numpy.copy(X.data[start:end])
A:sklearn.utils.sparsefuncs.median[f_ind]->_get_median(data, nz)
sklearn.utils.sparsefuncs._get_elem_at_rank(rank,data,n_negative,n_zeros)
sklearn.utils.sparsefuncs._get_median(data,n_zeros)
sklearn.utils.sparsefuncs._raise_error_wrong_axis(axis)
sklearn.utils.sparsefuncs._raise_typeerror(X)
sklearn.utils.sparsefuncs.count_nonzero(X,axis=None,sample_weight=None)
sklearn.utils.sparsefuncs.csc_median_axis_0(X)
sklearn.utils.sparsefuncs.incr_mean_variance_axis(X,axis,last_mean,last_var,last_n)
sklearn.utils.sparsefuncs.inplace_column_scale(X,scale)
sklearn.utils.sparsefuncs.inplace_csr_column_scale(X,scale)
sklearn.utils.sparsefuncs.inplace_csr_row_scale(X,scale)
sklearn.utils.sparsefuncs.inplace_row_scale(X,scale)
sklearn.utils.sparsefuncs.inplace_swap_column(X,m,n)
sklearn.utils.sparsefuncs.inplace_swap_row(X,m,n)
sklearn.utils.sparsefuncs.inplace_swap_row_csc(X,m,n)
sklearn.utils.sparsefuncs.inplace_swap_row_csr(X,m,n)
sklearn.utils.sparsefuncs.mean_variance_axis(X,axis)
sklearn.utils.sparsefuncs.min_max_axis(X,axis)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/utils/estimator_checks.py----------------------------------------
A:sklearn.utils.estimator_checks.estimator->clone(estimator_orig)
A:sklearn.utils.estimator_checks.rng->numpy.random.RandomState(0)
A:sklearn.utils.estimator_checks.X->numpy.array([[3, 0], [0, 1], [0, 2], [1, 1], [1, 2], [2, 1]])
A:sklearn.utils.estimator_checks.y->multioutput_estimator_convert_y_2d(estimator_orig, y)
A:sklearn.utils.estimator_checks.boston->load_boston()
A:sklearn.utils.estimator_checks.(X, y)->make_blobs(n_samples=100, random_state=0, n_features=4, centers=centers, cluster_std=1.0, shuffle=True)
A:sklearn.utils.estimator_checks.params->clone(estimator_orig).get_params()
A:sklearn.utils.estimator_checks.X_csr->scipy.sparse.csr_matrix(X)
A:sklearn.utils.estimator_checks.pred->pickle.loads(pickle.dumps(est)).predict(X)
A:sklearn.utils.estimator_checks.probs->clone(estimator_orig).predict_proba(X)
A:sklearn.utils.estimator_checks.weights->pandas.Series([1] * 6)
A:sklearn.utils.estimator_checks.rnd->numpy.random.RandomState(0)
A:sklearn.utils.estimator_checks.dict_before->clone(estimator_orig).__dict__.copy()
A:sklearn.utils.estimator_checks.dict_before_fit->clone(estimator_orig).__dict__.copy()
A:sklearn.utils.estimator_checks.this_X->NotAnArray(X)
A:sklearn.utils.estimator_checks.this_y->NotAnArray(np.asarray(y))
A:sklearn.utils.estimator_checks.transformer->clone(transformer_orig)
A:sklearn.utils.estimator_checks.transformer_clone->clone(transformer)
A:sklearn.utils.estimator_checks.X_pred->clone(transformer).fit_transform(X, y=y_)
A:sklearn.utils.estimator_checks.X_pred2->clone(clusterer_orig).fit(X).predict(X)
A:sklearn.utils.estimator_checks.X_pred3->clone(transformer_orig).fit_transform(X, y=y_)
A:sklearn.utils.estimator_checks.pipeline->make_pipeline(estimator)
A:sklearn.utils.estimator_checks.func->getattr(regressor, func_name, None)
A:sklearn.utils.estimator_checks.func_pipeline->getattr(pipeline, func_name)
A:sklearn.utils.estimator_checks.result->dict()
A:sklearn.utils.estimator_checks.result_pipe->func_pipeline(X, y)
A:sklearn.utils.estimator_checks.X_train_64->X_train_32.astype(np.float64)
A:sklearn.utils.estimator_checks.X_train_int_64->X_train_32.astype(np.int64)
A:sklearn.utils.estimator_checks.X_train_int_32->X_train_32.astype(np.int32)
A:sklearn.utils.estimator_checks.e->clone(estimator_orig)
A:sklearn.utils.estimator_checks.X_zero_samples->numpy.empty(0).reshape(0, 3)
A:sklearn.utils.estimator_checks.X_zero_features->numpy.empty(0).reshape(3, 0)
A:sklearn.utils.estimator_checks.X_train_finite->numpy.random.RandomState(0).uniform(size=(10, 3))
A:sklearn.utils.estimator_checks.X_train_nan->numpy.random.RandomState(0).uniform(size=(10, 3))
A:sklearn.utils.estimator_checks.X_train_inf->numpy.random.RandomState(0).uniform(size=(10, 3))
A:sklearn.utils.estimator_checks.result[method]->getattr(estimator, method)(X)
A:sklearn.utils.estimator_checks.pickled_estimator->pickle.dumps(estimator)
A:sklearn.utils.estimator_checks.unpickled_estimator->pickle.loads(pickled_estimator)
A:sklearn.utils.estimator_checks.unpickled_result->getattr(unpickled_estimator, method)(X)
A:sklearn.utils.estimator_checks.classes->numpy.unique(y_)
A:sklearn.utils.estimator_checks.clusterer->clone(clusterer_orig)
A:sklearn.utils.estimator_checks.pred2->clone(estimator_orig).predict(X)
A:sklearn.utils.estimator_checks.X_pred1->clone(clusterer_orig).fit(X).predict(X)
A:sklearn.utils.estimator_checks.X_train->numpy.random.RandomState(0).uniform(size=(10, 3))
A:sklearn.utils.estimator_checks.X_test->numpy.random.RandomState(0).uniform(size=(10, 3))
A:sklearn.utils.estimator_checks.classifier->LinearDiscriminantAnalysis()
A:sklearn.utils.estimator_checks.(X_m, y_m)->shuffle(X_m, y_m, random_state=7)
A:sklearn.utils.estimator_checks.X_m->StandardScaler().fit_transform(X_m)
A:sklearn.utils.estimator_checks.n_classes->float(len(np.unique(y)))
A:sklearn.utils.estimator_checks.y_pred->LinearDiscriminantAnalysis().predict(X_test)
A:sklearn.utils.estimator_checks.decision->LinearDiscriminantAnalysis().decision_function(X)
A:sklearn.utils.estimator_checks.dec_pred->(decision.ravel() > 0).astype(np.int)
A:sklearn.utils.estimator_checks.y_prob->LinearDiscriminantAnalysis().predict_proba(X)
A:sklearn.utils.estimator_checks.y_log_prob->LinearDiscriminantAnalysis().predict_log_proba(X)
A:sklearn.utils.estimator_checks.est->pickle.loads(pickle.dumps(est))
A:sklearn.utils.estimator_checks.y_pred_2d->clone(estimator_orig).predict(X)
A:sklearn.utils.estimator_checks.(X, _)->_boston_subset()
A:sklearn.utils.estimator_checks.regressor_1->clone(regressor_orig)
A:sklearn.utils.estimator_checks.regressor_2->clone(regressor_orig)
A:sklearn.utils.estimator_checks.y_->multioutput_estimator_convert_y_2d(estimator, y_)
A:sklearn.utils.estimator_checks.pred1->clone(estimator_orig).predict(X_)
A:sklearn.utils.estimator_checks.regressor->clone(regressor_orig)
A:sklearn.utils.estimator_checks.(X_train, X_test, y_train, y_test)->train_test_split(X, y, test_size=0.5, random_state=0)
A:sklearn.utils.estimator_checks.n_centers->len(np.unique(y_train))
A:sklearn.utils.estimator_checks.y_pred_balanced->LinearDiscriminantAnalysis().predict(X_test)
A:sklearn.utils.estimator_checks.coef_balanced->LinearDiscriminantAnalysis().fit(X, y).coef_.copy()
A:sklearn.utils.estimator_checks.n_samples->len(y)
A:sklearn.utils.estimator_checks.coef_manual->LinearDiscriminantAnalysis().fit(X, y).coef_.copy()
A:sklearn.utils.estimator_checks.original_params->deepcopy(params)
A:sklearn.utils.estimator_checks.new_params->clone(estimator_orig).get_params()
A:sklearn.utils.estimator_checks.pred_orig->pickle.loads(pickle.dumps(est)).predict(X)
A:sklearn.utils.estimator_checks.estimator_1->clone(estimator_orig)
A:sklearn.utils.estimator_checks.estimator_2->clone(estimator_orig)
A:sklearn.utils.estimator_checks.X_->NotAnArray(np.asarray(X))
A:sklearn.utils.estimator_checks.init->getattr(estimator.__init__, 'deprecated_original', estimator.__init__)
A:sklearn.utils.estimator_checks.iris->load_iris()
A:sklearn.utils.estimator_checks.(X, y_)->make_blobs(n_samples=30, centers=[[0, 0, 0], [1, 1, 1]], random_state=0, n_features=2, cluster_std=0.1)
A:sklearn.utils.estimator_checks.shallow_params->clone(estimator_orig).get_params(deep=False)
A:sklearn.utils.estimator_checks.deep_params->clone(estimator_orig).get_params(deep=True)
A:sklearn.utils.estimator_checks.b->clone(estimator_orig).decision_function(X_test)
sklearn.utils.estimator_checks.NotAnArray(self,data)
sklearn.utils.estimator_checks.NotAnArray.__array__(self,dtype=None)
sklearn.utils.estimator_checks.NotAnArray.__init__(self,data)
sklearn.utils.estimator_checks._boston_subset(n_samples=200)
sklearn.utils.estimator_checks._check_transformer(name,transformer_orig,X,y)
sklearn.utils.estimator_checks._is_32bit()
sklearn.utils.estimator_checks._yield_all_checks(name,estimator)
sklearn.utils.estimator_checks._yield_classifier_checks(name,classifier)
sklearn.utils.estimator_checks._yield_clustering_checks(name,clusterer)
sklearn.utils.estimator_checks._yield_non_meta_checks(name,estimator)
sklearn.utils.estimator_checks._yield_regressor_checks(name,regressor)
sklearn.utils.estimator_checks._yield_transformer_checks(name,transformer)
sklearn.utils.estimator_checks.check_class_weight_balanced_classifiers(name,classifier_orig,X_train,y_train,X_test,y_test,weights)
sklearn.utils.estimator_checks.check_class_weight_balanced_linear_classifier(name,Classifier)
sklearn.utils.estimator_checks.check_class_weight_classifiers(name,classifier_orig)
sklearn.utils.estimator_checks.check_classifier_data_not_an_array(name,estimator_orig)
sklearn.utils.estimator_checks.check_classifiers_classes(name,classifier_orig)
sklearn.utils.estimator_checks.check_classifiers_one_label(name,classifier_orig)
sklearn.utils.estimator_checks.check_classifiers_regression_target(name,estimator_orig)
sklearn.utils.estimator_checks.check_classifiers_train(name,classifier_orig)
sklearn.utils.estimator_checks.check_clusterer_compute_labels_predict(name,clusterer_orig)
sklearn.utils.estimator_checks.check_clustering(name,clusterer_orig)
sklearn.utils.estimator_checks.check_decision_proba_consistency(name,estimator_orig)
sklearn.utils.estimator_checks.check_dict_unchanged(name,estimator_orig)
sklearn.utils.estimator_checks.check_dont_overwrite_parameters(name,estimator_orig)
sklearn.utils.estimator_checks.check_dtype_object(name,estimator_orig)
sklearn.utils.estimator_checks.check_estimator(Estimator)
sklearn.utils.estimator_checks.check_estimator_sparse_data(name,estimator_orig)
sklearn.utils.estimator_checks.check_estimators_data_not_an_array(name,estimator_orig,X,y)
sklearn.utils.estimator_checks.check_estimators_dtypes(name,estimator_orig)
sklearn.utils.estimator_checks.check_estimators_empty_data_messages(name,estimator_orig)
sklearn.utils.estimator_checks.check_estimators_fit_returns_self(name,estimator_orig)
sklearn.utils.estimator_checks.check_estimators_nan_inf(name,estimator_orig)
sklearn.utils.estimator_checks.check_estimators_overwrite_params(name,estimator_orig)
sklearn.utils.estimator_checks.check_estimators_partial_fit_n_features(name,estimator_orig)
sklearn.utils.estimator_checks.check_estimators_pickle(name,estimator_orig)
sklearn.utils.estimator_checks.check_estimators_unfitted(name,estimator_orig)
sklearn.utils.estimator_checks.check_fit1d_1feature(name,estimator_orig)
sklearn.utils.estimator_checks.check_fit1d_1sample(name,estimator_orig)
sklearn.utils.estimator_checks.check_fit2d_1feature(name,estimator_orig)
sklearn.utils.estimator_checks.check_fit2d_1sample(name,estimator_orig)
sklearn.utils.estimator_checks.check_fit2d_predict1d(name,estimator_orig)
sklearn.utils.estimator_checks.check_fit_score_takes_y(name,estimator_orig)
sklearn.utils.estimator_checks.check_get_params_invariance(name,estimator_orig)
sklearn.utils.estimator_checks.check_no_fit_attributes_set_in_init(name,Estimator)
sklearn.utils.estimator_checks.check_non_transformer_estimators_n_iter(name,estimator_orig)
sklearn.utils.estimator_checks.check_parameters_default_constructible(name,Estimator)
sklearn.utils.estimator_checks.check_pipeline_consistency(name,estimator_orig)
sklearn.utils.estimator_checks.check_regressor_data_not_an_array(name,estimator_orig)
sklearn.utils.estimator_checks.check_regressors_int(name,regressor_orig)
sklearn.utils.estimator_checks.check_regressors_no_decision_function(name,regressor_orig)
sklearn.utils.estimator_checks.check_regressors_train(name,regressor_orig)
sklearn.utils.estimator_checks.check_sample_weights_list(name,estimator_orig)
sklearn.utils.estimator_checks.check_sample_weights_pandas_series(name,estimator_orig)
sklearn.utils.estimator_checks.check_sparsify_coefficients(name,estimator_orig)
sklearn.utils.estimator_checks.check_supervised_y_2d(name,estimator_orig)
sklearn.utils.estimator_checks.check_supervised_y_no_nan(name,estimator_orig)
sklearn.utils.estimator_checks.check_transformer_data_not_an_array(name,transformer)
sklearn.utils.estimator_checks.check_transformer_general(name,transformer)
sklearn.utils.estimator_checks.check_transformer_n_iter(name,estimator_orig)
sklearn.utils.estimator_checks.check_transformers_unfitted(name,transformer)
sklearn.utils.estimator_checks.is_public_parameter(attr)
sklearn.utils.estimator_checks.multioutput_estimator_convert_y_2d(estimator,y)
sklearn.utils.estimator_checks.set_checking_parameters(estimator)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/utils/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/utils/tests/test_fast_dict.py----------------------------------------
A:sklearn.utils.tests.test_fast_dict.rng->numpy.random.RandomState(0)
A:sklearn.utils.tests.test_fast_dict.keys->numpy.arange(100, dtype=np.intp)
A:sklearn.utils.tests.test_fast_dict.values->numpy.arange(100, dtype=np.float64)
A:sklearn.utils.tests.test_fast_dict.d->IntFloatDict(keys, values)
sklearn.utils.tests.test_fast_dict.test_int_float_dict()
sklearn.utils.tests.test_fast_dict.test_int_float_dict_argmin()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/utils/tests/test_optimize.py----------------------------------------
A:sklearn.utils.tests.test_optimize.rng->numpy.random.RandomState(0)
A:sklearn.utils.tests.test_optimize.A->numpy.random.RandomState(0).normal(size=(10, 10))
A:sklearn.utils.tests.test_optimize.x0->numpy.ones(10)
A:sklearn.utils.tests.test_optimize.Ax->numpy.random.RandomState(0).normal(size=(10, 10)).dot(x)
sklearn.utils.tests.test_optimize.test_newton_cg()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/utils/tests/test_graph.py----------------------------------------
A:sklearn.utils.tests.test_graph.sp_mat->scipy.sparse.csr_matrix(mat)
A:sklearn.utils.tests.test_graph.laplacian->graph_laplacian(mat, normed=normed)
sklearn.utils.tests.test_graph.test_graph_laplacian()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/utils/tests/test_validation.py----------------------------------------
A:sklearn.utils.tests.test_validation.X->numpy.array([0, np.inf])
A:sklearn.utils.tests.test_validation.X2->as_float_array(X, copy=True)
A:sklearn.utils.tests.test_validation.N->as_float_array(M, copy=True)
A:sklearn.utils.tests.test_validation.M->numpy.memmap(tmp, shape=(10, 10), dtype=np.float32)
A:sklearn.utils.tests.test_validation.B->check_array(A, order='F', copy=copy)
A:sklearn.utils.tests.test_validation.X_csr->scipy.sparse.csr_matrix(X)
A:sklearn.utils.tests.test_validation.X_array->check_array([0, 1, 2], ensure_2d=False)
A:sklearn.utils.tests.test_validation.X_ndim->numpy.arange(8).reshape(2, 2, 2)
A:sklearn.utils.tests.test_validation.X_inf->numpy.arange(4).reshape(2, 2).astype(np.float)
A:sklearn.utils.tests.test_validation.X_nan->numpy.arange(4).reshape(2, 2).astype(np.float)
A:sklearn.utils.tests.test_validation.X_C->numpy.arange(4).reshape(2, 2).copy('C')
A:sklearn.utils.tests.test_validation.X_F->numpy.arange(4).reshape(2, 2).copy('C').copy('F')
A:sklearn.utils.tests.test_validation.X_int->scipy.sparse.csc_matrix(X_C).astype(np.int)
A:sklearn.utils.tests.test_validation.X_float->scipy.sparse.csc_matrix(X_C).astype(np.float)
A:sklearn.utils.tests.test_validation.X_checked->assert_no_warnings(check_array, X_csc_float32, dtype=[np.float64, np.float32], accept_sparse=['csr', 'dok'], copy=False)
A:sklearn.utils.tests.test_validation.X_csc->scipy.sparse.csc_matrix(X_C)
A:sklearn.utils.tests.test_validation.X_coo->scipy.sparse.csc_matrix(X_C).tocoo()
A:sklearn.utils.tests.test_validation.X_dok->scipy.sparse.csc_matrix(X_C).todok()
A:sklearn.utils.tests.test_validation.message->str(w[0].message)
A:sklearn.utils.tests.test_validation.X_dense->check_array([[1, 2], [3, 4]])
A:sklearn.utils.tests.test_validation.X_no_array->NotAnArray(X_dense)
A:sklearn.utils.tests.test_validation.result->check_array(X_no_array)
A:sklearn.utils.tests.test_validation.X_df->pandas.DataFrame(X, columns=['a', 'b', 'fit'])
A:sklearn.utils.tests.test_validation.arr->numpy.array([[0.2, 0.7], [0.6, 0.5], [0.4, 0.1], [0.7, 0.2]])
A:sklearn.utils.tests.test_validation.mock_df->MockDataFrame(arr)
A:sklearn.utils.tests.test_validation.checked_arr->check_array(mock_df, dtype=np.float32)
A:sklearn.utils.tests.test_validation.X_float64->numpy.asarray(X_int_list, dtype=np.float64)
A:sklearn.utils.tests.test_validation.X_float32->numpy.asarray(X_int_list, dtype=np.float32)
A:sklearn.utils.tests.test_validation.X_int64->numpy.asarray(X_int_list, dtype=np.int64)
A:sklearn.utils.tests.test_validation.X_csr_float64->scipy.sparse.csr_matrix(X_float64)
A:sklearn.utils.tests.test_validation.X_csr_float32->scipy.sparse.csr_matrix(X_float32)
A:sklearn.utils.tests.test_validation.X_csc_float32->scipy.sparse.csc_matrix(X_float32)
A:sklearn.utils.tests.test_validation.X_csc_int32->scipy.sparse.csc_matrix(X_int64, dtype=np.int32)
A:sklearn.utils.tests.test_validation.(X_checked, y_checked)->check_X_y(X, y, allow_nd=True)
A:sklearn.utils.tests.test_validation.invalid_type->SVR()
A:sklearn.utils.tests.test_validation.y->numpy.ones(10)
A:sklearn.utils.tests.test_validation.arr_sym->numpy.array([[0, 1], [1, 2]])
A:sklearn.utils.tests.test_validation.arr_bad->numpy.ones(2)
A:sklearn.utils.tests.test_validation.arr_asym->numpy.array([[0, 2], [0, 2]])
A:sklearn.utils.tests.test_validation.output->check_symmetric(arr, raise_warning=False)
A:sklearn.utils.tests.test_validation.ard->ARDRegression()
A:sklearn.utils.tests.test_validation.svr->SVR()
A:sklearn.utils.tests.test_validation.memory->check_memory(dummy)
A:sklearn.utils.tests.test_validation.dummy->WrongDummyMemory()
sklearn.utils.tests.test_validation.DummyMemory(object)
sklearn.utils.tests.test_validation.DummyMemory.cache(self,func)
sklearn.utils.tests.test_validation.WrongDummyMemory(object)
sklearn.utils.tests.test_validation.test_as_float_array()
sklearn.utils.tests.test_validation.test_check_array()
sklearn.utils.tests.test_validation.test_check_array_accept_sparse_no_exception()
sklearn.utils.tests.test_validation.test_check_array_accept_sparse_type_exception()
sklearn.utils.tests.test_validation.test_check_array_dtype_stability()
sklearn.utils.tests.test_validation.test_check_array_dtype_warning()
sklearn.utils.tests.test_validation.test_check_array_min_samples_and_features_messages()
sklearn.utils.tests.test_validation.test_check_array_on_mock_dataframe()
sklearn.utils.tests.test_validation.test_check_array_pandas_dtype_object_conversion()
sklearn.utils.tests.test_validation.test_check_consistent_length()
sklearn.utils.tests.test_validation.test_check_dataframe_fit_attribute()
sklearn.utils.tests.test_validation.test_check_is_fitted()
sklearn.utils.tests.test_validation.test_check_memory()
sklearn.utils.tests.test_validation.test_check_symmetric()
sklearn.utils.tests.test_validation.test_has_fit_parameter()
sklearn.utils.tests.test_validation.test_memmap()
sklearn.utils.tests.test_validation.test_np_matrix()
sklearn.utils.tests.test_validation.test_ordering()
sklearn.utils.tests.test_validation.test_suppress_validation()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/utils/tests/test_class_weight.py----------------------------------------
A:sklearn.utils.tests.test_class_weight.y->numpy.asarray([1, 1, 1, 2, 2, 2])
A:sklearn.utils.tests.test_class_weight.classes->numpy.array([1, 0, 3])
A:sklearn.utils.tests.test_class_weight.cw->compute_class_weight('balanced', classes, y)
A:sklearn.utils.tests.test_class_weight.(X, y)->make_blobs(centers=2, random_state=0)
A:sklearn.utils.tests.test_class_weight.X_1->numpy.vstack([X] + [X[y == 1]] * 2)
A:sklearn.utils.tests.test_class_weight.y_1->numpy.hstack([y] + [y[y == 1]] * 2)
A:sklearn.utils.tests.test_class_weight.X_0->numpy.vstack([X] + [X[y == 0]] * 2)
A:sklearn.utils.tests.test_class_weight.y_0->numpy.hstack([y] + [y[y == 0]] * 2)
A:sklearn.utils.tests.test_class_weight.X_->numpy.vstack([X] * 2)
A:sklearn.utils.tests.test_class_weight.y_->numpy.asarray([[1, 0], [1, 0], [1, 0], [2, 1], [2, 1], [2, 1]])
A:sklearn.utils.tests.test_class_weight.logreg1->LogisticRegression(class_weight='balanced').fit(X_1, y_1)
A:sklearn.utils.tests.test_class_weight.logreg0->LogisticRegression(class_weight='balanced').fit(X_0, y_0)
A:sklearn.utils.tests.test_class_weight.logreg->LogisticRegression(class_weight='balanced').fit(X_, y_)
A:sklearn.utils.tests.test_class_weight.class_counts->numpy.bincount(y + 2)
A:sklearn.utils.tests.test_class_weight.sample_weight->compute_sample_weight('balanced', y, range(6))
A:sklearn.utils.tests.test_class_weight.expected_balanced->numpy.asarray([0.6, 0.6, 0.6, 3.0, 3.0, 3.0])
sklearn.utils.tests.test_class_weight.test_compute_class_weight()
sklearn.utils.tests.test_class_weight.test_compute_class_weight_balanced_negative()
sklearn.utils.tests.test_class_weight.test_compute_class_weight_balanced_unordered()
sklearn.utils.tests.test_class_weight.test_compute_class_weight_dict()
sklearn.utils.tests.test_class_weight.test_compute_class_weight_invariance()
sklearn.utils.tests.test_class_weight.test_compute_class_weight_not_present()
sklearn.utils.tests.test_class_weight.test_compute_sample_weight()
sklearn.utils.tests.test_class_weight.test_compute_sample_weight_errors()
sklearn.utils.tests.test_class_weight.test_compute_sample_weight_with_subsample()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/utils/tests/test_deprecation.py----------------------------------------
A:sklearn.utils.tests.test_deprecation.val->assert_warns_message(DeprecationWarning, 'deprecated', mock_function)
A:sklearn.utils.tests.test_deprecation.dd->DeprecationDict()
sklearn.utils.tests.test_deprecation.MockClass1
sklearn.utils.tests.test_deprecation.MockClass2
sklearn.utils.tests.test_deprecation.MockClass2.method(self)
sklearn.utils.tests.test_deprecation.MockClass3(self)
sklearn.utils.tests.test_deprecation.MockClass3.__init__(self)
sklearn.utils.tests.test_deprecation.MockClass4
sklearn.utils.tests.test_deprecation.mock_function()
sklearn.utils.tests.test_deprecation.test_deprecated()
sklearn.utils.tests.test_deprecation.test_deprecationdict()
sklearn.utils.tests.test_deprecation.test_is_deprecated()
sklearn.utils.tests.test_deprecation.test_pickle()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/utils/tests/test_estimator_checks.py----------------------------------------
A:sklearn.utils.tests.test_estimator_checks.(X, y)->check_X_y(X, y, accept_sparse=('csr', 'csc'), multi_output=True, y_numeric=True)
A:sklearn.utils.tests.test_estimator_checks.X->check_array(X)
A:sklearn.utils.tests.test_estimator_checks.self.coef_->numpy.ones(X.shape[1])
A:sklearn.utils.tests.test_estimator_checks.string_buffer->StringIO()
A:sklearn.utils.tests.test_estimator_checks.iris->load_iris()
A:sklearn.utils.tests.test_estimator_checks.est->Estimator()
A:sklearn.utils.tests.test_estimator_checks.old_hash->sklearn.externals.joblib.hash(est)
sklearn.utils.tests.test_estimator_checks.BaseBadClassifier(BaseEstimator,ClassifierMixin)
sklearn.utils.tests.test_estimator_checks.BaseBadClassifier.fit(self,X,y)
sklearn.utils.tests.test_estimator_checks.BaseBadClassifier.predict(self,X)
sklearn.utils.tests.test_estimator_checks.ChangesDict(self)
sklearn.utils.tests.test_estimator_checks.ChangesDict.__init__(self)
sklearn.utils.tests.test_estimator_checks.ChangesDict.fit(self,X,y=None)
sklearn.utils.tests.test_estimator_checks.ChangesDict.predict(self,X)
sklearn.utils.tests.test_estimator_checks.ChangesWrongAttribute(self)
sklearn.utils.tests.test_estimator_checks.ChangesWrongAttribute.__init__(self)
sklearn.utils.tests.test_estimator_checks.ChangesWrongAttribute.fit(self,X,y=None)
sklearn.utils.tests.test_estimator_checks.CorrectNotFittedError(ValueError)
sklearn.utils.tests.test_estimator_checks.CorrectNotFittedErrorClassifier(BaseBadClassifier)
sklearn.utils.tests.test_estimator_checks.CorrectNotFittedErrorClassifier.fit(self,X,y)
sklearn.utils.tests.test_estimator_checks.CorrectNotFittedErrorClassifier.predict(self,X)
sklearn.utils.tests.test_estimator_checks.NoCheckinPredict(BaseBadClassifier)
sklearn.utils.tests.test_estimator_checks.NoCheckinPredict.fit(self,X,y)
sklearn.utils.tests.test_estimator_checks.NoSampleWeightPandasSeriesType(BaseEstimator)
sklearn.utils.tests.test_estimator_checks.NoSampleWeightPandasSeriesType.fit(self,X,y,sample_weight=None)
sklearn.utils.tests.test_estimator_checks.NoSampleWeightPandasSeriesType.predict(self,X)
sklearn.utils.tests.test_estimator_checks.NoSparseClassifier(BaseBadClassifier)
sklearn.utils.tests.test_estimator_checks.NoSparseClassifier.fit(self,X,y)
sklearn.utils.tests.test_estimator_checks.NoSparseClassifier.predict(self,X)
sklearn.utils.tests.test_estimator_checks.SetsWrongAttribute(self)
sklearn.utils.tests.test_estimator_checks.SetsWrongAttribute.__init__(self)
sklearn.utils.tests.test_estimator_checks.SetsWrongAttribute.fit(self,X,y=None)
sklearn.utils.tests.test_estimator_checks.test_check_estimator()
sklearn.utils.tests.test_estimator_checks.test_check_estimator_clones()
sklearn.utils.tests.test_estimator_checks.test_check_estimators_unfitted()
sklearn.utils.tests.test_estimator_checks.test_check_no_fit_attributes_set_in_init()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/utils/tests/test_murmurhash.py----------------------------------------
A:sklearn.utils.tests.test_murmurhash.rng->numpy.random.RandomState(42)
A:sklearn.utils.tests.test_murmurhash.keys->keys.reshape((3, 2, 1)).reshape((3, 2, 1))
A:sklearn.utils.tests.test_murmurhash.expected->expected.reshape(keys.shape).reshape(keys.shape)
A:sklearn.utils.tests.test_murmurhash.previous_hashes->set()
A:sklearn.utils.tests.test_murmurhash.h->murmurhash3_32(' ' * i, 0)
A:sklearn.utils.tests.test_murmurhash.bins->numpy.zeros(n_bins, dtype=np.float64)
sklearn.utils.tests.test_murmurhash.test_mmhash3_bytes()
sklearn.utils.tests.test_murmurhash.test_mmhash3_int()
sklearn.utils.tests.test_murmurhash.test_mmhash3_int_array()
sklearn.utils.tests.test_murmurhash.test_mmhash3_unicode()
sklearn.utils.tests.test_murmurhash.test_no_collision_on_byte_range()
sklearn.utils.tests.test_murmurhash.test_uniform_distribution()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/utils/tests/test_extmath.py----------------------------------------
A:sklearn.utils.tests.test_extmath.rng->numpy.random.RandomState(0)
A:sklearn.utils.tests.test_extmath.X->numpy.random.RandomState(0).randn(3, 5)
A:sklearn.utils.tests.test_extmath.X_csr->scipy.sparse.csr_matrix(X)
A:sklearn.utils.tests.test_extmath.X_csc->scipy.sparse.csc_matrix(X)
A:sklearn.utils.tests.test_extmath.X_coo->scipy.sparse.coo_matrix(X)
A:sklearn.utils.tests.test_extmath.X_lil->scipy.sparse.lil_matrix(X)
A:sklearn.utils.tests.test_extmath.x->numpy.linspace(-2, 2, 50)
A:sklearn.utils.tests.test_extmath.weights->numpy.ones(x.shape)
A:sklearn.utils.tests.test_extmath.(mode, score)->weighted_mode(x, w, axis=1)
A:sklearn.utils.tests.test_extmath.(mode2, score2)->weighted_mode(x, weights, axis)
A:sklearn.utils.tests.test_extmath.w->numpy.random.RandomState(0).random_sample(x.shape)
A:sklearn.utils.tests.test_extmath.logx->numpy.log(x)
A:sklearn.utils.tests.test_extmath.logX->numpy.vstack([logx, logx])
A:sklearn.utils.tests.test_extmath.dtype->numpy.dtype(dtype)
A:sklearn.utils.tests.test_extmath.(U, s, V)->randomized_svd(X, n_components, n_iter=i, power_iteration_normalizer=normalizer, random_state=0)
A:sklearn.utils.tests.test_extmath.U->U.astype(dtype, copy=False).astype(dtype, copy=False)
A:sklearn.utils.tests.test_extmath.s->s.astype(dtype, copy=False).astype(dtype, copy=False)
A:sklearn.utils.tests.test_extmath.V->V.astype(dtype, copy=False).astype(dtype, copy=False)
A:sklearn.utils.tests.test_extmath.(Ua, sa, Va)->randomized_svd(X, k, power_iteration_normalizer=normalizer, random_state=0)
A:sklearn.utils.tests.test_extmath.sq_norm->(X ** 2).sum(axis=1)
A:sklearn.utils.tests.test_extmath.Xcsr->scipy.sparse.csr_matrix(X, dtype=dtype)
A:sklearn.utils.tests.test_extmath.(_, s, _)->scipy.linalg.svd(X, full_matrices=False)
A:sklearn.utils.tests.test_extmath.(_, sa, _)->randomized_svd(X, k, n_iter=0, power_iteration_normalizer=normalizer)
A:sklearn.utils.tests.test_extmath.(_, sap, _)->randomized_svd(X, k, n_iter=5, power_iteration_normalizer=normalizer)
A:sklearn.utils.tests.test_extmath.(U1, s1, V1)->randomized_svd(X, k, n_iter=3, transpose=False, random_state=0)
A:sklearn.utils.tests.test_extmath.(U2, s2, V2)->randomized_svd(X, k, n_iter=3, transpose=True, random_state=0)
A:sklearn.utils.tests.test_extmath.(U3, s3, V3)->randomized_svd(X, k, n_iter=3, transpose='auto', random_state=0)
A:sklearn.utils.tests.test_extmath.(U4, s4, V4)->scipy.linalg.svd(X, full_matrices=False)
A:sklearn.utils.tests.test_extmath.error_2->scipy.linalg.norm(A, ord='fro')
A:sklearn.utils.tests.test_extmath.error_20->scipy.linalg.norm(A, ord='fro')
A:sklearn.utils.tests.test_extmath.error->scipy.linalg.norm(A, ord='fro')
A:sklearn.utils.tests.test_extmath.rs->numpy.random.RandomState(1999)
A:sklearn.utils.tests.test_extmath.(U, S, V)->scipy.linalg.svd(XT, full_matrices=False)
A:sklearn.utils.tests.test_extmath.(U1, V1)->svd_flip(U, V, u_based_decision=False)
A:sklearn.utils.tests.test_extmath.(U2, V2)->svd_flip(U, V, u_based_decision=True)
A:sklearn.utils.tests.test_extmath.(U_flip1, V_flip1)->svd_flip(U, V, u_based_decision=True)
A:sklearn.utils.tests.test_extmath.(U_flip2, V_flip2)->svd_flip(U, V, u_based_decision=False)
A:sklearn.utils.tests.test_extmath.a->numpy.array([[2.0, 0.0], [0.0, 1.0]])
A:sklearn.utils.tests.test_extmath.(u1, s1, v1)->randomized_svd(a, 2, flip_sign=True, random_state=41)
A:sklearn.utils.tests.test_extmath.(u2, s2, v2)->randomized_svd(a, 2, flip_sign=True, random_state=seed)
A:sklearn.utils.tests.test_extmath.u_based->(np.abs(u).max(axis=0) == u.max(axis=0)).all()
A:sklearn.utils.tests.test_extmath.v_based->(np.abs(v).max(axis=1) == v.max(axis=1)).all()
A:sklearn.utils.tests.test_extmath.mat->numpy.arange(10 * 8).reshape(10, -1)
A:sklearn.utils.tests.test_extmath.(u_flipped, _, v_flipped)->randomized_svd(mat, 3, flip_sign=True)
A:sklearn.utils.tests.test_extmath.(u_based, v_based)->max_loading_is_positive(u_flipped_with_transpose, v_flipped_with_transpose)
A:sklearn.utils.tests.test_extmath.(u_flipped_with_transpose, _, v_flipped_with_transpose)->randomized_svd(mat, 3, flip_sign=True, transpose=True)
A:sklearn.utils.tests.test_extmath.true_out->numpy.array([[1, 4, 6], [1, 4, 7], [1, 5, 6], [1, 5, 7], [2, 4, 6], [2, 4, 7], [2, 5, 6], [2, 5, 7], [3, 4, 6], [3, 4, 7], [3, 5, 6], [3, 5, 7]])
A:sklearn.utils.tests.test_extmath.out->cartesian(axes)
A:sklearn.utils.tests.test_extmath.extreme_x->numpy.array([-100.0, 100.0])
A:sklearn.utils.tests.test_extmath.old_means->X1.mean(axis=0)
A:sklearn.utils.tests.test_extmath.old_variances->X1.var(axis=0)
A:sklearn.utils.tests.test_extmath.(final_means, final_variances, final_count)->_incremental_mean_and_var(X2, old_means, old_variances, old_sample_count)
A:sklearn.utils.tests.test_extmath.mean->numpy.random.RandomState(0).randn(3, 5).mean(axis=0)
A:sklearn.utils.tests.test_extmath.Y->numpy.random.RandomState(0).randn(3, 5).copy()
A:sklearn.utils.tests.test_extmath.x1->numpy.array(100000000.0, dtype=np.float64)
A:sklearn.utils.tests.test_extmath.x2->numpy.log(1e-05, dtype=np.float64)
A:sklearn.utils.tests.test_extmath.A->numpy.random.RandomState(36).randint(1000, size=(5, 5, 5))
A:sklearn.utils.tests.test_extmath.(mean, var, n)->_incremental_mean_and_var(A1[i, :].reshape((1, A1.shape[1])), mean, var, n)
A:sklearn.utils.tests.test_extmath.steps->numpy.hstack([steps, n_samples])
A:sklearn.utils.tests.test_extmath.incremental_means->batch.mean(axis=0)
A:sklearn.utils.tests.test_extmath.incremental_variances->batch.var(axis=0)
A:sklearn.utils.tests.test_extmath.result->_incremental_mean_and_var(batch, incremental_means, incremental_variances, sample_count)
A:sklearn.utils.tests.test_extmath.calculated_means->numpy.mean(X[:j], axis=0)
A:sklearn.utils.tests.test_extmath.calculated_variances->numpy.var(X[:j], axis=0)
A:sklearn.utils.tests.test_extmath.data->numpy.random.RandomState(36).randn(5, 5)
A:sklearn.utils.tests.test_extmath.max_abs_rows->numpy.argmax(np.abs(data), axis=1)
A:sklearn.utils.tests.test_extmath.data_flipped->_deterministic_vector_sign_flip(data)
A:sklearn.utils.tests.test_extmath.max_rows->numpy.argmax(data_flipped, axis=1)
A:sklearn.utils.tests.test_extmath.signs->numpy.sign(data[range(data.shape[0]), max_abs_rows])
A:sklearn.utils.tests.test_extmath.exp_X->numpy.exp(X)
A:sklearn.utils.tests.test_extmath.sum_exp_X->numpy.sum(exp_X, axis=1).reshape((-1, 1))
A:sklearn.utils.tests.test_extmath.r->numpy.random.RandomState(0).rand(100000)
sklearn.utils.tests.test_extmath.check_randomized_svd_low_rank(dtype)
sklearn.utils.tests.test_extmath.test_cartesian()
sklearn.utils.tests.test_extmath.test_density()
sklearn.utils.tests.test_extmath.test_incremental_variance_ddof()
sklearn.utils.tests.test_extmath.test_incremental_variance_numerical_stability()
sklearn.utils.tests.test_extmath.test_incremental_variance_update_formulas()
sklearn.utils.tests.test_extmath.test_logistic_sigmoid()
sklearn.utils.tests.test_extmath.test_logsumexp()
sklearn.utils.tests.test_extmath.test_norm_squared_norm()
sklearn.utils.tests.test_extmath.test_random_weights()
sklearn.utils.tests.test_extmath.test_randomized_svd_infinite_rank()
sklearn.utils.tests.test_extmath.test_randomized_svd_low_rank_all_dtypes()
sklearn.utils.tests.test_extmath.test_randomized_svd_low_rank_with_noise()
sklearn.utils.tests.test_extmath.test_randomized_svd_power_iteration_normalizer()
sklearn.utils.tests.test_extmath.test_randomized_svd_sign_flip()
sklearn.utils.tests.test_extmath.test_randomized_svd_sign_flip_with_transpose()
sklearn.utils.tests.test_extmath.test_randomized_svd_transpose_consistency()
sklearn.utils.tests.test_extmath.test_row_norms()
sklearn.utils.tests.test_extmath.test_softmax()
sklearn.utils.tests.test_extmath.test_stable_cumsum()
sklearn.utils.tests.test_extmath.test_svd_flip()
sklearn.utils.tests.test_extmath.test_uniform_weights()
sklearn.utils.tests.test_extmath.test_vector_sign_flip()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/utils/tests/test_stats.py----------------------------------------
A:sklearn.utils.tests.test_stats.r->rankdata(values, method=method)
sklearn.utils.tests.test_stats.test_cases()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/utils/tests/test_linear_assignment.py----------------------------------------
A:sklearn.utils.tests.test_linear_assignment.cost_matrix->numpy.array(cost_matrix)
A:sklearn.utils.tests.test_linear_assignment.indexes->_hungarian(cost_matrix.T)
sklearn.utils.tests.test_linear_assignment.test_hungarian()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/utils/tests/test_testing.py----------------------------------------
A:sklearn.utils.tests.test_testing.lda->LinearDiscriminantAnalysis()
A:sklearn.utils.tests.test_testing.tree->DecisionTreeClassifier()
A:sklearn.utils.tests.test_testing.x->numpy.arange(9).reshape(3, 3)
A:sklearn.utils.tests.test_testing.y->scipy.sparse.csc_matrix(x)
A:sklearn.utils.tests.test_testing.A->scipy.sparse.diags(np.ones(5), offsets=0).tocsr()
A:sklearn.utils.tests.test_testing.B->scipy.sparse.csr_matrix(np.ones((1, 5)))
A:sklearn.utils.tests.test_testing.incorrect->check_docstring_parameters(f_check_param_definition)
A:sklearn.utils.tests.test_testing.mock_meta->MockMetaEstimator(delegate=MockEst())
sklearn.utils.tests.test_testing.Klass(object)
sklearn.utils.tests.test_testing.Klass.f_bad_sections(self,X,y)
sklearn.utils.tests.test_testing.Klass.f_missing(self,X,y)
sklearn.utils.tests.test_testing.MockEst(self)
sklearn.utils.tests.test_testing.MockEst.__init__(self)
sklearn.utils.tests.test_testing.MockEst.fit(self,X,y)
sklearn.utils.tests.test_testing.MockEst.predict(self,X)
sklearn.utils.tests.test_testing.MockEst.predict_proba(self,X)
sklearn.utils.tests.test_testing.MockEst.score(self,X)
sklearn.utils.tests.test_testing.MockMetaEstimator(self,delegate)
sklearn.utils.tests.test_testing.MockMetaEstimator.__init__(self,delegate)
sklearn.utils.tests.test_testing.MockMetaEstimator.fit(self,X,y)
sklearn.utils.tests.test_testing.MockMetaEstimator.predict(self,X)
sklearn.utils.tests.test_testing.MockMetaEstimator.predict_log_proba(self,X)
sklearn.utils.tests.test_testing.MockMetaEstimator.predict_proba(self,X)
sklearn.utils.tests.test_testing.MockMetaEstimator.score(self,X)
sklearn.utils.tests.test_testing.TestWarns(unittest.TestCase)
sklearn.utils.tests.test_testing.TestWarns.test_warn(self)
sklearn.utils.tests.test_testing.TestWarns.test_warn_wrong_warning(self)
sklearn.utils.tests.test_testing.f_bad_order(b,a)
sklearn.utils.tests.test_testing.f_bad_sections(a,b)
sklearn.utils.tests.test_testing.f_check_param_definition(a,b,c,d)
sklearn.utils.tests.test_testing.f_missing(a,b)
sklearn.utils.tests.test_testing.f_ok(a,b)
sklearn.utils.tests.test_testing.test_assert_allclose_dense_sparse()
sklearn.utils.tests.test_testing.test_assert_greater()
sklearn.utils.tests.test_testing.test_assert_greater_equal()
sklearn.utils.tests.test_testing.test_assert_less()
sklearn.utils.tests.test_testing.test_assert_less_equal()
sklearn.utils.tests.test_testing.test_assert_raise_message()
sklearn.utils.tests.test_testing.test_check_docstring_parameters()
sklearn.utils.tests.test_testing.test_ignore_warning()
sklearn.utils.tests.test_testing.test_set_random_state()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/utils/tests/test_bench.py----------------------------------------
sklearn.utils.tests.test_bench.test_total_seconds()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/utils/tests/test_multiclass.py----------------------------------------
A:sklearn.utils.tests.test_multiclass.mix_clf_format->product(EXAMPLES['multilabel-indicator'], EXAMPLES['multiclass'] + EXAMPLES['binary'])
A:sklearn.utils.tests.test_multiclass.example->example.toarray().toarray()
A:sklearn.utils.tests.test_multiclass.y->numpy.array([[1, 0, 0, 1], [2, 2, 0, 1], [1, 3, 0, 1], [4, 2, 0, 1], [2, 0, 0, 1], [1, 3, 0, 1]])
A:sklearn.utils.tests.test_multiclass.data->numpy.array([1, 2, 1, 4, 2, 1, 0, 2, 3, 2, 3, 1, 1, 1, 1, 1, 1])
A:sklearn.utils.tests.test_multiclass.indices->numpy.array([0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 5, 0, 1, 2, 3, 4, 5])
A:sklearn.utils.tests.test_multiclass.indptr->numpy.array([0, 6, 11, 11, 17])
A:sklearn.utils.tests.test_multiclass.y_sp->scipy.sparse.csc_matrix((data, indices, indptr), shape=(6, 4))
A:sklearn.utils.tests.test_multiclass.(classes, n_classes, class_prior)->class_distribution(y, [1.0, 2.0, 1.0, 2.0, 1.0, 2.0])
A:sklearn.utils.tests.test_multiclass.(classes_sp, n_classes_sp, class_prior_sp)->class_distribution(y, [1.0, 2.0, 1.0, 2.0, 1.0, 2.0])
A:sklearn.utils.tests.test_multiclass.clf->SVC()
A:sklearn.utils.tests.test_multiclass.clfp->SVC(kernel='precomputed')
A:sklearn.utils.tests.test_multiclass.iris->sklearn.datasets.load_iris()
A:sklearn.utils.tests.test_multiclass.K->numpy.dot(X, X.T)
A:sklearn.utils.tests.test_multiclass.cv->ShuffleSplit(test_size=0.25, random_state=0)
A:sklearn.utils.tests.test_multiclass.(X_train, y_train)->_safe_split(clf, X, y, train)
A:sklearn.utils.tests.test_multiclass.(K_train, y_train2)->_safe_split(clfp, K, y, train)
A:sklearn.utils.tests.test_multiclass.(X_test, y_test)->_safe_split(clf, X, y, test, train)
A:sklearn.utils.tests.test_multiclass.(K_test, y_test2)->_safe_split(clfp, K, y, test, train)
sklearn.utils.tests.test_multiclass.NotAnArray(self,data)
sklearn.utils.tests.test_multiclass.NotAnArray.__array__(self,dtype=None)
sklearn.utils.tests.test_multiclass.NotAnArray.__init__(self,data)
sklearn.utils.tests.test_multiclass.test_check_classification_targets()
sklearn.utils.tests.test_multiclass.test_class_distribution()
sklearn.utils.tests.test_multiclass.test_is_multilabel()
sklearn.utils.tests.test_multiclass.test_safe_split_with_precomputed_kernel()
sklearn.utils.tests.test_multiclass.test_type_of_target()
sklearn.utils.tests.test_multiclass.test_unique_labels()
sklearn.utils.tests.test_multiclass.test_unique_labels_mixed_types()
sklearn.utils.tests.test_multiclass.test_unique_labels_non_specific()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/utils/tests/test_metaestimators.py----------------------------------------
A:sklearn.utils.tests.test_metaestimators.a_prefix->Prefix()
sklearn.utils.tests.test_metaestimators.HasNoPredict(object)
sklearn.utils.tests.test_metaestimators.HasPredict(object)
sklearn.utils.tests.test_metaestimators.HasPredict.predict(self)
sklearn.utils.tests.test_metaestimators.MetaEst(self,sub_est,better_sub_est=None)
sklearn.utils.tests.test_metaestimators.MetaEst.__init__(self,sub_est,better_sub_est=None)
sklearn.utils.tests.test_metaestimators.MetaEst.predict(self)
sklearn.utils.tests.test_metaestimators.MetaEstTestList(MetaEst)
sklearn.utils.tests.test_metaestimators.MetaEstTestList.predict(self)
sklearn.utils.tests.test_metaestimators.MetaEstTestTuple(MetaEst)
sklearn.utils.tests.test_metaestimators.MetaEstTestTuple.predict(self)
sklearn.utils.tests.test_metaestimators.MockMetaEstimator(object)
sklearn.utils.tests.test_metaestimators.MockMetaEstimator.func(self)
sklearn.utils.tests.test_metaestimators.Prefix(object)
sklearn.utils.tests.test_metaestimators.Prefix.func(self)
sklearn.utils.tests.test_metaestimators.test_delegated_docstring()
sklearn.utils.tests.test_metaestimators.test_if_delegate_has_method()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/utils/tests/test_fixes.py----------------------------------------
A:sklearn.utils.tests.test_fixes.marr->MaskedArray([1, None, 'a'], dtype=object)
A:sklearn.utils.tests.test_fixes.marr_pickled->pickle.loads(pickle.dumps(marr))
sklearn.utils.tests.test_fixes.test_divide()
sklearn.utils.tests.test_fixes.test_masked_array_obj_dtype_pickleable()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/utils/tests/test_sparsefuncs.py----------------------------------------
A:sklearn.utils.tests.test_sparsefuncs.(X, _)->make_classification(5, 4, random_state=0)
A:sklearn.utils.tests.test_sparsefuncs.X_lil->scipy.sparse.lil_matrix(X)
A:sklearn.utils.tests.test_sparsefuncs.X_csr->scipy.sparse.csr_matrix(X)
A:sklearn.utils.tests.test_sparsefuncs.X_csc->scipy.sparse.csc_matrix(X)
A:sklearn.utils.tests.test_sparsefuncs.X_test->rs.randn(10, 5).astype(dtype).astype(input_dtype)
A:sklearn.utils.tests.test_sparsefuncs.X_sparse->X_sparse.astype(input_dtype).astype(input_dtype)
A:sklearn.utils.tests.test_sparsefuncs.(X_means, X_vars)->mean_variance_axis(X_sparse, axis)
A:sklearn.utils.tests.test_sparsefuncs.rng->numpy.random.RandomState(0)
A:sklearn.utils.tests.test_sparsefuncs.last_mean->numpy.zeros(n_features)
A:sklearn.utils.tests.test_sparsefuncs.last_var->numpy.zeros_like(last_mean)
A:sklearn.utils.tests.test_sparsefuncs.X->RandomState(10).randn(10, 5).astype(dtype)
A:sklearn.utils.tests.test_sparsefuncs.(X_means_incr, X_vars_incr, n_incr)->incr_mean_variance_axis(X_sparse, axis, last_mean, last_var, last_n)
A:sklearn.utils.tests.test_sparsefuncs.X_rows->numpy.array([0, 2, 3], dtype=np.intp)
A:sklearn.utils.tests.test_sparsefuncs.out->numpy.ones((6, X.shape[1]), dtype=dtype)
A:sklearn.utils.tests.test_sparsefuncs.out_rows->numpy.array([1, 3, 4], dtype=np.intp)
A:sklearn.utils.tests.test_sparsefuncs.expect->numpy.ones_like(out)
A:sklearn.utils.tests.test_sparsefuncs.expect[out_rows]->X[X_rows, :].toarray()
A:sklearn.utils.tests.test_sparsefuncs.Xr->RandomState(10).randn(10, 5).astype(dtype).tocsr()
A:sklearn.utils.tests.test_sparsefuncs.Xc->RandomState(10).randn(10, 5).astype(dtype).tocsc()
A:sklearn.utils.tests.test_sparsefuncs.XA->RandomState(10).randn(10, 5).astype(dtype).toarray()
A:sklearn.utils.tests.test_sparsefuncs.scale->scale.astype(np.float32).astype(np.float32)
A:sklearn.utils.tests.test_sparsefuncs.swap->scipy.linalg.get_blas_funcs(('swap',), (X,))
A:sklearn.utils.tests.test_sparsefuncs.(X[0], X[-1])->swap(X[0], X[-1])
A:sklearn.utils.tests.test_sparsefuncs.(X[2], X[3])->swap(X[2], X[3])
A:sklearn.utils.tests.test_sparsefuncs.(X[:, 0], X[:, -1])->swap(X[:, 0], X[:, -1])
A:sklearn.utils.tests.test_sparsefuncs.(X[:, 0], X[:, 1])->swap(X[:, 0], X[:, 1])
A:sklearn.utils.tests.test_sparsefuncs.(mins_csr, maxs_csr)->min_max_axis(X_csr, axis=1)
A:sklearn.utils.tests.test_sparsefuncs.(mins_csc, maxs_csc)->min_max_axis(X_csc, axis=1)
A:sklearn.utils.tests.test_sparsefuncs.dense_median->numpy.median(X, axis=0)
A:sklearn.utils.tests.test_sparsefuncs.csc->scipy.sparse.csc_matrix(X)
A:sklearn.utils.tests.test_sparsefuncs.sparse_median->csc_median_axis_0(csc)
A:sklearn.utils.tests.test_sparsefuncs.ind->numpy.random.RandomState(0).randint(0, 50, 10)
A:sklearn.utils.tests.test_sparsefuncs.ones->numpy.ones((10, 1))
A:sklearn.utils.tests.test_sparsefuncs.rs->RandomState(10)
sklearn.utils.tests.test_sparsefuncs.test_count_nonzero()
sklearn.utils.tests.test_sparsefuncs.test_csc_row_median()
sklearn.utils.tests.test_sparsefuncs.test_densify_rows()
sklearn.utils.tests.test_sparsefuncs.test_incr_mean_variance_axis()
sklearn.utils.tests.test_sparsefuncs.test_inplace_column_scale()
sklearn.utils.tests.test_sparsefuncs.test_inplace_normalize()
sklearn.utils.tests.test_sparsefuncs.test_inplace_row_scale()
sklearn.utils.tests.test_sparsefuncs.test_inplace_swap_column()
sklearn.utils.tests.test_sparsefuncs.test_inplace_swap_row()
sklearn.utils.tests.test_sparsefuncs.test_mean_variance_axis0()
sklearn.utils.tests.test_sparsefuncs.test_mean_variance_axis1()
sklearn.utils.tests.test_sparsefuncs.test_mean_variance_illegal_axis()
sklearn.utils.tests.test_sparsefuncs.test_min_max_axis0()
sklearn.utils.tests.test_sparsefuncs.test_min_max_axis1()
sklearn.utils.tests.test_sparsefuncs.test_min_max_axis_errors()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/utils/tests/test_utils.py----------------------------------------
A:sklearn.utils.tests.test_utils.rng_42->numpy.random.RandomState(42)
A:sklearn.utils.tests.test_utils.spam->ham()
A:sklearn.utils.tests.test_utils.ham->Ham()
A:sklearn.utils.tests.test_utils.random_state->check_random_state(42)
A:sklearn.utils.tests.test_utils.X->numpy.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
A:sklearn.utils.tests.test_utils.X_csr->scipy.sparse.csr_matrix(X)
A:sklearn.utils.tests.test_utils.mask->safe_mask(X_csr, mask)
A:sklearn.utils.tests.test_utils.a->numpy.dot(a, a.conj().T)
A:sklearn.utils.tests.test_utils.a_pinv->pinvh(a)
A:sklearn.utils.tests.test_utils.(u, s, vt)->numpy.linalg.svd(a)
A:sklearn.utils.tests.test_utils.a_pinvh->pinvh(a)
A:sklearn.utils.tests.test_utils.A->numpy.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])
A:sklearn.utils.tests.test_utils.v0->check_random_state(42).uniform(-1, 1, A.shape[0])
A:sklearn.utils.tests.test_utils.(w, _)->eigsh(A, k=k, sigma=0.0, v0=v0)
A:sklearn.utils.tests.test_utils.inds->numpy.array([1, 2])
A:sklearn.utils.tests.test_utils.X_inds->safe_indexing(X, inds)
A:sklearn.utils.tests.test_utils.X_arrays->safe_indexing(np.array(X), inds)
A:sklearn.utils.tests.test_utils.X_df->MockDataFrame(X)
A:sklearn.utils.tests.test_utils.X_df_indexed->safe_indexing(X_df, inds)
A:sklearn.utils.tests.test_utils.X_indexed->safe_indexing(X_df, inds)
A:sklearn.utils.tests.test_utils.X_df_readonly->pandas.DataFrame(X)
A:sklearn.utils.tests.test_utils.inds_readonly->numpy.array([1, 2]).copy()
A:sklearn.utils.tests.test_utils.S->set(to_tuple(A))
A:sklearn.utils.tests.test_utils.b->numpy.array(['a', 'b', 'c'], dtype=object)
A:sklearn.utils.tests.test_utils.d->MockDataFrame(np.array([['a', 0], ['b', 1], ['c', 2]], dtype=object))
A:sklearn.utils.tests.test_utils.e->scipy.sparse.csc_matrix(np.arange(6).reshape(3, 2))
A:sklearn.utils.tests.test_utils.(a_s, b_s, c_s, d_s, e_s)->shuffle(a, b, c, d, e, random_state=0)
A:sklearn.utils.tests.test_utils.some_range->range(10)
A:sklearn.utils.tests.test_utils.joined_range->list(chain(*[some_range[slice] for slice in gen_even_slices(10, 3)]))
A:sklearn.utils.tests.test_utils.slices->gen_even_slices(10, -1)
sklearn.utils.tests.test_utils.test_arpack_eigsh_initialization()
sklearn.utils.tests.test_utils.test_column_or_1d()
sklearn.utils.tests.test_utils.test_deprecated()
sklearn.utils.tests.test_utils.test_gen_even_slices()
sklearn.utils.tests.test_utils.test_make_rng()
sklearn.utils.tests.test_utils.test_pinvh_nonpositive()
sklearn.utils.tests.test_utils.test_pinvh_simple_complex()
sklearn.utils.tests.test_utils.test_pinvh_simple_real()
sklearn.utils.tests.test_utils.test_resample()
sklearn.utils.tests.test_utils.test_safe_indexing()
sklearn.utils.tests.test_utils.test_safe_indexing_mock_pandas()
sklearn.utils.tests.test_utils.test_safe_indexing_pandas()
sklearn.utils.tests.test_utils.test_safe_mask()
sklearn.utils.tests.test_utils.test_shuffle_dont_convert_to_array()
sklearn.utils.tests.test_utils.test_shuffle_on_ndim_equals_three()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/utils/tests/test_shortest_path.py----------------------------------------
A:sklearn.utils.tests.test_shortest_path.graph->numpy.minimum(graph, graph.T)
A:sklearn.utils.tests.test_shortest_path.graph[i, j]->min(graph[i, j], graph[i, k] + graph[k, j])
A:sklearn.utils.tests.test_shortest_path.rng->numpy.random.RandomState(0)
A:sklearn.utils.tests.test_shortest_path.dist_matrix->numpy.minimum(dist_matrix, dist_matrix.T)
A:sklearn.utils.tests.test_shortest_path.graph_FW->graph_shortest_path(dist_matrix, directed, 'FW')
A:sklearn.utils.tests.test_shortest_path.graph_py->floyd_warshall_slow(dist_matrix.copy(), directed)
A:sklearn.utils.tests.test_shortest_path.graph_D->graph_shortest_path(dist_matrix, directed, 'D')
A:sklearn.utils.tests.test_shortest_path.dist_dict->defaultdict(int)
A:sklearn.utils.tests.test_shortest_path.X->numpy.array([[0.0, 0.0, 4.0], [1.0, 0.0, 2.0], [0.0, 5.0, 0.0]])
A:sklearn.utils.tests.test_shortest_path.dist_FW->graph_shortest_path(X, directed=False, method='FW')
A:sklearn.utils.tests.test_shortest_path.dist_D->graph_shortest_path(X, directed=False, method='D')
sklearn.utils.tests.test_shortest_path.floyd_warshall_slow(graph,directed=False)
sklearn.utils.tests.test_shortest_path.generate_graph(N=20)
sklearn.utils.tests.test_shortest_path.test_dijkstra()
sklearn.utils.tests.test_shortest_path.test_dijkstra_bug_fix()
sklearn.utils.tests.test_shortest_path.test_floyd_warshall()
sklearn.utils.tests.test_shortest_path.test_shortest_path()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/utils/tests/test_seq_dataset.py----------------------------------------
A:sklearn.utils.tests.test_seq_dataset.iris->load_iris()
A:sklearn.utils.tests.test_seq_dataset.X->load_iris().data.astype(np.float64)
A:sklearn.utils.tests.test_seq_dataset.y->load_iris().target.astype(np.float64)
A:sklearn.utils.tests.test_seq_dataset.X_csr->scipy.sparse.csr_matrix(X)
A:sklearn.utils.tests.test_seq_dataset.sample_weight->numpy.arange(y.size, dtype=np.float64)
A:sklearn.utils.tests.test_seq_dataset.dataset1->ArrayDataset(X, y, sample_weight, seed=42)
A:sklearn.utils.tests.test_seq_dataset.dataset2->CSRDataset(X_csr.data, X_csr.indptr, X_csr.indices, y, sample_weight, seed=42)
A:sklearn.utils.tests.test_seq_dataset.(xi_, yi, swi, idx)->dataset._random_py()
A:sklearn.utils.tests.test_seq_dataset.xi->scipy.sparse.csr_matrix(xi_, shape=(1, X.shape[1]))
A:sklearn.utils.tests.test_seq_dataset.(_, _, _, idx1)->ArrayDataset(X, y, sample_weight, seed=42)._random_py()
A:sklearn.utils.tests.test_seq_dataset.(_, _, _, idx2)->CSRDataset(X_csr.data, X_csr.indptr, X_csr.indices, y, sample_weight, seed=42)._random_py()
sklearn.utils.tests.test_seq_dataset.assert_csr_equal(X,Y)
sklearn.utils.tests.test_seq_dataset.test_seq_dataset()
sklearn.utils.tests.test_seq_dataset.test_seq_dataset_shuffle()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/utils/tests/test_random.py----------------------------------------
A:sklearn.utils.tests.test_random.s->sample_without_replacement(n_population, n_samples)
A:sklearn.utils.tests.test_random.unique->numpy.unique(s)
A:sklearn.utils.tests.test_random.n_expected->comb(n_population, n_samples, exact=True)
A:sklearn.utils.tests.test_random.got->random_choice_csc(n_samples=n_samples, classes=classes, random_state=random_state)
sklearn.utils.tests.test_random.check_edge_case_of_sample_int(sample_without_replacement)
sklearn.utils.tests.test_random.check_sample_int(sample_without_replacement)
sklearn.utils.tests.test_random.check_sample_int_distribution(sample_without_replacement)
sklearn.utils.tests.test_random.test_invalid_sample_without_replacement_algorithm()
sklearn.utils.tests.test_random.test_random_choice_csc(n_samples=10000,random_state=24)
sklearn.utils.tests.test_random.test_random_choice_csc_errors()
sklearn.utils.tests.test_random.test_sample_without_replacement_algorithms()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/utils/sparsetools/__init__.py----------------------------------------
sklearn.utils.sparsetools.__init__.connected_components(*args,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/utils/sparsetools/setup.py----------------------------------------
A:sklearn.utils.sparsetools.setup.config->Configuration('sparsetools', parent_package, top_path)
sklearn.utils.sparsetools.setup.configuration(parent_package='',top_path=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/utils/sparsetools/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/svm/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/svm/classes.py----------------------------------------
A:sklearn.svm.classes.self.loss->{'l1': 'epsilon_insensitive', 'l2': 'squared_epsilon_insensitive'}.get(self.loss)
A:sklearn.svm.classes.(X, y)->check_X_y(X, y, accept_sparse='csr', dtype=np.float64, order='C')
A:sklearn.svm.classes.self.classes_->numpy.unique(y)
A:sklearn.svm.classes.(self.coef_, self.intercept_, self.n_iter_)->_fit_liblinear(X, y, self.C, self.fit_intercept, self.intercept_scaling, None, penalty, self.dual, self.verbose, self.max_iter, self.tol, self.random_state, loss=self.loss, epsilon=self.epsilon, sample_weight=sample_weight)
A:sklearn.svm.classes.self.coef_->self.coef_.ravel()
A:sklearn.svm.classes.self.intercept_->numpy.array([intercept])
A:sklearn.svm.classes.dec->self._decision_function(X)
A:sklearn.svm.classes.y->super(OneClassSVM, self).predict(X)
sklearn.svm.LinearSVC(self,penalty='l2',loss='squared_hinge',dual=True,tol=0.0001,C=1.0,multi_class='ovr',fit_intercept=True,intercept_scaling=1,class_weight=None,verbose=0,random_state=None,max_iter=1000)
sklearn.svm.LinearSVC.fit(self,X,y,sample_weight=None)
sklearn.svm.LinearSVR(self,epsilon=0.0,tol=0.0001,C=1.0,loss='epsilon_insensitive',fit_intercept=True,intercept_scaling=1.0,dual=True,verbose=0,random_state=None,max_iter=1000)
sklearn.svm.LinearSVR.fit(self,X,y,sample_weight=None)
sklearn.svm.NuSVC(self,nu=0.5,kernel='rbf',degree=3,gamma='auto',coef0=0.0,shrinking=True,probability=False,tol=0.001,cache_size=200,class_weight=None,verbose=False,max_iter=-1,decision_function_shape='ovr',random_state=None)
sklearn.svm.NuSVR(self,nu=0.5,C=1.0,kernel='rbf',degree=3,gamma='auto',coef0=0.0,shrinking=True,tol=0.001,cache_size=200,verbose=False,max_iter=-1)
sklearn.svm.OneClassSVM(self,kernel='rbf',degree=3,gamma='auto',coef0=0.0,tol=0.001,nu=0.5,shrinking=True,cache_size=200,verbose=False,max_iter=-1,random_state=None)
sklearn.svm.OneClassSVM.decision_function(self,X)
sklearn.svm.OneClassSVM.fit(self,X,y=None,sample_weight=None,**params)
sklearn.svm.OneClassSVM.predict(self,X)
sklearn.svm.SVC(self,C=1.0,kernel='rbf',degree=3,gamma='auto',coef0=0.0,shrinking=True,probability=False,tol=0.001,cache_size=200,class_weight=None,verbose=False,max_iter=-1,decision_function_shape='ovr',random_state=None)
sklearn.svm.SVR(self,kernel='rbf',degree=3,gamma='auto',coef0=0.0,tol=0.001,C=1.0,epsilon=0.1,shrinking=True,cache_size=200,verbose=False,max_iter=-1)
sklearn.svm.classes.LinearSVC(self,penalty='l2',loss='squared_hinge',dual=True,tol=0.0001,C=1.0,multi_class='ovr',fit_intercept=True,intercept_scaling=1,class_weight=None,verbose=0,random_state=None,max_iter=1000)
sklearn.svm.classes.LinearSVC.__init__(self,penalty='l2',loss='squared_hinge',dual=True,tol=0.0001,C=1.0,multi_class='ovr',fit_intercept=True,intercept_scaling=1,class_weight=None,verbose=0,random_state=None,max_iter=1000)
sklearn.svm.classes.LinearSVC.fit(self,X,y,sample_weight=None)
sklearn.svm.classes.LinearSVR(self,epsilon=0.0,tol=0.0001,C=1.0,loss='epsilon_insensitive',fit_intercept=True,intercept_scaling=1.0,dual=True,verbose=0,random_state=None,max_iter=1000)
sklearn.svm.classes.LinearSVR.__init__(self,epsilon=0.0,tol=0.0001,C=1.0,loss='epsilon_insensitive',fit_intercept=True,intercept_scaling=1.0,dual=True,verbose=0,random_state=None,max_iter=1000)
sklearn.svm.classes.LinearSVR.fit(self,X,y,sample_weight=None)
sklearn.svm.classes.NuSVC(self,nu=0.5,kernel='rbf',degree=3,gamma='auto',coef0=0.0,shrinking=True,probability=False,tol=0.001,cache_size=200,class_weight=None,verbose=False,max_iter=-1,decision_function_shape='ovr',random_state=None)
sklearn.svm.classes.NuSVC.__init__(self,nu=0.5,kernel='rbf',degree=3,gamma='auto',coef0=0.0,shrinking=True,probability=False,tol=0.001,cache_size=200,class_weight=None,verbose=False,max_iter=-1,decision_function_shape='ovr',random_state=None)
sklearn.svm.classes.NuSVR(self,nu=0.5,C=1.0,kernel='rbf',degree=3,gamma='auto',coef0=0.0,shrinking=True,tol=0.001,cache_size=200,verbose=False,max_iter=-1)
sklearn.svm.classes.NuSVR.__init__(self,nu=0.5,C=1.0,kernel='rbf',degree=3,gamma='auto',coef0=0.0,shrinking=True,tol=0.001,cache_size=200,verbose=False,max_iter=-1)
sklearn.svm.classes.OneClassSVM(self,kernel='rbf',degree=3,gamma='auto',coef0=0.0,tol=0.001,nu=0.5,shrinking=True,cache_size=200,verbose=False,max_iter=-1,random_state=None)
sklearn.svm.classes.OneClassSVM.__init__(self,kernel='rbf',degree=3,gamma='auto',coef0=0.0,tol=0.001,nu=0.5,shrinking=True,cache_size=200,verbose=False,max_iter=-1,random_state=None)
sklearn.svm.classes.OneClassSVM.decision_function(self,X)
sklearn.svm.classes.OneClassSVM.fit(self,X,y=None,sample_weight=None,**params)
sklearn.svm.classes.OneClassSVM.predict(self,X)
sklearn.svm.classes.SVC(self,C=1.0,kernel='rbf',degree=3,gamma='auto',coef0=0.0,shrinking=True,probability=False,tol=0.001,cache_size=200,class_weight=None,verbose=False,max_iter=-1,decision_function_shape='ovr',random_state=None)
sklearn.svm.classes.SVC.__init__(self,C=1.0,kernel='rbf',degree=3,gamma='auto',coef0=0.0,shrinking=True,probability=False,tol=0.001,cache_size=200,class_weight=None,verbose=False,max_iter=-1,decision_function_shape='ovr',random_state=None)
sklearn.svm.classes.SVR(self,kernel='rbf',degree=3,gamma='auto',coef0=0.0,tol=0.001,C=1.0,epsilon=0.1,shrinking=True,cache_size=200,verbose=False,max_iter=-1)
sklearn.svm.classes.SVR.__init__(self,kernel='rbf',degree=3,gamma='auto',coef0=0.0,tol=0.001,C=1.0,epsilon=0.1,shrinking=True,cache_size=200,verbose=False,max_iter=-1)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/svm/bounds.py----------------------------------------
A:sklearn.svm.bounds.X->check_array(X, accept_sparse='csc')
A:sklearn.svm.bounds.den->max(den, abs(np.dot(Y, bias)).max())
sklearn.svm.bounds.l1_min_c(X,y,loss='squared_hinge',fit_intercept=True,intercept_scaling=1.0)
sklearn.svm.l1_min_c(X,y,loss='squared_hinge',fit_intercept=True,intercept_scaling=1.0)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/svm/setup.py----------------------------------------
A:sklearn.svm.setup.config->Configuration('svm', parent_package, top_path)
A:sklearn.svm.setup.(cblas_libs, blas_info)->get_blas_info()
sklearn.svm.setup.configuration(parent_package='',top_path=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/svm/base.py----------------------------------------
A:sklearn.svm.base.sv_locs->numpy.cumsum(np.hstack([[0], n_support]))
A:sklearn.svm.base.rnd->check_random_state(random_state)
A:sklearn.svm.base.sparse->scipy.sparse.isspmatrix(X)
A:sklearn.svm.base.(X, y)->check_X_y(X, y, dtype=np.float64, order='C', accept_sparse='csr')
A:sklearn.svm.base.y->super(BaseSVC, self).predict(X)
A:sklearn.svm.base.sample_weight->numpy.array(sample_weight, dtype=np.float64, order='C')
A:sklearn.svm.base.solver_type->_get_liblinear_solver_type(multi_class, penalty, loss, dual)
A:sklearn.svm.base.seed->check_random_state(random_state).randint(np.iinfo('i').max)
A:sklearn.svm.base.self._intercept_->self.intercept_.copy()
A:sklearn.svm.base.self.class_weight_->compute_class_weight(self.class_weight, cls, y_)
A:sklearn.svm.base.X->self._compute_kernel(X)
A:sklearn.svm.base.kernel->kernel.toarray().toarray()
A:sklearn.svm.base.(self.support_, self.support_vectors_, self.n_support_, self.dual_coef_, self.intercept_, self.probA_, self.probB_, self.fit_status_)->libsvm.fit(X, y, svm_type=solver_type, sample_weight=sample_weight, class_weight=self.class_weight_, kernel=kernel, C=self.C, nu=self.nu, probability=self.probability, degree=self.degree, shrinking=self.shrinking, tol=self.tol, cache_size=self.cache_size, coef0=self.coef0, gamma=self._gamma, epsilon=self.epsilon, max_iter=self.max_iter, random_seed=random_seed)
A:sklearn.svm.base.X.data->numpy.asarray(X.data, dtype=np.float64, order='C')
A:sklearn.svm.base.kernel_type->self._sparse_kernels.index(kernel)
A:sklearn.svm.base.(self.support_, self.support_vectors_, dual_coef_data, self.intercept_, self.n_support_, self.probA_, self.probB_, self.fit_status_)->libsvm_sparse.libsvm_sparse_train(X.shape[1], X.data, X.indices, X.indptr, y, solver_type, kernel_type, self.degree, self._gamma, self.coef0, self.tol, self.C, self.class_weight_, sample_weight, self.nu, self.cache_size, self.epsilon, int(self.shrinking), int(self.probability), self.max_iter, random_seed)
A:sklearn.svm.base.dual_coef_indices->numpy.tile(np.arange(n_SV), n_class)
A:sklearn.svm.base.dual_coef_indptr->numpy.arange(0, dual_coef_indices.size + 1, dual_coef_indices.size / n_class)
A:sklearn.svm.base.self.dual_coef_->scipy.sparse.csr_matrix((dual_coef_data, dual_coef_indices, dual_coef_indptr), (n_class, n_SV))
A:sklearn.svm.base.svm_type->LIBSVM_IMPL.index(self._impl)
A:sklearn.svm.base.dec_func->self._dense_decision_function(X)
A:sklearn.svm.base.coef->numpy.vstack(coef)
A:sklearn.svm.base.y_->column_or_1d(y, warn=True)
A:sklearn.svm.base.(cls, y)->numpy.unique(y_, return_inverse=True)
A:sklearn.svm.base.dec->self._decision_function(X)
A:sklearn.svm.base.pprob->libsvm.predict_proba(X, self.support_, self.support_vectors_, self.n_support_, self._dual_coef_, self._intercept_, self.probA_, self.probB_, svm_type=svm_type, kernel=kernel, degree=self.degree, cache_size=self.cache_size, coef0=self.coef0, gamma=self._gamma)
A:sklearn.svm.base._solver_pen->_solver_type_dict.get(loss, None)
A:sklearn.svm.base._solver_dual->_solver_type_dict.get(loss, None).get(penalty, None)
A:sklearn.svm.base.solver_num->_solver_type_dict.get(loss, None).get(penalty, None).get(dual, None)
A:sklearn.svm.base.enc->LabelEncoder()
A:sklearn.svm.base.y_ind->numpy.asarray(y_ind, dtype=np.float64).ravel()
A:sklearn.svm.base.class_weight_->numpy.empty(0, dtype=np.float64)
A:sklearn.svm.base.(raw_coef_, n_iter_)->liblinear.train_wrap(X, y_ind, sp.isspmatrix(X), solver_type, tol, bias, C, class_weight_, max_iter, rnd.randint(np.iinfo('i').max), epsilon, sample_weight)
A:sklearn.svm.base.n_iter_->max(n_iter_)
sklearn.svm.base.BaseLibSVM(self,impl,kernel,degree,gamma,coef0,tol,C,nu,epsilon,shrinking,probability,cache_size,class_weight,verbose,max_iter,random_state)
sklearn.svm.base.BaseLibSVM.__init__(self,impl,kernel,degree,gamma,coef0,tol,C,nu,epsilon,shrinking,probability,cache_size,class_weight,verbose,max_iter,random_state)
sklearn.svm.base.BaseLibSVM._compute_kernel(self,X)
sklearn.svm.base.BaseLibSVM._decision_function(self,X)
sklearn.svm.base.BaseLibSVM._dense_decision_function(self,X)
sklearn.svm.base.BaseLibSVM._dense_fit(self,X,y,sample_weight,solver_type,kernel,random_seed)
sklearn.svm.base.BaseLibSVM._dense_predict(self,X)
sklearn.svm.base.BaseLibSVM._get_coef(self)
sklearn.svm.base.BaseLibSVM._pairwise(self)
sklearn.svm.base.BaseLibSVM._sparse_decision_function(self,X)
sklearn.svm.base.BaseLibSVM._sparse_fit(self,X,y,sample_weight,solver_type,kernel,random_seed)
sklearn.svm.base.BaseLibSVM._sparse_predict(self,X)
sklearn.svm.base.BaseLibSVM._validate_for_predict(self,X)
sklearn.svm.base.BaseLibSVM._validate_targets(self,y)
sklearn.svm.base.BaseLibSVM._warn_from_fit_status(self)
sklearn.svm.base.BaseLibSVM.coef_(self)
sklearn.svm.base.BaseLibSVM.fit(self,X,y,sample_weight=None)
sklearn.svm.base.BaseLibSVM.predict(self,X)
sklearn.svm.base.BaseSVC(self,impl,kernel,degree,gamma,coef0,tol,C,nu,shrinking,probability,cache_size,class_weight,verbose,max_iter,decision_function_shape,random_state)
sklearn.svm.base.BaseSVC.__init__(self,impl,kernel,degree,gamma,coef0,tol,C,nu,shrinking,probability,cache_size,class_weight,verbose,max_iter,decision_function_shape,random_state)
sklearn.svm.base.BaseSVC._check_proba(self)
sklearn.svm.base.BaseSVC._dense_predict_proba(self,X)
sklearn.svm.base.BaseSVC._get_coef(self)
sklearn.svm.base.BaseSVC._predict_log_proba(self,X)
sklearn.svm.base.BaseSVC._predict_proba(self,X)
sklearn.svm.base.BaseSVC._sparse_predict_proba(self,X)
sklearn.svm.base.BaseSVC._validate_targets(self,y)
sklearn.svm.base.BaseSVC.decision_function(self,X)
sklearn.svm.base.BaseSVC.predict(self,X)
sklearn.svm.base.BaseSVC.predict_log_proba(self)
sklearn.svm.base.BaseSVC.predict_proba(self)
sklearn.svm.base._fit_liblinear(X,y,C,fit_intercept,intercept_scaling,class_weight,penalty,dual,verbose,max_iter,tol,random_state=None,multi_class='ovr',loss='logistic_regression',epsilon=0.1,sample_weight=None)
sklearn.svm.base._get_liblinear_solver_type(multi_class,penalty,loss,dual)
sklearn.svm.base._one_vs_one_coef(dual_coef,n_support,support_vectors)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/svm/tests/test_svm.py----------------------------------------
A:sklearn.svm.tests.test_svm.iris->sklearn.datasets.load_iris()
A:sklearn.svm.tests.test_svm.rng->check_random_state(42)
A:sklearn.svm.tests.test_svm.perm->check_random_state(42).permutation(iris.target.size)
A:sklearn.svm.tests.test_svm.clf->sklearn.svm.SVC(kernel='linear', decision_function_shape='ovr')
A:sklearn.svm.tests.test_svm.model->sklearn.svm.libsvm.fit(iris.data, iris.target.astype(np.float64), kernel='linear')
A:sklearn.svm.tests.test_svm.pred->sklearn.svm.SVC(kernel='linear', decision_function_shape='ovr').predict(T)
A:sklearn.svm.tests.test_svm.pred2->sklearn.svm.LinearSVC(random_state=0).fit(X_flat, y_flat).predict(T)
A:sklearn.svm.tests.test_svm.K->numpy.zeros_like(K)
A:sklearn.svm.tests.test_svm.KT->numpy.zeros_like(KT)
A:sklearn.svm.tests.test_svm.KT[i, j]->numpy.dot(T[i], X[j])
A:sklearn.svm.tests.test_svm.clf2->sklearn.svm.SVC(kernel='linear')
A:sklearn.svm.tests.test_svm.K[i, j]->numpy.dot(iris.data[i], iris.data[j])
A:sklearn.svm.tests.test_svm.diabetes->sklearn.datasets.load_diabetes()
A:sklearn.svm.tests.test_svm.lsvr->sklearn.svm.LinearSVR(C=1000.0).fit(diabetes.data, diabetes.target, sample_weight=unit_weight)
A:sklearn.svm.tests.test_svm.score1->sklearn.svm.LinearSVR(C=1000.0).fit(diabetes.data, diabetes.target, sample_weight=unit_weight).score(diabetes.data, diabetes.target)
A:sklearn.svm.tests.test_svm.svr->sklearn.svm.SVR(kernel='linear', C=1000.0).fit(diabetes.data, diabetes.target)
A:sklearn.svm.tests.test_svm.score2->sklearn.svm.LinearSVR(C=1000.0).fit(diabetes.data, diabetes.target).score(diabetes.data, diabetes.target)
A:sklearn.svm.tests.test_svm.n_samples->len(X)
A:sklearn.svm.tests.test_svm.unit_weight->numpy.ones(n_samples)
A:sklearn.svm.tests.test_svm.lsvr_no_weight->sklearn.svm.LinearSVR(C=1000.0).fit(diabetes.data, diabetes.target)
A:sklearn.svm.tests.test_svm.random_state->check_random_state(0)
A:sklearn.svm.tests.test_svm.random_weight->check_random_state(0).randint(0, 10, n_samples)
A:sklearn.svm.tests.test_svm.lsvr_unflat->sklearn.svm.LinearSVR(C=1000.0).fit(diabetes.data, diabetes.target, sample_weight=random_weight)
A:sklearn.svm.tests.test_svm.score3->sklearn.svm.LinearSVR(C=1000.0).fit(diabetes.data, diabetes.target, sample_weight=random_weight).score(diabetes.data, diabetes.target, sample_weight=random_weight)
A:sklearn.svm.tests.test_svm.X_flat->numpy.repeat(X, random_weight, axis=0)
A:sklearn.svm.tests.test_svm.y_flat->numpy.repeat(Y, random_weight, axis=0)
A:sklearn.svm.tests.test_svm.lsvr_flat->sklearn.svm.LinearSVR(C=1000.0).fit(X_flat, y_flat)
A:sklearn.svm.tests.test_svm.score4->sklearn.svm.LinearSVR(C=1000.0).fit(X_flat, y_flat).score(X_flat, y_flat)
A:sklearn.svm.tests.test_svm.rnd->check_random_state(2)
A:sklearn.svm.tests.test_svm.X_outliers->check_random_state(2).uniform(low=-4, high=4, size=(20, 2))
A:sklearn.svm.tests.test_svm.y_pred_test->sklearn.svm.SVC(kernel='linear', decision_function_shape='ovr').predict(X_test)
A:sklearn.svm.tests.test_svm.y_pred_outliers->sklearn.svm.SVC(kernel='linear', decision_function_shape='ovr').predict(X_outliers)
A:sklearn.svm.tests.test_svm.dec_func_test->sklearn.svm.SVC(kernel='linear', decision_function_shape='ovr').decision_function(X_test)
A:sklearn.svm.tests.test_svm.dec_func_outliers->sklearn.svm.SVC(kernel='linear', decision_function_shape='ovr').decision_function(X_outliers)
A:sklearn.svm.tests.test_svm.clf._dual_coef_->numpy.array([[0.0, 1.0]])
A:sklearn.svm.tests.test_svm.prob_predict->sklearn.svm.SVC(kernel='linear', decision_function_shape='ovr').predict_proba(iris.data)
A:sklearn.svm.tests.test_svm.prediction->sklearn.svm.SVC(kernel='linear', decision_function_shape='ovr').predict(X)
A:sklearn.svm.tests.test_svm.expected->numpy.array([-1.0, -0.66, -1.0, 0.66, 1.0, 1.0])
A:sklearn.svm.tests.test_svm.rbfs->rbf_kernel(X, reg.support_vectors_, gamma=reg.gamma)
A:sklearn.svm.tests.test_svm.dec->sklearn.svm.SVC(kernel='linear', decision_function_shape='ovr').decision_function(iris.data)
A:sklearn.svm.tests.test_svm.(X, y)->make_blobs(centers=n_classes, random_state=0)
A:sklearn.svm.tests.test_svm.(X_train, X_test, y_train, y_test)->train_test_split(X, y, random_state=0)
A:sklearn.svm.tests.test_svm.reg->sklearn.svm.SVR(kernel='rbf', gamma=1).fit(X, y)
A:sklearn.svm.tests.test_svm.(X_, y_)->make_classification(n_samples=200, n_features=10, weights=[0.833, 0.167], random_state=2)
A:sklearn.svm.tests.test_svm.y_pred->sklearn.svm.SVC(kernel='linear', decision_function_shape='ovr').predict(X_test)
A:sklearn.svm.tests.test_svm.unbalanced->numpy.delete(np.arange(y.size), np.where(y > 2)[0][::2])
A:sklearn.svm.tests.test_svm.classes->numpy.unique(y[unbalanced])
A:sklearn.svm.tests.test_svm.class_weights->compute_class_weight('balanced', classes, y[unbalanced])
A:sklearn.svm.tests.test_svm.y_pred_balanced->sklearn.svm.SVC(kernel='linear', decision_function_shape='ovr').fit(X[unbalanced], y[unbalanced]).predict(X)
A:sklearn.svm.tests.test_svm.Xf->numpy.asfortranarray(X)
A:sklearn.svm.tests.test_svm.yf->numpy.ascontiguousarray(np.tile(Y, (2, 1)).T)
A:sklearn.svm.tests.test_svm.sparse_gram->scipy.sparse.csr_matrix([[1, 0], [0, 1]])
A:sklearn.svm.tests.test_svm.ovr_clf->sklearn.svm.LinearSVC(random_state=0).fit(iris.data, iris.target)
A:sklearn.svm.tests.test_svm.cs_clf->sklearn.svm.LinearSVC(multi_class='crammer_singer', random_state=0)
A:sklearn.svm.tests.test_svm.clf_unitweight->sklearn.svm.LinearSVC(random_state=0).fit(X, Y, sample_weight=unit_weight)
A:sklearn.svm.tests.test_svm.lsvc_unflat->sklearn.svm.LinearSVC(random_state=0).fit(X, Y, sample_weight=random_weight)
A:sklearn.svm.tests.test_svm.pred1->sklearn.svm.LinearSVC(random_state=0).fit(X, Y, sample_weight=random_weight).predict(T)
A:sklearn.svm.tests.test_svm.lsvc_flat->sklearn.svm.LinearSVC(random_state=0).fit(X_flat, y_flat)
A:sklearn.svm.tests.test_svm.acc->sklearn.svm.LinearSVC(fit_intercept=fit_intercept, multi_class='crammer_singer', random_state=0).fit(X, y).score(X, y)
A:sklearn.svm.tests.test_svm.values->sklearn.svm.SVC(kernel='linear', decision_function_shape='ovr').decision_function(X)
A:sklearn.svm.tests.test_svm.clf.coef_->sklearn.svm.SVC(kernel='linear', decision_function_shape='ovr').coef_.copy()
A:sklearn.svm.tests.test_svm.clf.intercept_->sklearn.svm.SVC(kernel='linear', decision_function_shape='ovr').intercept_.copy()
A:sklearn.svm.tests.test_svm.values2->sklearn.svm.SVC(kernel='linear', decision_function_shape='ovr').decision_function(X)
A:sklearn.svm.tests.test_svm.stdout->os.dup(1)
A:sklearn.svm.tests.test_svm.svm_callable->sklearn.svm.SVC(kernel=lambda x, y: np.dot(x, y.T), probability=True, random_state=0, decision_function_shape='ovr')
A:sklearn.svm.tests.test_svm.svm_cloned->sklearn.base.clone(svm_callable)
A:sklearn.svm.tests.test_svm.svm_builtin->sklearn.svm.SVC(kernel='linear', probability=True, random_state=0, decision_function_shape='ovr')
A:sklearn.svm.tests.test_svm.svc->sklearn.svm.SVC(kernel=lambda x, y: x)
A:sklearn.svm.tests.test_svm.a->sklearn.svm.SVC(probability=True, max_iter=1, random_state=0)
A:sklearn.svm.tests.test_svm.proba_1->sklearn.svm.SVC(probability=True, max_iter=1, random_state=0).fit(X, Y).predict_proba(X)
A:sklearn.svm.tests.test_svm.proba_2->sklearn.svm.SVC(probability=True, max_iter=1, random_state=0).fit(X, Y).predict_proba(X)
A:sklearn.svm.tests.test_svm.lsvc->sklearn.svm.LinearSVC(fit_intercept=False)
A:sklearn.svm.tests.test_svm.X->numpy.random.RandomState(21).randn(10, 3)
A:sklearn.svm.tests.test_svm.y->numpy.random.RandomState(12).randn(10)
A:sklearn.svm.tests.test_svm.G->sklearn.svm.SVC(probability=False)
A:sklearn.svm.tests.test_svm.X_train->numpy.array([[1, 1], [-1, 1], [-1, -1], [1, -1]])
A:sklearn.svm.tests.test_svm.base_points->numpy.array([[5, 5], [10, 10]])
A:sklearn.svm.tests.test_svm.X_test->numpy.vstack((base_points * [1, 1], base_points * [-1, 1], base_points * [-1, -1], base_points * [1, -1]))
A:sklearn.svm.tests.test_svm.deci_val->sklearn.svm.SVC(kernel='linear', decision_function_shape='ovr').decision_function(X_test)
A:sklearn.svm.tests.test_svm.pred_class_deci_val->deci_val[range(8), y_pred].reshape((4, 2))
sklearn.svm.tests.test_svm.test_auto_weight()
sklearn.svm.tests.test_svm.test_bad_input()
sklearn.svm.tests.test_svm.test_consistent_proba()
sklearn.svm.tests.test_svm.test_crammer_singer_binary()
sklearn.svm.tests.test_svm.test_decision_function()
sklearn.svm.tests.test_svm.test_decision_function_shape()
sklearn.svm.tests.test_svm.test_decision_function_shape_two_class()
sklearn.svm.tests.test_svm.test_dense_liblinear_intercept_handling(classifier=svm.LinearSVC)
sklearn.svm.tests.test_svm.test_hasattr_predict_proba()
sklearn.svm.tests.test_svm.test_immutable_coef_property()
sklearn.svm.tests.test_svm.test_liblinear_set_coef()
sklearn.svm.tests.test_svm.test_libsvm_iris()
sklearn.svm.tests.test_svm.test_libsvm_parameters()
sklearn.svm.tests.test_svm.test_linear_svc_convergence_warnings()
sklearn.svm.tests.test_svm.test_linear_svc_intercept_scaling()
sklearn.svm.tests.test_svm.test_linear_svx_uppercase_loss_penality_raises_error()
sklearn.svm.tests.test_svm.test_linearsvc()
sklearn.svm.tests.test_svm.test_linearsvc_crammer_singer()
sklearn.svm.tests.test_svm.test_linearsvc_fit_sampleweight()
sklearn.svm.tests.test_svm.test_linearsvc_iris()
sklearn.svm.tests.test_svm.test_linearsvc_parameters()
sklearn.svm.tests.test_svm.test_linearsvc_verbose()
sklearn.svm.tests.test_svm.test_linearsvr()
sklearn.svm.tests.test_svm.test_linearsvr_fit_sampleweight()
sklearn.svm.tests.test_svm.test_linearsvx_loss_penalty_deprecations()
sklearn.svm.tests.test_svm.test_lsvc_intercept_scaling_zero()
sklearn.svm.tests.test_svm.test_oneclass()
sklearn.svm.tests.test_svm.test_oneclass_decision_function()
sklearn.svm.tests.test_svm.test_ovr_decision_function()
sklearn.svm.tests.test_svm.test_precomputed()
sklearn.svm.tests.test_svm.test_probability()
sklearn.svm.tests.test_svm.test_sample_weights()
sklearn.svm.tests.test_svm.test_sparse_precomputed()
sklearn.svm.tests.test_svm.test_svc_bad_kernel()
sklearn.svm.tests.test_svm.test_svc_clone_with_callable_kernel()
sklearn.svm.tests.test_svm.test_svr()
sklearn.svm.tests.test_svm.test_svr_coef_sign()
sklearn.svm.tests.test_svm.test_svr_errors()
sklearn.svm.tests.test_svm.test_svr_predict()
sklearn.svm.tests.test_svm.test_timeout()
sklearn.svm.tests.test_svm.test_tweak_params()
sklearn.svm.tests.test_svm.test_unfitted()
sklearn.svm.tests.test_svm.test_unicode_kernel()
sklearn.svm.tests.test_svm.test_weight()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/svm/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/svm/tests/test_bounds.py----------------------------------------
A:sklearn.svm.tests.test_bounds.sparse_X->scipy.sparse.csr_matrix(dense_X)
A:sklearn.svm.tests.test_bounds.min_c->l1_min_c(X, y, loss, fit_intercept, intercept_scaling)
sklearn.svm.tests.test_bounds.check_l1_min_c(X,y,loss,fit_intercept=True,intercept_scaling=None)
sklearn.svm.tests.test_bounds.test_ill_posed_min_c()
sklearn.svm.tests.test_bounds.test_l1_min_c()
sklearn.svm.tests.test_bounds.test_unsupported_loss()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/svm/tests/test_sparse.py----------------------------------------
A:sklearn.svm.tests.test_sparse.X->scipy.sparse.csr_matrix((data, indices, indptr))
A:sklearn.svm.tests.test_sparse.X_sp->scipy.sparse.lil_matrix(X)
A:sklearn.svm.tests.test_sparse.T->numpy.array([[-1, -1], [2, 2], [3, 2]])
A:sklearn.svm.tests.test_sparse.X2->numpy.array([[0, 0, 0], [1, 1, 1], [2, 0, 0], [0, 0, 2], [3, 3, 3]])
A:sklearn.svm.tests.test_sparse.X2_sp->scipy.sparse.dok_matrix(X2)
A:sklearn.svm.tests.test_sparse.T2->numpy.array([[-1, -1, -1], [1, 1, 1], [2, 2, 2]])
A:sklearn.svm.tests.test_sparse.iris->sklearn.datasets.load_iris()
A:sklearn.svm.tests.test_sparse.rng->numpy.random.RandomState(0)
A:sklearn.svm.tests.test_sparse.perm->numpy.random.RandomState(0).permutation(iris.target.size)
A:sklearn.svm.tests.test_sparse.iris.data->scipy.sparse.csr_matrix(iris.data)
A:sklearn.svm.tests.test_sparse.X_test_dense->scipy.sparse.csr_matrix(digits.data[50:100]).toarray()
A:sklearn.svm.tests.test_sparse.(X_blobs, y_blobs)->make_blobs(n_samples=100, centers=10, random_state=0)
A:sklearn.svm.tests.test_sparse.X_blobs->scipy.sparse.csr_matrix(X_blobs)
A:sklearn.svm.tests.test_sparse.clf->sklearn.svm.SVC(kernel='linear').fit(X.toarray(), y)
A:sklearn.svm.tests.test_sparse.sp_clf->sklearn.svm.SVC(kernel='linear').fit(sparse.coo_matrix(X), y)
A:sklearn.svm.tests.test_sparse.digits->load_digits()
A:sklearn.svm.tests.test_sparse.X_test->scipy.sparse.csr_matrix(digits.data[50:100])
A:sklearn.svm.tests.test_sparse.X_sparse->scipy.sparse.csr_matrix(X)
A:sklearn.svm.tests.test_sparse.sparse_svc->sklearn.svm.SVC(kernel='linear', probability=True, random_state=0).fit(X_sparse, y)
A:sklearn.svm.tests.test_sparse.unsorted_svc->sklearn.svm.SVC(kernel='linear', probability=True, random_state=0).fit(X_sparse_unsorted, y)
A:sklearn.svm.tests.test_sparse.clf_lin->sklearn.svm.SVC(kernel='linear').fit(X_sp, Y)
A:sklearn.svm.tests.test_sparse.clf_mylin->sklearn.svm.SVC(kernel=kfunc).fit(X_sp, Y)
A:sklearn.svm.tests.test_sparse.svc->sklearn.svm.SVC(kernel='linear', C=0.1, decision_function_shape='ovo')
A:sklearn.svm.tests.test_sparse.prediction->sklearn.svm.SVC(kernel='linear').fit(X.toarray(), y).predict(X)
A:sklearn.svm.tests.test_sparse.expected->numpy.array([-1.0, -0.66, -1.0, 0.66, 1.0, 1.0])
A:sklearn.svm.tests.test_sparse.pred->sklearn.base.clone(a).predict(X_sp)
A:sklearn.svm.tests.test_sparse.(X_, y_)->make_classification(n_samples=200, n_features=100, weights=[0.833, 0.167], random_state=0)
A:sklearn.svm.tests.test_sparse.X_->scipy.sparse.csr_matrix(X_)
A:sklearn.svm.tests.test_sparse.y_pred->sklearn.svm.SVC(kernel='linear').fit(X.toarray(), y).predict(X_[180:])
A:sklearn.svm.tests.test_sparse.(X_blobs, _)->make_blobs(n_samples=100, centers=10, random_state=0)
A:sklearn.svm.tests.test_sparse.data->numpy.array([0.03771744, 0.1003567, 0.01174647, 0.027069])
A:sklearn.svm.tests.test_sparse.indices->numpy.array([6, 5, 35, 31])
A:sklearn.svm.tests.test_sparse.indptr->numpy.array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4])
A:sklearn.svm.tests.test_sparse.y->numpy.array([1.0, 0.0, 2.0, 2.0, 1.0, 1.0, 1.0, 2.0, 2.0, 0.0, 1.0, 2.0, 2.0, 0.0, 2.0, 0.0, 3.0, 0.0, 3.0, 0.0, 1.0, 1.0, 3.0, 2.0, 3.0, 2.0, 0.0, 3.0, 1.0, 0.0, 2.0, 1.0, 2.0, 0.0, 1.0, 0.0, 2.0, 3.0, 1.0, 3.0, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 2.0, 2.0, 3.0, 2.0, 0.0, 3.0, 2.0, 1.0, 2.0, 3.0, 2.0, 2.0, 0.0, 1.0, 0.0, 1.0, 2.0, 3.0, 0.0, 0.0, 2.0, 2.0, 1.0, 3.0, 1.0, 1.0, 0.0, 1.0, 2.0, 1.0, 1.0, 3.0])
A:sklearn.svm.tests.test_sparse.a->sklearn.svm.SVC(probability=True, max_iter=1, random_state=0)
A:sklearn.svm.tests.test_sparse.b->sklearn.base.clone(a)
A:sklearn.svm.tests.test_sparse.dense_svm->sklearn.svm.SVC(C=1, kernel=lambda x, y: np.dot(x, y.T), probability=True, random_state=0)
A:sklearn.svm.tests.test_sparse.pred_dense->sklearn.svm.SVC(C=1, kernel=lambda x, y: np.dot(x, y.T), probability=True, random_state=0).fit(X, Y).predict(X)
A:sklearn.svm.tests.test_sparse.sp->sklearn.svm.SVC(C=1, kernel=lambda x, y: x * y.T, probability=True, random_state=0, max_iter=1)
A:sklearn.svm.tests.test_sparse.proba_1->sklearn.svm.SVC(probability=True, max_iter=1, random_state=0).fit(X, Y).predict_proba(X)
A:sklearn.svm.tests.test_sparse.proba_2->sklearn.svm.SVC(probability=True, max_iter=1, random_state=0).fit(X, Y).predict_proba(X)
sklearn.svm.tests.test_sparse.check_svm_model_equal(dense_svm,sparse_svm,X_train,y_train,X_test)
sklearn.svm.tests.test_sparse.test_consistent_proba()
sklearn.svm.tests.test_sparse.test_error()
sklearn.svm.tests.test_sparse.test_linearsvc()
sklearn.svm.tests.test_sparse.test_linearsvc_iris()
sklearn.svm.tests.test_sparse.test_sample_weights()
sklearn.svm.tests.test_sparse.test_sparse_decision_function()
sklearn.svm.tests.test_sparse.test_sparse_liblinear_intercept_handling()
sklearn.svm.tests.test_sparse.test_sparse_oneclasssvm()
sklearn.svm.tests.test_sparse.test_sparse_realdata()
sklearn.svm.tests.test_sparse.test_sparse_svc_clone_with_callable_kernel()
sklearn.svm.tests.test_sparse.test_svc()
sklearn.svm.tests.test_sparse.test_svc_iris()
sklearn.svm.tests.test_sparse.test_svc_with_custom_kernel()
sklearn.svm.tests.test_sparse.test_timeout()
sklearn.svm.tests.test_sparse.test_unsorted_indices()
sklearn.svm.tests.test_sparse.test_weight()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/neighbors/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/neighbors/kde.py----------------------------------------
A:sklearn.neighbors.kde.algorithm->self._choose_algorithm(self.algorithm, self.metric)
A:sklearn.neighbors.kde.X->check_random_state(random_state).normal(size=(n_samples, dim))
A:sklearn.neighbors.kde.self.tree_->TREE_DICT[algorithm](X, metric=self.metric, leaf_size=self.leaf_size, **kwargs)
A:sklearn.neighbors.kde.log_density->self.tree_.kernel_density(X, h=self.bandwidth, kernel=self.kernel, atol=atol_N, rtol=self.rtol, breadth_first=self.breadth_first, return_log=True)
A:sklearn.neighbors.kde.data->numpy.asarray(self.tree_.data)
A:sklearn.neighbors.kde.rng->check_random_state(random_state)
A:sklearn.neighbors.kde.i->check_random_state(random_state).randint(data.shape[0], size=n_samples)
A:sklearn.neighbors.kde.s_sq->row_norms(X, squared=True)
sklearn.neighbors.KernelDensity(self,bandwidth=1.0,algorithm='auto',kernel='gaussian',metric='euclidean',atol=0,rtol=0,breadth_first=True,leaf_size=40,metric_params=None)
sklearn.neighbors.KernelDensity._choose_algorithm(self,algorithm,metric)
sklearn.neighbors.KernelDensity.fit(self,X,y=None)
sklearn.neighbors.KernelDensity.sample(self,n_samples=1,random_state=None)
sklearn.neighbors.KernelDensity.score(self,X,y=None)
sklearn.neighbors.KernelDensity.score_samples(self,X)
sklearn.neighbors.kde.KernelDensity(self,bandwidth=1.0,algorithm='auto',kernel='gaussian',metric='euclidean',atol=0,rtol=0,breadth_first=True,leaf_size=40,metric_params=None)
sklearn.neighbors.kde.KernelDensity.__init__(self,bandwidth=1.0,algorithm='auto',kernel='gaussian',metric='euclidean',atol=0,rtol=0,breadth_first=True,leaf_size=40,metric_params=None)
sklearn.neighbors.kde.KernelDensity._choose_algorithm(self,algorithm,metric)
sklearn.neighbors.kde.KernelDensity.fit(self,X,y=None)
sklearn.neighbors.kde.KernelDensity.sample(self,n_samples=1,random_state=None)
sklearn.neighbors.kde.KernelDensity.score(self,X,y=None)
sklearn.neighbors.kde.KernelDensity.score_samples(self,X)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/neighbors/lof.py----------------------------------------
A:sklearn.neighbors.lof.self.n_neighbors_->max(1, min(self.n_neighbors, n_samples - 1))
A:sklearn.neighbors.lof.(self._distances_fit_X_, _neighbors_indices_fit_X_)->self.kneighbors(None, n_neighbors=self.n_neighbors_)
A:sklearn.neighbors.lof.self._lrd->self._local_reachability_density(self._distances_fit_X_, _neighbors_indices_fit_X_)
A:sklearn.neighbors.lof.X->check_array(X, accept_sparse='csr')
A:sklearn.neighbors.lof.is_inlier->numpy.ones(self._fit_X.shape[0], dtype=int)
A:sklearn.neighbors.lof.(distances_X, neighbors_indices_X)->self.kneighbors(X, n_neighbors=self.n_neighbors_)
A:sklearn.neighbors.lof.X_lrd->self._local_reachability_density(distances_X, neighbors_indices_X)
A:sklearn.neighbors.lof.reach_dist_array->numpy.maximum(distances_X, dist_k)
sklearn.neighbors.LocalOutlierFactor(self,n_neighbors=20,algorithm='auto',leaf_size=30,metric='minkowski',p=2,metric_params=None,contamination=0.1,n_jobs=1)
sklearn.neighbors.LocalOutlierFactor._decision_function(self,X)
sklearn.neighbors.LocalOutlierFactor._local_reachability_density(self,distances_X,neighbors_indices)
sklearn.neighbors.LocalOutlierFactor._predict(self,X=None)
sklearn.neighbors.LocalOutlierFactor.fit(self,X,y=None)
sklearn.neighbors.LocalOutlierFactor.fit_predict(self,X,y=None)
sklearn.neighbors.lof.LocalOutlierFactor(self,n_neighbors=20,algorithm='auto',leaf_size=30,metric='minkowski',p=2,metric_params=None,contamination=0.1,n_jobs=1)
sklearn.neighbors.lof.LocalOutlierFactor.__init__(self,n_neighbors=20,algorithm='auto',leaf_size=30,metric='minkowski',p=2,metric_params=None,contamination=0.1,n_jobs=1)
sklearn.neighbors.lof.LocalOutlierFactor._decision_function(self,X)
sklearn.neighbors.lof.LocalOutlierFactor._local_reachability_density(self,distances_X,neighbors_indices)
sklearn.neighbors.lof.LocalOutlierFactor._predict(self,X=None)
sklearn.neighbors.lof.LocalOutlierFactor.fit(self,X,y=None)
sklearn.neighbors.lof.LocalOutlierFactor.fit_predict(self,X,y=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/neighbors/classification.py----------------------------------------
A:sklearn.neighbors.classification.self.weights->_check_weights(weights)
A:sklearn.neighbors.classification.X->check_array(X, accept_sparse='csr')
A:sklearn.neighbors.classification.(neigh_dist, neigh_ind)->self.radius_neighbors(X)
A:sklearn.neighbors.classification._y->self._y.reshape((-1, 1))
A:sklearn.neighbors.classification.n_outputs->len(classes_)
A:sklearn.neighbors.classification.weights->_get_weights(neigh_dist, self.weights)
A:sklearn.neighbors.classification.y_pred->y_pred.ravel().ravel()
A:sklearn.neighbors.classification.(mode, _)->weighted_mode(_y[neigh_ind, k], weights, axis=1)
A:sklearn.neighbors.classification.mode->mode.ravel().ravel()
A:sklearn.neighbors.classification.y_pred[:, k]->classes_k.take(mode)
A:sklearn.neighbors.classification.all_rows->numpy.arange(X.shape[0])
A:sklearn.neighbors.classification.proba_k->numpy.zeros((n_samples, classes_k.size))
A:sklearn.neighbors.classification.pred_labels->numpy.zeros(len(neigh_ind), dtype=object)
A:sklearn.neighbors.classification.y_pred[inliers, k]->classes_k.take(mode)
sklearn.neighbors.KNeighborsClassifier(self,n_neighbors=5,weights='uniform',algorithm='auto',leaf_size=30,p=2,metric='minkowski',metric_params=None,n_jobs=1,**kwargs)
sklearn.neighbors.KNeighborsClassifier.predict(self,X)
sklearn.neighbors.KNeighborsClassifier.predict_proba(self,X)
sklearn.neighbors.RadiusNeighborsClassifier(self,radius=1.0,weights='uniform',algorithm='auto',leaf_size=30,p=2,metric='minkowski',outlier_label=None,metric_params=None,**kwargs)
sklearn.neighbors.RadiusNeighborsClassifier.predict(self,X)
sklearn.neighbors.classification.KNeighborsClassifier(self,n_neighbors=5,weights='uniform',algorithm='auto',leaf_size=30,p=2,metric='minkowski',metric_params=None,n_jobs=1,**kwargs)
sklearn.neighbors.classification.KNeighborsClassifier.__init__(self,n_neighbors=5,weights='uniform',algorithm='auto',leaf_size=30,p=2,metric='minkowski',metric_params=None,n_jobs=1,**kwargs)
sklearn.neighbors.classification.KNeighborsClassifier.predict(self,X)
sklearn.neighbors.classification.KNeighborsClassifier.predict_proba(self,X)
sklearn.neighbors.classification.RadiusNeighborsClassifier(self,radius=1.0,weights='uniform',algorithm='auto',leaf_size=30,p=2,metric='minkowski',outlier_label=None,metric_params=None,**kwargs)
sklearn.neighbors.classification.RadiusNeighborsClassifier.__init__(self,radius=1.0,weights='uniform',algorithm='auto',leaf_size=30,p=2,metric='minkowski',outlier_label=None,metric_params=None,**kwargs)
sklearn.neighbors.classification.RadiusNeighborsClassifier.predict(self,X)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/neighbors/graph.py----------------------------------------
A:sklearn.neighbors.graph.params->zip(['metric', 'p', 'metric_params'], [metric, p, metric_params])
A:sklearn.neighbors.graph.est_params->NearestNeighbors(radius=radius, metric=metric, p=p, metric_params=metric_params, n_jobs=n_jobs).fit(X).get_params()
A:sklearn.neighbors.graph.X->NearestNeighbors(radius=radius, metric=metric, p=p, metric_params=metric_params, n_jobs=n_jobs).fit(X)
A:sklearn.neighbors.graph.query->_query_include_self(X, include_self)
sklearn.neighbors.graph._check_params(X,metric,p,metric_params)
sklearn.neighbors.graph._query_include_self(X,include_self)
sklearn.neighbors.graph.kneighbors_graph(X,n_neighbors,mode='connectivity',metric='minkowski',p=2,metric_params=None,include_self=False,n_jobs=1)
sklearn.neighbors.graph.radius_neighbors_graph(X,radius,mode='connectivity',metric='minkowski',p=2,metric_params=None,include_self=False,n_jobs=1)
sklearn.neighbors.kneighbors_graph(X,n_neighbors,mode='connectivity',metric='minkowski',p=2,metric_params=None,include_self=False,n_jobs=1)
sklearn.neighbors.radius_neighbors_graph(X,radius,mode='connectivity',metric='minkowski',p=2,metric_params=None,include_self=False,n_jobs=1)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/neighbors/setup.py----------------------------------------
A:sklearn.neighbors.setup.config->Configuration('neighbors', parent_package, top_path)
sklearn.neighbors.setup.configuration(parent_package='',top_path=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/neighbors/approximate.py----------------------------------------
A:sklearn.neighbors.approximate.left_index->numpy.searchsorted(tree, bin_X & left_mask)
A:sklearn.neighbors.approximate.right_index->numpy.searchsorted(tree, bin_X | right_mask, side='right')
A:sklearn.neighbors.approximate.hi->numpy.empty_like(bin_X, dtype=np.intp)
A:sklearn.neighbors.approximate.lo->numpy.zeros_like(bin_X, dtype=np.intp)
A:sklearn.neighbors.approximate.res->numpy.empty_like(bin_X, dtype=np.intp)
A:sklearn.neighbors.approximate.(left_idx, right_idx)->_find_matching_indices(tree, bin_X.take(kept), left_masks[mid], right_masks[mid])
A:sklearn.neighbors.approximate.r->numpy.arange(bin_X.shape[0])
A:sklearn.neighbors.approximate.out->numpy.empty(len(list_of_arrays), dtype=object)
A:sklearn.neighbors.approximate.candidate_X->self._fit_X.take(candidates, axis=0, mode='clip')
A:sklearn.neighbors.approximate.distance_positions->numpy.argsort(distances)
A:sklearn.neighbors.approximate.distances->distances.take(distance_positions, mode='clip', axis=0).take(distance_positions, mode='clip', axis=0)
A:sklearn.neighbors.approximate.self._left_mask->numpy.packbits(left_mask).view(dtype=HASH_DTYPE)
A:sklearn.neighbors.approximate.self._right_mask->numpy.packbits(right_mask).view(dtype=HASH_DTYPE)
A:sklearn.neighbors.approximate.candidate_set->set()
A:sklearn.neighbors.approximate.(start, stop)->_find_matching_indices(self.trees_[i], bin_queries[i], left_mask, right_mask)
A:sklearn.neighbors.approximate.candidates->numpy.setdiff1d(candidates, total_candidates)
A:sklearn.neighbors.approximate.remaining->numpy.setdiff1d(np.arange(0, index_size), candidates)
A:sklearn.neighbors.approximate.(ranks, distances)->self._compute_distances(query, candidates)
A:sklearn.neighbors.approximate.total_candidates->numpy.append(total_candidates, candidates)
A:sklearn.neighbors.approximate.total_neighbors->numpy.insert(total_neighbors, positions, candidates[ranks[:m]])
A:sklearn.neighbors.approximate.total_distances->numpy.insert(total_distances, positions, distances[:m])
A:sklearn.neighbors.approximate.m->numpy.searchsorted(distances, radius, side='right')
A:sklearn.neighbors.approximate.positions->self.trees_[i].searchsorted(bin_X)
A:sklearn.neighbors.approximate.self._fit_X->numpy.row_stack((self._fit_X, X))
A:sklearn.neighbors.approximate.rng->check_random_state(self.random_state)
A:sklearn.neighbors.approximate.hasher->GaussianRandomProjectionHash(MAX_HASH_SIZE, rng.randint(0, int_max))
A:sklearn.neighbors.approximate.original_index->numpy.argsort(hashes)
A:sklearn.neighbors.approximate.bin_queries->numpy.rollaxis(bin_queries, 1)
A:sklearn.neighbors.approximate.X->check_array(X, accept_sparse='csr')
A:sklearn.neighbors.approximate.(bin_queries, max_depth)->self._query(X)
A:sklearn.neighbors.approximate.(neighs, dists)->self._get_radius_neighbors(X[[i]], max_depth[i], bin_queries[i], radius)
A:sklearn.neighbors.approximate.self.trees_[i]->numpy.insert(self.trees_[i], positions, bin_X)
A:sklearn.neighbors.approximate.self.original_indices_[i]->numpy.insert(self.original_indices_[i], positions, np.arange(n_indexed, n_indexed + n_samples))
sklearn.neighbors.LSHForest(self,n_estimators=10,radius=1.0,n_candidates=50,n_neighbors=5,min_hash_match=4,radius_cutoff_ratio=0.9,random_state=None)
sklearn.neighbors.LSHForest._compute_distances(self,query,candidates)
sklearn.neighbors.LSHForest._generate_masks(self)
sklearn.neighbors.LSHForest._get_candidates(self,query,max_depth,bin_queries,n_neighbors)
sklearn.neighbors.LSHForest._get_radius_neighbors(self,query,max_depth,bin_queries,radius)
sklearn.neighbors.LSHForest._query(self,X)
sklearn.neighbors.LSHForest.fit(self,X,y=None)
sklearn.neighbors.LSHForest.kneighbors(self,X,n_neighbors=None,return_distance=True)
sklearn.neighbors.LSHForest.partial_fit(self,X,y=None)
sklearn.neighbors.LSHForest.radius_neighbors(self,X,radius=None,return_distance=True)
sklearn.neighbors.approximate.GaussianRandomProjectionHash(self,n_components=32,random_state=None)
sklearn.neighbors.approximate.GaussianRandomProjectionHash.__init__(self,n_components=32,random_state=None)
sklearn.neighbors.approximate.LSHForest(self,n_estimators=10,radius=1.0,n_candidates=50,n_neighbors=5,min_hash_match=4,radius_cutoff_ratio=0.9,random_state=None)
sklearn.neighbors.approximate.LSHForest.__init__(self,n_estimators=10,radius=1.0,n_candidates=50,n_neighbors=5,min_hash_match=4,radius_cutoff_ratio=0.9,random_state=None)
sklearn.neighbors.approximate.LSHForest._compute_distances(self,query,candidates)
sklearn.neighbors.approximate.LSHForest._generate_masks(self)
sklearn.neighbors.approximate.LSHForest._get_candidates(self,query,max_depth,bin_queries,n_neighbors)
sklearn.neighbors.approximate.LSHForest._get_radius_neighbors(self,query,max_depth,bin_queries,radius)
sklearn.neighbors.approximate.LSHForest._query(self,X)
sklearn.neighbors.approximate.LSHForest.fit(self,X,y=None)
sklearn.neighbors.approximate.LSHForest.kneighbors(self,X,n_neighbors=None,return_distance=True)
sklearn.neighbors.approximate.LSHForest.partial_fit(self,X,y=None)
sklearn.neighbors.approximate.LSHForest.radius_neighbors(self,X,radius=None,return_distance=True)
sklearn.neighbors.approximate.ProjectionToHashMixin(object)
sklearn.neighbors.approximate.ProjectionToHashMixin._to_hash(projected)
sklearn.neighbors.approximate.ProjectionToHashMixin.fit_transform(self,X,y=None)
sklearn.neighbors.approximate.ProjectionToHashMixin.transform(self,X)
sklearn.neighbors.approximate._array_of_arrays(list_of_arrays)
sklearn.neighbors.approximate._find_longest_prefix_match(tree,bin_X,hash_size,left_masks,right_masks)
sklearn.neighbors.approximate._find_matching_indices(tree,bin_X,left_mask,right_mask)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/neighbors/unsupervised.py----------------------------------------
sklearn.neighbors.NearestNeighbors(self,n_neighbors=5,radius=1.0,algorithm='auto',leaf_size=30,metric='minkowski',p=2,metric_params=None,n_jobs=1,**kwargs)
sklearn.neighbors.unsupervised.NearestNeighbors(self,n_neighbors=5,radius=1.0,algorithm='auto',leaf_size=30,metric='minkowski',p=2,metric_params=None,n_jobs=1,**kwargs)
sklearn.neighbors.unsupervised.NearestNeighbors.__init__(self,n_neighbors=5,radius=1.0,algorithm='auto',leaf_size=30,metric='minkowski',p=2,metric_params=None,n_jobs=1,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/neighbors/base.py----------------------------------------
A:sklearn.neighbors.base.VALID_METRICS->dict(ball_tree=BallTree.valid_metrics, kd_tree=KDTree.valid_metrics, brute=list(PAIRWISE_DISTANCE_FUNCTIONS.keys()) + ['braycurtis', 'canberra', 'chebyshev', 'correlation', 'cosine', 'dice', 'hamming', 'jaccard', 'kulsinski', 'mahalanobis', 'matching', 'minkowski', 'rogerstanimoto', 'russellrao', 'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean', 'yule', 'wminkowski'])
A:sklearn.neighbors.base.VALID_METRICS_SPARSE->dict(ball_tree=[], kd_tree=[], brute=PAIRWISE_DISTANCE_FUNCTIONS.keys())
A:sklearn.neighbors.base.inf_mask->numpy.isinf(dist)
A:sklearn.neighbors.base.inf_row->numpy.any(inf_mask, axis=1)
A:sklearn.neighbors.base.self.effective_metric_params_->self.metric_params.copy()
A:sklearn.neighbors.base.effective_p->self.effective_metric_params_.get('p', self.p)
A:sklearn.neighbors.base.p->self.effective_metric_params_.pop('p', 2)
A:sklearn.neighbors.base.X->check_array(X, accept_sparse=['csr', 'csc', 'coo'])
A:sklearn.neighbors.base.self._fit_X->check_array(X, accept_sparse=['csr', 'csc', 'coo']).copy()
A:sklearn.neighbors.base.self._tree->KDTree(X, self.leaf_size, metric=self.effective_metric_, **self.effective_metric_params_)
A:sklearn.neighbors.base.n_jobs->_get_n_jobs(self.n_jobs)
A:sklearn.neighbors.base.dist->pairwise_distances(X, self._fit_X, self.effective_metric_, n_jobs=self.n_jobs, **self.effective_metric_params_)
A:sklearn.neighbors.base.neigh_ind->numpy.empty(n_samples, dtype='object')
A:sklearn.neighbors.base.result->numpy.vstack(result)
A:sklearn.neighbors.base.(dist, neigh_ind)->tuple(zip(*result))
A:sklearn.neighbors.base.dup_gr_nbrs->numpy.all(sample_mask, axis=1)
A:sklearn.neighbors.base.A_indptr->numpy.concatenate((np.zeros(1, dtype=int), np.cumsum(n_neighbors)))
A:sklearn.neighbors.base.A_data->numpy.ones(len(A_ind))
A:sklearn.neighbors.base.A_ind->numpy.concatenate(list(A_ind))
A:sklearn.neighbors.base.(A_data, A_ind)->self.kneighbors(X, n_neighbors, return_distance=True)
A:sklearn.neighbors.base.kneighbors_graph->csr_matrix((A_data, A_ind.ravel(), A_indptr), shape=(n_samples1, n_samples2))
A:sklearn.neighbors.base.dist_array->numpy.empty(n_samples, dtype='object')
A:sklearn.neighbors.base.results->self._tree.query_radius(X, radius, return_distance=return_distance)
A:sklearn.neighbors.base.(dist, A_ind)->self.radius_neighbors(X, radius, return_distance=True)
A:sklearn.neighbors.base.n_neighbors->numpy.array([len(a) for a in A_ind])
A:sklearn.neighbors.base.(X, y)->check_X_y(X, y, 'csr', multi_output=True)
A:sklearn.neighbors.base.y->y.reshape((-1, 1)).reshape((-1, 1))
A:sklearn.neighbors.base.self._y->self._y.ravel()
A:sklearn.neighbors.base.(classes, self._y[:, k])->numpy.unique(y[:, k], return_inverse=True)
sklearn.neighbors.base.KNeighborsMixin(object)
sklearn.neighbors.base.KNeighborsMixin.kneighbors(self,X=None,n_neighbors=None,return_distance=True)
sklearn.neighbors.base.KNeighborsMixin.kneighbors_graph(self,X=None,n_neighbors=None,mode='connectivity')
sklearn.neighbors.base.NeighborsBase(self)
sklearn.neighbors.base.NeighborsBase.__init__(self)
sklearn.neighbors.base.NeighborsBase._fit(self,X)
sklearn.neighbors.base.NeighborsBase._init_params(self,n_neighbors=None,radius=None,algorithm='auto',leaf_size=30,metric='minkowski',p=2,metric_params=None,n_jobs=1)
sklearn.neighbors.base.NeighborsBase._pairwise(self)
sklearn.neighbors.base.RadiusNeighborsMixin(object)
sklearn.neighbors.base.RadiusNeighborsMixin.radius_neighbors(self,X=None,radius=None,return_distance=True)
sklearn.neighbors.base.RadiusNeighborsMixin.radius_neighbors_graph(self,X=None,radius=None,mode='connectivity')
sklearn.neighbors.base.SupervisedFloatMixin(object)
sklearn.neighbors.base.SupervisedFloatMixin.fit(self,X,y)
sklearn.neighbors.base.SupervisedIntegerMixin(object)
sklearn.neighbors.base.SupervisedIntegerMixin.fit(self,X,y)
sklearn.neighbors.base.UnsupervisedMixin(object)
sklearn.neighbors.base.UnsupervisedMixin.fit(self,X,y=None)
sklearn.neighbors.base._check_weights(weights)
sklearn.neighbors.base._get_weights(dist,weights)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/neighbors/regression.py----------------------------------------
A:sklearn.neighbors.regression.self.weights->_check_weights(weights)
A:sklearn.neighbors.regression.X->check_array(X, accept_sparse='csr')
A:sklearn.neighbors.regression.(neigh_dist, neigh_ind)->self.radius_neighbors(X)
A:sklearn.neighbors.regression.weights->_get_weights(neigh_dist, self.weights)
A:sklearn.neighbors.regression._y->_y.reshape((-1, 1)).reshape((-1, 1))
A:sklearn.neighbors.regression.y_pred->y_pred.ravel().ravel()
A:sklearn.neighbors.regression.denom->numpy.sum(weights, axis=1)
A:sklearn.neighbors.regression.num->numpy.sum(_y[neigh_ind, j] * weights, axis=1)
sklearn.neighbors.KNeighborsRegressor(self,n_neighbors=5,weights='uniform',algorithm='auto',leaf_size=30,p=2,metric='minkowski',metric_params=None,n_jobs=1,**kwargs)
sklearn.neighbors.KNeighborsRegressor.predict(self,X)
sklearn.neighbors.RadiusNeighborsRegressor(self,radius=1.0,weights='uniform',algorithm='auto',leaf_size=30,p=2,metric='minkowski',metric_params=None,**kwargs)
sklearn.neighbors.RadiusNeighborsRegressor.predict(self,X)
sklearn.neighbors.regression.KNeighborsRegressor(self,n_neighbors=5,weights='uniform',algorithm='auto',leaf_size=30,p=2,metric='minkowski',metric_params=None,n_jobs=1,**kwargs)
sklearn.neighbors.regression.KNeighborsRegressor.__init__(self,n_neighbors=5,weights='uniform',algorithm='auto',leaf_size=30,p=2,metric='minkowski',metric_params=None,n_jobs=1,**kwargs)
sklearn.neighbors.regression.KNeighborsRegressor.predict(self,X)
sklearn.neighbors.regression.RadiusNeighborsRegressor(self,radius=1.0,weights='uniform',algorithm='auto',leaf_size=30,p=2,metric='minkowski',metric_params=None,**kwargs)
sklearn.neighbors.regression.RadiusNeighborsRegressor.__init__(self,radius=1.0,weights='uniform',algorithm='auto',leaf_size=30,p=2,metric='minkowski',metric_params=None,**kwargs)
sklearn.neighbors.regression.RadiusNeighborsRegressor.predict(self,X)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/neighbors/nearest_centroid.py----------------------------------------
A:sklearn.neighbors.nearest_centroid.(X, y)->check_X_y(X, y, ['csr', 'csc'])
A:sklearn.neighbors.nearest_centroid.is_X_sparse->scipy.sparse.issparse(X)
A:sklearn.neighbors.nearest_centroid.le->LabelEncoder()
A:sklearn.neighbors.nearest_centroid.y_ind->LabelEncoder().fit_transform(y)
A:sklearn.neighbors.nearest_centroid.self.centroids_->numpy.empty((n_classes, n_features), dtype=np.float64)
A:sklearn.neighbors.nearest_centroid.nk->numpy.zeros(n_classes)
A:sklearn.neighbors.nearest_centroid.nk[cur_class]->numpy.sum(center_mask)
A:sklearn.neighbors.nearest_centroid.self.centroids_[cur_class]->X[center_mask].mean(axis=0)
A:sklearn.neighbors.nearest_centroid.dataset_centroid_->numpy.mean(X, axis=0)
A:sklearn.neighbors.nearest_centroid.m->numpy.sqrt(1.0 / nk - 1.0 / n_samples)
A:sklearn.neighbors.nearest_centroid.variance->variance.sum(axis=0).sum(axis=0)
A:sklearn.neighbors.nearest_centroid.s->numpy.sqrt(variance / (n_samples - n_classes))
A:sklearn.neighbors.nearest_centroid.mm->numpy.sqrt(1.0 / nk - 1.0 / n_samples).reshape(len(m), 1)
A:sklearn.neighbors.nearest_centroid.signs->numpy.sign(deviation)
A:sklearn.neighbors.nearest_centroid.X->check_array(X, accept_sparse='csr')
sklearn.neighbors.NearestCentroid(self,metric='euclidean',shrink_threshold=None)
sklearn.neighbors.NearestCentroid.fit(self,X,y)
sklearn.neighbors.NearestCentroid.predict(self,X)
sklearn.neighbors.nearest_centroid.NearestCentroid(self,metric='euclidean',shrink_threshold=None)
sklearn.neighbors.nearest_centroid.NearestCentroid.__init__(self,metric='euclidean',shrink_threshold=None)
sklearn.neighbors.nearest_centroid.NearestCentroid.fit(self,X,y)
sklearn.neighbors.nearest_centroid.NearestCentroid.predict(self,X)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/neighbors/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/neighbors/tests/test_lof.py----------------------------------------
A:sklearn.neighbors.tests.test_lof.rng->numpy.random.RandomState(random_state)
A:sklearn.neighbors.tests.test_lof.iris->load_iris()
A:sklearn.neighbors.tests.test_lof.perm->numpy.random.RandomState(random_state).permutation(iris.target.size)
A:sklearn.neighbors.tests.test_lof.clf->sklearn.neighbors.LocalOutlierFactor(n_neighbors=500)
A:sklearn.neighbors.tests.test_lof.X_outliers->numpy.random.RandomState(random_state).uniform(low=-4, high=4, size=(20, 2))
A:sklearn.neighbors.tests.test_lof.y_test->numpy.array([0] * 20 + [1] * 20)
A:sklearn.neighbors.tests.test_lof.X->numpy.random.RandomState(random_state).random_sample((10, 4))
A:sklearn.neighbors.tests.test_lof.Y->numpy.random.RandomState(random_state).random_sample((3, 4))
A:sklearn.neighbors.tests.test_lof.DXX->sklearn.metrics.pairwise_distances(X, metric='euclidean')
A:sklearn.neighbors.tests.test_lof.DYX->sklearn.metrics.pairwise_distances(Y, X, metric='euclidean')
A:sklearn.neighbors.tests.test_lof.lof_X->sklearn.neighbors.LocalOutlierFactor(n_neighbors=3)
A:sklearn.neighbors.tests.test_lof.pred_X_X->sklearn.neighbors.LocalOutlierFactor(n_neighbors=3)._predict()
A:sklearn.neighbors.tests.test_lof.pred_X_Y->sklearn.neighbors.LocalOutlierFactor(n_neighbors=3)._predict(Y)
A:sklearn.neighbors.tests.test_lof.lof_D->sklearn.neighbors.LocalOutlierFactor(n_neighbors=3, algorithm='brute', metric='precomputed')
A:sklearn.neighbors.tests.test_lof.pred_D_X->sklearn.neighbors.LocalOutlierFactor(n_neighbors=3, algorithm='brute', metric='precomputed')._predict()
A:sklearn.neighbors.tests.test_lof.pred_D_Y->sklearn.neighbors.LocalOutlierFactor(n_neighbors=3, algorithm='brute', metric='precomputed')._predict(DYX)
sklearn.neighbors.tests.test_lof.test_lof()
sklearn.neighbors.tests.test_lof.test_lof_performance()
sklearn.neighbors.tests.test_lof.test_lof_precomputed(random_state=42)
sklearn.neighbors.tests.test_lof.test_lof_values()
sklearn.neighbors.tests.test_lof.test_n_neighbors_attribute()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/neighbors/tests/test_quad_tree.py----------------------------------------
A:sklearn.neighbors.tests.test_quad_tree.tree->_QuadTree(n_dimensions=n_dimensions, verbose=0)
A:sklearn.neighbors.tests.test_quad_tree.rng->check_random_state(0)
A:sklearn.neighbors.tests.test_quad_tree.X->check_random_state(0).random_sample((10, n_dimensions))
A:sklearn.neighbors.tests.test_quad_tree.s->pickle.dumps(tree, protocol=protocol)
A:sklearn.neighbors.tests.test_quad_tree.bt2->pickle.loads(s)
A:sklearn.neighbors.tests.test_quad_tree.cell_x_tree->_QuadTree(n_dimensions=n_dimensions, verbose=0).get_cell(x)
A:sklearn.neighbors.tests.test_quad_tree.cell_x_bt2->pickle.loads(s).get_cell(x)
A:sklearn.neighbors.tests.test_quad_tree.cell_id->_QuadTree(n_dimensions=n_dimensions, verbose=0).get_cell(x)
sklearn.neighbors.tests.test_quad_tree.test_qt_insert_duplicate()
sklearn.neighbors.tests.test_quad_tree.test_quad_tree_pickle()
sklearn.neighbors.tests.test_quad_tree.test_quadtree_boundary_computation()
sklearn.neighbors.tests.test_quad_tree.test_quadtree_similar_point()
sklearn.neighbors.tests.test_quad_tree.test_summarize()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/neighbors/tests/test_kde.py----------------------------------------
A:sklearn.neighbors.tests.test_kde.d->numpy.sqrt(((Y[:, None, :] - X) ** 2).sum(-1))
A:sklearn.neighbors.tests.test_kde.kde->KernelDensity(algorithm=algorithm, metric=metric)
A:sklearn.neighbors.tests.test_kde.log_dens->KernelDensity(algorithm=algorithm, metric=metric).fit(X).score_samples(Y)
A:sklearn.neighbors.tests.test_kde.rng->numpy.random.RandomState(0)
A:sklearn.neighbors.tests.test_kde.X->numpy.random.RandomState(0).randn(10, 2)
A:sklearn.neighbors.tests.test_kde.Y->numpy.random.RandomState(0).randn(10, 2)
A:sklearn.neighbors.tests.test_kde.dens_true->compute_kernel_slow(Y, X, kernel, bandwidth)
A:sklearn.neighbors.tests.test_kde.samp->KernelDensity(algorithm=algorithm, metric=metric).sample(100)
A:sklearn.neighbors.tests.test_kde.nbrs->NearestNeighbors(n_neighbors=1).fit(X)
A:sklearn.neighbors.tests.test_kde.(dist, ind)->NearestNeighbors(n_neighbors=1).fit(X).kneighbors(X, return_distance=True)
A:sklearn.neighbors.tests.test_kde.y_dens->KernelDensity(algorithm=algorithm, metric=metric).score_samples(Y)
A:sklearn.neighbors.tests.test_kde.(X, _)->make_blobs(cluster_std=0.1, random_state=1, centers=[[0, 1], [1, 0], [0, 0]])
A:sklearn.neighbors.tests.test_kde.pipe1->make_pipeline(StandardScaler(with_mean=False, with_std=False), KernelDensity(kernel='gaussian'))
A:sklearn.neighbors.tests.test_kde.params->dict(kerneldensity__bandwidth=[0.001, 0.01, 0.1, 1, 10])
A:sklearn.neighbors.tests.test_kde.search->GridSearchCV(pipe1, param_grid=params, cv=5)
sklearn.neighbors.tests.test_kde.check_results(kernel,bandwidth,atol,rtol,X,Y,dens_true)
sklearn.neighbors.tests.test_kde.compute_kernel_slow(Y,X,kernel,h)
sklearn.neighbors.tests.test_kde.test_kde_algorithm_metric_choice()
sklearn.neighbors.tests.test_kde.test_kde_badargs()
sklearn.neighbors.tests.test_kde.test_kde_pipeline_gridsearch()
sklearn.neighbors.tests.test_kde.test_kde_score(n_samples=100,n_features=3)
sklearn.neighbors.tests.test_kde.test_kernel_density(n_samples=100,n_features=3)
sklearn.neighbors.tests.test_kde.test_kernel_density_sampling(n_samples=100,n_features=3)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/neighbors/tests/test_ball_tree.py----------------------------------------
A:sklearn.neighbors.tests.test_ball_tree.rng->check_random_state(0)
A:sklearn.neighbors.tests.test_ball_tree.V_mahalanobis->numpy.dot(V_mahalanobis, V_mahalanobis.T)
A:sklearn.neighbors.tests.test_ball_tree.D->sklearn.neighbors.dist_metrics.DistanceMetric.get_metric('euclidean').pairwise(Y, X)
A:sklearn.neighbors.tests.test_ball_tree.X->check_random_state(0).random_sample((10, 3))
A:sklearn.neighbors.tests.test_ball_tree.Y->check_random_state(0).random_sample((n_samples, n_features))
A:sklearn.neighbors.tests.test_ball_tree.bt->BallTree(X, leaf_size=1, metric='haversine')
A:sklearn.neighbors.tests.test_ball_tree.(dist1, ind1)->BallTree(X, leaf_size=1, metric='haversine').query(X, k=5)
A:sklearn.neighbors.tests.test_ball_tree.(dist2, ind2)->brute_force_neighbors(X, X, k=5, metric='haversine')
A:sklearn.neighbors.tests.test_ball_tree.query_pt->numpy.zeros(n_features, dtype=float)
A:sklearn.neighbors.tests.test_ball_tree.rad->numpy.sqrt(((X - query_pt) ** 2).sum(1))
A:sklearn.neighbors.tests.test_ball_tree.(ind, dist)->BallTree(X, leaf_size=1, metric='haversine').query_radius([query_pt], r + eps, return_distance=True)
A:sklearn.neighbors.tests.test_ball_tree.d->numpy.sqrt(((Y[:, None, :] - X) ** 2).sum(-1))
A:sklearn.neighbors.tests.test_ball_tree.norm->kernel_norm(h, X.shape[1], kernel)
A:sklearn.neighbors.tests.test_ball_tree.dens->BallTree(X, leaf_size=1, metric='haversine').kernel_density(Y, h, atol=atol, rtol=rtol, kernel=kernel, breadth_first=breadth_first)
A:sklearn.neighbors.tests.test_ball_tree.dens_true->compute_kernel_slow(Y, X, kernel, h)
A:sklearn.neighbors.tests.test_ball_tree.x_in->check_random_state(0).normal(0, 1, n_samples)
A:sklearn.neighbors.tests.test_ball_tree.x_out->numpy.linspace(-5, 5, 30)
A:sklearn.neighbors.tests.test_ball_tree.gkde->gaussian_kde(x_in, bw_method=h / np.std(x_in))
A:sklearn.neighbors.tests.test_ball_tree.dens_gkde->gaussian_kde(x_in, bw_method=h / np.std(x_in)).evaluate(x_out)
A:sklearn.neighbors.tests.test_ball_tree.r->numpy.linspace(0, 1, 10)
A:sklearn.neighbors.tests.test_ball_tree.counts->BallTree(X, leaf_size=1, metric='haversine').two_point_correlation(Y, r=r, dualtree=dualtree)
A:sklearn.neighbors.tests.test_ball_tree.bt1->BallTree(X, leaf_size=1)
A:sklearn.neighbors.tests.test_ball_tree.bt1_pyfunc->BallTree(X, metric=dist_func, leaf_size=1, p=2)
A:sklearn.neighbors.tests.test_ball_tree.(ind1, dist1)->BallTree(X, leaf_size=1).query(X)
A:sklearn.neighbors.tests.test_ball_tree.(ind1_pyfunc, dist1_pyfunc)->BallTree(X, metric=dist_func, leaf_size=1, p=2).query(X)
A:sklearn.neighbors.tests.test_ball_tree.s->pickle.dumps(bt1, protocol=protocol)
A:sklearn.neighbors.tests.test_ball_tree.bt2->pickle.loads(s)
A:sklearn.neighbors.tests.test_ball_tree.s_pyfunc->pickle.dumps(bt1_pyfunc, protocol=protocol)
A:sklearn.neighbors.tests.test_ball_tree.bt2_pyfunc->pickle.loads(s_pyfunc)
A:sklearn.neighbors.tests.test_ball_tree.(ind2, dist2)->pickle.loads(s).query(X)
A:sklearn.neighbors.tests.test_ball_tree.(ind2_pyfunc, dist2_pyfunc)->pickle.loads(s_pyfunc).query(X)
A:sklearn.neighbors.tests.test_ball_tree.heap->NeighborsHeap(n_pts, n_nbrs)
A:sklearn.neighbors.tests.test_ball_tree.d_in->check_random_state(0).random_sample(2 * n_nbrs).astype(DTYPE)
A:sklearn.neighbors.tests.test_ball_tree.i_in->numpy.arange(2 * n_nbrs, dtype=ITYPE)
A:sklearn.neighbors.tests.test_ball_tree.ind->(np.arange(n_pts) + np.zeros((n_rows, 1))).astype(ITYPE)
A:sklearn.neighbors.tests.test_ball_tree.(d_heap, i_heap)->NeighborsHeap(n_pts, n_nbrs).get_arrays(sort=True)
A:sklearn.neighbors.tests.test_ball_tree.vals->check_random_state(0).random_sample(n_nodes).astype(DTYPE)
A:sklearn.neighbors.tests.test_ball_tree.i1->numpy.argsort(vals)
A:sklearn.neighbors.tests.test_ball_tree.(vals2, i2)->nodeheap_sort(vals)
A:sklearn.neighbors.tests.test_ball_tree.dist->check_random_state(0).random_sample((n_rows, n_pts)).astype(DTYPE)
A:sklearn.neighbors.tests.test_ball_tree.dist2->check_random_state(0).random_sample((n_rows, n_pts)).astype(DTYPE).copy()
A:sklearn.neighbors.tests.test_ball_tree.ind2->(np.arange(n_pts) + np.zeros((n_rows, 1))).astype(ITYPE).copy()
A:sklearn.neighbors.tests.test_ball_tree.i->numpy.argsort(dist2, axis=1)
sklearn.neighbors.tests.test_ball_tree.brute_force_neighbors(X,Y,k,metric,**kwargs)
sklearn.neighbors.tests.test_ball_tree.check_results(kernel,h,atol,rtol,breadth_first,bt,Y,dens_true)
sklearn.neighbors.tests.test_ball_tree.compute_kernel_slow(Y,X,kernel,h)
sklearn.neighbors.tests.test_ball_tree.dist_func(x1,x2,p)
sklearn.neighbors.tests.test_ball_tree.test_ball_tree_kde(n_samples=100,n_features=3)
sklearn.neighbors.tests.test_ball_tree.test_ball_tree_pickle()
sklearn.neighbors.tests.test_ball_tree.test_ball_tree_query()
sklearn.neighbors.tests.test_ball_tree.test_ball_tree_query_boolean_metrics()
sklearn.neighbors.tests.test_ball_tree.test_ball_tree_query_discrete_metrics()
sklearn.neighbors.tests.test_ball_tree.test_ball_tree_query_radius(n_samples=100,n_features=10)
sklearn.neighbors.tests.test_ball_tree.test_ball_tree_query_radius_distance(n_samples=100,n_features=10)
sklearn.neighbors.tests.test_ball_tree.test_ball_tree_two_point(n_samples=100,n_features=3)
sklearn.neighbors.tests.test_ball_tree.test_gaussian_kde(n_samples=1000)
sklearn.neighbors.tests.test_ball_tree.test_neighbors_heap(n_pts=5,n_nbrs=10)
sklearn.neighbors.tests.test_ball_tree.test_node_heap(n_nodes=50)
sklearn.neighbors.tests.test_ball_tree.test_query_haversine()
sklearn.neighbors.tests.test_ball_tree.test_simultaneous_sort(n_rows=10,n_pts=201)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/neighbors/tests/test_kd_tree.py----------------------------------------
A:sklearn.neighbors.tests.test_kd_tree.rng->check_random_state(0)
A:sklearn.neighbors.tests.test_kd_tree.V->numpy.dot(V, V.T)
A:sklearn.neighbors.tests.test_kd_tree.D->sklearn.neighbors.dist_metrics.DistanceMetric.get_metric('euclidean').pairwise(Y, X)
A:sklearn.neighbors.tests.test_kd_tree.kdt->KDTree(X, leaf_size=10)
A:sklearn.neighbors.tests.test_kd_tree.(dist1, ind1)->KDTree(X, leaf_size=10).query(Y, k, dualtree=dualtree, breadth_first=breadth_first)
A:sklearn.neighbors.tests.test_kd_tree.(dist2, ind2)->brute_force_neighbors(X, Y, k, metric, **kwargs)
A:sklearn.neighbors.tests.test_kd_tree.X->check_random_state(0).random_sample((10, 3))
A:sklearn.neighbors.tests.test_kd_tree.Y->check_random_state(0).random_sample((n_samples, n_features))
A:sklearn.neighbors.tests.test_kd_tree.query_pt->numpy.zeros(n_features, dtype=float)
A:sklearn.neighbors.tests.test_kd_tree.rad->numpy.sqrt(((X - query_pt) ** 2).sum(1))
A:sklearn.neighbors.tests.test_kd_tree.(ind, dist)->KDTree(X, leaf_size=10).query_radius([query_pt], r + eps, return_distance=True)
A:sklearn.neighbors.tests.test_kd_tree.d->numpy.sqrt(((Y[:, None, :] - X) ** 2).sum(-1))
A:sklearn.neighbors.tests.test_kd_tree.norm->kernel_norm(h, X.shape[1], kernel)
A:sklearn.neighbors.tests.test_kd_tree.dens->KDTree(X, leaf_size=10).kernel_density(Y, h, atol=atol, rtol=rtol, kernel=kernel, breadth_first=breadth_first)
A:sklearn.neighbors.tests.test_kd_tree.dens_true->compute_kernel_slow(Y, X, kernel, h)
A:sklearn.neighbors.tests.test_kd_tree.x_in->check_random_state(0).normal(0, 1, n_samples)
A:sklearn.neighbors.tests.test_kd_tree.x_out->numpy.linspace(-5, 5, 30)
A:sklearn.neighbors.tests.test_kd_tree.gkde->gaussian_kde(x_in, bw_method=h / np.std(x_in))
A:sklearn.neighbors.tests.test_kd_tree.dens_gkde->gaussian_kde(x_in, bw_method=h / np.std(x_in)).evaluate(x_out)
A:sklearn.neighbors.tests.test_kd_tree.r->numpy.linspace(0, 1, 10)
A:sklearn.neighbors.tests.test_kd_tree.counts->KDTree(X, leaf_size=10).two_point_correlation(Y, r=r, dualtree=dualtree)
A:sklearn.neighbors.tests.test_kd_tree.kdt1->KDTree(X, leaf_size=1)
A:sklearn.neighbors.tests.test_kd_tree.(ind1, dist1)->KDTree(X, leaf_size=1).query(X)
A:sklearn.neighbors.tests.test_kd_tree.s->pickle.dumps(kdt1, protocol=protocol)
A:sklearn.neighbors.tests.test_kd_tree.kdt2->pickle.loads(s)
A:sklearn.neighbors.tests.test_kd_tree.(ind2, dist2)->pickle.loads(s).query(X)
A:sklearn.neighbors.tests.test_kd_tree.heap->NeighborsHeap(n_pts, n_nbrs)
A:sklearn.neighbors.tests.test_kd_tree.d_in->check_random_state(0).random_sample(2 * n_nbrs).astype(DTYPE)
A:sklearn.neighbors.tests.test_kd_tree.i_in->numpy.arange(2 * n_nbrs, dtype=ITYPE)
A:sklearn.neighbors.tests.test_kd_tree.ind->(np.arange(n_pts) + np.zeros((n_rows, 1))).astype(ITYPE)
A:sklearn.neighbors.tests.test_kd_tree.(d_heap, i_heap)->NeighborsHeap(n_pts, n_nbrs).get_arrays(sort=True)
A:sklearn.neighbors.tests.test_kd_tree.vals->check_random_state(0).random_sample(n_nodes).astype(DTYPE)
A:sklearn.neighbors.tests.test_kd_tree.i1->numpy.argsort(vals)
A:sklearn.neighbors.tests.test_kd_tree.(vals2, i2)->nodeheap_sort(vals)
A:sklearn.neighbors.tests.test_kd_tree.dist->check_random_state(0).random_sample((n_rows, n_pts)).astype(DTYPE)
A:sklearn.neighbors.tests.test_kd_tree.dist2->check_random_state(0).random_sample((n_rows, n_pts)).astype(DTYPE).copy()
A:sklearn.neighbors.tests.test_kd_tree.ind2->(np.arange(n_pts) + np.zeros((n_rows, 1))).astype(ITYPE).copy()
A:sklearn.neighbors.tests.test_kd_tree.i->numpy.argsort(dist2, axis=1)
sklearn.neighbors.tests.test_kd_tree.brute_force_neighbors(X,Y,k,metric,**kwargs)
sklearn.neighbors.tests.test_kd_tree.check_neighbors(dualtree,breadth_first,k,metric,X,Y,kwargs)
sklearn.neighbors.tests.test_kd_tree.check_results(kernel,h,atol,rtol,breadth_first,Y,kdt,dens_true)
sklearn.neighbors.tests.test_kd_tree.compute_kernel_slow(Y,X,kernel,h)
sklearn.neighbors.tests.test_kd_tree.test_gaussian_kde(n_samples=1000)
sklearn.neighbors.tests.test_kd_tree.test_kd_tree_kde(n_samples=100,n_features=3)
sklearn.neighbors.tests.test_kd_tree.test_kd_tree_pickle()
sklearn.neighbors.tests.test_kd_tree.test_kd_tree_query()
sklearn.neighbors.tests.test_kd_tree.test_kd_tree_query_radius(n_samples=100,n_features=10)
sklearn.neighbors.tests.test_kd_tree.test_kd_tree_query_radius_distance(n_samples=100,n_features=10)
sklearn.neighbors.tests.test_kd_tree.test_kd_tree_two_point(n_samples=100,n_features=3)
sklearn.neighbors.tests.test_kd_tree.test_neighbors_heap(n_pts=5,n_nbrs=10)
sklearn.neighbors.tests.test_kd_tree.test_node_heap(n_nodes=50)
sklearn.neighbors.tests.test_kd_tree.test_simultaneous_sort(n_rows=10,n_pts=201)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/neighbors/tests/test_dist_metrics.py----------------------------------------
A:sklearn.neighbors.tests.test_dist_metrics.rng->check_random_state(0)
A:sklearn.neighbors.tests.test_dist_metrics.self.X1->check_random_state(0).random_sample((n1, d)).astype(dtype)
A:sklearn.neighbors.tests.test_dist_metrics.self.X2->check_random_state(0).random_sample((n2, d)).astype(dtype)
A:sklearn.neighbors.tests.test_dist_metrics.self.X1_bool->self.X1.round(0)
A:sklearn.neighbors.tests.test_dist_metrics.self.X2_bool->self.X2.round(0)
A:sklearn.neighbors.tests.test_dist_metrics.V->check_random_state(0).random_sample((d, d))
A:sklearn.neighbors.tests.test_dist_metrics.VI->numpy.dot(V, V.T)
A:sklearn.neighbors.tests.test_dist_metrics.keys->argdict.keys()
A:sklearn.neighbors.tests.test_dist_metrics.kwargs->dict(zip(keys, vals))
A:sklearn.neighbors.tests.test_dist_metrics.D_true->cdist(self.X1_bool, self.X1_bool, metric)
A:sklearn.neighbors.tests.test_dist_metrics.dm->sklearn.neighbors.dist_metrics.DistanceMetric.get_metric(metric, **kwargs)
A:sklearn.neighbors.tests.test_dist_metrics.D12->sklearn.neighbors.dist_metrics.DistanceMetric.get_metric(metric, **kwargs).pairwise(self.X1_bool)
A:sklearn.neighbors.tests.test_dist_metrics.D1->sklearn.neighbors.dist_metrics.DistanceMetric.get_metric('euclidean').pairwise(X)
A:sklearn.neighbors.tests.test_dist_metrics.dm2->pickle.loads(pickle.dumps(dm))
A:sklearn.neighbors.tests.test_dist_metrics.D2->sklearn.neighbors.dist_metrics.DistanceMetric.get_metric('pyfunc', func=dist_func, p=2).pairwise(X)
A:sklearn.neighbors.tests.test_dist_metrics.X->check_random_state(0).rand(10, 3)
A:sklearn.neighbors.tests.test_dist_metrics.haversine->sklearn.neighbors.dist_metrics.DistanceMetric.get_metric('haversine')
A:sklearn.neighbors.tests.test_dist_metrics.D2[i, j]->haversine_slow(x1, x2)
A:sklearn.neighbors.tests.test_dist_metrics.euclidean->sklearn.neighbors.dist_metrics.DistanceMetric.get_metric('euclidean')
A:sklearn.neighbors.tests.test_dist_metrics.pyfunc->sklearn.neighbors.dist_metrics.DistanceMetric.get_metric('pyfunc', func=dist_func, p=2)
A:sklearn.neighbors.tests.test_dist_metrics.euclidean_pkl->pickle.loads(pickle.dumps(euclidean))
A:sklearn.neighbors.tests.test_dist_metrics.pyfunc_pkl->pickle.loads(pickle.dumps(pyfunc))
A:sklearn.neighbors.tests.test_dist_metrics.D1_pkl->pickle.loads(pickle.dumps(euclidean)).pairwise(X)
A:sklearn.neighbors.tests.test_dist_metrics.D2_pkl->pickle.loads(pickle.dumps(pyfunc)).pairwise(X)
A:sklearn.neighbors.tests.test_dist_metrics.eucl->sklearn.neighbors.dist_metrics.DistanceMetric.get_metric('euclidean')
sklearn.neighbors.tests.test_dist_metrics.TestMetrics(self,n1=20,n2=25,d=4,zero_frac=0.5,rseed=0,dtype=np.float64)
sklearn.neighbors.tests.test_dist_metrics.TestMetrics.__init__(self,n1=20,n2=25,d=4,zero_frac=0.5,rseed=0,dtype=np.float64)
sklearn.neighbors.tests.test_dist_metrics.TestMetrics.check_cdist(self,metric,kwargs,D_true)
sklearn.neighbors.tests.test_dist_metrics.TestMetrics.check_cdist_bool(self,metric,D_true)
sklearn.neighbors.tests.test_dist_metrics.TestMetrics.check_pdist(self,metric,kwargs,D_true)
sklearn.neighbors.tests.test_dist_metrics.TestMetrics.check_pdist_bool(self,metric,D_true)
sklearn.neighbors.tests.test_dist_metrics.TestMetrics.check_pickle(self,metric,kwargs)
sklearn.neighbors.tests.test_dist_metrics.TestMetrics.check_pickle_bool(self,metric)
sklearn.neighbors.tests.test_dist_metrics.TestMetrics.test_cdist(self)
sklearn.neighbors.tests.test_dist_metrics.TestMetrics.test_pdist(self)
sklearn.neighbors.tests.test_dist_metrics.TestMetrics.test_pickle(self)
sklearn.neighbors.tests.test_dist_metrics.dist_func(x1,x2,p)
sklearn.neighbors.tests.test_dist_metrics.test_bad_pyfunc_metric()
sklearn.neighbors.tests.test_dist_metrics.test_haversine_metric()
sklearn.neighbors.tests.test_dist_metrics.test_input_data_size()
sklearn.neighbors.tests.test_dist_metrics.test_pyfunc_metric()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/neighbors/tests/test_nearest_centroid.py----------------------------------------
A:sklearn.neighbors.tests.test_nearest_centroid.X_csr->scipy.sparse.csr_matrix(X)
A:sklearn.neighbors.tests.test_nearest_centroid.T_csr->scipy.sparse.csr_matrix(T)
A:sklearn.neighbors.tests.test_nearest_centroid.iris->sklearn.datasets.load_iris()
A:sklearn.neighbors.tests.test_nearest_centroid.rng->numpy.random.RandomState(0)
A:sklearn.neighbors.tests.test_nearest_centroid.perm->numpy.random.RandomState(0).permutation(iris.target.size)
A:sklearn.neighbors.tests.test_nearest_centroid.clf->NearestCentroid(metric='manhattan')
A:sklearn.neighbors.tests.test_nearest_centroid.score->NearestCentroid().score(iris.data, iris.target)
A:sklearn.neighbors.tests.test_nearest_centroid.obj->NearestCentroid()
A:sklearn.neighbors.tests.test_nearest_centroid.s->pickle.dumps(obj)
A:sklearn.neighbors.tests.test_nearest_centroid.obj2->pickle.loads(s)
A:sklearn.neighbors.tests.test_nearest_centroid.score2->pickle.loads(s).score(iris.data, iris.target)
A:sklearn.neighbors.tests.test_nearest_centroid.X->numpy.random.RandomState(0).rand(50, 50)
A:sklearn.neighbors.tests.test_nearest_centroid.y->numpy.random.RandomState(0).randint(0, 3, 50)
A:sklearn.neighbors.tests.test_nearest_centroid.expected_result->numpy.array([[0.778731, 0.8545292], [2.814179, 2.763647]])
A:sklearn.neighbors.tests.test_nearest_centroid.y_ind->numpy.asarray(y)
A:sklearn.neighbors.tests.test_nearest_centroid.noise->numpy.random.RandomState(0).rand(50)
A:sklearn.neighbors.tests.test_nearest_centroid.y_init->NearestCentroid(metric='manhattan').predict(X)
A:sklearn.neighbors.tests.test_nearest_centroid.y_translate->NearestCentroid(metric='manhattan').predict(X_noise)
sklearn.neighbors.tests.test_nearest_centroid.test_classification_toy()
sklearn.neighbors.tests.test_nearest_centroid.test_iris()
sklearn.neighbors.tests.test_nearest_centroid.test_iris_shrinkage()
sklearn.neighbors.tests.test_nearest_centroid.test_manhattan_metric()
sklearn.neighbors.tests.test_nearest_centroid.test_pickle()
sklearn.neighbors.tests.test_nearest_centroid.test_precomputed()
sklearn.neighbors.tests.test_nearest_centroid.test_predict_translated_data()
sklearn.neighbors.tests.test_nearest_centroid.test_shrinkage_correct()
sklearn.neighbors.tests.test_nearest_centroid.test_shrinkage_threshold_decoded_y()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/neighbors/tests/test_neighbors.py----------------------------------------
A:sklearn.neighbors.tests.test_neighbors.rng->numpy.random.RandomState(0)
A:sklearn.neighbors.tests.test_neighbors.iris->sklearn.datasets.load_iris()
A:sklearn.neighbors.tests.test_neighbors.perm->numpy.random.RandomState(0).permutation(digits.target.size)
A:sklearn.neighbors.tests.test_neighbors.digits->sklearn.datasets.load_digits()
A:sklearn.neighbors.tests.test_neighbors.neighbors.kneighbors_graph->ignore_warnings(neighbors.kneighbors_graph)
A:sklearn.neighbors.tests.test_neighbors.neighbors.radius_neighbors_graph->ignore_warnings(neighbors.radius_neighbors_graph)
A:sklearn.neighbors.tests.test_neighbors.X->numpy.random.RandomState(0).uniform(size=(6, 5))
A:sklearn.neighbors.tests.test_neighbors.test->numpy.random.RandomState(0).rand(n_query_pts, n_features)
A:sklearn.neighbors.tests.test_neighbors.neigh->sklearn.neighbors.NearestNeighbors(n_neighbors=n_neighbors, algorithm=algorithm, metric=metric, p=p, metric_params=metric_params)
A:sklearn.neighbors.tests.test_neighbors.nbrs_fid->sklearn.neighbors.NearestNeighbors(n_neighbors=1)
A:sklearn.neighbors.tests.test_neighbors.(dist1, ind1)->sklearn.neighbors.NearestNeighbors(metric=metric, radius=radius).fit(X).kneighbors(X)
A:sklearn.neighbors.tests.test_neighbors.nbrs->sklearn.neighbors.NearestNeighbors().fit(X)
A:sklearn.neighbors.tests.test_neighbors.(dist2, ind2)->sklearn.neighbors.KNeighborsRegressor(1, algorithm='ball_tree').kneighbors(X)
A:sklearn.neighbors.tests.test_neighbors.Y->numpy.random.RandomState(0).random_sample((3, 4))
A:sklearn.neighbors.tests.test_neighbors.DXX->sklearn.metrics.pairwise_distances(X_precomputed, metric='euclidean')
A:sklearn.neighbors.tests.test_neighbors.DYX->sklearn.metrics.pairwise_distances(Y_precomputed, X_precomputed, metric='euclidean')
A:sklearn.neighbors.tests.test_neighbors.nbrs_X->sklearn.neighbors.NearestNeighbors(n_neighbors=3)
A:sklearn.neighbors.tests.test_neighbors.(dist_X, ind_X)->getattr(nbrs_X, method)(None)
A:sklearn.neighbors.tests.test_neighbors.nbrs_D->sklearn.neighbors.NearestNeighbors(n_neighbors=3, algorithm='auto', metric='precomputed')
A:sklearn.neighbors.tests.test_neighbors.(dist_D, ind_D)->getattr(nbrs_D, method)(None)
A:sklearn.neighbors.tests.test_neighbors.target->numpy.arange(X.shape[0])
A:sklearn.neighbors.tests.test_neighbors.est->Est(metric='euclidean')
A:sklearn.neighbors.tests.test_neighbors.pred_X->Est(metric='euclidean').fit(X, target).predict(Y)
A:sklearn.neighbors.tests.test_neighbors.pred_D->Est(metric='euclidean').fit(DXX, target).predict(DYX)
A:sklearn.neighbors.tests.test_neighbors.D->pairwise_distances(X, metric='euclidean')
A:sklearn.neighbors.tests.test_neighbors.y->sklearn.neighbors.KNeighborsClassifier(n_neighbors=3, algorithm=algorithm).predict(X_test)
A:sklearn.neighbors.tests.test_neighbors.metric_score->cross_val_score(Est(), X, y)
A:sklearn.neighbors.tests.test_neighbors.precomp_score->cross_val_score(Est(metric='precomputed'), D, y)
A:sklearn.neighbors.tests.test_neighbors.ind1->sklearn.neighbors.NearestNeighbors(n_neighbors=n_neighbors, algorithm=algorithm, metric=metric, p=p, metric_params=metric_params).radius_neighbors(test, return_distance=False)
A:sklearn.neighbors.tests.test_neighbors.(dist, ind)->sklearn.neighbors.KNeighborsClassifier(n_neighbors=3, algorithm=algorithm).kneighbors(X_test)
A:sklearn.neighbors.tests.test_neighbors.j->d.argsort()
A:sklearn.neighbors.tests.test_neighbors.y_str->sklearn.neighbors.KNeighborsClassifier(n_neighbors=3, algorithm=algorithm).predict(X_test).astype(str)
A:sklearn.neighbors.tests.test_neighbors.knn->sklearn.neighbors.KNeighborsRegressor(n_neighbors=n_neighbors, algorithm='auto')
A:sklearn.neighbors.tests.test_neighbors.y_pred->sklearn.neighbors.RadiusNeighborsRegressor(n_neighbors=n_neighbors, weights=weights, algorithm=algorithm).predict(X[:n_test_pts] + epsilon)
A:sklearn.neighbors.tests.test_neighbors.cls->sklearn.neighbors.KNeighborsClassifier(n_neighbors=2, p=1, weights='distance')
A:sklearn.neighbors.tests.test_neighbors.y_prob->sklearn.neighbors.KNeighborsClassifier(n_neighbors=2, p=1, weights='distance').predict_proba(np.array([[0, 2, 0], [2, 2, 2]]))
A:sklearn.neighbors.tests.test_neighbors.real_prob->numpy.array([[0, 1, 0], [0, 0.4, 0.6]])
A:sklearn.neighbors.tests.test_neighbors.z1->numpy.array([[1.01, 1.01], [2.0, 2.0]])
A:sklearn.neighbors.tests.test_neighbors.z2->numpy.array([[1.4, 1.4], [1.01, 1.01], [2.01, 2.01]])
A:sklearn.neighbors.tests.test_neighbors.clf->sklearn.neighbors.KNeighborsClassifier(n_neighbors=3, algorithm=algorithm)
A:sklearn.neighbors.tests.test_neighbors.correct_labels1->numpy.array([1, 2])
A:sklearn.neighbors.tests.test_neighbors.correct_labels2->numpy.array([-1, 1, 2])
A:sklearn.neighbors.tests.test_neighbors.z->numpy.array([[1.1, 1.1], [2.0, 2.0]])
A:sklearn.neighbors.tests.test_neighbors.rnn_correct_labels->numpy.array([1.25, 2.0])
A:sklearn.neighbors.tests.test_neighbors.knn_correct_unif->numpy.array([1.25, 1.0])
A:sklearn.neighbors.tests.test_neighbors.knn_correct_dist->numpy.array([1.25, 2.0])
A:sklearn.neighbors.tests.test_neighbors.rnn->sklearn.neighbors.RadiusNeighborsRegressor(n_neighbors=n_neighbors, weights=weights, algorithm=algorithm)
A:sklearn.neighbors.tests.test_neighbors.results->sklearn.neighbors.NearestNeighbors().fit(X).radius_neighbors([[0.0]], return_distance=False)
A:sklearn.neighbors.tests.test_neighbors.(X_train, X_test, y_train, y_test)->train_test_split(X, y)
A:sklearn.neighbors.tests.test_neighbors.rnn_mo->sklearn.neighbors.RadiusNeighborsClassifier(weights=weights, algorithm=algorithm)
A:sklearn.neighbors.tests.test_neighbors.y_pred_mo->sklearn.neighbors.KNeighborsClassifier(weights=weights, algorithm=algorithm).predict(X_test)
A:sklearn.neighbors.tests.test_neighbors.X_eps->sparsev(X[:n_test_pts] + epsilon)
A:sklearn.neighbors.tests.test_neighbors.knn_mo->sklearn.neighbors.KNeighborsClassifier(weights=weights, algorithm=algorithm)
A:sklearn.neighbors.tests.test_neighbors.y_pred_proba_mo->sklearn.neighbors.KNeighborsClassifier(weights=weights, algorithm=algorithm).predict_proba(X_test)
A:sklearn.neighbors.tests.test_neighbors.neigh_idx->sklearn.neighbors.RadiusNeighborsRegressor(n_neighbors=n_neighbors, weights=weights, algorithm=algorithm).radius_neighbors(X_test, return_distance=False)
A:sklearn.neighbors.tests.test_neighbors.y_pred_idx->numpy.array(y_pred_idx)
A:sklearn.neighbors.tests.test_neighbors.X2->sparsev(X)
A:sklearn.neighbors.tests.test_neighbors.rgs->sklearn.neighbors.KNeighborsRegressor(n_neighbors=5, algorithm=algorithm)
A:sklearn.neighbors.tests.test_neighbors.train_test_boundary->int(n_samples * 0.8)
A:sklearn.neighbors.tests.test_neighbors.train->numpy.arange(0, train_test_boundary)
A:sklearn.neighbors.tests.test_neighbors.score_uint8->sklearn.neighbors.KNeighborsClassifier(n_neighbors=3, algorithm=algorithm).fit(X_train, Y_train).score(X_test, Y_test)
A:sklearn.neighbors.tests.test_neighbors.score_float->sklearn.neighbors.KNeighborsClassifier(n_neighbors=3, algorithm=algorithm).fit(X_train.astype(float), Y_train).score(X_test.astype(float), Y_test)
A:sklearn.neighbors.tests.test_neighbors.A->sklearn.neighbors.radius_neighbors_graph(X, 1.5, mode='distance')
A:sklearn.neighbors.tests.test_neighbors.Xcsr->csr_matrix(X)
A:sklearn.neighbors.tests.test_neighbors.Xsparse->csr_matrix(X)
A:sklearn.neighbors.tests.test_neighbors.V->numpy.random.RandomState(0).rand(n_features, n_features)
A:sklearn.neighbors.tests.test_neighbors.VI->numpy.dot(X, X.T)
A:sklearn.neighbors.tests.test_neighbors.p->metric_params.pop('p', 2)
A:sklearn.neighbors.tests.test_neighbors.results[algorithm]->sklearn.neighbors.NearestNeighbors(n_neighbors=n_neighbors, algorithm=algorithm, metric=metric, p=p, metric_params=metric_params).kneighbors(test, return_distance=True)
A:sklearn.neighbors.tests.test_neighbors.nbrs1->sklearn.neighbors.NearestNeighbors(metric=metric, radius=radius).fit(X)
A:sklearn.neighbors.tests.test_neighbors.nbrs2->sklearn.neighbors.KNeighborsRegressor(1, algorithm='ball_tree')
A:sklearn.neighbors.tests.test_neighbors.nn->sklearn.neighbors.NearestNeighbors(n_neighbors=1)
A:sklearn.neighbors.tests.test_neighbors.X_precomputed->numpy.random.RandomState(0).random_sample((10, 4))
A:sklearn.neighbors.tests.test_neighbors.Y_precomputed->numpy.random.RandomState(0).random_sample((3, 4))
A:sklearn.neighbors.tests.test_neighbors.nb_p->sklearn.neighbors.NearestNeighbors(n_neighbors=3)
A:sklearn.neighbors.tests.test_neighbors.dist_array->pairwise_distances(X).flatten()
A:sklearn.neighbors.tests.test_neighbors.nbrs_graph->sklearn.neighbors.radius_neighbors_graph(X, radius, metric=metric, mode='connectivity', include_self=True).toarray()
A:sklearn.neighbors.tests.test_neighbors.X_nbrs->sklearn.neighbors.NearestNeighbors(radius=radius, metric='manhattan')
A:sklearn.neighbors.tests.test_neighbors.kng->sklearn.neighbors.NearestNeighbors(n_neighbors=1).kneighbors_graph(mode='distance')
A:sklearn.neighbors.tests.test_neighbors.(X, y)->sklearn.datasets.make_classification(n_samples=30, n_features=5, n_redundant=0, random_state=0)
A:sklearn.neighbors.tests.test_neighbors.graph->sklearn.neighbors.KNeighborsClassifier(n_neighbors=3, algorithm=algorithm).kneighbors_graph(X_test, mode='distance').toarray()
A:sklearn.neighbors.tests.test_neighbors.y_parallel->sklearn.neighbors.KNeighborsClassifier(n_neighbors=3, algorithm=algorithm).predict(X_test)
A:sklearn.neighbors.tests.test_neighbors.(dist_parallel, ind_parallel)->sklearn.neighbors.KNeighborsClassifier(n_neighbors=3, algorithm=algorithm).kneighbors(X_test)
A:sklearn.neighbors.tests.test_neighbors.graph_parallel->sklearn.neighbors.KNeighborsClassifier(n_neighbors=3, algorithm=algorithm).kneighbors_graph(X_test, mode='distance').toarray()
A:sklearn.neighbors.tests.test_neighbors.classifier->sklearn.neighbors.KNeighborsClassifier(n_neighbors=1)
A:sklearn.neighbors.tests.test_neighbors.result->sklearn.neighbors.KNeighborsClassifier(n_neighbors=1).fit(X, y).predict(X)
A:sklearn.neighbors.tests.test_neighbors.nn1->NN(metric='jaccard', algorithm='brute').fit(X)
A:sklearn.neighbors.tests.test_neighbors.nn2->NN(metric='jaccard', algorithm='ball_tree').fit(X)
sklearn.neighbors.tests.test_neighbors._weight_func(dist)
sklearn.neighbors.tests.test_neighbors.check_object_arrays(nparray,list_check)
sklearn.neighbors.tests.test_neighbors.test_KNeighborsClassifier_multioutput()
sklearn.neighbors.tests.test_neighbors.test_KNeighborsRegressor_multioutput_uniform_weight()
sklearn.neighbors.tests.test_neighbors.test_RadiusNeighborsClassifier_multioutput()
sklearn.neighbors.tests.test_neighbors.test_RadiusNeighborsRegressor_multioutput(n_samples=40,n_features=5,n_test_pts=10,n_neighbors=3,random_state=0)
sklearn.neighbors.tests.test_neighbors.test_RadiusNeighborsRegressor_multioutput_with_uniform_weight()
sklearn.neighbors.tests.test_neighbors.test_callable_metric()
sklearn.neighbors.tests.test_neighbors.test_dtype_convert()
sklearn.neighbors.tests.test_neighbors.test_include_self_neighbors_graph()
sklearn.neighbors.tests.test_neighbors.test_k_and_radius_neighbors_X_None()
sklearn.neighbors.tests.test_neighbors.test_k_and_radius_neighbors_duplicates()
sklearn.neighbors.tests.test_neighbors.test_k_and_radius_neighbors_train_is_not_query()
sklearn.neighbors.tests.test_neighbors.test_kneighbors_classifier(n_samples=40,n_features=5,n_test_pts=10,n_neighbors=5,random_state=0)
sklearn.neighbors.tests.test_neighbors.test_kneighbors_classifier_float_labels(n_samples=40,n_features=5,n_test_pts=10,n_neighbors=5,random_state=0)
sklearn.neighbors.tests.test_neighbors.test_kneighbors_classifier_predict_proba()
sklearn.neighbors.tests.test_neighbors.test_kneighbors_classifier_sparse(n_samples=40,n_features=5,n_test_pts=10,n_neighbors=5,random_state=0)
sklearn.neighbors.tests.test_neighbors.test_kneighbors_graph()
sklearn.neighbors.tests.test_neighbors.test_kneighbors_graph_sparse(seed=36)
sklearn.neighbors.tests.test_neighbors.test_kneighbors_regressor(n_samples=40,n_features=5,n_test_pts=10,n_neighbors=3,random_state=0)
sklearn.neighbors.tests.test_neighbors.test_kneighbors_regressor_multioutput(n_samples=40,n_features=5,n_test_pts=10,n_neighbors=3,random_state=0)
sklearn.neighbors.tests.test_neighbors.test_kneighbors_regressor_sparse(n_samples=40,n_features=5,n_test_pts=10,n_neighbors=5,random_state=0)
sklearn.neighbors.tests.test_neighbors.test_metric_params_interface()
sklearn.neighbors.tests.test_neighbors.test_neighbors_badargs()
sklearn.neighbors.tests.test_neighbors.test_neighbors_digits()
sklearn.neighbors.tests.test_neighbors.test_neighbors_iris()
sklearn.neighbors.tests.test_neighbors.test_neighbors_metrics(n_samples=20,n_features=3,n_query_pts=2,n_neighbors=5)
sklearn.neighbors.tests.test_neighbors.test_neighbors_regressors_zero_distance()
sklearn.neighbors.tests.test_neighbors.test_non_euclidean_kneighbors()
sklearn.neighbors.tests.test_neighbors.test_pairwise_boolean_distance()
sklearn.neighbors.tests.test_neighbors.test_precomputed(random_state=42)
sklearn.neighbors.tests.test_neighbors.test_precomputed_cross_validation()
sklearn.neighbors.tests.test_neighbors.test_predict_sparse_ball_kd_tree()
sklearn.neighbors.tests.test_neighbors.test_radius_neighbors_boundary_handling()
sklearn.neighbors.tests.test_neighbors.test_radius_neighbors_classifier(n_samples=40,n_features=5,n_test_pts=10,radius=0.5,random_state=0)
sklearn.neighbors.tests.test_neighbors.test_radius_neighbors_classifier_outlier_labeling()
sklearn.neighbors.tests.test_neighbors.test_radius_neighbors_classifier_when_no_neighbors()
sklearn.neighbors.tests.test_neighbors.test_radius_neighbors_classifier_zero_distance()
sklearn.neighbors.tests.test_neighbors.test_radius_neighbors_graph()
sklearn.neighbors.tests.test_neighbors.test_radius_neighbors_graph_sparse(seed=36)
sklearn.neighbors.tests.test_neighbors.test_radius_neighbors_regressor(n_samples=40,n_features=3,n_test_pts=10,radius=0.5,random_state=0)
sklearn.neighbors.tests.test_neighbors.test_same_knn_parallel()
sklearn.neighbors.tests.test_neighbors.test_unsupervised_inputs()
sklearn.neighbors.tests.test_neighbors.test_unsupervised_kneighbors(n_samples=20,n_features=5,n_query_pts=2,n_neighbors=5)
sklearn.neighbors.tests.test_neighbors.test_unsupervised_radius_neighbors(n_samples=20,n_features=5,n_query_pts=2,radius=0.5,random_state=0)
sklearn.neighbors.tests.test_neighbors.test_valid_brute_metric_for_auto_algorithm()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/neighbors/tests/test_approximate.py----------------------------------------
A:sklearn.neighbors.tests.test_approximate.n_candidates_values->numpy.array([0.1, 50, 500])
A:sklearn.neighbors.tests.test_approximate.rng->numpy.random.RandomState(42)
A:sklearn.neighbors.tests.test_approximate.accuracies->numpy.zeros(n_estimators.shape[0], dtype=float)
A:sklearn.neighbors.tests.test_approximate.X->numpy.random.RandomState(42).rand(n_samples, n_features)
A:sklearn.neighbors.tests.test_approximate.lshf->ignore_warnings(LSHForest, category=DeprecationWarning)(min_hash_match=0)
A:sklearn.neighbors.tests.test_approximate.query->X[rng.randint(0, n_samples)].reshape(1, -1)
A:sklearn.neighbors.tests.test_approximate.neighbors->ignore_warnings(LSHForest, category=DeprecationWarning)(min_hash_match=0).radius_neighbors(query, radius=mean_dist, return_distance=False)
A:sklearn.neighbors.tests.test_approximate.distances->pairwise_distances(query, X, metric='cosine')
A:sklearn.neighbors.tests.test_approximate.n_estimators->numpy.array([1, 10, 100])
A:sklearn.neighbors.tests.test_approximate.n_neighbors->numpy.random.RandomState(42).randint(0, n_samples)
A:sklearn.neighbors.tests.test_approximate.(distances, neighbors)->ignore_warnings(LSHForest, category=DeprecationWarning)(min_hash_match=0).kneighbors(X_test, n_neighbors=5)
A:sklearn.neighbors.tests.test_approximate.mean_dist->numpy.mean(pairwise_distances(query, X, metric='cosine'))
A:sklearn.neighbors.tests.test_approximate.nbrs->NearestNeighbors(algorithm='brute', metric='cosine').fit(X)
A:sklearn.neighbors.tests.test_approximate.(distances_exact, _)->NearestNeighbors(algorithm='brute', metric='cosine').fit(X).radius_neighbors(query, radius=mean_dist)
A:sklearn.neighbors.tests.test_approximate.(distances_approx, _)->ignore_warnings(LSHForest, category=DeprecationWarning)(min_hash_match=0).radius_neighbors(query, radius=mean_dist)
A:sklearn.neighbors.tests.test_approximate.sorted_dists_exact->numpy.sort(distances_exact[0])
A:sklearn.neighbors.tests.test_approximate.sorted_dists_approx->numpy.sort(distances_approx[0])
A:sklearn.neighbors.tests.test_approximate.n_points->len(X)
A:sklearn.neighbors.tests.test_approximate.nnbrs->NearestNeighbors(algorithm='brute', metric='cosine').fit(X)
A:sklearn.neighbors.tests.test_approximate.lsfh->ignore_warnings(LSHForest, category=DeprecationWarning)(min_hash_match=0, n_candidates=n_points, random_state=42).fit(X)
A:sklearn.neighbors.tests.test_approximate.dists->pairwise_distances(query, X, metric='cosine').ravel()
A:sklearn.neighbors.tests.test_approximate.(exact_dists, exact_idx)->NearestNeighbors(algorithm='brute', metric='cosine').fit(X).radius_neighbors(query, radius=1 - eps)
A:sklearn.neighbors.tests.test_approximate.(approx_dists, approx_idx)->ignore_warnings(LSHForest, category=DeprecationWarning)(min_hash_match=0, n_candidates=n_points, random_state=42).fit(X).radius_neighbors(query, radius=1 - eps)
A:sklearn.neighbors.tests.test_approximate.X_partial_fit->numpy.random.RandomState(42).rand(n_samples_partial_fit, n_features)
A:sklearn.neighbors.tests.test_approximate.X_train->numpy.array([[5, 5, 2], [21, 5, 5], [1, 1, 1], [8, 9, 1], [6, 10, 2]], dtype=np.float32)
A:sklearn.neighbors.tests.test_approximate.X_test->numpy.array([7, 10, 3], dtype=np.float32).reshape(1, -1)
A:sklearn.neighbors.tests.test_approximate.kneighbors_graph->ignore_warnings(LSHForest, category=DeprecationWarning)(min_hash_match=0).kneighbors_graph(X)
A:sklearn.neighbors.tests.test_approximate.radius_neighbors_graph->ignore_warnings(LSHForest, category=DeprecationWarning)(min_hash_match=0).radius_neighbors_graph(X)
A:sklearn.neighbors.tests.test_approximate.X1->scipy.sparse.rand(50, 100)
A:sklearn.neighbors.tests.test_approximate.X2->scipy.sparse.rand(10, 100)
A:sklearn.neighbors.tests.test_approximate.forest_sparse->ignore_warnings(LSHForest, category=DeprecationWarning)(radius=1, random_state=0).fit(X1)
A:sklearn.neighbors.tests.test_approximate.forest_dense->ignore_warnings(LSHForest, category=DeprecationWarning)(radius=1, random_state=0).fit(X1.A)
A:sklearn.neighbors.tests.test_approximate.(d_sparse, i_sparse)->ignore_warnings(LSHForest, category=DeprecationWarning)(radius=1, random_state=0).fit(X1).radius_neighbors(X2, return_distance=True)
A:sklearn.neighbors.tests.test_approximate.(d_dense, i_dense)->ignore_warnings(LSHForest, category=DeprecationWarning)(radius=1, random_state=0).fit(X1.A).radius_neighbors(X2.A, return_distance=True)
sklearn.neighbors.tests.test_approximate.test_candidates()
sklearn.neighbors.tests.test_approximate.test_distances()
sklearn.neighbors.tests.test_approximate.test_fit()
sklearn.neighbors.tests.test_approximate.test_graphs()
sklearn.neighbors.tests.test_approximate.test_hash_functions()
sklearn.neighbors.tests.test_approximate.test_kneighbors()
sklearn.neighbors.tests.test_approximate.test_lsh_forest_deprecation()
sklearn.neighbors.tests.test_approximate.test_neighbors_accuracy_with_n_candidates()
sklearn.neighbors.tests.test_approximate.test_neighbors_accuracy_with_n_estimators()
sklearn.neighbors.tests.test_approximate.test_partial_fit()
sklearn.neighbors.tests.test_approximate.test_radius_neighbors()
sklearn.neighbors.tests.test_approximate.test_radius_neighbors_boundary_handling()
sklearn.neighbors.tests.test_approximate.test_sparse_input()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/mixture/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/mixture/gaussian_mixture.py----------------------------------------
A:sklearn.mixture.gaussian_mixture.weights->check_array(weights, dtype=[np.float64, np.float32], ensure_2d=False)
A:sklearn.mixture.gaussian_mixture.means->check_array(means, dtype=[np.float64, np.float32], ensure_2d=False)
A:sklearn.mixture.gaussian_mixture.precisions->check_array(precisions, dtype=[np.float64, np.float32], ensure_2d=False, allow_nd=covariance_type == 'full')
A:sklearn.mixture.gaussian_mixture.covariances->{'full': _estimate_gaussian_covariances_full, 'tied': _estimate_gaussian_covariances_tied, 'diag': _estimate_gaussian_covariances_diag, 'spherical': _estimate_gaussian_covariances_spherical}[covariance_type](resp, X, nk, means, reg_covar)
A:sklearn.mixture.gaussian_mixture.avg_X2->numpy.dot(X.T, X)
A:sklearn.mixture.gaussian_mixture.avg_means2->numpy.dot(nk * means.T, means)
A:sklearn.mixture.gaussian_mixture.precisions_chol->numpy.empty((n_components, n_features, n_features))
A:sklearn.mixture.gaussian_mixture.cov_chol->scipy.linalg.cholesky(covariances, lower=True)
A:sklearn.mixture.gaussian_mixture.log_det_chol->numpy.sum(np.log(matrix_chol), axis=1)
A:sklearn.mixture.gaussian_mixture.log_det->_compute_log_det_cholesky(precisions_chol, covariance_type, n_features)
A:sklearn.mixture.gaussian_mixture.log_prob->numpy.empty((n_samples, n_components))
A:sklearn.mixture.gaussian_mixture.log_prob[:, k]->numpy.sum(np.square(y), axis=1)
A:sklearn.mixture.gaussian_mixture.self.weights_init->_check_weights(self.weights_init, self.n_components)
A:sklearn.mixture.gaussian_mixture.self.means_init->_check_means(self.means_init, self.n_components, n_features)
A:sklearn.mixture.gaussian_mixture.self.precisions_init->_check_precisions(self.precisions_init, self.covariance_type, self.n_components, n_features)
A:sklearn.mixture.gaussian_mixture.(weights, means, covariances)->_estimate_gaussian_parameters(X, resp, self.reg_covar, self.covariance_type)
A:sklearn.mixture.gaussian_mixture.self.precisions_cholesky_->_compute_precision_cholesky(self.covariances_, self.covariance_type)
A:sklearn.mixture.gaussian_mixture.(self.weights_, self.means_, self.covariances_)->_estimate_gaussian_parameters(X, np.exp(log_resp), self.reg_covar, self.covariance_type)
A:sklearn.mixture.gaussian_mixture.self.precisions_->numpy.dot(self.precisions_cholesky_, self.precisions_cholesky_.T)
A:sklearn.mixture.gaussian_mixture.self.precisions_[k]->numpy.dot(prec_chol, prec_chol.T)
sklearn.mixture.GaussianMixture(self,n_components=1,covariance_type='full',tol=0.001,reg_covar=1e-06,max_iter=100,n_init=1,init_params='kmeans',weights_init=None,means_init=None,precisions_init=None,random_state=None,warm_start=False,verbose=0,verbose_interval=10)
sklearn.mixture.GaussianMixture._check_is_fitted(self)
sklearn.mixture.GaussianMixture._check_parameters(self,X)
sklearn.mixture.GaussianMixture._compute_lower_bound(self,_,log_prob_norm)
sklearn.mixture.GaussianMixture._estimate_log_prob(self,X)
sklearn.mixture.GaussianMixture._estimate_log_weights(self)
sklearn.mixture.GaussianMixture._get_parameters(self)
sklearn.mixture.GaussianMixture._initialize(self,X,resp)
sklearn.mixture.GaussianMixture._m_step(self,X,log_resp)
sklearn.mixture.GaussianMixture._n_parameters(self)
sklearn.mixture.GaussianMixture._set_parameters(self,params)
sklearn.mixture.GaussianMixture.aic(self,X)
sklearn.mixture.GaussianMixture.bic(self,X)
sklearn.mixture.gaussian_mixture.GaussianMixture(self,n_components=1,covariance_type='full',tol=0.001,reg_covar=1e-06,max_iter=100,n_init=1,init_params='kmeans',weights_init=None,means_init=None,precisions_init=None,random_state=None,warm_start=False,verbose=0,verbose_interval=10)
sklearn.mixture.gaussian_mixture.GaussianMixture.__init__(self,n_components=1,covariance_type='full',tol=0.001,reg_covar=1e-06,max_iter=100,n_init=1,init_params='kmeans',weights_init=None,means_init=None,precisions_init=None,random_state=None,warm_start=False,verbose=0,verbose_interval=10)
sklearn.mixture.gaussian_mixture.GaussianMixture._check_is_fitted(self)
sklearn.mixture.gaussian_mixture.GaussianMixture._check_parameters(self,X)
sklearn.mixture.gaussian_mixture.GaussianMixture._compute_lower_bound(self,_,log_prob_norm)
sklearn.mixture.gaussian_mixture.GaussianMixture._estimate_log_prob(self,X)
sklearn.mixture.gaussian_mixture.GaussianMixture._estimate_log_weights(self)
sklearn.mixture.gaussian_mixture.GaussianMixture._get_parameters(self)
sklearn.mixture.gaussian_mixture.GaussianMixture._initialize(self,X,resp)
sklearn.mixture.gaussian_mixture.GaussianMixture._m_step(self,X,log_resp)
sklearn.mixture.gaussian_mixture.GaussianMixture._n_parameters(self)
sklearn.mixture.gaussian_mixture.GaussianMixture._set_parameters(self,params)
sklearn.mixture.gaussian_mixture.GaussianMixture.aic(self,X)
sklearn.mixture.gaussian_mixture.GaussianMixture.bic(self,X)
sklearn.mixture.gaussian_mixture._check_means(means,n_components,n_features)
sklearn.mixture.gaussian_mixture._check_precision_matrix(precision,covariance_type)
sklearn.mixture.gaussian_mixture._check_precision_positivity(precision,covariance_type)
sklearn.mixture.gaussian_mixture._check_precisions(precisions,covariance_type,n_components,n_features)
sklearn.mixture.gaussian_mixture._check_precisions_full(precisions,covariance_type)
sklearn.mixture.gaussian_mixture._check_weights(weights,n_components)
sklearn.mixture.gaussian_mixture._compute_log_det_cholesky(matrix_chol,covariance_type,n_features)
sklearn.mixture.gaussian_mixture._compute_precision_cholesky(covariances,covariance_type)
sklearn.mixture.gaussian_mixture._estimate_gaussian_covariances_diag(resp,X,nk,means,reg_covar)
sklearn.mixture.gaussian_mixture._estimate_gaussian_covariances_full(resp,X,nk,means,reg_covar)
sklearn.mixture.gaussian_mixture._estimate_gaussian_covariances_spherical(resp,X,nk,means,reg_covar)
sklearn.mixture.gaussian_mixture._estimate_gaussian_covariances_tied(resp,X,nk,means,reg_covar)
sklearn.mixture.gaussian_mixture._estimate_gaussian_parameters(X,resp,reg_covar,covariance_type)
sklearn.mixture.gaussian_mixture._estimate_log_gaussian_prob(X,means,precisions_chol,covariance_type)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/mixture/gmm.py----------------------------------------
A:sklearn.mixture.gmm.rng->check_random_state(random_state)
A:sklearn.mixture.gmm.n_dim->len(mean)
A:sklearn.mixture.gmm.rand->check_random_state(random_state).rand(n_samples)
A:sklearn.mixture.gmm.(s, U)->scipy.linalg.eigh(covar)
A:sklearn.mixture.gmm.covars->numpy.asarray(covars)
A:sklearn.mixture.gmm.X->check_array(X, dtype=np.float64, ensure_min_samples=2, estimator=self)
A:sklearn.mixture.gmm.logprob->logsumexp(lpr, axis=1)
A:sklearn.mixture.gmm.responsibilities->numpy.zeros((X.shape[0], self.n_components))
A:sklearn.mixture.gmm.(logprob, _)->self.score_samples(X)
A:sklearn.mixture.gmm.(logprob, responsibilities)->self.score_samples(X)
A:sklearn.mixture.gmm.random_state->check_random_state(random_state)
A:sklearn.mixture.gmm.weight_cdf->numpy.cumsum(self.weights_)
A:sklearn.mixture.gmm.comps->numpy.cumsum(self.weights_).searchsorted(rand)
A:sklearn.mixture.gmm.num_comp_in_X->comp_in_X.sum()
A:sklearn.mixture.gmm.start_init_time->time()
A:sklearn.mixture.gmm.self.weights_->numpy.tile(1.0 / self.n_components, self.n_components)
A:sklearn.mixture.gmm.self.covars_->covar_mstep_func(self, X, responsibilities, weighted_X_sum, inverse_weights, min_covar)
A:sklearn.mixture.gmm.start_iter_time->time()
A:sklearn.mixture.gmm.(log_likelihoods, responsibilities)->self.score_samples(X)
A:sklearn.mixture.gmm.current_log_likelihood->log_likelihoods.mean()
A:sklearn.mixture.gmm.change->abs(current_log_likelihood - prev_log_likelihood)
A:sklearn.mixture.gmm.weights->numpy.zeros((X.shape[0], self.n_components)).sum(axis=0)
A:sklearn.mixture.gmm.weighted_X_sum->numpy.dot(responsibilities.T, X)
A:sklearn.mixture.gmm.cv->numpy.empty((gmm.n_components, n_features, n_features))
A:sklearn.mixture.gmm.nmix->len(means)
A:sklearn.mixture.gmm.log_prob->numpy.empty((n_samples, nmix))
A:sklearn.mixture.gmm.cv_chol->scipy.linalg.cholesky(cv + min_covar * np.eye(n_dim), lower=True)
A:sklearn.mixture.gmm.avg_X2->numpy.dot(X.T, X)
A:sklearn.mixture.gmm.avg_means2->numpy.dot(gmm.means_.T, weighted_X_sum)
sklearn.mixture.GMM(self,n_components=1,covariance_type='diag',random_state=None,tol=0.001,min_covar=0.001,n_iter=100,n_init=1,params='wmc',init_params='wmc',verbose=0)
sklearn.mixture._validate_covars(covars,covariance_type,n_components)
sklearn.mixture.distribute_covar_matrix_to_match_covariance_type(tied_cv,covariance_type,n_components)
sklearn.mixture.gmm.GMM(self,n_components=1,covariance_type='diag',random_state=None,tol=0.001,min_covar=0.001,n_iter=100,n_init=1,params='wmc',init_params='wmc',verbose=0)
sklearn.mixture.gmm.GMM.__init__(self,n_components=1,covariance_type='diag',random_state=None,tol=0.001,min_covar=0.001,n_iter=100,n_init=1,params='wmc',init_params='wmc',verbose=0)
sklearn.mixture.gmm._GMMBase(self,n_components=1,covariance_type='diag',random_state=None,tol=0.001,min_covar=0.001,n_iter=100,n_init=1,params='wmc',init_params='wmc',verbose=0)
sklearn.mixture.gmm._GMMBase.__init__(self,n_components=1,covariance_type='diag',random_state=None,tol=0.001,min_covar=0.001,n_iter=100,n_init=1,params='wmc',init_params='wmc',verbose=0)
sklearn.mixture.gmm._GMMBase._do_mstep(self,X,responsibilities,params,min_covar=0)
sklearn.mixture.gmm._GMMBase._fit(self,X,y=None,do_prediction=False)
sklearn.mixture.gmm._GMMBase._get_covars(self)
sklearn.mixture.gmm._GMMBase._n_parameters(self)
sklearn.mixture.gmm._GMMBase._set_covars(self,covars)
sklearn.mixture.gmm._GMMBase.aic(self,X)
sklearn.mixture.gmm._GMMBase.bic(self,X)
sklearn.mixture.gmm._GMMBase.fit(self,X,y=None)
sklearn.mixture.gmm._GMMBase.fit_predict(self,X,y=None)
sklearn.mixture.gmm._GMMBase.predict(self,X)
sklearn.mixture.gmm._GMMBase.predict_proba(self,X)
sklearn.mixture.gmm._GMMBase.sample(self,n_samples=1,random_state=None)
sklearn.mixture.gmm._GMMBase.score(self,X,y=None)
sklearn.mixture.gmm._GMMBase.score_samples(self,X)
sklearn.mixture.gmm._covar_mstep_diag(gmm,X,responsibilities,weighted_X_sum,norm,min_covar)
sklearn.mixture.gmm._covar_mstep_full(gmm,X,responsibilities,weighted_X_sum,norm,min_covar)
sklearn.mixture.gmm._covar_mstep_spherical(*args)
sklearn.mixture.gmm._covar_mstep_tied(gmm,X,responsibilities,weighted_X_sum,norm,min_covar)
sklearn.mixture.gmm._log_multivariate_normal_density_diag(X,means,covars)
sklearn.mixture.gmm._log_multivariate_normal_density_full(X,means,covars,min_covar=1e-07)
sklearn.mixture.gmm._log_multivariate_normal_density_spherical(X,means,covars)
sklearn.mixture.gmm._log_multivariate_normal_density_tied(X,means,covars)
sklearn.mixture.gmm._sample_gaussian(mean,covar,covariance_type='diag',n_samples=1,random_state=None)
sklearn.mixture.gmm._validate_covars(covars,covariance_type,n_components)
sklearn.mixture.gmm.distribute_covar_matrix_to_match_covariance_type(tied_cv,covariance_type,n_components)
sklearn.mixture.gmm.log_multivariate_normal_density(X,means,covars,covariance_type='diag')
sklearn.mixture.gmm.sample_gaussian(mean,covar,covariance_type='diag',n_samples=1,random_state=None)
sklearn.mixture.log_multivariate_normal_density(X,means,covars,covariance_type='diag')
sklearn.mixture.sample_gaussian(mean,covar,covariance_type='diag',n_samples=1,random_state=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/mixture/bayesian_mixture.py----------------------------------------
A:sklearn.mixture.bayesian_mixture.self.mean_prior_->check_array(self.mean_prior, dtype=[np.float64, np.float32], ensure_2d=False)
A:sklearn.mixture.bayesian_mixture.self.covariance_prior_->check_array(self.covariance_prior, dtype=[np.float64, np.float32], ensure_2d=False)
A:sklearn.mixture.bayesian_mixture.(nk, xk, sk)->_estimate_gaussian_parameters(X, np.exp(log_resp), self.reg_covar, self.covariance_type)
A:sklearn.mixture.bayesian_mixture.self.precisions_cholesky_->_compute_precision_cholesky(self.covariances_, self.covariance_type)
A:sklearn.mixture.bayesian_mixture.self.covariances_->numpy.empty((self.n_components, n_features, n_features))
A:sklearn.mixture.bayesian_mixture.digamma_sum->digamma(self.weight_concentration_[0] + self.weight_concentration_[1])
A:sklearn.mixture.bayesian_mixture.digamma_a->digamma(self.weight_concentration_[0])
A:sklearn.mixture.bayesian_mixture.digamma_b->digamma(self.weight_concentration_[1])
A:sklearn.mixture.bayesian_mixture.log_wishart->numpy.sum(_log_wishart_norm(self.degrees_of_freedom_, log_det_precisions_chol, n_features))
A:sklearn.mixture.bayesian_mixture.log_norm_weight->_log_dirichlet_norm(self.weight_concentration_)
A:sklearn.mixture.bayesian_mixture.self.precisions_->numpy.dot(self.precisions_cholesky_, self.precisions_cholesky_.T)
sklearn.mixture.BayesianGaussianMixture(self,n_components=1,covariance_type='full',tol=0.001,reg_covar=1e-06,max_iter=100,n_init=1,init_params='kmeans',weight_concentration_prior_type='dirichlet_process',weight_concentration_prior=None,mean_precision_prior=None,mean_prior=None,degrees_of_freedom_prior=None,covariance_prior=None,random_state=None,warm_start=False,verbose=0,verbose_interval=10)
sklearn.mixture.BayesianGaussianMixture._check_is_fitted(self)
sklearn.mixture.BayesianGaussianMixture._check_means_parameters(self,X)
sklearn.mixture.BayesianGaussianMixture._check_parameters(self,X)
sklearn.mixture.BayesianGaussianMixture._check_precision_parameters(self,X)
sklearn.mixture.BayesianGaussianMixture._check_weights_parameters(self)
sklearn.mixture.BayesianGaussianMixture._checkcovariance_prior_parameter(self,X)
sklearn.mixture.BayesianGaussianMixture._compute_lower_bound(self,log_resp,log_prob_norm)
sklearn.mixture.BayesianGaussianMixture._estimate_log_prob(self,X)
sklearn.mixture.BayesianGaussianMixture._estimate_log_weights(self)
sklearn.mixture.BayesianGaussianMixture._estimate_means(self,nk,xk)
sklearn.mixture.BayesianGaussianMixture._estimate_precisions(self,nk,xk,sk)
sklearn.mixture.BayesianGaussianMixture._estimate_weights(self,nk)
sklearn.mixture.BayesianGaussianMixture._estimate_wishart_diag(self,nk,xk,sk)
sklearn.mixture.BayesianGaussianMixture._estimate_wishart_full(self,nk,xk,sk)
sklearn.mixture.BayesianGaussianMixture._estimate_wishart_spherical(self,nk,xk,sk)
sklearn.mixture.BayesianGaussianMixture._estimate_wishart_tied(self,nk,xk,sk)
sklearn.mixture.BayesianGaussianMixture._get_parameters(self)
sklearn.mixture.BayesianGaussianMixture._initialize(self,X,resp)
sklearn.mixture.BayesianGaussianMixture._m_step(self,X,log_resp)
sklearn.mixture.BayesianGaussianMixture._set_parameters(self,params)
sklearn.mixture.bayesian_mixture.BayesianGaussianMixture(self,n_components=1,covariance_type='full',tol=0.001,reg_covar=1e-06,max_iter=100,n_init=1,init_params='kmeans',weight_concentration_prior_type='dirichlet_process',weight_concentration_prior=None,mean_precision_prior=None,mean_prior=None,degrees_of_freedom_prior=None,covariance_prior=None,random_state=None,warm_start=False,verbose=0,verbose_interval=10)
sklearn.mixture.bayesian_mixture.BayesianGaussianMixture.__init__(self,n_components=1,covariance_type='full',tol=0.001,reg_covar=1e-06,max_iter=100,n_init=1,init_params='kmeans',weight_concentration_prior_type='dirichlet_process',weight_concentration_prior=None,mean_precision_prior=None,mean_prior=None,degrees_of_freedom_prior=None,covariance_prior=None,random_state=None,warm_start=False,verbose=0,verbose_interval=10)
sklearn.mixture.bayesian_mixture.BayesianGaussianMixture._check_is_fitted(self)
sklearn.mixture.bayesian_mixture.BayesianGaussianMixture._check_means_parameters(self,X)
sklearn.mixture.bayesian_mixture.BayesianGaussianMixture._check_parameters(self,X)
sklearn.mixture.bayesian_mixture.BayesianGaussianMixture._check_precision_parameters(self,X)
sklearn.mixture.bayesian_mixture.BayesianGaussianMixture._check_weights_parameters(self)
sklearn.mixture.bayesian_mixture.BayesianGaussianMixture._checkcovariance_prior_parameter(self,X)
sklearn.mixture.bayesian_mixture.BayesianGaussianMixture._compute_lower_bound(self,log_resp,log_prob_norm)
sklearn.mixture.bayesian_mixture.BayesianGaussianMixture._estimate_log_prob(self,X)
sklearn.mixture.bayesian_mixture.BayesianGaussianMixture._estimate_log_weights(self)
sklearn.mixture.bayesian_mixture.BayesianGaussianMixture._estimate_means(self,nk,xk)
sklearn.mixture.bayesian_mixture.BayesianGaussianMixture._estimate_precisions(self,nk,xk,sk)
sklearn.mixture.bayesian_mixture.BayesianGaussianMixture._estimate_weights(self,nk)
sklearn.mixture.bayesian_mixture.BayesianGaussianMixture._estimate_wishart_diag(self,nk,xk,sk)
sklearn.mixture.bayesian_mixture.BayesianGaussianMixture._estimate_wishart_full(self,nk,xk,sk)
sklearn.mixture.bayesian_mixture.BayesianGaussianMixture._estimate_wishart_spherical(self,nk,xk,sk)
sklearn.mixture.bayesian_mixture.BayesianGaussianMixture._estimate_wishart_tied(self,nk,xk,sk)
sklearn.mixture.bayesian_mixture.BayesianGaussianMixture._get_parameters(self)
sklearn.mixture.bayesian_mixture.BayesianGaussianMixture._initialize(self,X,resp)
sklearn.mixture.bayesian_mixture.BayesianGaussianMixture._m_step(self,X,log_resp)
sklearn.mixture.bayesian_mixture.BayesianGaussianMixture._set_parameters(self,params)
sklearn.mixture.bayesian_mixture._log_dirichlet_norm(dirichlet_concentration)
sklearn.mixture.bayesian_mixture._log_wishart_norm(degrees_of_freedom,log_det_precisions_chol,n_features)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/mixture/dpgmm.py----------------------------------------
A:sklearn.mixture.dpgmm.v->numpy.exp(v - out)
A:sklearn.mixture.dpgmm.out->logsumexp(v)
A:sklearn.mixture.dpgmm.l->numpy.sum(digamma(0.5 * (a - np.arange(-1, n_features - 1))))
A:sklearn.mixture.dpgmm.logprior->self._bound_concentration()
A:sklearn.mixture.dpgmm.q->(cdist(x, mu[np.newaxis], 'mahalanobis', VI=A) ** 2).reshape(-1)
A:sklearn.mixture.dpgmm.bound->numpy.sum(z * p, axis=-1)
A:sklearn.mixture.dpgmm.X->check_array(X)
A:sklearn.mixture.dpgmm.z->log_normalize(z, axis=-1)
A:sklearn.mixture.dpgmm.sd->digamma(self.gamma_.T[1] + self.gamma_.T[2])
A:sklearn.mixture.dpgmm.dgamma2->numpy.zeros(self.n_components)
A:sklearn.mixture.dpgmm.p->_bound_state_log_lik(X, self._initial_bound + self.bound_prec_, self.precs_, self.means_, self.covariance_type)
A:sklearn.mixture.dpgmm.sz->numpy.sum(z, axis=0)
A:sklearn.mixture.dpgmm.num->numpy.dot(cov, num)
A:sklearn.mixture.dpgmm.sq_diff->numpy.sum((X - self.means_[k]) ** 2, axis=1)
A:sklearn.mixture.dpgmm.self.scale_->numpy.identity(n_features)
A:sklearn.mixture.dpgmm.self.det_scale_->numpy.ones(self.n_components)
A:sklearn.mixture.dpgmm.sum_resp->numpy.sum(z.T[k])
A:sklearn.mixture.dpgmm.self.scale_[k]->pinvh(self.scale_[k])
A:sklearn.mixture.dpgmm.self.det_scale_[k]->scipy.linalg.det(self.scale_[k])
A:sklearn.mixture.dpgmm.dg12->digamma(self.gamma_.T[1] + self.gamma_.T[2])
A:sklearn.mixture.dpgmm.c->numpy.sum(z * _bound_state_log_lik(X, self._initial_bound + self.bound_prec_, self.precs_, self.means_, self.covariance_type))
A:sklearn.mixture.dpgmm.self.random_state_->check_random_state(self.random_state)
A:sklearn.mixture.dpgmm.self.weights_->numpy.tile(1.0 / self.n_components, self.n_components)
A:sklearn.mixture.dpgmm.self.dof_->numpy.ones(self.n_components)
A:sklearn.mixture.dpgmm.self.precs_->numpy.identity(n_features)
A:sklearn.mixture.dpgmm.self.bound_prec_->numpy.zeros(self.n_components)
A:sklearn.mixture.dpgmm.self.bound_prec_[k]->wishart_log_det(self.dof_[k], self.scale_[k], self.det_scale_[k], n_features)
A:sklearn.mixture.dpgmm.(curr_logprob, z)->self.score_samples(X)
A:sklearn.mixture.dpgmm.change->abs(current_log_likelihood - prev_log_likelihood)
A:sklearn.mixture.dpgmm.dg->digamma(self.gamma_)
A:sklearn.mixture.dpgmm.sg->digamma(np.sum(self.gamma_))
sklearn.mixture.DPGMM(self,n_components=1,covariance_type='diag',alpha=1.0,random_state=None,tol=0.001,verbose=0,min_covar=None,n_iter=10,params='wmc',init_params='wmc')
sklearn.mixture.VBGMM(self,n_components=1,covariance_type='diag',alpha=1.0,random_state=None,tol=0.001,verbose=0,min_covar=None,n_iter=10,params='wmc',init_params='wmc')
sklearn.mixture.VBGMM._bound_concentration(self)
sklearn.mixture.VBGMM._bound_proportions(self,z)
sklearn.mixture.VBGMM._fit(self,X,y=None)
sklearn.mixture.VBGMM._initialize_gamma(self)
sklearn.mixture.VBGMM._monitor(self,X,z,n,end=False)
sklearn.mixture.VBGMM._set_weights(self)
sklearn.mixture.VBGMM._update_concentration(self,z)
sklearn.mixture.VBGMM.score_samples(self,X)
sklearn.mixture.dpgmm.DPGMM(self,n_components=1,covariance_type='diag',alpha=1.0,random_state=None,tol=0.001,verbose=0,min_covar=None,n_iter=10,params='wmc',init_params='wmc')
sklearn.mixture.dpgmm.DPGMM.__init__(self,n_components=1,covariance_type='diag',alpha=1.0,random_state=None,tol=0.001,verbose=0,min_covar=None,n_iter=10,params='wmc',init_params='wmc')
sklearn.mixture.dpgmm.VBGMM(self,n_components=1,covariance_type='diag',alpha=1.0,random_state=None,tol=0.001,verbose=0,min_covar=None,n_iter=10,params='wmc',init_params='wmc')
sklearn.mixture.dpgmm.VBGMM.__init__(self,n_components=1,covariance_type='diag',alpha=1.0,random_state=None,tol=0.001,verbose=0,min_covar=None,n_iter=10,params='wmc',init_params='wmc')
sklearn.mixture.dpgmm.VBGMM._bound_concentration(self)
sklearn.mixture.dpgmm.VBGMM._bound_proportions(self,z)
sklearn.mixture.dpgmm.VBGMM._fit(self,X,y=None)
sklearn.mixture.dpgmm.VBGMM._initialize_gamma(self)
sklearn.mixture.dpgmm.VBGMM._monitor(self,X,z,n,end=False)
sklearn.mixture.dpgmm.VBGMM._set_weights(self)
sklearn.mixture.dpgmm.VBGMM._update_concentration(self,z)
sklearn.mixture.dpgmm.VBGMM.score_samples(self,X)
sklearn.mixture.dpgmm._DPGMMBase(self,n_components=1,covariance_type='diag',alpha=1.0,random_state=None,tol=0.001,verbose=0,min_covar=None,n_iter=10,params='wmc',init_params='wmc')
sklearn.mixture.dpgmm._DPGMMBase.__init__(self,n_components=1,covariance_type='diag',alpha=1.0,random_state=None,tol=0.001,verbose=0,min_covar=None,n_iter=10,params='wmc',init_params='wmc')
sklearn.mixture.dpgmm._DPGMMBase._bound_concentration(self)
sklearn.mixture.dpgmm._DPGMMBase._bound_means(self)
sklearn.mixture.dpgmm._DPGMMBase._bound_precisions(self)
sklearn.mixture.dpgmm._DPGMMBase._bound_proportions(self,z)
sklearn.mixture.dpgmm._DPGMMBase._do_mstep(self,X,z,params)
sklearn.mixture.dpgmm._DPGMMBase._fit(self,X,y=None)
sklearn.mixture.dpgmm._DPGMMBase._get_covars(self)
sklearn.mixture.dpgmm._DPGMMBase._get_precisions(self)
sklearn.mixture.dpgmm._DPGMMBase._initialize_gamma(self)
sklearn.mixture.dpgmm._DPGMMBase._logprior(self,z)
sklearn.mixture.dpgmm._DPGMMBase._monitor(self,X,z,n,end=False)
sklearn.mixture.dpgmm._DPGMMBase._set_covars(self,covars)
sklearn.mixture.dpgmm._DPGMMBase._set_weights(self)
sklearn.mixture.dpgmm._DPGMMBase._update_concentration(self,z)
sklearn.mixture.dpgmm._DPGMMBase._update_means(self,X,z)
sklearn.mixture.dpgmm._DPGMMBase._update_precisions(self,X,z)
sklearn.mixture.dpgmm._DPGMMBase.lower_bound(self,X,z)
sklearn.mixture.dpgmm._DPGMMBase.score_samples(self,X)
sklearn.mixture.dpgmm._bound_state_log_lik(X,initial_bound,precs,means,covariance_type)
sklearn.mixture.dpgmm._bound_wishart(a,B,detB)
sklearn.mixture.dpgmm._sym_quad_form(x,mu,A)
sklearn.mixture.dpgmm.digamma(x)
sklearn.mixture.dpgmm.gammaln(x)
sklearn.mixture.dpgmm.log_normalize(v,axis=0)
sklearn.mixture.dpgmm.wishart_log_det(a,b,detB,n_features)
sklearn.mixture.dpgmm.wishart_logz(v,s,dets,n_features)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/mixture/base.py----------------------------------------
A:sklearn.mixture.base.param->numpy.array(param)
A:sklearn.mixture.base.X->numpy.vstack([mean + rng.randn(sample, n_features) * np.sqrt(covariance) for (mean, covariance, sample) in zip(self.means_, self.covariances_, n_samples_comp)])
A:sklearn.mixture.base.resp->check_random_state(self.random_state).rand(n_samples, self.n_components)
A:sklearn.mixture.base.random_state->check_random_state(self.random_state)
A:sklearn.mixture.base.(log_prob_norm, log_resp)->self._estimate_log_prob_resp(X)
A:sklearn.mixture.base.self.lower_bound_->self._compute_lower_bound(log_resp, log_prob_norm)
A:sklearn.mixture.base.best_params->self._get_parameters()
A:sklearn.mixture.base.(_, log_resp)->self._estimate_log_prob_resp(X)
A:sklearn.mixture.base.rng->check_random_state(self.random_state)
A:sklearn.mixture.base.n_samples_comp->check_random_state(self.random_state).multinomial(n_samples, self.weights_)
A:sklearn.mixture.base.y->numpy.concatenate([j * np.ones(sample, dtype=int) for (j, sample) in enumerate(n_samples_comp)])
A:sklearn.mixture.base.weighted_log_prob->self._estimate_weighted_log_prob(X)
A:sklearn.mixture.base.log_prob_norm->logsumexp(weighted_log_prob, axis=1)
A:sklearn.mixture.base.self._init_prev_time->time()
A:sklearn.mixture.base.cur_time->time()
sklearn.mixture.base.BaseMixture(self,n_components,tol,reg_covar,max_iter,n_init,init_params,random_state,warm_start,verbose,verbose_interval)
sklearn.mixture.base.BaseMixture.__init__(self,n_components,tol,reg_covar,max_iter,n_init,init_params,random_state,warm_start,verbose,verbose_interval)
sklearn.mixture.base.BaseMixture._check_initial_parameters(self,X)
sklearn.mixture.base.BaseMixture._check_is_fitted(self)
sklearn.mixture.base.BaseMixture._check_parameters(self,X)
sklearn.mixture.base.BaseMixture._e_step(self,X)
sklearn.mixture.base.BaseMixture._estimate_log_prob(self,X)
sklearn.mixture.base.BaseMixture._estimate_log_prob_resp(self,X)
sklearn.mixture.base.BaseMixture._estimate_log_weights(self)
sklearn.mixture.base.BaseMixture._estimate_weighted_log_prob(self,X)
sklearn.mixture.base.BaseMixture._get_parameters(self)
sklearn.mixture.base.BaseMixture._initialize(self,X,resp)
sklearn.mixture.base.BaseMixture._initialize_parameters(self,X,random_state)
sklearn.mixture.base.BaseMixture._m_step(self,X,log_resp)
sklearn.mixture.base.BaseMixture._print_verbose_msg_init_beg(self,n_init)
sklearn.mixture.base.BaseMixture._print_verbose_msg_init_end(self,ll)
sklearn.mixture.base.BaseMixture._print_verbose_msg_iter_end(self,n_iter,diff_ll)
sklearn.mixture.base.BaseMixture._set_parameters(self,params)
sklearn.mixture.base.BaseMixture.fit(self,X,y=None)
sklearn.mixture.base.BaseMixture.predict(self,X)
sklearn.mixture.base.BaseMixture.predict_proba(self,X)
sklearn.mixture.base.BaseMixture.sample(self,n_samples=1)
sklearn.mixture.base.BaseMixture.score(self,X,y=None)
sklearn.mixture.base.BaseMixture.score_samples(self,X)
sklearn.mixture.base._check_X(X,n_components=None,n_features=None)
sklearn.mixture.base._check_shape(param,param_shape,name)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/mixture/tests/test_gmm.py----------------------------------------
A:sklearn.mixture.tests.test_gmm.rng->numpy.random.RandomState(1)
A:sklearn.mixture.tests.test_gmm.samples->sklearn.mixture.GMM(n_components=2, n_init=2, verbose=2).sample(n)
A:sklearn.mixture.tests.test_gmm.A->numpy.random.RandomState(1).randn(n_features, n_features)
A:sklearn.mixture.tests.test_gmm.x->sklearn.mixture.gmm._sample_gaussian([0, 0], [[4, 3], [1, 0.1]], covariance_type='full', random_state=42)
A:sklearn.mixture.tests.test_gmm.ref->_naive_lmvnpdf_diag(X, mu, cv)
A:sklearn.mixture.tests.test_gmm.stds->numpy.sqrt(cv)
A:sklearn.mixture.tests.test_gmm.ref[:, i]->numpy.log(stats.norm.pdf(X, m, std)).sum(axis=1)
A:sklearn.mixture.tests.test_gmm.lpr->assert_warns_message(DeprecationWarning, 'The function log_multivariate_normal_density is deprecated in 0.18 and will be removed in 0.20.', mixture.log_multivariate_normal_density, X, mu, fullcv, 'full')
A:sklearn.mixture.tests.test_gmm.cv->numpy.array([[[-1, 0], [0, 1]]])
A:sklearn.mixture.tests.test_gmm.reference->_naive_lmvnpdf_diag(X, mu, cv)
A:sklearn.mixture.tests.test_gmm.fullcv->numpy.array([np.diag(x) for x in cv])
A:sklearn.mixture.tests.test_gmm.mu->numpy.array([[8, 8]])
A:sklearn.mixture.tests.test_gmm.g->sklearn.mixture.GMM(n_components=2, n_init=2, verbose=2)
A:sklearn.mixture.tests.test_gmm.weights->numpy.random.RandomState(1).rand(n_components)
A:sklearn.mixture.tests.test_gmm.means->numpy.random.RandomState(1).randint(-20, 20, (n_components, n_features))
A:sklearn.mixture.tests.test_gmm.self.weights->numpy.random.RandomState(1).rand(self.n_components)
A:sklearn.mixture.tests.test_gmm.self.means->numpy.random.RandomState(1).randint(-20, 20, (self.n_components, self.n_features))
A:sklearn.mixture.tests.test_gmm.self.I->numpy.eye(self.n_features)
A:sklearn.mixture.tests.test_gmm.gaussidx->numpy.repeat(np.arange(self.n_components), 5)
A:sklearn.mixture.tests.test_gmm.n_samples->len(gaussidx)
A:sklearn.mixture.tests.test_gmm.(ll, responsibilities)->sklearn.mixture.GMM(n_components=2, n_init=2, verbose=2).score_samples(X)
A:sklearn.mixture.tests.test_gmm.g.covars_->numpy.maximum(self.covars[self.covariance_type], 0.1)
A:sklearn.mixture.tests.test_gmm.X->numpy.random.RandomState(1).randn(30, 5)
A:sklearn.mixture.tests.test_gmm.delta_min->numpy.diff(trainll).min()
A:sklearn.mixture.tests.test_gmm.trainll->sklearn.mixture.GMM(n_components=2, n_init=2, verbose=2).score(X)
A:sklearn.mixture.tests.test_gmm.train1->sklearn.mixture.GMM(n_components=2, n_init=2, verbose=2).fit(X).score(X).sum()
A:sklearn.mixture.tests.test_gmm.train2->sklearn.mixture.GMM(n_components=2, n_init=2, verbose=2).fit(X).score(X).sum()
A:sklearn.mixture.tests.test_gmm.g_full->sklearn.mixture.GMM(n_components=n_components, covariance_type='full', random_state=rng, min_covar=1e-07, n_iter=1)
A:sklearn.mixture.tests.test_gmm.g_full_bic->sklearn.mixture.GMM(n_components=n_components, covariance_type='full', random_state=rng, min_covar=1e-07, n_iter=1).bic(X)
A:sklearn.mixture.tests.test_gmm.model2->copy.deepcopy(model)
A:sklearn.mixture.tests.test_gmm.predictions_1->sklearn.mixture.GMM(n_components=n_comps, n_iter=0).fit(X).predict(X)
A:sklearn.mixture.tests.test_gmm.predictions_2->copy.deepcopy(model).fit_predict(X)
A:sklearn.mixture.tests.test_gmm.lrng->numpy.random.RandomState(101)
A:sklearn.mixture.tests.test_gmm.component_0->numpy.random.RandomState(101).randn(n_samples, n_dim)
A:sklearn.mixture.tests.test_gmm.model->sklearn.mixture.GMM(n_components=n_comps, n_iter=0)
A:sklearn.mixture.tests.test_gmm.z->sklearn.mixture.GMM(n_components=n_comps, n_iter=0).fit_predict(X)
A:sklearn.mixture.tests.test_gmm.gmm->sklearn.mixture.GMM(2, params='wc', covariance_type=covariance_type, min_covar=0.001)
A:sklearn.mixture.tests.test_gmm.sys.stdout->StringIO()
sklearn.mixture.tests.test_GMMTester
sklearn.mixture.tests.test_GMMTester._setUp(self)
sklearn.mixture.tests.test_GMMTester.score(self,g,X)
sklearn.mixture.tests.test_GMMTester.test_eval(self)
sklearn.mixture.tests.test_GMMTester.test_sample(self,n=100)
sklearn.mixture.tests.test_GMMTester.test_train(self,params='wmc')
sklearn.mixture.tests.test_GMMTester.test_train_1d(self,params='wmc')
sklearn.mixture.tests.test_GMMTester.test_train_degenerate(self,params='wmc')
sklearn.mixture.tests.test_gmm.GMMTester
sklearn.mixture.tests.test_gmm.GMMTester._setUp(self)
sklearn.mixture.tests.test_gmm.GMMTester.score(self,g,X)
sklearn.mixture.tests.test_gmm.GMMTester.test_eval(self)
sklearn.mixture.tests.test_gmm.GMMTester.test_sample(self,n=100)
sklearn.mixture.tests.test_gmm.GMMTester.test_train(self,params='wmc')
sklearn.mixture.tests.test_gmm.GMMTester.test_train_1d(self,params='wmc')
sklearn.mixture.tests.test_gmm.GMMTester.test_train_degenerate(self,params='wmc')
sklearn.mixture.tests.test_gmm.TestGMMWithDiagonalCovars(unittest.TestCase,GMMTester)
sklearn.mixture.tests.test_gmm.TestGMMWithFullCovars(unittest.TestCase,GMMTester)
sklearn.mixture.tests.test_gmm.TestGMMWithSphericalCovars(unittest.TestCase,GMMTester)
sklearn.mixture.tests.test_gmm.TestGMMWithTiedCovars(unittest.TestCase,GMMTester)
sklearn.mixture.tests.test_gmm._naive_lmvnpdf_diag(X,mu,cv)
sklearn.mixture.tests.test_gmm.assert_fit_predict_correct(model,X)
sklearn.mixture.tests.test_gmm.check_positive_definite_covars(covariance_type)
sklearn.mixture.tests.test_gmm.test_1d_1component()
sklearn.mixture.tests.test_gmm.test_GMM_attributes()
sklearn.mixture.tests.test_gmm.test_aic()
sklearn.mixture.tests.test_gmm.test_fit_predict()
sklearn.mixture.tests.test_gmm.test_lmvnpdf_diag()
sklearn.mixture.tests.test_gmm.test_lmvnpdf_full()
sklearn.mixture.tests.test_gmm.test_lmvnpdf_spherical()
sklearn.mixture.tests.test_gmm.test_lvmpdf_full_cv_non_positive_definite()
sklearn.mixture.tests.test_gmm.test_multiple_init()
sklearn.mixture.tests.test_gmm.test_n_parameters()
sklearn.mixture.tests.test_gmm.test_positive_definite_covars()
sklearn.mixture.tests.test_gmm.test_sample_gaussian()
sklearn.mixture.tests.test_gmm.test_verbose_first_level()
sklearn.mixture.tests.test_gmm.test_verbose_second_level()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/mixture/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/mixture/tests/test_gaussian_mixture.py----------------------------------------
A:sklearn.mixture.tests.test_gaussian_mixture.rng->numpy.random.RandomState(0)
A:sklearn.mixture.tests.test_gaussian_mixture.X->numpy.vstack((np.ones((n_samples // 2, n_features)), np.zeros((n_samples // 2, n_features))))
A:sklearn.mixture.tests.test_gaussian_mixture.self.weights->numpy.random.RandomState(0).rand(n_components)
A:sklearn.mixture.tests.test_gaussian_mixture.self.X->dict(zip(COVARIANCE_TYPE, [generate_data(n_samples, n_features, self.weights, self.means, self.covariances, covar_type) for covar_type in COVARIANCE_TYPE]))
A:sklearn.mixture.tests.test_gaussian_mixture.self.Y->numpy.hstack([k * np.ones(int(np.round(w * n_samples))) for (k, w) in enumerate(self.weights)])
A:sklearn.mixture.tests.test_gaussian_mixture.gmm->GaussianMixture(n_components=n_components, covariance_type=covar_type, random_state=rng)
A:sklearn.mixture.tests.test_gaussian_mixture.X_bad_dim->numpy.random.RandomState(0).rand(n_components, n_features + 1)
A:sklearn.mixture.tests.test_gaussian_mixture.rand_data->RandomData(np.random.RandomState(random_state), scale=1)
A:sklearn.mixture.tests.test_gaussian_mixture.g->GaussianMixture(n_components=n_components, n_init=1, max_iter=5, reg_covar=0, random_state=random_state, warm_start=False, tol=1e-06)
A:sklearn.mixture.tests.test_gaussian_mixture.weights_bad_shape->numpy.random.RandomState(0).rand(n_components, 1)
A:sklearn.mixture.tests.test_gaussian_mixture.weights_bad_norm->numpy.random.RandomState(0).rand(n_components)
A:sklearn.mixture.tests.test_gaussian_mixture.means_bad_shape->numpy.random.RandomState(0).rand(n_components + 1, n_features)
A:sklearn.mixture.tests.test_gaussian_mixture.precisions_not_pos->numpy.ones((n_components, n_features, n_features))
A:sklearn.mixture.tests.test_gaussian_mixture.precisions_not_pos[0]->numpy.eye(n_features)
A:sklearn.mixture.tests.test_gaussian_mixture.resp->GaussianMixture(n_components=n_components, n_init=1, max_iter=5, reg_covar=0, random_state=random_state, warm_start=False, tol=1e-06).predict_proba(X)
A:sklearn.mixture.tests.test_gaussian_mixture.nk->numpy.array([n_samples])
A:sklearn.mixture.tests.test_gaussian_mixture.xk->numpy.vstack((np.ones((n_samples // 2, n_features)), np.zeros((n_samples // 2, n_features)))).mean()
A:sklearn.mixture.tests.test_gaussian_mixture.covars_pred->_estimate_gaussian_covariances_full(resp, X, nk, xk, 0)
A:sklearn.mixture.tests.test_gaussian_mixture.ecov->EmpiricalCovariance()
A:sklearn.mixture.tests.test_gaussian_mixture.precs_chol_pred->_compute_precision_cholesky(covars_pred_spherical, 'spherical')
A:sklearn.mixture.tests.test_gaussian_mixture.precs_pred->numpy.dot(precs_chol_pred, precs_chol_pred.T)
A:sklearn.mixture.tests.test_gaussian_mixture.precs_est->scipy.linalg.inv(covars_pred_tied)
A:sklearn.mixture.tests.test_gaussian_mixture.covars_pred_full->_estimate_gaussian_covariances_full(resp, X, nk, xk, 0)
A:sklearn.mixture.tests.test_gaussian_mixture.covars_pred_tied->_estimate_gaussian_covariances_tied(resp, X, nk, xk, 0)
A:sklearn.mixture.tests.test_gaussian_mixture.covars_pred_diag->_estimate_gaussian_covariances_diag(resp, X, nk, xk, 0)
A:sklearn.mixture.tests.test_gaussian_mixture.ecov.covariance_->numpy.diag(np.diag(cov_full))
A:sklearn.mixture.tests.test_gaussian_mixture.cov_diag->numpy.diag(cov_diag)
A:sklearn.mixture.tests.test_gaussian_mixture.covars_pred_spherical->_estimate_gaussian_covariances_spherical(resp, X, nk, xk, 0)
A:sklearn.mixture.tests.test_gaussian_mixture.predected_det->numpy.array([np.prod(cov) for cov in covariance])
A:sklearn.mixture.tests.test_gaussian_mixture.expected_det->_compute_log_det_cholesky(_compute_precision_cholesky(covariance, covar_type), covar_type, n_features=n_features)
A:sklearn.mixture.tests.test_gaussian_mixture.stds->numpy.sqrt(covars)
A:sklearn.mixture.tests.test_gaussian_mixture.resp[:, i]->scipy.stats.norm.logpdf(X, mean, std).sum(axis=1)
A:sklearn.mixture.tests.test_gaussian_mixture.covars_diag->numpy.random.RandomState(0).rand(n_components, n_features)
A:sklearn.mixture.tests.test_gaussian_mixture.log_prob_naive->_naive_lmvnpdf_diag(X, means, [[k] * n_features for k in covars_spherical])
A:sklearn.mixture.tests.test_gaussian_mixture.precs_full->numpy.array([np.diag(1.0 / np.sqrt(x)) for x in covars_diag])
A:sklearn.mixture.tests.test_gaussian_mixture.log_prob->_estimate_log_gaussian_prob(X, means, precs_spherical, 'spherical')
A:sklearn.mixture.tests.test_gaussian_mixture.covars_tied->numpy.array([x for x in covars_diag]).mean(axis=0)
A:sklearn.mixture.tests.test_gaussian_mixture.precs_tied->numpy.diag(np.sqrt(1.0 / covars_tied))
A:sklearn.mixture.tests.test_gaussian_mixture.covars_spherical->numpy.random.RandomState(0).rand(n_components, n_features).mean(axis=1)
A:sklearn.mixture.tests.test_gaussian_mixture.Y_pred->GaussianMixture(n_components=n_components, n_init=1, max_iter=5, reg_covar=0, random_state=random_state, warm_start=False, tol=1e-06).predict(X)
A:sklearn.mixture.tests.test_gaussian_mixture.Y_pred_proba->GaussianMixture(n_components=n_components, n_init=1, max_iter=5, reg_covar=0, random_state=random_state, warm_start=False, tol=1e-06).predict_proba(X).argmax(axis=1)
A:sklearn.mixture.tests.test_gaussian_mixture.arg_idx1->numpy.trace(prec_pred, axis1=1, axis2=2).argsort()
A:sklearn.mixture.tests.test_gaussian_mixture.arg_idx2->numpy.trace(prec_test, axis1=1, axis2=2).argsort()
A:sklearn.mixture.tests.test_gaussian_mixture.prec_pred->numpy.array([np.diag(d) for d in g.precisions_])
A:sklearn.mixture.tests.test_gaussian_mixture.prec_test->numpy.array([np.diag(d) for d in rand_data.precisions['diag']])
A:sklearn.mixture.tests.test_gaussian_mixture.ll->numpy.array(ll)
A:sklearn.mixture.tests.test_gaussian_mixture.g_best->GaussianMixture(n_components=n_components, n_init=n_init, reg_covar=0, random_state=rng, covariance_type=covar_type)
A:sklearn.mixture.tests.test_gaussian_mixture.train1->GaussianMixture(n_components=n_components, covariance_type=cv_type, random_state=rng).fit(X).score(X)
A:sklearn.mixture.tests.test_gaussian_mixture.train2->GaussianMixture(n_components=n_components, covariance_type=cv_type, random_state=rng, n_init=5).fit(X).score(X)
A:sklearn.mixture.tests.test_gaussian_mixture.bic_full->GaussianMixture(n_components=n_components, covariance_type='full', random_state=rng).fit(X).bic(X)
A:sklearn.mixture.tests.test_gaussian_mixture.bic->GaussianMixture(n_components=n_components, covariance_type=covariance_type, random_state=rng).fit(X).bic(X)
A:sklearn.mixture.tests.test_gaussian_mixture.h->GaussianMixture(n_components=n_components, n_init=1, max_iter=5, reg_covar=0, random_state=random_state, warm_start=True, tol=1e-06)
A:sklearn.mixture.tests.test_gaussian_mixture.sys.stdout->StringIO()
A:sklearn.mixture.tests.test_gaussian_mixture.score1->GaussianMixture(n_components=n_components, n_init=1, max_iter=5, reg_covar=0, random_state=random_state, warm_start=True, tol=1e-06).fit(X).score(X)
A:sklearn.mixture.tests.test_gaussian_mixture.score2->GaussianMixture(n_components=n_components, n_init=1, max_iter=5, reg_covar=0, random_state=random_state, warm_start=True, tol=1e-06).fit(X).score(X)
A:sklearn.mixture.tests.test_gaussian_mixture.gmm1->GaussianMixture(n_components=n_components, n_init=1, max_iter=1, random_state=random_state).fit(X)
A:sklearn.mixture.tests.test_gaussian_mixture.gmm_score->GaussianMixture(n_components=n_components, n_init=1, max_iter=1, random_state=random_state).fit(X).score(X)
A:sklearn.mixture.tests.test_gaussian_mixture.gmm_score_proba->GaussianMixture(n_components=n_components, n_init=1, max_iter=1, random_state=random_state).fit(X).score_samples(X).mean()
A:sklearn.mixture.tests.test_gaussian_mixture.gmm2->GaussianMixture(n_components=n_components, n_init=100, max_iter=1, random_state=random_state).fit(X)
A:sklearn.mixture.tests.test_gaussian_mixture.gmm_score_samples->GaussianMixture(n_components=n_components, covariance_type=covar_type, random_state=rng).fit(X).score_samples(X)
A:sklearn.mixture.tests.test_gaussian_mixture.current_log_likelihood->GaussianMixture(n_components=n_components, covariance_type=covar_type, random_state=rng).fit(X).score(X)
A:sklearn.mixture.tests.test_gaussian_mixture.(X_s, y_s)->GaussianMixture(n_components=n_components, covariance_type=covar_type, random_state=rng).sample(n_samples)
A:sklearn.mixture.tests.test_gaussian_mixture.means_s->numpy.array([np.mean(X_s[y_s == k], 0) for k in range(n_components)])
A:sklearn.mixture.tests.test_gaussian_mixture.(X_s, _)->GaussianMixture(n_components=n_components, covariance_type=covar_type, random_state=rng).sample(sample_size)
sklearn.mixture.tests.test_gaussian_mixture.RandomData(self,rng,n_samples=500,n_components=2,n_features=2,scale=50)
sklearn.mixture.tests.test_gaussian_mixture.RandomData.__init__(self,rng,n_samples=500,n_components=2,n_features=2,scale=50)
sklearn.mixture.tests.test_gaussian_mixture._naive_lmvnpdf_diag(X,means,covars)
sklearn.mixture.tests.test_gaussian_mixture.generate_data(n_samples,n_features,weights,means,precisions,covariance_type)
sklearn.mixture.tests.test_gaussian_mixture.test_bic_1d_1component()
sklearn.mixture.tests.test_gaussian_mixture.test_check_X()
sklearn.mixture.tests.test_gaussian_mixture.test_check_means()
sklearn.mixture.tests.test_gaussian_mixture.test_check_precisions()
sklearn.mixture.tests.test_gaussian_mixture.test_check_weights()
sklearn.mixture.tests.test_gaussian_mixture.test_compute_log_det_cholesky()
sklearn.mixture.tests.test_gaussian_mixture.test_gaussian_mixture_aic_bic()
sklearn.mixture.tests.test_gaussian_mixture.test_gaussian_mixture_attributes()
sklearn.mixture.tests.test_gaussian_mixture.test_gaussian_mixture_estimate_log_prob_resp()
sklearn.mixture.tests.test_gaussian_mixture.test_gaussian_mixture_fit()
sklearn.mixture.tests.test_gaussian_mixture.test_gaussian_mixture_fit_best_params()
sklearn.mixture.tests.test_gaussian_mixture.test_gaussian_mixture_fit_convergence_warning()
sklearn.mixture.tests.test_gaussian_mixture.test_gaussian_mixture_log_probabilities()
sklearn.mixture.tests.test_gaussian_mixture.test_gaussian_mixture_n_parameters()
sklearn.mixture.tests.test_gaussian_mixture.test_gaussian_mixture_predict_predict_proba()
sklearn.mixture.tests.test_gaussian_mixture.test_gaussian_mixture_verbose()
sklearn.mixture.tests.test_gaussian_mixture.test_gaussian_suffstat_sk_spherical()
sklearn.mixture.tests.test_gaussian_mixture.test_init()
sklearn.mixture.tests.test_gaussian_mixture.test_monotonic_likelihood()
sklearn.mixture.tests.test_gaussian_mixture.test_multiple_init()
sklearn.mixture.tests.test_gaussian_mixture.test_property()
sklearn.mixture.tests.test_gaussian_mixture.test_regularisation()
sklearn.mixture.tests.test_gaussian_mixture.test_sample()
sklearn.mixture.tests.test_gaussian_mixture.test_score()
sklearn.mixture.tests.test_gaussian_mixture.test_score_samples()
sklearn.mixture.tests.test_gaussian_mixture.test_suffstat_sk_diag()
sklearn.mixture.tests.test_gaussian_mixture.test_suffstat_sk_full()
sklearn.mixture.tests.test_gaussian_mixture.test_suffstat_sk_tied()
sklearn.mixture.tests.test_gaussian_mixture.test_warm_start()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/mixture/tests/test_dpgmm.py----------------------------------------
A:sklearn.mixture.tests.test_dpgmm.(X, y)->make_blobs(random_state=1)
A:sklearn.mixture.tests.test_dpgmm.dpgmm->Model(n_components=10, random_state=1, alpha=20, n_iter=50, verbose=2)
A:sklearn.mixture.tests.test_dpgmm.indices->numpy.unique(dpgmm.predict(X))
A:sklearn.mixture.tests.test_dpgmm.active->numpy.zeros(10, dtype=np.bool)
A:sklearn.mixture.tests.test_dpgmm.dpgmm_bool->Model(n_components=10, random_state=1, alpha=20, n_iter=50, verbose=True)
A:sklearn.mixture.tests.test_dpgmm.dpgmm_int->Model(n_components=10, random_state=1, alpha=20, n_iter=50, verbose=1)
A:sklearn.mixture.tests.test_dpgmm.sys.stdout->StringIO()
A:sklearn.mixture.tests.test_dpgmm.bool_output->verbose_output.readline()
A:sklearn.mixture.tests.test_dpgmm.int_output->verbose_output.readline()
A:sklearn.mixture.tests.test_dpgmm.v->numpy.array([0.1, 0.8, 0.01, 0.09])
A:sklearn.mixture.tests.test_dpgmm.a->numpy.array([0.1, 0.8, 0.01, 0.09])
A:sklearn.mixture.tests.test_dpgmm.result->assert_warns_message(DeprecationWarning, 'The function log_normalize is deprecated in 0.18 and will be removed in 0.20.', log_normalize, a)
A:sklearn.mixture.tests.test_dpgmm.b->numpy.array([0.2, 0.7, 0.05, 0.1])
A:sklearn.mixture.tests.test_dpgmm.(_, z)->g.score_samples(train_obs)
A:sklearn.mixture.tests.test_dpgmm.vbgmm->VBGMM(n_components=n_components, alpha=alpha, n_iter=1)
sklearn.mixture.tests.test_DPGMMTester(GMMTester)
sklearn.mixture.tests.test_DPGMMTester.score(self,g,train_obs)
sklearn.mixture.tests.test_VBGMMTester(GMMTester)
sklearn.mixture.tests.test_VBGMMTester.score(self,g,train_obs)
sklearn.mixture.tests.test_dpgmm.DPGMMTester(GMMTester)
sklearn.mixture.tests.test_dpgmm.DPGMMTester.score(self,g,train_obs)
sklearn.mixture.tests.test_dpgmm.TestDPGMMWithDiagCovars(unittest.TestCase,DPGMMTester)
sklearn.mixture.tests.test_dpgmm.TestDPGMMWithFullCovars(unittest.TestCase,DPGMMTester)
sklearn.mixture.tests.test_dpgmm.TestDPGMMWithSphericalCovars(unittest.TestCase,DPGMMTester)
sklearn.mixture.tests.test_dpgmm.TestDPGMMWithTiedCovars(unittest.TestCase,DPGMMTester)
sklearn.mixture.tests.test_dpgmm.TestVBGMMWithDiagCovars(unittest.TestCase,VBGMMTester)
sklearn.mixture.tests.test_dpgmm.TestVBGMMWithFullCovars(unittest.TestCase,VBGMMTester)
sklearn.mixture.tests.test_dpgmm.TestVBGMMWithSphericalCovars(unittest.TestCase,VBGMMTester)
sklearn.mixture.tests.test_dpgmm.TestVBGMMWithTiedCovars(unittest.TestCase,VBGMMTester)
sklearn.mixture.tests.test_dpgmm.VBGMMTester(GMMTester)
sklearn.mixture.tests.test_dpgmm.VBGMMTester.score(self,g,train_obs)
sklearn.mixture.tests.test_dpgmm.do_model(self,**kwds)
sklearn.mixture.tests.test_dpgmm.test_DPGMM_deprecation()
sklearn.mixture.tests.test_dpgmm.test_VBGMM_deprecation()
sklearn.mixture.tests.test_dpgmm.test_class_weights()
sklearn.mixture.tests.test_dpgmm.test_digamma()
sklearn.mixture.tests.test_dpgmm.test_gammaln()
sklearn.mixture.tests.test_dpgmm.test_log_normalize()
sklearn.mixture.tests.test_dpgmm.test_vbgmm_no_modify_alpha()
sklearn.mixture.tests.test_dpgmm.test_verbose_boolean()
sklearn.mixture.tests.test_dpgmm.test_verbose_first_level()
sklearn.mixture.tests.test_dpgmm.test_verbose_second_level()
sklearn.mixture.tests.test_dpgmm.test_wishart_log_det()
sklearn.mixture.tests.test_dpgmm.test_wishart_logz()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn0.19.1/lib/python3.6/site-packages/sklearn/mixture/tests/test_bayesian_mixture.py----------------------------------------
A:sklearn.mixture.tests.test_bayesian_mixture.rng->numpy.random.RandomState(0)
A:sklearn.mixture.tests.test_bayesian_mixture.weight_concentration->numpy.random.RandomState(0).rand(2)
A:sklearn.mixture.tests.test_bayesian_mixture.predected_norm->_log_wishart_norm(degrees_of_freedom, log_det_precisions_chol, n_features)
A:sklearn.mixture.tests.test_bayesian_mixture.expected_norm->numpy.empty(5)
A:sklearn.mixture.tests.test_bayesian_mixture.X->numpy.random.RandomState(0).rand(n_samples, n_features)
A:sklearn.mixture.tests.test_bayesian_mixture.bgmm->BayesianGaussianMixture(n_components=n_components, max_iter=100, random_state=rng, tol=0.001, reg_covar=0)
A:sklearn.mixture.tests.test_bayesian_mixture.weight_concentration_prior->numpy.random.RandomState(0).rand()
A:sklearn.mixture.tests.test_bayesian_mixture.mean_precision_prior->numpy.random.RandomState(0).rand()
A:sklearn.mixture.tests.test_bayesian_mixture.mean_prior->numpy.random.RandomState(0).rand(n_features)
A:sklearn.mixture.tests.test_bayesian_mixture.dpgmm->BayesianGaussianMixture(weight_concentration_prior_type='dirichlet_process', n_components=3, random_state=rng).fit(X)
A:sklearn.mixture.tests.test_bayesian_mixture.rand_data->RandomData(rng, scale=100)
A:sklearn.mixture.tests.test_bayesian_mixture.bgmm1->BayesianGaussianMixture(weight_concentration_prior_type=prior_type, n_components=n_components, max_iter=100, random_state=0, tol=0.001, reg_covar=0).fit(X)
A:sklearn.mixture.tests.test_bayesian_mixture.bgmm2->BayesianGaussianMixture(weight_concentration_prior_type=prior_type, n_components=n_components, max_iter=100, random_state=0, tol=0.001, reg_covar=0).fit(X + 100)
sklearn.mixture.tests.test_bayesian_mixture.test_bayesian_mixture_check_is_fitted()
sklearn.mixture.tests.test_bayesian_mixture.test_bayesian_mixture_covariance_type()
sklearn.mixture.tests.test_bayesian_mixture.test_bayesian_mixture_means_prior_initialisation()
sklearn.mixture.tests.test_bayesian_mixture.test_bayesian_mixture_precisions_prior_initialisation()
sklearn.mixture.tests.test_bayesian_mixture.test_bayesian_mixture_weight_concentration_prior_type()
sklearn.mixture.tests.test_bayesian_mixture.test_bayesian_mixture_weights()
sklearn.mixture.tests.test_bayesian_mixture.test_bayesian_mixture_weights_prior_initialisation()
sklearn.mixture.tests.test_bayesian_mixture.test_check_covariance_precision()
sklearn.mixture.tests.test_bayesian_mixture.test_compare_covar_type()
sklearn.mixture.tests.test_bayesian_mixture.test_invariant_translation()
sklearn.mixture.tests.test_bayesian_mixture.test_log_dirichlet_norm()
sklearn.mixture.tests.test_bayesian_mixture.test_log_wishart_norm()
sklearn.mixture.tests.test_bayesian_mixture.test_monotonic_likelihood()

